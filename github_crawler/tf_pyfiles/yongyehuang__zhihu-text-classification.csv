file_path,api_count,code
data_helpers.py,0,"b'#!usr/bin/python \n# -*- coding:utf-8 -*- \n\n""""""\nConstruct a Data generator.\n""""""\nimport numpy as np\nfrom tqdm import tqdm\nimport os\n\n\nclass BatchGenerator(object):\n    """""" Construct a Data generator. The input X, y should be ndarray or list like type.\n    \n    Example:\n        Data_train = BatchGenerator(X=X_train_all, y=y_train_all, shuffle=True)\n        Data_test = BatchGenerator(X=X_test_all, y=y_test_all, shuffle=False)\n        X = Data_train.X\n        y = Data_train.y\n        or:\n        X_batch, y_batch = Data_train.next_batch(batch_size)\n     """""" \n    \n    def __init__(self, X, y, shuffle=False):\n        if type(X) != np.ndarray:\n            X = np.asarray(X)\n        if type(y) != np.ndarray:\n            y = np.asarray(y)\n        self._X = X\n        self._y = y\n        self._epochs_completed = 0\n        self._index_in_epoch = 0\n        self._number_examples = self._X.shape[0]\n        self._shuffle = shuffle\n        if self._shuffle:\n            new_index = np.random.permutation(self._number_examples)\n            self._X = self._X[new_index]\n            self._y = self._y[new_index]\n                \n    @property\n    def X(self):\n        return self._X\n    \n    @property\n    def y(self):\n        return self._y\n    \n    @property\n    def num_examples(self):\n        return self._number_examples\n    \n    @property\n    def epochs_completed(self):\n        return self._epochs_completed\n    \n    def next_batch(self, batch_size):\n        """""" Return the next \'batch_size\' examples from this data set.""""""\n        start = self._index_in_epoch\n        self._index_in_epoch += batch_size\n        if self._index_in_epoch > self._number_examples:\n            # finished epoch\n            self._epochs_completed += 1\n            # Shuffle the data \n            if self._shuffle:\n                new_index = np.random.permutation(self._number_examples)\n                self._X = self._X[new_index]\n                self._y = self._y[new_index]\n            start = 0\n            self._index_in_epoch = batch_size\n            assert batch_size <= self._number_examples\n        end = self._index_in_epoch\n        return self._X[start:end], self._y[start:end]\n    \n    \ndef to_categorical(topics):\n    """"""\xe6\x8a\x8a\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84topic id \xe8\xbd\xac\xe4\xb8\xba 0\xef\xbc\x8c1\xe5\xbd\xa2\xe5\xbc\x8f\xe3\x80\x82\n    Args:\n        topics: n_sample \xe4\xb8\xaa lists, \xe9\x97\xae\xe9\xa2\x98\xe7\x9a\x84\xe8\xaf\x9d\xe9\xa2\x98\xe6\xa0\x87\xe7\xad\xbe\xe3\x80\x82\xe6\xaf\x8f\xe4\xb8\xaalist\xe5\xaf\xb9\xe5\xba\x94\xe4\xb8\x80\xe4\xb8\xaa\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8ctopic\xe4\xb8\xaa\xe6\x95\xb0\xe4\xb8\x8d\xe5\xae\x9a\xe3\x80\x82\n    return:\n        y: ndarray, shape=(sample\xef\xbc\x8c n_class)\xef\xbc\x8c \xe5\x85\xb6\xe4\xb8\xad n_class = 1999.\n    Example:\n     >>> y_batch = to_categorical(topic_batch)\n     >>> print(y_batch.shape)\n     >>> (10, 1999)\n    """"""\n    n_sample = len(topics)\n    y = np.zeros(shape=(n_sample, 1999))\n    for i in xrange(n_sample):\n        topic_index = topics[i]\n        y[i, topic_index] = 1\n    return y\n\n\ndef pad_X30(words, max_len=30):\n    """"""\xe6\x8a\x8a word_ids \xe6\x95\xb4\xe7\x90\x86\xe6\x88\x90\xe5\x9b\xba\xe5\xae\x9a\xe9\x95\xbf\xe5\xba\xa6\xe3\x80\x82\n    """"""\n    words_len = len(words)\n    words = np.asarray(words)\n    if words_len == max_len:\n        return words\n    if words_len > max_len:\n        return words[:max_len]\n    return np.hstack([words, np.zeros(max_len-words_len, dtype=int)])\n\n\ndef pad_X50(words, max_len=50):\n    """"""\xe6\x8a\x8a word_ids \xe6\x95\xb4\xe7\x90\x86\xe6\x88\x90\xe5\x9b\xba\xe5\xae\x9a\xe9\x95\xbf\xe5\xba\xa6\xe3\x80\x82\n    """"""\n    words_len = len(words)\n    words = np.asarray(words)\n    if words_len == max_len:\n        return words\n    if words_len > max_len:\n        return words[:max_len]\n    return np.hstack([words, np.zeros(max_len-words_len, dtype=int)])\n\n\ndef pad_X52(words, max_len=52):\n    """"""\xe6\x8a\x8a word_ids \xe6\x95\xb4\xe7\x90\x86\xe6\x88\x90\xe5\x9b\xba\xe5\xae\x9a\xe9\x95\xbf\xe5\xba\xa6\xe3\x80\x82\n    """"""\n    words_len = len(words)\n    words = np.asarray(words)\n    if words_len == max_len:\n        return words\n    if words_len > max_len:\n        return words[:max_len]\n    return np.hstack([words, np.zeros(max_len-words_len, dtype=int)])\n\n\ndef pad_X150(words, max_len=150):\n    """"""\xe6\x8a\x8a word_ids \xe6\x95\xb4\xe7\x90\x86\xe6\x88\x90\xe5\x9b\xba\xe5\xae\x9a\xe9\x95\xbf\xe5\xba\xa6\xe3\x80\x82\n    """"""\n    words_len = len(words)\n    words = np.asarray(words)\n    if words_len == max_len:\n        return words\n    if words_len > max_len:\n        return words[:max_len]\n    return np.hstack([words, np.zeros(max_len-words_len, dtype=int)])\n\ndef pad_X300(words, max_len=300):\n    """"""\xe6\x8a\x8a word_ids \xe6\x95\xb4\xe7\x90\x86\xe6\x88\x90\xe5\x9b\xba\xe5\xae\x9a\xe9\x95\xbf\xe5\xba\xa6\xe3\x80\x82\n    """"""\n    words_len = len(words)\n    words = np.asarray(words)\n    if words_len == max_len:\n        return words\n    if words_len > max_len:\n        return words[:max_len]\n    return np.hstack([words, np.zeros(max_len-words_len, dtype=int)])\n\n\ndef wd_cut_docs(words_id, max_len=30):\n    """"""\n    \xe6\x8a\x8a doc \xe5\x88\x87\xe5\x89\xb2\xe6\x88\x90\xe5\x8f\xa5\xe5\xad\x90\xe3\x80\x82\xe5\xa6\x82\xe6\x9e\x9c\xe5\x8f\xa5\xe5\xad\x90\xe9\x95\xbf\xe5\xba\xa6\xe8\xb6\x85\xe8\xbf\x87 max_len, \xe5\xb0\x86\xe5\x8f\xa5\xe5\xad\x90\xe6\x8c\x89\xe7\x85\xa7\xe6\x9c\x80\xe9\x95\xbf max_len\xe5\x88\x87\xe5\x89\xb2\xe6\x88\x90\xe5\xa4\x9a\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe3\x80\x82\n    Args:\n        words_id: list or np.array, \xe6\x95\xb4\xe4\xb8\xaa\xe6\x96\x87\xe6\xa1\xa3\xe7\x9a\x84\xe8\xaf\x8d\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84 id\xef\xbc\x8c[ 2336, 1468, 69, 49241, 68, 5 ... ]\n        max_len: \xe5\x88\x87\xe5\x89\xb2\xe5\x90\x8e\xe6\x9c\x80\xe9\x95\xbf\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xe9\x95\xbf\xe5\xba\xa6\xe3\x80\x82\n    Returns:\n        segs: list of list\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaalist\xef\xbc\x8c\xe5\x8d\xb3\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe3\x80\x82 \xe6\xaf\x8f\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0list\xe5\x8c\x85\xe5\x90\xab\xe5\xa4\x9a\xe4\xb8\xaaid.\n    """"""\n    if type(words_id) is np.ndarray:\n        words_id = words_id.tolist()\n    if type(words_id) is not list:\n        print(\'Type error! the words_id should be list or numpy.ndarray\')\n    set_splits = set([5, 6, 50])      # \xe5\x88\x87\xe5\x89\xb2\xe7\xac\xa6\xe5\x8f\xb7\xe6\x89\x80\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84id\n    ws_len = len(words_id)\n    cut_index = filter(lambda i: words_id[i] in set_splits, range(len(words_id)))\n    segs = list()                        # \xe5\x88\x86\xe5\x89\xb2\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\n    if len(cut_index) == 0:                  # \xe5\xa6\x82\xe6\x9e\x9c\xe6\xb2\xa1\xe6\x9c\x89\xe5\x88\x87\xe5\x89\xb2\xe7\xac\xa6\xe5\x8f\xb7\n        seg_len = len(words_id)\n        if seg_len > max_len:                # \xe5\xa6\x82\xe6\x9e\x9c\xe8\xb6\x85\xe9\x95\xbf\xef\xbc\x8c\xe5\x88\x87\xe5\x89\xb2\n            for start in xrange(0, seg_len, max_len):\n                end = min(seg_len, start+max_len)\n                segs.append(words_id[start:end])\n        else:                         # \xe5\x90\xa6\xe5\x88\x99\xef\xbc\x8c\xe6\x95\xb4\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe8\xbf\x94\xe5\x9b\x9e\n            segs.append(words_id)\n        return segs\n    if cut_index[-1] != ws_len - 1:   # \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x8d\xe6\x98\xaf\xe5\x88\x87\xe5\x89\xb2\xe7\xac\xa6\xe5\x8f\xb7\n        cut_index = cut_index + [ws_len-1]\n    cut_index = np.asarray(cut_index) + 1\n    cut_index = cut_index.tolist()\n    start = [0] + cut_index[:-1]\n    end = cut_index\n    cut_indexs = zip(start, end)\n    for index in cut_indexs:\n        if index[1] == index[0]:                 # 1.\xe5\xa6\x82\xe6\x9e\x9c\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb0\xb1\xe6\x98\xaf\xe5\x88\x86\xe5\x89\xb2\xe7\xac\xa6\xe5\x8f\xb7\xef\xbc\x8c\xe5\x8e\xbb\xe6\x8e\x89\n            continue\n        seg_len = index[1] - index[0]\n        if seg_len == 1:                        # 2.\xe5\xa6\x82\xe6\x9e\x9c\xe5\x8f\xaa\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe8\xaf\x8d\n            if words_id[index[0]] not in set_splits:    # \xe5\xb9\xb6\xe4\xb8\x94\xe4\xb8\x8d\xe6\x98\xaf\xe5\x88\x86\xe9\x9a\x94\xe7\xac\xa6\n                segs.append([words_id[index[0]]])     # \xe9\x82\xa3\xe4\xb9\x88\xe6\xb7\xbb\xe5\x8a\xa0\n            continue                            # \xe5\x90\xa6\xe5\x88\x99\xe4\xb8\xa2\xe5\xbc\x83\n        if seg_len > max_len:                   # 3.\xe5\xa6\x82\xe6\x9e\x9c\xe8\xb6\x85\xe9\x95\xbf\xef\xbc\x8c\xe5\x88\x87\xe5\x89\xb2\n            for start in xrange(index[0], index[1], max_len):\n                end = min(index[1], start+max_len)\n                segs.append(words_id[start:end])\n        else:\n            segs.append(words_id[index[0]:index[1]])  # 4.\xe6\xb7\xbb\xe5\x8a\xa0\xe5\xba\x8f\xe5\x88\x97\n    return segs\n\n\ndef wd_pad_cut_docs(words_id, doc_len=10, sent_len=30):\n    """"""\xe6\x8a\x8a doc \xe5\x88\x87\xe5\x89\xb2\xe6\x88\x90\xe5\x8f\xa5\xe5\xad\x90\xef\xbc\x8c\xe5\xb9\xb6 padding \xe6\x88\x90\xe5\x9b\xba\xe5\xae\x9a\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe6\x95\xb0\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe9\x95\xbf\xe5\xba\xa6\xe4\xb9\x9f\xe5\x9b\xba\xe5\xae\x9a\xe4\xb8\xba sent_len.\n    Args:\n        words_id: list or np.array, \xe6\x95\xb4\xe4\xb8\xaa\xe6\x96\x87\xe6\xa1\xa3\xe7\x9a\x84\xe8\xaf\x8d\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84 id\xef\xbc\x8c[ 2336, 1468, 69, 49241, 68, 5 ... ]\n        doc_len: int, \xe6\xaf\x8f\xe4\xb8\xaa\xe6\x96\x87\xe6\xa1\xa3\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xe6\x95\xb0\xef\xbc\x8c\xe8\xb6\x85\xe8\xbf\x87 doc_len \xe7\x9a\x84\xe4\xb8\xa2\xe5\xbc\x83\xef\xbc\x9b\xe5\xb0\x91\xe4\xba\x8e doc_len \xe7\x9a\x84\xe8\xa1\xa5\xe5\x85\xa8\xe3\x80\x82\n        sent_len: int, \xe6\xaf\x8f\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\xef\xbc\x8c \xe4\xb8\x8d\xe8\xb6\xb3 sent_len \xe7\x9a\x84\xe4\xbd\xbf\xe7\x94\xa8 0 (id for \'UNKNOWN\')\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xa1\xa5\xe5\x85\xa8\xe3\x80\x82\n    Returns:\n        segs: np.adarray, shape=[doc_len, sent_len].\n    """"""\n    segs4doc = wd_cut_docs(words_id, max_len=sent_len)\n    segs4doc = np.asarray(map(pad_X30, segs4doc))    # \xe6\xaf\x8f\xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86\xe9\x83\xbd\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xa1\xa5\xe9\xbd\x90\n    segs_num = segs4doc.shape[0]  # \xe5\xbd\x93\xe5\x89\x8d\xe5\x8f\xa5\xe5\xad\x90\xe6\x95\xb0\n    if segs_num >= doc_len:       # \xe5\xa6\x82\xe6\x9e\x9c\xe5\x8f\xa5\xe5\xad\x90\xe6\x95\xb0\xe5\xa4\x9f\xe4\xba\x86\n        return segs4doc[:doc_len, :]\n    if segs_num == 0:\n        return np.zeros(shape=(doc_len, sent_len), dtype=int)\n    segs4doc = np.vstack([segs4doc, np.zeros(shape=(doc_len-segs_num, sent_len), dtype=int)])\n    return segs4doc\n\n\ndef ch_cut_docs(chs_id, max_len=52):\n    """"""\n    \xe6\x8a\x8a doc \xe5\x88\x87\xe5\x89\xb2\xe6\x88\x90\xe5\x8f\xa5\xe5\xad\x90\xe3\x80\x82\xe5\xa6\x82\xe6\x9e\x9c\xe5\x8f\xa5\xe5\xad\x90\xe9\x95\xbf\xe5\xba\xa6\xe8\xb6\x85\xe8\xbf\x87 max_len, \xe5\xb0\x86\xe5\x8f\xa5\xe5\xad\x90\xe6\x8c\x89\xe7\x85\xa7\xe6\x9c\x80\xe9\x95\xbf max_len\xe5\x88\x87\xe5\x89\xb2\xe6\x88\x90\xe5\xa4\x9a\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe3\x80\x82\n    Args:\n        chs_id: list or np.array, \xe6\x95\xb4\xe4\xb8\xaa\xe6\x96\x87\xe6\xa1\xa3\xe7\x9a\x84\xe5\xad\x97\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84 id\xef\xbc\x8c[ 2336, 1468, 69, 49241, 68, 5 ... ]\n        max_len: \xe5\x88\x87\xe5\x89\xb2\xe5\x90\x8e\xe6\x9c\x80\xe9\x95\xbf\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xe9\x95\xbf\xe5\xba\xa6\xe3\x80\x82\n    Returns:\n        segs: list of list\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaalist\xef\xbc\x8c\xe5\x8d\xb3\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe3\x80\x82 \xe6\xaf\x8f\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0list\xe5\x8c\x85\xe5\x90\xab\xe5\xa4\x9a\xe4\xb8\xaaid.\n    """"""\n    if type(chs_id) is np.ndarray:\n        chs_id = chs_id.tolist()\n    if type(chs_id) is not list:\n        print(\'Type error! the chs_id should be list or numpy.ndarray\')\n    set_splits = set([8, 14, 77])      # \xe5\x88\x87\xe5\x89\xb2\xe7\xac\xa6\xe5\x8f\xb7\xe6\x89\x80\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84id\n    chs_len = len(chs_id)\n    cut_index = filter(lambda i: chs_id[i] in set_splits, range(len(chs_id)))\n    segs = list()                     # \xe5\x88\x86\xe5\x89\xb2\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\n    if len(cut_index) == 0:           # \xe5\xa6\x82\xe6\x9e\x9c\xe6\xb2\xa1\xe6\x9c\x89\xe5\x88\x87\xe5\x89\xb2\xe7\xac\xa6\xe5\x8f\xb7\n        seg_len = len(chs_id)\n        if seg_len > max_len:   # \xe5\xa6\x82\xe6\x9e\x9c\xe8\xb6\x85\xe9\x95\xbf\xef\xbc\x8c\xe5\x88\x87\xe5\x89\xb2\n            for start in xrange(0, seg_len, max_len):\n                end = min(seg_len, start+max_len)\n                segs.append(chs_id[start:end])\n        else:                         # \xe5\x90\xa6\xe5\x88\x99\xef\xbc\x8c\xe6\x95\xb4\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe8\xbf\x94\xe5\x9b\x9e\n            segs.append(chs_id)\n        return segs\n    if cut_index[-1] != chs_len - 1:   # \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x8d\xe6\x98\xaf\xe5\x88\x87\xe5\x89\xb2\xe7\xac\xa6\xe5\x8f\xb7\n        cut_index = cut_index + [chs_len-1]\n    cut_index = np.asarray(cut_index) + 1\n    cut_index = cut_index.tolist()\n    start = [0] + cut_index[:-1]\n    end = cut_index\n    cut_indexs = zip(start, end)\n    for index in cut_indexs:\n        if index[1] == index[0]:                 # 1.\xe5\xa6\x82\xe6\x9e\x9c\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb0\xb1\xe6\x98\xaf\xe5\x88\x86\xe5\x89\xb2\xe7\xac\xa6\xe5\x8f\xb7\xef\xbc\x8c\xe5\x8e\xbb\xe6\x8e\x89\n            continue\n        seg_len = index[1] - index[0]\n        if seg_len == 1:                        # 2.\xe5\xa6\x82\xe6\x9e\x9c\xe5\x8f\xaa\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe8\xaf\x8d\n            if chs_id[index[0]] not in set_splits:  # \xe5\xb9\xb6\xe4\xb8\x94\xe4\xb8\x8d\xe6\x98\xaf\xe5\x88\x86\xe9\x9a\x94\xe7\xac\xa6\n                segs.append([chs_id[index[0]]])     # \xe9\x82\xa3\xe4\xb9\x88\xe6\xb7\xbb\xe5\x8a\xa0\n            continue                            # \xe5\x90\xa6\xe5\x88\x99\xe4\xb8\xa2\xe5\xbc\x83\n        if seg_len > max_len:                   # 3.\xe5\xa6\x82\xe6\x9e\x9c\xe8\xb6\x85\xe9\x95\xbf\xef\xbc\x8c\xe5\x88\x87\xe5\x89\xb2\n            for start in xrange(index[0], index[1], max_len):\n                end = min(index[1], start+max_len)\n                segs.append(chs_id[start:end])\n        else:\n            segs.append(chs_id[index[0]:index[1]])  # 4.\xe6\xb7\xbb\xe5\x8a\xa0\xe5\xba\x8f\xe5\x88\x97\n    return segs\n\n\ndef ch_pad_cut_docs(chs_id, doc_len=10, sent_len=52):\n    """"""\xe6\x8a\x8a doc \xe5\x88\x87\xe5\x89\xb2\xe6\x88\x90\xe5\x8f\xa5\xe5\xad\x90\xef\xbc\x8c\xe5\xb9\xb6 padding \xe6\x88\x90\xe5\x9b\xba\xe5\xae\x9a\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe6\x95\xb0\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe9\x95\xbf\xe5\xba\xa6\xe4\xb9\x9f\xe5\x9b\xba\xe5\xae\x9a\xe4\xb8\xba sent_len.\n    Args:\n        chs_id: list or np.array, \xe6\x95\xb4\xe4\xb8\xaa\xe6\x96\x87\xe6\xa1\xa3\xe7\x9a\x84\xe8\xaf\x8d\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84 id\xef\xbc\x8c[ 2336, 1468, 69, 49241, 68, 5 ... ]\n        doc_len: int, \xe6\xaf\x8f\xe4\xb8\xaa\xe6\x96\x87\xe6\xa1\xa3\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xe6\x95\xb0\xef\xbc\x8c\xe8\xb6\x85\xe8\xbf\x87 doc_len \xe7\x9a\x84\xe4\xb8\xa2\xe5\xbc\x83\xef\xbc\x9b\xe5\xb0\x91\xe4\xba\x8e doc_len \xe7\x9a\x84\xe8\xa1\xa5\xe5\x85\xa8\xe3\x80\x82\n        sent_len: int, \xe6\xaf\x8f\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\xef\xbc\x8c \xe4\xb8\x8d\xe8\xb6\xb3 sent_len \xe7\x9a\x84\xe4\xbd\xbf\xe7\x94\xa8 0 (id for \'UNKNOWN\')\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xa1\xa5\xe5\x85\xa8\xe3\x80\x82\n    Returns:\n        segs: np.adarray, shape=[doc_len, sent_len].\n    """"""\n    segs4doc = ch_cut_docs(chs_id, max_len=sent_len)\n    segs4doc = np.asarray(map(pad_X52, segs4doc))      # \xe6\xaf\x8f\xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86\xe9\x83\xbd\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xa1\xa5\xe9\xbd\x90\n    segs_num = segs4doc.shape[0]  # \xe5\xbd\x93\xe5\x89\x8d\xe5\x8f\xa5\xe5\xad\x90\xe6\x95\xb0\n    if segs_num >= doc_len:       # \xe5\xa6\x82\xe6\x9e\x9c\xe5\x8f\xa5\xe5\xad\x90\xe6\x95\xb0\xe5\xa4\x9f\xe4\xba\x86\n        return segs4doc[:doc_len, :]\n    if segs_num == 0:\n        return np.zeros(shape=(doc_len, sent_len), dtype=int)\n    segs4doc = np.vstack([segs4doc, np.zeros(shape=(doc_len-segs_num, sent_len), dtype=int)])\n    return segs4doc\n\n\ndef train_batch(X, y, batch_path, batch_size=128):\n    """"""\xe5\xaf\xb9\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\x89\x93batch.""""""\n    if not os.path.exists(batch_path):\n        os.makedirs(batch_path)\n    sample_num = len(X)\n    batch_num = 0\n    for start in tqdm(xrange(0, sample_num, batch_size)):\n        end = min(start + batch_size, sample_num)\n        batch_name = batch_path + str(batch_num) + \'.npz\'\n        X_batch = X[start:end]\n        y_batch = y[start:end]\n        np.savez(batch_name, X=X_batch, y=y_batch)\n        batch_num += 1\n    print(\'Finished, batch_num=%d\' % (batch_num+1))\n\n\ndef eval_batch(X, batch_path, batch_size=128):\n    """"""\xe5\xaf\xb9\xe6\xb5\x8b\xe8\xaf\x95\xe6\x95\xb0\xe6\x8d\xae\xe6\x89\x93batch.""""""\n    if not os.path.exists(batch_path):\n        os.makedirs(batch_path)\n    sample_num = len(X)\n    print(\'sample_num=%d\' % sample_num)\n    batch_num = 0\n    for start in tqdm(xrange(0, sample_num, batch_size)):\n        end = min(start + batch_size, sample_num)\n        batch_name = batch_path + str(batch_num) + \'.npy\'\n        X_batch = X[start:end]\n        np.save(batch_name, X_batch)\n        batch_num += 1\n    print(\'Finished, batch_num=%d\' % (batch_num+1))'"
ensemble.py,0,"b'\n# coding: utf-8\n\n# ## \xe7\xba\xbf\xe4\xb8\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe8\x9e\x8d\xe5\x90\x88\n\n# In[1]:\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport os\nimport sys\nimport time \n\n\n# \xe6\xb1\x82 softmax\ndef _softmax(score):\n    """"""\xe5\xaf\xb9\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\xb1\xbb\xe5\x88\xab\xe6\xa6\x82\xe7\x8e\x87\xe8\xbf\x9b\xe8\xa1\x8c softmax \xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96.\n    score: arr.shape=[1999].\n    """"""\n    max_sc = np.max(score)   # \xe6\x9c\x80\xe5\xa4\xa7\xe5\x88\x86\xe6\x95\xb0\n    score = score - max_sc\n    exp_sc = np.exp(score)\n    sum_exp_sc = np.sum(exp_sc)\n    softmax_sc = exp_sc / sum_exp_sc\n    return softmax_sc    # \xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n    \ndef softmax(scores):\n    """"""\xe5\xaf\xb9\xe6\x89\x80\xe6\x9c\x89\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe6\xa6\x82\xe7\x8e\x87\xe8\xbf\x9b\xe8\xa1\x8c softmax \xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe5\xa4\x84\xe7\x90\x86\xe3\x80\x82\n    scores: arr.shape=[n_sample, 1999].\n    """"""\n    softmax_scs = map(_softmax, scores)\n    return np.asarray(softmax_scs)\n\n\n# ### \xe6\xa8\xa1\xe5\x9e\x8b\xe6\xb1\x82\xe5\x8a\xa0\xe6\x9d\x83\xe5\xb9\xb3\xe5\x9d\x87\n# \xe8\xbf\x99\xe9\x87\x8c\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbb\xa5\xe5\x8f\x8a\xe7\x9b\xb8\xe5\xba\x94\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xe6\x98\xaf\xe9\x80\x9a\xe8\xbf\x87 local-ensemble \xe4\xb8\xad\xe5\xaf\xb9\xe7\xba\xbf\xe4\xb8\x8b\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x9d\x83\xe9\x87\x8d\xe8\xb0\x83\xe6\x95\xb4\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe6\x9c\x80\xe5\xa5\xbd\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\n\n# In[3]:\n\ntime0 = time.time()\nscores_names =[\n    \'p1-1-bigru-512.npy\',\n    \'p1-2-bigru-512-true.npy\',\n    \'textcnn-fc-drop-title-content-256-3457-drop0.5.npy\',\n    \'f1-1-cnn-256-23457-11.npy\',\n\n    \'han-cnn-title-content-256-345.npy\',\n    \'han-cnn-title-content-256-23457-1234.npy\',\n    \'m7-rnn-cnn-256-100.npy\',\n\n    \'p3-2-cnn-256-2357.npy\',\n    \'p3-cnn-512-23457.npy\',\n    \'textcnn-fc-drop-title-content-256-345.npy\',   \n    \'textcnn-fc-drop-title-content-256-3457-drop0.2.npy\',\n\n    \'m9-han-bigru-title-content-512-30.npy\',\n    \'m9-2-han-bigru-title-content-512-30.npy\',\n    \'han-bigru-title-content-256-30.npy\',\n    \'m8-han-bigru-256-30.npy\',\n\n    \'attention-bigru-title-content-256.npy\',\n    \'m7-2-rnn-cnn-128-100.npy\',\n    \'textcnn-fc-title-content-256-345.npy\',\n    \'m1-2-fasttext-topicinfo.npy\',\n\n    \'ch3-1-cnn-256-2345.npy\',\n    \'ch3-2-cnn-256-23457.npy\', \n    \'ch4-1-han-bigru-256-52.npy\',    \n    \'ch5-1-2embed-rnn256-cnn2345.npy\',\n\n    \'p4-1-han-bigru-256.npy\',\n    \'ch6-1-han-cnn-2345-1234.npy\',\n    \'p5-1-2embed-rnn256-cnn2345.npy\',\n    \'ch5-2-2embed-rnn512-cnn3457.npy\',\n\n    \'c1-1-cnn-max-256-23457.npy\',\n    \'c1-2-cnn-256-345710.npy\',     \n    \'c2-1-bigru-256.npy\',\n    \n    \'textcnn-fc-drop-title-content-256-345-cross3cross0.npy\',\n    \'textcnn-fc-drop-title-content-256-345-cross3cross1.npy\',\n    \'textcnn-fc-drop-title-content-256-345-cross3cross2.npy\',\n    \'p3-3-cnn-max-256-345710.npy\',\n    \'textcnn-title-256-len50.npy\',\n    \'ch7-1-2embed-rnn256-hcnn-2345-1234.npy\',\n\n#    \'p2-1-rnn-cnn-256-256.npy\',\n]  \n\n# weights = [  9.75938817,   8.63945014,  2.98289344,   3.72323394,   5.04378259,\n#   0.06551187,  -0.79412528,   4.90162676,   1.17452791,\n#  -1.46124679,  -0.25384273,   5.50925013,   2.84186738,  -0.93016907,\n#   5.16519035,  -0.47061662,   2.75998217,   2.58152296,  -1.24553333,\n#   2.43288558,   6.17376317,   5.59323762,  10.46123521,   5.29952925,\n#   3.72042086,   5.46707444,   5.51516916,   5.82352659,   1.27847427,\n#  -0.52930247,  -1.99052155,  -3.0938045,   -2.07007845,   4.19963813,\n#   2.10593832,   1.74174258, -0.21665029]\n\nweights = [  9.75938817,   8.63945014,  2.98289344,   3.72323394,   5.04378259,\n   0.06551187,  -0.79412528,   4.90162676,   1.17452791,\n  -1.46124679,  -0.25384273,   5.50925013,   2.84186738,  -0.93016907,\n   5.16519035,  -0.47061662,   2.75998217,   2.58152296,  -1.24553333,\n   2.43288558,   6.17376317,   5.59323762,  10.46123521,   5.29952925,\n   3.72042086,   5.46707444,   5.51516916,   5.82352659,   1.27847427,\n  -0.52930247,  -1.99052155,  -3.0938045,   -2.07007845,   4.19963813,\n   2.10593832,   1.74174258]\n\n\nprint(len(scores_names), len(weights))\nprint(\'All %d models\' % len(weights))\nsum_scores = np.zeros((217360, 1999), dtype=float)\nscores_path = \'scores/\'\nfor i in xrange(len(weights)):\n    scores_name = scores_names[i]\n    print(\'%d/%d, scores_name=%s\' %(i+1, len(weights),scores_name))\n    score = np.load(scores_path + scores_name)\n    score = softmax(score) # \xe5\x8a\xa0\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\n    sum_scores = sum_scores + score* weights[i]\nprint(\'sum_scores.shape=\',sum_scores.shape)\nprint(\'Finished , costed time %g s\' % (time.time() - time0))\n\n\n\n# \xe5\x86\x99\xe5\x85\xa5 result\nresult_path = \'ye-final36-result.csv\'\n\ndef write_result(sum_scores, result_path):\n    """"""\xe6\x8a\x8a\xe7\xbb\x93\xe6\x9e\x9c\xe5\x86\x99\xe5\x88\xb0 sum_result.csv \xe4\xb8\xad""""""\n    print(\'Begin computing...\')\n    predict_labels_list = map(lambda label: label.argsort()[-1:-6:-1], sum_scores) # \xe5\x8f\x96\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x845\xe4\xb8\xaa\xe4\xb8\x8b\xe6\xa0\x87\n    eval_question = np.load(\'data/eval_question.npy\')\n    with open(\'data/sr_topic2id.pkl\', \'rb\') as inp:\n        sr_topic2id = pickle.load(inp)\n        sr_id2topic = pickle.load(inp)\n    pred_labels = np.asarray(predict_labels_list).reshape([-1])\n    pred_topics = sr_id2topic[pred_labels].values.reshape([-1, 5])   # \xe8\xbd\xac\xe4\xb8\xba topic\n    df_result = pd.DataFrame({\'question\':eval_question, \'tid0\': pred_topics[:,0], \'tid1\':pred_topics[:, 1],\n                         \'tid2\': pred_topics[:,2], \'tid3\':pred_topics[:,3],\'tid4\': pred_topics[:,4]})\n    df_result.to_csv(result_path, index=False, header=False)\n    print(\'Finished writing the result\')\n    return df_result\n\ntime0 = time.time()\nwrite_result(sum_scores, result_path)\nprint(\'Result path %s, costed time %g s\' % (result_path, time.time() - time0))\n\n'"
evaluator.py,0,"b'# -*- coding:utf-8 -*-\nimport math\n\n""""""\xe7\x9f\xa5\xe4\xb9\x8e\xe6\x8f\x90\xe4\xbe\x9b\xe7\x9a\x84\xe8\xaf\x84\xe6\xb5\x8b\xe6\x96\xb9\xe6\xa1\x88""""""\n\ndef score_eval(predict_label_and_marked_label_list):\n    """"""\n    :param predict_label_and_marked_label_list: \xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\x83\xe7\xbb\x84\xe5\x88\x97\xe8\xa1\xa8\xe3\x80\x82\xe4\xbe\x8b\xe5\xa6\x82\n    [ ([1, 2, 3, 4, 5], [4, 5, 6, 7]),\n      ([3, 2, 1, 4, 7], [5, 7, 3])\n     ]\n    \xe9\x9c\x80\xe8\xa6\x81\xe6\xb3\xa8\xe6\x84\x8f\xe8\xbf\x99\xe9\x87\x8c predict_label \xe6\x98\xaf\xe5\x8e\xbb\xe9\x87\x8d\xe5\xa4\x8d\xe7\x9a\x84\xef\xbc\x8c\xe4\xbe\x8b\xe5\xa6\x82 [1,2,3,2,4,1,6]\xef\xbc\x8c\xe5\x8e\xbb\xe9\x87\x8d\xe5\x90\x8e\xe5\x8f\x98\xe6\x88\x90[1,2,3,4,6]\n    \n    marked_label_list \xe6\x9c\xac\xe8\xba\xab\xe6\xb2\xa1\xe6\x9c\x89\xe9\xa1\xba\xe5\xba\x8f\xe6\x80\xa7\xef\xbc\x8c\xe4\xbd\x86\xe6\x8f\x90\xe4\xba\xa4\xe7\xbb\x93\xe6\x9e\x9c\xe6\x9c\x89\xef\xbc\x8c\xe4\xbe\x8b\xe5\xa6\x82\xe4\xb8\x8a\xe4\xbe\x8b\xe7\x9a\x84\xe5\x91\xbd\xe4\xb8\xad\xe6\x83\x85\xe5\x86\xb5\xe5\x88\x86\xe5\x88\xab\xe4\xb8\xba\n    [0\xef\xbc\x8c0\xef\xbc\x8c0\xef\xbc\x8c1\xef\xbc\x8c1]   (4\xef\xbc\x8c5\xe5\x91\xbd\xe4\xb8\xad)\n    [1\xef\xbc\x8c0\xef\xbc\x8c0\xef\xbc\x8c0\xef\xbc\x8c1]   (3\xef\xbc\x8c7\xe5\x91\xbd\xe4\xb8\xad)\n\n    """"""\n    right_label_num = 0  #\xe6\x80\xbb\xe5\x91\xbd\xe4\xb8\xad\xe6\xa0\x87\xe7\xad\xbe\xe6\x95\xb0\xe9\x87\x8f\n    right_label_at_pos_num = [0, 0, 0, 0, 0]  #\xe5\x9c\xa8\xe5\x90\x84\xe4\xb8\xaa\xe4\xbd\x8d\xe7\xbd\xae\xe4\xb8\x8a\xe6\x80\xbb\xe5\x91\xbd\xe4\xb8\xad\xe6\x95\xb0\xe9\x87\x8f\n    sample_num = 0    #\xe6\x80\xbb\xe9\x97\xae\xe9\xa2\x98\xe6\x95\xb0\xe9\x87\x8f\n    all_marked_label_num = 0    #\xe6\x80\xbb\xe6\xa0\x87\xe7\xad\xbe\xe6\x95\xb0\xe9\x87\x8f\n    for predict_labels, marked_labels in predict_label_and_marked_label_list:\n        sample_num += 1\n        marked_label_set = set(marked_labels)\n        all_marked_label_num += len(marked_label_set)\n        for pos, label in zip(range(0, min(len(predict_labels), 5)), predict_labels):\n            if label in marked_label_set:     #\xe5\x91\xbd\xe4\xb8\xad\n                right_label_num += 1\n                right_label_at_pos_num[pos] += 1\n\n    precision = 0.0\n    for pos, right_num in zip(range(0, 5), right_label_at_pos_num):\n        precision += ((right_num / float(sample_num))) / math.log(2.0 + pos)  # \xe4\xb8\x8b\xe6\xa0\x870-4 \xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 pos1-5 + 1\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x9c\x80\xe7\xbb\x88+2\n    recall = float(right_label_num) / all_marked_label_num\n\n    return precision, recall, (precision * recall) / (precision + recall )'"
test.py,1,"b'# -*- coding:utf-8 -*-\r\n\r\nimport tensorflow as tf\r\n\r\na =tf.Variable([1,2])\r\nprint(a)'"
data_process/char2id.py,0,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport pickle\r\nfrom multiprocessing import Pool\r\nfrom tqdm import tqdm\r\nimport time\r\n\r\n\r\nsave_path = \'../data/\'\r\nwith open(save_path + \'sr_char2id.pkl\', \'rb\') as inp:\r\n    sr_id2char = pickle.load(inp)\r\n    sr_char2id = pickle.load(inp)\r\ndict_char2id = dict()\r\nfor i in xrange(len(sr_char2id)):\r\n    dict_char2id[sr_char2id.index[i]] = sr_char2id.values[i]\r\n\r\n\r\ndef get_id(char):\r\n    """"""\xe8\x8e\xb7\xe5\x8f\x96 char \xe6\x89\x80\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84 id.\r\n    \xe5\xa6\x82\xe6\x9e\x9c\xe8\xaf\xa5\xe5\xad\x97\xe4\xb8\x8d\xe5\x9c\xa8\xe5\xad\x97\xe5\x85\xb8\xe4\xb8\xad\xef\xbc\x8c\xe7\x94\xa81\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x9b\xbf\xe6\x8d\xa2\xe3\x80\x82\r\n    """"""\r\n    if char not in dict_char2id:\r\n        return 1\r\n    else:\r\n        return dict_char2id[char]\r\n\r\n\r\ndef get_id4chars(chars):\r\n    """"""\xe6\x8a\x8a chars \xe8\xbd\xac\xe4\xb8\xba \xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84 id""""""\r\n    chars = chars.strip().split(\',\')  # \xe5\x85\x88\xe5\x88\x86\xe5\xbc\x80\xe5\xad\x97\r\n    ids = map(get_id, chars)          # \xe8\x8e\xb7\xe5\x8f\x96id\r\n    return ids\r\n\r\n\r\ndef test_char2id():\r\n    """"""\xe6\x8a\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe5\xad\x97\xe8\xbd\xac\xe6\x88\x90\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84id\xe3\x80\x82""""""\r\n    time0 = time.time()\r\n    print(\'Processing eval data.\')\r\n    df_eval = pd.read_csv(\'../raw_data/question_eval_set.txt\', sep=\'\\t\',  usecols=[0, 1, 3],\r\n                          names=[\'question_id\', \'char_title\', \'char_content\'], dtype={\'question_id\': object})\r\n    print(\'test question number %d\' % len(df_eval))\r\n    # \xe6\xb2\xa1\xe6\x9c\x89 title \xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xe7\x94\xa8 content \xe6\x9d\xa5\xe6\x9b\xbf\xe6\x8d\xa2\r\n    na_title_indexs = list()\r\n    for i in xrange(len(df_eval)):\r\n        char_title = df_eval.char_title.values[i]\r\n        if type(char_title) is float:\r\n            na_title_indexs.append(i)\r\n    print(\'There are %d test questions without title.\' % len(na_title_indexs))\r\n    for na_index in na_title_indexs:\r\n        df_eval.at[na_index, \'char_title\'] = df_eval.at[na_index, \'char_content\']\r\n    # \xe6\xb2\xa1\xe6\x9c\x89 content \xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xe7\x94\xa8 title \xe6\x9d\xa5\xe6\x9b\xbf\xe6\x8d\xa2\r\n    na_content_indexs = list()\r\n    for i in tqdm(xrange(len(df_eval))):\r\n        char_content = df_eval.char_content.values[i]\r\n        if type(char_content) is float:\r\n            na_content_indexs.append(i)\r\n    print(\'There are %d test questions without content.\' % len(na_content_indexs))\r\n    for na_index in tqdm(na_content_indexs):\r\n        df_eval.at[na_index, \'char_content\'] = df_eval.at[na_index, \'char_title\']\r\n    # \xe8\xbd\xac\xe4\xb8\xba id \xe5\xbd\xa2\xe5\xbc\x8f\r\n    p = Pool()\r\n    eval_title = np.asarray(p.map(get_id4chars, df_eval.char_title.values))\r\n    np.save(\'../data/ch_eval_title.npy\', eval_title)\r\n    eval_content = np.asarray(p.map(get_id4chars, df_eval.char_content.values))\r\n    np.save(\'../data/ch_eval_content.npy\', eval_content)\r\n    p.close()\r\n    p.join()\r\n    print(\'Finished changing the eval chars to ids. Costed time %g s\' % (time.time()-time0))\r\n\r\n\r\ndef train_char2id():\r\n    """"""\xe6\x8a\x8a\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe5\xad\x97\xe8\xbd\xac\xe6\x88\x90\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84id\xe3\x80\x82""""""\r\n    time0 = time.time()\r\n    print(\'Processing train data.\')\r\n    df_train = pd.read_csv(\'../raw_data/question_train_set.txt\', sep=\'\\t\', usecols=[0, 1, 3],\r\n                           names=[\'question_id\', \'char_title\', \'char_content\'], dtype={\'question_id\': object})\r\n    print(\'training question number %d \' % len(df_train))\r\n    # \xe6\xb2\xa1\xe6\x9c\x89 content \xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xe7\x94\xa8 title \xe6\x9d\xa5\xe6\x9b\xbf\xe6\x8d\xa2\r\n    na_content_indexs = list()\r\n    for i in tqdm(xrange(len(df_train))):\r\n        char_content = df_train.char_content.values[i]\r\n        if type(char_content) is float:\r\n            na_content_indexs.append(i)\r\n    print(\'There are %d train questions without content.\' % len(na_content_indexs))\r\n    for na_index in tqdm(na_content_indexs):\r\n        df_train.at[na_index, \'char_content\'] = df_train.at[na_index, \'char_title\']\r\n    # \xe6\xb2\xa1\xe6\x9c\x89 title \xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c \xe4\xb8\x8e\xe8\xaf\x8d\xe4\xb8\x80\xe6\xa0\xb7\xe4\xb8\xa2\xe5\xbc\x83\xe4\xb8\x8b\xe9\x9d\xa2\xe6\xa0\xb7\xe6\x9c\xac\r\n    na_title_indexs = [328877, 422123, 633584, 768738, 818616, 876828, 1273673, 1527297,\r\n              1636237, 1682969, 2052477, 2628516, 2657464, 2904162, 2993517]\r\n    for i in xrange(len(df_train)):\r\n        char_title = df_train.char_title.values[i]\r\n        if type(char_title) is float:\r\n            na_title_indexs.append(i)\r\n    print(\'There are %d train questions without title.\' % len(na_title_indexs))\r\n    df_train = df_train.drop(na_title_indexs)\r\n    print(\'After dropping, training question number(should be 2999952) = %d\' % len(df_train))\r\n    # \xe8\xbd\xac\xe4\xb8\xba id \xe5\xbd\xa2\xe5\xbc\x8f\r\n    p = Pool()\r\n    train_title = np.asarray(p.map(get_id4chars, df_train.char_title.values))\r\n    np.save(\'../data/ch_train_title.npy\', train_title)\r\n    train_content = np.asarray(p.map(get_id4chars, df_train.char_content.values))\r\n    np.save(\'../data/ch_train_content.npy\', train_content)\r\n    p.close()\r\n    p.join()\r\n    print(\'Finished changing the training chars to ids. Costed time %g s\' % (time.time() - time0))\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    test_char2id()\r\n    train_char2id()\r\n\r\n\r\n\r\n\r\n\r\n\r\n'"
data_process/creat_batch_data.py,0,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport pickle\r\nfrom multiprocessing import Pool\r\nimport sys\r\nimport os\r\n\r\nsys.path.append(\'../\')\r\nfrom data_helpers import pad_X30\r\nfrom data_helpers import pad_X150\r\nfrom data_helpers import pad_X52\r\nfrom data_helpers import pad_X300\r\nfrom data_helpers import train_batch\r\nfrom data_helpers import eval_batch\r\n\r\n"""""" \xe6\x8a\x8a\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe6\x8c\x89\xe7\x85\xa7 batch_size(128) \xe8\xbf\x9b\xe8\xa1\x8c\xe6\x89\x93\xe5\x8c\x85\xe3\x80\x82\xe5\x8f\x96 10\xe4\xb8\x87 \xe6\xa0\xb7\xe6\x9c\xac\xe4\xbd\x9c\xe4\xb8\xba\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe3\x80\x82\r\nword_title_len = 30.\r\nword_content_len = 150.\r\nchar_title_len = 52.\r\nchar_content_len = 300.\r\n""""""\r\n\r\n\r\nwd_train_path = \'../data/wd-data/data_train/\'\r\nwd_valid_path = \'../data/wd-data/data_valid/\'\r\nwd_test_path = \'../data/wd-data/data_test/\'\r\nch_train_path = \'../data/ch-data/data_train/\'\r\nch_valid_path = \'../data/ch-data/data_valid/\'\r\nch_test_path = \'../data/ch-data/data_test/\'\r\npaths = [wd_train_path, wd_valid_path, wd_test_path,\r\n         ch_train_path, ch_valid_path, ch_test_path]\r\nfor each in paths:\r\n    if not os.path.exists(each):\r\n        os.makedirs(each)\r\n\r\nwith open(\'../data/sr_topic2id.pkl\', \'rb\') as inp:\r\n    sr_topic2id = pickle.load(inp)\r\ndict_topic2id = dict()\r\nfor i in xrange(len(sr_topic2id)):\r\n    dict_topic2id[sr_topic2id.index[i]] = sr_topic2id.values[i]\r\n\r\n\r\ndef topics2ids(topics):\r\n    """"""\xe6\x8a\x8a chars \xe8\xbd\xac\xe4\xb8\xba \xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84 id""""""\r\n    topics = topics.split(\',\')\r\n    ids = map(lambda topic: dict_topic2id[topic], topics)          # \xe8\x8e\xb7\xe5\x8f\x96id\r\n    return ids\r\n\r\n\r\ndef get_lables():\r\n    """"""\xe8\x8e\xb7\xe5\x8f\x96\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\x89\x80\xe6\x9c\x89\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\xe3\x80\x82\xe6\xb3\xa8\xe6\x84\x8f\xe4\xb9\x8b\xe5\x89\x8d\xe5\x9c\xa8\xe5\xa4\x84\xe7\x90\x86\xe6\x95\xb0\xe6\x8d\xae\xe6\x97\xb6\xe4\xb8\xa2\xe5\xbc\x83\xe4\xba\x86\xe9\x83\xa8\xe5\x88\x86\xe6\xb2\xa1\xe6\x9c\x89 title \xe7\x9a\x84\xe6\xa0\xb7\xe6\x9c\xac\xe3\x80\x82""""""\r\n    df_question_topic = pd.read_csv(\'../raw_data/question_topic_train_set.txt\', sep=\'\\t\',\r\n                                    names=[\'questions\', \'topics\'], dtype={\'questions\': object, \'topics\': object})\r\n    na_title_indexs = [328877, 422123, 633584, 768738, 818616, 876828, 1273673, 1527297,\r\n                       1636237, 1682969, 2052477, 2628516, 2657464, 2904162, 2993517]\r\n    df_question_topic = df_question_topic.drop(na_title_indexs)\r\n    p = Pool()\r\n    y = p.map(topics2ids, df_question_topic.topics.values)\r\n    p.close()\r\n    p.join()\r\n    return np.asarray(y)\r\n\r\n\r\n# word \xe6\x95\xb0\xe6\x8d\xae\xe6\x89\x93\xe5\x8c\x85\r\ndef wd_train_get_batch(title_len=30, content_len=150, batch_size=128):\r\n    print(\'loading word train_title and train_content.\')\r\n    train_title = np.load(\'../data/wd_train_title.npy\')\r\n    train_content = np.load(\'../data/wd_train_content.npy\')\r\n    p = Pool()\r\n    X_title = np.asarray(p.map(pad_X30, train_title))\r\n    X_content = np.asarray(p.map(pad_X150, train_content))\r\n    p.close()\r\n    p.join()\r\n    X = np.hstack([X_title, X_content])\r\n    print(\'getting labels, this should cost minutes, please wait.\')\r\n    y = get_lables()\r\n    print(\'y.shape=\', y.shape)\r\n    np.save(\'../data/y_tr.npy\', y)\r\n    # \xe5\x88\x92\xe5\x88\x86\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\r\n    sample_num = X.shape[0]\r\n    np.random.seed(13)\r\n    valid_num = 100000\r\n    new_index = np.random.permutation(sample_num)\r\n    X = X[new_index]\r\n    y = y[new_index]\r\n    X_valid = X[:valid_num]\r\n    y_valid = y[:valid_num]\r\n    X_train = X[valid_num:]\r\n    y_train = y[valid_num:]\r\n    print(\'X_train.shape=\', X_train.shape, \'y_train.shape=\', y_train.shape)\r\n    print(\'X_valid.shape=\', X_valid.shape, \'y_valid.shape=\', y_valid.shape)\r\n    print(\'creating batch data.\')\r\n    # \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe6\x89\x93batch\r\n    sample_num = len(X_valid)\r\n    print(\'valid_sample_num=%d\' % sample_num)\r\n    train_batch(X_valid, y_valid, wd_valid_path, batch_size)\r\n    # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\x89\x93batch\r\n    sample_num = len(X_train)\r\n    print(\'train_sample_num=%d\' % sample_num)\r\n    train_batch(X_train, y_train, wd_train_path, batch_size)\r\n\r\n\r\ndef wd_test_get_batch(title_len=30, content_len=150, batch_size=128):\r\n    eval_title = np.load(\'../data/wd_eval_title.npy\')\r\n    eval_content = np.load(\'../data/wd_eval_content.npy\')\r\n    p = Pool()\r\n    X_title = np.asarray(p.map(pad_X30, eval_title))\r\n    X_content = np.asarray(p.map(pad_X150, eval_content))\r\n    p.close()\r\n    p.join()\r\n    X = np.hstack([X_title, X_content])\r\n    sample_num = len(X)\r\n    print(\'eval_sample_num=%d\' % sample_num)\r\n    eval_batch(X, wd_test_path, batch_size)\r\n\r\n\r\n# char \xe6\x95\xb0\xe6\x8d\xae\xe6\x89\x93\xe5\x8c\x85\r\ndef ch_train_get_batch(title_len=52, content_len=300, batch_size=128):\r\n    print(\'loading char train_title and train_content.\')\r\n    train_title = np.load(\'../data/ch_train_title.npy\')\r\n    train_content = np.load(\'../data/ch_train_content.npy\')\r\n    p = Pool()\r\n    X_title = np.asarray(p.map(pad_X52, train_title))\r\n    X_content = np.asarray(p.map(pad_X300, train_content))\r\n    p.close()\r\n    p.join()\r\n    X = np.hstack([X_title, X_content])\r\n    y = np.load(\'../data/y_tr.npy\')\r\n    # \xe5\x88\x92\xe5\x88\x86\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\r\n    sample_num = X.shape[0]\r\n    np.random.seed(13)\r\n    valid_num = 100000\r\n    new_index = np.random.permutation(sample_num)\r\n    X = X[new_index]\r\n    y = y[new_index]\r\n    X_valid = X[:valid_num]\r\n    y_valid = y[:valid_num]\r\n    X_train = X[valid_num:]\r\n    y_train = y[valid_num:]\r\n    print(\'X_train.shape=\', X_train.shape, \'y_train.shape=\', y_train.shape)\r\n    print(\'X_valid.shape=\', X_valid.shape, \'y_valid.shape=\', y_valid.shape)\r\n    # \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe6\x89\x93batch\r\n    print(\'creating batch data.\')\r\n    sample_num = len(X_valid)\r\n    print(\'valid_sample_num=%d\' % sample_num)\r\n    train_batch(X_valid, y_valid, ch_valid_path, batch_size)\r\n    # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\x89\x93batch\r\n    sample_num = len(X_train)\r\n    print(\'train_sample_num=%d\' % sample_num)\r\n    train_batch(X_train, y_train, ch_train_path, batch_size)\r\n\r\n\r\ndef ch_test_get_batch(title_len=52, content_len=300, batch_size=128):\r\n    eval_title = np.load(\'../data/ch_eval_title.npy\')\r\n    eval_content = np.load(\'../data/ch_eval_content.npy\')\r\n    p = Pool()\r\n    X_title = np.asarray(p.map(pad_X52, eval_title))\r\n    X_content = np.asarray(p.map(pad_X300, eval_content))\r\n    p.close()\r\n    p.join()\r\n    X = np.hstack([X_title, X_content])\r\n    sample_num = len(X)\r\n    print(\'eval_sample_num=%d\' % sample_num)\r\n    eval_batch(X, ch_test_path, batch_size)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    wd_train_get_batch()\r\n    wd_test_get_batch()\r\n    ch_train_get_batch()\r\n    ch_test_get_batch()\r\n'"
data_process/creat_batch_seg.py,0,"b""# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\nfrom multiprocessing import Pool\r\nimport sys\r\nimport os\r\n\r\nsys.path.append('../')\r\nfrom data_helpers import pad_X30\r\nfrom data_helpers import pad_X52\r\nfrom data_helpers import wd_pad_cut_docs\r\nfrom data_helpers import ch_pad_cut_docs\r\nfrom data_helpers import train_batch\r\nfrom data_helpers import eval_batch\r\n\r\n\r\nwd_train_path = '../data/wd-data/seg_train/'\r\nwd_valid_path = '../data/wd-data/seg_valid/'\r\nwd_test_path = '../data/wd-data/seg_test/'\r\nch_train_path = '../data/ch-data/seg_train/'\r\nch_valid_path = '../data/ch-data/seg_valid/'\r\nch_test_path = '../data/ch-data/seg_test/'\r\npaths = [wd_train_path, wd_valid_path, wd_test_path,\r\n         ch_train_path, ch_valid_path, ch_test_path]\r\nfor each in paths:\r\n    if not os.path.exists(each):\r\n        os.makedirs(each)\r\n\r\n\r\n# word \xe6\x95\xb0\xe6\x8d\xae\xe6\x89\x93\xe5\x8c\x85\r\ndef wd_train_get_batch(title_len=30, batch_size=128):\r\n    print('loading word train_title and train_content, this should cost minutes, please wait.')\r\n    train_title = np.load('../data/wd_train_title.npy')\r\n    train_content = np.load('../data/wd_train_content.npy')\r\n    p = Pool(6)\r\n    X_title = np.asarray(p.map(pad_X30, train_title))\r\n    X_content = np.asarray(p.map(wd_pad_cut_docs, train_content))\r\n    p.close()\r\n    p.join()\r\n    X_content.shape = [-1, 30*10]\r\n    X = np.hstack([X_title, X_content])\r\n    y = np.load('../data/y_tr.npy')\r\n    # \xe5\x88\x92\xe5\x88\x86\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\r\n    sample_num = X.shape[0]\r\n    np.random.seed(13)\r\n    valid_num = 100000\r\n    new_index = np.random.permutation(sample_num)\r\n    X = X[new_index]\r\n    y = y[new_index]\r\n    X_valid = X[:valid_num]\r\n    y_valid = y[:valid_num]\r\n    X_train = X[valid_num:]\r\n    y_train = y[valid_num:]\r\n    print('X_train.shape=', X_train.shape, 'y_train.shape=', y_train.shape)\r\n    print('X_valid.shape=', X_valid.shape, 'y_valid.shape=', y_valid.shape)\r\n    # \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe6\x89\x93 batch\r\n    print('creating batch data.')\r\n    sample_num = len(X_valid)\r\n    print('valid_sample_num=%d' % sample_num)\r\n    train_batch(X_valid, y_valid, wd_valid_path, batch_size)\r\n    # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\x89\x93 batch\r\n    sample_num = len(X_train)\r\n    print('train_sample_num=%d' % sample_num)\r\n    train_batch(X_train, y_train, wd_train_path, batch_size)\r\n\r\n\r\ndef wd_test_get_batch(title_len=30, batch_size=128):\r\n    print('loading word eval_title and eval_content.')\r\n    eval_title = np.load('../data/wd_eval_title.npy')\r\n    eval_content = np.load('../data/wd_eval_content.npy')\r\n    p = Pool(6)\r\n    X_title = np.asarray(p.map(pad_X30, eval_title))\r\n    X_content = np.asarray(p.map(wd_pad_cut_docs, eval_content))\r\n    p.close()\r\n    p.join()\r\n    X_content.shape = [-1, 30*10]\r\n    X = np.hstack([X_title, X_content])\r\n    sample_num = len(X)\r\n    print('eval_sample_num=%d' % sample_num)\r\n    eval_batch(X, wd_test_path, batch_size)\r\n\r\n\r\n# char \xe6\x95\xb0\xe6\x8d\xae\xe6\x89\x93\xe5\x8c\x85\r\ndef ch_train_get_batch(title_len=52, batch_size=128):\r\n    print('loading char train_title and train_content, this should cost minutes, please wait.')\r\n    train_title = np.load('../data/ch_train_title.npy')\r\n    train_content = np.load('../data/ch_train_content.npy')\r\n    p = Pool(8)\r\n    X_title = np.asarray(p.map(pad_X52, train_title))\r\n    X_content = np.asarray(p.map(ch_pad_cut_docs, train_content))\r\n    p.close()\r\n    p.join()\r\n    X_content.shape = [-1, 52*10]\r\n    X = np.hstack([X_title, X_content])\r\n    y = np.load('../data/y_tr.npy')\r\n    # \xe5\x88\x92\xe5\x88\x86\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\r\n    sample_num = X.shape[0]\r\n    np.random.seed(13)\r\n    valid_num = 100000\r\n    new_index = np.random.permutation(sample_num)\r\n    X = X[new_index]\r\n    y = y[new_index]\r\n    X_valid = X[:valid_num]\r\n    y_valid = y[:valid_num]\r\n    X_train = X[valid_num:]\r\n    y_train = y[valid_num:]\r\n    print('X_train.shape=', X_train.shape, 'y_train.shape=', y_train.shape)\r\n    print('X_valid.shape=', X_valid.shape, 'y_valid.shape=', y_valid.shape)\r\n    # \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe6\x89\x93batch\r\n    print('creating batch data.')\r\n    sample_num = len(X_valid)\r\n    print('valid_sample_num=%d' % sample_num)\r\n    train_batch(X_valid, y_valid, ch_valid_path, batch_size)\r\n    # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\x89\x93batch\r\n    sample_num = len(X_train)\r\n    print('train_sample_num=%d' % sample_num)\r\n    train_batch(X_train, y_train, ch_train_path, batch_size)\r\n\r\n\r\ndef ch_test_get_batch(title_len=52, batch_size=128):\r\n    print('loading char eval_title and eval_content.')\r\n    eval_title = np.load('../data/ch_eval_title.npy')\r\n    eval_content = np.load('../data/ch_eval_content.npy')\r\n    p = Pool()\r\n    X_title = np.asarray(p.map(pad_X52, eval_title))\r\n    X_content = np.asarray(p.map(ch_pad_cut_docs, eval_content))\r\n    p.close()\r\n    p.join()\r\n    X_content.shape = [-1, 52*10]\r\n    X = np.hstack([X_title, X_content])\r\n    sample_num = len(X)\r\n    print('eval_sample_num=%d' % sample_num)\r\n    eval_batch(X, ch_test_path, batch_size)\r\n\r\n\r\nif __name__ == '__main__':\r\n    wd_train_get_batch()\r\n    wd_test_get_batch()\r\n    ch_train_get_batch()\r\n    ch_test_get_batch()\r\n"""
data_process/embed2ndarray.py,0,"b'# -*- coding:utf-8 -*- \r\n\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport word2vec\r\nimport pickle\r\nimport os\r\n\r\nSPECIAL_SYMBOL = [\'<PAD>\', \'<EOS>\']  # add these special symbols to word(char) embeddings.\r\n\r\n\r\ndef get_word_embedding():\r\n    """"""\xe6\x8f\x90\xe5\x8f\x96\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe5\xb9\xb6\xe4\xbf\x9d\xe5\xad\x98\xe8\x87\xb3 ../data/word_embedding.npy""""""\r\n    print(\'getting the word_embedding.npy\')\r\n    wv = word2vec.load(\'../raw_data/word_embedding.txt\')\r\n    word_embedding = wv.vectors\r\n    words = wv.vocab\r\n    n_special_sym = len(SPECIAL_SYMBOL)\r\n    sr_id2word = pd.Series(words, index=range(n_special_sym, n_special_sym + len(words)))\r\n    sr_word2id = pd.Series(range(n_special_sym, n_special_sym + len(words)), index=words)\r\n    # \xe6\xb7\xbb\xe5\x8a\xa0\xe7\x89\xb9\xe6\xae\x8a\xe7\xac\xa6\xe5\x8f\xb7\xef\xbc\x9a<PAD>:0, <UNK>:1\r\n    embedding_size = 256\r\n    vec_special_sym = np.random.randn(n_special_sym, embedding_size)\r\n    for i in range(n_special_sym):\r\n        sr_id2word[i] = SPECIAL_SYMBOL[i]\r\n        sr_word2id[SPECIAL_SYMBOL[i]] = i\r\n    word_embedding = np.vstack([vec_special_sym, word_embedding])\r\n    # \xe4\xbf\x9d\xe5\xad\x98\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\r\n    save_path = \'../data/\'\r\n    if not os.path.exists(save_path):\r\n        os.makedirs(save_path)\r\n    np.save(save_path + \'word_embedding.npy\', word_embedding)\r\n    # \xe4\xbf\x9d\xe5\xad\x98\xe8\xaf\x8d\xe4\xb8\x8eid\xe7\x9a\x84\xe5\xaf\xb9\xe5\xba\x94\xe5\x85\xb3\xe7\xb3\xbb\r\n    with open(save_path + \'sr_word2id.pkl\', \'wb\') as outp:\r\n        pickle.dump(sr_id2word, outp)\r\n        pickle.dump(sr_word2id, outp)\r\n    print(\'Saving the word_embedding.npy to ../data/word_embedding.npy\')\r\n\r\n\r\ndef get_char_embedding():\r\n    """"""\xe6\x8f\x90\xe5\x8f\x96\xe5\xad\x97\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe5\xb9\xb6\xe4\xbf\x9d\xe5\xad\x98\xe8\x87\xb3 ../data/char_embedding.npy""""""\r\n    print(\'getting the char_embedding.npy\')\r\n    wv = word2vec.load(\'../raw_data/char_embedding.txt\')\r\n    char_embedding = wv.vectors\r\n    chars = wv.vocab\r\n    n_special_sym = len(SPECIAL_SYMBOL)\r\n    sr_id2char = pd.Series(chars, index=range(n_special_sym, n_special_sym + len(chars)))\r\n    sr_char2id = pd.Series(range(n_special_sym, n_special_sym + len(chars)), index=chars)\r\n\r\n    # \xe6\xb7\xbb\xe5\x8a\xa0\xe7\x89\xb9\xe6\xae\x8a\xe7\xac\xa6\xe5\x8f\xb7\xef\xbc\x9a<PAD>:0, <UNK>:1\r\n    embedding_size = 256\r\n\r\n    vec_special_sym = np.random.randn(n_special_sym, embedding_size)\r\n    for i in range(n_special_sym):\r\n        sr_id2char[i] = SPECIAL_SYMBOL[i]\r\n        sr_char2id[SPECIAL_SYMBOL[i]] = i\r\n    char_embedding = np.vstack([vec_special_sym, char_embedding])\r\n    # \xe4\xbf\x9d\xe5\xad\x98\xe5\xad\x97\xe5\x90\x91\xe9\x87\x8f\r\n    save_path = \'../data/\'\r\n    if not os.path.exists(save_path):\r\n        os.makedirs(save_path)\r\n    np.save(save_path + \'char_embedding.npy\', char_embedding)\r\n    # \xe4\xbf\x9d\xe5\xad\x98\xe5\xad\x97\xe4\xb8\x8eid\xe7\x9a\x84\xe5\xaf\xb9\xe5\xba\x94\xe5\x85\xb3\xe7\xb3\xbb\r\n    with open(save_path + \'sr_char2id.pkl\', \'wb\') as outp:\r\n        pickle.dump(sr_id2char, outp)\r\n        pickle.dump(sr_char2id, outp)\r\n    print(\'Saving the char_embedding.npy to ../data/char_embedding.npy\')\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    get_word_embedding()\r\n    get_char_embedding()\r\n'"
data_process/question_and_topic_2id.py,0,"b'# -*- coding:utf-8 -*- \r\n\r\nimport pandas as pd\r\nimport pickle\r\nfrom itertools import chain\r\n\r\n\r\ndef question_and_topic_2id():\r\n    """"""\xe6\x8a\x8aquestion\xe5\x92\x8ctopic\xe8\xbd\xac\xe6\x88\x90id\xe5\xbd\xa2\xe5\xbc\x8f\xe5\xb9\xb6\xe4\xbf\x9d\xe5\xad\x98\xe8\x87\xb3 ../data/\xe7\x9b\xae\xe5\xbd\x95\xe4\xb8\x8b\xe3\x80\x82""""""\r\n    print(\'Changing the quetion and topic to id and save in sr_question2.pkl and sr_topic2id.pkl in ../data/\')\r\n    df_question_topic = pd.read_csv(\'../raw_data/question_topic_train_set.txt\', sep=\'\\t\', names=[\'question\', \'topics\'],\r\n                        dtype={\'question\': object, \'topics\': object})\r\n    df_question_topic.topics = df_question_topic.topics.apply(lambda tps: tps.split(\',\'))\r\n    save_path = \'../data/\'\r\n    print(\'questino number = %d \' % len(df_question_topic))\r\n    # \xe9\x97\xae\xe9\xa2\x98 id \xe6\x8c\x89\xe7\x85\xa7\xe7\xbb\x99\xe5\x87\xba\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xe9\xa1\xba\xe5\xba\x8f\xe7\xbc\x96\xe5\x8f\xb7\r\n    questions = df_question_topic.question.values\r\n    sr_question2id = pd.Series(range(len(questions)), index=questions) \r\n    sr_id2question = pd.Series(questions, index=range(len(questions)))\r\n    # topic \xe6\x8c\x89\xe7\x85\xa7\xe6\x95\xb0\xe9\x87\x8f\xe4\xbb\x8e\xe5\xa4\xa7\xe5\x88\xb0\xe5\xb0\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xbc\x96\xe5\x8f\xb7\r\n    topics = df_question_topic.topics.values\r\n    topics = list(chain(*topics))\r\n    sr_topics = pd.Series(topics)\r\n    topics_count = sr_topics.value_counts()\r\n    topics = topics_count.index\r\n    sr_topic2id = pd.Series(range(len(topics)),index=topics)\r\n    sr_id2topic = pd.Series(topics, index=range(len(topics))) \r\n\r\n    with open(save_path + \'sr_question2id.pkl\', \'wb\') as outp:\r\n        pickle.dump(sr_question2id, outp)\r\n        pickle.dump(sr_id2question, outp)\r\n    with open(save_path + \'sr_topic2id.pkl\', \'wb\') as outp:\r\n        pickle.dump(sr_topic2id, outp)\r\n        pickle.dump(sr_id2topic, outp)\r\n    print(\'Finished changing.\')\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    question_and_topic_2id()\r\n'"
data_process/test.py,0,"b'# -*- coding:utf-8 -*-\r\n\r\n\r\nfrom multiprocessing import Pool\r\nimport numpy as np\r\n\r\ndef func(a, b):\r\n    return a+b\r\n\r\np = Pool()\r\na = [1,2,3]\r\nb = [4,5,6]\r\npara = zip(a,b)\r\nresult = p.map(func, para)\r\np.close()\r\np.join()\r\nprint result'"
data_process/word2id.py,0,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport pickle\r\nfrom multiprocessing import Pool\r\nfrom tqdm import tqdm\r\nimport time\r\n\r\nsave_path = \'../data/\'\r\nwith open(save_path + \'sr_word2id.pkl\', \'rb\') as inp:\r\n    sr_id2word = pickle.load(inp)\r\n    sr_word2id = pickle.load(inp)\r\ndict_word2id = dict()\r\nfor i in xrange(len(sr_word2id)):\r\n    dict_word2id[sr_word2id.index[i]] = sr_word2id.values[i]\r\n\r\n\r\ndef get_id(word):\r\n    """"""\xe8\x8e\xb7\xe5\x8f\x96 word \xe6\x89\x80\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84 id.\r\n    \xe5\xa6\x82\xe6\x9e\x9c\xe8\xaf\xa5\xe8\xaf\x8d\xe4\xb8\x8d\xe5\x9c\xa8\xe8\xaf\x8d\xe5\x85\xb8\xe4\xb8\xad\xef\xbc\x8c\xe7\x94\xa8 <UNK>\xef\xbc\x88\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84 ID \xe4\xb8\xba 1 \xef\xbc\x89\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x9b\xbf\xe6\x8d\xa2\xe3\x80\x82\r\n    """"""\r\n    if word not in dict_word2id:\r\n        return 1\r\n    else:\r\n        return dict_word2id[word]\r\n\r\n\r\ndef get_id4words(words):\r\n    """"""\xe6\x8a\x8a words \xe8\xbd\xac\xe4\xb8\xba \xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84 id""""""\r\n    words = words.strip().split(\',\')  # \xe5\x85\x88\xe5\x88\x86\xe5\xbc\x80\xe8\xaf\x8d\r\n    ids = map(get_id, words)  # \xe8\x8e\xb7\xe5\x8f\x96id\r\n    return ids\r\n\r\n\r\ndef test_word2id():\r\n    """"""\xe6\x8a\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe8\xaf\x8d\xe8\xbd\xac\xe6\x88\x90\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84id\xe3\x80\x82""""""\r\n    time0 = time.time()\r\n    print(\'Processing eval data.\')\r\n    df_eval = pd.read_csv(\'../raw_data/question_eval_set.txt\', sep=\'\\t\', usecols=[0, 2, 4],\r\n                          names=[\'question_id\', \'word_title\', \'word_content\'], dtype={\'question_id\': object})\r\n    print(\'test question number %d\' % len(df_eval))\r\n    # \xe6\xb2\xa1\xe6\x9c\x89 title \xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xe7\x94\xa8 content \xe6\x9d\xa5\xe6\x9b\xbf\xe6\x8d\xa2\r\n    na_title_indexs = list()\r\n    for i in xrange(len(df_eval)):\r\n        word_title = df_eval.word_title.values[i]\r\n        if type(word_title) is float:\r\n            na_title_indexs.append(i)\r\n    print(\'There are %d test questions without title.\' % len(na_title_indexs))\r\n    for na_index in na_title_indexs:\r\n        df_eval.at[na_index, \'word_title\'] = df_eval.at[na_index, \'word_content\']\r\n    # \xe6\xb2\xa1\xe6\x9c\x89 content \xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xe7\x94\xa8 title \xe6\x9d\xa5\xe6\x9b\xbf\xe6\x8d\xa2\r\n    na_content_indexs = list()\r\n    for i in tqdm(xrange(len(df_eval))):\r\n        word_content = df_eval.word_content.values[i]\r\n        if type(word_content) is float:\r\n            na_content_indexs.append(i)\r\n    print(\'There are %d test questions without content.\' % len(na_content_indexs))\r\n    for na_index in tqdm(na_content_indexs):\r\n        df_eval.at[na_index, \'word_content\'] = df_eval.at[na_index, \'word_title\']\r\n    # \xe8\xbd\xac\xe4\xb8\xba id \xe5\xbd\xa2\xe5\xbc\x8f\r\n    p = Pool()\r\n    eval_title = np.asarray(p.map(get_id4words, df_eval.word_title.values))\r\n    np.save(\'../data/wd_eval_title.npy\', eval_title)\r\n    eval_content = np.asarray(p.map(get_id4words, df_eval.word_content.values))\r\n    np.save(\'../data/wd_eval_content.npy\', eval_content)\r\n    p.close()\r\n    p.join()\r\n    print(\'Finished changing the eval words to ids. Costed time %g s\' % (time.time() - time0))\r\n\r\n\r\ndef train_word2id():\r\n    """"""\xe6\x8a\x8a\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe8\xaf\x8d\xe8\xbd\xac\xe6\x88\x90\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84id\xe3\x80\x82""""""\r\n    time0 = time.time()\r\n    print(\'Processing train data.\')\r\n    df_train = pd.read_csv(\'../raw_data/question_train_set.txt\', sep=\'\\t\', usecols=[0, 2, 4],\r\n                           names=[\'question_id\', \'word_title\', \'word_content\'], dtype={\'question_id\': object})\r\n    print(\'training question number %d \' % len(df_train))\r\n    # \xe6\xb2\xa1\xe6\x9c\x89 content \xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xe7\x94\xa8 title \xe6\x9d\xa5\xe6\x9b\xbf\xe6\x8d\xa2\r\n    na_content_indexs = list()\r\n    for i in tqdm(xrange(len(df_train))):\r\n        word_content = df_train.word_content.values[i]\r\n        if type(word_content) is float:\r\n            na_content_indexs.append(i)\r\n    print(\'There are %d train questions without content.\' % len(na_content_indexs))\r\n    for na_index in tqdm(na_content_indexs):\r\n        df_train.at[na_index, \'word_content\'] = df_train.at[na_index, \'word_title\']\r\n    # \xe6\xb2\xa1\xe6\x9c\x89 title \xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c \xe4\xb8\xa2\xe5\xbc\x83\r\n    na_title_indexs = list()\r\n    for i in xrange(len(df_train)):\r\n        word_title = df_train.word_title.values[i]\r\n        if type(word_title) is float:\r\n            na_title_indexs.append(i)\r\n    print(\'There are %d train questions without title.\' % len(na_title_indexs))\r\n    df_train = df_train.drop(na_title_indexs)\r\n    print(\'After dropping, training question number(should be 2999952) = %d\' % len(df_train))\r\n    # \xe8\xbd\xac\xe4\xb8\xba id \xe5\xbd\xa2\xe5\xbc\x8f\r\n    p = Pool()\r\n    train_title = np.asarray(p.map(get_id4words, df_train.word_title.values))\r\n    np.save(\'../data/wd_train_title.npy\', train_title)\r\n    train_content = np.asarray(p.map(get_id4words, df_train.word_content.values))\r\n    np.save(\'../data/wd_train_content.npy\', train_content)\r\n    p.close()\r\n    p.join()\r\n    print(\'Finished changing the training words to ids. Costed time %g s\' % (time.time() - time0))\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    test_word2id()\r\n    train_word2id()\r\n'"
models/wd_1_1_cnn_concat/__init__.py,0,b'# -*- coding:utf-8 -*- \r\n\r\n'
models/wd_1_1_cnn_concat/network.py,54,"b'# -*- coding:utf-8 -*-\n\nimport tensorflow as tf\n\n""""""wd_1_1_cnn_concat\ntitle \xe9\x83\xa8\xe5\x88\x86\xe4\xbd\xbf\xe7\x94\xa8 TextCNN\xef\xbc\x9bcontent \xe9\x83\xa8\xe5\x88\x86\xe4\xbd\xbf\xe7\x94\xa8 TextCNN\xef\xbc\x9b \xe4\xb8\xa4\xe9\x83\xa8\xe5\x88\x86\xe8\xbe\x93\xe5\x87\xba\xe7\x9b\xb4\xe6\x8e\xa5 concat\xe3\x80\x82\n""""""\n\n\nclass Settings(object):\n    def __init__(self):\n        self.model_name = \'wd_1_1_cnn_concat\'\n        self.title_len = 30\n        self.content_len = 150\n        self.filter_sizes = [2, 3, 4, 5, 7]\n        self.n_filter = 256\n        self.fc_hidden_size = 1024\n        self.n_class = 1999\n        self.summary_path = \'../../summary/\' + self.model_name + \'/\'\n        self.ckpt_path = \'../../ckpt/\' + self.model_name + \'/\'\n\n\nclass TextCNN(object):\n    """"""\n    title: inputs->textcnn->output_title\n    content: inputs->textcnn->output_content\n    concat[output_title, output_content] -> fc+bn+relu -> sigmoid_entropy.\n    """"""\n\n    def __init__(self, W_embedding, settings):\n        self.model_name = settings.model_name\n        self.title_len = settings.title_len\n        self.content_len = settings.content_len\n        self.filter_sizes = settings.filter_sizes\n        self.n_filter = settings.n_filter\n        self.n_filter_total = self.n_filter * len(self.filter_sizes)\n        self.n_class = settings.n_class\n        self.fc_hidden_size = settings.fc_hidden_size\n        self._global_step = tf.Variable(0, trainable=False, name=\'Global_Step\')\n        self.update_emas = list()\n        # placeholders\n        self._tst = tf.placeholder(tf.bool)\n        self._keep_prob = tf.placeholder(tf.float32, [])\n        self._batch_size = tf.placeholder(tf.int32, [])\n\n        with tf.name_scope(\'Inputs\'):\n            self._X1_inputs = tf.placeholder(tf.int64, [None, self.title_len], name=\'X1_inputs\')\n            self._X2_inputs = tf.placeholder(tf.int64, [None, self.content_len], name=\'X2_inputs\')\n            self._y_inputs = tf.placeholder(tf.float32, [None, self.n_class], name=\'y_input\')\n\n        with tf.variable_scope(\'embedding\'):\n            self.embedding = tf.get_variable(name=\'embedding\', shape=W_embedding.shape,\n                                             initializer=tf.constant_initializer(W_embedding), trainable=True)\n        self.embedding_size = W_embedding.shape[1]\n\n        with tf.variable_scope(\'cnn_text\'):\n            output_title = self.cnn_inference(self._X1_inputs, self.title_len)\n\n        with tf.variable_scope(\'hcnn_content\'):\n            output_content = self.cnn_inference(self._X2_inputs, self.content_len)\n\n        with tf.variable_scope(\'fc-bn-layer\'):\n            output = tf.concat([output_title, output_content], axis=1)\n            W_fc = self.weight_variable([self.n_filter_total * 2, self.fc_hidden_size], name=\'Weight_fc\')\n            tf.summary.histogram(\'W_fc\', W_fc)\n            h_fc = tf.matmul(output, W_fc, name=\'h_fc\')\n            beta_fc = tf.Variable(tf.constant(0.1, tf.float32, shape=[self.fc_hidden_size], name=""beta_fc""))\n            tf.summary.histogram(\'beta_fc\', beta_fc)\n            fc_bn, update_ema_fc = self.batchnorm(h_fc, beta_fc, convolutional=False)\n            self.update_emas.append(update_ema_fc)\n            self.fc_bn_relu = tf.nn.relu(fc_bn, name=""relu"")\n            fc_bn_drop = tf.nn.dropout(self.fc_bn_relu, self.keep_prob)\n\n        with tf.variable_scope(\'out_layer\'):\n            W_out = self.weight_variable([self.fc_hidden_size, self.n_class], name=\'Weight_out\')\n            tf.summary.histogram(\'Weight_out\', W_out)\n            b_out = self.bias_variable([self.n_class], name=\'bias_out\')\n            tf.summary.histogram(\'bias_out\', b_out)\n            self._y_pred = tf.nn.xw_plus_b(fc_bn_drop, W_out, b_out, name=\'y_pred\')  # \xe6\xaf\x8f\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe5\x88\x86\xe6\x95\xb0 scores\n\n        with tf.name_scope(\'loss\'):\n            self._loss = tf.reduce_mean(\n                tf.nn.sigmoid_cross_entropy_with_logits(logits=self._y_pred, labels=self._y_inputs))\n            tf.summary.scalar(\'loss\', self._loss)\n\n        self.saver = tf.train.Saver(max_to_keep=2)\n\n    @property\n    def tst(self):\n        return self._tst\n\n    @property\n    def keep_prob(self):\n        return self._keep_prob\n\n    @property\n    def batch_size(self):\n        return self._batch_size\n\n    @property\n    def global_step(self):\n        return self._global_step\n\n    @property\n    def X1_inputs(self):\n        return self._X1_inputs\n\n    @property\n    def X2_inputs(self):\n        return self._X2_inputs\n\n    @property\n    def y_inputs(self):\n        return self._y_inputs\n\n    @property\n    def y_pred(self):\n        return self._y_pred\n\n    @property\n    def loss(self):\n        return self._loss\n\n    def weight_variable(self, shape, name):\n        """"""Create a weight variable with appropriate initialization.""""""\n        initial = tf.truncated_normal(shape, stddev=0.1)\n        return tf.Variable(initial, name=name)\n\n    def bias_variable(self, shape, name):\n        """"""Create a bias variable with appropriate initialization.""""""\n        initial = tf.constant(0.1, shape=shape)\n        return tf.Variable(initial, name=name)\n\n    def batchnorm(self, Ylogits, offset, convolutional=False):\n        """"""batchnormalization.\n        Args:\n            Ylogits: 1D\xe5\x90\x91\xe9\x87\x8f\xe6\x88\x96\xe8\x80\x85\xe6\x98\xaf3D\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\n            num_updates: \xe8\xbf\xad\xe4\xbb\xa3\xe7\x9a\x84global_step\n            offset\xef\xbc\x9a\xe8\xa1\xa8\xe7\xa4\xbabeta\xef\xbc\x8c\xe5\x85\xa8\xe5\xb1\x80\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x9b\xe5\x9c\xa8 RELU \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe4\xb8\x80\xe8\x88\xac\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xba 0.1\xe3\x80\x82\n            scale\xef\xbc\x9a\xe8\xa1\xa8\xe7\xa4\xbalambda\xef\xbc\x8c\xe5\x85\xa8\xe5\xb1\x80\xe6\x96\xb9\xe5\xb7\xae\xef\xbc\x9b\xe5\x9c\xa8 sigmoid \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe9\x9c\x80\xe8\xa6\x81\xef\xbc\x8c\xe8\xbf\x99 RELU \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe4\xbd\x9c\xe7\x94\xa8\xe4\xb8\x8d\xe5\xa4\xa7\xe3\x80\x82\n            m: \xe8\xa1\xa8\xe7\xa4\xbabatch\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x9bv:\xe8\xa1\xa8\xe7\xa4\xbabatch\xe6\x96\xb9\xe5\xb7\xae\xe3\x80\x82\n            bnepsilon\xef\xbc\x9a\xe4\xb8\x80\xe4\xb8\xaa\xe5\xbe\x88\xe5\xb0\x8f\xe7\x9a\x84\xe6\xb5\xae\xe7\x82\xb9\xe6\x95\xb0\xef\xbc\x8c\xe9\x98\xb2\xe6\xad\xa2\xe9\x99\xa4\xe4\xbb\xa5 0.\n        Returns:\n            Ybn: \xe5\x92\x8c Ylogits \xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf\xe7\xbb\x8f\xe8\xbf\x87 Batch Normalization \xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\n            update_moving_everages\xef\xbc\x9a\xe6\x9b\xb4\xe6\x96\xb0mean\xe5\x92\x8cvariance\xef\xbc\x8c\xe4\xb8\xbb\xe8\xa6\x81\xe6\x98\xaf\xe7\xbb\x99\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84 test \xe4\xbd\xbf\xe7\x94\xa8\xe3\x80\x82\n        """"""\n        exp_moving_avg = tf.train.ExponentialMovingAverage(0.999,\n                                                           self._global_step)  # adding the iteration prevents from averaging across non-existing iterations\n        bnepsilon = 1e-5\n        if convolutional:\n            mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])\n        else:\n            mean, variance = tf.nn.moments(Ylogits, [0])\n        update_moving_everages = exp_moving_avg.apply([mean, variance])\n        m = tf.cond(self.tst, lambda: exp_moving_avg.average(mean), lambda: mean)\n        v = tf.cond(self.tst, lambda: exp_moving_avg.average(variance), lambda: variance)\n        Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n        return Ybn, update_moving_everages\n\n    def cnn_inference(self, X_inputs, n_step):\n        """"""TextCNN \xe6\xa8\xa1\xe5\x9e\x8b\xe3\x80\x82\n        Args:\n            X_inputs: tensor.shape=(batch_size, n_step)\n        Returns:\n            title_outputs: tensor.shape=(batch_size, self.n_filter_total)\n        """"""\n        inputs = tf.nn.embedding_lookup(self.embedding, X_inputs)\n        inputs = tf.expand_dims(inputs, -1)\n        pooled_outputs = list()\n        for i, filter_size in enumerate(self.filter_sizes):\n            with tf.variable_scope(""conv-maxpool-%s"" % filter_size):\n                # Convolution Layer\n                filter_shape = [filter_size, self.embedding_size, 1, self.n_filter]\n                W_filter = self.weight_variable(shape=filter_shape, name=\'W_filter\')\n                beta = self.bias_variable(shape=[self.n_filter], name=\'beta_filter\')\n                tf.summary.histogram(\'beta\', beta)\n                conv = tf.nn.conv2d(inputs, W_filter, strides=[1, 1, 1, 1], padding=""VALID"", name=""conv"")\n                conv_bn, update_ema = self.batchnorm(conv, beta, convolutional=True)  # \xe5\x9c\xa8\xe6\xbf\x80\xe6\xb4\xbb\xe5\xb1\x82\xe5\x89\x8d\xe9\x9d\xa2\xe5\x8a\xa0 BN\n                # Apply nonlinearity, batch norm scaling is not useful with relus\n                # batch norm offsets are used instead of biases,\xe4\xbd\xbf\xe7\x94\xa8 BN \xe5\xb1\x82\xe7\x9a\x84 offset\xef\xbc\x8c\xe4\xb8\x8d\xe8\xa6\x81 biases\n                h = tf.nn.relu(conv_bn, name=""relu"")\n                # Maxpooling over the outputs\n                pooled = tf.nn.max_pool(h, ksize=[1, n_step - filter_size + 1, 1, 1],\n                                        strides=[1, 1, 1, 1], padding=\'VALID\', name=""pool"")\n                pooled_outputs.append(pooled)\n                self.update_emas.append(update_ema)\n        h_pool = tf.concat(pooled_outputs, 3)\n        h_pool_flat = tf.reshape(h_pool, [-1, self.n_filter_total])\n        return h_pool_flat  # shape = [batch_size, self.n_filter_total]\n\n\n# test the model\n# def test():\n#     import numpy as np\n#     print(\'Begin testing...\')\n#     settings = Settings()\n#     W_embedding = np.random.randn(50, 10)\n#     config = tf.ConfigProto()\n#     config.gpu_options.allow_growth = True\n#     batch_size = 128\n#     with tf.Session(config=config) as sess:\n#         model = TextCNN(W_embedding, settings)\n#         optimizer = tf.train.AdamOptimizer(0.001)\n#         train_op = optimizer.minimize(model.loss)\n#         update_op = tf.group(*model.update_emas)\n#         sess.run(tf.global_variables_initializer())\n#         fetch = [model.loss, model.y_pred, train_op, update_op]\n#         loss_list = list()\n#         for i in xrange(100):\n#             X1_batch = np.zeros((batch_size, 30), dtype=float)\n#             X2_batch = np.zeros((batch_size, 150), dtype=float)\n#             y_batch = np.zeros((batch_size, 1999), dtype=int)\n#             _batch_size = len(y_batch)\n#             feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\n#                          model.batch_size: _batch_size, model.tst: False, model.keep_prob: 0.5}\n#             loss, y_pred, _, _ = sess.run(fetch, feed_dict=feed_dict)\n#             loss_list.append(loss)\n#             print(i, loss)\n#\n# if __name__ == \'__main__\':\n#     test()\n'"
models/wd_1_1_cnn_concat/predict.py,4,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport os\r\nimport sys\r\nimport time\r\nimport network\r\n\r\nsys.path.append(\'../..\')\r\nfrom evaluator import score_eval\r\n\r\nsettings = network.Settings()\r\ntitle_len = settings.title_len\r\nmodel_name = settings.model_name\r\nckpt_path = settings.ckpt_path\r\n\r\nlocal_scores_path = \'../../local_scores/\'\r\nscores_path = \'../../scores/\'\r\nif not os.path.exists(local_scores_path):\r\n    os.makedirs(local_scores_path)\r\nif not os.path.exists(scores_path):\r\n    os.makedirs(scores_path)\r\n\r\nembedding_path = \'../../data/word_embedding.npy\'\r\ndata_valid_path = \'../../data/wd-data/data_valid/\'\r\ndata_test_path = \'../../data/wd-data/data_test/\'\r\nva_batches = os.listdir(data_valid_path)\r\nte_batches = os.listdir(data_test_path)  # batch \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8\r\nn_va_batches = len(va_batches)\r\nn_te_batches = len(te_batches)\r\n\r\n\r\ndef get_batch(batch_id):\r\n    """"""get a batch from valid data""""""\r\n    new_batch = np.load(data_valid_path + str(batch_id) + \'.npz\')\r\n    X_batch = new_batch[\'X\']\r\n    y_batch = new_batch[\'y\']\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch, y_batch]\r\n\r\n\r\ndef get_test_batch(batch_id):\r\n    """"""get a batch from test data""""""\r\n    X_batch = np.load(data_test_path + str(batch_id) + \'.npy\')\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch]\r\n\r\n\r\ndef local_predict(sess, model):\r\n    """"""Test on the valid data.""""""\r\n    time0 = time.time()\r\n    predict_labels_list = list()  # \xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\r\n    marked_labels_list = list()\r\n    predict_scores = list()\r\n    for i in tqdm(xrange(n_va_batches)):\r\n        [X1_batch, X2_batch, y_batch] = get_batch(i)\r\n        marked_labels_list.extend(y_batch)\r\n        _batch_size = len(X1_batch)\r\n        fetches = [model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        predict_labels = sess.run(fetches, feed_dict)[0]\r\n        predict_scores.append(predict_labels)\r\n        predict_labels = map(lambda label: label.argsort()[-1:-6:-1], predict_labels)  # \xe5\x8f\x96\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x845\xe4\xb8\xaa\xe4\xb8\x8b\xe6\xa0\x87\r\n        predict_labels_list.extend(predict_labels)\r\n    predict_label_and_marked_label_list = zip(predict_labels_list, marked_labels_list)\r\n    precision, recall, f1 = score_eval(predict_label_and_marked_label_list)\r\n    print(\'Local valid p=%g, r=%g, f1=%g\' % (precision, recall, f1))\r\n    predict_scores = np.vstack(np.asarray(predict_scores))\r\n    local_scores_name = local_scores_path + model_name + \'.npy\'\r\n    np.save(local_scores_name, predict_scores)\r\n    print(\'local_scores.shape=\', predict_scores.shape)\r\n    print(\'Writed the scores into %s, time %g s\' % (local_scores_name, time.time() - time0))\r\n\r\n\r\ndef predict(sess, model):\r\n    """"""Test on the test data.""""""\r\n    time0 = time.time()\r\n    predict_scores = list()\r\n    for i in tqdm(xrange(n_te_batches)):\r\n        [X1_batch, X2_batch] = get_test_batch(i)\r\n        _batch_size = len(X1_batch)\r\n        fetches = [model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        predict_labels = sess.run(fetches, feed_dict)[0]\r\n        predict_scores.append(predict_labels)\r\n    predict_scores = np.vstack(np.asarray(predict_scores))\r\n    scores_name = scores_path + model_name + \'.npy\'\r\n    np.save(scores_name, predict_scores)\r\n    print(\'scores.shape=\', predict_scores.shape)\r\n    print(\'Writed the scores into %s, time %g s\' % (scores_name, time.time() - time0))\r\n\r\n\r\ndef main(_):\r\n    if not os.path.exists(ckpt_path + \'checkpoint\'):\r\n        print(\'there is not saved model, please check the ckpt path\')\r\n        exit()\r\n    print(\'Loading model...\')\r\n    W_embedding = np.load(embedding_path)\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    with tf.Session(config=config) as sess:\r\n        model = network.TextCNN(W_embedding, settings)\r\n        model.saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\r\n        print(\'Local predicting...\')\r\n        local_predict(sess, model)\r\n        print(\'Test predicting...\')\r\n        predict(sess, model)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    tf.app.run()\r\n'"
models/wd_1_1_cnn_concat/train.py,21,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport os\r\nimport sys\r\nimport shutil\r\nimport time\r\nimport network\r\n\r\nsys.path.append(\'../..\')\r\nfrom data_helpers import to_categorical\r\nfrom evaluator import score_eval\r\n\r\nflags = tf.flags\r\nflags.DEFINE_bool(\'is_retrain\', False, \'if is_retrain is true, not rebuild the summary\')\r\nflags.DEFINE_integer(\'max_epoch\', 1, \'update the embedding after max_epoch, default: 1\')\r\nflags.DEFINE_integer(\'max_max_epoch\', 6, \'all training epoches, default: 6\')\r\nflags.DEFINE_float(\'lr\', 1e-3, \'initial learning rate, default: 1e-3\')\r\nflags.DEFINE_float(\'decay_rate\', 0.65, \'decay rate, default: 0.65\')\r\nflags.DEFINE_float(\'keep_prob\', 0.5, \'keep_prob for training, default: 0.5\')\r\n# \xe6\xad\xa3\xe5\xbc\x8f\r\nflags.DEFINE_integer(\'decay_step\', 15000, \'decay_step, default: 15000\')\r\nflags.DEFINE_integer(\'valid_step\', 10000, \'valid_step, default: 10000\')\r\nflags.DEFINE_float(\'last_f1\', 0.40, \'if valid_f1 > last_f1, save new model. default: 0.40\')\r\n\r\n# \xe6\xb5\x8b\xe8\xaf\x95\r\n# flags.DEFINE_integer(\'decay_step\', 1000, \'decay_step, default: 1000\')\r\n# flags.DEFINE_integer(\'valid_step\', 500, \'valid_step, default: 500\')\r\n# flags.DEFINE_float(\'last_f1\', 0.10, \'if valid_f1 > last_f1, save new model. default: 0.10\')\r\nFLAGS = flags.FLAGS\r\n\r\nlr = FLAGS.lr\r\nlast_f1 = FLAGS.last_f1\r\nsettings = network.Settings()\r\ntitle_len = settings.title_len\r\nsummary_path = settings.summary_path\r\nckpt_path = settings.ckpt_path\r\nmodel_path = ckpt_path + \'model.ckpt\'\r\n\r\nembedding_path = \'../../data/word_embedding.npy\'\r\ndata_train_path = \'../../data/wd-data/data_train/\'\r\ndata_valid_path = \'../../data/wd-data/data_valid/\'\r\ntr_batches = os.listdir(data_train_path)  # batch \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8\r\nva_batches = os.listdir(data_valid_path)\r\nn_tr_batches = len(tr_batches)\r\nn_va_batches = len(va_batches)\r\n\r\n# \xe6\xb5\x8b\xe8\xaf\x95\r\n# n_tr_batches = 1000\r\n# n_va_batches = 50\r\n\r\n\r\ndef get_batch(data_path, batch_id):\r\n    """"""get a batch from data_path""""""\r\n    new_batch = np.load(data_path + str(batch_id) + \'.npz\')\r\n    X_batch = new_batch[\'X\']\r\n    y_batch = new_batch[\'y\']\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch, y_batch]\r\n\r\n\r\ndef valid_epoch(data_path, sess, model):\r\n    """"""Test on the valid data.""""""\r\n    va_batches = os.listdir(data_path)\r\n    n_va_batches = len(va_batches)\r\n    _costs = 0.0\r\n    predict_labels_list = list()  # \xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\r\n    marked_labels_list = list()\r\n    for i in xrange(n_va_batches):\r\n        [X1_batch, X2_batch, y_batch] = get_batch(data_path, i)\r\n        marked_labels_list.extend(y_batch)\r\n        y_batch = to_categorical(y_batch)\r\n        _batch_size = len(y_batch)\r\n        fetches = [model.loss, model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        _cost, predict_labels = sess.run(fetches, feed_dict)\r\n        _costs += _cost\r\n        predict_labels = map(lambda label: label.argsort()[-1:-6:-1], predict_labels)  # \xe5\x8f\x96\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x845\xe4\xb8\xaa\xe4\xb8\x8b\xe6\xa0\x87\r\n        predict_labels_list.extend(predict_labels)\r\n    predict_label_and_marked_label_list = zip(predict_labels_list, marked_labels_list)\r\n    precision, recall, f1 = score_eval(predict_label_and_marked_label_list)\r\n    mean_cost = _costs / n_va_batches\r\n    return mean_cost, precision, recall, f1\r\n\r\n\r\ndef train_epoch(data_path, sess, model, train_fetches, valid_fetches, train_writer, test_writer):\r\n    global last_f1\r\n    global lr\r\n    time0 = time.time()\r\n    batch_indexs = np.random.permutation(n_tr_batches)  # shuffle the training data\r\n    for batch in tqdm(xrange(n_tr_batches)):\r\n        global_step = sess.run(model.global_step)\r\n        if 0 == (global_step + 1) % FLAGS.valid_step:\r\n            valid_cost, precision, recall, f1 = valid_epoch(data_valid_path, sess, model)\r\n            print(\'Global_step=%d: valid cost=%g; p=%g, r=%g, f1=%g, time=%g s\' % (\r\n                global_step, valid_cost, precision, recall, f1, time.time() - time0))\r\n            time0 = time.time()\r\n            if f1 > last_f1:\r\n                last_f1 = f1\r\n                saving_path = model.saver.save(sess, model_path, global_step+1)\r\n                print(\'saved new model to %s \' % saving_path)\r\n        # training\r\n        batch_id = batch_indexs[batch]\r\n        [X1_batch, X2_batch, y_batch] = get_batch(data_train_path, batch_id)\r\n        y_batch = to_categorical(y_batch)\r\n        _batch_size = len(y_batch)\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                     model.batch_size: _batch_size, model.tst: False, model.keep_prob: FLAGS.keep_prob}\r\n        summary, _cost, _, _ = sess.run(train_fetches, feed_dict)  # the cost is the mean cost of one batch\r\n        # valid per 500 steps\r\n        if 0 == (global_step + 1) % 500:\r\n            train_writer.add_summary(summary, global_step)\r\n            batch_id = np.random.randint(0, n_va_batches)  # \xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe4\xb8\x80\xe4\xb8\xaa\xe9\xaa\x8c\xe8\xaf\x81batch\r\n            [X1_batch, X2_batch, y_batch] = get_batch(data_valid_path, batch_id)\r\n            y_batch = to_categorical(y_batch)\r\n            _batch_size = len(y_batch)\r\n            feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                         model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n            summary, _cost = sess.run(valid_fetches, feed_dict)\r\n            test_writer.add_summary(summary, global_step)\r\n\r\n\r\ndef main(_):\r\n    global ckpt_path\r\n    global last_f1\r\n    if not os.path.exists(ckpt_path):\r\n        os.makedirs(ckpt_path)\r\n    if not os.path.exists(summary_path):\r\n        os.makedirs(summary_path)\r\n    elif not FLAGS.is_retrain:  # \xe9\x87\x8d\xe6\x96\xb0\xe8\xae\xad\xe7\xbb\x83\xe6\x9c\xac\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\x88\xa0\xe9\x99\xa4\xe4\xbb\xa5\xe5\x89\x8d\xe7\x9a\x84 summary\r\n        shutil.rmtree(summary_path)\r\n        os.makedirs(summary_path)\r\n    if not os.path.exists(summary_path):\r\n        os.makedirs(summary_path)\r\n\r\n    print(\'1.Loading data...\')\r\n    W_embedding = np.load(embedding_path)\r\n    print(\'training sample_num = %d\' % n_tr_batches)\r\n    print(\'valid sample_num = %d\' % n_va_batches)\r\n\r\n    # Initial or restore the model\r\n    print(\'2.Building model...\')\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    with tf.Session(config=config) as sess:\r\n        model = network.TextCNN(W_embedding, settings)\r\n        with tf.variable_scope(\'training_ops\') as vs:\r\n            learning_rate = tf.train.exponential_decay(FLAGS.lr, model.global_step, FLAGS.decay_step,\r\n                                                   FLAGS.decay_rate, staircase=True)\r\n            # two optimizer: op1, update embedding; op2, do not update embedding.\r\n            with tf.variable_scope(\'Optimizer1\'):\r\n                tvars1 = tf.trainable_variables()\r\n                grads1 = tf.gradients(model.loss, tvars1)\r\n                optimizer1 = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n                train_op1 = optimizer1.apply_gradients(zip(grads1, tvars1),\r\n                                                   global_step=model.global_step)\r\n            with tf.variable_scope(\'Optimizer2\'):\r\n                tvars2 = [tvar for tvar in tvars1 if \'embedding\' not in tvar.name]\r\n                grads2 = tf.gradients(model.loss, tvars2)\r\n                optimizer2 = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n                train_op2 = optimizer2.apply_gradients(zip(grads2, tvars2),\r\n                                                   global_step=model.global_step)\r\n            update_op = tf.group(*model.update_emas)\r\n            merged = tf.summary.merge_all()  # summary\r\n            train_writer = tf.summary.FileWriter(summary_path + \'train\', sess.graph)\r\n            test_writer = tf.summary.FileWriter(summary_path + \'test\')\r\n            training_ops = [v for v in tf.global_variables() if v.name.startswith(vs.name+\'/\')]\r\n\r\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe5\xb7\xb2\xe7\xbb\x8f\xe4\xbf\x9d\xe5\xad\x98\xe8\xbf\x87\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\xaf\xbc\xe5\x85\xa5\xe4\xb8\x8a\xe6\xac\xa1\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\r\n        if os.path.exists(ckpt_path + ""checkpoint""):\r\n            print(""Restoring Variables from Checkpoint..."")\r\n            model.saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\r\n            last_valid_cost, precision, recall, last_f1 = valid_epoch(data_valid_path, sess, model)\r\n            print(\' valid cost=%g; p=%g, r=%g, f1=%g\' % (last_valid_cost, precision, recall, last_f1))\r\n            sess.run(tf.variables_initializer(training_ops))\r\n            train_op2 = train_op1\r\n        else:\r\n            print(\'Initializing Variables...\')\r\n            sess.run(tf.global_variables_initializer())\r\n\r\n        print(\'3.Begin training...\')\r\n        print(\'max_epoch=%d, max_max_epoch=%d\' % (FLAGS.max_epoch, FLAGS.max_max_epoch))\r\n        train_op = train_op2\r\n        for epoch in xrange(FLAGS.max_max_epoch):\r\n            global_step = sess.run(model.global_step)\r\n            print(\'Global step %d, lr=%g\' % (global_step, sess.run(learning_rate)))\r\n            if epoch == FLAGS.max_epoch:  # update the embedding\r\n                train_op = train_op1\r\n\r\n            train_fetches = [merged, model.loss, train_op, update_op]\r\n            valid_fetches = [merged, model.loss]\r\n            train_epoch(data_train_path, sess, model, train_fetches, valid_fetches, train_writer, test_writer)\r\n        # \xe6\x9c\x80\xe5\x90\x8e\xe5\x86\x8d\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\r\n        valid_cost, precision, recall, f1 = valid_epoch(data_valid_path, sess, model)\r\n        print(\'END.Global_step=%d: valid cost=%g; p=%g, r=%g, f1=%g\' % (\r\n            sess.run(model.global_step), valid_cost, precision, recall, f1))\r\n        if f1 > last_f1:  # save the better model\r\n            saving_path = model.saver.save(sess, model_path, sess.run(model.global_step)+1)\r\n            print(\'saved new model to %s \' % saving_path)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    tf.app.run()\r\n'"
models/wd_1_2_cnn_max/__init__.py,0,b'# -*- coding:utf-8 -*- \r\n\r\n'
models/wd_1_2_cnn_max/network.py,57,"b'# -*- coding:utf-8 -*-\n\nimport tensorflow as tf\n\n""""""wd_1_2_cnn_max\ntitle \xe9\x83\xa8\xe5\x88\x86\xe4\xbd\xbf\xe7\x94\xa8 TextCNN\xef\xbc\x9bcontent \xe9\x83\xa8\xe5\x88\x86\xe4\xbd\xbf\xe7\x94\xa8 TextCNN\xef\xbc\x9b \xe4\xb8\xa4\xe9\x83\xa8\xe5\x88\x86\xe8\xbe\x93\xe5\x87\xba\xe6\x8c\x89\xe4\xbd\x8d\xe5\x8f\x96 max\xe3\x80\x82\n""""""\n\n\nclass Settings(object):\n    def __init__(self):\n        self.model_name = \'wd_1_2_cnn_max\'\n        self.title_len = 30\n        self.content_len = 150\n        self.filter_sizes = [2, 3, 4, 5, 7]\n        self.n_filter = 256\n        self.fc_hidden_size = 1024\n        self.n_class = 1999\n        self.summary_path = \'../../summary/\' + self.model_name + \'/\'\n        self.ckpt_path = \'../../ckpt/\' + self.model_name + \'/\'\n\n\nclass TextCNN(object):\n    """"""\n    title: inputs->textcnn->output_title\n    content: inputs->textcnn->output_content\n    max[output_title, output_content] -> fc+bn+relu -> sigmoid_entropy.\n    """"""\n\n    def __init__(self, W_embedding, settings):\n        self.model_name = settings.model_name\n        self.title_len = settings.title_len\n        self.content_len = settings.content_len\n        self.filter_sizes = settings.filter_sizes\n        self.n_filter = settings.n_filter\n        self.n_filter_total = self.n_filter * len(self.filter_sizes)\n        self.n_class = settings.n_class\n        self.fc_hidden_size = settings.fc_hidden_size\n        self._global_step = tf.Variable(0, trainable=False, name=\'Global_Step\')\n        self.update_emas = list()\n        # placeholders\n        self._tst = tf.placeholder(tf.bool)\n        self._keep_prob = tf.placeholder(tf.float32, [])\n        self._batch_size = tf.placeholder(tf.int32, [])\n\n        with tf.name_scope(\'Inputs\'):\n            self._X1_inputs = tf.placeholder(tf.int64, [None, self.title_len], name=\'X1_inputs\')\n            self._X2_inputs = tf.placeholder(tf.int64, [None, self.content_len], name=\'X2_inputs\')\n            self._y_inputs = tf.placeholder(tf.float32, [None, self.n_class], name=\'y_input\')\n\n        with tf.variable_scope(\'embedding\'):\n            self.embedding = tf.get_variable(name=\'embedding\', shape=W_embedding.shape,\n                                             initializer=tf.constant_initializer(W_embedding), trainable=True)\n        self.embedding_size = W_embedding.shape[1]\n\n        with tf.variable_scope(\'cnn_text\'):\n            output_title = self.cnn_inference(self._X1_inputs, self.title_len)\n            output_title = tf.expand_dims(output_title, 0)\n\n        with tf.variable_scope(\'hcnn_content\'):\n            output_content = self.cnn_inference(self._X2_inputs, self.content_len)\n            output_content = tf.expand_dims(output_content, 0)\n\n        with tf.variable_scope(\'fc-bn-layer\'):\n            output = tf.concat([output_title, output_content], axis=0)\n            output = tf.reduce_max(output, axis=0)\n            W_fc = self.weight_variable([self.n_filter_total, self.fc_hidden_size], name=\'Weight_fc\')\n            tf.summary.histogram(\'W_fc\', W_fc)\n            h_fc = tf.matmul(output, W_fc, name=\'h_fc\')\n            beta_fc = tf.Variable(tf.constant(0.1, tf.float32, shape=[self.fc_hidden_size], name=""beta_fc""))\n            tf.summary.histogram(\'beta_fc\', beta_fc)\n            fc_bn, update_ema_fc = self.batchnorm(h_fc, beta_fc, convolutional=False)\n            self.update_emas.append(update_ema_fc)\n            self.fc_bn_relu = tf.nn.relu(fc_bn, name=""relu"")\n            fc_bn_drop = tf.nn.dropout(self.fc_bn_relu, self.keep_prob)\n\n        with tf.variable_scope(\'out_layer\'):\n            W_out = self.weight_variable([self.fc_hidden_size, self.n_class], name=\'Weight_out\')\n            tf.summary.histogram(\'Weight_out\', W_out)\n            b_out = self.bias_variable([self.n_class], name=\'bias_out\')\n            tf.summary.histogram(\'bias_out\', b_out)\n            self._y_pred = tf.nn.xw_plus_b(fc_bn_drop, W_out, b_out, name=\'y_pred\')  # \xe6\xaf\x8f\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe5\x88\x86\xe6\x95\xb0 scores\n\n        with tf.name_scope(\'loss\'):\n            self._loss = tf.reduce_mean(\n                tf.nn.sigmoid_cross_entropy_with_logits(logits=self._y_pred, labels=self._y_inputs))\n            tf.summary.scalar(\'loss\', self._loss)\n\n        self.saver = tf.train.Saver(max_to_keep=2)\n\n    @property\n    def tst(self):\n        return self._tst\n\n    @property\n    def keep_prob(self):\n        return self._keep_prob\n\n    @property\n    def batch_size(self):\n        return self._batch_size\n\n    @property\n    def global_step(self):\n        return self._global_step\n\n    @property\n    def X1_inputs(self):\n        return self._X1_inputs\n\n    @property\n    def X2_inputs(self):\n        return self._X2_inputs\n\n    @property\n    def y_inputs(self):\n        return self._y_inputs\n\n    @property\n    def y_pred(self):\n        return self._y_pred\n\n    @property\n    def loss(self):\n        return self._loss\n\n    def weight_variable(self, shape, name):\n        """"""Create a weight variable with appropriate initialization.""""""\n        initial = tf.truncated_normal(shape, stddev=0.1)\n        return tf.Variable(initial, name=name)\n\n    def bias_variable(self, shape, name):\n        """"""Create a bias variable with appropriate initialization.""""""\n        initial = tf.constant(0.1, shape=shape)\n        return tf.Variable(initial, name=name)\n\n    def batchnorm(self, Ylogits, offset, convolutional=False):\n        """"""batchnormalization.\n        Args:\n            Ylogits: 1D\xe5\x90\x91\xe9\x87\x8f\xe6\x88\x96\xe8\x80\x85\xe6\x98\xaf3D\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\n            num_updates: \xe8\xbf\xad\xe4\xbb\xa3\xe7\x9a\x84global_step\n            offset\xef\xbc\x9a\xe8\xa1\xa8\xe7\xa4\xbabeta\xef\xbc\x8c\xe5\x85\xa8\xe5\xb1\x80\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x9b\xe5\x9c\xa8 RELU \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe4\xb8\x80\xe8\x88\xac\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xba 0.1\xe3\x80\x82\n            scale\xef\xbc\x9a\xe8\xa1\xa8\xe7\xa4\xbalambda\xef\xbc\x8c\xe5\x85\xa8\xe5\xb1\x80\xe6\x96\xb9\xe5\xb7\xae\xef\xbc\x9b\xe5\x9c\xa8 sigmoid \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe9\x9c\x80\xe8\xa6\x81\xef\xbc\x8c\xe8\xbf\x99 RELU \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe4\xbd\x9c\xe7\x94\xa8\xe4\xb8\x8d\xe5\xa4\xa7\xe3\x80\x82\n            m: \xe8\xa1\xa8\xe7\xa4\xbabatch\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x9bv:\xe8\xa1\xa8\xe7\xa4\xbabatch\xe6\x96\xb9\xe5\xb7\xae\xe3\x80\x82\n            bnepsilon\xef\xbc\x9a\xe4\xb8\x80\xe4\xb8\xaa\xe5\xbe\x88\xe5\xb0\x8f\xe7\x9a\x84\xe6\xb5\xae\xe7\x82\xb9\xe6\x95\xb0\xef\xbc\x8c\xe9\x98\xb2\xe6\xad\xa2\xe9\x99\xa4\xe4\xbb\xa5 0.\n        Returns:\n            Ybn: \xe5\x92\x8c Ylogits \xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf\xe7\xbb\x8f\xe8\xbf\x87 Batch Normalization \xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\n            update_moving_everages\xef\xbc\x9a\xe6\x9b\xb4\xe6\x96\xb0mean\xe5\x92\x8cvariance\xef\xbc\x8c\xe4\xb8\xbb\xe8\xa6\x81\xe6\x98\xaf\xe7\xbb\x99\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84 test \xe4\xbd\xbf\xe7\x94\xa8\xe3\x80\x82\n        """"""\n        exp_moving_avg = tf.train.ExponentialMovingAverage(0.999,\n                                                           self._global_step)  # adding the iteration prevents from averaging across non-existing iterations\n        bnepsilon = 1e-5\n        if convolutional:\n            mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])\n        else:\n            mean, variance = tf.nn.moments(Ylogits, [0])\n        update_moving_everages = exp_moving_avg.apply([mean, variance])\n        m = tf.cond(self.tst, lambda: exp_moving_avg.average(mean), lambda: mean)\n        v = tf.cond(self.tst, lambda: exp_moving_avg.average(variance), lambda: variance)\n        Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n        return Ybn, update_moving_everages\n\n    def cnn_inference(self, X_inputs, n_step):\n        """"""TextCNN \xe6\xa8\xa1\xe5\x9e\x8b\xe3\x80\x82\n        Args:\n            X_inputs: tensor.shape=(batch_size, n_step)\n        Returns:\n            title_outputs: tensor.shape=(batch_size, self.n_filter_total)\n        """"""\n        inputs = tf.nn.embedding_lookup(self.embedding, X_inputs)\n        inputs = tf.expand_dims(inputs, -1)\n        pooled_outputs = list()\n        for i, filter_size in enumerate(self.filter_sizes):\n            with tf.variable_scope(""conv-maxpool-%s"" % filter_size):\n                # Convolution Layer\n                filter_shape = [filter_size, self.embedding_size, 1, self.n_filter]\n                W_filter = self.weight_variable(shape=filter_shape, name=\'W_filter\')\n                beta = self.bias_variable(shape=[self.n_filter], name=\'beta_filter\')\n                tf.summary.histogram(\'beta\', beta)\n                conv = tf.nn.conv2d(inputs, W_filter, strides=[1, 1, 1, 1], padding=""VALID"", name=""conv"")\n                conv_bn, update_ema = self.batchnorm(conv, beta, convolutional=True)  # \xe5\x9c\xa8\xe6\xbf\x80\xe6\xb4\xbb\xe5\xb1\x82\xe5\x89\x8d\xe9\x9d\xa2\xe5\x8a\xa0 BN\n                # Apply nonlinearity, batch norm scaling is not useful with relus\n                # batch norm offsets are used instead of biases,\xe4\xbd\xbf\xe7\x94\xa8 BN \xe5\xb1\x82\xe7\x9a\x84 offset\xef\xbc\x8c\xe4\xb8\x8d\xe8\xa6\x81 biases\n                h = tf.nn.relu(conv_bn, name=""relu"")\n                # Maxpooling over the outputs\n                pooled = tf.nn.max_pool(h, ksize=[1, n_step - filter_size + 1, 1, 1],\n                                        strides=[1, 1, 1, 1], padding=\'VALID\', name=""pool"")\n                pooled_outputs.append(pooled)\n                self.update_emas.append(update_ema)\n        h_pool = tf.concat(pooled_outputs, 3)\n        h_pool_flat = tf.reshape(h_pool, [-1, self.n_filter_total])\n        return h_pool_flat  # shape = [batch_size, self.n_filter_total]\n\n\n# test the model\n# def test():\n#     import numpy as np\n#     print(\'Begin testing...\')\n#     settings = Settings()\n#     W_embedding = np.random.randn(50, 10)\n#     config = tf.ConfigProto()\n#     config.gpu_options.allow_growth = True\n#     batch_size = 128\n#     with tf.Session(config=config) as sess:\n#         model = TextCNN(W_embedding, settings)\n#         optimizer = tf.train.AdamOptimizer(0.001)\n#         train_op = optimizer.minimize(model.loss)\n#         update_op = tf.group(*model.update_emas)\n#         sess.run(tf.global_variables_initializer())\n#         fetch = [model.loss, model.y_pred, train_op, update_op]\n#         loss_list = list()\n#         for i in xrange(100):\n#             X1_batch = np.zeros((batch_size, 30), dtype=float)\n#             X2_batch = np.zeros((batch_size, 150), dtype=float)\n#             y_batch = np.zeros((batch_size, 1999), dtype=int)\n#             _batch_size = len(y_batch)\n#             feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\n#                          model.batch_size: _batch_size, model.tst: False, model.keep_prob: 0.5}\n#             loss, y_pred, _, _ = sess.run(fetch, feed_dict=feed_dict)\n#             loss_list.append(loss)\n#             print(i, loss)\n#\n# if __name__ == \'__main__\':\n#     test()\n'"
models/wd_1_2_cnn_max/predict.py,4,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport os\r\nimport sys\r\nimport time\r\nimport network\r\n\r\nsys.path.append(\'../..\')\r\nfrom evaluator import score_eval\r\n\r\nsettings = network.Settings()\r\ntitle_len = settings.title_len\r\nmodel_name = settings.model_name\r\nckpt_path = settings.ckpt_path\r\n\r\nlocal_scores_path = \'../../local_scores/\'\r\nscores_path = \'../../scores/\'\r\nif not os.path.exists(local_scores_path):\r\n    os.makedirs(local_scores_path)\r\nif not os.path.exists(scores_path):\r\n    os.makedirs(scores_path)\r\n\r\nembedding_path = \'../../data/word_embedding.npy\'\r\ndata_valid_path = \'../../data/wd-data/data_valid/\'\r\ndata_test_path = \'../../data/wd-data/data_test/\'\r\nva_batches = os.listdir(data_valid_path)\r\nte_batches = os.listdir(data_test_path)  # batch \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8\r\nn_va_batches = len(va_batches)\r\nn_te_batches = len(te_batches)\r\n\r\n\r\ndef get_batch(batch_id):\r\n    """"""get a batch from valid data""""""\r\n    new_batch = np.load(data_valid_path + str(batch_id) + \'.npz\')\r\n    X_batch = new_batch[\'X\']\r\n    y_batch = new_batch[\'y\']\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch, y_batch]\r\n\r\n\r\ndef get_test_batch(batch_id):\r\n    """"""get a batch from test data""""""\r\n    X_batch = np.load(data_test_path + str(batch_id) + \'.npy\')\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch]\r\n\r\n\r\ndef local_predict(sess, model):\r\n    """"""Test on the valid data.""""""\r\n    time0 = time.time()\r\n    predict_labels_list = list()  # \xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\r\n    marked_labels_list = list()\r\n    predict_scores = list()\r\n    for i in tqdm(xrange(n_va_batches)):\r\n        [X1_batch, X2_batch, y_batch] = get_batch(i)\r\n        marked_labels_list.extend(y_batch)\r\n        _batch_size = len(X1_batch)\r\n        fetches = [model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        predict_labels = sess.run(fetches, feed_dict)[0]\r\n        predict_scores.append(predict_labels)\r\n        predict_labels = map(lambda label: label.argsort()[-1:-6:-1], predict_labels)  # \xe5\x8f\x96\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x845\xe4\xb8\xaa\xe4\xb8\x8b\xe6\xa0\x87\r\n        predict_labels_list.extend(predict_labels)\r\n    predict_label_and_marked_label_list = zip(predict_labels_list, marked_labels_list)\r\n    precision, recall, f1 = score_eval(predict_label_and_marked_label_list)\r\n    print(\'Local valid p=%g, r=%g, f1=%g\' % (precision, recall, f1))\r\n    predict_scores = np.vstack(np.asarray(predict_scores))\r\n    local_scores_name = local_scores_path + model_name + \'.npy\'\r\n    np.save(local_scores_name, predict_scores)\r\n    print(\'local_scores.shape=\', predict_scores.shape)\r\n    print(\'Writed the scores into %s, time %g s\' % (local_scores_name, time.time() - time0))\r\n\r\n\r\ndef predict(sess, model):\r\n    """"""Test on the test data.""""""\r\n    time0 = time.time()\r\n    predict_scores = list()\r\n    for i in tqdm(xrange(n_te_batches)):\r\n        [X1_batch, X2_batch] = get_test_batch(i)\r\n        _batch_size = len(X1_batch)\r\n        fetches = [model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        predict_labels = sess.run(fetches, feed_dict)[0]\r\n        predict_scores.append(predict_labels)\r\n    predict_scores = np.vstack(np.asarray(predict_scores))\r\n    scores_name = scores_path + model_name + \'.npy\'\r\n    np.save(scores_name, predict_scores)\r\n    print(\'scores.shape=\', predict_scores.shape)\r\n    print(\'Writed the scores into %s, time %g s\' % (scores_name, time.time() - time0))\r\n\r\n\r\ndef main(_):\r\n    if not os.path.exists(ckpt_path + \'checkpoint\'):\r\n        print(\'there is not saved model, please check the ckpt path\')\r\n        exit()\r\n    print(\'Loading model...\')\r\n    W_embedding = np.load(embedding_path)\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    with tf.Session(config=config) as sess:\r\n        model = network.TextCNN(W_embedding, settings)\r\n        model.saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\r\n        print(\'Local predicting...\')\r\n        local_predict(sess, model)\r\n        print(\'Test predicting...\')\r\n        predict(sess, model)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    tf.app.run()\r\n'"
models/wd_1_2_cnn_max/train.py,21,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport os\r\nimport sys\r\nimport shutil\r\nimport time\r\nimport network\r\n\r\nsys.path.append(\'../..\')\r\nfrom data_helpers import to_categorical\r\nfrom evaluator import score_eval\r\n\r\nflags = tf.flags\r\nflags.DEFINE_bool(\'is_retrain\', False, \'if is_retrain is true, not rebuild the summary\')\r\nflags.DEFINE_integer(\'max_epoch\', 1, \'update the embedding after max_epoch, default: 1\')\r\nflags.DEFINE_integer(\'max_max_epoch\', 6, \'all training epoches, default: 6\')\r\nflags.DEFINE_float(\'lr\', 1e-3, \'initial learning rate, default: 1e-3\')\r\nflags.DEFINE_float(\'decay_rate\', 0.65, \'decay rate, default: 0.65\')\r\nflags.DEFINE_float(\'keep_prob\', 0.5, \'keep_prob for training, default: 0.5\')\r\n# \xe6\xad\xa3\xe5\xbc\x8f\r\n# flags.DEFINE_integer(\'decay_step\', 15000, \'decay_step, default: 15000\')\r\n# flags.DEFINE_integer(\'valid_step\', 10000, \'valid_step, default: 10000\')\r\n# flags.DEFINE_float(\'last_f1\', 0.40, \'if valid_f1 > last_f1, save new model. default: 0.40\')\r\n\r\n# \xe6\xb5\x8b\xe8\xaf\x95\r\nflags.DEFINE_integer(\'decay_step\', 1000, \'decay_step, default: 1000\')\r\nflags.DEFINE_integer(\'valid_step\', 500, \'valid_step, default: 500\')\r\nflags.DEFINE_float(\'last_f1\', 0.10, \'if valid_f1 > last_f1, save new model. default: 0.10\')\r\nFLAGS = flags.FLAGS\r\n\r\nlr = FLAGS.lr\r\nlast_f1 = FLAGS.last_f1\r\nsettings = network.Settings()\r\ntitle_len = settings.title_len\r\nsummary_path = settings.summary_path\r\nckpt_path = settings.ckpt_path\r\nmodel_path = ckpt_path + \'model.ckpt\'\r\n\r\nembedding_path = \'../../data/word_embedding.npy\'\r\ndata_train_path = \'../../data/wd-data/data_train/\'\r\ndata_valid_path = \'../../data/wd-data/data_valid/\'\r\ntr_batches = os.listdir(data_train_path)  # batch \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8\r\nva_batches = os.listdir(data_valid_path)\r\nn_tr_batches = len(tr_batches)\r\nn_va_batches = len(va_batches)\r\n\r\n# \xe6\xb5\x8b\xe8\xaf\x95\r\nn_tr_batches = 1000\r\nn_va_batches = 50\r\n\r\n\r\ndef get_batch(data_path, batch_id):\r\n    """"""get a batch from data_path""""""\r\n    new_batch = np.load(data_path + str(batch_id) + \'.npz\')\r\n    X_batch = new_batch[\'X\']\r\n    y_batch = new_batch[\'y\']\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch, y_batch]\r\n\r\n\r\ndef valid_epoch(data_path, sess, model):\r\n    """"""Test on the valid data.""""""\r\n    va_batches = os.listdir(data_path)\r\n    n_va_batches = len(va_batches)\r\n    _costs = 0.0\r\n    predict_labels_list = list()  # \xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\r\n    marked_labels_list = list()\r\n    for i in xrange(n_va_batches):\r\n        [X1_batch, X2_batch, y_batch] = get_batch(data_path, i)\r\n        marked_labels_list.extend(y_batch)\r\n        y_batch = to_categorical(y_batch)\r\n        _batch_size = len(y_batch)\r\n        fetches = [model.loss, model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        _cost, predict_labels = sess.run(fetches, feed_dict)\r\n        _costs += _cost\r\n        predict_labels = map(lambda label: label.argsort()[-1:-6:-1], predict_labels)  # \xe5\x8f\x96\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x845\xe4\xb8\xaa\xe4\xb8\x8b\xe6\xa0\x87\r\n        predict_labels_list.extend(predict_labels)\r\n    predict_label_and_marked_label_list = zip(predict_labels_list, marked_labels_list)\r\n    precision, recall, f1 = score_eval(predict_label_and_marked_label_list)\r\n    mean_cost = _costs / n_va_batches\r\n    return mean_cost, precision, recall, f1\r\n\r\n\r\ndef train_epoch(data_path, sess, model, train_fetches, valid_fetches, train_writer, test_writer):\r\n    global last_f1\r\n    global lr\r\n    time0 = time.time()\r\n    batch_indexs = np.random.permutation(n_tr_batches)  # shuffle the training data\r\n    for batch in tqdm(xrange(n_tr_batches)):\r\n        global_step = sess.run(model.global_step)\r\n        if 0 == (global_step + 1) % FLAGS.valid_step:\r\n            valid_cost, precision, recall, f1 = valid_epoch(data_valid_path, sess, model)\r\n            print(\'Global_step=%d: valid cost=%g; p=%g, r=%g, f1=%g, time=%g s\' % (\r\n                global_step, valid_cost, precision, recall, f1, time.time() - time0))\r\n            time0 = time.time()\r\n            if f1 > last_f1:\r\n                last_f1 = f1\r\n                saving_path = model.saver.save(sess, model_path, global_step+1)\r\n                print(\'saved new model to %s \' % saving_path)\r\n        # training\r\n        batch_id = batch_indexs[batch]\r\n        [X1_batch, X2_batch, y_batch] = get_batch(data_train_path, batch_id)\r\n        y_batch = to_categorical(y_batch)\r\n        _batch_size = len(y_batch)\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                     model.batch_size: _batch_size, model.tst: False, model.keep_prob: FLAGS.keep_prob}\r\n        summary, _cost, _, _ = sess.run(train_fetches, feed_dict)  # the cost is the mean cost of one batch\r\n        # valid per 500 steps\r\n        if 0 == (global_step + 1) % 500:\r\n            train_writer.add_summary(summary, global_step)\r\n            batch_id = np.random.randint(0, n_va_batches)  # \xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe4\xb8\x80\xe4\xb8\xaa\xe9\xaa\x8c\xe8\xaf\x81batch\r\n            [X1_batch, X2_batch, y_batch] = get_batch(data_valid_path, batch_id)\r\n            y_batch = to_categorical(y_batch)\r\n            _batch_size = len(y_batch)\r\n            feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                         model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n            summary, _cost = sess.run(valid_fetches, feed_dict)\r\n            test_writer.add_summary(summary, global_step)\r\n\r\n\r\ndef main(_):\r\n    global ckpt_path\r\n    global last_f1\r\n    if not os.path.exists(ckpt_path):\r\n        os.makedirs(ckpt_path)\r\n    if not os.path.exists(summary_path):\r\n        os.makedirs(summary_path)\r\n    elif not FLAGS.is_retrain:  # \xe9\x87\x8d\xe6\x96\xb0\xe8\xae\xad\xe7\xbb\x83\xe6\x9c\xac\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\x88\xa0\xe9\x99\xa4\xe4\xbb\xa5\xe5\x89\x8d\xe7\x9a\x84 summary\r\n        shutil.rmtree(summary_path)\r\n        os.makedirs(summary_path)\r\n    if not os.path.exists(summary_path):\r\n        os.makedirs(summary_path)\r\n\r\n    print(\'1.Loading data...\')\r\n    W_embedding = np.load(embedding_path)\r\n    print(\'training sample_num = %d\' % n_tr_batches)\r\n    print(\'valid sample_num = %d\' % n_va_batches)\r\n\r\n    # Initial or restore the model\r\n    print(\'2.Building model...\')\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    with tf.Session(config=config) as sess:\r\n        model = network.TextCNN(W_embedding, settings)\r\n        with tf.variable_scope(\'training_ops\') as vs:\r\n            learning_rate = tf.train.exponential_decay(FLAGS.lr, model.global_step, FLAGS.decay_step,\r\n                                                   FLAGS.decay_rate, staircase=True)\r\n            # two optimizer: op1, update embedding; op2, do not update embedding.\r\n            with tf.variable_scope(\'Optimizer1\'):\r\n                tvars1 = tf.trainable_variables()\r\n                grads1 = tf.gradients(model.loss, tvars1)\r\n                optimizer1 = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n                train_op1 = optimizer1.apply_gradients(zip(grads1, tvars1),\r\n                                                   global_step=model.global_step)\r\n            with tf.variable_scope(\'Optimizer2\'):\r\n                tvars2 = [tvar for tvar in tvars1 if \'embedding\' not in tvar.name]\r\n                grads2 = tf.gradients(model.loss, tvars2)\r\n                optimizer2 = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n                train_op2 = optimizer2.apply_gradients(zip(grads2, tvars2),\r\n                                                   global_step=model.global_step)\r\n            update_op = tf.group(*model.update_emas)\r\n            merged = tf.summary.merge_all()  # summary\r\n            train_writer = tf.summary.FileWriter(summary_path + \'train\', sess.graph)\r\n            test_writer = tf.summary.FileWriter(summary_path + \'test\')\r\n            training_ops = [v for v in tf.global_variables() if v.name.startswith(vs.name+\'/\')]\r\n\r\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe5\xb7\xb2\xe7\xbb\x8f\xe4\xbf\x9d\xe5\xad\x98\xe8\xbf\x87\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\xaf\xbc\xe5\x85\xa5\xe4\xb8\x8a\xe6\xac\xa1\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\r\n        if os.path.exists(ckpt_path + ""checkpoint""):\r\n            print(""Restoring Variables from Checkpoint..."")\r\n            model.saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\r\n            last_valid_cost, precision, recall, last_f1 = valid_epoch(data_valid_path, sess, model)\r\n            print(\' valid cost=%g; p=%g, r=%g, f1=%g\' % (last_valid_cost, precision, recall, last_f1))\r\n            sess.run(tf.variables_initializer(training_ops))\r\n            train_op2 = train_op1\r\n        else:\r\n            print(\'Initializing Variables...\')\r\n            sess.run(tf.global_variables_initializer())\r\n\r\n        print(\'3.Begin training...\')\r\n        print(\'max_epoch=%d, max_max_epoch=%d\' % (FLAGS.max_epoch, FLAGS.max_max_epoch))\r\n        train_op = train_op2\r\n        for epoch in xrange(FLAGS.max_max_epoch):\r\n            global_step = sess.run(model.global_step)\r\n            print(\'Global step %d, lr=%g\' % (global_step, sess.run(learning_rate)))\r\n            if epoch == FLAGS.max_epoch:  # update the embedding\r\n                train_op = train_op1\r\n            train_fetches = [merged, model.loss, train_op, update_op]\r\n            valid_fetches = [merged, model.loss]\r\n            train_epoch(data_train_path, sess, model, train_fetches, valid_fetches, train_writer, test_writer)\r\n        # \xe6\x9c\x80\xe5\x90\x8e\xe5\x86\x8d\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\r\n        valid_cost, precision, recall, f1 = valid_epoch(data_valid_path, sess, model)\r\n        print(\'END.Global_step=%d: valid cost=%g; p=%g, r=%g, f1=%g\' % (\r\n            sess.run(model.global_step), valid_cost, precision, recall, f1))\r\n        if f1 > last_f1:  # save the better model\r\n            saving_path = model.saver.save(sess, model_path, sess.run(model.global_step)+1)\r\n            print(\'saved new model to %s \' % saving_path)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    tf.app.run()\r\n'"
models/wd_2_hcnn/__init__.py,0,b'# -*- coding:utf-8 -*- \r\n\r\n'
models/wd_2_hcnn/network.py,62,"b'# -*- coding:utf-8 -*-\n\nimport tensorflow as tf\n\n""""""wd_2_hcnn\ntitle \xe9\x83\xa8\xe5\x88\x86\xe4\xbd\xbf\xe7\x94\xa8 TextCNN\xef\xbc\x9bcontent \xe9\x83\xa8\xe5\x88\x86\xe4\xbd\xbf\xe7\x94\xa8\xe5\x88\x86\xe5\xb1\x82\xe7\x9a\x84 TextCNN\xe3\x80\x82\n""""""\n\n\nclass Settings(object):\n    def __init__(self):\n        self.model_name = \'wd_2_hcnn\'\n        self.title_len = self.sent_len = 30\n        self.doc_len = 10\n        self.sent_filter_sizes = [2, 3, 4, 5]\n        self.doc_filter_sizes = [2, 3, 4]\n        self.n_filter = 256\n        self.fc_hidden_size = 1024\n        self.n_class = 1999\n        self.summary_path = \'../../summary/\' + self.model_name + \'/\'\n        self.ckpt_path = \'../../ckpt/\' + self.model_name + \'/\'\n\n\nclass HCNN(object):\n    """"""\n    title: inputs->textcnn->output_title\n    content: inputs->hcnn->output_content\n    concat[output_title, output_content] -> fc+bn+relu -> sigmoid_entropy.\n    """"""\n\n    def __init__(self, W_embedding, settings):\n        self.model_name = settings.model_name\n        self.sent_len = settings.sent_len\n        self.doc_len = settings.doc_len\n        self.sent_filter_sizes = settings.sent_filter_sizes\n        self.doc_filter_sizes = settings.doc_filter_sizes\n        self.n_filter = settings.n_filter\n        self.n_class = settings.n_class\n        self.fc_hidden_size = settings.fc_hidden_size\n        self._global_step = tf.Variable(0, trainable=False, name=\'Global_Step\')\n        self.update_emas = list()\n        # placeholders\n        self._tst = tf.placeholder(tf.bool)\n        self._keep_prob = tf.placeholder(tf.float32, [])\n        self._batch_size = tf.placeholder(tf.int32, [])\n\n        with tf.name_scope(\'Inputs\'):\n            self._X1_inputs = tf.placeholder(tf.int64, [None, self.sent_len], name=\'X1_inputs\')\n            self._X2_inputs = tf.placeholder(tf.int64, [None, self.doc_len * self.sent_len], name=\'X2_inputs\')\n            self._y_inputs = tf.placeholder(tf.float32, [None, self.n_class], name=\'y_input\')\n\n        with tf.variable_scope(\'embedding\'):\n            self.embedding = tf.get_variable(name=\'embedding\', shape=W_embedding.shape,\n                                             initializer=tf.constant_initializer(W_embedding), trainable=True)\n        self.embedding_size = W_embedding.shape[1]\n\n        with tf.variable_scope(\'cnn_text\'):\n            output_title = self.cnn_inference(self._X1_inputs)\n\n        with tf.variable_scope(\'hcnn_content\'):\n            output_content = self.hcnn_inference(self._X2_inputs)\n\n        with tf.variable_scope(\'fc-bn-layer\'):\n            output = tf.concat([output_title, output_content], axis=1)\n            output_size = self.n_filter * (len(self.sent_filter_sizes) + len(self.doc_filter_sizes))\n            W_fc = self.weight_variable([output_size, self.fc_hidden_size], name=\'Weight_fc\')\n            tf.summary.histogram(\'W_fc\', W_fc)\n            h_fc = tf.matmul(output, W_fc, name=\'h_fc\')\n            beta_fc = tf.Variable(tf.constant(0.1, tf.float32, shape=[self.fc_hidden_size], name=""beta_fc""))\n            tf.summary.histogram(\'beta_fc\', beta_fc)\n            fc_bn, update_ema_fc = self.batchnorm(h_fc, beta_fc, convolutional=False)\n            self.update_emas.append(update_ema_fc)\n            self.fc_bn_relu = tf.nn.relu(fc_bn, name=""relu"")\n            fc_bn_drop = tf.nn.dropout(self.fc_bn_relu, self.keep_prob)\n\n        with tf.variable_scope(\'out_layer\'):\n            W_out = self.weight_variable([self.fc_hidden_size, self.n_class], name=\'Weight_out\')\n            tf.summary.histogram(\'Weight_out\', W_out)\n            b_out = self.bias_variable([self.n_class], name=\'bias_out\')\n            tf.summary.histogram(\'bias_out\', b_out)\n            self._y_pred = tf.nn.xw_plus_b(fc_bn_drop, W_out, b_out, name=\'y_pred\')  # \xe6\xaf\x8f\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe5\x88\x86\xe6\x95\xb0 scores\n\n        with tf.name_scope(\'loss\'):\n            self._loss = tf.reduce_mean(\n                tf.nn.sigmoid_cross_entropy_with_logits(logits=self._y_pred, labels=self._y_inputs))\n            tf.summary.scalar(\'loss\', self._loss)\n\n        self.saver = tf.train.Saver(max_to_keep=2)\n\n    @property\n    def tst(self):\n        return self._tst\n\n    @property\n    def keep_prob(self):\n        return self._keep_prob\n\n    @property\n    def batch_size(self):\n        return self._batch_size\n\n    @property\n    def global_step(self):\n        return self._global_step\n\n    @property\n    def X1_inputs(self):\n        return self._X1_inputs\n\n    @property\n    def X2_inputs(self):\n        return self._X2_inputs\n\n    @property\n    def y_inputs(self):\n        return self._y_inputs\n\n    @property\n    def y_pred(self):\n        return self._y_pred\n\n    @property\n    def loss(self):\n        return self._loss\n\n    def weight_variable(self, shape, name):\n        """"""Create a weight variable with appropriate initialization.""""""\n        initial = tf.truncated_normal(shape, stddev=0.1)\n        return tf.Variable(initial, name=name)\n\n    def bias_variable(self, shape, name):\n        """"""Create a bias variable with appropriate initialization.""""""\n        initial = tf.constant(0.1, shape=shape)\n        return tf.Variable(initial, name=name)\n\n    def batchnorm(self, Ylogits, offset, convolutional=False):\n        """"""batchnormalization.\n        Args:\n            Ylogits: 1D\xe5\x90\x91\xe9\x87\x8f\xe6\x88\x96\xe8\x80\x85\xe6\x98\xaf3D\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\n            num_updates: \xe8\xbf\xad\xe4\xbb\xa3\xe7\x9a\x84global_step\n            offset\xef\xbc\x9a\xe8\xa1\xa8\xe7\xa4\xbabeta\xef\xbc\x8c\xe5\x85\xa8\xe5\xb1\x80\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x9b\xe5\x9c\xa8 RELU \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe4\xb8\x80\xe8\x88\xac\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xba 0.1\xe3\x80\x82\n            scale\xef\xbc\x9a\xe8\xa1\xa8\xe7\xa4\xbalambda\xef\xbc\x8c\xe5\x85\xa8\xe5\xb1\x80\xe6\x96\xb9\xe5\xb7\xae\xef\xbc\x9b\xe5\x9c\xa8 sigmoid \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe9\x9c\x80\xe8\xa6\x81\xef\xbc\x8c\xe8\xbf\x99 RELU \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe4\xbd\x9c\xe7\x94\xa8\xe4\xb8\x8d\xe5\xa4\xa7\xe3\x80\x82\n            m: \xe8\xa1\xa8\xe7\xa4\xbabatch\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x9bv:\xe8\xa1\xa8\xe7\xa4\xbabatch\xe6\x96\xb9\xe5\xb7\xae\xe3\x80\x82\n            bnepsilon\xef\xbc\x9a\xe4\xb8\x80\xe4\xb8\xaa\xe5\xbe\x88\xe5\xb0\x8f\xe7\x9a\x84\xe6\xb5\xae\xe7\x82\xb9\xe6\x95\xb0\xef\xbc\x8c\xe9\x98\xb2\xe6\xad\xa2\xe9\x99\xa4\xe4\xbb\xa5 0.\n        Returns:\n            Ybn: \xe5\x92\x8c Ylogits \xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf\xe7\xbb\x8f\xe8\xbf\x87 Batch Normalization \xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\n            update_moving_everages\xef\xbc\x9a\xe6\x9b\xb4\xe6\x96\xb0mean\xe5\x92\x8cvariance\xef\xbc\x8c\xe4\xb8\xbb\xe8\xa6\x81\xe6\x98\xaf\xe7\xbb\x99\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84 test \xe4\xbd\xbf\xe7\x94\xa8\xe3\x80\x82\n        """"""\n        exp_moving_avg = tf.train.ExponentialMovingAverage(0.999,\n                                                           self._global_step)  # adding the iteration prevents from averaging across non-existing iterations\n        bnepsilon = 1e-5\n        if convolutional:\n            mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])\n        else:\n            mean, variance = tf.nn.moments(Ylogits, [0])\n        update_moving_everages = exp_moving_avg.apply([mean, variance])\n        m = tf.cond(self.tst, lambda: exp_moving_avg.average(mean), lambda: mean)\n        v = tf.cond(self.tst, lambda: exp_moving_avg.average(variance), lambda: variance)\n        Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n        return Ybn, update_moving_everages\n\n    def textcnn(self, X_inputs, n_step, filter_sizes, embed_size):\n        """"""build the TextCNN network.\n        n_step: the sentence len.""""""\n        inputs = tf.expand_dims(X_inputs, -1)\n        pooled_outputs = list()\n        for i, filter_size in enumerate(filter_sizes):\n            with tf.name_scope(""conv-maxpool-%s"" % filter_size):\n                # Convolution Layer\n                filter_shape = [filter_size, embed_size, 1, self.n_filter]\n                W_filter = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W_filter"")\n                beta = tf.Variable(tf.constant(0.1, tf.float32, shape=[self.n_filter], name=""beta""))\n                tf.summary.histogram(\'beta\', beta)\n                conv = tf.nn.conv2d(inputs, W_filter, strides=[1, 1, 1, 1], padding=""VALID"", name=""conv"")\n                conv_bn, update_ema = self.batchnorm(conv, beta, convolutional=True)  # \xe5\x9c\xa8\xe6\xbf\x80\xe6\xb4\xbb\xe5\xb1\x82\xe5\x89\x8d\xe9\x9d\xa2\xe5\x8a\xa0 BN\n                # Apply nonlinearity, batch norm scaling is not useful with relus\n                # batch norm offsets are used instead of biases,\xe4\xbd\xbf\xe7\x94\xa8 BN \xe5\xb1\x82\xe7\x9a\x84 offset\xef\xbc\x8c\xe4\xb8\x8d\xe8\xa6\x81 biases\n                h = tf.nn.relu(conv_bn, name=""relu"")\n                # Maxpooling over the outputs\n                pooled = tf.nn.max_pool(h, ksize=[1, n_step - filter_size + 1, 1, 1],\n                                        strides=[1, 1, 1, 1], padding=\'VALID\', name=""pool"")\n                pooled_outputs.append(pooled)\n                self.update_emas.append(update_ema)\n        h_pool = tf.concat(pooled_outputs, 3)\n        n_filter_total = self.n_filter * len(filter_sizes)\n        h_pool_flat = tf.reshape(h_pool, [-1, n_filter_total])\n        return h_pool_flat  # shape = [-1, n_filter_total]\n\n    def cnn_inference(self, X_inputs):\n        """"""TextCNN \xe6\xa8\xa1\xe5\x9e\x8b\xe3\x80\x82title\xe9\x83\xa8\xe5\x88\x86\xe3\x80\x82\n        Args:\n            X_inputs: tensor.shape=(batch_size, title_len)\n        Returns:\n            title_outputs: tensor.shape=(batch_size, n_filter*filter_num_sent)\n        """"""\n        inputs = tf.nn.embedding_lookup(self.embedding, X_inputs)\n        with tf.variable_scope(\'title_encoder\'):  # \xe7\x94\x9f\xe6\x88\x90 title \xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f\xe8\xa1\xa8\xe7\xa4\xba\n            title_outputs = self.textcnn(inputs, self.sent_len, self.sent_filter_sizes, embed_size=self.embedding_size)\n        return title_outputs  # shape = [batch_size, n_filter*filter_num_sent]\n\n    def hcnn_inference(self, X_inputs):\n        """"""\xe5\x88\x86\xe5\xb1\x82 TextCNN \xe6\xa8\xa1\xe5\x9e\x8b\xe3\x80\x82content\xe9\x83\xa8\xe5\x88\x86\xe3\x80\x82\n        Args:\n            X_inputs: tensor.shape=(batch_size, doc_len*sent_len)\n        Returns:\n            doc_attn_outputs: tensor.shape=(batch_size, n_filter*filter_num_doc)\n        """"""\n        inputs = tf.nn.embedding_lookup(self.embedding,\n                                        X_inputs)  # inputs.shape=[batch_size, doc_len*sent_len, embedding_size]\n        sent_inputs = tf.reshape(inputs, [self.batch_size * self.doc_len, self.sent_len,\n                                          self.embedding_size])  # [batch_size*doc_len, sent_len, embedding_size]\n        with tf.variable_scope(\'sentence_encoder\'):  # \xe7\x94\x9f\xe6\x88\x90\xe5\x8f\xa5\xe5\x90\x91\xe9\x87\x8f\n            sent_outputs = self.textcnn(sent_inputs, self.sent_len, self.sent_filter_sizes, self.embedding_size)\n        with tf.variable_scope(\'doc_encoder\'):  # \xe7\x94\x9f\xe6\x88\x90\xe6\x96\x87\xe6\xa1\xa3\xe5\x90\x91\xe9\x87\x8f\n            doc_inputs = tf.reshape(sent_outputs, [self.batch_size, self.doc_len, self.n_filter * len(\n                self.sent_filter_sizes)])  # [batch_size, doc_len, n_filter*len(filter_sizes_sent)]\n            doc_outputs = self.textcnn(doc_inputs, self.doc_len, self.doc_filter_sizes, self.n_filter * len(\n                self.sent_filter_sizes))  # [batch_size, doc_len, n_filter*filter_num_doc]\n        return doc_outputs  # [batch_size,  n_filter*len(doc_filter_sizes)]\n\n# test the model\n# def test():\n#     import numpy as np\n#     print(\'Begin testing...\')\n#     settings = Settings()\n#     W_embedding = np.random.randn(50, 10)\n#     config = tf.ConfigProto()\n#     config.gpu_options.allow_growth = True\n#     batch_size = 128\n#     with tf.Session(config=config) as sess:\n#         model = HCNN(W_embedding, settings)\n#         optimizer = tf.train.AdamOptimizer(0.001)\n#         train_op = optimizer.minimize(model.loss)\n#         update_op = tf.group(*model.update_emas)\n#         sess.run(tf.global_variables_initializer())\n#         fetch = [model.loss, model.y_pred, train_op, update_op]\n#         loss_list = list()\n#         for i in xrange(100):\n#             X1_batch = np.zeros((batch_size, 30), dtype=float)\n#             X2_batch = np.zeros((batch_size, 10 * 30), dtype=float)\n#             y_batch = np.zeros((batch_size, 1999), dtype=int)\n#             _batch_size = len(y_batch)\n#             feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\n#                          model.batch_size: _batch_size, model.tst: False, model.keep_prob: 0.5}\n#             loss, y_pred, _, _ = sess.run(fetch, feed_dict=feed_dict)\n#             loss_list.append(loss)\n#             print(i, loss)\n\n# test()\n'"
models/wd_2_hcnn/predict.py,4,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport os\r\nimport sys\r\nimport time\r\nimport network\r\n\r\nsys.path.append(\'../..\')\r\nfrom evaluator import score_eval\r\n\r\nsettings = network.Settings()\r\ntitle_len = settings.title_len\r\nmodel_name = settings.model_name\r\nckpt_path = settings.ckpt_path\r\n\r\nlocal_scores_path = \'../../local_scores/\'\r\nscores_path = \'../../scores/\'\r\nif not os.path.exists(local_scores_path):\r\n    os.makedirs(local_scores_path)\r\nif not os.path.exists(scores_path):\r\n    os.makedirs(scores_path)\r\n\r\nembedding_path = \'../../data/word_embedding.npy\'\r\ndata_valid_path = \'../../data/wd-data/seg_valid/\'\r\ndata_test_path = \'../../data/wd-data/seg_test/\'\r\nva_batches = os.listdir(data_valid_path)\r\nte_batches = os.listdir(data_test_path)  # batch \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8\r\nn_va_batches = len(va_batches)\r\nn_te_batches = len(te_batches)\r\n\r\n\r\ndef get_batch(batch_id):\r\n    """"""get a batch from valid data""""""\r\n    new_batch = np.load(data_valid_path + str(batch_id) + \'.npz\')\r\n    X_batch = new_batch[\'X\']\r\n    y_batch = new_batch[\'y\']\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch, y_batch]\r\n\r\n\r\ndef get_test_batch(batch_id):\r\n    """"""get a batch from test data""""""\r\n    X_batch = np.load(data_test_path + str(batch_id) + \'.npy\')\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch]\r\n\r\n\r\ndef local_predict(sess, model):\r\n    """"""Test on the valid data.""""""\r\n    time0 = time.time()\r\n    predict_labels_list = list()  # \xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\r\n    marked_labels_list = list()\r\n    predict_scores = list()\r\n    for i in tqdm(xrange(n_va_batches)):\r\n        [X1_batch, X2_batch, y_batch] = get_batch(i)\r\n        marked_labels_list.extend(y_batch)\r\n        _batch_size = len(X1_batch)\r\n        fetches = [model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        predict_labels = sess.run(fetches, feed_dict)[0]\r\n        predict_scores.append(predict_labels)\r\n        predict_labels = map(lambda label: label.argsort()[-1:-6:-1], predict_labels)  # \xe5\x8f\x96\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x845\xe4\xb8\xaa\xe4\xb8\x8b\xe6\xa0\x87\r\n        predict_labels_list.extend(predict_labels)\r\n    predict_label_and_marked_label_list = zip(predict_labels_list, marked_labels_list)\r\n    precision, recall, f1 = score_eval(predict_label_and_marked_label_list)\r\n    print(\'Local valid p=%g, r=%g, f1=%g\' % (precision, recall, f1))\r\n    predict_scores = np.vstack(np.asarray(predict_scores))\r\n    local_scores_name = local_scores_path + model_name + \'.npy\'\r\n    np.save(local_scores_name, predict_scores)\r\n    print(\'local_scores.shape=\', predict_scores.shape)\r\n    print(\'Writed the scores into %s, time %g s\' % (local_scores_name, time.time() - time0))\r\n\r\n\r\ndef predict(sess, model):\r\n    """"""Test on the test data.""""""\r\n    time0 = time.time()\r\n    predict_scores = list()\r\n    for i in tqdm(xrange(n_te_batches)):\r\n        [X1_batch, X2_batch] = get_test_batch(i)\r\n        _batch_size = len(X1_batch)\r\n        fetches = [model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        predict_labels = sess.run(fetches, feed_dict)[0]\r\n        predict_scores.append(predict_labels)\r\n    predict_scores = np.vstack(np.asarray(predict_scores))\r\n    scores_name = scores_path + model_name + \'.npy\'\r\n    np.save(scores_name, predict_scores)\r\n    print(\'scores.shape=\', predict_scores.shape)\r\n    print(\'Writed the scores into %s, time %g s\' % (scores_name, time.time() - time0))\r\n\r\n\r\ndef main(_):\r\n    if not os.path.exists(ckpt_path + \'checkpoint\'):\r\n        print(\'there is not saved model, please check the ckpt path\')\r\n        exit()\r\n    print(\'Loading model...\')\r\n    W_embedding = np.load(embedding_path)\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    with tf.Session(config=config) as sess:\r\n        model = network.HCNN(W_embedding, settings)\r\n        model.saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\r\n        print(\'Local predicting...\')\r\n        local_predict(sess, model)\r\n        print(\'Test predicting...\')\r\n        predict(sess, model)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    tf.app.run()\r\n'"
models/wd_2_hcnn/train.py,21,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport os\r\nimport sys\r\nimport shutil\r\nimport time\r\nimport network\r\n\r\nsys.path.append(\'../..\')\r\nfrom data_helpers import to_categorical\r\nfrom evaluator import score_eval\r\n\r\nflags = tf.flags\r\nflags.DEFINE_bool(\'is_retrain\', False, \'if is_retrain is true, not rebuild the summary\')\r\nflags.DEFINE_integer(\'max_epoch\', 1, \'update the embedding after max_epoch, default: 1\')\r\nflags.DEFINE_integer(\'max_max_epoch\', 6, \'all training epoches, default: 6\')\r\nflags.DEFINE_float(\'lr\', 1e-3, \'initial learning rate, default: 1e-3\')\r\nflags.DEFINE_float(\'decay_rate\', 0.65, \'decay rate, default: 0.65\')\r\nflags.DEFINE_float(\'keep_prob\', 0.5, \'keep_prob for training, default: 0.5\')\r\n# \xe6\xad\xa3\xe5\xbc\x8f\r\n# flags.DEFINE_integer(\'decay_step\', 15000, \'decay_step, default: 15000\')\r\n# flags.DEFINE_integer(\'valid_step\', 10000, \'valid_step, default: 10000\')\r\n# flags.DEFINE_float(\'last_f1\', 0.40, \'if valid_f1 > last_f1, save new model. default: 0.40\')\r\n\r\n# \xe6\xb5\x8b\xe8\xaf\x95\r\nflags.DEFINE_integer(\'decay_step\', 1000, \'decay_step, default: 1000\')\r\nflags.DEFINE_integer(\'valid_step\', 500, \'valid_step, default: 500\')\r\nflags.DEFINE_float(\'last_f1\', 0.10, \'if valid_f1 > last_f1, save new model. default: 0.10\')\r\nFLAGS = flags.FLAGS\r\n\r\nlr = FLAGS.lr\r\nlast_f1 = FLAGS.last_f1\r\nsettings = network.Settings()\r\ntitle_len = settings.title_len\r\nsummary_path = settings.summary_path\r\nckpt_path = settings.ckpt_path\r\nmodel_path = ckpt_path + \'model.ckpt\'\r\n\r\nembedding_path = \'../../data/word_embedding.npy\'\r\ndata_train_path = \'../../data/wd-data/data_train/\'\r\ndata_valid_path = \'../../data/wd-data/data_valid/\'\r\ntr_batches = os.listdir(data_train_path)  # batch \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8\r\nva_batches = os.listdir(data_valid_path)\r\nn_tr_batches = len(tr_batches)\r\nn_va_batches = len(va_batches)\r\n\r\n# \xe6\xb5\x8b\xe8\xaf\x95\r\nn_tr_batches = 1000\r\nn_va_batches = 50\r\n\r\n\r\ndef get_batch(data_path, batch_id):\r\n    """"""get a batch from data_path""""""\r\n    new_batch = np.load(data_path + str(batch_id) + \'.npz\')\r\n    X_batch = new_batch[\'X\']\r\n    y_batch = new_batch[\'y\']\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch, y_batch]\r\n\r\n\r\ndef valid_epoch(data_path, sess, model):\r\n    """"""Test on the valid data.""""""\r\n    va_batches = os.listdir(data_path)\r\n    n_va_batches = len(va_batches)\r\n    _costs = 0.0\r\n    predict_labels_list = list()  # \xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\r\n    marked_labels_list = list()\r\n    for i in xrange(n_va_batches):\r\n        [X1_batch, X2_batch, y_batch] = get_batch(data_path, i)\r\n        marked_labels_list.extend(y_batch)\r\n        y_batch = to_categorical(y_batch)\r\n        _batch_size = len(y_batch)\r\n        fetches = [model.loss, model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        _cost, predict_labels = sess.run(fetches, feed_dict)\r\n        _costs += _cost\r\n        predict_labels = map(lambda label: label.argsort()[-1:-6:-1], predict_labels)  # \xe5\x8f\x96\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x845\xe4\xb8\xaa\xe4\xb8\x8b\xe6\xa0\x87\r\n        predict_labels_list.extend(predict_labels)\r\n    predict_label_and_marked_label_list = zip(predict_labels_list, marked_labels_list)\r\n    precision, recall, f1 = score_eval(predict_label_and_marked_label_list)\r\n    mean_cost = _costs / n_va_batches\r\n    return mean_cost, precision, recall, f1\r\n\r\n\r\ndef train_epoch(data_path, sess, model, train_fetches, valid_fetches, train_writer, test_writer):\r\n    global last_f1\r\n    global lr\r\n    time0 = time.time()\r\n    batch_indexs = np.random.permutation(n_tr_batches)  # shuffle the training data\r\n    for batch in tqdm(xrange(n_tr_batches)):\r\n        global_step = sess.run(model.global_step)\r\n        if 0 == (global_step + 1) % FLAGS.valid_step:\r\n            valid_cost, precision, recall, f1 = valid_epoch(data_valid_path, sess, model)\r\n            print(\'Global_step=%d: valid cost=%g; p=%g, r=%g, f1=%g, time=%g s\' % (\r\n                global_step, valid_cost, precision, recall, f1, time.time() - time0))\r\n            time0 = time.time()\r\n            if f1 > last_f1:\r\n                last_f1 = f1\r\n                saving_path = model.saver.save(sess, model_path, global_step+1)\r\n                print(\'saved new model to %s \' % saving_path)\r\n        # training\r\n        batch_id = batch_indexs[batch]\r\n        [X1_batch, X2_batch, y_batch] = get_batch(data_train_path, batch_id)\r\n        y_batch = to_categorical(y_batch)\r\n        _batch_size = len(y_batch)\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                     model.batch_size: _batch_size, model.tst: False, model.keep_prob: FLAGS.keep_prob}\r\n        summary, _cost, _, _ = sess.run(train_fetches, feed_dict)  # the cost is the mean cost of one batch\r\n        # valid per 500 steps\r\n        if 0 == (global_step + 1) % 500:\r\n            train_writer.add_summary(summary, global_step)\r\n            batch_id = np.random.randint(0, n_va_batches)  # \xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe4\xb8\x80\xe4\xb8\xaa\xe9\xaa\x8c\xe8\xaf\x81batch\r\n            [X1_batch, X2_batch, y_batch] = get_batch(data_valid_path, batch_id)\r\n            y_batch = to_categorical(y_batch)\r\n            _batch_size = len(y_batch)\r\n            feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                         model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n            summary, _cost = sess.run(valid_fetches, feed_dict)\r\n            test_writer.add_summary(summary, global_step)\r\n\r\n\r\ndef main(_):\r\n    global ckpt_path\r\n    global last_f1\r\n    if not os.path.exists(ckpt_path):\r\n        os.makedirs(ckpt_path)\r\n    if not os.path.exists(summary_path):\r\n        os.makedirs(summary_path)\r\n    elif not FLAGS.is_retrain:  # \xe9\x87\x8d\xe6\x96\xb0\xe8\xae\xad\xe7\xbb\x83\xe6\x9c\xac\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\x88\xa0\xe9\x99\xa4\xe4\xbb\xa5\xe5\x89\x8d\xe7\x9a\x84 summary\r\n        shutil.rmtree(summary_path)\r\n        os.makedirs(summary_path)\r\n    if not os.path.exists(summary_path):\r\n        os.makedirs(summary_path)\r\n\r\n    print(\'1.Loading data...\')\r\n    W_embedding = np.load(embedding_path)\r\n    print(\'training sample_num = %d\' % n_tr_batches)\r\n    print(\'valid sample_num = %d\' % n_va_batches)\r\n\r\n    # Initial or restore the model\r\n    print(\'2.Building model...\')\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    with tf.Session(config=config) as sess:\r\n        model = network.TextCNN(W_embedding, settings)\r\n        with tf.variable_scope(\'training_ops\') as vs:\r\n            learning_rate = tf.train.exponential_decay(FLAGS.lr, model.global_step, FLAGS.decay_step,\r\n                                                   FLAGS.decay_rate, staircase=True)\r\n            # two optimizer: op1, update embedding; op2, do not update embedding.\r\n            with tf.variable_scope(\'Optimizer1\'):\r\n                tvars1 = tf.trainable_variables()\r\n                grads1 = tf.gradients(model.loss, tvars1)\r\n                optimizer1 = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n                train_op1 = optimizer1.apply_gradients(zip(grads1, tvars1),\r\n                                                   global_step=model.global_step)\r\n            with tf.variable_scope(\'Optimizer2\'):\r\n                tvars2 = [tvar for tvar in tvars1 if \'embedding\' not in tvar.name]\r\n                grads2 = tf.gradients(model.loss, tvars2)\r\n                optimizer2 = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n                train_op2 = optimizer2.apply_gradients(zip(grads2, tvars2),\r\n                                                   global_step=model.global_step)\r\n            update_op = tf.group(*model.update_emas)\r\n            merged = tf.summary.merge_all()  # summary\r\n            train_writer = tf.summary.FileWriter(summary_path + \'train\', sess.graph)\r\n            test_writer = tf.summary.FileWriter(summary_path + \'test\')\r\n            training_ops = [v for v in tf.global_variables() if v.name.startswith(vs.name+\'/\')]\r\n\r\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe5\xb7\xb2\xe7\xbb\x8f\xe4\xbf\x9d\xe5\xad\x98\xe8\xbf\x87\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\xaf\xbc\xe5\x85\xa5\xe4\xb8\x8a\xe6\xac\xa1\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\r\n        if os.path.exists(ckpt_path + ""checkpoint""):\r\n            print(""Restoring Variables from Checkpoint..."")\r\n            model.saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\r\n            last_valid_cost, precision, recall, last_f1 = valid_epoch(data_valid_path, sess, model)\r\n            print(\' valid cost=%g; p=%g, r=%g, f1=%g\' % (last_valid_cost, precision, recall, last_f1))\r\n            sess.run(tf.variables_initializer(training_ops))\r\n            train_op2 = train_op1\r\n        else:\r\n            print(\'Initializing Variables...\')\r\n            sess.run(tf.global_variables_initializer())\r\n\r\n        print(\'3.Begin training...\')\r\n        print(\'max_epoch=%d, max_max_epoch=%d\' % (FLAGS.max_epoch, FLAGS.max_max_epoch))\r\n        train_op = train_op2\r\n        for epoch in xrange(FLAGS.max_max_epoch):\r\n            global_step = sess.run(model.global_step)\r\n            print(\'Global step %d, lr=%g\' % (global_step, sess.run(learning_rate)))\r\n            if epoch == FLAGS.max_epoch:  # update the embedding\r\n                train_op = train_op1\r\n            train_fetches = [merged, model.loss, train_op, update_op]\r\n            valid_fetches = [merged, model.loss]\r\n            train_epoch(data_train_path, sess, model, train_fetches, valid_fetches, train_writer, test_writer)\r\n        # \xe6\x9c\x80\xe5\x90\x8e\xe5\x86\x8d\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\r\n        valid_cost, precision, recall, f1 = valid_epoch(data_valid_path, sess, model)\r\n        print(\'END.Global_step=%d: valid cost=%g; p=%g, r=%g, f1=%g\' % (\r\n            sess.run(model.global_step), valid_cost, precision, recall, f1))\r\n        if f1 > last_f1:  # save the better model\r\n            saving_path = model.saver.save(sess, model_path, sess.run(model.global_step)+1)\r\n            print(\'saved new model to %s \' % saving_path)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    tf.app.run()\r\n'"
models/wd_3_bigru/__init__.py,0,b'# -*- coding:utf-8 -*- \r\n\r\n'
models/wd_3_bigru/network.py,59,"b'# -*- coding:utf-8 -*-\n\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\nimport tensorflow.contrib.layers as layers\n\n""""""wd_3_bigru\ntitle \xe9\x83\xa8\xe5\x88\x86\xe4\xbd\xbf\xe7\x94\xa8 bigru+attention\xef\xbc\x9bcontent \xe9\x83\xa8\xe5\x88\x86\xe4\xbd\xbf\xe7\x94\xa8 bigru+attention\xef\xbc\x9b \xe4\xb8\xa4\xe9\x83\xa8\xe5\x88\x86\xe8\xbe\x93\xe5\x87\xba\xe7\x9b\xb4\xe6\x8e\xa5 concat\xe3\x80\x82\n""""""\n\n\nclass Settings(object):\n    def __init__(self):\n        self.model_name = \'wd_3_bigru\'\n        self.title_len = 30\n        self.content_len = 150\n        self.hidden_size = 256\n        self.n_layer = 1\n        self.fc_hidden_size = 1024\n        self.n_class = 1999\n        self.summary_path = \'../../summary/\' + self.model_name + \'/\'\n        self.ckpt_path = \'../../ckpt/\' + self.model_name + \'/\'\n\n\nclass BiGRU(object):\n    """"""\n    title: inputs->bigru+attention->output_title\n    content: inputs->bigru+attention->output_content\n    concat[output_title, output_content] -> fc+bn+relu -> sigmoid_entropy.\n    """"""\n\n    def __init__(self, W_embedding, settings):\n        self.model_name = settings.model_name\n        self.title_len = settings.title_len\n        self.content_len = settings.content_len\n        self.hidden_size = settings.hidden_size\n        self.n_layer = settings.n_layer\n        self.n_class = settings.n_class\n        self.fc_hidden_size = settings.fc_hidden_size\n        self._global_step = tf.Variable(0, trainable=False, name=\'Global_Step\')\n        self.update_emas = list()\n        # placeholders\n        self._tst = tf.placeholder(tf.bool)\n        self._keep_prob = tf.placeholder(tf.float32, [])\n        self._batch_size = tf.placeholder(tf.int32, [])\n\n        with tf.name_scope(\'Inputs\'):\n            self._X1_inputs = tf.placeholder(tf.int64, [None, self.title_len], name=\'X1_inputs\')\n            self._X2_inputs = tf.placeholder(tf.int64, [None, self.content_len], name=\'X2_inputs\')\n            self._y_inputs = tf.placeholder(tf.float32, [None, self.n_class], name=\'y_input\')\n\n        with tf.variable_scope(\'embedding\'):\n            self.embedding = tf.get_variable(name=\'embedding\', shape=W_embedding.shape,\n                                             initializer=tf.constant_initializer(W_embedding), trainable=True)\n        self.embedding_size = W_embedding.shape[1]\n\n        with tf.variable_scope(\'bigru_text\'):\n            output_title = self.bigru_inference(self._X1_inputs)\n\n        with tf.variable_scope(\'bigru_content\'):\n            output_content = self.bigru_inference(self._X2_inputs)\n\n        with tf.variable_scope(\'fc-bn-layer\'):\n            output = tf.concat([output_title, output_content], axis=1)\n            W_fc = self.weight_variable([self.hidden_size * 4, self.fc_hidden_size], name=\'Weight_fc\')\n            tf.summary.histogram(\'W_fc\', W_fc)\n            h_fc = tf.matmul(output, W_fc, name=\'h_fc\')\n            beta_fc = tf.Variable(tf.constant(0.1, tf.float32, shape=[self.fc_hidden_size], name=""beta_fc""))\n            tf.summary.histogram(\'beta_fc\', beta_fc)\n            fc_bn, update_ema_fc = self.batchnorm(h_fc, beta_fc, convolutional=False)\n            self.update_emas.append(update_ema_fc)\n            self.fc_bn_relu = tf.nn.relu(fc_bn, name=""relu"")\n\n        with tf.variable_scope(\'out_layer\'):\n            W_out = self.weight_variable([self.fc_hidden_size, self.n_class], name=\'Weight_out\')\n            tf.summary.histogram(\'Weight_out\', W_out)\n            b_out = self.bias_variable([self.n_class], name=\'bias_out\')\n            tf.summary.histogram(\'bias_out\', b_out)\n            self._y_pred = tf.nn.xw_plus_b(self.fc_bn_relu, W_out, b_out, name=\'y_pred\')  # \xe6\xaf\x8f\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe5\x88\x86\xe6\x95\xb0 scores\n\n        with tf.name_scope(\'loss\'):\n            self._loss = tf.reduce_mean(\n                tf.nn.sigmoid_cross_entropy_with_logits(logits=self._y_pred, labels=self._y_inputs))\n            tf.summary.scalar(\'loss\', self._loss)\n\n        self.saver = tf.train.Saver(max_to_keep=1)\n\n    @property\n    def tst(self):\n        return self._tst\n\n    @property\n    def keep_prob(self):\n        return self._keep_prob\n\n    @property\n    def batch_size(self):\n        return self._batch_size\n\n    @property\n    def global_step(self):\n        return self._global_step\n\n    @property\n    def X1_inputs(self):\n        return self._X1_inputs\n\n    @property\n    def X2_inputs(self):\n        return self._X2_inputs\n\n    @property\n    def y_inputs(self):\n        return self._y_inputs\n\n    @property\n    def y_pred(self):\n        return self._y_pred\n\n    @property\n    def loss(self):\n        return self._loss\n\n    def weight_variable(self, shape, name):\n        """"""Create a weight variable with appropriate initialization.""""""\n        initial = tf.truncated_normal(shape, stddev=0.1)\n        return tf.Variable(initial, name=name)\n\n    def bias_variable(self, shape, name):\n        """"""Create a bias variable with appropriate initialization.""""""\n        initial = tf.constant(0.1, shape=shape)\n        return tf.Variable(initial, name=name)\n\n    def batchnorm(self, Ylogits, offset, convolutional=False):\n        """"""batchnormalization.\n        Args:\n            Ylogits: 1D\xe5\x90\x91\xe9\x87\x8f\xe6\x88\x96\xe8\x80\x85\xe6\x98\xaf3D\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\n            num_updates: \xe8\xbf\xad\xe4\xbb\xa3\xe7\x9a\x84global_step\n            offset\xef\xbc\x9a\xe8\xa1\xa8\xe7\xa4\xbabeta\xef\xbc\x8c\xe5\x85\xa8\xe5\xb1\x80\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x9b\xe5\x9c\xa8 RELU \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe4\xb8\x80\xe8\x88\xac\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xba 0.1\xe3\x80\x82\n            scale\xef\xbc\x9a\xe8\xa1\xa8\xe7\xa4\xbalambda\xef\xbc\x8c\xe5\x85\xa8\xe5\xb1\x80\xe6\x96\xb9\xe5\xb7\xae\xef\xbc\x9b\xe5\x9c\xa8 sigmoid \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe9\x9c\x80\xe8\xa6\x81\xef\xbc\x8c\xe8\xbf\x99 RELU \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe4\xbd\x9c\xe7\x94\xa8\xe4\xb8\x8d\xe5\xa4\xa7\xe3\x80\x82\n            m: \xe8\xa1\xa8\xe7\xa4\xbabatch\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x9bv:\xe8\xa1\xa8\xe7\xa4\xbabatch\xe6\x96\xb9\xe5\xb7\xae\xe3\x80\x82\n            bnepsilon\xef\xbc\x9a\xe4\xb8\x80\xe4\xb8\xaa\xe5\xbe\x88\xe5\xb0\x8f\xe7\x9a\x84\xe6\xb5\xae\xe7\x82\xb9\xe6\x95\xb0\xef\xbc\x8c\xe9\x98\xb2\xe6\xad\xa2\xe9\x99\xa4\xe4\xbb\xa5 0.\n        Returns:\n            Ybn: \xe5\x92\x8c Ylogits \xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf\xe7\xbb\x8f\xe8\xbf\x87 Batch Normalization \xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\n            update_moving_everages\xef\xbc\x9a\xe6\x9b\xb4\xe6\x96\xb0mean\xe5\x92\x8cvariance\xef\xbc\x8c\xe4\xb8\xbb\xe8\xa6\x81\xe6\x98\xaf\xe7\xbb\x99\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84 test \xe4\xbd\xbf\xe7\x94\xa8\xe3\x80\x82\n        """"""\n        exp_moving_avg = tf.train.ExponentialMovingAverage(0.999, self._global_step)  # adding the iteration prevents from averaging across non-existing iterations\n        bnepsilon = 1e-5\n        if convolutional:\n            mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])\n        else:\n            mean, variance = tf.nn.moments(Ylogits, [0])\n        update_moving_everages = exp_moving_avg.apply([mean, variance])\n        m = tf.cond(self.tst, lambda: exp_moving_avg.average(mean), lambda: mean)\n        v = tf.cond(self.tst, lambda: exp_moving_avg.average(variance), lambda: variance)\n        Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n        return Ybn, update_moving_everages\n\n    def gru_cell(self):\n        with tf.name_scope(\'gru_cell\'):\n            cell = rnn.GRUCell(self.hidden_size, reuse=tf.get_variable_scope().reuse)\n        return rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n\n    def bi_gru(self, inputs):\n        """"""build the bi-GRU network. \xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\xaa\xe6\x89\x80\xe6\x9c\x89\xe5\xb1\x82\xe7\x9a\x84\xe9\x9a\x90\xe5\x90\xab\xe7\x8a\xb6\xe6\x80\x81\xe3\x80\x82""""""\n        cells_fw = [self.gru_cell() for _ in range(self.n_layer)]\n        cells_bw = [self.gru_cell() for _ in range(self.n_layer)]\n        initial_states_fw = [cell_fw.zero_state(self.batch_size, tf.float32) for cell_fw in cells_fw]\n        initial_states_bw = [cell_bw.zero_state(self.batch_size, tf.float32) for cell_bw in cells_bw]\n        outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(cells_fw, cells_bw, inputs,\n                                                            initial_states_fw=initial_states_fw,\n                                                            initial_states_bw=initial_states_bw, dtype=tf.float32)\n        return outputs\n\n    def task_specific_attention(self, inputs, output_size,\n                                initializer=layers.xavier_initializer(),\n                                activation_fn=tf.tanh, scope=None):\n        """"""\n        Performs task-specific attention reduction, using learned\n        attention context vector (constant within task of interest).\n        Args:\n            inputs: Tensor of shape [batch_size, units, input_size]\n                `input_size` must be static (known)\n                `units` axis will be attended over (reduced from output)\n                `batch_size` will be preserved\n            output_size: Size of output\'s inner (feature) dimension\n        Returns:\n           outputs: Tensor of shape [batch_size, output_dim].\n        """"""\n        assert len(inputs.get_shape()) == 3 and inputs.get_shape()[-1].value is not None\n        with tf.variable_scope(scope or \'attention\') as scope:\n            # u_w, attention \xe5\x90\x91\xe9\x87\x8f\n            attention_context_vector = tf.get_variable(name=\'attention_context_vector\', shape=[output_size],\n                                                       initializer=initializer, dtype=tf.float32)\n            # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c\xe6\x8a\x8a h_i \xe8\xbd\xac\xe4\xb8\xba u_i \xef\xbc\x8c shape= [batch_size, units, input_size] -> [batch_size, units, output_size]\n            input_projection = layers.fully_connected(inputs, output_size, activation_fn=activation_fn, scope=scope)\n            # \xe8\xbe\x93\xe5\x87\xba [batch_size, units]\n            vector_attn = tf.reduce_sum(tf.multiply(input_projection, attention_context_vector), axis=2, keep_dims=True)\n            attention_weights = tf.nn.softmax(vector_attn, dim=1)\n            tf.summary.histogram(\'attention_weigths\', attention_weights)\n            weighted_projection = tf.multiply(inputs, attention_weights)\n            outputs = tf.reduce_sum(weighted_projection, axis=1)\n            return outputs  # \xe8\xbe\x93\xe5\x87\xba [batch_size, hidden_size*2]\n\n    def bigru_inference(self, X_inputs):\n        inputs = tf.nn.embedding_lookup(self.embedding, X_inputs)\n        output_bigru = self.bi_gru(inputs)\n        output_att = self.task_specific_attention(output_bigru, self.hidden_size*2)\n        return output_att\n\n\n# test the model\ndef test():\n    import numpy as np\n    print(\'Begin testing...\')\n    settings = Settings()\n    W_embedding = np.random.randn(50, 10)\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    batch_size = 128\n    with tf.Session(config=config) as sess:\n        model = BiGRU(W_embedding, settings)\n        optimizer = tf.train.AdamOptimizer(0.001)\n        train_op = optimizer.minimize(model.loss)\n        update_op = tf.group(*model.update_emas)\n        sess.run(tf.global_variables_initializer())\n        fetch = [model.loss, model.y_pred, train_op, update_op]\n        loss_list = list()\n        for i in xrange(100):\n            X1_batch = np.zeros((batch_size, 30), dtype=float)\n            X2_batch = np.zeros((batch_size, 150), dtype=float)\n            y_batch = np.zeros((batch_size, 1999), dtype=int)\n            _batch_size = len(y_batch)\n            feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\n                         model.batch_size: _batch_size, model.tst: False, model.keep_prob: 0.5}\n            loss, y_pred, _, _ = sess.run(fetch, feed_dict=feed_dict)\n            loss_list.append(loss)\n            print(i, loss)\n\nif __name__ == \'__main__\':\n    test()\n'"
models/wd_3_bigru/predict.py,4,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport os\r\nimport sys\r\nimport time\r\nimport network\r\n\r\nsys.path.append(\'../..\')\r\nfrom evaluator import score_eval\r\n\r\nsettings = network.Settings()\r\ntitle_len = settings.title_len\r\nmodel_name = settings.model_name\r\nckpt_path = settings.ckpt_path\r\n\r\nlocal_scores_path = \'../../local_scores/\'\r\nscores_path = \'../../scores/\'\r\nif not os.path.exists(local_scores_path):\r\n    os.makedirs(local_scores_path)\r\nif not os.path.exists(scores_path):\r\n    os.makedirs(scores_path)\r\n\r\nembedding_path = \'../../data/word_embedding.npy\'\r\ndata_valid_path = \'../../data/wd-data/data_valid/\'\r\ndata_test_path = \'../../data/wd-data/data_test/\'\r\nva_batches = os.listdir(data_valid_path)\r\nte_batches = os.listdir(data_test_path)  # batch \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8\r\nn_va_batches = len(va_batches)\r\nn_te_batches = len(te_batches)\r\n\r\n\r\ndef get_batch(batch_id):\r\n    """"""get a batch from valid data""""""\r\n    new_batch = np.load(data_valid_path + str(batch_id) + \'.npz\')\r\n    X_batch = new_batch[\'X\']\r\n    y_batch = new_batch[\'y\']\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch, y_batch]\r\n\r\n\r\ndef get_test_batch(batch_id):\r\n    """"""get a batch from test data""""""\r\n    X_batch = np.load(data_test_path + str(batch_id) + \'.npy\')\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch]\r\n\r\n\r\ndef local_predict(sess, model):\r\n    """"""Test on the valid data.""""""\r\n    time0 = time.time()\r\n    predict_labels_list = list()  # \xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\r\n    marked_labels_list = list()\r\n    predict_scores = list()\r\n    for i in tqdm(xrange(n_va_batches)):\r\n        [X1_batch, X2_batch, y_batch] = get_batch(i)\r\n        marked_labels_list.extend(y_batch)\r\n        _batch_size = len(X1_batch)\r\n        fetches = [model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        predict_labels = sess.run(fetches, feed_dict)[0]\r\n        predict_scores.append(predict_labels)\r\n        predict_labels = map(lambda label: label.argsort()[-1:-6:-1], predict_labels)  # \xe5\x8f\x96\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x845\xe4\xb8\xaa\xe4\xb8\x8b\xe6\xa0\x87\r\n        predict_labels_list.extend(predict_labels)\r\n    predict_label_and_marked_label_list = zip(predict_labels_list, marked_labels_list)\r\n    precision, recall, f1 = score_eval(predict_label_and_marked_label_list)\r\n    print(\'Local valid p=%g, r=%g, f1=%g\' % (precision, recall, f1))\r\n    predict_scores = np.vstack(np.asarray(predict_scores))\r\n    local_scores_name = local_scores_path + model_name + \'.npy\'\r\n    np.save(local_scores_name, predict_scores)\r\n    print(\'local_scores.shape=\', predict_scores.shape)\r\n    print(\'Writed the scores into %s, time %g s\' % (local_scores_name, time.time() - time0))\r\n\r\n\r\ndef predict(sess, model):\r\n    """"""Test on the test data.""""""\r\n    time0 = time.time()\r\n    predict_scores = list()\r\n    for i in tqdm(xrange(n_te_batches)):\r\n        [X1_batch, X2_batch] = get_test_batch(i)\r\n        _batch_size = len(X1_batch)\r\n        fetches = [model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        predict_labels = sess.run(fetches, feed_dict)[0]\r\n        predict_scores.append(predict_labels)\r\n    predict_scores = np.vstack(np.asarray(predict_scores))\r\n    scores_name = scores_path + model_name + \'.npy\'\r\n    np.save(scores_name, predict_scores)\r\n    print(\'scores.shape=\', predict_scores.shape)\r\n    print(\'Writed the scores into %s, time %g s\' % (scores_name, time.time() - time0))\r\n\r\n\r\ndef main(_):\r\n    if not os.path.exists(ckpt_path + \'checkpoint\'):\r\n        print(\'there is not saved model, please check the ckpt path\')\r\n        exit()\r\n    print(\'Loading model...\')\r\n    W_embedding = np.load(embedding_path)\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    with tf.Session(config=config) as sess:\r\n        model = network.BiGRU(W_embedding, settings)\r\n        model.saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\r\n        print(\'Local predicting...\')\r\n        local_predict(sess, model)\r\n        print(\'Test predicting...\')\r\n        predict(sess, model)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    tf.app.run()\r\n'"
models/wd_3_bigru/train.py,21,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport os\r\nimport sys\r\nimport shutil\r\nimport time\r\nimport network\r\n\r\nsys.path.append(\'../..\')\r\nfrom data_helpers import to_categorical\r\nfrom evaluator import score_eval\r\n\r\nflags = tf.flags\r\nflags.DEFINE_bool(\'is_retrain\', False, \'if is_retrain is true, not rebuild the summary\')\r\nflags.DEFINE_integer(\'max_epoch\', 2, \'update the embedding after max_epoch, default: 1\')\r\nflags.DEFINE_integer(\'max_max_epoch\', 6, \'all training epoches, default: 6\')\r\nflags.DEFINE_float(\'lr\', 8e-4, \'initial learning rate, default: 8e-4\')\r\nflags.DEFINE_float(\'decay_rate\', 0.85, \'decay rate, default: 0.85\')\r\nflags.DEFINE_float(\'keep_prob\', 0.5, \'keep_prob for training, default: 0.5\')\r\n# \xe6\xad\xa3\xe5\xbc\x8f\r\n# flags.DEFINE_integer(\'decay_step\', 15000, \'decay_step, default: 15000\')\r\n# flags.DEFINE_integer(\'valid_step\', 10000, \'valid_step, default: 10000\')\r\n# flags.DEFINE_float(\'last_f1\', 0.40, \'if valid_f1 > last_f1, save new model. default: 0.40\')\r\n\r\n# \xe6\xb5\x8b\xe8\xaf\x95\r\nflags.DEFINE_integer(\'decay_step\', 1000, \'decay_step, default: 1000\')\r\nflags.DEFINE_integer(\'valid_step\', 500, \'valid_step, default: 500\')\r\nflags.DEFINE_float(\'last_f1\', 0.10, \'if valid_f1 > last_f1, save new model. default: 0.10\')\r\nFLAGS = flags.FLAGS\r\n\r\nlr = FLAGS.lr\r\nlast_f1 = FLAGS.last_f1\r\nsettings = network.Settings()\r\ntitle_len = settings.title_len\r\nsummary_path = settings.summary_path\r\nckpt_path = settings.ckpt_path\r\nmodel_path = ckpt_path + \'model.ckpt\'\r\n\r\nembedding_path = \'../../data/word_embedding.npy\'\r\ndata_train_path = \'../../data/wd-data/data_train/\'\r\ndata_valid_path = \'../../data/wd-data/data_valid/\'\r\ntr_batches = os.listdir(data_train_path)  # batch \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8\r\nva_batches = os.listdir(data_valid_path)\r\nn_tr_batches = len(tr_batches)\r\nn_va_batches = len(va_batches)\r\n\r\n# \xe6\xb5\x8b\xe8\xaf\x95\r\nn_tr_batches = 1000\r\nn_va_batches = 50\r\n\r\n\r\ndef get_batch(data_path, batch_id):\r\n    """"""get a batch from data_path""""""\r\n    new_batch = np.load(data_path + str(batch_id) + \'.npz\')\r\n    X_batch = new_batch[\'X\']\r\n    y_batch = new_batch[\'y\']\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch, y_batch]\r\n\r\n\r\ndef valid_epoch(data_path, sess, model):\r\n    """"""Test on the valid data.""""""\r\n    va_batches = os.listdir(data_path)\r\n    n_va_batches = len(va_batches)\r\n    _costs = 0.0\r\n    predict_labels_list = list()  # \xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\r\n    marked_labels_list = list()\r\n    for i in xrange(n_va_batches):\r\n        [X1_batch, X2_batch, y_batch] = get_batch(data_path, i)\r\n        marked_labels_list.extend(y_batch)\r\n        y_batch = to_categorical(y_batch)\r\n        _batch_size = len(y_batch)\r\n        fetches = [model.loss, model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        _cost, predict_labels = sess.run(fetches, feed_dict)\r\n        _costs += _cost\r\n        predict_labels = map(lambda label: label.argsort()[-1:-6:-1], predict_labels)  # \xe5\x8f\x96\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x845\xe4\xb8\xaa\xe4\xb8\x8b\xe6\xa0\x87\r\n        predict_labels_list.extend(predict_labels)\r\n    predict_label_and_marked_label_list = zip(predict_labels_list, marked_labels_list)\r\n    precision, recall, f1 = score_eval(predict_label_and_marked_label_list)\r\n    mean_cost = _costs / n_va_batches\r\n    return mean_cost, precision, recall, f1\r\n\r\n\r\ndef train_epoch(data_path, sess, model, train_fetches, valid_fetches, train_writer, test_writer):\r\n    global last_f1\r\n    global lr\r\n    time0 = time.time()\r\n    batch_indexs = np.random.permutation(n_tr_batches)  # shuffle the training data\r\n    for batch in tqdm(xrange(n_tr_batches)):\r\n        global_step = sess.run(model.global_step)\r\n        if 0 == (global_step + 1) % FLAGS.valid_step:\r\n            valid_cost, precision, recall, f1 = valid_epoch(data_valid_path, sess, model)\r\n            print(\'Global_step=%d: valid cost=%g; p=%g, r=%g, f1=%g, time=%g s\' % (\r\n                global_step, valid_cost, precision, recall, f1, time.time() - time0))\r\n            time0 = time.time()\r\n            if f1 > last_f1:\r\n                last_f1 = f1\r\n                saving_path = model.saver.save(sess, model_path, global_step+1)\r\n                print(\'saved new model to %s \' % saving_path)\r\n        # training\r\n        batch_id = batch_indexs[batch]\r\n        [X1_batch, X2_batch, y_batch] = get_batch(data_train_path, batch_id)\r\n        y_batch = to_categorical(y_batch)\r\n        _batch_size = len(y_batch)\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                     model.batch_size: _batch_size, model.tst: False, model.keep_prob: FLAGS.keep_prob}\r\n        summary, _cost, _, _ = sess.run(train_fetches, feed_dict)  # the cost is the mean cost of one batch\r\n        # valid per 500 steps\r\n        if 0 == (global_step + 1) % 500:\r\n            train_writer.add_summary(summary, global_step)\r\n            batch_id = np.random.randint(0, n_va_batches)  # \xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe4\xb8\x80\xe4\xb8\xaa\xe9\xaa\x8c\xe8\xaf\x81batch\r\n            [X1_batch, X2_batch, y_batch] = get_batch(data_valid_path, batch_id)\r\n            y_batch = to_categorical(y_batch)\r\n            _batch_size = len(y_batch)\r\n            feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                         model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n            summary, _cost = sess.run(valid_fetches, feed_dict)\r\n            test_writer.add_summary(summary, global_step)\r\n\r\n\r\ndef main(_):\r\n    global ckpt_path\r\n    global last_f1\r\n    if not os.path.exists(ckpt_path):\r\n        os.makedirs(ckpt_path)\r\n    if not os.path.exists(summary_path):\r\n        os.makedirs(summary_path)\r\n    elif not FLAGS.is_retrain:  # \xe9\x87\x8d\xe6\x96\xb0\xe8\xae\xad\xe7\xbb\x83\xe6\x9c\xac\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\x88\xa0\xe9\x99\xa4\xe4\xbb\xa5\xe5\x89\x8d\xe7\x9a\x84 summary\r\n        shutil.rmtree(summary_path)\r\n        os.makedirs(summary_path)\r\n    if not os.path.exists(summary_path):\r\n        os.makedirs(summary_path)\r\n\r\n    print(\'1.Loading data...\')\r\n    W_embedding = np.load(embedding_path)\r\n    print(\'training sample_num = %d\' % n_tr_batches)\r\n    print(\'valid sample_num = %d\' % n_va_batches)\r\n\r\n    # Initial or restore the model\r\n    print(\'2.Building model...\')\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    with tf.Session(config=config) as sess:\r\n        model = network.BiGRU(W_embedding, settings)\r\n        with tf.variable_scope(\'training_ops\') as vs:\r\n            learning_rate = tf.train.exponential_decay(FLAGS.lr, model.global_step, FLAGS.decay_step,\r\n                                                   FLAGS.decay_rate, staircase=True)\r\n            # two optimizer: op1, update embedding; op2, do not update embedding.\r\n            with tf.variable_scope(\'Optimizer1\'):\r\n                tvars1 = tf.trainable_variables()\r\n                grads1 = tf.gradients(model.loss, tvars1)\r\n                optimizer1 = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n                train_op1 = optimizer1.apply_gradients(zip(grads1, tvars1),\r\n                                                   global_step=model.global_step)\r\n            with tf.variable_scope(\'Optimizer2\'):\r\n                tvars2 = [tvar for tvar in tvars1 if \'embedding\' not in tvar.name]\r\n                grads2 = tf.gradients(model.loss, tvars2)\r\n                optimizer2 = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n                train_op2 = optimizer2.apply_gradients(zip(grads2, tvars2),\r\n                                                   global_step=model.global_step)\r\n            update_op = tf.group(*model.update_emas)\r\n            merged = tf.summary.merge_all()  # summary\r\n            train_writer = tf.summary.FileWriter(summary_path + \'train\', sess.graph)\r\n            test_writer = tf.summary.FileWriter(summary_path + \'test\')\r\n            training_ops = [v for v in tf.global_variables() if v.name.startswith(vs.name+\'/\')]\r\n\r\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe5\xb7\xb2\xe7\xbb\x8f\xe4\xbf\x9d\xe5\xad\x98\xe8\xbf\x87\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\xaf\xbc\xe5\x85\xa5\xe4\xb8\x8a\xe6\xac\xa1\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\r\n        if os.path.exists(ckpt_path + ""checkpoint""):\r\n            print(""Restoring Variables from Checkpoint..."")\r\n            model.saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\r\n            last_valid_cost, precision, recall, last_f1 = valid_epoch(data_valid_path, sess, model)\r\n            print(\' valid cost=%g; p=%g, r=%g, f1=%g\' % (last_valid_cost, precision, recall, last_f1))\r\n            sess.run(tf.variables_initializer(training_ops))\r\n            train_op2 = train_op1\r\n        else:\r\n            print(\'Initializing Variables...\')\r\n            sess.run(tf.global_variables_initializer())\r\n\r\n        print(\'3.Begin training...\')\r\n\r\n        train_op = train_op2\r\n        print(\'max_epoch=%d, max_max_epoch=%d\' % (FLAGS.max_epoch, FLAGS.max_max_epoch))\r\n        for epoch in xrange(FLAGS.max_max_epoch):\r\n            global_step = sess.run(model.global_step)\r\n            print(\'Global step %d, lr=%g\' % (global_step, sess.run(learning_rate)))\r\n            if epoch == FLAGS.max_epoch:  # update the embedding\r\n                train_op = train_op1\r\n            train_fetches = [merged, model.loss, train_op, update_op]\r\n            valid_fetches = [merged, model.loss]\r\n            train_epoch(data_train_path, sess, model, train_fetches, valid_fetches, train_writer, test_writer)\r\n        # \xe6\x9c\x80\xe5\x90\x8e\xe5\x86\x8d\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\r\n        valid_cost, precision, recall, f1 = valid_epoch(data_valid_path, sess, model)\r\n        print(\'END.Global_step=%d: valid cost=%g; p=%g, r=%g, f1=%g\' % (\r\n            sess.run(model.global_step), valid_cost, precision, recall, f1))\r\n        if f1 > last_f1:  # save the better model\r\n            saving_path = model.saver.save(sess, model_path, sess.run(model.global_step)+1)\r\n            print(\'saved new model to %s \' % saving_path)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    tf.app.run()\r\n'"
models/wd_4_han/__init__.py,0,b'# -*- coding:utf-8 -*- \r\n\r\n'
models/wd_4_han/network.py,67,"b'# -*- coding:utf-8 -*-\n\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\nimport tensorflow.contrib.layers as layers\n\n""""""wd_4_han\ntitle \xe9\x83\xa8\xe5\x88\x86\xe4\xbd\xbf\xe7\x94\xa8 bigru+attention\xef\xbc\x9bcontent \xe9\x83\xa8\xe5\x88\x86\xe4\xbd\xbf\xe7\x94\xa8 han\xef\xbc\x9b \xe4\xb8\xa4\xe9\x83\xa8\xe5\x88\x86\xe8\xbe\x93\xe5\x87\xba\xe7\x9b\xb4\xe6\x8e\xa5 concat\xe3\x80\x82\n""""""\n\n\nclass Settings(object):\n    def __init__(self):\n        self.model_name = \'wd_4_han\'\n        self.title_len = self.sent_len = 30\n        self.doc_len = 10\n        self.hidden_size = 256\n        self.n_layer = 1\n        self.fc_hidden_size = 1024\n        self.n_class = 1999\n        self.summary_path = \'../../summary/\' + self.model_name + \'/\'\n        self.ckpt_path = \'../../ckpt/\' + self.model_name + \'/\'\n\n\nclass HAN(object):\n    """"""\n    title: inputs->bigru+attention->output_title\n    content: inputs->sent_encoder(bigru+attention)->doc_encoder(bigru+attention)->output_content\n    concat[output_title, output_content] -> fc+bn+relu -> sigmoid_entropy.\n    """"""\n\n    def __init__(self, W_embedding, settings):\n        self.model_name = settings.model_name\n        self.title_len = self.sent_len = settings.sent_len\n        self.doc_len = settings.doc_len\n        self.hidden_size = settings.hidden_size\n        self.n_layer = settings.n_layer\n        self.n_class = settings.n_class\n        self.fc_hidden_size = settings.fc_hidden_size\n        self._global_step = tf.Variable(0, trainable=False, name=\'Global_Step\')\n        self.update_emas = list()\n        # placeholders\n        self._tst = tf.placeholder(tf.bool)\n        self._keep_prob = tf.placeholder(tf.float32, [])\n        self._batch_size = tf.placeholder(tf.int32, [])\n\n        with tf.name_scope(\'Inputs\'):\n            self._X1_inputs = tf.placeholder(tf.int64, [None, self.title_len], name=\'X1_inputs\')\n            self._X2_inputs = tf.placeholder(tf.int64, [None, self.doc_len * self.sent_len], name=\'X2_inputs\')\n            self._y_inputs = tf.placeholder(tf.float32, [None, self.n_class], name=\'y_input\')\n\n        with tf.variable_scope(\'embedding\'):\n            self.embedding = tf.get_variable(name=\'embedding\', shape=W_embedding.shape,\n                                             initializer=tf.constant_initializer(W_embedding), trainable=True)\n        self.embedding_size = W_embedding.shape[1]\n\n        with tf.variable_scope(\'bigru_text\'):\n            output_title = self.bigru_inference(self._X1_inputs)\n\n        with tf.variable_scope(\'han_content\'):\n            output_content = self.han_inference(self._X2_inputs)\n\n        with tf.variable_scope(\'fc-bn-layer\'):\n            output = tf.concat([output_title, output_content], axis=1)\n            W_fc = self.weight_variable([self.hidden_size * 4, self.fc_hidden_size], name=\'Weight_fc\')\n            tf.summary.histogram(\'W_fc\', W_fc)\n            h_fc = tf.matmul(output, W_fc, name=\'h_fc\')\n            beta_fc = tf.Variable(tf.constant(0.1, tf.float32, shape=[self.fc_hidden_size], name=""beta_fc""))\n            tf.summary.histogram(\'beta_fc\', beta_fc)\n            fc_bn, update_ema_fc = self.batchnorm(h_fc, beta_fc, convolutional=False)\n            self.update_emas.append(update_ema_fc)\n            self.fc_bn_relu = tf.nn.relu(fc_bn, name=""relu"")\n            fc_bn_drop = tf.nn.dropout(self.fc_bn_relu, self.keep_prob)\n\n        with tf.variable_scope(\'out_layer\'):\n            W_out = self.weight_variable([self.fc_hidden_size, self.n_class], name=\'Weight_out\')\n            tf.summary.histogram(\'Weight_out\', W_out)\n            b_out = self.bias_variable([self.n_class], name=\'bias_out\')\n            tf.summary.histogram(\'bias_out\', b_out)\n            self._y_pred = tf.nn.xw_plus_b(fc_bn_drop, W_out, b_out, name=\'y_pred\')  # \xe6\xaf\x8f\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe5\x88\x86\xe6\x95\xb0 scores\n\n        with tf.name_scope(\'loss\'):\n            self._loss = tf.reduce_mean(\n                tf.nn.sigmoid_cross_entropy_with_logits(logits=self._y_pred, labels=self._y_inputs))\n            tf.summary.scalar(\'loss\', self._loss)\n\n        self.saver = tf.train.Saver(max_to_keep=1)\n\n    @property\n    def tst(self):\n        return self._tst\n\n    @property\n    def keep_prob(self):\n        return self._keep_prob\n\n    @property\n    def batch_size(self):\n        return self._batch_size\n\n    @property\n    def global_step(self):\n        return self._global_step\n\n    @property\n    def X1_inputs(self):\n        return self._X1_inputs\n\n    @property\n    def X2_inputs(self):\n        return self._X2_inputs\n\n    @property\n    def y_inputs(self):\n        return self._y_inputs\n\n    @property\n    def y_pred(self):\n        return self._y_pred\n\n    @property\n    def loss(self):\n        return self._loss\n\n    def weight_variable(self, shape, name):\n        """"""Create a weight variable with appropriate initialization.""""""\n        initial = tf.truncated_normal(shape, stddev=0.1)\n        return tf.Variable(initial, name=name)\n\n    def bias_variable(self, shape, name):\n        """"""Create a bias variable with appropriate initialization.""""""\n        initial = tf.constant(0.1, shape=shape)\n        return tf.Variable(initial, name=name)\n\n    def batchnorm(self, Ylogits, offset, convolutional=False):\n        """"""batchnormalization.\n        Args:\n            Ylogits: 1D\xe5\x90\x91\xe9\x87\x8f\xe6\x88\x96\xe8\x80\x85\xe6\x98\xaf3D\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\n            num_updates: \xe8\xbf\xad\xe4\xbb\xa3\xe7\x9a\x84global_step\n            offset\xef\xbc\x9a\xe8\xa1\xa8\xe7\xa4\xbabeta\xef\xbc\x8c\xe5\x85\xa8\xe5\xb1\x80\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x9b\xe5\x9c\xa8 RELU \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe4\xb8\x80\xe8\x88\xac\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xba 0.1\xe3\x80\x82\n            scale\xef\xbc\x9a\xe8\xa1\xa8\xe7\xa4\xbalambda\xef\xbc\x8c\xe5\x85\xa8\xe5\xb1\x80\xe6\x96\xb9\xe5\xb7\xae\xef\xbc\x9b\xe5\x9c\xa8 sigmoid \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe9\x9c\x80\xe8\xa6\x81\xef\xbc\x8c\xe8\xbf\x99 RELU \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe4\xbd\x9c\xe7\x94\xa8\xe4\xb8\x8d\xe5\xa4\xa7\xe3\x80\x82\n            m: \xe8\xa1\xa8\xe7\xa4\xbabatch\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x9bv:\xe8\xa1\xa8\xe7\xa4\xbabatch\xe6\x96\xb9\xe5\xb7\xae\xe3\x80\x82\n            bnepsilon\xef\xbc\x9a\xe4\xb8\x80\xe4\xb8\xaa\xe5\xbe\x88\xe5\xb0\x8f\xe7\x9a\x84\xe6\xb5\xae\xe7\x82\xb9\xe6\x95\xb0\xef\xbc\x8c\xe9\x98\xb2\xe6\xad\xa2\xe9\x99\xa4\xe4\xbb\xa5 0.\n        Returns:\n            Ybn: \xe5\x92\x8c Ylogits \xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf\xe7\xbb\x8f\xe8\xbf\x87 Batch Normalization \xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\n            update_moving_everages\xef\xbc\x9a\xe6\x9b\xb4\xe6\x96\xb0mean\xe5\x92\x8cvariance\xef\xbc\x8c\xe4\xb8\xbb\xe8\xa6\x81\xe6\x98\xaf\xe7\xbb\x99\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84 test \xe4\xbd\xbf\xe7\x94\xa8\xe3\x80\x82\n        """"""\n        exp_moving_avg = tf.train.ExponentialMovingAverage(0.999, self._global_step)  # adding the iteration prevents from averaging across non-existing iterations\n        bnepsilon = 1e-5\n        if convolutional:\n            mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])\n        else:\n            mean, variance = tf.nn.moments(Ylogits, [0])\n        update_moving_everages = exp_moving_avg.apply([mean, variance])\n        m = tf.cond(self.tst, lambda: exp_moving_avg.average(mean), lambda: mean)\n        v = tf.cond(self.tst, lambda: exp_moving_avg.average(variance), lambda: variance)\n        Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n        return Ybn, update_moving_everages\n\n    def gru_cell(self):\n        with tf.name_scope(\'gru_cell\'):\n            cell = rnn.GRUCell(self.hidden_size, reuse=tf.get_variable_scope().reuse)\n        return rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n\n    def bi_gru(self, inputs, seg_num):\n        """"""build the bi-GRU network. Return the encoder represented vector.\n        n_step: \xe5\x8f\xa5\xe5\xad\x90\xe7\x9a\x84\xe8\xaf\x8d\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x9b\xe6\x88\x96\xe8\x80\x85\xe6\x96\x87\xe6\xa1\xa3\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xe6\x95\xb0\xe3\x80\x82\n        seg_num: \xe5\xba\x8f\xe5\x88\x97\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x8c\xe5\x8e\x9f\xe6\x9c\xac\xe5\xba\x94\xe8\xaf\xa5\xe4\xb8\xba batch_size, \xe4\xbd\x86\xe6\x98\xaf\xe8\xbf\x99\xe9\x87\x8c\xe5\xb0\x86 batch_size \xe4\xb8\xaa doc\xe5\xb1\x95\xe5\xbc\x80\xe6\x88\x90\xe5\xbe\x88\xe5\xa4\x9a\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe3\x80\x82\n        """"""\n        cells_fw = [self.gru_cell() for _ in range(self.n_layer)]\n        cells_bw = [self.gru_cell() for _ in range(self.n_layer)]\n        initial_states_fw = [cell_fw.zero_state(seg_num, tf.float32) for cell_fw in cells_fw]\n        initial_states_bw = [cell_bw.zero_state(seg_num, tf.float32) for cell_bw in cells_bw]\n        outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(cells_fw, cells_bw, inputs,\n                        initial_states_fw = initial_states_fw, initial_states_bw = initial_states_bw, dtype=tf.float32)\n        # outputs: Output Tensor shaped: seg_num, max_time, layers_output]\xef\xbc\x8c\xe5\x85\xb6\xe4\xb8\xadlayers_output=hidden_size * 2 \xe5\x9c\xa8\xe8\xbf\x99\xe9\x87\x8c\xe3\x80\x82\n        return outputs\n\n    def task_specific_attention(self, inputs, output_size,\n                                initializer=layers.xavier_initializer(),\n                                activation_fn=tf.tanh, scope=None):\n        """"""\n        Performs task-specific attention reduction, using learned\n        attention context vector (constant within task of interest).\n        Args:\n            inputs: Tensor of shape [batch_size, units, input_size]\n                `input_size` must be static (known)\n                `units` axis will be attended over (reduced from output)\n                `batch_size` will be preserved\n            output_size: Size of output\'s inner (feature) dimension\n        Returns:\n           outputs: Tensor of shape [batch_size, output_dim].\n        """"""\n        assert len(inputs.get_shape()) == 3 and inputs.get_shape()[-1].value is not None\n        with tf.variable_scope(scope or \'attention\') as scope:\n            # u_w, attention \xe5\x90\x91\xe9\x87\x8f\n            attention_context_vector = tf.get_variable(name=\'attention_context_vector\', shape=[output_size],\n                                                       initializer=initializer, dtype=tf.float32)\n            # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c\xe6\x8a\x8a h_i \xe8\xbd\xac\xe4\xb8\xba u_i \xef\xbc\x8c shape= [batch_size, units, input_size] -> [batch_size, units, output_size]\n            input_projection = layers.fully_connected(inputs, output_size, activation_fn=activation_fn, scope=scope)\n            # \xe8\xbe\x93\xe5\x87\xba [batch_size, units]\n            vector_attn = tf.reduce_sum(tf.multiply(input_projection, attention_context_vector), axis=2, keep_dims=True)\n            attention_weights = tf.nn.softmax(vector_attn, dim=1)\n            tf.summary.histogram(\'attention_weigths\', attention_weights)\n            weighted_projection = tf.multiply(inputs, attention_weights)\n            outputs = tf.reduce_sum(weighted_projection, axis=1)\n            return outputs  # \xe8\xbe\x93\xe5\x87\xba [batch_size, hidden_size*2]\n\n    def bigru_inference(self, X_inputs):\n        inputs = tf.nn.embedding_lookup(self.embedding, X_inputs)\n        output_bigru = self.bi_gru(inputs, self.batch_size)\n        output_att = self.task_specific_attention(output_bigru, self.hidden_size*2)\n        return output_att   # \xe8\xbe\x93\xe5\x87\xba [batch_size, hidden_size*2]\n\n    def han_inference(self, X_inputs):\n        """"""\xe5\x88\x86\xe5\xb1\x82 attention \xe6\xa8\xa1\xe5\x9e\x8b\xe3\x80\x82content\xe9\x83\xa8\xe5\x88\x86\xe3\x80\x82\n        Args:\n            X_inputs: tensor.shape=(batch_size, doc_len*sent_len)\n        Returns:\n            doc_attn_outputs: tensor.shape=(batch_size, hidden_size(*2 for bigru))\n        """"""\n        inputs = tf.nn.embedding_lookup(self.embedding, X_inputs)    # inputs.shape=[batch_size, doc_len*sent_len, embedding_size]\n        sent_inputs = tf.reshape(inputs,[self.batch_size*self.doc_len, self.sent_len, self.embedding_size]) # shape=(?, 40, 256)\n        with tf.variable_scope(\'sentence_encoder\'):  # \xe7\x94\x9f\xe6\x88\x90\xe5\x8f\xa5\xe5\x90\x91\xe9\x87\x8f\n            sent_outputs = self.bi_gru(sent_inputs, seg_num=self.batch_size*self.doc_len)\n            sent_attn_outputs = self.task_specific_attention(sent_outputs, self.hidden_size*2) # [batch_size*doc_len, hidden_size*2]\n            with tf.variable_scope(\'dropout\'):\n                sent_attn_outputs = tf.nn.dropout(sent_attn_outputs, self.keep_prob)\n        with tf.variable_scope(\'doc_encoder\'):      # \xe7\x94\x9f\xe6\x88\x90\xe6\x96\x87\xe6\xa1\xa3\xe5\x90\x91\xe9\x87\x8f\n            doc_inputs = tf.reshape(sent_attn_outputs, [self.batch_size, self.doc_len, self.hidden_size*2])\n            doc_outputs = self.bi_gru(doc_inputs, self.batch_size)  # [batch_size, doc_len, hidden_size*2]\n            doc_attn_outputs = self.task_specific_attention(doc_outputs, self.hidden_size*2) # [batch_size, hidden_size*2]\n        return doc_attn_outputs    # [batch_size, hidden_size*2]\n\n\n\n# test the model\ndef test():\n    import numpy as np\n    print(\'Begin testing...\')\n    settings = Settings()\n    W_embedding = np.random.randn(50, 10)\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    batch_size = 128\n    with tf.Session(config=config) as sess:\n        model = HAN(W_embedding, settings)\n        optimizer = tf.train.AdamOptimizer(0.001)\n        train_op = optimizer.minimize(model.loss)\n        update_op = tf.group(*model.update_emas)\n        sess.run(tf.global_variables_initializer())\n        fetch = [model.loss, model.y_pred, train_op, update_op]\n        loss_list = list()\n        for i in xrange(100):\n            X1_batch = np.zeros((batch_size, 30), dtype=float)\n            X2_batch = np.zeros((batch_size, 10 * 30), dtype=float)\n            y_batch = np.zeros((batch_size, 1999), dtype=int)\n            _batch_size = len(y_batch)\n            feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\n                         model.batch_size: _batch_size, model.tst: False, model.keep_prob: 0.5}\n            loss, y_pred, _, _ = sess.run(fetch, feed_dict=feed_dict)\n            loss_list.append(loss)\n            print(i, loss)\n\nif __name__ == \'__main__\':\n    test()\n'"
models/wd_4_han/predict.py,4,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport os\r\nimport sys\r\nimport time\r\nimport network\r\n\r\nsys.path.append(\'../..\')\r\nfrom evaluator import score_eval\r\n\r\nsettings = network.Settings()\r\ntitle_len = settings.title_len\r\nmodel_name = settings.model_name\r\nckpt_path = settings.ckpt_path\r\n\r\nlocal_scores_path = \'../../local_scores/\'\r\nscores_path = \'../../scores/\'\r\nif not os.path.exists(local_scores_path):\r\n    os.makedirs(local_scores_path)\r\nif not os.path.exists(scores_path):\r\n    os.makedirs(scores_path)\r\n\r\nembedding_path = \'../../data/word_embedding.npy\'\r\ndata_valid_path = \'../../data/wd-data/seg_valid/\'\r\ndata_test_path = \'../../data/wd-data/seg_test/\'\r\nva_batches = os.listdir(data_valid_path)\r\nte_batches = os.listdir(data_test_path)  # batch \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8\r\nn_va_batches = len(va_batches)\r\nn_te_batches = len(te_batches)\r\n\r\n\r\ndef get_batch(batch_id):\r\n    """"""get a batch from valid data""""""\r\n    new_batch = np.load(data_valid_path + str(batch_id) + \'.npz\')\r\n    X_batch = new_batch[\'X\']\r\n    y_batch = new_batch[\'y\']\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch, y_batch]\r\n\r\n\r\ndef get_test_batch(batch_id):\r\n    """"""get a batch from test data""""""\r\n    X_batch = np.load(data_test_path + str(batch_id) + \'.npy\')\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch]\r\n\r\n\r\ndef local_predict(sess, model):\r\n    """"""Test on the valid data.""""""\r\n    time0 = time.time()\r\n    predict_labels_list = list()  # \xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\r\n    marked_labels_list = list()\r\n    predict_scores = list()\r\n    for i in tqdm(xrange(n_va_batches)):\r\n        [X1_batch, X2_batch, y_batch] = get_batch(i)\r\n        marked_labels_list.extend(y_batch)\r\n        _batch_size = len(X1_batch)\r\n        fetches = [model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        predict_labels = sess.run(fetches, feed_dict)[0]\r\n        predict_scores.append(predict_labels)\r\n        predict_labels = map(lambda label: label.argsort()[-1:-6:-1], predict_labels)  # \xe5\x8f\x96\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x845\xe4\xb8\xaa\xe4\xb8\x8b\xe6\xa0\x87\r\n        predict_labels_list.extend(predict_labels)\r\n    predict_label_and_marked_label_list = zip(predict_labels_list, marked_labels_list)\r\n    precision, recall, f1 = score_eval(predict_label_and_marked_label_list)\r\n    print(\'Local valid p=%g, r=%g, f1=%g\' % (precision, recall, f1))\r\n    predict_scores = np.vstack(np.asarray(predict_scores))\r\n    local_scores_name = local_scores_path + model_name + \'.npy\'\r\n    np.save(local_scores_name, predict_scores)\r\n    print(\'local_scores.shape=\', predict_scores.shape)\r\n    print(\'Writed the scores into %s, time %g s\' % (local_scores_name, time.time() - time0))\r\n\r\n\r\ndef predict(sess, model):\r\n    """"""Test on the test data.""""""\r\n    time0 = time.time()\r\n    predict_scores = list()\r\n    for i in tqdm(xrange(n_te_batches)):\r\n        [X1_batch, X2_batch] = get_test_batch(i)\r\n        _batch_size = len(X1_batch)\r\n        fetches = [model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        predict_labels = sess.run(fetches, feed_dict)[0]\r\n        predict_scores.append(predict_labels)\r\n    predict_scores = np.vstack(np.asarray(predict_scores))\r\n    scores_name = scores_path + model_name + \'.npy\'\r\n    np.save(scores_name, predict_scores)\r\n    print(\'scores.shape=\', predict_scores.shape)\r\n    print(\'Writed the scores into %s, time %g s\' % (scores_name, time.time() - time0))\r\n\r\n\r\ndef main(_):\r\n    if not os.path.exists(ckpt_path + \'checkpoint\'):\r\n        print(\'there is not saved model, please check the ckpt path\')\r\n        exit()\r\n    print(\'Loading model...\')\r\n    W_embedding = np.load(embedding_path)\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    with tf.Session(config=config) as sess:\r\n        model = network.HAN(W_embedding, settings)\r\n        model.saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\r\n        print(\'Local predicting...\')\r\n        local_predict(sess, model)\r\n        print(\'Test predicting...\')\r\n        predict(sess, model)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    tf.app.run()\r\n'"
models/wd_4_han/train.py,21,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport os\r\nimport sys\r\nimport shutil\r\nimport time\r\nimport network\r\n\r\nsys.path.append(\'../..\')\r\nfrom data_helpers import to_categorical\r\nfrom evaluator import score_eval\r\n\r\nflags = tf.flags\r\nflags.DEFINE_bool(\'is_retrain\', False, \'if is_retrain is true, not rebuild the summary\')\r\nflags.DEFINE_integer(\'max_epoch\', 2, \'update the embedding after max_epoch, default: 2\')\r\nflags.DEFINE_integer(\'max_max_epoch\', 6, \'all training epoches, default: 6\')\r\nflags.DEFINE_float(\'lr\', 8e-4, \'initial learning rate, default: 8e-4\')\r\nflags.DEFINE_float(\'decay_rate\', 0.85, \'decay rate, default: 0.85\')\r\nflags.DEFINE_float(\'keep_prob\', 0.5, \'keep_prob for training, default: 0.5\')\r\n# \xe6\xad\xa3\xe5\xbc\x8f\r\n# flags.DEFINE_integer(\'decay_step\', 15000, \'decay_step, default: 15000\')\r\n# flags.DEFINE_integer(\'valid_step\', 10000, \'valid_step, default: 10000\')\r\n# flags.DEFINE_float(\'last_f1\', 0.40, \'if valid_f1 > last_f1, save new model. default: 0.40\')\r\n\r\n# \xe6\xb5\x8b\xe8\xaf\x95\r\nflags.DEFINE_integer(\'decay_step\', 1000, \'decay_step, default: 1000\')\r\nflags.DEFINE_integer(\'valid_step\', 500, \'valid_step, default: 500\')\r\nflags.DEFINE_float(\'last_f1\', 0.10, \'if valid_f1 > last_f1, save new model. default: 0.10\')\r\nFLAGS = flags.FLAGS\r\n\r\nlr = FLAGS.lr\r\nlast_f1 = FLAGS.last_f1\r\nsettings = network.Settings()\r\ntitle_len = settings.title_len\r\nsummary_path = settings.summary_path\r\nckpt_path = settings.ckpt_path\r\nmodel_path = ckpt_path + \'model.ckpt\'\r\n\r\nembedding_path = \'../../data/word_embedding.npy\'\r\ndata_train_path = \'../../data/wd-data/seg_train/\'\r\ndata_valid_path = \'../../data/wd-data/seg_valid/\'\r\ntr_batches = os.listdir(data_train_path)  # batch \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8\r\nva_batches = os.listdir(data_valid_path)\r\nn_tr_batches = len(tr_batches)\r\nn_va_batches = len(va_batches)\r\n\r\n# \xe6\xb5\x8b\xe8\xaf\x95\r\nn_tr_batches = 1000\r\nn_va_batches = 50\r\n\r\n\r\ndef get_batch(data_path, batch_id):\r\n    """"""get a batch from data_path""""""\r\n    new_batch = np.load(data_path + str(batch_id) + \'.npz\')\r\n    X_batch = new_batch[\'X\']\r\n    y_batch = new_batch[\'y\']\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch, y_batch]\r\n\r\n\r\ndef valid_epoch(data_path, sess, model):\r\n    """"""Test on the valid data.""""""\r\n    va_batches = os.listdir(data_path)\r\n    n_va_batches = len(va_batches)\r\n    _costs = 0.0\r\n    predict_labels_list = list()  # \xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\r\n    marked_labels_list = list()\r\n    for i in xrange(n_va_batches):\r\n        [X1_batch, X2_batch, y_batch] = get_batch(data_path, i)\r\n        marked_labels_list.extend(y_batch)\r\n        y_batch = to_categorical(y_batch)\r\n        _batch_size = len(y_batch)\r\n        fetches = [model.loss, model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        _cost, predict_labels = sess.run(fetches, feed_dict)\r\n        _costs += _cost\r\n        predict_labels = map(lambda label: label.argsort()[-1:-6:-1], predict_labels)  # \xe5\x8f\x96\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x845\xe4\xb8\xaa\xe4\xb8\x8b\xe6\xa0\x87\r\n        predict_labels_list.extend(predict_labels)\r\n    predict_label_and_marked_label_list = zip(predict_labels_list, marked_labels_list)\r\n    precision, recall, f1 = score_eval(predict_label_and_marked_label_list)\r\n    mean_cost = _costs / n_va_batches\r\n    return mean_cost, precision, recall, f1\r\n\r\n\r\ndef train_epoch(data_path, sess, model, train_fetches, valid_fetches, train_writer, test_writer):\r\n    global last_f1\r\n    global lr\r\n    time0 = time.time()\r\n    batch_indexs = np.random.permutation(n_tr_batches)  # shuffle the training data\r\n    for batch in tqdm(xrange(n_tr_batches)):\r\n        global_step = sess.run(model.global_step)\r\n        if 0 == (global_step + 1) % FLAGS.valid_step:\r\n            valid_cost, precision, recall, f1 = valid_epoch(data_valid_path, sess, model)\r\n            print(\'Global_step=%d: valid cost=%g; p=%g, r=%g, f1=%g, time=%g s\' % (\r\n                global_step, valid_cost, precision, recall, f1, time.time() - time0))\r\n            time0 = time.time()\r\n            if f1 > last_f1:\r\n                last_f1 = f1\r\n                saving_path = model.saver.save(sess, model_path, global_step+1)\r\n                print(\'saved new model to %s \' % saving_path)\r\n        # training\r\n        batch_id = batch_indexs[batch]\r\n        [X1_batch, X2_batch, y_batch] = get_batch(data_train_path, batch_id)\r\n        y_batch = to_categorical(y_batch)\r\n        _batch_size = len(y_batch)\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                     model.batch_size: _batch_size, model.tst: False, model.keep_prob: FLAGS.keep_prob}\r\n        summary, _cost, _, _ = sess.run(train_fetches, feed_dict)  # the cost is the mean cost of one batch\r\n        # valid per 500 steps\r\n        if 0 == (global_step + 1) % 500:\r\n            train_writer.add_summary(summary, global_step)\r\n            batch_id = np.random.randint(0, n_va_batches)  # \xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe4\xb8\x80\xe4\xb8\xaa\xe9\xaa\x8c\xe8\xaf\x81batch\r\n            [X1_batch, X2_batch, y_batch] = get_batch(data_valid_path, batch_id)\r\n            y_batch = to_categorical(y_batch)\r\n            _batch_size = len(y_batch)\r\n            feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                         model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n            summary, _cost = sess.run(valid_fetches, feed_dict)\r\n            test_writer.add_summary(summary, global_step)\r\n\r\n\r\ndef main(_):\r\n    global ckpt_path\r\n    global last_f1\r\n    if not os.path.exists(ckpt_path):\r\n        os.makedirs(ckpt_path)\r\n    if not os.path.exists(summary_path):\r\n        os.makedirs(summary_path)\r\n    elif not FLAGS.is_retrain:  # \xe9\x87\x8d\xe6\x96\xb0\xe8\xae\xad\xe7\xbb\x83\xe6\x9c\xac\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\x88\xa0\xe9\x99\xa4\xe4\xbb\xa5\xe5\x89\x8d\xe7\x9a\x84 summary\r\n        shutil.rmtree(summary_path)\r\n        os.makedirs(summary_path)\r\n    if not os.path.exists(summary_path):\r\n        os.makedirs(summary_path)\r\n\r\n    print(\'1.Loading data...\')\r\n    W_embedding = np.load(embedding_path)\r\n    print(\'training sample_num = %d\' % n_tr_batches)\r\n    print(\'valid sample_num = %d\' % n_va_batches)\r\n\r\n    # Initial or restore the model\r\n    print(\'2.Building model...\')\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    with tf.Session(config=config) as sess:\r\n        model = network.HAN(W_embedding, settings)\r\n        with tf.variable_scope(\'training_ops\') as vs:\r\n            learning_rate = tf.train.exponential_decay(FLAGS.lr, model.global_step, FLAGS.decay_step,\r\n                                                   FLAGS.decay_rate, staircase=True)\r\n            # two optimizer: op1, update embedding; op2, do not update embedding.\r\n            with tf.variable_scope(\'Optimizer1\'):\r\n                tvars1 = tf.trainable_variables()\r\n                grads1 = tf.gradients(model.loss, tvars1)\r\n                optimizer1 = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n                train_op1 = optimizer1.apply_gradients(zip(grads1, tvars1),\r\n                                                   global_step=model.global_step)\r\n            with tf.variable_scope(\'Optimizer2\'):\r\n                tvars2 = [tvar for tvar in tvars1 if \'embedding\' not in tvar.name]\r\n                grads2 = tf.gradients(model.loss, tvars2)\r\n                optimizer2 = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n                train_op2 = optimizer2.apply_gradients(zip(grads2, tvars2),\r\n                                                   global_step=model.global_step)\r\n            update_op = tf.group(*model.update_emas)\r\n            merged = tf.summary.merge_all()  # summary\r\n            train_writer = tf.summary.FileWriter(summary_path + \'train\', sess.graph)\r\n            test_writer = tf.summary.FileWriter(summary_path + \'test\')\r\n            training_ops = [v for v in tf.global_variables() if v.name.startswith(vs.name+\'/\')]\r\n\r\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe5\xb7\xb2\xe7\xbb\x8f\xe4\xbf\x9d\xe5\xad\x98\xe8\xbf\x87\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\xaf\xbc\xe5\x85\xa5\xe4\xb8\x8a\xe6\xac\xa1\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\r\n        if os.path.exists(ckpt_path + ""checkpoint""):\r\n            print(""Restoring Variables from Checkpoint..."")\r\n            model.saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\r\n            last_valid_cost, precision, recall, last_f1 = valid_epoch(data_valid_path, sess, model)\r\n            print(\' valid cost=%g; p=%g, r=%g, f1=%g\' % (last_valid_cost, precision, recall, last_f1))\r\n            sess.run(tf.variables_initializer(training_ops))\r\n            train_op2 = train_op1\r\n        else:\r\n            print(\'Initializing Variables...\')\r\n            sess.run(tf.global_variables_initializer())\r\n\r\n        print(\'3.Begin training...\')\r\n        print(\'max_epoch=%d, max_max_epoch=%d\' % (FLAGS.max_epoch, FLAGS.max_max_epoch))\r\n        train_op = train_op2\r\n        for epoch in xrange(FLAGS.max_max_epoch):\r\n            global_step = sess.run(model.global_step)\r\n            print(\'Global step %d, lr=%g\' % (global_step, sess.run(learning_rate)))\r\n            if epoch == FLAGS.max_epoch:  # update the embedding\r\n                train_op = train_op1\r\n            train_fetches = [merged, model.loss, train_op, update_op]\r\n            valid_fetches = [merged, model.loss]\r\n            train_epoch(data_train_path, sess, model, train_fetches, valid_fetches, train_writer, test_writer)\r\n        # \xe6\x9c\x80\xe5\x90\x8e\xe5\x86\x8d\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\r\n        valid_cost, precision, recall, f1 = valid_epoch(data_valid_path, sess, model)\r\n        print(\'END.Global_step=%d: valid cost=%g; p=%g, r=%g, f1=%g\' % (\r\n            sess.run(model.global_step), valid_cost, precision, recall, f1))\r\n        if f1 > last_f1:  # save the better model\r\n            saving_path = model.saver.save(sess, model_path, sess.run(model.global_step)+1)\r\n            print(\'saved new model to %s \' % saving_path)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    tf.app.run()\r\n'"
models/wd_5_bigru_cnn/__init__.py,0,b'# -*- coding:utf-8 -*- \r\n\r\n'
models/wd_5_bigru_cnn/network.py,71,"b'# -*- coding:utf-8 -*-\n\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\nimport tensorflow.contrib.layers as layers\n\n""""""wd_5_bigru_cnn\n\xe4\xb8\xa4\xe9\x83\xa8\xe5\x88\x86\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84 embedding\xef\xbc\x8c \xe5\x9b\xa0\xe4\xb8\xbaRNN\xe4\xb8\x8eCNN\xe7\xbb\x93\xe6\x9e\x84\xe5\xae\x8c\xe5\x85\xa8\xe4\xb8\x8d\xe5\x90\x8c\xef\xbc\x8c\xe5\x85\xb1\xe7\x94\xa8embedding\xe4\xbc\x9a\xe9\x99\x8d\xe4\xbd\x8e\xe6\x80\xa7\xe8\x83\xbd\xe3\x80\x82\ntitle \xe9\x83\xa8\xe5\x88\x86\xe4\xbd\xbf\xe7\x94\xa8 bigru+attention\xef\xbc\x9bcontent \xe9\x83\xa8\xe5\x88\x86\xe4\xbd\xbf\xe7\x94\xa8 textcnn\xef\xbc\x9b \xe4\xb8\xa4\xe9\x83\xa8\xe5\x88\x86\xe8\xbe\x93\xe5\x87\xba\xe7\x9b\xb4\xe6\x8e\xa5 concat\xe3\x80\x82\n""""""\n\n\nclass Settings(object):\n    def __init__(self):\n        self.model_name = \'wd_5_bigru_cnn\'\n        self.title_len = 30\n        self.content_len = 150\n        self.hidden_size = 256\n        self.n_layer = 1\n        self.filter_sizes = [2, 3, 4, 5, 7]\n        self.n_filter = 256\n        self.fc_hidden_size = 1024\n        self.n_class = 1999\n        self.summary_path = \'../../summary/\' + self.model_name + \'/\'\n        self.ckpt_path = \'../../ckpt/\' + self.model_name + \'/\'\n\n\nclass BiGRU_CNN(object):\n    """"""\n    title: inputs->bigru+attention->output_title\n    content: inputs->textcnn->output_content\n    concat[output_title, output_content] -> fc+bn+relu -> sigmoid_entropy.\n    """"""\n\n    def __init__(self, W_embedding, settings):\n        self.model_name = settings.model_name\n        self.title_len = settings.title_len\n        self.content_len = settings.content_len\n        self.hidden_size = settings.hidden_size\n        self.n_layer = settings.n_layer\n        self.filter_sizes = settings.filter_sizes\n        self.n_filter = settings.n_filter\n        self.n_filter_total = self.n_filter * len(self.filter_sizes)\n        self.n_class = settings.n_class\n        self.fc_hidden_size = settings.fc_hidden_size\n        self._global_step = tf.Variable(0, trainable=False, name=\'Global_Step\')\n        self.update_emas = list()\n        # placeholders\n        self._tst = tf.placeholder(tf.bool)\n        self._keep_prob = tf.placeholder(tf.float32, [])\n        self._batch_size = tf.placeholder(tf.int32, [])\n\n        with tf.name_scope(\'Inputs\'):\n            self._X1_inputs = tf.placeholder(tf.int64, [None, self.title_len], name=\'X1_inputs\')\n            self._X2_inputs = tf.placeholder(tf.int64, [None, self.content_len], name=\'X2_inputs\')\n            self._y_inputs = tf.placeholder(tf.float32, [None, self.n_class], name=\'y_input\')\n\n        with tf.variable_scope(\'embedding\'):\n            self.title_embedding = tf.get_variable(name=\'title_embedding\', shape=W_embedding.shape,\n                                             initializer=tf.constant_initializer(W_embedding), trainable=True)\n            self.content_embedding = tf.get_variable(name=\'content_embedding\', shape=W_embedding.shape,\n                                             initializer=tf.constant_initializer(W_embedding), trainable=True)\n        self.embedding_size = W_embedding.shape[1]\n\n        with tf.variable_scope(\'bigru_text\'):\n            output_title = self.bigru_inference(self._X1_inputs)\n\n        with tf.variable_scope(\'cnn_content\'):\n            output_content = self.cnn_inference(self._X2_inputs, self.content_len)\n\n        with tf.variable_scope(\'fc-bn-layer\'):\n            output = tf.concat([output_title, output_content], axis=1)\n            W_fc = self.weight_variable([self.hidden_size*2 + self.n_filter_total, self.fc_hidden_size], name=\'Weight_fc\')\n            tf.summary.histogram(\'W_fc\', W_fc)\n            h_fc = tf.matmul(output, W_fc, name=\'h_fc\')\n            beta_fc = tf.Variable(tf.constant(0.1, tf.float32, shape=[self.fc_hidden_size], name=""beta_fc""))\n            tf.summary.histogram(\'beta_fc\', beta_fc)\n            fc_bn, update_ema_fc = self.batchnorm(h_fc, beta_fc, convolutional=False)\n            self.update_emas.append(update_ema_fc)\n            self.fc_bn_relu = tf.nn.relu(fc_bn, name=""relu"")\n            fc_bn_drop = tf.nn.dropout(self.fc_bn_relu, self.keep_prob)\n\n        with tf.variable_scope(\'out_layer\'):\n            W_out = self.weight_variable([self.fc_hidden_size, self.n_class], name=\'Weight_out\')\n            tf.summary.histogram(\'Weight_out\', W_out)\n            b_out = self.bias_variable([self.n_class], name=\'bias_out\')\n            tf.summary.histogram(\'bias_out\', b_out)\n            self._y_pred = tf.nn.xw_plus_b(fc_bn_drop, W_out, b_out, name=\'y_pred\')  # \xe6\xaf\x8f\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe5\x88\x86\xe6\x95\xb0 scores\n\n        with tf.name_scope(\'loss\'):\n            self._loss = tf.reduce_mean(\n                tf.nn.sigmoid_cross_entropy_with_logits(logits=self._y_pred, labels=self._y_inputs))\n            tf.summary.scalar(\'loss\', self._loss)\n\n        self.saver = tf.train.Saver(max_to_keep=1)\n\n    @property\n    def tst(self):\n        return self._tst\n\n    @property\n    def keep_prob(self):\n        return self._keep_prob\n\n    @property\n    def batch_size(self):\n        return self._batch_size\n\n    @property\n    def global_step(self):\n        return self._global_step\n\n    @property\n    def X1_inputs(self):\n        return self._X1_inputs\n\n    @property\n    def X2_inputs(self):\n        return self._X2_inputs\n\n    @property\n    def y_inputs(self):\n        return self._y_inputs\n\n    @property\n    def y_pred(self):\n        return self._y_pred\n\n    @property\n    def loss(self):\n        return self._loss\n\n    def weight_variable(self, shape, name):\n        """"""Create a weight variable with appropriate initialization.""""""\n        initial = tf.truncated_normal(shape, stddev=0.1)\n        return tf.Variable(initial, name=name)\n\n    def bias_variable(self, shape, name):\n        """"""Create a bias variable with appropriate initialization.""""""\n        initial = tf.constant(0.1, shape=shape)\n        return tf.Variable(initial, name=name)\n\n    def batchnorm(self, Ylogits, offset, convolutional=False):\n        """"""batchnormalization.\n        Args:\n            Ylogits: 1D\xe5\x90\x91\xe9\x87\x8f\xe6\x88\x96\xe8\x80\x85\xe6\x98\xaf3D\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\n            num_updates: \xe8\xbf\xad\xe4\xbb\xa3\xe7\x9a\x84global_step\n            offset\xef\xbc\x9a\xe8\xa1\xa8\xe7\xa4\xbabeta\xef\xbc\x8c\xe5\x85\xa8\xe5\xb1\x80\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x9b\xe5\x9c\xa8 RELU \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe4\xb8\x80\xe8\x88\xac\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xba 0.1\xe3\x80\x82\n            scale\xef\xbc\x9a\xe8\xa1\xa8\xe7\xa4\xbalambda\xef\xbc\x8c\xe5\x85\xa8\xe5\xb1\x80\xe6\x96\xb9\xe5\xb7\xae\xef\xbc\x9b\xe5\x9c\xa8 sigmoid \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe9\x9c\x80\xe8\xa6\x81\xef\xbc\x8c\xe8\xbf\x99 RELU \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe4\xbd\x9c\xe7\x94\xa8\xe4\xb8\x8d\xe5\xa4\xa7\xe3\x80\x82\n            m: \xe8\xa1\xa8\xe7\xa4\xbabatch\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x9bv:\xe8\xa1\xa8\xe7\xa4\xbabatch\xe6\x96\xb9\xe5\xb7\xae\xe3\x80\x82\n            bnepsilon\xef\xbc\x9a\xe4\xb8\x80\xe4\xb8\xaa\xe5\xbe\x88\xe5\xb0\x8f\xe7\x9a\x84\xe6\xb5\xae\xe7\x82\xb9\xe6\x95\xb0\xef\xbc\x8c\xe9\x98\xb2\xe6\xad\xa2\xe9\x99\xa4\xe4\xbb\xa5 0.\n        Returns:\n            Ybn: \xe5\x92\x8c Ylogits \xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf\xe7\xbb\x8f\xe8\xbf\x87 Batch Normalization \xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\n            update_moving_everages\xef\xbc\x9a\xe6\x9b\xb4\xe6\x96\xb0mean\xe5\x92\x8cvariance\xef\xbc\x8c\xe4\xb8\xbb\xe8\xa6\x81\xe6\x98\xaf\xe7\xbb\x99\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84 test \xe4\xbd\xbf\xe7\x94\xa8\xe3\x80\x82\n        """"""\n        exp_moving_avg = tf.train.ExponentialMovingAverage(0.999, self._global_step)  # adding the iteration prevents from averaging across non-existing iterations\n        bnepsilon = 1e-5\n        if convolutional:\n            mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])\n        else:\n            mean, variance = tf.nn.moments(Ylogits, [0])\n        update_moving_everages = exp_moving_avg.apply([mean, variance])\n        m = tf.cond(self.tst, lambda: exp_moving_avg.average(mean), lambda: mean)\n        v = tf.cond(self.tst, lambda: exp_moving_avg.average(variance), lambda: variance)\n        Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n        return Ybn, update_moving_everages\n\n    def gru_cell(self):\n        with tf.name_scope(\'gru_cell\'):\n            cell = rnn.GRUCell(self.hidden_size, reuse=tf.get_variable_scope().reuse)\n        return rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n\n    def bi_gru(self, inputs):\n        """"""build the bi-GRU network. \xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\xaa\xe6\x89\x80\xe6\x9c\x89\xe5\xb1\x82\xe7\x9a\x84\xe9\x9a\x90\xe5\x90\xab\xe7\x8a\xb6\xe6\x80\x81\xe3\x80\x82""""""\n        cells_fw = [self.gru_cell() for _ in range(self.n_layer)]\n        cells_bw = [self.gru_cell() for _ in range(self.n_layer)]\n        initial_states_fw = [cell_fw.zero_state(self.batch_size, tf.float32) for cell_fw in cells_fw]\n        initial_states_bw = [cell_bw.zero_state(self.batch_size, tf.float32) for cell_bw in cells_bw]\n        outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(cells_fw, cells_bw, inputs,\n                                                            initial_states_fw=initial_states_fw,\n                                                            initial_states_bw=initial_states_bw, dtype=tf.float32)\n        return outputs\n\n    def task_specific_attention(self, inputs, output_size,\n                                initializer=layers.xavier_initializer(),\n                                activation_fn=tf.tanh, scope=None):\n        """"""\n        Performs task-specific attention reduction, using learned\n        attention context vector (constant within task of interest).\n        Args:\n            inputs: Tensor of shape [batch_size, units, input_size]\n                `input_size` must be static (known)\n                `units` axis will be attended over (reduced from output)\n                `batch_size` will be preserved\n            output_size: Size of output\'s inner (feature) dimension\n        Returns:\n           outputs: Tensor of shape [batch_size, output_dim].\n        """"""\n        assert len(inputs.get_shape()) == 3 and inputs.get_shape()[-1].value is not None\n        with tf.variable_scope(scope or \'attention\') as scope:\n            # u_w, attention \xe5\x90\x91\xe9\x87\x8f\n            attention_context_vector = tf.get_variable(name=\'attention_context_vector\', shape=[output_size],\n                                                       initializer=initializer, dtype=tf.float32)\n            # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c\xe6\x8a\x8a h_i \xe8\xbd\xac\xe4\xb8\xba u_i \xef\xbc\x8c shape= [batch_size, units, input_size] -> [batch_size, units, output_size]\n            input_projection = layers.fully_connected(inputs, output_size, activation_fn=activation_fn, scope=scope)\n            # \xe8\xbe\x93\xe5\x87\xba [batch_size, units]\n            vector_attn = tf.reduce_sum(tf.multiply(input_projection, attention_context_vector), axis=2, keep_dims=True)\n            attention_weights = tf.nn.softmax(vector_attn, dim=1)\n            tf.summary.histogram(\'attention_weigths\', attention_weights)\n            weighted_projection = tf.multiply(inputs, attention_weights)\n            outputs = tf.reduce_sum(weighted_projection, axis=1)\n            return outputs  # \xe8\xbe\x93\xe5\x87\xba [batch_size, hidden_size*2]\n\n    def bigru_inference(self, X_inputs):\n        inputs = tf.nn.embedding_lookup(self.title_embedding, X_inputs)\n        output_bigru = self.bi_gru(inputs)\n        output_att = self.task_specific_attention(output_bigru, self.hidden_size*2)\n        return output_att\n\n    def cnn_inference(self, X_inputs, n_step):\n        """"""TextCNN \xe6\xa8\xa1\xe5\x9e\x8b\xe3\x80\x82\n        Args:\n            X_inputs: tensor.shape=(batch_size, n_step)\n        Returns:\n            title_outputs: tensor.shape=(batch_size, self.n_filter_total)\n        """"""\n        inputs = tf.nn.embedding_lookup(self.content_embedding, X_inputs)\n        inputs = tf.expand_dims(inputs, -1)\n        pooled_outputs = list()\n        for i, filter_size in enumerate(self.filter_sizes):\n            with tf.variable_scope(""conv-maxpool-%s"" % filter_size):\n                # Convolution Layer\n                filter_shape = [filter_size, self.embedding_size, 1, self.n_filter]\n                W_filter = self.weight_variable(shape=filter_shape, name=\'W_filter\')\n                beta = self.bias_variable(shape=[self.n_filter], name=\'beta_filter\')\n                tf.summary.histogram(\'beta\', beta)\n                conv = tf.nn.conv2d(inputs, W_filter, strides=[1, 1, 1, 1], padding=""VALID"", name=""conv"")\n                conv_bn, update_ema = self.batchnorm(conv, beta, convolutional=True)\n                # Apply nonlinearity, batch norm scaling is not useful with relus\n                h = tf.nn.relu(conv_bn, name=""relu"")\n                # Maxpooling over the outputs\n                pooled = tf.nn.max_pool(h, ksize=[1, n_step - filter_size + 1, 1, 1],\n                                        strides=[1, 1, 1, 1], padding=\'VALID\', name=""pool"")\n                pooled_outputs.append(pooled)\n                self.update_emas.append(update_ema)\n        h_pool = tf.concat(pooled_outputs, 3)\n        h_pool_flat = tf.reshape(h_pool, [-1, self.n_filter_total])\n        return h_pool_flat  # shape = [batch_size, self.n_filter_total]\n\n\n# test the model\ndef test():\n    import numpy as np\n    print(\'Begin testing...\')\n    settings = Settings()\n    W_embedding = np.random.randn(50, 10)\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    batch_size = 128\n    with tf.Session(config=config) as sess:\n        model = BiGRU_CNN(W_embedding, settings)\n        optimizer = tf.train.AdamOptimizer(0.001)\n        train_op = optimizer.minimize(model.loss)\n        update_op = tf.group(*model.update_emas)\n        sess.run(tf.global_variables_initializer())\n        fetch = [model.loss, model.y_pred, train_op, update_op]\n        loss_list = list()\n        for i in xrange(100):\n            X1_batch = np.zeros((batch_size, 30), dtype=float)\n            X2_batch = np.zeros((batch_size, 150), dtype=float)\n            y_batch = np.zeros((batch_size, 1999), dtype=int)\n            _batch_size = len(y_batch)\n            feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\n                         model.batch_size: _batch_size, model.tst: False, model.keep_prob: 0.5}\n            loss, y_pred, _, _ = sess.run(fetch, feed_dict=feed_dict)\n            loss_list.append(loss)\n            print(i, loss)\n\nif __name__ == \'__main__\':\n    test()\n'"
models/wd_5_bigru_cnn/predict.py,4,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport os\r\nimport sys\r\nimport time\r\nimport network\r\n\r\nsys.path.append(\'../..\')\r\nfrom evaluator import score_eval\r\n\r\nsettings = network.Settings()\r\ntitle_len = settings.title_len\r\nmodel_name = settings.model_name\r\nckpt_path = settings.ckpt_path\r\n\r\nlocal_scores_path = \'../../local_scores/\'\r\nscores_path = \'../../scores/\'\r\nif not os.path.exists(local_scores_path):\r\n    os.makedirs(local_scores_path)\r\nif not os.path.exists(scores_path):\r\n    os.makedirs(scores_path)\r\n\r\nembedding_path = \'../../data/word_embedding.npy\'\r\ndata_valid_path = \'../../data/wd-data/data_valid/\'\r\ndata_test_path = \'../../data/wd-data/data_test/\'\r\nva_batches = os.listdir(data_valid_path)\r\nte_batches = os.listdir(data_test_path)  # batch \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8\r\nn_va_batches = len(va_batches)\r\nn_te_batches = len(te_batches)\r\n\r\n\r\ndef get_batch(batch_id):\r\n    """"""get a batch from valid data""""""\r\n    new_batch = np.load(data_valid_path + str(batch_id) + \'.npz\')\r\n    X_batch = new_batch[\'X\']\r\n    y_batch = new_batch[\'y\']\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch, y_batch]\r\n\r\n\r\ndef get_test_batch(batch_id):\r\n    """"""get a batch from test data""""""\r\n    X_batch = np.load(data_test_path + str(batch_id) + \'.npy\')\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch]\r\n\r\n\r\ndef local_predict(sess, model):\r\n    """"""Test on the valid data.""""""\r\n    time0 = time.time()\r\n    predict_labels_list = list()  # \xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\r\n    marked_labels_list = list()\r\n    predict_scores = list()\r\n    for i in tqdm(xrange(n_va_batches)):\r\n        [X1_batch, X2_batch, y_batch] = get_batch(i)\r\n        marked_labels_list.extend(y_batch)\r\n        _batch_size = len(X1_batch)\r\n        fetches = [model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        predict_labels = sess.run(fetches, feed_dict)[0]\r\n        predict_scores.append(predict_labels)\r\n        predict_labels = map(lambda label: label.argsort()[-1:-6:-1], predict_labels)  # \xe5\x8f\x96\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x845\xe4\xb8\xaa\xe4\xb8\x8b\xe6\xa0\x87\r\n        predict_labels_list.extend(predict_labels)\r\n    predict_label_and_marked_label_list = zip(predict_labels_list, marked_labels_list)\r\n    precision, recall, f1 = score_eval(predict_label_and_marked_label_list)\r\n    print(\'Local valid p=%g, r=%g, f1=%g\' % (precision, recall, f1))\r\n    predict_scores = np.vstack(np.asarray(predict_scores))\r\n    local_scores_name = local_scores_path + model_name + \'.npy\'\r\n    np.save(local_scores_name, predict_scores)\r\n    print(\'local_scores.shape=\', predict_scores.shape)\r\n    print(\'Writed the scores into %s, time %g s\' % (local_scores_name, time.time() - time0))\r\n\r\n\r\ndef predict(sess, model):\r\n    """"""Test on the test data.""""""\r\n    time0 = time.time()\r\n    predict_scores = list()\r\n    for i in tqdm(xrange(n_te_batches)):\r\n        [X1_batch, X2_batch] = get_test_batch(i)\r\n        _batch_size = len(X1_batch)\r\n        fetches = [model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        predict_labels = sess.run(fetches, feed_dict)[0]\r\n        predict_scores.append(predict_labels)\r\n    predict_scores = np.vstack(np.asarray(predict_scores))\r\n    scores_name = scores_path + model_name + \'.npy\'\r\n    np.save(scores_name, predict_scores)\r\n    print(\'scores.shape=\', predict_scores.shape)\r\n    print(\'Writed the scores into %s, time %g s\' % (scores_name, time.time() - time0))\r\n\r\n\r\ndef main(_):\r\n    if not os.path.exists(ckpt_path + \'checkpoint\'):\r\n        print(\'there is not saved model, please check the ckpt path\')\r\n        exit()\r\n    print(\'Loading model...\')\r\n    W_embedding = np.load(embedding_path)\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    with tf.Session(config=config) as sess:\r\n        model = network.BiGRU_CNN(W_embedding, settings)\r\n        model.saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\r\n        print(\'Local predicting...\')\r\n        local_predict(sess, model)\r\n        print(\'Test predicting...\')\r\n        predict(sess, model)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    tf.app.run()\r\n'"
models/wd_5_bigru_cnn/train.py,21,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport os\r\nimport sys\r\nimport shutil\r\nimport time\r\nimport network\r\n\r\nsys.path.append(\'../..\')\r\nfrom data_helpers import to_categorical\r\nfrom evaluator import score_eval\r\n\r\nflags = tf.flags\r\nflags.DEFINE_bool(\'is_retrain\', False, \'if is_retrain is true, not rebuild the summary\')\r\nflags.DEFINE_integer(\'max_epoch\', 1, \'update the embedding after max_epoch, default: 1\')\r\nflags.DEFINE_integer(\'max_max_epoch\', 6, \'all training epoches, default: 6\')\r\nflags.DEFINE_float(\'lr\', 8e-4, \'initial learning rate, default: 8e-4\')\r\nflags.DEFINE_float(\'decay_rate\', 0.75, \'decay rate, default: 0.75\')\r\nflags.DEFINE_float(\'keep_prob\', 0.5, \'keep_prob for training, default: 0.5\')\r\n# \xe6\xad\xa3\xe5\xbc\x8f\r\n# flags.DEFINE_integer(\'decay_step\', 15000, \'decay_step, default: 15000\')\r\n# flags.DEFINE_integer(\'valid_step\', 10000, \'valid_step, default: 10000\')\r\n# flags.DEFINE_float(\'last_f1\', 0.40, \'if valid_f1 > last_f1, save new model. default: 0.40\')\r\n\r\n# \xe6\xb5\x8b\xe8\xaf\x95\r\nflags.DEFINE_integer(\'decay_step\', 1000, \'decay_step, default: 1000\')\r\nflags.DEFINE_integer(\'valid_step\', 500, \'valid_step, default: 500\')\r\nflags.DEFINE_float(\'last_f1\', 0.10, \'if valid_f1 > last_f1, save new model. default: 0.10\')\r\nFLAGS = flags.FLAGS\r\n\r\nlr = FLAGS.lr\r\nlast_f1 = FLAGS.last_f1\r\nsettings = network.Settings()\r\ntitle_len = settings.title_len\r\nsummary_path = settings.summary_path\r\nckpt_path = settings.ckpt_path\r\nmodel_path = ckpt_path + \'model.ckpt\'\r\n\r\nembedding_path = \'../../data/word_embedding.npy\'\r\ndata_train_path = \'../../data/wd-data/data_train/\'\r\ndata_valid_path = \'../../data/wd-data/data_valid/\'\r\ntr_batches = os.listdir(data_train_path)  # batch \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8\r\nva_batches = os.listdir(data_valid_path)\r\nn_tr_batches = len(tr_batches)\r\nn_va_batches = len(va_batches)\r\n\r\n# \xe6\xb5\x8b\xe8\xaf\x95\r\nn_tr_batches = 1000\r\nn_va_batches = 50\r\n\r\n\r\ndef get_batch(data_path, batch_id):\r\n    """"""get a batch from data_path""""""\r\n    new_batch = np.load(data_path + str(batch_id) + \'.npz\')\r\n    X_batch = new_batch[\'X\']\r\n    y_batch = new_batch[\'y\']\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch, y_batch]\r\n\r\n\r\ndef valid_epoch(data_path, sess, model):\r\n    """"""Test on the valid data.""""""\r\n    va_batches = os.listdir(data_path)\r\n    n_va_batches = len(va_batches)\r\n    _costs = 0.0\r\n    predict_labels_list = list()  # \xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\r\n    marked_labels_list = list()\r\n    for i in xrange(n_va_batches):\r\n        [X1_batch, X2_batch, y_batch] = get_batch(data_path, i)\r\n        marked_labels_list.extend(y_batch)\r\n        y_batch = to_categorical(y_batch)\r\n        _batch_size = len(y_batch)\r\n        fetches = [model.loss, model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        _cost, predict_labels = sess.run(fetches, feed_dict)\r\n        _costs += _cost\r\n        predict_labels = map(lambda label: label.argsort()[-1:-6:-1], predict_labels)  # \xe5\x8f\x96\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x845\xe4\xb8\xaa\xe4\xb8\x8b\xe6\xa0\x87\r\n        predict_labels_list.extend(predict_labels)\r\n    predict_label_and_marked_label_list = zip(predict_labels_list, marked_labels_list)\r\n    precision, recall, f1 = score_eval(predict_label_and_marked_label_list)\r\n    mean_cost = _costs / n_va_batches\r\n    return mean_cost, precision, recall, f1\r\n\r\n\r\ndef train_epoch(data_path, sess, model, train_fetches, valid_fetches, train_writer, test_writer):\r\n    global last_f1\r\n    global lr\r\n    time0 = time.time()\r\n    batch_indexs = np.random.permutation(n_tr_batches)  # shuffle the training data\r\n    for batch in tqdm(xrange(n_tr_batches)):\r\n        global_step = sess.run(model.global_step)\r\n        if 0 == (global_step + 1) % FLAGS.valid_step:\r\n            valid_cost, precision, recall, f1 = valid_epoch(data_valid_path, sess, model)\r\n            print(\'Global_step=%d: valid cost=%g; p=%g, r=%g, f1=%g, time=%g s\' % (\r\n                global_step, valid_cost, precision, recall, f1, time.time() - time0))\r\n            time0 = time.time()\r\n            if f1 > last_f1:\r\n                last_f1 = f1\r\n                saving_path = model.saver.save(sess, model_path, global_step+1)\r\n                print(\'saved new model to %s \' % saving_path)\r\n        # training\r\n        batch_id = batch_indexs[batch]\r\n        [X1_batch, X2_batch, y_batch] = get_batch(data_train_path, batch_id)\r\n        y_batch = to_categorical(y_batch)\r\n        _batch_size = len(y_batch)\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                     model.batch_size: _batch_size, model.tst: False, model.keep_prob: FLAGS.keep_prob}\r\n        summary, _cost, _, _ = sess.run(train_fetches, feed_dict)  # the cost is the mean cost of one batch\r\n        # valid per 500 steps\r\n        if 0 == (global_step + 1) % 500:\r\n            train_writer.add_summary(summary, global_step)\r\n            batch_id = np.random.randint(0, n_va_batches)  # \xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe4\xb8\x80\xe4\xb8\xaa\xe9\xaa\x8c\xe8\xaf\x81batch\r\n            [X1_batch, X2_batch, y_batch] = get_batch(data_valid_path, batch_id)\r\n            y_batch = to_categorical(y_batch)\r\n            _batch_size = len(y_batch)\r\n            feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                         model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n            summary, _cost = sess.run(valid_fetches, feed_dict)\r\n            test_writer.add_summary(summary, global_step)\r\n\r\n\r\ndef main(_):\r\n    global ckpt_path\r\n    global last_f1\r\n    if not os.path.exists(ckpt_path):\r\n        os.makedirs(ckpt_path)\r\n    if not os.path.exists(summary_path):\r\n        os.makedirs(summary_path)\r\n    elif not FLAGS.is_retrain:  # \xe9\x87\x8d\xe6\x96\xb0\xe8\xae\xad\xe7\xbb\x83\xe6\x9c\xac\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\x88\xa0\xe9\x99\xa4\xe4\xbb\xa5\xe5\x89\x8d\xe7\x9a\x84 summary\r\n        shutil.rmtree(summary_path)\r\n        os.makedirs(summary_path)\r\n    if not os.path.exists(summary_path):\r\n        os.makedirs(summary_path)\r\n\r\n    print(\'1.Loading data...\')\r\n    W_embedding = np.load(embedding_path)\r\n    print(\'training sample_num = %d\' % n_tr_batches)\r\n    print(\'valid sample_num = %d\' % n_va_batches)\r\n\r\n    # Initial or restore the model\r\n    print(\'2.Building model...\')\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    with tf.Session(config=config) as sess:\r\n        model = network.BiGRU_CNN(W_embedding, settings)\r\n        with tf.variable_scope(\'training_ops\') as vs:\r\n            learning_rate = tf.train.exponential_decay(FLAGS.lr, model.global_step, FLAGS.decay_step,\r\n                                                   FLAGS.decay_rate, staircase=True)\r\n            # two optimizer: op1, update embedding; op2, do not update embedding.\r\n            with tf.variable_scope(\'Optimizer1\'):\r\n                tvars1 = tf.trainable_variables()\r\n                grads1 = tf.gradients(model.loss, tvars1)\r\n                optimizer1 = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n                train_op1 = optimizer1.apply_gradients(zip(grads1, tvars1),\r\n                                                   global_step=model.global_step)\r\n            with tf.variable_scope(\'Optimizer2\'):\r\n                tvars2 = [tvar for tvar in tvars1 if \'embedding\' not in tvar.name]\r\n                grads2 = tf.gradients(model.loss, tvars2)\r\n                optimizer2 = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n                train_op2 = optimizer2.apply_gradients(zip(grads2, tvars2),\r\n                                                   global_step=model.global_step)\r\n            update_op = tf.group(*model.update_emas)\r\n            merged = tf.summary.merge_all()  # summary\r\n            train_writer = tf.summary.FileWriter(summary_path + \'train\', sess.graph)\r\n            test_writer = tf.summary.FileWriter(summary_path + \'test\')\r\n            training_ops = [v for v in tf.global_variables() if v.name.startswith(vs.name+\'/\')]\r\n\r\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe5\xb7\xb2\xe7\xbb\x8f\xe4\xbf\x9d\xe5\xad\x98\xe8\xbf\x87\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\xaf\xbc\xe5\x85\xa5\xe4\xb8\x8a\xe6\xac\xa1\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\r\n        if os.path.exists(ckpt_path + ""checkpoint""):\r\n            print(""Restoring Variables from Checkpoint..."")\r\n            model.saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\r\n            last_valid_cost, precision, recall, last_f1 = valid_epoch(data_valid_path, sess, model)\r\n            print(\' valid cost=%g; p=%g, r=%g, f1=%g\' % (last_valid_cost, precision, recall, last_f1))\r\n            sess.run(tf.variables_initializer(training_ops))\r\n            train_op2 = train_op1\r\n        else:\r\n            print(\'Initializing Variables...\')\r\n            sess.run(tf.global_variables_initializer())\r\n\r\n        print(\'3.Begin training...\')\r\n        print(\'max_epoch=%d, max_max_epoch=%d\' % (FLAGS.max_epoch, FLAGS.max_max_epoch))\r\n        train_op = train_op1\r\n        for epoch in xrange(FLAGS.max_max_epoch):\r\n            global_step = sess.run(model.global_step)\r\n            print(\'Global step %d, lr=%g\' % (global_step, sess.run(learning_rate)))\r\n            if epoch == FLAGS.max_epoch:  # update the embedding\r\n                train_op = train_op1\r\n            train_fetches = [merged, model.loss, train_op, update_op]\r\n            valid_fetches = [merged, model.loss]\r\n            train_epoch(data_train_path, sess, model, train_fetches, valid_fetches, train_writer, test_writer)\r\n        # \xe6\x9c\x80\xe5\x90\x8e\xe5\x86\x8d\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\r\n        valid_cost, precision, recall, f1 = valid_epoch(data_valid_path, sess, model)\r\n        print(\'END.Global_step=%d: valid cost=%g; p=%g, r=%g, f1=%g\' % (\r\n            sess.run(model.global_step), valid_cost, precision, recall, f1))\r\n        if f1 > last_f1:  # save the better model\r\n            saving_path = model.saver.save(sess, model_path, sess.run(model.global_step)+1)\r\n            print(\'saved new model to %s \' % saving_path)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    tf.app.run()\r\n'"
models/wd_6_rcnn/__init__.py,0,b'# -*- coding:utf-8 -*- \r\n\r\n'
models/wd_6_rcnn/network.py,62,"b'# -*- coding:utf-8 -*-\n\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\nimport tensorflow.contrib.layers as layers\n\n""""""wd_5_bigru_cnn\n\xe5\x9c\xa8\xe8\xae\xba\xe6\x96\x87 Recurrent Convolutional Neural Networks for Text Classification \xe4\xb8\xad\xe3\x80\x82\n\xe4\xbd\xbf\xe7\x94\xa8 BiRNN \xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe5\xb0\x86\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84\xe9\x9a\x90\xe8\x97\x8f\xe7\x8a\xb6\xe6\x80\x81\xe5\x92\x8c\xe5\x8e\x9f\xe8\xbe\x93\xe5\x85\xa5\xe6\x8b\xbc\xe8\xb5\xb7\xe6\x9d\xa5\xef\xbc\x8c\xe5\x9c\xa8\xe8\xbf\x9b\xe8\xa1\x8c max_pooling \xe6\x93\x8d\xe4\xbd\x9c\xe3\x80\x82\n\xe8\xbf\x99\xe9\x87\x8c\xe6\x9c\x89\xe4\xba\x9b\xe4\xb8\x8d\xe5\x90\x8c\xef\xbc\x8c\xe9\xa6\x96\xe5\x85\x88\xe4\xb9\x9f\xe6\x98\xaf\xe4\xbd\xbf\xe7\x94\xa8 bigru \xe5\xbe\x97\xe5\x88\xb0\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84\xef\xbc\x8c\xe5\xb0\x86\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84\xe9\x9a\x90\xe8\x97\x8f\xe7\x8a\xb6\xe6\x80\x81\xe5\x92\x8c\xe5\x8e\x9f\xe8\xbe\x93\xe5\x85\xa5\xe6\x8b\xbc\xe8\xb5\xb7\xe6\x9d\xa5\xef\xbc\x9b\n\xe7\x84\xb6\xe5\x90\x8e\xe4\xbd\xbf\xe7\x94\xa8\xe8\xbe\x93\xe5\x85\xa5\xe5\x88\xb0 TextCNN \xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe3\x80\x82\n""""""\n\n\nclass Settings(object):\n    def __init__(self):\n        self.model_name = \'wd_5_bigru_cnn\'\n        self.title_len = 30\n        self.content_len = 150\n        self.hidden_size = 256\n        self.n_layer = 1\n        self.filter_sizes = [2, 3, 4, 5, 7]\n        self.n_filter = 256\n        self.fc_hidden_size = 1024\n        self.n_class = 1999\n        self.summary_path = \'../../summary/\' + self.model_name + \'/\'\n        self.ckpt_path = \'../../ckpt/\' + self.model_name + \'/\'\n\n\nclass RCNN(object):\n    def __init__(self, W_embedding, settings):\n        self.model_name = settings.model_name\n        self.title_len = settings.title_len\n        self.content_len = settings.content_len\n        self.hidden_size = settings.hidden_size\n        self.n_layer = settings.n_layer\n        self.filter_sizes = settings.filter_sizes\n        self.n_filter = settings.n_filter\n        self.n_filter_total = self.n_filter * len(self.filter_sizes)\n        self.n_class = settings.n_class\n        self.fc_hidden_size = settings.fc_hidden_size\n        self._global_step = tf.Variable(0, trainable=False, name=\'Global_Step\')\n        self.update_emas = list()\n        # placeholders\n        self._tst = tf.placeholder(tf.bool)\n        self._keep_prob = tf.placeholder(tf.float32, [])\n        self._batch_size = tf.placeholder(tf.int32, [])\n\n        with tf.name_scope(\'Inputs\'):\n            self._X1_inputs = tf.placeholder(tf.int64, [None, self.title_len], name=\'X1_inputs\')\n            self._X2_inputs = tf.placeholder(tf.int64, [None, self.content_len], name=\'X2_inputs\')\n            self._y_inputs = tf.placeholder(tf.float32, [None, self.n_class], name=\'y_input\')\n\n        with tf.variable_scope(\'embedding\'):\n            self.embedding = tf.get_variable(name=\'embedding\', shape=W_embedding.shape,\n                                                   initializer=tf.constant_initializer(W_embedding), trainable=True)\n        self.embedding_size = W_embedding.shape[1]\n\n        with tf.variable_scope(\'rcnn_text\'):\n            output_title = self.rcnn_inference(self._X1_inputs, self.title_len)\n\n        with tf.variable_scope(\'rcnn_content\'):\n            output_content = self.rcnn_inference(self._X2_inputs, self.content_len)\n\n        with tf.variable_scope(\'fc-bn-layer\'):\n            output = tf.concat([output_title, output_content], axis=1)\n            W_fc = self.weight_variable([self.n_filter_total*2, self.fc_hidden_size],\n                                        name=\'Weight_fc\')\n            tf.summary.histogram(\'W_fc\', W_fc)\n            h_fc = tf.matmul(output, W_fc, name=\'h_fc\')\n            beta_fc = tf.Variable(tf.constant(0.1, tf.float32, shape=[self.fc_hidden_size], name=""beta_fc""))\n            tf.summary.histogram(\'beta_fc\', beta_fc)\n            fc_bn, update_ema_fc = self.batchnorm(h_fc, beta_fc, convolutional=False)\n            self.update_emas.append(update_ema_fc)\n            self.fc_bn_relu = tf.nn.relu(fc_bn, name=""relu"")\n            fc_bn_drop = tf.nn.dropout(self.fc_bn_relu, self.keep_prob)\n\n        with tf.variable_scope(\'out_layer\'):\n            W_out = self.weight_variable([self.fc_hidden_size, self.n_class], name=\'Weight_out\')\n            tf.summary.histogram(\'Weight_out\', W_out)\n            b_out = self.bias_variable([self.n_class], name=\'bias_out\')\n            tf.summary.histogram(\'bias_out\', b_out)\n            self._y_pred = tf.nn.xw_plus_b(fc_bn_drop, W_out, b_out, name=\'y_pred\')  # \xe6\xaf\x8f\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe5\x88\x86\xe6\x95\xb0 scores\n\n        with tf.name_scope(\'loss\'):\n            self._loss = tf.reduce_mean(\n                tf.nn.sigmoid_cross_entropy_with_logits(logits=self._y_pred, labels=self._y_inputs))\n            tf.summary.scalar(\'loss\', self._loss)\n\n        self.saver = tf.train.Saver(max_to_keep=1)\n\n    @property\n    def tst(self):\n        return self._tst\n\n    @property\n    def keep_prob(self):\n        return self._keep_prob\n\n    @property\n    def batch_size(self):\n        return self._batch_size\n\n    @property\n    def global_step(self):\n        return self._global_step\n\n    @property\n    def X1_inputs(self):\n        return self._X1_inputs\n\n    @property\n    def X2_inputs(self):\n        return self._X2_inputs\n\n    @property\n    def y_inputs(self):\n        return self._y_inputs\n\n    @property\n    def y_pred(self):\n        return self._y_pred\n\n    @property\n    def loss(self):\n        return self._loss\n\n    def weight_variable(self, shape, name):\n        """"""Create a weight variable with appropriate initialization.""""""\n        initial = tf.truncated_normal(shape, stddev=0.1)\n        return tf.Variable(initial, name=name)\n\n    def bias_variable(self, shape, name):\n        """"""Create a bias variable with appropriate initialization.""""""\n        initial = tf.constant(0.1, shape=shape)\n        return tf.Variable(initial, name=name)\n\n    def batchnorm(self, Ylogits, offset, convolutional=False):\n        """"""batchnormalization.\n        Args:\n            Ylogits: 1D\xe5\x90\x91\xe9\x87\x8f\xe6\x88\x96\xe8\x80\x85\xe6\x98\xaf3D\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\n            num_updates: \xe8\xbf\xad\xe4\xbb\xa3\xe7\x9a\x84global_step\n            offset\xef\xbc\x9a\xe8\xa1\xa8\xe7\xa4\xbabeta\xef\xbc\x8c\xe5\x85\xa8\xe5\xb1\x80\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x9b\xe5\x9c\xa8 RELU \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe4\xb8\x80\xe8\x88\xac\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xba 0.1\xe3\x80\x82\n            scale\xef\xbc\x9a\xe8\xa1\xa8\xe7\xa4\xbalambda\xef\xbc\x8c\xe5\x85\xa8\xe5\xb1\x80\xe6\x96\xb9\xe5\xb7\xae\xef\xbc\x9b\xe5\x9c\xa8 sigmoid \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe9\x9c\x80\xe8\xa6\x81\xef\xbc\x8c\xe8\xbf\x99 RELU \xe6\xbf\x80\xe6\xb4\xbb\xe4\xb8\xad\xe4\xbd\x9c\xe7\x94\xa8\xe4\xb8\x8d\xe5\xa4\xa7\xe3\x80\x82\n            m: \xe8\xa1\xa8\xe7\xa4\xbabatch\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x9bv:\xe8\xa1\xa8\xe7\xa4\xbabatch\xe6\x96\xb9\xe5\xb7\xae\xe3\x80\x82\n            bnepsilon\xef\xbc\x9a\xe4\xb8\x80\xe4\xb8\xaa\xe5\xbe\x88\xe5\xb0\x8f\xe7\x9a\x84\xe6\xb5\xae\xe7\x82\xb9\xe6\x95\xb0\xef\xbc\x8c\xe9\x98\xb2\xe6\xad\xa2\xe9\x99\xa4\xe4\xbb\xa5 0.\n        Returns:\n            Ybn: \xe5\x92\x8c Ylogits \xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf\xe7\xbb\x8f\xe8\xbf\x87 Batch Normalization \xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\n            update_moving_everages\xef\xbc\x9a\xe6\x9b\xb4\xe6\x96\xb0mean\xe5\x92\x8cvariance\xef\xbc\x8c\xe4\xb8\xbb\xe8\xa6\x81\xe6\x98\xaf\xe7\xbb\x99\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84 test \xe4\xbd\xbf\xe7\x94\xa8\xe3\x80\x82\n        """"""\n        exp_moving_avg = tf.train.ExponentialMovingAverage(0.999,\n                                                           self._global_step)  # adding the iteration prevents from averaging across non-existing iterations\n        bnepsilon = 1e-5\n        if convolutional:\n            mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])\n        else:\n            mean, variance = tf.nn.moments(Ylogits, [0])\n        update_moving_everages = exp_moving_avg.apply([mean, variance])\n        m = tf.cond(self.tst, lambda: exp_moving_avg.average(mean), lambda: mean)\n        v = tf.cond(self.tst, lambda: exp_moving_avg.average(variance), lambda: variance)\n        Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n        return Ybn, update_moving_everages\n\n    def gru_cell(self):\n        with tf.name_scope(\'gru_cell\'):\n            cell = rnn.GRUCell(self.hidden_size, reuse=tf.get_variable_scope().reuse)\n        return rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n\n    def bi_gru(self, X_inputs):\n        """"""build the bi-GRU network. Return the encoder represented vector.\n        X_inputs: [batch_size, n_step]\n        n_step: \xe5\x8f\xa5\xe5\xad\x90\xe7\x9a\x84\xe8\xaf\x8d\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x9b\xe6\x88\x96\xe8\x80\x85\xe6\x96\x87\xe6\xa1\xa3\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xe6\x95\xb0\xe3\x80\x82\n        outputs: [fw_state, embeddings, bw_state], shape=[batch_size, hidden_size+embedding_size+hidden_size]\n        """"""\n        inputs = tf.nn.embedding_lookup(self.embedding, X_inputs)   # [batch_size, n_step, embedding_size]\n        cells_fw = [self.gru_cell() for _ in range(self.n_layer)]\n        cells_bw = [self.gru_cell() for _ in range(self.n_layer)]\n        initial_states_fw = [cell_fw.zero_state(self.batch_size, tf.float32) for cell_fw in cells_fw]\n        initial_states_bw = [cell_bw.zero_state(self.batch_size, tf.float32) for cell_bw in cells_bw]\n        outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(cells_fw, cells_bw, inputs,\n                        initial_states_fw = initial_states_fw, initial_states_bw = initial_states_bw, dtype=tf.float32)\n        hidden_outputs = tf.concat([outputs, inputs], axis=2)\n        return hidden_outputs  # shape =[seg_num, n_steps, hidden_size*2+embedding_size]\n\n    def textcnn(self, cnn_inputs, n_step):\n        """"""build the TextCNN network. Return the h_drop""""""\n        # cnn_inputs.shape = [batchsize, n_step, hidden_size*2+embedding_size]\n        inputs = tf.expand_dims(cnn_inputs, -1)\n        pooled_outputs = list()\n        for i, filter_size in enumerate(self.filter_sizes):\n            with tf.variable_scope(""conv-maxpool-%s"" % filter_size):\n                # Convolution Layer\n                filter_shape = [filter_size, self.hidden_size*2+self.embedding_size, 1, self.n_filter]\n                W_filter = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W_filter"")\n                beta = tf.Variable(tf.constant(0.1, tf.float32, shape=[self.n_filter], name=""beta""))\n                tf.summary.histogram(\'beta\', beta)\n                conv = tf.nn.conv2d(inputs, W_filter, strides=[1, 1, 1, 1], padding=""VALID"", name=""conv"")\n                conv_bn, update_ema = self.batchnorm(conv, beta, convolutional=True)    # \xe5\x9c\xa8\xe6\xbf\x80\xe6\xb4\xbb\xe5\xb1\x82\xe5\x89\x8d\xe9\x9d\xa2\xe5\x8a\xa0 BN\n                # Apply nonlinearity, batch norm scaling is not useful with relus\n                h = tf.nn.relu(conv_bn, name=""relu"")\n                # Maxpooling over the outputs\n                pooled = tf.nn.max_pool(h,ksize=[1, n_step - filter_size + 1, 1, 1],\n                                        strides=[1, 1, 1, 1],padding=\'VALID\',name=""pool"")\n                pooled_outputs.append(pooled)\n                self.update_emas.append(update_ema)\n        h_pool = tf.concat(pooled_outputs, 3)\n        h_pool_flat = tf.reshape(h_pool, [-1, self.n_filter_total])\n        return h_pool_flat    # shape = [batch_size, n_filter_total]\n\n    def rcnn_inference(self, X_inputs, n_step):\n        output_bigru = self.bi_gru(X_inputs)\n        output_cnn = self.textcnn(output_bigru, n_step)\n        return output_cnn # shape = [batch_size, n_filter_total]\n\n\n# test the model\ndef test():\n    import numpy as np\n    print(\'Begin testing...\')\n    settings = Settings()\n    W_embedding = np.random.randn(50, 10)\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    batch_size = 128\n    with tf.Session(config=config) as sess:\n        model = RCNN(W_embedding, settings)\n        optimizer = tf.train.AdamOptimizer(0.001)\n        train_op = optimizer.minimize(model.loss)\n        update_op = tf.group(*model.update_emas)\n        sess.run(tf.global_variables_initializer())\n        fetch = [model.loss, model.y_pred, train_op, update_op]\n        loss_list = list()\n        for i in xrange(100):\n            X1_batch = np.zeros((batch_size, 30), dtype=float)\n            X2_batch = np.zeros((batch_size, 150), dtype=float)\n            y_batch = np.zeros((batch_size, 1999), dtype=int)\n            _batch_size = len(y_batch)\n            feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\n                         model.batch_size: _batch_size, model.tst: False, model.keep_prob: 0.5}\n            loss, y_pred, _, _ = sess.run(fetch, feed_dict=feed_dict)\n            loss_list.append(loss)\n            print(i, loss)\n\n\nif __name__ == \'__main__\':\n    test()\n'"
models/wd_6_rcnn/predict.py,4,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport os\r\nimport sys\r\nimport time\r\nimport network\r\n\r\nsys.path.append(\'../..\')\r\nfrom evaluator import score_eval\r\n\r\nsettings = network.Settings()\r\ntitle_len = settings.title_len\r\nmodel_name = settings.model_name\r\nckpt_path = settings.ckpt_path\r\n\r\nlocal_scores_path = \'../../local_scores/\'\r\nscores_path = \'../../scores/\'\r\nif not os.path.exists(local_scores_path):\r\n    os.makedirs(local_scores_path)\r\nif not os.path.exists(scores_path):\r\n    os.makedirs(scores_path)\r\n\r\nembedding_path = \'../../data/word_embedding.npy\'\r\ndata_valid_path = \'../../data/wd-data/data_valid/\'\r\ndata_test_path = \'../../data/wd-data/data_test/\'\r\nva_batches = os.listdir(data_valid_path)\r\nte_batches = os.listdir(data_test_path)  # batch \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8\r\nn_va_batches = len(va_batches)\r\nn_te_batches = len(te_batches)\r\n\r\n\r\ndef get_batch(batch_id):\r\n    """"""get a batch from valid data""""""\r\n    new_batch = np.load(data_valid_path + str(batch_id) + \'.npz\')\r\n    X_batch = new_batch[\'X\']\r\n    y_batch = new_batch[\'y\']\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch, y_batch]\r\n\r\n\r\ndef get_test_batch(batch_id):\r\n    """"""get a batch from test data""""""\r\n    X_batch = np.load(data_test_path + str(batch_id) + \'.npy\')\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch]\r\n\r\n\r\ndef local_predict(sess, model):\r\n    """"""Test on the valid data.""""""\r\n    time0 = time.time()\r\n    predict_labels_list = list()  # \xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\r\n    marked_labels_list = list()\r\n    predict_scores = list()\r\n    for i in tqdm(xrange(n_va_batches)):\r\n        [X1_batch, X2_batch, y_batch] = get_batch(i)\r\n        marked_labels_list.extend(y_batch)\r\n        _batch_size = len(X1_batch)\r\n        fetches = [model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        predict_labels = sess.run(fetches, feed_dict)[0]\r\n        predict_scores.append(predict_labels)\r\n        predict_labels = map(lambda label: label.argsort()[-1:-6:-1], predict_labels)  # \xe5\x8f\x96\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x845\xe4\xb8\xaa\xe4\xb8\x8b\xe6\xa0\x87\r\n        predict_labels_list.extend(predict_labels)\r\n    predict_label_and_marked_label_list = zip(predict_labels_list, marked_labels_list)\r\n    precision, recall, f1 = score_eval(predict_label_and_marked_label_list)\r\n    print(\'Local valid p=%g, r=%g, f1=%g\' % (precision, recall, f1))\r\n    predict_scores = np.vstack(np.asarray(predict_scores))\r\n    local_scores_name = local_scores_path + model_name + \'.npy\'\r\n    np.save(local_scores_name, predict_scores)\r\n    print(\'local_scores.shape=\', predict_scores.shape)\r\n    print(\'Writed the scores into %s, time %g s\' % (local_scores_name, time.time() - time0))\r\n\r\n\r\ndef predict(sess, model):\r\n    """"""Test on the test data.""""""\r\n    time0 = time.time()\r\n    predict_scores = list()\r\n    for i in tqdm(xrange(n_te_batches)):\r\n        [X1_batch, X2_batch] = get_test_batch(i)\r\n        _batch_size = len(X1_batch)\r\n        fetches = [model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        predict_labels = sess.run(fetches, feed_dict)[0]\r\n        predict_scores.append(predict_labels)\r\n    predict_scores = np.vstack(np.asarray(predict_scores))\r\n    scores_name = scores_path + model_name + \'.npy\'\r\n    np.save(scores_name, predict_scores)\r\n    print(\'scores.shape=\', predict_scores.shape)\r\n    print(\'Writed the scores into %s, time %g s\' % (scores_name, time.time() - time0))\r\n\r\n\r\ndef main(_):\r\n    if not os.path.exists(ckpt_path + \'checkpoint\'):\r\n        print(\'there is not saved model, please check the ckpt path\')\r\n        exit()\r\n    print(\'Loading model...\')\r\n    W_embedding = np.load(embedding_path)\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    with tf.Session(config=config) as sess:\r\n        model = network.RCNN(W_embedding, settings)\r\n        model.saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\r\n        print(\'Local predicting...\')\r\n        local_predict(sess, model)\r\n        print(\'Test predicting...\')\r\n        predict(sess, model)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    tf.app.run()\r\n'"
models/wd_6_rcnn/train.py,21,"b'# -*- coding:utf-8 -*-\r\n\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nimport os\r\nimport sys\r\nimport shutil\r\nimport time\r\nimport network\r\n\r\nsys.path.append(\'../..\')\r\nfrom data_helpers import to_categorical\r\nfrom evaluator import score_eval\r\n\r\nflags = tf.flags\r\nflags.DEFINE_bool(\'is_retrain\', False, \'if is_retrain is true, not rebuild the summary\')\r\nflags.DEFINE_integer(\'max_epoch\', 1, \'update the embedding after max_epoch, default: 1\')\r\nflags.DEFINE_integer(\'max_max_epoch\', 6, \'all training epoches, default: 6\')\r\nflags.DEFINE_float(\'lr\', 8e-4, \'initial learning rate, default: 8e-4\')\r\nflags.DEFINE_float(\'decay_rate\', 0.75, \'decay rate, default: 0.75\')\r\nflags.DEFINE_float(\'keep_prob\', 0.5, \'keep_prob for training, default: 0.5\')\r\n# \xe6\xad\xa3\xe5\xbc\x8f\r\n# flags.DEFINE_integer(\'decay_step\', 15000, \'decay_step, default: 15000\')\r\n# flags.DEFINE_integer(\'valid_step\', 10000, \'valid_step, default: 10000\')\r\n# flags.DEFINE_float(\'last_f1\', 0.40, \'if valid_f1 > last_f1, save new model. default: 0.40\')\r\n\r\n# \xe6\xb5\x8b\xe8\xaf\x95\r\nflags.DEFINE_integer(\'decay_step\', 1000, \'decay_step, default: 1000\')\r\nflags.DEFINE_integer(\'valid_step\', 500, \'valid_step, default: 500\')\r\nflags.DEFINE_float(\'last_f1\', 0.10, \'if valid_f1 > last_f1, save new model. default: 0.10\')\r\nFLAGS = flags.FLAGS\r\n\r\nlr = FLAGS.lr\r\nlast_f1 = FLAGS.last_f1\r\nsettings = network.Settings()\r\ntitle_len = settings.title_len\r\nsummary_path = settings.summary_path\r\nckpt_path = settings.ckpt_path\r\nmodel_path = ckpt_path + \'model.ckpt\'\r\n\r\nembedding_path = \'../../data/word_embedding.npy\'\r\ndata_train_path = \'../../data/wd-data/data_train/\'\r\ndata_valid_path = \'../../data/wd-data/data_valid/\'\r\ntr_batches = os.listdir(data_train_path)  # batch \xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8\r\nva_batches = os.listdir(data_valid_path)\r\nn_tr_batches = len(tr_batches)\r\nn_va_batches = len(va_batches)\r\n\r\n# \xe6\xb5\x8b\xe8\xaf\x95\r\nn_tr_batches = 1000\r\nn_va_batches = 50\r\n\r\n\r\ndef get_batch(data_path, batch_id):\r\n    """"""get a batch from data_path""""""\r\n    new_batch = np.load(data_path + str(batch_id) + \'.npz\')\r\n    X_batch = new_batch[\'X\']\r\n    y_batch = new_batch[\'y\']\r\n    X1_batch = X_batch[:, :title_len]\r\n    X2_batch = X_batch[:, title_len:]\r\n    return [X1_batch, X2_batch, y_batch]\r\n\r\n\r\ndef valid_epoch(data_path, sess, model):\r\n    """"""Test on the valid data.""""""\r\n    va_batches = os.listdir(data_path)\r\n    n_va_batches = len(va_batches)\r\n    _costs = 0.0\r\n    predict_labels_list = list()  # \xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\r\n    marked_labels_list = list()\r\n    for i in xrange(n_va_batches):\r\n        [X1_batch, X2_batch, y_batch] = get_batch(data_path, i)\r\n        marked_labels_list.extend(y_batch)\r\n        y_batch = to_categorical(y_batch)\r\n        _batch_size = len(y_batch)\r\n        fetches = [model.loss, model.y_pred]\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                     model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n        _cost, predict_labels = sess.run(fetches, feed_dict)\r\n        _costs += _cost\r\n        predict_labels = map(lambda label: label.argsort()[-1:-6:-1], predict_labels)  # \xe5\x8f\x96\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x845\xe4\xb8\xaa\xe4\xb8\x8b\xe6\xa0\x87\r\n        predict_labels_list.extend(predict_labels)\r\n    predict_label_and_marked_label_list = zip(predict_labels_list, marked_labels_list)\r\n    precision, recall, f1 = score_eval(predict_label_and_marked_label_list)\r\n    mean_cost = _costs / n_va_batches\r\n    return mean_cost, precision, recall, f1\r\n\r\n\r\ndef train_epoch(data_path, sess, model, train_fetches, valid_fetches, train_writer, test_writer):\r\n    global last_f1\r\n    global lr\r\n    time0 = time.time()\r\n    batch_indexs = np.random.permutation(n_tr_batches)  # shuffle the training data\r\n    for batch in tqdm(xrange(n_tr_batches)):\r\n        global_step = sess.run(model.global_step)\r\n        if 0 == (global_step + 1) % FLAGS.valid_step:\r\n            valid_cost, precision, recall, f1 = valid_epoch(data_valid_path, sess, model)\r\n            print(\'Global_step=%d: valid cost=%g; p=%g, r=%g, f1=%g, time=%g s\' % (\r\n                global_step, valid_cost, precision, recall, f1, time.time() - time0))\r\n            time0 = time.time()\r\n            if f1 > last_f1:\r\n                last_f1 = f1\r\n                saving_path = model.saver.save(sess, model_path, global_step+1)\r\n                print(\'saved new model to %s \' % saving_path)\r\n        # training\r\n        batch_id = batch_indexs[batch]\r\n        [X1_batch, X2_batch, y_batch] = get_batch(data_train_path, batch_id)\r\n        y_batch = to_categorical(y_batch)\r\n        _batch_size = len(y_batch)\r\n        feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                     model.batch_size: _batch_size, model.tst: False, model.keep_prob: FLAGS.keep_prob}\r\n        summary, _cost, _, _ = sess.run(train_fetches, feed_dict)  # the cost is the mean cost of one batch\r\n        # valid per 500 steps\r\n        if 0 == (global_step + 1) % 500:\r\n            train_writer.add_summary(summary, global_step)\r\n            batch_id = np.random.randint(0, n_va_batches)  # \xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe4\xb8\x80\xe4\xb8\xaa\xe9\xaa\x8c\xe8\xaf\x81batch\r\n            [X1_batch, X2_batch, y_batch] = get_batch(data_valid_path, batch_id)\r\n            y_batch = to_categorical(y_batch)\r\n            _batch_size = len(y_batch)\r\n            feed_dict = {model.X1_inputs: X1_batch, model.X2_inputs: X2_batch, model.y_inputs: y_batch,\r\n                         model.batch_size: _batch_size, model.tst: True, model.keep_prob: 1.0}\r\n            summary, _cost = sess.run(valid_fetches, feed_dict)\r\n            test_writer.add_summary(summary, global_step)\r\n\r\n\r\ndef main(_):\r\n    global ckpt_path\r\n    global last_f1\r\n    if not os.path.exists(ckpt_path):\r\n        os.makedirs(ckpt_path)\r\n    if not os.path.exists(summary_path):\r\n        os.makedirs(summary_path)\r\n    elif not FLAGS.is_retrain:  # \xe9\x87\x8d\xe6\x96\xb0\xe8\xae\xad\xe7\xbb\x83\xe6\x9c\xac\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\x88\xa0\xe9\x99\xa4\xe4\xbb\xa5\xe5\x89\x8d\xe7\x9a\x84 summary\r\n        shutil.rmtree(summary_path)\r\n        os.makedirs(summary_path)\r\n    if not os.path.exists(summary_path):\r\n        os.makedirs(summary_path)\r\n\r\n    print(\'1.Loading data...\')\r\n    W_embedding = np.load(embedding_path)\r\n    print(\'training sample_num = %d\' % n_tr_batches)\r\n    print(\'valid sample_num = %d\' % n_va_batches)\r\n\r\n    # Initial or restore the model\r\n    print(\'2.Building model...\')\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    with tf.Session(config=config) as sess:\r\n        model = network.RCNN(W_embedding, settings)\r\n        with tf.variable_scope(\'training_ops\') as vs:\r\n            learning_rate = tf.train.exponential_decay(FLAGS.lr, model.global_step, FLAGS.decay_step,\r\n                                                   FLAGS.decay_rate, staircase=True)\r\n            # two optimizer: op1, update embedding; op2, do not update embedding.\r\n            with tf.variable_scope(\'Optimizer1\'):\r\n                tvars1 = tf.trainable_variables()\r\n                grads1 = tf.gradients(model.loss, tvars1)\r\n                optimizer1 = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n                train_op1 = optimizer1.apply_gradients(zip(grads1, tvars1),\r\n                                                   global_step=model.global_step)\r\n            with tf.variable_scope(\'Optimizer2\'):\r\n                tvars2 = [tvar for tvar in tvars1 if \'embedding\' not in tvar.name]\r\n                grads2 = tf.gradients(model.loss, tvars2)\r\n                optimizer2 = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n                train_op2 = optimizer2.apply_gradients(zip(grads2, tvars2),\r\n                                                   global_step=model.global_step)\r\n            update_op = tf.group(*model.update_emas)\r\n            merged = tf.summary.merge_all()  # summary\r\n            train_writer = tf.summary.FileWriter(summary_path + \'train\', sess.graph)\r\n            test_writer = tf.summary.FileWriter(summary_path + \'test\')\r\n            training_ops = [v for v in tf.global_variables() if v.name.startswith(vs.name+\'/\')]\r\n\r\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe5\xb7\xb2\xe7\xbb\x8f\xe4\xbf\x9d\xe5\xad\x98\xe8\xbf\x87\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\xaf\xbc\xe5\x85\xa5\xe4\xb8\x8a\xe6\xac\xa1\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\r\n        if os.path.exists(ckpt_path + ""checkpoint""):\r\n            print(""Restoring Variables from Checkpoint..."")\r\n            model.saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\r\n            last_valid_cost, precision, recall, last_f1 = valid_epoch(data_valid_path, sess, model)\r\n            print(\' valid cost=%g; p=%g, r=%g, f1=%g\' % (last_valid_cost, precision, recall, last_f1))\r\n            sess.run(tf.variables_initializer(training_ops))\r\n            train_op2 = train_op1\r\n        else:\r\n            print(\'Initializing Variables...\')\r\n            sess.run(tf.global_variables_initializer())\r\n\r\n        print(\'3.Begin training...\')\r\n        print(\'max_epoch=%d, max_max_epoch=%d\' % (FLAGS.max_epoch, FLAGS.max_max_epoch))\r\n        train_op = train_op2\r\n        for epoch in xrange(FLAGS.max_max_epoch):\r\n            global_step = sess.run(model.global_step)\r\n            print(\'Global step %d, lr=%g\' % (global_step, sess.run(learning_rate)))\r\n            if epoch == FLAGS.max_epoch:  # update the embedding\r\n                train_op = train_op1\r\n            train_fetches = [merged, model.loss, train_op, update_op]\r\n            valid_fetches = [merged, model.loss]\r\n            train_epoch(data_train_path, sess, model, train_fetches, valid_fetches, train_writer, test_writer)\r\n        # \xe6\x9c\x80\xe5\x90\x8e\xe5\x86\x8d\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\r\n        valid_cost, precision, recall, f1 = valid_epoch(data_valid_path, sess, model)\r\n        print(\'END.Global_step=%d: valid cost=%g; p=%g, r=%g, f1=%g\' % (\r\n            sess.run(model.global_step), valid_cost, precision, recall, f1))\r\n        if f1 > last_f1:  # save the better model\r\n            saving_path = model.saver.save(sess, model_path, sess.run(model.global_step)+1)\r\n            print(\'saved new model to %s \' % saving_path)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    tf.app.run()\r\n'"
