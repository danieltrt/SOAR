file_path,api_count,code
deep_maxent_irl.py,18,"b'import numpy as np\nimport tensorflow as tf\nimport mdp.gridworld as gridworld\nimport mdp.value_iteration as value_iteration\nimport img_utils\nimport tf_utils\nfrom utils import *\n\n\n\nclass DeepIRLFC:\n\n\n  def __init__(self, n_input, lr, n_h1=400, n_h2=300, l2=10, name=\'deep_irl_fc\'):\n    self.n_input = n_input\n    self.lr = lr\n    self.n_h1 = n_h1\n    self.n_h2 = n_h2\n    self.name = name\n\n    self.sess = tf.Session()\n    self.input_s, self.reward, self.theta = self._build_network(self.name)\n    self.optimizer = tf.train.GradientDescentOptimizer(lr)\n    \n    self.grad_r = tf.placeholder(tf.float32, [None, 1])\n    self.l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in self.theta])\n    self.grad_l2 = tf.gradients(self.l2_loss, self.theta)\n\n    self.grad_theta = tf.gradients(self.reward, self.theta, -self.grad_r)\n    # apply l2 loss gradients\n    self.grad_theta = [tf.add(l2*self.grad_l2[i], self.grad_theta[i]) for i in range(len(self.grad_l2))]\n    self.grad_theta, _ = tf.clip_by_global_norm(self.grad_theta, 100.0)\n\n    self.grad_norms = tf.global_norm(self.grad_theta)\n    self.optimize = self.optimizer.apply_gradients(zip(self.grad_theta, self.theta))\n    self.sess.run(tf.global_variables_initializer())\n\n\n  def _build_network(self, name):\n    input_s = tf.placeholder(tf.float32, [None, self.n_input])\n    with tf.variable_scope(name):\n      fc1 = tf_utils.fc(input_s, self.n_h1, scope=""fc1"", activation_fn=tf.nn.elu,\n        initializer=tf.contrib.layers.variance_scaling_initializer(mode=""FAN_IN""))\n      fc2 = tf_utils.fc(fc1, self.n_h2, scope=""fc2"", activation_fn=tf.nn.elu,\n        initializer=tf.contrib.layers.variance_scaling_initializer(mode=""FAN_IN""))\n      reward = tf_utils.fc(fc2, 1, scope=""reward"")\n    theta = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n    return input_s, reward, theta\n\n\n  def get_theta(self):\n    return self.sess.run(self.theta)\n\n\n  def get_rewards(self, states):\n    rewards = self.sess.run(self.reward, feed_dict={self.input_s: states})\n    return rewards\n\n\n  def apply_grads(self, feat_map, grad_r):\n    grad_r = np.reshape(grad_r, [-1, 1])\n    feat_map = np.reshape(feat_map, [-1, self.n_input])\n    _, grad_theta, l2_loss, grad_norms = self.sess.run([self.optimize, self.grad_theta, self.l2_loss, self.grad_norms], \n      feed_dict={self.grad_r: grad_r, self.input_s: feat_map})\n    return grad_theta, l2_loss, grad_norms\n\n\n\ndef compute_state_visition_freq(P_a, gamma, trajs, policy, deterministic=True):\n  """"""compute the expected states visition frequency p(s| theta, T) \n  using dynamic programming\n\n  inputs:\n    P_a     NxNxN_ACTIONS matrix - transition dynamics\n    gamma   float - discount factor\n    trajs   list of list of Steps - collected from expert\n    policy  Nx1 vector (or NxN_ACTIONS if deterministic=False) - policy\n\n  \n  returns:\n    p       Nx1 vector - state visitation frequencies\n  """"""\n  N_STATES, _, N_ACTIONS = np.shape(P_a)\n\n  T = len(trajs[0])\n  # mu[s, t] is the prob of visiting state s at time t\n  mu = np.zeros([N_STATES, T]) \n\n  for traj in trajs:\n    mu[traj[0].cur_state, 0] += 1\n  mu[:,0] = mu[:,0]/len(trajs)\n\n  for s in range(N_STATES):\n    for t in range(T-1):\n      if deterministic:\n        mu[s, t+1] = sum([mu[pre_s, t]*P_a[pre_s, s, int(policy[pre_s])] for pre_s in range(N_STATES)])\n      else:\n        mu[s, t+1] = sum([sum([mu[pre_s, t]*P_a[pre_s, s, a1]*policy[pre_s, a1] for a1 in range(N_ACTIONS)]) for pre_s in range(N_STATES)])\n  p = np.sum(mu, 1)\n  return p\n\n\ndef demo_svf(trajs, n_states):\n  """"""\n  compute state visitation frequences from demonstrations\n  \n  input:\n    trajs   list of list of Steps - collected from expert\n  returns:\n    p       Nx1 vector - state visitation frequences   \n  """"""\n\n  p = np.zeros(n_states)\n  for traj in trajs:\n    for step in traj:\n      p[step.cur_state] += 1\n  p = p/len(trajs)\n  return p\n\ndef deep_maxent_irl(feat_map, P_a, gamma, trajs, lr, n_iters):\n  """"""\n  Maximum Entropy Inverse Reinforcement Learning (Maxent IRL)\n\n  inputs:\n    feat_map    NxD matrix - the features for each state\n    P_a         NxNxN_ACTIONS matrix - P_a[s0, s1, a] is the transition prob of \n                                       landing at state s1 when taking action \n                                       a at state s0\n    gamma       float - RL discount factor\n    trajs       a list of demonstrations\n    lr          float - learning rate\n    n_iters     int - number of optimization steps\n\n  returns\n    rewards     Nx1 vector - recoverred state rewards\n  """"""\n\n  # tf.set_random_seed(1)\n  \n  N_STATES, _, N_ACTIONS = np.shape(P_a)\n\n  # init nn model\n  nn_r = DeepIRLFC(feat_map.shape[1], lr, 3, 3)\n\n  # find state visitation frequencies using demonstrations\n  mu_D = demo_svf(trajs, N_STATES)\n\n  # training \n  for iteration in range(n_iters):\n    if iteration % (n_iters/10) == 0:\n      print \'iteration: {}\'.format(iteration)\n    \n    # compute the reward matrix\n    rewards = nn_r.get_rewards(feat_map)\n    \n    # compute policy \n    _, policy = value_iteration.value_iteration(P_a, rewards, gamma, error=0.01, deterministic=True)\n    \n    # compute expected svf\n    mu_exp = compute_state_visition_freq(P_a, gamma, trajs, policy, deterministic=True)\n    \n    # compute gradients on rewards:\n    grad_r = mu_D - mu_exp\n\n    # apply gradients to the neural network\n    grad_theta, l2_loss, grad_norm = nn_r.apply_grads(feat_map, grad_r)\n    \n\n  rewards = nn_r.get_rewards(feat_map)\n  # return sigmoid(normalize(rewards))\n  return normalize(rewards)\n\n\n\n\n\n'"
deep_maxent_irl_gridworld.py,0,"b'import numpy as np\nimport matplotlib.pyplot as plt\nimport argparse\nfrom collections import namedtuple\n\n\nimport img_utils\nfrom mdp import gridworld\nfrom mdp import value_iteration\nfrom deep_maxent_irl import *\nfrom maxent_irl import *\nfrom utils import *\nfrom lp_irl import *\n\nStep = namedtuple(\'Step\',\'cur_state action next_state reward done\')\n\n\nPARSER = argparse.ArgumentParser(description=None)\nPARSER.add_argument(\'-hei\', \'--height\', default=5, type=int, help=\'height of the gridworld\')\nPARSER.add_argument(\'-wid\', \'--width\', default=5, type=int, help=\'width of the gridworld\')\nPARSER.add_argument(\'-g\', \'--gamma\', default=0.9, type=float, help=\'discount factor\')\nPARSER.add_argument(\'-a\', \'--act_random\', default=0.3, type=float, help=\'probability of acting randomly\')\nPARSER.add_argument(\'-t\', \'--n_trajs\', default=200, type=int, help=\'number of expert trajectories\')\nPARSER.add_argument(\'-l\', \'--l_traj\', default=20, type=int, help=\'length of expert trajectory\')\nPARSER.add_argument(\'--rand_start\', dest=\'rand_start\', action=\'store_true\', help=\'when sampling trajectories, randomly pick start positions\')\nPARSER.add_argument(\'--no-rand_start\', dest=\'rand_start\',action=\'store_false\', help=\'when sampling trajectories, fix start positions\')\nPARSER.set_defaults(rand_start=True)\nPARSER.add_argument(\'-lr\', \'--learning_rate\', default=0.02, type=float, help=\'learning rate\')\nPARSER.add_argument(\'-ni\', \'--n_iters\', default=20, type=int, help=\'number of iterations\')\nARGS = PARSER.parse_args()\nprint ARGS\n\n\nGAMMA = ARGS.gamma\nACT_RAND = ARGS.act_random\nR_MAX = 1 # the constant r_max does not affect much the recoverred reward distribution\nH = ARGS.height\nW = ARGS.width\nN_TRAJS = ARGS.n_trajs\nL_TRAJ = ARGS.l_traj\nRAND_START = ARGS.rand_start\nLEARNING_RATE = ARGS.learning_rate\nN_ITERS = ARGS.n_iters\n\n\ndef generate_demonstrations(gw, policy, n_trajs=100, len_traj=20, rand_start=False, start_pos=[0,0]):\n  """"""gatheres expert demonstrations\n\n  inputs:\n  gw          Gridworld - the environment\n  policy      Nx1 matrix\n  n_trajs     int - number of trajectories to generate\n  rand_start  bool - randomly picking start position or not\n  start_pos   2x1 list - set start position, default [0,0]\n  returns:\n  trajs       a list of trajectories - each element in the list is a list of Steps representing an episode\n  """"""\n\n  trajs = []\n  for i in range(n_trajs):\n    if rand_start:\n      # override start_pos\n      start_pos = [np.random.randint(0, gw.height), np.random.randint(0, gw.width)]\n\n    episode = []\n    gw.reset(start_pos)\n    cur_state = start_pos\n    cur_state, action, next_state, reward, is_done = gw.step(int(policy[gw.pos2idx(cur_state)]))\n    episode.append(Step(cur_state=gw.pos2idx(cur_state), action=action, next_state=gw.pos2idx(next_state), reward=reward, done=is_done))\n    # while not is_done:\n    for _ in range(len_traj):\n        cur_state, action, next_state, reward, is_done = gw.step(int(policy[gw.pos2idx(cur_state)]))\n        episode.append(Step(cur_state=gw.pos2idx(cur_state), action=action, next_state=gw.pos2idx(next_state), reward=reward, done=is_done))\n        if is_done:\n            break\n    trajs.append(episode)\n  return trajs\n\n\ndef main():\n  N_STATES = H * W\n  N_ACTIONS = 5\n\n  rmap_gt = np.zeros([H, W])\n  rmap_gt[H-1, W-1] = R_MAX\n  rmap_gt[0, W-1] = R_MAX\n  rmap_gt[H-1, 0] = R_MAX\n\n  gw = gridworld.GridWorld(rmap_gt, {}, 1 - ACT_RAND)\n\n  rewards_gt = np.reshape(rmap_gt, H*W, order=\'F\')\n  P_a = gw.get_transition_mat()\n\n  values_gt, policy_gt = value_iteration.value_iteration(P_a, rewards_gt, GAMMA, error=0.01, deterministic=True)\n  \n  # use identity matrix as feature\n  feat_map = np.eye(N_STATES)\n\n  trajs = generate_demonstrations(gw, policy_gt, n_trajs=N_TRAJS, len_traj=L_TRAJ, rand_start=RAND_START)\n  \n  print \'Deep Max Ent IRL training ..\'\n  rewards = deep_maxent_irl(feat_map, P_a, GAMMA, trajs, LEARNING_RATE, N_ITERS)\n\n  values, _ = value_iteration.value_iteration(P_a, rewards, GAMMA, error=0.01, deterministic=True)\n  # plots\n  plt.figure(figsize=(20,4))\n  plt.subplot(1, 4, 1)\n  img_utils.heatmap2d(np.reshape(rewards_gt, (H,W), order=\'F\'), \'Rewards Map - Ground Truth\', block=False)\n  plt.subplot(1, 4, 2)\n  img_utils.heatmap2d(np.reshape(values_gt, (H,W), order=\'F\'), \'Value Map - Ground Truth\', block=False)\n  plt.subplot(1, 4, 3)\n  img_utils.heatmap2d(np.reshape(rewards, (H,W), order=\'F\'), \'Reward Map - Recovered\', block=False)\n  plt.subplot(1, 4, 4)\n  img_utils.heatmap2d(np.reshape(values, (H,W), order=\'F\'), \'Value Map - Recovered\', block=False)\n  plt.show()\n\n\n\n\nif __name__ == ""__main__"":\n  main()\n'"
demo.py,0,"b'import numpy as np\nimport matplotlib.pyplot as plt\nimport argparse\nfrom collections import namedtuple\n\n\nimport img_utils\nfrom mdp import gridworld\nfrom mdp import value_iteration\nfrom deep_maxent_irl import *\nfrom maxent_irl import *\nfrom utils import *\nfrom lp_irl import *\n\nStep = namedtuple(\'Step\',\'cur_state action next_state reward done\')\n\n\nPARSER = argparse.ArgumentParser(description=None)\nPARSER.add_argument(\'-hei\', \'--height\', default=5, type=int, help=\'height of the gridworld\')\nPARSER.add_argument(\'-wid\', \'--width\', default=5, type=int, help=\'width of the gridworld\')\nPARSER.add_argument(\'-g\', \'--gamma\', default=0.9, type=float, help=\'discount factor\')\nPARSER.add_argument(\'-a\', \'--act_random\', default=0.3, type=float, help=\'probability of acting randomly\')\nPARSER.add_argument(\'-t\', \'--n_trajs\', default=200, type=int, help=\'number of expert trajectories\')\nPARSER.add_argument(\'-l\', \'--l_traj\', default=20, type=int, help=\'length of expert trajectory\')\nPARSER.add_argument(\'--rand_start\', dest=\'rand_start\', action=\'store_true\', help=\'when sampling trajectories, randomly pick start positions\')\nPARSER.add_argument(\'--no-rand_start\', dest=\'rand_start\',action=\'store_false\', help=\'when sampling trajectories, fix start positions\')\nPARSER.set_defaults(rand_start=True)\nPARSER.add_argument(\'-lr\', \'--learning_rate\', default=0.02, type=float, help=\'learning rate\')\nPARSER.add_argument(\'-ni\', \'--n_iters\', default=20, type=int, help=\'number of iterations\')\nARGS = PARSER.parse_args()\nprint ARGS\n\n\nGAMMA = ARGS.gamma\nACT_RAND = ARGS.act_random\nR_MAX = 1 # the constant r_max does not affect much the recoverred reward distribution\nH = ARGS.height\nW = ARGS.width\nN_TRAJS = ARGS.n_trajs\nL_TRAJ = ARGS.l_traj\nRAND_START = ARGS.rand_start\nLEARNING_RATE = ARGS.learning_rate\nN_ITERS = ARGS.n_iters\n\n\ndef generate_demonstrations(gw, policy, n_trajs=100, len_traj=20, rand_start=False, start_pos=[0,0]):\n  """"""gatheres expert demonstrations\n\n  inputs:\n  gw          Gridworld - the environment\n  policy      Nx1 matrix\n  n_trajs     int - number of trajectories to generate\n  rand_start  bool - randomly picking start position or not\n  start_pos   2x1 list - set start position, default [0,0]\n  returns:\n  trajs       a list of trajectories - each element in the list is a list of Steps representing an episode\n  """"""\n\n  trajs = []\n  for i in range(n_trajs):\n    if rand_start:\n      # override start_pos\n      start_pos = [np.random.randint(0, gw.height), np.random.randint(0, gw.width)]\n\n    episode = []\n    gw.reset(start_pos)\n    cur_state = start_pos\n    cur_state, action, next_state, reward, is_done = gw.step(int(policy[gw.pos2idx(cur_state)]))\n    episode.append(Step(cur_state=gw.pos2idx(cur_state), action=action, next_state=gw.pos2idx(next_state), reward=reward, done=is_done))\n    # while not is_done:\n    for _ in range(len_traj):\n        cur_state, action, next_state, reward, is_done = gw.step(int(policy[gw.pos2idx(cur_state)]))\n        episode.append(Step(cur_state=gw.pos2idx(cur_state), action=action, next_state=gw.pos2idx(next_state), reward=reward, done=is_done))\n        if is_done:\n            break\n    trajs.append(episode)\n  return trajs\n\n\ndef main():\n  N_STATES = H * W\n  N_ACTIONS = 5\n\n  rmap_gt = np.zeros([H, W])\n  rmap_gt[H-2, W-2] = R_MAX\n  rmap_gt[1, 1] = R_MAX\n  # rmap_gt[H/2, W/2] = R_MAX\n\n\n  gw = gridworld.GridWorld(rmap_gt, {}, 1 - ACT_RAND)\n\n  rewards_gt = np.reshape(rmap_gt, H*W, order=\'F\')\n  P_a = gw.get_transition_mat()\n\n  values_gt, policy_gt = value_iteration.value_iteration(P_a, rewards_gt, GAMMA, error=0.01, deterministic=True)\n  \n  rewards_gt = normalize(values_gt)\n  gw = gridworld.GridWorld(np.reshape(rewards_gt, (H,W), order=\'F\'), {}, 1 - ACT_RAND)\n  P_a = gw.get_transition_mat()\n  values_gt, policy_gt = value_iteration.value_iteration(P_a, rewards_gt, GAMMA, error=0.01, deterministic=True)\n\n\n  # use identity matrix as feature\n  feat_map = np.eye(N_STATES)\n\n  trajs = generate_demonstrations(gw, policy_gt, n_trajs=N_TRAJS, len_traj=L_TRAJ, rand_start=RAND_START)\n  print \'LP IRL training ..\'\n  rewards_lpirl = lp_irl(P_a, policy_gt, gamma=0.3, l1=10, R_max=R_MAX)\n  print \'Max Ent IRL training ..\'\n  rewards_maxent = maxent_irl(feat_map, P_a, GAMMA, trajs, LEARNING_RATE*2, N_ITERS*2)\n  print \'Deep Max Ent IRL training ..\'\n  rewards = deep_maxent_irl(feat_map, P_a, GAMMA, trajs, LEARNING_RATE, N_ITERS)\n\n  # plots\n  plt.figure(figsize=(20,4))\n  plt.subplot(1, 4, 1)\n  img_utils.heatmap2d(np.reshape(rewards_gt, (H,W), order=\'F\'), \'Rewards Map - Ground Truth\', block=False)\n  plt.subplot(1, 4, 2)\n  img_utils.heatmap2d(np.reshape(rewards_lpirl, (H,W), order=\'F\'), \'Reward Map - LP\', block=False)\n  plt.subplot(1, 4, 3)\n  img_utils.heatmap2d(np.reshape(rewards_maxent, (H,W), order=\'F\'), \'Reward Map - Maxent\', block=False)\n  plt.subplot(1, 4, 4)\n  img_utils.heatmap2d(np.reshape(rewards, (H,W), order=\'F\'), \'Reward Map - Deep Maxent\', block=False)\n  plt.show()\n\n\n\n\nif __name__ == ""__main__"":\n  main()\n'"
demo_gridworld1d.py,0,"b'import numpy as np\nimport matplotlib.pyplot as plt\nimport argparse\n\nimport img_utils\nfrom mdp import gridworld1d\nfrom mdp import value_iteration\nfrom lp_irl import *\nfrom maxent_irl import *\nfrom deep_maxent_irl import *\n\n\nPARSER = argparse.ArgumentParser(description=None)\nPARSER.add_argument(\'-ns\', \'--n_states\', default=100, type=int, help=\'number of states in the 1d gridworld\')\nPARSER.add_argument(\'-g\', \'--gamma\', default=0.5, type=float, help=\'discount factor\')\nPARSER.add_argument(\'-a\', \'--act_random\', default=0.0, type=float, help=\'probability of acting randomly\')\nPARSER.add_argument(\'-t\', \'--n_trajs\', default=500, type=int, help=\'number of expert trajectories\')\nPARSER.add_argument(\'-l\', \'--l_traj\', default=20, type=int, help=\'length of expert trajectory\')\nPARSER.add_argument(\'--rand_start\', dest=\'rand_start\', action=\'store_true\', help=\'when sampling trajectories, randomly pick start positions\')\nPARSER.add_argument(\'--no-rand_start\', dest=\'rand_start\',action=\'store_false\', help=\'when sampling trajectories, fix start positions\')\nPARSER.set_defaults(rand_start=True)\nPARSER.add_argument(\'-lr\', \'--learning_rate\', default=0.02, type=float, help=\'learning rate\')\nPARSER.add_argument(\'-ni\', \'--n_iters\', default=20, type=int, help=\'number of iterations\')\nARGS = PARSER.parse_args()\nprint ARGS\n\n\nGAMMA = ARGS.gamma\nACT_RAND = ARGS.act_random\nR_MAX = 1 # the constant r_max does not affect much the recoverred reward distribution\nN_STATES = ARGS.n_states\nN_ACTIONS = 2\nN_TRAJS = ARGS.n_trajs\nL_TRAJ = ARGS.l_traj\nRAND_START = ARGS.rand_start\nLEARNING_RATE = ARGS.learning_rate\nN_ITERS = ARGS.n_iters\nSIGMA = 0.5\n\n\ndef to_plot(map, n=N_STATES):\n  return np.repeat(np.reshape(map, [n,1]), 10, axis=1)\n\ndef feat(s):\n  feat_vec = np.zeros(N_STATES)\n  for i in range(N_STATES):\n    # by approximity\n    feat_vec[i] = np.exp(-abs(s-i)/(2*SIGMA**2))\n  return feat_vec\n\ndef main():\n  # init the gridworld\n  rmap_gt = np.zeros(N_STATES)\n  rmap_gt[N_STATES-5] = R_MAX\n  rmap_gt[10] = R_MAX\n\n  gw = gridworld1d.GridWorld1D(rmap_gt, {}, ACT_RAND)\n  P_a = gw.get_transition_mat()\n  values_gt, policy_gt = value_iteration.value_iteration(P_a, rmap_gt, GAMMA, error=0.01, deterministic=True)\n\n  # gradient rewards \n  rmap_gt = values_gt\n  gw = gridworld1d.GridWorld1D(rmap_gt, {}, ACT_RAND)\n  P_a = gw.get_transition_mat()\n  values_gt, policy_gt = value_iteration.value_iteration(P_a, rmap_gt, GAMMA, error=0.01, deterministic=True)\n\n  # np.random.seed(1)\n  trajs = gw.generate_demonstrations(policy_gt, n_trajs=N_TRAJS, len_traj=L_TRAJ, rand_start=RAND_START)  \n  \n  # feat_map = np.eye(N_STATES)\n  feat_map = np.array([feat(s) for s in range(N_STATES)])\n  test_irl_algorithms(gw, P_a, rmap_gt, policy_gt, trajs, feat_map)\n\n\n\ndef test_irl_algorithms(gw, P_a, rmap_gt, policy_gt, trajs, feat_map):\n  print \'LP IRL training ..\'\n  rewards_lpirl = lp_irl(P_a, policy_gt, gamma=0.3, l1=10, R_max=R_MAX)\n  print \'Max Ent IRL training ..\'\n  rewards_maxent = maxent_irl(feat_map, P_a, GAMMA, trajs, LEARNING_RATE*2, N_ITERS*2)\n  print \'Deep Max Ent IRL training ..\'\n  rewards = deep_maxent_irl(feat_map, P_a, GAMMA, trajs, LEARNING_RATE, N_ITERS)    \n  values, _ = value_iteration.value_iteration(P_a, rewards, GAMMA, error=0.01, deterministic=True)\n\n  # plots\n  plt.figure(figsize=(20,8))\n  plt.subplot(1, 4, 1)\n  img_utils.heatmap2d(to_plot(rmap_gt), \'Rewards Map - Ground Truth\', block=False, text=False)\n  plt.subplot(1, 4, 2)\n  img_utils.heatmap2d(to_plot(rewards_lpirl), \'Reward Map - LP\', block=False, text=False)\n  plt.subplot(1, 4, 3)\n  img_utils.heatmap2d(to_plot(rewards_maxent), \'Reward Map - Maxent\', block=False, text=False)\n  plt.subplot(1, 4, 4)\n  img_utils.heatmap2d(to_plot(rewards), \'Reward Map - Deep Maxent\', block=False, text=False)\n  plt.show()\n\n\nif __name__ == ""__main__"":\n  main()'"
img_utils.py,0,"b'""""""Utility functions for process and visualize images""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef show_img(img):\n  print img.shape, img.dtype\n  plt.imshow(img[:,:,0])\n  plt.ion()\n  plt.show()\n  raw_input()\n\n\ndef heatmap2d(hm_mat, title=\'\', block=True, fig_num=1, text=True):\n  """"""\n  Display heatmap\n  input:\n    hm_mat:   mxn 2d np array\n  """"""\n  print \'map shape: {}, data type: {}\'.format(hm_mat.shape, hm_mat.dtype)\n\n  if block:\n    plt.figure(fig_num)\n    plt.clf()\n  \n  # plt.imshow(hm_mat, cmap=\'hot\', interpolation=\'nearest\')\n  plt.imshow(hm_mat, interpolation=\'nearest\')\n  plt.title(title)\n  plt.colorbar()\n  \n  if text:\n    for y in range(hm_mat.shape[0]):\n      for x in range(hm_mat.shape[1]):\n        plt.text(x, y, \'%.1f\' % hm_mat[y, x],\n                 horizontalalignment=\'center\',\n                 verticalalignment=\'center\',\n                 )\n\n  if block:\n    plt.ion()\n    print \'press enter to continue\'\n    plt.show()\n    raw_input()\n\n\ndef heatmap3d(hm_mat, title=\'\'):\n  from mpl_toolkits.mplot3d import Axes3D\n  import matplotlib.pyplot as plt\n  import numpy as np\n  #\n  # Assuming you have ""2D"" dataset like the following that you need\n  # to plot.\n  #\n  data_2d = hm_mat\n  #\n  # Convert it into an numpy array.\n  #\n  data_array = np.array(data_2d)\n  #\n  # Create a figure for plotting the data as a 3D histogram.\n  #\n  fig = plt.figure()\n  ax = fig.add_subplot(111, projection=\'3d\')\n  plt.title(title)\n  # _, ax = fig.add_subplot(111)\n  #\n  # Create an X-Y mesh of the same dimension as the 2D data. You can\n  # think of this as the floor of the plot.\n  #\n  x_data, y_data = np.meshgrid( np.arange(data_array.shape[1]),\n                                np.arange(data_array.shape[0]) )\n  #\n  # Flatten out the arrays so that they may be passed to ""ax.bar3d"".\n  # Basically, ax.bar3d expects three one-dimensional arrays:\n  # x_data, y_data, z_data. The following call boils down to picking\n  # one entry from each array and plotting a bar to from\n  # (x_data[i], y_data[i], 0) to (x_data[i], y_data[i], z_data[i]).\n  #\n  x_data = x_data.flatten()\n  y_data = y_data.flatten()\n  z_data = data_array.flatten()\n  ax.bar3d( x_data,\n            y_data,\n            np.zeros(len(z_data)),\n            1, 1, z_data )\n  #\n  # Finally, display the plot.\n  #\n  plt.show()\n  raw_input()'"
linear_irl_gridworld.py,0,"b'import numpy as np\nimport matplotlib.pyplot as plt\nimport argparse\n\nimport img_utils\nfrom mdp import gridworld\nfrom mdp import value_iteration\nfrom lp_irl import *\n\nPARSER = argparse.ArgumentParser(description=None)\nPARSER.add_argument(\'-l\', \'--l1\', default=10, type=float, help=\'l1 regularization\')\nPARSER.add_argument(\'-g\', \'--gamma\', default=0.5, type=float, help=\'discount factor\')\nPARSER.add_argument(\'-r\', \'--r_max\', default=10, type=float, help=\'maximum value of reward\')\nPARSER.add_argument(\'-a\', \'--act_random\', default=0.3, type=float, help=\'probability of acting randomly\')\nARGS = PARSER.parse_args()\nprint ARGS\n\n\nGAMMA = ARGS.gamma\nACT_RAND = ARGS.act_random\nR_MAX = ARGS.r_max\nL1 = ARGS.l1\n\n\ndef main():\n  """"""\n  Recover gridworld reward using linear programming IRL\n  """"""\n\n  H = 10\n  W = 10\n  N_STATES = H * W\n  N_ACTIONS = 5\n\n  # init the gridworld\n  grid = [[\'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\'],\n          [\'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\'],\n          [\'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\'],\n          [\'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\'],\n          [\'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\'],\n          [\'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\'],\n          [\'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\'],\n          [\'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\'],\n          [\'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\'],\n          [\'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', str(R_MAX)]]\n\n  gw = gridworld.GridWorld(grid, {(H - 1, W - 1)}, 1 - ACT_RAND)\n\n  # solve the MDP using value iteration\n  vi = value_iteration.ValueIterationAgent(gw, GAMMA, 100)\n\n  r_mat = gw.get_reward_mat()\n  print \'show rewards map. any key to continue\'\n  img_utils.heatmap2d(r_mat, \'Reward Map - Ground Truth\')\n\n  v_mat = gw.get_values_mat(vi.get_values())\n  print \'show values map. any key to continue\'\n  img_utils.heatmap2d(v_mat, \'Value Map - Ground Truth\')\n\n  # Construct transition matrix\n  P_a = np.zeros((N_STATES, N_STATES, N_ACTIONS))\n\n  for si in range(N_STATES):\n    statei = gw.idx2pos(si)\n    for a in range(N_ACTIONS):\n      probs = gw.get_transition_states_and_probs(statei, a)\n      for statej, prob in probs:\n        sj = gw.pos2idx(statej)\n        # Prob of si to sj given action a\n        P_a[si, sj, a] = prob\n\n  # display policy and value in gridworld just for debug use\n  gw.display_policy_grid(vi.get_optimal_policy())\n  gw.display_value_grid(vi.values)\n\n  # setup policy\n  policy = np.zeros(N_STATES)\n  for i in range(N_STATES):\n    policy[i] = vi.get_action(gw.idx2pos(i))\n\n  # solve for the rewards\n  rewards = lp_irl(P_a, policy, gamma=GAMMA, l1=L1, R_max=R_MAX)\n\n  # display recoverred rewards\n  print \'show recoverred rewards map. any key to continue\'\n  img_utils.heatmap2d(np.reshape(rewards, (H, W), order=\'F\'), \'Reward Map - Recovered\')\n  img_utils.heatmap3d(np.reshape(rewards, (H, W), order=\'F\'), \'Reward Map - Recovered\')\n\n\nif __name__ == ""__main__"":\n  main()\n'"
lp_irl.py,0,"b'\'\'\'\nImplementation of linear programming inverse reinforcement learning in\n  Ng & Russell 2000 paper: Algorithms for Inverse Reinforcement Learning\n  http://ai.stanford.edu/~ang/papers/icml00-irl.pdf\n\nBy Yiren Lu (luyirenmax@gmail.com), May 2017\n\'\'\'\nimport numpy as np\nfrom cvxopt import matrix, solvers\nfrom utils import *\n\ndef lp_irl(trans_probs, policy, gamma=0.5, l1=10, R_max=10):\n  """"""\n  inputs:\n    trans_probs       NxNxN_ACTIONS transition matrix\n    policy            policy vector / map\n    R_max             maximum possible value of recoverred rewards\n    gamma             RL discount factor\n    l1                l1 regularization lambda\n  returns:\n    rewards           Nx1 reward vector\n  """"""\n  print np.shape(trans_probs)\n  N_STATES, _, N_ACTIONS = np.shape(trans_probs)\n  N_STATES = int(N_STATES)\n  N_ACTIONS = int(N_ACTIONS)\n\n  # Formulate a linear IRL problem\n  A = np.zeros([2 * N_STATES * (N_ACTIONS + 1), 3 * N_STATES])\n  b = np.zeros([2 * N_STATES * (N_ACTIONS + 1)])\n  c = np.zeros([3 * N_STATES])\n\n  for i in range(N_STATES):\n    a_opt = int(policy[i])\n    tmp_inv = np.linalg.inv(np.identity(N_STATES) - gamma * trans_probs[:, :, a_opt])\n\n    cnt = 0\n    for a in range(N_ACTIONS):\n      if a != a_opt:\n        A[i * (N_ACTIONS - 1) + cnt, :N_STATES] = - \\\n            np.dot(trans_probs[i, :, a_opt] - trans_probs[i, :, a], tmp_inv)\n        A[N_STATES * (N_ACTIONS - 1) + i * (N_ACTIONS - 1) + cnt, :N_STATES] = - \\\n            np.dot(trans_probs[i, :, a_opt] - trans_probs[i, :, a], tmp_inv)\n        A[N_STATES * (N_ACTIONS - 1) + i * (N_ACTIONS - 1) + cnt, N_STATES + i] = 1\n        cnt += 1\n\n  for i in range(N_STATES):\n    A[2 * N_STATES * (N_ACTIONS - 1) + i, i] = 1\n    b[2 * N_STATES * (N_ACTIONS - 1) + i] = R_max\n\n  for i in range(N_STATES):\n    A[2 * N_STATES * (N_ACTIONS - 1) + N_STATES + i, i] = -1\n    b[2 * N_STATES * (N_ACTIONS - 1) + N_STATES + i] = 0\n\n  for i in range(N_STATES):\n    A[2 * N_STATES * (N_ACTIONS - 1) + 2 * N_STATES + i, i] = 1\n    A[2 * N_STATES * (N_ACTIONS - 1) + 2 * N_STATES + i, 2 * N_STATES + i] = -1\n\n  for i in range(N_STATES):\n    A[2 * N_STATES * (N_ACTIONS - 1) + 3 * N_STATES + i, i] = 1\n    A[2 * N_STATES * (N_ACTIONS - 1) + 3 * N_STATES + i, 2 * N_STATES + i] = -1\n\n  for i in range(N_STATES):\n    c[N_STATES:2 * N_STATES] = -1\n    c[2 * N_STATES:] = l1\n\n  sol = solvers.lp(matrix(c), matrix(A), matrix(b))\n  rewards = sol[\'x\'][:N_STATES]\n  rewards = normalize(rewards) * R_max\n  return rewards\n'"
maxent_irl.py,0,"b'\'\'\'\nImplementation of maximum entropy inverse reinforcement learning in\n  Ziebart et al. 2008 paper: Maximum Entropy Inverse Reinforcement Learning\n  https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf\n\nAcknowledgement:\n  This implementation is largely influenced by Matthew Alger\'s maxent implementation here:\n  https://github.com/MatthewJA/Inverse-Reinforcement-Learning/blob/master/irl/maxent.py\n\nBy Yiren Lu (luyirenmax@gmail.com), May 2017\n\'\'\'\nimport numpy as np\nimport mdp.gridworld as gridworld\nimport mdp.value_iteration as value_iteration\nimport img_utils\nfrom utils import *\n\n\ndef compute_state_visition_freq(P_a, gamma, trajs, policy, deterministic=True):\n  """"""compute the expected states visition frequency p(s| theta, T) \n  using dynamic programming\n\n  inputs:\n    P_a     NxNxN_ACTIONS matrix - transition dynamics\n    gamma   float - discount factor\n    trajs   list of list of Steps - collected from expert\n    policy  Nx1 vector (or NxN_ACTIONS if deterministic=False) - policy\n\n  \n  returns:\n    p       Nx1 vector - state visitation frequencies\n  """"""\n  N_STATES, _, N_ACTIONS = np.shape(P_a)\n\n  T = len(trajs[0])\n  # mu[s, t] is the prob of visiting state s at time t\n  mu = np.zeros([N_STATES, T]) \n\n  for traj in trajs:\n    mu[traj[0].cur_state, 0] += 1\n  mu[:,0] = mu[:,0]/len(trajs)\n\n  for s in range(N_STATES):\n    for t in range(T-1):\n      if deterministic:\n        mu[s, t+1] = sum([mu[pre_s, t]*P_a[pre_s, s, int(policy[pre_s])] for pre_s in range(N_STATES)])\n      else:\n        mu[s, t+1] = sum([sum([mu[pre_s, t]*P_a[pre_s, s, a1]*policy[pre_s, a1] for a1 in range(N_ACTIONS)]) for pre_s in range(N_STATES)])\n  p = np.sum(mu, 1)\n  return p\n\n\n\ndef maxent_irl(feat_map, P_a, gamma, trajs, lr, n_iters):\n  """"""\n  Maximum Entropy Inverse Reinforcement Learning (Maxent IRL)\n\n  inputs:\n    feat_map    NxD matrix - the features for each state\n    P_a         NxNxN_ACTIONS matrix - P_a[s0, s1, a] is the transition prob of \n                                       landing at state s1 when taking action \n                                       a at state s0\n    gamma       float - RL discount factor\n    trajs       a list of demonstrations\n    lr          float - learning rate\n    n_iters     int - number of optimization steps\n\n  returns\n    rewards     Nx1 vector - recoverred state rewards\n  """"""\n  N_STATES, _, N_ACTIONS = np.shape(P_a)\n\n  # init parameters\n  theta = np.random.uniform(size=(feat_map.shape[1],))\n\n  # calc feature expectations\n  feat_exp = np.zeros([feat_map.shape[1]])\n  for episode in trajs:\n    for step in episode:\n      feat_exp += feat_map[step.cur_state,:]\n  feat_exp = feat_exp/len(trajs)\n\n  # training\n  for iteration in range(n_iters):\n  \n    if iteration % (n_iters/20) == 0:\n      print \'iteration: {}/{}\'.format(iteration, n_iters)\n    \n    # compute reward function\n    rewards = np.dot(feat_map, theta)\n\n    # compute policy\n    _, policy = value_iteration.value_iteration(P_a, rewards, gamma, error=0.01, deterministic=False)\n    \n    # compute state visition frequences\n    svf = compute_state_visition_freq(P_a, gamma, trajs, policy, deterministic=False)\n    \n    # compute gradients\n    grad = feat_exp - feat_map.T.dot(svf)\n\n    # update params\n    theta += lr * grad\n\n  rewards = np.dot(feat_map, theta)\n  # return sigmoid(normalize(rewards))\n  return normalize(rewards)\n\n\n'"
maxent_irl_gridworld.py,0,"b'import numpy as np\nimport matplotlib.pyplot as plt\nimport argparse\nfrom collections import namedtuple\n\n\nimport img_utils\nfrom mdp import gridworld\nfrom mdp import value_iteration\nfrom maxent_irl import *\n\nStep = namedtuple(\'Step\',\'cur_state action next_state reward done\')\n\n\nPARSER = argparse.ArgumentParser(description=None)\nPARSER.add_argument(\'-hei\', \'--height\', default=5, type=int, help=\'height of the gridworld\')\nPARSER.add_argument(\'-wid\', \'--width\', default=5, type=int, help=\'width of the gridworld\')\nPARSER.add_argument(\'-g\', \'--gamma\', default=0.8, type=float, help=\'discount factor\')\nPARSER.add_argument(\'-a\', \'--act_random\', default=0.3, type=float, help=\'probability of acting randomly\')\nPARSER.add_argument(\'-t\', \'--n_trajs\', default=100, type=int, help=\'number of expert trajectories\')\nPARSER.add_argument(\'-l\', \'--l_traj\', default=20, type=int, help=\'length of expert trajectory\')\nPARSER.add_argument(\'--rand_start\', dest=\'rand_start\', action=\'store_true\', help=\'when sampling trajectories, randomly pick start positions\')\nPARSER.add_argument(\'--no-rand_start\', dest=\'rand_start\',action=\'store_false\', help=\'when sampling trajectories, fix start positions\')\nPARSER.set_defaults(rand_start=False)\nPARSER.add_argument(\'-lr\', \'--learning_rate\', default=0.01, type=float, help=\'learning rate\')\nPARSER.add_argument(\'-ni\', \'--n_iters\', default=20, type=int, help=\'number of iterations\')\nARGS = PARSER.parse_args()\nprint ARGS\n\n\nGAMMA = ARGS.gamma\nACT_RAND = ARGS.act_random\nR_MAX = 1 # the constant r_max does not affect much the recoverred reward distribution\nH = ARGS.height\nW = ARGS.width\nN_TRAJS = ARGS.n_trajs\nL_TRAJ = ARGS.l_traj\nRAND_START = ARGS.rand_start\nLEARNING_RATE = ARGS.learning_rate\nN_ITERS = ARGS.n_iters\n\ndef feature_coord(gw):\n  N = gw.height * gw.width\n  feat = np.zeros([N, 2])\n  for i in range(N):\n    iy, ix = gw.idx2pos(i)\n    feat[i,0] = iy\n    feat[i,1] = ix\n  return feat\n\ndef feature_basis(gw):\n  """"""\n  Generates a NxN feature map for gridworld\n  input:\n    gw      Gridworld\n  returns\n    feat    NxN feature map - feat[i, j] is the l1 distance between state i and state j\n  """"""\n  N = gw.height * gw.width\n  feat = np.zeros([N, N])\n  for i in range(N):\n    for y in range(gw.height):\n      for x in range(gw.width):\n        iy, ix = gw.idx2pos(i)\n        feat[i, gw.pos2idx([y, x])] = abs(iy-y) + abs(ix-x)\n  return feat\n\n\ndef generate_demonstrations(gw, policy, n_trajs=100, len_traj=20, rand_start=False, start_pos=[0,0]):\n  """"""gatheres expert demonstrations\n\n  inputs:\n  gw          Gridworld - the environment\n  policy      Nx1 matrix\n  n_trajs     int - number of trajectories to generate\n  rand_start  bool - randomly picking start position or not\n  start_pos   2x1 list - set start position, default [0,0]\n  returns:\n  trajs       a list of trajectories - each element in the list is a list of Steps representing an episode\n  """"""\n\n  trajs = []\n  for i in range(n_trajs):\n    if rand_start:\n      # override start_pos\n      start_pos = [np.random.randint(0, gw.height), np.random.randint(0, gw.width)]\n\n    episode = []\n    gw.reset(start_pos)\n    cur_state = start_pos\n    cur_state, action, next_state, reward, is_done = gw.step(int(policy[gw.pos2idx(cur_state)]))\n    episode.append(Step(cur_state=gw.pos2idx(cur_state), action=action, next_state=gw.pos2idx(next_state), reward=reward, done=is_done))\n    # while not is_done:\n    for _ in range(len_traj):\n        cur_state, action, next_state, reward, is_done = gw.step(int(policy[gw.pos2idx(cur_state)]))\n        episode.append(Step(cur_state=gw.pos2idx(cur_state), action=action, next_state=gw.pos2idx(next_state), reward=reward, done=is_done))\n        if is_done:\n            break\n    trajs.append(episode)\n  return trajs\n\n\n\ndef main():\n  N_STATES = H * W\n  N_ACTIONS = 5\n\n  # init the gridworld\n  # rmap_gt is the ground truth for rewards\n  rmap_gt = np.zeros([H, W])\n  rmap_gt[H-1, W-1] = R_MAX\n  # rmap_gt[H-1, 0] = R_MAX\n\n  gw = gridworld.GridWorld(rmap_gt, {}, 1 - ACT_RAND)\n\n  rewards_gt = np.reshape(rmap_gt, H*W, order=\'F\')\n  P_a = gw.get_transition_mat()\n\n  values_gt, policy_gt = value_iteration.value_iteration(P_a, rewards_gt, GAMMA, error=0.01, deterministic=True)\n  \n  \n  # use identity matrix as feature\n  feat_map = np.eye(N_STATES)\n\n  # other two features. due to the linear nature, \n  # the following two features might not work as well as the identity.\n  # feat_map = feature_basis(gw)\n  # feat_map = feature_coord(gw)\n  np.random.seed(1)\n  trajs = generate_demonstrations(gw, policy_gt, n_trajs=N_TRAJS, len_traj=L_TRAJ, rand_start=RAND_START)\n  rewards = maxent_irl(feat_map, P_a, GAMMA, trajs, LEARNING_RATE, N_ITERS)\n  \n  values, _ = value_iteration.value_iteration(P_a, rewards, GAMMA, error=0.01, deterministic=True)\n  # plots\n  plt.figure(figsize=(20,4))\n  plt.subplot(1, 4, 1)\n  img_utils.heatmap2d(rmap_gt, \'Rewards Map - Ground Truth\', block=False)\n  plt.subplot(1, 4, 2)\n  img_utils.heatmap2d(np.reshape(values_gt, (H,W), order=\'F\'), \'Value Map - Ground Truth\', block=False)\n  plt.subplot(1, 4, 3)\n  img_utils.heatmap2d(np.reshape(rewards, (H,W), order=\'F\'), \'Reward Map - Recovered\', block=False)\n  plt.subplot(1, 4, 4)\n  img_utils.heatmap2d(np.reshape(values, (H,W), order=\'F\'), \'Value Map - Recovered\', block=False)\n  plt.show()\n  # plt.subplot(2, 2, 4)\n  # img_utils.heatmap3d(np.reshape(rewards, (H,W), order=\'F\'), \'Reward Map - Recovered\', block=False)\n\n\nif __name__ == ""__main__"":\n  main()'"
tf_utils.py,17,"b'""""""Utility functions for tensorflow""""""\nimport tensorflow as tf\nimport numpy as np\n\n\ndef max_pool(x, k_sz=[2, 2]):\n  """"""max pooling layer wrapper\n  Args\n    x:      4d tensor [batch, height, width, channels]\n    k_sz:   The size of the window for each dimension of the input tensor\n  Returns\n    a max pooling layer\n  """"""\n  return tf.nn.max_pool(\n      x, ksize=[\n          1, k_sz[0], k_sz[1], 1], strides=[\n          1, k_sz[0], k_sz[1], 1], padding=\'SAME\')\n\n\ndef conv2d(x, n_kernel, k_sz, stride=1):\n  """"""convolutional layer with relu activation wrapper\n  Args:\n    x:          4d tensor [batch, height, width, channels]\n    n_kernel:   number of kernels (output size)\n    k_sz:       2d array, kernel size. e.g. [8,8]\n    stride:     stride\n  Returns\n    a conv2d layer\n  """"""\n  W = tf.Variable(tf.random_normal([k_sz[0], k_sz[1], int(x.get_shape()[3]), n_kernel]))\n  b = tf.Variable(tf.random_normal([n_kernel]))\n  # - strides[0] and strides[1] must be 1\n  # - padding can be \'VALID\'(without padding) or \'SAME\'(zero padding)\n  #     - http://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t\n  conv = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=\'SAME\')\n  conv = tf.nn.bias_add(conv, b)  # add bias term\n  # rectified linear unit: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n  return tf.nn.relu(conv)\n\n\ndef fc(x, n_output, scope=""fc"", activation_fn=None, initializer=None):\n  """"""fully connected layer with relu activation wrapper\n  Args\n    x:          2d tensor [batch, n_input]\n    n_output    output size\n  """"""\n  with tf.variable_scope(scope):\n    if initializer is None:\n      # default initialization\n      W = tf.Variable(tf.random_normal([int(x.get_shape()[1]), n_output]))\n      b = tf.Variable(tf.random_normal([n_output]))\n    else:\n      W = tf.get_variable(""W"", shape=[int(x.get_shape()[1]), n_output], initializer=initializer)\n      b = tf.get_variable(""b"", shape=[n_output],\n                          initializer=tf.constant_initializer(.0, dtype=tf.float32))\n    fc1 = tf.add(tf.matmul(x, W), b)\n    if not activation_fn is None:\n      fc1 = activation_fn(fc1)\n  return fc1\n\n\ndef flatten(x):\n  """"""flatten a 4d tensor into 2d\n  Args\n    x:          4d tensor [batch, height, width, channels]\n  Returns a flattened 2d tensor\n  """"""\n  return tf.reshape(x, [-1, int(x.get_shape()[1] * x.get_shape()[2] * x.get_shape()[3])])\n\n\ndef update_target_graph(from_scope, to_scope):\n  from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n  to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n\n  op_holder = []\n  for from_var, to_var in zip(from_vars, to_vars):\n      op_holder.append(to_var.assign(from_var))\n  return op_holder\n\n\n# Used to initialize weights for policy and value output layers\ndef normalized_columns_initializer(std=1.0):\n  def _initializer(shape, dtype=None, partition_info=None):\n      out = np.random.randn(*shape).astype(np.float32)\n      out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n      return tf.constant(out)\n  return _initializer\n'"
utils.py,0,"b'import numpy as np\nimport math\nfrom collections import namedtuple\n\nStep = namedtuple(\'Step\',\'cur_state action next_state reward done\')\n\n\ndef normalize(vals):\n  """"""\n  normalize to (0, max_val)\n  input:\n    vals: 1d array\n  """"""\n  min_val = np.min(vals)\n  max_val = np.max(vals)\n  return (vals - min_val) / (max_val - min_val)\n\n\ndef sigmoid(xs):\n  """"""\n  sigmoid function\n  inputs:\n    xs      1d array\n  """"""\n  return [1 / (1 + math.exp(-x)) for x in xs]'"
cartpole/cartpole_dqn.py,4,"b'import argparse\nimport gym\nimport numpy as np\nimport sys\nimport tensorflow as tf\nimport dqn\nimport exp_replay\nfrom exp_replay import Step\nimport matplotlib.pyplot as plt\nimport os\nimport pickle\n\n\nPARSER = argparse.ArgumentParser(description=None)\nPARSER.add_argument(\'-d\', \'--device\', default=\'cpu\', type=str, help=\'choose device: cpu/gpu\')\nPARSER.add_argument(\'-e\', \'--episodes\', default=150, type=int, help=\'number of episodes\')\nPARSER.add_argument(\'-m\', \'--model_dir\', default=\'cartpole-model/\', type=str, help=\'model directory\')\nPARSER.add_argument(\'-t\', \'--train\', default=False, type=str, help=\'train for [number of episodes] IF MODEL EXISTS\')\nARGS = PARSER.parse_args()\nprint ARGS\n\n\nDEVICE = ARGS.device\nNUM_EPISODES = ARGS.episodes\nACTIONS = {0:0, 1:1}\nMAX_STEPS = 300\nFAIL_PENALTY = 0\nEPSILON = 1\nEPSILON_DECAY = 0.01\nEND_EPSILON = 0.1\nLEARNING_RATE = 0.001\nDISCOUNT_FACTOR = 0.9\nBATCH_SIZE = 32\nMEM_SIZE = 1e4\nSTART_MEM = 1e2\nSTATE_SIZE = [4]\nEPOCH_SIZE = 100\n\nTRAIN = ARGS.train\n\nMODEL_DIR = ARGS.model_dir\nMODEL_PATH = MODEL_DIR + \'model\'\nMEMORY_PATH = MODEL_DIR + \'memory.p\'\n\n\ndef train(agent, exprep, env):\n  for i in xrange(NUM_EPISODES):\n    cur_state = env.reset()\n    for t in xrange(MAX_STEPS):\n      action = agent.get_action(cur_state)\n      next_state, reward, done, info = env.step(action)\n      if done:\n        reward = FAIL_PENALTY\n        exprep.add_step(Step(cur_step=cur_state, action=action, next_step=next_state, reward=reward, done=done))\n        print(""Episode {} finished after {} timesteps"".format(i, t + 1))\n        yield t + 1 \n        break\n      exprep.add_step(Step(cur_step=cur_state, action=action, next_step=next_state, reward=reward, done=done))\n      cur_state = next_state\n      if t == MAX_STEPS - 1:\n        print(""Episode {} finished after {} timesteps"".format(i, t + 1))\n        yield t + 1\n    agent.epsilon_decay()\n    agent.learn_epoch(exprep, EPOCH_SIZE)\n    print \'epsilon: {}\'.format(agent.epsilon)\n\n\nenv = gym.make(\'CartPole-v0\')\nexprep = exp_replay.ExpReplay(mem_size=MEM_SIZE, start_mem=START_MEM, state_size=STATE_SIZE, kth=-1, batch_size=BATCH_SIZE)\n\n\n\nsess = tf.Session()\nwith tf.device(\'/{}:0\'.format(DEVICE)):\n  agent = dqn.DQNAgent(session=sess, epsilon=EPSILON, epsilon_anneal=EPSILON_DECAY, end_epsilon=END_EPSILON, \n        lr=LEARNING_RATE, gamma=DISCOUNT_FACTOR, state_size=4, \n        action_size=len(ACTIONS), n_hidden_1=10, n_hidden_2=10)\n\nsess.run(tf.initialize_all_variables())\nsaver = tf.train.Saver()\nif os.path.isdir(MODEL_DIR):\n  saver.restore(sess, MODEL_PATH)\n  agent.epsilon = agent.end_epsilon\n  print \'restored model\'\n  if TRAIN:\n    exprep = pickle.load(open(MEMORY_PATH,""rb""))\n    history = [e_length for e_length in train(agent, exprep, env)]\n    saver.save(sess, MODEL_PATH)\n    pickle.dump(exprep, open(MEMORY_PATH, ""wb""))\n    print \'saved model\'\n    # plot\n    import matplotlib.pyplot as plt\n    avg_reward = [np.mean(history[i*10:(i+1)*10]) for i in xrange(int(len(history)/10))]\n    f_reward = plt.figure(1)\n    plt.plot(np.linspace(0, len(history), len(avg_reward)), avg_reward)\n    plt.ylabel(\'Episode length\')\n    plt.xlabel(\'Training episodes\')\n    f_reward.show()\n    print \'press enter to continue\'\n    raw_input()\n    plt.close()\n\nelse:\n  os.makedirs(MODEL_DIR)\n  history = [e_length for e_length in train(agent, exprep, env)]\n  saver.save(sess, MODEL_PATH)\n  pickle.dump(exprep, open(MEMORY_PATH, ""wb""))\n  pickle.dump(agent, open(MODEL_PATH, ""wb""))\n  print \'saved model\'\n  # plot\n  import matplotlib.pyplot as plt\n  avg_reward = [np.mean(history[i*10:(i+1)*10]) for i in xrange(int(len(history)/10))]\n  f_reward = plt.figure(1)\n  plt.plot(np.linspace(0, len(history), len(avg_reward)), avg_reward)\n  plt.ylabel(\'Episode length\')\n  plt.xlabel(\'Training episodes\')\n  f_reward.show()\n  print \'press enter to continue\'\n  raw_input()\n  plt.close()\n\n\n# Display:\nprint \'press ctrl-c to stop\'\nwhile True:\n  cur_state = env.reset()\n  done = False\n  t = 0\n  while not done:\n    env.render()\n    t = t+1\n    action = agent.get_optimal_action(cur_state)\n    next_state, reward, done, info = env.step(action)\n    cur_state = next_state\n    if done:\n      print(""Episode finished after {} timesteps"".format(t+1))\n      break\n    \n\n\n\n\n\n'"
cartpole/cartpole_dqn_history.py,4,"b'\'\'\'To collect dqn expert history on carpole environment. No training\'\'\'\nimport argparse\nimport gym\nimport numpy as np\nimport sys\nimport tensorflow as tf\nimport dqn\nimport exp_replay\nfrom exp_replay import Step\nimport matplotlib.pyplot as plt\nimport os\nimport pickle\n\n\nPARSER = argparse.ArgumentParser(description=None)\nPARSER.add_argument(\'-d\', \'--device\', default=\'cpu\', type=str, help=\'choose device: cpu/gpu\')\nPARSER.add_argument(\'-e\', \'--episodes\', default=150, type=int, help=\'number of episodes\')\nPARSER.add_argument(\'-m\', \'--model_dir\', default=\'cartpole-model/\', type=str, help=\'model directory\')\nPARSER.add_argument(\'-hf\', \'--history_file\', default=\'cartpole-model/history.p\', type=str, help=\'history file path\')\nARGS = PARSER.parse_args()\nprint ARGS\n\n\nDEVICE = ARGS.device\nNUM_EPISODES = ARGS.episodes\nACTIONS = {0:0, 1:1}\nMAX_STEPS = 300\nFAIL_PENALTY = 0\nEPSILON = 1\nEPSILON_DECAY = 0.01\nEND_EPSILON = 0.1\nLEARNING_RATE = 0.001\nDISCOUNT_FACTOR = 0.9\nBATCH_SIZE = 32\nMEM_SIZE = 1e4\nSTART_MEM = 1e2\nSTATE_SIZE = [4]\nEPOCH_SIZE = 100\n\nMODEL_DIR = ARGS.model_dir\nMODEL_PATH = MODEL_DIR + \'model\'\nMEMORY_PATH = MODEL_DIR + \'memory.p\'\nHISTORY_PATH = ARGS.history_file\n\nenv = gym.make(\'CartPole-v0\')\nexprep = exp_replay.ExpReplay(mem_size=MEM_SIZE, start_mem=START_MEM, state_size=STATE_SIZE, kth=-1, batch_size=BATCH_SIZE)\n\nsess = tf.Session()\nwith tf.device(\'/{}:0\'.format(DEVICE)):\n  agent = dqn.DQNAgent(session=sess, epsilon=EPSILON, epsilon_anneal=EPSILON_DECAY, end_epsilon=END_EPSILON, \n        lr=LEARNING_RATE, gamma=DISCOUNT_FACTOR, state_size=4, \n        action_size=len(ACTIONS), n_hidden_1=10, n_hidden_2=10)\n\nsess.run(tf.initialize_all_variables())\nsaver = tf.train.Saver()\nif os.path.isdir(MODEL_DIR):\n  saver.restore(sess, MODEL_PATH)\n  agent.epsilon = agent.end_epsilon\n  print \'restored model\'\n\nelse:\n  print \'Wrong model directory. Abort\'\n  exit\n\n\n\nhistory = []\n\n# Display:\nprint \'press ctrl-c to stop\'\nfor e in range(NUM_EPISODES):\n  episode = []\n  cur_state = env.reset()\n  done = False\n  t = 0\n  while not done:\n    env.render()\n    t = t+1\n    action = agent.get_optimal_action(cur_state)\n    q_values = agent.get_action_values(cur_state)\n    # print np.amax(q_values), np.amin(q_values), np.amax(q_values) - np.amin(q_values)\n    next_state, reward, done, info = env.step(action)\n    episode.append(Step(cur_step=cur_state, action=action, next_step=next_state, reward=reward, done=done))\n    cur_state = next_state\n    if done:\n      print(""Episode {} finished after {} timesteps"".format(e, t+1))\n      break\n  history.append(episode)\n\npickle.dump(exprep, open(HISTORY_PATH, ""wb""))\nprint \'history saved\'\n\n\n\n'"
cartpole/dqn.py,11,"b'# Deep Q-learning agent with q-value approximation\n# Following paper: Playing Atari with Deep Reinforcement Learning\n#     https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n#\n# ---\n# @author Yiren Lu\n# @email luyiren [at] seas [dot] upenn [dot] edu\n#\n# MIT License\n\n\nimport gym\nimport numpy as np\nimport random\nimport tensorflow as tf\nimport tf_utils\n\n\nclass DQNAgent():\n  """"""\n  DQN Agent with a 2-hidden-layer fully-connected q-network that acts epsilon-greedily.\n  """"""\n\n  def __init__(self,\n    session,\n    epsilon=0.5, \n    epsilon_anneal = 0.01,\n    end_epsilon=0.1,\n    lr=0.5, \n    gamma=0.99,\n    state_size=4,\n    action_size=2,\n    scope=""dqn"",\n    n_hidden_1=20,\n    n_hidden_2=20,\n    ):\n    """"""\n    args\n      epsilon           exploration rate\n      epsilon_anneal    linear decay rate per call of epsilon_decay() function\n      end_epsilon       lowest exploration rate\n      lr                learning rate\n      gamma             discount factor\n      state_size        network input size\n      action_size       network output size\n    """"""\n    self.epsilon = epsilon\n    self.epsilon_anneal = epsilon_anneal\n    self.end_epsilon = end_epsilon\n    self.lr = lr\n    self.gamma = gamma\n    self.state_size = state_size\n    self.action_size = action_size\n    self.scope = scope\n    self.n_hidden_1 = n_hidden_1\n    self.n_hidden_2 = n_hidden_2\n    self._build_qnet()\n    self.sess = session\n\n  def _build_qnet(self):\n    """"""\n    Build q-network\n    """"""\n    with tf.variable_scope(self.scope):\n      self.state_input = tf.placeholder(tf.float32, [None, self.state_size])\n      self.action = tf.placeholder(tf.int32, [None])\n      self.target_q = tf.placeholder(tf.float32, [None])\n\n      fc1 = tf_utils.fc(self.state_input, n_output=self.n_hidden_1, activation_fn=tf.nn.relu)\n      fc2 = tf_utils.fc(fc1, n_output=self.n_hidden_2, activation_fn=tf.nn.relu)\n      self.q_values = tf_utils.fc(fc2, self.action_size, activation_fn=None)\n\n      action_mask = tf.one_hot(self.action, self.action_size, 1.0, 0.0)\n      q_value_pred = tf.reduce_sum(self.q_values * action_mask, 1)\n\n      self.loss = tf.reduce_mean(tf.square(tf.subtract(self.target_q, q_value_pred)))\n      self.optimizer = tf.train.AdamOptimizer(self.lr)\n      self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n\n  def get_action_values(self, state):\n    actions = self.sess.run(self.q_values, feed_dict={self.state_input: [state]})\n    return actions\n\n  def get_optimal_action(self, state):\n    actions = self.sess.run(self.q_values, feed_dict={self.state_input: [state]})\n    return actions.argmax()\n\n  def get_action(self, state):\n    """"""\n    Epsilon-greedy action\n\n    args\n      state           current state      \n    returns\n      an action to take given the state\n    """"""\n    if np.random.random() < self.epsilon:\n      # act randomly\n      return np.random.randint(0, self.action_size)\n    else:\n      return self.get_optimal_action(state)\n\n  def epsilon_decay(self):    \n    if self.epsilon > self.end_epsilon:\n      self.epsilon = self.epsilon - self.epsilon_anneal\n\n  def learn_epoch(self, exprep, num_steps):\n    """"""\n    Deep Q-learing: train qnetwork for num_steps, for each step, sample a batch from exprep\n\n    Args\n      exprep:         experience replay\n      num_steps:      num of steps\n    """"""\n    for i in xrange(num_steps):\n      self.learn_batch(exprep.sample())\n\n  def learn_batch(self, batch_steps):\n    """"""\n    Deep Q-learing: train qnetwork with the input batch\n    Args\n      batch_steps:    a batch of sampled namedtuple Step, where Step.cur_step and \n                      Step.next_step are of shape {self.state_size}\n      sess:           tf session\n    Returns \n      batch loss (-1 if input is empty)\n    """"""\n    if len(batch_steps) == 0:\n      return -1\n\n    next_state_batch = [s.next_step for s in batch_steps]\n    q_values = self.sess.run(self.q_values, feed_dict={self.state_input: next_state_batch})\n\n    max_q_values = q_values.max(axis=1)\n    # compute target q value\n    target_q = np.array([s.reward + self.gamma*max_q_values[i]*(1-s.done) for i,s in enumerate(batch_steps)])\n    target_q = target_q.reshape([len(batch_steps)])\n    \n    # minimize the TD-error\n    cur_state_batch = [s.cur_step for s in batch_steps]\n    actions = [s.action for s in batch_steps]\n    l, _, = self.sess.run([self.loss, self.train_op], feed_dict={ self.state_input: cur_state_batch,\n                                                                  self.target_q: target_q,\n                                                                  self.action: actions })\n    return l\n\n'"
cartpole/exp_replay.py,0,"b'# Experience Replay\n# Following paper: Playing Atari with Deep Reinforcement Learning\n#     https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n#\n# ---\n# @author Yiren Lu\n# @email luyiren [at] seas [dot] upenn [dot] edu\n#\n# MIT License\n\n\nimport numpy as np\nimport random\nfrom collections import namedtuple\n\n\nStep = namedtuple(\'Step\',\'cur_step action next_step reward done\')\n\n\nclass ExpReplay():\n  """"""Experience replay""""""\n\n\n  def __init__(self, mem_size, start_mem=None, state_size=[84, 84], kth=4, drop_rate=0.2, batch_size=32):\n    # k = -1 for sending raw state\n    self.state_size = state_size\n    self.drop_rate = drop_rate\n    self.mem_size = mem_size\n    self.start_mem = start_mem\n    if start_mem == None:\n      self.start_mem = mem_size/20\n    self.kth = kth\n    self.batch_size = batch_size\n    self.mem = []\n    self.total_steps = 0\n\n\n  def add_step(self, step):\n    """"""\n    Store episode to memory and check if it reaches the mem_size. \n    If so, drop [self.drop_rate] of the oldest memory\n\n    args\n      step      namedtuple Step, where step.cur_step and step.next_step are of size {state_size}\n    """"""\n    self.mem.append(step)\n    self.total_steps = self.total_steps + 1\n    while len(self.mem) > self.mem_size:\n      self.mem = self.mem[int(len(self.mem)*self.drop_rate):]\n\n\n  def get_last_state(self):\n    if len(self.mem) > abs(self.kth):\n      if self.kth == -1:\n        return self.mem[-1].cur_step\n      if len(self.state_size) == 1:\n        return [s.cur_step for s in self.mem[-abs(self.kth):]]\n      last_state = np.stack([s.cur_step for s in self.mem[-abs(self.kth):]], axis=len(self.state_size))\n      return np.stack([s.cur_step for s in self.mem[-abs(self.kth):]], axis=len(self.state_size))\n    return []\n\n\n  def sample(self, num=None):\n    """"""Randomly draw [num] samples""""""\n    if num == None:\n      num = self.batch_size\n    if len(self.mem) < self.start_mem:\n      return []\n    sampled_idx = random.sample(range(abs(self.kth),len(self.mem)), num)\n    samples = []\n    for idx in sampled_idx:\n      steps = self.mem[idx-abs(self.kth):idx]\n      cur_state = np.stack([s.cur_step for s in steps], axis=len(self.state_size))\n      next_state = np.stack([s.next_step for s in steps], axis=len(self.state_size))\n      # handle special cases\n      if self.kth == -1:\n        cur_state = steps[0].cur_step\n        next_state = steps[0].next_step\n      elif len(self.state_size) == 1:\n        cur_state = [steps[0].cur_step]\n        next_state = [steps[0].next_step]\n      reward = steps[-1].reward\n      action = steps[-1].action\n      done = steps[-1].done\n      samples.append(Step(cur_step=cur_state, action=action, next_step=next_state, reward=reward, done=done))\n    return samples\n\n\n\n\n'"
cartpole/tf_utils.py,10,"b'""""""Utility functions for tensorflow""""""\nimport tensorflow as tf\n\n\ndef max_pool(x, k_sz=[2,2]):\n  """"""max pooling layer wrapper\n  Args\n    x:      4d tensor [batch, height, width, channels]\n    k_sz:   The size of the window for each dimension of the input tensor\n  Returns\n    a max pooling layer\n  """"""\n  return tf.nn.max_pool(x, ksize=[1, k_sz[0], k_sz[1], 1], strides=[1, k_sz[0], k_sz[1], 1], padding=\'SAME\')\n\ndef conv2d(x, n_kernel, k_sz, stride=1):\n  """"""convolutional layer with relu activation wrapper\n  Args:\n    x:          4d tensor [batch, height, width, channels]\n    n_kernel:   number of kernels (output size)\n    k_sz:       2d array, kernel size. e.g. [8,8]\n    stride:     stride\n  Returns\n    a conv2d layer\n  """"""\n  W = tf.Variable(tf.random_normal([k_sz[0], k_sz[1], int(x.get_shape()[3]), n_kernel]))\n  b = tf.Variable(tf.random_normal([n_kernel]))\n  # - strides[0] and strides[1] must be 1\n  # - padding can be \'VALID\'(without padding) or \'SAME\'(zero padding)\n  #     - http://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t\n  conv = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=\'SAME\')\n  conv = tf.nn.bias_add(conv, b) # add bias term\n  return tf.nn.relu(conv) # rectified linear unit: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n\n\ndef fc(x, n_output, activation_fn=None):\n  """"""fully connected layer with relu activation wrapper\n  Args\n    x:          2d tensor [batch, n_input]\n    n_output    output size\n  """"""\n  W=tf.Variable(tf.random_normal([int(x.get_shape()[1]), n_output]))\n  b=tf.Variable(tf.random_normal([n_output]))\n  fc1 = tf.add(tf.matmul(x, W), b)\n  if not activation_fn == None:\n    fc1 = activation_fn(fc1)\n  return fc1\n\n\ndef flatten(x):\n  """"""flatten a 4d tensor into 2d\n  Args\n    x:          4d tensor [batch, height, width, channels]\n  Returns a flattened 2d tensor\n  """"""\n  return tf.reshape(x, [-1, int(x.get_shape()[1]*x.get_shape()[2]*x.get_shape()[3])])\n\n\n'"
mdp/__init__.py,0,b''
mdp/gridworld.py,0,"b'# Gridworld provides a basic environment for RL agents to interact with\n#\n# ---\n# @author Yiren Lu\n# @email luyiren [at] seas [dot] upenn [dot] edu\n#\n# MIT License\n\nimport numpy as np\n\n\nclass GridWorld(object):\n  """"""\n  Grid world environment\n  """"""\n\n  def __init__(self, grid, terminals, trans_prob=1):\n    """"""\n    input:\n      grid        2-d list of the grid including the reward\n      terminals   a set of all the terminal states\n      trans_prob  transition probability when given a certain action\n    """"""\n    self.height = len(grid)\n    self.width = len(grid[0])\n    self.n_states = self.height*self.width\n    for i in range(self.height):\n      for j in range(self.width):\n        grid[i][j] = str(grid[i][j])\n\n\n    self.terminals = terminals\n    self.grid = grid\n    self.neighbors = [(0, 1), (0, -1), (1, 0), (-1, 0), (0, 0)]\n    self.actions = [0, 1, 2, 3, 4]\n    self.n_actions = len(self.actions)\n    # self.dirs = {0: \'s\', 1: \'r\', 2: \'l\', 3: \'d\', 4: \'u\'}\n    self.dirs = {0: \'r\', 1: \'l\', 2: \'d\', 3: \'u\', 4: \'s\'}\n    #              right,    left,   down,   up ,   stay\n    # self.action_nei = {0: (0,1), 1:(0,-1), 2:(1,0), 3:(-1,0)}\n\n    # If the mdp is deterministic, the transition probability of taken a certain action should be 1\n    # otherwise < 1, the rest of the probability are equally spreaded onto\n    # other neighboring states.\n    self.trans_prob = trans_prob\n\n  def show_grid(self):\n    for i in range(len(self.grid)):\n      print self.grid[i]\n\n  def get_grid(self):\n    return self.grid\n\n  def get_states(self):\n    """"""\n    returns\n      a list of all states\n    """"""\n    return filter(\n        lambda x: self.grid[x[0]][x[1]] != \'x\',\n        [(i, j) for i in range(self.height) for j in range(self.width)])\n\n  def get_actions(self, state):\n    """"""\n    get all the actions that can be takens on the current state\n    returns\n      a list of actions\n    """"""\n    if self.grid[state[0]][state[1]] == \'x\':\n      return [4]\n\n    actions = []\n    for i in range(len(self.actions) - 1):\n      inc = self.neighbors[i]\n      a = self.actions[i]\n      nei_s = (state[0] + inc[0], state[1] + inc[1])\n      if nei_s[0] >= 0 and nei_s[0] < self.height and nei_s[1] >= 0 and nei_s[\n              1] < self.width and self.grid[nei_s[0]][nei_s[1]] != \'x\':\n        actions.append(a)\n    return actions\n\n  def __get_action_states(self, state):\n    """"""\n    get all the actions that can be takens on the current state\n    returns\n      a list of (action, state) pairs\n    """"""\n    a_s = []\n    for i in range(len(self.actions)):\n      inc = self.neighbors[i]\n      a = self.actions[i]\n      nei_s = (state[0] + inc[0], state[1] + inc[1])\n      if nei_s[0] >= 0 and nei_s[0] < self.height and nei_s[1] >= 0 and nei_s[\n              1] < self.width and self.grid[nei_s[0]][nei_s[1]] != \'x\':\n        a_s.append((a, nei_s))\n    return a_s\n\n  def get_reward_sas(self, state, action, state1):\n    """"""\n    args\n      state     current state\n      action    action\n      state1    next state\n    returns\n      the reward on current state\n    """"""\n    if not self.grid[state[0]][state[1]] == \'x\':\n      return float(self.grid[state[0]][state[1]])\n    else:\n      return 0\n\n  def get_reward(self, state):\n    """"""\n    returns\n      the reward on current state\n    """"""\n    if not self.grid[state[0]][state[1]] == \'x\':\n      return float(self.grid[state[0]][state[1]])\n    else:\n      return 0\n\n  def get_transition_states_and_probs(self, state, action):\n    """"""\n    get all the possible transition states and their probabilities with [action] on [state]\n    args\n      state     (y, x)\n      action    int\n    returns\n      a list of (state, probability) pair\n    """"""\n    if self.is_terminal(tuple(state)):\n      return [(tuple(state), 1)]\n\n    if self.trans_prob == 1:\n      inc = self.neighbors[action]\n      nei_s = (state[0] + inc[0], state[1] + inc[1])\n      if nei_s[0] >= 0 and nei_s[0] < self.height and nei_s[\n              1] >= 0 and nei_s[1] < self.width and self.grid[nei_s[0]][nei_s[1]] != \'x\':\n        return [(nei_s, 1)]\n      else:\n        # if the state is invalid, stay in the current state\n        return [(state, 1)]\n    else:\n      # [(0, 1), (0, -1), (1, 0), (-1, 0), (0, 0)]\n      mov_probs = np.zeros([self.n_actions])\n      mov_probs[action] = self.trans_prob\n      mov_probs += (1-self.trans_prob)/self.n_actions\n\n      for a in range(self.n_actions):\n        inc = self.neighbors[a]\n        nei_s = (state[0] + inc[0], state[1] + inc[1])\n        if nei_s[0] < 0 or nei_s[0] >= self.height or \\\n           nei_s[1] < 0 or nei_s[1] >= self.width or self.grid[nei_s[0]][nei_s[1]] == \'x\':\n          # if the move is invalid, accumulates the prob to the current state\n          mov_probs[self.n_actions-1] += mov_probs[a]\n          mov_probs[a] = 0\n\n      res = []\n      for a in range(self.n_actions):\n        if mov_probs[a] != 0:\n          inc = self.neighbors[a]\n          nei_s = (state[0] + inc[0], state[1] + inc[1])\n          res.append((nei_s, mov_probs[a]))\n      return res\n\n\n  def is_terminal(self, state):\n    """"""\n    returns\n      True if the [state] is terminal\n    """"""\n    if tuple(state) in self.terminals:\n      return True\n    else:\n      return False\n\n  ##############################################\n  # Stateful Functions For Model-Free Leanring #\n  ##############################################\n\n  def reset(self, start_pos):\n    """"""\n    Reset the gridworld for model-free learning. It assumes only 1 agent in the gridworld.\n    args\n      start_pos     (i,j) pair of the start location\n    """"""\n    self._cur_state = start_pos\n\n  def get_current_state(self):\n    return self._cur_state\n\n  def step(self, action):\n    """"""\n    Step function for the agent to interact with gridworld\n    args\n      action        action taken by the agent\n    returns\n      current_state current state\n      action        input action\n      next_state    next_state\n      reward        reward on the next state\n      is_done       True/False - if the agent is already on the terminal states\n    """"""\n    if self.is_terminal(self._cur_state):\n      self._is_done = True\n      return self._cur_state, action, self._cur_state, self.get_reward(self._cur_state), True\n\n    st_prob = self.get_transition_states_and_probs(self._cur_state, action)\n\n    sampled_idx = np.random.choice(np.arange(0, len(st_prob)), p=[prob for st, prob in st_prob])\n    last_state = self._cur_state\n    next_state = st_prob[sampled_idx][0]\n    reward = self.get_reward(last_state)\n    self._cur_state = next_state\n    return last_state, action, next_state, reward, False\n\n  ###########################################\n  # Policy Evaluation for Model-free Agents #\n  ###########################################\n\n  def get_optimal_policy(self, agent):\n    states = self.get_states()\n    policy = {}\n    for s in states:\n      policy[s] = [(agent.get_optimal_action(s), 1)]\n    return policy\n\n  def get_values(self, agent):\n    states = self.get_states()\n    values = {}\n    for s in states:\n      values[s] = agent.get_value(s)\n    return values\n\n  def get_qvalues(self, agent):\n    states = self.get_states()\n    q_values = {}\n    for s in states:\n      for a in self.get_actions(s):\n        q_values[(s, a)] = agent.get_qvalue(s, a)\n    return q_values\n\n  ###################################\n  # For Display in the command line #\n  ###################################\n\n  def display_qvalue_grid(self, qvalues):\n    print ""==Display q-value grid==""\n\n    qvalues_grid = np.empty((len(self.grid), len(self.grid[0])), dtype=object)\n    for s in self.get_states():\n      if self.grid[s[0]][s[1]] == \'x\':\n        qvalues_grid[s[0]][s[1]] = \'-\'\n      else:\n        tmp_str = """"\n        for a in self.get_actions(s):\n          tmp_str = tmp_str + self.dirs[a]\n          tmp_str = tmp_str + str(\' {:.2f} \'.format(qvalues[(s, a)]))\n          # print tmp_str\n        qvalues_grid[s[0]][s[1]] = tmp_str\n\n    row_format = \'{:>40}\' * (len(self.grid[0]))\n    for row in qvalues_grid:\n      print row_format.format(*row)\n\n  def display_value_grid(self, values):\n    """"""\n    Prints a nice table of the values in grid\n    """"""\n    print ""==Display value grid==""\n\n    value_grid = np.zeros((len(self.grid), len(self.grid[0])))\n    for k in values:\n      value_grid[k[0]][k[1]] = float(values[k])\n\n    row_format = \'{:>20.4}\' * (len(self.grid[0]))\n    for row in value_grid:\n      print row_format.format(*row)\n\n  def display_policy_grid(self, policy):\n    """"""\n    prints a nice table of the policy in grid\n    input:\n      policy    a dictionary of the optimal policy {<state, action_dist>}\n    """"""\n    print ""==Display policy grid==""\n\n    policy_grid = np.chararray((len(self.grid), len(self.grid[0])))\n    for k in self.get_states():\n      if self.is_terminal((k[0], k[1])) or self.grid[k[0]][k[1]] == \'x\':\n        policy_grid[k[0]][k[1]] = \'-\'\n      else:\n        # policy_grid[k[0]][k[1]] = self.dirs[agent.get_action((k[0], k[1]))]\n        policy_grid[k[0]][k[1]] = self.dirs[policy[(k[0], k[1])][0][0]]\n\n    row_format = \'{:>20}\' * (len(self.grid[0]))\n    for row in policy_grid:\n      print row_format.format(*row)\n\n  #######################\n  # Some util functions #\n  #######################\n\n  def get_transition_mat(self):\n    """"""\n    get transition dynamics of the gridworld\n\n    return:\n      P_a         NxNxN_ACTIONS transition probabilities matrix - \n                    P_a[s0, s1, a] is the transition prob of \n                    landing at state s1 when taking action \n                    a at state s0\n    """"""\n    N_STATES = self.height*self.width\n    N_ACTIONS = len(self.actions)\n    P_a = np.zeros((N_STATES, N_STATES, N_ACTIONS))\n    for si in range(N_STATES):\n      posi = self.idx2pos(si)\n      for a in range(N_ACTIONS):\n        probs = self.get_transition_states_and_probs(posi, a)\n\n        for posj, prob in probs:\n          sj = self.pos2idx(posj)\n          # Prob of si to sj given action a\n          P_a[si, sj, a] = prob\n    return P_a\n\n  def get_values_mat(self, values):\n    """"""\n    inputs:\n      values: a dictionary {<state, value>}\n    """"""\n    shape = np.shape(self.grid)\n    v_mat = np.zeros(shape)\n    for i in range(shape[0]):\n      for j in range(shape[1]):\n        v_mat[i, j] = values[(i, j)]\n    return v_mat\n\n  def get_reward_mat(self):\n    """"""\n    Get reward matrix from gridworld\n    """"""\n    shape = np.shape(self.grid)\n    r_mat = np.zeros(shape)\n    for i in range(shape[0]):\n      for j in range(shape[1]):\n        r_mat[i, j] = float(self.grid[i][j])\n    return r_mat\n\n  def pos2idx(self, pos):\n    """"""\n    input:\n      column-major 2d position\n    returns:\n      1d index\n    """"""\n    return pos[0] + pos[1] * self.height\n\n  def idx2pos(self, idx):\n    """"""\n    input:\n      1d idx\n    returns:\n      2d column-major position\n    """"""\n    return (idx % self.height, idx / self.height)\n'"
mdp/gridworld1d.py,0,"b'# 1D Gridworld\n#\n# ---\n# @author Yiren Lu\n# @email luyiren [at] seas [dot] upenn [dot] edu\n#\n# MIT License\n\nimport numpy as np\nfrom utils import *\n\nclass GridWorld1D(object):\n  """"""\n  1D grid world environment (without terminal states)\n  """"""\n\n  def __init__(self, rewards, terminals, move_rand=0.0):\n    """"""\n    inputs:\n      rewards     1d float array - contains rewards\n      terminals   a set of all the terminal states\n    """"""\n    self.n_states = len(rewards)\n    self.rewards = rewards\n    self.terminals = terminals\n    self.actions = [-1, 1]\n    self.n_actions = len(self.actions)\n    self.move_rand = move_rand\n\n\n  def get_reward(self, state):\n    return self.rewards[state]\n\n\n  def get_transition_states_and_probs(self, state, action):\n    """"""\n    inputs: \n      state       int - state\n      action      int - action\n\n    returns\n      a list of (state, probability) pair\n    """"""\n    if action < 0 or action >= self.n_actions:\n      # invalid input\n      return []\n\n    if self.is_terminal(state):\n      return [(state, 1.0)]\n\n    if self.move_rand == 0:\n      if state+self.actions[action] < 0 or state+self.actions[action] >= self.n_states:\n        return [(state, 1.0)]\n      return [(state+self.actions[action], 1.0)]\n    else:\n      mov_probs = np.zeros(3)\n      mov_probs[1+self.actions[action]] += 1 - self.move_rand \n      for i in range(3):\n        mov_probs[i] += self.move_rand/3\n\n      if state == 0:\n        mov_probs[1] += mov_probs[0]\n        mov_probs[0] = 0\n      if state == self.n_states - 1:\n        mov_probs[1] += mov_probs[2]\n        mov_probs[2] = 0\n\n      res = []\n      for i in range(3):\n        if mov_probs[i] != 0:\n          res.append((state-1+i, mov_probs[i]))\n      return res\n\n\n  def is_terminal(self, state):\n    if state in self.terminals:\n      return True\n    else:\n      return False\n\n  ##############################################\n  # Stateful Functions For Model-Free Leanring #\n  ##############################################\n\n  def reset(self, start_pos):\n    self._cur_state = start_pos\n\n  def get_current_state(self):\n    return self._cur_state\n\n  def step(self, action):\n    """"""\n    Step function for the agent to interact with gridworld\n    inputs: \n      action        action taken by the agent\n    returns\n      current_state current state\n      action        input action\n      next_state    next_state\n      reward        reward on the next state\n      is_done       True/False - if the agent is already on the terminal states\n    """"""\n    if self.is_terminal(self._cur_state):\n      self._is_done = True\n      return self._cur_state, action, self._cur_state, self.get_reward(self._cur_state), True\n\n    st_prob = self.get_transition_states_and_probs(self._cur_state, action)\n\n    rand_idx = np.random.choice(np.arange(0, len(st_prob)), p=[prob for st, prob in st_prob])\n    last_state = self._cur_state\n    next_state = st_prob[rand_idx][0]\n    reward = self.get_reward(last_state)\n    self._cur_state = next_state\n    return last_state, action, next_state, reward, False\n\n  #######################\n  # Some util functions #\n  #######################\n\n  def get_transition_mat(self):\n    """"""\n    get transition dynamics of the gridworld\n\n    return:\n      P_a         NxNxN_ACTIONS transition probabilities matrix - \n                    P_a[s0, s1, a] is the transition prob of \n                    landing at state s1 when taking action \n                    a at state s0\n    """"""\n    N_STATES = self.n_states\n    N_ACTIONS = len(self.actions)\n    P_a = np.zeros((N_STATES, N_STATES, N_ACTIONS))\n    for si in range(N_STATES):\n      for a in range(N_ACTIONS):\n        probs = self.get_transition_states_and_probs(si, a)\n\n        for sj, prob in probs:\n          # Prob of si to sj given action a\n          P_a[si, sj, a] = prob\n    return P_a\n\n\n  def generate_demonstrations(self, policy, n_trajs=100, len_traj=20, rand_start=False, start_pos=0):\n    """"""gatheres expert demonstrations\n\n    inputs:\n    gw          Gridworld - the environment\n    policy      Nx1 matrix\n    n_trajs     int - number of trajectories to generate\n    rand_start  bool - randomly picking start position or not\n    start_pos   2x1 list - set start position, default [0,0]\n    returns:\n    trajs       a list of trajectories - each element in the list is a list of Steps representing an episode\n    """"""\n\n    trajs = []\n    for i in range(n_trajs):\n      if rand_start:\n        # override start_pos\n        start_pos = np.random.randint(0, self.n_states)\n\n      episode = []\n      self.reset(start_pos)\n      cur_state = start_pos\n      cur_state, action, next_state, reward, is_done = self.step(int(policy[cur_state]))\n      episode.append(Step(cur_state=cur_state, action=self.actions[action], next_state=next_state, reward=reward, done=is_done))\n      # while not is_done:\n      for _ in range(1,len_traj):\n          cur_state, action, next_state, reward, is_done = self.step(int(policy[cur_state]))\n          episode.append(Step(cur_state=cur_state, action=self.actions[action], next_state=next_state, reward=reward, done=is_done))\n          if is_done:\n              break\n      trajs.append(episode)\n    return trajs\n\n\n'"
mdp/test_gridworld.py,0,"b'import unittest\nimport gridworld\n\n\nclass GridWorldTest(unittest.TestCase):\n  """"""\n  Unit test for grid world\n  """"""\n\n  def setUp(self):\n    grid = [[\'0\', \'0\', \'0\', \'0\', \'10\'],\n            [\'0\', \'x\', \'0\', \'0\', \'-10\'],\n            [\'0\', \'0\', \'0\', \'0\', \'0\']]\n\n    self.grid = grid\n    self.gw_deterministic = gridworld.GridWorld(grid, {(0, 4), (1, 4)}, 1)\n    self.gw_non_deterministic = gridworld.GridWorld(\n        grid, {(0, 4), (1, 4)}, 0.8)\n\n  def test_grid_dims(self):\n    self.assertEqual(len(self.gw_deterministic.get_grid()), 3)\n    self.assertEqual(len(self.gw_deterministic.get_grid()[0]), 5)\n\n  def test_grid_values(self):\n    grid_tmp = self.gw_deterministic.get_grid()\n    for i in range(len(grid_tmp)):\n      for j in range(len(grid_tmp[0])):\n        self.assertEqual(self.grid[i][j], grid_tmp[i][j])\n\n  def test_get_states(self):\n    self.assertEqual(len(self.gw_deterministic.get_states()), 14)\n\n  def test_get_actions(self):\n    self.assertEqual(len(self.gw_deterministic.get_actions((0, 0))), 2)\n    self.assertEqual(len(self.gw_deterministic.get_actions((2, 0))), 2)\n    self.assertEqual(len(self.gw_deterministic.get_actions((2, 4))), 2)\n    self.assertEqual(len(self.gw_deterministic.get_actions((0, 4))), 2)\n    self.assertEqual(len(self.gw_deterministic.get_actions((1, 0))), 2)\n\n  def test_get_reward(self):\n    self.assertEqual(self.gw_deterministic.get_reward((0, 0)), 0)\n    self.assertEqual(self.gw_deterministic.get_reward((0, 4)), 10.0)\n    self.assertEqual(self.gw_deterministic.get_reward((1, 4)), -10.0)\n\n  def test_trans_prob_deter(self):\n    self.assertEqual(\n        len(\n            self.gw_deterministic.get_transition_states_and_probs(\n                (0, 0), 0)), 1)\n    self.assertEqual(\n        self.gw_deterministic.get_transition_states_and_probs(\n            (0, 0), 0)[0][0], (0, 1))\n    self.assertEqual(\n        self.gw_deterministic.get_transition_states_and_probs(\n            (0, 0), 0)[0][1], 1)\n\n    self.assertEqual(\n        len(\n            self.gw_deterministic.get_transition_states_and_probs(\n                (0, 0), 1)), 1)\n    self.assertEqual(\n        self.gw_deterministic.get_transition_states_and_probs(\n            (0, 0), 1)[0][0], (0, 0))\n    self.assertEqual(\n        self.gw_deterministic.get_transition_states_and_probs(\n            (0, 0), 1)[0][1], 1)\n\n  def test_trans_prob_non_deter(self):\n    self.assertEqual(\n        len(\n            self.gw_non_deterministic.get_transition_states_and_probs(\n                (0, 0), 0)), 3)\n    self.assertEqual(\n        self.gw_non_deterministic.get_transition_states_and_probs(\n            (0, 0), 0)[0][0], (0, 1))\n    self.assertEqual(\n        self.gw_non_deterministic.get_transition_states_and_probs(\n            (0, 0), 0)[0][1], 0.8)\n\n    self.assertTrue(\n        self.gw_non_deterministic.get_transition_states_and_probs(\n            (0, 0), 0)[1][1] - 0.1 < 1e-5)\n    self.assertTrue(\n        self.gw_non_deterministic.get_transition_states_and_probs(\n            (0, 0), 0)[2][1] - 0.1 < 1e-5)\n\n    self.assertEqual(\n        len(\n            self.gw_non_deterministic.get_transition_states_and_probs(\n                (1, 0), 0)), 3)\n\n  def test_terminals(self):\n    self.assertTrue(self.gw_deterministic.is_terminal((0, 4)))\n    self.assertTrue(self.gw_deterministic.is_terminal((1, 4)))\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
mdp/test_value_iteration.py,0,"b'import unittest\nimport sys\n# if ""../"" not in sys.path:\n# sys.path.append(""../"")\n# from envs import gridworld\nimport gridworld\nimport value_iteration\n\n\nclass ValueIterationAgentTest(unittest.TestCase):\n  """"""\n  Unit test for value iteration agent\n  """"""\n\n  def setUp(self):\n    grid = [[\'0\', \'0\', \'0\', \'1\'],\n            [\'0\', \'x\', \'0\', \'-1\'],\n            [\'0\', \'0\', \'0\', \'0\']]\n\n    self.grid = grid\n    self.gw_non_deterministic = gridworld.GridWorld(\n        grid, {(0, 3), (1, 3)}, 0.8)\n\n    self.agent = value_iteration.ValueIterationAgent(\n        self.gw_non_deterministic, 0.9, 100)\n\n  def test_eval_policy(self):\n    print \'Show evaluation of the optimal policy:\'\n    self.gw_non_deterministic.display_value_grid(\n        self.agent.eval_policy_dist(self.agent.get_policy_dist()))\n\n  def test_show_policy(self):\n    print \'Show policy learned by value iteration:\'\n    self.gw_non_deterministic.display_policy_grid(\n        self.agent.get_optimal_policy())\n\n  def test_values(self):\n    print \'Show value iteration results:\'\n    self.gw_non_deterministic.display_value_grid(self.agent.values)\n\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
mdp/value_iteration.py,0,"b'# Value iteration agent\n# Model-based learning which requires mdp.\n#\n# ---\n# @author Yiren Lu\n# @email luyiren [at] seas [dot] upenn [dot] edu\n#\n# MIT License\n\nimport math\nimport numpy as np\n\n\ndef value_iteration(P_a, rewards, gamma, error=0.01, deterministic=True):\n  """"""\n  static value iteration function. Perhaps the most useful function in this repo\n  \n  inputs:\n    P_a         NxNxN_ACTIONS transition probabilities matrix - \n                              P_a[s0, s1, a] is the transition prob of \n                              landing at state s1 when taking action \n                              a at state s0\n    rewards     Nx1 matrix - rewards for all the states\n    gamma       float - RL discount\n    error       float - threshold for a stop\n    deterministic   bool - to return deterministic policy or stochastic policy\n  \n  returns:\n    values    Nx1 matrix - estimated values\n    policy    Nx1 (NxN_ACTIONS if non-det) matrix - policy\n  """"""\n  N_STATES, _, N_ACTIONS = np.shape(P_a)\n\n  values = np.zeros([N_STATES])\n\n  # estimate values\n  while True:\n    values_tmp = values.copy()\n\n    for s in range(N_STATES):\n      v_s = []\n      values[s] = max([sum([P_a[s, s1, a]*(rewards[s] + gamma*values_tmp[s1]) for s1 in range(N_STATES)]) for a in range(N_ACTIONS)])\n\n    if max([abs(values[s] - values_tmp[s]) for s in range(N_STATES)]) < error:\n      break\n\n\n  if deterministic:\n    # generate deterministic policy\n    policy = np.zeros([N_STATES])\n    for s in range(N_STATES):\n      policy[s] = np.argmax([sum([P_a[s, s1, a]*(rewards[s]+gamma*values[s1]) \n                                  for s1 in range(N_STATES)]) \n                                  for a in range(N_ACTIONS)])\n\n    return values, policy\n  else:\n    # generate stochastic policy\n    policy = np.zeros([N_STATES, N_ACTIONS])\n    for s in range(N_STATES):\n      v_s = np.array([sum([P_a[s, s1, a]*(rewards[s] + gamma*values[s1]) for s1 in range(N_STATES)]) for a in range(N_ACTIONS)])\n      policy[s,:] = np.transpose(v_s/np.sum(v_s))\n    return values, policy\n\n\n\n\nclass ValueIterationAgent(object):\n\n  def __init__(self, mdp, gamma, iterations=100):\n    """"""\n    The constructor builds a value model from mdp using dynamic programming\n    \n    inputs:\n      mdp       markov decision process that is required by value iteration agent definition: \n                https://github.com/stormmax/reinforcement_learning/blob/master/envs/mdp.py\n      gamma     discount factor\n    """"""\n    self.mdp = mdp\n    self.gamma = gamma\n    states = mdp.get_states()\n    # init values\n    self.values = {}\n\n    for s in states:\n      if mdp.is_terminal(s):\n        self.values[s] = mdp.get_reward(s)\n      else:\n        self.values[s] = 0\n\n    # estimate values\n    for i in range(iterations):\n      values_tmp = self.values.copy()\n\n      for s in states:\n        if mdp.is_terminal(s):\n          continue\n\n        actions = mdp.get_actions(s)\n        v_s = []\n        for a in actions:\n          P_s1sa = mdp.get_transition_states_and_probs(s, a)\n          R_sas1 = [mdp.get_reward(s1) for s1 in [p[0] for p in P_s1sa]]\n          v_s.append(sum([P_s1sa[s1_id][1] * (mdp.get_reward(s) + gamma *\n                                              values_tmp[P_s1sa[s1_id][0]]) for s1_id in range(len(P_s1sa))]))\n        # V(s) = max_{a} \\sum_{s\'} P(s\'| s, a) (R(s,a,s\') + \\gamma V(s\'))\n        self.values[s] = max(v_s)\n\n  def get_values(self):\n    """"""\n    returns\n      a dictionary {<state, value>}\n    """"""\n    return self.values\n\n  def get_q_values(self, state, action):\n    """"""\n    returns qvalue of (state, action)\n    """"""\n    return sum([P_s1_s_a * (self.mdp.get_reward_sas(s, a, s1) + self.gamma * self.values[s1])\n                for s1, P_s1_s_a in self.mdp.get_transition_states_and_probs(state, action)])\n\n  def eval_policy_dist(self, policy, iterations=100):\n    """"""\n    evaluate a policy distribution\n    returns\n      a map {<state, value>}\n    """"""\n    values = {}\n    states = self.mdp.get_states()\n    for s in states:\n      if self.mdp.is_terminal(s):\n        values[s] = self.mdp.get_reward(s)\n      else:\n        values[s] = 0\n\n    for i in range(iterations):\n      values_tmp = values.copy()\n\n      for s in states:\n        if self.mdp.is_terminal(s):\n          continue\n        actions = self.mdp.get_actions(s)\n        # v(s) = \\sum_{a\\in A} \\pi(a|s) (R(s,a,s\') + \\gamma \\sum_{s\'\\in S}\n        # P(s\'| s, a) v(s\'))\n        values[s] = sum([policy[s][i][1] * (self.mdp.get_reward(s) + self.gamma * sum([s1_p * values_tmp[s1]\n                                                                                       for s1, s1_p in self.mdp.get_transition_states_and_probs(s, actions[i])]))\n                         for i in range(len(actions))])\n    return values\n\n\n  def get_optimal_policy(self):\n    """"""\n    returns\n      a dictionary {<state, action>}\n    """"""\n    states = self.mdp.get_states()\n    policy = {}\n    for s in states:\n      policy[s] = [(self.get_action(s), 1)]\n    return policy\n\n\n  def get_action_dist(self, state):\n    """"""\n    args\n      state    current state\n    returns\n      a list of {<action, prob>} pairs representing the action distribution on state\n    """"""\n    actions = self.mdp.get_actions(state)\n    # \\sum_{s\'} P(s\'|s,a)*(R(s,a,s\') + \\gamma v(s\'))\n    v_a = [sum([s1_p * (self.mdp.get_reward_sas(state, a, s1) + self.gamma * self.values[s1])\n                for s1, s1_p in self.mdp.get_transition_states_and_probs(state, a)])\n           for a in actions]\n\n    # I exponentiated the v_s^a\'s to make them positive\n    v_a = [math.exp(v) for v in v_a]\n    return [(actions[i], v_a[i] / sum(v_a)) for i in range(len(actions))]\n\n  def get_action(self, state):\n    """"""\n    args\n      state    current state\n    returns\n      an action to take given the state\n    """"""\n    actions = self.mdp.get_actions(state)\n    v_s = []\n    for a in actions:\n      P_s1sa = self.mdp.get_transition_states_and_probs(state, a)\n      R_sas1 = [self.mdp.get_reward(s1) for s1 in [p[0] for p in P_s1sa]]\n      v_s.append(sum([P_s1sa[s1_id][1] *\n                      (self.mdp.get_reward(state) +\n                       self.gamma *\n                       self.values[P_s1sa[s1_id][0]]) for s1_id in range(len(P_s1sa))]))\n    a_id = v_s.index(max(v_s))\n    return actions[a_id]\n\n\n\n\n\n\n\n'"
