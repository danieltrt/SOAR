file_path,api_count,code
run_actor_critic_acrobot.py,23,"b'from __future__ import print_function\nfrom collections import deque\n\nfrom rl.pg_actor_critic import PolicyGradientActorCritic\nimport tensorflow as tf\nimport numpy as np\nimport gym\n\nenv_name = \'Acrobot-v1\'\nenv = gym.make(env_name)\n\nsess = tf.Session()\noptimizer = tf.train.RMSPropOptimizer(learning_rate=0.0001, decay=0.9)\nwriter = tf.summary.FileWriter(""/tmp/{}-experiment-1"".format(env_name))\n\nstate_dim   = env.observation_space.shape[0]\nnum_actions = env.action_space.n\n\ndef actor_network(states):\n  # define policy neural network\n  W1 = tf.get_variable(""W1"", [state_dim, 20],\n                       initializer=tf.random_normal_initializer())\n  b1 = tf.get_variable(""b1"", [20],\n                       initializer=tf.constant_initializer(0))\n  h1 = tf.nn.tanh(tf.matmul(states, W1) + b1)\n  W2 = tf.get_variable(""W2"", [20, num_actions],\n                       initializer=tf.random_normal_initializer(stddev=0.1))\n  b2 = tf.get_variable(""b2"", [num_actions],\n                       initializer=tf.constant_initializer(0))\n  p = tf.matmul(h1, W2) + b2\n  return p\n\ndef critic_network(states):\n  # define policy neural network\n  W1 = tf.get_variable(""W1"", [state_dim, 20],\n                       initializer=tf.random_normal_initializer())\n  b1 = tf.get_variable(""b1"", [20],\n                       initializer=tf.constant_initializer(0))\n  h1 = tf.nn.tanh(tf.matmul(states, W1) + b1)\n  W2 = tf.get_variable(""W2"", [20, 1],\n                       initializer=tf.random_normal_initializer())\n  b2 = tf.get_variable(""b2"", [1],\n                       initializer=tf.constant_initializer(0))\n  v = tf.matmul(h1, W2) + b2\n  return v\n\npg_reinforce = PolicyGradientActorCritic(sess,\n                                         optimizer,\n                                         actor_network,\n                                         critic_network,\n                                         state_dim,\n                                         num_actions,\n                                         summary_writer=writer)\n\nMAX_EPISODES = 10000\nMAX_STEPS    = 1000\n\nno_reward_since = 0\n\nepisode_history = deque(maxlen=100)\nfor i_episode in range(MAX_EPISODES):\n\n  # initialize\n  state = env.reset()\n  total_rewards = 0\n\n  for t in range(MAX_STEPS):\n    env.render()\n    action = pg_reinforce.sampleAction(state[np.newaxis,:])\n    next_state, reward, done, _ = env.step(action)\n\n    total_rewards += reward\n    reward = 5.0 if done else -0.1\n    pg_reinforce.storeRollout(state, action, reward)\n\n    state = next_state\n    if done: break\n\n  # if we don\'t see rewards in consecutive episodes\n  # it\'s likely that the model gets stuck in bad local optima\n  # we simply reset the model and try again\n  if total_rewards <= -500:\n    no_reward_since += 1\n    if no_reward_since >= 5:\n      # create and initialize variables\n      print(\'Resetting model... start anew!\')\n      pg_reinforce.resetModel()\n      no_reward_since = 0\n      continue\n  else:\n    no_reward_since = 0\n\n  pg_reinforce.updateModel()\n\n  episode_history.append(total_rewards)\n  mean_rewards = np.mean(episode_history)\n\n  print(""Episode {}"".format(i_episode))\n  print(""Finished after {} timesteps"".format(t+1))\n  print(""Reward for this episode: {}"".format(total_rewards))\n  print(""Average reward for last 100 episodes: {:.2f}"".format(mean_rewards))\n  if mean_rewards >= -100.0 and len(episode_history) >= 100:\n    print(""Environment {} solved after {} episodes"".format(env_name, i_episode+1))\n    break\n'"
run_cem_cartpole.py,0,"b'from __future__ import print_function\nfrom collections import deque\nimport numpy as np\nimport gym\n\nenv_name = \'CartPole-v0\'\nenv = gym.make(env_name)\n\ndef observation_to_action(ob, theta):\n  # define policy neural network\n  W1 = theta[:-1]\n  b1 = theta[-1]\n  return int((ob.dot(W1) + b1) < 0)\n\ndef theta_rollout(env, theta, num_steps, render = False):\n  total_rewards = 0\n  observation = env.reset()\n  for t in range(num_steps):\n    action = observation_to_action(observation, theta)\n    observation, reward, done, _ = env.step(action)\n    total_rewards += reward\n    if render: env.render()\n    if done: break\n  return total_rewards, t\n\nMAX_EPISODES = 10000\nMAX_STEPS    = 200\nbatch_size   = 25\ntop_per      = 0.2 # percentage of theta with highest score selected from all the theta\nstd          = 1 # scale of standard deviation\n\n# initialize\ntheta_mean = np.zeros(env.observation_space.shape[0] + 1)\ntheta_std = np.ones_like(theta_mean) * std\n\nepisode_history = deque(maxlen=100)\nfor i_episode in range(MAX_EPISODES):\n  # maximize function theta_rollout through cross-entropy method\n  theta_sample = np.tile(theta_mean, (batch_size, 1)) + np.tile(theta_std, (batch_size, 1)) * np.random.randn(batch_size, theta_mean.size)\n  reward_sample = np.array([theta_rollout(env, th, MAX_STEPS)[0] for th in theta_sample])\n  top_idx = np.argsort(-reward_sample)[:int(np.round(batch_size * top_per))]\n  top_theta = theta_sample[top_idx]\n  theta_mean = top_theta.mean(axis = 0)\n  theta_std = top_theta.std(axis = 0)\n  total_rewards, t = theta_rollout(env, theta_mean, MAX_STEPS, render = True)\n\n  episode_history.append(total_rewards)\n  mean_rewards = np.mean(episode_history)\n\n  print(""Episode {}"".format(i_episode))\n  print(""Finished after {} timesteps"".format(t+1))\n  print(""Reward for this episode: {}"".format(total_rewards))\n  print(""Average reward for last 100 episodes: {}"".format(mean_rewards))\n  if mean_rewards >= 195.0:\n    print(""Environment {} solved after {} episodes"".format(env_name, i_episode+1))\n    break\n'"
run_ddpg_mujoco.py,34,"b'from __future__ import print_function\nfrom collections import deque\n\nfrom rl.pg_ddpg import DeepDeterministicPolicyGradient\nimport tensorflow as tf\nimport numpy as np\nimport gym\n\n# env_name = \'InvertedPendulum-v1\'\nenv_name = \'InvertedDoublePendulum-v1\'\nenv = gym.make(env_name)\n\nsess      = tf.Session()\noptimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\nwriter    = tf.summary.FileWriter(""/tmp/{}-experiment-1"".format(env_name))\n\nstate_dim  = env.observation_space.shape[0]\naction_dim = env.action_space.shape[0]\n\n# DDPG actor and critic architecture\n# Continuous control with deep reinforcement learning\n# Timothy P. Lillicrap, et al., 2015\n\ndef actor_network(states):\n  h1_dim = 400\n  h2_dim = 300\n\n  # define policy neural network\n  W1 = tf.get_variable(""W1"", [state_dim, h1_dim],\n                       initializer=tf.contrib.layers.xavier_initializer())\n  b1 = tf.get_variable(""b1"", [h1_dim],\n                       initializer=tf.constant_initializer(0))\n  h1 = tf.nn.relu(tf.matmul(states, W1) + b1)\n\n  W2 = tf.get_variable(""W2"", [h1_dim, h2_dim],\n                       initializer=tf.contrib.layers.xavier_initializer())\n  b2 = tf.get_variable(""b2"", [h2_dim],\n                       initializer=tf.constant_initializer(0))\n  h2 = tf.nn.relu(tf.matmul(h1, W2) + b2)\n\n  # use tanh to bound the action\n  W3 = tf.get_variable(""W3"", [h2_dim, action_dim],\n                       initializer=tf.contrib.layers.xavier_initializer())\n  b3 = tf.get_variable(""b3"", [action_dim],\n                       initializer=tf.constant_initializer(0))\n\n  # we assume actions range from [-1, 1]\n  # you can scale action outputs with any constant here\n  a = tf.nn.tanh(tf.matmul(h2, W3) + b3)\n  return a\n\ndef critic_network(states, action):\n  h1_dim = 400\n  h2_dim = 300\n\n  # define policy neural network\n  W1 = tf.get_variable(""W1"", [state_dim, h1_dim],\n                       initializer=tf.contrib.layers.xavier_initializer())\n  b1 = tf.get_variable(""b1"", [h1_dim],\n                       initializer=tf.constant_initializer(0))\n  h1 = tf.nn.relu(tf.matmul(states, W1) + b1)\n  # skip action from the first layer\n  h1_concat = tf.concat(axis=1, values=[h1, action])\n\n  W2 = tf.get_variable(""W2"", [h1_dim + action_dim, h2_dim],\n                       initializer=tf.contrib.layers.xavier_initializer())\n  b2 = tf.get_variable(""b2"", [h2_dim],\n                       initializer=tf.constant_initializer(0))\n  h2 = tf.nn.relu(tf.matmul(h1_concat, W2) + b2)\n\n  W3 = tf.get_variable(""W3"", [h2_dim, 1],\n                       initializer=tf.contrib.layers.xavier_initializer())\n  b3 = tf.get_variable(""b3"", [1],\n                       initializer=tf.constant_initializer(0))\n  v = tf.matmul(h2, W3) + b3\n  return v\n\npg_ddpg = DeepDeterministicPolicyGradient(sess,\n                                          optimizer,\n                                          actor_network,\n                                          critic_network,\n                                          state_dim,\n                                          action_dim,\n                                          summary_writer=writer)\n\nMAX_EPISODES = 10000\nMAX_STEPS    = 1000\n\nepisode_history = deque(maxlen=100)\nfor i_episode in range(MAX_EPISODES):\n\n  # initialize\n  state = env.reset()\n  for t in range(MAX_STEPS):\n    # env.render()\n    action = pg_ddpg.sampleAction(state[np.newaxis,:])\n    next_state, reward, done, _ = env.step(action)\n    pg_ddpg.storeExperience(state, action, reward, next_state, done)\n    pg_ddpg.updateModel()\n    state = next_state\n    if done: break\n\n  if i_episode % 500 == 0:\n\n    for i_eval in range(100):\n      total_rewards = 0\n      state = env.reset()\n      for t in range(MAX_STEPS):\n        # env.render()\n        action = pg_ddpg.sampleAction(state[np.newaxis,:], exploration=False)\n        next_state, reward, done, _ = env.step(action)\n        total_rewards += reward\n        state = next_state\n        if done: break\n\n      episode_history.append(total_rewards)\n      mean_rewards = np.mean(episode_history)\n\n    print(""Episode {}"".format(i_episode))\n    print(""Finished after {} timesteps"".format(t+1))\n    print(""Reward for this episode: {:.2f}"".format(total_rewards))\n    print(""Average reward for last 100 episodes: {:.2f}"".format(mean_rewards))\n    # if mean_rewards >= 950.0: # for InvertedPendulum-v1\n    if mean_rewards >= 9100.0: # for InvertedDoublePendulum-v1\n      print(""Environment {} solved after {} episodes"".format(env_name, i_episode+1))\n      break\n'"
run_dqn_cartpole.py,13,"b'from __future__ import print_function\nfrom collections import deque\n\nfrom rl.neural_q_learner import NeuralQLearner\nimport tensorflow as tf\nimport numpy as np\nimport gym\n\nenv_name = \'CartPole-v0\'\nenv = gym.make(env_name)\n\nsess = tf.Session()\noptimizer = tf.train.RMSPropOptimizer(learning_rate=0.0001, decay=0.9)\nwriter = tf.summary.FileWriter(""/tmp/{}-experiment-1"".format(env_name))\n\nstate_dim   = env.observation_space.shape[0]\nnum_actions = env.action_space.n\n\ndef observation_to_action(states):\n  # define policy neural network\n  W1 = tf.get_variable(""W1"", [state_dim, 20],\n                       initializer=tf.random_normal_initializer())\n  b1 = tf.get_variable(""b1"", [20],\n                       initializer=tf.constant_initializer(0))\n  h1 = tf.nn.relu(tf.matmul(states, W1) + b1)\n  W2 = tf.get_variable(""W2"", [20, num_actions],\n                       initializer=tf.random_normal_initializer())\n  b2 = tf.get_variable(""b2"", [num_actions],\n                       initializer=tf.constant_initializer(0))\n  q = tf.matmul(h1, W2) + b2\n  return q\n\nq_learner = NeuralQLearner(sess,\n                           optimizer,\n                           observation_to_action,\n                           state_dim,\n                           num_actions,\n                           summary_writer=writer)\n\nMAX_EPISODES = 10000\nMAX_STEPS    = 200\n\nepisode_history = deque(maxlen=100)\nfor i_episode in range(MAX_EPISODES):\n\n  # initialize\n  state = env.reset()\n  total_rewards = 0\n\n  for t in range(MAX_STEPS):\n    env.render()\n    action = q_learner.eGreedyAction(state[np.newaxis,:])\n    next_state, reward, done, _ = env.step(action)\n\n    total_rewards += reward\n    # reward = -10 if done else 0.1 # normalize reward\n    q_learner.storeExperience(state, action, reward, next_state, done)\n\n    q_learner.updateModel()\n    state = next_state\n\n    if done: break\n\n  episode_history.append(total_rewards)\n  mean_rewards = np.mean(episode_history)\n\n  print(""Episode {}"".format(i_episode))\n  print(""Finished after {} timesteps"".format(t+1))\n  print(""Reward for this episode: {}"".format(total_rewards))\n  print(""Average reward for last 100 episodes: {:.2f}"".format(mean_rewards))\n  if mean_rewards >= 195.0:\n    print(""Environment {} solved after {} episodes"".format(env_name, i_episode+1))\n    break\n'"
run_ql_cartpole.py,0,"b'from __future__ import print_function\nfrom collections import deque\nfrom rl.tabular_q_learner import QLearner\nimport numpy as np\nimport gym\n\nenv_name = \'CartPole-v0\'\nenv = gym.make(env_name)\n\ncart_position_bins = np.linspace(-2.4, 2.4, num = 11)[1:-1]\npole_angle_bins = np.linspace(-2, 2, num = 11)[1:-1]\ncart_velocity_bins = np.linspace(-1, 1, num = 11)[1:-1]\nangle_rate_bins = np.linspace(-3.5, 3.5, num = 11)[1:-1]\n\ndef digitalizeState(observation):\n  return int("""".join([str(o) for o in observation]))\n\nstate_dim   = 10 ** env.observation_space.shape[0]\nnum_actions = env.action_space.n\n\nq_learner = QLearner(state_dim, num_actions)\n\nMAX_EPISODES = 10000\nMAX_STEPS    = 200\n\nepisode_history = deque(maxlen=100)\nfor i_episode in range(MAX_EPISODES):\n\n  # initialize\n  observation = env.reset()\n  cart_position, pole_angle, cart_velocity, angle_rate_of_change = observation\n  state = digitalizeState([np.digitize(cart_position, cart_position_bins),\n                            np.digitize(pole_angle, pole_angle_bins),\n                            np.digitize(cart_velocity, cart_velocity_bins),\n                            np.digitize(angle_rate_of_change, angle_rate_bins)])\n  action = q_learner.initializeState(state)\n  total_rewards = 0\n\n  for t in range(MAX_STEPS):\n    env.render()\n    observation, reward, done, _ = env.step(action)\n    cart_position, pole_angle, cart_velocity, angle_rate_of_change = observation\n\n    state = digitalizeState([np.digitize(cart_position, cart_position_bins),\n                              np.digitize(pole_angle, pole_angle_bins),\n                              np.digitize(cart_velocity, cart_velocity_bins),\n                              np.digitize(angle_rate_of_change, angle_rate_bins)])\n    \n\n    total_rewards += reward\n    if done: reward = -200   # normalize reward\n    action = q_learner.updateModel(state, reward)\n\n    if done: break\n\n  episode_history.append(total_rewards)\n  mean_rewards = np.mean(episode_history)\n\n  print(""Episode {}"".format(i_episode))\n  print(""Finished after {} timesteps"".format(t+1))\n  print(""Reward for this episode: {}"".format(total_rewards))\n  print(""Average reward for last 100 episodes: {:.2f}"".format(mean_rewards))\n  if mean_rewards >= 195.0:\n    print(""Environment {} solved after {} episodes"".format(env_name, i_episode+1))\n    break\n'"
run_reinforce_cartpole.py,13,"b'from __future__ import print_function\nfrom collections import deque\n\nfrom rl.pg_reinforce import PolicyGradientREINFORCE\nimport tensorflow as tf\nimport numpy as np\nimport gym\n\nenv_name = \'CartPole-v0\'\nenv = gym.make(env_name)\n\nsess = tf.Session()\noptimizer = tf.train.RMSPropOptimizer(learning_rate=0.0001, decay=0.9)\nwriter = tf.summary.FileWriter(""/tmp/{}-experiment-1"".format(env_name))\n\nstate_dim   = env.observation_space.shape[0]\nnum_actions = env.action_space.n\n\ndef policy_network(states):\n  # define policy neural network\n  W1 = tf.get_variable(""W1"", [state_dim, 20],\n                       initializer=tf.random_normal_initializer())\n  b1 = tf.get_variable(""b1"", [20],\n                       initializer=tf.constant_initializer(0))\n  h1 = tf.nn.tanh(tf.matmul(states, W1) + b1)\n  W2 = tf.get_variable(""W2"", [20, num_actions],\n                       initializer=tf.random_normal_initializer(stddev=0.1))\n  b2 = tf.get_variable(""b2"", [num_actions],\n                       initializer=tf.constant_initializer(0))\n  p = tf.matmul(h1, W2) + b2\n  return p\n\npg_reinforce = PolicyGradientREINFORCE(sess,\n                                       optimizer,\n                                       policy_network,\n                                       state_dim,\n                                       num_actions,\n                                       summary_writer=writer)\n\nMAX_EPISODES = 10000\nMAX_STEPS    = 200\n\nepisode_history = deque(maxlen=100)\nfor i_episode in range(MAX_EPISODES):\n\n  # initialize\n  state = env.reset()\n  total_rewards = 0\n\n  for t in range(MAX_STEPS):\n    env.render()\n    action = pg_reinforce.sampleAction(state[np.newaxis,:])\n    next_state, reward, done, _ = env.step(action)\n\n    total_rewards += reward\n    reward = -10 if done else 0.1 # normalize reward\n    pg_reinforce.storeRollout(state, action, reward)\n\n    state = next_state\n    if done: break\n\n  pg_reinforce.updateModel()\n\n  episode_history.append(total_rewards)\n  mean_rewards = np.mean(episode_history)\n\n  print(""Episode {}"".format(i_episode))\n  print(""Finished after {} timesteps"".format(t+1))\n  print(""Reward for this episode: {}"".format(total_rewards))\n  print(""Average reward for last 100 episodes: {:.2f}"".format(mean_rewards))\n  if mean_rewards >= 195.0 and len(episode_history) >= 100:\n    print(""Environment {} solved after {} episodes"".format(env_name, i_episode+1))\n    break\n'"
rl/__init__.py,0,b''
rl/neural_q_learner.py,44,"b'import random\nimport numpy as np\nimport tensorflow as tf\nfrom .replay_buffer import ReplayBuffer\n\nclass NeuralQLearner(object):\n\n  def __init__(self, session,\n                     optimizer,\n                     q_network,\n                     state_dim,\n                     num_actions,\n                     batch_size=32,\n                     init_exp=0.5,       # initial exploration prob\n                     final_exp=0.1,      # final exploration prob\n                     anneal_steps=10000, # N steps for annealing exploration\n                     replay_buffer_size=10000,\n                     store_replay_every=5, # how frequent to store experience\n                     discount_factor=0.9, # discount future rewards\n                     target_update_rate=0.01,\n                     reg_param=0.01, # regularization constants\n                     max_gradient=5, # max gradient norms\n                     double_q_learning=False,\n                     summary_writer=None,\n                     summary_every=100):\n\n    # tensorflow machinery\n    self.session        = session\n    self.optimizer      = optimizer\n    self.summary_writer = summary_writer\n\n    # model components\n    self.q_network     = q_network\n    self.replay_buffer = ReplayBuffer(buffer_size=replay_buffer_size)\n\n    # Q learning parameters\n    self.batch_size      = batch_size\n    self.state_dim       = state_dim\n    self.num_actions     = num_actions\n    self.exploration     = init_exp\n    self.init_exp        = init_exp\n    self.final_exp       = final_exp\n    self.anneal_steps    = anneal_steps\n    self.discount_factor = discount_factor\n    self.target_update_rate = target_update_rate\n    self.double_q_learning = double_q_learning\n\n    # training parameters\n    self.max_gradient = max_gradient\n    self.reg_param    = reg_param\n\n    # counters\n    self.store_replay_every   = store_replay_every\n    self.store_experience_cnt = 0\n    self.train_iteration      = 0\n\n    # create and initialize variables\n    self.create_variables()\n    var_lists = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    self.session.run(tf.variables_initializer(var_lists))\n\n    # make sure all variables are initialized\n    self.session.run(tf.assert_variables_initialized())\n\n    if self.summary_writer is not None:\n      # graph was not available when journalist was created\n      self.summary_writer.add_graph(self.session.graph)\n      self.summary_every = summary_every\n\n  def create_variables(self):\n    # compute action from a state: a* = argmax_a Q(s_t,a)\n    with tf.name_scope(""predict_actions""):\n      # raw state representation\n      self.states = tf.placeholder(tf.float32, (None, self.state_dim), name=""states"")\n      # initialize Q network\n      with tf.variable_scope(""q_network""):\n        self.q_outputs = self.q_network(self.states)\n      # predict actions from Q network\n      self.action_scores = tf.identity(self.q_outputs, name=""action_scores"")\n      tf.summary.histogram(""action_scores"", self.action_scores)\n      self.predicted_actions = tf.argmax(self.action_scores, axis=1, name=""predicted_actions"")\n\n    # estimate rewards using the next state: r(s_t,a_t) + argmax_a Q(s_{t+1}, a)\n    with tf.name_scope(""estimate_future_rewards""):\n      self.next_states = tf.placeholder(tf.float32, (None, self.state_dim), name=""next_states"")\n      self.next_state_mask = tf.placeholder(tf.float32, (None,), name=""next_state_masks"")\n\n      if self.double_q_learning:\n        # reuse Q network for action selection\n        with tf.variable_scope(""q_network"", reuse=True):\n          self.q_next_outputs = self.q_network(self.next_states)\n        self.action_selection = tf.argmax(tf.stop_gradient(self.q_next_outputs), 1, name=""action_selection"")\n        tf.summary.histogram(""action_selection"", self.action_selection)\n        self.action_selection_mask = tf.one_hot(self.action_selection, self.num_actions, 1, 0)\n        # use target network for action evaluation\n        with tf.variable_scope(""target_network""):\n          self.target_outputs = self.q_network(self.next_states) * tf.cast(self.action_selection_mask, tf.float32)\n        self.action_evaluation = tf.reduce_sum(self.target_outputs, axis=[1,])\n        tf.summary.histogram(""action_evaluation"", self.action_evaluation)\n        self.target_values = self.action_evaluation * self.next_state_mask\n      else:\n        # initialize target network\n        with tf.variable_scope(""target_network""):\n          self.target_outputs = self.q_network(self.next_states)\n        # compute future rewards\n        self.next_action_scores = tf.stop_gradient(self.target_outputs)\n        self.target_values = tf.reduce_max(self.next_action_scores, axis=[1,]) * self.next_state_mask\n        tf.summary.histogram(""next_action_scores"", self.next_action_scores)\n\n      self.rewards = tf.placeholder(tf.float32, (None,), name=""rewards"")\n      self.future_rewards = self.rewards + self.discount_factor * self.target_values\n\n    # compute loss and gradients\n    with tf.name_scope(""compute_temporal_differences""):\n      # compute temporal difference loss\n      self.action_mask = tf.placeholder(tf.float32, (None, self.num_actions), name=""action_mask"")\n      self.masked_action_scores = tf.reduce_sum(self.action_scores * self.action_mask, axis=[1,])\n      self.temp_diff = self.masked_action_scores - self.future_rewards\n      self.td_loss = tf.reduce_mean(tf.square(self.temp_diff))\n      # regularization loss\n      q_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""q_network"")\n      self.reg_loss = self.reg_param * tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in q_network_variables])\n      # compute total loss and gradients\n      self.loss = self.td_loss + self.reg_loss\n      gradients = self.optimizer.compute_gradients(self.loss)\n      # clip gradients by norm\n      for i, (grad, var) in enumerate(gradients):\n        if grad is not None:\n          gradients[i] = (tf.clip_by_norm(grad, self.max_gradient), var)\n      # add histograms for gradients.\n      for grad, var in gradients:\n        tf.summary.histogram(var.name, var)\n        if grad is not None:\n          tf.summary.histogram(var.name + \'/gradients\', grad)\n      self.train_op = self.optimizer.apply_gradients(gradients)\n\n    # update target network with Q network\n    with tf.name_scope(""update_target_network""):\n      self.target_network_update = []\n      # slowly update target network parameters with Q network parameters\n      q_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""q_network"")\n      target_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""target_network"")\n      for v_source, v_target in zip(q_network_variables, target_network_variables):\n        # this is equivalent to target = (1-alpha) * target + alpha * source\n        update_op = v_target.assign_sub(self.target_update_rate * (v_target - v_source))\n        self.target_network_update.append(update_op)\n      self.target_network_update = tf.group(*self.target_network_update)\n\n    # scalar summaries\n    tf.summary.scalar(""td_loss"", self.td_loss)\n    tf.summary.scalar(""reg_loss"", self.reg_loss)\n    tf.summary.scalar(""total_loss"", self.loss)\n    tf.summary.scalar(""exploration"", self.exploration)\n\n    self.summarize = tf.summary.merge_all()\n    self.no_op = tf.no_op()\n\n  def storeExperience(self, state, action, reward, next_state, done):\n    # always store end states\n    if self.store_experience_cnt % self.store_replay_every == 0 or done:\n      self.replay_buffer.add(state, action, reward, next_state, done)\n    self.store_experience_cnt += 1\n\n  def eGreedyAction(self, states, explore=True):\n    if explore and self.exploration > random.random():\n      return random.randint(0, self.num_actions-1)\n    else:\n      return self.session.run(self.predicted_actions, {self.states: states})[0]\n\n  def annealExploration(self, stategy=\'linear\'):\n    ratio = max((self.anneal_steps - self.train_iteration)/float(self.anneal_steps), 0)\n    self.exploration = (self.init_exp - self.final_exp) * ratio + self.final_exp\n\n  def updateModel(self):\n    # not enough experiences yet\n    if self.replay_buffer.count() < self.batch_size:\n      return\n\n    batch           = self.replay_buffer.getBatch(self.batch_size)\n    states          = np.zeros((self.batch_size, self.state_dim))\n    rewards         = np.zeros((self.batch_size,))\n    action_mask     = np.zeros((self.batch_size, self.num_actions))\n    next_states     = np.zeros((self.batch_size, self.state_dim))\n    next_state_mask = np.zeros((self.batch_size,))\n\n    for k, (s0, a, r, s1, done) in enumerate(batch):\n      states[k] = s0\n      rewards[k] = r\n      action_mask[k][a] = 1\n      # check terminal state\n      if not done:\n        next_states[k] = s1\n        next_state_mask[k] = 1\n\n    # whether to calculate summaries\n    calculate_summaries = self.summary_writer is not None and self.train_iteration % self.summary_every == 0\n\n    # perform one update of training\n    cost, _, summary_str = self.session.run([\n      self.loss,\n      self.train_op,\n      self.summarize if calculate_summaries else self.no_op\n    ], {\n      self.states:          states,\n      self.next_states:     next_states,\n      self.next_state_mask: next_state_mask,\n      self.action_mask:     action_mask,\n      self.rewards:         rewards\n    })\n\n    # update target network using Q-network\n    self.session.run(self.target_network_update)\n\n    # emit summaries\n    if calculate_summaries:\n      self.summary_writer.add_summary(summary_str, self.train_iteration)\n\n    self.annealExploration()\n    self.train_iteration += 1\n'"
rl/pg_actor_critic.py,37,"b'import random\nimport numpy as np\nimport tensorflow as tf\n\nclass PolicyGradientActorCritic(object):\n\n  def __init__(self, session,\n                     optimizer,\n                     actor_network,\n                     critic_network,\n                     state_dim,\n                     num_actions,\n                     init_exp=0.1,         # initial exploration prob\n                     final_exp=0.0,        # final exploration prob\n                     anneal_steps=1000,    # N steps for annealing exploration\n                     discount_factor=0.99, # discount future rewards\n                     reg_param=0.001,      # regularization constants\n                     max_gradient=5,       # max gradient norms\n                     summary_writer=None,\n                     summary_every=100):\n\n    # tensorflow machinery\n    self.session        = session\n    self.optimizer      = optimizer\n    self.summary_writer = summary_writer\n\n    # model components\n    self.actor_network  = actor_network\n    self.critic_network = critic_network\n\n    # training parameters\n    self.state_dim       = state_dim\n    self.num_actions     = num_actions\n    self.discount_factor = discount_factor\n    self.max_gradient    = max_gradient\n    self.reg_param       = reg_param\n\n    # exploration parameters\n    self.exploration  = init_exp\n    self.init_exp     = init_exp\n    self.final_exp    = final_exp\n    self.anneal_steps = anneal_steps\n\n    # counters\n    self.train_iteration = 0\n\n    # rollout buffer\n    self.state_buffer  = []\n    self.reward_buffer = []\n    self.action_buffer = []\n\n    # create and initialize variables\n    self.create_variables()\n    var_lists = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    self.session.run(tf.variables_initializer(var_lists))\n\n    # make sure all variables are initialized\n    self.session.run(tf.assert_variables_initialized())\n\n    if self.summary_writer is not None:\n      # graph was not available when journalist was created\n      self.summary_writer.add_graph(self.session.graph)\n      self.summary_every = summary_every\n\n  def resetModel(self):\n    self.cleanUp()\n    self.train_iteration = 0\n    self.exploration     = self.init_exp\n    var_lists = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    self.session.run(tf.variables_initializer(var_lists))\n\n  def create_variables(self):\n\n    with tf.name_scope(""model_inputs""):\n      # raw state representation\n      self.states = tf.placeholder(tf.float32, (None, self.state_dim), name=""states"")\n\n    # rollout action based on current policy\n    with tf.name_scope(""predict_actions""):\n      # initialize actor-critic network\n      with tf.variable_scope(""actor_network""):\n        self.policy_outputs = self.actor_network(self.states)\n      with tf.variable_scope(""critic_network""):\n        self.value_outputs = self.critic_network(self.states)\n\n      # predict actions from policy network\n      self.action_scores = tf.identity(self.policy_outputs, name=""action_scores"")\n      # Note 1: tf.multinomial is not good enough to use yet\n      # so we don\'t use self.predicted_actions for now\n      self.predicted_actions = tf.multinomial(self.action_scores, 1)\n\n    # get variable list\n    actor_network_variables  = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""actor_network"")\n    critic_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""critic_network"")\n\n    # compute loss and gradients\n    with tf.name_scope(""compute_pg_gradients""):\n      # gradients for selecting action from policy network\n      self.taken_actions = tf.placeholder(tf.int32, (None,), name=""taken_actions"")\n      self.discounted_rewards = tf.placeholder(tf.float32, (None,), name=""discounted_rewards"")\n\n      with tf.variable_scope(""actor_network"", reuse=True):\n        self.logprobs = self.actor_network(self.states)\n\n      with tf.variable_scope(""critic_network"", reuse=True):\n        self.estimated_values = self.critic_network(self.states)\n\n      # compute policy loss and regularization loss\n      self.cross_entropy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logprobs, labels=self.taken_actions)\n      self.pg_loss            = tf.reduce_mean(self.cross_entropy_loss)\n      self.actor_reg_loss     = tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in actor_network_variables])\n      self.actor_loss         = self.pg_loss + self.reg_param * self.actor_reg_loss\n\n      # compute actor gradients\n      self.actor_gradients = self.optimizer.compute_gradients(self.actor_loss, actor_network_variables)\n      # compute advantages A(s) = R - V(s)\n      self.advantages = tf.reduce_sum(self.discounted_rewards - self.estimated_values)\n      # compute policy gradients\n      for i, (grad, var) in enumerate(self.actor_gradients):\n        if grad is not None:\n          self.actor_gradients[i] = (grad * self.advantages, var)\n\n      # compute critic gradients\n      self.mean_square_loss = tf.reduce_mean(tf.square(self.discounted_rewards - self.estimated_values))\n      self.critic_reg_loss  = tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in critic_network_variables])\n      self.critic_loss      = self.mean_square_loss + self.reg_param * self.critic_reg_loss\n      self.critic_gradients = self.optimizer.compute_gradients(self.critic_loss, critic_network_variables)\n\n      # collect all gradients\n      self.gradients = self.actor_gradients + self.critic_gradients\n\n      # clip gradients\n      for i, (grad, var) in enumerate(self.gradients):\n        # clip gradients by norm\n        if grad is not None:\n          self.gradients[i] = (tf.clip_by_norm(grad, self.max_gradient), var)\n\n      # summarize gradients\n      for grad, var in self.gradients:\n        tf.summary.histogram(var.name, var)\n        if grad is not None:\n          tf.summary.histogram(var.name + \'/gradients\', grad)\n\n      # emit summaries\n      tf.summary.histogram(""estimated_values"", self.estimated_values)\n      tf.summary.scalar(""actor_loss"", self.actor_loss)\n      tf.summary.scalar(""critic_loss"", self.critic_loss)\n      tf.summary.scalar(""reg_loss"", self.actor_reg_loss + self.critic_reg_loss)\n\n    # training update\n    with tf.name_scope(""train_actor_critic""):\n      # apply gradients to update actor network\n      self.train_op = self.optimizer.apply_gradients(self.gradients)\n\n    self.summarize = tf.summary.merge_all()\n    self.no_op = tf.no_op()\n\n  def sampleAction(self, states):\n    # TODO: use this code piece when tf.multinomial gets better\n    # sample action from current policy\n    # actions = self.session.run(self.predicted_actions, {self.states: states})[0]\n    # return actions[0]\n\n    # temporary workaround\n    def softmax(y):\n      """""" simple helper function here that takes unnormalized logprobs """"""\n      maxy = np.amax(y)\n      e = np.exp(y - maxy)\n      return e / np.sum(e)\n\n    # epsilon-greedy exploration strategy\n    if random.random() < self.exploration:\n      return random.randint(0, self.num_actions-1)\n    else:\n      action_scores = self.session.run(self.action_scores, {self.states: states})[0]\n      action_probs  = softmax(action_scores) - 1e-5\n      action = np.argmax(np.random.multinomial(1, action_probs))\n      return action\n\n  def updateModel(self):\n\n    N = len(self.reward_buffer)\n    r = 0 # use discounted reward to approximate Q value\n\n    # compute discounted future rewards\n    discounted_rewards = np.zeros(N)\n    for t in reversed(range(N)):\n      # future discounted reward from now on\n      r = self.reward_buffer[t] + self.discount_factor * r\n      discounted_rewards[t] = r\n\n    # whether to calculate summaries\n    calculate_summaries = self.summary_writer is not None and self.train_iteration % self.summary_every == 0\n\n    # update policy network with the rollout in batches\n    for t in range(N-1):\n\n      # prepare inputs\n      states  = self.state_buffer[t][np.newaxis, :]\n      actions = np.array([self.action_buffer[t]])\n      rewards = np.array([discounted_rewards[t]])\n\n      # perform one update of training\n      _, summary_str = self.session.run([\n        self.train_op,\n        self.summarize if calculate_summaries else self.no_op\n      ], {\n        self.states:             states,\n        self.taken_actions:      actions,\n        self.discounted_rewards: rewards\n      })\n\n      # emit summaries\n      if calculate_summaries:\n        self.summary_writer.add_summary(summary_str, self.train_iteration)\n\n    self.annealExploration()\n    self.train_iteration += 1\n\n    # clean up\n    self.cleanUp()\n\n  def annealExploration(self, stategy=\'linear\'):\n    ratio = max((self.anneal_steps - self.train_iteration)/float(self.anneal_steps), 0)\n    self.exploration = (self.init_exp - self.final_exp) * ratio + self.final_exp\n\n  def storeRollout(self, state, action, reward):\n    self.action_buffer.append(action)\n    self.reward_buffer.append(reward)\n    self.state_buffer.append(state)\n\n  def cleanUp(self):\n    self.state_buffer  = []\n    self.reward_buffer = []\n    self.action_buffer = []\n'"
rl/pg_ddpg.py,44,"b'import random\nimport numpy as np\nimport tensorflow as tf\nfrom .replay_buffer import ReplayBuffer\n\nclass DeepDeterministicPolicyGradient(object):\n\n  def __init__(self, session,\n                     optimizer,\n                     actor_network,\n                     critic_network,\n                     state_dim,\n                     action_dim,\n                     batch_size=32,\n                     replay_buffer_size=1000000, # size of replay buffer\n                     store_replay_every=1,       # how frequent to store experience\n                     discount_factor=0.99,       # discount future rewards\n                     target_update_rate=0.01,\n                     reg_param=0.01,             # regularization constants\n                     max_gradient=5,             # max gradient norms\n                     noise_sigma=0.20,\n                     noise_theta=0.15,\n                     summary_writer=None,\n                     summary_every=100):\n\n    # tensorflow machinery\n    self.session        = session\n    self.optimizer      = optimizer\n    self.summary_writer = summary_writer\n\n    # model components\n    self.actor_network  = actor_network\n    self.critic_network = critic_network\n    self.replay_buffer  = ReplayBuffer(buffer_size=replay_buffer_size)\n\n    # training parameters\n    self.batch_size         = batch_size\n    self.state_dim          = state_dim\n    self.action_dim         = action_dim\n    self.discount_factor    = discount_factor\n    self.target_update_rate = target_update_rate\n    self.max_gradient       = max_gradient\n    self.reg_param          = reg_param\n\n    # Ornstein-Uhlenbeck noise for exploration\n    self.noise_var = tf.Variable(tf.zeros([1, action_dim]))\n    noise_random = tf.random_normal([1, action_dim], stddev=noise_sigma)\n    self.noise = self.noise_var.assign_sub((noise_theta) * self.noise_var - noise_random)\n\n    # counters\n    self.store_replay_every   = store_replay_every\n    self.store_experience_cnt = 0\n    self.train_iteration      = 0\n\n    # create and initialize variables\n    self.create_variables()\n    var_lists = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    self.session.run(tf.variables_initializer(var_lists))\n\n    # make sure all variables are initialized\n    self.session.run(tf.assert_variables_initialized())\n\n    if self.summary_writer is not None:\n      # graph was not available when journalist was created\n      self.summary_writer.add_graph(self.session.graph)\n      self.summary_every = summary_every\n\n  def create_variables(self):\n\n    with tf.name_scope(""model_inputs""):\n      # raw state representation\n      self.states = tf.placeholder(tf.float32, (None, self.state_dim), name=""states"")\n      # action input used by critic network\n      self.action = tf.placeholder(tf.float32, (None, self.action_dim), name=""action"")\n\n    # define outputs from the actor and the critic\n    with tf.name_scope(""predict_actions""):\n      # initialize actor-critic network\n      with tf.variable_scope(""actor_network""):\n        self.policy_outputs = self.actor_network(self.states)\n      with tf.variable_scope(""critic_network""):\n        self.value_outputs    = self.critic_network(self.states, self.action)\n        self.action_gradients = tf.gradients(self.value_outputs, self.action)[0]\n\n      # predict actions from policy network\n      self.predicted_actions = tf.identity(self.policy_outputs, name=""predicted_actions"")\n      tf.summary.histogram(""predicted_actions"", self.predicted_actions)\n      tf.summary.histogram(""action_scores"", self.value_outputs)\n\n    # get variable list\n    actor_network_variables  = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""actor_network"")\n    critic_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""critic_network"")\n\n    # estimate rewards using the next state: r + argmax_a Q\'(s_{t+1}, u\'(a))\n    with tf.name_scope(""estimate_future_rewards""):\n      self.next_states = tf.placeholder(tf.float32, (None, self.state_dim), name=""next_states"")\n      self.next_state_mask = tf.placeholder(tf.float32, (None,), name=""next_state_masks"")\n      self.rewards = tf.placeholder(tf.float32, (None,), name=""rewards"")\n\n      # initialize target network\n      with tf.variable_scope(""target_actor_network""):\n        self.target_actor_outputs = self.actor_network(self.next_states)\n      with tf.variable_scope(""target_critic_network""):\n        self.target_critic_outputs = self.critic_network(self.next_states, self.target_actor_outputs)\n\n      # compute future rewards\n      self.next_action_scores = tf.stop_gradient(self.target_critic_outputs)[:,0] * self.next_state_mask\n      tf.summary.histogram(""next_action_scores"", self.next_action_scores)\n      self.future_rewards = self.rewards + self.discount_factor * self.next_action_scores\n\n    # compute loss and gradients\n    with tf.name_scope(""compute_pg_gradients""):\n\n      # compute gradients for critic network\n      self.temp_diff        = self.value_outputs[:,0] - self.future_rewards\n      self.mean_square_loss = tf.reduce_mean(tf.square(self.temp_diff))\n      self.critic_reg_loss  = tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in critic_network_variables])\n      self.critic_loss      = self.mean_square_loss + self.reg_param * self.critic_reg_loss\n      self.critic_gradients = self.optimizer.compute_gradients(self.critic_loss, critic_network_variables)\n\n      # compute actor gradients (we don\'t do weight decay for actor network)\n      self.q_action_grad = tf.placeholder(tf.float32, (None, self.action_dim), name=""q_action_grad"")\n      actor_policy_gradients = tf.gradients(self.policy_outputs, actor_network_variables, -self.q_action_grad)\n      self.actor_gradients = list(zip(actor_policy_gradients, actor_network_variables))\n\n      # collect all gradients\n      self.gradients = self.actor_gradients + self.critic_gradients\n\n      # clip gradients\n      for i, (grad, var) in enumerate(self.gradients):\n        # clip gradients by norm\n        if grad is not None:\n          self.gradients[i] = (tf.clip_by_norm(grad, self.max_gradient), var)\n\n      # summarize gradients\n      for grad, var in self.gradients:\n        tf.summary.histogram(var.name, var)\n        if grad is not None:\n          tf.summary.histogram(var.name + \'/gradients\', grad)\n\n      # emit summaries\n      tf.summary.scalar(""critic_loss"", self.critic_loss)\n      tf.summary.scalar(""critic_td_loss"", self.mean_square_loss)\n      tf.summary.scalar(""critic_reg_loss"", self.critic_reg_loss)\n\n      # apply gradients to update actor network\n      self.train_op = self.optimizer.apply_gradients(self.gradients)\n\n    # update target network with Q network\n    with tf.name_scope(""update_target_network""):\n      self.target_network_update = []\n\n      # slowly update target network parameters with the actor network parameters\n      actor_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""actor_network"")\n      target_actor_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""target_actor_network"")\n      for v_source, v_target in zip(actor_network_variables, target_actor_network_variables):\n        # this is equivalent to target = (1-alpha) * target + alpha * source\n        update_op = v_target.assign_sub(self.target_update_rate * (v_target - v_source))\n        self.target_network_update.append(update_op)\n\n      # same for the critic network\n      critic_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""critic_network"")\n      target_critic_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""target_critic_network"")\n      for v_source, v_target in zip(critic_network_variables, target_critic_network_variables):\n        # this is equivalent to target = (1-alpha) * target + alpha * source\n        update_op = v_target.assign_sub(self.target_update_rate * (v_target - v_source))\n        self.target_network_update.append(update_op)\n\n      # group all assignment operations together\n      self.target_network_update = tf.group(*self.target_network_update)\n\n    self.summarize = tf.summary.merge_all()\n    self.no_op = tf.no_op()\n\n  def sampleAction(self, states, exploration=True):\n    policy_outs, ou_noise = self.session.run([\n      self.policy_outputs,\n      self.noise\n    ], {\n      self.states: states\n    })\n    # add OU noise for exploration\n    policy_outs = policy_outs + ou_noise if exploration else policy_outs\n    return policy_outs\n\n  def updateModel(self):\n\n    # not enough experiences yet\n    if self.replay_buffer.count() < self.batch_size:\n      return\n\n    batch           = self.replay_buffer.getBatch(self.batch_size)\n    states          = np.zeros((self.batch_size, self.state_dim))\n    rewards         = np.zeros((self.batch_size,))\n    actions         = np.zeros((self.batch_size, self.action_dim))\n    next_states     = np.zeros((self.batch_size, self.state_dim))\n    next_state_mask = np.zeros((self.batch_size,))\n\n    for k, (s0, a, r, s1, done) in enumerate(batch):\n      states[k]  = s0\n      rewards[k] = r\n      actions[k] = a\n      if not done:\n        next_states[k] = s1\n        next_state_mask[k] = 1\n\n    # whether to calculate summaries\n    calculate_summaries = self.summary_writer is not None and self.train_iteration % self.summary_every == 0\n\n    # compute a = u(s)\n    policy_outs = self.session.run(self.policy_outputs, {\n      self.states: states\n    })\n\n    # compute d_a Q(s,a) where s=s_i, a=u(s)\n    action_grads = self.session.run(self.action_gradients, {\n      self.states: states,\n      self.action: policy_outs\n    })\n\n    critic_loss, _, summary_str = self.session.run([\n      self.critic_loss,\n      self.train_op,\n      self.summarize if calculate_summaries else self.no_op\n    ], {\n      self.states:          states,\n      self.next_states:     next_states,\n      self.next_state_mask: next_state_mask,\n      self.action:          actions,\n      self.rewards:         rewards,\n      self.q_action_grad:   action_grads\n    })\n\n    # update target network using Q-network\n    self.session.run(self.target_network_update)\n\n    # emit summaries\n    if calculate_summaries:\n      self.summary_writer.add_summary(summary_str, self.train_iteration)\n\n    self.train_iteration += 1\n\n  def storeExperience(self, state, action, reward, next_state, done):\n    # always store end states\n    if self.store_experience_cnt % self.store_replay_every == 0 or done:\n      self.replay_buffer.add(state, action, reward, next_state, done)\n    self.store_experience_cnt += 1\n'"
rl/pg_reinforce.py,29,"b'import random\nimport numpy as np\nimport tensorflow as tf\n\nclass PolicyGradientREINFORCE(object):\n\n  def __init__(self, session,\n                     optimizer,\n                     policy_network,\n                     state_dim,\n                     num_actions,\n                     init_exp=0.5,         # initial exploration prob\n                     final_exp=0.0,        # final exploration prob\n                     anneal_steps=10000,   # N steps for annealing exploration\n                     discount_factor=0.99, # discount future rewards\n                     reg_param=0.001,      # regularization constants\n                     max_gradient=5,       # max gradient norms\n                     summary_writer=None,\n                     summary_every=100):\n\n    # tensorflow machinery\n    self.session        = session\n    self.optimizer      = optimizer\n    self.summary_writer = summary_writer\n\n    # model components\n    self.policy_network = policy_network\n\n    # training parameters\n    self.state_dim       = state_dim\n    self.num_actions     = num_actions\n    self.discount_factor = discount_factor\n    self.max_gradient    = max_gradient\n    self.reg_param       = reg_param\n\n    # exploration parameters\n    self.exploration  = init_exp\n    self.init_exp     = init_exp\n    self.final_exp    = final_exp\n    self.anneal_steps = anneal_steps\n\n    # counters\n    self.train_iteration = 0\n\n    # rollout buffer\n    self.state_buffer  = []\n    self.reward_buffer = []\n    self.action_buffer = []\n\n    # record reward history for normalization\n    self.all_rewards = []\n    self.max_reward_length = 1000000\n\n    # create and initialize variables\n    self.create_variables()\n    var_lists = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    self.session.run(tf.variables_initializer(var_lists))\n\n    # make sure all variables are initialized\n    self.session.run(tf.assert_variables_initialized())\n\n    if self.summary_writer is not None:\n      # graph was not available when journalist was created\n      self.summary_writer.add_graph(self.session.graph)\n      self.summary_every = summary_every\n\n  def resetModel(self):\n    self.cleanUp()\n    self.train_iteration = 0\n    self.exploration     = self.init_exp\n    var_lists = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    self.session.run(tf.variables_initializer(var_lists))\n\n  def create_variables(self):\n\n    with tf.name_scope(""model_inputs""):\n      # raw state representation\n      self.states = tf.placeholder(tf.float32, (None, self.state_dim), name=""states"")\n\n    # rollout action based on current policy\n    with tf.name_scope(""predict_actions""):\n      # initialize policy network\n      with tf.variable_scope(""policy_network""):\n        self.policy_outputs = self.policy_network(self.states)\n\n      # predict actions from policy network\n      self.action_scores = tf.identity(self.policy_outputs, name=""action_scores"")\n      # Note 1: tf.multinomial is not good enough to use yet\n      # so we don\'t use self.predicted_actions for now\n      self.predicted_actions = tf.multinomial(self.action_scores, 1)\n\n    # regularization loss\n    policy_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""policy_network"")\n\n    # compute loss and gradients\n    with tf.name_scope(""compute_pg_gradients""):\n      # gradients for selecting action from policy network\n      self.taken_actions = tf.placeholder(tf.int32, (None,), name=""taken_actions"")\n      self.discounted_rewards = tf.placeholder(tf.float32, (None,), name=""discounted_rewards"")\n\n      with tf.variable_scope(""policy_network"", reuse=True):\n        self.logprobs = self.policy_network(self.states)\n\n      # compute policy loss and regularization loss\n      self.cross_entropy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logprobs, labels=self.taken_actions)\n      self.pg_loss            = tf.reduce_mean(self.cross_entropy_loss)\n      self.reg_loss           = tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in policy_network_variables])\n      self.loss               = self.pg_loss + self.reg_param * self.reg_loss\n\n      # compute gradients\n      self.gradients = self.optimizer.compute_gradients(self.loss)\n\n      # compute policy gradients\n      for i, (grad, var) in enumerate(self.gradients):\n        if grad is not None:\n          self.gradients[i] = (grad * self.discounted_rewards, var)\n\n      for grad, var in self.gradients:\n        tf.summary.histogram(var.name, var)\n        if grad is not None:\n          tf.summary.histogram(var.name + \'/gradients\', grad)\n\n      # emit summaries\n      tf.summary.scalar(""policy_loss"", self.pg_loss)\n      tf.summary.scalar(""reg_loss"", self.reg_loss)\n      tf.summary.scalar(""total_loss"", self.loss)\n\n    # training update\n    with tf.name_scope(""train_policy_network""):\n      # apply gradients to update policy network\n      self.train_op = self.optimizer.apply_gradients(self.gradients)\n\n    self.summarize = tf.summary.merge_all()\n    self.no_op = tf.no_op()\n\n  def sampleAction(self, states):\n    # TODO: use this code piece when tf.multinomial gets better\n    # sample action from current policy\n    # actions = self.session.run(self.predicted_actions, {self.states: states})[0]\n    # return actions[0]\n\n    # temporary workaround\n    def softmax(y):\n      """""" simple helper function here that takes unnormalized logprobs """"""\n      maxy = np.amax(y)\n      e = np.exp(y - maxy)\n      return e / np.sum(e)\n\n    # epsilon-greedy exploration strategy\n    if random.random() < self.exploration:\n      return random.randint(0, self.num_actions-1)\n    else:\n      action_scores = self.session.run(self.action_scores, {self.states: states})[0]\n      action_probs  = softmax(action_scores) - 1e-5\n      action = np.argmax(np.random.multinomial(1, action_probs))\n      return action\n\n  def updateModel(self):\n\n    N = len(self.reward_buffer)\n    r = 0 # use discounted reward to approximate Q value\n\n    # compute discounted future rewards\n    discounted_rewards = np.zeros(N)\n    for t in reversed(range(N)):\n      # future discounted reward from now on\n      r = self.reward_buffer[t] + self.discount_factor * r\n      discounted_rewards[t] = r\n\n    # reduce gradient variance by normalization\n    self.all_rewards += discounted_rewards.tolist()\n    self.all_rewards = self.all_rewards[:self.max_reward_length]\n    discounted_rewards -= np.mean(self.all_rewards)\n    discounted_rewards /= np.std(self.all_rewards)\n\n    # whether to calculate summaries\n    calculate_summaries = self.summary_writer is not None and self.train_iteration % self.summary_every == 0\n\n    # update policy network with the rollout in batches\n    for t in range(N-1):\n\n      # prepare inputs\n      states  = self.state_buffer[t][np.newaxis, :]\n      actions = np.array([self.action_buffer[t]])\n      rewards = np.array([discounted_rewards[t]])\n\n      # evaluate gradients\n      grad_evals = [grad for grad, var in self.gradients]\n\n      # perform one update of training\n      _, summary_str = self.session.run([\n        self.train_op,\n        self.summarize if calculate_summaries else self.no_op\n      ], {\n        self.states:             states,\n        self.taken_actions:      actions,\n        self.discounted_rewards: rewards\n      })\n\n      # emit summaries\n      if calculate_summaries:\n        self.summary_writer.add_summary(summary_str, self.train_iteration)\n\n    self.annealExploration()\n    self.train_iteration += 1\n\n    # clean up\n    self.cleanUp()\n\n  def annealExploration(self, stategy=\'linear\'):\n    ratio = max((self.anneal_steps - self.train_iteration)/float(self.anneal_steps), 0)\n    self.exploration = (self.init_exp - self.final_exp) * ratio + self.final_exp\n\n  def storeRollout(self, state, action, reward):\n    self.action_buffer.append(action)\n    self.reward_buffer.append(reward)\n    self.state_buffer.append(state)\n\n  def cleanUp(self):\n    self.state_buffer  = []\n    self.reward_buffer = []\n    self.action_buffer = []\n'"
rl/replay_buffer.py,0,"b'from collections import deque\nimport random\n\nclass ReplayBuffer(object):\n\n  def __init__(self, buffer_size):\n\n    self.buffer_size = buffer_size\n    self.num_experiences = 0\n    self.buffer = deque()\n\n  def getBatch(self, batch_size):\n    # random draw N\n    return random.sample(self.buffer, batch_size)\n\n  def size(self):\n    return self.buffer_size\n\n  def add(self, state, action, reward, next_action, done):\n    new_experience = (state, action, reward, next_action, done)\n    if self.num_experiences < self.buffer_size:\n      self.buffer.append(new_experience)\n      self.num_experiences += 1\n    else:\n      self.buffer.popleft()\n      self.buffer.append(new_experience)\n\n  def count(self):\n    # if buffer is full, return buffer size\n    # otherwise, return experience counter\n    return self.num_experiences\n\n  def erase(self):\n    self.buffer = deque()\n    self.num_experiences = 0'"
rl/tabular_q_learner.py,0,"b""import random\nimport numpy as np\n\n# tabular Q-learning where states and actions\n# are discrete and stored in a table\nclass QLearner(object):\n\n  def __init__(self, state_dim,\n                     num_actions,\n                     init_exp=0.5,     # initial exploration prob\n                     final_exp=0.0,    # final exploration prob\n                     anneal_steps=500, # N steps for annealing exploration \n                     alpha = 0.2,\n                     discount_factor=0.9): # discount future rewards\n\n    # Q learning parameters\n    self.state_dim       = state_dim\n    self.num_actions     = num_actions\n    self.exploration     = init_exp\n    self.init_exp        = init_exp\n    self.final_exp       = final_exp\n    self.anneal_steps    = anneal_steps\n    self.discount_factor = discount_factor\n    self.alpha           = alpha\n\n    # counters\n    self.train_iteration = 0\n\n    # table of q values\n    self.qtable = np.random.uniform(low=-1, high=1, size=(state_dim, num_actions))\n\n  def initializeState(self, state):\n    self.state = state\n    self.action = self.qtable[state].argsort()[-1]\n    return self.action\n\n  # select action based on epsilon-greedy strategy\n  def eGreedyAction(self, state):\n    if self.exploration > random.random():\n      action = random.randint(0, self.num_actions-1)\n    else:\n      action = self.qtable[state].argsort()[-1]\n    return action\n\n  # do one value iteration update\n  def updateModel(self, state, reward):\n    action = self.eGreedyAction(state)\n\n    self.train_iteration += 1\n    self.annealExploration()\n    self.qtable[self.state, self.action] = (1 - self.alpha) * self.qtable[self.state, self.action] + self.alpha * (reward + self.discount_factor * self.qtable[state, action])\n\n    self.state = state\n    self.action = action\n\n    return self.action\n\n  # anneal learning rate\n  def annealExploration(self, stategy='linear'):\n    ratio = max((self.anneal_steps - self.train_iteration)/float(self.anneal_steps), 0)\n    self.exploration = (self.init_exp - self.final_exp) * ratio + self.final_exp\n"""
