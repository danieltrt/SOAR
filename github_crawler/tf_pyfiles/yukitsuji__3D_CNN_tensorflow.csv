file_path,api_count,code
input_velodyne.py,0,"b'#!/usr/bin/env python\nimport sys\nimport os\nimport rospy\nimport numpy as np\nimport cv2\nimport pcl\nimport glob\nimport math\nimport std_msgs.msg\nimport sensor_msgs.point_cloud2 as pc2\nfrom sensor_msgs.msg import PointCloud2\nfrom parse_xml import parseXML\n\n\ndef load_pc_from_pcd(pcd_path):\n    """"""Load PointCloud data from pcd file.""""""\n    p = pcl.load(pcd_path)\n    return np.array(list(p), dtype=np.float32)\n\ndef load_pc_from_bin(bin_path):\n    """"""Load PointCloud data from pcd file.""""""\n    obj = np.fromfile(bin_path, dtype=np.float32).reshape(-1, 4)\n    return obj\n\ndef read_label_from_txt(label_path):\n    """"""Read label from txt file.""""""\n    text = np.fromfile(label_path)\n    bounding_box = []\n    with open(label_path, ""r"") as f:\n        labels = f.read().split(""\\n"")\n        for label in labels:\n            if not label:\n                continue\n            label = label.split("" "")\n            if (label[0] == ""DontCare""):\n                continue\n\n            if label[0] == (""Car"" or ""Van""): #  or ""Truck""\n                bounding_box.append(label[8:15])\n\n    if bounding_box:\n        data = np.array(bounding_box, dtype=np.float32)\n        return data[:, 3:6], data[:, :3], data[:, 6]\n    else:\n        return None, None, None\n\ndef read_label_from_xml(label_path):\n    """"""Read label from xml file.\n\n    # Returns:\n        label_dic (dictionary): labels for one sequence.\n        size (list): Bounding Box Size. [l, w. h]?\n    """"""\n    labels = parseXML(label_path)\n    label_dic = {}\n    for label in labels:\n        first_frame = label.firstFrame\n        nframes = label.nFrames\n        size = label.size\n        obj_type = label.objectType\n        for index, place, rotate in zip(range(first_frame, first_frame+nframes), label.trans, label.rots):\n            if index in label_dic.keys():\n                label_dic[index][""place""] = np.vstack((label_dic[index][""place""], place))\n                label_dic[index][""size""] = np.vstack((label_dic[index][""size""], np.array(size)))\n                label_dic[index][""rotate""] = np.vstack((label_dic[index][""rotate""], rotate))\n            else:\n                label_dic[index] = {}\n                label_dic[index][""place""] = place\n                label_dic[index][""rotate""] = rotate\n                label_dic[index][""size""] = np.array(size)\n    return label_dic, size\n\ndef read_calib_file(calib_path):\n    """"""Read a calibration file.""""""\n    data = {}\n    with open(calib_path, \'r\') as f:\n        for line in f.readlines():\n            if not line or line == ""\\n"":\n                continue\n            key, value = line.split(\':\', 1)\n            try:\n                data[key] = np.array([float(x) for x in value.split()])\n            except ValueError:\n                pass\n    return data\n\ndef proj_to_velo(calib_data):\n    """"""Projection matrix to 3D axis for 3D Label""""""\n    rect = calib_data[""R0_rect""].reshape(3, 3)\n    velo_to_cam = calib_data[""Tr_velo_to_cam""].reshape(3, 4)\n    inv_rect = np.linalg.inv(rect)\n    inv_velo_to_cam = np.linalg.pinv(velo_to_cam[:, :3])\n    return np.dot(inv_velo_to_cam, inv_rect)\n\n\ndef filter_camera_angle(places):\n    """"""Filter camera angles for KiTTI Datasets""""""\n    bool_in = np.logical_and((places[:, 1] < places[:, 0] - 0.27), (-places[:, 1] < places[:, 0] - 0.27))\n    # bool_in = np.logical_and((places[:, 1] < places[:, 0]), (-places[:, 1] < places[:, 0]))\n    return places[bool_in]\n\ndef create_publish_obj(obj, places, rotates, size):\n    """"""Create object of correct data for publisher""""""\n    for place, rotate, sz in zip(places, rotates, size):\n        x, y, z = place\n        obj.append((x, y, z))\n        h, w, l = sz\n        if l > 10:\n            continue\n        for hei in range(0, int(h*100)):\n            for wid in range(0, int(w*100)):\n                for le in range(0, int(l*100)):\n                    a = (x - l / 2.) + le / 100.\n                    b = (y - w / 2.) + wid / 100.\n                    c = (z) + hei / 100.\n                    obj.append((a, b, c))\n    return obj\n\ndef get_boxcorners(places, rotates, size):\n    """"""Create 8 corners of bounding box from bottom center.""""""\n    corners = []\n    for place, rotate, sz in zip(places, rotates, size):\n        x, y, z = place\n        h, w, l = sz\n        if l > 10:\n            continue\n\n        corner = np.array([\n            [x - l / 2., y - w / 2., z],\n            [x + l / 2., y - w / 2., z],\n            [x - l / 2., y + w / 2., z],\n            [x - l / 2., y - w / 2., z + h],\n            [x - l / 2., y + w / 2., z + h],\n            [x + l / 2., y + w / 2., z],\n            [x + l / 2., y - w / 2., z + h],\n            [x + l / 2., y + w / 2., z + h],\n        ])\n\n        corner -= np.array([x, y, z])\n\n        rotate_matrix = np.array([\n            [np.cos(rotate), -np.sin(rotate), 0],\n            [np.sin(rotate), np.cos(rotate), 0],\n            [0, 0, 1]\n        ])\n\n        a = np.dot(corner, rotate_matrix.transpose())\n        a += np.array([x, y, z])\n        corners.append(a)\n    return np.array(corners)\n\ndef publish_pc2(pc, obj):\n    """"""Publisher of PointCloud data""""""\n    pub = rospy.Publisher(""/points_raw"", PointCloud2, queue_size=1000000)\n    rospy.init_node(""pc2_publisher"")\n    header = std_msgs.msg.Header()\n    header.stamp = rospy.Time.now()\n    header.frame_id = ""velodyne""\n    points = pc2.create_cloud_xyz32(header, pc[:, :3])\n\n    pub2 = rospy.Publisher(""/points_raw1"", PointCloud2, queue_size=1000000)\n    header = std_msgs.msg.Header()\n    header.stamp = rospy.Time.now()\n    header.frame_id = ""velodyne""\n    points2 = pc2.create_cloud_xyz32(header, obj)\n\n    r = rospy.Rate(0.1)\n    while not rospy.is_shutdown():\n        pub.publish(points)\n        pub2.publish(points2)\n        r.sleep()\n\ndef raw_to_voxel(pc, resolution=0.50, x=(0, 90), y=(-50, 50), z=(-4.5, 5.5)):\n    """"""Convert PointCloud2 to Voxel""""""\n    logic_x = np.logical_and(pc[:, 0] >= x[0], pc[:, 0] < x[1])\n    logic_y = np.logical_and(pc[:, 1] >= y[0], pc[:, 1] < y[1])\n    logic_z = np.logical_and(pc[:, 2] >= z[0], pc[:, 2] < z[1])\n    pc = pc[:, :3][np.logical_and(logic_x, np.logical_and(logic_y, logic_z))]\n    pc =((pc - np.array([x[0], y[0], z[0]])) / resolution).astype(np.int32)\n    voxel = np.zeros((int((x[1] - x[0]) / resolution), int((y[1] - y[0]) / resolution), int(round((z[1]-z[0]) / resolution))))\n    voxel[pc[:, 0], pc[:, 1], pc[:, 2]] = 1\n    return voxel\n\ndef center_to_sphere(places, size, resolution=0.50, min_value=np.array([0., -50., -4.5]), scale=4, x=(0, 90), y=(-50, 50), z=(-4.5, 5.5)):\n    """"""Convert object label to Training label for objectness loss""""""\n    x_logical = np.logical_and((places[:, 0] < x[1]), (places[:, 0] >= x[0]))\n    y_logical = np.logical_and((places[:, 1] < y[1]), (places[:, 1] >= y[0]))\n    z_logical = np.logical_and((places[:, 2] < z[1]), (places[:, 2] >= z[0]))\n    xyz_logical = np.logical_and(x_logical, np.logical_and(y_logical, z_logical))\n    center = places.copy()\n    center[:, 2] = center[:, 2] + size[:, 0] / 2.\n    sphere_center = ((center[xyz_logical] - min_value) / (resolution * scale)).astype(np.int32)\n    return sphere_center\n\ndef sphere_to_center(p_sphere, resolution=0.5, scale=4, min_value=np.array([0., -50., -4.5])):\n    """"""from sphere center to label center""""""\n    center = p_sphere * (resolution*scale) + min_value\n    return center\n\ndef voxel_to_corner(corner_vox, resolution, center):#TODO\n    """"""Create 3D corner from voxel and the diff to corner""""""\n    corners = center + corner_vox\n    return corners\n\ndef read_labels(label_path, label_type, calib_path=None, is_velo_cam=False, proj_velo=None):\n    """"""Read labels from xml or txt file.\n    Original Label value is shifted about 0.27m from object center.\n    So need to revise the position of objects.\n    """"""\n    if label_type == ""txt"": #TODO\n        places, size, rotates = read_label_from_txt(label_path)\n        if places is None:\n            return None, None, None\n        rotates = np.pi / 2 - rotates\n        dummy = np.zeros_like(places)\n        dummy = places.copy()\n        if calib_path:\n            places = np.dot(dummy, proj_velo.transpose())[:, :3]\n        else:\n            places = dummy\n        if is_velo_cam:\n            places[:, 0] += 0.27\n\n    elif label_type == ""xml"":\n        bounding_boxes, size = read_label_from_xml(label_path)\n        places = bounding_boxes[30][""place""]\n        rotates = bounding_boxes[30][""rotate""][:, 2]\n        size = bounding_boxes[30][""size""]\n\n    return places, rotates, size\n\ndef create_label(places, size, corners, resolution=0.50, x=(0, 90), y=(-50, 50), z=(-4.5, 5.5), scale=4, min_value=np.array([0., -50., -4.5])):\n    """"""Create training Labels which satisfy the range of experiment""""""\n    x_logical = np.logical_and((places[:, 0] < x[1]), (places[:, 0] >= x[0]))\n    y_logical = np.logical_and((places[:, 1] < y[1]), (places[:, 1] >= y[0]))\n    z_logical = np.logical_and((places[:, 2] + size[:, 0]/2. < z[1]), (places[:, 2] + size[:, 0]/2. >= z[0]))\n    xyz_logical = np.logical_and(x_logical, np.logical_and(y_logical, z_logical))\n\n    center = places.copy()\n    center[:, 2] = center[:, 2] + size[:, 0] / 2. # Move bottom to center\n    sphere_center = ((center[xyz_logical] - min_value) / (resolution * scale)).astype(np.int32)\n\n    train_corners = corners[xyz_logical].copy()\n    anchor_center = sphere_to_center(sphere_center, resolution=resolution, scale=scale, min_value=min_value) #sphere to center\n    for index, (corner, center) in enumerate(zip(corners[xyz_logical], anchor_center)):\n        train_corners[index] = corner - center\n    return sphere_center, train_corners\n\ndef corner_to_train(corners, sphere_center, resolution=0.50, x=(0, 90), y=(-50, 50), z=(-4.5, 5.5), scale=4, min_value=np.array([0., -50., -4.5])):\n    """"""Convert corner to Training label for regression loss""""""\n    x_logical = np.logical_and((corners[:, :, 0] < x[1]), (corners[:, :, 0] >= x[0]))\n    y_logical = np.logical_and((corners[:, :, 1] < y[1]), (corners[:, :, 1] >= y[0]))\n    z_logical = np.logical_and((corners[:, :, 2] < z[1]), (corners[:, :, 2] >= z[0]))\n    xyz_logical = np.logical_and(x_logical, np.logical_and(y_logical, z_logical)).all(axis=1)\n    train_corners = corners[xyz_logical].copy()\n    sphere_center = sphere_to_center(sphere_center, resolution=resolution, scale=scale, min_value=min_value) #sphere to center\n    for index, (corner, center) in enumerate(zip(corners[xyz_logical], sphere_center)):\n        train_corners[index] = corner - center\n    return train_corners\n\ndef corner_to_voxel(voxel_shape, corners, sphere_center, scale=4):\n    """"""Create final regression label from corner""""""\n    corner_voxel = np.zeros((voxel_shape[0] / scale, voxel_shape[1] / scale, voxel_shape[2] / scale, 24))\n    corner_voxel[sphere_center[:, 0], sphere_center[:, 1], sphere_center[:, 2]] = corners\n    return corner_voxel\n\ndef create_objectness_label(sphere_center, resolution=0.5, x=90, y=100, z=10, scale=4):\n    """"""Create Objectness label""""""\n    obj_maps = np.zeros((int(x / (resolution * scale)), int(y / (resolution * scale)), int(round(z / (resolution * scale)))))\n    obj_maps[sphere_center[:, 0], sphere_center[:, 1], sphere_center[:, 2]] = 1\n    return obj_maps\n\ndef process(velodyne_path, label_path=None, calib_path=None, dataformat=""pcd"", label_type=""txt"", is_velo_cam=False):\n    p = []\n    pc = None\n    bounding_boxes = None\n    places = None\n    rotates = None\n    size = None\n    proj_velo = None\n\n    if dataformat == ""bin"":\n        pc = load_pc_from_bin(velodyne_path)\n    elif dataformat == ""pcd"":\n        pc = load_pc_from_pcd(velodyne_path)\n\n    if calib_path:\n        calib = read_calib_file(calib_path)\n        proj_velo = proj_to_velo(calib)[:, :3]\n\n    if label_path:\n        places, rotates, size = read_labels(label_path, label_type, calib_path=calib_path, is_velo_cam=is_velo_cam, proj_velo=proj_velo)\n\n    corners = get_boxcorners(places, rotates, size)\n    print(""################"", len(pc))\n    pc = filter_camera_angle(pc)\n    # obj = []\n    # obj = create_publish_obj(obj, places, rotates, size)\n\n    p.append((0, 0, 0))\n    p.append((0, 0, -1))\n    print pc.shape\n    print 1\n    # publish_pc2(pc, obj)\n    a = center_to_sphere(places, size, resolution=0.25)\n    print places\n    print a\n    print sphere_to_center(a, resolution=0.25)\n    bbox = sphere_to_center(a, resolution=0.25)\n    print corners.shape\n    # publish_pc2(pc, bbox.reshape(-1, 3))\n    publish_pc2(pc, corners.reshape(-1, 3))\n\nif __name__ == ""__main__"":\n    # pcd_path = ""../data/training/velodyne/000012.pcd""\n    # label_path = ""../data/training/label_2/000012.txt""\n    # calib_path = ""../data/training/calib/000012.txt""\n    # process(pcd_path, label_path, calib_path=calib_path, dataformat=""pcd"")\n\n    # bin_path = ""../data/2011_09_26/2011_09_26_drive_0001_sync/velodyne_points/data/0000000030.bin""\n    # xml_path = ""../data/2011_09_26/2011_09_26_drive_0001_sync/tracklet_labels.xml""\n    # process(bin_path, xml_path, dataformat=""bin"", label_type=""xml"")\n\n    pcd_path = ""/home/katou01/download/training/velodyne/000410.bin""\n    label_path = ""/home/katou01/download/training/label_2/000410.txt""\n    calib_path = ""/home/katou01/download/training/calib/000410.txt""\n    process(pcd_path, label_path, calib_path=calib_path, dataformat=""bin"", is_velo_cam=True)\n'"
model_01_deconv.py,87,"b'#!/usr/bin/env python\nimport sys\nimport numpy as np\nimport tensorflow as tf\nfrom input_velodyne import *\nimport glob\n\ndef batch_norm(inputs, phase_train, decay=0.9, eps=1e-5):\n    """"""Batch Normalization\n\n       Args:\n           inputs: input data(Batch size) from last layer\n           phase_train: when you test, please set phase_train ""None""\n       Returns:\n           output for next layer\n    """"""\n    gamma = tf.get_variable(""gamma"", shape=inputs.get_shape()[-1], dtype=tf.float32, initializer=tf.constant_initializer(1.0))\n    beta = tf.get_variable(""beta"", shape=inputs.get_shape()[-1], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n    pop_mean = tf.get_variable(""pop_mean"", trainable=False, shape=inputs.get_shape()[-1], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n    pop_var = tf.get_variable(""pop_var"", trainable=False, shape=inputs.get_shape()[-1], dtype=tf.float32, initializer=tf.constant_initializer(1.0))\n    axes = range(len(inputs.get_shape()) - 1)\n\n    if phase_train != None:\n        batch_mean, batch_var = tf.nn.moments(inputs, axes)\n        train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean*(1 - decay))\n        train_var = tf.assign(pop_var, pop_var * decay + batch_var * (1 - decay))\n        with tf.control_dependencies([train_mean, train_var]):\n            return tf.nn.batch_normalization(inputs, batch_mean, batch_var, beta, gamma, eps)\n    else:\n        return tf.nn.batch_normalization(inputs, pop_mean, pop_var, beta, gamma, eps)\n\ndef conv3DLayer(input_layer, input_dim, output_dim, height, width, length, stride, activation=tf.nn.relu, padding=""SAME"", name="""", is_training=True):\n    with tf.variable_scope(""conv3D"" + name):\n        kernel = tf.get_variable(""weights"", shape=[length, height, width, input_dim, output_dim], \\\n            dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.01))\n        b = tf.get_variable(""bias"", shape=[output_dim], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n        conv = tf.nn.conv3d(input_layer, kernel, stride, padding=padding)\n        bias = tf.nn.bias_add(conv, b)\n        if activation:\n            bias = activation(bias, name=""activation"")\n        bias = batch_norm(bias, is_training)\n    return bias\n\ndef conv3D_to_output(input_layer, input_dim, output_dim, height, width, length, stride, activation=tf.nn.relu, padding=""SAME"", name=""""):\n    with tf.variable_scope(""conv3D"" + name):\n        kernel = tf.get_variable(""weights"", shape=[length, height, width, input_dim, output_dim], \\\n            dtype=tf.float32, initializer=tf.constant_initializer(0.01))\n        conv = tf.nn.conv3d(input_layer, kernel, stride, padding=padding)\n    return conv\n\ndef deconv3D_to_output(input_layer, input_dim, output_dim, height, width, length, stride, output_shape, activation=tf.nn.relu, padding=""SAME"", name=""""):\n    with tf.variable_scope(""deconv3D""+name):\n        kernel = tf.get_variable(""weights"", shape=[length, height, width, output_dim, input_dim], \\\n            dtype=tf.float32, initializer=tf.constant_initializer(0.01))\n        deconv = tf.nn.conv3d_transpose(input_layer, kernel, output_shape, stride, padding=""SAME"")\n    return deconv\n\ndef fully_connected(input_layer, shape, name="""", is_training=True):\n    with tf.variable_scope(""fully"" + name):\n        kernel = tf.get_variable(""weights"", shape=shape, \\\n            dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.01))\n        fully = tf.matmul(input_layer, kernel)\n        fully = tf.nn.relu(fully)\n        fully = batch_norm(fully, is_training)\n        return fully\n\nclass BNBLayer(object):\n    def __init__(self):\n        pass\n\n    def build_graph(self, voxel, activation=tf.nn.relu, is_training=True):\n        self.layer1 = conv3DLayer(voxel, 1, 16, 5, 5, 5, [1, 2, 2, 2, 1], name=""layer1"", activation=activation, is_training=is_training)\n        self.layer2 = conv3DLayer(self.layer1, 16, 32, 5, 5, 5, [1, 2, 2, 2, 1], name=""layer2"", activation=activation, is_training=is_training)\n        self.layer3 = conv3DLayer(self.layer2, 32, 64, 3, 3, 3, [1, 2, 2, 2, 1], name=""layer3"", activation=activation, is_training=is_training)\n        self.layer4 = conv3DLayer(self.layer3, 64, 64, 3, 3, 3, [1, 1, 1, 1, 1], name=""layer4"", activation=activation, is_training=is_training)\n        # base_shape = self.layer3.get_shape().as_list()\n        # obj_output_shape = [tf.shape(self.layer4)[0], base_shape[1], base_shape[2], base_shape[3], 2]\n        # cord_output_shape = [tf.shape(self.layer4)[0], base_shape[1], base_shape[2], base_shape[3], 24]\n        self.objectness = conv3D_to_output(self.layer4, 64, 2, 3, 3, 3, [1, 1, 1, 1, 1], name=""objectness"", activation=None)\n        self.cordinate = conv3D_to_output(self.layer4, 64, 24, 3, 3, 3, [1, 1, 1, 1, 1], name=""cordinate"", activation=None)\n        # self.objectness = deconv3D_to_output(self.layer4, 32, 2, 3, 3, 3, [1, 2, 2, 2, 1], obj_output_shape, name=""objectness"", activation=None)\n        # self.cordinate = deconv3D_to_output(self.layer4, 32, 24, 3, 3, 3, [1, 2, 2, 2, 1], cord_output_shape, name=""cordinate"", activation=None)\n        self.y = tf.nn.softmax(self.objectness, dim=-1)\n    # #original\n    # def build_graph(self, voxel, activation=tf.nn.relu, is_training=True):\n    #     self.layer1 = conv3DLayer(voxel, 1, 10, 5, 5, 5, [1, 2, 2, 2, 1], name=""layer1"", activation=activation, is_training=is_training)\n    #     self.layer2 = conv3DLayer(self.layer1, 10, 20, 5, 5, 5, [1, 2, 2, 2, 1], name=""layer2"", activation=activation, is_training=is_training)\n    #     self.layer3 = conv3DLayer(self.layer2, 20, 30, 3, 3, 3, [1, 2, 2, 2, 1], name=""layer3"", activation=activation, is_training=is_training)\n    #     base_shape = self.layer2.get_shape().as_list()\n    #     obj_output_shape = [tf.shape(self.layer3)[0], base_shape[1], base_shape[2], base_shape[3], 2]\n    #     cord_output_shape = [tf.shape(self.layer3)[0], base_shape[1], base_shape[2], base_shape[3], 24]\n    #     self.objectness = deconv3D_to_output(self.layer3, 30, 2, 3, 3, 3, [1, 2, 2, 2, 1], obj_output_shape, name=""objectness"", activation=None)\n    #     self.cordinate = deconv3D_to_output(self.layer3, 30, 24, 3, 3, 3, [1, 2, 2, 2, 1], cord_output_shape, name=""cordinate"", activation=None)\n    #     self.y = tf.nn.softmax(self.objectness, dim=-1)\n\ndef ssd_model(sess, voxel_shape=(300, 300, 300),activation=tf.nn.relu, is_training=True):\n    voxel = tf.placeholder(tf.float32, [None, voxel_shape[0], voxel_shape[1], voxel_shape[2], 1])\n    phase_train = tf.placeholder(tf.bool, name=\'phase_train\') if is_training else None\n    with tf.variable_scope(""3D_CNN_model"") as scope:\n        bnb_model = BNBLayer()\n        bnb_model.build_graph(voxel, activation=activation, is_training=phase_train)\n\n    if is_training:\n        initialized_var = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=""3D_CNN_model"")\n        sess.run(tf.variables_initializer(initialized_var))\n    return bnb_model, voxel, phase_train\n\ndef loss_func(model):\n    g_map = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list()[:4])\n    g_cord = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list())\n    object_loss = tf.multiply(g_map, model.objectness[:, :, :, :, 0])\n    non_gmap = tf.subtract(tf.ones_like(g_map, dtype=tf.float32), g_map)\n    nonobject_loss = tf.multiply(non_gmap, model.objectness[:, :, :, :, 1])\n    # sum_object_loss = tf.add(tf.exp(object_loss), tf.exp(nonobject_loss))\n    sum_object_loss = tf.exp(-tf.add(object_loss, nonobject_loss))\n    # sum_object_loss = tf.exp(-nonobject_loss)\n    bunbo = tf.add(tf.exp(-model.objectness[:, :, :, :, 0]), tf.exp(-model.objectness[:, :, :, :, 1]))\n    obj_loss = 0.005 * tf.reduce_sum(-tf.log(tf.div(sum_object_loss, bunbo)))\n\n    cord_diff = tf.multiply(g_map, tf.reduce_sum(tf.square(tf.subtract(model.cordinate, g_cord)), 4))\n    cord_loss = tf.reduce_sum(cord_diff)\n    return obj_loss, obj_loss, cord_loss, g_map, g_cord\n\ndef loss_func2(model):\n    g_map = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list()[:4])\n    obj_loss = tf.reduce_sum(tf.square(tf.subtract(model.objectness[:, :, :, :, 0], g_map)))\n\n    g_cord = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list())\n    cord_diff = tf.multiply(g_map, tf.reduce_sum(tf.square(tf.subtract(model.cordinate, g_cord)), 4))\n    cord_loss = tf.reduce_sum(cord_diff) * 0.1\n    return tf.add(obj_loss, cord_loss), g_map, g_cord\n\ndef loss_func3(model):\n    g_map = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list()[:4])\n    g_cord = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list())\n    non_gmap = tf.subtract(tf.ones_like(g_map, dtype=tf.float32), g_map)\n\n    elosion = 0.00001\n    y = model.y\n    is_obj_loss = -tf.reduce_sum(tf.multiply(g_map,  tf.log(y[:, :, :, :, 0] + elosion)))\n    non_obj_loss = tf.multiply(-tf.reduce_sum(tf.multiply(non_gmap, tf.log(y[:, :, :, :, 1] + elosion))), 0.0008)\n    cross_entropy = tf.add(is_obj_loss, non_obj_loss)\n    obj_loss = cross_entropy\n\n    g_cord = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list())\n    cord_diff = tf.multiply(g_map, tf.reduce_sum(tf.square(tf.subtract(model.cordinate, g_cord)), 4))\n    cord_loss = tf.multiply(tf.reduce_sum(cord_diff), 0.02)\n    return tf.add(obj_loss, cord_loss), obj_loss, cord_loss, is_obj_loss, non_obj_loss, g_map, g_cord, y\n\ndef create_optimizer(all_loss, lr=0.001):\n    opt = tf.train.AdamOptimizer(lr)\n    optimizer = opt.minimize(all_loss)\n    return optimizer\n\ndef train(batch_num, velodyne_path, label_path=None, calib_path=None, resolution=0.2, \\\n        dataformat=""pcd"", label_type=""txt"", is_velo_cam=False, scale=4, lr=0.01, \\\n        voxel_shape=(800, 800, 40), x=(0, 80), y=(-40, 40), z=(-2.5, 1.5), epoch=101):\n    # tf Graph input\n    batch_size = batch_num\n    training_epochs = epoch\n\n    with tf.Session() as sess:\n        model, voxel, phase_train = ssd_model(sess, voxel_shape=voxel_shape, activation=tf.nn.relu, is_training=True)\n        saver = tf.train.Saver()\n        total_loss, obj_loss, cord_loss, is_obj_loss, non_obj_loss, g_map, g_cord, y_pred = loss_func3(model)\n        optimizer = create_optimizer(total_loss, lr=lr)\n        init = tf.global_variables_initializer()\n        sess.run(init)\n\n        for epoch in range(training_epochs):\n            for (batch_x, batch_g_map, batch_g_cord) in lidar_generator(batch_num, velodyne_path, label_path=label_path, \\\n               calib_path=calib_path,resolution=resolution, dataformat=dataformat, label_type=label_type, is_velo_cam=is_velo_cam, \\\n               scale=scale, x=x, y=y, z=z):\n                # print batch_x.shape, batch_g_map.shape, batch_g_cord.shape, batch_num\n                # print batch_x.shape\n                # print batch_g_map.shape\n                # print batch_g_cord.shape\n                sess.run(optimizer, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord, phase_train:True})\n                # ct = sess.run(total_loss, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord, phase_train:True})\n                # co = sess.run(obj_loss, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord, phase_train:True})\n                cc = sess.run(cord_loss, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord, phase_train:True})\n                iol = sess.run(is_obj_loss, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord, phase_train:True})\n                nol = sess.run(non_obj_loss, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord, phase_train:True})\n                # print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(ct))\n                # print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(co))\n                print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(cc))\n                print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(iol))\n                print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(nol))\n            if (epoch != 0) and (epoch % 10 == 0):\n                print ""Save epoch "" + str(epoch)\n                saver.save(sess, ""velodyne_025_deconv_norm_valid"" + str(epoch) + "".ckpt"")\n        print(""Optimization Finished!"")\n\ndef train_test(batch_num, velodyne_path, label_path=None, calib_path=None, resolution=0.2, dataformat=""pcd"", label_type=""txt"", is_velo_cam=False, \\\n             scale=4, voxel_shape=(800, 800, 40), x=(0, 80), y=(-40, 40), z=(-2.5, 1.5)):\n    batch_size = batch_num\n    p = []\n    pc = None\n    bounding_boxes = None\n    places = None\n    rotates = None\n    size = None\n    proj_velo = None\n\n    if dataformat == ""bin"":\n        pc = load_pc_from_bin(velodyne_path)\n    elif dataformat == ""pcd"":\n        pc = load_pc_from_pcd(velodyne_path)\n\n    if calib_path:\n        calib = read_calib_file(calib_path)\n        proj_velo = proj_to_velo(calib)[:, :3]\n\n    if label_path:\n        places, rotates, size = read_labels(label_path, label_type, calib_path=calib_path, is_velo_cam=is_velo_cam, proj_velo=proj_velo)\n\n    corners = get_boxcorners(places, rotates, size)\n    filter_car_data(corners)\n    pc = filter_camera_angle(pc)\n\n    voxel =  raw_to_voxel(pc, resolution=resolution, x=x, y=y, z=z)\n    center_sphere = center_to_sphere(places, size, resolution=resolution)\n    corner_label = corner_to_train(corners, center_sphere, resolution=resolution)\n    g_map = create_objectness_label(center_sphere, resolution=resolution)\n    g_cord = corner_label.reshape(corner_label.shape[0], -1)\n\n    voxel_x = voxel.reshape(1, voxel.shape[0], voxel.shape[1], voxel.shape[2], 1)\n\n    with tf.Session() as sess:\n        is_training=None\n        model, voxel, phase_train = ssd_model(sess, voxel_shape=voxel_shape, activation=tf.nn.relu, is_training=is_training)\n        saver = tf.train.Saver()\n        new_saver = tf.train.import_meta_graph(""velodyne_025_deconv_norm_valid40.ckpt.meta"")\n        last_model = ""./velodyne_025_deconv_norm_valid40.ckpt""\n        saver.restore(sess, last_model)\n\n        objectness = model.objectness\n        cordinate = model.cordinate\n        y_pred = model.y\n        objectness = sess.run(objectness, feed_dict={voxel: voxel_x})[0, :, :, :, 0]\n        cordinate = sess.run(cordinate, feed_dict={voxel: voxel_x})[0]\n        y_pred = sess.run(y_pred, feed_dict={voxel: voxel_x})[0, :, :, :, 0]\n        print objectness.shape, objectness.max(), objectness.min()\n        print y_pred.shape, y_pred.max(), y_pred.min()\n\n        # print np.where(objectness >= 0.55)\n        index = np.where(y_pred >= 0.995)\n        print np.vstack((index[0], np.vstack((index[1], index[2])))).transpose()\n        print np.vstack((index[0], np.vstack((index[1], index[2])))).transpose().shape\n\n        a = center_to_sphere(places, size, resolution=resolution, x=x, y=y, z=z, \\\n            scale=scale, min_value=np.array([x[0], y[0], z[0]]))\n        label_center = sphere_to_center(a, resolution=resolution, \\\n            scale=scale, min_value=np.array([x[0], y[0], z[0]]))\n        label_corners = get_boxcorners(label_center, rotates, size)\n        print a[a[:, 0].argsort()]\n        # center = np.array([20, 57, 3])\n        #\n        # pred_center = sphere_to_center(center, resolution=resolution)\n        # print pred_center\n        # print cordinate.shape\n        # corners = cordinate[center[0], center[1], center[2]].reshape(-1, 3)\n        centers = np.vstack((index[0], np.vstack((index[1], index[2])))).transpose()\n        centers = sphere_to_center(centers, resolution=resolution, \\\n            scale=scale, min_value=np.array([x[0], y[0], z[0]]))\n        corners = cordinate[index].reshape(-1, 8, 3) + centers[:, np.newaxis]\n        print corners.shape\n        print voxel.shape\n        # publish_pc2(pc, corners.reshape(-1, 3))\n        publish_pc2(pc, corners.reshape(-1, 3))\n        # pred_corners = corners + pred_center\n        # print pred_corners\n\ndef test(batch_num, velodyne_path, label_path=None, calib_path=None, resolution=0.2, dataformat=""pcd"", label_type=""txt"", is_velo_cam=False, \\\n             scale=4, voxel_shape=(800, 800, 40), x=(0, 80), y=(-40, 40), z=(-2.5, 1.5)):\n    batch_size = batch_num\n    p = []\n    pc = None\n    bounding_boxes = None\n    places = None\n    rotates = None\n    size = None\n    proj_velo = None\n\n    if dataformat == ""bin"":\n        pc = load_pc_from_bin(velodyne_path)\n    elif dataformat == ""pcd"":\n        pc = load_pc_from_pcd(velodyne_path)\n\n    pc = filter_camera_angle(pc)\n    voxel =  raw_to_voxel(pc, resolution=resolution, x=x, y=y, z=z)\n    voxel_x = voxel.reshape(1, voxel.shape[0], voxel.shape[1], voxel.shape[2], 1)\n\n    with tf.Session() as sess:\n        is_training=None\n        model, voxel, phase_train = ssd_model(sess, voxel_shape=voxel_shape, activation=tf.nn.relu, is_training=is_training)\n        saver = tf.train.Saver()\n        new_saver = tf.train.import_meta_graph(""velodyne_025_deconv_norm_valid40.ckpt.meta"")\n        last_model = ""./velodyne_025_deconv_norm_valid40.ckpt""\n        saver.restore(sess, last_model)\n\n        objectness = model.objectness\n        cordinate = model.cordinate\n        y_pred = model.y\n        objectness = sess.run(objectness, feed_dict={voxel: voxel_x})[0, :, :, :, 0]\n        cordinate = sess.run(cordinate, feed_dict={voxel: voxel_x})[0]\n        y_pred = sess.run(y_pred, feed_dict={voxel: voxel_x})[0, :, :, :, 0]\n        print objectness.shape, objectness.max(), objectness.min()\n        print y_pred.shape, y_pred.max(), y_pred.min()\n\n        index = np.where(y_pred >= 0.995)\n        print np.vstack((index[0], np.vstack((index[1], index[2])))).transpose()\n        print np.vstack((index[0], np.vstack((index[1], index[2])))).transpose().shape\n\n        centers = np.vstack((index[0], np.vstack((index[1], index[2])))).transpose()\n        centers = sphere_to_center(centers, resolution=resolution, \\\n            scale=scale, min_value=np.array([x[0], y[0], z[0]]))\n        corners = cordinate[index].reshape(-1, 8, 3) + centers[:, np.newaxis]\n        print corners.shape\n        print voxel.shape\n        # publish_pc2(pc, corners.reshape(-1, 3))\n        publish_pc2(pc, corners.reshape(-1, 3))\n        # pred_corners = corners + pred_center\n        # print pred_corners\n\ndef lidar_generator(batch_num, velodyne_path, label_path=None, calib_path=None, resolution=0.2, dataformat=""pcd"", label_type=""txt"", is_velo_cam=False, \\\n                        scale=4, x=(0, 80), y=(-40, 40), z=(-2.5, 1.5)):\n    velodynes_path = glob.glob(velodyne_path)\n    labels_path = glob.glob(label_path)\n    calibs_path = glob.glob(calib_path)\n    velodynes_path.sort()\n    labels_path.sort()\n    calibs_path.sort()\n    iter_num = len(velodynes_path) // batch_num\n\n    for itn in range(iter_num):\n        batch_voxel = []\n        batch_g_map = []\n        batch_g_cord = []\n\n        for velodynes, labels, calibs in zip(velodynes_path[itn*batch_num:(itn+1)*batch_num], \\\n            labels_path[itn*batch_num:(itn+1)*batch_num], calibs_path[itn*batch_num:(itn+1)*batch_num]):\n            p = []\n            pc = None\n            bounding_boxes = None\n            places = None\n            rotates = None\n            size = None\n            proj_velo = None\n\n            if dataformat == ""bin"":\n                pc = load_pc_from_bin(velodynes)\n            elif dataformat == ""pcd"":\n                pc = load_pc_from_pcd(velodynes)\n\n            if calib_path:\n                calib = read_calib_file(calibs)\n                proj_velo = proj_to_velo(calib)[:, :3]\n\n            if label_path:\n                places, rotates, size = read_labels(labels, label_type, calib_path=calib_path, is_velo_cam=is_velo_cam, proj_velo=proj_velo)\n                if places is None:\n                    continue\n\n            corners = get_boxcorners(places, rotates, size)\n            pc = filter_camera_angle(pc)\n\n            voxel =  raw_to_voxel(pc, resolution=resolution, x=x, y=y, z=z)\n            center_sphere, corner_label = create_label(places, size, corners, resolution=resolution, x=x, y=y, z=z, \\\n                scale=scale, min_value=np.array([x[0], y[0], z[0]]))\n\n            if not center_sphere.shape[0]:\n                print 1\n                continue\n            g_map = create_objectness_label(center_sphere, resolution=resolution, x=(x[1] - x[0]), y=(y[1] - y[0]), z=(z[1] - z[0]), scale=scale)\n            g_cord = corner_label.reshape(corner_label.shape[0], -1)\n            g_cord = corner_to_voxel(voxel.shape, g_cord, center_sphere, scale=scale)\n\n            batch_voxel.append(voxel)\n            batch_g_map.append(g_map)\n            batch_g_cord.append(g_cord)\n        yield np.array(batch_voxel, dtype=np.float32)[:, :, :, :, np.newaxis], np.array(batch_g_map, dtype=np.float32), np.array(batch_g_cord, dtype=np.float32)\n\n\nif __name__ == \'__main__\':\n    # pcd_path = ""../data/training/velodyne/*.bin""\n    # label_path = ""../data/training/label_2/*.txt""\n    # calib_path = ""../data/training/calib/*.txt""\n    # train(5, pcd_path, label_path=label_path, resolution=0.1, calib_path=calib_path, dataformat=""bin"", is_velo_cam=True, \\\n    #         scale=8, voxel_shape=(800, 800, 40), x=(0, 80), y=(-40, 40), z=(-2.5, 1.5))\n    #\n    # pcd_path = ""../data/training/velodyne/005000.bin""\n    # label_path = ""../data/training/label_2/005000.txt""\n    # calib_path = ""../data/training/calib/005000.txt""\n    # pcd_path = ""../data/testing/velodyne/005000.bin""\n    # label_path = ""../data/testing/label_2/005000.txt""\n    # calib_path = ""../data/testing/calib/005000.txt""\n    # train_test(1, pcd_path, label_path=label_path, resolution=0.1, calib_path=calib_path, dataformat=""bin"", is_velo_cam=True, \\\n    #         scale=8, voxel_shape=(800, 800, 40), x=(0, 80), y=(-40, 40), z=(-2.5, 1.5))\n\n    pcd_path = ""/home/katou01/download/testing/velodyne/002397.bin""\n    calib_path = ""/home/katou01/download/testing/calib/002397.txt""\n    test(1, pcd_path, label_path=None, resolution=0.1, calib_path=calib_path, dataformat=""bin"", is_velo_cam=True, \\\n            scale=8, voxel_shape=(800, 800, 40), x=(0, 80), y=(-40, 40), z=(-2.5, 1.5))\n'"
model_025_deconv_norm.py,87,"b'#!/usr/bin/env python\nimport sys\nimport numpy as np\nimport tensorflow as tf\nfrom input_velodyne import *\nimport glob\n\ndef batch_norm(inputs, is_training, decay=0.9, eps=1e-5):\n    """"""Batch Normalization\n\n       Args:\n           inputs: input data(Batch size) from last layer\n           is_training: when you test, please set is_training ""None""\n       Returns:\n           output for next layer\n    """"""\n    gamma = tf.Variable(tf.ones(inputs.get_shape()[1:]), name=""gamma"")\n    beta = tf.Variable(tf.zeros(inputs.get_shape()[1:]), name=""beta"")\n    pop_mean = tf.Variable(tf.zeros(inputs.get_shape()[1:]), trainable=False, name=""pop_mean"")\n    pop_var = tf.Variable(tf.ones(inputs.get_shape()[1:]), trainable=False, name=""pop_var"")\n\n\n    def is_true():\n        batch_mean, batch_var = tf.nn.moments(inputs, [0])\n        train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean*(1 - decay))\n        train_var = tf.assign(pop_var, pop_var * decay + batch_var * (1 - decay))\n        with tf.control_dependencies([train_mean, train_var]):\n            return pop_mean, pop_var\n            # return tf.nn.batch_normalization(inputs, batch_mean, batch_var, beta, gamma, eps)\n\n    def is_false():\n        return pop_mean, pop_var\n        # return tf.nn.batch_normalization(inputs, pop_mean, pop_var, beta, gamma, eps)\n\n    mean, var = tf.cond(is_training, is_true, is_false)\n    normed = tf.nn.batch_normalization(inputs, mean, var, beta, gamma, eps)\n    # normed = tf.cond(is_training, is_true, is_false)\n    return normed\n\ndef conv3DLayer(input_layer, input_dim, output_dim, height, width, length, stride, activation=tf.nn.relu, padding=""SAME"", name="""", is_training=True):\n    with tf.variable_scope(""conv3D"" + name) as c3:\n        kernel = tf.get_variable(""weights"", shape=[length, height, width, input_dim, output_dim], \\\n            dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.01))\n        b = tf.get_variable(""bias"", shape=[output_dim], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n        conv = tf.nn.conv3d(input_layer, kernel, stride, padding=padding)\n        bias = tf.nn.bias_add(conv, b)\n        if activation:\n            bias = activation(bias, name=""activation"")\n        bias = batch_norm(bias, is_training)\n    return bias\n\ndef conv3D_to_output(input_layer, input_dim, output_dim, height, width, length, stride, activation=tf.nn.relu, padding=""SAME"", name=""""):\n    with tf.variable_scope(""conv3D"" + name):\n        kernel = tf.get_variable(""weights"", shape=[length, height, width, input_dim, output_dim], \\\n            dtype=tf.float32, initializer=tf.constant_initializer(0.01))\n        conv = tf.nn.conv3d(input_layer, kernel, stride, padding=padding)\n    return conv\n\ndef deconv3D_to_output(input_layer, input_dim, output_dim, height, width, length, stride, output_shape, activation=tf.nn.relu, padding=""SAME"", name=""""):\n    with tf.variable_scope(""deconv3D""+name):\n        kernel = tf.get_variable(""weights"", shape=[length, height, width, output_dim, input_dim], \\\n            dtype=tf.float32, initializer=tf.constant_initializer(0.01))\n        deconv = tf.nn.conv3d_transpose(input_layer, kernel, output_shape, stride, padding=""SAME"")\n    return deconv\n\ndef fully_connected(input_layer, shape, name="""", is_training=True):\n    with tf.variable_scope(""fully"" + name):\n        kernel = tf.get_variable(""weights"", shape=shape, \\\n            dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.01))\n        fully = tf.matmul(input_layer, kernel)\n        fully = tf.nn.relu(fully)\n        fully = batch_norm(fully, is_training)\n        return fully\n\nclass BNBLayer(object):\n    def __init__(self):\n        pass\n\n    def build_graph(self, voxel, activation=tf.nn.relu, is_training=True):\n        self.layer1 = conv3DLayer(voxel, 1, 10, 5, 5, 5, [1, 2, 2, 2, 1], name=""layer1"", activation=activation, is_training=is_training)\n        self.layer2 = conv3DLayer(self.layer1, 10, 15, 5, 5, 5, [1, 2, 2, 2, 1], name=""layer2"", activation=activation, is_training=is_training)\n        # self.layer3 = conv3DLayer(self.layer2, 15, 30, 3, 3, 3, [1, 1, 1, 1, 1], name=""layer3"", activation=activation, is_training=is_training)\n        # self.layer4 = conv3DLayer(self.layer3, 32, 32, 3, 3, 3, [1, 1, 1, 1, 1], name=""layer4"", activation=activation, is_training=is_training)\n        # self.layer4 = conv3DLayer(self.layer3, 32, 32, 3, 3, 3, [1, 2, 2, 2, 1], name=""layer4"", activation=activation, is_training=is_training)\n        # base_shape = self.layer3.get_shape().as_list()\n        # obj_output_shape = [tf.shape(self.layer4)[0], base_shape[1], base_shape[2], base_shape[3], 2]\n        # cord_output_shape = [tf.shape(self.layer4)[0], base_shape[1], base_shape[2], base_shape[3], 24]\n        self.objectness = conv3D_to_output(self.layer2, 15, 2, 3, 3, 3, [1, 1, 1, 1, 1], name=""objectness"", activation=None)\n        self.cordinate = conv3D_to_output(self.layer2, 15, 24, 3, 3, 3, [1, 1, 1, 1, 1], name=""cordinate"", activation=None)\n        # self.objectness = deconv3D_to_output(self.layer4, 32, 2, 3, 3, 3, [1, 2, 2, 2, 1], obj_output_shape, name=""objectness"", activation=None)\n        # self.cordinate = deconv3D_to_output(self.layer4, 32, 24, 3, 3, 3, [1, 2, 2, 2, 1], cord_output_shape, name=""cordinate"", activation=None)\n        self.y = tf.nn.softmax(self.objectness, dim=-1)\n\n    #original\n    # def build_graph(self, voxel, activation=tf.nn.relu, is_training=True):\n    #     self.layer1 = conv3DLayer(voxel, 1, 10, 5, 5, 5, [1, 2, 2, 2, 1], name=""layer1"", activation=activation, is_training=is_training)\n    #     self.layer2 = conv3DLayer(self.layer1, 10, 16, 5, 5, 5, [1, 2, 2, 2, 1], name=""layer2"", activation=activation, is_training=is_training)\n    #     self.layer3 = conv3DLayer(self.layer2, 16, 30, 3, 3, 3, [1, 2, 2, 2, 1], name=""layer3"", activation=activation, is_training=is_training)\n    #     base_shape = self.layer2.get_shape().as_list()\n    #     obj_output_shape = [tf.shape(self.layer3)[0], base_shape[1], base_shape[2], base_shape[3], 2]\n    #     cord_output_shape = [tf.shape(self.layer3)[0], base_shape[1], base_shape[2], base_shape[3], 24]\n    #     self.objectness = deconv3D_to_output(self.layer3, 30, 2, 3, 3, 3, [1, 2, 2, 2, 1], obj_output_shape, name=""objectness"", activation=None)\n    #     self.cordinate = deconv3D_to_output(self.layer3, 30, 24, 3, 3, 3, [1, 2, 2, 2, 1], cord_output_shape, name=""cordinate"", activation=None)\n    #     self.y = tf.nn.softmax(self.objectness, dim=-1)\n\ndef ssd_model(sess, voxel_shape=(300, 300, 300),activation=tf.nn.relu):\n    voxel = tf.placeholder(tf.float32, [None, voxel_shape[0], voxel_shape[1], voxel_shape[2], 1])\n    phase_train = tf.placeholder(tf.bool, name=\'phase_train\')\n    with tf.variable_scope(""3D_CNN_model"") as scope:\n        bnb_model = BNBLayer()\n        bnb_model.build_graph(voxel, activation=activation, is_training=phase_train)\n\n    initialized_var = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=""3D_CNN_model"")\n    sess.run(tf.variables_initializer(initialized_var))\n    return bnb_model, voxel, phase_train\n\ndef loss_func(model):\n    g_map = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list()[:4])\n    g_cord = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list())\n    object_loss = tf.multiply(g_map, model.objectness[:, :, :, :, 0])\n    non_gmap = tf.subtract(tf.ones_like(g_map, dtype=tf.float32), g_map)\n    nonobject_loss = tf.multiply(non_gmap, model.objectness[:, :, :, :, 1])\n    # sum_object_loss = tf.add(tf.exp(object_loss), tf.exp(nonobject_loss))\n    sum_object_loss = tf.exp(-tf.add(object_loss, nonobject_loss))\n    # sum_object_loss = tf.exp(-nonobject_loss)\n    bunbo = tf.add(tf.exp(-model.objectness[:, :, :, :, 0]), tf.exp(-model.objectness[:, :, :, :, 1]))\n    obj_loss = 0.005 * tf.reduce_sum(-tf.log(tf.div(sum_object_loss, bunbo)))\n\n    cord_diff = tf.multiply(g_map, tf.reduce_sum(tf.square(tf.subtract(model.cordinate, g_cord)), 4))\n    cord_loss = tf.reduce_sum(cord_diff)\n    return obj_loss, obj_loss, cord_loss, g_map, g_cord\n\ndef loss_func2(model):\n    g_map = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list()[:4])\n    obj_loss = tf.reduce_sum(tf.square(tf.subtract(model.objectness[:, :, :, :, 0], g_map)))\n\n    g_cord = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list())\n    cord_diff = tf.multiply(g_map, tf.reduce_sum(tf.square(tf.subtract(model.cordinate, g_cord)), 4))\n    cord_loss = tf.reduce_sum(cord_diff) * 0.1\n    return tf.add(obj_loss, cord_loss), g_map, g_cord\n\ndef loss_func3(model):\n    g_map = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list()[:4])\n    g_cord = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list())\n    non_gmap = tf.subtract(tf.ones_like(g_map, dtype=tf.float32), g_map)\n\n    elosion = 0.00001\n    y = model.y\n    is_obj_loss = -tf.reduce_sum(tf.multiply(g_map,  tf.log(y[:, :, :, :, 0] + elosion)))\n    non_obj_loss = tf.multiply(-tf.reduce_sum(tf.multiply(non_gmap, tf.log(y[:, :, :, :, 1] + elosion))), 0.0008)\n    cross_entropy = tf.add(is_obj_loss, non_obj_loss)\n    obj_loss = cross_entropy\n\n    g_cord = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list())\n    cord_diff = tf.multiply(g_map, tf.reduce_sum(tf.square(tf.subtract(model.cordinate, g_cord)), 4))\n    cord_loss = tf.multiply(tf.reduce_sum(cord_diff), 0.02)\n    return tf.add(obj_loss, cord_loss), obj_loss, cord_loss, is_obj_loss, non_obj_loss, g_map, g_cord, y\n\ndef create_optimizer(all_loss, lr=0.001):\n    opt = tf.train.AdamOptimizer(lr)\n    optimizer = opt.minimize(all_loss)\n    return optimizer\n\ndef train(batch_num, velodyne_path, label_path=None, calib_path=None, resolution=0.2, dataformat=""pcd"", label_type=""txt"", is_velo_cam=False, \\\n        scale=4, voxel_shape=(800, 800, 40), x=(0, 80), y=(-40, 40), z=(-2.5, 1.5)):\n    # tf Graph input\n    batch_size = batch_num\n    training_epochs = 101\n\n    with tf.Session() as sess:\n        model, voxel, phase_train = ssd_model(sess, voxel_shape=voxel_shape, activation=tf.nn.relu)\n        saver = tf.train.Saver()\n        total_loss, obj_loss, cord_loss, is_obj_loss, non_obj_loss, g_map, g_cord, y_pred = loss_func3(model)\n        optimizer = create_optimizer(total_loss, lr=0.01)\n        init = tf.global_variables_initializer()\n        sess.run(init)\n\n        for epoch in range(training_epochs):\n            for (batch_x, batch_g_map, batch_g_cord) in lidar_generator(batch_num, velodyne_path, label_path=label_path, \\\n               calib_path=calib_path,resolution=resolution, dataformat=dataformat, label_type=label_type, is_velo_cam=is_velo_cam, \\\n               scale=scale, x=x, y=y, z=z):\n                # print batch_x.shape, batch_g_map.shape, batch_g_cord.shape, batch_num\n                # print batch_x.shape\n                # print batch_g_map.shape\n                # print batch_g_cord.shape\n                sess.run(optimizer, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord, phase_train:True})\n\n                # ct = sess.run(total_loss, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord, phase_train:True})\n                # co = sess.run(obj_loss, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord, phase_train:True})\n                cc = sess.run(cord_loss, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord, phase_train:True})\n                iol = sess.run(is_obj_loss, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord, phase_train:True})\n                nol = sess.run(non_obj_loss, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord, phase_train:True})\n                # soft = sess.run(y, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord})\n                # print soft[0, 0, 0, 0, :]\n                # print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(ct))\n                # print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(co))\n                print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(cc))\n                print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(iol))\n                print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(nol))\n                # print """"\n\n            if (epoch != 0) and (epoch % 1 == 0):\n                print ""Save epoch "" + str(epoch)\n                saver.save(sess, ""velodyne_025_deconv_norm"" + str(epoch) + "".ckpt"")\n        print(""Optimization Finished!"")\n\ndef test(batch_num, velodyne_path, label_path=None, calib_path=None, resolution=0.2, dataformat=""pcd"", label_type=""txt"", is_velo_cam=False, \\\n             scale=4, voxel_shape=(800, 800, 40), x=(0, 80), y=(-40, 40), z=(-2.5, 1.5)):\n    # tf Graph input\n    batch_size = batch_num # 1\n    training_epochs = 5\n    p = []\n    pc = None\n    bounding_boxes = None\n    places = None\n    rotates = None\n    size = None\n    proj_velo = None\n\n    if dataformat == ""bin"":\n        pc = load_pc_from_bin(velodyne_path)\n    elif dataformat == ""pcd"":\n        pc = load_pc_from_pcd(velodyne_path)\n\n    if calib_path:\n        calib = read_calib_file(calib_path)\n        proj_velo = proj_to_velo(calib)[:, :3]\n\n    if label_path:\n        places, rotates, size = read_labels(label_path, label_type, calib_path=calib_path, is_velo_cam=is_velo_cam, proj_velo=proj_velo)\n\n    corners = get_boxcorners(places, rotates, size)\n    filter_car_data(corners)\n    pc = filter_camera_angle(pc)\n\n    voxel =  raw_to_voxel(pc, resolution=resolution, x=x, y=y, z=z)\n    center_sphere = center_to_sphere(places, size, resolution=resolution)\n    corner_label = corner_to_train(corners, center_sphere, resolution=resolution)\n    g_map = create_objectness_label(center_sphere, resolution=resolution)\n    g_cord = corner_label.reshape(corner_label.shape[0], -1)\n\n    voxel_x = voxel.reshape(1, voxel.shape[0], voxel.shape[1], voxel.shape[2], 1)\n\n    with tf.Session() as sess:\n        model, voxel, phase_train = ssd_model(sess, voxel_shape=voxel_shape, activation=tf.nn.relu)\n        # optimizer = create_optimizer(total_loss)\n        saver = tf.train.Saver()\n        new_saver = tf.train.import_meta_graph(""velodyne_025_deconv_norm1.ckpt.meta"")\n        # last_model = tf.train.latest_checkpoint(\'./velodyne_10th_try_900.ckpt\')\n        last_model = ""./velodyne_025_deconv_norm1.ckpt""\n        saver.restore(sess, last_model)\n\n        objectness = model.objectness\n        cordinate = model.cordinate\n        y_pred = model.y\n        objectness = sess.run(objectness, feed_dict={voxel: voxel_x, phase_train:False})[0, :, :, :, 0]\n        cordinate = sess.run(cordinate, feed_dict={voxel: voxel_x, phase_train:False})[0]\n        y_pred = sess.run(y_pred, feed_dict={voxel: voxel_x, phase_train:False})[0, :, :, :, 0]\n        print objectness.shape, objectness.max(), objectness.min()\n        print y_pred.shape, y_pred.max(), y_pred.min()\n\n        # print np.where(objectness >= 0.55)\n        index = np.where(y_pred >= 0.995)\n        print np.vstack((index[0], np.vstack((index[1], index[2])))).transpose()\n        print np.vstack((index[0], np.vstack((index[1], index[2])))).transpose().shape\n\n        a = center_to_sphere(places, size, resolution=resolution, x=x, y=y, z=z, \\\n            scale=scale, min_value=np.array([x[0], y[0], z[0]]))\n        label_center = sphere_to_center(a, resolution=resolution, \\\n            scale=scale, min_value=np.array([x[0], y[0], z[0]]))\n        label_corners = get_boxcorners(label_center, rotates, size)\n        print a[a[:, 0].argsort()]\n        # center = np.array([20, 57, 3])\n        #\n        # pred_center = sphere_to_center(center, resolution=resolution)\n        # print pred_center\n        # print cordinate.shape\n        # corners = cordinate[center[0], center[1], center[2]].reshape(-1, 3)\n        centers = np.vstack((index[0], np.vstack((index[1], index[2])))).transpose()\n        centers = sphere_to_center(centers, resolution=resolution, \\\n            scale=scale, min_value=np.array([x[0], y[0], z[0]]))\n        corners = cordinate[index].reshape(-1, 8, 3) + centers[:, np.newaxis]\n        print corners.shape\n        print voxel.shape\n        # publish_pc2(pc, corners.reshape(-1, 3))\n        publish_pc2(pc, label_corners.reshape(-1, 3))\n        # pred_corners = corners + pred_center\n        # print pred_corners\n\ndef lidar_generator(batch_num, velodyne_path, label_path=None, calib_path=None, resolution=0.2, dataformat=""pcd"", label_type=""txt"", is_velo_cam=False, \\\n                        scale=4, x=(0, 80), y=(-40, 40), z=(-2.5, 1.5)):\n    velodynes_path = glob.glob(velodyne_path)\n    labels_path = glob.glob(label_path)\n    calibs_path = glob.glob(calib_path)\n    velodynes_path.sort()\n    labels_path.sort()\n    calibs_path.sort()\n    iter_num = len(velodynes_path) // batch_num\n\n    for itn in range(iter_num):\n        batch_voxel = []\n        batch_g_map = []\n        batch_g_cord = []\n\n        for velodynes, labels, calibs in zip(velodynes_path[itn*batch_num:(itn+1)*batch_num], \\\n            labels_path[itn*batch_num:(itn+1)*batch_num], calibs_path[itn*batch_num:(itn+1)*batch_num]):\n            p = []\n            pc = None\n            bounding_boxes = None\n            places = None\n            rotates = None\n            size = None\n            proj_velo = None\n\n            if dataformat == ""bin"":\n                pc = load_pc_from_bin(velodynes)\n            elif dataformat == ""pcd"":\n                pc = load_pc_from_pcd(velodynes)\n\n            if calib_path:\n                calib = read_calib_file(calibs)\n                proj_velo = proj_to_velo(calib)[:, :3]\n\n            if label_path:\n                places, rotates, size = read_labels(labels, label_type, calib_path=calib_path, is_velo_cam=is_velo_cam, proj_velo=proj_velo)\n                if places is None:\n                    continue\n\n            corners = get_boxcorners(places, rotates, size)\n            filter_car_data(corners)\n            pc = filter_camera_angle(pc)\n\n            voxel =  raw_to_voxel(pc, resolution=resolution, x=x, y=y, z=z)\n            # center_sphere = center_to_sphere(places, size, resolution=resolution, min_value=np.array([0., -40, -2.5]), scale=scale, x=x, y=y, z=(-2.5, 2.3))\n            # corner_label = corner_to_train(corners, center_sphere, resolution=resolution, x=x, y=y, z=(-2.5, 2.3), scale=scale, min_value=np.array([0., -40, -2.5]))\n            center_sphere, corner_label = create_label(places, size, corners, resolution=resolution, x=x, y=y, z=z, \\\n                scale=scale, min_value=np.array([x[0], y[0], z[0]]))\n\n            # print center_sphere\n            if not center_sphere.shape[0]:\n                print 1\n                continue\n            g_map = create_objectness_label(center_sphere, resolution=resolution, x=(x[1] - x[0]), y=(y[1] - y[0]), z=(z[1] - z[0]), scale=scale)\n            g_cord = corner_label.reshape(corner_label.shape[0], -1)\n            g_cord = corner_to_voxel(voxel.shape, g_cord, center_sphere, scale=scale)\n\n            batch_voxel.append(voxel)\n            batch_g_map.append(g_map)\n            batch_g_cord.append(g_cord)\n        yield np.array(batch_voxel, dtype=np.float32)[:, :, :, :, np.newaxis], np.array(batch_g_map, dtype=np.float32), np.array(batch_g_cord, dtype=np.float32)\n\n\nif __name__ == \'__main__\':\n    # pcd_path = ""../data/training/velodyne/*.bin""\n    # label_path = ""../data/training/label_2/*.txt""\n    # calib_path = ""../data/training/calib/*.txt""\n    # train(5, pcd_path, label_path=label_path, resolution=0.25, calib_path=calib_path, dataformat=""bin"", is_velo_cam=True, \\\n    #         scale=4, voxel_shape=(360, 400, 40), x=(0, 90), y=(-50, 50), z=(-5.5, 4.5))\n    # #\n    pcd_path = ""../data/training/velodyne/004000.bin""\n    label_path = ""../data/training/label_2/004000.txt""\n    calib_path = ""../data/training/calib/004000.txt""\n    test(1, pcd_path, label_path=label_path, resolution=0.25, calib_path=calib_path, dataformat=""bin"", is_velo_cam=True, \\\n            scale=4, voxel_shape=(360, 400, 40), x=(0, 90), y=(-50, 50), z=(-5.5, 4.5))\n    # test(1, pcd_path, label_path=label_path, resolution=0.1, calib_path=calib_path, dataformat=""bin"", is_velo_cam=True, scale=8, voxel_shape=(800, 800, 40))\n'"
model_025_deconv_norm_validate.py,103,"b'#!/usr/bin/env python\nimport sys\nimport numpy as np\nimport tensorflow as tf\nfrom input_velodyne import *\nimport glob\n\n#original\ndef batch_norm(inputs, phase_train, decay=0.9, eps=1e-5):\n    """"""Batch Normalization\n\n       Args:\n           inputs: input data(Batch size) from last layer\n           phase_train: when you test, please set phase_train ""None""\n       Returns:\n           output for next layer\n    """"""\n    gamma = tf.get_variable(""gamma"", shape=inputs.get_shape()[-1], dtype=tf.float32, initializer=tf.constant_initializer(1.0))\n    beta = tf.get_variable(""beta"", shape=inputs.get_shape()[-1], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n    pop_mean = tf.get_variable(""pop_mean"", trainable=False, shape=inputs.get_shape()[-1], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n    pop_var = tf.get_variable(""pop_var"", trainable=False, shape=inputs.get_shape()[-1], dtype=tf.float32, initializer=tf.constant_initializer(1.0))\n\n    # gamma = tf.Variable(tf.ones(inputs.get_shape()[1:]), name=""gamma"")\n    # beta = tf.Variable(tf.zeros(inputs.get_shape()[1:]), name=""beta"")\n    # pop_mean = tf.Variable(tf.zeros(inputs.get_shape()[1:]), trainable=False, name=""pop_mean"")\n    # pop_var = tf.Variable(tf.ones(inputs.get_shape()[1:]), trainable=False, name=""pop_var"")\n    axes = range(len(inputs.get_shape()) - 1)\n\n    if phase_train != None:\n        batch_mean, batch_var = tf.nn.moments(inputs, axes)\n        train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean*(1 - decay))\n        train_var = tf.assign(pop_var, pop_var * decay + batch_var * (1 - decay))\n        with tf.control_dependencies([train_mean, train_var]):\n            return tf.nn.batch_normalization(inputs, batch_mean, batch_var, beta, gamma, eps)\n    else:\n        return tf.nn.batch_normalization(inputs, pop_mean, pop_var, beta, gamma, eps)\n\n# def batch_norm(inputs, phase_train, decay=0.9, eps=1e-5):\n#     with tf.variable_scope(""bn""):\n#         gamma = tf.get_variable(""gamma"", shape=inputs.get_shape()[-1], dtype=tf.float32, initializer=tf.constant_initializer(1.0))\n#         beta = tf.get_variable(""beta"", shape=inputs.get_shape()[-1], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n#         pop_mean = tf.get_variable(""pop_mean"", trainable=True, shape=inputs.get_shape()[-1], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n#         pop_var = tf.get_variable(""pop_var"", trainable=True, shape=inputs.get_shape()[-1], dtype=tf.float32, initializer=tf.constant_initializer(1.0))\n#\n#         # if phase_train == None:\n#         #     print 21\n#         #     return tf.nn.batch_normalization(inputs, pop_mean, pop_var, beta, gamma, eps)\n#\n#         rank = len(inputs.get_shape())\n#         axes = range(rank - 1)\n#         batch_mean, batch_var = tf.nn.moments(inputs, axes)\n#         ema = tf.train.ExponentialMovingAverage(decay=decay)\n#\n#         def update():\n#             ema_apply_op = ema.apply([batch_mean, batch_var])\n#             train_mean = pop_mean.assign(ema.average(batch_mean))\n#             train_var = pop_var.assign(ema.average(batch_var))\n#             with tf.control_dependencies([ema_apply_op]):\n#                 return tf.nn.batch_normalization(inputs, tf.identity(batch_mean), tf.identity(batch_var), \\\n#                     beta, gamma, eps)\n#\n#         def average():\n#             train_mean = pop_mean.assign(ema.average(batch_mean))\n#             train_var = pop_var.assign(ema.average(batch_var))\n#             with tf.control_dependencies([train_mean, train_var]):\n#                 return tf.nn.batch_normalization(inputs, train_mean, train_var, beta, gamma, eps)\n#\n#         return tf.cond(phase_train, update, average)\n\n# def batch_norm(inputs, phase_train, decay=0.9, eps=1e-5):\n#     """"""Batch Normalization\n#\n#        Args:\n#            inputs: input data(Batch size) from last layer\n#            phase_train: when you test, please set phase_train ""None""\n#        Returns:\n#            output for next layer\n#     """"""\n#     bn = tf.contrib.layers.batch_norm(inputs, center=True, scale=True, is_training=False, scope=""bn"")\n#     return bn\n    # with tf.variable_scope(""bn"", reuse=False):\n    #     bn = tf.contrib.layers.batch_norm(inputs, center=True, scale=True, is_training=False, reuse=False, scope=""bn"")\n    #     return bn\n\ndef conv3DLayer(input_layer, input_dim, output_dim, height, width, length, stride, activation=tf.nn.relu, padding=""SAME"", name="""", is_training=True):\n    with tf.variable_scope(""conv3D"" + name):\n        kernel = tf.get_variable(""weights"", shape=[length, height, width, input_dim, output_dim], \\\n            dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.01))\n        b = tf.get_variable(""bias"", shape=[output_dim], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n        conv = tf.nn.conv3d(input_layer, kernel, stride, padding=padding)\n        bias = tf.nn.bias_add(conv, b)\n        if activation:\n            bias = activation(bias, name=""activation"")\n        bias = batch_norm(bias, is_training)\n    return bias\n\ndef conv3D_to_output(input_layer, input_dim, output_dim, height, width, length, stride, activation=tf.nn.relu, padding=""SAME"", name=""""):\n    with tf.variable_scope(""conv3D"" + name):\n        kernel = tf.get_variable(""weights"", shape=[length, height, width, input_dim, output_dim], \\\n            dtype=tf.float32, initializer=tf.constant_initializer(0.01))\n        conv = tf.nn.conv3d(input_layer, kernel, stride, padding=padding)\n    return conv\n\ndef deconv3D_to_output(input_layer, input_dim, output_dim, height, width, length, stride, output_shape, activation=tf.nn.relu, padding=""SAME"", name=""""):\n    with tf.variable_scope(""deconv3D""+name):\n        kernel = tf.get_variable(""weights"", shape=[length, height, width, output_dim, input_dim], \\\n            dtype=tf.float32, initializer=tf.constant_initializer(0.01))\n        deconv = tf.nn.conv3d_transpose(input_layer, kernel, output_shape, stride, padding=""SAME"")\n    return deconv\n\ndef fully_connected(input_layer, shape, name="""", is_training=True):\n    with tf.variable_scope(""fully"" + name):\n        kernel = tf.get_variable(""weights"", shape=shape, \\\n            dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.01))\n        fully = tf.matmul(input_layer, kernel)\n        fully = tf.nn.relu(fully)\n        fully = batch_norm(fully, is_training)\n        return fully\n\nclass BNBLayer(object):\n    def __init__(self):\n        pass\n\n    def build_graph(self, voxel, activation=tf.nn.relu, is_training=None):\n        self.layer1 = conv3DLayer(voxel, 1, 10, 5, 5, 5, [1, 2, 2, 2, 1], name=""layer1"", activation=activation, is_training=is_training)\n        self.layer2 = conv3DLayer(self.layer1, 10, 15, 5, 5, 5, [1, 2, 2, 2, 1], name=""layer2"", activation=activation, is_training=is_training)\n        # self.layer3 = conv3DLayer(self.layer2, 15, 30, 3, 3, 3, [1, 1, 1, 1, 1], name=""layer3"", activation=activation, is_training=is_training)\n        # self.layer4 = conv3DLayer(self.layer3, 32, 32, 3, 3, 3, [1, 1, 1, 1, 1], name=""layer4"", activation=activation, is_training=is_training)\n        # self.layer4 = conv3DLayer(self.layer3, 32, 32, 3, 3, 3, [1, 2, 2, 2, 1], name=""layer4"", activation=activation, is_training=is_training)\n        # base_shape = self.layer3.get_shape().as_list()\n        # obj_output_shape = [tf.shape(self.layer4)[0], base_shape[1], base_shape[2], base_shape[3], 2]\n        # cord_output_shape = [tf.shape(self.layer4)[0], base_shape[1], base_shape[2], base_shape[3], 24]\n        self.objectness = conv3D_to_output(self.layer2, 15, 2, 3, 3, 3, [1, 1, 1, 1, 1], name=""objectness"", activation=None)\n        self.cordinate = conv3D_to_output(self.layer2, 15, 24, 3, 3, 3, [1, 1, 1, 1, 1], name=""cordinate"", activation=None)\n        # self.objectness = deconv3D_to_output(self.layer4, 32, 2, 3, 3, 3, [1, 2, 2, 2, 1], obj_output_shape, name=""objectness"", activation=None)\n        # self.cordinate = deconv3D_to_output(self.layer4, 32, 24, 3, 3, 3, [1, 2, 2, 2, 1], cord_output_shape, name=""cordinate"", activation=None)\n        self.y = tf.nn.softmax(self.objectness, dim=-1)\n\n    #original\n    # def build_graph(self, voxel, activation=tf.nn.relu, is_training=True):\n    #     self.layer1 = conv3DLayer(voxel, 1, 10, 5, 5, 5, [1, 2, 2, 2, 1], name=""layer1"", activation=activation, is_training=is_training)\n    #     self.layer2 = conv3DLayer(self.layer1, 10, 16, 5, 5, 5, [1, 2, 2, 2, 1], name=""layer2"", activation=activation, is_training=is_training)\n    #     self.layer3 = conv3DLayer(self.layer2, 16, 30, 3, 3, 3, [1, 2, 2, 2, 1], name=""layer3"", activation=activation, is_training=is_training)\n    #     base_shape = self.layer2.get_shape().as_list()\n    #     obj_output_shape = [tf.shape(self.layer3)[0], base_shape[1], base_shape[2], base_shape[3], 2]\n    #     cord_output_shape = [tf.shape(self.layer3)[0], base_shape[1], base_shape[2], base_shape[3], 24]\n    #     self.objectness = deconv3D_to_output(self.layer3, 30, 2, 3, 3, 3, [1, 2, 2, 2, 1], obj_output_shape, name=""objectness"", activation=None)\n    #     self.cordinate = deconv3D_to_output(self.layer3, 30, 24, 3, 3, 3, [1, 2, 2, 2, 1], cord_output_shape, name=""cordinate"", activation=None)\n    #     self.y = tf.nn.softmax(self.objectness, dim=-1)\n\ndef ssd_model(sess, voxel_shape=(300, 300, 300),activation=tf.nn.relu, is_training=True):\n    voxel = tf.placeholder(tf.float32, [None, voxel_shape[0], voxel_shape[1], voxel_shape[2], 1])\n    phase_train = tf.placeholder(tf.bool, name=\'phase_train\') if is_training else None\n    with tf.variable_scope(""3D_CNN_model"") as scope:\n        bnb_model = BNBLayer()\n        bnb_model.build_graph(voxel, activation=activation, is_training=phase_train)\n\n    if is_training:\n        initialized_var = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=""3D_CNN_model"")\n        sess.run(tf.variables_initializer(initialized_var))\n    return bnb_model, voxel, phase_train\n\ndef loss_func(model):\n    g_map = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list()[:4])\n    g_cord = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list())\n    object_loss = tf.multiply(g_map, model.objectness[:, :, :, :, 0])\n    non_gmap = tf.subtract(tf.ones_like(g_map, dtype=tf.float32), g_map)\n    nonobject_loss = tf.multiply(non_gmap, model.objectness[:, :, :, :, 1])\n    # sum_object_loss = tf.add(tf.exp(object_loss), tf.exp(nonobject_loss))\n    sum_object_loss = tf.exp(-tf.add(object_loss, nonobject_loss))\n    # sum_object_loss = tf.exp(-nonobject_loss)\n    bunbo = tf.add(tf.exp(-model.objectness[:, :, :, :, 0]), tf.exp(-model.objectness[:, :, :, :, 1]))\n    obj_loss = 0.005 * tf.reduce_sum(-tf.log(tf.div(sum_object_loss, bunbo)))\n\n    cord_diff = tf.multiply(g_map, tf.reduce_sum(tf.square(tf.subtract(model.cordinate, g_cord)), 4))\n    cord_loss = tf.reduce_sum(cord_diff)\n    return obj_loss, obj_loss, cord_loss, g_map, g_cord\n\ndef loss_func2(model):\n    g_map = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list()[:4])\n    obj_loss = tf.reduce_sum(tf.square(tf.subtract(model.objectness[:, :, :, :, 0], g_map)))\n\n    g_cord = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list())\n    cord_diff = tf.multiply(g_map, tf.reduce_sum(tf.square(tf.subtract(model.cordinate, g_cord)), 4))\n    cord_loss = tf.reduce_sum(cord_diff) * 0.1\n    return tf.add(obj_loss, cord_loss), g_map, g_cord\n\ndef loss_func3(model):\n    g_map = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list()[:4])\n    g_cord = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list())\n    non_gmap = tf.subtract(tf.ones_like(g_map, dtype=tf.float32), g_map)\n\n    elosion = 0.00001\n    y = model.y\n    is_obj_loss = -tf.reduce_sum(tf.multiply(g_map,  tf.log(y[:, :, :, :, 0] + elosion)))\n    non_obj_loss = tf.multiply(-tf.reduce_sum(tf.multiply(non_gmap, tf.log(y[:, :, :, :, 1] + elosion))), 0.0008)\n    cross_entropy = tf.add(is_obj_loss, non_obj_loss)\n    obj_loss = cross_entropy\n\n    g_cord = tf.placeholder(tf.float32, model.cordinate.get_shape().as_list())\n    cord_diff = tf.multiply(g_map, tf.reduce_sum(tf.square(tf.subtract(model.cordinate, g_cord)), 4))\n    cord_loss = tf.multiply(tf.reduce_sum(cord_diff), 0.02)\n    return tf.add(obj_loss, cord_loss), obj_loss, cord_loss, is_obj_loss, non_obj_loss, g_map, g_cord, y\n\ndef create_optimizer(all_loss, lr=0.001):\n    opt = tf.train.AdamOptimizer(lr)\n    optimizer = opt.minimize(all_loss)\n    return optimizer\n\ndef train(batch_num, velodyne_path, label_path=None, calib_path=None, resolution=0.2, dataformat=""pcd"", label_type=""txt"", is_velo_cam=False, \\\n        scale=4, voxel_shape=(800, 800, 40), x=(0, 80), y=(-40, 40), z=(-2.5, 1.5)):\n    # tf Graph input\n    batch_size = batch_num\n    training_epochs = 101\n\n    with tf.Session() as sess:\n        model, voxel, phase_train = ssd_model(sess, voxel_shape=voxel_shape, activation=tf.nn.relu, is_training=True)\n        saver = tf.train.Saver()\n        total_loss, obj_loss, cord_loss, is_obj_loss, non_obj_loss, g_map, g_cord, y_pred = loss_func3(model)\n        optimizer = create_optimizer(total_loss, lr=0.01)\n        init = tf.global_variables_initializer()\n        sess.run(init)\n\n        for epoch in range(training_epochs):\n            for (batch_x, batch_g_map, batch_g_cord) in lidar_generator(batch_num, velodyne_path, label_path=label_path, \\\n               calib_path=calib_path,resolution=resolution, dataformat=dataformat, label_type=label_type, is_velo_cam=is_velo_cam, \\\n               scale=scale, x=x, y=y, z=z):\n                # print batch_x.shape, batch_g_map.shape, batch_g_cord.shape, batch_num\n                # print batch_x.shape\n                # print batch_g_map.shape\n                # print batch_g_cord.shape\n                sess.run(optimizer, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord, phase_train:True})\n\n                # ct = sess.run(total_loss, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord, phase_train:True})\n                # co = sess.run(obj_loss, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord, phase_train:True})\n                cc = sess.run(cord_loss, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord, phase_train:True})\n                iol = sess.run(is_obj_loss, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord, phase_train:True})\n                nol = sess.run(non_obj_loss, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord, phase_train:True})\n                # soft = sess.run(y, feed_dict={voxel: batch_x, g_map: batch_g_map, g_cord: batch_g_cord})\n                # print soft[0, 0, 0, 0, :]\n                # print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(ct))\n                # print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(co))\n                print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(cc))\n                print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(iol))\n                print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(nol))\n            if (epoch != 0) and (epoch % 10 == 0):\n                print ""Save epoch "" + str(epoch)\n                saver.save(sess, ""velodyne_025_deconv_norm_valid"" + str(epoch) + "".ckpt"")\n        print(""Optimization Finished!"")\n\ndef test(batch_num, velodyne_path, label_path=None, calib_path=None, resolution=0.2, dataformat=""pcd"", label_type=""txt"", is_velo_cam=False, \\\n             scale=4, voxel_shape=(800, 800, 40), x=(0, 80), y=(-40, 40), z=(-2.5, 1.5)):\n    # tf Graph input\n    batch_size = batch_num # 1\n    training_epochs = 5\n    p = []\n    pc = None\n    bounding_boxes = None\n    places = None\n    rotates = None\n    size = None\n    proj_velo = None\n\n    if dataformat == ""bin"":\n        pc = load_pc_from_bin(velodyne_path)\n    elif dataformat == ""pcd"":\n        pc = load_pc_from_pcd(velodyne_path)\n\n    if calib_path:\n        calib = read_calib_file(calib_path)\n        proj_velo = proj_to_velo(calib)[:, :3]\n\n    if label_path:\n        places, rotates, size = read_labels(label_path, label_type, calib_path=calib_path, is_velo_cam=is_velo_cam, proj_velo=proj_velo)\n\n    corners = get_boxcorners(places, rotates, size)\n    filter_car_data(corners)\n    pc = filter_camera_angle(pc)\n\n    voxel =  raw_to_voxel(pc, resolution=resolution, x=x, y=y, z=z)\n    center_sphere = center_to_sphere(places, size, resolution=resolution)\n    corner_label = corner_to_train(corners, center_sphere, resolution=resolution)\n    g_map = create_objectness_label(center_sphere, resolution=resolution)\n    g_cord = corner_label.reshape(corner_label.shape[0], -1)\n\n    voxel_x = voxel.reshape(1, voxel.shape[0], voxel.shape[1], voxel.shape[2], 1)\n\n    with tf.Session() as sess:\n        is_training=None\n        model, voxel, phase_train = ssd_model(sess, voxel_shape=voxel_shape, activation=tf.nn.relu, is_training=is_training)\n        saver = tf.train.Saver()\n        new_saver = tf.train.import_meta_graph(""velodyne_025_deconv_norm_valid0.ckpt.meta"")\n        last_model = ""./velodyne_025_deconv_norm_valid0.ckpt""\n        saver.restore(sess, last_model)\n\n        objectness = model.objectness\n        cordinate = model.cordinate\n        y_pred = model.y\n        objectness = sess.run(objectness, feed_dict={voxel: voxel_x})[0, :, :, :, 0]\n        cordinate = sess.run(cordinate, feed_dict={voxel: voxel_x})[0]\n        y_pred = sess.run(y_pred, feed_dict={voxel: voxel_x})[0, :, :, :, 0]\n        print objectness.shape, objectness.max(), objectness.min()\n        print y_pred.shape, y_pred.max(), y_pred.min()\n\n        # print np.where(objectness >= 0.55)\n        index = np.where(y_pred >= 0.995)\n        print np.vstack((index[0], np.vstack((index[1], index[2])))).transpose()\n        print np.vstack((index[0], np.vstack((index[1], index[2])))).transpose().shape\n\n        a = center_to_sphere(places, size, resolution=resolution, x=x, y=y, z=z, \\\n            scale=scale, min_value=np.array([x[0], y[0], z[0]]))\n        label_center = sphere_to_center(a, resolution=resolution, \\\n            scale=scale, min_value=np.array([x[0], y[0], z[0]]))\n        label_corners = get_boxcorners(label_center, rotates, size)\n        print a[a[:, 0].argsort()]\n        # center = np.array([20, 57, 3])\n        #\n        # pred_center = sphere_to_center(center, resolution=resolution)\n        # print pred_center\n        # print cordinate.shape\n        # corners = cordinate[center[0], center[1], center[2]].reshape(-1, 3)\n        centers = np.vstack((index[0], np.vstack((index[1], index[2])))).transpose()\n        centers = sphere_to_center(centers, resolution=resolution, \\\n            scale=scale, min_value=np.array([x[0], y[0], z[0]]))\n        corners = cordinate[index].reshape(-1, 8, 3) + centers[:, np.newaxis]\n        print corners.shape\n        print voxel.shape\n        # publish_pc2(pc, corners.reshape(-1, 3))\n        publish_pc2(pc, label_corners.reshape(-1, 3))\n        # pred_corners = corners + pred_center\n        # print pred_corners\n\ndef lidar_generator(batch_num, velodyne_path, label_path=None, calib_path=None, resolution=0.2, dataformat=""pcd"", label_type=""txt"", is_velo_cam=False, \\\n                        scale=4, x=(0, 80), y=(-40, 40), z=(-2.5, 1.5)):\n    velodynes_path = glob.glob(velodyne_path)\n    labels_path = glob.glob(label_path)\n    calibs_path = glob.glob(calib_path)\n    velodynes_path.sort()\n    labels_path.sort()\n    calibs_path.sort()\n    iter_num = len(velodynes_path) // batch_num\n\n    for itn in range(iter_num):\n        batch_voxel = []\n        batch_g_map = []\n        batch_g_cord = []\n\n        for velodynes, labels, calibs in zip(velodynes_path[itn*batch_num:(itn+1)*batch_num], \\\n            labels_path[itn*batch_num:(itn+1)*batch_num], calibs_path[itn*batch_num:(itn+1)*batch_num]):\n            p = []\n            pc = None\n            bounding_boxes = None\n            places = None\n            rotates = None\n            size = None\n            proj_velo = None\n\n            if dataformat == ""bin"":\n                pc = load_pc_from_bin(velodynes)\n            elif dataformat == ""pcd"":\n                pc = load_pc_from_pcd(velodynes)\n\n            if calib_path:\n                calib = read_calib_file(calibs)\n                proj_velo = proj_to_velo(calib)[:, :3]\n\n            if label_path:\n                places, rotates, size = read_labels(labels, label_type, calib_path=calib_path, is_velo_cam=is_velo_cam, proj_velo=proj_velo)\n                if places is None:\n                    continue\n\n            corners = get_boxcorners(places, rotates, size)\n            filter_car_data(corners)\n            pc = filter_camera_angle(pc)\n\n            voxel =  raw_to_voxel(pc, resolution=resolution, x=x, y=y, z=z)\n            # center_sphere = center_to_sphere(places, size, resolution=resolution, min_value=np.array([0., -40, -2.5]), scale=scale, x=x, y=y, z=(-2.5, 2.3))\n            # corner_label = corner_to_train(corners, center_sphere, resolution=resolution, x=x, y=y, z=(-2.5, 2.3), scale=scale, min_value=np.array([0., -40, -2.5]))\n            center_sphere, corner_label = create_label(places, size, corners, resolution=resolution, x=x, y=y, z=z, \\\n                scale=scale, min_value=np.array([x[0], y[0], z[0]]))\n\n            # print center_sphere\n            if not center_sphere.shape[0]:\n                print 1\n                continue\n            g_map = create_objectness_label(center_sphere, resolution=resolution, x=(x[1] - x[0]), y=(y[1] - y[0]), z=(z[1] - z[0]), scale=scale)\n            g_cord = corner_label.reshape(corner_label.shape[0], -1)\n            g_cord = corner_to_voxel(voxel.shape, g_cord, center_sphere, scale=scale)\n\n            batch_voxel.append(voxel)\n            batch_g_map.append(g_map)\n            batch_g_cord.append(g_cord)\n        yield np.array(batch_voxel, dtype=np.float32)[:, :, :, :, np.newaxis], np.array(batch_g_map, dtype=np.float32), np.array(batch_g_cord, dtype=np.float32)\n\n\nif __name__ == \'__main__\':\n    pcd_path = ""../data/training/velodyne/*.bin""\n    label_path = ""../data/training/label_2/*.txt""\n    calib_path = ""../data/training/calib/*.txt""\n    train(5, pcd_path, label_path=label_path, resolution=0.25, calib_path=calib_path, dataformat=""bin"", is_velo_cam=True, \\\n            scale=4, voxel_shape=(360, 400, 40), x=(0, 90), y=(-50, 50), z=(-5.5, 4.5))\n    # #\n    # pcd_path = ""../data/training/velodyne/005000.bin""\n    # label_path = ""../data/training/label_2/005000.txt""\n    # calib_path = ""../data/training/calib/005000.txt""\n    # test(1, pcd_path, label_path=label_path, resolution=0.25, calib_path=calib_path, dataformat=""bin"", is_velo_cam=True, \\\n    #         scale=4, voxel_shape=(360, 400, 40), x=(0, 90), y=(-50, 50), z=(-5.5, 4.5))\n    # test(1, pcd_path, label_path=label_path, resolution=0.1, calib_path=calib_path, dataformat=""bin"", is_velo_cam=True, scale=8, voxel_shape=(800, 800, 40))\n'"
multiview_2d.py,0,"b'#!/usr/bin/env python\n\nimport sys\nimport os\nimport rospy\nimport numpy as np\nimport cv2\nimport pcl\nimport glob\nimport std_msgs.msg\nimport sensor_msgs.point_cloud2 as pc2\nfrom sensor_msgs.msg import PointCloud2\nfrom parse_xml import parseXML\nfrom input_velodyne import *\nimport matplotlib.pyplot as plt\n\ndef convert_xyz_to_2d(places):\n    theta = np.arctan2(places[:, 1], places[:, 0])\n    ave_theta = np.average(theta)\n    phi = np.arctan2(places[:, 2], np.sqrt(places[:, 0]**2 + places[:, 1]**2))\n    ave_phi = np.average(phi)\n    r = (theta / ave_theta).astype(np.int32)\n    c = (phi / ave_phi).astype(np.int32)\n    print ""places"", places.shape\n    print np.max(places, axis=0)\n    print np.min(places, axis=0)\n    print ""theta"", theta.shape\n    print theta.max(axis=0)\n    print theta.min(axis=0)\n    print ave_theta\n    print ""phi"", phi.shape\n    print phi.min()\n    print phi.max()\n    print ave_phi\n    print r.max(), r.min(), c.max(), c.min()\n    plt.hist(phi)\n    plt.show()\n\ndef bird_view(places):\n    pass\n\ndef process(velodyne_path, label_path=None, calib_path=None, dataformat=""pcd"", label_type=""txt"", is_velo_cam=False):\n    p = []\n    pc = None\n    bounding_boxes = None\n    places = None\n    rotates = None\n    size = None\n    proj_velo = None\n\n    if dataformat == ""bin"":\n        pc = load_pc_from_bin(velodyne_path)\n    elif dataformat == ""pcd"":\n        pc = load_pc_from_pcd(velodyne_path)\n\n    if calib_path:\n        calib = read_calib_file(calib_path)\n        proj_velo = proj_to_velo(calib)[:, :3]\n\n    if label_path:\n        places, rotates, size = read_labels(label_path, label_type, calib_path=calib_path, is_velo_cam=is_velo_cam, proj_velo=proj_velo)\n\n    convert_xyz_to_2d(pc)\n\n    # corners = get_boxcorners(places, rotates, size)\n    # filter_car_data(corners)\n    #\n    # pc = filter_camera_angle(pc)\n    # # obj = []\n    # # obj = create_publish_obj(obj, places, rotates, size)\n    #\n    # p.append((0, 0, 0))\n    # print 1\n    # # publish_pc2(pc, obj)\n    # publish_pc2(pc, corners.reshape(-1, 3))\n\nif __name__ == ""__main__"":\n    # pcd_path = ""../data/training/velodyne/000012.pcd""\n    # label_path = ""../data/training/label_2/000012.txt""\n    # calib_path = ""../data/training/calib/000012.txt""\n    # process(pcd_path, label_path, calib_path=calib_path, dataformat=""pcd"")\n\n    # bin_path = ""../data/2011_09_26/2011_09_26_drive_0001_sync/velodyne_points/data/0000000030.bin""\n    # xml_path = ""../data/2011_09_26/2011_09_26_drive_0001_sync/tracklet_labels.xml""\n    # process(bin_path, xml_path, dataformat=""bin"", label_type=""xml"")\n\n\n    pcd_path = ""/home/katou01/download/training/velodyne/005080.bin""\n    label_path = ""/home/katou01/download/training/label_2/005080.txt""\n    calib_path = ""/home/katou01/download/training/calib/005080.txt""\n    process(pcd_path, label_path, calib_path=calib_path, dataformat=""bin"", is_velo_cam=True)\n'"
parse_xml.py,0,"b'#!/usr/bin/env python\n""""""\nparse XML files containing tracklet info for kitti data base (raw data section)\n(http://cvlibs.net/datasets/kitti/raw_data.php)\n\nNo guarantees that this code is correct, usage is at your own risk!\n\ncreated by Christian Herdtweck, Max Planck Institute for Biological Cybernetics\n  (christian.herdtweck@tuebingen.mpg.de)\n\nrequires numpy!\n\nexample usage:\n  import parseTrackletXML as xmlParser\n  kittiDir = \'/path/to/kitti/data\'\n  drive = \'2011_09_26_drive_0001\'\n  xmlParser.example(kittiDir, drive)\nor simply on command line:\n  python parseTrackletXML.py\n""""""\n\n# Version History:\n# 4/7/12 Christian Herdtweck: seems to work with a few random test xml tracklet files;\n#   converts file contents to ElementTree and then to list of Tracklet objects;\n#   Tracklet objects have str and iter functions\n# 5/7/12 ch: added constants for state, occlusion, truncation and added consistency checks\n# 30/1/14 ch: create example function from example code\n\nfrom sys import argv as cmdLineArgs\nfrom xml.etree.ElementTree import ElementTree\nimport numpy as np\nimport itertools\nfrom warnings import warn\n\nSTATE_UNSET = 0\nSTATE_INTERP = 1\nSTATE_LABELED = 2\nstateFromText = {\'0\':STATE_UNSET, \'1\':STATE_INTERP, \'2\':STATE_LABELED}\n\nOCC_UNSET = 255  # -1 as uint8\nOCC_VISIBLE = 0\nOCC_PARTLY = 1\nOCC_FULLY = 2\noccFromText = {\'-1\':OCC_UNSET, \'0\':OCC_VISIBLE, \'1\':OCC_PARTLY, \'2\':OCC_FULLY}\n\nTRUNC_UNSET = 255  # -1 as uint8, but in xml files the value \'99\' is used!\nTRUNC_IN_IMAGE = 0\nTRUNC_TRUNCATED = 1\nTRUNC_OUT_IMAGE = 2\nTRUNC_BEHIND_IMAGE = 3\ntruncFromText = {\'99\':TRUNC_UNSET, \'0\':TRUNC_IN_IMAGE, \'1\':TRUNC_TRUNCATED, \\\n                  \'2\':TRUNC_OUT_IMAGE, \'3\': TRUNC_BEHIND_IMAGE}\n\n\nclass Tracklet(object):\n    """""" representation an annotated object track\n\n    Tracklets are created in function parseXML and can most conveniently used as follows:\n\n    for trackletObj in parseXML(trackletFile):\n    for translation, rotation, state, occlusion, truncation, amtOcclusion, amtBorders, absoluteFrameNumber in trackletObj:\n      your code here\n    #end: for all frames\n    #end: for all tracklets\n\n    absoluteFrameNumber is in range [firstFrame, firstFrame+nFrames[\n    amtOcclusion and amtBorders could be None\n\n    You can of course also directly access the fields objType (string), size (len-3 ndarray), firstFrame/nFrames (int),\n    trans/rots (nFrames x 3 float ndarrays), states/truncs (len-nFrames uint8 ndarrays), occs (nFrames x 2 uint8 ndarray),\n    and for some tracklets amtOccs (nFrames x 2 float ndarray) and amtBorders (nFrames x 3 float ndarray). The last two\n    can be None if the xml file did not include these fields in poses\n    """"""\n\n    objectType = None\n    size = None  # len-3 float array: (height, width, length)\n    firstFrame = None\n    trans = None   # n x 3 float array (x,y,z)\n    rots = None    # n x 3 float array (x,y,z)\n    states = None  # len-n uint8 array of states\n    occs = None    # n x 2 uint8 array  (occlusion, occlusion_kf)\n    truncs = None  # len-n uint8 array of truncation\n    amtOccs = None    # None or (n x 2) float array  (amt_occlusion, amt_occlusion_kf)\n    amtBorders = None    # None (n x 3) float array  (amt_border_l / _r / _kf)\n    nFrames = None\n\n    def __init__(self):\n        """"""create Tracklet with no info set """"""\n        self.size = np.nan*np.ones(3, dtype=float)\n\n    def __str__(self):\n        """""" return human-readable string representation of tracklet object\n\n        called implicitly in\n        print trackletObj\n        or in\n        text = str(trackletObj)\n        """"""\n        return \'[Tracklet over {0} frames for {1}]\'.format(self.nFrames, self.objectType)\n\n    def __iter__(self):\n        """""" returns an iterator that yields tuple of all the available data for each frame\n\n        called whenever code iterates over a tracklet object, e.g. in\n        for translation, rotation, state, occlusion, truncation, amtOcclusion, amtBorders, absoluteFrameNumber in trackletObj:\n          ...do something ...\n        or\n        trackDataIter = iter(trackletObj)\n        """"""\n        if self.amtOccs is None:\n          return itertools.izip(self.trans, self.rots, self.states, self.occs, self.truncs, \\\n              itertools.repeat(None), itertools.repeat(None), xrange(self.firstFrame, self.firstFrame+self.nFrames))\n        else:\n          return itertools.izip(self.trans, self.rots, self.states, self.occs, self.truncs, \\\n              self.amtOccs, self.amtBorders, xrange(self.firstFrame, self.firstFrame+self.nFrames))\n#end: class Tracklet\n\n\ndef parseXML(trackletFile):\n  r"""""" parse tracklet xml file and convert results to list of Tracklet objects\n\n  :param trackletFile: name of a tracklet xml file\n  :returns: list of Tracklet objects read from xml file\n  """"""\n\n  # convert tracklet XML data to a tree structure\n  eTree = ElementTree()\n  print \'parsing tracklet file\', trackletFile\n  with open(trackletFile) as f:\n    eTree.parse(f)\n\n  # now convert output to list of Tracklet objects\n  trackletsElem = eTree.find(\'tracklets\')\n  tracklets = []\n  trackletIdx = 0\n  nTracklets = None\n  for trackletElem in trackletsElem:\n    #print \'track:\', trackletElem.tag\n    if trackletElem.tag == \'count\':\n      nTracklets = int(trackletElem.text)\n      print \'file contains\', nTracklets, \'tracklets\'\n    elif trackletElem.tag == \'item_version\':\n      pass\n    elif trackletElem.tag == \'item\':\n      #print \'tracklet {0} of {1}\'.format(trackletIdx, nTracklets)\n      # a tracklet\n      newTrack = Tracklet()\n      isFinished = False\n      hasAmt = False\n      frameIdx = None\n      for info in trackletElem:\n        #print \'trackInfo:\', info.tag\n        if isFinished:\n          raise ValueError(\'more info on element after finished!\')\n        if info.tag == \'objectType\':\n          newTrack.objectType = info.text\n        elif info.tag == \'h\':\n          newTrack.size[0] = float(info.text)\n        elif info.tag == \'w\':\n          newTrack.size[1] = float(info.text)\n        elif info.tag == \'l\':\n          newTrack.size[2] = float(info.text)\n        elif info.tag == \'first_frame\':\n          newTrack.firstFrame = int(info.text)\n        elif info.tag == \'poses\':\n          # this info is the possibly long list of poses\n          for pose in info:\n            #print \'trackInfoPose:\', pose.tag\n            if pose.tag == \'count\':   # this should come before the others\n              if newTrack.nFrames is not None:\n                raise ValueError(\'there are several pose lists for a single track!\')\n              elif frameIdx is not None:\n                raise ValueError(\'?!\')\n              newTrack.nFrames = int(pose.text)\n              newTrack.trans  = np.nan * np.ones((newTrack.nFrames, 3), dtype=float)\n              newTrack.rots   = np.nan * np.ones((newTrack.nFrames, 3), dtype=float)\n              newTrack.states = np.nan * np.ones(newTrack.nFrames, dtype=\'uint8\')\n              newTrack.occs   = np.nan * np.ones((newTrack.nFrames, 2), dtype=\'uint8\')\n              newTrack.truncs = np.nan * np.ones(newTrack.nFrames, dtype=\'uint8\')\n              newTrack.amtOccs = np.nan * np.ones((newTrack.nFrames, 2), dtype=float)\n              newTrack.amtBorders = np.nan * np.ones((newTrack.nFrames, 3), dtype=float)\n              frameIdx = 0\n            elif pose.tag == \'item_version\':\n              pass\n            elif pose.tag == \'item\':\n              # pose in one frame\n              if frameIdx is None:\n                raise ValueError(\'pose item came before number of poses!\')\n              for poseInfo in pose:\n                #print \'trackInfoPoseInfo:\', poseInfo.tag\n                if poseInfo.tag == \'tx\':\n                  newTrack.trans[frameIdx, 0] = float(poseInfo.text)\n                elif poseInfo.tag == \'ty\':\n                  newTrack.trans[frameIdx, 1] = float(poseInfo.text)\n                elif poseInfo.tag == \'tz\':\n                  newTrack.trans[frameIdx, 2] = float(poseInfo.text)\n                elif poseInfo.tag == \'rx\':\n                  newTrack.rots[frameIdx, 0] = float(poseInfo.text)\n                elif poseInfo.tag == \'ry\':\n                  newTrack.rots[frameIdx, 1] = float(poseInfo.text)\n                elif poseInfo.tag == \'rz\':\n                  newTrack.rots[frameIdx, 2] = float(poseInfo.text)\n                elif poseInfo.tag == \'state\':\n                  newTrack.states[frameIdx] = stateFromText[poseInfo.text]\n                elif poseInfo.tag == \'occlusion\':\n                  newTrack.occs[frameIdx, 0] = occFromText[poseInfo.text]\n                elif poseInfo.tag == \'occlusion_kf\':\n                  newTrack.occs[frameIdx, 1] = occFromText[poseInfo.text]\n                elif poseInfo.tag == \'truncation\':\n                  newTrack.truncs[frameIdx] = truncFromText[poseInfo.text]\n                elif poseInfo.tag == \'amt_occlusion\':\n                  newTrack.amtOccs[frameIdx,0] = float(poseInfo.text)\n                  hasAmt = True\n                elif poseInfo.tag == \'amt_occlusion_kf\':\n                  newTrack.amtOccs[frameIdx,1] = float(poseInfo.text)\n                  hasAmt = True\n                elif poseInfo.tag == \'amt_border_l\':\n                  newTrack.amtBorders[frameIdx,0] = float(poseInfo.text)\n                  hasAmt = True\n                elif poseInfo.tag == \'amt_border_r\':\n                  newTrack.amtBorders[frameIdx,1] = float(poseInfo.text)\n                  hasAmt = True\n                elif poseInfo.tag == \'amt_border_kf\':\n                  newTrack.amtBorders[frameIdx,2] = float(poseInfo.text)\n                  hasAmt = True\n                else:\n                  raise ValueError(\'unexpected tag in poses item: {0}!\'.format(poseInfo.tag))\n              frameIdx += 1\n            else:\n              raise ValueError(\'unexpected pose info: {0}!\'.format(pose.tag))\n        elif info.tag == \'finished\':\n          isFinished = True\n        else:\n          raise ValueError(\'unexpected tag in tracklets: {0}!\'.format(info.tag))\n      #end: for all fields in current tracklet\n\n      # some final consistency checks on new tracklet\n      if not isFinished:\n        warn(\'tracklet {0} was not finished!\'.format(trackletIdx))\n      if newTrack.nFrames is None:\n        warn(\'tracklet {0} contains no information!\'.format(trackletIdx))\n      elif frameIdx != newTrack.nFrames:\n        warn(\'tracklet {0} is supposed to have {1} frames, but perser found {1}!\'.format(\\\n            trackletIdx, newTrack.nFrames, frameIdx))\n      if np.abs(newTrack.rots[:,:2]).sum() > 1e-16:\n        warn(\'track contains rotation other than yaw!\')\n\n      # if amtOccs / amtBorders are not set, set them to None\n      if not hasAmt:\n        newTrack.amtOccs = None\n        newTrack.amtBorders = None\n\n      # add new tracklet to list\n      tracklets.append(newTrack)\n      trackletIdx += 1\n\n    else:\n      raise ValueError(\'unexpected tracklet info\')\n  #end: for tracklet list items\n\n  print \'loaded\', trackletIdx, \'tracklets\'\n\n  # final consistency check\n  if trackletIdx != nTracklets:\n    warn(\'according to xml information the file has {0} tracklets, but parser found {1}!\'.format(nTracklets, trackletIdx))\n\n  return tracklets\n#end: function parseXML\n\n\ndef example(kittiDir=None, drive=None):\n\n  from os.path import join, expanduser\n  import readline    # makes raw_input behave more fancy\n  # from xmlParser import parseXML, TRUNC_IN_IMAGE, TRUNC_TRUNCATED\n\n  DEFAULT_DRIVE = \'2011_09_26_drive_0001\'\n  twoPi = 2.*np.pi\n\n  # get dir names\n  if kittiDir is None:\n    kittiDir = expanduser(raw_input(\'please enter kitti base dir (e.g. ~/path/to/kitti): \').strip())\n  if drive is None:\n    drive    = raw_input(\'please enter drive name (default {0}): \'.format(DEFAULT_DRIVE)).strip()\n    if len(drive) == 0:\n      drive = DEFAULT_DRIVE\n\n  # read tracklets from file\n  myTrackletFile = join(kittiDir, drive, \'tracklet_labels.xml\')\n  tracklets = parseXML(myTrackletFile)\n\n  # loop over tracklets\n  for iTracklet, tracklet in enumerate(tracklets):\n    print \'tracklet {0: 3d}: {1}\'.format(iTracklet, tracklet)\n\n    # this part is inspired by kitti object development kit matlab code: computeBox3D\n    h,w,l = tracklet.size\n    trackletBox = np.array([ # in velodyne coordinates around zero point and without orientation yet\\\n        [-l/2, -l/2,  l/2, l/2, -l/2, -l/2,  l/2, l/2], \\\n        [ w/2, -w/2, -w/2, w/2,  w/2, -w/2, -w/2, w/2], \\\n        [ 0.0,  0.0,  0.0, 0.0,    h,     h,   h,   h]])\n\n    # loop over all data in tracklet\n    for translation, rotation, state, occlusion, truncation, amtOcclusion, amtBorders, absoluteFrameNumber \\\n        in tracklet:\n\n      # determine if object is in the image; otherwise continue\n      if truncation not in (TRUNC_IN_IMAGE, TRUNC_TRUNCATED):\n        continue\n\n      # re-create 3D bounding box in velodyne coordinate system\n      yaw = rotation[2]   # other rotations are 0 in all xml files I checked\n      assert np.abs(rotation[:2]).sum() == 0, \'object rotations other than yaw given!\'\n      rotMat = np.array([\\\n          [np.cos(yaw), -np.sin(yaw), 0.0], \\\n          [np.sin(yaw),  np.cos(yaw), 0.0], \\\n          [        0.0,          0.0, 1.0]])\n      cornerPosInVelo = np.dot(rotMat, trackletBox) + np.tile(translation, (8,1)).T\n\n      # calc yaw as seen from the camera (i.e. 0 degree = facing away from cam), as opposed to\n      #   car-centered yaw (i.e. 0 degree = same orientation as car).\n      #   makes quite a difference for objects in periphery!\n      # Result is in [0, 2pi]\n      x, y, z = translation\n      yawVisual = ( yaw - np.arctan2(y, x) ) % twoPi\n\n    #end: for all frames in track\n  #end: for all tracks\n#end: function example\n\n# when somebody runs this file as a script:\n#   run example if no arg or only \'example\' was given as arg\n#   otherwise run parseXML\nif __name__ == ""__main__"":\n  # cmdLineArgs[0] is \'parseTrackletXML.py\'\n  if len(cmdLineArgs) < 2:\n    example()\n  elif (len(cmdLineArgs) == 2) and (cmdLineArgs[1] == \'example\'):\n    example()\n  else:\n    parseXML(*cmdLineArgs[1:])\n'"
