file_path,api_count,code
lib/lfw.py,0,"b'""""""Helper for evaluation on the Labeled Faces in the Wild dataset \n""""""\n\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport utils\n\ndef evaluate(embeddings, actual_issame, nrof_folds=10):\n    # Calculate evaluation metrics\n    thresholds = np.arange(0, 4, 0.01/4)\n    embeddings1 = embeddings[0::2]\n    embeddings2 = embeddings[1::2]\n    tpr, fpr, accuracy = utils.calculate_roc(thresholds, embeddings1, embeddings2,\n        np.asarray(actual_issame), nrof_folds=nrof_folds)\n    thresholds = np.arange(0, 4, 0.001)\n    val, val_std, far = utils.calculate_val(thresholds, embeddings1, embeddings2,\n        np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds)\n    return tpr, fpr, accuracy, val, val_std, far\n\ndef get_paths(lfw_dir, pairs, file_ext):\n    nrof_skipped_pairs = 0\n    path_list = []\n    issame_list = []\n    for pair in pairs:\n        if len(pair) == 3:\n            path0 = os.path.join(lfw_dir, pair[0], pair[0] + \'_\' + \'%04d\' % int(pair[1])+\'.\'+file_ext)\n            path1 = os.path.join(lfw_dir, pair[0], pair[0] + \'_\' + \'%04d\' % int(pair[2])+\'.\'+file_ext)\n            issame = True\n        elif len(pair) == 4:\n            path0 = os.path.join(lfw_dir, pair[0], pair[0] + \'_\' + \'%04d\' % int(pair[1])+\'.\'+file_ext)\n            path1 = os.path.join(lfw_dir, pair[2], pair[2] + \'_\' + \'%04d\' % int(pair[3])+\'.\'+file_ext)\n            issame = False\n        if os.path.exists(path0) and os.path.exists(path1):    # Only add the pair if both paths exist\n            path_list += (path0,path1)\n            issame_list.append(issame)\n        else:\n            nrof_skipped_pairs += 1\n    if nrof_skipped_pairs>0:\n        print(\'Skipped %d image pairs\' % nrof_skipped_pairs)\n    \n    return path_list, issame_list\n\ndef read_pairs(pairs_filename):\n    pairs = []\n    with open(pairs_filename, \'r\') as f:\n        for line in f.readlines()[1:]:\n            pair = line.strip().split()\n            pairs.append(pair)\n    return np.array(pairs)\n\n\n\n'"
lib/utils.py,69,"b'""""""Functions for building the face recognition network.\n""""""\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# pylint: disable=missing-docstring\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport pdb\nfrom subprocess import Popen, PIPE\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nimport numpy as np\nimport time\nimport tensorflow.contrib.slim as slim\nimport pickle\nfrom scipy import misc\nfrom sklearn.model_selection import KFold\nfrom scipy import interpolate\nfrom tensorflow.python.training import training\nimport random\nimport re\nfrom tensorflow.python.platform import gfile\n\ndef py_func(func, inp, Tout, stateful = True, name=None, grad_func=None):\n    rand_name = \'PyFuncGrad\' + str(np.random.randint(0,1E+8))\n    tf.RegisterGradient(rand_name)(grad_func)\n    g = tf.get_default_graph()\n    with g.gradient_override_map({\'PyFunc\':rand_name}):\n        return tf.py_func(func,inp,Tout,stateful=stateful, name=name)\n\n\n\n\ndef coco_forward(xw, y, m, name=None):\n    #pdb.set_trace()\n    xw_copy = xw.copy()\n    num = len(y)\n    orig_ind = range(num)\n    xw_copy[orig_ind,y] -= m\n    return xw_copy\n\ndef coco_help(grad,y):\n    grad_copy = grad.copy()\n    return grad_copy\n\ndef coco_backward(op, grad):\n    \n    y = op.inputs[1]\n    m = op.inputs[2]\n    grad_copy = tf.py_func(coco_help,[grad,y],tf.float32)\n    return grad_copy,y,m\n\ndef coco_func(xw,y,m, name=None):\n    with tf.op_scope([xw,y,m],name,""Coco_func"") as name:\n        coco_out = py_func(coco_forward,[xw,y,m],tf.float32,name=name,grad_func=coco_backward)\n        return coco_out\n\ndef cos_loss(x, y,  num_cls, reuse=False, alpha=0.25, scale=64,name = \'cos_loss\'):\n    \'\'\'\n    x: B x D - features\n    y: B x 1 - labels\n    num_cls: 1 - total class number\n    alpah: 1 - margin\n    scale: 1 - scaling paramter\n    \'\'\'\n    # define the classifier weights\n    xs = x.get_shape()\n    with tf.variable_scope(\'centers_var\',reuse=reuse) as center_scope:\n        w = tf.get_variable(""centers"", [xs[1], num_cls], dtype=tf.float32, \n            initializer=tf.contrib.layers.xavier_initializer(),trainable=True)\n   \n    #normalize the feature and weight\n    #(N,D)\n    x_feat_norm = tf.nn.l2_normalize(x,1,1e-10)\n    #(D,C)\n    w_feat_norm = tf.nn.l2_normalize(w,0,1e-10)\n    \n    # get the scores after normalization \n    #(N,C)\n    xw_norm = tf.matmul(x_feat_norm, w_feat_norm)  \n    #implemented by py_func\n    #value = tf.identity(xw)\n    #substract the marigin and scale it\n    value = coco_func(xw_norm,y,alpha) * scale\n\n    #implemented by tf api\n    #margin_xw_norm = xw_norm - alpha\n    #label_onehot = tf.one_hot(y,num_cls)\n    #value = scale*tf.where(tf.equal(label_onehot,1), margin_xw_norm, xw_norm)\n\n    \n    # compute the loss as softmax loss\n    cos_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=value))\n\n    return cos_loss \n\n\ndef softmax_loss(prelogits,labels,nrof_classes,weight_decay,reuse):\n    logits = slim.fully_connected(prelogits, nrof_classes, activation_fn=None, \n        weights_initializer=tf.truncated_normal_initializer(stddev=0.1), \n        weights_regularizer=slim.l2_regularizer(weight_decay),\n        scope=\'softmax\', reuse=reuse)\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        labels=labels, logits=logits, name=\'cross_entropy_per_example\')\n    cross_entropy_mean = tf.reduce_mean(cross_entropy, name=\'cross_entropy\')\n    return cross_entropy_mean\n\n\n\n\ndef contrastive_index(dists, labels, alfa):\n    #pdb.set_trace()\n    base_inds = np.array(range(dists.shape[0]))\n    nrof_classes = dists.shape[1]\n    same_indexs = base_inds*nrof_classes + labels\n    dists = dists.reshape([-1])\n    #dist_same = dists[same_indexs].copy()\n    dists_copy = dists.copy()\n    dists_copy[same_indexs] = np.NaN\n    diff_indexs = np.where(dists_copy < alfa)[0]\n    #pdb.set_trace()\n    #dists[same_indexs] = \n    print(\'same indexs: {}, diff indexs: {}\'.format(len(same_indexs),len(diff_indexs)))\n    return np.array(same_indexs,dtype=np.int64), np.array(diff_indexs, dtype=np.int64)\n\n\ndef get_image_paths_and_labels(dataset):\n    image_paths_flat = []\n    labels_flat = []\n    for i in range(len(dataset)):\n        image_paths_flat += dataset[i].image_paths\n        labels_flat += [i] * len(dataset[i].image_paths)\n    return image_paths_flat, labels_flat\n\ndef shuffle_examples(image_paths, labels):\n    shuffle_list = list(zip(image_paths, labels))\n    random.shuffle(shuffle_list)\n    image_paths_shuff, labels_shuff = zip(*shuffle_list)\n    return image_paths_shuff, labels_shuff\n\ndef read_images_from_disk(input_queue):\n    """"""Consumes a single filename and label as a \' \'-delimited string.\n    Args:\n      filename_and_label_tensor: A scalar string tensor.\n    Returns:\n      Two tensors: the decoded image, and the string label.\n    """"""\n    label = input_queue[1]\n    file_contents = tf.read_file(input_queue[0])\n    example = tf.image.decode_image(file_contents, channels=3)\n    return example, label\n  \ndef random_rotate_image(image):\n    angle = np.random.uniform(low=-10.0, high=10.0)\n    return misc.imrotate(image, angle, \'bicubic\')\n  \ndef read_and_augment_data(image_list, label_list, image_size, batch_size, max_nrof_epochs, \n        random_crop, random_flip, random_rotate, nrof_preprocess_threads, shuffle=True):\n    \n    images = ops.convert_to_tensor(image_list, dtype=tf.string)\n    labels = ops.convert_to_tensor(label_list, dtype=tf.int32)\n    \n    # Makes an input queue\n    input_queue = tf.train.slice_input_producer([images, labels],\n        num_epochs=max_nrof_epochs, shuffle=shuffle)\n\n    images_and_labels = []\n    for _ in range(nrof_preprocess_threads):\n        image, label = read_images_from_disk(input_queue)\n        if random_rotate:\n            image = tf.py_func(random_rotate_image, [image], tf.uint8)\n        if random_crop:\n            image = tf.random_crop(image, [image_size, image_size, 3])\n        else:\n            image = tf.image.resize_image_with_crop_or_pad(image, image_size, image_size)\n        if random_flip:\n            image = tf.image.random_flip_left_right(image)\n        #pylint: disable=no-member\n        image.set_shape((image_size, image_size, 3))\n        image = tf.image.per_image_standardization(image)\n        images_and_labels.append([image, label])\n\n    image_batch, label_batch = tf.train.batch_join(\n        images_and_labels, batch_size=batch_size,\n        capacity=4 * nrof_preprocess_threads * batch_size,\n        allow_smaller_final_batch=True)\n  \n    return image_batch, label_batch\n  \ndef _add_loss_summaries(total_loss):\n    """"""Add summaries for losses.\n  \n    Generates moving average for all losses and associated summaries for\n    visualizing the performance of the network.\n  \n    Args:\n      total_loss: Total loss from loss().\n    Returns:\n      loss_averages_op: op for generating moving averages of losses.\n    """"""\n    # Compute the moving average of all individual losses and the total loss.\n    loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\'avg\')\n    losses = tf.get_collection(\'losses\')\n    loss_averages_op = loss_averages.apply(losses + [total_loss])\n  \n    # Attach a scalar summmary to all individual losses and the total loss; do the\n    # same for the averaged version of the losses.\n    for l in losses + [total_loss]:\n        # Name each loss as \'(raw)\' and name the moving average version of the loss\n        # as the original loss name.\n        tf.summary.scalar(l.op.name +\' (raw)\', l)\n        tf.summary.scalar(l.op.name, loss_averages.average(l))\n  \n    return loss_averages_op\n\ndef train(total_loss, global_step, optimizer, learning_rate, moving_average_decay, update_gradient_vars, log_histograms=True):\n    # Generate moving averages of all losses and associated summaries.\n    loss_averages_op = _add_loss_summaries(total_loss)\n\n    # Compute gradients.\n    with tf.control_dependencies([loss_averages_op]):\n        if optimizer==\'ADAGRAD\':\n            opt = tf.train.AdagradOptimizer(learning_rate)\n        elif optimizer==\'ADADELTA\':\n            opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n        elif optimizer==\'ADAM\':\n            opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n        elif optimizer==\'RMSPROP\':\n            opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n        elif optimizer==\'MOM\':\n            opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n        else:\n            raise ValueError(\'Invalid optimization algorithm\')\n    \n        grads = opt.compute_gradients(total_loss, update_gradient_vars)\n        \n    # Apply gradients.\n    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n  \n    # Add histograms for trainable variables.\n    if log_histograms:\n        for var in tf.trainable_variables():\n            tf.summary.histogram(var.op.name, var)\n   \n    # Add histograms for gradients.\n    if log_histograms:\n        for grad, var in grads:\n            if grad is not None:\n                tf.summary.histogram(var.op.name + \'/gradients\', grad)\n  \n    # Track the moving averages of all trainable variables.\n    variable_averages = tf.train.ExponentialMovingAverage(\n        moving_average_decay, global_step)\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n  \n    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n        train_op = tf.no_op(name=\'train\')\n  \n    return train_op\ndef compute_gradient(total_loss, global_step, optimizer, learning_rate, update_gradient_vars):\n\n    # Compute gradients.\n    with tf.control_dependencies([loss_averages_op]):\n        if optimizer==\'ADAGRAD\':\n            opt = tf.train.AdagradOptimizer(learning_rate)\n        elif optimizer==\'ADADELTA\':\n            opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n        elif optimizer==\'ADAM\':\n            opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n        elif optimizer==\'RMSPROP\':\n            opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n        elif optimizer==\'MOM\':\n            opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n        else:\n            raise ValueError(\'Invalid optimization algorithm\')\n    \n        grads = opt.compute_gradients(total_loss, update_gradient_vars)\n        \n    return opt,grads\ndef get_opt(optimizer,learning_rate):\n\n    # Compute gradients.\n    if optimizer==\'ADAGRAD\':\n        opt = tf.train.AdagradOptimizer(learning_rate)\n    elif optimizer==\'ADADELTA\':\n        opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n    elif optimizer==\'ADAM\':\n        opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n    elif optimizer==\'RMSPROP\':\n        opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n    elif optimizer==\'MOM\':\n        opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n    elif optimizer==\'SGD\':\n        opt = tf.train.GradientDescentOptimizer(learning_rate)\n    else:\n        raise ValueError(\'Invalid optimization algorithm\')\n\n        \n    return opt\n\n\n\ndef average_gradients(tower_grads):\n    \'\'\'\n    Calculate the average gradient for each shared variable across all towers\n    \'\'\'\n    average_grads = []\n    #pdb.set_trace()\n    for grad_and_vars in zip(*tower_grads):\n        #tower_grads[i] is the gradient computed in tower i, which have form ((g_var0,var0),...)\n        #so zip(*tower_grads) has form (((grad0_gpu0,var0_gpu0),...),...)\n        #this is to say: grad_and_vars is about the gradient and vars in variable i in all towers, which is gathered in a list.\n\n        #pdb.set_trace()\n        grads = []\n        for g,_ in grad_and_vars:\n            #print(g)\n            expanded_g = tf.expand_dims(g,0)\n            grads.append(expanded_g)\n        grad = tf.concat(grads,0)\n        grad = tf.reduce_mean(grad,0)\n\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad,v)\n        average_grads.append(grad_and_var)\n    return average_grads\ndef sum_gradients(tower_grads):\n    \'\'\'\n    Calculate the average gradient for each shared variable across all towers\n    \'\'\'\n    average_grads = []\n    #pdb.set_trace()\n    for grad_and_vars in zip(*tower_grads):\n        #tower_grads[i] is the gradient computed in tower i, which have form ((g_var0,var0),...)\n        #so zip(*tower_grads) has form (((grad0_gpu0,var0_gpu0),...),...)\n        #this is to say: grad_and_vars is about the gradient and vars in variable i in all towers, which is gathered in a list.\n\n        #pdb.set_trace()\n        grads = []\n        for g,v in grad_and_vars:\n            #print(g)\n            if \'Batch\' in v.name and \'mean\' in v.name:\n                print(grad_and_vars)\n            \n            expanded_g = tf.expand_dims(g,0)\n            grads.append(expanded_g)\n        grad = tf.concat(grads,0)\n        grad = tf.reduce_sum(grad,0)\n\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad,v)\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\ndef prewhiten(x):\n    mean = np.mean(x)\n    std = np.std(x)\n    std_adj = np.maximum(std, 1.0/np.sqrt(x.size))\n    y = np.multiply(np.subtract(x, mean), 1/std_adj)\n    return y  \n\ndef crop(image, random_crop, image_size):\n    if image.shape[1]>image_size:\n        sz1 = int(image.shape[1]//2)\n        sz2 = int(image_size//2)\n        if random_crop:\n            diff = sz1-sz2\n            (h, v) = (np.random.randint(-diff, diff+1), np.random.randint(-diff, diff+1))\n        else:\n            (h, v) = (0,0)\n        image = image[(sz1-sz2+v):(sz1+sz2+v),(sz1-sz2+h):(sz1+sz2+h),:]\n    return image\n  \ndef flip(image, random_flip):\n    if random_flip and np.random.choice([True, False]):\n        image = np.fliplr(image)\n    return image\n\ndef to_rgb(img):\n    w, h = img.shape\n    ret = np.empty((w, h, 3), dtype=np.uint8)\n    ret[:, :, 0] = ret[:, :, 1] = ret[:, :, 2] = img\n    return ret\n  \n\ndef load_data(image_paths, do_random_crop, do_flip, image_height,image_width, do_prewhiten=True,src_size=None):\n    nrof_samples = len(image_paths)\n    images = np.zeros((nrof_samples, image_height, image_width, 3))\n    for i in range(nrof_samples):\n        img = misc.imread(image_paths[i])\n        if src_size is not None:\n            img = misc.imresize(img,(src_size[0],src_size[1]))\n        if img.ndim == 2:\n            img = to_rgb(img)\n        if do_prewhiten:\n            img = prewhiten(img)\n        else:\n            img = img - 127.5\n            img = img / 128.\n\n        #img = crop(img, do_random_crop, image_size)\n        #img = flip(img, do_random_flip)\n        if do_flip:\n            img = np.fliplr(img)\n        images[i,:,:,:] = img\n    return images\n\ndef l2_normalize(x):\n    n,e = x.shape\n    mean = np.mean(x,axis=1)\n    mean = mean.reshape((n,1))\n    mean = np.repeat(mean,e,axis=1)\n    x -= mean\n    norm = np.linalg.norm(x,axis=1)\n    norm = norm.reshape((n,1))\n    norm = np.repeat(norm,e,axis=1)\n    y = np.multiply(x,1/norm)\n    return y\n\n\n\n\n\ndef data_from_mx(inputs,image_size,do_resize=True,do_prewhiten=True):\n    \'\'\'\n    convet data in mxnet format to tf format\n    params:\n    - inputs: [n,h,w,c]\n    \'\'\'\n    nrof_samples = len(inputs)\n    images = np.zeros((nrof_samples, image_size, image_size, 3))\n    for i in range(nrof_samples):\n        img = inputs[i].transpose((1,2,0))\n        #plt.imshow(img)\n        #plt.show()\n        #pdb.set_trace()\n        if img.ndim == 2:\n            img = to_rgb(img)\n        if do_resize:\n            img = misc.imresize(img,[image_size,image_size])\n        if do_prewhiten:\n            img = prewhiten(img)\n        #img = crop(img, do_random_crop, image_size)\n        #img = flip(img, do_random_flip)\n        images[i,:,:,:] = img\n    return images\n\ndef load_data_simple(image_paths,image_size,do_random_crop=False):\n    nrof_samples = len(image_paths)\n    images = np.zeros((nrof_samples, image_size, image_size, 3))\n    for i in range(nrof_samples):\n        img = misc.imread(image_paths[i])\n        #pdb.set_trace()\n        if img.ndim == 2:\n            img = to_rgb(img)\n        #if do_prewhiten:\n        #img = misc.imresize(img,[image_size,image_size])\n        img = prewhiten(img)\n        img = crop(img, do_random_crop, image_size)\n        #img = flip(img, do_random_flip)\n        images[i,:,:,:] = img\n    return images\ndef load_data_caffe(image_paths,image_size):\n    nrof_samples = len(image_paths)\n    images = np.zeros((nrof_samples, image_size, image_size, 3))\n    for i in range(nrof_samples):\n        img = misc.imread(image_paths[i])\n        #pdb.set_trace()\n        if img.ndim == 2:\n            img = to_rgb(img)\n        #if do_prewhiten:\n        img = misc.imresize(img,[image_size,image_size])\n        #img = crop(img, do_random_crop, image_size)\n        #img = flip(img, do_random_flip)\n        images[i,:,:,:] = img\n    return images\n\n\n\n\ndef get_label_batch(label_data, batch_size, batch_index):\n    nrof_examples = np.size(label_data, 0)\n    j = batch_index*batch_size % nrof_examples\n    if j+batch_size<=nrof_examples:\n        batch = label_data[j:j+batch_size]\n    else:\n        x1 = label_data[j:nrof_examples]\n        x2 = label_data[0:nrof_examples-j]\n        batch = np.vstack([x1,x2])\n    batch_int = batch.astype(np.int64)\n    return batch_int\n\ndef get_batch(image_data, batch_size, batch_index):\n    nrof_examples = np.size(image_data, 0)\n    j = batch_index*batch_size % nrof_examples\n    if j+batch_size<=nrof_examples:\n        batch = image_data[j:j+batch_size,:,:,:]\n    else:\n        x1 = image_data[j:nrof_examples,:,:,:]\n        x2 = image_data[0:nrof_examples-j,:,:,:]\n        batch = np.vstack([x1,x2])\n    batch_float = batch.astype(np.float32)\n    return batch_float\n\ndef get_triplet_batch(triplets, batch_index, batch_size):\n    ax, px, nx = triplets\n    a = get_batch(ax, int(batch_size/3), batch_index)\n    p = get_batch(px, int(batch_size/3), batch_index)\n    n = get_batch(nx, int(batch_size/3), batch_index)\n    batch = np.vstack([a, p, n])\n    return batch\n\ndef get_learning_rate_from_file(filename, epoch):\n    with open(filename, \'r\') as f:\n        for line in f.readlines():\n            line = line.split(\'#\', 1)[0]\n            if line:\n                par = line.strip().split(\':\')\n                e = int(par[0])\n                lr = float(par[1])\n                if e <= epoch:\n                    learning_rate = lr\n                else:\n                    return learning_rate\n\nclass ImageClass():\n    ""Stores the paths to images for a given class""\n    def __init__(self, name, image_paths):\n        self.name = name\n        self.image_paths = image_paths\n  \n    def __str__(self):\n        return self.name + \', \' + str(len(self.image_paths)) + \' images\'\n  \n    def __len__(self):\n        return len(self.image_paths)\n  \ndef get_dataset(paths, has_class_directories=True):\n    dataset = []\n    count = 1\n    for path in paths.split(\':\'):\n        path_exp = os.path.expanduser(path)\n        classes = os.listdir(path_exp)\n        classes.sort()\n        nrof_classes = len(classes)\n        for i in range(nrof_classes):\n            class_name = classes[i]\n            facedir = os.path.join(path_exp, class_name)\n            image_paths = get_image_paths(facedir)\n            class_name = \'{}_{:08d}\'.format(class_name,count)\n            count += 1\n            dataset.append(ImageClass(class_name, image_paths))\n  \n    return dataset\ndef dataset_from_list(data_dir,list_file):\n    dataset = []\n    lines = open(list_file,\'r\').read().strip().split(\'\\n\')\n    path_exp = os.path.expanduser(data_dir)\n    count = 1\n    class_paths = {}\n    for line in lines:\n        image_path, _ = line.split(\' \')\n        class_name, _ = image_path.split(\'/\')\n        if class_name not in class_paths:\n            class_paths[class_name] = []\n        full_image_path = os.path.join(path_exp,image_path)\n        assert os.path.exists(full_image_path), \'file {} not exist\'.format(full_image_path)\n        class_paths[class_name].append(full_image_path)\n    dataset = []\n    keys = class_paths.keys()\n    keys.sort()\n    for key in keys:\n        dataset.append(ImageClass(key,class_paths[key]))\n    return dataset\n\n        \n\n\ndef get_image_paths(facedir):\n    image_paths = []\n    if os.path.isdir(facedir):\n        images = os.listdir(facedir)\n        image_paths = [os.path.join(facedir,img) for img in images]\n    return image_paths\n  \ndef split_dataset(dataset, split_ratio, mode):\n    if mode==\'SPLIT_CLASSES\':\n        nrof_classes = len(dataset)\n        class_indices = np.arange(nrof_classes)\n        np.random.shuffle(class_indices)\n        split = int(round(nrof_classes*split_ratio))\n        train_set = [dataset[i] for i in class_indices[0:split]]\n        test_set = [dataset[i] for i in class_indices[split:-1]]\n    elif mode==\'SPLIT_IMAGES\':\n        train_set = []\n        test_set = []\n        min_nrof_images = 2\n        for cls in dataset:\n            paths = cls.image_paths\n            np.random.shuffle(paths)\n            split = int(round(len(paths)*split_ratio))\n            if split<min_nrof_images:\n                continue  # Not enough images for test set. Skip class...\n            train_set.append(ImageClass(cls.name, paths[0:split]))\n            test_set.append(ImageClass(cls.name, paths[split:-1]))\n    else:\n        raise ValueError(\'Invalid train/test split mode ""%s""\' % mode)\n    return train_set, test_set\n\ndef load_model(model):\n    # Check if the model is a model directory (containing a metagraph and a checkpoint file)\n    #  or if it is a protobuf file with a frozen graph\n    model_exp = os.path.expanduser(model)\n    if (os.path.isfile(model_exp)):\n        print(\'Model filename: %s\' % model_exp)\n        with gfile.FastGFile(model_exp,\'rb\') as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n            tf.import_graph_def(graph_def, name=\'\')\n    else:\n        print(\'Model directory: %s\' % model_exp)\n        meta_file, ckpt_file = get_model_filenames(model_exp)\n        \n        print(\'Metagraph file: %s\' % meta_file)\n        print(\'Checkpoint file: %s\' % ckpt_file)\n      \n        saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file))\n        saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n    \ndef get_model_filenames(model_dir):\n    files = os.listdir(model_dir)\n    meta_files = [s for s in files if s.endswith(\'.meta\')]\n    if len(meta_files)==0:\n        raise ValueError(\'No meta file found in the model directory (%s)\' % model_dir)\n    elif len(meta_files)>1:\n        raise ValueError(\'There should not be more than one meta file in the model directory (%s)\' % model_dir)\n    meta_file = meta_files[0]\n    meta_files = [s for s in files if \'.ckpt\' in s]\n    max_step = -1\n    for f in files:\n        step_str = re.match(r\'(^model-[\\w\\- ]+.ckpt-(\\d+))\', f)\n        if step_str is not None and len(step_str.groups())>=2:\n            step = int(step_str.groups()[1])\n            if step > max_step:\n                max_step = step\n                ckpt_file = step_str.groups()[0]\n    return meta_file, ckpt_file\n\ndef calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10):\n    assert(embeddings1.shape[0] == embeddings2.shape[0])\n    assert(embeddings1.shape[1] == embeddings2.shape[1])\n    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n    nrof_thresholds = len(thresholds)\n    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n    \n    tprs = np.zeros((nrof_folds,nrof_thresholds))\n    fprs = np.zeros((nrof_folds,nrof_thresholds))\n    accuracy = np.zeros((nrof_folds))\n    \n    diff = np.subtract(embeddings1, embeddings2)\n    dist = np.sum(np.square(diff),1)\n    indices = np.arange(nrof_pairs)\n    #pdb.set_trace() \n    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n        \n        # Find the best threshold for the fold\n        acc_train = np.zeros((nrof_thresholds))\n        for threshold_idx, threshold in enumerate(thresholds):\n            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n        best_threshold_index = np.argmax(acc_train)\n        for threshold_idx, threshold in enumerate(thresholds):\n            tprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set])\n        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\n          \n    tpr = np.mean(tprs,0)\n    fpr = np.mean(fprs,0)\n    return tpr, fpr, accuracy\n\ndef calculate_accuracy(threshold, dist, actual_issame):\n    predict_issame = np.less(dist, threshold)\n    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n  \n    tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\n    fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\n    acc = float(tp+tn)/dist.size\n    return tpr, fpr, acc\n\n\n  \ndef calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10):\n    assert(embeddings1.shape[0] == embeddings2.shape[0])\n    assert(embeddings1.shape[1] == embeddings2.shape[1])\n    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n    nrof_thresholds = len(thresholds)\n    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n    \n    val = np.zeros(nrof_folds)\n    far = np.zeros(nrof_folds)\n    \n    diff = np.subtract(embeddings1, embeddings2)\n    dist = np.sum(np.square(diff),1)\n    indices = np.arange(nrof_pairs)\n    \n    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n      \n        # Find the threshold that gives FAR = far_target\n        far_train = np.zeros(nrof_thresholds)\n        for threshold_idx, threshold in enumerate(thresholds):\n            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\n        if np.max(far_train)>=far_target:\n            f = interpolate.interp1d(far_train, thresholds, kind=\'slinear\')\n            threshold = f(far_target)\n        else:\n            threshold = 0.0\n    \n        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\n  \n    val_mean = np.mean(val)\n    far_mean = np.mean(far)\n    val_std = np.std(val)\n    return val_mean, val_std, far_mean\n\n\ndef calculate_val_far(threshold, dist, actual_issame):\n    predict_issame = np.less(dist, threshold)\n    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n    n_same = np.sum(actual_issame)\n    n_diff = np.sum(np.logical_not(actual_issame))\n    val = float(true_accept) / float(n_same)\n    far = float(false_accept) / float(n_diff)\n    return val, far\n\ndef store_revision_info(src_path, output_dir, arg_string):\n  \n    # Get git hash\n    gitproc = Popen([\'git\', \'rev-parse\', \'HEAD\'], stdout = PIPE, cwd=src_path)\n    (stdout, _) = gitproc.communicate()\n    git_hash = stdout.strip()\n  \n    # Get local changes\n    gitproc = Popen([\'git\', \'diff\', \'HEAD\'], stdout = PIPE, cwd=src_path)\n    (stdout, _) = gitproc.communicate()\n    git_diff = stdout.strip()\n    \n    # Store a text file in the log directory\n    rev_info_filename = os.path.join(output_dir, \'revision_info.txt\')\n    with open(rev_info_filename, ""w"") as text_file:\n        text_file.write(\'arguments: %s\\n--------------------\\n\' % arg_string)\n        text_file.write(\'git hash: %s\\n--------------------\\n\' % git_hash)\n        text_file.write(\'%s\' % git_diff)\n\ndef list_variables(filename):\n    reader = training.NewCheckpointReader(filename)\n    variable_map = reader.get_variable_to_shape_map()\n    names = sorted(variable_map.keys())\n    return names\n\ndef put_images_on_grid(images, shape=(16,8)):\n    nrof_images = images.shape[0]\n    img_size = images.shape[1]\n    bw = 3\n    img = np.zeros((shape[1]*(img_size+bw)+bw, shape[0]*(img_size+bw)+bw, 3), np.float32)\n    for i in range(shape[1]):\n        x_start = i*(img_size+bw)+bw\n        for j in range(shape[0]):\n            img_index = i*shape[0]+j\n            if img_index>=nrof_images:\n                break\n            y_start = j*(img_size+bw)+bw\n            img[x_start:x_start+img_size, y_start:y_start+img_size, :] = images[img_index, :, :, :]\n        if img_index>=nrof_images:\n            break\n    return img\n\ndef write_arguments_to_file(args, filename):\n    with open(filename, \'w\') as f:\n        for key, value in vars(args).iteritems():\n            f.write(\'%s: %s\\n\' % (key, str(value)))\n'"
networks/inception_resnet_v1.py,31,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Contains the definition of the Inception Resnet V1 architecture.\nAs described in http://arxiv.org/abs/1602.07261.\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n# Inception-Renset-A\ndef block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n    """"""Builds the 35x35 resnet block.""""""\n    with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n        with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n            tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv2_1 = slim.conv2d(tower_conv2_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n            tower_conv2_2 = slim.conv2d(tower_conv2_1, 32, 3, scope=\'Conv2d_0c_3x3\')\n        mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\n        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                         activation_fn=None, scope=\'Conv2d_1x1\')\n        net += scale * up\n        if activation_fn:\n            net = activation_fn(net)\n    return net\n\n# Inception-Renset-B\ndef block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n    """"""Builds the 17x17 resnet block.""""""\n    with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n        with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 128, 1, scope=\'Conv2d_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 128, [1, 7],\n                                        scope=\'Conv2d_0b_1x7\')\n            tower_conv1_2 = slim.conv2d(tower_conv1_1, 128, [7, 1],\n                                        scope=\'Conv2d_0c_7x1\')\n        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                         activation_fn=None, scope=\'Conv2d_1x1\')\n        net += scale * up\n        if activation_fn:\n            net = activation_fn(net)\n    return net\n\n\n# Inception-Resnet-C\ndef block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n    """"""Builds the 8x8 resnet block.""""""\n    with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n        with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 192, [1, 3],\n                                        scope=\'Conv2d_0b_1x3\')\n            tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [3, 1],\n                                        scope=\'Conv2d_0c_3x1\')\n        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                         activation_fn=None, scope=\'Conv2d_1x1\')\n        net += scale * up\n        if activation_fn:\n            net = activation_fn(net)\n    return net\n  \ndef reduction_a(net, k, l, m, n):\n    with tf.variable_scope(\'Branch_0\'):\n        tower_conv = slim.conv2d(net, n, 3, stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n    with tf.variable_scope(\'Branch_1\'):\n        tower_conv1_0 = slim.conv2d(net, k, 1, scope=\'Conv2d_0a_1x1\')\n        tower_conv1_1 = slim.conv2d(tower_conv1_0, l, 3,\n                                    scope=\'Conv2d_0b_3x3\')\n        tower_conv1_2 = slim.conv2d(tower_conv1_1, m, 3,\n                                    stride=2, padding=\'VALID\',\n                                    scope=\'Conv2d_1a_3x3\')\n    with tf.variable_scope(\'Branch_2\'):\n        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n    net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n    return net\n\ndef reduction_b(net):\n    with tf.variable_scope(\'Branch_0\'):\n        tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n        tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                   padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n    with tf.variable_scope(\'Branch_1\'):\n        tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n        tower_conv1_1 = slim.conv2d(tower_conv1, 256, 3, stride=2,\n                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n    with tf.variable_scope(\'Branch_2\'):\n        tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n        tower_conv2_1 = slim.conv2d(tower_conv2, 256, 3,\n                                    scope=\'Conv2d_0b_3x3\')\n        tower_conv2_2 = slim.conv2d(tower_conv2_1, 256, 3, stride=2,\n                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n    with tf.variable_scope(\'Branch_3\'):\n        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n    net = tf.concat([tower_conv_1, tower_conv1_1,\n                        tower_conv2_2, tower_pool], 3)\n    return net\n  \ndef inference(images, keep_probability, phase_train=True, \n              bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\n    batch_norm_params = {\n        # Decay for the moving averages.\n        \'decay\': 0.995,\n        # epsilon to prevent 0s in variance.\n        \'epsilon\': 0.001,\n        #\'is_training\': phase_train,\n        # force in-place updates of mean and variance estimates\n        #\'updates_collections\': None,\n        # Moving averages ends up in the trainable variables collection\n        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n    }\n    \n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n                        weights_regularizer=slim.l2_regularizer(weight_decay),\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params):\n        return inception_resnet_v1(images, is_training=phase_train,\n              dropout_keep_prob=keep_probability, bottleneck_layer_size=bottleneck_layer_size, reuse=reuse)\n\n\ndef inception_resnet_v1(inputs, is_training=True,\n                        dropout_keep_prob=0.8,\n                        bottleneck_layer_size=128,\n                        reuse=None, \n                        scope=\'InceptionResnetV1\'):\n    """"""Creates the Inception Resnet V1 model.\n    Args:\n      inputs: a 4-D tensor of size [batch_size, height, width, 3].\n      num_classes: number of predicted classes.\n      is_training: whether is training or not.\n      dropout_keep_prob: float, the fraction to keep before final layer.\n      reuse: whether or not the network and its variables should be reused. To be\n        able to reuse \'scope\' must be given.\n      scope: Optional variable_scope.\n    Returns:\n      logits: the logits outputs of the model.\n      end_points: the set of end_points from the inception model.\n    """"""\n    end_points = {}\n  \n    with tf.variable_scope(scope, \'InceptionResnetV1\', [inputs], reuse=reuse):\n        with slim.arg_scope([slim.batch_norm, slim.dropout],\n                            is_training=is_training):\n            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                                stride=1, padding=\'SAME\'):\n      \n                # 149 x 149 x 32\n                net = slim.conv2d(inputs, 32, 3, stride=2, padding=\'VALID\',\n                                  scope=\'Conv2d_1a_3x3\')\n                end_points[\'Conv2d_1a_3x3\'] = net\n                # 147 x 147 x 32\n                net = slim.conv2d(net, 32, 3, padding=\'VALID\',\n                                  scope=\'Conv2d_2a_3x3\')\n                end_points[\'Conv2d_2a_3x3\'] = net\n                # 147 x 147 x 64\n                net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n                end_points[\'Conv2d_2b_3x3\'] = net\n                # 73 x 73 x 64\n                net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                      scope=\'MaxPool_3a_3x3\')\n                end_points[\'MaxPool_3a_3x3\'] = net\n                # 73 x 73 x 80\n                net = slim.conv2d(net, 80, 1, padding=\'VALID\',\n                                  scope=\'Conv2d_3b_1x1\')\n                end_points[\'Conv2d_3b_1x1\'] = net\n                # 71 x 71 x 192\n                net = slim.conv2d(net, 192, 3, padding=\'VALID\',\n                                  scope=\'Conv2d_4a_3x3\')\n                end_points[\'Conv2d_4a_3x3\'] = net\n                # 35 x 35 x 256\n                net = slim.conv2d(net, 256, 3, stride=2, padding=\'VALID\',\n                                  scope=\'Conv2d_4b_3x3\')\n                end_points[\'Conv2d_4b_3x3\'] = net\n                \n                # 5 x Inception-resnet-A\n                net = slim.repeat(net, 5, block35, scale=0.17)\n                end_points[\'Mixed_5a\'] = net\n        \n                # Reduction-A\n                with tf.variable_scope(\'Mixed_6a\'):\n                    net = reduction_a(net, 192, 192, 256, 384)\n                end_points[\'Mixed_6a\'] = net\n                \n                # 10 x Inception-Resnet-B\n                net = slim.repeat(net, 10, block17, scale=0.10)\n                end_points[\'Mixed_6b\'] = net\n                \n                # Reduction-B\n                with tf.variable_scope(\'Mixed_7a\'):\n                    net = reduction_b(net)\n                end_points[\'Mixed_7a\'] = net\n                \n                # 5 x Inception-Resnet-C\n                net = slim.repeat(net, 5, block8, scale=0.20)\n                end_points[\'Mixed_8a\'] = net\n                \n                net = block8(net, activation_fn=None)\n                end_points[\'Mixed_8b\'] = net\n                \n                with tf.variable_scope(\'Logits\'):\n                    end_points[\'PrePool\'] = net\n                    #pylint: disable=no-member\n                    net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                                          scope=\'AvgPool_1a_8x8\')\n                    net = slim.flatten(net)\n          \n                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                                       scope=\'Dropout\')\n          \n                    end_points[\'PreLogitsFlatten\'] = net\n                \n                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \n                        scope=\'Bottleneck\', reuse=False)\n  \n    return net, end_points\n'"
networks/resface.py,20,"b""import tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n'''\nResface20 and Resface36 proposed in sphereface and applied in Additive Margin Softmax paper\nNotice:\nbatch norm is used in line 111. to cancel batch norm, simply commend out line 111 and use line 112\n'''\n\ndef prelu(x):\n    with tf.variable_scope('PRelu'):   \n        #alphas = tf.Variable(tf.constant(0.25,dtype=tf.float32,shape=[x.get_shape()[-1]]),name='prelu_alphas')\n        alphas = tf.get_variable(name='prelu_alphas',initializer=tf.constant(0.25,dtype=tf.float32,shape=[x.get_shape()[-1]]))\n        pos = tf.nn.relu(x)\n        neg = alphas * (x - abs(x)) * 0.5\n        return pos + neg\n\ndef resface_block(lower_input,output_channels,scope=None):\n    with tf.variable_scope(scope):\n        net = slim.conv2d(lower_input, output_channels,weights_initializer=tf.truncated_normal_initializer(stddev=0.01))\n        net = slim.conv2d(net, output_channels,weights_initializer=tf.truncated_normal_initializer(stddev=0.01))\n        return lower_input + net\n\ndef resface_pre(lower_input,output_channels,scope=None):\n    net = slim.conv2d(lower_input, output_channels, stride=2, scope=scope)\n    return net\n\ndef resface20(images, keep_probability, \n             phase_train=True, bottleneck_layer_size=512, \n             weight_decay=0.0, reuse=None):\n    '''\n    conv name\n    conv[conv_layer]_[block_index]_[block_layer_index]\n    '''\n    with tf.variable_scope('Conv1'):\n        net = resface_pre(images,64,scope='Conv1_pre')\n        net = slim.repeat(net,1,resface_block,64,scope='Conv1')\n    with tf.variable_scope('Conv2'):\n        net = resface_pre(net,128,scope='Conv2_pre')\n        net = slim.repeat(net,2,resface_block,128,scope='Conv2')\n    with tf.variable_scope('Conv3'):\n        net = resface_pre(net,256,scope='Conv3_pre')\n        net = slim.repeat(net,4,resface_block,256,scope='Conv3')\n    with tf.variable_scope('Conv4'):\n        net = resface_pre(net,512,scope='Conv4_pre')\n        net = slim.repeat(net,1,resface_block,512,scope='Conv4')\n\n    with tf.variable_scope('Logits'):\n        #pylint: disable=no-member\n        #net = slim.avg_pool2d(net, net.get_shape()[1:3], padding='VALID',\n        #                      scope='AvgPool')\n        net = slim.flatten(net)\n\n        net = slim.dropout(net, keep_probability, is_training=phase_train,\n                           scope='Dropout')\n    \n    net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \n            scope='Bottleneck', reuse=False)            \n    return net,''\n\ndef resface36(images, keep_probability, \n             phase_train=True, bottleneck_layer_size=512, \n             weight_decay=0.0, reuse=None):\n    '''\n    conv name\n    conv[conv_layer]_[block_index]_[block_layer_index]\n    '''\n    with tf.variable_scope('Conv1'):\n        net = resface_pre(images,64,scope='Conv1_pre')\n        net = slim.repeat(net,2,resface_block,64,scope='Conv_1')\n    with tf.variable_scope('Conv2'):\n        net = resface_pre(net,128,scope='Conv2_pre')\n        net = slim.repeat(net,4,resface_block,128,scope='Conv_2')\n    with tf.variable_scope('Conv3'):\n        net = resface_pre(net,256,scope='Conv3_pre')\n        net = slim.repeat(net,8,resface_block,256,scope='Conv_3')\n    with tf.variable_scope('Conv4'):\n        net = resface_pre(net,512,scope='Conv4_pre')\n        #net = resface_block(Conv4_pre,512,scope='Conv4_1')\n        net = slim.repeat(net,1,resface_block,512,scope='Conv4')\n\n    with tf.variable_scope('Logits'):\n        #pylint: disable=no-member\n        #net = slim.avg_pool2d(net, net.get_shape()[1:3], padding='VALID',\n        #                      scope='AvgPool')\n        net = slim.flatten(net)\n        net = slim.dropout(net, keep_probability, is_training=phase_train,\n                           scope='Dropout')\n    net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \n            scope='Bottleneck', reuse=False)    \n    return net,''\n\ndef inference(image_batch, keep_probability, \n              phase_train=True, bottleneck_layer_size=512, \n              weight_decay=0.0,reuse=False):\n    batch_norm_params = {\n        'decay': 0.995,\n        'epsilon': 0.001,\n        'scale':True,\n        'is_training': phase_train,\n        'updates_collections': None,\n        'variables_collections': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n    }    \n    with tf.variable_scope('Resface',reuse=reuse):\n        with slim.arg_scope([slim.conv2d, slim.fully_connected], \n                             weights_initializer=tf.contrib.layers.xavier_initializer(),\n                             weights_regularizer=slim.l2_regularizer(weight_decay), \n                             activation_fn=prelu,\n                             normalizer_fn=slim.batch_norm,\n                             #normalizer_fn=None,\n                             normalizer_params=batch_norm_params):\n            with slim.arg_scope([slim.conv2d], kernel_size=3):\n                return resface20(images=image_batch, \n                                keep_probability=keep_probability, \n                                phase_train=phase_train, \n                                bottleneck_layer_size=bottleneck_layer_size)\n"""
networks/sphere_network.py,28,"b""import tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.framework import ops\n\nl2_regularizer= tf.contrib.layers.l2_regularizer(1.0)\nxavier = tf.contrib.layers.xavier_initializer_conv2d() \ndef get_shape(tensor):\n    static_shape = tensor.shape.as_list()\n    dynamic_shape = tf.unstack(tf.shape(tensor))\n    dims = [s[1] if s[0] is None else s[0] for s in zip(static_shape,dynamic_shape)]\n    return dims\ndef infer(input,embedding_size=512):\n    with tf.variable_scope('conv1_'):\n        network = first_conv(input, 64, name = 'conv1')\n        network = block(network, 'conv1_23', 64)\n    with tf.variable_scope('conv2_'):\n        network = first_conv(network, 128, name = 'conv2')\n        network = block(network, 'conv2_23', 128)\n        network = block(network, 'conv2_45', 128)\n    with tf.variable_scope('conv3_'):\n        network = first_conv(network, 256, name = 'conv3')\n        network = block(network, 'conv3_23', 256)\n        network = block(network, 'conv3_45', 256)\n        network = block(network, 'conv3_67', 256)\n        network = block(network, 'conv3_89', 256)\n    with tf.variable_scope('conv4_'):\n        network = first_conv(network, 512, name = 'conv4')\n        network = block(network, 'conv4_23', 512)\n    with tf.variable_scope('feature'):\n        #BATCH_SIZE = network.get_shape()[0]\n        dims = get_shape(network)\n        print(dims)\n        #BATCH_SIZE = tf.shape(network)[0]\n        #feature = tf.layers.dense(tf.reshape(network,[BATCH_SIZE, -1]), 512, kernel_regularizer = l2_regularizer, kernel_initializer = xavier)\n        feature = tf.layers.dense(tf.reshape(network,[dims[0], np.prod(dims[1:])]), embedding_size, kernel_regularizer = l2_regularizer, kernel_initializer = xavier)\n    return feature\n\n\ndef prelu(x, name = 'prelu'):\n    with tf.variable_scope(name):\n        alphas = tf.get_variable('alpha', x.get_shape()[-1], initializer=tf.constant_initializer(0.25), regularizer = l2_regularizer, dtype = tf.float32)\n    pos = tf.nn.relu(x)\n    neg = tf.multiply(alphas,(x - abs(x)) * 0.5)\n    return pos + neg\n\ndef first_conv(input, num_output, name):\n    \n    zero_init = tf.zeros_initializer()\n    network = tf.layers.conv2d(input, num_output, kernel_size = [3, 3], strides = (2, 2), padding = 'same', kernel_initializer = xavier, bias_initializer = zero_init, kernel_regularizer = l2_regularizer, bias_regularizer = l2_regularizer)\n    network = prelu(network, name = name)\n    return network\n\n\ndef block(input, name, num_output):\n    with tf.variable_scope(name):\n        network = tf.layers.conv2d(input, num_output, kernel_size = [3, 3], strides = [1, 1], padding = 'same', kernel_initializer = tf.random_normal_initializer(stddev=0.01), use_bias = False , kernel_regularizer = l2_regularizer)\n        network = prelu(network, name = 'name'+ '1')\n        network = tf.layers.conv2d(network, num_output, kernel_size = [3, 3], strides = [1, 1], padding = 'same', kernel_initializer = tf.random_normal_initializer(stddev=0.01), use_bias = False, kernel_regularizer = l2_regularizer)\n        network = prelu(network, name = 'name'+ '2')\n        network = tf.add(input, network)\n        return network\n\ndef get_normal_loss(input, label, num_output, lambda_value, m_value = 4):\n    feature_dim = input.get_shape()[1]\n    weight = tf.get_variable('weight', shape = [num_output, feature_dim], regularizer = l2_regularizer, initializer = xavier)\n    prob_distribution = tf.one_hot(label, num_output)\n    weight = tf.nn.l2_normalize(weight, dim = 1)\n    label_float = tf.cast(label, tf.float32)\n    margin_out = marginInnerProduct_module.margin_inner_product(input, weight, tf.constant(m_value), lambda_value, label_float)\n    #margin_out = tf.layers.dense(input, num_output)\n\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=margin_out, labels = prob_distribution))\n    return loss\n\n"""
test/test.py,10,"b""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nimport sys\nsys.path.insert(0,'networks')\nsys.path.insert(0,'lib')\nimport utils\nimport lfw\nimport os\nimport math\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.contrib.slim.nets import resnet_v1, resnet_v2\n#from models import resnet_v1, resnet_v2,resnet_v1_modify,resnet_v2_modify\nimport sphere_network as network\nfrom sklearn import metrics\nfrom scipy.optimize import brentq\nfrom scipy import interpolate\nimport importlib\nimport pdb\n\ndef main(args):\n  \n    with tf.Graph().as_default():\n      \n        with tf.Session() as sess:\n            \n            # Read the file containing the pairs used for testing\n            pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\n            #pdb.set_trace()\n\n            # Get the paths for the corresponding images\n            paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs, args.lfw_file_ext)\n\n\n            \n            #image_size = images_placeholder.get_shape()[1]  # For some reason this doesn't work for frozen graphs\n            image_size = args.image_size\n            print('image size',image_size)\n            #images_placeholder = tf.placeholder(tf.float32,shape=(None,image_size,image_size,3),name='image')\n            images_placeholder = tf.placeholder(tf.float32,shape=(None,args.image_height,args.image_width,3),name='image')\n            phase_train_placeholder = tf.placeholder(tf.bool, name='phase_train')\n            #with slim.arg_scope(resnet_v1.resnet_arg_scope(False)):\n            if args.network_type == 'resnet50':\n                with slim.arg_scope(resnet_v2.resnet_arg_scope(False)):\n                    prelogits, end_points = resnet_v2.resnet_v2_50(images_placeholder,is_training=phase_train_placeholder,num_classes=256,output_stride=16)\n                    #prelogits, end_points = resnet_v2.resnet_v2_50(images_placeholder,is_training=phase_train_placeholder,num_classes=256,output_stride=8)\n                    #prelogits, end_points = resnet_v2_modify.resnet_v2_50(images_placeholder,is_training=phase_train_placeholder,num_classes=256)\n                    #prelogits = slim.batch_norm(prelogits, is_training=phase_train_placeholder,epsilon=1e-5, scale=True,scope='softmax_bn')\n                    prelogits = tf.squeeze(prelogits,[1,2],name='SpatialSqueeze')\n     \n            elif args.network_type == 'sphere_network':\n                prelogits = network.infer(images_placeholder,args.embedding_size)\n                if args.fc_bn:\n                    print('do batch norm after network')\n                    prelogits = slim.batch_norm(prelogits, is_training=phase_train_placeholder,epsilon=1e-5, scale=True,scope='softmax_bn')\n    \n\n            \n            #embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name='embeddings')\n            embeddings = tf.identity(prelogits)\n            #saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=3)\n            saver.restore(sess, args.model)\n            if args.save_model:\n                saver.save(sess,'./tmp_saved_model',global_step=1)\n                return 0\n \n            embedding_size = embeddings.get_shape()[1]\n            # Run forward pass to calculate embeddings\n            print('Runnning forward pass on LFW images')\n            batch_size = args.lfw_batch_size\n            nrof_images = len(paths)\n            nrof_batches = int(math.ceil(1.0*nrof_images / batch_size))\n            if args.do_flip:\n                embedding_size *= 2\n                emb_array = np.zeros((nrof_images, embedding_size))\n            else:\n                emb_array = np.zeros((nrof_images, embedding_size))\n            for i in range(nrof_batches):\n                start_index = i*batch_size\n                print('handing {}/{}'.format(start_index,nrof_images))\n                end_index = min((i+1)*batch_size, nrof_images)\n                paths_batch = paths[start_index:end_index]\n                #images = facenet.load_data(paths_batch, False, False, image_size,True,image_size)\n                #images = facenet.load_data2(paths_batch, False, False, args.image_height,args.image_width,True,)\n                images = utils.load_data(paths_batch, False, False, args.image_height,args.image_width,args.prewhiten,(args.image_height,args.image_width))\n                feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n                feats = sess.run(embeddings, feed_dict=feed_dict)\n                if args.do_flip:\n                    images_flip = utils.load_data(paths_batch, False, True, args.image_height,args.image_width,args.prewhiten,(args.image_height,args.image_width))\n                    feed_dict = { images_placeholder:images_flip, phase_train_placeholder:False }\n                    feats_flip = sess.run(embeddings, feed_dict=feed_dict)\n                    feats = np.concatenate((feats,feats_flip),axis=1)\n                    #feats = (feats+feats_flip)/2\n                #images = facenet.load_data(paths_batch, False, False, 160,True,182)\n                #images = facenet.load_data(paths_batch, False, False, image_size,src_size=256)\n                #feed_dict = { images_placeholder:images, phase_train_placeholder:True}\n                #pdb.set_trace()\n                #feats = facenet.prewhiten(feats)\n                feats = utils.l2_normalize(feats)\n                emb_array[start_index:end_index,:] = feats\n                #pdb.set_trace()\n        \n            tpr, fpr, accuracy, val, val_std, far = lfw.evaluate(emb_array, \n                actual_issame, nrof_folds=args.lfw_nrof_folds)\n\n            print('Accuracy: %1.3f+-%1.3f' % (np.mean(accuracy), np.std(accuracy)))\n            print('Validation rate: %2.5f+-%2.5f @ FAR=%2.5f' % (val, val_std, far))\n\n            auc = metrics.auc(fpr, tpr)\n            print('Area Under Curve (AUC): %1.3f' % auc)\n            eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\n            print('Equal Error Rate (EER): %1.3f' % eer)\n            \ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument('lfw_dir', type=str,\n        help='Path to the data directory containing aligned LFW face patches.')\n    parser.add_argument('--network_type', type=str,\n        help='Network structure.',default='resnet50')\n    parser.add_argument('--fc_bn', \n        help='wheather bn is followed by fc layer.',default=False,action='store_true')\n    parser.add_argument('--prewhiten', \n        help='wheather do prewhiten to preprocess image.',default=False,action='store_true')\n    parser.add_argument('--save_model', type=bool,\n        help='whether save model to disk.',default=False)\n    parser.add_argument('--do_flip', type=bool,\n        help='wheather flip is used in test.',default=False)\n    parser.add_argument('--lfw_batch_size', type=int,\n        help='Number of images to process in a batch in the LFW test set.', default=200)\n    parser.add_argument('--embedding_size', type=int,\n        help='Feature embedding size.', default=512)\n    parser.add_argument('model', type=str, \n        help='Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file')\n    parser.add_argument('--image_size', type=int,\n        help='Image size (height, width) in pixels.', default=224)\n    parser.add_argument('--image_height', type=int,\n        help='Image size (height, width) in pixels.', default=112)\n    parser.add_argument('--image_width', type=int,\n        help='Image size (height, width) in pixels.', default=96)\n    parser.add_argument('--lfw_pairs', type=str,\n        help='The file containing the pairs to use for validation.', default='data/pairs.txt')\n    parser.add_argument('--lfw_file_ext', type=str,\n        help='The file extension for the LFW dataset.', default='png', choices=['jpg', 'png'])\n    parser.add_argument('--lfw_nrof_folds', type=int,\n        help='Number of folds to use for cross validation. Mainly used for testing.', default=10)\n    parser.add_argument('--model_def', type=str,\n        help='Model definition. Points to a module containing the definition of the inference graph.', default='models.inception_resnet_v1')\n    return parser.parse_args(argv)\n\nif __name__ == '__main__':\n    main(parse_arguments(sys.argv[1:]))\n"""
train/train_multi_gpu.py,61,"b'\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport os.path\nimport os\nimport time\nimport sys\nsys.path.insert(0,\'lib\')\nsys.path.insert(0,\'networks\')\nimport tensorflow as tf\nfrom tensorflow.python.client import timeline\nfrom tensorflow.contrib import slim\nimport tensorflow.contrib.data as tf_data\nfrom collections import Counter\nimport numpy as np\nimport importlib\nimport itertools\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.contrib.slim.nets import resnet_v1, resnet_v2\nimport argparse\nimport utils\nimport sphere_network as network\nimport inception_resnet_v1 as inception_net\nimport resface as resface\n#import lfw\nimport pdb\n#import cv2\n#import pylab as plt\n\ndebug = False\nsoftmax_ind = 0\n\nfrom tensorflow.python.ops import data_flow_ops\n\ndef _from_tensor_slices(tensors_x,tensors_y):\n    #return TensorSliceDataset((tensors_x,tensors_y))\n    return tf_data.Dataset.from_tensor_slices((tensors_x,tensors_y))\n\n\n\ndef main(args):\n  \n    #network = importlib.import_module(args.model_def)\n\n    subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\n    if not os.path.isdir(log_dir):  # Create the log directory if it doesn\'t exist\n        os.makedirs(log_dir)\n    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\n    if not os.path.isdir(model_dir):  # Create the model directory if it doesn\'t exist\n        os.makedirs(model_dir)\n\n    # Write arguments to a text file\n    utils.write_arguments_to_file(args, os.path.join(log_dir, \'arguments.txt\'))\n        \n    # Store some git revision info in a text file in the log directory\n    src_path,_ = os.path.split(os.path.realpath(__file__))\n    utils.store_revision_info(src_path, log_dir, \' \'.join(sys.argv))\n\n    np.random.seed(seed=args.seed)\n\n    #train_set = utils.get_dataset(args.data_dir)\n    train_set = utils.dataset_from_list(args.data_dir,args.list_file)\n    nrof_classes = len(train_set)\n    print(\'nrof_classes: \',nrof_classes)\n    image_list, label_list = utils.get_image_paths_and_labels(train_set)\n    print(\'total images: \',len(image_list))\n    image_list = np.array(image_list)\n    label_list = np.array(label_list,dtype=np.int32)\n\n    dataset_size = len(image_list)\n    single_batch_size = args.people_per_batch*args.images_per_person\n    indices = range(dataset_size)\n    np.random.shuffle(indices)\n\n    def _sample_people_softmax(x):\n        global softmax_ind\n        if softmax_ind >= dataset_size:\n            np.random.shuffle(indices)\n            softmax_ind = 0\n        true_num_batch = min(single_batch_size,dataset_size - softmax_ind)\n\n        sample_paths = image_list[indices[softmax_ind:softmax_ind+true_num_batch]]\n        sample_labels = label_list[indices[softmax_ind:softmax_ind+true_num_batch]]\n\n        softmax_ind += true_num_batch\n\n        return (np.array(sample_paths), np.array(sample_labels,dtype=np.int32))\n\n    def _sample_people(x):\n        \'\'\'We sample people based on tf.data, where we can use transform and prefetch.\n\n        \'\'\'\n    \n        image_paths, num_per_class = sample_people(train_set,args.people_per_batch*(args.num_gpus-1),args.images_per_person)\n        labels = []\n        for i in range(len(num_per_class)):\n            labels.extend([i]*num_per_class[i])\n        return (np.array(image_paths),np.array(labels,dtype=np.int32))\n\n    def _parse_function(filename,label):\n        file_contents = tf.read_file(filename)\n        image = tf.image.decode_image(file_contents, channels=3)\n        #image = tf.image.decode_jpeg(file_contents, channels=3)\n        print(image.shape)\n        \n        if args.random_crop:\n            print(\'use random crop\')\n            image = tf.random_crop(image, [args.image_size, args.image_size, 3])\n        else:\n            print(\'Not use random crop\')\n            #image.set_shape((args.image_size, args.image_size, 3))\n            image.set_shape((None,None, 3))\n            image = tf.image.resize_images(image, size=(args.image_height, args.image_width))\n            #print(image.shape)\n        if args.random_flip:\n            image = tf.image.random_flip_left_right(image)\n\n        #pylint: disable=no-member\n        #image.set_shape((args.image_size, args.image_size, 3))\n        image.set_shape((args.image_height, args.image_width, 3))\n        if debug:\n            image = tf.cast(image,tf.float32)\n        else:\n            image = tf.cast(image,tf.float32)\n            image = tf.subtract(image,127.5)\n            image = tf.div(image,128.)\n            #image = tf.image.per_image_standardization(image)\n        return image, label\n\n    \n    print(\'Model directory: %s\' % model_dir)\n    print(\'Log directory: %s\' % log_dir)\n    if args.pretrained_model:\n        print(\'Pre-trained model: %s\' % os.path.expanduser(args.pretrained_model))\n    \n        \n    with tf.Graph().as_default():\n        tf.set_random_seed(args.seed)\n        global_step = tf.Variable(0, trainable=False,name=\'global_step\')\n\n        # Placeholder for the learning rate\n        learning_rate_placeholder = tf.placeholder(tf.float32, name=\'learning_rate\')\n        \n        \n        phase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n        \n                \n        \n        #the image is generated by sequence\n        with tf.device(""/cpu:0""):\n                    \n            softmax_dataset = tf_data.Dataset.range(args.epoch_size*args.max_nrof_epochs*100)\n            softmax_dataset = softmax_dataset.map(lambda x: tf.py_func(_sample_people_softmax,[x],[tf.string,tf.int32]))\n            softmax_dataset = softmax_dataset.flat_map(_from_tensor_slices)\n            softmax_dataset = softmax_dataset.map(_parse_function,num_threads=8,output_buffer_size=2000)\n            softmax_dataset = softmax_dataset.batch(args.num_gpus*single_batch_size)\n            softmax_iterator = softmax_dataset.make_initializable_iterator()\n            softmax_next_element = softmax_iterator.get_next()\n            softmax_next_element[0].set_shape((args.num_gpus*single_batch_size, args.image_height,args.image_width,3))\n            softmax_next_element[1].set_shape(args.num_gpus*single_batch_size)\n            batch_image_split = tf.split(softmax_next_element[0],args.num_gpus)\n            batch_label_split = tf.split(softmax_next_element[1],args.num_gpus)\n            \n\n    \n        \n        \n        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\n            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\n        tf.summary.scalar(\'learning_rate\', learning_rate)\n\n        print(\'Using optimizer: {}\'.format(args.optimizer))\n        if args.optimizer == \'ADAGRAD\':\n            opt = tf.train.AdagradOptimizer(learning_rate)\n        elif args.optimizer == \'MOM\':\n            opt = tf.train.MomentumOptimizer(learning_rate,0.9)\n        elif args.optimizer == \'ADAM\':\n            opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n        else:\n            raise Exception(""Not supported optimizer: {}"".format(args.optimizer))\n        tower_losses = []\n        tower_cross = []\n        tower_dist = []\n        tower_reg= []\n        for i in range(args.num_gpus):\n            with tf.device(""/gpu:"" + str(i)):\n                with tf.name_scope(""tower_"" + str(i)) as scope:\n                  with slim.arg_scope([slim.model_variable, slim.variable], device=""/cpu:0""):\n                    with tf.variable_scope(tf.get_variable_scope()) as var_scope:\n                        reuse = False if i ==0 else True\n                        #with slim.arg_scope(resnet_v2.resnet_arg_scope(args.weight_decay)):\n                                #prelogits, end_points = resnet_v2.resnet_v2_50(batch_image_split[i],is_training=True,\n                                #        output_stride=16,num_classes=args.embedding_size,reuse=reuse)\n                        #prelogits, end_points = network.inference(batch_image_split[i], args.keep_probability, \n                        #    phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size, \n                        #    weight_decay=args.weight_decay, reuse=reuse)\n                        if args.network ==  \'sphere_network\':\n                            prelogits = network.infer(batch_image_split[i],args.embedding_size)\n                            print(prelogits)\n                        elif args.network == \'resface\':\n                            prelogits, _ = resface.inference(batch_image_split[i],1.0,bottleneck_layer_size=args.embedding_size,weight_decay=args.weight_decay,reuse=reuse)\n                        elif args.network == \'inception_net\':\n                            prelogits, endpoints = inception_net.inference(batch_image_split[i],1,phase_train=True,bottleneck_layer_size=args.embedding_size,weight_decay=args.weight_decay,reuse=reuse)\n                            print(prelogits)\n\n                        elif args.network == \'resnet_v2\':\n                            with slim.arg_scope(resnet_v2.resnet_arg_scope(args.weight_decay)):\n                                prelogits, end_points = resnet_v2.resnet_v2_50(batch_image_split[i],is_training=True,\n                                        output_stride=16,num_classes=args.embedding_size,reuse=reuse)\n                                prelogits = tf.squeeze(prelogits,axis=[1,2])\n\n                        else:\n                            raise Exception(""Not supported network: {}"".format(args.network))\n                        if args.fc_bn: \n\n                            prelogits = slim.batch_norm(prelogits, is_training=True, decay=0.997,epsilon=1e-5,scale=True,updates_collections=tf.GraphKeys.UPDATE_OPS,reuse=reuse,scope=\'softmax_bn\')\n                        if args.loss_type == \'softmax\':\n                            cross_entropy_mean = utils.softmax_loss(prelogits,batch_label_split[i], len(train_set),args.weight_decay,reuse)\n                            regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n                            tower_cross.append(cross_entropy_mean)\n                            #loss = cross_entropy_mean + args.weight_decay*tf.add_n(regularization_losses)\n                            loss = cross_entropy_mean + tf.add_n(regularization_losses)\n                            #tower_dist.append(0)\n                            #tower_cross.append(cross_entropy_mean)\n                            #tower_th.append(0)\n                            tower_losses.append(loss)\n                            tower_reg.append(regularization_losses)\n                        elif args.loss_type == \'cosface\':\n                            label_reshape = tf.reshape(batch_label_split[i],[single_batch_size])\n                            label_reshape = tf.cast(label_reshape,tf.int64)\n                            coco_loss = utils.cos_loss(prelogits,label_reshape, len(train_set),reuse,alpha=args.alpha,scale=args.scale)\n                            #scatter_loss, _ = facenet.coco_loss(prelogits,label_reshape, len(train_set),reuse,alpha=args.alpha,scale=args.scale)\n                            #coco_loss = scatter_loss[\'loss_total\']\n                            regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n                            if args.network == \'sphere_network\':\n                                print(\'reg loss using weight_decay * tf.add_n\')\n                                reg_loss  = args.weight_decay*tf.add_n(regularization_losses)\n                            else:\n                                print(\'reg loss using tf.add_n\')\n                                reg_loss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n                            loss = coco_loss + reg_loss\n                            \n                            tower_losses.append(loss)\n                            tower_reg.append(reg_loss)\n\n                        #loss = tf.add_n([cross_entropy_mean] + regularization_losses, name=\'total_loss\')\n                        tf.get_variable_scope().reuse_variables()\n        total_loss = tf.reduce_mean(tower_losses)\n        total_reg = tf.reduce_mean(tower_reg)\n        losses = {}\n        losses[\'total_loss\'] = total_loss\n        losses[\'total_reg\'] = total_reg\n\n        grads = opt.compute_gradients(total_loss,tf.trainable_variables(),colocate_gradients_with_ops=True)\n        apply_gradient_op = opt.apply_gradients(grads,global_step=global_step)\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            train_op = tf.group(apply_gradient_op)\n\n        save_vars = [var for var in tf.global_variables() if \'Adagrad\' not in var.name and \'global_step\' not in var.name]\n         \n        #saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n        saver = tf.train.Saver(save_vars, max_to_keep=3)\n\n        # Build the summary operation based on the TF collection of Summaries.\n        summary_op = tf.summary.merge_all()\n\n        # Start running operations on the Graph.\n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=True))        \n\n        # Initialize variables\n        sess.run(tf.global_variables_initializer(), feed_dict={phase_train_placeholder:True})\n        sess.run(tf.local_variables_initializer(), feed_dict={phase_train_placeholder:True})\n\n        #sess.run(iterator.initializer)\n        sess.run(softmax_iterator.initializer)\n\n        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n        coord = tf.train.Coordinator()\n        tf.train.start_queue_runners(coord=coord, sess=sess)\n        \n        with sess.as_default():\n            #pdb.set_trace()\n\n            if args.pretrained_model:\n                print(\'Restoring pretrained model: %s\' % args.pretrained_model)\n                saver.restore(sess, os.path.expanduser(args.pretrained_model))\n\n            # Training and validation loop\n            epoch = 0\n            while epoch < args.max_nrof_epochs:\n                step = sess.run(global_step, feed_dict=None)\n                epoch = step // args.epoch_size\n                if debug:\n                    debug_train(args, sess, train_set, epoch, image_batch_gather, enqueue_op,batch_size_placeholder, image_batch_split,image_paths_split,num_per_class_split,\n                            image_paths_placeholder,image_paths_split_placeholder, labels_placeholder, labels_batch, num_per_class_placeholder,num_per_class_split_placeholder,len(gpus))\n                # Train for one epoch\n                train(args, sess, epoch, \n                     learning_rate_placeholder, phase_train_placeholder, global_step, \n                     losses, train_op, summary_op, summary_writer, args.learning_rate_schedule_file)\n\n                # Save variables and the metagraph if it doesn\'t exist already\n                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, step)\n\n    return model_dir\n\ndef train(args, sess, epoch, \n          learning_rate_placeholder, phase_train_placeholder, global_step, \n          loss, train_op, summary_op, summary_writer, learning_rate_schedule_file):\n    batch_number = 0\n    \n    if args.learning_rate>0.0:\n        lr = args.learning_rate\n    else:\n        lr = utils.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\n    while batch_number < args.epoch_size:\n        start_time = time.time()\n        \n        print(\'Running forward pass on sampled images: \', end=\'\')\n        feed_dict = {learning_rate_placeholder: lr, phase_train_placeholder: True}\n        start_time = time.time()\n        total_err, reg_err, _, step = sess.run([loss[\'total_loss\'], loss[\'total_reg\'], train_op, global_step ], feed_dict=feed_dict)\n        duration = time.time() - start_time\n        print(\'Epoch: [%d][%d/%d]\\tTime %.3f\\tTotal Loss %2.3f\\tReg Loss %2.3f, lr %2.5f\' %\n                  (epoch, batch_number+1, args.epoch_size, duration, total_err, reg_err, lr))\n\n        batch_number += 1\n    return step\n \n\ndef save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\n    # Save the model checkpoint\n    print(\'Saving variables\')\n    start_time = time.time()\n    checkpoint_path = os.path.join(model_dir, \'model-%s.ckpt\' % model_name)\n    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\n    save_time_variables = time.time() - start_time\n    print(\'Variables saved in %.2f seconds\' % save_time_variables)\n    metagraph_filename = os.path.join(model_dir, \'model-%s.meta\' % model_name)\n    save_time_metagraph = 0  \n    if not os.path.exists(metagraph_filename):\n        print(\'Saving metagraph\')\n        start_time = time.time()\n        saver.export_meta_graph(metagraph_filename)\n        save_time_metagraph = time.time() - start_time\n        print(\'Metagraph saved in %.2f seconds\' % save_time_metagraph)\n    summary = tf.Summary()\n    #pylint: disable=maybe-no-member\n    summary.value.add(tag=\'time/save_variables\', simple_value=save_time_variables)\n    summary.value.add(tag=\'time/save_metagraph\', simple_value=save_time_metagraph)\n    summary_writer.add_summary(summary, step)\n  \n  \ndef get_learning_rate_from_file(filename, epoch):\n    with open(filename, \'r\') as f:\n        for line in f.readlines():\n            line = line.split(\'#\', 1)[0]\n            if line:\n                par = line.strip().split(\':\')\n                e = int(par[0])\n                lr = float(par[1])\n                if e <= epoch:\n                    learning_rate = lr\n                else:\n                    return learning_rate\n    \n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'--logs_base_dir\', type=str, \n        help=\'Directory where to write event logs.\', default=\'logs/facenet_ms_mp\')\n    parser.add_argument(\'--models_base_dir\', type=str,\n        help=\'Directory where to write trained models and checkpoints.\', default=\'models/facenet_ms_mp\')\n    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=.9)\n    parser.add_argument(\'--pretrained_model\', type=str,\n        help=\'Load a pretrained model before training starts.\')\n    parser.add_argument(\'--loss_type\', type=str,\n        help=\'Which type loss to be used.\',default=\'softmax\')\n    parser.add_argument(\'--network\', type=str,\n        help=\'which network is used to extract feature.\',default=\'resnet50\')\n    parser.add_argument(\'--data_dir\', type=str,\n        help=\'Path to the data directory containing aligned face patches. Multiple directories are separated with colon.\',\n        default=\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\')\n    parser.add_argument(\'--list_file\', type=str,\n        help=\'Image list file\')\n    parser.add_argument(\'--model_def\', type=str,\n        help=\'Model definition. Points to a module containing the definition of the inference graph.\', default=\'models.inception_resnet_v1\')\n    parser.add_argument(\'--max_nrof_epochs\', type=int,\n        help=\'Number of epochs to run.\', default=500)\n    parser.add_argument(\'--batch_size\', type=int,\n        help=\'Number of images to process in a batch.\', default=90)\n    parser.add_argument(\'--image_size\', type=int,\n        help=\'Image size (height, width) in pixels.\', default=160)\n    parser.add_argument(\'--image_src_size\', type=int,\n        help=\'Src Image size (height, width) in pixels.\', default=256)\n    parser.add_argument(\'--image_height\', type=int,\n        help=\'Image size (height, width) in pixels.\', default=112)\n    parser.add_argument(\'--image_width\', type=int,\n        help=\'Image size (height, width) in pixels.\', default=96)\n    parser.add_argument(\'--people_per_batch\', type=int,\n        help=\'Number of people per batch.\', default=30)\n    parser.add_argument(\'--num_gpus\', type=int,\n        help=\'Number of gpus.\', default=4)\n    parser.add_argument(\'--images_per_person\', type=int,\n        help=\'Number of images per person.\', default=5)\n    parser.add_argument(\'--epoch_size\', type=int,\n        help=\'Number of batches per epoch.\', default=600)\n    parser.add_argument(\'--alpha\', type=float,\n        help=\'Margin for cos margin.\', default=0.15)\n    parser.add_argument(\'--scale\', type=float,\n        help=\'Scale as the fixed norm of weight and feature.\', default=64.)\n    parser.add_argument(\'--weight\', type=float,\n        help=\'weiht to balance the dist and th loss.\', default=3.)\n    parser.add_argument(\'--embedding_size\', type=int,\n        help=\'Dimensionality of the embedding.\', default=256)\n    parser.add_argument(\'--random_crop\', \n        help=\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \' +\n         \'If the size of the images in the data directory is equal to image_size no cropping is performed\', action=\'store_true\')\n    parser.add_argument(\'--random_flip\', \n        help=\'Performs random horizontal flipping of training images.\', action=\'store_true\')\n    parser.add_argument(\'--fc_bn\', \n        help=\'Wheater use bn after fc.\', action=\'store_true\')\n    parser.add_argument(\'--keep_probability\', type=float,\n        help=\'Keep probability of dropout for the fully connected layer(s).\', default=1.0)\n    parser.add_argument(\'--weight_decay\', type=float,\n        help=\'L2 weight regularization.\', default=0.0)\n    parser.add_argument(\'--optimizer\', type=str, choices=[\'ADAGRAD\', \'ADADELTA\', \'ADAM\', \'RMSPROP\', \'MOM\',\'SGD\'],\n        help=\'The optimization algorithm to use\', default=\'ADAGRAD\')\n    parser.add_argument(\'--center_loss_factor\', type=float,\n        help=\'Center loss factor.\', default=0.0)\n    parser.add_argument(\'--center_loss_alfa\', type=float,\n        help=\'Center update rate for center loss.\', default=0.95)\n    parser.add_argument(\'--learning_rate\', type=float,\n        help=\'Initial learning rate. If set to a negative value a learning rate \' +\n        \'schedule can be specified in the file ""learning_rate_schedule.txt""\', default=0.1)\n    parser.add_argument(\'--learning_rate_decay_epochs\', type=int,\n        help=\'Number of epochs between learning rate decay.\', default=100)\n    parser.add_argument(\'--learning_rate_decay_factor\', type=float,\n        help=\'Learning rate decay factor.\', default=1.0)\n    parser.add_argument(\'--moving_average_decay\', type=float,\n        help=\'Exponential decay for tracking of training parameters.\', default=0.9999)\n    parser.add_argument(\'--seed\', type=int,\n        help=\'Random seed.\', default=666)\n    parser.add_argument(\'--learning_rate_schedule_file\', type=str,\n        help=\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\', default=\'data/learning_rate_schedule.txt\')\n\n    \n    return parser.parse_args(argv)\n  \n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
