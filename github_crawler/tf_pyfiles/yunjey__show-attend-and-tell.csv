file_path,api_count,code
prepro.py,2,"b'from scipy import ndimage\nfrom collections import Counter\nfrom core.vggnet import Vgg19\nfrom core.utils import *\n\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport hickle\nimport os\nimport json\n\n\ndef _process_caption_data(caption_file, image_dir, max_length):\n    with open(caption_file) as f:\n        caption_data = json.load(f)\n\n    # id_to_filename is a dictionary such as {image_id: filename]} \n    id_to_filename = {image[\'id\']: image[\'file_name\'] for image in caption_data[\'images\']}\n\n    # data is a list of dictionary which contains \'captions\', \'file_name\' and \'image_id\' as key.\n    data = []\n    for annotation in caption_data[\'annotations\']:\n        image_id = annotation[\'image_id\']\n        annotation[\'file_name\'] = os.path.join(image_dir, id_to_filename[image_id])\n        data += [annotation]\n    \n    # convert to pandas dataframe (for later visualization or debugging)\n    caption_data = pd.DataFrame.from_dict(data)\n    del caption_data[\'id\']\n    caption_data.sort_values(by=\'image_id\', inplace=True)\n    caption_data = caption_data.reset_index(drop=True)\n    \n    del_idx = []\n    for i, caption in enumerate(caption_data[\'caption\']):\n        caption = caption.replace(\'.\',\'\').replace(\',\',\'\').replace(""\'"","""").replace(\'""\',\'\')\n        caption = caption.replace(\'&\',\'and\').replace(\'(\',\'\').replace("")"","""").replace(\'-\',\' \')\n        caption = "" "".join(caption.split())  # replace multiple spaces\n        \n        caption_data.set_value(i, \'caption\', caption.lower())\n        if len(caption.split("" "")) > max_length:\n            del_idx.append(i)\n    \n    # delete captions if size is larger than max_length\n    print ""The number of captions before deletion: %d"" %len(caption_data)\n    caption_data = caption_data.drop(caption_data.index[del_idx])\n    caption_data = caption_data.reset_index(drop=True)\n    print ""The number of captions after deletion: %d"" %len(caption_data)\n    return caption_data\n\n\ndef _build_vocab(annotations, threshold=1):\n    counter = Counter()\n    max_len = 0\n    for i, caption in enumerate(annotations[\'caption\']):\n        words = caption.split(\' \') # caption contrains only lower-case words\n        for w in words:\n            counter[w] +=1\n        \n        if len(caption.split("" "")) > max_len:\n            max_len = len(caption.split("" ""))\n\n    vocab = [word for word in counter if counter[word] >= threshold]\n    print (\'Filtered %d words to %d words with word count threshold %d.\' % (len(counter), len(vocab), threshold))\n\n    word_to_idx = {u\'<NULL>\': 0, u\'<START>\': 1, u\'<END>\': 2}\n    idx = 3\n    for word in vocab:\n        word_to_idx[word] = idx\n        idx += 1\n    print ""Max length of caption: "", max_len\n    return word_to_idx\n\n\ndef _build_caption_vector(annotations, word_to_idx, max_length=15):\n    n_examples = len(annotations)\n    captions = np.ndarray((n_examples,max_length+2)).astype(np.int32)   \n\n    for i, caption in enumerate(annotations[\'caption\']):\n        words = caption.split("" "") # caption contrains only lower-case words\n        cap_vec = []\n        cap_vec.append(word_to_idx[\'<START>\'])\n        for word in words:\n            if word in word_to_idx:\n                cap_vec.append(word_to_idx[word])\n        cap_vec.append(word_to_idx[\'<END>\'])\n        \n        # pad short caption with the special null token \'<NULL>\' to make it fixed-size vector\n        if len(cap_vec) < (max_length + 2):\n            for j in range(max_length + 2 - len(cap_vec)):\n                cap_vec.append(word_to_idx[\'<NULL>\']) \n        \n        captions[i, :] = np.asarray(cap_vec)\n    print ""Finished building caption vectors""\n    return captions\n\n\ndef _build_file_names(annotations):\n    image_file_names = []\n    id_to_idx = {}\n    idx = 0\n    image_ids = annotations[\'image_id\']\n    file_names = annotations[\'file_name\']\n    for image_id, file_name in zip(image_ids, file_names):\n        if not image_id in id_to_idx:\n            id_to_idx[image_id] = idx\n            image_file_names.append(file_name)\n            idx += 1\n\n    file_names = np.asarray(image_file_names)\n    return file_names, id_to_idx\n\n\ndef _build_image_idxs(annotations, id_to_idx):\n    image_idxs = np.ndarray(len(annotations), dtype=np.int32)\n    image_ids = annotations[\'image_id\']\n    for i, image_id in enumerate(image_ids):\n        image_idxs[i] = id_to_idx[image_id]\n    return image_idxs\n\n\ndef main():\n    # batch size for extracting feature vectors from vggnet.\n    batch_size = 100\n    # maximum length of caption(number of word). if caption is longer than max_length, deleted.  \n    max_length = 15\n    # if word occurs less than word_count_threshold in training dataset, the word index is special unknown token.\n    word_count_threshold = 1\n    # vgg model path \n    vgg_model_path = \'./data/imagenet-vgg-verydeep-19.mat\'\n\n    caption_file = \'data/annotations/captions_train2014.json\'\n    image_dir = \'image/%2014_resized/\'\n\n    # about 80000 images and 400000 captions for train dataset\n    train_dataset = _process_caption_data(caption_file=\'data/annotations/captions_train2014.json\',\n                                          image_dir=\'image/train2014_resized/\',\n                                          max_length=max_length)\n\n    # about 40000 images and 200000 captions\n    val_dataset = _process_caption_data(caption_file=\'data/annotations/captions_val2014.json\',\n                                        image_dir=\'image/val2014_resized/\',\n                                        max_length=max_length)\n\n    # about 4000 images and 20000 captions for val / test dataset\n    val_cutoff = int(0.1 * len(val_dataset))\n    test_cutoff = int(0.2 * len(val_dataset))\n    print \'Finished processing caption data\'\n\n    save_pickle(train_dataset, \'data/train/train.annotations.pkl\')\n    save_pickle(val_dataset[:val_cutoff], \'data/val/val.annotations.pkl\')\n    save_pickle(val_dataset[val_cutoff:test_cutoff].reset_index(drop=True), \'data/test/test.annotations.pkl\')\n\n    for split in [\'train\', \'val\', \'test\']:\n        annotations = load_pickle(\'./data/%s/%s.annotations.pkl\' % (split, split))\n\n        if split == \'train\':\n            word_to_idx = _build_vocab(annotations=annotations, threshold=word_count_threshold)\n            save_pickle(word_to_idx, \'./data/%s/word_to_idx.pkl\' % split)\n        \n        captions = _build_caption_vector(annotations=annotations, word_to_idx=word_to_idx, max_length=max_length)\n        save_pickle(captions, \'./data/%s/%s.captions.pkl\' % (split, split))\n\n        file_names, id_to_idx = _build_file_names(annotations)\n        save_pickle(file_names, \'./data/%s/%s.file.names.pkl\' % (split, split))\n\n        image_idxs = _build_image_idxs(annotations, id_to_idx)\n        save_pickle(image_idxs, \'./data/%s/%s.image.idxs.pkl\' % (split, split))\n\n        # prepare reference captions to compute bleu scores later\n        image_ids = {}\n        feature_to_captions = {}\n        i = -1\n        for caption, image_id in zip(annotations[\'caption\'], annotations[\'image_id\']):\n            if not image_id in image_ids:\n                image_ids[image_id] = 0\n                i += 1\n                feature_to_captions[i] = []\n            feature_to_captions[i].append(caption.lower() + \' .\')\n        save_pickle(feature_to_captions, \'./data/%s/%s.references.pkl\' % (split, split))\n        print ""Finished building %s caption dataset"" %split\n\n    # extract conv5_3 feature vectors\n    vggnet = Vgg19(vgg_model_path)\n    vggnet.build()\n    with tf.Session() as sess:\n        tf.initialize_all_variables().run()\n        for split in [\'train\', \'val\', \'test\']:\n            anno_path = \'./data/%s/%s.annotations.pkl\' % (split, split)\n            save_path = \'./data/%s/%s.features.hkl\' % (split, split)\n            annotations = load_pickle(anno_path)\n            image_path = list(annotations[\'file_name\'].unique())\n            n_examples = len(image_path)\n\n            all_feats = np.ndarray([n_examples, 196, 512], dtype=np.float32)\n\n            for start, end in zip(range(0, n_examples, batch_size),\n                                  range(batch_size, n_examples + batch_size, batch_size)):\n                image_batch_file = image_path[start:end]\n                image_batch = np.array(map(lambda x: ndimage.imread(x, mode=\'RGB\'), image_batch_file)).astype(\n                    np.float32)\n                feats = sess.run(vggnet.features, feed_dict={vggnet.images: image_batch})\n                all_feats[start:end, :] = feats\n                print (""Processed %d %s features.."" % (end, split))\n\n            # use hickle to save huge feature vectors\n            hickle.dump(all_feats, save_path)\n            print (""Saved %s.."" % (save_path))\n\n\nif __name__ == ""__main__"":\n    main()'"
resize.py,0,"b""from PIL import Image\nimport os\n\n\ndef resize_image(image):\n    width, height = image.size\n    if width > height:\n        left = (width - height) / 2\n        right = width - left\n        top = 0\n        bottom = height\n    else:\n        top = (height - width) / 2\n        bottom = height - top\n        left = 0\n        right = width\n    image = image.crop((left, top, right, bottom))\n    image = image.resize([224, 224], Image.ANTIALIAS)\n    return image\n\ndef main():\n    splits = ['train', 'val']\n    for split in splits:\n        folder = './image/%s2014' %split\n        resized_folder = './image/%s2014_resized/' %split\n        if not os.path.exists(resized_folder):\n            os.makedirs(resized_folder)\n        print 'Start resizing %s images.' %split\n        image_files = os.listdir(folder)\n        num_images = len(image_files)\n        for i, image_file in enumerate(image_files):\n            with open(os.path.join(folder, image_file), 'r+b') as f:\n                with Image.open(f) as image:\n                    image = resize_image(image)\n                    image.save(os.path.join(resized_folder, image_file), image.format)\n            if i % 100 == 0:\n                print 'Resized images: %d/%d' %(i, num_images)\n              \n            \nif __name__ == '__main__':\n    main()"""
train.py,0,"b'from core.solver import CaptioningSolver\nfrom core.model import CaptionGenerator\nfrom core.utils import load_coco_data\n\n\ndef main():\n    # load train dataset\n    data = load_coco_data(data_path=\'./data\', split=\'train\')\n    word_to_idx = data[\'word_to_idx\']\n    # load val dataset to print out bleu scores every epoch\n    val_data = load_coco_data(data_path=\'./data\', split=\'val\')\n\n    model = CaptionGenerator(word_to_idx, dim_feature=[196, 512], dim_embed=512,\n                                       dim_hidden=1024, n_time_step=16, prev2out=True, \n                                                 ctx2out=True, alpha_c=1.0, selector=True, dropout=True)\n\n    solver = CaptioningSolver(model, data, val_data, n_epochs=20, batch_size=128, update_rule=\'adam\',\n                                          learning_rate=0.001, print_every=1000, save_every=1, image_path=\'./image/\',\n                                    pretrained_model=None, model_path=\'model/lstm/\', test_model=\'model/lstm/model-10\',\n                                     print_bleu=True, log_path=\'log/\')\n\n    solver.train()\n\nif __name__ == ""__main__"":\n    main()'"
core/__init__.py,0,b''
core/bleu.py,0,"b'import cPickle as pickle\nimport os\nimport sys\nsys.path.append(\'../coco-caption\')\nfrom pycocoevalcap.bleu.bleu import Bleu\nfrom pycocoevalcap.rouge.rouge import Rouge\nfrom pycocoevalcap.cider.cider import Cider\nfrom pycocoevalcap.meteor.meteor import Meteor\n\ndef score(ref, hypo):\n    scorers = [\n        (Bleu(4),[""Bleu_1"",""Bleu_2"",""Bleu_3"",""Bleu_4""]),\n        (Meteor(),""METEOR""),\n        (Rouge(),""ROUGE_L""),\n        (Cider(),""CIDEr"")\n    ]\n    final_scores = {}\n    for scorer,method in scorers:\n        score,scores = scorer.compute_score(ref,hypo)\n        if type(score)==list:\n            for m,s in zip(method,score):\n                final_scores[m] = s\n        else:\n            final_scores[method] = score\n\n    return final_scores\n    \n\ndef evaluate(data_path=\'./data\', split=\'val\', get_scores=False):\n    reference_path = os.path.join(data_path, ""%s/%s.references.pkl"" %(split, split))\n    candidate_path = os.path.join(data_path, ""%s/%s.candidate.captions.pkl"" %(split, split))\n    \n    # load caption data\n    with open(reference_path, \'rb\') as f:\n        ref = pickle.load(f)\n    with open(candidate_path, \'rb\') as f:\n        cand = pickle.load(f)\n    \n    # make dictionary\n    hypo = {}\n    for i, caption in enumerate(cand):\n        hypo[i] = [caption]\n    \n    # compute bleu score\n    final_scores = score(ref, hypo)\n\n    # print out scores\n    print \'Bleu_1:\\t\',final_scores[\'Bleu_1\']  \n    print \'Bleu_2:\\t\',final_scores[\'Bleu_2\']  \n    print \'Bleu_3:\\t\',final_scores[\'Bleu_3\']  \n    print \'Bleu_4:\\t\',final_scores[\'Bleu_4\']  \n    print \'METEOR:\\t\',final_scores[\'METEOR\']  \n    print \'ROUGE_L:\',final_scores[\'ROUGE_L\']  \n    print \'CIDEr:\\t\',final_scores[\'CIDEr\']\n    \n    if get_scores:\n        return final_scores\n    \n   \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n\n'"
core/model.py,65,"b'# =========================================================================================\n# Implementation of ""Show, Attend and Tell: Neural Caption Generator With Visual Attention"".\n# There are some notations.\n# N is batch size.\n# L is spacial size of feature vector (196).\n# D is dimension of image feature vector (512).\n# T is the number of time step which is equal to caption\'s length-1 (16).\n# V is vocabulary size (about 10000).\n# M is dimension of word vector which is embedding size (default is 512).\n# H is dimension of hidden state (default is 1024).\n# =========================================================================================\n\nfrom __future__ import division\n\nimport tensorflow as tf\n\n\nclass CaptionGenerator(object):\n    def __init__(self, word_to_idx, dim_feature=[196, 512], dim_embed=512, dim_hidden=1024, n_time_step=16,\n                  prev2out=True, ctx2out=True, alpha_c=0.0, selector=True, dropout=True):\n        """"""\n        Args:\n            word_to_idx: word-to-index mapping dictionary.\n            dim_feature: (optional) Dimension of vggnet19 conv5_3 feature vectors.\n            dim_embed: (optional) Dimension of word embedding.\n            dim_hidden: (optional) Dimension of all hidden state.\n            n_time_step: (optional) Time step size of LSTM.\n            prev2out: (optional) previously generated word to hidden state. (see Eq (7) for explanation)\n            ctx2out: (optional) context to hidden state (see Eq (7) for explanation)\n            alpha_c: (optional) Doubly stochastic regularization coefficient. (see Section (4.2.1) for explanation)\n            selector: (optional) gating scalar for context vector. (see Section (4.2.1) for explanation)\n            dropout: (optional) If true then dropout layer is added.\n        """"""\n\n        self.word_to_idx = word_to_idx\n        self.idx_to_word = {i: w for w, i in word_to_idx.iteritems()}\n        self.prev2out = prev2out\n        self.ctx2out = ctx2out\n        self.alpha_c = alpha_c\n        self.selector = selector\n        self.dropout = dropout\n        self.V = len(word_to_idx)\n        self.L = dim_feature[0]\n        self.D = dim_feature[1]\n        self.M = dim_embed\n        self.H = dim_hidden\n        self.T = n_time_step\n        self._start = word_to_idx[\'<START>\']\n        self._null = word_to_idx[\'<NULL>\']\n\n        self.weight_initializer = tf.contrib.layers.xavier_initializer()\n        self.const_initializer = tf.constant_initializer(0.0)\n        self.emb_initializer = tf.random_uniform_initializer(minval=-1.0, maxval=1.0)\n\n        # Place holder for features and captions\n        self.features = tf.placeholder(tf.float32, [None, self.L, self.D])\n        self.captions = tf.placeholder(tf.int32, [None, self.T + 1])\n\n    def _get_initial_lstm(self, features):\n        with tf.variable_scope(\'initial_lstm\'):\n            features_mean = tf.reduce_mean(features, 1)\n\n            w_h = tf.get_variable(\'w_h\', [self.D, self.H], initializer=self.weight_initializer)\n            b_h = tf.get_variable(\'b_h\', [self.H], initializer=self.const_initializer)\n            h = tf.nn.tanh(tf.matmul(features_mean, w_h) + b_h)\n\n            w_c = tf.get_variable(\'w_c\', [self.D, self.H], initializer=self.weight_initializer)\n            b_c = tf.get_variable(\'b_c\', [self.H], initializer=self.const_initializer)\n            c = tf.nn.tanh(tf.matmul(features_mean, w_c) + b_c)\n            return c, h\n\n    def _word_embedding(self, inputs, reuse=False):\n        with tf.variable_scope(\'word_embedding\', reuse=reuse):\n            w = tf.get_variable(\'w\', [self.V, self.M], initializer=self.emb_initializer)\n            x = tf.nn.embedding_lookup(w, inputs, name=\'word_vector\')  # (N, T, M) or (N, M)\n            return x\n\n    def _project_features(self, features):\n        with tf.variable_scope(\'project_features\'):\n            w = tf.get_variable(\'w\', [self.D, self.D], initializer=self.weight_initializer)\n            features_flat = tf.reshape(features, [-1, self.D])\n            features_proj = tf.matmul(features_flat, w)\n            features_proj = tf.reshape(features_proj, [-1, self.L, self.D])\n            return features_proj\n\n    def _attention_layer(self, features, features_proj, h, reuse=False):\n        with tf.variable_scope(\'attention_layer\', reuse=reuse):\n            w = tf.get_variable(\'w\', [self.H, self.D], initializer=self.weight_initializer)\n            b = tf.get_variable(\'b\', [self.D], initializer=self.const_initializer)\n            w_att = tf.get_variable(\'w_att\', [self.D, 1], initializer=self.weight_initializer)\n\n            h_att = tf.nn.relu(features_proj + tf.expand_dims(tf.matmul(h, w), 1) + b)    # (N, L, D)\n            out_att = tf.reshape(tf.matmul(tf.reshape(h_att, [-1, self.D]), w_att), [-1, self.L])   # (N, L)\n            alpha = tf.nn.softmax(out_att)\n            context = tf.reduce_sum(features * tf.expand_dims(alpha, 2), 1, name=\'context\')   #(N, D)\n            return context, alpha\n\n    def _selector(self, context, h, reuse=False):\n        with tf.variable_scope(\'selector\', reuse=reuse):\n            w = tf.get_variable(\'w\', [self.H, 1], initializer=self.weight_initializer)\n            b = tf.get_variable(\'b\', [1], initializer=self.const_initializer)\n            beta = tf.nn.sigmoid(tf.matmul(h, w) + b, \'beta\')    # (N, 1)\n            context = tf.multiply(beta, context, name=\'selected_context\')\n            return context, beta\n\n    def _decode_lstm(self, x, h, context, dropout=False, reuse=False):\n        with tf.variable_scope(\'logits\', reuse=reuse):\n            w_h = tf.get_variable(\'w_h\', [self.H, self.M], initializer=self.weight_initializer)\n            b_h = tf.get_variable(\'b_h\', [self.M], initializer=self.const_initializer)\n            w_out = tf.get_variable(\'w_out\', [self.M, self.V], initializer=self.weight_initializer)\n            b_out = tf.get_variable(\'b_out\', [self.V], initializer=self.const_initializer)\n\n            if dropout:\n                h = tf.nn.dropout(h, 0.5)\n            h_logits = tf.matmul(h, w_h) + b_h\n\n            if self.ctx2out:\n                w_ctx2out = tf.get_variable(\'w_ctx2out\', [self.D, self.M], initializer=self.weight_initializer)\n                h_logits += tf.matmul(context, w_ctx2out)\n\n            if self.prev2out:\n                h_logits += x\n            h_logits = tf.nn.tanh(h_logits)\n\n            if dropout:\n                h_logits = tf.nn.dropout(h_logits, 0.5)\n            out_logits = tf.matmul(h_logits, w_out) + b_out\n            return out_logits\n\n    def _batch_norm(self, x, mode=\'train\', name=None):\n        return tf.contrib.layers.batch_norm(inputs=x,\n                                            decay=0.95,\n                                            center=True,\n                                            scale=True,\n                                            is_training=(mode==\'train\'),\n                                            updates_collections=None,\n                                            scope=(name+\'batch_norm\'))\n\n    def build_model(self):\n        features = self.features\n        captions = self.captions\n        batch_size = tf.shape(features)[0]\n\n        captions_in = captions[:, :self.T]\n        captions_out = captions[:, 1:]\n        mask = tf.to_float(tf.not_equal(captions_out, self._null))\n\n\n        # batch normalize feature vectors\n        features = self._batch_norm(features, mode=\'train\', name=\'conv_features\')\n\n        c, h = self._get_initial_lstm(features=features)\n        x = self._word_embedding(inputs=captions_in)\n        features_proj = self._project_features(features=features)\n\n        loss = 0.0\n        alpha_list = []\n        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=self.H)\n\n        for t in range(self.T):\n            context, alpha = self._attention_layer(features, features_proj, h, reuse=(t!=0))\n            alpha_list.append(alpha)\n\n            if self.selector:\n                context, beta = self._selector(context, h, reuse=(t!=0))\n\n            with tf.variable_scope(\'lstm\', reuse=(t!=0)):\n                _, (c, h) = lstm_cell(inputs=tf.concat( [x[:,t,:], context],1), state=[c, h])\n\n            logits = self._decode_lstm(x[:,t,:], h, context, dropout=self.dropout, reuse=(t!=0))\n\n            loss += tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=captions_out[:, t],logits=logits)*mask[:, t] )\n\n        if self.alpha_c > 0:\n            alphas = tf.transpose(tf.stack(alpha_list), (1, 0, 2))     # (N, T, L)\n            alphas_all = tf.reduce_sum(alphas, 1)      # (N, L)\n            alpha_reg = self.alpha_c * tf.reduce_sum((16./196 - alphas_all) ** 2)\n            loss += alpha_reg\n\n        return loss / tf.to_float(batch_size)\n\n    def build_sampler(self, max_len=20):\n        features = self.features\n\n        # batch normalize feature vectors\n        features = self._batch_norm(features, mode=\'test\', name=\'conv_features\')\n\n        c, h = self._get_initial_lstm(features=features)\n        features_proj = self._project_features(features=features)\n\n        sampled_word_list = []\n        alpha_list = []\n        beta_list = []\n        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=self.H)\n\n        for t in range(max_len):\n            if t == 0:\n                x = self._word_embedding(inputs=tf.fill([tf.shape(features)[0]], self._start))\n            else:\n                x = self._word_embedding(inputs=sampled_word, reuse=True)\n\n            context, alpha = self._attention_layer(features, features_proj, h, reuse=(t!=0))\n            alpha_list.append(alpha)\n\n            if self.selector:\n                context, beta = self._selector(context, h, reuse=(t!=0))\n                beta_list.append(beta)\n\n            with tf.variable_scope(\'lstm\', reuse=(t!=0)):\n                _, (c, h) = lstm_cell(inputs=tf.concat( [x, context],1), state=[c, h])\n\n            logits = self._decode_lstm(x, h, context, reuse=(t!=0))\n            sampled_word = tf.argmax(logits, 1)\n            sampled_word_list.append(sampled_word)\n\n        alphas = tf.transpose(tf.stack(alpha_list), (1, 0, 2))     # (N, T, L)\n        betas = tf.transpose(tf.squeeze(beta_list), (1, 0))    # (N, T)\n        sampled_captions = tf.transpose(tf.stack(sampled_word_list), (1, 0))     # (N, max_len)\n        return alphas, betas, sampled_captions\n'"
core/solver.py,26,"b'import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport skimage.transform\nimport numpy as np\nimport time\nimport os\nimport cPickle as pickle\nfrom scipy import ndimage\nfrom utils import *\nfrom bleu import evaluate\n\n\nclass CaptioningSolver(object):\n    def __init__(self, model, data, val_data, **kwargs):\n        """"""\n        Required Arguments:\n            - model: Show Attend and Tell caption generating model\n            - data: Training data; dictionary with the following keys:\n                - features: Feature vectors of shape (82783, 196, 512)\n                - file_names: Image file names of shape (82783, )\n                - captions: Captions of shape (400000, 17)\n                - image_idxs: Indices for mapping caption to image of shape (400000, )\n                - word_to_idx: Mapping dictionary from word to index\n            - val_data: validation data; for print out BLEU scores for each epoch.\n        Optional Arguments:\n            - n_epochs: The number of epochs to run for training.\n            - batch_size: Mini batch size.\n            - update_rule: A string giving the name of an update rule\n            - learning_rate: Learning rate; default value is 0.01.\n            - print_every: Integer; training losses will be printed every print_every iterations.\n            - save_every: Integer; model variables will be saved every save_every epoch.\n            - pretrained_model: String; pretrained model path\n            - model_path: String; model path for saving\n            - test_model: String; model path for test\n        """"""\n\n        self.model = model\n        self.data = data\n        self.val_data = val_data\n        self.n_epochs = kwargs.pop(\'n_epochs\', 10)\n        self.batch_size = kwargs.pop(\'batch_size\', 100)\n        self.update_rule = kwargs.pop(\'update_rule\', \'adam\')\n        self.learning_rate = kwargs.pop(\'learning_rate\', 0.01)\n        self.print_bleu = kwargs.pop(\'print_bleu\', False)\n        self.print_every = kwargs.pop(\'print_every\', 100)\n        self.save_every = kwargs.pop(\'save_every\', 1)\n        self.log_path = kwargs.pop(\'log_path\', \'./log/\')\n        self.model_path = kwargs.pop(\'model_path\', \'./model/\')\n        self.pretrained_model = kwargs.pop(\'pretrained_model\', None)\n        self.test_model = kwargs.pop(\'test_model\', \'./model/lstm/model-1\')\n\n        # set an optimizer by update rule\n        if self.update_rule == \'adam\':\n            self.optimizer = tf.train.AdamOptimizer\n        elif self.update_rule == \'momentum\':\n            self.optimizer = tf.train.MomentumOptimizer\n        elif self.update_rule == \'rmsprop\':\n            self.optimizer = tf.train.RMSPropOptimizer\n\n        if not os.path.exists(self.model_path):\n            os.makedirs(self.model_path)\n        if not os.path.exists(self.log_path):\n            os.makedirs(self.log_path)\n\n\n    def train(self):\n        # train/val dataset\n        # Changed this because I keep less features than captions, see prepro\n        # n_examples = self.data[\'captions\'].shape[0]\n        n_examples = self.data[\'features\'].shape[0]\n        n_iters_per_epoch = int(np.ceil(float(n_examples)/self.batch_size))\n        features = self.data[\'features\']\n        captions = self.data[\'captions\']\n        image_idxs = self.data[\'image_idxs\']\n        val_features = self.val_data[\'features\']\n        n_iters_val = int(np.ceil(float(val_features.shape[0])/self.batch_size))\n\n        # build graphs for training model and sampling captions\n        # This scope fixed things!!\n        with tf.variable_scope(tf.get_variable_scope()):\n            loss = self.model.build_model()\n            tf.get_variable_scope().reuse_variables()\n            _, _, generated_captions = self.model.build_sampler(max_len=20)\n\n        # train op\n        with tf.variable_scope(tf.get_variable_scope(), reuse=False):\n            optimizer = self.optimizer(learning_rate=self.learning_rate)\n            grads = tf.gradients(loss, tf.trainable_variables())\n            grads_and_vars = list(zip(grads, tf.trainable_variables()))\n            train_op = optimizer.apply_gradients(grads_and_vars=grads_and_vars)\n\n        # summary op\n        # tf.scalar_summary(\'batch_loss\', loss)\n        tf.summary.scalar(\'batch_loss\', loss)\n        for var in tf.trainable_variables():\n            #tf.histogram_summary(var.op.name, var)\n            tf.summary.histogram(var.op.name, var)\n        for grad, var in grads_and_vars:\n            #tf.histogram_summary(var.op.name+\'/gradient\', grad)\n            tf.summary.histogram(var.op.name+\'/gradient\', grad)\n\n        #summary_op = tf.merge_all_summaries()\n        summary_op = tf.summary.merge_all()\n\n        print ""The number of epoch: %d"" %self.n_epochs\n        print ""Data size: %d"" %n_examples\n        print ""Batch size: %d"" %self.batch_size\n        print ""Iterations per epoch: %d"" %n_iters_per_epoch\n\n        config = tf.ConfigProto(allow_soft_placement = True)\n        #config.gpu_options.per_process_gpu_memory_fraction=0.9\n        config.gpu_options.allow_growth = True\n        with tf.Session(config=config) as sess:\n            tf.global_variables_initializer().run()\n            #summary_writer = tf.train.SummaryWriter(self.log_path, graph=tf.get_default_graph())\n            summary_writer = tf.summary.FileWriter(self.log_path, graph=tf.get_default_graph())\n            saver = tf.train.Saver(max_to_keep=40)\n\n            if self.pretrained_model is not None:\n                print ""Start training with pretrained Model..""\n                saver.restore(sess, self.pretrained_model)\n\n            prev_loss = -1\n            curr_loss = 0\n            start_t = time.time()\n\n            for e in range(self.n_epochs):\n                rand_idxs = np.random.permutation(n_examples)\n                captions = captions[rand_idxs]\n                image_idxs = image_idxs[rand_idxs]\n\n                for i in range(n_iters_per_epoch):\n                    captions_batch = captions[i*self.batch_size:(i+1)*self.batch_size]\n                    image_idxs_batch = image_idxs[i*self.batch_size:(i+1)*self.batch_size]\n                    features_batch = features[image_idxs_batch]\n                    feed_dict = {self.model.features: features_batch, self.model.captions: captions_batch}\n                    _, l = sess.run([train_op, loss], feed_dict)\n                    curr_loss += l\n\n                    # write summary for tensorboard visualization\n                    if i % 10 == 0:\n                        summary = sess.run(summary_op, feed_dict)\n                        summary_writer.add_summary(summary, e*n_iters_per_epoch + i)\n\n                    if (i+1) % self.print_every == 0:\n                        print ""\\nTrain loss at epoch %d & iteration %d (mini-batch): %.5f"" %(e+1, i+1, l)\n                        ground_truths = captions[image_idxs == image_idxs_batch[0]]\n                        decoded = decode_captions(ground_truths, self.model.idx_to_word)\n                        for j, gt in enumerate(decoded):\n                            print ""Ground truth %d: %s"" %(j+1, gt)\n                        gen_caps = sess.run(generated_captions, feed_dict)\n                        decoded = decode_captions(gen_caps, self.model.idx_to_word)\n                        print ""Generated caption: %s\\n"" %decoded[0]\n\n                print ""Previous epoch loss: "", prev_loss\n                print ""Current epoch loss: "", curr_loss\n                print ""Elapsed time: "", time.time() - start_t\n                prev_loss = curr_loss\n                curr_loss = 0\n\n                # print out BLEU scores and file write\n                if self.print_bleu:\n                    all_gen_cap = np.ndarray((val_features.shape[0], 20))\n                    for i in range(n_iters_val):\n                        features_batch = val_features[i*self.batch_size:(i+1)*self.batch_size]\n                        feed_dict = {self.model.features: features_batch}\n                        gen_cap = sess.run(generated_captions, feed_dict=feed_dict)\n                        all_gen_cap[i*self.batch_size:(i+1)*self.batch_size] = gen_cap\n\n                    all_decoded = decode_captions(all_gen_cap, self.model.idx_to_word)\n                    save_pickle(all_decoded, ""./data/val/val.candidate.captions.pkl"")\n                    scores = evaluate(data_path=\'./data\', split=\'val\', get_scores=True)\n                    write_bleu(scores=scores, path=self.model_path, epoch=e)\n\n                # save model\'s parameters\n                if (e+1) % self.save_every == 0:\n                    saver.save(sess, os.path.join(self.model_path, \'model\'), global_step=e+1)\n                    print ""model-%s saved."" %(e+1)\n\n\n    def test(self, data, split=\'train\', attention_visualization=True, save_sampled_captions=True):\n        \'\'\'\n        Args:\n            - data: dictionary with the following keys:\n            - features: Feature vectors of shape (5000, 196, 512)\n            - file_names: Image file names of shape (5000, )\n            - captions: Captions of shape (24210, 17)\n            - image_idxs: Indices for mapping caption to image of shape (24210, )\n            - features_to_captions: Mapping feature to captions (5000, 4~5)\n            - split: \'train\', \'val\' or \'test\'\n            - attention_visualization: If True, visualize attention weights with images for each sampled word. (ipthon notebook)\n            - save_sampled_captions: If True, save sampled captions to pkl file for computing BLEU scores.\n        \'\'\'\n\n        features = data[\'features\']\n\n        # build a graph to sample captions\n        alphas, betas, sampled_captions = self.model.build_sampler(max_len=20)    # (N, max_len, L), (N, max_len)\n\n        config = tf.ConfigProto(allow_soft_placement=True)\n        config.gpu_options.allow_growth = True\n        with tf.Session(config=config) as sess:\n            saver = tf.train.Saver()\n            saver.restore(sess, self.test_model)\n            features_batch, image_files = sample_coco_minibatch(data, self.batch_size)\n            feed_dict = { self.model.features: features_batch }\n            alps, bts, sam_cap = sess.run([alphas, betas, sampled_captions], feed_dict)  # (N, max_len, L), (N, max_len)\n            decoded = decode_captions(sam_cap, self.model.idx_to_word)\n\n            if attention_visualization:\n                for n in range(10):\n                    print ""Sampled Caption: %s"" %decoded[n]\n\n                    # Plot original image\n                    img = ndimage.imread(image_files[n])\n                    plt.subplot(4, 5, 1)\n                    plt.imshow(img)\n                    plt.axis(\'off\')\n\n                    # Plot images with attention weights\n                    words = decoded[n].split("" "")\n                    for t in range(len(words)):\n                        if t > 18:\n                            break\n                        plt.subplot(4, 5, t+2)\n                        plt.text(0, 1, \'%s(%.2f)\'%(words[t], bts[n,t]) , color=\'black\', backgroundcolor=\'white\', fontsize=8)\n                        plt.imshow(img)\n                        alp_curr = alps[n,t,:].reshape(14,14)\n                        alp_img = skimage.transform.pyramid_expand(alp_curr, upscale=16, sigma=20)\n                        plt.imshow(alp_img, alpha=0.85)\n                        plt.axis(\'off\')\n                    plt.show()\n\n            if save_sampled_captions:\n                all_sam_cap = np.ndarray((features.shape[0], 20))\n                num_iter = int(np.ceil(float(features.shape[0]) / self.batch_size))\n                for i in range(num_iter):\n                    features_batch = features[i*self.batch_size:(i+1)*self.batch_size]\n                    feed_dict = { self.model.features: features_batch }\n                    all_sam_cap[i*self.batch_size:(i+1)*self.batch_size] = sess.run(sampled_captions, feed_dict)\n                all_decoded = decode_captions(all_sam_cap, self.model.idx_to_word)\n                save_pickle(all_decoded, ""./data/%s/%s.candidate.captions.pkl"" %(split,split))\n'"
core/utils.py,0,"b'import numpy as np\nimport cPickle as pickle\nimport hickle\nimport time\nimport os\n\n\ndef load_coco_data(data_path=\'./data\', split=\'train\'):\n    data_path = os.path.join(data_path, split)\n    start_t = time.time()\n    data = {}\n  \n    data[\'features\'] = hickle.load(os.path.join(data_path, \'%s.features.hkl\' %split))\n    with open(os.path.join(data_path, \'%s.file.names.pkl\' %split), \'rb\') as f:\n        data[\'file_names\'] = pickle.load(f)   \n    with open(os.path.join(data_path, \'%s.captions.pkl\' %split), \'rb\') as f:\n        data[\'captions\'] = pickle.load(f)\n    with open(os.path.join(data_path, \'%s.image.idxs.pkl\' %split), \'rb\') as f:\n        data[\'image_idxs\'] = pickle.load(f)\n            \n    if split == \'train\':       \n        with open(os.path.join(data_path, \'word_to_idx.pkl\'), \'rb\') as f:\n            data[\'word_to_idx\'] = pickle.load(f)\n          \n    for k, v in data.iteritems():\n        if type(v) == np.ndarray:\n            print k, type(v), v.shape, v.dtype\n        else:\n            print k, type(v), len(v)\n    end_t = time.time()\n    print ""Elapse time: %.2f"" %(end_t - start_t)\n    return data\n\ndef decode_captions(captions, idx_to_word):\n    if captions.ndim == 1:\n        T = captions.shape[0]\n        N = 1\n    else:\n        N, T = captions.shape\n\n    decoded = []\n    for i in range(N):\n        words = []\n        for t in range(T):\n            if captions.ndim == 1:\n                word = idx_to_word[captions[t]]\n            else:\n                word = idx_to_word[captions[i, t]]\n            if word == \'<END>\':\n                words.append(\'.\')\n                break\n            if word != \'<NULL>\':\n                words.append(word)\n        decoded.append(\' \'.join(words))\n    return decoded\n\ndef sample_coco_minibatch(data, batch_size):\n    data_size = data[\'features\'].shape[0]\n    mask = np.random.choice(data_size, batch_size)\n    features = data[\'features\'][mask]\n    file_names = data[\'file_names\'][mask]\n    return features, file_names\n\ndef write_bleu(scores, path, epoch):\n    if epoch == 0:\n        file_mode = \'w\'\n    else:\n        file_mode = \'a\'\n    with open(os.path.join(path, \'val.bleu.scores.txt\'), file_mode) as f:\n        f.write(\'Epoch %d\\n\' %(epoch+1))\n        f.write(\'Bleu_1: %f\\n\' %scores[\'Bleu_1\'])\n        f.write(\'Bleu_2: %f\\n\' %scores[\'Bleu_2\'])\n        f.write(\'Bleu_3: %f\\n\' %scores[\'Bleu_3\'])  \n        f.write(\'Bleu_4: %f\\n\' %scores[\'Bleu_4\']) \n        f.write(\'METEOR: %f\\n\' %scores[\'METEOR\'])  \n        f.write(\'ROUGE_L: %f\\n\' %scores[\'ROUGE_L\'])  \n        f.write(\'CIDEr: %f\\n\\n\' %scores[\'CIDEr\'])\n\ndef load_pickle(path):\n    with open(path, \'rb\') as f:\n        file = pickle.load(f)\n        print (\'Loaded %s..\' %path)\n        return file  \n\ndef save_pickle(data, path):\n    with open(path, \'wb\') as f:\n        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n        print (\'Saved %s..\' %path)'"
core/vggnet.py,8,"b""import tensorflow as tf\nimport scipy.io\n\n\nvgg_layers = ['conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n              'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n              'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3', 'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n              'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3', 'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n              'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3', 'relu5_3', 'conv5_4', 'relu5_4']\n\nclass Vgg19(object):\n    def __init__(self, vgg_path):\n        self.vgg_path = vgg_path\n\n    def build_inputs(self):\n        self.images = tf.placeholder(tf.float32, [None, 224, 224, 3], 'images')\n\n    def build_params(self):\n        model = scipy.io.loadmat(self.vgg_path)\n        layers = model['layers'][0]\n        self.params = {}\n        with tf.variable_scope('encoder'):\n            for i, layer in enumerate(layers):\n                layer_name = layer[0][0][0][0]\n                layer_type = layer[0][0][1][0]\n                if layer_type == 'conv':\n                    w = layer[0][0][2][0][0].transpose(1, 0, 2, 3)\n                    b = layer[0][0][2][0][1].reshape(-1)\n                    self.params[layer_name] = {}\n                    self.params[layer_name]['w'] = tf.get_variable(layer_name+'/w', initializer=tf.constant(w))\n                    self.params[layer_name]['b'] = tf.get_variable(layer_name+'/b',initializer=tf.constant(b))\n\n    def _conv(self, x, w, b):\n        return tf.nn.bias_add(tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME'), b)\n\n    def _relu(self, x):\n        return tf.nn.relu(x)\n\n    def _pool(self, x):\n        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n\n    def build_model(self):\n        for i, layer in enumerate(vgg_layers):\n            layer_type = layer[:4]\n            if layer_type == 'conv':\n                if layer == 'conv1_1':\n                    h = self.images\n                h = self._conv(h, self.params[layer]['w'], self.params[layer]['b'])\n            elif layer_type == 'relu':\n                h = self._relu(h)\n            elif layer_type == 'pool':\n                h = self._pool(h)\n            if layer == 'conv5_3':\n                self.features = tf.reshape(h, [-1, 196, 512])\n            \n\n    def build(self):\n        self.build_inputs()\n        self.build_params()\n        self.build_model()"""
