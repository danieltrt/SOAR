file_path,api_count,code
HyperLPRLite.py,0,"b'#coding=utf-8\nimport cv2\nimport numpy as np\nfrom keras import backend as K\nfrom keras.models import *\nfrom keras.layers import *\n\nchars = [u""\xe4\xba\xac"", u""\xe6\xb2\xaa"", u""\xe6\xb4\xa5"", u""\xe6\xb8\x9d"", u""\xe5\x86\x80"", u""\xe6\x99\x8b"", u""\xe8\x92\x99"", u""\xe8\xbe\xbd"", u""\xe5\x90\x89"", u""\xe9\xbb\x91"", u""\xe8\x8b\x8f"", u""\xe6\xb5\x99"", u""\xe7\x9a\x96"", u""\xe9\x97\xbd"", u""\xe8\xb5\xa3"", u""\xe9\xb2\x81"", u""\xe8\xb1\xab"", u""\xe9\x84\x82"", u""\xe6\xb9\x98"", u""\xe7\xb2\xa4"", u""\xe6\xa1\x82"",\n             u""\xe7\x90\xbc"", u""\xe5\xb7\x9d"", u""\xe8\xb4\xb5"", u""\xe4\xba\x91"", u""\xe8\x97\x8f"", u""\xe9\x99\x95"", u""\xe7\x94\x98"", u""\xe9\x9d\x92"", u""\xe5\xae\x81"", u""\xe6\x96\xb0"", u""0"", u""1"", u""2"", u""3"", u""4"", u""5"", u""6"", u""7"", u""8"", u""9"", u""A"",\n             u""B"", u""C"", u""D"", u""E"", u""F"", u""G"", u""H"", u""J"", u""K"", u""L"", u""M"", u""N"", u""P"", u""Q"", u""R"", u""S"", u""T"", u""U"", u""V"", u""W"", u""X"",\n             u""Y"", u""Z"",u""\xe6\xb8\xaf"",u""\xe5\xad\xa6"",u""\xe4\xbd\xbf"",u""\xe8\xad\xa6"",u""\xe6\xbe\xb3"",u""\xe6\x8c\x82"",u""\xe5\x86\x9b"",u""\xe5\x8c\x97"",u""\xe5\x8d\x97"",u""\xe5\xb9\xbf"",u""\xe6\xb2\x88"",u""\xe5\x85\xb0"",u""\xe6\x88\x90"",u""\xe6\xb5\x8e"",u""\xe6\xb5\xb7"",u""\xe6\xb0\x91"",u""\xe8\x88\xaa"",u""\xe7\xa9\xba""\n             ]\n\nclass LPR():\n    def __init__(self,model_detection,model_finemapping,model_seq_rec):\n        self.watch_cascade = cv2.CascadeClassifier(model_detection)\n        self.modelFineMapping = self.model_finemapping()\n        self.modelFineMapping.load_weights(model_finemapping)\n        self.modelSeqRec = self.model_seq_rec(model_seq_rec)\n\n    def computeSafeRegion(self,shape,bounding_rect):\n        top = bounding_rect[1] # y\n        bottom  = bounding_rect[1] + bounding_rect[3] # y +  h\n        left = bounding_rect[0] # x\n        right =   bounding_rect[0] + bounding_rect[2] # x +  w\n        min_top = 0\n        max_bottom = shape[0]\n        min_left = 0\n        max_right = shape[1]\n        if top < min_top:\n            top = min_top\n        if left < min_left:\n            left = min_left\n        if bottom > max_bottom:\n            bottom = max_bottom\n        if right > max_right:\n            right = max_right\n        return [left,top,right-left,bottom-top]\n\n    def cropImage(self,image,rect):\n        x, y, w, h = self.computeSafeRegion(image.shape,rect)\n        return image[y:y+h,x:x+w]\n\n    def detectPlateRough(self,image_gray,resize_h = 720,en_scale =1.08 ,top_bottom_padding_rate = 0.05):\n        if top_bottom_padding_rate>0.2:\n            print(""error:top_bottom_padding_rate > 0.2:"",top_bottom_padding_rate)\n            exit(1)\n        height = image_gray.shape[0]\n        padding =    int(height*top_bottom_padding_rate)\n        scale = image_gray.shape[1]/float(image_gray.shape[0])\n        image = cv2.resize(image_gray, (int(scale*resize_h), resize_h))\n        image_color_cropped = image[padding:resize_h-padding,0:image_gray.shape[1]]\n        image_gray = cv2.cvtColor(image_color_cropped,cv2.COLOR_RGB2GRAY)\n        watches = self.watch_cascade.detectMultiScale(image_gray, en_scale, 2, minSize=(36, 9),maxSize=(36*40, 9*40))\n        cropped_images = []\n        for (x, y, w, h) in watches:\n            x -= w * 0.14\n            w += w * 0.28\n            y -= h * 0.15\n            h += h * 0.3\n            cropped = self.cropImage(image_color_cropped, (int(x), int(y), int(w), int(h)))\n            cropped_images.append([cropped,[x, y+padding, w, h]])\n        return cropped_images\n\n    def fastdecode(self,y_pred):\n        results = """"\n        confidence = 0.0\n        table_pred = y_pred.reshape(-1, len(chars)+1)\n        res = table_pred.argmax(axis=1)\n        for i,one in enumerate(res):\n            if one<len(chars) and (i==0 or (one!=res[i-1])):\n                results+= chars[one]\n                confidence+=table_pred[i][one]\n        confidence/= len(results)\n        return results,confidence\n\n    def model_seq_rec(self,model_path):\n        width, height, n_len, n_class = 164, 48, 7, len(chars)+ 1\n        rnn_size = 256\n        input_tensor = Input((164, 48, 3))\n        x = input_tensor\n        base_conv = 32\n        for i in range(3):\n            x = Conv2D(base_conv * (2 ** (i)), (3, 3))(x)\n            x = BatchNormalization()(x)\n            x = Activation(\'relu\')(x)\n            x = MaxPooling2D(pool_size=(2, 2))(x)\n        conv_shape = x.get_shape()\n        x = Reshape(target_shape=(int(conv_shape[1]), int(conv_shape[2] * conv_shape[3])))(x)\n        x = Dense(32)(x)\n        x = BatchNormalization()(x)\n        x = Activation(\'relu\')(x)\n        gru_1 = GRU(rnn_size, return_sequences=True, kernel_initializer=\'he_normal\', name=\'gru1\')(x)\n        gru_1b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer=\'he_normal\', name=\'gru1_b\')(x)\n        gru1_merged = add([gru_1, gru_1b])\n        gru_2 = GRU(rnn_size, return_sequences=True, kernel_initializer=\'he_normal\', name=\'gru2\')(gru1_merged)\n        gru_2b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer=\'he_normal\', name=\'gru2_b\')(gru1_merged)\n        x = concatenate([gru_2, gru_2b])\n        x = Dropout(0.25)(x)\n        x = Dense(n_class, kernel_initializer=\'he_normal\', activation=\'softmax\')(x)\n        base_model = Model(inputs=input_tensor, outputs=x)\n        base_model.load_weights(model_path)\n        return base_model\n\n    def model_finemapping(self):\n        input = Input(shape=[16, 66, 3])  # change this shape to [None,None,3] to enable arbitraty shape input\n        x = Conv2D(10, (3, 3), strides=1, padding=\'valid\', name=\'conv1\')(input)\n        x = Activation(""relu"", name=\'relu1\')(x)\n        x = MaxPool2D(pool_size=2)(x)\n        x = Conv2D(16, (3, 3), strides=1, padding=\'valid\', name=\'conv2\')(x)\n        x = Activation(""relu"", name=\'relu2\')(x)\n        x = Conv2D(32, (3, 3), strides=1, padding=\'valid\', name=\'conv3\')(x)\n        x = Activation(""relu"", name=\'relu3\')(x)\n        x = Flatten()(x)\n        output = Dense(2,name = ""dense"")(x)\n        output = Activation(""relu"", name=\'relu4\')(output)\n        model = Model([input], [output])\n        return model\n\n    def finemappingVertical(self,image,rect):\n        resized = cv2.resize(image,(66,16))\n        resized = resized.astype(np.float)/255\n        res_raw= self.modelFineMapping.predict(np.array([resized]))[0]\n        res  =res_raw*image.shape[1]\n        res = res.astype(np.int)\n        H,T = res\n        H-=3\n        if H<0:\n            H=0\n        T+=2;\n        if T>= image.shape[1]-1:\n            T= image.shape[1]-1\n        rect[2] -=  rect[2]*(1-res_raw[1] + res_raw[0])\n        rect[0]+=res[0]\n        image = image[:,H:T+2]\n        image = cv2.resize(image, (int(136), int(36)))\n        return image,rect\n\n    def recognizeOne(self,src):\n        x_tempx = src\n        x_temp = cv2.resize(x_tempx,( 164,48))\n        x_temp = x_temp.transpose(1, 0, 2)\n        y_pred = self.modelSeqRec.predict(np.array([x_temp]))\n        y_pred = y_pred[:,2:,:]\n        return self.fastdecode(y_pred)\n\n    def SimpleRecognizePlateByE2E(self,image):\n        images = self.detectPlateRough(image,image.shape[0],top_bottom_padding_rate=0.1)\n        res_set = []\n        for j,plate in enumerate(images):\n            plate, rect  =plate\n            image_rgb,rect_refine = self.finemappingVertical(plate,rect)\n            res,confidence = self.recognizeOne(image_rgb)\n            res_set.append([res,confidence,rect_refine])\n        return res_set\n'"
HyperLprGUI.py,0,"b'""""""\nAuthor: youngorsu\nEmail : zhiyongsu@qq.com\nLast edited: 2018.1.29\n""""""\n# coding=utf-8\n\n\nimport sys\nimport os\nfrom PyQt5.QtWidgets import (\n    QMainWindow,\n    QLabel,\n    QLineEdit,\n    QPushButton,\n    QHBoxLayout,\n    QVBoxLayout,\n    QGridLayout,\n    QTableWidget,\n    QWidget,\n    QAbstractItemView,\n    QHeaderView,\n    QGraphicsView,\n    QGraphicsScene,\n    QGraphicsPixmapItem,\n    QSplitter,\n    QFileDialog,\n    QTableWidgetItem,\n    QGraphicsRectItem,\n    QCheckBox,\n    QMessageBox,\n    QGroupBox,\n    QGraphicsSimpleTextItem,\n    qApp,\n    QAction,\n    QApplication)\nfrom PyQt5.QtGui import QIcon, QColor, QPainter, QImage, QPixmap, QPen, QBrush, QFont, QPalette, QKeySequence\nfrom PyQt5.QtCore import Qt, QDir, QSize, QEventLoop, QThread, pyqtSignal\n\nfrom hyperlpr_py3 import pipline as pp\n\nimport cv2\n\nimport numpy as np\n\nimport time\n\nimport shutil\n\ndraw_plate_in_image_enable = 1\n\nplateTypeName = [""\xe8\x93\x9d"", ""\xe9\xbb\x84"", ""\xe7\xbb\xbf"", ""\xe7\x99\xbd"", ""\xe9\xbb\x91 ""]\n\n\ndef SimpleRecognizePlateWithGui(image):\n    t0 = time.time()\n\n    images = pp.detect.detectPlateRough(\n        image, image.shape[0], top_bottom_padding_rate=0.1)\n\n    res_set = []\n    y_offset = 32\n    for j, plate in enumerate(images):\n        plate, rect, origin_plate = plate\n\n        plate = cv2.resize(plate, (136, 36 * 2))\n        t1 = time.time()\n\n        plate_type = pp.td.SimplePredict(plate)\n        plate_color = plateTypeName[plate_type]\n\n        if (plate_type > 0) and (plate_type < 5):\n            plate = cv2.bitwise_not(plate)\n\n        if draw_plate_in_image_enable == 1:\n            image[y_offset:y_offset + plate.shape[0], 0:plate.shape[1]] = plate\n            y_offset = y_offset + plate.shape[0] + 4\n\n        image_rgb = pp.fm.findContoursAndDrawBoundingBox(plate)\n\n        if draw_plate_in_image_enable == 1:\n            image[y_offset:y_offset + image_rgb.shape[0],\n                  0:image_rgb.shape[1]] = image_rgb\n            y_offset = y_offset + image_rgb.shape[0] + 4\n\n        image_rgb = pp.fv.finemappingVertical(image_rgb)\n\n        if draw_plate_in_image_enable == 1:\n            image[y_offset:y_offset + image_rgb.shape[0],\n                  0:image_rgb.shape[1]] = image_rgb\n            y_offset = y_offset + image_rgb.shape[0] + 4\n\n        pp.cache.verticalMappingToFolder(image_rgb)\n\n        if draw_plate_in_image_enable == 1:\n            image[y_offset:y_offset + image_rgb.shape[0],\n                  0:image_rgb.shape[1]] = image_rgb\n            y_offset = y_offset + image_rgb.shape[0] + 4\n\n        e2e_plate, e2e_confidence = pp.e2e.recognizeOne(image_rgb)\n        print(""e2e:"", e2e_plate, e2e_confidence)\n\n        image_gray = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY)\n\n        #print(""\xe6\xa0\xa1\xe6\xad\xa3"", time.time() - t1, ""s"")\n\n        t2 = time.time()\n        val = pp.segmentation.slidingWindowsEval(image_gray)\n        # print val\n        #print(""\xe5\x88\x86\xe5\x89\xb2\xe5\x92\x8c\xe8\xaf\x86\xe5\x88\xab"", time.time() - t2, ""s"")\n\n        res=""""\n        confidence = 0\n        if len(val) == 3:\n            blocks, res, confidence = val\n            if confidence / 7 > 0.7:\n\n                if draw_plate_in_image_enable == 1:\n                    image = pp.drawRectBox(image, rect, res)\n                    for i, block in enumerate(blocks):\n                        block_ = cv2.resize(block, (24, 24))\n                        block_ = cv2.cvtColor(block_, cv2.COLOR_GRAY2BGR)\n                        image[j * 24:(j * 24) + 24, i *\n                              24:(i * 24) + 24] = block_\n                        if image[j * 24:(j * 24) + 24,\n                                 i * 24:(i * 24) + 24].shape == block_.shape:\n                            pass\n\n        res_set.append([res,\n                        confidence / 7,\n                        rect,\n                        plate_color,\n                        e2e_plate,\n                        e2e_confidence,\n                        len(blocks)])\n        print(""seg:"",res,confidence/7)\n    #print(time.time() - t0, ""s"")\n\n    print(""---------------------------------"")\n    return image, res_set\n\n\nclass LicenseRecognizationThread(QThread):\n\n    recognization_done_signal = pyqtSignal(list)\n\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.hyperlpr_dir_path = """"\n        self.filenames = []\n\n    def set_parameter(self, filename_list, path):\n        self.hyperlpr_dir_path = path\n        self.filenames = filename_list\n\n    def run(self):\n        while True:\n            time.sleep(1)\n            if len(self.hyperlpr_dir_path) > 0:\n                for i in range(0, len(self.filenames)):\n                    path = os.path.join(\n                        self.hyperlpr_dir_path, self.filenames[i])\n                    image = cv2.imdecode(np.fromfile(path, dtype=np.uint8), -1)\n                    image, res_set = SimpleRecognizePlateWithGui(image)\n                    self.recognization_done_signal.emit([i, res_set])\n\n                self.hyperlpr_dir_path = """"\n\n\nclass HyperLprImageView(QGraphicsView):\n\n    def __init__(self):\n\n        super().__init__()\n\n        self.init_ui()\n\n    def init_ui(self):\n\n        scene = QGraphicsScene()\n        scene.setBackgroundBrush(QColor(100, 100, 100))\n        scene.setItemIndexMethod(QGraphicsScene.BspTreeIndex)\n\n        scene.setSceneRect(scene.itemsBoundingRect())\n\n        self.setDragMode(QGraphicsView.RubberBandDrag)\n        self.setViewportUpdateMode(QGraphicsView.FullViewportUpdate)\n        self.setRenderHints(QPainter.Antialiasing | QPainter.TextAntialiasing)\n\n        self.frame_item = QGraphicsPixmapItem()\n\n        self.text_item_offset = 0\n        self.rect_item_array = []\n        self.text_item_array = []\n        for i in range(0, 5):\n            rect_item = QGraphicsRectItem()\n            rect_item.setVisible(False)\n            rect_item.setZValue(20.0)\n            rect_item.setPen(QPen(Qt.red, 5))\n            rect_item.setRect(20, 20, 20, 20)\n            scene.addItem(rect_item)\n            self.rect_item_array.append(rect_item)\n            text_item = QGraphicsSimpleTextItem("""")\n            text_item.setBrush(QBrush(Qt.red))\n            text_item.setZValue(20.0)\n            text_item.setPos(10, 50)\n            text_item.setFont(QFont(""\xe9\xbb\x91\xe4\xbd\x93"", 24))\n            text_item.setVisible(False)\n            scene.addItem(text_item)\n            self.text_item_array.append(text_item)\n\n        scene.addItem(self.frame_item)\n\n        self.curr_factor = 1.0\n\n        self.setScene(scene)\n\n    def resetRectText(self, res_set):\n        max_no = len(res_set)\n\n        if max_no > 5:\n            max_no = 5\n\n        for i in range(0, 5):\n            if i < max_no:\n                curr_rect = res_set[i][2]\n                self.rect_item_array[i].setRect(int(curr_rect[0]), int(\n                    curr_rect[1]), int(curr_rect[2]), int(curr_rect[3]))\n                self.rect_item_array[i].setVisible(True)\n\n                self.text_item_array[i].setText(\n                    res_set[i][4] + "" "" + res_set[i][3])\n                self.text_item_array[i].setPos(\n                    int(curr_rect[0]), int(curr_rect[1]) - 48)\n                self.text_item_array[i].setVisible(True)\n            else:\n                self.text_item_array[i].setVisible(False)\n                self.rect_item_array[i].setVisible(False)\n\n    def wheelEvent(self, event):\n        factor = event.angleDelta().y() / 120.0\n        if event.angleDelta().y() / 120.0 > 0:\n            factor = 1.08\n        else:\n            factor = 0.92\n\n        if self.curr_factor > 0.1 and self.curr_factor < 10:\n            self.curr_factor = self.curr_factor * factor\n            self.scale(factor, factor)\n\n    def resetPixmap(self, image):\n\n        self.frame_item.setPixmap(QPixmap.fromImage(image))\n\n\nclass HyperLprWindow(QMainWindow):\n\n    start_init_signal = pyqtSignal()\n\n    def __init__(self):\n\n        super().__init__()\n\n        self.initUI()\n\n    def initUI(self):\n\n        self.statusBar().showMessage(\'Ready\')\n\n        self.left_action = QAction(\'\xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaa\', self)\n        self.left_action.setShortcut(QKeySequence.MoveToPreviousChar)\n        self.left_action.triggered.connect(self.analyze_last_one_image)\n\n        self.right_action = QAction(\'\xe4\xb8\x8b\xe4\xb8\x80\xe4\xb8\xaa\', self)\n        self.right_action.setShortcut(QKeySequence.MoveToNextChar)\n        self.right_action.triggered.connect(self.analyze_next_one_image)\n\n        self.rename_image_action = QAction(\'\xe4\xbf\x9d\xe5\xad\x98e2e\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\', self)\n        self.rename_image_action.setShortcut(QKeySequence.MoveToPreviousLine)\n        self.rename_image_action.triggered.connect(self.rename_current_image_with_info)\n\n        self.statusBar()\n\n        menubar = self.menuBar()\n        fileMenu = menubar.addMenu(\'&Function\')\n        fileMenu.addAction(self.left_action)\n        fileMenu.addAction(self.right_action)\n        fileMenu.addAction(self.rename_image_action)\n\n        self.image_window_view = HyperLprImageView()\n\n        table_widget_header_labels = [\n            ""\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d"",\n            ""\xe5\x88\x86\xe5\x89\xb2\xe8\xaf\x86\xe5\x88\xab"",\n            ""\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6"",\n            ""\xe9\xa2\x9c\xe8\x89\xb2"",\n            ""E2E\xe8\xaf\x86\xe5\x88\xab"",\n            ""E2E\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6""]\n\n        self.hyperlpr_tableview = QTableWidget(\n            0, len(table_widget_header_labels))\n        self.hyperlpr_tableview.setHorizontalHeaderLabels(\n            table_widget_header_labels)\n\n        self.hyperlpr_tableview.setSelectionBehavior(\n            QAbstractItemView.SelectItems)\n        self.hyperlpr_tableview.setSelectionMode(\n            QAbstractItemView.SingleSelection)\n        self.hyperlpr_tableview.setEditTriggers(\n            QAbstractItemView.NoEditTriggers)\n        self.hyperlpr_tableview.horizontalHeader().setSectionResizeMode(\n            QHeaderView.ResizeToContents)\n        self.hyperlpr_tableview.setEditTriggers(\n            QAbstractItemView.NoEditTriggers)\n\n        self.hyperlpr_tableview.cellClicked.connect(\n            self.recognize_one_license_plate)\n\n        self.left_button = QPushButton(""<"")\n        self.left_button.setFixedWidth(60)\n        self.right_button = QPushButton("">"")\n        self.right_button.setFixedWidth(60)\n        self.left_button.setEnabled(False)\n        self.right_button.setEnabled(False)\n        self.left_button.clicked.connect(self.analyze_last_one_image)\n        self.right_button.clicked.connect(self.analyze_next_one_image)\n        left_right_layout = QHBoxLayout()\n        left_right_layout.addStretch()\n        left_right_layout.addWidget(self.left_button)\n        left_right_layout.addStretch()\n        left_right_layout.addWidget(self.right_button)\n        left_right_layout.addStretch()\n\n        self.location_label = QLabel(""\xe8\xbd\xa6\xe7\x89\x8c\xe7\x9b\xae\xe5\xbd\x95"", self)\n        self.location_text = QLineEdit(self)\n        self.location_text.setEnabled(False)\n        #self.location_text.setFixedWidth(300)\n        self.location_button = QPushButton(""..."")\n        self.location_button.clicked.connect(self.select_new_dir)\n\n        self.location_layout = QHBoxLayout()\n        self.location_layout.addWidget(self.location_label)\n        self.location_layout.addWidget(self.location_text)\n        self.location_layout.addWidget(self.location_button)\n        self.location_layout.addStretch()\n\n        self.check_box = QCheckBox(""\xe4\xb8\x8e\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe6\xaf\x94\xe8\xbe\x83\xe8\xbd\xa6\xe7\x89\x8c"")\n        self.check_box.setChecked(True)\n\n        self.update_file_path_button = QPushButton(\'\xe6\x89\xb9\xe9\x87\x8f\xe8\xaf\x86\xe5\x88\xab\')\n        self.update_file_path_button.clicked.connect(\n            self.batch_recognize_all_images)\n\n        self.update_file_path_layout = QHBoxLayout()\n        self.update_file_path_layout.addWidget(self.check_box)\n        self.update_file_path_layout.addWidget(self.update_file_path_button)\n        self.update_file_path_layout.addStretch()\n\n        self.save_as_e2e_filename_button = QPushButton(""\xe4\xbf\x9d\xe5\xad\x98e2e\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d"")\n        self.save_as_e2e_filename_button.setEnabled(False)\n        self.save_as_e2e_filename_button.clicked.connect(self.rename_current_image_with_info)\n        self.save_layout = QHBoxLayout()\n        self.save_layout.addWidget(self.save_as_e2e_filename_button)\n        self.save_layout.addStretch()\n\n        self.top_layout = QVBoxLayout()\n        self.top_layout.addLayout(left_right_layout)\n        self.top_layout.addLayout(self.location_layout)\n        self.top_layout.addLayout(self.update_file_path_layout)\n        self.top_layout.addLayout(self.save_layout)\n\n        function_groupbox = QGroupBox(""\xe5\x8a\x9f\xe8\x83\xbd\xe5\x8c\xba"")\n        function_groupbox.setLayout(self.top_layout)\n\n        license_plate_image_label = QLabel(""\xe8\xbd\xa6\xe7\x89\x8c\xe5\x9b\xbe"")\n        self.license_plate_widget = QLabel("""")\n\n        block_image_label = QLabel(""\xe5\x88\x86\xe5\x89\xb2\xe5\x9b\xbe"")\n        self.block_plate_widget = QLabel("""")\n\n        filename_label = QLabel(""\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xef\xbc\x9a"")\n        self.filename_edit = QLineEdit()\n\n        segmentation_recognition_label = QLabel(""\xe5\x88\x86\xe5\x89\xb2\xe8\xaf\x86\xe5\x88\xab\xef\xbc\x9a"")\n        self.segmentation_recognition_edit = QLineEdit()\n        self.segmentation_recognition_edit.setFont(QFont(""\xe9\xbb\x91\xe4\xbd\x93"", 24, QFont.Bold))\n        # self.segmentation_recognition_edit.setStyleSheet(""color:red"")\n\n        confidence_label = QLabel(""\xe5\x88\x86\xe5\x89\xb2\xe8\xaf\x86\xe5\x88\xab\\n\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6"")\n        self.confidence_edit = QLineEdit()\n        #self.confidence_edit.setFont(QFont(""\xe9\xbb\x91\xe4\xbd\x93"", 24, QFont.Bold))\n        # self.confidence_edit.setStyleSheet(""color:red"")\n\n        plate_color_label = QLabel(""\xe8\xbd\xa6\xe7\x89\x8c\xe9\xa2\x9c\xe8\x89\xb2"")\n        self.plate_color_edit = QLineEdit()\n        self.plate_color_edit.setFont(QFont(""\xe9\xbb\x91\xe4\xbd\x93"", 24, QFont.Bold))\n        # self.plate_color_edit.setStyleSheet(""color:red"")\n\n        e2e_recognization_label = QLabel(""e2e\xe8\xaf\x86\xe5\x88\xab\xef\xbc\x9a"")\n        self.e2e_recognization_edit = QLineEdit()\n        self.e2e_recognization_edit.setFont(QFont(""\xe9\xbb\x91\xe4\xbd\x93"", 24, QFont.Bold))\n        # self.e2e_recognization_edit.setStyleSheet(""color:red"")\n\n        e2e_confidence_label = QLabel(""e2e\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6"")\n        self.e2e_confidence_edit = QLineEdit()\n        #self.e2e_confidence_edit.setFont(QFont(""\xe9\xbb\x91\xe4\xbd\x93"", 24, QFont.Bold))\n        # self.e2e_confidence_edit.setStyleSheet(""color:red"")\n\n        info_gridlayout = QGridLayout()\n        line_index = 0\n        info_gridlayout.addWidget(filename_label, line_index, 0)\n        info_gridlayout.addWidget(self.filename_edit, line_index, 1)\n        line_index += 1\n        info_gridlayout.addWidget(license_plate_image_label, line_index, 0)\n        info_gridlayout.addWidget(self.license_plate_widget, line_index, 1)\n        line_index += 1\n        info_gridlayout.addWidget(e2e_recognization_label, line_index, 0)\n        info_gridlayout.addWidget(self.e2e_recognization_edit, line_index, 1)\n        line_index += 1\n        info_gridlayout.addWidget(\n            segmentation_recognition_label, line_index, 0)\n        info_gridlayout.addWidget(\n            self.segmentation_recognition_edit, line_index, 1)\n        line_index += 1\n        info_gridlayout.addWidget(plate_color_label, line_index, 0)\n        info_gridlayout.addWidget(self.plate_color_edit, line_index, 1)\n        line_index += 1\n        info_gridlayout.addWidget(block_image_label, line_index, 0)\n        info_gridlayout.addWidget(self.block_plate_widget, line_index, 1)\n        line_index += 1\n        info_gridlayout.addWidget(confidence_label, line_index, 0)\n        info_gridlayout.addWidget(self.confidence_edit, line_index, 1)\n        line_index += 1\n        info_gridlayout.addWidget(e2e_confidence_label, line_index, 0)\n        info_gridlayout.addWidget(self.e2e_confidence_edit, line_index, 1)\n\n        info_widget = QGroupBox(""\xe5\x88\x86\xe5\x89\xb2\xe8\xaf\x86\xe5\x88\xab&e2e"")\n\n        info_widget.setLayout(info_gridlayout)\n\n        right_splitter = QSplitter(Qt.Vertical)\n        right_splitter.addWidget(self.hyperlpr_tableview)\n        right_splitter.addWidget(function_groupbox)\n        right_splitter.addWidget(info_widget)\n        right_splitter.setStretchFactor(0, 2)\n        right_splitter.setStretchFactor(2, 1)\n\n        main_splitter = QSplitter(Qt.Horizontal)\n        main_splitter.addWidget(self.image_window_view)\n        main_splitter.addWidget(right_splitter)\n        main_splitter.setStretchFactor(0, 1)\n\n        self.image_filename_list = []\n        self.hyperlpr_dir_path = """"\n        self.segmentation_recognition_correct_number = 0\n        self.color_correct_number = 0\n        self.e2e_recognization_correct_number = 0\n        self.current_row = 0\n\n        self.batch_recognization_thread = LicenseRecognizationThread()\n        self.batch_recognization_thread.recognization_done_signal.connect(\n            self.recognization_done_slot)\n        self.batch_recognization_thread.start()\n\n        self.start_init_signal.connect(self.read_path_and_show_one_image)\n\n        self.setCentralWidget(main_splitter)\n\n        self.setWindowTitle(""HyperLPR\xe8\xbd\xa6\xe7\x89\x8c\xe8\xaf\x86\xe5\x88\xab\xe8\xbd\xaf\xe4\xbb\xb6v1.0"")\n\n        self.start_init_signal.emit()\n\n    def read_path_and_show_one_image(self):\n\n        hyperlpr_dir_info_filepath = QDir.homePath() + ""/hyperlpr_dir_file""\n        if os.path.exists(hyperlpr_dir_info_filepath):\n            with open(hyperlpr_dir_info_filepath, \'r\') as f:\n                self.hyperlpr_dir_path = f.read()\n\n        if len(self.hyperlpr_dir_path) > 0:\n            self.reset_info_gui()\n\n        if len(self.image_filename_list) > 0:\n            self.recognize_and_show_one_image(self.image_filename_list[0], 0)\n\n    def select_new_dir(self):\n\n        self.hyperlpr_dir_path = QFileDialog.getExistingDirectory(\n            self, ""\xe8\xaf\xbb\xe5\x8f\x96\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9"", QDir.currentPath())\n\n        if len(self.hyperlpr_dir_path) > 0:\n            hyperlpr_dir_info_filepath = QDir.homePath() + ""/hyperlpr_dir_file""\n            with open(hyperlpr_dir_info_filepath, \'w\') as f:\n                f.write(self.hyperlpr_dir_path)\n            self.reset_info_gui()\n\n    def rename_current_image_with_info(self):\n        if len(self.hyperlpr_dir_path) > 0:\n            target_dir_path = self.hyperlpr_dir_path + ""/result""\n            if not os.path.exists(target_dir_path):\n                os.makedirs(target_dir_path)\n            if len(self.plate_color_edit.text())>0 and len(self.e2e_recognization_edit.text())>0:\n                orign_path = os.path.join(self.hyperlpr_dir_path, self.filename_edit.text())\n                target_path = os.path.join(target_dir_path,self.plate_color_edit.text()+""-""+self.e2e_recognization_edit.text()+"".jpg"")\n                shutil.copyfile(orign_path, target_path)\n\n    def reset_info_gui(self):\n\n        self.location_text.setText(self.hyperlpr_dir_path)\n        self.scan_files_with_new_dir(self.hyperlpr_dir_path)\n        self.fill_table_with_new_info()\n\n    def scan_files_with_new_dir(self, path):\n\n        name_list = os.listdir(path)  # \xe5\x88\x97\xe5\x87\xba\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe4\xb8\x8b\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe7\x9b\xae\xe5\xbd\x95\xe4\xb8\x8e\xe6\x96\x87\xe4\xbb\xb6\n        self.image_filename_list.clear()\n        for i in range(0, len(name_list)):\n            if name_list[i].endswith(\n                    "".jpg"") or name_list[i].endswith("".png""):\n                self.image_filename_list.append(name_list[i])\n\n    def fill_table_with_new_info(self):\n        self.hyperlpr_tableview.clearContents()\n        row_count = self.hyperlpr_tableview.rowCount()\n        for i in range(row_count, -1, -1):\n            self.hyperlpr_tableview.removeRow(i)\n\n        for i in range(0, len(self.image_filename_list)):\n            row = self.hyperlpr_tableview.rowCount()\n            self.hyperlpr_tableview.insertRow(row)\n\n            item0 = QTableWidgetItem()\n            item0.setTextAlignment(Qt.AlignCenter)\n            self.hyperlpr_tableview.setItem(row, 0, item0)\n            self.hyperlpr_tableview.item(\n                row, 0).setText(\n                self.image_filename_list[i])\n\n            item1 = QTableWidgetItem()\n            item1.setTextAlignment(Qt.AlignCenter)\n            self.hyperlpr_tableview.setItem(row, 1, item1)\n\n            item2 = QTableWidgetItem()\n            item2.setTextAlignment(Qt.AlignCenter)\n            self.hyperlpr_tableview.setItem(row, 2, item2)\n\n            item3 = QTableWidgetItem()\n            item3.setTextAlignment(Qt.AlignCenter)\n            self.hyperlpr_tableview.setItem(row, 3, item3)\n\n            item4 = QTableWidgetItem()\n            item4.setTextAlignment(Qt.AlignCenter)\n            self.hyperlpr_tableview.setItem(row, 4, item4)\n\n            item5 = QTableWidgetItem()\n            item5.setTextAlignment(Qt.AlignCenter)\n            self.hyperlpr_tableview.setItem(row, 5, item5)\n\n        if len(self.image_filename_list) > 0:\n            self.left_button.setEnabled(True)\n            self.right_button.setEnabled(True)\n            self.save_as_e2e_filename_button.setEnabled(True)\n\n    def analyze_last_one_image(self):\n        if self.current_row > 0:\n            self.recognize_one_license_plate(self.current_row-1, 0)\n\n    def analyze_next_one_image(self):\n        if self.current_row < (len(self.image_filename_list)-1):\n            self.recognize_one_license_plate(self.current_row + 1, 0)\n\n    def recognize_one_license_plate(self, row, col):\n        if col == 0 and row < len(self.image_filename_list):\n            self.current_row = row\n            self.recognize_and_show_one_image(\n                self.image_filename_list[row], row)\n\n    def recognize_and_show_one_image(self, image_filename_text, row):\n\n        if image_filename_text.endswith("".jpg""):\n\n            print(image_filename_text)\n            path = os.path.join(self.hyperlpr_dir_path, image_filename_text)\n            image = cv2.imdecode(np.fromfile(path, dtype=np.uint8), -1)\n            image, res_set = SimpleRecognizePlateWithGui(image)\n            img = QImage(\n                image.data,\n                image.shape[1],\n                image.shape[0],\n                image.shape[1] * image.shape[2],\n                QImage.Format_RGB888)\n            self.image_window_view.resetPixmap(img.rgbSwapped())\n            self.image_window_view.resetRectText(res_set)\n\n            if len(res_set) > 0:\n                curr_rect = res_set[0][2]\n                image_crop = image[int(curr_rect[1]):int(\n                    curr_rect[1] + curr_rect[3]), int(curr_rect[0]):int(curr_rect[0] + curr_rect[2])]\n                curr_plate = cv2.resize(image_crop, (204, 108))\n                plate_img = QImage(\n                    curr_plate.data,\n                    curr_plate.shape[1],\n                    curr_plate.shape[0],\n                    curr_plate.shape[1] *\n                    curr_plate.shape[2],\n                    QImage.Format_RGB888)\n                self.license_plate_widget.setPixmap(\n                    QPixmap.fromImage(plate_img.rgbSwapped()))\n\n                # print(res_set[0][6])\n                block_crop = image[0:24, 0:(24 * int(res_set[0][6]))]\n                curr_block = cv2.resize(\n                    block_crop, (24 * int(res_set[0][6]), 24))\n                block_image = QImage(\n                    curr_block.data,\n                    curr_block.shape[1],\n                    curr_block.shape[0],\n                    curr_block.shape[1] *\n                    curr_block.shape[2],\n                    QImage.Format_RGB888)\n                self.block_plate_widget.setPixmap(\n                    QPixmap.fromImage(block_image.rgbSwapped()))\n\n                self.segmentation_recognition_edit.setText(res_set[0][0])\n                if res_set[0][0] in image_filename_text:\n                    self.segmentation_recognition_edit.setStyleSheet(""color:black"")\n                else:\n                    self.segmentation_recognition_edit.setStyleSheet(""color:red"")\n\n\n                self.filename_edit.setText(image_filename_text)\n                self.confidence_edit.setText(""%.3f"" % (float(res_set[0][1])))\n\n                self.plate_color_edit.setText(res_set[0][3])\n                if res_set[0][3] in image_filename_text:\n                    self.plate_color_edit.setStyleSheet(""color:black"")\n                else:\n                    self.plate_color_edit.setStyleSheet(""color:red"")\n\n                self.e2e_recognization_edit.setText(res_set[0][4])\n                if res_set[0][4] in image_filename_text:\n                    self.e2e_recognization_edit.setStyleSheet(""color:black"")\n                else:\n                    self.e2e_recognization_edit.setStyleSheet(""color:red"")\n\n                self.e2e_confidence_edit.setText(\n                    ""%.3f"" % (float(res_set[0][5])))\n            else:\n                self.license_plate_widget.clear()\n                self.block_plate_widget.clear()\n                self.segmentation_recognition_edit.setText("""")\n                self.filename_edit.setText(image_filename_text)\n                self.confidence_edit.setText("""")\n                self.plate_color_edit.setText("""")\n                self.e2e_recognization_edit.setText("""")\n                self.e2e_confidence_edit.setText("""")\n\n            self.fill_table_widget_with_res_info(res_set, row)\n\n    def batch_recognize_all_images(self):\n        self.segmentation_recognition_correct_number = 0\n        self.color_correct_number = 0\n        self.e2e_recognization_correct_number = 0\n        self.batch_recognization_thread.set_parameter(\n            self.image_filename_list, self.hyperlpr_dir_path)\n\n    def recognization_done_slot(self, result_list):\n        row = result_list[0]\n        res_set = result_list[1]\n        self.fill_table_widget_with_res_info(res_set, row)\n\n        if row == len(self.image_filename_list) - 1:\n            total_number = len(self.image_filename_list)\n\n            row_count = self.hyperlpr_tableview.rowCount()\n            if row_count > total_number:\n                self.hyperlpr_tableview.removeRow(total_number)\n\n            self.hyperlpr_tableview.insertRow(total_number)\n\n            item0 = QTableWidgetItem()\n            item0.setTextAlignment(Qt.AlignCenter)\n            self.hyperlpr_tableview.setItem(total_number, 0, item0)\n            self.hyperlpr_tableview.item(\n                total_number, 0).setText(\n                ""\xe7\xbb\x9f\xe8\xae\xa1\xe7\xbb\x93\xe6\x9e\x9c"")\n\n            item1 = QTableWidgetItem()\n            item1.setTextAlignment(Qt.AlignCenter)\n            self.hyperlpr_tableview.setItem(total_number, 1, item1)\n            self.hyperlpr_tableview.item(\n                total_number,\n                1).setText(\n                ""{0} / {1} = {2: .3f}"".format(\n                    self.segmentation_recognition_correct_number,\n                    total_number,\n                    self.segmentation_recognition_correct_number /\n                    total_number))\n\n            item2 = QTableWidgetItem()\n            item2.setTextAlignment(Qt.AlignCenter)\n            self.hyperlpr_tableview.setItem(total_number, 2, item2)\n\n            item3 = QTableWidgetItem()\n            item3.setTextAlignment(Qt.AlignCenter)\n            self.hyperlpr_tableview.setItem(total_number, 3, item3)\n            self.hyperlpr_tableview.item(\n                total_number, 3).setText(\n                ""{0} / {1} = {2: .3f}"".format(self.e2e_recognization_correct_number, total_number,\n                                              self.e2e_recognization_correct_number / total_number))\n\n            item4 = QTableWidgetItem()\n            item4.setTextAlignment(Qt.AlignCenter)\n            self.hyperlpr_tableview.setItem(total_number, 4, item4)\n            self.hyperlpr_tableview.item(\n                total_number, 4).setText(\n                ""{0} / {1} = {2: .3f}"".format(self.color_correct_number, total_number,\n                                              self.color_correct_number / total_number))\n\n            item5 = QTableWidgetItem()\n            item5.setTextAlignment(Qt.AlignCenter)\n            self.hyperlpr_tableview.setItem(total_number, 5, item5)\n\n    def fill_table_widget_with_res_info(self, res_set, row):\n        image_filename_text = self.image_filename_list[row]\n        if len(res_set) > 0:\n\n            self.hyperlpr_tableview.item(row, 1).setText(res_set[0][0])\n            if res_set[0][0] in image_filename_text:\n                self.hyperlpr_tableview.item(\n                    row, 1).setForeground(\n                    QBrush(\n                        QColor(\n                            0, 0, 255)))\n                self.segmentation_recognition_correct_number += 1\n            else:\n                self.hyperlpr_tableview.item(\n                    row, 1).setForeground(\n                    QBrush(\n                        QColor(\n                            255, 0, 0)))\n\n            self.hyperlpr_tableview.item(\n                row, 2).setText(\n                ""%.3f"" %\n                (float(\n                    res_set[0][1])))\n\n            self.hyperlpr_tableview.item(row, 3).setText(res_set[0][3])\n            if res_set[0][3] in image_filename_text:\n                self.hyperlpr_tableview.item(\n                    row, 3).setForeground(\n                    QBrush(\n                        QColor(\n                            0, 0, 255)))\n                self.color_correct_number += 1\n            else:\n                self.hyperlpr_tableview.item(\n                    row, 3).setForeground(\n                    QBrush(\n                        QColor(\n                            255, 0, 0)))\n\n            self.hyperlpr_tableview.item(row, 4).setText(res_set[0][4])\n            if res_set[0][4] in image_filename_text:\n                self.hyperlpr_tableview.item(\n                    row, 4).setForeground(\n                    QBrush(\n                        QColor(\n                            0, 0, 255)))\n                self.e2e_recognization_correct_number += 1\n            else:\n                self.hyperlpr_tableview.item(\n                    row, 4).setForeground(\n                    QBrush(\n                        QColor(\n                            255, 0, 0)))\n\n            self.hyperlpr_tableview.item(\n                row, 5).setText(\n                ""%.3f"" %\n                (float(\n                    res_set[0][5])))\n\n\nif __name__ == \'__main__\':\n\n    app = QApplication(sys.argv)\n\n    hyper_lpr_widow = HyperLprWindow()\n\n    hyper_lpr_widow.showMaximized()\n\n    sys.exit(app.exec_())\n'"
WebAPI.py,0,"b'#coding=utf-8\nfrom flask import Flask, render_template, request\nfrom werkzeug.utils import secure_filename\n\nimport cv2\nimport numpy as np\n\n#\xe5\xaf\xbc\xe5\x85\xa5opencv\n\nfrom hyperlpr_py3 import pipline\n#\xe5\xaf\xbc\xe5\x85\xa5\xe8\xbd\xa6\xe7\x89\x8c\xe8\xaf\x86\xe5\x88\xab\xe5\xba\x93\n\n\napp = Flask(__name__)\n#\xe8\xae\xbe\xe7\xbd\xaeApp name\n\n\ndef recognize(filename):\n    image = cv2.imread(filename)\n    #\xe9\x80\x9a\xe8\xbf\x87\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe8\xaf\xbb\xe5\x85\xa5\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87 \xe6\x94\xbe\xe5\x88\xb0 image\xe4\xb8\xad\n    return pipline.RecognizePlateJson(image)\n    #\xe8\xaf\x86\xe5\x88\xab\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe5\xb9\xb6\xe8\xbf\x94\xe5\x9b\x9ejson\xe7\xbb\x93\xe6\x9e\x9c\n\n#\xe8\xaf\x86\xe5\x88\xab\xe5\x87\xbd\xe6\x95\xb0\n\nimport base64\n\n\ndef recognizeBase64(base64_code):\n    file_bytes = np.asarray(bytearray(base64.b64decode(base64_code)),dtype=np.uint8)\n    image_data_ndarray = cv2.imdecode(file_bytes,1)\n    return pipline.RecognizePlateJson(image_data_ndarray)\n\n\nimport time\n\n@app.route(\'/uploader\', methods=[\'GET\', \'POST\'])#\xe8\xae\xbe\xe7\xbd\xae\xe8\xaf\xb7\xe6\xb1\x82\xe8\xb7\xaf\xe7\x94\xb1\ndef upload_file():\n    if request.method == \'POST\':\n        #\xe5\xa6\x82\xe6\x9e\x9c\xe8\xaf\xb7\xe6\xb1\x82\xe6\x96\xb9\xe6\xb3\x95\xe6\x98\xafPOST\n        f = request.files[\'file\']\n        f.save(""./images_rec/""+secure_filename(f.filename))\n        #\xe4\xbf\x9d\xe5\xad\x98\xe8\xaf\xb7\xe6\xb1\x82\xe4\xb8\x8a\xe6\x9d\xa5\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6\n        t0 = time.time()\n        res = recognize(""./images_rec/""+secure_filename(f.filename))\n        print(""\xe8\xaf\x86\xe5\x88\xab\xe6\x97\xb6\xe9\x97\xb4"",time.time() - t0)\n        return res\n        #\xe8\xbf\x94\xe5\x9b\x9e\xe8\xaf\x86\xe5\x88\xab\xe7\xbb\x93\xe6\x9e\x9c\n\n        # return \'file uploaded successfully\'\n    return render_template(\'upload.html\')\n\n\nif __name__ == \'__main__\':\n    #\xe5\x85\xa5\xe5\x8f\xa3\xe5\x87\xbd\xe6\x95\xb0\n    app.run(""0.0.0.0"", port=8000, threaded=False, debug=False)\n    #\xe8\xbf\x90\xe8\xa1\x8capp \xe6\x8c\x87\xe5\xae\x9aIP \xe6\x8c\x87\xe5\xae\x9a\xe7\xab\xaf\xe5\x8f\xa3\n\n'"
demo.py,0,"b'import sys\nreload(sys)\nsys.setdefaultencoding(""utf-8"")\n\n\n\n\nimport time\n\ndef SpeedTest(image_path):\n    grr = cv2.imread(image_path)\n    model = pr.LPR(""model/cascade.xml"", ""model/model12.h5"", ""model/ocr_plate_all_gru.h5"")\n    model.SimpleRecognizePlateByE2E(grr)\n    t0 = time.time()\n    for x in range(20):\n        model.SimpleRecognizePlateByE2E(grr)\n    t = (time.time() - t0)/20.0\n    print ""Image size :"" + str(grr.shape[1])+""x""+str(grr.shape[0]) +  "" need "" + str(round(t*1000,2))+""ms""\n\n    \n\nfrom PIL import ImageFont\nfrom PIL import Image\nfrom PIL import ImageDraw\nfontC = ImageFont.truetype(""./Font/platech.ttf"", 14, 0)\n\ndef drawRectBox(image,rect,addText):\n    cv2.rectangle(image, (int(rect[0]), int(rect[1])), (int(rect[0] + rect[2]), int(rect[1] + rect[3])), (0,0, 255), 2,cv2.LINE_AA)\n    cv2.rectangle(image, (int(rect[0]-1), int(rect[1])-16), (int(rect[0] + 115), int(rect[1])), (0, 0, 255), -1,\n                  cv2.LINE_AA)\n    img = Image.fromarray(image)\n    draw = ImageDraw.Draw(img)\n    draw.text((int(rect[0]+1), int(rect[1]-16)), addText.decode(""utf-8""), (255, 255, 255), font=fontC)\n    imagex = np.array(img)\n    return imagex\n\n\n\n\n\nimport HyperLPRLite as pr\nimport cv2\nimport numpy as np\ngrr = cv2.imread(""images_rec/2.jpg"")\nmodel = pr.LPR(""model/cascade.xml"",""model/model12.h5"",""model/ocr_plate_all_gru.h5"")\nfor pstr,confidence,rect in model.SimpleRecognizePlateByE2E(grr):\n        if confidence>0.7:\n            image = drawRectBox(grr, rect, pstr+"" ""+str(round(confidence,3)))\n            print ""plate_str:""\n            print pstr\n            print ""plate_confidence""\n            print confidence\n            \ncv2.imshow(""image"",image)\ncv2.waitKey(0)\n\n\n\n#SpeedTest(""images_rec/2.jpg"")\n'"
wxpy_uploader.py,0,"b'#coding=utf-8\n\nfrom wxpy import *\nimport numpy\nimport cv2\nimport time\nimport os\nfrom hyperlpr import pipline\n\ndef recognize(filename):\n    image = cv2.imread(filename)\n    return pipline.RecognizePlateJson(image)\n\nbot = Bot(console_qr=True, cache_path=True)\n\n@bot.register(Friend,PICTURE)\ndef pr_msg(msg):\n    image_name = msg.file_name\n    friend = msg.chat\n    print msg.chat\n    print \'\xe6\x8e\xa5\xe6\x94\xb6\xe5\x9b\xbe\xe7\x89\x87\'\n    # face(image_name)\n    msg.get_file(\'\' + msg.file_name)\n    json_text = recognize(image_name)\n    msg.reply(json_text)\n    msg.reply_image(""0.jpg"")\n    os.remove(image_name)\nembed()\n'"
hyperlpr/__init__.py,0,b''
hyperlpr/cache.py,0,"b'import cv2\nimport os\nimport hashlib\n\ndef verticalMappingToFolder(image):\n    name = hashlib.md5(image.data).hexdigest()[:8]\n    print name\n\n    cv2.imwrite(""./cache/finemapping/""+name+"".png"",image)\n\n\n'"
hyperlpr/colourDetection.py,0,"b'# -- coding: UTF-8\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport os\n\nboundaries = [\n    ([100,80,0],[240,220,110]), # yellow\n    ([0,40,50],[110,180,250]), # blue\n    ([0,60,0],[60,160,70]), # green\n]\ncolor_attr = [""\xe9\xbb\x84\xe7\x89\x8c"",""\xe8\x93\x9d\xe7\x89\x8c"",\'\xe7\xbb\xbf\xe7\x89\x8c\',\'\xe7\x99\xbd\xe7\x89\x8c\',\'\xe9\xbb\x91\xe7\x89\x8c\']\n\nthrehold_green = 13\nthrehold_blue = 13\nthrehold_yellow1 = 50\nthrehold_yellow2 = 70\n\n# plt.figure()\n# plt.axis(""off"")\n# plt.imshow(image)\n# plt.show()\n\nimport numpy as np\ndef centroid_histogram(clt):\n    numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)\n    (hist, _) = np.histogram(clt.labels_, bins=numLabels)\n\n    # normalize the histogram, such that it sums to one\n    hist = hist.astype(""float"")\n    hist /= hist.sum()\n\n    # return the histogram\n    return hist\n\n\ndef plot_colors(hist, centroids):\n    bar = np.zeros((50, 300, 3), dtype=""uint8"")\n    startX = 0\n\n    for (percent, color) in zip(hist, centroids):\n\n        endX = startX + (percent * 300)\n        cv2.rectangle(bar, (int(startX), 0), (int(endX), 50),\n                      color.astype(""uint8"").tolist(), -1)\n        startX = endX\n\n    # return the bar chart\n    return bar\n\ndef search_boundaries(color):\n    for i,color_bound in enumerate(boundaries):\n        if np.all(color >= color_bound[0]) and np.all(color <= color_bound[1]):\n            return i\n    return -1\n\ndef judge_color(color):\n    r = color[0]\n    g = color[1]\n    b = color[2]\n    if g - r >= threhold_green and g - b >= threhold_green:\n        return 2\n    if b - r >= threhold_blue and b - g >= threhold_blue:\n        return 1\n    if r- b > threhold_yellow2 and g - b > threhold_yellow2:\n        return 0\n    if r > 200 and b > 200 and g > 200:\n        return 3\n    if r < 50 and b < 50 and g < 50:\n        return 4\n    return -1\n\ndef judge_plate_color(img):\n    image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    image = image.reshape((image.shape[0] * image.shape[1], 3))\n    clt = KMeans(n_clusters=2)\n    clt.fit(image)\n\n    hist = centroid_histogram(clt)\n    index = np.argmax(hist)\n    #print clt.cluster_centers_[index]\n    #color_index = search_boundaries(clt.cluster_centers_[index])\n    color_index = judge_color(clt.cluster_centers_[index])\n    if color_index == -1:\n        if index == 0:\n            secound_index = 1\n        else:\n            secound_index = 0\n        color_index = judge_color(clt.cluster_centers_[secound_index])\n\n    if color_index == -1:\n        print clt.cluster_centers_\n        bar = plot_colors(hist, clt.cluster_centers_)\n        # show our color bart\n        plt.figure()\n        plt.axis(""off"")\n        plt.imshow(bar)\n        plt.show()\n\n    if color_index != -1:\n        return color_attr[color_index],clt.cluster_centers_[index]\n    else:\n        return None,clt.cluster_centers_[index]'"
hyperlpr/config.py,0,"b'import json\n\n\n\nwith open(""/Users/universe/ProgramUniverse/zeusees/HyperLPR/config.json"") as f:\n    configuration = json.load(f)\n'"
hyperlpr/deskew.py,0,"b'#coding=utf-8\nimport numpy as np\nimport cv2\nimport time;\nfrom matplotlib import pyplot as plt\nimport math\n\nfrom scipy.ndimage import filters\n#\n# def strokeFiter():\n#     pass;\n\ndef angle(x,y):\n    return int(math.atan2(float(y),float(x))*180.0/3.1415);\n\ndef h_rot(src, angle, scale=1.):\n    w = src.shape[1]\n    h = src.shape[0]\n    rangle = np.deg2rad(angle)\n    nw = (abs(np.sin(rangle)*h) + abs(np.cos(rangle)*w))*scale\n    nh = (abs(np.cos(rangle)*h) + abs(np.sin(rangle)*w))*scale\n\n    rot_mat = cv2.getRotationMatrix2D((nw*0.5, nh*0.5), angle, scale)\n\n    rot_move = np.dot(rot_mat, np.array([(nw-w)*0.5, (nh-h)*0.5,0]))\n\n    rot_mat[0,2] += rot_move[0]\n    rot_mat[1,2] += rot_move[1]\n    return cv2.warpAffine(src, rot_mat, (int(math.ceil(nw)), int(math.ceil(nh))), flags=cv2.INTER_LANCZOS4)\n    pass\n\n\ndef v_rot(img,angel,shape,max_angel):\n\n    size_o = [shape[1],shape[0]]\n\n    size = (shape[1]+ int(shape[0]*np.cos((float(max_angel )/180) * 3.14)),shape[0])\n\n\n    interval = abs( int( np.sin((float(angel) /180) * 3.14)* shape[0]));\n\n    pts1 = np.float32([[0,0]         ,[0,size_o[1]],[size_o[0],0],[size_o[0],size_o[1]]])\n    if(angel>0):\n\n        pts2 = np.float32([[interval,0],[0,size[1]  ],[size[0],0  ],[size[0]-interval,size_o[1]]])\n    else:\n        pts2 = np.float32([[0,0],[interval,size[1]  ],[size[0]-interval,0  ],[size[0],size_o[1]]])\n\n    M  = cv2.getPerspectiveTransform(pts1,pts2);\n    dst = cv2.warpPerspective(img,M,size);\n    return dst,M;\n\ndef skew_detection(image_gray):\n    h, w = image_gray.shape[:2]\n    eigen = cv2.cornerEigenValsAndVecs(image_gray,12, 5)\n    angle_sur = np.zeros(180,np.uint);\n    eigen = eigen.reshape(h, w, 3, 2)\n    flow = eigen[:,:,2]\n    vis = image_gray.copy()\n    vis[:] = (192 + np.uint32(vis)) / 2\n    d = 12\n    points =  np.dstack( np.mgrid[d/2:w:d, d/2:h:d] ).reshape(-1, 2)\n    for x, y in points:\n       vx, vy = np.int32(flow[y, x]*d)\n       # cv2.line(rgb, (x-vx, y-vy), (x+vx, y+vy), (0, 355, 0), 1, cv2.LINE_AA)\n       ang = angle(vx,vy);\n       angle_sur[(ang+180)%180] +=1;\n    # torr_bin = 30\n    angle_sur = angle_sur.astype(np.float)\n    angle_sur = (angle_sur-angle_sur.min())/(angle_sur.max()-angle_sur.min())\n    angle_sur = filters.gaussian_filter1d(angle_sur,5)\n    skew_v_val =  angle_sur[20:180-20].max();\n    skew_v = angle_sur[30:180-30].argmax() + 30;\n    skew_h_A = angle_sur[0:30].max()\n    skew_h_B = angle_sur[150:180].max()\n    skew_h = 0;\n    if (skew_h_A > skew_v_val*0.3 or skew_h_B > skew_v_val*0.3):\n        if skew_h_A>=skew_h_B:\n            skew_h = angle_sur[0:20].argmax()\n        else:\n            skew_h = - angle_sur[160:180].argmax()\n    return skew_h,skew_v\n\n\n\n\ndef fastDeskew(image):\n    image_gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n    skew_h,skew_v = skew_detection(image_gray)\n\n    print ""\xe6\xa0\xa1\xe6\xad\xa3\xe8\xa7\x92\xe5\xba\xa6 h "",skew_h,""v"",skew_v\n\n    deskew,M = v_rot(image,int((90-skew_v)*1.5),image.shape,60)\n    return deskew,M\n\n\n\nif __name__ == \'__main__\':\n    fn = \'test_data/test4.png\'\n\n    img = cv2.imread(fn)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    skew_h,skew_v = skew_detection(img,gray)\n    img = v_rot(img,(90-skew_v ),img.shape,60)\n    # img = h_rot(img,skew_h)\n    # if img.shape[0]>img.shape[1]:\n    #     img = h_rot(img, -90)\n\n    plt.show()\n\n    cv2.waitKey()\n'"
hyperlpr/detect.py,0,"b'\nimport cv2\nimport numpy as np\n\n\n\nwatch_cascade = cv2.CascadeClassifier(\'./model/cascade.xml\')\n\n\ndef computeSafeRegion(shape,bounding_rect):\n    top = bounding_rect[1] # y\n    bottom  = bounding_rect[1] + bounding_rect[3] # y +  h\n    left = bounding_rect[0] # x\n    right =   bounding_rect[0] + bounding_rect[2] # x +  w\n\n    min_top = 0\n    max_bottom = shape[0]\n    min_left = 0\n    max_right = shape[1]\n\n    # print ""computeSateRegion input shape"",shape\n    if top < min_top:\n        top = min_top\n        # print ""tap top 0""\n    if left < min_left:\n        left = min_left\n        # print ""tap left 0""\n\n    if bottom > max_bottom:\n        bottom = max_bottom\n        #print ""tap max_bottom max""\n    if right > max_right:\n        right = max_right\n        #print ""tap max_right max""\n\n    # print ""corr"",left,top,right,bottom\n    return [left,top,right-left,bottom-top]\n\n\ndef cropped_from_image(image,rect):\n    x, y, w, h = computeSafeRegion(image.shape,rect)\n    return image[y:y+h,x:x+w]\n\n\ndef detectPlateRough(image_gray,resize_h = 720,en_scale =1.08 ,top_bottom_padding_rate = 0.05):\n    print image_gray.shape\n\n    if top_bottom_padding_rate>0.2:\n        print ""error:top_bottom_padding_rate > 0.2:"",top_bottom_padding_rate\n        exit(1)\n\n    height = image_gray.shape[0]\n    padding =    int(height*top_bottom_padding_rate)\n    scale = image_gray.shape[1]/float(image_gray.shape[0])\n\n    image = cv2.resize(image_gray, (int(scale*resize_h), resize_h))\n\n    image_color_cropped = image[padding:resize_h-padding,0:image_gray.shape[1]]\n\n    image_gray = cv2.cvtColor(image_color_cropped,cv2.COLOR_RGB2GRAY)\n\n    watches = watch_cascade.detectMultiScale(image_gray, en_scale, 2, minSize=(36, 9),maxSize=(36*40, 9*40))\n\n    cropped_images = []\n    for (x, y, w, h) in watches:\n        cropped_origin = cropped_from_image(image_color_cropped, (int(x), int(y), int(w), int(h)))\n        x -= w * 0.14\n        w += w * 0.28\n        y -= h * 0.6\n        h += h * 1.1;\n\n        cropped = cropped_from_image(image_color_cropped, (int(x), int(y), int(w), int(h)))\n\n\n        cropped_images.append([cropped,[x, y+padding, w, h],cropped_origin])\n    return cropped_images\n'"
hyperlpr/e2e.py,0,"b'#coding=utf-8\r\nfrom keras import backend as K\r\nfrom keras.models import load_model\r\nfrom keras.layers import *\r\nimport numpy as np\r\nimport random\r\nimport string\r\n\r\nimport cv2\r\nimport e2emodel as model\r\nchars = [u""\xe4\xba\xac"", u""\xe6\xb2\xaa"", u""\xe6\xb4\xa5"", u""\xe6\xb8\x9d"", u""\xe5\x86\x80"", u""\xe6\x99\x8b"", u""\xe8\x92\x99"", u""\xe8\xbe\xbd"", u""\xe5\x90\x89"", u""\xe9\xbb\x91"", u""\xe8\x8b\x8f"", u""\xe6\xb5\x99"", u""\xe7\x9a\x96"", u""\xe9\x97\xbd"", u""\xe8\xb5\xa3"", u""\xe9\xb2\x81"", u""\xe8\xb1\xab"", u""\xe9\x84\x82"", u""\xe6\xb9\x98"", u""\xe7\xb2\xa4"", u""\xe6\xa1\x82"",\r\n             u""\xe7\x90\xbc"", u""\xe5\xb7\x9d"", u""\xe8\xb4\xb5"", u""\xe4\xba\x91"", u""\xe8\x97\x8f"", u""\xe9\x99\x95"", u""\xe7\x94\x98"", u""\xe9\x9d\x92"", u""\xe5\xae\x81"", u""\xe6\x96\xb0"", u""0"", u""1"", u""2"", u""3"", u""4"", u""5"", u""6"", u""7"", u""8"", u""9"", u""A"",\r\n             u""B"", u""C"", u""D"", u""E"", u""F"", u""G"", u""H"", u""J"", u""K"", u""L"", u""M"", u""N"", u""P"", u""Q"", u""R"", u""S"", u""T"", u""U"", u""V"", u""W"", u""X"",\r\n             u""Y"", u""Z"",u""\xe6\xb8\xaf"",u""\xe5\xad\xa6"",u""\xe4\xbd\xbf"",u""\xe8\xad\xa6"",u""\xe6\xbe\xb3"",u""\xe6\x8c\x82"",u""\xe5\x86\x9b"",u""\xe5\x8c\x97"",u""\xe5\x8d\x97"",u""\xe5\xb9\xbf"",u""\xe6\xb2\x88"",u""\xe5\x85\xb0"",u""\xe6\x88\x90"",u""\xe6\xb5\x8e"",u""\xe6\xb5\xb7"",u""\xe6\xb0\x91"",u""\xe8\x88\xaa"",u""\xe7\xa9\xba""\r\n             ];\r\npred_model = model.construct_model(""./model/ocr_plate_all_w_rnn_2.h5"",)\r\nimport time\r\n\r\n\r\n\r\ndef fastdecode(y_pred):\r\n    results = """"\r\n    confidence = 0.0\r\n    table_pred = y_pred.reshape(-1, len(chars)+1)\r\n\r\n    res = table_pred.argmax(axis=1)\r\n\r\n    for i,one in enumerate(res):\r\n        if one<len(chars) and (i==0 or (one!=res[i-1])):\r\n            results+= chars[one]\r\n            confidence+=table_pred[i][one]\r\n    confidence/= len(results)\r\n    return results,confidence\r\n\r\ndef recognizeOne(src):\r\n    # x_tempx= cv2.imread(src)\r\n    x_tempx = src\r\n    # x_tempx = cv2.bitwise_not(x_tempx)\r\n    x_temp = cv2.resize(x_tempx,( 160,40))\r\n    x_temp = x_temp.transpose(1, 0, 2)\r\n    t0 = time.time()\r\n    y_pred = pred_model.predict(np.array([x_temp]))\r\n    y_pred = y_pred[:,2:,:]\r\n    # plt.imshow(y_pred.reshape(16,66))\r\n    # plt.show()\r\n\r\n    #\r\n    # cv2.imshow(""x_temp"",x_tempx)\r\n    # cv2.waitKey(0)\r\n    return fastdecode(y_pred)\r\n#\r\n#\r\n# import os\r\n#\r\n# path = ""/Users/yujinke/PycharmProjects/HyperLPR_Python_web/cache/finemapping""\r\n# for filename in os.listdir(path):\r\n#     if filename.endswith("".png"") or filename.endswith("".jpg"") or filename.endswith("".bmp""):\r\n#         x = os.path.join(path,filename)\r\n#         recognizeOne(x)\r\n#         # print time.time() - t0\r\n#\r\n#         # cv2.imshow(""x"",x)\r\n#         # cv2.waitKey()\r\n'"
hyperlpr/e2emodel.py,0,"b'\nfrom keras import backend as K\nfrom keras.models import *\nfrom keras.layers import *\nimport e2e\n\n\ndef ctc_lambda_func(args):\n    y_pred, labels, input_length, label_length = args\n    y_pred = y_pred[:, 2:, :]\n    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n\n\ndef construct_model(model_path):\n    input_tensor = Input((None, 40, 3))\n    x = input_tensor\n    base_conv = 32\n\n    for i in range(3):\n        x = Conv2D(base_conv * (2 ** (i)), (3, 3),padding=""same"")(x)\n        x = BatchNormalization()(x)\n        x = Activation(\'relu\')(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = Conv2D(256, (5, 5))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\'relu\')(x)\n    x = Conv2D(1024, (1, 1))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\'relu\')(x)\n    x = Conv2D(len(e2e.chars)+1, (1, 1))(x)\n    x = Activation(\'softmax\')(x)\n    base_model = Model(inputs=input_tensor, outputs=x)\n    base_model.load_weights(model_path)\n    return base_model\n'"
hyperlpr/finemapping.py,0,"b'#coding=utf-8\nimport cv2\nimport numpy as np\n\n\nimport niblack_thresholding as nt\n\nimport deskew\n\ndef fitLine_ransac(pts,zero_add = 0 ):\n    if len(pts)>=2:\n        [vx, vy, x, y] = cv2.fitLine(pts, cv2.DIST_HUBER, 0, 0.01, 0.01)\n        lefty = int((-x * vy / vx) + y)\n        righty = int(((136- x) * vy / vx) + y)\n        return lefty+30+zero_add,righty+30+zero_add\n    return 0,0\n\n\n\n#\xe7\xb2\xbe\xe5\xae\x9a\xe4\xbd\x8d\xe7\xae\x97\xe6\xb3\x95\ndef findContoursAndDrawBoundingBox(image_rgb):\n\n\n    line_upper  = [];\n    line_lower = [];\n\n    line_experiment = []\n    grouped_rects = []\n    gray_image = cv2.cvtColor(image_rgb,cv2.COLOR_BGR2GRAY)\n\n    # for k in np.linspace(-1.5, -0.2,10):\n    for k in np.linspace(-50, 0, 15):\n\n        # thresh_niblack = threshold_niblack(gray_image, window_size=21, k=k)\n        # binary_niblack = gray_image > thresh_niblack\n        # binary_niblack = binary_niblack.astype(np.uint8) * 255\n\n        binary_niblack = cv2.adaptiveThreshold(gray_image,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,17,k)\n        # cv2.imshow(""image1"",binary_niblack)\n        # cv2.waitKey(0)\n        imagex, contours, hierarchy = cv2.findContours(binary_niblack.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n        for contour in contours:\n            bdbox = cv2.boundingRect(contour)\n            if (bdbox[3]/float(bdbox[2])>0.7 and bdbox[3]*bdbox[2]>100 and bdbox[3]*bdbox[2]<1200) or (bdbox[3]/float(bdbox[2])>3 and bdbox[3]*bdbox[2]<100):\n                # cv2.rectangle(rgb,(bdbox[0],bdbox[1]),(bdbox[0]+bdbox[2],bdbox[1]+bdbox[3]),(255,0,0),1)\n                line_upper.append([bdbox[0],bdbox[1]])\n                line_lower.append([bdbox[0]+bdbox[2],bdbox[1]+bdbox[3]])\n\n                line_experiment.append([bdbox[0],bdbox[1]])\n                line_experiment.append([bdbox[0]+bdbox[2],bdbox[1]+bdbox[3]])\n                # grouped_rects.append(bdbox)\n\n    rgb = cv2.copyMakeBorder(image_rgb,30,30,0,0,cv2.BORDER_REPLICATE)\n    leftyA, rightyA = fitLine_ransac(np.array(line_lower),3)\n    rows,cols = rgb.shape[:2]\n\n    # rgb = cv2.line(rgb, (cols - 1, rightyA), (0, leftyA), (0, 0, 255), 1,cv2.LINE_AA)\n\n    leftyB, rightyB = fitLine_ransac(np.array(line_upper),-3)\n\n    rows,cols = rgb.shape[:2]\n\n    # rgb = cv2.line(rgb, (cols - 1, rightyB), (0, leftyB), (0,255, 0), 1,cv2.LINE_AA)\n    pts_map1  = np.float32([[cols - 1, rightyA], [0, leftyA],[cols - 1, rightyB], [0, leftyB]])\n    pts_map2 = np.float32([[136,36],[0,36],[136,0],[0,0]])\n    mat = cv2.getPerspectiveTransform(pts_map1,pts_map2)\n    image = cv2.warpPerspective(rgb,mat,(136,36),flags=cv2.INTER_CUBIC)\n    image,M = deskew.fastDeskew(image)\n\n    return image\n\n\n\n#\xe5\xa4\x9a\xe7\xba\xa7\ndef findContoursAndDrawBoundingBox2(image_rgb):\n\n\n    line_upper  = [];\n    line_lower = [];\n\n    line_experiment = []\n\n    grouped_rects = []\n\n    gray_image = cv2.cvtColor(image_rgb,cv2.COLOR_BGR2GRAY)\n\n    for k in np.linspace(-1.6, -0.2,10):\n    # for k in np.linspace(-15, 0, 15):\n    # #\n    #     thresh_niblack = threshold_niblack(gray_image, window_size=21, k=k)\n    #     binary_niblack = gray_image > thresh_niblack\n    #     binary_niblack = binary_niblack.astype(np.uint8) * 255\n\n        binary_niblack = nt.niBlackThreshold(gray_image,19,k)\n        # cv2.imshow(""binary_niblack_opencv"",binary_niblack_)\n        # cv2.imshow(""binary_niblack_skimage"", binary_niblack)\n\n        # cv2.waitKey(0)\n        imagex, contours, hierarchy = cv2.findContours(binary_niblack.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n\n        for contour in contours:\n            bdbox = cv2.boundingRect(contour)\n            if (bdbox[3]/float(bdbox[2])>0.7 and bdbox[3]*bdbox[2]>100 and bdbox[3]*bdbox[2]<1000) or (bdbox[3]/float(bdbox[2])>3 and bdbox[3]*bdbox[2]<100):\n                # cv2.rectangle(rgb,(bdbox[0],bdbox[1]),(bdbox[0]+bdbox[2],bdbox[1]+bdbox[3]),(255,0,0),1)\n                line_upper.append([bdbox[0],bdbox[1]])\n                line_lower.append([bdbox[0]+bdbox[2],bdbox[1]+bdbox[3]])\n\n                line_experiment.append([bdbox[0],bdbox[1]])\n                line_experiment.append([bdbox[0]+bdbox[2],bdbox[1]+bdbox[3]])\n                # grouped_rects.append(bdbox)\n\n    rgb = cv2.copyMakeBorder(image_rgb,30,30,0,0,cv2.BORDER_REPLICATE)\n    leftyA, rightyA = fitLine_ransac(np.array(line_lower),2)\n    rows,cols = rgb.shape[:2]\n\n    # rgb = cv2.line(rgb, (cols - 1, rightyA), (0, leftyA), (0, 0, 255), 1,cv2.LINE_AA)\n\n    leftyB, rightyB = fitLine_ransac(np.array(line_upper),-4)\n\n    rows,cols = rgb.shape[:2]\n\n    # rgb = cv2.line(rgb, (cols - 1, rightyB), (0, leftyB), (0,255, 0), 1,cv2.LINE_AA)\n    pts_map1  = np.float32([[cols - 1, rightyA], [0, leftyA],[cols - 1, rightyB], [0, leftyB]])\n    pts_map2 = np.float32([[136,36],[0,36],[136,0],[0,0]])\n    mat = cv2.getPerspectiveTransform(pts_map1,pts_map2)\n    image = cv2.warpPerspective(rgb,mat,(136,36),flags=cv2.INTER_CUBIC)\n    image,M= deskew.fastDeskew(image)\n\n\n    return image\n'"
hyperlpr/finemapping_vertical.py,0,"b'#coding=utf-8\nfrom keras.layers import Conv2D, Input,MaxPool2D, Reshape,Activation,Flatten, Dense\nfrom keras.models import Model, Sequential\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.optimizers import adam\nimport numpy as np\n\nimport cv2\n\ndef getModel():\n    input = Input(shape=[16, 66, 3])  # change this shape to [None,None,3] to enable arbitraty shape input\n    x = Conv2D(10, (3, 3), strides=1, padding=\'valid\', name=\'conv1\')(input)\n    x = Activation(""relu"", name=\'relu1\')(x)\n    x = MaxPool2D(pool_size=2)(x)\n    x = Conv2D(16, (3, 3), strides=1, padding=\'valid\', name=\'conv2\')(x)\n    x = Activation(""relu"", name=\'relu2\')(x)\n    x = Conv2D(32, (3, 3), strides=1, padding=\'valid\', name=\'conv3\')(x)\n    x = Activation(""relu"", name=\'relu3\')(x)\n    x = Flatten()(x)\n    output = Dense(2,name = ""dense"")(x)\n    output = Activation(""relu"", name=\'relu4\')(output)\n    model = Model([input], [output])\n    return model\n\n\n\nmodel = getModel()\nmodel.load_weights(""./model/model12.h5"")\n\n\ndef getmodel():\n    return model\n\ndef gettest_model():\n    input = Input(shape=[16, 66, 3])  # change this shape to [None,None,3] to enable arbitraty shape input\n    A = Conv2D(10, (3, 3), strides=1, padding=\'valid\', name=\'conv1\')(input)\n    B = Activation(""relu"", name=\'relu1\')(A)\n    C = MaxPool2D(pool_size=2)(B)\n    x = Conv2D(16, (3, 3), strides=1, padding=\'valid\', name=\'conv2\')(C)\n    x = Activation(""relu"", name=\'relu2\')(x)\n    x = Conv2D(32, (3, 3), strides=1, padding=\'valid\', name=\'conv3\')(x)\n    K = Activation(""relu"", name=\'relu3\')(x)\n\n\n    x = Flatten()(K)\n    dense = Dense(2,name = ""dense"")(x)\n    output = Activation(""relu"", name=\'relu4\')(dense)\n    x = Model([input], [output])\n    x.load_weights(""./model/model12.h5"")\n    ok = Model([input], [dense])\n\n    for layer in ok.layers:\n        print layer\n\n    return ok\n\n\n\n\ndef finemappingVertical(image):\n    resized = cv2.resize(image,(66,16))\n    resized = resized.astype(np.float)/255\n    res= model.predict(np.array([resized]))[0]\n    print ""keras_predict"",res\n    res  =res*image.shape[1]\n    res = res.astype(np.int)\n    H,T = res\n    H-=3\n    #3 79.86\n    #4 79.3\n    #5 79.5\n    #6 78.3\n\n\n    #T\n    #T+1 80.9\n    #T+2 81.75\n    #T+3 81.75\n\n\n\n    if H<0:\n        H=0\n    T+=2;\n\n    if T>= image.shape[1]-1:\n        T= image.shape[1]-1\n\n    image = image[0:35,H:T+2]\n\n    image = cv2.resize(image, (int(136), int(36)))\n    return image'"
hyperlpr/niblack_thresholding.py,0,"b'import cv2\nimport numpy as np\n\n\n\ndef niBlackThreshold(  src,  blockSize,  k,  binarizationMethod= 0 ):\n    mean = cv2.boxFilter(src,cv2.CV_32F,(blockSize, blockSize),borderType=cv2.BORDER_REPLICATE)\n    sqmean = cv2.sqrBoxFilter(src, cv2.CV_32F, (blockSize, blockSize), borderType = cv2.BORDER_REPLICATE)\n    variance = sqmean - (mean*mean)\n    stddev  = np.sqrt(variance)\n    thresh = mean + stddev * float(-k)\n    thresh = thresh.astype(src.dtype)\n    k = (src>thresh)*255\n    k = k.astype(np.uint8)\n    return k\n\n\n# cv2.imshow()'"
hyperlpr/pipline.py,0,"b'#coding=utf-8\nimport detect\nimport  finemapping  as  fm\n\nimport segmentation\nimport cv2\n\nimport time\nimport numpy as np\n\nfrom PIL import ImageFont\nfrom PIL import Image\nfrom PIL import ImageDraw\nimport json\n\nimport sys\nimport typeDistinguish as td\n\n\nreload(sys)\nsys.setdefaultencoding(""utf-8"")\n\nfontC = ImageFont.truetype(""./Font/platech.ttf"", 14, 0);\n\nimport e2e\n#\xe5\xaf\xbb\xe6\x89\xbe\xe8\xbd\xa6\xe7\x89\x8c\xe5\xb7\xa6\xe5\x8f\xb3\xe8\xbe\xb9\xe7\x95\x8c\n\ndef find_edge(image):\n    sum_i = image.sum(axis=0)\n    sum_i =  sum_i.astype(np.float)\n    sum_i/=image.shape[0]*255\n    # print sum_i\n\n    start= 0 ;\n    end = image.shape[1]-1\n\n    for i,one in enumerate(sum_i):\n        if one>0.4:\n            start = i;\n            if start-3<0:\n                start = 0\n            else:\n                start -=3\n\n            break;\n    for i,one in enumerate(sum_i[::-1]):\n\n        if one>0.4:\n            end = end - i;\n            if end+4>image.shape[1]-1:\n                end = image.shape[1]-1\n            else:\n                end+=4\n            break\n    return start,end\n\n\n#\xe5\x9e\x82\xe7\x9b\xb4\xe8\xbe\xb9\xe7\xbc\x98\xe6\xa3\x80\xe6\xb5\x8b\n\ndef verticalEdgeDetection(image):\n    image_sobel = cv2.Sobel(image.copy(),cv2.CV_8U,1,0)\n    # image = auto_canny(image_sobel)\n\n    # img_sobel, CV_8U, 1, 0, 3, 1, 0, BORDER_DEFAULT\n    # canny_image  = auto_canny(image)\n    flag,thres = cv2.threshold(image_sobel,0,255,cv2.THRESH_OTSU|cv2.THRESH_BINARY)\n    print flag\n    flag,thres = cv2.threshold(image_sobel,int(flag*0.7),255,cv2.THRESH_BINARY)\n    # thres = simpleThres(image_sobel)\n    kernal = np.ones(shape=(3,15))\n    thres = cv2.morphologyEx(thres,cv2.MORPH_CLOSE,kernal)\n    return thres\n\n#\xe7\xa1\xae\xe5\xae\x9a\xe7\xb2\x97\xe7\x95\xa5\xe7\x9a\x84\xe5\xb7\xa6\xe5\x8f\xb3\xe8\xbe\xb9\xe7\x95\x8c\ndef horizontalSegmentation(image):\n\n    thres = verticalEdgeDetection(image)\n    # thres = thres*image\n    head,tail = find_edge(thres)\n    # print head,tail\n    # cv2.imshow(""edge"",thres)\n    tail = tail+5\n    if tail>135:\n        tail = 135\n    image = image[0:35,head:tail]\n    image = cv2.resize(image, (int(136), int(36)))\n    return image\n\n\n\n#\xe6\x89\x93\xe4\xb8\x8aboundingbox\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe\ndef drawRectBox(image,rect,addText):\n    cv2.rectangle(image, (int(rect[0]), int(rect[1])), (int(rect[0] + rect[2]), int(rect[1] + rect[3])), (0,0, 255), 2,cv2.LINE_AA)\n    cv2.rectangle(image, (int(rect[0]-1), int(rect[1])-16), (int(rect[0] + 115), int(rect[1])), (0, 0, 255), -1,\n                  cv2.LINE_AA)\n\n    img = Image.fromarray(image)\n    draw = ImageDraw.Draw(img)\n    draw.text((int(rect[0]+1), int(rect[1]-16)), addText.decode(""utf-8""), (255, 255, 255), font=fontC)\n    imagex = np.array(img)\n\n    return imagex\n\n\n\n\nimport cache\nimport finemapping_vertical as fv\n\n\ndef RecognizePlateJson(image):\n\n    images = detect.detectPlateRough(image,image.shape[0],top_bottom_padding_rate=0.1)\n\n    jsons = []\n\n    for j,plate in enumerate(images):\n\n\n        plate,rect,origin_plate =plate\n        res, confidence = e2e.recognizeOne(origin_plate)\n        print ""res"",res\n\n        cv2.imwrite(""./""+str(j)+""_rough.jpg"",plate)\n\n        # print ""\xe8\xbd\xa6\xe7\x89\x8c\xe7\xb1\xbb\xe5\x9e\x8b:"",ptype\n        # plate = cv2.cvtColor(plate, cv2.COLOR_RGB2GRAY)\n        plate  =cv2.resize(plate,(136,int(36*2.5)))\n        t1 = time.time()\n\n\n        ptype = td.SimplePredict(plate)\n        if ptype>0 and ptype<4:\n            plate = cv2.bitwise_not(plate)\n        # demo = verticalEdgeDetection(plate)\n\n        image_rgb = fm.findContoursAndDrawBoundingBox(plate)\n        image_rgb = fv.finemappingVertical(image_rgb)\n        cache.verticalMappingToFolder(image_rgb)\n        # print time.time() - t1,""\xe6\xa0\xa1\xe6\xad\xa3""\n        print ""e2e:"",e2e.recognizeOne(image_rgb)[0]\n        image_gray = cv2.cvtColor(image_rgb,cv2.COLOR_BGR2GRAY)\n\n\n        cv2.imwrite(""./""+str(j)+"".jpg"",image_gray)\n        # image_gray = horizontalSegmentation(image_gray)\n\n\n        t2 = time.time()\n        res, confidence = e2e.recognizeOne(image_rgb)\n        res_json = {}\n        if confidence  > 0.6:\n            res_json[""Name""] = res\n            res_json[""Type""] = td.plateType[ptype]\n            res_json[""Confidence""] = confidence;\n            res_json[""x""] = int(rect[0])\n            res_json[""y""] = int(rect[1])\n            res_json[""w""] = int(rect[2])\n            res_json[""h""] = int(rect[3])\n            jsons.append(res_json)\n    print json.dumps(jsons,ensure_ascii=False,encoding=""gb2312"")\n\n    return json.dumps(jsons,ensure_ascii=False,encoding=""gb2312"")\n\n\n\n\ndef SimpleRecognizePlateByE2E(image):\n    t0 = time.time()\n    images = detect.detectPlateRough(image,image.shape[0],top_bottom_padding_rate=0.1)\n    res_set = []\n    for j,plate in enumerate(images):\n        plate, rect, origin_plate  =plate\n        # plate = cv2.cvtColor(plate, cv2.COLOR_RGB2GRAY)\n        plate  =cv2.resize(plate,(136,36*2))\n        res,confidence = e2e.recognizeOne(origin_plate)\n        print ""res"",res\n\n        t1 = time.time()\n        ptype = td.SimplePredict(plate)\n        if ptype>0 and ptype<5:\n            # pass\n            plate = cv2.bitwise_not(plate)\n        image_rgb = fm.findContoursAndDrawBoundingBox(plate)\n        image_rgb = fv.finemappingVertical(image_rgb)\n        image_rgb = fv.finemappingVertical(image_rgb)\n        cache.verticalMappingToFolder(image_rgb)\n        #cv2.imwrite(""./""+str(j)+"".jpg"",image_rgb)\n        res,confidence = e2e.recognizeOne(image_rgb)\n        print res,confidence\n        res_set.append([[],res,confidence])\n\n        if confidence>0.7:\n            image = drawRectBox(image, rect, res+"" ""+str(round(confidence,3)))\n    return image,res_set\n\n\n\n\n\n\n\ndef SimpleRecognizePlate(image):\n    t0 = time.time()\n    images = detect.detectPlateRough(image,image.shape[0],top_bottom_padding_rate=0.1)\n    res_set = []\n    for j,plate in enumerate(images):\n        plate, rect, origin_plate  =plate\n        # plate = cv2.cvtColor(plate, cv2.COLOR_RGB2GRAY)\n        plate  =cv2.resize(plate,(136,36*2))\n        t1 = time.time()\n\n        ptype = td.SimplePredict(plate)\n        if ptype>0 and ptype<5:\n            plate = cv2.bitwise_not(plate)\n\n        image_rgb = fm.findContoursAndDrawBoundingBox(plate)\n\n        image_rgb = fv.finemappingVertical(image_rgb)\n        cache.verticalMappingToFolder(image_rgb)\n        print ""e2e:"", e2e.recognizeOne(image_rgb)\n        image_gray = cv2.cvtColor(image_rgb,cv2.COLOR_RGB2GRAY)\n\n        # image_gray = horizontalSegmentation(image_gray)\n        cv2.imshow(""image_gray"",image_gray)\n        # cv2.waitKey()\n\n        cv2.imwrite(""./""+str(j)+"".jpg"",image_gray)\n        # cv2.imshow(""image"",image_gray)\n        # cv2.waitKey(0)\n        print ""\xe6\xa0\xa1\xe6\xad\xa3"",time.time() - t1,""s""\n        # cv2.imshow(""image,"",image_gray)\n        # cv2.waitKey(0)\n        t2 = time.time()\n        val = segmentation.slidingWindowsEval(image_gray)\n        # print val\n        print ""\xe5\x88\x86\xe5\x89\xb2\xe5\x92\x8c\xe8\xaf\x86\xe5\x88\xab"",time.time() - t2,""s""\n        if len(val)==3:\n            blocks, res, confidence = val\n            if confidence/7>0.7:\n                image =  drawRectBox(image,rect,res)\n                res_set.append(res)\n                for i,block in enumerate(blocks):\n\n                    block_ = cv2.resize(block,(25,25))\n                    block_ = cv2.cvtColor(block_,cv2.COLOR_GRAY2BGR)\n                    image[j * 25:(j * 25) + 25, i * 25:(i * 25) + 25] = block_\n                    if image[j*25:(j*25)+25,i*25:(i*25)+25].shape == block_.shape:\n                        pass\n\n\n            if confidence>0:\n                print ""\xe8\xbd\xa6\xe7\x89\x8c:"",res,""\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6:"",confidence/7\n            else:\n                pass\n\n                # print ""\xe4\xb8\x8d\xe7\xa1\xae\xe5\xae\x9a\xe7\x9a\x84\xe8\xbd\xa6\xe7\x89\x8c:"", res, ""\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6:"", confidence\n\n    print time.time() - t0,""s""\n    return image,res_set\n\n\n\n\n'"
hyperlpr/plateStructure.py,0,b''
hyperlpr/precise.py,0,b''
hyperlpr/recognizer.py,0,"b'#coding=utf-8\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D,MaxPool2D\nfrom keras.optimizers import SGD\nfrom keras import backend as K\n\nK.image_data_format()\n\n\nimport cv2\nimport numpy as np\n\n\n\nindex = {u""\xe4\xba\xac"": 0, u""\xe6\xb2\xaa"": 1, u""\xe6\xb4\xa5"": 2, u""\xe6\xb8\x9d"": 3, u""\xe5\x86\x80"": 4, u""\xe6\x99\x8b"": 5, u""\xe8\x92\x99"": 6, u""\xe8\xbe\xbd"": 7, u""\xe5\x90\x89"": 8, u""\xe9\xbb\x91"": 9, u""\xe8\x8b\x8f"": 10, u""\xe6\xb5\x99"": 11, u""\xe7\x9a\x96"": 12,\n         u""\xe9\x97\xbd"": 13, u""\xe8\xb5\xa3"": 14, u""\xe9\xb2\x81"": 15, u""\xe8\xb1\xab"": 16, u""\xe9\x84\x82"": 17, u""\xe6\xb9\x98"": 18, u""\xe7\xb2\xa4"": 19, u""\xe6\xa1\x82"": 20, u""\xe7\x90\xbc"": 21, u""\xe5\xb7\x9d"": 22, u""\xe8\xb4\xb5"": 23, u""\xe4\xba\x91"": 24,\n         u""\xe8\x97\x8f"": 25, u""\xe9\x99\x95"": 26, u""\xe7\x94\x98"": 27, u""\xe9\x9d\x92"": 28, u""\xe5\xae\x81"": 29, u""\xe6\x96\xb0"": 30, u""0"": 31, u""1"": 32, u""2"": 33, u""3"": 34, u""4"": 35, u""5"": 36,\n         u""6"": 37, u""7"": 38, u""8"": 39, u""9"": 40, u""A"": 41, u""B"": 42, u""C"": 43, u""D"": 44, u""E"": 45, u""F"": 46, u""G"": 47, u""H"": 48,\n         u""J"": 49, u""K"": 50, u""L"": 51, u""M"": 52, u""N"": 53, u""P"": 54, u""Q"": 55, u""R"": 56, u""S"": 57, u""T"": 58, u""U"": 59, u""V"": 60,\n         u""W"": 61, u""X"": 62, u""Y"": 63, u""Z"": 64,u""\xe6\xb8\xaf"":65,u""\xe5\xad\xa6"":66 ,u""O"":67 ,u""\xe4\xbd\xbf"":68,u""\xe8\xad\xa6"":69,u""\xe6\xbe\xb3"":70,u""\xe6\x8c\x82"":71};\n\nchars = [""\xe4\xba\xac"", ""\xe6\xb2\xaa"", ""\xe6\xb4\xa5"", ""\xe6\xb8\x9d"", ""\xe5\x86\x80"", ""\xe6\x99\x8b"", ""\xe8\x92\x99"", ""\xe8\xbe\xbd"", ""\xe5\x90\x89"", ""\xe9\xbb\x91"", ""\xe8\x8b\x8f"", ""\xe6\xb5\x99"", ""\xe7\x9a\x96"", ""\xe9\x97\xbd"", ""\xe8\xb5\xa3"", ""\xe9\xb2\x81"", ""\xe8\xb1\xab"", ""\xe9\x84\x82"", ""\xe6\xb9\x98"", ""\xe7\xb2\xa4"", ""\xe6\xa1\x82"",\n             ""\xe7\x90\xbc"", ""\xe5\xb7\x9d"", ""\xe8\xb4\xb5"", ""\xe4\xba\x91"", ""\xe8\x97\x8f"", ""\xe9\x99\x95"", ""\xe7\x94\x98"", ""\xe9\x9d\x92"", ""\xe5\xae\x81"", ""\xe6\x96\xb0"", ""0"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""A"",\n             ""B"", ""C"", ""D"", ""E"", ""F"", ""G"", ""H"", ""J"", ""K"", ""L"", ""M"", ""N"", ""P"",\n         ""Q"", ""R"", ""S"", ""T"", ""U"", ""V"", ""W"", ""X"",\n             ""Y"", ""Z"",""\xe6\xb8\xaf"",""\xe5\xad\xa6"",""O"",""\xe4\xbd\xbf"",""\xe8\xad\xa6"",""\xe6\xbe\xb3"",""\xe6\x8c\x82"" ];\n\n\n\ndef Getmodel_tensorflow(nb_classes):\n    # nb_classes = len(charset)\n\n    img_rows, img_cols = 23, 23\n    # number of convolutional filters to use\n    nb_filters = 32\n    # size of pooling area for max pooling\n    nb_pool = 2\n    # convolution kernel size\n    nb_conv = 3\n\n    # x = np.load(\'x.npy\')\n    \n    # y = np_utils.to_categorical(range(3062)*45*5*2, nb_classes)\n    # weight = ((type_class - np.arange(type_class)) / type_class + 1) ** 3\n    # weight = dict(zip(range(3063), weight / weight.mean()))  # \xe8\xb0\x83\xe6\x95\xb4\xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x8c\xe9\xab\x98\xe9\xa2\x91\xe5\xad\x97\xe4\xbc\x98\xe5\x85\x88\n\n    model = Sequential()\n    model.add(Conv2D(32, (5, 5),input_shape=(img_rows, img_cols,1)))\n    model.add(Activation(\'relu\'))\n    model.add(MaxPool2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(32, (3, 3)))\n    model.add(Activation(\'relu\'))\n    model.add(MaxPool2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(512, (3, 3)))\n    # model.add(Activation(\'relu\'))\n    # model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n    # model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(512))\n    model.add(Activation(\'relu\'))\n    model.add(Dropout(0.5))\n    model.add(Dense(nb_classes))\n    model.add(Activation(\'softmax\'))\n    model.compile(loss=\'categorical_crossentropy\',\n                  optimizer=\'adam\',\n                  metrics=[\'accuracy\'])\n    return model\n\n\n\n\ndef Getmodel_ch(nb_classes):\n    # nb_classes = len(charset)\n\n    img_rows, img_cols = 23, 23\n    # number of convolutional filters to use\n    nb_filters = 32\n    # size of pooling area for max pooling\n    nb_pool = 2\n    # convolution kernel size\n    nb_conv = 3\n\n    # x = np.load(\'x.npy\')\n    # y = np_utils.to_categorical(range(3062)*45*5*2, nb_classes)\n    # weight = ((type_class - np.arange(type_class)) / type_class + 1) ** 3\n    # weight = dict(zip(range(3063), weight / weight.mean()))  # \xe8\xb0\x83\xe6\x95\xb4\xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x8c\xe9\xab\x98\xe9\xa2\x91\xe5\xad\x97\xe4\xbc\x98\xe5\x85\x88\n\n    model = Sequential()\n    model.add(Conv2D(32, (5, 5),input_shape=(img_rows, img_cols,1)))\n    model.add(Activation(\'relu\'))\n    model.add(MaxPool2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(32, (3, 3)))\n    model.add(Activation(\'relu\'))\n    model.add(MaxPool2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(512, (3, 3)))\n    # model.add(Activation(\'relu\'))\n    # model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n    # model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(756))\n    model.add(Activation(\'relu\'))\n    model.add(Dropout(0.5))\n    model.add(Dense(nb_classes))\n    model.add(Activation(\'softmax\'))\n    model.compile(loss=\'categorical_crossentropy\',\n                  optimizer=\'adam\',\n                  metrics=[\'accuracy\'])\n    return model\n\n\n\nmodel  = Getmodel_tensorflow(65)\n#\xe6\x9e\x84\xe5\xbb\xba\xe7\xbd\x91\xe7\xbb\x9c\n\nmodel_ch = Getmodel_ch(31)\n\nmodel_ch.load_weights(""./model/char_chi_sim.h5"")\n# model_ch.save_weights(""./model/char_chi_sim.h5"")\nmodel.load_weights(""./model/char_rec.h5"")\n# model.save(""./model/char_rec.h5"")\n\n\ndef SimplePredict(image,pos):\n    image = cv2.resize(image, (23, 23))\n    image = cv2.equalizeHist(image)\n    image = image.astype(np.float) / 255\n    image -= image.mean()\n    image = np.expand_dims(image, 3)\n    if pos!=0:\n        res = np.array(model.predict(np.array([image]))[0])\n    else:\n        res = np.array(model_ch.predict(np.array([image]))[0])\n\n    zero_add = 0 ;\n\n    if pos==0:\n        res = res[:31]\n    elif pos==1:\n        res = res[31+10:65]\n        zero_add = 31+10\n    else:\n        res = res[31:]\n        zero_add = 31\n\n    max_id = res.argmax()\n\n\n    return res.max(),chars[max_id+zero_add],max_id+zero_add\n\n'"
hyperlpr/segmentation.py,0,"b'#coding=utf-8\nimport cv2\nimport numpy as np\n\n# from matplotlib import pyplot as plt\nimport scipy.ndimage.filters as f\nimport scipy\n\nimport time\nimport scipy.signal as l\n\n\n\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPool2D\nfrom keras.optimizers import SGD\nfrom keras import backend as K\n\nK.image_data_format()\n\n\ndef Getmodel_tensorflow(nb_classes):\n    # nb_classes = len(charset)\n    img_rows, img_cols = 23, 23\n    # number of convolutional filters to use\n    nb_filters = 16\n    # size of pooling area for max pooling\n    nb_pool = 2\n    # convolution kernel size\n    nb_conv = 3\n    # x = np.load(\'x.npy\')\n    # y = np_utils.to_categorical(range(3062)*45*5*2, nb_classes)\n    # weight = ((type_class - np.arange(type_class)) / type_class + 1) ** 3\n    # weight = dict(zip(range(3063), weight / weight.mean()))  # \xe8\xb0\x83\xe6\x95\xb4\xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x8c\xe9\xab\x98\xe9\xa2\x91\xe5\xad\x97\xe4\xbc\x98\xe5\x85\x88\n\n    model = Sequential()\n    model.add(Conv2D(nb_filters, (nb_conv, nb_conv),input_shape=(img_rows, img_cols,1)))\n    model.add(Activation(\'relu\'))\n    model.add(MaxPool2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Conv2D(nb_filters, (nb_conv, nb_conv)))\n    model.add(Activation(\'relu\'))\n    model.add(MaxPool2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Flatten())\n    model.add(Dense(256))\n    model.add(Dropout(0.5))\n\n    model.add(Activation(\'relu\'))\n    model.add(Dense(nb_classes))\n    model.add(Activation(\'softmax\'))\n    model.compile(loss=\'categorical_crossentropy\',\n                  optimizer=\'sgd\',\n                  metrics=[\'accuracy\'])\n    return model\n\n\n\ndef Getmodel_tensorflow_light(nb_classes):\n    # nb_classes = len(charset)\n    img_rows, img_cols = 23, 23\n    # number of convolutional filters to use\n    nb_filters = 8\n    # size of pooling area for max pooling\n    nb_pool = 2\n    # convolution kernel size\n    nb_conv = 3\n    # x = np.load(\'x.npy\')\n    # y = np_utils.to_categorical(range(3062)*45*5*2, nb_classes)\n    # weight = ((type_class - np.arange(type_class)) / type_class + 1) ** 3\n    # weight = dict(zip(range(3063), weight / weight.mean()))  # \xe8\xb0\x83\xe6\x95\xb4\xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x8c\xe9\xab\x98\xe9\xa2\x91\xe5\xad\x97\xe4\xbc\x98\xe5\x85\x88\n\n    model = Sequential()\n    model.add(Conv2D(nb_filters, (nb_conv, nb_conv),input_shape=(img_rows, img_cols, 1)))\n    model.add(Activation(\'relu\'))\n    model.add(MaxPool2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Conv2D(nb_filters, (nb_conv * 2, nb_conv * 2)))\n    model.add(Activation(\'relu\'))\n    model.add(MaxPool2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Flatten())\n    model.add(Dense(32))\n    # model.add(Dropout(0.25))\n\n    model.add(Activation(\'relu\'))\n    model.add(Dense(nb_classes))\n    model.add(Activation(\'softmax\'))\n    model.compile(loss=\'categorical_crossentropy\',\n                  optimizer=\'adam\',\n                  metrics=[\'accuracy\'])\n    return model\n\n\n\n\nmodel  = Getmodel_tensorflow_light(3)\nmodel2  = Getmodel_tensorflow(3)\n\nimport os\nmodel.load_weights(""./model/char_judgement1.h5"")\n# model.save(""./model/char_judgement1.h5"")\nmodel2.load_weights(""./model/char_judgement.h5"")\n# model2.save(""./model/char_judgement.h5"")\n\n\nmodel = model2\ndef get_median(data):\n   data = sorted(data)\n   size = len(data)\n   # print size\n\n   if size % 2 == 0: # \xe5\x88\xa4\xe6\x96\xad\xe5\x88\x97\xe8\xa1\xa8\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba\xe5\x81\xb6\xe6\x95\xb0\n    median = (data[size//2]+data[size//2-1])/2\n    data[0] = median\n   if size % 2 == 1: # \xe5\x88\xa4\xe6\x96\xad\xe5\x88\x97\xe8\xa1\xa8\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba\xe5\xa5\x87\xe6\x95\xb0\n    median = data[(size-1)//2]\n    data[0] = median\n   return data[0]\nimport time\n\ndef searchOptimalCuttingPoint(rgb,res_map,start,width_boundingbox,interval_range):\n    t0  = time.time()\n    #\n    # for x in xrange(10):\n    #     res_map = np.vstack((res_map,res_map[-1]))\n    length = res_map.shape[0]\n    refine_s = -2;\n\n    if width_boundingbox>20:\n        refine_s = -9\n    score_list = []\n    interval_big = int(width_boundingbox * 0.3)  #\n    p = 0\n    for zero_add in xrange(start,start+50,3):\n        # for interval_small in xrange(-0,width_boundingbox/2):\n            for i in xrange(-8,int(width_boundingbox/1)-8):\n                for refine in xrange(refine_s,width_boundingbox/2+3):\n                    p1 = zero_add# this point is province\n                    p2 = p1 + width_boundingbox +refine #\n                    p3 = p2 + width_boundingbox + interval_big+i+1\n                    p4 = p3 + width_boundingbox +refine\n                    p5 = p4 + width_boundingbox +refine\n                    p6 = p5 + width_boundingbox +refine\n                    p7 = p6 + width_boundingbox +refine\n                    if p7>=length:\n                        continue\n                    score = res_map[p1][2]*3 -(res_map[p3][1]+res_map[p4][1]+res_map[p5][1]+res_map[p6][1]+res_map[p7][1])+7\n                    # print score\n                    score_list.append([score,[p1,p2,p3,p4,p5,p6,p7]])\n                    p+=1\n    print p\n\n    score_list = sorted(score_list , key=lambda x:x[0])\n    # for one in score_list[-1][1]:\n    #     cv2.line(debug,(one,0),(one,36),(255,0,0),1)\n    # #\n    # cv2.imshow(""one"",debug)\n    # cv2.waitKey(0)\n    #\n    print ""\xe5\xaf\xbb\xe6\x89\xbe\xe6\x9c\x80\xe4\xbd\xb3\xe7\x82\xb9"",time.time()-t0\n    return score_list[-1]\n\n\nimport sys\n\nsys.path.append(\'../\')\nimport recognizer as cRP\nimport niblack_thresholding as nt\n\ndef refineCrop(sections,width=16):\n    new_sections = []\n    for section in sections:\n        # cv2.imshow(""section\xc2\xa1"",section)\n\n        # cv2.blur(section,(3,3),3)\n\n        sec_center = np.array([section.shape[1]/2,section.shape[0]/2])\n        binary_niblack = nt.niBlackThreshold(section,17,-0.255)\n        imagex, contours, hierarchy  = cv2.findContours(binary_niblack,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n        boxs = []\n        for contour in contours:\n            x,y,w,h = cv2.boundingRect(contour)\n\n            ratio = w/float(h)\n            if ratio<1 and h>36*0.4 and y<16\\\n                    :\n                box = [x,y,w,h]\n\n                boxs.append([box,np.array([x+w/2,y+h/2])])\n                # cv2.rectangle(section,(x,y),(x+w,y+h),255,1)\n\n\n\n\n        # print boxs\n\n        dis_ = np.array([ ((one[1]-sec_center)**2).sum() for one in boxs])\n        if len(dis_)==0:\n            kernal = [0, 0, section.shape[1], section.shape[0]]\n        else:\n            kernal = boxs[dis_.argmin()][0]\n\n        center_c  = (kernal[0]+kernal[2]/2,kernal[1]+kernal[3]/2)\n        w_2 = int(width/2)\n        h_2 = kernal[3]/2\n\n        if center_c[0] - w_2< 0:\n            w_2 = center_c[0]\n        new_box = [center_c[0] - w_2,kernal[1],width,kernal[3]]\n        # print new_box[2]/float(new_box[3])\n        if new_box[2]/float(new_box[3])>0.5:\n            # print ""\xe5\xbc\x82\xe5\xb8\xb8""\n            h = int((new_box[2]/0.35 )/2)\n            if h>35:\n                h = 35\n            new_box[1] = center_c[1]- h\n            if new_box[1]<0:\n                new_box[1] = 1\n\n            new_box[3] = h*2\n\n\n\n\n\n\n\n\n        section  = section[new_box[1]:new_box[1]+new_box[3],new_box[0]:new_box[0]+new_box[2]]\n        # cv2.imshow(""section"",section)\n        # cv2.waitKey(0)\n        new_sections.append(section)\n        # print new_box\n\n\n    return new_sections\n\n\n\n\n\ndef slidingWindowsEval(image):\n    windows_size = 16;\n    stride = 1\n    height= image.shape[0]\n    t0 = time.time()\n    data_sets = []\n\n    for i in range(0,image.shape[1]-windows_size+1,stride):\n        data = image[0:height,i:i+windows_size]\n        data = cv2.resize(data,(23,23))\n        # cv2.imshow(""image"",data)\n        data = cv2.equalizeHist(data)\n        data = data.astype(np.float)/255\n        data=  np.expand_dims(data,3)\n        data_sets.append(data)\n\n    res = model2.predict(np.array(data_sets))\n    print ""\xe5\x88\x86\xe5\x89\xb2"",time.time() - t0\n\n    pin = res\n    p = 1 -  (res.T)[1]\n    p = f.gaussian_filter1d(np.array(p,dtype=np.float),3)\n    lmin = l.argrelmax(np.array(p),order = 3)[0]\n    interval = []\n    for i in xrange(len(lmin)-1):\n        interval.append(lmin[i+1]-lmin[i])\n\n    if(len(interval)>3):\n        mid  = get_median(interval)\n    else:\n        return []\n    pin = np.array(pin)\n    res =  searchOptimalCuttingPoint(image,pin,0,mid,3)\n\n    cutting_pts = res[1]\n    last =  cutting_pts[-1] + mid\n    if last < image.shape[1]:\n        cutting_pts.append(last)\n    else:\n        cutting_pts.append(image.shape[1]-1)\n    name = """"\n    confidence =0.00\n    seg_block = []\n    for x in xrange(1,len(cutting_pts)):\n        if x != len(cutting_pts)-1 and x!=1:\n            section = image[0:36,cutting_pts[x-1]-2:cutting_pts[x]+2]\n        elif  x==1:\n            c_head = cutting_pts[x - 1]- 2\n            if c_head<0:\n                c_head=0\n            c_tail = cutting_pts[x] + 2\n            section = image[0:36, c_head:c_tail]\n        elif x==len(cutting_pts)-1:\n            end = cutting_pts[x]\n            diff = image.shape[1]-end\n            c_head = cutting_pts[x - 1]\n            c_tail = cutting_pts[x]\n            if diff<7 :\n                section = image[0:36, c_head-5:c_tail+5]\n            else:\n                diff-=1\n                section = image[0:36, c_head - diff:c_tail + diff]\n        elif  x==2:\n            section = image[0:36, cutting_pts[x - 1] - 3:cutting_pts[x-1]+ mid]\n        else:\n            section = image[0:36,cutting_pts[x-1]:cutting_pts[x]]\n        seg_block.append(section)\n    refined = refineCrop(seg_block,mid-1)\n\n    t0 = time.time()\n    for i,one in enumerate(refined):\n        res_pre = cRP.SimplePredict(one, i )\n        # cv2.imshow(str(i),one)\n        # cv2.waitKey(0)\n        confidence+=res_pre[0]\n        name+= res_pre[1]\n    print ""\xe5\xad\x97\xe7\xac\xa6\xe8\xaf\x86\xe5\x88\xab"",time.time() - t0\n\n    return refined,name,confidence\n'"
hyperlpr/typeDistinguish.py,0,"b'#coding=utf-8\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPool2D\nfrom keras.optimizers import SGD\nfrom keras import backend as K\n\nK.image_data_format()\n\n\nimport cv2\nimport numpy as np\n\n\nplateType  = [u""\xe8\x93\x9d\xe7\x89\x8c"",u""\xe5\x8d\x95\xe5\xb1\x82\xe9\xbb\x84\xe7\x89\x8c"",u""\xe6\x96\xb0\xe8\x83\xbd\xe6\xba\x90\xe8\xbd\xa6\xe7\x89\x8c"",u""\xe7\x99\xbd\xe8\x89\xb2"",u""\xe9\xbb\x91\xe8\x89\xb2-\xe6\xb8\xaf\xe6\xbe\xb3""]\ndef Getmodel_tensorflow(nb_classes):\n    # nb_classes = len(charset)\n\n    img_rows, img_cols = 9, 34\n    # number of convolutional filters to use\n    nb_filters = 32\n    # size of pooling area for max pooling\n    nb_pool = 2\n    # convolution kernel size\n    nb_conv = 3\n\n    # x = np.load(\'x.npy\')\n    # y = np_utils.to_categorical(range(3062)*45*5*2, nb_classes)\n    # weight = ((type_class - np.arange(type_class)) / type_class + 1) ** 3\n    # weight = dict(zip(range(3063), weight / weight.mean()))  # \xe8\xb0\x83\xe6\x95\xb4\xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x8c\xe9\xab\x98\xe9\xa2\x91\xe5\xad\x97\xe4\xbc\x98\xe5\x85\x88\n\n    model = Sequential()\n    model.add(Conv2D(16, (5, 5),input_shape=(img_rows, img_cols,3)))\n    model.add(Activation(\'relu\'))\n    model.add(MaxPool2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Flatten())\n    model.add(Dense(64))\n    model.add(Activation(\'relu\'))\n    model.add(Dropout(0.5))\n    model.add(Dense(nb_classes))\n    model.add(Activation(\'softmax\'))\n    model.compile(loss=\'categorical_crossentropy\',\n                  optimizer=\'adam\',\n                  metrics=[\'accuracy\'])\n    return model\n\nmodel = Getmodel_tensorflow(5)\nmodel.load_weights(""./model/plate_type.h5"")\nmodel.save(""./model/plate_type.h5"")\ndef SimplePredict(image):\n    image = cv2.resize(image, (34, 9))\n    image = image.astype(np.float) / 255\n    res = np.array(model.predict(np.array([image]))[0])\n    return res.argmax()\n\n\n'"
hyperlpr_py3/__init__.py,0,b''
hyperlpr_py3/cache.py,0,"b'import cv2\nimport os\nimport hashlib\n\ndef verticalMappingToFolder(image):\n    name = hashlib.md5(image.data).hexdigest()[:8]\n    print(name)\n\n    cv2.imwrite(""./cache/finemapping/""+name+"".png"",image)\n\n\n'"
hyperlpr_py3/colourDetection.py,0,"b'# -- coding: UTF-8\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport os\n\nboundaries = [\n    ([100,80,0],[240,220,110]), # yellow\n    ([0,40,50],[110,180,250]), # blue\n    ([0,60,0],[60,160,70]), # green\n]\ncolor_attr = [""\xe9\xbb\x84\xe7\x89\x8c"",""\xe8\x93\x9d\xe7\x89\x8c"",\'\xe7\xbb\xbf\xe7\x89\x8c\',\'\xe7\x99\xbd\xe7\x89\x8c\',\'\xe9\xbb\x91\xe7\x89\x8c\']\n\nthrehold_green = 13\nthrehold_blue = 13\nthrehold_yellow1 = 50\nthrehold_yellow2 = 70\n\n# plt.figure()\n# plt.axis(""off"")\n# plt.imshow(image)\n# plt.show()\n\nimport numpy as np\ndef centroid_histogram(clt):\n    numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)\n    (hist, _) = np.histogram(clt.labels_, bins=numLabels)\n\n    # normalize the histogram, such that it sums to one\n    hist = hist.astype(""float"")\n    hist /= hist.sum()\n\n    # return the histogram\n    return hist\n\n\ndef plot_colors(hist, centroids):\n    bar = np.zeros((50, 300, 3), dtype=""uint8"")\n    startX = 0\n\n    for (percent, color) in zip(hist, centroids):\n\n        endX = startX + (percent * 300)\n        cv2.rectangle(bar, (int(startX), 0), (int(endX), 50),\n                      color.astype(""uint8"").tolist(), -1)\n        startX = endX\n\n    # return the bar chart\n    return bar\n\ndef search_boundaries(color):\n    for i,color_bound in enumerate(boundaries):\n        if np.all(color >= color_bound[0]) and np.all(color <= color_bound[1]):\n            return i\n    return -1\n\ndef judge_color(color):\n    r = color[0]\n    g = color[1]\n    b = color[2]\n    if g - r >= threhold_green and g - b >= threhold_green:\n        return 2\n    if b - r >= threhold_blue and b - g >= threhold_blue:\n        return 1\n    if r- b > threhold_yellow2 and g - b > threhold_yellow2:\n        return 0\n    if r > 200 and b > 200 and g > 200:\n        return 3\n    if r < 50 and b < 50 and g < 50:\n        return 4\n    return -1\n\ndef judge_plate_color(img):\n    image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    image = image.reshape((image.shape[0] * image.shape[1], 3))\n    clt = KMeans(n_clusters=2)\n    clt.fit(image)\n\n    hist = centroid_histogram(clt)\n    index = np.argmax(hist)\n    #print clt.cluster_centers_[index]\n    #color_index = search_boundaries(clt.cluster_centers_[index])\n    color_index = judge_color(clt.cluster_centers_[index])\n    if color_index == -1:\n        if index == 0:\n            secound_index = 1\n        else:\n            secound_index = 0\n        color_index = judge_color(clt.cluster_centers_[secound_index])\n\n    if color_index == -1:\n        print(clt.cluster_centers_)\n        bar = plot_colors(hist, clt.cluster_centers_)\n        # show our color bart\n        plt.figure()\n        plt.axis(""off"")\n        plt.imshow(bar)\n        plt.show()\n\n    if color_index != -1:\n        return color_attr[color_index],clt.cluster_centers_[index]\n    else:\n        return None,clt.cluster_centers_[index]'"
hyperlpr_py3/config.py,0,"b'import json\n\n\n\nwith open(""/Users/universe/ProgramUniverse/zeusees/HyperLPR/config.json"") as f:\n    configuration = json.load(f)\n'"
hyperlpr_py3/deskew.py,0,"b'#coding=utf-8\nimport numpy as np\nimport cv2\nimport time\nfrom matplotlib import pyplot as plt\nimport math\n\nfrom scipy.ndimage import filters\n#\n# def strokeFiter():\n#     pass;\n\ndef angle(x,y):\n    return int(math.atan2(float(y),float(x))*180.0/3.1415)\n\n\ndef h_rot(src, angle, scale=1.0):\n    w = src.shape[1]\n    h = src.shape[0]\n    rangle = np.deg2rad(angle)\n    nw = (abs(np.sin(rangle)*h) + abs(np.cos(rangle)*w))*scale\n    nh = (abs(np.cos(rangle)*h) + abs(np.sin(rangle)*w))*scale\n    rot_mat = cv2.getRotationMatrix2D((nw*0.5, nh*0.5), angle, scale)\n    rot_move = np.dot(rot_mat, np.array([(nw-w)*0.5, (nh-h)*0.5,0]))\n    rot_mat[0,2] += rot_move[0]\n    rot_mat[1,2] += rot_move[1]\n    return cv2.warpAffine(src, rot_mat, (int(math.ceil(nw)), int(math.ceil(nh))), flags=cv2.INTER_LANCZOS4)\n    pass\n\n\ndef v_rot(img, angel, shape, max_angel):\n    size_o = [shape[1],shape[0]]\n    size = (shape[1]+ int(shape[0]*np.cos((float(max_angel )/180) * 3.14)),shape[0])\n    interval = abs( int( np.sin((float(angel) /180) * 3.14)* shape[0]))\n    pts1 = np.float32([[0,0],[0,size_o[1]],[size_o[0],0],[size_o[0],size_o[1]]])\n    if(angel>0):\n        pts2 = np.float32([[interval,0],[0,size[1]  ],[size[0],0  ],[size[0]-interval,size_o[1]]])\n    else:\n        pts2 = np.float32([[0,0],[interval,size[1]  ],[size[0]-interval,0  ],[size[0],size_o[1]]])\n\n    M  = cv2.getPerspectiveTransform(pts1,pts2)\n    dst = cv2.warpPerspective(img,M,size)\n    return dst,M\n\n\ndef skew_detection(image_gray):\n    h, w = image_gray.shape[:2]\n    eigen = cv2.cornerEigenValsAndVecs(image_gray,12, 5)\n    angle_sur = np.zeros(180,np.uint)\n    eigen = eigen.reshape(h, w, 3, 2)\n    flow = eigen[:,:,2]\n    vis = image_gray.copy()\n    vis[:] = (192 + np.uint32(vis)) / 2\n    d = 12\n    points =  np.dstack( np.mgrid[d/2:w:d, d/2:h:d] ).reshape(-1, 2)\n    for x, y in points:\n        vx, vy = np.int32(flow[int(y), int(x)]*d)\n        # cv2.line(rgb, (x-vx, y-vy), (x+vx, y+vy), (0, 355, 0), 1, cv2.LINE_AA)\n        ang = angle(vx,vy)\n        angle_sur[(ang+180)%180] +=1\n\n    # torr_bin = 30\n    angle_sur = angle_sur.astype(np.float)\n    angle_sur = (angle_sur-angle_sur.min())/(angle_sur.max()-angle_sur.min())\n    angle_sur = filters.gaussian_filter1d(angle_sur,5)\n    skew_v_val =  angle_sur[20:180-20].max()\n    skew_v = angle_sur[30:180-30].argmax() + 30\n    skew_h_A = angle_sur[0:30].max()\n    skew_h_B = angle_sur[150:180].max()\n    skew_h = 0\n    if (skew_h_A > skew_v_val*0.3 or skew_h_B > skew_v_val*0.3):\n        if skew_h_A>=skew_h_B:\n            skew_h = angle_sur[0:20].argmax()\n        else:\n            skew_h = - angle_sur[160:180].argmax()\n    return skew_h,skew_v\n\n\ndef fastDeskew(image):\n    image_gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n    skew_h,skew_v = skew_detection(image_gray)\n    print(""\xe6\xa0\xa1\xe6\xad\xa3\xe8\xa7\x92\xe5\xba\xa6 h "",skew_h,""v"",skew_v)\n    deskew,M = v_rot(image,int((90-skew_v)*1.5),image.shape,60)\n    return deskew,M\n\n\n\nif __name__ == \'__main__\':\n    fn = \'./dataset/0.jpg\'\n\n    img = cv2.imread(fn)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    skew_h,skew_v = skew_detection(gray)\n    img = v_rot(img,(90-skew_v ),img.shape,60)\n    # img = h_rot(img,skew_h)\n    # if img.shape[0]>img.shape[1]:\n    #     img = h_rot(img, -90)\n\n    plt.show()\n    cv2.waitKey()\n'"
hyperlpr_py3/detect.py,0,"b'\nimport cv2\nimport numpy as np\n\n\n\nwatch_cascade = cv2.CascadeClassifier(\'./model/cascade.xml\')\n\n\ndef computeSafeRegion(shape,bounding_rect):\n    top = bounding_rect[1] # y\n    bottom  = bounding_rect[1] + bounding_rect[3] # y +  h\n    left = bounding_rect[0] # x\n    right =   bounding_rect[0] + bounding_rect[2] # x +  w\n\n    min_top = 0\n    max_bottom = shape[0]\n    min_left = 0\n    max_right = shape[1]\n\n    # print ""computeSateRegion input shape"",shape\n    if top < min_top:\n        top = min_top\n        # print ""tap top 0""\n    if left < min_left:\n        left = min_left\n        # print ""tap left 0""\n\n    if bottom > max_bottom:\n        bottom = max_bottom\n        #print ""tap max_bottom max""\n    if right > max_right:\n        right = max_right\n        #print ""tap max_right max""\n\n    # print ""corr"",left,top,right,bottom\n    return [left,top,right-left,bottom-top]\n\n\ndef cropped_from_image(image,rect):\n    x, y, w, h = computeSafeRegion(image.shape,rect)\n    return image[y:y+h,x:x+w]\n\n\ndef detectPlateRough(image_gray,resize_h = 720,en_scale =1.08 ,top_bottom_padding_rate = 0.05):\n    print(image_gray.shape)\n\n    if top_bottom_padding_rate>0.2:\n        print(""error:top_bottom_padding_rate > 0.2:"",top_bottom_padding_rate)\n        exit(1)\n\n    height = image_gray.shape[0]\n    padding =    int(height*top_bottom_padding_rate)\n    scale = image_gray.shape[1]/float(image_gray.shape[0])\n\n    image = cv2.resize(image_gray, (int(scale*resize_h), resize_h))\n\n    image_color_cropped = image[padding:resize_h-padding,0:image_gray.shape[1]]\n\n    image_gray = cv2.cvtColor(image_color_cropped,cv2.COLOR_RGB2GRAY)\n\n    watches = watch_cascade.detectMultiScale(image_gray, en_scale, 2, minSize=(36, 9),maxSize=(36*40, 9*40))\n\n    cropped_images = []\n    for (x, y, w, h) in watches:\n        cropped_origin = cropped_from_image(image_color_cropped, (int(x), int(y), int(w), int(h)))\n        x -= w * 0.14\n        w += w * 0.28\n        y -= h * 0.6\n        h += h * 1.1;\n\n        cropped = cropped_from_image(image_color_cropped, (int(x), int(y), int(w), int(h)))\n\n\n        cropped_images.append([cropped,[x, y+padding, w, h],cropped_origin])\n    return cropped_images\n'"
hyperlpr_py3/e2e.py,0,"b'#coding=utf-8\nfrom keras import backend as K\nfrom keras.models import load_model\nfrom keras.layers import *\nimport numpy as np\nimport random\nimport string\n\nimport cv2\nfrom . import e2emodel as model\nchars = [""\xe4\xba\xac"", ""\xe6\xb2\xaa"", ""\xe6\xb4\xa5"", ""\xe6\xb8\x9d"", ""\xe5\x86\x80"", ""\xe6\x99\x8b"", ""\xe8\x92\x99"", ""\xe8\xbe\xbd"", ""\xe5\x90\x89"", ""\xe9\xbb\x91"", ""\xe8\x8b\x8f"", ""\xe6\xb5\x99"", ""\xe7\x9a\x96"", ""\xe9\x97\xbd"", ""\xe8\xb5\xa3"", ""\xe9\xb2\x81"", ""\xe8\xb1\xab"", ""\xe9\x84\x82"", ""\xe6\xb9\x98"", ""\xe7\xb2\xa4"", ""\xe6\xa1\x82"",\n             ""\xe7\x90\xbc"", ""\xe5\xb7\x9d"", ""\xe8\xb4\xb5"", ""\xe4\xba\x91"", ""\xe8\x97\x8f"", ""\xe9\x99\x95"", ""\xe7\x94\x98"", ""\xe9\x9d\x92"", ""\xe5\xae\x81"", ""\xe6\x96\xb0"", ""0"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""A"",\n             ""B"", ""C"", ""D"", ""E"", ""F"", ""G"", ""H"", ""J"", ""K"", ""L"", ""M"", ""N"", ""P"", ""Q"", ""R"", ""S"", ""T"", ""U"", ""V"", ""W"", ""X"",\n             ""Y"", ""Z"",""\xe6\xb8\xaf"",""\xe5\xad\xa6"",""\xe4\xbd\xbf"",""\xe8\xad\xa6"",""\xe6\xbe\xb3"",""\xe6\x8c\x82"",""\xe5\x86\x9b"",""\xe5\x8c\x97"",""\xe5\x8d\x97"",""\xe5\xb9\xbf"",""\xe6\xb2\x88"",""\xe5\x85\xb0"",""\xe6\x88\x90"",""\xe6\xb5\x8e"",""\xe6\xb5\xb7"",""\xe6\xb0\x91"",""\xe8\x88\xaa"",""\xe7\xa9\xba""\n             ];\npred_model = model.construct_model(""./model/ocr_plate_all_w_rnn_2.h5"",)\nimport time\n\n\n\ndef fastdecode(y_pred):\n    results = """"\n    confidence = 0.0\n    table_pred = y_pred.reshape(-1, len(chars)+1)\n\n    res = table_pred.argmax(axis=1)\n\n    for i,one in enumerate(res):\n        if one<len(chars) and (i==0 or (one!=res[i-1])):\n            results+= chars[one]\n            confidence+=table_pred[i][one]\n    confidence/= len(results)\n    return results,confidence\n\ndef recognizeOne(src):\n    # x_tempx= cv2.imread(src)\n    x_tempx = src\n    # x_tempx = cv2.bitwise_not(x_tempx)\n    x_temp = cv2.resize(x_tempx,( 160,40))\n    x_temp = x_temp.transpose(1, 0, 2)\n    t0 = time.time()\n    y_pred = pred_model.predict(np.array([x_temp]))\n    y_pred = y_pred[:,2:,:]\n    # plt.imshow(y_pred.reshape(16,66))\n    # plt.show()\n\n    #\n    # cv2.imshow(""x_temp"",x_tempx)\n    # cv2.waitKey(0)\n    return fastdecode(y_pred)\n#\n#\n# import os\n#\n# path = ""/Users/yujinke/PycharmProjects/HyperLPR_Python_web/cache/finemapping""\n# for filename in os.listdir(path):\n#     if filename.endswith("".png"") or filename.endswith("".jpg"") or filename.endswith("".bmp""):\n#         x = os.path.join(path,filename)\n#         recognizeOne(x)\n#         # print time.time() - t0\n#\n#         # cv2.imshow(""x"",x)\n#         # cv2.waitKey()\n'"
hyperlpr_py3/e2emodel.py,0,"b'\nfrom keras import backend as K\nfrom keras.models import *\nfrom keras.layers import *\nfrom . import e2e\n\n\ndef ctc_lambda_func(args):\n    y_pred, labels, input_length, label_length = args\n    y_pred = y_pred[:, 2:, :]\n    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n\n\ndef construct_model(model_path):\n    input_tensor = Input((None, 40, 3))\n    x = input_tensor\n    base_conv = 32\n\n    for i in range(3):\n        x = Conv2D(base_conv * (2 ** (i)), (3, 3),padding=""same"")(x)\n        x = BatchNormalization()(x)\n        x = Activation(\'relu\')(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = Conv2D(256, (5, 5))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\'relu\')(x)\n    x = Conv2D(1024, (1, 1))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\'relu\')(x)\n    x = Conv2D(len(e2e.chars)+1, (1, 1))(x)\n    x = Activation(\'softmax\')(x)\n    base_model = Model(inputs=input_tensor, outputs=x)\n    base_model.load_weights(model_path)\n    return base_model\n'"
hyperlpr_py3/finemapping.py,0,"b'#coding=utf-8\nimport cv2\nimport numpy as np\n\n\nfrom . import niblack_thresholding as nt\n\nfrom . import deskew\n\ndef fitLine_ransac(pts,zero_add = 0 ):\n    if len(pts)>=2:\n        [vx, vy, x, y] = cv2.fitLine(pts, cv2.DIST_HUBER, 0, 0.01, 0.01)\n        lefty = int((-x * vy / vx) + y)\n        righty = int(((136- x) * vy / vx) + y)\n        return lefty+30+zero_add,righty+30+zero_add\n    return 0,0\n\n\n\n#\xe7\xb2\xbe\xe5\xae\x9a\xe4\xbd\x8d\xe7\xae\x97\xe6\xb3\x95\ndef findContoursAndDrawBoundingBox(image_rgb):\n\n\n    line_upper  = [];\n    line_lower = [];\n\n    line_experiment = []\n    grouped_rects = []\n    gray_image = cv2.cvtColor(image_rgb,cv2.COLOR_BGR2GRAY)\n\n    # for k in np.linspace(-1.5, -0.2,10):\n    for k in np.linspace(-50, 0, 15):\n\n        # thresh_niblack = threshold_niblack(gray_image, window_size=21, k=k)\n        # binary_niblack = gray_image > thresh_niblack\n        # binary_niblack = binary_niblack.astype(np.uint8) * 255\n\n        binary_niblack = cv2.adaptiveThreshold(gray_image,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,17,k)\n        # cv2.imshow(""image1"",binary_niblack)\n        # cv2.waitKey(0)\n        imagex, contours, hierarchy = cv2.findContours(binary_niblack.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n        for contour in contours:\n            bdbox = cv2.boundingRect(contour)\n            if (bdbox[3]/float(bdbox[2])>0.7 and bdbox[3]*bdbox[2]>100 and bdbox[3]*bdbox[2]<1200) or (bdbox[3]/float(bdbox[2])>3 and bdbox[3]*bdbox[2]<100):\n                # cv2.rectangle(rgb,(bdbox[0],bdbox[1]),(bdbox[0]+bdbox[2],bdbox[1]+bdbox[3]),(255,0,0),1)\n                line_upper.append([bdbox[0],bdbox[1]])\n                line_lower.append([bdbox[0]+bdbox[2],bdbox[1]+bdbox[3]])\n\n                line_experiment.append([bdbox[0],bdbox[1]])\n                line_experiment.append([bdbox[0]+bdbox[2],bdbox[1]+bdbox[3]])\n                # grouped_rects.append(bdbox)\n\n    rgb = cv2.copyMakeBorder(image_rgb,30,30,0,0,cv2.BORDER_REPLICATE)\n    leftyA, rightyA = fitLine_ransac(np.array(line_lower),3)\n    rows,cols = rgb.shape[:2]\n\n    # rgb = cv2.line(rgb, (cols - 1, rightyA), (0, leftyA), (0, 0, 255), 1,cv2.LINE_AA)\n\n    leftyB, rightyB = fitLine_ransac(np.array(line_upper),-3)\n\n    rows,cols = rgb.shape[:2]\n\n    # rgb = cv2.line(rgb, (cols - 1, rightyB), (0, leftyB), (0,255, 0), 1,cv2.LINE_AA)\n    pts_map1  = np.float32([[cols - 1, rightyA], [0, leftyA],[cols - 1, rightyB], [0, leftyB]])\n    pts_map2 = np.float32([[136,36],[0,36],[136,0],[0,0]])\n    mat = cv2.getPerspectiveTransform(pts_map1,pts_map2)\n    image = cv2.warpPerspective(rgb,mat,(136,36),flags=cv2.INTER_CUBIC)\n    image,M = deskew.fastDeskew(image)\n\n    return image\n\n\n\n#\xe5\xa4\x9a\xe7\xba\xa7\ndef findContoursAndDrawBoundingBox2(image_rgb):\n\n\n    line_upper  = [];\n    line_lower = [];\n\n    line_experiment = []\n\n    grouped_rects = []\n\n    gray_image = cv2.cvtColor(image_rgb,cv2.COLOR_BGR2GRAY)\n\n    for k in np.linspace(-1.6, -0.2,10):\n    # for k in np.linspace(-15, 0, 15):\n    # #\n    #     thresh_niblack = threshold_niblack(gray_image, window_size=21, k=k)\n    #     binary_niblack = gray_image > thresh_niblack\n    #     binary_niblack = binary_niblack.astype(np.uint8) * 255\n\n        binary_niblack = nt.niBlackThreshold(gray_image,19,k)\n        # cv2.imshow(""binary_niblack_opencv"",binary_niblack_)\n        # cv2.imshow(""binary_niblack_skimage"", binary_niblack)\n\n        # cv2.waitKey(0)\n        imagex, contours, hierarchy = cv2.findContours(binary_niblack.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n\n        for contour in contours:\n            bdbox = cv2.boundingRect(contour)\n            if (bdbox[3]/float(bdbox[2])>0.7 and bdbox[3]*bdbox[2]>100 and bdbox[3]*bdbox[2]<1000) or (bdbox[3]/float(bdbox[2])>3 and bdbox[3]*bdbox[2]<100):\n                # cv2.rectangle(rgb,(bdbox[0],bdbox[1]),(bdbox[0]+bdbox[2],bdbox[1]+bdbox[3]),(255,0,0),1)\n                line_upper.append([bdbox[0],bdbox[1]])\n                line_lower.append([bdbox[0]+bdbox[2],bdbox[1]+bdbox[3]])\n\n                line_experiment.append([bdbox[0],bdbox[1]])\n                line_experiment.append([bdbox[0]+bdbox[2],bdbox[1]+bdbox[3]])\n                # grouped_rects.append(bdbox)\n\n    rgb = cv2.copyMakeBorder(image_rgb,30,30,0,0,cv2.BORDER_REPLICATE)\n    leftyA, rightyA = fitLine_ransac(np.array(line_lower),2)\n    rows,cols = rgb.shape[:2]\n\n    # rgb = cv2.line(rgb, (cols - 1, rightyA), (0, leftyA), (0, 0, 255), 1,cv2.LINE_AA)\n\n    leftyB, rightyB = fitLine_ransac(np.array(line_upper),-4)\n\n    rows,cols = rgb.shape[:2]\n\n    # rgb = cv2.line(rgb, (cols - 1, rightyB), (0, leftyB), (0,255, 0), 1,cv2.LINE_AA)\n    pts_map1  = np.float32([[cols - 1, rightyA], [0, leftyA],[cols - 1, rightyB], [0, leftyB]])\n    pts_map2 = np.float32([[136,36],[0,36],[136,0],[0,0]])\n    mat = cv2.getPerspectiveTransform(pts_map1,pts_map2)\n    image = cv2.warpPerspective(rgb,mat,(136,36),flags=cv2.INTER_CUBIC)\n    image,M= deskew.fastDeskew(image)\n\n\n    return image\n'"
hyperlpr_py3/finemapping_vertical.py,0,"b'#coding=utf-8\nfrom keras.layers import Conv2D, Input,MaxPool2D, Reshape,Activation,Flatten, Dense\nfrom keras.models import Model, Sequential\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.optimizers import adam\nimport numpy as np\n\nimport cv2\n\ndef getModel():\n    input = Input(shape=[16, 66, 3])  # change this shape to [None,None,3] to enable arbitraty shape input\n    x = Conv2D(10, (3, 3), strides=1, padding=\'valid\', name=\'conv1\')(input)\n    x = Activation(""relu"", name=\'relu1\')(x)\n    x = MaxPool2D(pool_size=2)(x)\n    x = Conv2D(16, (3, 3), strides=1, padding=\'valid\', name=\'conv2\')(x)\n    x = Activation(""relu"", name=\'relu2\')(x)\n    x = Conv2D(32, (3, 3), strides=1, padding=\'valid\', name=\'conv3\')(x)\n    x = Activation(""relu"", name=\'relu3\')(x)\n    x = Flatten()(x)\n    output = Dense(2,name = ""dense"")(x)\n    output = Activation(""relu"", name=\'relu4\')(output)\n    model = Model([input], [output])\n    return model\n\n\n\nmodel = getModel()\nmodel.load_weights(""./model/model12.h5"")\n\n\ndef getmodel():\n    return model\n\ndef gettest_model():\n    input = Input(shape=[16, 66, 3])  # change this shape to [None,None,3] to enable arbitraty shape input\n    A = Conv2D(10, (3, 3), strides=1, padding=\'valid\', name=\'conv1\')(input)\n    B = Activation(""relu"", name=\'relu1\')(A)\n    C = MaxPool2D(pool_size=2)(B)\n    x = Conv2D(16, (3, 3), strides=1, padding=\'valid\', name=\'conv2\')(C)\n    x = Activation(""relu"", name=\'relu2\')(x)\n    x = Conv2D(32, (3, 3), strides=1, padding=\'valid\', name=\'conv3\')(x)\n    K = Activation(""relu"", name=\'relu3\')(x)\n\n\n    x = Flatten()(K)\n    dense = Dense(2,name = ""dense"")(x)\n    output = Activation(""relu"", name=\'relu4\')(dense)\n    x = Model([input], [output])\n    x.load_weights(""./model/model12.h5"")\n    ok = Model([input], [dense])\n\n    for layer in ok.layers:\n        print(layer)\n\n    return ok\n\n\n\n\ndef finemappingVertical(image):\n    resized = cv2.resize(image,(66,16))\n    resized = resized.astype(np.float)/255\n    res= model.predict(np.array([resized]))[0]\n    print(""keras_predict"",res)\n    res  =res*image.shape[1]\n    res = res.astype(np.int)\n    H,T = res\n    H-=3\n    #3 79.86\n    #4 79.3\n    #5 79.5\n    #6 78.3\n\n\n    #T\n    #T+1 80.9\n    #T+2 81.75\n    #T+3 81.75\n\n\n\n    if H<0:\n        H=0\n    T+=2;\n\n    if T>= image.shape[1]-1:\n        T= image.shape[1]-1\n\n    image = image[0:35,H:T+2]\n\n    image = cv2.resize(image, (int(136), int(36)))\n    return image'"
hyperlpr_py3/niblack_thresholding.py,0,"b'import cv2\nimport numpy as np\n\n\n\ndef niBlackThreshold(  src,  blockSize,  k,  binarizationMethod= 0 ):\n    mean = cv2.boxFilter(src,cv2.CV_32F,(blockSize, blockSize),borderType=cv2.BORDER_REPLICATE)\n    sqmean = cv2.sqrBoxFilter(src, cv2.CV_32F, (blockSize, blockSize), borderType = cv2.BORDER_REPLICATE)\n    variance = sqmean - (mean*mean)\n    stddev  = np.sqrt(variance)\n    thresh = mean + stddev * float(-k)\n    thresh = thresh.astype(src.dtype)\n    k = (src>thresh)*255\n    k = k.astype(np.uint8)\n    return k\n\n\n# cv2.imshow()'"
hyperlpr_py3/pipline.py,0,"b'#coding=utf-8\nfrom . import detect\nfrom . import  finemapping  as  fm\n\nfrom . import segmentation\nimport cv2\n\nimport time\nimport numpy as np\n\nfrom PIL import ImageFont\nfrom PIL import Image\nfrom PIL import ImageDraw\nimport json\n\nimport sys\nfrom . import typeDistinguish as td\nimport imp\n\n\nimp.reload(sys)\nfontC = ImageFont.truetype(""./Font/platech.ttf"", 14, 0);\n\nfrom . import e2e\n#\xe5\xaf\xbb\xe6\x89\xbe\xe8\xbd\xa6\xe7\x89\x8c\xe5\xb7\xa6\xe5\x8f\xb3\xe8\xbe\xb9\xe7\x95\x8c\n\ndef find_edge(image):\n    sum_i = image.sum(axis=0)\n    sum_i =  sum_i.astype(np.float)\n    sum_i/=image.shape[0]*255\n    # print sum_i\n\n    start= 0 ;\n    end = image.shape[1]-1\n\n    for i,one in enumerate(sum_i):\n        if one>0.4:\n            start = i;\n            if start-3<0:\n                start = 0\n            else:\n                start -=3\n\n            break;\n    for i,one in enumerate(sum_i[::-1]):\n\n        if one>0.4:\n            end = end - i;\n            if end+4>image.shape[1]-1:\n                end = image.shape[1]-1\n            else:\n                end+=4\n            break\n    return start,end\n\n\n#\xe5\x9e\x82\xe7\x9b\xb4\xe8\xbe\xb9\xe7\xbc\x98\xe6\xa3\x80\xe6\xb5\x8b\ndef verticalEdgeDetection(image):\n    image_sobel = cv2.Sobel(image.copy(),cv2.CV_8U,1,0)\n    # image = auto_canny(image_sobel)\n\n    # img_sobel, CV_8U, 1, 0, 3, 1, 0, BORDER_DEFAULT\n    # canny_image  = auto_canny(image)\n    flag,thres = cv2.threshold(image_sobel,0,255,cv2.THRESH_OTSU|cv2.THRESH_BINARY)\n    print(flag)\n    flag,thres = cv2.threshold(image_sobel,int(flag*0.7),255,cv2.THRESH_BINARY)\n    # thres = simpleThres(image_sobel)\n    kernal = np.ones(shape=(3,15))\n    thres = cv2.morphologyEx(thres,cv2.MORPH_CLOSE,kernal)\n    return thres\n\n\n#\xe7\xa1\xae\xe5\xae\x9a\xe7\xb2\x97\xe7\x95\xa5\xe7\x9a\x84\xe5\xb7\xa6\xe5\x8f\xb3\xe8\xbe\xb9\xe7\x95\x8c\ndef horizontalSegmentation(image):\n\n    thres = verticalEdgeDetection(image)\n    # thres = thres*image\n    head,tail = find_edge(thres)\n    # print head,tail\n    # cv2.imshow(""edge"",thres)\n    tail = tail+5\n    if tail>135:\n        tail = 135\n    image = image[0:35,head:tail]\n    image = cv2.resize(image, (int(136), int(36)))\n    return image\n\n\n#\xe6\x89\x93\xe4\xb8\x8aboundingbox\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe\ndef drawRectBox(image,rect,addText):\n    cv2.rectangle(image, (int(rect[0]), int(rect[1])), (int(rect[0] + rect[2]), int(rect[1] + rect[3])), (0,0, 255), 2, cv2.LINE_AA)\n    cv2.rectangle(image, (int(rect[0]-1), int(rect[1])-16), (int(rect[0] + 115), int(rect[1])), (0, 0, 255), -1, cv2.LINE_AA)\n\n    img = Image.fromarray(image)\n    draw = ImageDraw.Draw(img)\n    #draw.text((int(rect[0]+1), int(rect[1]-16)), addText.decode(""utf-8""), (255, 255, 255), font=fontC)\n    draw.text((int(rect[0]+1), int(rect[1]-16)), addText, (255, 255, 255), font=fontC)\n    imagex = np.array(img)\n\n    return imagex\n\n\nfrom . import cache\nfrom . import finemapping_vertical as fv\n\ndef RecognizePlateJson(image):\n    images = detect.detectPlateRough(image,image.shape[0],top_bottom_padding_rate=0.1)\n    jsons = []\n    for j,plate in enumerate(images):\n        plate,rect,origin_plate =plate\n        res, confidence = e2e.recognizeOne(origin_plate)\n        print(""res"",res)\n\n        cv2.imwrite(""./""+str(j)+""_rough.jpg"",plate)\n\n        # print ""\xe8\xbd\xa6\xe7\x89\x8c\xe7\xb1\xbb\xe5\x9e\x8b:"",ptype\n        # plate = cv2.cvtColor(plate, cv2.COLOR_RGB2GRAY)\n        plate  =cv2.resize(plate,(136,int(36*2.5)))\n        t1 = time.time()\n\n        ptype = td.SimplePredict(plate)\n        if ptype>0 and ptype<4:\n            plate = cv2.bitwise_not(plate)\n        # demo = verticalEdgeDetection(plate)\n\n        image_rgb = fm.findContoursAndDrawBoundingBox(plate)\n        image_rgb = fv.finemappingVertical(image_rgb)\n        cache.verticalMappingToFolder(image_rgb)\n        # print time.time() - t1,""\xe6\xa0\xa1\xe6\xad\xa3""\n        print(""e2e:"",e2e.recognizeOne(image_rgb)[0])\n        image_gray = cv2.cvtColor(image_rgb,cv2.COLOR_BGR2GRAY)\n\n        cv2.imwrite(""./""+str(j)+"".jpg"",image_gray)\n        # image_gray = horizontalSegmentation(image_gray)\n\n        t2 = time.time()\n        res, confidence = e2e.recognizeOne(image_rgb)\n        res_json = {}\n        if confidence  > 0.6:\n            res_json[""Name""] = res\n            res_json[""Type""] = td.plateType[ptype]\n            res_json[""Confidence""] = confidence;\n            res_json[""x""] = int(rect[0])\n            res_json[""y""] = int(rect[1])\n            res_json[""w""] = int(rect[2])\n            res_json[""h""] = int(rect[3])\n            jsons.append(res_json)\n    print(json.dumps(jsons,ensure_ascii=False))\n\n    return json.dumps(jsons,ensure_ascii=False)\n\n\n\n\ndef SimpleRecognizePlateByE2E(image):\n    t0 = time.time()\n    images = detect.detectPlateRough(image,image.shape[0],top_bottom_padding_rate=0.1)\n    res_set = []\n    for j,plate in enumerate(images):\n        plate, rect, origin_plate  =plate\n        # plate = cv2.cvtColor(plate, cv2.COLOR_RGB2GRAY)\n        plate  =cv2.resize(plate,(136,36*2))\n        res,confidence = e2e.recognizeOne(origin_plate)\n        print(""res"",res)\n\n        t1 = time.time()\n        ptype = td.SimplePredict(plate)\n        if ptype>0 and ptype<5:\n            # pass\n            plate = cv2.bitwise_not(plate)\n        image_rgb = fm.findContoursAndDrawBoundingBox(plate)\n        image_rgb = fv.finemappingVertical(image_rgb)\n        image_rgb = fv.finemappingVertical(image_rgb)\n        cache.verticalMappingToFolder(image_rgb)\n        #cv2.imwrite(""./""+str(j)+"".jpg"",image_rgb)\n        res,confidence = e2e.recognizeOne(image_rgb)\n        print(res,confidence)\n        res_set.append([[],res,confidence])\n\n        if confidence>0.7:\n            image = drawRectBox(image, rect, res+"" ""+str(round(confidence,3)))\n    return image,res_set\n\n\ndef SimpleRecognizePlate(image):\n    t0 = time.time()\n    images = detect.detectPlateRough(image,image.shape[0],top_bottom_padding_rate=0.1)\n    res_set = []\n    for j,plate in enumerate(images):\n        plate, rect, origin_plate  =plate\n        # plate = cv2.cvtColor(plate, cv2.COLOR_RGB2GRAY)\n        plate  =cv2.resize(plate,(136,36*2))\n        t1 = time.time()\n\n        ptype = td.SimplePredict(plate)\n        if ptype>0 and ptype<5:\n            plate = cv2.bitwise_not(plate)\n\n        image_rgb = fm.findContoursAndDrawBoundingBox(plate)\n\n        image_rgb = fv.finemappingVertical(image_rgb)\n        cache.verticalMappingToFolder(image_rgb)\n        print(""e2e:"", e2e.recognizeOne(image_rgb))\n        image_gray = cv2.cvtColor(image_rgb,cv2.COLOR_RGB2GRAY)\n\n        # image_gray = horizontalSegmentation(image_gray)\n        cv2.imshow(""image_gray"",image_gray)\n        # cv2.waitKey()\n\n        cv2.imwrite(""./""+str(j)+"".jpg"",image_gray)\n        # cv2.imshow(""image"",image_gray)\n        # cv2.waitKey(0)\n        print(""\xe6\xa0\xa1\xe6\xad\xa3"",time.time() - t1,""s"")\n        # cv2.imshow(""image,"",image_gray)\n        # cv2.waitKey(0)\n        t2 = time.time()\n        val = segmentation.slidingWindowsEval(image_gray)\n        # print val\n        print(""\xe5\x88\x86\xe5\x89\xb2\xe5\x92\x8c\xe8\xaf\x86\xe5\x88\xab"",time.time() - t2,""s"")\n        if len(val)==3:\n            blocks, res, confidence = val\n            if confidence/7>0.7:\n                image =  drawRectBox(image,rect,res)\n                res_set.append(res)\n                for i,block in enumerate(blocks):\n\n                    block_ = cv2.resize(block,(25,25))\n                    block_ = cv2.cvtColor(block_,cv2.COLOR_GRAY2BGR)\n                    image[j * 25:(j * 25) + 25, i * 25:(i * 25) + 25] = block_\n                    if image[j*25:(j*25)+25,i*25:(i*25)+25].shape == block_.shape:\n                        pass\n\n\n            if confidence>0:\n                print(""\xe8\xbd\xa6\xe7\x89\x8c:"",res,""\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6:"",confidence/7)\n            else:\n                pass\n\n                # print ""\xe4\xb8\x8d\xe7\xa1\xae\xe5\xae\x9a\xe7\x9a\x84\xe8\xbd\xa6\xe7\x89\x8c:"", res, ""\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6:"", confidence\n\n    print(time.time() - t0,""s"")\n    return image,res_set\n\n\n\n\n'"
hyperlpr_py3/plateStructure.py,0,b''
hyperlpr_py3/precise.py,0,b''
hyperlpr_py3/recognizer.py,0,"b'#coding=utf-8\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D,MaxPool2D\nfrom keras.optimizers import SGD\nfrom keras import backend as K\n\nK.image_data_format()\n\n\nimport cv2\nimport numpy as np\n\n\n\nindex = {""\xe4\xba\xac"": 0, ""\xe6\xb2\xaa"": 1, ""\xe6\xb4\xa5"": 2, ""\xe6\xb8\x9d"": 3, ""\xe5\x86\x80"": 4, ""\xe6\x99\x8b"": 5, ""\xe8\x92\x99"": 6, ""\xe8\xbe\xbd"": 7, ""\xe5\x90\x89"": 8, ""\xe9\xbb\x91"": 9, ""\xe8\x8b\x8f"": 10, ""\xe6\xb5\x99"": 11, ""\xe7\x9a\x96"": 12,\n         ""\xe9\x97\xbd"": 13, ""\xe8\xb5\xa3"": 14, ""\xe9\xb2\x81"": 15, ""\xe8\xb1\xab"": 16, ""\xe9\x84\x82"": 17, ""\xe6\xb9\x98"": 18, ""\xe7\xb2\xa4"": 19, ""\xe6\xa1\x82"": 20, ""\xe7\x90\xbc"": 21, ""\xe5\xb7\x9d"": 22, ""\xe8\xb4\xb5"": 23, ""\xe4\xba\x91"": 24,\n         ""\xe8\x97\x8f"": 25, ""\xe9\x99\x95"": 26, ""\xe7\x94\x98"": 27, ""\xe9\x9d\x92"": 28, ""\xe5\xae\x81"": 29, ""\xe6\x96\xb0"": 30, ""0"": 31, ""1"": 32, ""2"": 33, ""3"": 34, ""4"": 35, ""5"": 36,\n         ""6"": 37, ""7"": 38, ""8"": 39, ""9"": 40, ""A"": 41, ""B"": 42, ""C"": 43, ""D"": 44, ""E"": 45, ""F"": 46, ""G"": 47, ""H"": 48,\n         ""J"": 49, ""K"": 50, ""L"": 51, ""M"": 52, ""N"": 53, ""P"": 54, ""Q"": 55, ""R"": 56, ""S"": 57, ""T"": 58, ""U"": 59, ""V"": 60,\n         ""W"": 61, ""X"": 62, ""Y"": 63, ""Z"": 64,""\xe6\xb8\xaf"":65,""\xe5\xad\xa6"":66 ,""O"":67 ,""\xe4\xbd\xbf"":68,""\xe8\xad\xa6"":69,""\xe6\xbe\xb3"":70,""\xe6\x8c\x82"":71};\n\nchars = [""\xe4\xba\xac"", ""\xe6\xb2\xaa"", ""\xe6\xb4\xa5"", ""\xe6\xb8\x9d"", ""\xe5\x86\x80"", ""\xe6\x99\x8b"", ""\xe8\x92\x99"", ""\xe8\xbe\xbd"", ""\xe5\x90\x89"", ""\xe9\xbb\x91"", ""\xe8\x8b\x8f"", ""\xe6\xb5\x99"", ""\xe7\x9a\x96"", ""\xe9\x97\xbd"", ""\xe8\xb5\xa3"", ""\xe9\xb2\x81"", ""\xe8\xb1\xab"", ""\xe9\x84\x82"", ""\xe6\xb9\x98"", ""\xe7\xb2\xa4"", ""\xe6\xa1\x82"",\n             ""\xe7\x90\xbc"", ""\xe5\xb7\x9d"", ""\xe8\xb4\xb5"", ""\xe4\xba\x91"", ""\xe8\x97\x8f"", ""\xe9\x99\x95"", ""\xe7\x94\x98"", ""\xe9\x9d\x92"", ""\xe5\xae\x81"", ""\xe6\x96\xb0"", ""0"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""A"",\n             ""B"", ""C"", ""D"", ""E"", ""F"", ""G"", ""H"", ""J"", ""K"", ""L"", ""M"", ""N"", ""P"",\n         ""Q"", ""R"", ""S"", ""T"", ""U"", ""V"", ""W"", ""X"",\n             ""Y"", ""Z"",""\xe6\xb8\xaf"",""\xe5\xad\xa6"",""O"",""\xe4\xbd\xbf"",""\xe8\xad\xa6"",""\xe6\xbe\xb3"",""\xe6\x8c\x82"" ];\n\n\n\ndef Getmodel_tensorflow(nb_classes):\n    # nb_classes = len(charset)\n\n    img_rows, img_cols = 23, 23\n    # number of convolutional filters to use\n    nb_filters = 32\n    # size of pooling area for max pooling\n    nb_pool = 2\n    # convolution kernel size\n    nb_conv = 3\n\n    # x = np.load(\'x.npy\')\n    \n    # y = np_utils.to_categorical(range(3062)*45*5*2, nb_classes)\n    # weight = ((type_class - np.arange(type_class)) / type_class + 1) ** 3\n    # weight = dict(zip(range(3063), weight / weight.mean()))  # \xe8\xb0\x83\xe6\x95\xb4\xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x8c\xe9\xab\x98\xe9\xa2\x91\xe5\xad\x97\xe4\xbc\x98\xe5\x85\x88\n\n    model = Sequential()\n    model.add(Conv2D(32, (5, 5),input_shape=(img_rows, img_cols,1)))\n    model.add(Activation(\'relu\'))\n    model.add(MaxPool2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(32, (3, 3)))\n    model.add(Activation(\'relu\'))\n    model.add(MaxPool2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(512, (3, 3)))\n    # model.add(Activation(\'relu\'))\n    # model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n    # model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(512))\n    model.add(Activation(\'relu\'))\n    model.add(Dropout(0.5))\n    model.add(Dense(nb_classes))\n    model.add(Activation(\'softmax\'))\n    model.compile(loss=\'categorical_crossentropy\',\n                  optimizer=\'adam\',\n                  metrics=[\'accuracy\'])\n    return model\n\n\n\n\ndef Getmodel_ch(nb_classes):\n    # nb_classes = len(charset)\n\n    img_rows, img_cols = 23, 23\n    # number of convolutional filters to use\n    nb_filters = 32\n    # size of pooling area for max pooling\n    nb_pool = 2\n    # convolution kernel size\n    nb_conv = 3\n\n    # x = np.load(\'x.npy\')\n    # y = np_utils.to_categorical(range(3062)*45*5*2, nb_classes)\n    # weight = ((type_class - np.arange(type_class)) / type_class + 1) ** 3\n    # weight = dict(zip(range(3063), weight / weight.mean()))  # \xe8\xb0\x83\xe6\x95\xb4\xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x8c\xe9\xab\x98\xe9\xa2\x91\xe5\xad\x97\xe4\xbc\x98\xe5\x85\x88\n\n    model = Sequential()\n    model.add(Conv2D(32, (5, 5),input_shape=(img_rows, img_cols,1)))\n    model.add(Activation(\'relu\'))\n    model.add(MaxPool2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(32, (3, 3)))\n    model.add(Activation(\'relu\'))\n    model.add(MaxPool2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(512, (3, 3)))\n    # model.add(Activation(\'relu\'))\n    # model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n    # model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(756))\n    model.add(Activation(\'relu\'))\n    model.add(Dropout(0.5))\n    model.add(Dense(nb_classes))\n    model.add(Activation(\'softmax\'))\n    model.compile(loss=\'categorical_crossentropy\',\n                  optimizer=\'adam\',\n                  metrics=[\'accuracy\'])\n    return model\n\n\n\nmodel  = Getmodel_tensorflow(65)\n#\xe6\x9e\x84\xe5\xbb\xba\xe7\xbd\x91\xe7\xbb\x9c\n\nmodel_ch = Getmodel_ch(31)\n\nmodel_ch.load_weights(""./model/char_chi_sim.h5"")\n# model_ch.save_weights(""./model/char_chi_sim.h5"")\nmodel.load_weights(""./model/char_rec.h5"")\n# model.save(""./model/char_rec.h5"")\n\n\ndef SimplePredict(image,pos):\n    image = cv2.resize(image, (23, 23))\n    image = cv2.equalizeHist(image)\n    image = image.astype(np.float) / 255\n    image -= image.mean()\n    image = np.expand_dims(image, 3)\n    if pos!=0:\n        res = np.array(model.predict(np.array([image]))[0])\n    else:\n        res = np.array(model_ch.predict(np.array([image]))[0])\n\n    zero_add = 0 ;\n\n    if pos==0:\n        res = res[:31]\n    elif pos==1:\n        res = res[31+10:65]\n        zero_add = 31+10\n    else:\n        res = res[31:]\n        zero_add = 31\n\n    max_id = res.argmax()\n\n\n    return res.max(),chars[max_id+zero_add],max_id+zero_add\n\n'"
hyperlpr_py3/segmentation.py,0,"b'#coding=utf-8\nimport cv2\nimport numpy as np\n\n# from matplotlib import pyplot as plt\nimport scipy.ndimage.filters as f\nimport scipy\n\nimport time\nimport scipy.signal as l\n\n\n\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPool2D\nfrom keras.optimizers import SGD\nfrom keras import backend as K\n\nK.image_data_format()\n\n\ndef Getmodel_tensorflow(nb_classes):\n    # nb_classes = len(charset)\n    img_rows, img_cols = 23, 23\n    # number of convolutional filters to use\n    nb_filters = 16\n    # size of pooling area for max pooling\n    nb_pool = 2\n    # convolution kernel size\n    nb_conv = 3\n    # x = np.load(\'x.npy\')\n    # y = np_utils.to_categorical(range(3062)*45*5*2, nb_classes)\n    # weight = ((type_class - np.arange(type_class)) / type_class + 1) ** 3\n    # weight = dict(zip(range(3063), weight / weight.mean()))  # \xe8\xb0\x83\xe6\x95\xb4\xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x8c\xe9\xab\x98\xe9\xa2\x91\xe5\xad\x97\xe4\xbc\x98\xe5\x85\x88\n\n    model = Sequential()\n    model.add(Conv2D(nb_filters, (nb_conv, nb_conv),input_shape=(img_rows, img_cols,1)))\n    model.add(Activation(\'relu\'))\n    model.add(MaxPool2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Conv2D(nb_filters, (nb_conv, nb_conv)))\n    model.add(Activation(\'relu\'))\n    model.add(MaxPool2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Flatten())\n    model.add(Dense(256))\n    model.add(Dropout(0.5))\n\n    model.add(Activation(\'relu\'))\n    model.add(Dense(nb_classes))\n    model.add(Activation(\'softmax\'))\n    model.compile(loss=\'categorical_crossentropy\',\n                  optimizer=\'sgd\',\n                  metrics=[\'accuracy\'])\n    return model\n\n\n\ndef Getmodel_tensorflow_light(nb_classes):\n    # nb_classes = len(charset)\n    img_rows, img_cols = 23, 23\n    # number of convolutional filters to use\n    nb_filters = 8\n    # size of pooling area for max pooling\n    nb_pool = 2\n    # convolution kernel size\n    nb_conv = 3\n    # x = np.load(\'x.npy\')\n    # y = np_utils.to_categorical(range(3062)*45*5*2, nb_classes)\n    # weight = ((type_class - np.arange(type_class)) / type_class + 1) ** 3\n    # weight = dict(zip(range(3063), weight / weight.mean()))  # \xe8\xb0\x83\xe6\x95\xb4\xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x8c\xe9\xab\x98\xe9\xa2\x91\xe5\xad\x97\xe4\xbc\x98\xe5\x85\x88\n\n    model = Sequential()\n    model.add(Conv2D(nb_filters, (nb_conv, nb_conv),input_shape=(img_rows, img_cols, 1)))\n    model.add(Activation(\'relu\'))\n    model.add(MaxPool2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Conv2D(nb_filters, (nb_conv * 2, nb_conv * 2)))\n    model.add(Activation(\'relu\'))\n    model.add(MaxPool2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Flatten())\n    model.add(Dense(32))\n    # model.add(Dropout(0.25))\n\n    model.add(Activation(\'relu\'))\n    model.add(Dense(nb_classes))\n    model.add(Activation(\'softmax\'))\n    model.compile(loss=\'categorical_crossentropy\',\n                  optimizer=\'adam\',\n                  metrics=[\'accuracy\'])\n    return model\n\n\n\n\nmodel  = Getmodel_tensorflow_light(3)\nmodel2  = Getmodel_tensorflow(3)\n\nimport os\nmodel.load_weights(""./model/char_judgement1.h5"")\n# model.save(""./model/char_judgement1.h5"")\nmodel2.load_weights(""./model/char_judgement.h5"")\n# model2.save(""./model/char_judgement.h5"")\n\n\nmodel = model2\ndef get_median(data):\n   data = sorted(data)\n   size = len(data)\n   # print size\n\n   if size % 2 == 0: # \xe5\x88\xa4\xe6\x96\xad\xe5\x88\x97\xe8\xa1\xa8\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba\xe5\x81\xb6\xe6\x95\xb0\n    median = (data[size//2]+data[size//2-1])//2\n    data[0] = median\n   if size % 2 == 1: # \xe5\x88\xa4\xe6\x96\xad\xe5\x88\x97\xe8\xa1\xa8\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba\xe5\xa5\x87\xe6\x95\xb0\n    median = data[(size-1)//2]\n    data[0] = median\n   return data[0]\nimport time\n\ndef searchOptimalCuttingPoint(rgb,res_map,start,width_boundingbox,interval_range):\n    t0  = time.time()\n    #\n    # for x in xrange(10):\n    #     res_map = np.vstack((res_map,res_map[-1]))\n    length = res_map.shape[0]\n    refine_s = -2;\n\n    if width_boundingbox>20:\n        refine_s = -9\n    score_list = []\n    interval_big = int(width_boundingbox * 0.3)  #\n    p = 0\n    for zero_add in range(start,start+50,3):\n        # for interval_small in xrange(-0,width_boundingbox/2):\n        for i in range(-8,int(width_boundingbox/1)-8):\n            for refine in range(refine_s, int(width_boundingbox/2+3)):\n                p1 = zero_add# this point is province\n                p2 = p1 + width_boundingbox +refine #\n                p3 = p2 + width_boundingbox + interval_big+i+1\n                p4 = p3 + width_boundingbox +refine\n                p5 = p4 + width_boundingbox +refine\n                p6 = p5 + width_boundingbox +refine\n                p7 = p6 + width_boundingbox +refine\n                if p7>=length:\n                    continue\n                score = res_map[p1][2]*3 -(res_map[p3][1]+res_map[p4][1]+res_map[p5][1]+res_map[p6][1]+res_map[p7][1])+7\n                # print score\n                score_list.append([score,[p1,p2,p3,p4,p5,p6,p7]])\n                p+=1\n    print(p)\n\n    score_list = sorted(score_list , key=lambda x:x[0])\n    # for one in score_list[-1][1]:\n    #     cv2.line(debug,(one,0),(one,36),(255,0,0),1)\n    # #\n    # cv2.imshow(""one"",debug)\n    # cv2.waitKey(0)\n    #\n    print(""\xe5\xaf\xbb\xe6\x89\xbe\xe6\x9c\x80\xe4\xbd\xb3\xe7\x82\xb9"",time.time()-t0)\n    return score_list[-1]\n\n\nimport sys\n\nsys.path.append(\'../\')\nfrom . import recognizer as cRP\nfrom . import niblack_thresholding as nt\n\ndef refineCrop(sections,width=16):\n    new_sections = []\n    for section in sections:\n        # cv2.imshow(""section\xc2\xa1"",section)\n\n        # cv2.blur(section,(3,3),3)\n\n        sec_center = np.array([section.shape[1]/2,section.shape[0]/2])\n        binary_niblack = nt.niBlackThreshold(section,17,-0.255)\n        imagex, contours, hierarchy  = cv2.findContours(binary_niblack,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n        boxs = []\n        for contour in contours:\n            x,y,w,h = cv2.boundingRect(contour)\n\n            ratio = w/float(h)\n            if ratio<1 and h>36*0.4 and y<16\\\n                    :\n                box = [x,y,w,h]\n\n                boxs.append([box,np.array([x+w/2,y+h/2])])\n                # cv2.rectangle(section,(x,y),(x+w,y+h),255,1)\n\n\n\n\n        # print boxs\n\n        dis_ = np.array([ ((one[1]-sec_center)**2).sum() for one in boxs])\n        if len(dis_)==0:\n            kernal = [0, 0, section.shape[1], section.shape[0]]\n        else:\n            kernal = boxs[dis_.argmin()][0]\n\n        center_c  = (kernal[0]+kernal[2]/2,kernal[1]+kernal[3]/2)\n        w_2 = int(width/2)\n        h_2 = kernal[3]/2\n\n        if center_c[0] - w_2< 0:\n            w_2 = center_c[0]\n        new_box = [center_c[0] - w_2,kernal[1],width,kernal[3]]\n        # print new_box[2]/float(new_box[3])\n        if new_box[2]/float(new_box[3])>0.5:\n            # print ""\xe5\xbc\x82\xe5\xb8\xb8""\n            h = int((new_box[2]/0.35 )/2)\n            if h>35:\n                h = 35\n            new_box[1] = center_c[1]- h\n            if new_box[1]<0:\n                new_box[1] = 1\n            new_box[3] = h*2\n        \n        section  = section[int(new_box[1]):int(new_box[1]+new_box[3]), int(new_box[0]):int(new_box[0]+new_box[2])]\n        # cv2.imshow(""section"",section)\n        # cv2.waitKey(0)\n        new_sections.append(section)\n        # print new_box\n    return new_sections\n\n\ndef slidingWindowsEval(image):\n    windows_size = 16;\n    stride = 1\n    height= image.shape[0]\n    t0 = time.time()\n    data_sets = []\n\n    for i in range(0,image.shape[1]-windows_size+1,stride):\n        data = image[0:height,i:i+windows_size]\n        data = cv2.resize(data,(23,23))\n        # cv2.imshow(""image"",data)\n        data = cv2.equalizeHist(data)\n        data = data.astype(np.float)/255\n        data=  np.expand_dims(data,3)\n        data_sets.append(data)\n\n    res = model2.predict(np.array(data_sets))\n    print(""\xe5\x88\x86\xe5\x89\xb2"",time.time() - t0)\n\n    pin = res\n    p = 1 -  (res.T)[1]\n    p = f.gaussian_filter1d(np.array(p,dtype=np.float),3)\n    lmin = l.argrelmax(np.array(p),order = 3)[0]\n    interval = []\n    for i in range(len(lmin)-1):\n        interval.append(lmin[i+1]-lmin[i])\n\n    if(len(interval)>3):\n        mid  = get_median(interval)\n    else:\n        return []\n    pin = np.array(pin)\n    res =  searchOptimalCuttingPoint(image,pin,0,mid,3)\n\n    cutting_pts = res[1]\n    last =  cutting_pts[-1] + mid\n    if last < image.shape[1]:\n        cutting_pts.append(last)\n    else:\n        cutting_pts.append(image.shape[1]-1)\n    name = """"\n    confidence =0.00\n    seg_block = []\n    for x in range(1,len(cutting_pts)):\n        if x != len(cutting_pts)-1 and x!=1:\n            section = image[0:36,cutting_pts[x-1]-2:cutting_pts[x]+2]\n        elif  x==1:\n            c_head = cutting_pts[x - 1]- 2\n            if c_head<0:\n                c_head=0\n            c_tail = cutting_pts[x] + 2\n            section = image[0:36, c_head:c_tail]\n        elif x==len(cutting_pts)-1:\n            end = cutting_pts[x]\n            diff = image.shape[1]-end\n            c_head = cutting_pts[x - 1]\n            c_tail = cutting_pts[x]\n            if diff<7 :\n                section = image[0:36, c_head-5:c_tail+5]\n            else:\n                diff-=1\n                section = image[0:36, c_head - diff:c_tail + diff]\n        elif  x==2:\n            section = image[0:36, cutting_pts[x - 1] - 3:cutting_pts[x-1]+ mid]\n        else:\n            section = image[0:36,cutting_pts[x-1]:cutting_pts[x]]\n        seg_block.append(section)\n    refined = refineCrop(seg_block,mid-1)\n\n    t0 = time.time()\n    for i,one in enumerate(refined):\n        res_pre = cRP.SimplePredict(one, i )\n        # cv2.imshow(str(i),one)\n        # cv2.waitKey(0)\n        confidence+=res_pre[0]\n        name+= res_pre[1]\n    print(""\xe5\xad\x97\xe7\xac\xa6\xe8\xaf\x86\xe5\x88\xab"",time.time() - t0)\n\n    return refined,name,confidence\n'"
hyperlpr_py3/typeDistinguish.py,0,"b'#coding=utf-8\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPool2D\nfrom keras.optimizers import SGD\nfrom keras import backend as K\n\nK.image_data_format()\n\n\nimport cv2\nimport numpy as np\n\n\nplateType  = [""\xe8\x93\x9d\xe7\x89\x8c"",""\xe5\x8d\x95\xe5\xb1\x82\xe9\xbb\x84\xe7\x89\x8c"",""\xe6\x96\xb0\xe8\x83\xbd\xe6\xba\x90\xe8\xbd\xa6\xe7\x89\x8c"",""\xe7\x99\xbd\xe8\x89\xb2"",""\xe9\xbb\x91\xe8\x89\xb2-\xe6\xb8\xaf\xe6\xbe\xb3""]\ndef Getmodel_tensorflow(nb_classes):\n    # nb_classes = len(charset)\n\n    img_rows, img_cols = 9, 34\n    # number of convolutional filters to use\n    nb_filters = 32\n    # size of pooling area for max pooling\n    nb_pool = 2\n    # convolution kernel size\n    nb_conv = 3\n\n    # x = np.load(\'x.npy\')\n    # y = np_utils.to_categorical(range(3062)*45*5*2, nb_classes)\n    # weight = ((type_class - np.arange(type_class)) / type_class + 1) ** 3\n    # weight = dict(zip(range(3063), weight / weight.mean()))  # \xe8\xb0\x83\xe6\x95\xb4\xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x8c\xe9\xab\x98\xe9\xa2\x91\xe5\xad\x97\xe4\xbc\x98\xe5\x85\x88\n\n    model = Sequential()\n    model.add(Conv2D(16, (5, 5),input_shape=(img_rows, img_cols,3)))\n    model.add(Activation(\'relu\'))\n    model.add(MaxPool2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Flatten())\n    model.add(Dense(64))\n    model.add(Activation(\'relu\'))\n    model.add(Dropout(0.5))\n    model.add(Dense(nb_classes))\n    model.add(Activation(\'softmax\'))\n    model.compile(loss=\'categorical_crossentropy\',\n                  optimizer=\'adam\',\n                  metrics=[\'accuracy\'])\n    return model\n\nmodel = Getmodel_tensorflow(5)\nmodel.load_weights(""./model/plate_type.h5"")\nmodel.save(""./model/plate_type.h5"")\ndef SimplePredict(image):\n    image = cv2.resize(image, (34, 9))\n    image = image.astype(np.float) / 255\n    res = np.array(model.predict(np.array([image]))[0])\n    return res.argmax()\n\n\n'"
