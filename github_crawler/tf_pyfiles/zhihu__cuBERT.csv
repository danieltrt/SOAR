file_path,api_count,code
python/cuBERT_benchmark.py,0,"b'# -*- coding: UTF-8 -*-\nimport time\nimport numpy as np\nimport libcubert as cuBERT\n\nmax_batch_size = 128\nbatch_size = 128\nseq_length = 32\nhidden_size = 768\n\noutput_size = batch_size * hidden_size\noutput_type=cuBERT.OutputType.POOLED_OUTPUT\ncompute_type=cuBERT.ComputeType.FLOAT\n\ninput_ids = np.random.randint(0, 21120, size=(batch_size, seq_length), dtype=np.int32)\ninput_mask = np.random.randint(0, 1, size=(batch_size, seq_length), dtype=np.int8)\nsegment_ids = np.random.randint(0, 1, size=(batch_size, seq_length), dtype=np.int8)\noutput = np.zeros([batch_size, hidden_size], dtype=np.float32, order=\'C\')\n\ndef benchmark(model):\n    start = time.time() * 1000\n    model.compute(input_ids, input_mask, segment_ids, output, \n                  output_type=output_type)\n    finish = time.time() * 1000\n    milli = finish - start\n    print(""cuBERT: {} ms"".format(milli))\n\nif __name__ == \'__main__\':\n    model = cuBERT.Model(""../build/bert_frozen_seq32.pb"", max_batch_size, seq_length, 12, 12, compute_type=compute_type)\n    print(""=== warm_up ==="")\n    for i in range(10):\n        benchmark(model)\n    print(""=== benchmark ==="")\n    for i in range(10):\n        benchmark(model)\n'"
python/cuBERT_test.py,0,"b'# -*- coding: UTF-8 -*-\nimport numpy as np\nimport libcubert as cuBERT\n\ncuBERT.get_gpu_count()\n\nmax_batch_size = 128\nbatch_size = 2\nseq_length = 32\n\ninput_ids = np.array([\n    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n    [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n], dtype=np.int32)\n\ninput_mask = np.array([\n    [1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1],\n    [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0]\n], dtype=np.int8)\n\nsegment_ids = np.array([\n    [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0],\n    [0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0]\n], dtype=np.int8)\n\noutput_type=cuBERT.OutputType.LOGITS\ncompute_type=cuBERT.ComputeType.FLOAT\noutput = np.zeros([batch_size], dtype=np.float32, order=\'C\')\n\nnew_output = cuBERT.Output()\nnew_output.logits = np.zeros([batch_size], dtype=np.float32, order=\'C\')\n\nmodel = cuBERT.Model(""../build/bert_frozen_seq32.pb"", max_batch_size, seq_length, 12, 12, \n                     compute_type=compute_type)\nmodel.compute(input_ids, input_mask, segment_ids, output, output_type=output_type)\nnp.testing.assert_almost_equal([-2.9427543, -1.4876306], output, 5)\n\nmodel.compute_m(input_ids, input_mask, segment_ids, new_output)\nnp.testing.assert_almost_equal([-2.9427543, -1.4876306], new_output.logits, 5)\n\n\ntext_a = [u""\xe7\x9f\xa5\xe4\xb9\x8e"", u""\xe7\x9f\xa5\xe4\xb9\x8e""]\ntext_b = [u""\xe5\x9c\xa8\xe5\xae\xb6\xe5\x88\xb7\xe7\x9f\xa5\xe4\xb9\x8e"", u""\xe7\x9f\xa5\xe4\xb9\x8e\xe5\x8f\x91\xe7\x8e\xb0\xe6\x9b\xb4\xe5\xa4\xa7\xe7\x9a\x84\xe4\xb8\x96\xe7\x95\x8c""]\nmodel = cuBERT.Model(""../build/bert_frozen_seq32.pb"", max_batch_size, seq_length, 12, 12, \n                     compute_type=compute_type,\n                     vocab_file=""../build/vocab.txt"")\nmodel.tokenize_compute(text_a, text_b, output, output_type=output_type)\nnp.testing.assert_almost_equal([-2.51366, -1.47348], output, 5)\n\nmodel.tokenize_compute_m(text_a, text_b, new_output)\nnp.testing.assert_almost_equal([-2.51366, -1.47348], new_output.logits, 5)\n'"
python/setup.py,0,"b""import setuptools\nimport sys\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Build import cythonize\n\nimport numpy as np\nimport platform\nimport shutil\n\nextra_compile_args = ['-std=c++11']\nextra_link_args = []\nlibname = 'libcuBERT.so'\nif platform.system() == 'Darwin':\n    extra_compile_args += ['-stdlib=libc++']\n    extra_link_args += ['-stdlib=libc++']\n    libname = 'libcuBERT.dylib'\n\nshutil.copyfile('../build/' + libname, './libcubert/' + libname)\n\nsetup(\n    name='cuBERT',\n    version='0.0.5',\n    author='qinluo',\n    author_email='eric.x.sun@gmail.com',\n    description='python interface for cuBERT',\n    packages=setuptools.find_packages(),\n    package_data={'libcubert': ['libcuBERT.so', 'libcuBERT.dylib']},\n    ext_modules = cythonize([Extension('cuBERT',\n                                       sources=['cuBERT.pyx'],\n                                       libraries=['cuBERT'],\n                                       library_dirs=['../build'],\n                                       include_dirs=[np.get_include()],\n                                       language='c++',\n                                       extra_compile_args=extra_compile_args,\n                                       extra_link_args=extra_link_args)],\n                            compiler_directives={'language_level' : sys.version_info[0]}),\n)\n"""
python/libcubert/__init__.py,0,"b""from ctypes import cdll\nimport os\nimport platform\n\nlibname = {\n    'Linux': 'libcuBERT.so',\n    'Darwin': 'libcuBERT.dylib',\n}\ncdll.LoadLibrary(os.path.join(os.path.abspath(os.path.dirname(__file__)), libname[platform.system()]))\n\nfrom cuBERT import *\n"""
test/cuBERT/bert_test.py,5,"b""import numpy as np\nimport tensorflow as tf\n\nfrozen_graph_filename = '../bert_frozen_seq32.pb'\n\ninput_ids_ = np.array([\n    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n    [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n], dtype=np.int64)\n\ninput_mask_ = np.array([\n    [1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1],\n    [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0]\n], dtype=np.int64)\n\nsegment_ids_ = np.array([\n    [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0],\n    [0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0]\n], dtype=np.int64)\n\nwith tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n\nwith tf.Graph().as_default() as graph:\n    tf.graph_util.import_graph_def(graph_def)\n\n    for op in graph.get_operations():\n        print(op.name, op.values())\n\n    input_ids = graph.get_tensor_by_name('import/input_ids:0')\n    input_mask = graph.get_tensor_by_name('import/input_mask:0')\n    segment_ids = graph.get_tensor_by_name('import/segment_ids:0')\n    output = graph.get_tensor_by_name('import/loss/output:0')\n\n    embedding_output = graph.get_tensor_by_name('import/bert/embeddings/LayerNorm/batchnorm/add_1:0')\n\n    with tf.Session(graph=graph) as sess:\n        output_, embedding_output_ = sess.run((output, embedding_output), feed_dict={\n            input_ids: input_ids_,\n            input_mask: input_mask_,\n            segment_ids: segment_ids_,\n        })\n        print output_\n        print embedding_output_\n"""
test/cuBERT/op_att/attention_self_test.py,3,"b""import numpy as np\nimport tensorflow as tf\n\nfrom modeling import attention_layer\n\nbatch_size = 5\nnum_attention_heads = 1\nsize_per_head = 3\nseq_length = 2\n\nquery_kernel = np.array([\n    [0, 1, 2],\n    [3, 4, 5],\n    [6, 7, 8]\n], dtype=np.float32)\nquery_bias = np.array([-3, -2, -1], dtype=np.float32)\n\nkey_kernel = np.array([\n    [8, 7, 6],\n    [5, 4, 3],\n    [2, 1, 0]\n], dtype=np.float32)\nkey_bias = np.array([0, 1, 2], dtype=np.float32)\n\nvalue_kernel = np.array([\n    [-1, -1, -1],\n    [2, 2, 2],\n    [1, 1, 1]\n], dtype=np.float32)\nvalue_bias = np.array([3, 3, 3], dtype=np.float32)\n\n\nif __name__ == '__main__':\n    tensor = tf.placeholder(tf.float32, shape=[None, seq_length, num_attention_heads * size_per_head])\n    neg_attention_mask = tf.placeholder(tf.float32, shape=[None, num_attention_heads, seq_length, seq_length])\n\n    tensor_ = np.arange(30, dtype=np.float32)\n    tensor_ = np.reshape(tensor_, (batch_size, seq_length, num_attention_heads * size_per_head))\n\n    neg_attention_mask_ = np.zeros((batch_size, num_attention_heads, seq_length, seq_length), dtype=np.float32)\n    neg_attention_mask_[0, 0, 0, 0] = 1\n\n    with tf.Session() as sess:\n        context_layer = attention_layer(tensor, neg_attention_mask,\n                                        query_kernel=query_kernel, query_bias=query_bias,\n                                        key_kernel=key_kernel, key_bias=key_bias,\n                                        value_kernel=value_kernel, value_bias=value_bias,\n                                        num_attention_heads=num_attention_heads,\n                                        size_per_head=size_per_head,\n                                        batch_size=batch_size,\n                                        seq_length=seq_length)\n\n        context_layer_ = sess.run(context_layer, feed_dict={\n            tensor: tensor_,\n            neg_attention_mask: neg_attention_mask_,\n        })\n        print context_layer_\n"""
test/cuBERT/op_att/modeling.py,17,"b'import math\nimport tensorflow as tf\n\n\ndef transpose_for_scores(input_tensor, _batch_size, _num_attention_heads, _seq_length, width):\n    output_tensor = tf.reshape(input_tensor, [_batch_size, _seq_length, _num_attention_heads, width])\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n\ndef attention_layer(tensor,\n                    neg_attention_mask,\n                    query_kernel, query_bias,\n                    key_kernel, key_bias,\n                    value_kernel, value_bias,\n                    num_attention_heads=1,\n                    size_per_head=512,\n                    batch_size=None,\n                    seq_length=None):\n    """"""\n    :param seq_length:\n    :param batch_size:\n    :param size_per_head:\n    :param num_attention_heads:\n    :param value_bias:\n    :param value_kernel:\n    :param key_bias:\n    :param key_kernel:\n    :param query_bias:\n    :param query_kernel:\n    :param tensor: float Tensor of shape [batch_size, seq_length, width].\n    :param neg_attention_mask: [batch_size, num_attention_heads, seq_length, seq_length]\n    :return:\n    """"""\n    query_layer = tf.layers.Dense(\n        num_attention_heads * size_per_head,\n        name=""query"",\n        weights=[query_kernel, query_bias]\n    ).apply(tensor)\n\n    key_layer = tf.layers.Dense(\n        num_attention_heads * size_per_head,\n        name=""key"",\n        weights=[key_kernel, key_bias]\n    ).apply(tensor)\n\n    value_layer = tf.layers.Dense(\n        num_attention_heads * size_per_head,\n        name=""value"",\n        weights=[value_kernel, value_bias]\n    ).apply(tensor)\n\n    query_layer = transpose_for_scores(query_layer, batch_size, num_attention_heads, seq_length, size_per_head)\n    key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads, seq_length, size_per_head)\n\n    attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n    attention_scores = tf.multiply(attention_scores, 1.0 / math.sqrt(float(size_per_head)))\n\n    attention_scores += neg_attention_mask * -10000.0\n\n    attention_probs = tf.nn.softmax(attention_scores)\n\n    value_layer = tf.reshape(value_layer, [batch_size, seq_length, num_attention_heads, size_per_head])\n    value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n\n    context_layer = tf.matmul(attention_probs, value_layer)\n    context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n\n    context_layer = tf.reshape(context_layer, [batch_size * seq_length, num_attention_heads * size_per_head])\n    return context_layer\n\n\ndef layer_norm(inputs,\n               beta, gamma,\n               begin_norm_axis=-1,\n               begin_params_axis=-1):\n        inputs_shape = inputs.shape\n        inputs_rank = inputs_shape.ndims\n        if inputs_rank is None:\n            raise ValueError(\'Inputs %s has undefined rank.\' % inputs.name)\n        dtype = inputs.dtype.base_dtype\n        if begin_norm_axis < 0:\n            begin_norm_axis = inputs_rank + begin_norm_axis\n        if begin_params_axis >= inputs_rank or begin_norm_axis >= inputs_rank:\n            raise ValueError(\'begin_params_axis (%d) and begin_norm_axis (%d) must be < rank(inputs) (%d)\' %\n                             (begin_params_axis, begin_norm_axis, inputs_rank))\n        params_shape = inputs_shape[begin_params_axis:]\n        if not params_shape.is_fully_defined():\n            raise ValueError(\'Inputs %s: shape(inputs)[%s:] is not fully defined: %s\' %\n                             (inputs.name, begin_params_axis, inputs_shape))\n        # Allocate parameters for the beta and gamma of the normalization.\n        beta = tf.constant(beta, dtype=dtype)\n        gamma = tf.constant(gamma, dtype=dtype)\n        # Calculate the moments on the last axis (layer activations).\n        norm_axes = list(range(begin_norm_axis, inputs_rank))\n        mean, variance = tf.nn.moments(inputs, norm_axes, keep_dims=True)\n        # Compute layer normalization using the batch_normalization function.\n        variance_epsilon = 1e-12\n        outputs = tf.nn.batch_normalization(\n            inputs,\n            mean,\n            variance,\n            offset=beta,\n            scale=gamma,\n            variance_epsilon=variance_epsilon)\n        outputs.set_shape(inputs_shape)\n        return outputs\n'"
test/cuBERT/op_att/transformer_test.py,16,"b'import numpy as np\nimport tensorflow as tf\n\nfrom modeling import attention_layer, layer_norm\n\nbatch_size = 2\nnum_attention_heads = 2\nsize_per_head = 3\nseq_length = 4\nintermediate_size = 5\n\nquery_kernel = np.array([\n    [-0.07848196, -0.18097023, 0.06933199, -0.07760319, 0.11389876, 0.05236414],\n    [-0.02015782, 0.00233333, -0.00281469, -0.01525305, 0.17362033, -0.01600084],\n    [0.00521428, 0.06063714, -0.10533229, 0.0228875, -0.00108843, -0.05974746],\n    [-0.05530503, 0.06056419, 0.099603, 0.04929306, 0.08636444, 0.08424559],\n    [0.02739674, -0.08676406, -0.0819858, 0.03834791, -0.03903558, 0.01903536],\n    [0.01325864, 0.07587593, 0.20709228, -0.0421985, -0.10500058, -0.08004139]\n], dtype=np.float32)\nquery_bias = np.array([-0.01566293, -0.01429354, -0.02946532, 0.02332242, -0.03551506, 0.00519018], dtype=np.float32)\n\nkey_kernel = np.array([\n    [-0.19046976, -0.052291, 0.00774184, -0.04793982, -0.03272828, -0.07022775],\n    [0.05397043, 0.22157724, -0.28796428, -0.13628182, 0.10769557, -0.04396444],\n    [0.11023977, 0.11277004, -0.17019109, -0.00998783, -0.13503011, 0.03862515],\n    [-0.00570178, -0.03683843, -0.09878516, -0.08536254, -0.20706373, 0.07736684],\n    [0.09753255, 0.08549864, 0.07573727, -0.08459285, 0.11262332, -0.06660723],\n    [-0.05978908, 0.04687774, 0.20048976, -0.15552515, -0.09287686, -0.05736409]\n], dtype=np.float32)\nkey_bias = np.array([0.01119683, -0.00749641, 0.00929781, -0.00789247, 0.00374282, -0.0203852], dtype=np.float32)\n\nvalue_kernel = np.array([\n    [0.18298741, 0.13052676, 0.13003705, -0.07762788, -0.11298412, -0.09672086],\n    [-0.27567647, -0.11159269, -0.20191047, -0.04961415, 0.03338585, -0.00217377],\n    [0.0080993, -0.0092568, -0.07923323, -0.09595821, -0.0724212, 0.00234286],\n    [0.08350474, 0.10685625, -0.03265393, 0.12026393, 0.11865459, 0.03879681],\n    [0.09247954, -0.08354547, -0.04044447, 0.05576184, 0.063286, -0.06426957],\n    [0.11189654, 0.04743394, 0.04952021, 0.06824017, -0.0718908, 0.06118326]\n], dtype=np.float32)\nvalue_bias = np.array([-0.01532887, -0.02567805, 0.02993296, 0.00255634, 0.03075514, -0.02086536], dtype=np.float32)\n\nattention_output_kernel = np.array([\n    [-0.02547911, 0.04877987, 0.05000711, 0.04084699, -0.08732582, 0.09071281],\n    [-0.04081769, 0.21188675, 0.05063592, 0.04011015, -0.09087955, -0.02277032],\n    [0.11330121, -0.00220912, -0.21545858, -0.0109133, 0.12117786, -0.07627827],\n    [0.03476971, 0.113976, 0.0352498, -0.00169246, 0.17134688, 0.05991947],\n    [-0.04367283, -0.08021438, 0.07809242, 0.04896554, -0.09109284, -0.17430527],\n    [0.07785448, -0.08642721, 0.0911883, -0.00432356, -0.10407569, 0.03155923]\n], dtype=np.float32)\nattention_output_bias = np.array([0.00502381, 0.00164522, 0.00503161, 0.05414474, 0.00594567, -0.00136505],\n                                 dtype=np.float32)\n\nattention_norm_beta = np.array([0.01438165, 0.00893282, -0.00166658, -0.01515444, 0.01131669, -0.00312567],\n                               dtype=np.float32)\nattention_norm_gamma = np.array([0.98945833, 1.00672382, 1.00227484, 0.98692834, 1.00251162, 0.99780415],\n                                dtype=np.float32)\n\nintermediate_kernel = np.array([\n    [0.04110776, 0.00867842, -0.11692518, 0.00942204, 0.00212334],\n    [0.03458865, -0.00608362, -0.12785568, 0.01738149, -0.0735809],\n    [-0.03358123, -0.02204291, 0.19460295, 0.10060768, -0.11971488],\n    [0.02828389, -0.07767208, 0.03127521, 0.01363018, -0.14119004],\n    [0.01852505, -0.12854275, 0.0481119, -0.15679542, -0.08593457],\n    [0.00225799, -0.03674033, -0.10633834, 0.03639213, 0.07383945]\n], dtype=np.float32)\nintermediate_bias = np.array([-0.0094941, 0.00329734, 0.00365913, 0.02430543, 0.04413794], dtype=np.float32)\n\noutput_kernel = np.array([\n    [0.14096574, 0.0019019, 0.03194073, -0.01783772, 0.04542776, -0.17121975],\n    [-0.03054714, -0.03382285, -0.14785342, -0.04588855, -0.09048948, -0.04335051],\n    [0.12839685, -0.17706056, -0.01360187, 0.02532171, 0.08845975, 0.00350385],\n    [0.07184936, 0.11032352, 0.0339272, -0.04756412, -0.20521204, 0.12666636],\n    [0.06397831, -0.15246845, -0.00572673, -0.09259837, -0.00063671, -0.13432225]\n], dtype=np.float32)\noutput_bias = np.array([-0.01755394, 0.02878171, 0.04216052, 0.01562296, 0.01129209, 0.04988396], dtype=np.float32)\n\noutput_norm_beta = np.array([0.00422856, 0.04091637, 0.03255221, -0.03470522, 0.01916321, -0.00184435],\n                            dtype=np.float32)\noutput_norm_gamma = np.array([0.9903053, 0.95159506, 0.98762059, 0.99406842, 1.00686035, 0.97648946],\n                             dtype=np.float32)\n\n\ndef gelu(input_tensor):\n    """"""Gaussian Error Linear Unit.\n\n    This is a smoother version of the RELU.\n    Original paper: https://arxiv.org/abs/1606.08415\n\n    Args:\n      input_tensor: float Tensor to perform activation.\n\n    Returns:\n      `input_tensor` with the GELU activation applied.\n    """"""\n    cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))\n    return input_tensor * cdf\n\n\ndef transformer_model(input_tensor,\n                      neg_attention_mask,\n                      num_hidden_layers=12,\n                      intermediate_act_fn=gelu):\n    hidden_size = num_attention_heads * size_per_head\n\n    neg_attention_mask = tf.reshape(neg_attention_mask, [batch_size, 1, 1, seq_length])\n    neg_attention_mask *= tf.ones(shape=[batch_size, num_attention_heads, seq_length, seq_length],\n                                  dtype=tf.float32)\n\n    prev_output = input_tensor\n\n    for layer_idx in range(num_hidden_layers):\n        with tf.variable_scope(""layer_%d"" % layer_idx):\n            layer_input = prev_output\n\n            with tf.variable_scope(""attention""):\n                with tf.variable_scope(""self""):\n                    attention_head = attention_layer(\n                        layer_input, neg_attention_mask,\n                        query_kernel=query_kernel, query_bias=query_bias,\n                        key_kernel=key_kernel, key_bias=key_bias,\n                        value_kernel=value_kernel, value_bias=value_bias,\n                        num_attention_heads=num_attention_heads,\n                        size_per_head=size_per_head,\n                        batch_size=batch_size,\n                        seq_length=seq_length)\n\n                with tf.variable_scope(""output""):\n                    attention_output = tf.layers.Dense(\n                        hidden_size,\n                        weights=[attention_output_kernel, attention_output_bias]\n                    ).apply(attention_head)\n                    attention_output = layer_norm(attention_output + layer_input,\n                                                  beta=attention_norm_beta,\n                                                  gamma=attention_norm_gamma)\n\n            with tf.variable_scope(""intermediate""):\n                intermediate_output = tf.layers.Dense(\n                    intermediate_size,\n                    activation=intermediate_act_fn,\n                    weights=[intermediate_kernel, intermediate_bias]\n                ).apply(attention_output)\n\n            with tf.variable_scope(""output""):\n                layer_output = tf.layers.Dense(\n                    hidden_size,\n                    weights=[output_kernel, output_bias]\n                ).apply(intermediate_output)\n                layer_output = layer_norm(layer_output + attention_output,\n                                          beta=output_norm_beta,\n                                          gamma=output_norm_gamma)\n                prev_output = layer_output\n\n    return prev_output\n\n\nif __name__ == \'__main__\':\n    tensor = tf.placeholder(tf.float32, shape=[batch_size * seq_length, num_attention_heads * size_per_head])\n    neg_attention_mask = tf.placeholder(tf.float32, shape=[batch_size, seq_length])\n\n    tensor_ = np.arange(48, dtype=np.float32)\n    tensor_ = np.reshape(tensor_, (8, 6))\n\n    neg_attention_mask_ = np.zeros((batch_size, seq_length), dtype=np.float32)\n    neg_attention_mask_[0, 0] = 1\n\n    with tf.Session() as sess:\n        output = transformer_model(tensor, neg_attention_mask, num_hidden_layers=2)\n        output_ = sess.run(output, feed_dict={\n            tensor: tensor_,\n            neg_attention_mask: neg_attention_mask_,\n        })\n        print output_\n'"
