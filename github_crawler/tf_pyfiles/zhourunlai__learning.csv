file_path,api_count,code
Keras/addition_rnn.py,0,"b'# -*- coding: utf-8 -*-\n\'\'\'An implementation of sequence to sequence learning for performing addition\nInput: ""535+61""\nOutput: ""596""\nPadding is handled by using a repeated sentinel character (space)\n\nInput may optionally be inverted, shown to increase performance in many tasks in:\n""Learning to Execute""\nhttp://arxiv.org/abs/1410.4615\nand\n""Sequence to Sequence Learning with Neural Networks""\nhttp://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\nTheoretically it introduces shorter term dependencies between source and target.\n\nTwo digits inverted:\n+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n\nThree digits inverted:\n+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n\nFour digits inverted:\n+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n\nFive digits inverted:\n+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n\'\'\'\n\nfrom __future__ import print_function\nfrom keras.models import Sequential\nfrom keras import layers\nimport numpy as np\nfrom six.moves import range\n\n\nclass CharacterTable(object):\n    """"""Given a set of characters:\n    + Encode them to a one hot integer representation\n    + Decode the one hot integer representation to their character output\n    + Decode a vector of probabilities to their character output\n    """"""\n    def __init__(self, chars):\n        """"""Initialize character table.\n\n        # Arguments\n            chars: Characters that can appear in the input.\n        """"""\n        self.chars = sorted(set(chars))\n        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n\n    def encode(self, C, num_rows):\n        """"""One hot encode given string C.\n\n        # Arguments\n            num_rows: Number of rows in the returned one hot encoding. This is\n                used to keep the # of rows for each data the same.\n        """"""\n        x = np.zeros((num_rows, len(self.chars)))\n        for i, c in enumerate(C):\n            x[i, self.char_indices[c]] = 1\n        return x\n\n    def decode(self, x, calc_argmax=True):\n        if calc_argmax:\n            x = x.argmax(axis=-1)\n        return \'\'.join(self.indices_char[x] for x in x)\n\n\nclass colors:\n    ok = \'\\033[92m\'\n    fail = \'\\033[91m\'\n    close = \'\\033[0m\'\n\n# Parameters for the model and dataset.\nTRAINING_SIZE = 50000\nDIGITS = 3\nINVERT = True\n\n# Maximum length of input is \'int + int\' (e.g., \'345+678\'). Maximum length of\n# int is DIGITS.\nMAxLEN = DIGITS + 1 + DIGITS\n\n# All the numbers, plus sign and space for padding.\nchars = \'0123456789+ \'\nctable = CharacterTable(chars)\n\nquestions = []\nexpected = []\nseen = set()\nprint(\'Generating data...\')\nwhile len(questions) < TRAINING_SIZE:\n    f = lambda: int(\'\'.join(np.random.choice(list(\'0123456789\'))\n                    for i in range(np.random.randint(1, DIGITS + 1))))\n    a, b = f(), f()\n    # Skip any addition questions we\'ve already seen\n    # Also skip any such that x+Y == Y+x (hence the sorting).\n    key = tuple(sorted((a, b)))\n    if key in seen:\n        continue\n    seen.add(key)\n    # Pad the data with spaces such that it is always MAxLEN.\n    q = \'{}+{}\'.format(a, b)\n    query = q + \' \' * (MAxLEN - len(q))\n    ans = str(a + b)\n    # Answers can be of maximum size DIGITS + 1.\n    ans += \' \' * (DIGITS + 1 - len(ans))\n    if INVERT:\n        # Reverse the query, e.g., \'12+345  \' becomes \'  543+21\'. (Note the\n        # space used for padding.)\n        query = query[::-1]\n    questions.append(query)\n    expected.append(ans)\nprint(\'Total addition questions:\', len(questions))\n\nprint(\'Vectorization...\')\nx = np.zeros((len(questions), MAxLEN, len(chars)), dtype=np.bool)\ny = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\nfor i, sentence in enumerate(questions):\n    x[i] = ctable.encode(sentence, MAxLEN)\nfor i, sentence in enumerate(expected):\n    y[i] = ctable.encode(sentence, DIGITS + 1)\n\n# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n# digits.\nindices = np.arange(len(y))\nnp.random.shuffle(indices)\nx = x[indices]\ny = y[indices]\n\n# Explicitly set apart 10% for validation data that we never train over.\nsplit_at = len(x) - len(x) // 10\n(x_train, x_val) = x[:split_at], x[split_at:]\n(y_train, y_val) = y[:split_at], y[split_at:]\n\nprint(\'Training Data:\')\nprint(x_train.shape)\nprint(y_train.shape)\n\nprint(\'Validation Data:\')\nprint(x_val.shape)\nprint(y_val.shape)\n\n# Try replacing GRU, or SimpleRNN.\nRNN = layers.LSTM\nHIDDEN_SIZE = 128\nBATCH_SIZE = 128\nLAYERS = 1\n\nprint(\'Build model...\')\nmodel = Sequential()\n# ""Encode"" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n# Note: In a situation where your input sequences have a variable length,\n# use input_shape=(None, num_feature).\nmodel.add(RNN(HIDDEN_SIZE, input_shape=(MAxLEN, len(chars))))\n# As the decoder RNN\'s input, repeatedly provide with the last hidden state of\n# RNN for each time step. Repeat \'DIGITS + 1\' times as that\'s the maximum\n# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\nmodel.add(layers.RepeatVector(DIGITS + 1))\n# The decoder RNN could be multiple layers stacked or a single layer.\nfor _ in range(LAYERS):\n    # By setting return_sequences to True, return not only the last output but\n    # all the outputs so far in the form of (num_samples, timesteps,\n    # output_dim). This is necessary as TimeDistributed in the below expects\n    # the first dimension to be the timesteps.\n    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n\n# Apply a dense layer to the every temporal slice of an input. For each of step\n# of the output sequence, decide which character should be chosen.\nmodel.add(layers.TimeDistributed(layers.Dense(len(chars))))\nmodel.add(layers.Activation(\'softmax\'))\nmodel.compile(loss=\'categorical_crossentropy\',\n              optimizer=\'adam\',\n              metrics=[\'accuracy\'])\nmodel.summary()\n\n# Train the model each generation and show predictions against the validation\n# dataset.\nfor iteration in range(1, 200):\n    print()\n    print(\'-\' * 50)\n    print(\'Iteration\', iteration)\n    model.fit(x_train, y_train,\n              batch_size=BATCH_SIZE,\n              epochs=1,\n              validation_data=(x_val, y_val))\n    # Select 10 samples from the validation set at random so we can visualize\n    # errors.\n    for i in range(10):\n        ind = np.random.randint(0, len(x_val))\n        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n        preds = model.predict_classes(rowx, verbose=0)\n        q = ctable.decode(rowx[0])\n        correct = ctable.decode(rowy[0])\n        guess = ctable.decode(preds[0], calc_argmax=False)\n        print(\'Q\', q[::-1] if INVERT else q)\n        print(\'T\', correct)\n        if correct == guess:\n            print(colors.ok + \'\xe2\x98\x91\' + colors.close, end="" "")\n        else:\n            print(colors.fail + \'\xe2\x98\x92\' + colors.close, end="" "")\n        print(guess)\n        print(\'---\')\n'"
Keras/antirectifier.py,0,"b""'''The example demonstrates how to write custom layers for Keras.\n\nWe build a custom activation layer called 'Antirectifier',\nwhich modifies the shape of the tensor that passes through it.\nWe need to specify two methods: `compute_output_shape` and `call`.\n\nNote that the same result can also be achieved via a Lambda layer.\n\nBecause our custom layer is written with primitives from the Keras\nbackend (`K`), our code can run both on TensorFlow and Theano.\n'''\n\nfrom __future__ import print_function\nimport keras\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.datasets import mnist\nfrom keras import backend as K\n\n\nclass Antirectifier(layers.Layer):\n    '''This is the combination of a sample-wise\n    L2 normalization with the concatenation of the\n    positive part of the input with the negative part\n    of the input. The result is a tensor of samples that are\n    twice as large as the input samples.\n\n    It can be used in place of a ReLU.\n\n    # Input shape\n        2D tensor of shape (samples, n)\n\n    # Output shape\n        2D tensor of shape (samples, 2*n)\n\n    # Theoretical justification\n        When applying ReLU, assuming that the distribution\n        of the previous output is approximately centered around 0.,\n        you are discarding half of your input. This is inefficient.\n\n        Antirectifier allows to return all-positive outputs like ReLU,\n        without discarding any data.\n\n        Tests on MNIST show that Antirectifier allows to train networks\n        with twice less parameters yet with comparable\n        classification accuracy as an equivalent ReLU-based network.\n    '''\n\n    def compute_output_shape(self, input_shape):\n        shape = list(input_shape)\n        assert len(shape) == 2  # only valid for 2D tensors\n        shape[-1] *= 2\n        return tuple(shape)\n\n    def call(self, inputs):\n        inputs -= K.mean(inputs, axis=1, keepdims=True)\n        inputs = K.l2_normalize(inputs, axis=1)\n        pos = K.relu(inputs)\n        neg = K.relu(-inputs)\n        return K.concatenate([pos, neg], axis=1)\n\n# global parameters\nbatch_size = 128\nnum_classes = 10\nepochs = 40\n\n# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nx_train = x_train.reshape(60000, 784)\nx_test = x_test.reshape(10000, 784)\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n# build the model\nmodel = Sequential()\nmodel.add(layers.Dense(256, input_shape=(784,)))\nmodel.add(Antirectifier())\nmodel.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(256))\nmodel.add(Antirectifier())\nmodel.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(10))\nmodel.add(layers.Activation('softmax'))\n\n# compile the model\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\n# train the model\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\n\n# next, compare with an equivalent network\n# with2x bigger Dense layers and ReLU\n"""
Keras/babi_memnn.py,0,"b'\'\'\'Trains a memory network on the bAbI dataset.\n\nReferences:\n- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n  ""Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks"",\n  http://arxiv.org/abs/1502.05698\n\n- Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n  ""End-To-End Memory Networks"",\n  http://arxiv.org/abs/1503.08895\n\nReaches 98.6% accuracy on task \'single_supporting_fact_10k\' after 120 epochs.\nTime per epoch: 3s on CPU (core i7).\n\'\'\'\nfrom __future__ import print_function\n\nfrom keras.models import Sequential, Model\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate\nfrom keras.layers import LSTM\nfrom keras.utils.data_utils import get_file\nfrom keras.preprocessing.sequence import pad_sequences\nfrom functools import reduce\nimport tarfile\nimport numpy as np\nimport re\n\n\ndef tokenize(sent):\n    \'\'\'Return the tokens of a sentence including punctuation.\n\n    >>> tokenize(\'Bob dropped the apple. Where is the apple?\')\n    [\'Bob\', \'dropped\', \'the\', \'apple\', \'.\', \'Where\', \'is\', \'the\', \'apple\', \'?\']\n    \'\'\'\n    return [x.strip() for x in re.split(\'(\\W+)?\', sent) if x.strip()]\n\n\ndef parse_stories(lines, only_supporting=False):\n    \'\'\'Parse stories provided in the bAbi tasks format\n\n    If only_supporting is true, only the sentences\n    that support the answer are kept.\n    \'\'\'\n    data = []\n    story = []\n    for line in lines:\n        line = line.decode(\'utf-8\').strip()\n        nid, line = line.split(\' \', 1)\n        nid = int(nid)\n        if nid == 1:\n            story = []\n        if \'\\t\' in line:\n            q, a, supporting = line.split(\'\\t\')\n            q = tokenize(q)\n            substory = None\n            if only_supporting:\n                # Only select the related substory\n                supporting = map(int, supporting.split())\n                substory = [story[i - 1] for i in supporting]\n            else:\n                # Provide all the substories\n                substory = [x for x in story if x]\n            data.append((substory, q, a))\n            story.append(\'\')\n        else:\n            sent = tokenize(line)\n            story.append(sent)\n    return data\n\n\ndef get_stories(f, only_supporting=False, max_length=None):\n    \'\'\'Given a file name, read the file,\n    retrieve the stories,\n    and then convert the sentences into a single story.\n\n    If max_length is supplied,\n    any stories longer than max_length tokens will be discarded.\n    \'\'\'\n    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n    flatten = lambda data: reduce(lambda x, y: x + y, data)\n    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n    return data\n\n\ndef vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n    X = []\n    Xq = []\n    Y = []\n    for story, query, answer in data:\n        x = [word_idx[w] for w in story]\n        xq = [word_idx[w] for w in query]\n        # let\'s not forget that index 0 is reserved\n        y = np.zeros(len(word_idx) + 1)\n        y[word_idx[answer]] = 1\n        X.append(x)\n        Xq.append(xq)\n        Y.append(y)\n    return (pad_sequences(X, maxlen=story_maxlen),\n            pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))\n\ntry:\n    path = get_file(\'babi-tasks-v1-2.tar.gz\', origin=\'https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\')\nexcept:\n    print(\'Error downloading dataset, please download it manually:\\n\'\n          \'$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n\'\n          \'$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz\')\n    raise\ntar = tarfile.open(path)\n\nchallenges = {\n    # QA1 with 10,000 samples\n    \'single_supporting_fact_10k\': \'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt\',\n    # QA2 with 10,000 samples\n    \'two_supporting_facts_10k\': \'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt\',\n}\nchallenge_type = \'single_supporting_fact_10k\'\nchallenge = challenges[challenge_type]\n\nprint(\'Extracting stories for the challenge:\', challenge_type)\ntrain_stories = get_stories(tar.extractfile(challenge.format(\'train\')))\ntest_stories = get_stories(tar.extractfile(challenge.format(\'test\')))\n\nvocab = set()\nfor story, q, answer in train_stories + test_stories:\n    vocab |= set(story + q + [answer])\nvocab = sorted(vocab)\n\n# Reserve 0 for masking via pad_sequences\nvocab_size = len(vocab) + 1\nstory_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\nquery_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n\nprint(\'-\')\nprint(\'Vocab size:\', vocab_size, \'unique words\')\nprint(\'Story max length:\', story_maxlen, \'words\')\nprint(\'Query max length:\', query_maxlen, \'words\')\nprint(\'Number of training stories:\', len(train_stories))\nprint(\'Number of test stories:\', len(test_stories))\nprint(\'-\')\nprint(\'Here\\\'s what a ""story"" tuple looks like (input, query, answer):\')\nprint(train_stories[0])\nprint(\'-\')\nprint(\'Vectorizing the word sequences...\')\n\nword_idx = dict((c, i + 1) for i, c in enumerate(vocab))\ninputs_train, queries_train, answers_train = vectorize_stories(train_stories,\n                                                               word_idx,\n                                                               story_maxlen,\n                                                               query_maxlen)\ninputs_test, queries_test, answers_test = vectorize_stories(test_stories,\n                                                            word_idx,\n                                                            story_maxlen,\n                                                            query_maxlen)\n\nprint(\'-\')\nprint(\'inputs: integer tensor of shape (samples, max_length)\')\nprint(\'inputs_train shape:\', inputs_train.shape)\nprint(\'inputs_test shape:\', inputs_test.shape)\nprint(\'-\')\nprint(\'queries: integer tensor of shape (samples, max_length)\')\nprint(\'queries_train shape:\', queries_train.shape)\nprint(\'queries_test shape:\', queries_test.shape)\nprint(\'-\')\nprint(\'answers: binary (1 or 0) tensor of shape (samples, vocab_size)\')\nprint(\'answers_train shape:\', answers_train.shape)\nprint(\'answers_test shape:\', answers_test.shape)\nprint(\'-\')\nprint(\'Compiling...\')\n\n# placeholders\ninput_sequence = Input((story_maxlen,))\nquestion = Input((query_maxlen,))\n\n# encoders\n# embed the input sequence into a sequence of vectors\ninput_encoder_m = Sequential()\ninput_encoder_m.add(Embedding(input_dim=vocab_size,\n                              output_dim=64))\ninput_encoder_m.add(Dropout(0.3))\n# output: (samples, story_maxlen, embedding_dim)\n\n# embed the input into a sequence of vectors of size query_maxlen\ninput_encoder_c = Sequential()\ninput_encoder_c.add(Embedding(input_dim=vocab_size,\n                              output_dim=query_maxlen))\ninput_encoder_c.add(Dropout(0.3))\n# output: (samples, story_maxlen, query_maxlen)\n\n# embed the question into a sequence of vectors\nquestion_encoder = Sequential()\nquestion_encoder.add(Embedding(input_dim=vocab_size,\n                               output_dim=64,\n                               input_length=query_maxlen))\nquestion_encoder.add(Dropout(0.3))\n# output: (samples, query_maxlen, embedding_dim)\n\n# encode input sequence and questions (which are indices)\n# to sequences of dense vectors\ninput_encoded_m = input_encoder_m(input_sequence)\ninput_encoded_c = input_encoder_c(input_sequence)\nquestion_encoded = question_encoder(question)\n\n# compute a \'match\' between the first input vector sequence\n# and the question vector sequence\n# shape: `(samples, story_maxlen, query_maxlen)`\nmatch = dot([input_encoded_m, question_encoded], axes=(2, 2))\nmatch = Activation(\'softmax\')(match)\n\n# add the match matrix with the second input vector sequence\nresponse = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\nresponse = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n\n# concatenate the match matrix with the question vector sequence\nanswer = concatenate([response, question_encoded])\n\n# the original paper uses a matrix multiplication for this reduction step.\n# we choose to use a RNN instead.\nanswer = LSTM(32)(answer)  # (samples, 32)\n\n# one regularization layer -- more would probably be needed.\nanswer = Dropout(0.3)(answer)\nanswer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n# we output a probability distribution over the vocabulary\nanswer = Activation(\'softmax\')(answer)\n\n# build the final model\nmodel = Model([input_sequence, question], answer)\nmodel.compile(optimizer=\'rmsprop\', loss=\'categorical_crossentropy\',\n              metrics=[\'accuracy\'])\n\n# train\nmodel.fit([inputs_train, queries_train], answers_train,\n          batch_size=32,\n          epochs=120,\n          validation_data=([inputs_test, queries_test], answers_test))\n'"
Keras/babi_rnn.py,0,"b'\'\'\'Trains two recurrent neural networks based upon a story and a question.\nThe resulting merged vector is then queried to answer a range of bAbI tasks.\n\nThe results are comparable to those for an LSTM model provided in Weston et al.:\n""Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks""\nhttp://arxiv.org/abs/1502.05698\n\nTask Number                  | FB LSTM Baseline | Keras QA\n---                          | ---              | ---\nQA1 - Single Supporting Fact | 50               | 100.0\nQA2 - Two Supporting Facts   | 20               | 50.0\nQA3 - Three Supporting Facts | 20               | 20.5\nQA4 - Two Arg. Relations     | 61               | 62.9\nQA5 - Three Arg. Relations   | 70               | 61.9\nQA6 - yes/No Questions       | 48               | 50.7\nQA7 - Counting               | 49               | 78.9\nQA8 - Lists/Sets             | 45               | 77.2\nQA9 - Simple Negation        | 64               | 64.0\nQA10 - Indefinite Knowledge  | 44               | 47.7\nQA11 - Basic Coreference     | 72               | 74.9\nQA12 - Conjunction           | 74               | 76.4\nQA13 - Compound Coreference  | 94               | 94.4\nQA14 - Time Reasoning        | 27               | 34.8\nQA15 - Basic Deduction       | 21               | 32.4\nQA16 - Basic Induction       | 23               | 50.6\nQA17 - Positional Reasoning  | 51               | 49.1\nQA18 - Size Reasoning        | 52               | 90.8\nQA19 - Path Finding          | 8                | 9.0\nQA20 - Agent\'s Motivations   | 91               | 90.7\n\nFor the resources related to the bAbI project, refer to:\nhttps://research.facebook.com/researchers/1543934539189348\n\nNotes:\n\n- With default word, sentence, and query vector sizes, the GRU model achieves:\n  - 100% test accuracy on QA1 in 20 epochs (2 seconds per epoch on CPU)\n  - 50% test accuracy on QA2 in 20 epochs (16 seconds per epoch on CPU)\nIn comparison, the Facebook paper achieves 50% and 20% for the LSTM baseline.\n\n- The task does not traditionally parse the question separately. This likely\nimproves accuracy and is a good example of merging two RNNs.\n\n- The word vector embeddings are not shared between the story and question RNNs.\n\n- See how the accuracy changes given 10,000 training samples (en-10k) instead\nof only 1000. 1000 was used in order to be comparable to the original paper.\n\n- Experiment with GRU, LSTM, and JZS1-3 as they give subtly different results.\n\n- The length and noise (i.e. \'useless\' story components) impact the ability for\nLSTMs / GRUs to provide the correct answer. Given only the supporting facts,\nthese RNNs can achieve 100% accuracy on many tasks. Memory networks and neural\nnetworks that use attentional processes can efficiently search through this\nnoise to find the relevant statements, improving performance substantially.\nThis becomes especially obvious on QA2 and QA3, both far longer than QA1.\n\'\'\'\n\nfrom __future__ import print_function\nfrom functools import reduce\nimport re\nimport tarfile\n\nimport numpy as np\n\nfrom keras.utils.data_utils import get_file\nfrom keras.layers.embeddings import Embedding\nfrom keras import layers\nfrom keras.layers import recurrent\nfrom keras.models import Model\nfrom keras.preprocessing.sequence import pad_sequences\n\n\ndef tokenize(sent):\n    \'\'\'Return the tokens of a sentence including punctuation.\n\n    >>> tokenize(\'Bob dropped the apple. Where is the apple?\')\n    [\'Bob\', \'dropped\', \'the\', \'apple\', \'.\', \'Where\', \'is\', \'the\', \'apple\', \'?\']\n    \'\'\'\n    return [x.strip() for x in re.split(\'(\\W+)?\', sent) if x.strip()]\n\n\ndef parse_stories(lines, only_supporting=False):\n    \'\'\'Parse stories provided in the bAbi tasks format\n\n    If only_supporting is true,\n    only the sentences that support the answer are kept.\n    \'\'\'\n    data = []\n    story = []\n    for line in lines:\n        line = line.decode(\'utf-8\').strip()\n        nid, line = line.split(\' \', 1)\n        nid = int(nid)\n        if nid == 1:\n            story = []\n        if \'\\t\' in line:\n            q, a, supporting = line.split(\'\\t\')\n            q = tokenize(q)\n            substory = None\n            if only_supporting:\n                # Only select the related substory\n                supporting = map(int, supporting.split())\n                substory = [story[i - 1] for i in supporting]\n            else:\n                # Provide all the substories\n                substory = [x for x in story if x]\n            data.append((substory, q, a))\n            story.append(\'\')\n        else:\n            sent = tokenize(line)\n            story.append(sent)\n    return data\n\n\ndef get_stories(f, only_supporting=False, max_length=None):\n    \'\'\'Given a file name, read the file, retrieve the stories,\n    and then convert the sentences into a single story.\n\n    If max_length is supplied,\n    any stories longer than max_length tokens will be discarded.\n    \'\'\'\n    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n    flatten = lambda data: reduce(lambda x, y: x + y, data)\n    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n    return data\n\n\ndef vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n    xs = []\n    xqs = []\n    ys = []\n    for story, query, answer in data:\n        x = [word_idx[w] for w in story]\n        xq = [word_idx[w] for w in query]\n        # let\'s not forget that index 0 is reserved\n        y = np.zeros(len(word_idx) + 1)\n        y[word_idx[answer]] = 1\n        xs.append(x)\n        xqs.append(xq)\n        ys.append(y)\n    return pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen), np.array(ys)\n\nRNN = recurrent.LSTM\nEMBED_HIDDEN_SIZE = 50\nSENT_HIDDEN_SIZE = 100\nQUERY_HIDDEN_SIZE = 100\nBATCH_SIZE = 32\nEPOCHS = 40\nprint(\'RNN / Embed / Sent / Query = {}, {}, {}, {}\'.format(RNN,\n                                                           EMBED_HIDDEN_SIZE,\n                                                           SENT_HIDDEN_SIZE,\n                                                           QUERY_HIDDEN_SIZE))\n\ntry:\n    path = get_file(\'babi-tasks-v1-2.tar.gz\', origin=\'https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\')\nexcept:\n    print(\'Error downloading dataset, please download it manually:\\n\'\n          \'$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n\'\n          \'$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz\')\n    raise\ntar = tarfile.open(path)\n# Default QA1 with 1000 samples\n# challenge = \'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt\'\n# QA1 with 10,000 samples\n# challenge = \'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt\'\n# QA2 with 1000 samples\nchallenge = \'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt\'\n# QA2 with 10,000 samples\n# challenge = \'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt\'\ntrain = get_stories(tar.extractfile(challenge.format(\'train\')))\ntest = get_stories(tar.extractfile(challenge.format(\'test\')))\n\nvocab = set()\nfor story, q, answer in train + test:\n    vocab |= set(story + q + [answer])\nvocab = sorted(vocab)\n\n# Reserve 0 for masking via pad_sequences\nvocab_size = len(vocab) + 1\nword_idx = dict((c, i + 1) for i, c in enumerate(vocab))\nstory_maxlen = max(map(len, (x for x, _, _ in train + test)))\nquery_maxlen = max(map(len, (x for _, x, _ in train + test)))\n\nx, xq, y = vectorize_stories(train, word_idx, story_maxlen, query_maxlen)\ntx, txq, ty = vectorize_stories(test, word_idx, story_maxlen, query_maxlen)\n\nprint(\'vocab = {}\'.format(vocab))\nprint(\'x.shape = {}\'.format(x.shape))\nprint(\'xq.shape = {}\'.format(xq.shape))\nprint(\'y.shape = {}\'.format(y.shape))\nprint(\'story_maxlen, query_maxlen = {}, {}\'.format(story_maxlen, query_maxlen))\n\nprint(\'Build model...\')\n\nsentence = layers.Input(shape=(story_maxlen,), dtype=\'int32\')\nencoded_sentence = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(sentence)\nencoded_sentence = layers.Dropout(0.3)(encoded_sentence)\n\nquestion = layers.Input(shape=(query_maxlen,), dtype=\'int32\')\nencoded_question = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(question)\nencoded_question = layers.Dropout(0.3)(encoded_question)\nencoded_question = RNN(EMBED_HIDDEN_SIZE)(encoded_question)\nencoded_question = layers.RepeatVector(story_maxlen)(encoded_question)\n\nmerged = layers.add([encoded_sentence, encoded_question])\nmerged = RNN(EMBED_HIDDEN_SIZE)(merged)\nmerged = layers.Dropout(0.3)(merged)\npreds = layers.Dense(vocab_size, activation=\'softmax\')(merged)\n\nmodel = Model([sentence, question], preds)\nmodel.compile(optimizer=\'adam\',\n              loss=\'categorical_crossentropy\',\n              metrics=[\'accuracy\'])\n\nprint(\'Training\')\nmodel.fit([x, xq], y,\n          batch_size=BATCH_SIZE,\n          epochs=EPOCHS,\n          validation_split=0.05)\nloss, acc = model.evaluate([tx, txq], ty,\n                           batch_size=BATCH_SIZE)\nprint(\'Test loss / test accuracy = {:.4f} / {:.4f}\'.format(loss, acc))\n'"
Keras/cifar10_cnn.py,0,"b""'''Train a simple deep CNN on the CIFAR10 small images dataset.\n\nGPU run command with Theano backend (with TensorFlow, the GPU is automatically used):\n    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatx=float32 python cifar10_cnn.py\n\nIt gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs.\n(it's still underfitting at that point, though).\n'''\n\nfrom __future__ import print_function\nimport keras\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\n\nbatch_size = 32\nnum_classes = 10\nepochs = 200\ndata_augmentation = True\n\n# The data, shuffled and split between train and test sets:\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# Convert class vectors to binary class matrices.\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\n\n# initiate RMSprop optimizer\nopt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n\n# Let's train the model using RMSprop\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\nif not data_augmentation:\n    print('Not using data augmentation.')\n    model.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=(x_test, y_test),\n              shuffle=True)\nelse:\n    print('Using real-time data augmentation.')\n    # This will do preprocessing and realtime data augmentation:\n    datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n    # Compute quantities required for feature-wise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\n    datagen.fit(x_train)\n\n    # Fit the model on the batches generated by datagen.flow().\n    model.fit_generator(datagen.flow(x_train, y_train,\n                                     batch_size=batch_size),\n                        steps_per_epoch=x_train.shape[0] // batch_size,\n                        epochs=epochs,\n                        validation_data=(x_test, y_test))\n"""
Keras/conv_filter_visualization.py,0,"b'\'\'\'Visualization of the filters of VGG16, via gradient ascent in input space.\n\nThis script can run on CPU in a few minutes (with the TensorFlow backend).\n\nResults example: http://i.imgur.com/4nj4KjN.jpg\n\'\'\'\nfrom __future__ import print_function\n\nfrom scipy.misc import imsave\nimport numpy as np\nimport time\nfrom keras.applications import vgg16\nfrom keras import backend as K\n\n# dimensions of the generated pictures for each filter.\nimg_width = 128\nimg_height = 128\n\n# the name of the layer we want to visualize\n# (see model definition at keras/applications/vgg16.py)\nlayer_name = \'block5_conv1\'\n\n# util function to convert a tensor into a valid image\n\n\ndef deprocess_image(x):\n    # normalize tensor: center on 0., ensure std is 0.1\n    x -= x.mean()\n    x /= (x.std() + 1e-5)\n    x *= 0.1\n\n    # clip to [0, 1]\n    x += 0.5\n    x = np.clip(x, 0, 1)\n\n    # convert to RGB array\n    x *= 255\n    if K.image_data_format() == \'channels_first\':\n        x = x.transpose((1, 2, 0))\n    x = np.clip(x, 0, 255).astype(\'uint8\')\n    return x\n\n# build the VGG16 network with ImageNet weights\nmodel = vgg16.VGG16(weights=\'imagenet\', include_top=False)\nprint(\'Model loaded.\')\n\nmodel.summary()\n\n# this is the placeholder for the input images\ninput_img = model.input\n\n# get the symbolic outputs of each ""key"" layer (we gave them unique names).\nlayer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n\n\ndef normalize(x):\n    # utility function to normalize a tensor by its L2 norm\n    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n\n\nkept_filters = []\nfor filter_index in range(0, 200):\n    # we only scan through the first 200 filters,\n    # but there are actually 512 of them\n    print(\'Processing filter %d\' % filter_index)\n    start_time = time.time()\n\n    # we build a loss function that maximizes the activation\n    # of the nth filter of the layer considered\n    layer_output = layer_dict[layer_name].output\n    if K.image_data_format() == \'channels_first\':\n        loss = K.mean(layer_output[:, filter_index, :, :])\n    else:\n        loss = K.mean(layer_output[:, :, :, filter_index])\n\n    # we compute the gradient of the input picture wrt this loss\n    grads = K.gradients(loss, input_img)[0]\n\n    # normalization trick: we normalize the gradient\n    grads = normalize(grads)\n\n    # this function returns the loss and grads given the input picture\n    iterate = K.function([input_img], [loss, grads])\n\n    # step size for gradient ascent\n    step = 1.\n\n    # we start from a gray image with some random noise\n    if K.image_data_format() == \'channels_first\':\n        input_img_data = np.random.random((1, 3, img_width, img_height))\n    else:\n        input_img_data = np.random.random((1, img_width, img_height, 3))\n    input_img_data = (input_img_data - 0.5) * 20 + 128\n\n    # we run gradient ascent for 20 steps\n    for i in range(20):\n        loss_value, grads_value = iterate([input_img_data])\n        input_img_data += grads_value * step\n\n        print(\'Current loss value:\', loss_value)\n        if loss_value <= 0.:\n            # some filters get stuck to 0, we can skip them\n            break\n\n    # decode the resulting input image\n    if loss_value > 0:\n        img = deprocess_image(input_img_data[0])\n        kept_filters.append((img, loss_value))\n    end_time = time.time()\n    print(\'Filter %d processed in %ds\' % (filter_index, end_time - start_time))\n\n# we will stich the best 64 filters on a 8 x 8 grid.\nn = 8\n\n# the filters that have the highest loss are assumed to be better-looking.\n# we will only keep the top 64 filters.\nkept_filters.sort(key=lambda x: x[1], reverse=True)\nkept_filters = kept_filters[:n * n]\n\n# build a black picture with enough space for\n# our 8 x 8 filters of size 128 x 128, with a 5px margin in between\nmargin = 5\nwidth = n * img_width + (n - 1) * margin\nheight = n * img_height + (n - 1) * margin\nstitched_filters = np.zeros((width, height, 3))\n\n# fill the picture with our saved filters\nfor i in range(n):\n    for j in range(n):\n        img, loss = kept_filters[i * n + j]\n        stitched_filters[(img_width + margin) * i: (img_width + margin) * i + img_width,\n                         (img_height + margin) * j: (img_height + margin) * j + img_height, :] = img\n\n# save the result to disk\nimsave(\'stitched_filters_%dx%d.png\' % (n, n), stitched_filters)\n'"
Keras/conv_lstm.py,0,"b'"""""" This script demonstrates the use of a convolutional LSTM network.\nThis network is used to predict the next frame of an artificially\ngenerated movie which contains moving squares.\n""""""\nfrom keras.models import Sequential\nfrom keras.layers.convolutional import Conv3D\nfrom keras.layers.convolutional_recurrent import ConvLSTM2D\nfrom keras.layers.normalization import BatchNormalization\nimport numpy as np\nimport pylab as plt\n\n# We create a layer which take as input movies of shape\n# (n_frames, width, height, channels) and returns a movie\n# of identical shape.\n\nseq = Sequential()\nseq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n                   input_shape=(None, 40, 40, 1),\n                   padding=\'same\', return_sequences=True))\nseq.add(BatchNormalization())\n\nseq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n                   padding=\'same\', return_sequences=True))\nseq.add(BatchNormalization())\n\nseq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n                   padding=\'same\', return_sequences=True))\nseq.add(BatchNormalization())\n\nseq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n                   padding=\'same\', return_sequences=True))\nseq.add(BatchNormalization())\n\nseq.add(Conv3D(filters=1, kernel_size=(3, 3, 3),\n               activation=\'sigmoid\',\n               padding=\'same\', data_format=\'channels_last\'))\nseq.compile(loss=\'binary_crossentropy\', optimizer=\'adadelta\')\n\n\n# Artificial data generation:\n# Generate movies with 3 to 7 moving squares inside.\n# The squares are of shape 1x1 or 2x2 pixels,\n# which move linearly over time.\n# For convenience we first create movies with bigger width and height (80x80)\n# and at the end we select a 40x40 window.\n\ndef generate_movies(n_samples=1200, n_frames=15):\n    row = 80\n    col = 80\n    noisy_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float)\n    shifted_movies = np.zeros((n_samples, n_frames, row, col, 1),\n                              dtype=np.float)\n\n    for i in range(n_samples):\n        # Add 3 to 7 moving squares\n        n = np.random.randint(3, 8)\n\n        for j in range(n):\n            # Initial position\n            xstart = np.random.randint(20, 60)\n            ystart = np.random.randint(20, 60)\n            # Direction of motion\n            directionx = np.random.randint(0, 3) - 1\n            directiony = np.random.randint(0, 3) - 1\n\n            # Size of the square\n            w = np.random.randint(2, 4)\n\n            for t in range(n_frames):\n                x_shift = xstart + directionx * t\n                y_shift = ystart + directiony * t\n                noisy_movies[i, t, x_shift - w: x_shift + w,\n                             y_shift - w: y_shift + w, 0] += 1\n\n                # Make it more robust by adding noise.\n                # The idea is that if during inference,\n                # the value of the pixel is not exactly one,\n                # we need to train the network to be robust and still\n                # consider it as a pixel belonging to a square.\n                if np.random.randint(0, 2):\n                    noise_f = (-1)**np.random.randint(0, 2)\n                    noisy_movies[i, t,\n                                 x_shift - w - 1: x_shift + w + 1,\n                                 y_shift - w - 1: y_shift + w + 1,\n                                 0] += noise_f * 0.1\n\n                # Shift the ground truth by 1\n                x_shift = xstart + directionx * (t + 1)\n                y_shift = ystart + directiony * (t + 1)\n                shifted_movies[i, t, x_shift - w: x_shift + w,\n                               y_shift - w: y_shift + w, 0] += 1\n\n    # Cut to a 40x40 window\n    noisy_movies = noisy_movies[::, ::, 20:60, 20:60, ::]\n    shifted_movies = shifted_movies[::, ::, 20:60, 20:60, ::]\n    noisy_movies[noisy_movies >= 1] = 1\n    shifted_movies[shifted_movies >= 1] = 1\n    return noisy_movies, shifted_movies\n\n# Train the network\nnoisy_movies, shifted_movies = generate_movies(n_samples=1200)\nseq.fit(noisy_movies[:1000], shifted_movies[:1000], batch_size=10,\n        epochs=300, validation_split=0.05)\n\n# Testing the network on one movie\n# feed it with the first 7 positions and then\n# predict the new positions\nwhich = 1004\ntrack = noisy_movies[which][:7, ::, ::, ::]\n\nfor j in range(16):\n    new_pos = seq.predict(track[np.newaxis, ::, ::, ::, ::])\n    new = new_pos[::, -1, ::, ::, ::]\n    track = np.concatenate((track, new), axis=0)\n\n\n# And then compare the predictions\n# to the ground truth\ntrack2 = noisy_movies[which][::, ::, ::, ::]\nfor i in range(15):\n    fig = plt.figure(figsize=(10, 5))\n\n    ax = fig.add_subplot(121)\n\n    if i >= 7:\n        ax.text(1, 3, \'Predictions !\', fontsize=20, color=\'w\')\n    else:\n        ax.text(1, 3, \'Inital trajectory\', fontsize=20)\n\n    toplot = track[i, ::, ::, 0]\n\n    plt.imshow(toplot)\n    ax = fig.add_subplot(122)\n    plt.text(1, 3, \'Ground truth\', fontsize=20)\n\n    toplot = track2[i, ::, ::, 0]\n    if i >= 2:\n        toplot = shifted_movies[which][i - 1, ::, ::, 0]\n\n    plt.imshow(toplot)\n    plt.savefig(\'%i_animate.png\' % (i + 1))\n'"
Keras/deep_dream.py,0,"b'\'\'\'Deep Dreaming in Keras.\n\nRun the script with:\n```\npython deep_dream.py path_to_your_base_image.jpg prefix_for_results\n```\ne.g.:\n```\npython deep_dream.py img/mypic.jpg results/dream\n```\n\nIt is preferable to run this script on GPU, for speed.\nIf running on CPU, prefer the TensorFlow backend (much faster).\n\nExample results: http://i.imgur.com/FX6ROg9.jpg\n\'\'\'\nfrom __future__ import print_function\n\nfrom keras.preprocessing.image import load_img, img_to_array\nimport numpy as np\nfrom scipy.misc import imsave\nfrom scipy.optimize import fmin_l_bfgs_b\nimport time\nimport argparse\n\nfrom keras.applications import vgg16\nfrom keras import backend as K\nfrom keras.layers import Input\n\nparser = argparse.ArgumentParser(description=\'Deep Dreams with Keras.\')\nparser.add_argument(\'base_image_path\', metavar=\'base\', type=str,\n                    help=\'Path to the image to transform.\')\nparser.add_argument(\'result_prefix\', metavar=\'res_prefix\', type=str,\n                    help=\'Prefix for the saved results.\')\n\nargs = parser.parse_args()\nbase_image_path = args.base_image_path\nresult_prefix = args.result_prefix\n\n# dimensions of the generated picture.\nimg_height = 600\nimg_width = 600\n\n# some settings we found interesting\nsaved_settings = {\n    \'bad_trip\': {\'features\': {\'block4_conv1\': 0.05,\n                              \'block4_conv2\': 0.01,\n                              \'block4_conv3\': 0.01},\n                 \'continuity\': 0.1,\n                 \'dream_l2\': 0.8,\n                 \'jitter\': 5},\n    \'dreamy\': {\'features\': {\'block5_conv1\': 0.05,\n                            \'block5_conv2\': 0.02},\n               \'continuity\': 0.1,\n               \'dream_l2\': 0.02,\n               \'jitter\': 0},\n}\n# the settings we will use in this experiment\nsettings = saved_settings[\'dreamy\']\n\n\ndef preprocess_image(image_path):\n    # util function to open, resize and format pictures\n    # into appropriate tensors\n    img = load_img(image_path, target_size=(img_height, img_width))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = vgg16.preprocess_input(img)\n    return img\n\n\ndef deprocess_image(x):\n    # util function to convert a tensor into a valid image\n    if K.image_data_format() == \'channels_first\':\n        x = x.reshape((3, img_height, img_width))\n        x = x.transpose((1, 2, 0))\n    else:\n        x = x.reshape((img_height, img_width, 3))\n    # Remove zero-center by mean pixel\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    # \'BGR\'->\'RGB\'\n    x = x[:, :, ::-1]\n    x = np.clip(x, 0, 255).astype(\'uint8\')\n    return x\n\nif K.image_data_format() == \'channels_first\':\n    img_size = (3, img_height, img_width)\nelse:\n    img_size = (img_height, img_width, 3)\n# this will contain our generated image\ndream = Input(batch_shape=(1,) + img_size)\n\n# build the VGG16 network with our placeholder\n# the model will be loaded with pre-trained ImageNet weights\nmodel = vgg16.VGG16(input_tensor=dream,\n                    weights=\'imagenet\', include_top=False)\nprint(\'Model loaded.\')\n\n# get the symbolic outputs of each ""key"" layer (we gave them unique names).\nlayer_dict = dict([(layer.name, layer) for layer in model.layers])\n\n\ndef continuity_loss(x):\n    # continuity loss util function\n    assert K.ndim(x) == 4\n    if K.image_data_format() == \'channels_first\':\n        a = K.square(x[:, :, :img_height - 1, :img_width - 1] -\n                     x[:, :, 1:, :img_width - 1])\n        b = K.square(x[:, :, :img_height - 1, :img_width - 1] -\n                     x[:, :, :img_height - 1, 1:])\n    else:\n        a = K.square(x[:, :img_height - 1, :img_width - 1, :] -\n                     x[:, 1:, :img_width - 1, :])\n        b = K.square(x[:, :img_height - 1, :img_width - 1, :] -\n                     x[:, :img_height - 1, 1:, :])\n    return K.sum(K.pow(a + b, 1.25))\n\n# define the loss\nloss = K.variable(0.)\nfor layer_name in settings[\'features\']:\n    # add the L2 norm of the features of a layer to the loss\n    assert layer_name in layer_dict.keys(), \'Layer \' + layer_name + \' not found in model.\'\n    coeff = settings[\'features\'][layer_name]\n    x = layer_dict[layer_name].output\n    shape = layer_dict[layer_name].output_shape\n    # we avoid border artifacts by only involving non-border pixels in the loss\n    if K.image_data_format() == \'channels_first\':\n        loss -= coeff * K.sum(K.square(x[:, :, 2: shape[2] - 2, 2: shape[3] - 2])) / np.prod(shape[1:])\n    else:\n        loss -= coeff * K.sum(K.square(x[:, 2: shape[1] - 2, 2: shape[2] - 2, :])) / np.prod(shape[1:])\n\n# add continuity loss (gives image local coherence, can result in an artful blur)\nloss += settings[\'continuity\'] * continuity_loss(dream) / np.prod(img_size)\n# add image L2 norm to loss (prevents pixels from taking very high values, makes image darker)\nloss += settings[\'dream_l2\'] * K.sum(K.square(dream)) / np.prod(img_size)\n\n# feel free to further modify the loss as you see fit, to achieve new effects...\n\n# compute the gradients of the dream wrt the loss\ngrads = K.gradients(loss, dream)\n\noutputs = [loss]\nif isinstance(grads, (list, tuple)):\n    outputs += grads\nelse:\n    outputs.append(grads)\n\nf_outputs = K.function([dream], outputs)\n\n\ndef eval_loss_and_grads(x):\n    x = x.reshape((1,) + img_size)\n    outs = f_outputs([x])\n    loss_value = outs[0]\n    if len(outs[1:]) == 1:\n        grad_values = outs[1].flatten().astype(\'float64\')\n    else:\n        grad_values = np.array(outs[1:]).flatten().astype(\'float64\')\n    return loss_value, grad_values\n\n\nclass Evaluator(object):\n    """"""Loss and gradients evaluator.\n\n    This Evaluator class makes it possible\n    to compute loss and gradients in one pass\n    while retrieving them via two separate functions,\n    ""loss"" and ""grads"". This is done because scipy.optimize\n    requires separate functions for loss and gradients,\n    but computing them separately would be inefficient.\n    """"""\n\n    def __init__(self):\n        self.loss_value = None\n        self.grad_values = None\n\n    def loss(self, x):\n        assert self.loss_value is None\n        loss_value, grad_values = eval_loss_and_grads(x)\n        self.loss_value = loss_value\n        self.grad_values = grad_values\n        return self.loss_value\n\n    def grads(self, x):\n        assert self.loss_value is not None\n        grad_values = np.copy(self.grad_values)\n        self.loss_value = None\n        self.grad_values = None\n        return grad_values\n\nevaluator = Evaluator()\n\n# Run scipy-based optimization (L-BFGS) over the pixels of the generated image\n# so as to minimize the loss\nx = preprocess_image(base_image_path)\nfor i in range(5):\n    print(\'Start of iteration\', i)\n    start_time = time.time()\n\n    # Add a random jitter to the initial image.\n    # This will be reverted at decoding time\n    random_jitter = (settings[\'jitter\'] * 2) * (np.random.random(img_size) - 0.5)\n    x += random_jitter\n\n    # Run L-BFGS for 7 steps\n    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n                                     fprime=evaluator.grads, maxfun=7)\n    print(\'Current loss value:\', min_val)\n    # Decode the dream and save it\n    x = x.reshape(img_size)\n    x -= random_jitter\n    img = deprocess_image(np.copy(x))\n    fname = result_prefix + \'_at_iteration_%d.png\' % i\n    imsave(fname, img)\n    end_time = time.time()\n    print(\'Image saved as\', fname)\n    print(\'Iteration %d completed in %ds\' % (i, end_time - start_time))\n'"
Keras/image_ocr.py,0,"b'\'\'\'This example uses a convolutional stack followed by a recurrent stack\nand a CTC logloss function to perform optical character recognition\nof generated text images. I have no evidence of whether it actually\nlearns general shapes of text, or just is able to recognize all\nthe different fonts thrown at it...the purpose is more to demonstrate CTC\ninside of Keras.  Note that the font list may need to be updated\nfor the particular OS in use.\n\nThis starts off with 4 letter words.  For the first 12 epochs, the\ndifficulty is gradually increased using the TextImageGenerator class\nwhich is both a generator class for test/train data and a Keras\ncallback class. After 20 epochs, longer sequences are thrown at it\nby recompiling the model to handle a wider image and rebuilding\nthe word list to include two words separated by a space.\n\nThe table below shows normalized edit distance values. Theano uses\na slightly different CTC implementation, hence the different results.\n\n            Norm. ED\nEpoch |   TF   |   TH\n------------------------\n    10   0.027   0.064\n    15   0.038   0.035\n    20   0.043   0.045\n    25   0.014   0.019\n\nThis requires cairo and editdistance packages:\npip install cairocffi\npip install editdistance\n\nCreated by Mike Henry\nhttps://github.com/mbhenry/\n\'\'\'\nimport os\nimport itertools\nimport re\nimport datetime\nimport cairocffi as cairo\nimport editdistance\nimport numpy as np\nfrom scipy import ndimage\nimport pylab\nfrom keras import backend as K\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.layers import Input, Dense, Activation\nfrom keras.layers import Reshape, Lambda\nfrom keras.layers.merge import add, concatenate\nfrom keras.models import Model\nfrom keras.layers.recurrent import GRU\nfrom keras.optimizers import SGD\nfrom keras.utils.data_utils import get_file\nfrom keras.preprocessing import image\nimport keras.callbacks\n\n\nOUTPUT_DIR = \'image_ocr\'\n\nnp.random.seed(55)\n\n\n# this creates larger ""blotches"" of noise which look\n# more realistic than just adding gaussian noise\n# assumes greyscale with pixels ranging from 0 to 1\n\ndef speckle(img):\n    severity = np.random.uniform(0, 0.6)\n    blur = ndimage.gaussian_filter(np.random.randn(*img.shape) * severity, 1)\n    img_speck = (img + blur)\n    img_speck[img_speck > 1] = 1\n    img_speck[img_speck <= 0] = 0\n    return img_speck\n\n\n# paints the string in a random location the bounding box\n# also uses a random font, a slight random rotation,\n# and a random amount of speckle noise\n\ndef paint_text(text, w, h, rotate=False, ud=False, multi_fonts=False):\n    surface = cairo.ImageSurface(cairo.FORMAT_RGB24, w, h)\n    with cairo.Context(surface) as context:\n        context.set_source_rgb(1, 1, 1)  # White\n        context.paint()\n        # this font list works in Centos 7\n        if multi_fonts:\n            fonts = [\'Century Schoolbook\', \'Courier\', \'STIX\', \'URW Chancery L\', \'FreeMono\']\n            context.select_font_face(np.random.choice(fonts), cairo.FONT_SLANT_NORMAL,\n                                     np.random.choice([cairo.FONT_WEIGHT_BOLD, cairo.FONT_WEIGHT_NORMAL]))\n        else:\n            context.select_font_face(\'Courier\', cairo.FONT_SLANT_NORMAL, cairo.FONT_WEIGHT_BOLD)\n        context.set_font_size(25)\n        box = context.text_extents(text)\n        border_w_h = (4, 4)\n        if box[2] > (w - 2 * border_w_h[1]) or box[3] > (h - 2 * border_w_h[0]):\n            raise IOError(\'Could not fit string into image. Max char count is too large for given image width.\')\n\n        # teach the RNN translational invariance by\n        # fitting text box randomly on canvas, with some room to rotate\n        max_shift_x = w - box[2] - border_w_h[0]\n        max_shift_y = h - box[3] - border_w_h[1]\n        top_left_x = np.random.randint(0, int(max_shift_x))\n        if ud:\n            top_left_y = np.random.randint(0, int(max_shift_y))\n        else:\n            top_left_y = h // 2\n        context.move_to(top_left_x - int(box[0]), top_left_y - int(box[1]))\n        context.set_source_rgb(0, 0, 0)\n        context.show_text(text)\n\n    buf = surface.get_data()\n    a = np.frombuffer(buf, np.uint8)\n    a.shape = (h, w, 4)\n    a = a[:, :, 0]  # grab single channel\n    a = a.astype(np.float32) / 255\n    a = np.expand_dims(a, 0)\n    if rotate:\n        a = image.random_rotation(a, 3 * (w - top_left_x) / w + 1)\n    a = speckle(a)\n\n    return a\n\n\ndef shuffle_mats_or_lists(matrix_list, stop_ind=None):\n    ret = []\n    assert all([len(i) == len(matrix_list[0]) for i in matrix_list])\n    len_val = len(matrix_list[0])\n    if stop_ind is None:\n        stop_ind = len_val\n    assert stop_ind <= len_val\n\n    a = list(range(stop_ind))\n    np.random.shuffle(a)\n    a += list(range(stop_ind, len_val))\n    for mat in matrix_list:\n        if isinstance(mat, np.ndarray):\n            ret.append(mat[a])\n        elif isinstance(mat, list):\n            ret.append([mat[i] for i in a])\n        else:\n            raise TypeError(\'shuffle_mats_or_lists only supports \'\n                            \'numpy.array and list objects\')\n    return ret\n\n\ndef text_to_labels(text, num_classes):\n    ret = []\n    for char in text:\n        if char >= \'a\' and char <= \'z\':\n            ret.append(ord(char) - ord(\'a\'))\n        elif char == \' \':\n            ret.append(26)\n    return ret\n\n\n# only a-z and space..probably not to difficult\n# to expand to uppercase and symbols\n\ndef is_valid_str(in_str):\n    search = re.compile(r\'[^a-z\\ ]\').search\n    return not bool(search(in_str))\n\n\n# Uses generator functions to supply train/test with\n# data. Image renderings are text are created on the fly\n# each time with random perturbations\n\nclass TextImageGenerator(keras.callbacks.Callback):\n\n    def __init__(self, monogram_file, bigram_file, minibatch_size,\n                 img_w, img_h, downsample_factor, val_split,\n                 absolute_max_string_len=16):\n\n        self.minibatch_size = minibatch_size\n        self.img_w = img_w\n        self.img_h = img_h\n        self.monogram_file = monogram_file\n        self.bigram_file = bigram_file\n        self.downsample_factor = downsample_factor\n        self.val_split = val_split\n        self.blank_label = self.get_output_size() - 1\n        self.absolute_max_string_len = absolute_max_string_len\n\n    def get_output_size(self):\n        return 28\n\n    # num_words can be independent of the epoch size due to the use of generators\n    # as max_string_len grows, num_words can grow\n    def build_word_list(self, num_words, max_string_len=None, mono_fraction=0.5):\n        assert max_string_len <= self.absolute_max_string_len\n        assert num_words % self.minibatch_size == 0\n        assert (self.val_split * num_words) % self.minibatch_size == 0\n        self.num_words = num_words\n        self.string_list = [\'\'] * self.num_words\n        tmp_string_list = []\n        self.max_string_len = max_string_len\n        self.Y_data = np.ones([self.num_words, self.absolute_max_string_len]) * -1\n        self.X_text = []\n        self.Y_len = [0] * self.num_words\n\n        # monogram file is sorted by frequency in english speech\n        with open(self.monogram_file, \'rt\') as f:\n            for line in f:\n                if len(tmp_string_list) == int(self.num_words * mono_fraction):\n                    break\n                word = line.rstrip()\n                if max_string_len == -1 or max_string_len is None or len(word) <= max_string_len:\n                    tmp_string_list.append(word)\n\n        # bigram file contains common word pairings in english speech\n        with open(self.bigram_file, \'rt\') as f:\n            lines = f.readlines()\n            for line in lines:\n                if len(tmp_string_list) == self.num_words:\n                    break\n                columns = line.lower().split()\n                word = columns[0] + \' \' + columns[1]\n                if is_valid_str(word) and \\\n                        (max_string_len == -1 or max_string_len is None or len(word) <= max_string_len):\n                    tmp_string_list.append(word)\n        if len(tmp_string_list) != self.num_words:\n            raise IOError(\'Could not pull enough words from supplied monogram and bigram files. \')\n        # interlace to mix up the easy and hard words\n        self.string_list[::2] = tmp_string_list[:self.num_words // 2]\n        self.string_list[1::2] = tmp_string_list[self.num_words // 2:]\n\n        for i, word in enumerate(self.string_list):\n            self.Y_len[i] = len(word)\n            self.Y_data[i, 0:len(word)] = text_to_labels(word, self.get_output_size())\n            self.X_text.append(word)\n        self.Y_len = np.expand_dims(np.array(self.Y_len), 1)\n\n        self.cur_val_index = self.val_split\n        self.cur_train_index = 0\n\n    # each time an image is requested from train/val/test, a new random\n    # painting of the text is performed\n    def get_batch(self, index, size, train):\n        # width and height are backwards from typical Keras convention\n        # because width is the time dimension when it gets fed into the RNN\n        if K.image_data_format() == \'channels_first\':\n            X_data = np.ones([size, 1, self.img_w, self.img_h])\n        else:\n            X_data = np.ones([size, self.img_w, self.img_h, 1])\n\n        labels = np.ones([size, self.absolute_max_string_len])\n        input_length = np.zeros([size, 1])\n        label_length = np.zeros([size, 1])\n        source_str = []\n        for i in range(0, size):\n            # Mix in some blank inputs.  This seems to be important for\n            # achieving translational invariance\n            if train and i > size - 4:\n                if K.image_data_format() == \'channels_first\':\n                    X_data[i, 0, 0:self.img_w, :] = self.paint_func(\'\')[0, :, :].T\n                else:\n                    X_data[i, 0:self.img_w, :, 0] = self.paint_func(\'\',)[0, :, :].T\n                labels[i, 0] = self.blank_label\n                input_length[i] = self.img_w // self.downsample_factor - 2\n                label_length[i] = 1\n                source_str.append(\'\')\n            else:\n                if K.image_data_format() == \'channels_first\':\n                    X_data[i, 0, 0:self.img_w, :] = self.paint_func(self.X_text[index + i])[0, :, :].T\n                else:\n                    X_data[i, 0:self.img_w, :, 0] = self.paint_func(self.X_text[index + i])[0, :, :].T\n                labels[i, :] = self.Y_data[index + i]\n                input_length[i] = self.img_w // self.downsample_factor - 2\n                label_length[i] = self.Y_len[index + i]\n                source_str.append(self.X_text[index + i])\n        inputs = {\'the_input\': X_data,\n                  \'the_labels\': labels,\n                  \'input_length\': input_length,\n                  \'label_length\': label_length,\n                  \'source_str\': source_str  # used for visualization only\n                  }\n        outputs = {\'ctc\': np.zeros([size])}  # dummy data for dummy loss function\n        return (inputs, outputs)\n\n    def next_train(self):\n        while 1:\n            ret = self.get_batch(self.cur_train_index, self.minibatch_size, train=True)\n            self.cur_train_index += self.minibatch_size\n            if self.cur_train_index >= self.val_split:\n                self.cur_train_index = self.cur_train_index % 32\n                (self.X_text, self.Y_data, self.Y_len) = shuffle_mats_or_lists(\n                    [self.X_text, self.Y_data, self.Y_len], self.val_split)\n            yield ret\n\n    def next_val(self):\n        while 1:\n            ret = self.get_batch(self.cur_val_index, self.minibatch_size, train=False)\n            self.cur_val_index += self.minibatch_size\n            if self.cur_val_index >= self.num_words:\n                self.cur_val_index = self.val_split + self.cur_val_index % 32\n            yield ret\n\n    def on_train_begin(self, logs={}):\n        self.build_word_list(16000, 4, 1)\n        self.paint_func = lambda text: paint_text(text, self.img_w, self.img_h,\n                                                  rotate=False, ud=False, multi_fonts=False)\n\n    def on_epoch_begin(self, epoch, logs={}):\n        # rebind the paint function to implement curriculum learning\n        if epoch >= 3 and epoch < 6:\n            self.paint_func = lambda text: paint_text(text, self.img_w, self.img_h,\n                                                      rotate=False, ud=True, multi_fonts=False)\n        elif epoch >= 6 and epoch < 9:\n            self.paint_func = lambda text: paint_text(text, self.img_w, self.img_h,\n                                                      rotate=False, ud=True, multi_fonts=True)\n        elif epoch >= 9:\n            self.paint_func = lambda text: paint_text(text, self.img_w, self.img_h,\n                                                      rotate=True, ud=True, multi_fonts=True)\n        if epoch >= 21 and self.max_string_len < 12:\n            self.build_word_list(32000, 12, 0.5)\n\n\n# the actual loss calc occurs here despite it not being\n# an internal Keras loss function\n\ndef ctc_lambda_func(args):\n    y_pred, labels, input_length, label_length = args\n    # the 2 is critical here since the first couple outputs of the RNN\n    # tend to be garbage:\n    y_pred = y_pred[:, 2:, :]\n    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n\n\n# For a real OCR application, this should be beam search with a dictionary\n# and language model.  For this example, best path is sufficient.\n\ndef decode_batch(test_func, word_batch):\n    out = test_func([word_batch])[0]\n    ret = []\n    for j in range(out.shape[0]):\n        out_best = list(np.argmax(out[j, 2:], 1))\n        out_best = [k for k, g in itertools.groupby(out_best)]\n        # 26 is space, 27 is CTC blank char\n        outstr = \'\'\n        for c in out_best:\n            if c >= 0 and c < 26:\n                outstr += chr(c + ord(\'a\'))\n            elif c == 26:\n                outstr += \' \'\n        ret.append(outstr)\n    return ret\n\n\nclass VizCallback(keras.callbacks.Callback):\n\n    def __init__(self, run_name, test_func, text_img_gen, num_display_words=6):\n        self.test_func = test_func\n        self.output_dir = os.path.join(\n            OUTPUT_DIR, run_name)\n        self.text_img_gen = text_img_gen\n        self.num_display_words = num_display_words\n        if not os.path.exists(self.output_dir):\n            os.makedirs(self.output_dir)\n\n    def show_edit_distance(self, num):\n        num_left = num\n        mean_norm_ed = 0.0\n        mean_ed = 0.0\n        while num_left > 0:\n            word_batch = next(self.text_img_gen)[0]\n            num_proc = min(word_batch[\'the_input\'].shape[0], num_left)\n            decoded_res = decode_batch(self.test_func, word_batch[\'the_input\'][0:num_proc])\n            for j in range(0, num_proc):\n                edit_dist = editdistance.eval(decoded_res[j], word_batch[\'source_str\'][j])\n                mean_ed += float(edit_dist)\n                mean_norm_ed += float(edit_dist) / len(word_batch[\'source_str\'][j])\n            num_left -= num_proc\n        mean_norm_ed = mean_norm_ed / num\n        mean_ed = mean_ed / num\n        print(\'\\nOut of %d samples:  Mean edit distance: %.3f Mean normalized edit distance: %0.3f\'\n              % (num, mean_ed, mean_norm_ed))\n\n    def on_epoch_end(self, epoch, logs={}):\n        self.model.save_weights(os.path.join(self.output_dir, \'weights%02d.h5\' % (epoch)))\n        self.show_edit_distance(256)\n        word_batch = next(self.text_img_gen)[0]\n        res = decode_batch(self.test_func, word_batch[\'the_input\'][0:self.num_display_words])\n        if word_batch[\'the_input\'][0].shape[0] < 256:\n            cols = 2\n        else:\n            cols = 1\n        for i in range(self.num_display_words):\n            pylab.subplot(self.num_display_words // cols, cols, i + 1)\n            if K.image_data_format() == \'channels_first\':\n                the_input = word_batch[\'the_input\'][i, 0, :, :]\n            else:\n                the_input = word_batch[\'the_input\'][i, :, :, 0]\n            pylab.imshow(the_input.T, cmap=\'Greys_r\')\n            pylab.xlabel(\'Truth = \\\'%s\\\'\\nDecoded = \\\'%s\\\'\' % (word_batch[\'source_str\'][i], res[i]))\n        fig = pylab.gcf()\n        fig.set_size_inches(10, 13)\n        pylab.savefig(os.path.join(self.output_dir, \'e%02d.png\' % (epoch)))\n        pylab.close()\n\n\ndef train(run_name, start_epoch, stop_epoch, img_w):\n    # Input Parameters\n    img_h = 64\n    words_per_epoch = 16000\n    val_split = 0.2\n    val_words = int(words_per_epoch * (val_split))\n\n    # Network parameters\n    conv_filters = 16\n    kernel_size = (3, 3)\n    pool_size = 2\n    time_dense_size = 32\n    rnn_size = 512\n\n    if K.image_data_format() == \'channels_first\':\n        input_shape = (1, img_w, img_h)\n    else:\n        input_shape = (img_w, img_h, 1)\n\n    fdir = os.path.dirname(get_file(\'wordlists.tgz\',\n                                    origin=\'http://www.mythic-ai.com/datasets/wordlists.tgz\', untar=True))\n\n    img_gen = TextImageGenerator(monogram_file=os.path.join(fdir, \'wordlist_mono_clean.txt\'),\n                                 bigram_file=os.path.join(fdir, \'wordlist_bi_clean.txt\'),\n                                 minibatch_size=32,\n                                 img_w=img_w,\n                                 img_h=img_h,\n                                 downsample_factor=(pool_size ** 2),\n                                 val_split=words_per_epoch - val_words\n                                 )\n    act = \'relu\'\n    input_data = Input(name=\'the_input\', shape=input_shape, dtype=\'float32\')\n    inner = Conv2D(conv_filters, kernel_size, padding=\'same\',\n                   activation=act, kernel_initializer=\'he_normal\',\n                   name=\'conv1\')(input_data)\n    inner = MaxPooling2D(pool_size=(pool_size, pool_size), name=\'max1\')(inner)\n    inner = Conv2D(conv_filters, kernel_size, padding=\'same\',\n                   activation=act, kernel_initializer=\'he_normal\',\n                   name=\'conv2\')(inner)\n    inner = MaxPooling2D(pool_size=(pool_size, pool_size), name=\'max2\')(inner)\n\n    conv_to_rnn_dims = (img_w // (pool_size ** 2), (img_h // (pool_size ** 2)) * conv_filters)\n    inner = Reshape(target_shape=conv_to_rnn_dims, name=\'reshape\')(inner)\n\n    # cuts down input size going into RNN:\n    inner = Dense(time_dense_size, activation=act, name=\'dense1\')(inner)\n\n    # Two layers of bidirecitonal GRUs\n    # GRU seems to work as well, if not better than LSTM:\n    gru_1 = GRU(rnn_size, return_sequences=True, kernel_initializer=\'he_normal\', name=\'gru1\')(inner)\n    gru_1b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer=\'he_normal\', name=\'gru1_b\')(inner)\n    gru1_merged = add([gru_1, gru_1b])\n    gru_2 = GRU(rnn_size, return_sequences=True, kernel_initializer=\'he_normal\', name=\'gru2\')(gru1_merged)\n    gru_2b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer=\'he_normal\', name=\'gru2_b\')(gru1_merged)\n\n    # transforms RNN output to character activations:\n    inner = Dense(img_gen.get_output_size(), kernel_initializer=\'he_normal\',\n                  name=\'dense2\')(concatenate([gru_2, gru_2b]))\n    y_pred = Activation(\'softmax\', name=\'softmax\')(inner)\n    Model(inputs=input_data, outputs=y_pred).summary()\n\n    labels = Input(name=\'the_labels\', shape=[img_gen.absolute_max_string_len], dtype=\'float32\')\n    input_length = Input(name=\'input_length\', shape=[1], dtype=\'int64\')\n    label_length = Input(name=\'label_length\', shape=[1], dtype=\'int64\')\n    # Keras doesn\'t currently support loss funcs with extra parameters\n    # so CTC loss is implemented in a lambda layer\n    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name=\'ctc\')([y_pred, labels, input_length, label_length])\n\n    # clipnorm seems to speeds up convergence\n    sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n\n    model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n\n    # the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n    model.compile(loss={\'ctc\': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n    if start_epoch > 0:\n        weight_file = os.path.join(OUTPUT_DIR, os.path.join(run_name, \'weights%02d.h5\' % (start_epoch - 1)))\n        model.load_weights(weight_file)\n    # captures output of softmax so we can decode the output during visualization\n    test_func = K.function([input_data], [y_pred])\n\n    viz_cb = VizCallback(run_name, test_func, img_gen.next_val())\n\n    model.fit_generator(generator=img_gen.next_train(), steps_per_epoch=(words_per_epoch - val_words),\n                        epochs=stop_epoch, validation_data=img_gen.next_val(), validation_steps=val_words,\n                        callbacks=[viz_cb, img_gen], initial_epoch=start_epoch)\n\n\nif __name__ == \'__main__\':\n    run_name = datetime.datetime.now().strftime(\'%Y:%m:%d:%H:%M:%S\')\n    train(run_name, 0, 20, 128)\n    # increase to wider images and start at epoch 20. The learned weights are reloaded\n    train(run_name, 20, 25, 512)\n'"
Keras/imdb_bidirectional_lstm.py,0,"b'\'\'\'Train a Bidirectional LSTM on the IMDB sentiment classification task.\n\nOutput after 4 epochs on CPU: ~0.8146\nTime per epoch on CPU (Core i7): ~150s.\n\'\'\'\n\nfrom __future__ import print_function\nimport numpy as np\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\nfrom keras.datasets import imdb\n\n\nmax_features = 20000\n# cut texts after this number of words\n# (among top max_features most common words)\nmaxlen = 100\nbatch_size = 32\n\nprint(\'Loading data...\')\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(x_train), \'train sequences\')\nprint(len(x_test), \'test sequences\')\n\nprint(""Pad sequences (samples x time)"")\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint(\'x_train shape:\', x_train.shape)\nprint(\'x_test shape:\', x_test.shape)\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128, input_length=maxlen))\nmodel.add(Bidirectional(LSTM(64)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation=\'sigmoid\'))\n\n# try using different optimizers and different optimizer configs\nmodel.compile(\'adam\', \'binary_crossentropy\', metrics=[\'accuracy\'])\n\nprint(\'Train...\')\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=4,\n          validation_data=[x_test, y_test])\n'"
Keras/imdb_cnn.py,0,"b""'''This example demonstrates the use of Convolution1D for text classification.\n\nGets to 0.89 test accuracy after 2 epochs.\n90s/epoch on Intel i5 2.4Ghz CPU.\n10s/epoch on Tesla K40 GPU.\n\n'''\n\nfrom __future__ import print_function\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers import Embedding\nfrom keras.layers import Conv1D, GlobalMaxPooling1D\nfrom keras.datasets import imdb\n\n# set parameters:\nmax_features = 5000\nmaxlen = 400\nbatch_size = 32\nembedding_dims = 50\nfilters = 250\nkernel_size = 3\nhidden_dims = 250\nepochs = 2\n\nprint('Loading data...')\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')\n\nprint('Pad sequences (samples x time)')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n\nprint('Build model...')\nmodel = Sequential()\n\n# we start off with an efficient embedding layer which maps\n# our vocab indices into embedding_dims dimensions\nmodel.add(Embedding(max_features,\n                    embedding_dims,\n                    input_length=maxlen))\nmodel.add(Dropout(0.2))\n\n# we add a Convolution1D, which will learn filters\n# word group filters of size filter_length:\nmodel.add(Conv1D(filters,\n                 kernel_size,\n                 padding='valid',\n                 activation='relu',\n                 strides=1))\n# we use max pooling:\nmodel.add(GlobalMaxPooling1D())\n\n# We add a vanilla hidden layer:\nmodel.add(Dense(hidden_dims))\nmodel.add(Dropout(0.2))\nmodel.add(Activation('relu'))\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_data=(x_test, y_test))\n"""
Keras/imdb_cnn_lstm.py,0,"b""'''Train a recurrent convolutional network on the IMDB sentiment\nclassification task.\n\nGets to 0.8498 test accuracy after 2 epochs. 41s/epoch on K520 GPU.\n'''\nfrom __future__ import print_function\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers import Embedding\nfrom keras.layers import LSTM\nfrom keras.layers import Conv1D, MaxPooling1D\nfrom keras.datasets import imdb\n\n# Embedding\nmax_features = 20000\nmaxlen = 100\nembedding_size = 128\n\n# Convolution\nkernel_size = 5\nfilters = 64\npool_size = 4\n\n# LSTM\nlstm_output_size = 70\n\n# Training\nbatch_size = 30\nepochs = 2\n\n'''\nNote:\nbatch_size is highly sensitive.\nOnly 2 epochs are needed as the dataset is very small.\n'''\n\nprint('Loading data...')\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')\n\nprint('Pad sequences (samples x time)')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n\nprint('Build model...')\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, embedding_size, input_length=maxlen))\nmodel.add(Dropout(0.25))\nmodel.add(Conv1D(filters,\n                 kernel_size,\n                 padding='valid',\n                 activation='relu',\n                 strides=1))\nmodel.add(MaxPooling1D(pool_size=pool_size))\nmodel.add(LSTM(lstm_output_size))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nprint('Train...')\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_data=(x_test, y_test))\nscore, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n"""
Keras/imdb_fasttext.py,0,"b'\'\'\'This example demonstrates the use of fasttext for text classification\n\nBased on Joulin et al\'s paper:\n\nBags of Tricks for Efficient Text Classification\nhttps://arxiv.org/abs/1607.01759\n\nResults on IMDB datasets with uni and bi-gram embeddings:\n    Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu.\n    Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu.\n\'\'\'\n\nfrom __future__ import print_function\nimport numpy as np\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Embedding\nfrom keras.layers import GlobalAveragePooling1D\nfrom keras.datasets import imdb\n\n\ndef create_ngram_set(input_list, ngram_value=2):\n    """"""\n    Extract a set of n-grams from a list of integers.\n\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n    {(4, 9), (4, 1), (1, 4), (9, 4)}\n\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n    """"""\n    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n\n\ndef add_ngram(sequences, token_indice, ngram_range=2):\n    """"""\n    Augment the input list of list (sequences) by appending n-grams values.\n\n    Example: adding bi-gram\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n    >>> add_ngram(sequences, token_indice, ngram_range=2)\n    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n\n    Example: adding tri-gram\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n    >>> add_ngram(sequences, token_indice, ngram_range=3)\n    [[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]\n    """"""\n    new_sequences = []\n    for input_list in sequences:\n        new_list = input_list[:]\n        for i in range(len(new_list) - ngram_range + 1):\n            for ngram_value in range(2, ngram_range + 1):\n                ngram = tuple(new_list[i:i + ngram_value])\n                if ngram in token_indice:\n                    new_list.append(token_indice[ngram])\n        new_sequences.append(new_list)\n\n    return new_sequences\n\n# Set parameters:\n# ngram_range = 2 will add bi-grams features\nngram_range = 1\nmax_features = 20000\nmaxlen = 400\nbatch_size = 32\nembedding_dims = 50\nepochs = 5\n\nprint(\'Loading data...\')\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(x_train), \'train sequences\')\nprint(len(x_test), \'test sequences\')\nprint(\'Average train sequence length: {}\'.format(np.mean(list(map(len, x_train)), dtype=int)))\nprint(\'Average test sequence length: {}\'.format(np.mean(list(map(len, x_test)), dtype=int)))\n\nif ngram_range > 1:\n    print(\'Adding {}-gram features\'.format(ngram_range))\n    # Create set of unique n-gram from the training set.\n    ngram_set = set()\n    for input_list in x_train:\n        for i in range(2, ngram_range + 1):\n            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n            ngram_set.update(set_of_ngram)\n\n    # Dictionary mapping n-gram token to a unique integer.\n    # Integer values are greater than max_features in order\n    # to avoid collision with existing features.\n    start_index = max_features + 1\n    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n    indice_token = {token_indice[k]: k for k in token_indice}\n\n    # max_features is the highest integer that could be found in the dataset.\n    max_features = np.max(list(indice_token.keys())) + 1\n\n    # Augmenting x_train and x_test with n-grams features\n    x_train = add_ngram(x_train, token_indice, ngram_range)\n    x_test = add_ngram(x_test, token_indice, ngram_range)\n    print(\'Average train sequence length: {}\'.format(np.mean(list(map(len, x_train)), dtype=int)))\n    print(\'Average test sequence length: {}\'.format(np.mean(list(map(len, x_test)), dtype=int)))\n\nprint(\'Pad sequences (samples x time)\')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint(\'x_train shape:\', x_train.shape)\nprint(\'x_test shape:\', x_test.shape)\n\nprint(\'Build model...\')\nmodel = Sequential()\n\n# we start off with an efficient embedding layer which maps\n# our vocab indices into embedding_dims dimensions\nmodel.add(Embedding(max_features,\n                    embedding_dims,\n                    input_length=maxlen))\n\n# we add a GlobalAveragePooling1D, which will average the embeddings\n# of all words in the document\nmodel.add(GlobalAveragePooling1D())\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\nmodel.add(Dense(1, activation=\'sigmoid\'))\n\nmodel.compile(loss=\'binary_crossentropy\',\n              optimizer=\'adam\',\n              metrics=[\'accuracy\'])\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_data=(x_test, y_test))\n'"
Keras/imdb_lstm.py,0,"b""'''Trains a LSTM on the IMDB sentiment classification task.\nThe dataset is actually too small for LSTM to be of any advantage\ncompared to simpler, much faster methods such as TF-IDF + LogReg.\nNotes:\n\n- RNNs are tricky. Choice of batch size is important,\nchoice of loss and optimizer is critical, etc.\nSome configurations won't converge.\n\n- LSTM loss decrease patterns during training can be quite different\nfrom what you see with CNNs/MLPs/etc.\n'''\nfrom __future__ import print_function\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.layers import LSTM\nfrom keras.datasets import imdb\n\nmax_features = 20000\nmaxlen = 80  # cut texts after this number of words (among top max_features most common words)\nbatch_size = 32\n\nprint('Loading data...')\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')\n\nprint('Pad sequences (samples x time)')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n\nprint('Build model...')\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128))\nmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# try using different optimizers and different optimizer configs\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nprint('Train...')\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=15,\n          validation_data=(x_test, y_test))\nscore, acc = model.evaluate(x_test, y_test,\n                            batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n"""
Keras/lstm_benchmark.py,0,"b""'''Compare LSTM implementations on the IMDB sentiment classification task.\n\nimplementation=0 preprocesses input to the LSTM which typically results in\nfaster computations at the expense of increased peak memory usage as the\npreprocessed input must be kept in memory.\n\nimplementation=1 does away with the preprocessing, meaning that it might take\na little longer, but should require less peak memory.\n\nimplementation=2 concatenates the input, output and forget gate's weights\ninto one, large matrix, resulting in faster computation time as the GPU can\nutilize more cores, at the expense of reduced regularization because the same\ndropout is shared across the gates.\n\nNote that the relative performance of the different implementations can\nvary depending on your device, your model and the size of your data.\n'''\n\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Dense, LSTM, Dropout\nfrom keras.datasets import imdb\n\nmax_features = 20000\nmax_length = 80\nembedding_dim = 256\nbatch_size = 128\nepochs = 10\nmodes = [0, 1, 2]\n\nprint('Loading data...')\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\nX_train = sequence.pad_sequences(X_train, max_length)\nX_test = sequence.pad_sequences(X_test, max_length)\n\n# Compile and train different models while meauring performance.\nresults = []\nfor mode in modes:\n    print('Testing mode: implementation={}'.format(mode))\n\n    model = Sequential()\n    model.add(Embedding(max_features, embedding_dim,\n                        input_length=max_length))\n    model.add(Dropout(0.2))\n    model.add(LSTM(embedding_dim,\n                   dropout=0.2,\n                   recurrent_dropout=0.2,\n                   implementation=mode))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    start_time = time.time()\n    history = model.fit(X_train, y_train,\n                        batch_size=batch_size,\n                        epochs=epochs,\n                        validation_data=(X_test, y_test))\n    average_time_per_epoch = (time.time() - start_time) / epochs\n\n    results.append((history, average_time_per_epoch))\n\n# Compare models' accuracy, loss and elapsed time per epoch.\nplt.style.use('ggplot')\nax1 = plt.subplot2grid((2, 2), (0, 0))\nax1.set_title('Accuracy')\nax1.set_ylabel('Validation Accuracy')\nax1.set_xlabel('Epochs')\nax2 = plt.subplot2grid((2, 2), (1, 0))\nax2.set_title('Loss')\nax2.set_ylabel('Validation Loss')\nax2.set_xlabel('Epochs')\nax3 = plt.subplot2grid((2, 2), (0, 1), rowspan=2)\nax3.set_title('Time')\nax3.set_ylabel('Seconds')\nfor mode, result in zip(modes, results):\n    ax1.plot(result[0].epoch, result[0].history['val_acc'], label=mode)\n    ax2.plot(result[0].epoch, result[0].history['val_loss'], label=mode)\nax1.legend()\nax2.legend()\nax3.bar(np.arange(len(results)), [x[1] for x in results],\n        tick_label=modes, align='center')\nplt.tight_layout()\nplt.show()\n"""
Keras/lstm_text_generation.py,0,"b'\'\'\'Example script to generate text from Nietzsche\'s writings.\n\nAt least 20 epochs are required before the generated text\nstarts sounding coherent.\n\nIt is recommended to run this script on GPU, as recurrent\nnetworks are quite computationally intensive.\n\nIf you try this script on new data, make sure your corpus\nhas at least ~100k characters. ~1M is better.\n\'\'\'\n\nfrom __future__ import print_function\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import LSTM\nfrom keras.optimizers import RMSprop\nfrom keras.utils.data_utils import get_file\nimport numpy as np\nimport random\nimport sys\n\npath = get_file(\'nietzsche.txt\', origin=\'https://s3.amazonaws.com/text-datasets/nietzsche.txt\')\ntext = open(path).read().lower()\nprint(\'corpus length:\', len(text))\n\nchars = sorted(list(set(text)))\nprint(\'total chars:\', len(chars))\nchar_indices = dict((c, i) for i, c in enumerate(chars))\nindices_char = dict((i, c) for i, c in enumerate(chars))\n\n# cut the text in semi-redundant sequences of maxlen characters\nmaxlen = 40\nstep = 3\nsentences = []\nnext_chars = []\nfor i in range(0, len(text) - maxlen, step):\n    sentences.append(text[i: i + maxlen])\n    next_chars.append(text[i + maxlen])\nprint(\'nb sequences:\', len(sentences))\n\nprint(\'Vectorization...\')\nX = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\ny = np.zeros((len(sentences), len(chars)), dtype=np.bool)\nfor i, sentence in enumerate(sentences):\n    for t, char in enumerate(sentence):\n        X[i, t, char_indices[char]] = 1\n    y[i, char_indices[next_chars[i]]] = 1\n\n\n# build the model: a single LSTM\nprint(\'Build model...\')\nmodel = Sequential()\nmodel.add(LSTM(128, input_shape=(maxlen, len(chars))))\nmodel.add(Dense(len(chars)))\nmodel.add(Activation(\'softmax\'))\n\noptimizer = RMSprop(lr=0.01)\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=optimizer)\n\n\ndef sample(preds, temperature=1.0):\n    # helper function to sample an index from a probability array\n    preds = np.asarray(preds).astype(\'float64\')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n\n# train the model, output generated text after each iteration\nfor iteration in range(1, 60):\n    print()\n    print(\'-\' * 50)\n    print(\'Iteration\', iteration)\n    model.fit(X, y,\n              batch_size=128,\n              epochs=1)\n\n    start_index = random.randint(0, len(text) - maxlen - 1)\n\n    for diversity in [0.2, 0.5, 1.0, 1.2]:\n        print()\n        print(\'----- diversity:\', diversity)\n\n        generated = \'\'\n        sentence = text[start_index: start_index + maxlen]\n        generated += sentence\n        print(\'----- Generating with seed: ""\' + sentence + \'""\')\n        sys.stdout.write(generated)\n\n        for i in range(400):\n            x = np.zeros((1, maxlen, len(chars)))\n            for t, char in enumerate(sentence):\n                x[0, t, char_indices[char]] = 1.\n\n            preds = model.predict(x, verbose=0)[0]\n            next_index = sample(preds, diversity)\n            next_char = indices_char[next_index]\n\n            generated += next_char\n            sentence = sentence[1:] + next_char\n\n            sys.stdout.write(next_char)\n            sys.stdout.flush()\n        print()\n'"
Keras/mnist_acgan.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n""""""\nTrain an Auxiliary Classifier Generative Adversarial Network (ACGAN) on the\nMNIST dataset. See https://arxiv.org/abs/1610.09585 for more details.\n\nYou should start to see reasonable images after ~5 epochs, and good images\nby ~15 epochs. You should use a GPU, as the convolution-heavy operations are\nvery slow on the CPU. Prefer the TensorFlow backend if you plan on iterating,\nas the compilation time can be a blocker using Theano.\n\nTimings:\n\nHardware           | Backend | Time / Epoch\n-------------------------------------------\n CPU               | TF      | 3 hrs\n Titan X (maxwell) | TF      | 4 min\n Titan X (maxwell) | TH      | 7 min\n\nConsult https://github.com/lukedeo/keras-acgan for more information and\nexample output\n""""""\nfrom __future__ import print_function\n\nfrom collections import defaultdict\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\nfrom PIL import Image\n\nfrom six.moves import range\n\nimport keras.backend as K\nfrom keras.datasets import mnist\nfrom keras import layers\nfrom keras.layers import Input, Dense, Reshape, Flatten, Embedding, Dropout\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\nfrom keras.utils.generic_utils import Progbar\nimport numpy as np\n\nnp.random.seed(1337)\n\nK.set_image_data_format(\'channels_first\')\n\n\ndef build_generator(latent_size):\n    # we will map a pair of (z, L), where z is a latent vector and L is a\n    # label drawn from P_c, to image space (..., 1, 28, 28)\n    cnn = Sequential()\n\n    cnn.add(Dense(1024, input_dim=latent_size, activation=\'relu\'))\n    cnn.add(Dense(128 * 7 * 7, activation=\'relu\'))\n    cnn.add(Reshape((128, 7, 7)))\n\n    # upsample to (..., 14, 14)\n    cnn.add(UpSampling2D(size=(2, 2)))\n    cnn.add(Conv2D(256, 5, padding=\'same\',\n                   activation=\'relu\',\n                   kernel_initializer=\'glorot_normal\'))\n\n    # upsample to (..., 28, 28)\n    cnn.add(UpSampling2D(size=(2, 2)))\n    cnn.add(Conv2D(128, 5, padding=\'same\',\n                   activation=\'relu\',\n                   kernel_initializer=\'glorot_normal\'))\n\n    # take a channel axis reduction\n    cnn.add(Conv2D(1, 2, padding=\'same\',\n                   activation=\'tanh\',\n                   kernel_initializer=\'glorot_normal\'))\n\n    # this is the z space commonly refered to in GAN papers\n    latent = Input(shape=(latent_size, ))\n\n    # this will be our label\n    image_class = Input(shape=(1,), dtype=\'int32\')\n\n    # 10 classes in MNIST\n    cls = Flatten()(Embedding(10, latent_size,\n                              embeddings_initializer=\'glorot_normal\')(image_class))\n\n    # hadamard product between z-space and a class conditional embedding\n    h = layers.multiply([latent, cls])\n\n    fake_image = cnn(h)\n\n    return Model([latent, image_class], fake_image)\n\n\ndef build_discriminator():\n    # build a relatively standard conv net, with LeakyReLUs as suggested in\n    # the reference paper\n    cnn = Sequential()\n\n    cnn.add(Conv2D(32, 3, padding=\'same\', strides=2,\n                   input_shape=(1, 28, 28)))\n    cnn.add(LeakyReLU())\n    cnn.add(Dropout(0.3))\n\n    cnn.add(Conv2D(64, 3, padding=\'same\', strides=1))\n    cnn.add(LeakyReLU())\n    cnn.add(Dropout(0.3))\n\n    cnn.add(Conv2D(128, 3, padding=\'same\', strides=2))\n    cnn.add(LeakyReLU())\n    cnn.add(Dropout(0.3))\n\n    cnn.add(Conv2D(256, 3, padding=\'same\', strides=1))\n    cnn.add(LeakyReLU())\n    cnn.add(Dropout(0.3))\n\n    cnn.add(Flatten())\n\n    image = Input(shape=(1, 28, 28))\n\n    features = cnn(image)\n\n    # first output (name=generation) is whether or not the discriminator\n    # thinks the image that is being shown is fake, and the second output\n    # (name=auxiliary) is the class that the discriminator thinks the image\n    # belongs to.\n    fake = Dense(1, activation=\'sigmoid\', name=\'generation\')(features)\n    aux = Dense(10, activation=\'softmax\', name=\'auxiliary\')(features)\n\n    return Model(image, [fake, aux])\n\nif __name__ == \'__main__\':\n\n    # batch and latent size taken from the paper\n    epochs = 50\n    batch_size = 100\n    latent_size = 100\n\n    # Adam parameters suggested in https://arxiv.org/abs/1511.06434\n    adam_lr = 0.0002\n    adam_beta_1 = 0.5\n\n    # build the discriminator\n    discriminator = build_discriminator()\n    discriminator.compile(\n        optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n        loss=[\'binary_crossentropy\', \'sparse_categorical_crossentropy\']\n    )\n\n    # build the generator\n    generator = build_generator(latent_size)\n    generator.compile(optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n                      loss=\'binary_crossentropy\')\n\n    latent = Input(shape=(latent_size, ))\n    image_class = Input(shape=(1,), dtype=\'int32\')\n\n    # get a fake image\n    fake = generator([latent, image_class])\n\n    # we only want to be able to train generation for the combined model\n    discriminator.trainable = False\n    fake, aux = discriminator(fake)\n    combined = Model([latent, image_class], [fake, aux])\n\n    combined.compile(\n        optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n        loss=[\'binary_crossentropy\', \'sparse_categorical_crossentropy\']\n    )\n\n    # get our mnist data, and force it to be of shape (..., 1, 28, 28) with\n    # range [-1, 1]\n    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n    X_train = np.expand_dims(X_train, axis=1)\n\n    X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n    X_test = np.expand_dims(X_test, axis=1)\n\n    num_train, num_test = X_train.shape[0], X_test.shape[0]\n\n    train_history = defaultdict(list)\n    test_history = defaultdict(list)\n\n    for epoch in range(epochs):\n        print(\'Epoch {} of {}\'.format(epoch + 1, epochs))\n\n        num_batches = int(X_train.shape[0] / batch_size)\n        progress_bar = Progbar(target=num_batches)\n\n        epoch_gen_loss = []\n        epoch_disc_loss = []\n\n        for index in range(num_batches):\n            progress_bar.update(index)\n            # generate a new batch of noise\n            noise = np.random.uniform(-1, 1, (batch_size, latent_size))\n\n            # get a batch of real images\n            image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n            label_batch = y_train[index * batch_size:(index + 1) * batch_size]\n\n            # sample some labels from p_c\n            sampled_labels = np.random.randint(0, 10, batch_size)\n\n            # generate a batch of fake images, using the generated labels as a\n            # conditioner. We reshape the sampled labels to be\n            # (batch_size, 1) so that we can feed them into the embedding\n            # layer as a length one sequence\n            generated_images = generator.predict(\n                [noise, sampled_labels.reshape((-1, 1))], verbose=0)\n\n            X = np.concatenate((image_batch, generated_images))\n            y = np.array([1] * batch_size + [0] * batch_size)\n            aux_y = np.concatenate((label_batch, sampled_labels), axis=0)\n\n            # see if the discriminator can figure itself out...\n            epoch_disc_loss.append(discriminator.train_on_batch(X, [y, aux_y]))\n\n            # make new noise. we generate 2 * batch size here such that we have\n            # the generator optimize over an identical number of images as the\n            # discriminator\n            noise = np.random.uniform(-1, 1, (2 * batch_size, latent_size))\n            sampled_labels = np.random.randint(0, 10, 2 * batch_size)\n\n            # we want to train the generator to trick the discriminator\n            # For the generator, we want all the {fake, not-fake} labels to say\n            # not-fake\n            trick = np.ones(2 * batch_size)\n\n            epoch_gen_loss.append(combined.train_on_batch(\n                [noise, sampled_labels.reshape((-1, 1))],\n                [trick, sampled_labels]))\n\n        print(\'\\nTesting for epoch {}:\'.format(epoch + 1))\n\n        # evaluate the testing loss here\n\n        # generate a new batch of noise\n        noise = np.random.uniform(-1, 1, (num_test, latent_size))\n\n        # sample some labels from p_c and generate images from them\n        sampled_labels = np.random.randint(0, 10, num_test)\n        generated_images = generator.predict(\n            [noise, sampled_labels.reshape((-1, 1))], verbose=False)\n\n        X = np.concatenate((X_test, generated_images))\n        y = np.array([1] * num_test + [0] * num_test)\n        aux_y = np.concatenate((y_test, sampled_labels), axis=0)\n\n        # see if the discriminator can figure itself out...\n        discriminator_test_loss = discriminator.evaluate(\n            X, [y, aux_y], verbose=False)\n\n        discriminator_train_loss = np.mean(np.array(epoch_disc_loss), axis=0)\n\n        # make new noise\n        noise = np.random.uniform(-1, 1, (2 * num_test, latent_size))\n        sampled_labels = np.random.randint(0, 10, 2 * num_test)\n\n        trick = np.ones(2 * num_test)\n\n        generator_test_loss = combined.evaluate(\n            [noise, sampled_labels.reshape((-1, 1))],\n            [trick, sampled_labels], verbose=False)\n\n        generator_train_loss = np.mean(np.array(epoch_gen_loss), axis=0)\n\n        # generate an epoch report on performance\n        train_history[\'generator\'].append(generator_train_loss)\n        train_history[\'discriminator\'].append(discriminator_train_loss)\n\n        test_history[\'generator\'].append(generator_test_loss)\n        test_history[\'discriminator\'].append(discriminator_test_loss)\n\n        print(\'{0:<22s} | {1:4s} | {2:15s} | {3:5s}\'.format(\n            \'component\', *discriminator.metrics_names))\n        print(\'-\' * 65)\n\n        ROW_FMT = \'{0:<22s} | {1:<4.2f} | {2:<15.2f} | {3:<5.2f}\'\n        print(ROW_FMT.format(\'generator (train)\',\n                             *train_history[\'generator\'][-1]))\n        print(ROW_FMT.format(\'generator (test)\',\n                             *test_history[\'generator\'][-1]))\n        print(ROW_FMT.format(\'discriminator (train)\',\n                             *train_history[\'discriminator\'][-1]))\n        print(ROW_FMT.format(\'discriminator (test)\',\n                             *test_history[\'discriminator\'][-1]))\n\n        # save weights every epoch\n        generator.save_weights(\n            \'params_generator_epoch_{0:03d}.hdf5\'.format(epoch), True)\n        discriminator.save_weights(\n            \'params_discriminator_epoch_{0:03d}.hdf5\'.format(epoch), True)\n\n        # generate some digits to display\n        noise = np.random.uniform(-1, 1, (100, latent_size))\n\n        sampled_labels = np.array([\n            [i] * 10 for i in range(10)\n        ]).reshape(-1, 1)\n\n        # get a batch to display\n        generated_images = generator.predict(\n            [noise, sampled_labels], verbose=0)\n\n        # arrange them into a grid\n        img = (np.concatenate([r.reshape(-1, 28)\n                               for r in np.split(generated_images, 10)\n                               ], axis=-1) * 127.5 + 127.5).astype(np.uint8)\n\n        Image.fromarray(img).save(\n            \'plot_epoch_{0:03d}_generated.png\'.format(epoch))\n\n    pickle.dump({\'train\': train_history, \'test\': test_history},\n                open(\'acgan-history.pkl\', \'wb\'))\n'"
Keras/mnist_cnn.py,0,"b""'''Trains a simple convnet on the MNIST dataset.\n\nGets to 99.25% test accuracy after 12 epochs\n(there is still a lot of margin for parameter tuning).\n16 seconds per epoch on a GRID K520 GPU.\n'''\n\nfrom __future__ import print_function\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n"""
Keras/mnist_hierarchical_rnn.py,0,"b'""""""This is an example of using Hierarchical RNN (HRNN) to classify MNIST digits.\n\nHRNNs can learn across multiple levels of temporal hiearchy over a complex sequence.\nUsually, the first recurrent layer of an HRNN encodes a sentence (e.g. of word vectors)\ninto a  sentence vector. The second recurrent layer then encodes a sequence of\nsuch vectors (encoded by the first layer) into a document vector. This\ndocument vector is considered to preserve both the word-level and\nsentence-level structure of the context.\n\n# References\n    - [A Hierarchical Neural Autoencoder for Paragraphs and Documents](https://arxiv.org/abs/1506.01057)\n        Encodes paragraphs and documents with HRNN.\n        Results have shown that HRNN outperforms standard\n        RNNs and may play some role in more sophisticated generation tasks like\n        summarization or question answering.\n    - [Hierarchical recurrent neural network for skeleton based action recognition](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7298714)\n        Achieved state-of-the-art results on skeleton based action recognition with 3 levels\n        of bidirectional HRNN combined with fully connected layers.\n\nIn the below MNIST example the first LSTM layer first encodes every\ncolumn of pixels of shape (28, 1) to a column vector of shape (128,). The second LSTM\nlayer encodes then these 28 column vectors of shape (28, 128) to a image vector\nrepresenting the whole image. A final Dense layer is added for prediction.\n\nAfter 5 epochs: train acc: 0.9858, val acc: 0.9864\n""""""\nfrom __future__ import print_function\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, TimeDistributed\nfrom keras.layers import LSTM\n\n# Training parameters.\nbatch_size = 32\nnum_classes = 10\nepochs = 5\n\n# Embedding dimensions.\nrow_hidden = 128\ncol_hidden = 128\n\n# The data, shuffled and split between train and test sets.\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Reshapes data to 4D for Hierarchical RNN.\nx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\nx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\nx_train = x_train.astype(\'float32\')\nx_test = x_test.astype(\'float32\')\nx_train /= 255\nx_test /= 255\nprint(\'x_train shape:\', x_train.shape)\nprint(x_train.shape[0], \'train samples\')\nprint(x_test.shape[0], \'test samples\')\n\n# Converts class vectors to binary class matrices.\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nrow, col, pixel = x_train.shape[1:]\n\n# 4D input.\nx = Input(shape=(row, col, pixel))\n\n# Encodes a row of pixels using TimeDistributed Wrapper.\nencoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n\n# Encodes columns of encoded rows.\nencoded_columns = LSTM(col_hidden)(encoded_rows)\n\n# Final predictions and model.\nprediction = Dense(num_classes, activation=\'softmax\')(encoded_columns)\nmodel = Model(x, prediction)\nmodel.compile(loss=\'categorical_crossentropy\',\n              optimizer=\'rmsprop\',\n              metrics=[\'accuracy\'])\n\n# Training.\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\n\n# Evaluation.\nscores = model.evaluate(x_test, y_test, verbose=0)\nprint(\'Test loss:\', scores[0])\nprint(\'Test accuracy:\', scores[1])\n'"
Keras/mnist_irnn.py,0,"b'\'\'\'This is a reproduction of the IRNN experiment\nwith pixel-by-pixel sequential MNIST in\n""A Simple Way to Initialize Recurrent Networks of Rectified Linear Units""\nby Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton\n\narxiv:1504.00941v2 [cs.NE] 7 Apr 2015\nhttp://arxiv.org/pdf/1504.00941v2.pdf\n\nOptimizer is replaced with RMSprop which yields more stable and steady\nimprovement.\n\nReaches 0.93 train/test accuracy after 900 epochs\n(which roughly corresponds to 1687500 steps in the original paper.)\n\'\'\'\n\nfrom __future__ import print_function\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import SimpleRNN\nfrom keras import initializers\nfrom keras.optimizers import RMSprop\n\nbatch_size = 32\nnum_classes = 10\nepochs = 200\nhidden_units = 100\n\nlearning_rate = 1e-6\nclip_norm = 1.0\n\n# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nx_train = x_train.reshape(x_train.shape[0], -1, 1)\nx_test = x_test.reshape(x_test.shape[0], -1, 1)\nx_train = x_train.astype(\'float32\')\nx_test = x_test.astype(\'float32\')\nx_train /= 255\nx_test /= 255\nprint(\'x_train shape:\', x_train.shape)\nprint(x_train.shape[0], \'train samples\')\nprint(x_test.shape[0], \'test samples\')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nprint(\'Evaluate IRNN...\')\nmodel = Sequential()\nmodel.add(SimpleRNN(hidden_units,\n                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n                    recurrent_initializer=initializers.Identity(gain=1.0),\n                    activation=\'relu\',\n                    input_shape=x_train.shape[1:]))\nmodel.add(Dense(num_classes))\nmodel.add(Activation(\'softmax\'))\nrmsprop = RMSprop(lr=learning_rate)\nmodel.compile(loss=\'categorical_crossentropy\',\n              optimizer=rmsprop,\n              metrics=[\'accuracy\'])\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\n\nscores = model.evaluate(x_test, y_test, verbose=0)\nprint(\'IRNN test score:\', scores[0])\nprint(\'IRNN test accuracy:\', scores[1])\n'"
Keras/mnist_mlp.py,0,"b""'''Trains a simple deep NN on the MNIST dataset.\n\nGets to 98.40% test accuracy after 20 epochs\n(there is *a lot* of margin for parameter tuning).\n2 seconds per epoch on a K520 GPU.\n'''\n\nfrom __future__ import print_function\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import RMSprop\n\n\nbatch_size = 128\nnum_classes = 10\nepochs = 20\n\n# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nx_train = x_train.reshape(60000, 784)\nx_test = x_test.reshape(10000, 784)\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(784,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=RMSprop(),\n              metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n"""
Keras/mnist_net2net.py,0,"b""'''This is an implementation of Net2Net experiment with MNIST in\n'Net2Net: Accelerating Learning via Knowledge Transfer'\nby Tianqi Chen, Ian Goodfellow, and Jonathon Shlens\n\narXiv:1511.05641v4 [cs.LG] 23 Apr 2016\nhttp://arxiv.org/abs/1511.05641\n\nNotes\n- What:\n  + Net2Net is a group of methods to transfer knowledge from a teacher neural\n    net to a student net,so that the student net can be trained faster than\n    from scratch.\n  + The paper discussed two specific methods of Net2Net, i.e. Net2WiderNet\n    and Net2DeeperNet.\n  + Net2WiderNet replaces a model with an equivalent wider model that has\n    more units in each hidden layer.\n  + Net2DeeperNet replaces a model with an equivalent deeper model.\n  + Both are based on the idea of 'function-preserving transformations of\n    neural nets'.\n- Why:\n  + Enable fast exploration of multiple neural nets in experimentation and\n    design process,by creating a series of wider and deeper models with\n    transferable knowledge.\n  + Enable 'lifelong learning system' by gradually adjusting model complexity\n    to data availability,and reusing transferable knowledge.\n\nExperiments\n- Teacher model: a basic CNN model trained on MNIST for 3 epochs.\n- Net2WiderNet experiment:\n  + Student model has a wider Conv2D layer and a wider FC layer.\n  + Comparison of 'random-padding' vs 'net2wider' weight initialization.\n  + With both methods, student model should immediately perform as well as\n    teacher model, but 'net2wider' is slightly better.\n- Net2DeeperNet experiment:\n  + Student model has an extra Conv2D layer and an extra FC layer.\n  + Comparison of 'random-init' vs 'net2deeper' weight initialization.\n  + Starting performance of 'net2deeper' is better than 'random-init'.\n- Hyper-parameters:\n  + SGD with momentum=0.9 is used for training teacher and student models.\n  + Learning rate adjustment: it's suggested to reduce learning rate\n    to 1/10 for student model.\n  + Addition of noise in 'net2wider' is used to break weight symmetry\n    and thus enable full capacity of student models. It is optional\n    when a Dropout layer is used.\n\nResults\n- Tested with 'Theano' backend and 'channels_first' image_data_format.\n- Running on GPU GeForce GTX 980M\n- Performance Comparisons - validation loss values during first 3 epochs:\n(1) teacher_model:             0.075    0.041    0.041\n(2) wider_random_pad:          0.036    0.034    0.032\n(3) wider_net2wider:           0.032    0.030    0.030\n(4) deeper_random_init:        0.061    0.043    0.041\n(5) deeper_net2deeper:         0.032    0.031    0.029\n'''\n\nfrom __future__ import print_function\nfrom six.moves import xrange\nimport numpy as np\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\nfrom keras.optimizers import SGD\nfrom keras.datasets import mnist\n\nif keras.backend.image_data_format() == 'channels_first':\n    input_shape = (1, 28, 28)  # image shape\nelse:\n    input_shape = (28, 28, 1)  # image shape\nnum_class = 10  # number of class\n\n\n# load and pre-process data\ndef preprocess_input(x):\n    return x.reshape((-1, ) + input_shape) / 255.\n\n\ndef preprocess_output(y):\n    return keras.utils.to_categorical(y)\n\n(train_x, train_y), (validation_x, validation_y) = mnist.load_data()\ntrain_x, validation_x = map(preprocess_input, [train_x, validation_x])\ntrain_y, validation_y = map(preprocess_output, [train_y, validation_y])\nprint('Loading MNIST data...')\nprint('train_x shape:', train_x.shape, 'train_y shape:', train_y.shape)\nprint('validation_x shape:', validation_x.shape,\n      'validation_y shape', validation_y.shape)\n\n\n# knowledge transfer algorithms\ndef wider2net_conv2d(teacher_w1, teacher_b1, teacher_w2, new_width, init):\n    '''Get initial weights for a wider conv2d layer with a bigger filters,\n    by 'random-padding' or 'net2wider'.\n\n    # Arguments\n        teacher_w1: `weight` of conv2d layer to become wider,\n          of shape (filters1, num_channel1, kh1, kw1)\n        teacher_b1: `bias` of conv2d layer to become wider,\n          of shape (filters1, )\n        teacher_w2: `weight` of next connected conv2d layer,\n          of shape (filters2, num_channel2, kh2, kw2)\n        new_width: new `filters` for the wider conv2d layer\n        init: initialization algorithm for new weights,\n          either 'random-pad' or 'net2wider'\n    '''\n    assert teacher_w1.shape[0] == teacher_w2.shape[1], (\n        'successive layers from teacher model should have compatible shapes')\n    assert teacher_w1.shape[0] == teacher_b1.shape[0], (\n        'weight and bias from same layer should have compatible shapes')\n    assert new_width > teacher_w1.shape[0], (\n        'new width (filters) should be bigger than the existing one')\n\n    n = new_width - teacher_w1.shape[0]\n    if init == 'random-pad':\n        new_w1 = np.random.normal(0, 0.1, size=(n, ) + teacher_w1.shape[1:])\n        new_b1 = np.ones(n) * 0.1\n        new_w2 = np.random.normal(0, 0.1, size=(\n            teacher_w2.shape[0], n) + teacher_w2.shape[2:])\n    elif init == 'net2wider':\n        index = np.random.randint(teacher_w1.shape[0], size=n)\n        factors = np.bincount(index)[index] + 1.\n        new_w1 = teacher_w1[index, :, :, :]\n        new_b1 = teacher_b1[index]\n        new_w2 = teacher_w2[:, index, :, :] / factors.reshape((1, -1, 1, 1))\n    else:\n        raise ValueError('Unsupported weight initializer: %s' % init)\n\n    student_w1 = np.concatenate((teacher_w1, new_w1), axis=0)\n    if init == 'random-pad':\n        student_w2 = np.concatenate((teacher_w2, new_w2), axis=1)\n    elif init == 'net2wider':\n        # add small noise to break symmetry, so that student model will have\n        # full capacity later\n        noise = np.random.normal(0, 5e-2 * new_w2.std(), size=new_w2.shape)\n        student_w2 = np.concatenate((teacher_w2, new_w2 + noise), axis=1)\n        student_w2[:, index, :, :] = new_w2\n    student_b1 = np.concatenate((teacher_b1, new_b1), axis=0)\n\n    return student_w1, student_b1, student_w2\n\n\ndef wider2net_fc(teacher_w1, teacher_b1, teacher_w2, new_width, init):\n    '''Get initial weights for a wider fully connected (dense) layer\n       with a bigger nout, by 'random-padding' or 'net2wider'.\n\n    # Arguments\n        teacher_w1: `weight` of fc layer to become wider,\n          of shape (nin1, nout1)\n        teacher_b1: `bias` of fc layer to become wider,\n          of shape (nout1, )\n        teacher_w2: `weight` of next connected fc layer,\n          of shape (nin2, nout2)\n        new_width: new `nout` for the wider fc layer\n        init: initialization algorithm for new weights,\n          either 'random-pad' or 'net2wider'\n    '''\n    assert teacher_w1.shape[1] == teacher_w2.shape[0], (\n        'successive layers from teacher model should have compatible shapes')\n    assert teacher_w1.shape[1] == teacher_b1.shape[0], (\n        'weight and bias from same layer should have compatible shapes')\n    assert new_width > teacher_w1.shape[1], (\n        'new width (nout) should be bigger than the existing one')\n\n    n = new_width - teacher_w1.shape[1]\n    if init == 'random-pad':\n        new_w1 = np.random.normal(0, 0.1, size=(teacher_w1.shape[0], n))\n        new_b1 = np.ones(n) * 0.1\n        new_w2 = np.random.normal(0, 0.1, size=(n, teacher_w2.shape[1]))\n    elif init == 'net2wider':\n        index = np.random.randint(teacher_w1.shape[1], size=n)\n        factors = np.bincount(index)[index] + 1.\n        new_w1 = teacher_w1[:, index]\n        new_b1 = teacher_b1[index]\n        new_w2 = teacher_w2[index, :] / factors[:, np.newaxis]\n    else:\n        raise ValueError('Unsupported weight initializer: %s' % init)\n\n    student_w1 = np.concatenate((teacher_w1, new_w1), axis=1)\n    if init == 'random-pad':\n        student_w2 = np.concatenate((teacher_w2, new_w2), axis=0)\n    elif init == 'net2wider':\n        # add small noise to break symmetry, so that student model will have\n        # full capacity later\n        noise = np.random.normal(0, 5e-2 * new_w2.std(), size=new_w2.shape)\n        student_w2 = np.concatenate((teacher_w2, new_w2 + noise), axis=0)\n        student_w2[index, :] = new_w2\n    student_b1 = np.concatenate((teacher_b1, new_b1), axis=0)\n\n    return student_w1, student_b1, student_w2\n\n\ndef deeper2net_conv2d(teacher_w):\n    '''Get initial weights for a deeper conv2d layer by net2deeper'.\n\n    # Arguments\n        teacher_w: `weight` of previous conv2d layer,\n          of shape (filters, num_channel, kh, kw)\n    '''\n    filters, num_channel, kh, kw = teacher_w.shape\n    student_w = np.zeros((filters, filters, kh, kw))\n    for i in xrange(filters):\n        student_w[i, i, (kh - 1) / 2, (kw - 1) / 2] = 1.\n    student_b = np.zeros(filters)\n    return student_w, student_b\n\n\ndef copy_weights(teacher_model, student_model, layer_names):\n    '''Copy weights from teacher_model to student_model,\n     for layers with names listed in layer_names\n    '''\n    for name in layer_names:\n        weights = teacher_model.get_layer(name=name).get_weights()\n        student_model.get_layer(name=name).set_weights(weights)\n\n\n# methods to construct teacher_model and student_models\ndef make_teacher_model(train_data, validation_data, epochs=3):\n    '''Train a simple CNN as teacher model.\n    '''\n    model = Sequential()\n    model.add(Conv2D(64, 3, input_shape=input_shape,\n                     padding='same', name='conv1'))\n    model.add(MaxPooling2D(2, name='pool1'))\n    model.add(Conv2D(64, 3, padding='same', name='conv2'))\n    model.add(MaxPooling2D(2, name='pool2'))\n    model.add(Flatten(name='flatten'))\n    model.add(Dense(64, activation='relu', name='fc1'))\n    model.add(Dense(num_class, activation='softmax', name='fc2'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=SGD(lr=0.01, momentum=0.9),\n                  metrics=['accuracy'])\n\n    train_x, train_y = train_data\n    history = model.fit(train_x, train_y,\n                        epochs=epochs,\n                        validation_data=validation_data)\n    return model, history\n\n\ndef make_wider_student_model(teacher_model, train_data,\n                             validation_data, init, epochs=3):\n    '''Train a wider student model based on teacher_model,\n       with either 'random-pad' (baseline) or 'net2wider'\n    '''\n    new_conv1_width = 128\n    new_fc1_width = 128\n\n    model = Sequential()\n    # a wider conv1 compared to teacher_model\n    model.add(Conv2D(new_conv1_width, 3, input_shape=input_shape,\n                     padding='same', name='conv1'))\n    model.add(MaxPooling2D(2, name='pool1'))\n    model.add(Conv2D(64, 3, padding='same', name='conv2'))\n    model.add(MaxPooling2D(2, name='pool2'))\n    model.add(Flatten(name='flatten'))\n    # a wider fc1 compared to teacher model\n    model.add(Dense(new_fc1_width, activation='relu', name='fc1'))\n    model.add(Dense(num_class, activation='softmax', name='fc2'))\n\n    # The weights for other layers need to be copied from teacher_model\n    # to student_model, except for widened layers\n    # and their immediate downstreams, which will be initialized separately.\n    # For this example there are no other layers that need to be copied.\n\n    w_conv1, b_conv1 = teacher_model.get_layer('conv1').get_weights()\n    w_conv2, b_conv2 = teacher_model.get_layer('conv2').get_weights()\n    new_w_conv1, new_b_conv1, new_w_conv2 = wider2net_conv2d(\n        w_conv1, b_conv1, w_conv2, new_conv1_width, init)\n    model.get_layer('conv1').set_weights([new_w_conv1, new_b_conv1])\n    model.get_layer('conv2').set_weights([new_w_conv2, b_conv2])\n\n    w_fc1, b_fc1 = teacher_model.get_layer('fc1').get_weights()\n    w_fc2, b_fc2 = teacher_model.get_layer('fc2').get_weights()\n    new_w_fc1, new_b_fc1, new_w_fc2 = wider2net_fc(\n        w_fc1, b_fc1, w_fc2, new_fc1_width, init)\n    model.get_layer('fc1').set_weights([new_w_fc1, new_b_fc1])\n    model.get_layer('fc2').set_weights([new_w_fc2, b_fc2])\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=SGD(lr=0.001, momentum=0.9),\n                  metrics=['accuracy'])\n\n    train_x, train_y = train_data\n    history = model.fit(train_x, train_y,\n                        epochs=epochs,\n                        validation_data=validation_data)\n    return model, history\n\n\ndef make_deeper_student_model(teacher_model, train_data,\n                              validation_data, init, epochs=3):\n    '''Train a deeper student model based on teacher_model,\n       with either 'random-init' (baseline) or 'net2deeper'\n    '''\n    model = Sequential()\n    model.add(Conv2D(64, 3, input_shape=input_shape,\n                     padding='same', name='conv1'))\n    model.add(MaxPooling2D(2, name='pool1'))\n    model.add(Conv2D(64, 3, padding='same', name='conv2'))\n    # add another conv2d layer to make original conv2 deeper\n    if init == 'net2deeper':\n        prev_w, _ = model.get_layer('conv2').get_weights()\n        new_weights = deeper2net_conv2d(prev_w)\n        model.add(Conv2D(64, 3, padding='same',\n                         name='conv2-deeper', weights=new_weights))\n    elif init == 'random-init':\n        model.add(Conv2D(64, 3, padding='same', name='conv2-deeper'))\n    else:\n        raise ValueError('Unsupported weight initializer: %s' % init)\n    model.add(MaxPooling2D(2, name='pool2'))\n    model.add(Flatten(name='flatten'))\n    model.add(Dense(64, activation='relu', name='fc1'))\n    # add another fc layer to make original fc1 deeper\n    if init == 'net2deeper':\n        # net2deeper for fc layer with relu, is just an identity initializer\n        model.add(Dense(64, init='identity',\n                        activation='relu', name='fc1-deeper'))\n    elif init == 'random-init':\n        model.add(Dense(64, activation='relu', name='fc1-deeper'))\n    else:\n        raise ValueError('Unsupported weight initializer: %s' % init)\n    model.add(Dense(num_class, activation='softmax', name='fc2'))\n\n    # copy weights for other layers\n    copy_weights(teacher_model, model, layer_names=[\n                 'conv1', 'conv2', 'fc1', 'fc2'])\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=SGD(lr=0.001, momentum=0.9),\n                  metrics=['accuracy'])\n\n    train_x, train_y = train_data\n    history = model.fit(train_x, train_y,\n                        epochs=epochs,\n                        validation_data=validation_data)\n    return model, history\n\n\n# experiments setup\ndef net2wider_experiment():\n    '''Benchmark performances of\n    (1) a teacher model,\n    (2) a wider student model with `random_pad` initializer\n    (3) a wider student model with `Net2WiderNet` initializer\n    '''\n    train_data = (train_x, train_y)\n    validation_data = (validation_x, validation_y)\n    print('\\nExperiment of Net2WiderNet ...')\n    print('\\nbuilding teacher model ...')\n    teacher_model, _ = make_teacher_model(train_data,\n                                          validation_data,\n                                          epochs=3)\n\n    print('\\nbuilding wider student model by random padding ...')\n    make_wider_student_model(teacher_model, train_data,\n                             validation_data, 'random-pad',\n                             epochs=3)\n    print('\\nbuilding wider student model by net2wider ...')\n    make_wider_student_model(teacher_model, train_data,\n                             validation_data, 'net2wider',\n                             epochs=3)\n\n\ndef net2deeper_experiment():\n    '''Benchmark performances of\n    (1) a teacher model,\n    (2) a deeper student model with `random_init` initializer\n    (3) a deeper student model with `Net2DeeperNet` initializer\n    '''\n    train_data = (train_x, train_y)\n    validation_data = (validation_x, validation_y)\n    print('\\nExperiment of Net2DeeperNet ...')\n    print('\\nbuilding teacher model ...')\n    teacher_model, _ = make_teacher_model(train_data,\n                                          validation_data,\n                                          epochs=3)\n\n    print('\\nbuilding deeper student model by random init ...')\n    make_deeper_student_model(teacher_model, train_data,\n                              validation_data, 'random-init',\n                              epochs=3)\n    print('\\nbuilding deeper student model by net2deeper ...')\n    make_deeper_student_model(teacher_model, train_data,\n                              validation_data, 'net2deeper',\n                              epochs=3)\n\n# run the experiments\nnet2wider_experiment()\nnet2deeper_experiment()\n"""
Keras/mnist_siamese_graph.py,0,"b'\'\'\'Train a Siamese MLP on pairs of digits from the MNIST dataset.\n\nIt follows Hadsell-et-al.\'06 [1] by computing the Euclidean distance on the\noutput of the shared network and by optimizing the contrastive loss (see paper\nfor mode details).\n\n[1] ""Dimensionality Reduction by Learning an Invariant Mapping""\n    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n\nGets to 99.5% test accuracy after 20 epochs.\n3 seconds per epoch on a Titan X GPU\n\'\'\'\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nimport numpy as np\n\nimport random\nfrom keras.datasets import mnist\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Input, Lambda\nfrom keras.optimizers import RMSprop\nfrom keras import backend as K\n\n\ndef euclidean_distance(vects):\n    x, y = vects\n    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n\n\ndef eucl_dist_output_shape(shapes):\n    shape1, shape2 = shapes\n    return (shape1[0], 1)\n\n\ndef contrastive_loss(y_true, y_pred):\n    \'\'\'Contrastive loss from Hadsell-et-al.\'06\n    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n    \'\'\'\n    margin = 1\n    return K.mean(y_true * K.square(y_pred) +\n                  (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n\n\ndef create_pairs(x, digit_indices):\n    \'\'\'Positive and negative pair creation.\n    Alternates between positive and negative pairs.\n    \'\'\'\n    pairs = []\n    labels = []\n    n = min([len(digit_indices[d]) for d in range(10)]) - 1\n    for d in range(10):\n        for i in range(n):\n            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n            pairs += [[x[z1], x[z2]]]\n            inc = random.randrange(1, 10)\n            dn = (d + inc) % 10\n            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n            pairs += [[x[z1], x[z2]]]\n            labels += [1, 0]\n    return np.array(pairs), np.array(labels)\n\n\ndef create_base_network(input_dim):\n    \'\'\'Base network to be shared (eq. to feature extraction).\n    \'\'\'\n    seq = Sequential()\n    seq.add(Dense(128, input_shape=(input_dim,), activation=\'relu\'))\n    seq.add(Dropout(0.1))\n    seq.add(Dense(128, activation=\'relu\'))\n    seq.add(Dropout(0.1))\n    seq.add(Dense(128, activation=\'relu\'))\n    return seq\n\n\ndef compute_accuracy(predictions, labels):\n    \'\'\'Compute classification accuracy with a fixed threshold on distances.\n    \'\'\'\n    return labels[predictions.ravel() < 0.5].mean()\n\n\n# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train.reshape(60000, 784)\nx_test = x_test.reshape(10000, 784)\nx_train = x_train.astype(\'float32\')\nx_test = x_test.astype(\'float32\')\nx_train /= 255\nx_test /= 255\ninput_dim = 784\nepochs = 20\n\n# create training+test positive and negative pairs\ndigit_indices = [np.where(y_train == i)[0] for i in range(10)]\ntr_pairs, tr_y = create_pairs(x_train, digit_indices)\n\ndigit_indices = [np.where(y_test == i)[0] for i in range(10)]\nte_pairs, te_y = create_pairs(x_test, digit_indices)\n\n# network definition\nbase_network = create_base_network(input_dim)\n\ninput_a = Input(shape=(input_dim,))\ninput_b = Input(shape=(input_dim,))\n\n# because we re-use the same instance `base_network`,\n# the weights of the network\n# will be shared across the two branches\nprocessed_a = base_network(input_a)\nprocessed_b = base_network(input_b)\n\ndistance = Lambda(euclidean_distance,\n                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n\nmodel = Model([input_a, input_b], distance)\n\n# train\nrms = RMSprop()\nmodel.compile(loss=contrastive_loss, optimizer=rms)\nmodel.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n          batch_size=128,\n          epochs=epochs,\n          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))\n\n# compute final accuracy on training and test sets\npred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\ntr_acc = compute_accuracy(pred, tr_y)\npred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\nte_acc = compute_accuracy(pred, te_y)\n\nprint(\'* Accuracy on training set: %0.2f%%\' % (100 * tr_acc))\nprint(\'* Accuracy on test set: %0.2f%%\' % (100 * te_acc))\n'"
Keras/mnist_sklearn_wrapper.py,0,"b""'''Example of how to use sklearn wrapper\n\nBuilds simple CNN models on MNIST and uses sklearn's GridSearchCV to find best model\n'''\n\nfrom __future__ import print_function\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras import backend as K\nfrom sklearn.grid_search import GridSearchCV\n\n\nnum_classes = 10\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# load training data and do basic data normalization\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n\ndef make_model(dense_layer_sizes, filters, kernel_size, pool_size):\n    '''Creates model comprised of 2 convolutional layers followed by dense layers\n\n    dense_layer_sizes: List of layer sizes.\n        This list has one number for each layer\n    filters: Number of convolutional filters in each convolutional layer\n    kernel_size: Convolutional kernel size\n    pool_size: Size of pooling area for max pooling\n    '''\n\n    model = Sequential()\n    model.add(Conv2D(filters, kernel_size,\n                     padding='valid',\n                     input_shape=input_shape))\n    model.add(Activation('relu'))\n    model.add(Conv2D(filters, kernel_size))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=pool_size))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    for layer_size in dense_layer_sizes:\n        model.add(Dense(layer_size))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(num_classes))\n    model.add(Activation('softmax'))\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adadelta',\n                  metrics=['accuracy'])\n\n    return model\n\ndense_size_candidates = [[32], [64], [32, 32], [64, 64]]\nmy_classifier = KerasClassifier(make_model, batch_size=32)\nvalidator = GridSearchCV(my_classifier,\n                         param_grid={'dense_layer_sizes': dense_size_candidates,\n                                     # epochs is avail for tuning even when not\n                                     # an argument to model building function\n                                     'epochs': [3, 6],\n                                     'filters': [8],\n                                     'kernel_size': [3],\n                                     'pool_size': [2]},\n                         scoring='neg_log_loss',\n                         n_jobs=1)\nvalidator.fit(x_train, y_train)\n\nprint('The parameters of the best model are: ')\nprint(validator.best_params_)\n\n# validator.best_estimator_ returns sklearn-wrapped version of best model.\n# validator.best_estimator_.model returns the (unwrapped) keras model\nbest_model = validator.best_estimator_.model\nmetric_names = best_model.metrics_names\nmetric_values = best_model.evaluate(x_test, y_test)\nfor metric, value in zip(metric_names, metric_values):\n    print(metric, ': ', value)\n"""
Keras/mnist_swwae.py,0,"b'\'\'\'Trains a stacked what-where autoencoder built on residual blocks on the\r\nMNIST dataset.  It exemplifies two influential methods that have been developed\r\nin the past few years.\r\n\r\nThe first is the idea of properly \'unpooling.\' During any max pool, the\r\nexact location (the \'where\') of the maximal value in a pooled receptive field\r\nis lost, however it can be very useful in the overall reconstruction of an\r\ninput image.  Therefore, if the \'where\' is handed from the encoder\r\nto the corresponding decoder layer, features being decoded can be \'placed\' in\r\nthe right location, allowing for reconstructions of much higher fidelity.\r\n\r\nReferences:\r\n[1]\r\n\'Visualizing and Understanding Convolutional Networks\'\r\nMatthew D Zeiler, Rob Fergus\r\nhttps://arxiv.org/abs/1311.2901v3\r\n\r\n[2]\r\n\'Stacked What-Where Auto-encoders\'\r\nJunbo Zhao, Michael Mathieu, Ross Goroshin, Yann LeCun\r\nhttps://arxiv.org/abs/1506.02351v8\r\n\r\nThe second idea exploited here is that of residual learning.  Residual blocks\r\nease the training process by allowing skip connections that give the network\r\nthe ability to be as linear (or non-linear) as the data sees fit.  This allows\r\nfor much deep networks to be easily trained.  The residual element seems to\r\nbe advantageous in the context of this example as it allows a nice symmetry\r\nbetween the encoder and decoder.  Normally, in the decoder, the final\r\nprojection to the space where the image is reconstructed is linear, however\r\nthis does not have to be the case for a residual block as the degree to which\r\nits output is linear or non-linear is determined by the data it is fed.\r\nHowever, in order to cap the reconstruction in this example, a hard softmax is\r\napplied as a bias because we know the MNIST digits are mapped to [0,1].\r\n\r\nReferences:\r\n[3]\r\n\'Deep Residual Learning for Image Recognition\'\r\nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\r\nhttps://arxiv.org/abs/1512.03385v1\r\n\r\n[4]\r\n\'Identity Mappings in Deep Residual Networks\'\r\nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\r\nhttps://arxiv.org/abs/1603.05027v3\r\n\r\n\'\'\'\r\nfrom __future__ import print_function\r\nimport numpy as np\r\n\r\nfrom keras.datasets import mnist\r\nfrom keras.models import Model\r\nfrom keras.layers import Activation\r\nfrom keras.layers import UpSampling2D, Conv2D, MaxPooling2D\r\nfrom keras.layers import Input, BatchNormalization, ELU\r\nimport matplotlib.pyplot as plt\r\nimport keras.backend as K\r\nfrom keras import layers\r\n\r\n\r\ndef convresblock(x, nfeats=8, ksize=3, nskipped=2, elu=True):\r\n    """"""The proposed residual block from [4].\r\n\r\n    Running with elu=True will use ELU nonlinearity and running with\r\n    elu=False will use BatchNorm + RELU nonlinearity.  While ELU\'s are fast\r\n    due to the fact they do not suffer from BatchNorm overhead, they may\r\n    overfit because they do not offer the stochastic element of the batch\r\n    formation process of BatchNorm, which acts as a good regularizer.\r\n\r\n    # Arguments\r\n        x: 4D tensor, the tensor to feed through the block\r\n        nfeats: Integer, number of feature maps for conv layers.\r\n        ksize: Integer, width and height of conv kernels in first convolution.\r\n        nskipped: Integer, number of conv layers for the residual function.\r\n        elu: Boolean, whether to use ELU or BN+RELU.\r\n\r\n    # Input shape\r\n        4D tensor with shape:\r\n        `(batch, channels, rows, cols)`\r\n\r\n    # Output shape\r\n        4D tensor with shape:\r\n        `(batch, filters, rows, cols)`\r\n    """"""\r\n    y0 = Conv2D(nfeats, ksize, padding=\'same\')(x)\r\n    y = y0\r\n    for i in range(nskipped):\r\n        if elu:\r\n            y = ELU()(y)\r\n        else:\r\n            y = BatchNormalization(axis=1)(y)\r\n            y = Activation(\'relu\')(y)\r\n        y = Conv2D(nfeats, 1, padding=\'same\')(y)\r\n    return layers.add([y0, y])\r\n\r\n\r\ndef getwhere(x):\r\n    \'\'\' Calculate the \'where\' mask that contains switches indicating which\r\n    index contained the max value when MaxPool2D was applied.  Using the\r\n    gradient of the sum is a nice trick to keep everything high level.\'\'\'\r\n    y_prepool, y_postpool = x\r\n    return K.gradients(K.sum(y_postpool), y_prepool)\r\n\r\nif K.backend() == \'tensorflow\':\r\n    raise RuntimeError(\'This example can only run with the \'\r\n                       \'Theano backend for the time being, \'\r\n                       \'because it requires taking the gradient \'\r\n                       \'of a gradient, which isn\\\'t \'\r\n                       \'supported for all TF ops.\')\r\n\r\n# This example assume \'channels_first\' data format.\r\nK.set_image_data_format(\'channels_first\')\r\n\r\n# input image dimensions\r\nimg_rows, img_cols = 28, 28\r\n\r\n# the data, shuffled and split between train and test sets\r\n(x_train, _), (x_test, _) = mnist.load_data()\r\n\r\nx_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\nx_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\nx_train = x_train.astype(\'float32\')\r\nx_test = x_test.astype(\'float32\')\r\nx_train /= 255\r\nx_test /= 255\r\nprint(\'x_train shape:\', x_train.shape)\r\nprint(x_train.shape[0], \'train samples\')\r\nprint(x_test.shape[0], \'test samples\')\r\n\r\n# The size of the kernel used for the MaxPooling2D\r\npool_size = 2\r\n# The total number of feature maps at each layer\r\nnfeats = [8, 16, 32, 64, 128]\r\n# The sizes of the pooling kernel at each layer\r\npool_sizes = np.array([1, 1, 1, 1, 1]) * pool_size\r\n# The convolution kernel size\r\nksize = 3\r\n# Number of epochs to train for\r\nepochs = 5\r\n# Batch size during training\r\nbatch_size = 128\r\n\r\nif pool_size == 2:\r\n    # if using a 5 layer net of pool_size = 2\r\n    x_train = np.pad(x_train, [[0, 0], [0, 0], [2, 2], [2, 2]],\r\n                     mode=\'constant\')\r\n    x_test = np.pad(x_test, [[0, 0], [0, 0], [2, 2], [2, 2]], mode=\'constant\')\r\n    nlayers = 5\r\nelif pool_size == 3:\r\n    # if using a 3 layer net of pool_size = 3\r\n    x_train = x_train[:, :, :-1, :-1]\r\n    x_test = x_test[:, :, :-1, :-1]\r\n    nlayers = 3\r\nelse:\r\n    import sys\r\n    sys.exit(\'Script supports pool_size of 2 and 3.\')\r\n\r\n# Shape of input to train on (note that model is fully convolutional however)\r\ninput_shape = x_train.shape[1:]\r\n# The final list of the size of axis=1 for all layers, including input\r\nnfeats_all = [input_shape[0]] + nfeats\r\n\r\n# First build the encoder, all the while keeping track of the \'where\' masks\r\nimg_input = Input(shape=input_shape)\r\n\r\n# We push the \'where\' masks to the following list\r\nwheres = [None] * nlayers\r\ny = img_input\r\nfor i in range(nlayers):\r\n    y_prepool = convresblock(y, nfeats=nfeats_all[i + 1], ksize=ksize)\r\n    y = MaxPooling2D(pool_size=(pool_sizes[i], pool_sizes[i]))(y_prepool)\r\n    wheres[i] = layers.Lambda(\r\n        getwhere, output_shape=lambda x: x[0])([y_prepool, y])\r\n\r\n# Now build the decoder, and use the stored \'where\' masks to place the features\r\nfor i in range(nlayers):\r\n    ind = nlayers - 1 - i\r\n    y = UpSampling2D(size=(pool_sizes[ind], pool_sizes[ind]))(y)\r\n    y = layers.multiply([y, wheres[ind]])\r\n    y = convresblock(y, nfeats=nfeats_all[ind], ksize=ksize)\r\n\r\n# Use hard_simgoid to clip range of reconstruction\r\ny = Activation(\'hard_sigmoid\')(y)\r\n\r\n# Define the model and it\'s mean square error loss, and compile it with Adam\r\nmodel = Model(img_input, y)\r\nmodel.compile(\'adam\', \'mse\')\r\n\r\n# Fit the model\r\nmodel.fit(x_train, x_train,\r\n          batch_size=batch_size,\r\n          epochs=epochs,\r\n          validation_data=(x_test, x_test))\r\n\r\n# Plot\r\nx_recon = model.predict(x_test[:25])\r\nx_plot = np.concatenate((x_test[:25], x_recon), axis=1)\r\nx_plot = x_plot.reshape((5, 10, input_shape[-2], input_shape[-1]))\r\nx_plot = np.vstack([np.hstack(x) for x in x_plot])\r\nplt.figure()\r\nplt.axis(\'off\')\r\nplt.title(\'Test Samples: Originals/Reconstructions\')\r\nplt.imshow(x_plot, interpolation=\'none\', cmap=\'gray\')\r\nplt.savefig(\'reconstructions.png\')\r\n'"
Keras/mnist_transfer_cnn.py,0,"b""'''Transfer learning toy example:\n\n1- Train a simple convnet on the MNIST dataset the first 5 digits [0..4].\n2- Freeze convolutional layers and fine-tune dense layers\n   for the classification of digits [5..9].\n\nRun on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_transfer_cnn.py\n\nGet to 99.8% test accuracy after 5 epochs\nfor the first five digits classifier\nand 99.2% for the last five digits after transfer + fine-tuning.\n'''\n\nfrom __future__ import print_function\n\nimport datetime\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K\n\nnow = datetime.datetime.now\n\nbatch_size = 128\nnum_classes = 5\nepochs = 5\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n# number of convolutional filters to use\nfilters = 32\n# size of pooling area for max pooling\npool_size = 2\n# convolution kernel size\nkernel_size = 3\n\nif K.image_data_format() == 'channels_first':\n    input_shape = (1, img_rows, img_cols)\nelse:\n    input_shape = (img_rows, img_cols, 1)\n\n\ndef train_model(model, train, test, num_classes):\n    x_train = train[0].reshape((train[0].shape[0],) + input_shape)\n    x_test = test[0].reshape((test[0].shape[0],) + input_shape)\n    x_train = x_train.astype('float32')\n    x_test = x_test.astype('float32')\n    x_train /= 255\n    x_test /= 255\n    print('x_train shape:', x_train.shape)\n    print(x_train.shape[0], 'train samples')\n    print(x_test.shape[0], 'test samples')\n\n    # convert class vectors to binary class matrices\n    y_train = keras.utils.to_categorical(train[1], num_classes)\n    y_test = keras.utils.to_categorical(test[1], num_classes)\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adadelta',\n                  metrics=['accuracy'])\n\n    t = now()\n    model.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              verbose=1,\n              validation_data=(x_test, y_test))\n    print('Training time: %s' % (now() - t))\n    score = model.evaluate(x_test, y_test, verbose=0)\n    print('Test score:', score[0])\n    print('Test accuracy:', score[1])\n\n\n# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# create two datasets one with digits below 5 and one with 5 and above\nx_train_lt5 = x_train[y_train < 5]\ny_train_lt5 = y_train[y_train < 5]\nx_test_lt5 = x_test[y_test < 5]\ny_test_lt5 = y_test[y_test < 5]\n\nx_train_gte5 = x_train[y_train >= 5]\ny_train_gte5 = y_train[y_train >= 5] - 5\nx_test_gte5 = x_test[y_test >= 5]\ny_test_gte5 = y_test[y_test >= 5] - 5\n\n# define two groups of layers: feature (convolutions) and classification (dense)\nfeature_layers = [\n    Conv2D(filters, kernel_size,\n           padding='valid',\n           input_shape=input_shape),\n    Activation('relu'),\n    Conv2D(filters, kernel_size),\n    Activation('relu'),\n    MaxPooling2D(pool_size=pool_size),\n    Dropout(0.25),\n    Flatten(),\n]\n\nclassification_layers = [\n    Dense(128),\n    Activation('relu'),\n    Dropout(0.5),\n    Dense(num_classes),\n    Activation('softmax')\n]\n\n# create complete model\nmodel = Sequential(feature_layers + classification_layers)\n\n# train model for 5-digit classification [0..4]\ntrain_model(model,\n            (x_train_lt5, y_train_lt5),\n            (x_test_lt5, y_test_lt5), num_classes)\n\n# freeze feature layers and rebuild model\nfor l in feature_layers:\n    l.trainable = False\n\n# transfer: train dense layers for new classification task [5..9]\ntrain_model(model,\n            (x_train_gte5, y_train_gte5),\n            (x_test_gte5, y_test_gte5), num_classes)\n"""
Keras/neural_doodle.py,0,"b""'''Neural doodle with Keras\r\n\r\nScript Usage:\r\n    # Arguments:\r\n    ```\r\n    --nlabels:              # of regions (colors) in mask images\r\n    --style-image:          image to learn style from\r\n    --style-mask:           semantic labels for style image\r\n    --target-mask:          semantic labels for target image (your doodle)\r\n    --content-image:        optional image to learn content from\r\n    --target-image-prefix:  path prefix for generated target images\r\n    ```\r\n\r\n    # Example 1: doodle using a style image, style mask\r\n    and target mask.\r\n    ```\r\n    python neural_doodle.py --nlabels 4 --style-image Monet/style.png \\\r\n    --style-mask Monet/style_mask.png --target-mask Monet/target_mask.png \\\r\n    --target-image-prefix generated/monet\r\n    ```\r\n\r\n    # Example 2: doodle using a style image, style mask,\r\n    target mask and an optional content image.\r\n    ```\r\n    python neural_doodle.py --nlabels 4 --style-image Renoir/style.png \\\r\n    --style-mask Renoir/style_mask.png --target-mask Renoir/target_mask.png \\\r\n    --content-image Renoir/creek.jpg \\\r\n    --target-image-prefix generated/renoir\r\n    ```\r\n\r\nReferences:\r\n[Dmitry Ulyanov's blog on fast-neural-doodle](http://dmitryulyanov.github.io/feed-forward-neural-doodle/)\r\n[Torch code for fast-neural-doodle](https://github.com/DmitryUlyanov/fast-neural-doodle)\r\n[Torch code for online-neural-doodle](https://github.com/DmitryUlyanov/online-neural-doodle)\r\n[Paper Texture Networks: Feed-forward Synthesis of Textures and Stylized Images](http://arxiv.org/abs/1603.03417)\r\n[Discussion on parameter tuning](https://github.com/fchollet/keras/issues/3705)\r\n\r\nResources:\r\nExample images can be downloaded from\r\nhttps://github.com/DmitryUlyanov/fast-neural-doodle/tree/master/data\r\n'''\r\nfrom __future__ import print_function\r\nimport time\r\nimport argparse\r\nimport numpy as np\r\nfrom scipy.optimize import fmin_l_bfgs_b\r\nfrom scipy.misc import imread, imsave\r\n\r\nfrom keras import backend as K\r\nfrom keras.layers import Input, AveragePooling2D\r\nfrom keras.models import Model\r\nfrom keras.preprocessing.image import load_img, img_to_array\r\nfrom keras.applications import vgg19\r\n\r\n# Command line arguments\r\nparser = argparse.ArgumentParser(description='Keras neural doodle example')\r\nparser.add_argument('--nlabels', type=int,\r\n                    help='number of semantic labels'\r\n                    ' (regions in differnet colors)'\r\n                    ' in style_mask/target_mask')\r\nparser.add_argument('--style-image', type=str,\r\n                    help='path to image to learn style from')\r\nparser.add_argument('--style-mask', type=str,\r\n                    help='path to semantic mask of style image')\r\nparser.add_argument('--target-mask', type=str,\r\n                    help='path to semantic mask of target image')\r\nparser.add_argument('--content-image', type=str, default=None,\r\n                    help='path to optional content image')\r\nparser.add_argument('--target-image-prefix', type=str,\r\n                    help='path prefix for generated results')\r\nargs = parser.parse_args()\r\n\r\nstyle_img_path = args.style_image\r\nstyle_mask_path = args.style_mask\r\ntarget_mask_path = args.target_mask\r\ncontent_img_path = args.content_image\r\ntarget_img_prefix = args.target_image_prefix\r\nuse_content_img = content_img_path is not None\r\n\r\nnum_labels = args.nlabels\r\nnum_colors = 3  # RGB\r\n# determine image sizes based on target_mask\r\nref_img = imread(target_mask_path)\r\nimg_nrows, img_ncols = ref_img.shape[:2]\r\n\r\ntotal_variation_weight = 50.\r\nstyle_weight = 1.\r\ncontent_weight = 0.1 if use_content_img else 0\r\n\r\ncontent_feature_layers = ['block5_conv2']\r\n# To get better generation qualities, use more conv layers for style features\r\nstyle_feature_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1',\r\n                        'block4_conv1', 'block5_conv1']\r\n\r\n\r\n# helper functions for reading/processing images\r\ndef preprocess_image(image_path):\r\n    img = load_img(image_path, target_size=(img_nrows, img_ncols))\r\n    img = img_to_array(img)\r\n    img = np.expand_dims(img, axis=0)\r\n    img = vgg19.preprocess_input(img)\r\n    return img\r\n\r\n\r\ndef deprocess_image(x):\r\n    if K.image_data_format() == 'channels_first':\r\n        x = x.reshape((3, img_nrows, img_ncols))\r\n        x = x.transpose((1, 2, 0))\r\n    else:\r\n        x = x.reshape((img_nrows, img_ncols, 3))\r\n    # Remove zero-center by mean pixel\r\n    x[:, :, 0] += 103.939\r\n    x[:, :, 1] += 116.779\r\n    x[:, :, 2] += 123.68\r\n    # 'BGR'->'RGB'\r\n    x = x[:, :, ::-1]\r\n    x = np.clip(x, 0, 255).astype('uint8')\r\n    return x\r\n\r\n\r\ndef kmeans(xs, k):\r\n    assert xs.ndim == 2\r\n    try:\r\n        from sklearn.cluster import k_means\r\n        _, labels, _ = k_means(xs.astype('float64'), k)\r\n    except ImportError:\r\n        from scipy.cluster.vq import kmeans2\r\n        _, labels = kmeans2(xs, k, missing='raise')\r\n    return labels\r\n\r\n\r\ndef load_mask_labels():\r\n    '''Load both target and style masks.\r\n    A mask image (nr x nc) with m labels/colors will be loaded\r\n    as a 4D boolean tensor: (1, m, nr, nc) for 'channels_first' or (1, nr, nc, m) for 'channels_last'\r\n    '''\r\n    target_mask_img = load_img(target_mask_path,\r\n                               target_size=(img_nrows, img_ncols))\r\n    target_mask_img = img_to_array(target_mask_img)\r\n    style_mask_img = load_img(style_mask_path,\r\n                              target_size=(img_nrows, img_ncols))\r\n    style_mask_img = img_to_array(style_mask_img)\r\n    if K.image_data_format() == 'channels_first':\r\n        mask_vecs = np.vstack([style_mask_img.reshape((3, -1)).T,\r\n                               target_mask_img.reshape((3, -1)).T])\r\n    else:\r\n        mask_vecs = np.vstack([style_mask_img.reshape((-1, 3)),\r\n                               target_mask_img.reshape((-1, 3))])\r\n\r\n    labels = kmeans(mask_vecs, num_labels)\r\n    style_mask_label = labels[:img_nrows *\r\n                              img_ncols].reshape((img_nrows, img_ncols))\r\n    target_mask_label = labels[img_nrows *\r\n                               img_ncols:].reshape((img_nrows, img_ncols))\r\n\r\n    stack_axis = 0 if K.image_data_format() == 'channels_first' else -1\r\n    style_mask = np.stack([style_mask_label == r for r in xrange(num_labels)],\r\n                          axis=stack_axis)\r\n    target_mask = np.stack([target_mask_label == r for r in xrange(num_labels)],\r\n                           axis=stack_axis)\r\n\r\n    return (np.expand_dims(style_mask, axis=0),\r\n            np.expand_dims(target_mask, axis=0))\r\n\r\n# Create tensor variables for images\r\nif K.image_data_format() == 'channels_first':\r\n    shape = (1, num_colors, img_nrows, img_ncols)\r\nelse:\r\n    shape = (1, img_nrows, img_ncols, num_colors)\r\n\r\nstyle_image = K.variable(preprocess_image(style_img_path))\r\ntarget_image = K.placeholder(shape=shape)\r\nif use_content_img:\r\n    content_image = K.variable(preprocess_image(content_img_path))\r\nelse:\r\n    content_image = K.zeros(shape=shape)\r\n\r\nimages = K.concatenate([style_image, target_image, content_image], axis=0)\r\n\r\n# Create tensor variables for masks\r\nraw_style_mask, raw_target_mask = load_mask_labels()\r\nstyle_mask = K.variable(raw_style_mask.astype('float32'))\r\ntarget_mask = K.variable(raw_target_mask.astype('float32'))\r\nmasks = K.concatenate([style_mask, target_mask], axis=0)\r\n\r\n# index constants for images and tasks variables\r\nSTYLE, TARGET, CONTENT = 0, 1, 2\r\n\r\n# Build image model, mask model and use layer outputs as features\r\n# image model as VGG19\r\nimage_model = vgg19.VGG19(include_top=False, input_tensor=images)\r\n\r\n# mask model as a series of pooling\r\nmask_input = Input(tensor=masks, shape=(None, None, None), name='mask_input')\r\nx = mask_input\r\nfor layer in image_model.layers[1:]:\r\n    name = 'mask_%s' % layer.name\r\n    if 'conv' in layer.name:\r\n        x = AveragePooling2D((3, 3), strides=(\r\n            1, 1), name=name, border_mode='same')(x)\r\n    elif 'pool' in layer.name:\r\n        x = AveragePooling2D((2, 2), name=name)(x)\r\nmask_model = Model(mask_input, x)\r\n\r\n# Collect features from image_model and task_model\r\nimage_features = {}\r\nmask_features = {}\r\nfor img_layer, mask_layer in zip(image_model.layers, mask_model.layers):\r\n    if 'conv' in img_layer.name:\r\n        assert 'mask_' + img_layer.name == mask_layer.name\r\n        layer_name = img_layer.name\r\n        img_feat, mask_feat = img_layer.output, mask_layer.output\r\n        image_features[layer_name] = img_feat\r\n        mask_features[layer_name] = mask_feat\r\n\r\n\r\n# Define loss functions\r\ndef gram_matrix(x):\r\n    assert K.ndim(x) == 3\r\n    features = K.batch_flatten(x)\r\n    gram = K.dot(features, K.transpose(features))\r\n    return gram\r\n\r\n\r\ndef region_style_loss(style_image, target_image, style_mask, target_mask):\r\n    '''Calculate style loss between style_image and target_image,\r\n    for one common region specified by their (boolean) masks\r\n    '''\r\n    assert 3 == K.ndim(style_image) == K.ndim(target_image)\r\n    assert 2 == K.ndim(style_mask) == K.ndim(target_mask)\r\n    if K.image_data_format() == 'channels_first':\r\n        masked_style = style_image * style_mask\r\n        masked_target = target_image * target_mask\r\n        num_channels = K.shape(style_image)[0]\r\n    else:\r\n        masked_style = K.permute_dimensions(\r\n            style_image, (2, 0, 1)) * style_mask\r\n        masked_target = K.permute_dimensions(\r\n            target_image, (2, 0, 1)) * target_mask\r\n        num_channels = K.shape(style_image)[-1]\r\n    s = gram_matrix(masked_style) / K.mean(style_mask) / num_channels\r\n    c = gram_matrix(masked_target) / K.mean(target_mask) / num_channels\r\n    return K.mean(K.square(s - c))\r\n\r\n\r\ndef style_loss(style_image, target_image, style_masks, target_masks):\r\n    '''Calculate style loss between style_image and target_image,\r\n    in all regions.\r\n    '''\r\n    assert 3 == K.ndim(style_image) == K.ndim(target_image)\r\n    assert 3 == K.ndim(style_masks) == K.ndim(target_masks)\r\n    loss = K.variable(0)\r\n    for i in xrange(num_labels):\r\n        if K.image_data_format() == 'channels_first':\r\n            style_mask = style_masks[i, :, :]\r\n            target_mask = target_masks[i, :, :]\r\n        else:\r\n            style_mask = style_masks[:, :, i]\r\n            target_mask = target_masks[:, :, i]\r\n        loss += region_style_loss(style_image,\r\n                                  target_image, style_mask, target_mask)\r\n    return loss\r\n\r\n\r\ndef content_loss(content_image, target_image):\r\n    return K.sum(K.square(target_image - content_image))\r\n\r\n\r\ndef total_variation_loss(x):\r\n    assert 4 == K.ndim(x)\r\n    if K.image_data_format() == 'channels_first':\r\n        a = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] -\r\n                     x[:, :, 1:, :img_ncols - 1])\r\n        b = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] -\r\n                     x[:, :, :img_nrows - 1, 1:])\r\n    else:\r\n        a = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] -\r\n                     x[:, 1:, :img_ncols - 1, :])\r\n        b = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] -\r\n                     x[:, :img_nrows - 1, 1:, :])\r\n    return K.sum(K.pow(a + b, 1.25))\r\n\r\n# Overall loss is the weighted sum of content_loss, style_loss and tv_loss\r\n# Each individual loss uses features from image/mask models.\r\nloss = K.variable(0)\r\nfor layer in content_feature_layers:\r\n    content_feat = image_features[layer][CONTENT, :, :, :]\r\n    target_feat = image_features[layer][TARGET, :, :, :]\r\n    loss += content_weight * content_loss(content_feat, target_feat)\r\n\r\nfor layer in style_feature_layers:\r\n    style_feat = image_features[layer][STYLE, :, :, :]\r\n    target_feat = image_features[layer][TARGET, :, :, :]\r\n    style_masks = mask_features[layer][STYLE, :, :, :]\r\n    target_masks = mask_features[layer][TARGET, :, :, :]\r\n    sl = style_loss(style_feat, target_feat, style_masks, target_masks)\r\n    loss += (style_weight / len(style_feature_layers)) * sl\r\n\r\nloss += total_variation_weight * total_variation_loss(target_image)\r\nloss_grads = K.gradients(loss, target_image)\r\n\r\n# Evaluator class for computing efficiency\r\noutputs = [loss]\r\nif isinstance(loss_grads, (list, tuple)):\r\n    outputs += loss_grads\r\nelse:\r\n    outputs.append(loss_grads)\r\n\r\nf_outputs = K.function([target_image], outputs)\r\n\r\n\r\ndef eval_loss_and_grads(x):\r\n    if K.image_data_format() == 'channels_first':\r\n        x = x.reshape((1, 3, img_nrows, img_ncols))\r\n    else:\r\n        x = x.reshape((1, img_nrows, img_ncols, 3))\r\n    outs = f_outputs([x])\r\n    loss_value = outs[0]\r\n    if len(outs[1:]) == 1:\r\n        grad_values = outs[1].flatten().astype('float64')\r\n    else:\r\n        grad_values = np.array(outs[1:]).flatten().astype('float64')\r\n    return loss_value, grad_values\r\n\r\n\r\nclass Evaluator(object):\r\n\r\n    def __init__(self):\r\n        self.loss_value = None\r\n        self.grads_values = None\r\n\r\n    def loss(self, x):\r\n        assert self.loss_value is None\r\n        loss_value, grad_values = eval_loss_and_grads(x)\r\n        self.loss_value = loss_value\r\n        self.grad_values = grad_values\r\n        return self.loss_value\r\n\r\n    def grads(self, x):\r\n        assert self.loss_value is not None\r\n        grad_values = np.copy(self.grad_values)\r\n        self.loss_value = None\r\n        self.grad_values = None\r\n        return grad_values\r\n\r\nevaluator = Evaluator()\r\n\r\n# Generate images by iterative optimization\r\nif K.image_data_format() == 'channels_first':\r\n    x = np.random.uniform(0, 255, (1, 3, img_nrows, img_ncols)) - 128.\r\nelse:\r\n    x = np.random.uniform(0, 255, (1, img_nrows, img_ncols, 3)) - 128.\r\n\r\nfor i in range(50):\r\n    print('Start of iteration', i)\r\n    start_time = time.time()\r\n    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\r\n                                     fprime=evaluator.grads, maxfun=20)\r\n    print('Current loss value:', min_val)\r\n    # save current generated image\r\n    img = deprocess_image(x.copy())\r\n    fname = target_img_prefix + '_at_iteration_%d.png' % i\r\n    imsave(fname, img)\r\n    end_time = time.time()\r\n    print('Image saved as', fname)\r\n    print('Iteration %d completed in %ds' % (i, end_time - start_time))\r\n"""
Keras/neural_style_transfer.py,0,"b'\'\'\'Neural style transfer with Keras.\n\nRun the script with:\n```\npython neural_style_transfer.py path_to_your_base_image.jpg path_to_your_reference.jpg prefix_for_results\n```\ne.g.:\n```\npython neural_style_transfer.py img/tuebingen.jpg img/starry_night.jpg results/my_result\n```\nOptional parameters:\n```\n--iter, To specify the number of iterations the style transfer takes place (Default is 10)\n--content_weight, The weight given to the content loss (Default is 0.025)\n--style_weight, The weight given to the style loss (Default is 1.0)\n--tv_weight, The weight given to the total variation loss (Default is 1.0)\n```\n\nIt is preferable to run this script on GPU, for speed.\n\nExample result: https://twitter.com/fchollet/status/686631033085677568\n\n# Details\n\nStyle transfer consists in generating an image\nwith the same ""content"" as a base image, but with the\n""style"" of a different picture (typically artistic).\n\nThis is achieved through the optimization of a loss function\nthat has 3 components: ""style loss"", ""content loss"",\nand ""total variation loss"":\n\n- The total variation loss imposes local spatial continuity between\nthe pixels of the combination image, giving it visual coherence.\n\n- The style loss is where the deep learning keeps in --that one is defined\nusing a deep convolutional neural network. Precisely, it consists in a sum of\nL2 distances between the Gram matrices of the representations of\nthe base image and the style reference image, extracted from\ndifferent layers of a convnet (trained on ImageNet). The general idea\nis to capture color/texture information at different spatial\nscales (fairly large scales --defined by the depth of the layer considered).\n\n - The content loss is a L2 distance between the features of the base\nimage (extracted from a deep layer) and the features of the combination image,\nkeeping the generated image close enough to the original one.\n\n# References\n    - [A Neural Algorithm of Artistic Style](http://arxiv.org/abs/1508.06576)\n\'\'\'\n\nfrom __future__ import print_function\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom scipy.misc import imsave\nimport numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\nimport time\nimport argparse\n\nfrom keras.applications import vgg16\nfrom keras import backend as K\n\nparser = argparse.ArgumentParser(description=\'Neural style transfer with Keras.\')\nparser.add_argument(\'base_image_path\', metavar=\'base\', type=str,\n                    help=\'Path to the image to transform.\')\nparser.add_argument(\'style_reference_image_path\', metavar=\'ref\', type=str,\n                    help=\'Path to the style reference image.\')\nparser.add_argument(\'result_prefix\', metavar=\'res_prefix\', type=str,\n                    help=\'Prefix for the saved results.\')\nparser.add_argument(\'--iter\', type=int, default=10, required=False,\n                    help=\'Number of iterations to run.\')\nparser.add_argument(\'--content_weight\', type=float, default=0.025, required=False,\n                    help=\'Content weight.\')\nparser.add_argument(\'--style_weight\', type=float, default=1.0, required=False,\n                    help=\'Style weight.\')\nparser.add_argument(\'--tv_weight\', type=float, default=1.0, required=False,\n                    help=\'Total Variation weight.\')\n\nargs = parser.parse_args()\nbase_image_path = args.base_image_path\nstyle_reference_image_path = args.style_reference_image_path\nresult_prefix = args.result_prefix\niterations = args.iter\n\n# these are the weights of the different loss components\ntotal_variation_weight = args.tv_weight\nstyle_weight = args.style_weight\ncontent_weight = args.content_weight\n\n# dimensions of the generated picture.\nwidth, height = load_img(base_image_path).size\nimg_nrows = 400\nimg_ncols = int(width * img_nrows / height)\n\n# util function to open, resize and format pictures into appropriate tensors\n\n\ndef preprocess_image(image_path):\n    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = vgg16.preprocess_input(img)\n    return img\n\n# util function to convert a tensor into a valid image\n\n\ndef deprocess_image(x):\n    if K.image_data_format() == \'channels_first\':\n        x = x.reshape((3, img_nrows, img_ncols))\n        x = x.transpose((1, 2, 0))\n    else:\n        x = x.reshape((img_nrows, img_ncols, 3))\n    # Remove zero-center by mean pixel\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    # \'BGR\'->\'RGB\'\n    x = x[:, :, ::-1]\n    x = np.clip(x, 0, 255).astype(\'uint8\')\n    return x\n\n# get tensor representations of our images\nbase_image = K.variable(preprocess_image(base_image_path))\nstyle_reference_image = K.variable(preprocess_image(style_reference_image_path))\n\n# this will contain our generated image\nif K.image_data_format() == \'channels_first\':\n    combination_image = K.placeholder((1, 3, img_nrows, img_ncols))\nelse:\n    combination_image = K.placeholder((1, img_nrows, img_ncols, 3))\n\n# combine the 3 images into a single Keras tensor\ninput_tensor = K.concatenate([base_image,\n                              style_reference_image,\n                              combination_image], axis=0)\n\n# build the VGG16 network with our 3 images as input\n# the model will be loaded with pre-trained ImageNet weights\nmodel = vgg16.VGG16(input_tensor=input_tensor,\n                    weights=\'imagenet\', include_top=False)\nprint(\'Model loaded.\')\n\n# get the symbolic outputs of each ""key"" layer (we gave them unique names).\noutputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n\n# compute the neural style loss\n# first we need to define 4 util functions\n\n# the gram matrix of an image tensor (feature-wise outer product)\n\n\ndef gram_matrix(x):\n    assert K.ndim(x) == 3\n    if K.image_data_format() == \'channels_first\':\n        features = K.batch_flatten(x)\n    else:\n        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n    gram = K.dot(features, K.transpose(features))\n    return gram\n\n# the ""style loss"" is designed to maintain\n# the style of the reference image in the generated image.\n# It is based on the gram matrices (which capture style) of\n# feature maps from the style reference image\n# and from the generated image\n\n\ndef style_loss(style, combination):\n    assert K.ndim(style) == 3\n    assert K.ndim(combination) == 3\n    S = gram_matrix(style)\n    C = gram_matrix(combination)\n    channels = 3\n    size = img_nrows * img_ncols\n    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n\n# an auxiliary loss function\n# designed to maintain the ""content"" of the\n# base image in the generated image\n\n\ndef content_loss(base, combination):\n    return K.sum(K.square(combination - base))\n\n# the 3rd loss function, total variation loss,\n# designed to keep the generated image locally coherent\n\n\ndef total_variation_loss(x):\n    assert K.ndim(x) == 4\n    if K.image_data_format() == \'channels_first\':\n        a = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1])\n        b = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:])\n    else:\n        a = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n        b = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n    return K.sum(K.pow(a + b, 1.25))\n\n# combine these loss functions into a single scalar\nloss = K.variable(0.)\nlayer_features = outputs_dict[\'block4_conv2\']\nbase_image_features = layer_features[0, :, :, :]\ncombination_features = layer_features[2, :, :, :]\nloss += content_weight * content_loss(base_image_features,\n                                      combination_features)\n\nfeature_layers = [\'block1_conv1\', \'block2_conv1\',\n                  \'block3_conv1\', \'block4_conv1\',\n                  \'block5_conv1\']\nfor layer_name in feature_layers:\n    layer_features = outputs_dict[layer_name]\n    style_reference_features = layer_features[1, :, :, :]\n    combination_features = layer_features[2, :, :, :]\n    sl = style_loss(style_reference_features, combination_features)\n    loss += (style_weight / len(feature_layers)) * sl\nloss += total_variation_weight * total_variation_loss(combination_image)\n\n# get the gradients of the generated image wrt the loss\ngrads = K.gradients(loss, combination_image)\n\noutputs = [loss]\nif isinstance(grads, (list, tuple)):\n    outputs += grads\nelse:\n    outputs.append(grads)\n\nf_outputs = K.function([combination_image], outputs)\n\n\ndef eval_loss_and_grads(x):\n    if K.image_data_format() == \'channels_first\':\n        x = x.reshape((1, 3, img_nrows, img_ncols))\n    else:\n        x = x.reshape((1, img_nrows, img_ncols, 3))\n    outs = f_outputs([x])\n    loss_value = outs[0]\n    if len(outs[1:]) == 1:\n        grad_values = outs[1].flatten().astype(\'float64\')\n    else:\n        grad_values = np.array(outs[1:]).flatten().astype(\'float64\')\n    return loss_value, grad_values\n\n# this Evaluator class makes it possible\n# to compute loss and gradients in one pass\n# while retrieving them via two separate functions,\n# ""loss"" and ""grads"". This is done because scipy.optimize\n# requires separate functions for loss and gradients,\n# but computing them separately would be inefficient.\n\n\nclass Evaluator(object):\n\n    def __init__(self):\n        self.loss_value = None\n        self.grads_values = None\n\n    def loss(self, x):\n        assert self.loss_value is None\n        loss_value, grad_values = eval_loss_and_grads(x)\n        self.loss_value = loss_value\n        self.grad_values = grad_values\n        return self.loss_value\n\n    def grads(self, x):\n        assert self.loss_value is not None\n        grad_values = np.copy(self.grad_values)\n        self.loss_value = None\n        self.grad_values = None\n        return grad_values\n\nevaluator = Evaluator()\n\n# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n# so as to minimize the neural style loss\nif K.image_data_format() == \'channels_first\':\n    x = np.random.uniform(0, 255, (1, 3, img_nrows, img_ncols)) - 128.\nelse:\n    x = np.random.uniform(0, 255, (1, img_nrows, img_ncols, 3)) - 128.\n\nfor i in range(iterations):\n    print(\'Start of iteration\', i)\n    start_time = time.time()\n    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n                                     fprime=evaluator.grads, maxfun=20)\n    print(\'Current loss value:\', min_val)\n    # save current generated image\n    img = deprocess_image(x.copy())\n    fname = result_prefix + \'_at_iteration_%d.png\' % i\n    imsave(fname, img)\n    end_time = time.time()\n    print(\'Image saved as\', fname)\n    print(\'Iteration %d completed in %ds\' % (i, end_time - start_time))\n'"
Keras/pretrained_word_embeddings.py,0,"b""'''This script loads pre-trained word embeddings (GloVe embeddings)\ninto a frozen Keras Embedding layer, and uses it to\ntrain a text classification model on the 20 Newsgroup dataset\n(classication of newsgroup messages into 20 different categories).\n\nGloVe embedding data can be found at:\nhttp://nlp.stanford.edu/data/glove.6B.zip\n(source page: http://nlp.stanford.edu/projects/glove/)\n\n20 Newsgroup data can be found at:\nhttp://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html\n'''\n\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Input, Flatten\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model\n\n\nBASE_DIR = ''\nGLOVE_DIR = BASE_DIR + '/glove.6B/'\nTEXT_DATA_DIR = BASE_DIR + '/20_newsgroup/'\nMAX_SEQUENCE_LENGTH = 1000\nMAX_NB_WORDS = 20000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.2\n\n# first, build index mapping words in the embeddings set\n# to their embedding vector\n\nprint('Indexing word vectors.')\n\nembeddings_index = {}\nf = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))\n\n# second, prepare text samples and their labels\nprint('Processing text dataset')\n\ntexts = []  # list of text samples\nlabels_index = {}  # dictionary mapping label name to numeric id\nlabels = []  # list of label ids\nfor name in sorted(os.listdir(TEXT_DATA_DIR)):\n    path = os.path.join(TEXT_DATA_DIR, name)\n    if os.path.isdir(path):\n        label_id = len(labels_index)\n        labels_index[name] = label_id\n        for fname in sorted(os.listdir(path)):\n            if fname.isdigit():\n                fpath = os.path.join(path, fname)\n                if sys.version_info < (3,):\n                    f = open(fpath)\n                else:\n                    f = open(fpath, encoding='latin-1')\n                t = f.read()\n                i = t.find('\\n\\n')  # skip header\n                if 0 < i:\n                    t = t[i:]\n                texts.append(t)\n                f.close()\n                labels.append(label_id)\n\nprint('Found %s texts.' % len(texts))\n\n# finally, vectorize the text samples into a 2D integer tensor\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n\nlabels = to_categorical(np.asarray(labels))\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)\n\n# split the data into a training set and a validation set\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nnum_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n\nx_train = data[:-num_validation_samples]\ny_train = labels[:-num_validation_samples]\nx_val = data[-num_validation_samples:]\ny_val = labels[-num_validation_samples:]\n\nprint('Preparing embedding matrix.')\n\n# prepare embedding matrix\nnum_words = min(MAX_NB_WORDS, len(word_index))\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i >= MAX_NB_WORDS:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n\n# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(num_words,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)\n\nprint('Training model.')\n\n# train a 1D convnet with global maxpooling\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nx = Conv1D(128, 5, activation='relu')(embedded_sequences)\nx = MaxPooling1D(5)(x)\nx = Conv1D(128, 5, activation='relu')(x)\nx = MaxPooling1D(5)(x)\nx = Conv1D(128, 5, activation='relu')(x)\nx = MaxPooling1D(35)(x)\nx = Flatten()(x)\nx = Dense(128, activation='relu')(x)\npreds = Dense(len(labels_index), activation='softmax')(x)\n\nmodel = Model(sequence_input, preds)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['acc'])\n\nmodel.fit(x_train, y_train,\n          batch_size=128,\n          epochs=10,\n          validation_data=(x_val, y_val))\n"""
Keras/reuters_mlp.py,0,"b""'''Trains and evaluate a simple MLP\non the Reuters newswire topic classification task.\n'''\nfrom __future__ import print_function\n\nimport numpy as np\nimport keras\nfrom keras.datasets import reuters\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.preprocessing.text import Tokenizer\n\nmax_words = 1000\nbatch_size = 32\nepochs = 5\n\nprint('Loading data...')\n(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n                                                         test_split=0.2)\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')\n\nnum_classes = np.max(y_train) + 1\nprint(num_classes, 'classes')\n\nprint('Vectorizing sequence data...')\ntokenizer = Tokenizer(num_words=max_words)\nx_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\nx_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n\nprint('Convert class vector to binary class matrix '\n      '(for use with categorical_crossentropy)')\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\nprint('y_train shape:', y_train.shape)\nprint('y_test shape:', y_test.shape)\n\nprint('Building model...')\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(max_words,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_split=0.1)\nscore = model.evaluate(x_test, y_test,\n                       batch_size=batch_size, verbose=1)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n"""
Keras/stateful_lstm.py,0,"b'\'\'\'Example script showing how to use stateful RNNs\nto model long sequences efficiently.\n\'\'\'\nfrom __future__ import print_function\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\n\n\n# since we are using stateful rnn tsteps can be set to 1\ntsteps = 1\nbatch_size = 25\nepochs = 25\n# number of elements ahead that are used to make the prediction\nlahead = 1\n\n\ndef gen_cosine_amp(amp=100, period=1000, x0=0, xn=50000, step=1, k=0.0001):\n    """"""Generates an absolute cosine time series with the amplitude\n    exponentially decreasing\n\n    Arguments:\n        amp: amplitude of the cosine function\n        period: period of the cosine function\n        x0: initial x of the time series\n        xn: final x of the time series\n        step: step of the time series discretization\n        k: exponential rate\n    """"""\n    cos = np.zeros(((xn - x0) * step, 1, 1))\n    for i in range(len(cos)):\n        idx = x0 + i * step\n        cos[i, 0, 0] = amp * np.cos(2 * np.pi * idx / period)\n        cos[i, 0, 0] = cos[i, 0, 0] * np.exp(-k * idx)\n    return cos\n\n\nprint(\'Generating Data...\')\ncos = gen_cosine_amp()\nprint(\'Input shape:\', cos.shape)\n\nexpected_output = np.zeros((len(cos), 1))\nfor i in range(len(cos) - lahead):\n    expected_output[i, 0] = np.mean(cos[i + 1:i + lahead + 1])\n\nprint(\'Output shape:\', expected_output.shape)\n\nprint(\'Creating Model...\')\nmodel = Sequential()\nmodel.add(LSTM(50,\n               input_shape=(tsteps, 1),\n               batch_size=batch_size,\n               return_sequences=True,\n               stateful=True))\nmodel.add(LSTM(50,\n               return_sequences=False,\n               stateful=True))\nmodel.add(Dense(1))\nmodel.compile(loss=\'mse\', optimizer=\'rmsprop\')\n\nprint(\'Training\')\nfor i in range(epochs):\n    print(\'Epoch\', i, \'/\', epochs)\n\n    # Note that the last state for sample i in a batch will\n    # be used as initial state for sample i in the next batch.\n    # Thus we are simultaneously training on batch_size series with\n    # lower resolution than the original series contained in cos.\n    # Each of these series are offset by one step and can be\n    # extracted with cos[i::batch_size].\n\n    model.fit(cos, expected_output,\n              batch_size=batch_size,\n              epochs=1,\n              verbose=1,\n              shuffle=False)\n    model.reset_states()\n\nprint(\'Predicting\')\npredicted_output = model.predict(cos, batch_size=batch_size)\n\nprint(\'Plotting Results\')\nplt.subplot(2, 1, 1)\nplt.plot(expected_output)\nplt.title(\'Expected\')\nplt.subplot(2, 1, 2)\nplt.plot(predicted_output)\nplt.title(\'Predicted\')\nplt.show()\n'"
Keras/variational_autoencoder.py,0,"b'\'\'\'This script demonstrates how to build a variational autoencoder with Keras.\n\nReference: ""Auto-Encoding Variational Bayes"" https://arxiv.org/abs/1312.6114\n\'\'\'\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nfrom keras.layers import Input, Dense, Lambda\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras import metrics\nfrom keras.datasets import mnist\n\nbatch_size = 100\noriginal_dim = 784\nlatent_dim = 2\nintermediate_dim = 256\nepochs = 50\nepsilon_std = 1.0\n\nx = Input(batch_shape=(batch_size, original_dim))\nh = Dense(intermediate_dim, activation=\'relu\')(x)\nz_mean = Dense(latent_dim)(h)\nz_log_var = Dense(latent_dim)(h)\n\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n                              stddev=epsilon_std)\n    return z_mean + K.exp(z_log_var / 2) * epsilon\n\n# note that ""output_shape"" isn\'t necessary with the TensorFlow backend\nz = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n\n# we instantiate these layers separately so as to reuse them later\ndecoder_h = Dense(intermediate_dim, activation=\'relu\')\ndecoder_mean = Dense(original_dim, activation=\'sigmoid\')\nh_decoded = decoder_h(z)\nx_decoded_mean = decoder_mean(h_decoded)\n\n\ndef vae_loss(x, x_decoded_mean):\n    xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n    return xent_loss + kl_loss\n\nvae = Model(x, x_decoded_mean)\nvae.compile(optimizer=\'rmsprop\', loss=vae_loss)\n\n# train the VAE on MNIST digits\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nx_train = x_train.astype(\'float32\') / 255.\nx_test = x_test.astype(\'float32\') / 255.\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n\nvae.fit(x_train, x_train,\n        shuffle=True,\n        epochs=epochs,\n        batch_size=batch_size,\n        validation_data=(x_test, x_test))\n\n# build a model to project inputs on the latent space\nencoder = Model(x, z_mean)\n\n# display a 2D plot of the digit classes in the latent space\nx_test_encoded = encoder.predict(x_test, batch_size=batch_size)\nplt.figure(figsize=(6, 6))\nplt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\nplt.colorbar()\nplt.show()\n\n# build a digit generator that can sample from the learned distribution\ndecoder_input = Input(shape=(latent_dim,))\n_h_decoded = decoder_h(decoder_input)\n_x_decoded_mean = decoder_mean(_h_decoded)\ngenerator = Model(decoder_input, _x_decoded_mean)\n\n# display a 2D manifold of the digits\nn = 15  # figure with 15x15 digits\ndigit_size = 28\nfigure = np.zeros((digit_size * n, digit_size * n))\n# linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian\n# to produce values of the latent variables z, since the prior of the latent space is Gaussian\ngrid_x = norm.ppf(np.linspace(0.05, 0.95, n))\ngrid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n\nfor i, yi in enumerate(grid_x):\n    for j, xi in enumerate(grid_y):\n        z_sample = np.array([[xi, yi]])\n        x_decoded = generator.predict(z_sample)\n        digit = x_decoded[0].reshape(digit_size, digit_size)\n        figure[i * digit_size: (i + 1) * digit_size,\n               j * digit_size: (j + 1) * digit_size] = digit\n\nplt.figure(figsize=(10, 10))\nplt.imshow(figure, cmap=\'Greys_r\')\nplt.show()\n'"
Keras/variational_autoencoder_deconv.py,0,"b'\'\'\'This script demonstrates how to build a variational autoencoder\nwith Keras and deconvolution layers.\n\nReference: ""Auto-Encoding Variational Bayes"" https://arxiv.org/abs/1312.6114\n\'\'\'\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nfrom keras.layers import Input, Dense, Lambda, Flatten, Reshape\nfrom keras.layers import Conv2D, Conv2DTranspose\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras import metrics\nfrom keras.datasets import mnist\n\n# input image dimensions\nimg_rows, img_cols, img_chns = 28, 28, 1\n# number of convolutional filters to use\nfilters = 64\n# convolution kernel size\nnum_conv = 3\n\nbatch_size = 100\nif K.image_data_format() == \'channels_first\':\n    original_img_size = (img_chns, img_rows, img_cols)\nelse:\n    original_img_size = (img_rows, img_cols, img_chns)\nlatent_dim = 2\nintermediate_dim = 128\nepsilon_std = 1.0\nepochs = 5\n\nx = Input(batch_shape=(batch_size,) + original_img_size)\nconv_1 = Conv2D(img_chns,\n                kernel_size=(2, 2),\n                padding=\'same\', activation=\'relu\')(x)\nconv_2 = Conv2D(filters,\n                kernel_size=(2, 2),\n                padding=\'same\', activation=\'relu\',\n                strides=(2, 2))(conv_1)\nconv_3 = Conv2D(filters,\n                kernel_size=num_conv,\n                padding=\'same\', activation=\'relu\',\n                strides=1)(conv_2)\nconv_4 = Conv2D(filters,\n                kernel_size=num_conv,\n                padding=\'same\', activation=\'relu\',\n                strides=1)(conv_3)\nflat = Flatten()(conv_4)\nhidden = Dense(intermediate_dim, activation=\'relu\')(flat)\n\nz_mean = Dense(latent_dim)(hidden)\nz_log_var = Dense(latent_dim)(hidden)\n\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n                              mean=0., stddev=epsilon_std)\n    return z_mean + K.exp(z_log_var) * epsilon\n\n# note that ""output_shape"" isn\'t necessary with the TensorFlow backend\n# so you could write `Lambda(sampling)([z_mean, z_log_var])`\nz = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n\n# we instantiate these layers separately so as to reuse them later\ndecoder_hid = Dense(intermediate_dim, activation=\'relu\')\ndecoder_upsample = Dense(filters * 14 * 14, activation=\'relu\')\n\nif K.image_data_format() == \'channels_first\':\n    output_shape = (batch_size, filters, 14, 14)\nelse:\n    output_shape = (batch_size, 14, 14, filters)\n\ndecoder_reshape = Reshape(output_shape[1:])\ndecoder_deconv_1 = Conv2DTranspose(filters,\n                                   kernel_size=num_conv,\n                                   padding=\'same\',\n                                   strides=1,\n                                   activation=\'relu\')\ndecoder_deconv_2 = Conv2DTranspose(filters, num_conv,\n                                   padding=\'same\',\n                                   strides=1,\n                                   activation=\'relu\')\nif K.image_data_format() == \'channels_first\':\n    output_shape = (batch_size, filters, 29, 29)\nelse:\n    output_shape = (batch_size, 29, 29, filters)\ndecoder_deconv_3_upsamp = Conv2DTranspose(filters,\n                                          kernel_size=(3, 3),\n                                          strides=(2, 2),\n                                          padding=\'valid\',\n                                          activation=\'relu\')\ndecoder_mean_squash = Conv2D(img_chns,\n                             kernel_size=2,\n                             padding=\'valid\',\n                             activation=\'sigmoid\')\n\nhid_decoded = decoder_hid(z)\nup_decoded = decoder_upsample(hid_decoded)\nreshape_decoded = decoder_reshape(up_decoded)\ndeconv_1_decoded = decoder_deconv_1(reshape_decoded)\ndeconv_2_decoded = decoder_deconv_2(deconv_1_decoded)\nx_decoded_relu = decoder_deconv_3_upsamp(deconv_2_decoded)\nx_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n\n\ndef vae_loss(x, x_decoded_mean):\n    # NOTE: binary_crossentropy expects a batch_size by dim\n    # for x and x_decoded_mean, so we MUST flatten these!\n    x = K.flatten(x)\n    x_decoded_mean = K.flatten(x_decoded_mean)\n    xent_loss = img_rows * img_cols * metrics.binary_crossentropy(x, x_decoded_mean)\n    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n    return xent_loss + kl_loss\n\nvae = Model(x, x_decoded_mean_squash)\nvae.compile(optimizer=\'rmsprop\', loss=vae_loss)\nvae.summary()\n\n# train the VAE on MNIST digits\n(x_train, _), (x_test, y_test) = mnist.load_data()\n\nx_train = x_train.astype(\'float32\') / 255.\nx_train = x_train.reshape((x_train.shape[0],) + original_img_size)\nx_test = x_test.astype(\'float32\') / 255.\nx_test = x_test.reshape((x_test.shape[0],) + original_img_size)\n\nprint(\'x_train.shape:\', x_train.shape)\n\nvae.fit(x_train, x_train,\n        shuffle=True,\n        epochs=epochs,\n        batch_size=batch_size,\n        validation_data=(x_test, x_test))\n\n# build a model to project inputs on the latent space\nencoder = Model(x, z_mean)\n\n# display a 2D plot of the digit classes in the latent space\nx_test_encoded = encoder.predict(x_test, batch_size=batch_size)\nplt.figure(figsize=(6, 6))\nplt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\nplt.colorbar()\nplt.show()\n\n# build a digit generator that can sample from the learned distribution\ndecoder_input = Input(shape=(latent_dim,))\n_hid_decoded = decoder_hid(decoder_input)\n_up_decoded = decoder_upsample(_hid_decoded)\n_reshape_decoded = decoder_reshape(_up_decoded)\n_deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n_deconv_2_decoded = decoder_deconv_2(_deconv_1_decoded)\n_x_decoded_relu = decoder_deconv_3_upsamp(_deconv_2_decoded)\n_x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\ngenerator = Model(decoder_input, _x_decoded_mean_squash)\n\n# display a 2D manifold of the digits\nn = 15  # figure with 15x15 digits\ndigit_size = 28\nfigure = np.zeros((digit_size * n, digit_size * n))\n# linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian\n# to produce values of the latent variables z, since the prior of the latent space is Gaussian\ngrid_x = norm.ppf(np.linspace(0.05, 0.95, n))\ngrid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n\nfor i, yi in enumerate(grid_x):\n    for j, xi in enumerate(grid_y):\n        z_sample = np.array([[xi, yi]])\n        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n        x_decoded = generator.predict(z_sample, batch_size=batch_size)\n        digit = x_decoded[0].reshape(digit_size, digit_size)\n        figure[i * digit_size: (i + 1) * digit_size,\n               j * digit_size: (j + 1) * digit_size] = digit\n\nplt.figure(figsize=(10, 10))\nplt.imshow(figure, cmap=\'Greys_r\')\nplt.show()\n'"
TensorFlow/input_data.py,0,"b'""""""Functions for downloading and reading MNIST data.""""""\nfrom __future__ import print_function\nimport gzip\nimport os\nimport urllib\nimport numpy\nSOURCE_URL = \'http://yann.lecun.com/exdb/mnist/\'\ndef maybe_download(filename, work_directory):\n  """"""Download the data from Yann\'s website, unless it\'s already here.""""""\n  if not os.path.exists(work_directory):\n    os.mkdir(work_directory)\n  filepath = os.path.join(work_directory, filename)\n  if not os.path.exists(filepath):\n    filepath, _ = urllib.urlretrieve(SOURCE_URL + filename, filepath)\n    statinfo = os.stat(filepath)\n    print(\'Succesfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n  return filepath\ndef _read32(bytestream):\n  dt = numpy.dtype(numpy.uint32).newbyteorder(\'>\')\n  return numpy.frombuffer(bytestream.read(4), dtype=dt)\ndef extract_images(filename):\n  """"""Extract the images into a 4D uint8 numpy array [index, y, x, depth].""""""\n  print(\'Extracting\', filename)\n  with gzip.open(filename) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2051:\n      raise ValueError(\n          \'Invalid magic number %d in MNIST image file: %s\' %\n          (magic, filename))\n    num_images = _read32(bytestream)\n    rows = _read32(bytestream)\n    cols = _read32(bytestream)\n    buf = bytestream.read(rows * cols * num_images)\n    data = numpy.frombuffer(buf, dtype=numpy.uint8)\n    data = data.reshape(num_images, rows, cols, 1)\n    return data\ndef dense_to_one_hot(labels_dense, num_classes=10):\n  """"""Convert class labels from scalars to one-hot vectors.""""""\n  num_labels = labels_dense.shape[0]\n  index_offset = numpy.arange(num_labels) * num_classes\n  labels_one_hot = numpy.zeros((num_labels, num_classes))\n  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n  return labels_one_hot\ndef extract_labels(filename, one_hot=False):\n  """"""Extract the labels into a 1D uint8 numpy array [index].""""""\n  print(\'Extracting\', filename)\n  with gzip.open(filename) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2049:\n      raise ValueError(\n          \'Invalid magic number %d in MNIST label file: %s\' %\n          (magic, filename))\n    num_items = _read32(bytestream)\n    buf = bytestream.read(num_items)\n    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n    if one_hot:\n      return dense_to_one_hot(labels)\n    return labels\nclass DataSet(object):\n  def __init__(self, images, labels, fake_data=False):\n    if fake_data:\n      self._num_examples = 10000\n    else:\n      assert images.shape[0] == labels.shape[0], (\n          ""images.shape: %s labels.shape: %s"" % (images.shape,\n                                                 labels.shape))\n      self._num_examples = images.shape[0]\n      # Convert shape from [num examples, rows, columns, depth]\n      # to [num examples, rows*columns] (assuming depth == 1)\n      assert images.shape[3] == 1\n      images = images.reshape(images.shape[0],\n                              images.shape[1] * images.shape[2])\n      # Convert from [0, 255] -> [0.0, 1.0].\n      images = images.astype(numpy.float32)\n      images = numpy.multiply(images, 1.0 / 255.0)\n    self._images = images\n    self._labels = labels\n    self._epochs_completed = 0\n    self._index_in_epoch = 0\n  @property\n  def images(self):\n    return self._images\n  @property\n  def labels(self):\n    return self._labels\n  @property\n  def num_examples(self):\n    return self._num_examples\n  @property\n  def epochs_completed(self):\n    return self._epochs_completed\n  def next_batch(self, batch_size, fake_data=False):\n    """"""Return the next `batch_size` examples from this data set.""""""\n    if fake_data:\n      fake_image = [1.0 for _ in xrange(784)]\n      fake_label = 0\n      return [fake_image for _ in xrange(batch_size)], [\n          fake_label for _ in xrange(batch_size)]\n    start = self._index_in_epoch\n    self._index_in_epoch += batch_size\n    if self._index_in_epoch > self._num_examples:\n      # Finished epoch\n      self._epochs_completed += 1\n      # Shuffle the data\n      perm = numpy.arange(self._num_examples)\n      numpy.random.shuffle(perm)\n      self._images = self._images[perm]\n      self._labels = self._labels[perm]\n      # Start next epoch\n      start = 0\n      self._index_in_epoch = batch_size\n      assert batch_size <= self._num_examples\n    end = self._index_in_epoch\n    return self._images[start:end], self._labels[start:end]\ndef read_data_sets(train_dir, fake_data=False, one_hot=False):\n  class DataSets(object):\n    pass\n  data_sets = DataSets()\n  if fake_data:\n    data_sets.train = DataSet([], [], fake_data=True)\n    data_sets.validation = DataSet([], [], fake_data=True)\n    data_sets.test = DataSet([], [], fake_data=True)\n    return data_sets\n  TRAIN_IMAGES = \'train-images-idx3-ubyte.gz\'\n  TRAIN_LABELS = \'train-labels-idx1-ubyte.gz\'\n  TEST_IMAGES = \'t10k-images-idx3-ubyte.gz\'\n  TEST_LABELS = \'t10k-labels-idx1-ubyte.gz\'\n  VALIDATION_SIZE = 5000\n  local_file = maybe_download(TRAIN_IMAGES, train_dir)\n  train_images = extract_images(local_file)\n  local_file = maybe_download(TRAIN_LABELS, train_dir)\n  train_labels = extract_labels(local_file, one_hot=one_hot)\n  local_file = maybe_download(TEST_IMAGES, train_dir)\n  test_images = extract_images(local_file)\n  local_file = maybe_download(TEST_LABELS, train_dir)\n  test_labels = extract_labels(local_file, one_hot=one_hot)\n  validation_images = train_images[:VALIDATION_SIZE]\n  validation_labels = train_labels[:VALIDATION_SIZE]\n  train_images = train_images[VALIDATION_SIZE:]\n  train_labels = train_labels[VALIDATION_SIZE:]\n  data_sets.train = DataSet(train_images, train_labels)\n  data_sets.validation = DataSet(validation_images, validation_labels)\n  data_sets.test = DataSet(test_images, test_labels)\n  return data_sets'"
TensorFlow/tensorflow_distributed_mnist_demo.py,33,"b'\'\'\'\n@authou:zhourunlai\n@time:20161122\n1. For ps:\n    python tensorflow_distributed_mnist_demo.py --job_name=""ps"" --task_index=0\n2. For worker:\n    python tensorflow_distributed_mnist_demo.py --job_name=""worker"" --task_index=0\n    python tensorflow_distributed_mnist_demo.py --job_name=""worker"" --task_index=1\n    python tensorflow_distributed_mnist_demo.py --job_name=""worker"" --task_index=2\n    python tensorflow_distributed_mnist_demo.py --job_name=""worker"" --task_index=3\n\'\'\'\nimport math\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n# Flags for defining the tf.train.ClusterSpec\ntf.app.flags.DEFINE_string(""ps_hosts"", ""deadbird-master:22222"",\n                           ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""worker_hosts"", ""badboy-slave:2222,smartgirl-slave:22222,littleboy-slave:22222,fatboy-slave:222222"",\n                           ""Comma-separated list of hostname:port pairs"")\n\n# Flags for defining the tf.train.Server\ntf.app.flags.DEFINE_string(""job_name"", """", ""One of \'ps\', \'worker\'"")\ntf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")\ntf.app.flags.DEFINE_integer(""hidden_units"", 100,\n                            ""Number of units in the hidden layer of the NN"")\ntf.app.flags.DEFINE_string(""data_dir"", ""/tmp/mnist-data"",\n                           ""Directory for storing mnist data"")\ntf.app.flags.DEFINE_integer(""batch_size"", 100, ""Training batch size"")\n\nFLAGS = tf.app.flags.FLAGS\n\nIMAGE_PIXELS = 28\n\ndef main(_):\n  ps_hosts = FLAGS.ps_hosts.split("","")\n  worker_hosts = FLAGS.worker_hosts.split("","")\n\n  # Create a cluster from the parameter server and worker hosts.\n  cluster = tf.train.ClusterSpec({""ps"": ps_hosts, ""worker"": worker_hosts})\n\n  # Create and start a server for the local task.\n  server = tf.train.Server(cluster,\n                           job_name=FLAGS.job_name,\n                           task_index=FLAGS.task_index)\n\n  if FLAGS.job_name == ""ps"":\n    server.join()\n  elif FLAGS.job_name == ""worker"":\n\n    # Assigns ops to the local worker by default.\n    with tf.device(tf.train.replica_device_setter(\n        worker_device=""/job:worker/task:%d"" % FLAGS.task_index,\n        cluster=cluster)):\n\n      # Variables of the hidden layer\n      hid_w = tf.Variable(\n          tf.truncated_normal([IMAGE_PIXELS * IMAGE_PIXELS, FLAGS.hidden_units],\n                              stddev=1.0 / IMAGE_PIXELS), name=""hid_w"")\n      hid_b = tf.Variable(tf.zeros([FLAGS.hidden_units]), name=""hid_b"")\n\n      # Variables of the softmax layer\n      sm_w = tf.Variable(\n          tf.truncated_normal([FLAGS.hidden_units, 10],\n                              stddev=1.0 / math.sqrt(FLAGS.hidden_units)),\n          name=""sm_w"")\n      sm_b = tf.Variable(tf.zeros([10]), name=""sm_b"")\n\n      x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS])\n      y_ = tf.placeholder(tf.float32, [None, 10])\n\n      hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n      hid = tf.nn.relu(hid_lin)\n\n      y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n      loss = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n\n      global_step = tf.Variable(0)\n\n      train_op = tf.train.AdagradOptimizer(0.01).minimize(\n          loss, global_step=global_step)\n\n      saver = tf.train.Saver()\n      summary_op = tf.merge_all_summaries()\n      init_op = tf.initialize_all_variables()\n\n    # Create a ""supervisor"", which oversees the training process.\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                             logdir=""/tmp/train_logs"",\n                             init_op=init_op,\n                             summary_op=summary_op,\n                             saver=saver,\n                             global_step=global_step,\n                             save_model_secs=600)\n\n    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n\n    # The supervisor takes care of session initialization, restoring from\n    # a checkpoint, and closing when done or an error occurs.\n    with sv.managed_session(server.target) as sess:\n      # Loop until the supervisor shuts down or 1000000 steps have completed.\n      step = 0\n      while not sv.should_stop() and step < 1000000:\n        # Run a training step asynchronously.\n        # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n        # perform *synchronous* training.\n\n        batch_xs, batch_ys = mnist.train.next_batch(FLAGS.batch_size)\n        train_feed = {x: batch_xs, y_: batch_ys}\n\n        _, step = sess.run([train_op, global_step], feed_dict=train_feed)\n        if step % 100 == 0:\n            print ""Done step %d"" % step\n\n    # Ask for all the services to stop.\n    sv.stop()\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
Theano/cnn.py,0,"b""import theano\nfrom theano import tensor as T\nfrom theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\nimport numpy as np\nfrom load import mnist\nfrom theano.tensor.nnet.conv import conv2d\nfrom theano.tensor.signal.downsample import max_pool_2d\n\nsrng = RandomStreams()\n\ndef floatX(X):\n    return np.asarray(X, dtype=theano.config.floatX)\n\ndef init_weights(shape):\n    return theano.shared(floatX(np.random.randn(*shape) * 0.01))\n\ndef rectify(X):\n    return T.maximum(X, 0.)\n\ndef softmax(X):\n    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n\ndef dropout(X, p=0.):\n    if p > 0:\n        retain_prob = 1 - p\n        X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n        X /= retain_prob\n    return X\n\ndef RMSprop(cost, params, lr=0.001, rho=0.9, epsilon=1e-6):\n    grads = T.grad(cost=cost, wrt=params)\n    updates = []\n    for p, g in zip(params, grads):\n        acc = theano.shared(p.get_value() * 0.)\n        acc_new = rho * acc + (1 - rho) * g ** 2\n        gradient_scaling = T.sqrt(acc_new + epsilon)\n        g = g / gradient_scaling\n        updates.append((acc, acc_new))\n        updates.append((p, p - lr * g))\n    return updates\n\ndef model(X, w, w2, w3, w4, p_drop_conv, p_drop_hidden):\n    l1a = rectify(conv2d(X, w, border_mode='full'))\n    l1 = max_pool_2d(l1a, (2, 2))\n    l1 = dropout(l1, p_drop_conv)\n\n    l2a = rectify(conv2d(l1, w2))\n    l2 = max_pool_2d(l2a, (2, 2))\n    l2 = dropout(l2, p_drop_conv)\n\n    l3a = rectify(conv2d(l2, w3))\n    l3b = max_pool_2d(l3a, (2, 2))\n    l3 = T.flatten(l3b, outdim=2)\n    l3 = dropout(l3, p_drop_conv)\n\n    l4 = rectify(T.dot(l3, w4))\n    l4 = dropout(l4, p_drop_hidden)\n\n    pyx = softmax(T.dot(l4, w_o))\n    return l1, l2, l3, l4, pyx\n\ntrX, teX, trY, teY = mnist(onehot=True)\n\ntrX = trX.reshape(-1, 1, 28, 28)\nteX = teX.reshape(-1, 1, 28, 28)\n\nX = T.ftensor4()\nY = T.fmatrix()\n\nw = init_weights((32, 1, 3, 3))\nw2 = init_weights((64, 32, 3, 3))\nw3 = init_weights((128, 64, 3, 3))\nw4 = init_weights((128 * 3 * 3, 625))\nw_o = init_weights((625, 10))\n\nnoise_l1, noise_l2, noise_l3, noise_l4, noise_py_x = model(X, w, w2, w3, w4, 0.2, 0.5)\nl1, l2, l3, l4, py_x = model(X, w, w2, w3, w4, 0., 0.)\ny_x = T.argmax(py_x, axis=1)\n\n\ncost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\nparams = [w, w2, w3, w4, w_o]\nupdates = RMSprop(cost, params, lr=0.001)\n\ntrain = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\npredict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n\nfor i in range(100):\n    for start, end in zip(range(0, len(trX), 128), range(128, len(trX), 128)):\n        cost = train(trX[start:end], trY[start:end])\n    print np.mean(np.argmax(teY, axis=1) == predict(teX))\n"""
Theano/linear_regression.py,0,"b'import theano\nfrom theano import tensor as T\nimport numpy as np\n\ntrX = np.linspace(-1, 1, 101)\ntrY = 2 * trX + np.random.randn(*trX.shape) * 0.33\n\nX = T.scalar()\nY = T.scalar()\n\ndef model(X, w):\n    return X * w\n\nw = theano.shared(np.asarray(0., dtype=theano.config.floatX))\ny = model(X, w)\n\ncost = T.mean(T.sqr(y - Y))\ngradient = T.grad(cost=cost, wrt=w)\nupdates = [[w, w - gradient * 0.01]]\n\ntrain = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n\nfor i in range(100):\n    for x, y in zip(trX, trY):\n        train(x, y)\n        \nprint w.get_value() #something around 2\n\n'"
Theano/neural_networks.py,0,"b""import theano\nfrom theano import tensor as T\nfrom theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\nimport numpy as np\nfrom load import mnist\n\nsrng = RandomStreams()\n\ndef floatX(X):\n    return np.asarray(X, dtype=theano.config.floatX)\n\ndef init_weights(shape):\n    return theano.shared(floatX(np.random.randn(*shape) * 0.01))\n\ndef rectify(X):\n    return T.maximum(X, 0.)\n\ndef softmax(X):\n    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n\ndef RMSprop(cost, params, lr=0.001, rho=0.9, epsilon=1e-6):\n    grads = T.grad(cost=cost, wrt=params)\n    updates = []\n    for p, g in zip(params, grads):\n        acc = theano.shared(p.get_value() * 0.)\n        acc_new = rho * acc + (1 - rho) * g ** 2\n        gradient_scaling = T.sqrt(acc_new + epsilon)\n        g = g / gradient_scaling\n        updates.append((acc, acc_new))\n        updates.append((p, p - lr * g))\n    return updates\n\ndef dropout(X, p=0.):\n    if p > 0:\n        retain_prob = 1 - p\n        X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n        X /= retain_prob\n    return X\n\ndef model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n    X = dropout(X, p_drop_input)\n    h = rectify(T.dot(X, w_h))\n\n    h = dropout(h, p_drop_hidden)\n    h2 = rectify(T.dot(h, w_h2))\n\n    h2 = dropout(h2, p_drop_hidden)\n    py_x = softmax(T.dot(h2, w_o))\n    return h, h2, py_x\n\ntrX, teX, trY, teY = mnist(onehot=True)\n\nX = T.fmatrix()\nY = T.fmatrix()\n\nw_h = init_weights((784, 625))\nw_h2 = init_weights((625, 625))\nw_o = init_weights((625, 10))\n\nnoise_h, noise_h2, noise_py_x = model(X, w_h, w_h2, w_o, 0.2, 0.5)\nh, h2, py_x = model(X, w_h, w_h2, w_o, 0., 0.)\ny_x = T.argmax(py_x, axis=1)\n\ncost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\nparams = [w_h, w_h2, w_o]\nupdates = RMSprop(cost, params, lr=0.001)\n\ntrain = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\npredict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n\nfor i in range(100):\n    for start, end in zip(range(0, len(trX), 128), range(128, len(trX), 128)):\n        cost = train(trX[start:end], trY[start:end])\n    print np.mean(np.argmax(teY, axis=1) == predict(teX))\n\n"""
Caffe/finetune_flickr_style/assemble_data.py,0,"b'#!/usr/bin/env python\n""""""\nForm a subset of the Flickr Style data, download images to dirname, and write\nCaffe ImagesDataLayer training file.\n""""""\nimport os\nimport urllib\nimport hashlib\nimport argparse\nimport numpy as np\nimport pandas as pd\nfrom skimage import io\nimport multiprocessing\n\n# Flickr returns a special image if the request is unavailable.\nMISSING_IMAGE_SHA1 = \'6a92790b1c2a301c6e7ddef645dca1f53ea97ac2\'\n\nexample_dirname = os.path.abspath(os.path.dirname(__file__))\ncaffe_dirname = os.path.abspath(os.path.join(example_dirname, \'../..\'))\ntraining_dirname = os.path.join(caffe_dirname, \'data/flickr_style\')\n\n\ndef download_image(args_tuple):\n    ""For use with multiprocessing map. Returns filename on fail.""\n    try:\n        url, filename = args_tuple\n        if not os.path.exists(filename):\n            urllib.urlretrieve(url, filename)\n        with open(filename) as f:\n            assert hashlib.sha1(f.read()).hexdigest() != MISSING_IMAGE_SHA1\n        test_read_image = io.imread(filename)\n        return True\n    except KeyboardInterrupt:\n        raise Exception()  # multiprocessing doesn\'t catch keyboard exceptions\n    except:\n        return False\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n        description=\'Download a subset of Flickr Style to a directory\')\n    parser.add_argument(\n        \'-s\', \'--seed\', type=int, default=0,\n        help=""random seed"")\n    parser.add_argument(\n        \'-i\', \'--images\', type=int, default=-1,\n        help=""number of images to use (-1 for all [default])"",\n    )\n    parser.add_argument(\n        \'-w\', \'--workers\', type=int, default=-1,\n        help=""num workers used to download images. -x uses (all - x) cores [-1 default].""\n    )\n    parser.add_argument(\n        \'-l\', \'--labels\', type=int, default=0,\n        help=""if set to a positive value, only sample images from the first number of labels.""\n    )\n\n    args = parser.parse_args()\n    np.random.seed(args.seed)\n\n    # Read data, shuffle order, and subsample.\n    csv_filename = os.path.join(example_dirname, \'flickr_style.csv.gz\')\n    df = pd.read_csv(csv_filename, index_col=0, compression=\'gzip\')\n    df = df.iloc[np.random.permutation(df.shape[0])]\n    if args.labels > 0:\n        df = df.loc[df[\'label\'] < args.labels]\n    if args.images > 0 and args.images < df.shape[0]:\n        df = df.iloc[:args.images]\n\n    # Make directory for images and get local filenames.\n    if training_dirname is None:\n        training_dirname = os.path.join(caffe_dirname, \'data/flickr_style\')\n    images_dirname = os.path.join(training_dirname, \'images\')\n    if not os.path.exists(images_dirname):\n        os.makedirs(images_dirname)\n    df[\'image_filename\'] = [\n        os.path.join(images_dirname, _.split(\'/\')[-1]) for _ in df[\'image_url\']\n    ]\n\n    # Download images.\n    num_workers = args.workers\n    if num_workers <= 0:\n        num_workers = multiprocessing.cpu_count() + num_workers\n    print(\'Downloading {} images with {} workers...\'.format(\n        df.shape[0], num_workers))\n    pool = multiprocessing.Pool(processes=num_workers)\n    map_args = zip(df[\'image_url\'], df[\'image_filename\'])\n    results = pool.map(download_image, map_args)\n\n    # Only keep rows with valid images, and write out training file lists.\n    df = df[results]\n    for split in [\'train\', \'test\']:\n        split_df = df[df[\'_split\'] == split]\n        filename = os.path.join(training_dirname, \'{}.txt\'.format(split))\n        split_df[[\'image_filename\', \'label\']].to_csv(\n            filename, sep=\' \', header=None, index=None)\n    print(\'Writing train/val for {} successfully downloaded images.\'.format(\n        df.shape[0]))\n'"
Caffe/pycaffe/caffenet.py,0,"b""from __future__ import print_function\nfrom caffe import layers as L, params as P, to_proto\nfrom caffe.proto import caffe_pb2\n\n# helper function for common structures\n\ndef conv_relu(bottom, ks, nout, stride=1, pad=0, group=1):\n    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n                                num_output=nout, pad=pad, group=group)\n    return conv, L.ReLU(conv, in_place=True)\n\ndef fc_relu(bottom, nout):\n    fc = L.InnerProduct(bottom, num_output=nout)\n    return fc, L.ReLU(fc, in_place=True)\n\ndef max_pool(bottom, ks, stride=1):\n    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n\ndef caffenet(lmdb, batch_size=256, include_acc=False):\n    data, label = L.Data(source=lmdb, backend=P.Data.LMDB, batch_size=batch_size, ntop=2,\n        transform_param=dict(crop_size=227, mean_value=[104, 117, 123], mirror=True))\n\n    # the net itself\n    conv1, relu1 = conv_relu(data, 11, 96, stride=4)\n    pool1 = max_pool(relu1, 3, stride=2)\n    norm1 = L.LRN(pool1, local_size=5, alpha=1e-4, beta=0.75)\n    conv2, relu2 = conv_relu(norm1, 5, 256, pad=2, group=2)\n    pool2 = max_pool(relu2, 3, stride=2)\n    norm2 = L.LRN(pool2, local_size=5, alpha=1e-4, beta=0.75)\n    conv3, relu3 = conv_relu(norm2, 3, 384, pad=1)\n    conv4, relu4 = conv_relu(relu3, 3, 384, pad=1, group=2)\n    conv5, relu5 = conv_relu(relu4, 3, 256, pad=1, group=2)\n    pool5 = max_pool(relu5, 3, stride=2)\n    fc6, relu6 = fc_relu(pool5, 4096)\n    drop6 = L.Dropout(relu6, in_place=True)\n    fc7, relu7 = fc_relu(drop6, 4096)\n    drop7 = L.Dropout(relu7, in_place=True)\n    fc8 = L.InnerProduct(drop7, num_output=1000)\n    loss = L.SoftmaxWithLoss(fc8, label)\n\n    if include_acc:\n        acc = L.Accuracy(fc8, label)\n        return to_proto(loss, acc)\n    else:\n        return to_proto(loss)\n\ndef make_net():\n    with open('train.prototxt', 'w') as f:\n        print(caffenet('/path/to/caffe-train-lmdb'), file=f)\n\n    with open('test.prototxt', 'w') as f:\n        print(caffenet('/path/to/caffe-val-lmdb', batch_size=50, include_acc=True), file=f)\n\nif __name__ == '__main__':\n    make_net()\n"""
Caffe/pycaffe/tools.py,0,"b'import numpy as np\n\n\nclass SimpleTransformer:\n\n    """"""\n    SimpleTransformer is a simple class for preprocessing and deprocessing\n    images for caffe.\n    """"""\n\n    def __init__(self, mean=[128, 128, 128]):\n        self.mean = np.array(mean, dtype=np.float32)\n        self.scale = 1.0\n\n    def set_mean(self, mean):\n        """"""\n        Set the mean to subtract for centering the data.\n        """"""\n        self.mean = mean\n\n    def set_scale(self, scale):\n        """"""\n        Set the data scaling.\n        """"""\n        self.scale = scale\n\n    def preprocess(self, im):\n        """"""\n        preprocess() emulate the pre-processing occuring in the vgg16 caffe\n        prototxt.\n        """"""\n\n        im = np.float32(im)\n        im = im[:, :, ::-1]  # change to BGR\n        im -= self.mean\n        im *= self.scale\n        im = im.transpose((2, 0, 1))\n\n        return im\n\n    def deprocess(self, im):\n        """"""\n        inverse of preprocess()\n        """"""\n        im = im.transpose(1, 2, 0)\n        im /= self.scale\n        im += self.mean\n        im = im[:, :, ::-1]  # change to RGB\n\n        return np.uint8(im)\n\n\nclass CaffeSolver:\n\n    """"""\n    Caffesolver is a class for creating a solver.prototxt file. It sets default\n    values and can export a solver parameter file.\n    Note that all parameters are stored as strings. Strings variables are\n    stored as strings in strings.\n    """"""\n\n    def __init__(self, testnet_prototxt_path=""testnet.prototxt"",\n                 trainnet_prototxt_path=""trainnet.prototxt"", debug=False):\n\n        self.sp = {}\n\n        # critical:\n        self.sp[\'base_lr\'] = \'0.001\'\n        self.sp[\'momentum\'] = \'0.9\'\n\n        # speed:\n        self.sp[\'test_iter\'] = \'100\'\n        self.sp[\'test_interval\'] = \'250\'\n\n        # looks:\n        self.sp[\'display\'] = \'25\'\n        self.sp[\'snapshot\'] = \'2500\'\n        self.sp[\'snapshot_prefix\'] = \'""snapshot""\'  # string withing a string!\n\n        # learning rate policy\n        self.sp[\'lr_policy\'] = \'""fixed""\'\n\n        # important, but rare:\n        self.sp[\'gamma\'] = \'0.1\'\n        self.sp[\'weight_decay\'] = \'0.0005\'\n        self.sp[\'train_net\'] = \'""\' + trainnet_prototxt_path + \'""\'\n        self.sp[\'test_net\'] = \'""\' + testnet_prototxt_path + \'""\'\n\n        # pretty much never change these.\n        self.sp[\'max_iter\'] = \'100000\'\n        self.sp[\'test_initialization\'] = \'false\'\n        self.sp[\'average_loss\'] = \'25\'  # this has to do with the display.\n        self.sp[\'iter_size\'] = \'1\'  # this is for accumulating gradients\n\n        if (debug):\n            self.sp[\'max_iter\'] = \'12\'\n            self.sp[\'test_iter\'] = \'1\'\n            self.sp[\'test_interval\'] = \'4\'\n            self.sp[\'display\'] = \'1\'\n\n    def add_from_file(self, filepath):\n        """"""\n        Reads a caffe solver prototxt file and updates the Caffesolver\n        instance parameters.\n        """"""\n        with open(filepath, \'r\') as f:\n            for line in f:\n                if line[0] == \'#\':\n                    continue\n                splitLine = line.split(\':\')\n                self.sp[splitLine[0].strip()] = splitLine[1].strip()\n\n    def write(self, filepath):\n        """"""\n        Export solver parameters to INPUT ""filepath"". Sorted alphabetically.\n        """"""\n        f = open(filepath, \'w\')\n        for key, value in sorted(self.sp.items()):\n            if not(type(value) is str):\n                raise TypeError(\'All solver parameters must be strings\')\n            f.write(\'%s: %s\\n\' % (key, value))\n'"
Caffe/web_demo/app.py,0,"b'import os\nimport time\nimport cPickle\nimport datetime\nimport logging\nimport flask\nimport werkzeug\nimport optparse\nimport tornado.wsgi\nimport tornado.httpserver\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport cStringIO as StringIO\nimport urllib\nimport exifutil\n\nimport caffe\n\nREPO_DIRNAME = os.path.abspath(os.path.dirname(os.path.abspath(__file__)) + \'/../..\')\nUPLOAD_FOLDER = \'/tmp/caffe_demos_uploads\'\nALLOWED_IMAGE_EXTENSIONS = set([\'png\', \'bmp\', \'jpg\', \'jpe\', \'jpeg\', \'gif\'])\n\n# Obtain the flask app object\napp = flask.Flask(__name__)\n\n\n@app.route(\'/\')\ndef index():\n    return flask.render_template(\'index.html\', has_result=False)\n\n\n@app.route(\'/classify_url\', methods=[\'GET\'])\ndef classify_url():\n    imageurl = flask.request.args.get(\'imageurl\', \'\')\n    try:\n        string_buffer = StringIO.StringIO(\n            urllib.urlopen(imageurl).read())\n        image = caffe.io.load_image(string_buffer)\n\n    except Exception as err:\n        # For any exception we encounter in reading the image, we will just\n        # not continue.\n        logging.info(\'URL Image open error: %s\', err)\n        return flask.render_template(\n            \'index.html\', has_result=True,\n            result=(False, \'Cannot open image from URL.\')\n        )\n\n    logging.info(\'Image: %s\', imageurl)\n    result = app.clf.classify_image(image)\n    return flask.render_template(\n        \'index.html\', has_result=True, result=result, imagesrc=imageurl)\n\n\n@app.route(\'/classify_upload\', methods=[\'POST\'])\ndef classify_upload():\n    try:\n        # We will save the file to disk for possible data collection.\n        imagefile = flask.request.files[\'imagefile\']\n        filename_ = str(datetime.datetime.now()).replace(\' \', \'_\') + \\\n            werkzeug.secure_filename(imagefile.filename)\n        filename = os.path.join(UPLOAD_FOLDER, filename_)\n        imagefile.save(filename)\n        logging.info(\'Saving to %s.\', filename)\n        image = exifutil.open_oriented_im(filename)\n\n    except Exception as err:\n        logging.info(\'Uploaded image open error: %s\', err)\n        return flask.render_template(\n            \'index.html\', has_result=True,\n            result=(False, \'Cannot open uploaded image.\')\n        )\n\n    result = app.clf.classify_image(image)\n    return flask.render_template(\n        \'index.html\', has_result=True, result=result,\n        imagesrc=embed_image_html(image)\n    )\n\n\ndef embed_image_html(image):\n    """"""Creates an image embedded in HTML base64 format.""""""\n    image_pil = Image.fromarray((255 * image).astype(\'uint8\'))\n    image_pil = image_pil.resize((256, 256))\n    string_buf = StringIO.StringIO()\n    image_pil.save(string_buf, format=\'png\')\n    data = string_buf.getvalue().encode(\'base64\').replace(\'\\n\', \'\')\n    return \'data:image/png;base64,\' + data\n\n\ndef allowed_file(filename):\n    return (\n        \'.\' in filename and\n        filename.rsplit(\'.\', 1)[1] in ALLOWED_IMAGE_EXTENSIONS\n    )\n\n\nclass ImagenetClassifier(object):\n    default_args = {\n        \'model_def_file\': (\n            \'{}/models/bvlc_reference_caffenet/deploy.prototxt\'.format(REPO_DIRNAME)),\n        \'pretrained_model_file\': (\n            \'{}/models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel\'.format(REPO_DIRNAME)),\n        \'mean_file\': (\n            \'{}/python/caffe/imagenet/ilsvrc_2012_mean.npy\'.format(REPO_DIRNAME)),\n        \'class_labels_file\': (\n            \'{}/data/ilsvrc12/synset_words.txt\'.format(REPO_DIRNAME)),\n        \'bet_file\': (\n            \'{}/data/ilsvrc12/imagenet.bet.pickle\'.format(REPO_DIRNAME)),\n    }\n    for key, val in default_args.iteritems():\n        if not os.path.exists(val):\n            raise Exception(\n                ""File for {} is missing. Should be at: {}"".format(key, val))\n    default_args[\'image_dim\'] = 256\n    default_args[\'raw_scale\'] = 255.\n\n    def __init__(self, model_def_file, pretrained_model_file, mean_file,\n                 raw_scale, class_labels_file, bet_file, image_dim, gpu_mode):\n        logging.info(\'Loading net and associated files...\')\n        if gpu_mode:\n            caffe.set_mode_gpu()\n        else:\n            caffe.set_mode_cpu()\n        self.net = caffe.Classifier(\n            model_def_file, pretrained_model_file,\n            image_dims=(image_dim, image_dim), raw_scale=raw_scale,\n            mean=np.load(mean_file).mean(1).mean(1), channel_swap=(2, 1, 0)\n        )\n\n        with open(class_labels_file) as f:\n            labels_df = pd.DataFrame([\n                {\n                    \'synset_id\': l.strip().split(\' \')[0],\n                    \'name\': \' \'.join(l.strip().split(\' \')[1:]).split(\',\')[0]\n                }\n                for l in f.readlines()\n            ])\n        self.labels = labels_df.sort(\'synset_id\')[\'name\'].values\n\n        self.bet = cPickle.load(open(bet_file))\n        # A bias to prefer children nodes in single-chain paths\n        # I am setting the value to 0.1 as a quick, simple model.\n        # We could use better psychological models here...\n        self.bet[\'infogain\'] -= np.array(self.bet[\'preferences\']) * 0.1\n\n    def classify_image(self, image):\n        try:\n            starttime = time.time()\n            scores = self.net.predict([image], oversample=True).flatten()\n            endtime = time.time()\n\n            indices = (-scores).argsort()[:5]\n            predictions = self.labels[indices]\n\n            # In addition to the prediction text, we will also produce\n            # the length for the progress bar visualization.\n            meta = [\n                (p, \'%.5f\' % scores[i])\n                for i, p in zip(indices, predictions)\n            ]\n            logging.info(\'result: %s\', str(meta))\n\n            # Compute expected information gain\n            expected_infogain = np.dot(\n                self.bet[\'probmat\'], scores[self.bet[\'idmapping\']])\n            expected_infogain *= self.bet[\'infogain\']\n\n            # sort the scores\n            infogain_sort = expected_infogain.argsort()[::-1]\n            bet_result = [(self.bet[\'words\'][v], \'%.5f\' % expected_infogain[v])\n                          for v in infogain_sort[:5]]\n            logging.info(\'bet result: %s\', str(bet_result))\n\n            return (True, meta, bet_result, \'%.3f\' % (endtime - starttime))\n\n        except Exception as err:\n            logging.info(\'Classification error: %s\', err)\n            return (False, \'Something went wrong when classifying the \'\n                           \'image. Maybe try another one?\')\n\n\ndef start_tornado(app, port=5000):\n    http_server = tornado.httpserver.HTTPServer(\n        tornado.wsgi.WSGIContainer(app))\n    http_server.listen(port)\n    print(""Tornado server starting on port {}"".format(port))\n    tornado.ioloop.IOLoop.instance().start()\n\n\ndef start_from_terminal(app):\n    """"""\n    Parse command line options and start the server.\n    """"""\n    parser = optparse.OptionParser()\n    parser.add_option(\n        \'-d\', \'--debug\',\n        help=""enable debug mode"",\n        action=""store_true"", default=False)\n    parser.add_option(\n        \'-p\', \'--port\',\n        help=""which port to serve content on"",\n        type=\'int\', default=5000)\n    parser.add_option(\n        \'-g\', \'--gpu\',\n        help=""use gpu mode"",\n        action=\'store_true\', default=False)\n\n    opts, args = parser.parse_args()\n    ImagenetClassifier.default_args.update({\'gpu_mode\': opts.gpu})\n\n    # Initialize classifier + warm start by forward for allocation\n    app.clf = ImagenetClassifier(**ImagenetClassifier.default_args)\n    app.clf.net.forward()\n\n    if opts.debug:\n        app.run(debug=True, host=\'0.0.0.0\', port=opts.port)\n    else:\n        start_tornado(app, opts.port)\n\n\nif __name__ == \'__main__\':\n    logging.getLogger().setLevel(logging.INFO)\n    if not os.path.exists(UPLOAD_FOLDER):\n        os.makedirs(UPLOAD_FOLDER)\n    start_from_terminal(app)\n'"
Caffe/web_demo/exifutil.py,0,"b'""""""\nThis script handles the skimage exif problem.\n""""""\n\nfrom PIL import Image\nimport numpy as np\n\nORIENTATIONS = {   # used in apply_orientation\n    2: (Image.FLIP_LEFT_RIGHT,),\n    3: (Image.ROTATE_180,),\n    4: (Image.FLIP_TOP_BOTTOM,),\n    5: (Image.FLIP_LEFT_RIGHT, Image.ROTATE_90),\n    6: (Image.ROTATE_270,),\n    7: (Image.FLIP_LEFT_RIGHT, Image.ROTATE_270),\n    8: (Image.ROTATE_90,)\n}\n\n\ndef open_oriented_im(im_path):\n    im = Image.open(im_path)\n    if hasattr(im, \'_getexif\'):\n        exif = im._getexif()\n        if exif is not None and 274 in exif:\n            orientation = exif[274]\n            im = apply_orientation(im, orientation)\n    img = np.asarray(im).astype(np.float32) / 255.\n    if img.ndim == 2:\n        img = img[:, :, np.newaxis]\n        img = np.tile(img, (1, 1, 3))\n    elif img.shape[2] == 4:\n        img = img[:, :, :3]\n    return img\n\n\ndef apply_orientation(im, orientation):\n    if orientation in ORIENTATIONS:\n        for method in ORIENTATIONS[orientation]:\n            im = im.transpose(method)\n    return im\n'"
Caffe/pycaffe/layers/pascal_multilabel_datalayers.py,0,"b'# imports\nimport json\nimport time\nimport pickle\nimport scipy.misc\nimport skimage.io\nimport caffe\n\nimport numpy as np\nimport os.path as osp\n\nfrom xml.dom import minidom\nfrom random import shuffle\nfrom threading import Thread\nfrom PIL import Image\n\nfrom tools import SimpleTransformer\n\n\nclass PascalMultilabelDataLayerSync(caffe.Layer):\n\n    """"""\n    This is a simple synchronous datalayer for training a multilabel model on\n    PASCAL.\n    """"""\n\n    def setup(self, bottom, top):\n\n        self.top_names = [\'data\', \'label\']\n\n        # === Read input parameters ===\n\n        # params is a python dictionary with layer parameters.\n        params = eval(self.param_str)\n\n        # Check the parameters for validity.\n        check_params(params)\n\n        # store input as class variables\n        self.batch_size = params[\'batch_size\']\n\n        # Create a batch loader to load the images.\n        self.batch_loader = BatchLoader(params, None)\n\n        # === reshape tops ===\n        # since we use a fixed input image size, we can shape the data layer\n        # once. Else, we\'d have to do it in the reshape call.\n        top[0].reshape(\n            self.batch_size, 3, params[\'im_shape\'][0], params[\'im_shape\'][1])\n        # Note the 20 channels (because PASCAL has 20 classes.)\n        top[1].reshape(self.batch_size, 20)\n\n        print_info(""PascalMultilabelDataLayerSync"", params)\n\n    def forward(self, bottom, top):\n        """"""\n        Load data.\n        """"""\n        for itt in range(self.batch_size):\n            # Use the batch loader to load the next image.\n            im, multilabel = self.batch_loader.load_next_image()\n\n            # Add directly to the caffe data layer\n            top[0].data[itt, ...] = im\n            top[1].data[itt, ...] = multilabel\n\n    def reshape(self, bottom, top):\n        """"""\n        There is no need to reshape the data, since the input is of fixed size\n        (rows and columns)\n        """"""\n        pass\n\n    def backward(self, top, propagate_down, bottom):\n        """"""\n        These layers does not back propagate\n        """"""\n        pass\n\n\nclass BatchLoader(object):\n\n    """"""\n    This class abstracts away the loading of images.\n    Images can either be loaded singly, or in a batch. The latter is used for\n    the asyncronous data layer to preload batches while other processing is\n    performed.\n    """"""\n\n    def __init__(self, params, result):\n        self.result = result\n        self.batch_size = params[\'batch_size\']\n        self.pascal_root = params[\'pascal_root\']\n        self.im_shape = params[\'im_shape\']\n        # get list of image indexes.\n        list_file = params[\'split\'] + \'.txt\'\n        self.indexlist = [line.rstrip(\'\\n\') for line in open(\n            osp.join(self.pascal_root, \'ImageSets/Main\', list_file))]\n        self._cur = 0  # current image\n        # this class does some simple data-manipulations\n        self.transformer = SimpleTransformer()\n\n        print ""BatchLoader initialized with {} images"".format(\n            len(self.indexlist))\n\n    def load_next_image(self):\n        """"""\n        Load the next image in a batch.\n        """"""\n        # Did we finish an epoch?\n        if self._cur == len(self.indexlist):\n            self._cur = 0\n            shuffle(self.indexlist)\n\n        # Load an image\n        index = self.indexlist[self._cur]  # Get the image index\n        image_file_name = index + \'.jpg\'\n        im = np.asarray(Image.open(\n            osp.join(self.pascal_root, \'JPEGImages\', image_file_name)))\n        im = scipy.misc.imresize(im, self.im_shape)  # resize\n\n        # do a simple horizontal flip as data augmentation\n        flip = np.random.choice(2)*2-1\n        im = im[:, ::flip, :]\n\n        # Load and prepare ground truth\n        multilabel = np.zeros(20).astype(np.float32)\n        anns = load_pascal_annotation(index, self.pascal_root)\n        for label in anns[\'gt_classes\']:\n            # in the multilabel problem we don\'t care how MANY instances\n            # there are of each class. Only if they are present.\n            # The ""-1"" is b/c we are not interested in the background\n            # class.\n            multilabel[label - 1] = 1\n\n        self._cur += 1\n        return self.transformer.preprocess(im), multilabel\n\n\ndef load_pascal_annotation(index, pascal_root):\n    """"""\n    This code is borrowed from Ross Girshick\'s FAST-RCNN code\n    (https://github.com/rbgirshick/fast-rcnn).\n    It parses the PASCAL .xml metadata files.\n    See publication for further details: (http://arxiv.org/abs/1504.08083).\n\n    Thanks Ross!\n\n    """"""\n    classes = (\'__background__\',  # always index 0\n               \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n               \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n                         \'cow\', \'diningtable\', \'dog\', \'horse\',\n                         \'motorbike\', \'person\', \'pottedplant\',\n                         \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n    class_to_ind = dict(zip(classes, xrange(21)))\n\n    filename = osp.join(pascal_root, \'Annotations\', index + \'.xml\')\n    # print \'Loading: {}\'.format(filename)\n\n    def get_data_from_tag(node, tag):\n        return node.getElementsByTagName(tag)[0].childNodes[0].data\n\n    with open(filename) as f:\n        data = minidom.parseString(f.read())\n\n    objs = data.getElementsByTagName(\'object\')\n    num_objs = len(objs)\n\n    boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n    gt_classes = np.zeros((num_objs), dtype=np.int32)\n    overlaps = np.zeros((num_objs, 21), dtype=np.float32)\n\n    # Load object bounding boxes into a data frame.\n    for ix, obj in enumerate(objs):\n        # Make pixel indexes 0-based\n        x1 = float(get_data_from_tag(obj, \'xmin\')) - 1\n        y1 = float(get_data_from_tag(obj, \'ymin\')) - 1\n        x2 = float(get_data_from_tag(obj, \'xmax\')) - 1\n        y2 = float(get_data_from_tag(obj, \'ymax\')) - 1\n        cls = class_to_ind[\n            str(get_data_from_tag(obj, ""name"")).lower().strip()]\n        boxes[ix, :] = [x1, y1, x2, y2]\n        gt_classes[ix] = cls\n        overlaps[ix, cls] = 1.0\n\n    overlaps = scipy.sparse.csr_matrix(overlaps)\n\n    return {\'boxes\': boxes,\n            \'gt_classes\': gt_classes,\n            \'gt_overlaps\': overlaps,\n            \'flipped\': False,\n            \'index\': index}\n\n\ndef check_params(params):\n    """"""\n    A utility function to check the parameters for the data layers.\n    """"""\n    assert \'split\' in params.keys(\n    ), \'Params must include split (train, val, or test).\'\n\n    required = [\'batch_size\', \'pascal_root\', \'im_shape\']\n    for r in required:\n        assert r in params.keys(), \'Params must include {}\'.format(r)\n\n\ndef print_info(name, params):\n    """"""\n    Output some info regarding the class\n    """"""\n    print ""{} initialized for split: {}, with bs: {}, im_shape: {}."".format(\n        name,\n        params[\'split\'],\n        params[\'batch_size\'],\n        params[\'im_shape\'])\n'"
Caffe/pycaffe/layers/pyloss.py,0,"b'import caffe\nimport numpy as np\n\n\nclass EuclideanLossLayer(caffe.Layer):\n    """"""\n    Compute the Euclidean Loss in the same manner as the C++ EuclideanLossLayer\n    to demonstrate the class interface for developing layers in Python.\n    """"""\n\n    def setup(self, bottom, top):\n        # check input pair\n        if len(bottom) != 2:\n            raise Exception(""Need two inputs to compute distance."")\n\n    def reshape(self, bottom, top):\n        # check input dimensions match\n        if bottom[0].count != bottom[1].count:\n            raise Exception(""Inputs must have the same dimension."")\n        # difference is shape of inputs\n        self.diff = np.zeros_like(bottom[0].data, dtype=np.float32)\n        # loss output is scalar\n        top[0].reshape(1)\n\n    def forward(self, bottom, top):\n        self.diff[...] = bottom[0].data - bottom[1].data\n        top[0].data[...] = np.sum(self.diff**2) / bottom[0].num / 2.\n\n    def backward(self, top, propagate_down, bottom):\n        for i in range(2):\n            if not propagate_down[i]:\n                continue\n            if i == 0:\n                sign = 1\n            else:\n                sign = -1\n            bottom[i].diff[...] = sign * self.diff / bottom[i].num\n'"
TensorFlow/examples/1_Introduction/basic_operations.py,12,"b'\'\'\'\nBasic Operations example using TensorFlow library.\n\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n\'\'\'\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n# Basic constant operations\n# The value returned by the constructor represents the output\n# of the Constant op.\na = tf.constant(2)\nb = tf.constant(3)\n\n# Launch the default graph.\nwith tf.Session() as sess:\n    print(""a=2, b=3"")\n    print(""Addition with constants: %i"" % sess.run(a+b))\n    print(""Multiplication with constants: %i"" % sess.run(a*b))\n\n# Basic Operations with variable as graph input\n# The value returned by the constructor represents the output\n# of the Variable op. (define as input when running session)\n# tf Graph input\na = tf.placeholder(tf.int16)\nb = tf.placeholder(tf.int16)\n\n# Define some operations\nadd = tf.add(a, b)\nmul = tf.mul(a, b)\n\n# Launch the default graph.\nwith tf.Session() as sess:\n    # Run every operation with variable input\n    print(""Addition with variables: %i"" % sess.run(add, feed_dict={a: 2, b: 3}))\n    print(""Multiplication with variables: %i"" % sess.run(mul, feed_dict={a: 2, b: 3}))\n\n\n# ----------------\n# More in details:\n# Matrix Multiplication from TensorFlow official tutorial\n\n# Create a Constant op that produces a 1x2 matrix.  The op is\n# added as a node to the default graph.\n#\n# The value returned by the constructor represents the output\n# of the Constant op.\nmatrix1 = tf.constant([[3., 3.]])\n\n# Create another Constant that produces a 2x1 matrix.\nmatrix2 = tf.constant([[2.],[2.]])\n\n# Create a Matmul op that takes \'matrix1\' and \'matrix2\' as inputs.\n# The returned value, \'product\', represents the result of the matrix\n# multiplication.\nproduct = tf.matmul(matrix1, matrix2)\n\n# To run the matmul op we call the session \'run()\' method, passing \'product\'\n# which represents the output of the matmul op.  This indicates to the call\n# that we want to get the output of the matmul op back.\n#\n# All inputs needed by the op are run automatically by the session.  They\n# typically are run in parallel.\n#\n# The call \'run(product)\' thus causes the execution of threes ops in the\n# graph: the two constants and matmul.\n#\n# The output of the op is returned in \'result\' as a numpy `ndarray` object.\nwith tf.Session() as sess:\n    result = sess.run(product)\n    print(result)\n    # ==> [[ 12.]]\n'"
TensorFlow/examples/1_Introduction/helloworld.py,2,"b""'''\nHelloWorld example using TensorFlow library.\n\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n'''\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n# Simple hello world using TensorFlow\n\n# Create a Constant op\n# The op is added as a node to the default graph.\n#\n# The value returned by the constructor represents the output\n# of the Constant op.\nhello = tf.constant('Hello, TensorFlow!')\n\n# Start tf session\nsess = tf.Session()\n\n# Run the op\nprint(sess.run(hello))\n"""
TensorFlow/examples/2_BasicModels/linear_regression.py,10,"b'\'\'\'\nA linear regression learning algorithm example using TensorFlow library.\n\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n\'\'\'\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy\nimport matplotlib.pyplot as plt\nrng = numpy.random\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 1000\ndisplay_step = 50\n\n# Training Data\ntrain_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])\ntrain_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])\nn_samples = train_X.shape[0]\n\n# tf Graph Input\nX = tf.placeholder(""float"")\nY = tf.placeholder(""float"")\n\n# Set model weights\nW = tf.Variable(rng.randn(), name=""weight"")\nb = tf.Variable(rng.randn(), name=""bias"")\n\n# Construct a linear model\npred = tf.add(tf.mul(X, W), b)\n\n# Mean squared error\ncost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\n# Gradient descent\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # Fit all training data\n    for epoch in range(training_epochs):\n        for (x, y) in zip(train_X, train_Y):\n            sess.run(optimizer, feed_dict={X: x, Y: y})\n\n        # Display logs per epoch step\n        if (epoch+1) % display_step == 0:\n            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})\n            print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(c), \\\n                ""W="", sess.run(W), ""b="", sess.run(b))\n\n    print(""Optimization Finished!"")\n    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n    print(""Training cost="", training_cost, ""W="", sess.run(W), ""b="", sess.run(b), \'\\n\')\n\n    # Graphic display\n    plt.plot(train_X, train_Y, \'ro\', label=\'Original data\')\n    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label=\'Fitted line\')\n    plt.legend()\n    plt.show()\n\n    # Testing example, as requested (Issue #2)\n    test_X = numpy.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])\n    test_Y = numpy.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])\n\n    print(""Testing... (Mean square loss Comparison)"")\n    testing_cost = sess.run(\n        tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * test_X.shape[0]),\n        feed_dict={X: test_X, Y: test_Y})  # same function as cost above\n    print(""Testing cost="", testing_cost)\n    print(""Absolute mean square loss difference:"", abs(\n        training_cost - testing_cost))\n\n    plt.plot(test_X, test_Y, \'bo\', label=\'Testing data\')\n    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label=\'Fitted line\')\n    plt.legend()\n    plt.show()\n'"
TensorFlow/examples/2_BasicModels/logistic_regression.py,11,"b'\'\'\'\nA logistic regression learning algorithm example using TensorFlow library.\nThis example is using the MNIST database of handwritten digits\n(http://yann.lecun.com/exdb/mnist/)\n\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n\'\'\'\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 25\nbatch_size = 100\ndisplay_step = 1\n\n# tf Graph Input\nx = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\ny = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n\n# Set model weights\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\n\n# Construct model\npred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n\n# Minimize error using cross entropy\ncost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n# Gradient Descent\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # Training cycle\n    for epoch in range(training_epochs):\n        avg_cost = 0.\n        total_batch = int(mnist.train.num_examples/batch_size)\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n            # Run optimization op (backprop) and cost op (to get loss value)\n            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n                                                          y: batch_ys})\n            # Compute average loss\n            avg_cost += c / total_batch\n        # Display logs per epoch step\n        if (epoch+1) % display_step == 0:\n            print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(avg_cost))\n\n    print(""Optimization Finished!"")\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    # Calculate accuracy\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(""Accuracy:"", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n'"
TensorFlow/examples/2_BasicModels/nearest_neighbor.py,6,"b'\'\'\'\nA nearest neighbor learning algorithm example using TensorFlow library.\nThis example is using the MNIST database of handwritten digits\n(http://yann.lecun.com/exdb/mnist/)\n\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n\'\'\'\n\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\n# In this example, we limit mnist data\nXtr, Ytr = mnist.train.next_batch(5000) #5000 for training (nn candidates)\nXte, Yte = mnist.test.next_batch(200) #200 for testing\n\n# tf Graph Input\nxtr = tf.placeholder(""float"", [None, 784])\nxte = tf.placeholder(""float"", [784])\n\n# Nearest Neighbor calculation using L1 Distance\n# Calculate L1 Distance\ndistance = tf.reduce_sum(tf.abs(tf.add(xtr, tf.negative(xte))), reduction_indices=1)\n# Prediction: Get min distance index (Nearest neighbor)\npred = tf.arg_min(distance, 0)\n\naccuracy = 0.\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # loop over test data\n    for i in range(len(Xte)):\n        # Get nearest neighbor\n        nn_index = sess.run(pred, feed_dict={xtr: Xtr, xte: Xte[i, :]})\n        # Get nearest neighbor class label and compare it to its true label\n        print(""Test"", i, ""Prediction:"", np.argmax(Ytr[nn_index]), \\\n            ""True Class:"", np.argmax(Yte[i]))\n        # Calculate accuracy\n        if np.argmax(Ytr[nn_index]) == np.argmax(Yte[i]):\n            accuracy += 1./len(Xte)\n    print(""Done!"")\n    print(""Accuracy:"", accuracy)\n'"
TensorFlow/examples/3_NeuralNetworks/autoencoder.py,17,"b'# -*- coding: utf-8 -*-\n\n"""""" Auto Encoder Example.\nUsing an auto encoder on MNIST handwritten digits.\nReferences:\n    Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. ""Gradient-based\n    learning applied to document recognition."" Proceedings of the IEEE,\n    86(11):2278-2324, November 1998.\nLinks:\n    [MNIST Dataset] http://yann.lecun.com/exdb/mnist/\n""""""\nfrom __future__ import division, print_function, absolute_import\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""MNIST_data"", one_hot=True)\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 20\nbatch_size = 256\ndisplay_step = 1\nexamples_to_show = 10\n\n# Network Parameters\nn_hidden_1 = 256 # 1st layer num features\nn_hidden_2 = 128 # 2nd layer num features\nn_input = 784 # MNIST data input (img shape: 28*28)\n\n# tf Graph input (only pictures)\nX = tf.placeholder(""float"", [None, n_input])\n\nweights = {\n    \'encoder_h1\': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n    \'encoder_h2\': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n    \'decoder_h1\': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n    \'decoder_h2\': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n}\nbiases = {\n    \'encoder_b1\': tf.Variable(tf.random_normal([n_hidden_1])),\n    \'encoder_b2\': tf.Variable(tf.random_normal([n_hidden_2])),\n    \'decoder_b1\': tf.Variable(tf.random_normal([n_hidden_1])),\n    \'decoder_b2\': tf.Variable(tf.random_normal([n_input])),\n}\n\n\n# Building the encoder\ndef encoder(x):\n    # Encoder Hidden layer with sigmoid activation #1\n    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[\'encoder_h1\']),\n                                   biases[\'encoder_b1\']))\n    # Decoder Hidden layer with sigmoid activation #2\n    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights[\'encoder_h2\']),\n                                   biases[\'encoder_b2\']))\n    return layer_2\n\n\n# Building the decoder\ndef decoder(x):\n    # Encoder Hidden layer with sigmoid activation #1\n    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[\'decoder_h1\']),\n                                   biases[\'decoder_b1\']))\n    # Decoder Hidden layer with sigmoid activation #2\n    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights[\'decoder_h2\']),\n                                   biases[\'decoder_b2\']))\n    return layer_2\n\n# Construct model\nencoder_op = encoder(X)\ndecoder_op = decoder(encoder_op)\n\n# Prediction\ny_pred = decoder_op\n# Targets (Labels) are the input data.\ny_true = X\n\n# Define loss and optimizer, minimize the squared error\ncost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\noptimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n    total_batch = int(mnist.train.num_examples/batch_size)\n    # Training cycle\n    for epoch in range(training_epochs):\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n            # Run optimization op (backprop) and cost op (to get loss value)\n            _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs})\n        # Display logs per epoch step\n        if epoch % display_step == 0:\n            print(""Epoch:"", \'%04d\' % (epoch+1),\n                  ""cost="", ""{:.9f}"".format(c))\n\n    print(""Optimization Finished!"")\n\n    # Applying encode and decode over test set\n    encode_decode = sess.run(\n        y_pred, feed_dict={X: mnist.test.images[:examples_to_show]})\n    # Compare original images with their reconstructions\n    f, a = plt.subplots(2, 10, figsize=(10, 2))\n    for i in range(examples_to_show):\n        a[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28)))\n        a[1][i].imshow(np.reshape(encode_decode[i], (28, 28)))\n    f.show()\n    plt.draw()\n    plt.waitforbuttonpress()\n'"
TensorFlow/examples/3_NeuralNetworks/bidirectional_rnn.py,16,"b'\'\'\'\nA Bidirectional Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\nThis example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\nLong Short Term Memory paper: http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n\'\'\'\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\nimport numpy as np\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\n\'\'\'\nTo classify images using a bidirectional recurrent neural network, we consider\nevery image row as a sequence of pixels. Because MNIST image shape is 28*28px,\nwe will then handle 28 sequences of 28 steps for every sample.\n\'\'\'\n\n# Parameters\nlearning_rate = 0.001\ntraining_iters = 100000\nbatch_size = 128\ndisplay_step = 10\n\n# Network Parameters\nn_input = 28 # MNIST data input (img shape: 28*28)\nn_steps = 28 # timesteps\nn_hidden = 128 # hidden layer num of features\nn_classes = 10 # MNIST total classes (0-9 digits)\n\n# tf Graph input\nx = tf.placeholder(""float"", [None, n_steps, n_input])\ny = tf.placeholder(""float"", [None, n_classes])\n\n# Define weights\nweights = {\n    # Hidden layer weights => 2*n_hidden because of forward + backward cells\n    \'out\': tf.Variable(tf.random_normal([2*n_hidden, n_classes]))\n}\nbiases = {\n    \'out\': tf.Variable(tf.random_normal([n_classes]))\n}\n\n\ndef BiRNN(x, weights, biases):\n\n    # Prepare data shape to match `bidirectional_rnn` function requirements\n    # Current data input shape: (batch_size, n_steps, n_input)\n    # Required shape: \'n_steps\' tensors list of shape (batch_size, n_input)\n\n    # Permuting batch_size and n_steps\n    x = tf.transpose(x, [1, 0, 2])\n    # Reshape to (n_steps*batch_size, n_input)\n    x = tf.reshape(x, [-1, n_input])\n    # Split to get a list of \'n_steps\' tensors of shape (batch_size, n_input)\n    x = tf.split(x, n_steps, 0)\n\n    # Define lstm cells with tensorflow\n    # Forward direction cell\n    lstm_fw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n    # Backward direction cell\n    lstm_bw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n\n    # Get lstm cell output\n    try:\n        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n                                              dtype=tf.float32)\n    except Exception: # Old TensorFlow version only returns outputs not states\n        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n                                        dtype=tf.float32)\n\n    # Linear activation, using rnn inner loop last output\n    return tf.matmul(outputs[-1], weights[\'out\']) + biases[\'out\']\n\npred = BiRNN(x, weights, biases)\n\n# Define loss and optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# Evaluate model\ncorrect_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n    step = 1\n    # Keep training until reach max iterations\n    while step * batch_size < training_iters:\n        batch_x, batch_y = mnist.train.next_batch(batch_size)\n        # Reshape data to get 28 seq of 28 elements\n        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n        # Run optimization op (backprop)\n        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n        if step % display_step == 0:\n            # Calculate batch accuracy\n            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n            # Calculate batch loss\n            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n            print(""Iter "" + str(step*batch_size) + "", Minibatch Loss= "" + \\\n                  ""{:.6f}"".format(loss) + "", Training Accuracy= "" + \\\n                  ""{:.5f}"".format(acc))\n        step += 1\n    print(""Optimization Finished!"")\n\n    # Calculate accuracy for 128 mnist test images\n    test_len = 128\n    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n    test_label = mnist.test.labels[:test_len]\n    print(""Testing Accuracy:"", \\\n        sess.run(accuracy, feed_dict={x: test_data, y: test_label}))\n'"
TensorFlow/examples/3_NeuralNetworks/convolutional_network.py,27,"b'\'\'\'\nA Convolutional Network implementation example using TensorFlow library.\nThis example is using the MNIST database of handwritten digits\n(http://yann.lecun.com/exdb/mnist/)\n\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n\'\'\'\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\n# Parameters\nlearning_rate = 0.001\ntraining_iters = 200000\nbatch_size = 128\ndisplay_step = 10\n\n# Network Parameters\nn_input = 784 # MNIST data input (img shape: 28*28)\nn_classes = 10 # MNIST total classes (0-9 digits)\ndropout = 0.75 # Dropout, probability to keep units\n\n# tf Graph input\nx = tf.placeholder(tf.float32, [None, n_input])\ny = tf.placeholder(tf.float32, [None, n_classes])\nkeep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n\n\n# Create some wrappers for simplicity\ndef conv2d(x, W, b, strides=1):\n    # Conv2D wrapper, with bias and relu activation\n    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=\'SAME\')\n    x = tf.nn.bias_add(x, b)\n    return tf.nn.relu(x)\n\n\ndef maxpool2d(x, k=2):\n    # MaxPool2D wrapper\n    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n                          padding=\'SAME\')\n\n\n# Create model\ndef conv_net(x, weights, biases, dropout):\n    # Reshape input picture\n    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n\n    # Convolution Layer\n    conv1 = conv2d(x, weights[\'wc1\'], biases[\'bc1\'])\n    # Max Pooling (down-sampling)\n    conv1 = maxpool2d(conv1, k=2)\n\n    # Convolution Layer\n    conv2 = conv2d(conv1, weights[\'wc2\'], biases[\'bc2\'])\n    # Max Pooling (down-sampling)\n    conv2 = maxpool2d(conv2, k=2)\n\n    # Fully connected layer\n    # Reshape conv2 output to fit fully connected layer input\n    fc1 = tf.reshape(conv2, [-1, weights[\'wd1\'].get_shape().as_list()[0]])\n    fc1 = tf.add(tf.matmul(fc1, weights[\'wd1\']), biases[\'bd1\'])\n    fc1 = tf.nn.relu(fc1)\n    # Apply Dropout\n    fc1 = tf.nn.dropout(fc1, dropout)\n\n    # Output, class prediction\n    out = tf.add(tf.matmul(fc1, weights[\'out\']), biases[\'out\'])\n    return out\n\n# Store layers weight & bias\nweights = {\n    # 5x5 conv, 1 input, 32 outputs\n    \'wc1\': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n    # 5x5 conv, 32 inputs, 64 outputs\n    \'wc2\': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n    # fully connected, 7*7*64 inputs, 1024 outputs\n    \'wd1\': tf.Variable(tf.random_normal([7*7*64, 1024])),\n    # 1024 inputs, 10 outputs (class prediction)\n    \'out\': tf.Variable(tf.random_normal([1024, n_classes]))\n}\n\nbiases = {\n    \'bc1\': tf.Variable(tf.random_normal([32])),\n    \'bc2\': tf.Variable(tf.random_normal([64])),\n    \'bd1\': tf.Variable(tf.random_normal([1024])),\n    \'out\': tf.Variable(tf.random_normal([n_classes]))\n}\n\n# Construct model\npred = conv_net(x, weights, biases, keep_prob)\n\n# Define loss and optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# Evaluate model\ncorrect_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n    step = 1\n    # Keep training until reach max iterations\n    while step * batch_size < training_iters:\n        batch_x, batch_y = mnist.train.next_batch(batch_size)\n        # Run optimization op (backprop)\n        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n                                       keep_prob: dropout})\n        if step % display_step == 0:\n            # Calculate batch loss and accuracy\n            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n                                                              y: batch_y,\n                                                              keep_prob: 1.})\n            print(""Iter "" + str(step*batch_size) + "", Minibatch Loss= "" + \\\n                  ""{:.6f}"".format(loss) + "", Training Accuracy= "" + \\\n                  ""{:.5f}"".format(acc))\n        step += 1\n    print(""Optimization Finished!"")\n\n    # Calculate accuracy for 256 mnist test images\n    print(""Testing Accuracy:"", \\\n        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n                                      y: mnist.test.labels[:256],\n                                      keep_prob: 1.}))\n'"
TensorFlow/examples/3_NeuralNetworks/dynamic_rnn.py,22,"b'\'\'\'\nA Dynamic Recurrent Neural Network (LSTM) implementation example using\nTensorFlow library. This example is using a toy dataset to classify linear\nsequences. The generated sequences have variable length.\n\nLong Short Term Memory paper: http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n\'\'\'\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport random\n\n\n# ====================\n#  TOY DATA GENERATOR\n# ====================\nclass ToySequenceData(object):\n    """""" Generate sequence of data with dynamic length.\n    This class generate samples for training:\n    - Class 0: linear sequences (i.e. [0, 1, 2, 3,...])\n    - Class 1: random sequences (i.e. [1, 3, 10, 7,...])\n\n    NOTICE:\n    We have to pad each sequence to reach \'max_seq_len\' for TensorFlow\n    consistency (we cannot feed a numpy array with inconsistent\n    dimensions). The dynamic calculation will then be perform thanks to\n    \'seqlen\' attribute that records every actual sequence length.\n    """"""\n    def __init__(self, n_samples=1000, max_seq_len=20, min_seq_len=3,\n                 max_value=1000):\n        self.data = []\n        self.labels = []\n        self.seqlen = []\n        for i in range(n_samples):\n            # Random sequence length\n            len = random.randint(min_seq_len, max_seq_len)\n            # Monitor sequence length for TensorFlow dynamic calculation\n            self.seqlen.append(len)\n            # Add a random or linear int sequence (50% prob)\n            if random.random() < .5:\n                # Generate a linear sequence\n                rand_start = random.randint(0, max_value - len)\n                s = [[float(i)/max_value] for i in\n                     range(rand_start, rand_start + len)]\n                # Pad sequence for dimension consistency\n                s += [[0.] for i in range(max_seq_len - len)]\n                self.data.append(s)\n                self.labels.append([1., 0.])\n            else:\n                # Generate a random sequence\n                s = [[float(random.randint(0, max_value))/max_value]\n                     for i in range(len)]\n                # Pad sequence for dimension consistency\n                s += [[0.] for i in range(max_seq_len - len)]\n                self.data.append(s)\n                self.labels.append([0., 1.])\n        self.batch_id = 0\n\n    def next(self, batch_size):\n        """""" Return a batch of data. When dataset end is reached, start over.\n        """"""\n        if self.batch_id == len(self.data):\n            self.batch_id = 0\n        batch_data = (self.data[self.batch_id:min(self.batch_id +\n                                                  batch_size, len(self.data))])\n        batch_labels = (self.labels[self.batch_id:min(self.batch_id +\n                                                  batch_size, len(self.data))])\n        batch_seqlen = (self.seqlen[self.batch_id:min(self.batch_id +\n                                                  batch_size, len(self.data))])\n        self.batch_id = min(self.batch_id + batch_size, len(self.data))\n        return batch_data, batch_labels, batch_seqlen\n\n\n# ==========\n#   MODEL\n# ==========\n\n# Parameters\nlearning_rate = 0.01\ntraining_iters = 1000000\nbatch_size = 128\ndisplay_step = 10\n\n# Network Parameters\nseq_max_len = 20 # Sequence max length\nn_hidden = 64 # hidden layer num of features\nn_classes = 2 # linear sequence or not\n\ntrainset = ToySequenceData(n_samples=1000, max_seq_len=seq_max_len)\ntestset = ToySequenceData(n_samples=500, max_seq_len=seq_max_len)\n\n# tf Graph input\nx = tf.placeholder(""float"", [None, seq_max_len, 1])\ny = tf.placeholder(""float"", [None, n_classes])\n# A placeholder for indicating each sequence length\nseqlen = tf.placeholder(tf.int32, [None])\n\n# Define weights\nweights = {\n    \'out\': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n}\nbiases = {\n    \'out\': tf.Variable(tf.random_normal([n_classes]))\n}\n\n\ndef dynamicRNN(x, seqlen, weights, biases):\n\n    # Prepare data shape to match `rnn` function requirements\n    # Current data input shape: (batch_size, n_steps, n_input)\n    # Required shape: \'n_steps\' tensors list of shape (batch_size, n_input)\n\n    # Permuting batch_size and n_steps\n    x = tf.transpose(x, [1, 0, 2])\n    # Reshaping to (n_steps*batch_size, n_input)\n    x = tf.reshape(x, [-1, 1])\n    # Split to get a list of \'n_steps\' tensors of shape (batch_size, n_input)\n    x = tf.split(0, seq_max_len, x)\n\n    # Define a lstm cell with tensorflow\n    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n\n    # Get lstm cell output, providing \'sequence_length\' will perform dynamic\n    # calculation.\n    outputs, states = tf.nn.rnn(lstm_cell, x, dtype=tf.float32,\n                                sequence_length=seqlen)\n\n    # When performing dynamic calculation, we must retrieve the last\n    # dynamically computed output, i.e., if a sequence length is 10, we need\n    # to retrieve the 10th output.\n    # However TensorFlow doesn\'t support advanced indexing yet, so we build\n    # a custom op that for each sample in batch size, get its length and\n    # get the corresponding relevant output.\n\n    # \'outputs\' is a list of output at every timestep, we pack them in a Tensor\n    # and change back dimension to [batch_size, n_step, n_input]\n    outputs = tf.pack(outputs)\n    outputs = tf.transpose(outputs, [1, 0, 2])\n\n    # Hack to build the indexing and retrieve the right output.\n    batch_size = tf.shape(outputs)[0]\n    # Start indices for each sample\n    index = tf.range(0, batch_size) * seq_max_len + (seqlen - 1)\n    # Indexing\n    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n\n    # Linear activation, using outputs computed above\n    return tf.matmul(outputs, weights[\'out\']) + biases[\'out\']\n\npred = dynamicRNN(x, seqlen, weights, biases)\n\n# Define loss and optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# Evaluate model\ncorrect_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n    step = 1\n    # Keep training until reach max iterations\n    while step * batch_size < training_iters:\n        batch_x, batch_y, batch_seqlen = trainset.next(batch_size)\n        # Run optimization op (backprop)\n        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n                                       seqlen: batch_seqlen})\n        if step % display_step == 0:\n            # Calculate batch accuracy\n            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y,\n                                                seqlen: batch_seqlen})\n            # Calculate batch loss\n            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y,\n                                             seqlen: batch_seqlen})\n            print(""Iter "" + str(step*batch_size) + "", Minibatch Loss= "" + \\\n                  ""{:.6f}"".format(loss) + "", Training Accuracy= "" + \\\n                  ""{:.5f}"".format(acc))\n        step += 1\n    print(""Optimization Finished!"")\n\n    # Calculate accuracy\n    test_data = testset.data\n    test_label = testset.labels\n    test_seqlen = testset.seqlen\n    print(""Testing Accuracy:"", \\\n        sess.run(accuracy, feed_dict={x: test_data, y: test_label,\n                                      seqlen: test_seqlen}))\n'"
TensorFlow/examples/3_NeuralNetworks/multilayer_perceptron.py,19,"b'\'\'\'\nA Multilayer Perceptron implementation example using TensorFlow library.\nThis example is using the MNIST database of handwritten digits\n(http://yann.lecun.com/exdb/mnist/)\n\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n\'\'\'\n\nfrom __future__ import print_function\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\nimport tensorflow as tf\n\n# Parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\ndisplay_step = 1\n\n# Network Parameters\nn_hidden_1 = 256 # 1st layer number of features\nn_hidden_2 = 256 # 2nd layer number of features\nn_input = 784 # MNIST data input (img shape: 28*28)\nn_classes = 10 # MNIST total classes (0-9 digits)\n\n# tf Graph input\nx = tf.placeholder(""float"", [None, n_input])\ny = tf.placeholder(""float"", [None, n_classes])\n\n\n# Create model\ndef multilayer_perceptron(x, weights, biases):\n    # Hidden layer with RELU activation\n    layer_1 = tf.add(tf.matmul(x, weights[\'h1\']), biases[\'b1\'])\n    layer_1 = tf.nn.relu(layer_1)\n    # Hidden layer with RELU activation\n    layer_2 = tf.add(tf.matmul(layer_1, weights[\'h2\']), biases[\'b2\'])\n    layer_2 = tf.nn.relu(layer_2)\n    # Output layer with linear activation\n    out_layer = tf.matmul(layer_2, weights[\'out\']) + biases[\'out\']\n    return out_layer\n\n# Store layers weight & bias\nweights = {\n    \'h1\': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n    \'h2\': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n    \'out\': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n}\nbiases = {\n    \'b1\': tf.Variable(tf.random_normal([n_hidden_1])),\n    \'b2\': tf.Variable(tf.random_normal([n_hidden_2])),\n    \'out\': tf.Variable(tf.random_normal([n_classes]))\n}\n\n# Construct model\npred = multilayer_perceptron(x, weights, biases)\n\n# Define loss and optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # Training cycle\n    for epoch in range(training_epochs):\n        avg_cost = 0.\n        total_batch = int(mnist.train.num_examples/batch_size)\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            # Run optimization op (backprop) and cost op (to get loss value)\n            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n                                                          y: batch_y})\n            # Compute average loss\n            avg_cost += c / total_batch\n        # Display logs per epoch step\n        if epoch % display_step == 0:\n            print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", \\\n                ""{:.9f}"".format(avg_cost))\n    print(""Optimization Finished!"")\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    # Calculate accuracy\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))\n    print(""Accuracy:"", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n'"
TensorFlow/examples/3_NeuralNetworks/recurrent_network.py,15,"b'\'\'\'\nA Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\nThis example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\nLong Short Term Memory paper: http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n\'\'\'\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\n\'\'\'\nTo classify images using a recurrent neural network, we consider every image\nrow as a sequence of pixels. Because MNIST image shape is 28*28px, we will then\nhandle 28 sequences of 28 steps for every sample.\n\'\'\'\n\n# Parameters\nlearning_rate = 0.001\ntraining_iters = 100000\nbatch_size = 128\ndisplay_step = 10\n\n# Network Parameters\nn_input = 28 # MNIST data input (img shape: 28*28)\nn_steps = 28 # timesteps\nn_hidden = 128 # hidden layer num of features\nn_classes = 10 # MNIST total classes (0-9 digits)\n\n# tf Graph input\nx = tf.placeholder(""float"", [None, n_steps, n_input])\ny = tf.placeholder(""float"", [None, n_classes])\n\n# Define weights\nweights = {\n    \'out\': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n}\nbiases = {\n    \'out\': tf.Variable(tf.random_normal([n_classes]))\n}\n\n\ndef RNN(x, weights, biases):\n\n    # Prepare data shape to match `rnn` function requirements\n    # Current data input shape: (batch_size, n_steps, n_input)\n    # Required shape: \'n_steps\' tensors list of shape (batch_size, n_input)\n\n    # Permuting batch_size and n_steps\n    x = tf.transpose(x, [1, 0, 2])\n    # Reshaping to (n_steps*batch_size, n_input)\n    x = tf.reshape(x, [-1, n_input])\n    # Split to get a list of \'n_steps\' tensors of shape (batch_size, n_input)\n    x = tf.split(x, n_steps, 0)\n\n    # Define a lstm cell with tensorflow\n    lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n\n    # Get lstm cell output\n    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n\n    # Linear activation, using rnn inner loop last output\n    return tf.matmul(outputs[-1], weights[\'out\']) + biases[\'out\']\n\npred = RNN(x, weights, biases)\n\n# Define loss and optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# Evaluate model\ncorrect_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n    step = 1\n    # Keep training until reach max iterations\n    while step * batch_size < training_iters:\n        batch_x, batch_y = mnist.train.next_batch(batch_size)\n        # Reshape data to get 28 seq of 28 elements\n        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n        # Run optimization op (backprop)\n        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n        if step % display_step == 0:\n            # Calculate batch accuracy\n            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n            # Calculate batch loss\n            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n            print(""Iter "" + str(step*batch_size) + "", Minibatch Loss= "" + \\\n                  ""{:.6f}"".format(loss) + "", Training Accuracy= "" + \\\n                  ""{:.5f}"".format(acc))\n        step += 1\n    print(""Optimization Finished!"")\n\n    # Calculate accuracy for 128 mnist test images\n    test_len = 128\n    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n    test_label = mnist.test.labels[:test_len]\n    print(""Testing Accuracy:"", \\\n        sess.run(accuracy, feed_dict={x: test_data, y: test_label}))\n'"
TensorFlow/examples/4_Utils/save_restore_model.py,23,"b'\'\'\'\nSave and Restore a model using TensorFlow.\nThis example is using the MNIST database of handwritten digits\n(http://yann.lecun.com/exdb/mnist/)\n\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n\'\'\'\n\nfrom __future__ import print_function\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n\nimport tensorflow as tf\n\n# Parameters\nlearning_rate = 0.001\nbatch_size = 100\ndisplay_step = 1\nmodel_path = ""/tmp/model.ckpt""\n\n# Network Parameters\nn_hidden_1 = 256 # 1st layer number of features\nn_hidden_2 = 256 # 2nd layer number of features\nn_input = 784 # MNIST data input (img shape: 28*28)\nn_classes = 10 # MNIST total classes (0-9 digits)\n\n# tf Graph input\nx = tf.placeholder(""float"", [None, n_input])\ny = tf.placeholder(""float"", [None, n_classes])\n\n\n# Create model\ndef multilayer_perceptron(x, weights, biases):\n    # Hidden layer with RELU activation\n    layer_1 = tf.add(tf.matmul(x, weights[\'h1\']), biases[\'b1\'])\n    layer_1 = tf.nn.relu(layer_1)\n    # Hidden layer with RELU activation\n    layer_2 = tf.add(tf.matmul(layer_1, weights[\'h2\']), biases[\'b2\'])\n    layer_2 = tf.nn.relu(layer_2)\n    # Output layer with linear activation\n    out_layer = tf.matmul(layer_2, weights[\'out\']) + biases[\'out\']\n    return out_layer\n\n# Store layers weight & bias\nweights = {\n    \'h1\': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n    \'h2\': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n    \'out\': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n}\nbiases = {\n    \'b1\': tf.Variable(tf.random_normal([n_hidden_1])),\n    \'b2\': tf.Variable(tf.random_normal([n_hidden_2])),\n    \'out\': tf.Variable(tf.random_normal([n_classes]))\n}\n\n# Construct model\npred = multilayer_perceptron(x, weights, biases)\n\n# Define loss and optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# \'Saver\' op to save and restore all the variables\nsaver = tf.train.Saver()\n\n# Running first session\nprint(""Starting 1st session..."")\nwith tf.Session() as sess:\n    # Initialize variables\n    sess.run(init)\n\n    # Training cycle\n    for epoch in range(3):\n        avg_cost = 0.\n        total_batch = int(mnist.train.num_examples/batch_size)\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            # Run optimization op (backprop) and cost op (to get loss value)\n            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n                                                          y: batch_y})\n            # Compute average loss\n            avg_cost += c / total_batch\n        # Display logs per epoch step\n        if epoch % display_step == 0:\n            print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", \\\n                ""{:.9f}"".format(avg_cost))\n    print(""First Optimization Finished!"")\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    # Calculate accuracy\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))\n    print(""Accuracy:"", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n\n    # Save model weights to disk\n    save_path = saver.save(sess, model_path)\n    print(""Model saved in file: %s"" % save_path)\n\n# Running a new session\nprint(""Starting 2nd session..."")\nwith tf.Session() as sess:\n    # Initialize variables\n    sess.run(init)\n\n    # Restore model weights from previously saved model\n    saver.restore(sess, model_path)\n    print(""Model restored from file: %s"" % save_path)\n\n    # Resume training\n    for epoch in range(7):\n        avg_cost = 0.\n        total_batch = int(mnist.train.num_examples / batch_size)\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            # Run optimization op (backprop) and cost op (to get loss value)\n            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n                                                          y: batch_y})\n            # Compute average loss\n            avg_cost += c / total_batch\n        # Display logs per epoch step\n        if epoch % display_step == 0:\n            print(""Epoch:"", \'%04d\' % (epoch + 1), ""cost="", \\\n                ""{:.9f}"".format(avg_cost))\n    print(""Second Optimization Finished!"")\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    # Calculate accuracy\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))\n    print(""Accuracy:"", accuracy.eval(\n        {x: mnist.test.images, y: mnist.test.labels}))\n'"
TensorFlow/examples/4_Utils/tensorboard_advanced.py,35,"b'\'\'\'\nGraph and Loss visualization using Tensorboard.\nThis example is using the MNIST database of handwritten digits\n(http://yann.lecun.com/exdb/mnist/)\n\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n\'\'\'\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 25\nbatch_size = 100\ndisplay_step = 1\nlogs_path = \'/tmp/tensorflow_logs/example\'\n\n# Network Parameters\nn_hidden_1 = 256 # 1st layer number of features\nn_hidden_2 = 256 # 2nd layer number of features\nn_input = 784 # MNIST data input (img shape: 28*28)\nn_classes = 10 # MNIST total classes (0-9 digits)\n\n# tf Graph Input\n# mnist data image of shape 28*28=784\nx = tf.placeholder(tf.float32, [None, 784], name=\'InputData\')\n# 0-9 digits recognition => 10 classes\ny = tf.placeholder(tf.float32, [None, 10], name=\'LabelData\')\n\n\n# Create model\ndef multilayer_perceptron(x, weights, biases):\n    # Hidden layer with RELU activation\n    layer_1 = tf.add(tf.matmul(x, weights[\'w1\']), biases[\'b1\'])\n    layer_1 = tf.nn.relu(layer_1)\n    # Create a summary to visualize the first layer ReLU activation\n    tf.summary.histogram(""relu1"", layer_1)\n    # Hidden layer with RELU activation\n    layer_2 = tf.add(tf.matmul(layer_1, weights[\'w2\']), biases[\'b2\'])\n    layer_2 = tf.nn.relu(layer_2)\n    # Create another summary to visualize the second layer ReLU activation\n    tf.summary.histogram(""relu2"", layer_2)\n    # Output layer\n    out_layer = tf.add(tf.matmul(layer_2, weights[\'w3\']), biases[\'b3\'])\n    return out_layer\n\n# Store layers weight & bias\nweights = {\n    \'w1\': tf.Variable(tf.random_normal([n_input, n_hidden_1]), name=\'W1\'),\n    \'w2\': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]), name=\'W2\'),\n    \'w3\': tf.Variable(tf.random_normal([n_hidden_2, n_classes]), name=\'W3\')\n}\nbiases = {\n    \'b1\': tf.Variable(tf.random_normal([n_hidden_1]), name=\'b1\'),\n    \'b2\': tf.Variable(tf.random_normal([n_hidden_2]), name=\'b2\'),\n    \'b3\': tf.Variable(tf.random_normal([n_classes]), name=\'b3\')\n}\n\n# Encapsulating all ops into scopes, making Tensorboard\'s Graph\n# Visualization more convenient\nwith tf.name_scope(\'Model\'):\n    # Build model\n    pred = multilayer_perceptron(x, weights, biases)\n\nwith tf.name_scope(\'Loss\'):\n    # Softmax Cross entropy (cost function)\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n\nwith tf.name_scope(\'SGD\'):\n    # Gradient Descent\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    # Op to calculate every variable gradient\n    grads = tf.gradients(loss, tf.trainable_variables())\n    grads = list(zip(grads, tf.trainable_variables()))\n    # Op to update all variables according to their gradient\n    apply_grads = optimizer.apply_gradients(grads_and_vars=grads)\n\nwith tf.name_scope(\'Accuracy\'):\n    # Accuracy\n    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Create a summary to monitor cost tensor\ntf.summary.scalar(""loss"", loss)\n# Create a summary to monitor accuracy tensor\ntf.summary.scalar(""accuracy"", acc)\n# Create summaries to visualize weights\nfor var in tf.trainable_variables():\n    tf.summary.histogram(var.name, var)\n# Summarize all gradients\nfor grad, var in grads:\n    tf.summary.histogram(var.name + \'/gradient\', grad)\n# Merge all summaries into a single op\nmerged_summary_op = tf.summary.merge_all()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # op to write logs to Tensorboard\n    summary_writer = tf.summary.FileWriter(logs_path,\n                                            graph=tf.get_default_graph())\n\n    # Training cycle\n    for epoch in range(training_epochs):\n        avg_cost = 0.\n        total_batch = int(mnist.train.num_examples/batch_size)\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n            # Run optimization op (backprop), cost op (to get loss value)\n            # and summary nodes\n            _, c, summary = sess.run([apply_grads, loss, merged_summary_op],\n                                     feed_dict={x: batch_xs, y: batch_ys})\n            # Write logs at every iteration\n            summary_writer.add_summary(summary, epoch * total_batch + i)\n            # Compute average loss\n            avg_cost += c / total_batch\n        # Display logs per epoch step\n        if (epoch+1) % display_step == 0:\n            print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(avg_cost))\n\n    print(""Optimization Finished!"")\n\n    # Test model\n    # Calculate accuracy\n    print(""Accuracy:"", acc.eval({x: mnist.test.images, y: mnist.test.labels}))\n\n    print(""Run the command line:\\n"" \\\n          ""--> tensorboard --logdir=/tmp/tensorflow_logs "" \\\n          ""\\nThen open http://0.0.0.0:6006/ into your web browser"")\n'"
TensorFlow/examples/4_Utils/tensorboard_basic.py,19,"b'\'\'\'\nGraph and Loss visualization using Tensorboard.\nThis example is using the MNIST database of handwritten digits\n(http://yann.lecun.com/exdb/mnist/)\n\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n\'\'\'\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 25\nbatch_size = 100\ndisplay_step = 1\nlogs_path = \'/tmp/tensorflow_logs/example\'\n\n# tf Graph Input\n# mnist data image of shape 28*28=784\nx = tf.placeholder(tf.float32, [None, 784], name=\'InputData\')\n# 0-9 digits recognition => 10 classes\ny = tf.placeholder(tf.float32, [None, 10], name=\'LabelData\')\n\n# Set model weights\nW = tf.Variable(tf.zeros([784, 10]), name=\'Weights\')\nb = tf.Variable(tf.zeros([10]), name=\'Bias\')\n\n# Construct model and encapsulating all ops into scopes, making\n# Tensorboard\'s Graph visualization more convenient\nwith tf.name_scope(\'Model\'):\n    # Model\n    pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\nwith tf.name_scope(\'Loss\'):\n    # Minimize error using cross entropy\n    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\nwith tf.name_scope(\'SGD\'):\n    # Gradient Descent\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\nwith tf.name_scope(\'Accuracy\'):\n    # Accuracy\n    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Create a summary to monitor cost tensor\ntf.summary.scalar(""loss"", cost)\n# Create a summary to monitor accuracy tensor\ntf.summary.scalar(""accuracy"", acc)\n# Merge all summaries into a single op\nmerged_summary_op = tf.summary.merge_all()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # op to write logs to Tensorboard\n    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n\n    # Training cycle\n    for epoch in range(training_epochs):\n        avg_cost = 0.\n        total_batch = int(mnist.train.num_examples/batch_size)\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n            # Run optimization op (backprop), cost op (to get loss value)\n            # and summary nodes\n            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n                                     feed_dict={x: batch_xs, y: batch_ys})\n            # Write logs at every iteration\n            summary_writer.add_summary(summary, epoch * total_batch + i)\n            # Compute average loss\n            avg_cost += c / total_batch\n        # Display logs per epoch step\n        if (epoch+1) % display_step == 0:\n            print(""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(avg_cost))\n\n    print(""Optimization Finished!"")\n\n    # Test model\n    # Calculate accuracy\n    print(""Accuracy:"", acc.eval({x: mnist.test.images, y: mnist.test.labels}))\n\n    print(""Run the command line:\\n"" \\\n          ""--> tensorboard --logdir=/tmp/tensorflow_logs "" \\\n          ""\\nThen open http://0.0.0.0:6006/ into your web browser"")\n'"
TensorFlow/examples/5_MultiGPU/multigpu_basics.py,14,"b'from __future__ import print_function\n\'\'\'\nBasic Multi GPU computation example using TensorFlow library.\n\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n\'\'\'\n\n\'\'\'\nThis tutorial requires your machine to have 2 GPUs\n""/cpu:0"": The CPU of your machine.\n""/gpu:0"": The first GPU of your machine\n""/gpu:1"": The second GPU of your machine\n\'\'\'\n\n\n\nimport numpy as np\nimport tensorflow as tf\nimport datetime\n\n# Processing Units logs\nlog_device_placement = True\n\n# Num of multiplications to perform\nn = 10\n\n\'\'\'\nExample: compute A^n + B^n on 2 GPUs\nResults on 8 cores with 2 GTX-980:\n * Single GPU computation time: 0:00:11.277449\n * Multi GPU computation time: 0:00:07.131701\n\'\'\'\n# Create random large matrix\nA = np.random.rand(10000, 10000).astype(\'float32\')\nB = np.random.rand(10000, 10000).astype(\'float32\')\n\n# Create a graph to store results\nc1 = []\nc2 = []\n\ndef matpow(M, n):\n    if n < 1: #Abstract cases where n < 1\n        return M\n    else:\n        return tf.matmul(M, matpow(M, n-1))\n\n\'\'\'\nSingle GPU computing\n\'\'\'\nwith tf.device(\'/gpu:0\'):\n    a = tf.placeholder(tf.float32, [10000, 10000])\n    b = tf.placeholder(tf.float32, [10000, 10000])\n    # Compute A^n and B^n and store results in c1\n    c1.append(matpow(a, n))\n    c1.append(matpow(b, n))\n\nwith tf.device(\'/cpu:0\'):\n  sum = tf.add_n(c1) #Addition of all elements in c1, i.e. A^n + B^n\n\nt1_1 = datetime.datetime.now()\nwith tf.Session(config=tf.ConfigProto(log_device_placement=log_device_placement)) as sess:\n    # Run the op.\n    sess.run(sum, {a:A, b:B})\nt2_1 = datetime.datetime.now()\n\n\n\'\'\'\nMulti GPU computing\n\'\'\'\n# GPU:0 computes A^n\nwith tf.device(\'/gpu:0\'):\n    # Compute A^n and store result in c2\n    a = tf.placeholder(tf.float32, [10000, 10000])\n    c2.append(matpow(a, n))\n\n# GPU:1 computes B^n\nwith tf.device(\'/gpu:1\'):\n    # Compute B^n and store result in c2\n    b = tf.placeholder(tf.float32, [10000, 10000])\n    c2.append(matpow(b, n))\n\nwith tf.device(\'/cpu:0\'):\n  sum = tf.add_n(c2) #Addition of all elements in c2, i.e. A^n + B^n\n\nt1_2 = datetime.datetime.now()\nwith tf.Session(config=tf.ConfigProto(log_device_placement=log_device_placement)) as sess:\n    # Run the op.\n    sess.run(sum, {a:A, b:B})\nt2_2 = datetime.datetime.now()\n\n\nprint(""Single GPU computation time: "" + str(t2_1-t1_1))\nprint(""Multi GPU computation time: "" + str(t2_2-t1_2))\n'"
