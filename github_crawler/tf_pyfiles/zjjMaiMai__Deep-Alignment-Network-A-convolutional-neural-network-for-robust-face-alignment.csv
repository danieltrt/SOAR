file_path,api_count,code
DAN_V2/DAN_V2.py,12,"b'import dan_model\nimport dan_run_loop\n\nimport os\nimport sys\nimport glob\n\nimport numpy as np\nimport cv2\nimport tensorflow as tf\n\n\n\nclass VGG16Model(dan_model.Model):\n    def __init__(self,num_lmark,data_format=None):\n        \n        img_size=112\n        filter_sizes=[64,128,256,512]\n        num_convs=2\n        kernel_size=3\n\n        super(VGG16Model,self).__init__(\n            num_lmark=num_lmark,\n            img_size=img_size,\n            filter_sizes=filter_sizes,\n            num_convs=num_convs,\n            kernel_size=kernel_size,\n            data_format=data_format\n        )\n\ndef get_filenames(data_dir):\n    listext = [\'*.png\',\'*.jpg\']\n\n    imagelist = []\n    for ext in listext:\n        p = os.path.join(data_dir, ext)\n        imagelist.extend(glob.glob(p))\n\n    ptslist = []\n    for image in imagelist:\n        ptslist.append(os.path.splitext(image)[0] + "".ptv"")\n\n    return imagelist, ptslist\n\n\ndef get_synth_input_fn():\n    return dan_run_loop.get_synth_input_fn(112, 112, 1, 68)\n\ndef vgg16_input_fn(is_training,data_dir,batch_size=64,num_epochs=1,num_parallel_calls=1, multi_gpu=False):\n    img_path,pts_path = get_filenames(data_dir)\n\n    def decode_img_pts(img,pts,is_training):\n        img = cv2.imread(img.decode(), cv2.IMREAD_GRAYSCALE)\n        pts = np.loadtxt(pts.decode(),dtype=np.float32,delimiter=\',\')\n        return img[:,:,np.newaxis].astype(np.float32),pts.astype(np.float32)\n\n    map_func=lambda img,pts,is_training:tuple(tf.py_func(decode_img_pts,[img,pts,is_training],[tf.float32,tf.float32]))\n\n    img = tf.data.Dataset.from_tensor_slices(img_path)\n    pts = tf.data.Dataset.from_tensor_slices(pts_path)\n\n    dataset = tf.data.Dataset.zip((img, pts))\n    num_images = len(img_path)\n\n    return dan_run_loop.process_record_dataset(dataset,is_training,batch_size,\n                                               num_images,map_func,num_epochs,num_parallel_calls,\n                                               examples_per_epoch=num_images, multi_gpu=multi_gpu)\n\ndef read_dataset_info(data_dir):\n    mean_shape = np.loadtxt(os.path.join(data_dir,\'mean_shape.ptv\'),dtype=np.float32,delimiter=\',\')\n    imgs_mean = np.loadtxt(os.path.join(data_dir,\'imgs_mean.ptv\'),dtype=np.float32,delimiter=\',\')\n    imgs_std = np.loadtxt(os.path.join(data_dir,\'imgs_std.ptv\'),dtype=np.float32,delimiter=\',\')\n    return mean_shape.astype(np.float32) ,imgs_mean.astype(np.float32),imgs_std.astype(np.float32)\n\ndef video_input_fn(data_dir,img_size,num_lmark):\n    video = cv2.VideoCapture(data_dir)\n\n    def _get_frame():\n        while True:\n            _,frame = video.read()\n            if len(frame.shape) == 3:\n                frame = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n            frame = cv2.resize(frame,(img_size,img_size)).astype(np.float32)\n            yield (frame,np.zeros([num_lmark,2],np.float32))\n\n    def input_fn():\n        dataset = tf.data.Dataset.from_generator(_get_frame,(tf.float32,tf.float32),(tf.TensorShape([img_size,img_size]),tf.TensorShape([num_lmark,2])))\n        return dataset\n\n    return input_fn\n\n\n\ndef main(argv):\n    parser = dan_run_loop.DANArgParser()\n    parser.set_defaults(data_dir=\'./data_dir\',\n                        model_dir=\'./model_dir\',\n                        data_format=\'channels_last\',\n                        train_epochs=20,\n                        epochs_per_eval=10,\n                        batch_size=64)\n\n    flags = parser.parse_args(args=argv[1:])\n\n    mean_shape = None\n    imgs_mean = None\n    imgs_std = None\n\n    flags_trans = { \n        \'train\':tf.estimator.ModeKeys.TRAIN,\n        \'eval\':tf.estimator.ModeKeys.EVAL,\n        \'predict\':tf.estimator.ModeKeys.PREDICT\n                  }\n\n    flags.mode = flags_trans[flags.mode]\n\n    if flags.mode == tf.estimator.ModeKeys.TRAIN:\n        mean_shape,imgs_mean,imgs_std = read_dataset_info(flags.data_dir)\n\n    def vgg16_model_fn(features, labels, mode, params):\n        return dan_run_loop.dan_model_fn(features=features,\n                            groundtruth=labels,\n                            mode=mode,\n                            stage=params[\'dan_stage\'],                                                    \n                            num_lmark=params[\'num_lmark\'],\n                            model_class=VGG16Model,\n                            mean_shape=mean_shape,\n                            imgs_mean=imgs_mean,\n                            imgs_std=imgs_std,\n                            data_format=params[\'data_format\'],\n                            multi_gpu=params[\'multi_gpu\'])\n\n    input_function = flags.use_synthetic_data and get_synth_input_fn() or vgg16_input_fn\n\n    if flags.mode == tf.estimator.ModeKeys.PREDICT:\n        input_function = video_input_fn(flags.data_dir,112,flags.num_lmark)\n    dan_run_loop.dan_main(flags,vgg16_model_fn,input_function)\n\nif __name__ == \'__main__\':\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run(argv=sys.argv)\n'"
DAN_V2/dan_model.py,64,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\n_BATCH_NORM_DECAY = 0.997\n_BATCH_NORM_EPSILON = 1e-5\nDEFAULT_VERSION = 2\n\ndef batch_norm(inputs,training,data_format):\n  """"""Performs a batch normalization using a standard set of parameters.""""""\n  # We set fused=True for a significant performance boost.  See\n  # https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n  return tf.layers.batch_normalization(inputs=inputs, axis=1 if data_format == \'channels_first\' else -1,\n      momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True,\n      scale=True, training=training, fused=True)\n\ndef vgg_block(inputs,filters,num_convs,training,kernel_size,maxpool,data_format):\n    for i in range(num_convs):\n        inputs = batch_norm(tf.layers.conv2d(inputs,filters,kernel_size,1,\n                                             padding=\'same\',activation=tf.nn.relu,\n                                             kernel_initializer=tf.glorot_uniform_initializer(),\n                                             data_format=data_format),training=training,data_format=data_format)\n    if maxpool:\n        inputs = tf.layers.max_pooling2d(inputs,2,2)\n\n    return inputs\n\n\nclass Model(object):\n\n    def __init__(self,\n                 num_lmark,\n                 img_size,\n                 filter_sizes,\n                 num_convs,\n                 kernel_size,\n                 data_format=None):\n        if not data_format:\n            data_format = (\'channels_first\' if tf.test.is_built_with_cuda() else \'channels_last\')\n\n        self.data_format = data_format\n        self.filter_sizes = filter_sizes\n        self.num_convs = num_convs\n        self.num_lmark = num_lmark\n        self.kernel_size = kernel_size\n        self.img_size = img_size\n\n        self.__pixels__ = tf.constant([(x, y) for y in range(self.img_size) for x in range(self.img_size)],\n                                      dtype=tf.float32,shape=[1,self.img_size,self.img_size,2])\n        #self.__pixels__ = tf.tile(self.__pixels__,[num_lmark,1,1,1])\n\n    def __calc_affine_params(self,from_shape,to_shape):\n        from_shape = tf.reshape(from_shape,[-1,self.num_lmark,2])\n        to_shape = tf.reshape(to_shape,[-1,self.num_lmark,2])\n\n        from_mean = tf.reduce_mean(from_shape, axis=1, keepdims=True)\n        to_mean = tf.reduce_mean(to_shape, axis=1, keepdims=True)\n\n        from_centralized = from_shape - from_mean\n        to_centralized = to_shape - to_mean\n\n        dot_result = tf.reduce_sum(tf.multiply(from_centralized, to_centralized),\n                                  axis=[1, 2])\n        norm_pow_2 = tf.pow(tf.norm(from_centralized, axis=[1, 2]), 2)\n\n        a = dot_result / norm_pow_2\n        b = tf.reduce_sum(tf.multiply(from_centralized[:, :, 0], to_centralized[:, :, 1]) - tf.multiply(from_centralized[:, :, 1], to_centralized[:, :, 0]), 1) / norm_pow_2\n\n        r = tf.reshape(tf.stack([a, b, -b, a], axis=1), [-1, 2, 2])\n        t = to_mean - tf.matmul(from_mean, r)\n        return r,t\n\n    def __affine_image(self,imgs,r,t):\n        # The Tensor [imgs].format is [NHWC]\n        r = tf.matrix_inverse(r)\n        r = tf.matrix_transpose(r)\n\n        rm = tf.reshape(tf.pad(r, [[0, 0], [0, 0], [0, 1]], mode=\'CONSTANT\'), [-1, 6])\n        rm = tf.pad(rm, [[0, 0], [0, 2]], mode=\'CONSTANT\')\n\n        tm = tf.contrib.image.translations_to_projective_transforms(tf.reshape(t, [-1, 2]))\n        rtm = tf.contrib.image.compose_transforms(rm, tm)\n\n        return tf.contrib.image.transform(imgs, rtm, ""BILINEAR"")\n\n    def __affine_shape(self,shapes,r,t,isinv=False):\n        if isinv:\n            r = tf.matrix_inverse(r)\n            t = tf.matmul(-t,r)\n        shapes = tf.matmul(shapes,r) + t\n        return shapes\n\n    def __gen_heatmap(self,shapes):\n        shapes = shapes[:,:,tf.newaxis,tf.newaxis,:]\n        value = self.__pixels__ - shapes\n        value = tf.norm(value,axis=-1)\n        value = 1.0 / (tf.reduce_min(value,axis=1) + 1.0)\n        value = tf.expand_dims(value,axis=-1)\n        return value\n\n    def __call__(self,\n                 inputs_imgs,\n                 s1_training,\n                 s2_training,\n                 mean_shape,\n                 imgs_mean,\n                 imgs_std):\n        rd = {}\n        inputs_imgs = tf.reshape(inputs_imgs, [-1, self.img_size, self.img_size, 1])\n        tf.summary.image(\'image\', inputs_imgs, max_outputs=6)\n\n        rd[\'img\'] = inputs_imgs\n\n        mean_shape = tf.reshape(mean_shape,[self.num_lmark,2]) if mean_shape is not None else tf.zeros([self.num_lmark,2],tf.float32)\n        imgs_mean = tf.reshape(imgs_mean,[self.img_size,self.img_size,1]) if imgs_mean is not None else tf.zeros([self.img_size,self.img_size,1],tf.float32)\n        imgs_std = tf.reshape(imgs_std,[self.img_size,self.img_size,1]) if imgs_std is not None else tf.ones([self.img_size,self.img_size,1],tf.float32)\n\n        imgs_mean_tensor = tf.get_variable(\'imgs_mean\',trainable=False,initializer=imgs_mean)\n        imgs_std_tensor = tf.get_variable(\'imgs_std\',trainable=False,initializer=imgs_std)\n        shape_mean_tensor = tf.get_variable(\'shape_mean\',trainable=False,initializer=mean_shape)\n\n        inputs_imgs = (inputs_imgs - imgs_mean_tensor) / imgs_std_tensor\n        # Convert the inputs from channels_last (NHWC) to channels_first\n        # (NCHW).\n        # This provides a large performance boost on GPU.  See\n        # https://www.tensorflow.org/performance/performance_guide#data_formats\n        with tf.variable_scope(\'s1\'):\n            inputs = inputs_imgs\n\n            if self.data_format == \'channels_first\':\n                inputs = tf.transpose(inputs, [0, 3, 1, 2])\n\n            for i, num_filter in enumerate(self.filter_sizes):\n                inputs = vgg_block(inputs=inputs,filters=num_filter,num_convs=self.num_convs,\n                                  training=s1_training,kernel_size=self.kernel_size,maxpool=True,\n                                  data_format=self.data_format)\n        \n            inputs = tf.contrib.layers.flatten(inputs)\n            inputs = tf.layers.dropout(inputs,0.5,training=s1_training)\n\n            s1_fc1 = tf.layers.dense(inputs,256,activation=tf.nn.relu,kernel_initializer=tf.glorot_uniform_initializer())\n            s1_fc1 = batch_norm(s1_fc1,s1_training,data_format=self.data_format)\n\n            s1_fc2 = tf.layers.dense(s1_fc1,self.num_lmark * 2,activation=None)\n            rd[\'s1_ret\'] = tf.identity(tf.reshape(s1_fc2,[-1,self.num_lmark,2]) + shape_mean_tensor,name=\'output_landmark\')\n        \n        with tf.variable_scope(\'s2\'):\n            r,t = self.__calc_affine_params(rd[\'s1_ret\'],shape_mean_tensor)\n            inputs = self.__affine_image(inputs_imgs,r,t)\n            s2_lmark = self.__affine_shape(rd[\'s1_ret\'],r,t)\n            s2_heatmap = self.__gen_heatmap(s2_lmark)\n            s2_feature = tf.layers.dense(s1_fc1,(self.img_size // 2) ** 2,activation=tf.nn.relu,kernel_initializer=tf.glorot_uniform_initializer())\n\n            s2_feature = tf.reshape(s2_feature,[-1,self.img_size // 2,self.img_size // 2,1])\n            s2_feature_upscale = tf.image.resize_images(s2_feature,[self.img_size,self.img_size])\n\n            tf.summary.image(\'heatmap\', s2_heatmap, max_outputs=6)\n            tf.summary.image(\'feature\', s2_feature, max_outputs=6)\n            tf.summary.image(\'image\', inputs, max_outputs=6)\n\n            if self.data_format == \'channels_first\':\n                inputs = tf.transpose(inputs, [0, 3, 1, 2])\n                s2_heatmap = tf.transpose(s2_heatmap,[0, 3, 1, 2])\n                s2_feature_upscale = tf.transpose(s2_feature_upscale, [0, 3, 1, 2])\n\n            inputs = tf.concat([inputs,s2_heatmap,s2_feature_upscale],axis= 1 if self.data_format == \'channels_first\' else 3)\n            inputs = batch_norm(inputs,s2_training,self.data_format)\n\n            for i, num_filter in enumerate(self.filter_sizes):\n                inputs = vgg_block(inputs=inputs,filters=num_filter,num_convs=self.num_convs,\n                                  training=s2_training,kernel_size=self.kernel_size,maxpool=True,\n                                  data_format=self.data_format)\n        \n            inputs = tf.contrib.layers.flatten(inputs)\n            inputs = tf.layers.dropout(inputs,0.5,training=s2_training)\n\n            s2_fc1 = tf.layers.dense(inputs,256,activation=tf.nn.relu,kernel_initializer=tf.glorot_uniform_initializer())\n            s2_fc1 = batch_norm(s2_fc1,s2_training,data_format=self.data_format)\n\n            s2_fc2 = tf.layers.dense(s2_fc1,self.num_lmark * 2,activation=None)\n            s2_fc2 = tf.reshape(s2_fc2,[-1,self.num_lmark,2]) + s2_lmark\n            rd[\'s2_ret\'] = tf.identity(self.__affine_shape(s2_fc2,r,t,isinv=True),name=\'output_landmark\')\n\n        return rd\n'"
DAN_V2/dan_run_loop.py,30,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\n\nimport tensorflow as tf\n\nfrom official.utils.arg_parsers import parsers\nfrom official.utils.logging import hooks_helper\n\nimport dan_model\n\ndef validate_batch_size_for_multi_gpu(batch_size):\n  """"""For multi-gpu, batch-size must be a multiple of the number of\n  available GPUs.\n\n  Note that this should eventually be handled by replicate_model_fn\n  directly. Multi-GPU support is currently experimental, however,\n  so doing the work here until that feature is in place.\n  """"""\n  from tensorflow.python.client import device_lib\n\n  local_device_protos = device_lib.list_local_devices()\n  num_gpus = sum([1 for d in local_device_protos if d.device_type == \'GPU\'])\n  if not num_gpus:\n    raise ValueError(\'Multi-GPU mode was specified, but no GPUs \'\n      \'were found. To use CPU, run without --multi_gpu.\')\n\n  remainder = batch_size % num_gpus\n  if remainder:\n    err = (\'When running with multiple GPUs, batch size \'\n      \'must be a multiple of the number of available GPUs. \'\n      \'Found {} GPUs with a batch size of {}; try --batch_size={} instead.\'\n      ).format(num_gpus, batch_size, batch_size - remainder)\n    raise ValueError(err)\n\ndef process_record_dataset(dataset, is_training, batch_size, shuffle_buffer,\n                           parse_record_fn, num_epochs=1, num_parallel_calls=1,\n                           examples_per_epoch=0, multi_gpu=False):\n    dataset = dataset.prefetch(buffer_size=batch_size)\n    if is_training:\n        dataset = dataset.shuffle(buffer_size=shuffle_buffer)\n    dataset = dataset.repeat(num_epochs)\n\n    if multi_gpu:\n        total_examples = num_epochs * examples_per_epoch\n        dataset = dataset.take(batch_size * (total_examples // batch_size))\n\n    dataset = dataset.map(lambda img,pts: parse_record_fn(img,pts,is_training),\n                          num_parallel_calls=num_parallel_calls)\n\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(1)\n\n    return dataset\n\n\ndef get_synth_input_fn(height, width, num_channels, num_lmark):\n    """"""Returns an input function that returns a dataset with zeroes.\n\n    This is useful in debugging input pipeline performance, as it removes all\n    elements of file reading and image preprocessing.\n\n    Args:\n    height: Integer height that will be used to create a fake image tensor.\n    width: Integer width that will be used to create a fake image tensor.\n    num_channels: Integer depth that will be used to create a fake image tensor.\n    num_classes: Number of classes that should be represented in the fake labels\n        tensor\n\n    Returns:\n    An input_fn that can be used in place of a real one to return a dataset\n    that can be used for iteration.\n    """"""\n    def input_fn(is_training, data_dir, batch_size, *args):\n        images = tf.zeros((batch_size, height, width, num_channels), tf.float32)\n        labels = tf.zeros((batch_size, num_lmark, 2), tf.float32)\n        return tf.data.Dataset.from_tensors((images, labels)).repeat()\n\n    return input_fn\n\ndef dan_model_fn(features,\n                 groundtruth,\n                 mode,\n                 stage,                 \n                 num_lmark,\n                 model_class,\n                 mean_shape,\n                 imgs_mean,\n                 imgs_std,\n                 data_format, multi_gpu=False):\n\n    if isinstance(features, dict):\n        features = features[\'image\']\n\n    model = model_class(num_lmark,data_format)\n    resultdict = model(features,\n                       stage==1 and mode==tf.estimator.ModeKeys.TRAIN,\n                       stage==2 and mode==tf.estimator.ModeKeys.TRAIN,\n                       mean_shape,imgs_mean,imgs_std)\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=resultdict\n        )\n\n    loss_s1 = tf.reduce_mean(tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.squared_difference(groundtruth,resultdict[\'s1_ret\']),-1)),-1) / tf.sqrt(tf.reduce_sum(tf.squared_difference(tf.reduce_max(groundtruth,1),tf.reduce_min(groundtruth,1)),-1)))\n    loss_s2 = tf.reduce_mean(tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.squared_difference(groundtruth,resultdict[\'s2_ret\']),-1)),-1) / tf.sqrt(tf.reduce_sum(tf.squared_difference(tf.reduce_max(groundtruth,1),tf.reduce_min(groundtruth,1)),-1)))\n\n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS,\'s1\')):\n        optimizer_s1 = tf.train.AdamOptimizer(0.001)\n        if multi_gpu:\n            optimizer_s1 = tf.contrib.estimator.TowerOptimizer(optimizer_s1)\n        train_op_s1 = optimizer_s1.minimize(loss_s1,global_step=tf.train.get_or_create_global_step(),\n                                            var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \'s1\'))\n\n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS,\'s2\')):\n        optimizer_s2 = tf.train.AdamOptimizer(0.001)\n        if multi_gpu:\n            optimizer_s2 = tf.contrib.estimator.TowerOptimizer(optimizer_s2)\n        train_op_s2 = optimizer_s2.minimize(loss_s2,global_step=tf.train.get_or_create_global_step(),\n                                            var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \'s2\'))\n\n    loss = loss_s1 if stage == 1 else loss_s2\n    train_op = train_op_s1 if stage == 1 else train_op_s2\n\n    if (mode == tf.estimator.ModeKeys.TRAIN or\n        mode == tf.estimator.ModeKeys.EVAL):\n            loss = loss_s1 if stage == 1 else loss_s2\n    else:\n        loss = None\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        train_op = train_op_s1 if stage == 1 else train_op_s2\n    else:\n        train_op = None\n\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=resultdict,\n        loss=loss,\n        train_op=train_op\n        )\n\ndef dan_main(flags, model_function, input_function):\n    os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n    if flags.multi_gpu:\n        validate_batch_size_for_multi_gpu(flags.batch_size)\n        model_function = tf.contrib.estimator.replicate_model_fn(model_function,loss_reduction=tf.losses.Reduction.MEAN)\n\n    session_config = tf.ConfigProto(\n        inter_op_parallelism_threads=flags.inter_op_parallelism_threads,\n        intra_op_parallelism_threads=flags.intra_op_parallelism_threads,\n        allow_soft_placement=True)\n    run_config = tf.estimator.RunConfig().replace(save_checkpoints_secs=1e9,\n                                                    session_config=session_config)\n    estimator = tf.estimator.Estimator(\n        model_fn=model_function, model_dir=flags.model_dir, config=run_config,\n        params={\n                \'dan_stage\':flags.dan_stage,\n                \'num_lmark\':flags.num_lmark,\n                \'data_format\': flags.data_format,\n                \'batch_size\': flags.batch_size,\n                \'multi_gpu\': flags.multi_gpu,\n            })\n\n    if flags.mode == tf.estimator.ModeKeys.PREDICT:\n        import cv2\n        predict_results = estimator.predict(input_function)\n        for x in predict_results:\n            landmark = x[\'s2_ret\']\n            img = x[\'img\']\n\n            cv2.imshow(\'t\',img)\n            cv2.waitKey(30)\n        return\n\n\n    def input_fn_eval():\n        return input_function(False, flags.data_dir if flags.data_dir_test is not None else flags.data_dir_test, flags.batch_size,\n                              1, flags.num_parallel_calls, flags.multi_gpu)\n\n    def input_fn_train():\n        return input_function(True, flags.data_dir, flags.batch_size,\n                              flags.epochs_per_eval, flags.num_parallel_calls,\n                              flags.multi_gpu)\n\n    if flags.mode == tf.estimator.ModeKeys.EVAL:\n        eval_results = estimator.evaluate(input_fn=input_fn_eval,steps=flags.max_train_steps)\n        print(eval_results)\n\n    if flags.mode == tf.estimator.ModeKeys.TRAIN:\n        for _ in range(flags.train_epochs // flags.epochs_per_eval):\n            train_hooks = hooks_helper.get_train_hooks([""LoggingTensorHook""], batch_size=flags.batch_size)\n\n            print(\'Starting a training cycle.\')\n            estimator.train(input_fn=input_fn_train,\n                            max_steps=flags.max_train_steps)\n\n            print(\'Starting to evaluate.\')\n            eval_results = estimator.evaluate(input_fn=input_fn_eval,\n                                                steps=flags.max_train_steps)\n            print(eval_results)\n            \n\n    \n\nclass DANArgParser(argparse.ArgumentParser):\n  """"""Arguments for configuring and running a Resnet Model.\n  """"""\n\n  def __init__(self):\n    super(DANArgParser, self).__init__(parents=[\n        parsers.BaseParser(),\n        parsers.PerformanceParser(),\n        parsers.ImageModelParser(),\n    ])\n\n    self.add_argument(\n        ""--data_dir_test"", ""-ddt"", default=None,\n        help=""[default: %(default)s] The location of the test data."",\n        metavar=""<DD>"",\n    )\n\n    self.add_argument(\n        \'--dan_stage\', \'-ds\', type=int, default=1,\n        choices=[1,2],\n        help=\'[default: %(default)s] The stage of the DAN model.\'\n    )\n\n    self.add_argument(\n        \'--mode\',\'-mode\',type=str,default=\'train\',\n        choices=[\'train\',\'eval\',\'predict\']\n    )\n\n    self.add_argument(\n        \'--num_lmark\',\'-nlm\',type=int,default=68\n        )'"
DAN_V2/preprocessing.py,16,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os \nimport sys\nimport glob\nimport random\nimport numpy as np\nimport cv2\nimport uuid\n\nimport tensorflow as tf\n\ntf.app.flags.DEFINE_string(\'input_dir\', None, ""input_dir"")\ntf.app.flags.DEFINE_string(\'output_dir\', None, ""output_dir"")\ntf.app.flags.DEFINE_boolean(\'istrain\', False, ""istrain"")\ntf.app.flags.DEFINE_integer(\'repeat\', 1, \'repeat\')\ntf.app.flags.DEFINE_integer(\'img_size\', 112, \'img_size\')\ntf.app.flags.DEFINE_string(\'mirror_file\', None, \'mirror_file\')\n\nFLAGS = tf.app.flags.FLAGS\nBATCH_SIZE = 128\n\ndef getAffine(From, To):\n    FromMean = np.mean(From, axis=0)\n    ToMean = np.mean(To, axis=0)\n\n    FromCentralized = From - FromMean\n    ToCentralized = To - ToMean\n\n    FromVector = (FromCentralized).flatten()\n    ToVector = (ToCentralized).flatten()\n\n    DotResult = np.dot(FromVector, ToVector)\n    NormPow2 = np.linalg.norm(FromCentralized) ** 2\n\n    a = DotResult / NormPow2\n    b = np.sum(np.cross(FromCentralized, ToCentralized)) / NormPow2\n\n    R = np.array([[a, b], [-b, a]])\n    T = ToMean - np.dot(FromMean, R)\n\n    return R, T\n\n\ndef _load_data(imagepath, ptspath, is_train,mirror_array):\n    def makerotate(angle):\n        rad = angle * np.pi / 180.0\n        return np.array([[np.cos(rad), np.sin(rad)], [-np.sin(rad), np.cos(rad)]], dtype=np.float32)\n\n    srcpts = np.genfromtxt(ptspath.decode(), skip_header=3, skip_footer=1)\n    x, y = np.min(srcpts, axis=0).astype(np.int32)\n    w, h = np.ptp(srcpts, axis=0).astype(np.int32)\n    pts = (srcpts - [x, y]) / [w, h]\n\n    img = cv2.imread(imagepath.decode(), cv2.IMREAD_GRAYSCALE)\n    center = [0.5, 0.5]\n\n    if is_train:\n        pts = pts - center\n        pts = np.dot(pts, makerotate(np.random.normal(0, 20)))\n        pts = pts * np.random.normal(0.8, 0.05)\n        pts = pts + [np.random.normal(0, 0.05),\n                     np.random.normal(0, 0.05)] + center\n\n        pts = pts * FLAGS.img_size\n\n        R, T = getAffine(srcpts, pts)\n        M = np.zeros((2, 3), dtype=np.float32)\n        M[0:2, 0:2] = R.T\n        M[:, 2] = T\n        img = cv2.warpAffine(img, M, (FLAGS.img_size, FLAGS.img_size))\n\n        if any(mirror_array) and random.choice((True, False)):\n            pts[:,0] = FLAGS.img_size - 1 - pts[:,0]\n            pts = pts[mirror_array]\n            img = cv2.flip(img, 1)\n\n    else:\n        pts = pts - center\n        pts = pts * 0.8\n        pts = pts + center\n\n        pts = pts * FLAGS.img_size\n\n        R, T = getAffine(srcpts, pts)\n        M = np.zeros((2, 3), dtype=np.float32)\n        M[0:2, 0:2] = R.T\n        M[:, 2] = T\n        img = cv2.warpAffine(img, M, (FLAGS.img_size, FLAGS.img_size))\n\n\n    _,filename = os.path.split(imagepath.decode())\n    filename,_ = os.path.splitext(filename)\n\n    uid = str(uuid.uuid1())\n\n    cv2.imwrite(os.path.join(FLAGS.output_dir,filename + \'@\' + uid + \'.png\'),img)\n    np.savetxt(os.path.join(FLAGS.output_dir,filename + \'@\' + uid + \'.ptv\'),pts,delimiter=\',\')\n\n    return img,pts.astype(np.float32)\n\ndef _input_fn(img, pts, is_train,mirror_array):\n    dataset_image = tf.data.Dataset.from_tensor_slices(img)\n    dataset_pts = tf.data.Dataset.from_tensor_slices(pts)\n    dataset = tf.data.Dataset.zip((dataset_image, dataset_pts))\n\n    dataset = dataset.prefetch(BATCH_SIZE)\n    dataset = dataset.repeat(FLAGS.repeat)\n    dataset = dataset.map(lambda imagepath, ptspath: tuple(tf.py_func(_load_data, [\n                          imagepath, ptspath, is_train,mirror_array], [tf.uint8,tf.float32])), num_parallel_calls=8)                     \n    dataset = dataset.prefetch(1)\n\n    return dataset\n\ndef _get_filenames(data_dir, listext):\n    imagelist = []\n    for ext in listext:\n        p = os.path.join(data_dir, ext)\n        imagelist.extend(glob.glob(p))\n\n    ptslist = []\n    for image in imagelist:\n        ptslist.append(os.path.splitext(image)[0] + "".pts"")\n\n    return imagelist, ptslist\n\ndef main(argv):\n    imagenames, ptsnames = _get_filenames(FLAGS.input_dir, [""*.jpg"", ""*.png""])\n    mirror_array = np.genfromtxt(FLAGS.mirror_file, dtype=int, delimiter=\',\') if FLAGS.mirror_file else np.zeros(1)\n    \n    dataset = _input_fn(imagenames,ptsnames,FLAGS.istrain,mirror_array)\n    next_element = dataset.make_one_shot_iterator().get_next()\n\n    img_list = []\n    pts_list = []\n\n    with tf.Session() as sess:\n        count = 0\n        while True:\n            try:\n                img,pts = sess.run(next_element)\n                img_list.append(img)\n                pts_list.append(pts)\n            except tf.errors.OutOfRangeError:\n                img_list = np.stack(img_list)\n                pts_list = np.stack(pts_list)\n\n                mean_shape = np.mean(pts_list,axis=0)\n                imgs_mean = np.mean(img_list,axis=0)\n                imgs_std = np.std(img_list,axis=0)\n\n                np.savetxt(os.path.join(FLAGS.output_dir,\'mean_shape.ptv\'),mean_shape,delimiter=\',\')\n                np.savetxt(os.path.join(FLAGS.output_dir,\'imgs_mean.ptv\'),imgs_mean,delimiter=\',\')\n                np.savetxt(os.path.join(FLAGS.output_dir,\'imgs_std.ptv\'),imgs_std,delimiter=\',\')\n\n                print(""end"")\n                break\n\n\nif __name__ == ""__main__"":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run(argv=sys.argv)'"
DAN_V2/video_test.py,0,b'\n'
DAN_V2/official/utils/__init__.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================='"
DAN_V2/official/utils/arg_parsers/__init__.py,0,b''
DAN_V2/official/utils/arg_parsers/parsers.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Collection of parsers which are shared among the official models.\n\nThe parsers in this module are intended to be used as parents to all arg\nparsers in official models. For instance, one might define a new class:\n\nclass ExampleParser(argparse.ArgumentParser):\n  def __init__(self):\n    super(ExampleParser, self).__init__(parents=[\n      arg_parsers.LocationParser(data_dir=True, model_dir=True),\n      arg_parsers.DummyParser(use_synthetic_data=True),\n    ])\n\n    self.add_argument(\n      ""--application_specific_arg"", ""-asa"", type=int, default=123,\n      help=""[default: %(default)s] This arg is application specific."",\n      metavar=""<ASA>""\n    )\n\nNotes about add_argument():\n    Argparse will automatically template in default values in help messages if\n  the ""%(default)s"" string appears in the message. Using the example above:\n\n    parser = ExampleParser()\n    parser.set_defaults(application_specific_arg=3141592)\n    parser.parse_args([""-h""])\n\n    When the help text is generated, it will display 3141592 to the user. (Even\n  though the default was 123 when the flag was created.)\n\n\n    The metavar variable determines how the flag will appear in help text. If\n  not specified, the convention is to use name.upper(). Thus rather than:\n\n    --application_specific_arg APPLICATION_SPECIFIC_ARG, -asa APPLICATION_SPECIFIC_ARG\n\n  if metavar=""<ASA>"" is set, the user sees:\n\n    --application_specific_arg <ASA>, -asa <ASA>\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport argparse\n\n\nclass BaseParser(argparse.ArgumentParser):\n  """"""Parser to contain flags which will be nearly universal across models.\n\n  Args:\n    add_help: Create the ""--help"" flag. False if class instance is a parent.\n    data_dir: Create a flag for specifying the input data directory.\n    model_dir: Create a flag for specifying the model file directory.\n    train_epochs: Create a flag to specify the number of training epochs.\n    epochs_per_eval: Create a flag to specify the frequency of testing.\n    batch_size: Create a flag to specify the batch size.\n    multi_gpu: Create a flag to allow the use of all available GPUs.\n    hooks: Create a flag to specify hooks for logging.\n  """"""\n\n  def __init__(self, add_help=False, data_dir=True, model_dir=True,\n               train_epochs=True, epochs_per_eval=True, batch_size=True,\n               multi_gpu=True, hooks=True):\n    super(BaseParser, self).__init__(add_help=add_help)\n\n    if data_dir:\n      self.add_argument(\n          ""--data_dir"", ""-dd"", default=""/tmp"",\n          help=""[default: %(default)s] The location of the input data."",\n          metavar=""<DD>"",\n      )\n\n    if model_dir:\n      self.add_argument(\n          ""--model_dir"", ""-md"", default=""/tmp"",\n          help=""[default: %(default)s] The location of the model files."",\n          metavar=""<MD>"",\n      )\n\n    if train_epochs:\n      self.add_argument(\n          ""--train_epochs"", ""-te"", type=int, default=1,\n          help=""[default: %(default)s] The number of epochs used to train."",\n          metavar=""<TE>""\n      )\n\n    if epochs_per_eval:\n      self.add_argument(\n          ""--epochs_per_eval"", ""-epe"", type=int, default=1,\n          help=""[default: %(default)s] The number of training epochs to run ""\n               ""between evaluations."",\n          metavar=""<EPE>""\n      )\n\n    if batch_size:\n      self.add_argument(\n          ""--batch_size"", ""-bs"", type=int, default=32,\n          help=""[default: %(default)s] Batch size for training and evaluation."",\n          metavar=""<BS>""\n      )\n\n    if multi_gpu:\n      self.add_argument(\n          ""--multi_gpu"", action=""store_true"",\n          help=""If set, run across all available GPUs.""\n      )\n\n    if hooks:\n      self.add_argument(\n          ""--hooks"", ""-hk"", nargs=""+"", default=[""LoggingTensorHook""],\n          help=""[default: %(default)s] A list of strings to specify the names ""\n               ""of train hooks. ""\n               ""Example: --hooks LoggingTensorHook ExamplesPerSecondHook. ""\n               ""Allowed hook names (case-insensitive): LoggingTensorHook, ""\n               ""ProfilerHook, ExamplesPerSecondHook. ""\n               ""See official.utils.logging.hooks_helper for details."",\n          metavar=""<HK>""\n      )\n\n\nclass PerformanceParser(argparse.ArgumentParser):\n  """"""Default parser for specifying performance tuning arguments.\n\n  Args:\n    add_help: Create the ""--help"" flag. False if class instance is a parent.\n    num_parallel_calls: Create a flag to specify parallelism of data loading.\n    inter_op: Create a flag to allow specification of inter op threads.\n    intra_op: Create a flag to allow specification of intra op threads.\n  """"""\n\n  def __init__(self, add_help=False, num_parallel_calls=True, inter_op=True,\n               intra_op=True, use_synthetic_data=True, max_train_steps=True):\n    super(PerformanceParser, self).__init__(add_help=add_help)\n\n    if num_parallel_calls:\n      self.add_argument(\n          ""--num_parallel_calls"", ""-npc"",\n          type=int, default=5,\n          help=""[default: %(default)s] The number of records that are ""\n               ""processed in parallel  during input processing. This can be ""\n               ""optimized per data set but for generally homogeneous data ""\n               ""sets, should be approximately the number of available CPU ""\n               ""cores."",\n          metavar=""<NPC>""\n      )\n\n    if inter_op:\n      self.add_argument(\n          ""--inter_op_parallelism_threads"", ""-inter"",\n          type=int, default=0,\n          help=""[default: %(default)s Number of inter_op_parallelism_threads ""\n               ""to use for CPU. See TensorFlow config.proto for details."",\n          metavar=""<INTER>""\n      )\n\n    if intra_op:\n      self.add_argument(\n          ""--intra_op_parallelism_threads"", ""-intra"",\n          type=int, default=0,\n          help=""[default: %(default)s Number of intra_op_parallelism_threads ""\n               ""to use for CPU. See TensorFlow config.proto for details."",\n          metavar=""<INTRA>""\n      )\n\n    if use_synthetic_data:\n      self.add_argument(\n          ""--use_synthetic_data"", ""-synth"",\n          action=""store_true"",\n          help=""If set, use fake data (zeroes) instead of a real dataset. ""\n               ""This mode is useful for performance debugging, as it removes ""\n               ""input processing steps, but will not learn anything.""\n      )\n\n    if max_train_steps:\n      self.add_argument(\n          ""--max_train_steps"", ""-mts"", type=int, default=None,\n          help=""[default: %(default)s] The model will stop training if the ""\n               ""global_step reaches this value. If not set, training will run""\n               ""until the specified number of epochs have run as usual. It is""\n               ""generally recommended to set --train_epochs=1 when using this""\n               ""flag."",\n          metavar=""<MTS>""\n      )\n\n\nclass ImageModelParser(argparse.ArgumentParser):\n  """"""Default parser for specification image specific behavior.\n\n  Args:\n    add_help: Create the ""--help"" flag. False if class instance is a parent.\n    data_format: Create a flag to specify image axis convention.\n  """"""\n\n  def __init__(self, add_help=False, data_format=True):\n    super(ImageModelParser, self).__init__(add_help=add_help)\n    if data_format:\n      self.add_argument(\n          ""--data_format"", ""-df"",\n          help=""A flag to override the data format used in the model. ""\n               ""channels_first provides a performance boost on GPU but is not ""\n               ""always compatible with CPU. If left unspecified, the data ""\n               ""format will be chosen automatically based on whether TensorFlow""\n               ""was built for CPU or GPU."",\n          metavar=""<CF>""\n      )\n'"
DAN_V2/official/utils/arg_parsers/parsers_test.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport argparse\nimport unittest\n\n\nfrom official.utils.arg_parsers import parsers\n\n\nclass TestParser(argparse.ArgumentParser):\n  """"""Class to test canned parser functionality.""""""\n\n  def __init__(self):\n    super(TestParser, self).__init__(parents=[\n        parsers.BaseParser(),\n        parsers.PerformanceParser(num_parallel_calls=True, inter_op=True,\n                                  intra_op=True, use_synthetic_data=True),\n        parsers.ImageModelParser(data_format=True)\n    ])\n\n\nclass BaseTester(unittest.TestCase):\n\n  def test_default_setting(self):\n    """"""Test to ensure fields exist and defaults can be set.\n    """"""\n\n    defaults = dict(\n        data_dir=""dfgasf"",\n        model_dir=""dfsdkjgbs"",\n        train_epochs=534,\n        epochs_per_eval=15,\n        batch_size=256,\n        hooks=[""LoggingTensorHook""],\n        num_parallel_calls=18,\n        inter_op_parallelism_threads=5,\n        intra_op_parallelism_thread=10,\n        data_format=""channels_first""\n    )\n\n    parser = TestParser()\n    parser.set_defaults(**defaults)\n\n    namespace_vars = vars(parser.parse_args([]))\n    for key, value in defaults.items():\n      assert namespace_vars[key] == value\n\n  def test_booleans(self):\n    """"""Test to ensure boolean flags trigger as expected.\n    """"""\n\n    parser = TestParser()\n    namespace = parser.parse_args([""--multi_gpu"", ""--use_synthetic_data""])\n\n    assert namespace.multi_gpu\n    assert namespace.use_synthetic_data\n\n\nif __name__ == ""__main__"":\n  unittest.main()\n'"
DAN_V2/official/utils/logging/__init__.py,0,b''
DAN_V2/official/utils/logging/hooks.py,5,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Hook that counts examples per second every N steps or seconds.""""""\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\nclass ExamplesPerSecondHook(tf.train.SessionRunHook):\n  """"""Hook to print out examples per second.\n\n  Total time is tracked and then divided by the total number of steps\n  to get the average step time and then batch_size is used to determine\n  the running average of examples per second. The examples per second for the\n  most recent interval is also logged.\n  """"""\n\n  def __init__(self,\n               batch_size,\n               every_n_steps=None,\n               every_n_secs=None,\n               warm_steps=0):\n    """"""Initializer for ExamplesPerSecondHook.\n\n    Args:\n      batch_size: Total batch size across all workers used to calculate\n        examples/second from global time.\n      every_n_steps: Log stats every n steps.\n      every_n_secs: Log stats every n seconds. Exactly one of the\n        `every_n_steps` or `every_n_secs` should be set.\n      warm_steps: The number of steps to be skipped before logging and running\n        average calculation. warm_steps steps refers to global steps across all\n        workers, not on each worker\n\n    Raises:\n      ValueError: if neither `every_n_steps` or `every_n_secs` is set, or\n      both are set.\n    """"""\n\n    if (every_n_steps is None) == (every_n_secs is None):\n      raise ValueError(\'exactly one of every_n_steps\'\n                       \' and every_n_secs should be provided.\')\n\n    self._timer = tf.train.SecondOrStepTimer(\n        every_steps=every_n_steps, every_secs=every_n_secs)\n\n    self._step_train_time = 0\n    self._total_steps = 0\n    self._batch_size = batch_size\n    self._warm_steps = warm_steps\n\n  def begin(self):\n    """"""Called once before using the session to check global step.""""""\n    self._global_step_tensor = tf.train.get_global_step()\n    if self._global_step_tensor is None:\n      raise RuntimeError(\n          \'Global step should be created to use StepCounterHook.\')\n\n  def before_run(self, run_context):  # pylint: disable=unused-argument\n    """"""Called before each call to run().\n\n    Args:\n      run_context: A SessionRunContext object.\n\n    Returns:\n      A SessionRunArgs object or None if never triggered.\n    """"""\n    return tf.train.SessionRunArgs(self._global_step_tensor)\n\n  def after_run(self, run_context, run_values):  # pylint: disable=unused-argument\n    """"""Called after each call to run().\n\n    Args:\n      run_context: A SessionRunContext object.\n      run_values: A SessionRunValues object.\n    """"""\n    global_step = run_values.results\n\n    if self._timer.should_trigger_for_step(\n        global_step) and global_step > self._warm_steps:\n      elapsed_time, elapsed_steps = self._timer.update_last_triggered_step(\n          global_step)\n      if elapsed_time is not None:\n        self._step_train_time += elapsed_time\n        self._total_steps += elapsed_steps\n\n        # average examples per second is based on the total (accumulative)\n        # training steps and training time so far\n        average_examples_per_sec = self._batch_size * (\n            self._total_steps / self._step_train_time)\n        # current examples per second is based on the elapsed training steps\n        # and training time per batch\n        current_examples_per_sec = self._batch_size * (\n            elapsed_steps / elapsed_time)\n        # Current examples/sec followed by average examples/sec\n        tf.logging.info(\'Batch [%g]:  current exp/sec = %g, average exp/sec = \'\n                        \'%g\', self._total_steps, current_examples_per_sec,\n                        average_examples_per_sec)\n'"
DAN_V2/official/utils/logging/hooks_helper.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Hooks helper to return a list of TensorFlow hooks for training by name.\n\nMore hooks can be added to this set. To add a new hook, 1) add the new hook to\nthe registry in HOOKS, 2) add a corresponding function that parses out necessary\nparameters.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom official.utils.logging import hooks\n\n_TENSORS_TO_LOG = dict((x, x) for x in [\'learning_rate\',\n                                        \'cross_entropy\',\n                                        \'train_accuracy\'])\n\n\ndef get_train_hooks(name_list, **kwargs):\n  """"""Factory for getting a list of TensorFlow hooks for training by name.\n\n  Args:\n    name_list: a list of strings to name desired hook classes. Allowed:\n      LoggingTensorHook, ProfilerHook, ExamplesPerSecondHook, which are defined\n      as keys in HOOKS\n    kwargs: a dictionary of arguments to the hooks.\n\n  Returns:\n    list of instantiated hooks, ready to be used in a classifier.train call.\n\n  Raises:\n    ValueError: if an unrecognized name is passed.\n  """"""\n\n  if not name_list:\n    return []\n\n  train_hooks = []\n  for name in name_list:\n    hook_name = HOOKS.get(name.strip().lower())\n    if hook_name is None:\n      raise ValueError(\'Unrecognized training hook requested: {}\'.format(name))\n    else:\n      train_hooks.append(hook_name(**kwargs))\n\n  return train_hooks\n\n\ndef get_logging_tensor_hook(every_n_iter=100, **kwargs):  # pylint: disable=unused-argument\n  """"""Function to get LoggingTensorHook.\n\n  Args:\n    every_n_iter: `int`, print the values of `tensors` once every N local\n      steps taken on the current worker.\n    kwargs: a dictionary of arguments to LoggingTensorHook.\n\n  Returns:\n    Returns a LoggingTensorHook with a standard set of tensors that will be\n    printed to stdout.\n  """"""\n  return tf.train.LoggingTensorHook(\n      tensors=_TENSORS_TO_LOG,\n      every_n_iter=every_n_iter)\n\n\ndef get_profiler_hook(save_steps=1000, **kwargs):  # pylint: disable=unused-argument\n  """"""Function to get ProfilerHook.\n\n  Args:\n    save_steps: `int`, print profile traces every N steps.\n    kwargs: a dictionary of arguments to ProfilerHook.\n\n  Returns:\n    Returns a ProfilerHook that writes out timelines that can be loaded into\n    profiling tools like chrome://tracing.\n  """"""\n  return tf.train.ProfilerHook(save_steps=save_steps)\n\n\ndef get_examples_per_second_hook(every_n_steps=100,\n                                 batch_size=128,\n                                 warm_steps=5,\n                                 **kwargs):  # pylint: disable=unused-argument\n  """"""Function to get ExamplesPerSecondHook.\n\n  Args:\n    every_n_steps: `int`, print current and average examples per second every\n      N steps.\n    batch_size: `int`, total batch size used to calculate examples/second from\n      global time.\n    warm_steps: skip this number of steps before logging and running average.\n    kwargs: a dictionary of arguments to ExamplesPerSecondHook.\n\n  Returns:\n    Returns a ProfilerHook that writes out timelines that can be loaded into\n    profiling tools like chrome://tracing.\n  """"""\n  return hooks.ExamplesPerSecondHook(every_n_steps=every_n_steps,\n                                     batch_size=batch_size,\n                                     warm_steps=warm_steps)\n\n\n# A dictionary to map one hook name and its corresponding function\nHOOKS = {\n    \'loggingtensorhook\': get_logging_tensor_hook,\n    \'profilerhook\': get_profiler_hook,\n    \'examplespersecondhook\': get_examples_per_second_hook,\n}\n\n'"
DAN_V2/official/utils/logging/hooks_helper_test.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for hooks_helper.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport tensorflow as tf\n\nfrom official.utils.logging import hooks_helper\n\n\nclass BaseTest(unittest.TestCase):\n\n  def test_raise_in_non_list_names(self):\n    with self.assertRaises(ValueError):\n      hooks_helper.get_train_hooks(\n          \'LoggingTensorHook, ProfilerHook\', batch_size=256)\n\n  def test_raise_in_invalid_names(self):\n    invalid_names = [\'StepCounterHook\', \'StopAtStepHook\']\n    with self.assertRaises(ValueError):\n      hooks_helper.get_train_hooks(invalid_names, batch_size=256)\n\n  def validate_train_hook_name(self,\n                               test_hook_name,\n                               expected_hook_name,\n                               **kwargs):\n    returned_hook = hooks_helper.get_train_hooks([test_hook_name], **kwargs)\n    self.assertEqual(len(returned_hook), 1)\n    self.assertIsInstance(returned_hook[0], tf.train.SessionRunHook)\n    self.assertEqual(returned_hook[0].__class__.__name__.lower(),\n                     expected_hook_name)\n\n  def test_get_train_hooks_logging_tensor_hook(self):\n    test_hook_name = \'LoggingTensorHook\'\n    self.validate_train_hook_name(test_hook_name, \'loggingtensorhook\')\n\n  def test_get_train_hooks_profiler_hook(self):\n    test_hook_name = \'ProfilerHook\'\n    self.validate_train_hook_name(test_hook_name, \'profilerhook\')\n\n  def test_get_train_hooks_examples_per_second_hook(self):\n    test_hook_name = \'ExamplesPerSecondHook\'\n    self.validate_train_hook_name(test_hook_name, \'examplespersecondhook\')\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
DAN_V2/official/utils/logging/hooks_test.py,17,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for hooks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\n\nimport tensorflow as tf\n\nfrom tensorflow.python.training import monitored_session\nfrom official.utils.logging import hooks\n\n\ntf.logging.set_verbosity(tf.logging.ERROR)\n\n\nclass ExamplesPerSecondHookTest(tf.test.TestCase):\n\n  def setUp(self):\n    """"""Mock out logging calls to verify if correct info is being monitored.""""""\n    self._actual_log = tf.logging.info\n    self.logged_message = None\n\n    def mock_log(*args, **kwargs):\n      self.logged_message = args\n      self._actual_log(*args, **kwargs)\n\n    tf.logging.info = mock_log\n\n    self.graph = tf.Graph()\n    with self.graph.as_default():\n      self.global_step = tf.train.get_or_create_global_step()\n      self.train_op = tf.assign_add(self.global_step, 1)\n\n  def tearDown(self):\n    tf.logging.info = self._actual_log\n\n  def test_raise_in_both_secs_and_steps(self):\n    with self.assertRaises(ValueError):\n      hooks.ExamplesPerSecondHook(\n          batch_size=256,\n          every_n_steps=10,\n          every_n_secs=20)\n\n  def test_raise_in_none_secs_and_steps(self):\n    with self.assertRaises(ValueError):\n      hooks.ExamplesPerSecondHook(\n          batch_size=256,\n          every_n_steps=None,\n          every_n_secs=None)\n\n  def _validate_log_every_n_steps(self, sess, every_n_steps, warm_steps):\n    hook = hooks.ExamplesPerSecondHook(\n        batch_size=256,\n        every_n_steps=every_n_steps,\n        warm_steps=warm_steps)\n    hook.begin()\n    mon_sess = monitored_session._HookedSession(sess, [hook])\n    sess.run(tf.global_variables_initializer())\n\n    self.logged_message = \'\'\n    for _ in range(every_n_steps):\n      mon_sess.run(self.train_op)\n      self.assertEqual(str(self.logged_message).find(\'exp/sec\'), -1)\n\n    mon_sess.run(self.train_op)\n    global_step_val = sess.run(self.global_step)\n    # assertNotRegexpMatches is not supported by python 3.1 and later\n    if global_step_val > warm_steps:\n      self.assertRegexpMatches(str(self.logged_message), \'exp/sec\')\n    else:\n      self.assertEqual(str(self.logged_message).find(\'exp/sec\'), -1)\n\n    # Add additional run to verify proper reset when called multiple times.\n    self.logged_message = \'\'\n    mon_sess.run(self.train_op)\n    global_step_val = sess.run(self.global_step)\n    if every_n_steps == 1 and global_step_val > warm_steps:\n      self.assertRegexpMatches(str(self.logged_message), \'exp/sec\')\n    else:\n      self.assertEqual(str(self.logged_message).find(\'exp/sec\'), -1)\n\n    hook.end(sess)\n\n  def test_examples_per_sec_every_1_steps(self):\n    with self.graph.as_default(), tf.Session() as sess:\n      self._validate_log_every_n_steps(sess, 1, 0)\n\n  def test_examples_per_sec_every_5_steps(self):\n    with self.graph.as_default(), tf.Session() as sess:\n      self._validate_log_every_n_steps(sess, 5, 0)\n\n  def test_examples_per_sec_every_1_steps_with_warm_steps(self):\n    with self.graph.as_default(), tf.Session() as sess:\n      self._validate_log_every_n_steps(sess, 1, 10)\n\n  def test_examples_per_sec_every_5_steps_with_warm_steps(self):\n    with self.graph.as_default(), tf.Session() as sess:\n      self._validate_log_every_n_steps(sess, 5, 10)\n\n  def _validate_log_every_n_secs(self, sess, every_n_secs):\n    hook = hooks.ExamplesPerSecondHook(\n        batch_size=256,\n        every_n_steps=None,\n        every_n_secs=every_n_secs)\n    hook.begin()\n    mon_sess = monitored_session._HookedSession(sess, [hook])\n    sess.run(tf.global_variables_initializer())\n\n    self.logged_message = \'\'\n    mon_sess.run(self.train_op)\n    self.assertEqual(str(self.logged_message).find(\'exp/sec\'), -1)\n    time.sleep(every_n_secs)\n\n    self.logged_message = \'\'\n    mon_sess.run(self.train_op)\n    self.assertRegexpMatches(str(self.logged_message), \'exp/sec\')\n\n    hook.end(sess)\n\n  def test_examples_per_sec_every_1_secs(self):\n    with self.graph.as_default(), tf.Session() as sess:\n      self._validate_log_every_n_secs(sess, 1)\n\n  def test_examples_per_sec_every_5_secs(self):\n    with self.graph.as_default(), tf.Session() as sess:\n      self._validate_log_every_n_secs(sess, 5)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
DAN_V2/official/utils/logging/logger.py,5,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Logging utilities for benchmark.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport datetime\nimport json\nimport numbers\nimport os\n\nimport tensorflow as tf\n\n_METRIC_LOG_FILE_NAME = ""metric.log""\n_DATE_TIME_FORMAT_PATTERN = ""%Y-%m-%dT%H:%M:%S.%fZ""\n\n\nclass BenchmarkLogger(object):\n  """"""Class to log the benchmark information to local disk.""""""\n\n  def __init__(self, logging_dir):\n    self._logging_dir = logging_dir\n    if not tf.gfile.IsDirectory(self._logging_dir):\n      tf.gfile.MakeDirs(self._logging_dir)\n\n  def log_metric(self, name, value, unit=None, global_step=None, extras=None):\n    """"""Log the benchmark metric information to local file.\n\n    Currently the logging is done in a synchronized way. This should be updated\n    to log asynchronously.\n\n    Args:\n      name: string, the name of the metric to log.\n      value: number, the value of the metric. The value will not be logged if it\n        is not a number type.\n      unit: string, the unit of the metric, E.g ""image per second"".\n      global_step: int, the global_step when the metric is logged.\n      extras: map of string:string, the extra information about the metric.\n    """"""\n    if not isinstance(value, numbers.Number):\n      tf.logging.warning(\n        ""Metric value to log should be a number. Got %s"", type(value))\n      return\n\n    with tf.gfile.GFile(\n        os.path.join(self._logging_dir, _METRIC_LOG_FILE_NAME), ""a"") as f:\n      metric = {\n          ""name"": name,\n          ""value"": float(value),\n          ""unit"": unit,\n          ""global_step"": global_step,\n          ""timestamp"": datetime.datetime.now().strftime(\n              _DATE_TIME_FORMAT_PATTERN),\n          ""extras"": extras}\n      try:\n        json.dump(metric, f)\n        f.write(""\\n"")\n      except (TypeError, ValueError) as e:\n        tf.logging.warning(""Failed to dump metric to log file: name %s, value %s, error %s"",\n                           name, value, e)\n\n'"
DAN_V2/official/utils/logging/logger_test.py,11,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for benchmark logger.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport json\nimport os\nimport tempfile\n\n\nfrom official.utils.logging import logger\nimport tensorflow as tf\n\n\nclass BenchmarkLoggerTest(tf.test.TestCase):\n\n  def tearDown(self):\n    super(BenchmarkLoggerTest, self).tearDown()\n    tf.gfile.DeleteRecursively(self.get_temp_dir())\n\n  def test_create_logging_dir(self):\n    non_exist_temp_dir = os.path.join(self.get_temp_dir(), ""unknown_dir"")\n    self.assertFalse(tf.gfile.IsDirectory(non_exist_temp_dir))\n\n    logger.BenchmarkLogger(non_exist_temp_dir)\n    self.assertTrue(tf.gfile.IsDirectory(non_exist_temp_dir))\n\n  def test_log_metric(self):\n    log_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    log = logger.BenchmarkLogger(log_dir)\n    log.log_metric(""accuracy"", 0.999, global_step=1e4, extras={""name"": ""value""})\n\n    metric_log = os.path.join(log_dir, ""metric.log"")\n    self.assertTrue(tf.gfile.Exists(metric_log))\n    with tf.gfile.GFile(metric_log) as f:\n      metric = json.loads(f.readline())\n      self.assertEqual(metric[""name""], ""accuracy"")\n      self.assertEqual(metric[""value""], 0.999)\n      self.assertEqual(metric[""unit""], None)\n      self.assertEqual(metric[""global_step""], 1e4)\n      self.assertEqual(metric[""extras""], {""name"": ""value""})\n\n  def test_log_multiple_metrics(self):\n    log_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    log = logger.BenchmarkLogger(log_dir)\n    log.log_metric(""accuracy"", 0.999, global_step=1e4, extras={""name"": ""value""})\n    log.log_metric(""loss"", 0.02, global_step=1e4)\n\n    metric_log = os.path.join(log_dir, ""metric.log"")\n    self.assertTrue(tf.gfile.Exists(metric_log))\n    with tf.gfile.GFile(metric_log) as f:\n      accuracy = json.loads(f.readline())\n      self.assertEqual(accuracy[""name""], ""accuracy"")\n      self.assertEqual(accuracy[""value""], 0.999)\n      self.assertEqual(accuracy[""unit""], None)\n      self.assertEqual(accuracy[""global_step""], 1e4)\n      self.assertEqual(accuracy[""extras""], {""name"": ""value""})\n\n      loss = json.loads(f.readline())\n      self.assertEqual(loss[""name""], ""loss"")\n      self.assertEqual(loss[""value""], 0.02)\n      self.assertEqual(loss[""unit""], None)\n      self.assertEqual(loss[""global_step""], 1e4)\n\n  def test_log_non_nubmer_value(self):\n    log_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    log = logger.BenchmarkLogger(log_dir)\n    const = tf.constant(1)\n    log.log_metric(""accuracy"", const)\n\n    metric_log = os.path.join(log_dir, ""metric.log"")\n    self.assertFalse(tf.gfile.Exists(metric_log))\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
DAN_V2/official/utils/testing/__init__.py,0,b''
DAN_V2/official/utils/testing/integration.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Helper code to run complete models from within python.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport os\nimport shutil\nimport sys\nimport tempfile\n\n\ndef run_synthetic(main, tmp_root, extra_flags=None):\n  """"""Performs a minimal run of a model.\n\n    This function is intended to test for syntax errors throughout a model. A\n  very limited run is performed using synthetic data.\n\n  Args:\n    main: The primary function used to excercise a code path. Generally this\n      function is ""<MODULE>.main(argv)"".\n    tmp_root: Root path for the temp directory created by the test class.\n    extra_flags: Additional flags passed by the the caller of this function.\n  """"""\n\n  extra_flags = [] if extra_flags is None else extra_flags\n\n  model_dir = tempfile.mkdtemp(dir=tmp_root)\n\n  args = [sys.argv[0], ""--model_dir"", model_dir, ""--train_epochs"", ""1"",\n          ""--epochs_per_eval"", ""1"", ""--use_synthetic_data"",\n          ""--max_train_steps"", ""1""] + extra_flags\n\n  try:\n    main(args)\n  finally:\n    if os.path.exists(model_dir):\n      shutil.rmtree(model_dir)\n'"
