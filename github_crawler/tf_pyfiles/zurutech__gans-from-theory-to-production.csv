file_path,api_count,code
4. Production/deploy/main.py,20,"b'""""""\nServing a TF 2.0 Model via Cloud Function.\n\nMore info: http://bit.ly/310wRZl\n""""""\nfrom __future__ import annotations\n\nimport base64\nimport os\nimport tempfile\nfrom io import BytesIO\nfrom typing import TYPE_CHECKING, List\n\nimport numpy as np\nfrom PIL import Image\nimport tensorflow as tf  # pylint: disable=import-error\nfrom google.cloud import storage  # pylint: disable=import-error,no-name-in-module\n\nif TYPE_CHECKING:\n    from flask import Request  # pylint: disable=no-name-in-module\n\n# Download model configuration\nDOWNLOAD_CONFIG = {\n    ""bucket_name"": ""euroscipy-2019-workshop"",  # name of the bucket\n    ""model_id"": ""dcgan-weights"",  # name of the model\n    ""destination_folder"": tempfile.gettempdir(),  # tmp directory\n}\n\n# Header definitions needed for the response (Needed for CORS)\n# CORS: https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS)\nheaders = {\n    ""Access-Control-Allow-Headers"": ""Content-Type"",\n    ""Access-Control-Allow-Origin"": ""*"",\n}\n\n# Model variables\n\n# Dimension of the latent space of the model\nLATENT_DIMENSION = 100\n\n# We keep model as global variable so we don\'t have to reload it in case of warm invocations\nMODEL: tf.keras.Model = None\n\n\ndef get_model() -> tf.keras.Model:\n    """"""Returns the Generator""""""\n    G = tf.keras.Sequential(\n        [\n            tf.keras.layers.Dense(\n                1024 * 4 * 4, use_bias=False, input_shape=(LATENT_DIMENSION,)\n            ),\n            tf.keras.layers.Reshape((4, 4, 1024)),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.LeakyReLU(),\n            tf.keras.layers.Conv2DTranspose(\n                256, (5, 5), strides=(2, 2), padding=""same"", use_bias=False\n            ),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.LeakyReLU(),\n            tf.keras.layers.Conv2DTranspose(\n                128, (5, 5), strides=(2, 2), padding=""same"", use_bias=False\n            ),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.LeakyReLU(),\n            tf.keras.layers.Conv2DTranspose(\n                64, (5, 5), strides=(2, 2), padding=""same"", use_bias=False\n            ),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.LeakyReLU(),\n            tf.keras.layers.Conv2DTranspose(\n                3, (5, 5), strides=(2, 2), padding=""same"", use_bias=False\n            ),\n            tf.keras.layers.Activation(tf.math.tanh),\n        ]\n    )\n    return G\n\n\ndef inspect_bucket(\n    model_id: str, bucket_name: str = ""zuru-ml-models"", pretty_log=True\n) -> List[storage.Blob]:\n    """"""\n    Inspect the content of a bucket retrieving the blob of the desired models.\n\n    Args:\n        bucket_name (str): Name of the bucket holding the model - Default to ""zuru-ml-models"".\n        model_id (str): Name of the model: es. ""semantikaiser"".\n        pretty_log (bool): If True pretty print logs\n\n    Returns:\n        :py:obj:`list` of [:py:class:`google.cloud.storage.Blob`]: Blobs belonging to the target model.\n\n    """"""\n    storage_client = storage.Client()\n    blobs = storage_client.list_blobs(bucket_name, prefix=model_id)\n    if pretty_log:\n        from pprint import pprint\n\n        pprint([b.name for b in blobs])\n        blobs = storage_client.list_blobs(bucket_name, prefix=model_id)\n    return blobs\n\n\ndef download_blobs(model_id: str, destination_folder: str, bucket_name: str) -> str:\n    """"""\n    Download all the models related blobs from the bucket.\n\n    Args:\n        model_id (str): ID of the model we want to download AKA the name of the folder on\n            GCS. This is used as a prefix filtering out all unneeded blobs.\n        destination_folder (str): Path of the destination folder where blobs will be downloaded.\n        bucket_name (str): Name of the bucket to use as target storage.\n\n    Returns:\n        :py:obj:`list` of [:py:obj:`str`]: Returns a list containing the path of the stored SavedModel(s).\n    """"""\n    weights_uri: str = """"\n\n    prefix = model_id\n\n    # Instantiate a google storage client\n    storage_client = storage.Client()\n\n    # get the bucket we are interested in\n    bucket = storage_client.get_bucket(bucket_name)\n\n    # List blobs iterate in folder\n    blobs = bucket.list_blobs(prefix=prefix)  # Excluding folder inside bucket\n    for blob in blobs:\n        print(""Downloading "", blob.name)\n        blob_name = blob.name.split(os.path.sep)[-1]\n        if blob_name != """":\n            destination_uri = os.path.join(destination_folder, blob_name)\n            # We are downloading the blob of a file\n            try:\n                blob.download_to_filename(destination_uri)\n                if blob.name.endswith("".index""):\n                    weights_uri = destination_uri\n            except IsADirectoryError:\n                # We cannot download the blob of a folder\n                continue\n\n    if weights_uri == """":\n        raise ValueError(f""No index file found in {bucket_name}/{model_id}"")\n\n    # return the uri of the weights, needed for loading weights\n    return weights_uri\n\n\ndef postprocess(output: np.ndarray) -> str:\n    """"""\n    Post process the model output\n    Args:\n        output: output image to process\n\n    Returns:\n        The image encoded as str\n    """"""\n    pil_img = Image.fromarray(output)\n    buff = BytesIO()\n    image_format = ""PNG""\n    pil_img.save(buff, format=image_format)\n    encoded_img = base64.b64encode(buff.getvalue()).decode(""utf-8"")\n    return encoded_img\n\n\ndef handler(request: Request = None):\n    """"""\n    Entry point of the Serveless call.\n\n    Args:\n        request (:py:class:`flask.Request`): Flask Request holding our payload.\n\n    """"""\n    # Model load which only happens during cold starts\n    global MODEL\n    if not MODEL:\n        # The model is not defined, we need to instantiate it and download its weights\n        print(""Cold start: Loading model"")\n        print(""Downloading saved models"")\n        weights_path = download_blobs(**DOWNLOAD_CONFIG)\n\n        # instantiate the model\n        MODEL = get_model()\n\n        # load weights\n        MODEL.load_weights(weights_path.replace("".index"", """"))\n\n        print(""weights loaded"")\n    else:\n        # the model is already defined, we are in warm start phase\n        print(""Warm start: Using cached model"")\n\n    # log the request\n    print(""Received"", request)\n\n    # get the request payload\n    request = request.get_json(silent=True)\n\n    # get the noise vector if present\n    try:\n\n        # use the fed noise\n        noise = tf.constant(np.array(request[""noise_vector""]))\n\n        # check the correct dimensions\n        if noise.shape != (1, LATENT_DIMENSION):\n            return ({}, 400, headers)\n        print(""Using fed noise"")\n\n    except Exception:\n        # the noise vector is not present, we need to generate a new vector\n        print(""No noise vector provided, generating noise"")\n        noise = tf.random.normal((1, LATENT_DIMENSION))\n\n    # call the model\n    output = MODEL.call(noise).numpy().squeeze()\n\n    # back in the rage [0, 255]\n    output = ((output + 1) * 127.5).astype(np.uint8)\n\n    # postprocess stage (encode)\n    encoded_image = postprocess(output)\n\n    # compose the output response\n    return (\n        {\n            ""base64_image"": encoded_image,\n            ""format"": ""png"",\n            ""noise_vector"": noise.numpy().tolist(),\n        },\n        200,\n        headers,\n    )\n'"
4. Production/deploy/tests.py,0,"b'import base64\nfrom unittest.mock import Mock\nimport numpy as np\nimport cv2\n\nfrom main import handler\n\n\ndef test_main():\n    """"""\n    Test the handler function.\n    Assert the return value is 200\n    and the response contains the noise vector and the image\n    """"""\n\n    # mock the request\n    req = Mock()\n    req.get_json = lambda silent: {}\n\n    # call the handler\n    out = handler(req)\n\n    # assert the response is correctly formed\n    assert out[1] == 200\n    assert out[0][""noise_vector""]\n    assert out[0][""base64_image""]\n\n    # try decoding the image\n    # decode image bytes\n    image_bytes = base64.b64decode(out[0][""base64_image""])\n\n    # decode bytes as rgb image\n    decoded = cv2.imdecode(np.frombuffer(image_bytes, np.uint8), -1)\n\n    # assert the image has the correct shape\n    assert decoded.shape == (64, 64, 3)\n\n\ndef test_with_noise():\n    """"""\n    Test the handler function.\n    Assert the return value is 200\n    and the response contains the noise vector and the image\n    """"""\n\n    # generate random noise vector\n    noise = np.random.normal(size=(1, 100))\n\n    # mock the request with the noise vector\n    req = Mock()\n    req.get_json = lambda silent: {""noise_vector"": noise.tolist()}\n\n    # call the handler passing the request\n    out = handler(req)\n\n    # assert the response is correctly formed\n    assert out[1] == 200\n\n    # assert the latent vector is the same\n    np.testing.assert_almost_equal(np.array(out[0][""noise_vector""]), noise)\n    assert out[0][""base64_image""]\n\n    # try decoding the image\n    # decode image bytes\n    image_bytes = base64.b64decode(out[0][""base64_image""])\n\n    # decode bytes as rgb image\n    decoded = cv2.imdecode(np.frombuffer(image_bytes, np.uint8), -1)\n\n    # assert the image has the correct shape\n    assert decoded.shape == (64, 64, 3)\n\n\ndef test_noise_error():\n    # the latent space is 200 instead of 100\n    noise = np.random.normal(size=(1, 200))\n\n    # mock the request with the wrong noise vector\n    req = Mock()\n    req.get_json = lambda silent: {""noise_vector"": noise.tolist()}\n\n    # call the handler passing the request\n    out = handler(req)\n\n    # the response should be a user error\n    assert out[1] == 400\n'"
