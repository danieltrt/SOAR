file_path,api_count,code
setup.py,0,"b""# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : setup.py\n# Description  : Setup script\n# ******************************************************\n\nimport os\nimport configparser\nfrom setuptools import setup, find_packages\n\n\nVERSION = '1.0.0'\n\nsetup(\n    name='SpeechValley',\n    version=VERSION,\n    description='Speech Processing including ASR and TTS Powered by Artificial Intelligence',\n    author='zzw922cn',\n    author_email='zzw922cn@gmail.com',\n    packages=find_packages(),\n    python_requires='>=3.5',\n    install_requires=[\n        'numpy',\n        'scipy',\n        'leven',\n        'sklearn'\n    ]\n)\n\n"""
speechvalley/__init__.py,0,b''
speechvalley/feature/__init__.py,0,b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : __init__.py\n# Description  : Feature preprocessing library for Automatic Speech Recognition\n# ******************************************************\n\nfrom speechvalley.feature.libri import wav2feature as libri_wav2feature\nfrom speechvalley.feature.timit import wav2feature as timit_wav2feature\nfrom speechvalley.feature.wsj import wav2feature as wsj_wav2feature\n'
speechvalley/lm/__init__.py,0,b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : __init__.py\n# Description  : Spelling Checker\n# ******************************************************\n\n'
speechvalley/main/__init__.py,0,b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : __init__.py\n# Description  : Libraries for training models for Automatic Speech Recognition\n# ******************************************************\n\n'
speechvalley/main/libri_train.py,2,"b""#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# File              : libri_train.py\n# Author            : zewangzhang <zzw922cn@gmail.com>\n# Date              : 17.10.2019\n# Last Modified Date: 17.10.2019\n# Last Modified By  : zewangzhang <zzw922cn@gmail.com>\n# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : libri_train.py\n# Description  : Training models on LibriSpeech dataset for Automatic Speech Recognition\n# ******************************************************\n\nimport time\nimport datetime\nimport os\nfrom six.moves import cPickle\nfrom functools import wraps\nimport random\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops import ctc_ops as ctc\n\nfrom speechvalley.utils import load_batched_data, describe, describe, getAttrs, output_to_sequence, list_dirs, logging, count_params, target2phoneme, get_edit_distance, get_num_classes, check_path_exists, dotdict, activation_functions_dict, optimizer_functions_dict\nfrom speechvalley.models import DBiRNN, DeepSpeech2\n\n\nfrom tensorflow.python.platform import flags\nfrom tensorflow.python.platform import app\n\nflags.DEFINE_string('task', 'libri', 'set task name of this program')\nflags.DEFINE_string('train_dataset', 'train-clean-100', 'set the training dataset')\nflags.DEFINE_string('dev_dataset', 'dev-clean', 'set the development dataset')\nflags.DEFINE_string('test_dataset', 'test-clean', 'set the test dataset')\n\nflags.DEFINE_string('mode', 'train', 'set whether to train, dev or test')\n\nflags.DEFINE_boolean('keep', False, 'set whether to restore a model, when test mode, keep should be set to True')\nflags.DEFINE_string('level', 'cha', 'set the task level, phn, cha, or seq2seq, seq2seq will be supported soon')\nflags.DEFINE_string('model', 'DBiRNN', 'set the model to use, DBiRNN, BiRNN, ResNet..')\nflags.DEFINE_string('rnncell', 'lstm', 'set the rnncell to use, rnn, gru, lstm...')\nflags.DEFINE_integer('num_layer', 2, 'set the layers for rnn')\nflags.DEFINE_string('activation', 'tanh', 'set the activation to use, sigmoid, tanh, relu, elu...')\nflags.DEFINE_string('optimizer', 'adam', 'set the optimizer to use, sgd, adam...')\nflags.DEFINE_boolean('layerNormalization', False, 'set whether to apply layer normalization to rnn cell')\n\nflags.DEFINE_integer('batch_size', 64, 'set the batch size')\nflags.DEFINE_integer('num_hidden', 256, 'set the hidden size of rnn cell')\nflags.DEFINE_integer('num_feature', 60, 'set the size of input feature')\nflags.DEFINE_integer('num_classes', 30, 'set the number of output classes')\nflags.DEFINE_integer('num_epochs', 1, 'set the number of epochs')\nflags.DEFINE_float('lr', 0.0001, 'set the learning rate')\nflags.DEFINE_float('dropout_prob', 0.1, 'set probability of dropout')\nflags.DEFINE_float('grad_clip', 1, 'set the threshold of gradient clipping, -1 denotes no clipping')\nflags.DEFINE_string('datadir', '/home/pony/github/data/libri', 'set the data root directory')\nflags.DEFINE_string('logdir', '/home/pony/github/log/libri', 'set the log directory')\n\n\nFLAGS = flags.FLAGS\ntask = FLAGS.task\n\ntrain_dataset = FLAGS.train_dataset\ndev_dataset = FLAGS.dev_dataset\ntest_dataset = FLAGS.test_dataset\n\nlevel = FLAGS.level\nmodel_fn = DBiRNN\nrnncell = FLAGS.rnncell\nnum_layer = FLAGS.num_layer\n\nactivation_fn = activation_functions_dict[FLAGS.activation]\noptimizer_fn = optimizer_functions_dict[FLAGS.optimizer]\n\nbatch_size = FLAGS.batch_size\nnum_hidden = FLAGS.num_hidden\nnum_feature = FLAGS.num_feature\nnum_classes = get_num_classes(level)\nnum_epochs = FLAGS.num_epochs\nlr = FLAGS.lr\ngrad_clip = FLAGS.grad_clip\ndatadir = FLAGS.datadir\n\nlogdir = FLAGS.logdir\nsavedir = os.path.join(logdir, level, 'save')\nresultdir = os.path.join(logdir, level, 'result')\nloggingdir = os.path.join(logdir, level, 'logging')\ncheck_path_exists([logdir, savedir, resultdir, loggingdir])\n\nmode = FLAGS.mode\nkeep = FLAGS.keep\nkeep_prob = 1-FLAGS.dropout_prob\n\nprint('%s mode...'%str(mode))\nif mode == 'test' or mode == 'dev':\n  batch_size = 10\n  num_epochs = 1\n\n\ndef get_data(datadir, level, train_dataset, dev_dataset, test_dataset, mode):\n    if mode == 'train':\n        train_feature_dirs = [os.path.join(os.path.join(datadir, level, train_dataset),\n            i, 'feature') for i in os.listdir(os.path.join(datadir, level, train_dataset))]\n\n        train_label_dirs = [os.path.join(os.path.join(datadir, level, train_dataset),\n            i, 'label') for i in os.listdir(os.path.join(datadir, level, train_dataset))]\n        return train_feature_dirs, train_label_dirs\n\n    if mode == 'dev':\n        dev_feature_dirs = [os.path.join(os.path.join(datadir, level, dev_dataset),\n            i, 'feature') for i in os.listdir(os.path.join(datadir, level, dev_dataset))]\n\n        dev_label_dirs = [os.path.join(os.path.join(datadir, level, dev_dataset),\n            i, 'label') for i in os.listdir(os.path.join(datadir, level, dev_dataset))]\n        return dev_feature_dirs, dev_label_dirs\n\n    if mode == 'test':\n        test_feature_dirs = [os.path.join(os.path.join(datadir, level, test_dataset),\n            i, 'feature') for i in os.listdir(os.path.join(datadir, level, test_dataset))]\n\n        test_label_dirs = [os.path.join(os.path.join(datadir, level, test_dataset),\n            i, 'label') for i in os.listdir(os.path.join(datadir, level, test_dataset))]\n        return test_feature_dirs, test_label_dirs\n\nlogfile = os.path.join(loggingdir, str(datetime.datetime.strftime(datetime.datetime.now(),\n    '%Y-%m-%d %H:%M:%S') + '.txt').replace(' ', '').replace('/', ''))\n\nclass Runner(object):\n\n    def _default_configs(self):\n      return {'level': level,\n              'rnncell': rnncell,\n              'batch_size': batch_size,\n              'num_hidden': num_hidden,\n              'num_feature': num_feature,\n              'num_class': num_classes,\n              'num_layer': num_layer,\n              'activation': activation_fn,\n              'optimizer': optimizer_fn,\n              'learning_rate': lr,\n              'keep_prob': keep_prob,\n              'grad_clip': grad_clip,\n            }\n\n    @describe\n    def load_data(self, feature_dir, label_dir, mode, level):\n        return load_batched_data(feature_dir, label_dir, batch_size, mode, level)\n\n\n    def run(self):\n        # load data\n        args_dict = self._default_configs()\n        args = dotdict(args_dict)\n        feature_dirs, label_dirs = get_data(datadir, level, train_dataset, dev_dataset, test_dataset, mode)\n        batchedData, maxTimeSteps, totalN = self.load_data(feature_dirs[0], label_dirs[0], mode, level)\n        model = model_fn(args, maxTimeSteps)\n\n        ## shuffle feature_dir and label_dir by same order\n        FL_pair = list(zip(feature_dirs, label_dirs))\n        random.shuffle(FL_pair)\n        feature_dirs, label_dirs = zip(*FL_pair)\n\n        for feature_dir, label_dir in zip(feature_dirs, label_dirs):\n            id_dir = feature_dirs.index(feature_dir)\n            print('dir id:{}'.format(id_dir))\n            batchedData, maxTimeSteps, totalN = self.load_data(feature_dir, label_dir, mode, level)\n            model = model_fn(args, maxTimeSteps)\n            num_params = count_params(model, mode='trainable')\n            all_num_params = count_params(model, mode='all')\n            model.config['trainable params'] = num_params\n            model.config['all params'] = all_num_params\n            print(model.config)\n            with tf.Session(graph=model.graph) as sess:\n                # restore from stored model\n                if keep == True:\n                    ckpt = tf.train.get_checkpoint_state(savedir)\n                    if ckpt and ckpt.model_checkpoint_path:\n                        model.saver.restore(sess, ckpt.model_checkpoint_path)\n                        print('Model restored from:' + savedir)\n                else:\n                    print('Initializing')\n                    sess.run(model.initial_op)\n\n                for epoch in range(num_epochs):\n                    ## training\n                    start = time.time()\n                    if mode == 'train':\n                        print('Epoch {} ...'.format(epoch + 1))\n\n                    batchErrors = np.zeros(len(batchedData))\n                    batchRandIxs = np.random.permutation(len(batchedData))\n\n                    for batch, batchOrigI in enumerate(batchRandIxs):\n                        batchInputs, batchTargetSparse, batchSeqLengths = batchedData[batchOrigI]\n                        batchTargetIxs, batchTargetVals, batchTargetShape = batchTargetSparse\n                        feedDict = {model.inputX: batchInputs, model.targetIxs: batchTargetIxs,\n                                    model.targetVals: batchTargetVals, model.targetShape: batchTargetShape,\n                                    model.seqLengths: batchSeqLengths}\n\n                        if level == 'cha':\n                            if mode == 'train':\n                                _, l, pre, y, er = sess.run([model.optimizer, model.loss,\n                                    model.predictions, model.targetY, model.errorRate],\n                                    feed_dict=feedDict)\n\n                                batchErrors[batch] = er\n                                print('\\n{} mode, total:{},subdir:{}/{},batch:{}/{},epoch:{}/{},train loss={:.3f},mean train CER={:.3f}\\n'.format(\n                                    level, totalN, id_dir+1, len(feature_dirs), batch+1, len(batchRandIxs), epoch+1, num_epochs, l, er/batch_size))\n\n                            elif mode == 'dev':\n                                l, pre, y, er = sess.run([model.loss, model.predictions,\n                                    model.targetY, model.errorRate], feed_dict=feedDict)\n                                batchErrors[batch] = er\n                                print('\\n{} mode, total:{},subdir:{}/{},batch:{}/{},dev loss={:.3f},mean dev CER={:.3f}\\n'.format(\n                                    level, totalN, id_dir+1, len(feature_dirs), batch+1, len(batchRandIxs), l, er/batch_size))\n\n                            elif mode == 'test':\n                                l, pre, y, er = sess.run([model.loss, model.predictions,\n                                    model.targetY, model.errorRate], feed_dict=feedDict)\n                                batchErrors[batch] = er\n                                print('\\n{} mode, total:{},subdir:{}/{},batch:{}/{},test loss={:.3f},mean test CER={:.3f}\\n'.format(\n                                    level, totalN, id_dir+1, len(feature_dirs), batch+1, len(batchRandIxs), l, er/batch_size))\n                        elif level=='seq2seq':\n                            raise ValueError('level %s is not supported now'%str(level))\n\n\n                        # NOTE:\n                        if er / batch_size == 1.0:\n                            break\n\n                        if batch % 20 == 0:\n                            print('Truth:\\n' + output_to_sequence(y, type=level))\n                            print('Output:\\n' + output_to_sequence(pre, type=level))\n\n\n                        if mode=='train' and ((epoch * len(batchRandIxs) + batch + 1) % 20 == 0 or (\n                            epoch == num_epochs - 1 and batch == len(batchRandIxs) - 1)):\n                            checkpoint_path = os.path.join(savedir, 'model.ckpt')\n                            model.saver.save(sess, checkpoint_path, global_step=epoch)\n                            print('Model has been saved in {}'.format(savedir))\n\n                    end = time.time()\n                    delta_time = end - start\n                    print('Epoch ' + str(epoch + 1) + ' needs time:' + str(delta_time) + ' s')\n\n                    if mode=='train':\n                        if (epoch + 1) % 1 == 0:\n                            checkpoint_path = os.path.join(savedir, 'model.ckpt')\n                            model.saver.save(sess, checkpoint_path, global_step=epoch)\n                            print('Model has been saved in {}'.format(savedir))\n                        epochER = batchErrors.sum() / totalN\n                        print('Epoch', epoch + 1, 'mean train error rate:', epochER)\n                        logging(model, logfile, epochER, epoch, delta_time, mode='config')\n                        logging(model, logfile, epochER, epoch, delta_time, mode=mode)\n\n                    if mode=='test' or mode=='dev':\n                        with open(os.path.join(resultdir, level + '_result.txt'), 'a') as result:\n                            result.write(output_to_sequence(y, type=level) + '\\n')\n                            result.write(output_to_sequence(pre, type=level) + '\\n')\n                            result.write('\\n')\n                        epochER = batchErrors.sum() / totalN\n                        print(' test error rate:', epochER)\n                        logging(model, logfile, epochER, mode=mode)\n\n\n\nif __name__ == '__main__':\n  runner = Runner()\n  runner.run()\n"""
speechvalley/main/madarian_train.py,2,"b""#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# File              : madarian_train.py\n# Author            : zewangzhang <zzw922cn@gmail.com>\n# Date              : 17.10.2019\n# Last Modified Date: 17.10.2019\n# Last Modified By  : zewangzhang <zzw922cn@gmail.com>\n# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : madarian_train.py\n# Description  : Training models on some madarian dataset(like thcs, aishell, etc) for Automatic Speech Recognition\n# ******************************************************\n\nimport time\nimport datetime\nimport os\nfrom six.moves import cPickle\nfrom functools import wraps\nimport random\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops import ctc_ops as ctc\n\nfrom speechvalley.utils import load_batched_data, describe, describe, getAttrs, output_to_sequence, list_dirs, logging, count_params, target2phoneme, get_edit_distance, get_num_classes, check_path_exists, dotdict, activation_functions_dict, optimizer_functions_dict\nfrom speechvalley.models import DBiRNN, DeepSpeech2\n\nfrom tensorflow.python.platform import flags\nfrom tensorflow.python.platform import app\n\nflags.DEFINE_string('task', 'libri', 'set task name of this program')\nflags.DEFINE_string('train_dataset', 'train-clean-100', 'set the training dataset')\nflags.DEFINE_string('dev_dataset', 'dev-clean', 'set the development dataset')\nflags.DEFINE_string('test_dataset', 'test-clean', 'set the test dataset')\n\nflags.DEFINE_string('mode', 'train', 'set whether to train, dev or test')\n\nflags.DEFINE_boolean('keep', False, 'set whether to restore a model, when test mode, keep should be set to True')\nflags.DEFINE_string('level', 'cha', 'set the task level, phn, cha, or seq2seq, seq2seq will be supported soon')\nflags.DEFINE_string('model', 'DBiRNN', 'set the model to use, DBiRNN, BiRNN, ResNet..')\nflags.DEFINE_string('rnncell', 'lstm', 'set the rnncell to use, rnn, gru, lstm...')\nflags.DEFINE_integer('num_layer', 2, 'set the layers for rnn')\nflags.DEFINE_string('activation', 'tanh', 'set the activation to use, sigmoid, tanh, relu, elu...')\nflags.DEFINE_string('optimizer', 'adam', 'set the optimizer to use, sgd, adam...')\nflags.DEFINE_boolean('layerNormalization', False, 'set whether to apply layer normalization to rnn cell')\n\nflags.DEFINE_integer('batch_size', 64, 'set the batch size')\nflags.DEFINE_integer('num_hidden', 256, 'set the hidden size of rnn cell')\nflags.DEFINE_integer('num_feature', 60, 'set the size of input feature')\nflags.DEFINE_integer('num_classes', 30, 'set the number of output classes')\nflags.DEFINE_integer('num_epochs', 1, 'set the number of epochs')\nflags.DEFINE_float('lr', 0.0001, 'set the learning rate')\nflags.DEFINE_float('dropout_prob', 0.1, 'set probability of dropout')\nflags.DEFINE_float('grad_clip', 1, 'set the threshold of gradient clipping, -1 denotes no clipping')\nflags.DEFINE_string('datadir', '/home/pony/github/data/libri', 'set the data root directory')\nflags.DEFINE_string('logdir', '/home/pony/github/log/libri', 'set the log directory')\n\n\nFLAGS = flags.FLAGS\ntask = FLAGS.task\n\ntrain_dataset = FLAGS.train_dataset\ndev_dataset = FLAGS.dev_dataset\ntest_dataset = FLAGS.test_dataset\n\nlevel = FLAGS.level\nmodel_fn = DBiRNN\nrnncell = FLAGS.rnncell\nnum_layer = FLAGS.num_layer\n\nactivation_fn = activation_functions_dict[FLAGS.activation]\noptimizer_fn = optimizer_functions_dict[FLAGS.optimizer]\n\nbatch_size = FLAGS.batch_size\nnum_hidden = FLAGS.num_hidden\nnum_feature = FLAGS.num_feature\nnum_classes = get_num_classes(level)\nnum_epochs = FLAGS.num_epochs\nlr = FLAGS.lr\ngrad_clip = FLAGS.grad_clip\ndatadir = FLAGS.datadir\n\nlogdir = FLAGS.logdir\nsavedir = os.path.join(logdir, level, 'save')\nresultdir = os.path.join(logdir, level, 'result')\nloggingdir = os.path.join(logdir, level, 'logging')\ncheck_path_exists([logdir, savedir, resultdir, loggingdir])\n\nmode = FLAGS.mode\nkeep = FLAGS.keep\nkeep_prob = 1-FLAGS.dropout_prob\n\nprint('%s mode...'%str(mode))\nif mode == 'test' or mode == 'dev':\n  batch_size = 10\n  num_epochs = 1\n\n\ndef get_data(datadir, level, train_dataset, dev_dataset, test_dataset, mode):\n    if mode == 'train':\n        train_feature_dirs = [os.path.join(os.path.join(datadir, level, train_dataset),\n            i, 'feature') for i in os.listdir(os.path.join(datadir, level, train_dataset))]\n\n        train_label_dirs = [os.path.join(os.path.join(datadir, level, train_dataset),\n            i, 'label') for i in os.listdir(os.path.join(datadir, level, train_dataset))]\n        return train_feature_dirs, train_label_dirs\n\n    if mode == 'dev':\n        dev_feature_dirs = [os.path.join(os.path.join(datadir, level, dev_dataset),\n            i, 'feature') for i in os.listdir(os.path.join(datadir, level, dev_dataset))]\n\n        dev_label_dirs = [os.path.join(os.path.join(datadir, level, dev_dataset),\n            i, 'label') for i in os.listdir(os.path.join(datadir, level, dev_dataset))]\n        return dev_feature_dirs, dev_label_dirs\n\n    if mode == 'test':\n        test_feature_dirs = [os.path.join(os.path.join(datadir, level, test_dataset),\n            i, 'feature') for i in os.listdir(os.path.join(datadir, level, test_dataset))]\n\n        test_label_dirs = [os.path.join(os.path.join(datadir, level, test_dataset),\n            i, 'label') for i in os.listdir(os.path.join(datadir, level, test_dataset))]\n        return test_feature_dirs, test_label_dirs\n\nlogfile = os.path.join(loggingdir, str(datetime.datetime.strftime(datetime.datetime.now(),\n    '%Y-%m-%d %H:%M:%S') + '.txt').replace(' ', '').replace('/', ''))\n\nclass Runner(object):\n\n    def _default_configs(self):\n      return {'level': level,\n              'rnncell': rnncell,\n              'batch_size': batch_size,\n              'num_hidden': num_hidden,\n              'num_feature': num_feature,\n              'num_class': num_classes,\n              'num_layer': num_layer,\n              'activation': activation_fn,\n              'optimizer': optimizer_fn,\n              'learning_rate': lr,\n              'keep_prob': keep_prob,\n              'grad_clip': grad_clip,\n            }\n\n    @describe\n    def load_data(self, feature_dir, label_dir, mode, level):\n        return load_batched_data(feature_dir, label_dir, batch_size, mode, level)\n\n\n    def run(self):\n        # load data\n        args_dict = self._default_configs()\n        args = dotdict(args_dict)\n        feature_dirs, label_dirs = get_data(datadir, level, train_dataset, dev_dataset, test_dataset, mode)\n        batchedData, maxTimeSteps, totalN = self.load_data(feature_dirs[0], label_dirs[0], mode, level)\n        model = model_fn(args, maxTimeSteps)\n\n        ## shuffle feature_dir and label_dir by same order\n        FL_pair = list(zip(feature_dirs, label_dirs))\n        random.shuffle(FL_pair)\n        feature_dirs, label_dirs = zip(*FL_pair)\n\n        for feature_dir, label_dir in zip(feature_dirs, label_dirs):\n            id_dir = feature_dirs.index(feature_dir)\n            print('dir id:{}'.format(id_dir))\n            batchedData, maxTimeSteps, totalN = self.load_data(feature_dir, label_dir, mode, level)\n            model = model_fn(args, maxTimeSteps)\n            num_params = count_params(model, mode='trainable')\n            all_num_params = count_params(model, mode='all')\n            model.config['trainable params'] = num_params\n            model.config['all params'] = all_num_params\n            print(model.config)\n            with tf.Session(graph=model.graph) as sess:\n                # restore from stored model\n                if keep == True:\n                    ckpt = tf.train.get_checkpoint_state(savedir)\n                    if ckpt and ckpt.model_checkpoint_path:\n                        model.saver.restore(sess, ckpt.model_checkpoint_path)\n                        print('Model restored from:' + savedir)\n                else:\n                    print('Initializing')\n                    sess.run(model.initial_op)\n\n                for epoch in range(num_epochs):\n                    ## training\n                    start = time.time()\n                    if mode == 'train':\n                        print('Epoch {} ...'.format(epoch + 1))\n\n                    batchErrors = np.zeros(len(batchedData))\n                    batchRandIxs = np.random.permutation(len(batchedData))\n\n                    for batch, batchOrigI in enumerate(batchRandIxs):\n                        batchInputs, batchTargetSparse, batchSeqLengths = batchedData[batchOrigI]\n                        batchTargetIxs, batchTargetVals, batchTargetShape = batchTargetSparse\n                        feedDict = {model.inputX: batchInputs, model.targetIxs: batchTargetIxs,\n                                    model.targetVals: batchTargetVals, model.targetShape: batchTargetShape,\n                                    model.seqLengths: batchSeqLengths}\n\n                        if level == 'cha':\n                            if mode == 'train':\n                                _, l, pre, y, er = sess.run([model.optimizer, model.loss,\n                                    model.predictions, model.targetY, model.errorRate],\n                                    feed_dict=feedDict)\n\n                                batchErrors[batch] = er\n                                print('\\n{} mode, total:{},subdir:{}/{},batch:{}/{},epoch:{}/{},train loss={:.3f},mean train CER={:.3f}\\n'.format(\n                                    level, totalN, id_dir+1, len(feature_dirs), batch+1, len(batchRandIxs), epoch+1, num_epochs, l, er/batch_size))\n\n                            elif mode == 'dev':\n                                l, pre, y, er = sess.run([model.loss, model.predictions,\n                                    model.targetY, model.errorRate], feed_dict=feedDict)\n                                batchErrors[batch] = er\n                                print('\\n{} mode, total:{},subdir:{}/{},batch:{}/{},dev loss={:.3f},mean dev CER={:.3f}\\n'.format(\n                                    level, totalN, id_dir+1, len(feature_dirs), batch+1, len(batchRandIxs), l, er/batch_size))\n\n                            elif mode == 'test':\n                                l, pre, y, er = sess.run([model.loss, model.predictions,\n                                    model.targetY, model.errorRate], feed_dict=feedDict)\n                                batchErrors[batch] = er\n                                print('\\n{} mode, total:{},subdir:{}/{},batch:{}/{},test loss={:.3f},mean test CER={:.3f}\\n'.format(\n                                    level, totalN, id_dir+1, len(feature_dirs), batch+1, len(batchRandIxs), l, er/batch_size))\n                        elif level=='seq2seq':\n                            raise ValueError('level %s is not supported now'%str(level))\n\n\n                        # NOTE:\n                        if er / batch_size == 1.0:\n                            break\n\n                        if batch % 20 == 0:\n                            print('Truth:\\n' + output_to_sequence(y, type=level))\n                            print('Output:\\n' + output_to_sequence(pre, type=level))\n\n\n                        if mode=='train' and ((epoch * len(batchRandIxs) + batch + 1) % 20 == 0 or (\n                            epoch == num_epochs - 1 and batch == len(batchRandIxs) - 1)):\n                            checkpoint_path = os.path.join(savedir, 'model.ckpt')\n                            model.saver.save(sess, checkpoint_path, global_step=epoch)\n                            print('Model has been saved in {}'.format(savedir))\n\n                    end = time.time()\n                    delta_time = end - start\n                    print('Epoch ' + str(epoch + 1) + ' needs time:' + str(delta_time) + ' s')\n\n                    if mode=='train':\n                        if (epoch + 1) % 1 == 0:\n                            checkpoint_path = os.path.join(savedir, 'model.ckpt')\n                            model.saver.save(sess, checkpoint_path, global_step=epoch)\n                            print('Model has been saved in {}'.format(savedir))\n                        epochER = batchErrors.sum() / totalN\n                        print('Epoch', epoch + 1, 'mean train error rate:', epochER)\n                        logging(model, logfile, epochER, epoch, delta_time, mode='config')\n                        logging(model, logfile, epochER, epoch, delta_time, mode=mode)\n\n                    if mode=='test' or mode=='dev':\n                        with open(os.path.join(resultdir, level + '_result.txt'), 'a') as result:\n                            result.write(output_to_sequence(y, type=level) + '\\n')\n                            result.write(output_to_sequence(pre, type=level) + '\\n')\n                            result.write('\\n')\n                        epochER = batchErrors.sum() / totalN\n                        print(' test error rate:', epochER)\n                        logging(model, logfile, epochER, mode=mode)\n\n\n\nif __name__ == '__main__':\n  runner = Runner()\n  runner.run()\n"""
speechvalley/main/timit_train.py,3,"b""#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# File              : timit_train.py\n# Author            : zewangzhang <zzw922cn@gmail.com>\n# Date              : 17.10.2019\n# Last Modified Date: 17.10.2019\n# Last Modified By  : zewangzhang <zzw922cn@gmail.com>\n# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : timit_train.py\n# Description  : Training models on TIMIT dataset for Automatic Speech Recognition\n# ******************************************************\n\nimport time\nimport datetime\nimport os\nfrom six.moves import cPickle\nfrom functools import wraps\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops import ctc_ops as ctc\n\nfrom speechvalley.utils import describe, describe, getAttrs, output_to_sequence, load_batched_data, list_dirs, logging, count_params, target2phoneme, get_edit_distance, get_num_classes, check_path_exists, dotdict, activation_functions_dict, optimizer_functions_dict\nfrom speechvalley.models import DBiRNN, DeepSpeech2, CapsuleNetwork\n\nfrom tensorflow.python.platform import flags\nfrom tensorflow.python.platform import app\n\nflags.DEFINE_string('task', 'timit', 'set task name of this program')\nflags.DEFINE_string('mode', 'train', 'set whether to train or test')\nflags.DEFINE_boolean('keep', False, 'set whether to restore a model, when test mode, keep should be set to True')\nflags.DEFINE_string('level', 'phn', 'set the task level, phn, cha, or seq2seq, seq2seq will be supported soon')\nflags.DEFINE_string('model', 'CapsuleNetwork', 'set the model to use, DBiRNN, BiRNN, ResNet..')\nflags.DEFINE_string('rnncell', 'lstm', 'set the rnncell to use, rnn, gru, lstm...')\nflags.DEFINE_integer('num_layer', 2, 'set the layers for rnn')\nflags.DEFINE_string('activation', 'tanh', 'set the activation to use, sigmoid, tanh, relu, elu...')\nflags.DEFINE_string('optimizer', 'adam', 'set the optimizer to use, sgd, adam...')\nflags.DEFINE_boolean('layerNormalization', False, 'set whether to apply layer normalization to rnn cell')\n\nflags.DEFINE_integer('batch_size', 16, 'set the batch size')\nflags.DEFINE_integer('num_hidden', 128, 'set the hidden size of rnn cell')\nflags.DEFINE_integer('num_feature', 39, 'set the size of input feature')\nflags.DEFINE_integer('num_classes', 30, 'set the number of output classes')\nflags.DEFINE_integer('num_epochs', 500, 'set the number of epochs')\nflags.DEFINE_integer('num_iter', 3, 'set the number of iterations in routing')\nflags.DEFINE_float('lr', 0.0001, 'set the learning rate')\nflags.DEFINE_float('dropout_prob', 0.1, 'set probability of dropout')\nflags.DEFINE_float('grad_clip', 1, 'set the threshold of gradient clipping, -1 denotes no clipping')\nflags.DEFINE_string('datadir', '/path/to/your/data/directory', 'set the data root directory')\nflags.DEFINE_string('logdir', '/path/to/your/log/directory', 'set the log directory')\n\n\nFLAGS = flags.FLAGS\n\ntask = FLAGS.task\nlevel = FLAGS.level\n# define model type\nif FLAGS.model == 'DBiRNN':\n    model_fn = DBiRNN\nelif FLAGS.model == 'DeepSpeech2':\n    model_fn = DBiRNN\nelif FLAGS.model == 'CapsuleNetwork':\n    model_fn = CapsuleNetwork\nelse:\n    model_fn = None\nrnncell = FLAGS.rnncell\nnum_layer = FLAGS.num_layer\n\nactivation_fn = activation_functions_dict[FLAGS.activation]\noptimizer_fn = optimizer_functions_dict[FLAGS.optimizer]\n\nbatch_size = FLAGS.batch_size\nnum_hidden = FLAGS.num_hidden\nnum_feature = FLAGS.num_feature\nnum_classes = get_num_classes(level)\nnum_epochs = FLAGS.num_epochs\nnum_iter = FLAGS.num_iter\nlr = FLAGS.lr\ngrad_clip = FLAGS.grad_clip\ndatadir = FLAGS.datadir\n\nlogdir = FLAGS.logdir\nsavedir = os.path.join(logdir, level, 'save')\nresultdir = os.path.join(logdir, level, 'result')\nloggingdir = os.path.join(logdir, level, 'logging')\ncheck_path_exists([logdir, savedir, resultdir, loggingdir])\n\nmode = FLAGS.mode\nkeep = FLAGS.keep\nkeep_prob = 1-FLAGS.dropout_prob\n\nprint('%s mode...'%str(mode))\nif mode == 'test':\n  batch_size = 100\n  num_epochs = 1\n\ntrain_mfcc_dir = os.path.join(datadir, level, 'train', 'mfcc')\ntrain_label_dir = os.path.join(datadir, level, 'train', 'label')\ntest_mfcc_dir = os.path.join(datadir, level, 'test', 'mfcc')\ntest_label_dir = os.path.join(datadir, level, 'test', 'label')\nlogfile = os.path.join(loggingdir, str(datetime.datetime.strftime(datetime.datetime.now(),\n    '%Y-%m-%d %H:%M:%S') + '.txt').replace(' ', '').replace('/', ''))\n\n\nclass Runner(object):\n\n    def _default_configs(self):\n      return {'level': level,\n              'rnncell': rnncell,\n              'batch_size': batch_size,\n              'num_hidden': num_hidden,\n              'num_feature': num_feature,\n              'num_classes': num_classes,\n              'num_layer': num_layer,\n              'num_iter': num_iter,\n              'activation': activation_fn,\n              'optimizer': optimizer_fn,\n              'learning_rate': lr,\n              'keep_prob': keep_prob,\n              'grad_clip': grad_clip,\n            }\n\n    @describe\n    def load_data(self, args, mode, type):\n        if mode == 'train':\n            return load_batched_data(train_mfcc_dir, train_label_dir, batch_size, mode, type)\n        elif mode == 'test':\n            return load_batched_data(test_mfcc_dir, test_label_dir, batch_size, mode, type)\n        else:\n            raise TypeError('mode should be train or test.')\n\n    def run(self):\n        # load data\n        args_dict = self._default_configs()\n        args = dotdict(args_dict)\n        batchedData, maxTimeSteps, totalN = self.load_data(args, mode=mode, type=level)\n        model = model_fn(args, maxTimeSteps)\n\n        # count the num of params\n        num_params = count_params(model, mode='trainable')\n        all_num_params = count_params(model, mode='all')\n        model.config['trainable params'] = num_params\n        model.config['all params'] = all_num_params\n        print(model.config)\n\n        #with tf.Session(graph=model.graph) as sess:\n        with tf.Session() as sess:\n            # restore from stored model\n            if keep == True:\n                ckpt = tf.train.get_checkpoint_state(savedir)\n                if ckpt and ckpt.model_checkpoint_path:\n                    model.saver.restore(sess, ckpt.model_checkpoint_path)\n                    print('Model restored from:' + savedir)\n            else:\n                print('Initializing')\n                sess.run(model.initial_op)\n\n            for epoch in range(num_epochs):\n                ## training\n                start = time.time()\n                if mode == 'train':\n                    print('Epoch', epoch + 1, '...')\n                batchErrors = np.zeros(len(batchedData))\n                batchRandIxs = np.random.permutation(len(batchedData))\n\n                for batch, batchOrigI in enumerate(batchRandIxs):\n                    batchInputs, batchTargetSparse, batchSeqLengths = batchedData[batchOrigI]\n                    batchTargetIxs, batchTargetVals, batchTargetShape = batchTargetSparse\n                    feedDict = {model.inputX: batchInputs, model.targetIxs: batchTargetIxs,\n                                model.targetVals: batchTargetVals, model.targetShape: batchTargetShape,\n                                model.seqLengths: batchSeqLengths}\n\n                    if level == 'cha':\n                        if mode == 'train':\n                            _, l, pre, y, er = sess.run([model.optimizer, model.loss,\n                                model.predictions, model.targetY, model.errorRate],\n                                feed_dict=feedDict)\n\n                            batchErrors[batch] = er\n                            print('\\n{} mode, total:{},batch:{}/{},epoch:{}/{},train loss={:.3f},mean train CER={:.3f}\\n'.format(\n                                level, totalN, batch+1, len(batchRandIxs), epoch+1, num_epochs, l, er/batch_size))\n\n                        elif mode == 'test':\n                            l, pre, y, er = sess.run([model.loss, model.predictions,\n                                model.targetY, model.errorRate], feed_dict=feedDict)\n                            batchErrors[batch] = er\n                            print('\\n{} mode, total:{},batch:{}/{},test loss={:.3f},mean test CER={:.3f}\\n'.format(\n                                level, totalN, batch+1, len(batchRandIxs), l, er/batch_size))\n\n                    elif level == 'phn':\n                        if mode == 'train':\n                            _, l, pre, y = sess.run([model.optimizer, model.loss,\n                                model.predictions, model.targetY],\n                                feed_dict=feedDict)\n\n                            er = get_edit_distance([pre.values], [y.values], True, level)\n                            print('\\n{} mode, total:{},batch:{}/{},epoch:{}/{},train loss={:.3f},mean train PER={:.3f}\\n'.format(\n                                level, totalN, batch+1, len(batchRandIxs), epoch+1, num_epochs, l, er))\n                            batchErrors[batch] = er * len(batchSeqLengths)\n                        elif mode == 'test':\n                            l, pre, y = sess.run([model.loss, model.predictions, model.targetY], feed_dict=feedDict)\n                            er = get_edit_distance([pre.values], [y.values], True, level)\n                            print('\\n{} mode, total:{},batch:{}/{},test loss={:.3f},mean test PER={:.3f}\\n'.format(\n                                level, totalN, batch+1, len(batchRandIxs), l, er))\n                            batchErrors[batch] = er * len(batchSeqLengths)\n\n                    # NOTE:\n                    if er / batch_size == 1.0:\n                        break\n\n                    if batch % 30 == 0:\n                        print('Truth:\\n' + output_to_sequence(y, type=level))\n                        print('Output:\\n' + output_to_sequence(pre, type=level))\n\n\n                    if mode=='train' and ((epoch * len(batchRandIxs) + batch + 1) % 20 == 0 or (\n                           epoch == num_epochs - 1 and batch == len(batchRandIxs) - 1)):\n                        checkpoint_path = os.path.join(savedir, 'model.ckpt')\n                        model.saver.save(sess, checkpoint_path, global_step=epoch)\n                        print('Model has been saved in {}'.format(savedir))\n                end = time.time()\n                delta_time = end - start\n                print('Epoch ' + str(epoch + 1) + ' needs time:' + str(delta_time) + ' s')\n\n                if mode=='train':\n                    if (epoch + 1) % 1 == 0:\n                        checkpoint_path = os.path.join(savedir, 'model.ckpt')\n                        model.saver.save(sess, checkpoint_path, global_step=epoch)\n                        print('Model has been saved in {}'.format(savedir))\n                    epochER = batchErrors.sum() / totalN\n                    print('Epoch', epoch + 1, 'mean train error rate:', epochER)\n                    logging(model, logfile, epochER, epoch, delta_time, mode='config')\n                    logging(model, logfile, epochER, epoch, delta_time, mode=mode)\n\n\n                if mode=='test':\n                    with open(os.path.join(resultdir, level + '_result.txt'), 'a') as result:\n                        result.write(output_to_sequence(y, type=level) + '\\n')\n                        result.write(output_to_sequence(pre, type=level) + '\\n')\n                        result.write('\\n')\n                    epochER = batchErrors.sum() / totalN\n                    print(' test error rate:', epochER)\n                    logging(model, logfile, epochER, mode=mode)\n\n\n\nif __name__ == '__main__':\n  runner = Runner()\n  runner.run()\n"""
speechvalley/models/__init__.py,0,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : __init__.py\n# Description  : libraries of models\n# ******************************************************\n\nfrom speechvalley.models.dynamic_brnn import DBiRNN \nfrom speechvalley.models.deepSpeech2 import DeepSpeech2\nfrom speechvalley.models.capsuleNetwork import CapsuleLayer, CapsuleNetwork\n'"
speechvalley/models/capsuleNetwork.py,50,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# File              : capsuleNetwork.py\n# Author            : zewangzhang <zzw922cn@gmail.com>\n# Date              : 17.10.2019\n# Last Modified Date: 17.10.2019\n# Last Modified By  : zewangzhang <zzw922cn@gmail.com>\n# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : CapsuleNetwork.py\n# Description  : Capsule network for Automatic Speech Recognition\n# ******************************************************\n\nimport sys\nimport tensorflow as tf\nimport os\nimport numpy as np\n\n\ndef squashing(s):\n    """""" Squashing function for normalization\n    size of s: [batch_size, 1, next_channels*next_num_capsules, next_output_vector_len, 1]\n    size of v: [batch_size, 1, next_channels*next_num_capsules, next_output_vector_len, 1]\n    """"""\n    assert s.dtype == tf.float32\n    squared_s = tf.reduce_sum(tf.square(s), axis=2, keep_dims=True)\n    normed_s = tf.norm(s, axis=2, keep_dims=True)\n    v = squared_s / (1+squared_s) / normed_s * s\n    assert v.get_shape()==s.get_shape()\n    return v\n\ndef routing(u, next_num_channels, next_num_capsules, next_output_vector_len, num_iter, scope=None):\n    """""" Routing algorithm for capsules of two adjacent layers\n    size of u: [batch_size, channels, num_capsules, output_vector_len]\n    size of w: [batch_size, channels, num_capsules, next_channels, next_num_capsules, vec_len, next_vec_len]\n    """"""\n    scope = scope or ""routing""\n    shape = u.get_shape()\n    u = tf.reshape(u, [shape[0], shape[1], shape[2], 1, 1, shape[3], 1])\n    u_ij = tf.tile(u, [1, 1, 1, next_num_channels, next_num_capsules, 1, 1])\n    with tf.variable_scope(scope):\n        w_shape = [1, shape[1], shape[2], next_num_channels, next_num_capsules, shape[3], next_output_vector_len]\n        w = tf.get_variable(""w"", shape=w_shape, dtype=tf.float32)\n        w = tf.tile(w, [shape[0], 1, 1, 1, 1, 1, 1])\n        u_hat = tf.matmul(w, u_ij, transpose_a=True)\n        # size of u_hat: [batch_size, channels*num_capsules, next_channels*next_num_capsules, next_vec_len, 1]\n        u_hat = tf.reshape(u_hat, [shape[0], shape[1]*shape[2], -1, next_output_vector_len, 1])\n        u_hat_without_backprop = tf.stop_gradient(u_hat, ""u_hat_without_backprop"")\n        b_ij = tf.constant(np.zeros([shape[0], shape[1]*shape[2], next_num_channels*next_num_capsules, 1, 1]), dtype=tf.float32)\n        c_ij = tf.nn.softmax(b_ij, dim=2)\n        for r in range(num_iter):\n            if r != num_iter-1:\n                # size of s_j: [batch_size, 1, next_channels*next_num_capsules, next_output_vector_len, 1]\n                s_j = tf.reduce_sum(tf.multiply(c_ij, u_hat_without_backprop), axis=1, keep_dims=True)\n                v_j = squashing(s_j)\n                v_j =tf.tile(v_j, [1, shape[1]*shape[2], 1, 1, 1])\n                # b_ij += u_hat * v_j\n                b_ij = b_ij + tf.matmul(u_hat, v_j, transpose_a=True)\n            else:\n                s_j = tf.reduce_sum(tf.multiply(c_ij, u_hat), axis=1, keep_dims=True)\n                v_j = squashing(s_j)\n    # size of v_j: [batch_size, 1, next_channels*next_num_capsules, next_output_vector_len, 1]\n    return v_j\n\n\n\nclass CapsuleLayer(object):\n    """""" Capsule layer based on convolutional neural network\n    """"""\n    def __init__(self, num_capsules, num_channels, output_vector_len, layer_type=\'conv\', vars_scope=None):\n        self._num_capsules = num_capsules\n        self._num_channels = num_channels\n        self._output_vector_len = output_vector_len\n        self._layer_type = layer_type\n        self._vars_scope = vars_scope or ""capsule_layer""\n\n    @property\n    def num_capsules(self):\n        return self._num_capsules\n\n    @property\n    def output_vector_len(self):\n        return self._output_vector_len\n\n    def __call__(self, inputX, kernel_size, strides, num_iter, with_routing=True, padding=\'VALID\'):\n        input_shape = inputX.get_shape()\n        with tf.variable_scope(self._vars_scope) as scope:\n            if self._layer_type==\'conv\':\n                # shape of conv1:  [batch, height, width, channels]\n                kernel = tf.get_variable(""conv_kernel"", shape=[kernel_size[0], kernel_size[1], input_shape[-1],\n                       self._num_channels*self._num_capsules*self._output_vector_len], dtype=tf.float32)\n                conv_output = tf.nn.conv2d(inputX, kernel, strides, padding)\n                shape1 = conv_output.get_shape()\n                capsule_output = tf.reshape(conv_output, [shape1[0], 1, -1, self._output_vector_len, 1])\n                if with_routing:\n                    # routing(u, next_num_channels, next_num_capsules, next_output_vector_len, num_iter, scope=None):\n                    # size of u: [batch_size, channels, num_capsules, output_vector_len]\n                    capsule_output = routing(capsule_output, self._num_channels, self._num_capsules, self._output_vector_len, num_iter, scope)\n                capsule_output = squashing(capsule_output)\n                # size of capsule_output: [batch_size, num_capsules, num_vector_len, output_vector_len]\n                capsule_output = tf.reshape(capsule_output, [input_shape[0], self._num_capsules, self._output_vector_len, self._num_channels])\n            elif self._layer_type==\'dnn\':\n                # here, we set with_routing to be True defaultly\n                inputX = tf.reshape(inputX, [input_shape[0], 1, input_shape[1]*input_shape[3], input_shape[2], 1])\n                capsule_output = routing(inputX, self._num_channels, self._num_capsules, self._output_vector_len, num_iter, scope)\n                # size of s: [batch_size, 1, num_channels*num_capsules, output_vector_len, 1]\n                capsule_output = squashing(capsule_output)\n                # size of capsule_output: [batch_size, num_channels*num_capsules, output_vector_len]\n                capsule_output = tf.squeeze(capsule_output, axis=[1, 4])\n            else:\n                capsule_output = None\n        return capsule_output\n\nclass CapsuleNetwork(object):\n    def __init__(self, args, maxTimeSteps):\n        self.args = args\n        self.maxTimeSteps = maxTimeSteps\n        self.build_graph(self.args, self.maxTimeSteps)\n\n    def build_graph(self, args, maxTimeSteps):\n        self.maxTimeSteps = maxTimeSteps\n        self.inputX = tf.placeholder(tf.float32,shape=[maxTimeSteps, args.batch_size, args.num_feature])\n\n        # define tf.SparseTensor for ctc loss\n        self.targetIxs = tf.placeholder(tf.int64)\n        self.targetVals = tf.placeholder(tf.int32)\n        self.targetShape = tf.placeholder(tf.int64)\n        self.targetY = tf.SparseTensor(self.targetIxs, self.targetVals, self.targetShape)\n        self.seqLengths = tf.placeholder(tf.int32, shape=(args.batch_size))\n\n        self.config = {\'name\': args.model,\n                           \'num_layer\': args.num_layer,\n                           \'num_hidden\': args.num_hidden,\n                           \'num_class\': args.num_class,\n                           \'activation\': args.activation,\n                           \'optimizer\': args.optimizer,\n                           \'learning rate\': args.learning_rate,\n                           \'keep prob\': args.keep_prob,\n                           \'batch size\': args.batch_size}\n\n\n        inputX = tf.reshape(self.inputX, [args.batch_size, maxTimeSteps, args.num_feature, 1])\n        print(inputX.get_shape())\n        with tf.variable_scope(""layer_conv1""):\n            # shape of kernel: [batch, in_height, in_width, in_channels]\n            kernel = tf.get_variable(""kernel"", shape=[3, 3, 1, 16], dtype=tf.float32)\n            # shape of conv1:  [batch, height, width, channels]\n            conv1 = tf.nn.conv2d(inputX, kernel, (1,1,1,1), padding=\'VALID\')\n\n        print(conv1.get_shape())\n        output = conv1\n        for layer_id in range(args.num_layer):\n            vars_scope = ""capsule_cnn_layer_""+str(layer_id+1)\n            # (self, num_capsules, num_channels, output_vector_len, layer_type=\'conv\', vars_scope=None):\n            capLayer = CapsuleLayer(4, 8, 2, layer_type=\'conv\', vars_scope=vars_scope)\n            # (self, inputX, kernel_size, strides, routing=True, padding=\'VALID\'):\n            output = capLayer(output, [2, 2], (1,1,1,1), args.num_iter)\n            print(output.get_shape())\n\n        # last dnn layer for classification\n        vars_scope = ""capsule_dnn_layer""\n        capLayer = CapsuleLayer(8, 16, args.num_classes, layer_type=\'dnn\', vars_scope=vars_scope)\n        logits3d = capLayer(output, [3, 3], (1,1,1,1), args.num_iter)\n        logits3d = tf.transpose(logits3d, perm=[1, 0, 2])\n        self.loss = tf.reduce_mean(tf.nn.ctc_loss(self.targetY, logits3d, self.seqLengths))\n        self.var_op = tf.global_variables()\n        self.var_trainable_op = tf.trainable_variables()\n        if args.grad_clip == -1:\n            # not apply gradient clipping\n            self.optimizer = tf.train.AdamOptimizer(args.learning_rate).minimize(self.loss)\n        else:\n            # apply gradient clipping\n            grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, self.var_trainable_op), args.grad_clip)\n            opti = tf.train.AdamOptimizer(args.learning_rate)\n            self.optimizer = opti.apply_gradients(zip(grads, self.var_trainable_op))\n        self.predictions = tf.to_int32(tf.nn.ctc_beam_search_decoder(logits3d, self.seqLengths, merge_repeated=False)[0][0])\n        if args.level == \'cha\':\n            self.errorRate = tf.reduce_sum(tf.edit_distance(self.predictions, self.targetY, normalize=True))\n        self.initial_op = tf.global_variables_initializer()\n        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=5, keep_checkpoint_every_n_hours=1)\n\n\n## test code for simplicity\nif __name__ == \'__main__\':\n    sess=tf.InteractiveSession()\n    conv1 = tf.constant(np.random.rand(2,20,20,2), dtype=tf.float32)\n    # (self, num_capsules, num_channels, output_vector_len, layer_type=\'conv\', vars_scope=None):\n    # (self, inputX, kernel_size, strides, routing=True, padding=\'VALID\'):\n    capLayer1 = CapsuleLayer(2, 3, 10, layer_type=\'conv\', vars_scope=""testlayer1"")\n    output = capLayer1(conv1, [3, 3], (1,1,1,1), 3)\n\n    capLayer2 = CapsuleLayer(2, 3, 10, layer_type=\'dnn\', vars_scope=""testlayer2"")\n    output = capLayer2(output, [3, 3], (1,1,1,1), 3)\n\n    sess.run(tf.global_variables_initializer())\n    print(sess.run(output))\n'"
speechvalley/models/deepSpeech2.py,48,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# File              : deepSpeech2.py\n# Author            : zewangzhang <zzw922cn@gmail.com>\n# Date              : 17.10.2019\n# Last Modified Date: 17.10.2019\n# Last Modified By  : zewangzhang <zzw922cn@gmail.com>\n# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : deepSpeech2.py\n# Description  : Deep Speech2 model for Automatic Speech Recognition\n# ******************************************************\n\nimport argparse\nimport time\nimport datetime\nimport os\nfrom six.moves import cPickle\nfrom functools import wraps\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops.rnn import bidirectional_dynamic_rnn\nfrom speechvalley.utils import load_batched_data, describe, setAttrs, list_to_sparse_tensor, dropout, get_edit_distance\nfrom speechvalley.utils import lnBasicRNNCell, lnGRUCell, lnBasicLSTMCell\n\ndef build_deepSpeech2(args,\n                      maxTimeSteps,\n                      inputX,\n                      cell_fn,\n                      seqLengths,\n                      time_major=True):\n    \'\'\' Parameters:\n\n          maxTimeSteps: maximum time steps of input spectrogram power\n          inputX: spectrogram power of audios, [batch, freq_bin, time_len, in_channels]\n          seqLengths: lengths of samples in a mini-batch\n    \'\'\'\n\n    # 3 2-D convolution layers\n    layer1_filter = tf.get_variable(\'layer1_filter\', shape=(41, 11, 1, 32), dtype=tf.float32)\n    layer1_stride = [1, 2, 2, 1]\n    layer2_filter = tf.get_variable(\'layer2_filter\', shape=(21, 11, 32, 32), dtype=tf.float32)\n    layer2_stride = [1, 2, 1, 1]\n    layer3_filter = tf.get_variable(\'layer3_filter\', shape=(21, 11, 32, 96), dtype=tf.float32)\n    layer3_stride = [1, 2, 1, 1]\n    layer1 = tf.nn.conv2d(inputX, layer1_filter, layer1_stride, padding=\'SAME\')\n    layer1 = tf.layers.batch_normalization(layer1, training=args.is_training)\n    layer1 = tf.contrib.layers.dropout(layer1, keep_prob=args.keep_prob[0], is_training=args.is_training)\n\n    layer2 = tf.nn.conv2d(layer1, layer2_filter, layer2_stride, padding=\'SAME\')\n    layer2 = tf.layers.batch_normalization(layer2, training=args.isTraining)\n    layer2 = tf.contrib.layers.dropout(layer2, keep_prob=args.keep_prob[1], is_training=args.is_training)\n\n    layer3 = tf.nn.conv2d(layer2, layer3_filter, layer3_stride, padding=\'SAME\')\n    layer3 = tf.layers.batch_normalization(layer3, training=args.isTraining)\n    layer3 = tf.contrib.layers.dropout(layer3, keep_prob=args.keep_prob[2], is_training=args.is_training)\n\n    # 4 recurrent layers\n    # inputs must be [max_time, batch_size ,...]\n    layer4_cell = cell_fn(args.num_hidden, activation=args.activation)\n    layer4 = tf.nn.dynamic_rnn(layer4_cell, layer3, sequence_length=seqLengths, time_major=True)\n    layer4 = tf.layers.batch_normalization(layer4, training=args.isTraining)\n    layer4 = tf.contrib.layers.dropout(layer4, keep_prob=args.keep_prob[3], is_training=args.is_training)\n\n    layer5_cell = cell_fn(args.num_hidden, activation=args.activation)\n    layer5 = tf.nn.dynamic_rnn(layer5_cell, layer4, sequence_length=seqLengths, time_major=True)\n    layer5 = tf.layers.batch_normalization(layer5, training=args.isTraining)\n    layer5 = tf.contrib.layers.dropout(layer5, keep_prob=args.keep_prob[4], is_training=args.is_training)\n\n    layer6_cell = cell_fn(args.num_hidden, activation=args.activation)\n    layer6 = tf.nn.dynamic_rnn(layer6_cell, layer5, sequence_length=seqLengths, time_major=True)\n    layer6 = tf.layers.batch_normalization(layer6, training=args.isTraining)\n    layer6 = tf.contrib.layers.dropout(layer6, keep_prob=args.keep_prob[5], is_training=args.is_training)\n\n    layer7_cell = cell_fn(args.num_hidden, activation=args.activation)\n    layer7 = tf.nn.dynamic_rnn(layer7_cell, layer6, sequence_length=seqLengths, time_major=True)\n    layer7 = tf.layers.batch_normalization(layer7, training=args.isTraining)\n    layer7 = tf.contrib.layers.dropout(layer7, keep_prob=args.keep_prob[6], is_training=args.is_training)\n\n    # fully-connected layer\n    layer_fc = tf.layers.dense(layer7, args.num_hidden_fc)\n\n    return layer_fc\n\n\nclass DeepSpeech2(object):\n    def __init__(self, args, maxTimeSteps):\n        self.args = args\n        self.maxTimeSteps = maxTimeSteps\n        if args.layerNormalization is True:\n            if args.rnncell == \'rnn\':\n                self.cell_fn = lnBasicRNNCell\n            elif args.rnncell == \'gru\':\n                self.cell_fn = lnGRUCell\n            elif args.rnncell == \'lstm\':\n                self.cell_fn = lnBasicLSTMCell\n            else:\n                raise Exception(""rnncell type not supported: {}"".format(args.rnncell))\n        else:\n            if args.rnncell == \'rnn\':\n                self.cell_fn = tf.contrib.rnn.BasicRNNCell\n            elif args.rnncell == \'gru\':\n                self.cell_fn = tf.contrib.rnn.GRUCell\n            elif args.rnncell == \'lstm\':\n                self.cell_fn = tf.contrib.rnn.BasicLSTMCell\n            else:\n                raise Exception(""rnncell type not supported: {}"".format(args.rnncell))\n        self.build_graph(args, maxTimeSteps)\n\n    @describe\n    def build_graph(self, args, maxTimeSteps):\n        self.graph = tf.Graph()\n        with self.graph.as_default():\n            # according to DeepSpeech2 paper, input is the spectrogram power of audio, but if you like,\n            # you can also use mfcc feature as the input.\n            self.inputX = tf.placeholder(tf.float32,\n                                         shape=(maxTimeSteps, args.batch_size, args.num_feature))\n            inputXrs = tf.reshape(self.inputX, [args.batch_size, args.num_feature, maxTimeSteps, 1])\n            #self.inputList = tf.split(inputXrs, maxTimeSteps, 0)  # convert inputXrs from [32*maxL,39] to [32,maxL,39]\n\n            self.targetIxs = tf.placeholder(tf.int64)\n            self.targetVals = tf.placeholder(tf.int32)\n            self.targetShape = tf.placeholder(tf.int64)\n            self.targetY = tf.SparseTensor(self.targetIxs, self.targetVals, self.targetShape)\n            self.seqLengths = tf.placeholder(tf.int32, shape=(args.batch_size))\n            self.config = {\'name\': args.model,\n                           \'rnncell\': self.cell_fn,\n                           \'num_layer\': args.num_layer,\n                           \'num_hidden\': args.num_hidden,\n                           \'num_class\': args.num_class,\n                           \'activation\': args.activation,\n                           \'optimizer\': args.optimizer,\n                           \'learning rate\': args.learning_rate,\n                           \'keep prob\': args.keep_prob,\n                           \'batch size\': args.batch_size}\n\n            output_fc = build_deepSpeech2(self.args, maxTimeSteps, self.inputX, self.cell_fn, self.seqLengths)\n            self.loss = tf.reduce_mean(tf.nn.ctc_loss(self.targetY, output_fc, self.seqLengths))\n            self.var_op = tf.global_variables()\n            self.var_trainable_op = tf.trainable_variables()\n\n            if args.grad_clip == -1:\n                # not apply gradient clipping\n                self.optimizer = tf.train.AdamOptimizer(args.learning_rate).minimize(self.loss)\n            else:\n                # apply gradient clipping\n                grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, self.var_trainable_op), args.grad_clip)\n                opti = tf.train.AdamOptimizer(args.learning_rate)\n                self.optimizer = opti.apply_gradients(zip(grads, self.var_trainable_op))\n            self.predictions = tf.to_int32(\n                tf.nn.ctc_beam_search_decoder(output_fc, self.seqLengths, merge_repeated=False)[0][0])\n            if args.level == \'cha\':\n                self.errorRate = tf.reduce_sum(tf.edit_distance(self.predictions, self.targetY, normalize=True))\n            self.initial_op = tf.global_variables_initializer()\n            self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=5, keep_checkpoint_every_n_hours=1)\n'"
speechvalley/models/dynamic_brnn.py,40,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : dynamic_brnn.py\n# Description  : Dynamic Bidirectional RNN model for Automatic Speech Recognition\n# ******************************************************\n\nimport argparse\nimport time\nimport datetime\nimport os\nfrom six.moves import cPickle\nfrom functools import wraps\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops.rnn import bidirectional_dynamic_rnn\n\nfrom speechvalley.utils import load_batched_data, describe, setAttrs, list_to_sparse_tensor, dropout, get_edit_distance\nfrom speechvalley.utils import lnBasicRNNCell, lnGRUCell, lnBasicLSTMCell\n\ndef build_multi_dynamic_brnn(args,\n                             maxTimeSteps,\n                             inputX,\n                             cell_fn,\n                             seqLengths,\n                             time_major=True):\n    hid_input = inputX\n    for i in range(args.num_layer):\n        scope = \'DBRNN_\' + str(i + 1)\n        forward_cell = cell_fn(args.num_hidden, activation=args.activation)\n        backward_cell = cell_fn(args.num_hidden, activation=args.activation)\n        # tensor of shape: [max_time, batch_size, input_size]\n        outputs, output_states = bidirectional_dynamic_rnn(forward_cell, backward_cell,\n                                                           inputs=hid_input,\n                                                           dtype=tf.float32,\n                                                           sequence_length=seqLengths,\n                                                           time_major=True,\n                                                           scope=scope)\n        # forward output, backward ouput\n        # tensor of shape: [max_time, batch_size, input_size]\n        output_fw, output_bw = outputs\n        # forward states, backward states\n        output_state_fw, output_state_bw = output_states\n        # output_fb = tf.concat(2, [output_fw, output_bw])\n        output_fb = tf.concat([output_fw, output_bw], 2)\n        shape = output_fb.get_shape().as_list()\n        output_fb = tf.reshape(output_fb, [shape[0], shape[1], 2, int(shape[2] / 2)])\n        hidden = tf.reduce_sum(output_fb, 2)\n        hidden = dropout(hidden, args.keep_prob, (args.mode == \'train\'))\n\n        if i != args.num_layer - 1:\n            hid_input = hidden\n        else:\n            outputXrs = tf.reshape(hidden, [-1, args.num_hidden])\n            # output_list = tf.split(0, maxTimeSteps, outputXrs)\n            output_list = tf.split(outputXrs, maxTimeSteps, 0)\n            fbHrs = [tf.reshape(t, [args.batch_size, args.num_hidden]) for t in output_list]\n    return fbHrs\n\n\nclass DBiRNN(object):\n    def __init__(self, args, maxTimeSteps):\n        self.args = args\n        self.maxTimeSteps = maxTimeSteps\n        if args.layerNormalization is True:\n            if args.rnncell == \'rnn\':\n                self.cell_fn = lnBasicRNNCell\n            elif args.rnncell == \'gru\':\n                self.cell_fn = lnGRUCell\n            elif args.rnncell == \'lstm\':\n                self.cell_fn = lnBasicLSTMCell\n            else:\n                raise Exception(""rnncell type not supported: {}"".format(args.rnncell))\n        else:\n            if args.rnncell == \'rnn\':\n                self.cell_fn = tf.contrib.rnn.BasicRNNCell\n            elif args.rnncell == \'gru\':\n                self.cell_fn = tf.contrib.rnn.GRUCell\n            elif args.rnncell == \'lstm\':\n                self.cell_fn = tf.contrib.rnn.BasicLSTMCell\n            else:\n                raise Exception(""rnncell type not supported: {}"".format(args.rnncell))\n\n        self.build_graph(args, maxTimeSteps)\n\n    @describe\n    def build_graph(self, args, maxTimeSteps):\n        self.graph = tf.Graph()\n        with self.graph.as_default():\n            self.inputX = tf.placeholder(tf.float32,\n                                         shape=(maxTimeSteps, args.batch_size, args.num_feature))  # [maxL,32,39]\n            inputXrs = tf.reshape(self.inputX, [-1, args.num_feature])\n            # self.inputList = tf.split(0, maxTimeSteps, inputXrs) #convert inputXrs from [32*maxL,39] to [32,maxL,39]\n            self.inputList = tf.split(inputXrs, maxTimeSteps, 0)  # convert inputXrs from [32*maxL,39] to [32,maxL,39]\n            self.targetIxs = tf.placeholder(tf.int64)\n            self.targetVals = tf.placeholder(tf.int32)\n            self.targetShape = tf.placeholder(tf.int64)\n            self.targetY = tf.SparseTensor(self.targetIxs, self.targetVals, self.targetShape)\n            self.seqLengths = tf.placeholder(tf.int32, shape=(args.batch_size))\n            self.config = {\'name\': args.model,\n                           \'rnncell\': self.cell_fn,\n                           \'num_layer\': args.num_layer,\n                           \'num_hidden\': args.num_hidden,\n                           \'num_class\': args.num_class,\n                           \'activation\': args.activation,\n                           \'optimizer\': args.optimizer,\n                           \'learning rate\': args.learning_rate,\n                           \'keep prob\': args.keep_prob,\n                           \'batch size\': args.batch_size}\n\n            fbHrs = build_multi_dynamic_brnn(self.args, maxTimeSteps, self.inputX, self.cell_fn, self.seqLengths)\n            with tf.name_scope(\'fc-layer\'):\n                with tf.variable_scope(\'fc\'):\n                    weightsClasses = tf.Variable(\n                        tf.truncated_normal([args.num_hidden, args.num_class], name=\'weightsClasses\'))\n                    biasesClasses = tf.Variable(tf.zeros([args.num_class]), name=\'biasesClasses\')\n                    logits = [tf.matmul(t, weightsClasses) + biasesClasses for t in fbHrs]\n            logits3d = tf.stack(logits)\n            self.loss = tf.reduce_mean(tf.nn.ctc_loss(self.targetY, logits3d, self.seqLengths))\n            self.var_op = tf.global_variables()\n            self.var_trainable_op = tf.trainable_variables()\n\n            if args.grad_clip == -1:\n                # not apply gradient clipping\n                self.optimizer = tf.train.AdamOptimizer(args.learning_rate).minimize(self.loss)\n            else:\n                # apply gradient clipping\n                grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, self.var_trainable_op), args.grad_clip)\n                opti = tf.train.AdamOptimizer(args.learning_rate)\n                self.optimizer = opti.apply_gradients(zip(grads, self.var_trainable_op))\n            self.predictions = tf.to_int32(\n                tf.nn.ctc_beam_search_decoder(logits3d, self.seqLengths, merge_repeated=False)[0][0])\n            if args.level == \'cha\':\n                self.errorRate = tf.reduce_sum(tf.edit_distance(self.predictions, self.targetY, normalize=True))\n            self.initial_op = tf.global_variables_initializer()\n            self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=5, keep_checkpoint_every_n_hours=1)\n'"
speechvalley/pipeline/__init__.py,0,b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : __init__.py\n# Description  : Examples for input pipeline\n# ******************************************************\n'
speechvalley/pipeline/big_input.py,30,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : big_input.py\n# Description  : Input pipeline for big dataset \n# ******************************************************\n\nimport tensorflow as tf\nfrom tensorflow.python.training import queue_runner_impl\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.core.protobuf import saver_pb2\nfrom tensorflow.python.training import saver as saver_lib\nfrom tensorflow.python.platform import app\nfrom tensorflow.python.platform import flags\n\nimport numpy as np\nimport os\nimport time\nimport math\n\n\n\n# for any data type except int\ndef _bytes_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n# for int data type\ndef _int64_feature(value):\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n## write tfrecord file\nclass RecordWriter(object):\n  def __init__(self, path):\n    self.path = path\n\n  def write(self, content, filename, feature_num=2):\n    tfrecords_filename = os.path.join(self.path, filename)\n    writer = tf.python_io.TFRecordWriter(tfrecords_filename)\n    if feature_num>1:\n      assert isinstance(content, list), \'content must be a list now\'\n      feature_dict = {}\n      for i in range(feature_num):\n        feature = content[i]\n        if isinstance(feature, int):\n          feature_dict[\'feature\'+str(i+1)]=_int64_feature(feature)\n        else:\n          feature_raw = np.array(feature).tostring()\n          feature_dict[\'feature\'+str(i+1)]=_bytes_feature(feature_raw)\n      features_to_write = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n      writer.write(features_to_write.SerializeToString())\n      writer.close()\n      print(\'Record has been writen:\'+tfrecords_filename)\n\n\n## read tfrecord file\ndef read(filename_queue, feature_num=2, dtypes=[list, int]):\n  reader = tf.TFRecordReader()\n  _, serialized_example = reader.read(filename_queue)\n  feature_dict={}\n  for i in range(feature_num):\n    # here, only three data types are allowed: tf.float32, tf.int64, tf.string\n    if dtypes[i] is int:\n      feature_dict[\'feature\'+str(i+1)]=tf.FixedLenFeature([], tf.int64)\n    else:\n      feature_dict[\'feature\'+str(i+1)]=tf.FixedLenFeature([], tf.string)\n  features = tf.parse_single_example(\n      serialized_example,\n      features=feature_dict)\n  return features\n\n#======================================================================================\n## test code\nflags.DEFINE_string(""scale"", ""big"", ""specify your dataset scale"")\nflags.DEFINE_string(""logdir"", ""/home/pony/github/data/inputpipeline/big"", ""specify the location to store log or model"")\nflags.DEFINE_integer(""samples_num"", 80, ""specify your total number of samples"")\nflags.DEFINE_integer(""time_length"", 2, ""specify max time length of sample"")\nflags.DEFINE_integer(""feature_size"", 2, ""specify feature size of sample"")\nflags.DEFINE_integer(""num_epochs"", 100, ""specify number of training epochs"")\nflags.DEFINE_integer(""batch_size"", 2, ""specify batch size when training"")\nflags.DEFINE_integer(""num_classes"", 10, ""specify number of output classes"")\nFLAGS = flags.FLAGS\n\n\nif __name__ == \'__main__\':\n  scale = FLAGS.scale\n  logdir = FLAGS.logdir\n  sn = FLAGS.samples_num\n  tl = FLAGS.time_length\n  fs = FLAGS.feature_size\n  num_epochs = FLAGS.num_epochs\n  batch_size = FLAGS.batch_size\n  num_classes = FLAGS.num_classes\n  num_batches = int(math.ceil(1.0*sn/batch_size))\n\n  # x:[sn, tl, fs]\n  with tf.variable_scope(\'train-samples\'):\n    x = []\n    for n in range(sn):\n      sub_x = np.random.rand(fs).astype(np.float32)\n      x.append(sub_x)\n       \n  # y:[sn, tl]\n  with tf.variable_scope(\'train-labels\'):\n    y = []\n    for n in range(sn):\n      sub_y = np.random.randint(0, num_classes)\n      y_one_hot = np.eye(num_classes)[sub_y]\n      y.append(y_one_hot.astype(np.int32))\n\n  with tf.variable_scope(\'TFRecordWriter\'):\n    record_writer = RecordWriter(logdir)\n    for n in range(sn):\n      record_writer.write([x[n], y[n]], \'samples-labels[%s].tfrecords\'%str(n))\n\n  with tf.variable_scope(\'FilesProducer\'):\n    filenames = [os.path.join(logdir, \'samples-labels[%s].tfrecords\' % str(i)) for i in range(sn)]\n    filenamesQueue = tf.train.string_input_producer(filenames, num_epochs, shuffle=False)\n\n  with tf.variable_scope(\'Reader\'):\n    features = read(filenamesQueue, dtypes=[list, list])\n    # when handling array, must specify its shape, so reshape operation\n    feature_x = tf.reshape(tf.decode_raw(features[\'feature1\'], tf.float32), [fs])\n    feature_y = tf.reshape(tf.decode_raw(features[\'feature2\'], tf.int32), [num_classes])\n\n  with tf.variable_scope(\'InputProducer\'):\n    batched_x, batched_y = tf.train.batch([feature_x, feature_y], batch_size=batch_size, dynamic_pad=False, allow_smaller_final_batch=True)\n    batched_x = tf.layers.dense(batched_x, 2*fs)\n    batched_x = tf.layers.dense(batched_x, num_classes)\n\n  with tf.variable_scope(\'Loss\'):\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=batched_y, logits=batched_x))\n    optimizer = tf.train.AdamOptimizer(0.1)\n    train_op = optimizer.minimize(loss)\n    tf.summary.scalar(\'Loss\', loss)\n  merged = tf.summary.merge_all()\n\n  t1 = time.time()\n  sess = tf.Session()\n  checkpoint_path = os.path.join(logdir, scale+\'_model\')\n  writer = tf.summary.FileWriter(logdir, sess.graph)\n  sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\n  coord = tf.train.Coordinator()\n  threads = queue_runner_impl.start_queue_runners(sess=sess)\n  saver = saver_lib.Saver(write_version=saver_pb2.SaverDef.V2)\n  saver.save(sess, checkpoint_path)\n\n  for i in range(num_batches*num_epochs):\n    l, _, summary = sess.run([loss, train_op, merged])\n    writer.add_summary(summary, i)\n    print \'batch \'+str(i+1)+\'/\'+str(num_batches*num_epochs)+\'\\tLoss:\'+str(l)\n  writer.close()\n  coord.request_stop()\n  coord.join(threads)\n  print \'program takes time:\'+str(time.time()-t1)\n'"
speechvalley/pipeline/small_input.py,19,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : small_input.py\n# Description  : Input pipeline for small dataset\n# ******************************************************\n\nimport tensorflow as tf\nfrom tensorflow.python.training import queue_runner_impl\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.core.protobuf import saver_pb2\nfrom tensorflow.python.training import saver as saver_lib\nfrom tensorflow.python.platform import app\nfrom tensorflow.python.platform import flags\n\nimport os\nimport time\nimport math\nimport numpy as np\n\nflags.DEFINE_string(""scale"", ""small"", ""specify your dataset scale"")\nflags.DEFINE_string(""logdir"", ""/home/pony/github/data/inputpipeline/small"", ""specify the location to store log or model"")\nflags.DEFINE_integer(""samples_num"", 80, ""specify your total number of samples"")\nflags.DEFINE_integer(""time_length"", 2, ""specify max time length of sample"")\nflags.DEFINE_integer(""feature_size"", 2, ""specify feature size of sample"")\nflags.DEFINE_integer(""num_epochs"", 100, ""specify number of training epochs"")\nflags.DEFINE_integer(""batch_size"", 2, ""specify batch size when training"")\nflags.DEFINE_integer(""num_classes"", 10, ""specify number of output classes"")\nFLAGS = flags.FLAGS\n\nif __name__ == \'__main__\':\n\n  scale = FLAGS.scale\n  logdir = FLAGS.logdir\n  sn = FLAGS.samples_num\n  fs = FLAGS.feature_size\n  num_epochs = FLAGS.num_epochs\n  batch_size = FLAGS.batch_size\n  num_classes = FLAGS.num_classes\n  num_batches = int(math.ceil(1.0*sn/batch_size))\n\n  with tf.variable_scope(\'train-samples\'):\n    x = tf.constant(np.random.rand(sn, fs).astype(np.float32))\n\n  with tf.variable_scope(\'train-labels\'):\n    indices = np.random.randint(0, num_classes, size=sn).astype(np.int32)\n    y = tf.one_hot(indices, depth=num_classes,\n                   on_value=1.0,\n                   off_value=0.0,\n                   axis=-1,\n                   dtype=tf.float32)\n\n  # dequeue ops\n  with tf.variable_scope(\'InputProducer\'):\n    slice_x, slice_y = tf.train.slice_input_producer([x, y], \n        num_epochs = num_epochs, seed=22, \n        capacity=36, shuffle=True)\n\n    batched_x, batched_y = tf.train.batch([slice_x, slice_y], \n        batch_size=batch_size, dynamic_pad=False, \n        allow_smaller_final_batch=True)\n\n    batched_x = tf.layers.dense(batched_x, 2*fs)\n    batched_x = tf.layers.dense(batched_x, num_classes)\n  \n  with tf.variable_scope(\'Loss\'):\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n        labels=batched_y, logits=batched_x))\n    optimizer = tf.train.AdamOptimizer(0.1)\n    train_op = optimizer.minimize(loss)\n    tf.summary.scalar(\'Loss\', loss)\n\n  merged = tf.summary.merge_all()\n\n  t1 = time.time()\n  sess = tf.Session()\n  checkpoint_path = os.path.join(logdir, scale+\'_model\')\n  writer = tf.summary.FileWriter(logdir, sess.graph)\n  sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\n  coord = tf.train.Coordinator()\n  threads = queue_runner_impl.start_queue_runners(sess=sess)\n  saver = saver_lib.Saver(write_version=saver_pb2.SaverDef.V2)\n  saver.save(sess, checkpoint_path)\n  for i in range(num_batches*num_epochs):\n    l, _, summary = sess.run([loss, train_op, merged])\n    writer.add_summary(summary, i)\n    print \'batch \'+str(i+1)+\'/\'+str(num_batches*num_epochs)+\'\\tLoss:\'+str(l)\n  writer.close()\t\n  coord.request_stop()\n  coord.join(threads)\n  print \'program takes time:\'+str(time.time()-t1)\n'"
speechvalley/utils/__init__.py,0,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : __init__.py\n# Description  : Utils function for Automatic Speech Recognition\n# ******************************************************\n\nfrom speechvalley.utils.calcPER import calc_PER\nfrom speechvalley.utils.functionDictUtils import activation_functions_dict, optimizer_functions_dict \nfrom speechvalley.utils.lnRNNCell import BasicRNNCell as lnBasicRNNCell\nfrom speechvalley.utils.lnRNNCell import GRUCell as lnGRUCell\nfrom speechvalley.utils.lnRNNCell import BasicLSTMCell as lnBasicLSTMCell\nfrom speechvalley.utils.taskUtils import check_path_exists, get_num_classes, dotdict \nfrom speechvalley.utils.utils import setAttrs, getAttrs, describe, dropout, batch_norm, data_lists_to_batches, load_batched_data, list_dirs, get_edit_distance, output_to_sequence, target2phoneme, count_params, logging, list_to_sparse_tensor \nfrom speechvalley.utils.visualization import plotWaveform\n'"
speechvalley/utils/calcPER.py,1,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : calcPER.py\n# Description  : Calculating Phoneme Error Rate(PER) based on python leven\n# ******************************************************\n\n\nimport leven\nimport numpy as np\nfrom collections import namedtuple\n\nSparseTensor = namedtuple(\'SparseTensor\', \'indices vals shape\')\n\nPHN_MAPPING = {\'iy\': \'iy\', \'ix\': \'ix\', \'ih\': \'ix\', \'eh\': \'eh\', \'ae\': \'ae\', \'ax\': \'ax\', \'ah\': \'ax\',\n               \'ax-h\': \'ax\', \'uw\': \'uw\', \'ux\': \'uw\', \'uh\': \'uh\', \'ao\': \'ao\', \'aa\': \'ao\', \'ey\': \'ey\',\n               \'ay\': \'ay\', \'oy\': \'oy\', \'aw\': \'aw\', \'ow\': \'ow\', \'er\': \'er\', \'axr\': \'er\', \'l\': \'l\', \'el\': \'l\',\n               \'r\': \'r\', \'w\': \'w\', \'y\': \'y\', \'m\': \'m\', \'em\': \'m\', \'n\': \'n\', \'en\': \'n\', \'nx\': \'n\', \'ng\': \'ng\',\n               \'eng\': \'ng\', \'v\': \'v\', \'f\': \'f\', \'dh\': \'dh\', \'th\': \'th\', \'z\': \'z\', \'s\': \'s\', \'zh\': \'zh\',\n               \'sh\': \'zh\', \'jh\': \'jh\', \'ch\': \'ch\', \'b\': \'b\', \'p\': \'p\', \'d\': \'d\', \'dx\': \'dx\', \'t\': \'t\',\n               \'g\': \'g\', \'k\': \'k\', \'hh\': \'hh\', \'hv\': \'hh\', \'bcl\': \'h#\', \'pcl\': \'h#\', \'dcl\': \'h#\', \'tcl\': \'h#\',\n               \'gcl\': \'h#\', \'kcl\': \'h#\', \'q\': \'h#\', \'epi\': \'h#\', \'pau\': \'h#\', \'h#\': \'h#\'}\n\nIDX_MAPPING = {0: 3, 1: 1, 2: 5, 3: 3, 4: 4, 5: 5, 6: 5, 7: 22, 8: 8, 9: 9, 10: 27, 11: 11, 12: 12, 13: 27,\n               14: 14, 15: 15, 16: 16, 17: 36, 18: 37, 19: 38, 20: 39, 21: 27, 22: 22, 23: 23, 24: 24, 25: 25,\n               26: 27, 27: 27, 28: 28, 29: 28, 30: 31, 31: 31, 32: 32, 33: 33, 34: 34, 35: 27, 36: 36, 37: 37,\n               38: 38, 39: 39, 40: 38, 41: 41, 42: 42, 43: 43, 44: 27, 45: 27, 46: 27, 47: 47, 48: 48, 49: 60,\n               50: 50, 51: 27, 52: 52, 53: 53, 54: 54, 55: 54, 56: 56, 57: 57, 58: 58, 59: 59, 60: 60}\n\n\ndef calc_PER(pred, ground_truth, normalize=True, merge_phn=True):\n    """"""Calculates the Phoneme Error Rate based on python package leven, which produce the same results as \n    tf.edit_distance and tf.reduce_mean based calculation\n    \n    :param pred: tuple with 3 numpy-typed element representing sparse tensor\n    :param ground_truth: tuple with 3 numpy-typed element representing sparse tensor\n    :param normalize: if True, the distance between sequence will be divided by the length of the ground_truth length\n    :param merge_phn: if True, 61 phonemes will be merged into 39 phonemes, then do the distance calculation\n    :return: the PER\n    """"""\n\n    pred_seq_list = seq_to_single_char_strings(sparse_tensor_to_seq_list(pred, merge_phn=merge_phn))\n    truth_seq_list = seq_to_single_char_strings(sparse_tensor_to_seq_list(ground_truth, merge_phn=merge_phn))\n\n    assert len(truth_seq_list) == len(pred_seq_list)\n\n    distances = []\n    for i in range(len(truth_seq_list)):\n        dist_i = leven.levenshtein(pred_seq_list[i], truth_seq_list[i])\n        if normalize:\n            dist_i /= float(len(truth_seq_list[i]))\n        distances.append(dist_i)\n\n    return np.mean(distances)\n\n\ndef seq_to_single_char_strings(seq):\n    strings = []\n    for s in seq:\n        strings.append(\'\'.join([chr(65 + p) for p in s]))\n\n    return strings\n\n\ndef sparse_tensor_to_seq_list(sparse_seq, merge_phn=True):\n    phonemes_list = []\n    it = 0\n    num_samples = np.max(sparse_seq.indices, axis=0)[0] + 1\n    for n in range(num_samples):\n        cur_sample_indices = sparse_seq.indices[sparse_seq.indices[:, 0] == n, 1]\n\n        if len(cur_sample_indices) == 0:\n            seq_length = 0\n        else:\n            seq_length = np.max(cur_sample_indices) + 1\n\n        seq = sparse_seq.vals[it:it+seq_length]\n        _seq = [IDX_MAPPING[p] for p in seq] if merge_phn else seq\n        phonemes_list.append(_seq)\n        it += seq_length\n\n    return phonemes_list\n\n'"
speechvalley/utils/ed.py,5,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : ed.py\n# Description  : Calculating edit distance for Automatic Speech Recognition\n# ******************************************************\n\nimport tensorflow as tf\nimport numpy as np\n\nphn = [\'aa\', \'ae\', \'ah\', \'ao\', \'aw\', \'ax\', \'ax-h\',\n       \'axr\', \'ay\', \'b\', \'bcl\', \'ch\', \'d\', \'dcl\',\n       \'dh\', \'dx\', \'eh\', \'el\', \'em\', \'en\', \'eng\',\n       \'epi\', \'er\', \'ey\', \'f\', \'g\', \'gcl\', \'h#\',\n       \'hh\', \'hv\', \'ih\', \'ix\', \'iy\', \'jh\', \'k\',\n       \'kcl\', \'l\', \'m\', \'n\', \'ng\', \'nx\', \'ow\',\n       \'oy\', \'p\', \'pau\', \'pcl\', \'q\', \'r\', \'s\',\n       \'sh\', \'t\', \'tcl\', \'th\', \'uh\', \'uw\', \'ux\',\n       \'v\', \'w\', \'y\', \'z\', \'zh\']\n\nmapping = {\'ux\':\'uw\',\'axr\':\'er\',\'em\':\'m\',\'nx\':\'en\',\'n\':\'en\',\n              \'eng\':\'ng\',\'hv\':\'hh\',\'cl\':\'sil\',\'bcl\':\'sil\',\'dcl\':\'sil\',\n              \'gcl\':\'sil\',\'epi\':\'sil\',\'h#\':\'sil\',\'kcl\':\'sil\',\'pau\':\'sil\',\n              \'pcl\':\'sil\',\'tcl\':\'sil\',\'vcl\':\'sil\',\'l\':\'el\',\'zh\':\'sh\',\n              \'aa\':\'ao\',\'ix\':\'ih\',\'ax\':\'ah\'}\n\ndef group_phoneme(orig_phn,mapping):\n    group_phn = []\n    for val in orig_phn:\n        group_phn.append(val)\n    group_phn.append(\'sil\')\n    for key in mapping.keys():\n        if key in orig_phn:\n            group_phn.remove(key)\n    group_phn.sort()\n    return group_phn\n\ndef list_to_sparse_tensor(targetList,mode=\'train\'):\n    \'\'\' turn 2-D List to SparseTensor\n    \'\'\'\n    # NOTE: \'sil\' is a new phoneme, you should care this.\n\n    indices = [] #index\n    vals = [] #value\n    group_phn = group_phoneme(phn,mapping)\n    for tI, target in enumerate(targetList):\n        for seqI, val in enumerate(target):\n            if(mode == \'train\'):\n                indices.append([tI, seqI])\n                vals.append(val)\n            elif(mode == \'test\'):\n                if(phn[val] in mapping.keys()):\n                    val = group_phn.index(mapping[phn[val]])\n                indices.append([tI, seqI])\n                vals.append(val)\n            else:\n                raise ValueError(""Invalid mode."",mode)\n    shape = [len(targetList), np.asarray(indices).max(0)[1]+1] #shape\n    return (np.array(indices), np.array(vals), np.array(shape))\n\ndef get_edit_distance(hyp_arr,truth_arr,mode=\'train\'):\n    \'\'\' calculate edit distance\n    \'\'\'\n    graph = tf.Graph()\n    with graph.as_default():\n        truth = tf.sparse_placeholder(tf.int32)\n        hyp = tf.sparse_placeholder(tf.int32)\n        editDist = tf.edit_distance(hyp, truth, normalize=True)\n\n    with tf.Session(graph=graph) as session:\n        truthTest = list_to_sparse_tensor(truth_arr, mode)\n        hypTest = list_to_sparse_tensor(hyp_arr, mode)\n        feedDict = {truth: truthTest, hyp: hypTest}\n        dist = session.run(editDist, feed_dict=feedDict)\n    return dist\n\nif __name__ == \'__main__\':\n    a=[[0,5,49]]\t\n    b=[[21,5,10]]\n    print(get_edit_distance(a,b,mode=\'test\'))\n    print(len(phn))\n    print(len(mapping))\n'"
speechvalley/utils/functionDictUtils.py,7,"b""# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : functionDictUtils.py\n# Description  : Common function dictionaries for Automatic Speech Recognition\n# ******************************************************\n\n\nimport tensorflow as tf \n\nactivation_functions_dict = {\n    'sigmoid': tf.sigmoid, 'tanh': tf.tanh, 'relu': tf.nn.relu, 'relu6': tf.nn.relu6,\n    'elu': tf.nn.elu, 'softplus': tf.nn.softplus, 'softsign': tf.nn.softsign\n    # for detailed intro, go to https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/activation_functions_\n    }\n\noptimizer_functions_dict = {\n    'gd': tf.train.GradientDescentOptimizer,\n    'adadelta': tf.train.AdadeltaOptimizer,\n    'adagrad': tf.train.AdagradOptimizer,\n    'adam': tf.train.AdamOptimizer,\n    'rmsprop': tf.train.RMSPropOptimizer\n    }\n\n"""
speechvalley/utils/lnRNNCell.py,22,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : lnRNNCell.py\n# Description  : RNN Cell with Layer Normalization for Automatic Speech Recognition\n# ******************************************************\n\nimport tensorflow as tf\nimport numpy as np\n\ndef ln(layer, gain, bias):\n    self.dims = layer.get_shape().as_list()\n    assert len(self.dims)==3, \'layer must be 3-D tensor\'\n    miu, sigma = tf.nn.moments(self.layer, axes=[2], keep_dims=True)\n    ln_layer = tf.div(tf.subtract(self.layer, miu), tf.sqrt(sigma))\n    ln_layer = ln_layer*gain + bias\n    return ln_layer\n\nclass BasicRNNCell(tf.contrib.rnn.RNNCell):\n  """"""The most basic RNN cell.\n  Args:\n    num_units: int, The number of units in the RNN cell.\n    activation: Nonlinearity to use.  Default: `tanh`.\n    reuse: (optional) Python boolean describing whether to reuse variables\n     in an existing scope.  If not `True`, and the existing scope already has\n     the given variables, an error is raised.\n  """"""\n\n  def __init__(self, num_units, activation=None, reuse=None):\n    super(BasicRNNCell, self).__init__(_reuse=reuse)\n    self._num_units = num_units\n    self._activation = activation or tf.nn.tanh\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def call(self, inputs, state):\n    """"""Most basic RNN: output = new_state = act(W * input + U * state + B).""""""\n    with tf.variable_scope(\'layer_normalization\'):\n      gain = tf.get_variable(\'gain\', shape=[self._num_units], initializer=tf.ones_initializer())\n      bias = tf.get_variable(\'bias\', shape=[self._num_units], initializer=tf.zeros_initializer())\n    output = ln(_linear([inputs, state], self._num_units, True), gain, bias)\n    return output, output\n\nclass GRUCell(tf.contrib.rnn.RNNCell):\n\n  def __init__(self,\n               num_units,\n               activation=None,\n               reuse=None,\n               kernel_initializer=None,\n               bias_initializer=None):\n    super(GRUCell, self).__init__(_reuse=reuse)\n    self._num_units = num_units\n    self._activation = activation or tf.nn.tanh\n    self._kernel_initializer = kernel_initializer\n    self._bias_initializer = bias_initializer\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def call(self, inputs, state):\n    """"""Gated recurrent unit (GRU) with nunits cells.""""""\n    with tf.variable_scope(\'layer_normalization\'):\n      gain1 = tf.get_variable(\'gain1\', shape=[2*self._num_units], initializer=tf.ones_initializer())\n      bias1 = tf.get_variable(\'bias1\', shape=[2*self._num_units], initializer=tf.zeros_initializer())\n      gain2 = tf.get_variable(\'gain2\', shape=[self._num_units], initializer=tf.ones_initializer())\n      bias2 = tf.get_variable(\'bias2\', shape=[self._num_units], initializer=tf.zeros_initializer())\n\n    with vs.variable_scope(""gates""):  # Reset gate and update gate.\n      # We start with bias of 1.0 to not reset and not update.\n      bias_ones = self._bias_initializer\n      if self._bias_initializer is None:\n        dtype = [a.dtype for a in [inputs, state]][0]\n        bias_ones = tf.constant_initializer(1.0, dtype=dtype)\n      value = tf.nn.sigmoid(ln(\n          _linear([inputs, state], 2 * self._num_units, True, bias_ones,\n                  self._kernel_initializer), gain1, bias1))\n      r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1)\n    with vs.variable_scope(""candidate""):\n      c = self._activation(ln(\n          _linear([inputs, r * state], self._num_units, True,\n                  self._bias_initializer, self._kernel_initializer), gain2, bias2))\n    new_h = u * state + (1 - u) * c\n    return new_h, new_h\n\nclass BasicLSTMCell(tf.contrib.rnn.RNNCell):\n\n  def __init__(self, num_units, forget_bias=1.0,\n               state_is_tuple=True, activation=None, reuse=None):\n    super(BasicLSTMCell, self).__init__(_reuse=reuse)\n    if not state_is_tuple:\n      logging.warn(""%s: Using a concatenated state is slower and will soon be ""\n                   ""deprecated.  Use state_is_tuple=True."", self)\n    self._num_units = num_units\n    self._forget_bias = forget_bias\n    self._state_is_tuple = state_is_tuple\n    self._activation = activation or math_ops.tanh\n\n  @property\n  def state_size(self):\n    return (LSTMStateTuple(self._num_units, self._num_units)\n            if self._state_is_tuple else 2 * self._num_units)\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def call(self, inputs, state):\n    with tf.variable_scope(\'layer_normalization\'):\n      gain_h = tf.get_variable(\'gain_h\', shape=[4*self._num_units], initializer=tf.ones_initializer())\n      bias_h = tf.get_variable(\'bias_h\', shape=[4*self._num_units], initializer=tf.zeros_initializer())\n      gain_c = tf.get_variable(\'gain_c\', shape=[self._num_units], initializer=tf.ones_initializer())\n      bias_c = tf.get_variable(\'bias_c\', shape=[self._num_units], initializer=tf.zeros_initializer())\n    sigmoid = math_ops.sigmoid\n    # Parameters of gates are concatenated into one multiply for efficiency.\n    if self._state_is_tuple:\n      c, h = state\n    else:\n      c, h = array_ops.split(value=state, num_or_size_splits=2, axis=1)\n\n    concat = ln(_linear([inputs, h], 4 * self._num_units, True), gain_h, bias_h)\n\n    # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n    i, j, f, o = array_ops.split(value=concat, num_or_size_splits=4, axis=1)\n\n    new_c = (\n        c * sigmoid(f + self._forget_bias) + sigmoid(i) * self._activation(j))\n    new_h = self._activation(ln(new_c, gain_c, bias_c)) * sigmoid(o)\n\n    if self._state_is_tuple:\n      new_state = LSTMStateTuple(new_c, new_h)\n    else:\n      new_state = array_ops.concat([new_c, new_h], 1)\n    return new_h, new_state\n'"
speechvalley/utils/taskUtils.py,0,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : taskUtils.py\n# Description  : Utils function for Automatic Speech Recognition\n# ******************************************************\n\nimport os\n\ndef get_num_classes(level):\n    if level == \'phn\':\n        num_classes = 62\n    elif level == \'cha\':\n        num_classes = 29\n    elif level == \'seq2seq\':\n        num_classes = 30\n    else:\n        raise ValueError(\'level must be phn, cha or seq2seq, but the given level is %s\'%str(level))\n    return num_classes\n\n\ndef check_path_exists(path):\n    """""" check a path exists or not\n    """"""\n    if isinstance(path, list):\n        for p in path:\n            if not os.path.exists(p):\n                os.makedirs(p)\n    else:\n        if not os.path.exists(path):\n            os.makedirs(path)\n\nclass dotdict(dict):\n    __getattr__ = dict.get\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n'"
speechvalley/utils/utils.py,15,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : utils.py\n# Description  : Function utils library for Automatic Speech Recognition\n# ******************************************************\n\nimport time\nfrom functools import wraps\nimport os\nfrom glob import glob\nimport numpy as np\nimport tensorflow as tf\nimport math\n\ndef describe(func):\n    \'\'\' wrap function,to add some descriptions for function and its running time\n    \'\'\'\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        print(func.__name__+\'...\')\n        start = time.time()\n        result = func(*args, **kwargs)\n        end = time.time()\n        print(str(func.__name__+\' in \'+ str(end-start)+\' s\'))\n        return result\n    return wrapper\n\ndef getAttrs(object, name):\n    \'\'\' get attributes for object\n    \'\'\'\n    assert type(name) == list, \'name must be a list\'\n    value = []\n    for n in name:\n        value.append(getattr(object, n, \'None\'))\n    return value\n\ndef setAttrs(object, attrsName, attrsValue):\n    \'\'\' register attributes for this class \'\'\'\n    assert type(attrsName) == list, \'attrsName must be a list\'\n    assert type(attrsValue) == list, \'attrsValue must be a list\'\n    for name, value in zip(attrsName, attrsValue):\n        object.__dict__[name] = value\n\ndef output_to_sequence(lmt, type=\'phn\'):\n    \'\'\' convert the output into sequences of characters or phonemes\n    \'\'\'\n    phn = [\'aa\', \'ae\', \'ah\', \'ao\', \'aw\', \'ax\', \'ax-h\',\n       \'axr\', \'ay\', \'b\', \'bcl\', \'ch\', \'d\', \'dcl\',\n       \'dh\', \'dx\', \'eh\', \'el\', \'em\', \'en\', \'eng\',\n       \'epi\', \'er\', \'ey\', \'f\', \'g\', \'gcl\', \'h#\',\n       \'hh\', \'hv\', \'ih\', \'ix\', \'iy\', \'jh\', \'k\',\n       \'kcl\', \'l\', \'m\', \'n\', \'ng\', \'nx\', \'ow\',\n       \'oy\', \'p\', \'pau\', \'pcl\', \'q\', \'r\', \'s\',\n       \'sh\', \'t\', \'tcl\', \'th\', \'uh\', \'uw\', \'ux\',\n       \'v\', \'w\', \'y\', \'z\', \'zh\']\n    sequences = []\n    start = 0\n    sequences.append([])\n    for i in range(len(lmt[0])):\n        if lmt[0][i][0] == start:\n            sequences[start].append(lmt[1][i])\n        else:\n            start = start + 1\n            sequences.append([])\n\n    #here, we only print the first sequence of batch\n    indexes = sequences[0] #here, we only print the first sequence of batch\n    if type == \'phn\':\n        seq = []\n        for ind in indexes:\n            if ind == len(phn):\n                pass\n            else:\n                seq.append(phn[ind])\n        seq = \' \'.join(seq)\n        return seq\n\n    elif type == \'cha\':\n        seq = []\n        for ind in indexes:\n            if ind == 0:\n                seq.append(\' \')\n            elif ind == 27:\n                seq.append(""\'"")\n            elif ind == 28:\n                pass\n            else:\n                seq.append(chr(ind+96))\n        seq = \'\'.join(seq)\n        return seq\n    else:\n        raise TypeError(\'mode should be phoneme or character\')\n\ndef target2phoneme(target):\n    seq = []\n    for t in target:\n        if t == len(phn):\n            pass\n        else:\n            seq.append(phn[t])\n    seq = \' \'.join(seq)\n    return seq\n\n@describe\ndef logging(model,logfile,errorRate,epoch=0,delta_time=0,mode=\'train\'):\n    \'\'\' log the cost and error rate and time while training or testing\n    \'\'\'\n    if mode != \'train\' and mode != \'test\' and mode != \'config\' and mode != \'dev\':\n        raise TypeError(\'mode should be train or test or config.\')\n    logfile = logfile\n    if mode == \'config\':\n        with open(logfile, ""a"") as myfile:\n            myfile.write(str(model.config)+\'\\n\')\n\n    elif mode == \'train\':\n        with open(logfile, ""a"") as myfile:\n            myfile.write(str(time.strftime(\'%X %x %Z\'))+\'\\n\')\n            myfile.write(""Epoch:""+str(epoch+1)+\' \'+""train error rate:""+str(errorRate)+\'\\n\')\n            myfile.write(""Epoch:""+str(epoch+1)+\' \'+""train time:""+str(delta_time)+\' s\\n\')\n    elif mode == \'test\':\n        logfile = logfile+\'_TEST\'\n        with open(logfile, ""a"") as myfile:\n            myfile.write(str(model.config)+\'\\n\')\n            myfile.write(str(time.strftime(\'%X %x %Z\'))+\'\\n\')\n            myfile.write(""test error rate:""+str(errorRate)+\'\\n\')\n    elif mode == \'dev\':\n        logfile = logfile+\'_DEV\'\n        with open(logfile, ""a"") as myfile:\n            myfile.write(str(model.config)+\'\\n\')\n            myfile.write(str(time.strftime(\'%X %x %Z\'))+\'\\n\')\n            myfile.write(""development error rate:""+str(errorRate)+\'\\n\')\n\n@describe\ndef count_params(model, mode=\'trainable\'):\n    \'\'\' count all parameters of a tensorflow graph\n    \'\'\'\n    if mode == \'all\':\n        num = np.sum([np.product([xi.value for xi in x.get_shape()]) for x in model.var_op])\n    elif mode == \'trainable\':\n        num = np.sum([np.product([xi.value for xi in x.get_shape()]) for x in model.var_trainable_op])\n    else:\n        raise TypeError(\'mode should be all or trainable.\')\n    print(\'number of \'+mode+\' parameters: \'+str(num))\n    return num\n\ndef list_to_sparse_tensor(targetList, level):\n    \'\'\' turn 2-D List to SparseTensor\n    \'\'\'\n    indices = [] #index\n    vals = [] #value\n    assert level == \'phn\' or level == \'cha\', \'type must be phoneme or character, seq2seq will be supported in future\'\n    phn = [\'aa\', \'ae\', \'ah\', \'ao\', \'aw\', \'ax\', \'ax-h\',\\\n       \'axr\', \'ay\', \'b\', \'bcl\', \'ch\', \'d\', \'dcl\',\\\n       \'dh\', \'dx\', \'eh\', \'el\', \'em\', \'en\', \'eng\',\\\n       \'epi\', \'er\', \'ey\', \'f\', \'g\', \'gcl\', \'h#\',\\\n       \'hh\', \'hv\', \'ih\', \'ix\', \'iy\', \'jh\', \'k\',\\\n       \'kcl\', \'l\', \'m\', \'n\', \'ng\', \'nx\', \'ow\',\\\n       \'oy\', \'p\', \'pau\', \'pcl\', \'q\', \'r\', \'s\',\\\n       \'sh\', \'t\', \'tcl\', \'th\', \'uh\', \'uw\', \'ux\',\\\n       \'v\', \'w\', \'y\', \'z\', \'zh\']\n\n    mapping = {\'ah\': \'ax\', \'ax-h\': \'ax\', \'ux\': \'uw\', \'aa\': \'ao\', \'ih\': \'ix\', \\\n               \'axr\': \'er\', \'el\': \'l\', \'em\': \'m\', \'en\': \'n\', \'nx\': \'n\',\\\n               \'eng\': \'ng\', \'sh\': \'zh\', \'hv\': \'hh\', \'bcl\': \'h#\', \'pcl\': \'h#\',\\\n               \'dcl\': \'h#\', \'tcl\': \'h#\', \'gcl\': \'h#\', \'kcl\': \'h#\',\\\n               \'q\': \'h#\', \'epi\': \'h#\', \'pau\': \'h#\'}\n\n    group_phn = [\'ae\', \'ao\', \'aw\', \'ax\', \'ay\', \'b\', \'ch\', \'d\', \'dh\', \'dx\', \'eh\', \\\n                 \'er\', \'ey\', \'f\', \'g\', \'h#\', \'hh\', \'ix\', \'iy\', \'jh\', \'k\', \'l\', \\\n                 \'m\', \'n\', \'ng\', \'ow\', \'oy\', \'p\', \'r\', \'s\', \'t\', \'th\', \'uh\', \'uw\',\\\n                 \'v\', \'w\', \'y\', \'z\', \'zh\']\n\n\n    mapping = {\'ah\': \'ax\', \'ax-h\': \'ax\', \'ux\': \'uw\', \'aa\': \'ao\', \'ih\': \'ix\', \\\n               \'axr\': \'er\', \'el\': \'l\', \'em\': \'m\', \'en\': \'n\', \'nx\': \'n\',\\\n               \'eng\': \'ng\', \'sh\': \'zh\', \'hv\': \'hh\', \'bcl\': \'h#\', \'pcl\': \'h#\',\\\n               \'dcl\': \'h#\', \'tcl\': \'h#\', \'gcl\': \'h#\', \'kcl\': \'h#\',\\\n               \'q\': \'h#\', \'epi\': \'h#\', \'pau\': \'h#\'}\n\n    group_phn = [\'ae\', \'ao\', \'aw\', \'ax\', \'ay\', \'b\', \'ch\', \'d\', \'dh\', \'dx\', \'eh\', \\\n                 \'er\', \'ey\', \'f\', \'g\', \'h#\', \'hh\', \'ix\', \'iy\', \'jh\', \'k\', \'l\', \\\n                 \'m\', \'n\', \'ng\', \'ow\', \'oy\', \'p\', \'r\', \'s\', \'t\', \'th\', \'uh\', \'uw\',\\\n                 \'v\', \'w\', \'y\', \'z\', \'zh\']\n\n    if level == \'cha\':\n        for tI, target in enumerate(targetList):\n            for seqI, val in enumerate(target):\n                indices.append([tI, seqI])\n                vals.append(val)\n        shape = [len(targetList), np.asarray(indices).max(axis=0)[1]+1] #shape\n        return (np.array(indices), np.array(vals), np.array(shape))\n\n    elif level == \'phn\':\n        \'\'\'\n        for phn level, we should collapse 61 labels into 39 labels before scoring\n        \n        Reference:\n          Heterogeneous Acoustic Measurements and Multiple Classifiers for Speech Recognition(1986), \n            Andrew K. Halberstadt, https://groups.csail.mit.edu/sls/publications/1998/phdthesis-drew.pdf\n        \'\'\'\n        for tI, target in enumerate(targetList):\n            for seqI, val in enumerate(target):\n                if val < len(phn) and (phn[val] in mapping.keys()):\n                    val = group_phn.index(mapping[phn[val]])\n                indices.append([tI, seqI])\n                vals.append(val)\n        shape = [len(targetList), np.asarray(indices).max(0)[1]+1] #shape\n        return (np.array(indices), np.array(vals), np.array(shape))\n\n    else:\n        ##support seq2seq in future here\n        raise ValueError(\'Invalid level: %s\'%str(level))\n\ndef get_edit_distance(hyp_arr, truth_arr, normalize, level):\n    \'\'\' calculate edit distance\n    This is very universal, both for cha-level and phn-level\n    \'\'\'\n\n    graph = tf.Graph()\n    with graph.as_default():\n        truth = tf.sparse_placeholder(tf.int32)\n        hyp = tf.sparse_placeholder(tf.int32)\n        editDist = tf.reduce_sum(tf.edit_distance(hyp, truth, normalize=normalize))\n\n    with tf.Session(graph=graph) as session:\n        truthTest = list_to_sparse_tensor(truth_arr, level)\n        hypTest = list_to_sparse_tensor(hyp_arr, level)\n        feedDict = {truth: truthTest, hyp: hypTest}\n        dist = session.run(editDist, feed_dict=feedDict)\n    return dist\n\ndef data_lists_to_batches(inputList, targetList, batchSize, level):\n    \'\'\' padding the input list to a same dimension, integrate all data into batchInputs\n    \'\'\'\n    assert len(inputList) == len(targetList)\n    # dimensions of inputList:batch*39*time_length\n\n    nFeatures = inputList[0].shape[0]\n    maxLength = 0\n    for inp in inputList:\n\t    # find the max time_length\n        maxLength = max(maxLength, inp.shape[1])\n\n    # randIxs is the shuffled index from range(0,len(inputList))\n    randIxs = np.random.permutation(len(inputList))\n    start, end = (0, batchSize)\n    dataBatches = []\n\n    while end <= len(inputList):\n\t    # batchSeqLengths store the time-length of each sample in a mini-batch\n        batchSeqLengths = np.zeros(batchSize)\n\n  \t    # randIxs is the shuffled index of input list\n        for batchI, origI in enumerate(randIxs[start:end]):\n            batchSeqLengths[batchI] = inputList[origI].shape[-1]\n\n        batchInputs = np.zeros((maxLength, batchSize, nFeatures))\n        batchTargetList = []\n        for batchI, origI in enumerate(randIxs[start:end]):\n\t        # padSecs is the length of padding\n            padSecs = maxLength - inputList[origI].shape[1]\n\t        # numpy.pad pad the inputList[origI] with zeos at the tail\n            batchInputs[:,batchI,:] = np.pad(inputList[origI].T, ((0,padSecs),(0,0)), \'constant\', constant_values=0)\n\t        # target label\n            batchTargetList.append(targetList[origI])\n        dataBatches.append((batchInputs, list_to_sparse_tensor(batchTargetList, level), batchSeqLengths))\n        start += batchSize\n        end += batchSize\n    return (dataBatches, maxLength)\n\ndef load_batched_data(mfccPath, labelPath, batchSize, mode, level):\n    \'\'\'returns 3-element tuple: batched data (list), maxTimeLength (int), and\n       total number of samples (int)\'\'\'\n    return data_lists_to_batches([np.load(os.path.join(mfccPath, fn)) for fn in os.listdir(mfccPath)],\n                                 [np.load(os.path.join(labelPath, fn)) for fn in os.listdir(labelPath)],\n                                 batchSize, level) + (len(os.listdir(mfccPath)),)\n\ndef list_dirs(mfcc_dir, label_dir):\n    mfcc_dirs = glob(mfcc_dir)\n    label_dirs = glob(label_dir)\n    for mfcc,label in zip(mfcc_dirs,label_dirs):\n        yield (mfcc,label)\n\ndef batch_norm(x, is_training=True):\n    """""" Batch normalization.\n    """"""\n    with tf.variable_scope(\'BatchNorm\'):\n        inputs_shape = x.get_shape()\n        axis = list(range(len(inputs_shape) - 1))\n        param_shape = inputs_shape[-1:]\n\n        beta = tf.get_variable(\'beta\', param_shape, initializer=tf.constant_initializer(0.))\n        gamma = tf.get_variable(\'gamma\', param_shape, initializer=tf.constant_initializer(1.))\n        batch_mean, batch_var = tf.nn.moments(x, axis)\n        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n\n        def mean_var_with_update():\n            ema_apply_op = ema.apply([batch_mean, batch_var])\n            with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n\n        mean, var = tf.cond(is_training,\n                            mean_var_with_update,\n                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n    return normed\n\ndef _get_dims(shape):\n    """"""get shape for initialization\n    """"""\n    fan_in = shape[0] if len(shape) == 2 else np.prod(shape[:-1])\n    fan_out = shape[1] if len(shape) == 2 else shape[-1]\n    return fan_in, fan_out\n\ndef dropout(x, keep_prob, is_training):\n    """""" Apply dropout to a tensor\n    """"""\n    return tf.contrib.layers.dropout(x, keep_prob=keep_prob, is_training=is_training)\n\n\n'"
speechvalley/utils/visualization.py,0,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-23 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : visualization.py\n# Description  : Some common functions for visualization\n# ******************************************************\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport wave\nimport sys\n\ndef plotWaveform(audio):\n    """"""\n    Plotting the waveform of a wav file\n    """"""\n    spf = wave.open(audio,\'r\')\n    signal = spf.readframes(-1)\n    signal = np.fromstring(signal, \'Int16\')\n    fs = spf.getframerate()\n    if spf.getnchannels() == 2:\n        print \'Just mono files\'\n        sys.exit(0)\n    Time=np.linspace(0, len(signal)/fs, num=len(signal))\n    plt.figure(1)\n    plt.title(\'Waveform of an audio\')\n    plt.plot(Time,signal)\n    plt.show()\n'"
speechvalley/feature/core/__init__.py,0,"b'#-*- coding:utf-8 -*-\n#!/usr/bin/env python3\n"""""" Speech Valley\n\n@author: zzw922cn\n@date: 2017-12-02\n""""""\nfrom speechvalley.feature.core.calcmfcc import calcfeat_delta_delta, calcMFCC\nfrom speechvalley.feature.core.nist2wav import nist2wav\nfrom speechvalley.feature.core.spectrogram import spectrogramPower\n'"
speechvalley/feature/core/calcmfcc.py,0,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : calcmfcc.py\n# Description  : Calculating MFCC feature for Automatic Speech Recognition\n# ******************************************************\nimport numpy\nfrom scipy.fftpack import dct\n\nfrom speechvalley.feature.core.sigprocess import audio2frame, pre_emphasis, spectrum_power\ntry:\n    xrange(1)\nexcept:\n    xrange=range\n\n\n\ndef calcfeat_delta_delta(signal,samplerate=16000,win_length=0.025,win_step=0.01,filters_num=26,NFFT=512,low_freq=0,high_freq=None,pre_emphasis_coeff=0.97,cep_lifter=22,appendEnergy=True,mode=\'mfcc\',feature_len=13):\n    """"""Calculate features, fist order difference, and second order difference coefficients.\n        13 Mel-Frequency Cepstral Coefficients(MFCC), 13 first order difference\n       coefficients, and 13 second order difference coefficients. There are 39 features\n       in total.\n\n    Args:\n        signal: 1-D numpy array.\n        samplerate: Sampling rate. Defaulted to 16KHz.\n        win_length: Window length. Defaulted to 0.025, which is 25ms/frame.\n        win_step: Interval between the start points of adjacent frames.\n            Defaulted to 0.01, which is 10ms.\n        feature_len: Numbers of features. Defaulted to 13.\n        filters_num: Numbers of filters. Defaulted to 26.\n        NFFT: Size of FFT. Defaulted to 512.\n        low_freq: Lowest frequency.\n        high_freq: Highest frequency.\n        pre_emphasis_coeff: Coefficient for pre-emphasis. Pre-emphasis increase\n            the energy of signal at higher frequency. Defaulted to 0.97.\n        cep_lifter: Numbers of lifter for cepstral. Defaulted to 22.\n        appendEnergy: Wheter to append energy. Defaulted to True.\n        mode: \'mfcc\' or \'fbank\'.\n            \'mfcc\': Mel-Frequency Cepstral Coefficients(MFCC).\n                    Complete process: Mel filtering -> log -> DCT.\n            \'fbank\': Apply Mel filtering -> log.\n\n    Returns:\n        2-D numpy array with shape:(NUMFRAMES, 39). In each frame, coefficients are\n            concatenated in (feature, delta features, delta delta feature) way.\n    """"""\n    filters_num = 2*feature_len\n    feat = calcMFCC(signal,samplerate,win_length,win_step,feature_len,filters_num,NFFT,low_freq,high_freq,pre_emphasis_coeff,cep_lifter,appendEnergy,mode=mode)   #\xe9\xa6\x96\xe5\x85\x88\xe8\x8e\xb7\xe5\x8f\x9613\xe4\xb8\xaa\xe4\xb8\x80\xe8\x88\xacMFCC\xe7\xb3\xbb\xe6\x95\xb0\n    feat_delta = delta(feat)\n    feat_delta_delta = delta(feat_delta)\n\n    result = numpy.concatenate((feat,feat_delta,feat_delta_delta),axis=1)\n    return result\n\ndef delta(feat, N=2):\n    """"""Compute delta features from a feature vector sequence.\n\n    Args:\n        feat: A numpy array of size (NUMFRAMES by number of features) containing features. Each row holds 1 feature vector.\n        N: For each frame, calculate delta features based on preceding and following N frames.\n    Returns:\n        A numpy array of size (NUMFRAMES by number of features) containing delta features. Each row holds 1 delta feature vector.\n    """"""\n    NUMFRAMES = len(feat)\n    feat = numpy.concatenate(([feat[0] for i in range(N)], feat, [feat[-1] for i in range(N)]))\n    denom = sum([2*i*i for i in range(1,N+1)])\n    dfeat = []\n    for j in range(NUMFRAMES):\n        dfeat.append(numpy.sum([n*feat[N+j+n] for n in range(-1*N,N+1)], axis=0)/denom)\n    return dfeat\n\ndef calcMFCC(signal,samplerate=16000,win_length=0.025,win_step=0.01,feature_len=13,filters_num=26,NFFT=512,low_freq=0,high_freq=None,pre_emphasis_coeff=0.97,cep_lifter=22,appendEnergy=True,mode=\'mfcc\'):\n    """"""Caculate Features.\n    Args:\n        signal: 1-D numpy array.\n        samplerate: Sampling rate. Defaulted to 16KHz.\n        win_length: Window length. Defaulted to 0.025, which is 25ms/frame.\n        win_step: Interval between the start points of adjacent frames.\n            Defaulted to 0.01, which is 10ms.\n        feature_len: Numbers of features. Defaulted to 13.\n        filters_num: Numbers of filters. Defaulted to 26.\n        NFFT: Size of FFT. Defaulted to 512.\n        low_freq: Lowest frequency.\n        high_freq: Highest frequency.\n        pre_emphasis_coeff: Coefficient for pre-emphasis. Pre-emphasis increase\n            the energy of signal at higher frequency. Defaulted to 0.97.\n        cep_lifter: Numbers of lifter for cepstral. Defaulted to 22.\n        appendEnergy: Wheter to append energy. Defaulted to True.\n        mode: \'mfcc\' or \'fbank\'.\n            \'mfcc\': Mel-Frequency Cepstral Coefficients(MFCC).\n                    Complete process: Mel filtering -> log -> DCT.\n            \'fbank\': Apply Mel filtering -> log.\n\n    Returns:\n        2-D numpy array with shape (NUMFRAMES, features). Each frame containing feature_len of features.\n    """"""\n    filters_num = 2*feature_len\n    feat,energy=fbank(signal,samplerate,win_length,win_step,filters_num,NFFT,low_freq,high_freq,pre_emphasis_coeff)\n    feat=numpy.log(feat)\n    # Performing DCT and get first 13 coefficients\n    if mode == \'mfcc\':\n        feat=dct(feat,type=2,axis=1,norm=\'ortho\')[:,:feature_len]\n        feat=lifter(feat,cep_lifter)\n    elif mode == \'fbank\':\n        feat = feat[:,:feature_len]\n    if appendEnergy:\n        # Replace the first coefficient with logE and get 2-13 coefficients.\n        feat[:,0]=numpy.log(energy)\n    return feat\n\ndef fbank(signal,samplerate=16000,win_length=0.025,win_step=0.01,filters_num=26,NFFT=512,low_freq=0,high_freq=None,pre_emphasis_coeff=0.97):\n    """"""Perform pre-emphasis -> framing -> get magnitude -> FFT -> Mel Filtering.\n    Args:\n        signal: 1-D numpy array.\n        samplerate: Sampling rate. Defaulted to 16KHz.\n        win_length: Window length. Defaulted to 0.025, which is 25ms/frame.\n        win_step: Interval between the start points of adjacent frames.\n            Defaulted to 0.01, which is 10ms.\n        cep_num: Numbers of cepstral coefficients. Defaulted to 13.\n        filters_num: Numbers of filters. Defaulted to 26.\n        NFFT: Size of FFT. Defaulted to 512.\n        low_freq: Lowest frequency.\n        high_freq: Highest frequency.\n        pre_emphasis_coeff: Coefficient for pre-emphasis. Pre-emphasis increase\n            the energy of signal at higher frequency. Defaulted to 0.97.\n    Returns:\n        feat: Features.\n        energy: Energy.\n    """"""\n    # Calculate the highest frequency.\n    high_freq=high_freq or samplerate/2\n    # Pre-emphasis\n    signal=pre_emphasis(signal,pre_emphasis_coeff)\n    # rames: 2-D numpy array with shape (frame_num, frame_length)\n    frames=audio2frame(signal,win_length*samplerate,win_step*samplerate)\n    # Caculate energy and modify all zeros to eps.\n    spec_power=spectrum_power(frames,NFFT)\n    energy=numpy.sum(spec_power,1)\n    energy=numpy.where(energy==0,numpy.finfo(float).eps,energy)\n    # Get Mel filter banks.\n    fb=get_filter_banks(filters_num,NFFT,samplerate,low_freq,high_freq)\n    # Get MFCC and modify all zeros to eps.\n    feat=numpy.dot(spec_power,fb.T)\n    feat=numpy.where(feat==0,numpy.finfo(float).eps,feat)\n\n    return feat,energy\n\ndef log_fbank(signal,samplerate=16000,win_length=0.025,win_step=0.01,filters_num=26,NFFT=512,low_freq=0,high_freq=None,pre_emphasis_coeff=0.97):\n    """"""Calculate log of features.\n    """"""\n    feat,energy=fbank(signal,samplerate,win_length,win_step,filters_num,NFFT,low_freq,high_freq,pre_emphasis_coeff)\n    return numpy.log(feat)\n\ndef ssc(signal,samplerate=16000,win_length=0.025,win_step=0.01,filters_num=26,NFFT=512,low_freq=0,high_freq=None,pre_emphasis_coeff=0.97):\n    \'\'\'\n    \xe5\xbe\x85\xe8\xa1\xa5\xe5\x85\x85\n    \'\'\'\n    high_freq=high_freq or samplerate/2\n    signal=pre_emphasis(signal,pre_emphasis_coeff)\n    frames=audio2frame(signal,win_length*samplerate,win_step*samplerate)\n    spec_power=spectrum_power(frames,NFFT)\n    spec_power=numpy.where(spec_power==0,numpy.finfo(float).eps,spec_power) #\xe8\x83\xbd\xe9\x87\x8f\xe8\xb0\xb1\n    fb=get_filter_banks(filters_num,NFFT,samplerate,low_freq,high_freq)\n    feat=numpy.dot(spec_power,fb.T)  #\xe8\xae\xa1\xe7\xae\x97\xe8\x83\xbd\xe9\x87\x8f\n    R=numpy.tile(numpy.linspace(1,samplerate/2,numpy.size(spec_power,1)),(numpy.size(spec_power,0),1))\n    return numpy.dot(spec_power*R,fb.T)/feat\n\ndef hz2mel(hz):\n    """"""Convert frequency to Mel frequency.\n    Args:\n        hz: Frequency.\n    Returns:\n        Mel frequency.\n    """"""\n    return 2595*numpy.log10(1+hz/700.0)\n\ndef mel2hz(mel):\n    """"""Convert Mel frequency to frequency.\n    Args:\n        mel:Mel frequency\n    Returns:\n        Frequency.\n    """"""\n    return 700*(10**(mel/2595.0)-1)\n\ndef get_filter_banks(filters_num=20,NFFT=512,samplerate=16000,low_freq=0,high_freq=None):\n    \'\'\'Calculate Mel filter banks.\n    Args:\n        filters_num: Numbers of Mel filters.\n        NFFT:FFT size. Defaulted to 512.\n        samplerate: Sampling rate. Defaulted to 16KHz.\n        low_freq: Lowest frequency.\n        high_freq: Highest frequency.\n    \'\'\'\n    # Convert frequency to Mel frequency.\n    low_mel=hz2mel(low_freq)\n    high_mel=hz2mel(high_freq)\n    # Insert filters_num of points between low_mel and high_mel. In total there are filters_num+2 points\n    mel_points=numpy.linspace(low_mel,high_mel,filters_num+2)\n    # Convert Mel frequency to frequency and find corresponding position.\n    hz_points=mel2hz(mel_points)\n    # Find corresponding position of these hz_points in fft.\n    bin=numpy.floor((NFFT+1)*hz_points/samplerate)\n    # Build Mel filters\' expression.First and third points of each filter are zeros.\n    fbank=numpy.zeros([filters_num,NFFT/2+1])\n    for j in xrange(0,filters_num):\n        for i in xrange(int(bin[j]),int(bin[j+1])):\n            fbank[j,i]=(i-bin[j])/(bin[j+1]-bin[j])\n        for i in xrange(int(bin[j+1]),int(bin[j+2])):\n            fbank[j,i]=(bin[j+2]-i)/(bin[j+2]-bin[j+1])\n    return fbank\n\ndef lifter(cepstra,L=22):\n    \'\'\'Lifter function.\n    Args:\n        cepstra: MFCC coefficients.\n        L: Numbers of lifters. Defaulted to 22.\n    \'\'\'\n    if L>0:\n        nframes,ncoeff=numpy.shape(cepstra)\n        n=numpy.arange(ncoeff)\n        lift=1+(L/2)*numpy.sin(numpy.pi*n/L)\n        return lift*cepstra\n    else:\n        return cepstra\n'"
speechvalley/feature/core/nist2wav.py,0,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : nist2wav.py\n# Description  : Converting nist format to wav format for Automatic Speech Recognition\n# ******************************************************\n\n\nimport subprocess\nimport os\n\ndef nist2wav(src_dir):\n    count = 0\n    for subdir, dirs, files in os.walk(src_dir):\n        for f in files:\n            fullFilename = os.path.join(subdir, f)\n            if f.endswith(\'.wv1\') or f.endswith(\'.wv2\'):\n                count += 1\n                os.system(""./sph2pipe_v2.5/sph2pipe ""+fullFilename+"" -f rif "" +fullFilename+"".wav"")\n                print(fullFilename)\n\nif __name__ == \'__main__\':\n    nist2wav(\'/home/pony/wsj/\')\n'"
speechvalley/feature/core/sigprocess.py,0,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : sigprocess.py\n# Description  : Handling signal\n# ******************************************************\n\nimport numpy\nimport math\n\ndef audio2frame(signal,frame_length,frame_step,winfunc=lambda x:numpy.ones((x,))):\n    """""" Framing audio signal. Uses numbers of samples as unit.\n\n    Args:\n    signal: 1-D numpy array.\n\tframe_length: In this situation, frame_length=samplerate*win_length, since we\n        use numbers of samples as unit.\n    frame_step:In this situation, frame_step=samplerate*win_step,\n        representing the number of samples between the start point of adjacent frames.\n\twinfunc:lambda function, to generate a vector with shape (x,) filled with ones.\n\n    Returns:\n        frames*win: 2-D numpy array with shape (frames_num, frame_length).\n    """"""\n    signal_length=len(signal)\n    # Use round() to ensure length and step are integer, considering that we use numbers\n    # of samples as unit.\n    frame_length=int(round(frame_length))\n    frame_step=int(round(frame_step))\n    if signal_length<=frame_length:\n        frames_num=1\n    else:\n        frames_num=1+int(math.ceil((1.0*signal_length-frame_length)/frame_step))\n    pad_length=int((frames_num-1)*frame_step+frame_length)\n    # Padding zeros at the end of signal if pad_length > signal_length.\n    zeros=numpy.zeros((pad_length-signal_length,))\n    pad_signal=numpy.concatenate((signal,zeros))\n    # Calculate the indice of signal for every sample in frames, shape (frams_nums, frams_length)\n    indices=numpy.tile(numpy.arange(0,frame_length),(frames_num,1))+numpy.tile(\n        numpy.arange(0,frames_num*frame_step,frame_step),(frame_length,1)).T\n    indices=numpy.array(indices,dtype=numpy.int32)\n    # Get signal data according to indices.\n    frames=pad_signal[indices]\n    win=numpy.tile(winfunc(frame_length),(frames_num,1))\n    return frames*win\n\ndef deframesignal(frames,signal_length,frame_length,frame_step,winfunc=lambda x:numpy.ones((x,))):\n    \'\'\'\xe5\xae\x9a\xe4\xb9\x89\xe5\x87\xbd\xe6\x95\xb0\xe5\xaf\xb9\xe5\x8e\x9f\xe4\xbf\xa1\xe5\x8f\xb7\xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\x80\xe5\xb8\xa7\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8f\x98\xe6\x8d\xa2\xef\xbc\x8c\xe5\xba\x94\xe8\xaf\xa5\xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe6\xb6\x88\xe9\x99\xa4\xe5\x85\xb3\xe8\x81\x94\xe6\x80\xa7\n    \xe5\x8f\x82\xe6\x95\xb0\xe5\xae\x9a\xe4\xb9\x89\xef\xbc\x9a\n    frames:audio2frame\xe5\x87\xbd\xe6\x95\xb0\xe8\xbf\x94\xe5\x9b\x9e\xe7\x9a\x84\xe5\xb8\xa7\xe7\x9f\xa9\xe9\x98\xb5\n    signal_length:\xe4\xbf\xa1\xe5\x8f\xb7\xe9\x95\xbf\xe5\xba\xa6\n    frame_length:\xe5\xb8\xa7\xe9\x95\xbf\xe5\xba\xa6\n    frame_step:\xe5\xb8\xa7\xe9\x97\xb4\xe9\x9a\x94\n    winfunc:\xe5\xaf\xb9\xe6\xaf\x8f\xe4\xb8\x80\xe5\xb8\xa7\xe5\x8a\xa0window\xe5\x87\xbd\xe6\x95\xb0\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe6\x9e\x90\xef\xbc\x8c\xe9\xbb\x98\xe8\xae\xa4\xe6\xad\xa4\xe5\xa4\x84\xe4\xb8\x8d\xe5\x8a\xa0window\n    \'\'\'\n    #\xe5\xaf\xb9\xe5\x8f\x82\xe6\x95\xb0\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8f\x96\xe6\x95\xb4\xe6\x93\x8d\xe4\xbd\x9c\n    signal_length=round(signal_length) #\xe4\xbf\xa1\xe5\x8f\xb7\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\n    frame_length=round(frame_length) #\xe5\xb8\xa7\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\n    frames_num=numpy.shape(frames)[0] #\xe5\xb8\xa7\xe7\x9a\x84\xe6\x80\xbb\xe6\x95\xb0\n    assert numpy.shape(frames)[1]==frame_length,\'""frames""\xe7\x9f\xa9\xe9\x98\xb5\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\x8d\xe6\xad\xa3\xe7\xa1\xae\xef\xbc\x8c\xe5\xae\x83\xe7\x9a\x84\xe5\x88\x97\xe6\x95\xb0\xe5\xba\x94\xe8\xaf\xa5\xe7\xad\x89\xe4\xba\x8e\xe4\xb8\x80\xe5\xb8\xa7\xe9\x95\xbf\xe5\xba\xa6\'  #\xe5\x88\xa4\xe6\x96\xadframes\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x8c\xe5\xa4\x84\xe7\x90\x86\xe5\xbc\x82\xe5\xb8\xb8\n    indices=numpy.tile(numpy.arange(0,frame_length),(frames_num,1))+numpy.tile(numpy.arange(0,frames_num*frame_step,frame_step),(frame_length,1)).T  #\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe5\xaf\xb9\xe6\x89\x80\xe6\x9c\x89\xe5\xb8\xa7\xe7\x9a\x84\xe6\x97\xb6\xe9\x97\xb4\xe7\x82\xb9\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x8a\xbd\xe5\x8f\x96\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0frames_num*frame_length\xe9\x95\xbf\xe5\xba\xa6\xe7\x9a\x84\xe7\x9f\xa9\xe9\x98\xb5\n    indices=numpy.array(indices,dtype=numpy.int32)\n    pad_length=(frames_num-1)*frame_step+frame_length #\xe9\x93\xba\xe5\xb9\xb3\xe5\x90\x8e\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe4\xbf\xa1\xe5\x8f\xb7\n    if signal_length<=0:\n        signal_length=pad_length\n    recalc_signal=numpy.zeros((pad_length,)) #\xe8\xb0\x83\xe6\x95\xb4\xe5\x90\x8e\xe7\x9a\x84\xe4\xbf\xa1\xe5\x8f\xb7\n    window_correction=numpy.zeros((pad_length,1)) #\xe7\xaa\x97\xe5\x85\xb3\xe8\x81\x94\n    win=winfunc(frame_length)\n    for i in range(0,frames_num):\n        window_correction[indices[i,:]]=window_correction[indices[i,:]]+win+1e-15 #\xe8\xa1\xa8\xe7\xa4\xba\xe4\xbf\xa1\xe5\x8f\xb7\xe7\x9a\x84\xe9\x87\x8d\xe5\x8f\xa0\xe7\xa8\x8b\xe5\xba\xa6\n        recalc_signal[indices[i,:]]=recalc_signal[indices[i,:]]+frames[i,:] #\xe5\x8e\x9f\xe4\xbf\xa1\xe5\x8f\xb7\xe5\x8a\xa0\xe4\xb8\x8a\xe9\x87\x8d\xe5\x8f\xa0\xe7\xa8\x8b\xe5\xba\xa6\xe6\x9e\x84\xe6\x88\x90\xe8\xb0\x83\xe6\x95\xb4\xe5\x90\x8e\xe7\x9a\x84\xe4\xbf\xa1\xe5\x8f\xb7\n    recalc_signal=recalc_signal/window_correction #\xe6\x96\xb0\xe7\x9a\x84\xe8\xb0\x83\xe6\x95\xb4\xe5\x90\x8e\xe7\x9a\x84\xe4\xbf\xa1\xe5\x8f\xb7\xe7\xad\x89\xe4\xba\x8e\xe8\xb0\x83\xe6\x95\xb4\xe4\xbf\xa1\xe5\x8f\xb7\xe5\xa4\x84\xe4\xbb\xa5\xe6\xaf\x8f\xe5\xa4\x84\xe7\x9a\x84\xe9\x87\x8d\xe5\x8f\xa0\xe7\xa8\x8b\xe5\xba\xa6\n    return recalc_signal[0:signal_length] #\xe8\xbf\x94\xe5\x9b\x9e\xe8\xaf\xa5\xe6\x96\xb0\xe7\x9a\x84\xe8\xb0\x83\xe6\x95\xb4\xe4\xbf\xa1\xe5\x8f\xb7\n\ndef spectrum_magnitude(frames,NFFT):\n    \'\'\'Apply FFT and Calculate magnitude of the spectrum.\n    Args:\n        frames: 2-D frames array calculated by audio2frame(...).\n        NFFT:FFT size.\n    Returns:\n        Return magnitude of the spectrum after FFT, with shape (frames_num, NFFT).\n    \'\'\'\n    complex_spectrum=numpy.fft.rfft(frames,NFFT)\n    return numpy.absolute(complex_spectrum)\n\ndef spectrum_power(frames,NFFT):\n    """"""Calculate power spectrum for every frame after FFT.\n    Args:\n        frames: 2-D frames array calculated by audio2frame(...).\n        NFFT:FFT size\n    Returns:\n        Power spectrum: PS = magnitude^2/NFFT\n    """"""\n    return 1.0/NFFT * numpy.square(spectrum_magnitude(frames,NFFT))\n\ndef log_spectrum_power(frames,NFFT,norm=1):\n    \'\'\'Calculate log power spectrum.\n    Args:\n        frames:2-D frames array calculated by audio2frame(...)\n        NFFT\xef\xbc\x9aFFT size\n        norm: Norm.\n    \'\'\'\n    spec_power=spectrum_power(frames,NFFT)\n    # In case of calculating log0, we set 0 in spec_power to 0.\n    spec_power[spec_power<1e-30]=1e-30\n    log_spec_power=10*numpy.log10(spec_power)\n    if norm:\n        return log_spec_power-numpy.max(log_spec_power)\n    else:\n        return log_spec_power\n\ndef pre_emphasis(signal,coefficient=0.95):\n    \'\'\'Pre-emphasis.\n    Args:\n        signal: 1-D numpy array.\n        coefficient:Coefficient for pre-emphasis. Defauted to 0.95.\n    Returns:\n        pre-emphasis signal.\n    \'\'\'\n    return numpy.append(signal[0],signal[1:]-coefficient*signal[:-1])\n\n\n\n'"
speechvalley/feature/core/spectrogram.py,0,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : spectrogram.py\n# Description  : Calculating spectrogram for Automatic Speech Recognition\n# ******************************************************\n\nimport numpy as np\nimport scipy.io.wavfile as wav\nimport librosa\nfrom sklearn import preprocessing\n\ndef spectrogramPower(audio, window_size=0.02, window_stride=0.01):\n    """""" short time fourier transform\n\n    Details:\n        audio - This is the input time-domain signal you wish to find the spectrogram of. It can\'t get much simpler than that. In your case, the \n                signal you want to find the spectrogram of is defined in the following code:\n\n        win_length - If you recall, we decompose the image into chunks, and each chunk has a specified width.  window defines the width of each \n                 chunkin terms of samples. As this is a discrete-time signal, you know that this signal was sampled with a particular sampling \n                 frequency and sampling period. You can determine how large the window is in terms of samples by:\n\n                 window_samples = window_time/Ts\n        hop_length - the same as stride in convolution network, overlapping width\n\n    """"""\n    samplingRate, samples = wav.read(audio)\n    win_length = int(window_size * samplingRate)\n    hop_length = int(window_stride * samplingRate)\n    n_fft = win_length\n    D = librosa.core.stft(samples, n_fft=n_fft,hop_length=hop_length,\n                      win_length=win_length)\n    mag = np.abs(D)\n    log_mag = np.log1p(mag)\n    # normalization\n    log_mag = preprocessing.scale(log_mag)\n    # size: frequency_bins*time_len\n    return log_mag\n    \n    \nif __name__ == \'__main__\':\n    print(np.shape(spectrogramPower(\'test.wav\')))\n'"
speechvalley/feature/libri/__init__.py,0,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : __init__.py\n# Description  : Feature preprocessing for LibriSpeech dataset\n# ******************************************************\n\nfrom speechvalley.feature.libri.libri_preprocess import preprocess, wav2feature\n'"
speechvalley/feature/libri/libri_preprocess.py,0,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : libri_preprocess.py\n# Description  : Feature preprocessing for LibriSpeech dataset\n# ******************************************************\n\nimport os\nimport glob\nimport sklearn\nimport argparse\nimport numpy as np\nimport scipy.io.wavfile as wav\nfrom sklearn import preprocessing\nfrom subprocess import check_call, CalledProcessError\nfrom speechvalley.feature.core import calcfeat_delta_delta\n\ndef preprocess(root_directory):\n    """"""\n    Function to walk through the directory and convert flac to wav files\n    """"""\n    try:\n        check_call([\'flac\'])\n    except OSError:\n        raise OSError(""""""Flac not installed. Install using apt-get install flac"""""")\n    for subdir, dirs, files in os.walk(root_directory):\n        for f in files:\n            filename = os.path.join(subdir, f)\n            if f.endswith(\'.flac\'):\n                try:\n                    check_call([\'flac\', \'-d\', filename])\n                    os.remove(filename)\n                except CalledProcessError as e:\n                    print(""Failed to convert file {}"".format(filename))\n            elif f.endswith(\'.TXT\'):\n                os.remove(filename)\n            elif f.endswith(\'.txt\'):\n                with open(filename, \'r\') as fp:\n                    lines = fp.readlines()\n                    for line in lines:\n                        sub_n = line.split(\' \')[0] + \'.label\'\n                        subfile = os.path.join(subdir, sub_n)\n                        sub_c = \' \'.join(line.split(\' \')[1:])\n                        sub_c = sub_c.lower()\n                        with open(subfile, \'w\') as sp:\n                            sp.write(sub_c)\n            elif f.endswith(\'.wav\'):\n                if not os.path.isfile(os.path.splitext(filename)[0] +\n                                      \'.label\'):\n                    raise ValueError("".label file not found for {}"".format(filename))\n            else:\n                pass\n\n\ndef wav2feature(root_directory, save_directory, name, win_len, win_step, mode, feature_len, seq2seq, save):\n    count = 0\n    dirid = 0\n    level = \'cha\' if seq2seq is False else \'seq2seq\'\n    data_dir = os.path.join(root_directory, name)\n    preprocess(data_dir)\n    for subdir, dirs, files in os.walk(data_dir):\n        for f in files:\n            fullFilename = os.path.join(subdir, f)\n            filenameNoSuffix =  os.path.splitext(fullFilename)[0]\n            if f.endswith(\'.wav\'):\n                rate = None\n                sig = None\n                try:\n                    (rate,sig)= wav.read(fullFilename)\n                except ValueError as e:\n                    if e.message == ""File format \'NIST\'... not understood."":\n                        sf = Sndfile(fullFilename, \'r\')\n                    nframes = sf.nframes\n                    sig = sf.read_frames(nframes)\n                    rate = sf.samplerate\n                feat = calcfeat_delta_delta(sig,rate,win_length=win_len,win_step=win_step,mode=mode,feature_len=feature_len)\n                feat = preprocessing.scale(feat)\n                feat = np.transpose(feat)\n                print(feat.shape)\n                labelFilename = filenameNoSuffix + \'.label\'\n                with open(labelFilename,\'r\') as f:\n                    characters = f.readline().strip().lower()\n                targets = []\n                if seq2seq is True:\n                    targets.append(28)\n                for c in characters:\n                    if c == \' \':\n                        targets.append(0)\n                    elif c == ""\'"":\n                        targets.append(27)\n                    else:\n                        targets.append(ord(c)-96)\n                if seq2seq is True:\n                    targets.append(29)\n                print(targets)\n                if save:\n                    count+=1\n                    if count%4000 == 0:\n                        dirid += 1\n                    print(\'file index:\',count)\n                    print(\'dir index:\',dirid)\n                    label_dir = os.path.join(save_directory, level, name, str(dirid), \'label\')\n                    feat_dir = os.path.join(save_directory, level, name, str(dirid), \'feature\')\n                    if not os.path.isdir(label_dir):\n                        os.makedirs(label_dir)\n                    if not os.path.isdir(feat_dir):\n                        os.makedirs(feat_dir)\n                    featureFilename = os.path.join(feat_dir, filenameNoSuffix.split(\'/\')[-1] +\'.npy\')\n                    np.save(featureFilename,feat)\n                    t_f = os.path.join(label_dir, filenameNoSuffix.split(\'/\')[-1] +\'.npy\')\n                    print(t_f)\n                    np.save(t_f,targets)\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(prog=\'libri_preprocess\',\n                                     description=\'Script to preprocess libri data\')\n    parser.add_argument(""path"", help=""Directory of LibriSpeech dataset"", type=str)\n\n    parser.add_argument(""save"", help=""Directory where preprocessed arrays are to be saved"",\n                        type=str)\n    parser.add_argument(""-n"", ""--name"", help=""Name of the dataset"",\n                        choices=[\'dev-clean\', \'dev-other\', \'test-clean\',\n                                 \'test-other\', \'train-clean-100\', \'train-clean-360\',\n                                 \'train-other-500\'], type=str, default=\'dev-clean\')\n\n    parser.add_argument(""-m"", ""--mode"", help=""Mode"",\n                        choices=[\'mfcc\', \'fbank\'],\n                        type=str, default=\'mfcc\')\n    parser.add_argument(""--featlen"", help=\'Features length\', type=int, default=13)\n    parser.add_argument(""-s"", ""--seq2seq"", default=False,\n                        help=""set this flag to use seq2seq"", action=""store_true"")\n\n    parser.add_argument(""-wl"", ""--winlen"", type=float,\n                        default=0.02, help=""specify the window length of feature"")\n\n    parser.add_argument(""-ws"", ""--winstep"", type=float,\n                        default=0.01, help=""specify the window step length of feature"")\n\n    args = parser.parse_args()\n    root_directory = args.path\n    save_directory = args.save\n    mode = args.mode\n    feature_len = args.featlen\n    seq2seq = args.seq2seq\n    name = args.name\n    win_len = args.winlen\n    win_step = args.winstep\n\n    if root_directory == \'.\':\n        root_directory = os.getcwd()\n\n    if save_directory == \'.\':\n        save_directory = os.getcwd()\n\n    if not os.path.isdir(root_directory):\n        raise ValueError(""LibriSpeech Directory does not exist!"")\n\n    if not os.path.isdir(save_directory):\n        os.makedirs(save_directory)\n\n    wav2feature(root_directory, save_directory, name=name, win_len=win_len, win_step=win_step,\n                mode=mode, feature_len=feature_len, seq2seq=seq2seq, save=True)\n'"
speechvalley/feature/madarian/__init__.py,0,b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : __init__.py\n# Description  : Feature preprocessing for some Madarian dataset\n# ******************************************************\n\nfrom speechvalley.feature.madarian.digit2character import convertDigit2Character \nfrom speechvalley.feature.madarian.character2digit import convertCharacter2Digit \n'
speechvalley/feature/madarian/character2digit.py,0,"b""# encoding: utf-8\r\n# ******************************************************\r\n# Author       : deepxuexi, zzw922cn\r\n# Last modified: 2017-02-01 11:00\r\n# Email        : zzw922cn@gmail.com\r\n# Filename     : madarian_preprocess.py\r\n# Description  : Feature preprocessing for some Madarian dataset\r\n# ******************************************************\r\n\r\nimport codecs\r\nimport re\r\n\r\ndef _c2n(c_str):\r\n    '''\r\n    \xe5\xb0\x86\xe6\xb1\x89\xe5\xad\x97\xe8\xbd\xac\xe5\x8c\x96\xe6\x88\x90\xe6\x95\xb0\xe5\xad\x97\r\n    '''\r\n    if c_str=='':\r\n        return u'0'\r\n    src=u'\xe7\x82\xb9\xe9\x9b\xb6\xe4\xb8\x80\xe4\xba\x8c\xe4\xb8\x89\xe5\x9b\x9b\xe4\xba\x94\xe5\x85\xad\xe4\xb8\x83\xe5\x85\xab\xe4\xb9\x9d'\r\n    dst=u'.0123456789'\r\n    for i, c in enumerate(src):\r\n        c_str=c_str.replace(c,dst[i])\r\n    return c_str\r\n\r\ndef _get_gewei(c_str):\r\n    '''\r\n    \xe5\x88\x86\xe5\x89\xb2\xe5\x87\xba\xe4\xb8\xaa\xe4\xbd\x8d\xe6\x95\xb0\xe5\xad\x97\r\n    '''\r\n    if u'\xe7\x99\xbe\xe9\x9b\xb6' in c_str:\r\n        return _c2n(c_str.split(u'\xe7\x99\xbe\xe9\x9b\xb6')[1])\r\n    elif u'\xe5\x8d\x81' in c_str:\r\n        return _c2n(c_str.split(u'\xe5\x8d\x81')[1])\r\n    elif u'\xe5\x8d\x83\xe9\x9b\xb6' in c_str:\r\n        return _c2n(c_str.split(u'\xe5\x8d\x83\xe9\x9b\xb6')[1])\r\n    else:\r\n        return '0'\r\n\r\ndef _get_shiwei(c_str):\r\n    '''\r\n    \xe5\x88\x86\xe5\x89\xb2\xe5\x87\xba\xe5\x8d\x81\xe4\xbd\x8d\xe6\x95\xb0\xe5\xad\x97\r\n    '''\r\n    if u'\xe7\x99\xbe\xe9\x9b\xb6' in c_str:\r\n        return u'0'\r\n    elif u'\xe7\x99\xbe' in c_str:\r\n        return _c2n(c_str.split(u'\xe7\x99\xbe')[1].split(u'\xe5\x8d\x81')[0])\r\n    elif u'\xe5\x8d\x83\xe9\x9b\xb6' in c_str and u'\xe5\x8d\x81' in c_str:\r\n        return _c2n(c_str.split(u'\xe5\x8d\x83\xe9\x9b\xb6')[1].split(u'\xe5\x8d\x81')[0])\r\n    elif u'\xe5\x8d\x81' in c_str:\r\n        if c_str.split(u'\xe5\x8d\x81')[0]=='':\r\n            return u'1'\r\n        return _c2n(c_str.split(u'\xe5\x8d\x81')[0])\r\n    else:\r\n        return u'0'\r\n\r\ndef _get_baiwei(c_str):\r\n    '''\r\n    \xe5\x88\x86\xe5\x89\xb2\xe5\x87\xba\xe7\x99\xbe\xe4\xbd\x8d\xe6\x95\xb0\xe5\xad\x97\r\n    '''\r\n    if u'\xe5\x8d\x83\xe9\x9b\xb6' in c_str:\r\n        return u'0'\r\n    elif u'\xe5\x8d\x83' in c_str:\r\n        return _c2n(c_str.split(u'\xe5\x8d\x83')[1].split(u'\xe7\x99\xbe')[0])\r\n    elif u'\xe7\x99\xbe' in c_str:\r\n        return _c2n(c_str.split(u'\xe7\x99\xbe')[0])\r\n    else:\r\n        return ''\r\n\r\ndef _get_qianwei(c_str):\r\n    '''\r\n    \xe5\x88\x86\xe5\x89\xb2\xe5\x87\xba\xe5\x8d\x83\xe4\xbd\x8d\xe6\x95\xb0\xe5\xad\x97\r\n    '''\r\n    if u'\xe4\xb8\x87\xe9\x9b\xb6' in c_str:\r\n        return u'0'\r\n    elif u'\xe4\xb8\x87' in c_str:\r\n        return _c2n(c_str.split(u'\xe4\xb8\x87')[1].split(u'\xe5\x8d\x83')[0])\r\n    elif u'\xe5\x8d\x83' in c_str:\r\n        return _c2n(c_str.split(u'\xe5\x8d\x83')[0])\r\n    else:\r\n        return ''\r\n\r\ndef _get_complex(c_str):\r\n    gewei = _get_gewei(c_str)\r\n    shiwei = _get_shiwei(c_str)\r\n    baiwei = _get_baiwei(c_str)\r\n    qianwei = _get_qianwei(c_str)\r\n    c_str = qianwei+baiwei+shiwei+gewei\r\n    return c_str\r\n\r\ndef _check_whether_special(c_str):\r\n    for i in u'\xe5\x8d\x81\xe7\x99\xbe\xe5\x8d\x83\xe4\xb8\x87\xe4\xba\xbf':\r\n        if i in c_str:\r\n            return False\r\n    return True\r\n\r\ndef _convert_section(c_str):\r\n    if _check_whether_special(c_str):\r\n        return _c2n(c_str)\r\n    else:\r\n        return _get_complex(c_str)\r\n\r\ndef _convert_all(c_str):\r\n    if _check_whether_special(c_str):\r\n        return _c2n(c_str)\r\n    result=''\r\n    flag=0\r\n    float_part=''\r\n    if u'\xe7\x82\xb9' in c_str:\r\n        flag1=1\r\n        i = c_str.split(u'\xe7\x82\xb9')[1]\r\n        c_str = c_str.split(u'\xe7\x82\xb9')[0]\r\n        float_part = '.'+_convert_section(i)\r\n\r\n    if u'\xe4\xba\xbf' in c_str:\r\n        flag=8\r\n        i = c_str.split(u'\xe4\xba\xbf')[0]\r\n        c_str = c_str.split(u'\xe4\xba\xbf')[1]\r\n        result += _convert_section(i)\r\n        if c_str=='':\r\n            result += '00000000'\r\n            return result\r\n    if u'\xe4\xb8\x87' in c_str: \r\n        flag=4\r\n        i = c_str.split(u'\xe4\xb8\x87')[0]\r\n        c_str = c_str.split(u'\xe4\xb8\x87')[1]\r\n        result += _convert_section(i)\r\n        if c_str=='':\r\n            result += '0000'\r\n            return result\r\n    right = _get_complex(c_str)\r\n    return result + '0'*(flag-len(_get_complex(c_str))) + right + float_part\r\n\r\ndef convertCharacter2Digit(string):\r\n    chinese_numbers=re.findall(u'[\xe7\x82\xb9\xe9\x9b\xb6\xe4\xb8\x80\xe4\xba\x8c\xe4\xb8\x89\xe5\x9b\x9b\xe4\xba\x94\xe5\x85\xad\xe4\xb8\x83\xe5\x85\xab\xe4\xb9\x9d\xe5\x8d\x81\xe7\x99\xbe\xe5\x8d\x83\xe4\xb8\x87\xe4\xba\xbf]{1,}', \r\n        string, re.S)\r\n    sub_str = re.sub(u'[\xe7\x82\xb9\xe9\x9b\xb6\xe4\xb8\x80\xe4\xba\x8c\xe4\xb8\x89\xe5\x9b\x9b\xe4\xba\x94\xe5\x85\xad\xe4\xb8\x83\xe5\x85\xab\xe4\xb9\x9d\xe5\x8d\x81\xe7\x99\xbe\xe5\x8d\x83\xe4\xb8\x87\xe4\xba\xbf]{1,}', '_', string)\r\n    for chinese_number in chinese_numbers:\r\n        digit = _convert_all(chinese_number)\r\n        sub_str = sub_str.replace('_', digit, 1)\r\n    print('\xe5\x8e\x9f\xe5\x8f\xa5\xe5\xad\x90:', string)\r\n    print('\xe6\x96\xb0\xe5\x8f\xa5\xe5\xad\x90:', sub_str)\r\n    print('\\n')\r\n    return sub_str\r\n\r\n\r\nif __name__ == '__main__':\r\n    with codecs.open('sample.txt','r','utf-8') as f:\r\n        content=f.readlines()\r\n    for string in content:\r\n        convertCharacter2Digit(string.strip())\r\n"""
speechvalley/feature/madarian/digit2character.py,0,"b""# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2018-01-10 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : processDigit.py\n# Description  : Converting digit into Chinese character \n#                according to its pronunciation\n# ******************************************************\n\nimport re\nimport os\nimport codecs\n\ndef convertDigit2Character(string): \n    '''\n    Main function to be called for converting digits\n    into characters in a sentence\n    '''\n    return _prepString(string)\n\ndef _replaceDecimal(decimal_set, sub_str):\n    ''' \n    Replacing decimal numbers with Chinese expression\n    '''\n    dec_str_set = []\n    for dec in decimal_set:\n        dec_str=_float2Chinese(float(dec))\n        dec_str_set.append(dec_str)\n    newStr=''\n    count=0\n    for c in sub_str:\n        if c=='_':\n            newStr+=dec_str_set[count]\n            count+=1\n        else:\n            newStr+=c\n    return newStr\n    \ndef _replaceInteger(integer_set, sub_str):\n    ''' \n    Replacing integer numbers with Chinese expression\n    '''\n    int_str_set = []\n    for inte in integer_set:\n        int_str=_integer2Chinese(int(inte))\n        int_str_set.append(int_str)\n    newStr=''\n    count=0\n    for c in sub_str:\n        if c=='_':\n            newStr+=int_str_set[count]\n            count+=1\n        else:\n            newStr+=c\n    return newStr\n\ndef _replaceSpecial(integer_set, sub_str):\n    ''' \n    Replacing special numbers with Chinese expression, eg: 2018\xe5\xb9\xb4 --> \xe4\xba\x8c\xe5\xb9\xb4\xe4\xb8\x80\xe5\x85\xab\xe5\xb9\xb4\n    '''\n    charNumSet=['\xe9\x9b\xb6', '\xe4\xb8\x80', '\xe4\xba\x8c', '\xe4\xb8\x89', '\xe5\x9b\x9b', '\xe4\xba\x94', '\xe5\x85\xad',\n             '\xe4\xb8\x83', '\xe5\x85\xab', '\xe4\xb9\x9d']\n    int_str_set = []\n    for inte in integer_set:\n        int_sub_result=''\n        for c in inte:\n            if c=='\xe5\xb9\xb4':\n                int_sub_result+='\xe5\xb9\xb4'\n            else:\n                int_sub_result+=charNumSet[int(c)]\n        int_str_set.append(int_sub_result)\n    newStr=''\n    count=0\n    for c in sub_str:\n        if c=='_':\n            newStr+=int_str_set[count]\n            count+=1\n        else:\n            newStr+=c\n    return newStr\n\ndef _prepString(string):\n    '''\n    Preprocessing the sentence and splitting the decimal, integer or special\n    '''\n    decimal_set = re.findall(r'\\d+\\.\\d+', string)\n    sub_str = re.sub(r'\\d+\\.\\d+', '_', string)\n    newStr=_replaceDecimal(decimal_set, sub_str)\n\n    integer_set = re.findall(r'\\d+\xe5\xb9\xb4', newStr)\n    sub_str = re.sub(r'\\d+\xe5\xb9\xb4', '_', newStr)\n    newStr=_replaceSpecial(integer_set, sub_str)\n\n    integer_set = re.findall(r'\\d+', newStr)\n    sub_str = re.sub(r'\\d+', '_', newStr)\n    newStr=_replaceInteger(integer_set, sub_str)\n    print('\xe5\x8e\x9f\xe5\x8f\xa5\xe5\xad\x90:', string)\n    print('\xe6\x96\xb0\xe5\x8f\xa5\xe5\xad\x90:', newStr)\n    print('\\n')\n    return newStr\n\ndef _section2Chinese(section):\n    '''\n    Converting section to Chinese expression\n    '''\n    result=''\n    charNumSet=['\xe9\x9b\xb6', '\xe4\xb8\x80', '\xe4\xba\x8c', '\xe4\xb8\x89', '\xe5\x9b\x9b', \n                '\xe4\xba\x94', '\xe5\x85\xad', '\xe4\xb8\x83', '\xe5\x85\xab', '\xe4\xb9\x9d']\n    charUnitSet=['', '\xe5\x8d\x81', '\xe7\x99\xbe', '\xe5\x8d\x83']\n    zero=True\n    unitPos=0\n    while section>0:\n        v=section%10\n        if v==0:\n            if section==0 or zero is False:\n                zero=True\n                result=charNumSet[v]+result\n        elif (section//10)==0 and v==1 and unitPos==1:\n            result=charUnitSet[1]+result\n        else:\n            zero=False\n            strIns=charNumSet[v]\n            strIns+=charUnitSet[unitPos] \n            result=strIns+result\n        unitPos+=1\n        section=section//10\n    return result\n\ndef _integer2Chinese(number):\n    '''\n    Converting integer to Chinese expression\n    '''\n    charSectionSet=['', '\xe4\xb8\x87', '\xe4\xba\xbf', '\xe4\xb8\x87\xe4\xba\xbf']\n    result=''\n    zero=False\n    unitPos=0\n    if number==0:\n        return '\xe9\x9b\xb6'\n    while number>0:\n        section=number%10000\n        if zero:\n            result='\xe9\x9b\xb6'+result\n        sec_result = _section2Chinese(section)\n        if section!=0:\n            sec_result+=charSectionSet[unitPos]\n        result=sec_result+result\n        if section<1000 and section>0:\n            zero=True\n        number=number//10000\n        unitPos+=1\n    return result\n\ndef _float2Chinese(number):\n    '''\n    Converting floating number to Chinese expression\n    '''\n    charNumSet=['\xe9\x9b\xb6', '\xe4\xb8\x80', '\xe4\xba\x8c', '\xe4\xb8\x89', '\xe5\x9b\x9b', '\xe4\xba\x94', '\xe5\x85\xad',\n             '\xe4\xb8\x83', '\xe5\x85\xab', '\xe4\xb9\x9d']\n    integer_part, decimal_part=str(number).split('.')\n    int_result=_integer2Chinese(int(integer_part))\n    dec_result=''\n    for c in decimal_part:\n        dec_result+=charNumSet[int(c)]\n    return int_result+'\xe7\x82\xb9'+dec_result\n\ndef convertDigit2Character(string): \n    return _prepString(string)\n\nif __name__ == '__main__':\n    with codecs.open('sample.txt','r','utf-8') as f:\n        content=f.readlines()\n    for string in content:\n        convertDigit2Character(string.strip())\n\n    '''\n    rootdir='/media/pony/DLdigest/data/ASR_zh'\n    r1 = re.compile(r'\\d+')\n    for subdir, dirs, files in os.walk(rootdir):\n        for file in files:\n            fullFilename = os.path.join(subdir, file)\n            filenameNoSuffix =  os.path.splitext(fullFilename)[0]\n            if file.endswith('.label'):\n                with open(fullFilename, 'r') as f:\n                    line = f.read()\n                    if re.match(r1, line):\n                        print(fullFilename)\n                        try:\n                            prepString(line)\n                        except IndexError:\n                            pass\n    '''\n"""
speechvalley/feature/madarian/preprocess.py,0,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : madarian_preprocess.py\n# Description  : Feature preprocessing for some Madarian dataset\n# ******************************************************\n\nfrom speechvalley.feature.madarian import convertDigit2Character\nfrom speechvalley.feature.madarian import convertCharacter2Digit\n\nclass DigitPrecessor(object):\n\n    def __init__(self, mode):\n        assert mode==\'digit2char\' or mode==\'char2digit\', ""Wrong mode: %s"" % str(mode)\n        self.mode = mode\n\n    def processString(self, string):\n        if self.mode == \'digit2char\':\n            return convertDigit2Character(string)\n        else:\n            return convertCharacter2Digit(string)\n\n    def processFile(self, fileName):\n        result = []\n        assert os.path.isfile(fileName), ""Wrong file path: %s"" % str(fileName)\n        with codecs.open(fileName,\'r\',\'utf-8\') as f:\n            content=f.readlines()\n        if self.mode == \'digit2char\':\n            for string in content:\n                result.append(convertDigit2Character(string))\n        else:\n            for string in content:\n                result.append(convertCharacter2Digit(string))\n        return result\n        \n\nif __name__ == \'__main__\':\n    DP = DigitProcessor(mode=\'digit2char\')\n'"
speechvalley/feature/timit/__init__.py,0,b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : __init__.py\n# Description  : Feature preprocessing for TIMIT dataset\n# ******************************************************\n\nfrom speechvalley.feature.timit.timit_preprocess import wav2feature\n'
speechvalley/feature/timit/timit_preprocess.py,0,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : timit_preprocess.py\n# Description  : Feature preprocessing for TIMIT dataset\n# ******************************************************\n\n""""""\nDo MFCC over all *.wav files and parse label file Use os.walk to iterate all files in a root directory\n\noriginal phonemes:\n\nphn = [\'aa\', \'ae\', \'ah\', \'ao\', \'aw\', \'ax\', \'ax-h\', \'axr\', \'ay\', \'b\', \'bcl\', \'ch\', \'d\', \'dcl\', \'dh\', \'dx\', \'eh\', \'el\', \'em\', \'en\', \'eng\', \'epi\', \'er\', \'ey\', \'f\', \'g\', \'gcl\', \'h#\', \'hh\', \'hv\', \'ih\', \'ix\', \'iy\', \'jh\', \'k\', \'kcl\', \'l\', \'m\', \'n\', \'ng\', \'nx\', \'ow\', \'oy\', \'p\', \'pau\', \'pcl\', \'q\', \'r\', \'s\', \'sh\', \'t\', \'tcl\', \'th\', \'uh\', \'uw\', \'ux\', \'v\', \'w\', \'y\', \'z\', \'zh\']\n\nmapped phonemes(For more details, you can read the main page of this repo):\n\nphn = [\'sil\', \'aa\', \'ae\', \'ah\', \'ao\', \'aw\', \'ax\', \'ax-h\', \'ay\', \'b\', \'ch\', \'d\', \'dh\', \'dx\', \'eh\', \'el\', \'en\', \'epi\', \'er\', \'ey\', \'f\', \'g\', \'hh\', \'ih\', \'ix\', \'iy\', \'jh\', \'k\', \'l\', \'m\', \'n\', \'ng\', \'ow\', \'oy\', \'p\', \'q\', \'r\', \'s\', \'sh\', \'t\', \'th\', \'uh\', \'uw\', \'v\', \'w\', \'y\', \'z\', \'zh\']\n""""""\n\nimport os\nimport argparse\nimport glob\nimport sys\nimport sklearn\nimport numpy as np\nimport scipy.io.wavfile as wav\nfrom sklearn import preprocessing\nfrom speechvalley.feature.core import calcfeat_delta_delta, spectrogramPower\n\n## original phonemes\nphn = [\'aa\', \'ae\', \'ah\', \'ao\', \'aw\', \'ax\', \'ax-h\', \'axr\', \'ay\', \'b\', \'bcl\', \'ch\', \'d\', \'dcl\', \'dh\', \'dx\', \'eh\', \'el\', \'em\', \'en\', \'eng\', \'epi\', \'er\', \'ey\', \'f\', \'g\', \'gcl\', \'h#\', \'hh\', \'hv\', \'ih\', \'ix\', \'iy\', \'jh\', \'k\', \'kcl\', \'l\', \'m\', \'n\', \'ng\', \'nx\', \'ow\', \'oy\', \'p\', \'pau\', \'pcl\', \'q\', \'r\', \'s\', \'sh\', \'t\', \'tcl\', \'th\', \'uh\', \'uw\', \'ux\', \'v\', \'w\', \'y\', \'z\', \'zh\']\n\n## cleaned phonemes\n#phn = [\'sil\', \'aa\', \'ae\', \'ah\', \'ao\', \'aw\', \'ax\', \'ax-h\', \'ay\', \'b\', \'ch\', \'d\', \'dh\', \'dx\', \'eh\', \'el\', \'en\', \'epi\', \'er\', \'ey\', \'f\', \'g\', \'hh\', \'ih\', \'ix\', \'iy\', \'jh\', \'k\', \'l\', \'m\', \'n\', \'ng\', \'ow\', \'oy\', \'p\', \'q\', \'r\', \'s\', \'sh\', \'t\', \'th\', \'uh\', \'uw\', \'v\', \'w\', \'y\', \'z\', \'zh\']\n\ndef wav2feature(rootdir, save_directory, mode, feature_len, level, keywords, win_len, win_step,  seq2seq, save):\n    feat_dir = os.path.join(save_directory, level, keywords, mode)\n    label_dir = os.path.join(save_directory, level, keywords, \'label\')\n    if not os.path.exists(label_dir):\n        os.makedirs(label_dir)\n    if not os.path.exists(feat_dir):\n        os.makedirs(feat_dir)\n    count = 0\n    for subdir, dirs, files in os.walk(rootdir):\n        for file in files:\n            fullFilename = os.path.join(subdir, file)\n            filenameNoSuffix =  os.path.splitext(fullFilename)[0]\n            if file.endswith(\'.WAV\'):\n                rate = None\n                sig = None\n                try:\n                    (rate,sig)= wav.read(fullFilename)\n                except ValueError as e:\n                    if e.message == ""File format \'NIST\'... not understood."":\n                        print(\'You should use nist2wav.sh to convert NIST format files to WAV files first, nist2wav.sh is in core folder.\')\n                        return\n                feat = calcfeat_delta_delta(sig,rate,win_length=win_len,win_step=win_step,mode=mode,feature_len=feature_len)\n                feat = preprocessing.scale(feat)\n                feat = np.transpose(feat)\n                print(feat.shape)\n\n                if level == \'phn\':\n                    labelFilename = filenameNoSuffix + \'.PHN\'\n                    phenome = []\n                    with open(labelFilename,\'r\') as f:\n                        if seq2seq is True:\n                            phenome.append(len(phn)) # <start token>\n                        for line in f.read().splitlines():\n                            s=line.split(\' \')[2]\n                            p_index = phn.index(s)\n                            phenome.append(p_index)\n                        if seq2seq is True:\n                            phenome.append(len(phn)+1) # <end token>\n                        print(phenome)\n                    phenome = np.array(phenome)\n\n                elif level == \'cha\':\n                    labelFilename = filenameNoSuffix + \'.WRD\'\n                    phenome = []\n                    sentence = \'\'\n                    with open(labelFilename,\'r\') as f:\n                        for line in f.read().splitlines():\n                            s=line.split(\' \')[2]\n                            sentence += s+\' \'\n                            if seq2seq is True:\n                                phenome.append(28)\n                            for c in s:\n                                if c==""\'"":\n                                    phenome.append(27)\n                                else:\n                                    phenome.append(ord(c)-96)\n                            phenome.append(0)\n\n                        phenome = phenome[:-1]\n                        if seq2seq is True:\n                            phenome.append(29)\n                    print(phenome)\n                    print(sentence)\n\n                count+=1\n                print(\'file index:\',count)\n                if save:\n                    featureFilename = feat_dir + filenameNoSuffix.split(\'/\')[-2]+\'-\'+filenameNoSuffix.split(\'/\')[-1]+\'.npy\'\n                    np.save(featureFilename,feat)\n                    labelFilename = label_dir + filenameNoSuffix.split(\'/\')[-2]+\'-\'+filenameNoSuffix.split(\'/\')[-1]+\'.npy\'\n                    print(labelFilename)\n                    np.save(labelFilename,phenome)\n\n\nif __name__ == \'__main__\':\n    # character or phoneme\n    parser = argparse.ArgumentParser(prog=\'timit_preprocess\',\n                                     description=""""""\n                                     Script to preprocess timit data\n                                     """""")\n    parser.add_argument(""path"", help=""Directory where Timit dataset is contained"", type=str)\n    parser.add_argument(""save"", help=""Directory where preprocessed arrays are to be saved"",\n                        type=str)\n    parser.add_argument(""-n"", ""--name"", help=""Name of the dataset"",\n                        choices=[\'train\', \'test\'],\n                        type=str, default=\'train\')\n    parser.add_argument(""-l"", ""--level"", help=""Level"",\n                        choices=[\'cha\', \'phn\'],\n                        type=str, default=\'cha\')\n    parser.add_argument(""-m"", ""--mode"", help=""Mode"",\n                        choices=[\'mfcc\', \'fbank\'],\n                        type=str, default=\'mfcc\')\n    parser.add_argument(\'--featlen\', type=int, default=13, help=\'Features length\')\n    parser.add_argument(""--seq2seq"", help=""set this flag to use seq2seq"", action=""store_true"")\n\n    parser.add_argument(""-winlen"", ""--winlen"", type=float,\n                        default=0.02, help=""specify the window length of feature"")\n\n    parser.add_argument(""-winstep"", ""--winstep"", type=float,\n                        default=0.01, help=""specify the window step length of feature"")\n\n    args = parser.parse_args()\n    root_directory = args.path\n    save_directory = args.save\n    level = args.level\n    mode = args.mode\n    feature_len = args.featlen\n    name = args.name\n    seq2seq = args.seq2seq\n    win_len = args.winlen\n    win_step = args.winstep\n\n    root_directory = os.path.join(root_directory, name)\n    if root_directory == ""."":\n        root_directory = os.getcwd()\n    if save_directory == ""."":\n        save_directory = os.getcwd()\n    if not os.path.isdir(root_directory):\n        raise ValueError(""Root directory does not exist!"")\n    if not os.path.exists(save_directory):\n        os.makedirs(save_directory)\n    wav2feature(root_directory, save_directory, mode=mode, feature_len=feature_len,\n                level=level, keywords=name, win_len=win_len, win_step=win_step,\n                seq2seq=seq2seq, save=True)\n'"
speechvalley/feature/wsj/__init__.py,0,b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : __init__.py\n# Description  : Feature preprocessing for WSJ dataset\n# ******************************************************\n\nfrom speechvalley.feature.wsj.extract_wsj import extract \nfrom speechvalley.feature.wsj.rename_wsj import renameCD\nfrom speechvalley.feature.wsj.split_data_by_s5 import split_data_by_s5\nfrom speechvalley.feature.wsj.wsj_preprocess import wav2feature\n'
speechvalley/feature/wsj/extract_wsj.py,0,"b""# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : extract_wsj.py\n# Description  : Extracting WSJ dataset\n# ******************************************************\n\n\nimport os\nimport subprocess\n\ndef extract(rootdir):\n  for subdir, dirs, files in os.walk(rootdir):\n    for f in files:\n      if f.endswith('.zip'):\n        fullFilename = os.path.join(rootdir, f)\n        subprocess.call(['atool', '-x', fullFilename])\n        print f\n\n"""
speechvalley/feature/wsj/rename_wsj.py,0,"b""# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : rename_wsj.py\n# Description  : Renaming some folders for WSJ dataset\n# ******************************************************\n\nimport subprocess\nimport os\n\ndef lookup(cd_id, logfile):\n  # find new name according to indexing\n  with open(logfile, 'r') as f:\n    content = f.readlines()\n  for line in content:\n    if int(line.split(' ')[-1][2:]) == int(cd_id[2:]):\n      if '.' in line.split(' ')[-3]:\n        newName = line.split(' ')[-3]\n        return newName\n    else:\n      continue\n\ndef renameCD(src_dir, mode):\n  # rename CD directory to new name\n  logfile = mode+'.links.log'\n  cd_dir = os.path.join(src_dir, mode)\n  count = 0\n  for subdir in os.listdir(cd_dir):\n    if subdir.startswith('CD') or subdir.startswith('cd'):\n      newName = lookup(subdir, os.path.join(src_dir, logfile))\n      cd_path = os.path.join(src_dir, mode, subdir)\n      new_cd_path = os.path.join(src_dir, mode, newName)\n      os.rename(cd_path, new_cd_path)\n      count += 1\n      print('new file ', count, ': ', new_cd_path)\n\nif __name__ == '__main__':\n  renameCD('/media/pony/DLdigest/study/ASR/corpus/wsj', mode='wsj0')\n  ## you should add cd34, cd16 to the wsj1.links.log file and then execute this command\n  renameCD('/media/pony/DLdigest/study/ASR/corpus/wsj', mode='wsj1')\n"""
speechvalley/feature/wsj/split_data_by_s5.py,0,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : split_data_by_s5.py\n# Description  : Splitting data by s5 recipe\n# ******************************************************\n\n""""""\nNOTE:\nwe process the data using Kaldi s5 recipe\ntrain set: si284\nvalidation set: eval92\ntest set: dev93\n""""""\n\nimport shutil\nimport os\nfrom speechvalley.feature.core import check_path_exists\n\ndef split_data_by_s5(src_dir, des_dir, keywords=[\'train_si284\', \'test_eval92\', \'test_dev93\']):\n  count = 0\n  for key in keywords:\n    wav_file_list = os.path.join(src_dir, key+\'.flist\') \n    label_file_list = os.path.join(src_dir, key+\'.txt\') \n    new_path = check_path_exists(os.path.join(des_dir, key))\n\n    with open(wav_file_list, \'r\') as wfl:\n      wfl_contents = wfl.readlines()\n      for line in wfl_contents:\n        line = line.strip()\n        if os.path.isfile(line):\n          shutil.copyfile(line, os.path.join(des_dir, key, line.split(\'/\')[-1]))\n          print(line)\n        else:\n          tmp = \'/\'.join(line.split(\'/\')[:-1]+[line.split(\'/\')[-1].upper()])\n          shutil.copyfile(tmp, os.path.join(des_dir, key, line.split(\'/\')[-1].replace(\'WV1\', \'wv1\')))\n          print(tmp)\n\n    with open(label_file_list, \'r\') as lfl:\n      lfl_contents = lfl.readlines()\n      for line in lfl_contents:\n        label = \' \'.join(line.strip().split(\' \')[1:])\n        with open(os.path.join(des_dir, key, line.strip().split(\' \')[0]+\'.label\'), \'w\') as lf:\n          lf.writelines(label)\n        print(key, label)\n        \n\nif __name__ == \'__main__\':\n  src_dir = \'/media/pony/DLdigest/study/ASR/corpus/wsj/s5/data\'\n  des_dir = \'/media/pony/DLdigest/study/ASR/corpus/wsj/standard\'\n  split_data_by_s5(src_dir, des_dir)\n\n'"
speechvalley/feature/wsj/wsj_preprocess.py,0,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : wsj_preprocess.py\n# Description  : Feature preprocessing for WSJ dataset\n# ******************************************************\n\n\nimport os\nimport cPickle\nimport glob\nimport sklearn\nimport argparse\nimport numpy as np\nimport scipy.io.wavfile as wav\nfrom sklearn import preprocessing\nfrom subprocess import check_call, CalledProcessError\nfrom speechvalley.feature.core import calcfeat_delta_delta\n\ndef wav2feature(root_directory, save_directory, name, win_len, win_step, mode, feature_len, seq2seq, save):\n  """"""\n  To run for WSJ corpus, you should download sph2pipe_v2.5 first!\n  """"""\n  \n\n  count = 0\n  dirid = 0\n  level = \'cha\' if seq2seq is False else \'seq2seq\'\n  for subdir, dirs, files in os.walk(root_directory):\n    for f in files:\n      fullFilename = os.path.join(subdir, f)\n      filenameNoSuffix =  os.path.splitext(fullFilename)[0]\n      if f.endswith(\'.wv1\') or f.endswith(\'.wav\'):\n        rate = None\n        sig = None\n        try:\n          (rate,sig)= wav.read(fullFilename)\n        except ValueError as e:\n          sph2pipe = os.path.join(sph2pipe_dir, \'sph2pipe\')\n          wav_name = fullFilename.replace(\'wv1\', \'wav\')\n          check_call([\'./sph2pipe\', \'-f\', \'rif\', fullFilename, wav_name])\n          os.remove(fullFilename)\n          print(wav_name)\n          (rate,sig)= wav.read(wav_name)\n          os.remove(fullFilename)\n\n        feat = calcfeat_delta_delta(sig,rate,win_length=win_len,win_step=win_step,feature_len=feature_len,mode=mode)\n        feat = preprocessing.scale(feat)\n        feat = np.transpose(feat)\n        print(feat.shape)\n        labelFilename = filenameNoSuffix + \'.label\'\n        with open(labelFilename,\'r\') as f:\n          characters = f.readline().strip().lower()\n        targets = []\n        if seq2seq is True:\n          targets.append(28)\n        for c in characters:\n          if c == \' \':\n            targets.append(0)\n          elif c == ""\'"":\n            targets.append(27)\n          else:\n            targets.append(ord(c)-96)\n        if seq2seq is True:\n          targets.append(29)\n        targets = np.array(targets)\n        print(targets)\n        if save:\n          count += 1\n          if count%1000 == 0:\n              dirid += 1\n          print(\'file index:\',count)\n          print(\'dir index:\',dirid)\n          label_dir = os.path.join(save_directory, level, name, str(dirid), \'label\')\n          feat_dir = os.path.join(save_directory, level, name, str(dirid), mode)\n          if not os.path.isdir(label_dir):\n              os.makedirs(label_dir)\n          if not os.path.isdir(feat_dir):\n              os.makedirs(feat_dir)\n          featureFilename = os.path.join(feat_dir, filenameNoSuffix.split(\'/\')[-1] +\'.npy\')\n          np.save(featureFilename,feat)\n          t_f = os.path.join(label_dir, filenameNoSuffix.split(\'/\')[-1] +\'.npy\')\n          print(t_f)\n          np.save(t_f,targets)\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(prog=\'wsj_preprocess\',\n                                     description=\'Script to preprocess WSJ data\')\n    parser.add_argument(""path"", help=""Directory of WSJ dataset"", type=str)\n\n    parser.add_argument(""save"", help=""Directory where preprocessed arrays are to be saved"",\n                        type=str)\n    parser.add_argument(""-n"", ""--name"", help=""Name of the dataset"",\n                        choices=[\'train_si284\', \'test_eval92\', \'test_dev\'], \n                        type=str, default=\'train_si284\')\n\n    parser.add_argument(""-m"", ""--mode"", help=""Mode"",\n                        choices=[\'mfcc\', \'fbank\'],\n                        type=str, default=\'mfcc\')\n    parser.add_argument(""--featlen"", help=\'Features length\', type=int, default=13)\n    parser.add_argument(""-s"", ""--seq2seq"", default=False,\n                        help=""set this flag to use seq2seq"", action=""store_true"")\n\n    parser.add_argument(""-wl"", ""--winlen"", type=float,\n                        default=0.02, help=""specify the window length of feature"")\n\n    parser.add_argument(""-ws"", ""--winstep"", type=float,\n                        default=0.01, help=""specify the window step length of feature"")\n\n    args = parser.parse_args()\n    root_directory = args.path\n    save_directory = args.save\n    mode = args.mode\n    feature_len = args.featlen\n    seq2seq = args.seq2seq\n    name = args.name\n    win_len = args.winlen\n    win_step = args.winstep\n\n    if root_directory == \'.\':\n        root_directory = os.getcwd()\n\n    if save_directory == \'.\':\n        save_directory = os.getcwd()\n\n    if not os.path.isdir(root_directory):\n        raise ValueError(""WSJ Directory does not exist!"")\n\n    if not os.path.isdir(save_directory):\n        os.makedirs(save_directory)\n\n    wav2feature(root_directory, save_directory, name=name, win_len=win_len, win_step=win_step,\n                mode=mode, feature_len=feature_len, seq2seq=seq2seq, save=True)\n'"
speechvalley/lm/spellingChecker4CN/__init__.py,0,b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : __init__.py\n# Description  : Spelling Checker\n# ******************************************************\n\n'
speechvalley/lm/spellingChecker4CN/gardener.py,0,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-20 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : gardener.py\n# Description  : Spelling Corrector for Chinese\n# ******************************************************\n\nimport os\nimport json\nimport numpy as np\nfrom hanziconv import HanziConv\nfrom pypinyin import pinyin\nfrom speechvalley.lm.spellingChecker4CN.utils import filter_punctuation\nfrom speechvalley.utils.taskUtils import check_path_exists\n\nclass CorpusGardener(object):\n    """"""\n    Preprocessing multiple language corpuses, and gathering\n    them into batches\n    """"""\n  \n    def __init__(self, remove_duplicate_space=True):\n        self.remove_dubplicate_space = remove_duplicate_space\n        self.save_dir = \'/home/pony/github/data/spellingChecker/raw\'\n\n    def process_poetry(self, data_dir=\'/media/pony/DLdigest/data/languageModel/chinese-poetry/json\'):\n        """"""\n        Process Tang and Song poems dataset\n        """"""\n        save_dir = os.path.join(self.save_dir, \'poem\')\n        check_path_exists(save_dir)\n        count = 0\n        for entry in os.scandir(data_dir):\n            if entry.name.startswith(\'poet\'):\n                with open(entry.path, \'r\') as json_file:\n                    poems = json.load(json_file)\n                    for p in poems: \n                        paras = HanziConv.toSimplified(\'\'.join(p[\'paragraphs\']).replace(\'\\n\', \'\'))\n                        paras = filter_punctuation(paras)\n                        for para in paras.split(\' \'):\n                            if len(para.strip())>1:\n                                pys = \' \'.join(np.array(pinyin(para)).flatten())\n                                with open(os.path.join(save_dir, str(count//400000+1)+\'.txt\'), \'a\') as f:\n                                    f.write(para+\',\'+pys+\'\\n\')\n                                count += 1\n        \n    def process_dureader(self, data_dir=\'/media/pony/DLdigest/data/languageModel/dureader-raw/\'): \n        """"""\n        Processing Baidu released QA Reader Dataset\n        """"""\n        save_dir = os.path.join(self.save_dir, \'dureader\')\n        check_path_exists(save_dir)\n        count = 0\n        for entry in os.scandir(data_dir):\n            if entry.name.endswith(\'json\'):\n                print(entry.path)\n                with open(entry.path, \'r\') as f:\n                    for line in f:\n                        contents = json.loads(line)\n                        con = []\n                        try:\n                            answers = \'\'.join(contents[\'answers\'])\n                            con.append(answers)\n                            questions = contents[\'question\']\n                            con.append(questions)\n                            for doc in contents[\'documents\']:\n                                paragraphs = \'\'.join(doc[\'paragraphs\'])\n                                title = doc[\'title\']\n                                con.append(paragraphs)\n                                con.append(title)\n                            con = HanziConv.toSimplified(\'\'.join(con).replace(\'\\n\', \'\'))\n                            cons = filter_punctuation(con)\n                            for c in cons.split(\' \'):\n                                if len(c.strip())>1:\n                                    pys = \' \'.join(np.array(pinyin(c)).flatten())\n                                    count += 1\n                                    with open(os.path.join(save_dir, str(count//400000+1)+\'.txt\'), \'a\') as f:\n                                        f.write(c+\',\'+pys+\'\\n\')\n                        except KeyError:\n                            continue\n\n    def process_audioLabels(self, data_dir=\'/media/pony/DLdigest/data/ASR_zh/\'): \n        """"""\n        Processing label files in collected Chinese audio dataset\n        """"""\n        save_dir = os.path.join(self.save_dir, \'audioLabels\')\n        check_path_exists(save_dir)\n        count = 0\n        for subdir, dirs, files in os.walk(data_dir):\n            print(subdir)\n            for f in files:\n                if f.endswith(""label""):\n                    fullFilename = os.path.join(subdir, f)\n                    with open(fullFilename, \'r\') as f:\n                        line = f.read()\n                        con = HanziConv.toSimplified(line)\n                        con = filter_punctuation(con)\n                        for c in con.split(\' \'):\n                            if len(c.strip())>1:\n                                pys = \' \'.join(np.array(pinyin(c)).flatten())\n                                count += 1\n                                with open(os.path.join(save_dir, str(count//400000+1)+\'.txt\'), \'a\') as f:\n                                    f.write(c+\',\'+pys+\'\\n\')\n\nif __name__ == \'__main__\':\n    cg = CorpusGardener()\n    cg.process_audioLabels()\n'"
speechvalley/lm/spellingChecker4CN/utils.py,0,"b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-20 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : utils.py\n# Description  : Spelling Corrector for Chinese\n# ******************************************************\n\nimport string\nimport re\ndef filter_punctuation(input_str, remove_duplicate_space=True):\n    """"""\n    all common punctuations in both Chinese and English, if any marker is \n    not included, welcome to pull issues in github repo.\n    """"""\n    \'\'\'\n    punctuation=string.punctuation + string.ascii_letters + \\\n                \'\xef\xbc\x81\xef\xbc\x9f\xef\xbd\xa1\xef\xbc\x82\xef\xbc\x83\xef\xbc\x84\xef\xbc\x85\xef\xbc\x86\xef\xbc\x87\xef\xbc\x88\xef\xbc\x89\xef\xbc\x8a\xef\xbc\x8b\xef\xbc\x8c\xef\xbc\x8d\xef\xbc\x8f\xef\xbc\x9a\xef\xbc\x9b\xef\xbc\x9c\xef\xbc\x9d\xef\xbc\x9e\xef\xbc\xa0\xef\xbc\xbb\xef\xbc\xbc\xef\xbc\xbd\xef\xbc\xbe\xef\xbc\xbf\xef\xbd\x80\' + \\\n                \'\xef\xbd\x9b\xef\xbd\x9c\xef\xbd\x9d\xef\xbd\x9e\xef\xbd\x9f\xef\xbd\xa0\xef\xbd\xa2\xef\xbd\xa3\xef\xbd\xa4\xe3\x80\x81\xe3\x80\x83\xe3\x80\x8b\xe3\x80\x8c\xe3\x80\x8d\xe3\x80\x8e\xe3\x80\x8f\xe3\x80\x90\xe3\x80\x91\xe3\x80\x94\xe3\x80\x95\xe3\x80\x96\xe3\x80\x97\xe3\x80\x98\xe3\x80\x99\xe3\x80\x9a\xe3\x80\x9b\xe3\x80\x9c\xe3\x80\x9d\xe3\x80\x9e\' + \\\n                \'\xe3\x80\x9f\xe3\x80\xb0\xe3\x80\xbe\xe3\x80\xbf\xe2\x80\x93\xe2\x80\x94\xe2\x80\x98\xe2\x80\x99\xe2\x80\x9b\xe2\x80\x9c\xe2\x80\x9d\xe2\x80\x9e\xe2\x80\x9f\xe2\x80\xa6\xe2\x80\xa7\xef\xb9\x8f.\xc2\xb7\xe3\x80\x82\xe3\x80\x8a\xe3\x80\x8b\'\n    regex = re.compile(\'[%s]\' % re.escape(punctuation))\n    \'\'\'\n\n    regex = re.compile(u\'[^\\u4E00-\\u9FA5]\')#\xe9\x9d\x9e\xe4\xb8\xad\xe6\x96\x87\n    if remove_duplicate_space:\n        result = re.sub(\' +\', \' \', regex.sub(\' \', input_str))\n    else:\n        result = regex.sub(\' \', input_str)\n    result = re.sub(""\\d+"", "" "", result)\n    result = strQ2B(result)\n    return result\n\ndef strQ2B(ustring):\n    """"""\n    Converting full-width characters to half-width characters\n    """"""\n    rstring = """"\n    for uchar in ustring:\n        inside_code = ord(uchar)\n        if inside_code == 12288: \n            inside_code = 32\n        elif (inside_code >= 65281 and inside_code <= 65374): \n            inside_code -= 65248\n        rstring += chr(inside_code)\n    return rstring\n\ndef digits2Chinese(string, mode=\'int\'):\n    if mode == \'integer\':\n    \n\ndef reviseString(string):\n    re_int = re.compile(\'\\d+\')\n    re_float = re.compile(\'\\d+\\.\\d+\')\n\n\n\nif __name__ == \'__main__\':\n    a = \'abcd\xe6\x88\x91\xe6\x98\xaf\xef\xbc\x8c,,,...\xe4\xb8\x8a\xe5\x8d\x87\xef\xbc\x81!!!~[][][]\xc2\xb7\xe3\x80\x8c\xc2\xb7\xe3\x80\x8d\xe3\x80\x8c{}345\'\n    print(filter_punctuation(a))\n'"
speechvalley/models/n-gram/__init__.py,0,b'# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : __init__.py\n# Description  : Libraries for n-gram model\n# ******************************************************\n\nfrom speechvalley.models.ngram.generate import *\nfrom speechvalley.models.ngram.ngram import *\n'
speechvalley/models/n-gram/generate.py,0,"b""# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : generate.py\n# Description  : Generation for characters by n-gram model\n# ******************************************************\n\nimport numpy as np\nimport pickle\n\ndef load_obj(name):\n    with open(name + '.pkl', 'rb') as f:\n        return pickle.load(f)\n\ndef frequence(gram, type=2):\n    if type == 2:\n        for key, value in gram.items():\n            total = 0.0\n            for subkey, subvalue in value.items():\n                total += subvalue\n            for subkey, subvalue in value.items():\n                gram[key][subkey] = subvalue/total\n    else:\n        raise NotImplementedError('%s-gram is being developed'%type)\n    return gram\n\ndef generate_sentence(corpus_dir, seed='what are', length=10):\n    bigram = load_obj(corpus_dir+'bigram')\n    freq_bigram = frequence(bigram)\n    sent = ''\n    if not ' ' in seed:\n        sent += seed\n        prev = seed\n        for i in range(length):\n            probs = []\n            for _, value in freq_bigram[prev].items():\n                probs.append(value)\n            sample = np.random.choice(range(len(freq_bigram[prev])),p=probs)\n            prev = freq_bigram[prev].keys()[sample]\n            sent += ' '+prev\n    print sent\n\ngenerate_sentence('/home/pony/github/data/libri/ngram/', seed='love', length=10)\n"""
speechvalley/models/n-gram/ngram.py,0,"b""# encoding: utf-8\n# ******************************************************\n# Author       : zzw922cn\n# Last modified: 2017-12-09 11:00\n# Email        : zzw922cn@gmail.com\n# Filename     : ngram.py\n# Description  : ngram model \n# ******************************************************\n\nimport numpy as np\nimport os\nimport operator\nimport pickle\n\ndef save_obj(name, obj):\n    with open(name + '.pkl', 'wb') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n\nclass NGram:\n    def __init__(self, rootdir):\n        self.rootdir = rootdir\n\n    def get_corpus(self):\n        corpus = []\n        word_count = {}\n        biword_count = {}\n        bigram = {}\n        bigram['SOS'] = {}\n        trigram = {}\n        for subdir, dirs, files in os.walk(self.rootdir):\n            for f in files:\n                fullFilename = os.path.join(subdir, f)\n                filenameNoSuffix =  os.path.splitext(fullFilename)[0]\n            if f.endswith('.label'):\n                with open(fullFilename, 'r') as f:\n                    line = f.readline()\n                    corpus.append(line)\n                    line = line.strip().split(' ')\n                    len_sent = range(len(line))\n                    for idx in len_sent:\n                        word = line[idx]\n                        word_count = inc_dict(word_count, word)\n                        if not bigram.has_key(word):\n                            bigram[word] = {}\n                        if idx == 0:\n                            bigram['SOS'] = inc_dict(bigram['SOS'], word)\n                        elif idx != len(line)-1:\n                            bigram[word] = inc_dict(bigram[word], line[idx+1])\n                        else:\n                            bigram[word] = inc_dict(bigram[word], 'EOS')\n                        if idx == 0:\n                            tri_key = 'SOS ' + word\n                        else:\n                            tri_key = line[idx-1]+' '+word\n                        if not trigram.has_key(tri_key):\n                            trigram[tri_key] = {}\n                        if idx == len(line)-1:\n                            trigram[tri_key] = inc_dict(trigram[tri_key], 'EOS')\n                        else:\n                            trigram[tri_key] = inc_dict(trigram[tri_key], line[idx+1])\n\n        return corpus, word_count, bigram, trigram\n\ndef inc_dict(dic, key):\n    if not dic.has_key(key):\n        dic[key] = 0\n        dic[key] += 1\n    return dic\n\n\nif __name__ == '__main__':\n    ngram = NGram('/media/pony/Seagate Expansion Drive/\xe5\xad\xa6\xe4\xb9\xa0/\xe8\xaf\xad\xe9\x9f\xb3\xe8\xaf\x86\xe5\x88\xab/ASR\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93/LibriSpeech/')\n    corpus, word_count, bigram, trigram = ngram.get_corpus()\n    savedir = '/home/pony/github/data/libri/ngram/'\n    save_obj(savedir+'corpus', corpus)\n    save_obj(savedir+'word_count', word_count)\n    save_obj(savedir+'bigram', bigram)\n    save_obj(savedir+'trigram', trigram)\n    #sorted_word_count = sorted(word_count.items(), key=operator.itemgetter(1), reverse=True)\n"""
