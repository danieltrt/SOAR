file_path,api_count,code
setup.py,0,"b'import io\nimport os\nfrom os import path\nimport re\nfrom setuptools import setup, find_packages\n# To use consisten encodings\nfrom codecs import open\n\n# Function from: https://github.com/pytorch/vision/blob/master/setup.py\n\n\ndef read(*names, **kwargs):\n    with io.open(\n        os.path.join(os.path.dirname(__file__), *names),\n        encoding=kwargs.get(""encoding"", ""utf8"")\n    ) as fp:\n        return fp.read()\n\n# Function from: https://github.com/pytorch/vision/blob/master/setup.py\n\n\ndef find_version(*file_paths):\n    version_file = read(*file_paths)\n    version_match = re.search(r""^__version__ = [\'\\""]([^\'\\""]*)[\'\\""]"",\n                              version_file, re.M)\n    if version_match:\n        return version_match.group(1)\n    raise RuntimeError(""Unable to find version string."")\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(path.join(here, \'README.md\'), encoding=\'utf-8\') as readme_file:\n    long_description = readme_file.read()\n\nVERSION = find_version(\'face_alignment\', \'__init__.py\')\n\nrequirements = [\n    \'torch\',\n    \'numpy\',\n    \'scipy>=0.17\',\n    \'scikit-image\',\n    \'opencv-python\',\n    \'tqdm\',\n    \'enum34;python_version<""3.4""\'\n]\n\nsetup(\n    name=\'face_alignment\',\n    version=VERSION,\n\n    description=""Detector 2D or 3D face landmarks from Python"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n\n    # Author details\n    author=""Adrian Bulat"",\n    author_email=""adrian.bulat@nottingham.ac.uk"",\n    url=""https://github.com/1adrianb/face-alignment"",\n\n    # Package info\n    packages=find_packages(exclude=(\'test\',)),\n\n    install_requires=requirements,\n    license=\'BSD\',\n    zip_safe=True,\n\n    classifiers=[\n        \'Development Status :: 5 - Production/Stable\',\n        \'Operating System :: OS Independent\',\n        \'License :: OSI Approved :: BSD License\',\n        \'Natural Language :: English\',\n\n        # Supported python versions\n        \'Programming Language :: Python :: 2\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.3\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n    ],\n)\n'"
examples/detect_landmarks_in_image.py,0,"b""import face_alignment\r\nimport matplotlib.pyplot as plt\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\nfrom skimage import io\r\nimport collections\r\n\r\n\r\n# Run the 3D face alignment on a test image, without CUDA.\r\nfa = face_alignment.FaceAlignment(face_alignment.LandmarksType._3D, device='cpu', flip_input=True)\r\n\r\ntry:\r\n    input_img = io.imread('../test/assets/aflw-test.jpg')\r\nexcept FileNotFoundError:\r\n    input_img = io.imread('test/assets/aflw-test.jpg')\r\n\r\npreds = fa.get_landmarks(input_img)[-1]\r\n\r\n# 2D-Plot\r\nplot_style = dict(marker='o',\r\n                  markersize=4,\r\n                  linestyle='-',\r\n                  lw=2)\r\n\r\npred_type = collections.namedtuple('prediction_type', ['slice', 'color'])\r\npred_types = {'face': pred_type(slice(0, 17), (0.682, 0.780, 0.909, 0.5)),\r\n              'eyebrow1': pred_type(slice(17, 22), (1.0, 0.498, 0.055, 0.4)),\r\n              'eyebrow2': pred_type(slice(22, 27), (1.0, 0.498, 0.055, 0.4)),\r\n              'nose': pred_type(slice(27, 31), (0.345, 0.239, 0.443, 0.4)),\r\n              'nostril': pred_type(slice(31, 36), (0.345, 0.239, 0.443, 0.4)),\r\n              'eye1': pred_type(slice(36, 42), (0.596, 0.875, 0.541, 0.3)),\r\n              'eye2': pred_type(slice(42, 48), (0.596, 0.875, 0.541, 0.3)),\r\n              'lips': pred_type(slice(48, 60), (0.596, 0.875, 0.541, 0.3)),\r\n              'teeth': pred_type(slice(60, 68), (0.596, 0.875, 0.541, 0.4))\r\n              }\r\n\r\nfig = plt.figure(figsize=plt.figaspect(.5))\r\nax = fig.add_subplot(1, 2, 1)\r\nax.imshow(input_img)\r\n\r\nfor pred_type in pred_types.values():\r\n    ax.plot(preds[pred_type.slice, 0],\r\n            preds[pred_type.slice, 1],\r\n            color=pred_type.color, **plot_style)\r\n\r\nax.axis('off')\r\n\r\n# 3D-Plot\r\nax = fig.add_subplot(1, 2, 2, projection='3d')\r\nsurf = ax.scatter(preds[:, 0] * 1.2,\r\n                  preds[:, 1],\r\n                  preds[:, 2],\r\n                  c='cyan',\r\n                  alpha=1.0,\r\n                  edgecolor='b')\r\n\r\nfor pred_type in pred_types.values():\r\n    ax.plot3D(preds[pred_type.slice, 0] * 1.2,\r\n              preds[pred_type.slice, 1],\r\n              preds[pred_type.slice, 2], color='blue')\r\n\r\nax.view_init(elev=90., azim=90.)\r\nax.set_xlim(ax.get_xlim()[::-1])\r\nplt.show()\r\n"""
face_alignment/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\n__author__ = """"""Adrian Bulat""""""\n__email__ = \'adrian.bulat@nottingham.ac.uk\'\n__version__ = \'1.0.1\'\n\nfrom .api import FaceAlignment, LandmarksType, NetworkSize\n'"
face_alignment/api.py,11,"b'from __future__ import print_function\r\nimport os\r\nimport torch\r\nfrom torch.utils.model_zoo import load_url\r\nfrom enum import Enum\r\nfrom skimage import io\r\nfrom skimage import color\r\nimport numpy as np\r\nimport cv2\r\ntry:\r\n    import urllib.request as request_file\r\nexcept BaseException:\r\n    import urllib as request_file\r\n\r\nfrom .models import FAN, ResNetDepth\r\nfrom .utils import *\r\n\r\n\r\nclass LandmarksType(Enum):\r\n    """"""Enum class defining the type of landmarks to detect.\r\n\r\n    ``_2D`` - the detected points ``(x,y)`` are detected in a 2D space and follow the visible contour of the face\r\n    ``_2halfD`` - this points represent the projection of the 3D points into 3D\r\n    ``_3D`` - detect the points ``(x,y,z)``` in a 3D space\r\n\r\n    """"""\r\n    _2D = 1\r\n    _2halfD = 2\r\n    _3D = 3\r\n\r\n\r\nclass NetworkSize(Enum):\r\n    # TINY = 1\r\n    # SMALL = 2\r\n    # MEDIUM = 3\r\n    LARGE = 4\r\n\r\n    def __new__(cls, value):\r\n        member = object.__new__(cls)\r\n        member._value_ = value\r\n        return member\r\n\r\n    def __int__(self):\r\n        return self.value\r\n\r\nmodels_urls = {\r\n    \'2DFAN-4\': \'https://www.adrianbulat.com/downloads/python-fan/2DFAN4-11f355bf06.pth.tar\',\r\n    \'3DFAN-4\': \'https://www.adrianbulat.com/downloads/python-fan/3DFAN4-7835d9f11d.pth.tar\',\r\n    \'depth\': \'https://www.adrianbulat.com/downloads/python-fan/depth-2a464da4ea.pth.tar\',\r\n}\r\n\r\n\r\nclass FaceAlignment:\r\n    def __init__(self, landmarks_type, network_size=NetworkSize.LARGE,\r\n                 device=\'cuda\', flip_input=False, face_detector=\'sfd\', verbose=False):\r\n        self.device = device\r\n        self.flip_input = flip_input\r\n        self.landmarks_type = landmarks_type\r\n        self.verbose = verbose\r\n\r\n        network_size = int(network_size)\r\n\r\n        if \'cuda\' in device:\r\n            torch.backends.cudnn.benchmark = True\r\n\r\n        # Get the face detector\r\n        face_detector_module = __import__(\'face_alignment.detection.\' + face_detector,\r\n                                          globals(), locals(), [face_detector], 0)\r\n        self.face_detector = face_detector_module.FaceDetector(device=device, verbose=verbose)\r\n\r\n        # Initialise the face alignemnt networks\r\n        self.face_alignment_net = FAN(network_size)\r\n        if landmarks_type == LandmarksType._2D:\r\n            network_name = \'2DFAN-\' + str(network_size)\r\n        else:\r\n            network_name = \'3DFAN-\' + str(network_size)\r\n\r\n        fan_weights = load_url(models_urls[network_name], map_location=lambda storage, loc: storage)\r\n        self.face_alignment_net.load_state_dict(fan_weights)\r\n\r\n        self.face_alignment_net.to(device)\r\n        self.face_alignment_net.eval()\r\n\r\n        # Initialiase the depth prediciton network\r\n        if landmarks_type == LandmarksType._3D:\r\n            self.depth_prediciton_net = ResNetDepth()\r\n\r\n            depth_weights = load_url(models_urls[\'depth\'], map_location=lambda storage, loc: storage)\r\n            depth_dict = {\r\n                k.replace(\'module.\', \'\'): v for k,\r\n                v in depth_weights[\'state_dict\'].items()}\r\n            self.depth_prediciton_net.load_state_dict(depth_dict)\r\n\r\n            self.depth_prediciton_net.to(device)\r\n            self.depth_prediciton_net.eval()\r\n\r\n    def get_landmarks(self, image_or_path, detected_faces=None):\r\n        """"""Deprecated, please use get_landmarks_from_image\r\n\r\n        Arguments:\r\n            image_or_path {string or numpy.array or torch.tensor} -- The input image or path to it.\r\n\r\n        Keyword Arguments:\r\n            detected_faces {list of numpy.array} -- list of bounding boxes, one for each face found\r\n            in the image (default: {None})\r\n        """"""\r\n        return self.get_landmarks_from_image(image_or_path, detected_faces)\r\n\r\n    @torch.no_grad()\r\n    def get_landmarks_from_image(self, image_or_path, detected_faces=None):\r\n        """"""Predict the landmarks for each face present in the image.\r\n\r\n        This function predicts a set of 68 2D or 3D images, one for each image present.\r\n        If detect_faces is None the method will also run a face detector.\r\n\r\n         Arguments:\r\n            image_or_path {string or numpy.array or torch.tensor} -- The input image or path to it.\r\n\r\n        Keyword Arguments:\r\n            detected_faces {list of numpy.array} -- list of bounding boxes, one for each face found\r\n            in the image (default: {None})\r\n        """"""\r\n        if isinstance(image_or_path, str):\r\n            try:\r\n                image = io.imread(image_or_path)\r\n            except IOError:\r\n                print(""error opening file :: "", image_or_path)\r\n                return None\r\n        elif isinstance(image_or_path, torch.Tensor):\r\n            image = image_or_path.detach().cpu().numpy()\r\n        else:\r\n            image = image_or_path\r\n\r\n        if image.ndim == 2:\r\n            image = color.gray2rgb(image)\r\n        elif image.ndim == 4:\r\n            image = image[..., :3]\r\n\r\n        if detected_faces is None:\r\n            detected_faces = self.face_detector.detect_from_image(image[..., ::-1].copy())\r\n\r\n        if len(detected_faces) == 0:\r\n            print(""Warning: No faces were detected."")\r\n            return None\r\n\r\n        landmarks = []\r\n        for i, d in enumerate(detected_faces):\r\n            center = torch.FloatTensor(\r\n                [d[2] - (d[2] - d[0]) / 2.0, d[3] - (d[3] - d[1]) / 2.0])\r\n            center[1] = center[1] - (d[3] - d[1]) * 0.12\r\n            scale = (d[2] - d[0] + d[3] - d[1]) / self.face_detector.reference_scale\r\n\r\n            inp = crop(image, center, scale)\r\n            inp = torch.from_numpy(inp.transpose(\r\n                (2, 0, 1))).float()\r\n\r\n            inp = inp.to(self.device)\r\n            inp.div_(255.0).unsqueeze_(0)\r\n\r\n            out = self.face_alignment_net(inp)[-1].detach()\r\n            if self.flip_input:\r\n                out += flip(self.face_alignment_net(flip(inp))\r\n                            [-1].detach(), is_label=True)\r\n            out = out.cpu()\r\n\r\n            pts, pts_img = get_preds_fromhm(out, center, scale)\r\n            pts, pts_img = pts.view(68, 2) * 4, pts_img.view(68, 2)\r\n\r\n            if self.landmarks_type == LandmarksType._3D:\r\n                heatmaps = np.zeros((68, 256, 256), dtype=np.float32)\r\n                for i in range(68):\r\n                    if pts[i, 0] > 0:\r\n                        heatmaps[i] = draw_gaussian(\r\n                            heatmaps[i], pts[i], 2)\r\n                heatmaps = torch.from_numpy(\r\n                    heatmaps).unsqueeze_(0)\r\n\r\n                heatmaps = heatmaps.to(self.device)\r\n                depth_pred = self.depth_prediciton_net(\r\n                    torch.cat((inp, heatmaps), 1)).data.cpu().view(68, 1)\r\n                pts_img = torch.cat(\r\n                    (pts_img, depth_pred * (1.0 / (256.0 / (200.0 * scale)))), 1)\r\n\r\n            landmarks.append(pts_img.numpy())\r\n\r\n        return landmarks\r\n\r\n    def get_landmarks_from_directory(self, path, extensions=[\'.jpg\', \'.png\'], recursive=True, show_progress_bar=True):\r\n        detected_faces = self.face_detector.detect_from_directory(path, extensions, recursive, show_progress_bar)\r\n\r\n        predictions = {}\r\n        for image_path, bounding_boxes in detected_faces.items():\r\n            image = io.imread(image_path)\r\n            preds = self.get_landmarks_from_image(image, bounding_boxes)\r\n            predictions[image_path] = preds\r\n\r\n        return predictions\r\n\r\n    @staticmethod\r\n    def remove_models(self):\r\n        base_path = os.path.join(appdata_dir(\'face_alignment\'), ""data"")\r\n        for data_model in os.listdir(base_path):\r\n            file_path = os.path.join(base_path, data_model)\r\n            try:\r\n                if os.path.isfile(file_path):\r\n                    print(\'Removing \' + data_model + \' ...\')\r\n                    os.unlink(file_path)\r\n            except Exception as e:\r\n                print(e)\r\n'"
face_alignment/models.py,3,"b'import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport math\r\n\r\n\r\ndef conv3x3(in_planes, out_planes, strd=1, padding=1, bias=False):\r\n    ""3x3 convolution with padding""\r\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3,\r\n                     stride=strd, padding=padding, bias=bias)\r\n\r\n\r\nclass ConvBlock(nn.Module):\r\n    def __init__(self, in_planes, out_planes):\r\n        super(ConvBlock, self).__init__()\r\n        self.bn1 = nn.BatchNorm2d(in_planes)\r\n        self.conv1 = conv3x3(in_planes, int(out_planes / 2))\r\n        self.bn2 = nn.BatchNorm2d(int(out_planes / 2))\r\n        self.conv2 = conv3x3(int(out_planes / 2), int(out_planes / 4))\r\n        self.bn3 = nn.BatchNorm2d(int(out_planes / 4))\r\n        self.conv3 = conv3x3(int(out_planes / 4), int(out_planes / 4))\r\n\r\n        if in_planes != out_planes:\r\n            self.downsample = nn.Sequential(\r\n                nn.BatchNorm2d(in_planes),\r\n                nn.ReLU(True),\r\n                nn.Conv2d(in_planes, out_planes,\r\n                          kernel_size=1, stride=1, bias=False),\r\n            )\r\n        else:\r\n            self.downsample = None\r\n\r\n    def forward(self, x):\r\n        residual = x\r\n\r\n        out1 = self.bn1(x)\r\n        out1 = F.relu(out1, True)\r\n        out1 = self.conv1(out1)\r\n\r\n        out2 = self.bn2(out1)\r\n        out2 = F.relu(out2, True)\r\n        out2 = self.conv2(out2)\r\n\r\n        out3 = self.bn3(out2)\r\n        out3 = F.relu(out3, True)\r\n        out3 = self.conv3(out3)\r\n\r\n        out3 = torch.cat((out1, out2, out3), 1)\r\n\r\n        if self.downsample is not None:\r\n            residual = self.downsample(residual)\r\n\r\n        out3 += residual\r\n\r\n        return out3\r\n\r\n\r\nclass Bottleneck(nn.Module):\r\n\r\n    expansion = 4\r\n\r\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\r\n        super(Bottleneck, self).__init__()\r\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(planes)\r\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\r\n                               padding=1, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(planes)\r\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\r\n        self.bn3 = nn.BatchNorm2d(planes * 4)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.downsample = downsample\r\n        self.stride = stride\r\n\r\n    def forward(self, x):\r\n        residual = x\r\n\r\n        out = self.conv1(x)\r\n        out = self.bn1(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv2(out)\r\n        out = self.bn2(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv3(out)\r\n        out = self.bn3(out)\r\n\r\n        if self.downsample is not None:\r\n            residual = self.downsample(x)\r\n\r\n        out += residual\r\n        out = self.relu(out)\r\n\r\n        return out\r\n\r\n\r\nclass HourGlass(nn.Module):\r\n    def __init__(self, num_modules, depth, num_features):\r\n        super(HourGlass, self).__init__()\r\n        self.num_modules = num_modules\r\n        self.depth = depth\r\n        self.features = num_features\r\n\r\n        self._generate_network(self.depth)\r\n\r\n    def _generate_network(self, level):\r\n        self.add_module(\'b1_\' + str(level), ConvBlock(self.features, self.features))\r\n\r\n        self.add_module(\'b2_\' + str(level), ConvBlock(self.features, self.features))\r\n\r\n        if level > 1:\r\n            self._generate_network(level - 1)\r\n        else:\r\n            self.add_module(\'b2_plus_\' + str(level), ConvBlock(self.features, self.features))\r\n\r\n        self.add_module(\'b3_\' + str(level), ConvBlock(self.features, self.features))\r\n\r\n    def _forward(self, level, inp):\r\n        # Upper branch\r\n        up1 = inp\r\n        up1 = self._modules[\'b1_\' + str(level)](up1)\r\n\r\n        # Lower branch\r\n        low1 = F.avg_pool2d(inp, 2, stride=2)\r\n        low1 = self._modules[\'b2_\' + str(level)](low1)\r\n\r\n        if level > 1:\r\n            low2 = self._forward(level - 1, low1)\r\n        else:\r\n            low2 = low1\r\n            low2 = self._modules[\'b2_plus_\' + str(level)](low2)\r\n\r\n        low3 = low2\r\n        low3 = self._modules[\'b3_\' + str(level)](low3)\r\n\r\n        up2 = F.interpolate(low3, scale_factor=2, mode=\'nearest\')\r\n\r\n        return up1 + up2\r\n\r\n    def forward(self, x):\r\n        return self._forward(self.depth, x)\r\n\r\n\r\nclass FAN(nn.Module):\r\n\r\n    def __init__(self, num_modules=1):\r\n        super(FAN, self).__init__()\r\n        self.num_modules = num_modules\r\n\r\n        # Base part\r\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\r\n        self.bn1 = nn.BatchNorm2d(64)\r\n        self.conv2 = ConvBlock(64, 128)\r\n        self.conv3 = ConvBlock(128, 128)\r\n        self.conv4 = ConvBlock(128, 256)\r\n\r\n        # Stacking part\r\n        for hg_module in range(self.num_modules):\r\n            self.add_module(\'m\' + str(hg_module), HourGlass(1, 4, 256))\r\n            self.add_module(\'top_m_\' + str(hg_module), ConvBlock(256, 256))\r\n            self.add_module(\'conv_last\' + str(hg_module),\r\n                            nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0))\r\n            self.add_module(\'bn_end\' + str(hg_module), nn.BatchNorm2d(256))\r\n            self.add_module(\'l\' + str(hg_module), nn.Conv2d(256,\r\n                                                            68, kernel_size=1, stride=1, padding=0))\r\n\r\n            if hg_module < self.num_modules - 1:\r\n                self.add_module(\r\n                    \'bl\' + str(hg_module), nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0))\r\n                self.add_module(\'al\' + str(hg_module), nn.Conv2d(68,\r\n                                                                 256, kernel_size=1, stride=1, padding=0))\r\n\r\n    def forward(self, x):\r\n        x = F.relu(self.bn1(self.conv1(x)), True)\r\n        x = F.avg_pool2d(self.conv2(x), 2, stride=2)\r\n        x = self.conv3(x)\r\n        x = self.conv4(x)\r\n\r\n        previous = x\r\n\r\n        outputs = []\r\n        for i in range(self.num_modules):\r\n            hg = self._modules[\'m\' + str(i)](previous)\r\n\r\n            ll = hg\r\n            ll = self._modules[\'top_m_\' + str(i)](ll)\r\n\r\n            ll = F.relu(self._modules[\'bn_end\' + str(i)]\r\n                        (self._modules[\'conv_last\' + str(i)](ll)), True)\r\n\r\n            # Predict heatmaps\r\n            tmp_out = self._modules[\'l\' + str(i)](ll)\r\n            outputs.append(tmp_out)\r\n\r\n            if i < self.num_modules - 1:\r\n                ll = self._modules[\'bl\' + str(i)](ll)\r\n                tmp_out_ = self._modules[\'al\' + str(i)](tmp_out)\r\n                previous = previous + ll + tmp_out_\r\n\r\n        return outputs\r\n\r\n\r\nclass ResNetDepth(nn.Module):\r\n\r\n    def __init__(self, block=Bottleneck, layers=[3, 8, 36, 3], num_classes=68):\r\n        self.inplanes = 64\r\n        super(ResNetDepth, self).__init__()\r\n        self.conv1 = nn.Conv2d(3 + 68, 64, kernel_size=7, stride=2, padding=3,\r\n                               bias=False)\r\n        self.bn1 = nn.BatchNorm2d(64)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n        self.layer1 = self._make_layer(block, 64, layers[0])\r\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\r\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\r\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\r\n        self.avgpool = nn.AvgPool2d(7)\r\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\r\n\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\r\n                m.weight.data.normal_(0, math.sqrt(2. / n))\r\n            elif isinstance(m, nn.BatchNorm2d):\r\n                m.weight.data.fill_(1)\r\n                m.bias.data.zero_()\r\n\r\n    def _make_layer(self, block, planes, blocks, stride=1):\r\n        downsample = None\r\n        if stride != 1 or self.inplanes != planes * block.expansion:\r\n            downsample = nn.Sequential(\r\n                nn.Conv2d(self.inplanes, planes * block.expansion,\r\n                          kernel_size=1, stride=stride, bias=False),\r\n                nn.BatchNorm2d(planes * block.expansion),\r\n            )\r\n\r\n        layers = []\r\n        layers.append(block(self.inplanes, planes, stride, downsample))\r\n        self.inplanes = planes * block.expansion\r\n        for i in range(1, blocks):\r\n            layers.append(block(self.inplanes, planes))\r\n\r\n        return nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        x = self.bn1(x)\r\n        x = self.relu(x)\r\n        x = self.maxpool(x)\r\n\r\n        x = self.layer1(x)\r\n        x = self.layer2(x)\r\n        x = self.layer3(x)\r\n        x = self.layer4(x)\r\n\r\n        x = self.avgpool(x)\r\n        x = x.view(x.size(0), -1)\r\n        x = self.fc(x)\r\n\r\n        return x\r\n'"
face_alignment/utils.py,18,"b'from __future__ import print_function\nimport os\nimport sys\nimport time\nimport torch\nimport math\nimport numpy as np\nimport cv2\n\n\ndef _gaussian(\n        size=3, sigma=0.25, amplitude=1, normalize=False, width=None,\n        height=None, sigma_horz=None, sigma_vert=None, mean_horz=0.5,\n        mean_vert=0.5):\n    # handle some defaults\n    if width is None:\n        width = size\n    if height is None:\n        height = size\n    if sigma_horz is None:\n        sigma_horz = sigma\n    if sigma_vert is None:\n        sigma_vert = sigma\n    center_x = mean_horz * width + 0.5\n    center_y = mean_vert * height + 0.5\n    gauss = np.empty((height, width), dtype=np.float32)\n    # generate kernel\n    for i in range(height):\n        for j in range(width):\n            gauss[i][j] = amplitude * math.exp(-(math.pow((j + 1 - center_x) / (\n                sigma_horz * width), 2) / 2.0 + math.pow((i + 1 - center_y) / (sigma_vert * height), 2) / 2.0))\n    if normalize:\n        gauss = gauss / np.sum(gauss)\n    return gauss\n\n\ndef draw_gaussian(image, point, sigma):\n    # Check if the gaussian is inside\n    ul = [math.floor(point[0] - 3 * sigma), math.floor(point[1] - 3 * sigma)]\n    br = [math.floor(point[0] + 3 * sigma), math.floor(point[1] + 3 * sigma)]\n    if (ul[0] > image.shape[1] or ul[1] > image.shape[0] or br[0] < 1 or br[1] < 1):\n        return image\n    size = 6 * sigma + 1\n    g = _gaussian(size)\n    g_x = [int(max(1, -ul[0])), int(min(br[0], image.shape[1])) - int(max(1, ul[0])) + int(max(1, -ul[0]))]\n    g_y = [int(max(1, -ul[1])), int(min(br[1], image.shape[0])) - int(max(1, ul[1])) + int(max(1, -ul[1]))]\n    img_x = [int(max(1, ul[0])), int(min(br[0], image.shape[1]))]\n    img_y = [int(max(1, ul[1])), int(min(br[1], image.shape[0]))]\n    assert (g_x[0] > 0 and g_y[1] > 0)\n    image[img_y[0] - 1:img_y[1], img_x[0] - 1:img_x[1]\n          ] = image[img_y[0] - 1:img_y[1], img_x[0] - 1:img_x[1]] + g[g_y[0] - 1:g_y[1], g_x[0] - 1:g_x[1]]\n    image[image > 1] = 1\n    return image\n\n\ndef transform(point, center, scale, resolution, invert=False):\n    """"""Generate and affine transformation matrix.\n\n    Given a set of points, a center, a scale and a targer resolution, the\n    function generates and affine transformation matrix. If invert is ``True``\n    it will produce the inverse transformation.\n\n    Arguments:\n        point {torch.tensor} -- the input 2D point\n        center {torch.tensor or numpy.array} -- the center around which to perform the transformations\n        scale {float} -- the scale of the face/object\n        resolution {float} -- the output resolution\n\n    Keyword Arguments:\n        invert {bool} -- define wherever the function should produce the direct or the\n        inverse transformation matrix (default: {False})\n    """"""\n    _pt = torch.ones(3)\n    _pt[0] = point[0]\n    _pt[1] = point[1]\n\n    h = 200.0 * scale\n    t = torch.eye(3)\n    t[0, 0] = resolution / h\n    t[1, 1] = resolution / h\n    t[0, 2] = resolution * (-center[0] / h + 0.5)\n    t[1, 2] = resolution * (-center[1] / h + 0.5)\n\n    if invert:\n        t = torch.inverse(t)\n\n    new_point = (torch.matmul(t, _pt))[0:2]\n\n    return new_point.int()\n\n\ndef crop(image, center, scale, resolution=256.0):\n    """"""Center crops an image or set of heatmaps\n\n    Arguments:\n        image {numpy.array} -- an rgb image\n        center {numpy.array} -- the center of the object, usually the same as of the bounding box\n        scale {float} -- scale of the face\n\n    Keyword Arguments:\n        resolution {float} -- the size of the output cropped image (default: {256.0})\n\n    Returns:\n        [type] -- [description]\n    """"""  # Crop around the center point\n    """""" Crops the image around the center. Input is expected to be an np.ndarray """"""\n    ul = transform([1, 1], center, scale, resolution, True)\n    br = transform([resolution, resolution], center, scale, resolution, True)\n    # pad = math.ceil(torch.norm((ul - br).float()) / 2.0 - (br[0] - ul[0]) / 2.0)\n    if image.ndim > 2:\n        newDim = np.array([br[1] - ul[1], br[0] - ul[0],\n                           image.shape[2]], dtype=np.int32)\n        newImg = np.zeros(newDim, dtype=np.uint8)\n    else:\n        newDim = np.array([br[1] - ul[1], br[0] - ul[0]], dtype=np.int)\n        newImg = np.zeros(newDim, dtype=np.uint8)\n    ht = image.shape[0]\n    wd = image.shape[1]\n    newX = np.array(\n        [max(1, -ul[0] + 1), min(br[0], wd) - ul[0]], dtype=np.int32)\n    newY = np.array(\n        [max(1, -ul[1] + 1), min(br[1], ht) - ul[1]], dtype=np.int32)\n    oldX = np.array([max(1, ul[0] + 1), min(br[0], wd)], dtype=np.int32)\n    oldY = np.array([max(1, ul[1] + 1), min(br[1], ht)], dtype=np.int32)\n    newImg[newY[0] - 1:newY[1], newX[0] - 1:newX[1]\n           ] = image[oldY[0] - 1:oldY[1], oldX[0] - 1:oldX[1], :]\n    newImg = cv2.resize(newImg, dsize=(int(resolution), int(resolution)),\n                        interpolation=cv2.INTER_LINEAR)\n    return newImg\n\n\ndef get_preds_fromhm(hm, center=None, scale=None):\n    """"""Obtain (x,y) coordinates given a set of N heatmaps. If the center\n    and the scale is provided the function will return the points also in\n    the original coordinate frame.\n\n    Arguments:\n        hm {torch.tensor} -- the predicted heatmaps, of shape [B, N, W, H]\n\n    Keyword Arguments:\n        center {torch.tensor} -- the center of the bounding box (default: {None})\n        scale {float} -- face scale (default: {None})\n    """"""\n    max, idx = torch.max(\n        hm.view(hm.size(0), hm.size(1), hm.size(2) * hm.size(3)), 2)\n    idx += 1\n    preds = idx.view(idx.size(0), idx.size(1), 1).repeat(1, 1, 2).float()\n    preds[..., 0].apply_(lambda x: (x - 1) % hm.size(3) + 1)\n    preds[..., 1].add_(-1).div_(hm.size(2)).floor_().add_(1)\n\n    for i in range(preds.size(0)):\n        for j in range(preds.size(1)):\n            hm_ = hm[i, j, :]\n            pX, pY = int(preds[i, j, 0]) - 1, int(preds[i, j, 1]) - 1\n            if pX > 0 and pX < 63 and pY > 0 and pY < 63:\n                diff = torch.FloatTensor(\n                    [hm_[pY, pX + 1] - hm_[pY, pX - 1],\n                     hm_[pY + 1, pX] - hm_[pY - 1, pX]])\n                preds[i, j].add_(diff.sign_().mul_(.25))\n\n    preds.add_(-.5)\n\n    preds_orig = torch.zeros(preds.size())\n    if center is not None and scale is not None:\n        for i in range(hm.size(0)):\n            for j in range(hm.size(1)):\n                preds_orig[i, j] = transform(\n                    preds[i, j], center, scale, hm.size(2), True)\n\n    return preds, preds_orig\n\n\ndef create_target_heatmap(target_landmarks, centers, scales):\n    heatmaps = np.zeros((target_landmarks.shape[0], 68, 64, 64), dtype=np.float32)\n    for i in range(heatmaps.shape[0]):\n        for p in range(68):\n            landmark_cropped_coor = transform(target_landmarks[i, p] + 1, centers[i], scales[i], 64, invert=False)\n            heatmaps[i, p] = draw_gaussian(heatmaps[i, p], landmark_cropped_coor + 1, 1)\n    return torch.tensor(heatmaps)\n\n\ndef create_bounding_box(target_landmarks, expansion_factor=0.0):\n    """"""\n    gets a batch of landmarks and calculates a bounding box that includes all the landmarks per set of landmarks in\n    the batch\n    :param target_landmarks: batch of landmarks of dim (n x 68 x 2). Where n is the batch size\n    :param expansion_factor: expands the bounding box by this factor. For example, a `expansion_factor` of 0.2 leads\n    to 20% increase in width and height of the boxes\n    :return: a batch of bounding boxes of dim (n x 4) where the second dim is (x1,y1,x2,y2)\n    """"""\n    # Calc bounding box\n    x_y_min, _ = target_landmarks.reshape(-1, 68, 2).min(dim=1)\n    x_y_max, _ = target_landmarks.reshape(-1, 68, 2).max(dim=1)\n    # expanding the bounding box\n    expansion_factor /= 2\n    bb_expansion_x = (x_y_max[:, 0] - x_y_min[:, 0]) * expansion_factor\n    bb_expansion_y = (x_y_max[:, 1] - x_y_min[:, 1]) * expansion_factor\n    x_y_min[:, 0] -= bb_expansion_x\n    x_y_max[:, 0] += bb_expansion_x\n    x_y_min[:, 1] -= bb_expansion_y\n    x_y_max[:, 1] += bb_expansion_y\n    return torch.cat([x_y_min, x_y_max], dim=1)\n\n\ndef shuffle_lr(parts, pairs=None):\n    """"""Shuffle the points left-right according to the axis of symmetry\n    of the object.\n\n    Arguments:\n        parts {torch.tensor} -- a 3D or 4D object containing the\n        heatmaps.\n\n    Keyword Arguments:\n        pairs {list of integers} -- [order of the flipped points] (default: {None})\n    """"""\n    if pairs is None:\n        pairs = [16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0,\n                 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 27, 28, 29, 30, 35,\n                 34, 33, 32, 31, 45, 44, 43, 42, 47, 46, 39, 38, 37, 36, 41,\n                 40, 54, 53, 52, 51, 50, 49, 48, 59, 58, 57, 56, 55, 64, 63,\n                 62, 61, 60, 67, 66, 65]\n    if parts.ndimension() == 3:\n        parts = parts[pairs, ...]\n    else:\n        parts = parts[:, pairs, ...]\n\n    return parts\n\n\ndef flip(tensor, is_label=False):\n    """"""Flip an image or a set of heatmaps left-right\n\n    Arguments:\n        tensor {numpy.array or torch.tensor} -- [the input image or heatmaps]\n\n    Keyword Arguments:\n        is_label {bool} -- [denote wherever the input is an image or a set of heatmaps ] (default: {False})\n    """"""\n    if not torch.is_tensor(tensor):\n        tensor = torch.from_numpy(tensor)\n\n    if is_label:\n        tensor = shuffle_lr(tensor).flip(tensor.ndimension() - 1)\n    else:\n        tensor = tensor.flip(tensor.ndimension() - 1)\n\n    return tensor\n\n# From pyzolib/paths.py (https://bitbucket.org/pyzo/pyzolib/src/tip/paths.py)\n\n\ndef appdata_dir(appname=None, roaming=False):\n    """""" appdata_dir(appname=None, roaming=False)\n\n    Get the path to the application directory, where applications are allowed\n    to write user specific files (e.g. configurations). For non-user specific\n    data, consider using common_appdata_dir().\n    If appname is given, a subdir is appended (and created if necessary).\n    If roaming is True, will prefer a roaming directory (Windows Vista/7).\n    """"""\n\n    # Define default user directory\n    userDir = os.getenv(\'FACEALIGNMENT_USERDIR\', None)\n    if userDir is None:\n        userDir = os.path.expanduser(\'~\')\n        if not os.path.isdir(userDir):  # pragma: no cover\n            userDir = \'/var/tmp\'  # issue #54\n\n    # Get system app data dir\n    path = None\n    if sys.platform.startswith(\'win\'):\n        path1, path2 = os.getenv(\'LOCALAPPDATA\'), os.getenv(\'APPDATA\')\n        path = (path2 or path1) if roaming else (path1 or path2)\n    elif sys.platform.startswith(\'darwin\'):\n        path = os.path.join(userDir, \'Library\', \'Application Support\')\n    # On Linux and as fallback\n    if not (path and os.path.isdir(path)):\n        path = userDir\n\n    # Maybe we should store things local to the executable (in case of a\n    # portable distro or a frozen application that wants to be portable)\n    prefix = sys.prefix\n    if getattr(sys, \'frozen\', None):\n        prefix = os.path.abspath(os.path.dirname(sys.executable))\n    for reldir in (\'settings\', \'../settings\'):\n        localpath = os.path.abspath(os.path.join(prefix, reldir))\n        if os.path.isdir(localpath):  # pragma: no cover\n            try:\n                open(os.path.join(localpath, \'test.write\'), \'wb\').close()\n                os.remove(os.path.join(localpath, \'test.write\'))\n            except IOError:\n                pass  # We cannot write in this directory\n            else:\n                path = localpath\n                break\n\n    # Get path specific for this app\n    if appname:\n        if path == userDir:\n            appname = \'.\' + appname.lstrip(\'.\')  # Make it a hidden directory\n        path = os.path.join(path, appname)\n        if not os.path.isdir(path):  # pragma: no cover\n            os.mkdir(path)\n\n    # Done\n    return path\n'"
test/facealignment_test.py,0,"b""import unittest\r\nimport face_alignment\r\n\r\n\r\nclass Tester(unittest.TestCase):\r\n    def test_predict_points(self):\r\n        fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._3D, device='cpu')\r\n        fa.get_landmarks('test/assets/aflw-test.jpg')\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n"""
test/smoke_test.py,0,b'import torch\r\nimport face_alignment\r\n'
test/test_utils.py,6,"b""import sys\r\nsys.path.append('.')\r\nimport unittest\r\nfrom face_alignment.utils import *\r\nimport numpy as np\r\nimport torch\r\n\r\n\r\nclass Tester(unittest.TestCase):\r\n    def test_flip_is_label(self):\r\n        # Generate the points\r\n        heatmaps = torch.from_numpy(np.random.randint(1, high=250, size=(68, 64, 64)).astype('float32'))\r\n\r\n        flipped_heatmaps = flip(flip(heatmaps.clone(), is_label=True), is_label=True)\r\n\r\n        assert np.allclose(heatmaps.numpy(), flipped_heatmaps.numpy())\r\n\r\n    def test_flip_is_image(self):\r\n        fake_image = torch.torch.rand(3, 256, 256)\r\n        fliped_fake_image = flip(flip(fake_image.clone()))\r\n\r\n        assert np.allclose(fake_image.numpy(), fliped_fake_image.numpy())\r\n\r\n    def test_getpreds(self):\r\n        pts = torch.from_numpy(np.random.randint(1, high=63, size=(68, 2)).astype('float32'))\r\n\r\n        heatmaps = np.zeros((68, 256, 256))\r\n        for i in range(68):\r\n            if pts[i, 0] > 0:\r\n                heatmaps[i] = draw_gaussian(heatmaps[i], pts[i], 2)\r\n        heatmaps = torch.from_numpy(np.expand_dims(heatmaps, axis=0))\r\n\r\n        preds, _ = get_preds_fromhm(heatmaps)\r\n\r\n        assert np.allclose(pts.numpy(), preds.numpy(), atol=5)\r\n\r\n    def test_create_heatmaps(self):\r\n        reference_scale = 195\r\n        target_landmarks = torch.randint(0, 255, (1, 68, 2)).type(torch.float)  # simulated dataset\r\n        bb = create_bounding_box(target_landmarks)\r\n        centers = torch.stack([bb[:, 2] - (bb[:, 2] - bb[:, 0]) / 2.0, bb[:, 3] - (bb[:, 3] - bb[:, 1]) / 2.0], dim=1)\r\n        centers[:, 1] = centers[:, 1] - (bb[:, 3] - bb[:, 1]) * 0.12  # Not sure where 0.12 comes from\r\n        scales = (bb[:, 2] - bb[:, 0] + bb[:, 3] - bb[:, 1]) / reference_scale\r\n        heatmaps = create_target_heatmap(target_landmarks, centers, scales)\r\n        preds = get_preds_fromhm(heatmaps, centers.squeeze(), scales.squeeze())[1]\r\n\r\n        assert np.allclose(preds.numpy(), target_landmarks.numpy(), atol=5)\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n"""
face_alignment/detection/__init__.py,0,b'from .core import FaceDetector'
face_alignment/detection/core.py,4,"b'import logging\r\nimport glob\r\nfrom tqdm import tqdm\r\nimport numpy as np\r\nimport torch\r\nimport cv2\r\nfrom skimage import io\r\n\r\n\r\nclass FaceDetector(object):\r\n    """"""An abstract class representing a face detector.\r\n\r\n    Any other face detection implementation must subclass it. All subclasses\r\n    must implement ``detect_from_image``, that return a list of detected\r\n    bounding boxes. Optionally, for speed considerations detect from path is\r\n    recommended.\r\n    """"""\r\n\r\n    def __init__(self, device, verbose):\r\n        self.device = device\r\n        self.verbose = verbose\r\n\r\n        if verbose:\r\n            if \'cpu\' in device:\r\n                logger = logging.getLogger(__name__)\r\n                logger.warning(""Detection running on CPU, this may be potentially slow."")\r\n\r\n        if \'cpu\' not in device and \'cuda\' not in device:\r\n            if verbose:\r\n                logger.error(""Expected values for device are: {cpu, cuda} but got: %s"", device)\r\n            raise ValueError\r\n\r\n    def detect_from_image(self, tensor_or_path):\r\n        """"""Detects faces in a given image.\r\n\r\n        This function detects the faces present in a provided BGR(usually)\r\n        image. The input can be either the image itself or the path to it.\r\n\r\n        Arguments:\r\n            tensor_or_path {numpy.ndarray, torch.tensor or string} -- the path\r\n            to an image or the image itself.\r\n\r\n        Example::\r\n\r\n            >>> path_to_image = \'data/image_01.jpg\'\r\n            ...   detected_faces = detect_from_image(path_to_image)\r\n            [A list of bounding boxes (x1, y1, x2, y2)]\r\n            >>> image = cv2.imread(path_to_image)\r\n            ...   detected_faces = detect_from_image(image)\r\n            [A list of bounding boxes (x1, y1, x2, y2)]\r\n\r\n        """"""\r\n        raise NotImplementedError\r\n\r\n    def detect_from_directory(self, path, extensions=[\'.jpg\', \'.png\'], recursive=False, show_progress_bar=True):\r\n        """"""Detects faces from all the images present in a given directory.\r\n\r\n        Arguments:\r\n            path {string} -- a string containing a path that points to the folder containing the images\r\n\r\n        Keyword Arguments:\r\n            extensions {list} -- list of string containing the extensions to be\r\n            consider in the following format: ``.extension_name`` (default:\r\n            {[\'.jpg\', \'.png\']}) recursive {bool} -- option wherever to scan the\r\n            folder recursively (default: {False}) show_progress_bar {bool} --\r\n            display a progressbar (default: {True})\r\n\r\n        Example:\r\n        >>> directory = \'data\'\r\n        ...   detected_faces = detect_from_directory(directory)\r\n        {A dictionary of [lists containing bounding boxes(x1, y1, x2, y2)]}\r\n\r\n        """"""\r\n        if self.verbose:\r\n            logger = logging.getLogger(__name__)\r\n\r\n        if len(extensions) == 0:\r\n            if self.verbose:\r\n                logger.error(""Expected at list one extension, but none was received."")\r\n            raise ValueError\r\n\r\n        if self.verbose:\r\n            logger.info(""Constructing the list of images."")\r\n        additional_pattern = \'/**/*\' if recursive else \'/*\'\r\n        files = []\r\n        for extension in extensions:\r\n            files.extend(glob.glob(path + additional_pattern + extension, recursive=recursive))\r\n\r\n        if self.verbose:\r\n            logger.info(""Finished searching for images. %s images found"", len(files))\r\n            logger.info(""Preparing to run the detection."")\r\n\r\n        predictions = {}\r\n        for image_path in tqdm(files, disable=not show_progress_bar):\r\n            if self.verbose:\r\n                logger.info(""Running the face detector on image: %s"", image_path)\r\n            predictions[image_path] = self.detect_from_image(image_path)\r\n\r\n        if self.verbose:\r\n            logger.info(""The detector was successfully run on all %s images"", len(files))\r\n\r\n        return predictions\r\n\r\n    @property\r\n    def reference_scale(self):\r\n        raise NotImplementedError\r\n\r\n    @property\r\n    def reference_x_shift(self):\r\n        raise NotImplementedError\r\n\r\n    @property\r\n    def reference_y_shift(self):\r\n        raise NotImplementedError\r\n\r\n    @staticmethod\r\n    def tensor_or_path_to_ndarray(tensor_or_path, rgb=True):\r\n        """"""Convert path (represented as a string) or torch.tensor to a numpy.ndarray\r\n\r\n        Arguments:\r\n            tensor_or_path {numpy.ndarray, torch.tensor or string} -- path to the image, or the image itself\r\n        """"""\r\n        if isinstance(tensor_or_path, str):\r\n            return cv2.imread(tensor_or_path) if not rgb else io.imread(tensor_or_path)\r\n        elif torch.is_tensor(tensor_or_path):\r\n            # Call cpu in case its coming from cuda\r\n            return tensor_or_path.cpu().numpy()[..., ::-1].copy() if not rgb else tensor_or_path.cpu().numpy()\r\n        elif isinstance(tensor_or_path, np.ndarray):\r\n            return tensor_or_path[..., ::-1].copy() if not rgb else tensor_or_path\r\n        else:\r\n            raise TypeError\r\n'"
face_alignment/detection/dlib/__init__.py,0,b'from .dlib_detector import DlibDetector as FaceDetector'
face_alignment/detection/dlib/dlib_detector.py,0,"b'import os\r\nimport cv2\r\nimport dlib\r\n\r\ntry:\r\n    import urllib.request as request_file\r\nexcept BaseException:\r\n    import urllib as request_file\r\n\r\nfrom ..core import FaceDetector\r\nfrom ...utils import appdata_dir\r\n\r\n\r\nclass DlibDetector(FaceDetector):\r\n    def __init__(self, device, path_to_detector=None, verbose=False):\r\n        super().__init__(device, verbose)\r\n\r\n        print(\'Warning: this detector is deprecated. Please use a different one, i.e.: S3FD.\')\r\n        base_path = os.path.join(appdata_dir(\'face_alignment\'), ""data"")\r\n\r\n        # Initialise the face detector\r\n        if \'cuda\' in device:\r\n            if path_to_detector is None:\r\n                path_to_detector = os.path.join(\r\n                    base_path, ""mmod_human_face_detector.dat"")\r\n\r\n                if not os.path.isfile(path_to_detector):\r\n                    print(""Downloading the face detection CNN. Please wait..."")\r\n\r\n                    path_to_temp_detector = os.path.join(\r\n                        base_path, ""mmod_human_face_detector.dat.download"")\r\n\r\n                    if os.path.isfile(path_to_temp_detector):\r\n                        os.remove(os.path.join(path_to_temp_detector))\r\n\r\n                    request_file.urlretrieve(\r\n                        ""https://www.adrianbulat.com/downloads/dlib/mmod_human_face_detector.dat"",\r\n                        os.path.join(path_to_temp_detector))\r\n\r\n                    os.rename(os.path.join(path_to_temp_detector), os.path.join(path_to_detector))\r\n\r\n            self.face_detector = dlib.cnn_face_detection_model_v1(path_to_detector)\r\n        else:\r\n            self.face_detector = dlib.get_frontal_face_detector()\r\n\r\n    def detect_from_image(self, tensor_or_path):\r\n        image = self.tensor_or_path_to_ndarray(tensor_or_path, rgb=False)\r\n\r\n        detected_faces = self.face_detector(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))\r\n\r\n        if \'cuda\' not in self.device:\r\n            detected_faces = [[d.left(), d.top(), d.right(), d.bottom()] for d in detected_faces]\r\n        else:\r\n            detected_faces = [[d.rect.left(), d.rect.top(), d.rect.right(), d.rect.bottom()] for d in detected_faces]\r\n\r\n        return detected_faces\r\n\r\n    @property\r\n    def reference_scale(self):\r\n        return 195\r\n\r\n    @property\r\n    def reference_x_shift(self):\r\n        return 0\r\n\r\n    @property\r\n    def reference_y_shift(self):\r\n        return 0\r\n'"
face_alignment/detection/folder/__init__.py,0,b'from .folder_detector import FolderDetector as FaceDetector'
face_alignment/detection/folder/folder_detector.py,2,"b""import os\r\nimport numpy as np\r\nimport torch\r\n\r\nfrom ..core import FaceDetector\r\n\r\n\r\nclass FolderDetector(FaceDetector):\r\n    '''This is a simple helper module that assumes the faces were detected already\r\n        (either previously or are provided as ground truth).\r\n\r\n        The class expects to find the bounding boxes in the same format used by\r\n        the rest of face detectors, mainly ``list[(x1,y1,x2,y2),...]``.\r\n        For each image the detector will search for a file with the same name and with one of the\r\n        following extensions: .npy, .t7 or .pth\r\n\r\n    '''\r\n\r\n    def __init__(self, device, path_to_detector=None, verbose=False):\r\n        super(FolderDetector, self).__init__(device, verbose)\r\n\r\n    def detect_from_image(self, tensor_or_path):\r\n        # Only strings supported\r\n        if not isinstance(tensor_or_path, str):\r\n            raise ValueError\r\n\r\n        base_name = os.path.splitext(tensor_or_path)[0]\r\n\r\n        if os.path.isfile(base_name + '.npy'):\r\n            detected_faces = np.load(base_name + '.npy')\r\n        elif os.path.isfile(base_name + '.t7'):\r\n            detected_faces = torch.load(base_name + '.t7')\r\n        elif os.path.isfile(base_name + '.pth'):\r\n            detected_faces = torch.load(base_name + '.pth')\r\n        else:\r\n            raise FileNotFoundError\r\n\r\n        if not isinstance(detected_faces, list):\r\n            raise TypeError\r\n\r\n        return detected_faces\r\n\r\n    @property\r\n    def reference_scale(self):\r\n        return 195\r\n\r\n    @property\r\n    def reference_x_shift(self):\r\n        return 0\r\n\r\n    @property\r\n    def reference_y_shift(self):\r\n        return 0\r\n"""
face_alignment/detection/sfd/__init__.py,0,b'from .sfd_detector import SFDDetector as FaceDetector'
face_alignment/detection/sfd/bbox.py,4,"b'from __future__ import print_function\r\nimport os\r\nimport sys\r\nimport cv2\r\nimport random\r\nimport datetime\r\nimport time\r\nimport math\r\nimport argparse\r\nimport numpy as np\r\nimport torch\r\n\r\ntry:\r\n    from iou import IOU\r\nexcept BaseException:\r\n    # IOU cython speedup 10x\r\n    def IOU(ax1, ay1, ax2, ay2, bx1, by1, bx2, by2):\r\n        sa = abs((ax2 - ax1) * (ay2 - ay1))\r\n        sb = abs((bx2 - bx1) * (by2 - by1))\r\n        x1, y1 = max(ax1, bx1), max(ay1, by1)\r\n        x2, y2 = min(ax2, bx2), min(ay2, by2)\r\n        w = x2 - x1\r\n        h = y2 - y1\r\n        if w < 0 or h < 0:\r\n            return 0.0\r\n        else:\r\n            return 1.0 * w * h / (sa + sb - w * h)\r\n\r\n\r\ndef bboxlog(x1, y1, x2, y2, axc, ayc, aww, ahh):\r\n    xc, yc, ww, hh = (x2 + x1) / 2, (y2 + y1) / 2, x2 - x1, y2 - y1\r\n    dx, dy = (xc - axc) / aww, (yc - ayc) / ahh\r\n    dw, dh = math.log(ww / aww), math.log(hh / ahh)\r\n    return dx, dy, dw, dh\r\n\r\n\r\ndef bboxloginv(dx, dy, dw, dh, axc, ayc, aww, ahh):\r\n    xc, yc = dx * aww + axc, dy * ahh + ayc\r\n    ww, hh = math.exp(dw) * aww, math.exp(dh) * ahh\r\n    x1, x2, y1, y2 = xc - ww / 2, xc + ww / 2, yc - hh / 2, yc + hh / 2\r\n    return x1, y1, x2, y2\r\n\r\n\r\ndef nms(dets, thresh):\r\n    if 0 == len(dets):\r\n        return []\r\n    x1, y1, x2, y2, scores = dets[:, 0], dets[:, 1], dets[:, 2], dets[:, 3], dets[:, 4]\r\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\r\n    order = scores.argsort()[::-1]\r\n\r\n    keep = []\r\n    while order.size > 0:\r\n        i = order[0]\r\n        keep.append(i)\r\n        xx1, yy1 = np.maximum(x1[i], x1[order[1:]]), np.maximum(y1[i], y1[order[1:]])\r\n        xx2, yy2 = np.minimum(x2[i], x2[order[1:]]), np.minimum(y2[i], y2[order[1:]])\r\n\r\n        w, h = np.maximum(0.0, xx2 - xx1 + 1), np.maximum(0.0, yy2 - yy1 + 1)\r\n        ovr = w * h / (areas[i] + areas[order[1:]] - w * h)\r\n\r\n        inds = np.where(ovr <= thresh)[0]\r\n        order = order[inds + 1]\r\n\r\n    return keep\r\n\r\n\r\ndef encode(matched, priors, variances):\r\n    """"""Encode the variances from the priorbox layers into the ground truth boxes\r\n    we have matched (based on jaccard overlap) with the prior boxes.\r\n    Args:\r\n        matched: (tensor) Coords of ground truth for each prior in point-form\r\n            Shape: [num_priors, 4].\r\n        priors: (tensor) Prior boxes in center-offset form\r\n            Shape: [num_priors,4].\r\n        variances: (list[float]) Variances of priorboxes\r\n    Return:\r\n        encoded boxes (tensor), Shape: [num_priors, 4]\r\n    """"""\r\n\r\n    # dist b/t match center and prior\'s center\r\n    g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]\r\n    # encode variance\r\n    g_cxcy /= (variances[0] * priors[:, 2:])\r\n    # match wh / prior wh\r\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\r\n    g_wh = torch.log(g_wh) / variances[1]\r\n    # return target for smooth_l1_loss\r\n    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\r\n\r\n\r\ndef decode(loc, priors, variances):\r\n    """"""Decode locations from predictions using priors to undo\r\n    the encoding we did for offset regression at train time.\r\n    Args:\r\n        loc (tensor): location predictions for loc layers,\r\n            Shape: [num_priors,4]\r\n        priors (tensor): Prior boxes in center-offset form.\r\n            Shape: [num_priors,4].\r\n        variances: (list[float]) Variances of priorboxes\r\n    Return:\r\n        decoded bounding box predictions\r\n    """"""\r\n\r\n    boxes = torch.cat((\r\n        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\r\n        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\r\n    boxes[:, :2] -= boxes[:, 2:] / 2\r\n    boxes[:, 2:] += boxes[:, :2]\r\n    return boxes\r\n'"
face_alignment/detection/sfd/detect.py,5,"b""import torch\r\nimport torch.nn.functional as F\r\n\r\nimport os\r\nimport sys\r\nimport cv2\r\nimport random\r\nimport datetime\r\nimport math\r\nimport argparse\r\nimport numpy as np\r\n\r\nimport scipy.io as sio\r\nimport zipfile\r\nfrom .net_s3fd import s3fd\r\nfrom .bbox import *\r\n\r\n\r\ndef detect(net, img, device):\r\n    img = img - np.array([104, 117, 123])\r\n    img = img.transpose(2, 0, 1)\r\n    img = img.reshape((1,) + img.shape)\r\n\r\n    if 'cuda' in device:\r\n        torch.backends.cudnn.benchmark = True\r\n\r\n    img = torch.from_numpy(img).float().to(device)\r\n    BB, CC, HH, WW = img.size()\r\n    with torch.no_grad():\r\n        olist = net(img)\r\n\r\n    bboxlist = []\r\n    for i in range(len(olist) // 2):\r\n        olist[i * 2] = F.softmax(olist[i * 2], dim=1)\r\n    olist = [oelem.data.cpu() for oelem in olist]\r\n    for i in range(len(olist) // 2):\r\n        ocls, oreg = olist[i * 2], olist[i * 2 + 1]\r\n        FB, FC, FH, FW = ocls.size()  # feature map size\r\n        stride = 2**(i + 2)    # 4,8,16,32,64,128\r\n        anchor = stride * 4\r\n        poss = zip(*np.where(ocls[:, 1, :, :] > 0.05))\r\n        for Iindex, hindex, windex in poss:\r\n            axc, ayc = stride / 2 + windex * stride, stride / 2 + hindex * stride\r\n            score = ocls[0, 1, hindex, windex]\r\n            loc = oreg[0, :, hindex, windex].contiguous().view(1, 4)\r\n            priors = torch.Tensor([[axc / 1.0, ayc / 1.0, stride * 4 / 1.0, stride * 4 / 1.0]])\r\n            variances = [0.1, 0.2]\r\n            box = decode(loc, priors, variances)\r\n            x1, y1, x2, y2 = box[0] * 1.0\r\n            # cv2.rectangle(imgshow,(int(x1),int(y1)),(int(x2),int(y2)),(0,0,255),1)\r\n            bboxlist.append([x1, y1, x2, y2, score])\r\n    bboxlist = np.array(bboxlist)\r\n    if 0 == len(bboxlist):\r\n        bboxlist = np.zeros((1, 5))\r\n\r\n    return bboxlist\r\n\r\n\r\ndef flip_detect(net, img, device):\r\n    img = cv2.flip(img, 1)\r\n    b = detect(net, img, device)\r\n\r\n    bboxlist = np.zeros(b.shape)\r\n    bboxlist[:, 0] = img.shape[1] - b[:, 2]\r\n    bboxlist[:, 1] = b[:, 1]\r\n    bboxlist[:, 2] = img.shape[1] - b[:, 0]\r\n    bboxlist[:, 3] = b[:, 3]\r\n    bboxlist[:, 4] = b[:, 4]\r\n    return bboxlist\r\n\r\n\r\ndef pts_to_bb(pts):\r\n    min_x, min_y = np.min(pts, axis=0)\r\n    max_x, max_y = np.max(pts, axis=0)\r\n    return np.array([min_x, min_y, max_x, max_y])\r\n"""
face_alignment/detection/sfd/net_s3fd.py,6,"b'import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass L2Norm(nn.Module):\r\n    def __init__(self, n_channels, scale=1.0):\r\n        super(L2Norm, self).__init__()\r\n        self.n_channels = n_channels\r\n        self.scale = scale\r\n        self.eps = 1e-10\r\n        self.weight = nn.Parameter(torch.Tensor(self.n_channels))\r\n        self.weight.data *= 0.0\r\n        self.weight.data += self.scale\r\n\r\n    def forward(self, x):\r\n        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt() + self.eps\r\n        x = x / norm * self.weight.view(1, -1, 1, 1)\r\n        return x\r\n\r\n\r\nclass s3fd(nn.Module):\r\n    def __init__(self):\r\n        super(s3fd, self).__init__()\r\n        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\r\n        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\r\n\r\n        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\r\n        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\r\n\r\n        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\r\n        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\r\n        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\r\n\r\n        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\r\n        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\r\n        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\r\n\r\n        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\r\n        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\r\n        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\r\n\r\n        self.fc6 = nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=3)\r\n        self.fc7 = nn.Conv2d(1024, 1024, kernel_size=1, stride=1, padding=0)\r\n\r\n        self.conv6_1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\r\n        self.conv6_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\r\n\r\n        self.conv7_1 = nn.Conv2d(512, 128, kernel_size=1, stride=1, padding=0)\r\n        self.conv7_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\r\n\r\n        self.conv3_3_norm = L2Norm(256, scale=10)\r\n        self.conv4_3_norm = L2Norm(512, scale=8)\r\n        self.conv5_3_norm = L2Norm(512, scale=5)\r\n\r\n        self.conv3_3_norm_mbox_conf = nn.Conv2d(256, 4, kernel_size=3, stride=1, padding=1)\r\n        self.conv3_3_norm_mbox_loc = nn.Conv2d(256, 4, kernel_size=3, stride=1, padding=1)\r\n        self.conv4_3_norm_mbox_conf = nn.Conv2d(512, 2, kernel_size=3, stride=1, padding=1)\r\n        self.conv4_3_norm_mbox_loc = nn.Conv2d(512, 4, kernel_size=3, stride=1, padding=1)\r\n        self.conv5_3_norm_mbox_conf = nn.Conv2d(512, 2, kernel_size=3, stride=1, padding=1)\r\n        self.conv5_3_norm_mbox_loc = nn.Conv2d(512, 4, kernel_size=3, stride=1, padding=1)\r\n\r\n        self.fc7_mbox_conf = nn.Conv2d(1024, 2, kernel_size=3, stride=1, padding=1)\r\n        self.fc7_mbox_loc = nn.Conv2d(1024, 4, kernel_size=3, stride=1, padding=1)\r\n        self.conv6_2_mbox_conf = nn.Conv2d(512, 2, kernel_size=3, stride=1, padding=1)\r\n        self.conv6_2_mbox_loc = nn.Conv2d(512, 4, kernel_size=3, stride=1, padding=1)\r\n        self.conv7_2_mbox_conf = nn.Conv2d(256, 2, kernel_size=3, stride=1, padding=1)\r\n        self.conv7_2_mbox_loc = nn.Conv2d(256, 4, kernel_size=3, stride=1, padding=1)\r\n\r\n    def forward(self, x):\r\n        h = F.relu(self.conv1_1(x))\r\n        h = F.relu(self.conv1_2(h))\r\n        h = F.max_pool2d(h, 2, 2)\r\n\r\n        h = F.relu(self.conv2_1(h))\r\n        h = F.relu(self.conv2_2(h))\r\n        h = F.max_pool2d(h, 2, 2)\r\n\r\n        h = F.relu(self.conv3_1(h))\r\n        h = F.relu(self.conv3_2(h))\r\n        h = F.relu(self.conv3_3(h))\r\n        f3_3 = h\r\n        h = F.max_pool2d(h, 2, 2)\r\n\r\n        h = F.relu(self.conv4_1(h))\r\n        h = F.relu(self.conv4_2(h))\r\n        h = F.relu(self.conv4_3(h))\r\n        f4_3 = h\r\n        h = F.max_pool2d(h, 2, 2)\r\n\r\n        h = F.relu(self.conv5_1(h))\r\n        h = F.relu(self.conv5_2(h))\r\n        h = F.relu(self.conv5_3(h))\r\n        f5_3 = h\r\n        h = F.max_pool2d(h, 2, 2)\r\n\r\n        h = F.relu(self.fc6(h))\r\n        h = F.relu(self.fc7(h))\r\n        ffc7 = h\r\n        h = F.relu(self.conv6_1(h))\r\n        h = F.relu(self.conv6_2(h))\r\n        f6_2 = h\r\n        h = F.relu(self.conv7_1(h))\r\n        h = F.relu(self.conv7_2(h))\r\n        f7_2 = h\r\n\r\n        f3_3 = self.conv3_3_norm(f3_3)\r\n        f4_3 = self.conv4_3_norm(f4_3)\r\n        f5_3 = self.conv5_3_norm(f5_3)\r\n\r\n        cls1 = self.conv3_3_norm_mbox_conf(f3_3)\r\n        reg1 = self.conv3_3_norm_mbox_loc(f3_3)\r\n        cls2 = self.conv4_3_norm_mbox_conf(f4_3)\r\n        reg2 = self.conv4_3_norm_mbox_loc(f4_3)\r\n        cls3 = self.conv5_3_norm_mbox_conf(f5_3)\r\n        reg3 = self.conv5_3_norm_mbox_loc(f5_3)\r\n        cls4 = self.fc7_mbox_conf(ffc7)\r\n        reg4 = self.fc7_mbox_loc(ffc7)\r\n        cls5 = self.conv6_2_mbox_conf(f6_2)\r\n        reg5 = self.conv6_2_mbox_loc(f6_2)\r\n        cls6 = self.conv7_2_mbox_conf(f7_2)\r\n        reg6 = self.conv7_2_mbox_loc(f7_2)\r\n\r\n        # max-out background label\r\n        chunk = torch.chunk(cls1, 4, 1)\r\n        bmax = torch.max(torch.max(chunk[0], chunk[1]), chunk[2])\r\n        cls1 = torch.cat([bmax, chunk[3]], dim=1)\r\n\r\n        return [cls1, reg1, cls2, reg2, cls3, reg3, cls4, reg4, cls5, reg5, cls6, reg6]\r\n'"
face_alignment/detection/sfd/sfd_detector.py,2,"b""import os\r\nimport cv2\r\nfrom torch.utils.model_zoo import load_url\r\n\r\nfrom ..core import FaceDetector\r\n\r\nfrom .net_s3fd import s3fd\r\nfrom .bbox import *\r\nfrom .detect import *\r\n\r\nmodels_urls = {\r\n    's3fd': 'https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth',\r\n}\r\n\r\n\r\nclass SFDDetector(FaceDetector):\r\n    def __init__(self, device, path_to_detector=None, verbose=False):\r\n        super(SFDDetector, self).__init__(device, verbose)\r\n\r\n        # Initialise the face detector\r\n        if path_to_detector is None:\r\n            model_weights = load_url(models_urls['s3fd'])\r\n        else:\r\n            model_weights = torch.load(path_to_detector)\r\n\r\n        self.face_detector = s3fd()\r\n        self.face_detector.load_state_dict(model_weights)\r\n        self.face_detector.to(device)\r\n        self.face_detector.eval()\r\n\r\n    def detect_from_image(self, tensor_or_path):\r\n        image = self.tensor_or_path_to_ndarray(tensor_or_path)\r\n\r\n        bboxlist = detect(self.face_detector, image, device=self.device)\r\n        keep = nms(bboxlist, 0.3)\r\n        bboxlist = bboxlist[keep, :]\r\n        bboxlist = [x for x in bboxlist if x[-1] > 0.5]\r\n\r\n        return bboxlist\r\n\r\n    @property\r\n    def reference_scale(self):\r\n        return 195\r\n\r\n    @property\r\n    def reference_x_shift(self):\r\n        return 0\r\n\r\n    @property\r\n    def reference_y_shift(self):\r\n        return 0\r\n"""
