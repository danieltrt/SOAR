file_path,api_count,code
archs.py,14,"b""import torch\nfrom torch import nn\n\n__all__ = ['UNet', 'NestedUNet']\n\n\nclass VGGBlock(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super().__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_channels, middle_channels, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(middle_channels)\n        self.conv2 = nn.Conv2d(middle_channels, out_channels, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        return out\n\n\nclass UNet(nn.Module):\n    def __init__(self, num_classes, input_channels=3, **kwargs):\n        super().__init__()\n\n        nb_filter = [32, 64, 128, 256, 512]\n\n        self.pool = nn.MaxPool2d(2, 2)\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\n        self.conv0_0 = VGGBlock(input_channels, nb_filter[0], nb_filter[0])\n        self.conv1_0 = VGGBlock(nb_filter[0], nb_filter[1], nb_filter[1])\n        self.conv2_0 = VGGBlock(nb_filter[1], nb_filter[2], nb_filter[2])\n        self.conv3_0 = VGGBlock(nb_filter[2], nb_filter[3], nb_filter[3])\n        self.conv4_0 = VGGBlock(nb_filter[3], nb_filter[4], nb_filter[4])\n\n        self.conv3_1 = VGGBlock(nb_filter[3]+nb_filter[4], nb_filter[3], nb_filter[3])\n        self.conv2_2 = VGGBlock(nb_filter[2]+nb_filter[3], nb_filter[2], nb_filter[2])\n        self.conv1_3 = VGGBlock(nb_filter[1]+nb_filter[2], nb_filter[1], nb_filter[1])\n        self.conv0_4 = VGGBlock(nb_filter[0]+nb_filter[1], nb_filter[0], nb_filter[0])\n\n        self.final = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n\n\n    def forward(self, input):\n        x0_0 = self.conv0_0(input)\n        x1_0 = self.conv1_0(self.pool(x0_0))\n        x2_0 = self.conv2_0(self.pool(x1_0))\n        x3_0 = self.conv3_0(self.pool(x2_0))\n        x4_0 = self.conv4_0(self.pool(x3_0))\n\n        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))\n        x2_2 = self.conv2_2(torch.cat([x2_0, self.up(x3_1)], 1))\n        x1_3 = self.conv1_3(torch.cat([x1_0, self.up(x2_2)], 1))\n        x0_4 = self.conv0_4(torch.cat([x0_0, self.up(x1_3)], 1))\n\n        output = self.final(x0_4)\n        return output\n\n\nclass NestedUNet(nn.Module):\n    def __init__(self, num_classes, input_channels=3, deep_supervision=False, **kwargs):\n        super().__init__()\n\n        nb_filter = [32, 64, 128, 256, 512]\n\n        self.deep_supervision = deep_supervision\n\n        self.pool = nn.MaxPool2d(2, 2)\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\n        self.conv0_0 = VGGBlock(input_channels, nb_filter[0], nb_filter[0])\n        self.conv1_0 = VGGBlock(nb_filter[0], nb_filter[1], nb_filter[1])\n        self.conv2_0 = VGGBlock(nb_filter[1], nb_filter[2], nb_filter[2])\n        self.conv3_0 = VGGBlock(nb_filter[2], nb_filter[3], nb_filter[3])\n        self.conv4_0 = VGGBlock(nb_filter[3], nb_filter[4], nb_filter[4])\n\n        self.conv0_1 = VGGBlock(nb_filter[0]+nb_filter[1], nb_filter[0], nb_filter[0])\n        self.conv1_1 = VGGBlock(nb_filter[1]+nb_filter[2], nb_filter[1], nb_filter[1])\n        self.conv2_1 = VGGBlock(nb_filter[2]+nb_filter[3], nb_filter[2], nb_filter[2])\n        self.conv3_1 = VGGBlock(nb_filter[3]+nb_filter[4], nb_filter[3], nb_filter[3])\n\n        self.conv0_2 = VGGBlock(nb_filter[0]*2+nb_filter[1], nb_filter[0], nb_filter[0])\n        self.conv1_2 = VGGBlock(nb_filter[1]*2+nb_filter[2], nb_filter[1], nb_filter[1])\n        self.conv2_2 = VGGBlock(nb_filter[2]*2+nb_filter[3], nb_filter[2], nb_filter[2])\n\n        self.conv0_3 = VGGBlock(nb_filter[0]*3+nb_filter[1], nb_filter[0], nb_filter[0])\n        self.conv1_3 = VGGBlock(nb_filter[1]*3+nb_filter[2], nb_filter[1], nb_filter[1])\n\n        self.conv0_4 = VGGBlock(nb_filter[0]*4+nb_filter[1], nb_filter[0], nb_filter[0])\n\n        if self.deep_supervision:\n            self.final1 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n            self.final2 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n            self.final3 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n            self.final4 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n        else:\n            self.final = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n\n\n    def forward(self, input):\n        x0_0 = self.conv0_0(input)\n        x1_0 = self.conv1_0(self.pool(x0_0))\n        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], 1))\n\n        x2_0 = self.conv2_0(self.pool(x1_0))\n        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))\n        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], 1))\n\n        x3_0 = self.conv3_0(self.pool(x2_0))\n        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], 1))\n        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], 1))\n        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1))\n\n        x4_0 = self.conv4_0(self.pool(x3_0))\n        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))\n        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], 1))\n        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], 1))\n        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1))\n\n        if self.deep_supervision:\n            output1 = self.final1(x0_1)\n            output2 = self.final2(x0_2)\n            output3 = self.final3(x0_3)\n            output4 = self.final4(x0_4)\n            return [output1, output2, output3, output4]\n\n        else:\n            output = self.final(x0_4)\n            return output\n"""
dataset.py,2,"b'import os\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.utils.data\n\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, img_ids, img_dir, mask_dir, img_ext, mask_ext, num_classes, transform=None):\n        """"""\n        Args:\n            img_ids (list): Image ids.\n            img_dir: Image file directory.\n            mask_dir: Mask file directory.\n            img_ext (str): Image file extension.\n            mask_ext (str): Mask file extension.\n            num_classes (int): Number of classes.\n            transform (Compose, optional): Compose transforms of albumentations. Defaults to None.\n        \n        Note:\n            Make sure to put the files as the following structure:\n            <dataset name>\n            \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 images\n            |   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 0a7e06.jpg\n            \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 0aab0a.jpg\n            \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 0b1761.jpg\n            \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 ...\n            |\n            \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 masks\n                \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 0\n                |   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 0a7e06.png\n                |   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 0aab0a.png\n                |   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 0b1761.png\n                |   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 ...\n                |\n                \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 1\n                |   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 0a7e06.png\n                |   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 0aab0a.png\n                |   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 0b1761.png\n                |   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 ...\n                ...\n        """"""\n        self.img_ids = img_ids\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.img_ext = img_ext\n        self.mask_ext = mask_ext\n        self.num_classes = num_classes\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_ids)\n\n    def __getitem__(self, idx):\n        img_id = self.img_ids[idx]\n        \n        img = cv2.imread(os.path.join(self.img_dir, img_id + self.img_ext))\n\n        mask = []\n        for i in range(self.num_classes):\n            mask.append(cv2.imread(os.path.join(self.mask_dir, str(i),\n                        img_id + self.mask_ext), cv2.IMREAD_GRAYSCALE)[..., None])\n        mask = np.dstack(mask)\n\n        if self.transform is not None:\n            augmented = self.transform(image=img, mask=mask)\n            img = augmented[\'image\']\n            mask = augmented[\'mask\']\n        \n        img = img.astype(\'float32\') / 255\n        img = img.transpose(2, 0, 1)\n        mask = mask.astype(\'float32\') / 255\n        mask = mask.transpose(2, 0, 1)\n        \n        return img, mask, {\'img_id\': img_id}\n'"
losses.py,4,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntry:\n    from LovaszSoftmax.pytorch.lovasz_losses import lovasz_hinge\nexcept ImportError:\n    pass\n\n__all__ = ['BCEDiceLoss', 'LovaszHingeLoss']\n\n\nclass BCEDiceLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, input, target):\n        bce = F.binary_cross_entropy_with_logits(input, target)\n        smooth = 1e-5\n        input = torch.sigmoid(input)\n        num = target.size(0)\n        input = input.view(num, -1)\n        target = target.view(num, -1)\n        intersection = (input * target)\n        dice = (2. * intersection.sum(1) + smooth) / (input.sum(1) + target.sum(1) + smooth)\n        dice = 1 - dice.sum() / num\n        return 0.5 * bce + dice\n\n\nclass LovaszHingeLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, input, target):\n        input = input.squeeze(1)\n        target = target.squeeze(1)\n        loss = lovasz_hinge(input, target, per_image=True)\n\n        return loss\n"""
metrics.py,5,"b'import numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\ndef iou_score(output, target):\n    smooth = 1e-5\n\n    if torch.is_tensor(output):\n        output = torch.sigmoid(output).data.cpu().numpy()\n    if torch.is_tensor(target):\n        target = target.data.cpu().numpy()\n    output_ = output > 0.5\n    target_ = target > 0.5\n    intersection = (output_ & target_).sum()\n    union = (output_ | target_).sum()\n\n    return (intersection + smooth) / (union + smooth)\n\n\ndef dice_coef(output, target):\n    smooth = 1e-5\n\n    output = torch.sigmoid(output).view(-1).data.cpu().numpy()\n    target = target.view(-1).data.cpu().numpy()\n    intersection = (output * target).sum()\n\n    return (2. * intersection + smooth) / \\\n        (output.sum() + target.sum() + smooth)\n'"
preprocess_dsb2018.py,0,"b""import os\nfrom glob import glob\n\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\n\ndef main():\n    img_size = 96\n\n    paths = glob('inputs/data-science-bowl-2018/stage1_train/*')\n\n    os.makedirs('inputs/dsb2018_%d/images' % img_size, exist_ok=True)\n    os.makedirs('inputs/dsb2018_%d/masks/0' % img_size, exist_ok=True)\n\n    for i in tqdm(range(len(paths))):\n        path = paths[i]\n        img = cv2.imread(os.path.join(path, 'images',\n                         os.path.basename(path) + '.png'))\n        mask = np.zeros((img.shape[0], img.shape[1]))\n        for mask_path in glob(os.path.join(path, 'masks', '*')):\n            mask_ = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) > 127\n            mask[mask_] = 1\n        if len(img.shape) == 2:\n            img = np.tile(img[..., None], (1, 1, 3))\n        if img.shape[2] == 4:\n            img = img[..., :3]\n        img = cv2.resize(img, (img_size, img_size))\n        mask = cv2.resize(mask, (img_size, img_size))\n        cv2.imwrite(os.path.join('inputs/dsb2018_%d/images' % img_size,\n                    os.path.basename(path) + '.png'), img)\n        cv2.imwrite(os.path.join('inputs/dsb2018_%d/masks/0' % img_size,\n                    os.path.basename(path) + '.png'), (mask * 255).astype('uint8'))\n\n\nif __name__ == '__main__':\n    main()\n"""
train.py,9,"b'import argparse\nimport os\nfrom collections import OrderedDict\nfrom glob import glob\n\nimport pandas as pd\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.optim as optim\nimport yaml\nfrom albumentations.augmentations import transforms\nfrom albumentations.core.composition import Compose, OneOf\nfrom sklearn.model_selection import train_test_split\nfrom torch.optim import lr_scheduler\nfrom tqdm import tqdm\n\nimport archs\nimport losses\nfrom dataset import Dataset\nfrom metrics import iou_score\nfrom utils import AverageMeter, str2bool\n\nARCH_NAMES = archs.__all__\nLOSS_NAMES = losses.__all__\nLOSS_NAMES.append(\'BCEWithLogitsLoss\')\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--name\', default=None,\n                        help=\'model name: (default: arch+timestamp)\')\n    parser.add_argument(\'--epochs\', default=100, type=int, metavar=\'N\',\n                        help=\'number of total epochs to run\')\n    parser.add_argument(\'-b\', \'--batch_size\', default=16, type=int,\n                        metavar=\'N\', help=\'mini-batch size (default: 16)\')\n    \n    # model\n    parser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'NestedUNet\',\n                        choices=ARCH_NAMES,\n                        help=\'model architecture: \' +\n                        \' | \'.join(ARCH_NAMES) +\n                        \' (default: NestedUNet)\')\n    parser.add_argument(\'--deep_supervision\', default=False, type=str2bool)\n    parser.add_argument(\'--input_channels\', default=3, type=int,\n                        help=\'input channels\')\n    parser.add_argument(\'--num_classes\', default=1, type=int,\n                        help=\'number of classes\')\n    parser.add_argument(\'--input_w\', default=96, type=int,\n                        help=\'image width\')\n    parser.add_argument(\'--input_h\', default=96, type=int,\n                        help=\'image height\')\n    \n    # loss\n    parser.add_argument(\'--loss\', default=\'BCEDiceLoss\',\n                        choices=LOSS_NAMES,\n                        help=\'loss: \' +\n                        \' | \'.join(LOSS_NAMES) +\n                        \' (default: BCEDiceLoss)\')\n    \n    # dataset\n    parser.add_argument(\'--dataset\', default=\'dsb2018_96\',\n                        help=\'dataset name\')\n    parser.add_argument(\'--img_ext\', default=\'.png\',\n                        help=\'image file extension\')\n    parser.add_argument(\'--mask_ext\', default=\'.png\',\n                        help=\'mask file extension\')\n\n    # optimizer\n    parser.add_argument(\'--optimizer\', default=\'SGD\',\n                        choices=[\'Adam\', \'SGD\'],\n                        help=\'loss: \' +\n                        \' | \'.join([\'Adam\', \'SGD\']) +\n                        \' (default: Adam)\')\n    parser.add_argument(\'--lr\', \'--learning_rate\', default=1e-3, type=float,\n                        metavar=\'LR\', help=\'initial learning rate\')\n    parser.add_argument(\'--momentum\', default=0.9, type=float,\n                        help=\'momentum\')\n    parser.add_argument(\'--weight_decay\', default=1e-4, type=float,\n                        help=\'weight decay\')\n    parser.add_argument(\'--nesterov\', default=False, type=str2bool,\n                        help=\'nesterov\')\n\n    # scheduler\n    parser.add_argument(\'--scheduler\', default=\'CosineAnnealingLR\',\n                        choices=[\'CosineAnnealingLR\', \'ReduceLROnPlateau\', \'MultiStepLR\', \'ConstantLR\'])\n    parser.add_argument(\'--min_lr\', default=1e-5, type=float,\n                        help=\'minimum learning rate\')\n    parser.add_argument(\'--factor\', default=0.1, type=float)\n    parser.add_argument(\'--patience\', default=2, type=int)\n    parser.add_argument(\'--milestones\', default=\'1,2\', type=str)\n    parser.add_argument(\'--gamma\', default=2/3, type=float)\n    parser.add_argument(\'--early_stopping\', default=-1, type=int,\n                        metavar=\'N\', help=\'early stopping (default: -1)\')\n    \n    parser.add_argument(\'--num_workers\', default=4, type=int)\n\n    config = parser.parse_args()\n\n    return config\n\n\ndef train(config, train_loader, model, criterion, optimizer):\n    avg_meters = {\'loss\': AverageMeter(),\n                  \'iou\': AverageMeter()}\n\n    model.train()\n\n    pbar = tqdm(total=len(train_loader))\n    for input, target, _ in train_loader:\n        input = input.cuda()\n        target = target.cuda()\n\n        # compute output\n        if config[\'deep_supervision\']:\n            outputs = model(input)\n            loss = 0\n            for output in outputs:\n                loss += criterion(output, target)\n            loss /= len(outputs)\n            iou = iou_score(outputs[-1], target)\n        else:\n            output = model(input)\n            loss = criterion(output, target)\n            iou = iou_score(output, target)\n\n        # compute gradient and do optimizing step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        avg_meters[\'loss\'].update(loss.item(), input.size(0))\n        avg_meters[\'iou\'].update(iou, input.size(0))\n\n        postfix = OrderedDict([\n            (\'loss\', avg_meters[\'loss\'].avg),\n            (\'iou\', avg_meters[\'iou\'].avg),\n        ])\n        pbar.set_postfix(postfix)\n        pbar.update(1)\n    pbar.close()\n\n    return OrderedDict([(\'loss\', avg_meters[\'loss\'].avg),\n                        (\'iou\', avg_meters[\'iou\'].avg)])\n\n\ndef validate(config, val_loader, model, criterion):\n    avg_meters = {\'loss\': AverageMeter(),\n                  \'iou\': AverageMeter()}\n\n    # switch to evaluate mode\n    model.eval()\n\n    with torch.no_grad():\n        pbar = tqdm(total=len(val_loader))\n        for input, target, _ in val_loader:\n            input = input.cuda()\n            target = target.cuda()\n\n            # compute output\n            if config[\'deep_supervision\']:\n                outputs = model(input)\n                loss = 0\n                for output in outputs:\n                    loss += criterion(output, target)\n                loss /= len(outputs)\n                iou = iou_score(outputs[-1], target)\n            else:\n                output = model(input)\n                loss = criterion(output, target)\n                iou = iou_score(output, target)\n\n            avg_meters[\'loss\'].update(loss.item(), input.size(0))\n            avg_meters[\'iou\'].update(iou, input.size(0))\n\n            postfix = OrderedDict([\n                (\'loss\', avg_meters[\'loss\'].avg),\n                (\'iou\', avg_meters[\'iou\'].avg),\n            ])\n            pbar.set_postfix(postfix)\n            pbar.update(1)\n        pbar.close()\n\n    return OrderedDict([(\'loss\', avg_meters[\'loss\'].avg),\n                        (\'iou\', avg_meters[\'iou\'].avg)])\n\n\ndef main():\n    config = vars(parse_args())\n\n    if config[\'name\'] is None:\n        if config[\'deep_supervision\']:\n            config[\'name\'] = \'%s_%s_wDS\' % (config[\'dataset\'], config[\'arch\'])\n        else:\n            config[\'name\'] = \'%s_%s_woDS\' % (config[\'dataset\'], config[\'arch\'])\n    os.makedirs(\'models/%s\' % config[\'name\'], exist_ok=True)\n\n    print(\'-\' * 20)\n    for key in config:\n        print(\'%s: %s\' % (key, config[key]))\n    print(\'-\' * 20)\n\n    with open(\'models/%s/config.yml\' % config[\'name\'], \'w\') as f:\n        yaml.dump(config, f)\n\n    # define loss function (criterion)\n    if config[\'loss\'] == \'BCEWithLogitsLoss\':\n        criterion = nn.BCEWithLogitsLoss().cuda()\n    else:\n        criterion = losses.__dict__[config[\'loss\']]().cuda()\n\n    cudnn.benchmark = True\n\n    # create model\n    print(""=> creating model %s"" % config[\'arch\'])\n    model = archs.__dict__[config[\'arch\']](config[\'num_classes\'],\n                                           config[\'input_channels\'],\n                                           config[\'deep_supervision\'])\n\n    model = model.cuda()\n\n    params = filter(lambda p: p.requires_grad, model.parameters())\n    if config[\'optimizer\'] == \'Adam\':\n        optimizer = optim.Adam(\n            params, lr=config[\'lr\'], weight_decay=config[\'weight_decay\'])\n    elif config[\'optimizer\'] == \'SGD\':\n        optimizer = optim.SGD(params, lr=config[\'lr\'], momentum=config[\'momentum\'],\n                              nesterov=config[\'nesterov\'], weight_decay=config[\'weight_decay\'])\n    else:\n        raise NotImplementedError\n\n    if config[\'scheduler\'] == \'CosineAnnealingLR\':\n        scheduler = lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=config[\'epochs\'], eta_min=config[\'min_lr\'])\n    elif config[\'scheduler\'] == \'ReduceLROnPlateau\':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=config[\'factor\'], patience=config[\'patience\'],\n                                                   verbose=1, min_lr=config[\'min_lr\'])\n    elif config[\'scheduler\'] == \'MultiStepLR\':\n        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[int(e) for e in config[\'milestones\'].split(\',\')], gamma=config[\'gamma\'])\n    elif config[\'scheduler\'] == \'ConstantLR\':\n        scheduler = None\n    else:\n        raise NotImplementedError\n\n    # Data loading code\n    img_ids = glob(os.path.join(\'inputs\', config[\'dataset\'], \'images\', \'*\' + config[\'img_ext\']))\n    img_ids = [os.path.splitext(os.path.basename(p))[0] for p in img_ids]\n\n    train_img_ids, val_img_ids = train_test_split(img_ids, test_size=0.2, random_state=41)\n\n    train_transform = Compose([\n        transforms.RandomRotate90(),\n        transforms.Flip(),\n        OneOf([\n            transforms.HueSaturationValue(),\n            transforms.RandomBrightness(),\n            transforms.RandomContrast(),\n        ], p=1),\n        transforms.Resize(config[\'input_h\'], config[\'input_w\']),\n        transforms.Normalize(),\n    ])\n\n    val_transform = Compose([\n        transforms.Resize(config[\'input_h\'], config[\'input_w\']),\n        transforms.Normalize(),\n    ])\n\n    train_dataset = Dataset(\n        img_ids=train_img_ids,\n        img_dir=os.path.join(\'inputs\', config[\'dataset\'], \'images\'),\n        mask_dir=os.path.join(\'inputs\', config[\'dataset\'], \'masks\'),\n        img_ext=config[\'img_ext\'],\n        mask_ext=config[\'mask_ext\'],\n        num_classes=config[\'num_classes\'],\n        transform=train_transform)\n    val_dataset = Dataset(\n        img_ids=val_img_ids,\n        img_dir=os.path.join(\'inputs\', config[\'dataset\'], \'images\'),\n        mask_dir=os.path.join(\'inputs\', config[\'dataset\'], \'masks\'),\n        img_ext=config[\'img_ext\'],\n        mask_ext=config[\'mask_ext\'],\n        num_classes=config[\'num_classes\'],\n        transform=val_transform)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config[\'batch_size\'],\n        shuffle=True,\n        num_workers=config[\'num_workers\'],\n        drop_last=True)\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=config[\'batch_size\'],\n        shuffle=False,\n        num_workers=config[\'num_workers\'],\n        drop_last=False)\n\n    log = OrderedDict([\n        (\'epoch\', []),\n        (\'lr\', []),\n        (\'loss\', []),\n        (\'iou\', []),\n        (\'val_loss\', []),\n        (\'val_iou\', []),\n    ])\n\n    best_iou = 0\n    trigger = 0\n    for epoch in range(config[\'epochs\']):\n        print(\'Epoch [%d/%d]\' % (epoch, config[\'epochs\']))\n\n        # train for one epoch\n        train_log = train(config, train_loader, model, criterion, optimizer)\n        # evaluate on validation set\n        val_log = validate(config, val_loader, model, criterion)\n\n        if config[\'scheduler\'] == \'CosineAnnealingLR\':\n            scheduler.step()\n        elif config[\'scheduler\'] == \'ReduceLROnPlateau\':\n            scheduler.step(val_log[\'loss\'])\n\n        print(\'loss %.4f - iou %.4f - val_loss %.4f - val_iou %.4f\'\n              % (train_log[\'loss\'], train_log[\'iou\'], val_log[\'loss\'], val_log[\'iou\']))\n\n        log[\'epoch\'].append(epoch)\n        log[\'lr\'].append(config[\'lr\'])\n        log[\'loss\'].append(train_log[\'loss\'])\n        log[\'iou\'].append(train_log[\'iou\'])\n        log[\'val_loss\'].append(val_log[\'loss\'])\n        log[\'val_iou\'].append(val_log[\'iou\'])\n\n        pd.DataFrame(log).to_csv(\'models/%s/log.csv\' %\n                                 config[\'name\'], index=False)\n\n        trigger += 1\n\n        if val_log[\'iou\'] > best_iou:\n            torch.save(model.state_dict(), \'models/%s/model.pth\' %\n                       config[\'name\'])\n            best_iou = val_log[\'iou\']\n            print(""=> saved best model"")\n            trigger = 0\n\n        # early stopping\n        if config[\'early_stopping\'] >= 0 and trigger >= config[\'early_stopping\']:\n            print(""=> early stopping"")\n            break\n\n        torch.cuda.empty_cache()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils.py,0,"b'import argparse\n\n\ndef str2bool(v):\n    if v.lower() in [\'true\', 1]:\n        return True\n    elif v.lower() in [\'false\', 0]:\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\n\ndef count_params(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n'"
val.py,6,"b'import argparse\nimport os\nfrom glob import glob\n\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\nimport yaml\nfrom albumentations.augmentations import transforms\nfrom albumentations.core.composition import Compose\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\nimport archs\nfrom dataset import Dataset\nfrom metrics import iou_score\nfrom utils import AverageMeter\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--name\', default=None,\n                        help=\'model name\')\n\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    with open(\'models/%s/config.yml\' % args.name, \'r\') as f:\n        config = yaml.load(f, Loader=yaml.FullLoader)\n\n    print(\'-\'*20)\n    for key in config.keys():\n        print(\'%s: %s\' % (key, str(config[key])))\n    print(\'-\'*20)\n\n    cudnn.benchmark = True\n\n    # create model\n    print(""=> creating model %s"" % config[\'arch\'])\n    model = archs.__dict__[config[\'arch\']](config[\'num_classes\'],\n                                           config[\'input_channels\'],\n                                           config[\'deep_supervision\'])\n\n    model = model.cuda()\n\n    # Data loading code\n    img_ids = glob(os.path.join(\'inputs\', config[\'dataset\'], \'images\', \'*\' + config[\'img_ext\']))\n    img_ids = [os.path.splitext(os.path.basename(p))[0] for p in img_ids]\n\n    _, val_img_ids = train_test_split(img_ids, test_size=0.2, random_state=41)\n\n    model.load_state_dict(torch.load(\'models/%s/model.pth\' %\n                                     config[\'name\']))\n    model.eval()\n\n    val_transform = Compose([\n        transforms.Resize(config[\'input_h\'], config[\'input_w\']),\n        transforms.Normalize(),\n    ])\n\n    val_dataset = Dataset(\n        img_ids=val_img_ids,\n        img_dir=os.path.join(\'inputs\', config[\'dataset\'], \'images\'),\n        mask_dir=os.path.join(\'inputs\', config[\'dataset\'], \'masks\'),\n        img_ext=config[\'img_ext\'],\n        mask_ext=config[\'mask_ext\'],\n        num_classes=config[\'num_classes\'],\n        transform=val_transform)\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=config[\'batch_size\'],\n        shuffle=False,\n        num_workers=config[\'num_workers\'],\n        drop_last=False)\n\n    avg_meter = AverageMeter()\n\n    for c in range(config[\'num_classes\']):\n        os.makedirs(os.path.join(\'outputs\', config[\'name\'], str(c)), exist_ok=True)\n    with torch.no_grad():\n        for input, target, meta in tqdm(val_loader, total=len(val_loader)):\n            input = input.cuda()\n            target = target.cuda()\n\n            # compute output\n            if config[\'deep_supervision\']:\n                output = model(input)[-1]\n            else:\n                output = model(input)\n\n            iou = iou_score(output, target)\n            avg_meter.update(iou, input.size(0))\n\n            output = torch.sigmoid(output).cpu().numpy()\n\n            for i in range(len(output)):\n                for c in range(config[\'num_classes\']):\n                    cv2.imwrite(os.path.join(\'outputs\', config[\'name\'], str(c), meta[\'img_id\'][i] + \'.jpg\'),\n                                (output[i, c] * 255).astype(\'uint8\'))\n\n    print(\'IoU: %.4f\' % avg_meter.avg)\n\n    torch.cuda.empty_cache()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
