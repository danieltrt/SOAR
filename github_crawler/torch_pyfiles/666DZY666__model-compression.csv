file_path,api_count,code
pruning/gc_prune.py,12,"b'import os\nimport argparse\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\nfrom models import nin_gc\nimport numpy as np\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--data\', action=\'store\', default=\'../data\',\n                    help=\'dataset path\')\nparser.add_argument(\'--cpu\', action=\'store_true\',\n                    help=\'disables CUDA training\')\nparser.add_argument(\'--percent\', type=float, default=0.4,\n                    help=\'nin_gc:0.4\')\nparser.add_argument(\'--layers\', type=int, default=9,\n                    help=\'layers (default: 9)\')\nparser.add_argument(\'--model\', default=\'models_save/nin_gc_preprune.pth\', type=str, metavar=\'PATH\',\n                    help=\'path to raw trained model (default: none)\')\nargs = parser.parse_args()\nlayers = args.layers\nprint(args)\n\nmodel = nin_gc.Net()\nif args.model:\n    if os.path.isfile(args.model):\n        print(""=> loading checkpoint \'{}\'"".format(args.model))\n        model.load_state_dict(torch.load(args.model)[\'state_dict\'])\n    else:\n        print(""=> no checkpoint found at \'{}\'"".format(args.resume))\nprint(\'\xe6\x97\xa7\xe6\xa8\xa1\xe5\x9e\x8b: \', model)\ntotal = 0\ni = 0\nfor m in model.modules():\n    if isinstance(m, nn.BatchNorm2d):\n        if i < layers - 1:\n            i += 1\n            total += m.weight.data.shape[0]\n\nbn = torch.zeros(total)\nindex = 0\ni = 0\nfor m in model.modules():\n    if isinstance(m, nn.BatchNorm2d):\n        if i < layers - 1:\n            i += 1\n            size = m.weight.data.shape[0]\n            bn[index:(index+size)] = m.weight.data.abs().clone()\n            index += size\ny, j = torch.sort(bn)\nthre_index = int(total * args.percent)\nif thre_index == total:\n    thre_index = total - 1\nthre_0 = y[thre_index]\n\n#****************\xe8\x8e\xb7\xe5\x8f\x96\xe9\x92\x88\xe5\xaf\xb9\xe5\x88\x86\xe7\xbb\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x89\xaa\xe6\x9e\x9d\xe6\xaf\x8f\xe5\xb1\x82\xe7\x9a\x84\xe5\x9f\xba\xe6\x95\xb0\xef\xbc\x88base_bumber\xef\xbc\x89********************\nnums = []\nchannels = []\ngroups = [1]\nprune_base_num = []\nj = 0\nfor k, m in enumerate(model.modules()):\n    if isinstance(m, nn.Conv2d):\n        s_0 = m.weight.data.data.size()[0]\n        s_1 = m.weight.data.data.size()[1]\n        nums.append(s_0)\n        channels.append(s_1)\nwhile  j < len(nums) - 1:\n    groups.append(int(nums[j] / channels[j+1])) \n    j += 1\n#print(groups)\nj = 0\nwhile  j < len(groups) - 1:\n    for i in range(1, (groups[j] * groups[j+1])+1):\n        if i % groups[j] == 0 and i % groups[j+1] == 0:\n            prune_base_num.append(i)       \n            break\n    j += 1\n#print(prune_base_num)\n\n#********************************\xe9\xa2\x84\xe5\x89\xaa\xe6\x9e\x9d*********************************\npruned = 0\ncfg_0 = []\ncfg = []\ncfg_mask = []\ni = 0\nfor k, m in enumerate(model.modules()):\n    if isinstance(m, nn.BatchNorm2d):\n        if i < layers - 1:\n            i += 1\n\n            weight_copy = m.weight.data.clone()\n            mask = weight_copy.abs().gt(thre_0).float()\n            remain_channels = torch.sum(mask)\n\n            if remain_channels == 0:\n                print(\'\\r\\n!please turn down the prune_ratio!\\r\\n\')\n                remain_channels = 1\n                mask[int(torch.argmax(weight_copy))]=1\n\n            # ******************\xe5\x88\x86\xe7\xbb\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x89\xaa\xe6\x9e\x9d******************\n            base_number = prune_base_num[i-1]\n            v = 0\n            n = 1\n            if remain_channels % base_number != 0:\n                if remain_channels > base_number:\n                    while v < remain_channels:\n                        n += 1\n                        v = base_number * n\n                    if remain_channels - (v - base_number) < v - remain_channels:\n                        remain_channels = v - base_number\n                    else:\n                        remain_channels = v\n                    if remain_channels > m.weight.data.size()[0]:\n                        remain_channels = m.weight.data.size()[0]\n                    remain_channels = torch.tensor(remain_channels)\n                        \n                    y, j = torch.sort(weight_copy.abs())\n                    thre_1 = y[-remain_channels]\n                    mask = weight_copy.abs().ge(thre_1).float()\n            pruned = pruned + mask.shape[0] - torch.sum(mask)\n            m.weight.data.mul_(mask)\n            m.bias.data.mul_(mask)\n            cfg_0.append(mask.shape[0])\n            cfg.append(int(remain_channels))\n            cfg_mask.append(mask.clone())\n            print(\'layer_index: {:d} \\t total_channel: {:d} \\t remaining_channel: {:d} \\t pruned_ratio: {:f}\'.\n                format(k, mask.shape[0], int(torch.sum(mask)), (mask.shape[0] - torch.sum(mask)) / mask.shape[0]))\npruned_ratio = float(pruned/total)\nprint(\'\\r\\n!\xe9\xa2\x84\xe5\x89\xaa\xe6\x9e\x9d\xe5\xae\x8c\xe6\x88\x90!\')\nprint(\'total_pruned_ratio: \', pruned_ratio)\n#********************************\xe9\xa2\x84\xe5\x89\xaa\xe6\x9e\x9d\xe5\x90\x8emodel\xe6\xb5\x8b\xe8\xaf\x95*********************************\ndef test():\n    test_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(root = args.data, train=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),\n        batch_size = 64, shuffle=False, num_workers=1)\n    model.eval()\n    correct = 0\n    \n    for data, target in test_loader:\n        if not args.cpu:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        output = model(data)\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n    acc = 100. * float(correct) / len(test_loader.dataset)\n    print(\'Accuracy: {:.2f}%\\n\'.format(acc))\n    return\nprint(\'************\xe9\xa2\x84\xe5\x89\xaa\xe6\x9e\x9d\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xb5\x8b\xe8\xaf\x95************\')\nif not args.cpu:\n    model.cuda()\ntest()\n\n#*****************************\xe5\x89\xaa\xe6\x9e\x9d\xe5\x90\x8emodel\xe7\xbb\x93\xe6\x9e\x84********************************\nnewmodel = nin_gc.Net(cfg)\nprint(\'\xe6\x96\xb0\xe6\xa8\xa1\xe5\x9e\x8b: \', newmodel)\n\n#*****************************\xe5\x89\xaa\xe6\x9e\x9d\xe5\x89\x8d\xe5\x90\x8emodel\xe5\xaf\xb9\xe6\xaf\x94********************************\nprint(\'************\xe6\x97\xa7\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84************\')\nprint(cfg_0)\nprint(\'************\xe6\x96\xb0\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84************\')\nprint(cfg, \'\\r\\n\')\n'"
pruning/main.py,23,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport math\nimport argparse\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torchvision\nimport torchvision.transforms as transforms\nfrom models import nin\n#from models import nin_gc\nimport os\n\ndef setup_seed(seed):\n    torch.manual_seed(seed)                    \n    #torch.cuda.manual_seed(seed)               \n    torch.cuda.manual_seed_all(seed)           \n    np.random.seed(seed)                       \n    torch.backends.cudnn.deterministic = True\n\ndef save_state(model, best_acc):\n    print(\'==> Saving model ...\')\n    state = {\n            \'best_acc\': best_acc,\n            \'state_dict\': model.state_dict(),\n            }\n    state_copy = state[\'state_dict\'].copy()\n    for key in state_copy.keys():\n        if \'module\' in key:\n            state[\'state_dict\'][key.replace(\'module.\', \'\')] = \\\n                    state[\'state_dict\'].pop(key)\n    torch.save(state, \'models_save/nin.pth\')\n    #torch.save(state, \'models_save/nin_gc.pth\')\n    #torch.save(state, \'models_save/nin_preprune.pth\')\n    #torch.save(state, \'models_save/nin_gc_preprune.pth\')\n    #torch.save({\'cfg\': cfg, \'best_acc\': best_acc, \'state_dict\': state[\'state_dict\']}, \'models_save/nin_refine.pth\')\n    #torch.save({\'cfg\': cfg, \'best_acc\': best_acc, \'state_dict\': state[\'state_dict\']}, \'models_save/nin_gc_refine.pth\')\n    \n#***********************\xe7\xa8\x80\xe7\x96\x8f\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x88\xe5\xaf\xb9BN\xe5\xb1\x82\xce\xb3\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xba\xa6\xe6\x9d\x9f\xef\xbc\x89**************************\ndef updateBN():\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            if hasattr(m.weight, \'data\'):\n                m.weight.grad.data.add_(args.s*torch.sign(m.weight.data)) #L1\xe6\xad\xa3\xe5\x88\x99\n\ndef train(epoch):\n    model.train()\n\n    for batch_idx, (data, target) in enumerate(trainloader):       \n        if not args.cpu:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        output = model(data)\n        loss = criterion(output, target)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        \n        #***********************\xe7\xa8\x80\xe7\x96\x8f\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x88\xe5\xaf\xb9BN\xe5\xb1\x82\xce\xb3\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xba\xa6\xe6\x9d\x9f\xef\xbc\x89**************************\n        if args.sr:\n            updateBN()\n\n        optimizer.step()\n\n        if batch_idx % 100 == 0:\n            print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR: {}\'.format(\n                epoch, batch_idx * len(data), len(trainloader.dataset),\n                100. * batch_idx / len(trainloader), loss.data.item(),\n                optimizer.param_groups[0][\'lr\']))\n    return\n\ndef test():\n    global best_acc\n    model.eval()\n    test_loss = 0\n    correct = 0\n\n    for data, target in testloader:\n        if not args.cpu:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)             \n        output = model(data)\n        test_loss += criterion(output, target).data.item()\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n    acc = 100. * float(correct) / len(testloader.dataset)\n\n    if acc > best_acc:\n        best_acc = acc\n        save_state(model, best_acc)\n    \n    test_loss /= len(testloader.dataset)\n    print(\'\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\'.format(\n        test_loss * 256., correct, len(testloader.dataset),\n        100. * float(correct) / len(testloader.dataset)))\n    print(\'Best Accuracy: {:.2f}%\\n\'.format(best_acc))\n    return\n\ndef adjust_learning_rate(optimizer, epoch):\n    update_list = [80, 130, 180, 230, 280]\n    if epoch in update_list:\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = param_group[\'lr\'] * 0.1\n    return\n\nif __name__==\'__main__\':\n    # prepare the options\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--cpu\', action=\'store_true\',\n            help=\'set if only CPU is available\')\n    # gpu_id\n    parser.add_argument(\'--gpu_id\', action=\'store\', default=\'\',\n            help=\'gpu_id\')\n    parser.add_argument(\'--data\', action=\'store\', default=\'../data\',\n            help=\'dataset path\')\n    parser.add_argument(\'--lr\', action=\'store\', default=0.01,\n            help=\'the intial learning rate\')\n    parser.add_argument(\'--wd\', action=\'store\', default=1e-7,\n            help=\'nin_gc:0, nin:1e-5\')\n    parser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n            help=\'the path to the resume model\')\n    parser.add_argument(\'--refine\', default=\'\', type=str, metavar=\'PATH\',\n            help=\'the path to the refine(prune) model\')\n    parser.add_argument(\'--evaluate\', action=\'store_true\',\n            help=\'evaluate the model\')\n    # sr(\xe7\xa8\x80\xe7\x96\x8f\xe6\xa0\x87\xe5\xbf\x97)\n    parser.add_argument(\'--sparsity-regularization\', \'-sr\', dest=\'sr\', action=\'store_true\',\n            help=\'train with channel sparsity regularization\')\n    # s(\xe7\xa8\x80\xe7\x96\x8f\xe7\x8e\x87)\n    parser.add_argument(\'--s\', type=float, default=0.0001,\n            help=\'nin:0.0001, nin_gc:0.001\')\n    parser.add_argument(\'--train_batch_size\', type=int, default=50)\n    parser.add_argument(\'--eval_batch_size\', type=int, default=256)\n    parser.add_argument(\'--num_workers\', type=int, default=2)\n    parser.add_argument(\'--epochs\', type=int, default=300, metavar=\'N\',\n            help=\'number of epochs to train\')\n    args = parser.parse_args()\n    print(\'==> Options:\',args)\n\n    if args.gpu_id:\n        os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu_id\n\n    setup_seed(1)\n\n    print(\'==> Preparing data..\')\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n\n    trainset = torchvision.datasets.CIFAR10(root = args.data, train = True, download = True, transform = transform_train)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.train_batch_size, shuffle=True, num_workers=2)\n\n    testset = torchvision.datasets.CIFAR10(root = args.data, train = False, download = True, transform = transform_test)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=args.eval_batch_size, shuffle=False, num_workers=2)\n\n    # define classes\n    classes = (\'plane\', \'car\', \'bird\', \'cat\', \'deer\', \'dog\', \'frog\', \'horse\', \'ship\', \'truck\')\n\n    if args.refine:\n        print(\'******Refine model******\')\n        #checkpoint = torch.load(\'models_save/nin_prune.pth\')\n        checkpoint = torch.load(args.refine)\n        cfg = checkpoint[\'cfg\']\n        model = nin.Net(cfg=checkpoint[\'cfg\'])\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        best_acc = 0\n    else:\n        print(\'******Initializing model******\')\n        model = nin.Net()\n        #model = nin_gc.Net()\n        \'\'\'\n        cfg = []   #gc_prune \xe2\x80\x94\xe2\x80\x94 cfg\n        model = nin_gc.Net(cfg=cfg)\n        \'\'\'\n        best_acc = 0\n        for m in model.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight.data)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n    if args.resume:\n        print(\'******Reume model******\')\n        #pretrained_model = torch.load(\'models_save/nin.pth\')\n        #pretrained_model = torch.load(\'models_save/nin_preprune.pth\')\n        #pretrained_model = torch.load(\'models_save/nin_refine.pth\')\n        pretrained_model = torch.load(args.resume)\n        best_acc = pretrained_model[\'best_acc\']\n        model.load_state_dict(pretrained_model[\'state_dict\'])\n\n    if not args.cpu:\n        model.cuda()\n        model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n    print(model)\n\n    base_lr = float(args.lr)\n    param_dict = dict(model.named_parameters())\n    params = []\n\n    for key, value in param_dict.items():\n        params += [{\'params\':[value], \'lr\': base_lr, \'weight_decay\':args.wd}]\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(params, lr=base_lr, weight_decay=args.wd)\n\n    if args.evaluate:\n        test()\n        exit(0)\n\n    for epoch in range(1, args.epochs):\n        adjust_learning_rate(optimizer, epoch)\n        train(epoch)\n        test()\n'"
pruning/normal_regular_prune.py,14,"b'import os\nimport argparse\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\nfrom models import nin\nimport numpy as np\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--data\', action=\'store\', default=\'../data\',\n                    help=\'dataset path\')\nparser.add_argument(\'--cpu\', action=\'store_true\',\n                    help=\'disables CUDA training\')\n# percent(\xe5\x89\xaa\xe6\x9e\x9d\xe7\x8e\x87)\nparser.add_argument(\'--percent\', type=float, default=0.5,\n                    help=\'nin:0.5\')\n# \xe6\xad\xa3\xe5\xb8\xb8|\xe8\xa7\x84\xe6\x95\xb4\xe5\x89\xaa\xe6\x9e\x9d\xe6\xa0\x87\xe5\xbf\x97\nparser.add_argument(\'--normal_regular\', type=int, default=1,\n                    help=\'--normal_regular_flag (default: normal)\')\n# model\xe5\xb1\x82\xe6\x95\xb0\nparser.add_argument(\'--layers\', type=int, default=9,\n                    help=\'layers (default: 9)\')\n# \xe7\xa8\x80\xe7\x96\x8f\xe8\xae\xad\xe7\xbb\x83\xe5\x90\x8e\xe7\x9a\x84model\nparser.add_argument(\'--model\', default=\'models_save/nin_preprune.pth\', type=str, metavar=\'PATH\',\n                    help=\'path to raw trained model (default: none)\')\n# \xe5\x89\xaa\xe6\x9e\x9d\xe5\x90\x8e\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84model\nparser.add_argument(\'--save\', default=\'models_save/nin_prune.pth\', type=str, metavar=\'PATH\',\n                    help=\'path to save prune model (default: none)\')\nargs = parser.parse_args()\nbase_number = args.normal_regular\nlayers = args.layers\nprint(args)\n\nif base_number <= 0:\n    print(\'\\r\\n!base_number is error!\\r\\n\')\n    base_number = 1\n\nmodel = nin.Net()\nif args.model:\n    if os.path.isfile(args.model):\n        print(""=> loading checkpoint \'{}\'"".format(args.model))\n        model.load_state_dict(torch.load(args.model)[\'state_dict\'])\n    else:\n        print(""=> no checkpoint found at \'{}\'"".format(args.resume))\nprint(\'\xe6\x97\xa7\xe6\xa8\xa1\xe5\x9e\x8b: \', model)\ntotal = 0\ni = 0\nfor m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            if i < layers - 1:\n                i += 1\n                total += m.weight.data.shape[0]\n\n# \xe7\xa1\xae\xe5\xae\x9a\xe5\x89\xaa\xe6\x9e\x9d\xe7\x9a\x84\xe5\x85\xa8\xe5\xb1\x80\xe9\x98\x88\xe5\x80\xbc\nbn = torch.zeros(total)\nindex = 0\ni = 0\nfor m in model.modules():\n    if isinstance(m, nn.BatchNorm2d):\n        if i < layers - 1:\n            i += 1\n            size = m.weight.data.shape[0]\n            bn[index:(index+size)] = m.weight.data.abs().clone()\n            index += size\ny, j = torch.sort(bn)\nthre_index = int(total * args.percent)\nif thre_index == total:\n    thre_index = total - 1\nthre_0 = y[thre_index]\n\n#********************************\xe9\xa2\x84\xe5\x89\xaa\xe6\x9e\x9d*********************************\npruned = 0\ncfg_0 = []\ncfg = []\ncfg_mask = []\ni = 0\nfor k, m in enumerate(model.modules()):\n    if isinstance(m, nn.BatchNorm2d):\n        if i < layers - 1:\n            i += 1\n\n            weight_copy = m.weight.data.clone()\n            mask = weight_copy.abs().gt(thre_0).float()\n            remain_channels = torch.sum(mask)\n\n            if remain_channels == 0:\n                print(\'\\r\\n!please turn down the prune_ratio!\\r\\n\')\n                remain_channels = 1\n                mask[int(torch.argmax(weight_copy))]=1\n\n            # ******************\xe8\xa7\x84\xe6\x95\xb4\xe5\x89\xaa\xe6\x9e\x9d******************\n            v = 0\n            n = 1\n            if remain_channels % base_number != 0:\n                if remain_channels > base_number:\n                    while v < remain_channels:\n                        n += 1\n                        v = base_number * n\n                    if remain_channels - (v - base_number) < v - remain_channels:\n                        remain_channels = v - base_number\n                    else:\n                        remain_channels = v\n                    if remain_channels > m.weight.data.size()[0]:\n                        remain_channels = m.weight.data.size()[0]\n                    remain_channels = torch.tensor(remain_channels)\n                        \n                    y, j = torch.sort(weight_copy.abs())\n                    thre_1 = y[-remain_channels]\n                    mask = weight_copy.abs().ge(thre_1).float()\n            pruned = pruned + mask.shape[0] - torch.sum(mask)\n            m.weight.data.mul_(mask)\n            m.bias.data.mul_(mask)\n            cfg_0.append(mask.shape[0])\n            cfg.append(int(remain_channels))\n            cfg_mask.append(mask.clone())\n            print(\'layer_index: {:d} \\t total_channel: {:d} \\t remaining_channel: {:d} \\t pruned_ratio: {:f}\'.\n                format(k, mask.shape[0], int(torch.sum(mask)), (mask.shape[0] - torch.sum(mask)) / mask.shape[0]))\npruned_ratio = float(pruned/total)\nprint(\'\\r\\n!\xe9\xa2\x84\xe5\x89\xaa\xe6\x9e\x9d\xe5\xae\x8c\xe6\x88\x90!\')\nprint(\'total_pruned_ratio: \', pruned_ratio)\n#********************************\xe9\xa2\x84\xe5\x89\xaa\xe6\x9e\x9d\xe5\x90\x8emodel\xe6\xb5\x8b\xe8\xaf\x95*********************************\ndef test():\n    test_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(root = args.data, train=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),\n        batch_size = 64, shuffle=False, num_workers=1)\n    model.eval()\n    correct = 0\n    \n    for data, target in test_loader:\n        if not args.cpu:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        output = model(data)\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n    acc = 100. * float(correct) / len(test_loader.dataset)\n    print(\'Accuracy: {:.2f}%\\n\'.format(acc))\n    return\nprint(\'************\xe9\xa2\x84\xe5\x89\xaa\xe6\x9e\x9d\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xb5\x8b\xe8\xaf\x95************\')\nif not args.cpu:\n    model.cuda()\ntest()\n\n#********************************\xe5\x89\xaa\xe6\x9e\x9d*********************************\nnewmodel = nin.Net(cfg)\nif not args.cpu:\n    newmodel.cuda()\nlayer_id_in_cfg = 0\nstart_mask = torch.ones(3)\nend_mask = cfg_mask[layer_id_in_cfg]\ni = 0\nfor [m0, m1] in zip(model.modules(), newmodel.modules()):\n    if isinstance(m0, nn.BatchNorm2d):\n        if i < layers - 1:\n            i += 1\n            idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n            if idx1.size == 1:\n                idx1 = np.resize(idx1, (1,))\n            m1.weight.data = m0.weight.data[idx1].clone()\n            m1.bias.data = m0.bias.data[idx1].clone()\n            m1.running_mean = m0.running_mean[idx1].clone()\n            m1.running_var = m0.running_var[idx1].clone()\n            layer_id_in_cfg += 1\n            start_mask = end_mask.clone()\n            if layer_id_in_cfg < len(cfg_mask):  \n                end_mask = cfg_mask[layer_id_in_cfg]\n        else:\n            m1.weight.data = m0.weight.data.clone()\n            m1.bias.data = m0.bias.data.clone()\n            m1.running_mean = m0.running_mean.clone()\n            m1.running_var = m0.running_var.clone()\n    elif isinstance(m0, nn.Conv2d):\n        if i < layers - 1:\n            idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n            idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n            if idx0.size == 1:\n                idx0 = np.resize(idx0, (1,))\n            if idx1.size == 1:\n                idx1 = np.resize(idx1, (1,))\n            w = m0.weight.data[:, idx0, :, :].clone()\n            m1.weight.data = w[idx1, :, :, :].clone()\n            m1.bias.data = m0.bias.data[idx1].clone()\n        else:\n            idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n            if idx0.size == 1:\n                idx0 = np.resize(idx0, (1,))\n            m1.weight.data = m0.weight.data[:, idx0, :, :].clone()\n            m1.bias.data = m0.bias.data.clone()\n    elif isinstance(m0, nn.Linear):\n            idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n            if idx0.size == 1:\n                idx0 = np.resize(idx0, (1,))\n            m1.weight.data = m0.weight.data[:, idx0].clone()\n#******************************\xe5\x89\xaa\xe6\x9e\x9d\xe5\x90\x8emodel\xe6\xb5\x8b\xe8\xaf\x95*********************************\nprint(\'\xe6\x96\xb0\xe6\xa8\xa1\xe5\x9e\x8b: \', newmodel)\nprint(\'**********\xe5\x89\xaa\xe6\x9e\x9d\xe5\x90\x8e\xe6\x96\xb0\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xb5\x8b\xe8\xaf\x95*********\')\nmodel = newmodel\ntest()\n#******************************\xe5\x89\xaa\xe6\x9e\x9d\xe5\x90\x8emodel\xe4\xbf\x9d\xe5\xad\x98*********************************\nprint(\'**********\xe5\x89\xaa\xe6\x9e\x9d\xe5\x90\x8e\xe6\x96\xb0\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98*********\')\ntorch.save({\'cfg\': cfg, \'state_dict\': newmodel.state_dict()}, args.save)\nprint(\'**********\xe4\xbf\x9d\xe5\xad\x98\xe6\x88\x90\xe5\x8a\x9f*********\\r\\n\')\n\n#*****************************\xe5\x89\xaa\xe6\x9e\x9d\xe5\x89\x8d\xe5\x90\x8emodel\xe5\xaf\xb9\xe6\xaf\x94********************************\nprint(\'************\xe6\x97\xa7\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84************\')\nprint(cfg_0)\nprint(\'************\xe6\x96\xb0\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84************\')\nprint(cfg, \'\\r\\n\')'"
pruning/models/__init__.py,0,b''
pruning/models/nin.py,2,"b""import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\nclass FP_Conv2d(nn.Module):\n    def __init__(self, input_channels, output_channels,\n            kernel_size=-1, stride=-1, padding=-1, dropout=0, groups=1, channel_shuffle=0, shuffle_groups=1, last=0, first=0):\n        super(FP_Conv2d, self).__init__()\n        self.dropout_ratio = dropout\n        self.last = last\n        self.first_flag = first\n        if dropout!=0:\n            self.dropout = nn.Dropout(dropout)\n        self.conv = nn.Conv2d(input_channels, output_channels,\n                kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n        self.bn = nn.BatchNorm2d(output_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        if self.first_flag:\n            x = self.relu(x)\n        if self.dropout_ratio!=0:\n            x = self.dropout(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\nclass Net(nn.Module):\n    def __init__(self, cfg = None):\n        super(Net, self).__init__()\n        if cfg is None:\n            cfg = [192, 160, 96, 192, 192, 192, 192, 192]\n        \n        self.tnn_bin = nn.Sequential(\n                nn.Conv2d(3, cfg[0], kernel_size=5, stride=1, padding=2),\n                nn.BatchNorm2d(cfg[0]),\n                FP_Conv2d(cfg[0], cfg[1], kernel_size=1, stride=1, padding=0, first=1),\n                FP_Conv2d(cfg[1], cfg[2], kernel_size=1, stride=1, padding=0),\n                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n\n                FP_Conv2d(cfg[2], cfg[3], kernel_size=5, stride=1, padding=2),\n                FP_Conv2d(cfg[3], cfg[4], kernel_size=1, stride=1, padding=0),\n                FP_Conv2d(cfg[4], cfg[5], kernel_size=1, stride=1, padding=0),\n                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n\n                FP_Conv2d(cfg[5], cfg[6], kernel_size=3, stride=1, padding=1),\n                FP_Conv2d(cfg[6], cfg[7], kernel_size=1, stride=1, padding=0),\n                nn.Conv2d(cfg[7],  10, kernel_size=1, stride=1, padding=0),\n                nn.BatchNorm2d(10),\n                nn.ReLU(inplace=True),\n                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n                )\n        '''\n        self.dorefa = nn.Sequential(\n                nn.Conv2d(3, cfg[0], kernel_size=5, stride=1, padding=2),\n                nn.BatchNorm2d(cfg[0]),\n                FP_Conv2d(cfg[0], cfg[1], kernel_size=1, stride=1, padding=0, first=1),\n                FP_Conv2d(cfg[1], cfg[2], kernel_size=1, stride=1, padding=0),\n                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n\n                FP_Conv2d(cfg[2], cfg[3], kernel_size=5, stride=1, padding=2),\n                FP_Conv2d(cfg[3], cfg[4], kernel_size=1, stride=1, padding=0),\n                FP_Conv2d(cfg[4], cfg[5], kernel_size=1, stride=1, padding=0),\n                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n\n                FP_Conv2d(cfg[5], cfg[6], kernel_size=3, stride=1, padding=1),\n                FP_Conv2d(cfg[6], cfg[7], kernel_size=1, stride=1, padding=0),\n                nn.Conv2d(cfg[7],  10, kernel_size=1, stride=1, padding=0),\n                nn.BatchNorm2d(10),\n                nn.ReLU(inplace=True),\n                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n                )\n        '''\n    def forward(self, x):\n        x = self.tnn_bin(x)\n        #x = self.dorefa(x)\n        x = x.view(x.size(0), 10)\n        return x"""
pruning/models/nin_gc.py,2,"b'import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\ndef channel_shuffle(x, groups):\n    """"""shuffle channels of a 4-D Tensor""""""\n    batch_size, channels, height, width = x.size()\n    assert channels % groups == 0\n    channels_per_group = channels // groups\n    # split into groups\n    x = x.view(batch_size, groups, channels_per_group, height, width)\n    # transpose 1, 2 axis\n    x = x.transpose(1, 2).contiguous()\n    # reshape into orignal\n    x = x.view(batch_size, channels, height, width)\n    return x\n\nclass FP_Conv2d(nn.Module):\n    def __init__(self, input_channels, output_channels,\n            kernel_size=-1, stride=-1, padding=-1, dropout=0, groups=1, channel_shuffle=0, shuffle_groups=1, last=0, bin_mp=0, bin_nor=1, last_relu=0, first=0):\n        super(FP_Conv2d, self).__init__()\n        self.dropout_ratio = dropout\n        self.last = last\n        self.first_flag = first\n        self.channel_shuffle_flag = channel_shuffle\n        self.shuffle_groups = shuffle_groups\n        if dropout!=0:\n            self.dropout = nn.Dropout(dropout)\n        self.conv = nn.Conv2d(input_channels, output_channels,\n                kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n        self.bn = nn.BatchNorm2d(output_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        if self.first_flag:\n            x = self.relu(x)\n        if self.channel_shuffle_flag:\n            x = channel_shuffle(x, groups=self.shuffle_groups)\n        if self.dropout_ratio!=0:\n            x = self.dropout(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\nclass Net(nn.Module):\n    def __init__(self, cfg = None):\n        super(Net, self).__init__()\n        if cfg is None:\n            cfg = [256, 256, 256, 512, 512, 512, 1024, 1024]\n\n        self.tnn_bin = nn.Sequential(\n                nn.Conv2d(3, cfg[0], kernel_size=5, stride=1, padding=2),\n                nn.BatchNorm2d(cfg[0]),\n                FP_Conv2d(cfg[0], cfg[1], kernel_size=1, stride=1, padding=0, first=1, groups=2, channel_shuffle=0),\n                FP_Conv2d(cfg[1], cfg[2], kernel_size=1, stride=1, padding=0, groups=2, channel_shuffle=1, shuffle_groups=2, bin_mp=1),\n                nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n\n                FP_Conv2d(cfg[2], cfg[3], kernel_size=3, stride=1, padding=1, groups=16, channel_shuffle=1, shuffle_groups=2, bin_nor=0),\n                FP_Conv2d(cfg[3], cfg[4], kernel_size=1, stride=1, padding=0, groups=4, channel_shuffle=1, shuffle_groups=16),\n                FP_Conv2d(cfg[4], cfg[5], kernel_size=1, stride=1, padding=0, groups=4, channel_shuffle=1, shuffle_groups=4, bin_mp=1),\n                nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n\n                FP_Conv2d(cfg[5], cfg[6], kernel_size=3, stride=1, padding=1, groups=32, channel_shuffle=1, shuffle_groups=4, bin_nor=0),\n                FP_Conv2d(cfg[6], cfg[7], kernel_size=1, stride=1, padding=0, groups=8, channel_shuffle=1, shuffle_groups=32),\n                nn.Conv2d(cfg[7],  10, kernel_size=1, stride=1, padding=0),\n                nn.BatchNorm2d(10),\n                nn.ReLU(inplace=True),\n                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n                )\n        \'\'\'\n        self.dorefa = nn.Sequential(\n                nn.Conv2d(3, cfg[0], kernel_size=5, stride=1, padding=2),\n                nn.BatchNorm2d(cfg[0]),\n                FP_Conv2d(cfg[0], cfg[1], kernel_size=1, stride=1, padding=0, first=1, groups=2, channel_shuffle=0),\n                FP_Conv2d(cfg[1], cfg[2], kernel_size=1, stride=1, padding=0, groups=2, channel_shuffle=1, shuffle_groups=2, bin_mp=1),\n                nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n\n                FP_Conv2d(cfg[2], cfg[3], kernel_size=3, stride=1, padding=1, groups=16, channel_shuffle=1, shuffle_groups=2, bin_nor=0),\n                FP_Conv2d(cfg[3], cfg[4], kernel_size=1, stride=1, padding=0, groups=4, channel_shuffle=1, shuffle_groups=16),\n                FP_Conv2d(cfg[4], cfg[5], kernel_size=1, stride=1, padding=0, groups=4, channel_shuffle=1, shuffle_groups=4, bin_mp=1),\n                nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n\n                FP_Conv2d(cfg[5], cfg[6], kernel_size=3, stride=1, padding=1, groups=32, channel_shuffle=1, shuffle_groups=4, bin_nor=0),\n                FP_Conv2d(cfg[6], cfg[7], kernel_size=1, stride=1, padding=0, groups=8, channel_shuffle=1, shuffle_groups=32),\n                nn.Conv2d(cfg[7],  10, kernel_size=1, stride=1, padding=0),\n                nn.BatchNorm2d(10),\n                nn.ReLU(inplace=True),\n                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n                )\n        \'\'\'\n    def forward(self, x):\n        x = self.tnn_bin(x)\n        #x = self.dorefa(x)\n        x = x.view(x.size(0), 10)\n        return x\n'"
quantization/WbWtAb/main.py,16,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport math\nimport argparse\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torchvision\nimport torchvision.transforms as transforms\nfrom models import nin_gc\nfrom models import nin\n#from models import nin_bn_conv\nimport os\n\n# \xe9\x9a\x8f\xe6\x9c\xba\xe7\xa7\x8d\xe5\xad\x90\xe2\x80\x94\xe2\x80\x94\xe8\xae\xad\xe7\xbb\x83\xe7\xbb\x93\xe6\x9e\x9c\xe5\x8f\xaf\xe5\xa4\x8d\xe7\x8e\xb0\ndef setup_seed(seed):\n    torch.manual_seed(seed)                    \n    #torch.cuda.manual_seed(seed)              \n    torch.cuda.manual_seed_all(seed)           \n    np.random.seed(seed)                       \n    torch.backends.cudnn.deterministic = True\n\n# \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\ndef save_state(model, best_acc):\n    print(\'==> Saving model ...\')\n    state = {\n            \'best_acc\': best_acc,\n            \'state_dict\': model.state_dict(),\n            }\n    state_copy = state[\'state_dict\'].copy()\n    for key in state_copy.keys():\n        if \'module\' in key:\n            state[\'state_dict\'][key.replace(\'module.\', \'\')] = \\\n                    state[\'state_dict\'].pop(key)\n    if args.model_type == 0:\n        torch.save(state, \'models_save/nin.pth\')\n    else:\n        torch.save(state, \'models_save/nin_gc.pth\')\n\n# \xe8\xae\xad\xe7\xbb\x83lr\xe8\xb0\x83\xe6\x95\xb4\ndef adjust_learning_rate(optimizer, epoch):\n    update_list = [80, 130, 180, 230, 280]\n    if epoch in update_list:\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = param_group[\'lr\'] * 0.1\n    return\n\n# \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xae\xad\xe7\xbb\x83\ndef train(epoch):\n    model.train()\n\n    for batch_idx, (data, target) in enumerate(trainloader):\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        if not args.cpu:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        output = model(data)\n        loss = criterion(output, target)\n\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        optimizer.zero_grad()\n        loss.backward() # \xe6\xb1\x82\xe6\xa2\xaf\xe5\xba\xa6\n        optimizer.step() # \xe5\x8f\x82\xe6\x95\xb0\xe6\x9b\xb4\xe6\x96\xb0\n\n        # \xe6\x98\xbe\xe7\xa4\xba\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86loss(/100\xe4\xb8\xaabatch)\n        if batch_idx % 100 == 0:\n            print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR: {}\'.format(\n                epoch, batch_idx * len(data), len(trainloader.dataset),\n                100. * batch_idx / len(trainloader), loss.data.item(),\n                optimizer.param_groups[0][\'lr\']))\n    return\n\n# \xe6\xa8\xa1\xe5\x9e\x8b\xe6\xb5\x8b\xe8\xaf\x95\ndef test():\n    global best_acc\n    model.eval()\n    test_loss = 0\n    average_test_loss = 0\n    correct = 0\n\n    for data, target in testloader:\n        if not args.cpu:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        output = model(data)\n        test_loss += criterion(output, target).data.item()\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n    # \xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n    acc = 100. * float(correct) / len(testloader.dataset)\n\n    # \xe6\x9c\x80\xe4\xbc\x98\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe5\x8f\x8amodel\xe4\xbf\x9d\xe5\xad\x98\n    if acc > best_acc:\n        best_acc = acc\n        save_state(model, best_acc)\n    average_test_loss = test_loss / (len(testloader.dataset) / args.eval_batch_size)\n\n    # \xe6\x98\xbe\xe7\xa4\xba\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x81\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n    print(\'\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\'.format(\n        average_test_loss, correct, len(testloader.dataset),\n        100. * float(correct) / len(testloader.dataset)))\n\n    # \xe6\x98\xbe\xe7\xa4\xba\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe6\x9c\x80\xe4\xbc\x98\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n    print(\'Best Accuracy: {:.2f}%\\n\'.format(best_acc))\n    return\n\nif __name__==\'__main__\':\n    parser = argparse.ArgumentParser()\n    # cpu\xe3\x80\x81gpu\n    parser.add_argument(\'--cpu\', action=\'store_true\',\n            help=\'set if only CPU is available\')\n    # gpu_id\n    parser.add_argument(\'--gpu_id\', action=\'store\', default=\'\',\n            help=\'gpu_id\')\n    # dataset\n    parser.add_argument(\'--data\', action=\'store\', default=\'../../data\',\n            help=\'dataset path\')\n    # lr\n    parser.add_argument(\'--lr\', action=\'store\', default=0.01,\n            help=\'the intial learning rate\')\n    # weight_dacay\n    parser.add_argument(\'--wd\', action=\'store\', default=0,\n            help=\'nin_gc:0, nin:1e-5\')\n    # resume\n    parser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n            help=\'the path to the resume model\')\n    # refine\n    parser.add_argument(\'--refine\', default=\'\', type=str, metavar=\'PATH\',\n            help=\'the path to the refine(prune) model\')\n    # evaluate\n    parser.add_argument(\'--evaluate\', action=\'store_true\',\n            help=\'evaluate the model\')\n    # batch_size\xe3\x80\x81num_workers\n    parser.add_argument(\'--train_batch_size\', type=int, default=50)\n    parser.add_argument(\'--eval_batch_size\', type=int, default=256)\n    parser.add_argument(\'--num_workers\', type=int, default=2)\n    # epochs\n    parser.add_argument(\'--start_epochs\', type=int, default=1, metavar=\'N\',\n            help=\'number of epochs to train_start\')\n    parser.add_argument(\'--end_epochs\', type=int, default=300, metavar=\'N\',\n            help=\'number of epochs to train_end\')\n    # W/A \xe2\x80\x94 FP/\xe4\xb8\x89\xe5\x80\xbc/\xe4\xba\x8c\xe5\x80\xbc\n    parser.add_argument(\'--W\', type=int, default=2,\n            help=\'Wb:2, Wt:3, Wfp:32\')\n    parser.add_argument(\'--A\', type=int, default=2,\n            help=\'Ab:2, Afp:32\')\n    # \xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84\xe9\x80\x89\xe6\x8b\xa9\n    parser.add_argument(\'--model_type\', type=int, default=1,\n            help=\'model type:0-nin,1-nin_gc\')\n    args = parser.parse_args()\n    print(\'==> Options:\',args)\n\n    if args.gpu_id:\n        os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu_id\n    \n    setup_seed(1)#\xe9\x9a\x8f\xe6\x9c\xba\xe7\xa7\x8d\xe5\xad\x90\xe2\x80\x94\xe2\x80\x94\xe8\xae\xad\xe7\xbb\x83\xe7\xbb\x93\xe6\x9e\x9c\xe5\x8f\xaf\xe5\xa4\x8d\xe7\x8e\xb0\n\n    print(\'==> Preparing data..\')\n    # \xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\n    # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xef\xbc\x9a\xe9\x9a\x8f\xe6\x9c\xba\xe8\xa3\x81\xe5\x89\xaa + \xe6\xb0\xb4\xe5\xb9\xb3\xe7\xbf\xbb\xe8\xbd\xac + \xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n    # \xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xef\xbc\x9a\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n\n    # \xe6\x95\xb0\xe6\x8d\xae\xe5\x8a\xa0\xe8\xbd\xbd\n    trainset = torchvision.datasets.CIFAR10(root = args.data, train = True, download = True, transform = transform_train)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.train_batch_size, shuffle=True, num_workers=args.num_workers) # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\x95\xb0\xe6\x8d\xae\n\n    testset = torchvision.datasets.CIFAR10(root = args.data, train = False, download = True, transform = transform_test)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=args.eval_batch_size, shuffle=False, num_workers=args.num_workers) # \xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe6\x95\xb0\xe6\x8d\xae\n\n    # cifar10\xe7\xb1\xbb\xe5\x88\xab\n    classes = (\'plane\', \'car\', \'bird\', \'cat\', \'deer\', \'dog\', \'frog\', \'horse\', \'ship\', \'truck\')\n\n    # model\n    if args.refine:\n        print(\'******Refine model******\')\n        #checkpoint = torch.load(\'../prune/models_save/nin_refine.pth\')\n        checkpoint = torch.load(args.refine)\n        if args.model_type == 0:\n            model = nin.Net(cfg=checkpoint[\'cfg\'], A=args.A, W=args.W)\n        else:\n            model = nin_gc.Net(cfg=checkpoint[\'cfg\'], A=args.A, W=args.W)\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        best_acc = 0\n    else:\n        print(\'******Initializing model******\')\n        # ******************** \xe5\x9c\xa8model\xe7\x9a\x84\xe9\x87\x8f\xe5\x8c\x96\xe5\x8d\xb7\xe7\xa7\xaf\xe4\xb8\xad\xe5\x90\x8c\xe6\x97\xb6\xe9\x87\x8f\xe5\x8c\x96A(\xe7\x89\xb9\xe5\xbe\x81)\xe5\x92\x8cW(\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0) ************************\n        if args.model_type == 0:\n            model = nin.Net(A=args.A, W=args.W)\n        else:\n            model = nin_gc.Net(A=args.A, W=args.W)\n        #model = nin_bn_conv.Net()\n        best_acc = 0\n        for m in model.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight.data)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n    if args.resume:\n        print(\'******Reume model******\')\n        #pretrained_model = torch.load(\'models_save/nin_gc.pth\')\n        pretrained_model = torch.load(args.resume)\n        best_acc = pretrained_model[\'best_acc\']\n        model.load_state_dict(pretrained_model[\'state_dict\'])\n\n    # cpu\xe3\x80\x81gpu\n    if not args.cpu:\n        model.cuda()\n        model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))# gpu\xe5\xb9\xb6\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\n    # \xe6\x89\x93\xe5\x8d\xb0\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84\n    print(model)\n\n    # \xe8\xb6\x85\xe5\x8f\x82\xe6\x95\xb0\n    base_lr = float(args.lr)\n    param_dict = dict(model.named_parameters())\n    params = []\n    for key, value in param_dict.items():\n        params += [{\'params\':[value], \'lr\': base_lr, \'weight_decay\':args.wd}]\n\n    # \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\n    criterion = nn.CrossEntropyLoss()\n    # \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n    optimizer = optim.Adam(params, lr=base_lr, weight_decay=args.wd)\n\n    # \xe6\xb5\x8b\xe8\xaf\x95\xe6\xa8\xa1\xe5\x9e\x8b\n    if args.evaluate:\n        test()\n        exit(0)\n\n    # \xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\n    for epoch in range(args.start_epochs, args.end_epochs):\n        adjust_learning_rate(optimizer, epoch)\n        train(epoch)\n        test()\n'"
quantization/WbWtAb/bn_folding/bn_folding.py,13,"b'import sys\nimport numpy as np\nimport argparse\nimport torch\nimport torch.nn as nn\nimport nin_gc_inference\nimport nin_gc_training\nfrom util_wt_bab import weight_tnn_bin, Conv2d_Q\n\n# ******************** \xe6\x98\xaf\xe5\x90\xa6\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\xe5\xae\x8c\xe6\x95\xb4\xe5\x8f\x82\xe6\x95\xb0 ********************\n#torch.set_printoptions(precision=8, edgeitems=sys.maxsize, linewidth=200, sci_mode=False)\n\n# \xe5\x8e\x9fBN\xe6\x9b\xbf\xe4\xbb\xa3\xe5\xb1\x82\nclass DummyModule(nn.Module):\n    def __init__(self):\n        super(DummyModule, self).__init__()\n\n    def forward(self, x):\n        return x\n\n# BN\xe8\x9e\x8d\xe5\x90\x88\ndef bn_folding(conv, bn):\n    # \xe5\x8f\xaf\xe4\xbb\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe2\x80\x9c\xe9\x92\x88\xe5\xaf\xb9\xe7\x89\xb9\xe5\xbe\x81(A)\xe4\xba\x8c\xe5\x80\xbc\xe7\x9a\x84BN\xe8\x9e\x8d\xe5\x90\x88\xe2\x80\x9d\xe7\x9a\x84BN\xe5\xb1\x82\xe4\xbd\x8d\xe7\xbd\xae\n    global bn_counter, bn_folding_range_min, bn_folding_range_max\n    bn_counter = bn_counter + 1\n    # ******************** BN\xe5\x8f\x82\xe6\x95\xb0 *********************\n    mean = bn.running_mean\n    std = torch.sqrt(bn.running_var + bn.eps)\n    gamma = bn.weight\n    beta = bn.bias\n    # ******************* conv\xe5\x8f\x82\xe6\x95\xb0 ********************\n    w = conv.weight\n    w_fold = w.clone()\n    if conv.bias is not None:\n        b = conv.bias\n    else:\n        b = mean.new_zeros(mean.shape)\n    b_fold = b.clone()\n    # ******************* \xe9\x92\x88\xe5\xaf\xb9\xe7\x89\xb9\xe5\xbe\x81(A)\xe4\xba\x8c\xe5\x80\xbc\xe7\x9a\x84BN\xe8\x9e\x8d\xe5\x90\x88 *******************\n    if(bn_counter >= bn_folding_range_min and bn_counter <= bn_folding_range_max + 1):\n        mask_positive = gamma.data.gt(0)\n        mask_negetive = gamma.data.lt(0)\n\n        w_fold[mask_positive] = w[mask_positive]\n        b_fold[mask_positive] = b[mask_positive] - mean[mask_positive] + beta[mask_positive] * (std[mask_positive] / gamma[mask_positive])\n\n        w_fold[mask_negetive] = w[mask_negetive] * -1\n        b_fold[mask_negetive] = mean[mask_negetive] - b[mask_negetive] - beta[mask_negetive] * (std[mask_negetive] / gamma[mask_negetive])\n    # ******************* \xe6\x99\xae\xe9\x80\x9aBN\xe8\x9e\x8d\xe5\x90\x88 *******************\n    else:\n        w_fold = w * (gamma / std).reshape([conv.out_channels, 1, 1, 1])\n        b_fold = beta + (b - mean) * (gamma / std) \n    # \xe6\x96\xb0\xe5\xbb\xbabnfold_conv(BN\xe8\x9e\x8d\xe5\x90\x88\xe7\x9a\x84Conv\xe5\xb1\x82),\xe5\xb0\x86w_fold,b_fold\xe8\xb5\x8b\xe5\x80\xbc\xe4\xba\x8e\xe5\x85\xb6\xe5\x8f\x82\xe6\x95\xb0\n    bnfold_conv = nn.Conv2d(conv.in_channels,\n                         conv.out_channels,\n                         conv.kernel_size,\n                         conv.stride,\n                         conv.padding,\n                         groups=conv.groups,\n                         bias=True)\n    bnfold_conv.weight.data = w_fold\n    bnfold_conv.bias.data = b_fold\n    return bnfold_conv\n\n# \xe6\xa8\xa1\xe5\x9e\x8bBN\xe8\x9e\x8d\xe5\x90\x88\ndef model_bn_folding(model):\n    children = list(model.named_children())\n    name_temp = None\n    child_temp = None\n    for name, child in children:\n        if isinstance(child, nn.BatchNorm2d):\n            bnfold_conv = bn_folding(child_temp, child) # BN\xe8\x9e\x8d\xe5\x90\x88\n            model._modules[name_temp] = bnfold_conv\n            model._modules[name] = DummyModule()\n            child_temp = None\n        elif isinstance(child, nn.Conv2d):\n            name_temp = name\n            child_temp = child\n        else:\n            model_bn_folding(child)\n    return model\n\nif __name__==\'__main__\':\n    # ********************** \xe5\x8f\xaf\xe9\x80\x89\xe9\x85\x8d\xe7\xbd\xae\xe5\x8f\x82\xe6\x95\xb0 **********************\n    parser = argparse.ArgumentParser()\n    # W \xe2\x80\x94\xe2\x80\x94 \xe4\xb8\x89\xe5\x80\xbc/\xe4\xba\x8c\xe5\x80\xbc(\xe6\x8d\xae\xe8\xae\xad\xe7\xbb\x83\xe6\x97\xb6W\xe9\x87\x8f\xe5\x8c\x96(\xe4\xb8\x89/\xe4\xba\x8c\xe5\x80\xbc)\xe6\x83\x85\xe5\x86\xb5\xe8\x80\x8c\xe5\xae\x9a)\n    parser.add_argument(\'--W\', type=int, default=2,\n                help=\'Wb:2, Wt:3\')\n    args = parser.parse_args()\n    print(\'==> Options:\',args)\n\n    weight_quantizer = weight_tnn_bin(W=args.W)  # \xe5\xae\x9e\xe4\xbe\x8b\xe5\x8c\x96W\xe9\x87\x8f\xe5\x8c\x96\xe5\x99\xa8\n\n    print(""************* \xe5\x8f\x82\xe6\x95\xb0\xe9\x87\x8f\xe5\x8c\x96\xe8\xa1\xa8\xe7\xa4\xba + BN_folding \xe2\x80\x94\xe2\x80\x94 Beginning **************"")\n    # ********************** \xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8a\xa0\xe8\xbd\xbd ************************\n    model_0 = nin_gc_training.Net()\n    model_1 = nin_gc_inference.Net()\n    model_0.load_state_dict(torch.load(\'../models_save/nin_gc.pth\')[\'state_dict\'])\n\n    # ********************** W\xe5\x85\xa8\xe7\xb2\xbe\xe5\xba\xa6\xe8\xa1\xa8\xe7\xa4\xba ************************\n    torch.save(model_0, \'models_save/model.pth\')\n    torch.save(model_0.state_dict(), \'models_save/model_para.pth\')\n    model_array = np.array(model_0)\n    model_para_array = np.array(model_0.state_dict())\n    np.savetxt(\'models_save/model.txt\', [model_array], fmt = \'%s\', delimiter=\',\')\n    np.savetxt(\'models_save/model_para.txt\', [model_para_array], fmt = \'%s\', delimiter=\',\')\n    # ********************** W\xe9\x87\x8f\xe5\x8c\x96\xe8\xa1\xa8\xe7\xa4\xba(\xe6\x8d\xae\xe8\xae\xad\xe7\xbb\x83\xe6\x97\xb6W\xe9\x87\x8f\xe5\x8c\x96(\xe4\xb8\x89/\xe4\xba\x8c\xe5\x80\xbc)\xe6\x83\x85\xe5\x86\xb5\xe8\x80\x8c\xe5\xae\x9a) *************************\n    bn_folding_range = []\n    bn_folding_num = 0\n    bn_folding_range_min = 0\n    bn_folding_range_max = 0\n    for m in model_0.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            bn_folding_num += 1\n        if isinstance(m, Conv2d_Q):\n            m.weight.data = weight_quantizer(m.weight)  # W\xe9\x87\x8f\xe5\x8c\x96\xe8\xa1\xa8\xe7\xa4\xba\n            bn_folding_range.append(bn_folding_num)     # \xe7\xbb\x9f\xe8\xae\xa1\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe2\x80\x9c\xe9\x92\x88\xe5\xaf\xb9\xe7\x89\xb9\xe5\xbe\x81(A)\xe4\xba\x8c\xe5\x80\xbc\xe7\x9a\x84BN\xe8\x9e\x8d\xe5\x90\x88\xe2\x80\x9d\xe7\x9a\x84BN\xe5\xb1\x82\xe4\xbd\x8d\xe7\xbd\xae\n    bn_folding_range_min = bn_folding_range[0]\n    bn_folding_range_max = bn_folding_range[-1]\n    torch.save(model_0, \'models_save/quan_model.pth\')                    # \xe4\xbf\x9d\xe5\xad\x98\xe9\x87\x8f\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9e\x8b(\xe7\xbb\x93\xe6\x9e\x84+\xe5\x8f\x82\xe6\x95\xb0)\n    torch.save(model_0.state_dict(), \'models_save/quan_model_para.pth\')  # \xe4\xbf\x9d\xe5\xad\x98\xe9\x87\x8f\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\n    model_array = np.array(model_0)\n    model_para_array = np.array(model_0.state_dict())\n    np.savetxt(\'models_save/quan_model.txt\', [model_array], fmt = \'%s\', delimiter=\',\')\n    np.savetxt(\'models_save/quan_model_para.txt\', [model_para_array], fmt = \'%s\', delimiter=\',\')\n    print(""************* \xe5\x8f\x82\xe6\x95\xb0\xe9\x87\x8f\xe5\x8c\x96\xe8\xa1\xa8\xe7\xa4\xba-\xe5\xae\x8c\xe6\x88\x90 **************"")\n\n    # ********************* BN\xe8\x9e\x8d\xe5\x90\x88 **********************\n    bn_counter = 0\n    model_1.load_state_dict(torch.load(\'models_save/quan_model_para.pth\'))\n    quan_bn_folding_model = model_bn_folding(model_1) #  \xe6\xa8\xa1\xe5\x9e\x8bBN\xe8\x9e\x8d\xe5\x90\x88\n    torch.save(quan_bn_folding_model, \'models_save/quan_bn_folding_model.pth\')                   # \xe4\xbf\x9d\xe5\xad\x98\xe9\x87\x8f\xe5\x8c\x96\xe8\x9e\x8d\xe5\x90\x88\xe6\xa8\xa1\xe5\x9e\x8b(\xe7\xbb\x93\xe6\x9e\x84+\xe5\x8f\x82\xe6\x95\xb0)\n    torch.save(quan_bn_folding_model.state_dict(), \'models_save/quan_bn_folding_model_para.pth\') # \xe4\xbf\x9d\xe5\xad\x98\xe9\x87\x8f\xe5\x8c\x96\xe8\x9e\x8d\xe5\x90\x88\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\n    model_array = np.array(quan_bn_folding_model)\n    model_para_array = np.array(quan_bn_folding_model.state_dict())\n    np.savetxt(\'models_save/quan_bn_folding_model.txt\', [model_array], fmt = \'%s\', delimiter=\',\')\n    np.savetxt(\'models_save/quan_bn_folding_model_para.txt\', [model_para_array], fmt = \'%s\', delimiter=\',\')\n    print(""************* BN_folding-\xe5\xae\x8c\xe6\x88\x90 **************"")\n\n    # *********************** \xe8\xbd\xac\xe6\x8d\xa2\xe9\xa2\x84\xe6\xb5\x8b\xe8\xaf\x95(dataset\xe6\xb5\x8b\xe8\xaf\x95\xe5\x9c\xa8bn_folding_test_model.py\xe4\xb8\xad\xe8\xbf\x9b\xe8\xa1\x8c) *************************\n    quan_model = nin_gc_inference.Net()\n    quan_model.load_state_dict(torch.load(\'models_save/quan_model_para.pth\'))    # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\x87\x8f\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9e\x8b\n    quan_model.eval()\n    quan_bn_folding_model.eval()\n    softmax = nn.Softmax(dim=1)\n    f = 0\n    epochs = 100\n    print(""\\r\\n************* \xe8\xbd\xac\xe6\x8d\xa2\xe9\xa2\x84\xe6\xb5\x8b\xe8\xaf\x95 **************"")\n    for i in range(0, epochs):\n        p = torch.rand([1, 3, 32, 32])\n        out = softmax(quan_model(p))                       # \xe9\x87\x8f\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xb5\x8b\xe8\xaf\x95\n        out_bn_folding = softmax(quan_bn_folding_model(p)) # \xe9\x87\x8f\xe5\x8c\x96\xe8\x9e\x8d\xe5\x90\x88\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xb5\x8b\xe8\xaf\x95\n        #print(out_bn_folding)\n        if(out.argmax() == out_bn_folding.argmax()):\n            f += 1\n    print(\'The last result:\')\n    print(\'quan_model_output:\', out)\n    print(\'quan_bn_folding_model_output:\', out_bn_folding)\n    print(""bn_folding_success_rate: {:.2f}%"".format((f / epochs) * 100))\n'"
quantization/WbWtAb/bn_folding/bn_folding_model_test.py,7,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport os\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torchvision\nimport torchvision.transforms as transforms\nimport argparse\nimport numpy as np\nimport time\nimport nin_gc_inference\nfrom bn_folding import DummyModule\n\n# \xe9\x87\x8f\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xb5\x8b\xe8\xaf\x95\ndef test_quan_model():\n    quan_model.eval()\n    test_loss = 0\n    average_test_loss = 0\n    correct = 0\n  \n    start_time = time.time()\n    for data, target in testloader:\n        if not args.cpu:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n                                    \n        output = quan_model(data)\n\n        test_loss += criterion(output, target).data.item()\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n    end_time = time.time()\n    inference_time = end_time - start_time\n    FPS = len(testloader.dataset) / inference_time\n\n    acc = 100. * float(correct) / len(testloader.dataset)\n    average_test_loss = test_loss / (len(testloader.dataset) / args.eval_batch_size)\n\n    print(\'\\nquan_model: Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%), inference_time:{:.4f}ms, FPS:{:.4f}\'.format(\n        average_test_loss, correct, len(testloader.dataset), acc, inference_time * 1000, FPS))\n    return\n\n# \xe9\x87\x8f\xe5\x8c\x96BN\xe8\x9e\x8d\xe5\x90\x88\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xb5\x8b\xe8\xaf\x95\ndef test_quan_bn_folding_model():\n    quan_bn_folding_model.eval()\n    test_loss_bn_folding = 0\n    average_test_loss_bn_folding = 0\n    correct_bn_folding = 0\n\n    start_time = time.time()\n    for data, target in testloader:\n        if not args.cpu:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n                                    \n        output_bn_folding = quan_bn_folding_model(data)\n\n        test_loss_bn_folding += criterion(output_bn_folding, target).data.item()\n        pred_bn_folding = output_bn_folding.data.max(1, keepdim=True)[1]\n        correct_bn_folding += pred_bn_folding.eq(target.data.view_as(pred_bn_folding)).cpu().sum()\n    end_time = time.time()\n    inference_time = end_time - start_time\n    FPS = len(testloader.dataset) / inference_time\n\n    acc_bn_folding = 100. * float(correct_bn_folding) / len(testloader.dataset)\n    average_test_loss_bn_folding = test_loss_bn_folding / (len(testloader.dataset) / args.eval_batch_size)\n\n    print(\'\\nquan_bn_folding_model: Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%), inference_time:{:.4f}ms, FPS:{:.4f}\'.format(\n        average_test_loss_bn_folding, correct_bn_folding, len(testloader.dataset), acc_bn_folding, inference_time * 1000, FPS))\n    return\n\nif __name__==\'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--cpu\', action=\'store_true\',\n            help=\'set if only CPU is available\')\n    parser.add_argument(\'--gpu_id\', action=\'store\', default=\'\',\n            help=\'gpu_id\')\n    parser.add_argument(\'--data\', action=\'store\', default=\'../../../data\',\n            help=\'dataset path\')\n    parser.add_argument(\'--eval_batch_size\', type=int, default=256)\n    parser.add_argument(\'--num_workers\', type=int, default=2)\n    parser.add_argument(\'--epochs\', type=int, default=300, metavar=\'N\',\n            help=\'number of epochs to train (default: 160)\')\n    args = parser.parse_args()\n    print(\'==> Options:\',args)\n\n    if args.gpu_id:\n        os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu_id\n\n    print(\'==> Preparing data..\')\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n\n    testset = torchvision.datasets.CIFAR10(root = args.data, train = False, download = True, transform = transform_test)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=args.eval_batch_size, shuffle=False, num_workers=args.num_workers)\n\n    # define classes\n    classes = (\'plane\', \'car\', \'bird\', \'cat\', \'deer\', \'dog\', \'frog\', \'horse\', \'ship\', \'truck\')\n\n    quan_model = nin_gc_inference.Net()\n    quan_model.load_state_dict(torch.load(\'models_save/quan_model_para.pth\'))   # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\x87\x8f\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9e\x8b\n    quan_bn_folding_model = torch.load(\'models_save/quan_bn_folding_model.pth\') # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\x87\x8f\xe5\x8c\x96BN\xe8\x9e\x8d\xe5\x90\x88\xe6\xa8\xa1\xe5\x9e\x8b\n    quan_model.eval()\n    quan_bn_folding_model.eval()\n    if not args.cpu:\n        quan_model.cuda()\n        quan_bn_folding_model.cuda()\n        quan_model = torch.nn.DataParallel(quan_model, device_ids=range(torch.cuda.device_count()))\n        quan_bn_folding_model = torch.nn.DataParallel(quan_bn_folding_model, device_ids=range(torch.cuda.device_count()))\n\n    criterion = nn.CrossEntropyLoss()\n    \n    print(""********* bn_folding_model_test start *********"")\n    # \xe8\x9e\x8d\xe5\x90\x88\xe5\x89\x8d\xe5\x90\x8e\xe6\xa8\xa1\xe5\x9e\x8b\xe5\xaf\xb9\xe6\xaf\x94\xe6\xb5\x8b\xe8\xaf\x95,\xe8\xbe\x93\xe5\x87\xbaacc\xe5\x92\x8cFPS,\xe7\x94\xb1\xe7\xbb\x93\xe6\x9e\x9c\xe5\x8f\xaf\xe7\x9f\xa5:BN\xe8\x9e\x8d\xe5\x90\x88\xe6\x88\x90\xe5\x8a\x9f,\xe5\xae\x9e\xe7\x8e\xb0\xe6\x97\xa0\xe6\x8d\x9f\xe5\x8a\xa0\xe9\x80\x9f\n    for epoch in range(1, args.epochs):\n        test_quan_model()            # \xe9\x87\x8f\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xb5\x8b\xe8\xaf\x95\n        test_quan_bn_folding_model() # \xe9\x87\x8f\xe5\x8c\x96BN\xe8\x9e\x8d\xe5\x90\x88\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xb5\x8b\xe8\xaf\x95\n'"
quantization/WbWtAb/bn_folding/nin_gc_inference.py,1,"b'import torch.nn as nn\nfrom util_wt_bab import activation_bin\n\n# \xe9\x80\x9a\xe9\x81\x93\xe6\xb7\xb7\xe5\x90\x88\ndef channel_shuffle(x, groups):\n    """"""shuffle channels of a 4-D Tensor""""""\n    batch_size, channels, height, width = x.size()\n    assert channels % groups == 0\n    channels_per_group = channels // groups\n    # split into groups\n    x = x.view(batch_size, groups, channels_per_group, height, width)\n    # transpose 1, 2 axis\n    x = x.transpose(1, 2).contiguous()\n    # reshape into orignal\n    x = x.view(batch_size, channels, height, width)\n    return x\n\n# *********************\xe9\x87\x8f\xe5\x8c\x96(\xe4\xb8\x89\xe5\x80\xbc\xe3\x80\x81\xe4\xba\x8c\xe5\x80\xbc)\xe5\x8d\xb7\xe7\xa7\xaf*********************\nclass Tnn_Bin_Conv2d(nn.Module):\n    # \xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x9agroups-\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x88\x86\xe7\xbb\x84\xe6\x95\xb0\xe3\x80\x81channel_shuffle-\xe9\x80\x9a\xe9\x81\x93\xe6\xb7\xb7\xe5\x90\x88\xe6\xa0\x87\xe5\xbf\x97\xe3\x80\x81shuffle_groups-\xe9\x80\x9a\xe9\x81\x93\xe6\xb7\xb7\xe5\x90\x88\xe6\x95\xb0\xef\xbc\x88\xe6\x9c\xac\xe5\xb1\x82\xe9\x9c\x80\xe4\xb8\x8e\xe4\xb8\x8a\xe4\xb8\x80\xe5\xb1\x82\xe5\x88\x86\xe7\xbb\x84\xe6\x95\xb0\xe4\xbf\x9d\xe6\x8c\x81\xe4\xb8\x80\xe8\x87\xb4\xef\xbc\x89\xe3\x80\x81last_bin-\xe5\xb0\xbe\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe8\xbe\x93\xe5\x85\xa5\xe4\xba\x8c\xe5\x80\xbc\n    def __init__(self, input_channels, output_channels,\n            kernel_size=-1, stride=-1, padding=-1, groups=1, channel_shuffle=0, shuffle_groups=1, last_bin=0):\n        super(Tnn_Bin_Conv2d, self).__init__()\n        self.channel_shuffle_flag = channel_shuffle\n        self.shuffle_groups = shuffle_groups\n        self.last_bin = last_bin\n\n        self.tnn_bin_conv = nn.Conv2d(input_channels, output_channels,\n                kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n        self.bn = nn.BatchNorm2d(output_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.bin_a = activation_bin(A=2)\n    \n    def forward(self, x):\n        x = self.bin_a(x)\n        if self.channel_shuffle_flag:\n            x = channel_shuffle(x, groups=self.shuffle_groups)\n        x = self.tnn_bin_conv(x)\n        x = self.bn(x)\n        if self.last_bin:\n            x = self.bin_a(x)\n        return x\n\nclass Net(nn.Module):\n    def __init__(self, cfg = None, A=2):\n        super(Net, self).__init__()\n        if cfg is None:\n            # \xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84\n            cfg = [256, 256, 256, 512, 512, 512, 1024, 1024]\n\n        self.tnn_bin = nn.Sequential(\n                nn.Conv2d(3, cfg[0], kernel_size=5, stride=1, padding=2),\n                nn.BatchNorm2d(cfg[0]),\n                Tnn_Bin_Conv2d(cfg[0], cfg[1], kernel_size=1, stride=1, padding=0, groups=2, channel_shuffle=0),\n                Tnn_Bin_Conv2d(cfg[1], cfg[2], kernel_size=1, stride=1, padding=0, groups=2, channel_shuffle=1, shuffle_groups=2),\n                nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n\n                Tnn_Bin_Conv2d(cfg[2], cfg[3], kernel_size=3, stride=1, padding=1, groups=16, channel_shuffle=1, shuffle_groups=2),\n                Tnn_Bin_Conv2d(cfg[3], cfg[4], kernel_size=1, stride=1, padding=0, groups=4, channel_shuffle=1, shuffle_groups=16),\n                Tnn_Bin_Conv2d(cfg[4], cfg[5], kernel_size=1, stride=1, padding=0, groups=4, channel_shuffle=1, shuffle_groups=4),\n                nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n\n                Tnn_Bin_Conv2d(cfg[5], cfg[6], kernel_size=3, stride=1, padding=1, groups=32, channel_shuffle=1, shuffle_groups=4),\n                Tnn_Bin_Conv2d(cfg[6], cfg[7], kernel_size=1, stride=1, padding=0, groups=8, channel_shuffle=1, shuffle_groups=32, last_bin=1),\n                nn.Conv2d(cfg[7],  10, kernel_size=1, stride=1, padding=0),\n                nn.BatchNorm2d(10),\n                nn.ReLU(inplace=True),\n                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n                )\n\n    def forward(self, x):\n        x = self.tnn_bin(x)\n        x = x.view(x.size(0), -1)\n        return x\n'"
quantization/WbWtAb/bn_folding/nin_gc_training.py,1,"b'import torch.nn as nn\nfrom util_wt_bab import activation_bin, Conv2d_Q\n\n# \xe9\x80\x9a\xe9\x81\x93\xe6\xb7\xb7\xe5\x90\x88\ndef channel_shuffle(x, groups):\n    """"""shuffle channels of a 4-D Tensor""""""\n    batch_size, channels, height, width = x.size()\n    assert channels % groups == 0\n    channels_per_group = channels // groups\n    # split into groups\n    x = x.view(batch_size, groups, channels_per_group, height, width)\n    # transpose 1, 2 axis\n    x = x.transpose(1, 2).contiguous()\n    # reshape into orignal\n    x = x.view(batch_size, channels, height, width)\n    return x\n\n# ********************* \xe9\x87\x8f\xe5\x8c\x96(\xe4\xb8\x89/\xe4\xba\x8c\xe5\x80\xbc)\xe6\xa8\xa1\xe5\x9d\x97 ************************\nclass Tnn_Bin_Conv2d(nn.Module):\n    # \xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x9agroups-\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x88\x86\xe7\xbb\x84\xe6\x95\xb0\xe3\x80\x81channel_shuffle-\xe9\x80\x9a\xe9\x81\x93\xe6\xb7\xb7\xe5\x90\x88\xe6\xa0\x87\xe5\xbf\x97\xe3\x80\x81shuffle_groups-\xe9\x80\x9a\xe9\x81\x93\xe6\xb7\xb7\xe5\x90\x88\xe6\x95\xb0\xef\xbc\x88\xe6\x9c\xac\xe5\xb1\x82\xe9\x9c\x80\xe4\xb8\x8e\xe4\xb8\x8a\xe4\xb8\x80\xe5\xb1\x82\xe5\x88\x86\xe7\xbb\x84\xe6\x95\xb0\xe4\xbf\x9d\xe6\x8c\x81\xe4\xb8\x80\xe8\x87\xb4)\xe3\x80\x81last_relu\xef\xbd\x9clast_bin-\xe5\xb0\xbe\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xaf\xe5\x90\xa6\xe4\xba\x8c\xe5\x80\xbc(\xe4\xba\x8c\xe5\x80\xbc:last_relu=0,last_bin=1)\n    def __init__(self, input_channels, output_channels,\n            kernel_size=-1, stride=-1, padding=-1, groups=1, channel_shuffle=0, shuffle_groups=1, A=2, W=2, last_relu=0, last_bin=0):\n        super(Tnn_Bin_Conv2d, self).__init__()\n        self.channel_shuffle_flag = channel_shuffle\n        self.shuffle_groups = shuffle_groups\n        self.last_relu = last_relu\n        self.last_bin = last_bin\n        \n        # ********************* \xe9\x87\x8f\xe5\x8c\x96(\xe4\xb8\x89/\xe4\xba\x8c\xe5\x80\xbc)\xe5\x8d\xb7\xe7\xa7\xaf *********************\n        self.tnn_bin_conv = Conv2d_Q(input_channels, output_channels,\n                kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, A=A, W=W)\n        self.bn = nn.BatchNorm2d(output_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.bin_a = activation_bin(A=A)\n    \n    def forward(self, x):\n        if self.channel_shuffle_flag:\n            x = channel_shuffle(x, groups=self.shuffle_groups)\n        x = self.tnn_bin_conv(x)\n        x = self.bn(x)\n        if self.last_relu:\n            x = self.relu(x)\n        if self.last_bin:\n            x =  self.bin_a(x)\n        return x\n\nclass Net(nn.Module):\n    def __init__(self, cfg = None, A=2, W=2):\n        super(Net, self).__init__()\n        # \xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84\xe4\xb8\x8e\xe6\x90\xad\xe5\xbb\xba\n        if cfg is None:\n            cfg = [256, 256, 256, 512, 512, 512, 1024, 1024]\n        self.tnn_bin = nn.Sequential(\n                nn.Conv2d(3, cfg[0], kernel_size=5, stride=1, padding=2),\n                nn.BatchNorm2d(cfg[0]),\n                Tnn_Bin_Conv2d(cfg[0], cfg[1], kernel_size=1, stride=1, padding=0, groups=2, channel_shuffle=0, A=A, W=W),\n                Tnn_Bin_Conv2d(cfg[1], cfg[2], kernel_size=1, stride=1, padding=0, groups=2, channel_shuffle=1, shuffle_groups=2, A=A, W=W),\n                nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n\n                Tnn_Bin_Conv2d(cfg[2], cfg[3], kernel_size=3, stride=1, padding=1, groups=16, channel_shuffle=1, shuffle_groups=2, A=A, W=W),\n                Tnn_Bin_Conv2d(cfg[3], cfg[4], kernel_size=1, stride=1, padding=0, groups=4, channel_shuffle=1, shuffle_groups=16, A=A, W=W),\n                Tnn_Bin_Conv2d(cfg[4], cfg[5], kernel_size=1, stride=1, padding=0, groups=4, channel_shuffle=1, shuffle_groups=4, A=A, W=W),\n                nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n\n                Tnn_Bin_Conv2d(cfg[5], cfg[6], kernel_size=3, stride=1, padding=1, groups=32, channel_shuffle=1, shuffle_groups=4, A=A, W=W),\n                Tnn_Bin_Conv2d(cfg[6], cfg[7], kernel_size=1, stride=1, padding=0, groups=8, channel_shuffle=1, shuffle_groups=32, A=A, W=W, last_relu=0, last_bin=1),#\xe4\xba\x8c\xe5\x80\xbc\xe9\x87\x8f\xe5\x8c\x96:last_relu=0, last_bin=1;\xe5\x85\xa8\xe7\xb2\xbe\xe5\xba\xa6:last_relu=1, last_bin=0\n                nn.Conv2d(cfg[7],  10, kernel_size=1, stride=1, padding=0),\n                nn.BatchNorm2d(10),\n                nn.ReLU(inplace=True),\n                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n                )\n    # \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x90\xe8\xa1\x8c\n    def forward(self, x):\n        x = self.tnn_bin(x)\n        x = x.view(x.size(0), -1)\n        return x\n'"
quantization/WbWtAb/bn_folding/util_wt_bab.py,16,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Function\n\n# ********************* \xe4\xba\x8c\xe5\x80\xbc(+-1) ***********************\nclass Binary(Function):\n\n    @staticmethod\n    def forward(self, input):\n        self.save_for_backward(input)\n        output = torch.sign(input)\n        return output\n\n    @staticmethod\n    def backward(self, grad_output):\n        input, = self.saved_tensors\n        #*******************ste*********************\n        grad_input = grad_output.clone()\n        #****************saturate_ste***************\n        grad_input[input.ge(1)] = 0\n        grad_input[input.le(-1)] = 0\n        '''\n        #******************soft_ste*****************\n        size = input.size()\n        zeros = torch.zeros(size).cuda()\n        grad = torch.max(zeros, 1 - torch.abs(input))\n        #print(grad)\n        grad_input = grad_output * grad\n        '''\n        return grad_input\n# ********************* \xe4\xb8\x89\xe5\x80\xbc(+-1\xe3\x80\x810) ***********************\nclass Ternary(Function):\n\n    @staticmethod\n    def forward(self, input):\n        # **************** channel\xe7\xba\xa7 - E(|W|) ****************\n        E = torch.mean(torch.abs(input), (3, 2, 1), keepdim=True)\n        # **************** \xe9\x98\x88\xe5\x80\xbc ****************\n        threshold = E * 0.7\n        # ************** W \xe2\x80\x94\xe2\x80\x94 +-1\xe3\x80\x810 **************\n        output = torch.sign(torch.add(torch.sign(torch.add(input, threshold)),torch.sign(torch.add(input, -threshold))))\n        return output, threshold\n\n    @staticmethod\n    def backward(self, grad_output, grad_threshold):\n        #*******************ste*********************\n        grad_input = grad_output.clone()\n        return grad_input\n\n# ********************* A(\xe7\x89\xb9\xe5\xbe\x81)\xe9\x87\x8f\xe5\x8c\x96(\xe4\xba\x8c\xe5\x80\xbc) ***********************\nclass activation_bin(nn.Module):\n  def __init__(self, A):\n    super().__init__()\n    self.A = A\n    self.relu = nn.ReLU(inplace=True)\n\n  def binary(self, input):\n    output = Binary.apply(input)\n    return output\n\n  def forward(self, input):\n    if self.A == 2:\n      output = self.binary(input)\n      # ******************** A \xe2\x80\x94\xe2\x80\x94 1\xe3\x80\x810 *********************\n      #a = torch.clamp(a, min=0)\n    else:\n      output = self.relu(input)\n    return output\n# ********************* W(\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0)\xe9\x87\x8f\xe5\x8c\x96(\xe4\xb8\x89/\xe4\xba\x8c\xe5\x80\xbc) ***********************\ndef meancenter_clampConvParams(w):\n    mean = torch.mean(w, 1, keepdim=True)\n    w = torch.sub(w, mean)# W\xe4\xb8\xad\xe5\xbf\x83\xe5\x8c\x96\n    w = torch.clamp(w, min=-1.0, max=1.0)# W\xe6\x88\xaa\xe6\x96\xad\n    return w\nclass weight_tnn_bin(nn.Module):\n  def __init__(self, W):\n    super().__init__()\n    self.W = W\n\n  def binary(self, input):\n    output = Binary.apply(input)\n    return output\n\n  def ternary(self, input):\n    output = Ternary.apply(input)\n    return output\n\n  def forward(self, input):\n    if self.W == 2 or self.W == 3:\n        # **************************************** W\xe4\xba\x8c\xe5\x80\xbc *****************************************\n        if self.W == 2:\n            output = meancenter_clampConvParams(input)# W\xe4\xb8\xad\xe5\xbf\x83\xe5\x8c\x96+\xe6\x88\xaa\xe6\x96\xad\n            # **************** channel\xe7\xba\xa7 - E(|W|) ****************\n            E = torch.mean(torch.abs(output), (3, 2, 1), keepdim=True)\n            # **************** \xce\xb1(\xe7\xbc\xa9\xe6\x94\xbe\xe5\x9b\xa0\xe5\xad\x90) ****************\n            alpha = E\n            # ************** W \xe2\x80\x94\xe2\x80\x94 +-1 **************\n            output = self.binary(output)\n            # ************** W * \xce\xb1 **************\n            output = output * alpha # \xe8\x8b\xa5\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xce\xb1(\xe7\xbc\xa9\xe6\x94\xbe\xe5\x9b\xa0\xe5\xad\x90)\xef\xbc\x8c\xe6\xb3\xa8\xe9\x87\x8a\xe6\x8e\x89\xe5\x8d\xb3\xe5\x8f\xaf\n            # **************************************** W\xe4\xb8\x89\xe5\x80\xbc *****************************************\n        elif self.W == 3:\n            output_fp = input.clone()\n            # ************** W \xe2\x80\x94\xe2\x80\x94 +-1\xe3\x80\x810 **************\n            output, threshold = self.ternary(input)\n            # **************** \xce\xb1(\xe7\xbc\xa9\xe6\x94\xbe\xe5\x9b\xa0\xe5\xad\x90) ****************\n            output_abs = torch.abs(output_fp)\n            mask_le = output_abs.le(threshold)\n            mask_gt = output_abs.gt(threshold)\n            output_abs[mask_le] = 0\n            output_abs_th = output_abs.clone()\n            output_abs_th_sum = torch.sum(output_abs_th, (3, 2, 1), keepdim=True)\n            mask_gt_sum = torch.sum(mask_gt, (3, 2, 1), keepdim=True).float()\n            alpha = output_abs_th_sum / mask_gt_sum # \xce\xb1(\xe7\xbc\xa9\xe6\x94\xbe\xe5\x9b\xa0\xe5\xad\x90)\n            # *************** W * \xce\xb1 ****************\n            output = output * alpha # \xe8\x8b\xa5\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xce\xb1(\xe7\xbc\xa9\xe6\x94\xbe\xe5\x9b\xa0\xe5\xad\x90)\xef\xbc\x8c\xe6\xb3\xa8\xe9\x87\x8a\xe6\x8e\x89\xe5\x8d\xb3\xe5\x8f\xaf\n    else:\n      output = input\n    return output\n\n# ********************* \xe9\x87\x8f\xe5\x8c\x96\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x88\xe5\x90\x8c\xe6\x97\xb6\xe9\x87\x8f\xe5\x8c\x96A/W\xef\xbc\x8c\xe5\xb9\xb6\xe5\x81\x9a\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x89 ***********************\nclass Conv2d_Q(nn.Conv2d):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        dilation=1,\n        groups=1,\n        bias=True,\n        A=2,\n        W=2\n      ):\n        super().__init__(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias\n        )\n        # \xe5\xae\x9e\xe4\xbe\x8b\xe5\x8c\x96\xe8\xb0\x83\xe7\x94\xa8A\xe5\x92\x8cW\xe9\x87\x8f\xe5\x8c\x96\xe5\x99\xa8\n        self.activation_quantizer = activation_bin(A=A)\n        self.weight_quantizer = weight_tnn_bin(W=W)\n          \n    def forward(self, input):\n        # \xe9\x87\x8f\xe5\x8c\x96A\xe5\x92\x8cW\n        bin_input = self.activation_quantizer(input)\n        tnn_bin_weight = self.weight_quantizer(self.weight)    \n        #print(bin_input)\n        #print(tnn_bin_weight)\n        # \xe7\x94\xa8\xe9\x87\x8f\xe5\x8c\x96\xe5\x90\x8e\xe7\x9a\x84A\xe5\x92\x8cW\xe5\x81\x9a\xe5\x8d\xb7\xe7\xa7\xaf\n        output = F.conv2d(\n            input=bin_input, \n            weight=tnn_bin_weight, \n            bias=self.bias, \n            stride=self.stride, \n            padding=self.padding, \n            dilation=self.dilation, \n            groups=self.groups)\n        return output\n"""
quantization/WbWtAb/models/__init__.py,0,b''
quantization/WbWtAb/models/nin.py,1,"b'import torch.nn as nn\nfrom .util_wt_bab import Conv2d_Q\n\n# *********************\xe9\x87\x8f\xe5\x8c\x96(\xe4\xb8\x89\xe5\x80\xbc\xe3\x80\x81\xe4\xba\x8c\xe5\x80\xbc)\xe5\x8d\xb7\xe7\xa7\xaf*********************\nclass Tnn_Bin_Conv2d(nn.Module):\n    # \xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x9alast_relu-\xe5\xb0\xbe\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe8\xbe\x93\xe5\x85\xa5\xe6\xbf\x80\xe6\xb4\xbb\n    def __init__(self, input_channels, output_channels,\n            kernel_size=-1, stride=-1, padding=-1, groups=1, last_relu=0, A=2, W=2):\n        super(Tnn_Bin_Conv2d, self).__init__()\n        self.A = A\n        self.W = W\n        self.last_relu = last_relu\n\n        # ********************* \xe9\x87\x8f\xe5\x8c\x96(\xe4\xb8\x89/\xe4\xba\x8c\xe5\x80\xbc)\xe5\x8d\xb7\xe7\xa7\xaf *********************\n        self.tnn_bin_conv = Conv2d_Q(input_channels, output_channels,\n                kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, A=A, W=W)\n        self.bn = nn.BatchNorm2d(output_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.tnn_bin_conv(x)\n        x = self.bn(x)\n        if self.last_relu:\n            x = self.relu(x)\n        return x\n\nclass Net(nn.Module):\n    def __init__(self, cfg = None, A=2, W=2):\n        super(Net, self).__init__()\n        # \xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84\xe4\xb8\x8e\xe6\x90\xad\xe5\xbb\xba\n        if cfg is None:\n            cfg = [192, 160, 96, 192, 192, 192, 192, 192]\n        self.tnn_bin = nn.Sequential(\n                nn.Conv2d(3, cfg[0], kernel_size=5, stride=1, padding=2),\n                nn.BatchNorm2d(cfg[0]),\n                Tnn_Bin_Conv2d(cfg[0], cfg[1], kernel_size=1, stride=1, padding=0, A=A, W=W),\n                Tnn_Bin_Conv2d(cfg[1], cfg[2], kernel_size=1, stride=1, padding=0, A=A, W=W),\n                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n\n                Tnn_Bin_Conv2d(cfg[2], cfg[3], kernel_size=5, stride=1, padding=2, A=A, W=W),\n                Tnn_Bin_Conv2d(cfg[3], cfg[4], kernel_size=1, stride=1, padding=0, A=A, W=W),\n                Tnn_Bin_Conv2d(cfg[4], cfg[5], kernel_size=1, stride=1, padding=0, A=A, W=W),\n                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n\n                Tnn_Bin_Conv2d(cfg[5], cfg[6], kernel_size=3, stride=1, padding=1, A=A, W=W),\n                Tnn_Bin_Conv2d(cfg[6], cfg[7], kernel_size=1, stride=1, padding=0, last_relu=1, A=A, W=W),\n                nn.Conv2d(cfg[7],  10, kernel_size=1, stride=1, padding=0),\n                nn.BatchNorm2d(10),\n                nn.ReLU(inplace=True),\n                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n                )\n\n    def forward(self, x):\n        x = self.tnn_bin(x)\n        x = x.view(x.size(0), -1)\n        return x\n'"
quantization/WbWtAb/models/nin_bn_conv.py,7,"b""import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\n# *********************A\xef\xbc\x88\xe7\x89\xb9\xe5\xbe\x81\xef\xbc\x89\xe9\x87\x8f\xe5\x8c\x96\xef\xbc\x88\xe4\xba\x8c\xe5\x80\xbc)***********************\nclass BinActive(torch.autograd.Function):\n\n    def forward(self, input):\n        self.save_for_backward(input)\n        size = input.size()\n        mean = torch.mean(input.abs(), 1, keepdim=True)\n        output = input.sign()\n        # ********************A\xe4\xba\x8c\xe5\x80\xbc\xe2\x80\x94\xe2\x80\x941\xe3\x80\x810*********************\n        #input = torch.clamp(input, min=0)\n        #print(input)\n        return output, mean\n\n    def backward(self, grad_output, grad_output_mean):\n        input, = self.saved_tensors\n        #*******************ste*********************\n        grad_input = grad_output.clone()\n        #****************saturate_ste***************\n        grad_input[input.ge(1)] = 0\n        grad_input[input.le(-1)] = 0\n        '''\n        #******************soft_ste*****************\n        size = input.size()\n        zeros = torch.zeros(size).cuda()\n        grad = torch.max(zeros, 1 - torch.abs(input))\n        #print(grad)\n        grad_input = grad_output * grad\n        '''\n        return grad_input\n\nclass Tnn_Bin_Conv2d(nn.Module):\n    def __init__(self, input_channels, output_channels,\n            kernel_size=-1, stride=-1, padding=-1, dropout=0):\n        super(Tnn_Bin_Conv2d, self).__init__()\n        self.layer_type = 'Tnn_Bin_Conv2d'\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dropout_ratio = dropout\n\n        self.bn = nn.BatchNorm2d(input_channels, eps=1e-4, momentum=0.1, affine=True)\n        self.bn.weight.data = self.bn.weight.data.zero_().add(1.0)\n        if dropout!=0:\n            self.dropout = nn.Dropout(dropout)\n        self.conv = nn.Conv2d(input_channels, output_channels,\n                kernel_size=kernel_size, stride=stride, padding=padding)\n        self.relu = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.bn(x)\n        x, mean = BinActive()(x)\n        if self.dropout_ratio!=0:\n            x = self.dropout(x)\n        x = self.conv(x)\n        x = self.relu(x)\n        return x\n\nclass Net(nn.Module):\n    def __init__(self, cfg = None):\n        super(Net, self).__init__()\n        if cfg is None:\n            # \xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84\n            cfg = [192, 160, 96, 192, 192, 192, 192, 192]\n        self.tnn_bin = nn.Sequential(\n                nn.Conv2d(3, cfg[0], kernel_size=5, stride=1, padding=2),\n                nn.BatchNorm2d(cfg[0], eps=1e-4, momentum=0.1, affine=False),\n                nn.ReLU(inplace=True),\n                Tnn_Bin_Conv2d(cfg[0], cfg[1], kernel_size=1, stride=1, padding=0),\n                Tnn_Bin_Conv2d(cfg[1],  cfg[2], kernel_size=1, stride=1, padding=0),\n                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n\n                Tnn_Bin_Conv2d(cfg[2], cfg[3], kernel_size=5, stride=1, padding=2, dropout=0.5),\n                Tnn_Bin_Conv2d(cfg[3], cfg[4], kernel_size=1, stride=1, padding=0),\n                Tnn_Bin_Conv2d(cfg[4], cfg[5], kernel_size=1, stride=1, padding=0),\n                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n\n                Tnn_Bin_Conv2d(cfg[5], cfg[6], kernel_size=3, stride=1, padding=1, dropout=0.5),\n                Tnn_Bin_Conv2d(cfg[6], cfg[7], kernel_size=1, stride=1, padding=0),\n                nn.BatchNorm2d(cfg[7], eps=1e-4, momentum=0.1, affine=False),\n                nn.Conv2d(cfg[7],  10, kernel_size=1, stride=1, padding=0),\n                nn.ReLU(inplace=True),\n                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n                )\n\n    def forward(self, x):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n                if hasattr(m.weight, 'data'):\n                    m.weight.data.clamp_(min=0.01)\n        x = self.tnn_bin(x)\n        x = x.view(x.size(0), 10)\n        return x\n"""
quantization/WbWtAb/models/nin_gc.py,1,"b'import torch.nn as nn\nfrom .util_wt_bab import activation_bin, Conv2d_Q\n\n# \xe9\x80\x9a\xe9\x81\x93\xe6\xb7\xb7\xe5\x90\x88\ndef channel_shuffle(x, groups):\n    """"""shuffle channels of a 4-D Tensor""""""\n    batch_size, channels, height, width = x.size()\n    assert channels % groups == 0\n    channels_per_group = channels // groups\n    # split into groups\n    x = x.view(batch_size, groups, channels_per_group, height, width)\n    # transpose 1, 2 axis\n    x = x.transpose(1, 2).contiguous()\n    # reshape into orignal\n    x = x.view(batch_size, channels, height, width)\n    return x\n\n# ********************* \xe9\x87\x8f\xe5\x8c\x96(\xe4\xb8\x89/\xe4\xba\x8c\xe5\x80\xbc)\xe6\xa8\xa1\xe5\x9d\x97 ************************\nclass Tnn_Bin_Conv2d(nn.Module):\n    # \xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x9agroups-\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x88\x86\xe7\xbb\x84\xe6\x95\xb0\xe3\x80\x81channel_shuffle-\xe9\x80\x9a\xe9\x81\x93\xe6\xb7\xb7\xe5\x90\x88\xe6\xa0\x87\xe5\xbf\x97\xe3\x80\x81shuffle_groups-\xe9\x80\x9a\xe9\x81\x93\xe6\xb7\xb7\xe5\x90\x88\xe6\x95\xb0\xef\xbc\x88\xe6\x9c\xac\xe5\xb1\x82\xe9\x9c\x80\xe4\xb8\x8e\xe4\xb8\x8a\xe4\xb8\x80\xe5\xb1\x82\xe5\x88\x86\xe7\xbb\x84\xe6\x95\xb0\xe4\xbf\x9d\xe6\x8c\x81\xe4\xb8\x80\xe8\x87\xb4)\xe3\x80\x81last_relu\xef\xbd\x9clast_bin-\xe5\xb0\xbe\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xaf\xe5\x90\xa6\xe4\xba\x8c\xe5\x80\xbc(\xe4\xba\x8c\xe5\x80\xbc:last_relu=0,last_bin=1)\n    def __init__(self, input_channels, output_channels,\n            kernel_size=-1, stride=-1, padding=-1, groups=1, channel_shuffle=0, shuffle_groups=1, A=2, W=2, last_relu=0, last_bin=0):\n        super(Tnn_Bin_Conv2d, self).__init__()\n        self.channel_shuffle_flag = channel_shuffle\n        self.shuffle_groups = shuffle_groups\n        self.last_relu = last_relu\n        self.last_bin = last_bin\n        \n        # ********************* \xe9\x87\x8f\xe5\x8c\x96(\xe4\xb8\x89/\xe4\xba\x8c\xe5\x80\xbc)\xe5\x8d\xb7\xe7\xa7\xaf *********************\n        self.tnn_bin_conv = Conv2d_Q(input_channels, output_channels,\n                kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, A=A, W=W)\n        self.bn = nn.BatchNorm2d(output_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.bin_a = activation_bin(A=A)\n    \n    def forward(self, x):\n        if self.channel_shuffle_flag:\n            x = channel_shuffle(x, groups=self.shuffle_groups)\n        x = self.tnn_bin_conv(x)\n        x = self.bn(x)\n        if self.last_relu:\n            x = self.relu(x)\n        if self.last_bin:\n            x =  self.bin_a(x)\n        return x\n\nclass Net(nn.Module):\n    def __init__(self, cfg = None, A=2, W=2):\n        super(Net, self).__init__()\n        # \xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84\xe4\xb8\x8e\xe6\x90\xad\xe5\xbb\xba\n        if cfg is None:\n            cfg = [256, 256, 256, 512, 512, 512, 1024, 1024]\n        self.tnn_bin = nn.Sequential(\n                nn.Conv2d(3, cfg[0], kernel_size=5, stride=1, padding=2),\n                nn.BatchNorm2d(cfg[0]),\n                Tnn_Bin_Conv2d(cfg[0], cfg[1], kernel_size=1, stride=1, padding=0, groups=2, channel_shuffle=0, A=A, W=W),\n                Tnn_Bin_Conv2d(cfg[1], cfg[2], kernel_size=1, stride=1, padding=0, groups=2, channel_shuffle=1, shuffle_groups=2, A=A, W=W),\n                nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n\n                Tnn_Bin_Conv2d(cfg[2], cfg[3], kernel_size=3, stride=1, padding=1, groups=16, channel_shuffle=1, shuffle_groups=2, A=A, W=W),\n                Tnn_Bin_Conv2d(cfg[3], cfg[4], kernel_size=1, stride=1, padding=0, groups=4, channel_shuffle=1, shuffle_groups=16, A=A, W=W),\n                Tnn_Bin_Conv2d(cfg[4], cfg[5], kernel_size=1, stride=1, padding=0, groups=4, channel_shuffle=1, shuffle_groups=4, A=A, W=W),\n                nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n\n                Tnn_Bin_Conv2d(cfg[5], cfg[6], kernel_size=3, stride=1, padding=1, groups=32, channel_shuffle=1, shuffle_groups=4, A=A, W=W),\n                Tnn_Bin_Conv2d(cfg[6], cfg[7], kernel_size=1, stride=1, padding=0, groups=8, channel_shuffle=1, shuffle_groups=32, A=A, W=W, last_relu=0, last_bin=1),#\xe4\xba\x8c\xe5\x80\xbc\xe9\x87\x8f\xe5\x8c\x96:last_relu=0, last_bin=1;\xe5\x85\xa8\xe7\xb2\xbe\xe5\xba\xa6:last_relu=1, last_bin=0\n                nn.Conv2d(cfg[7],  10, kernel_size=1, stride=1, padding=0),\n                nn.BatchNorm2d(10),\n                nn.ReLU(inplace=True),\n                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n                )\n    # \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x90\xe8\xa1\x8c\n    def forward(self, x):\n        x = self.tnn_bin(x)\n        x = x.view(x.size(0), -1)\n        return x\n'"
quantization/WbWtAb/models/util_wt_bab.py,14,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Function\n\n# ********************* \xe4\xba\x8c\xe5\x80\xbc(+-1) ***********************\n# A\nclass Binary_a(Function):\n\n    @staticmethod\n    def forward(self, input):\n        self.save_for_backward(input)\n        output = torch.sign(input)\n        return output\n\n    @staticmethod\n    def backward(self, grad_output):\n        input, = self.saved_tensors\n        #*******************ste*********************\n        grad_input = grad_output.clone()\n        #****************saturate_ste***************\n        grad_input[input.ge(1)] = 0\n        grad_input[input.le(-1)] = 0\n        '''\n        #******************soft_ste*****************\n        size = input.size()\n        zeros = torch.zeros(size).cuda()\n        grad = torch.max(zeros, 1 - torch.abs(input))\n        #print(grad)\n        grad_input = grad_output * grad\n        '''\n        return grad_input\n# W\nclass Binary_w(Function):\n\n    @staticmethod\n    def forward(self, input):\n        output = torch.sign(input)\n        return output\n\n    @staticmethod\n    def backward(self, grad_output):\n        #*******************ste*********************\n        grad_input = grad_output.clone()\n        return grad_input\n# ********************* \xe4\xb8\x89\xe5\x80\xbc(+-1\xe3\x80\x810) ***********************\nclass Ternary(Function):\n\n    @staticmethod\n    def forward(self, input):\n        # **************** channel\xe7\xba\xa7 - E(|W|) ****************\n        E = torch.mean(torch.abs(input), (3, 2, 1), keepdim=True)\n        # **************** \xe9\x98\x88\xe5\x80\xbc ****************\n        threshold = E * 0.7\n        # ************** W \xe2\x80\x94\xe2\x80\x94 +-1\xe3\x80\x810 **************\n        output = torch.sign(torch.add(torch.sign(torch.add(input, threshold)),torch.sign(torch.add(input, -threshold))))\n        return output, threshold\n\n    @staticmethod\n    def backward(self, grad_output, grad_threshold):\n        #*******************ste*********************\n        grad_input = grad_output.clone()\n        return grad_input\n\n# ********************* A(\xe7\x89\xb9\xe5\xbe\x81)\xe9\x87\x8f\xe5\x8c\x96(\xe4\xba\x8c\xe5\x80\xbc) ***********************\nclass activation_bin(nn.Module):\n  def __init__(self, A):\n    super().__init__()\n    self.A = A\n    self.relu = nn.ReLU(inplace=True)\n\n  def binary(self, input):\n    output = Binary_a.apply(input)\n    return output\n\n  def forward(self, input):\n    if self.A == 2:\n      output = self.binary(input)\n      # ******************** A \xe2\x80\x94\xe2\x80\x94 1\xe3\x80\x810 *********************\n      #a = torch.clamp(a, min=0)\n    else:\n      output = self.relu(input)\n    return output\n# ********************* W(\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0)\xe9\x87\x8f\xe5\x8c\x96(\xe4\xb8\x89/\xe4\xba\x8c\xe5\x80\xbc) ***********************\ndef meancenter_clampConvParams(w):\n    mean = w.data.mean(1, keepdim=True)\n    w.data.sub(mean) # W\xe4\xb8\xad\xe5\xbf\x83\xe5\x8c\x96(C\xe6\x96\xb9\xe5\x90\x91)\n    w.data.clamp(-1.0, 1.0) # W\xe6\x88\xaa\xe6\x96\xad\n    return w\nclass weight_tnn_bin(nn.Module):\n  def __init__(self, W):\n    super().__init__()\n    self.W = W\n\n  def binary(self, input):\n    output = Binary_w.apply(input)\n    return output\n\n  def ternary(self, input):\n    output = Ternary.apply(input)\n    return output\n\n  def forward(self, input):\n    if self.W == 2 or self.W == 3:\n        # **************************************** W\xe4\xba\x8c\xe5\x80\xbc *****************************************\n        if self.W == 2:\n            output = meancenter_clampConvParams(input) # W\xe4\xb8\xad\xe5\xbf\x83\xe5\x8c\x96+\xe6\x88\xaa\xe6\x96\xad\n            # **************** channel\xe7\xba\xa7 - E(|W|) ****************\n            E = torch.mean(torch.abs(output), (3, 2, 1), keepdim=True)\n            # **************** \xce\xb1(\xe7\xbc\xa9\xe6\x94\xbe\xe5\x9b\xa0\xe5\xad\x90) ****************\n            alpha = E\n            # ************** W \xe2\x80\x94\xe2\x80\x94 +-1 **************\n            output = self.binary(output)\n            # ************** W * \xce\xb1 **************\n            output = output * alpha # \xe8\x8b\xa5\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xce\xb1(\xe7\xbc\xa9\xe6\x94\xbe\xe5\x9b\xa0\xe5\xad\x90)\xef\xbc\x8c\xe6\xb3\xa8\xe9\x87\x8a\xe6\x8e\x89\xe5\x8d\xb3\xe5\x8f\xaf\n            # **************************************** W\xe4\xb8\x89\xe5\x80\xbc *****************************************\n        elif self.W == 3:\n            output_fp = input.clone()\n            # ************** W \xe2\x80\x94\xe2\x80\x94 +-1\xe3\x80\x810 **************\n            output, threshold = self.ternary(input)\n            # **************** \xce\xb1(\xe7\xbc\xa9\xe6\x94\xbe\xe5\x9b\xa0\xe5\xad\x90) ****************\n            output_abs = torch.abs(output_fp)\n            mask_le = output_abs.le(threshold)\n            mask_gt = output_abs.gt(threshold)\n            output_abs[mask_le] = 0\n            output_abs_th = output_abs.clone()\n            output_abs_th_sum = torch.sum(output_abs_th, (3, 2, 1), keepdim=True)\n            mask_gt_sum = torch.sum(mask_gt, (3, 2, 1), keepdim=True).float()\n            alpha = output_abs_th_sum / mask_gt_sum # \xce\xb1(\xe7\xbc\xa9\xe6\x94\xbe\xe5\x9b\xa0\xe5\xad\x90)\n            # *************** W * \xce\xb1 ****************\n            output = output * alpha # \xe8\x8b\xa5\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xce\xb1(\xe7\xbc\xa9\xe6\x94\xbe\xe5\x9b\xa0\xe5\xad\x90)\xef\xbc\x8c\xe6\xb3\xa8\xe9\x87\x8a\xe6\x8e\x89\xe5\x8d\xb3\xe5\x8f\xaf\n    else:\n      output = input\n    return output\n\n# ********************* \xe9\x87\x8f\xe5\x8c\x96\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x88\xe5\x90\x8c\xe6\x97\xb6\xe9\x87\x8f\xe5\x8c\x96A/W\xef\xbc\x8c\xe5\xb9\xb6\xe5\x81\x9a\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x89 ***********************\nclass Conv2d_Q(nn.Conv2d):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        dilation=1,\n        groups=1,\n        bias=True,\n        A=2,\n        W=2\n      ):\n        super().__init__(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias\n        )\n        # \xe5\xae\x9e\xe4\xbe\x8b\xe5\x8c\x96\xe8\xb0\x83\xe7\x94\xa8A\xe5\x92\x8cW\xe9\x87\x8f\xe5\x8c\x96\xe5\x99\xa8\n        self.activation_quantizer = activation_bin(A=A)\n        self.weight_quantizer = weight_tnn_bin(W=W)\n          \n    def forward(self, input):\n        # \xe9\x87\x8f\xe5\x8c\x96A\xe5\x92\x8cW\n        bin_input = self.activation_quantizer(input)\n        tnn_bin_weight = self.weight_quantizer(self.weight)    \n        #print(bin_input)\n        #print(tnn_bin_weight)\n        # \xe7\x94\xa8\xe9\x87\x8f\xe5\x8c\x96\xe5\x90\x8e\xe7\x9a\x84A\xe5\x92\x8cW\xe5\x81\x9a\xe5\x8d\xb7\xe7\xa7\xaf\n        output = F.conv2d(\n            input=bin_input, \n            weight=tnn_bin_weight, \n            bias=self.bias, \n            stride=self.stride, \n            padding=self.padding, \n            dilation=self.dilation, \n            groups=self.groups)\n        return output\n"""
quantization/WqAq/IAO/main.py,17,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport math\nimport argparse\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torchvision\nimport torchvision.transforms as transforms\nfrom models import nin_gc\nfrom models import nin\nimport os\n\ndef setup_seed(seed):\n    torch.manual_seed(seed)                    \n    #torch.cuda.manual_seed(seed)              \n    torch.cuda.manual_seed_all(seed)           \n    np.random.seed(seed)                       \n    torch.backends.cudnn.deterministic = True\n\ndef save_state(model, best_acc):\n    print(\'==> Saving model ...\')\n    state = {\n            \'best_acc\': best_acc,\n            \'state_dict\': model.state_dict(),\n            }\n    state_copy = state[\'state_dict\'].copy()\n    for key in state_copy.keys():\n        if \'module\' in key:\n            state[\'state_dict\'][key.replace(\'module.\', \'\')] = \\\n                    state[\'state_dict\'].pop(key)\n    if args.model_type == 0:\n        torch.save(state, \'models_save/nin.pth\')\n    else:\n        if args.bn_fold == 1:\n            torch.save(state, \'models_save/nin_gc_bn_fold.pth\')\n        else:\n            torch.save(state, \'models_save/nin_gc.pth\')\n    \ndef adjust_learning_rate(optimizer, epoch):\n    if args.bn_fold == 1:\n        if args.model_type == 0:\n            update_list = [12, 15, 25]\n        else:\n            update_list = [8, 12, 20, 25]\n    else:\n        update_list = [15, 17, 20]\n    if epoch in update_list:\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = param_group[\'lr\'] * 0.1\n    return\n\ndef train(epoch):\n    model.train()\n\n    for batch_idx, (data, target) in enumerate(trainloader):\n        if not args.cpu:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        output = model(data)\n        loss = criterion(output, target)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step() \n\n        if batch_idx % 100 == 0:\n            print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR: {}\'.format(\n                epoch, batch_idx * len(data), len(trainloader.dataset),\n                100. * batch_idx / len(trainloader), loss.data.item(),\n                optimizer.param_groups[0][\'lr\']))\n    return\n\ndef test():\n    global best_acc\n    model.eval()\n    test_loss = 0\n    average_test_loss = 0\n    correct = 0\n\n    for data, target in testloader:\n        if not args.cpu:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        output = model(data)\n        test_loss += criterion(output, target).data.item()\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n    acc = 100. * float(correct) / len(testloader.dataset)\n\n    if acc > best_acc:\n        best_acc = acc\n        save_state(model, best_acc)\n    average_test_loss = test_loss / (len(testloader.dataset) / args.eval_batch_size)\n\n    print(\'\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\'.format(\n        average_test_loss, correct, len(testloader.dataset),\n        100. * float(correct) / len(testloader.dataset)))\n\n    print(\'Best Accuracy: {:.2f}%\\n\'.format(best_acc))\n    return\n\nif __name__==\'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--cpu\', action=\'store_true\',\n            help=\'set if only CPU is available\')\n    # gpu_id\n    parser.add_argument(\'--gpu_id\', action=\'store\', default=\'\',\n            help=\'gpu_id\')\n    parser.add_argument(\'--data\', action=\'store\', default=\'../../../data\',\n            help=\'dataset path\')\n    parser.add_argument(\'--lr\', action=\'store\', default=0.01,\n            help=\'the intial learning rate\')\n    parser.add_argument(\'--wd\', action=\'store\', default=1e-5,\n            help=\'the intial learning rate\')\n    parser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n            help=\'the path to the resume model\')\n    parser.add_argument(\'--refine\', default=\'\', type=str, metavar=\'PATH\',\n            help=\'the path to the refine(prune) model\')\n    parser.add_argument(\'--evaluate\', action=\'store_true\',\n            help=\'evaluate the model\')\n    parser.add_argument(\'--train_batch_size\', type=int, default=512)\n    parser.add_argument(\'--eval_batch_size\', type=int, default=256)\n    parser.add_argument(\'--num_workers\', type=int, default=2)\n    parser.add_argument(\'--start_epochs\', type=int, default=1, metavar=\'N\',\n            help=\'number of epochs to train_start\')\n    parser.add_argument(\'--end_epochs\', type=int, default=30, metavar=\'N\',\n            help=\'number of epochs to train_end\')\n    # W/A \xe2\x80\x94 bits\n    parser.add_argument(\'--Wbits\', type=int, default=8)\n    parser.add_argument(\'--Abits\', type=int, default=8)\n    # bn\xe8\x9e\x8d\xe5\x90\x88\xe6\xa0\x87\xe5\xbf\x97\xe4\xbd\x8d\n    parser.add_argument(\'--bn_fold\', type=int, default=0,\n            help=\'bn_fold:1\')\n    # \xe9\x87\x8f\xe5\x8c\x96\xe6\x96\xb9\xe6\xb3\x95\xe9\x80\x89\xe6\x8b\xa9\n    parser.add_argument(\'--q_type\', type=int, default=1,\n            help=\'quantization type:0-symmetric,1-asymmetric\')\n    # \xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84\xe9\x80\x89\xe6\x8b\xa9\n    parser.add_argument(\'--model_type\', type=int, default=1,\n            help=\'model type:0-nin,1-nin_gc\')\n    args = parser.parse_args()\n    print(\'==> Options:\',args)\n\n    if args.gpu_id:\n        os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu_id\n\n    setup_seed(1)\n\n    print(\'==> Preparing data..\')\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n\n    trainset = torchvision.datasets.CIFAR10(root = args.data, train = True, download = True, transform = transform_train)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.train_batch_size, shuffle=True, num_workers=args.num_workers) # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\x95\xb0\xe6\x8d\xae\n\n    testset = torchvision.datasets.CIFAR10(root = args.data, train = False, download = True, transform = transform_test)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=args.eval_batch_size, shuffle=False, num_workers=args.num_workers) # \xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe6\x95\xb0\xe6\x8d\xae\n\n    classes = (\'plane\', \'car\', \'bird\', \'cat\', \'deer\', \'dog\', \'frog\', \'horse\', \'ship\', \'truck\')\n\n    if args.refine:\n        print(\'******Refine model******\')\n        #checkpoint = torch.load(\'../prune/models_save/nin_refine.pth\')\n        checkpoint = torch.load(args.refine)\n        if args.model_type == 0:\n            model = nin.Net(cfg=checkpoint[\'cfg\'], abits=args.Abits, wbits=args.Wbits, bn_fold=args.bn_fold, q_type=args.q_type)\n        else:\n            model = nin_gc.Net(cfg=checkpoint[\'cfg\'], abits=args.Abits, wbits=args.Wbits, bn_fold=args.bn_fold, q_type=args.q_type)\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        best_acc = 0\n    else:\n        print(\'******Initializing model******\')\n        if args.model_type == 0:\n            model = nin.Net(abits=args.Abits, wbits=args.Wbits, bn_fold=args.bn_fold, q_type=args.q_type)\n        else:\n            model = nin_gc.Net(abits=args.Abits, wbits=args.Wbits, bn_fold=args.bn_fold, q_type=args.q_type)\n        best_acc = 0\n        for m in model.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight.data)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n    if args.resume:\n        print(\'******Reume model******\')\n        #pretrained_model = torch.load(\'models_save/nin_gc.pth\')\n        pretrained_model = torch.load(args.resume)\n        best_acc = pretrained_model[\'best_acc\']\n        model.load_state_dict(pretrained_model[\'state_dict\'])\n\n    if not args.cpu:\n        model.cuda()\n        model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n    print(model)\n\n    base_lr = float(args.lr)\n    param_dict = dict(model.named_parameters())\n    params = []\n    for key, value in param_dict.items():\n        params += [{\'params\':[value], \'lr\': base_lr, \'weight_decay\':args.wd}]\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(params, lr=base_lr, weight_decay=args.wd)\n\n    if args.evaluate:\n        test()\n        exit(0)\n\n    for epoch in range(args.start_epochs, args.end_epochs):\n        adjust_learning_rate(optimizer, epoch)\n        train(epoch)\n        test()'"
quantization/WqAq/dorefa/main.py,16,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport math\nimport argparse\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torchvision\nimport torchvision.transforms as transforms\nfrom models import nin_gc\nfrom models import nin\nimport os\n\ndef setup_seed(seed):\n    torch.manual_seed(seed)                    \n    #torch.cuda.manual_seed(seed)              \n    torch.cuda.manual_seed_all(seed)           \n    np.random.seed(seed)                       \n    torch.backends.cudnn.deterministic = True\n\ndef save_state(model, best_acc):\n    print(\'==> Saving model ...\')\n    state = {\n            \'best_acc\': best_acc,\n            \'state_dict\': model.state_dict(),\n            }\n    state_copy = state[\'state_dict\'].copy()\n    for key in state_copy.keys():\n        if \'module\' in key:\n            state[\'state_dict\'][key.replace(\'module.\', \'\')] = \\\n                    state[\'state_dict\'].pop(key)\n    if args.model_type == 0:\n        torch.save(state, \'models_save/nin.pth\')\n    else:\n        torch.save(state, \'models_save/nin_gc.pth\')\n\ndef adjust_learning_rate(optimizer, epoch):\n    update_list = [80, 130, 180, 230, 280]\n    if epoch in update_list:\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = param_group[\'lr\'] * 0.1\n    return\n\ndef train(epoch):\n    model.train()\n\n    for batch_idx, (data, target) in enumerate(trainloader):\n        if not args.cpu:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        output = model(data)\n        loss = criterion(output, target)\n        \n        optimizer.zero_grad()\n        loss.backward()  \n        optimizer.step() \n\n        if batch_idx % 100 == 0:\n            print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR: {}\'.format(\n                epoch, batch_idx * len(data), len(trainloader.dataset),\n                100. * batch_idx / len(trainloader), loss.data.item(),\n                optimizer.param_groups[0][\'lr\']))\n    return\n\ndef test():\n    global best_acc\n    model.eval()\n    test_loss = 0\n    average_test_loss = 0\n    correct = 0\n\n    for data, target in testloader:\n        if not args.cpu:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        output = model(data)\n        test_loss += criterion(output, target).data.item()\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n    acc = 100. * float(correct) / len(testloader.dataset)\n\n    if acc > best_acc:\n        best_acc = acc\n        save_state(model, best_acc)\n    average_test_loss = test_loss / (len(testloader.dataset) / args.eval_batch_size)\n\n    print(\'\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\'.format(\n        average_test_loss, correct, len(testloader.dataset),\n        100. * float(correct) / len(testloader.dataset)))\n\n    print(\'Best Accuracy: {:.2f}%\\n\'.format(best_acc))\n    return\n\nif __name__==\'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--cpu\', action=\'store_true\',\n            help=\'set if only CPU is available\')\n    # gpu_id\n    parser.add_argument(\'--gpu_id\', action=\'store\', default=\'\',\n            help=\'gpu_id\')\n    parser.add_argument(\'--data\', action=\'store\', default=\'../../../data\',\n            help=\'dataset path\')\n    parser.add_argument(\'--lr\', action=\'store\', default=0.01,\n            help=\'the intial learning rate\')\n    parser.add_argument(\'--wd\', action=\'store\', default=1e-5,\n            help=\'the intial learning rate\')\n    parser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n            help=\'the path to the resume model\')\n    parser.add_argument(\'--refine\', default=\'\', type=str, metavar=\'PATH\',\n            help=\'the path to the refine(prune) model\')\n    parser.add_argument(\'--evaluate\', action=\'store_true\',\n            help=\'evaluate the model\')\n    parser.add_argument(\'--train_batch_size\', type=int, default=50)\n    parser.add_argument(\'--eval_batch_size\', type=int, default=256)\n    parser.add_argument(\'--num_workers\', type=int, default=2)\n    parser.add_argument(\'--start_epochs\', type=int, default=1, metavar=\'N\',\n            help=\'number of epochs to train_start\')\n    parser.add_argument(\'--end_epochs\', type=int, default=300, metavar=\'N\',\n            help=\'number of epochs to train_end\')\n    # W/A \xe2\x80\x94 bits\n    parser.add_argument(\'--Wbits\', type=int, default=8)\n    parser.add_argument(\'--Abits\', type=int, default=8)\n    # \xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84\xe9\x80\x89\xe6\x8b\xa9\n    parser.add_argument(\'--model_type\', type=int, default=1,\n            help=\'model type:0-nin,1-nin_gc\')\n    args = parser.parse_args()\n    print(\'==> Options:\',args)\n\n    if args.gpu_id:\n        os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu_id\n\n    setup_seed(1)\n\n    print(\'==> Preparing data..\')\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n\n    trainset = torchvision.datasets.CIFAR10(root = args.data, train = True, download = True, transform = transform_train)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.train_batch_size, shuffle=True, num_workers=args.num_workers) # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\x95\xb0\xe6\x8d\xae\n\n    testset = torchvision.datasets.CIFAR10(root = args.data, train = False, download = True, transform = transform_test)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=args.eval_batch_size, shuffle=False, num_workers=args.num_workers) # \xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe6\x95\xb0\xe6\x8d\xae\n\n    classes = (\'plane\', \'car\', \'bird\', \'cat\', \'deer\', \'dog\', \'frog\', \'horse\', \'ship\', \'truck\')\n\n    if args.refine:\n        print(\'******Refine model******\')\n        #checkpoint = torch.load(\'../prune/models_save/nin_refine.pth\')\n        checkpoint = torch.load(args.refine)\n        if args.model_type == 0:\n            model = nin.Net(cfg=checkpoint[\'cfg\'], abits=args.Abits, wbits=args.Wbits)\n        else:\n            model = nin_gc.Net(cfg=checkpoint[\'cfg\'], abits=args.Abits, wbits=args.Wbits)\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        best_acc = 0\n    else:\n        print(\'******Initializing model******\')\n        if args.model_type == 0:\n            model = nin.Net(abits=args.Abits, wbits=args.Wbits)\n        else:\n            model = nin_gc.Net(abits=args.Abits, wbits=args.Wbits)\n        best_acc = 0\n        for m in model.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight.data)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n    if args.resume:\n        print(\'******Reume model******\')\n        #pretrained_model = torch.load(\'models_save/nin_gc.pth\')\n        pretrained_model = torch.load(args.resume)\n        best_acc = pretrained_model[\'best_acc\']\n        model.load_state_dict(pretrained_model[\'state_dict\'])\n\n    if not args.cpu:\n        model.cuda()\n        model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n    print(model)\n\n    base_lr = float(args.lr)\n    param_dict = dict(model.named_parameters())\n    params = []\n    for key, value in param_dict.items():\n        params += [{\'params\':[value], \'lr\': base_lr, \'weight_decay\':args.wd}]\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(params, lr=base_lr, weight_decay=args.wd)\n\n    if args.evaluate:\n        test()\n        exit(0)\n\n    for epoch in range(args.start_epochs, args.end_epochs):\n        adjust_learning_rate(optimizer, epoch)\n        train(epoch)\n        test()'"
quantization/WqAq/IAO/models/__init__.py,0,b''
quantization/WqAq/IAO/models/nin.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .util_wqaq import Conv2d_Q, BNFold_Conv2d_Q\n\nclass QuanConv2d(nn.Module):\n    def __init__(self, input_channels, output_channels,\n            kernel_size=-1, stride=-1, padding=-1, groups=1, last_relu=0, abits=8, wbits=8, bn_fold=0, q_type=1, first_layer=0):\n        super(QuanConv2d, self).__init__()\n        self.last_relu = last_relu\n        self.bn_fold = bn_fold\n        self.first_layer = first_layer\n\n        if self.bn_fold == 1:\n            self.bn_q_conv = BNFold_Conv2d_Q(input_channels, output_channels,\n                    kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, a_bits=abits, w_bits=wbits, q_type=q_type, first_layer=first_layer)\n        else:\n            self.q_conv = Conv2d_Q(input_channels, output_channels,\n                    kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, a_bits=abits, w_bits=wbits, q_type=q_type, first_layer=first_layer)\n            self.bn = nn.BatchNorm2d(output_channels, momentum=0.01) # \xe8\x80\x83\xe8\x99\x91\xe9\x87\x8f\xe5\x8c\x96\xe5\xb8\xa6\xe6\x9d\xa5\xe7\x9a\x84\xe6\x8a\x96\xe5\x8a\xa8\xe5\xbd\xb1\xe5\x93\x8d,\xe5\xaf\xb9momentum\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xb0\x83\xe6\x95\xb4(0.1 \xe2\x80\x94\xe2\x80\x94> 0.01),\xe5\x89\x8a\xe5\xbc\xb1batch\xe7\xbb\x9f\xe8\xae\xa1\xe5\x8f\x82\xe6\x95\xb0\xe5\x8d\xa0\xe6\xaf\x94\xef\xbc\x8c\xe4\xb8\x80\xe5\xae\x9a\xe7\xa8\x8b\xe5\xba\xa6\xe6\x8a\x91\xe5\x88\xb6\xe6\x8a\x96\xe5\x8a\xa8\xe3\x80\x82\xe7\xbb\x8f\xe5\xae\x9e\xe9\xaa\x8c\xe9\x87\x8f\xe5\x8c\x96\xe8\xae\xad\xe7\xbb\x83\xe6\x95\x88\xe6\x9e\x9c\xe6\x9b\xb4\xe5\xa5\xbd,acc\xe6\x8f\x90\xe5\x8d\x871%\xe5\xb7\xa6\xe5\x8f\xb3\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        if not self.first_layer:\n            x = self.relu(x)\n        if self.bn_fold == 1:\n            x = self.bn_q_conv(x)\n        else:\n            x = self.q_conv(x)\n            x = self.bn(x)\n        if self.last_relu:\n            x = self.relu(x)\n        return x\n\nclass Net(nn.Module):\n    def __init__(self, cfg = None, abits=8, wbits=8, bn_fold=0, q_type=1):\n        super(Net, self).__init__()\n        if cfg is None:\n            cfg = [192, 160, 96, 192, 192, 192, 192, 192]\n        # model - A/W\xe5\x85\xa8\xe9\x87\x8f\xe5\x8c\x96(\xe9\x99\xa4\xe8\xbe\x93\xe5\x85\xa5\xe3\x80\x81\xe8\xbe\x93\xe5\x87\xba\xe5\xa4\x96)\n        self.quan_model = nn.Sequential(\n                QuanConv2d(3, cfg[0], kernel_size=5, stride=1, padding=2, abits=abits, wbits=wbits, bn_fold=bn_fold, q_type=q_type, first_layer=1),\n                QuanConv2d(cfg[0], cfg[1], kernel_size=1, stride=1, padding=0, abits=abits, wbits=wbits, bn_fold=bn_fold, q_type=q_type),\n                QuanConv2d(cfg[1], cfg[2], kernel_size=1, stride=1, padding=0, abits=abits, wbits=wbits, bn_fold=bn_fold, q_type=q_type),\n                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n                \n                QuanConv2d(cfg[2], cfg[3], kernel_size=5, stride=1, padding=2, abits=abits, wbits=wbits, bn_fold=bn_fold, q_type=q_type),\n                QuanConv2d(cfg[3], cfg[4], kernel_size=1, stride=1, padding=0, abits=abits, wbits=wbits, bn_fold=bn_fold, q_type=q_type),\n                QuanConv2d(cfg[4], cfg[5], kernel_size=1, stride=1, padding=0, abits=abits, wbits=wbits, bn_fold=bn_fold, q_type=q_type),\n                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n                \n                QuanConv2d(cfg[5], cfg[6], kernel_size=3, stride=1, padding=1, abits=abits, wbits=wbits, bn_fold=bn_fold, q_type=q_type),\n                QuanConv2d(cfg[6], cfg[7], kernel_size=1, stride=1, padding=0, abits=abits, wbits=wbits, bn_fold=bn_fold, q_type=q_type),\n                QuanConv2d(cfg[7], 10, kernel_size=1, stride=1, padding=0, last_relu=1, abits=abits, wbits=wbits, bn_fold=bn_fold, q_type=q_type),\n                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n                )\n\n    def forward(self, x):\n        x = self.quan_model(x)\n        x = x.view(x.size(0), -1)\n        return x'"
quantization/WqAq/IAO/models/nin_gc.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .util_wqaq import Conv2d_Q, BNFold_Conv2d_Q\n\ndef channel_shuffle(x, groups):\n    """"""shuffle channels of a 4-D Tensor""""""\n    batch_size, channels, height, width = x.size()\n    assert channels % groups == 0\n    channels_per_group = channels // groups\n    # split into groups\n    x = x.view(batch_size, groups, channels_per_group, height, width)\n    # transpose 1, 2 axis\n    x = x.transpose(1, 2).contiguous()\n    # reshape into orignal\n    x = x.view(batch_size, channels, height, width)\n    return x\n\nclass QuanConv2d(nn.Module):\n    def __init__(self, input_channels, output_channels,\n            kernel_size=-1, stride=-1, padding=-1, groups=1, channel_shuffle=0, shuffle_groups=1, last_relu=0, abits=8, wbits=8, bn_fold=0, q_type=1, first_layer=0):\n        super(QuanConv2d, self).__init__()\n        self.last_relu = last_relu\n        self.channel_shuffle_flag = channel_shuffle\n        self.shuffle_groups = shuffle_groups\n        self.bn_fold = bn_fold\n        self.first_layer = first_layer\n\n        if self.bn_fold == 1:\n            self.bn_q_conv = BNFold_Conv2d_Q(input_channels, output_channels,\n                    kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, a_bits=abits, w_bits=wbits, q_type=q_type, first_layer=first_layer)\n        else:\n            self.q_conv = Conv2d_Q(input_channels, output_channels,\n                    kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, a_bits=abits, w_bits=wbits, q_type=q_type, first_layer=first_layer)\n            self.bn = nn.BatchNorm2d(output_channels, momentum=0.01) # \xe8\x80\x83\xe8\x99\x91\xe9\x87\x8f\xe5\x8c\x96\xe5\xb8\xa6\xe6\x9d\xa5\xe7\x9a\x84\xe6\x8a\x96\xe5\x8a\xa8\xe5\xbd\xb1\xe5\x93\x8d,\xe5\xaf\xb9momentum\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xb0\x83\xe6\x95\xb4(0.1 \xe2\x80\x94\xe2\x80\x94> 0.01),\xe5\x89\x8a\xe5\xbc\xb1batch\xe7\xbb\x9f\xe8\xae\xa1\xe5\x8f\x82\xe6\x95\xb0\xe5\x8d\xa0\xe6\xaf\x94\xef\xbc\x8c\xe4\xb8\x80\xe5\xae\x9a\xe7\xa8\x8b\xe5\xba\xa6\xe6\x8a\x91\xe5\x88\xb6\xe6\x8a\x96\xe5\x8a\xa8\xe3\x80\x82\xe7\xbb\x8f\xe5\xae\x9e\xe9\xaa\x8c\xe9\x87\x8f\xe5\x8c\x96\xe8\xae\xad\xe7\xbb\x83\xe6\x95\x88\xe6\x9e\x9c\xe6\x9b\xb4\xe5\xa5\xbd,acc\xe6\x8f\x90\xe5\x8d\x871%\xe5\xb7\xa6\xe5\x8f\xb3\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        if not self.first_layer:\n            x = self.relu(x)\n        if self.channel_shuffle_flag:\n            x = channel_shuffle(x, groups=self.shuffle_groups)\n        if self.bn_fold == 1:\n            x = self.bn_q_conv(x)\n        else:\n            x = self.q_conv(x)\n            x = self.bn(x)\n        if self.last_relu:\n            x = self.relu(x)\n        return x\n\nclass Net(nn.Module):\n    def __init__(self, cfg = None, abits=8, wbits=8, bn_fold=0, q_type=1):\n        super(Net, self).__init__()\n        if cfg is None:\n            cfg = [256, 256, 256, 512, 512, 512, 1024, 1024]\n        # model - A/W\xe5\x85\xa8\xe9\x87\x8f\xe5\x8c\x96(\xe9\x99\xa4\xe8\xbe\x93\xe5\x85\xa5\xe3\x80\x81\xe8\xbe\x93\xe5\x87\xba\xe5\xa4\x96)\n        self.quan_model = nn.Sequential(\n                QuanConv2d(3, cfg[0], kernel_size=5, stride=1, padding=2, abits=abits, wbits=wbits, bn_fold=bn_fold, q_type=q_type, first_layer=1),\n                QuanConv2d(cfg[0], cfg[1], kernel_size=1, stride=1, padding=0, groups=2, channel_shuffle=0, abits=abits, wbits=wbits, bn_fold=bn_fold, q_type=q_type),\n                QuanConv2d(cfg[1], cfg[2], kernel_size=1, stride=1, padding=0, groups=2, channel_shuffle=1, shuffle_groups=2, abits=abits, wbits=wbits, bn_fold=bn_fold, q_type=q_type),\n                nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n                \n                QuanConv2d(cfg[2], cfg[3], kernel_size=3, stride=1, padding=1, groups=16, channel_shuffle=1, shuffle_groups=2, abits=abits, wbits=wbits, bn_fold=bn_fold, q_type=q_type),\n                QuanConv2d(cfg[3], cfg[4], kernel_size=1, stride=1, padding=0, groups=4, channel_shuffle=1, shuffle_groups=16, abits=abits, wbits=wbits, bn_fold=bn_fold, q_type=q_type),\n                QuanConv2d(cfg[4], cfg[5], kernel_size=1, stride=1, padding=0, groups=4, channel_shuffle=1, shuffle_groups=4, abits=abits, wbits=wbits, bn_fold=bn_fold, q_type=q_type),\n                nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n                \n                QuanConv2d(cfg[5], cfg[6], kernel_size=3, stride=1, padding=1, groups=32, channel_shuffle=1, shuffle_groups=4, abits=abits, wbits=wbits, bn_fold=bn_fold, q_type=q_type),\n                QuanConv2d(cfg[6], cfg[7], kernel_size=1, stride=1, padding=0, groups=8, channel_shuffle=1, shuffle_groups=32, abits=abits, wbits=wbits, bn_fold=bn_fold, q_type=q_type),\n                QuanConv2d(cfg[7], 10, kernel_size=1, stride=1, padding=0, last_relu=1, abits=abits, wbits=wbits, bn_fold=bn_fold, q_type=q_type),\n                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n                )\n\n    def forward(self, x):\n        x = self.quan_model(x)\n        x = x.view(x.size(0), -1)\n        return x'"
quantization/WqAq/IAO/models/util_wqaq.py,43,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import distributed\nfrom torch.nn import init\nfrom torch.nn.parameter import Parameter\nfrom torch.autograd import Function\n\n# ********************* range_trackers(\xe8\x8c\x83\xe5\x9b\xb4\xe7\xbb\x9f\xe8\xae\xa1\xe5\x99\xa8\xef\xbc\x8c\xe7\xbb\x9f\xe8\xae\xa1\xe9\x87\x8f\xe5\x8c\x96\xe5\x89\x8d\xe8\x8c\x83\xe5\x9b\xb4) *********************\nclass RangeTracker(nn.Module):\n    def __init__(self, q_level):\n        super().__init__()\n        self.q_level = q_level\n\n    def update_range(self, min_val, max_val):\n        raise NotImplementedError\n\n    @torch.no_grad()\n    def forward(self, input):\n        if self.q_level == 'L':    # A,min_max_shape=(1, 1, 1, 1),layer\xe7\xba\xa7\n            min_val = torch.min(input)\n            max_val = torch.max(input)\n        elif self.q_level == 'C':  # W,min_max_shape=(N, 1, 1, 1),channel\xe7\xba\xa7\n            min_val = torch.min(torch.min(torch.min(input, 3, keepdim=True)[0], 2, keepdim=True)[0], 1, keepdim=True)[0]\n            max_val = torch.max(torch.max(torch.max(input, 3, keepdim=True)[0], 2, keepdim=True)[0], 1, keepdim=True)[0]\n            \n        self.update_range(min_val, max_val)\nclass GlobalRangeTracker(RangeTracker):  # W,min_max_shape=(N, 1, 1, 1),channel\xe7\xba\xa7,\xe5\x8f\x96\xe6\x9c\xac\xe6\xac\xa1\xe5\x92\x8c\xe4\xb9\x8b\xe5\x89\x8d\xe7\x9b\xb8\xe6\xaf\x94\xe7\x9a\x84min_max \xe2\x80\x94\xe2\x80\x94 (N, C, W, H)\n    def __init__(self, q_level, out_channels):\n        super().__init__(q_level)\n        self.register_buffer('min_val', torch.zeros(out_channels, 1, 1, 1))\n        self.register_buffer('max_val', torch.zeros(out_channels, 1, 1, 1))\n        self.register_buffer('first_w', torch.zeros(1))\n\n    def update_range(self, min_val, max_val):\n        temp_minval = self.min_val\n        temp_maxval = self.max_val\n        if self.first_w == 0:\n            self.first_w.add_(1)\n            self.min_val.add_(min_val)\n            self.max_val.add_(max_val)\n        else:\n            self.min_val.add_(-temp_minval).add_(torch.min(temp_minval, min_val))\n            self.max_val.add_(-temp_maxval).add_(torch.max(temp_maxval, max_val))\nclass AveragedRangeTracker(RangeTracker):  # A,min_max_shape=(1, 1, 1, 1),layer\xe7\xba\xa7,\xe5\x8f\x96running_min_max \xe2\x80\x94\xe2\x80\x94 (N, C, W, H)\n    def __init__(self, q_level, momentum=0.1):\n        super().__init__(q_level)\n        self.momentum = momentum\n        self.register_buffer('min_val', torch.zeros(1))\n        self.register_buffer('max_val', torch.zeros(1))\n        self.register_buffer('first_a', torch.zeros(1))\n\n    def update_range(self, min_val, max_val):\n        if self.first_a == 0:\n            self.first_a.add_(1)\n            self.min_val.add_(min_val)\n            self.max_val.add_(max_val)\n        else:\n            self.min_val.mul_(1 - self.momentum).add_(min_val * self.momentum)\n            self.max_val.mul_(1 - self.momentum).add_(max_val * self.momentum)\n        \n# ********************* quantizers\xef\xbc\x88\xe9\x87\x8f\xe5\x8c\x96\xe5\x99\xa8\xef\xbc\x8c\xe9\x87\x8f\xe5\x8c\x96\xef\xbc\x89 *********************\nclass Round(Function):\n\n    @staticmethod\n    def forward(self, input):\n        output = torch.round(input)\n        return output\n\n    @staticmethod\n    def backward(self, grad_output):\n        grad_input = grad_output.clone()\n        return grad_input\nclass Quantizer(nn.Module):\n    def __init__(self, bits, range_tracker):\n        super().__init__()\n        self.bits = bits\n        self.range_tracker = range_tracker\n        self.register_buffer('scale', None)      # \xe9\x87\x8f\xe5\x8c\x96\xe6\xaf\x94\xe4\xbe\x8b\xe5\x9b\xa0\xe5\xad\x90\n        self.register_buffer('zero_point', None) # \xe9\x87\x8f\xe5\x8c\x96\xe9\x9b\xb6\xe7\x82\xb9\n\n    def update_params(self):\n        raise NotImplementedError\n\n    # \xe9\x87\x8f\xe5\x8c\x96\n    def quantize(self, input):\n        output = input * self.scale - self.zero_point\n        return output\n\n    def round(self, input):\n        output = Round.apply(input)\n        return output\n\n    # \xe6\x88\xaa\xe6\x96\xad\n    def clamp(self, input):\n        output = torch.clamp(input, self.min_val, self.max_val)\n        return output\n\n    # \xe5\x8f\x8d\xe9\x87\x8f\xe5\x8c\x96\n    def dequantize(self, input):\n        output = (input + self.zero_point) / self.scale\n        return output\n\n    def forward(self, input):\n        if self.bits == 32:\n            output = input\n        elif self.bits == 1:\n            print('\xef\xbc\x81Binary quantization is not supported \xef\xbc\x81')\n            assert self.bits != 1\n        else:\n            self.range_tracker(input)\n            self.update_params()\n            output = self.quantize(input)   # \xe9\x87\x8f\xe5\x8c\x96\n            output = self.round(output)\n            output = self.clamp(output)     # \xe6\x88\xaa\xe6\x96\xad\n            output = self.dequantize(output)# \xe5\x8f\x8d\xe9\x87\x8f\xe5\x8c\x96\n        return output\nclass SignedQuantizer(Quantizer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.register_buffer('min_val', torch.tensor(-(1 << (self.bits - 1))))\n        self.register_buffer('max_val', torch.tensor((1 << (self.bits - 1)) - 1))\nclass UnsignedQuantizer(Quantizer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.register_buffer('min_val', torch.tensor(0))\n        self.register_buffer('max_val', torch.tensor((1 << self.bits) - 1))\n# \xe5\xaf\xb9\xe7\xa7\xb0\xe9\x87\x8f\xe5\x8c\x96\nclass SymmetricQuantizer(SignedQuantizer):\n\n    def update_params(self):\n        quantized_range = torch.min(torch.abs(self.min_val), torch.abs(self.max_val))  # \xe9\x87\x8f\xe5\x8c\x96\xe5\x90\x8e\xe8\x8c\x83\xe5\x9b\xb4\n        float_range = torch.max(torch.abs(self.range_tracker.min_val), torch.abs(self.range_tracker.max_val))  # \xe9\x87\x8f\xe5\x8c\x96\xe5\x89\x8d\xe8\x8c\x83\xe5\x9b\xb4\n        self.scale = quantized_range / float_range      # \xe9\x87\x8f\xe5\x8c\x96\xe6\xaf\x94\xe4\xbe\x8b\xe5\x9b\xa0\xe5\xad\x90\n        self.zero_point = torch.zeros_like(self.scale)  # \xe9\x87\x8f\xe5\x8c\x96\xe9\x9b\xb6\xe7\x82\xb9\n# \xe9\x9d\x9e\xe5\xaf\xb9\xe7\xa7\xb0\xe9\x87\x8f\xe5\x8c\x96\nclass AsymmetricQuantizer(UnsignedQuantizer):\n\n    def update_params(self):\n        quantized_range = self.max_val - self.min_val  # \xe9\x87\x8f\xe5\x8c\x96\xe5\x90\x8e\xe8\x8c\x83\xe5\x9b\xb4\n        float_range = self.range_tracker.max_val - self.range_tracker.min_val   # \xe9\x87\x8f\xe5\x8c\x96\xe5\x89\x8d\xe8\x8c\x83\xe5\x9b\xb4\n        self.scale = quantized_range / float_range  # \xe9\x87\x8f\xe5\x8c\x96\xe6\xaf\x94\xe4\xbe\x8b\xe5\x9b\xa0\xe5\xad\x90\n        self.zero_point = torch.round(self.range_tracker.min_val * self.scale)  # \xe9\x87\x8f\xe5\x8c\x96\xe9\x9b\xb6\xe7\x82\xb9\n\n# ********************* \xe9\x87\x8f\xe5\x8c\x96\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x88\xe5\x90\x8c\xe6\x97\xb6\xe9\x87\x8f\xe5\x8c\x96A/W\xef\xbc\x8c\xe5\xb9\xb6\xe5\x81\x9a\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x89 *********************\nclass Conv2d_Q(nn.Conv2d):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        dilation=1,\n        groups=1,\n        bias=True,\n        a_bits=8,\n        w_bits=8,\n        q_type=1,\n        first_layer=0,\n    ):\n        super().__init__(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias\n        )\n        # \xe5\xae\x9e\xe4\xbe\x8b\xe5\x8c\x96\xe9\x87\x8f\xe5\x8c\x96\xe5\x99\xa8\xef\xbc\x88A-layer\xe7\xba\xa7\xef\xbc\x8cW-channel\xe7\xba\xa7\xef\xbc\x89\n        if q_type == 0:\n            self.activation_quantizer = SymmetricQuantizer(bits=a_bits, range_tracker=AveragedRangeTracker(q_level='L'))\n            self.weight_quantizer = SymmetricQuantizer(bits=w_bits, range_tracker=GlobalRangeTracker(q_level='C', out_channels=out_channels))\n        else:\n            self.activation_quantizer = AsymmetricQuantizer(bits=a_bits, range_tracker=AveragedRangeTracker(q_level='L'))\n            self.weight_quantizer = AsymmetricQuantizer(bits=w_bits, range_tracker=GlobalRangeTracker(q_level='C', out_channels=out_channels))\n        self.first_layer = first_layer\n\n    def forward(self, input):\n        # \xe9\x87\x8f\xe5\x8c\x96A\xe5\x92\x8cW\n        if not self.first_layer:\n            input = self.activation_quantizer(input)\n        q_input = input\n        q_weight = self.weight_quantizer(self.weight) \n        # \xe9\x87\x8f\xe5\x8c\x96\xe5\x8d\xb7\xe7\xa7\xaf\n        output = F.conv2d(\n            input=q_input,\n            weight=q_weight,\n            bias=self.bias,\n            stride=self.stride,\n            padding=self.padding,\n            dilation=self.dilation,\n            groups=self.groups\n        )\n        return output\n\ndef reshape_to_activation(input):\n  return input.reshape(1, -1, 1, 1)\ndef reshape_to_weight(input):\n  return input.reshape(-1, 1, 1, 1)\ndef reshape_to_bias(input):\n  return input.reshape(-1)\n# ********************* bn\xe8\x9e\x8d\xe5\x90\x88_\xe9\x87\x8f\xe5\x8c\x96\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x88bn\xe8\x9e\x8d\xe5\x90\x88\xe5\x90\x8e\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe9\x87\x8f\xe5\x8c\x96A/W\xef\xbc\x8c\xe5\xb9\xb6\xe5\x81\x9a\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x89 *********************\nclass BNFold_Conv2d_Q(Conv2d_Q):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        dilation=1,\n        groups=1,\n        bias=False,\n        eps=1e-5,\n        momentum=0.01, # \xe8\x80\x83\xe8\x99\x91\xe9\x87\x8f\xe5\x8c\x96\xe5\xb8\xa6\xe6\x9d\xa5\xe7\x9a\x84\xe6\x8a\x96\xe5\x8a\xa8\xe5\xbd\xb1\xe5\x93\x8d,\xe5\xaf\xb9momentum\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xb0\x83\xe6\x95\xb4(0.1 \xe2\x80\x94\xe2\x80\x94> 0.01),\xe5\x89\x8a\xe5\xbc\xb1batch\xe7\xbb\x9f\xe8\xae\xa1\xe5\x8f\x82\xe6\x95\xb0\xe5\x8d\xa0\xe6\xaf\x94\xef\xbc\x8c\xe4\xb8\x80\xe5\xae\x9a\xe7\xa8\x8b\xe5\xba\xa6\xe6\x8a\x91\xe5\x88\xb6\xe6\x8a\x96\xe5\x8a\xa8\xe3\x80\x82\xe7\xbb\x8f\xe5\xae\x9e\xe9\xaa\x8c\xe9\x87\x8f\xe5\x8c\x96\xe8\xae\xad\xe7\xbb\x83\xe6\x95\x88\xe6\x9e\x9c\xe6\x9b\xb4\xe5\xa5\xbd,acc\xe6\x8f\x90\xe5\x8d\x871%\xe5\xb7\xa6\xe5\x8f\xb3\n        a_bits=8,\n        w_bits=8,\n        q_type=1,\n        first_layer=0,\n    ):\n        super().__init__(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias\n        )\n        self.eps = eps\n        self.momentum = momentum\n        self.gamma = Parameter(torch.Tensor(out_channels))\n        self.beta = Parameter(torch.Tensor(out_channels))\n        self.register_buffer('running_mean', torch.zeros(out_channels))\n        self.register_buffer('running_var', torch.ones(out_channels))\n        self.register_buffer('first_bn', torch.zeros(1))\n        init.uniform_(self.gamma)\n        init.zeros_(self.beta)\n        \n        # \xe5\xae\x9e\xe4\xbe\x8b\xe5\x8c\x96\xe9\x87\x8f\xe5\x8c\x96\xe5\x99\xa8\xef\xbc\x88A-layer\xe7\xba\xa7\xef\xbc\x8cW-channel\xe7\xba\xa7\xef\xbc\x89\n        if q_type == 0:\n            self.activation_quantizer = SymmetricQuantizer(bits=a_bits, range_tracker=AveragedRangeTracker(q_level='L'))\n            self.weight_quantizer = SymmetricQuantizer(bits=w_bits, range_tracker=GlobalRangeTracker(q_level='C', out_channels=out_channels))\n        else:\n            self.activation_quantizer = AsymmetricQuantizer(bits=a_bits, range_tracker=AveragedRangeTracker(q_level='L'))\n            self.weight_quantizer = AsymmetricQuantizer(bits=w_bits, range_tracker=GlobalRangeTracker(q_level='C', out_channels=out_channels))\n        self.first_layer = first_layer\n\n    def forward(self, input):\n        # \xe8\xae\xad\xe7\xbb\x83\xe6\x80\x81\n        if self.training:\n            # \xe5\x85\x88\xe5\x81\x9a\xe6\x99\xae\xe9\x80\x9a\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xbe\x97\xe5\x88\xb0A\xef\xbc\x8c\xe4\xbb\xa5\xe5\x8f\x96\xe5\xbe\x97BN\xe5\x8f\x82\xe6\x95\xb0\n            output = F.conv2d(\n                input=input,\n                weight=self.weight,\n                bias=self.bias,\n                stride=self.stride,\n                padding=self.padding,\n                dilation=self.dilation,\n                groups=self.groups\n            )\n            # \xe6\x9b\xb4\xe6\x96\xb0BN\xe7\xbb\x9f\xe8\xae\xa1\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x88batch\xe5\x92\x8crunning\xef\xbc\x89\n            dims = [dim for dim in range(4) if dim != 1]\n            batch_mean = torch.mean(output, dim=dims)\n            batch_var = torch.var(output, dim=dims)\n            with torch.no_grad():\n                if self.first_bn == 0:\n                    self.first_bn.add_(1)\n                    self.running_mean.add_(batch_mean)\n                    self.running_var.add_(batch_var)\n                else:\n                    self.running_mean.mul_(1 - self.momentum).add_(batch_mean * self.momentum)\n                    self.running_var.mul_(1 - self.momentum).add_(batch_var * self.momentum)\n            # BN\xe8\x9e\x8d\xe5\x90\x88\n            if self.bias is not None:  \n              bias = reshape_to_bias(self.beta + (self.bias -  batch_mean) * (self.gamma / torch.sqrt(batch_var + self.eps)))\n            else:\n              bias = reshape_to_bias(self.beta - batch_mean  * (self.gamma / torch.sqrt(batch_var + self.eps)))# b\xe8\x9e\x8dbatch\n            weight = self.weight * reshape_to_weight(self.gamma / torch.sqrt(self.running_var + self.eps))     # w\xe8\x9e\x8drunning\n        # \xe6\xb5\x8b\xe8\xaf\x95\xe6\x80\x81\n        else:\n            #print(self.running_mean, self.running_var)\n            # BN\xe8\x9e\x8d\xe5\x90\x88\n            if self.bias is not None:\n              bias = reshape_to_bias(self.beta + (self.bias - self.running_mean) * (self.gamma / torch.sqrt(self.running_var + self.eps)))\n            else:\n              bias = reshape_to_bias(self.beta - self.running_mean * (self.gamma / torch.sqrt(self.running_var + self.eps)))  # b\xe8\x9e\x8drunning\n            weight = self.weight * reshape_to_weight(self.gamma / torch.sqrt(self.running_var + self.eps))  # w\xe8\x9e\x8drunning\n        \n        # \xe9\x87\x8f\xe5\x8c\x96A\xe5\x92\x8cbn\xe8\x9e\x8d\xe5\x90\x88\xe5\x90\x8e\xe7\x9a\x84W\n        if not self.first_layer:\n            input = self.activation_quantizer(input)\n        q_input = input\n        q_weight = self.weight_quantizer(weight) \n        # \xe9\x87\x8f\xe5\x8c\x96\xe5\x8d\xb7\xe7\xa7\xaf\n        if self.training:  # \xe8\xae\xad\xe7\xbb\x83\xe6\x80\x81\n          output = F.conv2d(\n              input=q_input,\n              weight=q_weight,\n              bias=self.bias,  # \xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe4\xb8\x8d\xe5\x8a\xa0bias\xef\xbc\x88self.bias\xe4\xb8\xbaNone\xef\xbc\x89\n              stride=self.stride,\n              padding=self.padding,\n              dilation=self.dilation,\n              groups=self.groups\n          )\n          # \xef\xbc\x88\xe8\xbf\x99\xe9\x87\x8c\xe5\xb0\x86\xe8\xae\xad\xe7\xbb\x83\xe6\x80\x81\xe4\xb8\x8b\xef\xbc\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xe4\xb8\xadw\xe8\x9e\x8d\xe5\x90\x88running\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\x95\x88\xe6\x9e\x9c\xe8\xbd\xac\xe4\xb8\xba\xe8\x9e\x8d\xe5\x90\x88batch\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\x95\x88\xe6\x9e\x9c\xef\xbc\x89running \xe2\x80\x94\xe2\x80\x94> batch\n          output *= reshape_to_activation(torch.sqrt(self.running_var + self.eps) / torch.sqrt(batch_var + self.eps))\n          output += reshape_to_activation(bias)\n        else:  # \xe6\xb5\x8b\xe8\xaf\x95\xe6\x80\x81\n          output = F.conv2d(\n              input=q_input,\n              weight=q_weight,\n              bias=bias,  # \xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe5\x8a\xa0bias\xef\xbc\x8c\xe5\x81\x9a\xe5\xae\x8c\xe6\x95\xb4\xe7\x9a\x84conv+bn\n              stride=self.stride,\n              padding=self.padding,\n              dilation=self.dilation,\n              groups=self.groups\n          )\n        return output"""
quantization/WqAq/dorefa/models/__init__.py,0,b''
quantization/WqAq/dorefa/models/nin.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .util_wqaq import Conv2d_Q\n\nclass DorefaConv2d(nn.Module):\n    def __init__(self, input_channels, output_channels,\n            kernel_size=-1, stride=-1, padding=-1, groups=1, last_relu=0, abits=8, wbits=8, first_layer=0):\n        super(DorefaConv2d, self).__init__()\n        self.last_relu = last_relu\n        self.first_layer = first_layer\n\n        self.q_conv = Conv2d_Q(input_channels, output_channels,\n                    kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, a_bits=abits, w_bits=wbits, first_layer=first_layer)\n        self.bn = nn.BatchNorm2d(output_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        if not self.first_layer:\n            x = self.relu(x)\n        x = self.q_conv(x)\n        x = self.bn(x)\n        if self.last_relu:\n            x = self.relu(x)\n        return x\n\nclass Net(nn.Module):\n    def __init__(self, cfg = None, abits=8, wbits=8):\n        super(Net, self).__init__()\n        if cfg is None:\n            cfg = [192, 160, 96, 192, 192, 192, 192, 192]\n\n        # model - A/W\xe5\x85\xa8\xe9\x87\x8f\xe5\x8c\x96(\xe9\x99\xa4\xe8\xbe\x93\xe5\x85\xa5\xe3\x80\x81\xe8\xbe\x93\xe5\x87\xba\xe5\xa4\x96)   \n        self.dorefa = nn.Sequential(\n                DorefaConv2d(3, cfg[0], kernel_size=5, stride=1, padding=2, abits=abits, wbits=wbits, first_layer=1),\n                DorefaConv2d(cfg[0], cfg[1], kernel_size=1, stride=1, padding=0, abits=abits, wbits=wbits),\n                DorefaConv2d(cfg[1], cfg[2], kernel_size=1, stride=1, padding=0, abits=abits, wbits=wbits),\n                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n                \n                DorefaConv2d(cfg[2], cfg[3], kernel_size=5, stride=1, padding=2, abits=abits, wbits=wbits),\n                DorefaConv2d(cfg[3], cfg[4], kernel_size=1, stride=1, padding=0, abits=abits, wbits=wbits),\n                DorefaConv2d(cfg[4], cfg[5], kernel_size=1, stride=1, padding=0, abits=abits, wbits=wbits),\n                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n                \n                DorefaConv2d(cfg[5], cfg[6], kernel_size=3, stride=1, padding=1, abits=abits, wbits=wbits),\n                DorefaConv2d(cfg[6], cfg[7], kernel_size=1, stride=1, padding=0, abits=abits, wbits=wbits),\n                DorefaConv2d(cfg[7], 10, kernel_size=1, stride=1, padding=0, last_relu=1, abits=abits, wbits=wbits),\n                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n                )\n\n    def forward(self, x):\n        x = self.dorefa(x)\n        x = x.view(x.size(0), -1)\n        return x'"
quantization/WqAq/dorefa/models/nin_gc.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .util_wqaq import Conv2d_Q\n\ndef channel_shuffle(x, groups):\n    """"""shuffle channels of a 4-D Tensor""""""\n    batch_size, channels, height, width = x.size()\n    assert channels % groups == 0\n    channels_per_group = channels // groups\n    # split into groups\n    x = x.view(batch_size, groups, channels_per_group, height, width)\n    # transpose 1, 2 axis\n    x = x.transpose(1, 2).contiguous()\n    # reshape into orignal\n    x = x.view(batch_size, channels, height, width)\n    return x\n\nclass DorefaConv2d(nn.Module):\n    def __init__(self, input_channels, output_channels,\n            kernel_size=-1, stride=-1, padding=-1, groups=1, channel_shuffle=0, shuffle_groups=1, last_relu=0, abits=8, wbits=8, first_layer=0):\n        super(DorefaConv2d, self).__init__()\n        self.last_relu = last_relu\n        self.channel_shuffle_flag = channel_shuffle\n        self.shuffle_groups = shuffle_groups\n        self.first_layer = first_layer\n\n        self.q_conv = Conv2d_Q(input_channels, output_channels,\n                kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, a_bits=abits, w_bits=wbits, first_layer=first_layer)\n        self.bn = nn.BatchNorm2d(output_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        if not self.first_layer:\n            x = self.relu(x)\n        if self.channel_shuffle_flag:\n            x = channel_shuffle(x, groups=self.shuffle_groups)\n        x = self.q_conv(x)\n        x = self.bn(x)\n        if self.last_relu:\n            x = self.relu(x)\n        return x\n\nclass Net(nn.Module):\n    def __init__(self, cfg = None, abits=8, wbits=8):\n        super(Net, self).__init__()\n        if cfg is None:\n            cfg = [256, 256, 256, 512, 512, 512, 1024, 1024]\n\n        # model - A/W\xe5\x85\xa8\xe9\x87\x8f\xe5\x8c\x96(\xe9\x99\xa4\xe8\xbe\x93\xe5\x85\xa5\xe3\x80\x81\xe8\xbe\x93\xe5\x87\xba\xe5\xa4\x96)\n        self.dorefa = nn.Sequential(\n                DorefaConv2d(3, cfg[0], kernel_size=5, stride=1, padding=2, abits=abits, wbits=wbits, first_layer=1),\n                DorefaConv2d(cfg[0], cfg[1], kernel_size=1, stride=1, padding=0, groups=2, channel_shuffle=0, abits=abits, wbits=wbits),\n                DorefaConv2d(cfg[1], cfg[2], kernel_size=1, stride=1, padding=0, groups=2, channel_shuffle=1, shuffle_groups=2, abits=abits, wbits=wbits),\n                nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n                \n                DorefaConv2d(cfg[2], cfg[3], kernel_size=3, stride=1, padding=1, groups=16, channel_shuffle=1, shuffle_groups=2, abits=abits, wbits=wbits),\n                DorefaConv2d(cfg[3], cfg[4], kernel_size=1, stride=1, padding=0, groups=4, channel_shuffle=1, shuffle_groups=16, abits=abits, wbits=wbits),\n                DorefaConv2d(cfg[4], cfg[5], kernel_size=1, stride=1, padding=0, groups=4, channel_shuffle=1, shuffle_groups=4, abits=abits, wbits=wbits),\n                nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n                \n                DorefaConv2d(cfg[5], cfg[6], kernel_size=3, stride=1, padding=1, groups=32, channel_shuffle=1, shuffle_groups=4, abits=abits, wbits=wbits),\n                DorefaConv2d(cfg[6], cfg[7], kernel_size=1, stride=1, padding=0, groups=8, channel_shuffle=1, shuffle_groups=32, abits=abits, wbits=wbits),\n                DorefaConv2d(cfg[7], 10, kernel_size=1, stride=1, padding=0, last_relu=1, abits=abits, wbits=wbits),\n                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n                )\n\n    def forward(self, x):\n        x = self.dorefa(x)\n        x = x.view(x.size(0), -1)\n        return x'"
quantization/WqAq/dorefa/models/util_wqaq.py,7,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Function\n\nclass Round(Function):\n    \n    @staticmethod\n    def forward(self, input):\n        output = torch.round(input)\n        return output\n\n    @staticmethod\n    def backward(self, grad_output):\n        grad_input = grad_output.clone()\n        return grad_input\n\n# ********************* A(\xe7\x89\xb9\xe5\xbe\x81)\xe9\x87\x8f\xe5\x8c\x96 ***********************\nclass activation_quantize(nn.Module):\n  def __init__(self, a_bits):\n    super().__init__()\n    self.a_bits = a_bits\n\n  def round(self, input):\n    output = Round.apply(input)\n    return output\n\n  def forward(self, input):\n    if self.a_bits == 32:\n      output = input\n    elif self.a_bits == 1:\n      print('\xef\xbc\x81Binary quantization is not supported \xef\xbc\x81')\n      assert self.a_bits != 1\n    else:\n      output = torch.clamp(input * 0.1, 0, 1)  # \xe7\x89\xb9\xe5\xbe\x81A\xe6\x88\xaa\xe6\x96\xad\xe5\x89\x8d\xe5\x85\x88\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xbc\xa9\xe6\x94\xbe\xef\xbc\x88* 0.1\xef\xbc\x89\xef\xbc\x8c\xe4\xbb\xa5\xe5\x87\x8f\xe5\xb0\x8f\xe6\x88\xaa\xe6\x96\xad\xe8\xaf\xaf\xe5\xb7\xae\n      scale = float(2 ** self.a_bits - 1)\n      output = output * scale\n      output = self.round(output)\n      output = output / scale\n    return output\n# ********************* W(\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0)\xe9\x87\x8f\xe5\x8c\x96 ***********************\nclass weight_quantize(nn.Module):\n  def __init__(self, w_bits):\n    super().__init__()\n    self.w_bits = w_bits\n\n  def round(self, input):\n    output = Round.apply(input)\n    return output\n\n  def forward(self, input):\n    if self.w_bits == 32:\n      output = input\n    elif self.w_bits == 1:\n      print('\xef\xbc\x81Binary quantization is not supported \xef\xbc\x81')\n      assert self.w_bits != 1                      \n    else:\n      output = torch.tanh(input)\n      output = output / 2 / torch.max(torch.abs(output)) + 0.5  #\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96-[0,1]\n      scale = float(2 ** self.w_bits - 1)\n      output = output * scale\n      output = self.round(output)\n      output = output / scale\n      output = 2 * output - 1\n    return output\n\n# ********************* \xe9\x87\x8f\xe5\x8c\x96\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x88\xe5\x90\x8c\xe6\x97\xb6\xe9\x87\x8f\xe5\x8c\x96A/W\xef\xbc\x8c\xe5\xb9\xb6\xe5\x81\x9a\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x89 ***********************\nclass Conv2d_Q(nn.Conv2d):\n  def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        dilation=1,\n        groups=1,\n        bias=True,\n        a_bits=8,\n        w_bits=8,\n        first_layer=0\n      ):\n        super().__init__(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias\n        )\n        # \xe5\xae\x9e\xe4\xbe\x8b\xe5\x8c\x96\xe8\xb0\x83\xe7\x94\xa8A\xe5\x92\x8cW\xe9\x87\x8f\xe5\x8c\x96\xe5\x99\xa8\n        self.activation_quantizer = activation_quantize(a_bits=a_bits)\n        self.weight_quantizer = weight_quantize(w_bits=w_bits)    \n        self.first_layer = first_layer\n\n  def forward(self, input):\n    # \xe9\x87\x8f\xe5\x8c\x96A\xe5\x92\x8cW\n    if not self.first_layer:\n      input = self.activation_quantizer(input)\n    q_input = input\n    q_weight = self.weight_quantizer(self.weight) \n    # \xe9\x87\x8f\xe5\x8c\x96\xe5\x8d\xb7\xe7\xa7\xaf\n    output = F.conv2d(\n            input=q_input,\n            weight=q_weight,\n            bias=self.bias,\n            stride=self.stride,\n            padding=self.padding,\n            dilation=self.dilation,\n            groups=self.groups\n        )\n    return output\n# ********************* \xe9\x87\x8f\xe5\x8c\x96\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xef\xbc\x88\xe5\x90\x8c\xe6\x97\xb6\xe9\x87\x8f\xe5\x8c\x96A/W\xef\xbc\x8c\xe5\xb9\xb6\xe5\x81\x9a\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xef\xbc\x89 ***********************\nclass Linear_Q(nn.Linear):\n  def __init__(self, in_features, out_features, bias=True, a_bits=2, w_bits=2):\n    super().__init__(in_features=in_features, out_features=out_features, bias=bias)\n    self.activation_quantizer = activation_quantize(a_bits=a_bits)\n    self.weight_quantizer = weight_quantize(w_bits=w_bits) \n\n  def forward(self, input):\n    # \xe9\x87\x8f\xe5\x8c\x96A\xe5\x92\x8cW\n    q_input = self.activation_quantizer(input)\n    q_weight = self.weight_quantizer(self.weight) \n    # \xe9\x87\x8f\xe5\x8c\x96\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\n    output = F.linear(input=q_input, weight=q_weight, bias=self.bias)\n    return output"""
