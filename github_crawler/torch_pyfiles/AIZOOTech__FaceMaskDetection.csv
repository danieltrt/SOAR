file_path,api_count,code
caffe_infer.py,0,"b'# -*- coding:utf-8 -*-\nimport cv2\nimport time\nimport caffe\nimport argparse\nimport numpy as np\nfrom PIL import Image\nfrom utils.anchor_generator import generate_anchors\nfrom utils.anchor_decode import decode_bbox\nfrom utils.nms import single_class_non_max_suppression\nfrom load_model.caffe_loader import load_caffe_model, caffe_inference\n\nmodel = load_caffe_model(\'models/face_mask_detection.prototxt\',\'models/face_mask_detection.caffemodel\');\n\n# anchor configuration\nfeature_map_sizes = [[33, 33], [17, 17], [9, 9], [5, 5], [3, 3]]\nanchor_sizes = [[0.04, 0.056], [0.08, 0.11], [0.16, 0.22], [0.32, 0.45], [0.64, 0.72]]\nanchor_ratios = [[1, 0.62, 0.42]] * 5\n\n# generate anchors\nanchors = generate_anchors(feature_map_sizes, anchor_sizes, anchor_ratios)\n\n# for inference , the batch size is 1, the model output shape is [1, N, 4],\n# so we expand dim for anchors to [1, anchor_num, 4]\nanchors_exp = np.expand_dims(anchors, axis=0)\n\nid2class = {0: \'Mask\', 1: \'NoMask\'}\n\n\ndef inference(image,\n              conf_thresh=0.5,\n              iou_thresh=0.4,\n              target_shape=(160, 160),\n              draw_result=True,\n              show_result=True\n              ):\n    \'\'\'\n    Main function of detection inference\n    :param image: 3D numpy array of image\n    :param conf_thresh: the min threshold of classification probabity.\n    :param iou_thresh: the IOU threshold of NMS\n    :param target_shape: the model input size.\n    :param draw_result: whether to daw bounding box to the image.\n    :param show_result: whether to display the image.\n    :return:\n    \'\'\'\n    # image = np.copy(image)\n    output_info = []\n    height, width, _ = image.shape\n    image_resized = cv2.resize(image, target_shape)\n    image_np = image_resized / 255.0  # \xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe5\x88\xb00~1\n    image_exp = np.expand_dims(image_np, axis=0)\n\n    image_transposed = image_exp.transpose((0, 3, 1, 2))\n\n    y_bboxes_output, y_cls_output = caffe_inference(model, image_transposed)\n    # remove the batch dimension, for batch is always 1 for inference.\n    y_bboxes = decode_bbox(anchors_exp, y_bboxes_output)[0]\n    y_cls = y_cls_output[0]\n    # To speed up, do single class NMS, not multiple classes NMS.\n    bbox_max_scores = np.max(y_cls, axis=1)\n    bbox_max_score_classes = np.argmax(y_cls, axis=1)\n    \n    # keep_idx is the alive bounding box after nms.\n    keep_idxs = single_class_non_max_suppression(y_bboxes,\n                                                    bbox_max_scores,\n                                                    conf_thresh=conf_thresh,\n                                                    iou_thresh=iou_thresh,\n                                                    )\n\n    for idx in keep_idxs:\n        conf = float(bbox_max_scores[idx])\n        class_id = bbox_max_score_classes[idx]\n        bbox = y_bboxes[idx]\n        # clip the coordinate, avoid the value exceed the image boundary.\n        xmin = max(0, int(bbox[0] * width))\n        ymin = max(0, int(bbox[1] * height))\n        xmax = min(int(bbox[2] * width), width)\n        ymax = min(int(bbox[3] * height), height)\n\n        if draw_result:\n            if class_id == 0:\n                color = (0, 255, 0)\n            else:\n                color = (255, 0 , 0)\n            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 2)\n            cv2.putText(image, ""%s: %.2f"" % (id2class[class_id], conf), (xmin + 2, ymin-2),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, color)\n        output_info.append([class_id, conf, xmin, ymin, xmax, ymax])\n\n    if show_result:\n        Image.fromarray(image).show()\n    return output_info\n\n\ndef run_on_video(video_path, output_video_name, conf_thresh):\n    cap = cv2.VideoCapture(video_path)\n    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    fourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n    # writer = cv2.VideoWriter(output_video_name, fourcc, int(fps), (int(width), int(height)))\n    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    if not cap.isOpened():\n        raise ValueError(""Video open failed."")\n        return\n    status = True\n    idx = 0\n    while status:\n        start_stamp = time.time()\n        status, img_raw = cap.read()\n        img_raw = cv2.cvtColor(img_raw, cv2.COLOR_BGR2RGB)\n        read_frame_stamp = time.time()\n        if (status):\n            inference(img_raw,\n                             conf_thresh,\n                             iou_thresh=0.5,\n                             target_shape=(260, 260),\n                             draw_result=True,\n                             show_result=False)\n            cv2.imshow(\'image\', img_raw[:,:,::-1])\n            cv2.waitKey(1)\n            inference_stamp = time.time()\n            # writer.write(img_raw)\n            write_frame_stamp = time.time()\n            idx += 1\n            print(""%d of %d"" % (idx, total_frames))\n            print(""read_frame:%f, infer time:%f, write time:%f"" % (read_frame_stamp - start_stamp,\n                                                                   inference_stamp - read_frame_stamp,\n                                                                   write_frame_stamp - inference_stamp))\n    # writer.release()\n\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""Face Mask Detection"")\n    parser.add_argument(\'--img-mode\', type=int, default=1, help=\'set 1 to run on image, 0 to run on video.\')\n    parser.add_argument(\'--img-path\', type=str, help=\'path to your image.\')\n    parser.add_argument(\'--video-path\', type=str, default=\'0\', help=\'path to your video, `0` means to use camera.\')\n    # parser.add_argument(\'--hdf5\', type=str, help=\'keras hdf5 file\')\n    args = parser.parse_args()\n    if args.img_mode:\n        imgPath = args.img_path\n        img = cv2.imread(imgPath)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        inference(img, show_result=True, target_shape=(260, 260))\n    else:\n        video_path = args.video_path\n        if args.video_path == \'0\':\n            video_path = 0\n        run_on_video(video_path, \'\', conf_thresh=0.5)\n'"
keras_infer.py,0,"b'# -*- coding:utf-8 -*-\nimport cv2\nimport time\nimport argparse\n\nimport numpy as np\nfrom PIL import Image\nfrom keras.models import model_from_json\nfrom utils.anchor_generator import generate_anchors\nfrom utils.anchor_decode import decode_bbox\nfrom utils.nms import single_class_non_max_suppression\nfrom load_model.keras_loader import load_keras_model, keras_inference\n\nmodel = load_keras_model(\'models/face_mask_detection.json\', \'models/face_mask_detection.hdf5\')\n\n# anchor configuration\nfeature_map_sizes = [[33, 33], [17, 17], [9, 9], [5, 5], [3, 3]]\nanchor_sizes = [[0.04, 0.056], [0.08, 0.11], [0.16, 0.22], [0.32, 0.45], [0.64, 0.72]]\nanchor_ratios = [[1, 0.62, 0.42]] * 5\n\n# generate anchors\nanchors = generate_anchors(feature_map_sizes, anchor_sizes, anchor_ratios)\n\n# for inference , the batch size is 1, the model output shape is [1, N, 4],\n# so we expand dim for anchors to [1, anchor_num, 4]\nanchors_exp = np.expand_dims(anchors, axis=0)\n\nid2class = {0: \'Mask\', 1: \'NoMask\'}\n\n\ndef inference(image,\n              conf_thresh=0.5,\n              iou_thresh=0.4,\n              target_shape=(160, 160),\n              draw_result=True,\n              show_result=True\n              ):\n    \'\'\'\n    Main function of detection inference\n    :param image: 3D numpy array of image\n    :param conf_thresh: the min threshold of classification probabity.\n    :param iou_thresh: the IOU threshold of NMS\n    :param target_shape: the model input size.\n    :param draw_result: whether to daw bounding box to the image.\n    :param show_result: whether to display the image.\n    :return:\n    \'\'\'\n    # image = np.copy(image)\n    output_info = []\n    height, width, _ = image.shape\n    image_resized = cv2.resize(image, target_shape)\n    image_np = image_resized / 255.0  # \xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe5\x88\xb00~1\n    image_exp = np.expand_dims(image_np, axis=0)\n\n    y_bboxes_output, y_cls_output = keras_inference(model, image_exp)\n    # remove the batch dimension, for batch is always 1 for inference.\n    y_bboxes = decode_bbox(anchors_exp, y_bboxes_output)[0]\n    y_cls = y_cls_output[0]\n    # To speed up, do single class NMS, not multiple classes NMS.\n    bbox_max_scores = np.max(y_cls, axis=1)\n    bbox_max_score_classes = np.argmax(y_cls, axis=1)\n\n    # keep_idx is the alive bounding box after nms.\n    keep_idxs = single_class_non_max_suppression(y_bboxes,\n                                                 bbox_max_scores,\n                                                 conf_thresh=conf_thresh,\n                                                 iou_thresh=iou_thresh,\n                                                 )\n\n    for idx in keep_idxs:\n        conf = float(bbox_max_scores[idx])\n        class_id = bbox_max_score_classes[idx]\n        bbox = y_bboxes[idx]\n        # clip the coordinate, avoid the value exceed the image boundary.\n        xmin = max(0, int(bbox[0] * width))\n        ymin = max(0, int(bbox[1] * height))\n        xmax = min(int(bbox[2] * width), width)\n        ymax = min(int(bbox[3] * height), height)\n\n        if draw_result:\n            if class_id == 0:\n                color = (0, 255, 0)\n            else:\n                color = (255, 0, 0)\n            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 2)\n            cv2.putText(image, ""%s: %.2f"" % (id2class[class_id], conf), (xmin + 2, ymin - 2),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, color)\n        output_info.append([class_id, conf, xmin, ymin, xmax, ymax])\n\n    if show_result:\n        Image.fromarray(image).show()\n    return output_info\n\n\ndef run_on_video(video_path, output_video_name, conf_thresh):\n    cap = cv2.VideoCapture(video_path)\n    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    fourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n    # writer = cv2.VideoWriter(output_video_name, fourcc, int(fps), (int(width), int(height)))\n    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    if not cap.isOpened():\n        raise ValueError(""Video open failed."")\n        return\n    status = True\n    idx = 0\n    while status:\n        start_stamp = time.time()\n        status, img_raw = cap.read()\n        img_raw = cv2.cvtColor(img_raw, cv2.COLOR_BGR2RGB)\n        read_frame_stamp = time.time()\n        if (status):\n            inference(img_raw,\n                      conf_thresh,\n                      iou_thresh=0.5,\n                      target_shape=(260, 260),\n                      draw_result=True,\n                      show_result=False)\n            cv2.imshow(\'image\', img_raw[:, :, ::-1])\n            cv2.waitKey(1)\n            inference_stamp = time.time()\n            # writer.write(img_raw)\n            write_frame_stamp = time.time()\n            idx += 1\n            print(""%d of %d"" % (idx, total_frames))\n            print(""read_frame:%f, infer time:%f, write time:%f"" % (read_frame_stamp - start_stamp,\n                                                                   inference_stamp - read_frame_stamp,\n                                                                   write_frame_stamp - inference_stamp))\n    # writer.release()\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""Face Mask Detection"")\n    parser.add_argument(\'--img-mode\', type=int, default=1, help=\'set 1 to run on image, 0 to run on video.\')\n    parser.add_argument(\'--img-path\', type=str, help=\'path to your image.\')\n    parser.add_argument(\'--video-path\', type=str, default=\'0\', help=\'path to your video, `0` means to use camera.\')\n    # parser.add_argument(\'--hdf5\', type=str, help=\'keras hdf5 file\')\n    args = parser.parse_args()\n    if args.img_mode:\n        imgPath = args.img_path\n        img = cv2.imread(imgPath)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        inference(img, show_result=True, target_shape=(260, 260))\n    else:\n        video_path = args.video_path\n        if args.video_path == \'0\':\n            video_path = 0\n        run_on_video(video_path, \'\', conf_thresh=0.5)\n'"
mxnet_infer.py,0,"b'# -*- coding:utf-8 -*-\nimport cv2\nimport time\n\nimport argparse\nimport numpy as np\nfrom PIL import Image\nfrom utils.anchor_generator import generate_anchors\nfrom utils.anchor_decode import decode_bbox\nfrom utils.nms import single_class_non_max_suppression\n\nfrom load_model.mxnet_loader import load_mxnet_model, mxnet_inference\n\nmodel = load_mxnet_model(\'models/face_mask_detection.params\')\n\n# anchor configuration\nfeature_map_sizes = [[33, 33], [17, 17], [9, 9], [5, 5], [3, 3]]\nanchor_sizes = [[0.04, 0.056], [0.08, 0.11], [0.16, 0.22], [0.32, 0.45], [0.64, 0.72]]\nanchor_ratios = [[1, 0.62, 0.42]] * 5\n\n# generate anchors\nanchors = generate_anchors(feature_map_sizes, anchor_sizes, anchor_ratios)\n\n# for inference , the batch size is 1, the model output shape is [1, N, 4],\n# so we expand dim for anchors to [1, anchor_num, 4]\nanchors_exp = np.expand_dims(anchors, axis=0)\n\nid2class = {0: \'Mask\', 1: \'NoMask\'}\n\n\ndef inference(image,\n              conf_thresh=0.5,\n              iou_thresh=0.4,\n              target_shape=(160, 160),\n              draw_result=True,\n              show_result=True\n              ):\n    \'\'\'\n    Main function of detection inference\n    :param image: 3D numpy array of image\n    :param conf_thresh: the min threshold of classification probabity.\n    :param iou_thresh: the IOU threshold of NMS\n    :param target_shape: the model input size.\n    :param draw_result: whether to daw bounding box to the image.\n    :param show_result: whether to display the image.\n    :return:\n    \'\'\'\n    # image = np.copy(image)\n    output_info = []\n    height, width, _ = image.shape\n    image_resized = cv2.resize(image, target_shape)\n    image_np = image_resized / 255.0  # \xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe5\x88\xb00~1\n    image_exp = np.expand_dims(image_np, axis=0)\n\n    image_transposed = image_exp.transpose((0, 3, 1, 2))\n\n    y_bboxes_output, y_cls_output = mxnet_inference(model, image_transposed)\n    # remove the batch dimension, for batch is always 1 for inference.\n    y_bboxes = decode_bbox(anchors_exp, y_bboxes_output)[0]\n    y_cls = y_cls_output[0]\n    # To speed up, do single class NMS, not multiple classes NMS.\n    bbox_max_scores = np.max(y_cls, axis=1)\n    bbox_max_score_classes = np.argmax(y_cls, axis=1)\n\n    # keep_idx is the alive bounding box after nms.\n    keep_idxs = single_class_non_max_suppression(y_bboxes,\n                                                 bbox_max_scores,\n                                                 conf_thresh=conf_thresh,\n                                                 iou_thresh=iou_thresh,\n                                                 )\n\n    for idx in keep_idxs:\n        conf = float(bbox_max_scores[idx])\n        class_id = bbox_max_score_classes[idx]\n        bbox = y_bboxes[idx]\n        # clip the coordinate, avoid the value exceed the image boundary.\n        xmin = max(0, int(bbox[0] * width))\n        ymin = max(0, int(bbox[1] * height))\n        xmax = min(int(bbox[2] * width), width)\n        ymax = min(int(bbox[3] * height), height)\n\n        if draw_result:\n            if class_id == 0:\n                color = (0, 255, 0)\n            else:\n                color = (255, 0, 0)\n            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 2)\n            cv2.putText(image, ""%s: %.2f"" % (id2class[class_id], conf), (xmin + 2, ymin - 2),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, color)\n        output_info.append([class_id, conf, xmin, ymin, xmax, ymax])\n\n    if show_result:\n        Image.fromarray(image).show()\n    return output_info\n\n\ndef run_on_video(video_path, output_video_name, conf_thresh):\n    cap = cv2.VideoCapture(video_path)\n    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    fourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n    # writer = cv2.VideoWriter(output_video_name, fourcc, int(fps), (int(width), int(height)))\n    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    if not cap.isOpened():\n        raise ValueError(""Video open failed."")\n        return\n    status = True\n    idx = 0\n    while status:\n        start_stamp = time.time()\n        status, img_raw = cap.read()\n        img_raw = cv2.cvtColor(img_raw, cv2.COLOR_BGR2RGB)\n        read_frame_stamp = time.time()\n        if (status):\n            inference(img_raw,\n                      conf_thresh,\n                      iou_thresh=0.5,\n                      target_shape=(260, 260),\n                      draw_result=True,\n                      show_result=False)\n            cv2.imshow(\'image\', img_raw[:, :, ::-1])\n            cv2.waitKey(1)\n            inference_stamp = time.time()\n            # writer.write(img_raw)\n            write_frame_stamp = time.time()\n            idx += 1\n            print(""%d of %d"" % (idx, total_frames))\n            print(""read_frame:%f, infer time:%f, write time:%f"" % (read_frame_stamp - start_stamp,\n                                                                   inference_stamp - read_frame_stamp,\n                                                                   write_frame_stamp - inference_stamp))\n    # writer.release()\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""Face Mask Detection"")\n    parser.add_argument(\'--img-mode\', type=int, default=1, help=\'set 1 to run on image, 0 to run on video.\')\n    parser.add_argument(\'--img-path\', type=str, help=\'path to your image.\')\n    parser.add_argument(\'--video-path\', type=str, default=\'0\', help=\'path to your video, `0` means to use camera.\')\n    # parser.add_argument(\'--hdf5\', type=str, help=\'keras hdf5 file\')\n    args = parser.parse_args()\n    if args.img_mode:\n        imgPath = args.img_path\n        img = cv2.imread(imgPath)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        inference(img, show_result=True, target_shape=(260, 260))\n    else:\n        video_path = args.video_path\n        if args.video_path == \'0\':\n            video_path = 0\n        run_on_video(video_path, \'\', conf_thresh=0.5)\n'"
pytorch_infer.py,0,"b'# -*- coding:utf-8 -*-\nimport cv2\nimport time\n\nimport argparse\nimport numpy as np\nfrom PIL import Image\nfrom utils.anchor_generator import generate_anchors\nfrom utils.anchor_decode import decode_bbox\nfrom utils.nms import single_class_non_max_suppression\nfrom load_model.pytorch_loader import load_pytorch_model, pytorch_inference\n\n# model = load_pytorch_model(\'models/face_mask_detection.pth\');\nmodel = load_pytorch_model(\'models/model360.pth\');\n# anchor configuration\n#feature_map_sizes = [[33, 33], [17, 17], [9, 9], [5, 5], [3, 3]]\nfeature_map_sizes = [[45, 45], [23, 23], [12, 12], [6, 6], [4, 4]]\nanchor_sizes = [[0.04, 0.056], [0.08, 0.11], [0.16, 0.22], [0.32, 0.45], [0.64, 0.72]]\nanchor_ratios = [[1, 0.62, 0.42]] * 5\n\n# generate anchors\nanchors = generate_anchors(feature_map_sizes, anchor_sizes, anchor_ratios)\n\n# for inference , the batch size is 1, the model output shape is [1, N, 4],\n# so we expand dim for anchors to [1, anchor_num, 4]\nanchors_exp = np.expand_dims(anchors, axis=0)\n\nid2class = {0: \'Mask\', 1: \'NoMask\'}\n\n\ndef inference(image,\n              conf_thresh=0.5,\n              iou_thresh=0.4,\n              target_shape=(160, 160),\n              draw_result=True,\n              show_result=True\n              ):\n    \'\'\'\n    Main function of detection inference\n    :param image: 3D numpy array of image\n    :param conf_thresh: the min threshold of classification probabity.\n    :param iou_thresh: the IOU threshold of NMS\n    :param target_shape: the model input size.\n    :param draw_result: whether to daw bounding box to the image.\n    :param show_result: whether to display the image.\n    :return:\n    \'\'\'\n    # image = np.copy(image)\n    output_info = []\n    height, width, _ = image.shape\n    image_resized = cv2.resize(image, target_shape)\n    image_np = image_resized / 255.0  # \xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe5\x88\xb00~1\n    image_exp = np.expand_dims(image_np, axis=0)\n\n    image_transposed = image_exp.transpose((0, 3, 1, 2))\n\n    y_bboxes_output, y_cls_output = pytorch_inference(model, image_transposed)\n    # remove the batch dimension, for batch is always 1 for inference.\n    y_bboxes = decode_bbox(anchors_exp, y_bboxes_output)[0]\n    y_cls = y_cls_output[0]\n    # To speed up, do single class NMS, not multiple classes NMS.\n    bbox_max_scores = np.max(y_cls, axis=1)\n    bbox_max_score_classes = np.argmax(y_cls, axis=1)\n\n    # keep_idx is the alive bounding box after nms.\n    keep_idxs = single_class_non_max_suppression(y_bboxes,\n                                                 bbox_max_scores,\n                                                 conf_thresh=conf_thresh,\n                                                 iou_thresh=iou_thresh,\n                                                 )\n\n    for idx in keep_idxs:\n        conf = float(bbox_max_scores[idx])\n        class_id = bbox_max_score_classes[idx]\n        bbox = y_bboxes[idx]\n        # clip the coordinate, avoid the value exceed the image boundary.\n        xmin = max(0, int(bbox[0] * width))\n        ymin = max(0, int(bbox[1] * height))\n        xmax = min(int(bbox[2] * width), width)\n        ymax = min(int(bbox[3] * height), height)\n\n        if draw_result:\n            if class_id == 0:\n                color = (0, 255, 0)\n            else:\n                color = (255, 0, 0)\n            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 2)\n            cv2.putText(image, ""%s: %.2f"" % (id2class[class_id], conf), (xmin + 2, ymin - 2),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, color)\n        output_info.append([class_id, conf, xmin, ymin, xmax, ymax])\n\n    if show_result:\n        Image.fromarray(image).show()\n    return output_info\n\n\ndef run_on_video(video_path, output_video_name, conf_thresh):\n    cap = cv2.VideoCapture(video_path)\n    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    fourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n    # writer = cv2.VideoWriter(output_video_name, fourcc, int(fps), (int(width), int(height)))\n    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    if not cap.isOpened():\n        raise ValueError(""Video open failed."")\n        return\n    status = True\n    idx = 0\n    while status:\n        start_stamp = time.time()\n        status, img_raw = cap.read()\n        img_raw = cv2.cvtColor(img_raw, cv2.COLOR_BGR2RGB)\n        read_frame_stamp = time.time()\n        if (status):\n            inference(img_raw,\n                      conf_thresh,\n                      iou_thresh=0.5,\n                      target_shape=(360, 360),\n                      draw_result=True,\n                      show_result=False)\n            cv2.imshow(\'image\', img_raw[:, :, ::-1])\n            cv2.waitKey(1)\n            inference_stamp = time.time()\n            # writer.write(img_raw)\n            write_frame_stamp = time.time()\n            idx += 1\n            print(""%d of %d"" % (idx, total_frames))\n            print(""read_frame:%f, infer time:%f, write time:%f"" % (read_frame_stamp - start_stamp,\n                                                                   inference_stamp - read_frame_stamp,\n                                                                   write_frame_stamp - inference_stamp))\n    # writer.release()\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""Face Mask Detection"")\n    parser.add_argument(\'--img-mode\', type=int, default=1, help=\'set 1 to run on image, 0 to run on video.\')\n    parser.add_argument(\'--img-path\', type=str, help=\'path to your image.\')\n    parser.add_argument(\'--video-path\', type=str, default=\'0\', help=\'path to your video, `0` means to use camera.\')\n    # parser.add_argument(\'--hdf5\', type=str, help=\'keras hdf5 file\')\n    args = parser.parse_args()\n    if args.img_mode:\n        imgPath = args.img_path\n        img = cv2.imread(imgPath)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        inference(img, show_result=True, target_shape=(360, 360))\n    else:\n        video_path = args.video_path\n        if args.video_path == \'0\':\n            video_path = 0\n        run_on_video(video_path, \'\', conf_thresh=0.5)\n'"
tensorflow_infer.py,0,"b'# -*- coding:utf-8 -*-\nimport cv2\nimport time\nimport argparse\n\nimport numpy as np\nfrom PIL import Image\n#from keras.models import model_from_json\nfrom utils.anchor_generator import generate_anchors\nfrom utils.anchor_decode import decode_bbox\nfrom utils.nms import single_class_non_max_suppression\nfrom load_model.tensorflow_loader import load_tf_model, tf_inference\n\nsess, graph = load_tf_model(\'models/face_mask_detection.pb\')\n# anchor configuration\nfeature_map_sizes = [[33, 33], [17, 17], [9, 9], [5, 5], [3, 3]]\nanchor_sizes = [[0.04, 0.056], [0.08, 0.11], [0.16, 0.22], [0.32, 0.45], [0.64, 0.72]]\nanchor_ratios = [[1, 0.62, 0.42]] * 5\n\n# generate anchors\nanchors = generate_anchors(feature_map_sizes, anchor_sizes, anchor_ratios)\n\n# for inference , the batch size is 1, the model output shape is [1, N, 4],\n# so we expand dim for anchors to [1, anchor_num, 4]\nanchors_exp = np.expand_dims(anchors, axis=0)\n\nid2class = {0: \'Mask\', 1: \'NoMask\'}\n\n\ndef inference(image,\n              conf_thresh=0.5,\n              iou_thresh=0.4,\n              target_shape=(160, 160),\n              draw_result=True,\n              show_result=True\n              ):\n    \'\'\'\n    Main function of detection inference\n    :param image: 3D numpy array of image\n    :param conf_thresh: the min threshold of classification probabity.\n    :param iou_thresh: the IOU threshold of NMS\n    :param target_shape: the model input size.\n    :param draw_result: whether to daw bounding box to the image.\n    :param show_result: whether to display the image.\n    :return:\n    \'\'\'\n    # image = np.copy(image)\n    output_info = []\n    height, width, _ = image.shape\n    image_resized = cv2.resize(image, target_shape)\n    image_np = image_resized / 255.0  # \xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe5\x88\xb00~1\n    image_exp = np.expand_dims(image_np, axis=0)\n    y_bboxes_output, y_cls_output = tf_inference(sess, graph, image_exp)\n\n    # remove the batch dimension, for batch is always 1 for inference.\n    y_bboxes = decode_bbox(anchors_exp, y_bboxes_output)[0]\n    y_cls = y_cls_output[0]\n    # To speed up, do single class NMS, not multiple classes NMS.\n    bbox_max_scores = np.max(y_cls, axis=1)\n    bbox_max_score_classes = np.argmax(y_cls, axis=1)\n\n    # keep_idx is the alive bounding box after nms.\n    keep_idxs = single_class_non_max_suppression(y_bboxes,\n                                                 bbox_max_scores,\n                                                 conf_thresh=conf_thresh,\n                                                 iou_thresh=iou_thresh,\n                                                 )\n\n    for idx in keep_idxs:\n        conf = float(bbox_max_scores[idx])\n        class_id = bbox_max_score_classes[idx]\n        bbox = y_bboxes[idx]\n        # clip the coordinate, avoid the value exceed the image boundary.\n        xmin = max(0, int(bbox[0] * width))\n        ymin = max(0, int(bbox[1] * height))\n        xmax = min(int(bbox[2] * width), width)\n        ymax = min(int(bbox[3] * height), height)\n\n        if draw_result:\n            if class_id == 0:\n                color = (0, 255, 0)\n            else:\n                color = (255, 0, 0)\n            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 2)\n            cv2.putText(image, ""%s: %.2f"" % (id2class[class_id], conf), (xmin + 2, ymin - 2),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, color)\n        output_info.append([class_id, conf, xmin, ymin, xmax, ymax])\n\n    if show_result:\n        Image.fromarray(image).show()\n    return output_info\n\n\ndef run_on_video(video_path, output_video_name, conf_thresh):\n    cap = cv2.VideoCapture(video_path)\n    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    fourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n    # writer = cv2.VideoWriter(output_video_name, fourcc, int(fps), (int(width), int(height)))\n    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    if not cap.isOpened():\n        raise ValueError(""Video open failed."")\n        return\n    status = True\n    idx = 0\n    while status:\n        start_stamp = time.time()\n        status, img_raw = cap.read()\n        img_raw = cv2.cvtColor(img_raw, cv2.COLOR_BGR2RGB)\n        read_frame_stamp = time.time()\n        if (status):\n            inference(img_raw,\n                      conf_thresh,\n                      iou_thresh=0.5,\n                      target_shape=(260, 260),\n                      draw_result=True,\n                      show_result=False)\n            cv2.imshow(\'image\', img_raw[:, :, ::-1])\n            cv2.waitKey(1)\n            inference_stamp = time.time()\n            # writer.write(img_raw)\n            write_frame_stamp = time.time()\n            idx += 1\n            print(""%d of %d"" % (idx, total_frames))\n            print(""read_frame:%f, infer time:%f, write time:%f"" % (read_frame_stamp - start_stamp,\n                                                                   inference_stamp - read_frame_stamp,\n                                                                   write_frame_stamp - inference_stamp))\n    # writer.release()\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""Face Mask Detection"")\n    parser.add_argument(\'--img-mode\', type=int, default=1, help=\'set 1 to run on image, 0 to run on video.\')\n    parser.add_argument(\'--img-path\', type=str, help=\'path to your image.\')\n    parser.add_argument(\'--video-path\', type=str, default=\'0\', help=\'path to your video, `0` means to use camera.\')\n    # parser.add_argument(\'--hdf5\', type=str, help=\'keras hdf5 file\')\n    args = parser.parse_args()\n    if args.img_mode:\n        imgPath = args.img_path\n        img = cv2.imread(imgPath)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        inference(img, show_result=True, target_shape=(260, 260))\n    else:\n        video_path = args.video_path\n        if args.video_path == \'0\':\n            video_path = 0\n        run_on_video(video_path, \'\', conf_thresh=0.5)\n'"
load_model/MainModel.py,20,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n__weights_dict = dict()\n\ndef load_weights(weight_file):\n    if weight_file == None:\n        return\n\n    try:\n        weights_dict = np.load(weight_file).item()\n    except:\n        weights_dict = np.load(weight_file, encoding='bytes').item()\n\n    return weights_dict\n\nclass KitModel(nn.Module):\n\n    def __init__(self, weight_file):\n        super(KitModel, self).__init__()\n        global __weights_dict\n        __weights_dict = load_weights(weight_file)\n\n        self.conv2d_0 = self.__conv(2, name='conv2d_0', in_channels=3, out_channels=32, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.conv2d_0_bn = self.__batch_normalization(2, 'conv2d_0_bn', num_features=32, eps=0.0010000000474974513, momentum=0.0)\n        self.conv2d_1 = self.__conv(2, name='conv2d_1', in_channels=32, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.conv2d_1_bn = self.__batch_normalization(2, 'conv2d_1_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.conv2d_2 = self.__conv(2, name='conv2d_2', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.conv2d_2_bn = self.__batch_normalization(2, 'conv2d_2_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.conv2d_3 = self.__conv(2, name='conv2d_3', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.conv2d_3_bn = self.__batch_normalization(2, 'conv2d_3_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.cls_0_insert_conv2d = self.__conv(2, name='cls_0_insert_conv2d', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.loc_0_insert_conv2d = self.__conv(2, name='loc_0_insert_conv2d', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.conv2d_4 = self.__conv(2, name='conv2d_4', in_channels=64, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.cls_0_insert_conv2d_bn = self.__batch_normalization(2, 'cls_0_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.loc_0_insert_conv2d_bn = self.__batch_normalization(2, 'loc_0_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.conv2d_4_bn = self.__batch_normalization(2, 'conv2d_4_bn', num_features=128, eps=0.0010000000474974513, momentum=0.0)\n        self.cls_0_conv = self.__conv(2, name='cls_0_conv', in_channels=64, out_channels=8, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n        self.loc_0_conv = self.__conv(2, name='loc_0_conv', in_channels=64, out_channels=16, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n        self.cls_1_insert_conv2d = self.__conv(2, name='cls_1_insert_conv2d', in_channels=128, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.loc_1_insert_conv2d = self.__conv(2, name='loc_1_insert_conv2d', in_channels=128, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.conv2d_5 = self.__conv(2, name='conv2d_5', in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.cls_1_insert_conv2d_bn = self.__batch_normalization(2, 'cls_1_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.loc_1_insert_conv2d_bn = self.__batch_normalization(2, 'loc_1_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.conv2d_5_bn = self.__batch_normalization(2, 'conv2d_5_bn', num_features=128, eps=0.0010000000474974513, momentum=0.0)\n        self.cls_1_conv = self.__conv(2, name='cls_1_conv', in_channels=64, out_channels=8, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n        self.loc_1_conv = self.__conv(2, name='loc_1_conv', in_channels=64, out_channels=16, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n        self.cls_2_insert_conv2d = self.__conv(2, name='cls_2_insert_conv2d', in_channels=128, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.loc_2_insert_conv2d = self.__conv(2, name='loc_2_insert_conv2d', in_channels=128, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.conv2d_6 = self.__conv(2, name='conv2d_6', in_channels=128, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.cls_2_insert_conv2d_bn = self.__batch_normalization(2, 'cls_2_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.loc_2_insert_conv2d_bn = self.__batch_normalization(2, 'loc_2_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.conv2d_6_bn = self.__batch_normalization(2, 'conv2d_6_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.cls_2_conv = self.__conv(2, name='cls_2_conv', in_channels=64, out_channels=8, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n        self.loc_2_conv = self.__conv(2, name='loc_2_conv', in_channels=64, out_channels=16, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n        self.conv2d_7 = self.__conv(2, name='conv2d_7', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.cls_3_insert_conv2d = self.__conv(2, name='cls_3_insert_conv2d', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.loc_3_insert_conv2d = self.__conv(2, name='loc_3_insert_conv2d', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.conv2d_7_bn = self.__batch_normalization(2, 'conv2d_7_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.cls_3_insert_conv2d_bn = self.__batch_normalization(2, 'cls_3_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.loc_3_insert_conv2d_bn = self.__batch_normalization(2, 'loc_3_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.cls_4_insert_conv2d = self.__conv(2, name='cls_4_insert_conv2d', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.loc_4_insert_conv2d = self.__conv(2, name='loc_4_insert_conv2d', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.cls_3_conv = self.__conv(2, name='cls_3_conv', in_channels=64, out_channels=8, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n        self.loc_3_conv = self.__conv(2, name='loc_3_conv', in_channels=64, out_channels=16, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n        self.cls_4_insert_conv2d_bn = self.__batch_normalization(2, 'cls_4_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.loc_4_insert_conv2d_bn = self.__batch_normalization(2, 'loc_4_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.cls_4_conv = self.__conv(2, name='cls_4_conv', in_channels=64, out_channels=8, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n        self.loc_4_conv = self.__conv(2, name='loc_4_conv', in_channels=64, out_channels=16, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n\n    def forward(self, x):\n        conv2d_0_pad    = F.pad(x, (1, 1, 1, 1))\n        conv2d_0        = self.conv2d_0(conv2d_0_pad)\n        conv2d_0_bn     = self.conv2d_0_bn(conv2d_0)\n        conv2d_0_activation = F.relu(conv2d_0_bn)\n        maxpool2d_0     = F.max_pool2d(conv2d_0_activation, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\n        conv2d_1_pad    = F.pad(maxpool2d_0, (1, 1, 1, 1))\n        conv2d_1        = self.conv2d_1(conv2d_1_pad)\n        conv2d_1_bn     = self.conv2d_1_bn(conv2d_1)\n        conv2d_1_activation = F.relu(conv2d_1_bn)\n        maxpool2d_1     = F.max_pool2d(conv2d_1_activation, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\n        conv2d_2_pad    = F.pad(maxpool2d_1, (1, 1, 1, 1))\n        conv2d_2        = self.conv2d_2(conv2d_2_pad)\n        conv2d_2_bn     = self.conv2d_2_bn(conv2d_2)\n        conv2d_2_activation = F.relu(conv2d_2_bn)\n        maxpool2d_2_pad = F.pad(conv2d_2_activation, (0, 1, 0, 1), value=float('-inf'))\n        maxpool2d_2     = F.max_pool2d(maxpool2d_2_pad, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\n        conv2d_3_pad    = F.pad(maxpool2d_2, (1, 1, 1, 1))\n        conv2d_3        = self.conv2d_3(conv2d_3_pad)\n        conv2d_3_bn     = self.conv2d_3_bn(conv2d_3)\n        conv2d_3_activation = F.relu(conv2d_3_bn)\n        maxpool2d_3_pad = F.pad(conv2d_3_activation, (0, 1, 0, 1), value=float('-inf'))\n        maxpool2d_3     = F.max_pool2d(maxpool2d_3_pad, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\n        cls_0_insert_conv2d_pad = F.pad(conv2d_3_activation, (1, 1, 1, 1))\n        cls_0_insert_conv2d = self.cls_0_insert_conv2d(cls_0_insert_conv2d_pad)\n        loc_0_insert_conv2d_pad = F.pad(conv2d_3_activation, (1, 1, 1, 1))\n        loc_0_insert_conv2d = self.loc_0_insert_conv2d(loc_0_insert_conv2d_pad)\n        conv2d_4_pad    = F.pad(maxpool2d_3, (1, 1, 1, 1))\n        conv2d_4        = self.conv2d_4(conv2d_4_pad)\n        cls_0_insert_conv2d_bn = self.cls_0_insert_conv2d_bn(cls_0_insert_conv2d)\n        loc_0_insert_conv2d_bn = self.loc_0_insert_conv2d_bn(loc_0_insert_conv2d)\n        conv2d_4_bn     = self.conv2d_4_bn(conv2d_4)\n        cls_0_insert_conv2d_activation = F.relu(cls_0_insert_conv2d_bn)\n        loc_0_insert_conv2d_activation = F.relu(loc_0_insert_conv2d_bn)\n        conv2d_4_activation = F.relu(conv2d_4_bn)\n        cls_0_conv_pad  = F.pad(cls_0_insert_conv2d_activation, (1, 1, 1, 1))\n        cls_0_conv      = self.cls_0_conv(cls_0_conv_pad)\n        loc_0_conv_pad  = F.pad(loc_0_insert_conv2d_activation, (1, 1, 1, 1))\n        loc_0_conv      = self.loc_0_conv(loc_0_conv_pad)\n        maxpool2d_4_pad = F.pad(conv2d_4_activation, (0, 1, 0, 1), value=float('-inf'))\n        maxpool2d_4     = F.max_pool2d(maxpool2d_4_pad, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\n        cls_1_insert_conv2d_pad = F.pad(conv2d_4_activation, (1, 1, 1, 1))\n        cls_1_insert_conv2d = self.cls_1_insert_conv2d(cls_1_insert_conv2d_pad)\n        loc_1_insert_conv2d_pad = F.pad(conv2d_4_activation, (1, 1, 1, 1))\n        loc_1_insert_conv2d = self.loc_1_insert_conv2d(loc_1_insert_conv2d_pad)\n        cls_0_reshape   = torch.reshape(input = cls_0_conv.permute(0,2,3,1) , shape = (cls_0_conv.size(0),-1,2))\n        loc_0_reshape   = torch.reshape(input = loc_0_conv.permute(0,2,3,1) , shape = (loc_0_conv.size(0),-1,4))\n        conv2d_5_pad    = F.pad(maxpool2d_4, (1, 1, 1, 1))\n        conv2d_5        = self.conv2d_5(conv2d_5_pad)\n        cls_1_insert_conv2d_bn = self.cls_1_insert_conv2d_bn(cls_1_insert_conv2d)\n        loc_1_insert_conv2d_bn = self.loc_1_insert_conv2d_bn(loc_1_insert_conv2d)\n        cls_0_activation = F.sigmoid(cls_0_reshape)\n        conv2d_5_bn     = self.conv2d_5_bn(conv2d_5)\n        cls_1_insert_conv2d_activation = F.relu(cls_1_insert_conv2d_bn)\n        loc_1_insert_conv2d_activation = F.relu(loc_1_insert_conv2d_bn)\n        conv2d_5_activation = F.relu(conv2d_5_bn)\n        cls_1_conv_pad  = F.pad(cls_1_insert_conv2d_activation, (1, 1, 1, 1))\n        cls_1_conv      = self.cls_1_conv(cls_1_conv_pad)\n        loc_1_conv_pad  = F.pad(loc_1_insert_conv2d_activation, (1, 1, 1, 1))\n        loc_1_conv      = self.loc_1_conv(loc_1_conv_pad)\n        maxpool2d_5_pad = F.pad(conv2d_5_activation, (0, 1, 0, 1), value=float('-inf'))\n        maxpool2d_5     = F.max_pool2d(maxpool2d_5_pad, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\n        cls_2_insert_conv2d_pad = F.pad(conv2d_5_activation, (1, 1, 1, 1))\n        cls_2_insert_conv2d = self.cls_2_insert_conv2d(cls_2_insert_conv2d_pad)\n        loc_2_insert_conv2d_pad = F.pad(conv2d_5_activation, (1, 1, 1, 1))\n        loc_2_insert_conv2d = self.loc_2_insert_conv2d(loc_2_insert_conv2d_pad)\n        cls_1_reshape   = torch.reshape(input = cls_1_conv.permute(0,2,3,1) , shape = (cls_1_conv.size(0),-1,2))\n        loc_1_reshape   = torch.reshape(input = loc_1_conv.permute(0,2,3,1) , shape = (loc_1_conv.size(0),-1,4))\n        conv2d_6_pad    = F.pad(maxpool2d_5, (1, 1, 1, 1))\n        conv2d_6        = self.conv2d_6(conv2d_6_pad)\n        cls_2_insert_conv2d_bn = self.cls_2_insert_conv2d_bn(cls_2_insert_conv2d)\n        loc_2_insert_conv2d_bn = self.loc_2_insert_conv2d_bn(loc_2_insert_conv2d)\n        cls_1_activation = F.sigmoid(cls_1_reshape)\n        conv2d_6_bn     = self.conv2d_6_bn(conv2d_6)\n        cls_2_insert_conv2d_activation = F.relu(cls_2_insert_conv2d_bn)\n        loc_2_insert_conv2d_activation = F.relu(loc_2_insert_conv2d_bn)\n        conv2d_6_activation = F.relu(conv2d_6_bn)\n        cls_2_conv_pad  = F.pad(cls_2_insert_conv2d_activation, (1, 1, 1, 1))\n        cls_2_conv      = self.cls_2_conv(cls_2_conv_pad)\n        loc_2_conv_pad  = F.pad(loc_2_insert_conv2d_activation, (1, 1, 1, 1))\n        loc_2_conv      = self.loc_2_conv(loc_2_conv_pad)\n        conv2d_7        = self.conv2d_7(conv2d_6_activation)\n        cls_3_insert_conv2d_pad = F.pad(conv2d_6_activation, (1, 1, 1, 1))\n        cls_3_insert_conv2d = self.cls_3_insert_conv2d(cls_3_insert_conv2d_pad)\n        loc_3_insert_conv2d_pad = F.pad(conv2d_6_activation, (1, 1, 1, 1))\n        loc_3_insert_conv2d = self.loc_3_insert_conv2d(loc_3_insert_conv2d_pad)\n        cls_2_reshape   = torch.reshape(input = cls_2_conv.permute(0,2,3,1) , shape = (cls_2_conv.size(0),-1,2))\n        loc_2_reshape   = torch.reshape(input = loc_2_conv.permute(0,2,3,1) , shape = (loc_2_conv.size(0),-1,4))\n        conv2d_7_bn     = self.conv2d_7_bn(conv2d_7)\n        cls_3_insert_conv2d_bn = self.cls_3_insert_conv2d_bn(cls_3_insert_conv2d)\n        loc_3_insert_conv2d_bn = self.loc_3_insert_conv2d_bn(loc_3_insert_conv2d)\n        cls_2_activation = F.sigmoid(cls_2_reshape)\n        conv2d_7_activation = F.relu(conv2d_7_bn)\n        cls_3_insert_conv2d_activation = F.relu(cls_3_insert_conv2d_bn)\n        loc_3_insert_conv2d_activation = F.relu(loc_3_insert_conv2d_bn)\n        cls_4_insert_conv2d_pad = F.pad(conv2d_7_activation, (1, 1, 1, 1))\n        cls_4_insert_conv2d = self.cls_4_insert_conv2d(cls_4_insert_conv2d_pad)\n        loc_4_insert_conv2d_pad = F.pad(conv2d_7_activation, (1, 1, 1, 1))\n        loc_4_insert_conv2d = self.loc_4_insert_conv2d(loc_4_insert_conv2d_pad)\n        cls_3_conv_pad  = F.pad(cls_3_insert_conv2d_activation, (1, 1, 1, 1))\n        cls_3_conv      = self.cls_3_conv(cls_3_conv_pad)\n        loc_3_conv_pad  = F.pad(loc_3_insert_conv2d_activation, (1, 1, 1, 1))\n        loc_3_conv      = self.loc_3_conv(loc_3_conv_pad)\n        cls_4_insert_conv2d_bn = self.cls_4_insert_conv2d_bn(cls_4_insert_conv2d)\n        loc_4_insert_conv2d_bn = self.loc_4_insert_conv2d_bn(loc_4_insert_conv2d)\n        cls_3_reshape   = torch.reshape(input = cls_3_conv.permute(0,2,3,1) , shape = (cls_3_conv.size(0),-1,2))\n        loc_3_reshape   = torch.reshape(input = loc_3_conv.permute(0,2,3,1) , shape = (loc_3_conv.size(0),-1,4))\n        cls_4_insert_conv2d_activation = F.relu(cls_4_insert_conv2d_bn)\n        loc_4_insert_conv2d_activation = F.relu(loc_4_insert_conv2d_bn)\n        cls_3_activation = F.sigmoid(cls_3_reshape)\n        cls_4_conv_pad  = F.pad(cls_4_insert_conv2d_activation, (1, 1, 1, 1))\n        cls_4_conv      = self.cls_4_conv(cls_4_conv_pad)\n        loc_4_conv_pad  = F.pad(loc_4_insert_conv2d_activation, (1, 1, 1, 1))\n        loc_4_conv      = self.loc_4_conv(loc_4_conv_pad)\n        cls_4_reshape   = torch.reshape(input = cls_4_conv.permute(0,2,3,1) , shape = (cls_4_conv.size(0),-1,2))\n        loc_4_reshape   = torch.reshape(input = loc_4_conv.permute(0,2,3,1) , shape = (loc_4_conv.size(0),-1,4))\n        cls_4_activation = F.sigmoid(cls_4_reshape)\n        loc_branch_concat = torch.cat((loc_0_reshape, loc_1_reshape, loc_2_reshape, loc_3_reshape, loc_4_reshape), 1)\n        cls_branch_concat = torch.cat((cls_0_activation, cls_1_activation, cls_2_activation, cls_3_activation, cls_4_activation), 1)\n        return loc_branch_concat, cls_branch_concat\n\n\n    @staticmethod\n    def __batch_normalization(dim, name, **kwargs):\n        if   dim == 0 or dim == 1:  layer = nn.BatchNorm1d(**kwargs)\n        elif dim == 2:  layer = nn.BatchNorm2d(**kwargs)\n        elif dim == 3:  layer = nn.BatchNorm3d(**kwargs)\n        else:           raise NotImplementedError()\n\n        if 'scale' in __weights_dict[name]:\n            layer.state_dict()['weight'].copy_(torch.from_numpy(__weights_dict[name]['scale']))\n        else:\n            layer.weight.data.fill_(1)\n\n        if 'bias' in __weights_dict[name]:\n            layer.state_dict()['bias'].copy_(torch.from_numpy(__weights_dict[name]['bias']))\n        else:\n            layer.bias.data.fill_(0)\n\n        layer.state_dict()['running_mean'].copy_(torch.from_numpy(__weights_dict[name]['mean']))\n        layer.state_dict()['running_var'].copy_(torch.from_numpy(__weights_dict[name]['var']))\n        return layer\n\n    @staticmethod\n    def __conv(dim, name, **kwargs):\n        if   dim == 1:  layer = nn.Conv1d(**kwargs)\n        elif dim == 2:  layer = nn.Conv2d(**kwargs)\n        elif dim == 3:  layer = nn.Conv3d(**kwargs)\n        else:           raise NotImplementedError()\n\n        layer.state_dict()['weight'].copy_(torch.from_numpy(__weights_dict[name]['weights']))\n        if 'bias' in __weights_dict[name]:\n            layer.state_dict()['bias'].copy_(torch.from_numpy(__weights_dict[name]['bias']))\n        return layer\n\n"""
load_model/__init__.py,0,b''
load_model/caffe_loader.py,0,"b""import caffe\nimport numpy as np\n\ndef load_caffe_model(prototxt_path, caffemodel_path):\n    model = caffe.Net(prototxt_path, caffemodel_path, caffe.TEST)\n    return model\n\ndef caffe_inference(model, img_arr):\n    model.blobs['data'].data[...] = img_arr\n    result = model.forward() # \xe8\xbe\x93\xe5\x87\xba\xe5\x9b\x9b\xe4\xb8\xaa\xe5\x88\x86\xe6\x94\xaf\n    y_bboxes = result['loc_branch_concat']\n    y_scores = result['cls_branch_concat']\n    return y_bboxes, y_scores"""
load_model/keras_loader.py,0,"b""from keras.models import model_from_json\n\ndef load_keras_model(json_path, weight_path):\n    model = model_from_json(open('models/face_mask_detection.json').read())\n    model.load_weights('models/face_mask_detection.hdf5')\n    return model\n\n\ndef keras_inference(model, img_arr):\n    result = model.predict(img_arr)\n    y_bboxes= result[0]\n    y_scores = result[1]\n    return y_bboxes, y_scores"""
load_model/mxnet_loader.py,0,"b'from load_model.mxnet_model_structure import SSD\nimport mxnet.ndarray as F\nfrom mxnet import  gluon,nd\nfrom mxnet.gluon import nn\n\n\ndef cls_predictor(num_anchors, num_classes, idx):\n    blk = nn.Sequential()\n    blk.add(nn.Conv2D(64, kernel_size=3, padding=1, prefix= \'cls_%d_insert_conv2d\' % idx, activation=\'relu\'))\n    blk.add(nn.Conv2D(num_anchors * num_classes, kernel_size=3, padding=1, prefix=\'cls_%d_conv\' %idx))\n    return blk\n\n\ndef loc_predictor(num_anchors, num_classes, idx):\n    blk = nn.Sequential()\n    blk.add(nn.Conv2D(64, kernel_size=3, padding=1, prefix= \'loc_%d_insert_conv2d\' % idx, activation=\'relu\'))\n    blk.add(nn.Conv2D(num_anchors * 4, kernel_size=3, padding=1, prefix=\'loc_%d_conv\' %idx))\n    return blk\n\ndef down_sample_blk(num_channels, layer_idx):\n    blk = nn.Sequential()\n    blk.add(nn.MaxPool2D(2, prefix=\'maxpool2d_%d\' % (layer_idx - 1)))\n    blk.add(nn.Conv2D(num_channels, kernel_size=3, padding=1, prefix=""conv2d_%d"" % layer_idx),\n                nn.Activation(\'relu\', prefix= \'conv2d_%d_activation\' % layer_idx))\n    return blk\n\n\nclass SSD(gluon.Block):\n    def __init__(self, **kwargs):\n        super(SSD, self).__init__(**kwargs)\n        self.filters = [32, 64, 64, 64, 128, 128, 64, 64]\n        self.conv_0 = nn.Conv2D(self.filters[0], kernel_size=(3, 3), prefix=""conv2d_0"", padding=(1, 1), activation=\'relu\')\n        for i in range(1, 7):\n            setattr(self, \'conv_%d\' % i, down_sample_blk(self.filters[i], i))\n        self.conv_7 = nn.Conv2D(self.filters[7], kernel_size=(3, 3), prefix=""conv2d_7"",padding=(0, 0), activation=\'relu\')\n        for i in range(5):\n            setattr(self, \'cls_%d_conv\' % i, cls_predictor(4, 2, i))\n            setattr(self, \'loc_%d_conv\' %i, loc_predictor(4, 2, i))\n\n    def forward(self, x):\n        cls_preds, loc_preds = [None] * 5, [None] * 5\n        x = self.conv_0(x)\n\n        for i in range(1, 8):\n            x = getattr(self, \'conv_%d\' % i)(x)\n            if i in [3, 4, 5, 6, 7]:\n                cls_x = getattr(self, \'cls_%d_conv\' % (i-3))(x)\n                cls_preds[i-3] =F.sigmoid(cls_x.transpose((0,2,3,1)).reshape((0,-1,2)))\n                loc_x = getattr(self, \'loc_%d_conv\' % (i-3))(x)\n                loc_preds[i-3]  = loc_x.transpose((0,2,3,1)).reshape((0,-1,4))\n\n            if i in [2,3,4,5]:\n                x = F.Pad(x,pad_width=(0,0,0,0,0,1,0,1),mode=\'edge\')\n        return nd.concat(*cls_preds, dim=1), nd.concat(*loc_preds, dim=1)\n\n\ndef copy_weight(caffenet, gluonnet):\n    gluon_weights = gluonnet.collect_params()\n    for key in caffenet.params.keys():\n        layer = caffenet.params[key]\n        weight = layer[0].data\n        bias = layer[1].data\n        gluon_weights[key + \'weight\'].set_data(weight)\n        gluon_weights[key + \'bias\'].set_data(bias)\n        print(\'set weights for %s\' % key)\n        print(""caffe weight"", weight.sum(), bias.sum())\n        print(""gluon weight"", gluon_weights[key + \'weight\'].data().sum(),\n              gluon_weights[key + \'bias\'].data().sum())\n\n\ndef load_mxnet_model(weight_path):\n    ssd = SSD()\n    ssd.load_parameters(weight_path)\n    return ssd\n\ndef mxnet_inference(model, img_arr):\n    y_scores, y_bboxes = model.forward(nd.array(img_arr))\n    return y_bboxes.asnumpy(), y_scores.asnumpy()'"
load_model/mxnet_model_structure.py,0,"b'import mxnet.ndarray as F\nfrom mxnet import  gluon,nd\nfrom mxnet.gluon import nn\n\n\ndef cls_predictor(num_anchors, num_classes, idx):\n    blk = nn.Sequential()\n    blk.add(nn.Conv2D(64, kernel_size=3, padding=1, prefix= \'cls_%d_insert_conv2d\' % idx, activation=\'relu\'))\n    blk.add(nn.Conv2D(num_anchors * num_classes, kernel_size=3, padding=1, prefix=\'cls_%d_conv\' %idx))\n    return blk\n\n\ndef loc_predictor(num_anchors, num_classes, idx):\n    blk = nn.Sequential()\n    blk.add(nn.Conv2D(64, kernel_size=3, padding=1, prefix= \'loc_%d_insert_conv2d\' % idx, activation=\'relu\'))\n    blk.add(nn.Conv2D(num_anchors * 4, kernel_size=3, padding=1, prefix=\'loc_%d_conv\' %idx))\n    return blk\n\ndef down_sample_blk(num_channels, layer_idx):\n    blk = nn.Sequential()\n    blk.add(nn.MaxPool2D(2, prefix=\'maxpool2d_%d\' % (layer_idx - 1)))\n    blk.add(nn.Conv2D(num_channels, kernel_size=3, padding=1, prefix=""conv2d_%d"" % layer_idx),\n                nn.Activation(\'relu\', prefix= \'conv2d_%d_activation\' % layer_idx))\n    return blk\n\n\nclass SSD(gluon.Block):\n    def __init__(self, **kwargs):\n        super(SSD, self).__init__(**kwargs)\n        self.filters = [32, 64, 64, 64, 128, 128, 64, 64]\n        self.conv_0 = nn.Conv2D(self.filters[0], kernel_size=(3, 3), prefix=""conv2d_0"", padding=(1, 1), activation=\'relu\')\n        for i in range(1, 7):\n            setattr(self, \'conv_%d\' % i, down_sample_blk(self.filters[i], i))\n        self.conv_7 = nn.Conv2D(self.filters[7], kernel_size=(3, 3), prefix=""conv2d_7"",padding=(0, 0), activation=\'relu\')\n        for i in range(5):\n            setattr(self, \'cls_%d_conv\' % i, cls_predictor(4, 2, i))\n            setattr(self, \'loc_%d_conv\' %i, loc_predictor(4, 2, i))\n\n    def forward(self, x):\n        cls_preds, loc_preds = [None] * 5, [None] * 5\n        x = self.conv_0(x)\n\n        for i in range(1, 8):\n            x = getattr(self, \'conv_%d\' % i)(x)\n            if i in [3, 4, 5, 6, 7]:\n                cls_x = getattr(self, \'cls_%d_conv\' % (i-3))(x)\n                cls_preds[i-3] =F.sigmoid(cls_x.transpose((0,2,3,1)).reshape((0,-1,2)))\n                loc_x = getattr(self, \'loc_%d_conv\' % (i-3))(x)\n                loc_preds[i-3]  = loc_x.transpose((0,2,3,1)).reshape((0,-1,4))\n\n            if i in [2,3,4,5]:\n                x = F.Pad(x,pad_width=(0,0,0,0,0,1,0,1),mode=\'edge\')\n        return nd.concat(*cls_preds, dim=1), nd.concat(*loc_preds, dim=1)\n\n\ndef copy_weight(caffenet, gluonnet):\n    gluon_weights = gluonnet.collect_params()\n    for key in caffenet.params.keys():\n        layer = caffenet.params[key]\n        weight = layer[0].data\n        bias = layer[1].data\n        gluon_weights[key + \'weight\'].set_data(weight)\n        gluon_weights[key + \'bias\'].set_data(bias)\n        print(\'set weights for %s\' % key)\n        print(""caffe weight"", weight.sum(), bias.sum())\n        print(""gluon weight"", gluon_weights[key + \'weight\'].data().sum(),\n              gluon_weights[key + \'bias\'].data().sum())\n\n'"
load_model/pytorch_loader.py,4,"b""import sys\nimport torch\nsys.path.append('models/')\n\ndef load_pytorch_model(model_path):\n    model = torch.load(model_path)\n    return model\n\ndef pytorch_inference(model, img_arr):\n    if torch.cuda.is_available():\n        dev = 'cuda:0'\n    else:\n        dev = 'cpu'\n    device = torch.device(dev)\n    model.to(device)\n    input_tensor = torch.tensor(img_arr).float().to(device)\n    y_bboxes, y_scores, = model.forward(input_tensor)\n    return y_bboxes.detach().cpu().numpy(), y_scores.detach().cpu().numpy()\n"""
load_model/tensorflow_loader.py,0,"b""# -*- encoding=utf-8 -*-\nimport tensorflow as tf\nif tf.__version__ > '2':\n    import tensorflow.compat.v1 as tf  \n\nimport numpy as np\n\nPATH_TO_TENSORFLOW_MODEL = 'models/face_mask_detection.pb'\n\ndef load_tf_model(tf_model_path):\n    '''\n    Load the model.\n    :param tf_model_path: model to tensorflow model.\n    :return: session and graph\n    '''\n    detection_graph = tf.Graph()\n    with detection_graph.as_default():\n        od_graph_def = tf.GraphDef()\n        with tf.gfile.GFile(tf_model_path, 'rb') as fid:\n            serialized_graph = fid.read()\n            od_graph_def.ParseFromString(serialized_graph)\n            tf.import_graph_def(od_graph_def, name='')\n            with detection_graph.as_default():\n                sess = tf.Session(graph=detection_graph)\n                return sess, detection_graph\n\n\ndef tf_inference(sess, detection_graph, img_arr):\n    '''\n    Receive an image array and run inference\n    :param sess: tensorflow session.\n    :param detection_graph: tensorflow graph.\n    :param img_arr: 3D numpy array, RGB order.\n    :return:\n    '''\n    image_tensor = detection_graph.get_tensor_by_name('data_1:0')\n    detection_bboxes = detection_graph.get_tensor_by_name('loc_branch_concat_1/concat:0')\n    detection_scores = detection_graph.get_tensor_by_name('cls_branch_concat_1/concat:0')\n    # image_np_expanded = np.expand_dims(img_arr, axis=0)\n    bboxes, scores = sess.run([detection_bboxes, detection_scores],\n                            feed_dict={image_tensor: img_arr})\n\n    return bboxes, scores\n\n"""
models/MainModel.py,20,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n__weights_dict = dict()\n\ndef load_weights(weight_file):\n    if weight_file == None:\n        return\n\n    try:\n        weights_dict = np.load(weight_file).item()\n    except:\n        weights_dict = np.load(weight_file, encoding='bytes').item()\n\n    return weights_dict\n\nclass KitModel(nn.Module):\n\n    def __init__(self, weight_file):\n        super(KitModel, self).__init__()\n        global __weights_dict\n        __weights_dict = load_weights(weight_file)\n\n        self.conv2d_0 = self.__conv(2, name='conv2d_0', in_channels=3, out_channels=32, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.conv2d_0_bn = self.__batch_normalization(2, 'conv2d_0_bn', num_features=32, eps=0.0010000000474974513, momentum=0.0)\n        self.conv2d_1 = self.__conv(2, name='conv2d_1', in_channels=32, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.conv2d_1_bn = self.__batch_normalization(2, 'conv2d_1_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.conv2d_2 = self.__conv(2, name='conv2d_2', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.conv2d_2_bn = self.__batch_normalization(2, 'conv2d_2_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.conv2d_3 = self.__conv(2, name='conv2d_3', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.conv2d_3_bn = self.__batch_normalization(2, 'conv2d_3_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.cls_0_insert_conv2d = self.__conv(2, name='cls_0_insert_conv2d', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.loc_0_insert_conv2d = self.__conv(2, name='loc_0_insert_conv2d', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.conv2d_4 = self.__conv(2, name='conv2d_4', in_channels=64, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.cls_0_insert_conv2d_bn = self.__batch_normalization(2, 'cls_0_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.loc_0_insert_conv2d_bn = self.__batch_normalization(2, 'loc_0_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.conv2d_4_bn = self.__batch_normalization(2, 'conv2d_4_bn', num_features=128, eps=0.0010000000474974513, momentum=0.0)\n        self.cls_0_conv = self.__conv(2, name='cls_0_conv', in_channels=64, out_channels=8, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n        self.loc_0_conv = self.__conv(2, name='loc_0_conv', in_channels=64, out_channels=16, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n        self.cls_1_insert_conv2d = self.__conv(2, name='cls_1_insert_conv2d', in_channels=128, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.loc_1_insert_conv2d = self.__conv(2, name='loc_1_insert_conv2d', in_channels=128, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.conv2d_5 = self.__conv(2, name='conv2d_5', in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.cls_1_insert_conv2d_bn = self.__batch_normalization(2, 'cls_1_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.loc_1_insert_conv2d_bn = self.__batch_normalization(2, 'loc_1_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.conv2d_5_bn = self.__batch_normalization(2, 'conv2d_5_bn', num_features=128, eps=0.0010000000474974513, momentum=0.0)\n        self.cls_1_conv = self.__conv(2, name='cls_1_conv', in_channels=64, out_channels=8, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n        self.loc_1_conv = self.__conv(2, name='loc_1_conv', in_channels=64, out_channels=16, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n        self.cls_2_insert_conv2d = self.__conv(2, name='cls_2_insert_conv2d', in_channels=128, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.loc_2_insert_conv2d = self.__conv(2, name='loc_2_insert_conv2d', in_channels=128, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.conv2d_6 = self.__conv(2, name='conv2d_6', in_channels=128, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.cls_2_insert_conv2d_bn = self.__batch_normalization(2, 'cls_2_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.loc_2_insert_conv2d_bn = self.__batch_normalization(2, 'loc_2_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.conv2d_6_bn = self.__batch_normalization(2, 'conv2d_6_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.cls_2_conv = self.__conv(2, name='cls_2_conv', in_channels=64, out_channels=8, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n        self.loc_2_conv = self.__conv(2, name='loc_2_conv', in_channels=64, out_channels=16, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n        self.conv2d_7 = self.__conv(2, name='conv2d_7', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.cls_3_insert_conv2d = self.__conv(2, name='cls_3_insert_conv2d', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.loc_3_insert_conv2d = self.__conv(2, name='loc_3_insert_conv2d', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.conv2d_7_bn = self.__batch_normalization(2, 'conv2d_7_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.cls_3_insert_conv2d_bn = self.__batch_normalization(2, 'cls_3_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.loc_3_insert_conv2d_bn = self.__batch_normalization(2, 'loc_3_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.cls_4_insert_conv2d = self.__conv(2, name='cls_4_insert_conv2d', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.loc_4_insert_conv2d = self.__conv(2, name='loc_4_insert_conv2d', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)\n        self.cls_3_conv = self.__conv(2, name='cls_3_conv', in_channels=64, out_channels=8, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n        self.loc_3_conv = self.__conv(2, name='loc_3_conv', in_channels=64, out_channels=16, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n        self.cls_4_insert_conv2d_bn = self.__batch_normalization(2, 'cls_4_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.loc_4_insert_conv2d_bn = self.__batch_normalization(2, 'loc_4_insert_conv2d_bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)\n        self.cls_4_conv = self.__conv(2, name='cls_4_conv', in_channels=64, out_channels=8, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n        self.loc_4_conv = self.__conv(2, name='loc_4_conv', in_channels=64, out_channels=16, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n\n    def forward(self, x):\n        conv2d_0_pad    = F.pad(x, (1, 1, 1, 1))\n        conv2d_0        = self.conv2d_0(conv2d_0_pad)\n        conv2d_0_bn     = self.conv2d_0_bn(conv2d_0)\n        conv2d_0_activation = F.relu(conv2d_0_bn)\n        maxpool2d_0     = F.max_pool2d(conv2d_0_activation, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\n        conv2d_1_pad    = F.pad(maxpool2d_0, (1, 1, 1, 1))\n        conv2d_1        = self.conv2d_1(conv2d_1_pad)\n        conv2d_1_bn     = self.conv2d_1_bn(conv2d_1)\n        conv2d_1_activation = F.relu(conv2d_1_bn)\n        maxpool2d_1     = F.max_pool2d(conv2d_1_activation, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\n        conv2d_2_pad    = F.pad(maxpool2d_1, (1, 1, 1, 1))\n        conv2d_2        = self.conv2d_2(conv2d_2_pad)\n        conv2d_2_bn     = self.conv2d_2_bn(conv2d_2)\n        conv2d_2_activation = F.relu(conv2d_2_bn)\n        #maxpool2d_2_pad = F.pad(conv2d_2_activation, (0, 1, 0, 1), value=float('-inf'))\n        maxpool2d_2     = F.max_pool2d(conv2d_2_activation, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\n        conv2d_3_pad    = F.pad(maxpool2d_2, (1, 1, 1, 1))\n        conv2d_3        = self.conv2d_3(conv2d_3_pad)\n        conv2d_3_bn     = self.conv2d_3_bn(conv2d_3)\n        conv2d_3_activation = F.relu(conv2d_3_bn)\n        maxpool2d_3_pad = F.pad(conv2d_3_activation, (0, 1, 0, 1), value=float('-inf'))\n        maxpool2d_3     = F.max_pool2d(maxpool2d_3_pad, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\n        cls_0_insert_conv2d_pad = F.pad(conv2d_3_activation, (1, 1, 1, 1))\n        cls_0_insert_conv2d = self.cls_0_insert_conv2d(cls_0_insert_conv2d_pad)\n        loc_0_insert_conv2d_pad = F.pad(conv2d_3_activation, (1, 1, 1, 1))\n        loc_0_insert_conv2d = self.loc_0_insert_conv2d(loc_0_insert_conv2d_pad)\n        conv2d_4_pad    = F.pad(maxpool2d_3, (1, 1, 1, 1))\n        conv2d_4        = self.conv2d_4(conv2d_4_pad)\n        cls_0_insert_conv2d_bn = self.cls_0_insert_conv2d_bn(cls_0_insert_conv2d)\n        loc_0_insert_conv2d_bn = self.loc_0_insert_conv2d_bn(loc_0_insert_conv2d)\n        conv2d_4_bn     = self.conv2d_4_bn(conv2d_4)\n        cls_0_insert_conv2d_activation = F.relu(cls_0_insert_conv2d_bn)\n        loc_0_insert_conv2d_activation = F.relu(loc_0_insert_conv2d_bn)\n        conv2d_4_activation = F.relu(conv2d_4_bn)\n        cls_0_conv_pad  = F.pad(cls_0_insert_conv2d_activation, (1, 1, 1, 1))\n        cls_0_conv      = self.cls_0_conv(cls_0_conv_pad)\n        loc_0_conv_pad  = F.pad(loc_0_insert_conv2d_activation, (1, 1, 1, 1))\n        loc_0_conv      = self.loc_0_conv(loc_0_conv_pad)\n        maxpool2d_4_pad = F.pad(conv2d_4_activation, (0, 1, 0, 1), value=float('-inf'))\n        maxpool2d_4     = F.max_pool2d(maxpool2d_4_pad, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\n        cls_1_insert_conv2d_pad = F.pad(conv2d_4_activation, (1, 1, 1, 1))\n        cls_1_insert_conv2d = self.cls_1_insert_conv2d(cls_1_insert_conv2d_pad)\n        loc_1_insert_conv2d_pad = F.pad(conv2d_4_activation, (1, 1, 1, 1))\n        loc_1_insert_conv2d = self.loc_1_insert_conv2d(loc_1_insert_conv2d_pad)\n        cls_0_reshape   = torch.reshape(input = cls_0_conv.permute(0,2,3,1) , shape = (cls_0_conv.size(0),-1,2))\n        loc_0_reshape   = torch.reshape(input = loc_0_conv.permute(0,2,3,1) , shape = (loc_0_conv.size(0),-1,4))\n        conv2d_5_pad    = F.pad(maxpool2d_4, (1, 1, 1, 1))\n        conv2d_5        = self.conv2d_5(conv2d_5_pad)\n        cls_1_insert_conv2d_bn = self.cls_1_insert_conv2d_bn(cls_1_insert_conv2d)\n        loc_1_insert_conv2d_bn = self.loc_1_insert_conv2d_bn(loc_1_insert_conv2d)\n        cls_0_activation = F.sigmoid(cls_0_reshape)\n        conv2d_5_bn     = self.conv2d_5_bn(conv2d_5)\n        cls_1_insert_conv2d_activation = F.relu(cls_1_insert_conv2d_bn)\n        loc_1_insert_conv2d_activation = F.relu(loc_1_insert_conv2d_bn)\n        conv2d_5_activation = F.relu(conv2d_5_bn)\n        cls_1_conv_pad  = F.pad(cls_1_insert_conv2d_activation, (1, 1, 1, 1))\n        cls_1_conv      = self.cls_1_conv(cls_1_conv_pad)\n        loc_1_conv_pad  = F.pad(loc_1_insert_conv2d_activation, (1, 1, 1, 1))\n        loc_1_conv      = self.loc_1_conv(loc_1_conv_pad)\n        maxpool2d_5_pad = F.pad(conv2d_5_activation, (0, 1, 0, 1), value=float('-inf'))\n        maxpool2d_5     = F.max_pool2d(maxpool2d_5_pad, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\n        cls_2_insert_conv2d_pad = F.pad(conv2d_5_activation, (1, 1, 1, 1))\n        cls_2_insert_conv2d = self.cls_2_insert_conv2d(cls_2_insert_conv2d_pad)\n        loc_2_insert_conv2d_pad = F.pad(conv2d_5_activation, (1, 1, 1, 1))\n        loc_2_insert_conv2d = self.loc_2_insert_conv2d(loc_2_insert_conv2d_pad)\n        cls_1_reshape   = torch.reshape(input = cls_1_conv.permute(0,2,3,1) , shape = (cls_1_conv.size(0),-1,2))\n        loc_1_reshape   = torch.reshape(input = loc_1_conv.permute(0,2,3,1) , shape = (loc_1_conv.size(0),-1,4))\n        conv2d_6_pad    = F.pad(maxpool2d_5, (1, 1, 1, 1))\n        conv2d_6        = self.conv2d_6(conv2d_6_pad)\n        cls_2_insert_conv2d_bn = self.cls_2_insert_conv2d_bn(cls_2_insert_conv2d)\n        loc_2_insert_conv2d_bn = self.loc_2_insert_conv2d_bn(loc_2_insert_conv2d)\n        cls_1_activation = F.sigmoid(cls_1_reshape)\n        conv2d_6_bn     = self.conv2d_6_bn(conv2d_6)\n        cls_2_insert_conv2d_activation = F.relu(cls_2_insert_conv2d_bn)\n        loc_2_insert_conv2d_activation = F.relu(loc_2_insert_conv2d_bn)\n        conv2d_6_activation = F.relu(conv2d_6_bn)\n        cls_2_conv_pad  = F.pad(cls_2_insert_conv2d_activation, (1, 1, 1, 1))\n        cls_2_conv      = self.cls_2_conv(cls_2_conv_pad)\n        loc_2_conv_pad  = F.pad(loc_2_insert_conv2d_activation, (1, 1, 1, 1))\n        loc_2_conv      = self.loc_2_conv(loc_2_conv_pad)\n        conv2d_7        = self.conv2d_7(conv2d_6_activation)\n        cls_3_insert_conv2d_pad = F.pad(conv2d_6_activation, (1, 1, 1, 1))\n        cls_3_insert_conv2d = self.cls_3_insert_conv2d(cls_3_insert_conv2d_pad)\n        loc_3_insert_conv2d_pad = F.pad(conv2d_6_activation, (1, 1, 1, 1))\n        loc_3_insert_conv2d = self.loc_3_insert_conv2d(loc_3_insert_conv2d_pad)\n        cls_2_reshape   = torch.reshape(input = cls_2_conv.permute(0,2,3,1) , shape = (cls_2_conv.size(0),-1,2))\n        loc_2_reshape   = torch.reshape(input = loc_2_conv.permute(0,2,3,1) , shape = (loc_2_conv.size(0),-1,4))\n        conv2d_7_bn     = self.conv2d_7_bn(conv2d_7)\n        cls_3_insert_conv2d_bn = self.cls_3_insert_conv2d_bn(cls_3_insert_conv2d)\n        loc_3_insert_conv2d_bn = self.loc_3_insert_conv2d_bn(loc_3_insert_conv2d)\n        cls_2_activation = F.sigmoid(cls_2_reshape)\n        conv2d_7_activation = F.relu(conv2d_7_bn)\n        cls_3_insert_conv2d_activation = F.relu(cls_3_insert_conv2d_bn)\n        loc_3_insert_conv2d_activation = F.relu(loc_3_insert_conv2d_bn)\n        cls_4_insert_conv2d_pad = F.pad(conv2d_7_activation, (1, 1, 1, 1))\n        cls_4_insert_conv2d = self.cls_4_insert_conv2d(cls_4_insert_conv2d_pad)\n        loc_4_insert_conv2d_pad = F.pad(conv2d_7_activation, (1, 1, 1, 1))\n        loc_4_insert_conv2d = self.loc_4_insert_conv2d(loc_4_insert_conv2d_pad)\n        cls_3_conv_pad  = F.pad(cls_3_insert_conv2d_activation, (1, 1, 1, 1))\n        cls_3_conv      = self.cls_3_conv(cls_3_conv_pad)\n        loc_3_conv_pad  = F.pad(loc_3_insert_conv2d_activation, (1, 1, 1, 1))\n        loc_3_conv      = self.loc_3_conv(loc_3_conv_pad)\n        cls_4_insert_conv2d_bn = self.cls_4_insert_conv2d_bn(cls_4_insert_conv2d)\n        loc_4_insert_conv2d_bn = self.loc_4_insert_conv2d_bn(loc_4_insert_conv2d)\n        cls_3_reshape   = torch.reshape(input = cls_3_conv.permute(0,2,3,1) , shape = (cls_3_conv.size(0),-1,2))\n        loc_3_reshape   = torch.reshape(input = loc_3_conv.permute(0,2,3,1) , shape = (loc_3_conv.size(0),-1,4))\n        cls_4_insert_conv2d_activation = F.relu(cls_4_insert_conv2d_bn)\n        loc_4_insert_conv2d_activation = F.relu(loc_4_insert_conv2d_bn)\n        cls_3_activation = F.sigmoid(cls_3_reshape)\n        cls_4_conv_pad  = F.pad(cls_4_insert_conv2d_activation, (1, 1, 1, 1))\n        cls_4_conv      = self.cls_4_conv(cls_4_conv_pad)\n        loc_4_conv_pad  = F.pad(loc_4_insert_conv2d_activation, (1, 1, 1, 1))\n        loc_4_conv      = self.loc_4_conv(loc_4_conv_pad)\n        cls_4_reshape   = torch.reshape(input = cls_4_conv.permute(0,2,3,1) , shape = (cls_4_conv.size(0),-1,2))\n        loc_4_reshape   = torch.reshape(input = loc_4_conv.permute(0,2,3,1) , shape = (loc_4_conv.size(0),-1,4))\n        cls_4_activation = F.sigmoid(cls_4_reshape)\n        loc_branch_concat = torch.cat((loc_0_reshape, loc_1_reshape, loc_2_reshape, loc_3_reshape, loc_4_reshape), 1)\n        cls_branch_concat = torch.cat((cls_0_activation, cls_1_activation, cls_2_activation, cls_3_activation, cls_4_activation), 1)\n        return loc_branch_concat, cls_branch_concat\n\n\n    @staticmethod\n    def __batch_normalization(dim, name, **kwargs):\n        if   dim == 0 or dim == 1:  layer = nn.BatchNorm1d(**kwargs)\n        elif dim == 2:  layer = nn.BatchNorm2d(**kwargs)\n        elif dim == 3:  layer = nn.BatchNorm3d(**kwargs)\n        else:           raise NotImplementedError()\n\n        if 'scale' in __weights_dict[name]:\n            layer.state_dict()['weight'].copy_(torch.from_numpy(__weights_dict[name]['scale']))\n        else:\n            layer.weight.data.fill_(1)\n\n        if 'bias' in __weights_dict[name]:\n            layer.state_dict()['bias'].copy_(torch.from_numpy(__weights_dict[name]['bias']))\n        else:\n            layer.bias.data.fill_(0)\n\n        layer.state_dict()['running_mean'].copy_(torch.from_numpy(__weights_dict[name]['mean']))\n        layer.state_dict()['running_var'].copy_(torch.from_numpy(__weights_dict[name]['var']))\n        return layer\n\n    @staticmethod\n    def __conv(dim, name, **kwargs):\n        if   dim == 1:  layer = nn.Conv1d(**kwargs)\n        elif dim == 2:  layer = nn.Conv2d(**kwargs)\n        elif dim == 3:  layer = nn.Conv3d(**kwargs)\n        else:           raise NotImplementedError()\n\n        layer.state_dict()['weight'].copy_(torch.from_numpy(__weights_dict[name]['weights']))\n        if 'bias' in __weights_dict[name]:\n            layer.state_dict()['bias'].copy_(torch.from_numpy(__weights_dict[name]['bias']))\n        return layer\n\n"""
models/__init__.py,0,b''
utils/__init__.py,0,b''
utils/anchor_decode.py,0,"b""# -*- coding:utf-8 -*-\nimport numpy as np\n\ndef decode_bbox(anchors, raw_outputs, variances=[0.1, 0.1, 0.2, 0.2]):\n    '''\n    Decode the actual bbox according to the anchors.\n    the anchor value order is:[xmin,ymin, xmax, ymax]\n    :param anchors: numpy array with shape [batch, num_anchors, 4]\n    :param raw_outputs: numpy array with the same shape with anchors\n    :param variances: list of float, default=[0.1, 0.1, 0.2, 0.2]\n    :return:\n    '''\n    anchor_centers_x = (anchors[:, :, 0:1] + anchors[:, :, 2:3]) / 2\n    anchor_centers_y = (anchors[:, :, 1:2] + anchors[:, :, 3:]) / 2\n    anchors_w = anchors[:, :, 2:3] - anchors[:, :, 0:1]\n    anchors_h = anchors[:, :, 3:] - anchors[:, :, 1:2]\n    raw_outputs_rescale = raw_outputs * np.array(variances)\n    predict_center_x = raw_outputs_rescale[:, :, 0:1] * anchors_w + anchor_centers_x\n    predict_center_y = raw_outputs_rescale[:, :, 1:2] * anchors_h + anchor_centers_y\n    predict_w = np.exp(raw_outputs_rescale[:, :, 2:3]) * anchors_w\n    predict_h = np.exp(raw_outputs_rescale[:, :, 3:]) * anchors_h\n    predict_xmin = predict_center_x - predict_w / 2\n    predict_ymin = predict_center_y - predict_h / 2\n    predict_xmax = predict_center_x + predict_w / 2\n    predict_ymax = predict_center_y + predict_h / 2\n    predict_bbox = np.concatenate([predict_xmin, predict_ymin, predict_xmax, predict_ymax], axis=-1)\n    return predict_bbox"""
utils/anchor_generator.py,0,"b""# -*- encoding=utf-8 -*-\nimport numpy as np\n\ndef generate_anchors(feature_map_sizes, anchor_sizes, anchor_ratios, offset=0.5):\n    '''\n    generate anchors.\n    :param feature_map_sizes: list of list, for example: [[40,40], [20,20]]\n    :param anchor_sizes: list of list, for example: [[0.05, 0.075], [0.1, 0.15]]\n    :param anchor_ratios: list of list, for example: [[1, 0.5], [1, 0.5]]\n    :param offset: default to 0.5\n    :return:\n    '''\n    anchor_bboxes = []\n    for idx, feature_size in enumerate(feature_map_sizes):\n        cx = (np.linspace(0, feature_size[0] - 1, feature_size[0]) + 0.5) / feature_size[0]\n        cy = (np.linspace(0, feature_size[1] - 1, feature_size[1]) + 0.5) / feature_size[1]\n        cx_grid, cy_grid = np.meshgrid(cx, cy)\n        cx_grid_expend = np.expand_dims(cx_grid, axis=-1)\n        cy_grid_expend = np.expand_dims(cy_grid, axis=-1)\n        center = np.concatenate((cx_grid_expend, cy_grid_expend), axis=-1)\n\n        num_anchors = len(anchor_sizes[idx]) +  len(anchor_ratios[idx]) - 1\n        center_tiled = np.tile(center, (1, 1, 2* num_anchors))\n        anchor_width_heights = []\n\n        # different scales with the first aspect ratio\n        for scale in anchor_sizes[idx]:\n            ratio = anchor_ratios[idx][0] # select the first ratio\n            width = scale * np.sqrt(ratio)\n            height = scale / np.sqrt(ratio)\n            anchor_width_heights.extend([-width / 2.0, -height / 2.0, width / 2.0, height / 2.0])\n\n        # the first scale, with different aspect ratios (except the first one)\n        for ratio in anchor_ratios[idx][1:]:\n            s1 = anchor_sizes[idx][0] # select the first scale\n            width = s1 * np.sqrt(ratio)\n            height = s1 / np.sqrt(ratio)\n            anchor_width_heights.extend([-width / 2.0, -height / 2.0, width / 2.0, height / 2.0])\n\n        bbox_coords = center_tiled + np.array(anchor_width_heights)\n        bbox_coords_reshape = bbox_coords.reshape((-1, 4))\n        anchor_bboxes.append(bbox_coords_reshape)\n    anchor_bboxes = np.concatenate(anchor_bboxes, axis=0)\n    return anchor_bboxes\n\n\nif __name__ == '__main__':\n    anchors = generate_anchors(feature_map_sizes, anchor_sizes, anchor_ratios)\n"""
utils/nms.py,0,"b""# -*- encoding=utf-8\nimport numpy as np\n\ndef single_class_non_max_suppression(bboxes, confidences, conf_thresh=0.2, iou_thresh=0.5, keep_top_k=-1):\n    '''\n    do nms on single class.\n    Hint: for the specific class, given the bbox and its confidence,\n    1) sort the bbox according to the confidence from top to down, we call this a set\n    2) select the bbox with the highest confidence, remove it from set, and do IOU calculate with the rest bbox\n    3) remove the bbox whose IOU is higher than the iou_thresh from the set,\n    4) loop step 2 and 3, util the set is empty.\n    :param bboxes: numpy array of 2D, [num_bboxes, 4]\n    :param confidences: numpy array of 1D. [num_bboxes]\n    :param conf_thresh:\n    :param iou_thresh:\n    :param keep_top_k:\n    :return:\n    '''\n    if len(bboxes) == 0: return []\n\n    conf_keep_idx = np.where(confidences > conf_thresh)[0]\n\n    bboxes = bboxes[conf_keep_idx]\n    confidences = confidences[conf_keep_idx]\n\n    pick = []\n    xmin = bboxes[:, 0]\n    ymin = bboxes[:, 1]\n    xmax = bboxes[:, 2]\n    ymax = bboxes[:, 3]\n\n    area = (xmax - xmin + 1e-3) * (ymax - ymin + 1e-3)\n    idxs = np.argsort(confidences)\n\n    while len(idxs) > 0:\n        last = len(idxs) - 1\n        i = idxs[last]\n        pick.append(i)\n\n        # keep top k\n        if keep_top_k != -1:\n            if len(pick) >= keep_top_k:\n                break\n\n        overlap_xmin = np.maximum(xmin[i], xmin[idxs[:last]])\n        overlap_ymin = np.maximum(ymin[i], ymin[idxs[:last]])\n        overlap_xmax = np.minimum(xmax[i], xmax[idxs[:last]])\n        overlap_ymax = np.minimum(ymax[i], ymax[idxs[:last]])\n        overlap_w = np.maximum(0, overlap_xmax - overlap_xmin)\n        overlap_h = np.maximum(0, overlap_ymax - overlap_ymin)\n        overlap_area = overlap_w * overlap_h\n        overlap_ratio = overlap_area / (area[idxs[:last]] + area[i] - overlap_area)\n\n        need_to_be_deleted_idx = np.concatenate(([last], np.where(overlap_ratio > iou_thresh)[0]))\n        idxs = np.delete(idxs, need_to_be_deleted_idx)\n\n    # if the number of final bboxes is less than keep_top_k, we need to pad it.\n    # TODO\n    return conf_keep_idx[pick]\n"""
