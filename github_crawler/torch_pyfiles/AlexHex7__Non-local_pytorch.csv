file_path,api_count,code
demo_MNIST_train.py,8,"b""import torch\nimport torch.utils.data as Data\nimport torchvision\nfrom lib.network import Network\nfrom torch import nn\nimport time\n\n\ntrain_data = torchvision.datasets.MNIST(root='./mnist', train=True,\n                                        transform=torchvision.transforms.ToTensor(),\n                                        download=True)\ntest_data = torchvision.datasets.MNIST(root='./mnist/',\n                                       transform=torchvision.transforms.ToTensor(),\n                                       train=False)\n\ntrain_loader = Data.DataLoader(dataset=train_data, batch_size=128, shuffle=True)\ntest_loader = Data.DataLoader(dataset=test_data, batch_size=128, shuffle=False)\n\ntrain_batch_num = len(train_loader)\ntest_batch_num = len(test_loader)\n\nnet = Network()\nif torch.cuda.is_available():\n    net = nn.DataParallel(net)\n    net.cuda()\n\nopt = torch.optim.Adam(net.parameters(), lr=0.001)\nloss_func = nn.CrossEntropyLoss()\n\nfor epoch_index in range(10):\n    st = time.time()\n\n    torch.set_grad_enabled(True)\n    net.train()\n    for train_batch_index, (img_batch, label_batch) in enumerate(train_loader):\n        if torch.cuda.is_available():\n            img_batch = img_batch.cuda()\n            label_batch = label_batch.cuda()\n\n        predict = net(img_batch)\n        loss = loss_func(predict, label_batch)\n\n        net.zero_grad()\n        loss.backward()\n        opt.step()\n\n    print('(LR:%f) Time of a epoch:%.4fs' % (opt.param_groups[0]['lr'], time.time()-st))\n\n    torch.set_grad_enabled(False)\n    net.eval()\n    total_loss = []\n    total_acc = 0\n    total_sample = 0\n\n    for test_batch_index, (img_batch, label_batch) in enumerate(test_loader):\n        if torch.cuda.is_available():\n            img_batch = img_batch.cuda()\n            label_batch = label_batch.cuda()\n\n        predict = net(img_batch)\n        loss = loss_func(predict, label_batch)\n\n        predict = predict.argmax(dim=1)\n        acc = (predict == label_batch).sum()\n\n        total_loss.append(loss)\n        total_acc += acc\n        total_sample += img_batch.size(0)\n\n    net.train()\n\n    mean_acc = total_acc.item() * 1.0 / total_sample\n    mean_loss = sum(total_loss) / total_loss.__len__()\n\n    print('[Test] epoch[%d/%d] acc:%.4f%% loss:%.4f\\n'\n          % (epoch_index, 10, mean_acc * 100, mean_loss.item()))\n\n# weight_path = 'weights/net.pth'\n# print('Save Net weights to', weight_path)\n# net.cpu()\n# torch.save(net.state_dict(), weight_path)\n"""
nl_map_save.py,4,"b""import torch\nimport torch.utils.data as Data\nimport torchvision\nfrom lib.network import Network\nfrom torch import nn\nimport numpy as np\n\n\ntest_data = torchvision.datasets.MNIST(root='./mnist/',\n                                       transform=torchvision.transforms.ToTensor(),\n                                       train=False)\n\ntest_loader = iter(Data.DataLoader(dataset=test_data, batch_size=1, shuffle=False))\n\nnet = Network()\nif torch.cuda.is_available():\n    net = nn.DataParallel(net)\n    net.cuda()\n\nnet.load_state_dict(torch.load('weights/net.pth'))\n\n\nimg_batch, label_batch = test_loader.__next__()\nimg_batch = img_batch.cuda()\nlabel_batch = label_batch.cuda()\n\ntorch.set_grad_enabled(False)\nnet.eval()\n\n_, nl_mep_list = net.module.forward_with_nl_map(img_batch)\n\n# (b, h1*w1, h2*w2)\nnl_map_1 = nl_mep_list[0].cpu().numpy()\nnl_map_2 = nl_mep_list[1].cpu().numpy()\n\nimg = torchvision.transforms.ToPILImage()(img_batch.cpu()[0])\nimg.save('nl_map_vis/sample.png')\nnp.save('nl_map_vis/nl_map_1', nl_map_1)\nnp.save('nl_map_vis/nl_map_2', nl_map_2)\n"""
Non-Local_pytorch_0.3.1/demo_MNIST.py,7,"b""import torch\nimport torch.utils.data as Data\nimport torchvision\nfrom lib.network import Network\nfrom torch.autograd import Variable\nfrom torch import nn\nimport time\n\n\ndef calc_acc(x, y):\n    x = torch.max(x, dim=-1)[1]\n    accuracy = sum(x == y) / x.size(0)\n    return accuracy\n\n\ntrain_data = torchvision.datasets.MNIST(root='./mnist', train=True,\n                                        transform=torchvision.transforms.ToTensor(),\n                                        download=True)\ntest_data = torchvision.datasets.MNIST(root='./mnist/',\n                                       transform=torchvision.transforms.ToTensor(),\n                                       train=False)\n\ntrain_loader = Data.DataLoader(dataset=train_data, batch_size=128, shuffle=True)\ntest_loader = Data.DataLoader(dataset=test_data, batch_size=128, shuffle=False)\n\ntrain_batch_num = len(train_loader)\ntest_batch_num = len(test_loader)\n\nnet = Network()\nif torch.cuda.is_available():\n    net = nn.DataParallel(net)\n    net.cuda()\n\nopt = torch.optim.Adam(net.parameters(), lr=0.001)\nloss_func = nn.CrossEntropyLoss()\n\n\nfor epoch_index in range(20):\n    st = time.time()\n    for train_batch_index, (img_batch, label_batch) in enumerate(train_loader):\n        img_batch = Variable(img_batch)\n        label_batch = Variable(label_batch)\n\n        if torch.cuda.is_available():\n            img_batch = img_batch.cuda()\n            label_batch = label_batch.cuda()\n\n        predict = net(img_batch)\n        acc = calc_acc(predict.cpu().data, label_batch.cpu().data)\n        loss = loss_func(predict, label_batch)\n\n        net.zero_grad()\n        loss.backward()\n        opt.step()\n\n    print('(LR:%f) Time of a epoch:%.4fs' % (opt.param_groups[0]['lr'], time.time()-st))\n\n    net.eval()\n    total_loss = 0\n    total_acc = 0\n\n    for test_batch_index, (img_batch, label_batch) in enumerate(test_loader):\n        img_batch = Variable(img_batch, volatile=True)\n        label_batch = Variable(label_batch, volatile=True)\n\n        if torch.cuda.is_available():\n            img_batch = img_batch.cuda()\n            label_batch = label_batch.cuda()\n\n        predict = net(img_batch)\n        acc = calc_acc(predict.cpu().data, label_batch.cpu().data)\n        loss = loss_func(predict, label_batch)\n\n        total_loss += loss\n        total_acc += acc\n\n    net.train()\n\n    mean_acc = total_acc / test_batch_num\n    mean_loss = total_loss / test_batch_num\n\n    print('[Test] epoch[%d/%d] acc:%.4f loss:%.4f\\n'\n          % (epoch_index, 100, mean_acc, mean_loss.data[0]))\n"""
Non-Local_pytorch_0.4.1_to_1.1.0/demo_MNIST.py,8,"b""import torch\nimport torch.utils.data as Data\nimport torchvision\nfrom lib.network import Network\nfrom torch import nn\nimport time\n\n\n# def calc_acc(x, y):\n#     x = torch.max(x, dim=-1)[1]\n#     accuracy = sum(x == y) / x.size(0)\n#     return accuracy\n\n\ntrain_data = torchvision.datasets.MNIST(root='./mnist', train=True,\n                                        transform=torchvision.transforms.ToTensor(),\n                                        download=True)\ntest_data = torchvision.datasets.MNIST(root='./mnist/',\n                                       transform=torchvision.transforms.ToTensor(),\n                                       train=False)\n\ntrain_loader = Data.DataLoader(dataset=train_data, batch_size=128, shuffle=True)\ntest_loader = Data.DataLoader(dataset=test_data, batch_size=128, shuffle=False)\n\ntrain_batch_num = len(train_loader)\ntest_batch_num = len(test_loader)\n\nnet = Network()\nif torch.cuda.is_available():\n    net = nn.DataParallel(net)\n    net.cuda()\n\nopt = torch.optim.Adam(net.parameters(), lr=0.001)\nloss_func = nn.CrossEntropyLoss()\n\nfor epoch_index in range(20):\n    st = time.time()\n\n    torch.set_grad_enabled(True)\n    net.train()\n    for train_batch_index, (img_batch, label_batch) in enumerate(train_loader):\n        if torch.cuda.is_available():\n            img_batch = img_batch.cuda()\n            label_batch = label_batch.cuda()\n\n        predict = net(img_batch)\n        # acc = calc_acc(predict.cpu().data, label_batch.cpu().data)\n        loss = loss_func(predict, label_batch)\n\n        net.zero_grad()\n        loss.backward()\n        opt.step()\n\n    print('(LR:%f) Time of a epoch:%.4fs' % (opt.param_groups[0]['lr'], time.time()-st))\n\n    torch.set_grad_enabled(False)\n    net.eval()\n    total_loss = []\n    total_acc = 0\n    total_sample = 0\n\n    for test_batch_index, (img_batch, label_batch) in enumerate(test_loader):\n        if torch.cuda.is_available():\n            img_batch = img_batch.cuda()\n            label_batch = label_batch.cuda()\n\n        predict = net(img_batch)\n        loss = loss_func(predict, label_batch)\n\n        predict = predict.argmax(dim=1)\n        acc = (predict == label_batch).sum()\n\n        total_loss.append(loss)\n        total_acc += acc\n        total_sample += img_batch.size(0)\n\n    net.train()\n\n    mean_acc = total_acc.item() * 1.0 / total_sample\n    mean_loss = sum(total_loss) / total_loss.__len__()\n\n    print('[Test] epoch[%d/%d] acc:%.4f%% loss:%.4f\\n'\n          % (epoch_index, 100, mean_acc * 100, mean_loss.item()))\n"""
lib/network.py,1,"b""from torch import nn\n# from lib.non_local_concatenation import NONLocalBlock2D\n# from lib.non_local_gaussian import NONLocalBlock2D\nfrom lib.non_local_embedded_gaussian import NONLocalBlock2D\n# from lib.non_local_dot_product import NONLocalBlock2D\n\n\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n\n        self.conv_1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n        )\n\n        self.nl_1 = NONLocalBlock2D(in_channels=32)\n        self.conv_2 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n        )\n\n        self.nl_2 = NONLocalBlock2D(in_channels=64)\n        self.conv_3 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n        )\n\n        self.fc = nn.Sequential(\n            nn.Linear(in_features=128*3*3, out_features=256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n\n            nn.Linear(in_features=256, out_features=10)\n        )\n\n    def forward(self, x):\n        batch_size = x.size(0)\n\n        feature_1 = self.conv_1(x)\n        nl_feature_1 = self.nl_1(feature_1)\n\n        feature_2 = self.conv_2(nl_feature_1)\n        nl_feature_2 = self.nl_2(feature_2)\n\n        output = self.conv_3(nl_feature_2).view(batch_size, -1)\n        output = self.fc(output)\n\n        return output\n\n    def forward_with_nl_map(self, x):\n        batch_size = x.size(0)\n\n        feature_1 = self.conv_1(x)\n        nl_feature_1, nl_map_1 = self.nl_1(feature_1, return_nl_map=True)\n\n        feature_2 = self.conv_2(nl_feature_1)\n        nl_feature_2, nl_map_2 = self.nl_2(feature_2, return_nl_map=True)\n\n        output = self.conv_3(nl_feature_2).view(batch_size, -1)\n        output = self.fc(output)\n\n        return output, [nl_map_1, nl_map_2]\n\n\nif __name__ == '__main__':\n    import torch\n\n    img = torch.randn(3, 1, 28, 28)\n    net = Network()\n    out = net(img)\n    print(out.size())\n\n"""
lib/non_local_concatenation.py,6,"b""import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass _NonLocalBlockND(nn.Module):\n    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n        super(_NonLocalBlockND, self).__init__()\n\n        assert dimension in [1, 2, 3]\n\n        self.dimension = dimension\n        self.sub_sample = sub_sample\n\n        self.in_channels = in_channels\n        self.inter_channels = inter_channels\n\n        if self.inter_channels is None:\n            self.inter_channels = in_channels // 2\n            if self.inter_channels == 0:\n                self.inter_channels = 1\n\n        if dimension == 3:\n            conv_nd = nn.Conv3d\n            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n            bn = nn.BatchNorm3d\n        elif dimension == 2:\n            conv_nd = nn.Conv2d\n            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n            bn = nn.BatchNorm2d\n        else:\n            conv_nd = nn.Conv1d\n            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n            bn = nn.BatchNorm1d\n\n        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                         kernel_size=1, stride=1, padding=0)\n\n        if bn_layer:\n            self.W = nn.Sequential(\n                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                        kernel_size=1, stride=1, padding=0),\n                bn(self.in_channels)\n            )\n            nn.init.constant_(self.W[1].weight, 0)\n            nn.init.constant_(self.W[1].bias, 0)\n        else:\n            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                             kernel_size=1, stride=1, padding=0)\n            nn.init.constant_(self.W.weight, 0)\n            nn.init.constant_(self.W.bias, 0)\n\n        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                             kernel_size=1, stride=1, padding=0)\n\n        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                           kernel_size=1, stride=1, padding=0)\n\n        self.concat_project = nn.Sequential(\n            nn.Conv2d(self.inter_channels * 2, 1, 1, 1, 0, bias=False),\n            nn.ReLU()\n        )\n\n        if sub_sample:\n            self.g = nn.Sequential(self.g, max_pool_layer)\n            self.phi = nn.Sequential(self.phi, max_pool_layer)\n\n    def forward(self, x, return_nl_map=False):\n        '''\n        :param x: (b, c, t, h, w)\n        :param return_nl_map: if True return z, nl_map, else only return z.\n        :return:\n        '''\n\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n        g_x = g_x.permute(0, 2, 1)\n\n        # (b, c, N, 1)\n        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1, 1)\n        # (b, c, 1, N)\n        phi_x = self.phi(x).view(batch_size, self.inter_channels, 1, -1)\n\n        h = theta_x.size(2)\n        w = phi_x.size(3)\n        theta_x = theta_x.repeat(1, 1, 1, w)\n        phi_x = phi_x.repeat(1, 1, h, 1)\n\n        concat_feature = torch.cat([theta_x, phi_x], dim=1)\n        f = self.concat_project(concat_feature)\n        b, _, h, w = f.size()\n        f = f.view(b, h, w)\n\n        N = f.size(-1)\n        f_div_C = f / N\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        if return_nl_map:\n            return z, f_div_C\n        return z\n\n\nclass NONLocalBlock1D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock1D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=1, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock2D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock2D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=2, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock3D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True,):\n        super(NONLocalBlock3D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=3, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nif __name__ == '__main__':\n    import torch\n\n    for (sub_sample_, bn_layer_) in [(True, True), (False, False), (True, False), (False, True)]:\n        img = torch.zeros(2, 3, 20)\n        net = NONLocalBlock1D(3, sub_sample=sub_sample_, bn_layer=bn_layer_)\n        out = net(img)\n        print(out.size())\n\n        img = torch.zeros(2, 3, 20, 20)\n        net = NONLocalBlock2D(3, sub_sample=sub_sample_, bn_layer=bn_layer_)\n        out = net(img)\n        print(out.size())\n\n        img = torch.randn(2, 3, 8, 20, 20)\n        net = NONLocalBlock3D(3, sub_sample=sub_sample_, bn_layer=bn_layer_)\n        out = net(img)\n        print(out.size())\n"""
lib/non_local_dot_product.py,6,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass _NonLocalBlockND(nn.Module):\n    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n        super(_NonLocalBlockND, self).__init__()\n\n        assert dimension in [1, 2, 3]\n\n        self.dimension = dimension\n        self.sub_sample = sub_sample\n\n        self.in_channels = in_channels\n        self.inter_channels = inter_channels\n\n        if self.inter_channels is None:\n            self.inter_channels = in_channels // 2\n            if self.inter_channels == 0:\n                self.inter_channels = 1\n\n        if dimension == 3:\n            conv_nd = nn.Conv3d\n            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n            bn = nn.BatchNorm3d\n        elif dimension == 2:\n            conv_nd = nn.Conv2d\n            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n            bn = nn.BatchNorm2d\n        else:\n            conv_nd = nn.Conv1d\n            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n            bn = nn.BatchNorm1d\n\n        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                         kernel_size=1, stride=1, padding=0)\n\n        if bn_layer:\n            self.W = nn.Sequential(\n                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                        kernel_size=1, stride=1, padding=0),\n                bn(self.in_channels)\n            )\n            nn.init.constant_(self.W[1].weight, 0)\n            nn.init.constant_(self.W[1].bias, 0)\n        else:\n            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                             kernel_size=1, stride=1, padding=0)\n            nn.init.constant_(self.W.weight, 0)\n            nn.init.constant_(self.W.bias, 0)\n\n        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                             kernel_size=1, stride=1, padding=0)\n\n        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                           kernel_size=1, stride=1, padding=0)\n\n        if sub_sample:\n            self.g = nn.Sequential(self.g, max_pool_layer)\n            self.phi = nn.Sequential(self.phi, max_pool_layer)\n\n    def forward(self, x, return_nl_map=False):\n        """"""\n        :param x: (b, c, t, h, w)\n        :param return_nl_map: if True return z, nl_map, else only return z.\n        :return:\n        """"""\n\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n        g_x = g_x.permute(0, 2, 1)\n\n        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n        theta_x = theta_x.permute(0, 2, 1)\n        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n        f = torch.matmul(theta_x, phi_x)\n        N = f.size(-1)\n        f_div_C = f / N\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        if return_nl_map:\n            return z, f_div_C\n        return z\n\n\nclass NONLocalBlock1D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock1D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=1, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock2D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock2D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=2, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock3D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock3D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=3, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nif __name__ == \'__main__\':\n    import torch\n\n    for (sub_sample_, bn_layer_) in [(True, True), (False, False), (True, False), (False, True)]:\n        img = torch.zeros(2, 3, 20)\n        net = NONLocalBlock1D(3, sub_sample=sub_sample_, bn_layer=bn_layer_)\n        out = net(img)\n        print(out.size())\n\n        img = torch.zeros(2, 3, 20, 20)\n        net = NONLocalBlock2D(3, sub_sample=sub_sample_, bn_layer=bn_layer_)\n        out = net(img)\n        print(out.size())\n\n        img = torch.randn(2, 3, 8, 20, 20)\n        net = NONLocalBlock3D(3, sub_sample=sub_sample_, bn_layer=bn_layer_)\n        out = net(img)\n        print(out.size())\n\n\n\n'"
lib/non_local_embedded_gaussian.py,6,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass _NonLocalBlockND(nn.Module):\n    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n        """"""\n        :param in_channels:\n        :param inter_channels:\n        :param dimension:\n        :param sub_sample:\n        :param bn_layer:\n        """"""\n\n        super(_NonLocalBlockND, self).__init__()\n\n        assert dimension in [1, 2, 3]\n\n        self.dimension = dimension\n        self.sub_sample = sub_sample\n\n        self.in_channels = in_channels\n        self.inter_channels = inter_channels\n\n        if self.inter_channels is None:\n            self.inter_channels = in_channels // 2\n            if self.inter_channels == 0:\n                self.inter_channels = 1\n\n        if dimension == 3:\n            conv_nd = nn.Conv3d\n            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n            bn = nn.BatchNorm3d\n        elif dimension == 2:\n            conv_nd = nn.Conv2d\n            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n            bn = nn.BatchNorm2d\n        else:\n            conv_nd = nn.Conv1d\n            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n            bn = nn.BatchNorm1d\n\n        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                         kernel_size=1, stride=1, padding=0)\n\n        if bn_layer:\n            self.W = nn.Sequential(\n                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                        kernel_size=1, stride=1, padding=0),\n                bn(self.in_channels)\n            )\n            nn.init.constant_(self.W[1].weight, 0)\n            nn.init.constant_(self.W[1].bias, 0)\n        else:\n            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                             kernel_size=1, stride=1, padding=0)\n            nn.init.constant_(self.W.weight, 0)\n            nn.init.constant_(self.W.bias, 0)\n\n        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                             kernel_size=1, stride=1, padding=0)\n        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                           kernel_size=1, stride=1, padding=0)\n\n        if sub_sample:\n            self.g = nn.Sequential(self.g, max_pool_layer)\n            self.phi = nn.Sequential(self.phi, max_pool_layer)\n\n    def forward(self, x, return_nl_map=False):\n        """"""\n        :param x: (b, c, t, h, w)\n        :param return_nl_map: if True return z, nl_map, else only return z.\n        :return:\n        """"""\n\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n        g_x = g_x.permute(0, 2, 1)\n\n        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n        theta_x = theta_x.permute(0, 2, 1)\n        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n        f = torch.matmul(theta_x, phi_x)\n        f_div_C = F.softmax(f, dim=-1)\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        if return_nl_map:\n            return z, f_div_C\n        return z\n\n\nclass NONLocalBlock1D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock1D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=1, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock2D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock2D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=2, sub_sample=sub_sample,\n                                              bn_layer=bn_layer,)\n\n\nclass NONLocalBlock3D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock3D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=3, sub_sample=sub_sample,\n                                              bn_layer=bn_layer,)\n\n\nif __name__ == \'__main__\':\n    import torch\n\n    for (sub_sample_, bn_layer_) in [(True, True), (False, False), (True, False), (False, True)]:\n        img = torch.zeros(2, 3, 20)\n        net = NONLocalBlock1D(3, sub_sample=sub_sample_, bn_layer=bn_layer_)\n        out = net(img)\n        print(out.size())\n\n        img = torch.zeros(2, 3, 20, 20)\n        net = NONLocalBlock2D(3, sub_sample=sub_sample_, bn_layer=bn_layer_, store_last_batch_nl_map=True)\n        out = net(img)\n        print(out.size())\n\n        img = torch.randn(2, 3, 8, 20, 20)\n        net = NONLocalBlock3D(3, sub_sample=sub_sample_, bn_layer=bn_layer_, store_last_batch_nl_map=True)\n        out = net(img)\n        print(out.size())\n\n\n'"
lib/non_local_gaussian.py,6,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass _NonLocalBlockND(nn.Module):\n    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n        super(_NonLocalBlockND, self).__init__()\n\n        assert dimension in [1, 2, 3]\n\n        self.dimension = dimension\n        self.sub_sample = sub_sample\n\n        self.in_channels = in_channels\n        self.inter_channels = inter_channels\n\n        if self.inter_channels is None:\n            self.inter_channels = in_channels // 2\n            if self.inter_channels == 0:\n                self.inter_channels = 1\n\n        if dimension == 3:\n            conv_nd = nn.Conv3d\n            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n            bn = nn.BatchNorm3d\n        elif dimension == 2:\n            conv_nd = nn.Conv2d\n            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n            bn = nn.BatchNorm2d\n        else:\n            conv_nd = nn.Conv1d\n            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n            bn = nn.BatchNorm1d\n\n        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                         kernel_size=1, stride=1, padding=0)\n\n        if bn_layer:\n            self.W = nn.Sequential(\n                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                        kernel_size=1, stride=1, padding=0),\n                bn(self.in_channels)\n            )\n            nn.init.constant_(self.W[1].weight, 0)\n            nn.init.constant_(self.W[1].bias, 0)\n        else:\n            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                             kernel_size=1, stride=1, padding=0)\n            nn.init.constant_(self.W.weight, 0)\n            nn.init.constant_(self.W.bias, 0)\n\n        if sub_sample:\n            self.g = nn.Sequential(self.g, max_pool_layer)\n            self.phi = max_pool_layer\n\n    def forward(self, x, return_nl_map=False):\n        """"""\n        :param x: (b, c, t, h, w)\n        :param return_nl_map: if True return z, nl_map, else only return z.\n        :return:\n        """"""\n\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n\n        g_x = g_x.permute(0, 2, 1)\n\n        theta_x = x.view(batch_size, self.in_channels, -1)\n        theta_x = theta_x.permute(0, 2, 1)\n\n        if self.sub_sample:\n            phi_x = self.phi(x).view(batch_size, self.in_channels, -1)\n        else:\n            phi_x = x.view(batch_size, self.in_channels, -1)\n\n        f = torch.matmul(theta_x, phi_x)\n        f_div_C = F.softmax(f, dim=-1)\n\n        if self.store_last_batch_nl_map:\n            self.nl_map = f_div_C\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        if return_nl_map:\n            return z, f_div_C\n        return z\n\n\nclass NONLocalBlock1D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock1D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=1, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock2D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock2D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=2, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock3D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock3D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=3, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nif __name__ == \'__main__\':\n    import torch\n\n    for (sub_sample_, bn_layer_) in [(True, True), (False, False), (True, False), (False, True)]:\n        img = torch.zeros(2, 3, 20)\n        net = NONLocalBlock1D(3, sub_sample=sub_sample_, bn_layer=bn_layer_)\n        out = net(img)\n        print(out.size())\n\n        img = torch.zeros(2, 3, 20, 20)\n        net = NONLocalBlock2D(3, sub_sample=sub_sample_, bn_layer=bn_layer_)\n        out = net(img)\n        print(out.size())\n\n        img = torch.randn(2, 3, 8, 20, 20)\n        net = NONLocalBlock3D(3, sub_sample=sub_sample_, bn_layer=bn_layer_)\n        out = net(img)\n        print(out.size())\n\n\n\n\n\n\n'"
nl_map_vis/nl_map_vis.py,0,"b'""""""\n(tips: if the Non-local type you select is **non_local_concatenation**\nor **non_local_dot_product** (without Softmax operation),\nyou may need to normalize NL_MAP in the visualize code)\n""""""\nimport numpy as np\nimport cv2\nimport math\nimport os\n\n\ndef vis_nl_map(img_path, nl_map_path, vis_size=(56, 56)):\n    dst_dir = nl_map_path.split(\'.\')[0]\n    if not os.path.exists(dst_dir):\n        os.mkdir(dst_dir)\n\n    img = cv2.imread(img_path, 1)\n    img = cv2.resize(img, dsize=vis_size)\n    h, w, c = img.shape\n\n    nl_map_1 = np.load(nl_map_path)[0]\n\n    total_region, nl_map_length = nl_map_1.shape\n    region_per_row = round(math.sqrt(total_region))\n    size_of_region = round(w / region_per_row)\n\n    nl_map_size = round(math.sqrt(nl_map_length))\n\n    for index in range(total_region):\n        img_draw = img.copy()\n        nl_map = nl_map_1[index]\n        nl_map = nl_map.reshape(nl_map_size, nl_map_size)\n        nl_map = cv2.resize(nl_map, dsize=(h, w))\n\n        nl_map = np.uint8(nl_map * 255)\n\n        heat_img = cv2.applyColorMap(nl_map, cv2.COLORMAP_JET)\n        heat_img = cv2.cvtColor(heat_img, cv2.COLOR_BGR2RGB)\n        img_add = cv2.addWeighted(img_draw, 0.3, heat_img, 0.7, 0)\n\n        x0 = index // region_per_row * size_of_region\n        x1 = x0 + size_of_region\n\n        y0 = index % region_per_row * size_of_region\n        y1 = y0 + size_of_region\n\n        cv2.rectangle(img_add, (y0, x0), (y1, x1), (255, 0, 0), 1)\n        cv2.imwrite(\'%s/%d.png\' % (dst_dir, index), cv2.cvtColor(img_add, cv2.COLOR_BGR2RGB))\n\n\nif __name__ == \'__main__\':\n    vis_nl_map(img_path=\'sample.png\', nl_map_path=\'nl_map_1.npy\', vis_size=(56, 56))\n    vis_nl_map(img_path=\'sample.png\', nl_map_path=\'nl_map_2.npy\', vis_size=(56, 56))\n'"
Non-Local_pytorch_0.3.1/lib/network.py,2,"b""from torch import nn\n# from lib.non_local_concatenation import NONLocalBlock2D\n# from lib.non_local_gaussian import NONLocalBlock2D\nfrom lib.non_local_embedded_gaussian import NONLocalBlock2D\n# from lib.non_local_dot_product import NONLocalBlock2D\n\n\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n\n        self.convs = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            NONLocalBlock2D(in_channels=32),\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            NONLocalBlock2D(in_channels=64),\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n        )\n\n        self.fc = nn.Sequential(\n            nn.Linear(in_features=128*3*3, out_features=256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n\n            nn.Linear(in_features=256, out_features=10)\n        )\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        output = self.convs(x).view(batch_size, -1)\n        output = self.fc(output)\n        return output\n\nif __name__ == '__main__':\n    import torch\n    from torch.autograd import Variable\n\n    img = Variable(torch.randn(3, 1, 28, 28))\n    net = Network()\n    out = net(img)\n    print(out.size())\n\n"""
Non-Local_pytorch_0.3.1/lib/non_local_concatenation.py,7,"b""import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass _NonLocalBlockND(nn.Module):\n    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n        super(_NonLocalBlockND, self).__init__()\n\n        assert dimension in [1, 2, 3]\n\n        self.dimension = dimension\n        self.sub_sample = sub_sample\n\n        self.in_channels = in_channels\n        self.inter_channels = inter_channels\n\n        if self.inter_channels is None:\n            self.inter_channels = in_channels // 2\n            if self.inter_channels == 0:\n                self.inter_channels = 1\n\n        if dimension == 3:\n            conv_nd = nn.Conv3d\n            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n            bn = nn.BatchNorm3d\n        elif dimension == 2:\n            conv_nd = nn.Conv2d\n            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n            bn = nn.BatchNorm2d\n        else:\n            conv_nd = nn.Conv1d\n            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n            bn = nn.BatchNorm1d\n\n        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                         kernel_size=1, stride=1, padding=0)\n\n        if bn_layer:\n            self.W = nn.Sequential(\n                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                        kernel_size=1, stride=1, padding=0),\n                bn(self.in_channels)\n            )\n            nn.init.constant(self.W[1].weight, 0)\n            nn.init.constant(self.W[1].bias, 0)\n        else:\n            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                             kernel_size=1, stride=1, padding=0)\n            nn.init.constant(self.W.weight, 0)\n            nn.init.constant(self.W.bias, 0)\n\n        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                             kernel_size=1, stride=1, padding=0)\n\n        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                           kernel_size=1, stride=1, padding=0)\n\n        self.concat_project = nn.Sequential(\n            nn.Conv2d(self.inter_channels * 2, 1, 1, 1, 0, bias=False),\n            nn.ReLU()\n        )\n\n        if sub_sample:\n            self.g = nn.Sequential(self.g, max_pool_layer)\n            self.phi = nn.Sequential(self.phi, max_pool_layer)\n\n    def forward(self, x):\n        '''\n        :param x: (b, c, t, h, w)\n        :return:\n        '''\n\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n        g_x = g_x.permute(0, 2, 1)\n\n        # (b, c, N, 1)\n        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1, 1)\n        # (b, c, 1, N)\n        phi_x = self.phi(x).view(batch_size, self.inter_channels, 1, -1)\n\n        h = theta_x.size(2)\n        w = phi_x.size(3)\n        theta_x = theta_x.repeat(1, 1, 1, w)\n        phi_x = phi_x.repeat(1, 1, h, 1)\n\n        concat_feature = torch.cat([theta_x, phi_x], dim=1)\n        f = self.concat_project(concat_feature)\n        b, _, h, w = f.size()\n        f = f.view(b, h, w)\n\n        N = f.size(-1)\n        f_div_C = f / N\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        return z\n\n\nclass NONLocalBlock1D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock1D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=1, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock2D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock2D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=2, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock3D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock3D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=3, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nif __name__ == '__main__':\n    from torch.autograd import Variable\n    import torch\n\n    for (sub_sample, bn_layer) in [(True, True), (False, False), (True, False), (False, True)]:\n        img = Variable(torch.zeros(2, 3, 20))\n        net = NONLocalBlock1D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n\n        img = Variable(torch.zeros(2, 3, 20, 20))\n        net = NONLocalBlock2D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n\n        img = Variable(torch.randn(2, 3, 8, 20, 20))\n        net = NONLocalBlock3D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n"""
Non-Local_pytorch_0.3.1/lib/non_local_dot_product.py,7,"b""import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass _NonLocalBlockND(nn.Module):\n    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n        super(_NonLocalBlockND, self).__init__()\n\n        assert dimension in [1, 2, 3]\n\n        self.dimension = dimension\n        self.sub_sample = sub_sample\n\n        self.in_channels = in_channels\n        self.inter_channels = inter_channels\n\n        if self.inter_channels is None:\n            self.inter_channels = in_channels // 2\n            if self.inter_channels == 0:\n                self.inter_channels = 1\n\n        if dimension == 3:\n            conv_nd = nn.Conv3d\n            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n            bn = nn.BatchNorm3d\n        elif dimension == 2:\n            conv_nd = nn.Conv2d\n            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n            bn = nn.BatchNorm2d\n        else:\n            conv_nd = nn.Conv1d\n            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n            bn = nn.BatchNorm1d\n\n        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                         kernel_size=1, stride=1, padding=0)\n\n        if bn_layer:\n            self.W = nn.Sequential(\n                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                        kernel_size=1, stride=1, padding=0),\n                bn(self.in_channels)\n            )\n            nn.init.constant(self.W[1].weight, 0)\n            nn.init.constant(self.W[1].bias, 0)\n        else:\n            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                             kernel_size=1, stride=1, padding=0)\n            nn.init.constant(self.W.weight, 0)\n            nn.init.constant(self.W.bias, 0)\n\n        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                             kernel_size=1, stride=1, padding=0)\n\n        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                           kernel_size=1, stride=1, padding=0)\n\n        if sub_sample:\n            self.g = nn.Sequential(self.g, max_pool_layer)\n            self.phi = nn.Sequential(self.phi, max_pool_layer)\n\n    def forward(self, x):\n        '''\n        :param x: (b, c, t, h, w)\n        :return:\n        '''\n\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n        g_x = g_x.permute(0, 2, 1)\n\n        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n        theta_x = theta_x.permute(0, 2, 1)\n        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n        f = torch.matmul(theta_x, phi_x)\n        N = f.size(-1)\n        f_div_C = f / N\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        return z\n\n\nclass NONLocalBlock1D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock1D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=1, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock2D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock2D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=2, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock3D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock3D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=3, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nif __name__ == '__main__':\n    from torch.autograd import Variable\n    import torch\n\n    for (sub_sample, bn_layer) in [(True, True), (False, False), (True, False), (False, True)]:\n        img = Variable(torch.zeros(2, 3, 20))\n        net = NONLocalBlock1D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n\n        img = Variable(torch.zeros(2, 3, 20, 20))\n        net = NONLocalBlock2D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n\n        img = Variable(torch.randn(2, 3, 8, 20, 20))\n        net = NONLocalBlock3D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n\n\n\n"""
Non-Local_pytorch_0.3.1/lib/non_local_embedded_gaussian.py,7,"b""import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass _NonLocalBlockND(nn.Module):\n    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n        super(_NonLocalBlockND, self).__init__()\n\n        assert dimension in [1, 2, 3]\n\n        self.dimension = dimension\n        self.sub_sample = sub_sample\n\n        self.in_channels = in_channels\n        self.inter_channels = inter_channels\n\n        if self.inter_channels is None:\n            self.inter_channels = in_channels // 2\n            if self.inter_channels == 0:\n                self.inter_channels = 1\n\n        if dimension == 3:\n            conv_nd = nn.Conv3d\n            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n            bn = nn.BatchNorm3d\n        elif dimension == 2:\n            conv_nd = nn.Conv2d\n            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n            bn = nn.BatchNorm2d\n        else:\n            conv_nd = nn.Conv1d\n            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n            bn = nn.BatchNorm1d\n\n        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                         kernel_size=1, stride=1, padding=0)\n\n        if bn_layer:\n            self.W = nn.Sequential(\n                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                        kernel_size=1, stride=1, padding=0),\n                bn(self.in_channels)\n            )\n            nn.init.constant(self.W[1].weight, 0)\n            nn.init.constant(self.W[1].bias, 0)\n        else:\n            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                             kernel_size=1, stride=1, padding=0)\n            nn.init.constant(self.W.weight, 0)\n            nn.init.constant(self.W.bias, 0)\n\n        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                             kernel_size=1, stride=1, padding=0)\n        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                           kernel_size=1, stride=1, padding=0)\n\n        if sub_sample:\n            self.g = nn.Sequential(self.g, max_pool_layer)\n            self.phi = nn.Sequential(self.phi, max_pool_layer)\n\n    def forward(self, x):\n        '''\n        :param x: (b, c, t, h, w)\n        :return:\n        '''\n\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n        g_x = g_x.permute(0, 2, 1)\n\n        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n        theta_x = theta_x.permute(0, 2, 1)\n        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n        f = torch.matmul(theta_x, phi_x)\n        f_div_C = F.softmax(f, dim=-1)\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        return z\n\n\nclass NONLocalBlock1D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock1D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=1, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock2D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock2D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=2, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock3D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock3D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=3, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nif __name__ == '__main__':\n    from torch.autograd import Variable\n    import torch\n\n    sub_sample = True\n    bn_layer = True\n\n    img = Variable(torch.zeros(2, 3, 20))\n    net = NONLocalBlock1D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n    out = net(img)\n    print(out.size())\n\n    img = Variable(torch.zeros(2, 3, 20, 20))\n    net = NONLocalBlock2D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n    out = net(img)\n    print(out.size())\n\n    img = Variable(torch.randn(2, 3, 10, 20, 20))\n    net = NONLocalBlock3D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n    out = net(img)\n    print(out.size())\n\n"""
Non-Local_pytorch_0.3.1/lib/non_local_gaussian.py,7,"b""import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass _NonLocalBlockND(nn.Module):\n    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n        super(_NonLocalBlockND, self).__init__()\n\n        assert dimension in [1, 2, 3]\n\n        self.dimension = dimension\n        self.sub_sample = sub_sample\n\n        self.in_channels = in_channels\n        self.inter_channels = inter_channels\n\n        if self.inter_channels is None:\n            self.inter_channels = in_channels // 2\n            if self.inter_channels == 0:\n                self.inter_channels = 1\n\n        if dimension == 3:\n            conv_nd = nn.Conv3d\n            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n            bn = nn.BatchNorm3d\n        elif dimension == 2:\n            conv_nd = nn.Conv2d\n            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n            bn = nn.BatchNorm2d\n        else:\n            conv_nd = nn.Conv1d\n            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n            bn = nn.BatchNorm1d\n\n        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                         kernel_size=1, stride=1, padding=0)\n\n        if bn_layer:\n            self.W = nn.Sequential(\n                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                        kernel_size=1, stride=1, padding=0),\n                bn(self.in_channels)\n            )\n            nn.init.constant(self.W[1].weight, 0)\n            nn.init.constant(self.W[1].bias, 0)\n        else:\n            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                             kernel_size=1, stride=1, padding=0)\n            nn.init.constant(self.W.weight, 0)\n            nn.init.constant(self.W.bias, 0)\n\n        if sub_sample:\n            self.g = nn.Sequential(self.g, max_pool_layer)\n            self.phi = max_pool_layer\n\n    def forward(self, x):\n        '''\n        :param x: (b, c, t, h, w)\n        :return:\n        '''\n\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n\n        g_x = g_x.permute(0, 2, 1)\n\n        theta_x = x.view(batch_size, self.in_channels, -1)\n        theta_x = theta_x.permute(0, 2, 1)\n\n        if self.sub_sample:\n            phi_x = self.phi(x).view(batch_size, self.in_channels, -1)\n        else:\n            phi_x = x.view(batch_size, self.in_channels, -1)\n\n        f = torch.matmul(theta_x, phi_x)\n        f_div_C = F.softmax(f, dim=-1)\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        return z\n\n\nclass NONLocalBlock1D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock1D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=1, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock2D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock2D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=2, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock3D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock3D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=3, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nif __name__ == '__main__':\n    from torch.autograd import Variable\n    import torch\n\n    for (sub_sample, bn_layer) in [(True, True), (False, False), (True, False), (False, True)]:\n        img = Variable(torch.zeros(2, 3, 20))\n        net = NONLocalBlock1D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n\n        img = Variable(torch.zeros(2, 3, 20, 20))\n        net = NONLocalBlock2D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n\n        img = Variable(torch.randn(2, 3, 8, 20, 20))\n        net = NONLocalBlock3D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n\n\n\n\n\n"""
Non-Local_pytorch_0.4.1_to_1.1.0/lib/network.py,1,"b""from torch import nn\n# from lib.non_local_concatenation import NONLocalBlock2D\n# from lib.non_local_gaussian import NONLocalBlock2D\nfrom lib.non_local_embedded_gaussian import NONLocalBlock2D\n# from lib.non_local_dot_product import NONLocalBlock2D\n\n\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n\n        self.convs = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            NONLocalBlock2D(in_channels=32),\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            NONLocalBlock2D(in_channels=64),\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n        )\n\n        self.fc = nn.Sequential(\n            nn.Linear(in_features=128*3*3, out_features=256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n\n            nn.Linear(in_features=256, out_features=10)\n        )\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        output = self.convs(x).view(batch_size, -1)\n        output = self.fc(output)\n        return output\n\nif __name__ == '__main__':\n    import torch\n\n    img = torch.randn(3, 1, 28, 28)\n    net = Network()\n    out = net(img)\n    print(out.size())\n\n"""
Non-Local_pytorch_0.4.1_to_1.1.0/lib/non_local_concatenation.py,6,"b""import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass _NonLocalBlockND(nn.Module):\n    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n        super(_NonLocalBlockND, self).__init__()\n\n        assert dimension in [1, 2, 3]\n\n        self.dimension = dimension\n        self.sub_sample = sub_sample\n\n        self.in_channels = in_channels\n        self.inter_channels = inter_channels\n\n        if self.inter_channels is None:\n            self.inter_channels = in_channels // 2\n            if self.inter_channels == 0:\n                self.inter_channels = 1\n\n        if dimension == 3:\n            conv_nd = nn.Conv3d\n            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n            bn = nn.BatchNorm3d\n        elif dimension == 2:\n            conv_nd = nn.Conv2d\n            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n            bn = nn.BatchNorm2d\n        else:\n            conv_nd = nn.Conv1d\n            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n            bn = nn.BatchNorm1d\n\n        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                         kernel_size=1, stride=1, padding=0)\n\n        if bn_layer:\n            self.W = nn.Sequential(\n                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                        kernel_size=1, stride=1, padding=0),\n                bn(self.in_channels)\n            )\n            nn.init.constant_(self.W[1].weight, 0)\n            nn.init.constant_(self.W[1].bias, 0)\n        else:\n            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                             kernel_size=1, stride=1, padding=0)\n            nn.init.constant_(self.W.weight, 0)\n            nn.init.constant_(self.W.bias, 0)\n\n        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                             kernel_size=1, stride=1, padding=0)\n\n        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                           kernel_size=1, stride=1, padding=0)\n\n        self.concat_project = nn.Sequential(\n            nn.Conv2d(self.inter_channels * 2, 1, 1, 1, 0, bias=False),\n            nn.ReLU()\n        )\n\n        if sub_sample:\n            self.g = nn.Sequential(self.g, max_pool_layer)\n            self.phi = nn.Sequential(self.phi, max_pool_layer)\n\n    def forward(self, x):\n        '''\n        :param x: (b, c, t, h, w)\n        :return:\n        '''\n\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n        g_x = g_x.permute(0, 2, 1)\n\n        # (b, c, N, 1)\n        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1, 1)\n        # (b, c, 1, N)\n        phi_x = self.phi(x).view(batch_size, self.inter_channels, 1, -1)\n\n        h = theta_x.size(2)\n        w = phi_x.size(3)\n        theta_x = theta_x.repeat(1, 1, 1, w)\n        phi_x = phi_x.repeat(1, 1, h, 1)\n\n        concat_feature = torch.cat([theta_x, phi_x], dim=1)\n        f = self.concat_project(concat_feature)\n        b, _, h, w = f.size()\n        f = f.view(b, h, w)\n\n        N = f.size(-1)\n        f_div_C = f / N\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        return z\n\n\nclass NONLocalBlock1D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock1D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=1, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock2D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock2D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=2, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock3D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock3D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=3, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nif __name__ == '__main__':\n    import torch\n\n    for (sub_sample, bn_layer) in [(True, True), (False, False), (True, False), (False, True)]:\n        img = torch.zeros(2, 3, 20)\n        net = NONLocalBlock1D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n\n        img = torch.zeros(2, 3, 20, 20)\n        net = NONLocalBlock2D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n\n        img = torch.randn(2, 3, 8, 20, 20)\n        net = NONLocalBlock3D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n"""
Non-Local_pytorch_0.4.1_to_1.1.0/lib/non_local_dot_product.py,6,"b""import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass _NonLocalBlockND(nn.Module):\n    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n        super(_NonLocalBlockND, self).__init__()\n\n        assert dimension in [1, 2, 3]\n\n        self.dimension = dimension\n        self.sub_sample = sub_sample\n\n        self.in_channels = in_channels\n        self.inter_channels = inter_channels\n\n        if self.inter_channels is None:\n            self.inter_channels = in_channels // 2\n            if self.inter_channels == 0:\n                self.inter_channels = 1\n\n        if dimension == 3:\n            conv_nd = nn.Conv3d\n            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n            bn = nn.BatchNorm3d\n        elif dimension == 2:\n            conv_nd = nn.Conv2d\n            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n            bn = nn.BatchNorm2d\n        else:\n            conv_nd = nn.Conv1d\n            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n            bn = nn.BatchNorm1d\n\n        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                         kernel_size=1, stride=1, padding=0)\n\n        if bn_layer:\n            self.W = nn.Sequential(\n                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                        kernel_size=1, stride=1, padding=0),\n                bn(self.in_channels)\n            )\n            nn.init.constant_(self.W[1].weight, 0)\n            nn.init.constant_(self.W[1].bias, 0)\n        else:\n            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                             kernel_size=1, stride=1, padding=0)\n            nn.init.constant_(self.W.weight, 0)\n            nn.init.constant_(self.W.bias, 0)\n\n        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                             kernel_size=1, stride=1, padding=0)\n\n        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                           kernel_size=1, stride=1, padding=0)\n\n        if sub_sample:\n            self.g = nn.Sequential(self.g, max_pool_layer)\n            self.phi = nn.Sequential(self.phi, max_pool_layer)\n\n    def forward(self, x):\n        '''\n        :param x: (b, c, t, h, w)\n        :return:\n        '''\n\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n        g_x = g_x.permute(0, 2, 1)\n\n        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n        theta_x = theta_x.permute(0, 2, 1)\n        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n        f = torch.matmul(theta_x, phi_x)\n        N = f.size(-1)\n        f_div_C = f / N\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        return z\n\n\nclass NONLocalBlock1D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock1D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=1, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock2D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock2D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=2, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock3D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock3D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=3, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nif __name__ == '__main__':\n    import torch\n\n    for (sub_sample, bn_layer) in [(True, True), (False, False), (True, False), (False, True)]:\n        img = torch.zeros(2, 3, 20)\n        net = NONLocalBlock1D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n\n        img = torch.zeros(2, 3, 20, 20)\n        net = NONLocalBlock2D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n\n        img = torch.randn(2, 3, 8, 20, 20)\n        net = NONLocalBlock3D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n\n\n\n"""
Non-Local_pytorch_0.4.1_to_1.1.0/lib/non_local_embedded_gaussian.py,6,"b""import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass _NonLocalBlockND(nn.Module):\n    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n        super(_NonLocalBlockND, self).__init__()\n\n        assert dimension in [1, 2, 3]\n\n        self.dimension = dimension\n        self.sub_sample = sub_sample\n\n        self.in_channels = in_channels\n        self.inter_channels = inter_channels\n\n        if self.inter_channels is None:\n            self.inter_channels = in_channels // 2\n            if self.inter_channels == 0:\n                self.inter_channels = 1\n\n        if dimension == 3:\n            conv_nd = nn.Conv3d\n            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n            bn = nn.BatchNorm3d\n        elif dimension == 2:\n            conv_nd = nn.Conv2d\n            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n            bn = nn.BatchNorm2d\n        else:\n            conv_nd = nn.Conv1d\n            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n            bn = nn.BatchNorm1d\n\n        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                         kernel_size=1, stride=1, padding=0)\n\n        if bn_layer:\n            self.W = nn.Sequential(\n                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                        kernel_size=1, stride=1, padding=0),\n                bn(self.in_channels)\n            )\n            nn.init.constant_(self.W[1].weight, 0)\n            nn.init.constant_(self.W[1].bias, 0)\n        else:\n            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                             kernel_size=1, stride=1, padding=0)\n            nn.init.constant_(self.W.weight, 0)\n            nn.init.constant_(self.W.bias, 0)\n\n        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                             kernel_size=1, stride=1, padding=0)\n        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                           kernel_size=1, stride=1, padding=0)\n\n        if sub_sample:\n            self.g = nn.Sequential(self.g, max_pool_layer)\n            self.phi = nn.Sequential(self.phi, max_pool_layer)\n\n    def forward(self, x):\n        '''\n        :param x: (b, c, t, h, w)\n        :return:\n        '''\n\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n        g_x = g_x.permute(0, 2, 1)\n\n        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n        theta_x = theta_x.permute(0, 2, 1)\n        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n        f = torch.matmul(theta_x, phi_x)\n        f_div_C = F.softmax(f, dim=-1)\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        return z\n\n\nclass NONLocalBlock1D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock1D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=1, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock2D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock2D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=2, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock3D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock3D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=3, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nif __name__ == '__main__':\n    import torch\n\n    for (sub_sample, bn_layer) in [(True, True), (False, False), (True, False), (False, True)]:\n        img = torch.zeros(2, 3, 20)\n        net = NONLocalBlock1D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n\n        img = torch.zeros(2, 3, 20, 20)\n        net = NONLocalBlock2D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n\n        img = torch.randn(2, 3, 8, 20, 20)\n        net = NONLocalBlock3D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n\n\n"""
Non-Local_pytorch_0.4.1_to_1.1.0/lib/non_local_gaussian.py,6,"b""import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass _NonLocalBlockND(nn.Module):\n    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n        super(_NonLocalBlockND, self).__init__()\n\n        assert dimension in [1, 2, 3]\n\n        self.dimension = dimension\n        self.sub_sample = sub_sample\n\n        self.in_channels = in_channels\n        self.inter_channels = inter_channels\n\n        if self.inter_channels is None:\n            self.inter_channels = in_channels // 2\n            if self.inter_channels == 0:\n                self.inter_channels = 1\n\n        if dimension == 3:\n            conv_nd = nn.Conv3d\n            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n            bn = nn.BatchNorm3d\n        elif dimension == 2:\n            conv_nd = nn.Conv2d\n            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n            bn = nn.BatchNorm2d\n        else:\n            conv_nd = nn.Conv1d\n            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n            bn = nn.BatchNorm1d\n\n        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                         kernel_size=1, stride=1, padding=0)\n\n        if bn_layer:\n            self.W = nn.Sequential(\n                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                        kernel_size=1, stride=1, padding=0),\n                bn(self.in_channels)\n            )\n            nn.init.constant_(self.W[1].weight, 0)\n            nn.init.constant_(self.W[1].bias, 0)\n        else:\n            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                             kernel_size=1, stride=1, padding=0)\n            nn.init.constant_(self.W.weight, 0)\n            nn.init.constant_(self.W.bias, 0)\n\n        if sub_sample:\n            self.g = nn.Sequential(self.g, max_pool_layer)\n            self.phi = max_pool_layer\n\n    def forward(self, x):\n        '''\n        :param x: (b, c, t, h, w)\n        :return:\n        '''\n\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n\n        g_x = g_x.permute(0, 2, 1)\n\n        theta_x = x.view(batch_size, self.in_channels, -1)\n        theta_x = theta_x.permute(0, 2, 1)\n\n        if self.sub_sample:\n            phi_x = self.phi(x).view(batch_size, self.in_channels, -1)\n        else:\n            phi_x = x.view(batch_size, self.in_channels, -1)\n\n        f = torch.matmul(theta_x, phi_x)\n        f_div_C = F.softmax(f, dim=-1)\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        return z\n\n\nclass NONLocalBlock1D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock1D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=1, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock2D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock2D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=2, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock3D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock3D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=3, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nif __name__ == '__main__':\n    import torch\n\n    for (sub_sample, bn_layer) in [(True, True), (False, False), (True, False), (False, True)]:\n        img = torch.zeros(2, 3, 20)\n        net = NONLocalBlock1D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n\n        img = torch.zeros(2, 3, 20, 20)\n        net = NONLocalBlock2D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n\n        img = torch.randn(2, 3, 8, 20, 20)\n        net = NONLocalBlock3D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n        out = net(img)\n        print(out.size())\n\n\n\n\n\n\n"""
Non-Local_pytorch_0.3.1/lib/backup/non_local.py,13,"b""import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass _NonLocalBlockND(nn.Module):\n    def __init__(self, in_channels, inter_channels=None, dimension=3, mode='embedded_gaussian',\n                 sub_sample=True, bn_layer=True):\n        super(_NonLocalBlockND, self).__init__()\n\n        assert dimension in [1, 2, 3]\n        assert mode in ['embedded_gaussian', 'gaussian', 'dot_product', 'concatenation']\n\n        # print('Dimension: %d, mode: %s' % (dimension, mode))\n\n        self.mode = mode\n        self.dimension = dimension\n        self.sub_sample = sub_sample\n\n        self.in_channels = in_channels\n        self.inter_channels = inter_channels\n\n        if self.inter_channels is None:\n            self.inter_channels = in_channels // 2\n            if self.inter_channels == 0:\n                self.inter_channels = 1\n\n        if dimension == 3:\n            conv_nd = nn.Conv3d\n            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n            bn = nn.BatchNorm3d\n        elif dimension == 2:\n            conv_nd = nn.Conv2d\n            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n            bn = nn.BatchNorm2d\n        else:\n            conv_nd = nn.Conv1d\n            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n            bn = nn.BatchNorm1d\n\n        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                         kernel_size=1, stride=1, padding=0)\n\n        if bn_layer:\n            self.W = nn.Sequential(\n                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                        kernel_size=1, stride=1, padding=0),\n                bn(self.in_channels)\n            )\n            nn.init.constant(self.W[1].weight, 0)\n            nn.init.constant(self.W[1].bias, 0)\n        else:\n            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                             kernel_size=1, stride=1, padding=0)\n            nn.init.constant(self.W.weight, 0)\n            nn.init.constant(self.W.bias, 0)\n\n        self.theta = None\n        self.phi = None\n        self.concat_project = None\n\n        # if mode in ['embedded_gaussian', 'dot_product', 'concatenation']:\n        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                             kernel_size=1, stride=1, padding=0)\n\n        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                           kernel_size=1, stride=1, padding=0)\n        #       elif mode == 'concatenation':\n        self.concat_project = nn.Sequential(\n            nn.Conv2d(self.inter_channels * 2, 1, 1, 1, 0, bias=False),\n            nn.ReLU()\n        )\n\n        # if mode in ['embedded_gaussian', 'dot_product', 'concatenation']:\n        #     self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n        #                          kernel_size=1, stride=1, padding=0)\n        #     self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n        #                        kernel_size=1, stride=1, padding=0)\n        #\n        #     if mode == 'embedded_gaussian':\n        #         self.operation_function = self._embedded_gaussian\n        #     elif mode == 'dot_product':\n        #         self.operation_function = self._dot_product\n        #     elif mode == 'concatenation':\n        #         self.operation_function = self._concatenation\n        #         self.concat_project = nn.Sequential(\n        #             nn.Conv2d(self.inter_channels * 2, 1, 1, 1, 0, bias=False),\n        #             nn.ReLU()\n        #         )\n        # elif mode == 'gaussian':\n        #     self.operation_function = self._gaussian\n\n        if sub_sample:\n            self.g = nn.Sequential(self.g, max_pool_layer)\n            if self.phi is None:\n                self.phi = max_pool_layer\n            else:\n                self.phi = nn.Sequential(self.phi, max_pool_layer)\n\n        print(self.phi)\n\n    def forward(self, x):\n        '''\n        :param x: (b, c, t, h, w)\n        :return:\n        '''\n\n        if self.mode == 'embedded_gaussian':\n            output = self._embedded_gaussian(x)\n        elif mode == 'dot_product':\n            output = self._dot_product(x)\n        elif mode == 'concatenation':\n            output = self._concatenation(x)\n        elif mode == 'gaussian':\n            output = self._gaussian(x)\n\n        return output\n\n    def _embedded_gaussian(self, x):\n        batch_size = x.size(0)\n\n        # g=>(b, c, t, h, w)->(b, 0.5c, t, h, w)->(b, thw, 0.5c)\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n        g_x = g_x.permute(0, 2, 1)\n\n        # theta=>(b, c, t, h, w)[->(b, 0.5c, t, h, w)]->(b, thw, 0.5c)\n        # phi  =>(b, c, t, h, w)[->(b, 0.5c, t, h, w)]->(b, 0.5c, thw)\n        # f=>(b, thw, 0.5c)dot(b, 0.5c, twh) = (b, thw, thw)\n        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n        theta_x = theta_x.permute(0, 2, 1)\n        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n        f = torch.matmul(theta_x, phi_x)\n        f_div_C = F.softmax(f, dim=-1)\n\n        # (b, thw, thw)dot(b, thw, 0.5c) = (b, thw, 0.5c)->(b, 0.5c, t, h, w)->(b, c, t, h, w)\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        return z\n\n    def _gaussian(self, x):\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n\n        g_x = g_x.permute(0, 2, 1)\n\n        theta_x = x.view(batch_size, self.in_channels, -1)\n        theta_x = theta_x.permute(0, 2, 1)\n\n        if self.sub_sample:\n            print(self.phi(x).size())\n            phi_x = self.phi(x).view(batch_size, self.in_channels, -1)\n        else:\n            phi_x = x.view(batch_size, self.in_channels, -1)\n        print(phi_x.size())\n        f = torch.matmul(theta_x, phi_x)\n        f_div_C = F.softmax(f, dim=-1)\n\n        print(f_div_C.size(), g_x.size())\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        return z\n\n    def _dot_product(self, x):\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n        g_x = g_x.permute(0, 2, 1)\n\n        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n        theta_x = theta_x.permute(0, 2, 1)\n        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n        f = torch.matmul(theta_x, phi_x)\n        N = f.size(-1)\n        f_div_C = f / N\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        return z\n\n    def _concatenation(self, x):\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n        g_x = g_x.permute(0, 2, 1)\n\n        # (b, c, N, 1)\n        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1, 1)\n        # (b, c, 1, N)\n        phi_x = self.phi(x).view(batch_size, self.inter_channels, 1, -1)\n\n        h = theta_x.size(2)\n        w = phi_x.size(3)\n        theta_x = theta_x.repeat(1, 1, 1, w)\n        phi_x = phi_x.repeat(1, 1, h, 1)\n\n        concat_feature = torch.cat([theta_x, phi_x], dim=1)\n        f = self.concat_project(concat_feature)\n        b, _, h, w = f.size()\n        f = f.view(b, h, w)\n\n        N = f.size(-1)\n        f_div_C = f / N\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        return z\n\n\nclass NONLocalBlock1D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, mode='embedded_gaussian', sub_sample=True, bn_layer=True):\n        super(NONLocalBlock1D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=1, mode=mode,\n                                              sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock2D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, mode='embedded_gaussian', sub_sample=True, bn_layer=True):\n        super(NONLocalBlock2D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=2, mode=mode,\n                                              sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock3D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, mode='embedded_gaussian', sub_sample=True, bn_layer=True):\n        super(NONLocalBlock3D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=3, mode=mode,\n                                              sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nif __name__ == '__main__':\n    from torch.autograd import Variable\n\n    # mode_list = ['concatenation', 'embedded_gaussian', 'gaussian', 'dot_product', ]\n    mode_list = ['gaussian']\n\n    for mode in mode_list:\n        print(mode)\n        img = Variable(torch.zeros(2, 6, 20))\n        net = NONLocalBlock1D(6, mode=mode, sub_sample=True)\n        out = net(img)\n        print(out.size())\n\n        # img = Variable(torch.zeros(2, 4, 20, 20))\n        # net = NONLocalBlock2D(4, mode=mode, sub_sample=False, bn_layer=False)\n        # out = net(img)\n        # print(out.size())\n        #\n        # img = Variable(torch.zeros(2, 4, 10, 20, 20))\n        # net = NONLocalBlock3D(4, mode=mode)\n        # out = net(img)\n        # print(out.size())\n\n"""
Non-Local_pytorch_0.3.1/lib/backup/non_local_simple_version.py,7,"b""import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass _NonLocalBlockND(nn.Module):\n    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n        super(_NonLocalBlockND, self).__init__()\n\n        assert dimension in [1, 2, 3]\n\n        self.dimension = dimension\n        self.sub_sample = sub_sample\n\n        self.in_channels = in_channels\n        self.inter_channels = inter_channels\n\n        if self.inter_channels is None:\n            self.inter_channels = in_channels // 2\n            if self.inter_channels == 0:\n                self.inter_channels = 1\n\n        if dimension == 3:\n            conv_nd = nn.Conv3d\n            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n            bn = nn.BatchNorm3d\n        elif dimension == 2:\n            conv_nd = nn.Conv2d\n            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n            bn = nn.BatchNorm2d\n        else:\n            conv_nd = nn.Conv1d\n            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n            bn = nn.BatchNorm1d\n\n        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                         kernel_size=1, stride=1, padding=0)\n\n        if bn_layer:\n            self.W = nn.Sequential(\n                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                        kernel_size=1, stride=1, padding=0),\n                bn(self.in_channels)\n            )\n            nn.init.constant(self.W[1].weight, 0)\n            nn.init.constant(self.W[1].bias, 0)\n        else:\n            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                             kernel_size=1, stride=1, padding=0)\n            nn.init.constant(self.W.weight, 0)\n            nn.init.constant(self.W.bias, 0)\n\n        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                             kernel_size=1, stride=1, padding=0)\n        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                           kernel_size=1, stride=1, padding=0)\n\n        if sub_sample:\n            self.g = nn.Sequential(self.g, max_pool_layer)\n            self.phi = nn.Sequential(self.phi, max_pool_layer)\n\n    def forward(self, x):\n        '''\n        :param x: (b, c, t, h, w)\n        :return:\n        '''\n\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n        g_x = g_x.permute(0, 2, 1)\n\n        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n        theta_x = theta_x.permute(0, 2, 1)\n        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n        f = torch.matmul(theta_x, phi_x)\n        f_div_C = F.softmax(f, dim=-1)\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        return z\n\n\nclass NONLocalBlock1D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock1D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=1, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock2D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock2D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=2, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nclass NONLocalBlock3D(_NonLocalBlockND):\n    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n        super(NONLocalBlock3D, self).__init__(in_channels,\n                                              inter_channels=inter_channels,\n                                              dimension=3, sub_sample=sub_sample,\n                                              bn_layer=bn_layer)\n\n\nif __name__ == '__main__':\n    from torch.autograd import Variable\n    import torch\n\n    sub_sample = True\n    bn_layer = True\n\n    img = Variable(torch.zeros(2, 3, 20))\n    net = NONLocalBlock1D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n    out = net(img)\n    print(out.size())\n\n    img = Variable(torch.zeros(2, 3, 20, 20))\n    net = NONLocalBlock2D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n    out = net(img)\n    print(out.size())\n\n    img = Variable(torch.randn(2, 3, 10, 20, 20))\n    net = NONLocalBlock3D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n    out = net(img)\n    print(out.size())\n\n"""
