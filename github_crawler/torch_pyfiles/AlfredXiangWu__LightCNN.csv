file_path,api_count,code
extract_features.py,10,"b'\'\'\'\r\n    implement the feature extractions for light CNN\r\n    @author: Alfred Xiang Wu\r\n    @date: 2017.07.04\r\n\'\'\'\r\n\r\nfrom __future__ import print_function\r\nimport argparse\r\nimport os\r\nimport shutil\r\nimport time\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.parallel\r\nimport torch.backends.cudnn as cudnn\r\nimport torch.optim\r\nimport torch.utils.data\r\nimport torch.nn.functional as F\r\nimport torchvision.transforms as transforms\r\nimport torchvision.datasets as datasets\r\n\r\nimport numpy as np\r\nimport cv2\r\n\r\nfrom light_cnn import LightCNN_9Layers, LightCNN_29Layers, LightCNN_29Layers_v2\r\nfrom load_imglist import ImageList\r\n\r\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Feature Extracting\')\r\nparser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'LightCNN\')\r\nparser.add_argument(\'--cuda\', \'-c\', default=True)\r\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\r\n                    help=\'path to latest checkpoint (default: none)\')\r\nparser.add_argument(\'--model\', default=\'\', type=str, metavar=\'Model\',\r\n                    help=\'model type: LightCNN-9, LightCNN-29\')\r\nparser.add_argument(\'--root_path\', default=\'\', type=str, metavar=\'PATH\', \r\n                    help=\'root path of face images (default: none).\')\r\nparser.add_argument(\'--img_list\', default=\'\', type=str, metavar=\'PATH\', \r\n                    help=\'list of face images for feature extraction (default: none).\')\r\nparser.add_argument(\'--save_path\', default=\'\', type=str, metavar=\'PATH\', \r\n                    help=\'save root path for features of face images.\')\r\nparser.add_argument(\'--num_classes\', default=79077, type=int,\r\n                    metavar=\'N\', help=\'mini-batch size (default: 79077)\')\r\n\r\ndef main():\r\n    global args\r\n    args = parser.parse_args()\r\n\r\n    if args.model == \'LightCNN-9\':\r\n        model = LightCNN_9Layers(num_classes=args.num_classes)\r\n    elif args.model == \'LightCNN-29\':\r\n        model = LightCNN_29Layers(num_classes=args.num_classes)\r\n    elif args.model == \'LightCNN-29v2\':\r\n        model = LightCNN_29Layers_v2(num_classes=args.num_classes)\r\n    else:\r\n        print(\'Error model type\\n\')\r\n\r\n    model.eval()\r\n    if args.cuda:\r\n        model = torch.nn.DataParallel(model).cuda()\r\n\r\n    if args.resume:\r\n        if os.path.isfile(args.resume):\r\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\r\n            checkpoint = torch.load(args.resume)\r\n            model.load_state_dict(checkpoint[\'state_dict\'])\r\n    else:\r\n        print(""=> no checkpoint found at \'{}\'"".format(args.resume))\r\n\r\n    img_list  = read_list(args.img_list)\r\n    transform = transforms.Compose([transforms.ToTensor()])\r\n    count     = 0\r\n    input     = torch.zeros(1, 1, 128, 128)\r\n    for img_name in img_list:\r\n        count = count + 1\r\n        img   = cv2.imread(os.path.join(args.root_path, img_name), cv2.IMREAD_GRAYSCALE)\r\n        img   = np.reshape(img, (128, 128, 1))\r\n        img   = transform(img)\r\n        input[0,:,:,:] = img\r\n\r\n        start = time.time()\r\n        if args.cuda:\r\n            input = input.cuda()\r\n        input_var   = torch.autograd.Variable(input, volatile=True)\r\n        _, features = model(input_var)\r\n        end         = time.time() - start\r\n        print(""{}({}/{}). Time: {}"".format(os.path.join(args.root_path, img_name), count, len(img_list), end))\r\n        save_feature(args.save_path, img_name, features.data.cpu().numpy()[0])\r\n\r\n\r\ndef read_list(list_path):\r\n    img_list = []\r\n    with open(list_path, \'r\') as f:\r\n        for line in f.readlines()[0:]:\r\n            img_path = line.strip().split()\r\n            img_list.append(img_path[0])\r\n    print(\'There are {} images..\'.format(len(img_list)))\r\n    return img_list\r\n\r\ndef save_feature(save_path, img_name, features):\r\n    img_path = os.path.join(save_path, img_name)\r\n    img_dir  = os.path.dirname(img_path) + \'/\';\r\n    if not os.path.exists(img_dir):\r\n        os.makedirs(img_dir)\r\n    fname = os.path.splitext(img_path)[0]\r\n    fname = fname + \'.feat\'\r\n    fid   = open(fname, \'wb\')\r\n    fid.write(features)\r\n    fid.close()\r\n\r\nif __name__ == \'__main__\':\r\n    main()'"
light_cnn.py,4,"b""'''\r\n    implement Light CNN\r\n    @author: Alfred Xiang Wu\r\n    @date: 2017.07.04\r\n'''\r\n\r\nimport math\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass mfm(nn.Module):\r\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, type=1):\r\n        super(mfm, self).__init__()\r\n        self.out_channels = out_channels\r\n        if type == 1:\r\n            self.filter = nn.Conv2d(in_channels, 2*out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\r\n        else:\r\n            self.filter = nn.Linear(in_channels, 2*out_channels)\r\n\r\n    def forward(self, x):\r\n        x = self.filter(x)\r\n        out = torch.split(x, self.out_channels, 1)\r\n        return torch.max(out[0], out[1])\r\n\r\nclass group(nn.Module):\r\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\r\n        super(group, self).__init__()\r\n        self.conv_a = mfm(in_channels, in_channels, 1, 1, 0)\r\n        self.conv   = mfm(in_channels, out_channels, kernel_size, stride, padding)\r\n\r\n    def forward(self, x):\r\n        x = self.conv_a(x)\r\n        x = self.conv(x)\r\n        return x\r\n\r\nclass resblock(nn.Module):\r\n    def __init__(self, in_channels, out_channels):\r\n        super(resblock, self).__init__()\r\n        self.conv1 = mfm(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\r\n        self.conv2 = mfm(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\r\n\r\n    def forward(self, x):\r\n        res = x\r\n        out = self.conv1(x)\r\n        out = self.conv2(out)\r\n        out = out + res\r\n        return out\r\n\r\nclass network_9layers(nn.Module):\r\n    def __init__(self, num_classes=79077):\r\n        super(network_9layers, self).__init__()\r\n        self.features = nn.Sequential(\r\n            mfm(1, 48, 5, 1, 2), \r\n            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True), \r\n            group(48, 96, 3, 1, 1), \r\n            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\r\n            group(96, 192, 3, 1, 1),\r\n            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True), \r\n            group(192, 128, 3, 1, 1),\r\n            group(128, 128, 3, 1, 1),\r\n            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\r\n            )\r\n        self.fc1 = mfm(8*8*128, 256, type=0)\r\n        self.fc2 = nn.Linear(256, num_classes)\r\n\r\n    def forward(self, x):\r\n        x = self.features(x)\r\n        x = x.view(x.size(0), -1)\r\n        x = self.fc1(x)\r\n        x = F.dropout(x, training=self.training)\r\n        out = self.fc2(x)\r\n        return out, x\r\n\r\nclass network_29layers(nn.Module):\r\n    def __init__(self, block, layers, num_classes=79077):\r\n        super(network_29layers, self).__init__()\r\n        self.conv1  = mfm(1, 48, 5, 1, 2)\r\n        self.pool1  = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\r\n        self.block1 = self._make_layer(block, layers[0], 48, 48)\r\n        self.group1 = group(48, 96, 3, 1, 1)\r\n        self.pool2  = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\r\n        self.block2 = self._make_layer(block, layers[1], 96, 96)\r\n        self.group2 = group(96, 192, 3, 1, 1)\r\n        self.pool3  = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\r\n        self.block3 = self._make_layer(block, layers[2], 192, 192)\r\n        self.group3 = group(192, 128, 3, 1, 1)\r\n        self.block4 = self._make_layer(block, layers[3], 128, 128)\r\n        self.group4 = group(128, 128, 3, 1, 1)\r\n        self.pool4  = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\r\n        self.fc     = mfm(8*8*128, 256, type=0)\r\n        self.fc2    = nn.Linear(256, num_classes)\r\n            \r\n    def _make_layer(self, block, num_blocks, in_channels, out_channels):\r\n        layers = []\r\n        for i in range(0, num_blocks):\r\n            layers.append(block(in_channels, out_channels))\r\n        return nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        x = self.pool1(x)\r\n\r\n        x = self.block1(x)\r\n        x = self.group1(x)\r\n        x = self.pool2(x)\r\n\r\n        x = self.block2(x)\r\n        x = self.group2(x)\r\n        x = self.pool3(x)\r\n\r\n        x = self.block3(x)\r\n        x = self.group3(x)\r\n        x = self.block4(x)\r\n        x = self.group4(x)\r\n        x = self.pool4(x)\r\n\r\n        x = x.view(x.size(0), -1)\r\n        fc = self.fc(x)\r\n        fc = F.dropout(fc, training=self.training)\r\n        out = self.fc2(fc)\r\n        return out, fc\r\n\r\n\r\nclass network_29layers_v2(nn.Module):\r\n    def __init__(self, block, layers, num_classes=79077):\r\n        super(network_29layers_v2, self).__init__()\r\n        self.conv1    = mfm(1, 48, 5, 1, 2)\r\n        self.block1   = self._make_layer(block, layers[0], 48, 48)\r\n        self.group1   = group(48, 96, 3, 1, 1)\r\n        self.block2   = self._make_layer(block, layers[1], 96, 96)\r\n        self.group2   = group(96, 192, 3, 1, 1)\r\n        self.block3   = self._make_layer(block, layers[2], 192, 192)\r\n        self.group3   = group(192, 128, 3, 1, 1)\r\n        self.block4   = self._make_layer(block, layers[3], 128, 128)\r\n        self.group4   = group(128, 128, 3, 1, 1)\r\n        self.fc       = nn.Linear(8*8*128, 256)\r\n        self.fc2 = nn.Linear(256, num_classes, bias=False)\r\n            \r\n    def _make_layer(self, block, num_blocks, in_channels, out_channels):\r\n        layers = []\r\n        for i in range(0, num_blocks):\r\n            layers.append(block(in_channels, out_channels))\r\n        return nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        x = F.max_pool2d(x, 2) + F.avg_pool2d(x, 2)\r\n\r\n        x = self.block1(x)\r\n        x = self.group1(x)\r\n        x = F.max_pool2d(x, 2) + F.avg_pool2d(x, 2)\r\n\r\n        x = self.block2(x)\r\n        x = self.group2(x)\r\n        x = F.max_pool2d(x, 2) + F.avg_pool2d(x, 2)\r\n\r\n        x = self.block3(x)\r\n        x = self.group3(x)\r\n        x = self.block4(x)\r\n        x = self.group4(x)\r\n        x = F.max_pool2d(x, 2) + F.avg_pool2d(x, 2)\r\n\r\n        x = x.view(x.size(0), -1)\r\n        fc = self.fc(x)\r\n        x = F.dropout(fc, training=self.training)\r\n        out = self.fc2(x)\r\n        return out, fc\r\n\r\ndef LightCNN_9Layers(**kwargs):\r\n    model = network_9layers(**kwargs)\r\n    return model\r\n\r\ndef LightCNN_29Layers(**kwargs):\r\n    model = network_29layers(resblock, [1, 2, 3, 4], **kwargs)\r\n    return model\r\n\r\ndef LightCNN_29Layers_v2(**kwargs):\r\n    model = network_29layers_v2(resblock, [1, 2, 3, 4], **kwargs)\r\n    return model"""
load_imglist.py,1,"b""import torch.utils.data as data\r\n\r\nfrom PIL import Image\r\nimport os\r\nimport os.path\r\n\r\ndef default_loader(path):\r\n    img = Image.open(path).convert('L')\r\n    return img\r\n\r\ndef default_list_reader(fileList):\r\n    imgList = []\r\n    with open(fileList, 'r') as file:\r\n        for line in file.readlines():\r\n            imgPath, label = line.strip().split(' ')\r\n            imgList.append((imgPath, int(label)))\r\n    return imgList\r\n\r\nclass ImageList(data.Dataset):\r\n    def __init__(self, root, fileList, transform=None, list_reader=default_list_reader, loader=default_loader):\r\n        self.root      = root\r\n        self.imgList   = list_reader(fileList)\r\n        self.transform = transform\r\n        self.loader    = loader\r\n\r\n    def __getitem__(self, index):\r\n        imgPath, target = self.imgList[index]\r\n        img = self.loader(os.path.join(self.root, imgPath))\r\n\r\n        if self.transform is not None:\r\n            img = self.transform(img)\r\n        return img, target\r\n\r\n    def __len__(self):\r\n        return len(self.imgList)"""
train.py,15,"b'\'\'\'\n    implement training process for Light CNN\n    @author: Alfred Xiang Wu\n    @date: 2017.07.04\n\'\'\'\nfrom __future__ import print_function\nimport argparse\nimport os\nimport shutil\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nimport numpy as np\n\nfrom light_cnn import LightCNN_9Layers, LightCNN_29Layers, LightCNN_29Layers_v2\nfrom load_imglist import ImageList\n\nparser = argparse.ArgumentParser(description=\'PyTorch Light CNN Training\')\nparser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'LightCNN\')\nparser.add_argument(\'--cuda\', \'-c\', default=True)\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 16)\')\nparser.add_argument(\'--epochs\', default=80, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=128, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 128)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.01, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--print-freq\', \'-p\', default=100, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 100)\')\nparser.add_argument(\'--model\', default=\'\', type=str, metavar=\'Model\',\n                    help=\'model type: LightCNN-9, LightCNN-29, LightCNN-29v2\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--root_path\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to root path of images (default: none)\')\nparser.add_argument(\'--train_list\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to training list (default: none)\')\nparser.add_argument(\'--val_list\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to validation list (default: none)\')\nparser.add_argument(\'--save_path\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to save checkpoint (default: none)\')\nparser.add_argument(\'--num_classes\', default=99891, type=int,\n                    metavar=\'N\', help=\'number of classes (default: 99891)\')\n\ndef main():\n    global args\n    args = parser.parse_args()\n\n    # create Light CNN for face recognition\n    if args.model == \'LightCNN-9\':\n        model = LightCNN_9Layers(num_classes=args.num_classes)\n    elif args.model == \'LightCNN-29\':\n        model = LightCNN_29Layers(num_classes=args.num_classes)\n    elif args.model == \'LightCNN-29v2\':\n        model = LightCNN_29Layers_v2(num_classes=args.num_classes)\n    else:\n        print(\'Error model type\\n\')\n\n    if args.cuda:\n        model = torch.nn.DataParallel(model).cuda()\n\n    print(model)\n\n    # large lr for last fc parameters\n    params = []\n    for name, value in model.named_parameters():\n        if \'bias\' in name:\n            if \'fc2\' in name:\n                params += [{\'params\':value, \'lr\': 20 * args.lr, \'weight_decay\': 0}]\n            else:\n                params += [{\'params\':value, \'lr\': 2 * args.lr, \'weight_decay\': 0}]\n        else:\n            if \'fc2\' in name:\n                params += [{\'params\':value, \'lr\': 10 * args.lr}]\n            else:\n                params += [{\'params\':value, \'lr\': 1 * args.lr}]\n\n    optimizer = torch.optim.SGD(params, args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n\n    # optionally resume from a checkpoint\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.resume, checkpoint[\'epoch\']))\n        else:\n            print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n\n    cudnn.benchmark = True\n\n    #load image\n    train_loader = torch.utils.data.DataLoader(\n        ImageList(root=args.root_path, fileList=args.train_list, \n            transform=transforms.Compose([ \n                transforms.RandomCrop(128),\n                transforms.RandomHorizontalFlip(), \n                transforms.ToTensor(),\n            ])),\n        batch_size=args.batch_size, shuffle=True,\n        num_workers=args.workers, pin_memory=True)\n\n    val_loader = torch.utils.data.DataLoader(\n        ImageList(root=args.root_path, fileList=args.val_list, \n            transform=transforms.Compose([ \n                transforms.CenterCrop(128),\n                transforms.ToTensor(),\n            ])),\n        batch_size=args.batch_size, shuffle=False,\n        num_workers=args.workers, pin_memory=True)   \n\n    # define loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n\n    if args.cuda:\n        criterion.cuda()\n\n    validate(val_loader, model, criterion)    \n\n    for epoch in range(args.start_epoch, args.epochs):\n\n        adjust_learning_rate(optimizer, epoch)\n\n        # train for one epoch\n        train(train_loader, model, criterion, optimizer, epoch)\n\n        # evaluate on validation set\n        prec1 = validate(val_loader, model, criterion)\n\n        save_name = args.save_path + \'lightCNN_\' + str(epoch+1) + \'_checkpoint.pth.tar\'\n        save_checkpoint({\n            \'epoch\': epoch + 1,\n            \'arch\': args.arch,\n            \'state_dict\': model.state_dict(),\n            \'prec1\': prec1,\n        }, save_name)\n\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time  = AverageMeter()\n    losses     = AverageMeter()\n    top1       = AverageMeter()\n    top5       = AverageMeter()\n\n    model.train()\n\n    end = time.time()\n    for i, (input, target) in enumerate(train_loader):\n        data_time.update(time.time() - end)\n\n        input      = input.cuda()\n        target     = target.cuda()\n        input_var  = torch.autograd.Variable(input)\n        target_var = torch.autograd.Variable(target)\n\n        # compute output\n        output, _ = model(input_var)\n        loss   = criterion(output, target_var)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output.data, target, topk=(1,5))\n        losses.update(loss.data[0], input.size(0))\n        top1.update(prec1[0], input.size(0))\n        top5.update(prec5[0], input.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print(\'Epoch: [{0}][{1}/{2}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                  \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                   epoch, i, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n\ndef validate(val_loader, model, criterion):\n    batch_time = AverageMeter()\n    losses     = AverageMeter()\n    top1       = AverageMeter()\n    top5       = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    for i, (input, target) in enumerate(val_loader):\n        input      = input.cuda()\n        target     = target.cuda()\n        input_var  = torch.autograd.Variable(input, volatile=True)\n        target_var = torch.autograd.Variable(target, volatile=True)\n\n        # compute output\n        output, _ = model(input_var)\n        loss   = criterion(output, target_var)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output.data, target, topk=(1,5))\n        losses.update(loss.data[0], input.size(0))\n        top1.update(prec1[0], input.size(0))\n        top5.update(prec5[0], input.size(0))\n\n\n    print(\'\\nTest set: Average loss: {}, Accuracy: ({})\\n\'.format(losses.avg, top1.avg))\n\n    return top1.avg\n\ndef save_checkpoint(state, filename):\n    torch.save(state, filename)\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val   = 0\n        self.avg   = 0\n        self.sum   = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val   = val\n        self.sum   += val * n\n        self.count += n\n        self.avg   = self.sum / self.count\n\n\ndef adjust_learning_rate(optimizer, epoch):\n    scale = 0.457305051927326\n    step  = 10\n    lr = args.lr * (scale ** (epoch // step))\n    print(\'lr: {}\'.format(lr))\n    if (epoch != 0) & (epoch % step == 0):\n        print(\'Change lr\')\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = param_group[\'lr\'] * scale\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred    = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\nif __name__ == \'__main__\':\n    main()'"
