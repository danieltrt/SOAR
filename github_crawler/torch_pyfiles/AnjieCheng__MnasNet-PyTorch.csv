file_path,api_count,code
MnasNet.py,3,"b""from torch.autograd import Variable\nimport torch.nn as nn\nimport torch\nimport math\n\n\ndef Conv_3x3(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef Conv_1x1(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\ndef SepConv_3x3(inp, oup): #input=32, output=16\n    return nn.Sequential(\n        # dw\n        nn.Conv2d(inp, inp , 3, 1, 1, groups=inp, bias=False),\n        nn.BatchNorm2d(inp),\n        nn.ReLU6(inplace=True),\n        # pw-linear\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio, kernel):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        self.conv = nn.Sequential(\n            # pw\n            nn.Conv2d(inp, inp * expand_ratio, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(inp * expand_ratio),\n            nn.ReLU6(inplace=True),\n            # dw\n            nn.Conv2d(inp * expand_ratio, inp * expand_ratio, kernel, stride, kernel // 2, groups=inp * expand_ratio, bias=False),\n            nn.BatchNorm2d(inp * expand_ratio),\n            nn.ReLU6(inplace=True),\n            # pw-linear\n            nn.Conv2d(inp * expand_ratio, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup),\n        )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MnasNet(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n        super(MnasNet, self).__init__()\n\n        # setting of inverted residual blocks\n        self.interverted_residual_setting = [\n            # t, c, n, s, k\n            [3, 24,  3, 2, 3],  # -> 56x56\n            [3, 40,  3, 2, 5],  # -> 28x28\n            [6, 80,  3, 2, 5],  # -> 14x14\n            [6, 96,  2, 1, 3],  # -> 14x14\n            [6, 192, 4, 2, 5],  # -> 7x7\n            [6, 320, 1, 1, 3],  # -> 7x7\n        ]\n\n        assert input_size % 32 == 0\n        input_channel = int(32 * width_mult)\n        self.last_channel = int(1280 * width_mult) if width_mult > 1.0 else 1280\n\n        # building first two layer\n        self.features = [Conv_3x3(3, input_channel, 2), SepConv_3x3(input_channel, 16)]\n        input_channel = 16\n\n        # building inverted residual blocks (MBConv)\n        for t, c, n, s, k in self.interverted_residual_setting:\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                if i == 0:\n                    self.features.append(InvertedResidual(input_channel, output_channel, s, t, k))\n                else:\n                    self.features.append(InvertedResidual(input_channel, output_channel, 1, t, k))\n                input_channel = output_channel\n\n        # building last several layers\n        self.features.append(Conv_1x1(input_channel, self.last_channel))\n        self.features.append(nn.AdaptiveAvgPool2d(1))\n\n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(self.last_channel, n_class),\n        )\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(-1, self.last_channel)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n\nif __name__ == '__main__':\n    net = MnasNet()\n    x_image = Variable(torch.randn(1, 3, 224, 224))\n    y = net(x_image)\n    # print(y)\n"""
