file_path,api_count,code
Model_CharCNN/config.py,0,b'# config.py\n\nclass Config(object):\n    num_channels = 256\n    linear_size = 256\n    output_size = 4\n    max_epochs = 10\n    lr = 0.001\n    batch_size = 128\n    seq_len = 300 # 1014 in original paper\n    dropout_keep = 0.5'
Model_CharCNN/model.py,3,"b'# model.py\n\nimport torch\nfrom torch import nn\nimport numpy as np\nfrom utils import *\n\nclass CharCNN(nn.Module):\n    def __init__(self, config, vocab_size, embeddings):\n        super(CharCNN, self).__init__()\n        self.config = config\n        embed_size = vocab_size\n        \n        # Embedding Layer\n        self.embeddings = nn.Embedding(vocab_size, embed_size)\n        self.embeddings.weight = nn.Parameter(embeddings, requires_grad=False)\n        \n        # This stackoverflow thread explains how conv1d works\n        # https://stackoverflow.com/questions/46503816/keras-conv1d-layer-parameters-filters-and-kernel-size/46504997\n        conv1 = nn.Sequential(\n            nn.Conv1d(in_channels=embed_size, out_channels=self.config.num_channels, kernel_size=7),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=3)\n        ) # (batch_size, num_channels, (seq_len-6)/3)\n        conv2 = nn.Sequential(\n            nn.Conv1d(in_channels=self.config.num_channels, out_channels=self.config.num_channels, kernel_size=7),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=3)\n        ) # (batch_size, num_channels, (seq_len-6-18)/(3*3))\n        conv3 = nn.Sequential(\n            nn.Conv1d(in_channels=self.config.num_channels, out_channels=self.config.num_channels, kernel_size=3),\n            nn.ReLU()\n        ) # (batch_size, num_channels, (seq_len-6-18-18)/(3*3))\n        conv4 = nn.Sequential(\n            nn.Conv1d(in_channels=self.config.num_channels, out_channels=self.config.num_channels, kernel_size=3),\n            nn.ReLU()\n        ) # (batch_size, num_channels, (seq_len-6-18-18-18)/(3*3))\n        conv5 = nn.Sequential(\n            nn.Conv1d(in_channels=self.config.num_channels, out_channels=self.config.num_channels, kernel_size=3),\n            nn.ReLU()\n        ) # (batch_size, num_channels, (seq_len-6-18-18-18-18)/(3*3))\n        conv6 = nn.Sequential(\n            nn.Conv1d(in_channels=self.config.num_channels, out_channels=self.config.num_channels, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=3)\n        ) # (batch_size, num_channels, (seq_len-6-18-18-18-18-18)/(3*3*3))\n        \n        # Length of output after conv6        \n        conv_output_size = self.config.num_channels * ((self.config.seq_len - 96) // 27)\n        \n        linear1 = nn.Sequential(\n            nn.Linear(conv_output_size, self.config.linear_size),\n            nn.ReLU(),\n            nn.Dropout(self.config.dropout_keep)\n        )\n        linear2 = nn.Sequential(\n            nn.Linear(self.config.linear_size, self.config.linear_size),\n            nn.ReLU(),\n            nn.Dropout(self.config.dropout_keep)\n        )\n        linear3 = nn.Sequential(\n            nn.Linear(self.config.linear_size, self.config.output_size),\n            nn.Softmax()\n        )\n        \n        self.convolutional_layers = nn.Sequential(conv1,conv2,conv3,conv4,conv5,conv6)\n        self.linear_layers = nn.Sequential(linear1, linear2, linear3)\n    \n    def forward(self, x):\n        embedded_sent = self.embeddings(x).permute(1,2,0) # shape=(batch_size,embed_size,seq_len)\n        conv_out = self.convolutional_layers(embedded_sent)\n        conv_out = conv_out.view(conv_out.shape[0], -1)\n        linear_output = self.linear_layers(conv_out)\n        return linear_output\n    \n    def add_optimizer(self, optimizer):\n        self.optimizer = optimizer\n        \n    def add_loss_op(self, loss_op):\n        self.loss_op = loss_op\n    \n    def reduce_lr(self):\n        print(""Reducing LR"")\n        for g in self.optimizer.param_groups:\n            g[\'lr\'] = g[\'lr\'] / 2\n                \n    def run_epoch(self, train_iterator, val_iterator, epoch):\n        train_losses = []\n        val_accuracies = []\n        losses = []\n        \n        # Reduce learning rate as number of epochs increase\n        if (epoch > 0) and (epoch % 3 == 0):\n            self.reduce_lr()\n            \n        for i, batch in enumerate(train_iterator):\n            self.optimizer.zero_grad()\n            if torch.cuda.is_available():\n                x = batch.text.cuda()\n                y = (batch.label).type(torch.cuda.LongTensor)\n            else:\n                x = batch.text\n                y = (batch.label).type(torch.LongTensor)\n            y_pred = self.__call__(x)\n            loss = self.loss_op(y_pred, y)\n            loss.backward()\n            losses.append(loss.data.cpu().numpy())\n            self.optimizer.step()\n    \n            if i % 100 == 0:\n                print(""Iter: {}"".format(i+1))\n                avg_train_loss = np.mean(losses)\n                train_losses.append(avg_train_loss)\n                print(""\\tAverage training loss: {:.5f}"".format(avg_train_loss))\n                losses = []\n                \n                # Evalute Accuracy on validation set\n                val_accuracy = evaluate_model(self, val_iterator)\n                print(""\\tVal Accuracy: {:.4f}"".format(val_accuracy))\n                self.train()\n                \n        return train_losses, val_accuracies'"
Model_CharCNN/train.py,2,"b'# train.py\n\nfrom utils import *\nfrom model import *\nfrom config import Config\nimport sys\nimport torch\nimport torch.optim as optim\nfrom torch import nn\n\nif __name__==\'__main__\':\n    config = Config()\n    train_file = \'../data/ag_news.train\'\n    if len(sys.argv) > 2:\n        train_file = sys.argv[1]\n    test_file = \'../data/ag_news.test\'\n    if len(sys.argv) > 3:\n        test_file = sys.argv[2]\n    \n    dataset = Dataset(config)\n    dataset.load_data(train_file, test_file)\n    \n    # Create Model with specified optimizer and loss function\n    ##############################################################\n    model = CharCNN(config, len(dataset.vocab), dataset.embeddings)\n    if torch.cuda.is_available():\n        model.cuda()\n    model.train()\n    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n    loss_fn = nn.CrossEntropyLoss()\n    model.add_optimizer(optimizer)\n    model.add_loss_op(loss_fn)\n    ##############################################################\n    \n    train_losses = []\n    val_accuracies = []\n    \n    for i in range(config.max_epochs):\n        print (""Epoch: {}"".format(i))\n        train_loss,val_accuracy = model.run_epoch(dataset.train_iterator, dataset.val_iterator, i)\n        train_losses.append(train_loss)\n        val_accuracies.append(val_accuracy)\n\n    train_acc = evaluate_model(model, dataset.train_iterator)\n    val_acc = evaluate_model(model, dataset.val_iterator)\n    test_acc = evaluate_model(model, dataset.test_iterator)\n\n    print (\'Final Training Accuracy: {:.4f}\'.format(train_acc))\n    print (\'Final Validation Accuracy: {:.4f}\'.format(val_acc))\n    print (\'Final Test Accuracy: {:.4f}\'.format(test_acc))'"
Model_CharCNN/utils.py,3,"b'# utils.py\n\nimport torch\nfrom torchtext import data\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\ndef get_embedding_matrix(vocab_chars):\n    # one hot embedding plus all-zero vector\n    vocabulary_size = len(vocab_chars)\n    onehot_matrix = np.eye(vocabulary_size, vocabulary_size)\n    return onehot_matrix\n\nclass Dataset(object):\n    def __init__(self, config):\n        self.config = config\n        self.train_iterator = None\n        self.test_iterator = None\n        self.val_iterator = None\n    \n    def parse_label(self, label):\n        \'\'\'\n        Get the actual labels from label string\n        Input:\n            label (string) : labels of the form \'__label__2\'\n        Returns:\n            label (int) : integer value corresponding to label string\n        \'\'\'\n        return int(label.strip()[-1]) - 1\n\n    def get_pandas_df(self, filename):\n        \'\'\'\n        Load the data into Pandas.DataFrame object\n        This will be used to convert data to torchtext object\n        \'\'\'\n        with open(filename, \'r\') as datafile:\n            data = [line.strip().split(\',\', maxsplit=1) for line in datafile]\n            data_text = list(map(lambda x: x[1], data))\n            data_label = list(map(lambda x: self.parse_label(x[0]), data))\n\n        full_df = pd.DataFrame({""text"":data_text, ""label"":data_label})\n        return full_df\n    \n    def load_data(self, train_file, test_file, val_file=None):\n        \'\'\'\n        Loads the data from files\n        Sets up iterators for training, validation and test data\n        Also create vocabulary and word embeddings based on the data\n        \n        Inputs:\n            train_file (String): absolute path to training file\n            test_file (String): absolute path to test file\n            val_file (String): absolute path to validation file\n        \'\'\'\n\n        tokenizer = lambda sent: list(sent[::-1])\n        \n        # Creating Field for data\n        TEXT = data.Field(tokenize=tokenizer, lower=True, fix_length=self.config.seq_len)\n        LABEL = data.Field(sequential=False, use_vocab=False)\n        datafields = [(""text"",TEXT),(""label"",LABEL)]\n        \n        # Load data from pd.DataFrame into torchtext.data.Dataset\n        train_df = self.get_pandas_df(train_file)\n        train_examples = [data.Example.fromlist(i, datafields) for i in train_df.values.tolist()]\n        train_data = data.Dataset(train_examples, datafields)\n        \n        test_df = self.get_pandas_df(test_file)\n        test_examples = [data.Example.fromlist(i, datafields) for i in test_df.values.tolist()]\n        test_data = data.Dataset(test_examples, datafields)\n        \n        # If validation file exists, load it. Otherwise get validation data from training data\n        if val_file:\n            val_df = self.get_pandas_df(val_file)\n            val_examples = [data.Example.fromlist(i, datafields) for i in val_df.values.tolist()]\n            val_data = data.Dataset(val_examples, datafields)\n        else:\n            train_data, val_data = train_data.split(split_ratio=0.9)\n        \n        TEXT.build_vocab(train_data)\n        embedding_mat = get_embedding_matrix(list(TEXT.vocab.stoi.keys()))\n        TEXT.vocab.set_vectors(TEXT.vocab.stoi, torch.FloatTensor(embedding_mat), len(TEXT.vocab.stoi))\n        self.vocab = TEXT.vocab\n        self.embeddings = TEXT.vocab.vectors\n        \n        self.train_iterator = data.BucketIterator(\n            (train_data),\n            batch_size=self.config.batch_size,\n            sort_key=lambda x: len(x.text),\n            repeat=False,\n            shuffle=True)\n        \n        self.val_iterator, self.test_iterator = data.BucketIterator.splits(\n            (val_data, test_data),\n            batch_size=self.config.batch_size,\n            sort_key=lambda x: len(x.text),\n            repeat=False,\n            shuffle=False)\n        \n        print (""Loaded {} training examples"".format(len(train_data)))\n        print (""Loaded {} test examples"".format(len(test_data)))\n        print (""Loaded {} validation examples"".format(len(val_data)))\n\ndef evaluate_model(model, iterator):\n    all_preds = []\n    all_y = []\n    for idx,batch in enumerate(iterator):\n        if torch.cuda.is_available():\n            x = batch.text.cuda()\n        else:\n            x = batch.text\n        y_pred = model(x)\n        predicted = torch.max(y_pred.cpu().data, 1)[1]\n        all_preds.extend(predicted.numpy())\n        all_y.extend(batch.label.numpy())\n    score = accuracy_score(all_y, np.array(all_preds).flatten())\n    return score'"
Model_RCNN/config.py,0,b'# config.py\n\nclass Config(object):\n    embed_size = 300\n    hidden_layers = 1\n    hidden_size = 64\n    output_size = 4\n    max_epochs = 15\n    hidden_size_linear = 64\n    lr = 0.5\n    batch_size = 128\n    seq_len = None # Sequence length for RNN\n    dropout_keep = 0.8\n'
Model_RCNN/model.py,5,"b'# model.py\n\nimport torch\nfrom torch import nn\nimport numpy as np\nfrom torch.nn import functional as F\nfrom utils import *\n\nclass RCNN(nn.Module):\n    def __init__(self, config, vocab_size, word_embeddings):\n        super(RCNN, self).__init__()\n        self.config = config\n        \n        # Embedding Layer\n        self.embeddings = nn.Embedding(vocab_size, self.config.embed_size)\n        self.embeddings.weight = nn.Parameter(word_embeddings, requires_grad=False)\n        \n        # Bi-directional LSTM for RCNN\n        self.lstm = nn.LSTM(input_size = self.config.embed_size,\n                            hidden_size = self.config.hidden_size,\n                            num_layers = self.config.hidden_layers,\n                            dropout = self.config.dropout_keep,\n                            bidirectional = True)\n        \n        self.dropout = nn.Dropout(self.config.dropout_keep)\n        \n        # Linear layer to get ""convolution output"" to be passed to Pooling Layer\n        self.W = nn.Linear(\n            self.config.embed_size + 2*self.config.hidden_size,\n            self.config.hidden_size_linear\n        )\n        \n        # Tanh non-linearity\n        self.tanh = nn.Tanh()\n        \n        # Fully-Connected Layer\n        self.fc = nn.Linear(\n            self.config.hidden_size_linear,\n            self.config.output_size\n        )\n        \n        # Softmax non-linearity\n        self.softmax = nn.Softmax()\n        \n    def forward(self, x):\n        # x.shape = (seq_len, batch_size)\n        embedded_sent = self.embeddings(x)\n        # embedded_sent.shape = (seq_len, batch_size, embed_size)\n\n        lstm_out, (h_n,c_n) = self.lstm(embedded_sent)\n        # lstm_out.shape = (seq_len, batch_size, 2 * hidden_size)\n        \n        input_features = torch.cat([lstm_out,embedded_sent], 2).permute(1,0,2)\n        # final_features.shape = (batch_size, seq_len, embed_size + 2*hidden_size)\n        \n        linear_output = self.tanh(\n            self.W(input_features)\n        )\n        # linear_output.shape = (batch_size, seq_len, hidden_size_linear)\n        \n        linear_output = linear_output.permute(0,2,1) # Reshaping fot max_pool\n        \n        max_out_features = F.max_pool1d(linear_output, linear_output.shape[2]).squeeze(2)\n        # max_out_features.shape = (batch_size, hidden_size_linear)\n        \n        max_out_features = self.dropout(max_out_features)\n        final_out = self.fc(max_out_features)\n        return self.softmax(final_out)\n    \n    def add_optimizer(self, optimizer):\n        self.optimizer = optimizer\n        \n    def add_loss_op(self, loss_op):\n        self.loss_op = loss_op\n    \n    def reduce_lr(self):\n        print(""Reducing LR"")\n        for g in self.optimizer.param_groups:\n            g[\'lr\'] = g[\'lr\'] / 2\n                \n    def run_epoch(self, train_iterator, val_iterator, epoch):\n        train_losses = []\n        val_accuracies = []\n        losses = []\n        \n        # Reduce learning rate as number of epochs increase\n        if (epoch == int(self.config.max_epochs/3)) or (epoch == int(2*self.config.max_epochs/3)):\n            self.reduce_lr()\n            \n        for i, batch in enumerate(train_iterator):\n            self.optimizer.zero_grad()\n            if torch.cuda.is_available():\n                x = batch.text.cuda()\n                y = (batch.label - 1).type(torch.cuda.LongTensor)\n            else:\n                x = batch.text\n                y = (batch.label - 1).type(torch.LongTensor)\n            y_pred = self.__call__(x)\n            loss = self.loss_op(y_pred, y)\n            loss.backward()\n            losses.append(loss.data.cpu().numpy())\n            self.optimizer.step()\n    \n            if i % 100 == 0:\n                print(""Iter: {}"".format(i+1))\n                avg_train_loss = np.mean(losses)\n                train_losses.append(avg_train_loss)\n                print(""\\tAverage training loss: {:.5f}"".format(avg_train_loss))\n                losses = []\n                \n                # Evalute Accuracy on validation set\n                val_accuracy = evaluate_model(self, val_iterator)\n                print(""\\tVal Accuracy: {:.4f}"".format(val_accuracy))\n                self.train()\n                \n        return train_losses, val_accuracies'"
Model_RCNN/train.py,2,"b'# train.py\n\nfrom utils import *\nfrom model import *\nfrom config import Config\nimport sys\nimport torch.optim as optim\nfrom torch import nn\nimport torch\n\nif __name__==\'__main__\':\n    config = Config()\n    train_file = \'../data/ag_news.train\'\n    if len(sys.argv) > 2:\n        train_file = sys.argv[1]\n    test_file = \'../data/ag_news.test\'\n    if len(sys.argv) > 3:\n        test_file = sys.argv[2]\n    \n    w2v_file = \'../data/glove.840B.300d.txt\'\n    \n    dataset = Dataset(config)\n    dataset.load_data(w2v_file, train_file, test_file)\n    \n    # Create Model with specified optimizer and loss function\n    ##############################################################\n    model = RCNN(config, len(dataset.vocab), dataset.word_embeddings)\n    if torch.cuda.is_available():\n        model.cuda()\n    model.train()\n    optimizer = optim.SGD(model.parameters(), lr=config.lr)\n    NLLLoss = nn.NLLLoss()\n    model.add_optimizer(optimizer)\n    model.add_loss_op(NLLLoss)\n    ##############################################################\n    \n    train_losses = []\n    val_accuracies = []\n    \n    for i in range(config.max_epochs):\n        print (""Epoch: {}"".format(i))\n        train_loss,val_accuracy = model.run_epoch(dataset.train_iterator, dataset.val_iterator, i)\n        train_losses.append(train_loss)\n        val_accuracies.append(val_accuracy)\n\n    train_acc = evaluate_model(model, dataset.train_iterator)\n    val_acc = evaluate_model(model, dataset.val_iterator)\n    test_acc = evaluate_model(model, dataset.test_iterator)\n\n    print (\'Final Training Accuracy: {:.4f}\'.format(train_acc))\n    print (\'Final Validation Accuracy: {:.4f}\'.format(val_acc))\n    print (\'Final Test Accuracy: {:.4f}\'.format(test_acc))'"
Model_RCNN/utils.py,2,"b'# utils.py\n\nimport torch\nfrom torchtext import data\nfrom torchtext.vocab import Vectors\nimport spacy\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\nclass Dataset(object):\n    def __init__(self, config):\n        self.config = config\n        self.train_iterator = None\n        self.test_iterator = None\n        self.val_iterator = None\n        self.vocab = []\n        self.word_embeddings = {}\n    \n    def parse_label(self, label):\n        \'\'\'\n        Get the actual labels from label string\n        Input:\n            label (string) : labels of the form \'__label__2\'\n        Returns:\n            label (int) : integer value corresponding to label string\n        \'\'\'\n        return int(label.strip()[-1])\n\n    def get_pandas_df(self, filename):\n        \'\'\'\n        Load the data into Pandas.DataFrame object\n        This will be used to convert data to torchtext object\n        \'\'\'\n        with open(filename, \'r\') as datafile:     \n            data = [line.strip().split(\',\', maxsplit=1) for line in datafile]\n            data_text = list(map(lambda x: x[1], data))\n            data_label = list(map(lambda x: self.parse_label(x[0]), data))\n\n        full_df = pd.DataFrame({""text"":data_text, ""label"":data_label})\n        return full_df\n    \n    def load_data(self, w2v_file, train_file, test_file, val_file=None):\n        \'\'\'\n        Loads the data from files\n        Sets up iterators for training, validation and test data\n        Also create vocabulary and word embeddings based on the data\n        \n        Inputs:\n            w2v_file (String): absolute path to file containing word embeddings (GloVe/Word2Vec)\n            train_file (String): absolute path to training file\n            test_file (String): absolute path to test file\n            val_file (String): absolute path to validation file\n        \'\'\'\n\n        NLP = spacy.load(\'en\')\n        tokenizer = lambda sent: [x.text for x in NLP.tokenizer(sent) if x.text != "" ""]\n        \n        # Creating Field for data\n        TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, fix_length=self.config.max_sen_len)\n        LABEL = data.Field(sequential=False, use_vocab=False)\n        datafields = [(""text"",TEXT),(""label"",LABEL)]\n        \n        # Load data from pd.DataFrame into torchtext.data.Dataset\n        train_df = self.get_pandas_df(train_file)\n        train_examples = [data.Example.fromlist(i, datafields) for i in train_df.values.tolist()]\n        train_data = data.Dataset(train_examples, datafields)\n        \n        test_df = self.get_pandas_df(test_file)\n        test_examples = [data.Example.fromlist(i, datafields) for i in test_df.values.tolist()]\n        test_data = data.Dataset(test_examples, datafields)\n        \n        # If validation file exists, load it. Otherwise get validation data from training data\n        if val_file:\n            val_df = self.get_pandas_df(val_file)\n            val_examples = [data.Example.fromlist(i, datafields) for i in val_df.values.tolist()]\n            val_data = data.Dataset(val_examples, datafields)\n        else:\n            train_data, val_data = train_data.split(split_ratio=0.8)\n        \n        TEXT.build_vocab(train_data, vectors=Vectors(w2v_file))\n        self.word_embeddings = TEXT.vocab.vectors\n        self.vocab = TEXT.vocab\n        \n        self.train_iterator = data.BucketIterator(\n            (train_data),\n            batch_size=self.config.batch_size,\n            sort_key=lambda x: len(x.text),\n            repeat=False,\n            shuffle=True)\n        \n        self.val_iterator, self.test_iterator = data.BucketIterator.splits(\n            (val_data, test_data),\n            batch_size=self.config.batch_size,\n            sort_key=lambda x: len(x.text),\n            repeat=False,\n            shuffle=False)\n        \n        print (""Loaded {} training examples"".format(len(train_data)))\n        print (""Loaded {} test examples"".format(len(test_data)))\n        print (""Loaded {} validation examples"".format(len(val_data)))\n\n\ndef evaluate_model(model, iterator):\n    all_preds = []\n    all_y = []\n    for idx,batch in enumerate(iterator):\n        if torch.cuda.is_available():\n            x = batch.text.cuda()\n        else:\n            x = batch.text\n        y_pred = model(x)\n        predicted = torch.max(y_pred.cpu().data, 1)[1] + 1\n        all_preds.extend(predicted.numpy())\n        all_y.extend(batch.label.numpy())\n    score = accuracy_score(all_y, np.array(all_preds).flatten())\n    return score'"
Model_Seq2Seq_Attention/config.py,0,b'# config.py\n\nclass Config(object):\n    embed_size = 300\n    hidden_layers = 1\n    hidden_size = 32\n    bidirectional = True\n    output_size = 4\n    max_epochs = 15\n    lr = 0.5\n    batch_size = 128\n    dropout_keep = 0.8\n    max_sen_len = None # Sequence length for RNN'
Model_Seq2Seq_Attention/model.py,8,"b'# model.py\n\nimport torch\nfrom torch import nn\nimport numpy as np\nfrom torch.nn import functional as F\nfrom utils import *\n\nclass Seq2SeqAttention(nn.Module):\n    def __init__(self, config, vocab_size, word_embeddings):\n        super(Seq2SeqAttention, self).__init__()\n        self.config = config\n        \n        # Embedding Layer\n        self.embeddings = nn.Embedding(vocab_size, self.config.embed_size)\n        self.embeddings.weight = nn.Parameter(word_embeddings, requires_grad=False)\n        \n        # Encoder RNN\n        self.lstm = nn.LSTM(input_size = self.config.embed_size,\n                            hidden_size = self.config.hidden_size,\n                            num_layers = self.config.hidden_layers,\n                            bidirectional = self.config.bidirectional)\n        \n        # Dropout Layer\n        self.dropout = nn.Dropout(self.config.dropout_keep)\n        \n        # Fully-Connected Layer\n        self.fc = nn.Linear(\n            self.config.hidden_size * (1+self.config.bidirectional) * 2,\n            self.config.output_size\n        )\n        \n        # Softmax non-linearity\n        self.softmax = nn.Softmax()\n                \n    def apply_attention(self, rnn_output, final_hidden_state):\n        \'\'\'\n        Apply Attention on RNN output\n        \n        Input:\n            rnn_output (batch_size, seq_len, num_directions * hidden_size): tensor representing hidden state for every word in the sentence\n            final_hidden_state (batch_size, num_directions * hidden_size): final hidden state of the RNN\n            \n        Returns:\n            attention_output(batch_size, num_directions * hidden_size): attention output vector for the batch\n        \'\'\'\n        hidden_state = final_hidden_state.unsqueeze(2)\n        attention_scores = torch.bmm(rnn_output, hidden_state).squeeze(2)\n        soft_attention_weights = F.softmax(attention_scores, 1).unsqueeze(2) #shape = (batch_size, seq_len, 1)\n        attention_output = torch.bmm(rnn_output.permute(0,2,1), soft_attention_weights).squeeze(2)\n        return attention_output\n        \n    def forward(self, x):\n        # x.shape = (max_sen_len, batch_size)\n        embedded_sent = self.embeddings(x)\n        # embedded_sent.shape = (max_sen_len=20, batch_size=64,embed_size=300)\n\n        ##################################### Encoder #######################################\n        lstm_output, (h_n,c_n) = self.lstm(embedded_sent)\n        # lstm_output.shape = (seq_len, batch_size, num_directions * hidden_size)\n        \n        # Final hidden state of last layer (num_directions, batch_size, hidden_size)\n        batch_size = h_n.shape[1]\n        h_n_final_layer = h_n.view(self.config.hidden_layers,\n                                   self.config.bidirectional + 1,\n                                   batch_size,\n                                   self.config.hidden_size)[-1,:,:,:]\n        \n        ##################################### Attention #####################################\n        # Convert input to (batch_size, num_directions * hidden_size) for attention\n        final_hidden_state = torch.cat([h_n_final_layer[i,:,:] for i in range(h_n_final_layer.shape[0])], dim=1)\n        \n        attention_out = self.apply_attention(lstm_output.permute(1,0,2), final_hidden_state)\n        # Attention_out.shape = (batch_size, num_directions * hidden_size)\n        \n        #################################### Linear #########################################\n        concatenated_vector = torch.cat([final_hidden_state, attention_out], dim=1)\n        final_feature_map = self.dropout(concatenated_vector) # shape=(batch_size, num_directions * hidden_size)\n        final_out = self.fc(final_feature_map)\n        return self.softmax(final_out)\n    \n    def add_optimizer(self, optimizer):\n        self.optimizer = optimizer\n        \n    def add_loss_op(self, loss_op):\n        self.loss_op = loss_op\n    \n    def reduce_lr(self):\n        print(""Reducing LR"")\n        for g in self.optimizer.param_groups:\n            g[\'lr\'] = g[\'lr\'] / 2\n                \n    def run_epoch(self, train_iterator, val_iterator, epoch):\n        train_losses = []\n        val_accuracies = []\n        losses = []\n        \n        # Reduce learning rate as number of epochs increase\n        if (epoch == int(self.config.max_epochs/3)) or (epoch == int(2*self.config.max_epochs/3)):\n            self.reduce_lr()\n            \n        for i, batch in enumerate(train_iterator):\n            self.optimizer.zero_grad()\n            if torch.cuda.is_available():\n                x = batch.text.cuda()\n                y = (batch.label - 1).type(torch.cuda.LongTensor)\n            else:\n                x = batch.text\n                y = (batch.label - 1).type(torch.LongTensor)\n            y_pred = self.__call__(x)\n            loss = self.loss_op(y_pred, y)\n            loss.backward()\n            losses.append(loss.data.cpu().numpy())\n            self.optimizer.step()\n    \n            if i % 100 == 0:\n                print(""Iter: {}"".format(i+1))\n                avg_train_loss = np.mean(losses)\n                train_losses.append(avg_train_loss)\n                print(""\\tAverage training loss: {:.5f}"".format(avg_train_loss))\n                losses = []\n                \n                # Evalute Accuracy on validation set\n                val_accuracy = evaluate_model(self, val_iterator)\n                print(""\\tVal Accuracy: {:.4f}"".format(val_accuracy))\n                self.train()\n                \n        return train_losses, val_accuracies'"
Model_Seq2Seq_Attention/train.py,2,"b'# train.py\n\nfrom utils import *\nfrom model import *\nfrom config import Config\nimport sys\nimport torch.optim as optim\nfrom torch import nn\nimport torch\n\nif __name__==\'__main__\':\n    config = Config()\n    train_file = \'../data/ag_news.train\'\n    if len(sys.argv) > 2:\n        train_file = sys.argv[1]\n    test_file = \'../data/ag_news.test\'\n    if len(sys.argv) > 3:\n        test_file = sys.argv[2]\n    \n    w2v_file = \'../data/glove.840B.300d.txt\'\n    \n    dataset = Dataset(config)\n    dataset.load_data(w2v_file, train_file, test_file)\n    \n    # Create Model with specified optimizer and loss function\n    ##############################################################\n    model = Seq2SeqAttention(config, len(dataset.vocab), dataset.word_embeddings)\n    if torch.cuda.is_available():\n        model.cuda()\n    model.train()\n    optimizer = optim.SGD(model.parameters(), lr=config.lr)\n    NLLLoss = nn.NLLLoss()\n    model.add_optimizer(optimizer)\n    model.add_loss_op(NLLLoss)\n    ##############################################################\n    \n    train_losses = []\n    val_accuracies = []\n    \n    for i in range(config.max_epochs):\n        print (""Epoch: {}"".format(i))\n        train_loss,val_accuracy = model.run_epoch(dataset.train_iterator, dataset.val_iterator, i)\n        train_losses.append(train_loss)\n        val_accuracies.append(val_accuracy)\n\n    train_acc = evaluate_model(model, dataset.train_iterator)\n    val_acc = evaluate_model(model, dataset.val_iterator)\n    test_acc = evaluate_model(model, dataset.test_iterator)\n\n    print (\'Final Training Accuracy: {:.4f}\'.format(train_acc))\n    print (\'Final Validation Accuracy: {:.4f}\'.format(val_acc))\n    print (\'Final Test Accuracy: {:.4f}\'.format(test_acc))'"
Model_Seq2Seq_Attention/utils.py,2,"b'# utils.py\n\nimport torch\nfrom torchtext import data\nfrom torchtext.vocab import Vectors\nimport spacy\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\nclass Dataset(object):\n    def __init__(self, config):\n        self.config = config\n        self.train_iterator = None\n        self.test_iterator = None\n        self.val_iterator = None\n        self.vocab = []\n        self.word_embeddings = {}\n    \n    def parse_label(self, label):\n        \'\'\'\n        Get the actual labels from label string\n        Input:\n            label (string) : labels of the form \'__label__2\'\n        Returns:\n            label (int) : integer value corresponding to label string\n        \'\'\'\n        return int(label.strip()[-1])\n\n    def get_pandas_df(self, filename):\n        \'\'\'\n        Load the data into Pandas.DataFrame object\n        This will be used to convert data to torchtext object\n        \'\'\'\n        with open(filename, \'r\') as datafile:     \n            data = [line.strip().split(\',\', maxsplit=1) for line in datafile]\n            data_text = list(map(lambda x: x[1], data))\n            data_label = list(map(lambda x: self.parse_label(x[0]), data))\n\n        full_df = pd.DataFrame({""text"":data_text, ""label"":data_label})\n        return full_df\n    \n    def load_data(self, w2v_file, train_file, test_file=None, val_file=None):\n        \'\'\'\n        Loads the data from files\n        Sets up iterators for training, validation and test data\n        Also create vocabulary and word embeddings based on the data\n        \n        Inputs:\n            w2v_file (String): path to file containing word embeddings (GloVe/Word2Vec)\n            train_file (String): path to training file\n            test_file (String): path to test file\n            val_file (String): path to validation file\n        \'\'\'\n\n        NLP = spacy.load(\'en\')\n        tokenizer = lambda sent: [x.text for x in NLP.tokenizer(sent) if x.text != "" ""]\n        \n        # Creating Field for data\n        TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, fix_length=self.config.max_sen_len)\n        LABEL = data.Field(sequential=False, use_vocab=False)\n        datafields = [(""text"",TEXT),(""label"",LABEL)]\n        \n        # Load data from pd.DataFrame into torchtext.data.Dataset\n        train_df = self.get_pandas_df(train_file)\n        train_examples = [data.Example.fromlist(i, datafields) for i in train_df.values.tolist()]\n        train_data = data.Dataset(train_examples, datafields)\n        \n        test_df = self.get_pandas_df(test_file)\n        test_examples = [data.Example.fromlist(i, datafields) for i in test_df.values.tolist()]\n        test_data = data.Dataset(test_examples, datafields)\n        \n        # If validation file exists, load it. Otherwise get validation data from training data\n        if val_file:\n            val_df = self.get_pandas_df(val_file)\n            val_examples = [data.Example.fromlist(i, datafields) for i in val_df.values.tolist()]\n            val_data = data.Dataset(val_examples, datafields)\n        else:\n            train_data, val_data = train_data.split(split_ratio=0.8)\n        \n        if w2v_file:\n            TEXT.build_vocab(train_data, vectors=Vectors(w2v_file))\n        self.word_embeddings = TEXT.vocab.vectors\n        self.vocab = TEXT.vocab\n        \n        self.train_iterator = data.BucketIterator(\n            (train_data),\n            batch_size=self.config.batch_size,\n            sort_key=lambda x: len(x.text),\n            repeat=False,\n            shuffle=True)\n        \n        self.val_iterator, self.test_iterator = data.BucketIterator.splits(\n            (val_data, test_data),\n            batch_size=self.config.batch_size,\n            sort_key=lambda x: len(x.text),\n            repeat=False,\n            shuffle=False)\n        \n        print (""Loaded {} training examples"".format(len(train_data)))\n        print (""Loaded {} test examples"".format(len(test_data)))\n        print (""Loaded {} validation examples"".format(len(val_data)))\n\n\ndef evaluate_model(model, iterator):\n    all_preds = []\n    all_y = []\n    for idx,batch in enumerate(iterator):\n        if torch.cuda.is_available():\n            x = batch.text.cuda()\n        else:\n            x = batch.text\n        y_pred = model(x)\n        predicted = torch.max(y_pred.cpu().data, 1)[1] + 1\n        all_preds.extend(predicted.numpy())\n        all_y.extend(batch.label.numpy())\n    score = accuracy_score(all_y, np.array(all_preds).flatten())\n    return score'"
Model_TextCNN/config.py,0,"b'# config.py\n\nclass Config(object):\n    embed_size = 300\n    num_channels = 100\n    kernel_size = [3,4,5]\n    output_size = 4\n    max_epochs = 15\n    lr = 0.3\n    batch_size = 64\n    max_sen_len = 30\n    dropout_keep = 0.8'"
Model_TextCNN/model.py,4,"b'# model.py\n\nimport torch\nfrom torch import nn\nimport numpy as np\nfrom utils import *\n\nclass TextCNN(nn.Module):\n    def __init__(self, config, vocab_size, word_embeddings):\n        super(TextCNN, self).__init__()\n        self.config = config\n        \n        # Embedding Layer\n        self.embeddings = nn.Embedding(vocab_size, self.config.embed_size)\n        self.embeddings.weight = nn.Parameter(word_embeddings, requires_grad=False)\n        \n        # This stackoverflow thread clarifies how conv1d works\n        # https://stackoverflow.com/questions/46503816/keras-conv1d-layer-parameters-filters-and-kernel-size/46504997\n        self.conv1 = nn.Sequential(\n            nn.Conv1d(in_channels=self.config.embed_size, out_channels=self.config.num_channels, kernel_size=self.config.kernel_size[0]),\n            nn.ReLU(),\n            nn.MaxPool1d(self.config.max_sen_len - self.config.kernel_size[0]+1)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv1d(in_channels=self.config.embed_size, out_channels=self.config.num_channels, kernel_size=self.config.kernel_size[1]),\n            nn.ReLU(),\n            nn.MaxPool1d(self.config.max_sen_len - self.config.kernel_size[1]+1)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv1d(in_channels=self.config.embed_size, out_channels=self.config.num_channels, kernel_size=self.config.kernel_size[2]),\n            nn.ReLU(),\n            nn.MaxPool1d(self.config.max_sen_len - self.config.kernel_size[2]+1)\n        )\n        \n        self.dropout = nn.Dropout(self.config.dropout_keep)\n        \n        # Fully-Connected Layer\n        self.fc = nn.Linear(self.config.num_channels*len(self.config.kernel_size), self.config.output_size)\n        \n        # Softmax non-linearity\n        self.softmax = nn.Softmax()\n        \n    def forward(self, x):\n        # x.shape = (max_sen_len, batch_size)\n        embedded_sent = self.embeddings(x).permute(1,2,0)\n        # embedded_sent.shape = (batch_size=64,embed_size=300,max_sen_len=20)\n        \n        conv_out1 = self.conv1(embedded_sent).squeeze(2) #shape=(64, num_channels, 1) (squeeze 1)\n        conv_out2 = self.conv2(embedded_sent).squeeze(2)\n        conv_out3 = self.conv3(embedded_sent).squeeze(2)\n        \n        all_out = torch.cat((conv_out1, conv_out2, conv_out3), 1)\n        final_feature_map = self.dropout(all_out)\n        final_out = self.fc(final_feature_map)\n        return self.softmax(final_out)\n    \n    def add_optimizer(self, optimizer):\n        self.optimizer = optimizer\n        \n    def add_loss_op(self, loss_op):\n        self.loss_op = loss_op\n    \n    def reduce_lr(self):\n        print(""Reducing LR"")\n        for g in self.optimizer.param_groups:\n            g[\'lr\'] = g[\'lr\'] / 2\n                \n    def run_epoch(self, train_iterator, val_iterator, epoch):\n        train_losses = []\n        val_accuracies = []\n        losses = []\n        \n        # Reduce learning rate as number of epochs increase\n        if (epoch == int(self.config.max_epochs/3)) or (epoch == int(2*self.config.max_epochs/3)):\n            self.reduce_lr()\n            \n        for i, batch in enumerate(train_iterator):\n            self.optimizer.zero_grad()\n            if torch.cuda.is_available():\n                x = batch.text.cuda()\n                y = (batch.label - 1).type(torch.cuda.LongTensor)\n            else:\n                x = batch.text\n                y = (batch.label - 1).type(torch.LongTensor)\n            y_pred = self.__call__(x)\n            loss = self.loss_op(y_pred, y)\n            loss.backward()\n            losses.append(loss.data.cpu().numpy())\n            self.optimizer.step()\n    \n            if i % 100 == 0:\n                print(""Iter: {}"".format(i+1))\n                avg_train_loss = np.mean(losses)\n                train_losses.append(avg_train_loss)\n                print(""\\tAverage training loss: {:.5f}"".format(avg_train_loss))\n                losses = []\n                \n                # Evalute Accuracy on validation set\n                val_accuracy = evaluate_model(self, val_iterator)\n                print(""\\tVal Accuracy: {:.4f}"".format(val_accuracy))\n                self.train()\n                \n        return train_losses, val_accuracies'"
Model_TextCNN/train.py,2,"b'# train.py\n\nfrom utils import *\nfrom model import *\nfrom config import Config\nimport sys\nimport torch.optim as optim\nfrom torch import nn\nimport torch\n\nif __name__==\'__main__\':\n    config = Config()\n    train_file = \'../data/ag_news.train\'\n    if len(sys.argv) > 2:\n        train_file = sys.argv[1]\n    test_file = \'../data/ag_news.test\'\n    if len(sys.argv) > 3:\n        test_file = sys.argv[2]\n    \n    w2v_file = \'../data/glove.840B.300d.txt\'\n    \n    dataset = Dataset(config)\n    dataset.load_data(w2v_file, train_file, test_file)\n    \n    # Create Model with specified optimizer and loss function\n    ##############################################################\n    model = TextCNN(config, len(dataset.vocab), dataset.word_embeddings)\n    if torch.cuda.is_available():\n        model.cuda()\n    model.train()\n    optimizer = optim.SGD(model.parameters(), lr=config.lr)\n    NLLLoss = nn.NLLLoss()\n    model.add_optimizer(optimizer)\n    model.add_loss_op(NLLLoss)\n    ##############################################################\n    \n    train_losses = []\n    val_accuracies = []\n    \n    for i in range(config.max_epochs):\n        print (""Epoch: {}"".format(i))\n        train_loss,val_accuracy = model.run_epoch(dataset.train_iterator, dataset.val_iterator, i)\n        train_losses.append(train_loss)\n        val_accuracies.append(val_accuracy)\n\n    train_acc = evaluate_model(model, dataset.train_iterator)\n    val_acc = evaluate_model(model, dataset.val_iterator)\n    test_acc = evaluate_model(model, dataset.test_iterator)\n\n    print (\'Final Training Accuracy: {:.4f}\'.format(train_acc))\n    print (\'Final Validation Accuracy: {:.4f}\'.format(val_acc))\n    print (\'Final Test Accuracy: {:.4f}\'.format(test_acc))'"
Model_TextCNN/utils.py,2,"b'# utils.py\n\nimport torch\nfrom torchtext import data\nfrom torchtext.vocab import Vectors\nimport spacy\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\nclass Dataset(object):\n    def __init__(self, config):\n        self.config = config\n        self.train_iterator = None\n        self.test_iterator = None\n        self.val_iterator = None\n        self.vocab = []\n        self.word_embeddings = {}\n    \n    def parse_label(self, label):\n        \'\'\'\n        Get the actual labels from label string\n        Input:\n            label (string) : labels of the form \'__label__2\'\n        Returns:\n            label (int) : integer value corresponding to label string\n        \'\'\'\n        return int(label.strip()[-1])\n\n    def get_pandas_df(self, filename):\n        \'\'\'\n        Load the data into Pandas.DataFrame object\n        This will be used to convert data to torchtext object\n        \'\'\'\n        with open(filename, \'r\') as datafile:     \n            data = [line.strip().split(\',\', maxsplit=1) for line in datafile]\n            data_text = list(map(lambda x: x[1], data))\n            data_label = list(map(lambda x: self.parse_label(x[0]), data))\n\n        full_df = pd.DataFrame({""text"":data_text, ""label"":data_label})\n        return full_df\n    \n    def load_data(self, w2v_file, train_file, test_file, val_file=None):\n        \'\'\'\n        Loads the data from files\n        Sets up iterators for training, validation and test data\n        Also create vocabulary and word embeddings based on the data\n        \n        Inputs:\n            w2v_file (String): absolute path to file containing word embeddings (GloVe/Word2Vec)\n            train_file (String): absolute path to training file\n            test_file (String): absolute path to test file\n            val_file (String): absolute path to validation file\n        \'\'\'\n\n        NLP = spacy.load(\'en\')\n        tokenizer = lambda sent: [x.text for x in NLP.tokenizer(sent) if x.text != "" ""]\n        \n        # Creating Field for data\n        TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, fix_length=self.config.max_sen_len)\n        LABEL = data.Field(sequential=False, use_vocab=False)\n        datafields = [(""text"",TEXT),(""label"",LABEL)]\n        \n        # Load data from pd.DataFrame into torchtext.data.Dataset\n        train_df = self.get_pandas_df(train_file)\n        train_examples = [data.Example.fromlist(i, datafields) for i in train_df.values.tolist()]\n        train_data = data.Dataset(train_examples, datafields)\n        \n        test_df = self.get_pandas_df(test_file)\n        test_examples = [data.Example.fromlist(i, datafields) for i in test_df.values.tolist()]\n        test_data = data.Dataset(test_examples, datafields)\n        \n        # If validation file exists, load it. Otherwise get validation data from training data\n        if val_file:\n            val_df = self.get_pandas_df(val_file)\n            val_examples = [data.Example.fromlist(i, datafields) for i in val_df.values.tolist()]\n            val_data = data.Dataset(val_examples, datafields)\n        else:\n            train_data, val_data = train_data.split(split_ratio=0.8)\n        \n        TEXT.build_vocab(train_data, vectors=Vectors(w2v_file))\n        self.word_embeddings = TEXT.vocab.vectors\n        self.vocab = TEXT.vocab\n        \n        self.train_iterator = data.BucketIterator(\n            (train_data),\n            batch_size=self.config.batch_size,\n            sort_key=lambda x: len(x.text),\n            repeat=False,\n            shuffle=True)\n        \n        self.val_iterator, self.test_iterator = data.BucketIterator.splits(\n            (val_data, test_data),\n            batch_size=self.config.batch_size,\n            sort_key=lambda x: len(x.text),\n            repeat=False,\n            shuffle=False)\n        \n        print (""Loaded {} training examples"".format(len(train_data)))\n        print (""Loaded {} test examples"".format(len(test_data)))\n        print (""Loaded {} validation examples"".format(len(val_data)))\n\n\ndef evaluate_model(model, iterator):\n    all_preds = []\n    all_y = []\n    for idx,batch in enumerate(iterator):\n        if torch.cuda.is_available():\n            x = batch.text.cuda()\n        else:\n            x = batch.text\n        y_pred = model(x)\n        predicted = torch.max(y_pred.cpu().data, 1)[1] + 1\n        all_preds.extend(predicted.numpy())\n        all_y.extend(batch.label.numpy())\n    score = accuracy_score(all_y, np.array(all_preds).flatten())\n    return score'"
Model_TextRNN/config.py,0,b'# config.py\n\nclass Config(object):\n    embed_size = 300\n    hidden_layers = 2\n    hidden_size = 32\n    bidirectional = True\n    output_size = 4\n    max_epochs = 10\n    lr = 0.25\n    batch_size = 64\n    max_sen_len = 20 # Sequence length for RNN\n    dropout_keep = 0.8'
Model_TextRNN/model.py,4,"b'# model.py\n\nimport torch\nfrom torch import nn\nimport numpy as np\nfrom utils import *\n\nclass TextRNN(nn.Module):\n    def __init__(self, config, vocab_size, word_embeddings):\n        super(TextRNN, self).__init__()\n        self.config = config\n        \n        # Embedding Layer\n        self.embeddings = nn.Embedding(vocab_size, self.config.embed_size)\n        self.embeddings.weight = nn.Parameter(word_embeddings, requires_grad=False)\n        \n        self.lstm = nn.LSTM(input_size = self.config.embed_size,\n                            hidden_size = self.config.hidden_size,\n                            num_layers = self.config.hidden_layers,\n                            dropout = self.config.dropout_keep,\n                            bidirectional = self.config.bidirectional)\n        \n        self.dropout = nn.Dropout(self.config.dropout_keep)\n        \n        # Fully-Connected Layer\n        self.fc = nn.Linear(\n            self.config.hidden_size * self.config.hidden_layers * (1+self.config.bidirectional),\n            self.config.output_size\n        )\n        \n        # Softmax non-linearity\n        self.softmax = nn.Softmax()\n        \n    def forward(self, x):\n        # x.shape = (max_sen_len, batch_size)\n        embedded_sent = self.embeddings(x)\n        # embedded_sent.shape = (max_sen_len=20, batch_size=64,embed_size=300)\n\n        lstm_out, (h_n,c_n) = self.lstm(embedded_sent)\n        final_feature_map = self.dropout(h_n) # shape=(num_layers * num_directions, 64, hidden_size)\n        \n        # Convert input to (64, hidden_size * hidden_layers * num_directions) for linear layer\n        final_feature_map = torch.cat([final_feature_map[i,:,:] for i in range(final_feature_map.shape[0])], dim=1)\n        final_out = self.fc(final_feature_map)\n        return self.softmax(final_out)\n    \n    def add_optimizer(self, optimizer):\n        self.optimizer = optimizer\n        \n    def add_loss_op(self, loss_op):\n        self.loss_op = loss_op\n    \n    def reduce_lr(self):\n        print(""Reducing LR"")\n        for g in self.optimizer.param_groups:\n            g[\'lr\'] = g[\'lr\'] / 2\n                \n    def run_epoch(self, train_iterator, val_iterator, epoch):\n        train_losses = []\n        val_accuracies = []\n        losses = []\n        \n        # Reduce learning rate as number of epochs increase\n        if (epoch == int(self.config.max_epochs/3)) or (epoch == int(2*self.config.max_epochs/3)):\n            self.reduce_lr()\n            \n        for i, batch in enumerate(train_iterator):\n            self.optimizer.zero_grad()\n            if torch.cuda.is_available():\n                x = batch.text.cuda()\n                y = (batch.label - 1).type(torch.cuda.LongTensor)\n            else:\n                x = batch.text\n                y = (batch.label - 1).type(torch.LongTensor)\n            y_pred = self.__call__(x)\n            loss = self.loss_op(y_pred, y)\n            loss.backward()\n            losses.append(loss.data.cpu().numpy())\n            self.optimizer.step()\n    \n            if i % 100 == 0:\n                print(""Iter: {}"".format(i+1))\n                avg_train_loss = np.mean(losses)\n                train_losses.append(avg_train_loss)\n                print(""\\tAverage training loss: {:.5f}"".format(avg_train_loss))\n                losses = []\n                \n                # Evalute Accuracy on validation set\n                val_accuracy = evaluate_model(self, val_iterator)\n                print(""\\tVal Accuracy: {:.4f}"".format(val_accuracy))\n                self.train()\n                \n        return train_losses, val_accuracies'"
Model_TextRNN/train.py,2,"b'# train.py\n\nfrom utils import *\nfrom model import *\nfrom config import Config\nimport sys\nimport torch.optim as optim\nfrom torch import nn\nimport torch\n\nif __name__==\'__main__\':\n    config = Config()\n    train_file = \'../data/ag_news.train\'\n    if len(sys.argv) > 2:\n        train_file = sys.argv[1]\n    test_file = \'../data/ag_news.test\'\n    if len(sys.argv) > 3:\n        test_file = sys.argv[2]\n    \n    w2v_file = \'../data/glove.840B.300d.txt\'\n    \n    dataset = Dataset(config)\n    dataset.load_data(w2v_file, train_file, test_file)\n    \n    # Create Model with specified optimizer and loss function\n    ##############################################################\n    model = TextRNN(config, len(dataset.vocab), dataset.word_embeddings)\n    if torch.cuda.is_available():\n        model.cuda()\n    model.train()\n    optimizer = optim.SGD(model.parameters(), lr=config.lr)\n    NLLLoss = nn.NLLLoss()\n    model.add_optimizer(optimizer)\n    model.add_loss_op(NLLLoss)\n    ##############################################################\n    \n    train_losses = []\n    val_accuracies = []\n    \n    for i in range(config.max_epochs):\n        print (""Epoch: {}"".format(i))\n        train_loss,val_accuracy = model.run_epoch(dataset.train_iterator, dataset.val_iterator, i)\n        train_losses.append(train_loss)\n        val_accuracies.append(val_accuracy)\n\n    train_acc = evaluate_model(model, dataset.train_iterator)\n    val_acc = evaluate_model(model, dataset.val_iterator)\n    test_acc = evaluate_model(model, dataset.test_iterator)\n\n    print (\'Final Training Accuracy: {:.4f}\'.format(train_acc))\n    print (\'Final Validation Accuracy: {:.4f}\'.format(val_acc))\n    print (\'Final Test Accuracy: {:.4f}\'.format(test_acc))'"
Model_TextRNN/utils.py,2,"b'# utils.py\n\nimport torch\nfrom torchtext import data\nfrom torchtext.vocab import Vectors\nimport spacy\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\nclass Dataset(object):\n    def __init__(self, config):\n        self.config = config\n        self.train_iterator = None\n        self.test_iterator = None\n        self.val_iterator = None\n        self.vocab = []\n        self.word_embeddings = {}\n    \n    def parse_label(self, label):\n        \'\'\'\n        Get the actual labels from label string\n        Input:\n            label (string) : labels of the form \'__label__2\'\n        Returns:\n            label (int) : integer value corresponding to label string\n        \'\'\'\n        return int(label.strip()[-1])\n\n    def get_pandas_df(self, filename):\n        \'\'\'\n        Load the data into Pandas.DataFrame object\n        This will be used to convert data to torchtext object\n        \'\'\'\n        with open(filename, \'r\') as datafile:     \n            data = [line.strip().split(\',\', maxsplit=1) for line in datafile]\n            data_text = list(map(lambda x: x[1], data))\n            data_label = list(map(lambda x: self.parse_label(x[0]), data))\n\n        full_df = pd.DataFrame({""text"":data_text, ""label"":data_label})\n        return full_df\n    \n    def load_data(self, w2v_file, train_file, test_file, val_file=None):\n        \'\'\'\n        Loads the data from files\n        Sets up iterators for training, validation and test data\n        Also create vocabulary and word embeddings based on the data\n        \n        Inputs:\n            w2v_file (String): absolute path to file containing word embeddings (GloVe/Word2Vec)\n            train_file (String): absolute path to training file\n            test_file (String): absolute path to test file\n            val_file (String): absolute path to validation file\n        \'\'\'\n\n        NLP = spacy.load(\'en\')\n        tokenizer = lambda sent: [x.text for x in NLP.tokenizer(sent) if x.text != "" ""]\n        \n        # Creating Field for data\n        TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, fix_length=self.config.max_sen_len)\n        LABEL = data.Field(sequential=False, use_vocab=False)\n        datafields = [(""text"",TEXT),(""label"",LABEL)]\n        \n        # Load data from pd.DataFrame into torchtext.data.Dataset\n        train_df = self.get_pandas_df(train_file)\n        train_examples = [data.Example.fromlist(i, datafields) for i in train_df.values.tolist()]\n        train_data = data.Dataset(train_examples, datafields)\n        \n        test_df = self.get_pandas_df(test_file)\n        test_examples = [data.Example.fromlist(i, datafields) for i in test_df.values.tolist()]\n        test_data = data.Dataset(test_examples, datafields)\n        \n        # If validation file exists, load it. Otherwise get validation data from training data\n        if val_file:\n            val_df = self.get_pandas_df(val_file)\n            val_examples = [data.Example.fromlist(i, datafields) for i in val_df.values.tolist()]\n            val_data = data.Dataset(val_examples, datafields)\n        else:\n            train_data, val_data = train_data.split(split_ratio=0.8)\n        \n        TEXT.build_vocab(train_data, vectors=Vectors(w2v_file))\n        self.word_embeddings = TEXT.vocab.vectors\n        self.vocab = TEXT.vocab\n        \n        self.train_iterator = data.BucketIterator(\n            (train_data),\n            batch_size=self.config.batch_size,\n            sort_key=lambda x: len(x.text),\n            repeat=False,\n            shuffle=True)\n        \n        self.val_iterator, self.test_iterator = data.BucketIterator.splits(\n            (val_data, test_data),\n            batch_size=self.config.batch_size,\n            sort_key=lambda x: len(x.text),\n            repeat=False,\n            shuffle=False)\n        \n        print (""Loaded {} training examples"".format(len(train_data)))\n        print (""Loaded {} test examples"".format(len(test_data)))\n        print (""Loaded {} validation examples"".format(len(val_data)))\n\n\ndef evaluate_model(model, iterator):\n    all_preds = []\n    all_y = []\n    for idx,batch in enumerate(iterator):\n        if torch.cuda.is_available():\n            x = batch.text.cuda()\n        else:\n            x = batch.text\n        y_pred = model(x)\n        predicted = torch.max(y_pred.cpu().data, 1)[1] + 1\n        all_preds.extend(predicted.numpy())\n        all_y.extend(batch.label.numpy())\n    score = accuracy_score(all_y, np.array(all_preds).flatten())\n    return score'"
Model_Transformer/attention.py,3,"b'# attention.py\n\nimport torch\nfrom torch import nn\nimport math\nimport torch.nn.functional as F\nfrom train_utils import clones\n\ndef attention(query, key, value, mask=None, dropout=None):\n    ""Implementation of Scaled dot product attention""\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim = -1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        ""Take in model size and number of heads.""\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d_k\n        self.d_k = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n        \n    def forward(self, query, key, value, mask=None):\n        ""Implements Multi-head attention""\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        \n        # 1) Do all the linear projections in batch from d_model => h x d_k \n        query, key, value = \\\n            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key, value))]\n        \n        # 2) Apply attention on all the projected vectors in batch. \n        x, self.attn = attention(query, key, value, mask=mask, \n                                 dropout=self.dropout)\n        \n        # 3) ""Concat"" using a view and apply a final linear. \n        x = x.transpose(1, 2).contiguous() \\\n             .view(nbatches, -1, self.h * self.d_k)\n        return self.linears[-1](x)\n'"
Model_Transformer/config.py,0,b'# config.py\n\nclass Config(object):\n    N = 1 #6 in Transformer Paper\n    d_model = 256 #512 in Transformer Paper\n    d_ff = 512 #2048 in Transformer Paper\n    h = 8\n    dropout = 0.1\n    output_size = 4\n    lr = 0.0003\n    max_epochs = 35\n    batch_size = 128\n    max_sen_len = 60'
Model_Transformer/encoder.py,0,"b'# encoder.py\n\nfrom torch import nn\nfrom train_utils import clones\nfrom sublayer import LayerNorm, SublayerOutput\n\nclass Encoder(nn.Module):\n    \'\'\'\n    Transformer Encoder\n    \n    It is a stack of N layers.\n    \'\'\'\n    def __init__(self, layer, N):\n        super(Encoder, self).__init__()\n        self.layers = clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n        \n    def forward(self, x, mask=None):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)\n    \nclass EncoderLayer(nn.Module):\n    \'\'\'\n    An encoder layer\n    \n    Made up of self-attention and a feed forward layer.\n    Each of these sublayers have residual and layer norm, implemented by SublayerOutput.\n    \'\'\'\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.sublayer_output = clones(SublayerOutput(size, dropout), 2)\n        self.size = size\n\n    def forward(self, x, mask=None):\n        ""Transformer Encoder""\n        x = self.sublayer_output[0](x, lambda x: self.self_attn(x, x, x, mask)) # Encoder self-attention\n        return self.sublayer_output[1](x, self.feed_forward)'"
Model_Transformer/feed_forward.py,1,"b'# feed_forward.py\n\nfrom torch import nn\nimport torch.nn.functional as F\n\nclass PositionwiseFeedForward(nn.Module):\n    ""Positionwise feed-forward network.""\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        ""Implements FFN equation.""\n        return self.w_2(self.dropout(F.relu(self.w_1(x))))'"
Model_Transformer/model.py,4,"b'# Model.py\n\nimport torch\nimport torch.nn as nn\nfrom copy import deepcopy\nfrom train_utils import Embeddings,PositionalEncoding\nfrom attention import MultiHeadedAttention\nfrom encoder import EncoderLayer, Encoder\nfrom feed_forward import PositionwiseFeedForward\nimport numpy as np\nfrom utils import *\n\nclass Transformer(nn.Module):\n    def __init__(self, config, src_vocab):\n        super(Transformer, self).__init__()\n        self.config = config\n        \n        h, N, dropout = self.config.h, self.config.N, self.config.dropout\n        d_model, d_ff = self.config.d_model, self.config.d_ff\n        \n        attn = MultiHeadedAttention(h, d_model)\n        ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n        position = PositionalEncoding(d_model, dropout)\n        \n        self.encoder = Encoder(EncoderLayer(config.d_model, deepcopy(attn), deepcopy(ff), dropout), N)\n        self.src_embed = nn.Sequential(Embeddings(config.d_model, src_vocab), deepcopy(position)) #Embeddings followed by PE\n\n        # Fully-Connected Layer\n        self.fc = nn.Linear(\n            self.config.d_model,\n            self.config.output_size\n        )\n        \n        # Softmax non-linearity\n        self.softmax = nn.Softmax()\n\n    def forward(self, x):\n        embedded_sents = self.src_embed(x.permute(1,0)) # shape = (batch_size, sen_len, d_model)\n        encoded_sents = self.encoder(embedded_sents)\n        \n        # Convert input to (batch_size, d_model) for linear layer\n        final_feature_map = encoded_sents[:,-1,:]\n        final_out = self.fc(final_feature_map)\n        return self.softmax(final_out)\n    \n    def add_optimizer(self, optimizer):\n        self.optimizer = optimizer\n        \n    def add_loss_op(self, loss_op):\n        self.loss_op = loss_op\n    \n    def reduce_lr(self):\n        print(""Reducing LR"")\n        for g in self.optimizer.param_groups:\n            g[\'lr\'] = g[\'lr\'] / 2\n                \n    def run_epoch(self, train_iterator, val_iterator, epoch):\n        train_losses = []\n        val_accuracies = []\n        losses = []\n        \n        # Reduce learning rate as number of epochs increase\n        if (epoch == int(self.config.max_epochs/3)) or (epoch == int(2*self.config.max_epochs/3)):\n            self.reduce_lr()\n            \n        for i, batch in enumerate(train_iterator):\n            self.optimizer.zero_grad()\n            if torch.cuda.is_available():\n                x = batch.text.cuda()\n                y = (batch.label - 1).type(torch.cuda.LongTensor)\n            else:\n                x = batch.text\n                y = (batch.label - 1).type(torch.LongTensor)\n            y_pred = self.__call__(x)\n            loss = self.loss_op(y_pred, y)\n            loss.backward()\n            losses.append(loss.data.cpu().numpy())\n            self.optimizer.step()\n    \n            if i % 100 == 0:\n                print(""Iter: {}"".format(i+1))\n                avg_train_loss = np.mean(losses)\n                train_losses.append(avg_train_loss)\n                print(""\\tAverage training loss: {:.5f}"".format(avg_train_loss))\n                losses = []\n                \n                # Evalute Accuracy on validation set\n                val_accuracy = evaluate_model(self, val_iterator)\n                print(""\\tVal Accuracy: {:.4f}"".format(val_accuracy))\n                self.train()\n                \n        return train_losses, val_accuracies'"
Model_Transformer/sublayer.py,2,"b'# sublayer.py\n\nimport torch\nfrom torch import nn\n\nclass LayerNorm(nn.Module):\n    ""Construct a layer normalization module.""\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n    \nclass SublayerOutput(nn.Module):\n    \'\'\'\n    A residual connection followed by a layer norm.\n    \'\'\'\n    def __init__(self, size, dropout):\n        super(SublayerOutput, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        ""Apply residual connection to any sublayer with the same size.""\n        return x + self.dropout(sublayer(self.norm(x)))\n'"
Model_Transformer/train.py,2,"b'# train.py\n\nfrom utils import *\nfrom model import *\nfrom config import Config\nimport sys\nimport torch.optim as optim\nfrom torch import nn\nimport torch\n\nif __name__==\'__main__\':\n    config = Config()\n    train_file = \'../data/ag_news.train\'\n    if len(sys.argv) > 2:\n        train_file = sys.argv[1]\n    test_file = \'../data/ag_news.test\'\n    if len(sys.argv) > 3:\n        test_file = sys.argv[2]\n    \n    dataset = Dataset(config)\n    dataset.load_data(train_file, test_file)\n    \n    # Create Model with specified optimizer and loss function\n    ##############################################################\n    model = Transformer(config, len(dataset.vocab))\n    if torch.cuda.is_available():\n        model.cuda()\n    model.train()\n    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n    NLLLoss = nn.NLLLoss()\n    model.add_optimizer(optimizer)\n    model.add_loss_op(NLLLoss)\n    ##############################################################\n    \n    train_losses = []\n    val_accuracies = []\n    \n    for i in range(config.max_epochs):\n        print (""Epoch: {}"".format(i))\n        train_loss,val_accuracy = model.run_epoch(dataset.train_iterator, dataset.val_iterator, i)\n        train_losses.append(train_loss)\n        val_accuracies.append(val_accuracy)\n\n    train_acc = evaluate_model(model, dataset.train_iterator)\n    val_acc = evaluate_model(model, dataset.val_iterator)\n    test_acc = evaluate_model(model, dataset.test_iterator)\n\n    print (\'Final Training Accuracy: {:.4f}\'.format(train_acc))\n    print (\'Final Validation Accuracy: {:.4f}\'.format(val_acc))\n    print (\'Final Test Accuracy: {:.4f}\'.format(test_acc))'"
Model_Transformer/train_utils.py,6,"b'# train_utils.py\n\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport copy\nimport math\n\ndef clones(module, N):\n    ""Produce N identical layers.""\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n\nclass Embeddings(nn.Module):\n    \'\'\'\n    Usual Embedding layer with weights multiplied by sqrt(d_model)\n    \'\'\'\n    def __init__(self, d_model, vocab):\n        super(Embeddings, self).__init__()\n        self.lut = nn.Embedding(vocab, d_model)\n        self.d_model = d_model\n\n    def forward(self, x):\n        return self.lut(x) * math.sqrt(self.d_model)\n    \nclass PositionalEncoding(nn.Module):\n    ""Implement the PE function.""\n    def __init__(self, d_model, dropout, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n                             -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(torch.as_tensor(position.numpy() * div_term.unsqueeze(0).numpy()))\n        pe[:, 1::2] = torch.cos(torch.as_tensor(position.numpy() * div_term.unsqueeze(0).numpy()))#torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\'pe\', pe)\n        \n    def forward(self, x):\n        x = x + Variable(self.pe[:, :x.size(1)], \n                         requires_grad=False)\n        return self.dropout(x)'"
Model_Transformer/utils.py,2,"b'# utils.py\n\nimport torch\nfrom torchtext import data\nimport spacy\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\nclass Dataset(object):\n    def __init__(self, config):\n        self.config = config\n        self.train_iterator = None\n        self.test_iterator = None\n        self.val_iterator = None\n        self.vocab = []\n        self.word_embeddings = {}\n    \n    def parse_label(self, label):\n        \'\'\'\n        Get the actual labels from label string\n        Input:\n            label (string) : labels of the form \'__label__2\'\n        Returns:\n            label (int) : integer value corresponding to label string\n        \'\'\'\n        return int(label.strip()[-1])\n\n    def get_pandas_df(self, filename):\n        \'\'\'\n        Load the data into Pandas.DataFrame object\n        This will be used to convert data to torchtext object\n        \'\'\'\n        with open(filename, \'r\') as datafile:     \n            data = [line.strip().split(\',\', maxsplit=1) for line in datafile]\n            data_text = list(map(lambda x: x[1], data))\n            data_label = list(map(lambda x: self.parse_label(x[0]), data))\n\n        full_df = pd.DataFrame({""text"":data_text, ""label"":data_label})\n        return full_df\n    \n    def load_data(self, train_file, test_file=None, val_file=None):\n        \'\'\'\n        Loads the data from files\n        Sets up iterators for training, validation and test data\n        Also create vocabulary and word embeddings based on the data\n        \n        Inputs:\n            train_file (String): path to training file\n            test_file (String): path to test file\n            val_file (String): path to validation file\n        \'\'\'\n\n        NLP = spacy.load(\'en\')\n        tokenizer = lambda sent: [x.text for x in NLP.tokenizer(sent) if x.text != "" ""]\n        \n        # Creating Field for data\n        TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, fix_length=self.config.max_sen_len)\n        LABEL = data.Field(sequential=False, use_vocab=False)\n        datafields = [(""text"",TEXT),(""label"",LABEL)]\n        \n        # Load data from pd.DataFrame into torchtext.data.Dataset\n        train_df = self.get_pandas_df(train_file)\n        train_examples = [data.Example.fromlist(i, datafields) for i in train_df.values.tolist()]\n        train_data = data.Dataset(train_examples, datafields)\n        \n        test_df = self.get_pandas_df(test_file)\n        test_examples = [data.Example.fromlist(i, datafields) for i in test_df.values.tolist()]\n        test_data = data.Dataset(test_examples, datafields)\n        \n        # If validation file exists, load it. Otherwise get validation data from training data\n        if val_file:\n            val_df = self.get_pandas_df(val_file)\n            val_examples = [data.Example.fromlist(i, datafields) for i in val_df.values.tolist()]\n            val_data = data.Dataset(val_examples, datafields)\n        else:\n            train_data, val_data = train_data.split(split_ratio=0.8)\n        \n        TEXT.build_vocab(train_data)\n        self.vocab = TEXT.vocab\n        \n        self.train_iterator = data.BucketIterator(\n            (train_data),\n            batch_size=self.config.batch_size,\n            sort_key=lambda x: len(x.text),\n            repeat=False,\n            shuffle=True)\n        \n        self.val_iterator, self.test_iterator = data.BucketIterator.splits(\n            (val_data, test_data),\n            batch_size=self.config.batch_size,\n            sort_key=lambda x: len(x.text),\n            repeat=False,\n            shuffle=False)\n        \n        print (""Loaded {} training examples"".format(len(train_data)))\n        print (""Loaded {} test examples"".format(len(test_data)))\n        print (""Loaded {} validation examples"".format(len(val_data)))\n\ndef evaluate_model(model, iterator):\n    all_preds = []\n    all_y = []\n    for idx,batch in enumerate(iterator):\n        if torch.cuda.is_available():\n            x = batch.text.cuda()\n        else:\n            x = batch.text\n        y_pred = model(x)\n        predicted = torch.max(y_pred.cpu().data, 1)[1] + 1\n        all_preds.extend(predicted.numpy())\n        all_y.extend(batch.label.numpy())\n    score = accuracy_score(all_y, np.array(all_preds).flatten())\n    return score'"
Model_fastText/config.py,0,b'# config.py\n\nclass Config(object):\n    embed_size = 300\n    hidden_size = 10\n    output_size = 4\n    max_epochs = 30\n    lr = 0.5\n    batch_size = 128'
Model_fastText/model.py,3,"b'# model.py\n\nimport torch\nfrom torch import nn\nimport numpy as np\nfrom utils import *\n\nclass fastText(nn.Module):\n    def __init__(self, config, vocab_size, word_embeddings):\n        super(fastText, self).__init__()\n        self.config = config\n        \n        # Embedding Layer\n        self.embeddings = nn.Embedding(vocab_size, self.config.embed_size)\n        self.embeddings.weight = nn.Parameter(word_embeddings, requires_grad=False)\n        \n        # Hidden Layer\n        self.fc1 = nn.Linear(self.config.embed_size, self.config.hidden_size)\n        \n        # Output Layer\n        self.fc2 = nn.Linear(self.config.hidden_size, self.config.output_size)\n        \n        # Softmax non-linearity\n        self.softmax = nn.Softmax()\n        \n    def forward(self, x):\n        embedded_sent = self.embeddings(x).permute(1,0,2)\n        h = self.fc1(embedded_sent.mean(1))\n        z = self.fc2(h)\n        return self.softmax(z)\n    \n    def add_optimizer(self, optimizer):\n        self.optimizer = optimizer\n        \n    def add_loss_op(self, loss_op):\n        self.loss_op = loss_op\n    \n    def reduce_lr(self):\n        print(""Reducing LR"")\n        for g in self.optimizer.param_groups:\n            g[\'lr\'] = g[\'lr\'] / 2\n                \n    def run_epoch(self, train_iterator, val_iterator, epoch):\n        train_losses = []\n        val_accuracies = []\n        losses = []\n        \n        # Reduce learning rate as number of epochs increase\n        if (epoch == int(self.config.max_epochs/3)) or (epoch == int(2*self.config.max_epochs/3)):\n            self.reduce_lr()\n            \n        for i, batch in enumerate(train_iterator):\n            self.optimizer.zero_grad()\n            if torch.cuda.is_available():\n                x = batch.text.cuda()\n                y = (batch.label - 1).type(torch.cuda.LongTensor)\n            else:\n                x = batch.text\n                y = (batch.label - 1).type(torch.LongTensor)\n            y_pred = self.__call__(x)\n            loss = self.loss_op(y_pred, y)\n            loss.backward()\n            losses.append(loss.data.cpu().numpy())\n            self.optimizer.step()\n    \n            if i % 100 == 0:\n                print(""Iter: {}"".format(i+1))\n                avg_train_loss = np.mean(losses)\n                train_losses.append(avg_train_loss)\n                print(""\\tAverage training loss: {:.5f}"".format(avg_train_loss))\n                losses = []\n                \n                # Evalute Accuracy on validation set\n                val_accuracy = evaluate_model(self, val_iterator)\n                print(""\\tVal Accuracy: {:.4f}"".format(val_accuracy))\n                self.train()\n                \n        return train_losses, val_accuracies'"
Model_fastText/train.py,2,"b'# train.py\n\nfrom utils import *\nfrom model import *\nfrom config import Config\nimport numpy as np\nimport sys\nimport torch.optim as optim\nfrom torch import nn\nimport torch\n\nif __name__==\'__main__\':\n    config = Config()\n    train_file = \'../data/ag_news.train\'\n    if len(sys.argv) > 2:\n        train_file = sys.argv[1]\n    test_file = \'../data/ag_news.test\'\n    if len(sys.argv) > 3:\n        test_file = sys.argv[2]\n    \n    w2v_file = \'../data/glove.840B.300d.txt\'\n    \n    dataset = Dataset(config)\n    dataset.load_data(w2v_file, train_file, test_file)\n    \n    # Create Model with specified optimizer and loss function\n    ##############################################################\n    model = fastText(config, len(dataset.vocab), dataset.word_embeddings)\n    if torch.cuda.is_available():\n        model.cuda()\n    model.train()\n    optimizer = optim.SGD(model.parameters(), lr=config.lr)\n    NLLLoss = nn.NLLLoss()\n    model.add_optimizer(optimizer)\n    model.add_loss_op(NLLLoss)\n    ##############################################################\n    \n    train_losses = []\n    val_accuracies = []\n    \n    for i in range(config.max_epochs):\n        print (""Epoch: {}"".format(i))\n        train_loss,val_accuracy = model.run_epoch(dataset.train_iterator, dataset.val_iterator, i)\n        train_losses.append(train_loss)\n        val_accuracies.append(val_accuracy)\n\n    train_acc = evaluate_model(model, dataset.train_iterator)\n    val_acc = evaluate_model(model, dataset.val_iterator)\n    test_acc = evaluate_model(model, dataset.test_iterator)\n\n    print (\'Final Training Accuracy: {:.4f}\'.format(train_acc))\n    print (\'Final Validation Accuracy: {:.4f}\'.format(val_acc))\n    print (\'Final Test Accuracy: {:.4f}\'.format(test_acc))'"
Model_fastText/utils.py,2,"b'# utils.py\n\nimport torch\nfrom torchtext import data\nfrom torchtext.vocab import Vectors\nimport spacy\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\nclass Dataset(object):\n    def __init__(self, config):\n        self.config = config\n        self.train_iterator = None\n        self.test_iterator = None\n        self.val_iterator = None\n        self.vocab = []\n        self.word_embeddings = {}\n    \n    def parse_label(self, label):\n        \'\'\'\n        Get the actual labels from label string\n        Input:\n            label (string) : labels of the form \'__label__2\'\n        Returns:\n            label (int) : integer value corresponding to label string\n        \'\'\'\n        return int(label.strip()[-1])\n\n    def get_pandas_df(self, filename):\n        \'\'\'\n        Load the data into Pandas.DataFrame object\n        This will be used to convert data to torchtext object\n        \'\'\'\n        with open(filename, \'r\') as datafile:     \n            data = [line.strip().split(\',\', maxsplit=1) for line in datafile]\n            data_text = list(map(lambda x: x[1], data))\n            data_label = list(map(lambda x: self.parse_label(x[0]), data))\n\n        full_df = pd.DataFrame({""text"":data_text, ""label"":data_label})\n        return full_df\n    \n    def load_data(self, w2v_file, train_file, test_file, val_file=None):\n        \'\'\'\n        Loads the data from files\n        Sets up iterators for training, validation and test data\n        Also create vocabulary and word embeddings based on the data\n        \n        Inputs:\n            w2v_file (String): absolute path to file containing word embeddings (GloVe/Word2Vec)\n            train_file (String): absolute path to training file\n            test_file (String): absolute path to test file\n            val_file (String): absolute path to validation file\n        \'\'\'\n\n        NLP = spacy.load(\'en\')\n        tokenizer = lambda sent: [x.text for x in NLP.tokenizer(sent) if x.text != "" ""]\n        \n        # Creating Field for data\n        TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True)\n        LABEL = data.Field(sequential=False, use_vocab=False)\n        datafields = [(""text"",TEXT),(""label"",LABEL)]\n        \n        # Load data from pd.DataFrame into torchtext.data.Dataset\n        train_df = self.get_pandas_df(train_file)\n        train_examples = [data.Example.fromlist(i, datafields) for i in train_df.values.tolist()]\n        train_data = data.Dataset(train_examples, datafields)\n        \n        test_df = self.get_pandas_df(test_file)\n        test_examples = [data.Example.fromlist(i, datafields) for i in test_df.values.tolist()]\n        test_data = data.Dataset(test_examples, datafields)\n        \n        # If validation file exists, load it. Otherwise get validation data from training data\n        if val_file:\n            val_df = self.get_pandas_df(val_file)\n            val_examples = [data.Example.fromlist(i, datafields) for i in val_df.values.tolist()]\n            val_data = data.Dataset(val_examples, datafields)\n        else:\n            train_data, val_data = train_data.split(split_ratio=0.8)\n        \n        TEXT.build_vocab(train_data, vectors=Vectors(w2v_file))\n        self.word_embeddings = TEXT.vocab.vectors\n        self.vocab = TEXT.vocab\n        \n        self.train_iterator = data.BucketIterator(\n            (train_data),\n            batch_size=self.config.batch_size,\n            sort_key=lambda x: len(x.text),\n            repeat=False,\n            shuffle=True)\n        \n        self.val_iterator, self.test_iterator = data.BucketIterator.splits(\n            (val_data, test_data),\n            batch_size=self.config.batch_size,\n            sort_key=lambda x: len(x.text),\n            repeat=False,\n            shuffle=False)\n        \n        print (""Loaded {} training examples"".format(len(train_data)))\n        print (""Loaded {} test examples"".format(len(test_data)))\n        print (""Loaded {} validation examples"".format(len(val_data)))\n\n\ndef evaluate_model(model, iterator):\n    all_preds = []\n    all_y = []\n    for idx,batch in enumerate(iterator):\n        if torch.cuda.is_available():\n            x = batch.text.cuda()\n        else:\n            x = batch.text\n        y_pred = model(x)\n        predicted = torch.max(y_pred.cpu().data, 1)[1] + 1\n        all_preds.extend(predicted.numpy())\n        all_y.extend(batch.label.numpy())\n    score = accuracy_score(all_y, np.array(all_preds).flatten())\n    return score'"
Model_CharCNN/without_torchtext/config.py,0,b'# config.py\n\nclass Config(object):\n    num_channels = 256\n    linear_size = 256\n    output_size = 4\n    max_epochs = 10\n    lr = 0.001\n    batch_size = 128\n    vocab_size = 68\n    max_len = 300 # 1014 in original paper\n    dropout_keep = 0.5'
Model_CharCNN/without_torchtext/model.py,2,"b'# model.py\n\nimport torch\nfrom torch import nn\nimport numpy as np\nfrom torch.autograd import Variable\nfrom utils import *\n\nclass CharCNN(nn.Module):\n    def __init__(self, config):\n        super(CharCNN, self).__init__()\n        self.config = config\n        \n        # This stackoverflow thread explains how conv1d works\n        # https://stackoverflow.com/questions/46503816/keras-conv1d-layer-parameters-filters-and-kernel-size/46504997\n        conv1 = nn.Sequential(\n            nn.Conv1d(in_channels=self.config.vocab_size, out_channels=self.config.num_channels, kernel_size=7),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=3)\n        ) # (batch_size, num_channels, (max_len-6)/3)\n        conv2 = nn.Sequential(\n            nn.Conv1d(in_channels=self.config.num_channels, out_channels=self.config.num_channels, kernel_size=7),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=3)\n        ) # (batch_size, num_channels, (max_len-6-18)/(3*3))\n        conv3 = nn.Sequential(\n            nn.Conv1d(in_channels=self.config.num_channels, out_channels=self.config.num_channels, kernel_size=3),\n            nn.ReLU()\n        ) # (batch_size, num_channels, (max_len-6-18-18)/(3*3))\n        conv4 = nn.Sequential(\n            nn.Conv1d(in_channels=self.config.num_channels, out_channels=self.config.num_channels, kernel_size=3),\n            nn.ReLU()\n        ) # (batch_size, num_channels, (max_len-6-18-18-18)/(3*3))\n        conv5 = nn.Sequential(\n            nn.Conv1d(in_channels=self.config.num_channels, out_channels=self.config.num_channels, kernel_size=3),\n            nn.ReLU()\n        ) # (batch_size, num_channels, (max_len-6-18-18-18-18)/(3*3))\n        conv6 = nn.Sequential(\n            nn.Conv1d(in_channels=self.config.num_channels, out_channels=self.config.num_channels, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=3)\n        ) # (batch_size, num_channels, (max_len-6-18-18-18-18-18)/(3*3*3))\n        \n        # Length of output after conv6        \n        conv_output_size = self.config.num_channels * ((self.config.max_len - 96) // 27)\n        \n        linear1 = nn.Sequential(\n            nn.Linear(conv_output_size, self.config.linear_size),\n            nn.ReLU(),\n            nn.Dropout(self.config.dropout_keep)\n        )\n        linear2 = nn.Sequential(\n            nn.Linear(self.config.linear_size, self.config.linear_size),\n            nn.ReLU(),\n            nn.Dropout(self.config.dropout_keep)\n        )\n        linear3 = nn.Sequential(\n            nn.Linear(self.config.linear_size, self.config.output_size),\n            nn.Softmax()\n        )\n        \n        self.convolutional_layers = nn.Sequential(conv1,conv2,conv3,conv4,conv5,conv6)\n        self.linear_layers = nn.Sequential(linear1, linear2, linear3)\n        \n        # Initialize Weights\n        self._create_weights(mean=0.0, std=0.05)\n    \n    def _create_weights(self, mean=0.0, std=0.05):\n        for module in self.modules():\n            if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n                module.weight.data.normal_(mean, std)\n    \n    def forward(self, embedded_sent):\n        embedded_sent = embedded_sent.transpose(1,2)#.permute(0,2,1) # shape=(batch_size,embed_size,max_len)\n        conv_out = self.convolutional_layers(embedded_sent)\n        conv_out = conv_out.view(conv_out.shape[0], -1)\n        linear_output = self.linear_layers(conv_out)\n        return linear_output\n    \n    def add_optimizer(self, optimizer):\n        self.optimizer = optimizer\n        \n    def add_loss_op(self, loss_op):\n        self.loss_op = loss_op\n    \n    def reduce_lr(self):\n        print(""Reducing LR"")\n        for g in self.optimizer.param_groups:\n            g[\'lr\'] = g[\'lr\'] / 2\n                \n    def run_epoch(self, train_iterator, val_iterator, epoch):\n        train_losses = []\n        val_accuracies = []\n        losses = []\n        \n        # Reduce learning rate as number of epochs increase\n        if (epoch > 0 and epoch%3 == 0):\n            self.reduce_lr()\n            \n        for i, batch in enumerate(train_iterator):\n            _, n_true_label = batch\n            if torch.cuda.is_available():\n                batch = [Variable(record).cuda() for record in batch]\n            else:\n                batch = [Variable(record) for record in batch]\n            x, y = batch\n            \n            self.optimizer.zero_grad()\n            y_pred = self.__call__(x)\n            loss = self.loss_op(y_pred, y)\n            loss.backward()\n            losses.append(loss.data.cpu().numpy())\n            self.optimizer.step()\n    \n            if i % 100 == 0:\n                self.eval()\n                print(""Iter: {}"".format(i+1))\n                avg_train_loss = np.mean(losses)\n                train_losses.append(avg_train_loss)\n                print(""\\tAverage training loss: {:.5f}"".format(avg_train_loss))\n                losses = []\n                \n                # Evalute Accuracy on validation set\n                val_accuracy = evaluate_model(self, val_iterator)\n                print(""\\tVal Accuracy: {:.4f}"".format(val_accuracy))\n                self.train()\n                    \n        return train_losses, val_accuracies'"
Model_CharCNN/without_torchtext/train.py,2,"b'# train.py\n\nfrom utils import *\nfrom model import *\nfrom config import Config\nimport sys\nimport torch\nimport torch.optim as optim\nfrom torch import nn\n\nif __name__==\'__main__\':\n    config = Config()\n    train_file = \'../data/ag_news.train\'\n    if len(sys.argv) > 2:\n        train_file = sys.argv[1]\n    test_file = \'../data/ag_news.test\'\n    if len(sys.argv) > 3:\n        test_file = sys.argv[2]\n    \n    train_iterator, test_iterator, val_iterator = get_iterators(config, train_file, test_file)\n    \n    # Create Model with specified optimizer and loss function\n    ##############################################################\n    model = CharCNN(config)\n    if torch.cuda.is_available():\n        model.cuda()\n    model.train()\n    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n    loss_fn = nn.CrossEntropyLoss()\n    model.add_optimizer(optimizer)\n    model.add_loss_op(loss_fn)\n    ##############################################################\n    \n    train_losses = []\n    val_accuracies = []\n    \n    for i in range(config.max_epochs):\n        print (""Epoch: {}"".format(i))\n        train_loss,val_accuracy = model.run_epoch(train_iterator, val_iterator, i)\n        train_losses.append(train_loss)\n        val_accuracies.append(val_accuracy)\n\n    train_acc = evaluate_model(model, train_iterator)\n    val_acc = evaluate_model(model, val_iterator)\n    test_acc = evaluate_model(model, test_iterator)\n\n    print (\'Final Training Accuracy: {:.4f}\'.format(train_acc))\n    print (\'Final Validation Accuracy: {:.4f}\'.format(val_acc))\n    print (\'Final Test Accuracy: {:.4f}\'.format(test_acc))'"
Model_CharCNN/without_torchtext/utils.py,6,"b'# utils.py\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils import data\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\nfrom sklearn.metrics import accuracy_score\n\n# Used part of code to read the dataset from: https://github.com/1991viet/Character-level-cnn-pytorch/blob/master/src/dataset.py\nclass MyDataset(Dataset):\n    def __init__(self, data_path, config):\n        self.config = config\n        self.vocabulary = list(""""""abcdefghijklmnopqrstuvwxyz0123456789,;.!?:\'\\""/\\\\|_@#$%^&*~`+-=<>()[]{}"""""")\n        self.identity_mat = np.identity(len(self.vocabulary))\n        data = get_pandas_df(data_path)\n        self.texts = list(data.text)\n        self.labels = list(data.label)\n        self.length = len(self.labels)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        raw_text = self.texts[index]\n        data = np.array([self.identity_mat[self.vocabulary.index(i)] for i in list(raw_text) if i in self.vocabulary],\n                        dtype=np.float32)\n        if len(data) > self.config.max_len:\n            data = data[:self.config.max_len]\n        elif 0 < len(data) < self.config.max_len:\n            data = np.concatenate(\n                (data, np.zeros((self.config.max_len - len(data), len(self.vocabulary)), dtype=np.float32)))\n        elif len(data) == 0:\n            data = np.zeros((self.config.max_len, len(self.vocabulary)), dtype=np.float32)\n        label = self.labels[index]\n        return data, label\n\ndef parse_label(label):\n    \'\'\'\n    Get the actual labels from label string\n    Input:\n        label (string) : labels of the form \'__label__2\'\n    Returns:\n        label (int) : integer value corresponding to label string\n    \'\'\'\n    return int(label.strip()[-1]) - 1\n\ndef get_pandas_df(filename):\n    \'\'\'\n    Load the data into Pandas.DataFrame object\n    This will be used to convert data to torchtext object\n    \'\'\'\n    with open(filename, \'r\') as datafile:\n        data = [line.strip().split(\',\', maxsplit=1) for line in datafile]\n        data_text = list(map(lambda x: x[1], data))\n        data_label = list(map(lambda x: parse_label(x[0]), data))\n\n    full_df = pd.DataFrame({""text"":data_text, ""label"":data_label})\n    return full_df\n\ndef get_iterators(config, train_file, test_file, val_file=None):\n    train_set = MyDataset(train_file, config)\n    test_set = MyDataset(test_file, config)\n    \n    # If validation file exists, load it. Otherwise get validation data from training data\n    if val_file:\n        val_set = MyDataset(val_file, config)\n    else:\n        train_size = int(0.9 * len(train_set))\n        test_size = len(train_set) - train_size\n        train_set, val_set = data.random_split(train_set, [train_size, test_size])\n    \n    train_iterator = DataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n    test_iterator = DataLoader(test_set, batch_size=config.batch_size)\n    val_iterator = DataLoader(val_set, batch_size=config.batch_size)\n    return train_iterator, test_iterator, val_iterator\n\ndef evaluate_model(model, iterator):\n    all_preds = []\n    all_y = []\n    for idx,batch in enumerate(iterator):\n        if torch.cuda.is_available():\n            batch = [Variable(record).cuda() for record in batch]\n        else:\n            batch = [Variable(record).cuda() for record in batch]\n        x, y = batch\n        y_pred = model(x)\n        predicted = torch.max(y_pred.cpu().data, 1)[1]\n        all_preds.extend(predicted.numpy())\n        all_y.extend(y.cpu().numpy())\n        \n    score = accuracy_score(all_y, np.array(all_preds).flatten())\n    return score'"
Model_TextCNN/old_code/config.py,0,"b'# config.py\n\nclass Config(object):\n    embed_size = 300\n    in_channels = 1\n    num_channels = 100\n    kernel_size = [3,4,5]\n    output_size = 4\n    max_epochs = 10\n    lr = 0.25\n    batch_size = 64\n    max_sen_len = 20\n    dropout_keep = 0.6'"
Model_TextCNN/old_code/model.py,4,"b'# model.py\n\nimport torch\nfrom torch import nn\nfrom torch import Tensor\nfrom torch.autograd import Variable\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\nclass CNNText(nn.Module):\n    def __init__(self, config):\n        super(CNNText, self).__init__()\n        self.config = config\n        \n        # Convolutional Layer\n        # We use 3 kernels as in original paper\n        # Size of kernels: (3,300),(4,300),(5,300)\n        \n        self.conv1 = nn.Conv2d(in_channels=self.config.in_channels, out_channels=self.config.num_channels,\n                               kernel_size=(self.config.kernel_size[0],self.config.embed_size),\n                               stride=1, padding=0)\n        self.activation1 = nn.ReLU()\n        self.max_out1 = nn.MaxPool1d(self.config.max_sen_len - self.config.kernel_size[0]+1)\n\n        self.conv2 = nn.Conv2d(in_channels=self.config.in_channels, out_channels=self.config.num_channels,\n                               kernel_size=(self.config.kernel_size[1],self.config.embed_size),\n                               stride=1, padding=0)\n        self.activation2 = nn.ReLU()\n        self.max_out2 = nn.MaxPool1d(self.config.max_sen_len - self.config.kernel_size[1]+1)\n        \n        self.conv3 = nn.Conv2d(in_channels=self.config.in_channels, out_channels=self.config.num_channels,\n                               kernel_size=(self.config.kernel_size[2],self.config.embed_size),\n                               stride=1, padding=0)\n        self.activation3 = nn.ReLU()\n        self.max_out3 = nn.MaxPool1d(self.config.max_sen_len - self.config.kernel_size[2]+1)\n        \n        self.dropout = nn.Dropout(self.config.dropout_keep)\n        \n        # Fully-Connected Layer\n        self.fc = nn.Linear(self.config.num_channels*len(self.config.kernel_size), self.config.output_size)\n        \n        # Softmax non-linearity\n        self.softmax = nn.Softmax()\n        \n    def forward(self, x):\n        x = x.unsqueeze(1) # (batch_size,max_seq_len,embed_size) => (batch_size,1,max_seq_len,embed_size)\n        \n        conv_out1 = self.conv1(x).squeeze(3)\n        activation_out1 = self.activation1(conv_out1)\n        max_out1 = self.max_out1(activation_out1).squeeze(2)\n        \n        conv_out2 = self.conv2(x).squeeze(3)\n        activation_out2 = self.activation2(conv_out2)\n        max_out2 = self.max_out2(activation_out2).squeeze(2)\n        \n        conv_out3 = self.conv3(x).squeeze(3)\n        activation_out3 = self.activation3(conv_out3)\n        max_out3 = self.max_out3(activation_out3).squeeze(2)\n        \n        all_out = torch.cat((max_out1, max_out2, max_out3), 1)\n        \n        final_feature_map = self.dropout(all_out)\n        final_out = self.fc(final_feature_map)\n        return self.softmax(final_out)\n    \n    def add_optimizer(self, optimizer):\n        self.optimizer = optimizer\n        \n    def add_loss_op(self, loss_op):\n        self.loss_op = loss_op\n    \n    def run_epoch(self, train_data, val_data):\n        train_x, train_y = train_data[0], train_data[1]\n        val_x, val_y = val_data[0], val_data[1]\n        iterator = data_iterator(train_x, train_y, self.config.batch_size)\n        train_losses = []\n        val_accuracies = []\n        losses = []\n    \n        for i, (x,y) in enumerate(iterator):\n            self.optimizer.zero_grad()\n    \n            x = Tensor(x).cuda()\n            y_pred = self.__call__(x)\n            loss = self.loss_op(y_pred, torch.cuda.LongTensor(y-1))\n            loss.backward()\n    \n            losses.append(loss.data.cpu().numpy())\n            self.optimizer.step()\n    \n            if (i + 1) % 50 == 0:\n                print(""Iter: {}"".format(i+1))\n                avg_train_loss = np.mean(losses)\n                train_losses.append(avg_train_loss)\n                print(""\\tAverage training loss: {:.5f}"".format(avg_train_loss))\n                losses = []\n                \n                # Evalute Accuracy on validation set\n                self.eval()\n                all_preds = []\n                val_iterator = data_iterator(val_x, val_y, self.config.batch_size)\n                for j, (x,y) in enumerate(val_iterator):\n                    x = Variable(Tensor(x))\n                    y_pred = self.__call__(x.cuda())\n                    predicted = torch.max(y_pred.cpu().data, 1)[1] + 1\n                    all_preds.extend(predicted.numpy())\n                score = accuracy_score(val_y, np.array(all_preds).flatten())\n                val_accuracies.append(score)\n                print(""\\tVal Accuracy: {:.4f}"".format(score))\n                self.train()\n                \n        return train_losses, val_accuracies\n'"
Model_TextCNN/old_code/train.py,3,"b'# train.py\n\nfrom utils import *\nfrom config import Config\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom tqdm import tqdm\nimport sys\nimport torch.optim as optim\nfrom torch import nn, Tensor\nfrom torch.autograd import Variable\nimport torch\nfrom sklearn.metrics import accuracy_score\n\ndef get_accuracy(model, test_x, test_y):\n    all_preds = []\n    test_iterator = data_iterator(test_x, test_y)\n    for x, y in test_iterator:\n        x = Variable(Tensor(x))\n        y_pred = model(x.cuda())\n        predicted = torch.max(y_pred.cpu().data, 1)[1] + 1\n        all_preds.extend(predicted.numpy())\n    score = accuracy_score(test_y, np.array(all_preds).flatten())\n    return score\n\nif __name__==\'__main__\':\n    train_path = \'../data/ag_news.train\'\n    if len(sys.argv) > 2:\n        train_path = sys.argv[1]\n    test_path = \'../data/ag_news.test\'\n    if len(sys.argv) > 3:\n        test_path = sys.argv[2]\n        \n    train_text, train_labels, vocab = get_data(train_path)\n    train_text, val_text, train_label, val_label = train_test_split(train_text, train_labels, test_size=0.2)\n    \n    # Read Word Embeddings\n    w2vfile = \'../data/glove.840B.300d.txt\'\n    word_embeddings = get_word_embeddings(w2vfile, vocab.word_to_index, embedsize=300)\n    \n    # Get all configuration parameters\n    config = Config()\n    \n    train_x = np.array([encode_text(text, word_embeddings, config.max_sen_len) for text in tqdm(train_text)]) #(num_examples, max_sen_len, embed_size)\n    train_y = np.array(train_label) #(num_examples)\n        \n    val_x = np.array([encode_text(text, word_embeddings, config.max_sen_len) for text in tqdm(val_text)])\n    val_y = np.array(val_label)\n    \n    # Create Model with specified optimizer and loss function\n    ##############################################################\n    model = CNNText(config)\n    model.cuda()\n    model.train()\n    optimizer = optim.SGD(model.parameters(), lr=config.lr)\n    NLLLoss = nn.NLLLoss()\n    model.add_optimizer(optimizer)\n    model.add_loss_op(NLLLoss)\n    ##############################################################\n\n    train_data = [train_x, train_y]\n    val_data = [val_x, val_y]\n\n    for i in range(config.max_epochs):\n        print (""Epoch: {}"".format(i))\n        train_losses,val_accuracies = model.run_epoch(train_data, val_data)\n        print(""\\tAverage training loss: {:.5f}"".format(np.mean(train_losses)))\n        print(""\\tAverage Val Accuracy (per 50 iterations): {:.4f}"".format(np.mean(val_accuracies)))\n\n        # Reduce learning rate as number of epochs increase\n        if i > 0.5 * config.max_epochs:\n            print(""Reducing LR"")\n            for g in optimizer.param_groups:\n                g[\'lr\'] = 0.1\n        if i > 0.75 * config.max_epochs:\n            print(""Reducing LR"")\n            for g in optimizer.param_groups:\n                g[\'lr\'] = 0.05\n\n    # Get Accuracy of final model\n    test_text, test_labels, test_vocab = get_data(test_path)\n    test_x = np.array([encode_text(text, word_embeddings, config.max_sen_len) for text in tqdm(test_text)])\n    test_y = np.array(test_labels)\n\n    train_acc = get_accuracy(model, train_x, train_y)\n    val_acc = get_accuracy(model, val_x, val_y)\n    test_acc = get_accuracy(model, test_x, test_y)\n\n    print (\'Final Training Accuracy: {:.4f}\'.format(train_acc))\n    print (\'Final Validation Accuracy: {:.4f}\'.format(val_acc))\n    print (\'Final Test Accuracy: {:.4f}\'.format(test_acc))'"
Model_TextCNN/old_code/utils.py,0,"b'# utils.py\n\nfrom nltk import word_tokenize\nfrom tqdm import tqdm\nfrom gensim.models import KeyedVectors\nimport numpy as np\nimport math\n\nclass Vocab(object):\n    def __init__(self):\n        self.word_to_index = {}\n        self.index_to_word = {}\n        self.unknown = \'<unk>\'\n        self.add_word(self.unknown)\n        \n    def add_word(self, word):\n        \'\'\' Add a word to the vocabulary\n        Inputs:\n            word (string) : Word to be added to vocabulary\n        \'\'\'\n        \n        if word not in self.word_to_index:\n            index = len(self.word_to_index)\n            self.word_to_index[word] = index\n            self.index_to_word[index] = word\n    \n    def construct(self, words):\n        \'\'\' Construct the vocabulary\n        Inputs:\n            words (list[string]) : List of words defining the vocabulary\n        \'\'\'\n        \n        for word in words:\n            self.add_word(word)\n        print (""Constructed vocabulary of size: {}"".format(len(self.word_to_index)))\n        \n    def encode(self, word):\n        \'\'\'\n        Given a word, get corresponding index in vocabulary.\n        \'\'\'\n        if word not in self.word_to_index:\n            word = self.unknown\n        return self.word_to_index(word)\n    \n    def decode(self, index):\n        \'\'\'\n        Given a word index, get corresponding word from vocabulary.\n        \'\'\'\n        return self.index_to_word(index)\n    \n    def __len__(self):\n        return len(self.word_to_index)\n\ndef parse_label(label):\n    \'\'\'\n    Get the actual labels from label string\n    Input:\n        label (string) : labels of the form \'__label__2\'\n    Returns:\n        label (int) : integer value corresponding to label string\n    \'\'\'\n    return int(label.strip()[-1])\n\ndef get_data(filename):\n    \'\'\' Loads the data from file\n    Inputs:\n        filename (String): absolute path to the datafile\n    Returns:\n        X (list[string]): list of contents of documents\n        y (list[integer]): labels of documents\n        vocab : Vocab object corresponding to words in X\n    \'\'\'\n    \n    print (\'Reading data from {}\'.format(filename))\n    with open(filename, \'r\') as datafile:     \n        data = [line.strip().split(\',\', maxsplit=1) for line in datafile]\n        data_text = list(map(lambda x: x[1], data))\n        data_label = list(map(lambda x: parse_label(x[0]), data))\n    vocab = Vocab()    \n    words = set(\' \'.join(data_text).lower().split())\n    vocab.construct(list(words))\n    return data_text, data_label, vocab\n\ndef get_word_embeddings(w2vfile, word_to_index, embedsize=300):\n    \'\'\'\n    For each word in our vocabulary, get the word2vec encoding of the word\n    Inputs:\n        w2vfile (string) : Path to the file containing (pre-trained) word embeddings\n        embedsize (int) : Length of each word vector\n    Returns:\n        word_embeddings : Dictionary mapping each word to corresponding embedding\n    \'\'\'\n\n    word_embeddings = {}\n    if w2vfile.endswith(\'.txt\'):\n        f = open(w2vfile)\n        for line in tqdm(f):\n            values = line.split("" "")\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype=\'float32\')\n            if word in word_to_index:\n                word_embeddings[word] = coefs\n        f.close()\n    elif w2vfile.endswith(\'.bin\'):\n        word2vec = KeyedVectors.load_word2vec_format(w2vfile, binary=True, limit=1000000)\n        for word in tqdm(word_to_index):\n            try:\n                word_embeddings[word] = word2vec[word.lower()]\n            except KeyError:\n                pass\n    else:\n        print (\'Can\\\'t load word embeddings.\')\n        exit(-1)\n\n    print(\'Found {0}/{1} word vectors.\'.format(len(word_embeddings), len(word_to_index)))\n    if len(word_to_index) > len(word_embeddings):\n        print(\'Initializing remaining {} word vectors with zeros.\'.format(len(word_to_index) - len(word_embeddings)))\n\n    for word in word_to_index:\n        if word not in word_embeddings:\n            word_embeddings[word] = np.zeros((embedsize,))\n    return word_embeddings\n\ndef encode_text(text, word_embeddings, max_sen_len):\n    \'\'\'\n    Encode a sequence of words into corresponding vector representation\n    Input:\n        text (string) : text (space separated words, etc..)\n        word_embeddings (dict) : dictionary mapping from words to their representation\n        max_sen_len (int) : maximum sentence length (in words)\n    Returns:\n        X (np.matrix) : matrix of shape (max_sen_len, embedding_size) after zero padding\n    \'\'\'\n    \n    default_embed = np.zeros(300)\n    words = word_tokenize(text)[:max_sen_len]\n    embeds = [word_embeddings.get(x, default_embed) for x in words]\n    embeds += [default_embed] * (max_sen_len - len(embeds))\n    return np.array(embeds, dtype=np.float32)\n\ndef data_iterator(train_x, train_y, batch_size = 256):\n    \'\'\'\n    Generate batches of training data for training (for single epoch)\n    Inputs:\n        train_df (pd.DataFrame) : complete training data\n        batch_size (int) : Size of each batch\n    Returns:\n        text_arr (np.matrix) : Matrix of shape (batch_size,embed_size)\n        lebel_arr (np.array) : Labels of this batch. Array of shape (batch_size,)\n    \'\'\'\n    n_batches = math.ceil(len(train_x) / batch_size)\n    for idx in range(n_batches):\n        x = train_x[idx *batch_size:(idx+1) * batch_size]\n        y = train_y[idx *batch_size:(idx+1) * batch_size]\n        yield x, y'"
Model_fastText/old_code/config.py,0,b'# config.py\n\nclass Config(object):\n    embed_size = 300\n    hidden_size = 10\n    output_size = 4\n    max_epochs = 20\n    lr = 0.5\n    batch_size = 128'
Model_fastText/old_code/model.py,3,"b'# model.py\n\nimport torch\nfrom torch import nn\nfrom torch import Tensor\nfrom torch.autograd import Variable\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\nclass fastText(nn.Module):\n    def __init__(self, config):\n        super(fastText, self).__init__()\n        self.config = config\n        \n        # Hidden Layer\n        self.fc1 = nn.Linear(self.config.embed_size, self.config.hidden_size)\n        \n        # Output Layer\n        self.fc2 = nn.Linear(self.config.hidden_size, self.config.output_size)\n        \n        # Softmax non-linearity\n        self.softmax = nn.Softmax()\n        \n    def forward(self, x):\n        h = self.fc1(x)\n        z = self.fc2(h)\n        return self.softmax(z)\n    \n    def add_optimizer(self, optimizer):\n        self.optimizer = optimizer\n        \n    def add_loss_op(self, loss_op):\n        self.loss_op = loss_op\n    \n    def run_epoch(self, train_data, val_data):\n        train_x, train_y = train_data[0], train_data[1]\n        val_x, val_y = val_data[0], val_data[1]\n        iterator = data_iterator(train_x, train_y, self.config.batch_size)\n        train_losses = []\n        val_accuracies = []\n        losses = []\n    \n        for i, (x,y) in enumerate(iterator):\n            self.optimizer.zero_grad()\n    \n            x = Tensor(x).cuda()\n            y_pred = self.__call__(x)\n            loss = self.loss_op(y_pred, torch.cuda.LongTensor(y-1))\n            loss.backward()\n    \n            losses.append(loss.data.cpu().numpy())\n            self.optimizer.step()\n    \n            if (i + 1) % 50 == 0:\n                print(""Iter: {}"".format(i+1))\n                avg_train_loss = np.mean(losses)\n                train_losses.append(avg_train_loss)\n                print(""\\tAverage training loss: {:.5f}"".format(avg_train_loss))\n                losses = []\n                \n                # Evalute Accuracy on validation set\n                self.eval()\n                all_preds = []\n                val_iterator = data_iterator(val_x, val_y, self.config.batch_size)\n                for x, y in val_iterator:\n                    x = Variable(Tensor(x))\n                    y_pred = self.__call__(x.cuda())\n                    predicted = torch.max(y_pred.cpu().data, 1)[1] + 1\n                    all_preds.extend(predicted.numpy())\n                score = accuracy_score(val_y, np.array(all_preds).flatten())\n                val_accuracies.append(score)\n                print(""\\tVal Accuracy: {:.4f}"".format(score))\n                self.train()\n                \n        return train_losses, val_accuracies\n'"
Model_fastText/old_code/train.py,3,"b'# train.py\n\nfrom utils import *\nfrom config import Config\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom tqdm import tqdm\nimport sys\nimport torch.optim as optim\nfrom torch import nn, Tensor\nfrom torch.autograd import Variable\nimport torch\nfrom sklearn.metrics import accuracy_score\n\ndef get_accuracy(model, test_x, test_y):\n    all_preds = []\n    test_iterator = data_iterator(test_x, test_y)\n    for x, y in test_iterator:\n        x = Variable(Tensor(x))\n        y_pred = model(x.cuda())\n        predicted = torch.max(y_pred.cpu().data, 1)[1] + 1\n        all_preds.extend(predicted.numpy())\n    score = accuracy_score(test_y, np.array(all_preds).flatten())\n    return score\n\nif __name__==\'__main__\':\n    train_path = \'../data/ag_news.train\'\n    if len(sys.argv) > 2:\n        train_path = sys.argv[1]\n    test_path = \'../data/ag_news.test\'\n    if len(sys.argv) > 3:\n        test_path = sys.argv[2]\n    \n    train_text, train_labels, vocab = get_data(train_path)\n    train_text, val_text, train_label, val_label = train_test_split(train_text, train_labels, test_size=0.2)\n    \n    # Read Word Embeddings\n    w2vfile = \'../data/glove.840B.300d.txt\'\n    word_embeddings = get_word_embeddings(w2vfile, vocab.word_to_index, embedsize=300)\n    \n    train_x = np.array([encode_text(text, word_embeddings) for text in tqdm(train_text)])\n    train_y = np.array(train_label)\n    val_x = np.array([encode_text(text, word_embeddings) for text in tqdm(val_text)])\n    val_y = np.array(val_label)\n    \n    # Create Model with specified optimizer and loss function\n    ##############################################################\n    config = Config()\n    model = fastText(config)\n    model.cuda()\n    model.train()\n    optimizer = optim.SGD(model.parameters(), lr=config.lr)\n    NLLLoss = nn.NLLLoss()\n    model.add_optimizer(optimizer)\n    model.add_loss_op(NLLLoss)\n    ##############################################################\n    \n    train_data = [train_x, train_y]\n    val_data = [val_x, val_y]\n    \n    for i in range(config.max_epochs):\n        print (""Epoch: {}"".format(i))\n        train_losses,val_accuracies = model.run_epoch(train_data, val_data)\n        print(""\\tAverage training loss: {:.5f}"".format(np.mean(train_losses)))\n        print(""\\tAverage Val Accuracy (per 50 iterations): {:.4f}"".format(np.mean(val_accuracies)))\n        \n        # Reduce learning rate as number of epochs increase\n        if i > 0.5 * config.max_epochs:\n            print(""Reducing LR"")\n            for g in optimizer.param_groups:\n                g[\'lr\'] = 0.25\n        if i > 0.75 * config.max_epochs:\n            print(""Reducing LR"")\n            for g in optimizer.param_groups:\n                g[\'lr\'] = 0.15\n                \n    # Get Accuracy of final model\n    test_text, test_labels, test_vocab = get_data(test_path)\n    test_x = np.array([encode_text(text, word_embeddings) for text in tqdm(test_text)])\n    test_y = np.array(test_labels)\n    \n    train_acc = get_accuracy(model, train_x, train_y)\n    val_acc = get_accuracy(model, val_x, val_y)\n    test_acc = get_accuracy(model, test_x, test_y)\n    \n    print (\'Final Training Accuracy: {:.4f}\'.format(train_acc))\n    print (\'Final Validation Accuracy: {:.4f}\'.format(val_acc))\n    print (\'Final Test Accuracy: {:.4f}\'.format(test_acc))\n'"
Model_fastText/old_code/utils.py,0,"b'# utils.py\n\nfrom nltk import word_tokenize\nfrom tqdm import tqdm\nfrom gensim.models import KeyedVectors\nimport numpy as np\nimport math\n\nclass Vocab(object):\n    def __init__(self):\n        self.word_to_index = {}\n        self.index_to_word = {}\n        self.unknown = \'<unk>\'\n        self.add_word(self.unknown)\n        \n    def add_word(self, word):\n        \'\'\' Add a word to the vocabulary\n        Inputs:\n            word (string) : Word to be added to vocabulary\n        \'\'\'\n        \n        if word not in self.word_to_index:\n            index = len(self.word_to_index)\n            self.word_to_index[word] = index\n            self.index_to_word[index] = word\n    \n    def construct(self, words):\n        \'\'\' Construct the vocabulary\n        Inputs:\n            words (list[string]) : List of words defining the vocabulary\n        \'\'\'\n        \n        for word in words:\n            self.add_word(word)\n        print (""Constructed vocabulary of size: {}"".format(len(self.word_to_index)))\n        \n    def encode(self, word):\n        \'\'\'\n        Given a word, get corresponding index in vocabulary.\n        \'\'\'\n        if word not in self.word_to_index:\n            word = self.unknown\n        return self.word_to_index(word)\n    \n    def decode(self, index):\n        \'\'\'\n        Given a word index, get corresponding word from vocabulary.\n        \'\'\'\n        return self.index_to_word(index)\n    \n    def __len__(self):\n        return len(self.word_to_index)\n\ndef parse_label(label):\n    \'\'\'\n    Get the actual labels from label string\n    Input:\n        label (string) : labels of the form \'__label__2\'\n    Returns:\n        label (int) : integer value corresponding to label string\n    \'\'\'\n    return int(label.strip()[-1])\n\ndef get_data(filename):\n    \'\'\' Loads the data from file\n    Inputs:\n        filename (String): absolute path to the datafile\n    Returns:\n        X (list[string]): list of contents of documents\n        y (list[integer]): labels of documents\n        vocab : Vocab object corresponding to words in X\n    \'\'\'\n    \n    print (\'Reading data from {}\'.format(filename))\n    with open(filename, \'r\') as datafile:     \n        data = [line.strip().split(\',\', maxsplit=1) for line in datafile]\n        data_text = list(map(lambda x: x[1], data))\n        data_label = list(map(lambda x: parse_label(x[0]), data))\n    vocab = Vocab()    \n    words = set(\' \'.join(data_text).lower().split())\n    vocab.construct(list(words))\n    return data_text, data_label, vocab\n\ndef get_word_embeddings(w2vfile, word_to_index, embedsize=300):\n    \'\'\'\n    For each word in our vocabulary, get the word2vec encoding of the word\n    Inputs:\n        w2vfile (string) : Path to the file containing (pre-trained) word embeddings\n        embedsize (int) : Length of each word vector\n    Returns:\n        word_embeddings : Dictionary mapping each word to corresponding embedding\n    \'\'\'\n\n    word_embeddings = {}\n    if w2vfile.endswith(\'.txt\'):\n        f = open(w2vfile)\n        for line in tqdm(f):\n            values = line.split("" "")\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype=\'float32\')\n            if word in word_to_index:\n                word_embeddings[word] = coefs\n        f.close()\n    elif w2vfile.endswith(\'.bin\'):\n        word2vec = KeyedVectors.load_word2vec_format(w2vfile, binary=True, limit=1000000)\n        for word in tqdm(word_to_index):\n            try:\n                word_embeddings[word] = word2vec[word.lower()]\n            except KeyError:\n                pass\n    else:\n        print (\'Can\\\'t load word embeddings.\')\n        exit(-1)\n\n    print(\'Found {0}/{1} word vectors.\'.format(len(word_embeddings), len(word_to_index)))\n    if len(word_to_index) > len(word_embeddings):\n        print(\'Initializing remaining {} word vectors with zeros.\'.format(len(word_to_index) - len(word_embeddings)))\n\n    for word in word_to_index:\n        if word not in word_embeddings:\n            word_embeddings[word] = np.zeros((embedsize,))\n    return word_embeddings\n\ndef encode_text(text, word_embeddings):\n    \'\'\'\n    Encode a sequence of words into corresponding vector representation\n    Input:\n        text (string) : text (space separated words, etc..)\n        word_embeddings (dict) : dictionary mapping from words to their representation\n        max_sent_len (int) : maximum sentence length (in words)\n    Returns:\n        X (np.array) : array of shape (embedding_size,) averaging all word vectors of text\n    \'\'\'\n    \n    embed = np.zeros(300)\n    count = 0\n    words = word_tokenize(text)\n    for word in words:\n        if word in word_embeddings:\n            embed += word_embeddings[word]\n            count += 1\n    return embed / count\n\ndef data_iterator(train_x, train_y, batch_size = 256):\n    \'\'\'\n    Generate batches of training data for training (for single epoch)\n    Inputs:\n        train_df (pd.DataFrame) : complete training data\n        batch_size (int) : Size of each batch\n    Returns:\n        text_arr (np.matrix) : Matrix of shape (batch_size,embed_size)\n        lebel_arr (np.array) : Labels of this batch. Array of shape (batch_size,)\n    \'\'\'\n    n_batches = math.ceil(len(train_x) / batch_size)\n    for idx in range(n_batches):\n        x = train_x[idx *batch_size:(idx+1) * batch_size]\n        y = train_y[idx *batch_size:(idx+1) * batch_size]\n        yield x, y'"
data/query_wellformedness/reformat_data.py,0,"b'import sys\nimport os\n\nif __name__==\'__main__\':\n\tif len(sys.argv) < 2:\n\t\tprint(""Expected filename as an argument"")\n\t\tsys.exit()\n\tfilepath = sys.argv[1]\n\tpath, filename = os.path.split(filepath)\n\tname, ext = os.path.splitext(os.path.basename(filename))\n\tnew_filepath = os.path.join(path, \'processed_\'+name+\'.txt\')\n\twith open(new_filepath, \'w\') as new_file:\n\t\twith open(filepath, \'r\') as old_file:\n\t\t\tfor line in old_file:\n\t\t\t\tquestion, number = line.strip().split(\'\\t\')\n\t\t\t\ty = \'2\' if float(number) >= 0.5 else \'1\'\n\t\t\t\tlabel = \'__label__\'+y\n\t\t\t\tnew_line = label + \' , \' + question + \'\\n\'\n\t\t\t\tnew_file.write(new_line)\n\tprint(\'Finished\')\n'"
