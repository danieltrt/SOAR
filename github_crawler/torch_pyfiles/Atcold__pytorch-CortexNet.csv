file_path,api_count,code
main.py,16,"b'import argparse\nimport os\nimport os.path as path\nimport time\nfrom datetime import timedelta\nfrom sys import exit, argv\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable as V\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms as trn\n\nfrom data.VideoFolder import VideoFolder, BatchSampler, VideoCollate\nfrom utils.image_plot import show_four, show_ten\n\nparser = argparse.ArgumentParser(description=\'PyTorch MatchNet generative model training script\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n_ = parser.add_argument  # define add_argument shortcut\n_(\'--data\', type=str, default=\'./data/processed-data\', help=\'location of the video data\')\n_(\'--model\', type=str, default=\'CortexNet\', help=\'type of auto-encoder\')\n_(\'--mode\', type=str, required=True, help=\'training mode [MatchNet|TempoNet]\')\n_(\'--size\', type=int, default=(3, 32, 64, 128, 256), nargs=\'*\', help=\'number and size of hidden layers\', metavar=\'S\')\n_(\'--spatial-size\', type=int, default=(256, 256), nargs=2, help=\'frame cropping size\', metavar=(\'H\', \'W\'))\n_(\'--lr\', type=float, default=0.1, help=\'initial learning rate\')\n_(\'--momentum\', type=float, default=0.9, metavar=\'M\', help=\'momentum\')\n_(\'--weight-decay\', type=float, default=1e-4, metavar=\'W\', help=\'weight decay\')\n_(\'--mu\', type=float, default=1, help=\'matching MSE multiplier\', dest=\'mu\', metavar=\'\xce\xbc\')\n_(\'--tau\', type=float, default=0.1, help=\'temporal CE multiplier\', dest=\'tau\', metavar=\'\xcf\x84\')\n_(\'--pi\', default=\'\xcf\x84\', help=\'periodical CE multiplier\', dest=\'pi\', metavar=\'\xcf\x80\')\n_(\'--epochs\', type=int, default=10, help=\'upper epoch limit\')\n_(\'--batch-size\', type=int, default=20, metavar=\'B\', help=\'batch size\')\n_(\'--big-t\', type=int, default=10, help=\'sequence length\', metavar=\'T\')\n_(\'--seed\', type=int, default=0, help=\'random seed\')\n_(\'--log-interval\', type=int, default=10, metavar=\'N\', help=\'report interval\')\n_(\'--save\', type=str, default=\'last/model.pth.tar\', help=\'path to save the final model\')\n_(\'--cuda\', action=\'store_true\', help=\'use CUDA\')\n_(\'--view\', type=int, default=tuple(), help=\'samples to view at the end of every log-interval batches\', metavar=\'V\')\n_(\'--show-x_hat\', action=\'store_true\', help=\'show x_hat\')\n_(\'--lr-decay\', type=float, default=None, nargs=2, metavar=(\'D\', \'E\'),\n  help=\'decay of D (e.g. 3.16, 10) times, every E (e.g. 3) epochs\')\n_(\'--pre-trained\', type=str, default=\'\', help=\'path to pre-trained model\', metavar=\'P\')\nargs = parser.parse_args()\nargs.size = tuple(args.size)  # cast to tuple\nif args.lr_decay: args.lr_decay = tuple(args.lr_decay)\nif type(args.view) is int: args.view = (args.view,)  # cast to tuple\nargs.pi = args.tau if args.pi == \'\xcf\x84\' else float(args.pi)\n\n# Print current options\nprint(\'CLI arguments:\', \' \'.join(argv[1:]))\n\n# Print current commit\nif path.isdir(\'.git\'):  # if we are in a repo\n    with os.popen(\'git rev-parse HEAD\') as pipe:  # get the HEAD\'s hash\n        print(\'Current commit hash:\', pipe.read(), end=\'\')\n\n# Set the random seed manually for reproducibility.\ntorch.manual_seed(args.seed)\nif torch.cuda.is_available():\n    if not args.cuda:\n        print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n    else:\n        torch.cuda.manual_seed(args.seed)\n\n\ndef main():\n    # Load data\n    print(\'Define image pre-processing\')\n    # normalise? do we care?\n    t = trn.Compose((trn.ToPILImage(), trn.CenterCrop(args.spatial_size), trn.ToTensor()))\n\n    print(\'Define train data loader\')\n    train_data_name = \'train_data.tar\'\n    if os.access(train_data_name, os.R_OK):\n        train_data = torch.load(train_data_name)\n    else:\n        train_path = path.join(args.data, \'train\')\n        if args.mode == \'MatchNet\':\n            train_data = VideoFolder(root=train_path, transform=t, video_index=True)\n        elif args.mode == \'TempoNet\':\n            train_data = VideoFolder(root=train_path, transform=t, shuffle=True)\n        torch.save(train_data, train_data_name)\n\n    train_loader = DataLoader(\n        dataset=train_data,\n        batch_size=args.batch_size * args.big_t,  # batch_size rows and T columns\n        shuffle=False,\n        sampler=BatchSampler(data_source=train_data, batch_size=args.batch_size),  # given that BatchSampler knows it\n        num_workers=1,\n        collate_fn=VideoCollate(batch_size=args.batch_size),\n        pin_memory=True\n    )\n\n    print(\'Define validation data loader\')\n    val_data_name = \'val_data.tar\'\n    if os.access(val_data_name, os.R_OK):\n        val_data = torch.load(val_data_name)\n    else:\n        val_path = path.join(args.data, \'val\')\n        if args.mode == \'MatchNet\':\n            val_data = VideoFolder(root=val_path, transform=t, video_index=True)\n        elif args.mode == \'TempoNet\':\n            val_data = VideoFolder(root=val_path, transform=t, shuffle=\'init\')\n        torch.save(val_data, val_data_name)\n\n    val_loader = DataLoader(\n        dataset=val_data,\n        batch_size=args.batch_size,  # just one column of size batch_size\n        shuffle=False,\n        sampler=BatchSampler(data_source=val_data, batch_size=args.batch_size),\n        num_workers=1,\n        collate_fn=VideoCollate(batch_size=args.batch_size),\n        pin_memory=True\n    )\n\n    # Build the model\n    if args.model == \'model_01\':\n        from model.Model01 import Model01 as Model\n    elif args.model == \'model_02\' or args.model == \'CortexNet\':\n        from model.Model02 import Model02 as Model\n    elif args.model == \'model_02_rg\':\n        from model.Model02 import Model02RG as Model\n    else:\n        print(\'\\n{:#^80}\\n\'.format(\' Please select a valid model \'))\n        exit()\n\n    print(\'Define model\')\n    if args.mode == \'MatchNet\':\n        nb_train_videos = len(train_data.videos)\n        model = Model(args.size + (nb_train_videos,), args.spatial_size)\n    elif args.mode == \'TempoNet\':\n        nb_classes = len(train_data.classes)\n        model = Model(args.size + (nb_classes,), args.spatial_size)\n\n    if args.pre_trained:\n        print(\'Load pre-trained weights\')\n        # args.pre_trained = \'image-pretraining/model02D-33IS/model_best.pth.tar\'\n        dict_33 = torch.load(args.pre_trained)[\'state_dict\']\n\n        def load_state_dict(new_model, state_dict):\n            own_state = new_model.state_dict()\n            for name, param in state_dict.items():\n                name = name[19:]  # remove \'module.inner_model.\' part\n                if name not in own_state:\n                    raise KeyError(\'unexpected key ""{}"" in state_dict\'\n                                   .format(name))\n                if name.startswith(\'stabiliser\'):\n                    print(\'Skipping\', name)\n                    continue\n                if isinstance(param, nn.Parameter):\n                    # backwards compatibility for serialized parameters\n                    param = param.data\n                own_state[name].copy_(param)\n\n            missing = set(own_state.keys()) - set([k[19:] for k in state_dict.keys()])\n            if len(missing) > 0:\n                raise KeyError(\'missing keys in state_dict: ""{}""\'.format(missing))\n\n        load_state_dict(model, dict_33)\n\n    print(\'Create a MSE and balanced NLL criterions\')\n    mse = nn.MSELoss()\n\n    # independent CE computation\n    nll_final = nn.CrossEntropyLoss(size_average=False)\n    # balance classes based on frames per video; default balancing weight is 1.0f\n    w = torch.Tensor(train_data.frames_per_video if args.mode == \'MatchNet\' else train_data.frames_per_class)\n    w.div_(w.mean()).pow_(-1)\n    nll_train = nn.CrossEntropyLoss(w)\n    w = torch.Tensor(val_data.frames_per_video if args.mode == \'MatchNet\' else val_data.frames_per_class)\n    w.div_(w.mean()).pow_(-1)\n    nll_val = nn.CrossEntropyLoss(w)\n\n    if args.cuda:\n        model.cuda()\n        mse.cuda()\n        nll_final.cuda()\n        nll_train.cuda()\n        nll_val.cuda()\n\n    print(\'Instantiate a SGD optimiser\')\n    optimiser = optim.SGD(\n        params=model.parameters(),\n        lr=args.lr,\n        momentum=args.momentum,\n        weight_decay=args.weight_decay\n    )\n\n    # Loop over epochs\n    for epoch in range(0, args.epochs):\n        if args.lr_decay: adjust_learning_rate(optimiser, epoch)\n        epoch_start_time = time.time()\n        train(train_loader, model, (mse, nll_final, nll_train), optimiser, epoch)\n        print(80 * \'-\', \'| end of epoch {:3d} |\'.format(epoch + 1), sep=\'\\n\', end=\' \')\n        val_loss = validate(val_loader, model, (mse, nll_final, nll_val))\n        elapsed_time = str(timedelta(seconds=int(time.time() - epoch_start_time)))  # HH:MM:SS time format\n        print(\'time: {} | mMSE {:.2e} | CE {:.2e} | rpl mMSE {:.2e} | per CE {:.2e} |\'.\n              format(elapsed_time, val_loss[\'mse\'] * 1e3, val_loss[\'ce\'], val_loss[\'rpl\'] * 1e3, val_loss[\'per_ce\']))\n        print(80 * \'-\')\n\n        if args.save != \'\':\n            torch.save(model, args.save)\n\n\ndef adjust_learning_rate(opt, epoch):\n    """"""Sets the learning rate to the initial LR decayed by D every E epochs""""""\n    d, e = args.lr_decay\n    lr = args.lr * (d ** -(epoch // e))\n    for param_group in opt.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef selective_zero(s, new, forward=True):\n    if new.any():  # if at least one video changed\n        b = new.nonzero().squeeze(1)  # get the list of indices\n        if forward:  # no state forward, no grad backward\n            if isinstance(s[0], list):  # recurrent G\n                for layer in range(len(s[0])):  # for every layer having a state\n                    s[0][layer] = s[0][layer].index_fill(0, V(b), 0)  # mask state, zero selected indices\n                for layer in range(len(s[1])):  # for every layer having a state\n                    s[1][layer] = s[1][layer].index_fill(0, V(b), 0)  # mask state, zero selected indices\n            else:  # simple convolutional G\n                for layer in range(len(s)):  # for every layer having a state\n                    s[layer] = s[layer].index_fill(0, V(b), 0)  # mask state, zero selected indices\n        else:  # just no grad backward\n            if isinstance(s[0], list):  # recurrent G\n                for layer in range(len(s[0])):  # for every layer having a state\n                    s[0][layer].register_hook(lambda g: g.index_fill(0, V(b), 0))  # zero selected gradients\n                for layer in range(len(s[1])):  # for every layer having a state\n                    s[1][layer].register_hook(lambda g: g.index_fill(0, V(b), 0))  # zero selected gradients\n            else:  # simple convolutional G\n                for layer in range(len(s)):  # for every layer having a state\n                    s[layer].register_hook(lambda g: g.index_fill(0, V(b), 0))  # zero selected gradients\n\n\ndef selective_match(x_hat, x, new):\n    if new.any():  # if at least one video changed\n        b = new.nonzero().squeeze(1)  # get the list of indices\n        for bb in b: x_hat[bb].copy_(x[bb])  # force the output to be the expected output\n\n\ndef selective_cross_entropy(logits, y, new, loss, count):\n    if not new.any():\n        return V(logits.data.new(1).zero_())  # returns a variable, so we don\'t care about what happened here\n    b = new.nonzero().squeeze(1)  # get the list of indices\n    count[\'ce_count\'] += len(b)\n    return loss(logits.index_select(0, V(b)), y.index_select(0, V(b)))  # performs loss for selected indices only\n\n\ndef train(train_loader, model, loss_fun, optimiser, epoch):\n    print(\'Training epoch\', epoch + 1)\n    model.train()  # set model in train mode\n    total_loss = {\'mse\': 0, \'ce\': 0, \'ce_count\': 0, \'per_ce\': 0, \'rpl\': 0}\n    mse, nll_final, nll_periodic = loss_fun\n\n    def compute_loss(x_, next_x, y_, state_, periodic=False):\n        nonlocal previous_mismatch  # write access to variables of the enclosing function\n        if args.mode == \'MatchNet\':\n            if not periodic and state_: selective_zero(state_, mismatch, forward=False)  # no grad to the past\n        (x_hat, state_), (_, idx) = model(V(x_), state_)\n        selective_zero(state_, mismatch)  # no state to the future, no grad from the future\n        selective_match(x_hat.data, next_x, mismatch + previous_mismatch)  # last frame or first frame\n        previous_mismatch = mismatch  # last frame <- first frame\n        mse_loss_ = mse(x_hat, V(next_x))\n        total_loss[\'mse\'] += mse_loss_.data[0]\n        ce_loss_ = selective_cross_entropy(idx, V(y_), mismatch, nll_final, total_loss)\n        total_loss[\'ce\'] += ce_loss_.data[0]\n        if periodic:\n            ce_loss_ = (ce_loss_, nll_periodic(idx, V(y_)))\n            total_loss[\'per_ce\'] += ce_loss_[1].data[0]\n        total_loss[\'rpl\'] += mse(x_hat, V(x_, volatile=True)).data[0]\n        return ce_loss_, mse_loss_, state_, x_hat.data\n\n    data_time = 0\n    batch_time = 0\n    end_time = time.time()\n    state = None  # reset state at the beginning of a new epoch\n    from_past = None  # performs only T - 1 steps for the first temporal batch\n    previous_mismatch = torch.ByteTensor(args.batch_size).fill_(1)  # ignore first prediction\n    if args.cuda: previous_mismatch = previous_mismatch.cuda()\n    for batch_nb, (x, y) in enumerate(train_loader):\n        data_time += time.time() - end_time\n        if args.cuda:\n            x = x.cuda(async=True)\n            y = y.cuda(async=True)\n        state = repackage_state(state)\n        loss = 0\n        # BTT loop\n        if args.mode == \'MatchNet\':\n            if from_past:\n                mismatch = y[0] != from_past[1]\n                ce_loss, mse_loss, state, _ = compute_loss(from_past[0], x[0], from_past[1], state, periodic=True)\n                loss += mse_loss * args.mu + ce_loss[0] * args.tau + ce_loss[1] * args.pi\n            for t in range(0, min(args.big_t, x.size(0)) - 1):  # first batch we go only T - 1 steps forward / backward\n                mismatch = y[t + 1] != y[t]\n                ce_loss, mse_loss, state, x_hat_data = compute_loss(x[t], x[t + 1], y[t], state)\n                loss += mse_loss * args.mu + ce_loss * args.tau\n        elif args.mode == \'TempoNet\':\n            if from_past:\n                mismatch = y[0] != from_past[1]\n                ce_loss, mse_loss, state, _ = compute_loss(from_past[0], x[0], from_past[1], state)\n                loss += mse_loss * args.mu + ce_loss * args.tau\n            for t in range(0, min(args.big_t, x.size(0)) - 1):  # first batch we go only T - 1 steps forward / backward\n                mismatch = y[t + 1] != y[t]\n                last = t == min(args.big_t, x.size(0)) - 2\n                ce_loss, mse_loss, state, x_hat_data = compute_loss(x[t], x[t + 1], y[t], state, periodic=last)\n                if not last:\n                    loss += mse_loss * args.mu + ce_loss * args.tau\n                else:\n                    loss += mse_loss * args.mu + ce_loss[0] * args.tau + ce_loss[1] * args.pi\n\n        # compute gradient and do SGD step\n        model.zero_grad()\n        loss.backward()\n        optimiser.step()\n\n        # save last column for future\n        from_past = x[-1], y[-1]\n\n        # measure batch time\n        batch_time += time.time() - end_time\n        end_time = time.time()  # for computing data_time\n\n        if (batch_nb + 1) % args.log_interval == 0:\n            if args.view:\n                for f in args.view:\n                    show_four(x[t][f], x[t + 1][f], x_hat_data[f], f + 1)\n                    if args.show_x_hat: show_ten(x[t][f], x_hat_data[f])\n            total_loss[\'mse\'] /= args.log_interval * args.big_t\n            total_loss[\'rpl\'] /= args.log_interval * args.big_t\n            total_loss[\'per_ce\'] /= args.log_interval\n            if total_loss[\'ce_count\']: total_loss[\'ce\'] /= total_loss[\'ce_count\']\n            avg_batch_time = batch_time * 1e3 / args.log_interval\n            avg_data_time = data_time * 1e3 / args.log_interval\n            lr = optimiser.param_groups[0][\'lr\']  # assumes len(param_groups) == 1\n            print(\'| epoch {:3d} | {:4d}/{:4d} batches | lr {:.3f} |\'\n                  \' ms/batch {:7.2f} | ms/data {:7.2f} | mMSE {:.2e} | CE {:.2e} | rpl mMSE {:.2e} | per CE {:.2e} |\'.\n                  format(epoch + 1, batch_nb + 1, len(train_loader), lr, avg_batch_time, avg_data_time,\n                         total_loss[\'mse\'] * 1e3, total_loss[\'ce\'], total_loss[\'rpl\'] * 1e3, total_loss[\'per_ce\']))\n            for k in total_loss: total_loss[k] = 0  # zero the losses\n            batch_time = 0\n            data_time = 0\n\n\ndef validate(val_loader, model, loss_fun):\n    model.eval()  # set model in evaluation mode\n    total_loss = {\'mse\': 0, \'ce\': 0, \'ce_count\': 0, \'per_ce\': 0, \'rpl\': 0}\n    mse, nll_final, nll_periodic = loss_fun\n    batches = enumerate(val_loader)\n\n    _, (x, y) = next(batches)\n    if args.cuda:\n        x = x.cuda(async=True)\n        y = y.cuda(async=True)\n    previous_mismatch = y[0].byte().fill_(1)  # ignore first prediction\n    state = None  # reset state at the beginning of a new epoch\n    for batch_nb, (next_x, next_y) in batches:\n        if args.cuda:\n            next_x = next_x.cuda(async=True)\n            next_y = next_y.cuda(async=True)\n        mismatch = next_y[0] != y[0]\n        (x_hat, state), (_, idx) = model(V(x[0], volatile=True), state)  # do not compute graph (volatile)\n        selective_zero(state, mismatch)  # no state to the future\n        selective_match(x_hat.data, next_x[0], mismatch + previous_mismatch)  # last frame or first frame\n        previous_mismatch = mismatch  # last frame <- first frame\n        total_loss[\'mse\'] += mse(x_hat, V(next_x[0])).data[0]\n        ce_loss = selective_cross_entropy(idx, V(y[0]), mismatch, nll_final, total_loss)\n        total_loss[\'ce\'] += ce_loss.data[0]\n        if batch_nb % args.big_t == 0: total_loss[\'per_ce\'] += nll_periodic(idx, V(y[0])).data[0]\n        total_loss[\'rpl\'] += mse(x_hat, V(x[0])).data[0]\n        x, y = next_x, next_y\n\n    total_loss[\'mse\'] /= len(val_loader)  # average out\n    total_loss[\'rpl\'] /= len(val_loader)  # average out\n    total_loss[\'per_ce\'] /= len(val_loader) / args.big_t  # average out\n    total_loss[\'ce\'] /= total_loss[\'ce_count\']  # average out\n    return total_loss\n\n\ndef repackage_state(h):\n    """"""\n    Wraps hidden states in new Variables, to detach them from their history.\n    """"""\n    if not h:\n        return None\n    elif type(h) == V:\n        return V(h.data)\n    else:\n        return list(repackage_state(v) for v in h)\n\n\nif __name__ == \'__main__\':\n    main()\n\n__author__ = ""Alfredo Canziani""\n__credits__ = [""Alfredo Canziani""]\n__maintainer__ = ""Alfredo Canziani""\n__email__ = ""alfredo.canziani@gmail.com""\n__status__ = ""Production""  # ""Prototype"", ""Development"", or ""Production""\n__date__ = ""Feb 17""\n'"
data/VideoFolder.py,10,"b'import collections\nimport torch\nimport torch.utils.data as data\n\nfrom random import shuffle as list_shuffle  # for shuffling list\nfrom math import ceil\nfrom os import listdir\nfrom os.path import isdir, join, isfile\nfrom itertools import islice\nfrom numpy.core.multiarray import concatenate, ndarray\nfrom skvideo.io import FFmpegReader, ffprobe\nfrom torch.utils.data.sampler import Sampler\nfrom torchvision import transforms as trn\nfrom tqdm import tqdm\nfrom time import sleep\nfrom bisect import bisect\n\n# Implement object from https://discuss.pytorch.org/t/loading-videos-from-folders-as-a-dataset-object/568\n\nVIDEO_EXTENSIONS = [\'.mp4\']  # pre-processing outputs MP4s only\n\n\nclass BatchSampler(Sampler):\n    def __init__(self, data_source, batch_size):\n        """"""\n        Samples batches sequentially, always in the same order.\n\n        :param data_source: data set to sample from\n        :type data_source: Dataset\n        :param batch_size: concurrent number of video streams\n        :type batch_size: int\n        """"""\n        self.batch_size = batch_size\n        self.samples_per_row = ceil(len(data_source) / batch_size)\n        self.num_samples = self.samples_per_row * batch_size\n\n    def __iter__(self):\n        return (self.samples_per_row * i + j for j in range(self.samples_per_row) for i in range(self.batch_size))\n\n    def __len__(self):\n        return self.num_samples  # fake nb of samples, transparent wrapping around\n\n\nclass VideoCollate:\n    def __init__(self, batch_size):\n        self.batch_size = batch_size\n\n    def __call__(self, batch: iter) -> torch.Tensor or list(torch.Tensor):\n        """"""\n        Puts each data field into a tensor with outer dimension batch size\n\n        :param batch: samples from a Dataset object\n        :type batch: list\n        :return: temporal batch of frames of size (t, batch_size, *frame.size()), 0 <= t < T, most likely t = T - 1\n        :rtype: tuple\n        """"""\n        if torch.is_tensor(batch[0]):\n            return torch.cat(tuple(t.unsqueeze(0) for t in batch), 0).view(-1, self.batch_size, *batch[0].size())\n        elif isinstance(batch[0], int):\n            return torch.LongTensor(batch).view(-1, self.batch_size)\n        elif isinstance(batch[0], collections.Iterable):\n            # if each batch element is not a tensor, then it should be a tuple\n            # of tensors; in that case we collate each element in the tuple\n            transposed = zip(*batch)\n            return tuple(self.__call__(samples) for samples in transposed)\n\n        raise TypeError((""batch must contain tensors, numbers, or lists; found {}""\n                         .format(type(batch[0]))))\n\n\nclass VideoFolder(data.Dataset):\n    def __init__(self, root, transform=None, target_transform=None, video_index=False, shuffle=None):\n        """"""\n        Initialise a ``data.Dataset`` object for concurrent frame fetching from videos in a directory of folders of videos\n\n        :param root: Data directory (train or validation folders path)\n        :type root: str\n        :param transform: image transform-ing object from ``torchvision.transforms``\n        :type transform: object\n        :param target_transform: label transformation / mapping\n        :type target_transform: object\n        :param video_index: if ``True``, the label will be the video index instead of target class\n        :type video_index: bool\n        :param shuffle: ``None``, ``\'init\'`` or ``True``\n        :type shuffle: str\n        """"""\n        classes, class_to_idx = self._find_classes(root)\n        video_paths = self._find_videos(root, classes)\n        videos, frames, frames_per_video, frames_per_class = self._make_data_set(\n            root, video_paths, class_to_idx, shuffle, video_index\n        )\n\n        self.root = root\n        self.video_paths = video_paths\n        self.videos = videos\n        self.opened_videos = [[] for _ in videos]\n        self.frames = frames\n        self.frames_per_video = frames_per_video\n        self.frames_per_class = frames_per_class\n        self.classes = classes\n        self.class_to_idx = class_to_idx\n        self.transform = transform\n        self.target_transform = target_transform\n        self.alternative_target = video_index\n        self.shuffle = shuffle\n\n    def __getitem__(self, frame_idx):\n        if frame_idx == 0:\n            self.free()\n            if self.shuffle is True:\n                self._shuffle()\n\n        frame_idx %= self.frames  # wrap around indexing, if asking too much\n        video_idx = bisect(self.videos, ((frame_idx,),))  # video to which frame_idx belongs\n        (last, first), (path, target) = self.videos[video_idx]  # get video metadata\n        frame = self._get_frame(frame_idx - first, video_idx, frame_idx == last)  # get frame from video\n        if self.transform is not None:  # image processing\n            frame = self.transform(frame)\n        if self.target_transform is not None:  # target processing\n            target = self.target_transform(target)\n\n        if self.alternative_target: return frame, video_idx\n\n        return frame, target\n\n    def __len__(self):\n        return self.frames\n\n    def _get_frame(self, seek, video_idx, last):\n\n        opened_video = None  # handle to opened target video\n        if self.opened_videos[video_idx]:  # if handle(s) exists for target video\n            current = self.opened_videos[video_idx]  # get handles list\n            opened_video = next((ov for ov in current if ov[0] == seek), None)  # look for matching seek\n\n        if opened_video is None:  # no (matching) handle found\n            video_path = join(self.root, self.videos[video_idx][1][0])  # build video path\n            video_file = FFmpegReader(video_path)  # get a video file pointer\n            video_iter = video_file.nextFrame()  # get an iterator\n            opened_video = [seek, islice(video_iter, seek, None), video_file]  # seek video and create o.v. item\n            self.opened_videos[video_idx].append(opened_video)  # add opened video object to o.v. list\n\n        opened_video[0] = seek + 1  # update seek pointer\n        frame = next(opened_video[1])  # cache output frame\n        if last:\n            opened_video[2]._close()  # close video file (private method?!)\n            self.opened_videos[video_idx].remove(opened_video)  # remove o.v. item\n\n        return frame\n\n    def free(self):\n        """"""\n        Frees all video files\' pointers\n        """"""\n        for video in self.opened_videos:  # for every opened video\n            for _ in range(len(video)):  # for as many times as pointers\n                opened_video = video.pop()  # pop an item\n                opened_video[2]._close()  # close the file\n\n    def _shuffle(self):\n        """"""\n        Shuffles the video list\n        by regenerating the sequence to sample sequentially\n        """"""\n        def _is_video_file(filename_):\n            return any(filename_.endswith(extension) for extension in VIDEO_EXTENSIONS)\n\n        root = self.root\n        video_paths = self.video_paths\n        class_to_idx = self.class_to_idx\n        list_shuffle(video_paths)  # shuffle\n\n        videos = list()\n        frames_per_video = list()\n        frames_counter = 0\n        for filename in tqdm(video_paths, ncols=80):\n            class_ = filename.split(\'/\')[0]\n            data_path = join(root, filename)\n            if _is_video_file(data_path):\n                video_meta = ffprobe(data_path)\n                start_idx = frames_counter\n                frames = int(video_meta[\'video\'].get(\'@nb_frames\'))\n                frames_per_video.append(frames)\n                frames_counter += frames\n                item = ((frames_counter - 1, start_idx), (filename, class_to_idx[class_]))\n                videos.append(item)\n\n        sleep(0.5)  # allows for progress bar completion\n        # update the attributes with the altered sequence\n        self.video_paths = video_paths\n        self.videos = videos\n        self.frames = frames_counter\n        self.frames_per_video = frames_per_video\n\n    @staticmethod\n    def _find_classes(data_path):\n        classes = [d for d in listdir(data_path) if isdir(join(data_path, d))]\n        classes.sort()\n        class_to_idx = {classes[i]: i for i in range(len(classes))}\n        return classes, class_to_idx\n\n    @staticmethod\n    def _find_videos(root, classes):\n        return [join(c, d) for c in classes for d in listdir(join(root, c))]\n\n    @staticmethod\n    def _make_data_set(root, video_paths, class_to_idx, init_shuffle, video_index):\n        def _is_video_file(filename_):\n            return any(filename_.endswith(extension) for extension in VIDEO_EXTENSIONS)\n\n        if init_shuffle and not video_index:\n            list_shuffle(video_paths)  # shuffle\n\n        videos = list()\n        frames_per_video = list()\n        frames_per_class = [0] * len(class_to_idx)\n        frames_counter = 0\n        for filename in tqdm(video_paths, ncols=80):\n            class_ = filename.split(\'/\')[0]\n            data_path = join(root, filename)\n            if _is_video_file(data_path):\n                video_meta = ffprobe(data_path)\n                start_idx = frames_counter\n                frames = int(video_meta[\'video\'].get(\'@nb_frames\'))\n                frames_per_video.append(frames)\n                frames_per_class[class_to_idx[class_]] += frames\n                frames_counter += frames\n                item = ((frames_counter - 1, start_idx), (filename, class_to_idx[class_]))\n                videos.append(item)\n\n        sleep(0.5)  # allows for progress bar completion\n        return videos, frames_counter, frames_per_video, frames_per_class\n\n\ndef _test_video_folder():\n    from textwrap import fill, indent\n\n    batch_size = 5\n\n    video_data_set = VideoFolder(\'small_data_set/\')\n    nb_of_classes = len(video_data_set.classes)\n    print(\'There are\', nb_of_classes, \'classes\')\n    print(indent(fill(\' \'.join(video_data_set.classes), 77), \'   \'))\n    print(\'There are {} frames\'.format(len(video_data_set)))\n    print(\'Videos in the data set:\', *video_data_set.videos, sep=\'\\n\')\n\n    import inflect\n    ordinal = inflect.engine().ordinal\n\n    def print_list(my_list):\n        for a, b in enumerate(my_list):\n            print(a, \':\', end=\' [\')\n            print(*b, sep=\',\\n     \', end=\']\\n\')\n\n    # get first 3 batches\n    n = ceil(len(video_data_set) / batch_size)\n    print(\'Batch size:\', batch_size)\n    print(\'Frames per row:\', n)\n    for big_j in range(0, n, 90):\n        batch = list()\n        for j in range(big_j, big_j + 90):\n            if j >= n: break  # there are no more frames\n            batch.append(tuple(video_data_set[i * n + j][0] for i in range(batch_size)))\n            batch[-1] = concatenate(batch[-1], 0)\n        batch = concatenate(batch, 1)\n        _show_numpy(batch, 1e-1)\n        print(ordinal(big_j // 90 + 1), \'90 batches of shape\', batch.shape)\n        print_list(video_data_set.opened_videos)\n\n    print(\'Freeing resources\')\n    video_data_set.free()\n    print_list(video_data_set.opened_videos)\n\n    # get frames 50 -> 52\n    batch = list()\n    for i in range(50, 53):\n        batch.append(video_data_set[i][0])\n    _show_numpy(concatenate(batch, 1))\n    print_list(video_data_set.opened_videos)\n\n\ndef _test_data_loader():\n    big_t = 10\n    batch_size = 5\n    t = trn.Compose((trn.ToPILImage(), trn.ToTensor()))  # <-- add trn.CenterCrop(224) in between for training\n    data_set = VideoFolder(\'small_data_set\', t)\n    my_loader = data.DataLoader(dataset=data_set, batch_size=batch_size * big_t, shuffle=False,\n                                sampler=BatchSampler(data_set, batch_size), num_workers=0,\n                                collate_fn=VideoCollate(batch_size))\n    print(\'Is my_loader an iterator [has __next__()]:\', isinstance(my_loader, collections.Iterator))\n    print(\'Is my_loader an iterable [has __iter__()]:\', isinstance(my_loader, collections.Iterable))\n    my_iter = iter(my_loader)\n    my_batch = next(my_iter)\n    print(\'my_batch is a\', type(my_batch), \'of length\', len(my_batch))\n    print(\'my_batch[0] is a\', my_batch[0].type(), \'of size\', tuple(my_batch[0].size()), \'  # will 224, 224\')\n    _show_torch(_tile_up(my_batch), .2)\n    for i in range(3): _show_torch(_tile_up(next(my_iter)), .2)\n\n\ndef _show_numpy(tensor: ndarray, zoom: float = 1.) -> None:\n    """"""\n    Display a ndarray image on screen\n\n    :param tensor: image to visualise, of size (h, w, 1/3)\n    :type tensor: ndarray\n    :param zoom: zoom factor\n    :type zoom: float\n    """"""\n    from PIL import Image\n    shape = tuple(map(lambda s: round(s * zoom), tensor.shape))\n    Image.fromarray(tensor).resize((shape[1], shape[0])).show()\n\n\ndef _show_torch(tensor: torch.FloatTensor, zoom: float = 1.) -> None:\n    numpy_tensor = tensor.clone().mul(255).int().numpy().astype(\'u1\').transpose(1, 2, 0)\n    _show_numpy(numpy_tensor, zoom)\n\n\ndef _tile_up(temporal_batch):\n    a = torch.cat(tuple(temporal_batch[0][:, i] for i in range(temporal_batch[0].size(1))), 2)\n    a = torch.cat(tuple(a[j] for j in range(a.size(0))), 2)\n    return a\n\n\nif __name__ == \'__main__\':\n    _test_video_folder()\n    _test_data_loader()\n\n\n__author__ = ""Alfredo Canziani""\n__credits__ = [""Alfredo Canziani""]\n__maintainer__ = ""Alfredo Canziani""\n__email__ = ""alfredo.canziani@gmail.com""\n__status__ = ""Production""  # ""Prototype"", ""Development"", or ""Production""\n__date__ = ""Feb 17""\n'"
image-pretraining/main.py,16,"b'import argparse\nimport os\nimport shutil\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\n\n\nmodel_names = sorted(name for name in models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(models.__dict__[name]))\n\n\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\nparser.add_argument(\'data\', metavar=\'DIR\',\n                    help=\'path to dataset\')\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\nparser.add_argument(\'--epochs\', default=90, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=256, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 256)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.1, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--print-freq\', \'-p\', default=10, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 10)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                    help=\'evaluate model on validation set\')\nparser.add_argument(\'--pretrained\', dest=\'pretrained\', action=\'store_true\',\n                    help=\'use pre-trained model\')\nparser.add_argument(\'--size\', type=int, default=(3, 32, 64, 128, 256, 256, 256), nargs=\'*\',\n                    help=\'number and size of hidden layers\', metavar=\'S\')\n\nbest_prec1 = 0\n\n\ndef main():\n    global args, best_prec1\n    args = parser.parse_args()\n    args.size = tuple(args.size)\n\n    # create model\n    from model.Model02 import Model02 as Model\n\n    class Capsule(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            nb_of_classes = 33  # 970 (vid) or 35 (vid obj) or 33 (imgs)\n            self.inner_model = Model(args.size + (nb_of_classes,), (256, 256))\n\n        def forward(self, x):\n            (_, _), (_, video_index) = self.inner_model(x, None)\n            return video_index\n\n    model = Capsule()\n\n    model = torch.nn.DataParallel(model).cuda()\n\n    cudnn.benchmark = True\n\n    # Data loading code\n    traindir = os.path.join(args.data, \'train\')\n    valdir = os.path.join(args.data, \'val\')\n#    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n#                                     std=[0.229, 0.224, 0.225])\n\n    train_data = datasets.ImageFolder(traindir, transforms.Compose([\n            transforms.CenterCrop(256),\n            transforms.ToTensor(),\n        ]))\n    train_loader = torch.utils.data.DataLoader(\n        train_data,\n        batch_size=args.batch_size, shuffle=True,\n        num_workers=args.workers, pin_memory=True\n    )\n\n    val_data = datasets.ImageFolder(valdir, transforms.Compose([transforms.CenterCrop(256), transforms.ToTensor(), ]))\n    val_loader = torch.utils.data.DataLoader(\n        val_data,\n        batch_size=args.batch_size, shuffle=False,\n        num_workers=args.workers, pin_memory=True\n    )\n\n    # define loss function (criterion) and optimizer\n    class_count = [0] * len(train_data.classes)\n    for i in train_data.imgs: class_count[i[1]] += 1\n    train_crit_weight = torch.Tensor(class_count)\n    train_crit_weight.div_(train_crit_weight.mean()).pow_(-1)\n    train_criterion = nn.CrossEntropyLoss(train_crit_weight).cuda()\n\n    class_count = [0] * len(val_data.classes)\n    for i in val_data.imgs: class_count[i[1]] += 1\n    val_crit_weight = torch.Tensor(class_count)\n    val_crit_weight.div_(val_crit_weight.mean()).pow_(-1)\n    val_criterion = nn.CrossEntropyLoss(val_crit_weight).cuda()\n\n    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n\n    if args.evaluate:\n        validate(val_loader, model, val_criterion)\n        return\n\n    for epoch in range(args.start_epoch, args.epochs):\n        adjust_learning_rate(optimizer, epoch)\n\n        # train for one epoch\n        train(train_loader, model, train_criterion, optimizer, epoch)\n\n        # evaluate on validation set\n        prec1 = validate(val_loader, model, val_criterion)\n\n        # remember best prec@1 and save checkpoint\n        is_best = prec1 > best_prec1\n        best_prec1 = max(prec1, best_prec1)\n        save_checkpoint({\n            \'epoch\': epoch + 1,\n            \'state_dict\': model.state_dict(),\n            \'best_prec1\': best_prec1,\n        }, is_best)\n\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (input, target) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        target = target.cuda(async=True)\n        input_var = torch.autograd.Variable(input)\n        target_var = torch.autograd.Variable(target)\n\n        # compute output\n        output = model(input_var)\n        loss = criterion(output, target_var)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n        losses.update(loss.data[0], input.size(0))\n        top1.update(prec1[0], input.size(0))\n        top5.update(prec5[0], input.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print(\'Epoch: [{0}][{1}/{2}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                  \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                   epoch, i, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n\n\ndef validate(val_loader, model, criterion):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    for i, (input, target) in enumerate(val_loader):\n        target = target.cuda(async=True)\n        input_var = torch.autograd.Variable(input, volatile=True)\n        target_var = torch.autograd.Variable(target, volatile=True)\n\n        # compute output\n        output = model(input_var)\n        loss = criterion(output, target_var)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n        losses.update(loss.data[0], input.size(0))\n        top1.update(prec1[0], input.size(0))\n        top5.update(prec5[0], input.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print(\'Test: [{0}/{1}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                  \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                   i, len(val_loader), batch_time=batch_time, loss=losses,\n                   top1=top1, top5=top5))\n\n    print(\' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}\'\n          .format(top1=top1, top5=top5))\n\n    return top1.avg\n\n\ndef save_checkpoint(state, is_best, filename=\'checkpoint.pth.tar\'):\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, \'model_best.pth.tar\')\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef adjust_learning_rate(optimizer, epoch):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n    lr = args.lr * (0.1 ** (epoch // 30))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\n\nif __name__ == \'__main__\':\n    main()\n'"
model/ConvLSTMCell.py,8,"b'import torch\nfrom torch import nn\nimport torch.nn.functional as f\nfrom torch.autograd import Variable\n\n\n# Define some constants\nKERNEL_SIZE = 3\nPADDING = KERNEL_SIZE // 2\n\n\nclass ConvLSTMCell(nn.Module):\n    """"""\n    Generate a convolutional LSTM cell\n    """"""\n\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Gates = nn.Conv2d(input_size + hidden_size, 4 * hidden_size, KERNEL_SIZE, padding=PADDING)\n\n    def forward(self, input_, prev_state):\n\n        # get batch and spatial sizes\n        batch_size = input_.data.size()[0]\n        spatial_size = input_.data.size()[2:]\n\n        # generate empty prev_state, if None is provided\n        if prev_state is None:\n            state_size = [batch_size, self.hidden_size] + list(spatial_size)\n            prev_state = (\n                Variable(torch.zeros(state_size)),\n                Variable(torch.zeros(state_size))\n            )\n\n        prev_hidden, prev_cell = prev_state\n\n        # data size is [batch, channel, height, width]\n        stacked_inputs = torch.cat((input_, prev_hidden), 1)\n        gates = self.Gates(stacked_inputs)\n\n        # chunk across channel dimension\n        in_gate, remember_gate, out_gate, cell_gate = gates.chunk(4, 1)\n\n        # apply sigmoid non linearity\n        in_gate = f.sigmoid(in_gate)\n        remember_gate = f.sigmoid(remember_gate)\n        out_gate = f.sigmoid(out_gate)\n\n        # apply tanh non linearity\n        cell_gate = f.tanh(cell_gate)\n\n        # compute current cell and hidden state\n        cell = (remember_gate * prev_cell) + (in_gate * cell_gate)\n        hidden = out_gate * f.tanh(cell)\n\n        return hidden, cell\n\n\ndef _main():\n    """"""\n    Run some basic tests on the API\n    """"""\n\n    # define batch_size, channels, height, width\n    b, c, h, w = 1, 3, 4, 8\n    d = 5           # hidden state size\n    lr = 1e-1       # learning rate\n    T = 6           # sequence length\n    max_epoch = 20  # number of epochs\n\n    # set manual seed\n    torch.manual_seed(0)\n\n    print(\'Instantiate model\')\n    model = ConvLSTMCell(c, d)\n    print(repr(model))\n\n    print(\'Create input and target Variables\')\n    x = Variable(torch.rand(T, b, c, h, w))\n    y = Variable(torch.randn(T, b, d, h, w))\n\n    print(\'Create a MSE criterion\')\n    loss_fn = nn.MSELoss()\n\n    print(\'Run for\', max_epoch, \'iterations\')\n    for epoch in range(0, max_epoch):\n        state = None\n        loss = 0\n        for t in range(0, T):\n            state = model(x[t], state)\n            loss += loss_fn(state[0], y[t])\n\n        print(\' > Epoch {:2d} loss: {:.3f}\'.format((epoch+1), loss.data[0]))\n\n        # zero grad parameters\n        model.zero_grad()\n\n        # compute new grad parameters through time!\n        loss.backward()\n\n        # learning_rate step against the gradient\n        for p in model.parameters():\n            p.data.sub_(p.grad.data * lr)\n\n    print(\'Input size:\', list(x.data.size()))\n    print(\'Target size:\', list(y.data.size()))\n    print(\'Last hidden state size:\', list(state[0].size()))\n\n\nif __name__ == \'__main__\':\n    _main()\n\n\n__author__ = ""Alfredo Canziani""\n__credits__ = [""Alfredo Canziani""]\n__maintainer__ = ""Alfredo Canziani""\n__email__ = ""alfredo.canziani@gmail.com""\n__status__ = ""Prototype""  # ""Prototype"", ""Development"", or ""Production""\n__date__ = ""Jan 17""\n'"
model/DiscriminativeCell.py,6,"b'import torch\nfrom torch import nn\nimport torch.nn.functional as f\nfrom torch.autograd import Variable\n\n\n# Define some constants\nKERNEL_SIZE = 3\nPADDING = KERNEL_SIZE // 2\nPOOL = 2\n\n\nclass DiscriminativeCell(nn.Module):\n    """"""\n    Single discriminative layer\n    """"""\n\n    def __init__(self, input_size, hidden_size, first=False):\n        """"""\n        Create a discriminative cell (bottom_up, r_state) -> error\n\n        :param input_size: {\'input\': bottom_up_size, \'state\': r_state_size}\n        :param hidden_size: int, shooting dimensionality\n        :param first: True/False\n        """"""\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.first = first\n        if not first:\n            self.from_bottom = nn.Conv2d(input_size[\'input\'], hidden_size, KERNEL_SIZE, padding=PADDING)\n        self.from_state = nn.Conv2d(input_size[\'state\'], hidden_size, KERNEL_SIZE, padding=PADDING)\n\n    def forward(self, bottom_up, state):\n        input_projection = self.first and bottom_up or f.relu(f.max_pool2d(self.from_bottom(bottom_up), POOL, POOL))\n        state_projection = f.relu(self.from_state(state))\n        error = f.relu(torch.cat((input_projection - state_projection, state_projection - input_projection), 1))\n        return error\n\n\ndef _test_layer1():\n    print(\'Define model for layer 1\')\n    discriminator = DiscriminativeCell(input_size={\'input\': 3, \'state\': 3}, hidden_size=3, first=True)\n\n    print(\'Define input and state\')\n    # at the first layer we have that system_state match the input_image dimensionality\n    input_image = Variable(torch.rand(1, 3, 8, 12))\n    system_state = Variable(torch.randn(1, 3, 8, 12))\n\n    print(\'Input has size\', list(input_image.data.size()))\n\n    print(\'Forward input and state to the model\')\n    e = discriminator(input_image, system_state)\n\n    # print output size\n    print(\'Layer 1 error has size\', list(e.data.size()))\n\n    return e\n\n\ndef _test_layer2(input_error):\n    print(\'Define model for layer 2\')\n    discriminator = DiscriminativeCell(input_size={\'input\': 6, \'state\': 32}, hidden_size=32, first=False)\n\n    print(\'Define a new, smaller state\')\n    system_state = Variable(torch.randn(1, 32, 4, 6))\n\n    print(\'Forward layer 1 output and state to the model\')\n    e = discriminator(input_error, system_state)\n\n    # print output size\n    print(\'Layer 2 error has size\', list(e.data.size()))\n\n\ndef _test_layers():\n    error = _test_layer1()\n    _test_layer2(input_error=error)\n\n\nif __name__ == \'__main__\':\n    _test_layers()\n\n\n__author__ = ""Alfredo Canziani""\n__credits__ = [""Alfredo Canziani""]\n__maintainer__ = ""Alfredo Canziani""\n__email__ = ""alfredo.canziani@gmail.com""\n__status__ = ""Prototype""  # ""Prototype"", ""Development"", or ""Production""\n__date__ = ""Feb 17""\n'"
model/GenerativeCell.py,6,"b'import torch\nfrom torch import nn\nimport torch.nn.functional as f\nfrom torch.autograd import Variable\n\nfrom model.ConvLSTMCell import ConvLSTMCell\n\n\nclass GenerativeCell(nn.Module):\n    """"""\n    Single generative layer\n    """"""\n\n    def __init__(self, input_size, hidden_size, error_init_size=None):\n        """"""\n        Create a generative cell (error, top_down_state, r_state) -> r_state\n\n        :param input_size: {\'error\': error_size, \'up_state\': r_state_size}, r_state_size can be 0\n        :param hidden_size: int, shooting dimensionality\n        :param error_init_size: tuple, full size of initial (null) error\n        """"""\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.error_init_size = error_init_size\n        self.memory = ConvLSTMCell(input_size[\'error\']+input_size[\'up_state\'], hidden_size)\n\n    def forward(self, error, top_down_state, state):\n        if error is None:  # we just started\n            error = Variable(torch.zeros(self.error_init_size))\n        model_input = error\n        if top_down_state is not None:\n            model_input = torch.cat((error, f.upsample_nearest(top_down_state, scale_factor=2)), 1)\n        return self.memory(model_input, state)\n\n\ndef _test_layer2():\n    print(\'Define model for layer 2\')\n    generator = GenerativeCell(input_size={\'error\': 2*16, \'up_state\': 0}, hidden_size=16)\n\n    print(\'Define error and top down state\')\n    input_error = Variable(torch.randn(1, 2*16, 4, 6))\n    topdown_state = None\n\n    print(\'Input error has size\', list(input_error.data.size()))\n    print(\'Top down state is None\')\n\n    print(\'Forward error and top down state to the model\')\n    state = None\n    state = generator(input_error, topdown_state, state)\n\n    # print output size\n    print(\'Layer 2 state has size\', list(state[0].data.size()))\n\n    return state[0]  # the element 1 is the cell state\n\n\ndef _test_layer1(top_down_state):\n    print(\'Define model for layer 1\')\n    generator = GenerativeCell(input_size={\'error\': 2*3, \'up_state\': 16}, hidden_size=3)\n\n    print(\'Define error and top down state\')\n    input_error = Variable(torch.randn(1, 2*3, 8, 12))\n\n    print(\'Input error has size\', list(input_error.data.size()))\n    print(\'Top down state has size\', list(top_down_state.data.size()))\n\n    print(\'Forward error and top down state to the model\')\n    state = None\n    state = generator(input_error, top_down_state, state)\n\n    # print output size\n    print(\'Layer 1 state has size\', list(state[0].data.size()))\n\n\ndef _test_layers():\n    state = _test_layer2()\n    _test_layer1(top_down_state=state)\n\n\nif __name__ == \'__main__\':\n    _test_layers()\n\n\n__author__ = ""Alfredo Canziani""\n__credits__ = [""Alfredo Canziani""]\n__maintainer__ = ""Alfredo Canziani""\n__email__ = ""alfredo.canziani@gmail.com""\n__status__ = ""Prototype""  # ""Prototype"", ""Development"", or ""Production""\n__date__ = ""Feb 17""\n'"
model/Model01.py,7,"b'import torch\nfrom torch import nn\nimport torch.nn.functional as f\nfrom torch.autograd import Variable as V\nfrom math import ceil\n\n\n# Define some constants\nKERNEL_SIZE = 3\nPADDING = KERNEL_SIZE // 2\nKERNEL_STRIDE = 2\nOUTPUT_ADJUST = KERNEL_SIZE - 2 * PADDING\n\n\nclass Model01(nn.Module):\n    """"""\n    Generate a constructor for model_01 type of network\n    """"""\n\n    def __init__(self, network_size: tuple, input_spatial_size: tuple) -> None:\n        """"""\n        Initialise Model01 constructor\n\n        :param network_size: (n, h1, h2, ..., emb_size, nb_videos)\n        :type network_size: tuple\n        :param input_spatial_size: (height, width)\n        :type input_spatial_size: tuple\n        """"""\n        super().__init__()\n        self.hidden_layers = len(network_size) - 2\n\n        print(\'\\n{:-^80}\'.format(\' Building model \'))\n        print(\'Hidden layers:\', self.hidden_layers)\n        print(\'Net sizing:\', network_size)\n        print(\'Input spatial size: {} x {}\'.format(network_size[0], input_spatial_size))\n\n        # main auto-encoder blocks\n        self.activation_size = [input_spatial_size]\n        for layer in range(0, self.hidden_layers):\n            # print some annotation when building model\n            print(\'{:-<80}\'.format(\'Layer \' + str(layer + 1) + \' \'))\n            print(\'Bottom size: {} x {}\'.format(network_size[layer], self.activation_size[-1]))\n            self.activation_size.append(tuple(ceil(s / 2) for s in self.activation_size[layer]))\n            print(\'Top size: {} x {}\'.format(network_size[layer + 1], self.activation_size[-1]))\n\n            # init D (discriminative) blocks\n            setattr(self, \'D_\' + str(layer + 1), nn.Conv2d(\n                in_channels=network_size[layer], out_channels=network_size[layer + 1],\n                kernel_size=KERNEL_SIZE, stride=KERNEL_STRIDE, padding=PADDING\n            ))\n            setattr(self, \'BN_D_\' + str(layer + 1), nn.BatchNorm2d(network_size[layer + 1]))\n\n            # init G (generative) blocks\n            setattr(self, \'G_\' + str(layer + 1), nn.ConvTranspose2d(\n                in_channels=network_size[layer + 1], out_channels=network_size[layer],\n                kernel_size=KERNEL_SIZE, stride=KERNEL_STRIDE, padding=PADDING\n            ))\n            setattr(self, \'BN_G_\' + str(layer + 1), nn.BatchNorm2d(network_size[layer]))\n\n        # init auxiliary classifier\n        print(\'{:-<80}\'.format(\'Classifier \'))\n        print(network_size[-2], \'-->\', network_size[-1])\n        self.average = nn.AvgPool2d(self.activation_size[-1])\n        self.stabiliser = nn.Linear(network_size[-2], network_size[-1])\n        print(80 * \'-\', end=\'\\n\\n\')\n\n    def forward(self, x, state):\n        activation_sizes = [x.size()]  # start from the input\n        residuals = list()\n        for layer in range(0, self.hidden_layers):  # connect discriminative blocks\n            x = getattr(self, \'D_\' + str(layer + 1))(x)\n            residuals.append(x)\n            if layer < self.hidden_layers - 1 and state: x += state[layer]\n            x = f.relu(x)\n            x = getattr(self, \'BN_D_\' + str(layer + 1))(x)\n            activation_sizes.append(x.size())  # cache output size for later retrieval\n        state = state or [None] * (self.hidden_layers - 1)\n        for layer in reversed(range(0, self.hidden_layers)):  # connect generative blocks\n            x = getattr(self, \'G_\' + str(layer + 1))(x, activation_sizes[layer])\n            if layer:\n                state[layer - 1] = x\n                x += residuals[layer - 1]\n            x = f.relu(x)\n            x = getattr(self, \'BN_G_\' + str(layer + 1))(x)\n        x_mean = self.average(residuals[-1])\n        video_index = self.stabiliser(x_mean.view(x_mean.size(0), -1))\n\n        return (x, state), (x_mean, video_index)\n\n\ndef _test_model():\n    T = 2\n    x = torch.rand(T + 1, 1, 3, 4 * 2**3 + 3, 6 * 2**3 + 5)\n    K = 10\n    y = torch.LongTensor(T, 1).random_(K)\n    model_01 = Model01(network_size=(3, 6, 12, 18, K), input_spatial_size=x[0].size()[2:])\n\n    state = None\n    (x_hat, state), (emb, idx) = model_01(V(x[0]), state)\n\n    print(\'Input size:\', tuple(x.size()))\n    print(\'Output size:\', tuple(x_hat.data.size()))\n    print(\'Video index size:\', tuple(idx.size()))\n    for i, s in enumerate(state):\n        print(\'State\', i + 1, \'has size:\', tuple(s.size()))\n    print(\'Embedding has size:\', emb.data.numel())\n\n    mse = nn.MSELoss()\n    nll = nn.CrossEntropyLoss()\n    x_next = V(x[1])\n    y_var = V(y[0])\n    loss_t1 = mse(x_hat, x_next) + nll(idx, y_var)\n\n    from utils.visualise import show_graph\n    show_graph(loss_t1)\n\n    # run one more time\n    (x_hat, _), (_, idx) = model_01(V(x[1]), state)\n\n    x_next = V(x[2])\n    y_var = V(y[1])\n    loss_t2 = mse(x_hat, x_next) + nll(idx, y_var)\n    loss_tot = loss_t2 + loss_t1\n\n    show_graph(loss_tot)\n\n\ndef _test_training():\n    K = 10  # number of training videos\n    network_size = (3, 6, 12, 18, K)\n    T = 6  # sequence length\n    max_epoch = 10  # number of epochs\n    lr = 1e-1  # learning rate\n\n    # set manual seed\n    torch.manual_seed(0)\n\n    print(\'\\n{:-^80}\'.format(\' Train a \' + str(network_size[:-1]) + \' layer network \'))\n    print(\'Sequence length T:\', T)\n    print(\'Create the input image and target sequences\')\n    x = torch.rand(T + 1, 1, 3, 4 * 2**3 + 3, 6 * 2**3 + 5)\n    y = torch.LongTensor(T, 1).random_(K)\n    print(\'Input has size\', tuple(x.size()))\n    print(\'Target index has size\', tuple(y.size()))\n\n    print(\'Define model\')\n    model = Model01(network_size=network_size, input_spatial_size=x[0].size()[2:])\n\n    print(\'Create a MSE and NLL criterions\')\n    mse = nn.MSELoss()\n    nll = nn.CrossEntropyLoss()\n\n    print(\'Run for\', max_epoch, \'iterations\')\n    for epoch in range(0, max_epoch):\n        state = None\n        loss = 0\n        for t in range(0, T):\n            (x_hat, state), (emb, idx) = model(V(x[t]), state)\n            loss += mse(x_hat, V(x[t + 1])) + nll(idx, V(y[t]))\n\n        print(\' > Epoch {:2d} loss: {:.3f}\'.format((epoch + 1), loss.data[0]))\n\n        # zero grad parameters\n        model.zero_grad()\n\n        # compute new grad parameters through time!\n        loss.backward()\n\n        # learning_rate step against the gradient\n        for p in model.parameters():\n            p.data.sub_(p.grad.data * lr)\n\n\nif __name__ == \'__main__\':\n    _test_model()\n    _test_training()\n\n\n__author__ = ""Alfredo Canziani""\n__credits__ = [""Alfredo Canziani""]\n__maintainer__ = ""Alfredo Canziani""\n__email__ = ""alfredo.canziani@gmail.com""\n__status__ = ""Production""  # ""Prototype"", ""Development"", or ""Production""\n__date__ = ""Feb 17""\n'"
model/Model02.py,9,"b'import torch\nfrom torch import nn\nimport torch.nn.functional as f\nfrom torch.autograd import Variable as V\nfrom math import ceil\n\n\n# Define some constants\nfrom model.RG import RG\n\nKERNEL_SIZE = 3\nPADDING = KERNEL_SIZE // 2\nKERNEL_STRIDE = 2\nOUTPUT_ADJUST = KERNEL_SIZE - 2 * PADDING\n\n\nclass Model02(nn.Module):\n    """"""\n    Generate a constructor for model_02 type of network\n    """"""\n\n    def __init__(self, network_size: tuple, input_spatial_size: tuple) -> None:\n        """"""\n        Initialise Model02 constructor\n\n        :param network_size: (n, h1, h2, ..., emb_size, nb_videos)\n        :type network_size: tuple\n        :param input_spatial_size: (height, width)\n        :type input_spatial_size: tuple\n        """"""\n        super().__init__()\n        self.hidden_layers = len(network_size) - 2\n\n        print(\'\\n{:-^80}\'.format(\' Building model Model02 \'))\n        print(\'Hidden layers:\', self.hidden_layers)\n        print(\'Net sizing:\', network_size)\n        print(\'Input spatial size: {} x {}\'.format(network_size[0], input_spatial_size))\n\n        # main auto-encoder blocks\n        self.activation_size = [input_spatial_size]\n        for layer in range(0, self.hidden_layers):\n            # print some annotation when building model\n            print(\'{:-<80}\'.format(\'Layer \' + str(layer + 1) + \' \'))\n            print(\'Bottom size: {} x {}\'.format(network_size[layer], self.activation_size[-1]))\n            self.activation_size.append(tuple(ceil(s / 2) for s in self.activation_size[layer]))\n            print(\'Top size: {} x {}\'.format(network_size[layer + 1], self.activation_size[-1]))\n\n            # init D (discriminative) blocks\n            multiplier = layer and 2 or 1  # D_n, n > 1, has intra-layer feedback\n            setattr(self, \'D_\' + str(layer + 1), nn.Conv2d(\n                in_channels=network_size[layer] * multiplier, out_channels=network_size[layer + 1],\n                kernel_size=KERNEL_SIZE, stride=KERNEL_STRIDE, padding=PADDING\n            ))\n            setattr(self, \'BN_D_\' + str(layer + 1), nn.BatchNorm2d(network_size[layer + 1]))\n\n            # init G (generative) blocks\n            setattr(self, \'G_\' + str(layer + 1), nn.ConvTranspose2d(\n                in_channels=network_size[layer + 1], out_channels=network_size[layer],\n                kernel_size=KERNEL_SIZE, stride=KERNEL_STRIDE, padding=PADDING\n            ))\n            setattr(self, \'BN_G_\' + str(layer + 1), nn.BatchNorm2d(network_size[layer]))\n\n        # init auxiliary classifier\n        print(\'{:-<80}\'.format(\'Classifier \'))\n        print(network_size[-2], \'-->\', network_size[-1])\n        self.average = nn.AvgPool2d(self.activation_size[-1])\n        self.stabiliser = nn.Linear(network_size[-2], network_size[-1])\n        print(80 * \'-\', end=\'\\n\\n\')\n\n    def forward(self, x, state):\n        activation_sizes = [x.size()]  # start from the input\n        residuals = list()\n        state = state or [None] * (self.hidden_layers - 1)\n        for layer in range(0, self.hidden_layers):  # connect discriminative blocks\n            if layer:  # concat the input with the state for D_n, n > 1\n                s = state[layer - 1] or V(x.data.clone().zero_())\n                x = torch.cat((x, s), 1)\n            x = getattr(self, \'D_\' + str(layer + 1))(x)\n            residuals.append(x)\n            x = f.relu(x)\n            x = getattr(self, \'BN_D_\' + str(layer + 1))(x)\n            activation_sizes.append(x.size())  # cache output size for later retrieval\n        for layer in reversed(range(0, self.hidden_layers)):  # connect generative blocks\n            x = getattr(self, \'G_\' + str(layer + 1))(x, activation_sizes[layer])\n            if layer:\n                state[layer - 1] = x\n                x += residuals[layer - 1]\n            x = f.relu(x)\n            x = getattr(self, \'BN_G_\' + str(layer + 1))(x)\n        x_mean = self.average(residuals[-1])\n        video_index = self.stabiliser(x_mean.view(x_mean.size(0), -1))\n\n        return (x, state), (x_mean, video_index)\n\n\nclass Model02RG(nn.Module):\n    """"""\n    Generate a constructor for model_02_rg type of network\n    """"""\n\n    def __init__(self, network_size: tuple, input_spatial_size: tuple) -> None:\n        """"""\n        Initialise Model02RG constructor\n\n        :param network_size: (n, h1, h2, ..., emb_size, nb_videos)\n        :type network_size: tuple\n        :param input_spatial_size: (height, width)\n        :type input_spatial_size: tuple\n        """"""\n        super().__init__()\n        self.hidden_layers = len(network_size) - 2\n\n        print(\'\\n{:-^80}\'.format(\' Building model Model02RG \'))\n        print(\'Hidden layers:\', self.hidden_layers)\n        print(\'Net sizing:\', network_size)\n        print(\'Input spatial size: {} x {}\'.format(network_size[0], input_spatial_size))\n\n        # main auto-encoder blocks\n        self.activation_size = [input_spatial_size]\n        for layer in range(0, self.hidden_layers):\n            # print some annotation when building model\n            print(\'{:-<80}\'.format(\'Layer \' + str(layer + 1) + \' \'))\n            print(\'Bottom size: {} x {}\'.format(network_size[layer], self.activation_size[-1]))\n            self.activation_size.append(tuple(ceil(s / 2) for s in self.activation_size[layer]))\n            print(\'Top size: {} x {}\'.format(network_size[layer + 1], self.activation_size[-1]))\n\n            # init D (discriminative) blocks\n            multiplier = layer and 2 or 1  # D_n, n > 1, has intra-layer feedback\n            setattr(self, \'D_\' + str(layer + 1), nn.Conv2d(\n                in_channels=network_size[layer] * multiplier, out_channels=network_size[layer + 1],\n                kernel_size=KERNEL_SIZE, stride=KERNEL_STRIDE, padding=PADDING\n            ))\n            setattr(self, \'BN_D_\' + str(layer + 1), nn.BatchNorm2d(network_size[layer + 1]))\n\n            # init G (generative) blocks\n            setattr(self, \'G_\' + str(layer + 1), RG(\n                in_channels=network_size[layer + 1], out_channels=network_size[layer],\n                kernel_size=KERNEL_SIZE, stride=KERNEL_STRIDE, padding=PADDING\n            ))\n            setattr(self, \'BN_G_\' + str(layer + 1), nn.BatchNorm2d(network_size[layer]))\n\n        # init auxiliary classifier\n        print(\'{:-<80}\'.format(\'Classifier \'))\n        print(network_size[-2], \'-->\', network_size[-1])\n        self.average = nn.AvgPool2d(self.activation_size[-1])\n        self.stabiliser = nn.Linear(network_size[-2], network_size[-1])\n        print(80 * \'-\', end=\'\\n\\n\')\n\n    def forward(self, x, state):\n        activation_sizes = [x.size()]  # start from the input\n        residuals = list()\n        # state[0] --> network layer state; state[1] --> generative state\n        state = state or [[None] * (self.hidden_layers - 1), [None] * self.hidden_layers]\n        for layer in range(0, self.hidden_layers):  # connect discriminative blocks\n            if layer:  # concat the input with the state for D_n, n > 1\n                s = state[0][layer - 1] or V(x.data.clone().zero_())\n                x = torch.cat((x, s), 1)\n            x = getattr(self, \'D_\' + str(layer + 1))(x)\n            residuals.append(x)\n            x = f.relu(x)\n            x = getattr(self, \'BN_D_\' + str(layer + 1))(x)\n            activation_sizes.append(x.size())  # cache output size for later retrieval\n        for layer in reversed(range(0, self.hidden_layers)):  # connect generative blocks\n            x = getattr(self, \'G_\' + str(layer + 1))((x, activation_sizes[layer]), state[1][layer])\n            state[1][layer] = x  # h[t - 1] <- h[t]\n            if layer:\n                state[0][layer - 1] = x\n                x += residuals[layer - 1]\n            x = f.relu(x)\n            x = getattr(self, \'BN_G_\' + str(layer + 1))(x)\n        x_mean = self.average(residuals[-1])\n        video_index = self.stabiliser(x_mean.view(x_mean.size(0), -1))\n\n        return (x, state), (x_mean, video_index)\n\n\ndef _test_models():\n    _test_model(Model02)\n    _test_model(Model02RG)\n\n\ndef _test_model(Model):\n    big_t = 2\n    x = torch.rand(big_t + 1, 1, 3, 4 * 2**3 + 3, 6 * 2**3 + 5)\n    big_k = 10\n    y = torch.LongTensor(big_t, 1).random_(big_k)\n    model = Model(network_size=(3, 6, 12, 18, big_k), input_spatial_size=x[0].size()[2:])\n\n    state = None\n    (x_hat, state), (emb, idx) = model(V(x[0]), state)\n\n    print(\'Input size:\', tuple(x.size()))\n    print(\'Output size:\', tuple(x_hat.data.size()))\n    print(\'Video index size:\', tuple(idx.size()))\n    for i, s in enumerate(state):\n        if isinstance(s, list):\n            for i, s in enumerate(state[0]):\n                print(\'Net state\', i + 1, \'has size:\', tuple(s.size()))\n            for i, s in enumerate(state[1]):\n                print(\'G\', i + 1, \'state has size:\', tuple(s.size()))\n            break\n        else:\n            print(\'State\', i + 1, \'has size:\', tuple(s.size()))\n    print(\'Embedding has size:\', emb.data.numel())\n\n    mse = nn.MSELoss()\n    nll = nn.CrossEntropyLoss()\n    x_next = V(x[1])\n    y_var = V(y[0])\n    loss_t1 = mse(x_hat, x_next) + nll(idx, y_var)\n\n    from utils.visualise import show_graph\n    show_graph(loss_t1)\n\n    # run one more time\n    (x_hat, _), (_, idx) = model(V(x[1]), state)\n\n    x_next = V(x[2])\n    y_var = V(y[1])\n    loss_t2 = mse(x_hat, x_next) + nll(idx, y_var)\n    loss_tot = loss_t2 + loss_t1\n\n    show_graph(loss_tot)\n\n\ndef _test_training_models():\n    _test_training(Model02)\n    _test_training(Model02RG)\n\n\ndef _test_training(Model):\n    big_k = 10  # number of training videos\n    network_size = (3, 6, 12, 18, big_k)\n    big_t = 6  # sequence length\n    max_epoch = 10  # number of epochs\n    lr = 3.16e-2  # learning rate\n\n    # set manual seed\n    torch.manual_seed(0)\n\n    print(\'\\n{:-^80}\'.format(\' Train a \' + str(network_size[:-1]) + \' layer network \'))\n    print(\'Sequence length T:\', big_t)\n    print(\'Create the input image and target sequences\')\n    x = torch.rand(big_t + 1, 1, 3, 4 * 2**3 + 3, 6 * 2**3 + 5)\n    y = torch.LongTensor(big_t, 1).random_(big_k)\n    print(\'Input has size\', tuple(x.size()))\n    print(\'Target index has size\', tuple(y.size()))\n\n    print(\'Define model\')\n    model = Model(network_size=network_size, input_spatial_size=x[0].size()[2:])\n\n    print(\'Create a MSE and NLL criterions\')\n    mse = nn.MSELoss()\n    nll = nn.CrossEntropyLoss()\n\n    print(\'Run for\', max_epoch, \'iterations\')\n    for epoch in range(0, max_epoch):\n        state = None\n        loss = 0\n        for t in range(0, big_t):\n            (x_hat, state), (emb, idx) = model(V(x[t]), state)\n            loss += mse(x_hat, V(x[t + 1])) + nll(idx, V(y[t]))\n\n        print(\' > Epoch {:2d} loss: {:.3f}\'.format((epoch + 1), loss.data[0]))\n\n        # zero grad parameters\n        model.zero_grad()\n\n        # compute new grad parameters through time!\n        loss.backward()\n\n        # learning_rate step against the gradient\n        for p in model.parameters():\n            p.data.sub_(p.grad.data * lr)\n\n\nif __name__ == \'__main__\':\n    _test_models()\n    _test_training_models()\n\n\n__author__ = ""Alfredo Canziani""\n__credits__ = [""Alfredo Canziani""]\n__maintainer__ = ""Alfredo Canziani""\n__email__ = ""alfredo.canziani@gmail.com""\n__status__ = ""Production""  # ""Prototype"", ""Development"", or ""Production""\n__date__ = ""Feb, Mar 17""\n'"
model/PrednetModel.py,7,"b'import torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\nfrom model.DiscriminativeCell import DiscriminativeCell\nfrom model.GenerativeCell import GenerativeCell\n\n\n# Define some constants\nOUT_LAYER_SIZE = (3,) + tuple(2 ** p for p in range(4, 10))\nERR_LAYER_SIZE = tuple(size * 2 for size in OUT_LAYER_SIZE)\nIN_LAYER_SIZE = (3,) + ERR_LAYER_SIZE\n\n\nclass PrednetModel(nn.Module):\n    """"""\n    Build the Prednet model\n    """"""\n\n    def __init__(self, error_size_list):\n        super().__init__()\n        self.number_of_layers = len(error_size_list)\n        for layer in range(0, self.number_of_layers):\n            setattr(self, \'discriminator_\' + str(layer + 1), DiscriminativeCell(\n                input_size={\'input\': IN_LAYER_SIZE[layer], \'state\': OUT_LAYER_SIZE[layer]},\n                hidden_size=OUT_LAYER_SIZE[layer],\n                first=(not layer)\n            ))\n            setattr(self, \'generator_\' + str(layer + 1), GenerativeCell(\n                input_size={\'error\': ERR_LAYER_SIZE[layer], \'up_state\':\n                    OUT_LAYER_SIZE[layer + 1] if layer != self.number_of_layers - 1 else 0},\n                hidden_size=OUT_LAYER_SIZE[layer],\n                error_init_size=error_size_list[layer]\n            ))\n\n    def forward(self, bottom_up_input, error, state):\n\n        # generative branch\n        up_state = None\n        for layer in reversed(range(0, self.number_of_layers)):\n            state[layer] = getattr(self, \'generator_\' + str(layer + 1))(\n                error[layer], up_state, state[layer]\n            )\n            up_state = state[layer][0]\n\n        # discriminative branch\n        for layer in range(0, self.number_of_layers):\n            error[layer] = getattr(self, \'discriminator_\' + str(layer + 1))(\n                layer and error[layer - 1] or bottom_up_input,\n                state[layer][0]\n            )\n\n        return error, state\n\n\nclass _BuildOneLayerModel(nn.Module):\n    """"""\n    Build a one layer Prednet model\n    """"""\n\n    def __init__(self, error_size_list):\n        super().__init__()\n        self.discriminator = DiscriminativeCell(\n            input_size={\'input\': IN_LAYER_SIZE[0], \'state\': OUT_LAYER_SIZE[0]},\n            hidden_size=OUT_LAYER_SIZE[0],\n            first=True\n        )\n        self.generator = GenerativeCell(\n            input_size={\'error\': ERR_LAYER_SIZE[0], \'up_state\': 0},\n            hidden_size=OUT_LAYER_SIZE[0],\n            error_init_size=error_size_list[0]\n        )\n\n    def forward(self, bottom_up_input, prev_error, state):\n        state = self.generator(prev_error, None, state)\n        error = self.discriminator(bottom_up_input, state[0])\n        return error, state\n\n\nclass _BuildTwoLayerModel(nn.Module):\n    """"""\n    Build a two layer Prednet model\n    """"""\n\n    def __init__(self, error_size_list):\n        super().__init__()\n        self.discriminator_1 = DiscriminativeCell(\n            input_size={\'input\': IN_LAYER_SIZE[0], \'state\': OUT_LAYER_SIZE[0]},\n            hidden_size=OUT_LAYER_SIZE[0],\n            first=True\n        )\n        self.discriminator_2 = DiscriminativeCell(\n            input_size={\'input\': IN_LAYER_SIZE[1], \'state\': OUT_LAYER_SIZE[1]},\n            hidden_size=OUT_LAYER_SIZE[1]\n        )\n        self.generator_1 = GenerativeCell(\n            input_size={\'error\': ERR_LAYER_SIZE[0], \'up_state\': OUT_LAYER_SIZE[1]},\n            hidden_size=OUT_LAYER_SIZE[0],\n            error_init_size=error_size_list[0]\n        )\n        self.generator_2 = GenerativeCell(\n            input_size={\'error\': ERR_LAYER_SIZE[1], \'up_state\': 0},\n            hidden_size=OUT_LAYER_SIZE[1],\n            error_init_size=error_size_list[1]\n        )\n\n    def forward(self, bottom_up_input, error, state):\n        state[1] = self.generator_2(error[1], None, state[1])\n        state[0] = self.generator_1(error[0], state[1][0], state[0])\n        error[0] = self.discriminator_1(bottom_up_input, state[0][0])\n        error[1] = self.discriminator_2(error[0], state[1][0])\n        return error, state\n\n\ndef _test_one_layer_model():\n    print(\'\\nCreate the input image\')\n    input_image = Variable(torch.rand(1, 3, 8, 12))\n\n    print(\'Input has size\', list(input_image.data.size()))\n\n    error_init_size = (1, 6, 8, 12)\n    print(\'The error initialisation size is\', error_init_size)\n\n    print(\'Define a 1 layer Prednet\')\n    model = _BuildOneLayerModel((error_init_size,))\n\n    print(\'Forward input and state to the model\')\n    state = None\n    error = None\n    error, state = model(input_image, prev_error=error, state=state)\n\n    print(\'The error has size\', list(error.data.size()))\n    print(\'The state has size\', list(state[0].data.size()))\n\n\ndef _test_two_layer_model():\n    print(\'\\nCreate the input image\')\n    input_image = Variable(torch.rand(1, 3, 8, 12))\n\n    print(\'Input has size\', list(input_image.data.size()))\n\n    error_init_size_list = ((1, 6, 8, 12), (1, 32, 4, 6))\n    print(\'The error initialisation sizes are\', *error_init_size_list)\n\n    print(\'Define a 2 layer Prednet\')\n    model = _BuildTwoLayerModel(error_init_size_list)\n\n    print(\'Forward input and state to the model\')\n    state = [None] * 2\n    error = [None] * 2\n    error, state = model(input_image, error=error, state=state)\n\n    for layer in range(0, 2):\n        print(\'Layer\', layer + 1, \'error has size\', list(error[layer].data.size()))\n        print(\'Layer\', layer + 1, \'state has size\', list(state[layer][0].data.size()))\n\n\ndef _test_L_layer_model():\n\n    max_number_of_layers = 5\n    for L in range(0, max_number_of_layers):\n        print(\'\\n---------- Test\', str(L + 1), \'layer network ----------\')\n\n        print(\'Create the input image\')\n        input_image = Variable(torch.rand(1, 3, 4 * 2 ** L, 6 * 2 ** L))\n\n        print(\'Input has size\', list(input_image.data.size()))\n\n        error_init_size_list = tuple(\n            (1, ERR_LAYER_SIZE[l], 4 * 2 ** (L-l), 6 * 2 ** (L-l)) for l in range(0, L + 1)\n        )\n        print(\'The error initialisation sizes are\', *error_init_size_list)\n\n        print(\'Define a\', str(L + 1), \'layer Prednet\')\n        model = PrednetModel(error_init_size_list)\n\n        print(\'Forward input and state to the model\')\n        state = [None] * (L + 1)\n        error = [None] * (L + 1)\n        error, state = model(input_image, error=error, state=state)\n\n        for layer in range(0, L + 1):\n            print(\'Layer\', layer + 1, \'error has size\', list(error[layer].data.size()))\n            print(\'Layer\', layer + 1, \'state has size\', list(state[layer][0].data.size()))\n\n\ndef _test_training():\n    number_of_layers = 3\n    T = 6  # sequence length\n    max_epoch = 10  # number of epochs\n    lr = 1e-1       # learning rate\n\n    # set manual seed\n    torch.manual_seed(0)\n\n    L = number_of_layers - 1\n    print(\'\\n---------- Train a\', str(L + 1), \'layer network ----------\')\n    print(\'Create the input image and target sequences\')\n    input_sequence = Variable(torch.rand(T, 1, 3, 4 * 2 ** L, 6 * 2 ** L))\n    print(\'Input has size\', list(input_sequence.data.size()))\n\n    error_init_size_list = tuple(\n        (1, ERR_LAYER_SIZE[l], 4 * 2 ** (L - l), 6 * 2 ** (L - l)) for l in range(0, L + 1)\n    )\n    print(\'The error initialisation sizes are\', *error_init_size_list)\n    target_sequence = Variable(torch.zeros(T, *error_init_size_list[0]))\n\n    print(\'Define a\', str(L + 1), \'layer Prednet\')\n    model = PrednetModel(error_init_size_list)\n\n    print(\'Create a MSE criterion\')\n    loss_fn = nn.MSELoss()\n\n    print(\'Run for\', max_epoch, \'iterations\')\n    for epoch in range(0, max_epoch):\n        state = [None] * (L + 1)\n        error = [None] * (L + 1)\n        loss = 0\n        for t in range(0, T):\n            error, state = model(input_sequence[t], error, state)\n            loss += loss_fn(error[0], target_sequence[t])\n\n        print(\' > Epoch {:2d} loss: {:.3f}\'.format((epoch + 1), loss.data[0]))\n\n        # zero grad parameters\n        model.zero_grad()\n\n        # compute new grad parameters through time!\n        loss.backward()\n\n        # learning_rate step against the gradient\n        for p in model.parameters():\n            p.data.sub_(p.grad.data * lr)\n\n\ndef _main():\n    _test_one_layer_model()\n    _test_two_layer_model()\n    _test_L_layer_model()\n    _test_training()\n\n\nif __name__ == \'__main__\':\n    _main()\n\n\n__author__ = ""Alfredo Canziani""\n__credits__ = [""Alfredo Canziani""]\n__maintainer__ = ""Alfredo Canziani""\n__email__ = ""alfredo.canziani@gmail.com""\n__status__ = ""Prototype""  # ""Prototype"", ""Development"", or ""Production""\n__date__ = ""Feb 17""\n'"
model/RG.py,2,"b'from torch import nn\n\n\nclass RG(nn.Module):\n    """"""Recurrent Generative Module""""""\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n        """""" Initialise RG Module (parameters as nn.ConvTranspose2d)""""""\n        super().__init__()\n        self.from_input = nn.ConvTranspose2d(\n            in_channels=in_channels, out_channels=out_channels,\n            kernel_size=kernel_size, stride=stride, padding=padding\n        )\n        self.from_state = nn.Conv2d(\n            in_channels=out_channels, out_channels=out_channels,\n            kernel_size=kernel_size, padding=padding, bias=False\n        )\n\n    def forward(self, x, state):\n        """"""\n        Calling signature\n\n        :param x: (input, output_size)\n        :type x: tuple\n        :param state: previous output\n        :type state: torch.Tensor\n        :return: current state\n        :rtype: torch.Tensor\n        """"""\n        x = self.from_input(*x)  # the very first x is a tuple (input, expected_output_size)\n        if state: x += self.from_state(state)\n        return x\n'"
notebook/plot_conf.py,0,"b'# matplotlib and stuff\nimport matplotlib.pyplot as plt\n\n\ndef plt_style(c=\'k\'):\n    """"""\n    Set plotting style for bright (``c = \'w\'``) or dark (``c = \'k\'``) backgrounds\n\n    :param c: colour, can be set to ``\'w\'`` or ``\'k\'`` (which is the default)\n    :type c: str\n    """"""\n    import matplotlib as mpl\n    from matplotlib import rc\n\n    # Reset previous configuration\n    mpl.rcParams.update(mpl.rcParamsDefault)\n    # %matplotlib inline  # not from script\n    get_ipython().run_line_magic(\'matplotlib\', \'inline\')\n\n    # configuration for bright background\n    if c == \'w\':\n        plt.style.use(\'bmh\')\n\n    # configurations for dark background\n    if c == \'k\':\n        # noinspection PyTypeChecker\n        plt.style.use([\'dark_background\', \'bmh\'])\n\n    # remove background colour, set figure size\n    rc(\'figure\', figsize=(16, 8), max_open_warning=False)\n    rc(\'axes\', facecolor=\'none\')\n\nplt_style()\n'"
utils/image_plot.py,6,"b'import torch  # if torch is not imported BEFORE pyplot you get a FUCKING segmentation fault\nfrom matplotlib import pyplot as plt\nfrom os.path import isdir, join\nfrom os import mkdir\n\n\ndef _hist_show(a, k):\n    a = _to_view(a)\n    plt.subplot(2, 3, k)\n    plt.hist(a.reshape(-1), 50)\n    plt.grid(\'on\')\n    plt.gca().axes.get_yaxis().set_visible(False)\n\n\ndef show_four(x, next_x, x_hat, fig):\n    """"""\n    Saves/overwrites a PDF named fig.pdf with x, next_x, x_hat histogram and x_hat\n\n    :param x: x[t]\n    :type x: torch.FloatTensor\n    :param next_x: x[t + 1]\n    :type next_x: torch.FloatTensor\n    :param x_hat: ~x[t + 1]\n    :type x_hat: torch.FloatTensor\n    :param fig: figure number\n    :type fig: int\n    :return: nothing\n    :rtype: None\n    """"""\n    f = plt.figure(fig)\n    plt.clf()\n    _sub(x, 1)\n    _sub(next_x, 4)\n    dif = next_x - x\n    _sub(dif, 2)\n    _hist_show(dif, 3)\n    _sub(x_hat, 5)\n    _hist_show(x_hat, 6)\n    plt.subplots_adjust(left=0.01, bottom=0.06, right=.99, top=1, wspace=0, hspace=.12)\n    f.savefig(str(fig) + \'.pdf\')\n\n\n# Setup output folder for figures collection\ndef _show_ten_setup(pdf_path):\n    if isdir(pdf_path):\n        print(\'Folder ""{}"" already existent. Exiting.\'.format(pdf_path))\n        exit()\n    mkdir(pdf_path)\n\n\ndef show_ten(x, x_hat, pdf_path=\'PDFs\'):\n    """"""\n    First two rows 10 ~x[t + 1], second two rows 10 x[t]\n\n    :param x: x[t]\n    :type x: torch.FloatTensor\n    :param x_hat: ~x[t + 1]\n    :type x_hat: torch.FloatTensor\n    :param pdf_path: saving path\n    :type pdf_path: str\n    :return: nothing\n    :rtype: None\n    """"""\n    if show_ten.c == 0 and pdf_path: _show_ten_setup(pdf_path)\n    if show_ten.c % 10 == 0: show_ten.f = plt.figure()\n    plt.figure(show_ten.f.number)\n    plt.subplot(4, 5, 1 + show_ten.c % 10)\n    _img_show(x_hat, y0=-.16, s=8)\n    plt.subplot(4, 5, 11 + show_ten.c % 10)\n    _img_show(x, y0=-.16, s=8)\n    show_ten.c += 1\n    plt.subplots_adjust(left=0, bottom=0.02, right=1, top=1, wspace=0, hspace=.12)\n    if show_ten.c % 10 == 0: show_ten.f.savefig(join(pdf_path, str(show_ten.c // 10) + \'_10.pdf\'))\nshow_ten.c = 0\n\n\ndef _img_show(a, y0=-.13, s=12):\n    a = _to_view(a)\n    plt.imshow(a)\n    plt.title(\'<{:.2f}> [{:.2f}, {:.2f}]\'.format(a.mean(), a.min(), a.max()), y=y0, fontsize=s)\n    plt.axis(\'off\')\n\n\ndef _sub(a, k):\n    plt.subplot(2, 3, k)\n    _img_show(a)\n\n\ndef _to_view(a):\n    return a.cpu().numpy().transpose((1, 2, 0))\n\n\ndef _test_4():\n    img = _test_setup()\n    show_four(img, img, img, 1)\n\n\ndef _test_10():\n    img = _test_setup()\n    for i in range(20): show_ten(img, -img, \'\')\n\n\ndef _test_setup():\n    from skimage.data import astronaut\n    from skimage.transform import resize\n    from matplotlib.figure import Figure\n    Figure.savefig = lambda self, _: plt.show()  # patch Figure class to simply display the figure\n    img = torch.from_numpy(resize(astronaut(), (256, 256)).astype(\'f4\').transpose((2, 0, 1)))\n    return img\n\n\nif __name__ == \'__main__\':\n    _test_4()\n    _test_10()\n\n__author__ = ""Alfredo Canziani""\n__credits__ = [""Alfredo Canziani""]\n__maintainer__ = ""Alfredo Canziani""\n__email__ = ""alfredo.canziani@gmail.com""\n__status__ = ""Development""  # ""Prototype"", ""Development"", or ""Production""\n__date__ = ""Mar 17""\n'"
utils/visualise.py,1,"b'from graphviz import Digraph\nfrom torch.autograd import Variable\nimport sys, subprocess\nimport uuid\n\n\ndef make_dot(root):\n    node_attr = dict(style=\'filled\',\n                     shape=\'box\',\n                     align=\'left\',\n                     fontsize=\'12\',\n                     ranksep=\'0.1\',\n                     height=\'0.2\')\n    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=""12,12""))\n    seen = set()\n\n    def add_nodes(var):\n        if var not in seen:\n            if isinstance(var, Variable):\n                value = \'(\' + \', \'.join([\'%d\'% v for v in var.size()]) + \')\'\n                dot.node(str(id(var)), str(value), fillcolor=\'lightblue\')\n            else:\n                dot.node(str(id(var)), str(type(var).__name__))\n            seen.add(var)\n            if hasattr(var, \'next_functions\'):\n                for u in var.next_functions:\n                    dot.edge(str(id(u[0])), str(id(var)))\n                    add_nodes(u[0])\n    add_nodes(root.grad_fn)\n    return dot\n\n\ndef show_graph(root):\n    dot_file_name = \'/tmp/\' + str(uuid.uuid4())\n    make_dot(root).render(dot_file_name)\n    pdf_file_name = dot_file_name + \'.pdf\'\n    if sys.platform == \'darwin\':\n        subprocess.call((\'open\', pdf_file_name))\n    elif sys.platform == \'linux\':\n        subprocess.call((\'xdg-open\', pdf_file_name))\n\n\n__author__ = ""Sergey Zagoruyko and Alfredo Canziani""\n__credits__ = [""Sergey Zagoruyko"", ""Alfredo Canziani""]\n__maintainer__ = ""Alfredo Canziani""\n__email__ = ""alfredo.canziani@gmail.com""\n__status__ = ""Production""  # ""Prototype"", ""Development"", or ""Production""\n__date__ = ""Feb 17""\n'"
