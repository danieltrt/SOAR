file_path,api_count,code
preprocess_sequential.py,0,"b""# loads imagenet and writes it into one massive binary file\n\nimport os\nimport numpy as np\nfrom tensorpack.dataflow import *\n\nif __name__ == '__main__':\n    class BinaryILSVRC12(dataset.ILSVRC12Files):\n        def get_data(self):\n            for fname, label in super(BinaryILSVRC12, self).get_data():\n                with open(fname, 'rb') as f:\n                    jpeg = f.read()\n                jpeg = np.asarray(bytearray(jpeg), dtype='uint8')\n                yield [jpeg, label]\n    imagenet_path = os.environ['IMAGENET']\n\n    for name in ['train', 'val']:\n        ds0 = BinaryILSVRC12(imagenet_path, name)\n        ds1 = PrefetchDataZMQ(ds0, nr_proc=1)\n        dftools.dump_dataflow_to_lmdb(ds1, os.path.join(imagenet_path,'ILSVRC-%s.lmdb'%name))\n"""
setup.py,0,"b'""""""Based on example https://github.com/pypa/sampleproject""""""\n\n# Always prefer setuptools over distutils\nfrom setuptools import setup, find_packages\n# To use a consistent encoding\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nsetup(\n    name=\'pytorch-imagenet-dataloader\',\n\n    # Versions should comply with PEP440.  For a discussion on single-sourcing\n    # the version across setup.py and the project code, see\n    # https://packaging.python.org/en/latest/single_source_version.html\n    version=\'0.0.1\',\n\n    description=\'A faster PyTorch ImageNet loader.\',\n    long_description=long_description,\n\n    # The project\'s main homepage.\n    url=\'https://github.com/gngdb/pytorch-imagenet-dataloader\',\n\n    # Author details\n    author=\'Gavin Gray\',\n    author_email=\'gavingray1729@gmail.com\',\n\n    # Choose your license\n    license=\'MIT\',\n\n    # What does your project relate to?\n    keywords=\'pytorch\',\n\n    # You can just specify the packages manually here if your project is\n    # simple. Or you can use find_packages().\n    packages=find_packages(),\n\n    # Alternatively, if you want to distribute just a my_module.py, uncomment\n    # this:\n    #   py_modules=[""my_module""],\n\n    # List run-time dependencies here.  These will be installed by pip when\n    # your project is installed. For an analysis of ""install_requires"" vs pip\'s\n    # requirements files see:\n    # https://packaging.python.org/en/latest/requirements.html\n    install_requires=[\'lmdb\', \'tqdm\'],\n)\n'"
test_lmdb.py,0,"b""\nimport os\nimport numpy as np\nfrom tensorpack.dataflow import *\nfrom tensorpack import imgaug\n\nif __name__ == '__main__':\n    ds = LMDBData(os.path.join(os.environ['IMAGENET'],'ILSVRC-train.lmdb'), shuffle=False)\n    ds = BatchData(ds, 256, use_list=True)\n    print(dir(ds))\n    print(ds.size())\n    for x in ds.get_data():\n        print(x)\n        assert False\n    TestDataSpeed(ds).start()\n"""
test_random_read.py,0,"b""\nimport os\nimport numpy as np\nfrom tensorpack.dataflow import *\n\nif __name__ == '__main__':\n    imagenet_loc = os.environ['IMAGENET']\n    ds0 = dataset.ILSVRC12(imagenet_loc, 'train', shuffle=True)\n    ds1 = BatchData(ds0, 256, use_list=True)\n    TestDataSpeed(ds1).start()\n\n"""
imagenet_seq/__init__.py,0,b'import data\n'
imagenet_seq/data.py,16,"b'# dataloader respecting the PyTorch conventions, but using tensorpack to load and process\n# includes typical augmentations for ImageNet training\n\nimport os\n\nimport cv2\nimport torch\n\nimport numpy as np\nimport tensorpack.dataflow as td\nfrom tensorpack import imgaug\nfrom tensorpack.dataflow import (AugmentImageComponent, PrefetchDataZMQ,\n                                BatchData, MultiThreadMapData)\n\n#####################################################################################################\n# copied from: https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/imagenet_utils.py #\n#####################################################################################################\nclass GoogleNetResize(imgaug.ImageAugmentor):\n    """"""\n    crop 8%~100% of the original image\n    See `Going Deeper with Convolutions` by Google.\n    """"""\n    def __init__(self, crop_area_fraction=0.08,\n                 aspect_ratio_low=0.75, aspect_ratio_high=1.333,\n                 target_shape=224):\n        self._init(locals())\n\n    def _augment(self, img, _):\n        h, w = img.shape[:2]\n        area = h * w\n        for _ in range(10):\n            targetArea = self.rng.uniform(self.crop_area_fraction, 1.0) * area\n            aspectR = self.rng.uniform(self.aspect_ratio_low, self.aspect_ratio_high)\n            ww = int(np.sqrt(targetArea * aspectR) + 0.5)\n            hh = int(np.sqrt(targetArea / aspectR) + 0.5)\n            if self.rng.uniform() < 0.5:\n                ww, hh = hh, ww\n            if hh <= h and ww <= w:\n                x1 = 0 if w == ww else self.rng.randint(0, w - ww)\n                y1 = 0 if h == hh else self.rng.randint(0, h - hh)\n                out = img[y1:y1 + hh, x1:x1 + ww]\n                out = cv2.resize(out, (self.target_shape, self.target_shape), interpolation=cv2.INTER_CUBIC)\n                return out\n        out = imgaug.ResizeShortestEdge(self.target_shape, interp=cv2.INTER_CUBIC).augment(img)\n        out = imgaug.CenterCrop(self.target_shape).augment(out)\n        return out\n\n\ndef fbresnet_augmentor(isTrain):\n    """"""\n    Augmentor used in fb.resnet.torch, for BGR images in range [0,255].\n    """"""\n    if isTrain:\n        augmentors = [\n            GoogleNetResize(),\n            imgaug.RandomOrderAug(\n                [imgaug.BrightnessScale((0.6, 1.4), clip=False),\n                 imgaug.Contrast((0.6, 1.4), clip=False),\n                 imgaug.Saturation(0.4, rgb=False),\n                 # rgb-bgr conversion for the constants copied from fb.resnet.torch\n                 imgaug.Lighting(0.1,\n                                 eigval=np.asarray(\n                                     [0.2175, 0.0188, 0.0045][::-1]) * 255.0,\n                                 eigvec=np.array(\n                                     [[-0.5675, 0.7192, 0.4009],\n                                      [-0.5808, -0.0045, -0.8140],\n                                      [-0.5836, -0.6948, 0.4203]],\n                                     dtype=\'float32\')[::-1, ::-1]\n                                 )]),\n            imgaug.Flip(horiz=True),\n        ]\n    else:\n        augmentors = [\n            imgaug.ResizeShortestEdge(256, cv2.INTER_CUBIC),\n            imgaug.CenterCrop((224, 224)),\n        ]\n    return augmentors\n#####################################################################################################\n#####################################################################################################\n\n\nnumpy_type_map = {\n    \'float64\': torch.DoubleTensor,\n    \'float32\': torch.FloatTensor,\n    \'float16\': torch.HalfTensor,\n    \'int64\': torch.LongTensor,\n    \'int32\': torch.IntTensor,\n    \'int16\': torch.ShortTensor,\n    \'int8\': torch.CharTensor,\n    \'uint8\': torch.ByteTensor,\n}\n\n\ndef default_collate(batch):\n    ""Puts each data field into a tensor with outer dimension batch size""\n\n    error_msg = ""batch must contain tensors, numbers, dicts or lists; found {}""\n    elem_type = type(batch[0])\n    if torch.is_tensor(batch[0]):\n        out = None\n        if _use_shared_memory:\n            # If we\'re in a background process, concatenate directly into a\n            # shared memory tensor to avoid an extra copy\n            numel = sum([x.numel() for x in batch])\n            storage = batch[0].storage()._new_shared(numel)\n            out = batch[0].new(storage)\n        return torch.stack(batch, 0, out=out)\n    elif elem_type.__module__ == \'numpy\' and elem_type.__name__ != \'str_\' \\\n            and elem_type.__name__ != \'string_\':\n        elem = batch[0]\n        if elem_type.__name__ == \'ndarray\':\n            # array of string classes and object\n            if re.search(\'[SaUO]\', elem.dtype.str) is not None:\n                raise TypeError(error_msg.format(elem.dtype))\n\n            return torch.stack([torch.from_numpy(b) for b in batch], 0)\n        if elem.shape == ():  # scalars\n            py_type = float if elem.dtype.name.startswith(\'float\') else int\n            return numpy_type_map[elem.dtype.name](list(map(py_type, batch)))\n    elif isinstance(batch[0], int):\n        return torch.LongTensor(batch)\n    elif isinstance(batch[0], float):\n        return torch.DoubleTensor(batch)\n    elif isinstance(batch[0], string_classes):\n        return batch\n    elif isinstance(batch[0], collections.Mapping):\n        return {key: default_collate([d[key] for d in batch]) for key in batch[0]}\n    elif isinstance(batch[0], collections.Sequence):\n        transposed = zip(*batch)\n        return [default_collate(samples) for samples in transposed]\n\n    raise TypeError((error_msg.format(type(batch[0]))))\n\n\nclass Loader(object):\n    """"""\n    Data loader. Combines a dataset and a sampler, and provides\n    single- or multi-process iterators over the dataset.\n\n    Arguments:\n        mode (str, required): mode of dataset to operate in, one of [\'train\', \'val\']\n        batch_size (int, optional): how many samples per batch to load\n            (default: 1).\n        shuffle (bool, optional): set to ``True`` to have the data reshuffled\n            at every epoch (default: False).\n        num_workers (int, optional): how many subprocesses to use for data\n            loading. 0 means that the data will be loaded in the main process\n            (default: 0)\n        cache (int, optional): cache size to use when loading data,\n        drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n            if the dataset size is not divisible by the batch size. If ``False`` and\n            the size of dataset is not divisible by the batch size, then the last batch\n            will be smaller. (default: False)\n        cuda (bool, optional): set to ``True`` and the PyTorch tensors will get preloaded\n            to the GPU for you (necessary because this lets us to uint8 conversion on the \n            GPU, which is faster).\n    """"""\n\n    def __init__(self, mode, batch_size=256, shuffle=False, num_workers=25, cache=50000,\n            collate_fn=default_collate,  drop_last=False, cuda=False):\n        # enumerate standard imagenet augmentors\n        imagenet_augmentors = fbresnet_augmentor(mode == \'train\')\n\n        # load the lmdb if we can find it\n        lmdb_loc = os.path.join(os.environ[\'IMAGENET\'],\'ILSVRC-%s.lmdb\'%mode)\n        ds = td.LMDBData(lmdb_loc, shuffle=False)\n        ds = td.LocallyShuffleData(ds, cache)\n        ds = td.PrefetchData(ds, 5000, 1)\n        ds = td.LMDBDataPoint(ds)\n        ds = td.MapDataComponent(ds, lambda x: cv2.imdecode(x, cv2.IMREAD_COLOR), 0)\n        ds = td.AugmentImageComponent(ds, imagenet_augmentors)\n        ds = td.PrefetchDataZMQ(ds, num_workers)\n        self.ds = td.BatchData(ds, batch_size)\n        self.ds.reset_state()\n\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.cuda = cuda\n        #self.drop_last = drop_last\n\n    def __iter__(self):\n        for x, y in self.ds.get_data():\n            if self.cuda:\n                # images come out as uint8, which are faster to copy onto the gpu\n                x = torch.ByteTensor(x).cuda()\n                y = torch.IntTensor(y).cuda()\n                # but once they\'re on the gpu, we\'ll need them in \n                yield uint8_to_float(x), y.long()\n            else:\n                yield uint8_to_float(torch.ByteTensor(x)), torch.IntTensor(y).long()\n\n    def __len__(self):\n        return self.ds.size()\n\ndef uint8_to_float(x):\n    x = x.permute(0,3,1,2) # pytorch is (n,c,w,h)\n    return x.float()/128. - 1.\n\nif __name__ == \'__main__\':\n    from tqdm import tqdm\n    dl = Loader(\'train\', cuda=True)\n    for x in tqdm(dl, total=len(dl)):\n        pass\n'"
