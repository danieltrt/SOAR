file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\n\nwith open(""README.md"") as f:\n    long_description = f.read()\n\nsetup(\n    name=""BindsNET"",\n    version=""0.2.7"",\n    description=""Spiking neural networks for ML in Python"",\n    license=""AGPL-3.0"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",  # This is important!\n    url=""http://github.com/Hananel-Hazan/bindsnet"",\n    author=""Hananel Hazan, Daniel Saunders, Darpan Sanghavi, Hassaan Khan"",\n    author_email=""hananel@hazan.org.il"",\n    packages=find_packages(),\n    zip_safe=False,\n    install_requires=[\n        ""numpy>=1.14.2"",\n        ""torch==1.4.0"",\n        ""torchvision==0.5.0"",\n        ""tensorboardX>=1.7"",\n        ""tqdm>=4.19.9"",\n        ""matplotlib>=2.1.0"",\n        ""gym>=0.10.4"",\n        ""scikit_image>=0.13.1"",\n        ""scikit_learn>=0.19.1"",\n        ""opencv-python>=3.4.0.12"",\n        ""pytest>=3.4.0"",\n        ""scipy>=1.1.0"",\n        ""cython>=0.28.5"",\n        ""pandas>=0.23.4"",\n    ],\n)\n'"
bindsnet/__init__.py,0,"b'from pathlib import Path\n\nfrom . import (\n    utils,\n    network,\n    models,\n    analysis,\n    preprocessing,\n    datasets,\n    encoding,\n    pipeline,\n    learning,\n    evaluation,\n    environment,\n    conversion,\n)\n\nROOT_DIR = Path(__file__).parents[0].parents[0]\n'"
bindsnet/utils.py,11,"b'import math\nimport torch\nimport numpy as np\n\nfrom torch import Tensor\nimport torch.nn.functional as F\nfrom numpy import ndarray\nfrom typing import Tuple, Union\nfrom torch.nn.modules.utils import _pair\n\n\ndef im2col_indices(\n    x: Tensor,\n    kernel_height: int,\n    kernel_width: int,\n    padding: Tuple[int, int] = (0, 0),\n    stride: Tuple[int, int] = (1, 1),\n) -> Tensor:\n    # language=rst\n    """"""\n    im2col is a special case of unfold which is implemented inside of Pytorch.\n\n    :param x: Input image tensor to be reshaped to column-wise format.\n    :param kernel_height: Height of the convolutional kernel in pixels.\n    :param kernel_width: Width of the convolutional kernel in pixels.\n    :param padding: Amount of zero padding on the input image.\n    :param stride: Amount to stride over image by per convolution.\n    :return: Input tensor reshaped to column-wise format.\n    """"""\n    return F.unfold(x, (kernel_height, kernel_width), padding=padding, stride=stride)\n\n\ndef col2im_indices(\n    cols: Tensor,\n    x_shape: Tuple[int, int, int, int],\n    kernel_height: int,\n    kernel_width: int,\n    padding: Tuple[int, int] = (0, 0),\n    stride: Tuple[int, int] = (1, 1),\n) -> Tensor:\n    # language=rst\n    """"""\n    col2im is a special case of fold which is implemented inside of Pytorch.\n\n    :param cols: Image tensor in column-wise format.\n    :param x_shape: Shape of original image tensor.\n    :param kernel_height: Height of the convolutional kernel in pixels.\n    :param kernel_width: Width of the convolutional kernel in pixels.\n    :param padding: Amount of zero padding on the input image.\n    :param stride: Amount to stride over image by per convolution.\n    :return: Image tensor in original image shape.\n    """"""\n    return F.fold(\n        cols, x_shape, (kernel_height, kernel_width), padding=padding, stride=stride\n    )\n\n\ndef get_square_weights(\n    weights: Tensor, n_sqrt: int, side: Union[int, Tuple[int, int]]\n) -> Tensor:\n    # language=rst\n    """"""\n    Return a grid of a number of filters ``sqrt ** 2`` with side lengths ``side``.\n\n    :param weights: Two-dimensional tensor of weights for two-dimensional data.\n    :param n_sqrt: Square root of no. of filters.\n    :param side: Side length(s) of filter.\n    :return: Reshaped weights to square matrix of filters.\n    """"""\n    if isinstance(side, int):\n        side = (side, side)\n\n    square_weights = torch.zeros(side[0] * n_sqrt, side[1] * n_sqrt)\n    for i in range(n_sqrt):\n        for j in range(n_sqrt):\n            n = i * n_sqrt + j\n\n            if not n < weights.size(1):\n                break\n\n            x = i * side[0]\n            y = (j % n_sqrt) * side[1]\n            filter_ = weights[:, n].contiguous().view(*side)\n            square_weights[x : x + side[0], y : y + side[1]] = filter_\n\n    return square_weights\n\n\ndef get_square_assignments(assignments: Tensor, n_sqrt: int) -> Tensor:\n    # language=rst\n    """"""\n    Return a grid of assignments.\n\n    :param assignments: Vector of integers corresponding to class labels.\n    :param n_sqrt: Square root of no. of assignments.\n    :return: Reshaped square matrix of assignments.\n    """"""\n    square_assignments = torch.mul(torch.ones(n_sqrt, n_sqrt), -1.0)\n    for i in range(n_sqrt):\n        for j in range(n_sqrt):\n            n = i * n_sqrt + j\n\n            if not n < assignments.size(0):\n                break\n\n            square_assignments[\n                i : (i + 1), (j % n_sqrt) : ((j % n_sqrt) + 1)\n            ] = assignments[n]\n\n    return square_assignments\n\n\ndef reshape_locally_connected_weights(\n    w: Tensor,\n    n_filters: int,\n    kernel_size: Union[int, Tuple[int, int]],\n    conv_size: Union[int, Tuple[int, int]],\n    locations: Tensor,\n    input_sqrt: Union[int, Tuple[int, int]],\n) -> Tensor:\n    # language=rst\n    """"""\n    Get the weights from a locally connected layer and reshape them to be two-dimensional and square.\n\n    :param w: Weights from a locally connected layer.\n    :param n_filters: No. of neuron filters.\n    :param kernel_size: Side length(s) of convolutional kernel.\n    :param conv_size: Side length(s) of convolution population.\n    :param locations: Binary mask indicating receptive fields of convolution population neurons.\n    :param input_sqrt: Sides length(s) of input neurons.\n    :return: Locally connected weights reshaped as a collection of spatially ordered square grids.\n    """"""\n    kernel_size = _pair(kernel_size)\n    conv_size = _pair(conv_size)\n    input_sqrt = _pair(input_sqrt)\n\n    k1, k2 = kernel_size\n    c1, c2 = conv_size\n    i1, i2 = input_sqrt\n    c1sqrt, c2sqrt = int(math.ceil(math.sqrt(c1))), int(math.ceil(math.sqrt(c2)))\n    fs = int(math.ceil(math.sqrt(n_filters)))\n\n    w_ = torch.zeros((n_filters * k1, k2 * c1 * c2))\n\n    for n1 in range(c1):\n        for n2 in range(c2):\n            for feature in range(n_filters):\n                n = n1 * c2 + n2\n                filter_ = w[\n                    locations[:, n],\n                    feature * (c1 * c2) + (n // c2sqrt) * c2sqrt + (n % c2sqrt),\n                ].view(k1, k2)\n                w_[feature * k1 : (feature + 1) * k1, n * k2 : (n + 1) * k2] = filter_\n\n    if c1 == 1 and c2 == 1:\n        square = torch.zeros((i1 * fs, i2 * fs))\n\n        for n in range(n_filters):\n            square[\n                (n // fs) * i1 : ((n // fs) + 1) * i2,\n                (n % fs) * i2 : ((n % fs) + 1) * i2,\n            ] = w_[n * i1 : (n + 1) * i2]\n\n        return square\n    else:\n        square = torch.zeros((k1 * fs * c1, k2 * fs * c2))\n\n        for n1 in range(c1):\n            for n2 in range(c2):\n                for f1 in range(fs):\n                    for f2 in range(fs):\n                        if f1 * fs + f2 < n_filters:\n                            square[\n                                k1 * (n1 * fs + f1) : k1 * (n1 * fs + f1 + 1),\n                                k2 * (n2 * fs + f2) : k2 * (n2 * fs + f2 + 1),\n                            ] = w_[\n                                (f1 * fs + f2) * k1 : (f1 * fs + f2 + 1) * k1,\n                                (n1 * c2 + n2) * k2 : (n1 * c2 + n2 + 1) * k2,\n                            ]\n\n        return square\n\n\ndef reshape_conv2d_weights(weights: torch.Tensor) -> torch.Tensor:\n    # language=rst\n    """"""\n    Flattens a connection weight matrix of a Conv2dConnection\n\n    :param weights: Weight matrix of Conv2dConnection object.\n    :param wmin: Minimum allowed weight value.\n    :param wmax: Maximum allowed weight value.\n    """"""\n    sqrt1 = int(np.ceil(np.sqrt(weights.size(0))))\n    sqrt2 = int(np.ceil(np.sqrt(weights.size(1))))\n    height, width = weights.size(2), weights.size(3)\n    reshaped = torch.zeros(\n        sqrt1 * sqrt2 * weights.size(2), sqrt1 * sqrt2 * weights.size(3)\n    )\n\n    for i in range(sqrt1):\n        for j in range(sqrt1):\n            for k in range(sqrt2):\n                for l in range(sqrt2):\n                    if i * sqrt1 + j < weights.size(0) and k * sqrt2 + l < weights.size(\n                        1\n                    ):\n                        fltr = weights[i * sqrt1 + j, k * sqrt2 + l].view(height, width)\n                        reshaped[\n                            i * height\n                            + k * height * sqrt1 : (i + 1) * height\n                            + k * height * sqrt1,\n                            (j % sqrt1) * width\n                            + (l % sqrt2) * width * sqrt1 : ((j % sqrt1) + 1) * width\n                            + (l % sqrt2) * width * sqrt1,\n                        ] = fltr\n\n    return reshaped\n'"
bindsnet/analysis/__init__.py,0,"b'from . import plotting, visualization, pipeline_analysis\n'"
bindsnet/analysis/pipeline_analysis.py,15,"b'from abc import ABC, abstractmethod\nfrom typing import Dict, Optional\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom tensorboardX import SummaryWriter\nfrom torchvision.utils import make_grid\n\nfrom .plotting import plot_spikes, plot_voltages, plot_conv2d_weights\nfrom ..utils import reshape_conv2d_weights\n\n\nclass PipelineAnalyzer(ABC):\n    # language=rst\n    """"""\n    Responsible for pipeline analysis. Subclasses maintain state\n    information related to plotting or logging.\n    """"""\n\n    @abstractmethod\n    def finalize_step(self) -> None:\n        # language=rst\n        """"""\n        Flush the output from the current step.\n        """"""\n        pass\n\n    @abstractmethod\n    def plot_obs(self, obs: torch.Tensor, tag: str = ""obs"", step: int = None) -> None:\n        # language=rst\n        """"""\n        Pulls the observation from PyTorch and sets up for Matplotlib\n        plotting.\n\n        :param obs: A 2D array of floats depicting an input image.\n        :param tag: A unique tag to associate the data with.\n        :param step: The step of the pipeline.\n        """"""\n        pass\n\n    @abstractmethod\n    def plot_reward(\n        self,\n        reward_list: list,\n        reward_window: int = None,\n        tag: str = ""reward"",\n        step: int = None,\n    ) -> None:\n        # language=rst\n        """"""\n        Plot the accumulated reward for each episode.\n\n        :param reward_list: The list of recent rewards to be plotted.\n        :param reward_window: The length of the window to compute a moving average over.\n        :param tag: A unique tag to associate the data with.\n        :param step: The step of the pipeline.\n        """"""\n        pass\n\n    @abstractmethod\n    def plot_spikes(\n        self,\n        spike_record: Dict[str, torch.Tensor],\n        tag: str = ""spike"",\n        step: int = None,\n    ) -> None:\n        # language=rst\n        """"""\n        Plots all spike records inside of ``spike_record``. Keeps unique\n        plots for all unique tags that are given.\n\n        :param spike_record: Dictionary of spikes to be rasterized.\n        :param tag: A unique tag to associate the data with.\n        :param step: The step of the pipeline.\n        """"""\n        pass\n\n    @abstractmethod\n    def plot_voltages(\n        self,\n        voltage_record: Dict[str, torch.Tensor],\n        thresholds: Optional[Dict[str, torch.Tensor]] = None,\n        tag: str = ""voltage"",\n        step: int = None,\n    ) -> None:\n        # language=rst\n        """"""\n        Plots all voltage records and given thresholds. Keeps unique\n        plots for all unique tags that are given.\n\n        :param voltage_record: Dictionary of voltages for neurons inside of networks\n                               organized by the layer they correspond to.\n        :param thresholds: Optional dictionary of threshold values for neurons.\n        :param tag: A unique tag to associate the data with.\n        :param step: The step of the pipeline.\n        """"""\n        pass\n\n    @abstractmethod\n    def plot_conv2d_weights(\n        self, weights: torch.Tensor, tag: str = ""conv2d"", step: int = None\n    ) -> None:\n        # language=rst\n        """"""\n        Plot a connection weight matrix of a ``Conv2dConnection``.\n\n        :param weights: Weight matrix of ``Conv2dConnection`` object.\n        :param tag: A unique tag to associate the data with.\n        :param step: The step of the pipeline.\n        """"""\n        pass\n\n\nclass MatplotlibAnalyzer(PipelineAnalyzer):\n    # language=rst\n    """"""\n    Renders output using Matplotlib.\n\n    Matplotlib requires objects to be kept around over the full lifetime\n    of the plots; this is done through ``self.plots``. An interactive session\n    is needed so that we can continue processing and just update the\n    plots.\n    """"""\n\n    def __init__(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Initializes the analyzer.\n\n        Keyword arguments:\n\n        :param str volts_type: Type of plotting for voltages (``""color""`` or ``""line""``).\n        """"""\n        self.volts_type = kwargs.get(""volts_type"", ""color"")\n        plt.ion()\n        self.plots = {}\n\n    def plot_obs(self, obs: torch.Tensor, tag: str = ""obs"", step: int = None) -> None:\n        # language=rst\n        """"""\n        Pulls the observation off of torch and sets up for Matplotlib\n        plotting.\n\n        :param obs: A 2D array of floats depicting an input image.\n        :param tag: A unique tag to associate the data with.\n        :param step: The step of the pipeline.\n        """"""\n        obs = obs.detach().cpu().numpy()\n        obs = np.transpose(obs, (1, 2, 0)).squeeze()\n\n        if tag in self.plots:\n            obs_ax, obs_im = self.plots[tag]\n        else:\n            obs_ax, obs_im = None, None\n\n        if obs_im is None and obs_ax is None:\n            fig, obs_ax = plt.subplots()\n            obs_ax.set_title(""Observation"")\n            obs_ax.set_xticks(())\n            obs_ax.set_yticks(())\n            obs_im = obs_ax.imshow(obs, cmap=""gray"")\n\n            self.plots[tag] = obs_ax, obs_im\n        else:\n            obs_im.set_data(obs)\n\n    def plot_reward(\n        self,\n        reward_list: list,\n        reward_window: int = None,\n        tag: str = ""reward"",\n        step: int = None,\n    ) -> None:\n        # language=rst\n        """"""\n        Plot the accumulated reward for each episode.\n\n        :param reward_list: The list of recent rewards to be plotted.\n        :param reward_window: The length of the window to compute a moving average over.\n        :param tag: A unique tag to associate the data with.\n        :param step: The step of the pipeline.\n        """"""\n        if tag in self.plots:\n            reward_im, reward_ax, reward_plot = self.plots[tag]\n        else:\n            reward_im, reward_ax, reward_plot = None, None, None\n\n        # Compute moving average.\n        if reward_window is not None:\n            # Ensure window size > 0 and < size of reward list.\n            window = max(min(len(reward_list), reward_window), 0)\n\n            # Fastest implementation of moving average.\n            reward_list_ = (\n                pd.Series(reward_list)\n                .rolling(window=window, min_periods=1)\n                .mean()\n                .values\n            )\n        else:\n            reward_list_ = reward_list[:]\n\n        if reward_im is None and reward_ax is None:\n            reward_im, reward_ax = plt.subplots()\n            reward_ax.set_title(""Accumulated reward"")\n            reward_ax.set_xlabel(""Episode"")\n            reward_ax.set_ylabel(""Reward"")\n            (reward_plot,) = reward_ax.plot(reward_list_)\n\n            self.plots[tag] = reward_im, reward_ax, reward_plot\n        else:\n            reward_plot.set_data(range(len(reward_list_)), reward_list_)\n            reward_ax.relim()\n            reward_ax.autoscale_view()\n\n    def plot_spikes(\n        self,\n        spike_record: Dict[str, torch.Tensor],\n        tag: str = ""spike"",\n        step: int = None,\n    ) -> None:\n        # language=rst\n        """"""\n        Plots all spike records inside of ``spike_record``. Keeps unique\n        plots for all unique tags that are given.\n\n        :param spike_record: Dictionary of spikes to be rasterized.\n        :param tag: A unique tag to associate the data with.\n        :param step: The step of the pipeline.\n        """"""\n        if tag not in self.plots:\n            self.plots[tag] = plot_spikes(spike_record)\n        else:\n            s_im, s_ax = self.plots[tag]\n            self.plots[tag] = plot_spikes(spike_record, ims=s_im, axes=s_ax)\n\n    def plot_voltages(\n        self,\n        voltage_record: Dict[str, torch.Tensor],\n        thresholds: Optional[Dict[str, torch.Tensor]] = None,\n        tag: str = ""voltage"",\n        step: int = None,\n    ) -> None:\n        # language=rst\n        """"""\n        Plots all voltage records and given thresholds. Keeps unique\n        plots for all unique tags that are given.\n\n        :param voltage_record: Dictionary of voltages for neurons inside of networks\n                               organized by the layer they correspond to.\n        :param thresholds: Optional dictionary of threshold values for neurons.\n        :param tag: A unique tag to associate the data with.\n        :param step: The step of the pipeline.\n        """"""\n        if tag not in self.plots:\n            self.plots[tag] = plot_voltages(\n                voltage_record, plot_type=self.volts_type, thresholds=thresholds\n            )\n        else:\n            v_im, v_ax = self.plots[tag]\n            self.plots[tag] = plot_voltages(\n                voltage_record,\n                ims=v_im,\n                axes=v_ax,\n                plot_type=self.volts_type,\n                thresholds=thresholds,\n            )\n\n    def plot_conv2d_weights(\n        self, weights: torch.Tensor, tag: str = ""conv2d"", step: int = None\n    ) -> None:\n        # language=rst\n        """"""\n        Plot a connection weight matrix of a ``Conv2dConnection``.\n\n        :param weights: Weight matrix of ``Conv2dConnection`` object.\n        :param tag: A unique tag to associate the data with.\n        :param step: The step of the pipeline.\n        """"""\n        wmin = weights.min().item()\n        wmax = weights.max().item()\n\n        if tag not in self.plots:\n            self.plots[tag] = plot_conv2d_weights(weights, wmin, wmax)\n        else:\n            im = self.plots[tag]\n            plot_conv2d_weights(weights, wmin, wmax, im=im)\n\n    def finalize_step(self) -> None:\n        # language=rst\n        """"""\n        Flush the output from the current step\n        """"""\n        plt.draw()\n        plt.pause(1e-8)\n        plt.show()\n\n\nclass TensorboardAnalyzer(PipelineAnalyzer):\n    def __init__(self, summary_directory: str = ""./logs""):\n        # language=rst\n        """"""\n        Initializes the analyzer.\n\n        :param summary_directory: Directory to save log files.\n        """"""\n        self.writer = SummaryWriter(summary_directory)\n\n    def finalize_step(self) -> None:\n        # language=rst\n        """"""\n        No-op for ``TensorboardAnalyzer``.\n        """"""\n        pass\n\n    def plot_obs(self, obs: torch.Tensor, tag: str = ""obs"", step: int = None) -> None:\n        # language=rst\n        """"""\n        Pulls the observation off of torch and sets up for Matplotlib\n        plotting.\n\n        :param obs: A 2D array of floats depicting an input image.\n        :param tag: A unique tag to associate the data with.\n        :param step: The step of the pipeline.\n        """"""\n        obs_grid = make_grid(obs.float(), nrow=4, normalize=True)\n        self.writer.add_image(tag, obs_grid, step)\n\n    def plot_reward(\n        self,\n        reward_list: list,\n        reward_window: int = None,\n        tag: str = ""reward"",\n        step: int = None,\n    ) -> None:\n        # language=rst\n        """"""\n        Plot the accumulated reward for each episode.\n\n        :param reward_list: The list of recent rewards to be plotted.\n        :param reward_window: The length of the window to compute a moving average over.\n        :param tag: A unique tag to associate the data with.\n        :param step: The step of the pipeline.\n        """"""\n        self.writer.add_scalar(tag, reward_list[-1], step)\n\n    def plot_spikes(\n        self,\n        spike_record: Dict[str, torch.Tensor],\n        tag: str = ""spike"",\n        step: int = None,\n    ) -> None:\n        # language=rst\n        """"""\n        Plots all spike records inside of ``spike_record``. Keeps unique\n        plots for all unique tags that are given.\n\n        :param spike_record: Dictionary of spikes to be rasterized.\n        :param tag: A unique tag to associate the data with.\n        :param step: The step of the pipeline.\n        """"""\n        for k, spikes in spike_record.items():\n            # shuffle spikes into 1x1x#NueronsxT\n            spikes = spikes.view(1, 1, -1, spikes.shape[-1]).float()\n            spike_grid_img = make_grid(spikes, nrow=1, pad_value=0.5)\n\n            self.writer.add_image(tag + ""_"" + str(k), spike_grid_img, step)\n\n    def plot_voltages(\n        self,\n        voltage_record: Dict[str, torch.Tensor],\n        thresholds: Optional[Dict[str, torch.Tensor]] = None,\n        tag: str = ""voltage"",\n        step: int = None,\n    ) -> None:\n        # language=rst\n        """"""\n        Plots all voltage records and given thresholds. Keeps unique\n        plots for all unique tags that are given.\n\n        :param voltage_record: Dictionary of voltages for neurons inside of networks\n                               organized by the layer they correspond to.\n        :param thresholds: Optional dictionary of threshold values for neurons.\n        :param tag: A unique tag to associate the data with.\n        :param step: The step of the pipeline.\n        """"""\n        for k, v in voltage_record.items():\n            # Shuffle voltages into 1x1x#neuronsxT\n            v = v.view(1, 1, -1, v.shape[-1])\n            voltage_grid_img = make_grid(v, nrow=1, pad_value=0)\n\n            self.writer.add_image(tag + ""_"" + str(k), voltage_grid_img, step)\n\n    def plot_conv2d_weights(\n        self, weights: torch.Tensor, tag: str = ""conv2d"", step: int = None\n    ) -> None:\n        # language=rst\n        """"""\n        Plot a connection weight matrix of a ``Conv2dConnection``.\n\n        :param weights: Weight matrix of ``Conv2dConnection`` object.\n        :param tag: A unique tag to associate the data with.\n        :param step: The step of the pipeline.\n        """"""\n        reshaped = reshape_conv2d_weights(weights).unsqueeze(0)\n\n        reshaped -= reshaped.min()\n        reshaped /= reshaped.max()\n\n        self.writer.add_image(tag, reshaped, step)\n'"
bindsnet/analysis/plotting.py,15,"b'import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom matplotlib.axes import Axes\nfrom matplotlib.image import AxesImage\nfrom torch.nn.modules.utils import _pair\nfrom matplotlib.collections import PathCollection\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom typing import Tuple, List, Optional, Sized, Dict, Union\n\nfrom ..utils import reshape_locally_connected_weights, reshape_conv2d_weights\n\nplt.ion()\n\n\ndef plot_input(\n    image: torch.Tensor,\n    inpt: torch.Tensor,\n    label: Optional[int] = None,\n    axes: List[Axes] = None,\n    ims: List[AxesImage] = None,\n    figsize: Tuple[int, int] = (8, 4),\n) -> Tuple[List[Axes], List[AxesImage]]:\n    # language=rst\n    """"""\n    Plots a two-dimensional image and its corresponding spike-train representation.\n\n    :param image: A 2D array of floats depicting an input image.\n    :param inpt: A 2D array of floats depicting an image\'s spike-train encoding.\n    :param label: Class label of the input data.\n    :param axes: Used for re-drawing the input plots.\n    :param ims: Used for re-drawing the input plots.\n    :param figsize: Horizontal, vertical figure size in inches.\n    :return: Tuple of ``(axes, ims)`` used for re-drawing the input plots.\n    """"""\n    local_image = image.detach().clone().cpu().numpy()\n    local_inpy = inpt.detach().clone().cpu().numpy()\n\n    if axes is None:\n        fig, axes = plt.subplots(1, 2, figsize=figsize)\n        ims = (\n            axes[0].imshow(local_image, cmap=""binary""),\n            axes[1].imshow(local_inpy, cmap=""binary""),\n        )\n\n        if label is None:\n            axes[0].set_title(""Current image"")\n        else:\n            axes[0].set_title(""Current image (label = %d)"" % label)\n\n        axes[1].set_title(""Reconstruction"")\n\n        for ax in axes:\n            ax.set_xticks(())\n            ax.set_yticks(())\n\n        fig.tight_layout()\n    else:\n        if label is not None:\n            axes[0].set_title(""Current image (label = %d)"" % label)\n\n        ims[0].set_data(local_image)\n        ims[1].set_data(local_inpy)\n\n    return axes, ims\n\n\ndef plot_spikes(\n    spikes: Dict[str, torch.Tensor],\n    time: Optional[Tuple[int, int]] = None,\n    n_neurons: Optional[Dict[str, Tuple[int, int]]] = None,\n    ims: Optional[List[PathCollection]] = None,\n    axes: Optional[Union[Axes, List[Axes]]] = None,\n    figsize: Tuple[float, float] = (8.0, 4.5),\n) -> Tuple[List[AxesImage], List[Axes]]:\n    # language=rst\n    """"""\n    Plot spikes for any group(s) of neurons.\n\n    :param spikes: Mapping from layer names to spiking data. Spike data has shape\n        ``[time, n_1, ..., n_k]``, where ``[n_1, ..., n_k]`` is the shape of the\n        recorded layer.\n    :param time: Plot spiking activity of neurons in the given time range. Default is\n        entire simulation time.\n    :param n_neurons: Plot spiking activity of neurons in the given range of neurons.\n        Default is all neurons.\n    :param ims: Used for re-drawing the plots.\n    :param axes: Used for re-drawing the plots.\n    :param figsize: Horizontal, vertical figure size in inches.\n    :return: ``ims, axes``: Used for re-drawing the plots.\n    """"""\n    n_subplots = len(spikes.keys())\n    if n_neurons is None:\n        n_neurons = {}\n\n    spikes = {k: v.view(v.size(0), -1) for (k, v) in spikes.items()}\n    if time is None:\n        # Set it for entire duration\n        for key in spikes.keys():\n            time = (0, spikes[key].shape[0])\n            break\n\n    # Use all neurons if no argument provided.\n    for key, val in spikes.items():\n        if key not in n_neurons.keys():\n            n_neurons[key] = (0, val.shape[1])\n\n    if ims is None:\n        fig, axes = plt.subplots(n_subplots, 1, figsize=figsize)\n        if n_subplots == 1:\n            axes = [axes]\n\n        ims = []\n        for i, datum in enumerate(spikes.items()):\n            spikes = (\n                datum[1][\n                    time[0] : time[1], n_neurons[datum[0]][0] : n_neurons[datum[0]][1]\n                ]\n                .detach()\n                .clone()\n                .cpu()\n                .numpy()\n            )\n            ims.append(\n                axes[i].scatter(\n                    x=np.array(spikes.nonzero()).T[:, 0],\n                    y=np.array(spikes.nonzero()).T[:, 1],\n                    s=1,\n                )\n            )\n            args = (\n                datum[0],\n                n_neurons[datum[0]][0],\n                n_neurons[datum[0]][1],\n                time[0],\n                time[1],\n            )\n            axes[i].set_title(\n                ""%s spikes for neurons (%d - %d) from t = %d to %d "" % args\n            )\n        for ax in axes:\n            ax.set_aspect(""auto"")\n\n        plt.setp(\n            axes, xticks=[], yticks=[], xlabel=""Simulation time"", ylabel=""Neuron index""\n        )\n        plt.tight_layout()\n    else:\n        for i, datum in enumerate(spikes.items()):\n            spikes = (\n                datum[1][\n                    time[0] : time[1], n_neurons[datum[0]][0] : n_neurons[datum[0]][1]\n                ]\n                .detach()\n                .clone()\n                .cpu()\n                .numpy()\n            )\n            ims[i].set_offsets(np.array(spikes.nonzero()).T)\n            args = (\n                datum[0],\n                n_neurons[datum[0]][0],\n                n_neurons[datum[0]][1],\n                time[0],\n                time[1],\n            )\n            axes[i].set_title(\n                ""%s spikes for neurons (%d - %d) from t = %d to %d "" % args\n            )\n\n    plt.draw()\n\n    return ims, axes\n\n\ndef plot_weights(\n    weights: torch.Tensor,\n    wmin: Optional[float] = 0,\n    wmax: Optional[float] = 1,\n    im: Optional[AxesImage] = None,\n    figsize: Tuple[int, int] = (5, 5),\n    cmap: str = ""hot_r"",\n    save: Optional[str] = None,\n) -> AxesImage:\n    # language=rst\n    """"""\n    Plot a connection weight matrix.\n\n    :param weights: Weight matrix of ``Connection`` object.\n    :param wmin: Minimum allowed weight value.\n    :param wmax: Maximum allowed weight value.\n    :param im: Used for re-drawing the weights plot.\n    :param figsize: Horizontal, vertical figure size in inches.\n    :param cmap: Matplotlib colormap.\n    :param save: file name to save fig, if None = not saving fig.\n    :return: ``AxesImage`` for re-drawing the weights plot.\n    """"""\n    local_weights = weights.detach().clone().cpu().numpy()\n    if save is not None:\n        plt.ioff()\n\n        fig, ax = plt.subplots(figsize=figsize)\n\n        im = ax.imshow(local_weights, cmap=cmap, vmin=wmin, vmax=wmax)\n        div = make_axes_locatable(ax)\n        cax = div.append_axes(""right"", size=""5%"", pad=0.05)\n\n        ax.set_xticks(())\n        ax.set_yticks(())\n        ax.set_aspect(""auto"")\n\n        plt.colorbar(im, cax=cax)\n        fig.tight_layout()\n\n        a = save.split(""."")\n        if len(a) == 2:\n            save = a[0] + "".1."" + a[1]\n        else:\n            a[1] = ""."" + str(1 + int(a[1])) + "".png""\n            save = a[0] + a[1]\n\n        plt.savefig(save, bbox_inches=""tight"")\n\n        plt.close(fig)\n        plt.ion()\n        return im, save\n    else:\n        if not im:\n            fig, ax = plt.subplots(figsize=figsize)\n\n            im = ax.imshow(local_weights, cmap=cmap, vmin=wmin, vmax=wmax)\n            div = make_axes_locatable(ax)\n            cax = div.append_axes(""right"", size=""5%"", pad=0.05)\n\n            ax.set_xticks(())\n            ax.set_yticks(())\n            ax.set_aspect(""auto"")\n\n            plt.colorbar(im, cax=cax)\n            fig.tight_layout()\n        else:\n            im.set_data(local_weights)\n\n        return im\n\n\ndef plot_conv2d_weights(\n    weights: torch.Tensor,\n    wmin: float = 0.0,\n    wmax: float = 1.0,\n    im: Optional[AxesImage] = None,\n    figsize: Tuple[int, int] = (5, 5),\n    cmap: str = ""hot_r"",\n) -> AxesImage:\n    # language=rst\n    """"""\n    Plot a connection weight matrix of a Conv2dConnection.\n\n    :param weights: Weight matrix of Conv2dConnection object.\n    :param wmin: Minimum allowed weight value.\n    :param wmax: Maximum allowed weight value.\n    :param im: Used for re-drawing the weights plot.\n    :param figsize: Horizontal, vertical figure size in inches.\n    :param cmap: Matplotlib colormap.\n    :return: Used for re-drawing the weights plot.\n    """"""\n\n    sqrt1 = int(np.ceil(np.sqrt(weights.size(0))))\n    sqrt2 = int(np.ceil(np.sqrt(weights.size(1))))\n    height, width = weights.size(2), weights.size(3)\n    reshaped = reshape_conv2d_weights(weights)\n\n    if not im:\n        fig, ax = plt.subplots(figsize=figsize)\n        im = ax.imshow(reshaped, cmap=cmap, vmin=wmin, vmax=wmax)\n        div = make_axes_locatable(ax)\n        cax = div.append_axes(""right"", size=""5%"", pad=0.05)\n\n        for i in range(height, sqrt1 * sqrt2 * height, height):\n            ax.axhline(i - 0.5, color=""g"", linestyle=""--"")\n            if i % sqrt1 == 0:\n                ax.axhline(i - 0.5, color=""g"", linestyle=""-"")\n\n        for i in range(width, sqrt1 * sqrt2 * width, width):\n            ax.axvline(i - 0.5, color=""g"", linestyle=""--"")\n            if i % sqrt1 == 0:\n                ax.axvline(i - 0.5, color=""g"", linestyle=""-"")\n\n        ax.set_xticks(())\n        ax.set_yticks(())\n        ax.set_aspect(""auto"")\n\n        plt.colorbar(im, cax=cax)\n        fig.tight_layout()\n    else:\n        im.set_data(reshaped)\n\n    return im\n\n\ndef plot_locally_connected_weights(\n    weights: torch.Tensor,\n    n_filters: int,\n    kernel_size: Union[int, Tuple[int, int]],\n    conv_size: Union[int, Tuple[int, int]],\n    locations: torch.Tensor,\n    input_sqrt: Union[int, Tuple[int, int]],\n    wmin: float = 0.0,\n    wmax: float = 1.0,\n    im: Optional[AxesImage] = None,\n    lines: bool = True,\n    figsize: Tuple[int, int] = (5, 5),\n    cmap: str = ""hot_r"",\n) -> AxesImage:\n    # language=rst\n    """"""\n    Plot a connection weight matrix of a :code:`Connection` with `locally connected\n    structure <http://yann.lecun.com/exdb/publis/pdf/gregor-nips-11.pdf>_.\n\n    :param weights: Weight matrix of Conv2dConnection object.\n    :param n_filters: No. of convolution kernels in use.\n    :param kernel_size: Side length(s) of 2D convolution kernels.\n    :param conv_size: Side length(s) of 2D convolution population.\n    :param locations: Indices of input receptive fields for convolution population\n        neurons.\n    :param input_sqrt: Side length(s) of 2D input data.\n    :param wmin: Minimum allowed weight value.\n    :param wmax: Maximum allowed weight value.\n    :param im: Used for re-drawing the weights plot.\n    :param lines: Whether or not to draw horizontal and vertical lines separating input\n        regions.\n    :param figsize: Horizontal, vertical figure size in inches.\n    :param cmap: Matplotlib colormap.\n    :return: Used for re-drawing the weights plot.\n    """"""\n    kernel_size = _pair(kernel_size)\n    conv_size = _pair(conv_size)\n    input_sqrt = _pair(input_sqrt)\n\n    reshaped = reshape_locally_connected_weights(\n        weights, n_filters, kernel_size, conv_size, locations, input_sqrt\n    )\n    n_sqrt = int(np.ceil(np.sqrt(n_filters)))\n\n    if not im:\n        fig, ax = plt.subplots(figsize=figsize)\n\n        im = ax.imshow(reshaped.cpu(), cmap=cmap, vmin=wmin, vmax=wmax)\n        div = make_axes_locatable(ax)\n        cax = div.append_axes(""right"", size=""5%"", pad=0.05)\n\n        if lines:\n            for i in range(\n                n_sqrt * kernel_size[0],\n                n_sqrt * conv_size[0] * kernel_size[0],\n                n_sqrt * kernel_size[0],\n            ):\n                ax.axhline(i - 0.5, color=""g"", linestyle=""--"")\n\n            for i in range(\n                n_sqrt * kernel_size[1],\n                n_sqrt * conv_size[1] * kernel_size[1],\n                n_sqrt * kernel_size[1],\n            ):\n                ax.axvline(i - 0.5, color=""g"", linestyle=""--"")\n\n        ax.set_xticks(())\n        ax.set_yticks(())\n        ax.set_aspect(""auto"")\n\n        plt.colorbar(im, cax=cax)\n        fig.tight_layout()\n    else:\n        im.set_data(reshaped)\n\n    return im\n\n\ndef plot_assignments(\n    assignments: torch.Tensor,\n    im: Optional[AxesImage] = None,\n    figsize: Tuple[int, int] = (5, 5),\n    classes: Optional[Sized] = None,\n    save: Optional[str] = None,\n) -> AxesImage:\n    # language=rst\n    """"""\n    Plot the two-dimensional neuron assignments.\n\n    :param assignments: Vector of neuron label assignments.\n    :param im: Used for re-drawing the assignments plot.\n    :param figsize: Horizontal, vertical figure size in inches.\n    :param classes: Iterable of labels for colorbar ticks corresponding to data labels.\n    :param save: file name to save fig, if None = not saving fig.\n    :return: Used for re-drawing the assigments plot.\n    """"""\n    locals_assignments = assignments.detach().clone().cpu().numpy()\n\n    if save is not None:\n        plt.ioff()\n\n        fig, ax = plt.subplots(figsize=figsize)\n        ax.set_title(""Categorical assignments"")\n\n        if classes is None:\n            color = plt.get_cmap(""RdBu"", 11)\n            im = ax.matshow(locals_assignments, cmap=color, vmin=-1.5, vmax=9.5)\n        else:\n            color = plt.get_cmap(""RdBu"", len(classes) + 1)\n            im = ax.matshow(\n                locals_assignments, cmap=color, vmin=-1.5, vmax=len(classes) - 0.5\n            )\n\n        div = make_axes_locatable(ax)\n        cax = div.append_axes(""right"", size=""5%"", pad=0.05)\n\n        if classes is None:\n            cbar = plt.colorbar(im, cax=cax, ticks=list(range(-1, 11)))\n            cbar.ax.set_yticklabels([""none""] + list(range(10)))\n        else:\n            cbar = plt.colorbar(im, cax=cax, ticks=np.arange(-1, len(classes)))\n            cbar.ax.set_yticklabels([""none""] + list(classes))\n\n        ax.set_xticks(())\n        ax.set_yticks(())\n        # fig.tight_layout()\n\n        fig.savefig(save, bbox_inches=""tight"")\n        plt.close()\n\n        plt.ion()\n        return im\n    else:\n        if not im:\n            fig, ax = plt.subplots(figsize=figsize)\n            ax.set_title(""Categorical assignments"")\n\n            if classes is None:\n                color = plt.get_cmap(""RdBu"", 11)\n                im = ax.matshow(locals_assignments, cmap=color, vmin=-1.5, vmax=9.5)\n            else:\n                color = plt.get_cmap(""RdBu"", len(classes) + 1)\n                im = ax.matshow(\n                    locals_assignments, cmap=color, vmin=-1.5, vmax=len(classes) - 0.5\n                )\n\n            div = make_axes_locatable(ax)\n            cax = div.append_axes(""right"", size=""5%"", pad=0.05)\n\n            if classes is None:\n                cbar = plt.colorbar(im, cax=cax, ticks=list(range(-1, 11)))\n                cbar.ax.set_yticklabels([""none""] + list(range(10)))\n            else:\n                cbar = plt.colorbar(im, cax=cax, ticks=np.arange(-1, len(classes)))\n                cbar.ax.set_yticklabels([""none""] + list(classes))\n\n            ax.set_xticks(())\n            ax.set_yticks(())\n            fig.tight_layout()\n        else:\n            im.set_data(locals_assignments)\n\n        return im\n\n\ndef plot_performance(\n    performances: Dict[str, List[float]],\n    ax: Optional[Axes] = None,\n    figsize: Tuple[int, int] = (7, 4),\n    save: Optional[str] = None,\n) -> Axes:\n    # language=rst\n    """"""\n    Plot training accuracy curves.\n\n    :param performances: Lists of training accuracy estimates per voting scheme.\n    :param ax: Used for re-drawing the performance plot.\n    :param figsize: Horizontal, vertical figure size in inches.\n    :param save: file name to save fig, if None = not saving fig.\n    :return: Used for re-drawing the performance plot.\n    """"""\n\n    if save is not None:\n        plt.ioff()\n        _, ax = plt.subplots(figsize=figsize)\n\n        for scheme in performances:\n            ax.plot(\n                range(len(performances[scheme])),\n                [p for p in performances[scheme]],\n                label=scheme,\n            )\n\n        ax.set_ylim([0, 100])\n        ax.set_title(""Estimated classification accuracy"")\n        ax.set_xlabel(""No. of examples"")\n        ax.set_ylabel(""Accuracy"")\n        ax.set_xticks(())\n        ax.set_yticks(range(0, 110, 10))\n        ax.legend()\n\n        plt.savefig(save, bbox_inches=""tight"")\n        plt.close()\n        plt.ion()\n    else:\n        if not ax:\n            _, ax = plt.subplots(figsize=figsize)\n        else:\n            ax.clear()\n\n        for scheme in performances:\n            ax.plot(\n                range(len(performances[scheme])),\n                [p for p in performances[scheme]],\n                label=scheme,\n            )\n\n        ax.set_ylim([0, 100])\n        ax.set_title(""Estimated classification accuracy"")\n        ax.set_xlabel(""No. of examples"")\n        ax.set_ylabel(""Accuracy"")\n        ax.set_xticks(())\n        ax.set_yticks(range(0, 110, 10))\n        ax.legend()\n\n    return ax\n\n\ndef plot_voltages(\n    voltages: Dict[str, torch.Tensor],\n    ims: Optional[List[AxesImage]] = None,\n    axes: Optional[List[Axes]] = None,\n    time: Tuple[int, int] = None,\n    n_neurons: Optional[Dict[str, Tuple[int, int]]] = None,\n    cmap: Optional[str] = ""jet"",\n    plot_type: str = ""color"",\n    thresholds: Dict[str, torch.Tensor] = None,\n    figsize: Tuple[float, float] = (8.0, 4.5),\n) -> Tuple[List[AxesImage], List[Axes]]:\n    # language=rst\n    """"""\n    Plot voltages for any group(s) of neurons.\n\n    :param voltages: Contains voltage data by neuron layers.\n    :param ims: Used for re-drawing the plots.\n    :param axes: Used for re-drawing the plots.\n    :param time: Plot voltages of neurons in given time range. Default is entire\n        simulation time.\n    :param n_neurons: Plot voltages of neurons in given range of neurons. Default is all\n        neurons.\n    :param cmap: Matplotlib colormap to use.\n    :param figsize: Horizontal, vertical figure size in inches.\n    :param plot_type: The way how to draw graph. \'color\' for pcolormesh, \'line\' for\n        curved lines.\n    :param thresholds: Thresholds of the neurons in each layer.\n    :return: ``ims, axes``: Used for re-drawing the plots.\n    """"""\n    n_subplots = len(voltages.keys())\n\n    # for key in voltages.keys():\n    #     voltages[key] = voltages[key].view(-1, voltages[key].size(-1))\n    voltages = {k: v.view(v.size(0), -1) for (k, v) in voltages.items()}\n\n    if time is None:\n        for key in voltages.keys():\n            time = (0, voltages[key].size(0))\n            break\n\n    if n_neurons is None:\n        n_neurons = {}\n\n    for key, val in voltages.items():\n        if key not in n_neurons.keys():\n            n_neurons[key] = (0, val.size(1))\n\n    if not ims:\n        fig, axes = plt.subplots(n_subplots, 1, figsize=figsize)\n        ims = []\n        if n_subplots == 1:  # Plotting only one image\n            for v in voltages.items():\n                if plot_type == ""line"":\n                    ims.append(\n                        axes.plot(\n                            v[1]\n                            .detach()\n                            .clone()\n                            .cpu()\n                            .numpy()[\n                                time[0] : time[1],\n                                n_neurons[v[0]][0]: n_neurons[v[0]][1],\n                            ]\n                        )\n                    )\n\n                    if thresholds is not None and thresholds[v[0]].size() == torch.Size(\n                        []\n                    ):\n                        ims.append(\n                            axes.axhline(\n                                y=thresholds[v[0]].item(), c=""r"", linestyle=""--""\n                            )\n                        )\n                else:\n                    ims.append(\n                        axes.pcolormesh(\n                            v[1]\n                            .cpu()\n                            .numpy()[\n                                time[0] : time[1],\n                                    n_neurons[v[0]][0] : n_neurons[v[0]][1],\n                            ]\n                            .T,\n                            cmap=cmap,\n                        )\n                    )\n\n                args = (v[0], n_neurons[v[0]][0], n_neurons[v[0]][1], time[0], time[1])\n                plt.title(""%s voltages for neurons (%d - %d) from t = %d to %d "" % args)\n                plt.xlabel(""Time (ms)"")\n\n                if plot_type == ""line"":\n                    plt.ylabel(""Voltage"")\n                else:\n                    plt.ylabel(""Neuron index"")\n\n                axes.set_aspect(""auto"")\n\n        else:  # Plot each layer at a time\n            for i, v in enumerate(voltages.items()):\n                if plot_type == ""line"":\n                    ims.append(\n                        axes[i].plot(\n                            v[1]\n                            .cpu()\n                            .numpy()[\n                                time[0] : time[1],\n                                n_neurons[v[0]][0] : n_neurons[v[0]][1],\n                            ]\n                        )\n                    )\n                    if thresholds is not None and thresholds[v[0]].size() == torch.Size(\n                        []\n                    ):\n                        ims.append(\n                            axes[i].axhline(\n                                y=thresholds[v[0]].item(), c=""r"", linestyle=""--""\n                            )\n                        )\n                else:\n                    ims.append(\n                        axes[i].matshow(\n                            v[1]\n                            .cpu()\n                            .numpy()[\n                                time[0] : time[1],\n                                n_neurons[v[0]][0] : n_neurons[v[0]][1],\n                            ]\n                            .T,\n                            cmap=cmap,\n                        )\n                    )\n                args = (v[0], n_neurons[v[0]][0], n_neurons[v[0]][1], time[0], time[1])\n                axes[i].set_title(\n                    ""%s voltages for neurons (%d - %d) from t = %d to %d "" % args\n                )\n\n            for ax in axes:\n                ax.set_aspect(""auto"")\n\n        if plot_type == ""color"":\n            plt.setp(axes, xlabel=""Simulation time"", ylabel=""Neuron index"")\n        elif plot_type == ""line"":\n            plt.setp(axes, xlabel=""Simulation time"", ylabel=""Voltage"")\n\n        plt.tight_layout()\n\n    else:\n        # Plotting figure given\n        if n_subplots == 1:  # Plotting only one image\n            for v in voltages.items():\n                axes.clear()\n                if plot_type == ""line"":\n                    axes.plot(\n                        v[1]\n                        .cpu()\n                        .numpy()[\n                           time[0] : time[1],  n_neurons[v[0]][0] : n_neurons[v[0]][1]\n                        ]\n                    )\n                    if thresholds is not None and thresholds[v[0]].size() == torch.Size(\n                        []\n                    ):\n                        axes.axhline(y=thresholds[v[0]].item(), c=""r"", linestyle=""--"")\n                else:\n                    axes.matshow(\n                        v[1]\n                        .cpu()\n                        .numpy()[\n                            time[0] : time[1], n_neurons[v[0]][0] : n_neurons[v[0]][1]\n                        ]\n                        .T,\n                        cmap=cmap,\n                    )\n                args = (v[0], n_neurons[v[0]][0], n_neurons[v[0]][1], time[0], time[1])\n                axes.set_title(\n                    ""%s voltages for neurons (%d - %d) from t = %d to %d "" % args\n                )\n                axes.set_aspect(""auto"")\n\n        else:\n            # Plot each layer at a time\n            for i, v in enumerate(voltages.items()):\n                axes[i].clear()\n                if plot_type == ""line"":\n                    axes[i].plot(\n                        v[1]\n                        .cpu()\n                        .numpy()[\n                            time[0] : time[1], n_neurons[v[0]][0] : n_neurons[v[0]][1],\n                        ]\n                    )\n                    if thresholds is not None and thresholds[v[0]].size() == torch.Size(\n                        []\n                    ):\n                        axes[i].axhline(\n                            y=thresholds[v[0]].item(), c=""r"", linestyle=""--""\n                        )\n                else:\n                    axes[i].matshow(\n                        v[1]\n                        .cpu()\n                        .numpy()[\n                            time[0] : time[1], n_neurons[v[0]][0] : n_neurons[v[0]][1],\n                        ]\n                        .T,\n                        cmap=cmap,\n                    )\n                args = (v[0], n_neurons[v[0]][0], n_neurons[v[0]][1], time[0], time[1])\n                axes[i].set_title(\n                    ""%s voltages for neurons (%d - %d) from t = %d to %d "" % args\n                )\n\n            for ax in axes:\n                ax.set_aspect(""auto"")\n\n        if plot_type == ""color"":\n            plt.setp(axes, xlabel=""Simulation time"", ylabel=""Neuron index"")\n        elif plot_type == ""line"":\n            plt.setp(axes, xlabel=""Simulation time"", ylabel=""Voltage"")\n\n        plt.tight_layout()\n\n    return ims, axes\n'"
bindsnet/analysis/visualization.py,2,"b'import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nfrom typing import List, Tuple, Optional\n\n\ndef plot_weights_movie(ws: np.ndarray, sample_every: int = 1) -> None:\n    # language=rst\n    """"""\n    Create and plot movie of weights.\n    \n    :param ws: Array of shape ``[n_examples, source, target, time]``.\n    :param sample_every: Sub-sample using this parameter.\n    """"""\n    weights = []\n\n    # Obtain samples from the weights for every example.\n    for i in range(ws.shape[0]):\n        sub_sampled_weight = ws[i, :, :, range(0, ws[i].shape[2], sample_every)]\n        weights.append(sub_sampled_weight)\n    else:\n        weights = np.concatenate(weights, axis=0)\n\n    # Initialize plot.\n    fig = plt.figure()\n    im = plt.imshow(weights[0, :, :], cmap=""hot_r"", animated=True, vmin=0, vmax=1)\n    plt.axis(""off"")\n    plt.colorbar(im)\n\n    # Update function for the animation.\n    def update(j):\n        im.set_data(weights[j, :, :])\n        return [im]\n\n    # Initialize animation.\n    global ani\n    ani = 0\n    ani = animation.FuncAnimation(\n        fig, update, frames=weights.shape[-1], interval=1000, blit=True\n    )\n    plt.show()\n\n\ndef plot_spike_trains_for_example(\n    spikes: torch.Tensor,\n    n_ex: Optional[int] = None,\n    top_k: Optional[int] = None,\n    indices: Optional[List[int]] = None,\n) -> None:\n    # language=rst\n    """"""\n    Plot spike trains for top-k neurons or for specific indices.\n\n    :param spikes: Spikes for one simulation run of shape\n        ``(n_examples, n_neurons, time)``.\n    :param n_ex: Allows user to pick which example to plot spikes for.\n    :param top_k: Plot k neurons that spiked the most for n_ex example.\n    :param indices: Plot specific neurons\' spiking activity instead of top_k.\n    """"""\n    assert n_ex is not None and 0 <= n_ex < spikes.shape[0]\n\n    plt.figure()\n\n    if top_k is None and indices is None:  # Plot all neurons\' spiking activity\n        spike_per_neuron = [np.argwhere(i == 1).flatten() for i in spikes[n_ex, :, :]]\n        plt.title(""Spiking activity for all %d neurons"" % spikes.shape[1])\n\n    elif top_k is None:  # Plot based on indices parameter\n        assert indices is not None\n        spike_per_neuron = [\n            np.argwhere(i == 1).flatten() for i in spikes[n_ex, indices, :]\n        ]\n\n    elif indices is None:  # Plot based on top_k parameter\n        assert top_k is not None\n        # Obtain the top k neurons that fired the most\n        top_k_loc = np.argsort(np.sum(spikes[n_ex, :, :], axis=1), axis=0)[::-1]\n        spike_per_neuron = [\n            np.argwhere(i == 1).flatten() for i in spikes[n_ex, top_k_loc[0:top_k], :]\n        ]\n        plt.title(""Spiking activity for top %d neurons"" % top_k)\n\n    else:\n        raise ValueError(\'One of ""top_k"" or ""indices"" or both must be None\')\n\n    plt.eventplot(spike_per_neuron, linelengths=[0.5] * len(spike_per_neuron))\n    plt.xlabel(""Simulation Time"")\n    plt.ylabel(""Neuron index"")\n    plt.show()\n\n\ndef plot_voltage(\n    voltage: torch.Tensor,\n    n_ex: int = 0,\n    n_neuron: int = 0,\n    time: Optional[Tuple[int, int]] = None,\n    threshold: float = None,\n) -> None:\n    # language=rst\n    """"""\n    Plot voltage for a single neuron on a specific example.\n\n    :param voltage: Tensor or array of shape ``[n_examples, n_neurons, time]``.\n    :param n_ex: Allows user to pick which example to plot voltage for.\n    :param n_neuron: Neuron index for which to plot voltages for.\n    :param time: Plot spiking activity of neurons between the given range of time.\n    :param threshold: Neuron spiking threshold.\n    """"""\n    assert n_ex >= 0 and n_neuron >= 0\n    assert n_ex < voltage.shape[0] and n_neuron < voltage.shape[1]\n\n    if time is None:\n        time = (0, voltage.shape[-1])\n    else:\n        assert time[0] < time[1]\n        assert time[1] <= voltage.shape[-1]\n\n    timer = np.arange(time[0], time[1])\n    time_ticks = np.arange(time[0], time[1] + 1, 10)\n\n    plt.figure()\n    plt.plot(voltage[n_ex, n_neuron, timer])\n    plt.xlabel(""Simulation Time"")\n    plt.ylabel(""Voltage"")\n    plt.title(""Membrane voltage of neuron %d for example %d"" % (n_neuron, n_ex + 1))\n    locs, labels = plt.xticks()\n    locs = range(int(locs[1]), int(locs[-1]), 10)\n    plt.xticks(locs, time_ticks)\n\n    # Draw threshold line only if given\n    if threshold is not None:\n        plt.axhline(threshold, linestyle=""--"", color=""black"", zorder=0)\n\n    plt.show()\n\n\ndef summary(net) -> str:\n    # language=rst\n    """"""\n    Summarizes informations about a Network.\n    Includes layers and connection informations.\n\n    :param net: Network\n    :return: string\n    """"""\n    total_neurons = 0\n    total_trainable_neurons = 0\n    total_weights = 0\n    total_trainable_weights = 0\n    out = ""\\033[92m         ===============\\n""\n    out += ""         NETWORK SUMMARY\\n""\n    out += ""         ===============\\n""\n    out += ""         \\033[0mbatch size:"" + str(net.batch_size) + ""\\n""\n    for l in net.layers:\n        out += ""    \\033[0m\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\xc2\xb7\\n""\n        out += ""    Layer: \'"" + l + ""\'""\n\n        if net.layers[l].learning:\n            out += "" (trainable)\\n""\n            total_trainable_neurons += net.layers[l].n\n        else:\n            out += "" (not trainable)\\n""\n\n        out += (\n            ""   ""\n            + ""{:,}"".format(net.layers[l].n)\n            + "" neurons ""\n            + str(net.layers[l].shape)\n            + ""\\n""\n        )\n        total_neurons += net.layers[l].n\n        for c in net.connections:\n            if c[0] == l:\n                w_size = 1\n                for dim in net.connections[c[0], c[1]].w.shape:\n                    w_size *= dim\n                out += (\n                    ""       \\033[94m\xc2\xb7connected to \'""\n                    + c[1]\n                    + ""\' by ""\n                    + ""{:,}"".format(w_size)\n                    + "" synapses\\n""\n                )\n                total_weights += w_size\n                if net.layers[c[1]].learning:\n                    total_trainable_weights += w_size\n\n    out += ""     \\033[92m==========================\\n""\n    out += (\n        ""\\033[95mTotal neurons: ""\n        + ""{:,}"".format(total_neurons)\n        + "" ({:,} trainable)"".format(total_trainable_neurons)\n        + ""\\n""\n    )\n    out += (\n        ""Total synapses weights: ""\n        + ""{:,}"".format(total_weights)\n        + "" ({:,} trainable)"".format(total_trainable_weights)\n        + ""\\033[0m""\n    )\n    return out\n'"
bindsnet/conversion/__init__.py,0,"b'from .conversion import (\n    Permute,\n    FeatureExtractor,\n    SubtractiveResetIFNodes,\n    PassThroughNodes,\n    PermuteConnection,\n    ConstantPad2dConnection,\n    data_based_normalization,\n    ann_to_snn,\n)\n'"
bindsnet/conversion/conversion.py,35,"b'import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.nn.modules.utils import _pair\n\nfrom copy import deepcopy\nfrom typing import Union, Sequence, Optional, Tuple, Dict, Iterable\n\nimport bindsnet.network.nodes as nodes\nimport bindsnet.network.topology as topology\n\nfrom bindsnet.network import Network\n\n\nclass Permute(nn.Module):\n    # language=rst\n    """"""\n    PyTorch module for the explicit permutation of a tensor\'s dimensions in a parent\n    module\'s ``forward`` pass (as opposed to ``torch.permute``).\n    """"""\n\n    def __init__(self, dims):\n        # language=rst\n        """"""\n        Constructor for ``Permute`` module.\n\n        :param dims: Ordering of dimensions for permutation.\n        """"""\n        super(Permute, self).__init__()\n\n        self.dims = dims\n\n    def forward(self, x):\n        # language=rst\n        """"""\n        Forward pass of permutation module.\n\n        :param x: Input tensor to permute.\n        :return: Permuted input tensor.\n        """"""\n        return x.permute(*self.dims).contiguous()\n\n\nclass FeatureExtractor(nn.Module):\n    # language=rst\n    """"""\n    Special-purpose PyTorch module for the extraction of child module\'s activations.\n    """"""\n\n    def __init__(self, submodule):\n        # language=rst\n        """"""\n        Constructor for ``FeatureExtractor`` module.\n\n        :param submodule: The module who\'s children modules are to be extracted.\n        """"""\n        super(FeatureExtractor, self).__init__()\n\n        self.submodule = submodule\n\n    def forward(self, x: torch.Tensor) -> Dict[nn.Module, torch.Tensor]:\n        # language=rst\n        """"""\n        Forward pass of the feature extractor.\n\n        :param x: Input data for the ``submodule\'\'.\n        :return: A dictionary mapping\n        """"""\n        activations = {""input"": x}\n        for name, module in self.submodule._modules.items():\n            if isinstance(module, nn.Linear):\n                x = x.view(-1, module.in_features)\n\n            x = module(x)\n            activations[name] = x\n\n        return activations\n\n\nclass SubtractiveResetIFNodes(nodes.Nodes):\n    # language=rst\n    """"""\n    Layer of `integrate-and-fire (IF) neurons\n    <http://neuronaldynamics.epfl.ch/online/Ch1.S3.html>` using reset by subtraction.\n    """"""\n\n    def __init__(\n        self,\n        n: Optional[int] = None,\n        shape: Optional[Iterable[int]] = None,\n        traces: bool = False,\n        traces_additive: bool = False,\n        tc_trace: Union[float, torch.Tensor] = 20.0,\n        trace_scale: Union[float, torch.Tensor] = 1.0,\n        sum_input: bool = False,\n        thresh: Union[float, torch.Tensor] = -52.0,\n        reset: Union[float, torch.Tensor] = -65.0,\n        refrac: Union[int, torch.Tensor] = 5,\n        lbound: float = None,\n        **kwargs,\n    ) -> None:\n        # language=rst\n        """"""\n        Instantiates a layer of IF neurons with the subtractive reset mechanism from\n        `this paper\n        <https://www.frontiersin.org/articles/10.3389/fnins.2017.00682/full>`_.\n\n        :param n: The number of neurons in the layer.\n        :param shape: The dimensionality of the layer.\n        :param traces: Whether to record spike traces.\n        :param traces_additive: Whether to record spike traces additively.\n        :param tc_trace: Time constant of spike trace decay.\n        :param trace_scale: Scaling factor for spike trace.\n        :param sum_input: Whether to sum all inputs.\n        :param thresh: Spike threshold voltage.\n        :param reset: Post-spike reset voltage.\n        :param refrac: Refractory (non-firing) period of the neuron.\n        :param lbound: Lower bound of the voltage.\n        """"""\n        super().__init__(\n            n=n,\n            shape=shape,\n            traces=traces,\n            traces_additive=traces_additive,\n            tc_trace=tc_trace,\n            trace_scale=trace_scale,\n            sum_input=sum_input,\n        )\n\n        self.register_buffer(\n            ""reset"", torch.tensor(reset, dtype=torch.float)\n        )  # Post-spike reset voltage.\n        self.register_buffer(\n            ""thresh"", torch.tensor(thresh, dtype=torch.float)\n        )  # Spike threshold voltage.\n        self.register_buffer(\n            ""refrac"", torch.tensor(refrac)\n        )  # Post-spike refractory period.\n        self.register_buffer(""v"", torch.FloatTensor())  # Neuron voltages.\n        self.register_buffer(\n            ""refrac_count"", torch.FloatTensor()\n        )  # Refractory period counters.\n\n        self.lbound = lbound  # Lower bound of voltage.\n\n    def forward(self, x: torch.Tensor) -> None:\n        # language=rst\n        """"""\n        Runs a single simulation step.\n\n        :param x: Inputs to the layer.\n        """"""\n        # Integrate input voltages.\n        self.v += (self.refrac_count == 0).float() * x\n\n        # Decrement refractory counters.\n        self.refrac_count = (self.refrac_count > 0).float() * (\n            self.refrac_count - self.dt\n        )\n\n        # Check for spiking neurons.\n        self.s = self.v >= self.thresh\n\n        # Refractoriness and voltage reset.\n        self.refrac_count.masked_fill_(self.s, self.refrac)\n        self.v[self.s] = self.v[self.s] - self.thresh\n\n        # Voltage clipping to lower bound.\n        if self.lbound is not None:\n            self.v.masked_fill_(self.v < self.lbound, self.lbound)\n\n        super().forward(x)\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Resets relevant state variables.\n        """"""\n        super().reset_state_variables()\n        self.v.fill_(self.reset)  # Neuron voltages.\n        self.refrac_count.zero_()  # Refractory period counters.\n\n    def set_batch_size(self, batch_size) -> None:\n        # language=rst\n        """"""\n        Sets mini-batch size. Called when layer is added to a network.\n\n        :param batch_size: Mini-batch size.\n        """"""\n        super().set_batch_size(batch_size=batch_size)\n        self.v = self.reset * torch.ones(batch_size, *self.shape, device=self.v.device)\n        self.refrac_count = torch.zeros_like(self.v, device=self.refrac_count.device)\n\n\nclass PassThroughNodes(nodes.Nodes):\n    # language=rst\n    """"""\n    Layer of `integrate-and-fire (IF) neurons\n    <http://neuronaldynamics.epfl.ch/online/Ch1.S3.html>`_ with using reset by\n    subtraction.\n    """"""\n\n    def __init__(\n        self,\n        n: Optional[int] = None,\n        shape: Optional[Sequence[int]] = None,\n        traces: bool = False,\n        traces_additive: bool = False,\n        tc_trace: Union[float, torch.Tensor] = 20.0,\n        trace_scale: Union[float, torch.Tensor] = 1.0,\n        sum_input: bool = False,\n    ) -> None:\n        # language=rst\n        """"""\n        Instantiates a layer of IF neurons.\n\n        :param n: The number of neurons in the layer.\n        :param shape: The dimensionality of the layer.\n        :param traces: Whether to record spike traces.\n        :param trace_tc: Time constant of spike trace decay.\n        :param sum_input: Whether to sum all inputs.\n        """"""\n        super().__init__(\n            n=n,\n            shape=shape,\n            traces=traces,\n            traces_additive=traces_additive,\n            tc_trace=tc_trace,\n            trace_scale=trace_scale,\n            sum_input=sum_input,\n        )\n        self.register_buffer(""v"", torch.zeros(self.shape))\n\n    def forward(self, x: torch.Tensor) -> None:\n        # language=rst\n        """"""\n        Runs a single simulation step.\n\n        :param inputs: Inputs to the layer.\n        :param dt: Simulation time step.\n        """"""\n        self.s = x\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Resets relevant state variables.\n        """"""\n        self.s.zero_()\n\n\nclass PermuteConnection(topology.AbstractConnection):\n    # language=rst\n    """"""\n    Special-purpose connection for emulating the custom ``Permute`` module in spiking\n    neural networks.\n    """"""\n\n    def __init__(\n        self,\n        source: nodes.Nodes,\n        target: nodes.Nodes,\n        dims: Sequence,\n        nu: Optional[Union[float, Sequence[float]]] = None,\n        weight_decay: float = 0.0,\n        **kwargs,\n    ) -> None:\n        # language=rst\n        """"""\n        Constructor for ``PermuteConnection``.\n\n        :param source: A layer of nodes from which the connection originates.\n        :param target: A layer of nodes to which the connection connects.\n        :param dims: Order of dimensions to permute.\n        :param nu: Learning rate for both pre- and post-synaptic events.\n        :param weight_decay: Constant multiple to decay weights by on each iteration.\n\n        Keyword arguments:\n\n        :param function update_rule: Modifies connection parameters according to some\n            rule.\n        :param float wmin: The minimum value on the connection weights.\n        :param float wmax: The maximum value on the connection weights.\n        :param float norm: Total weight per target neuron normalization.\n        """"""\n        super().__init__(source, target, nu, weight_decay, **kwargs)\n\n        self.dims = dims\n\n    def compute(self, s: torch.Tensor) -> torch.Tensor:\n        # language=rst\n        """"""\n        Permute input.\n\n        :param s: Input.\n        :return: Permuted input.\n        """"""\n        return s.permute(self.dims).float()\n\n\nclass ConstantPad2dConnection(topology.AbstractConnection):\n    # language=rst\n    """"""\n    Special-purpose connection for emulating the ``ConstantPad2d`` PyTorch module in\n        spiking neural networks.\n    """"""\n\n    def __init__(\n        self,\n        source: nodes.Nodes,\n        target: nodes.Nodes,\n        padding: Tuple,\n        nu: Optional[Union[float, Sequence[float]]] = None,\n        weight_decay: float = 0.0,\n        **kwargs,\n    ) -> None:\n        # language=rst\n        """"""\n        Constructor for ``ConstantPad2dConnection``.\n\n        :param source: A layer of nodes from which the connection originates.\n        :param target: A layer of nodes to which the connection connects.\n        :param padding: Padding of input tensors; passed to ``torch.nn.functional.pad``.\n        :param nu: Learning rate for both pre- and post-synaptic events.\n        :param weight_decay: Constant multiple to decay weights by on each iteration.\n\n        Keyword arguments:\n\n        :param function update_rule: Modifies connection parameters according to some\n            rule.\n        :param float wmin: The minimum value on the connection weights.\n        :param float wmax: The maximum value on the connection weights.\n        :param float norm: Total weight per target neuron normalization.\n        """"""\n\n        super().__init__(source, target, nu, weight_decay, **kwargs)\n\n        self.padding = padding\n\n    def compute(self, s: torch.Tensor):\n        # language=rst\n        """"""\n        Pad input.\n\n        :param s: Input.\n        :return: Padding input.\n        """"""\n        return F.pad(s, self.padding).float()\n\n\ndef data_based_normalization(\n    ann: Union[nn.Module, str], data: torch.Tensor, percentile: float = 99.9\n):\n    # language=rst\n    """"""\n    Use a dataset to rescale ANN weights and biases such that that the max ReLU\n    activation is less than 1.\n\n    :param ann: Artificial neural network implemented in PyTorch. Accepts either\n        ``torch.nn.Module`` or path to network saved using ``torch.save()``.\n    :param data: Data to use to perform data-based weight normalization of shape\n        ``[n_examples, ...]``.\n    :param percentile: Percentile (in ``[0, 100]``) of activations to scale by in\n        data-based normalization scheme.\n    :return: Artificial neural network with rescaled weights and biases according to\n        activations on the dataset.\n    """"""\n    if isinstance(ann, str):\n        ann = torch.load(ann)\n\n    assert isinstance(ann, nn.Module)\n\n    def set_requires_grad(module, value):\n        for param in module.parameters():\n            param.requires_grad = value\n\n    set_requires_grad(ann, value=False)\n    extractor = FeatureExtractor(ann)\n    all_activations = extractor.forward(data)\n\n    prev_module = None\n    prev_factor = 1\n    for name, module in ann._modules.items():\n        if isinstance(module, nn.Sequential):\n\n            extractor2 = FeatureExtractor(module)\n            all_activations2 = extractor2.forward(data)\n            for name2, module2 in module.named_children():\n                activations = all_activations2[name2]\n\n                if isinstance(module2, nn.ReLU):\n                    if prev_module is not None:\n                        scale_factor = np.percentile(activations.cpu(), percentile)\n\n                        prev_module.weight *= prev_factor / scale_factor\n                        prev_module.bias /= scale_factor\n\n                        prev_factor = scale_factor\n\n                elif isinstance(module2, nn.Linear) or isinstance(module2, nn.Conv2d):\n                    prev_module = module2\n\n        else:\n            activations = all_activations[name]\n            if isinstance(module, nn.ReLU):\n                if prev_module is not None:\n                    scale_factor = np.percentile(activations.cpu(), percentile)\n\n                    prev_module.weight *= prev_factor / scale_factor\n                    prev_module.bias /= scale_factor\n\n                    prev_factor = scale_factor\n\n            elif isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                prev_module = module\n\n    return ann\n\n\ndef _ann_to_snn_helper(prev, current, node_type, last=False, **kwargs):\n    # language=rst\n    """"""\n    Helper function for main ``ann_to_snn`` method.\n\n    :param prev: Previous PyTorch module in artificial neural network.\n    :param current: Current PyTorch module in artificial neural network.\n    :param node_type: Type of ``bindsnet.network.nodes`` to use.\n    :param last: Whether this connection and layer is the last to be converted.\n    :return: Spiking neural network layer and connection corresponding to ``prev`` and\n        ``current`` PyTorch modules.\n    """"""\n    if isinstance(current, nn.Linear):\n        layer = node_type(\n            n=current.out_features,\n            reset=0,\n            thresh=1,\n            refrac=0,\n            sum_input=last,\n            **kwargs,\n        )\n        bias = current.bias if current.bias is not None else torch.zeros(layer.n)\n        connection = topology.Connection(\n            source=prev, target=layer, w=current.weight.t(), b=bias\n        )\n\n    elif isinstance(current, nn.Conv2d):\n        input_height, input_width = prev.shape[2], prev.shape[3]\n        out_channels, output_height, output_width = (\n            current.out_channels,\n            prev.shape[2],\n            prev.shape[3],\n        )\n\n        width = (\n            input_height - current.kernel_size[0] + 2 * current.padding[0]\n        ) / current.stride[0] + 1\n        height = (\n            input_width - current.kernel_size[1] + 2 * current.padding[1]\n        ) / current.stride[1] + 1\n        shape = (1, out_channels, int(width), int(height))\n\n        layer = node_type(\n            shape=shape, reset=0, thresh=1, refrac=0, sum_input=last, **kwargs\n        )\n        bias = current.bias if current.bias is not None else torch.zeros(layer.shape[1])\n        connection = topology.Conv2dConnection(\n            source=prev,\n            target=layer,\n            kernel_size=current.kernel_size,\n            stride=current.stride,\n            padding=current.padding,\n            dilation=current.dilation,\n            w=current.weight,\n            b=bias,\n        )\n\n    elif isinstance(current, nn.MaxPool2d):\n        input_height, input_width = prev.shape[2], prev.shape[3]\n        current.kernel_size = _pair(current.kernel_size)\n        current.padding = _pair(current.padding)\n        current.stride = _pair(current.stride)\n\n        width = (\n            input_height - current.kernel_size[0] + 2 * current.padding[0]\n        ) / current.stride[0] + 1\n        height = (\n            input_width - current.kernel_size[1] + 2 * current.padding[1]\n        ) / current.stride[1] + 1\n        shape = (1, prev.shape[1], int(width), int(height))\n\n        layer = PassThroughNodes(shape=shape)\n        connection = topology.MaxPool2dConnection(\n            source=prev,\n            target=layer,\n            kernel_size=current.kernel_size,\n            stride=current.stride,\n            padding=current.padding,\n            dilation=current.dilation,\n            decay=1,\n        )\n\n    elif isinstance(current, Permute):\n        layer = PassThroughNodes(\n            shape=[\n                prev.shape[current.dims[0]],\n                prev.shape[current.dims[1]],\n                prev.shape[current.dims[2]],\n                prev.shape[current.dims[3]],\n            ]\n        )\n\n        connection = PermuteConnection(source=prev, target=layer, dims=current.dims)\n\n    elif isinstance(current, nn.ConstantPad2d):\n        layer = PassThroughNodes(\n            shape=[\n                prev.shape[0],\n                prev.shape[1],\n                current.padding[0] + current.padding[1] + prev.shape[2],\n                current.padding[2] + current.padding[3] + prev.shape[3],\n            ]\n        )\n\n        connection = ConstantPad2dConnection(\n            source=prev, target=layer, padding=current.padding\n        )\n\n    else:\n        return None, None\n\n    return layer, connection\n\n\ndef ann_to_snn(\n    ann: Union[nn.Module, str],\n    input_shape: Sequence[int],\n    data: Optional[torch.Tensor] = None,\n    percentile: float = 99.9,\n    node_type: Optional[nodes.Nodes] = SubtractiveResetIFNodes,\n    **kwargs,\n) -> Network:\n    # language=rst\n    """"""\n    Converts an artificial neural network (ANN) written as a ``torch.nn.Module`` into a\n    near-equivalent spiking neural network.\n\n    :param ann: Artificial neural network implemented in PyTorch. Accepts either\n        ``torch.nn.Module`` or path to network saved using ``torch.save()``.\n    :param input_shape: Shape of input data.\n    :param data: Data to use to perform data-based weight normalization of shape\n        ``[n_examples, ...]``.\n    :param percentile: Percentile (in ``[0, 100]``) of activations to scale by in\n        data-based normalization scheme.\n    :param node_type: Class of ``Nodes`` to use in replacing ``torch.nn.Linear`` layers\n        in original ANN.\n    :return: Spiking neural network implemented in PyTorch.\n    """"""\n    if isinstance(ann, str):\n        ann = torch.load(ann)\n    else:\n        ann = deepcopy(ann)\n\n    assert isinstance(ann, nn.Module)\n\n    if data is None:\n        import warnings\n\n        warnings.warn(""Data is None. Weights will not be scaled."", RuntimeWarning)\n    else:\n        ann = data_based_normalization(\n            ann=ann, data=data.detach(), percentile=percentile\n        )\n\n    snn = Network()\n\n    input_layer = nodes.Input(shape=input_shape)\n    snn.add_layer(input_layer, name=""Input"")\n\n    children = []\n    for c in ann.children():\n        if isinstance(c, nn.Sequential):\n            for c2 in list(c.children()):\n                children.append(c2)\n        else:\n            children.append(c)\n\n    i = 0\n    prev = input_layer\n    while i < len(children) - 1:\n        current, nxt = children[i : i + 2]\n        layer, connection = _ann_to_snn_helper(prev, current, node_type, **kwargs)\n\n        i += 1\n\n        if layer is None or connection is None:\n            continue\n\n        snn.add_layer(layer, name=str(i))\n        snn.add_connection(connection, source=str(i - 1), target=str(i))\n\n        prev = layer\n\n    current = children[-1]\n    layer, connection = _ann_to_snn_helper(\n        prev, current, node_type, last=True, **kwargs\n    )\n\n    i += 1\n\n    if layer is not None or connection is not None:\n        snn.add_layer(layer, name=str(i))\n        snn.add_connection(connection, source=str(i - 1), target=str(i))\n\n    return snn\n'"
bindsnet/datasets/__init__.py,0,"b'from .torchvision_wrapper import create_torchvision_dataset_wrapper\nfrom .spoken_mnist import SpokenMNIST\nfrom .davis import Davis\nfrom .alov300 import ALOV300\n\nfrom .collate import time_aware_collate\nfrom .dataloader import DataLoader\n\n\nCIFAR10 = create_torchvision_dataset_wrapper(""CIFAR10"")\nCIFAR100 = create_torchvision_dataset_wrapper(""CIFAR100"")\nCityscapes = create_torchvision_dataset_wrapper(""Cityscapes"")\nCocoCaptions = create_torchvision_dataset_wrapper(""CocoCaptions"")\nCocoDetection = create_torchvision_dataset_wrapper(""CocoDetection"")\nDatasetFolder = create_torchvision_dataset_wrapper(""DatasetFolder"")\nEMNIST = create_torchvision_dataset_wrapper(""EMNIST"")\nFakeData = create_torchvision_dataset_wrapper(""FakeData"")\nFashionMNIST = create_torchvision_dataset_wrapper(""FashionMNIST"")\nFlickr30k = create_torchvision_dataset_wrapper(""Flickr30k"")\nFlickr8k = create_torchvision_dataset_wrapper(""Flickr8k"")\nImageFolder = create_torchvision_dataset_wrapper(""ImageFolder"")\nKMNIST = create_torchvision_dataset_wrapper(""KMNIST"")\nLSUN = create_torchvision_dataset_wrapper(""LSUN"")\nLSUNClass = create_torchvision_dataset_wrapper(""LSUNClass"")\nMNIST = create_torchvision_dataset_wrapper(""MNIST"")\nOmniglot = create_torchvision_dataset_wrapper(""Omniglot"")\nPhotoTour = create_torchvision_dataset_wrapper(""PhotoTour"")\nSBU = create_torchvision_dataset_wrapper(""SBU"")\nSEMEION = create_torchvision_dataset_wrapper(""SEMEION"")\nSTL10 = create_torchvision_dataset_wrapper(""STL10"")\nSVHN = create_torchvision_dataset_wrapper(""SVHN"")\nVOCDetection = create_torchvision_dataset_wrapper(""VOCDetection"")\nVOCSegmentation = create_torchvision_dataset_wrapper(""VOCSegmentation"")\n'"
bindsnet/datasets/alov300.py,1,"b'from __future__ import print_function, division\n\nimport os\nimport numpy as np\nimport torch\nimport zipfile\nimport warnings\nimport sys\nimport time\nimport cv2\nimport bindsnet.datasets.preprocess\n\n\nfrom PIL import Image\nfrom glob import glob\nfrom typing import Optional, Tuple, List, Iterable\nfrom urllib.request import urlretrieve\nfrom torch.utils.data import Dataset\n\nwarnings.filterwarnings(""ignore"")\n\n\nclass ALOV300(Dataset):\n\n    DATASET_WEB = ""http://alov300pp.joomlafree.it/dataset-resources.html""\n    VOID_LABEL = 255\n\n    def __init__(self, root, transform, input_size, download=False):\n        """"""\n        Class to read the ALOV dataset\n        \n        :param root: Path to the ALOV folder that contains JPEGImages, Annotations, etc. folders.\n        :param input_size: The input size of network that is using this data, for rescaling\n        :param download: Specify whether to download the dataset if it is not present\n        :param num_samples: Number of samples to pass to the batch\n        """"""\n        super(ALOV300, self).__init__()\n\n        # Makes a unique path for a given instance of davis\n        self.root = root\n        self.download = download\n        self.img_path = os.path.join(self.root, ""JPEGImages"")\n        self.box_path = os.path.join(self.root, ""box/"")\n        self.frame_path = os.path.join(self.root, ""frame/"")\n\n        # Check if Davis is installed and download it if necessary\n        self._check_directories()\n\n        self.input_size = input_size\n        self.transform = transform\n        self.x, self.y = self._parse_data(self.frame_path, self.box_path)\n        self.len = len(self.y)\n\n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, idx):\n        sample, _ = self.get_sample(idx)\n        if self.transform:\n            sample = self.transform(sample)\n        return sample\n\n    def _parse_data(self, root, target_dir):\n        """"""\n        Parses ALOV dataset and builds tuples of (template, search region)\n        tuples from consecutive annotated frames.\n        """"""\n        self.exclude = [\n            # ""01-Light_video00016"",\n            # ""01-Light_video00022"",\n            # ""01-Light_video00023"",\n            # ""02-SurfaceCover_video00012"",\n            # ""03-Specularity_video00003"",\n            # ""03-Specularity_video00012"",\n            # ""10-LowContrast_video00013"",\n        ]\n\n        x = []\n        y = []\n        envs = os.listdir(target_dir)\n        num_anno = 0\n        print(""Parsing ALOV dataset..."")\n        for env in envs:\n            env_videos = os.listdir(root + env)\n            for vid in env_videos:\n                if vid in self.exclude:\n                    continue\n                vid_src = f""{self.frame_path}{env}/{vid}""\n                vid_ann = f""{self.box_path}{env}/{vid}.ann""\n                frames = os.listdir(vid_src)\n                frames.sort()\n                frames = [vid_src + ""/"" + frame for frame in frames]\n                f = open(vid_ann, ""r"")\n                annotations = f.readlines()\n                f.close()\n                frame_idxs = [int(ann.split("" "")[0]) - 1 for ann in annotations]\n                frames = np.array(frames)\n                num_anno += len(annotations)\n                for i in range(len(frame_idxs) - 1):\n                    idx = frame_idxs[i]\n                    next_idx = frame_idxs[i + 1]\n                    x.append([frames[idx], frames[next_idx]])\n                    y.append([annotations[i], annotations[i + 1]])\n        x = np.array(x)\n        y = np.array(y)\n        self.len = len(y)\n        print(""ALOV dataset parsing done."")\n        print(""Total number of annotations in ALOV dataset = %d"" % num_anno)\n        return x, y\n\n    def get_sample(self, idx):\n        """"""\n        Returns sample without transformation for visualization.\n\n        Sample consists of resized previous and current frame with target\n        which is passed to the network. Bounding box values are normalized\n        between 0 and 1 with respect to the target frame and then scaled by\n        factor of 10.\n        """"""\n        opts_curr = {}\n        curr_sample = {}\n        curr_img = self.get_orig_sample(idx, 1)[""image""]\n        currbb = self.get_orig_sample(idx, 1)[""bb""]\n        prevbb = self.get_orig_sample(idx, 0)[""bb""]\n        bbox_curr_shift = BoundingBox(prevbb[0], prevbb[1], prevbb[2], prevbb[3])\n        (\n            rand_search_region,\n            rand_search_location,\n            edge_spacing_x,\n            edge_spacing_y,\n        ) = cropPadImage(bbox_curr_shift, curr_img)\n        bbox_curr_gt = BoundingBox(currbb[0], currbb[1], currbb[2], currbb[3])\n        bbox_gt_recentered = BoundingBox(0, 0, 0, 0)\n        bbox_gt_recentered = bbox_curr_gt.recenter(\n            rand_search_location, edge_spacing_x, edge_spacing_y, bbox_gt_recentered\n        )\n        curr_sample[""image""] = rand_search_region\n        curr_sample[""bb""] = bbox_gt_recentered.get_bb_list()\n\n        # additional options for visualization\n        opts_curr[""edge_spacing_x""] = edge_spacing_x\n        opts_curr[""edge_spacing_y""] = edge_spacing_y\n        opts_curr[""search_location""] = rand_search_location\n        opts_curr[""search_region""] = rand_search_region\n\n        # build prev sample\n        prev_sample = self.get_orig_sample(idx, 0)\n        prev_sample, opts_prev = crop_sample(prev_sample)\n\n        # scale\n        scale = Rescale((self.input_size, self.input_size))\n        scaled_curr_obj = scale(curr_sample, opts_curr)\n        scaled_prev_obj = scale(prev_sample, opts_prev)\n        training_sample = {\n            ""previmg"": scaled_prev_obj[""image""],\n            ""currimg"": scaled_curr_obj[""image""],\n            ""currbb"": scaled_curr_obj[""bb""],\n        }\n        return training_sample, opts_curr\n\n    def get_orig_sample(self, idx, i=1):\n        """"""\n        Returns original image with bounding box at a specific index.\n        Range of valid index: [0, self.len-1].\n        """"""\n        curr = cv2.imread(self.x[idx][i])\n        curr = bgr2rgb(curr)\n        currbb = self.get_bb(self.y[idx][i])\n        sample = {""image"": curr, ""bb"": currbb}\n        return sample\n\n    def get_bb(self, ann):\n        """"""\n        Parses ALOV annotation and returns bounding box in the format:\n        [left, upper, width, height]\n        """"""\n        ann = map(lambda x: float(x), ann.strip().split("" ""))\n        left = min(ann[1], ann[3], ann[5], ann[7])\n        top = min(ann[2], ann[4], ann[6], ann[8])\n        right = max(ann[1], ann[3], ann[5], ann[7])\n        bottom = max(ann[2], ann[4], ann[6], ann[8])\n        return [left, top, right, bottom]\n\n    def show(self, idx, is_current=1):\n        """"""\n        Helper function to display image at a particular index with grounttruth\n        bounding box.\n\n        Arguments:\n            idx: index\n            is_current: 0 for previous frame and 1 for current frame\n        """"""\n        sample = self.get_orig_sample(idx, is_current)\n        image = sample[""image""]\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        bb = sample[""bb""]\n        bb = [int(val) for val in bb]\n        image = cv2.rectangle(image, (bb[0], bb[1]), (bb[2], bb[3]), (0, 255, 0), 2)\n        cv2.imshow(""alov dataset sample: "" + str(idx), image)\n        cv2.waitKey(0)\n\n    def show_sample(self, idx):\n        """"""\n        Helper function to display sample, which is passed to GOTURN.\n        Shows previous frame and current frame with bounding box.\n        """"""\n        x, _ = self.get_sample(idx)\n        prev_image = x[""previmg""]\n        curr_image = x[""currimg""]\n        bb = x[""currbb""]\n        bbox = BoundingBox(bb[0], bb[1], bb[2], bb[3])\n        bbox.unscale(curr_image)\n        bb = bbox.get_bb_list()\n        bb = [int(val) for val in bb]\n        prev_image = cv2.cvtColor(prev_image, cv2.COLOR_RGB2BGR)\n        curr_image = cv2.cvtColor(curr_image, cv2.COLOR_RGB2BGR)\n        curr_image = cv2.rectangle(\n            curr_image, (bb[0], bb[1]), (bb[2], bb[3]), (0, 255, 0), 2\n        )\n        concat_image = np.hstack((prev_image, curr_image))\n        cv2.imshow(""alov dataset sample: "" + str(idx), concat_image)\n        cv2.waitKey(0)\n\n    def _check_directories(self):\n        """"""\n        Verifies that the correct dataset is downloaded; downloads if it isn\'t and download=True.\n\n        :raises: FileNotFoundError if the subset sequence, annotation or root folder is missing.\n        """"""\n        if not os.path.exists(self.root):\n            if self.download:\n                self._download()\n            else:\n                raise FileNotFoundError(\n                    f""ALOV300 not found in the specified directory, download it from {self.DATASET_WEB} or add download=True to your call""\n                )\n        if not os.path.exists(self.frame_path):\n            raise FileNotFoundError(\n                f""Frames not found, check the directory: {self.root}""\n            )\n        if not os.path.exists(self.box_path):\n            raise FileNotFoundError(\n                f""Boxes not found, check the directory: {self.root}""\n            )\n\n    def _download(self):\n        """"""\n        Downloads the correct dataset based on the given parameters\n\n        Relies on self.tag to determine both the name of the folder created for the dataset and for the finding the correct download url. \n        """"""\n\n        os.makedirs(self.root)\n\n        # Grabs the correct zip url based on parameters\n        self.frame_zip_path = os.path.join(self.root, ""frame.zip"")\n        self.text_zip_path = os.path.join(self.root, ""text.zip"")\n        frame_zip_url = f""http://isis-data.science.uva.nl/alov/alov300++_frames.zip""\n        text_zip_url = f""http://isis-data.science.uva.nl/alov/alov300++GT_txtFiles.zip""\n\n        # Downloads the relevant dataset\n        print(""\\nDownloading ALOV300++ frame set from "" + frame_zip_url + ""\\n"")\n        urlretrieve(frame_zip_url, self.frame_zip_path, reporthook=self.progress)\n\n        print(""\\nDownloading ALOV300++ text set from "" + text_zip_url + ""\\n"")\n        urlretrieve(text_zip_url, self.text_zip_path, reporthook=self.progress)\n\n        print(""\\nDone! \\n\\nUnzipping and restructuring"")\n\n        # Extracts the dataset\n        z = zipfile.ZipFile(self.frame_zip_path, ""r"")\n        z.extractall(path=self.root)\n        z.close()\n        os.remove(self.frame_zip_path)\n\n        z = zipfile.ZipFile(self.text_zip_path, ""r"")\n        z.extractall(path=self.root)\n        z.close()\n        os.remove(self.text_zip_path)\n\n        # Renames the folders containing the dataset\n        box_folder = os.path.join(self.root, ""alov300++_rectangleAnnotation_full/"")\n        frame_folder = os.path.join(self.root, ""imagedata++"")\n\n        os.rename(box_folder, self.box_path)\n        os.rename(frame_folder, self.frame_path)\n\n    # Simple progress indicator for the download of the dataset\n    def progress(self, count, block_size, total_size):\n        global start_time\n        if count == 0:\n            start_time = time.time()\n            return\n        duration = time.time() - start_time\n        progress_size = int(count * block_size)\n        speed = int(progress_size / (1024 * duration))\n        percent = min(int(count * block_size * 100 / total_size), 100)\n        sys.stdout.write(\n            ""\\r...%d%%, %d MB, %d KB/s, %d seconds passed""\n            % (percent, progress_size / (1024 * 1024), speed, duration)\n        )\n        sys.stdout.flush()\n\n\n# Copyright (c) 2018 Abhinav Moudgil\n'"
bindsnet/datasets/collate.py,9,"b'# language=rst\n""""""\nThis code is directly pulled from the pytorch version found at:\n\nhttps://github.com/pytorch/pytorch/blob/master/torch/utils/data/_utils/collate.py\n\nModifications exist to have [time, batch, n_0, ... n_k] instead of batch in dimension 0.\n""""""\n\nimport torch\nfrom torch._six import container_abcs, string_classes, int_classes\n\nfrom torch.utils.data._utils import collate as pytorch_collate\n\n\ndef safe_worker_check():\n    # language=rst\n    """"""\n    Method to check to use shared memory.\n    """"""\n    try:\n        return torch.utils.data.get_worker_info() is not None\n    except:\n        return pytorch_collate._use_shared_memory\n\n\ndef time_aware_collate(batch):\n    # language=rst\n    """"""\n    Puts each data field into a tensor with dimensions ``[time, batch size, ...]``\n\n    Interpretation of dimensions being input:\n    -  0 dim (,) - (1, batch_size, 1)\n    -  1 dim (time,) - (time, batch_size, 1)\n    - >2 dim (time, n_0, ...) - (time, batch_size, n_0, ...)\n    """"""\n    elem = batch[0]\n    elem_type = type(elem)\n    if isinstance(elem, torch.Tensor):\n        # catch 0 and 1 dimension cases and view as specified\n        if elem.dim() == 0:\n            batch = [x.view((1, 1)) for x in batch]\n        elif elem.dim() == 1:\n            batch = [x.view((x.shape[0], 1)) for x in batch]\n\n        out = None\n        if safe_worker_check():\n            # If we\'re in a background process, concatenate directly into a\n            # shared memory tensor to avoid an extra copy\n            numel = sum([x.numel() for x in batch])\n            storage = elem.storage()._new_shared(numel)\n            out = elem.new(storage)\n        return torch.stack(batch, 1, out=out)\n    elif (\n        elem_type.__module__ == ""numpy""\n        and elem_type.__name__ != ""str_""\n        and elem_type.__name__ != ""string_""\n    ):\n        elem = batch[0]\n        if elem_type.__name__ == ""ndarray"":\n            # array of string classes and object\n            if (\n                pytorch_collate.np_str_obj_array_pattern.search(elem.dtype.str)\n                is not None\n            ):\n                raise TypeError(\n                    pytorch_collate.default_collate_err_msg_format.format(elem.dtype)\n                )\n\n            return time_aware_collate([torch.as_tensor(b) for b in batch])\n        elif elem.shape == ():  # scalars\n            return torch.as_tensor(batch)\n    elif isinstance(elem, float):\n        return torch.tensor(batch, dtype=torch.float64)\n    elif isinstance(elem, int_classes):\n        return torch.tensor(batch)\n    elif isinstance(elem, string_classes):\n        return batch\n    elif isinstance(elem, container_abcs.Mapping):\n        return {key: time_aware_collate([d[key] for d in batch]) for key in elem}\n    elif isinstance(elem, tuple) and hasattr(elem, ""_fields""):  # namedtuple\n        return elem_type(*(time_aware_collate(samples) for samples in zip(*batch)))\n    elif isinstance(elem, container_abcs.Sequence):\n        transposed = zip(*batch)\n        return [time_aware_collate(samples) for samples in transposed]\n\n    raise TypeError(pytorch_collate.default_collate_err_msg_format.format(elem_type))\n'"
bindsnet/datasets/dataloader.py,1,"b'import torch\n\nfrom .collate import time_aware_collate\n\n\nclass DataLoader(torch.utils.data.DataLoader):\n    def __init__(\n        self,\n        dataset,\n        batch_size=1,\n        shuffle=False,\n        sampler=None,\n        batch_sampler=None,\n        num_workers=0,\n        collate_fn=time_aware_collate,\n        pin_memory=False,\n        drop_last=False,\n        timeout=0,\n        worker_init_fn=None,\n    ):\n        super().__init__(\n            dataset,\n            sampler=sampler,\n            shuffle=shuffle,\n            batch_size=batch_size,\n            drop_last=drop_last,\n            pin_memory=pin_memory,\n            timeout=timeout,\n            num_workers=num_workers,\n            worker_init_fn=worker_init_fn,\n            batch_sampler=batch_sampler,\n            collate_fn=collate_fn,\n        )\n'"
bindsnet/datasets/davis.py,1,"b'import os\nimport sys\nimport time\nimport shutil\nimport zipfile\nfrom glob import glob\nfrom collections import defaultdict\nfrom urllib.request import urlretrieve\n\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\n\n\nclass Davis(torch.utils.data.Dataset):\n    SUBSET_OPTIONS = [""train"", ""val"", ""test-dev"", ""test-challenge""]\n    TASKS = [""semi-supervised"", ""unsupervised""]\n    RESOLUTION_OPTIONS = [""480p"", ""Full-Resolution""]\n    DATASET_WEB = ""https://davischallenge.org/davis2017/code.html""\n    VOID_LABEL = 255\n\n    def __init__(\n        self,\n        root,\n        task=""unsupervised"",\n        subset=""train"",\n        sequences=""all"",\n        resolution=""480p"",\n        size=(600, 480),\n        codalab=False,\n        download=False,\n        num_samples: int = -1,\n    ):\n        # language=rst\n        """"""\n        Class to read the DAVIS dataset\n        :param root: Path to the DAVIS folder that contains JPEGImages, Annotations,\n            etc. folders.\n        :param task: Task to load the annotations, choose between semi-supervised or\n            unsupervised.\n        :param subset: Set to load the annotations\n        :param sequences: Sequences to consider, \'all\' to use all the sequences in a\n            set.\n        :param resolution: Specify the resolution to use the dataset, choose between\n            \'480\' and \'Full-Resolution\'\n        :param download: Specify whether to download the dataset if it is not present\n        :param num_samples: Number of samples to pass to the batch\n        """"""\n        super().__init__()\n\n        if subset not in self.SUBSET_OPTIONS:\n            raise ValueError(f""Subset should be in {self.SUBSET_OPTIONS}"")\n        if task not in self.TASKS:\n            raise ValueError(f""The only tasks that are supported are {self.TASKS}"")\n        if resolution not in self.RESOLUTION_OPTIONS:\n            raise ValueError(\n                f""You may only use one of these resolutions: {self.RESOLUTION_OPTIONS}""\n            )\n\n        self.task = task\n        self.subset = subset\n        self.resolution = resolution\n        self.size = size\n        self.codalab = codalab\n\n        # Sets the boolean converted if the size of the images must be scaled down\n        self.converted = not self.size == (600, 480)\n\n        # Sets a tag for naming the folder containing the dataset\n        self.tag = """"\n        if self.task == ""unsupervised"":\n            self.tag += ""Unsupervised-""\n        if self.subset == ""train"" or self.subset == ""val"":\n            self.tag += ""trainval""\n        else:\n            self.tag += self.subset\n        self.tag += ""-"" + self.resolution\n\n        # Makes a unique path for a given instance of davis\n        self.converted_root = os.path.join(\n            root, self.tag + ""-"" + str(self.size[0]) + ""x"" + str(self.size[1])\n        )\n        self.root = os.path.join(root, self.tag)\n        self.download = download\n        self.num_samples = num_samples\n        self.zip_path = os.path.join(self.root, ""repo.zip"")\n        self.img_path = os.path.join(self.root, ""JPEGImages"", resolution)\n        annotations_folder = (\n            ""Annotations"" if task == ""semi-supervised"" else ""Annotations_unsupervised""\n        )\n        self.mask_path = os.path.join(self.root, annotations_folder, resolution)\n        year = (\n            ""2019""\n            if task == ""unsupervised""\n            and (subset == ""test-dev"" or subset == ""test-challenge"")\n            else ""2017""\n        )\n        self.imagesets_path = os.path.join(self.root, ""ImageSets"", year)\n\n        # Makes a converted path for scaled images\n        if self.converted:\n            self.converted_img_path = os.path.join(\n                self.converted_root, ""JPEGImages"", resolution\n            )\n            self.converted_mask_path = os.path.join(\n                self.converted_root, annotations_folder, resolution\n            )\n            self.converted_imagesets_path = os.path.join(\n                self.converted_root, ""ImageSets"", year\n            )\n\n        # Sets seqence_names to the relevant sequences\n        if sequences == ""all"":\n            with open(\n                os.path.join(self.imagesets_path, f""{self.subset}.txt""), ""r""\n            ) as f:\n                tmp = f.readlines()\n            self.sequences_names = [x.strip() for x in tmp]\n        else:\n            self.sequences_names = (\n                sequences if isinstance(sequences, list) else [sequences]\n            )\n        self.sequences = defaultdict(dict)\n\n        # Check if Davis is installed and download it if necessary\n        self._check_directories()\n\n        # Sets the images and masks for each sequence resizing for the given size\n        for seq in self.sequences_names:\n            images = np.sort(glob(os.path.join(self.img_path, seq, ""*.jpg""))).tolist()\n            if len(images) == 0 and not self.codalab:\n                raise FileNotFoundError(f""Images for sequence {seq} not found."")\n            self.sequences[seq][""images""] = images\n            masks = np.sort(glob(os.path.join(self.mask_path, seq, ""*.png""))).tolist()\n            masks.extend([-1] * (len(images) - len(masks)))\n            self.sequences[seq][""masks""] = masks\n\n        # Creates an enumeration for the sequences for __getitem__\n        self.enum_sequences = []\n        for seq in self.sequences_names:\n            self.enum_sequences.append(self.sequences[seq])\n\n    def __len__(self):\n        # language=rst\n        """"""\n        Calculates the number of sequences the dataset holds.\n\n        :return: The number of sequences in the dataset.\n        """"""\n        return len(self.sequences)\n\n    def _convert_sequences(self):\n        # language=rst\n        """"""\n        Creates a new root for the dataset to be converted and placed into,\n        then copies each image and mask into the given size and stores correctly.\n        """"""\n        os.makedirs(os.path.join(self.converted_imagesets_path, f""{self.subset}.txt""))\n        os.makedirs(self.converted_img_path)\n        os.makedirs(self.converted_mask_path)\n\n        shutil.copy(\n            os.path.join(self.imagesets_path, f""{self.subset}.txt""),\n            os.path.join(self.converted_imagesets_path, f""{self.subset}.txt""),\n        )\n\n        print(""Converting sequences to size: {0}"".format(self.size))\n        for seq in tqdm(self.sequences_names):\n            os.makedirs(os.path.join(self.converted_img_path, seq))\n            os.makedirs(os.path.join(self.converted_mask_path, seq))\n            images = np.sort(glob(os.path.join(self.img_path, seq, ""*.jpg""))).tolist()\n            if len(images) == 0 and not self.codalab:\n                raise FileNotFoundError(f""Images for sequence {seq} not found."")\n            for ind, img in enumerate(images):\n                im = Image.open(img)\n                im.thumbnail(self.size, Image.ANTIALIAS)\n                im.save(\n                    os.path.join(\n                        self.converted_img_path, seq, str(ind).zfill(5) + "".jpg""\n                    )\n                )\n            masks = np.sort(glob(os.path.join(self.mask_path, seq, ""*.png""))).tolist()\n            for ind, msk in enumerate(masks):\n                im = Image.open(msk)\n                im.thumbnail(self.size, Image.ANTIALIAS)\n                im.convert(""RGB"").save(\n                    os.path.join(\n                        self.converted_mask_path, seq, str(ind).zfill(5) + "".png""\n                    )\n                )\n\n    def _check_directories(self):\n        # language=rst\n        """"""\n        Verifies that the correct dataset is downloaded; downloads if it isn\'t and\n        ``download=True``.\n\n        :raises: FileNotFoundError if the subset sequence, annotation or root folder is\n            missing.\n        """"""\n        if not os.path.exists(self.root):\n            if self.download:\n                self._download()\n            else:\n                raise FileNotFoundError(\n                    f""DAVIS not found in the specified directory, download it from ""\n                    f""{self.DATASET_WEB} or add download=True to your call""\n                )\n        if not os.path.exists(os.path.join(self.imagesets_path, f""{self.subset}.txt"")):\n            raise FileNotFoundError(\n                f""Subset sequences list for {self.subset} not found, download the ""\n                f""missing subset for the {self.task} task from {self.DATASET_WEB}""\n            )\n        if self.subset in [""train"", ""val""] and not os.path.exists(self.mask_path):\n            raise FileNotFoundError(\n                f""Annotations folder for the {self.task} task not found, ""\n                f""download it from {self.DATASET_WEB}""\n            )\n        if self.converted:\n            if not os.path.exists(self.converted_img_path):\n                self._convert_sequences()\n            self.img_path = self.converted_img_path\n            self.mask_path = self.converted_mask_path\n            self.imagesets_path = self.converted_imagesets_path\n\n    def get_frames(self, sequence):\n        for img, msk in zip(\n            self.sequences[sequence][""images""], self.sequences[sequence][""masks""]\n        ):\n            image = np.array(Image.open(img))\n            mask = None if msk is None else np.array(Image.open(msk))\n            yield image, mask\n\n    def _get_all_elements(self, sequence, obj_type):\n        obj = np.array(Image.open(self.sequences[sequence][obj_type][0]))\n        all_objs = np.zeros((len(self.sequences[sequence][obj_type]), *obj.shape))\n        obj_id = []\n        for i, obj in enumerate(self.sequences[sequence][obj_type]):\n            all_objs[i, ...] = np.array(Image.open(obj))\n            obj_id.append("""".join(obj.split(""/"")[-1].split(""."")[:-1]))\n        return all_objs, obj_id\n\n    def get_all_images(self, sequence):\n        return self._get_all_elements(sequence, ""images"")\n\n    def get_all_masks(self, sequence, separate_objects_masks=False):\n        masks, masks_id = self._get_all_elements(sequence, ""masks"")\n        masks_void = np.zeros_like(masks)\n\n        # Separate void and object masks\n        for i in range(masks.shape[0]):\n            masks_void[i, ...] = masks[i, ...] == 255\n            masks[i, masks[i, ...] == 255] = 0\n\n        if separate_objects_masks:\n            num_objects = int(np.max(masks[0, ...]))\n            tmp = np.ones((num_objects, *masks.shape))\n            tmp = tmp * np.arange(1, num_objects + 1)[:, None, None, None]\n            masks = tmp == masks[None, ...]\n            masks = masks > 0\n        return masks, masks_void, masks_id\n\n    def get_sequences(self):\n        for seq in self.sequences:\n            yield seq\n\n    def _download(self):\n        # language=rst\n        """"""\n        Downloads the correct dataset based on the given parameters.\n\n        Relies on ``self.tag`` to determine both the name of the folder created for the\n        dataset and for the finding the correct download url.\n        """"""\n        os.makedirs(self.root)\n\n        # Grabs the correct zip url based on parameters\n        zip_url = f""https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-{self.tag}.zip""\n\n        print(""\\nDownloading Davis data set from "" + zip_url + ""\\n"")\n\n        # Downloads the relevant dataset\n        urlretrieve(zip_url, self.zip_path, reporthook=self.progress)\n\n        print(""\\nDone! \\n\\nUnzipping and restructuring"")\n\n        # Extracts the dataset\n        z = zipfile.ZipFile(self.zip_path, ""r"")\n        z.extractall(path=self.root)\n        z.close()\n        os.remove(self.zip_path)\n\n        temp_folder = os.path.join(self.root, ""DAVIS\\\\"")\n\n        # Deletes an unnecessary containing folder ""DAVIS"" which comes with every download\n        for file in os.listdir(temp_folder):\n            shutil.move(temp_folder + file, self.root)\n        cwd = os.getcwd()\n        os.chdir(self.root)\n        os.rmdir(""DAVIS"")\n        os.chdir(cwd)\n\n        print(""\\nDone!\\n"")\n\n    def __getitem__(self, ind):\n        # language=rst\n        """"""\n        Gets an item of the ``Dataset`` based on index.\n\n        :param ind: Index of item to take from dataset.\n        :return: A sequence which contains a list of images and masks.\n        """"""\n        seq = self.enum_sequences[ind]\n        return seq\n\n    @staticmethod\n    def progress(count, block_size, total_size):\n        # language=rst\n        """"""\n        Simple progress indicator for the download of the dataset.\n        """"""\n        global start_time\n        if count == 0:\n            start_time = time.time()\n            return\n        duration = time.time() - start_time\n        progress_size = int(count * block_size)\n        speed = int(progress_size / (1024 * duration))\n        percent = min(int(count * block_size * 100 / total_size), 100)\n        sys.stdout.write(\n            ""\\r...%d%%, %d MB, %d KB/s, %d seconds passed""\n            % (percent, progress_size / (1024 * 1024), speed, duration)\n        )\n        sys.stdout.flush()\n'"
bindsnet/datasets/preprocess.py,1,"b'import cv2\nimport math\nimport warnings\nimport numpy as np\nfrom torchvision import transforms\n\n\ndef gray_scale(image: np.ndarray) -> np.ndarray:\n    # language=rst\n    """"""\n    Converts RGB image into grayscale.\n\n    :param image: RGB image.\n    :return: Gray-scaled image.\n    """"""\n    return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n\ndef crop(image: np.ndarray, x1: int, x2: int, y1: int, y2: int) -> np.ndarray:\n    # language=rst\n    """"""\n    Crops an image given coordinates of cropping box.\n\n    :param image: 3-dimensional image.\n    :param x1: Left x coordinate.\n    :param x2: Right x coordinate.\n    :param y1: Bottom y coordinate.\n    :param y2: Top y coordinate.\n    :return: Image cropped using coordinates (x1, x2, y1, y2).\n    """"""\n    return image[x1:x2, y1:y2, :]\n\n\ndef binary_image(image: np.ndarray) -> np.ndarray:\n    # language=rst\n    """"""\n    Converts input image into black and white (binary)\n\n    :param image: Gray-scaled image.\n    :return: Black and white image.\n    """"""\n    return cv2.threshold(image, 0, 1, cv2.THRESH_BINARY)[1]\n\n\ndef subsample(image: np.ndarray, x: int, y: int) -> np.ndarray:\n    # language=rst\n    """"""\n    Scale the image to (x, y).\n\n    :param image: Image to be rescaled.\n    :param x: Output value for ``image``\'s x dimension.\n    :param y: Output value for ``image``\'s y dimension.\n    :return: Re-scaled image.\n    """"""\n    return cv2.resize(image, (x, y))\n\n\n"""""" Below is the implementation of necessary preprocessing for the dataset ALOV300 """"""\n\n\nclass Rescale(object):\n    """"""Rescale image and bounding box.\n    Args:\n        output_size (tuple or int): Desired output size. If int, square crop\n        is made.\n    """"""\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample, opts):\n        image, bb = sample[""image""], sample[""bb""]\n        h, w = image.shape[:2]\n        if isinstance(self.output_size, int):\n            if h > w:\n                new_h, new_w = self.output_size * h / w, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w / h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n        # make sure that gray image has 3 channels\n        img = cv2.resize(image, (new_h, new_w), interpolation=cv2.INTER_CUBIC)\n        bbox = BoundingBox(bb[0], bb[1], bb[2], bb[3])\n        bbox.scale(opts[""search_region""])\n        return {""image"": img, ""bb"": bbox.get_bb_list()}\n\n\ndef bgr2rgb(image):\n    if image.ndim == 2:\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n    else:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image\n\n\ndef shift_crop_training_sample(sample, bb_params):\n    """"""\n    Given an image with bounding box, this method randomly shifts the box and\n    generates a training example. It returns current image crop with shifted\n    box (with respect to current image).\n    """"""\n    output_sample = {}\n    opts = {}\n    currimg = sample[""image""]\n    currbb = sample[""bb""]\n    bbox_curr_gt = BoundingBox(currbb[0], currbb[1], currbb[2], currbb[3])\n    bbox_curr_shift = BoundingBox(0, 0, 0, 0)\n    bbox_curr_shift = bbox_curr_gt.shift(\n        currimg,\n        bb_params[""lambda_scale_frac""],\n        bb_params[""lambda_shift_frac""],\n        bb_params[""min_scale""],\n        bb_params[""max_scale""],\n        True,\n        bbox_curr_shift,\n    )\n    (\n        rand_search_region,\n        rand_search_location,\n        edge_spacing_x,\n        edge_spacing_y,\n    ) = cropPadImage(bbox_curr_shift, currimg)\n    bbox_curr_gt = BoundingBox(currbb[0], currbb[1], currbb[2], currbb[3])\n    bbox_gt_recentered = BoundingBox(0, 0, 0, 0)\n    bbox_gt_recentered = bbox_curr_gt.recenter(\n        rand_search_location, edge_spacing_x, edge_spacing_y, bbox_gt_recentered\n    )\n    output_sample[""image""] = rand_search_region\n    output_sample[""bb""] = bbox_gt_recentered.get_bb_list()\n\n    # additional options for visualization\n\n    opts[""edge_spacing_x""] = edge_spacing_x\n    opts[""edge_spacing_y""] = edge_spacing_y\n    opts[""search_location""] = rand_search_location\n    opts[""search_region""] = rand_search_region\n    return output_sample, opts\n\n\ndef crop_sample(sample):\n    """"""\n    Given a sample image with bounding box, this method returns the image crop\n    at the bounding box location with twice the width and height for context.\n    """"""\n    output_sample = {}\n    opts = {}\n    image, bb = sample[""image""], sample[""bb""]\n    orig_bbox = BoundingBox(bb[0], bb[1], bb[2], bb[3])\n    (output_image, pad_image_location, edge_spacing_x, edge_spacing_y) = cropPadImage(\n        orig_bbox, image\n    )\n    new_bbox = BoundingBox(0, 0, 0, 0)\n    new_bbox = new_bbox.recenter(\n        pad_image_location, edge_spacing_x, edge_spacing_y, new_bbox\n    )\n    output_sample[""image""] = output_image\n    output_sample[""bb""] = new_bbox.get_bb_list()\n\n    # additional options for visualization\n    opts[""edge_spacing_x""] = edge_spacing_x\n    opts[""edge_spacing_y""] = edge_spacing_y\n    opts[""search_location""] = pad_image_location\n    opts[""search_region""] = output_image\n    return output_sample, opts\n\n\ndef cropPadImage(bbox_tight, image):\n    pad_image_location = computeCropPadImageLocation(bbox_tight, image)\n    roi_left = min(pad_image_location.x1, (image.shape[1] - 1))\n    roi_bottom = min(pad_image_location.y1, (image.shape[0] - 1))\n    roi_width = min(\n        image.shape[1],\n        max(1.0, math.ceil(pad_image_location.x2 - pad_image_location.x1)),\n    )\n    roi_height = min(\n        image.shape[0],\n        max(1.0, math.ceil(pad_image_location.y2 - pad_image_location.y1)),\n    )\n\n    err = 0.000000001  # To take care of floating point arithmetic errors\n    cropped_image = image[\n        int(roi_bottom + err) : int(roi_bottom + roi_height),\n        int(roi_left + err) : int(roi_left + roi_width),\n    ]\n    output_width = max(math.ceil(bbox_tight.compute_output_width()), roi_width)\n    output_height = max(math.ceil(bbox_tight.compute_output_height()), roi_height)\n    if image.ndim > 2:\n        output_image = np.zeros(\n            (int(output_height), int(output_width), image.shape[2]), dtype=image.dtype\n        )\n    else:\n        output_image = np.zeros(\n            (int(output_height), int(output_width)), dtype=image.dtype\n        )\n\n    edge_spacing_x = min(bbox_tight.edge_spacing_x(), (image.shape[1] - 1))\n    edge_spacing_y = min(bbox_tight.edge_spacing_y(), (image.shape[0] - 1))\n\n    # rounding should be done to match the width and height\n    output_image[\n        int(edge_spacing_y) : int(edge_spacing_y) + cropped_image.shape[0],\n        int(edge_spacing_x) : int(edge_spacing_x) + cropped_image.shape[1],\n    ] = cropped_image\n    return output_image, pad_image_location, edge_spacing_x, edge_spacing_y\n\n\ndef computeCropPadImageLocation(bbox_tight, image):\n    # Center of the bounding box\n    bbox_center_x = bbox_tight.get_center_x()\n    bbox_center_y = bbox_tight.get_center_y()\n\n    image_height = image.shape[0]\n    image_width = image.shape[1]\n\n    # Padded output width and height\n    output_width = bbox_tight.compute_output_width()\n    output_height = bbox_tight.compute_output_height()\n\n    roi_left = max(0.0, bbox_center_x - (output_width / 2.0))\n    roi_bottom = max(0.0, bbox_center_y - (output_height / 2.0))\n\n    # Padded roi width\n    left_half = min(output_width / 2.0, bbox_center_x)\n    right_half = min(output_width / 2.0, image_width - bbox_center_x)\n    roi_width = max(1.0, left_half + right_half)\n\n    # Padded roi height\n    top_half = min(output_height / 2.0, bbox_center_y)\n    bottom_half = min(output_height / 2.0, image_height - bbox_center_y)\n    roi_height = max(1.0, top_half + bottom_half)\n\n    # Padded image location in the original image\n    objPadImageLocation = BoundingBox(\n        roi_left, roi_bottom, roi_left + roi_width, roi_bottom + roi_height\n    )\n    return objPadImageLocation\n\n\ndef sample_rand_uniform():\n    RAND_MAX = 2147483647\n    return (random.randint(0, RAND_MAX) + 1) * 1.0 / (RAND_MAX + 2)\n\n\ndef sample_exp_two_sides(lambda_):\n    RAND_MAX = 2147483647\n    pos_or_neg = random.randint(0, RAND_MAX)\n    if (pos_or_neg % 2) == 0:\n        pos_or_neg = 1\n    else:\n        pos_or_neg = -1\n\n    rand_uniform = sample_rand_uniform()\n    return math.log(rand_uniform) / (lambda_ * pos_or_neg)\n\n\n"""""" implementation of bounding box class for cropping images """"""\n\n\nclass BoundingBox:\n    def __init__(self, x1, y1, x2, y2):\n\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n        self.kContextFactor = 2\n        self.kScaleFactor = 10\n\n    def print_bb(self):\n        print(""------Bounding-box-------"")\n        print(""(x1, y1): ({}, {})"".format(self.x1, self.y1))\n        print(""(x2, y2): ({}, {})"".format(self.x2, self.y2))\n        print(""(w, h)  : ({}, {})"".format(self.x2 - self.x1 + 1, self.y2 - self.y1 + 1))\n        print(""--------------------------"")\n\n    def get_bb_list(self):\n        return [self.x1, self.y1, self.x2, self.y2]\n\n    def get_center_x(self):\n        return (self.x1 + self.x2) / 2.0\n\n    def get_center_y(self):\n        return (self.y1 + self.y2) / 2.0\n\n    def compute_output_height(self):\n        bbox_height = self.y2 - self.y1\n        output_height = self.kContextFactor * bbox_height\n\n        return max(1.0, output_height)\n\n    def compute_output_width(self):\n        bbox_width = self.x2 - self.x1\n        output_width = self.kContextFactor * bbox_width\n\n        return max(1.0, output_width)\n\n    def edge_spacing_x(self):\n        output_width = self.compute_output_width()\n        bbox_center_x = self.get_center_x()\n\n        return max(0.0, (output_width / 2) - bbox_center_x)\n\n    def edge_spacing_y(self):\n        output_height = self.compute_output_height()\n        bbox_center_y = self.get_center_y()\n\n        return max(0.0, (output_height / 2) - bbox_center_y)\n\n    def unscale(self, image):\n        height = image.shape[0]\n        width = image.shape[1]\n\n        self.x1 = self.x1 / self.kScaleFactor\n        self.x2 = self.x2 / self.kScaleFactor\n        self.y1 = self.y1 / self.kScaleFactor\n        self.y2 = self.y2 / self.kScaleFactor\n\n        self.x1 = self.x1 * width\n        self.x2 = self.x2 * width\n        self.y1 = self.y1 * height\n        self.y2 = self.y2 * height\n\n    def uncenter(self, raw_image, search_location, edge_spacing_x, edge_spacing_y):\n        self.x1 = max(0.0, self.x1 + search_location.x1 - edge_spacing_x)\n        self.y1 = max(0.0, self.y1 + search_location.y1 - edge_spacing_y)\n        self.x2 = min(raw_image.shape[1], self.x2 + search_location.x1 - edge_spacing_x)\n        self.y2 = min(raw_image.shape[0], self.y2 + search_location.y1 - edge_spacing_y)\n\n    def recenter(self, search_loc, edge_spacing_x, edge_spacing_y, bbox_gt_recentered):\n        bbox_gt_recentered.x1 = self.x1 - search_loc.x1 + edge_spacing_x\n        bbox_gt_recentered.y1 = self.y1 - search_loc.y1 + edge_spacing_y\n        bbox_gt_recentered.x2 = self.x2 - search_loc.x1 + edge_spacing_x\n        bbox_gt_recentered.y2 = self.y2 - search_loc.y1 + edge_spacing_y\n\n        return bbox_gt_recentered\n\n    def scale(self, image):\n        height = image.shape[0]\n        width = image.shape[1]\n\n        self.x1 = self.x1 / width\n        self.y1 = self.y1 / height\n        self.x2 = self.x2 / width\n        self.y2 = self.y2 / height\n\n        self.x1 = self.x1 * self.kScaleFactor\n        self.y1 = self.y1 * self.kScaleFactor\n        self.x2 = self.x2 * self.kScaleFactor\n        self.y2 = self.y2 * self.kScaleFactor\n\n    def get_width(self):\n        return self.x2 - self.x1\n\n    def get_height(self):\n        return self.y2 - self.y1\n\n    def shift(\n        self,\n        image,\n        lambda_scale_frac,\n        lambda_shift_frac,\n        min_scale,\n        max_scale,\n        shift_motion_model,\n        bbox_rand,\n    ):\n        width = self.get_width()\n        height = self.get_height()\n\n        center_x = self.get_center_x()\n        center_y = self.get_center_y()\n\n        kMaxNumTries = 10\n\n        new_width = -1\n        num_tries_width = 0\n        while ((new_width < 0) or (new_width > image.shape[1] - 1)) and (\n            num_tries_width < kMaxNumTries\n        ):\n            if shift_motion_model:\n                width_scale_factor = max(\n                    min_scale, min(max_scale, sample_exp_two_sides(lambda_scale_frac))\n                )\n            else:\n                rand_num = sample_rand_uniform()\n                width_scale_factor = rand_num * (max_scale - min_scale) + min_scale\n\n            new_width = width * (1 + width_scale_factor)\n            new_width = max(1.0, min((image.shape[1] - 1), new_width))\n            num_tries_width = num_tries_width + 1\n\n        new_height = -1\n        num_tries_height = 0\n        while ((new_height < 0) or (new_height > image.shape[0] - 1)) and (\n            num_tries_height < kMaxNumTries\n        ):\n            if shift_motion_model:\n                height_scale_factor = max(\n                    min_scale, min(max_scale, sample_exp_two_sides(lambda_scale_frac))\n                )\n            else:\n                rand_num = sample_rand_uniform()\n                height_scale_factor = rand_num * (max_scale - min_scale) + min_scale\n\n            new_height = height * (1 + height_scale_factor)\n            new_height = max(1.0, min((image.shape[0] - 1), new_height))\n            num_tries_height = num_tries_height + 1\n\n        first_time_x = True\n        new_center_x = -1\n        num_tries_x = 0\n\n        while (\n            first_time_x\n            or (new_center_x < center_x - width * self.kContextFactor / 2)\n            or (new_center_x > center_x + width * self.kContextFactor / 2)\n            or ((new_center_x - new_width / 2) < 0)\n            or ((new_center_x + new_width / 2) > image.shape[1])\n        ) and (num_tries_x < kMaxNumTries):\n\n            if shift_motion_model:\n                new_x_temp = center_x + width * sample_exp_two_sides(lambda_shift_frac)\n            else:\n                rand_num = sample_rand_uniform()\n                new_x_temp = center_x + rand_num * (2 * new_width) - new_width\n\n            new_center_x = min(\n                image.shape[1] - new_width / 2, max(new_width / 2, new_x_temp)\n            )\n            first_time_x = False\n            num_tries_x = num_tries_x + 1\n\n        first_time_y = True\n        new_center_y = -1\n        num_tries_y = 0\n\n        while (\n            first_time_y\n            or (new_center_y < center_y - height * self.kContextFactor / 2)\n            or (new_center_y > center_y + height * self.kContextFactor / 2)\n            or ((new_center_y - new_height / 2) < 0)\n            or ((new_center_y + new_height / 2) > image.shape[0])\n        ) and (num_tries_y < kMaxNumTries):\n\n            if shift_motion_model:\n                new_y_temp = center_y + height * sample_exp_two_sides(lambda_shift_frac)\n            else:\n                rand_num = sample_rand_uniform()\n                new_y_temp = center_y + rand_num * (2 * new_height) - new_height\n\n            new_center_y = min(\n                image.shape[0] - new_height / 2, max(new_height / 2, new_y_temp)\n            )\n            first_time_y = False\n            num_tries_y = num_tries_y + 1\n\n        bbox_rand.x1 = new_center_x - new_width / 2\n        bbox_rand.x2 = new_center_x + new_width / 2\n        bbox_rand.y1 = new_center_y - new_height / 2\n        bbox_rand.y2 = new_center_y + new_height / 2\n\n        return bbox_rand\n\n\nclass NormalizeToTensor(object):\n    """"""Returns torch tensor normalized images.""""""\n\n    def __call__(self, sample):\n        prev_img, curr_img = sample[""previmg""], sample[""currimg""]\n        self.transform = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n                ),\n            ]\n        )\n        prev_img = self.transform(prev_img)\n        curr_img = self.transform(curr_img)\n        if ""currbb"" in sample:\n            currbb = np.array(sample[""currbb""])\n            return {\n                ""previmg"": prev_img,\n                ""currimg"": curr_img,\n                ""currbb"": torch.from_numpy(currbb).float(),\n            }\n        else:\n            return {""previmg"": prev_img, ""currimg"": curr_img}\n\n\n# Copyright (c) 2018 Abhinav Moudgil\n'"
bindsnet/datasets/spoken_mnist.py,16,"b'from typing import Optional, Tuple, List, Iterable\nimport os\nimport torch\nimport numpy as np\nimport shutil\nimport zipfile\n\n\nfrom urllib.request import urlretrieve\nfrom scipy.io import wavfile\n\nimport warnings\n\n\nclass SpokenMNIST(torch.utils.data.Dataset):\n    # language=rst\n    """"""\n    Handles loading and saving of the Spoken MNIST audio dataset `(link)\n    <https://github.com/Jakobovski/free-spoken-digit-dataset>`_.\n    """"""\n    train_pickle = ""train.pt""\n    test_pickle = ""test.pt""\n\n    url = ""https://github.com/Jakobovski/free-spoken-digit-dataset/archive/master.zip""\n\n    files = []\n    for digit in range(10):\n        for speaker in [""jackson"", ""nicolas"", ""theo""]:\n            for example in range(50):\n                files.append(""_"".join([str(digit), speaker, str(example)]) + "".wav"")\n\n    n_files = len(files)\n\n    def __init__(\n        self,\n        path: str,\n        download: bool = False,\n        shuffle: bool = True,\n        train: bool = True,\n        split: float = 0.8,\n        num_samples: int = -1,\n    ) -> None:\n        # language=rst\n        """"""\n        Constructor for the ``SpokenMNIST`` object. Makes the data directory if it\n        doesn\'t already exist.\n\n        :param path: Pathname of directory in which to store the dataset.\n        :param download: Whether or not to download the dataset (requires internet\n            connection).\n        :param shuffle: Whether to randomly permute order of dataset.\n        :param train: Load training split if true else load test split\n        :param split: Train, test split; in range ``(0, 1)``.\n        :param num_samples: Number of samples to pass to the batch\n        """"""\n        super().__init__()\n\n        if not os.path.isdir(path):\n            os.makedirs(path)\n\n        self.path = path\n        self.download = download\n        self.shuffle = shuffle\n\n        self.zip_path = os.path.join(path, ""repo.zip"")\n\n        if train:\n            self.audio, self.labels = self._get_train(split)\n        else:\n            self.audio, self.labels = self._get_test(split)\n\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return len(self.audio)\n\n    def __getitem__(self, ind):\n        audio = self.audio[ind][: self.num_samples, :]\n        label = self.labels[ind]\n\n        return {""audio"": audio, ""label"": label}\n\n    def _get_train(self, split: float = 0.8) -> Tuple[torch.Tensor, torch.Tensor]:\n        # language=rst\n        """"""\n        Gets the Spoken MNIST training audio and labels.\n\n        :param split: Train, test split; in range ``(0, 1)``.\n        :return: Spoken MNIST training audio and labels.\n        """"""\n        split_index = int(split * SpokenMNIST.n_files)\n        path = os.path.join(self.path, ""_"".join([SpokenMNIST.train_pickle, str(split)]))\n\n        if not all([os.path.isfile(os.path.join(self.path, f)) for f in self.files]):\n            # Download data if it isn\'t on disk.\n            if self.download:\n                print(""Downloading Spoken MNIST data.\\n"")\n                self._download()\n\n                # Process data into audio, label (input, output) pairs.\n                audio, labels = self.process_data(SpokenMNIST.files[:split_index])\n\n                # Serialize image data on disk for next time.\n                torch.save((audio, labels), open(path, ""wb""))\n            else:\n                msg = ""Dataset not found on disk; specify \'download=True\' to allow downloads.""\n                raise FileNotFoundError(msg)\n        else:\n            if not os.path.isdir(path):\n                # Process image and label data if pickled file doesn\'t exist.\n                audio, labels = self.process_data(SpokenMNIST.files)\n\n                # Serialize image data on disk for next time.\n                torch.save((audio, labels), open(path, ""wb""))\n            else:\n                # Load image data from disk if it has already been processed.\n                print(""Loading training data from serialized object file.\\n"")\n                audio, labels = torch.load(open(path, ""rb""))\n\n        labels = torch.Tensor(labels)\n\n        if self.shuffle:\n            perm = np.random.permutation(np.arange(labels.shape[0]))\n            audio, labels = [torch.Tensor(audio[_]) for _ in perm], labels[perm]\n\n        return audio, torch.Tensor(labels)\n\n    def _get_test(self, split: float = 0.8) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n        # language=rst\n        """"""\n        Gets the Spoken MNIST training audio and labels.\n\n        :param split: Train, test split; in range ``(0, 1)``.\n        :return: The Spoken MNIST test audio and labels.\n        """"""\n        split_index = int(split * SpokenMNIST.n_files)\n        path = os.path.join(self.path, ""_"".join([SpokenMNIST.test_pickle, str(split)]))\n\n        if not all([os.path.isfile(os.path.join(self.path, f)) for f in self.files]):\n            # Download data if it isn\'t on disk.\n            if self.download:\n                print(""Downloading Spoken MNIST data.\\n"")\n                self._download()\n\n                # Process data into audio, label (input, output) pairs.\n                audio, labels = self.process_data(SpokenMNIST.files[split_index:])\n\n                # Serialize image data on disk for next time.\n                torch.save((audio, labels), open(path, ""wb""))\n            else:\n                msg = ""Dataset not found on disk; specify \'download=True\' to allow downloads.""\n                raise FileNotFoundError(msg)\n        else:\n            if not os.path.isdir(path):\n                # Process image and label data if pickled file doesn\'t exist.\n                audio, labels = self.process_data(SpokenMNIST.files)\n\n                # Serialize image data on disk for next time.\n                torch.save((audio, labels), open(path, ""wb""))\n            else:\n                # Load image data from disk if it has already been processed.\n                print(""Loading test data from serialized object file.\\n"")\n                audio, labels = torch.load(open(path, ""rb""))\n\n        labels = torch.Tensor(labels)\n\n        if self.shuffle:\n            perm = np.random.permutation(np.arange(labels.shape[0]))\n            audio, labels = audio[perm], labels[perm]\n\n        return audio, torch.Tensor(labels)\n\n    def _download(self) -> None:\n        # language=rst\n        """"""\n        Downloads and unzips all Spoken MNIST data.\n        """"""\n        urlretrieve(SpokenMNIST.url, self.zip_path)\n\n        z = zipfile.ZipFile(self.zip_path, ""r"")\n        z.extractall(path=self.path)\n        z.close()\n\n        path = os.path.join(self.path, ""free-spoken-digit-dataset-master"", ""recordings"")\n        for f in os.listdir(path):\n            shutil.move(os.path.join(path, f), os.path.join(self.path))\n\n        cwd = os.getcwd()\n        os.chdir(self.path)\n        shutil.rmtree(""free-spoken-digit-dataset-master"")\n        os.chdir(cwd)\n\n    def process_data(\n        self, file_names: Iterable[str]\n    ) -> Tuple[List[torch.Tensor], torch.Tensor]:\n        # language=rst\n        """"""\n        Opens files of Spoken MNIST data and processes them into ``numpy`` arrays.\n\n        :param file_names: Names of the files containing Spoken MNIST audio to load.\n        :return: Processed Spoken MNIST audio and label data.\n        """"""\n        audio, labels = [], []\n\n        for f in file_names:\n            label = int(f.split(""_"")[0])\n\n            sample_rate, signal = wavfile.read(os.path.join(self.path, f))\n            pre_emphasis = 0.97\n            emphasized_signal = np.append(\n                signal[0], signal[1:] - pre_emphasis * signal[:-1]\n            )\n\n            # Popular settings are 25 ms for the frame size and a 10 ms stride (15 ms overlap)\n            frame_size = 0.025\n            frame_stride = 0.01\n\n            # Convert from seconds to samples\n            frame_length, frame_step = (\n                frame_size * sample_rate,\n                frame_stride * sample_rate,\n            )\n            signal_length = len(emphasized_signal)\n            frame_length = int(round(frame_length))\n            frame_step = int(round(frame_step))\n\n            # Make sure that we have at least 1 frame\n            num_frames = int(\n                np.ceil(float(np.abs(signal_length - frame_length)) / frame_step)\n            )\n\n            pad_signal_length = num_frames * frame_step + frame_length\n            z = np.zeros((pad_signal_length - signal_length))\n            pad_signal = np.append(emphasized_signal, z)  # Pad signal\n\n            indices = (\n                np.tile(np.arange(0, frame_length), (num_frames, 1))\n                + np.tile(\n                    np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)\n                ).T\n            )\n            frames = pad_signal[indices.astype(np.int32, copy=False)]\n\n            # Hamming Window\n            frames *= np.hamming(frame_length)\n\n            # Fast Fourier Transform and Power Spectrum\n            NFFT = 512\n            mag_frames = np.absolute(np.fft.rfft(frames, NFFT))  # Magnitude of the FFT\n            pow_frames = (1.0 / NFFT) * (mag_frames ** 2)  # Power Spectrum\n\n            # Log filter banks\n            nfilt = 40\n            low_freq_mel = 0\n            high_freq_mel = 2595 * np.log10(\n                1 + (sample_rate / 2) / 700\n            )  # Convert Hz to Mel\n            mel_points = np.linspace(\n                low_freq_mel, high_freq_mel, nfilt + 2\n            )  # Equally spaced in Mel scale\n            hz_points = 700 * (10 ** (mel_points / 2595) - 1)  # Convert Mel to Hz\n            bin = np.floor((NFFT + 1) * hz_points / sample_rate)\n\n            fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\n            for m in range(1, nfilt + 1):\n                f_m_minus = int(bin[m - 1])  # left\n                f_m = int(bin[m])  # center\n                f_m_plus = int(bin[m + 1])  # right\n\n                for k in range(f_m_minus, f_m):\n                    fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n                for k in range(f_m, f_m_plus):\n                    fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n\n            filter_banks = np.dot(pow_frames, fbank.T)\n            filter_banks = np.where(\n                filter_banks == 0, np.finfo(float).eps, filter_banks\n            )  # Numerical Stability\n            filter_banks = 20 * np.log10(filter_banks)  # dB\n\n            audio.append(filter_banks), labels.append(label)\n\n        return audio, torch.Tensor(labels)\n'"
bindsnet/datasets/torchvision_wrapper.py,2,"b'from typing import Optional, Dict\n\nimport torch\nimport torchvision\n\nfrom ..encoding import Encoder, NullEncoder\n\n\ndef create_torchvision_dataset_wrapper(ds_type):\n    # language=rst\n    """"""\n    Creates wrapper classes for datasets that output ``(image, label)`` from\n    ``__getitem__``. This applies to all of the datasets inside of ``torchvision``.\n    """"""\n    if type(ds_type) == str:\n        ds_type = getattr(torchvision.datasets, ds_type)\n\n    class TorchvisionDatasetWrapper(ds_type):\n        __doc__ = (\n            """"""BindsNET torchvision dataset wrapper for:\n\n        The core difference is the output of __getitem__ is no longer\n        (image, label) rather a dictionary containing the image, label,\n        and their encoded versions if encoders were provided.\n\n            \\n\\n""""""\n            + str(ds_type)\n            if ds_type.__doc__ is None\n            else ds_type.__doc__\n        )\n\n        def __init__(\n            self,\n            image_encoder: Optional[Encoder] = None,\n            label_encoder: Optional[Encoder] = None,\n            *args,\n            **kwargs\n        ):\n            # language=rst\n            """"""\n            Constructor for the BindsNET torchvision dataset wrapper.\n            For details on the dataset you\'re interested in visit\n\n            https://pytorch.org/docs/stable/torchvision/datasets.html\n\n            :param image_encoder: Spike encoder for use on the image\n            :param label_encoder: Spike encoder for use on the label\n            :param *args: Arguments for the original dataset\n            :param **kwargs: Keyword arguments for the original dataset\n            """"""\n            super().__init__(*args, **kwargs)\n\n            self.args = args\n            self.kwargs = kwargs\n\n            # Allow the passthrough of None, but change to NullEncoder\n            if image_encoder is None:\n                image_encoder = NullEncoder()\n\n            if label_encoder is None:\n                label_encoder = NullEncoder()\n\n            self.image_encoder = image_encoder\n            self.label_encoder = label_encoder\n\n        def __getitem__(self, ind: int) -> Dict[str, torch.Tensor]:\n            # language=rst\n            """"""\n            Utilizes the ``torchvision.dataset`` parent class to grab the data, then\n            encodes using the supplied encoders.\n\n            :param int ind: Index to grab data at.\n            :return: The relevant data and encoded data from the requested index.\n            """"""\n            image, label = super().__getitem__(ind)\n\n            output = {\n                ""image"": image,\n                ""label"": label,\n                ""encoded_image"": self.image_encoder(image),\n                ""encoded_label"": self.label_encoder(label),\n            }\n\n            return output\n\n        def __len__(self):\n            return super().__len__()\n\n    return TorchvisionDatasetWrapper\n'"
bindsnet/encoding/__init__.py,0,"b'from .encodings import single, repeat, bernoulli, poisson, rank_order\nfrom .loaders import bernoulli_loader, poisson_loader, rank_order_loader\nfrom .encoders import (\n    Encoder,\n    NullEncoder,\n    SingleEncoder,\n    RepeatEncoder,\n    BernoulliEncoder,\n    PoissonEncoder,\n    RankOrderEncoder,\n)\n'"
bindsnet/encoding/encoders.py,1,"b'from . import encodings\n\n\nclass Encoder:\n    # language=rst\n    """"""\n    Base class for spike encodings transforms.\n\n    Calls ``self.enc`` from the subclass and passes whatever arguments were provided.\n    ``self.enc`` must be callable with ``torch.Tensor``, ``*args``, ``**kwargs``\n    """"""\n\n    def __init__(self, *args, **kwargs) -> None:\n        self.enc_args = args\n        self.enc_kwargs = kwargs\n\n    def __call__(self, img):\n        return self.enc(img, *self.enc_args, **self.enc_kwargs)\n\n\nclass NullEncoder(Encoder):\n    # language=rst\n    """"""\n    Pass through of the datum that was input.\n    \n    .. note::\n        This is not a real spike encoder. Be careful with the usage of this class.\n    """"""\n\n    def __init__(self):\n        super().__init__()\n\n    def __call__(self, img):\n        return img\n\n\nclass SingleEncoder(Encoder):\n    def __init__(self, time: int, dt: float = 1.0, sparsity: float = 0.5, **kwargs):\n        # language=rst\n        """"""\n        Creates a callable SingleEncoder which encodes as defined in\n        ``bindsnet.encoding.single``\n\n        :param time: Length of single spike train per input variable.\n        :param dt: Simulation time step.\n        :param sparsity: Sparsity of the input representation. 0 for no spikes and 1 for\n            all spikes.\n        """"""\n        super().__init__(time, dt=dt, sparsity=sparsity, **kwargs)\n\n        self.enc = encodings.single\n\n\nclass RepeatEncoder(Encoder):\n    def __init__(self, time: int, dt: float = 1.0, **kwargs):\n        # language=rst\n        """"""\n        Creates a callable ``RepeatEncoder`` which encodes as defined in\n        ``bindsnet.encoding.repeat``\n\n        :param time: Length of repeat spike train per input variable.\n        :param dt: Simulation time step.\n        """"""\n        super().__init__(time, dt=dt, **kwargs)\n\n        self.enc = encodings.repeat\n\n\nclass BernoulliEncoder(Encoder):\n    def __init__(self, time: int, dt: float = 1.0, **kwargs):\n        # language=rst\n        """"""\n        Creates a callable ``BernoulliEncoder`` which encodes as defined in\n        :code:`bindsnet.encoding.bernoulli`\n\n        :param time: Length of Bernoulli spike train per input variable.\n        :param dt: Simulation time step.\n\n        Keyword arguments:\n\n        :param float max_prob: Maximum probability of spike per time step.\n        """"""\n        super().__init__(time, dt=dt, **kwargs)\n\n        self.enc = encodings.bernoulli\n\n\nclass PoissonEncoder(Encoder):\n    def __init__(self, time: int, dt: float = 1.0, **kwargs):\n        # language=rst\n        """"""\n        Creates a callable PoissonEncoder which encodes as defined in\n        ``bindsnet.encoding.poisson`\n\n        :param time: Length of Poisson spike train per input variable.\n        :param dt: Simulation time step.\n        """"""\n        super().__init__(time, dt=dt, **kwargs)\n\n        self.enc = encodings.poisson\n\n\nclass RankOrderEncoder(Encoder):\n    def __init__(self, time: int, dt: float = 1.0, **kwargs):\n        # language=rst\n        """"""\n        Creates a callable RankOrderEncoder which encodes as defined in\n        :code:`bindsnet.encoding.rank_order`\n\n        :param time: Length of RankOrder spike train per input variable.\n        :param dt: Simulation time step.\n        """"""\n        super().__init__(time, dt=dt, **kwargs)\n\n        self.enc = encodings.rank_order\n'"
bindsnet/encoding/encodings.py,21,"b'from typing import Optional\n\nimport torch\nimport numpy as np\n\n\ndef single(\n    datum: torch.Tensor,\n    time: int,\n    dt: float = 1.0,\n    sparsity: float = 0.5,\n    device=""cpu"",\n    **kwargs\n) -> torch.Tensor:\n    # language=rst\n    """"""\n    Generates timing based single-spike encoding. Spike occurs earlier if the\n    intensity of the input feature is higher. Features whose value is lower than\n    threshold is remain silent.\n\n    :param datum: Tensor of shape ``[n_1, ..., n_k]``.\n    :param time: Length of the input and output.\n    :param dt: Simulation time step.\n    :param sparsity: Sparsity of the input representation. 0 for no spikes and 1 for all\n        spikes.\n    :return: Tensor of shape ``[time, n_1, ..., n_k]``.\n    """"""\n    time = int(time / dt)\n    shape = list(datum.shape)\n    datum = np.copy(datum)\n    quantile = np.quantile(datum, 1 - sparsity)\n    s = np.zeros([time, *shape], device=device)\n    s[0] = np.where(datum > quantile, np.ones(shape), np.zeros(shape))\n    return torch.Tensor(s).byte()\n\n\ndef repeat(datum: torch.Tensor, time: int, dt: float = 1.0, **kwargs) -> torch.Tensor:\n    # language=rst\n    """"""\n    :param datum: Repeats a tensor along a new dimension in the 0th position for\n        ``int(time / dt)`` timesteps.\n    :param time: Tensor of shape ``[n_1, ..., n_k]``.\n    :param dt: Simulation time step.\n    :return: Tensor of shape ``[time, n_1, ..., n_k]`` of repeated data along the 0-th\n        dimension.\n    """"""\n    time = int(time / dt)\n    return datum.repeat([time, *([1] * len(datum.shape))])\n\n\ndef bernoulli(\n    datum: torch.Tensor,\n    time: Optional[int] = None,\n    dt: float = 1.0,\n    device=""cpu"",\n    **kwargs\n) -> torch.Tensor:\n    # language=rst\n    """"""\n    Generates Bernoulli-distributed spike trains based on input intensity. Inputs must\n    be non-negative. Spikes correspond to successful Bernoulli trials, with success\n    probability equal to (normalized in [0, 1]) input value.\n\n    :param datum: Tensor of shape ``[n_1, ..., n_k]``.\n    :param time: Length of Bernoulli spike train per input variable.\n    :param dt: Simulation time step.\n    :return: Tensor of shape ``[time, n_1, ..., n_k]`` of Bernoulli-distributed spikes.\n\n    Keyword arguments:\n\n    :param float max_prob: Maximum probability of spike per Bernoulli trial.\n    """"""\n    # Setting kwargs.\n    max_prob = kwargs.get(""max_prob"", 1.0)\n\n    assert 0 <= max_prob <= 1, ""Maximum firing probability must be in range [0, 1]""\n    assert (datum >= 0).all(), ""Inputs must be non-negative""\n\n    shape, size = datum.shape, datum.numel()\n    datum = datum.flatten()\n\n    if time is not None:\n        time = int(time / dt)\n\n    # Normalize inputs and rescale (spike probability proportional to input intensity).\n    if datum.max() > 1.0:\n        datum /= datum.max()\n\n    # Make spike data from Bernoulli sampling.\n    if time is None:\n        spikes = torch.bernoulli(max_prob * datum).to(device)\n        spikes = spikes.view(*shape)\n    else:\n        spikes = torch.bernoulli(max_prob * datum.repeat([time, 1]))\n        spikes = spikes.view(time, *shape)\n\n    return spikes.byte()\n\n\ndef poisson(\n    datum: torch.Tensor, time: int, dt: float = 1.0, device=""cpu"", **kwargs\n) -> torch.Tensor:\n    # language=rst\n    """"""\n    Generates Poisson-distributed spike trains based on input intensity. Inputs must be\n    non-negative, and give the firing rate in Hz. Inter-spike intervals (ISIs) for\n    non-negative data incremented by one to avoid zero intervals while maintaining ISI\n    distributions.\n\n    :param datum: Tensor of shape ``[n_1, ..., n_k]``.\n    :param time: Length of Poisson spike train per input variable.\n    :param dt: Simulation time step.\n    :return: Tensor of shape ``[time, n_1, ..., n_k]`` of Poisson-distributed spikes.\n    """"""\n    assert (datum >= 0).all(), ""Inputs must be non-negative""\n\n    # Get shape and size of data.\n    shape, size = datum.shape, datum.numel()\n    datum = datum.flatten()\n    time = int(time / dt)\n\n    # Compute firing rates in seconds as function of data intensity,\n    # accounting for simulation time step.\n    rate = torch.zeros(size, device=device)\n    rate[datum != 0] = 1 / datum[datum != 0] * (1000 / dt)\n\n    # Create Poisson distribution and sample inter-spike intervals\n    # (incrementing by 1 to avoid zero intervals).\n    dist = torch.distributions.Poisson(rate=rate)\n    intervals = dist.sample(sample_shape=torch.Size([time + 1]))\n    intervals[:, datum != 0] += (intervals[:, datum != 0] == 0).float()\n\n    # Calculate spike times by cumulatively summing over time dimension.\n    times = torch.cumsum(intervals, dim=0).long()\n    times[times >= time + 1] = 0\n\n    # Create tensor of spikes.\n    spikes = torch.zeros(time + 1, size, device=device).byte()\n    spikes[times, torch.arange(size)] = 1\n    spikes = spikes[1:]\n\n    return spikes.view(time, *shape)\n\n\ndef rank_order(\n    datum: torch.Tensor, time: int, dt: float = 1.0, **kwargs\n) -> torch.Tensor:\n    # language=rst\n    """"""\n    Encodes data via a rank order coding-like representation. One spike per neuron,\n    temporally ordered by decreasing intensity. Inputs must be non-negative.\n\n    :param datum: Tensor of shape ``[n_samples, n_1, ..., n_k]``.\n    :param time: Length of rank order-encoded spike train per input variable.\n    :param dt: Simulation time step.\n    :return: Tensor of shape ``[time, n_1, ..., n_k]`` of rank order-encoded spikes.\n    """"""\n    assert (datum >= 0).all(), ""Inputs must be non-negative""\n\n    shape, size = datum.shape, datum.numel()\n    datum = datum.flatten()\n    time = int(time / dt)\n\n    # Create spike times in order of decreasing intensity.\n    datum /= datum.max()\n    times = torch.zeros(size)\n    times[datum != 0] = 1 / datum[datum != 0]\n    times *= time / times.max()  # Extended through simulation time.\n    times = torch.ceil(times).long()\n\n    # Create spike times tensor.\n    spikes = torch.zeros(time, size).byte()\n    for i in range(size):\n        if 0 < times[i] < time:\n            spikes[times[i] - 1, i] = 1\n\n    return spikes.reshape(time, *shape)\n'"
bindsnet/encoding/loaders.py,6,"b'from typing import Optional, Union, Iterable, Iterator\n\nimport torch\n\nfrom .encodings import bernoulli, poisson, rank_order\n\n\ndef bernoulli_loader(\n    data: Union[torch.Tensor, Iterable[torch.Tensor]],\n    time: Optional[int] = None,\n    dt: float = 1.0,\n    **kwargs\n) -> Iterator[torch.Tensor]:\n    # language=rst\n    """"""\n    Lazily invokes ``bindsnet.encoding.bernoulli`` to iteratively encode a sequence of\n    data.\n\n    :param data: Tensor of shape ``[n_samples, n_1, ..., n_k]``.\n    :param time: Length of Bernoulli spike train per input variable.\n    :param dt: Simulation time step.\n    :return: Tensors of shape ``[time, n_1, ..., n_k]`` of Bernoulli-distributed spikes.\n\n    Keyword arguments:\n\n    :param float max_prob: Maximum probability of spike per Bernoulli trial.\n    """"""\n    # Setting kwargs.\n    max_prob = kwargs.get(""dt"", 1.0)\n\n    for i in range(len(data)):\n        # Encode datum as Bernoulli spike trains.\n        yield bernoulli(datum=data[i], time=time, dt=dt, max_prob=max_prob)\n\n\ndef poisson_loader(\n    data: Union[torch.Tensor, Iterable[torch.Tensor]],\n    time: int,\n    dt: float = 1.0,\n    **kwargs\n) -> Iterator[torch.Tensor]:\n    # language=rst\n    """"""\n    Lazily invokes ``bindsnet.encoding.poisson`` to iteratively encode a sequence of\n    data.\n\n    :param data: Tensor of shape ``[n_samples, n_1, ..., n_k]``.\n    :param time: Length of Poisson spike train per input variable.\n    :param dt: Simulation time step.\n    :return: Tensors of shape ``[time, n_1, ..., n_k]`` of Poisson-distributed spikes.\n    """"""\n    for i in range(len(data)):\n        # Encode datum as Poisson spike trains.\n        yield poisson(datum=data[i], time=time, dt=dt)\n\n\ndef rank_order_loader(\n    data: Union[torch.Tensor, Iterable[torch.Tensor]],\n    time: int,\n    dt: float = 1.0,\n    **kwargs\n) -> Iterator[torch.Tensor]:\n    # language=rst\n    """"""\n    Lazily invokes ``bindsnet.encoding.rank_order`` to iteratively encode a sequence of\n    data.\n\n    :param data: Tensor of shape ``[n_samples, n_1, ..., n_k]``.\n    :param time: Length of rank order-encoded spike train per input variable.\n    :param dt: Simulation time step.\n    :return: Tensors of shape ``[time, n_1, ..., n_k]`` of rank order-encoded spikes.\n    """"""\n    for i in range(len(data)):\n        # Encode datum as rank order-encoded spike trains.\n        yield rank_order(datum=data[i], time=time, dt=dt)\n'"
bindsnet/environment/__init__.py,0,"b'from .environment import Environment, GymEnvironment\n'"
bindsnet/environment/environment.py,6,"b'from abc import ABC, abstractmethod\nfrom typing import Tuple, Dict, Any\n\nimport gym\nimport numpy as np\nimport torch\n\nfrom ..datasets.preprocess import subsample, gray_scale, binary_image, crop\nfrom ..encoding import Encoder, NullEncoder\n\n\nclass Environment(ABC):\n    # language=rst\n    """"""\n    Abstract environment class.\n    """"""\n\n    @abstractmethod\n    def step(self, a: int) -> Tuple[Any, ...]:\n        # language=rst\n        """"""\n        Abstract method head for ``step()``.\n\n        :param a: Integer action to take in environment.\n        """"""\n        pass\n\n    @abstractmethod\n    def reset(self) -> None:\n        # language=rst\n        """"""\n        Abstract method header for ``reset()``.\n        """"""\n        pass\n\n    @abstractmethod\n    def render(self) -> None:\n        # language=rst\n        """"""\n        Abstract method header for ``render()``.\n        """"""\n        pass\n\n    @abstractmethod\n    def close(self) -> None:\n        # language=rst\n        """"""\n        Abstract method header for ``close()``.\n        """"""\n        pass\n\n    @abstractmethod\n    def preprocess(self) -> None:\n        # language=rst\n        """"""\n        Abstract method header for ``preprocess()``.\n        """"""\n        pass\n\n\nclass GymEnvironment(Environment):\n    # language=rst\n    """"""\n    A wrapper around the OpenAI ``gym`` environments.\n    """"""\n\n    def __init__(self, name: str, encoder: Encoder = NullEncoder(), **kwargs) -> None:\n        # language=rst\n        """"""\n        Initializes the environment wrapper. This class makes the\n        assumption that the OpenAI ``gym`` environment will provide an image\n        of format HxW or CxHxW as an observation (we will add the C\n        dimension to HxW tensors) or a 1D observation in which case no\n        dimensions will be added.\n\n        :param name: The name of an OpenAI ``gym`` environment.\n        :param encoder: Function to encode observations into spike trains.\n\n        Keyword arguments:\n\n        :param float max_prob: Maximum spiking probability.\n        :param bool clip_rewards: Whether or not to use ``np.sign`` of rewards.\n\n        :param int history: Number of observations to keep track of.\n        :param int delta: Step size to save observations in history.\n        :param bool add_channel_dim: Allows for the adding of the channel dimension in\n            2D inputs.\n        """"""\n        self.name = name\n        self.env = gym.make(name)\n        self.action_space = self.env.action_space\n\n        self.encoder = encoder\n\n        # Keyword arguments.\n        self.max_prob = kwargs.get(""max_prob"", 1.0)\n        self.clip_rewards = kwargs.get(""clip_rewards"", True)\n\n        self.history_length = kwargs.get(""history_length"", None)\n        self.delta = kwargs.get(""delta"", 1)\n        self.add_channel_dim = kwargs.get(""add_channel_dim"", True)\n\n        if self.history_length is not None and self.delta is not None:\n            self.history = {\n                i: torch.Tensor()\n                for i in range(1, self.history_length * self.delta + 1, self.delta)\n            }\n        else:\n            self.history = {}\n\n        self.episode_step_count = 0\n        self.history_index = 1\n\n        self.obs = None\n        self.reward = None\n\n        assert (\n            0.0 < self.max_prob <= 1.0\n        ), ""Maximum spiking probability must be in (0, 1].""\n\n    def step(self, a: int) -> Tuple[torch.Tensor, float, bool, Dict[Any, Any]]:\n        # language=rst\n        """"""\n        Wrapper around the OpenAI ``gym`` environment ``step()`` function.\n\n        :param a: Action to take in the environment.\n        :return: Observation, reward, done flag, and information dictionary.\n        """"""\n        # Call gym\'s environment step function.\n        self.obs, self.reward, self.done, info = self.env.step(a)\n\n        if self.clip_rewards:\n            self.reward = np.sign(self.reward)\n\n        self.preprocess()\n\n        # Add the raw observation from the gym environment into the info\n        # for debugging and display.\n        info[""gym_obs""] = self.obs\n\n        # Store frame of history and encode the inputs.\n        if len(self.history) > 0:\n            self.update_history()\n            self.update_index()\n            # Add the delta observation into the info for debugging and display.\n            info[""delta_obs""] = self.obs\n\n        # The new standard for images is BxTxCxHxW.\n        # The gym environment doesn\'t follow exactly the same protocol.\n        #\n        # 1D observations will be left as is before the encoder and will become BxTxL.\n        # 2D observations are assumed to be mono images will become BxTx1xHxW\n        # 3D observations will become BxTxCxHxW\n        if self.obs.dim() == 2 and self.add_channel_dim:\n            # We want CxHxW, it is currently HxW.\n            self.obs = self.obs.unsqueeze(0)\n\n        # The encoder will add time - now Tx...\n        if self.encoder is not None:\n            self.obs = self.encoder(self.obs)\n\n        # Add the batch - now BxTx...\n        self.obs = self.obs.unsqueeze(0)\n\n        self.episode_step_count += 1\n\n        # Return converted observations and other information.\n        return self.obs, self.reward, self.done, info\n\n    def reset(self) -> torch.Tensor:\n        # language=rst\n        """"""\n        Wrapper around the OpenAI ``gym`` environment ``reset()`` function.\n\n        :return: Observation from the environment.\n        """"""\n        # Call gym\'s environment reset function.\n        self.obs = self.env.reset()\n        self.preprocess()\n\n        self.history = {i: torch.Tensor() for i in self.history}\n\n        self.episode_step_count = 0\n\n        return self.obs\n\n    def render(self) -> None:\n        # language=rst\n        """"""\n        Wrapper around the OpenAI ``gym`` environment ``render()`` function.\n        """"""\n        self.env.render()\n\n    def close(self) -> None:\n        # language=rst\n        """"""\n        Wrapper around the OpenAI ``gym`` environment ``close()`` function.\n        """"""\n        self.env.close()\n\n    def preprocess(self) -> None:\n        # language=rst\n        """"""\n        Pre-processing step for an observation from a ``gym`` environment.\n        """"""\n        if self.name == ""SpaceInvaders-v0"":\n            self.obs = subsample(gray_scale(self.obs), 84, 110)\n            self.obs = self.obs[26:104, :]\n            self.obs = binary_image(self.obs)\n        elif self.name == ""BreakoutDeterministic-v4"":\n            self.obs = subsample(gray_scale(crop(self.obs, 34, 194, 0, 160)), 80, 80)\n            self.obs = binary_image(self.obs)\n        else:  # Default pre-processing step.\n            pass\n\n        self.obs = torch.from_numpy(self.obs).float()\n\n    def update_history(self) -> None:\n        # language=rst\n        """"""\n        Updates the observations inside history by performing subtraction from most\n        recent observation and the sum of previous observations. If there are not enough\n        observations to take a difference from, simply store the observation without any\n        differencing.\n        """"""\n        # Recording initial observations.\n        if self.episode_step_count < len(self.history) * self.delta:\n            # Store observation based on delta value.\n            if self.episode_step_count % self.delta == 0:\n                self.history[self.history_index] = self.obs\n        else:\n            # Take difference between stored frames and current frame.\n            temp = torch.clamp(self.obs - sum(self.history.values()), 0, 1)\n\n            # Store observation based on delta value.\n            if self.episode_step_count % self.delta == 0:\n                self.history[self.history_index] = self.obs\n\n            assert (\n                len(self.history) == self.history_length\n            ), ""History size is out of bounds""\n            self.obs = temp\n\n    def update_index(self) -> None:\n        # language=rst\n        """"""\n        Updates the index to keep track of history. For example: ``history = 4``,\n        ``delta = 3`` will produce ``self.history = {1, 4, 7, 10}`` and\n        ``self.history_index`` will be updated according to ``self.delta`` and will wrap\n        around the history dictionary.\n        """"""\n        if self.episode_step_count % self.delta == 0:\n            if self.history_index != max(self.history.keys()):\n                self.history_index += self.delta\n            else:\n                # Wrap around the history.\n                self.history_index = (self.history_index % max(self.history.keys())) + 1\n'"
bindsnet/evaluation/__init__.py,0,"b'from .evaluation import (\n    assign_labels,\n    logreg_fit,\n    logreg_predict,\n    all_activity,\n    proportion_weighting,\n    ngram,\n    update_ngram_scores,\n)\n'"
bindsnet/evaluation/evaluation.py,42,"b'from itertools import product\nfrom typing import Optional, Tuple, Dict\n\nimport torch\nfrom sklearn.linear_model import LogisticRegression\n\n\ndef assign_labels(\n    spikes: torch.Tensor,\n    labels: torch.Tensor,\n    n_labels: int,\n    rates: Optional[torch.Tensor] = None,\n    alpha: float = 1.0,\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    # language=rst\n    """"""\n    Assign labels to the neurons based on highest average spiking activity.\n\n    :param spikes: Binary tensor of shape ``(n_samples, time, n_neurons)`` of a single\n        layer\'s spiking activity.\n    :param labels: Vector of shape ``(n_samples,)`` with data labels corresponding to\n        spiking activity.\n    :param n_labels: The number of target labels in the data.\n    :param rates: If passed, these represent spike rates from a previous\n        ``assign_labels()`` call.\n    :param alpha: Rate of decay of label assignments.\n    :return: Tuple of class assignments, per-class spike proportions, and per-class\n        firing rates.\n    """"""\n    n_neurons = spikes.size(2)\n\n    if rates is None:\n        rates = torch.zeros(n_neurons, n_labels)\n\n    # Sum over time dimension (spike ordering doesn\'t matter).\n    spikes = spikes.sum(1)\n\n    for i in range(n_labels):\n        # Count the number of samples with this label.\n        n_labeled = torch.sum(labels == i).float()\n\n        if n_labeled > 0:\n            # Get indices of samples with this label.\n            indices = torch.nonzero(labels == i).view(-1)\n\n            # Compute average firing rates for this label.\n            rates[:, i] = alpha * rates[:, i] + (\n                torch.sum(spikes[indices], 0) / n_labeled\n            )\n\n    # Compute proportions of spike activity per class.\n    proportions = rates / rates.sum(1, keepdim=True)\n    proportions[proportions != proportions] = 0  # Set NaNs to 0\n\n    # Neuron assignments are the labels they fire most for.\n    assignments = torch.max(proportions, 1)[1]\n\n    return assignments, proportions, rates\n\n\ndef logreg_fit(\n    spikes: torch.Tensor, labels: torch.Tensor, logreg: LogisticRegression\n) -> LogisticRegression:\n    # language=rst\n    """"""\n    (Re)fit logistic regression model to spike data summed over time.\n\n    :param spikes: Summed (over time) spikes of shape ``(n_examples, time, n_neurons)``.\n    :param labels: Vector of shape ``(n_samples,)`` with data labels corresponding to\n        spiking activity.\n    :param logreg: Logistic regression model from previous fits.\n    :return: (Re)fitted logistic regression model.\n    """"""\n    # (Re)fit logistic regression model.\n    logreg.fit(spikes, labels)\n    return logreg\n\n\ndef logreg_predict(spikes: torch.Tensor, logreg: LogisticRegression) -> torch.Tensor:\n    # language=rst\n    """"""\n    Predicts classes according to spike data summed over time.\n\n    :param spikes: Summed (over time) spikes of shape ``(n_examples, time, n_neurons)``.\n    :param logreg: Logistic regression model from previous fits.\n    :return: Predictions per example.\n    """"""\n    # Make class label predictions.\n    if not hasattr(logreg, ""coef_"") or logreg.coef_ is None:\n        return -1 * torch.ones(spikes.size(0)).long()\n\n    predictions = logreg.predict(spikes)\n    return torch.Tensor(predictions).long()\n\n\ndef all_activity(\n    spikes: torch.Tensor, assignments: torch.Tensor, n_labels: int\n) -> torch.Tensor:\n    # language=rst\n    """"""\n    Classify data with the label with highest average spiking activity over all neurons.\n\n    :param spikes: Binary tensor of shape ``(n_samples, time, n_neurons)`` of a layer\'s\n        spiking activity.\n    :param assignments: A vector of shape ``(n_neurons,)`` of neuron label assignments.\n    :param n_labels: The number of target labels in the data.\n    :return: Predictions tensor of shape ``(n_samples,)`` resulting from the ""all\n        activity"" classification scheme.\n    """"""\n    n_samples = spikes.size(0)\n\n    # Sum over time dimension (spike ordering doesn\'t matter).\n    spikes = spikes.sum(1)\n\n    rates = torch.zeros(n_samples, n_labels)\n    for i in range(n_labels):\n        # Count the number of neurons with this label assignment.\n        n_assigns = torch.sum(assignments == i).float()\n\n        if n_assigns > 0:\n            # Get indices of samples with this label.\n            indices = torch.nonzero(assignments == i).view(-1)\n\n            # Compute layer-wise firing rate for this label.\n            rates[:, i] = torch.sum(spikes[:, indices], 1) / n_assigns\n\n    # Predictions are arg-max of layer-wise firing rates.\n    return torch.sort(rates, dim=1, descending=True)[1][:, 0]\n\n\ndef proportion_weighting(\n    spikes: torch.Tensor,\n    assignments: torch.Tensor,\n    proportions: torch.Tensor,\n    n_labels: int,\n) -> torch.Tensor:\n    # language=rst\n    """"""\n    Classify data with the label with highest average spiking activity over all neurons,\n    weighted by class-wise proportion.\n\n    :param spikes: Binary tensor of shape ``(n_samples, time, n_neurons)`` of a single\n        layer\'s spiking activity.\n    :param assignments: A vector of shape ``(n_neurons,)`` of neuron label assignments.\n    :param proportions: A matrix of shape ``(n_neurons, n_labels)`` giving the per-class\n        proportions of neuron spiking activity.\n    :param n_labels: The number of target labels in the data.\n    :return: Predictions tensor of shape ``(n_samples,)`` resulting from the ""proportion\n        weighting"" classification scheme.\n    """"""\n    n_samples = spikes.size(0)\n\n    # Sum over time dimension (spike ordering doesn\'t matter).\n    spikes = spikes.sum(1)\n\n    rates = torch.zeros(n_samples, n_labels)\n    for i in range(n_labels):\n        # Count the number of neurons with this label assignment.\n        n_assigns = torch.sum(assignments == i).float()\n\n        if n_assigns > 0:\n            # Get indices of samples with this label.\n            indices = torch.nonzero(assignments == i).view(-1)\n\n            # Compute layer-wise firing rate for this label.\n            rates[:, i] += (\n                torch.sum((proportions[:, i] * spikes)[:, indices], 1) / n_assigns\n            )\n\n    # Predictions are arg-max of layer-wise firing rates.\n    predictions = torch.sort(rates, dim=1, descending=True)[1][:, 0]\n\n    return predictions\n\n\ndef ngram(\n    spikes: torch.Tensor,\n    ngram_scores: Dict[Tuple[int, ...], torch.Tensor],\n    n_labels: int,\n    n: int,\n) -> torch.Tensor:\n    # language=rst\n    """"""\n    Predicts between ``n_labels`` using ``ngram_scores``.\n\n    :param spikes: Spikes of shape ``(n_examples, time, n_neurons)``.\n    :param ngram_scores: Previously recorded scores to update.\n    :param n_labels: The number of target labels in the data.\n    :param n: The max size of n-gram to use.\n    :return: Predictions per example.\n    """"""\n    predictions = []\n    for activity in spikes:\n        score = torch.zeros(n_labels)\n\n        # Aggregate all of the firing neurons\' indices\n        fire_order = []\n        for t in range(activity.size()[0]):\n            ordering = torch.nonzero(activity[t].view(-1))\n            if ordering.numel() > 0:\n                fire_order += ordering[:, 0].tolist()\n\n        # Consider all n-gram sequences.\n        for j in range(len(fire_order) - n):\n            if tuple(fire_order[j : j + n]) in ngram_scores:\n                score += ngram_scores[tuple(fire_order[j : j + n])]\n\n        predictions.append(torch.argmax(score))\n\n    return torch.tensor(predictions).long()\n\n\ndef update_ngram_scores(\n    spikes: torch.Tensor,\n    labels: torch.Tensor,\n    n_labels: int,\n    n: int,\n    ngram_scores: Dict[Tuple[int, ...], torch.Tensor],\n) -> Dict[Tuple[int, ...], torch.Tensor]:\n    # language=rst\n    """"""\n    Updates ngram scores by adding the count of each spike sequence of length n from the\n    past ``n_examples``.\n\n    :param spikes: Spikes of shape ``(n_examples, time, n_neurons)``.\n    :param labels: The ground truth labels of shape ``(n_examples)``.\n    :param n_labels: The number of target labels in the data.\n    :param n: The max size of n-gram to use.\n    :param ngram_scores: Previously recorded scores to update.\n    :return: Dictionary mapping n-grams to vectors of per-class spike counts.\n    """"""\n    for i, activity in enumerate(spikes):\n        # Obtain firing order for spiking activity.\n        fire_order = []\n\n        # Aggregate all of the firing neurons\' indices.\n        for t in range(spikes.size(1)):\n            # Gets the indices of the neurons which fired on this timestep.\n            ordering = torch.nonzero(activity[t]).view(-1)\n            if ordering.numel() > 0:  # If there was more than one spike...\n                # Add the indices of spiked neurons to the fire ordering.\n                ordering = ordering.tolist()\n                fire_order.append(ordering)\n\n        # Check every sequence of length n.\n        for order in zip(*(fire_order[k:] for k in range(n))):\n            for sequence in product(*order):\n                if sequence not in ngram_scores:\n                    ngram_scores[sequence] = torch.zeros(n_labels)\n\n                ngram_scores[sequence][int(labels[i])] += 1\n\n    return ngram_scores\n'"
bindsnet/learning/__init__.py,0,"b'from .learning import (\n    LearningRule,\n    NoOp,\n    PostPre,\n    WeightDependentPostPre,\n    Hebbian,\n    MSTDP,\n    MSTDPET,\n    Rmax,\n)\n'"
bindsnet/learning/learning.py,67,"b'from abc import ABC\nfrom typing import Union, Optional, Sequence\n\nimport torch\nimport numpy as np\n\nfrom ..network.nodes import SRM0Nodes\nfrom ..network.topology import (\n    AbstractConnection,\n    Connection,\n    Conv2dConnection,\n    LocalConnection,\n)\nfrom ..utils import im2col_indices\n\n\nclass LearningRule(ABC):\n    # language=rst\n    """"""\n    Abstract base class for learning rules.\n    """"""\n\n    def __init__(\n        self,\n        connection: AbstractConnection,\n        nu: Optional[Union[float, Sequence[float]]] = None,\n        reduction: Optional[callable] = None,\n        weight_decay: float = 0.0,\n        **kwargs\n    ) -> None:\n        # language=rst\n        """"""\n        Abstract constructor for the ``LearningRule`` object.\n\n        :param connection: An ``AbstractConnection`` object.\n        :param nu: Single or pair of learning rates for pre- and post-synaptic events.\n        :param reduction: Method for reducing parameter updates along the batch\n            dimension.\n        :param weight_decay: Constant multiple to decay weights by on each iteration.\n        """"""\n        # Connection parameters.\n        self.connection = connection\n        self.source = connection.source\n        self.target = connection.target\n\n        self.wmin = connection.wmin\n        self.wmax = connection.wmax\n\n        # Learning rate(s).\n        if nu is None:\n            nu = [0.0, 0.0]\n        elif isinstance(nu, float) or isinstance(nu, int):\n            nu = [nu, nu]\n\n        self.nu = nu\n\n        # Parameter update reduction across minibatch dimension.\n        if reduction is None:\n            reduction = torch.mean\n\n        self.reduction = reduction\n\n        # Weight decay.\n        self.weight_decay = weight_decay\n\n    def update(self) -> None:\n        # language=rst\n        """"""\n        Abstract method for a learning rule update.\n        """"""\n        # Implement weight decay.\n        if self.weight_decay:\n            self.connection.w -= self.weight_decay * self.connection.w\n\n        # Bound weights.\n        if (\n            self.connection.wmin != -np.inf or self.connection.wmax != np.inf\n        ) and not isinstance(self, NoOp):\n            self.connection.w.clamp_(self.connection.wmin, self.connection.wmax)\n\n\nclass NoOp(LearningRule):\n    # language=rst\n    """"""\n    Learning rule with no effect.\n    """"""\n\n    def __init__(\n        self,\n        connection: AbstractConnection,\n        nu: Optional[Union[float, Sequence[float]]] = None,\n        reduction: Optional[callable] = None,\n        weight_decay: float = 0.0,\n        **kwargs\n    ) -> None:\n        # language=rst\n        """"""\n        Abstract constructor for the ``LearningRule`` object.\n\n        :param connection: An ``AbstractConnection`` object.\n        :param nu: Single or pair of learning rates for pre- and post-synaptic events.\n        :param reduction: Method for reducing parameter updates along the batch\n            dimension.\n        :param weight_decay: Constant multiple to decay weights by on each iteration.\n        """"""\n        super().__init__(\n            connection=connection,\n            nu=nu,\n            reduction=reduction,\n            weight_decay=weight_decay,\n            **kwargs\n        )\n\n    def update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Abstract method for a learning rule update.\n        """"""\n        super().update()\n\n\nclass PostPre(LearningRule):\n    # language=rst\n    """"""\n    Simple STDP rule involving both pre- and post-synaptic spiking activity. By default,\n    pre-synaptic update is negative and the post-synaptic update is positive.\n    """"""\n\n    def __init__(\n        self,\n        connection: AbstractConnection,\n        nu: Optional[Union[float, Sequence[float]]] = None,\n        reduction: Optional[callable] = None,\n        weight_decay: float = 0.0,\n        **kwargs\n    ) -> None:\n        # language=rst\n        """"""\n        Constructor for ``PostPre`` learning rule.\n\n        :param connection: An ``AbstractConnection`` object whose weights the\n            ``PostPre`` learning rule will modify.\n        :param nu: Single or pair of learning rates for pre- and post-synaptic events.\n        :param reduction: Method for reducing parameter updates along the batch\n            dimension.\n        :param weight_decay: Constant multiple to decay weights by on each iteration.\n        """"""\n        super().__init__(\n            connection=connection,\n            nu=nu,\n            reduction=reduction,\n            weight_decay=weight_decay,\n            **kwargs\n        )\n\n        assert (\n            self.source.traces and self.target.traces\n        ), ""Both pre- and post-synaptic nodes must record spike traces.""\n\n        if isinstance(connection, (Connection, LocalConnection)):\n            self.update = self._connection_update\n        elif isinstance(connection, Conv2dConnection):\n            self.update = self._conv2d_connection_update\n        else:\n            raise NotImplementedError(\n                ""This learning rule is not supported for this Connection type.""\n            )\n\n    def _connection_update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Post-pre learning rule for ``Connection`` subclass of ``AbstractConnection``\n        class.\n        """"""\n        batch_size = self.source.batch_size\n\n        source_s = self.source.s.view(batch_size, -1).unsqueeze(2).float()\n        source_x = self.source.x.view(batch_size, -1).unsqueeze(2)\n        target_s = self.target.s.view(batch_size, -1).unsqueeze(1).float()\n        target_x = self.target.x.view(batch_size, -1).unsqueeze(1)\n\n        # Pre-synaptic update.\n        if self.nu[0]:\n            update = self.reduction(torch.bmm(source_s, target_x), dim=0)\n            self.connection.w -= self.nu[0] * update\n\n        # Post-synaptic update.\n        if self.nu[1]:\n            update = self.reduction(torch.bmm(source_x, target_s), dim=0)\n            self.connection.w += self.nu[1] * update\n\n        super().update()\n\n    def _conv2d_connection_update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Post-pre learning rule for ``Conv2dConnection`` subclass of\n        ``AbstractConnection`` class.\n        """"""\n        # Get convolutional layer parameters.\n        out_channels, _, kernel_height, kernel_width = self.connection.w.size()\n        padding, stride = self.connection.padding, self.connection.stride\n        batch_size = self.source.batch_size\n\n        # Reshaping spike traces and spike occurrences.\n        source_x = im2col_indices(\n            self.source.x, kernel_height, kernel_width, padding=padding, stride=stride\n        )\n        target_x = self.target.x.view(batch_size, out_channels, -1)\n        source_s = im2col_indices(\n            self.source.s.float(),\n            kernel_height,\n            kernel_width,\n            padding=padding,\n            stride=stride,\n        )\n        target_s = self.target.s.view(batch_size, out_channels, -1).float()\n\n        # Pre-synaptic update.\n        if self.nu[0]:\n            pre = self.reduction(\n                torch.bmm(target_x, source_s.permute((0, 2, 1))), dim=0\n            )\n            self.connection.w -= self.nu[0] * pre.view(self.connection.w.size())\n\n        # Post-synaptic update.\n        if self.nu[1]:\n            post = self.reduction(\n                torch.bmm(target_s, source_x.permute((0, 2, 1))), dim=0\n            )\n            self.connection.w += self.nu[1] * post.view(self.connection.w.size())\n\n        super().update()\n\n\nclass WeightDependentPostPre(LearningRule):\n    # language=rst\n    """"""\n    STDP rule involving both pre- and post-synaptic spiking activity. The post-synaptic\n    update is positive and the pre- synaptic update is negative, and both are dependent\n    on the magnitude of the synaptic weights.\n    """"""\n\n    def __init__(\n        self,\n        connection: AbstractConnection,\n        nu: Optional[Union[float, Sequence[float]]] = None,\n        reduction: Optional[callable] = None,\n        weight_decay: float = 0.0,\n        **kwargs\n    ) -> None:\n        # language=rst\n        """"""\n        Constructor for ``WeightDependentPostPre`` learning rule.\n\n        :param connection: An ``AbstractConnection`` object whose weights the\n            ``WeightDependentPostPre`` learning rule will modify.\n        :param nu: Single or pair of learning rates for pre- and post-synaptic events.\n        :param reduction: Method for reducing parameter updates along the batch\n            dimension.\n        :param weight_decay: Constant multiple to decay weights by on each iteration.\n        """"""\n        super().__init__(\n            connection=connection,\n            nu=nu,\n            reduction=reduction,\n            weight_decay=weight_decay,\n            **kwargs\n        )\n\n        assert self.source.traces, ""Pre-synaptic nodes must record spike traces.""\n        assert (\n            connection.wmin != -np.inf and connection.wmax != np.inf\n        ), ""Connection must define finite wmin and wmax.""\n\n        self.wmin = connection.wmin\n        self.wmax = connection.wmax\n\n        if isinstance(connection, (Connection, LocalConnection)):\n            self.update = self._connection_update\n        elif isinstance(connection, Conv2dConnection):\n            self.update = self._conv2d_connection_update\n        else:\n            raise NotImplementedError(\n                ""This learning rule is not supported for this Connection type.""\n            )\n\n    def _connection_update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Post-pre learning rule for ``Connection`` subclass of ``AbstractConnection``\n        class.\n        """"""\n        batch_size = self.source.batch_size\n\n        source_s = self.source.s.view(batch_size, -1).unsqueeze(2).float()\n        source_x = self.source.x.view(batch_size, -1).unsqueeze(2)\n        target_s = self.target.s.view(batch_size, -1).unsqueeze(1).float()\n        target_x = self.target.x.view(batch_size, -1).unsqueeze(1)\n\n        update = 0\n\n        # Pre-synaptic update.\n        if self.nu[0]:\n            outer_product = self.reduction(torch.bmm(source_s, target_x), dim=0)\n            update -= self.nu[0] * outer_product * (self.connection.w - self.wmin)\n\n        # Post-synaptic update.\n        if self.nu[1]:\n            outer_product = self.reduction(torch.bmm(source_x, target_s), dim=0)\n            update += self.nu[1] * outer_product * (self.wmax - self.connection.w)\n\n        self.connection.w += update\n\n        super().update()\n\n    def _conv2d_connection_update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Post-pre learning rule for ``Conv2dConnection`` subclass of\n        ``AbstractConnection`` class.\n        """"""\n        # Get convolutional layer parameters.\n        (\n            out_channels,\n            in_channels,\n            kernel_height,\n            kernel_width,\n        ) = self.connection.w.size()\n        padding, stride = self.connection.padding, self.connection.stride\n        batch_size = self.source.batch_size\n\n        # Reshaping spike traces and spike occurrences.\n        source_x = im2col_indices(\n            self.source.x, kernel_height, kernel_width, padding=padding, stride=stride\n        )\n        target_x = self.target.x.view(batch_size, out_channels, -1)\n        source_s = im2col_indices(\n            self.source.s.float(),\n            kernel_height,\n            kernel_width,\n            padding=padding,\n            stride=stride,\n        )\n        target_s = self.target.s.view(batch_size, out_channels, -1).float()\n\n        update = 0\n\n        # Pre-synaptic update.\n        if self.nu[0]:\n            pre = self.reduction(\n                torch.bmm(target_x, source_s.permute((0, 2, 1))), dim=0\n            )\n            update -= (\n                self.nu[0]\n                * pre.view(self.connection.w.size())\n                * (self.connection.w - self.wmin)\n            )\n\n        # Post-synaptic update.\n        if self.nu[1]:\n            post = self.reduction(\n                torch.bmm(target_s, source_x.permute((0, 2, 1))), dim=0\n            )\n            update += (\n                self.nu[1]\n                * post.view(self.connection.w.size())\n                * (self.wmax - self.connection.wmin)\n            )\n\n        self.connection.w += update\n\n        super().update()\n\n\nclass Hebbian(LearningRule):\n    # language=rst\n    """"""\n    Simple Hebbian learning rule. Pre- and post-synaptic updates are both positive.\n    """"""\n\n    def __init__(\n        self,\n        connection: AbstractConnection,\n        nu: Optional[Union[float, Sequence[float]]] = None,\n        reduction: Optional[callable] = None,\n        weight_decay: float = 0.0,\n        **kwargs\n    ) -> None:\n        # language=rst\n        """"""\n        Constructor for ``Hebbian`` learning rule.\n\n        :param connection: An ``AbstractConnection`` object whose weights the\n            ``Hebbian`` learning rule will modify.\n        :param nu: Single or pair of learning rates for pre- and post-synaptic events.\n        :param reduction: Method for reducing parameter updates along the batch\n            dimension.\n        :param weight_decay: Constant multiple to decay weights by on each iteration.\n        """"""\n        super().__init__(\n            connection=connection,\n            nu=nu,\n            reduction=reduction,\n            weight_decay=weight_decay,\n            **kwargs\n        )\n\n        assert (\n            self.source.traces and self.target.traces\n        ), ""Both pre- and post-synaptic nodes must record spike traces.""\n\n        if isinstance(connection, (Connection, LocalConnection)):\n            self.update = self._connection_update\n        elif isinstance(connection, Conv2dConnection):\n            self.update = self._conv2d_connection_update\n        else:\n            raise NotImplementedError(\n                ""This learning rule is not supported for this Connection type.""\n            )\n\n    def _connection_update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Hebbian learning rule for ``Connection`` subclass of ``AbstractConnection``\n        class.\n        """"""\n        batch_size = self.source.batch_size\n\n        source_s = self.source.s.view(batch_size, -1).unsqueeze(2).float()\n        source_x = self.source.x.view(batch_size, -1).unsqueeze(2)\n        target_s = self.target.s.view(batch_size, -1).unsqueeze(1).float()\n        target_x = self.target.x.view(batch_size, -1).unsqueeze(1)\n\n        # Pre-synaptic update.\n        update = self.reduction(torch.bmm(source_s, target_x), dim=0)\n        self.connection.w += self.nu[0] * update\n\n        # Post-synaptic update.\n        update = self.reduction(torch.bmm(source_x, target_s), dim=0)\n        self.connection.w += self.nu[1] * update\n\n        super().update()\n\n    def _conv2d_connection_update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Hebbian learning rule for ``Conv2dConnection`` subclass of\n        ``AbstractConnection`` class.\n        """"""\n        out_channels, _, kernel_height, kernel_width = self.connection.w.size()\n        padding, stride = self.connection.padding, self.connection.stride\n        batch_size = self.source.batch_size\n\n        # Reshaping spike traces and spike occurrences.\n        source_x = im2col_indices(\n            self.source.x, kernel_height, kernel_width, padding=padding, stride=stride\n        )\n        target_x = self.target.x.view(batch_size, out_channels, -1)\n        source_s = im2col_indices(\n            self.source.s.float(),\n            kernel_height,\n            kernel_width,\n            padding=padding,\n            stride=stride,\n        )\n        target_s = self.target.s.view(batch_size, out_channels, -1).float()\n\n        # Pre-synaptic update.\n        pre = self.reduction(torch.bmm(target_x, source_s.permute((0, 2, 1))), dim=0)\n        self.connection.w += self.nu[0] * pre.view(self.connection.w.size())\n\n        # Post-synaptic update.\n        post = self.reduction(torch.bmm(target_s, source_x.permute((0, 2, 1))), dim=0)\n        self.connection.w += self.nu[1] * post.view(self.connection.w.size())\n\n        super().update()\n\n\nclass MSTDP(LearningRule):\n    # language=rst\n    """"""\n    Reward-modulated STDP. Adapted from `(Florian 2007)\n    <https://florian.io/papers/2007_Florian_Modulated_STDP.pdf>`_.\n    """"""\n\n    def __init__(\n        self,\n        connection: AbstractConnection,\n        nu: Optional[Union[float, Sequence[float]]] = None,\n        reduction: Optional[callable] = None,\n        weight_decay: float = 0.0,\n        **kwargs\n    ) -> None:\n        # language=rst\n        """"""\n        Constructor for ``MSTDP`` learning rule.\n\n        :param connection: An ``AbstractConnection`` object whose weights the ``MSTDP``\n            learning rule will modify.\n        :param nu: Single or pair of learning rates for pre- and post-synaptic events,\n            respectively.\n        :param reduction: Method for reducing parameter updates along the minibatch\n            dimension.\n        :param weight_decay: Constant multiple to decay weights by on each iteration.\n\n        Keyword arguments:\n\n        :param tc_plus: Time constant for pre-synaptic firing trace.\n        :param tc_minus: Time constant for post-synaptic firing trace.\n        """"""\n        super().__init__(\n            connection=connection,\n            nu=nu,\n            reduction=reduction,\n            weight_decay=weight_decay,\n            **kwargs\n        )\n\n        if isinstance(connection, (Connection, LocalConnection)):\n            self.update = self._connection_update\n        elif isinstance(connection, Conv2dConnection):\n            self.update = self._conv2d_connection_update\n        else:\n            raise NotImplementedError(\n                ""This learning rule is not supported for this Connection type.""\n            )\n\n        self.tc_plus = torch.tensor(kwargs.get(""tc_plus"", 20.0))\n        self.tc_minus = torch.tensor(kwargs.get(""tc_minus"", 20.0))\n\n    def _connection_update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        MSTDP learning rule for ``Connection`` subclass of ``AbstractConnection`` class.\n\n        Keyword arguments:\n\n        :param Union[float, torch.Tensor] reward: Reward signal from reinforcement\n            learning task.\n        :param float a_plus: Learning rate (post-synaptic).\n        :param float a_minus: Learning rate (pre-synaptic).\n        """"""\n        batch_size = self.source.batch_size\n\n        # Initialize eligibility, P^+, and P^-.\n        if not hasattr(self, ""p_plus""):\n            self.p_plus = torch.zeros(batch_size, *self.source.shape)\n        if not hasattr(self, ""p_minus""):\n            self.p_minus = torch.zeros(batch_size, *self.target.shape)\n        if not hasattr(self, ""eligibility""):\n            self.eligibility = torch.zeros(batch_size, *self.connection.w.shape)\n\n        # Reshape pre- and post-synaptic spikes.\n        source_s = self.source.s.view(batch_size, -1).float()\n        target_s = self.target.s.view(batch_size, -1).float()\n\n        # Parse keyword arguments.\n        reward = kwargs[""reward""]\n        a_plus = torch.tensor(kwargs.get(""a_plus"", 1.0))\n        a_minus = torch.tensor(kwargs.get(""a_minus"", -1.0))\n\n        # Compute weight update based on the eligibility value of the past timestep.\n        update = reward * self.eligibility\n        self.connection.w += self.nu[0] * self.reduction(update, dim=0)\n\n        # Update P^+ and P^- values.\n        self.p_plus *= torch.exp(-self.connection.dt / self.tc_plus)\n        self.p_plus += a_plus * source_s\n        self.p_minus *= torch.exp(-self.connection.dt / self.tc_minus)\n        self.p_minus += a_minus * target_s\n\n        # Calculate point eligibility value.\n        self.eligibility = torch.bmm(\n            self.p_plus.unsqueeze(2), target_s.unsqueeze(1)\n        ) + torch.bmm(source_s.unsqueeze(2), self.p_minus.unsqueeze(1))\n\n        super().update()\n\n    def _conv2d_connection_update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        MSTDP learning rule for ``Conv2dConnection`` subclass of ``AbstractConnection``\n        class.\n\n        Keyword arguments:\n\n        :param Union[float, torch.Tensor] reward: Reward signal from reinforcement\n            learning task.\n        :param float a_plus: Learning rate (post-synaptic).\n        :param float a_minus: Learning rate (pre-synaptic).\n        """"""\n        batch_size = self.source.batch_size\n\n        # Initialize eligibility.\n        if not hasattr(self, ""eligibility""):\n            self.eligibility = torch.zeros(batch_size, *self.connection.w.shape)\n\n        # Parse keyword arguments.\n        reward = kwargs[""reward""]\n        a_plus = torch.tensor(kwargs.get(""a_plus"", 1.0))\n        a_minus = torch.tensor(kwargs.get(""a_minus"", -1.0))\n\n        batch_size = self.source.batch_size\n\n        # Compute weight update based on the eligibility value of the past timestep.\n        update = reward * self.eligibility\n        self.connection.w += self.nu[0] * torch.sum(update, dim=0)\n\n        out_channels, _, kernel_height, kernel_width = self.connection.w.size()\n        padding, stride = self.connection.padding, self.connection.stride\n\n        # Initialize P^+ and P^-.\n        if not hasattr(self, ""p_plus""):\n            self.p_plus = torch.zeros(batch_size, *self.source.shape)\n            self.p_plus = im2col_indices(\n                self.p_plus, kernel_height, kernel_width, padding=padding, stride=stride\n            )\n        if not hasattr(self, ""p_minus""):\n            self.p_minus = torch.zeros(batch_size, *self.target.shape)\n            self.p_minus = self.p_minus.view(batch_size, out_channels, -1).float()\n\n        # Reshaping spike occurrences.\n        source_s = im2col_indices(\n            self.source.s.float(),\n            kernel_height,\n            kernel_width,\n            padding=padding,\n            stride=stride,\n        )\n        target_s = self.target.s.view(batch_size, out_channels, -1).float()\n\n        # Update P^+ and P^- values.\n        self.p_plus *= torch.exp(-self.connection.dt / self.tc_plus)\n        self.p_plus += a_plus * source_s\n        self.p_minus *= torch.exp(-self.connection.dt / self.tc_minus)\n        self.p_minus += a_minus * target_s\n\n        # Calculate point eligibility value.\n        self.eligibility = torch.bmm(\n            target_s, self.p_plus.permute((0, 2, 1))\n        ) + torch.bmm(self.p_minus, source_s.permute((0, 2, 1)))\n        self.eligibility = self.eligibility.view(self.connection.w.size())\n\n        super().update()\n\n\nclass MSTDPET(LearningRule):\n    # language=rst\n    """"""\n    Reward-modulated STDP with eligibility trace. Adapted from\n    `(Florian 2007) <https://florian.io/papers/2007_Florian_Modulated_STDP.pdf>`_.\n    """"""\n\n    def __init__(\n        self,\n        connection: AbstractConnection,\n        nu: Optional[Union[float, Sequence[float]]] = None,\n        reduction: Optional[callable] = None,\n        weight_decay: float = 0.0,\n        **kwargs\n    ) -> None:\n        # language=rst\n        """"""\n        Constructor for ``MSTDPET`` learning rule.\n\n        :param connection: An ``AbstractConnection`` object whose weights the\n            ``MSTDPET`` learning rule will modify.\n        :param nu: Single or pair of learning rates for pre- and post-synaptic events,\n            respectively.\n        :param reduction: Method for reducing parameter updates along the minibatch\n            dimension.\n        :param weight_decay: Constant multiple to decay weights by on each iteration.\n\n        Keyword arguments:\n\n        :param float tc_plus: Time constant for pre-synaptic firing trace.\n        :param float tc_minus: Time constant for post-synaptic firing trace.\n        :param float tc_e_trace: Time constant for the eligibility trace.\n        """"""\n        super().__init__(\n            connection=connection,\n            nu=nu,\n            reduction=reduction,\n            weight_decay=weight_decay,\n            **kwargs\n        )\n\n        if isinstance(connection, (Connection, LocalConnection)):\n            self.update = self._connection_update\n        elif isinstance(connection, Conv2dConnection):\n            self.update = self._conv2d_connection_update\n        else:\n            raise NotImplementedError(\n                ""This learning rule is not supported for this Connection type.""\n            )\n\n        self.tc_plus = torch.tensor(kwargs.get(""tc_plus"", 20.0))\n        self.tc_minus = torch.tensor(kwargs.get(""tc_minus"", 20.0))\n        self.tc_e_trace = torch.tensor(kwargs.get(""tc_e_trace"", 25.0))\n\n    def _connection_update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        MSTDPET learning rule for ``Connection`` subclass of ``AbstractConnection``\n        class.\n\n        Keyword arguments:\n\n        :param Union[float, torch.Tensor] reward: Reward signal from reinforcement\n            learning task.\n        :param float a_plus: Learning rate (post-synaptic).\n        :param float a_minus: Learning rate (pre-synaptic).\n        """"""\n        # Initialize eligibility, eligibility trace, P^+, and P^-.\n        if not hasattr(self, ""p_plus""):\n            self.p_plus = torch.zeros(self.source.n)\n        if not hasattr(self, ""p_minus""):\n            self.p_minus = torch.zeros(self.target.n)\n        if not hasattr(self, ""eligibility""):\n            self.eligibility = torch.zeros(*self.connection.w.shape)\n        if not hasattr(self, ""eligibility_trace""):\n            self.eligibility_trace = torch.zeros(*self.connection.w.shape)\n\n        # Reshape pre- and post-synaptic spikes.\n        source_s = self.source.s.view(-1).float()\n        target_s = self.target.s.view(-1).float()\n\n        # Parse keyword arguments.\n        reward = kwargs[""reward""]\n        a_plus = torch.tensor(kwargs.get(""a_plus"", 1.0))\n        a_minus = torch.tensor(kwargs.get(""a_minus"", -1.0))\n\n        # Calculate value of eligibility trace based on the value\n        # of the point eligibility value of the past timestep.\n        self.eligibility_trace *= torch.exp(-self.connection.dt / self.tc_e_trace)\n        self.eligibility_trace += self.eligibility / self.tc_e_trace\n\n        # Compute weight update.\n        self.connection.w += (\n            self.nu[0] * self.connection.dt * reward * self.eligibility_trace\n        )\n\n        # Update P^+ and P^- values.\n        self.p_plus *= torch.exp(-self.connection.dt / self.tc_plus)\n        self.p_plus += a_plus * source_s\n        self.p_minus *= torch.exp(-self.connection.dt / self.tc_minus)\n        self.p_minus += a_minus * target_s\n\n        # Calculate point eligibility value.\n        self.eligibility = torch.ger(self.p_plus, target_s) + torch.ger(\n            source_s, self.p_minus\n        )\n\n        super().update()\n\n    def _conv2d_connection_update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        MSTDPET learning rule for ``Conv2dConnection`` subclass of\n        ``AbstractConnection`` class.\n\n        Keyword arguments:\n\n        :param Union[float, torch.Tensor] reward: Reward signal from reinforcement\n            learning task.\n        :param float a_plus: Learning rate (post-synaptic).\n        :param float a_minus: Learning rate (pre-synaptic).\n        """"""\n        batch_size = self.source.batch_size\n\n        # Initialize eligibility and eligibility trace.\n        if not hasattr(self, ""eligibility""):\n            self.eligibility = torch.zeros(batch_size, *self.connection.w.shape)\n        if not hasattr(self, ""eligibility_trace""):\n            self.eligibility_trace = torch.zeros(batch_size, *self.connection.w.shape)\n\n        # Parse keyword arguments.\n        reward = kwargs[""reward""]\n        a_plus = torch.tensor(kwargs.get(""a_plus"", 1.0))\n        a_minus = torch.tensor(kwargs.get(""a_minus"", -1.0))\n\n        # Calculate value of eligibility trace based on the value\n        # of the point eligibility value of the past timestep.\n        self.eligibility_trace *= torch.exp(-self.connection.dt / self.tc_e_trace)\n\n        # Compute weight update.\n        update = reward * self.eligibility_trace\n        self.connection.w += self.nu[0] * self.connection.dt * torch.sum(update, dim=0)\n\n        out_channels, _, kernel_height, kernel_width = self.connection.w.size()\n        padding, stride = self.connection.padding, self.connection.stride\n\n        # Initialize P^+ and P^-.\n        if not hasattr(self, ""p_plus""):\n            self.p_plus = torch.zeros(batch_size, *self.source.shape)\n            self.p_plus = im2col_indices(\n                self.p_plus, kernel_height, kernel_width, padding=padding, stride=stride\n            )\n        if not hasattr(self, ""p_minus""):\n            self.p_minus = torch.zeros(batch_size, *self.target.shape)\n            self.p_minus = self.p_minus.view(batch_size, out_channels, -1).float()\n\n        # Reshaping spike occurrences.\n        source_s = im2col_indices(\n            self.source.s.float(),\n            kernel_height,\n            kernel_width,\n            padding=padding,\n            stride=stride,\n        )\n        target_s = (\n            self.target.s.permute(1, 2, 3, 0).view(batch_size, out_channels, -1).float()\n        )\n\n        # Update P^+ and P^- values.\n        self.p_plus *= torch.exp(-self.connection.dt / self.tc_plus)\n        self.p_plus += a_plus * source_s\n        self.p_minus *= torch.exp(-self.connection.dt / self.tc_minus)\n        self.p_minus += a_minus * target_s\n\n        # Calculate point eligibility value.\n        self.eligibility = torch.bmm(\n            target_s, self.p_plus.permute((0, 2, 1))\n        ) + torch.bmm(self.p_minus, source_s.permute((0, 2, 1)))\n        self.eligibility = self.eligibility.view(self.connection.w.size())\n\n        super().update()\n\n\nclass Rmax(LearningRule):\n    # language=rst\n    """"""\n    Reward-modulated learning rule derived from reward maximization principles. Adapted\n    from `(Vasilaki et al., 2009)\n    <https://intranet.physio.unibe.ch/Publikationen/Dokumente/Vasilaki2009PloSComputBio_1.pdf>`_.\n    """"""\n\n    def __init__(\n        self,\n        connection: AbstractConnection,\n        nu: Optional[Union[float, Sequence[float]]] = None,\n        reduction: Optional[callable] = None,\n        weight_decay: float = 0.0,\n        **kwargs\n    ) -> None:\n        # language=rst\n        """"""\n        Constructor for ``R-max`` learning rule.\n\n        :param connection: An ``AbstractConnection`` object whose weights the ``R-max``\n            learning rule will modify.\n        :param nu: Single or pair of learning rates for pre- and post-synaptic events,\n            respectively.\n        :param reduction: Method for reducing parameter updates along the minibatch\n            dimension.\n        :param weight_decay: Constant multiple to decay weights by on each iteration.\n\n        Keyword arguments:\n\n        :param float tc_c: Time constant for balancing naive Hebbian and policy gradient\n            learning.\n        :param float tc_e_trace: Time constant for the eligibility trace.\n        """"""\n        super().__init__(\n            connection=connection,\n            nu=nu,\n            reduction=reduction,\n            weight_decay=weight_decay,\n            **kwargs\n        )\n\n        # Trace is needed for computing epsilon.\n        assert (\n            self.source.traces and self.source.traces_additive\n        ), ""Pre-synaptic nodes must use additive spike traces.""\n\n        # Derivation of R-max depends on stochastic SRM neurons!\n        assert isinstance(\n            self.target, SRM0Nodes\n        ), ""R-max needs stochastically firing neurons, use SRM0Nodes.""\n\n        if isinstance(connection, (Connection, LocalConnection)):\n            self.update = self._connection_update\n        else:\n            raise NotImplementedError(\n                ""This learning rule is not supported for this Connection type.""\n            )\n\n        self.tc_c = torch.tensor(\n            kwargs.get(""tc_c"", 5.0)\n        )  # 0 for pure naive Hebbian, inf for pure policy gradient.\n        self.tc_e_trace = torch.tensor(kwargs.get(""tc_e_trace"", 25.0))\n\n    def _connection_update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        R-max learning rule for ``Connection`` subclass of ``AbstractConnection`` class.\n\n        Keyword arguments:\n\n        :param Union[float, torch.Tensor] reward: Reward signal from reinforcement\n            learning task.\n        """"""\n        # Initialize eligibility trace.\n        if not hasattr(self, ""eligibility_trace""):\n            self.eligibility_trace = torch.zeros(*self.connection.w.shape)\n\n        # Reshape variables.\n        target_s = self.target.s.view(-1).float()\n        target_s_prob = self.target.s_prob.view(-1)\n        source_x = self.source.x.view(-1)\n\n        # Parse keyword arguments.\n        reward = kwargs[""reward""]\n\n        # New eligibility trace.\n        self.eligibility_trace *= 1 - self.connection.dt / self.tc_e_trace\n        self.eligibility_trace += (\n            target_s\n            - (target_s_prob / (1.0 + self.tc_c / self.connection.dt * target_s_prob))\n        ) * source_x[:, None]\n\n        # Compute weight update.\n        self.connection.w += self.nu[0] * reward * self.eligibility_trace\n\n        super().update()\n'"
bindsnet/learning/reward.py,7,"b'from abc import ABC, abstractmethod\n\nimport torch\n\n\nclass AbstractReward(ABC):\n    # language=rst\n    """"""\n    Abstract base class for reward computation.\n    """"""\n\n    @abstractmethod\n    def compute(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Computes/modifies reward.\n        """"""\n        pass\n\n    @abstractmethod\n    def update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Updates internal variables needed to modify reward. Usually called once per\n        episode.\n        """"""\n        pass\n\n\nclass MovingAvgRPE(AbstractReward):\n    # language=rst\n    """"""\n    Computes reward prediction error (RPE) based on an exponential moving average (EMA)\n    of past rewards.\n    """"""\n\n    def __init__(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Constructor for EMA reward prediction error.\n        """"""\n        self.reward_predict = torch.tensor(0.0)  # Predicted reward (per step).\n        self.reward_predict_episode = torch.tensor(0.0)  # Predicted reward per episode.\n        self.rewards_predict_episode = (\n            []\n        )  # List of predicted rewards per episode (used for plotting).\n\n    def compute(self, **kwargs) -> torch.Tensor:\n        # language=rst\n        """"""\n        Computes the reward prediction error using EMA.\n\n        Keyword arguments:\n\n        :param Union[float, torch.Tensor] reward: Current reward.\n        :return: Reward prediction error.\n        """"""\n        # Get keyword arguments.\n        reward = kwargs[""reward""]\n\n        return reward - self.reward_predict\n\n    def update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Updates the EMAs. Called once per episode.\n\n        Keyword arguments:\n\n        :param Union[float, torch.Tensor] accumulated_reward: Reward accumulated over\n            one episode.\n        :param int steps: Steps in that episode.\n        :param float ema_window: Width of the averaging window.\n        """"""\n        # Get keyword arguments.\n        accumulated_reward = kwargs[""accumulated_reward""]\n        steps = torch.tensor(kwargs[""steps""]).float()\n        ema_window = torch.tensor(kwargs.get(""ema_window"", 10.0))\n\n        # Compute average reward per step.\n        reward = accumulated_reward / steps\n\n        # Update EMAs.\n        self.reward_predict = (\n            1 - 1 / ema_window\n        ) * self.reward_predict + 1 / ema_window * reward\n        self.reward_predict_episode = (\n            1 - 1 / ema_window\n        ) * self.reward_predict_episode + 1 / ema_window * accumulated_reward\n        self.rewards_predict_episode.append(self.reward_predict_episode.item())\n'"
bindsnet/models/__init__.py,0,"b'from .models import (\n    TwoLayerNetwork,\n    DiehlAndCook2015,\n    DiehlAndCook2015v2,\n    IncreasingInhibitionNetwork,\n    LocallyConnectedNetwork,\n)\n'"
bindsnet/models/models.py,14,"b'from typing import Optional, Union, Tuple, List, Sequence, Iterable\n\nimport numpy as np\nimport torch\nfrom scipy.spatial.distance import euclidean\nfrom torch.nn.modules.utils import _pair\nimport torch.nn as nn\nfrom torchvision import models\n\nfrom ..learning import PostPre\nfrom ..network import Network\nfrom ..network.nodes import Input, LIFNodes, DiehlAndCookNodes, AdaptiveLIFNodes\nfrom ..network.topology import Connection, LocalConnection\n\n\nclass TwoLayerNetwork(Network):\n    # language=rst\n    """"""\n    Implements an ``Input`` instance connected to a ``LIFNodes`` instance with a\n    fully-connected ``Connection``.\n    """"""\n\n    def __init__(\n        self,\n        n_inpt: int,\n        n_neurons: int = 100,\n        dt: float = 1.0,\n        wmin: float = 0.0,\n        wmax: float = 1.0,\n        nu: Optional[Union[float, Sequence[float]]] = (1e-4, 1e-2),\n        reduction: Optional[callable] = None,\n        norm: float = 78.4,\n    ) -> None:\n        # language=rst\n        """"""\n        Constructor for class ``TwoLayerNetwork``.\n\n        :param n_inpt: Number of input neurons. Matches the 1D size of the input data.\n        :param n_neurons: Number of neurons in the ``LIFNodes`` population.\n        :param dt: Simulation time step.\n        :param nu: Single or pair of learning rates for pre- and post-synaptic events,\n            respectively.\n        :param reduction: Method for reducing parameter updates along the minibatch\n            dimension.\n        :param wmin: Minimum allowed weight on ``Input`` to ``LIFNodes`` synapses.\n        :param wmax: Maximum allowed weight on ``Input`` to ``LIFNodes`` synapses.\n        :param norm: ``Input`` to ``LIFNodes`` layer connection weights normalization\n            constant.\n        """"""\n        super().__init__(dt=dt)\n\n        self.n_inpt = n_inpt\n        self.n_neurons = n_neurons\n        self.dt = dt\n\n        self.add_layer(Input(n=self.n_inpt, traces=True, tc_trace=20.0), name=""X"")\n        self.add_layer(\n            LIFNodes(\n                n=self.n_neurons,\n                traces=True,\n                rest=-65.0,\n                reset=-65.0,\n                thresh=-52.0,\n                refrac=5,\n                tc_decay=100.0,\n                tc_trace=20.0,\n            ),\n            name=""Y"",\n        )\n\n        w = 0.3 * torch.rand(self.n_inpt, self.n_neurons)\n        self.add_connection(\n            Connection(\n                source=self.layers[""X""],\n                target=self.layers[""Y""],\n                w=w,\n                update_rule=PostPre,\n                nu=nu,\n                reduction=reduction,\n                wmin=wmin,\n                wmax=wmax,\n                norm=norm,\n            ),\n            source=""X"",\n            target=""Y"",\n        )\n\n\nclass DiehlAndCook2015(Network):\n    # language=rst\n    """"""\n    Implements the spiking neural network architecture from `(Diehl & Cook 2015)\n    <https://www.frontiersin.org/articles/10.3389/fncom.2015.00099/full>`_.\n    """"""\n\n    def __init__(\n        self,\n        n_inpt: int,\n        n_neurons: int = 100,\n        exc: float = 22.5,\n        inh: float = 17.5,\n        dt: float = 1.0,\n        nu: Optional[Union[float, Sequence[float]]] = (1e-4, 1e-2),\n        reduction: Optional[callable] = None,\n        wmin: float = 0.0,\n        wmax: float = 1.0,\n        norm: float = 78.4,\n        theta_plus: float = 0.05,\n        tc_theta_decay: float = 1e7,\n        inpt_shape: Optional[Iterable[int]] = None,\n    ) -> None:\n        # language=rst\n        """"""\n        Constructor for class ``DiehlAndCook2015``.\n\n        :param n_inpt: Number of input neurons. Matches the 1D size of the input data.\n        :param n_neurons: Number of excitatory, inhibitory neurons.\n        :param exc: Strength of synapse weights from excitatory to inhibitory layer.\n        :param inh: Strength of synapse weights from inhibitory to excitatory layer.\n        :param dt: Simulation time step.\n        :param nu: Single or pair of learning rates for pre- and post-synaptic events,\n            respectively.\n        :param reduction: Method for reducing parameter updates along the minibatch\n            dimension.\n        :param wmin: Minimum allowed weight on input to excitatory synapses.\n        :param wmax: Maximum allowed weight on input to excitatory synapses.\n        :param norm: Input to excitatory layer connection weights normalization\n            constant.\n        :param theta_plus: On-spike increment of ``DiehlAndCookNodes`` membrane\n            threshold potential.\n        :param tc_theta_decay: Time constant of ``DiehlAndCookNodes`` threshold\n            potential decay.\n        :param inpt_shape: The dimensionality of the input layer.\n        """"""\n        super().__init__(dt=dt)\n\n        self.n_inpt = n_inpt\n        self.inpt_shape = inpt_shape\n        self.n_neurons = n_neurons\n        self.exc = exc\n        self.inh = inh\n        self.dt = dt\n\n        # Layers\n        input_layer = Input(\n            n=self.n_inpt, shape=self.inpt_shape, traces=True, tc_trace=20.0\n        )\n        exc_layer = DiehlAndCookNodes(\n            n=self.n_neurons,\n            traces=True,\n            rest=-65.0,\n            reset=-60.0,\n            thresh=-52.0,\n            refrac=5,\n            tc_decay=100.0,\n            tc_trace=20.0,\n            theta_plus=theta_plus,\n            tc_theta_decay=tc_theta_decay,\n        )\n        inh_layer = LIFNodes(\n            n=self.n_neurons,\n            traces=False,\n            rest=-60.0,\n            reset=-45.0,\n            thresh=-40.0,\n            tc_decay=10.0,\n            refrac=2,\n            tc_trace=20.0,\n        )\n\n        # Connections\n        w = 0.3 * torch.rand(self.n_inpt, self.n_neurons)\n        input_exc_conn = Connection(\n            source=input_layer,\n            target=exc_layer,\n            w=w,\n            update_rule=PostPre,\n            nu=nu,\n            reduction=reduction,\n            wmin=wmin,\n            wmax=wmax,\n            norm=norm,\n        )\n        w = self.exc * torch.diag(torch.ones(self.n_neurons))\n        exc_inh_conn = Connection(\n            source=exc_layer, target=inh_layer, w=w, wmin=0, wmax=self.exc\n        )\n        w = -self.inh * (\n            torch.ones(self.n_neurons, self.n_neurons)\n            - torch.diag(torch.ones(self.n_neurons))\n        )\n        inh_exc_conn = Connection(\n            source=inh_layer, target=exc_layer, w=w, wmin=-self.inh, wmax=0\n        )\n\n        # Add to network\n        self.add_layer(input_layer, name=""X"")\n        self.add_layer(exc_layer, name=""Ae"")\n        self.add_layer(inh_layer, name=""Ai"")\n        self.add_connection(input_exc_conn, source=""X"", target=""Ae"")\n        self.add_connection(exc_inh_conn, source=""Ae"", target=""Ai"")\n        self.add_connection(inh_exc_conn, source=""Ai"", target=""Ae"")\n\n\nclass DiehlAndCook2015v2(Network):\n    # language=rst\n    """"""\n    Slightly modifies the spiking neural network architecture from `(Diehl & Cook 2015)\n    <https://www.frontiersin.org/articles/10.3389/fncom.2015.00099/full>`_ by removing\n    the inhibitory layer and replacing it with a recurrent inhibitory connection in the\n    output layer (what used to be the excitatory layer).\n    """"""\n\n    def __init__(\n        self,\n        n_inpt: int,\n        n_neurons: int = 100,\n        inh: float = 17.5,\n        dt: float = 1.0,\n        nu: Optional[Union[float, Sequence[float]]] = (1e-4, 1e-2),\n        reduction: Optional[callable] = None,\n        wmin: Optional[float] = 0.0,\n        wmax: Optional[float] = 1.0,\n        norm: float = 78.4,\n        theta_plus: float = 0.05,\n        tc_theta_decay: float = 1e7,\n        inpt_shape: Optional[Iterable[int]] = None,\n    ) -> None:\n        # language=rst\n        """"""\n        Constructor for class ``DiehlAndCook2015v2``.\n\n        :param n_inpt: Number of input neurons. Matches the 1D size of the input data.\n        :param n_neurons: Number of excitatory, inhibitory neurons.\n        :param inh: Strength of synapse weights from inhibitory to excitatory layer.\n        :param dt: Simulation time step.\n        :param nu: Single or pair of learning rates for pre- and post-synaptic events,\n            respectively.\n        :param reduction: Method for reducing parameter updates along the minibatch\n            dimension.\n        :param wmin: Minimum allowed weight on input to excitatory synapses.\n        :param wmax: Maximum allowed weight on input to excitatory synapses.\n        :param norm: Input to excitatory layer connection weights normalization\n            constant.\n        :param theta_plus: On-spike increment of ``DiehlAndCookNodes`` membrane\n            threshold potential.\n        :param tc_theta_decay: Time constant of ``DiehlAndCookNodes`` threshold\n            potential decay.\n        :param inpt_shape: The dimensionality of the input layer.\n        """"""\n        super().__init__(dt=dt)\n\n        self.n_inpt = n_inpt\n        self.inpt_shape = inpt_shape\n        self.n_neurons = n_neurons\n        self.inh = inh\n        self.dt = dt\n\n        input_layer = Input(\n            n=self.n_inpt, shape=self.inpt_shape, traces=True, tc_trace=20.0\n        )\n        self.add_layer(input_layer, name=""X"")\n\n        output_layer = DiehlAndCookNodes(\n            n=self.n_neurons,\n            traces=True,\n            rest=-65.0,\n            reset=-60.0,\n            thresh=-52.0,\n            refrac=5,\n            tc_decay=100.0,\n            tc_trace=20.0,\n            theta_plus=theta_plus,\n            tc_theta_decay=tc_theta_decay,\n        )\n        self.add_layer(output_layer, name=""Y"")\n\n        w = 0.3 * torch.rand(self.n_inpt, self.n_neurons)\n        input_connection = Connection(\n            source=self.layers[""X""],\n            target=self.layers[""Y""],\n            w=w,\n            update_rule=PostPre,\n            nu=nu,\n            reduction=reduction,\n            wmin=wmin,\n            wmax=wmax,\n            norm=norm,\n        )\n        self.add_connection(input_connection, source=""X"", target=""Y"")\n\n        w = -self.inh * (\n            torch.ones(self.n_neurons, self.n_neurons)\n            - torch.diag(torch.ones(self.n_neurons))\n        )\n        recurrent_connection = Connection(\n            source=self.layers[""Y""],\n            target=self.layers[""Y""],\n            w=w,\n            wmin=-self.inh,\n            wmax=0,\n        )\n        self.add_connection(recurrent_connection, source=""Y"", target=""Y"")\n\n\nclass IncreasingInhibitionNetwork(Network):\n    # language=rst\n    """"""\n    Implements the inhibitory layer structure of the spiking neural network architecture\n    from `(Hazan et al. 2018) <https://arxiv.org/abs/1807.09374>`_\n    """"""\n\n    def __init__(\n        self,\n        n_input: int,\n        n_neurons: int = 100,\n        start_inhib: float = 1.0,\n        max_inhib: float = 100.0,\n        dt: float = 1.0,\n        nu: Optional[Union[float, Sequence[float]]] = (1e-4, 1e-2),\n        reduction: Optional[callable] = None,\n        wmin: float = 0.0,\n        wmax: float = 1.0,\n        norm: float = 78.4,\n        theta_plus: float = 0.05,\n        tc_theta_decay: float = 1e7,\n        inpt_shape: Optional[Iterable[int]] = None,\n    ) -> None:\n        # language=rst\n        """"""\n        Constructor for class ``IncreasingInhibitionNetwork``.\n\n        :param n_inpt: Number of input neurons. Matches the 1D size of the input data.\n        :param n_neurons: Number of excitatory, inhibitory neurons.\n        :param inh: Strength of synapse weights from inhibitory to excitatory layer.\n        :param dt: Simulation time step.\n        :param nu: Single or pair of learning rates for pre- and post-synaptic events,\n            respectively.\n        :param reduction: Method for reducing parameter updates along the minibatch\n            dimension.\n        :param wmin: Minimum allowed weight on input to excitatory synapses.\n        :param wmax: Maximum allowed weight on input to excitatory synapses.\n        :param norm: Input to excitatory layer connection weights normalization\n            constant.\n        :param theta_plus: On-spike increment of ``DiehlAndCookNodes`` membrane\n            threshold potential.\n        :param tc_theta_decay: Time constant of ``DiehlAndCookNodes`` threshold\n            potential decay.\n        :param inpt_shape: The dimensionality of the input layer.\n        """"""\n        super().__init__(dt=dt)\n\n        self.n_input = n_input\n        self.n_neurons = n_neurons\n        self.n_sqrt = int(np.sqrt(n_neurons))\n        self.start_inhib = start_inhib\n        self.max_inhib = max_inhib\n        self.dt = dt\n        self.inpt_shape = inpt_shape\n\n        input_layer = Input(\n            n=self.n_input, shape=self.inpt_shape, traces=True, tc_trace=20.0\n        )\n        self.add_layer(input_layer, name=""X"")\n\n        output_layer = DiehlAndCookNodes(\n            n=self.n_neurons,\n            traces=True,\n            rest=-65.0,\n            reset=-60.0,\n            thresh=-52.0,\n            refrac=5,\n            tc_decay=100.0,\n            tc_trace=20.0,\n            theta_plus=theta_plus,\n            tc_theta_decay=tc_theta_decay,\n        )\n        self.add_layer(output_layer, name=""Y"")\n\n        w = 0.3 * torch.rand(self.n_input, self.n_neurons)\n        input_output_conn = Connection(\n            source=self.layers[""X""],\n            target=self.layers[""Y""],\n            w=w,\n            update_rule=PostPre,\n            nu=nu,\n            reduction=reduction,\n            wmin=wmin,\n            wmax=wmax,\n            norm=norm,\n        )\n        self.add_connection(input_output_conn, source=""X"", target=""Y"")\n\n        # add internal inhibetory connections\n        w = torch.ones(self.n_neurons, self.n_neurons) - torch.diag(\n            torch.ones(self.n_neurons)\n        )\n        for i in range(self.n_neurons):\n            for j in range(self.n_neurons):\n                if i != j:\n                    x1, y1 = i // self.n_sqrt, i % self.n_sqrt\n                    x2, y2 = j // self.n_sqrt, j % self.n_sqrt\n\n                    w[i, j] = np.sqrt(euclidean([x1, y1], [x2, y2]))\n        w = w / w.max()\n        w = (w * self.max_inhib) + self.start_inhib\n        recurrent_output_conn = Connection(\n            source=self.layers[""Y""], target=self.layers[""Y""], w=w,\n        )\n        self.add_connection(recurrent_output_conn, source=""Y"", target=""Y"")\n\n\nclass LocallyConnectedNetwork(Network):\n    # language=rst\n    """"""\n    Defines a two-layer network in which the input layer is ""locally connected"" to the\n    output layer, and the output layer is recurrently inhibited connected such that\n    neurons with the same input receptive field inhibit each other.\n    """"""\n\n    def __init__(\n        self,\n        n_inpt: int,\n        input_shape: List[int],\n        kernel_size: Union[int, Tuple[int, int]],\n        stride: Union[int, Tuple[int, int]],\n        n_filters: int,\n        inh: float = 25.0,\n        dt: float = 1.0,\n        nu: Optional[Union[float, Sequence[float]]] = (1e-4, 1e-2),\n        reduction: Optional[callable] = None,\n        theta_plus: float = 0.05,\n        tc_theta_decay: float = 1e7,\n        wmin: float = 0.0,\n        wmax: float = 1.0,\n        norm: Optional[float] = 0.2,\n    ) -> None:\n        # language=rst\n        """"""\n        Constructor for class ``LocallyConnectedNetwork``. Uses ``DiehlAndCookNodes`` to\n        avoid multiple spikes per timestep in the output layer population.\n\n        :param n_inpt: Number of input neurons. Matches the 1D size of the input data.\n        :param input_shape: Two-dimensional shape of input population.\n        :param kernel_size: Size of input windows. Integer or two-tuple of integers.\n        :param stride: Length of horizontal, vertical stride across input space. Integer\n            or two-tuple of integers.\n        :param n_filters: Number of locally connected filters per input region. Integer\n            or two-tuple of integers.\n        :param inh: Strength of synapse weights from output layer back onto itself.\n        :param dt: Simulation time step.\n        :param nu: Single or pair of learning rates for pre- and post-synaptic events,\n            respectively.\n        :param reduction: Method for reducing parameter updates along the minibatch\n            dimension.\n        :param wmin: Minimum allowed weight on ``Input`` to ``DiehlAndCookNodes``\n            synapses.\n        :param wmax: Maximum allowed weight on ``Input`` to ``DiehlAndCookNodes``\n            synapses.\n        :param theta_plus: On-spike increment of ``DiehlAndCookNodes`` membrane\n            threshold potential.\n        :param tc_theta_decay: Time constant of ``DiehlAndCookNodes`` threshold\n            potential decay.\n        :param norm: ``Input`` to ``DiehlAndCookNodes`` layer connection weights\n            normalization constant.\n        """"""\n        super().__init__(dt=dt)\n\n        kernel_size = _pair(kernel_size)\n        stride = _pair(stride)\n\n        self.n_inpt = n_inpt\n        self.input_shape = input_shape\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.n_filters = n_filters\n        self.inh = inh\n        self.dt = dt\n        self.theta_plus = theta_plus\n        self.tc_theta_decay = tc_theta_decay\n        self.wmin = wmin\n        self.wmax = wmax\n        self.norm = norm\n\n        if kernel_size == input_shape:\n            conv_size = [1, 1]\n        else:\n            conv_size = (\n                int((input_shape[0] - kernel_size[0]) / stride[0]) + 1,\n                int((input_shape[1] - kernel_size[1]) / stride[1]) + 1,\n            )\n\n        input_layer = Input(n=self.n_inpt, traces=True, tc_trace=20.0)\n\n        output_layer = DiehlAndCookNodes(\n            n=self.n_filters * conv_size[0] * conv_size[1],\n            traces=True,\n            rest=-65.0,\n            reset=-60.0,\n            thresh=-52.0,\n            refrac=5,\n            tc_decay=100.0,\n            tc_trace=20.0,\n            theta_plus=theta_plus,\n            tc_theta_decay=tc_theta_decay,\n        )\n        input_output_conn = LocalConnection(\n            input_layer,\n            output_layer,\n            kernel_size=kernel_size,\n            stride=stride,\n            n_filters=n_filters,\n            nu=nu,\n            reduction=reduction,\n            update_rule=PostPre,\n            wmin=wmin,\n            wmax=wmax,\n            norm=norm,\n            input_shape=input_shape,\n        )\n\n        w = torch.zeros(n_filters, *conv_size, n_filters, *conv_size)\n        for fltr1 in range(n_filters):\n            for fltr2 in range(n_filters):\n                if fltr1 != fltr2:\n                    for i in range(conv_size[0]):\n                        for j in range(conv_size[1]):\n                            w[fltr1, i, j, fltr2, i, j] = -inh\n\n        w = w.view(\n            n_filters * conv_size[0] * conv_size[1],\n            n_filters * conv_size[0] * conv_size[1],\n        )\n        recurrent_conn = Connection(output_layer, output_layer, w=w)\n\n        self.add_layer(input_layer, name=""X"")\n        self.add_layer(output_layer, name=""Y"")\n        self.add_connection(input_output_conn, source=""X"", target=""Y"")\n        self.add_connection(recurrent_conn, source=""Y"", target=""Y"")\n'"
bindsnet/network/__init__.py,0,"b'from .network import Network, load\nfrom . import nodes, topology, monitors\n'"
bindsnet/network/monitors.py,17,"b'import os\nimport torch\nimport numpy as np\n\nfrom abc import ABC\nfrom typing import Union, Optional, Iterable, Dict\n\nfrom .nodes import Nodes\nfrom .topology import AbstractConnection\n\n\nclass AbstractMonitor(ABC):\n    # language=rst\n    """"""\n    Abstract base class for state variable monitors.\n    """"""\n\n\nclass Monitor(AbstractMonitor):\n    # language=rst\n    """"""\n    Records state variables of interest.\n    """"""\n\n    def __init__(\n        self,\n        obj: Union[Nodes, AbstractConnection],\n        state_vars: Iterable[str],\n        time: Optional[int] = None,\n        batch_size: int = 1,\n    ):\n        # language=rst\n        """"""\n        Constructs a ``Monitor`` object.\n\n        :param obj: An object to record state variables from during network simulation.\n        :param state_vars: Iterable of strings indicating names of state variables to\n            record.\n        :param time: If not ``None``, pre-allocate memory for state variable recording.\n        """"""\n        super().__init__()\n\n        self.obj = obj\n        self.state_vars = state_vars\n        self.time = time\n        self.batch_size = batch_size\n\n        # Deal with time later, the same underlying list is used\n        self.recording = {v: [] for v in self.state_vars}\n\n    def get(self, var: str) -> torch.Tensor:\n        # language=rst\n        """"""\n        Return recording to user.\n\n        :param var: State variable recording to return.\n        :return: Tensor of shape ``[time, n_1, ..., n_k]``, where ``[n_1, ..., n_k]`` is\n            the shape of the recorded state variable.\n        """"""\n        return torch.cat(self.recording[var], 0)\n\n    def record(self) -> None:\n        # language=rst\n        """"""\n        Appends the current value of the recorded state variables to the recording.\n        """"""\n        for v in self.state_vars:\n            data = getattr(self.obj, v).unsqueeze(0)\n            self.recording[v].append(data.detach().clone())\n\n        # remove the oldest element (first in the list)\n        if self.time is not None:\n            for v in self.state_vars:\n                if len(self.recording[v]) > self.time:\n                    self.recording[v].pop(0)\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Resets recordings to empty ``torch.Tensor``s.\n        """"""\n        self.recording = {v: [] for v in self.state_vars}\n\n\nclass NetworkMonitor(AbstractMonitor):\n    # language=rst\n    """"""\n    Record state variables of all layers and connections.\n    """"""\n\n    def __init__(\n        self,\n        network: ""Network"",\n        layers: Optional[Iterable[str]] = None,\n        connections: Optional[Iterable[str]] = None,\n        state_vars: Optional[Iterable[str]] = None,\n        time: Optional[int] = None,\n    ):\n        # language=rst\n        """"""\n        Constructs a ``NetworkMonitor`` object.\n\n        :param network: Network to record state variables from.\n        :param layers: Layers to record state variables from.\n        :param connections: Connections to record state variables from.\n        :param state_vars: List of strings indicating names of state variables to\n            record.\n        :param time: If not ``None``, pre-allocate memory for state variable recording.\n        """"""\n        super().__init__()\n\n        self.network = network\n        self.layers = layers if layers is not None else list(self.network.layers.keys())\n        self.connections = (\n            connections\n            if connections is not None\n            else list(self.network.connections.keys())\n        )\n        self.state_vars = state_vars if state_vars is not None else (""v"", ""s"", ""w"")\n        self.time = time\n\n        if self.time is not None:\n            self.i = 0\n\n        # Initialize empty recording.\n        self.recording = {k: {} for k in self.layers + self.connections}\n\n        # If no simulation time is specified, specify 0-dimensional recordings.\n        if self.time is None:\n            for v in self.state_vars:\n                for l in self.layers:\n                    if hasattr(self.network.layers[l], v):\n                        self.recording[l][v] = torch.Tensor()\n\n                for c in self.connections:\n                    if hasattr(self.network.connections[c], v):\n                        self.recording[c][v] = torch.Tensor()\n\n        # If simulation time is specified, pre-allocate recordings in memory for speed.\n        else:\n            for v in self.state_vars:\n                for l in self.layers:\n                    if hasattr(self.network.layers[l], v):\n                        self.recording[l][v] = torch.zeros(\n                            self.time, *getattr(self.network.layers[l], v).size()\n                        )\n\n                for c in self.connections:\n                    if hasattr(self.network.connections[c], v):\n                        self.recording[c][v] = torch.zeros(\n                            self.time, *getattr(self.network.connections[c], v).size()\n                        )\n\n    def get(self) -> Dict[str, Dict[str, Union[Nodes, AbstractConnection]]]:\n        # language=rst\n        """"""\n        Return entire recording to user.\n\n        :return: Dictionary of dictionary of all layers\' and connections\' recorded\n            state variables.\n        """"""\n        return self.recording\n\n    def record(self) -> None:\n        # language=rst\n        """"""\n        Appends the current value of the recorded state variables to the recording.\n        """"""\n        if self.time is None:\n            for v in self.state_vars:\n                for l in self.layers:\n                    if hasattr(self.network.layers[l], v):\n                        data = getattr(self.network.layers[l], v).unsqueeze(0).float()\n                        self.recording[l][v] = torch.cat(\n                            (self.recording[l][v], data), 0\n                        )\n\n                for c in self.connections:\n                    if hasattr(self.network.connections[c], v):\n                        data = getattr(self.network.connections[c], v).unsqueeze(0)\n                        self.recording[c][v] = torch.cat(\n                            (self.recording[c][v], data), 0\n                        )\n\n        else:\n            for v in self.state_vars:\n                for l in self.layers:\n                    if hasattr(self.network.layers[l], v):\n                        data = getattr(self.network.layers[l], v).float().unsqueeze(0)\n                        self.recording[l][v] = torch.cat(\n                            (self.recording[l][v][1:].type(data.type()), data), 0\n                        )\n\n                for c in self.connections:\n                    if hasattr(self.network.connections[c], v):\n                        data = getattr(self.network.connections[c], v).unsqueeze(0)\n                        self.recording[c][v] = torch.cat(\n                            (self.recording[c][v][1:].type(data.type()), data), 0\n                        )\n\n            self.i += 1\n\n    def save(self, path: str, fmt: str = ""npz"") -> None:\n        # language=rst\n        """"""\n        Write the recording dictionary out to file.\n\n        :param path: The directory to which to write the monitor\'s recording.\n        :param fmt: Type of file to write to disk. One of ``""pickle""`` or ``""npz""``.\n        """"""\n        if not os.path.exists(os.path.dirname(path)):\n            os.makedirs(os.path.dirname(path))\n\n        if fmt == ""npz"":\n            # Build a list of arrays to write to disk.\n            arrays = {}\n            for o in self.recording:\n                if type(o) == tuple:\n                    arrays.update(\n                        {\n                            ""_"".join([""-"".join(o), v]): self.recording[o][v]\n                            for v in self.recording[o]\n                        }\n                    )\n                elif type(o) == str:\n                    arrays.update(\n                        {\n                            ""_"".join([o, v]): self.recording[o][v]\n                            for v in self.recording[o]\n                        }\n                    )\n\n            np.savez_compressed(path, **arrays)\n\n        elif fmt == ""pickle"":\n            with open(path, ""wb"") as f:\n                torch.save(self.recording, f)\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Resets recordings to empty ``torch.Tensors``.\n        """"""\n        # Reset to empty recordings\n        self.recording = {k: {} for k in self.layers + self.connections}\n\n        if self.time is not None:\n            self.i = 0\n\n        # If no simulation time is specified, specify 0-dimensional recordings.\n        if self.time is None:\n            for v in self.state_vars:\n                for l in self.layers:\n                    if hasattr(self.network.layers[l], v):\n                        self.recording[l][v] = torch.Tensor()\n\n                for c in self.connections:\n                    if hasattr(self.network.connections[c], v):\n                        self.recording[c][v] = torch.Tensor()\n\n        # If simulation time is specified, pre-allocate recordings in memory for speed.\n        else:\n            for v in self.state_vars:\n                for l in self.layers:\n                    if hasattr(self.network.layers[l], v):\n                        self.recording[l][v] = torch.zeros(\n                            self.time, *getattr(self.network.layers[l], v).size()\n                        )\n\n                for c in self.connections:\n                    if hasattr(self.network.connections[c], v):\n                        self.recording[c][v] = torch.zeros(\n                            self.time, *getattr(self.network.layers[c], v).size()\n                        )\n'"
bindsnet/network/network.py,19,"b'import tempfile\nfrom typing import Dict, Optional, Type, Iterable\n\nimport torch\n\nfrom .monitors import AbstractMonitor\nfrom .nodes import Nodes\nfrom .topology import AbstractConnection\nfrom ..learning.reward import AbstractReward\n\n\ndef load(file_name: str, map_location: str = ""cpu"", learning: bool = None) -> ""Network"":\n    # language=rst\n    """"""\n    Loads serialized network object from disk.\n\n    :param file_name: Path to serialized network object on disk.\n    :param map_location: One of ``""cpu""`` or ``""cuda""``. Defaults to ``""cpu""``.\n    :param learning: Whether to load with learning enabled. Default loads value from\n        disk.\n    """"""\n    network = torch.load(open(file_name, ""rb""), map_location=map_location)\n    if learning is not None and ""learning"" in vars(network):\n        network.learning = learning\n\n    return network\n\n\nclass Network(torch.nn.Module):\n    # language=rst\n    """"""\n    Central object of the ``bindsnet`` package. Responsible for the simulation and\n    interaction of nodes and connections.\n\n    **Example:**\n\n    .. code-block:: python\n\n        import torch\n        import matplotlib.pyplot as plt\n\n        from bindsnet         import encoding\n        from bindsnet.network import Network, nodes, topology, monitors\n\n        network = Network(dt=1.0)  # Instantiates network.\n\n        X = nodes.Input(100)  # Input layer.\n        Y = nodes.LIFNodes(100)  # Layer of LIF neurons.\n        C = topology.Connection(source=X, target=Y, w=torch.rand(X.n, Y.n))  # Connection from X to Y.\n\n        # Spike monitor objects.\n        M1 = monitors.Monitor(obj=X, state_vars=[\'s\'])\n        M2 = monitors.Monitor(obj=Y, state_vars=[\'s\'])\n\n        # Add everything to the network object.\n        network.add_layer(layer=X, name=\'X\')\n        network.add_layer(layer=Y, name=\'Y\')\n        network.add_connection(connection=C, source=\'X\', target=\'Y\')\n        network.add_monitor(monitor=M1, name=\'X\')\n        network.add_monitor(monitor=M2, name=\'Y\')\n\n        # Create Poisson-distributed spike train inputs.\n        data = 15 * torch.rand(100)  # Generate random Poisson rates for 100 input neurons.\n        train = encoding.poisson(datum=data, time=5000)  # Encode input as 5000ms Poisson spike trains.\n\n        # Simulate network on generated spike trains.\n        inputs = {\'X\' : train}  # Create inputs mapping.\n        network.run(inputs=inputs, time=5000)  # Run network simulation.\n\n        # Plot spikes of input and output layers.\n        spikes = {\'X\' : M1.get(\'s\'), \'Y\' : M2.get(\'s\')}\n\n        fig, axes = plt.subplots(2, 1, figsize=(12, 7))\n        for i, layer in enumerate(spikes):\n            axes[i].matshow(spikes[layer], cmap=\'binary\')\n            axes[i].set_title(\'%s spikes\' % layer)\n            axes[i].set_xlabel(\'Time\'); axes[i].set_ylabel(\'Index of neuron\')\n            axes[i].set_xticks(()); axes[i].set_yticks(())\n            axes[i].set_aspect(\'auto\')\n\n        plt.tight_layout(); plt.show()\n    """"""\n\n    def __init__(\n        self,\n        dt: float = 1.0,\n        batch_size: int = 1,\n        learning: bool = True,\n        reward_fn: Optional[Type[AbstractReward]] = None,\n    ) -> None:\n        # language=rst\n        """"""\n        Initializes network object.\n\n        :param dt: Simulation timestep.\n        :param batch_size: Mini-batch size.\n        :param learning: Whether to allow connection updates. True by default.\n        :param reward_fn: Optional class allowing for modification of reward in case of\n            reward-modulated learning.\n        """"""\n        super().__init__()\n\n        self.dt = dt\n        self.batch_size = batch_size\n\n        self.layers = {}\n        self.connections = {}\n        self.monitors = {}\n\n        self.train(learning)\n\n        if reward_fn is not None:\n            self.reward_fn = reward_fn()\n        else:\n            self.reward_fn = None\n\n    def add_layer(self, layer: Nodes, name: str) -> None:\n        # language=rst\n        """"""\n        Adds a layer of nodes to the network.\n\n        :param layer: A subclass of the ``Nodes`` object.\n        :param name: Logical name of layer.\n        """"""\n        self.layers[name] = layer\n        self.add_module(name, layer)\n\n        layer.train(self.learning)\n        layer.compute_decays(self.dt)\n        layer.set_batch_size(self.batch_size)\n\n    def add_connection(\n        self, connection: AbstractConnection, source: str, target: str\n    ) -> None:\n        # language=rst\n        """"""\n        Adds a connection between layers of nodes to the network.\n\n        :param connection: An instance of class ``Connection``.\n        :param source: Logical name of the connection\'s source layer.\n        :param target: Logical name of the connection\'s target layer.\n        """"""\n        self.connections[(source, target)] = connection\n        self.add_module(source + ""_to_"" + target, connection)\n\n        connection.dt = self.dt\n        connection.train(self.learning)\n\n    def add_monitor(self, monitor: AbstractMonitor, name: str) -> None:\n        # language=rst\n        """"""\n        Adds a monitor on a network object to the network.\n\n        :param monitor: An instance of class ``Monitor``.\n        :param name: Logical name of monitor object.\n        """"""\n        self.monitors[name] = monitor\n        monitor.network = self\n        monitor.dt = self.dt\n\n    def save(self, file_name: str) -> None:\n        # language=rst\n        """"""\n        Serializes the network object to disk.\n\n        :param file_name: Path to store serialized network object on disk.\n\n        **Example:**\n\n        .. code-block:: python\n\n            import torch\n            import matplotlib.pyplot as plt\n\n            from pathlib          import Path\n            from bindsnet.network import *\n            from bindsnet.network import topology\n\n            # Build simple network.\n            network = Network(dt=1.0)\n\n            X = nodes.Input(100)  # Input layer.\n            Y = nodes.LIFNodes(100)  # Layer of LIF neurons.\n            C = topology.Connection(source=X, target=Y, w=torch.rand(X.n, Y.n))  # Connection from X to Y.\n\n            # Add everything to the network object.\n            network.add_layer(layer=X, name=\'X\')\n            network.add_layer(layer=Y, name=\'Y\')\n            network.add_connection(connection=C, source=\'X\', target=\'Y\')\n\n            # Save the network to disk.\n            network.save(str(Path.home()) + \'/network.pt\')\n        """"""\n        torch.save(self, open(file_name, ""wb""))\n\n    def clone(self) -> ""Network"":\n        # language=rst\n        """"""\n        Returns a cloned network object.\n        \n        :return: A copy of this network.\n        """"""\n        virtual_file = tempfile.SpooledTemporaryFile()\n        torch.save(self, virtual_file)\n        virtual_file.seek(0)\n        return torch.load(virtual_file)\n\n    def _get_inputs(self, layers: Iterable = None) -> Dict[str, torch.Tensor]:\n        # language=rst\n        """"""\n        Fetches outputs from network layers to use as input to downstream layers.\n\n        :param layers: Layers to update inputs for. Defaults to all network layers.\n        :return: Inputs to all layers for the current iteration.\n        """"""\n        inputs = {}\n\n        if layers is None:\n            layers = self.layers\n\n        # Loop over network connections.\n        for c in self.connections:\n            if c[1] in layers:\n                # Fetch source and target populations.\n                source = self.connections[c].source\n                target = self.connections[c].target\n\n                if not c[1] in inputs:\n                    inputs[c[1]] = torch.zeros(\n                        self.batch_size, *target.shape, device=target.s.device\n                    )\n\n                # Add to input: source\'s spikes multiplied by connection weights.\n                inputs[c[1]] += self.connections[c].compute(source.s)\n\n        return inputs\n\n    def run(\n        self, inputs: Dict[str, torch.Tensor], time: int, one_step=False, **kwargs\n    ) -> None:\n        # language=rst\n        """"""\n        Simulate network for given inputs and time.\n\n        :param inputs: Dictionary of ``Tensor``s of shape ``[time, *input_shape]`` or\n                      ``[time, batch_size, *input_shape]``.\n        :param time: Simulation time.\n        :param one_step: Whether to run the network in ""feed-forward"" mode, where inputs\n            propagate all the way through the network in a single simulation time step.\n            Layers are updated in the order they are added to the network.\n\n        Keyword arguments:\n\n        :param Dict[str, torch.Tensor] clamp: Mapping of layer names to boolean masks if\n            neurons should be clamped to spiking. The ``Tensor``s have shape\n            ``[n_neurons]`` or ``[time, n_neurons]``.\n        :param Dict[str, torch.Tensor] unclamp: Mapping of layer names to boolean masks\n            if neurons should be clamped to not spiking. The ``Tensor``s should have\n            shape ``[n_neurons]`` or ``[time, n_neurons]``.\n        :param Dict[str, torch.Tensor] injects_v: Mapping of layer names to boolean\n            masks if neurons should be added voltage. The ``Tensor``s should have shape\n            ``[n_neurons]`` or ``[time, n_neurons]``.\n        :param Union[float, torch.Tensor] reward: Scalar value used in reward-modulated\n            learning.\n        :param Dict[Tuple[str], torch.Tensor] masks: Mapping of connection names to\n            boolean masks determining which weights to clamp to zero.\n\n        **Example:**\n\n        .. code-block:: python\n\n            import torch\n            import matplotlib.pyplot as plt\n\n            from bindsnet.network import Network\n            from bindsnet.network.nodes import Input\n            from bindsnet.network.monitors import Monitor\n\n            # Build simple network.\n            network = Network()\n            network.add_layer(Input(500), name=\'I\')\n            network.add_monitor(Monitor(network.layers[\'I\'], state_vars=[\'s\']), \'I\')\n\n            # Generate spikes by running Bernoulli trials on Uniform(0, 0.5) samples.\n            spikes = torch.bernoulli(0.5 * torch.rand(500, 500))\n\n            # Run network simulation.\n            network.run(inputs={\'I\' : spikes}, time=500)\n\n            # Look at input spiking activity.\n            spikes = network.monitors[\'I\'].get(\'s\')\n            plt.matshow(spikes, cmap=\'binary\')\n            plt.xticks(()); plt.yticks(());\n            plt.xlabel(\'Time\'); plt.ylabel(\'Neuron index\')\n            plt.title(\'Input spiking\')\n            plt.show()\n        """"""\n        # Parse keyword arguments.\n        clamps = kwargs.get(""clamp"", {})\n        unclamps = kwargs.get(""unclamp"", {})\n        masks = kwargs.get(""masks"", {})\n        injects_v = kwargs.get(""injects_v"", {})\n\n        # Compute reward.\n        if self.reward_fn is not None:\n            kwargs[""reward""] = self.reward_fn.compute(**kwargs)\n\n        # Dynamic setting of batch size.\n        if inputs != {}:\n            for key in inputs:\n                # goal shape is [time, batch, n_0, ...]\n                if len(inputs[key].size()) == 1:\n                    # current shape is [n_0, ...]\n                    # unsqueeze twice to make [1, 1, n_0, ...]\n                    inputs[key] = inputs[key].unsqueeze(0).unsqueeze(0)\n                elif len(inputs[key].size()) == 2:\n                    # current shape is [time, n_0, ...]\n                    # unsqueeze dim 1 so that we have\n                    # [time, 1, n_0, ...]\n                    inputs[key] = inputs[key].unsqueeze(1)\n\n            for key in inputs:\n                # batch dimension is 1, grab this and use for batch size\n                if inputs[key].size(1) != self.batch_size:\n                    self.batch_size = inputs[key].size(1)\n\n                    for l in self.layers:\n                        self.layers[l].set_batch_size(self.batch_size)\n\n                    for m in self.monitors:\n                        self.monitors[m].reset_state_variables()\n\n                break\n\n        # Effective number of timesteps.\n        timesteps = int(time / self.dt)\n\n        # Simulate network activity for `time` timesteps.\n        for t in range(timesteps):\n            # Get input to all layers (synchronous mode).\n            current_inputs = {}\n            if not one_step:\n                current_inputs.update(self._get_inputs())\n\n            for l in self.layers:\n                # Update each layer of nodes.\n                if l in inputs:\n                    if l in current_inputs:\n                        current_inputs[l] += inputs[l][t]\n                    else:\n                        current_inputs[l] = inputs[l][t]\n\n                if one_step:\n                    # Get input to this layer (one-step mode).\n                    current_inputs.update(self._get_inputs(layers=[l]))\n\n                self.layers[l].forward(x=current_inputs[l])\n\n                # Clamp neurons to spike.\n                clamp = clamps.get(l, None)\n                if clamp is not None:\n                    if clamp.ndimension() == 1:\n                        self.layers[l].s[:, clamp] = 1\n                    else:\n                        self.layers[l].s[:, clamp[t]] = 1\n\n                # Clamp neurons not to spike.\n                unclamp = unclamps.get(l, None)\n                if unclamp is not None:\n                    if unclamp.ndimension() == 1:\n                        self.layers[l].s[unclamp] = 0\n                    else:\n                        self.layers[l].s[unclamp[t]] = 0\n\n                # Inject voltage to neurons.\n                inject_v = injects_v.get(l, None)\n                if inject_v is not None:\n                    if inject_v.ndimension() == 1:\n                        self.layers[l].v += inject_v\n                    else:\n                        self.layers[l].v += inject_v[t]\n\n            # Run synapse updates.\n            for c in self.connections:\n                self.connections[c].update(\n                    mask=masks.get(c, None), learning=self.learning, **kwargs\n                )\n\n            # Get input to all layers.\n            current_inputs.update(self._get_inputs())\n\n            # Record state variables of interest.\n            for m in self.monitors:\n                self.monitors[m].record()\n\n        # Re-normalize connections.\n        for c in self.connections:\n            self.connections[c].normalize()\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Reset state variables of objects in network.\n        """"""\n        for layer in self.layers:\n            self.layers[layer].reset_state_variables()\n\n        for connection in self.connections:\n            self.connections[connection].reset_state_variables()\n\n        for monitor in self.monitors:\n            self.monitors[monitor].reset_state_variables()\n\n    def train(self, mode: bool = True) -> ""torch.nn.Module"":\n        # language=rst\n        """"""\n        Sets the node in training mode.\n\n        :param mode: Turn training on or off.\n\n        :return: ``self`` as specified in ``torch.nn.Module``.\n        """"""\n        self.learning = mode\n        return super().train(mode)\n'"
bindsnet/network/nodes.py,202,"b'from abc import ABC, abstractmethod\nfrom functools import reduce\nfrom operator import mul\nfrom typing import Iterable, Optional, Union\n\nimport torch\n\n\nclass Nodes(torch.nn.Module):\n    # language=rst\n    """"""\n    Abstract base class for groups of neurons.\n    """"""\n\n    def __init__(\n        self,\n        n: Optional[int] = None,\n        shape: Optional[Iterable[int]] = None,\n        traces: bool = False,\n        traces_additive: bool = False,\n        tc_trace: Union[float, torch.Tensor] = 20.0,\n        trace_scale: Union[float, torch.Tensor] = 1.0,\n        sum_input: bool = False,\n        learning: bool = True,\n        **kwargs,\n    ) -> None:\n        # language=rst\n        """"""\n        Abstract base class constructor.\n\n        :param n: The number of neurons in the layer.\n        :param shape: The dimensionality of the layer.\n        :param traces: Whether to record decaying spike traces.\n        :param traces_additive: Whether to record spike traces additively.\n        :param tc_trace: Time constant of spike trace decay.\n        :param trace_scale: Scaling factor for spike trace.\n        :param sum_input: Whether to sum all inputs.\n        :param learning: Whether to be in learning or testing.\n        """"""\n        super().__init__()\n\n        assert (\n            n is not None or shape is not None\n        ), ""Must provide either no. of neurons or shape of layer""\n\n        if n is None:\n            self.n = reduce(mul, shape)  # No. of neurons product of shape.\n        else:\n            self.n = n  # No. of neurons provided.\n\n        if shape is None:\n            self.shape = [self.n]  # Shape is equal to the size of the layer.\n        else:\n            self.shape = shape  # Shape is passed in as an argument.\n\n        assert self.n == reduce(\n            mul, self.shape\n        ), ""No. of neurons and shape do not match""\n\n        self.traces = traces  # Whether to record synaptic traces.\n        self.traces_additive = (\n            traces_additive  # Whether to record spike traces additively.\n        )\n        self.register_buffer(""s"", torch.ByteTensor())  # Spike occurrences.\n\n        self.sum_input = sum_input  # Whether to sum all inputs.\n\n        if self.traces:\n            self.register_buffer(""x"", torch.Tensor())  # Firing traces.\n            self.register_buffer(\n                ""tc_trace"", torch.tensor(tc_trace)\n            )  # Time constant of spike trace decay.\n            if self.traces_additive:\n                self.register_buffer(\n                    ""trace_scale"", torch.tensor(trace_scale)\n                )  # Scaling factor for spike trace.\n            self.register_buffer(\n                ""trace_decay"", torch.empty_like(self.tc_trace)\n            )  # Set in compute_decays.\n\n        if self.sum_input:\n            self.register_buffer(""summed"", torch.FloatTensor())  # Summed inputs.\n\n        self.dt = None\n        self.batch_size = None\n        self.trace_decay = None\n        self.learning = learning\n\n    @abstractmethod\n    def forward(self, x: torch.Tensor) -> None:\n        # language=rst\n        """"""\n        Abstract base class method for a single simulation step.\n\n        :param x: Inputs to the layer.\n        """"""\n        if self.traces:\n            # Decay and set spike traces.\n            self.x *= self.trace_decay\n\n            if self.traces_additive:\n                self.x += self.trace_scale * self.s.float()\n            else:\n                self.x.masked_fill_(self.s != 0, 1)\n\n        if self.sum_input:\n            # Add current input to running sum.\n            self.summed += x.float()\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Abstract base class method for resetting state variables.\n        """"""\n        self.s.zero_()\n\n        if self.traces:\n            self.x.zero_()  # Spike traces.\n\n        if self.sum_input:\n            self.summed.zero_()  # Summed inputs.\n\n    def compute_decays(self, dt) -> None:\n        # language=rst\n        """"""\n        Abstract base class method for setting decays.\n        """"""\n        self.dt = dt\n        if self.traces:\n            self.trace_decay = torch.exp(\n                -self.dt / self.tc_trace\n            )  # Spike trace decay (per timestep).\n\n    def set_batch_size(self, batch_size) -> None:\n        # language=rst\n        """"""\n        Sets mini-batch size. Called when layer is added to a network.\n\n        :param batch_size: Mini-batch size.\n        """"""\n        self.batch_size = batch_size\n        self.s = torch.zeros(batch_size, *self.shape, device=self.s.device)\n\n        if self.traces:\n            self.x = torch.zeros(batch_size, *self.shape, device=self.x.device)\n\n        if self.sum_input:\n            self.summed = torch.zeros(\n                batch_size, *self.shape, device=self.summed.device\n            )\n\n    def train(self, mode: bool = True) -> ""Nodes"":\n        # language=rst\n        """"""\n        Sets the layer in training mode.\n\n        :param bool mode: Turn training on or off\n        :return: self as specified in `torch.nn.Module`\n        """"""\n        self.learning = mode\n        return super().train(mode)\n\n\nclass AbstractInput(ABC):\n    # language=rst\n    """"""\n    Abstract base class for groups of input neurons.\n    """"""\n\n\nclass Input(Nodes, AbstractInput):\n    # language=rst\n    """"""\n    Layer of nodes with user-specified spiking behavior.\n    """"""\n\n    def __init__(\n        self,\n        n: Optional[int] = None,\n        shape: Optional[Iterable[int]] = None,\n        traces: bool = False,\n        traces_additive: bool = False,\n        tc_trace: Union[float, torch.Tensor] = 20.0,\n        trace_scale: Union[float, torch.Tensor] = 1.0,\n        sum_input: bool = False,\n        **kwargs,\n    ) -> None:\n        # language=rst\n        """"""\n        Instantiates a layer of input neurons.\n\n        :param n: The number of neurons in the layer.\n        :param shape: The dimensionality of the layer.\n        :param traces: Whether to record decaying spike traces.\n        :param traces_additive: Whether to record spike traces additively.\n        :param tc_trace: Time constant of spike trace decay.\n        :param trace_scale: Scaling factor for spike trace.\n        :param sum_input: Whether to sum all inputs.\n        """"""\n        super().__init__(\n            n=n,\n            shape=shape,\n            traces=traces,\n            traces_additive=traces_additive,\n            tc_trace=tc_trace,\n            trace_scale=trace_scale,\n            sum_input=sum_input,\n        )\n\n    def forward(self, x: torch.Tensor) -> None:\n        # language=rst\n        """"""\n        On each simulation step, set the spikes of the population equal to the inputs.\n\n        :param x: Inputs to the layer.\n        """"""\n        # Set spike occurrences to input values.\n        self.s = x\n\n        super().forward(x)\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Resets relevant state variables.\n        """"""\n        super().reset_state_variables()\n\n\nclass McCullochPitts(Nodes):\n    # language=rst\n    """"""\n    Layer of `McCulloch-Pitts neurons\n    <http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html>`_.\n    """"""\n\n    def __init__(\n        self,\n        n: Optional[int] = None,\n        shape: Optional[Iterable[int]] = None,\n        traces: bool = False,\n        traces_additive: bool = False,\n        tc_trace: Union[float, torch.Tensor] = 20.0,\n        trace_scale: Union[float, torch.Tensor] = 1.0,\n        sum_input: bool = False,\n        thresh: Union[float, torch.Tensor] = 1.0,\n        **kwargs,\n    ) -> None:\n        # language=rst\n        """"""\n        Instantiates a McCulloch-Pitts layer of neurons.\n\n        :param n: The number of neurons in the layer.\n        :param shape: The dimensionality of the layer.\n        :param traces: Whether to record spike traces.\n        :param traces_additive: Whether to record spike traces additively.\n        :param tc_trace: Time constant of spike trace decay.\n        :param trace_scale: Scaling factor for spike trace.\n        :param sum_input: Whether to sum all inputs.\n        :param thresh: Spike threshold voltage.\n        """"""\n        super().__init__(\n            n=n,\n            shape=shape,\n            traces=traces,\n            traces_additive=traces_additive,\n            tc_trace=tc_trace,\n            trace_scale=trace_scale,\n            sum_input=sum_input,\n        )\n\n        self.register_buffer(\n            ""thresh"", torch.tensor(thresh, dtype=torch.float)\n        )  # Spike threshold voltage.\n        self.register_buffer(""v"", torch.FloatTensor())  # Neuron voltages.\n\n    def forward(self, x: torch.Tensor) -> None:\n        # language=rst\n        """"""\n        Runs a single simulation step.\n\n        :param x: Inputs to the layer.\n        """"""\n        self.v = x  # Voltages are equal to the inputs.\n        self.s = self.v >= self.thresh  # Check for spiking neurons.\n\n        super().forward(x)\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Resets relevant state variables.\n        """"""\n        super().reset_state_variables()\n\n    def set_batch_size(self, batch_size) -> None:\n        # language=rst\n        """"""\n        Sets mini-batch size. Called when layer is added to a network.\n\n        :param batch_size: Mini-batch size.\n        """"""\n        super().set_batch_size(batch_size=batch_size)\n        self.v = torch.zeros(batch_size, *self.shape, device=self.v.device)\n\n\nclass IFNodes(Nodes):\n    # language=rst\n    """"""\n    Layer of `integrate-and-fire (IF) neurons <http://neuronaldynamics.epfl.ch/online/Ch1.S3.html>`_.\n    """"""\n\n    def __init__(\n        self,\n        n: Optional[int] = None,\n        shape: Optional[Iterable[int]] = None,\n        traces: bool = False,\n        traces_additive: bool = False,\n        tc_trace: Union[float, torch.Tensor] = 20.0,\n        trace_scale: Union[float, torch.Tensor] = 1.0,\n        sum_input: bool = False,\n        thresh: Union[float, torch.Tensor] = -52.0,\n        reset: Union[float, torch.Tensor] = -65.0,\n        refrac: Union[int, torch.Tensor] = 5,\n        lbound: float = None,\n        **kwargs,\n    ) -> None:\n        # language=rst\n        """"""\n        Instantiates a layer of IF neurons.\n\n        :param n: The number of neurons in the layer.\n        :param shape: The dimensionality of the layer.\n        :param traces: Whether to record spike traces.\n        :param traces_additive: Whether to record spike traces additively.\n        :param tc_trace: Time constant of spike trace decay.\n        :param trace_scale: Scaling factor for spike trace.\n        :param sum_input: Whether to sum all inputs.\n        :param thresh: Spike threshold voltage.\n        :param reset: Post-spike reset voltage.\n        :param refrac: Refractory (non-firing) period of the neuron.\n        :param lbound: Lower bound of the voltage.\n        """"""\n        super().__init__(\n            n=n,\n            shape=shape,\n            traces=traces,\n            traces_additive=traces_additive,\n            tc_trace=tc_trace,\n            trace_scale=trace_scale,\n            sum_input=sum_input,\n        )\n\n        self.register_buffer(\n            ""reset"", torch.tensor(reset, dtype=torch.float)\n        )  # Post-spike reset voltage.\n        self.register_buffer(\n            ""thresh"", torch.tensor(thresh, dtype=torch.float)\n        )  # Spike threshold voltage.\n        self.register_buffer(\n            ""refrac"", torch.tensor(refrac)\n        )  # Post-spike refractory period.\n        self.register_buffer(""v"", torch.FloatTensor())  # Neuron voltages.\n        self.register_buffer(\n            ""refrac_count"", torch.FloatTensor()\n        )  # Refractory period counters.\n\n        self.lbound = lbound  # Lower bound of voltage.\n\n    def forward(self, x: torch.Tensor) -> None:\n        # language=rst\n        """"""\n        Runs a single simulation step.\n\n        :param x: Inputs to the layer.\n        """"""\n        # Integrate input voltages.\n        self.v += (self.refrac_count == 0).float() * x\n\n        # Decrement refractory counters.\n        self.refrac_count = (self.refrac_count > 0).float() * (\n            self.refrac_count - self.dt\n        )\n\n        # Check for spiking neurons.\n        self.s = self.v >= self.thresh\n\n        # Refractoriness and voltage reset.\n        self.refrac_count.masked_fill_(self.s, self.refrac)\n        self.v.masked_fill_(self.s, self.reset)\n\n        # Voltage clipping to lower bound.\n        if self.lbound is not None:\n            self.v.masked_fill_(self.v < self.lbound, self.lbound)\n\n        super().forward(x)\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Resets relevant state variables.\n        """"""\n        super().reset_state_variables()\n        self.v.fill_(self.reset)  # Neuron voltages.\n        self.refrac_count.zero_()  # Refractory period counters.\n\n    def set_batch_size(self, batch_size) -> None:\n        # language=rst\n        """"""\n        Sets mini-batch size. Called when layer is added to a network.\n\n        :param batch_size: Mini-batch size.\n        """"""\n        super().set_batch_size(batch_size=batch_size)\n        self.v = self.reset * torch.ones(batch_size, *self.shape, device=self.v.device)\n        self.refrac_count = torch.zeros_like(self.v, device=self.refrac_count.device)\n\n\nclass LIFNodes(Nodes):\n    # language=rst\n    """"""\n    Layer of `leaky integrate-and-fire (LIF) neurons\n    <http://icwww.epfl.ch/~gerstner/SPNM/node26.html#SECTION02311000000000000000>`_.\n    """"""\n\n    def __init__(\n        self,\n        n: Optional[int] = None,\n        shape: Optional[Iterable[int]] = None,\n        traces: bool = False,\n        traces_additive: bool = False,\n        tc_trace: Union[float, torch.Tensor] = 20.0,\n        trace_scale: Union[float, torch.Tensor] = 1.0,\n        sum_input: bool = False,\n        thresh: Union[float, torch.Tensor] = -52.0,\n        rest: Union[float, torch.Tensor] = -65.0,\n        reset: Union[float, torch.Tensor] = -65.0,\n        refrac: Union[int, torch.Tensor] = 5,\n        tc_decay: Union[float, torch.Tensor] = 100.0,\n        lbound: float = None,\n        **kwargs,\n    ) -> None:\n        # language=rst\n        """"""\n        Instantiates a layer of LIF neurons.\n\n        :param n: The number of neurons in the layer.\n        :param shape: The dimensionality of the layer.\n        :param traces: Whether to record spike traces.\n        :param traces_additive: Whether to record spike traces additively.\n        :param tc_trace: Time constant of spike trace decay.\n        :param trace_scale: Scaling factor for spike trace.\n        :param sum_input: Whether to sum all inputs.\n        :param thresh: Spike threshold voltage.\n        :param rest: Resting membrane voltage.\n        :param reset: Post-spike reset voltage.\n        :param refrac: Refractory (non-firing) period of the neuron.\n        :param tc_decay: Time constant of neuron voltage decay.\n        :param lbound: Lower bound of the voltage.\n        """"""\n        super().__init__(\n            n=n,\n            shape=shape,\n            traces=traces,\n            traces_additive=traces_additive,\n            tc_trace=tc_trace,\n            trace_scale=trace_scale,\n            sum_input=sum_input,\n        )\n\n        self.register_buffer(\n            ""rest"", torch.tensor(rest, dtype=torch.float)\n        )  # Rest voltage.\n        self.register_buffer(\n            ""reset"", torch.tensor(reset, dtype=torch.float)\n        )  # Post-spike reset voltage.\n        self.register_buffer(\n            ""thresh"", torch.tensor(thresh, dtype=torch.float)\n        )  # Spike threshold voltage.\n        self.register_buffer(\n            ""refrac"", torch.tensor(refrac)\n        )  # Post-spike refractory period.\n        self.register_buffer(\n            ""tc_decay"", torch.tensor(tc_decay, dtype=torch.float)\n        )  # Time constant of neuron voltage decay.\n        self.register_buffer(\n            ""decay"", torch.zeros(*self.shape)\n        )  # Set in compute_decays.\n        self.register_buffer(""v"", torch.FloatTensor())  # Neuron voltages.\n        self.register_buffer(\n            ""refrac_count"", torch.FloatTensor()\n        )  # Refractory period counters.\n\n        if lbound is None:\n            self.lbound = None  # Lower bound of voltage.\n        else:\n            self.lbound = torch.tensor(\n                lbound, dtype=torch.float\n            )  # Lower bound of voltage.\n\n    def forward(self, x: torch.Tensor) -> None:\n        # language=rst\n        """"""\n        Runs a single simulation step.\n\n        :param x: Inputs to the layer.\n        """"""\n        # Decay voltages.\n        self.v = self.decay * (self.v - self.rest) + self.rest\n\n        # Integrate inputs.\n        self.v += (self.refrac_count == 0).float() * x\n\n        # Decrement refractory counters.\n        self.refrac_count = (self.refrac_count > 0).float() * (\n            self.refrac_count - self.dt\n        )\n\n        # Check for spiking neurons.\n        self.s = self.v >= self.thresh\n\n        # Refractoriness and voltage reset.\n        self.refrac_count.masked_fill_(self.s, self.refrac)\n        self.v.masked_fill_(self.s, self.reset)\n\n        # Voltage clipping to lower bound.\n        if self.lbound is not None:\n            self.v.masked_fill_(self.v < self.lbound, self.lbound)\n\n        super().forward(x)\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Resets relevant state variables.\n        """"""\n        super().reset_state_variables()\n        self.v.fill_(self.rest)  # Neuron voltages.\n        self.refrac_count.zero_()  # Refractory period counters.\n\n    def compute_decays(self, dt) -> None:\n        # language=rst\n        """"""\n        Sets the relevant decays.\n        """"""\n        super().compute_decays(dt=dt)\n        self.decay = torch.exp(\n            -self.dt / self.tc_decay\n        )  # Neuron voltage decay (per timestep).\n\n    def set_batch_size(self, batch_size) -> None:\n        # language=rst\n        """"""\n        Sets mini-batch size. Called when layer is added to a network.\n\n        :param batch_size: Mini-batch size.\n        """"""\n        super().set_batch_size(batch_size=batch_size)\n        self.v = self.rest * torch.ones(batch_size, *self.shape, device=self.v.device)\n        self.refrac_count = torch.zeros_like(self.v, device=self.refrac_count.device)\n\n\nclass CurrentLIFNodes(Nodes):\n    # language=rst\n    """"""\n    Layer of `current-based leaky integrate-and-fire (LIF) neurons\n    <http://icwww.epfl.ch/~gerstner/SPNM/node26.html#SECTION02313000000000000000>`_.\n    Total synaptic input current is modeled as a decaying memory of input spikes multiplied by synaptic strengths.\n    """"""\n\n    def __init__(\n        self,\n        n: Optional[int] = None,\n        shape: Optional[Iterable[int]] = None,\n        traces: bool = False,\n        traces_additive: bool = False,\n        tc_trace: Union[float, torch.Tensor] = 20.0,\n        trace_scale: Union[float, torch.Tensor] = 1.0,\n        sum_input: bool = False,\n        thresh: Union[float, torch.Tensor] = -52.0,\n        rest: Union[float, torch.Tensor] = -65.0,\n        reset: Union[float, torch.Tensor] = -65.0,\n        refrac: Union[int, torch.Tensor] = 5,\n        tc_decay: Union[float, torch.Tensor] = 100.0,\n        tc_i_decay: Union[float, torch.Tensor] = 2.0,\n        lbound: float = None,\n        **kwargs,\n    ) -> None:\n        # language=rst\n        """"""\n        Instantiates a layer of synaptic input current-based LIF neurons.\n        :param n: The number of neurons in the layer.\n        :param shape: The dimensionality of the layer.\n        :param traces: Whether to record spike traces.\n        :param traces_additive: Whether to record spike traces additively.\n        :param tc_trace: Time constant of spike trace decay.\n        :param trace_scale: Scaling factor for spike trace.\n        :param sum_input: Whether to sum all inputs.\n        :param thresh: Spike threshold voltage.\n        :param rest: Resting membrane voltage.\n        :param reset: Post-spike reset voltage.\n        :param refrac: Refractory (non-firing) period of the neuron.\n        :param tc_decay: Time constant of neuron voltage decay.\n        :param tc_i_decay: Time constant of synaptic input current decay.\n        :param lbound: Lower bound of the voltage.\n        """"""\n        super().__init__(\n            n=n,\n            shape=shape,\n            traces=traces,\n            traces_additive=traces_additive,\n            tc_trace=tc_trace,\n            trace_scale=trace_scale,\n            sum_input=sum_input,\n        )\n\n        self.register_buffer(""rest"", torch.tensor(rest))  # Rest voltage.\n        self.register_buffer(""reset"", torch.tensor(reset))  # Post-spike reset voltage.\n        self.register_buffer(""thresh"", torch.tensor(thresh))  # Spike threshold voltage.\n        self.register_buffer(\n            ""refrac"", torch.tensor(refrac)\n        )  # Post-spike refractory period.\n        self.register_buffer(\n            ""tc_decay"", torch.tensor(tc_decay)\n        )  # Time constant of neuron voltage decay.\n        self.register_buffer(\n            ""decay"", torch.empty_like(self.tc_decay)\n        )  # Set in compute_decays.\n        self.register_buffer(\n            ""tc_i_decay"", torch.tensor(tc_i_decay)\n        )  # Time constant of synaptic input current decay.\n        self.register_buffer(\n            ""i_decay"", torch.empty_like(self.tc_i_decay)\n        )  # Set in compute_decays.\n\n        self.register_buffer(""v"", torch.FloatTensor())  # Neuron voltages.\n        self.register_buffer(""i"", torch.FloatTensor())  # Synaptic input currents.\n        self.register_buffer(\n            ""refrac_count"", torch.FloatTensor()\n        )  # Refractory period counters.\n\n        self.lbound = lbound  # Lower bound of voltage.\n\n    def forward(self, x: torch.Tensor) -> None:\n        # language=rst\n        """"""\n        Runs a single simulation step.\n\n        :param x: Inputs to the layer.\n        """"""\n        # Decay voltages and current.\n        self.v = self.decay * (self.v - self.rest) + self.rest\n        self.i *= self.i_decay\n\n        # Decrement refractory counters.\n        self.refrac_count = (self.refrac_count > 0).float() * (\n            self.refrac_count - self.dt\n        )\n\n        # Integrate inputs.\n        self.i += x\n        self.v += (self.refrac_count == 0).float() * self.i\n\n        # Check for spiking neurons.\n        self.s = self.v >= self.thresh\n\n        # Refractoriness and voltage reset.\n        self.refrac_count.masked_fill_(self.s, self.refrac)\n        self.v.masked_fill_(self.s, self.reset)\n\n        # Voltage clipping to lower bound.\n        if self.lbound is not None:\n            self.v.masked_fill_(self.v < self.lbound, self.lbound)\n\n        super().forward(x)\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Resets relevant state variables.\n        """"""\n        super().reset_state_variables()\n        self.v.fill_(self.rest)  # Neuron voltages.\n        self.i.zero_()  # Synaptic input currents.\n        self.refrac_count.zero_()  # Refractory period counters.\n\n    def compute_decays(self, dt) -> None:\n        # language=rst\n        """"""\n        Sets the relevant decays.\n        """"""\n        super().compute_decays(dt=dt)\n        self.decay = torch.exp(\n            -self.dt / self.tc_decay\n        )  # Neuron voltage decay (per timestep).\n        self.i_decay = torch.exp(\n            -self.dt / self.tc_i_decay\n        )  # Synaptic input current decay (per timestep).\n\n    def set_batch_size(self, batch_size) -> None:\n        # language=rst\n        """"""\n        Sets mini-batch size. Called when layer is added to a network.\n\n        :param batch_size: Mini-batch size.\n        """"""\n        super().set_batch_size(batch_size=batch_size)\n        self.v = self.rest * torch.ones(batch_size, *self.shape, device=self.v.device)\n        self.i = torch.zeros_like(self.v, device=self.i.device)\n        self.refrac_count = torch.zeros_like(self.v, device=self.refrac_count.device)\n\n\nclass AdaptiveLIFNodes(Nodes):\n    # language=rst\n    """"""\n    Layer of leaky integrate-and-fire (LIF) neurons with adaptive thresholds. A neuron\'s voltage threshold is increased\n    by some constant each time it spikes; otherwise, it is decaying back to its default value.\n    """"""\n\n    def __init__(\n        self,\n        n: Optional[int] = None,\n        shape: Optional[Iterable[int]] = None,\n        traces: bool = False,\n        traces_additive: bool = False,\n        tc_trace: Union[float, torch.Tensor] = 20.0,\n        trace_scale: Union[float, torch.Tensor] = 1.0,\n        sum_input: bool = False,\n        rest: Union[float, torch.Tensor] = -65.0,\n        reset: Union[float, torch.Tensor] = -65.0,\n        thresh: Union[float, torch.Tensor] = -52.0,\n        refrac: Union[int, torch.Tensor] = 5,\n        tc_decay: Union[float, torch.Tensor] = 100.0,\n        theta_plus: Union[float, torch.Tensor] = 0.05,\n        tc_theta_decay: Union[float, torch.Tensor] = 1e7,\n        lbound: float = None,\n        **kwargs,\n    ) -> None:\n        # language=rst\n        """"""\n        Instantiates a layer of LIF neurons with adaptive firing thresholds.\n\n        :param n: The number of neurons in the layer.\n        :param shape: The dimensionality of the layer.\n        :param traces: Whether to record spike traces.\n        :param traces_additive: Whether to record spike traces additively.\n        :param tc_trace: Time constant of spike trace decay.\n        :param trace_scale: Scaling factor for spike trace.\n        :param sum_input: Whether to sum all inputs.\n        :param rest: Resting membrane voltage.\n        :param reset: Post-spike reset voltage.\n        :param thresh: Spike threshold voltage.\n        :param refrac: Refractory (non-firing) period of the neuron.\n        :param tc_decay: Time constant of neuron voltage decay.\n        :param theta_plus: Voltage increase of threshold after spiking.\n        :param tc_theta_decay: Time constant of adaptive threshold decay.\n        :param lbound: Lower bound of the voltage.\n        """"""\n        super().__init__(\n            n=n,\n            shape=shape,\n            traces=traces,\n            traces_additive=traces_additive,\n            tc_trace=tc_trace,\n            trace_scale=trace_scale,\n            sum_input=sum_input,\n        )\n\n        self.register_buffer(""rest"", torch.tensor(rest))  # Rest voltage.\n        self.register_buffer(""reset"", torch.tensor(reset))  # Post-spike reset voltage.\n        self.register_buffer(""thresh"", torch.tensor(thresh))  # Spike threshold voltage.\n        self.register_buffer(\n            ""refrac"", torch.tensor(refrac)\n        )  # Post-spike refractory period.\n        self.register_buffer(\n            ""tc_decay"", torch.tensor(tc_decay)\n        )  # Time constant of neuron voltage decay.\n        self.register_buffer(\n            ""decay"", torch.empty_like(self.tc_decay)\n        )  # Set in compute_decays.\n        self.register_buffer(\n            ""theta_plus"", torch.tensor(theta_plus)\n        )  # Constant threshold increase on spike.\n        self.register_buffer(\n            ""tc_theta_decay"", torch.tensor(tc_theta_decay)\n        )  # Time constant of adaptive threshold decay.\n        self.register_buffer(\n            ""theta_decay"", torch.empty_like(self.tc_theta_decay)\n        )  # Set in compute_decays.\n\n        self.register_buffer(""v"", torch.FloatTensor())  # Neuron voltages.\n        self.register_buffer(""theta"", torch.zeros(*self.shape))  # Adaptive thresholds.\n        self.register_buffer(\n            ""refrac_count"", torch.FloatTensor()\n        )  # Refractory period counters.\n        self.lbound = lbound  # Lower bound of voltage.\n\n    def forward(self, x: torch.Tensor) -> None:\n        # language=rst\n        """"""\n        Runs a single simulation step.\n\n        :param x: Inputs to the layer.\n        """"""\n        # Decay voltages and adaptive thresholds.\n        self.v = self.decay * (self.v - self.rest) + self.rest\n        if self.learning:\n            self.theta *= self.theta_decay\n\n        # Integrate inputs.\n        self.v += (self.refrac_count == 0).float() * x\n\n        # Decrement refractory counters.\n        self.refrac_count = (self.refrac_count > 0).float() * (\n            self.refrac_count - self.dt\n        )\n\n        # Check for spiking neurons.\n        self.s = self.v >= self.thresh + self.theta\n\n        # Refractoriness, voltage reset, and adaptive thresholds.\n        self.refrac_count.masked_fill_(self.s, self.refrac)\n        self.v.masked_fill_(self.s, self.reset)\n        if self.learning:\n            self.theta += self.theta_plus * self.s.float().sum(0)\n\n        # voltage clipping to lowerbound\n        if self.lbound is not None:\n            self.v.masked_fill_(self.v < self.lbound, self.lbound)\n\n        super().forward(x)\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Resets relevant state variables.\n        """"""\n        super().reset_state_variables()\n        self.v.fill_(self.rest)  # Neuron voltages.\n        self.refrac_count.zero_()  # Refractory period counters.\n\n    def compute_decays(self, dt) -> None:\n        # language=rst\n        """"""\n        Sets the relevant decays.\n        """"""\n        super().compute_decays(dt=dt)\n        self.decay = torch.exp(\n            -self.dt / self.tc_decay\n        )  # Neuron voltage decay (per timestep).\n        self.theta_decay = torch.exp(\n            -self.dt / self.tc_theta_decay\n        )  # Adaptive threshold decay (per timestep).\n\n    def set_batch_size(self, batch_size) -> None:\n        # language=rst\n        """"""\n        Sets mini-batch size. Called when layer is added to a network.\n\n        :param batch_size: Mini-batch size.\n        """"""\n        super().set_batch_size(batch_size=batch_size)\n        self.v = self.rest * torch.ones(batch_size, *self.shape, device=self.v.device)\n        self.refrac_count = torch.zeros_like(self.v, device=self.refrac_count.device)\n\n\nclass DiehlAndCookNodes(Nodes):\n    # language=rst\n    """"""\n    Layer of leaky integrate-and-fire (LIF) neurons with adaptive thresholds (modified for Diehl & Cook 2015\n    replication).\n    """"""\n\n    def __init__(\n        self,\n        n: Optional[int] = None,\n        shape: Optional[Iterable[int]] = None,\n        traces: bool = False,\n        traces_additive: bool = False,\n        tc_trace: Union[float, torch.Tensor] = 20.0,\n        trace_scale: Union[float, torch.Tensor] = 1.0,\n        sum_input: bool = False,\n        thresh: Union[float, torch.Tensor] = -52.0,\n        rest: Union[float, torch.Tensor] = -65.0,\n        reset: Union[float, torch.Tensor] = -65.0,\n        refrac: Union[int, torch.Tensor] = 5,\n        tc_decay: Union[float, torch.Tensor] = 100.0,\n        theta_plus: Union[float, torch.Tensor] = 0.05,\n        tc_theta_decay: Union[float, torch.Tensor] = 1e7,\n        lbound: float = None,\n        one_spike: bool = True,\n        **kwargs,\n    ) -> None:\n        # language=rst\n        """"""\n        Instantiates a layer of Diehl & Cook 2015 neurons.\n\n        :param n: The number of neurons in the layer.\n        :param shape: The dimensionality of the layer.\n        :param traces: Whether to record spike traces.\n        :param traces_additive: Whether to record spike traces additively.\n        :param tc_trace: Time constant of spike trace decay.\n        :param trace_scale: Scaling factor for spike trace.\n        :param sum_input: Whether to sum all inputs.\n        :param thresh: Spike threshold voltage.\n        :param rest: Resting membrane voltage.\n        :param reset: Post-spike reset voltage.\n        :param refrac: Refractory (non-firing) period of the neuron.\n        :param tc_decay: Time constant of neuron voltage decay.\n        :param theta_plus: Voltage increase of threshold after spiking.\n        :param tc_theta_decay: Time constant of adaptive threshold decay.\n        :param lbound: Lower bound of the voltage.\n        :param one_spike: Whether to allow only one spike per timestep.\n        """"""\n        super().__init__(\n            n=n,\n            shape=shape,\n            traces=traces,\n            traces_additive=traces_additive,\n            tc_trace=tc_trace,\n            trace_scale=trace_scale,\n            sum_input=sum_input,\n        )\n\n        self.register_buffer(""rest"", torch.tensor(rest))  # Rest voltage.\n        self.register_buffer(""reset"", torch.tensor(reset))  # Post-spike reset voltage.\n        self.register_buffer(""thresh"", torch.tensor(thresh))  # Spike threshold voltage.\n        self.register_buffer(\n            ""refrac"", torch.tensor(refrac)\n        )  # Post-spike refractory period.\n        self.register_buffer(\n            ""tc_decay"", torch.tensor(tc_decay)\n        )  # Time constant of neuron voltage decay.\n        self.register_buffer(\n            ""decay"", torch.empty_like(self.tc_decay)\n        )  # Set in compute_decays.\n        self.register_buffer(\n            ""theta_plus"", torch.tensor(theta_plus)\n        )  # Constant threshold increase on spike.\n        self.register_buffer(\n            ""tc_theta_decay"", torch.tensor(tc_theta_decay)\n        )  # Time constant of adaptive threshold decay.\n        self.register_buffer(\n            ""theta_decay"", torch.empty_like(self.tc_theta_decay)\n        )  # Set in compute_decays.\n        self.register_buffer(""v"", torch.FloatTensor())  # Neuron voltages.\n        self.register_buffer(""theta"", torch.zeros(*self.shape))  # Adaptive thresholds.\n        self.register_buffer(\n            ""refrac_count"", torch.FloatTensor()\n        )  # Refractory period counters.\n\n        self.lbound = lbound  # Lower bound of voltage.\n        self.one_spike = one_spike  # One spike per timestep.\n\n    def forward(self, x: torch.Tensor) -> None:\n        # language=rst\n        """"""\n        Runs a single simulation step.\n\n        :param x: Inputs to the layer.\n        """"""\n        # Decay voltages and adaptive thresholds.\n        self.v = self.decay * (self.v - self.rest) + self.rest\n        if self.learning:\n            self.theta *= self.theta_decay\n\n        # Integrate inputs.\n        self.v += (self.refrac_count == 0).float() * x\n\n        # Decrement refractory counters.\n        self.refrac_count = (self.refrac_count > 0).float() * (\n            self.refrac_count - self.dt\n        )\n\n        # Check for spiking neurons.\n        self.s = self.v >= self.thresh + self.theta\n\n        # Refractoriness, voltage reset, and adaptive thresholds.\n        self.refrac_count.masked_fill_(self.s, self.refrac)\n        self.v.masked_fill_(self.s, self.reset)\n        if self.learning:\n            self.theta += self.theta_plus * self.s.float().sum(0)\n\n        # Choose only a single neuron to spike.\n        if self.one_spike:\n            if self.s.any():\n                _any = self.s.view(self.batch_size, -1).any(1)\n                ind = torch.multinomial(\n                    self.s.float().view(self.batch_size, -1)[_any], 1\n                )\n                _any = _any.nonzero()\n                self.s.zero_()\n                self.s.view(self.batch_size, -1)[_any, ind] = 1\n\n        # Voltage clipping to lower bound.\n        if self.lbound is not None:\n            self.v.masked_fill_(self.v < self.lbound, self.lbound)\n\n        super().forward(x)\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Resets relevant state variables.\n        """"""\n        super().reset_state_variables()\n        self.v.fill_(self.rest)  # Neuron voltages.\n        self.refrac_count.zero_()  # Refractory period counters.\n\n    def compute_decays(self, dt) -> None:\n        # language=rst\n        """"""\n        Sets the relevant decays.\n        """"""\n        super().compute_decays(dt=dt)\n        self.decay = torch.exp(\n            -self.dt / self.tc_decay\n        )  # Neuron voltage decay (per timestep).\n        self.theta_decay = torch.exp(\n            -self.dt / self.tc_theta_decay\n        )  # Adaptive threshold decay (per timestep).\n\n    def set_batch_size(self, batch_size) -> None:\n        # language=rst\n        """"""\n        Sets mini-batch size. Called when layer is added to a network.\n\n        :param batch_size: Mini-batch size.\n        """"""\n        super().set_batch_size(batch_size=batch_size)\n        self.v = self.rest * torch.ones(batch_size, *self.shape, device=self.v.device)\n        self.refrac_count = torch.zeros_like(self.v, device=self.refrac_count.device)\n\n\nclass IzhikevichNodes(Nodes):\n    # language=rst\n    """"""\n    Layer of Izhikevich neurons.\n    """"""\n\n    def __init__(\n        self,\n        n: Optional[int] = None,\n        shape: Optional[Iterable[int]] = None,\n        traces: bool = False,\n        traces_additive: bool = False,\n        tc_trace: Union[float, torch.Tensor] = 20.0,\n        trace_scale: Union[float, torch.Tensor] = 1.0,\n        sum_input: bool = False,\n        excitatory: float = 1,\n        thresh: Union[float, torch.Tensor] = 45.0,\n        rest: Union[float, torch.Tensor] = -65.0,\n        lbound: float = None,\n        **kwargs,\n    ) -> None:\n        # language=rst\n        """"""\n        Instantiates a layer of Izhikevich neurons.\n\n        :param n: The number of neurons in the layer.\n        :param shape: The dimensionality of the layer.\n        :param traces: Whether to record spike traces.\n        :param traces_additive: Whether to record spike traces additively.\n        :param tc_trace: Time constant of spike trace decay.\n        :param trace_scale: Scaling factor for spike trace.\n        :param sum_input: Whether to sum all inputs.\n        :param excitatory: Percent of excitatory (vs. inhibitory) neurons in the layer; in range ``[0, 1]``.\n        :param thresh: Spike threshold voltage.\n        :param rest: Resting membrane voltage.\n        :param lbound: Lower bound of the voltage.\n        """"""\n        super().__init__(\n            n=n,\n            shape=shape,\n            traces=traces,\n            traces_additive=traces_additive,\n            tc_trace=tc_trace,\n            trace_scale=trace_scale,\n            sum_input=sum_input,\n        )\n\n        self.register_buffer(""rest"", torch.tensor(rest))  # Rest voltage.\n        self.register_buffer(""thresh"", torch.tensor(thresh))  # Spike threshold voltage.\n        self.lbound = lbound\n\n        self.register_buffer(""r"", None)\n        self.register_buffer(""a"", None)\n        self.register_buffer(""b"", None)\n        self.register_buffer(""c"", None)\n        self.register_buffer(""d"", None)\n        self.register_buffer(""S"", None)\n        self.register_buffer(""excitatory"", None)\n\n        if excitatory > 1:\n            excitatory = 1\n        elif excitatory < 0:\n            excitatory = 0\n\n        if excitatory == 1:\n            self.r = torch.rand(n)\n            self.a = 0.02 * torch.ones(n)\n            self.b = 0.2 * torch.ones(n)\n            self.c = -65.0 + 15 * (self.r ** 2)\n            self.d = 8 - 6 * (self.r ** 2)\n            self.S = 0.5 * torch.rand(n, n)\n            self.excitatory = torch.ones(n).byte()\n\n        elif excitatory == 0:\n            self.r = torch.rand(n)\n            self.a = 0.02 + 0.08 * self.r\n            self.b = 0.25 - 0.05 * self.r\n            self.c = -65.0 * torch.ones(n)\n            self.d = 2 * torch.ones(n)\n            self.S = -torch.rand(n, n)\n\n            self.excitatory = torch.zeros(n).byte()\n\n        else:\n            self.excitatory = torch.zeros(n).byte()\n\n            ex = int(n * excitatory)\n            inh = n - ex\n\n            # init\n            self.r = torch.zeros(n)\n            self.a = torch.zeros(n)\n            self.b = torch.zeros(n)\n            self.c = torch.zeros(n)\n            self.d = torch.zeros(n)\n            self.S = torch.zeros(n, n)\n\n            # excitatory\n            self.r[:ex] = torch.rand(ex)\n            self.a[:ex] = 0.02 * torch.ones(ex)\n            self.b[:ex] = 0.2 * torch.ones(ex)\n            self.c[:ex] = -65.0 + 15 * self.r[:ex] ** 2\n            self.d[:ex] = 8 - 6 * self.r[:ex] ** 2\n            self.S[:, :ex] = 0.5 * torch.rand(n, ex)\n            self.excitatory[:ex] = 1\n\n            # inhibitory\n            self.r[ex:] = torch.rand(inh)\n            self.a[ex:] = 0.02 + 0.08 * self.r[ex:]\n            self.b[ex:] = 0.25 - 0.05 * self.r[ex:]\n            self.c[ex:] = -65.0 * torch.ones(inh)\n            self.d[ex:] = 2 * torch.ones(inh)\n            self.S[:, ex:] = -torch.rand(n, inh)\n            self.excitatory[ex:] = 0\n\n        self.register_buffer(""v"", self.rest * torch.ones(n))  # Neuron voltages.\n        self.register_buffer(""u"", self.b * self.v)  # Neuron recovery.\n\n    def forward(self, x: torch.Tensor) -> None:\n        # language=rst\n        """"""\n        Runs a single simulation step.\n\n        :param x: Inputs to the layer.\n        """"""\n        # Check for spiking neurons.\n        self.s = self.v >= self.thresh\n\n        # Voltage and recovery reset.\n        self.v = torch.where(self.s, self.c, self.v)\n        self.u = torch.where(self.s, self.u + self.d, self.u)\n\n        # Add inter-columnar input.\n        if self.s.any():\n            x += torch.cat(\n                [self.S[:, self.s[i]].sum(dim=1)[None] for i in range(self.s.shape[0])],\n                dim=0,\n            )\n\n        # Apply v and u updates.\n        self.v += self.dt * 0.5 * (0.04 * self.v ** 2 + 5 * self.v + 140 - self.u + x)\n        self.v += self.dt * 0.5 * (0.04 * self.v ** 2 + 5 * self.v + 140 - self.u + x)\n        self.u += self.dt * self.a * (self.b * self.v - self.u)\n\n        # Voltage clipping to lower bound.\n        if self.lbound is not None:\n            self.v.masked_fill_(self.v < self.lbound, self.lbound)\n\n        super().forward(x)\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Resets relevant state variables.\n        """"""\n        super().reset_state_variables()\n        self.v.fill_(self.rest)  # Neuron voltages.\n        self.u = self.b * self.v  # Neuron recovery.\n\n    def set_batch_size(self, batch_size) -> None:\n        # language=rst\n        """"""\n        Sets mini-batch size. Called when layer is added to a network.\n\n        :param batch_size: Mini-batch size.\n        """"""\n        super().set_batch_size(batch_size=batch_size)\n        self.v = self.rest * torch.ones(batch_size, *self.shape, device=self.v.device)\n        self.u = self.b * self.v\n\n\nclass SRM0Nodes(Nodes):\n    # language=rst\n    """"""\n    Layer of simplified spike response model (SRM0) neurons with stochastic threshold (escape noise). Adapted from\n    `(Vasilaki et al., 2009) <https://intranet.physio.unibe.ch/Publikationen/Dokumente/Vasilaki2009PloSComputBio_1.pdf>`_.\n    """"""\n\n    def __init__(\n        self,\n        n: Optional[int] = None,\n        shape: Optional[Iterable[int]] = None,\n        traces: bool = False,\n        traces_additive: bool = False,\n        tc_trace: Union[float, torch.Tensor] = 20.0,\n        trace_scale: Union[float, torch.Tensor] = 1.0,\n        sum_input: bool = False,\n        thresh: Union[float, torch.Tensor] = -50.0,\n        rest: Union[float, torch.Tensor] = -70.0,\n        reset: Union[float, torch.Tensor] = -70.0,\n        refrac: Union[int, torch.Tensor] = 5,\n        tc_decay: Union[float, torch.Tensor] = 10.0,\n        lbound: float = None,\n        eps_0: Union[float, torch.Tensor] = 1.0,\n        rho_0: Union[float, torch.Tensor] = 1.0,\n        d_thresh: Union[float, torch.Tensor] = 5.0,\n        **kwargs,\n    ) -> None:\n        # language=rst\n        """"""\n        Instantiates a layer of SRM0 neurons.\n\n        :param n: The number of neurons in the layer.\n        :param shape: The dimensionality of the layer.\n        :param traces: Whether to record spike traces.\n        :param traces_additive: Whether to record spike traces additively.\n        :param tc_trace: Time constant of spike trace decay.\n        :param trace_scale: Scaling factor for spike trace.\n        :param sum_input: Whether to sum all inputs.\n        :param thresh: Spike threshold voltage.\n        :param rest: Resting membrane voltage.\n        :param reset: Post-spike reset voltage.\n        :param refrac: Refractory (non-firing) period of the neuron.\n        :param tc_decay: Time constant of neuron voltage decay.\n        :param lbound: Lower bound of the voltage.\n        :param eps_0: Scaling factor for pre-synaptic spike contributions.\n        :param rho_0: Stochastic intensity at threshold.\n        :param d_thresh: Width of the threshold region.\n        """"""\n        super().__init__(\n            n=n,\n            shape=shape,\n            traces=traces,\n            traces_additive=traces_additive,\n            tc_trace=tc_trace,\n            trace_scale=trace_scale,\n            sum_input=sum_input,\n        )\n\n        self.register_buffer(""rest"", torch.tensor(rest))  # Rest voltage.\n        self.register_buffer(""reset"", torch.tensor(reset))  # Post-spike reset voltage.\n        self.register_buffer(""thresh"", torch.tensor(thresh))  # Spike threshold voltage.\n        self.register_buffer(\n            ""refrac"", torch.tensor(refrac)\n        )  # Post-spike refractory period.\n        self.register_buffer(\n            ""tc_decay"", torch.tensor(tc_decay)\n        )  # Time constant of neuron voltage decay.\n        self.register_buffer(""decay"", torch.tensor(tc_decay))  # Set in compute_decays.\n        self.register_buffer(\n            ""eps_0"", torch.tensor(eps_0)\n        )  # Scaling factor for pre-synaptic spike contributions.\n        self.register_buffer(\n            ""rho_0"", torch.tensor(rho_0)\n        )  # Stochastic intensity at threshold.\n        self.register_buffer(\n            ""d_thresh"", torch.tensor(d_thresh)\n        )  # Width of the threshold region.\n        self.register_buffer(""v"", torch.FloatTensor())  # Neuron voltages.\n        self.register_buffer(\n            ""refrac_count"", torch.FloatTensor()\n        )  # Refractory period counters.\n\n        self.lbound = lbound  # Lower bound of voltage.\n\n    def forward(self, x: torch.Tensor) -> None:\n        # language=rst\n        """"""\n        Runs a single simulation step.\n\n        :param x: Inputs to the layer.\n        """"""\n        # Decay voltages.\n        self.v = self.decay * (self.v - self.rest) + self.rest\n\n        # Integrate inputs.\n        self.v += (self.refrac_count == 0).float() * self.eps_0 * x\n\n        # Compute (instantaneous) probabilities of spiking, clamp between 0 and 1 using exponentials.\n        # Also known as \'escape noise\', this simulates nearby neurons.\n        self.rho = self.rho_0 * torch.exp((self.v - self.thresh) / self.d_thresh)\n        self.s_prob = 1.0 - torch.exp(-self.rho * self.dt)\n\n        # Decrement refractory counters.\n        self.refrac_count = (self.refrac_count > 0).float() * (\n            self.refrac_count - self.dt\n        )\n\n        # Check for spiking neurons (spike when probability > some random number).\n        self.s = torch.rand_like(self.s_prob) < self.s_prob\n\n        # Refractoriness and voltage reset.\n        self.refrac_count.masked_fill_(self.s, self.refrac)\n        self.v.masked_fill_(self.s, self.reset)\n\n        # Voltage clipping to lower bound.\n        if self.lbound is not None:\n            self.v.masked_fill_(self.v < self.lbound, self.lbound)\n\n        super().forward(x)\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Resets relevant state variables.\n        """"""\n        super().reset_state_variables()\n        self.v.fill_(self.rest)  # Neuron voltages.\n        self.refrac_count.zero_()  # Refractory period counters.\n\n    def compute_decays(self, dt) -> None:\n        # language=rst\n        """"""\n        Sets the relevant decays.\n        """"""\n        super().compute_decays(dt=dt)\n        self.decay = torch.exp(\n            -self.dt / self.tc_decay\n        )  # Neuron voltage decay (per timestep).\n\n    def set_batch_size(self, batch_size) -> None:\n        # language=rst\n        """"""\n        Sets mini-batch size. Called when layer is added to a network.\n\n        :param batch_size: Mini-batch size.\n        """"""\n        super().set_batch_size(batch_size=batch_size)\n        self.v = self.rest * torch.ones(batch_size, *self.shape, device=self.v.device)\n        self.refrac_count = torch.zeros_like(self.v, device=self.refrac_count.device)\n'"
bindsnet/network/topology.py,45,"b'from abc import ABC, abstractmethod\nfrom typing import Union, Tuple, Optional, Sequence\n\nimport numpy as np\nimport torch\nfrom torch.nn import Module, Parameter\nimport torch.nn.functional as F\nfrom torch.nn.modules.utils import _pair\n\nfrom .nodes import Nodes\n\n\nclass AbstractConnection(ABC, Module):\n    # language=rst\n    """"""\n    Abstract base method for connections between ``Nodes``.\n    """"""\n\n    def __init__(\n        self,\n        source: Nodes,\n        target: Nodes,\n        nu: Optional[Union[float, Sequence[float]]] = None,\n        reduction: Optional[callable] = None,\n        weight_decay: float = 0.0,\n        **kwargs\n    ) -> None:\n        # language=rst\n        """"""\n        Constructor for abstract base class for connection objects.\n\n        :param source: A layer of nodes from which the connection originates.\n        :param target: A layer of nodes to which the connection connects.\n        :param nu: Learning rate for both pre- and post-synaptic events.\n        :param reduction: Method for reducing parameter updates along the minibatch\n            dimension.\n        :param weight_decay: Constant multiple to decay weights by on each iteration.\n\n        Keyword arguments:\n\n        :param LearningRule update_rule: Modifies connection parameters according to\n            some rule.\n        :param float wmin: The minimum value on the connection weights.\n        :param float wmax: The maximum value on the connection weights.\n        :param float norm: Total weight per target neuron normalization.\n        """"""\n        super().__init__()\n\n        assert isinstance(source, Nodes), ""Source is not a Nodes object""\n        assert isinstance(target, Nodes), ""Target is not a Nodes object""\n\n        self.source = source\n        self.target = target\n\n        self.nu = nu\n        self.weight_decay = weight_decay\n        self.reduction = reduction\n\n        from ..learning import NoOp\n\n        self.update_rule = kwargs.get(""update_rule"", NoOp)\n        self.wmin = kwargs.get(""wmin"", -np.inf)\n        self.wmax = kwargs.get(""wmax"", np.inf)\n        self.norm = kwargs.get(""norm"", None)\n        self.decay = kwargs.get(""decay"", None)\n\n        if self.update_rule is None:\n            self.update_rule = NoOp\n\n        self.update_rule = self.update_rule(\n            connection=self,\n            nu=nu,\n            reduction=reduction,\n            weight_decay=weight_decay,\n            **kwargs\n        )\n\n    @abstractmethod\n    def compute(self, s: torch.Tensor) -> None:\n        # language=rst\n        """"""\n        Compute pre-activations of downstream neurons given spikes of upstream neurons.\n\n        :param s: Incoming spikes.\n        """"""\n        pass\n\n    @abstractmethod\n    def update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Compute connection\'s update rule.\n\n        Keyword arguments:\n\n        :param bool learning: Whether to allow connection updates.\n        :param ByteTensor mask: Boolean mask determining which weights to clamp to zero.\n        """"""\n        learning = kwargs.get(""learning"", True)\n\n        if learning:\n            self.update_rule.update(**kwargs)\n\n        mask = kwargs.get(""mask"", None)\n        if mask is not None:\n            self.w.masked_fill_(mask, 0)\n\n    @abstractmethod\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Contains resetting logic for the connection.\n        """"""\n        pass\n\n\nclass Connection(AbstractConnection):\n    # language=rst\n    """"""\n    Specifies synapses between one or two populations of neurons.\n    """"""\n\n    def __init__(\n        self,\n        source: Nodes,\n        target: Nodes,\n        nu: Optional[Union[float, Sequence[float]]] = None,\n        reduction: Optional[callable] = None,\n        weight_decay: float = 0.0,\n        **kwargs\n    ) -> None:\n        # language=rst\n        """"""\n        Instantiates a :code:`Connection` object.\n\n        :param source: A layer of nodes from which the connection originates.\n        :param target: A layer of nodes to which the connection connects.\n        :param nu: Learning rate for both pre- and post-synaptic events.\n        :param reduction: Method for reducing parameter updates along the minibatch\n            dimension.\n        :param weight_decay: Constant multiple to decay weights by on each iteration.\n\n        Keyword arguments:\n\n        :param LearningRule update_rule: Modifies connection parameters according to\n            some rule.\n        :param torch.Tensor w: Strengths of synapses.\n        :param torch.Tensor b: Target population bias.\n        :param float wmin: Minimum allowed value on the connection weights.\n        :param float wmax: Maximum allowed value on the connection weights.\n        :param float norm: Total weight per target neuron normalization constant.\n        """"""\n        super().__init__(source, target, nu, reduction, weight_decay, **kwargs)\n\n        w = kwargs.get(""w"", None)\n        if w is None:\n            if self.wmin == -np.inf or self.wmax == np.inf:\n                w = torch.clamp(torch.rand(source.n, target.n), self.wmin, self.wmax)\n            else:\n                w = self.wmin + torch.rand(source.n, target.n) * (self.wmax - self.wmin)\n        else:\n            if self.wmin != -np.inf or self.wmax != np.inf:\n                w = torch.clamp(w, self.wmin, self.wmax)\n\n        self.w = Parameter(w, requires_grad=False)\n        self.b = Parameter(kwargs.get(""b"", torch.zeros(target.n)), requires_grad=False)\n\n    def compute(self, s: torch.Tensor) -> torch.Tensor:\n        # language=rst\n        """"""\n        Compute pre-activations given spikes using connection weights.\n\n        :param s: Incoming spikes.\n        :return: Incoming spikes multiplied by synaptic weights (with or without\n                 decaying spike activation).\n        """"""\n        # Compute multiplication of spike activations by weights and add bias.\n        post = s.float().view(s.size(0), -1) @ self.w + self.b\n        return post.view(s.size(0), *self.target.shape)\n\n    def update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Compute connection\'s update rule.\n        """"""\n        super().update(**kwargs)\n\n    def normalize(self) -> None:\n        # language=rst\n        """"""\n        Normalize weights so each target neuron has sum of connection weights equal to\n        ``self.norm``.\n        """"""\n        if self.norm is not None:\n            w_abs_sum = self.w.abs().sum(0).unsqueeze(0)\n            w_abs_sum[w_abs_sum == 0] = 1.0\n            self.w *= self.norm / w_abs_sum\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Contains resetting logic for the connection.\n        """"""\n        super().reset_state_variables()\n\n\nclass Conv2dConnection(AbstractConnection):\n    # language=rst\n    """"""\n    Specifies convolutional synapses between one or two populations of neurons.\n    """"""\n\n    def __init__(\n        self,\n        source: Nodes,\n        target: Nodes,\n        kernel_size: Union[int, Tuple[int, int]],\n        stride: Union[int, Tuple[int, int]] = 1,\n        padding: Union[int, Tuple[int, int]] = 0,\n        dilation: Union[int, Tuple[int, int]] = 1,\n        nu: Optional[Union[float, Sequence[float]]] = None,\n        reduction: Optional[callable] = None,\n        weight_decay: float = 0.0,\n        **kwargs\n    ) -> None:\n        # language=rst\n        """"""\n        Instantiates a ``Conv2dConnection`` object.\n\n        :param source: A layer of nodes from which the connection originates.\n        :param target: A layer of nodes to which the connection connects.\n        :param kernel_size: Horizontal and vertical size of convolutional kernels.\n        :param stride: Horizontal and vertical stride for convolution.\n        :param padding: Horizontal and vertical padding for convolution.\n        :param dilation: Horizontal and vertical dilation for convolution.\n        :param nu: Learning rate for both pre- and post-synaptic events.\n        :param reduction: Method for reducing parameter updates along the minibatch\n            dimension.\n        :param weight_decay: Constant multiple to decay weights by on each iteration.\n\n        Keyword arguments:\n\n        :param LearningRule update_rule: Modifies connection parameters according to\n            some rule.\n        :param torch.Tensor w: Strengths of synapses.\n        :param torch.Tensor b: Target population bias.\n        :param float wmin: Minimum allowed value on the connection weights.\n        :param float wmax: Maximum allowed value on the connection weights.\n        :param float norm: Total weight per target neuron normalization constant.\n        """"""\n        super().__init__(source, target, nu, reduction, weight_decay, **kwargs)\n\n        self.kernel_size = _pair(kernel_size)\n        self.stride = _pair(stride)\n        self.padding = _pair(padding)\n        self.dilation = _pair(dilation)\n\n        self.in_channels, input_height, input_width = (\n            source.shape[0],\n            source.shape[1],\n            source.shape[2],\n        )\n        self.out_channels, output_height, output_width = (\n            target.shape[0],\n            target.shape[1],\n            target.shape[2],\n        )\n\n        width = (\n            input_height - self.kernel_size[0] + 2 * self.padding[0]\n        ) / self.stride[0] + 1\n        height = (\n            input_width - self.kernel_size[1] + 2 * self.padding[1]\n        ) / self.stride[1] + 1\n        shape = (self.in_channels, self.out_channels, int(width), int(height))\n\n        error = (\n            ""Target dimensionality must be (out_channels, ?,""\n            ""(input_height - filter_height + 2 * padding_height) / stride_height + 1,""\n            ""(input_width - filter_width + 2 * padding_width) / stride_width + 1""\n        )\n\n        assert (\n            target.shape[0] == shape[1]\n            and target.shape[1] == shape[2]\n            and target.shape[2] == shape[3]\n        ), error\n\n        w = kwargs.get(""w"", None)\n        if w is None:\n            if self.wmin == -np.inf or self.wmax == np.inf:\n                w = torch.clamp(\n                    torch.rand(self.out_channels, self.in_channels, *self.kernel_size),\n                    self.wmin,\n                    self.wmax,\n                )\n            else:\n                w = (self.wmax - self.wmin) * torch.rand(\n                    self.out_channels, self.in_channels, *self.kernel_size\n                )\n                w += self.wmin\n        else:\n            if self.wmin != -np.inf or self.wmax != np.inf:\n                w = torch.clamp(w, self.wmin, self.wmax)\n\n        self.w = Parameter(w, requires_grad=False)\n        self.b = Parameter(\n            kwargs.get(""b"", torch.zeros(self.out_channels)), requires_grad=False\n        )\n\n    def compute(self, s: torch.Tensor) -> torch.Tensor:\n        # language=rst\n        """"""\n        Compute convolutional pre-activations given spikes using layer weights.\n\n        :param s: Incoming spikes.\n        :return: Incoming spikes multiplied by synaptic weights (with or without\n            decaying spike activation).\n        """"""\n        return F.conv2d(\n            s.float(),\n            self.w,\n            self.b,\n            stride=self.stride,\n            padding=self.padding,\n            dilation=self.dilation,\n        )\n\n    def update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Compute connection\'s update rule.\n        """"""\n        super().update(**kwargs)\n\n    def normalize(self) -> None:\n        # language=rst\n        """"""\n        Normalize weights along the first axis according to total weight per target\n        neuron.\n        """"""\n        if self.norm is not None:\n            # get a view and modify in place\n            w = self.w.view(\n                self.w.size(0) * self.w.size(1), self.w.size(2) * self.w.size(3)\n            )\n\n            for fltr in range(w.size(0)):\n                w[fltr] *= self.norm / w[fltr].sum(0)\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Contains resetting logic for the connection.\n        """"""\n        super().reset_state_variables()\n\n\nclass MaxPool2dConnection(AbstractConnection):\n    # language=rst\n    """"""\n    Specifies max-pooling synapses between one or two populations of neurons by keeping\n    online estimates of maximally firing neurons.\n    """"""\n\n    def __init__(\n        self,\n        source: Nodes,\n        target: Nodes,\n        kernel_size: Union[int, Tuple[int, int]],\n        stride: Union[int, Tuple[int, int]] = 1,\n        padding: Union[int, Tuple[int, int]] = 0,\n        dilation: Union[int, Tuple[int, int]] = 1,\n        **kwargs\n    ) -> None:\n        # language=rst\n        """"""\n        Instantiates a ``MaxPool2dConnection`` object.\n\n        :param source: A layer of nodes from which the connection originates.\n        :param target: A layer of nodes to which the connection connects.\n        :param kernel_size: Horizontal and vertical size of convolutional kernels.\n        :param stride: Horizontal and vertical stride for convolution.\n        :param padding: Horizontal and vertical padding for convolution.\n        :param dilation: Horizontal and vertical dilation for convolution.\n\n        Keyword arguments:\n\n        :param decay: Decay rate of online estimates of average firing activity.\n        """"""\n        super().__init__(source, target, None, None, 0.0, **kwargs)\n\n        self.kernel_size = _pair(kernel_size)\n        self.stride = _pair(stride)\n        self.padding = _pair(padding)\n        self.dilation = _pair(dilation)\n\n        self.register_buffer(""firing_rates"", torch.zeros(source.shape))\n\n    def compute(self, s: torch.Tensor) -> torch.Tensor:\n        # language=rst\n        """"""\n        Compute max-pool pre-activations given spikes using online firing rate\n        estimates.\n\n        :param s: Incoming spikes.\n        :return: Incoming spikes multiplied by synaptic weights (with or without\n            decaying spike activation).\n        """"""\n        self.firing_rates -= self.decay * self.firing_rates\n        self.firing_rates += s.float()\n\n        _, indices = F.max_pool2d(\n            self.firing_rates,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n            dilation=self.dilation,\n            return_indices=True,\n        )\n\n        return s.take(indices).float()\n\n    def update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Compute connection\'s update rule.\n        """"""\n        super().update(**kwargs)\n\n    def normalize(self) -> None:\n        # language=rst\n        """"""\n        No weights -> no normalization.\n        """"""\n        pass\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Contains resetting logic for the connection.\n        """"""\n        super().reset_state_variables()\n\n        self.firing_rates = torch.zeros(self.source.shape)\n\n\nclass LocalConnection(AbstractConnection):\n    # language=rst\n    """"""\n    Specifies a locally connected connection between one or two populations of neurons.\n    """"""\n\n    def __init__(\n        self,\n        source: Nodes,\n        target: Nodes,\n        kernel_size: Union[int, Tuple[int, int]],\n        stride: Union[int, Tuple[int, int]],\n        n_filters: int,\n        nu: Optional[Union[float, Sequence[float]]] = None,\n        reduction: Optional[callable] = None,\n        weight_decay: float = 0.0,\n        **kwargs\n    ) -> None:\n        # language=rst\n        """"""\n        Instantiates a ``LocalConnection`` object. Source population should be\n        two-dimensional.\n\n        Neurons in the post-synaptic population are ordered by receptive field; that is,\n        if there are ``n_conv`` neurons in each post-synaptic patch, then the first\n        ``n_conv`` neurons in the post-synaptic population correspond to the first\n        receptive field, the second ``n_conv`` to the second receptive field, and so on.\n\n        :param source: A layer of nodes from which the connection originates.\n        :param target: A layer of nodes to which the connection connects.\n        :param kernel_size: Horizontal and vertical size of convolutional kernels.\n        :param stride: Horizontal and vertical stride for convolution.\n        :param n_filters: Number of locally connected filters per pre-synaptic region.\n        :param nu: Learning rate for both pre- and post-synaptic events.\n        :param reduction: Method for reducing parameter updates along the minibatch\n            dimension.\n        :param weight_decay: Constant multiple to decay weights by on each iteration.\n\n        Keyword arguments:\n\n        :param LearningRule update_rule: Modifies connection parameters according to\n            some rule.\n        :param torch.Tensor w: Strengths of synapses.\n        :param torch.Tensor b: Target population bias.\n        :param float wmin: Minimum allowed value on the connection weights.\n        :param float wmax: Maximum allowed value on the connection weights.\n        :param float norm: Total weight per target neuron normalization constant.\n        :param Tuple[int, int] input_shape: Shape of input population if it\'s not\n            ``[sqrt, sqrt]``.\n        """"""\n        super().__init__(source, target, nu, reduction, weight_decay, **kwargs)\n\n        kernel_size = _pair(kernel_size)\n        stride = _pair(stride)\n\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.n_filters = n_filters\n\n        shape = kwargs.get(""input_shape"", None)\n        if shape is None:\n            sqrt = int(np.sqrt(source.n))\n            shape = _pair(sqrt)\n\n        if kernel_size == shape:\n            conv_size = [1, 1]\n        else:\n            conv_size = (\n                int((shape[0] - kernel_size[0]) / stride[0]) + 1,\n                int((shape[1] - kernel_size[1]) / stride[1]) + 1,\n            )\n\n        self.conv_size = conv_size\n\n        conv_prod = int(np.prod(conv_size))\n        kernel_prod = int(np.prod(kernel_size))\n\n        assert (\n            target.n == n_filters * conv_prod\n        ), ""Target layer size must be n_filters * (kernel_size ** 2).""\n\n        locations = torch.zeros(\n            kernel_size[0], kernel_size[1], conv_size[0], conv_size[1]\n        ).long()\n        for c1 in range(conv_size[0]):\n            for c2 in range(conv_size[1]):\n                for k1 in range(kernel_size[0]):\n                    for k2 in range(kernel_size[1]):\n                        location = (\n                            c1 * stride[0] * shape[1]\n                            + c2 * stride[1]\n                            + k1 * shape[0]\n                            + k2\n                        )\n                        locations[k1, k2, c1, c2] = location\n\n        self.register_buffer(""locations"", locations.view(kernel_prod, conv_prod))\n        w = kwargs.get(""w"", None)\n\n        if w is None:\n            w = torch.zeros(source.n, target.n)\n            for f in range(n_filters):\n                for c in range(conv_prod):\n                    for k in range(kernel_prod):\n                        if self.wmin == -np.inf or self.wmax == np.inf:\n                            w[self.locations[k, c], f * conv_prod + c] = np.clip(\n                                np.random.rand(), self.wmin, self.wmax\n                            )\n                        else:\n                            w[\n                                self.locations[k, c], f * conv_prod + c\n                            ] = self.wmin + np.random.rand() * (self.wmax - self.wmin)\n        else:\n            if self.wmin != -np.inf or self.wmax != np.inf:\n                w = torch.clamp(w, self.wmin, self.wmax)\n\n        self.w = Parameter(w, requires_grad=False)\n\n        self.register_buffer(""mask"", self.w == 0)\n\n        self.b = Parameter(kwargs.get(""b"", torch.zeros(target.n)), requires_grad=False)\n\n        if self.norm is not None:\n            self.norm *= kernel_prod\n\n    def compute(self, s: torch.Tensor) -> torch.Tensor:\n        # language=rst\n        """"""\n        Compute pre-activations given spikes using layer weights.\n\n        :param s: Incoming spikes.\n        :return: Incoming spikes multiplied by synaptic weights (with or without\n            decaying spike activation).\n        """"""\n        # Compute multiplication of pre-activations by connection weights.\n        if self.w.shape[0] == self.source.n and self.w.shape[1] == self.target.n:\n            return s.float().view(s.size(0), -1) @ self.w + self.b\n        else:\n            a_post = (\n                s.float().view(s.size(0), -1)\n                @ self.w.view(self.source.n, self.target.n)\n                + self.b\n            )\n            return a_post.view(*self.target.shape)\n\n    def update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Compute connection\'s update rule.\n\n        Keyword arguments:\n\n        :param ByteTensor mask: Boolean mask determining which weights to clamp to zero.\n        """"""\n        if kwargs[""mask""] is None:\n            kwargs[""mask""] = self.mask\n\n        super().update(**kwargs)\n\n    def normalize(self) -> None:\n        # language=rst\n        """"""\n        Normalize weights so each target neuron has sum of connection weights equal to\n        ``self.norm``.\n        """"""\n        if self.norm is not None:\n            w = self.w.view(self.source.n, self.target.n)\n            w *= self.norm / self.w.sum(0).view(1, -1)\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Contains resetting logic for the connection.\n        """"""\n        super().reset_state_variables()\n\n\nclass MeanFieldConnection(AbstractConnection):\n    # language=rst\n    """"""\n    A connection between one or two populations of neurons which computes a summary of\n    the pre-synaptic population to use as weighted input to the post-synaptic\n    population.\n    """"""\n\n    def __init__(\n        self,\n        source: Nodes,\n        target: Nodes,\n        nu: Optional[Union[float, Sequence[float]]] = None,\n        weight_decay: float = 0.0,\n        **kwargs\n    ) -> None:\n        # language=rst\n        """"""\n        Instantiates a :code:`MeanFieldConnection` object.\n        :param source: A layer of nodes from which the connection originates.\n        :param target: A layer of nodes to which the connection connects.\n        :param nu: Learning rate for both pre- and post-synaptic events.\n        :param weight_decay: Constant multiple to decay weights by on each iteration.\n        Keyword arguments:\n        :param LearningRule update_rule: Modifies connection parameters according to\n            some rule.\n        :param torch.Tensor w: Strengths of synapses.\n        :param float wmin: Minimum allowed value on the connection weights.\n        :param float wmax: Maximum allowed value on the connection weights.\n        :param float norm: Total weight per target neuron normalization constant.\n        """"""\n        super().__init__(source, target, nu, weight_decay, **kwargs)\n\n        w = kwargs.get(""w"", None)\n        if w is None:\n            if self.wmin == -np.inf or self.wmax == np.inf:\n                w = torch.clamp((torch.randn(1)[0] + 1) / 10, self.wmin, self.wmax)\n            else:\n                w = self.wmin + ((torch.randn(1)[0] + 1) / 10) * (self.wmax - self.wmin)\n        else:\n            if self.wmin != -np.inf or self.wmax != np.inf:\n                w = torch.clamp(w, self.wmin, self.wmax)\n\n        self.w = Parameter(w, requires_grad=False)\n\n    def compute(self, s: torch.Tensor) -> torch.Tensor:\n        # language=rst\n        """"""\n        Compute pre-activations given spikes using layer weights.\n        :param s: Incoming spikes.\n        :return: Incoming spikes multiplied by synaptic weights (with or without\n            decaying spike activation).\n        """"""\n        # Compute multiplication of mean-field pre-activation by connection weights.\n        return s.float().mean() * self.w\n\n    def update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Compute connection\'s update rule.\n        """"""\n        super().update(**kwargs)\n\n    def normalize(self) -> None:\n        # language=rst\n        """"""\n        Normalize weights so each target neuron has sum of connection weights equal to\n        ``self.norm``.\n        """"""\n        if self.norm is not None:\n            self.w = self.w.view(1, self.target.n)\n            self.w *= self.norm / self.w.sum()\n            self.w = self.w.view(1, *self.target.shape)\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Contains resetting logic for the connection.\n        """"""\n        super().reset_state_variables()\n\n\nclass SparseConnection(AbstractConnection):\n    # language=rst\n    """"""\n    Specifies sparse synapses between one or two populations of neurons.\n    """"""\n\n    def __init__(\n        self,\n        source: Nodes,\n        target: Nodes,\n        nu: Optional[Union[float, Sequence[float]]] = None,\n        reduction: Optional[callable] = None,\n        weight_decay: float = None,\n        **kwargs\n    ) -> None:\n        # language=rst\n        """"""\n        Instantiates a :code:`Connection` object with sparse weights.\n\n        :param source: A layer of nodes from which the connection originates.\n        :param target: A layer of nodes to which the connection connects.\n        :param nu: Learning rate for both pre- and post-synaptic events.\n        :param reduction: Method for reducing parameter updates along the minibatch\n            dimension.\n        :param weight_decay: Constant multiple to decay weights by on each iteration.\n\n        Keyword arguments:\n\n        :param torch.Tensor w: Strengths of synapses.\n        :param float sparsity: Fraction of sparse connections to use.\n        :param LearningRule update_rule: Modifies connection parameters according to\n            some rule.\n        :param float wmin: Minimum allowed value on the connection weights.\n        :param float wmax: Maximum allowed value on the connection weights.\n        :param float norm: Total weight per target neuron normalization constant.\n        """"""\n        super().__init__(source, target, nu, reduction, weight_decay, **kwargs)\n\n        w = kwargs.get(""w"", None)\n        self.sparsity = kwargs.get(""sparsity"", None)\n\n        assert (\n            w is not None\n            and self.sparsity is None\n            or w is None\n            and self.sparsity is not None\n        ), \'Only one of ""weights"" or ""sparsity"" must be specified\'\n\n        if w is None and self.sparsity is not None:\n            i = torch.bernoulli(\n                1 - self.sparsity * torch.ones(*source.shape, *target.shape)\n            )\n            if self.wmin == -np.inf or self.wmax == np.inf:\n                v = torch.clamp(\n                    torch.rand(*source.shape, *target.shape)[i.byte()],\n                    self.wmin,\n                    self.wmax,\n                )\n            else:\n                v = self.wmin + torch.rand(*source.shape, *target.shape)[i.byte()] * (\n                    self.wmax - self.wmin\n                )\n            w = torch.sparse.FloatTensor(i.nonzero().t(), v)\n        elif w is not None and self.sparsity is None:\n            assert w.is_sparse, ""Weight matrix is not sparse (see torch.sparse module)""\n            if self.wmin != -np.inf or self.wmax != np.inf:\n                w = torch.clamp(w, self.wmin, self.wmax)\n\n        self.w = Parameter(w, requires_grad=False)\n\n    def compute(self, s: torch.Tensor) -> torch.Tensor:\n        # language=rst\n        """"""\n        Compute convolutional pre-activations given spikes using layer weights.\n\n        :param s: Incoming spikes.\n        :return: Incoming spikes multiplied by synaptic weights (with or without\n            decaying spike activation).\n        """"""\n        return torch.mm(self.w, s.unsqueeze(-1).float()).squeeze(-1)\n\n    def update(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Compute connection\'s update rule.\n        """"""\n        pass\n\n    def normalize(self) -> None:\n        # language=rst\n        """"""\n        Normalize weights along the first axis according to total weight per target\n        neuron.\n        """"""\n        pass\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Contains resetting logic for the connection.\n        """"""\n        super().reset_state_variables()\n'"
bindsnet/pipeline/__init__.py,0,"b'from .environment_pipeline import EnvironmentPipeline\nfrom .base_pipeline import BasePipeline\nfrom .dataloader_pipeline import DataLoaderPipeline, TorchVisionDatasetPipeline\nfrom . import action\n'"
bindsnet/pipeline/action.py,13,"b'import torch\nimport numpy as np\n\nfrom . import EnvironmentPipeline\n\n\ndef select_multinomial(pipeline: EnvironmentPipeline, **kwargs) -> int:\n    # language=rst\n    """"""\n    Selects an action probabilistically based on spiking activity from a network layer.\n\n    :param pipeline: EnvironmentPipeline with environment that has an integer action\n        space.\n    :return: Action sampled from multinomial over activity of similarly-sized output\n        layer.\n\n    Keyword arguments:\n\n    :param str output: Name of output layer whose activity to base action selection on.\n    """"""\n    try:\n        output = kwargs[""output""]\n    except KeyError:\n        raise KeyError(\'select_multinomial() requires an ""output"" layer argument.\')\n\n    output = pipeline.network.layers[output]\n    action_space = pipeline.env.action_space\n\n    assert (\n        output.n % action_space.n == 0\n    ), f""Output layer size of {output.n} is not divisible by action space size of {action_space.n}.""\n\n    pop_size = int(output.n / action_space.n)\n    spikes = output.s\n    _sum = spikes.sum().float()\n\n    # Choose action based on population\'s spiking.\n    if _sum == 0:\n        action = torch.randint(low=0, high=pipeline.env.action_space.n, size=(1,))[0]\n    else:\n        pop_spikes = torch.tensor(\n            [\n                spikes[(i * pop_size) : (i * pop_size) + pop_size].sum()\n                for i in range(action_space.n)\n            ]\n        )\n        action = torch.multinomial((pop_spikes.float() / _sum).view(-1), 1)[0].item()\n\n    return action\n\n\ndef select_softmax(pipeline: EnvironmentPipeline, **kwargs) -> int:\n    # language=rst\n    """"""\n    Selects an action using softmax function based on spiking from a network layer.\n\n    :param pipeline: EnvironmentPipeline with environment that has an integer action\n        space and :code:`spike_record` set.\n    :return: Action sampled from softmax over activity of similarly-sized output layer.\n\n    Keyword arguments:\n\n    :param str output: Name of output layer whose activity to base action selection on.\n    """"""\n    try:\n        output = kwargs[""output""]\n    except KeyError:\n        raise KeyError(\'select_softmax() requires an ""output"" layer argument.\')\n\n    assert (\n        pipeline.network.layers[output].n == pipeline.env.action_space.n\n    ), ""Output layer size is not equal to the size of the action space.""\n\n    assert hasattr(\n        pipeline, ""spike_record""\n    ), ""EnvironmentPipeline is missing the attribute: spike_record.""\n\n    spikes = torch.sum(pipeline.spike_record[output], dim=0)\n    probabilities = torch.softmax(spikes, dim=0)\n    return torch.multinomial(probabilities, num_samples=1).item()\n\n\ndef select_highest(pipeline: EnvironmentPipeline, **kwargs) -> int:\n    # language=rst\n    """"""\n    Selects an action with have the highst spikes. In case of equal spiking select randomly\n\n    :param pipeline: EnvironmentPipeline with environment that has an integer action\n        space and :code:`spike_record` set.\n    :return: Action sampled from softmax over activity of similarly-sized output layer.\n\n    Keyword arguments:\n\n    :param str output: Name of output layer whose activity to base action selection on.\n    """"""\n    try:\n        output = kwargs[""output""]\n    except KeyError:\n        raise KeyError(\'select_softmax() requires an ""output"" layer argument.\')\n\n    assert (\n        pipeline.network.layers[output].n == pipeline.env.action_space.n\n    ), ""Output layer size is not equal to the size of the action space.""\n\n    assert hasattr(\n        pipeline, ""spike_record""\n    ), ""EnvironmentPipeline is missing the attribute: spike_record.""\n\n    spikes = torch.sum(pipeline.spike_record[output], dim=0).squeeze()\n    action = torch.where(spikes == spikes.max())[0]\n    if torch.sum(spikes) == 0:\n        # choose random between fire(1) start(0).\n        action[0] = torch.randint(low=0, high=1, size=(1,))[0]\n    # elif action.shape[0] > 1:\n    #     # shuffle the action array and place the random index at the first location.\n    #     p = torch.randint(low=0, high=action.shape[0], size=(1,))[0]\n    #     action[0] = action[p]\n\n    return action[0]\n\n\ndef select_first_spike(pipeline: EnvironmentPipeline, **kwargs) -> int:\n    # language=rst\n    """"""\n    Selects an action with have the highst spikes. In case of equal spiking select randomly\n\n    :param pipeline: EnvironmentPipeline with environment that has an integer action\n        space and :code:`spike_record` set.\n    :return: Action sampled from softmax over activity of similarly-sized output layer.\n\n    Keyword arguments:\n\n    :param str output: Name of output layer whose activity to base action selection on.\n    """"""\n    try:\n        output = kwargs[""output""]\n    except KeyError:\n        raise KeyError(\'select_softmax() requires an ""output"" layer argument.\')\n\n    assert (\n        pipeline.network.layers[output].n == pipeline.env.action_space.n\n    ), ""Output layer size is not equal to the size of the action space.""\n\n    assert hasattr(\n        pipeline, ""spike_record""\n    ), ""EnvironmentPipeline is missing the attribute: spike_record.""\n\n    spikes = pipeline.spike_record[output].squeeze().squeeze().nonzero()\n    if spikes.shape[0] == 0:\n        # choose random between fire(1) start(0).\n        action = torch.randint(low=0, high=1, size=(1,))[0]\n    else:\n        action = spikes[0, 1]\n\n    return action\n\n\ndef select_random(pipeline: EnvironmentPipeline, **kwargs) -> int:\n    # language=rst\n    """"""\n    Selects an action randomly from the action space.\n\n    :param pipeline: EnvironmentPipeline with environment that has an integer action\n        space.\n    :return: Action randomly sampled over size of pipeline\'s action space.\n    """"""\n    # Choose action randomly from the action space.\n    # return np.random.choice(pipeline.env.action_space.n)\n    return torch.randint(low=0, high=pipeline.env.action_space.n, size=(1,))[0]\n'"
bindsnet/pipeline/base_pipeline.py,8,"b'import time\nfrom typing import Tuple, Dict, Any\n\nimport torch\nfrom torch._six import container_abcs, string_classes\n\nfrom ..network import Network\nfrom ..network.monitors import Monitor\n\n\ndef recursive_to(item, device):\n    # language=rst\n    """"""\n    Recursively transfers everything contained in item to the target\n    device.\n\n    :param item: An individual tensor or container of tensors.\n    :param device: ``torch.device`` pointing to ``""cuda""`` or ``""cpu""``.\n\n    :return: A version of the item that has been sent to a device.\n    """"""\n\n    if isinstance(item, torch.Tensor):\n        return item.to(device)\n    elif isinstance(item, (string_classes, int, float, bool)):\n        return item\n    elif isinstance(item, container_abcs.Mapping):\n        return {key: recursive_to(item[key], device) for key in item}\n    elif isinstance(item, tuple) and hasattr(item, ""_fields""):\n        return type(item)(*(recursive_to(i, device) for i in item))\n    elif isinstance(item, container_abcs.Sequence):\n        return [recursive_to(i, device) for i in item]\n    else:\n        raise NotImplementedError(f""Target type {type(item)} not supported."")\n\n\nclass BasePipeline:\n    # language=rst\n    """"""\n    A generic pipeline that handles high level functionality.\n    """"""\n\n    def __init__(self, network: Network, **kwargs) -> None:\n        # language=rst\n        """"""\n        Initializes the pipeline.\n\n        :param network: Arbitrary network object, will be managed by the\n            ``BasePipeline`` class.\n\n        Keyword arguments:\n\n        :param int save_interval: How often to save the network to disk.\n        :param str save_dir: Directory to save network object to.\n        :param Dict[str, Any] plot_config: Dict containing the plot configuration.\n            Includes length, type (``""color""`` or ``""line""``), and interval per plot\n            type.\n        :param int print_interval: Interval to print text output.\n        :param bool allow_gpu: Allows automatic transfer to the GPU.\n        """"""\n        self.network = network\n\n        # Network saving handles caching of intermediate results.\n        self.save_dir = kwargs.get(""save_dir"", ""network.pt"")\n        self.save_interval = kwargs.get(""save_interval"", None)\n\n        # Handles plotting of all layer spikes and voltages.\n        # This constructs monitors at every level.\n        self.plot_config = kwargs.get(\n            ""plot_config"", {""data_step"": True, ""data_length"": 100}\n        )\n\n        if self.plot_config[""data_step""] is not None:\n            for l in self.network.layers:\n                self.network.add_monitor(\n                    Monitor(\n                        self.network.layers[l], ""s"", self.plot_config[""data_length""]\n                    ),\n                    name=f""{l}_spikes"",\n                )\n                if hasattr(self.network.layers[l], ""v""):\n                    self.network.add_monitor(\n                        Monitor(\n                            self.network.layers[l], ""v"", self.plot_config[""data_length""]\n                        ),\n                        name=f""{l}_voltages"",\n                    )\n\n        self.print_interval = kwargs.get(""print_interval"", None)\n        self.test_interval = kwargs.get(""test_interval"", None)\n        self.step_count = 0\n        self.init_fn()\n        self.clock = time.time()\n        self.allow_gpu = kwargs.get(""allow_gpu"", True)\n\n        if torch.cuda.is_available() and self.allow_gpu:\n            self.device = torch.device(""cuda"")\n        else:\n            self.device = torch.device(""cpu"")\n\n        self.network.to(self.device)\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Reset the pipeline.\n        """"""\n        self.network.reset_state_variables()\n        self.step_count = 0\n\n    def step(self, batch: Any, **kwargs) -> Any:\n        # language=rst\n        """"""\n        Single step of any pipeline at a high level.\n\n        :param batch: A batch of inputs to be handed to the ``step_()`` function.\n                      Standard in subclasses of ``BasePipeline``.\n        :return: The output from the subclass\'s ``step_()`` method, which could be\n            anything. Passed to plotting to accommodate this.\n        """"""\n        self.step_count += 1\n\n        batch = recursive_to(batch, self.device)\n        step_out = self.step_(batch, **kwargs)\n\n        if (\n            self.print_interval is not None\n            and self.step_count % self.print_interval == 0\n        ):\n            print(\n                f""Iteration: {self.step_count} (Time: {time.time() - self.clock:.4f})""\n            )\n            self.clock = time.time()\n\n        self.plots(batch, step_out)\n\n        if self.save_interval is not None and self.step_count % self.save_interval == 0:\n            self.network.save(self.save_dir)\n\n        if self.test_interval is not None and self.step_count % self.test_interval == 0:\n            self.test()\n\n        return step_out\n\n    def get_spike_data(self) -> Dict[str, torch.Tensor]:\n        # language=rst\n        """"""\n        Get the spike data from all layers in the pipeline\'s network.\n\n        :return: A dictionary containing all spike monitors from the network.\n        """"""\n        return {\n            l: self.network.monitors[f""{l}_spikes""].get(""s"")\n            for l in self.network.layers\n        }\n\n    def get_voltage_data(\n        self,\n    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:\n        # language=rst\n        """"""\n        Get the voltage data and threshold value from all applicable layers in the\n        pipeline\'s network.\n\n        :return: Two dictionaries containing the voltage data and threshold values from\n            the network.\n        """"""\n        voltage_record = {}\n        threshold_value = {}\n        for l in self.network.layers:\n            if hasattr(self.network.layers[l], ""v""):\n                voltage_record[l] = self.network.monitors[f""{l}_voltages""].get(""v"")\n            if hasattr(self.network.layers[l], ""thresh""):\n                threshold_value[l] = self.network.layers[l].thresh\n\n        return voltage_record, threshold_value\n\n    def step_(self, batch: Any, **kwargs) -> Any:\n        # language=rst\n        """"""\n        Perform a pass of the network given the input batch.\n\n        :param batch: The current batch. This could be anything as long as the subclass\n            agrees upon the format in some way.\n        :return: Any output that is need for recording purposes.\n        """"""\n        raise NotImplementedError(""You need to provide a step_ method."")\n\n    def train(self) -> None:\n        # language=rst\n        """"""\n        A fully self-contained training loop.\n        """"""\n        raise NotImplementedError(""You need to provide a train method."")\n\n    def test(self) -> None:\n        # language=rst\n        """"""\n        A fully self contained test function.\n        """"""\n        raise NotImplementedError(""You need to provide a test method."")\n\n    def init_fn(self) -> None:\n        # language=rst\n        """"""\n        Placeholder function for subclass-specific actions that need to\n        happen during the construction of the ``BasePipeline``.\n        """"""\n        raise NotImplementedError(""You need to provide an init_fn method."")\n\n    def plots(self, batch: Any, step_out: Any) -> None:\n        # language=rst\n        """"""\n        Create any plots and logs for a step given the input batch and step output.\n\n        :param batch: The current batch. This could be anything as long as the subclass\n            agrees upon the format in some way.\n        :param step_out: The output from the ``step_()`` method.\n        """"""\n        raise NotImplementedError(""You need to provide a plots method."")\n'"
bindsnet/pipeline/dataloader_pipeline.py,6,"b'from typing import Optional, Dict\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\n\nfrom ..network import Network\nfrom .base_pipeline import BasePipeline\nfrom ..analysis.pipeline_analysis import PipelineAnalyzer\nfrom ..datasets import DataLoader\n\n\nclass DataLoaderPipeline(BasePipeline):\n    # language=rst\n    """"""\n    A generic ``DataLoader`` pipeline that leverages the ``torch.utils.data`` setup.\n    This still needs to be subclassed for specific implementations for functions given\n    the dataset that will be used. An example can be seen in\n    ``TorchVisionDatasetPipeline``.\n    """"""\n\n    def __init__(\n        self,\n        network: Network,\n        train_ds: Dataset,\n        test_ds: Optional[Dataset] = None,\n        **kwargs\n    ) -> None:\n        # language=rst\n        """"""\n        Initializes the pipeline.\n\n        :param network: Arbitrary ``network`` object.\n        :param train_ds: Arbitrary ``torch.utils.data.Dataset`` object.\n        :param test_ds: Arbitrary ``torch.utils.data.Dataset`` object.\n        """"""\n        super().__init__(network, **kwargs)\n\n        self.train_ds = train_ds\n        self.test_ds = test_ds\n\n        self.num_epochs = kwargs.get(""num_epochs"", 10)\n        self.batch_size = kwargs.get(""batch_size"", 1)\n        self.num_workers = kwargs.get(""num_workers"", 0)\n        self.pin_memory = kwargs.get(""pin_memory"", True)\n        self.shuffle = kwargs.get(""shuffle"", True)\n\n    def train(self) -> None:\n        # language=rst\n        """"""\n        Training loop that runs for the set number of epochs and creates a new\n        ``DataLoader`` at each epoch.\n        """"""\n        for epoch in range(self.num_epochs):\n            train_dataloader = DataLoader(\n                self.train_ds,\n                batch_size=self.batch_size,\n                num_workers=self.num_workers,\n                pin_memory=self.pin_memory,\n                shuffle=self.shuffle,\n            )\n\n            for step, batch in enumerate(\n                tqdm(\n                    train_dataloader,\n                    desc=""Epoch %d/%d"" % (epoch + 1, self.num_epochs),\n                    total=len(self.train_ds) // self.batch_size,\n                )\n            ):\n                self.step(batch)\n\n    def test(self) -> None:\n        raise NotImplementedError(""You need to provide a test function."")\n\n\nclass TorchVisionDatasetPipeline(DataLoaderPipeline):\n    # language=rst\n    """"""\n    An example implementation of ``DataLoaderPipeline`` that runs all of the datasets\n    inside of ``bindsnet.datasets`` that inherit from an instance of a\n    ``torchvision.datasets``. These are documented in ``bindsnet/datasets/README.md``.\n    This specific class just runs an unsupervised network.\n    """"""\n\n    def __init__(\n        self,\n        network: Network,\n        train_ds: Dataset,\n        pipeline_analyzer: Optional[PipelineAnalyzer] = None,\n        **kwargs\n    ) -> None:\n        # language=rst\n        """"""\n        Initializes the pipeline.\n\n        :param network: Arbitrary ``network`` object.\n        :param train_ds: A ``torchvision.datasets`` wrapper dataset from\n            ``bindsnet.datasets``.\n\n        Keyword arguments:\n\n        :param str input_layer: Layer of the network that receives input.\n        """"""\n        super().__init__(network, train_ds, None, **kwargs)\n\n        self.input_layer = kwargs.get(""input_layer"", ""X"")\n        self.pipeline_analyzer = pipeline_analyzer\n\n    def step_(self, batch: Dict[str, torch.Tensor], **kwargs) -> None:\n        # language=rst\n        """"""\n        Perform a pass of the network given the input batch. Unsupervised training\n        (implying everything is stored inside of the ``network`` object, therefore\n        returns ``None``.\n\n        :param batch: A dictionary of the current batch. Includes image, label and\n            encoded versions.\n        """"""\n        self.network.reset_state_variables()\n        inputs = {self.input_layer: batch[""encoded_image""]}\n        self.network.run(inputs, time=batch[""encoded_image""].shape[0])\n\n    def init_fn(self) -> None:\n        pass\n\n    def plots(self, batch: Dict[str, torch.Tensor], *args) -> None:\n        # language=rst\n        """"""\n        Create any plots and logs for a step given the input batch.\n\n        :param batch: A dictionary of the current batch. Includes image, label and\n            encoded versions.\n        """"""\n        if self.pipeline_analyzer is not None:\n            self.pipeline_analyzer.plot_obs(\n                batch[""encoded_image""][0, ...].sum(0), step=self.step_count\n            )\n\n            self.pipeline_analyzer.plot_spikes(\n                self.get_spike_data(), step=self.step_count\n            )\n\n            vr, tv = self.get_voltage_data()\n            self.pipeline_analyzer.plot_voltages(vr, tv, step=self.step_count)\n\n            self.pipeline_analyzer.finalize_step()\n\n    def test_step(self):\n        pass\n'"
bindsnet/pipeline/environment_pipeline.py,20,"b'import itertools\nfrom typing import Callable, Optional, Tuple, Dict\n\nimport torch\n\nfrom tqdm import tqdm\n\nfrom .base_pipeline import BasePipeline\nfrom ..analysis.pipeline_analysis import MatplotlibAnalyzer\nfrom ..environment import Environment\nfrom ..network import Network\nfrom ..network.nodes import AbstractInput\nfrom ..network.monitors import Monitor\n\n\nclass EnvironmentPipeline(BasePipeline):\n    # language=rst\n    """"""\n    Abstracts the interaction between ``Network``, ``Environment``, and environment\n    feedback action.\n    """"""\n\n    def __init__(\n        self,\n        network: Network,\n        environment: Environment,\n        action_function: Optional[Callable] = None,\n        encoding: Optional[Callable] = None,\n        **kwargs,\n    ):\n        # language=rst\n        """"""\n        Initializes the pipeline.\n\n        :param network: Arbitrary network object.\n        :param environment: Arbitrary environment.\n        :param action_function: Function to convert network outputs into environment inputs.\n        :param encoding: Function to encoding input.\n\n        Keyword arguments:\n\n        :param str device: PyTorch computing device\n        :param encode_factor: coefficient for the input before encoding.\n        :param int num_episodes: Number of episodes to train for. Defaults to 100.\n        :param str output: String name of the layer from which to take output.\n        :param int render_interval: Interval to render the environment.\n        :param int reward_delay: How many iterations to delay delivery of reward.\n        :param int time: Time for which to run the network. Defaults to the network\'s\n        :param int overlay_input: Overlay the last X previous input\n        :param float percent_of_random_action: chance to choose random action\n        :param int random_action_after: take random action if same output action counter reach\n\n            timestep.\n        """"""\n        super().__init__(network, **kwargs)\n\n        self.episode = 0\n\n        self.env = environment\n        self.action_function = action_function\n        self.encoding = encoding\n\n        self.accumulated_reward = 0.0\n        self.reward_list = []\n\n        # Setting kwargs.\n        self.num_episodes = kwargs.get(""num_episodes"", 100)\n        self.output = kwargs.get(""output"", None)\n        self.render_interval = kwargs.get(""render_interval"", None)\n        self.plot_interval = kwargs.get(""plot_interval"", None)\n        self.reward_delay = kwargs.get(""reward_delay"", None)\n        self.time = kwargs.get(""time"", int(network.dt))\n        self.overlay_t = kwargs.get(""overlay_input"", 1)\n        self.percent_of_random_action = kwargs.get(""percent_of_random_action"", 0.0)\n        self.device = kwargs.get(""device"", ""cpu"")\n        self.encode_factor = kwargs.get(""encode_factor"", 1.0)\n\n        # var for overlay process\n        if self.overlay_t > 1:\n            self.overlay_time_effect = torch.tensor(\n                [i / self.overlay_t for i in range(1, self.overlay_t + 1)],\n                dtype=torch.float,\n                device=self.device,\n            )\n        self.overlay_start = True\n\n        if self.reward_delay is not None:\n            assert self.reward_delay > 0\n            self.rewards = torch.zeros(self.reward_delay)\n\n        # Set up for multiple layers of input layers.\n        self.inputs = [\n            name\n            for name, layer in network.layers.items()\n            if isinstance(layer, AbstractInput)\n        ]\n\n        self.action = torch.tensor(-1)\n        self.last_action = torch.tensor(-1)\n        self.action_counter = 0\n        self.random_action_after = kwargs.get(""random_action_after"", self.time)\n\n        self.voltage_record = None\n        self.threshold_value = None\n        self.reward_plot = None\n        self.first = True\n\n        self.analyzer = MatplotlibAnalyzer(**self.plot_config)\n\n        if self.output is not None:\n            self.network.add_monitor(\n                Monitor(self.network.layers[self.output], [""s""], time=self.time),\n                self.output,\n            )\n\n            self.spike_record = {\n                self.output: torch.zeros((self.time, self.env.action_space.n)).to(\n                    self.device\n                )\n            }\n\n    def init_fn(self) -> None:\n        pass\n\n    def train(self, **kwargs) -> None:\n        # language=rst\n        """"""\n        Trains for the specified number of episodes. Each episode can be of arbitrary\n        length.\n        """"""\n        while self.episode < self.num_episodes:\n            self.reset_state_variables()\n\n            for _ in itertools.count():\n                obs, reward, done, info = self.env_step()\n\n                self.step((obs, reward, done, info), **kwargs)\n\n                if done:\n                    break\n\n            print(\n                f""Episode: {self.episode} - ""\n                f""accumulated reward: {self.accumulated_reward:.2f}""\n            )\n            self.episode += 1\n\n    def env_step(self) -> Tuple[torch.Tensor, float, bool, Dict]:\n        # language=rst\n        """"""\n        Single step of the environment which includes rendering, getting and performing\n        the action, and accumulating/delaying rewards.\n\n        :return: An OpenAI ``gym`` compatible tuple with modified reward and info.\n        """"""\n        # Render game.\n        if (\n            self.render_interval is not None\n            and self.step_count % self.render_interval == 0\n        ):\n            self.env.render()\n\n        # Choose action based on output neuron spiking.\n        if self.action_function is not None:\n            self.last_action = self.action\n            if torch.rand(1) < self.percent_of_random_action:\n                self.action = torch.randint(\n                    low=0, high=self.spike_record[self.output].shape[-1], size=(1,)\n                )[0]\n            elif self.action_counter > self.random_action_after:\n                if self.last_action == 0:  # last action was start b\n                    self.action = 1  # next action will be fire b\n                    tqdm.write(f""Fire -> too many times {self.last_action} "")\n                else:\n                    self.action = torch.randint(\n                        low=2, high=self.spike_record[self.output].shape[-1], size=(1,)\n                    )[0]\n                    tqdm.write(f""too many times {self.last_action} "")\n            else:\n                self.action = self.action_function(self, output=self.output)\n\n            if self.last_action == self.action:\n                self.action_counter += 1\n            else:\n                self.action_counter = 0\n\n        # Run a step of the environment.\n        obs, reward, done, info = self.env.step(self.action)\n\n        # Set reward in case of delay.\n        if self.reward_delay is not None:\n            self.rewards = torch.tensor([reward, *self.rewards[1:]]).float()\n            reward = self.rewards[-1]\n\n        # Accumulate reward.\n        self.accumulated_reward += reward\n\n        info[""accumulated_reward""] = self.accumulated_reward\n\n        return obs, reward, done, info\n\n    def step_(\n        self, gym_batch: Tuple[torch.Tensor, float, bool, Dict], **kwargs\n    ) -> None:\n        # language=rst\n        """"""\n        Run a single iteration of the network and update it and the reward list when\n        done.\n\n        :param gym_batch: An OpenAI ``gym`` compatible tuple.\n        """"""\n        obs, reward, done, info = gym_batch\n\n        if self.overlay_t > 1:\n            if self.overlay_start:\n                self.overlay_last_obs = (\n                    obs.view(obs.shape[2], obs.shape[3]).clone().to(self.device)\n                )\n                self.overlay_buffer = torch.stack(\n                    [self.overlay_last_obs] * self.overlay_t, dim=2\n                ).to(self.device)\n                self.overlay_start = False\n            else:\n                obs = obs.to(self.device)\n                self.overlay_next_stat = torch.clamp(\n                    self.overlay_last_obs - obs, min=0\n                ).to(self.device)\n                self.overlay_last_obs = obs.clone()\n                self.overlay_buffer = torch.cat(\n                    (\n                        self.overlay_buffer[:, :, 1:],\n                        self.overlay_next_stat.view(\n                            [\n                                self.overlay_next_stat.shape[2],\n                                self.overlay_next_stat.shape[3],\n                                1,\n                            ]\n                        ),\n                    ),\n                    dim=2,\n                )\n            obs = (\n                torch.sum(self.overlay_time_effect * self.overlay_buffer, dim=2)\n                * self.encode_factor\n            )\n\n        # Place the observations into the inputs.\n        if self.encoding is None:\n            obs = obs.unsqueeze(0).unsqueeze(0)\n            obs_shape = torch.tensor([1] * len(obs.shape[1:]), device=self.device)\n            inputs = {\n                k: self.encoding(\n                    obs.repeat(self.time, *obs_shape).to(self.device),\n                    device=self.device,\n                )\n                for k in self.inputs\n            }\n        else:\n            obs = obs.unsqueeze(0)\n            inputs = {\n                k: self.encoding(obs, self.time, device=self.device)\n                for k in self.inputs\n            }\n\n        # Run the network on the spike train-encoded inputs.\n        self.network.run(inputs=inputs, time=self.time, reward=reward, **kwargs)\n\n        if self.output is not None:\n            self.spike_record[self.output] = (\n                self.network.monitors[self.output].get(""s"").float()\n            )\n\n        if done:\n            if self.network.reward_fn is not None:\n                self.network.reward_fn.update(\n                    accumulated_reward=self.accumulated_reward,\n                    steps=self.step_count,\n                    **kwargs,\n                )\n            self.reward_list.append(self.accumulated_reward)\n\n    def reset_state_variables(self) -> None:\n        # language=rst\n        """"""\n        Reset the pipeline.\n        """"""\n        self.env.reset()\n        self.network.reset_state_variables()\n        self.accumulated_reward = 0.0\n        self.step_count = 0\n        self.overlay_start = True\n        self.action = torch.tensor(-1)\n        self.last_action = torch.tensor(-1)\n        self.action_counter = 0\n\n    def plots(self, gym_batch: Tuple[torch.Tensor, float, bool, Dict], *args) -> None:\n        # language=rst\n        """"""\n        Plot the encoded input, layer spikes, and layer voltages.\n\n        :param gym_batch: An OpenAI ``gym`` compatible tuple.\n        """"""\n        if self.plot_interval is None:\n            return\n\n        obs, reward, done, info = gym_batch\n\n        for key, item in self.plot_config.items():\n            if key == ""obs_step"" and item is not None:\n                if self.step_count % item == 0:\n                    self.analyzer.plot_obs(obs[0, ...].sum(0))\n            elif key == ""data_step"" and item is not None:\n                if self.step_count % item == 0:\n                    self.analyzer.plot_spikes(self.get_spike_data())\n                    self.analyzer.plot_voltages(*self.get_voltage_data())\n            elif key == ""reward_eps"" and item is not None:\n                if self.episode % item == 0 and done:\n                    self.analyzer.plot_reward(self.reward_list)\n\n        self.analyzer.finalize_step()\n'"
bindsnet/preprocessing/__init__.py,0,b'from .preprocessing import AbstractPreprocessor\n'
bindsnet/preprocessing/preprocessing.py,1,"b'import hashlib\nimport os\nimport pickle\nimport torch\n\nfrom abc import abstractmethod, ABC\n\n\nclass AbstractPreprocessor(ABC):\n    # language=rst\n    """"""\n    Abstract base class for Preprocessor.\n    """"""\n\n    def process(\n        self,\n        csvfile: str,\n        use_cache: bool = True,\n        cachedfile: str = ""./processed/data.pt"",\n    ) -> torch.tensor:\n        # cache dictionary for storing encodings if previously encoded\n        cache = {""verify"": """", ""data"": None}\n\n        # if the file exists\n        if use_cache:\n            # generate a hash\n            cache[""verify""] = self.__gen_hash(csvfile)\n\n            # compare hash, if valid return cached value\n            if self.__check_file(cachedfile, cache):\n                return cache[""data""]\n\n        # otherwise process the data\n        self._process(csvfile, cache)\n\n        # save if use_cache\n        if use_cache:\n            self.__save(cachedfile, cache)\n\n        # return data\n        return cache[""data""]\n\n    @abstractmethod\n    def _process(self, filename: str, cache: dict):\n        # language=rst\n        """"""\n        Method for defining how to preprocess the data.\n\n        :param filename: File to load raw data from.\n        :param cache: Dictionary for caching \'data\' needs to be updated for caching to\n            work.\n        """"""\n        pass\n\n    def __gen_hash(self, filename: str) -> str:\n        # language=rst\n        """"""\n        Generates an hash for a csv file and the preprocessor name.\n\n        :param filename: File to generate hash for.\n        :return: Hash for the csv file.\n        """"""\n        # read all the lines\n        with open(filename, ""r"") as f:\n            lines = f.readlines()\n\n        # generate md5 hash after concatenating all of the lines\n        pre = """".join(lines) + str(self.__class__.__name__)\n        m = hashlib.md5(pre.encode(""utf-8""))\n        return m.hexdigest()\n\n    @staticmethod\n    def __check_file(cachedfile: str, cache: dict) -> bool:\n        # language=rst\n        """"""\n        Compares the csv file and the saved file to see if a new encoding needs to be\n        generated.\n\n        :param cachedfile: The filename of the cached data.\n        :param cache: Dictionary containing the current csv file hash. This is updated\n            if the cache file has valid data.\n        :return: Whether the cache is valid.\n        """"""\n        # try opening the cached file\n        try:\n            with open(cachedfile, ""rb"") as f:\n                temp = pickle.load(f)\n        except FileNotFoundError:\n            temp = {""verify"": """", ""data"": None}\n\n        # if the hash matches up, keep the data from the cache\n        if cache[""verify""] == temp[""verify""]:\n            cache[""data""] = temp[""data""]\n            return True\n\n        # otherwise don\'t do anything\n        return False\n\n    @staticmethod\n    def __save(filename: str, data: dict) -> None:\n        # language=rst\n        """"""\n        Creates or overwrites existing encoding file.\n\n        :param filename: Filename to save to.\n        """"""\n        # if the directories in path don\'t exist create them\n        if not os.path.exists(os.path.dirname(filename)):\n            os.makedirs(os.path.dirname(filename), exist_ok=True)\n\n        # save file\n        with open(filename, ""wb"") as f:\n            pickle.dump(data, f)\n'"
docs/source/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# bindsnet documentation build configuration file, created by\n# sphinx-quickstart on Tue May  1 21:54:09 2018.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(""../..""))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    ""sphinx.ext.autodoc"",\n    ""sphinx.ext.doctest"",\n    ""sphinx.ext.coverage"",\n    ""sphinx.ext.mathjax"",\n    ""sphinx.ext.viewcode"",\n    ""sphinx.ext.githubpages"",\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [""_templates""]\n\n# autodoc of module special functions\nautoclass_content = ""both""\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = "".rst""\n\n# The master toctree document.\nmaster_doc = ""index""\n\n# General information about the project.\nproject = ""bindsnet""\ncopyright = ""2019, Daniel Saunders, Hananel Hazan""\nauthor = ""Daniel Saunders, Hananel Hazan""\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = ""0.2.5""\n# The full version, including alpha/beta/rc tags.\nrelease = ""0.2.5""\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = ""sphinx""\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [""_static""]\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# This is required for the alabaster theme\n# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars\nhtml_sidebars = {\n    ""**"": [\n        ""about.html"",\n        ""navigation.html"",\n        ""relations.html"",  # needs \'show_related\': True theme option to display\n        ""searchbox.html"",\n        ""donate.html"",\n    ]\n}\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = ""bindsnetdoc""\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (\n        master_doc,\n        ""bindsnet.tex"",\n        ""bindsnet Documentation"",\n        ""Daniel Saunders, Hananel Hazan"",\n        ""manual"",\n    )\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, ""bindsnet"", ""bindsnet Documentation"", [author], 1)]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        master_doc,\n        ""bindsnet"",\n        ""bindsnet Documentation"",\n        author,\n        ""bindsnet"",\n        ""One line description of project."",\n        ""Miscellaneous"",\n    )\n]\n'"
examples/benchmark/annarchy.py,0,"b'from __future__ import print_function\n\nimport os\nimport argparse\nimport ANNarchy\nimport numpy as np\nimport pandas as pd\n\nfrom time import time as t\n\nplots_path = os.path.join("".."", "".."", ""figures"")\nbenchmark_path = os.path.join("".."", "".."", ""benchmark"")\nif not os.path.isdir(benchmark_path):\n    os.makedirs(benchmark_path)\n\n\ndef ANNarchy_cpu(n_neurons, time):\n    ANNarchy.setup(paradigm=""openmp"", dt=1.0)\n    ANNarchy.clear()\n\n    t1 = t()\n\n    IF = ANNarchy.Neuron(\n        parameters=""""""\n            tau_m = 10.0\n            tau_e = 5.0\n            vt = -54.0\n            vr = -60.0\n            El = -74.0\n            Ee = 0.0\n        """""",\n        equations=""""""\n            tau_m * dv/dt = El - v + g_exc *  (Ee - vr) : init = -60.0\n            tau_e * dg_exc/dt = - g_exc\n        """""",\n        spike=""""""\n            v > vt\n        """""",\n        reset=""""""\n            v = vr\n        """""",\n    )\n\n    Input = ANNarchy.PoissonPopulation(name=""Input"", geometry=n_neurons, rates=50.0)\n    Output = ANNarchy.Population(name=""Output"", geometry=n_neurons, neuron=IF)\n    proj = ANNarchy.Projection(pre=Input, post=Output, target=""exc"", synapse=None)\n    proj.connect_all_to_all(weights=ANNarchy.Uniform(0.0, 1.0))\n\n    ANNarchy.compile()\n    ANNarchy.simulate(duration=time)\n\n    return t() - t1\n\n\ndef ANNarchy_gpu(n_neurons, time):\n    ANNarchy.setup(paradigm=""cuda"", dt=1.0)\n    ANNarchy.clear()\n\n    t1 = t()\n\n    IF = ANNarchy.Neuron(\n        parameters=""""""\n            tau_m = 10.0\n            tau_e = 5.0\n            vt = -54.0\n            vr = -60.0\n            El = -74.0\n            Ee = 0.0\n        """""",\n        equations=""""""\n            tau_m * dv/dt = El - v + g_exc *  (Ee - vr) : init = -60.0\n            tau_e * dg_exc/dt = - g_exc\n        """""",\n        spike=""""""\n            v > vt\n        """""",\n        reset=""""""\n            v = vr\n        """""",\n    )\n\n    Input = ANNarchy.PoissonPopulation(name=""Input"", geometry=n_neurons, rates=50.0)\n    Output = ANNarchy.Population(name=""Output"", geometry=n_neurons, neuron=IF)\n    proj = ANNarchy.Projection(pre=Input, post=Output, target=""exc"", synapse=None)\n    proj.connect_all_to_all(weights=ANNarchy.Uniform(0.0, 1.0))\n\n    ANNarchy.compile()\n    ANNarchy.simulate(duration=time)\n\n    return t() - t1\n\n\ndef main(start=100, stop=1000, step=100, time=1000, interval=100, plot=False):\n    times = {""ANNarchy_cpu"": []}\n\n    f = os.path.join(\n        benchmark_path, ""benchmark_{start}_{stop}_{step}_{time}.csv"".format(**locals())\n    )\n    if not os.path.isfile(f):\n        raise Exception(""{0} not found."".format(f))\n\n    for n_neurons in range(start, stop + step, step):\n        print(""\\nRunning benchmark with {0} neurons."".format(n_neurons))\n        for framework in times.keys():\n            if framework == ""ANNarchy_cpu"" and n_neurons > 5000:\n                times[framework].append(np.nan)\n                continue\n\n            print(""- {0}:"".format(framework), end="" "")\n\n            fn = globals()[framework]\n            elapsed = fn(n_neurons=n_neurons, time=time)\n            times[framework].append(elapsed)\n\n            print(""(elapsed: {0:.4f})"".format(elapsed))\n\n    df = pd.read_csv(f, index_col=0)\n\n    for framework in times.keys():\n        print(pd.Series(times[framework]))\n        df[framework] = times[framework]\n\n    print()\n    print(df)\n    print()\n\n    df.to_csv(f)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--start"", type=int, default=100)\n    parser.add_argument(""--stop"", type=int, default=1000)\n    parser.add_argument(""--step"", type=int, default=100)\n    parser.add_argument(""--time"", type=int, default=1000)\n    parser.add_argument(""--interval"", type=int, default=100)\n    parser.add_argument(""--plot"", dest=""plot"", action=""store_true"")\n    parser.set_defaults(plot=False)\n    args = parser.parse_args()\n\n    print(args)\n\n    main(\n        start=args.start,\n        stop=args.stop,\n        step=args.step,\n        time=args.time,\n        interval=args.interval,\n        plot=args.plot,\n    )\n'"
examples/benchmark/benchmark.py,7,"b'import os\nimport nengo\nimport torch\nimport argparse\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom brian2 import *\nfrom nest import *\nfrom time import time as t\nfrom experiments import ROOT_DIR\n\nimport brian2genn\n\nfrom bindsnet.network import Network\nfrom bindsnet.network.topology import Connection\nfrom bindsnet.network.nodes import Input, LIFNodes\nfrom bindsnet.encoding import poisson\n\nfrom experiments.benchmark import plot_benchmark\n\nplots_path = os.path.join(ROOT_DIR, ""figures"")\nbenchmark_path = os.path.join(ROOT_DIR, ""benchmark"")\nif not os.path.isdir(benchmark_path):\n    os.makedirs(benchmark_path)\n\n# ""Warm up"" the GPU.\ntorch.set_default_tensor_type(""torch.cuda.FloatTensor"")\nx = torch.rand(1000)\ndel x\n\n# BRIAN2 clock\ndefaultclock = 1.0 * ms\n\n\ndef BindsNET_cpu(n_neurons, time):\n    t0 = t()\n\n    torch.set_default_tensor_type(""torch.FloatTensor"")\n\n    t1 = t()\n\n    network = Network()\n    network.add_layer(Input(n=n_neurons), name=""X"")\n    network.add_layer(LIFNodes(n=n_neurons), name=""Y"")\n    network.add_connection(\n        Connection(source=network.layers[""X""], target=network.layers[""Y""]),\n        source=""X"",\n        target=""Y"",\n    )\n\n    data = {""X"": poisson(datum=torch.rand(n_neurons), time=time)}\n    network.run(inputs=data, time=time)\n\n    return t() - t0, t() - t1\n\n\ndef BindsNET_gpu(n_neurons, time):\n    if torch.cuda.is_available():\n        t0 = t()\n\n        torch.set_default_tensor_type(""torch.cuda.FloatTensor"")\n\n        t1 = t()\n\n        network = Network()\n        network.add_layer(Input(n=n_neurons), name=""X"")\n        network.add_layer(LIFNodes(n=n_neurons), name=""Y"")\n        network.add_connection(\n            Connection(source=network.layers[""X""], target=network.layers[""Y""]),\n            source=""X"",\n            target=""Y"",\n        )\n\n        data = {""X"": poisson(datum=torch.rand(n_neurons), time=time)}\n        network.run(inputs=data, time=time)\n\n        return t() - t0, t() - t1\n\n\ndef BRIAN2(n_neurons, time):\n    t0 = t()\n\n    set_device(""runtime"", build_on_run=False)\n    defaultclock = 1.0 * ms\n    device.build()\n\n    t1 = t()\n\n    eqs_neurons = """"""\n        dv/dt = (ge * (-60 * mV) + (-74 * mV) - v) / (10 * ms) : volt\n        dge/dt = -ge / (5 * ms) : 1\n    """"""\n\n    input = PoissonGroup(n_neurons, rates=15 * Hz)\n    neurons = NeuronGroup(\n        n_neurons,\n        eqs_neurons,\n        threshold=""v > (-54 * mV)"",\n        reset=""v = -60 * mV"",\n        method=""exact"",\n    )\n    S = Synapses(input, neurons, """"""w: 1"""""")\n    S.connect()\n    S.w = ""rand() * 0.01""\n\n    run(time * ms)\n\n    return t() - t0, t() - t1\n\n\ndef BRIAN2GENN(n_neurons, time):\n    t0 = t()\n\n    set_device(""genn"", build_on_run=False)\n    defaultclock = 1.0 * ms\n    device.build()\n\n    t1 = t()\n\n    eqs_neurons = """"""\n        dv/dt = (ge * (-60 * mV) + (-74 * mV) - v) / (10 * ms) : volt\n        dge/dt = -ge / (5 * ms) : 1\n    """"""\n\n    input = PoissonGroup(n_neurons, rates=15 * Hz)\n    neurons = NeuronGroup(\n        n_neurons,\n        eqs_neurons,\n        threshold=""v > (-54 * mV)"",\n        reset=""v = -60 * mV"",\n        method=""exact"",\n    )\n    S = Synapses(input, neurons, """"""w: 1"""""")\n    S.connect()\n    S.w = ""rand() * 0.01""\n\n    run(time * ms)\n\n    device.reinit()\n    device.activate()\n\n    return t() - t0, t() - t1\n\n\ndef PyNEST(n_neurons, time):\n    t0 = t()\n\n    ResetKernel()\n    SetKernelStatus({""local_num_threads"": 8, ""resolution"": 1.0})\n\n    t1 = t()\n\n    r_ex = 60.0  # [Hz] rate of exc. neurons\n\n    neuron = Create(""iaf_psc_alpha"", n_neurons)\n    noise = Create(""poisson_generator"", n_neurons)\n\n    SetStatus(noise, [{""rate"": r_ex}])\n    Connect(noise, neuron)\n\n    Simulate(time)\n\n    return t() - t0, t() - t1\n\n\ndef Nengo(n_neurons, time):\n    t0 = t()\n    t1 = t()\n\n    model = nengo.Network()\n    with model:\n        X = nengo.Ensemble(n_neurons, dimensions=1, neuron_type=nengo.LIF())\n        Y = nengo.Ensemble(n_neurons, dimensions=2, neuron_type=nengo.LIF())\n        nengo.Connection(X, Y, transform=np.random.rand(n_neurons, n_neurons))\n\n    with nengo.Simulator(model) as sim:\n        sim.run(time / 1000)\n\n    return t() - t0, t() - t1\n\n\ndef main(start=100, stop=1000, step=100, time=1000, interval=100, plot=False):\n    f = os.path.join(benchmark_path, f""benchmark_{start}_{stop}_{step}_{time}.csv"")\n    if os.path.isfile(f):\n        os.remove(f)\n\n    times = {\n        ""BindsNET_cpu"": [],\n        ""BindsNET_gpu"": [],\n        ""BRIAN2"": [],\n        ""BRIAN2GENN"": [],\n        ""BRIAN2GENN comp."": [],\n        ""PyNEST"": [],  # , \'Nengo\': []\n    }\n\n    for n_neurons in range(start, stop + step, step):\n        print(f""\\nRunning benchmark with {n_neurons} neurons."")\n        for framework in times.keys():\n            if n_neurons > 2500 and framework == ""PyNEST"":\n                times[framework].append(np.nan)\n                continue\n\n            if framework == ""BRIAN2GENN comp."":\n                continue\n\n            print(f""- {framework}:"", end="" "")\n\n            fn = globals()[framework]\n            total, sim = fn(n_neurons=n_neurons, time=time)\n            times[framework].append(sim)\n\n            if framework == ""BRIAN2GENN"":\n                times[""BRIAN2GENN comp.""].append(total - sim)\n\n            print(f""(total: {total:.4f}; sim: {sim:.4f})"")\n\n    print(times)\n\n    df = pd.DataFrame.from_dict(times)\n    df.index = list(range(start, stop + step, step))\n\n    print()\n    print(df)\n    print()\n\n    df.to_csv(f)\n\n    plot_benchmark.main(\n        start=start, stop=stop, step=step, time=time, interval=interval, plot=plot\n    )\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--start"", type=int, default=100)\n    parser.add_argument(""--stop"", type=int, default=1000)\n    parser.add_argument(""--step"", type=int, default=100)\n    parser.add_argument(""--time"", type=int, default=1000)\n    parser.add_argument(""--interval"", type=int, default=100)\n    parser.add_argument(""--plot"", dest=""plot"", action=""store_true"")\n    parser.set_defaults(plot=False)\n    args = parser.parse_args()\n\n    main(\n        start=args.start,\n        stop=args.stop,\n        step=args.step,\n        time=args.time,\n        interval=args.interval,\n        plot=args.plot,\n    )\n'"
examples/benchmark/gpu_annarchy.py,0,"b'from __future__ import print_function\n\nimport os\nimport argparse\nimport ANNarchy\nimport numpy as np\nimport pandas as pd\n\nfrom time import time as t\n\nplots_path = os.path.join("".."", "".."", ""figures"")\nbenchmark_path = os.path.join("".."", "".."", ""benchmark"")\nif not os.path.isdir(benchmark_path):\n    os.makedirs(benchmark_path)\n\n\ndef ANNarchy_gpu(n_neurons, time):\n    t0 = t()\n\n    ANNarchy.setup(paradigm=""cuda"", dt=1.0)\n    ANNarchy.clear()\n\n    IF = ANNarchy.Neuron(\n        parameters=""""""\n            tau_m = 10.0\n            tau_e = 5.0\n            vt = -54.0\n            vr = -60.0\n            El = -74.0\n            Ee = 0.0\n        """""",\n        equations=""""""\n            tau_m * dv/dt = El - v + g_exc *  (Ee - vr) : init = -60.0\n            tau_e * dg_exc/dt = - g_exc\n        """""",\n        spike=""""""\n            v > vt\n        """""",\n        reset=""""""\n            v = vr\n        """""",\n    )\n\n    Input = ANNarchy.PoissonPopulation(name=""Input"", geometry=n_neurons, rates=50.0)\n    Output = ANNarchy.Population(name=""Output"", geometry=n_neurons, neuron=IF)\n    proj = ANNarchy.Projection(pre=Input, post=Output, target=""exc"", synapse=None)\n    proj.connect_all_to_all(weights=ANNarchy.Uniform(0.0, 1.0))\n\n    ANNarchy.compile()\n\n    t1 = t()\n\n    ANNarchy.simulate(duration=time)\n\n    return t() - t0, t() - t1\n\n\ndef main(start=100, stop=1000, step=100, time=1000, interval=100, plot=False):\n    times = {""ANNarchy_gpu"": [], ""ANNarchy_gpu (w/ comp.)"": []}\n\n    f = os.path.join(\n        benchmark_path, ""benchmark_{start}_{stop}_{step}_{time}.csv"".format(**locals())\n    )\n    if not os.path.isfile(f):\n        raise Exception(""{0} not found."".format(f))\n\n    for n_neurons in range(start, stop + step, step):\n        print(""\\nRunning benchmark with {0} neurons."".format(n_neurons))\n        for framework in times.keys():\n            if ""comp"" in framework:\n                continue\n\n            print(""- {0}:"".format(framework), end="" "")\n\n            fn = globals()[framework]\n            total, sim = fn(n_neurons=n_neurons, time=time)\n            times[framework].append(sim)\n            times[framework + "" (w/ comp.)""].append(total)\n\n            print(""(total, sim: {0:.4f}, {1:.4f})"".format(total, sim))\n\n    df = pd.read_csv(f, index_col=0)\n\n    for framework in times.keys():\n        print(pd.Series(times[framework]))\n        df[framework] = times[framework]\n\n    print()\n    print(df)\n    print()\n\n    df.to_csv(f)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--start"", type=int, default=100)\n    parser.add_argument(""--stop"", type=int, default=1000)\n    parser.add_argument(""--step"", type=int, default=100)\n    parser.add_argument(""--time"", type=int, default=1000)\n    parser.add_argument(""--interval"", type=int, default=100)\n    parser.add_argument(""--plot"", dest=""plot"", action=""store_true"")\n    parser.set_defaults(plot=False)\n    args = parser.parse_args()\n\n    print(args)\n\n    main(\n        start=args.start,\n        stop=args.stop,\n        step=args.step,\n        time=args.time,\n        interval=args.interval,\n        plot=args.plot,\n    )\n'"
examples/benchmark/plot_benchmark.py,0,"b'import os\nimport argparse\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom experiments import ROOT_DIR\n\nbenchmark_path = os.path.join(ROOT_DIR, ""benchmark"")\nfigure_path = os.path.join(ROOT_DIR, ""figures"")\n\nif not os.path.isdir(benchmark_path):\n    os.makedirs(benchmark_path)\n\n\ndef main(start=100, stop=1000, step=100, time=1000, interval=100, plot=False):\n    name = f""benchmark_{start}_{stop}_{step}_{time}""\n    f = os.path.join(benchmark_path, name + "".csv"")\n    df = pd.read_csv(f, index_col=0)\n\n    plt.plot(df[""BindsNET_cpu""], label=""BindsNET (CPU)"", linestyle=""-"", color=""b"")\n    plt.plot(df[""BindsNET_gpu""], label=""BindsNET (GPU)"", linestyle=""-"", color=""g"")\n    plt.plot(df[""BRIAN2""], label=""BRIAN2"", linestyle=""--"", color=""r"")\n    plt.plot(df[""BRIAN2GENN""], label=""brian2genn"", linestyle=""--"", color=""c"")\n    plt.plot(df[""BRIAN2GENN comp.""], label=""brian2genn comp."", linestyle="":"", color=""c"")\n    plt.plot(df[""PyNEST""], label=""PyNEST"", linestyle=""--"", color=""y"")\n    plt.plot(df[""ANNarchy_cpu""], label=""ANNarchy (CPU)"", linestyle=""--"", color=""m"")\n    plt.plot(df[""ANNarchy_gpu""], label=""ANNarchy (GPU)"", linestyle=""--"", color=""k"")\n    plt.plot(\n        df[""ANNarchy_gpu comp.""], label=""ANNarchy (GPU) comp."", linestyle="":"", color=""k""\n    )\n\n    # for c in df.columns:\n    #     if \'BindsNET\' in c:\n    #         plt.plot(df[c], label=c, linestyle=\'-\')\n    #     else:\n    #         plt.plot(df[c], label=c, linestyle=\'--\')\n\n    plt.title(""Benchmark comparison of SNN simulation libraries"")\n    plt.xticks(range(0, stop + interval, interval))\n    plt.xlabel(""Number of input / output neurons"")\n    plt.ylabel(""Simulation time (seconds)"")\n    plt.legend(loc=1, prop={""size"": 5})\n    plt.yscale(""log"")\n\n    plt.savefig(os.path.join(figure_path, name + "".png""))\n\n    if plot:\n        plt.show()\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--start"", type=int, default=100)\n    parser.add_argument(""--stop"", type=int, default=1000)\n    parser.add_argument(""--step"", type=int, default=100)\n    parser.add_argument(""--time"", type=int, default=1000)\n    parser.add_argument(""--interval"", type=int, default=1000)\n    parser.add_argument(""--plot"", dest=""plot"", action=""store_true"")\n    parser.set_defaults(plot=False)\n    args = parser.parse_args()\n\n    main(\n        start=args.start,\n        stop=args.stop,\n        step=args.step,\n        time=args.time,\n        interval=args.interval,\n        plot=args.plot,\n    )\n'"
examples/breakout/breakout.py,0,"b'from bindsnet.network import Network\nfrom bindsnet.pipeline import EnvironmentPipeline\nfrom bindsnet.encoding import bernoulli\nfrom bindsnet.network.topology import Connection\nfrom bindsnet.environment import GymEnvironment\nfrom bindsnet.network.nodes import Input, IzhikevichNodes\nfrom bindsnet.pipeline.action import select_softmax\n\n# Build network.\nnetwork = Network(dt=1.0)\n\n# Layers of neurons.\ninpt = Input(n=80 * 80, shape=[1, 1, 1, 80, 80], traces=True)\nmiddle = IzhikevichNodes(n=100, traces=True)\nout = IzhikevichNodes(n=4, refrac=0, traces=True)\n\n# Connections between layers.\ninpt_middle = Connection(source=inpt, target=middle, wmin=0, wmax=1)\nmiddle_out = Connection(source=middle, target=out, wmin=0, wmax=1)\n\n# Add all layers and connections to the network.\nnetwork.add_layer(inpt, name=""Input Layer"")\nnetwork.add_layer(middle, name=""Hidden Layer"")\nnetwork.add_layer(out, name=""Output Layer"")\nnetwork.add_connection(inpt_middle, source=""Input Layer"", target=""Hidden Layer"")\nnetwork.add_connection(middle_out, source=""Hidden Layer"", target=""Output Layer"")\n\n# Load the Breakout environment.\nenvironment = GymEnvironment(""BreakoutDeterministic-v4"")\nenvironment.reset()\n\n# Build pipeline from specified components.\npipeline = EnvironmentPipeline(\n    network,\n    environment,\n    encoding=bernoulli,\n    action_function=select_softmax,\n    output=""Output Layer"",\n    time=100,\n    history_length=1,\n    delta=1,\n    plot_interval=1,\n    render_interval=1,\n)\n\n# Run environment simulation for 100 episodes.\nfor i in range(100):\n    total_reward = 0\n    pipeline.reset_state_variables()\n    is_done = False\n    while not is_done:\n        result = pipeline.env_step()\n        pipeline.step(result)\n\n        reward = result[1]\n        total_reward += reward\n\n        is_done = result[2]\n    print(f""Episode {i} total reward:{total_reward}"")\n'"
examples/breakout/breakout_stdp.py,0,"b'from bindsnet.network import Network\nfrom bindsnet.pipeline import EnvironmentPipeline\nfrom bindsnet.learning import MSTDP\nfrom bindsnet.encoding import bernoulli\nfrom bindsnet.network.topology import Connection\nfrom bindsnet.environment import GymEnvironment\nfrom bindsnet.network.nodes import Input, LIFNodes\nfrom bindsnet.pipeline.action import select_softmax\n\n# Build network.\nnetwork = Network(dt=1.0)\n\n# Layers of neurons.\ninpt = Input(n=80 * 80, shape=[1, 1, 1, 80, 80], traces=True)\nmiddle = LIFNodes(n=100, traces=True)\nout = LIFNodes(n=4, refrac=0, traces=True)\n\n# Connections between layers.\ninpt_middle = Connection(source=inpt, target=middle, wmin=0, wmax=1e-1)\nmiddle_out = Connection(\n    source=middle,\n    target=out,\n    wmin=0,\n    wmax=1,\n    update_rule=MSTDP,\n    nu=1e-1,\n    norm=0.5 * middle.n,\n)\n\n# Add all layers and connections to the network.\nnetwork.add_layer(inpt, name=""Input Layer"")\nnetwork.add_layer(middle, name=""Hidden Layer"")\nnetwork.add_layer(out, name=""Output Layer"")\nnetwork.add_connection(inpt_middle, source=""Input Layer"", target=""Hidden Layer"")\nnetwork.add_connection(middle_out, source=""Hidden Layer"", target=""Output Layer"")\n\n# Load the Breakout environment.\nenvironment = GymEnvironment(""BreakoutDeterministic-v4"")\nenvironment.reset()\n\n# Build pipeline from specified components.\nenvironment_pipeline = EnvironmentPipeline(\n    network,\n    environment,\n    encoding=bernoulli,\n    action_function=select_softmax,\n    output=""Output Layer"",\n    time=100,\n    history_length=1,\n    delta=1,\n    plot_interval=1,\n    render_interval=1,\n)\n\n\ndef run_pipeline(pipeline, episode_count):\n    for i in range(episode_count):\n        total_reward = 0\n        pipeline.reset_state_variables()\n        is_done = False\n        while not is_done:\n            result = pipeline.env_step()\n            pipeline.step(result)\n\n            reward = result[1]\n            total_reward += reward\n\n            is_done = result[2]\n        print(f""Episode {i} total reward:{total_reward}"")\n\n\nprint(""Training: "")\nrun_pipeline(environment_pipeline, episode_count=100)\n\n# stop MSTDP\nenvironment_pipeline.network.learning = False\n\nprint(""Testing: "")\nrun_pipeline(environment_pipeline, episode_count=100)\n'"
examples/breakout/play_breakout_from_ANN.py,7,"b'import argparse\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\n\nfrom bindsnet.network import Network\nfrom bindsnet.pipeline import EnvironmentPipeline\nfrom bindsnet.encoding import bernoulli, poisson\nfrom bindsnet.network.topology import Connection\nfrom bindsnet.environment import GymEnvironment\nfrom bindsnet.network.nodes import Input, LIFNodes, IzhikevichNodes, IFNodes\nfrom bindsnet.pipeline.action import *\n\nfrom bindsnet.network.nodes import Nodes, AbstractInput\nfrom typing import Iterable, Optional, Union\n\nparser = argparse.ArgumentParser(prefix_chars=""@"")\nparser.add_argument(""@@seed"", type=int, default=42)\nparser.add_argument(""@@dt"", type=float, default=1.0)\nparser.add_argument(""@@gpu"", dest=""gpu"", action=""store_true"")\nparser.add_argument(""@@layer1scale"", dest=""layer1scale"", type=float, default=57.68)\nparser.add_argument(""@@layer2scale"", dest=""layer2scale"", type=float, default=77.48)\nparser.add_argument(""@@num_episodes"", type=int, default=10)\nparser.add_argument(""@@plot_interval"", type=int, default=1)\nparser.add_argument(""@@rander_interval"", type=int, default=1)\nparser.set_defaults(plot=False, render=False, gpu=True, probabilistic=False)\nlocals().update(vars(parser.parse_args()))\n\n# Setup PyTorch computing device\ndevice = torch.device(""cuda"" if torch.cuda.is_available() and gpu else ""cpu"")\ntorch.random.manual_seed(seed)\n\n\n# Build ANN\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(6400, 1000)\n        self.fc2 = nn.Linear(1000, 4)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n\n# load ANN\ndqn_network = torch.load(""trained_shallow_ANN.pt"", map_location=device)\n\n# Build Spiking network.\nnetwork = Network(dt=dt).to(device)\n\n# Layers of neurons.\ninpt = Input(n=6400, traces=False)  # Input layer\nmiddle = LIFNodes(\n    n=1000, refrac=0, traces=True, thresh=-52.0, rest=-65.0\n)  # Hidden layer\nreadout = LIFNodes(\n    n=4, refrac=0, traces=True, thresh=-52.0, rest=-65.0\n)  # Readout layer\nlayers = {""X"": inpt, ""M"": middle, ""R"": readout}\n\n# Set the connections between layers with the values set by the ANN\n# Input -> hidden.\ninpt_middle = Connection(\n    source=layers[""X""],\n    target=layers[""M""],\n    w=torch.transpose(dqn_network.fc1.weight, 0, 1) * layer1scale,\n)\n# hidden -> readout.\nmiddle_out = Connection(\n    source=layers[""M""],\n    target=layers[""R""],\n    w=torch.transpose(dqn_network.fc2.weight, 0, 1) * layer2scale,\n)\n\n# Add all layers and connections to the network.\nnetwork.add_layer(inpt, name=""Input Layer"")\nnetwork.add_layer(middle, name=""Hidden Layer"")\nnetwork.add_layer(readout, name=""Output Layer"")\nnetwork.add_connection(inpt_middle, source=""Input Layer"", target=""Hidden Layer"")\nnetwork.add_connection(middle_out, source=""Hidden Layer"", target=""Output Layer"")\n\n# Load the Breakout environment.\nenvironment = GymEnvironment(""BreakoutDeterministic-v4"")\nenvironment.reset()\n\n# Build pipeline from specified components.\npipeline = EnvironmentPipeline(\n    network,\n    environment,\n    encoding=poisson,\n    encode_factor=50,\n    action_function=select_highest,\n    percent_of_random_action=0.05,\n    random_action_after=5,\n    output=""Output Layer"",\n    reset_output_spikes=True,\n    time=500,\n    overlay_input=4,\n    history_length=1,\n    plot_interval=plot_interval if plot else None,\n    render_interval=render_interval if render else None,\n    device=device,\n)\n\n# Run environment simulation for number of episodes.\nfor i in tqdm(range(num_episodes)):\n    total_reward = 0\n    pipeline.reset_state_variables()\n    is_done = False\n    pipeline.env.step(1)  # start with fire the ball\n    pipeline.env.step(1)  # start with fire the ball\n    while not is_done:\n        result = pipeline.env_step()\n        pipeline.step(result)\n\n        reward = result[1]\n        total_reward += reward\n\n        is_done = result[2]\n    tqdm.write(f""Episode {i} total reward:{total_reward}"")\n    with open(""play-breakout_results.csv"", ""a"") as myfile:\n        myfile.write(f""{i},{layer1scale},{layer2scale},{total_reward}\\n"")\n'"
examples/breakout/random_baseline.py,0,"b'import os\nimport argparse\nimport numpy as np\n\nfrom bindsnet.environment import GymEnvironment\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""-n"", type=int, default=1000000)\nparser.add_argument(""--render"", dest=""render"", action=""store_true"")\nparser.set_defaults(render=False)\n\nargs = parser.parse_args()\n\nn = args.n\nrender = args.render\n\n# Load Breakout environment.\nenv = GymEnvironment(""BreakoutDeterministic-v4"")\nenv.reset()\n\ntotal = 0\nrewards = []\navg_rewards = []\nlengths = []\navg_lengths = []\n\ni, j, k = 0, 0, 0\nwhile i < n:\n    if render:\n        env.render()\n\n    # Select random action.\n    a = np.random.choice(4)\n\n    # Step environment with random action.\n    obs, reward, done, info = env.step(a)\n\n    total += reward\n\n    rewards.append(reward)\n    if i == 0:\n        avg_rewards.append(reward)\n    else:\n        avg = (avg_rewards[-1] * (i - 1)) / i + reward / i\n        avg_rewards.append(avg)\n\n    if i % 100 == 0:\n        print(\n            ""Iteration %d: last reward: %.2f, average reward: %.2f""\n            % (i, reward, avg_rewards[-1])\n        )\n\n    if done:\n        # Restart game if out of lives.\n        env.reset()\n\n        length = i - j\n        lengths.append(length)\n        if j == 0:\n            avg_lengths.append(length)\n        else:\n            avg = (avg_lengths[-1] * (k - 1)) / k + length / k\n            avg_lengths.append(avg)\n\n        print(\n            ""Episode %d: last length: %.2f, average length: %.2f""\n            % (k, length, avg_lengths[-1])\n        )\n\n        j += length\n        k += 1\n\n    i += 1\n'"
examples/breakout/random_network_baseline.py,6,"b'import torch\nimport argparse\n\nfrom bindsnet.network import Network\nfrom bindsnet.learning import Hebbian\nfrom bindsnet.pipeline import EnvironmentPipeline\nfrom bindsnet.encoding import bernoulli\nfrom bindsnet.network.monitors import Monitor\nfrom bindsnet.environment import GymEnvironment\nfrom bindsnet.network.topology import Connection\nfrom bindsnet.network.nodes import Input, LIFNodes\nfrom bindsnet.pipeline.action import select_multinomial\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""-n"", type=int, default=1000000)\nparser.add_argument(""--seed"", type=int, default=0)\nparser.add_argument(""--n_neurons"", type=int, default=100)\nparser.add_argument(""--dt"", type=float, default=1.0)\nparser.add_argument(""--plot_interval"", type=int, default=10)\nparser.add_argument(""--render_interval"", type=int, default=10)\nparser.add_argument(""--print_interval"", type=int, default=100)\nparser.add_argument(""--gpu"", dest=""gpu"", action=""store_true"")\nparser.set_defaults(plot=False, render=False, gpu=False)\n\nargs = parser.parse_args()\n\nn = args.n\nseed = args.seed\nn_neurons = args.n_neurons\ndt = args.dt\nplot_interval = args.plot_interval\nrender_interval = args.render_interval\nprint_interval = args.print_interval\ngpu = args.gpu\n\nif gpu:\n    torch.set_default_tensor_type(""torch.cuda.FloatTensor"")\n    torch.cuda.manual_seed_all(seed)\nelse:\n    torch.manual_seed(seed)\n\n# Build network.\nnetwork = Network(dt=dt)\n\n# Layers of neurons.\ninpt = Input(shape=(1, 1, 1, 80, 80), traces=True)  # Input layer\nexc = LIFNodes(n=n_neurons, refrac=0, traces=True)  # Excitatory layer\nreadout = LIFNodes(n=16, refrac=0, traces=True)  # Readout layer\nlayers = {""X"": inpt, ""E"": exc, ""R"": readout}\n\n# Connections between layers.\n# Input -> excitatory.\nw = 0.01 * torch.rand(layers[""X""].n, layers[""E""].n)\ninput_exc_conn = Connection(\n    source=layers[""X""],\n    target=layers[""E""],\n    w=0.01 * torch.rand(layers[""X""].n, layers[""E""].n),\n    wmax=0.02,\n    norm=0.01 * layers[""X""].n,\n)\n\n# Excitatory -> readout.\nexc_readout_conn = Connection(\n    source=layers[""E""],\n    target=layers[""R""],\n    w=0.01 * torch.rand(layers[""E""].n, layers[""R""].n),\n    update_rule=Hebbian,\n    nu=[1e-2, 1e-2],\n    norm=0.5 * layers[""E""].n,\n)\n\n# Spike recordings for all layers.\nspikes = {}\nfor layer in layers:\n    spikes[layer] = Monitor(layers[layer], [""s""], time=plot_interval)\n\n# Voltage recordings for excitatory and readout layers.\nvoltages = {}\nfor layer in set(layers.keys()) - {""X""}:\n    voltages[layer] = Monitor(layers[layer], [""v""], time=plot_interval)\n\n# Add all layers and connections to the network.\nfor layer in layers:\n    network.add_layer(layers[layer], name=layer)\n\nnetwork.add_connection(input_exc_conn, source=""X"", target=""E"")\nnetwork.add_connection(exc_readout_conn, source=""E"", target=""R"")\n\n# Add all monitors to the network.\nfor layer in layers:\n    network.add_monitor(spikes[layer], name=""%s_spikes"" % layer)\n\n    if layer in voltages:\n        network.add_monitor(voltages[layer], name=""%s_voltages"" % layer)\n\n# Load the Breakout environment.\nenvironment = GymEnvironment(""BreakoutDeterministic-v4"")\nenvironment.reset()\n\npipeline = EnvironmentPipeline(\n    network,\n    environment,\n    encoding=bernoulli,\n    time=1,\n    history=5,\n    delta=10,\n    plot_interval=plot_interval,\n    print_interval=print_interval,\n    render_interval=render_interval,\n    action_function=select_multinomial,\n    output=""R"",\n)\n\ntotal = 0\nrewards = []\navg_rewards = []\nlengths = []\navg_lengths = []\n\ni = 0\ntry:\n    while i < n:\n        result = pipeline.env_step()\n        pipeline.step(result)\n\n        is_done = result[2]\n        if is_done:\n            pipeline.reset_state_variables()\n\n        i += 1\n\nexcept KeyboardInterrupt:\n    environment.close()\n'"
examples/mnist/SOM_LM-SNNs.py,15,"b'import os\nimport torch\nimport argparse\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torchvision import transforms\nfrom tqdm import tqdm\n\nfrom time import time as t\n\nfrom bindsnet.datasets import MNIST\nfrom bindsnet.encoding import PoissonEncoder, poisson\nfrom bindsnet.models import IncreasingInhibitionNetwork\nfrom bindsnet.network.monitors import Monitor\nfrom bindsnet.utils import get_square_weights, get_square_assignments\nfrom bindsnet.evaluation import all_activity, proportion_weighting, assign_labels\nfrom bindsnet.analysis.plotting import (\n    plot_input,\n    plot_spikes,\n    plot_weights,\n    plot_assignments,\n    plot_performance,\n    plot_voltages,\n)\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--seed"", type=int, default=0)\nparser.add_argument(""--n_neurons"", type=int, default=100)\nparser.add_argument(""--n_epochs"", type=int, default=1)\nparser.add_argument(""--n_test"", type=int, default=10000)\nparser.add_argument(""--n_workers"", type=int, default=-1)\nparser.add_argument(""--theta_plus"", type=float, default=0.05)\nparser.add_argument(""--time"", type=int, default=250)\nparser.add_argument(""--dt"", type=int, default=1.0)\nparser.add_argument(""--intensity"", type=float, default=64)\nparser.add_argument(""--progress_interval"", type=int, default=10)\nparser.add_argument(""--update_interval"", type=int, default=250)\nparser.add_argument(""--update_inhibation_weights"", type=int, default=500)\nparser.add_argument(""--plot_interval"", type=int, default=250)\nparser.add_argument(""--plot"", dest=""plot"", action=""store_true"")\nparser.add_argument(""--gpu"", dest=""gpu"", action=""store_true"")\nparser.set_defaults(plot=True, gpu=True)\n\nargs = parser.parse_args()\n\nseed = args.seed\nn_neurons = args.n_neurons\nn_epochs = args.n_epochs\nn_test = args.n_test\nn_workers = args.n_workers\ntheta_plus = args.theta_plus\ntime = args.time\ndt = args.dt\nintensity = args.intensity\nprogress_interval = args.progress_interval\nplot_interval = args.plot_interval\nupdate_interval = args.update_interval\nplot = args.plot\ngpu = args.gpu\nupdate_inhibation_weights = args.update_inhibation_weights\n\n# Sets up Gpu use\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\nprint(""Running on Device = "", device)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\nelse:\n    torch.manual_seed(seed)\ntorch.set_num_threads(os.cpu_count() - 1)\n\n# Determines number of workers to use\nif n_workers == -1:\n    n_workers = torch.cuda.is_available() * 4 * torch.cuda.device_count()\n\nn_sqrt = int(np.ceil(np.sqrt(n_neurons)))\nstart_intensity = intensity\n\n# Build network.\nnetwork = IncreasingInhibitionNetwork(\n    n_input=784,\n    n_neurons=n_neurons,\n    start_inhib=10,\n    max_inhib=-40.0,\n    theta_plus=0.05,\n    tc_theta_decay=1e7,\n    inpt_shape=(1, 28, 28),\n    nu=(1e-4, 1e-2),\n)\n\nnetwork.to(device)\n\n# Load MNIST data.\ndataset = MNIST(\n    PoissonEncoder(time=time, dt=dt),\n    None,\n    root=os.path.join("".."", "".."", ""data"", ""MNIST""),\n    download=True,\n    transform=transforms.Compose(\n        [transforms.ToTensor(), transforms.Lambda(lambda x: x * intensity)]\n    ),\n)\n\n# Record spikes during the simulation.\nspike_record = torch.zeros(update_interval, time, n_neurons).cpu()\n\n# Neuron assignments and spike proportions.\nn_classes = 10\nassignments = -torch.ones(n_neurons).cpu()\nproportions = torch.zeros(n_neurons, n_classes).cpu()\nrates = torch.zeros(n_neurons, n_classes).cpu()\n\n# Sequence of accuracy estimates.\naccuracy = {""all"": [], ""proportion"": []}\n\n# Voltage recording for excitatory and inhibitory layers.\nsom_voltage_monitor = Monitor(network.layers[""Y""], [""v""], time=time)\nnetwork.add_monitor(som_voltage_monitor, name=""som_voltage"")\n\n# Set up monitors for spikes and voltages\nspikes = {}\nfor layer in set(network.layers):\n    spikes[layer] = Monitor(network.layers[layer], state_vars=[""s""], time=time)\n    network.add_monitor(spikes[layer], name=""%s_spikes"" % layer)\n\nvoltages = {}\nfor layer in set(network.layers) - {""X""}:\n    voltages[layer] = Monitor(network.layers[layer], state_vars=[""v""], time=time)\n    network.add_monitor(voltages[layer], name=""%s_voltages"" % layer)\n\ninpt_ims, inpt_axes = None, None\nspike_ims, spike_axes = None, None\nweights_im = None\nassigns_im = None\nperf_ax = None\nvoltage_axes, voltage_ims = None, None\nsave_weights_fn = ""plots/weights/weights.png""\nsave_performance_fn = ""plots/performance/performance.png""\nsave_assaiments_fn = ""plots/assaiments/assaiments.png""\n\ndirectorys = [""plots"", ""plots/weights"", ""plots/performance"", ""plots/assaiments""]\nfor directory in directorys:\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n# diagonal weights for increassing the inhibitiosn\nweights_mask = (1 - torch.diag(torch.ones(n_neurons))).to(device)\n\n# Train the network.\nprint(""\\nBegin training.\\n"")\nstart = t()\n\nfor epoch in range(n_epochs):\n    labels = []\n\n    if epoch % progress_interval == 0:\n        print(""Progress: %d / %d (%.4f seconds)"" % (epoch, n_epochs, t() - start))\n        start = t()\n\n    # Create a dataloader to iterate and batch data\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=1, shuffle=True, num_workers=n_workers, pin_memory=gpu\n    )\n\n    for step, batch in enumerate(tqdm(dataloader)):\n        # Get next input sample.\n        inputs = {""X"": batch[""encoded_image""].view(time, 1, 1, 28, 28).to(device)}\n\n        if step > 0:\n            if step % update_inhibation_weights == 0:\n                if step % (update_inhibation_weights * 10) == 0:\n                    network.Y_to_Y.w -= weights_mask * 50\n                else:\n                    # Inhibit the connection even more\n                    # network.Y_to_Y.w -= weights_mask * network.Y_to_Y.w.abs()*0.2\n                    network.Y_to_Y.w -= weights_mask * 0.5\n\n            if step % update_interval == 0:\n                # Convert the array of labels into a tensor\n                label_tensor = torch.tensor(labels).cpu()\n\n                # Get network predictions.\n                all_activity_pred = all_activity(\n                    spikes=spike_record, assignments=assignments, n_labels=n_classes\n                )\n                proportion_pred = proportion_weighting(\n                    spikes=spike_record,\n                    assignments=assignments,\n                    proportions=proportions,\n                    n_labels=n_classes,\n                )\n\n                # Compute network accuracy according to available classification strategies.\n                accuracy[""all""].append(\n                    100\n                    * torch.sum(label_tensor.long() == all_activity_pred).item()\n                    / len(label_tensor)\n                )\n                accuracy[""proportion""].append(\n                    100\n                    * torch.sum(label_tensor.long() == proportion_pred).item()\n                    / len(label_tensor)\n                )\n\n                tqdm.write(\n                    ""\\nAll activity accuracy: %.2f (last), %.2f (average), %.2f (best)""\n                    % (\n                        accuracy[""all""][-1],\n                        np.mean(accuracy[""all""]),\n                        np.max(accuracy[""all""]),\n                    )\n                )\n                tqdm.write(\n                    ""Proportion weighting accuracy: %.2f (last), %.2f (average), %.2f (best)\\n""\n                    % (\n                        accuracy[""proportion""][-1],\n                        np.mean(accuracy[""proportion""]),\n                        np.max(accuracy[""proportion""]),\n                    )\n                )\n\n                # Assign labels to excitatory layer neurons.\n                assignments, proportions, rates = assign_labels(\n                    spikes=spike_record,\n                    labels=label_tensor,\n                    n_labels=n_classes,\n                    rates=rates,\n                )\n\n                labels = []\n\n        labels.append(batch[""label""])\n\n        temp_spikes = 0\n        factor = 1.2\n        for retry in range(5):\n            # Run the network on the input.\n            network.run(inputs=inputs, time=time, input_time_dim=1)\n\n            # Get spikes from the network\n            temp_spikes = spikes[""Y""].get(""s"").squeeze()\n\n            if temp_spikes.sum().sum() < 2:\n                inputs[""X""] *= (\n                    poisson(\n                        datum=factor * batch[""image""].clamp(min=0), dt=dt, time=time\n                    )\n                    .to(device)\n                    .view(time, 1, 1, 28, 28)\n                )\n                factor *= factor\n            else:\n                break\n\n        # Get voltage recording.\n        exc_voltages = som_voltage_monitor.get(""v"")\n\n        # Add to spikes recording.\n        spike_record[step % update_interval] = temp_spikes.detach().clone().cpu()\n        spike_record[step % update_interval].copy_(temp_spikes, non_blocking=True)\n\n        # Optionally plot various simulation information.\n        if plot and step % plot_interval == 0:\n            image = batch[""image""].view(28, 28)\n            inpt = inputs[""X""].view(time, 784).sum(0).view(28, 28)\n            input_exc_weights = network.connections[(""X"", ""Y"")].w\n            square_weights = get_square_weights(\n                input_exc_weights.view(784, n_neurons), n_sqrt, 28\n            )\n            square_assignments = get_square_assignments(assignments, n_sqrt)\n            spikes_ = {layer: spikes[layer].get(""s"") for layer in spikes}\n            voltages = {""Y"": exc_voltages}\n            inpt_axes, inpt_ims = plot_input(\n                image, inpt, label=batch[""label""], axes=inpt_axes, ims=inpt_ims\n            )\n            spike_ims, spike_axes = plot_spikes(spikes_, ims=spike_ims, axes=spike_axes)\n            [weights_im, save_weights_fn] = plot_weights(\n                square_weights, im=weights_im, save=save_weights_fn\n            )\n            assigns_im = plot_assignments(\n                square_assignments, im=assigns_im, save=save_assaiments_fn\n            )\n            perf_ax = plot_performance(accuracy, ax=perf_ax, save=save_performance_fn)\n            voltage_ims, voltage_axes = plot_voltages(\n                voltages, ims=voltage_ims, axes=voltage_axes, plot_type=""line""\n            )\n            #\n            plt.pause(1e-8)\n\n        network.reset_state_variables()  # Reset state variables.\n\nprint(""Progress: %d / %d (%.4f seconds)"" % (epoch + 1, n_epochs, t() - start))\nprint(""Training complete.\\n"")\n'"
examples/mnist/batch_eth_mnist.py,11,"b'import os\nimport torch\nimport argparse\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torchvision import transforms\nfrom tqdm import tqdm\n\nfrom time import time as t\n\nfrom bindsnet import ROOT_DIR\nfrom bindsnet.datasets import MNIST, DataLoader\nfrom bindsnet.encoding import PoissonEncoder\nfrom bindsnet.evaluation import all_activity, proportion_weighting, assign_labels\nfrom bindsnet.models import DiehlAndCook2015\nfrom bindsnet.network.monitors import Monitor\nfrom bindsnet.utils import get_square_weights, get_square_assignments\nfrom bindsnet.analysis.plotting import (\n    plot_input,\n    plot_spikes,\n    plot_weights,\n    plot_performance,\n    plot_assignments,\n    plot_voltages,\n)\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--seed"", type=int, default=0)\nparser.add_argument(""--n_neurons"", type=int, default=100)\nparser.add_argument(""--batch_size"", type=int, default=32)\nparser.add_argument(""--n_epochs"", type=int, default=1)\nparser.add_argument(""--n_test"", type=int, default=10000)\nparser.add_argument(""--n_workers"", type=int, default=-1)\nparser.add_argument(""--update_steps"", type=int, default=256)\nparser.add_argument(""--exc"", type=float, default=22.5)\nparser.add_argument(""--inh"", type=float, default=120)\nparser.add_argument(""--theta_plus"", type=float, default=0.05)\nparser.add_argument(""--time"", type=int, default=100)\nparser.add_argument(""--dt"", type=int, default=1.0)\nparser.add_argument(""--intensity"", type=float, default=128)\nparser.add_argument(""--progress_interval"", type=int, default=10)\nparser.add_argument(""--train"", dest=""train"", action=""store_true"")\nparser.add_argument(""--test"", dest=""train"", action=""store_false"")\nparser.add_argument(""--plot"", dest=""plot"", action=""store_true"")\nparser.add_argument(""--gpu"", dest=""gpu"", action=""store_true"")\nparser.set_defaults(plot=False, gpu=False, train=True)\n\nargs = parser.parse_args()\n\nseed = args.seed\nn_neurons = args.n_neurons\nbatch_size = args.batch_size\nn_epochs = args.n_epochs\nn_test = args.n_test\nn_workers = args.n_workers\nupdate_steps = args.update_steps\nexc = args.exc\ninh = args.inh\ntheta_plus = args.theta_plus\ntime = args.time\ndt = args.dt\nintensity = args.intensity\nprogress_interval = args.progress_interval\ntrain = args.train\nplot = args.plot\ngpu = args.gpu\n\nupdate_interval = update_steps * batch_size\n\n# Sets up Gpu use\nif gpu and torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\nelse:\n    torch.manual_seed(seed)\n\n# Determines number of workers to use\nif n_workers == -1:\n    n_workers = gpu * 4 * torch.cuda.device_count()\n\nn_sqrt = int(np.ceil(np.sqrt(n_neurons)))\nstart_intensity = intensity\n\n# Build network.\nnetwork = DiehlAndCook2015(\n    n_inpt=784,\n    n_neurons=n_neurons,\n    exc=exc,\n    inh=inh,\n    dt=dt,\n    norm=78.4,\n    nu=(1e-4, 1e-2),\n    theta_plus=theta_plus,\n    inpt_shape=(1, 28, 28),\n)\n\n# Directs network to GPU\nif gpu:\n    network.to(""cuda"")\n\n# Load MNIST data.\ndataset = MNIST(\n    PoissonEncoder(time=time, dt=dt),\n    None,\n    root=os.path.join(ROOT_DIR, ""data"", ""MNIST""),\n    download=True,\n    transform=transforms.Compose(\n        [transforms.ToTensor(), transforms.Lambda(lambda x: x * intensity)]\n    ),\n)\n\n# Neuron assignments and spike proportions.\nn_classes = 10\nassignments = -torch.ones(n_neurons)\nproportions = torch.zeros(n_neurons, n_classes)\nrates = torch.zeros(n_neurons, n_classes)\n\n# Sequence of accuracy estimates.\naccuracy = {""all"": [], ""proportion"": []}\n\n# Voltage recording for excitatory and inhibitory layers.\nexc_voltage_monitor = Monitor(network.layers[""Ae""], [""v""], time=time)\ninh_voltage_monitor = Monitor(network.layers[""Ai""], [""v""], time=time)\nnetwork.add_monitor(exc_voltage_monitor, name=""exc_voltage"")\nnetwork.add_monitor(inh_voltage_monitor, name=""inh_voltage"")\n\n# Set up monitors for spikes and voltages\nspikes = {}\nfor layer in set(network.layers):\n    spikes[layer] = Monitor(network.layers[layer], state_vars=[""s""], time=time)\n    network.add_monitor(spikes[layer], name=""%s_spikes"" % layer)\n\nvoltages = {}\nfor layer in set(network.layers) - {""X""}:\n    voltages[layer] = Monitor(network.layers[layer], state_vars=[""v""], time=time)\n    network.add_monitor(voltages[layer], name=""%s_voltages"" % layer)\n\ninpt_ims, inpt_axes = None, None\nspike_ims, spike_axes = None, None\nweights_im = None\nassigns_im = None\nperf_ax = None\nvoltage_axes, voltage_ims = None, None\n\nspike_record = torch.zeros(update_interval, time, n_neurons)\n\n# Train the network.\nprint(""\\nBegin training.\\n"")\nstart = t()\n\nfor epoch in range(n_epochs):\n    labels = []\n\n    if epoch % progress_interval == 0:\n        print(""Progress: %d / %d (%.4f seconds)"" % (epoch, n_epochs, t() - start))\n        start = t()\n\n    # Create a dataloader to iterate and batch data\n    dataloader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=n_workers,\n        pin_memory=gpu,\n    )\n\n    for step, batch in enumerate(tqdm(dataloader)):\n        # Get next input sample.\n        inputs = {""X"": batch[""encoded_image""]}\n        if gpu:\n            inputs = {k: v.cuda() for k, v in inputs.items()}\n\n        if step % update_steps == 0 and step > 0:\n            # Convert the array of labels into a tensor\n            label_tensor = torch.tensor(labels)\n\n            # Get network predictions.\n            all_activity_pred = all_activity(\n                spikes=spike_record, assignments=assignments, n_labels=n_classes\n            )\n            proportion_pred = proportion_weighting(\n                spikes=spike_record,\n                assignments=assignments,\n                proportions=proportions,\n                n_labels=n_classes,\n            )\n\n            # Compute network accuracy according to available classification strategies.\n            accuracy[""all""].append(\n                100\n                * torch.sum(label_tensor.long() == all_activity_pred).item()\n                / len(label_tensor)\n            )\n            accuracy[""proportion""].append(\n                100\n                * torch.sum(label_tensor.long() == proportion_pred).item()\n                / len(label_tensor)\n            )\n\n            print(\n                ""\\nAll activity accuracy: %.2f (last), %.2f (average), %.2f (best)""\n                % (\n                    accuracy[""all""][-1],\n                    np.mean(accuracy[""all""]),\n                    np.max(accuracy[""all""]),\n                )\n            )\n            print(\n                ""Proportion weighting accuracy: %.2f (last), %.2f (average), %.2f (best)\\n""\n                % (\n                    accuracy[""proportion""][-1],\n                    np.mean(accuracy[""proportion""]),\n                    np.max(accuracy[""proportion""]),\n                )\n            )\n\n            # Assign labels to excitatory layer neurons.\n            assignments, proportions, rates = assign_labels(\n                spikes=spike_record,\n                labels=label_tensor,\n                n_labels=n_classes,\n                rates=rates,\n            )\n\n            labels = []\n\n        labels.extend(batch[""label""].tolist())\n\n        # Run the network on the input.\n        network.run(inputs=inputs, time=time, input_time_dim=1)\n\n        # Add to spikes recording.\n        s = spikes[""Ae""].get(""s"").permute((1, 0, 2))\n        spike_record[\n            (step * batch_size)\n            % update_interval : (step * batch_size % update_interval)\n            + s.size(0)\n        ] = s\n\n        # Get voltage recording.\n        exc_voltages = exc_voltage_monitor.get(""v"")\n        inh_voltages = inh_voltage_monitor.get(""v"")\n\n        # Optionally plot various simulation information.\n        if plot:\n            image = batch[""image""][:, 0].view(28, 28)\n            inpt = inputs[""X""][:, 0].view(time, 784).sum(0).view(28, 28)\n            input_exc_weights = network.connections[(""X"", ""Ae"")].w\n            square_weights = get_square_weights(\n                input_exc_weights.view(784, n_neurons), n_sqrt, 28\n            )\n            square_assignments = get_square_assignments(assignments, n_sqrt)\n            spikes_ = {\n                layer: spikes[layer].get(""s"")[:, 0].contiguous() for layer in spikes\n            }\n            voltages = {""Ae"": exc_voltages, ""Ai"": inh_voltages}\n\n            inpt_axes, inpt_ims = plot_input(\n                image, inpt, label=labels[step], axes=inpt_axes, ims=inpt_ims\n            )\n            spike_ims, spike_axes = plot_spikes(spikes_, ims=spike_ims, axes=spike_axes)\n            weights_im = plot_weights(square_weights, im=weights_im)\n            assigns_im = plot_assignments(square_assignments, im=assigns_im)\n            perf_ax = plot_performance(\n                accuracy, x_scale=update_steps * batch_size, ax=perf_ax\n            )\n            voltage_ims, voltage_axes = plot_voltages(\n                voltages, ims=voltage_ims, axes=voltage_axes, plot_type=""line""\n            )\n\n            plt.pause(1e-8)\n\n        network.reset_state_variables()  # Reset state variables.\n\nprint(""Progress: %d / %d (%.4f seconds)"" % (epoch + 1, n_epochs, t() - start))\nprint(""Training complete.\\n"")\n'"
examples/mnist/conv_mnist.py,4,"b'import torch\nimport argparse\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\n\nfrom time import time as t\nfrom tqdm import tqdm\n\nfrom bindsnet.datasets import MNIST\nfrom bindsnet.encoding import PoissonEncoder\nfrom bindsnet.network import Network\nfrom bindsnet.learning import PostPre\nfrom bindsnet.network.monitors import Monitor\nfrom bindsnet.network.nodes import DiehlAndCookNodes, Input\nfrom bindsnet.network.topology import Conv2dConnection, Connection\nfrom bindsnet.analysis.plotting import (\n    plot_input,\n    plot_spikes,\n    plot_conv2d_weights,\n    plot_voltages,\n)\n\nprint()\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--seed"", type=int, default=0)\nparser.add_argument(""--n_epochs"", type=int, default=1)\nparser.add_argument(""--n_test"", type=int, default=10000)\nparser.add_argument(""--kernel_size"", type=int, default=16)\nparser.add_argument(""--stride"", type=int, default=4)\nparser.add_argument(""--n_filters"", type=int, default=25)\nparser.add_argument(""--padding"", type=int, default=0)\nparser.add_argument(""--time"", type=int, default=50)\nparser.add_argument(""--dt"", type=int, default=1.0)\nparser.add_argument(""--intensity"", type=float, default=128.0)\nparser.add_argument(""--progress_interval"", type=int, default=10)\nparser.add_argument(""--update_interval"", type=int, default=250)\nparser.add_argument(""--train"", dest=""train"", action=""store_true"")\nparser.add_argument(""--test"", dest=""train"", action=""store_false"")\nparser.add_argument(""--plot"", dest=""plot"", action=""store_true"")\nparser.add_argument(""--gpu"", dest=""gpu"", action=""store_true"")\nparser.set_defaults(plot=False, gpu=False, train=True)\n\nargs = parser.parse_args()\n\nseed = args.seed\nn_epochs = args.n_epochs\nn_test = args.n_test\nkernel_size = args.kernel_size\nstride = args.stride\nn_filters = args.n_filters\npadding = args.padding\ntime = args.time\ndt = args.dt\nintensity = args.intensity\nprogress_interval = args.progress_interval\nupdate_interval = args.update_interval\ntrain = args.train\nplot = args.plot\ngpu = args.gpu\n\nif gpu:\n    torch.cuda.manual_seed_all(seed)\nelse:\n    torch.manual_seed(seed)\n\nif not train:\n    update_interval = n_test\n\nconv_size = int((28 - kernel_size + 2 * padding) / stride) + 1\nper_class = int((n_filters * conv_size * conv_size) / 10)\n\n# Build network.\nnetwork = Network()\ninput_layer = Input(n=784, shape=(1, 28, 28), traces=True)\n\nconv_layer = DiehlAndCookNodes(\n    n=n_filters * conv_size * conv_size,\n    shape=(n_filters, conv_size, conv_size),\n    traces=True,\n)\n\nconv_conn = Conv2dConnection(\n    input_layer,\n    conv_layer,\n    kernel_size=kernel_size,\n    stride=stride,\n    update_rule=PostPre,\n    norm=0.4 * kernel_size ** 2,\n    nu=[1e-4, 1e-2],\n    wmax=1.0,\n)\n\nw = torch.zeros(n_filters, conv_size, conv_size, n_filters, conv_size, conv_size)\nfor fltr1 in range(n_filters):\n    for fltr2 in range(n_filters):\n        if fltr1 != fltr2:\n            for i in range(conv_size):\n                for j in range(conv_size):\n                    w[fltr1, i, j, fltr2, i, j] = -100.0\n\nw = w.view(n_filters * conv_size * conv_size, n_filters * conv_size * conv_size)\nrecurrent_conn = Connection(conv_layer, conv_layer, w=w)\n\nnetwork.add_layer(input_layer, name=""X"")\nnetwork.add_layer(conv_layer, name=""Y"")\nnetwork.add_connection(conv_conn, source=""X"", target=""Y"")\nnetwork.add_connection(recurrent_conn, source=""Y"", target=""Y"")\n\n# Voltage recording for excitatory and inhibitory layers.\nvoltage_monitor = Monitor(network.layers[""Y""], [""v""], time=time)\nnetwork.add_monitor(voltage_monitor, name=""output_voltage"")\n\nif gpu:\n    network.to(""cuda"")\n\n# Load MNIST data.\ntrain_dataset = MNIST(\n    PoissonEncoder(time=time, dt=dt),\n    None,\n    ""../../data/MNIST"",\n    download=True,\n    train=True,\n    transform=transforms.Compose(\n        [transforms.ToTensor(), transforms.Lambda(lambda x: x * intensity)]\n    ),\n)\n\nspikes = {}\nfor layer in set(network.layers):\n    spikes[layer] = Monitor(network.layers[layer], state_vars=[""s""], time=time)\n    network.add_monitor(spikes[layer], name=""%s_spikes"" % layer)\n\nvoltages = {}\nfor layer in set(network.layers) - {""X""}:\n    voltages[layer] = Monitor(network.layers[layer], state_vars=[""v""], time=time)\n    network.add_monitor(voltages[layer], name=""%s_voltages"" % layer)\n\n# Train the network.\nprint(""Begin training.\\n"")\nstart = t()\n\ninpt_axes = None\ninpt_ims = None\nspike_ims = None\nspike_axes = None\nweights1_im = None\nvoltage_ims = None\nvoltage_axes = None\n\nfor epoch in range(n_epochs):\n    if epoch % progress_interval == 0:\n        print(""Progress: %d / %d (%.4f seconds)"" % (epoch, n_epochs, t() - start))\n        start = t()\n\n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=gpu\n    )\n\n    for step, batch in enumerate(tqdm(train_dataloader)):\n        # Get next input sample.\n\n        inputs = {""X"": batch[""encoded_image""].view(time, 1, 1, 28, 28)}\n        if gpu:\n            inputs = {k: v.cuda() for k, v in inputs.items()}\n        label = batch[""label""]\n\n        # Run the network on the input.\n        network.run(inputs=inputs, time=time, input_time_dim=1)\n\n        # Optionally plot various simulation information.\n        if plot:\n            image = batch[""image""].view(28, 28)\n\n            inpt = inputs[""X""].view(time, 784).sum(0).view(28, 28)\n            weights1 = conv_conn.w\n            _spikes = {\n                ""X"": spikes[""X""].get(""s"").view(time, -1),\n                ""Y"": spikes[""Y""].get(""s"").view(time, -1),\n            }\n            _voltages = {""Y"": voltages[""Y""].get(""v"").view(time, -1)}\n\n            inpt_axes, inpt_ims = plot_input(\n                image, inpt, label=label, axes=inpt_axes, ims=inpt_ims\n            )\n            spike_ims, spike_axes = plot_spikes(_spikes, ims=spike_ims, axes=spike_axes)\n            weights1_im = plot_conv2d_weights(weights1, im=weights1_im)\n            voltage_ims, voltage_axes = plot_voltages(\n                _voltages, ims=voltage_ims, axes=voltage_axes\n            )\n\n            plt.pause(1)\n\n        network.reset_state_variables()  # Reset state variables.\n\nprint(""Progress: %d / %d (%.4f seconds)\\n"" % (n_epochs, n_epochs, t() - start))\nprint(""Training complete.\\n"")\n'"
examples/mnist/eth_mnist.py,11,"b'import os\nimport torch\nimport argparse\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torchvision import transforms\nfrom tqdm import tqdm\n\nfrom time import time as t\n\nfrom bindsnet.datasets import MNIST\nfrom bindsnet.encoding import PoissonEncoder\nfrom bindsnet.models import DiehlAndCook2015\nfrom bindsnet.network.monitors import Monitor\nfrom bindsnet.utils import get_square_weights, get_square_assignments\nfrom bindsnet.evaluation import all_activity, proportion_weighting, assign_labels\nfrom bindsnet.analysis.plotting import (\n    plot_input,\n    plot_spikes,\n    plot_weights,\n    plot_assignments,\n    plot_performance,\n    plot_voltages,\n)\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--seed"", type=int, default=0)\nparser.add_argument(""--n_neurons"", type=int, default=100)\nparser.add_argument(""--n_epochs"", type=int, default=1)\nparser.add_argument(""--n_test"", type=int, default=10000)\nparser.add_argument(""--n_workers"", type=int, default=-1)\nparser.add_argument(""--exc"", type=float, default=22.5)\nparser.add_argument(""--inh"", type=float, default=120)\nparser.add_argument(""--theta_plus"", type=float, default=0.05)\nparser.add_argument(""--time"", type=int, default=250)\nparser.add_argument(""--dt"", type=int, default=1.0)\nparser.add_argument(""--intensity"", type=float, default=128)\nparser.add_argument(""--progress_interval"", type=int, default=10)\nparser.add_argument(""--update_interval"", type=int, default=250)\nparser.add_argument(""--train"", dest=""train"", action=""store_true"")\nparser.add_argument(""--test"", dest=""train"", action=""store_false"")\nparser.add_argument(""--plot"", dest=""plot"", action=""store_true"")\nparser.add_argument(""--gpu"", dest=""gpu"", action=""store_true"")\nparser.set_defaults(plot=False, gpu=False, train=True)\n\nargs = parser.parse_args()\n\nseed = args.seed\nn_neurons = args.n_neurons\nn_epochs = args.n_epochs\nn_test = args.n_test\nn_workers = args.n_workers\nexc = args.exc\ninh = args.inh\ntheta_plus = args.theta_plus\ntime = args.time\ndt = args.dt\nintensity = args.intensity\nprogress_interval = args.progress_interval\nupdate_interval = args.update_interval\ntrain = args.train\nplot = args.plot\ngpu = args.gpu\n\n# Sets up Gpu use\nif gpu:\n    torch.cuda.manual_seed_all(seed)\nelse:\n    torch.manual_seed(seed)\n\n# Determines number of workers to use\nif n_workers == -1:\n    n_workers = gpu * 4 * torch.cuda.device_count()\n\nif not train:\n    update_interval = n_test\n\nn_sqrt = int(np.ceil(np.sqrt(n_neurons)))\nstart_intensity = intensity\n\n# Build network.\nnetwork = DiehlAndCook2015(\n    n_inpt=784,\n    n_neurons=n_neurons,\n    exc=exc,\n    inh=inh,\n    dt=dt,\n    norm=78.4,\n    theta_plus=theta_plus,\n    inpt_shape=(1, 28, 28),\n)\n\n# Directs network to GPU\nif gpu:\n    network.to(""cuda"")\n\n# Load MNIST data.\ndataset = MNIST(\n    PoissonEncoder(time=time, dt=dt),\n    None,\n    root=os.path.join("".."", "".."", ""data"", ""MNIST""),\n    download=True,\n    transform=transforms.Compose(\n        [transforms.ToTensor(), transforms.Lambda(lambda x: x * intensity)]\n    ),\n)\n\n# Record spikes during the simulation.\nspike_record = torch.zeros(update_interval, time, n_neurons)\n\n# Neuron assignments and spike proportions.\nn_classes = 10\nassignments = -torch.ones(n_neurons)\nproportions = torch.zeros(n_neurons, n_classes)\nrates = torch.zeros(n_neurons, n_classes)\n\n# Sequence of accuracy estimates.\naccuracy = {""all"": [], ""proportion"": []}\n\n# Voltage recording for excitatory and inhibitory layers.\nexc_voltage_monitor = Monitor(network.layers[""Ae""], [""v""], time=time)\ninh_voltage_monitor = Monitor(network.layers[""Ai""], [""v""], time=time)\nnetwork.add_monitor(exc_voltage_monitor, name=""exc_voltage"")\nnetwork.add_monitor(inh_voltage_monitor, name=""inh_voltage"")\n\n# Set up monitors for spikes and voltages\nspikes = {}\nfor layer in set(network.layers):\n    spikes[layer] = Monitor(network.layers[layer], state_vars=[""s""], time=time)\n    network.add_monitor(spikes[layer], name=""%s_spikes"" % layer)\n\nvoltages = {}\nfor layer in set(network.layers) - {""X""}:\n    voltages[layer] = Monitor(network.layers[layer], state_vars=[""v""], time=time)\n    network.add_monitor(voltages[layer], name=""%s_voltages"" % layer)\n\ninpt_ims, inpt_axes = None, None\nspike_ims, spike_axes = None, None\nweights_im = None\nassigns_im = None\nperf_ax = None\nvoltage_axes, voltage_ims = None, None\n\n# Train the network.\nprint(""\\nBegin training.\\n"")\nstart = t()\n\nfor epoch in range(n_epochs):\n    labels = []\n\n    if epoch % progress_interval == 0:\n        print(""Progress: %d / %d (%.4f seconds)"" % (epoch, n_epochs, t() - start))\n        start = t()\n\n    # Create a dataloader to iterate and batch data\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=1, shuffle=True, num_workers=n_workers, pin_memory=gpu\n    )\n\n    for step, batch in enumerate(tqdm(dataloader)):\n        # Get next input sample.\n        inputs = {""X"": batch[""encoded_image""].view(time, 1, 1, 28, 28)}\n        if gpu:\n            inputs = {k: v.cuda() for k, v in inputs.items()}\n\n        if step % update_interval == 0 and step > 0:\n            # Convert the array of labels into a tensor\n            label_tensor = torch.tensor(labels)\n\n            # Get network predictions.\n            all_activity_pred = all_activity(\n                spikes=spike_record, assignments=assignments, n_labels=n_classes\n            )\n            proportion_pred = proportion_weighting(\n                spikes=spike_record,\n                assignments=assignments,\n                proportions=proportions,\n                n_labels=n_classes,\n            )\n\n            # Compute network accuracy according to available classification strategies.\n            accuracy[""all""].append(\n                100\n                * torch.sum(label_tensor.long() == all_activity_pred).item()\n                / len(label_tensor)\n            )\n            accuracy[""proportion""].append(\n                100\n                * torch.sum(label_tensor.long() == proportion_pred).item()\n                / len(label_tensor)\n            )\n\n            print(\n                ""\\nAll activity accuracy: %.2f (last), %.2f (average), %.2f (best)""\n                % (\n                    accuracy[""all""][-1],\n                    np.mean(accuracy[""all""]),\n                    np.max(accuracy[""all""]),\n                )\n            )\n            print(\n                ""Proportion weighting accuracy: %.2f (last), %.2f (average), %.2f (best)\\n""\n                % (\n                    accuracy[""proportion""][-1],\n                    np.mean(accuracy[""proportion""]),\n                    np.max(accuracy[""proportion""]),\n                )\n            )\n\n            # Assign labels to excitatory layer neurons.\n            assignments, proportions, rates = assign_labels(\n                spikes=spike_record,\n                labels=label_tensor,\n                n_labels=n_classes,\n                rates=rates,\n            )\n\n            labels = []\n\n        labels.append(batch[""label""])\n\n        # Run the network on the input.\n        network.run(inputs=inputs, time=time, input_time_dim=1)\n\n        # Get voltage recording.\n        exc_voltages = exc_voltage_monitor.get(""v"")\n        inh_voltages = inh_voltage_monitor.get(""v"")\n\n        # Add to spikes recording.\n        spike_record[step % update_interval] = spikes[""Ae""].get(""s"").squeeze()\n\n        # Optionally plot various simulation information.\n        if plot:\n            image = batch[""image""].view(28, 28)\n            inpt = inputs[""X""].view(time, 784).sum(0).view(28, 28)\n            input_exc_weights = network.connections[(""X"", ""Ae"")].w\n            square_weights = get_square_weights(\n                input_exc_weights.view(784, n_neurons), n_sqrt, 28\n            )\n            square_assignments = get_square_assignments(assignments, n_sqrt)\n            spikes_ = {layer: spikes[layer].get(""s"") for layer in spikes}\n            voltages = {""Ae"": exc_voltages, ""Ai"": inh_voltages}\n            inpt_axes, inpt_ims = plot_input(\n                image, inpt, label=batch[""label""], axes=inpt_axes, ims=inpt_ims\n            )\n            spike_ims, spike_axes = plot_spikes(spikes_, ims=spike_ims, axes=spike_axes)\n            weights_im = plot_weights(square_weights, im=weights_im)\n            assigns_im = plot_assignments(square_assignments, im=assigns_im)\n            perf_ax = plot_performance(accuracy, x_scale=update_interval, ax=perf_ax)\n            voltage_ims, voltage_axes = plot_voltages(\n                voltages, ims=voltage_ims, axes=voltage_axes, plot_type=""line""\n            )\n\n            plt.pause(1e-8)\n\n        network.reset_state_variables()  # Reset state variables.\n\nprint(""Progress: %d / %d (%.4f seconds)"" % (epoch + 1, n_epochs, t() - start))\nprint(""Training complete.\\n"")\n'"
examples/mnist/reservoir.py,16,"b'import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport argparse\nimport matplotlib.pyplot as plt\n\nfrom torchvision import transforms\nfrom tqdm import tqdm\n\nfrom bindsnet.analysis.plotting import (\n    plot_input,\n    plot_spikes,\n    plot_voltages,\n    plot_weights,\n)\nfrom bindsnet.datasets import MNIST\nfrom bindsnet.encoding import PoissonEncoder\nfrom bindsnet.network import Network\nfrom bindsnet.network.nodes import Input\n\n# Build a simple two-layer, input-output network.\nfrom bindsnet.network.monitors import Monitor\nfrom bindsnet.network.nodes import LIFNodes\nfrom bindsnet.network.topology import Connection\nfrom bindsnet.utils import get_square_weights\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--seed"", type=int, default=0)\nparser.add_argument(""--n_neurons"", type=int, default=500)\nparser.add_argument(""--n_epochs"", type=int, default=100)\nparser.add_argument(""--examples"", type=int, default=500)\nparser.add_argument(""--n_workers"", type=int, default=-1)\nparser.add_argument(""--time"", type=int, default=250)\nparser.add_argument(""--dt"", type=int, default=1.0)\nparser.add_argument(""--intensity"", type=float, default=64)\nparser.add_argument(""--progress_interval"", type=int, default=10)\nparser.add_argument(""--update_interval"", type=int, default=250)\nparser.add_argument(""--plot"", dest=""plot"", action=""store_true"")\nparser.add_argument(""--gpu"", dest=""gpu"", action=""store_true"")\nparser.add_argument(""--device_id"", type=int, default=0)\nparser.set_defaults(plot=True, gpu=True, train=True)\n\nargs = parser.parse_args()\n\nseed = args.seed\nn_neurons = args.n_neurons\nn_epochs = args.n_epochs\nexamples = args.examples\nn_workers = args.n_workers\ntime = args.time\ndt = args.dt\nintensity = args.intensity\nprogress_interval = args.progress_interval\nupdate_interval = args.update_interval\ntrain = args.train\nplot = args.plot\ngpu = args.gpu\ndevice_id = args.device_id\n\nnp.random.seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.manual_seed(seed)\n\n# Sets up Gpu use\nif gpu and torch.cuda.is_available():\n    torch.cuda.set_device(device_id)\n    # torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\nelse:\n    torch.manual_seed(seed)\n\n\nnetwork = Network(dt=dt)\ninpt = Input(784, shape=(1, 28, 28))\nnetwork.add_layer(inpt, name=""I"")\noutput = LIFNodes(n_neurons, thresh=-52 + np.random.randn(n_neurons).astype(float))\nnetwork.add_layer(output, name=""O"")\nC1 = Connection(source=inpt, target=output, w=0.5 * torch.randn(inpt.n, output.n))\nC2 = Connection(source=output, target=output, w=0.5 * torch.randn(output.n, output.n))\n\nnetwork.add_connection(C1, source=""I"", target=""O"")\nnetwork.add_connection(C2, source=""O"", target=""O"")\n\nspikes = {}\nfor l in network.layers:\n    spikes[l] = Monitor(network.layers[l], [""s""], time=time)\n    network.add_monitor(spikes[l], name=""%s_spikes"" % l)\n\nvoltages = {""O"": Monitor(network.layers[""O""], [""v""], time=time)}\nnetwork.add_monitor(voltages[""O""], name=""O_voltages"")\n\n# Directs network to GPU\nif gpu:\n    network.to(""cuda"")\n\n# Get MNIST training images and labels.\n# Load MNIST data.\ndataset = MNIST(\n    PoissonEncoder(time=time, dt=dt),\n    None,\n    root=os.path.join("".."", "".."", ""data"", ""MNIST""),\n    download=True,\n    transform=transforms.Compose(\n        [transforms.ToTensor(), transforms.Lambda(lambda x: x * intensity)]\n    ),\n)\n\ninpt_axes = None\ninpt_ims = None\nspike_axes = None\nspike_ims = None\nweights_im = None\nweights_im2 = None\nvoltage_ims = None\nvoltage_axes = None\n\n# Create a dataloader to iterate and batch data\ndataloader = torch.utils.data.DataLoader(\n    dataset, batch_size=1, shuffle=True, num_workers=0, pin_memory=gpu\n)\n\n# Run training data on reservoir computer and store (spikes per neuron, label) per example.\nn_iters = examples\ntraining_pairs = []\npbar = tqdm(enumerate(dataloader))\nfor (i, dataPoint) in pbar:\n    if i > n_iters:\n        break\n    datum = dataPoint[""encoded_image""].view(time, 1, 1, 28, 28).to(device_id)\n    label = dataPoint[""label""]\n    pbar.set_description_str(""Train progress: (%d / %d)"" % (i, n_iters))\n\n    network.run(inputs={""I"": datum}, time=time, input_time_dim=1)\n    training_pairs.append([spikes[""O""].get(""s"").sum(0), label])\n\n    if plot:\n\n        inpt_axes, inpt_ims = plot_input(\n            dataPoint[""image""].view(28, 28),\n            datum.view(time, 784).sum(0).view(28, 28),\n            label=label,\n            axes=inpt_axes,\n            ims=inpt_ims,\n        )\n        spike_ims, spike_axes = plot_spikes(\n            {layer: spikes[layer].get(""s"").view(-1, time) for layer in spikes},\n            axes=spike_axes,\n            ims=spike_ims,\n        )\n        voltage_ims, voltage_axes = plot_voltages(\n            {layer: voltages[layer].get(""v"").view(-1, time) for layer in voltages},\n            ims=voltage_ims,\n            axes=voltage_axes,\n        )\n        weights_im = plot_weights(\n            get_square_weights(C1.w, 23, 28), im=weights_im, wmin=-2, wmax=2\n        )\n        weights_im2 = plot_weights(C2.w, im=weights_im2, wmin=-2, wmax=2)\n\n        plt.pause(1e-8)\n    network.reset_state_variables()\n\n\n# Define logistic regression model using PyTorch.\nclass NN(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(NN, self).__init__()\n        # h = int(input_size/2)\n        self.linear_1 = nn.Linear(input_size, num_classes)\n        # self.linear_1 = nn.Linear(input_size, h)\n        # self.linear_2 = nn.Linear(h, num_classes)\n\n    def forward(self, x):\n        out = torch.sigmoid(self.linear_1(x.float().view(-1)))\n        # out = torch.sigmoid(self.linear_2(out))\n        return out\n\n\n# Create and train logistic regression model on reservoir outputs.\nmodel = NN(n_neurons, 10).to(device_id)\ncriterion = torch.nn.MSELoss(reduction=""sum"")\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n\n# Training the Model\nprint(""\\n Training the read out"")\npbar = tqdm(enumerate(range(n_epochs)))\nfor epoch, _ in pbar:\n    avg_loss = 0\n    for i, (s, l) in enumerate(training_pairs):\n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs = model(s)\n        label = torch.zeros(1, 1, 10).float().to(device_id)\n        label[0, 0, l] = 1.0\n        loss = criterion(outputs.view(1, 1, -1), label)\n        avg_loss += loss.data\n        loss.backward()\n        optimizer.step()\n\n    pbar.set_description_str(\n        ""Epoch: %d/%d, Loss: %.4f""\n        % (epoch + 1, n_epochs, avg_loss / len(training_pairs))\n    )\n\nn_iters = examples\ntest_pairs = []\npbar = tqdm(enumerate(dataloader))\nfor (i, dataPoint) in pbar:\n    if i > n_iters:\n        break\n    datum = dataPoint[""encoded_image""].view(time, 1, 1, 28, 28).to(device_id)\n    label = dataPoint[""label""]\n    pbar.set_description_str(""Testing progress: (%d / %d)"" % (i, n_iters))\n\n    network.run(inputs={""I"": datum}, time=250, input_time_dim=1)\n    test_pairs.append([spikes[""O""].get(""s"").sum(0), label])\n\n    if plot:\n        inpt_axes, inpt_ims = plot_input(\n            dataPoint[""image""].view(28, 28),\n            datum.view(time, 784).sum(0).view(28, 28),\n            label=label,\n            axes=inpt_axes,\n            ims=inpt_ims,\n        )\n        spike_ims, spike_axes = plot_spikes(\n            {layer: spikes[layer].get(""s"").view(-1, 250) for layer in spikes},\n            axes=spike_axes,\n            ims=spike_ims,\n        )\n        voltage_ims, voltage_axes = plot_voltages(\n            {layer: voltages[layer].get(""v"").view(-1, 250) for layer in voltages},\n            ims=voltage_ims,\n            axes=voltage_axes,\n        )\n        weights_im = plot_weights(\n            get_square_weights(C1.w, 23, 28), im=weights_im, wmin=-2, wmax=2\n        )\n        weights_im2 = plot_weights(C2.w, im=weights_im2, wmin=-2, wmax=2)\n\n        plt.pause(1e-8)\n    network.reset_state_variables()\n\n# Test the Model\ncorrect, total = 0, 0\nfor s, label in test_pairs:\n    outputs = model(s)\n    _, predicted = torch.max(outputs.data.unsqueeze(0), 1)\n    total += 1\n    correct += int(predicted == label.long().to(device_id))\n\nprint(\n    ""\\n Accuracy of the model on %d test images: %.2f %%""\n    % (n_iters, 100 * correct / total)\n)\n'"
examples/mnist/supervised_mnist.py,15,"b'import os\nimport torch\nimport numpy as np\nimport argparse\nimport matplotlib.pyplot as plt\n\nfrom torchvision import transforms\nfrom tqdm import tqdm\n\n\nfrom bindsnet.datasets import MNIST\nfrom bindsnet.encoding import PoissonEncoder\nfrom bindsnet.models import DiehlAndCook2015\nfrom bindsnet.network.monitors import Monitor\nfrom bindsnet.utils import get_square_assignments, get_square_weights\nfrom bindsnet.evaluation import all_activity, proportion_weighting, assign_labels\nfrom bindsnet.analysis.plotting import (\n    plot_input,\n    plot_assignments,\n    plot_performance,\n    plot_weights,\n    plot_spikes,\n    plot_voltages,\n)\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--seed"", type=int, default=0)\nparser.add_argument(""--n_neurons"", type=int, default=100)\nparser.add_argument(""--n_train"", type=int, default=5000)\nparser.add_argument(""--n_test"", type=int, default=1000)\nparser.add_argument(""--n_clamp"", type=int, default=1)\nparser.add_argument(""--exc"", type=float, default=22.5)\nparser.add_argument(""--inh"", type=float, default=120)\nparser.add_argument(""--theta_plus"", type=float, default=0.05)\nparser.add_argument(""--time"", type=int, default=250)\nparser.add_argument(""--dt"", type=int, default=1.0)\nparser.add_argument(""--intensity"", type=float, default=32)\nparser.add_argument(""--progress_interval"", type=int, default=10)\nparser.add_argument(""--update_interval"", type=int, default=250)\nparser.add_argument(""--train"", dest=""train"", action=""store_true"")\nparser.add_argument(""--test"", dest=""train"", action=""store_false"")\nparser.add_argument(""--plot"", dest=""plot"", action=""store_true"")\nparser.add_argument(""--gpu"", dest=""gpu"", action=""store_true"")\nparser.add_argument(""--device_id"", type=int, default=0)\nparser.set_defaults(plot=False, gpu=False, train=True)\n\nargs = parser.parse_args()\n\nseed = args.seed\nn_neurons = args.n_neurons\nn_train = args.n_train\nn_test = args.n_test\nn_clamp = args.n_clamp\nexc = args.exc\ninh = args.inh\ntheta_plus = args.theta_plus\ntime = args.time\ndt = args.dt\nintensity = args.intensity\nprogress_interval = args.progress_interval\nupdate_interval = args.update_interval\ntrain = args.train\nplot = args.plot\ngpu = args.gpu\ndevice_id = args.device_id\n\n# Sets up Gpu use\nif gpu and torch.cuda.is_available():\n    # torch.set_default_tensor_type(""torch.cuda.FloatTensor"")\n    torch.cuda.set_device(device_id)\n    torch.cuda.manual_seed_all(seed)\nelse:\n    torch.manual_seed(seed)\n\nif not train:\n    update_interval = n_test\n\nn_sqrt = int(np.ceil(np.sqrt(n_neurons)))\nstart_intensity = intensity\nper_class = int(n_neurons / 10)\n\n# Build Diehl & Cook 2015 network.\nnetwork = DiehlAndCook2015(\n    n_inpt=784,\n    n_neurons=n_neurons,\n    exc=exc,\n    inh=inh,\n    dt=dt,\n    nu=[1e-2, 1e-4],\n    norm=78.4,\n    theta_plus=theta_plus,\n    inpt_shape=(1, 28, 28),\n)\n\n# Directs network to GPU\nif gpu:\n    network.to(""cuda"")\n\n# Voltage recording for excitatory and inhibitory layers.\nexc_voltage_monitor = Monitor(network.layers[""Ae""], [""v""], time=time)\ninh_voltage_monitor = Monitor(network.layers[""Ai""], [""v""], time=time)\nnetwork.add_monitor(exc_voltage_monitor, name=""exc_voltage"")\nnetwork.add_monitor(inh_voltage_monitor, name=""inh_voltage"")\n\n# Load MNIST data.\ndataset = MNIST(\n    PoissonEncoder(time=time, dt=dt),\n    None,\n    root=os.path.join("".."", "".."", ""data"", ""MNIST""),\n    download=True,\n    transform=transforms.Compose(\n        [transforms.ToTensor(), transforms.Lambda(lambda x: x * intensity)]\n    ),\n)\n\n# Create a dataloader to iterate and batch data\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n\n# Record spikes during the simulation.\nspike_record = torch.zeros(update_interval, time, n_neurons)\n\n# Neuron assignments and spike proportions.\nassignments = -torch.ones_like(torch.Tensor(n_neurons))\nproportions = torch.zeros_like(torch.Tensor(n_neurons, 10))\nrates = torch.zeros_like(torch.Tensor(n_neurons, 10))\n\n# Sequence of accuracy estimates.\naccuracy = {""all"": [], ""proportion"": []}\n\n# Labels to determine neuron assignments and spike proportions and estimate accuracy\nlabels = torch.empty(update_interval)\n\nspikes = {}\nfor layer in set(network.layers):\n    spikes[layer] = Monitor(network.layers[layer], state_vars=[""s""], time=time)\n    network.add_monitor(spikes[layer], name=""%s_spikes"" % layer)\n\n# Train the network.\nprint(""Begin training.\\n"")\n\ninpt_axes = None\ninpt_ims = None\nspike_axes = None\nspike_ims = None\nweights_im = None\nassigns_im = None\nperf_ax = None\nvoltage_axes = None\nvoltage_ims = None\n\npbar = tqdm(total=n_train)\nfor (i, datum) in enumerate(dataloader):\n    if i > n_train:\n        break\n\n    image = datum[""encoded_image""]\n    label = datum[""label""]\n\n    if i % update_interval == 0 and i > 0:\n        # Get network predictions.\n        all_activity_pred = all_activity(spike_record, assignments, 10)\n        proportion_pred = proportion_weighting(\n            spike_record, assignments, proportions, 10\n        )\n\n        # Compute network accuracy according to available classification strategies.\n        accuracy[""all""].append(\n            100 * torch.sum(labels.long() == all_activity_pred).item() / update_interval\n        )\n        accuracy[""proportion""].append(\n            100 * torch.sum(labels.long() == proportion_pred).item() / update_interval\n        )\n\n        print(\n            ""\\nAll activity accuracy: %.2f (last), %.2f (average), %.2f (best)""\n            % (accuracy[""all""][-1], np.mean(accuracy[""all""]), np.max(accuracy[""all""]))\n        )\n        print(\n            ""Proportion weighting accuracy: %.2f (last), %.2f (average), %.2f (best)\\n""\n            % (\n                accuracy[""proportion""][-1],\n                np.mean(accuracy[""proportion""]),\n                np.max(accuracy[""proportion""]),\n            )\n        )\n\n        # Assign labels to excitatory layer neurons.\n        assignments, proportions, rates = assign_labels(spike_record, labels, 10, rates)\n\n    # Add the current label to the list of labels for this update_interval\n    labels[i % update_interval] = label[0]\n\n    # Run the network on the input.\n    choice = np.random.choice(int(n_neurons / 10), size=n_clamp, replace=False)\n    clamp = {""Ae"": per_class * label.long() + torch.Tensor(choice).long()}\n    if gpu:\n        inputs = {""X"": image.cuda().view(time, 1, 1, 28, 28)}\n    else:\n        inputs = {""X"": image.view(time, 1, 1, 28, 28)}\n    network.run(inputs=inputs, time=time, clamp=clamp)\n\n    # Get voltage recording.\n    exc_voltages = exc_voltage_monitor.get(""v"")\n    inh_voltages = inh_voltage_monitor.get(""v"")\n\n    # Add to spikes recording.\n    spike_record[i % update_interval] = spikes[""Ae""].get(""s"").view(time, n_neurons)\n\n    # Optionally plot various simulation information.\n    if plot:\n        inpt = inputs[""X""].view(time, 784).sum(0).view(28, 28)\n        input_exc_weights = network.connections[(""X"", ""Ae"")].w\n        square_weights = get_square_weights(\n            input_exc_weights.view(784, n_neurons), n_sqrt, 28\n        )\n        square_assignments = get_square_assignments(assignments, n_sqrt)\n        voltages = {""Ae"": exc_voltages, ""Ai"": inh_voltages}\n\n        inpt_axes, inpt_ims = plot_input(\n            image.sum(1).view(28, 28), inpt, label=label, axes=inpt_axes, ims=inpt_ims\n        )\n        spike_ims, spike_axes = plot_spikes(\n            {layer: spikes[layer].get(""s"").view(time, 1, -1) for layer in spikes},\n            ims=spike_ims,\n            axes=spike_axes,\n        )\n        weights_im = plot_weights(square_weights, im=weights_im)\n        assigns_im = plot_assignments(square_assignments, im=assigns_im)\n        perf_ax = plot_performance(accuracy, ax=perf_ax)\n        voltage_ims, voltage_axes = plot_voltages(\n            voltages, ims=voltage_ims, axes=voltage_axes\n        )\n\n        plt.pause(1e-8)\n\n    network.reset_state_variables()  # Reset state variables.\n    pbar.set_description_str(""Train progress: "")\n    pbar.update()\n\nprint(""Progress: %d / %d \\n"" % (n_train, n_train))\nprint(""Training complete.\\n"")\n\n\nprint(""Testing....\\n"")\n\nhit = 0\npbar = tqdm(total=n_test)\nfor (i, datum) in enumerate(dataloader):\n    if i > n_test:\n        break\n\n    image = datum[""encoded_image""]\n    label = datum[""label""]\n    if gpu:\n        inputs = {""X"": image.cuda().view(time, 1, 1, 28, 28)}\n    else:\n        inputs = {""X"": image.view(time, 1, 1, 28, 28)}\n\n    network.run(inputs=inputs, time=time)\n\n    out_spikes = network.monitors[""Ae_spikes""].get(""s"")\n\n    out_spikes = out_spikes.squeeze().sum(dim=0)\n    class_spike = torch.zeros(10)\n    for c in range(10):\n        class_spike[c] = out_spikes[i * 10 : (c + 1) * 10].sum()\n    maxInd = class_spike.argmax()\n    if maxInd == label[0]:\n        hit += 1\n\n    pbar.set_description_str(f""Accuracy: {hit / (i+1)}"")\n    pbar.update()\n\nacc = hit / n_test\nprint(""\\n accuracy: "" + str(acc) + ""\\n"")\n'"
examples/tensorboard/tensorboard.py,2,"b'import os\nimport argparse\n\nimport torch\nfrom torchvision import transforms\n\nfrom time import time as t\nfrom tqdm import tqdm\n\nimport bindsnet.datasets\nfrom bindsnet.encoding import PoissonEncoder, NullEncoder\n\nfrom bindsnet.network import Network\nfrom bindsnet.learning import PostPre\nfrom bindsnet.network.nodes import LIFNodes, Input\nfrom bindsnet.network.topology import Conv2dConnection, Connection\nfrom bindsnet.analysis.pipeline_analysis import TensorboardAnalyzer, MatplotlibAnalyzer\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    ""--dataset"",\n    type=str,\n    default=""MNIST"",\n    choices=[""MNIST"", ""KMNIST"", ""FashionMNIST"", ""CIFAR10"", ""CIFAR100""],\n)\nparser.add_argument(""--seed"", type=int, default=0)\nparser.add_argument(""--time"", type=int, default=50)\nparser.add_argument(""--dt"", type=int, default=1.0)\nparser.add_argument(""--tensorboard"", dest=""tensorboard"", action=""store_true"")\nparser.set_defaults(plot=False, gpu=False, train=True)\n\nargs = parser.parse_args()\n\nseed = args.seed\ntorch.manual_seed(seed)\n\n# Encoding parameters\ntime = args.time\ndt = args.dt\n\n# Convolution parameters\nkernel_size = 5\nstride = 2\nn_filters = 5\npadding = 0\n\n# Create the datasets and loaders\n# This is dynamic so you can test each dataset easily\ndataset_type = getattr(bindsnet.datasets, args.dataset)\ndataset_path = os.path.join("".."", "".."", ""data"", args.dataset)\ntrain_dataset = dataset_type(\n    PoissonEncoder(time=time, dt=dt),\n    NullEncoder(),\n    dataset_path,\n    download=True,\n    train=True,\n    transform=transforms.Compose(\n        [transforms.ToTensor(), transforms.Lambda(lambda x: x * 128.0)]\n    ),\n)\n\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=1, shuffle=True, num_workers=0\n)\n\n# Grab the shape of a single sample (not including batch)\n# So, TxCxHxW\nsample_shape = train_dataset[0][""encoded_image""].shape\nprint(args.dataset, "" has shape "", sample_shape)\n\nconv_size = int((sample_shape[-1] - kernel_size + 2 * padding) / stride) + 1\nper_class = int((n_filters * conv_size * conv_size) / 10)\n\n# Build a small convolutional network\nnetwork = Network()\n\n# Make sure to include the batch dimension but not time\ninput_layer = Input(shape=(sample_shape[1:]), traces=True)\n\nconv_layer = LIFNodes(\n    n=n_filters * conv_size * conv_size,\n    shape=(n_filters, conv_size, conv_size),\n    traces=True,\n)\n\nconv_conn = Conv2dConnection(\n    input_layer,\n    conv_layer,\n    kernel_size=kernel_size,\n    stride=stride,\n    update_rule=PostPre,\n    norm=0.4 * kernel_size ** 2,\n    nu=[1e-4, 1e-2],\n    wmax=1.0,\n)\n\n\nnetwork.add_layer(input_layer, name=""X"")\nnetwork.add_layer(conv_layer, name=""Y"")\nnetwork.add_connection(conv_conn, source=""X"", target=""Y"")\n\n# Train the network.\nprint(""Begin training.\\n"")\n\nif args.tensorboard:\n    analyzer = TensorboardAnalyzer(""logs/conv"")\nelse:\n    analyzer = MatplotlibAnalyzer()\n\nfor step, batch in enumerate(tqdm(train_dataloader)):\n    # batch contains image, label, encoded_image since an image_encoder\n    # was provided\n\n    # batch[""encoded_image""] is in BxTxCxHxW format\n    inputs = {""X"": batch[""encoded_image""].view(time, 1, 1, 28, 28)}\n\n    # Run the network on the input.\n    # Specify the location of the time dimension\n    network.run(inputs=inputs, time=time, input_time_dim=1)\n\n    network.reset_state_variables()  # Reset state variables.\n\n    analyzer.plot_conv2d_weights(conv_conn.w, step=step)\n\n    analyzer.finalize_step()\n'"
test/analysis/test_analyzers.py,5,"b'import torch\n\nfrom bindsnet.analysis.pipeline_analysis import TensorboardAnalyzer, MatplotlibAnalyzer\n\nimport matplotlib.pyplot as plt\nimport os\n\n\nclass TestAnalyzer:\n    """"""\n    Sanity checks all plotting functions for analyzers\n    """"""\n\n    def test_init(self):\n        ma = MatplotlibAnalyzer()\n        assert plt.isinteractive()\n\n        ta = TensorboardAnalyzer(""./logs/init"")\n\n        # check to ensure path was written\n        assert os.path.isdir(""./logs/init"")\n\n        # check to ensure we can write data\n        ta.writer.add_scalar(""init_scalar"", 100.0, 0)\n        ta.writer.close()\n\n    def test_plot_runs(self):\n        ma = MatplotlibAnalyzer()\n        ta = TensorboardAnalyzer(""./logs/runs"")\n\n        for analyzer in [ma, ta]:\n            obs = torch.rand(1, 28, 28)\n            analyzer.plot_obs(obs)\n\n            # 4 channels out, 1 channel in, 8x8 kernels\n            conv_weights = torch.rand(4, 1, 8, 8)\n            analyzer.plot_conv2d_weights(conv_weights)\n\n            rewards = [0, 0, 0, 0, 0]\n            analyzer.plot_reward(rewards)\n\n            # Monitors have time as last dimension\n            v = torch.rand(50, 1, 1, 28, 28)\n            voltage_dict = {""X"": v}\n            threshold_dict = {""X"": torch.tensor(0.75)}\n            analyzer.plot_voltages(voltage_dict, threshold_dict)\n\n            # The monitors have time as last dimension\n            spikes = torch.rand(50, 1, 1, 28, 28) > 0.5\n            spike_dict = {""X"": spikes}\n            analyzer.plot_spikes(spike_dict)\n\n            analyzer.finalize_step()\n\n        ta.writer.close()\n\n\nif __name__ == ""__main__"":\n    tester = TestAnalyzer()\n\n    tester.test_init()\n    tester.test_plot_runs()\n'"
test/conversion/test_conversion.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\n\nfrom bindsnet.conversion import ann_to_snn\n\n\nclass FullyConnectedNetwork(nn.Module):\n    # language=rst\n    """"""\n    Simply fully-connected network implemented in PyTorch.\n    """"""\n\n    def __init__(self):\n        super(FullyConnectedNetwork, self).__init__()\n\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\ndef test_conversion():\n    ann = FullyConnectedNetwork()\n    snn = ann_to_snn(ann, input_shape=(784,))\n\n\ndef main():\n    ann = FullyConnectedNetwork()\n    return ann_to_snn(ann, input_shape=(28, 28))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
test/encoding/test_encoding.py,10,"b'from bindsnet.encoding import *\n\nimport torch\n\n\nclass TestEncodings:\n    """"""\n    Tests all stable encoding functions and generators.\n    """"""\n\n    def test_bernoulli(self):\n        for n in [1, 100]:  # number of nodes in layer\n            for t in [1, 100]:  # number of timesteps\n                for m in [0.1, 1.0]:  # maximum spiking probability\n                    datum = torch.empty(n).uniform_(0, m)\n                    spikes = bernoulli(datum, time=t, max_prob=m)\n\n                    assert spikes.size() == torch.Size((t, n))\n\n    def test_multidim_bernoulli(self):\n        for shape in [[5, 5], [10, 10], [25, 25]]:  # shape of nodes in layer\n            for t in [1, 100]:  # number of timesteps\n                for m in [0.1, 1.0]:  # maximum spiking probability\n                    datum = torch.empty(shape).uniform_(0, m)\n                    spikes = bernoulli(datum, time=t, max_prob=m)\n\n                    assert spikes.size() == torch.Size((t, *shape))\n\n    def test_bernoulli_loader(self):\n        for s in [1, 100]:  # number of data samples\n            for n in [1, 100]:  # number of nodes in layer\n                for m in [0.1, 1.0]:  # maximum spiking probability\n                    for t in [1, 100]:  # number of timesteps\n                        data = torch.empty(s, n).uniform_(0, 1)\n                        spike_loader = bernoulli_loader(data, time=t, max_prob=m)\n\n                        for i, spikes in enumerate(spike_loader):\n                            assert spikes.size() == torch.Size((t, n))\n\n    def test_poisson(self):\n        for n in [1, 100]:  # number of nodes in layer\n            for t in [1000]:  # number of timesteps\n                datum = torch.empty(n).uniform_(20, 100)  # Generate firing rates.\n                spikes = poisson(datum, time=t)  # Encode as spikes.\n\n                assert spikes.size() == torch.Size((t, n))\n\n    def test_poisson_loader(self):\n        for s in [1, 10]:  # number of data samples\n            for n in [1, 100]:  # number of nodes in layer\n                for t in [1000]:  # number of timesteps\n                    data = torch.empty(s, n).uniform_(20, 100)  # Generate firing rates.\n                    spike_loader = poisson_loader(data, time=t)  # Encode as spikes.\n\n                    for i, spikes in enumerate(spike_loader):\n                        assert spikes.size() == torch.Size((t, n))\n'"
test/import/test_import.py,0,b'import bindsnet\n\nimport bindsnet.analysis\nimport bindsnet.conversion\nimport bindsnet.datasets\nimport bindsnet.encoding\nimport bindsnet.environment\nimport bindsnet.learning\nimport bindsnet.models\nimport bindsnet.network\nimport bindsnet.pipeline\nimport bindsnet.preprocessing\n\nimport bindsnet.utils\n\nfrom bindsnet.analysis import *\nfrom bindsnet.conversion import *\nfrom bindsnet.datasets import *\nfrom bindsnet.encoding import *\nfrom bindsnet.environment import *\nfrom bindsnet.learning import *\nfrom bindsnet.models import *\nfrom bindsnet.network import *\nfrom bindsnet.pipeline import *\nfrom bindsnet.preprocessing import *\n'
test/models/test_models.py,0,"b'from bindsnet.network.topology import Connection\nfrom bindsnet.models import TwoLayerNetwork, DiehlAndCook2015\nfrom bindsnet.network.nodes import Input, LIFNodes, DiehlAndCookNodes\n\n\nclass TestTwoLayerNetwork:\n    def test_init(self):\n        for n_inpt in [50, 100, 200]:\n            for n_neurons in [50, 100, 200]:\n                for dt in [1.0, 2.0]:\n                    network = TwoLayerNetwork(n_inpt, n_neurons=n_neurons, dt=dt)\n\n                    assert network.n_inpt == n_inpt\n                    assert network.n_neurons == n_neurons\n                    assert network.dt == dt\n\n                    assert (\n                        isinstance(network.layers[""X""], Input)\n                        and network.layers[""X""].n == n_inpt\n                    )\n                    assert (\n                        isinstance(network.layers[""Y""], LIFNodes)\n                        and network.layers[""Y""].n == n_neurons\n                    )\n                    assert isinstance(network.connections[(""X"", ""Y"")], Connection)\n                    assert (\n                        network.connections[(""X"", ""Y"")].source.n == n_inpt\n                        and network.connections[(""X"", ""Y"")].target.n == n_neurons\n                    )\n\n\nclass TestDiehlAndCook2015:\n    def test_init(self):\n        for n_inpt in [50, 100, 200]:\n            for n_neurons in [50, 100, 200]:\n                for dt in [1.0, 2.0]:\n                    for exc in [13.3, 14.53]:\n                        for inh in [10.5, 12.2]:\n                            network = DiehlAndCook2015(\n                                n_inpt=n_inpt,\n                                n_neurons=n_neurons,\n                                exc=exc,\n                                inh=inh,\n                                dt=dt,\n                            )\n\n                            assert network.n_inpt == n_inpt\n                            assert network.n_neurons == n_neurons\n                            assert network.dt == dt\n                            assert network.exc == exc\n                            assert network.inh == inh\n\n                            assert (\n                                isinstance(network.layers[""X""], Input)\n                                and network.layers[""X""].n == n_inpt\n                            )\n                            assert (\n                                isinstance(network.layers[""Ae""], DiehlAndCookNodes)\n                                and network.layers[""Ae""].n == n_neurons\n                            )\n                            assert (\n                                isinstance(network.layers[""Ai""], LIFNodes)\n                                and network.layers[""Ae""].n == n_neurons\n                            )\n\n                            for conn in [(""X"", ""Ae""), (""Ae"", ""Ai""), (""Ai"", ""Ae"")]:\n                                assert conn in network.connections\n'"
test/network/test_connections.py,5,"b'import torch\n\nfrom bindsnet.network.nodes import LIFNodes\n\nfrom bindsnet.network.topology import *\n\n\nclass TestConnection:\n    """"""\n    Tests all stable groups of neurons / nodes.\n    """"""\n\n    def test_transfer(self):\n        if not torch.cuda.is_available():\n            return\n\n        connection_types = [\n            Connection,\n            Conv2dConnection,\n            MaxPool2dConnection,\n            LocalConnection,\n            MeanFieldConnection,\n            SparseConnection,\n        ]\n        args = [[], [3], [3], [3, 1, 1], [], []]\n        kwargs = [{}, {}, {}, {}, {}, {""sparsity"": 0.9}]\n        for conn_type, args, kwargs in zip(connection_types, args, kwargs):\n            l_a = LIFNodes(shape=[1, 28, 28])\n            l_b = LIFNodes(shape=[1, 26, 26])\n            connection = conn_type(l_a, l_b, *args, **kwargs)\n\n            connection.to(torch.device(""cuda:0""))\n\n            connection_tensors = [\n                k\n                for k, v in connection.state_dict().items()\n                if isinstance(v, torch.Tensor) and not ""."" in k\n            ]\n\n            print(\n                ""State dict in {} : {}"".format(\n                    conn_type, connection.state_dict().keys()\n                )\n            )\n            print(""__dict__ in {} : {}"".format(conn_type, connection.__dict__.keys()))\n            print(""Tensors in {} : {}"".format(conn_type, connection_tensors))\n\n            tensor_devs = [getattr(connection, k).device for k in connection_tensors]\n            print(\n                ""Tensor devices {}"".format(list(zip(connection_tensors, tensor_devs)))\n            )\n\n            for d in tensor_devs:\n                print(d, d == torch.device(""cuda:0""))\n                assert d == torch.device(""cuda:0"")\n\n\nif __name__ == ""__main__"":\n    tester = TestConnection()\n\n    tester.test_transfer()\n'"
test/network/test_learning.py,11,"b'import torch\n\nfrom bindsnet.network import Network\nfrom bindsnet.network.nodes import Input, LIFNodes, SRM0Nodes\nfrom bindsnet.network.topology import Connection, Conv2dConnection\nfrom bindsnet.learning import (\n    Hebbian,\n    PostPre,\n    WeightDependentPostPre,\n    MSTDP,\n    MSTDPET,\n    Rmax,\n)\n\n\nclass TestLearningRules:\n    """"""\n    Tests all stable learning rules for compatible ``Connection`` types.\n    """"""\n\n    def test_hebbian(self):\n        # Connection test\n        network = Network(dt=1.0)\n        network.add_layer(Input(n=100, traces=True), name=""input"")\n        network.add_layer(LIFNodes(n=100, traces=True), name=""output"")\n        network.add_connection(\n            Connection(\n                source=network.layers[""input""],\n                target=network.layers[""output""],\n                nu=1e-2,\n                update_rule=Hebbian,\n            ),\n            source=""input"",\n            target=""output"",\n        )\n        network.run(\n            inputs={""input"": torch.bernoulli(torch.rand(250, 100)).byte()}, time=250\n        )\n\n        # Conv2dConnection test\n        network = Network(dt=1.0)\n        network.add_layer(Input(shape=[1, 10, 10], traces=True), name=""input"")\n        network.add_layer(LIFNodes(shape=[32, 8, 8], traces=True), name=""output"")\n        network.add_connection(\n            Conv2dConnection(\n                source=network.layers[""input""],\n                target=network.layers[""output""],\n                kernel_size=3,\n                stride=1,\n                nu=1e-2,\n                update_rule=Hebbian,\n            ),\n            source=""input"",\n            target=""output"",\n        )\n        # shape is [time, batch, channels, height, width]\n        network.run(\n            inputs={""input"": torch.bernoulli(torch.rand(250, 1, 1, 10, 10)).byte()},\n            time=250,\n        )\n\n    def test_post_pre(self):\n        # Connection test\n        network = Network(dt=1.0)\n        network.add_layer(Input(n=100, traces=True), name=""input"")\n        network.add_layer(LIFNodes(n=100, traces=True), name=""output"")\n        network.add_connection(\n            Connection(\n                source=network.layers[""input""],\n                target=network.layers[""output""],\n                nu=1e-2,\n                update_rule=PostPre,\n            ),\n            source=""input"",\n            target=""output"",\n        )\n        network.run(\n            inputs={""input"": torch.bernoulli(torch.rand(250, 100)).byte()}, time=250\n        )\n\n        # Conv2dConnection test\n        network = Network(dt=1.0)\n        network.add_layer(Input(shape=[1, 10, 10], traces=True), name=""input"")\n        network.add_layer(LIFNodes(shape=[32, 8, 8], traces=True), name=""output"")\n        network.add_connection(\n            Conv2dConnection(\n                source=network.layers[""input""],\n                target=network.layers[""output""],\n                kernel_size=3,\n                stride=1,\n                nu=1e-2,\n                update_rule=PostPre,\n            ),\n            source=""input"",\n            target=""output"",\n        )\n        network.run(\n            inputs={""input"": torch.bernoulli(torch.rand(250, 1, 1, 10, 10)).byte()},\n            time=250,\n        )\n\n    def test_weight_dependent_post_pre(self):\n        # Connection test\n        network = Network(dt=1.0)\n        network.add_layer(Input(n=100, traces=True), name=""input"")\n        network.add_layer(LIFNodes(n=100, traces=True), name=""output"")\n        network.add_connection(\n            Connection(\n                source=network.layers[""input""],\n                target=network.layers[""output""],\n                nu=1e-2,\n                update_rule=WeightDependentPostPre,\n                wmin=-1,\n                wmax=1,\n            ),\n            source=""input"",\n            target=""output"",\n        )\n        network.run(\n            inputs={""input"": torch.bernoulli(torch.rand(250, 100)).byte()}, time=250\n        )\n\n        # Conv2dConnection test\n        network = Network(dt=1.0)\n        network.add_layer(Input(shape=[1, 10, 10], traces=True), name=""input"")\n        network.add_layer(LIFNodes(shape=[32, 8, 8], traces=True), name=""output"")\n        network.add_connection(\n            Conv2dConnection(\n                source=network.layers[""input""],\n                target=network.layers[""output""],\n                kernel_size=3,\n                stride=1,\n                nu=1e-2,\n                update_rule=WeightDependentPostPre,\n                wmin=-1,\n                wmax=1,\n            ),\n            source=""input"",\n            target=""output"",\n        )\n        network.run(\n            inputs={""input"": torch.bernoulli(torch.rand(250, 1, 1, 10, 10)).byte()},\n            time=250,\n        )\n\n    def test_mstdp(self):\n        # Connection test\n        network = Network(dt=1.0)\n        network.add_layer(Input(n=100), name=""input"")\n        network.add_layer(LIFNodes(n=100), name=""output"")\n        network.add_connection(\n            Connection(\n                source=network.layers[""input""],\n                target=network.layers[""output""],\n                nu=1e-2,\n                update_rule=MSTDP,\n            ),\n            source=""input"",\n            target=""output"",\n        )\n        network.run(\n            inputs={""input"": torch.bernoulli(torch.rand(250, 100)).byte()},\n            time=250,\n            reward=1.0,\n        )\n\n        # Conv2dConnection test\n        network = Network(dt=1.0)\n        network.add_layer(Input(shape=[1, 10, 10]), name=""input"")\n        network.add_layer(LIFNodes(shape=[32, 8, 8]), name=""output"")\n        network.add_connection(\n            Conv2dConnection(\n                source=network.layers[""input""],\n                target=network.layers[""output""],\n                kernel_size=3,\n                stride=1,\n                nu=1e-2,\n                update_rule=MSTDP,\n            ),\n            source=""input"",\n            target=""output"",\n        )\n\n        network.run(\n            inputs={""input"": torch.bernoulli(torch.rand(250, 1, 1, 10, 10)).byte()},\n            time=250,\n            reward=1.0,\n        )\n\n    def test_mstdpet(self):\n        # Connection test\n        network = Network(dt=1.0)\n        network.add_layer(Input(n=100), name=""input"")\n        network.add_layer(LIFNodes(n=100), name=""output"")\n        network.add_connection(\n            Connection(\n                source=network.layers[""input""],\n                target=network.layers[""output""],\n                nu=1e-2,\n                update_rule=MSTDPET,\n            ),\n            source=""input"",\n            target=""output"",\n        )\n        network.run(\n            inputs={""input"": torch.bernoulli(torch.rand(250, 100)).byte()},\n            time=250,\n            reward=1.0,\n        )\n\n        # Conv2dConnection test\n        network = Network(dt=1.0)\n        network.add_layer(Input(shape=[1, 10, 10]), name=""input"")\n        network.add_layer(LIFNodes(shape=[32, 8, 8]), name=""output"")\n        network.add_connection(\n            Conv2dConnection(\n                source=network.layers[""input""],\n                target=network.layers[""output""],\n                kernel_size=3,\n                stride=1,\n                nu=1e-2,\n                update_rule=MSTDPET,\n            ),\n            source=""input"",\n            target=""output"",\n        )\n\n        network.run(\n            inputs={""input"": torch.bernoulli(torch.rand(250, 1, 1, 10, 10)).byte()},\n            time=250,\n            reward=1.0,\n        )\n\n    def test_rmax(self):\n        # Connection test\n        network = Network(dt=1.0)\n        network.add_layer(Input(n=100, traces=True, traces_additive=True), name=""input"")\n        network.add_layer(SRM0Nodes(n=100), name=""output"")\n        network.add_connection(\n            Connection(\n                source=network.layers[""input""],\n                target=network.layers[""output""],\n                nu=1e-2,\n                update_rule=Rmax,\n            ),\n            source=""input"",\n            target=""output"",\n        )\n        network.run(\n            inputs={""input"": torch.bernoulli(torch.rand(250, 100)).byte()},\n            time=250,\n            reward=1.0,\n        )\n'"
test/network/test_monitors.py,18,"b'import torch\n\nfrom bindsnet.network import Network\nfrom bindsnet.network.monitors import Monitor, NetworkMonitor\nfrom bindsnet.network.nodes import Input, IFNodes\nfrom bindsnet.network.topology import Connection\n\n\nclass TestMonitor:\n    """"""\n    Testing Monitor object.\n    """"""\n\n    network = Network()\n\n    inpt = Input(75)\n    network.add_layer(inpt, name=""X"")\n    _if = IFNodes(25)\n    network.add_layer(_if, name=""Y"")\n    conn = Connection(inpt, _if, w=torch.rand(inpt.n, _if.n))\n    network.add_connection(conn, source=""X"", target=""Y"")\n\n    inpt_mon = Monitor(inpt, state_vars=[""s""])\n    network.add_monitor(inpt_mon, name=""X"")\n    _if_mon = Monitor(_if, state_vars=[""s"", ""v""])\n    network.add_monitor(_if_mon, name=""Y"")\n\n    network.run(inputs={""X"": torch.bernoulli(torch.rand(100, inpt.n))}, time=100)\n\n    assert inpt_mon.get(""s"").size() == torch.Size([100, 1, inpt.n])\n    assert _if_mon.get(""s"").size() == torch.Size([100, 1, _if.n])\n    assert _if_mon.get(""v"").size() == torch.Size([100, 1, _if.n])\n\n    del network.monitors[""X""], network.monitors[""Y""]\n\n    inpt_mon = Monitor(inpt, state_vars=[""s""], time=500)\n    network.add_monitor(inpt_mon, name=""X"")\n    _if_mon = Monitor(_if, state_vars=[""s"", ""v""], time=500)\n    network.add_monitor(_if_mon, name=""Y"")\n\n    network.run(inputs={""X"": torch.bernoulli(torch.rand(500, inpt.n))}, time=500)\n\n    assert inpt_mon.get(""s"").size() == torch.Size([500, 1, inpt.n])\n    assert _if_mon.get(""s"").size() == torch.Size([500, 1, _if.n])\n    assert _if_mon.get(""v"").size() == torch.Size([500, 1, _if.n])\n\n\nclass TestNetworkMonitor:\n    """"""\n    Testing NetworkMonitor object.\n    """"""\n\n    network = Network()\n\n    inpt = Input(25)\n    network.add_layer(inpt, name=""X"")\n    _if = IFNodes(75)\n    network.add_layer(_if, name=""Y"")\n    conn = Connection(inpt, _if, w=torch.rand(inpt.n, _if.n))\n    network.add_connection(conn, source=""X"", target=""Y"")\n\n    mon = NetworkMonitor(network, state_vars=[""s"", ""v"", ""w""])\n    network.add_monitor(mon, name=""monitor"")\n\n    network.run(inputs={""X"": torch.bernoulli(torch.rand(50, inpt.n))}, time=50)\n\n    recording = mon.get()\n\n    assert recording[""X""][""s""].size() == torch.Size([50, 1, inpt.n])\n    assert recording[""Y""][""s""].size() == torch.Size([50, 1, _if.n])\n    assert recording[""Y""][""s""].size() == torch.Size([50, 1, _if.n])\n\n    del network.monitors[""monitor""]\n\n    mon = NetworkMonitor(network, state_vars=[""s"", ""v"", ""w""], time=50)\n    network.add_monitor(mon, name=""monitor"")\n\n    network.run(inputs={""X"": torch.bernoulli(torch.rand(50, inpt.n))}, time=50)\n\n    recording = mon.get()\n\n    assert recording[""X""][""s""].size() == torch.Size([50, 1, inpt.n])\n    assert recording[""Y""][""s""].size() == torch.Size([50, 1, _if.n])\n    assert recording[""Y""][""s""].size() == torch.Size([50, 1, _if.n])\n\n\nif __name__ == ""__main__"":\n    tm = TestMonitor()\n    tnm = TestNetworkMonitor()\n'"
test/network/test_network.py,0,"b'import os\n\nfrom bindsnet.network.monitors import Monitor\nfrom bindsnet.network.topology import Connection\nfrom bindsnet.network.nodes import Input, LIFNodes\nfrom bindsnet.network import Network, load\n\n\nclass TestNetwork:\n    """"""\n    Tests basic network functionality.\n    """"""\n\n    def test_empty(self):\n        for dt in [0.1, 1.0, 5.0]:\n            network = Network(dt=dt)\n            assert network.dt == dt\n\n            network.run(inputs={}, time=1000)\n\n            network.save(""net.pt"")\n            _network = load(""net.pt"")\n            assert _network.dt == dt\n            assert _network.learning\n            del _network\n\n            _network = load(""net.pt"", learning=True)\n            assert _network.dt == dt\n            assert _network.learning\n            del _network\n\n            _network = load(""net.pt"", learning=False)\n            assert _network.dt == dt\n            assert not _network.learning\n            del _network\n\n            os.remove(""net.pt"")\n\n    def test_add_objects(self):\n        network = Network(dt=1.0, learning=False)\n\n        inpt = Input(100)\n        network.add_layer(inpt, name=""X"")\n        lif = LIFNodes(50)\n        network.add_layer(lif, name=""Y"")\n\n        assert inpt == network.layers[""X""]\n        assert lif == network.layers[""Y""]\n\n        conn = Connection(inpt, lif)\n        network.add_connection(conn, source=""X"", target=""Y"")\n\n        assert conn == network.connections[(""X"", ""Y"")]\n\n        monitor = Monitor(lif, state_vars=[""s"", ""v""])\n        network.add_monitor(monitor, ""Y"")\n\n        assert monitor == network.monitors[""Y""]\n\n        network.save(""net.pt"")\n        _network = load(""net.pt"", learning=True)\n        assert _network.learning\n        assert ""X"" in _network.layers\n        assert ""Y"" in _network.layers\n        assert (""X"", ""Y"") in _network.connections\n        assert ""Y"" in _network.monitors\n        del _network\n\n        os.remove(""net.pt"")\n'"
test/network/test_nodes.py,16,"b'import torch\n\nfrom bindsnet.network import Network\nfrom bindsnet.network.nodes import (\n    Nodes,\n    Input,\n    McCullochPitts,\n    IFNodes,\n    LIFNodes,\n    AdaptiveLIFNodes,\n    SRM0Nodes,\n)\n\n\nclass TestNodes:\n    """"""\n    Tests all stable groups of neurons / nodes.\n    """"""\n\n    def test_init(self):\n        network = Network()\n        for i, nodes in enumerate(\n            [Input, McCullochPitts, IFNodes, LIFNodes, AdaptiveLIFNodes, SRM0Nodes]\n        ):\n            for n in [1, 100, 10000]:\n                layer = nodes(n)\n                network.add_layer(layer=layer, name=f""{i}_{n}"")\n\n                assert layer.n == n\n                assert (layer.s.float() == torch.zeros(n)).all()\n\n                if nodes in [LIFNodes, AdaptiveLIFNodes]:\n                    assert (layer.v == layer.rest * torch.ones(n)).all()\n\n                layer = nodes(n, traces=True, tc_trace=1e5)\n                network.add_layer(layer=layer, name=f""{i}_traces_{n}"")\n\n                assert layer.n == n\n                assert layer.tc_trace == 1e5\n                assert (layer.s.float() == torch.zeros(n)).all()\n                assert (layer.x == torch.zeros(n)).all()\n                assert (layer.x == torch.zeros(n)).all()\n\n                if nodes in [LIFNodes, AdaptiveLIFNodes, SRM0Nodes]:\n                    assert (layer.v == layer.rest * torch.ones(n)).all()\n\n        for nodes in [LIFNodes, AdaptiveLIFNodes]:\n            for n in [1, 100, 10000]:\n                layer = nodes(\n                    n, rest=0.0, reset=-10.0, thresh=10.0, refrac=3, tc_decay=1.5e3\n                )\n                network.add_layer(layer=layer, name=f""{i}_params_{n}"")\n\n                assert layer.rest == 0.0\n                assert layer.reset == -10.0\n                assert layer.thresh == 10.0\n                assert layer.refrac == 3\n                assert layer.tc_decay == 1.5e3\n                assert (layer.s.float() == torch.zeros(n)).all()\n                assert (layer.v == layer.rest * torch.ones(n)).all()\n\n    def test_transfer(self):\n        if not torch.cuda.is_available():\n            return\n\n        for nodes in Nodes.__subclasses__():\n            layer = nodes(10)\n\n            layer.to(torch.device(""cuda:0""))\n\n            layer_tensors = [\n                k for k, v in layer.state_dict().items() if isinstance(v, torch.Tensor)\n            ]\n\n            tensor_devs = [getattr(layer, k).device for k in layer_tensors]\n\n            print(""State dict in {} : {}"".format(nodes, layer.state_dict().keys()))\n            print(""__dict__ in {} : {}"".format(nodes, layer.__dict__.keys()))\n            print(""Tensors in {} : {}"".format(nodes, layer_tensors))\n            print(""Tensor devices {}"".format(list(zip(layer_tensors, tensor_devs))))\n\n            for d in tensor_devs:\n                print(d, d == torch.device(""cuda:0""))\n                assert d == torch.device(""cuda:0"")\n\n            print(""Reset layer"")\n            layer.reset_state_variables()\n            layer_tensors = [\n                k for k, v in layer.state_dict().items() if isinstance(v, torch.Tensor)\n            ]\n\n            tensor_devs = [getattr(layer, k).device for k in layer_tensors]\n\n            for d in tensor_devs:\n                print(d, d == torch.device(""cuda:0""))\n                assert d == torch.device(""cuda:0"")\n\n\nif __name__ == ""__main__"":\n    tester = TestNodes()\n\n    tester.test_init()\n    tester.test_transfer()\n'"
