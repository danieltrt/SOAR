file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# For a fully annotated version of this file and what it does, see\n# https://github.com/pypa/sampleproject/blob/master/setup.py\n\n# To upload this file to PyPI you must build it then upload it:\n# python setup.py sdist bdist_wheel  # build in \'dist\' folder\n# python-m twine upload dist/*  # \'twine\' must be installed: \'pip install twine\'\n\n\nimport ast\nimport io\nimport re\nimport os\nimport sys\n\nfrom setuptools import find_packages, setup\n\n# Package meta-data.\nNAME = ""pytorch_toolbelt""\nDESCRIPTION = ""PyTorch extensions for fast R&D prototyping and Kaggle farming""\nURL = ""https://github.com/BloodAxe/pytorch-toolbelt""\nEMAIL = ""ekhvedchenya@gmail.com""\nAUTHOR = ""Eugene Khvedchenya""\nREQUIRES_PYTHON = "">=3.6.0""\n\nDEPENDENCIES = [""torch>=1.4"", ""torchvision>=0.5"", ""opencv-python>=4.1"", ""Pillow>=6.0,<7.0"", ""torchnet>=0.0.4""]\nEXCLUDE_FROM_PACKAGES = [""contrib"", ""docs"", ""tests"", ""examples""]\nCURDIR = os.path.abspath(os.path.dirname(__file__))\n\n\ndef get_version():\n    main_file = os.path.join(CURDIR, ""pytorch_toolbelt"", ""__init__.py"")\n    _version_re = re.compile(r""__version__\\s+=\\s+(?P<version>.*)"")\n    with open(main_file, ""r"", encoding=""utf8"") as f:\n        match = _version_re.search(f.read())\n        version = match.group(""version"") if match is not None else \'""unknown""\'\n    return str(ast.literal_eval(version))\n\n\ndef load_readme():\n    readme_path = os.path.join(CURDIR, ""README.md"")\n    with io.open(readme_path, encoding=""utf-8"") as f:\n        return ""\\n"" + f.read()\n\n\ndef get_test_requirements():\n    requirements = [""pytest"", ""catalyst>=19.6.4"", ""black==19.3b0""]\n    if sys.version_info < (3, 3):\n        requirements.append(""mock"")\n    return requirements\n\n\nsetup(\n    name=NAME,\n    version=get_version(),\n    author=AUTHOR,\n    author_email=EMAIL,\n    description=DESCRIPTION,\n    long_description=load_readme(),\n    long_description_content_type=""text/markdown"",\n    url=URL,\n    packages=find_packages(exclude=EXCLUDE_FROM_PACKAGES),\n    install_requires=DEPENDENCIES,\n    python_requires=REQUIRES_PYTHON,\n    extras_require={""tests"": get_test_requirements()},\n    include_package_data=True,\n    keywords=[\n        ""PyTorch"",\n        ""Kaggle"",\n        ""Deep Learning"",\n        ""Machine Learning"",\n        ""ResNet"",\n        ""VGG"",\n        ""ResNext"",\n        ""Unet"",\n        ""Focal"",\n        ""FPN"",\n    ],\n    scripts=[],\n    license=""License :: OSI Approved :: MIT License"",\n    classifiers=[\n        ""Development Status :: 4 - Beta"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Science/Research"",\n        ""Programming Language :: Python"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Operating System :: OS Independent"",\n        ""Topic :: Scientific/Engineering :: Mathematics"",\n        ""Topic :: Scientific/Engineering :: Image Recognition"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n        ""Topic :: Software Development :: Libraries"",\n        ""Topic :: Software Development :: Libraries :: Python Modules"",\n        ""Topic :: Software Development :: Libraries :: Application Frameworks""\n        # ""Private :: Do Not Upload""\n    ],\n)\n'"
demo/demo_losses.py,3,"b'from torch.nn import BCEWithLogitsLoss\n\nfrom pytorch_toolbelt import losses as L\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\n\n\ndef main():\n    losses = {\n        ""bce"": BCEWithLogitsLoss(),\n        # ""focal"": L.BinaryFocalLoss(),\n        # ""jaccard"": L.BinaryJaccardLoss(),\n        # ""jaccard_log"": L.BinaryJaccardLogLoss(),\n        # ""dice"": L.BinaryDiceLoss(),\n        # ""dice_log"": L.BinaryDiceLogLoss(),\n        # ""sdice"": L.BinarySymmetricDiceLoss(),\n        # ""sdice_log"": L.BinarySymmetricDiceLoss(log_loss=True),\n        ""bce+lovasz"": L.JointLoss(BCEWithLogitsLoss(), L.BinaryLovaszLoss()),\n        # ""lovasz"": L.BinaryLovaszLoss(),\n        # ""bce+jaccard"": L.JointLoss(BCEWithLogitsLoss(),\n        #                            L.BinaryJaccardLoss(), 1, 0.5),\n        # ""bce+log_jaccard"": L.JointLoss(BCEWithLogitsLoss(),\n        #                            L.BinaryJaccardLogLoss(), 1, 0.5),\n        # ""bce+log_dice"": L.JointLoss(BCEWithLogitsLoss(),\n        #                                L.BinaryDiceLogLoss(), 1, 0.5)\n        # ""reduced_focal"": L.BinaryFocalLoss(reduced=True)\n    }\n\n    dx = 0.01\n    x_vec = torch.arange(-5, 5, dx).view(-1, 1).expand((-1, 100))\n\n    f, ax = plt.subplots(3, figsize=(16, 16))\n\n    for name, loss in losses.items():\n        x_arr = []\n        y_arr = []\n        target = torch.tensor(1.0).view(1).expand((100))\n\n        for x in x_vec:\n            y = loss(x, target).item()\n\n            x_arr.append(float(x[0]))\n            y_arr.append(float(y))\n\n        ax[0].plot(x_arr, y_arr, label=name)\n        ax[1].plot(x_arr, np.gradient(y_arr, dx))\n        ax[2].plot(x_arr, np.gradient(np.gradient(y_arr, dx), dx))\n\n    f.legend()\n    f.show()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
pytorch_toolbelt/__init__.py,0,"b'from __future__ import absolute_import\n\n__version__ = ""0.3.3""\n'"
tests/test_activations.py,7,"b'import torch\nimport pytest\n\nfrom pytorch_toolbelt.modules.activations import instantiate_activation_block\n\nskip_if_no_cuda = pytest.mark.skipif(not torch.cuda.is_available(), reason=""Cuda is not available"")\n\n\n@pytest.mark.parametrize(\n    ""activation_name"",\n    [""none"", ""relu"", ""relu6"", ""leaky_relu"", ""elu"", ""selu"", ""celu"", ""mish"", ""swish"", ""hard_sigmoid"", ""hard_swish""],\n)\ndef test_activations(activation_name):\n    act = instantiate_activation_block(activation_name)\n    x = torch.randn(128).float()\n    y = act(x)\n    assert y.dtype == torch.float32\n\n\n@pytest.mark.parametrize(\n    ""activation_name"",\n    [""none"", ""relu"", ""relu6"", ""leaky_relu"", ""elu"", ""selu"", ""celu"", ""mish"", ""swish"", ""hard_sigmoid"", ""hard_swish""],\n)\n@skip_if_no_cuda\ndef test_activations_cuda(activation_name):\n    act = instantiate_activation_block(activation_name)\n    x = torch.randn(128).float().cuda()\n    y = act(x)\n    assert y.dtype == torch.float32\n\n    act = instantiate_activation_block(activation_name)\n    x = torch.randn(128).half().cuda()\n    y = act(x)\n    assert y.dtype == torch.float16\n'"
tests/test_decoders.py,13,"b'import pytest\nimport pytorch_toolbelt.modules.decoders as D\nimport pytorch_toolbelt.modules.encoders as E\nimport torch\nfrom pytorch_toolbelt.modules import FPNFuse\nfrom pytorch_toolbelt.modules.decoders import FPNSumDecoder, FPNCatDecoder\nfrom pytorch_toolbelt.utils.torch_utils import count_parameters\nfrom torch import nn\n\nskip_if_no_cuda = pytest.mark.skipif(not torch.cuda.is_available(), reason=""CUDA is not available"")\n\n\n@torch.no_grad()\ndef test_unet_encoder_decoder():\n    encoder = E.UnetEncoder(3, 32, 5)\n    decoder = D.UNetDecoder(encoder.channels)\n    x = torch.randn((2, 3, 256, 256))\n    model = nn.Sequential(encoder, decoder).eval()\n\n    output = model(x)\n\n    print(count_parameters(encoder))\n    print(count_parameters(decoder))\n    for o in output:\n        print(o.size(), o.mean(), o.std())\n\n\n@torch.no_grad()\ndef test_unet_decoder():\n    encoder = E.Resnet18Encoder(pretrained=False, layers=[0, 1, 2, 3, 4])\n    decoder = D.UNetDecoder(encoder.channels)\n    x = torch.randn((16, 3, 256, 256))\n    model = nn.Sequential(encoder, decoder)\n\n    output = model(x)\n\n    print(count_parameters(encoder))\n    print(count_parameters(decoder))\n    for o in output:\n        print(o.size(), o.mean(), o.std())\n\n\n@torch.no_grad()\ndef test_fpn_sum():\n    channels = [256, 512, 1024, 2048]\n    sizes = [64, 32, 16, 8]\n\n    decoder = FPNSumDecoder(channels, 5).eval()\n\n    x = [torch.randn(4, ch, sz, sz) for sz, ch in zip(sizes, channels)]\n    outputs = decoder(x)\n\n    print(count_parameters(decoder))\n    for o in outputs:\n        print(o.size(), o.mean(), o.std())\n\n\n@torch.no_grad()\ndef test_fpn_sum_with_encoder():\n    x = torch.randn((16, 3, 256, 256))\n    encoder = E.Resnet18Encoder(pretrained=False)\n    decoder = FPNSumDecoder(encoder.channels, 128)\n    model = nn.Sequential(encoder, decoder)\n\n    output = model(x)\n\n    print(count_parameters(decoder))\n    for o in output:\n        print(o.size(), o.mean(), o.std())\n\n\n@torch.no_grad()\ndef test_fpn_cat_with_encoder():\n    x = torch.randn((16, 3, 256, 256))\n    encoder = E.Resnet18Encoder(pretrained=False)\n    decoder = FPNCatDecoder(encoder.channels, 128)\n    model = nn.Sequential(encoder, decoder)\n\n    output = model(x)\n\n    print(count_parameters(decoder))\n    for o in output:\n        print(o.size(), o.mean(), o.std())\n\n    fuse = FPNFuse()\n    o = fuse(output)\n    print(o.size(), o.mean(), o.std())\n\n\n@torch.no_grad()\ndef test_fpn_cat():\n    channels = [256, 512, 1024, 2048]\n    sizes = [64, 32, 16, 8]\n\n    net = FPNCatDecoder(channels, 5).eval()\n\n    x = [torch.randn(4, ch, sz, sz) for sz, ch in zip(sizes, channels)]\n    outputs = net(x)\n\n    print(count_parameters(net))\n    for output in outputs:\n        print(output.size(), output.mean(), output.std())\n'"
tests/test_encoders.py,18,"b'import pytest\nimport torch\n\nimport pytorch_toolbelt.modules.encoders as E\nfrom pytorch_toolbelt.modules.backbone.inceptionv4 import inceptionv4\nfrom pytorch_toolbelt.utils.torch_utils import maybe_cuda, count_parameters\n\nskip_if_no_cuda = pytest.mark.skipif(not torch.cuda.is_available(), reason=""CUDA is not available"")\n\n\n@pytest.mark.parametrize(\n    [""encoder"", ""encoder_params""],\n    [\n        [E.MobilenetV2Encoder, {}],\n        [E.MobilenetV3Encoder, {}],\n        [E.Resnet34Encoder, {""pretrained"": False}],\n        [E.Resnet50Encoder, {""pretrained"": False}],\n        [E.SEResNeXt50Encoder, {""pretrained"": False, ""layers"": [0, 1, 2, 3, 4]}],\n        [E.SEResnet50Encoder, {""pretrained"": False}],\n        [E.Resnet152Encoder, {""pretrained"": False}],\n        [E.Resnet101Encoder, {""pretrained"": False}],\n        [E.SEResnet152Encoder, {""pretrained"": False}],\n        [E.SEResNeXt101Encoder, {""pretrained"": False}],\n        [E.SEResnet101Encoder, {""pretrained"": False}],\n        [E.SENet154Encoder, {""pretrained"": False}],\n        [E.WiderResnet16Encoder, {}],\n        [E.WiderResnet20Encoder, {}],\n        [E.WiderResnet38Encoder, {}],\n        [E.WiderResnet16A2Encoder, {}],\n        [E.WiderResnet20A2Encoder, {}],\n        [E.WiderResnet38A2Encoder, {}],\n        [E.EfficientNetB0Encoder, {}],\n        [E.EfficientNetB1Encoder, {}],\n        [E.EfficientNetB2Encoder, {}],\n        [E.EfficientNetB3Encoder, {}],\n        [E.EfficientNetB4Encoder, {}],\n        [E.EfficientNetB5Encoder, {}],\n        [E.EfficientNetB6Encoder, {}],\n        [E.EfficientNetB7Encoder, {}],\n        [E.DenseNet121Encoder, {""pretrained"": False}],\n        [E.DenseNet161Encoder, {""pretrained"": False}],\n        [E.DenseNet169Encoder, {""pretrained"": False}],\n        [E.DenseNet201Encoder, {""pretrained"": False}],\n    ],\n)\n@torch.no_grad()\n@skip_if_no_cuda\ndef test_encoders(encoder: E.EncoderModule, encoder_params):\n    net = encoder(**encoder_params).eval()\n    print(net.__class__.__name__, count_parameters(net))\n    print(net.output_strides)\n    print(net.out_channels)\n    x = torch.rand((4, 3, 256, 256))\n    x = maybe_cuda(x)\n    net = maybe_cuda(net)\n    output = net(x)\n    assert len(output) == len(net.channels)\n    for feature_map, expected_stride, expected_channels in zip(output, net.strides, net.channels):\n        assert feature_map.size(1) == expected_channels\n        assert feature_map.size(2) * expected_stride == 256\n        assert feature_map.size(3) * expected_stride == 256\n\n\n@pytest.mark.parametrize(\n    [""encoder"", ""encoder_params""],\n    [\n        [E.EfficientNetB0Encoder, {}],\n        [E.EfficientNetB1Encoder, {}],\n        [E.EfficientNetB2Encoder, {}],\n        [E.EfficientNetB3Encoder, {}],\n        [E.EfficientNetB4Encoder, {}],\n        [E.EfficientNetB5Encoder, {}],\n        [E.EfficientNetB6Encoder, {}],\n        [E.EfficientNetB7Encoder, {}],\n    ],\n)\n@torch.no_grad()\n@skip_if_no_cuda\ndef test_efficientnet(encoder: E.EncoderModule, encoder_params):\n    net = encoder(**encoder_params).eval()\n    print(net.__class__.__name__, count_parameters(net))\n    print(net.strides)\n    print(net.channels)\n    x = torch.rand((4, 3, 256, 256))\n    x = maybe_cuda(x)\n    net = maybe_cuda(net)\n    output = net(x)\n    assert len(output) == len(net.channels)\n    for feature_map, expected_stride, expected_channels in zip(output, net.strides, net.channels):\n        assert feature_map.size(1) == expected_channels\n        assert feature_map.size(2) * expected_stride == 256\n        assert feature_map.size(3) * expected_stride == 256\n\n\n@torch.no_grad()\n@skip_if_no_cuda\ndef test_unet_encoder():\n    net = E.UnetEncoder().eval()\n    print(net.__class__.__name__, count_parameters(net))\n    print(net.output_strides)\n    print(net.channels)\n    x = torch.rand((4, 3, 256, 256))\n    x = maybe_cuda(x)\n    net = maybe_cuda(net)\n    output = net(x)\n    assert len(output) == len(net.output_filters)\n    for feature_map, expected_stride, expected_channels in zip(output, net.output_strides, net.output_filters):\n        print(feature_map.size(), feature_map.mean(), feature_map.std())\n        assert feature_map.size(1) == expected_channels\n        assert feature_map.size(2) * expected_stride == 256\n        assert feature_map.size(3) * expected_stride == 256\n\n\n@torch.no_grad()\n@skip_if_no_cuda\ndef test_inceptionv4_encoder():\n    backbone = inceptionv4(pretrained=False)\n    backbone.last_linear = None\n\n    net = E.InceptionV4Encoder(pretrained=False, layers=[0, 1, 2, 3, 4]).cuda()\n\n    print(count_parameters(net))\n\n    x = torch.randn((4, 3, 512, 512)).cuda()\n\n    out = net(x)\n    for fm in out:\n        print(fm.size())\n\n\n@torch.no_grad()\n@skip_if_no_cuda\ndef test_densenet():\n    from torchvision.models import densenet121\n\n    net1 = E.DenseNet121Encoder(pretrained=False)\n    net2 = densenet121(pretrained=False)\n    net2.classifier = None\n\n    print(count_parameters(net1), count_parameters(net2))\n\n\n@pytest.mark.parametrize(\n    [""encoder"", ""encoder_params""],\n    [\n        [E.HRNetV2Encoder18, {""pretrained"": False, ""layers"": [0, 1, 2, 3, 4]}],\n        [E.HRNetV2Encoder34, {""pretrained"": False, ""layers"": [0, 1, 2, 3, 4]}],\n        [E.HRNetV2Encoder48, {""pretrained"": False, ""layers"": [0, 1, 2, 3, 4]}],\n    ],\n)\n@torch.no_grad()\n@skip_if_no_cuda\ndef test_hrnet_encoder(encoder: E.EncoderModule, encoder_params):\n    net = encoder(**encoder_params).eval()\n    print(net.__class__.__name__, count_parameters(net))\n    print(net.output_strides)\n    print(net.out_channels)\n    x = torch.rand((4, 3, 256, 256))\n    x = maybe_cuda(x)\n    net = maybe_cuda(net)\n    output = net(x)\n    assert len(output) == len(net.output_filters)\n    for feature_map, expected_stride, expected_channels in zip(output, net.output_strides, net.output_filters):\n        assert feature_map.size(1) == expected_channels\n        assert feature_map.size(2) * expected_stride == 256\n        assert feature_map.size(3) * expected_stride == 256\n\n\n@pytest.mark.parametrize(\n    [""encoder"", ""encoder_params""],\n    [\n        [E.XResNet18Encoder, {""pretrained"": False, ""layers"": [0, 1, 2, 3, 4]}],\n        [E.XResNet34Encoder, {""pretrained"": False, ""layers"": [0, 1, 2, 3, 4]}],\n        [E.XResNet50Encoder, {""pretrained"": False, ""layers"": [0, 1, 2, 3, 4]}],\n        [E.XResNet101Encoder, {""pretrained"": False, ""layers"": [0, 1, 2, 3, 4]}],\n        [E.XResNet152Encoder, {""pretrained"": False, ""layers"": [0, 1, 2, 3, 4]}],\n        [E.SEXResNet18Encoder, {""pretrained"": False, ""layers"": [0, 1, 2, 3, 4]}],\n        [E.SEXResNet34Encoder, {""pretrained"": False, ""layers"": [0, 1, 2, 3, 4]}],\n        [E.SEXResNet50Encoder, {""pretrained"": False, ""layers"": [0, 1, 2, 3, 4]}],\n        [E.SEXResNet101Encoder, {""pretrained"": False, ""layers"": [0, 1, 2, 3, 4]}],\n        [E.SEXResNet152Encoder, {""pretrained"": False, ""layers"": [0, 1, 2, 3, 4]}],\n    ],\n)\n@torch.no_grad()\n@skip_if_no_cuda\ndef test_xresnet_encoder(encoder, encoder_params):\n    net = encoder(**encoder_params).eval()\n    print(net.__class__.__name__, count_parameters(net))\n    print(net.output_strides)\n    print(net.out_channels)\n    x = torch.rand((4, 3, 256, 256))\n    x = maybe_cuda(x)\n    net = maybe_cuda(net)\n    output = net(x)\n    assert len(output) == len(net.output_filters)\n    for feature_map, expected_stride, expected_channels in zip(output, net.output_strides, net.output_filters):\n        assert feature_map.size(1) == expected_channels\n        assert feature_map.size(2) * expected_stride == 256\n        assert feature_map.size(3) * expected_stride == 256\n\n\n@pytest.mark.parametrize(\n    [""encoder"", ""encoder_params""],\n    [\n        [E.StackedHGEncoder, {""repeats"": 1}],\n        [E.StackedHGEncoder, {""repeats"": 3}],\n        [E.StackedHGEncoder, {""repeats"": 1, ""stack_level"": 4}],\n        [E.StackedHGEncoder, {""repeats"": 3, ""stack_level"": 4}],\n        [E.StackedHGEncoder, {""repeats"": 1, ""stack_level"": 4, ""features"": 128}],\n        [E.StackedHGEncoder, {""repeats"": 3, ""stack_level"": 4, ""features"": 128}],\n    ],\n)\n@torch.no_grad()\n@skip_if_no_cuda\ndef test_hourglass_encoder(encoder, encoder_params):\n    net = encoder(**encoder_params).eval()\n    print(repr(net), count_parameters(net))\n    print(""Strides "", net.strides)\n    print(""Channels"", net.channels)\n    x = torch.rand((4, 3, 256, 256))\n    x = maybe_cuda(x)\n    net = maybe_cuda(net)\n    output = net(x)\n    assert len(output) == len(net.channels)\n    for feature_map, expected_stride, expected_channels in zip(output, net.strides, net.channels):\n        assert feature_map.size(1) == expected_channels\n        assert feature_map.size(2) * expected_stride == 256\n        assert feature_map.size(3) * expected_stride == 256\n\n\n@pytest.mark.parametrize([""encoder"", ""encoder_params""], [[E.StackedSupervisedHGEncoder, {""supervision_channels"": 1}]])\n@torch.no_grad()\n@skip_if_no_cuda\ndef test_supervised_hourglass_encoder(encoder, encoder_params):\n    net = encoder(**encoder_params).eval()\n    print(net.__class__.__name__, count_parameters(net))\n    print(net.output_strides)\n    print(net.out_channels)\n    x = torch.rand((4, 3, 256, 256))\n    x = maybe_cuda(x)\n    net = maybe_cuda(net)\n    output, supervision = net(x)\n    assert len(output) == len(net.output_filters)\n    assert len(supervision) == len(net.output_filters) - 2\n\n    for feature_map, expected_stride, expected_channels in zip(output, net.output_strides, net.output_filters):\n        assert feature_map.size(1) == expected_channels\n        assert feature_map.size(2) * expected_stride == 256\n        assert feature_map.size(3) * expected_stride == 256\n\n    # for feature_map, expected_stride, expected_channels in zip(supervision, net.output_strides, net.output_filters):\n    #     assert feature_map.size(1) == expected_channels\n    #     assert feature_map.size(2) * expected_stride == 256\n    #     assert feature_map.size(3) * expected_stride == 256\n'"
tests/test_losses.py,57,"b'import pytest\nimport pytorch_toolbelt.losses.functional as F\nimport torch\nfrom pytorch_toolbelt.losses import DiceLoss, JaccardLoss, SoftBCEWithLogitsLoss, SoftCrossEntropyLoss\n\n\ndef test_sigmoid_focal_loss():\n    input_good = torch.tensor([10, -10, 10]).float()\n    input_bad = torch.tensor([-1, 2, 0]).float()\n    target = torch.tensor([1, 0, 1])\n\n    loss_good = F.focal_loss_with_logits(input_good, target)\n    loss_bad = F.focal_loss_with_logits(input_bad, target)\n    assert loss_good < loss_bad\n\n\ndef test_reduced_focal_loss():\n    input_good = torch.tensor([10, -10, 10]).float()\n    input_bad = torch.tensor([-1, 2, 0]).float()\n    target = torch.tensor([1, 0, 1])\n\n    loss_good = F.reduced_focal_loss(input_good, target)\n    loss_bad = F.reduced_focal_loss(input_bad, target)\n    assert loss_good < loss_bad\n\n\n@pytest.mark.parametrize(\n    [""y_true"", ""y_pred"", ""expected"", ""eps""],\n    [\n        [[1, 1, 1, 1], [1, 1, 1, 1], 1.0, 1e-5],\n        [[0, 1, 1, 0], [0, 1, 1, 0], 1.0, 1e-5],\n        [[1, 1, 1, 1], [1, 1, 0, 0], 0.5, 1e-5],\n    ],\n)\ndef test_soft_jaccard_score(y_true, y_pred, expected, eps):\n    y_true = torch.tensor(y_true, dtype=torch.float32)\n    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n    actual = F.soft_jaccard_score(y_pred, y_true, eps=eps)\n    assert float(actual) == pytest.approx(expected, eps)\n\n\n@pytest.mark.parametrize(\n    [""y_true"", ""y_pred"", ""expected"", ""eps""],\n    [\n        [[[1, 1, 0, 0], [0, 0, 1, 1]], [[1, 1, 0, 0], [0, 0, 1, 1]], 1.0, 1e-5],\n        [[[1, 1, 0, 0], [0, 0, 1, 1]], [[0, 0, 1, 0], [0, 1, 0, 0]], 0.0, 1e-5],\n        [[[1, 1, 0, 0], [0, 0, 0, 1]], [[1, 1, 0, 0], [0, 0, 0, 0]], 0.5, 1e-5],\n    ],\n)\ndef test_soft_jaccard_score_2(y_true, y_pred, expected, eps):\n    y_true = torch.tensor(y_true, dtype=torch.float32)\n    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n    actual = F.soft_jaccard_score(y_pred, y_true, dims=[1], eps=eps)\n    actual = actual.mean()\n    assert float(actual) == pytest.approx(expected, eps)\n\n\n@pytest.mark.parametrize(\n    [""y_true"", ""y_pred"", ""expected"", ""eps""],\n    [\n        [[1, 1, 1, 1], [1, 1, 1, 1], 1.0, 1e-5],\n        [[0, 1, 1, 0], [0, 1, 1, 0], 1.0, 1e-5],\n        [[1, 1, 1, 1], [1, 1, 0, 0], 2.0 / 3.0, 1e-5],\n    ],\n)\ndef test_soft_dice_score(y_true, y_pred, expected, eps):\n    y_true = torch.tensor(y_true, dtype=torch.float32)\n    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n    actual = F.soft_dice_score(y_pred, y_true, eps=eps)\n    assert float(actual) == pytest.approx(expected, eps)\n\n\n@torch.no_grad()\ndef test_dice_loss_binary():\n    eps = 1e-5\n    criterion = DiceLoss(mode=""binary"", from_logits=False)\n\n    # Ideal case\n    y_pred = torch.tensor([1.0, 1.0, 1.0]).view(1, 1, 1, -1)\n    y_true = torch.tensor(([1, 1, 1])).view(1, 1, 1, -1)\n    loss = criterion(y_pred, y_true)\n    assert float(loss) == pytest.approx(0.0, abs=eps)\n\n    y_pred = torch.tensor([1.0, 0.0, 1.0]).view(1, 1, 1, -1)\n    y_true = torch.tensor(([1, 0, 1])).view(1, 1, 1, -1)\n    loss = criterion(y_pred, y_true)\n    assert float(loss) == pytest.approx(0.0, abs=eps)\n\n    y_pred = torch.tensor([0.0, 0.0, 0.0]).view(1, 1, 1, -1)\n    y_true = torch.tensor(([0, 0, 0])).view(1, 1, 1, -1)\n    loss = criterion(y_pred, y_true)\n    assert float(loss) == pytest.approx(0.0, abs=eps)\n\n    # Worst case\n    y_pred = torch.tensor([1.0, 1.0, 1.0]).view(1, 1, -1)\n    y_true = torch.tensor([0, 0, 0]).view(1, 1, 1, -1)\n    loss = criterion(y_pred, y_true)\n    assert float(loss) == pytest.approx(0.0, abs=eps)\n\n    y_pred = torch.tensor([1.0, 0.0, 1.0]).view(1, 1, -1)\n    y_true = torch.tensor([0, 1, 0]).view(1, 1, 1, -1)\n    loss = criterion(y_pred, y_true)\n    assert float(loss) == pytest.approx(1.0, abs=eps)\n\n    y_pred = torch.tensor([0.0, 0.0, 0.0]).view(1, 1, -1)\n    y_true = torch.tensor([1, 1, 1]).view(1, 1, 1, -1)\n    loss = criterion(y_pred, y_true)\n    assert float(loss) == pytest.approx(1.0, abs=eps)\n\n\n@torch.no_grad()\ndef test_binary_jaccard_loss():\n    eps = 1e-5\n    criterion = JaccardLoss(mode=""binary"", from_logits=False)\n\n    # Ideal case\n    y_pred = torch.tensor([1.0]).view(1, 1, 1, 1)\n    y_true = torch.tensor(([1])).view(1, 1, 1, 1)\n    loss = criterion(y_pred, y_true)\n    assert float(loss) == pytest.approx(0.0, abs=eps)\n\n    y_pred = torch.tensor([1.0, 0.0, 1.0]).view(1, 1, 1, -1)\n    y_true = torch.tensor(([1, 0, 1])).view(1, 1, 1, -1)\n    loss = criterion(y_pred, y_true)\n    assert float(loss) == pytest.approx(0.0, abs=eps)\n\n    y_pred = torch.tensor([0.0, 0.0, 0.0]).view(1, 1, 1, -1)\n    y_true = torch.tensor(([0, 0, 0])).view(1, 1, 1, -1)\n    loss = criterion(y_pred, y_true)\n    assert float(loss) == pytest.approx(0.0, abs=eps)\n\n    # Worst case\n    y_pred = torch.tensor([1.0, 1.0, 1.0]).view(1, 1, -1)\n    y_true = torch.tensor([0, 0, 0]).view(1, 1, 1, -1)\n    loss = criterion(y_pred, y_true)\n    assert float(loss) == pytest.approx(0.0, abs=eps)\n\n    y_pred = torch.tensor([1.0, 0.0, 1.0]).view(1, 1, -1)\n    y_true = torch.tensor([0, 1, 0]).view(1, 1, 1, -1)\n    loss = criterion(y_pred, y_true)\n    assert float(loss) == pytest.approx(1.0, eps)\n\n    y_pred = torch.tensor([0.0, 0.0, 0.0]).view(1, 1, -1)\n    y_true = torch.tensor([1, 1, 1]).view(1, 1, 1, -1)\n    loss = criterion(y_pred, y_true)\n    assert float(loss) == pytest.approx(1.0, eps)\n\n\n@torch.no_grad()\ndef test_multiclass_jaccard_loss():\n    eps = 1e-5\n    criterion = JaccardLoss(mode=""multiclass"", from_logits=False)\n\n    # Ideal case\n    y_pred = torch.tensor([[[1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 1.0]]])\n    y_true = torch.tensor([[0, 0, 1, 1]])\n\n    loss = criterion(y_pred, y_true)\n    assert float(loss) == pytest.approx(0.0, abs=eps)\n\n    # Worst case\n    y_pred = torch.tensor([[[1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 1.0]]])\n    y_true = torch.tensor([[1, 1, 0, 0]])\n\n    loss = criterion(y_pred, y_true)\n    assert float(loss) == pytest.approx(1.0, abs=eps)\n\n    # 1 - 1/3 case\n    y_pred = torch.tensor([[[1.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 1.0]]])\n    y_true = torch.tensor([[1, 1, 0, 0]])\n\n    loss = criterion(y_pred, y_true)\n    assert float(loss) == pytest.approx(1.0 - 1.0 / 3.0, abs=eps)\n\n\n@torch.no_grad()\ndef test_multilabel_jaccard_loss():\n    eps = 1e-5\n    criterion = JaccardLoss(mode=""multilabel"", from_logits=False)\n\n    # Ideal case\n    y_pred = torch.tensor([[[1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 1.0]]])\n    y_true = torch.tensor([[[1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 1.0]]])\n\n    loss = criterion(y_pred, y_true)\n    assert float(loss) == pytest.approx(0.0, abs=eps)\n\n    # Worst case\n    y_pred = torch.tensor([[[1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 1.0]]])\n    y_true = 1 - y_pred\n    loss = criterion(y_pred, y_true)\n    assert float(loss) == pytest.approx(1.0, abs=eps)\n\n    # 1 - 1/3 case\n    y_pred = torch.tensor([[[0.0, 1.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0]]])\n    y_true = torch.tensor([[[1.0, 1.0, 0.0, 0.0], [1.0, 1.0, 0.0, 0.0]]])\n\n    loss = criterion(y_pred, y_true)\n    assert float(loss) == pytest.approx(1.0 - 1.0 / 3.0, abs=eps)\n\n\n@torch.no_grad()\ndef test_soft_ce_loss():\n    criterion = SoftCrossEntropyLoss(smooth_factor=0.1, ignore_index=-100)\n\n    # Ideal case\n    y_pred = torch.tensor([[+9, -9, -9, -9], [-9, +9, -9, -9], [-9, -9, +9, -9], [-9, -9, -9, +9]]).float()\n    y_true = torch.tensor([0, 1, -100, 3]).long()\n\n    loss = criterion(y_pred, y_true)\n    print(loss)\n\n\n@torch.no_grad()\ndef test_soft_bce_loss():\n    criterion = SoftBCEWithLogitsLoss(smooth_factor=0.1, ignore_index=-100)\n\n    # Ideal case\n    y_pred = torch.tensor([-9, 9, 1, 9, -9]).float()\n    y_true = torch.tensor([0, 1, -100, 1, 0]).long()\n\n    loss = criterion(y_pred, y_true)\n    print(loss)\n'"
tests/test_modules.py,11,"b'import pytest\nimport torch\nfrom pytorch_toolbelt.modules import HFF, ResidualDeconvolutionUpsample2d\n\nskip_if_no_cuda = pytest.mark.skipif(not torch.cuda.is_available(), reason=""Cuda is not available"")\n\n\ndef test_hff_dynamic_size():\n    feature_maps = [\n        torch.randn((4, 3, 512, 512)),\n        torch.randn((4, 3, 256, 256)),\n        torch.randn((4, 3, 128, 128)),\n        torch.randn((4, 3, 64, 64)),\n    ]\n\n    hff = HFF(upsample_scale=2)\n    output = hff(feature_maps)\n    assert output.size(2) == 512\n    assert output.size(3) == 512\n\n\ndef test_hff_static_size():\n    feature_maps = [\n        torch.randn((4, 3, 512, 512)),\n        torch.randn((4, 3, 384, 384)),\n        torch.randn((4, 3, 256, 256)),\n        torch.randn((4, 3, 128, 128)),\n        torch.randn((4, 3, 32, 32)),\n    ]\n\n    hff = HFF(sizes=[(512, 512), (384, 384), (256, 256), (128, 128), (32, 32)])\n    output = hff(feature_maps)\n    assert output.size(2) == 512\n    assert output.size(3) == 512\n\n\n# def test_upsample():\n#     block = DepthToSpaceUpsample2d(1)\n#     original = np.expand_dims(cv2.imread(""lena.png"", cv2.IMREAD_GRAYSCALE), -1)\n#     input = tensor_from_rgb_image(original / 255.0).unsqueeze(0).float()\n#     output = block(input)\n#\n#     output_rgb = rgb_image_from_tensor(output.squeeze(0), mean=0, std=1, max_pixel_value=1, dtype=np.float32)\n#\n#     cv2.imshow(""Original"", original)\n#     cv2.imshow(""Upsampled (cv2)"", cv2.resize(original, None, fx=2, fy=2, interpolation=cv2.INTER_LINEAR))\n#     cv2.imshow(""Upsampled"", cv2.normalize(output_rgb, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U))\n#     cv2.waitKey(-1)\n\n\ndef test_residualdeconvolutionupsampleblock():\n    x = torch.randn((4, 16, 32, 32))\n    block = ResidualDeconvolutionUpsample2d(16)\n    output = block(x)\n    print(x.size(), x.mean(), x.std())\n    print(output.size(), output.mean(), x.std())\n'"
tests/test_tiles.py,3,"b'import numpy as np\nimport torch\nfrom pytorch_toolbelt.inference.tiles import ImageSlicer, CudaTileMerger\nfrom pytorch_toolbelt.utils.torch_utils import tensor_from_rgb_image, rgb_image_from_tensor, to_numpy\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nimport pytest\n\n\nskip_if_no_cuda = pytest.mark.skipif(not torch.cuda.is_available(), reason=""Cuda is not available"")\n\n\ndef test_tiles_split_merge():\n    image = np.random.random((500, 500, 3)).astype(np.uint8)\n    tiler = ImageSlicer(image.shape, tile_size=51, tile_step=26, weight=""mean"")\n    tiles = tiler.split(image)\n    merged = tiler.merge(tiles, dtype=np.uint8)\n    np.testing.assert_equal(merged, image)\n\n\ndef test_tiles_split_merge_non_dividable():\n    image = np.random.random((563, 512, 3)).astype(np.uint8)\n    tiler = ImageSlicer(image.shape, tile_size=(128, 128), tile_step=(128, 128), weight=""mean"")\n    tiles = tiler.split(image)\n    merged = tiler.merge(tiles, dtype=np.uint8)\n    np.testing.assert_equal(merged, image)\n\n\n@skip_if_no_cuda\ndef test_tiles_split_merge_non_dividable_cuda():\n\n    image = np.random.random((5632, 5120, 3)).astype(np.uint8)\n    tiler = ImageSlicer(image.shape, tile_size=(1280, 1280), tile_step=(1280, 1280), weight=""mean"")\n    tiles = tiler.split(image)\n\n    merger = CudaTileMerger(tiler.target_shape, channels=image.shape[2], weight=tiler.weight)\n    for tile, coordinates in zip(tiles, tiler.crops):\n        # Integrate as batch of size 1\n        merger.integrate_batch(tensor_from_rgb_image(tile).unsqueeze(0).float().cuda(), [coordinates])\n\n    merged = merger.merge()\n    merged = rgb_image_from_tensor(merged, mean=0, std=1, max_pixel_value=1)\n    merged = tiler.crop_to_orignal_size(merged)\n\n    np.testing.assert_equal(merged, image)\n\n\ndef test_tiles_split_merge_2():\n    image = np.random.random((5000, 5000, 3)).astype(np.uint8)\n    tiler = ImageSlicer(image.shape, tile_size=(512, 512), tile_step=(256, 256), weight=""pyramid"")\n\n    np.testing.assert_allclose(tiler.weight, tiler.weight.T)\n\n    tiles = tiler.split(image)\n    merged = tiler.merge(tiles, dtype=np.uint8)\n    np.testing.assert_equal(merged, image)\n\n\n@skip_if_no_cuda\ndef test_tiles_split_merge_cuda():\n    class MaxChannelIntensity(nn.Module):\n        def __init__(self):\n            super().__init__()\n\n        def forward(self, input):\n            max_channel, _ = torch.max(input, dim=1, keepdim=True)\n            return max_channel\n\n    image = np.random.random((5000, 5000, 3)).astype(np.uint8)\n    tiler = ImageSlicer(image.shape, tile_size=(512, 512), tile_step=(256, 256), weight=""pyramid"")\n    tiles = [tensor_from_rgb_image(tile) for tile in tiler.split(image)]\n\n    model = MaxChannelIntensity().eval().cuda()\n\n    merger = CudaTileMerger(tiler.target_shape, 1, tiler.weight)\n    for tiles_batch, coords_batch in DataLoader(list(zip(tiles, tiler.crops)), batch_size=8, pin_memory=True):\n        tiles_batch = tiles_batch.float().cuda()\n        pred_batch = model(tiles_batch)\n\n        merger.integrate_batch(pred_batch, coords_batch)\n\n    merged = np.moveaxis(to_numpy(merger.merge()), 0, -1).astype(np.uint8)\n    merged = tiler.crop_to_orignal_size(merged)\n\n    np.testing.assert_equal(merged, image.max(axis=2, keepdims=True))\n'"
tests/test_tta.py,6,"b'import torch\nimport numpy as np\nfrom pytorch_toolbelt.inference import tta\nfrom pytorch_toolbelt.utils.torch_utils import to_numpy\nfrom torch import nn\n\n\nclass NoOp(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, input):\n        return input\n\n\nclass SumAll(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, input):\n        return input.sum(dim=[1, 2, 3])\n\n\ndef test_d4_image2mask():\n    x = torch.rand((4, 3, 224, 224))\n    model = NoOp()\n\n    output = tta.d4_image2mask(model, x)\n    np.testing.assert_allclose(to_numpy(output), to_numpy(x), atol=1e-6, rtol=1e-6)\n\n\ndef test_fliplr_image2mask():\n    x = torch.rand((4, 3, 224, 224))\n    model = NoOp()\n\n    output = tta.fliplr_image2mask(model, x)\n    np.testing.assert_allclose(to_numpy(output), to_numpy(x), atol=1e-6, rtol=1e-6)\n\n\ndef test_d4_image2label():\n    x = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 0, 1, 2], [3, 4, 5, 6]]).unsqueeze(0).unsqueeze(0).float()\n    model = SumAll()\n\n    output = tta.d4_image2label(model, x)\n    expected = int(x.sum())\n\n    assert int(output) == expected\n\n\ndef test_fliplr_image2label():\n    x = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 0, 1, 2], [3, 4, 5, 6]]).unsqueeze(0).unsqueeze(0).float()\n    model = SumAll()\n\n    output = tta.fliplr_image2label(model, x)\n    expected = int(x.sum())\n\n    assert int(output) == expected\n\n\ndef test_fivecrop_image2label():\n    x = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 0, 1, 2], [3, 4, 5, 6]]).unsqueeze(0).unsqueeze(0).float()\n    model = SumAll()\n\n    output = tta.fivecrop_image2label(model, x, (2, 2))\n    expected = ((1 + 2 + 5 + 6) + (3 + 4 + 7 + 8) + (9 + 0 + 3 + 4) + (1 + 2 + 5 + 6) + (6 + 7 + 0 + 1)) / 5\n\n    assert int(output) == expected\n\n\ndef test_tencrop_image2label():\n    x = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 0, 1, 2], [3, 4, 5, 6]]).unsqueeze(0).unsqueeze(0).float()\n    model = SumAll()\n\n    output = tta.tencrop_image2label(model, x, (2, 2))\n    expected = (2 * ((1 + 2 + 5 + 6) + (3 + 4 + 7 + 8) + (9 + 0 + 3 + 4) + (1 + 2 + 5 + 6) + (6 + 7 + 0 + 1))) / 10\n\n    assert int(output) == expected\n'"
tests/test_utils_functional.py,4,"b'import pytest\nimport torch\nfrom pytorch_toolbelt.inference.functional import unpad_xyxy_bboxes, pad_image_tensor, unpad_image_tensor\nfrom pytorch_toolbelt.modules.encoders import make_n_channel_input\nfrom torch import nn\n\n\ndef test_unpad_xyxy_bboxes():\n    bboxes1 = torch.rand((1, 32, 4))\n    bboxes2 = torch.rand((1, 32, 4, 20))\n\n    pad = [2, 3, 4, 5]\n    bboxes1_unpad = unpad_xyxy_bboxes(bboxes1, pad, dim=-1)\n    assert bboxes1_unpad.size(0) == 1\n    assert bboxes1_unpad.size(1) == 32\n    assert bboxes1_unpad.size(2) == 4\n\n    bboxes2_unpad = unpad_xyxy_bboxes(bboxes2, pad, dim=2)\n    assert bboxes2_unpad.size(0) == 1\n    assert bboxes2_unpad.size(1) == 32\n    assert bboxes2_unpad.size(2) == 4\n    assert bboxes2_unpad.size(3) == 20\n\n\ndef test_make_n_channel_input():\n    conv = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n\n    conv6 = make_n_channel_input(conv, in_channels=6)\n    assert conv6.weight.size(0) == conv.weight.size(0)\n    assert conv6.weight.size(1) == 6\n    assert conv6.weight.size(2) == conv.weight.size(2)\n    assert conv6.weight.size(3) == conv.weight.size(3)\n\n    conv5 = make_n_channel_input(conv, in_channels=5)\n    assert conv5.weight.size(0) == conv.weight.size(0)\n    assert conv5.weight.size(1) == 5\n    assert conv5.weight.size(2) == conv.weight.size(2)\n    assert conv5.weight.size(3) == conv.weight.size(3)\n\n\n@pytest.mark.parametrize(\n    [""shape"", ""padding""],\n    [((1, 3, 221, 234), 32), ((1, 3, 256, 256), 32), ((1, 3, 512, 512), 16), ((1, 3, 512, 512), 7)],\n)\ndef test_pad_unpad(shape, padding):\n    x = torch.randn(shape)\n\n    x_padded, pad_params = pad_image_tensor(x, pad_size=padding)\n    assert x_padded.size(2) % padding == 0\n    assert x_padded.size(3) % padding == 0\n\n    y = unpad_image_tensor(x_padded, pad_params)\n    assert (x == y).all()\n\n\n@pytest.mark.parametrize([""shape"", ""padding""], [((1, 3, 512, 512), (7, 13))])\ndef test_pad_unpad_nonsymmetric(shape, padding):\n    x = torch.randn(shape)\n\n    x_padded, pad_params = pad_image_tensor(x, pad_size=padding)\n    assert x_padded.size(2) % padding[0] == 0\n    assert x_padded.size(3) % padding[1] == 0\n\n    y = unpad_image_tensor(x_padded, pad_params)\n    assert (x == y).all()\n'"
pytorch_toolbelt/inference/__init__.py,0,b''
pytorch_toolbelt/inference/ensembling.py,0,"b'from torch import nn, Tensor\nfrom typing import List, Union\n\n__all__ = [""ApplySoftmaxTo"", ""ApplySigmoidTo"", ""Ensembler"", ""PickModelOutput""]\n\n\nclass ApplySoftmaxTo(nn.Module):\n    def __init__(self, model: nn.Module, output_key: Union[str, List[str]] = ""logits"", dim=1, temperature=1):\n        """"""\n        Apply softmax activation on given output(s) of the model\n        :param model: Model to wrap\n        :param output_key: string or list of strings, indicating to what outputs softmax activation should be applied.\n        :param dim: Tensor dimension for softmax activation\n        :param temperature: Temperature scaling coefficient. Values > 1 will make logits sharper.\n        """"""\n        super().__init__()\n        output_key = output_key if isinstance(output_key, (list, tuple)) else [output_key]\n        # By converting to set, we prevent double-activation by passing output_key=[""logits"", ""logits""]\n        self.output_keys = set(output_key)\n        self.model = model\n        self.dim = dim\n        self.temperature = temperature\n\n    def forward(self, *input, **kwargs):\n        output = self.model(*input, **kwargs)\n        for key in self.output_keys:\n            output[key] = output[key].mul(self.temperature).softmax(dim=1)\n        return output\n\n\nclass ApplySigmoidTo(nn.Module):\n    def __init__(self, model: nn.Module, output_key: Union[str, List[str]] = ""logits"", temperature=1):\n        """"""\n        Apply sigmoid activation on given output(s) of the model\n        :param model: Model to wrap\n        :param output_key: string or list of strings, indicating to what outputs sigmoid activation should be applied.\n        :param temperature: Temperature scaling coefficient. Values > 1 will make logits sharper.\n        """"""\n        super().__init__()\n        output_key = output_key if isinstance(output_key, (list, tuple)) else [output_key]\n        # By converting to set, we prevent double-activation by passing output_key=[""logits"", ""logits""]\n        self.output_keys = set(output_key)\n        self.model = model\n        self.temperature = temperature\n\n    def forward(self, *input, **kwargs):  # skipcq: PYL-W0221\n        output = self.model(*input, **kwargs)\n        for key in self.output_keys:\n            output[key] = output[key].mul(self.temperature).sigmoid()\n        return output\n\n\nclass Ensembler(nn.Module):\n    """"""\n    Compute sum (or average) of outputs of several models.\n    """"""\n\n    def __init__(self, models: List[nn.Module], average=True, outputs=None):\n        """"""\n\n        :param models:\n        :param average:\n        :param outputs: Name of model outputs to average and return from Ensembler.\n            If None, all outputs from the first model will be used.\n        """"""\n        super().__init__()\n        self.outputs = outputs\n        self.models = nn.ModuleList(models)\n        self.average = average\n\n    def forward(self, *input, **kwargs):  # skipcq: PYL-W0221\n        output_0 = self.models[0](*input, **kwargs)\n        num_models = len(self.models)\n\n        if self.outputs:\n            keys = self.outputs\n        else:\n            keys = output_0.keys()\n\n        for index in range(1, num_models):\n            output_i = self.models[index](*input, **kwargs)\n\n            # Sum outputs\n            for key in keys:\n                output_0[key].add_(output_i[key])\n\n        if self.average:\n            for key in keys:\n                output_0[key].mul_(1. / num_models)\n\n        return output_0\n\n\nclass PickModelOutput(nn.Module):\n    """"""\n    Assuming you have a model that outputs a dictionary, this module returns only a given element by it\'s key\n    """"""\n\n    def __init__(self, model: nn.Module, key: str):\n        super().__init__()\n        self.model = model\n        self.target_key = key\n\n    def forward(self, *input, **kwargs) -> Tensor:\n        output = self.model(*input, **kwargs)\n        return output[self.target_key]\n'"
pytorch_toolbelt/inference/functional.py,6,"b'from collections import Sized, Iterable\nfrom typing import Union, Tuple\n\nimport torch\nfrom torch import Tensor\n\n__all__ = [\n    ""torch_none"",\n    ""torch_rot90"",\n    ""torch_rot180"",\n    ""torch_rot270"",\n    ""torch_fliplr"",\n    ""torch_flipud"",\n    ""torch_transpose"",\n    ""torch_transpose2"",\n    ""torch_transpose_"",\n    ""pad_image_tensor"",\n    ""unpad_image_tensor"",\n    ""unpad_xyxy_bboxes"",\n]\n\n\ndef torch_none(x: Tensor) -> Tensor:\n    """"""\n    Return input argument without any modifications\n    :param x: input tensor\n    :return: x\n    """"""\n    return x\n\n\ndef torch_rot90(x: Tensor):\n    """"""\n    Rotate 4D image tensor by 90 degrees\n    :param x:\n    :return:\n    """"""\n    return torch.rot90(x, k=1, dims=(2, 3))\n\n\ndef torch_rot180(x: Tensor):\n    """"""\n    Rotate 4D image tensor by 180 degrees\n    :param x:\n    :return:\n    """"""\n    return torch.rot90(x, k=2, dims=(2, 3))\n\n\ndef torch_rot270(x: Tensor):\n    """"""\n    Rotate 4D image tensor by 270 degrees\n    :param x:\n    :return:\n    """"""\n    return torch.rot90(x, k=3, dims=(2, 3))\n\n\ndef torch_flipud(x: Tensor):\n    """"""\n    Flip 4D image tensor vertically\n    :param x:\n    :return:\n    """"""\n    return x.flip(2)\n\n\ndef torch_fliplr(x: Tensor):\n    """"""\n    Flip 4D image tensor horizontally\n    :param x:\n    :return:\n    """"""\n    return x.flip(3)\n\n\ndef torch_transpose(x: Tensor):\n    """"""\n    Transpose 4D image tensor by main image diagonal\n    :param x:\n    :return:\n    """"""\n    return x.transpose(2, 3)\n\n\ndef torch_transpose_(x: Tensor):\n    return x.transpose_(2, 3)\n\n\ndef torch_transpose2(x: Tensor):\n    """"""\n    Transpose 4D image tensor by second image diagonal\n    :param x:\n    :return:\n    """"""\n    return x.transpose(3, 2)\n\n\ndef pad_image_tensor(image_tensor: Tensor, pad_size: Union[int, Tuple[int, int]] = 32):\n    """"""Pad input tensor to make it\'s height and width dividable by @pad_size\n\n    :param image_tensor: 4D image tensor of shape NCHW\n    :param pad_size: Pad size\n    :return: Tuple of output tensor and pad params. Second argument can be used to reverse pad operation of model output\n    """"""\n    rows, cols = image_tensor.size(2), image_tensor.size(3)\n    if isinstance(pad_size, Sized) and isinstance(pad_size, Iterable) and len(pad_size) == 2:\n        pad_height, pad_width = [int(val) for val in pad_size]\n    elif isinstance(pad_size, int):\n        pad_height = pad_width = pad_size\n    else:\n        raise ValueError(\n            f""Unsupported pad_size: {pad_size}, must be either tuple(pad_rows,pad_cols) or single int scalar.""\n        )\n\n    if rows > pad_height:\n        pad_rows = rows % pad_height\n        pad_rows = pad_height - pad_rows if pad_rows > 0 else 0\n    else:\n        pad_rows = pad_height - rows\n\n    if cols > pad_width:\n        pad_cols = cols % pad_width\n        pad_cols = pad_width - pad_cols if pad_cols > 0 else 0\n    else:\n        pad_cols = pad_width - cols\n\n    if pad_rows == 0 and pad_cols == 0:\n        return image_tensor, (0, 0, 0, 0)\n\n    pad_top = pad_rows // 2\n    pad_btm = pad_rows - pad_top\n\n    pad_left = pad_cols // 2\n    pad_right = pad_cols - pad_left\n\n    pad = [pad_left, pad_right, pad_top, pad_btm]\n    image_tensor = torch.nn.functional.pad(image_tensor, pad)\n    return image_tensor, pad\n\n\ndef unpad_image_tensor(image_tensor: Tensor, pad) -> Tensor:\n    pad_left, pad_right, pad_top, pad_btm = pad\n    rows, cols = image_tensor.size(2), image_tensor.size(3)\n    return image_tensor[..., pad_top : rows - pad_btm, pad_left : cols - pad_right]\n\n\ndef unpad_xyxy_bboxes(bboxes_tensor: torch.Tensor, pad, dim=-1):\n    pad_left, pad_right, pad_top, pad_btm = pad\n    pad = torch.tensor([pad_left, pad_top, pad_left, pad_top], dtype=bboxes_tensor.dtype).to(bboxes_tensor.device)\n\n    if dim == -1:\n        dim = len(bboxes_tensor.size()) - 1\n\n    expand_dims = list(set(range(len(bboxes_tensor.size()))) - {dim})\n    for i, dim in enumerate(expand_dims):\n        pad = pad.unsqueeze(dim)\n\n    return bboxes_tensor - pad\n'"
pytorch_toolbelt/inference/tiles.py,5,"b'""""""Implementation of tile-based inference allowing to predict huge images that does not fit into GPU memory entirely\nin a sliding-window fashion and merging prediction mask back to full-resolution.\n""""""\nimport math\nfrom typing import List\n\nimport cv2\nimport numpy as np\nimport torch\n\n__all__ = [""ImageSlicer"", ""CudaTileMerger"", ""compute_pyramid_patch_weight_loss""]\n\n\ndef compute_pyramid_patch_weight_loss(width: int, height: int) -> np.ndarray:\n    """"""Compute a weight matrix that assigns bigger weight on pixels in center and\n    less weight to pixels on image boundary.\n    This weight matrix then used for merging individual tile predictions and helps dealing\n    with prediction artifacts on tile boundaries.\n\n    :param width: Tile width\n    :param height: Tile height\n    :return: Since-channel image [Width x Height]\n    """"""\n    xc = width * 0.5\n    yc = height * 0.5\n    xl = 0\n    xr = width\n    yb = 0\n    yt = height\n    Dc = np.zeros((width, height))\n    De = np.zeros((width, height))\n\n    Dcx = np.square(np.arange(width) - xc + 0.5)\n    Dcy = np.square(np.arange(height) - yc + 0.5)\n    Dc = np.sqrt(Dcx[np.newaxis].transpose() + Dcy)\n\n    De_l = np.square(np.arange(width) - xl + 0.5) + np.square(0.5)\n    De_r = np.square(np.arange(width) - xr + 0.5) + np.square(0.5)\n    De_b = np.square(0.5) + np.square(np.arange(height) - yb + 0.5)\n    De_t = np.square(0.5) + np.square(np.arange(height) - yt + 0.5)\n\n    De_x = np.sqrt(np.minimum(De_l, De_r))\n    De_y = np.sqrt(np.minimum(De_b, De_t))\n    De = np.minimum(De_x[np.newaxis].transpose(), De_y)\n\n    alpha = (width * height) / np.sum(np.divide(De, np.add(Dc, De)))\n    W = alpha * np.divide(De, np.add(Dc, De))\n    return W, Dc, De\n\n\nclass ImageSlicer:\n    """"""\n    Helper class to slice image into tiles and merge them back\n    """"""\n\n    def __init__(self, image_shape, tile_size, tile_step=0, image_margin=0, weight=""mean""):\n        """"""\n\n        :param image_shape: Shape of the source image (H, W)\n        :param tile_size: Tile size (Scalar or tuple (H, W)\n        :param tile_step: Step in pixels between tiles (Scalar or tuple (H, W))\n        :param image_margin:\n        :param weight: Fusion algorithm. \'mean\' - avergaing\n        """"""\n        self.image_height = image_shape[0]\n        self.image_width = image_shape[1]\n\n        if isinstance(tile_size, (tuple, list)):\n            assert len(tile_size) == 2\n            self.tile_size = int(tile_size[0]), int(tile_size[1])\n        else:\n            self.tile_size = int(tile_size), int(tile_size)\n\n        if isinstance(tile_step, (tuple, list)):\n            assert len(tile_step) == 2\n            self.tile_step = int(tile_step[0]), int(tile_step[1])\n        else:\n            self.tile_step = int(tile_step), int(tile_step)\n\n        weights = {""mean"": self._mean, ""pyramid"": self._pyramid}\n\n        self.weight = weight if isinstance(weight, np.ndarray) else weights[weight](self.tile_size)\n\n        if self.tile_step[0] < 1 or self.tile_step[0] > self.tile_size[0]:\n            raise ValueError()\n        if self.tile_step[1] < 1 or self.tile_step[1] > self.tile_size[1]:\n            raise ValueError()\n\n        overlap = [self.tile_size[0] - self.tile_step[0], self.tile_size[1] - self.tile_step[1]]\n\n        self.margin_left = 0\n        self.margin_right = 0\n        self.margin_top = 0\n        self.margin_bottom = 0\n\n        if image_margin == 0:\n            # In case margin is not set, we compute it manually\n\n            nw = max(1, math.ceil((self.image_width - overlap[1]) / self.tile_step[1]))\n            nh = max(1, math.ceil((self.image_height - overlap[0]) / self.tile_step[0]))\n\n            extra_w = self.tile_step[1] * nw - (self.image_width - overlap[1])\n            extra_h = self.tile_step[0] * nh - (self.image_height - overlap[0])\n\n            self.margin_left = extra_w // 2\n            self.margin_right = extra_w - self.margin_left\n            self.margin_top = extra_h // 2\n            self.margin_bottom = extra_h - self.margin_top\n\n        else:\n            if (self.image_width - overlap[1] + 2 * image_margin) % self.tile_step[1] != 0:\n                raise ValueError()\n\n            if (self.image_height - overlap[0] + 2 * image_margin) % self.tile_step[0] != 0:\n                raise ValueError()\n\n            self.margin_left = image_margin\n            self.margin_right = image_margin\n            self.margin_top = image_margin\n            self.margin_bottom = image_margin\n\n        crops = []\n        bbox_crops = []\n\n        for y in range(\n            0, self.image_height + self.margin_top + self.margin_bottom - self.tile_size[0] + 1, self.tile_step[0]\n        ):\n            for x in range(\n                0, self.image_width + self.margin_left + self.margin_right - self.tile_size[1] + 1, self.tile_step[1]\n            ):\n                crops.append((x, y, self.tile_size[1], self.tile_size[0]))\n                bbox_crops.append((x - self.margin_left, y - self.margin_top, self.tile_size[1], self.tile_size[0]))\n\n        self.crops = np.array(crops)\n        self.bbox_crops = np.array(bbox_crops)\n\n    def split(self, image, border_type=cv2.BORDER_CONSTANT, value=0):\n        assert image.shape[0] == self.image_height\n        assert image.shape[1] == self.image_width\n\n        orig_shape_len = len(image.shape)\n        image = cv2.copyMakeBorder(\n            image,\n            self.margin_top,\n            self.margin_bottom,\n            self.margin_left,\n            self.margin_right,\n            borderType=border_type,\n            value=value,\n        )\n\n        # This check recovers possible lack of last dummy dimension for single-channel images\n        if len(image.shape) != orig_shape_len:\n            image = np.expand_dims(image, axis=-1)\n\n        tiles = []\n        for x, y, tile_width, tile_height in self.crops:\n            tile = image[y : y + tile_height, x : x + tile_width].copy()\n            assert tile.shape[0] == self.tile_size[0]\n            assert tile.shape[1] == self.tile_size[1]\n\n            tiles.append(tile)\n\n        return tiles\n\n    def cut_patch(self, image: np.ndarray, slice_index, border_type=cv2.BORDER_CONSTANT, value=0):\n        assert image.shape[0] == self.image_height\n        assert image.shape[1] == self.image_width\n\n        orig_shape_len = len(image.shape)\n        image = cv2.copyMakeBorder(\n            image,\n            self.margin_top,\n            self.margin_bottom,\n            self.margin_left,\n            self.margin_right,\n            borderType=border_type,\n            value=value,\n        )\n\n        # This check recovers possible lack of last dummy dimension for single-channel images\n        if len(image.shape) != orig_shape_len:\n            image = np.expand_dims(image, axis=-1)\n\n        x, y, tile_width, tile_height = self.crops[slice_index]\n\n        tile = image[y : y + tile_height, x : x + tile_width].copy()\n        assert tile.shape[0] == self.tile_size[0]\n        assert tile.shape[1] == self.tile_size[1]\n        return tile\n\n    @property\n    def target_shape(self):\n        target_shape = (\n            self.image_height + self.margin_bottom + self.margin_top,\n            self.image_width + self.margin_right + self.margin_left,\n        )\n        return target_shape\n\n    def merge(self, tiles: List[np.ndarray], dtype=np.float32):\n        if len(tiles) != len(self.crops):\n            raise ValueError\n\n        channels = 1 if len(tiles[0].shape) == 2 else tiles[0].shape[2]\n        target_shape = (\n            self.image_height + self.margin_bottom + self.margin_top,\n            self.image_width + self.margin_right + self.margin_left,\n            channels,\n        )\n\n        image = np.zeros(target_shape, dtype=np.float64)\n        norm_mask = np.zeros(target_shape, dtype=np.float64)\n\n        w = np.dstack([self.weight] * channels)\n\n        for tile, (x, y, tile_width, tile_height) in zip(tiles, self.crops):\n            # print(x, y, tile_width, tile_height, image.shape)\n            image[y : y + tile_height, x : x + tile_width] += tile * w\n            norm_mask[y : y + tile_height, x : x + tile_width] += w\n\n        # print(norm_mask.min(), norm_mask.max())\n        norm_mask = np.clip(norm_mask, a_min=np.finfo(norm_mask.dtype).eps, a_max=None)\n        normalized = np.divide(image, norm_mask).astype(dtype)\n        crop = self.crop_to_orignal_size(normalized)\n        return crop\n\n    def crop_to_orignal_size(self, image):\n        assert image.shape[0] == self.target_shape[0]\n        assert image.shape[1] == self.target_shape[1]\n        crop = image[\n            self.margin_top : self.image_height + self.margin_top,\n            self.margin_left : self.image_width + self.margin_left,\n        ]\n        assert crop.shape[0] == self.image_height\n        assert crop.shape[1] == self.image_width\n        return crop\n\n    def _mean(self, tile_size):\n        return np.ones((tile_size[0], tile_size[1]), dtype=np.float32)\n\n    def _pyramid(self, tile_size):\n        w, _, _ = compute_pyramid_patch_weight_loss(tile_size[0], tile_size[1])\n        return w\n\n\nclass CudaTileMerger:\n    """"""\n    Helper class to merge final image on GPU. This generally faster than moving individual tiles to CPU.\n    """"""\n\n    def __init__(self, image_shape, channels, weight):\n        """"""\n\n        :param image_shape: Shape of the source image\n        :param image_margin:\n        :param weight: Weighting matrix\n        """"""\n        self.image_height = image_shape[0]\n        self.image_width = image_shape[1]\n\n        self.weight = torch.from_numpy(np.expand_dims(weight, axis=0)).float().cuda()\n        self.channels = channels\n        self.image = torch.zeros((channels, self.image_height, self.image_width)).cuda()\n        self.norm_mask = torch.zeros((1, self.image_height, self.image_width)).cuda()\n\n    def integrate_batch(self, batch: torch.Tensor, crop_coords):\n        """"""\n        Accumulates batch of tile predictions\n        :param batch: Predicted tiles\n        :param crop_coords: Corresponding tile crops w.r.t to original image\n        """"""\n        if len(batch) != len(crop_coords):\n            raise ValueError(""Number of images in batch does not correspond to number of coordinates"")\n\n        for tile, (x, y, tile_width, tile_height) in zip(batch, crop_coords):\n            self.image[:, y : y + tile_height, x : x + tile_width] += tile * self.weight\n            self.norm_mask[:, y : y + tile_height, x : x + tile_width] += self.weight\n\n    def merge(self) -> torch.Tensor:\n        return self.image / self.norm_mask\n'"
pytorch_toolbelt/inference/tta.py,1,"b'""""""Implementation of GPU-friendly test-time augmentation for image segmentation and classification tasks.\n\nDespite this is called test-time augmentation, these method can be used at training time as well since all\ntransformation written in PyTorch and respect gradients flow.\n""""""\nfrom functools import partial\nfrom typing import Tuple, List\n\nfrom torch import Tensor, nn\nfrom torch.nn.functional import interpolate\n\nfrom . import functional as F\n\n__all__ = [\n    ""d4_image2label"",\n    ""d4_image2mask"",\n    ""fivecrop_image2label"",\n    ""tencrop_image2label"",\n    ""fliplr_image2mask"",\n    ""fliplr_image2label"",\n    ""TTAWrapper"",\n    ""MultiscaleTTAWrapper"",\n]\n\n\ndef fliplr_image2label(model: nn.Module, image: Tensor) -> Tensor:\n    """"""Test-time augmentation for image classification that averages predictions\n    for input image and horizontally flipped one.\n\n    :param model:\n    :param image:\n    :return:\n    """"""\n    output = model(image) + model(F.torch_fliplr(image))\n    one_over_2 = float(1.0 / 2.0)\n    return output * one_over_2\n\n\ndef fivecrop_image2label(model: nn.Module, image: Tensor, crop_size: Tuple) -> Tensor:\n    """"""Test-time augmentation for image classification that takes five crops out of input tensor (4 on corners and central)\n    and averages predictions from them.\n\n    :param model: Classification model\n    :param image: Input image tensor\n    :param crop_size: Crop size. Must be smaller than image size\n    :return: Averaged logits\n    """"""\n    image_height, image_width = int(image.size(2)), int(image.size(3))\n    crop_height, crop_width = crop_size\n\n    assert crop_height <= image_height\n    assert crop_width <= image_width\n\n    bottom_crop_start = image_height - crop_height\n    right_crop_start = image_width - crop_width\n    crop_tl = image[..., :crop_height, :crop_width]\n    crop_tr = image[..., :crop_height, right_crop_start:]\n    crop_bl = image[..., bottom_crop_start:, :crop_width]\n    crop_br = image[..., bottom_crop_start:, right_crop_start:]\n\n    assert crop_tl.size(2) == crop_height\n    assert crop_tr.size(2) == crop_height\n    assert crop_bl.size(2) == crop_height\n    assert crop_br.size(2) == crop_height\n\n    assert crop_tl.size(3) == crop_width\n    assert crop_tr.size(3) == crop_width\n    assert crop_bl.size(3) == crop_width\n    assert crop_br.size(3) == crop_width\n\n    center_crop_y = (image_height - crop_height) // 2\n    center_crop_x = (image_width - crop_width) // 2\n\n    crop_cc = image[..., center_crop_y : center_crop_y + crop_height, center_crop_x : center_crop_x + crop_width]\n    assert crop_cc.size(2) == crop_height\n    assert crop_cc.size(3) == crop_width\n\n    output = model(crop_tl) + model(crop_tr) + model(crop_bl) + model(crop_br) + model(crop_cc)\n    one_over_5 = float(1.0 / 5.0)\n    return output * one_over_5\n\n\ndef tencrop_image2label(model: nn.Module, image: Tensor, crop_size: Tuple) -> Tensor:\n    """"""Test-time augmentation for image classification that takes five crops out of input tensor (4 on corners and central)\n    and averages predictions from them and from their horisontally-flipped versions (10-Crop TTA).\n\n    :param model: Classification model\n    :param image: Input image tensor\n    :param crop_size: Crop size. Must be smaller than image size\n    :return: Averaged logits\n    """"""\n    image_height, image_width = int(image.size(2)), int(image.size(3))\n    crop_height, crop_width = crop_size\n\n    assert crop_height <= image_height\n    assert crop_width <= image_width\n\n    bottom_crop_start = image_height - crop_height\n    right_crop_start = image_width - crop_width\n    crop_tl = image[..., :crop_height, :crop_width]\n    crop_tr = image[..., :crop_height, right_crop_start:]\n    crop_bl = image[..., bottom_crop_start:, :crop_width]\n    crop_br = image[..., bottom_crop_start:, right_crop_start:]\n\n    assert crop_tl.size(2) == crop_height\n    assert crop_tr.size(2) == crop_height\n    assert crop_bl.size(2) == crop_height\n    assert crop_br.size(2) == crop_height\n\n    assert crop_tl.size(3) == crop_width\n    assert crop_tr.size(3) == crop_width\n    assert crop_bl.size(3) == crop_width\n    assert crop_br.size(3) == crop_width\n\n    center_crop_y = (image_height - crop_height) // 2\n    center_crop_x = (image_width - crop_width) // 2\n\n    crop_cc = image[..., center_crop_y : center_crop_y + crop_height, center_crop_x : center_crop_x + crop_width]\n    assert crop_cc.size(2) == crop_height\n    assert crop_cc.size(3) == crop_width\n\n    output = (\n        model(crop_tl)\n        + model(F.torch_fliplr(crop_tl))\n        + model(crop_tr)\n        + model(F.torch_fliplr(crop_tr))\n        + model(crop_bl)\n        + model(F.torch_fliplr(crop_bl))\n        + model(crop_br)\n        + model(F.torch_fliplr(crop_br))\n        + model(crop_cc)\n        + model(F.torch_fliplr(crop_cc))\n    )\n\n    one_over_10 = float(1.0 / 10.0)\n    return output * one_over_10\n\n\ndef fliplr_image2mask(model: nn.Module, image: Tensor) -> Tensor:\n    """"""Test-time augmentation for image segmentation that averages predictions\n    for input image and vertically flipped one.\n\n    For segmentation we need to reverse the transformation after making a prediction\n    on augmented input.\n    :param model: Model to use for making predictions.\n    :param image: Model input.\n    :return: Arithmetically averaged predictions\n    """"""\n    output = model(image) + F.torch_fliplr(model(F.torch_fliplr(image)))\n    one_over_2 = float(1.0 / 2.0)\n    return output * one_over_2\n\n\ndef d4_image2label(model: nn.Module, image: Tensor) -> Tensor:\n    """"""Test-time augmentation for image classification that averages predictions\n    of all D4 augmentations applied to input image.\n\n    :param model: Model to use for making predictions.\n    :param image: Model input.\n    :return: Arithmetically averaged predictions\n    """"""\n    output = model(image)\n\n    for aug in [F.torch_rot90, F.torch_rot180, F.torch_rot270]:\n        x = model(aug(image))\n        output = output + x\n\n    image = F.torch_transpose(image)\n\n    for aug in [F.torch_none, F.torch_rot90, F.torch_rot180, F.torch_rot270]:\n        x = model(aug(image))\n        output = output + x\n\n    one_over_8 = float(1.0 / 8.0)\n    return output * one_over_8\n\n\ndef d4_image2mask(model: nn.Module, image: Tensor) -> Tensor:\n    """"""Test-time augmentation for image segmentation that averages predictions\n    of all D4 augmentations applied to input image.\n\n    For segmentation we need to reverse the augmentation after making a prediction\n    on augmented input.\n    :param model: Model to use for making predictions.\n    :param image: Model input.\n    :return: Arithmetically averaged predictions\n    """"""\n    output = model(image)\n\n    for aug, deaug in zip(\n        [F.torch_rot90, F.torch_rot180, F.torch_rot270], [F.torch_rot270, F.torch_rot180, F.torch_rot90]\n    ):\n        x = deaug(model(aug(image)))\n        output += x\n\n    image = F.torch_transpose(image)\n\n    for aug, deaug in zip(\n        [F.torch_none, F.torch_rot90, F.torch_rot180, F.torch_rot270],\n        [F.torch_none, F.torch_rot270, F.torch_rot180, F.torch_rot90],\n    ):\n        x = deaug(model(aug(image)))\n        output += F.torch_transpose(x)\n\n    one_over_8 = float(1.0 / 8.0)\n    output *= one_over_8\n    return output\n\n\nclass TTAWrapper(nn.Module):\n    def __init__(self, model: nn.Module, tta_function, **kwargs):\n        super().__init__()\n        self.model = model\n        self.tta = partial(tta_function, **kwargs)\n\n    def forward(self, *input):\n        return self.tta(self.model, *input)\n\n\nclass MultiscaleTTAWrapper(nn.Module):\n    """"""\n    Multiscale TTA wrapper module\n    """"""\n\n    def __init__(self, model: nn.Module, scale_levels: List[float] = None, size_offsets: List[int] = None):\n        """"""\n        Initialize multi-scale TTA wrapper\n\n        :param model: Base model for inference\n        :param scale_levels: List of additional scale levels,\n            e.g: [0.5, 0.75, 1.25]\n        """"""\n        super().__init__()\n        assert scale_levels or size_offsets, ""Either scale_levels or size_offsets must be set""\n        assert not (scale_levels and size_offsets), ""Either scale_levels or size_offsets must be set""\n        self.model = model\n        self.scale_levels = scale_levels\n        self.size_offsets = size_offsets\n\n    def forward(self, input: Tensor) -> Tensor:\n        h = input.size(2)\n        w = input.size(3)\n\n        out_size = h, w\n        output = self.model(input)\n\n        if self.scale_levels:\n            for scale in self.scale_levels:\n                dst_size = int(h * scale), int(w * scale)\n                input_scaled = interpolate(input, dst_size, mode=""bilinear"", align_corners=False)\n                output_scaled = self.model(input_scaled)\n                output_scaled = interpolate(output_scaled, out_size, mode=""bilinear"", align_corners=False)\n                output += output_scaled\n            output /= 1.0 + len(self.scale_levels)\n        elif self.size_offsets:\n            for offset in self.size_offsets:\n                dst_size = int(h + offset), int(w + offset)\n                input_scaled = interpolate(input, dst_size, mode=""bilinear"", align_corners=False)\n                output_scaled = self.model(input_scaled)\n                output_scaled = interpolate(output_scaled, out_size, mode=""bilinear"", align_corners=False)\n                output += output_scaled\n            output /= 1.0 + len(self.size_offsets)\n\n        return output\n'"
pytorch_toolbelt/losses/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .dice import *\nfrom .focal import *\nfrom .jaccard import *\nfrom .joint_loss import *\nfrom .lovasz import *\nfrom .soft_bce import *\nfrom .soft_ce import *\nfrom .wing_loss import *\n'
pytorch_toolbelt/losses/dice.py,4,"b'from typing import List\n\nimport torch\nimport torch.nn.functional as F\nfrom pytorch_toolbelt.utils.torch_utils import to_tensor\nfrom torch import Tensor\nfrom torch.nn.modules.loss import _Loss\n\nfrom .functional import soft_dice_score\n\n__all__ = [""DiceLoss""]\n\nBINARY_MODE = ""binary""\nMULTICLASS_MODE = ""multiclass""\nMULTILABEL_MODE = ""multilabel""\n\n\nclass DiceLoss(_Loss):\n    """"""\n    Implementation of Dice loss for image segmentation task.\n    It supports binary, multiclass and multilabel cases\n    """"""\n\n    def __init__(self, mode: str, classes: List[int] = None, log_loss=False, from_logits=True, smooth=0, eps=1e-7):\n        """"""\n\n        :param mode: Metric mode {\'binary\', \'multiclass\', \'multilabel\'}\n        :param classes: Optional list of classes that contribute in loss computation;\n        By default, all channels are included.\n        :param log_loss: If True, loss computed as `-log(jaccard)`; otherwise `1 - jaccard`\n        :param from_logits: If True assumes input is raw logits\n        :param smooth:\n        :param eps: Small epsilon for numerical stability\n        """"""\n        assert mode in {BINARY_MODE, MULTILABEL_MODE, MULTICLASS_MODE}\n        super(DiceLoss, self).__init__()\n        self.mode = mode\n        if classes is not None:\n            assert mode != BINARY_MODE, ""Masking classes is not supported with mode=binary""\n            classes = to_tensor(classes, dtype=torch.long)\n\n        self.classes = classes\n        self.from_logits = from_logits\n        self.smooth = smooth\n        self.eps = eps\n        self.log_loss = log_loss\n\n    def forward(self, y_pred: Tensor, y_true: Tensor) -> Tensor:\n        """"""\n\n        :param y_pred: NxCxHxW\n        :param y_true: NxHxW\n        :return: scalar\n        """"""\n        assert y_true.size(0) == y_pred.size(0)\n\n        if self.from_logits:\n            # Apply activations to get [0..1] class probabilities\n            # Using Log-Exp as this gives more numerically stable result and does not cause vanishing gradient on\n            # extreme values 0 and 1\n            if self.mode == MULTICLASS_MODE:\n                y_pred = y_pred.log_softmax(dim=1).exp()\n            else:\n                y_pred = F.logsigmoid(y_pred).exp()\n\n        bs = y_true.size(0)\n        num_classes = y_pred.size(1)\n        dims = (0, 2)\n\n        if self.mode == BINARY_MODE:\n            y_true = y_true.view(bs, 1, -1)\n            y_pred = y_pred.view(bs, 1, -1)\n\n        if self.mode == MULTICLASS_MODE:\n            y_true = y_true.view(bs, -1)\n            y_pred = y_pred.view(bs, num_classes, -1)\n\n            y_true = F.one_hot(y_true, num_classes)  # N,H*W -> N,H*W, C\n            y_true = y_true.permute(0, 2, 1)  # H, C, H*W\n\n        if self.mode == MULTILABEL_MODE:\n            y_true = y_true.view(bs, num_classes, -1)\n            y_pred = y_pred.view(bs, num_classes, -1)\n\n        scores = soft_dice_score(y_pred, y_true.type_as(y_pred), self.smooth, self.eps, dims=dims)\n\n        if self.log_loss:\n            loss = -torch.log(scores.clamp_min(self.eps))\n        else:\n            loss = 1 - scores\n\n        # Dice loss is undefined for non-empty classes\n        # So we zero contribution of channel that does not have true pixels\n        # NOTE: A better workaround would be to use loss term `mean(y_pred)`\n        # for this case, however it will be a modified jaccard loss\n\n        mask = y_true.sum(dims) > 0\n        loss *= mask.to(loss.dtype)\n\n        if self.classes is not None:\n            loss = loss[self.classes]\n\n        return loss.mean()\n'"
pytorch_toolbelt/losses/focal.py,1,"b'from functools import partial\n\nfrom torch.nn.modules.loss import _Loss\n\nfrom .functional import focal_loss_with_logits\n\n__all__ = [""BinaryFocalLoss"", ""FocalLoss""]\n\n\nclass BinaryFocalLoss(_Loss):\n    def __init__(\n        self, alpha=None, gamma=2, ignore_index=None, reduction=""mean"", normalized=False, reduced_threshold=None\n    ):\n        """"""\n\n        :param alpha: Prior probability of having positive value in target.\n        :param gamma: Power factor for dampening weight (focal strenght).\n        :param ignore_index: If not None, targets may contain values to be ignored.\n        Target values equal to ignore_index will be ignored from loss computation.\n        :param reduced:\n        :param threshold:\n        """"""\n        super().__init__()\n        self.ignore_index = ignore_index\n        self.focal_loss_fn = partial(\n            focal_loss_with_logits,\n            alpha=alpha,\n            gamma=gamma,\n            reduced_threshold=reduced_threshold,\n            reduction=reduction,\n            normalized=normalized,\n        )\n\n    def forward(self, label_input, label_target):\n        """"""Compute focal loss for binary classification problem.\n        """"""\n        label_target = label_target.view(-1)\n        label_input = label_input.view(-1)\n\n        if self.ignore_index is not None:\n            # Filter predictions with ignore label from loss computation\n            not_ignored = label_target != self.ignore_index\n            label_input = label_input[not_ignored]\n            label_target = label_target[not_ignored]\n\n        loss = self.focal_loss_fn(label_input, label_target)\n        return loss\n\n\nclass FocalLoss(_Loss):\n    def __init__(\n        self, alpha=None, gamma=2, ignore_index=None, reduction=""mean"", normalized=False, reduced_threshold=None\n    ):\n        """"""\n        Focal loss for multi-class problem.\n\n        :param alpha:\n        :param gamma:\n        :param ignore_index: If not None, targets with given index are ignored\n        :param reduced_threshold: A threshold factor for computing reduced focal loss\n        """"""\n        super().__init__()\n        self.ignore_index = ignore_index\n        self.focal_loss_fn = partial(\n            focal_loss_with_logits,\n            alpha=alpha,\n            gamma=gamma,\n            reduced_threshold=reduced_threshold,\n            reduction=reduction,\n            normalized=normalized,\n        )\n\n    def forward(self, label_input, label_target):\n        num_classes = label_input.size(1)\n        loss = 0\n\n        # Filter anchors with -1 label from loss computation\n        if self.ignore_index is not None:\n            not_ignored = label_target != self.ignore_index\n\n        for cls in range(num_classes):\n            cls_label_target = (label_target == cls).long()\n            cls_label_input = label_input[:, cls, ...]\n\n            if self.ignore_index is not None:\n                cls_label_target = cls_label_target[not_ignored]\n                cls_label_input = cls_label_input[not_ignored]\n\n            loss += self.focal_loss_fn(cls_label_input, cls_label_target)\n        return loss\n'"
pytorch_toolbelt/losses/functional.py,20,"b'import math\nfrom typing import Optional\n\nimport torch\nimport torch.nn.functional as F\n\n__all__ = [""focal_loss_with_logits"", ""sigmoid_focal_loss"", ""soft_jaccard_score"", ""soft_dice_score"", ""wing_loss""]\n\n\ndef focal_loss_with_logits(\n    input: torch.Tensor,\n    target: torch.Tensor,\n    gamma=2.0,\n    alpha: Optional[float] = 0.25,\n    reduction=""mean"",\n    normalized=False,\n    reduced_threshold: Optional[float] = None,\n) -> torch.Tensor:\n    """"""Compute binary focal loss between target and output logits.\n\n    See :class:`~pytorch_toolbelt.losses.FocalLoss` for details.\n\n    Args:\n        input: Tensor of arbitrary shape\n        target: Tensor of the same shape as input\n        reduction (string, optional): Specifies the reduction to apply to the output:\n            \'none\' | \'mean\' | \'sum\' | \'batchwise_mean\'. \'none\': no reduction will be applied,\n            \'mean\': the sum of the output will be divided by the number of\n            elements in the output, \'sum\': the output will be summed. Note: :attr:`size_average`\n            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n            specifying either of those two args will override :attr:`reduction`.\n            \'batchwise_mean\' computes mean loss per sample in batch. Default: \'mean\'\n        normalized (bool): Compute normalized focal loss (https://arxiv.org/pdf/1909.07829.pdf).\n        reduced_threshold (float, optional): Compute reduced focal loss (https://arxiv.org/abs/1903.01347).\n    References::\n\n        https://github.com/open-mmlab/mmdetection/blob/master/mmdet/core/loss/losses.py\n    """"""\n    target = target.type(input.type())\n\n    logpt = F.binary_cross_entropy_with_logits(input, target, reduction=""none"")\n    pt = torch.exp(-logpt)\n\n    # compute the loss\n    if reduced_threshold is None:\n        focal_term = (1 - pt).pow(gamma)\n    else:\n        focal_term = ((1.0 - pt) / reduced_threshold).pow(gamma)\n        focal_term[pt < reduced_threshold] = 1\n\n    loss = focal_term * logpt\n\n    if alpha is not None:\n        loss *= alpha * target + (1 - alpha) * (1 - target)\n\n    if normalized:\n        norm_factor = focal_term.sum() + 1e-5\n        loss /= norm_factor\n\n    if reduction == ""mean"":\n        loss = loss.mean()\n    if reduction == ""sum"":\n        loss = loss.sum()\n    if reduction == ""batchwise_mean"":\n        loss = loss.sum(0)\n\n    return loss\n\n\n# TODO: Mark as deprecated and emit warning\nsigmoid_focal_loss = focal_loss_with_logits\n\n\n# TODO: Mark as deprecated and emit warning\ndef reduced_focal_loss(input: torch.Tensor, target: torch.Tensor, threshold=0.5, gamma=2.0, reduction=""mean""):\n    return focal_loss_with_logits(\n        input, target, alpha=None, gamma=gamma, reduction=reduction, reduced_threshold=threshold\n    )\n\n\ndef soft_jaccard_score(y_pred: torch.Tensor, y_true: torch.Tensor, smooth=0.0, eps=1e-7, dims=None) -> torch.Tensor:\n    """"""\n\n    :param y_pred:\n    :param y_true:\n    :param smooth:\n    :param eps:\n    :return:\n\n    Shape:\n        - Input: :math:`(N, NC, *)` where :math:`*` means\n            any number of additional dimensions\n        - Target: :math:`(N, NC, *)`, same shape as the input\n        - Output: scalar.\n\n    """"""\n    assert y_pred.size() == y_true.size()\n\n    if dims is not None:\n        intersection = torch.sum(y_pred * y_true, dim=dims)\n        cardinality = torch.sum(y_pred + y_true, dim=dims)\n    else:\n        intersection = torch.sum(y_pred * y_true)\n        cardinality = torch.sum(y_pred + y_true)\n\n    union = cardinality - intersection\n    jaccard_score = (intersection + smooth) / (union.clamp_min(eps) + smooth)\n    return jaccard_score\n\n\ndef soft_dice_score(y_pred: torch.Tensor, y_true: torch.Tensor, smooth=0, eps=1e-7, dims=None) -> torch.Tensor:\n    """"""\n\n    :param y_pred:\n    :param y_true:\n    :param smooth:\n    :param eps:\n    :return:\n\n    Shape:\n        - Input: :math:`(N, NC, *)` where :math:`*` means any number\n            of additional dimensions\n        - Target: :math:`(N, NC, *)`, same shape as the input\n        - Output: scalar.\n\n    """"""\n    assert y_pred.size() == y_true.size()\n    if dims is not None:\n        intersection = torch.sum(y_pred * y_true, dim=dims)\n        cardinality = torch.sum(y_pred + y_true, dim=dims)\n    else:\n        intersection = torch.sum(y_pred * y_true)\n        cardinality = torch.sum(y_pred + y_true)\n    dice_score = (2.0 * intersection + smooth) / (cardinality.clamp_min(eps) + smooth)\n    return dice_score\n\n\ndef wing_loss(prediction: torch.Tensor, target: torch.Tensor, width=5, curvature=0.5, reduction=""mean""):\n    """"""\n    https://arxiv.org/pdf/1711.06753.pdf\n    :param prediction:\n    :param target:\n    :param width:\n    :param curvature:\n    :param reduction:\n    :return:\n    """"""\n    diff_abs = (target - prediction).abs()\n    loss = diff_abs.clone()\n\n    idx_smaller = diff_abs < width\n    idx_bigger = diff_abs >= width\n\n    loss[idx_smaller] = width * torch.log(1 + diff_abs[idx_smaller] / curvature)\n\n    C = width - width * math.log(1 + width / curvature)\n    loss[idx_bigger] = loss[idx_bigger] - C\n\n    if reduction == ""sum"":\n        loss = loss.sum()\n\n    if reduction == ""mean"":\n        loss = loss.mean()\n\n    return loss\n\n\ndef label_smoothed_nll_loss(\n    lprobs: torch.Tensor, target: torch.Tensor, epsilon: float, ignore_index=None, reduction=""mean"", dim=-1\n) -> torch.Tensor:\n    """"""\n\n    Source: https://github.com/pytorch/fairseq/blob/master/fairseq/criterions/label_smoothed_cross_entropy.py\n\n    :param lprobs: Log-probabilities of predictions (e.g after log_softmax)\n    :param target:\n    :param epsilon:\n    :param ignore_index:\n    :param reduce:\n    :return:\n    """"""\n    if target.dim() == lprobs.dim() - 1:\n        target = target.unsqueeze(dim)\n\n    if ignore_index is not None:\n        pad_mask = target.eq(ignore_index)\n        target = target.masked_fill(pad_mask, 0)\n        nll_loss = -lprobs.gather(dim=dim, index=target)\n        smooth_loss = -lprobs.sum(dim=dim, keepdim=True)\n\n        # nll_loss.masked_fill_(pad_mask, 0.0)\n        # smooth_loss.masked_fill_(pad_mask, 0.0)\n        nll_loss = nll_loss.masked_fill(pad_mask, 0.0)\n        smooth_loss = smooth_loss.masked_fill(pad_mask, 0.0)\n    else:\n        nll_loss = -lprobs.gather(dim=dim, index=target)\n        smooth_loss = -lprobs.sum(dim=dim, keepdim=True)\n\n        nll_loss = nll_loss.squeeze(dim)\n        smooth_loss = smooth_loss.squeeze(dim)\n\n    if reduction == ""sum"":\n        nll_loss = nll_loss.sum()\n        smooth_loss = smooth_loss.sum()\n    if reduction == ""mean"":\n        nll_loss = nll_loss.mean()\n        smooth_loss = smooth_loss.mean()\n\n    eps_i = epsilon / lprobs.size(dim)\n    loss = (1.0 - epsilon) * nll_loss + eps_i * smooth_loss\n    return loss\n'"
pytorch_toolbelt/losses/jaccard.py,4,"b'from typing import List\n\nimport torch\nimport torch.nn.functional as F\nfrom pytorch_toolbelt.utils.torch_utils import to_tensor\nfrom torch import Tensor\nfrom torch.nn.modules.loss import _Loss\n\nfrom .functional import soft_jaccard_score\n\n__all__ = [""JaccardLoss"", ""BINARY_MODE"", ""MULTICLASS_MODE"", ""MULTILABEL_MODE""]\n\nBINARY_MODE = ""binary""\nMULTICLASS_MODE = ""multiclass""\nMULTILABEL_MODE = ""multilabel""\n\n\nclass JaccardLoss(_Loss):\n    """"""\n    Implementation of Jaccard loss for image segmentation task.\n    It supports binary, multi-class and multi-label cases.\n    """"""\n\n    def __init__(self, mode: str, classes: List[int] = None, log_loss=False, from_logits=True, smooth=0, eps=1e-7):\n        """"""\n\n        :param mode: Metric mode {\'binary\', \'multiclass\', \'multilabel\'}\n        :param classes: Optional list of classes that contribute in loss computation;\n        By default, all channels are included.\n        :param log_loss: If True, loss computed as `-log(jaccard)`; otherwise `1 - jaccard`\n        :param from_logits: If True assumes input is raw logits\n        :param smooth:\n        :param eps: Small epsilon for numerical stability\n        """"""\n        assert mode in {BINARY_MODE, MULTILABEL_MODE, MULTICLASS_MODE}\n        super(JaccardLoss, self).__init__()\n        self.mode = mode\n        if classes is not None:\n            assert mode != BINARY_MODE, ""Masking classes is not supported with mode=binary""\n            classes = to_tensor(classes, dtype=torch.long)\n\n        self.classes = classes\n        self.from_logits = from_logits\n        self.smooth = smooth\n        self.eps = eps\n        self.log_loss = log_loss\n\n    def forward(self, y_pred: Tensor, y_true: Tensor) -> Tensor:\n        """"""\n\n        :param y_pred: NxCxHxW\n        :param y_true: NxHxW\n        :return: scalar\n        """"""\n        assert y_true.size(0) == y_pred.size(0)\n\n        if self.from_logits:\n            # Apply activations to get [0..1] class probabilities\n            # Using Log-Exp as this gives more numerically stable result and does not cause vanishing gradient on\n            # extreme values 0 and 1\n            if self.mode == MULTICLASS_MODE:\n                y_pred = y_pred.log_softmax(dim=1).exp()\n            else:\n                y_pred = F.logsigmoid(y_pred).exp()\n\n        bs = y_true.size(0)\n        num_classes = y_pred.size(1)\n        dims = (0, 2)\n\n        if self.mode == BINARY_MODE:\n            y_true = y_true.view(bs, 1, -1)\n            y_pred = y_pred.view(bs, 1, -1)\n\n        if self.mode == MULTICLASS_MODE:\n            y_true = y_true.view(bs, -1)\n            y_pred = y_pred.view(bs, num_classes, -1)\n\n            y_true = F.one_hot(y_true, num_classes)  # N,H*W -> N,H*W, C\n            y_true = y_true.permute(0, 2, 1)  # H, C, H*W\n\n        if self.mode == MULTILABEL_MODE:\n            y_true = y_true.view(bs, num_classes, -1)\n            y_pred = y_pred.view(bs, num_classes, -1)\n\n        scores = soft_jaccard_score(y_pred, y_true.type(y_pred.dtype), self.smooth, self.eps, dims=dims)\n\n        if self.log_loss:\n            loss = -torch.log(scores.clamp_min(self.eps))\n        else:\n            loss = 1 - scores\n\n        # IoU loss is defined for non-empty classes\n        # So we zero contribution of channel that does not have true pixels\n        # NOTE: A better workaround would be to use loss term `mean(y_pred)`\n        # for this case, however it will be a modified jaccard loss\n\n        mask = y_true.sum(dims) > 0\n        loss *= mask.float()\n\n        if self.classes is not None:\n            loss = loss[self.classes]\n\n        return loss.mean()\n'"
pytorch_toolbelt/losses/joint_loss.py,1,"b'from torch import nn\nfrom torch.nn.modules.loss import _Loss\n\n__all__ = [""JointLoss"", ""WeightedLoss""]\n\n\nclass WeightedLoss(_Loss):\n    """"""Wrapper class around loss function that applies weighted with fixed factor.\n    This class helps to balance multiple losses if they have different scales\n    """"""\n\n    def __init__(self, loss, weight=1.0):\n        super().__init__()\n        self.loss = loss\n        self.weight = weight\n\n    def forward(self, *input):\n        return self.loss(*input) * self.weight\n\n\nclass JointLoss(_Loss):\n    """"""\n    Wrap two loss functions into one. This class computes a weighted sum of two losses.\n    """"""\n    def __init__(self, first: nn.Module, second: nn.Module, first_weight=1.0, second_weight=1.0):\n        super().__init__()\n        self.first = WeightedLoss(first, first_weight)\n        self.second = WeightedLoss(second, second_weight)\n\n    def forward(self, *input):\n        return self.first(*input) + self.second(*input)\n'"
pytorch_toolbelt/losses/lovasz.py,7,"b'""""""\nLovasz-Softmax and Jaccard hinge loss in PyTorch\nMaxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n""""""\n\nfrom __future__ import print_function, division\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.modules.loss import _Loss\n\ntry:\n    from itertools import ifilterfalse\nexcept ImportError:  # py3k\n    from itertools import filterfalse as ifilterfalse\n\n__all__ = [""BinaryLovaszLoss"", ""LovaszLoss""]\n\n\ndef _lovasz_grad(gt_sorted):\n    """"""Compute gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    """"""\n    p = len(gt_sorted)\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.float().cumsum(0)\n    union = gts + (1 - gt_sorted).float().cumsum(0)\n    jaccard = 1.0 - intersection / union\n    if p > 1:  # cover 1-pixel case\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard\n\n\ndef _lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    """"""\n    Binary Lovasz hinge loss\n        logits: [B, H, W] Variable, logits at each pixel (between -infinity and +infinity)\n        labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n        per_image: compute the loss per image instead of per batch\n        ignore: void class id\n    """"""\n    if per_image:\n        loss = mean(\n            _lovasz_hinge_flat(*_flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n            for log, lab in zip(logits, labels)\n        )\n    else:\n        loss = _lovasz_hinge_flat(*_flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef _lovasz_hinge_flat(logits, labels):\n    """"""Binary Lovasz hinge loss\n    Args:\n        logits: [P] Variable, logits at each prediction (between -iinfinity and +iinfinity)\n        labels: [P] Tensor, binary ground truth labels (0 or 1)\n        ignore: label to ignore\n    """"""\n    if len(labels) == 0:\n        # only void pixels, the gradients should be 0\n        return logits.sum() * 0.0\n    signs = 2.0 * labels.float() - 1.0\n    errors = 1.0 - logits * Variable(signs)\n    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n    perm = perm.data\n    gt_sorted = labels[perm]\n    grad = _lovasz_grad(gt_sorted)\n    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n    return loss\n\n\ndef _flatten_binary_scores(scores, labels, ignore=None):\n    """"""Flattens predictions in the batch (binary case)\n    Remove labels equal to \'ignore\'\n    """"""\n    scores = scores.view(-1)\n    labels = labels.view(-1)\n    if ignore is None:\n        return scores, labels\n    valid = labels != ignore\n    vscores = scores[valid]\n    vlabels = labels[valid]\n    return vscores, vlabels\n\n\n# --------------------------- MULTICLASS LOSSES ---------------------------\n\n\ndef _lovasz_softmax(probas, labels, classes=""present"", per_image=False, ignore=None):\n    """"""Multi-class Lovasz-Softmax loss\n    Args:\n        @param probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1).\n        Interpreted as binary (sigmoid) output with outputs of size [B, H, W].\n        @param labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n        @param classes: \'all\' for all, \'present\' for classes present in labels, or a list of classes to average.\n        @param per_image: compute the loss per image instead of per batch\n        @param ignore: void class labels\n    """"""\n    if per_image:\n        loss = mean(\n            _lovasz_softmax_flat(*_flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), classes=classes)\n            for prob, lab in zip(probas, labels)\n        )\n    else:\n        loss = _lovasz_softmax_flat(*_flatten_probas(probas, labels, ignore), classes=classes)\n    return loss\n\n\ndef _lovasz_softmax_flat(probas, labels, classes=""present""):\n    """"""Multi-class Lovasz-Softmax loss\n    Args:\n        @param probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n        @param labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n        @param classes: \'all\' for all, \'present\' for classes present in labels, or a list of classes to average.\n    """"""\n    if probas.numel() == 0:\n        # only void pixels, the gradients should be 0\n        return probas * 0.0\n    C = probas.size(1)\n    losses = []\n    class_to_sum = list(range(C)) if classes in [""all"", ""present""] else classes\n    for c in class_to_sum:\n        fg = (labels == c).float()  # foreground for class c\n        if classes == ""present"" and fg.sum() == 0:\n            continue\n        if C == 1:\n            if len(classes) > 1:\n                raise ValueError(""Sigmoid output possible only with 1 class"")\n            class_pred = probas[:, 0]\n        else:\n            class_pred = probas[:, c]\n        errors = (Variable(fg) - class_pred).abs()\n        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n        perm = perm.data\n        fg_sorted = fg[perm]\n        losses.append(torch.dot(errors_sorted, Variable(_lovasz_grad(fg_sorted))))\n    return mean(losses)\n\n\ndef _flatten_probas(probas, labels, ignore=None):\n    """"""Flattens predictions in the batch\n    """"""\n    if probas.dim() == 3:\n        # assumes output of a sigmoid layer\n        B, H, W = probas.size()\n        probas = probas.view(B, 1, H, W)\n    B, C, H, W = probas.size()\n    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n    labels = labels.view(-1)\n    if ignore is None:\n        return probas, labels\n    valid = labels != ignore\n    vprobas = probas[valid.nonzero().squeeze()]\n    vlabels = labels[valid]\n    return vprobas, vlabels\n\n\n# --------------------------- HELPER FUNCTIONS ---------------------------\ndef isnan(x):\n    return x != x\n\n\ndef mean(values, ignore_nan=False, empty=0):\n    """"""Nanmean compatible with generators.\n    """"""\n    values = iter(values)\n    if ignore_nan:\n        values = ifilterfalse(isnan, values)\n    try:\n        n = 1\n        acc = next(values)\n    except StopIteration:\n        if empty == ""raise"":\n            raise ValueError(""Empty mean"")\n        return empty\n    for n, v in enumerate(values, 2):\n        acc += v\n    if n == 1:\n        return acc\n    return acc / n\n\n\nclass BinaryLovaszLoss(_Loss):\n    def __init__(self, per_image=False, ignore=None):\n        super().__init__()\n        self.ignore = ignore\n        self.per_image = per_image\n\n    def forward(self, logits, target):\n        return _lovasz_hinge(logits, target, per_image=self.per_image, ignore=self.ignore)\n\n\nclass LovaszLoss(_Loss):\n    def __init__(self, per_image=False, ignore=None):\n        super().__init__()\n        self.ignore = ignore\n        self.per_image = per_image\n\n    def forward(self, logits, target):\n        return _lovasz_softmax(logits, target, per_image=self.per_image, ignore=self.ignore)\n'"
pytorch_toolbelt/losses/soft_bce.py,1,"b'from typing import Optional\n\nimport torch.nn.functional as F\nfrom torch import nn, Tensor\n\n__all__ = [""SoftBCEWithLogitsLoss""]\n\n\nclass SoftBCEWithLogitsLoss(nn.Module):\n    """"""\n    Drop-in replacement for nn.BCEWithLogitsLoss with few additions:\n    - Support of ignore_index value\n    - Support of label smoothing\n    """"""\n\n    __constants__ = [""weight"", ""pos_weight"", ""reduction"", ""ignore_index"", ""smooth_factor""]\n\n    def __init__(\n        self, weight=None, ignore_index: Optional[int] = -100, reduction=""mean"", smooth_factor=None, pos_weight=None\n    ):\n        super().__init__()\n        self.ignore_index = ignore_index\n        self.reduction = reduction\n        self.smooth_factor = smooth_factor\n        self.register_buffer(""weight"", weight)\n        self.register_buffer(""pos_weight"", pos_weight)\n\n    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n        if self.smooth_factor is not None:\n            soft_targets = (1 - target) * self.smooth_factor + target * (1 - self.smooth_factor)\n        else:\n            soft_targets = target\n\n        loss = F.binary_cross_entropy_with_logits(\n            input, soft_targets, self.weight, pos_weight=self.pos_weight, reduction=""none""\n        )\n\n        if self.ignore_index is not None:\n            not_ignored_mask = target != self.ignore_index\n            size = not_ignored_mask.sum()\n            if size == 0:\n                # If there are zero elements, loss is zero\n                return 0\n            loss *= not_ignored_mask.to(loss.dtype)\n        else:\n            size = loss.numel()\n\n        if self.reduction == ""mean"":\n            loss = loss.sum() / size\n\n        if self.reduction == ""sum"":\n            loss = loss.sum()\n\n        return loss\n'"
pytorch_toolbelt/losses/soft_ce.py,1,"b'from typing import Optional\nfrom torch import nn, Tensor\nimport torch.nn.functional as F\nfrom .functional import label_smoothed_nll_loss\n\n__all__ = [""SoftCrossEntropyLoss""]\n\n\nclass SoftCrossEntropyLoss(nn.Module):\n    """"""\n    Drop-in replacement for nn.CrossEntropyLoss with few additions:\n    - Support of label smoothing\n    """"""\n\n    __constants__ = [""reduction"", ""ignore_index"", ""smooth_factor""]\n\n    def __init__(self, reduction=""mean"", smooth_factor=None, ignore_index: Optional[int] = -100, dim=1):\n        super().__init__()\n        self.smooth_factor = smooth_factor\n        self.ignore_index = ignore_index\n        self.reduction = reduction\n        self.dim = dim\n\n    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n        log_prob = F.log_softmax(input, dim=self.dim)\n        return label_smoothed_nll_loss(\n            log_prob,\n            target,\n            epsilon=self.smooth_factor,\n            ignore_index=self.ignore_index,\n            reduction=self.reduction,\n            dim=self.dim,\n        )\n'"
pytorch_toolbelt/losses/wing_loss.py,1,"b'from torch.nn.modules.loss import _Loss\n\nfrom . import functional as F\n\n__all__ = [""WingLoss""]\n\n\nclass WingLoss(_Loss):\n    def __init__(self, width=5, curvature=0.5, reduction=""mean""):\n        super(WingLoss, self).__init__(reduction=reduction)\n        self.width = width\n        self.curvature = curvature\n\n    def forward(self, prediction, target):\n        return F.wing_loss(prediction, target, self.width, self.curvature, self.reduction)\n'"
pytorch_toolbelt/modules/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .simple import *\nfrom .activations import *\nfrom .coord_conv import *\nfrom .dropblock import *\nfrom .dsconv import *\nfrom .fpn import *\nfrom .hypercolumn import *\nfrom .identity import *\nfrom .ocnet import *\nfrom .pooling import *\nfrom .scse import *\nfrom .srm import *\nfrom .unet import *\nfrom .upsample import *\nfrom .encoders import *\nfrom .decoders import *\nfrom .normalize import *\n'
pytorch_toolbelt/modules/activations.py,6,"b'from collections import OrderedDict\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .identity import Identity\n\n__all__ = [\n    ""ACT_ELU"",\n    ""ACT_HARD_SIGMOID"",\n    ""ACT_HARD_SWISH"",\n    ""ACT_LEAKY_RELU"",\n    ""ACT_NONE"",\n    ""ACT_RELU"",\n    ""ACT_RELU6"",\n    ""ACT_SELU"",\n    ""ACT_SWISH"",\n    ""ACT_MISH"",\n    ""mish"",\n    ""swish"",\n    ""hard_sigmoid"",\n    ""hard_swish"",\n    ""HardSigmoid"",\n    ""HardSwish"",\n    ""Swish"",\n    ""instantiate_activation_block"",\n    ""get_activation_block"",\n    ""sanitize_activation_name"",\n    ""ABN"",\n    ""AGN"",\n]\n\n# Activation names\nACT_CELU = ""celu""\nACT_ELU = ""elu""\nACT_GLU = ""glu""\nACT_HARD_SIGMOID = ""hard_sigmoid""\nACT_HARD_SWISH = ""hard_swish""\nACT_LEAKY_RELU = ""leaky_relu""\nACT_MISH = ""mish""\nACT_NONE = ""none""\nACT_PRELU = ""prelu""\nACT_RELU = ""relu""\nACT_RELU6 = ""relu6""\nACT_SELU = ""selu""\nACT_SWISH = ""swish""\nACT_SWISH_NAIVE = ""swish_naive""\n\n\nclass SwishFunction(torch.autograd.Function):\n    """"""\n    Memory efficient Swish implementation.\n\n    Credit: https://blog.ceshine.net/post/pytorch-memory-swish/\n    """"""\n\n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.sigmoid(i)\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n        sigmoid_i = torch.sigmoid(i)\n        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n\n\ndef mish(input):\n    """"""\n    Apply the mish function element-wise:\n    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n    See additional documentation for mish class.\n    Credit: https://github.com/digantamisra98/Mish\n    """"""\n    return input * torch.tanh(F.softplus(input))\n\n\nclass Mish(nn.Module):\n    """"""\n    Applies the mish function element-wise:\n    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n    Shape:\n        - Input: (N, *) where * means, any number of additional\n          dimensions\n        - Output: (N, *), same shape as the input\n    Examples:\n        >>> m = Mish()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n\n    Credit: https://github.com/digantamisra98/Mish\n    """"""\n\n    def __init__(self, inplace=False):\n        """"""\n        Init method.\n        :param inplace: Not used, exists only for compatibility\n        """"""\n        super().__init__()\n\n    def forward(self, input):\n        """"""\n        Forward pass of the function.\n        """"""\n        return mish(input)\n\n\ndef swish(x):\n    return SwishFunction.apply(x)\n\n\ndef swish_naive(x):\n    return x * x.sigmoid()\n\n\ndef hard_sigmoid(x, inplace=False):\n    return F.relu6(x + 3, inplace) / 6\n\n\ndef hard_swish(x, inplace=False):\n    return x * hard_sigmoid(x, inplace)\n\n\nclass HardSigmoid(nn.Module):\n    def __init__(self, inplace=False):\n        super(HardSigmoid, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return hard_sigmoid(x, inplace=self.inplace)\n\n\nclass SwishNaive(nn.Module):\n    def forward(self, input_tensor):\n        return swish_naive(input_tensor)\n\n\nclass Swish(nn.Module):\n    def forward(self, input_tensor):\n        return swish(input_tensor)\n\n\nclass HardSwish(nn.Module):\n    def __init__(self, inplace=False):\n        super(HardSwish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return hard_swish(x, inplace=self.inplace)\n\n\ndef get_activation_block(activation_name: str):\n    ACTIVATIONS = {\n        ACT_CELU: nn.CELU,\n        ACT_GLU: nn.GLU,\n        ACT_PRELU: nn.PReLU,\n        ACT_ELU: nn.ELU,\n        ACT_HARD_SIGMOID: HardSigmoid,\n        ACT_HARD_SWISH: HardSwish,\n        ACT_LEAKY_RELU: nn.LeakyReLU,\n        ACT_MISH: Mish,\n        ACT_NONE: Identity,\n        ACT_RELU6: nn.ReLU6,\n        ACT_RELU: nn.ReLU,\n        ACT_SELU: nn.SELU,\n        ACT_SWISH: Swish,\n        ACT_SWISH_NAIVE: SwishNaive,\n    }\n\n    return ACTIVATIONS[activation_name.lower()]\n\n\ndef instantiate_activation_block(activation_name: str, **kwargs) -> nn.Module:\n    block = get_activation_block(activation_name)\n\n    act_params = {}\n\n    if ""inplace"" in kwargs and activation_name in {ACT_RELU, ACT_RELU6, ACT_LEAKY_RELU, ACT_SELU, ACT_CELU, ACT_ELU}:\n        act_params[""inplace""] = kwargs[""inplace""]\n\n    if ""slope"" in kwargs and activation_name in {ACT_LEAKY_RELU}:\n        act_params[""slope""] = kwargs[""slope""]\n\n    return block(**act_params)\n\n\ndef sanitize_activation_name(activation_name: str) -> str:\n    """"""\n    Return reasonable activation name for initialization in `kaiming_uniform_` for hipster activations\n    """"""\n    if activation_name in {ACT_MISH, ACT_SWISH, ACT_SWISH_NAIVE}:\n        return ACT_LEAKY_RELU\n\n    return activation_name\n\n\ndef ABN(\n    num_features: int,\n    eps=1e-5,\n    momentum=0.1,\n    affine=True,\n    track_running_stats=True,\n    activation=ACT_RELU,\n    slope=0.01,\n    inplace=True,\n):\n    bn = nn.BatchNorm2d(\n        num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats\n    )\n    act = instantiate_activation_block(activation, inplace=inplace, slope=slope)\n    return nn.Sequential(OrderedDict([(""bn"", bn), (activation, act)]))\n\n\ndef AGN(num_features: int, num_groups: int, eps=1e-5, affine=True, activation=ACT_RELU, slope=0.01, inplace=True):\n    gn = nn.GroupNorm(num_channels=num_features, num_groups=num_groups, eps=eps, affine=affine)\n    act = instantiate_activation_block(activation, inplace=inplace, slope=slope)\n    return nn.Sequential(OrderedDict([(""gn"", gn), (activation, act)]))\n'"
pytorch_toolbelt/modules/coord_conv.py,7,"b'""""""\nImplementation of the CoordConv modules from https://arxiv.org/abs/1807.03247\n""""""\n\nimport torch\nimport torch.nn as nn\n\n__all__ = [""append_coords"", ""AddCoords"", ""CoordConv""]\n\n\ndef append_coords(input_tensor, with_r=False):\n    batch_size, _, x_dim, y_dim = input_tensor.size()\n\n    xx_channel = torch.arange(x_dim).repeat(1, y_dim, 1)\n    yy_channel = torch.arange(y_dim).repeat(1, x_dim, 1).transpose(1, 2)\n\n    xx_channel = xx_channel.float() / (x_dim - 1)\n    yy_channel = yy_channel.float() / (y_dim - 1)\n\n    xx_channel = xx_channel * 2 - 1\n    yy_channel = yy_channel * 2 - 1\n\n    xx_channel = xx_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3)\n    yy_channel = yy_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3)\n\n    ret = torch.cat([input_tensor, xx_channel.type_as(input_tensor), yy_channel.type_as(input_tensor)], dim=1)\n\n    if with_r:\n        rr = torch.sqrt(\n            torch.pow(xx_channel.type_as(input_tensor) - 0.5, 2) + torch.pow(yy_channel.type_as(input_tensor) - 0.5, 2)\n        )\n        ret = torch.cat([ret, rr], dim=1)\n\n    return ret\n\n\nclass AddCoords(nn.Module):\n    """"""\n    An alternative implementation for PyTorch with auto-infering the x-y dimensions.\n    https://github.com/mkocabas/CoordConv-pytorch/blob/master/CoordConv.py\n    """"""\n\n    def __init__(self, with_r=False):\n        super().__init__()\n        self.with_r = with_r\n\n    def forward(self, x):  # skipcq: PYL-W0221\n        """"""\n        Args:\n            x: shape(batch, channel, x_dim, y_dim)\n        """"""\n        return append_coords(x, self.with_r)\n\n\nclass CoordConv(nn.Module):\n    def __init__(self, in_channels, out_channels, with_r=False, **kwargs):\n        super().__init__()\n        self.addcoords = AddCoords(with_r=with_r)\n        in_size = in_channels + 2\n        if with_r:\n            in_size += 1\n        self.conv = nn.Conv2d(in_size, out_channels, **kwargs)\n\n    def forward(self, x):  # skipcq: PYL-W0221\n        ret = self.addcoords(x)\n        ret = self.conv(ret)\n        return ret\n'"
pytorch_toolbelt/modules/dropblock.py,5,"b'import torch\nimport torch.functional as F\nfrom torch import nn\n\n__all__ = [""DropBlock2D"", ""DropBlock3D"", ""DropBlockScheduled""]\n\n\nclass DropBlock2D(nn.Module):\n    r""""""Randomly zeroes 2D spatial blocks of the input tensor.\n    As described in the paper\n    `DropBlock: A regularization method for convolutional networks`_ ,\n    dropping whole blocks of feature map allows to remove semantic\n    information as compared to regular dropout.\n    Args:\n        drop_prob (float): probability of an element to be dropped.\n        block_size (int): size of the block to drop\n    Shape:\n        - Input: `(N, C, H, W)`\n        - Output: `(N, C, H, W)`\n    .. _DropBlock: A regularization method for convolutional networks:\n       https://arxiv.org/abs/1810.12890\n    """"""\n\n    def __init__(self, drop_prob, block_size):\n        super(DropBlock2D, self).__init__()\n\n        self.drop_prob = drop_prob\n        self.block_size = block_size\n\n    def forward(self, x):\n        # shape: (bsize, channels, height, width)\n\n        assert x.dim() == 4, ""Expected input with 4 dimensions (bsize, channels, height, width)""\n\n        if not self.training or self.drop_prob == 0.0:\n            return x\n        else:\n            # get gamma value\n            gamma = self._compute_gamma(x)\n\n            # sample mask and place on input device\n            mask = (torch.rand(x.shape[0], *x.shape[2:]) < gamma).to(x)\n\n            # compute block mask\n            block_mask, keeped = self._compute_block_mask(mask)\n\n            # apply block mask\n            out = x * block_mask[:, None, :, :]\n\n            # scale output\n            out = out * (block_mask.numel() / keeped).to(out)\n            return out\n\n    def _compute_block_mask(self, mask):\n        block_mask = F.max_pool2d(\n            input=mask[:, None, :, :],\n            kernel_size=(self.block_size, self.block_size),\n            stride=(1, 1),\n            padding=self.block_size // 2,\n        )\n\n        if self.block_size % 2 == 0:\n            block_mask = block_mask[:, :, :-1, :-1]\n\n        keeped = block_mask.numel() - block_mask.sum().to(torch.float32)  # prevent overflow in float16\n        block_mask = 1 - block_mask.squeeze(1)\n\n        return block_mask, keeped\n\n    def _compute_gamma(self, x):\n        return self.drop_prob / (self.block_size ** 2)\n\n\nclass DropBlock3D(DropBlock2D):\n    r""""""Randomly zeroes 3D spatial blocks of the input tensor.\n    An extension to the concept described in the paper\n    `DropBlock: A regularization method for convolutional networks`_ ,\n    dropping whole blocks of feature map allows to remove semantic\n    information as compared to regular dropout.\n    Args:\n        drop_prob (float): probability of an element to be dropped.\n        block_size (int): size of the block to drop\n    Shape:\n        - Input: `(N, C, D, H, W)`\n        - Output: `(N, C, D, H, W)`\n    .. _DropBlock: A regularization method for convolutional networks:\n       https://arxiv.org/abs/1810.12890\n    """"""\n\n    def __init__(self, drop_prob, block_size):\n        super(DropBlock3D, self).__init__(drop_prob, block_size)\n\n    def forward(self, x):\n        # shape: (bsize, channels, depth, height, width)\n\n        assert x.dim() == 5, ""Expected input with 5 dimensions (bsize, channels, depth, height, width)""\n\n        if not self.training or self.drop_prob == 0.0:\n            return x\n        else:\n            # get gamma value\n            gamma = self._compute_gamma(x)\n\n            # sample mask and place on input device\n            mask = (torch.rand(x.shape[0], *x.shape[2:]) < gamma).to(x)\n\n            # compute block mask\n            block_mask = self._compute_block_mask(mask)\n\n            # apply block mask\n            out = x * block_mask[:, None, :, :, :]\n\n            # scale output\n            out = out * block_mask.numel() / block_mask.sum()\n\n            return out\n\n    def _compute_block_mask(self, mask):\n        block_mask = F.max_pool3d(\n            input=mask[:, None, :, :, :],\n            kernel_size=(self.block_size, self.block_size, self.block_size),\n            stride=(1, 1, 1),\n            padding=self.block_size // 2,\n        )\n\n        if self.block_size % 2 == 0:\n            block_mask = block_mask[:, :, :-1, :-1, :-1]\n\n        block_mask = 1 - block_mask.squeeze(1)\n\n        return block_mask\n\n    def _compute_gamma(self, x):\n        return self.drop_prob / (self.block_size ** 3)\n\n\nclass DropBlockScheduled(nn.Module):\n    def __init__(self, dropblock, start_value, stop_value, nr_steps, start_step=0):\n        super(DropBlockScheduled, self).__init__()\n        self.dropblock = dropblock\n        self.register_buffer(""i"", torch.zeros(1, dtype=torch.int64))\n        self.start_step = start_step\n        self.nr_steps = nr_steps\n        self.step_size = (stop_value - start_value) / nr_steps\n\n    def forward(self, x):\n        if self.training:\n            self.step()\n        return self.dropblock(x)\n\n    def step(self):\n        idx = self.i.item()\n        if self.start_step < idx < self.start_step + self.nr_steps:\n            self.dropblock.drop_prob += self.step_size\n\n        self.i += 1\n'"
pytorch_toolbelt/modules/dsconv.py,0,"b'from torch import nn\n\n__all__ = [""DepthwiseSeparableConv2d""]\n\n\nclass DepthwiseSeparableConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n        super(DepthwiseSeparableConv2d, self).__init__()\n        self.depthwise = nn.Conv2d(\n            in_channels,\n            in_channels,\n            kernel_size=kernel_size,\n            dilation=dilation,\n            padding=padding,\n            stride=stride,\n            bias=bias,\n            groups=in_channels,\n        )\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, groups=groups, bias=bias)\n\n    def forward(self, x):\n        out = self.depthwise(x)\n        out = self.pointwise(out)\n        return out\n'"
pytorch_toolbelt/modules/fpn.py,3,"b'from __future__ import absolute_import\n\nfrom typing import List\n\nimport torch\nfrom torch import nn, Tensor\nfrom torch.nn import functional as F\n\nfrom ..modules.activations import ABN\n\n__all__ = [""FPNContextBlock"", ""FPNBottleneckBlock"", ""FPNFuse"", ""FPNFuseSum"", ""HFF""]\n\n\nclass FPNContextBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, abn_block=ABN, dropout=0.0):\n        """"""\n        Center FPN block that aggregates multi-scale context using strided average poolings\n\n        :param in_channels: Number of input features\n        :param out_channels: Number of output features\n        :param abn_block: Block for Activation + BatchNorm2d\n        :param dropout: Dropout rate after context fusion\n        """"""\n        super().__init__()\n        self.bottleneck = nn.Conv2d(in_channels, in_channels // 2, kernel_size=1)\n\n        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.proj2 = nn.Conv2d(in_channels // 2, in_channels // 8, kernel_size=1)\n\n        self.pool4 = nn.AvgPool2d(kernel_size=4, stride=4)\n        self.proj4 = nn.Conv2d(in_channels // 2, in_channels // 8, kernel_size=1)\n\n        self.pool8 = nn.AvgPool2d(kernel_size=8, stride=8)\n        self.proj8 = nn.Conv2d(in_channels // 2, in_channels // 8, kernel_size=1)\n\n        self.pool_global = nn.AdaptiveAvgPool2d(1)\n        self.proj_global = nn.Conv2d(in_channels // 2, in_channels // 8, kernel_size=1)\n\n        self.blend = nn.Conv2d(4 * in_channels // 8, out_channels, kernel_size=1)\n\n        self.conv1 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.abn1 = abn_block(out_channels)\n\n        self.dropout = nn.Dropout2d(dropout, inplace=True)\n\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.abn2 = abn_block(out_channels)\n\n    def forward(self, x: Tensor) -> Tensor:  # skipcq: PYL-W0221\n        x = self.bottleneck(x)\n\n        p2 = self.proj2(self.pool2(x))\n        p4 = self.proj4(self.pool4(x))\n        p8 = self.proj8(self.pool8(x))\n        pg = self.proj_global(self.pool_global(x))\n\n        out_size = p2.size()[2:]\n\n        x = torch.cat(\n            [\n                p2,\n                F.interpolate(p4, size=out_size, mode=""nearest""),\n                F.interpolate(p8, size=out_size, mode=""nearest""),\n                F.interpolate(pg, size=out_size, mode=""nearest""),\n            ],\n            dim=1,\n        )\n\n        x = self.blend(x)\n        x = self.conv1(x)\n        x = self.abn1(x)\n        x = self.dropout(x)\n        x = self.conv2(x)\n        x = self.abn2(x)\n        return x\n\n\nclass FPNBottleneckBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, abn_block=ABN, dropout=0.0):\n        """"""\n\n        Args:\n            encoder_features:\n            decoder_features:\n            output_features:\n            supervision_channels:\n            abn_block:\n            dropout:\n        """"""\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.abn1 = abn_block(out_channels)\n\n        self.drop1 = nn.Dropout2d(dropout, inplace=False)\n\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.abn2 = abn_block(out_channels)\n\n    def forward(self, x: Tensor) -> Tensor:  # skipcq: PYL-W0221\n\n        x = self.conv1(x)\n        x = self.abn1(x)\n        x = self.drop1(x)\n\n        x = self.conv2(x)\n        x = self.abn2(x)\n        return x\n\n\nclass FPNFuse(nn.Module):\n    def __init__(self, mode=""bilinear"", align_corners=False):\n        super().__init__()\n        self.mode = mode\n        self.align_corners = align_corners\n\n    def forward(self, features: List[Tensor]):  # skipcq: PYL-W0221\n        layers = []\n        dst_size = features[0].size()[-2:]\n\n        for f in features:\n            layers.append(F.interpolate(f, size=dst_size, mode=self.mode, align_corners=self.align_corners))\n\n        return torch.cat(layers, dim=1)\n\n\nclass FPNFuseSum(nn.Module):\n    """"""Compute a sum of individual FPN layers""""""\n\n    def __init__(self, mode=""bilinear"", align_corners=False):\n        super().__init__()\n        self.mode = mode\n        self.align_corners = align_corners\n\n    def forward(self, features: List[Tensor]) -> Tensor:  # skipcq: PYL-W0221\n        output = features[0]\n        dst_size = features[0].size()[-2:]\n\n        for f in features[1:]:\n            output = output + F.interpolate(f, size=dst_size, mode=self.mode, align_corners=self.align_corners)\n\n        return output\n\n\nclass HFF(nn.Module):\n    """"""\n    Hierarchical feature fusion module.\n    https://arxiv.org/pdf/1811.11431.pdf\n    https://arxiv.org/pdf/1803.06815.pdf\n\n    What it does is easily explained in code:\n    feature_map_0 - feature_map of the highest resolution\n    feature_map_N - feature_map of the smallest resolution\n\n    >>> feature_map = feature_map_0 + up(feature_map[1] + up(feature_map[2] + up(feature_map[3] + ...))))\n    """"""\n\n    def __init__(self, sizes=None, upsample_scale=2, mode=""nearest"", align_corners=None):\n        super().__init__()\n        self.sizes = sizes\n        self.interpolation_mode = mode\n        self.align_corners = align_corners\n        self.upsample_scale = upsample_scale\n\n    def forward(self, features: List[Tensor]) -> Tensor:  # skipcq: PYL-W0221\n        num_feature_maps = len(features)\n\n        current_map = features[-1]\n        for feature_map_index in reversed(range(num_feature_maps - 1)):\n            if self.sizes is not None:\n                prev_upsampled = self._upsample(current_map, self.sizes[feature_map_index])\n            else:\n                prev_upsampled = self._upsample(current_map)\n\n            current_map = features[feature_map_index] + prev_upsampled\n\n        return current_map\n\n    def _upsample(self, x, output_size=None):\n        if output_size is not None:\n            x = F.interpolate(\n                x,\n                size=(output_size[0], output_size[1]),\n                mode=self.interpolation_mode,\n                align_corners=self.align_corners,\n            )\n        else:\n            x = F.interpolate(\n                x, scale_factor=self.upsample_scale, mode=self.interpolation_mode, align_corners=self.align_corners\n            )\n        return x\n'"
pytorch_toolbelt/modules/hypercolumn.py,0,"b'""""""Implementation of hypercolumn module from ""Hypercolumns for Object Segmentation and Fine-grained Localization""\n\nOriginal paper: https://arxiv.org/abs/1411.5752\n""""""\n\nfrom .fpn import FPNFuse\n\n__all__ = [""HyperColumn""]\n\nHyperColumn = FPNFuse\n'"
pytorch_toolbelt/modules/identity.py,0,"b'from torch import nn\n\n__all__ = [""Identity""]\n\n\nclass Identity(nn.Module):\n    """"""The most useful module. A pass-through module which does nothing.""""""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def forward(self, x):\n        return x\n'"
pytorch_toolbelt/modules/normalize.py,3,"b'import torch\nfrom torch import nn\n\n__all__ = [""Normalize""]\n\n\nclass Normalize(nn.Module):\n    def __init__(self, mean, std):\n        super().__init__()\n        self.register_buffer(""mean"", torch.tensor(mean).float().reshape(1, len(mean), 1, 1).contiguous())\n        self.register_buffer(""std"", torch.tensor(std).float().reshape(1, len(std), 1, 1).reciprocal().contiguous())\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        return (input - self.mean) * self.std\n'"
pytorch_toolbelt/modules/ocnet.py,13,"b'# Credit: https://github.com/PkuRainBow/OCNet.pytorch/blob/master/oc_module/asp_oc_block.py\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom ..modules.activations import ABN\n\n__all__ = [""ObjectContextBlock"", ""ASPObjectContextBlock"", ""PyramidObjectContextBlock""]\n\n\nclass _SelfAttentionBlock(nn.Module):\n    """"""\n    The basic implementation for self-attention block/non-local block\n    Input:\n        N X C X H X W\n    Parameters:\n        in_channels       : the dimension of the input feature map\n        key_channels      : the dimension after the key/query transform\n        value_channels    : the dimension after the value transform\n        scale             : choose the scale to downsample the input feature maps (save memory cost)\n    Return:\n        N X C X H X W\n        position-aware context features.(w/o concate or add with the input)\n    """"""\n\n    def __init__(self, in_channels, key_channels, value_channels, out_channels=None, scale=1, abn_block=ABN):\n        super(_SelfAttentionBlock, self).__init__()\n        self.scale = scale\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.key_channels = key_channels\n        self.value_channels = value_channels\n        if out_channels is None:\n            self.out_channels = in_channels\n        self.pool = nn.MaxPool2d(kernel_size=(scale, scale))\n        self.f_key = nn.Sequential(\n            nn.Conv2d(\n                in_channels=self.in_channels, out_channels=self.key_channels, kernel_size=1, stride=1, padding=0\n            ),\n            abn_block(self.key_channels),\n        )\n        self.f_query = self.f_key\n        self.f_value = nn.Conv2d(\n            in_channels=self.in_channels, out_channels=self.value_channels, kernel_size=1, stride=1, padding=0\n        )\n        self.W = nn.Conv2d(\n            in_channels=self.value_channels, out_channels=self.out_channels, kernel_size=1, stride=1, padding=0\n        )\n        # Eugene Khvedchenya: Original implementation initialized weight of context convolution with zeros, which does not make sense to me\n        # nn.init.constant(self.W.weight, 0)\n        nn.init.constant_(self.W.bias, 0)\n\n    def forward(self, x):  # skipcq: PYL-W0221\n        batch_size, h, w = x.size(0), x.size(2), x.size(3)\n        if self.scale > 1:\n            x = self.pool(x)\n\n        value = self.f_value(x).view(batch_size, self.value_channels, -1)\n        value = value.permute(0, 2, 1)\n        query = self.f_query(x).view(batch_size, self.key_channels, -1)\n        query = query.permute(0, 2, 1)\n        key = self.f_key(x).view(batch_size, self.key_channels, -1)\n\n        sim_map = torch.matmul(query, key)\n        sim_map = (self.key_channels ** -0.5) * sim_map\n        sim_map = F.softmax(sim_map, dim=-1)\n\n        context = torch.matmul(sim_map, value)\n        context = context.permute(0, 2, 1).contiguous()\n        context = context.view(batch_size, self.value_channels, *x.size()[2:])\n        context = self.W(context)\n        if self.scale > 1:\n            context = F.interpolate(input=context, size=(h, w), mode=""bilinear"", align_corners=False)\n        return context\n\n\nclass SelfAttentionBlock2D(_SelfAttentionBlock):\n    def __init__(self, in_channels, key_channels, value_channels, out_channels=None, scale=1):\n        super(SelfAttentionBlock2D, self).__init__(in_channels, key_channels, value_channels, out_channels, scale)\n\n\nclass BaseOC_Module(nn.Module):\n    """"""\n    Implementation of the BaseOC module\n    Parameters:\n        in_features / out_features: the channels of the input / output feature maps.\n        dropout: we choose 0.05 as the default value.\n        size: you can apply multiple sizes. Here we only use one size.\n    Return:\n        features fused with Object context information.\n    """"""\n\n    def __init__(self, in_channels, out_channels, key_channels, value_channels, dropout, sizes=([1]), abn_block=ABN):\n        super(BaseOC_Module, self).__init__()\n        self.stages = []\n        self.stages = nn.ModuleList(\n            [self._make_stage(in_channels, out_channels, key_channels, value_channels, size) for size in sizes]\n        )\n        self.conv_bn_dropout = nn.Sequential(\n            nn.Conv2d(2 * in_channels, out_channels, kernel_size=1, padding=0),\n            abn_block(out_channels),\n            nn.Dropout2d(dropout),\n        )\n\n    def _make_stage(self, in_channels, output_channels, key_channels, value_channels, size):\n        return SelfAttentionBlock2D(in_channels, key_channels, value_channels, output_channels, size)\n\n    def forward(self, feats):\n        priors = [stage(feats) for stage in self.stages]\n        context = priors[0]\n        for i in range(1, len(priors)):\n            context += priors[i]\n        output = self.conv_bn_dropout(torch.cat([context, feats], 1))\n        return output\n\n\nclass ObjectContextBlock(nn.Module):\n    """"""\n    Output only the context features.\n    Parameters:\n        in_features / out_features: the channels of the input / output feature maps.\n        dropout: specify the dropout ratio\n        fusion: We provide two different fusion method, ""concat"" or ""add""\n        size: we find that directly learn the attention weights on even 1/8 feature maps is hard.\n    Return:\n        features after ""concat"" or ""add""\n    """"""\n\n    def __init__(self, in_channels, out_channels, key_channels, value_channels, dropout, sizes=([1]), abn_block=ABN):\n        super(ObjectContextBlock, self).__init__()\n        self.stages = []\n        self.stages = nn.ModuleList(\n            [self._make_stage(in_channels, out_channels, key_channels, value_channels, size) for size in sizes]\n        )\n        self.conv_bn_dropout = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, bias=False), abn_block(out_channels)\n        )\n\n    def _make_stage(self, in_channels, output_channels, key_channels, value_channels, size):\n        return SelfAttentionBlock2D(in_channels, key_channels, value_channels, output_channels, size)\n\n    def forward(self, feats):\n        priors = [stage(feats) for stage in self.stages]\n        context = priors[0]\n        for i in range(1, len(priors)):\n            context += priors[i]\n        output = self.conv_bn_dropout(context)\n        return output\n\n\nclass ASPObjectContextBlock(nn.Module):\n    def __init__(self, features, out_features=256, dilations=(12, 24, 36), abn_block=ABN, dropout=0.1):\n        super(ASPObjectContextBlock, self).__init__()\n        self.context = nn.Sequential(\n            nn.Conv2d(features, out_features, kernel_size=3, padding=1, dilation=1, bias=False),\n            abn_block(out_features),\n            ObjectContextBlock(\n                in_channels=out_features,\n                out_channels=out_features,\n                key_channels=out_features // 2,\n                value_channels=out_features,\n                dropout=dropout,\n                sizes=([2]),\n            ),\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(features, out_features, kernel_size=1, padding=0, dilation=1, bias=False),\n            abn_block(out_features),\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(features, out_features, kernel_size=3, padding=dilations[0], dilation=dilations[0], bias=False),\n            abn_block(out_features),\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(features, out_features, kernel_size=3, padding=dilations[1], dilation=dilations[1], bias=False),\n            abn_block(out_features),\n        )\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(features, out_features, kernel_size=3, padding=dilations[2], dilation=dilations[2], bias=False),\n            abn_block(out_features),\n        )\n\n        self.conv_bn_dropout = nn.Sequential(\n            nn.Conv2d(out_features * 5, out_features * 2, kernel_size=1, padding=0, dilation=1, bias=False),\n            abn_block(out_features * 2),\n            nn.Dropout2d(dropout),\n        )\n\n    def _cat_each(self, feat1, feat2, feat3, feat4, feat5):\n        assert len(feat1) == len(feat2)\n        z = []\n        for i in range(len(feat1)):\n            z.append(torch.cat((feat1[i], feat2[i], feat3[i], feat4[i], feat5[i]), dim=1))\n        return z\n\n    def forward(self, x):\n        if isinstance(x, torch.Tensor):\n            _, _, h, w = x.size()\n        elif isinstance(x, tuple) or isinstance(x, list):\n            _, _, h, w = x[0].size()\n        else:\n            raise RuntimeError(""unknown input type"")\n\n        feat1 = self.context(x)\n        feat2 = self.conv2(x)\n        feat3 = self.conv3(x)\n        feat4 = self.conv4(x)\n        feat5 = self.conv5(x)\n\n        if isinstance(x, torch.Tensor):\n            out = torch.cat((feat1, feat2, feat3, feat4, feat5), dim=1)\n        elif isinstance(x, tuple) or isinstance(x, list):\n            out = self._cat_each(feat1, feat2, feat3, feat4, feat5)\n        else:\n            raise RuntimeError(""unknown input type"")\n\n        output = self.conv_bn_dropout(out)\n        return output\n\n\nclass _PyramidSelfAttentionBlock(nn.Module):\n    """"""\n    The basic implementation for self-attention block/non-local block\n    Input:\n        N X C X H X W\n    Parameters:\n        in_channels       : the dimension of the input feature map\n        key_channels      : the dimension after the key/query transform\n        value_channels    : the dimension after the value transform\n        scale             : choose the scale to downsample the input feature maps\n    Return:\n        N X C X H X W\n        position-aware context features.(w/o concate or add with the input)\n    """"""\n\n    def __init__(self, in_channels, key_channels, value_channels, out_channels=None, scale=1, abn_block=ABN):\n        super(_PyramidSelfAttentionBlock, self).__init__()\n        self.scale = scale\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.key_channels = key_channels\n        self.value_channels = value_channels\n        if out_channels is None:\n            self.out_channels = in_channels\n        self.f_key = nn.Sequential(\n            nn.Conv2d(\n                in_channels=self.in_channels,\n                out_channels=self.key_channels,\n                kernel_size=1,\n                stride=1,\n                padding=0,\n                bias=False,\n            ),\n            abn_block(self.key_channels),\n        )\n        self.f_query = self.f_key\n        self.f_value = nn.Conv2d(\n            in_channels=self.in_channels, out_channels=self.value_channels, kernel_size=1, stride=1, padding=0\n        )\n        self.W = nn.Conv2d(\n            in_channels=self.value_channels, out_channels=self.out_channels, kernel_size=1, stride=1, padding=0\n        )\n        # nn.init.constant(self.W.weight, 0)\n        nn.init.constant(self.W.bias, 0)\n\n    def forward(self, x):\n        batch_size, _, h, w = x.size(0), x.size(1), x.size(2), x.size(3)\n\n        local_x = []\n        local_y = []\n        step_h, step_w = h // self.scale, w // self.scale\n        for i in range(0, self.scale):\n            for j in range(0, self.scale):\n                start_x, start_y = i * step_h, j * step_w\n                end_x, end_y = min(start_x + step_h, h), min(start_y + step_w, w)\n                if i == (self.scale - 1):\n                    end_x = h\n                if j == (self.scale - 1):\n                    end_y = w\n                local_x += [start_x, end_x]\n                local_y += [start_y, end_y]\n\n        value = self.f_value(x)\n        query = self.f_query(x)\n        key = self.f_key(x)\n\n        local_list = []\n        local_block_cnt = 2 * self.scale * self.scale\n        for i in range(0, local_block_cnt, 2):\n            value_local = value[:, :, local_x[i] : local_x[i + 1], local_y[i] : local_y[i + 1]]\n            query_local = query[:, :, local_x[i] : local_x[i + 1], local_y[i] : local_y[i + 1]]\n            key_local = key[:, :, local_x[i] : local_x[i + 1], local_y[i] : local_y[i + 1]]\n\n            h_local, w_local = value_local.size(2), value_local.size(3)\n            value_local = value_local.contiguous().view(batch_size, self.value_channels, -1)\n            value_local = value_local.permute(0, 2, 1)\n\n            query_local = query_local.contiguous().view(batch_size, self.key_channels, -1)\n            query_local = query_local.permute(0, 2, 1)\n            key_local = key_local.contiguous().view(batch_size, self.key_channels, -1)\n\n            sim_map = torch.matmul(query_local, key_local)\n            sim_map = (self.key_channels ** -0.5) * sim_map\n            sim_map = F.softmax(sim_map, dim=-1)\n\n            context_local = torch.matmul(sim_map, value_local)\n            context_local = context_local.permute(0, 2, 1).contiguous()\n            context_local = context_local.view(batch_size, self.value_channels, h_local, w_local)\n            local_list.append(context_local)\n\n        context_list = []\n        for i in range(0, self.scale):\n            row_tmp = []\n            for j in range(0, self.scale):\n                row_tmp.append(local_list[j + i * self.scale])\n            context_list.append(torch.cat(row_tmp, 3))\n\n        context = torch.cat(context_list, 2)\n        context = self.W(context)\n\n        return context\n\n\nclass PyramidSelfAttentionBlock2D(_PyramidSelfAttentionBlock):\n    def __init__(self, in_channels, key_channels, value_channels, out_channels=None, scale=1):\n        super(PyramidSelfAttentionBlock2D, self).__init__(\n            in_channels, key_channels, value_channels, out_channels, scale\n        )\n\n\nclass PyramidObjectContextBlock(nn.Module):\n    """"""\n    Output the combination of the context features and the original features.\n    Parameters:\n        in_features / out_features: the channels of the input / output feature maps.\n        dropout: specify the dropout ratio\n        size: we find that directly learn the attention weights on even 1/8 feature maps is hard.\n    Return:\n        features after ""concat"" or ""add""\n    """"""\n\n    def __init__(self, in_channels, out_channels, dropout=0.05, sizes=([1, 2, 3, 6]), abn_block=ABN):\n        super(PyramidObjectContextBlock, self).__init__()\n        self.group = len(sizes)\n        self.stages = []\n        self.stages = nn.ModuleList(\n            [self._make_stage(in_channels, out_channels, in_channels // 2, in_channels, size) for size in sizes]\n        )\n        self.conv_bn_dropout = nn.Sequential(\n            nn.Conv2d(2 * in_channels * self.group, out_channels, kernel_size=1, padding=0, bias=False),\n            abn_block(out_channels),\n            nn.Dropout2d(dropout),\n        )\n        self.up_dr = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels * self.group, kernel_size=1, padding=0, bias=False),\n            abn_block(in_channels * self.group),\n        )\n\n    def _make_stage(self, in_channels, output_channels, key_channels, value_channels, size):\n        return PyramidSelfAttentionBlock2D(in_channels, key_channels, value_channels, output_channels, size)\n\n    def forward(self, feats):\n        priors = [stage(feats) for stage in self.stages]\n        context = [self.up_dr(feats)]\n        for i in range(0, len(priors)):\n            context += [priors[i]]\n        output = self.conv_bn_dropout(torch.cat(context, 1))\n        return output\n'"
pytorch_toolbelt/modules/pooling.py,5,"b'""""""Implementation of different pooling modules\n\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\n    ""GlobalAvgPool2d"",\n    ""GlobalMaxPool2d"",\n    ""GlobalWeightedAvgPool2d"",\n    ""GWAP"",\n    ""RMSPool"",\n    ""MILCustomPoolingModule"",\n    ""GlobalRankPooling"",\n]\n\n\nclass GlobalAvgPool2d(nn.Module):\n    def __init__(self, flatten=False):\n        """"""Global average pooling over the input\'s spatial dimensions""""""\n        super(GlobalAvgPool2d, self).__init__()\n        self.flatten = flatten\n\n    def forward(self, x):  # skipcq: PYL-W0221\n        x = F.adaptive_avg_pool2d(x, output_size=1)\n        if self.flatten:\n            x = x.view(x.size(0), x.size(1))\n        return x\n\n\nclass GlobalMaxPool2d(nn.Module):\n    def __init__(self, flatten=False):\n        """"""Global average pooling over the input\'s spatial dimensions""""""\n        super(GlobalMaxPool2d, self).__init__()\n        self.flatten = flatten\n\n    def forward(self, x):  # skipcq: PYL-W0221\n        x = F.adaptive_max_pool2d(x, output_size=1)\n        if self.flatten:\n            x = x.view(x.size(0), x.size(1))\n        return x\n\n\nclass GlobalWeightedAvgPool2d(nn.Module):\n    """"""\n    Global Weighted Average Pooling from paper ""Global Weighted Average\n    Pooling Bridges Pixel-level Localization and Image-level Classification""\n    """"""\n\n    def __init__(self, features: int, flatten=False):\n        super().__init__()\n        self.conv = nn.Conv2d(features, 1, kernel_size=1, bias=True)\n        self.flatten = flatten\n\n    def fscore(self, x):\n        m = self.conv(x)\n        m = m.sigmoid().exp()\n        return m\n\n    def norm(self, x: torch.Tensor):\n        return x / x.sum(dim=[2, 3], keepdim=True)\n\n    def forward(self, x):  # skipcq: PYL-W0221\n        input_x = x\n        x = self.fscore(x)\n        x = self.norm(x)\n        x = x * input_x\n        x = x.sum(dim=[2, 3], keepdim=not self.flatten)\n        return x\n\n\nGWAP = GlobalWeightedAvgPool2d\n\n\nclass RMSPool(nn.Module):\n    """"""\n    Root mean square pooling\n    """"""\n\n    def __init__(self):\n        super().__init__()\n        self.avg_pool = GlobalAvgPool2d()\n\n    def forward(self, x):  # skipcq: PYL-W0221\n        x_mean = x.mean(dim=[2, 3])\n        avg_pool = self.avg_pool((x - x_mean) ** 2)\n        return avg_pool.sqrt()\n\n\nclass MILCustomPoolingModule(nn.Module):\n    def __init__(self, in_channels, out_channels, reduction=4):\n        super().__init__()\n        self.classifier = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.weight_generator = nn.Sequential(\n            nn.BatchNorm2d(in_channels),\n            nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels // reduction, out_channels, kernel_size=1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):  # skipcq: PYL-W0221\n        weight = self.weight_generator(x)\n        loss = self.classifier(x)\n        logits = torch.sum(weight * loss, dim=[2, 3]) / (torch.sum(weight, dim=[2, 3]) + 1e-6)\n        return logits\n\n\nclass GlobalRankPooling(nn.Module):\n    """"""\n    https://arxiv.org/abs/1704.02112\n    """"""\n\n    def __init__(self, num_features, spatial_size, flatten=False):\n        super().__init__()\n        self.conv = nn.Conv1d(num_features, num_features, spatial_size, groups=num_features)\n        self.flatten = flatten\n\n    def forward(self, x: torch.Tensor):  # skipcq: PYL-W0221\n        spatial_size = x.size(2) * x.size(3)\n        assert spatial_size == self.conv.kernel_size[0], (\n            f""Expected spatial size {self.conv.kernel_size[0]}, "" f""got {x.size(2)}x{x.size(3)}""\n        )\n\n        x = x.view(x.size(0), x.size(1), -1)  # Flatten spatial dimensions\n        x_sorted, index = x.topk(spatial_size, dim=2)\n\n        x = self.conv(x_sorted)  # [B, C, 1]\n\n        if self.flatten:\n            x = x.squeeze(2)\n        return x\n'"
pytorch_toolbelt/modules/scse.py,1,"b'""""""Implementation of the CoordConv modules from\n""Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks""\n\nOriginal paper: https://arxiv.org/abs/1803.02579\n""""""\n\nfrom torch import nn, Tensor\nfrom torch.nn import functional as F\n\n__all__ = [""ChannelGate2d"", ""SpatialGate2d"", ""ChannelSpatialGate2d"", ""SpatialGate2dV2"", ""ChannelSpatialGate2dV2""]\n\n\nclass ChannelGate2d(nn.Module):\n    """"""\n    Channel Squeeze module\n    """"""\n\n    def __init__(self, channels):\n        super().__init__()\n        self.squeeze = nn.Conv2d(channels, 1, kernel_size=1, padding=0)\n\n    def forward(self, x: Tensor):  # skipcq: PYL-W0221\n        module_input = x\n        x = self.squeeze(x)\n        x = x.sigmoid()\n        return module_input * x\n\n\nclass SpatialGate2d(nn.Module):\n    """"""\n    Spatial squeeze module\n    """"""\n\n    def __init__(self, channels, reduction=None, squeeze_channels=None):\n        """"""\n        Instantiate module\n\n        :param channels: Number of input channels\n        :param reduction: Reduction factor\n        :param squeeze_channels: Number of channels in squeeze block.\n        """"""\n        super().__init__()\n        assert reduction or squeeze_channels, ""One of \'reduction\' and \'squeeze_channels\' must be set""\n        assert not (reduction and squeeze_channels), ""\'reduction\' and \'squeeze_channels\' are mutually exclusive""\n\n        if squeeze_channels is None:\n            squeeze_channels = max(1, channels // reduction)\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.squeeze = nn.Conv2d(channels, squeeze_channels, kernel_size=1)\n        self.expand = nn.Conv2d(squeeze_channels, channels, kernel_size=1)\n\n    def forward(self, x: Tensor):  # skipcq: PYL-W0221\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.squeeze(x)\n        x = F.relu(x, inplace=True)\n        x = self.expand(x)\n        x = x.sigmoid()\n        return module_input * x\n\n\nclass ChannelSpatialGate2d(nn.Module):\n    """"""\n    Concurrent Spatial and Channel Squeeze & Excitation\n    """"""\n\n    def __init__(self, channels, reduction=4):\n        super().__init__()\n        self.channel_gate = ChannelGate2d(channels)\n        self.spatial_gate = SpatialGate2d(channels, reduction=reduction)\n\n    def forward(self, x):  # skipcq: PYL-W0221\n        return self.channel_gate(x) + self.spatial_gate(x)\n\n\nclass SpatialGate2dV2(nn.Module):\n    """"""\n    Spatial Squeeze and Channel Excitation module\n    """"""\n\n    def __init__(self, channels, reduction=4):\n        super().__init__()\n        squeeze_channels = max(1, channels // reduction)\n        self.squeeze = nn.Conv2d(channels, squeeze_channels, kernel_size=1, padding=0)\n        self.conv = nn.Conv2d(squeeze_channels, squeeze_channels, kernel_size=7, dilation=3, padding=3 * 3)\n        self.expand = nn.Conv2d(squeeze_channels, channels, kernel_size=1, padding=0)\n\n    def forward(self, x: Tensor):  # skipcq: PYL-W0221\n        module_input = x\n\n        x = self.squeeze(x)\n        x = self.conv(x)\n        x = F.relu(x, inplace=True)\n        x = self.expand(x)\n        x = x.sigmoid()\n        return module_input * x\n\n\nclass ChannelSpatialGate2dV2(nn.Module):\n    def __init__(self, channels, reduction=4):\n        super().__init__()\n        self.channel_gate = ChannelGate2d(channels)\n        self.spatial_gate = SpatialGate2dV2(channels, reduction)\n\n    def forward(self, x):  # skipcq: PYL-W0221\n        return self.channel_gate(x) + self.spatial_gate(x)\n'"
pytorch_toolbelt/modules/simple.py,0,"b'from torch import nn\n\n__all__ = [""conv1x1""]\n\n\ndef conv1x1(in_channels: int, out_channels: int, groups=1, bias=True) -> nn.Conv2d:\n    return nn.Conv2d(in_channels, out_channels, kernel_size=1, groups=groups, bias=bias)\n'"
pytorch_toolbelt/modules/srm.py,3,"b'import torch\nfrom torch import nn\n\n\nclass SRMLayer(nn.Module):\n    """"""An implementation of SRM block, proposed in\n    ""SRM : A Style-based Recalibration Module for Convolutional Neural Networks"".\n\n    """"""\n\n    def __init__(self, channels: int):\n        super(SRMLayer, self).__init__()\n\n        # Equal to torch.einsum(\'bck,ck->bc\', A, B)\n        self.cfc = nn.Conv1d(channels, channels, kernel_size=2, bias=False, groups=channels)\n        self.bn = nn.BatchNorm1d(channels)\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n\n        # Style pooling\n        mean = x.view(b, c, -1).mean(-1).unsqueeze(-1)\n        std = x.view(b, c, -1).std(-1).unsqueeze(-1)\n        u = torch.cat((mean, std), -1)  # (b, c, 2)\n\n        # Style integration\n        z = self.cfc(u)  # (b, c, 1)\n        z = self.bn(z)\n        g = torch.sigmoid(z)\n        g = g.view(b, c, 1, 1)\n\n        return x * g.expand_as(x)\n'"
pytorch_toolbelt/modules/unet.py,3,"b'from typing import Optional\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom ..modules.activations import ABN\n\n__all__ = [""UnetBlock"", ""UnetCentralBlock"", ""UnetDecoderBlock""]\n\n\nclass UnetBlock(nn.Module):\n    """"""\n    Vanilla U-Net block containing of two convolutions interleaved with batch-norm and RELU\n    """"""\n\n    def __init__(self, in_channels: int, out_channels: int, abn_block=ABN):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False)\n        self.abn1 = abn_block(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False)\n        self.abn2 = abn_block(out_channels)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.abn1(x)\n        x = self.conv2(x)\n        x = self.abn2(x)\n        return x\n\n\nclass UnetCentralBlock(nn.Module):\n    def __init__(self, in_dec_filters: int, out_filters: int, abn_block=ABN):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_dec_filters, out_filters, kernel_size=3, padding=1, stride=2, bias=False)\n        self.abn1 = abn_block(out_filters)\n        self.conv2 = nn.Conv2d(out_filters, out_filters, kernel_size=3, padding=1, bias=False)\n        self.abn2 = abn_block(out_filters)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.abn1(x)\n        x = self.conv2(x)\n        x = self.abn2(x)\n        return x\n\n\nclass UnetDecoderBlock(nn.Module):\n    """"""\n    """"""\n\n    def __init__(\n        self,\n        in_dec_filters: int,\n        in_enc_filters: int,\n        out_filters: int,\n        abn_block=ABN,\n        dropout_rate=0.0,\n        scale_factor=None,\n        scale_mode=""nearest"",\n        align_corners=None,\n    ):\n        super(UnetDecoderBlock, self).__init__()\n\n        self.scale_factor = scale_factor\n        self.scale_mode = scale_mode\n        self.align_corners = align_corners\n\n        self.conv1 = nn.Conv2d(in_dec_filters + in_enc_filters, out_filters, kernel_size=3, padding=1, bias=False)\n        self.abn1 = abn_block(out_filters)\n\n        self.drop = nn.Dropout2d(dropout_rate, inplace=False)\n\n        self.conv2 = nn.Conv2d(out_filters, out_filters, kernel_size=3, padding=1, bias=False)\n        self.abn2 = abn_block(out_filters)\n\n    def forward(self, x: torch.Tensor, enc: Optional[torch.Tensor] = None) -> torch.Tensor:\n        if self.scale_factor is not None:\n            x = F.interpolate(\n                x, scale_factor=self.scale_factor, mode=self.scale_mode, align_corners=self.align_corners\n            )\n        else:\n            lat_size = enc.size()[2:]\n            x = F.interpolate(x, size=lat_size, mode=self.scale_mode, align_corners=self.align_corners)\n\n        if enc is not None:\n            x = torch.cat([x, enc], dim=1)\n\n        x = self.conv1(x)\n        x = self.abn1(x)\n\n        x = self.drop(x)\n\n        x = self.conv2(x)\n        x = self.abn2(x)\n\n        return x\n'"
pytorch_toolbelt/modules/upsample.py,6,"b'import torch\nfrom torch import nn, Tensor\nfrom math import hypot\n\n__all__ = [\n    ""bilinear_upsample_initializer"",\n    ""icnr_init"",\n    ""DepthToSpaceUpsample2d"",\n    ""BilinearAdditiveUpsample2d"",\n    ""DeconvolutionUpsample2d"",\n    ""ResidualDeconvolutionUpsample2d"",\n]\n\n\ndef bilinear_upsample_initializer(x):\n\n    cc = x.size(2) // 2\n    cr = x.size(3) // 2\n\n    for i in range(x.size(2)):\n        for j in range(x.size(3)):\n            x[..., i, j] = hypot(cc - i, cr - j)\n\n    y = 1 - x / x.sum(dim=(2, 3), keepdim=True)\n    y = y / y.sum(dim=(2, 3), keepdim=True)\n    return y\n\n\ndef icnr_init(tensor: torch.Tensor, upscale_factor=2, initializer=nn.init.kaiming_normal):\n    """"""Fill the input Tensor or Variable with values according to the method\n    described in ""Checkerboard artifact free sub-pixel convolution""\n    - Andrew Aitken et al. (2017), this inizialization should be used in the\n    last convolutional layer before a PixelShuffle operation\n    Args:\n        tensor: an n-dimensional torch.Tensor or autograd.Variable\n        upscale_factor: factor to increase spatial resolution by\n        initializer: inizializer to be used for sub_kernel inizialization\n    Examples:\n        >>> upscale = 8\n        >>> num_classes = 10\n        >>> previous_layer_features = Variable(torch.Tensor(8, 64, 32, 32))\n        >>> conv_shuffle = Conv2d(64, num_classes * (upscale ** 2), 3, padding=1, bias=0)\n        >>> ps = PixelShuffle(upscale)\n        >>> kernel = ICNR(conv_shuffle.weight, scale_factor=upscale)\n        >>> conv_shuffle.weight.data.copy_(kernel)\n        >>> output = ps(conv_shuffle(previous_layer_features))\n        >>> print(output.shape)\n        torch.Size([8, 10, 256, 256])\n    .. _Checkerboard artifact free sub-pixel convolution:\n        https://arxiv.org/abs/1707.02937\n    """"""\n    new_shape = [int(tensor.shape[0] / (upscale_factor ** 2))] + list(tensor.shape[1:])\n    subkernel = torch.zeros(new_shape)\n    subkernel = initializer(subkernel)\n    subkernel = subkernel.transpose(0, 1)\n\n    subkernel = subkernel.contiguous().view(subkernel.shape[0], subkernel.shape[1], -1)\n\n    kernel = subkernel.repeat(1, 1, upscale_factor ** 2)\n\n    transposed_shape = [tensor.shape[1]] + [tensor.shape[0]] + list(tensor.shape[2:])\n    kernel = kernel.contiguous().view(transposed_shape)\n\n    kernel = kernel.transpose(0, 1)\n    return kernel\n\n\nclass DepthToSpaceUpsample2d(nn.Module):\n    """"""\n    NOTE: This block is not fully ready yet. Need to figure out how to correctly initialize\n    default weights to have bilinear upsample identical to OpenCV results\n\n    https://github.com/pytorch/pytorch/pull/5429\n    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n    """"""\n\n    def __init__(self, features: int, scale_factor: int = 2):\n        super().__init__()\n        self.n = 2 ** scale_factor\n        self.conv = nn.Conv2d(features, features * self.n, kernel_size=3, padding=1, bias=False)\n        with torch.no_grad():\n            self.conv.weight.data = icnr_init(\n                self.conv.weight, upscale_factor=scale_factor, initializer=bilinear_upsample_initializer\n            )\n        self.shuffle = nn.PixelShuffle(upscale_factor=scale_factor)\n\n    def forward(self, x: Tensor) -> Tensor:  # skipcq: PYL-W0221\n        x = self.shuffle(self.conv(x))\n        return x\n\n\nclass BilinearAdditiveUpsample2d(nn.Module):\n    """"""\n    https://arxiv.org/abs/1707.05847\n    """"""\n\n    def __init__(self, in_channels: int, scale_factor: int = 2, n: int = 4):\n        super().__init__()\n        if in_channels % n != 0:\n            raise ValueError(f""Number of input channels ({in_channels})must be divisable by n ({n})"")\n\n        self.in_channels = in_channels\n        self.out_channels = in_channels // n\n        self.upsample = nn.UpsamplingBilinear2d(scale_factor=scale_factor)\n        self.n = n\n\n    def forward(self, x: Tensor) -> Tensor:  # skipcq: PYL-W0221\n        x = self.upsample(x)\n        n, c, h, w = x.size()\n        x = x.reshape(n, c // self.n, self.n, h, w).mean(2)\n        return x\n\n\nclass DeconvolutionUpsample2d(nn.Module):\n    def __init__(self, in_channels: int, n=4):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = in_channels // n\n        self.conv = nn.ConvTranspose2d(in_channels, in_channels // n, kernel_size=3, padding=1, stride=2)\n\n    def forward(self, x: Tensor) -> Tensor:  # skipcq: PYL-W0221\n        return self.conv(x)\n\n\nclass ResidualDeconvolutionUpsample2d(nn.Module):\n    def __init__(self, in_channels, scale_factor=2, n=4):\n        if scale_factor != 2:\n            raise NotImplementedError(""Scale factor other than 2 is not implemented"")\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = in_channels // n\n        self.conv = nn.ConvTranspose2d(\n            in_channels, in_channels // n, kernel_size=3, padding=1, stride=scale_factor, output_padding=1\n        )\n        self.residual = BilinearAdditiveUpsample2d(in_channels, scale_factor=scale_factor, n=n)\n\n    def forward(self, x: Tensor) -> Tensor:  # skipcq: PYL-W0221\n        residual_up = self.residual(x)\n        return self.conv(x, output_size=residual_up.size()) + residual_up\n'"
pytorch_toolbelt/optimization/__init__.py,0,b''
pytorch_toolbelt/optimization/functional.py,0,"b'from typing import Optional, Iterator\nfrom torch import nn\n\n__all__ = [""get_lr_decay_parameters"", ""get_optimizable_parameters"", ""freeze_model""]\n\n\ndef get_lr_decay_parameters(parameters, learning_rate: float, groups: dict):\n    custom_lr_parameters = dict(\n        (group_name, {""params"": [], ""lr"": learning_rate * lr_factor}) for (group_name, lr_factor) in groups.items()\n    )\n    custom_lr_parameters[""default""] = {""params"": [], ""lr"": learning_rate}\n\n    for parameter_name, parameter in parameters:\n        matches = False\n        for group_name, lr in groups.items():\n            if str.startswith(parameter_name, group_name):\n                custom_lr_parameters[group_name][""params""].append(parameter)\n                matches = True\n                break\n\n        if not matches:\n            custom_lr_parameters[""default""][""params""].append(parameter)\n\n    return custom_lr_parameters.values()\n\n\ndef get_optimizable_parameters(model: nn.Module) -> Iterator[nn.Parameter]:\n    """"""\n    Return list of parameters with requires_grad=True from the model.\n    This function allows easily get all parameters that should be optimized.\n    :param model: An instance of nn.Module.\n    :return: Parameters with requires_grad=True.\n    """"""\n    return filter(lambda x: x.requires_grad, model.parameters())\n\n\ndef freeze_model(module: nn.Module, freeze_parameters: Optional[bool] = True, freeze_bn: Optional[bool] = True):\n    """"""\n    Change \'requires_grad\' value for module and it\'s child modules and\n    optionally freeze batchnorm modules.\n    :param module: Module to change\n    :param freeze_parameters: True to freeze parameters; False - to enable parameters optimization.\n        If None - current state is not changed.\n    :param freeze_bn: True to freeze batch norm; False - to enable BatchNorm updates.\n        If None - current state is not changed.\n    :return: None\n    """"""\n    bn_types = nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.SyncBatchNorm\n\n    if freeze_parameters is not None:\n        for param in module.parameters():\n            param.requires_grad = not freeze_parameters\n\n    if freeze_bn is not None:\n        if isinstance(module, bn_types):\n            module.track_running_stats = not freeze_bn\n\n        for m in module.modules():\n            if isinstance(m, bn_types):\n                module.track_running_stats = not freeze_bn\n'"
pytorch_toolbelt/optimization/lr_schedules.py,3,"b'import math\n\nimport numpy as np\nfrom torch import nn\nfrom torch.optim.lr_scheduler import _LRScheduler, LambdaLR\nfrom torch.optim.optimizer import Optimizer\n\n__all__ = [""OnceCycleLR"", ""CosineAnnealingLRWithDecay"", ""PolyLR""]\n\n\ndef set_learning_rate(optimizer, lr):\n    for i, param_group in enumerate(optimizer.param_groups):\n        param_group[""lr""] = lr\n\n\nclass OnceCycleLR(_LRScheduler):\n    def __init__(self, optimizer, epochs, min_lr_factor=0.05, max_lr=1.0):\n        half_epochs = epochs // 2\n        decay_epochs = epochs * 0.05\n\n        lr_grow = np.linspace(min_lr_factor, max_lr, half_epochs)\n        lr_down = np.linspace(max_lr, min_lr_factor, half_epochs - decay_epochs)\n        lr_decay = np.linspace(min_lr_factor, min_lr_factor * 0.01, decay_epochs)\n        self.learning_rates = np.concatenate((lr_grow, lr_down, lr_decay)) / max_lr\n        super().__init__(optimizer)\n\n    def get_lr(self):\n        return [base_lr * self.learning_rates[self.last_epoch] for base_lr in self.base_lrs]\n\n\nclass CosineAnnealingLRWithDecay(_LRScheduler):\n    r""""""Set the learning rate of each parameter group using a cosine annealing\n    schedule, where :math:`\\eta_{max}` is set to the initial lr and\n    :math:`T_{cur}` is the number of epochs since the last restart in SGDR:\n\n    .. math::\n\n        \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 +\n        \\cos(\\frac{T_{cur}}{T_{max}}\\pi))\n\n    When last_epoch=-1, sets initial lr as lr.\n\n    It has been proposed in\n    `SGDR: Stochastic Gradient Descent with Warm Restarts`_. Note that this only\n    implements the cosine annealing part of SGDR, and not the restarts.\n\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        T_max (int): Maximum number of iterations.\n        eta_min (float): Minimum learning rate. Default: 0.\n        last_epoch (int): The index of last epoch. Default: -1.\n\n    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n        https://arxiv.org/abs/1608.03983\n    """"""\n\n    def __init__(self, optimizer, T_max, gamma, eta_min=0, last_epoch=-1):\n        self.gamma = gamma\n        self.T_max = T_max\n        self.eta_min = eta_min\n        super(CosineAnnealingLRWithDecay, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        def compute_lr(base_lr):\n            return (\n                self.eta_min\n                + (base_lr * self.gamma ** self.last_epoch - self.eta_min)\n                * (1 + math.cos(math.pi * self.last_epoch / self.T_max))\n                / 2\n            )\n\n        return [compute_lr(base_lr) for base_lr in self.base_lrs]\n\n\nclass PolyLR(LambdaLR):\n    def __init__(self, optimizer: Optimizer, max_epoch, gamma=0.9):\n        def poly_lr(epoch):\n            return (1.0 - float(epoch) / max_epoch) ** gamma\n\n        super().__init__(optimizer, poly_lr)\n\n\nif __name__ == ""__main__"":\n    import matplotlib as mpl\n\n    mpl.use(""module://backend_interagg"")\n    import matplotlib.pyplot as plt\n\n    from torch.optim import SGD, Optimizer\n\n    net = nn.Conv2d(1, 1, 1)\n    opt = SGD(net.parameters(), lr=1e-3)\n\n    epochs = 100\n\n    plt.figure()\n\n    scheduler = OnceCycleLR(opt, epochs, min_lr_factor=0.01)\n    lrs = []\n    for epoch in range(epochs):\n        scheduler.step(epoch)\n        lrs.append(scheduler.get_lr()[0])\n    plt.plot(range(epochs), lrs, label=""1cycle"")\n\n    scheduler = CosineAnnealingLRWithDecay(opt, epochs / 5, gamma=0.99)\n    lrs = []\n    for epoch in range(epochs):\n        scheduler.step(epoch)\n        lrs.append(scheduler.get_lr()[0])\n    plt.plot(range(epochs), lrs, label=""cosine"")\n\n    scheduler = PolyLR(opt, epochs, gamma=0.9)\n    lrs = []\n    for epoch in range(epochs):\n        scheduler.step(epoch)\n        lrs.append(scheduler.get_lr()[0])\n    plt.plot(range(epochs), lrs, label=""poly"")\n\n    plt.legend()\n    plt.show()\n'"
pytorch_toolbelt/utils/__init__.py,0,b'from .fs import *\nfrom .random import *\nfrom .support import *\nfrom .visualization import *\nfrom .torch_utils import *\nfrom .rle import *\nfrom .dataset_utils import *\nfrom .namesgenerator import *\n'
pytorch_toolbelt/utils/dataset_utils.py,1,"b'from typing import Callable, List\n\nfrom torch.utils.data import Dataset, ConcatDataset\n\nfrom .fs import id_from_fname\nfrom .torch_utils import tensor_from_rgb_image, tensor_from_mask_image\nfrom ..inference.tiles import ImageSlicer\n\n\nclass ImageMaskDataset(Dataset):\n    def __init__(\n        self, image_filenames, target_filenames, image_loader, target_loader, transform=None, keep_in_mem=False\n    ):\n        if len(image_filenames) != len(target_filenames):\n            raise ValueError(""Number of images does not corresponds to number of targets"")\n\n        self.image_ids = [id_from_fname(fname) for fname in image_filenames]\n\n        if keep_in_mem:\n            self.images = [image_loader(fname) for fname in image_filenames]\n            self.masks = [target_loader(fname) for fname in target_filenames]\n            self.get_image = lambda x: x\n            self.get_loader = lambda x: x\n        else:\n            self.images = image_filenames\n            self.masks = target_filenames\n            self.get_image = image_loader\n            self.get_loader = target_loader\n\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        image = self.get_image(self.images[index])\n        mask = self.get_loader(self.masks[index])\n\n        data = self.transform(image=image, mask=mask)\n\n        return {\n            ""features"": tensor_from_rgb_image(data[""image""]),\n            ""targets"": tensor_from_mask_image(data[""mask""]).float(),\n            ""image_id"": self.image_ids[index],\n        }\n\n\nclass TiledSingleImageDataset(Dataset):\n    def __init__(\n        self,\n        image_fname: str,\n        mask_fname: str,\n        image_loader: Callable,\n        target_loader: Callable,\n        tile_size,\n        tile_step,\n        image_margin=0,\n        transform=None,\n        target_shape=None,\n        keep_in_mem=False,\n    ):\n        self.image_fname = image_fname\n        self.mask_fname = mask_fname\n        self.image_loader = image_loader\n        self.mask_loader = target_loader\n        self.image = None\n        self.mask = None\n\n        if target_shape is None or keep_in_mem:\n            image = image_loader(image_fname)\n            mask = target_loader(mask_fname)\n            if image.shape[0] != mask.shape[0] or image.shape[1] != mask.shape[1]:\n                raise ValueError(\n                    f""Image size {image.shape} and mask shape {mask.shape} must have equal width and height""\n                )\n\n            target_shape = image.shape\n\n        self.slicer = ImageSlicer(target_shape, tile_size, tile_step, image_margin)\n\n        if keep_in_mem:\n            self.images = self.slicer.split(image)\n            self.masks = self.slicer.split(mask)\n        else:\n            self.images = None\n            self.masks = None\n\n        self.transform = transform\n        self.image_ids = [\n            id_from_fname(image_fname) + f"" [{crop[0]};{crop[1]};{crop[2]};{crop[3]};]"" for crop in self.slicer.crops\n        ]\n\n    def _get_image(self, index):\n        if self.images is None:\n            image = self.image_loader(self.image_fname)\n            image = self.slicer.cut_patch(image, index)\n        else:\n            image = self.images[index]\n        return image\n\n    def _get_mask(self, index):\n        if self.masks is None:\n            mask = self.mask_loader(self.mask_fname)\n            mask = self.slicer.cut_patch(mask, index)\n        else:\n            mask = self.masks[index]\n        return mask\n\n    def __len__(self):\n        return len(self.slicer.crops)\n\n    def __getitem__(self, index):\n        image = self._get_image(index)\n        mask = self._get_mask(index)\n        data = self.transform(image=image, mask=mask)\n\n        return {\n            ""features"": tensor_from_rgb_image(data[""image""]),\n            ""targets"": tensor_from_mask_image(data[""mask""]).float(),\n            ""image_id"": self.image_ids[index],\n        }\n\n\nclass TiledImageMaskDataset(ConcatDataset):\n    def __init__(\n        self,\n        image_filenames: List[str],\n        target_filenames: List[str],\n        image_loader: Callable,\n        target_loader: Callable,\n        **kwargs,\n    ):\n        if len(image_filenames) != len(target_filenames):\n            raise ValueError(""Number of images does not corresponds to number of targets"")\n\n        datasets = []\n        for image, mask in zip(image_filenames, target_filenames):\n            dataset = TiledSingleImageDataset(image, mask, image_loader, target_loader, **kwargs)\n            datasets.append(dataset)\n        super().__init__(datasets)\n'"
pytorch_toolbelt/utils/distributed.py,8,"b'import pickle\nimport torch\nimport torch.distributed as dist\n\n__all__ = [""is_dist_avail_and_initialized"", ""get_world_size"", ""get_rank"", ""is_main_process"", ""all_gather""]\n\n\ndef is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\n\n\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_main_process():\n    return get_rank() == 0\n\n\ndef all_gather(data):\n    """"""\n    Run all_gather on arbitrary picklable data (not necessarily tensors)\n    Args:\n        data: any picklable object\n    Returns:\n        list[data]: list of data gathered from each rank\n    """"""\n    world_size = get_world_size()\n    if world_size == 1:\n        return [data]\n\n    # serialized to a Tensor\n    buffer = pickle.dumps(data)\n    storage = torch.ByteStorage.from_buffer(buffer)\n    tensor = torch.ByteTensor(storage).to(""cuda"")\n\n    # obtain Tensor size of each rank\n    local_size = torch.tensor([tensor.numel()], device=""cuda"")\n    size_list = [torch.tensor([0], device=""cuda"") for _ in range(world_size)]\n    dist.all_gather(size_list, local_size)\n    size_list = [int(size.item()) for size in size_list]\n    max_size = max(size_list)\n\n    # receiving Tensor from all ranks\n    # we pad the tensor because torch all_gather does not support\n    # gathering tensors of different shapes\n    tensor_list = []\n    for _ in size_list:\n        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=""cuda""))\n    if local_size != max_size:\n        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=""cuda"")\n        tensor = torch.cat((tensor, padding), dim=0)\n    dist.all_gather(tensor_list, tensor)\n\n    data_list = []\n    for size, tensor in zip(size_list, tensor_list):\n        buffer = tensor.cpu().numpy().tobytes()[:size]\n        data_list.append(pickle.loads(buffer))\n\n    return data_list\n'"
pytorch_toolbelt/utils/fs.py,0,"b'""""""Filesystem utilities\n\n""""""\n\nimport glob\nimport os\n\nimport cv2\nimport numpy as np\n\n\ndef has_image_ext(fname: str) -> bool:\n    name, ext = os.path.splitext(fname)\n    return ext.lower() in {"".bmp"", "".png"", "".jpeg"", "".jpg"", "".tiff"", "".tif""}\n\n\ndef find_in_dir(dirname: str):\n    return [os.path.join(dirname, fname) for fname in sorted(os.listdir(dirname))]\n\n\ndef find_images_in_dir(dirname: str):\n    return [fname for fname in find_in_dir(dirname) if has_image_ext(fname)]\n\n\ndef find_in_dir_glob(dirname: str, recursive=False):\n    files = list(glob.iglob(dirname, recursive=recursive))\n    return list(sorted(files))\n\n\ndef id_from_fname(fname: str):\n    return os.path.splitext(os.path.basename(fname))[0]\n\n\ndef change_extension(fname: str, new_ext: str):\n    return os.path.splitext(fname)[0] + new_ext\n\n\ndef auto_file(filename: str, where: str = ""."") -> str:\n    """"""Get a full path to file using it\'s name.\n    This function recisively search for matching filename in @where and returns single match.\n    :param where:\n    :param filename:\n    :return:\n    """"""\n    if os.path.isabs(filename):\n        return filename\n\n    prob = os.path.join(where, filename)\n    if os.path.exists(prob) and os.path.isfile(prob):\n        return prob\n\n    files = list(glob.iglob(os.path.join(where, ""**"", filename), recursive=True))\n    if len(files) == 0:\n        raise FileNotFoundError(""Given file could not be found with recursive search:"" + filename)\n\n    if len(files) > 1:\n        raise FileNotFoundError(\n            ""More than one file matches given filename. Please specify it explicitly:\\n"" + ""\\n"".join(files)\n        )\n\n    return files[0]\n\n\ndef read_rgb_image(fname: str) -> np.ndarray:\n    """"""\n    Read RGB image from filesystem.\n    This function uses PIL to load image since PIL respects EXIF image orientation flag.\n    :param fname: Image file path\n    :return: A numpy array with a loaded image in RGB format\n    """"""\n    from PIL import Image\n\n    im = Image.open(fname)\n    if im.mode != ""RGB"":\n        im = im.convert(""RGB"")\n    image = np.asarray(im)\n    return image\n\n\ndef read_image_as_is(fname: str) -> np.ndarray:\n    image = cv2.imread(fname, cv2.IMREAD_UNCHANGED)\n    if image is None:\n        raise IOError(f\'Cannot read image ""{fname}""\')\n    return image\n'"
pytorch_toolbelt/utils/namesgenerator.py,0,"b'# coding: utf-8\n\n# Docker names generator, Python port\n# https://github.com/shamrin/namesgenerator\n# Copyright (c) 2017 Alexey Shamrin\n# MIT License\n\nimport random\n\nleft = [\n    ""admiring"",\n    ""adoring"",\n    ""affectionate"",\n    ""agitated"",\n    ""amazing"",\n    ""angry"",\n    ""awesome"",\n    ""blissful"",\n    ""boring"",\n    ""brave"",\n    ""clever"",\n    ""cocky"",\n    ""compassionate"",\n    ""competent"",\n    ""condescending"",\n    ""confident"",\n    ""cranky"",\n    ""dazzling"",\n    ""determined"",\n    ""distracted"",\n    ""dreamy"",\n    ""eager"",\n    ""ecstatic"",\n    ""elastic"",\n    ""elated"",\n    ""elegant"",\n    ""eloquent"",\n    ""epic"",\n    ""fervent"",\n    ""festive"",\n    ""flamboyant"",\n    ""focused"",\n    ""friendly"",\n    ""frosty"",\n    ""gallant"",\n    ""gifted"",\n    ""goofy"",\n    ""gracious"",\n    ""happy"",\n    ""hardcore"",\n    ""heuristic"",\n    ""hopeful"",\n    ""hungry"",\n    ""infallible"",\n    ""inspiring"",\n    ""jolly"",\n    ""jovial"",\n    ""keen"",\n    ""kind"",\n    ""laughing"",\n    ""loving"",\n    ""lucid"",\n    ""mystifying"",\n    ""modest"",\n    ""musing"",\n    ""naughty"",\n    ""nervous"",\n    ""nifty"",\n    ""nostalgic"",\n    ""objective"",\n    ""optimistic"",\n    ""peaceful"",\n    ""pedantic"",\n    ""pensive"",\n    ""practical"",\n    ""priceless"",\n    ""quirky"",\n    ""quizzical"",\n    ""relaxed"",\n    ""reverent"",\n    ""romantic"",\n    ""sad"",\n    ""serene"",\n    ""sharp"",\n    ""silly"",\n    ""sleepy"",\n    ""stoic"",\n    ""stupefied"",\n    ""suspicious"",\n    ""tender"",\n    ""thirsty"",\n    ""trusting"",\n    ""unruffled"",\n    ""upbeat"",\n    ""vibrant"",\n    ""vigilant"",\n    ""vigorous"",\n    ""wizardly"",\n    ""wonderful"",\n    ""xenodochial"",\n    ""youthful"",\n    ""zealous"",\n    ""zen"",\n]\n\nright = [\n    # Muhammad ibn J\xc4\x81bir al-\xe1\xb8\xa4arr\xc4\x81n\xc4\xab al-Batt\xc4\x81n\xc4\xab was a founding father of astronomy.\n    # https://en.wikipedia.org/wiki/Mu%E1%B8%A5ammad_ibn_J%C4%81bir_al-%E1%B8%A4arr%C4%81n%C4%AB_al-Batt%C4%81n%C4%AB\n    ""albattani"",\n    # Frances E. Allen, became the first female IBM Fellow in 1989. In 2006, she became the first female recipient of the ACM\'s Turing Award.\n    # https://en.wikipedia.org/wiki/Frances_E._Allen\n    ""allen"",\n    # June Almeida - Scottish virologist who took the first pictures of the rubella virus - https://en.wikipedia.org/wiki/June_Almeida\n    ""almeida"",\n    # Maria Gaetana Agnesi - Italian mathematician, philosopher, theologian and humanitarian.\n    # She was the first woman to write a mathematics handbook and the first woman appointed as a\n    # Mathematics Professor at a University. https://en.wikipedia.org/wiki/Maria_Gaetana_Agnesi\n    ""agnesi"",\n    # Archimedes was a physicist, engineer and mathematician who invented too many things to list them here.\n    # https://en.wikipedia.org/wiki/Archimedes\n    ""archimedes"",\n    # Maria Ardinghelli - Italian translator, mathematician and physicist - https://en.wikipedia.org/wiki/Maria_Ardinghelli\n    ""ardinghelli"",\n    # Aryabhata - Ancient Indian mathematician-astronomer during 476-550 CE https://en.wikipedia.org/wiki/Aryabhata\n    ""aryabhata"",\n    # Wanda Austin - Wanda Austin is the President and CEO of The Aerospace Corporation,\n    # a leading architect for the US security space programs. https://en.wikipedia.org/wiki/Wanda_Austin\n    ""austin"",\n    # Charles Babbage invented the concept of a programmable computer. https://en.wikipedia.org/wiki/Charles_Babbage.\n    ""babbage"",\n    # Stefan Banach - Polish mathematician, was one of the founders of modern functional analysis. https://en.wikipedia.org/wiki/Stefan_Banach\n    ""banach"",\n    # John Bardeen co-invented the transistor - https://en.wikipedia.org/wiki/John_Bardeen\n    ""bardeen"",\n    # Jean Bartik, born Betty Jean Jennings, was one of the original programmers for the ENIAC computer. https://en.wikipedia.org/wiki/Jean_Bartik\n    ""bartik"",\n    # Laura Bassi, the world\'s first female professor https://en.wikipedia.org/wiki/Laura_Bassi\n    ""bassi"",\n    # Hugh Beaver, British engineer, founder of the Guinness Book of World Records https://en.wikipedia.org/wiki/Hugh_Beaver\n    ""beaver"",\n    # Alexander Graham Bell - an eminent Scottish-born scientist, inventor, engineer and innovator who is\n    # credited with inventing the first practical telephone - https://en.wikipedia.org/wiki/Alexander_Graham_Bell\n    ""bell"",\n    # Karl Friedrich Benz - a German automobile engineer. Inventor of the first practical motorcar. https://en.wikipedia.org/wiki/Karl_Benz\n    ""benz"",\n    # Homi J Bhabha - was an Indian nuclear physicist, founding director, and professor of physics at the Tata Institute of Fundamental Research.\n    # Colloquially known as \'father of Indian nuclear programme\'- https://en.wikipedia.org/wiki/Homi_J._Bhabha\n    ""bhabha"",\n    # Bhaskara II - Ancient Indian mathematician-astronomer whose work on calculus predates Newton and Leibniz\n    # by over half a millennium - https://en.wikipedia.org/wiki/Bh%C4%81skara_II#Calculus\n    ""bhaskara"",\n    # Elizabeth Blackwell - American doctor and first American woman to receive a medical degree - https://en.wikipedia.org/wiki/Elizabeth_Blackwell\n    ""blackwell"",\n    # Niels Bohr is the father of quantum theory. https://en.wikipedia.org/wiki/Niels_Bohr.\n    ""bohr"",\n    # Kathleen Booth, she\'s credited with writing the first assembly language. https://en.wikipedia.org/wiki/Kathleen_Booth\n    ""booth"",\n    # Anita Borg - Anita Borg was the founding director of the Institute for Women and Technology (IWT). https://en.wikipedia.org/wiki/Anita_Borg\n    ""borg"",\n    # Satyendra Nath Bose - He provided the foundation for Bose\xe2\x80\x93Einstein statistics and the theory of the Bose\xe2\x80\x93Einstein condensate.\n    # https://en.wikipedia.org/wiki/Satyendra_Nath_Bose\n    ""bose"",\n    # Evelyn Boyd Granville - She was one of the first African-American woman to receive a Ph.D. in mathematics;\n    # she earned it in 1949 from Yale University. https://en.wikipedia.org/wiki/Evelyn_Boyd_Granville\n    ""boyd"",\n    # Brahmagupta - Ancient Indian mathematician during 598-670 CE who gave rules to compute\n    # with zero - https://en.wikipedia.org/wiki/Brahmagupta#Zero\n    ""brahmagupta"",\n    # Walter Houser Brattain co-invented the transistor - https://en.wikipedia.org/wiki/Walter_Houser_Brattain\n    ""brattain"",\n    # Emmett Brown invented time travel. https://en.wikipedia.org/wiki/Emmett_Brown (thanks Brian Goff)\n    ""brown"",\n    # Rachel Carson - American marine biologist and conservationist, her book Silent Spring and other\n    # writings are credited with advancing the global environmental movement. https://en.wikipedia.org/wiki/Rachel_Carson\n    ""carson"",\n    # Subrahmanyan Chandrasekhar - Astrophysicist known for his mathematical theory on different stages and evolution in structures of the stars.\n    # He has won nobel prize for physics - https://en.wikipedia.org/wiki/Subrahmanyan_Chandrasekhar\n    ""chandrasekhar"",\n    # Claude Shannon - The father of information theory and founder of digital circuit design theory. (https://en.wikipedia.org/wiki/Claude_Shannon)\n    ""shannon"",\n    # Joan Clarke - Bletchley Park code breaker during the Second World War who pioneered techniques\n    # that remained top secret for decades. Also an accomplished numismatist https://en.wikipedia.org/wiki/Joan_Clarke\n    ""clarke"",\n    # Jane Colden - American botanist widely considered the first female American botanist - https://en.wikipedia.org/wiki/Jane_Colden\n    ""colden"",\n    # Gerty Theresa Cori - American biochemist who became the third woman\xe2\x80\x94and first American woman\xe2\x80\x94to win a Nobel Prize in science,\n    # and the first woman to be awarded the Nobel Prize in Physiology or Medicine.\n    # Cori was born in Prague. https://en.wikipedia.org/wiki/Gerty_Cori\n    ""cori"",\n    # Seymour Roger Cray was an American electrical engineer and supercomputer architect who designed a series of computers\n    # that were the fastest in the world for decades. https://en.wikipedia.org/wiki/Seymour_Cray\n    ""cray"",\n    # This entry reflects a husband and wife team who worked together:\n    # Joan Curran was a Welsh scientist who developed radar and invented chaff, a radar countermeasure.\n    # https://en.wikipedia.org/wiki/Joan_Curran\n    # Samuel Curran was an Irish physicist who worked alongside his wife during WWII and invented the proximity fuse.\n    # https://en.wikipedia.org/wiki/Samuel_Curran\n    ""curran"",\n    # Marie Curie discovered radioactivity. https://en.wikipedia.org/wiki/Marie_Curie.\n    ""curie"",\n    # Charles Darwin established the principles of natural evolution. https://en.wikipedia.org/wiki/Charles_Darwin.\n    ""darwin"",\n    # Leonardo Da Vinci invented too many things to list here. https://en.wikipedia.org/wiki/Leonardo_da_Vinci.\n    ""davinci"",\n    # Edsger Wybe Dijkstra was a Dutch computer scientist and mathematical scientist. https://en.wikipedia.org/wiki/Edsger_W._Dijkstra.\n    ""dijkstra"",\n    # Donna Dubinsky - played an integral role in the development of personal digital assistants (PDAs)\n    # serving as CEO of Palm, Inc. and co-founding Handspring. https://en.wikipedia.org/wiki/Donna_Dubinsky\n    ""dubinsky"",\n    # Annie Easley - She was a leading member of the team which developed software for the Centaur rocket\n    # stage and one of the first African-Americans in her field. https://en.wikipedia.org/wiki/Annie_Easley\n    ""easley"",\n    # Thomas Alva Edison, prolific inventor https://en.wikipedia.org/wiki/Thomas_Edison\n    ""edison"",\n    # Albert Einstein invented the general theory of relativity. https://en.wikipedia.org/wiki/Albert_Einstein\n    ""einstein"",\n    # Gertrude Elion - American biochemist, pharmacologist and the 1988 recipient of\n    # the Nobel Prize in Medicine - https://en.wikipedia.org/wiki/Gertrude_Elion\n    ""elion"",\n    # Douglas Engelbart gave the mother of all demos: https://en.wikipedia.org/wiki/Douglas_Engelbart\n    ""engelbart"",\n    # Euclid invented geometry. https://en.wikipedia.org/wiki/Euclid\n    ""euclid"",\n    # Leonhard Euler invented large parts of modern mathematics. https://de.wikipedia.org/wiki/Leonhard_Euler\n    ""euler"",\n    # Pierre de Fermat pioneered several aspects of modern mathematics. https://en.wikipedia.org/wiki/Pierre_de_Fermat\n    ""fermat"",\n    # Enrico Fermi invented the first nuclear reactor. https://en.wikipedia.org/wiki/Enrico_Fermi.\n    ""fermi"",\n    # Richard Feynman was a key contributor to quantum mechanics and particle physics. https://en.wikipedia.org/wiki/Richard_Feynman\n    ""feynman"",\n    # Benjamin Franklin is famous for his experiments in electricity and the invention of the lightning rod.\n    ""franklin"",\n    # Galileo was a founding father of modern astronomy, and faced politics and obscurantism\n    # to establish scientific truth.  https://en.wikipedia.org/wiki/Galileo_Galilei\n    ""galileo"",\n    # William Henry \'Bill\' Gates III is an American business magnate, philanthropist, investor,\n    # computer programmer, and inventor. https://en.wikipedia.org/wiki/Bill_Gates\n    ""gates"",\n    # Adele Goldberg, was one of the designers and developers of the Smalltalk language.\n    # https://en.wikipedia.org/wiki/Adele_Goldberg_(computer_scientist)\n    ""goldberg"",\n    # Adele Goldstine, born Adele Katz, wrote the complete technical description for the\n    # first electronic digital computer, ENIAC. https://en.wikipedia.org/wiki/Adele_Goldstine\n    ""goldstine"",\n    # Shafi Goldwasser is a computer scientist known for creating theoretical foundations\n    # of modern cryptography. Winner of 2012 ACM Turing Award. https://en.wikipedia.org/wiki/Shafi_Goldwasser\n    ""goldwasser"",\n    # James Golick, all around gangster.\n    ""golick"",\n    # Jane Goodall - British primatologist, ethologist, and anthropologist who is considered\n    # to be the world\'s foremost expert on chimpanzees - https://en.wikipedia.org/wiki/Jane_Goodall\n    ""goodall"",\n    # Lois Haibt - American computer scientist, part of the team at IBM that developed FORTRAN - https://en.wikipedia.org/wiki/Lois_Haibt\n    ""haibt"",\n    # Margaret Hamilton - Director of the Software Engineering Division of the MIT Instrumentation Laboratory,\n    # which developed on-board flight software for the Apollo space program.\n    # https://en.wikipedia.org/wiki/Margaret_Hamilton_(scientist)\n    ""hamilton"",\n    # Stephen Hawking pioneered the field of cosmology by combining general relativity and\n    # quantum mechanics. https://en.wikipedia.org/wiki/Stephen_Hawking\n    ""hawking"",\n    # Werner Heisenberg was a founding father of quantum mechanics. https://en.wikipedia.org/wiki/Werner_Heisenberg\n    ""heisenberg"",\n    # Grete Hermann was a German philosopher noted for her philosophical work on the foundations of\n    # quantum mechanics. https://en.wikipedia.org/wiki/Grete_Hermann\n    ""hermann"",\n    # Jaroslav Heyrovsk\xc3\xbd was the inventor of the polarographic method, father of the electroanalytical method, and recipient of the Nobel Prize in 1959.\n    # His main field of work was polarography. https://en.wikipedia.org/wiki/Jaroslav_Heyrovsk%C3%BD\n    ""heyrovsky"",\n    # Dorothy Hodgkin was a British biochemist, credited with the development of protein crystallography.\n    # She was awarded the Nobel Prize in Chemistry in 1964. https://en.wikipedia.org/wiki/Dorothy_Hodgkin\n    ""hodgkin"",\n    # Erna Schneider Hoover revolutionized modern communication by inventing a computerized telephone switching method.\n    # https://en.wikipedia.org/wiki/Erna_Schneider_Hoover\n    ""hoover"",\n    # Grace Hopper developed the first compiler for a computer programming language and  is credited with popularizing\n    # the term \'debugging\' for fixing computer glitches. https://en.wikipedia.org/wiki/Grace_Hopper\n    ""hopper"",\n    # Frances Hugle, she was an American scientist, engineer, and inventor who contributed to the understanding of semiconductors,\n    # integrated circuitry, and the unique electrical principles of microscopic materials.\n    # https://en.wikipedia.org/wiki/Frances_Hugle\n    ""hugle"",\n    # Hypatia - Greek Alexandrine Neoplatonist philosopher in Egypt who was one of the earliest\n    # mothers of mathematics - https://en.wikipedia.org/wiki/Hypatia\n    ""hypatia"",\n    # Mary Jackson, American mathematician and aerospace engineer who earned the highest title within\n    # NASA\'s engineering department - https://en.wikipedia.org/wiki/Mary_Jackson_(engineer)\n    ""jackson"",\n    # Yeong-Sil Jang was a Korean scientist and astronomer during the Joseon Dynasty;\n    # he invented the first metal printing press and water gauge. https://en.wikipedia.org/wiki/Jang_Yeong-sil\n    ""jang"",\n    # Betty Jennings - one of the original programmers of the ENIAC.\n    # https://en.wikipedia.org/wiki/ENIAC - https://en.wikipedia.org/wiki/Jean_Bartik\n    ""jennings"",\n    # Mary Lou Jepsen, was the founder and chief technology officer of One Laptop Per Child (OLPC), and the founder of Pixel Qi.\n    # https://en.wikipedia.org/wiki/Mary_Lou_Jepsen\n    ""jepsen"",\n    # Katherine Coleman Goble Johnson - American physicist and mathematician contributed to the NASA.\n    # https://en.wikipedia.org/wiki/Katherine_Johnson\n    ""johnson"",\n    # Ir\xc3\xa8ne Joliot-Curie - French scientist who was awarded the Nobel Prize for Chemistry in 1935.\n    # Daughter of Marie and Pierre Curie. https://en.wikipedia.org/wiki/Ir%C3%A8ne_Joliot-Curie\n    ""joliot"",\n    # Karen Sp\xc3\xa4rck Jones came up with the concept of inverse document frequency,\n    # which is used in most search engines today. https://en.wikipedia.org/wiki/Karen_Sp%C3%A4rck_Jones\n    ""jones"",\n    # A. P. J. Abdul Kalam - is an Indian scientist aka Missile Man of India for his work on the development of\n    # ballistic missile and launch vehicle technology - https://en.wikipedia.org/wiki/A._P._J._Abdul_Kalam\n    ""kalam"",\n    # Susan Kare, created the icons and many of the interface elements for the original Apple Macintosh in the 1980s,\n    # and was an original employee of NeXT, working as the Creative Director.\n    # https://en.wikipedia.org/wiki/Susan_Kare\n    ""kare"",\n    # Mary Kenneth Keller, Sister Mary Kenneth Keller became the first American woman to earn a PhD in Computer Science in 1965.\n    # https://en.wikipedia.org/wiki/Mary_Kenneth_Keller\n    ""keller"",\n    # Johannes Kepler, German astronomer known for his three laws of planetary motion -\n    # https://en.wikipedia.org/wiki/Johannes_Kepler\n    ""kepler"",\n    # Har Gobind Khorana - Indian-American biochemist who shared the 1968 Nobel Prize for Physiology -\n    # https://en.wikipedia.org/wiki/Har_Gobind_Khorana\n    ""khorana"",\n    # Jack Kilby invented silicone integrated circuits and gave Silicon Valley its name. -\n    # https://en.wikipedia.org/wiki/Jack_Kilby\n    ""kilby"",\n    # Maria Kirch - German astronomer and first woman to discover a comet -\n    # https://en.wikipedia.org/wiki/Maria_Margarethe_Kirch\n    ""kirch"",\n    # Donald Knuth - American computer scientist, author of \'The Art of Computer Programming\' and creator of the TeX typesetting system.\n    # https://en.wikipedia.org/wiki/Donald_Knuth\n    ""knuth"",\n    # Sophie Kowalevski - Russian mathematician responsible for important original contributions to analysis,\n    # differential equations and mechanics -\n    # https://en.wikipedia.org/wiki/Sofia_Kovalevskaya\n    ""kowalevski"",\n    # Marie-Jeanne de Lalande - French astronomer, mathematician and cataloguer of stars -\n    # https://en.wikipedia.org/wiki/Marie-Jeanne_de_Lalande\n    ""lalande"",\n    # Hedy Lamarr - Actress and inventor. The principles of her work are now incorporated into modern Wi-Fi, CDMA and Bluetooth technology.\n    # https://en.wikipedia.org/wiki/Hedy_Lamarr\n    ""lamarr"",\n    # Leslie B. Lamport - American computer scientist. Lamport is best known for his seminal work in distributed systems and was the winner of the 2013 Turing Award.\n    # https://en.wikipedia.org/wiki/Leslie_Lamport\n    ""lamport"",\n    # Mary Leakey - British paleoanthropologist who discovered the first fossilized Proconsul skull -\n    # https://en.wikipedia.org/wiki/Mary_Leakey\n    ""leakey"",\n    # Henrietta Swan Leavitt - she was an American astronomer who discovered the relation between the luminosity and the period of Cepheid variable stars.\n    # https://en.wikipedia.org/wiki/Henrietta_Swan_Leavitt\n    ""leavitt"",\n    # Daniel Lewin -  Mathematician, Akamai co-founder, soldier, 9/11 victim-- Developed optimization techniques for routing traffic on the internet.\n    # Died attempting to stop the 9-11 hijackers.\n    # https://en.wikipedia.org/wiki/Daniel_Lewin\n    ""lewin"",\n    # Ruth Lichterman - one of the original programmers of the ENIAC. https://en.wikipedia.org/wiki/ENIAC - https://en.wikipedia.org/wiki/Ruth_Teitelbaum\n    ""lichterman"",\n    # Barbara Liskov - co-developed the Liskov substitution principle. Liskov was also the winner of the Turing Prize in 2008. -\n    # https://en.wikipedia.org/wiki/Barbara_Liskov\n    ""liskov"",\n    # Ada Lovelace invented the first algorithm. https://en.wikipedia.org/wiki/Ada_Lovelace (thanks James Turnbull)\n    ""lovelace"",\n    # Auguste and Louis Lumi\xc3\xa8re - the first filmmakers in history - https://en.wikipedia.org/wiki/Auguste_and_Louis_Lumi%C3%A8re\n    ""lumiere"",\n    # Mahavira - Ancient Indian mathematician during 9th century AD who discovered basic algebraic identities -\n    # https://en.wikipedia.org/wiki/Mah%C4%81v%C4%ABra_(mathematician)\n    ""mahavira"",\n    # Maria Mayer - American theoretical physicist and Nobel laureate in Physics for proposing the nuclear\n    # shell model of the atomic nucleus - https://en.wikipedia.org/wiki/Maria_Mayer\n    ""mayer"",\n    # John McCarthy invented LISP: https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)\n    ""mccarthy"",\n    # Barbara McClintock - a distinguished American cytogeneticist, 1983 Nobel Laureate in Physiology or Medicine for discovering transposons.\n    # https://en.wikipedia.org/wiki/Barbara_McClintock\n    ""mcclintock"",\n    # Malcolm McLean invented the modern shipping container: https://en.wikipedia.org/wiki/Malcom_McLean\n    ""mclean"",\n    # Kay McNulty - one of the original programmers of the ENIAC. https://en.wikipedia.org/wiki/ENIAC -\n    # https://en.wikipedia.org/wiki/Kathleen_Antonelli\n    ""mcnulty"",\n    # Lise Meitner - Austrian/Swedish physicist who was involved in the discovery of nuclear fission.\n    # The element meitnerium is named after her - https://en.wikipedia.org/wiki/Lise_Meitner\n    ""meitner"",\n    # Carla Meninsky, was the game designer and programmer for Atari 2600 games Dodge \'Em and Warlords.\n    # https://en.wikipedia.org/wiki/Carla_Meninsky\n    ""meninsky"",\n    # Johanna Mestorf - German prehistoric archaeologist and first female museum director in Germany -\n    # https://en.wikipedia.org/wiki/Johanna_Mestorf\n    ""mestorf"",\n    # Marvin Minsky - Pioneer in Artificial Intelligence, co-founder of the MIT\'s AI Lab, won the Turing Award in 1969.\n    # https://en.wikipedia.org/wiki/Marvin_Minsky\n    ""minsky"",\n    # Maryam Mirzakhani - an Iranian mathematician and the first woman to win the Fields Medal.\n    # https://en.wikipedia.org/wiki/Maryam_Mirzakhani\n    ""mirzakhani"",\n    # Samuel Morse - contributed to the invention of a single-wire telegraph system based on\n    # European telegraphs and was a co-developer of the Morse code -\n    # https://en.wikipedia.org/wiki/Samuel_Morse\n    ""morse"",\n    # Ian Murdock - founder of the Debian project - https://en.wikipedia.org/wiki/Ian_Murdock\n    ""murdock"",\n    # John von Neumann - todays computer architectures are based on the von Neumann architecture.\n    # https://en.wikipedia.org/wiki/Von_Neumann_architecture\n    ""neumann"",\n    # Isaac Newton invented classic mechanics and modern optics. https://en.wikipedia.org/wiki/Isaac_Newton\n    ""newton"",\n    # Florence Nightingale, more prominently known as a nurse, was also the first female member of the\n    # Royal Statistical Society and a pioneer in statistical graphics\n    # https://en.wikipedia.org/wiki/Florence_Nightingale#Statistics_and_sanitary_reform\n    ""nightingale"",\n    # Alfred Nobel - a Swedish chemist, engineer, innovator, and armaments manufacturer (inventor of dynamite) -\n    # https://en.wikipedia.org/wiki/Alfred_Nobel\n    ""nobel"",\n    # Emmy Noether, German mathematician. Noether\'s Theorem is named after her. https://en.wikipedia.org/wiki/Emmy_Noether\n    ""noether"",\n    # Poppy Northcutt. Poppy Northcutt was the first woman to work as part of NASA\xe2\x80\x99s Mission Control.\n    # http://www.businessinsider.com/poppy-northcutt-helped-apollo-astronauts-2014-12?op=1\n    ""northcutt"",\n    # Robert Noyce invented silicone integrated circuits and gave Silicon Valley its name. -\n    # https://en.wikipedia.org/wiki/Robert_Noyce\n    ""noyce"",\n    # Panini - Ancient Indian linguist and grammarian from 4th century CE who worked on the world\'s first formal system -\n    # https://en.wikipedia.org/wiki/P%C4%81%E1%B9%87ini#Comparison_with_modern_formal_systems\n    ""panini"",\n    # Ambroise Pare invented modern surgery. https://en.wikipedia.org/wiki/Ambroise_Par%C3%A9\n    ""pare"",\n    # Louis Pasteur discovered vaccination, fermentation and pasteurization. https://en.wikipedia.org/wiki/Louis_Pasteur.\n    ""pasteur"",\n    # Cecilia Payne-Gaposchkin was an astronomer and astrophysicist who, in 1925, proposed\n    # in her Ph.D. thesis an explanation for the composition of stars in terms of the relative abundances of hydrogen and helium.\n    # https://en.wikipedia.org/wiki/Cecilia_Payne-Gaposchkin\n    ""payne"",\n    # Radia Perlman is a software designer and network engineer and most famous for her\n    # invention of the spanning-tree protocol (STP). https://en.wikipedia.org/wiki/Radia_Perlman\n    ""perlman"",\n    # Rob Pike was a key contributor to Unix, Plan 9, the X graphic system, utf-8, and the Go programming language.\n    # https://en.wikipedia.org/wiki/Rob_Pike\n    ""pike"",\n    # Henri Poincar\xc3\xa9 made fundamental contributions in several fields of mathematics.\n    # https://en.wikipedia.org/wiki/Henri_Poincar%C3%A9\n    ""poincare"",\n    # Laura Poitras is a director and producer whose work, made possible by open source crypto tools, advances\n    # the causes of truth and freedom of information by reporting disclosures by\n    # whistleblowers such as Edward Snowden. https://en.wikipedia.org/wiki/Laura_Poitras\n    ""poitras"",\n    # Claudius Ptolemy - a Greco-Egyptian writer of Alexandria, known as a mathematician, astronomer, geographer, astrologer,\n    # and poet of a single epigram in the Greek Anthology - https://en.wikipedia.org/wiki/Ptolemy\n    ""ptolemy"",\n    # C. V. Raman - Indian physicist who won the Nobel Prize in 1930 for proposing the Raman effect. -\n    # https://en.wikipedia.org/wiki/C._V._Raman\n    ""raman"",\n    # Srinivasa Ramanujan - Indian mathematician and autodidact who made extraordinary contributions to\n    # mathematical analysis, number theory, infinite series, and continued fractions.\n    # https://en.wikipedia.org/wiki/Srinivasa_Ramanujan\n    ""ramanujan"",\n    # Sally Kristen Ride was an American physicist and astronaut. She was the first American woman in space,\n    # and the youngest American astronaut. https://en.wikipedia.org/wiki/Sally_Ride\n    ""ride"",\n    # Rita Levi-Montalcini - Won Nobel Prize in Physiology or Medicine jointly with colleague Stanley Cohen\n    # for the discovery of nerve growth factor (https://en.wikipedia.org/wiki/Rita_Levi-Montalcini)\n    ""montalcini"",\n    # Dennis Ritchie - co-creator of UNIX and the C programming language. -\n    # https://en.wikipedia.org/wiki/Dennis_Ritchie\n    ""ritchie"",\n    # Wilhelm Conrad R\xc3\xb6ntgen - German physicist who was awarded the first Nobel Prize in Physics in 1901\n    # for the discovery of X-rays (R\xc3\xb6ntgen rays). https://en.wikipedia.org/wiki/Wilhelm_R%C3%B6ntgen\n    ""roentgen"",\n    # Rosalind Franklin - British biophysicist and X-ray crystallographer whose research was critical to\n    # the understanding of DNA - https://en.wikipedia.org/wiki/Rosalind_Franklin\n    ""rosalind"",\n    # Meghnad Saha - Indian astrophysicist best known for his development of the Saha equation, used to\n    # describe chemical and physical conditions in stars - https://en.wikipedia.org/wiki/Meghnad_Saha\n    ""saha"",\n    # Jean E. Sammet developed FORMAC, the first widely used computer language for symbolic manipulation\n    # of mathematical formulas. https://en.wikipedia.org/wiki/Jean_E._Sammet\n    ""sammet"",\n    # Carol Shaw - Originally an Atari employee, Carol Shaw is said to be the first female video game designer.\n    # https://en.wikipedia.org/wiki/Carol_Shaw_(video_game_designer)\n    ""shaw"",\n    # Dame Stephanie \'Steve\' Shirley - Founded a software company in 1962 employing women working from home.\n    # https://en.wikipedia.org/wiki/Steve_Shirley\n    ""shirley"",\n    # William Shockley co-invented the transistor - https://en.wikipedia.org/wiki/William_Shockley\n    ""shockley"",\n    # Fran\xc3\xa7oise Barr\xc3\xa9-Sinoussi - French virologist and Nobel Prize Laureate in Physiology or Medicine;\n    # her work was fundamental in identifying HIV as the cause of AIDS.\n    # https://en.wikipedia.org/wiki/Fran%C3%A7oise_Barr%C3%A9-Sinoussi\n    ""sinoussi"",\n    # Betty Snyder - one of the original programmers of the ENIAC. https://en.wikipedia.org/wiki/ENIAC -\n    # https://en.wikipedia.org/wiki/Betty_Holberton\n    ""snyder"",\n    # Frances Spence - one of the original programmers of the ENIAC. https://en.wikipedia.org/wiki/ENIAC -\n    # https://en.wikipedia.org/wiki/Frances_Spence\n    ""spence"",\n    # Richard Matthew Stallman - the founder of the Free Software movement, the GNU project, the Free Software Foundation,\n    # and the League for Programming Freedom.\n    # He also invented the concept of copyleft to protect the ideals of this movement, and enshrined\n    # this concept in the widely-used GPL (General Public License) for software.\n    # https://en.wikiquote.org/wiki/Richard_Stallman\n    ""stallman"",\n    # Michael Stonebraker is a database research pioneer and architect of Ingres, Postgres, VoltDB and SciDB.\n    # Winner of 2014 ACM Turing Award. https://en.wikipedia.org/wiki/Michael_Stonebraker\n    ""stonebraker"",\n    # Janese Swanson (with others) developed the first of the Carmen Sandiego games.\n    # She went on to found Girl Tech. https://en.wikipedia.org/wiki/Janese_Swanson\n    ""swanson"",\n    # Aaron Swartz was influential in creating RSS, Markdown, Creative Commons, Reddit,\n    # and much of the internet as we know it today. He was devoted to freedom of information on the web.\n    # https://en.wikiquote.org/wiki/Aaron_Swartz\n    ""swartz"",\n    # Bertha Swirles was a theoretical physicist who made a number of contributions to early quantum theory.\n    # https://en.wikipedia.org/wiki/Bertha_Swirles\n    ""swirles"",\n    # Nikola Tesla invented the AC electric system and every gadget ever used by a James Bond villain.\n    # https://en.wikipedia.org/wiki/Nikola_Tesla\n    ""tesla"",\n    # Ken Thompson - co-creator of UNIX and the C programming language -\n    # https://en.wikipedia.org/wiki/Ken_Thompson\n    ""thompson"",\n    # Linus Torvalds invented Linux and Git.\n    # https://en.wikipedia.org/wiki/Linus_Torvalds\n    ""torvalds"",\n    # Alan Turing was a founding father of computer science.\n    # https://en.wikipedia.org/wiki/Alan_Turing.\n    ""turing"",\n    # Varahamihira - Ancient Indian mathematician who discovered trigonometric formulae during\n    # 505-587 CE - https://en.wikipedia.org/wiki/Var%C4%81hamihira#Contributions\n    ""varahamihira"",\n    # Sir Mokshagundam Visvesvaraya - is a notable Indian engineer.\n    # He is a recipient of the Indian Republic\'s highest honour,\n    # the Bharat Ratna, in 1955. On his birthday,\n    # 15 September is celebrated as Engineer\'s Day in India in his memory - https://en.wikipedia.org/wiki/Visvesvaraya\n    ""visvesvaraya"",\n    # Christiane N\xc3\xbcsslein-Volhard - German biologist, won Nobel Prize in Physiology or Medicine in 1995\n    # for research on the genetic control of embryonic development.\n    # https://en.wikipedia.org/wiki/Christiane_N%C3%BCsslein-Volhard\n    ""volhard"",\n    # Marlyn Wescoff - one of the original programmers of the ENIAC.\n    # https://en.wikipedia.org/wiki/ENIAC - https://en.wikipedia.org/wiki/Marlyn_Meltzer\n    ""wescoff"",\n    # Andrew Wiles - Notable British mathematician who proved the enigmatic Fermat\'s Last Theorem -\n    # https://en.wikipedia.org/wiki/Andrew_Wiles\n    ""wiles"",\n    # Roberta Williams, did pioneering work in graphical adventure games for personal computers,\n    # particularly the King\'s Quest series. https://en.wikipedia.org/wiki/Roberta_Williams\n    ""williams"",\n    # Sophie Wilson designed the first Acorn Micro-Computer and the instruction set for ARM processors. https://en.wikipedia.org/wiki/Sophie_Wilson\n    ""wilson"",\n    # Jeannette Wing - co-developed the Liskov substitution principle. - https://en.wikipedia.org/wiki/Jeannette_Wing\n    ""wing"",\n    # Steve Wozniak invented the Apple I and Apple II.\n    # https://en.wikipedia.org/wiki/Steve_Wozniak\n    ""wozniak"",\n    # The Wright brothers, Orville and Wilbur - credited with inventing and building\n    # the world\'s first successful airplane and making the first controlled, powered and sustained heavier-than-air human flight -\n    # https://en.wikipedia.org/wiki/Wright_brothers\n    ""wright"",\n    # Rosalyn Sussman Yalow - Rosalyn Sussman Yalow was an American medical physicist,\n    # and a co-winner of the 1977 Nobel Prize in Physiology or Medicine for development of the radioimmunoassay technique.\n    # https://en.wikipedia.org/wiki/Rosalyn_Sussman_Yalow\n    ""yalow"",\n    # Ada Yonath - an Israeli crystallographer, the first woman from the Middle East to win a Nobel prize in the sciences. https://en.wikipedia.org/wiki/Ada_Yonath\n    ""yonath"",\n]\n\n\ndef get_random_name(sep=""_""):\n    r = random.SystemRandom()\n    while 1:\n        name = ""%s%s%s"" % (r.choice(left), sep, r.choice(right))\n        if name == ""boring"" + sep + ""wozniak"":  # Steve Wozniak is not boring\n            continue\n        return name\n\n\nif __name__ == ""__main__"":\n    print(get_random_name())\n'"
pytorch_toolbelt/utils/random.py,5,"b'""""""Utility functions to make your experiments reproducible\n\n""""""\nimport random\nimport warnings\n\nimport torch\n\n\ndef set_manual_seed(seed):\n    """"""Set random seed for Python and PyTorch random generators.\n    """"""\n    random.seed(seed)\n    torch.manual_seed(seed)\n\n    print(""Using manual seed: {seed}"".format(seed=seed))\n\n\ndef get_rng_state() -> dict:\n    return {\n        ""torch_rng"": torch.get_rng_state(),\n        ""torch_rng_cuda"": torch.cuda.get_rng_state_all(),\n        ""python_rng"": random.getstate(),\n    }\n\n\ndef set_rng_state(rng_state: dict):\n    try:\n        torch_rng = rng_state[""torch_rng""]\n        torch.set_rng_state(torch_rng)\n        print(""Set torch rng state"")\n    except ValueError as e:\n        warnings.warn(e)\n\n    try:\n        torch_rng_cuda = rng_state[""torch_rng_cuda""]\n        torch.cuda.set_rng_state(torch_rng_cuda)\n        print(""Set torch rng cuda state"")\n    except ValueError as e:\n        warnings.warn(e)\n\n    try:\n        python_rng = rng_state[""python_rng""]\n        random.setstate(python_rng)\n        print(""Set python rng state"")\n    except ValueError as e:\n        warnings.warn(e)\n\n\ndef get_random_name() -> str:\n    from . import namesgenerator as ng\n\n    return ng.get_random_name()\n'"
pytorch_toolbelt/utils/rle.py,0,"b'import numpy as np\n\n__all__ = [""rle_decode"", ""rle_encode"", ""rle_to_string""]\n\n\ndef rle_encode(mask: np.ndarray):\n    """"""\n    Convert mask to EncodedPixels in run-length encoding\n    from https://www.kaggle.com/stainsby/fast-tested-rle-and-input-routines\n    """"""\n    pixels = mask.T.flatten()\n    # We need to allow for cases where there is a \'1\' at either end of the sequence.\n    # We do this by padding with a zero at each end when needed.\n    use_padding = False\n    if pixels[0] or pixels[-1]:\n        use_padding = True\n        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n        pixel_padded[1:-1] = pixels\n        pixels = pixel_padded\n    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    if use_padding:\n        rle = rle - 1\n    rle[1::2] = rle[1::2] - rle[:-1:2]\n    return rle\n\n\ndef rle_to_string(runs) -> str:\n    return "" "".join(str(x) for x in runs)\n\n\ndef rle_decode(rle_str, shape, dtype) -> np.ndarray:\n    s = rle_str.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    mask = np.zeros(np.prod(shape), dtype=dtype)\n    for lo, hi in zip(starts, ends):\n        mask[lo:hi] = 1\n    return mask.reshape(shape[::-1]).T\n'"
pytorch_toolbelt/utils/support.py,0,"b'import functools\nimport inspect\nimport warnings\n\nstring_types = (type(b""""), type(""""))\n\n__all__ = [""pytorch_toolbelt_deprecated""]\n\n\ndef pytorch_toolbelt_deprecated(reason):\n    """"""\n    Mark function or class as deprecated.\n    It will result in a warning being emitted when the function is used.\n    """"""\n    if isinstance(reason, string_types):\n\n        # The @deprecated is used with a \'reason\'.\n        #\n        # .. code-block:: python\n        #\n        #    @deprecated(""please, use another function"")\n        #    def old_function(x, y):\n        #      pass\n\n        def decorator(func1):\n\n            if inspect.isclass(func1):\n                fmt1 = ""Call to deprecated class {name} ({reason}).""\n            else:\n                fmt1 = ""Call to deprecated function {name} ({reason}).""\n\n            @functools.wraps(func1)\n            def new_func1(*args, **kwargs):\n                warnings.simplefilter(""always"", DeprecationWarning)\n                warnings.warn(\n                    fmt1.format(name=func1.__name__, reason=reason), category=DeprecationWarning, stacklevel=2\n                )\n                warnings.simplefilter(""default"", DeprecationWarning)\n                return func1(*args, **kwargs)\n\n            return new_func1\n\n        return decorator\n\n    elif inspect.isclass(reason) or inspect.isfunction(reason):\n\n        # The @deprecated is used without any \'reason\'.\n        #\n        # .. code-block:: python\n        #\n        #    @deprecated\n        #    def old_function(x, y):\n        #      pass\n\n        func2 = reason\n\n        if inspect.isclass(func2):\n            fmt2 = ""Call to deprecated class {name}.""\n        else:\n            fmt2 = ""Call to deprecated function {name}.""\n\n        @functools.wraps(func2)\n        def new_func2(*args, **kwargs):\n            warnings.simplefilter(""always"", DeprecationWarning)\n            warnings.warn(fmt2.format(name=func2.__name__), category=DeprecationWarning, stacklevel=2)\n            warnings.simplefilter(""default"", DeprecationWarning)\n            return func2(*args, **kwargs)\n\n        return new_func2\n\n    else:\n        raise TypeError(repr(type(reason)))\n'"
pytorch_toolbelt/utils/torch_utils.py,15,"b'""""""Common functions to marshal data to/from PyTorch\n\n""""""\nimport collections\nfrom typing import Optional, Sequence, Union, Dict\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\n__all__ = [\n    ""rgb_image_from_tensor"",\n    ""tensor_from_mask_image"",\n    ""tensor_from_rgb_image"",\n    ""count_parameters"",\n    ""transfer_weights"",\n    ""maybe_cuda"",\n    ""mask_from_tensor"",\n    ""logit"",\n    ""to_numpy"",\n    ""to_tensor"",\n]\n\n\ndef logit(x: torch.Tensor, eps=1e-5) -> torch.Tensor:\n    """"""\n    Compute inverse of sigmoid of the input.\n    Note: This function has not been tested for numerical stability.\n    :param x:\n    :param eps:\n    :return:\n    """"""\n    x = torch.clamp(x, eps, 1.0 - eps)\n    return torch.log(x / (1.0 - x))\n\n\ndef count_parameters(model: nn.Module, keys: Optional[Sequence[str]] = None) -> Dict[str, int]:\n    """"""\n    Count number of total and trainable parameters of a model\n    :param model: A model\n    :param keys: Optional list of top-level blocks\n    :return: Tuple (total, trainable)\n    """"""\n    if keys is None:\n        keys = [""encoder"", ""decoder"", ""logits"", ""head"", ""final""]\n    total = int(sum(p.numel() for p in model.parameters()))\n    trainable = int(sum(p.numel() for p in model.parameters() if p.requires_grad))\n    parameters = {""total"": total, ""trainable"": trainable}\n\n    for key in keys:\n        if hasattr(model, key) and model.__getattr__(key) is not None:\n            parameters[key] = int(sum(p.numel() for p in model.__getattr__(key).parameters()))\n\n    return parameters\n\n\ndef to_numpy(x) -> np.ndarray:\n    """"""\n    Convert whatever to numpy array\n    :param x: List, tuple, PyTorch tensor or numpy array\n    :return: Numpy array\n    """"""\n    if isinstance(x, np.ndarray):\n        return x\n    elif isinstance(x, torch.Tensor):\n        return x.detach().cpu().numpy()\n    elif isinstance(x, (list, tuple, int, float)):\n        return np.array(x)\n    else:\n        raise ValueError(""Unsupported type"")\n\n\ndef to_tensor(x, dtype=None) -> torch.Tensor:\n    if isinstance(x, torch.Tensor):\n        if dtype is not None:\n            x = x.type(dtype)\n        return x\n    if isinstance(x, np.ndarray):\n        x = torch.from_numpy(x)\n        if dtype is not None:\n            x = x.type(dtype)\n        return x\n    if isinstance(x, (list, tuple)):\n        x = np.ndarray(x)\n        x = torch.from_numpy(x)\n        if dtype is not None:\n            x = x.type(dtype)\n        return x\n\n    raise ValueError(""Unsupported input type"" + str(type(x)))\n\n\ndef tensor_from_rgb_image(image: np.ndarray) -> torch.Tensor:\n    image = np.moveaxis(image, -1, 0)\n    image = np.ascontiguousarray(image)\n    image = torch.from_numpy(image)\n    return image\n\n\ndef tensor_from_mask_image(mask: np.ndarray) -> torch.Tensor:\n    if len(mask.shape) == 2:\n        mask = np.expand_dims(mask, -1)\n    return tensor_from_rgb_image(mask)\n\n\ndef rgb_image_from_tensor(image: torch.Tensor, mean, std, max_pixel_value=255.0, dtype=np.uint8) -> np.ndarray:\n    image = np.moveaxis(to_numpy(image), 0, -1)\n    mean = to_numpy(mean)\n    std = to_numpy(std)\n    rgb_image = (max_pixel_value * (image * std + mean)).astype(dtype)\n    return rgb_image\n\n\ndef mask_from_tensor(mask: torch.Tensor, squeeze_single_channel=False, dtype=None) -> np.ndarray:\n    mask = np.moveaxis(to_numpy(mask), 0, -1)\n    if squeeze_single_channel and mask.shape[-1] == 1:\n        mask = np.squeeze(mask, -1)\n\n    if dtype is not None:\n        mask = mask.astype(dtype)\n    return mask\n\n\ndef maybe_cuda(x: Union[torch.Tensor, nn.Module]) -> Union[torch.Tensor, nn.Module]:\n    """"""\n    Move input Tensor or Module to CUDA device if CUDA is available.\n    :param x:\n    :return:\n    """"""\n    if torch.cuda.is_available():\n        return x.cuda()\n    return x\n\n\ndef transfer_weights(model: nn.Module, model_state_dict: collections.OrderedDict):\n    """"""\n    Copy weights from state dict to model, skipping layers that are incompatible.\n    This method is helpful if you are doing some model surgery and want to load\n    part of the model weights into different model.\n    :param model: Model to load weights into\n    :param model_state_dict: Model state dict to load weights from\n    :return: None\n    """"""\n    for name, value in model_state_dict.items():\n        try:\n            model.load_state_dict(collections.OrderedDict([(name, value)]), strict=False)\n        except Exception as e:\n            print(e)\n'"
pytorch_toolbelt/utils/visualization.py,0,"b'from __future__ import absolute_import\n\nimport itertools\n\nimport numpy as np\n\nfrom .torch_utils import tensor_from_rgb_image\n\n\ndef plot_confusion_matrix(\n    cm, class_names, figsize=(16, 16), normalize=False, title=""Confusion matrix"", fname=None, noshow=False\n):\n    """"""Render the confusion matrix and return matplotlib\'s figure with it.\n    Normalization can be applied by setting `normalize=True`.\n    """"""\n    import matplotlib\n\n    matplotlib.use(""Agg"")\n    import matplotlib.pyplot as plt\n\n    cmap = plt.cm.Oranges\n\n    if normalize:\n        cm = cm.astype(np.float32) / cm.sum(axis=1)[:, np.newaxis]\n\n    f = plt.figure(figsize=figsize)\n    plt.title(title)\n    plt.imshow(cm, interpolation=""nearest"", cmap=cmap)\n\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names, rotation=45, ha=""right"")\n    # f.tick_params(direction=\'inout\')\n    # f.set_xticklabels(varLabels, rotation=45, ha=\'right\')\n    # f.set_yticklabels(varLabels, rotation=45, va=\'top\')\n\n    plt.yticks(tick_marks, class_names)\n\n    fmt = "".2f"" if normalize else ""d""\n    thresh = cm.max() / 2.0\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(\n            j, i, format(cm[i, j], fmt), horizontalalignment=""center"", color=""white"" if cm[i, j] > thresh else ""black""\n        )\n\n    plt.tight_layout()\n    plt.ylabel(""True label"")\n    plt.xlabel(""Predicted label"")\n\n    if fname is not None:\n        plt.savefig(fname=fname)\n\n    if not noshow:\n        plt.show()\n\n    return f\n\n\ndef render_figure_to_tensor(figure):\n    import matplotlib\n\n    matplotlib.use(""Agg"")\n    import matplotlib.pyplot as plt\n\n    figure.canvas.draw()\n\n    # string = figure.canvas.tostring_argb()\n\n    image = np.array(figure.canvas.renderer._renderer)\n    plt.close(figure)\n    del figure\n\n    image = tensor_from_rgb_image(image)\n    return image\n'"
pytorch_toolbelt/zoo/__init__.py,0,b'from .classification import *\nfrom .segmentation import *\n'
pytorch_toolbelt/zoo/classification.py,0,b'__all__ = []\n'
pytorch_toolbelt/zoo/segmentation.py,0,b'__all__ = []\n'
pytorch_toolbelt/modules/backbone/__init__.py,0,b''
pytorch_toolbelt/modules/backbone/inceptionv4.py,13,"b'# Orignal source\n# https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/inceptionv4.py\n\nfrom __future__ import print_function, division, absolute_import\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [""InceptionV4"", ""inceptionv4""]\n\npretrained_settings = {\n    ""inceptionv4"": {\n        ""imagenet"": {\n            ""url"": ""http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth"",\n            ""input_space"": ""RGB"",\n            ""input_size"": [3, 299, 299],\n            ""input_range"": [0, 1],\n            ""mean"": [0.5, 0.5, 0.5],\n            ""std"": [0.5, 0.5, 0.5],\n            ""num_classes"": 1000,\n        },\n        ""imagenet+background"": {\n            ""url"": ""http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth"",\n            ""input_space"": ""RGB"",\n            ""input_size"": [3, 299, 299],\n            ""input_range"": [0, 1],\n            ""mean"": [0.5, 0.5, 0.5],\n            ""std"": [0.5, 0.5, 0.5],\n            ""num_classes"": 1001,\n        },\n    }\n}\n\n\nclass BasicConv2d(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(\n            in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False\n        )  # verify bias false\n        self.bn = nn.BatchNorm2d(\n            out_planes, eps=0.001, momentum=0.1, affine=True  # value found in tensorflow  # default pytorch value\n        )\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass Mixed_3a(nn.Module):\n    def __init__(self):\n        super(Mixed_3a, self).__init__()\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n        self.conv = BasicConv2d(64, 96, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        x0 = self.maxpool(x)\n        x1 = self.conv(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Mixed_4a(nn.Module):\n    def __init__(self):\n        super(Mixed_4a, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1, stride=1), BasicConv2d(64, 96, kernel_size=3, stride=1)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 64, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            BasicConv2d(64, 64, kernel_size=(7, 1), stride=1, padding=(3, 0)),\n            BasicConv2d(64, 96, kernel_size=(3, 3), stride=1),\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Mixed_5a(nn.Module):\n    def __init__(self):\n        super(Mixed_5a, self).__init__()\n        self.conv = BasicConv2d(192, 192, kernel_size=3, stride=2)\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.conv(x)\n        x1 = self.maxpool(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Inception_A(nn.Module):\n    def __init__(self):\n        super(Inception_A, self).__init__()\n        self.branch0 = BasicConv2d(384, 96, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(384, 64, kernel_size=1, stride=1), BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(384, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1),\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(384, 96, kernel_size=1, stride=1),\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Reduction_A(nn.Module):\n    def __init__(self):\n        super(Reduction_A, self).__init__()\n        self.branch0 = BasicConv2d(384, 384, kernel_size=3, stride=2)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(384, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(224, 256, kernel_size=3, stride=2),\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Inception_B(nn.Module):\n    def __init__(self):\n        super(Inception_B, self).__init__()\n        self.branch0 = BasicConv2d(1024, 384, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            BasicConv2d(224, 256, kernel_size=(7, 1), stride=1, padding=(3, 0)),\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 192, kernel_size=(7, 1), stride=1, padding=(3, 0)),\n            BasicConv2d(192, 224, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            BasicConv2d(224, 224, kernel_size=(7, 1), stride=1, padding=(3, 0)),\n            BasicConv2d(224, 256, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(1024, 128, kernel_size=1, stride=1),\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Reduction_B(nn.Module):\n    def __init__(self):\n        super(Reduction_B, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1), BasicConv2d(192, 192, kernel_size=3, stride=2)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1024, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            BasicConv2d(256, 320, kernel_size=(7, 1), stride=1, padding=(3, 0)),\n            BasicConv2d(320, 320, kernel_size=3, stride=2),\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Inception_C(nn.Module):\n    def __init__(self):\n        super(Inception_C, self).__init__()\n\n        self.branch0 = BasicConv2d(1536, 256, kernel_size=1, stride=1)\n\n        self.branch1_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)\n        self.branch1_1a = BasicConv2d(384, 256, kernel_size=(1, 3), stride=1, padding=(0, 1))\n        self.branch1_1b = BasicConv2d(384, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))\n\n        self.branch2_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)\n        self.branch2_1 = BasicConv2d(384, 448, kernel_size=(3, 1), stride=1, padding=(1, 0))\n        self.branch2_2 = BasicConv2d(448, 512, kernel_size=(1, 3), stride=1, padding=(0, 1))\n        self.branch2_3a = BasicConv2d(512, 256, kernel_size=(1, 3), stride=1, padding=(0, 1))\n        self.branch2_3b = BasicConv2d(512, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(1536, 256, kernel_size=1, stride=1),\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n\n        x1_0 = self.branch1_0(x)\n        x1_1a = self.branch1_1a(x1_0)\n        x1_1b = self.branch1_1b(x1_0)\n        x1 = torch.cat((x1_1a, x1_1b), 1)\n\n        x2_0 = self.branch2_0(x)\n        x2_1 = self.branch2_1(x2_0)\n        x2_2 = self.branch2_2(x2_1)\n        x2_3a = self.branch2_3a(x2_2)\n        x2_3b = self.branch2_3b(x2_2)\n        x2 = torch.cat((x2_3a, x2_3b), 1)\n\n        x3 = self.branch3(x)\n\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass InceptionV4(nn.Module):\n    def __init__(self, num_classes=1001):\n        super(InceptionV4, self).__init__()\n        # Special attributs\n        self.input_space = None\n        self.input_size = (299, 299, 3)\n        self.mean = None\n        self.std = None\n        # Modules\n        self.features = nn.Sequential(\n            BasicConv2d(3, 32, kernel_size=3, stride=2),  # 0, layer0\n            BasicConv2d(32, 32, kernel_size=3, stride=1),  # 1, layer0\n            BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1),  # 2\n            Mixed_3a(),  # 3\n            Mixed_4a(),  # 4\n            Mixed_5a(),  # 5\n            Inception_A(),  # 6\n            Inception_A(),  # 7\n            Inception_A(),  # 8\n            Inception_A(),  # 9\n            Reduction_A(),  # 10 # Mixed_6a\n            Inception_B(),  # 11\n            Inception_B(),  # 12\n            Inception_B(),  # 13\n            Inception_B(),  # 14\n            Inception_B(),  # 15\n            Inception_B(),  # 16\n            Inception_B(),  # 17\n            Reduction_B(),  # 18  # Mixed_7a\n            Inception_C(),  # 19\n            Inception_C(),  # 20\n            Inception_C(),  # 21\n        )\n        self.last_linear = nn.Linear(1536, num_classes)\n\n    def logits(self, features):\n        # Allows image of any size to be processed\n        adaptiveAvgPoolWidth = features.shape[2]\n        x = F.avg_pool2d(features, kernel_size=adaptiveAvgPoolWidth)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef inceptionv4(num_classes=1000, pretrained=""imagenet""):\n    if pretrained:\n        settings = pretrained_settings[""inceptionv4""][pretrained]\n        assert num_classes == settings[""num_classes""], ""num_classes should be {}, but is {}"".format(\n            settings[""num_classes""], num_classes\n        )\n\n        # both \'imagenet\'&\'imagenet+background\' are loaded from same parameters\n        model = InceptionV4(num_classes=1001)\n        model.load_state_dict(model_zoo.load_url(settings[""url""]))\n\n        if pretrained == ""imagenet"":\n            new_last_linear = nn.Linear(1536, 1000)\n            new_last_linear.weight.data = model.last_linear.weight.data[1:]\n            new_last_linear.bias.data = model.last_linear.bias.data[1:]\n            model.last_linear = new_last_linear\n\n        model.input_space = settings[""input_space""]\n        model.input_size = settings[""input_size""]\n        model.input_range = settings[""input_range""]\n        model.mean = settings[""mean""]\n        model.std = settings[""std""]\n    else:\n        model = InceptionV4(num_classes=num_classes)\n    return model\n'"
pytorch_toolbelt/modules/backbone/mobilenet.py,1,"b'from __future__ import absolute_import\n\nimport math\n\nimport torch.nn as nn\n\nfrom ..activations import get_activation_block\n\n\ndef conv_bn(inp, oup, stride, activation):\n    return nn.Sequential(nn.Conv2d(inp, oup, 3, stride, 1, bias=False), nn.BatchNorm2d(oup), activation())\n\n\ndef conv_1x1_bn(inp, oup, activation):\n    return nn.Sequential(nn.Conv2d(inp, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), activation())\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio, activation):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                activation(),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                activation(),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                activation(),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1.0, activation=""relu6""):\n        super(MobileNetV2, self).__init__()\n\n        activation_block = get_activation_block(activation)\n\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n        interverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        assert input_size % 32 == 0\n        input_channel = int(input_channel * width_mult)\n        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n        self.layer0 = conv_bn(3, input_channel, 2, activation_block)\n\n        # building inverted residual blocks\n        for layer_index, (t, c, n, s) in enumerate(interverted_residual_setting):\n            output_channel = int(c * width_mult)\n\n            blocks = []\n            for i in range(n):\n                if i == 0:\n                    blocks.append(block(input_channel, output_channel, s, expand_ratio=t, activation=activation_block))\n                else:\n                    blocks.append(block(input_channel, output_channel, 1, expand_ratio=t, activation=activation_block))\n\n                input_channel = output_channel\n\n            self.add_module(f""layer{layer_index + 1}"", nn.Sequential(*blocks))\n\n        # building last several layers\n        self.final_layer = conv_1x1_bn(input_channel, self.last_channel, activation=activation_block)\n\n        # building classifier\n        self.classifier = nn.Sequential(nn.Dropout(0.2), nn.Linear(self.last_channel, n_class))\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        x = self.layer6(x)\n        x = self.layer7(x)\n        x = self.final_layer(x)\n\n        x = x.mean(3).mean(2)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2.0 / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n'"
pytorch_toolbelt/modules/backbone/mobilenetv3.py,2,"b'# https://github.com/Randl/MobileNetV3-pytorch/blob/master/MobileNetV3.py\n\nfrom collections import OrderedDict\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..activations import HardSwish, HardSigmoid\nfrom ..identity import Identity\n\n\ndef _make_divisible(v, divisor, min_value=None):\n    """"""\n    Ensure that all layers have a channel number that is divisible by 8\n\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    :param v:\n    :param divisor:\n    :param min_value:\n    :return:\n    """"""\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\nclass SqEx(nn.Module):\n    """"""Squeeze-Excitation block. Implemented in ONNX & CoreML friendly way.\n    Original implementation: https://github.com/jonnedtc/Squeeze-Excitation-PyTorch/blob/master/networks.py\n    """"""\n\n    def __init__(self, n_features, reduction=4):\n        super(SqEx, self).__init__()\n\n        if n_features % reduction != 0:\n            raise ValueError(""n_features must be divisible by reduction (default = 4)"")\n\n        self.linear1 = nn.Conv2d(n_features, n_features // reduction, kernel_size=1, bias=True)\n        self.nonlin1 = nn.ReLU(inplace=True)\n        self.linear2 = nn.Conv2d(n_features // reduction, n_features, kernel_size=1, bias=True)\n        self.nonlin2 = HardSigmoid(inplace=True)\n\n    def forward(self, x):\n        y = F.adaptive_avg_pool2d(x, output_size=1)\n        y = self.nonlin1(self.linear1(y))\n        y = self.nonlin2(self.linear2(y))\n        y = x * y\n        return y\n\n\nclass LinearBottleneck(nn.Module):\n    def __init__(\n        self,\n        inplanes,\n        outplanes,\n        expplanes,\n        k=3,\n        stride=1,\n        drop_prob=0,\n        num_steps=3e5,\n        start_step=0,\n        activation=nn.ReLU,\n        act_params={""inplace"": True},\n        SE=False,\n    ):\n        super(LinearBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, expplanes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(expplanes)\n        self.db1 = nn.Dropout2d(drop_prob)\n        # self.db1 = DropBlockScheduled(DropBlock2D(drop_prob=drop_prob, block_size=7), start_value=0.,\n        #                               stop_value=drop_prob, nr_steps=num_steps, start_step=start_step)\n        self.act1 = activation(**act_params)  # first does have act according to MobileNetV2\n\n        self.conv2 = nn.Conv2d(\n            expplanes, expplanes, kernel_size=k, stride=stride, padding=k // 2, bias=False, groups=expplanes\n        )\n        self.bn2 = nn.BatchNorm2d(expplanes)\n        self.db2 = nn.Dropout2d(drop_prob)\n        # self.db2 = DropBlockScheduled(DropBlock2D(drop_prob=drop_prob, block_size=7), start_value=0.,\n        #                               stop_value=drop_prob, nr_steps=num_steps, start_step=start_step)\n        self.act2 = activation(**act_params)\n\n        self.se = SqEx(expplanes) if SE else Identity()\n\n        self.conv3 = nn.Conv2d(expplanes, outplanes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(outplanes)\n        self.db3 = nn.Dropout2d(drop_prob)\n        # self.db3 = DropBlockScheduled(DropBlock2D(drop_prob=drop_prob, block_size=7), start_value=0.,\n        #                               stop_value=drop_prob, nr_steps=num_steps, start_step=start_step)\n\n        self.stride = stride\n        self.expplanes = expplanes\n        self.inplanes = inplanes\n        self.outplanes = outplanes\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.db1(out)\n        out = self.act1(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.db2(out)\n        out = self.act2(out)\n\n        out = self.se(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        out = self.db3(out)\n\n        if self.stride == 1 and self.inplanes == self.outplanes:  # TODO: or add 1x1?\n            out += residual  # No inplace if there is in-place activation before\n\n        return out\n\n\nclass LastBlockLarge(nn.Module):\n    def __init__(self, inplanes, num_classes, expplanes1, expplanes2):\n        super(LastBlockLarge, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, expplanes1, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(expplanes1)\n        self.act1 = HardSwish(inplace=True)\n\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n\n        self.conv2 = nn.Conv2d(expplanes1, expplanes2, kernel_size=1, stride=1)\n        self.act2 = HardSwish(inplace=True)\n\n        self.dropout = nn.Dropout(p=0.2, inplace=True)\n        self.fc = nn.Linear(expplanes2, num_classes)\n\n        self.expplanes1 = expplanes1\n        self.expplanes2 = expplanes2\n        self.inplanes = inplanes\n        self.num_classes = num_classes\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.act1(out)\n\n        out = self.avgpool(out)\n\n        out = self.conv2(out)\n        out = self.act2(out)\n\n        # flatten for input to fully-connected layer\n        out = out.view(out.size(0), -1)\n        out = self.fc(self.dropout(out))\n\n        return out\n\n\nclass LastBlockSmall(nn.Module):\n    def __init__(self, inplanes, num_classes, expplanes1, expplanes2):\n        super(LastBlockSmall, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, expplanes1, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(expplanes1)\n        self.act1 = HardSwish(inplace=True)\n\n        self.se = SqEx(expplanes1)\n\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n\n        self.conv2 = nn.Conv2d(expplanes1, expplanes2, kernel_size=1, stride=1, bias=False)\n        self.act2 = HardSwish(inplace=True)\n\n        self.dropout = nn.Dropout(p=0.2, inplace=True)\n        self.fc = nn.Linear(expplanes2, num_classes)\n\n        self.expplanes1 = expplanes1\n        self.expplanes2 = expplanes2\n        self.inplanes = inplanes\n        self.num_classes = num_classes\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.act1(out)\n\n        out = self.se(out)\n        out = self.avgpool(out)\n\n        out = self.conv2(out)\n        out = self.act2(out)\n\n        # flatten for input to fully-connected layer\n        out = out.view(out.size(0), -1)\n        out = self.fc(self.dropout(out))\n\n        return out\n\n\nclass MobileNetV3(nn.Module):\n    """"""MobileNetV3 implementation.\n    """"""\n\n    def __init__(\n        self, num_classes=1000, scale=1.0, in_channels=3, drop_prob=0.0, num_steps=3e5, start_step=0, small=False\n    ):\n        super(MobileNetV3, self).__init__()\n\n        self.num_steps = num_steps\n        self.start_step = start_step\n        self.scale = scale\n        self.num_classes = num_classes\n        self.small = small\n\n        # setting of bottlenecks blocks\n        self.bottlenecks_setting_large = [\n            # in, exp, out, s, k,         dp,    se,      act\n            [16, 16, 16, 1, 3, 0, False, nn.ReLU],  # -> 112x112\n            [16, 64, 24, 2, 3, 0, False, nn.ReLU],  # -> 56x56\n            [24, 72, 24, 1, 3, 0, False, nn.ReLU],  # -> 56x56\n            [24, 72, 40, 2, 5, 0, True, nn.ReLU],  # -> 28x28\n            [40, 120, 40, 1, 5, 0, True, nn.ReLU],  # -> 28x28\n            [40, 120, 40, 1, 5, 0, True, nn.ReLU],  # -> 28x28\n            [40, 240, 80, 2, 3, drop_prob, False, HardSwish],  # -> 14x14\n            [80, 200, 80, 1, 3, drop_prob, False, HardSwish],  # -> 14x14\n            [80, 184, 80, 1, 3, drop_prob, False, HardSwish],  # -> 14x14\n            [80, 184, 80, 1, 3, drop_prob, False, HardSwish],  # -> 14x14\n            [80, 480, 112, 1, 3, drop_prob, True, HardSwish],  # -> 14x14\n            [112, 672, 112, 1, 3, drop_prob, True, HardSwish],  # -> 14x14\n            [112, 672, 160, 2, 5, drop_prob, True, HardSwish],  # -> 7x7\n            [160, 672, 160, 1, 5, drop_prob, True, HardSwish],  # -> 7x7\n            [160, 960, 160, 1, 5, drop_prob, True, HardSwish],  # -> 7x7\n        ]\n        self.bottlenecks_setting_small = [\n            # in, exp, out, s, k,         dp,    se,      act\n            [16, 64, 16, 2, 3, 0, True, nn.ReLU],  # -> 56x56\n            [16, 72, 24, 2, 3, 0, False, nn.ReLU],  # -> 28x28\n            [24, 88, 24, 1, 3, 0, False, nn.ReLU],  # -> 28x28\n            [24, 96, 40, 2, 5, 0, True, HardSwish],  # -> 14x14\n            [40, 240, 40, 1, 5, drop_prob, True, HardSwish],  # -> 14x14\n            [40, 240, 40, 1, 5, drop_prob, True, HardSwish],  # -> 14x14\n            [40, 120, 48, 1, 5, drop_prob, True, HardSwish],  # -> 14x14\n            [48, 144, 96, 1, 5, drop_prob, True, HardSwish],  # -> 14x14\n            [96, 288, 96, 2, 5, drop_prob, True, HardSwish],  # -> 7x7\n            [96, 576, 96, 1, 5, drop_prob, True, HardSwish],  # -> 7x7\n            [96, 576, 96, 1, 5, drop_prob, True, HardSwish],  # -> 7x7\n        ]\n\n        self.bottlenecks_setting = self.bottlenecks_setting_small if small else self.bottlenecks_setting_large\n        for l in self.bottlenecks_setting:\n            l[0] = _make_divisible(l[0] * self.scale, 8)\n            l[1] = _make_divisible(l[1] * self.scale, 8)\n            l[2] = _make_divisible(l[2] * self.scale, 8)\n\n        self.conv1 = nn.Conv2d(\n            in_channels, self.bottlenecks_setting[0][0], kernel_size=3, bias=False, stride=2, padding=1\n        )\n        self.bn1 = nn.BatchNorm2d(self.bottlenecks_setting[0][0])\n        self.act1 = HardSwish(inplace=True)\n        self.layer0, self.layer1, self.layer2, self.layer3, self.layer4 = self._make_bottlenecks()\n\n        # Last convolution has 1280 output channels for scale <= 1\n        self.last_exp2 = 1280 if self.scale <= 1 else _make_divisible(1280 * self.scale, 8)\n        if small:\n            self.last_exp1 = _make_divisible(576 * self.scale, 8)\n            self.last_block = LastBlockSmall(\n                self.bottlenecks_setting[-1][2], num_classes, self.last_exp1, self.last_exp2\n            )\n        else:\n            self.last_exp1 = _make_divisible(960 * self.scale, 8)\n            self.last_block = LastBlockLarge(\n                self.bottlenecks_setting[-1][2], num_classes, self.last_exp1, self.last_exp2\n            )\n\n    def _make_bottlenecks(self):\n        layers = []\n        modules = OrderedDict()\n        stage_name = ""Bottleneck""\n\n        # add LinearBottleneck\n        for i, setup in enumerate(self.bottlenecks_setting):\n            name = stage_name + ""_{}"".format(i)\n            module = LinearBottleneck(\n                setup[0],\n                setup[2],\n                setup[1],\n                k=setup[4],\n                stride=setup[3],\n                drop_prob=setup[5],\n                num_steps=self.num_steps,\n                start_step=self.start_step,\n                activation=setup[7],\n                act_params={""inplace"": True},\n                SE=setup[6],\n            )\n            modules[name] = module\n\n            if setup[3] == 2:\n                layer = nn.Sequential(modules)\n                layers.append(layer)\n                modules = OrderedDict()\n\n        if len(modules):\n            layer = nn.Sequential(modules)\n            layers.append(layer)\n\n        return layers\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.last_block(x)\n        return x\n'"
pytorch_toolbelt/modules/backbone/senet.py,2,"b'""""""\nResNet code gently borrowed from\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n""""""\nfrom __future__ import print_function, division, absolute_import\n\nimport math\nfrom collections import OrderedDict\n\nimport torch.nn as nn\nfrom torch.utils import model_zoo\n\n__all__ = [\n    ""SENet"",\n    ""senet154"",\n    ""se_resnet50"",\n    ""se_resnet101"",\n    ""se_resnet152"",\n    ""se_resnext50_32x4d"",\n    ""se_resnext101_32x4d"",\n]\n\npretrained_settings = {\n    ""senet154"": {\n        ""imagenet"": {\n            ""url"": ""http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth"",\n            ""input_space"": ""RGB"",\n            ""input_size"": [3, 224, 224],\n            ""input_range"": [0, 1],\n            ""mean"": [0.485, 0.456, 0.406],\n            ""std"": [0.229, 0.224, 0.225],\n            ""num_classes"": 1000,\n        }\n    },\n    ""se_resnet50"": {\n        ""imagenet"": {\n            ""url"": ""http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth"",\n            ""input_space"": ""RGB"",\n            ""input_size"": [3, 224, 224],\n            ""input_range"": [0, 1],\n            ""mean"": [0.485, 0.456, 0.406],\n            ""std"": [0.229, 0.224, 0.225],\n            ""num_classes"": 1000,\n        }\n    },\n    ""se_resnet101"": {\n        ""imagenet"": {\n            ""url"": ""http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth"",\n            ""input_space"": ""RGB"",\n            ""input_size"": [3, 224, 224],\n            ""input_range"": [0, 1],\n            ""mean"": [0.485, 0.456, 0.406],\n            ""std"": [0.229, 0.224, 0.225],\n            ""num_classes"": 1000,\n        }\n    },\n    ""se_resnet152"": {\n        ""imagenet"": {\n            ""url"": ""http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth"",\n            ""input_space"": ""RGB"",\n            ""input_size"": [3, 224, 224],\n            ""input_range"": [0, 1],\n            ""mean"": [0.485, 0.456, 0.406],\n            ""std"": [0.229, 0.224, 0.225],\n            ""num_classes"": 1000,\n        }\n    },\n    ""se_resnext50_32x4d"": {\n        ""imagenet"": {\n            ""url"": ""http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth"",\n            ""input_space"": ""RGB"",\n            ""input_size"": [3, 224, 224],\n            ""input_range"": [0, 1],\n            ""mean"": [0.485, 0.456, 0.406],\n            ""std"": [0.229, 0.224, 0.225],\n            ""num_classes"": 1000,\n        }\n    },\n    ""se_resnext101_32x4d"": {\n        ""imagenet"": {\n            ""url"": ""http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth"",\n            ""input_space"": ""RGB"",\n            ""input_size"": [3, 224, 224],\n            ""input_range"": [0, 1],\n            ""mean"": [0.485, 0.456, 0.406],\n            ""std"": [0.229, 0.224, 0.225],\n            ""num_classes"": 1000,\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1, padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1, padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    """"""\n    Base class for bottlenecks that implements `forward()` method.\n    """"""\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    """"""\n    Bottleneck for SENet154.\n    """"""\n\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1, downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(\n            planes * 2, planes * 4, kernel_size=3, stride=stride, padding=1, groups=groups, bias=False\n        )\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    """"""\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    """"""\n\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1, downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False, stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    """"""\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    """"""\n\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1, downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width / 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False, stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride, padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n    def __init__(\n        self,\n        block,\n        layers,\n        groups,\n        reduction,\n        dropout_p=0.2,\n        inplanes=128,\n        input_3x3=True,\n        downsample_kernel_size=3,\n        downsample_padding=1,\n        num_classes=1000,\n    ):\n        """"""\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        """"""\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                (""conv1"", nn.Conv2d(3, 64, 3, stride=2, padding=1, bias=False)),\n                (""bn1"", nn.BatchNorm2d(64)),\n                (""relu1"", nn.ReLU(inplace=True)),\n                (""conv2"", nn.Conv2d(64, 64, 3, stride=1, padding=1, bias=False)),\n                (""bn2"", nn.BatchNorm2d(64)),\n                (""relu2"", nn.ReLU(inplace=True)),\n                (""conv3"", nn.Conv2d(64, inplanes, 3, stride=1, padding=1, bias=False)),\n                (""bn3"", nn.BatchNorm2d(inplanes)),\n                (""relu3"", nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                (""conv1"", nn.Conv2d(3, inplanes, kernel_size=7, stride=2, padding=3, bias=False)),\n                (""bn1"", nn.BatchNorm2d(inplanes)),\n                (""relu1"", nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append((""pool"", nn.MaxPool2d(3, stride=2, ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0,\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding,\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding,\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding,\n        )\n        self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(\n        self, block, planes, blocks, groups, reduction, stride=1, downsample_kernel_size=1, downsample_padding=0\n    ):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.inplanes,\n                    planes * block.expansion,\n                    kernel_size=downsample_kernel_size,\n                    stride=stride,\n                    padding=downsample_padding,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings[""num_classes""], ""num_classes should be {}, but is {}"".format(\n        settings[""num_classes""], num_classes\n    )\n    model.load_state_dict(model_zoo.load_url(settings[""url""]))\n    model.input_space = settings[""input_space""]\n    model.input_size = settings[""input_size""]\n    model.input_range = settings[""input_range""]\n    model.mean = settings[""mean""]\n    model.std = settings[""std""]\n\n\ndef senet154(num_classes=1000, pretrained=""imagenet""):\n    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16, dropout_p=0.2, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[""senet154""][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet50(num_classes=1000, pretrained=""imagenet""):\n    model = SENet(\n        SEResNetBottleneck,\n        [3, 4, 6, 3],\n        groups=1,\n        reduction=16,\n        dropout_p=None,\n        inplanes=64,\n        input_3x3=False,\n        downsample_kernel_size=1,\n        downsample_padding=0,\n        num_classes=num_classes,\n    )\n    if pretrained is not None:\n        settings = pretrained_settings[""se_resnet50""][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet101(num_classes=1000, pretrained=""imagenet""):\n    model = SENet(\n        SEResNetBottleneck,\n        [3, 4, 23, 3],\n        groups=1,\n        reduction=16,\n        dropout_p=None,\n        inplanes=64,\n        input_3x3=False,\n        downsample_kernel_size=1,\n        downsample_padding=0,\n        num_classes=num_classes,\n    )\n    if pretrained is not None:\n        settings = pretrained_settings[""se_resnet101""][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet152(num_classes=1000, pretrained=""imagenet""):\n    model = SENet(\n        SEResNetBottleneck,\n        [3, 8, 36, 3],\n        groups=1,\n        reduction=16,\n        dropout_p=None,\n        inplanes=64,\n        input_3x3=False,\n        downsample_kernel_size=1,\n        downsample_padding=0,\n        num_classes=num_classes,\n    )\n    if pretrained is not None:\n        settings = pretrained_settings[""se_resnet152""][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext50_32x4d(num_classes=1000, pretrained=""imagenet""):\n    model = SENet(\n        SEResNeXtBottleneck,\n        [3, 4, 6, 3],\n        groups=32,\n        reduction=16,\n        dropout_p=None,\n        inplanes=64,\n        input_3x3=False,\n        downsample_kernel_size=1,\n        downsample_padding=0,\n        num_classes=num_classes,\n    )\n    if pretrained is not None:\n        settings = pretrained_settings[""se_resnext50_32x4d""][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext101_32x4d(num_classes=1000, pretrained=""imagenet""):\n    model = SENet(\n        SEResNeXtBottleneck,\n        [3, 4, 23, 3],\n        groups=32,\n        reduction=16,\n        dropout_p=None,\n        inplanes=64,\n        input_3x3=False,\n        downsample_kernel_size=1,\n        downsample_padding=0,\n        num_classes=num_classes,\n    )\n    if pretrained is not None:\n        settings = pretrained_settings[""se_resnext101_32x4d""][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n'"
pytorch_toolbelt/modules/backbone/wider_resnet.py,0,"b'from collections import OrderedDict\nfrom functools import partial\n\nfrom torch import nn\n\nfrom ..activations import ABN\nfrom ..pooling import GlobalAvgPool2d\n\n\nclass IdentityResidualBlock(nn.Module):\n    def __init__(self, in_channels, channels, stride=1, dilation=1, groups=1, norm_act=ABN, dropout=None):\n        """"""Identity-mapping residual block\n        Parameters\n        ----------\n        in_channels : int\n            Number of input channels.\n        channels : list of int\n            Number of channels in the internal feature maps. Can either have two or three elements: if three construct\n            a residual block with two `3 x 3` convolutions, otherwise construct a bottleneck block with `1 x 1`, then\n            `3 x 3` then `1 x 1` convolutions.\n        stride : int\n            Stride of the first `3 x 3` convolution\n        dilation : int\n            Dilation to apply to the `3 x 3` convolutions.\n        groups : int\n            Number of convolution groups. This is used to create ResNeXt-style blocks and is only compatible with\n            bottleneck blocks.\n        norm_act : callable\n            Function to create normalization / activation Module.\n        dropout: callable\n            Function to create Dropout Module.\n        """"""\n        super(IdentityResidualBlock, self).__init__()\n\n        # Check parameters for inconsistencies\n        if len(channels) != 2 and len(channels) != 3:\n            raise ValueError(""channels must contain either two or three values"")\n        if len(channels) == 2 and groups != 1:\n            raise ValueError(""groups > 1 are only valid if len(channels) == 3"")\n\n        is_bottleneck = len(channels) == 3\n        need_proj_conv = stride != 1 or in_channels != channels[-1]\n\n        self.bn1 = norm_act(in_channels)\n        if not is_bottleneck:\n            layers = [\n                (\n                    ""conv1"",\n                    nn.Conv2d(\n                        in_channels, channels[0], 3, stride=stride, padding=dilation, bias=False, dilation=dilation\n                    ),\n                ),\n                (""bn2"", norm_act(channels[0])),\n                (\n                    ""conv2"",\n                    nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False, dilation=dilation),\n                ),\n            ]\n            if dropout is not None:\n                layers = layers[0:2] + [(""dropout"", dropout())] + layers[2:]\n        else:\n            layers = [\n                (""conv1"", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),\n                (""bn2"", norm_act(channels[0])),\n                (\n                    ""conv2"",\n                    nn.Conv2d(\n                        channels[0],\n                        channels[1],\n                        3,\n                        stride=1,\n                        padding=dilation,\n                        bias=False,\n                        groups=groups,\n                        dilation=dilation,\n                    ),\n                ),\n                (""bn3"", norm_act(channels[1])),\n                (""conv3"", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False)),\n            ]\n            if dropout is not None:\n                layers = layers[0:4] + [(""dropout"", dropout())] + layers[4:]\n        self.convs = nn.Sequential(OrderedDict(layers))\n\n        if need_proj_conv:\n            self.proj_conv = nn.Conv2d(in_channels, channels[-1], 1, stride=stride, padding=0, bias=False)\n\n    def forward(self, x):\n        if hasattr(self, ""proj_conv""):\n            bn1 = self.bn1(x)\n            shortcut = self.proj_conv(bn1)\n        else:\n            shortcut = x.clone()\n            bn1 = self.bn1(x)\n\n        out = self.convs(bn1)\n        out.add_(shortcut)\n\n        return out\n\n\nclass WiderResNet(nn.Module):\n    def __init__(self, structure, norm_act=ABN, classes=0):\n        """"""Wider ResNet with pre-activation (identity mapping) blocks\n\n        Parameters\n        ----------\n        structure : list of int\n            Number of residual blocks in each of the six modules of the network.\n        norm_act : callable\n            Function to create normalization / activation Module.\n        classes : int\n            If not `0` also include global average pooling and a fully-connected layer with `classes` outputs at the end\n            of the network.\n        """"""\n        super(WiderResNet, self).__init__()\n        self.structure = structure\n\n        if len(structure) != 6:\n            raise ValueError(""Expected a structure with six values"")\n\n        # Initial layers\n        self.mod1 = nn.Sequential(OrderedDict([(""conv1"", nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False))]))\n\n        # Groups of residual blocks\n        in_channels = 64\n        channels = [(128, 128), (256, 256), (512, 512), (512, 1024), (512, 1024, 2048), (1024, 2048, 4096)]\n        for mod_id, num in enumerate(structure):\n            # Create blocks for module\n            blocks = []\n            for block_id in range(num):\n                blocks.append(\n                    (\n                        ""block%d"" % (block_id + 1),\n                        IdentityResidualBlock(in_channels, channels[mod_id], norm_act=norm_act),\n                    )\n                )\n\n                # Update channels and p_keep\n                in_channels = channels[mod_id][-1]\n\n            # Create module\n            if mod_id <= 4:\n                self.add_module(""pool%d"" % (mod_id + 2), nn.MaxPool2d(3, stride=2, padding=1))\n            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n\n        # Pooling and predictor\n        self.bn_out = norm_act(in_channels)\n        if classes != 0:\n            self.classifier = nn.Sequential(\n                OrderedDict([(""avg_pool"", GlobalAvgPool2d()), (""fc"", nn.Linear(in_channels, classes))])\n            )\n\n    def forward(self, img):\n        out = self.mod1(img)\n        out = self.mod2(self.pool2(out))\n        out = self.mod3(self.pool3(out))\n        out = self.mod4(self.pool4(out))\n        out = self.mod5(self.pool5(out))\n        out = self.mod6(self.pool6(out))\n        out = self.mod7(out)\n        out = self.bn_out(out)\n\n        if hasattr(self, ""classifier""):\n            out = self.classifier(out)\n\n        return out\n\n\nclass WiderResNetA2(nn.Module):\n    def __init__(self, structure, norm_act=ABN, classes=0, dilation=False):\n        """"""Wider ResNet with pre-activation (identity mapping) blocks.\n        This variant uses down-sampling by max-pooling in the first two blocks and by strided convolution in the others.\n\n        Parameters\n        ----------\n        structure : list of int\n            Number of residual blocks in each of the six modules of the network.\n        norm_act : callable\n            Function to create normalization / activation Module.\n        classes : int\n            If not `0` also include global average pooling and a fully-connected layer with `classes` outputs at the end\n            of the network.\n        dilation : bool\n            If `True` apply dilation to the last three modules and change the down-sampling factor from 32 to 8.\n        """"""\n        super(WiderResNetA2, self).__init__()\n        self.structure = structure\n        self.dilation = dilation\n\n        if len(structure) != 6:\n            raise ValueError(""Expected a structure with six values"")\n\n        # Initial layers\n        self.mod1 = nn.Sequential(OrderedDict([(""conv1"", nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False))]))\n\n        # Groups of residual blocks\n        in_channels = 64\n        channels = [(128, 128), (256, 256), (512, 512), (512, 1024), (512, 1024, 2048), (1024, 2048, 4096)]\n        for mod_id, num in enumerate(structure):\n            # Create blocks for module\n            blocks = []\n            for block_id in range(num):\n                if not dilation:\n                    dil = 1\n                    stride = 2 if block_id == 0 and 2 <= mod_id <= 4 else 1\n                else:\n                    if mod_id == 3:\n                        dil = 2\n                    elif mod_id > 3:\n                        dil = 4\n                    else:\n                        dil = 1\n                    stride = 2 if block_id == 0 and mod_id == 2 else 1\n\n                if mod_id == 4:\n                    drop = partial(nn.Dropout2d, p=0.3)\n                elif mod_id == 5:\n                    drop = partial(nn.Dropout2d, p=0.5)\n                else:\n                    drop = None\n\n                blocks.append(\n                    (\n                        ""block%d"" % (block_id + 1),\n                        IdentityResidualBlock(\n                            in_channels, channels[mod_id], norm_act=norm_act, stride=stride, dilation=dil, dropout=drop\n                        ),\n                    )\n                )\n\n                # Update channels and p_keep\n                in_channels = channels[mod_id][-1]\n\n            # Create module\n            if mod_id < 2:\n                self.add_module(""pool%d"" % (mod_id + 2), nn.MaxPool2d(3, stride=2, padding=1))\n            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n\n        # Pooling and predictor\n        self.bn_out = norm_act(in_channels)\n        if classes != 0:\n            self.classifier = nn.Sequential(\n                OrderedDict([(""avg_pool"", GlobalAvgPool2d()), (""fc"", nn.Linear(in_channels, classes))])\n            )\n\n    def forward(self, img):\n        out = self.mod1(img)\n        out = self.mod2(self.pool2(out))\n        out = self.mod3(self.pool3(out))\n        out = self.mod4(out)\n        out = self.mod5(out)\n        out = self.mod6(out)\n        out = self.mod7(out)\n        out = self.bn_out(out)\n\n        if hasattr(self, ""classifier""):\n            return self.classifier(out)\n        else:\n            return out\n\n\ndef wider_resnet_16(num_classes=0, norm_act=ABN):\n    return WiderResNet(structure=[1, 1, 1, 1, 1, 1], norm_act=norm_act, classes=num_classes)\n\n\ndef wider_resnet_20(num_classes=0, norm_act=ABN):\n    return WiderResNet(structure=[1, 1, 1, 3, 1, 1], norm_act=norm_act, classes=num_classes)\n\n\ndef wider_resnet_38(num_classes=0, norm_act=ABN):\n    return WiderResNet(structure=[3, 3, 6, 3, 1, 1], norm_act=norm_act, classes=num_classes)\n\n\ndef wider_resnet_16_a2(num_classes=0, norm_act=ABN):\n    return WiderResNetA2(structure=[1, 1, 1, 1, 1, 1], norm_act=norm_act, classes=num_classes)\n\n\ndef wider_resnet_20_a2(num_classes=0, norm_act=ABN):\n    return WiderResNetA2(structure=[1, 1, 1, 3, 1, 1], norm_act=norm_act, classes=num_classes)\n\n\ndef wider_resnet_38_a2(num_classes=0, norm_act=ABN):\n    return WiderResNetA2(structure=[3, 3, 6, 3, 1, 1], norm_act=norm_act, classes=num_classes)\n'"
pytorch_toolbelt/modules/decoders/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .common import *\nfrom .deeplab import *\nfrom .fpn_cat import *\nfrom .fpn_sum import *\nfrom .hrnet import *\nfrom .pyramid_pooling import *\nfrom .unet import *\nfrom .unet_v2 import *\nfrom .upernet import *\nfrom .can import *'
pytorch_toolbelt/modules/decoders/can.py,3,"b'from typing import List\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom ..dsconv import DepthwiseSeparableConv2d\n\n__all__ = [""CANDecoder""]\n\n\nclass RCM(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n\n        self.block = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n        )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        return self.block(x) + x\n\n\ndef cfm_branch(in_channels: int, out_channels: int, kernel_size: int):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2, bias=False),\n        nn.BatchNorm2d(out_channels),\n    )\n\n\ndef ds_cfm_branch(in_channels: int, out_channels: int, kernel_size: int):\n    return nn.Sequential(\n        DepthwiseSeparableConv2d(\n            in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2, bias=False\n        ),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True),\n        DepthwiseSeparableConv2d(\n            out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2, bias=False\n        ),\n        nn.BatchNorm2d(out_channels),\n    )\n\n\nclass CFM(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_sizes=[3, 5, 7, 11]):\n        super().__init__()\n        self.gp_branch = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n        )\n\n        self.conv_branches = nn.ModuleList(ds_cfm_branch(in_channels, out_channels, ks) for ks in kernel_sizes)\n\n    def forward(self, x):\n        gp = self.gp_branch(x)\n        gp = gp.expand_as(x)\n\n        conv_branches = [conv(x) for conv in self.conv_branches]\n        return torch.cat(conv_branches + [gp], dim=1)\n\n\nclass AMM(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int):\n        super().__init__()\n        self.conv_bn_relu = nn.Sequential(\n            DepthwiseSeparableConv2d(in_channels + out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, encoder, decoder):\n        decoder = F.interpolate(decoder, size=encoder.size()[2:], mode=""bilinear"", align_corners=False)\n        x = torch.cat([encoder, decoder], dim=1)\n        x = self.conv_bn_relu(x)\n        x = F.adaptive_avg_pool2d(x, 1) * x\n        return encoder + x\n\n\nclass CANDecoder(nn.Module):\n    """"""\n    Context Aggregation Network\n    """"""\n\n    def __init__(self, features: List[int], out_channels=256):\n        super().__init__()\n\n        self.encoder_rcm = nn.ModuleList(RCM(in_channels, out_channels) for in_channels in features)\n        self.cfm = nn.Sequential(CFM(out_channels, out_channels), RCM(out_channels * 5, out_channels))\n\n        self.amm_blocks = nn.ModuleList(AMM(out_channels, out_channels) for in_channels in features[:-1])\n        self.rcm_blocks = nn.ModuleList(RCM(out_channels, out_channels) for in_channels in features[:-1])\n\n        self.output_filters = [out_channels] * len(features)\n\n    def forward(self, features):\n        features = [rcm(x) for x, rcm in zip(features, self.encoder_rcm)]\n\n        x = self.cfm(features[-1])\n        outputs = [x]\n        num_blocks = len(self.amm_blocks)\n        for index in range(num_blocks):\n            block_index = num_blocks - index - 1\n            encoder_input = features[block_index]\n            x = self.amm_blocks[block_index](encoder_input, x)\n            x = self.rcm_blocks[block_index](x)\n            outputs.append(x)\n\n        return outputs[::-1]\n'"
pytorch_toolbelt/modules/decoders/common.py,0,"b'from torch import nn, Tensor\n\n__all__ = [""DecoderModule"", ""SegmentationDecoderModule""]\n\nfrom typing import List\n\n\nclass DecoderModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, feature_maps: List[Tensor]) -> List[Tensor]:  # skipcq: PYL-W0221\n        raise NotImplementedError\n\n    def set_trainable(self, trainable):\n        for param in self.parameters():\n            param.requires_grad = bool(trainable)\n\n\nclass SegmentationDecoderModule(DecoderModule):\n    """"""\n    A placeholder for future. Indicates sub-class decoders are suitable for segmentation tasks\n    """"""\n\n    pass\n'"
pytorch_toolbelt/modules/decoders/deeplab.py,3,"b'from typing import List\n\nimport torch\nfrom torch import nn, Tensor\nimport torch.nn.functional as F\n\nfrom .common import DecoderModule\nfrom ..activations import ABN\n\n__all__ = [""DeeplabV3Decoder""]\n\n\nclass ASPPModule(nn.Module):\n    def __init__(self, inplanes, planes, kernel_size, padding, dilation, abn_block=ABN):\n        super(ASPPModule, self).__init__()\n        self.atrous_conv = nn.Conv2d(\n            inplanes, planes, kernel_size=kernel_size, stride=1, padding=padding, dilation=dilation, bias=False\n        )\n        self.abn = abn_block(planes)\n\n    def forward(self, x):  # skipcq: PYL-W0221\n        x = self.atrous_conv(x)\n        x = self.abn(x)\n        return x\n\n\nclass ASPP(nn.Module):\n    def __init__(self, inplanes: int, output_stride: int, output_features: int, dropout=0.5, abn_block=ABN):\n        super(ASPP, self).__init__()\n\n        if output_stride == 32:\n            dilations = [1, 3, 6, 9]\n        elif output_stride == 16:\n            dilations = [1, 6, 12, 18]\n        elif output_stride == 8:\n            dilations = [1, 12, 24, 36]\n        else:\n            raise NotImplementedError\n\n        self.aspp1 = ASPPModule(inplanes, output_features, 1, padding=0, dilation=dilations[0])\n        self.aspp2 = ASPPModule(inplanes, output_features, 3, padding=dilations[1], dilation=dilations[1])\n        self.aspp3 = ASPPModule(inplanes, output_features, 3, padding=dilations[2], dilation=dilations[2])\n        self.aspp4 = ASPPModule(inplanes, output_features, 3, padding=dilations[3], dilation=dilations[3])\n\n        self.global_avg_pool = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Conv2d(inplanes, output_features, 1, stride=1, bias=False),\n            abn_block(output_features),\n        )\n        self.conv1 = nn.Conv2d(output_features * 5, output_features, 1, bias=False)\n        self.abn1 = abn_block(output_features)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):  # skipcq: PYL-W0221\n        x1 = self.aspp1(x)\n        x2 = self.aspp2(x)\n        x3 = self.aspp3(x)\n        x4 = self.aspp4(x)\n        x5 = self.global_avg_pool(x)\n        x5 = F.interpolate(x5, size=x4.size()[2:], mode=""bilinear"", align_corners=False)\n        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n\n        x = self.conv1(x)\n        x = self.abn1(x)\n\n        return self.dropout(x)\n\n\nclass DeeplabV3Decoder(DecoderModule):\n    def __init__(\n        self,\n        feature_maps: List[int],\n        num_classes: int,\n        output_stride=32,\n        high_level_bottleneck=256,\n        low_level_bottleneck=32,\n        dropout=0.5,\n        abn_block=ABN,\n    ):\n        super(DeeplabV3Decoder, self).__init__()\n\n        self.aspp = ASPP(feature_maps[-1], output_stride, high_level_bottleneck, dropout=dropout, abn_block=abn_block)\n\n        self.conv1 = nn.Conv2d(feature_maps[0], low_level_bottleneck, 1, bias=False)\n        self.abn1 = abn_block(low_level_bottleneck)\n\n        self.last_conv = nn.Sequential(\n            nn.Conv2d(\n                high_level_bottleneck + low_level_bottleneck,\n                high_level_bottleneck,\n                kernel_size=3,\n                padding=1,\n                bias=False,\n            ),\n            abn_block(high_level_bottleneck),\n            nn.Dropout(dropout),\n            nn.Conv2d(high_level_bottleneck, high_level_bottleneck, kernel_size=3, padding=1, bias=False),\n            abn_block(high_level_bottleneck),\n            nn.Dropout(dropout * 0.2),  # 5 times smaller dropout rate\n            nn.Conv2d(high_level_bottleneck, num_classes, kernel_size=1),\n        )\n\n        self.dsv = nn.Conv2d(high_level_bottleneck, num_classes, kernel_size=1)\n\n    def forward(self, feature_maps: List[Tensor]) -> List[Tensor]:\n        low_level_feat = feature_maps[0]\n        low_level_feat = self.conv1(low_level_feat)\n        low_level_feat = self.abn1(low_level_feat)\n\n        high_level_features = feature_maps[-1]\n        high_level_features = self.aspp(high_level_features)\n\n        mask_dsv = self.dsv(high_level_features)\n\n        high_level_features = F.interpolate(\n            high_level_features, size=low_level_feat.size()[2:], mode=""bilinear"", align_corners=False\n        )\n        high_level_features = torch.cat([high_level_features, low_level_feat], dim=1)\n        mask = self.last_conv(high_level_features)\n\n        return [mask, mask_dsv]\n'"
pytorch_toolbelt/modules/decoders/fpn_cat.py,1,"b'from typing import List, Union\n\nimport torch\nfrom torch import nn, Tensor\n\nfrom .common import SegmentationDecoderModule\nfrom .. import conv1x1\nfrom ..activations import ABN\nfrom ..fpn import FPNContextBlock, FPNBottleneckBlock\n\n__all__ = [""FPNCatDecoderBlock"", ""FPNCatDecoder""]\n\n\nclass FPNCatDecoderBlock(nn.Module):\n    """"""\n    Simple prediction block composed of (Conv + BN + Activation) repeated twice\n    """"""\n\n    def __init__(self, input_features: int, output_features: int, abn_block=ABN, dropout=0.0):\n        super().__init__()\n        self.conv1 = nn.Conv2d(input_features, output_features, kernel_size=3, padding=1, bias=False)\n        self.abn1 = abn_block(output_features)\n        self.conv2 = nn.Conv2d(output_features, output_features, kernel_size=3, padding=1, bias=False)\n        self.abn2 = abn_block(output_features)\n        self.drop2 = nn.Dropout2d(dropout)\n\n    def forward(self, x: Tensor) -> Tensor:\n        x = self.conv1(x)\n        x = self.abn1(x)\n        x = self.conv2(x)\n        x = self.abn2(x)\n        x = self.drop2(x)\n        return x\n\n\nclass FPNCatDecoder(SegmentationDecoderModule):\n    """"""\n    Feature pyramid network decoder with concatenation between intermediate layers:\n\n        Input\n        fm[0] -> predict(concat(bottleneck[0](fm[0]), upsample(fpn[1]))) -> fpn[0] -> output[0](fpn[0])\n        fm[1] -> predict(concat(bottleneck[1](fm[1]), upsample(fpn[2]))) -> fpn[1] -> output[1](fpn[1])\n        fm[2] -> predict(concat(bottleneck[2](fm[2]), upsample(fpn[3]))) -> fpn[2] -> output[2](fpn[2])\n        ...\n        fm[n] -> predict(concat(bottleneck[n](fm[n]), upsample(context)) -> fpn[n] -> output[n](fpn[n])\n        fm[n] -> context_block(feature_map[n]) -> context\n    """"""\n\n    def __init__(\n        self,\n        feature_maps: List[int],\n        fpn_channels: int,\n        context_block=FPNContextBlock,\n        bottleneck_block=FPNBottleneckBlock,\n        predict_block: Union[nn.Identity, conv1x1, nn.Module] = conv1x1,\n        output_block: Union[nn.Identity, conv1x1, nn.Module] = nn.Identity,\n        prediction_channels: int = None,\n        upsample_block=nn.Upsample,\n    ):\n        """"""\n        Create a new instance of FPN decoder with concatenation of consecutive feature maps.\n        :param feature_maps: Number of channels in input feature maps (fine to coarse).\n            For instance - [64, 256, 512, 2048]\n        :param fpn_channels: FPN channels\n        :param context_block:\n        :param bottleneck_block:\n        :param predict_block:\n        :param output_block: Optional prediction block to apply to FPN feature maps before returning from decoder\n        :param prediction_channels: Number of prediction channels\n        :param upsample_block:\n        """"""\n        super().__init__()\n\n        self.context = context_block(feature_maps[-1], fpn_channels)\n\n        self.bottlenecks = nn.ModuleList(\n            [bottleneck_block(in_channels, fpn_channels) for in_channels in reversed(feature_maps)]\n        )\n\n        self.predicts = nn.ModuleList(\n            [predict_block(fpn_channels + fpn_channels, fpn_channels) for _ in reversed(feature_maps)]\n        )\n\n        if issubclass(output_block, nn.Identity):\n            self.channels = [fpn_channels] * len(feature_maps)\n            self.outputs = nn.ModuleList([output_block() for _ in reversed(feature_maps)])\n        else:\n            self.channels = [prediction_channels] * len(feature_maps)\n            self.outputs = nn.ModuleList(\n                [output_block(fpn_channels, prediction_channels) for _ in reversed(feature_maps)]\n            )\n\n        if issubclass(upsample_block, nn.Upsample):\n            self.upsamples = nn.ModuleList([upsample_block(scale_factor=2) for _ in reversed(feature_maps)])\n        else:\n            self.upsamples = nn.ModuleList(\n                [upsample_block(fpn_channels, fpn_channels) for in_channels in reversed(feature_maps)]\n            )\n\n    def forward(self, feature_maps: List[Tensor]) -> List[Tensor]:\n        last_feature_map = feature_maps[-1]\n        feature_maps = reversed(feature_maps)\n\n        outputs = []\n\n        fpn = self.context(last_feature_map)\n\n        for feature_map, bottleneck, upsample, predict_block, output_block in zip(\n            feature_maps, self.bottlenecks, self.upsamples, self.predicts, self.outputs\n        ):\n            fpn = torch.cat([bottleneck(feature_map), upsample(fpn)], dim=1)\n            fpn = predict_block(fpn)\n            outputs.append(output_block(fpn))\n\n        # Returns list of tensors in same order as input (fine-to-coarse)\n        return outputs[::-1]\n'"
pytorch_toolbelt/modules/decoders/fpn_sum.py,0,"b'from typing import List, Union\nfrom torch import Tensor, nn\nimport inspect\n\nfrom .common import SegmentationDecoderModule\nfrom .. import conv1x1, FPNContextBlock, FPNBottleneckBlock\n\n__all__ = [""FPNSumDecoder""]\n\n\nclass FPNSumDecoder(SegmentationDecoderModule):\n    """"""\n    Feature pyramid network decoder with summation between intermediate layers:\n\n        Input\n        feature_map[0] -> bottleneck[0](feature_map[0]) + upsample(fpn[1]) -> fpn[0]\n        feature_map[1] -> bottleneck[1](feature_map[1]) + upsample(fpn[2]) -> fpn[1]\n        feature_map[2] -> bottleneck[2](feature_map[2]) + upsample(fpn[3]) -> fpn[2]\n        ...\n        feature_map[n] -> bottleneck[n](feature_map[n]) + upsample(context) -> fpn[n]\n        feature_map[n] -> context_block(feature_map[n]) -> context\n    """"""\n\n    def __init__(\n        self,\n        feature_maps: List[int],\n        fpn_channels: int,\n        context_block=FPNContextBlock,\n        bottleneck_block=FPNBottleneckBlock,\n        prediction_block: Union[nn.Identity, conv1x1, nn.Module] = nn.Identity,\n        prediction_channels: int = None,\n        upsample_block=nn.Upsample,\n    ):\n        """"""\n        Create a new instance of FPN decoder with summation of consecutive feature maps.\n        :param feature_maps: Number of channels in input feature maps (fine to coarse).\n            For instance - [64, 256, 512, 2048]\n        :param fpn_channels: FPN channels\n        :param context_block:\n        :param bottleneck_block:\n        :param prediction_block: Optional prediction block to apply to FPN feature maps before returning from decoder\n        :param prediction_channels: Number of prediction channels\n        :param upsample_block:\n        """"""\n        super().__init__()\n\n        self.context = context_block(feature_maps[-1], fpn_channels)\n\n        self.bottlenecks = nn.ModuleList(\n            [bottleneck_block(in_channels, fpn_channels) for in_channels in reversed(feature_maps)]\n        )\n\n        if inspect.isclass(prediction_block) and issubclass(prediction_block, nn.Identity):\n            self.outputs = nn.ModuleList([prediction_block() for _ in reversed(feature_maps)])\n            self.channels = [fpn_channels] * len(feature_maps)\n        else:\n            self.outputs = nn.ModuleList(\n                [prediction_block(fpn_channels, prediction_channels) for _ in reversed(feature_maps)]\n            )\n            self.channels = [prediction_channels] * len(feature_maps)\n\n        if issubclass(upsample_block, nn.Upsample):\n            self.upsamples = nn.ModuleList([upsample_block(scale_factor=2) for _ in reversed(feature_maps)])\n        else:\n            self.upsamples = nn.ModuleList(\n                [upsample_block(fpn_channels, fpn_channels) for in_channels in reversed(feature_maps)]\n            )\n\n    def forward(self, feature_maps: List[Tensor]) -> List[Tensor]:\n        last_feature_map = feature_maps[-1]\n        feature_maps = reversed(feature_maps)\n\n        outputs = []\n\n        fpn = self.context(last_feature_map)\n\n        for feature_map, bottleneck, upsample, output_block in zip(\n            feature_maps, self.bottlenecks, self.upsamples, self.outputs\n        ):\n            fpn = bottleneck(feature_map) + upsample(fpn)\n            outputs.append(output_block(fpn))\n\n        # Returns list of tensors in same order as input (fine-to-coarse)\n        return outputs[::-1]\n'"
pytorch_toolbelt/modules/decoders/hrnet.py,2,"b'from collections import OrderedDict\nfrom typing import List\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, Tensor\n\nfrom .common import SegmentationDecoderModule\n\n__all__ = [""HRNetSegmentationDecoder""]\n\n\nclass HRNetSegmentationDecoder(SegmentationDecoderModule):\n    def __init__(\n        self,\n        feature_maps: List[int],\n        output_channels: int,\n        dropout=0.0,\n        interpolation_mode=""nearest"",\n        align_corners=None,\n    ):\n        super().__init__()\n        self.interpolation_mode = interpolation_mode\n        self.align_corners = align_corners\n\n        features = sum(feature_maps)\n        self.embedding = nn.Sequential(\n            OrderedDict(\n                [\n                    (\n                        ""conv1"",\n                        nn.Conv2d(in_channels=features, out_channels=features, kernel_size=3, padding=1, bias=False),\n                    ),\n                    (""bn1"", nn.BatchNorm2d(features)),\n                    (""relu"", nn.ReLU(inplace=True)),\n                ]\n            )\n        )\n\n        self.logits = nn.Sequential(\n            OrderedDict(\n                [\n                    (""drop"", nn.Dropout2d(dropout)),\n                    (""final"", nn.Conv2d(in_channels=features, out_channels=output_channels, kernel_size=1)),\n                ]\n            )\n        )\n\n    def forward(self, feature_maps: List[Tensor]):\n        x_size = feature_maps[0].size()[2:]\n\n        resized_feature_maps = [feature_maps[0]]\n        for feature_map in feature_maps[1:]:\n            feature_map = F.interpolate(\n                feature_map, size=x_size, mode=self.interpolation_mode, align_corners=self.align_corners\n            )\n            resized_feature_maps.append(feature_map)\n\n        feature_map = torch.cat(resized_feature_maps, dim=1)\n        embedding = self.embedding(feature_map)\n        return self.logits(embedding)\n'"
pytorch_toolbelt/modules/decoders/pyramid_pooling.py,3,"b'from typing import List\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom .common import DecoderModule\n\n\nclass PPMDecoder(DecoderModule):\n    """"""\n    Pyramid pooling decoder module\n\n    https://github.com/CSAILVision/semantic-segmentation-pytorch/blob/42b7567a43b1dab568e2bbfcbc8872778fbda92a/models/models.py\n    """"""\n\n    def __init__(self, feature_maps: List[int], num_classes=150, channels=512, pool_scales=(1, 2, 3, 6)):\n        super(PPMDecoder, self).__init__()\n\n        fc_dim = feature_maps[-1]\n        self.ppm = []\n        for scale in pool_scales:\n            self.ppm.append(\n                nn.Sequential(\n                    nn.AdaptiveAvgPool2d(scale),\n                    nn.Conv2d(fc_dim, channels, kernel_size=1, bias=False),\n                    nn.BatchNorm2d(channels),\n                    nn.ReLU(inplace=True),\n                )\n            )\n        self.ppm = nn.ModuleList(self.ppm)\n\n        self.conv_last = nn.Sequential(\n            nn.Conv2d(fc_dim + len(pool_scales) * channels, channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(channels),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.1),\n            nn.Conv2d(channels, num_classes, kernel_size=1),\n        )\n\n    def forward(self, feature_maps: List[torch.Tensor]):\n        last_fm = feature_maps[-1]\n\n        input_size = last_fm.size()\n        ppm_out = [last_fm]\n        for pool_scale in self.ppm:\n            input_pooled = pool_scale(last_fm)\n            input_pooled = F.interpolate(input_pooled, size=input_size[2:], mode=""bilinear"", align_corners=False)\n            ppm_out.append(input_pooled)\n        ppm_out = torch.cat(ppm_out, dim=1)\n\n        x = self.conv_last(ppm_out)\n        return x\n'"
pytorch_toolbelt/modules/decoders/unet.py,2,"b'from typing import List, Union\n\nimport torch\nfrom torch import nn\n\nfrom .common import DecoderModule\nfrom ..unet import UnetBlock\n\n__all__ = [""UNetDecoder""]\n\n\nclass UNetDecoder(DecoderModule):\n    def __init__(\n        self,\n        feature_maps: List[int],\n        decoder_features: Union[int, List[int]] = None,\n        unet_block=UnetBlock,\n        upsample_block: Union[nn.Upsample, nn.ConvTranspose2d] = None,\n    ):\n        super().__init__()\n\n        # if not isinstance(decoder_features, list):\n        #     decoder_features = [decoder_features * (2 ** i) for i in range(len(feature_maps))]\n        # else:\n        #     assert len(decoder_features) == len(\n        #         feature_maps\n        #     ), f""Incorrect number of decoder features: {decoder_features}, {feature_maps}""\n\n        if upsample_block is None:\n            upsample_block = nn.ConvTranspose2d\n\n        blocks = []\n        upsamples = []\n\n        num_blocks = len(feature_maps) - 1  # Number of outputs is one less than encoder layers\n\n        if decoder_features is None:\n            decoder_features = [None] * num_blocks\n        else:\n            if len(decoder_features) != num_blocks:\n                raise ValueError(f""decoder_features must have length of {num_blocks}"")\n        in_channels_for_upsample_block = feature_maps[-1]\n\n        for block_index in reversed(range(num_blocks)):\n            features_from_encoder = feature_maps[block_index]\n\n            if isinstance(upsample_block, nn.Upsample):\n                upsamples.append(upsample_block)\n                out_channels_from_upsample_block = in_channels_for_upsample_block\n            elif issubclass(upsample_block, nn.Upsample):\n                upsamples.append(upsample_block(scale_factor=2))\n                out_channels_from_upsample_block = in_channels_for_upsample_block\n            elif issubclass(upsample_block, nn.ConvTranspose2d):\n                up = upsample_block(\n                    in_channels_for_upsample_block,\n                    in_channels_for_upsample_block // 2,\n                    kernel_size=3,\n                    stride=2,\n                    padding=1,\n                )\n                upsamples.append(up)\n                out_channels_from_upsample_block = up.out_channels\n            else:\n                up = upsample_block(in_channels_for_upsample_block)\n                upsamples.append(up)\n                out_channels_from_upsample_block = up.out_channels\n\n            in_channels = features_from_encoder + out_channels_from_upsample_block\n            out_channels = decoder_features[block_index] or in_channels // 2\n            blocks.append(unet_block(in_channels, out_channels))\n\n            in_channels_for_upsample_block = out_channels\n            decoder_features[block_index] = out_channels\n\n        self.blocks = nn.ModuleList(blocks)\n        self.upsamples = nn.ModuleList(upsamples)\n        self.output_filters = decoder_features\n\n    @property\n    def channels(self) -> List[int]:\n        return self.output_filters\n\n    def forward(self, feature_maps: List[torch.Tensor]) -> List[torch.Tensor]:\n        x = feature_maps[-1]\n        outputs = []\n        num_feature_maps = len(feature_maps)\n        for index, (upsample_block, decoder_block) in enumerate(zip(self.upsamples, self.blocks)):\n            encoder_input = feature_maps[num_feature_maps - index - 2]\n\n            if isinstance(upsample_block, nn.ConvTranspose2d):\n                x = upsample_block(x, output_size=encoder_input.size())\n            else:\n                x = upsample_block(x)\n\n            x = torch.cat([x, encoder_input], dim=1)\n            x = decoder_block(x)\n            outputs.append(x)\n\n        # Returns list of tensors in same order as input (fine-to-coarse)\n        return outputs[::-1]\n'"
pytorch_toolbelt/modules/decoders/unet_v2.py,3,"b'from typing import Tuple, List\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom .common import DecoderModule\nfrom ..activations import ABN\n\n__all__ = [""UNetDecoderV2"", ""UnetCentralBlockV2"", ""UnetDecoderBlockV2""]\n\n\nclass UnetCentralBlockV2(nn.Module):\n    def __init__(self, in_dec_filters, out_filters, mask_channels, abn_block=ABN):\n        super().__init__()\n        self.bottleneck = nn.Conv2d(in_dec_filters, out_filters, kernel_size=1)\n\n        self.conv1 = nn.Conv2d(out_filters, out_filters, kernel_size=3, padding=1, stride=2, bias=False)\n        self.abn1 = abn_block(out_filters)\n        self.conv2 = nn.Conv2d(out_filters, out_filters, kernel_size=3, padding=1, bias=False)\n        self.abn2 = abn_block(out_filters)\n        self.dsv = nn.Conv2d(out_filters, mask_channels, kernel_size=1)\n\n    def forward(self, x):\n        x = self.bottleneck(x)\n\n        x = self.conv1(x)\n        x = self.abn1(x)\n        x = self.conv2(x)\n        x = self.abn2(x)\n\n        dsv = self.dsv(x)\n\n        return x, dsv\n\n\nclass UnetDecoderBlockV2(nn.Module):\n    """"""\n    """"""\n\n    def __init__(\n        self,\n        in_dec_filters: int,\n        in_enc_filters: int,\n        out_filters: int,\n        mask_channels: int,\n        abn_block=ABN,\n        pre_dropout_rate=0.0,\n        post_dropout_rate=0.0,\n        scale_factor=None,\n        scale_mode=""nearest"",\n        align_corners=None,\n    ):\n        super(UnetDecoderBlockV2, self).__init__()\n\n        self.scale_factor = scale_factor\n        self.scale_mode = scale_mode\n        self.align_corners = align_corners\n\n        self.bottleneck = nn.Conv2d(in_dec_filters + in_enc_filters, out_filters, kernel_size=1)\n\n        self.conv1 = nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=1, padding=1, bias=False)\n        self.abn1 = abn_block(out_filters)\n        self.conv2 = nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=1, padding=1, bias=False)\n        self.abn2 = abn_block(out_filters)\n\n        self.pre_drop = nn.Dropout2d(pre_dropout_rate, inplace=True)\n\n        self.post_drop = nn.Dropout2d(post_dropout_rate)\n\n        self.dsv = nn.Conv2d(out_filters, mask_channels, kernel_size=1)\n\n    def forward(self, x: torch.Tensor, enc: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n        if self.scale_factor is not None:\n            x = F.interpolate(\n                x, scale_factor=self.scale_factor, mode=self.scale_mode, align_corners=self.align_corners\n            )\n        else:\n            lat_size = enc.size()[2:]\n            x = F.interpolate(x, size=lat_size, mode=self.scale_mode, align_corners=self.align_corners)\n\n        x = torch.cat([x, enc], 1)\n\n        x = self.bottleneck(x)\n        x = self.pre_drop(x)\n\n        x = self.conv1(x)\n        x = self.abn1(x)\n\n        x = self.conv2(x)\n        x = self.abn2(x)\n\n        x = self.post_drop(x)\n\n        dsv = self.dsv(x)\n        return x, dsv\n\n\nclass UNetDecoderV2(DecoderModule):\n    def __init__(self, feature_maps: List[int], decoder_features: int, mask_channels: int, dropout=0.0, abn_block=ABN):\n        super().__init__()\n\n        if not isinstance(decoder_features, list):\n            decoder_features = [decoder_features * (2 ** i) for i in range(len(feature_maps))]\n\n        blocks = []\n        for block_index, in_enc_features in enumerate(feature_maps[:-1]):\n            blocks.append(\n                UnetDecoderBlockV2(\n                    decoder_features[block_index + 1],\n                    in_enc_features,\n                    decoder_features[block_index],\n                    mask_channels,\n                    abn_block=abn_block,\n                    post_dropout_rate=dropout,\n                )\n            )\n\n        self.center = UnetCentralBlockV2(feature_maps[-1], decoder_features[-1], mask_channels, abn_block=abn_block)\n        self.blocks = nn.ModuleList(blocks)\n        self.output_filters = decoder_features\n\n        self.final = nn.Conv2d(decoder_features[0], mask_channels, kernel_size=1)\n\n    def forward(self, feature_maps):\n\n        output, dsv = self.center(feature_maps[-1])\n        dsv_list = [dsv]\n\n        for decoder_block, encoder_output in zip(reversed(self.blocks), reversed(feature_maps[:-1])):\n            output, dsv = decoder_block(output, encoder_output)\n            dsv_list.append(dsv)\n\n        dsv_list = list(reversed(dsv_list))\n\n        output = self.final(output)\n        return output, dsv_list\n'"
pytorch_toolbelt/modules/decoders/upernet.py,3,"b'from typing import List\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\ndef conv3x3_bn_relu(in_planes, out_planes, stride=1):\n    """"""\n    3x3 convolution + BN + relu\n    """"""\n    return nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False),\n        nn.BatchNorm2d(out_planes),\n        nn.ReLU(inplace=True),\n    )\n\n\nclass UPerNet(nn.Module):\n    def __init__(self, output_filters: List[int], num_classes=150, pool_scales=(1, 2, 3, 6), fpn_dim=256):\n        super(UPerNet, self).__init__()\n\n        last_fm_dim = output_filters[-1]\n\n        # PPM Module\n        self.ppm_pooling = []\n        self.ppm_conv = []\n\n        for scale in pool_scales:\n            self.ppm_pooling.append(nn.AdaptiveAvgPool2d(scale))\n            self.ppm_conv.append(\n                nn.Sequential(\n                    nn.Conv2d(last_fm_dim, 512, kernel_size=1, bias=False), nn.BatchNorm2d(512), nn.ReLU(inplace=True)\n                )\n            )\n        self.ppm_pooling = nn.ModuleList(self.ppm_pooling)\n        self.ppm_conv = nn.ModuleList(self.ppm_conv)\n        self.ppm_last_conv = conv3x3_bn_relu(last_fm_dim + len(pool_scales) * 512, fpn_dim, 1)\n\n        # FPN Module\n        self.fpn_in = []\n        for fpn_inplane in output_filters[:-1]:  # skip the top layer\n            self.fpn_in.append(\n                nn.Sequential(\n                    nn.Conv2d(fpn_inplane, fpn_dim, kernel_size=1, bias=False),\n                    nn.BatchNorm2d(fpn_dim),\n                    nn.ReLU(inplace=True),\n                )\n            )\n        self.fpn_in = nn.ModuleList(self.fpn_in)\n\n        self.fpn_out = []\n        for i in range(len(output_filters) - 1):  # skip the top layer\n            self.fpn_out.append(nn.Sequential(conv3x3_bn_relu(fpn_dim, fpn_dim, 1)))\n        self.fpn_out = nn.ModuleList(self.fpn_out)\n\n        self.conv_last = nn.Sequential(\n            conv3x3_bn_relu(len(output_filters) * fpn_dim, fpn_dim, 1), nn.Conv2d(fpn_dim, num_classes, kernel_size=1)\n        )\n\n    def forward(self, feature_maps):\n        last_fm = feature_maps[-1]\n\n        input_size = last_fm.size()\n        ppm_out = [last_fm]\n        for pool_scale, pool_conv in zip(self.ppm_pooling, self.ppm_conv):\n            ppm_out.append(\n                pool_conv(\n                    F.interpolate(\n                        pool_scale(last_fm), (input_size[2], input_size[3]), mode=""bilinear"", align_corners=False\n                    )\n                )\n            )\n        ppm_out = torch.cat(ppm_out, 1)\n        f = self.ppm_last_conv(ppm_out)\n\n        fpn_feature_list = [f]\n        for i in reversed(range(len(feature_maps) - 1)):\n            conv_x = feature_maps[i]\n            conv_x = self.fpn_in[i](conv_x)  # lateral branch\n\n            f = F.interpolate(f, size=conv_x.size()[2:], mode=""bilinear"", align_corners=False)  # top-down branch\n            f = conv_x + f\n\n            fpn_feature_list.append(self.fpn_out[i](f))\n\n        fpn_feature_list.reverse()  # [P2 - P5]\n        output_size = fpn_feature_list[0].size()[2:]\n        fusion_list = [fpn_feature_list[0]]\n        for i in range(1, len(fpn_feature_list)):\n            fusion_list.append(F.interpolate(fpn_feature_list[i], output_size, mode=""bilinear"", align_corners=False))\n\n        fusion_out = torch.cat(fusion_list, 1)\n        x = self.conv_last(fusion_out)\n        return x\n'"
pytorch_toolbelt/modules/encoders/__init__.py,0,"b'""""""Wrappers for different backbones for models that follows Encoder-Decoder architecture.\n\nEncodes listed here provides easy way to swap backbone of classification/segmentation/detection model.\n""""""\nfrom .common import *\nfrom .densenet import *\nfrom .efficientnet import *\nfrom .hrnet import *\nfrom .inception import *\nfrom .mobilenet import *\nfrom .resnet import *\nfrom .seresnet import *\nfrom .squeezenet import *\nfrom .unet import *\nfrom .wide_resnet import *\nfrom .xresnet import *\nfrom .hourglass import *\n'"
pytorch_toolbelt/modules/encoders/common.py,1,"b'""""""Wrappers for different backbones for models that follows Encoder-Decoder architecture.\n\nEncodes listed here provides easy way to swap backbone of classification/segmentation/detection model.\n""""""\nimport math\nimport warnings\nfrom typing import List\n\nimport torch\nfrom torch import nn, Tensor\n\n__all__ = [""EncoderModule"", ""_take"", ""make_n_channel_input""]\n\nfrom pytorch_toolbelt.utils.support import pytorch_toolbelt_deprecated\n\n\ndef _take(elements, indexes):\n    return list([elements[i] for i in indexes])\n\n\ndef make_n_channel_input(conv: nn.Conv2d, in_channels: int, mode=""auto""):\n    assert isinstance(conv, nn.Conv2d)\n    if conv.in_channels == in_channels:\n        warnings.warn(""make_n_channel_input call is spurious"")\n        return conv\n\n    new_conv = nn.Conv2d(\n        in_channels,\n        out_channels=conv.out_channels,\n        kernel_size=conv.kernel_size,\n        stride=conv.stride,\n        padding=conv.padding,\n        dilation=conv.dilation,\n        groups=conv.groups,\n        bias=conv.bias is not None,\n        padding_mode=conv.padding_mode,\n    )\n\n    w = conv.weight\n    if in_channels > conv.in_channels:\n        n = math.ceil(in_channels / float(conv.in_channels))\n        w = torch.cat([w] * n, dim=1)\n        w = w[:, :in_channels, ...]\n        new_conv.weight = nn.Parameter(w, requires_grad=True)\n    else:\n        w = w[:, 0:in_channels, ...]\n        new_conv.weight = nn.Parameter(w, requires_grad=True)\n\n    return new_conv\n\n\nclass EncoderModule(nn.Module):\n    def __init__(self, channels: List[int], strides: List[int], layers: List[int]):\n        super().__init__()\n        assert len(channels) == len(strides)\n\n        self._layers = layers\n\n        self._output_strides = _take(strides, layers)\n        self._output_filters = _take(channels, layers)\n\n    def forward(self, x: Tensor) -> List[Tensor]:  # skipcq: PYL-W0221\n        output_features = []\n        for layer in self.encoder_layers:\n            output = layer(x)\n            output_features.append(output)\n            x = output\n        # Return only features that were requested\n        return _take(output_features, self._layers)\n\n    @property\n    def channels(self) -> List[int]:\n        return self._output_filters\n\n    @property\n    def strides(self) -> List[int]:\n        return self._output_strides\n\n    @property\n    @pytorch_toolbelt_deprecated(""This property is deprecated, please use .strides instead."")\n    def output_strides(self) -> List[int]:\n        return self.strides\n\n    @property\n    @pytorch_toolbelt_deprecated(""This property is deprecated, please use .channels instead."")\n    def output_filters(self) -> List[int]:\n        return self.channels\n\n    @property\n    @pytorch_toolbelt_deprecated(""This property is deprecated, please don\'t use it"")\n    def encoder_layers(self) -> List[nn.Module]:\n        raise NotImplementedError\n\n    def set_trainable(self, trainable):\n        for param in self.parameters():\n            param.requires_grad = bool(trainable)\n\n    def change_input_channels(self, input_channels: int, mode=""auto""):\n        """"""\n        Change number of channels expected in the input tensor. By default,\n        all encoders assume 3-channel image in BCHW notation with C=3.\n        This method changes first convolution to have user-defined number of\n        channels as input.\n        """"""\n        raise NotImplementedError\n'"
pytorch_toolbelt/modules/encoders/densenet.py,0,"b'from collections import OrderedDict\nfrom typing import List\n\nfrom torch import nn\nfrom torchvision.models import densenet121, densenet161, densenet169, densenet201, DenseNet\n\nfrom .common import EncoderModule, _take, make_n_channel_input\n\n__all__ = [""DenseNetEncoder"", ""DenseNet121Encoder"", ""DenseNet169Encoder"", ""DenseNet161Encoder"", ""DenseNet201Encoder""]\n\n\nclass DenseNetEncoder(EncoderModule):\n    def __init__(\n        self, densenet: DenseNet, strides: List[int], channels: List[int], layers: List[int], first_avg_pool=False\n    ):\n        if layers is None:\n            layers = [1, 2, 3, 4]\n\n        super().__init__(channels, strides, layers)\n\n        def except_pool(block: nn.Module):\n            del block.pool\n            return block\n\n        self.layer0 = nn.Sequential(\n            OrderedDict(\n                [\n                    (""conv0"", densenet.features.conv0),\n                    (""bn0"", densenet.features.norm0),\n                    (""act0"", densenet.features.relu0),\n                ]\n            )\n        )\n\n        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n        self.pool0 = self.avg_pool if first_avg_pool else densenet.features.pool0\n\n        self.layer1 = nn.Sequential(densenet.features.denseblock1, except_pool(densenet.features.transition1))\n\n        self.layer2 = nn.Sequential(densenet.features.denseblock2, except_pool(densenet.features.transition2))\n\n        self.layer3 = nn.Sequential(densenet.features.denseblock3, except_pool(densenet.features.transition3))\n\n        self.layer4 = nn.Sequential(densenet.features.denseblock4)\n\n        self._output_strides = _take(strides, layers)\n        self._output_filters = _take(channels, layers)\n\n    @property\n    def encoder_layers(self):\n        return [self.layer0, self.layer1, self.layer2, self.layer3, self.layer4]\n\n    @property\n    def output_strides(self):\n        return self._output_strides\n\n    @property\n    def output_filters(self):\n        return self._output_filters\n\n    def forward(self, x):\n        output_features = []\n        for layer in self.encoder_layers:\n            output = layer(x)\n            output_features.append(output)\n\n            if layer == self.layer0:\n                # Fist maxpool operator is not a part of layer0 because we want that layer0 output to have stride of 2\n                output = self.pool0(output)\n            else:\n                output = self.avg_pool(output)\n\n            x = output\n\n        # Return only features that were requested\n        return _take(output_features, self._layers)\n\n    def change_input_channels(self, input_channels: int, mode=""auto""):\n        self.layer0.conv0 = make_n_channel_input(self.layer0.conv0, input_channels, mode)\n        return self\n\n\nclass DenseNet121Encoder(DenseNetEncoder):\n    def __init__(self, layers=None, pretrained=True, memory_efficient=False, first_avg_pool=False):\n        densenet = densenet121(pretrained=pretrained, memory_efficient=memory_efficient)\n        strides = [2, 4, 8, 16, 32]\n        channels = [64, 128, 256, 512, 1024]\n        super().__init__(densenet, strides, channels, layers, first_avg_pool)\n\n\nclass DenseNet161Encoder(DenseNetEncoder):\n    def __init__(self, layers=None, pretrained=True, memory_efficient=False, first_avg_pool=False):\n        densenet = densenet161(pretrained=pretrained, memory_efficient=memory_efficient)\n        strides = [2, 4, 8, 16, 32]\n        channels = [96, 192, 384, 1056, 2208]\n        super().__init__(densenet, strides, channels, layers, first_avg_pool)\n\n\nclass DenseNet169Encoder(DenseNetEncoder):\n    def __init__(self, layers=None, pretrained=True, memory_efficient=False, first_avg_pool=False):\n        densenet = densenet169(pretrained=pretrained, memory_efficient=memory_efficient)\n        strides = [2, 4, 8, 16, 32]\n        channels = [64, 128, 256, 640, 1664]\n        super().__init__(densenet, strides, channels, layers, first_avg_pool)\n\n\nclass DenseNet201Encoder(DenseNetEncoder):\n    def __init__(self, layers=None, pretrained=True, memory_efficient=False, first_avg_pool=False):\n        densenet = densenet201(pretrained=pretrained, memory_efficient=memory_efficient)\n        strides = [2, 4, 8, 16, 32]\n        channels = [64, 128, 256, 896, 1920]\n        super().__init__(densenet, strides, channels, layers, first_avg_pool)\n'"
pytorch_toolbelt/modules/encoders/efficientnet.py,6,"b'import math\nfrom collections import OrderedDict\nfrom copy import deepcopy\nfrom functools import partial\nfrom typing import List\n\nimport torch\nfrom torch import nn\n\nfrom .common import EncoderModule, make_n_channel_input, _take\n\n__all__ = [\n    ""EfficientNetEncoder"",\n    ""EfficientNetB0Encoder"",\n    ""EfficientNetB1Encoder"",\n    ""EfficientNetB2Encoder"",\n    ""EfficientNetB3Encoder"",\n    ""EfficientNetB4Encoder"",\n    ""EfficientNetB5Encoder"",\n    ""EfficientNetB6Encoder"",\n    ""EfficientNetB7Encoder"",\n]\n\nfrom .. import ABN, SpatialGate2d, ACT_SWISH\n\n\ndef round_filters(filters: int, width_coefficient, depth_divisor, min_depth) -> int:\n    """"""\n    Calculate and round number of filters based on depth multiplier.\n    """"""\n    filters *= width_coefficient\n    min_depth = min_depth or depth_divisor\n    new_filters = max(min_depth, int(filters + depth_divisor / 2) // depth_divisor * depth_divisor)\n    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%\n        new_filters += depth_divisor\n    return int(new_filters)\n\n\ndef round_repeats(repeats: int, depth_multiplier):\n    """"""\n    Round number of filters based on depth multiplier.\n    """"""\n    if not depth_multiplier:\n        return repeats\n    return int(math.ceil(depth_multiplier * repeats))\n\n\ndef drop_connect(inputs, p, training):\n    """"""\n    Drop connect implementation.\n    """"""\n    if not training:\n        return inputs\n    batch_size = inputs.shape[0]\n    keep_prob = 1 - p\n    random_tensor = keep_prob\n    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype)  # uniform [0,1)\n    binary_tensor = torch.floor(random_tensor)\n    output = inputs / keep_prob * binary_tensor\n    return output\n\n\nclass EfficientNetBlockArgs:\n    def __init__(\n        self,\n        input_filters,\n        output_filters,\n        expand_ratio,\n        repeats=1,\n        kernel_size=3,\n        stride=1,\n        se_reduction=4,\n        dropout=0.0,\n        id_skip=True,\n    ):\n        self.in_channels = input_filters\n        self.out_channels = output_filters\n        self.expand_ratio = expand_ratio\n        self.num_repeat = repeats\n        self.se_reduction = se_reduction\n        self.dropout = dropout\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.width_coefficient = 1.0\n        self.depth_coefficient = 1.0\n        self.depth_divisor = 8\n        self.min_filters = None\n        self.id_skip = id_skip\n\n    def __repr__(self):\n        """"""Encode a block args class to a string representation.""""""\n        args = [\n            ""r%d"" % self.num_repeat,\n            ""k%d"" % self.kernel_size,\n            ""s%d"" % self.stride,\n            ""e%s"" % self.expand_ratio,\n            ""i%d"" % self.in_channels,\n            ""o%d"" % self.out_channels,\n        ]\n        if self.se_reduction > 0:\n            args.append(""se%s"" % self.se_reduction)\n        return ""_"".join(args)\n\n    def copy(self):\n        return deepcopy(self)\n\n    def scale(\n        self, width_coefficient: float, depth_coefficient: float, depth_divisor: float = 8.0, min_filters: int = None\n    ):\n        copy = self.copy()\n        copy.in_channels = round_filters(self.in_channels, width_coefficient, depth_divisor, min_filters)\n        copy.out_channels = round_filters(self.out_channels, width_coefficient, depth_divisor, min_filters)\n        copy.num_repeat = round_repeats(self.num_repeat, depth_coefficient)\n        copy.width_coefficient = width_coefficient\n        copy.depth_coefficient = depth_coefficient\n        copy.depth_divisor = depth_divisor\n        copy.min_filters = min_filters\n        return copy\n\n    @staticmethod\n    def B0():\n        params = get_default_efficientnet_params(dropout=0.2)\n        params = [p.scale(width_coefficient=1.0, depth_coefficient=1.0) for p in params]\n        return params\n\n    @staticmethod\n    def B1():\n        params = get_default_efficientnet_params(dropout=0.2)\n        params = [p.scale(width_coefficient=1.0, depth_coefficient=1.1) for p in params]\n        return params\n\n    @staticmethod\n    def B2():\n        params = get_default_efficientnet_params(dropout=0.3)\n        params = [p.scale(width_coefficient=1.1, depth_coefficient=1.2) for p in params]\n        return params\n\n    @staticmethod\n    def B3():\n        params = get_default_efficientnet_params(dropout=0.3)\n        params = [p.scale(width_coefficient=1.2, depth_coefficient=1.4) for p in params]\n        return params\n\n    @staticmethod\n    def B4():\n        params = get_default_efficientnet_params(dropout=0.4)\n        params = [p.scale(width_coefficient=1.4, depth_coefficient=1.8) for p in params]\n        return params\n\n    @staticmethod\n    def B5():\n        params = get_default_efficientnet_params(dropout=0.4)\n        params = [p.scale(width_coefficient=1.6, depth_coefficient=2.2) for p in params]\n        return params\n\n    @staticmethod\n    def B6():\n        params = get_default_efficientnet_params(dropout=0.5)\n        params = [p.scale(width_coefficient=1.8, depth_coefficient=2.6) for p in params]\n        return params\n\n    @staticmethod\n    def B7():\n        params = get_default_efficientnet_params(dropout=0.5)\n        params = [p.scale(width_coefficient=2.0, depth_coefficient=3.1) for p in params]\n        return params\n\n\ndef get_default_efficientnet_params(dropout=0.2) -> List[EfficientNetBlockArgs]:\n    #  _DEFAULT_BLOCKS_ARGS = [\n    #     \'r1_k3_s11_e1_i32_o16_se0.25\', \'r2_k3_s22_e6_i16_o24_se0.25\',\n    #     \'r2_k5_s22_e6_i24_o40_se0.25\', \'r3_k3_s22_e6_i40_o80_se0.25\',\n    #     \'r3_k5_s11_e6_i80_o112_se0.25\', \'r4_k5_s22_e6_i112_o192_se0.25\',\n    #     \'r1_k3_s11_e6_i192_o320_se0.25\',\n    # ]\n    return [\n        EfficientNetBlockArgs(\n            repeats=1, kernel_size=3, stride=1, expand_ratio=1, input_filters=32, output_filters=16, dropout=dropout,\n        ),\n        EfficientNetBlockArgs(\n            repeats=2, kernel_size=3, stride=2, expand_ratio=6, input_filters=16, output_filters=24, dropout=dropout,\n        ),\n        EfficientNetBlockArgs(\n            repeats=2, kernel_size=5, stride=2, expand_ratio=6, input_filters=24, output_filters=40, dropout=dropout,\n        ),\n        EfficientNetBlockArgs(\n            repeats=3, kernel_size=3, stride=2, expand_ratio=6, input_filters=40, output_filters=80, dropout=dropout,\n        ),\n        EfficientNetBlockArgs(\n            repeats=3, kernel_size=5, stride=1, expand_ratio=6, input_filters=80, output_filters=112, dropout=dropout,\n        ),\n        EfficientNetBlockArgs(\n            repeats=4, kernel_size=5, stride=2, expand_ratio=6, input_filters=112, output_filters=192, dropout=dropout,\n        ),\n        EfficientNetBlockArgs(\n            repeats=1, kernel_size=3, stride=1, expand_ratio=6, input_filters=192, output_filters=320, dropout=dropout,\n        ),\n    ]\n\n\nclass MBConvBlock(nn.Module):\n    """"""\n    Mobile Inverted Residual Bottleneck Block\n    Args:\n        block_args (namedtuple): BlockArgs, see above\n        global_params (namedtuple): GlobalParam, see above\n    Attributes:\n        has_se (bool): Whether the block contains a Squeeze and Excitation layer.\n    """"""\n\n    def __init__(self, block_args: EfficientNetBlockArgs, abn_block: ABN):\n        super().__init__()\n\n        self.has_se = block_args.se_reduction is not None\n        self.id_skip = block_args.id_skip  # skip connection and drop connect\n        self.expand_ratio = block_args.expand_ratio\n        self.stride = block_args.stride\n\n        # Expansion phase\n        inp = block_args.in_channels  # number of input channels\n        oup = block_args.in_channels * block_args.expand_ratio  # number of output channels\n\n        if block_args.expand_ratio != 1:\n            self.expand_conv = nn.Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n            self.abn0 = abn_block(oup)\n\n        # Depthwise convolution phase\n        self.depthwise_conv = nn.Conv2d(\n            in_channels=oup,\n            out_channels=oup,\n            groups=oup,  # groups makes it depthwise\n            kernel_size=block_args.kernel_size,\n            padding=block_args.kernel_size // 2,\n            stride=block_args.stride,\n            bias=False,\n        )\n        self.abn1 = abn_block(oup)\n\n        # Squeeze and Excitation layer, if desired\n        if self.has_se:\n            se_channels = max(1, inp // block_args.se_reduction)\n            self.se_block = SpatialGate2d(oup, squeeze_channels=se_channels)\n\n        # Output phase\n        self.project_conv = nn.Conv2d(in_channels=oup, out_channels=block_args.out_channels, kernel_size=1, bias=False)\n        self.abn2 = abn_block(block_args.out_channels)\n\n        self.input_filters = block_args.in_channels\n        self.output_filters = block_args.out_channels\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        pass\n\n    #     if hasattr(self, ""expand_conv""):\n    #         torch.nn.init.kaiming_uniform_(\n    #             self.expand_conv.weight,\n    #             a=abn_params.get(""slope"", 0),\n    #             nonlinearity=sanitize_activation_name(self.abn2[""activation""]),\n    #         )\n    #\n    #     torch.nn.init.kaiming_uniform_(\n    #         self.depthwise_conv.weight,\n    #         a=abn_params.get(""slope"", 0),\n    #         nonlinearity=sanitize_activation_name(abn_params[""activation""]),\n    #     )\n    #     torch.nn.init.kaiming_uniform_(\n    #         self.project_conv.weight,\n    #         a=abn_params.get(""slope"", 0),\n    #         nonlinearity=sanitize_activation_name(abn_params[""activation""]),\n    #     )\n\n    def forward(self, inputs, drop_connect_rate=None):\n        """"""\n        :param inputs: input tensor\n        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n        :return: output of block\n        """"""\n        x = inputs\n        if self.expand_ratio != 1:\n            # Expansion and Depthwise Convolution\n            x = self.abn0(self.expand_conv(inputs))\n\n        x = self.abn1(self.depthwise_conv(x))\n\n        # Squeeze and Excitation\n        if self.has_se:\n            x = self.se_block(x)\n\n        x = self.abn2(self.project_conv(x))\n\n        # Skip connection and drop connect\n        if self.id_skip and self.stride == 1 and self.input_filters == self.output_filters:\n            if drop_connect_rate:\n                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n            x = x + inputs  # skip connection\n        return x\n\n\nclass EfficientNetStem(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, abn_block: ABN):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, bias=True)\n        self.abn = abn_block(out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.abn(x)\n        return x\n\n\nclass EfficientNetEncoder(EncoderModule):\n    @staticmethod\n    def build_layer(block_args: List[EfficientNetBlockArgs], abn_block: ABN):\n        blocks = []\n        for block_index, cfg in enumerate(block_args):\n            module = []\n            # The first block needs to take care of stride and filter size increase.\n            module.append((""mbconv_0"", MBConvBlock(cfg, abn_block)))\n\n            if cfg.num_repeat > 1:\n                cfg = cfg.copy()\n                cfg.stride = 1\n                cfg.in_channels = cfg.out_channels\n\n                for i in range(cfg.num_repeat - 1):\n                    module.append((f""mbconv_{i+1}"", MBConvBlock(cfg, abn_block)))\n\n            module = nn.Sequential(OrderedDict(module))\n            blocks.append((f""block_{block_index}"", module))\n\n        return nn.Sequential(OrderedDict(blocks))\n\n    def __init__(self, encoder_config: List[EfficientNetBlockArgs], in_channels=3, activation=ACT_SWISH, layers=None):\n        if layers is None:\n            layers = [1, 2, 3, 4]\n\n        blocks2layers = [\n            # Layer 0\n            [encoder_config[0], encoder_config[1]],\n            # Layer 1\n            [encoder_config[2]],\n            # Layer 2\n            [encoder_config[3], encoder_config[4]],\n            # Layer 3\n            [encoder_config[5], encoder_config[6]],\n        ]\n\n        filters = [encoder_config[0].in_channels] + [cfg[-1].out_channels for cfg in blocks2layers]\n        strides = [2, 4, 8, 16, 32]\n        abn_block = partial(ABN, activation=activation)\n        super().__init__(filters, strides, layers)\n\n        self.stem = EfficientNetStem(in_channels, encoder_config[0].in_channels, abn_block)\n        self.layers = nn.ModuleList([self.build_layer(cfg, abn_block) for cfg in blocks2layers])\n\n    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:  # skipcq: PYL-W0221\n        x = self.stem(x)\n        output_features = [x]\n        for layer in self.layers:\n            output = layer(x)\n            output_features.append(output)\n            x = output\n        # Return only features that were requested\n        return _take(output_features, self._layers)\n\n    def change_input_channels(self, input_channels: int, mode=""auto""):\n        self.stem.conv = make_n_channel_input(self.stem.conv, input_channels, mode)\n        return self\n\n\nclass EfficientNetB0Encoder(EfficientNetEncoder):\n    def __init__(self, in_channels=3, activation=ACT_SWISH, layers=None):\n        super().__init__(EfficientNetBlockArgs.B0(), in_channels=in_channels, activation=activation, layers=layers)\n\n\nclass EfficientNetB1Encoder(EfficientNetEncoder):\n    def __init__(self, in_channels=3, activation=ACT_SWISH, layers=None):\n        super().__init__(EfficientNetBlockArgs.B1(), in_channels=in_channels, activation=activation, layers=layers)\n\n\nclass EfficientNetB2Encoder(EfficientNetEncoder):\n    def __init__(self, in_channels=3, activation=ACT_SWISH, layers=None):\n        super().__init__(EfficientNetBlockArgs.B2(), in_channels=in_channels, activation=activation, layers=layers)\n\n\nclass EfficientNetB3Encoder(EfficientNetEncoder):\n    def __init__(self, in_channels=3, activation=ACT_SWISH, layers=None):\n        super().__init__(EfficientNetBlockArgs.B3(), in_channels=in_channels, activation=activation, layers=layers)\n\n\nclass EfficientNetB4Encoder(EfficientNetEncoder):\n    def __init__(self, in_channels=3, activation=ACT_SWISH, layers=None):\n        super().__init__(EfficientNetBlockArgs.B4(), in_channels=in_channels, activation=activation, layers=layers)\n\n\nclass EfficientNetB5Encoder(EfficientNetEncoder):\n    def __init__(self, in_channels=3, activation=ACT_SWISH, layers=None):\n        super().__init__(EfficientNetBlockArgs.B5(), in_channels=in_channels, activation=activation, layers=layers)\n\n\nclass EfficientNetB6Encoder(EfficientNetEncoder):\n    def __init__(self, in_channels=3, activation=ACT_SWISH, layers=None):\n        super().__init__(EfficientNetBlockArgs.B6(), in_channels=in_channels, activation=activation, layers=layers)\n\n\nclass EfficientNetB7Encoder(EfficientNetEncoder):\n    def __init__(self, in_channels=3, activation=ACT_SWISH, layers=None):\n        super().__init__(EfficientNetBlockArgs.B7(), in_channels=in_channels, activation=activation, layers=layers)\n'"
pytorch_toolbelt/modules/encoders/hourglass.py,2,"b'from collections import OrderedDict\nfrom typing import List, Callable, Tuple\n\nimport torch\n\nfrom pytorch_toolbelt.modules import ACT_RELU, get_activation_block\nfrom pytorch_toolbelt.modules.encoders import EncoderModule, make_n_channel_input\nfrom torch import nn, Tensor\n\n__all__ = [""StackedHGEncoder"", ""StackedSupervisedHGEncoder""]\n\n\ndef conv1x1_bn_act(in_channels, out_channels, activation=nn.ReLU):\n    return nn.Sequential(\n        OrderedDict(\n            [\n                (""conv"", nn.Conv2d(in_channels, out_channels, kernel_size=1)),\n                (""bn"", nn.BatchNorm2d(out_channels)),\n                (""act"", activation(inplace=True)),\n            ]\n        )\n    )\n\n\nclass HGResidualBlock(nn.Module):\n    def __init__(self, input_channels: int, output_channels: int, reduction=2, activation: Callable = nn.ReLU):\n        super(HGResidualBlock, self).__init__()\n\n        mid_channels = input_channels // reduction\n\n        self.bn1 = nn.BatchNorm2d(input_channels)\n        self.act1 = activation(inplace=True)\n        self.conv1 = nn.Conv2d(input_channels, mid_channels, kernel_size=1, bias=False)\n\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        self.act2 = activation(inplace=True)\n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, padding=1, bias=False)\n\n        self.bn3 = nn.BatchNorm2d(mid_channels)\n        self.act3 = activation(inplace=True)\n        self.conv3 = nn.Conv2d(mid_channels, output_channels, kernel_size=1, bias=True)\n\n        if input_channels == output_channels:\n            self.skip_layer = nn.Identity()\n        else:\n            self.skip_layer = nn.Conv2d(input_channels, output_channels, kernel_size=1)\n            torch.nn.init.zeros_(self.skip_layer.bias)\n\n        torch.nn.init.zeros_(self.conv3.bias)\n\n    def forward(self, x: Tensor) -> Tensor:  # skipcq: PYL-W0221\n        residual = self.skip_layer(x)\n\n        out = self.bn1(x)\n        out = self.act1(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.act2(out)\n        out = self.conv2(out)\n\n        out = self.bn3(out)\n        out = self.act3(out)\n        out = self.conv3(out)\n        out += residual\n        return out\n\n\nclass HGStemBlock(nn.Module):\n    def __init__(self, input_channels, output_channels, activation: Callable = nn.ReLU):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=3, padding=1, stride=2, bias=False)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.act1 = activation(inplace=True)\n\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1, stride=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(32)\n        self.act2 = activation(inplace=True)\n\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=2, bias=False)\n        self.bn3 = nn.BatchNorm2d(64)\n        self.act3 = activation(inplace=True)\n\n        self.residual1 = HGResidualBlock(64, 128)\n        self.residual2 = HGResidualBlock(128, output_channels)\n\n    def forward(self, x: Tensor) -> Tensor:  # skipcq: PYL-W0221\n        x = self.act1(self.bn1(self.conv1(x)))\n        x = self.act2(self.bn2(self.conv2(x)))\n        x = self.act3(self.bn3(self.conv3(x)))\n        x = self.residual1(x)\n        x = self.residual2(x)\n        return x\n\n\nclass HGBlock(nn.Module):\n    def __init__(\n        self,\n        depth: int,\n        input_features: int,\n        features,\n        increase=0,\n        activation=nn.ReLU,\n        repeats=1,\n        pooling_block=nn.MaxPool2d,\n    ):\n        super(HGBlock, self).__init__()\n        nf = features + increase\n\n        self.down = pooling_block(kernel_size=2, padding=0, stride=2)\n\n        if repeats == 1:\n            self.up1 = HGResidualBlock(input_features, features, activation=activation)\n            self.low1 = HGResidualBlock(input_features, nf, activation=activation)\n        else:\n            up_blocks = []\n            up_input_features = input_features\n            for _ in range(repeats):\n                up_blocks.append(HGResidualBlock(up_input_features, features))\n                up_input_features = features\n            self.up1 = nn.Sequential(*up_blocks)\n\n            down_blocks = []\n            down_input_features = input_features\n            for _ in range(repeats):\n                up_blocks.append(HGResidualBlock(down_input_features, nf))\n                down_input_features = nf\n            self.low1 = nn.Sequential(*down_blocks)\n\n        self.depth = depth\n        # Recursive hourglass\n        if self.depth > 1:\n            self.low2 = HGBlock(depth - 1, nf, nf, increase=increase, activation=activation)\n        else:\n            self.low2 = HGResidualBlock(nf, nf, activation=activation)\n        self.low3 = HGResidualBlock(nf, features, activation=activation)\n        self.up = nn.Upsample(scale_factor=2, mode=""nearest"")\n\n    def forward(self, x: Tensor) -> Tensor:  # skipcq: PYL-W0221\n        up1 = self.up1(x)\n        pool1 = self.down(x)\n        low1 = self.low1(pool1)\n        low2 = self.low2(low1)\n        low3 = self.low3(low2)\n        up2 = self.up(low3)\n        hg = up1 + up2\n        return hg\n\n\nclass HGFeaturesBlock(nn.Module):\n    def __init__(self, features: int, activation: Callable, blocks=1):\n        super().__init__()\n        residual_blocks = [HGResidualBlock(features, features, activation=activation) for _ in range(blocks)]\n        self.residuals = nn.Sequential(*residual_blocks)\n        self.linear = conv1x1_bn_act(features, features, activation=activation)\n\n    def forward(self, x: Tensor) -> Tensor:  # skipcq: PYL-W0221\n        x = self.residuals(x)\n        x = self.linear(x)\n        return x\n\n\nclass HGSupervisionBlock(nn.Module):\n    def __init__(self, features, supervision_channels: int):\n        super().__init__()\n        self.squeeze = nn.Conv2d(features, supervision_channels, kernel_size=1)\n        self.expand = nn.Conv2d(supervision_channels, features, kernel_size=1)\n\n    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor]:  # skipcq: PYL-W0221\n        sup_mask = self.squeeze(x)\n        sup_features = self.expand(sup_mask)\n        return sup_mask, sup_features\n\n\nclass StackedHGEncoder(EncoderModule):\n    """"""\n    Original implementation: https://github.com/princeton-vl/pytorch_stacked_hourglass/blob/master/models/layers.py\n    """"""\n\n    def __init__(\n        self,\n        input_channels: int = 3,\n        stack_level: int = 8,\n        depth: int = 4,\n        features: int = 256,\n        activation=ACT_RELU,\n        repeats=1,\n        pooling_block=nn.MaxPool2d,\n    ):\n        super().__init__(\n            channels=[features] + [features] * stack_level,\n            strides=[4] + [4] * stack_level,\n            layers=list(range(0, stack_level + 1)),\n        )\n\n        self.stack_level = stack_level\n        self.depth_level = depth\n        self.num_features = features\n\n        act = get_activation_block(activation)\n        self.stem = HGStemBlock(input_channels, features, activation=act)\n\n        input_features = features\n        modules = []\n\n        for _ in range(stack_level):\n            modules.append(\n                HGBlock(\n                    depth,\n                    input_features,\n                    features,\n                    increase=0,\n                    activation=act,\n                    repeats=repeats,\n                    pooling_block=pooling_block,\n                )\n            )\n            input_features = features\n\n        self.num_blocks = len(modules)\n        self.blocks = nn.ModuleList(modules)\n        self.features = nn.ModuleList(\n            [HGFeaturesBlock(features, blocks=4, activation=act) for _ in range(stack_level)]\n        )\n        self.merge_features = nn.ModuleList(\n            [nn.Conv2d(features, features, kernel_size=1) for _ in range(stack_level - 1)]\n        )\n\n    def __repr__(self):\n        return f""hg_s{self.stack_level}_d{self.depth_level}_f{self.num_features}""\n\n    def forward(self, x: Tensor) -> List[Tensor]:  # skipcq: PYL-W0221\n        x = self.stem(x)\n        outputs = [x]\n\n        for i, hourglass in enumerate(self.blocks):\n            features = self.features[i](hourglass(x))\n            outputs.append(features)\n\n            if i < self.num_blocks - 1:\n                x = x + self.merge_features[i](features)\n\n        return outputs\n\n    def change_input_channels(self, input_channels: int, mode=""auto""):\n        self.stem.conv1 = make_n_channel_input(self.stem.conv1, input_channels, mode)\n        return self\n\n    @property\n    def encoder_layers(self) -> List[nn.Module]:\n        return [self.stem] + list(self.blocks)\n\n\nclass StackedSupervisedHGEncoder(StackedHGEncoder):\n    def __init__(\n        self,\n        supervision_channels: int,\n        input_channels: int = 3,\n        stack_level: int = 8,\n        depth: int = 4,\n        features: int = 256,\n        activation=ACT_RELU,\n        repeats=1,\n        pooling_block=nn.MaxPool2d,\n        supervision_block=HGSupervisionBlock,\n    ):\n        super().__init__(\n            input_channels=input_channels,\n            stack_level=stack_level,\n            depth=depth,\n            features=features,\n            activation=activation,\n            repeats=repeats,\n            pooling_block=pooling_block,\n        )\n\n        self.supervision_blocks = nn.ModuleList(\n            [supervision_block(features, supervision_channels) for _ in range(stack_level - 1)]\n        )\n\n    def forward(self, x: Tensor) -> Tuple[List[Tensor], List[Tensor]]:  # skipcq: PYL-W0221\n        x = self.stem(x)\n        outputs = [x]\n        supervision = []\n\n        for i, hourglass in enumerate(self.blocks):\n            features = self.features[i](hourglass(x))\n            outputs.append(features)\n\n            if i < self.num_blocks - 1:\n                sup_mask, sup_features = self.supervision_blocks[i](features)\n                supervision.append(sup_mask)\n                x = x + self.merge_features[i](features) + sup_features\n\n        return outputs, supervision\n'"
pytorch_toolbelt/modules/encoders/hrnet.py,1,"b'from collections import OrderedDict\nfrom typing import List\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom .common import EncoderModule, make_n_channel_input, _take\n\n__all__ = [""HRNetV2Encoder18"", ""HRNetV2Encoder34"", ""HRNetV2Encoder48""]\n\n\nHRNETV2_BN_MOMENTUM = 0.1\n\n\ndef hrnet_conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\nclass HRNetBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(HRNetBasicBlock, self).__init__()\n        self.conv1 = hrnet_conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=HRNETV2_BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = hrnet_conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=HRNETV2_BN_MOMENTUM)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass HRNetBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(HRNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=HRNETV2_BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=HRNETV2_BN_MOMENTUM)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion, momentum=HRNETV2_BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass HighResolutionModule(nn.Module):\n    def __init__(\n        self, num_branches, blocks, num_blocks, num_inchannels, num_channels, fuse_method, multi_scale_output=True\n    ):\n        super(HighResolutionModule, self).__init__()\n        self._check_branches(num_branches, blocks, num_blocks, num_inchannels, num_channels)\n\n        self.num_inchannels = num_inchannels\n        self.fuse_method = fuse_method\n        self.num_branches = num_branches\n\n        self.multi_scale_output = multi_scale_output\n\n        self.branches = self._make_branches(num_branches, blocks, num_blocks, num_channels)\n        self.fuse_layers = self._make_fuse_layers()\n        self.relu = nn.ReLU(inplace=True)\n\n    def _check_branches(self, num_branches, blocks, num_blocks, num_inchannels, num_channels):\n        if num_branches != len(num_blocks):\n            error_msg = ""NUM_BRANCHES({}) <> NUM_BLOCKS({})"".format(num_branches, len(num_blocks))\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_channels):\n            error_msg = ""NUM_BRANCHES({}) <> NUM_CHANNELS({})"".format(num_branches, len(num_channels))\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_inchannels):\n            error_msg = ""NUM_BRANCHES({}) <> NUM_INCHANNELS({})"".format(num_branches, len(num_inchannels))\n            raise ValueError(error_msg)\n\n    def _make_one_branch(self, branch_index, block, num_blocks, num_channels, stride=1):\n        downsample = None\n        if stride != 1 or self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.num_inchannels[branch_index],\n                    num_channels[branch_index] * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(num_channels[branch_index] * block.expansion, momentum=HRNETV2_BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index], stride, downsample))\n        self.num_inchannels[branch_index] = num_channels[branch_index] * block.expansion\n        for i in range(1, num_blocks[branch_index]):\n            layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index]))\n\n        return nn.Sequential(*layers)\n\n    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n        branches = []\n\n        for i in range(num_branches):\n            branches.append(self._make_one_branch(i, block, num_blocks, num_channels))\n\n        return nn.ModuleList(branches)\n\n    def _make_fuse_layers(self):\n        if self.num_branches == 1:\n            return None\n\n        num_branches = self.num_branches\n        num_inchannels = self.num_inchannels\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(\n                        nn.Sequential(\n                            nn.Conv2d(num_inchannels[j], num_inchannels[i], 1, 1, 0, bias=False),\n                            nn.BatchNorm2d(num_inchannels[i], momentum=HRNETV2_BN_MOMENTUM),\n                        )\n                    )\n                elif j == i:\n                    fuse_layer.append(None)\n                else:\n                    conv3x3s = []\n                    for k in range(i - j):\n                        if k == i - j - 1:\n                            num_outchannels_conv3x3 = num_inchannels[i]\n                            conv3x3s.append(\n                                nn.Sequential(\n                                    nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False),\n                                    nn.BatchNorm2d(num_outchannels_conv3x3, momentum=HRNETV2_BN_MOMENTUM),\n                                )\n                            )\n                        else:\n                            num_outchannels_conv3x3 = num_inchannels[j]\n                            conv3x3s.append(\n                                nn.Sequential(\n                                    nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False),\n                                    nn.BatchNorm2d(num_outchannels_conv3x3, momentum=HRNETV2_BN_MOMENTUM),\n                                    nn.ReLU(inplace=True),\n                                )\n                            )\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\n            fuse_layers.append(nn.ModuleList(fuse_layer))\n\n        return nn.ModuleList(fuse_layers)\n\n    def get_num_inchannels(self):\n        return self.num_inchannels\n\n    def forward(self, x):\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i in range(self.num_branches):\n            x[i] = self.branches[i](x[i])\n\n        x_fuse = []\n        for i in range(len(self.fuse_layers)):\n            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n            for j in range(1, self.num_branches):\n                if i == j:\n                    y = y + x[j]\n                elif j > i:\n                    width_output = x[i].shape[-1]\n                    height_output = x[i].shape[-2]\n                    y = y + F.interpolate(\n                        self.fuse_layers[i][j](x[j]),\n                        size=(height_output, width_output),\n                        mode=""bilinear"",\n                        align_corners=False,\n                    )\n                else:\n                    y = y + self.fuse_layers[i][j](x[j])\n            x_fuse.append(self.relu(y))\n\n        return x_fuse\n\n\nclass HRNetEncoderBase(EncoderModule):\n    def __init__(self, input_channels=3, width=48, layers: List[int] = None):\n        if layers is None:\n            layers = [1, 2, 3, 4]\n\n        channels = [64, width, width * 2, width * 4, width * 8]\n\n        strides = [4, 4, 8, 16, 32]\n\n        super().__init__(channels=channels, strides=strides, layers=layers)\n\n        blocks_dict = {""BASIC"": HRNetBasicBlock, ""BOTTLENECK"": HRNetBottleneck}\n\n        extra = {\n            ""STAGE2"": {\n                ""NUM_MODULES"": 1,\n                ""NUM_BRANCHES"": 2,\n                ""BLOCK"": ""BASIC"",\n                ""NUM_BLOCKS"": (4, 4),\n                ""NUM_CHANNELS"": (width, width * 2),\n                ""FUSE_METHOD"": ""SUM"",\n            },\n            ""STAGE3"": {\n                ""NUM_MODULES"": 4,\n                ""NUM_BRANCHES"": 3,\n                ""BLOCK"": ""BASIC"",\n                ""NUM_BLOCKS"": (4, 4, 4),\n                ""NUM_CHANNELS"": (width, width * 2, width * 4),\n                ""FUSE_METHOD"": ""SUM"",\n            },\n            ""STAGE4"": {\n                ""NUM_MODULES"": 3,\n                ""NUM_BRANCHES"": 4,\n                ""BLOCK"": ""BASIC"",\n                ""NUM_BLOCKS"": (4, 4, 4, 4),\n                ""NUM_CHANNELS"": (width, width * 2, width * 4, width * 8),\n                ""FUSE_METHOD"": ""SUM"",\n            },\n            ""FINAL_CONV_KERNEL"": 1,\n        }\n\n        # stem net\n        self.layer0 = nn.Sequential(\n            OrderedDict(\n                [\n                    (""conv1"", nn.Conv2d(input_channels, 64, kernel_size=3, stride=2, padding=1, bias=False)),\n                    (""bn1"", nn.BatchNorm2d(64, momentum=HRNETV2_BN_MOMENTUM)),\n                    (""relu"", nn.ReLU(inplace=True)),\n                    (""conv2"", nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False)),\n                    (""bn2"", nn.BatchNorm2d(64, momentum=HRNETV2_BN_MOMENTUM)),\n                    (""relu2"", nn.ReLU(inplace=True)),\n                ]\n            )\n        )\n\n        self.layer1 = self._make_layer(HRNetBottleneck, 64, 64, 4)\n\n        self.stage2_cfg = extra[""STAGE2""]\n        num_channels = self.stage2_cfg[""NUM_CHANNELS""]\n        block = blocks_dict[self.stage2_cfg[""BLOCK""]]\n        num_channels = [num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition1 = self._make_transition_layer([256], num_channels)\n        self.stage2, pre_stage_channels = self._make_stage(self.stage2_cfg, num_channels)\n\n        self.stage3_cfg = extra[""STAGE3""]\n        num_channels = self.stage3_cfg[""NUM_CHANNELS""]\n        block = blocks_dict[self.stage3_cfg[""BLOCK""]]\n        num_channels = [num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition2 = self._make_transition_layer(pre_stage_channels, num_channels)\n        self.stage3, pre_stage_channels = self._make_stage(self.stage3_cfg, num_channels)\n\n        self.stage4_cfg = extra[""STAGE4""]\n        num_channels = self.stage4_cfg[""NUM_CHANNELS""]\n        block = blocks_dict[self.stage4_cfg[""BLOCK""]]\n        num_channels = [num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition3 = self._make_transition_layer(pre_stage_channels, num_channels)\n        self.stage4, pre_stage_channels = self._make_stage(self.stage4_cfg, num_channels, multi_scale_output=True)\n\n    def _make_transition_layer(self, num_channels_pre_layer, num_channels_cur_layer):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(\n                        nn.Sequential(\n                            nn.Conv2d(num_channels_pre_layer[i], num_channels_cur_layer[i], 3, 1, 1, bias=False),\n                            nn.BatchNorm2d(num_channels_cur_layer[i], momentum=HRNETV2_BN_MOMENTUM),\n                            nn.ReLU(inplace=True),\n                        )\n                    )\n                else:\n                    transition_layers.append(None)\n            else:\n                conv3x3s = []\n                for j in range(i + 1 - num_branches_pre):\n                    inchannels = num_channels_pre_layer[-1]\n                    outchannels = num_channels_cur_layer[i] if j == i - num_branches_pre else inchannels\n                    conv3x3s.append(\n                        nn.Sequential(\n                            nn.Conv2d(inchannels, outchannels, 3, 2, 1, bias=False),\n                            nn.BatchNorm2d(outchannels, momentum=HRNETV2_BN_MOMENTUM),\n                            nn.ReLU(inplace=True),\n                        )\n                    )\n                transition_layers.append(nn.Sequential(*conv3x3s))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion, momentum=HRNETV2_BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(inplanes, planes, stride, downsample))\n        inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_stage(self, layer_config, num_inchannels, multi_scale_output=True):\n\n        blocks_dict = {""BASIC"": HRNetBasicBlock, ""BOTTLENECK"": HRNetBottleneck}\n\n        num_modules = layer_config[""NUM_MODULES""]\n        num_branches = layer_config[""NUM_BRANCHES""]\n        num_blocks = layer_config[""NUM_BLOCKS""]\n        num_channels = layer_config[""NUM_CHANNELS""]\n        block = blocks_dict[layer_config[""BLOCK""]]\n        fuse_method = layer_config[""FUSE_METHOD""]\n\n        modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            if not multi_scale_output and i == num_modules - 1:\n                reset_multi_scale_output = False\n            else:\n                reset_multi_scale_output = True\n            modules.append(\n                HighResolutionModule(\n                    num_branches,\n                    block,\n                    num_blocks,\n                    num_inchannels,\n                    num_channels,\n                    fuse_method,\n                    reset_multi_scale_output,\n                )\n            )\n            num_inchannels = modules[-1].get_num_inchannels()\n\n        return nn.Sequential(*modules), num_inchannels\n\n    def forward(self, x):\n        layer0 = self.layer0(x)\n        x = self.layer1(layer0)\n\n        x_list = []\n        for i in range(self.stage2_cfg[""NUM_BRANCHES""]):\n            if self.transition1[i] is not None:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(self.stage3_cfg[""NUM_BRANCHES""]):\n            if self.transition2[i] is not None:\n                x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(self.stage4_cfg[""NUM_BRANCHES""]):\n            if self.transition3[i] is not None:\n                x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage4(x_list)\n\n        outputs = _take([layer0] + y_list, self._layers)\n        return outputs\n\n    def change_input_channels(self, input_channels: int, mode=""auto""):\n        self.layer0.conv1 = make_n_channel_input(self.layer0.conv1, input_channels, mode)\n        return self\n\n\nclass HRNetV2Encoder18(HRNetEncoderBase):\n    def __init__(self, pretrained=None, layers=None):\n        super().__init__(width=18, layers=layers)\n\n\nclass HRNetV2Encoder34(HRNetEncoderBase):\n    def __init__(self, pretrained=None, layers=None):\n        super().__init__(width=34, layers=layers)\n\n\nclass HRNetV2Encoder48(HRNetEncoderBase):\n    def __init__(self, pretrained=None, layers=None):\n        super().__init__(width=48, layers=layers)\n'"
pytorch_toolbelt/modules/encoders/inception.py,0,"b'from .common import EncoderModule, _take, make_n_channel_input\nfrom ..backbone.inceptionv4 import inceptionv4\n\n__all__ = [""InceptionV4Encoder""]\n\n\nclass InceptionV4Encoder(EncoderModule):\n    def __init__(self, pretrained=True, layers=None, **kwargs):\n        backbone = inceptionv4(pretrained=""imagenet"" if pretrained else None)\n        channels = [64, 192, 384, 1024, 1536]\n        strides = [2, 4, 8, 16, 32]  # Note output strides are approximate\n        if layers is None:\n            layers = [1, 2, 3, 4]\n        features = backbone.features\n        super().__init__(channels, strides, layers)\n\n        self.layer0 = features[0:3]\n        self.layer1 = features[3:5]\n        self.layer2 = features[5:10]\n        self.layer3 = features[10:18]\n        self.layer4 = features[18:22]\n\n        self._output_strides = _take(strides, layers)\n        self._output_filters = _take(channels, layers)\n\n    def forward(self, x):\n        output_features = []\n        for layer in self.encoder_layers:\n            output = layer(x)\n            output_features.append(output)\n            x = output\n\n        # Return only features that were requested\n        return _take(output_features, self._layers)\n\n    @property\n    def encoder_layers(self):\n        return [self.layer0, self.layer1, self.layer2, self.layer3, self.layer4]\n\n    def change_input_channels(self, input_channels: int, mode=""auto""):\n        self.layer0[0].conv = make_n_channel_input(self.layer0[0].conv, input_channels, mode)\n        return self\n'"
pytorch_toolbelt/modules/encoders/mobilenet.py,0,"b'from .common import EncoderModule, _take, make_n_channel_input\nfrom ..backbone.mobilenet import MobileNetV2\nfrom ..backbone.mobilenetv3 import MobileNetV3\n\n__all__ = [""MobilenetV2Encoder"", ""MobilenetV3Encoder""]\n\n\nclass MobilenetV2Encoder(EncoderModule):\n    def __init__(self, layers=[2, 3, 5, 7], activation=""relu6""):\n        super().__init__([32, 16, 24, 32, 64, 96, 160, 320], [2, 2, 4, 8, 16, 16, 32, 32], layers)\n        encoder = MobileNetV2(activation=activation)\n\n        self.layer0 = encoder.layer0\n        self.layer1 = encoder.layer1\n        self.layer2 = encoder.layer2\n        self.layer3 = encoder.layer3\n        self.layer4 = encoder.layer4\n        self.layer5 = encoder.layer5\n        self.layer6 = encoder.layer6\n        self.layer7 = encoder.layer7\n\n    @property\n    def encoder_layers(self):\n        return [self.layer0, self.layer1, self.layer2, self.layer3, self.layer4, self.layer5, self.layer6, self.layer7]\n\n    def change_input_channels(self, input_channels: int, mode=""auto""):\n        self.layer0.conv1 = make_n_channel_input(self.layer0.conv1, input_channels, mode)\n        return self\n\n\nclass MobilenetV3Encoder(EncoderModule):\n    def __init__(self, input_channels=3, small=False, drop_prob=0.0, layers=[1, 2, 3, 4]):\n        super().__init__([24, 24, 40, 96, 96] if small else [16, 40, 80, 160, 160], [4, 8, 16, 32, 32], layers)\n        encoder = MobileNetV3(in_channels=input_channels, small=small, drop_prob=drop_prob)\n\n        self.conv1 = encoder.conv1\n        self.bn1 = encoder.bn1\n        self.act1 = encoder.act1\n\n        self.layer0 = encoder.layer0\n        self.layer1 = encoder.layer1\n        self.layer2 = encoder.layer2\n        self.layer3 = encoder.layer3\n        self.layer4 = encoder.layer4\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n\n        output_features = []\n\n        x = self.layer0(x)\n        output_features.append(x)\n\n        x = self.layer1(x)\n        output_features.append(x)\n\n        x = self.layer2(x)\n        output_features.append(x)\n\n        x = self.layer3(x)\n        output_features.append(x)\n\n        x = self.layer4(x)\n        output_features.append(x)\n\n        # Return only features that were requested\n        return _take(output_features, self._layers)\n\n    @property\n    def encoder_layers(self):\n        return [self.layer0, self.layer1, self.layer2, self.layer3, self.layer4]\n\n    def change_input_channels(self, input_channels: int, mode=""auto""):\n        self.layer0.conv1 = make_n_channel_input(self.layer0.conv1, input_channels, mode)\n        return self\n'"
pytorch_toolbelt/modules/encoders/resnet.py,0,"b'""""""Wrappers for different backbones for models that follows Encoder-Decoder architecture.\n\nEncodes listed here provides easy way to swap backbone of classification/segmentation/detection model.\n""""""\n\nfrom collections import OrderedDict\n\nfrom torch import nn\nfrom torchvision.models import resnet50, resnet34, resnet18, resnet101, resnet152\n\nfrom .common import EncoderModule, _take, make_n_channel_input\n\n__all__ = [\n    ""ResnetEncoder"",\n    ""Resnet18Encoder"",\n    ""Resnet34Encoder"",\n    ""Resnet50Encoder"",\n    ""Resnet101Encoder"",\n    ""Resnet152Encoder"",\n]\n\n\nclass ResnetEncoder(EncoderModule):\n    def __init__(self, resnet, filters, strides, layers=None):\n        if layers is None:\n            layers = [1, 2, 3, 4]\n        super().__init__(filters, strides, layers)\n\n        self.layer0 = nn.Sequential(OrderedDict([(""conv0"", resnet.conv1), (""bn0"", resnet.bn1), (""act0"", resnet.relu)]))\n        self.maxpool = resnet.maxpool\n\n        self.layer1 = resnet.layer1\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n    @property\n    def encoder_layers(self):\n        return [self.layer0, self.layer1, self.layer2, self.layer3, self.layer4]\n\n    def forward(self, x):\n        output_features = []\n        for layer in self.encoder_layers:\n            output = layer(x)\n            output_features.append(output)\n\n            if layer == self.layer0:\n                # Fist maxpool operator is not a part of layer0 because we want that layer0 output to have stride of 2\n                output = self.maxpool(output)\n            x = output\n\n        # Return only features that were requested\n        return _take(output_features, self._layers)\n\n    def change_input_channels(self, input_channels: int, mode=""auto""):\n        self.layer0.conv0 = make_n_channel_input(self.layer0.conv0, input_channels, mode)\n        return self\n\n\nclass Resnet18Encoder(ResnetEncoder):\n    def __init__(self, pretrained=True, layers=None):\n        super().__init__(resnet18(pretrained=pretrained), [64, 64, 128, 256, 512], [2, 4, 8, 16, 32], layers)\n\n\nclass Resnet34Encoder(ResnetEncoder):\n    def __init__(self, pretrained=True, layers=None):\n        super().__init__(resnet34(pretrained=pretrained), [64, 64, 128, 256, 512], [2, 4, 8, 16, 32], layers)\n\n\nclass Resnet50Encoder(ResnetEncoder):\n    def __init__(self, pretrained=True, layers=None):\n        super().__init__(resnet50(pretrained=pretrained), [64, 256, 512, 1024, 2048], [2, 4, 8, 16, 32], layers)\n\n\nclass Resnet101Encoder(ResnetEncoder):\n    def __init__(self, pretrained=True, layers=None):\n        super().__init__(resnet101(pretrained=pretrained), [64, 256, 512, 1024, 2048], [2, 4, 8, 16, 32], layers)\n\n\nclass Resnet152Encoder(ResnetEncoder):\n    def __init__(self, pretrained=True, layers=None):\n        super().__init__(resnet152(pretrained=pretrained), [64, 256, 512, 1024, 2048], [2, 4, 8, 16, 32], layers)\n'"
pytorch_toolbelt/modules/encoders/seresnet.py,0,"b'""""""Wrappers for different backbones for models that follows Encoder-Decoder architecture.\n\nEncodes listed here provides easy way to swap backbone of classification/segmentation/detection model.\n""""""\nfrom typing import List\n\nfrom torch import Tensor\n\nfrom .common import EncoderModule, _take, make_n_channel_input\nfrom ..backbone.senet import (\n    SENet,\n    se_resnext50_32x4d,\n    se_resnext101_32x4d,\n    se_resnet50,\n    se_resnet101,\n    se_resnet152,\n    senet154,\n)\n\n__all__ = [\n    ""SEResnetEncoder"",\n    ""SEResnet50Encoder"",\n    ""SEResnet101Encoder"",\n    ""SEResnet152Encoder"",\n    ""SEResNeXt50Encoder"",\n    ""SEResNeXt101Encoder"",\n    ""SENet154Encoder"",\n]\n\n\nclass SEResnetEncoder(EncoderModule):\n    """"""\n    The only difference from vanilla ResNet is that it has \'layer0\' module\n    """"""\n\n    def __init__(self, seresnet: SENet, channels, strides, layers=None):\n        if layers is None:\n            layers = [1, 2, 3, 4]\n        super().__init__(channels, strides, layers)\n\n        self.maxpool = seresnet.layer0.pool\n        del seresnet.layer0.pool\n\n        self.layer0 = seresnet.layer0\n        self.layer1 = seresnet.layer1\n        self.layer2 = seresnet.layer2\n        self.layer3 = seresnet.layer3\n        self.layer4 = seresnet.layer4\n\n        self._output_strides = _take(strides, layers)\n        self._output_filters = _take(channels, layers)\n\n    @property\n    def encoder_layers(self):\n        return [self.layer0, self.layer1, self.layer2, self.layer3, self.layer4]\n\n    @property\n    def output_strides(self):\n        return self._output_strides\n\n    @property\n    def output_filters(self):\n        return self._output_filters\n\n    def forward(self, x: Tensor) -> List[Tensor]:\n        output_features = []\n        for layer in self.encoder_layers:\n            output = layer(x)\n            output_features.append(output)\n\n            if layer == self.layer0:\n                # Fist maxpool operator is not a part of layer0\n                # because we want that layer0 output to have stride of 2\n                output = self.maxpool(output)\n            x = output\n\n        # Return only features that were requested\n        return _take(output_features, self._layers)\n\n    def change_input_channels(self, input_channels: int, mode=""auto""):\n        self.layer0.conv1 = make_n_channel_input(self.layer0.conv1, input_channels, mode)\n        return self\n\n\nclass SEResnet50Encoder(SEResnetEncoder):\n    def __init__(self, pretrained=True, layers=None):\n        encoder = se_resnet50(pretrained=""imagenet"" if pretrained else None)\n        super().__init__(encoder, [64, 256, 512, 1024, 2048], [2, 4, 8, 16, 32], layers)\n\n\nclass SEResnet101Encoder(SEResnetEncoder):\n    def __init__(self, pretrained=True, layers=None):\n        encoder = se_resnet101(pretrained=""imagenet"" if pretrained else None)\n        super().__init__(encoder, [64, 256, 512, 1024, 2048], [2, 4, 8, 16, 32], layers)\n\n\nclass SEResnet152Encoder(SEResnetEncoder):\n    def __init__(self, pretrained=True, layers=None):\n        encoder = se_resnet152(pretrained=""imagenet"" if pretrained else None)\n        super().__init__(encoder, [64, 256, 512, 1024, 2048], [2, 4, 8, 16, 32], layers)\n\n\nclass SENet154Encoder(SEResnetEncoder):\n    def __init__(self, pretrained=True, layers=None):\n        encoder = senet154(pretrained=""imagenet"" if pretrained else None)\n        super().__init__(encoder, [64, 256, 512, 1024, 2048], [2, 4, 8, 16, 32], layers)\n\n\nclass SEResNeXt50Encoder(SEResnetEncoder):\n    def __init__(self, pretrained=True, layers=None):\n        encoder = se_resnext50_32x4d(pretrained=""imagenet"" if pretrained else None)\n        super().__init__(encoder, [64, 256, 512, 1024, 2048], [2, 4, 8, 16, 32], layers)\n\n\nclass SEResNeXt101Encoder(SEResnetEncoder):\n    def __init__(self, pretrained=True, layers=None):\n        encoder = se_resnext101_32x4d(pretrained=""imagenet"" if pretrained else None)\n        super().__init__(encoder, [64, 256, 512, 1024, 2048], [2, 4, 8, 16, 32], layers)\n'"
pytorch_toolbelt/modules/encoders/squeezenet.py,0,"b'from collections import OrderedDict\n\nfrom torch import nn\nfrom torchvision.models import squeezenet1_1\n\nfrom .common import EncoderModule, make_n_channel_input\n\n__all__ = [""SqueezenetEncoder""]\n\n\nclass SqueezenetEncoder(EncoderModule):\n    def __init__(self, pretrained=True, layers=[1, 2, 3]):\n        super().__init__([64, 128, 256, 512], [4, 8, 16, 16], layers)\n        squeezenet = squeezenet1_1(pretrained=pretrained)\n\n        # nn.Conv2d(3, 64, kernel_size=3, stride=2),\n        # nn.ReLU(inplace=True),\n        # nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n        self.layer0 = nn.Sequential(\n            OrderedDict(\n                [\n                    (""conv1"", squeezenet.features[0]),\n                    (""relu1"", nn.ReLU(inplace=True)),\n                    (""pool1"", nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n                ]\n            )\n        )\n\n        # Fire(64, 16, 64, 64),\n        # Fire(128, 16, 64, 64),\n        # nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n        self.layer1 = nn.Sequential(\n            squeezenet.features[3],\n            squeezenet.features[4],\n            # squeezenet.features[5],\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n        )\n\n        # Fire(128, 32, 128, 128),\n        # Fire(256, 32, 128, 128),\n        # nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n        self.layer2 = nn.Sequential(\n            squeezenet.features[6],\n            squeezenet.features[7],\n            # squeezenet.features[8],\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n        )\n\n        # Fire(256, 48, 192, 192),\n        # Fire(384, 48, 192, 192),\n        # Fire(384, 64, 256, 256),\n        # Fire(512, 64, 256, 256),\n        self.layer3 = nn.Sequential(\n            squeezenet.features[9], squeezenet.features[10], squeezenet.features[11], squeezenet.features[12]\n        )\n\n    @property\n    def encoder_layers(self):\n        return [self.layer0, self.layer1, self.layer2, self.layer3]\n\n    def change_input_channels(self, input_channels: int, mode=""auto""):\n        self.layer0.conv1 = make_n_channel_input(self.layer0.conv1, input_channels, mode)\n        return self\n'"
pytorch_toolbelt/modules/encoders/unet.py,0,"b'from collections import OrderedDict\nfrom functools import partial\nfrom typing import Union\n\nfrom torch import nn\n\nfrom .common import EncoderModule, make_n_channel_input\nfrom ..unet import UnetBlock\n\n__all__ = [""UnetEncoder""]\n\n\nclass UnetEncoder(EncoderModule):\n    """"""\n    Vanilla U-Net encoder\n    """"""\n\n    def __init__(\n        self,\n        in_channels=3,\n        out_channels=32,\n        num_layers=4,\n        growth_factor=2,\n        pool_block: Union[nn.MaxPool2d, nn.AvgPool2d] = None,\n        unet_block: Union[nn.Module, UnetBlock] = UnetBlock,\n    ):\n        if pool_block is None:\n            pool_block = partial(nn.MaxPool2d, kernel_size=2, stride=2)\n\n        feature_maps = [out_channels * (growth_factor ** i) for i in range(num_layers)]\n        strides = [2 ** i for i in range(num_layers)]\n        super().__init__(feature_maps, strides, layers=list(range(num_layers)))\n\n        input_filters = in_channels\n        self.num_layers = num_layers\n        for layer in range(num_layers):\n            block = unet_block(input_filters, feature_maps[layer])\n\n            if layer > 0:\n                pool = pool_block()\n                block = nn.Sequential(OrderedDict([(""pool"", pool), (""conv"", block)]))\n\n            input_filters = feature_maps[layer]\n            self.add_module(f""layer{layer}"", block)\n\n    @property\n    def encoder_layers(self):\n        return [self.__getattr__(f""layer{layer}"") for layer in range(self.num_layers)]\n\n    def change_input_channels(self, input_channels: int, mode=""auto""):\n        self.layer0.conv1 = make_n_channel_input(self.layer0.conv1, input_channels, mode)\n        return self\n'"
pytorch_toolbelt/modules/encoders/wide_resnet.py,0,"b'from typing import List\n\nfrom .common import EncoderModule, _take, make_n_channel_input\nfrom ..activations import ABN\nfrom ..backbone.wider_resnet import WiderResNet, WiderResNetA2\n\n__all__ = [\n    ""WiderResnetEncoder"",\n    ""WiderResnet16A2Encoder"",\n    ""WiderResnet16Encoder"",\n    ""WiderResnet20Encoder"",\n    ""WiderResnet38A2Encoder"",\n    ""WiderResnet38Encoder"",\n    ""WiderResnetA2Encoder"",\n    ""WiderResnet20A2Encoder"",\n]\n\n\nclass WiderResnetEncoder(EncoderModule):\n    def __init__(self, structure: List[int], layers: List[int], norm_act=ABN):\n        super().__init__([64, 128, 256, 512, 1024, 2048, 4096], [1, 2, 4, 8, 16, 32, 32], layers)\n\n        encoder: WiderResNet = WiderResNet(structure, classes=0, norm_act=norm_act)\n        self.layer0 = encoder.mod1\n        self.layer1 = encoder.mod2\n        self.layer2 = encoder.mod3\n        self.layer3 = encoder.mod4\n        self.layer4 = encoder.mod5\n        self.layer5 = encoder.mod6\n        self.layer6 = encoder.mod7\n\n        self.pool2 = encoder.pool2\n        self.pool3 = encoder.pool3\n        self.pool4 = encoder.pool4\n        self.pool5 = encoder.pool5\n        self.pool6 = encoder.pool6\n\n    @property\n    def encoder_layers(self):\n        return [self.layer0, self.layer1, self.layer2, self.layer3, self.layer4, self.layer5, self.layer6]\n\n    def forward(self, input):\n        output_features = []\n\n        x = self.layer0(input)\n        output_features.append(x)\n\n        x = self.layer1(self.pool2(x))\n        output_features.append(x)\n\n        x = self.layer2(self.pool3(x))\n        output_features.append(x)\n\n        x = self.layer3(self.pool4(x))\n        output_features.append(x)\n\n        x = self.layer4(self.pool5(x))\n        output_features.append(x)\n\n        x = self.layer5(self.pool6(x))\n        output_features.append(x)\n\n        x = self.layer6(x)\n        output_features.append(x)\n\n        # Return only features that were requested\n        return _take(output_features, self._layers)\n\n    def change_input_channels(self, input_channels: int, mode=""auto""):\n        self.layer0.conv1 = make_n_channel_input(self.layer0.conv1, input_channels, mode)\n        return self\n\n\nclass WiderResnet16Encoder(WiderResnetEncoder):\n    def __init__(self, layers=None):\n        if layers is None:\n            layers = [2, 3, 4, 5, 6]\n        super().__init__(structure=[1, 1, 1, 1, 1, 1], layers=layers)\n\n\nclass WiderResnet20Encoder(WiderResnetEncoder):\n    def __init__(self, layers=None):\n        if layers is None:\n            layers = [2, 3, 4, 5, 6]\n        super().__init__(structure=[1, 1, 1, 3, 1, 1], layers=layers)\n\n\nclass WiderResnet38Encoder(WiderResnetEncoder):\n    def __init__(self, layers=None):\n        if layers is None:\n            layers = [2, 3, 4, 5, 6]\n        super().__init__(structure=[3, 3, 6, 3, 1, 1], layers=layers)\n\n\nclass WiderResnetA2Encoder(EncoderModule):\n    def __init__(self, structure: List[int], layers: List[int], norm_act=ABN):\n        super().__init__([64, 128, 256, 512, 1024, 2048, 4096], [1, 2, 4, 8, 16, 32, 32], layers)\n\n        encoder = WiderResNetA2(structure=structure, classes=0, norm_act=norm_act)\n        self.layer0 = encoder.mod1\n        self.layer1 = encoder.mod2\n        self.layer2 = encoder.mod3\n        self.layer3 = encoder.mod4\n        self.layer4 = encoder.mod5\n        self.layer5 = encoder.mod6\n        self.layer6 = encoder.mod7\n\n        self.pool2 = encoder.pool2\n        self.pool3 = encoder.pool3\n\n    @property\n    def encoder_layers(self):\n        return [self.layer0, self.layer1, self.layer2, self.layer3, self.layer4, self.layer5, self.layer6]\n\n    def forward(self, input):\n        output_features = []\n\n        out = self.layer0(input)\n        output_features.append(out)\n\n        out = self.layer1(self.pool2(out))\n        output_features.append(out)\n\n        out = self.layer2(self.pool3(out))\n        output_features.append(out)\n\n        out = self.layer3(out)\n        output_features.append(out)\n\n        out = self.layer4(out)\n        output_features.append(out)\n\n        out = self.layer5(out)\n        output_features.append(out)\n\n        out = self.layer6(out)\n        output_features.append(out)\n\n        # Return only features that were requested\n        return _take(output_features, self._layers)\n\n    def change_input_channels(self, input_channels: int, mode=""auto""):\n        self.layer0.conv1 = make_n_channel_input(self.layer0.conv1, input_channels, mode)\n        return self\n\n\nclass WiderResnet16A2Encoder(WiderResnetA2Encoder):\n    def __init__(self, layers=None):\n        if layers is None:\n            layers = [2, 3, 4, 5, 6]\n        super().__init__(structure=[1, 1, 1, 1, 1, 1], layers=layers)\n\n\nclass WiderResnet20A2Encoder(WiderResnetA2Encoder):\n    def __init__(self, layers=None):\n        if layers is None:\n            layers = [2, 3, 4, 5, 6]\n        super().__init__(structure=[1, 1, 1, 3, 1, 1], layers=layers)\n\n\nclass WiderResnet38A2Encoder(WiderResnetA2Encoder):\n    def __init__(self, layers=None):\n        if layers is None:\n            layers = [2, 3, 4, 5, 6]\n        super().__init__(structure=[3, 3, 6, 3, 1, 1], layers=layers)\n'"
pytorch_toolbelt/modules/encoders/xresnet.py,1,"b'from collections import OrderedDict\nfrom typing import List, Union\n\nimport torch\nfrom torch import nn\n\nfrom pytorch_toolbelt.modules import ACT_RELU, instantiate_activation_block, ChannelSpatialGate2d\nfrom pytorch_toolbelt.modules.encoders import EncoderModule, make_n_channel_input\n\n__all__ = [\n    ""XResNet18Encoder"",\n    ""XResNet34Encoder"",\n    ""XResNet50Encoder"",\n    ""XResNet101Encoder"",\n    ""XResNet152Encoder"",\n    ""SEXResNet18Encoder"",\n    ""SEXResNet34Encoder"",\n    ""SEXResNet50Encoder"",\n    ""SEXResNet101Encoder"",\n    ""SEXResNet152Encoder"",\n]\n\n\ndef make_conv_bn_act(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int = 3,\n    stride: int = 1,\n    zero_batch_norm: bool = False,\n    use_activation: bool = True,\n    activation: str = ACT_RELU,\n) -> torch.nn.Sequential:\n    """"""\n    Create a nn.Conv2d block followed by nn.BatchNorm2d and (optional) activation block.\n    """"""\n    batch_norm = nn.BatchNorm2d(out_channels)\n    # initializer batch normalization to 0 if its the final conv layer\n    nn.init.constant_(batch_norm.weight, 0.0 if zero_batch_norm else 1.0)\n    layers = [\n        (\n            ""conv"",\n            nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=kernel_size // 2, bias=False),\n        ),\n        (""bn"", batch_norm),\n    ]\n    if use_activation:\n        activation_block = instantiate_activation_block(activation, inplace=True)\n        layers.append((activation, activation_block))\n\n    return nn.Sequential(OrderedDict(layers))\n\n\nclass StemBlock(nn.Module):\n    def __init__(self, input_channels: int, output_channels: int, activation: str = ACT_RELU):\n        super().__init__()\n        self.conv_bn_relu_1 = make_conv_bn_act(input_channels, 8, stride=2, activation=activation)\n        self.conv_bn_relu_2 = make_conv_bn_act(8, 64, activation=activation)\n        self.conv_bn_relu_3 = make_conv_bn_act(64, output_channels, activation=activation)\n\n    def forward(self, x):\n        x = self.conv_bn_relu_1(x)\n        x = self.conv_bn_relu_2(x)\n        x = self.conv_bn_relu_3(x)\n        return x\n\n\nclass XResNetBlock(nn.Module):\n    """"""Creates the standard `XResNet` block.""""""\n\n    def __init__(self, expansion: int, n_inputs: int, n_hidden: int, stride: int = 1, activation: str = ACT_RELU):\n        super().__init__()\n\n        n_inputs = n_inputs * expansion\n        n_filters = n_hidden * expansion\n\n        # convolution path\n        if expansion == 1:\n            layers = [\n                make_conv_bn_act(n_inputs, n_hidden, 3, stride=stride, activation=activation),\n                make_conv_bn_act(n_hidden, n_filters, 3, zero_batch_norm=True, use_activation=False),\n            ]\n        else:\n            layers = [\n                make_conv_bn_act(n_inputs, n_hidden, 1, activation=activation),\n                make_conv_bn_act(n_hidden, n_hidden, 3, stride=stride, activation=activation),\n                make_conv_bn_act(n_hidden, n_filters, 1, zero_batch_norm=True, use_activation=False),\n            ]\n\n        self.convs = nn.Sequential(*layers)\n        self.activation = instantiate_activation_block(activation, inplace=True)\n\n        # identity path\n        if n_inputs == n_filters:\n            self.id_conv = nn.Identity()\n        else:\n            self.id_conv = make_conv_bn_act(n_inputs, n_filters, kernel_size=1, use_activation=False)\n        if stride == 1:\n            self.pool = nn.Identity()\n        else:\n            self.pool = nn.AvgPool2d(2, ceil_mode=True)\n\n    def forward(self, x):\n        return self.activation(self.convs(x) + self.id_conv(self.pool(x)))\n\n\nclass SEXResNetBlock(nn.Module):\n    """"""Creates the Squeeze&Excitation + XResNet block.""""""\n\n    def __init__(self, expansion: int, n_inputs: int, n_hidden: int, stride: int = 1, activation: str = ACT_RELU):\n        super().__init__()\n\n        n_inputs = n_inputs * expansion\n        n_filters = n_hidden * expansion\n\n        # convolution path\n        if expansion == 1:\n            layers = [\n                make_conv_bn_act(n_inputs, n_hidden, 3, stride=stride, activation=activation),\n                make_conv_bn_act(n_hidden, n_filters, 3, zero_batch_norm=True, use_activation=False),\n            ]\n        else:\n            layers = [\n                make_conv_bn_act(n_inputs, n_hidden, 1, activation=activation),\n                make_conv_bn_act(n_hidden, n_hidden, 3, stride=stride, activation=activation),\n                make_conv_bn_act(n_hidden, n_filters, 1, zero_batch_norm=True, use_activation=False),\n            ]\n\n        self.convs = nn.Sequential(*layers)\n        self.activation = instantiate_activation_block(activation, inplace=True)\n        self.se = ChannelSpatialGate2d(n_filters, reduction=4)\n\n        # identity path\n        if n_inputs == n_filters:\n            self.id_conv = nn.Identity()\n        else:\n            self.id_conv = make_conv_bn_act(n_inputs, n_filters, kernel_size=1, use_activation=False)\n        if stride == 1:\n            self.pool = nn.Identity()\n        else:\n            self.pool = nn.AvgPool2d(2, ceil_mode=True)\n\n    def forward(self, x):\n        return self.activation(self.convs(x) + self.id_conv(self.pool(x)))\n\n\nclass XResNet(EncoderModule):\n    def __init__(\n        self,\n        expansion: int,\n        blocks: List[int],\n        input_channels: int = 3,\n        activation=ACT_RELU,\n        layers=None,\n        first_pool: Union[nn.MaxPool2d, nn.AvgPool2d] = nn.MaxPool2d,\n        pretrained=None,\n        block: Union[XResNetBlock, SEXResNetBlock] = XResNetBlock,\n    ):\n        assert len(blocks) == 4\n        if layers is None:\n            layers = [1, 2, 3, 4]\n\n        n_filters = [64 // expansion, 64, 128, 256, 512]\n        channels = [64, 64 * expansion, 128 * expansion, 256 * expansion, 512 * expansion]\n\n        super().__init__(channels, [2, 4, 8, 16, 32], layers)\n\n        res_layers = [\n            self._make_layer(\n                block,\n                expansion,\n                n_filters[i],\n                n_filters[i + 1],\n                n_blocks=l,\n                stride=1 if i == 0 else 2,\n                activation=activation,\n            )\n            for i, l in enumerate(blocks)\n        ]\n\n        self.stem = StemBlock(input_channels, 64, activation=activation)\n        self.layer1 = nn.Sequential(\n            OrderedDict([(""pool"", first_pool(kernel_size=3, stride=2, padding=1)), (""block"", res_layers[0])])\n        )\n\n        self.layer2 = res_layers[1]\n        self.layer3 = res_layers[2]\n        self.layer4 = res_layers[3]\n\n    @property\n    def encoder_layers(self) -> List[nn.Module]:\n        return [self.stem, self.layer1, self.layer2, self.layer3, self.layer4]\n\n    @staticmethod\n    def _make_layer(block, expansion, n_inputs: int, n_filters: int, n_blocks: int, stride: int, activation: str):\n        return nn.Sequential(\n            *[\n                block(\n                    expansion,\n                    n_inputs if i == 0 else n_filters,\n                    n_filters,\n                    stride if i == 0 else 1,\n                    activation=activation,\n                )\n                for i in range(n_blocks)\n            ]\n        )\n\n    def change_input_channels(self, input_channels: int, mode=""auto""):\n        self.stem.conv_bn_relu_1.conv = make_n_channel_input(self.stem.conv_bn_relu_1.conv, input_channels, mode)\n\n\ndef XResNet18Encoder(**kwargs):\n    return XResNet(1, [2, 2, 2, 2], **kwargs)\n\n\ndef XResNet34Encoder(**kwargs):\n    return XResNet(1, [3, 4, 6, 3], **kwargs)\n\n\ndef XResNet50Encoder(**kwargs):\n    return XResNet(4, [3, 4, 6, 3], **kwargs)\n\n\ndef XResNet101Encoder(**kwargs):\n    return XResNet(4, [3, 4, 23, 3], **kwargs)\n\n\ndef XResNet152Encoder(**kwargs):\n    return XResNet(4, [3, 8, 36, 3], **kwargs)\n\n\n# SE-XResNet\n\n\ndef SEXResNet18Encoder(**kwargs):\n    return XResNet(1, [2, 2, 2, 2], block=SEXResNetBlock, **kwargs)\n\n\ndef SEXResNet34Encoder(**kwargs):\n    return XResNet(1, [3, 4, 6, 3], block=SEXResNetBlock, **kwargs)\n\n\ndef SEXResNet50Encoder(**kwargs):\n    return XResNet(4, [3, 4, 6, 3], block=SEXResNetBlock, **kwargs)\n\n\ndef SEXResNet101Encoder(**kwargs):\n    return XResNet(4, [3, 4, 23, 3], block=SEXResNetBlock, **kwargs)\n\n\ndef SEXResNet152Encoder(**kwargs):\n    return XResNet(4, [3, 8, 36, 3], block=SEXResNetBlock, **kwargs)\n'"
pytorch_toolbelt/utils/catalyst/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .criterions import *\nfrom .metrics import *\nfrom .opl import *\nfrom .visualization import *\nfrom .utils import *\nfrom .loss_adapter import *\n'
pytorch_toolbelt/utils/catalyst/criterions.py,4,"b'import math\n\nimport torch\nfrom catalyst.dl import CriterionCallback, RunnerState\nfrom catalyst.dl.callbacks.criterion import _add_loss_to_state\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = [""LPRegularizationCallback"", ""TSACriterionCallback"", ""get_multiplier""]\n\n\ndef get_multiplier(training_progress, schedule, start, end):\n    if schedule is None or schedule == ""none"":\n        threshold = 0\n    elif schedule == ""linear_schedule"":\n        threshold = training_progress\n    elif schedule == ""exp_schedule"":\n        scale = 5\n        threshold = math.exp((training_progress - 1) * scale)\n        # [exp(-5), exp(0)] = [1e-2, 1]\n    elif schedule == ""log_schedule"":\n        scale = 5\n        # [1 - exp(0), 1 - exp(-5)] = [0, 0.99]\n        threshold = 1 - math.exp((-training_progress) * scale)\n    else:\n        raise KeyError(schedule)\n\n    return threshold * (end - start) + start\n\n\nclass LPRegularizationCallback(CriterionCallback):\n    """"""\n    Generalized L1/L2 weight decay callback that may exponentially grow\n    """"""\n\n    def __init__(\n        self,\n        on_train_only=True,\n        apply_to_bias=False,\n        prefix: str = None,\n        p=1,\n        start_wd=0,\n        end_wd=1e-4,\n        schedule=""exp_schedule"",\n    ):\n        """"""\n\n        :param on_train_only:\n        :param apply_to_bias:\n        :param prefix:\n        :param p:\n        :param start_wd:\n        :param end_wd:\n        :param schedule:\n        """"""\n        if prefix is None:\n            prefix = f""l{self.p}_loss""\n\n        super().__init__(prefix=prefix, multiplier=start_wd)\n\n        self.on_train_only = on_train_only\n        self.is_needed = True\n        self.apply_to_bias = apply_to_bias\n        self.schedule = schedule\n        self.start_wd = start_wd\n        self.end_wd = end_wd\n        self.p = p\n        self.multiplier = None\n\n    def on_loader_start(self, state: RunnerState):\n        self.is_needed = not self.on_train_only or state.loader_name.startswith(""train"")\n        if self.is_needed:\n            state.metrics.epoch_values[state.loader_name][f""l{self.p}_weight_decay""] = self.multiplier\n\n    def on_epoch_start(self, state: RunnerState):\n        training_progress = float(state.epoch) / float(state.num_epochs)\n        self.multiplier = get_multiplier(training_progress, self.schedule, self.start_wd, self.end_wd)\n\n    def on_batch_end(self, state: RunnerState):\n        if not self.is_needed:\n            return\n\n        lp_reg = 0\n\n        for module in state.model.children():\n            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm1d, nn.BatchNorm3d)):\n                continue\n\n            for param_name, param in module.named_parameters():\n                if param_name.endswith(""bias"") and not self.apply_to_bias:\n                    continue\n\n                if param.requires_grad:\n                    lp_reg = param.norm(self.p) * self.multiplier + lp_reg\n\n        state.metrics.add_batch_value(metrics_dict={self.prefix: lp_reg.item()})\n        _add_loss_to_state(self.prefix, state, lp_reg)\n\n\nclass TSACriterionCallback(CriterionCallback):\n    """"""\n    Criterion callback with training signal annealing support.\n\n    This callback requires that criterion key returns loss per each element in batch\n\n    Reference:\n        Unsupervised Data Augmentation for Consistency Training\n        https://arxiv.org/abs/1904.12848\n    """"""\n\n    def __init__(\n        self,\n        num_classes,\n        num_epochs,\n        input_key: str = ""targets"",\n        output_key: str = ""logits"",\n        prefix: str = ""loss"",\n        criterion_key: str = None,\n        multiplier: float = 1.0,\n        unsupervised_label=-100,\n    ):\n        super().__init__(\n            input_key=input_key,\n            output_key=output_key,\n            prefix=prefix,\n            criterion_key=criterion_key,\n            multiplier=multiplier,\n        )\n        self.num_epochs = num_epochs\n        self.num_classes = num_classes\n        self.tsa_threshold = None\n        self.unsupervised_label = unsupervised_label\n\n    def get_tsa_threshold(self, current_epoch, schedule, start, end) -> float:\n        training_progress = float(current_epoch) / float(self.num_epochs)\n        threshold = None\n        if schedule == ""linear_schedule"":\n            threshold = training_progress\n        elif schedule == ""exp_schedule"":\n            scale = 5\n            threshold = math.exp((training_progress - 1) * scale)\n            # [exp(-5), exp(0)] = [1e-2, 1]\n        elif schedule == ""log_schedule"":\n            scale = 5\n            # [1 - exp(0), 1 - exp(-5)] = [0, 0.99]\n            threshold = 1 - math.exp((-training_progress) * scale)\n        else:\n            raise KeyError(f""Unsupported schedule name {schedule}"")\n        return threshold * (end - start) + start\n\n    def on_epoch_start(self, state: RunnerState):\n        if state.loader_name == ""train"":\n            self.tsa_threshold = self.get_tsa_threshold(state.epoch, ""exp_schedule"", 1.0 / self.num_classes, 1.0)\n            state.metrics.epoch_values[""train""][""tsa_threshold""] = self.tsa_threshold\n\n    def _compute_loss(self, state: RunnerState, criterion):\n\n        logits = state.output[self.output_key]\n        targets = state.input[self.input_key]\n        supervised_mask = targets != self.unsupervised_label  # Mask indicating labeled samples\n\n        targets = targets[supervised_mask]\n        logits = logits[supervised_mask]\n\n        if not len(targets):\n            return torch.tensor(0, dtype=logits.dtype, device=logits.device)\n\n        with torch.no_grad():\n            one_hot_targets = F.one_hot(targets, num_classes=self.num_classes).to(logits.dtype)\n            sup_probs = logits.detach().softmax(dim=1)\n            correct_label_probs = torch.sum(one_hot_targets * sup_probs, dim=1)\n            larger_than_threshold = correct_label_probs > self.tsa_threshold\n            loss_mask = 1.0 - larger_than_threshold.to(logits.dtype)\n\n        loss = criterion(logits, targets)\n        loss = loss * loss_mask\n\n        loss = loss.sum() / loss_mask.sum().clamp_min(1)\n        return loss\n'"
pytorch_toolbelt/utils/catalyst/loss_adapter.py,2,"b'import torch\nfrom catalyst.dl import (\n    CriterionCallback,\n    RunnerState,\n    OptimizerCallback,\n    CheckpointCallback,\n    SchedulerCallback,\n    SupervisedExperiment,\n    Callback,\n)\nfrom catalyst.dl.callbacks import VerboseLogger, ConsoleLogger, TensorboardLogger, RaiseExceptionCallback\nfrom torch import nn, Tensor\nfrom typing import Dict\n\n__all__ = [\n    ""TrainOnlyCriterionCallback"",\n    ""PassthroughCriterionCallback"",\n    ""ParallelLossSupervisedExperiment"",\n    ""LossModule"",\n    ""LossWrapper"",\n]\n\n\nclass TrainOnlyCriterionCallback(CriterionCallback):\n    """"""\n    Computes loss only on training stage\n    """"""\n\n    def _compute_loss_value(self, state: RunnerState, criterion):\n        predictions = self._get_output(state.output, self.output_key)\n        targets = self._get_input(state.input, self.input_key)\n\n        if state.loader_name != ""train"":\n            return torch.tensor(0, device=predictions.device, dtype=predictions.dtype)\n\n        loss = criterion(predictions, targets)\n        return loss\n\n    def _compute_loss_key_value(self, state: RunnerState, criterion):\n        output = self._get_output(state.output, self.output_key)\n        input = self._get_input(state.input, self.input_key)\n\n        if state.loader_name != ""train"":\n            return torch.tensor(0, device=output.device, dtype=output.dtype)\n\n        loss = criterion(**output, **input)\n        return loss\n\n\nclass ParallelLossSupervisedExperiment(SupervisedExperiment):\n    """"""\n    Custom experiment class. To use in conjunction with LossWrapper.\n    """"""\n\n    def get_callbacks(self, stage: str) -> ""OrderedDict[str, Callback]"":\n        """"""\n        Override of ``BaseExperiment.get_callbacks`` method.\n        Will add several of callbacks by default in case they missed.\n\n        Args:\n            stage (str): name of stage. It should start with `infer` if you\n                don\'t need default callbacks, as they required only for\n                training stages.\n        Returns:\n            List[Callback]: list of callbacks for experiment\n        """"""\n        callbacks = self._callbacks\n        default_callbacks = []\n        if self._verbose:\n            default_callbacks.append((""verbose"", VerboseLogger))\n        if not stage.startswith(""infer""):\n            # default_callbacks.append((""_criterion"", CriterionCallback)) # Commented\n            default_callbacks.append((""_optimizer"", OptimizerCallback))\n            if self._scheduler is not None:\n                default_callbacks.append((""_scheduler"", SchedulerCallback))\n            default_callbacks.append((""_saver"", CheckpointCallback))\n            default_callbacks.append((""console"", ConsoleLogger))\n            default_callbacks.append((""tensorboard"", TensorboardLogger))\n        default_callbacks.append((""exception"", RaiseExceptionCallback))\n\n        for callback_name, callback_fn in default_callbacks:\n            is_already_present = any(isinstance(x, callback_fn) for x in callbacks.values())\n            if not is_already_present:\n                callbacks[callback_name] = callback_fn()\n        return callbacks\n\n\nclass PassthroughCriterionCallback(CriterionCallback):\n    """"""\n    Returns one of model\'s outputs as loss values\n    """"""\n\n    def __init__(self, output_key, multiplier=1.0):\n        super().__init__(output_key=output_key, prefix=output_key, multiplier=multiplier)\n\n    def _compute_loss_value(self, state: RunnerState, criterion):\n        loss = self._get_output(state.output, self.output_key)\n        return loss.mean()\n\n    def _compute_loss_key_value(self, state: RunnerState, criterion):\n        loss = self._get_output(state.output, self.output_key)\n        return loss.mean()\n\n\nclass LossModule(nn.Module):\n    def __init__(self, output_key: str, target_key: str, loss_fn):\n        super().__init__()\n        self.output_key = output_key\n        self.target_ley = target_key\n        self.loss_fn = loss_fn\n\n    def forward(self, outputs, targets):  # skipcq: PYL-W0221\n        return self.loss_fn(outputs[self.output_key], targets[self.target_ley])\n\n\nclass LossWrapper(nn.Module):\n    """"""\n    A wrapper module around model that computes one or many loss functions and extends output dictionary with\n    their values. The point of this wrapper is that loss computed on each GPU node in parallel.\n\n    Usage:\n    >>> from catalyst.dl import SupervisedRunner\n    >>> runner = SupervisedRunner(input_key=None, output_key=None, device=""cuda"")\n    >>> runner._default_experiment = ParallelLossSupervisedExperiment\n    >>> loss_modules = {\n    >>>     ""my_loss"": LossModule(\n    >>>         output_key=""logits"",\n    >>>         target_key=""targets"",\n    >>>         loss_fn=nn.BCEWithLogitsLoss(),\n    >>>     )}\n    >>> loss_callback = PassthroughCriterionCallback(""my_loss"")\n    >>> runner.train(\n    >>>     callbacks=[loss_callback, ...]\n    >>>     model=LossWrapper(model, ""image"", loss_modules),\n    >>>     ...)\n\n    Note, that SupervisedRunner adds default CriterionCallback\n    """"""\n\n    def __init__(self, model: nn.Module, input_key: str, losses: Dict[str, LossModule]):\n        super().__init__()\n        self.model = model\n        self.input_key = input_key\n        self.loss_names = list(losses.keys())\n        self.losses = nn.ModuleList([losses[key] for key in self.loss_names])\n\n    def forward(self, **input: Dict[str, Tensor]) -> Dict[str, Tensor]:  # skipcq: PYL-W0221\n        output: Dict[str, Tensor] = self.model(input[self.input_key])\n\n        for output_loss_key, loss in zip(self.loss_names, self.losses):\n            output[output_loss_key] = loss(output, input)\n\n        return output\n\n    def state_dict(self, destination=None, prefix="""", keep_vars=False):\n        return self.model.state_dict()\n'"
pytorch_toolbelt/utils/catalyst/metrics.py,20,"b'import warnings\nfrom functools import partial\nfrom typing import List\n\nimport numpy as np\nimport torch\nfrom catalyst.dl import Callback, RunnerState, MetricCallback, CallbackOrder\nfrom sklearn.metrics import f1_score\nfrom torchnet.meter import ConfusionMeter\n\nfrom .visualization import get_tensorboard_logger\nfrom ..torch_utils import to_numpy\nfrom ..visualization import render_figure_to_tensor, plot_confusion_matrix\n\n__all__ = [\n    ""pixel_accuracy"",\n    ""binary_dice_iou_score"",\n    ""multiclass_dice_iou_score"",\n    ""multilabel_dice_iou_score"",\n    ""PixelAccuracyCallback"",\n    ""MacroF1Callback"",\n    ""ConfusionMatrixCallback"",\n    ""IoUMetricsCallback"",\n]\n\nBINARY_MODE = ""binary""\nMULTICLASS_MODE = ""multiclass""\nMULTILABEL_MODE = ""multilabel""\n\n\n@torch.no_grad()\ndef pixel_accuracy(outputs: torch.Tensor, targets: torch.Tensor, ignore_index=None):\n    """"""\n    Compute the pixel accuracy\n    """"""\n    outputs = outputs.detach()\n    targets = targets.detach()\n    if ignore_index is not None:\n        mask = targets != ignore_index\n        outputs = outputs[mask]\n        targets = targets[mask]\n\n    outputs = (outputs > 0).float()\n\n    correct = float(torch.sum(outputs == targets))\n    total = targets.numel()\n    return correct / total\n\n\nclass PixelAccuracyCallback(MetricCallback):\n    """"""Pixel accuracy metric callback\n    """"""\n\n    def __init__(\n        self, input_key: str = ""targets"", output_key: str = ""logits"", prefix: str = ""accuracy"", ignore_index=None\n    ):\n        """"""\n        :param input_key: input key to use for iou calculation;\n            specifies our `y_true`.\n        :param output_key: output key to use for iou calculation;\n            specifies our `y_pred`\n        :param ignore_index: same meaning as in nn.CrossEntropyLoss\n        """"""\n        super().__init__(\n            prefix=prefix,\n            metric_fn=partial(pixel_accuracy, ignore_index=ignore_index),\n            input_key=input_key,\n            output_key=output_key,\n        )\n\n\nclass ConfusionMatrixCallback(Callback):\n    """"""\n    Compute and log confusion matrix to Tensorboard.\n    For use with Multiclass classification/segmentation.\n    """"""\n\n    def __init__(\n        self,\n        input_key: str = ""targets"",\n        output_key: str = ""logits"",\n        prefix: str = ""confusion_matrix"",\n        class_names: List[str] = None,\n        num_classes: int = None,\n        ignore_index=None,\n        activation_fn=partial(torch.argmax, dim=1),\n    ):\n        """"""\n        :param input_key: input key to use for precision calculation;\n            specifies our `y_true`.\n        :param output_key: output key to use for precision calculation;\n            specifies our `y_pred`.\n        :param ignore_index: same meaning as in nn.CrossEntropyLoss\n        """"""\n        super().__init__(CallbackOrder.Metric)\n        self.prefix = prefix\n        self.class_names = class_names\n        self.num_classes = num_classes if class_names is None else len(class_names)\n        self.output_key = output_key\n        self.input_key = input_key\n        self.ignore_index = ignore_index\n        self.confusion_matrix = None\n        self.activation_fn = activation_fn\n\n    def on_loader_start(self, state):\n        self.confusion_matrix = ConfusionMeter(self.num_classes)\n\n    @torch.no_grad()\n    def on_batch_end(self, state: RunnerState):\n        outputs: torch.Tensor = state.output[self.output_key].detach().cpu()\n        outputs: torch.Tensor = self.activation_fn(outputs)\n\n        targets: torch.Tensor = state.input[self.input_key].detach().cpu()\n\n        # Flatten\n        outputs = outputs.view(-1)\n        targets = targets.view(-1)\n\n        if self.ignore_index is not None:\n            mask = targets != self.ignore_index\n            outputs = outputs[mask]\n            targets = targets[mask]\n\n        if len(targets):\n            targets = targets.type_as(outputs)\n            self.confusion_matrix.add(predicted=outputs, target=targets)\n\n    def on_loader_end(self, state):\n        if self.class_names is None:\n            class_names = [str(i) for i in range(self.num_classes)]\n        else:\n            class_names = self.class_names\n\n        num_classes = len(class_names)\n        cm = self.confusion_matrix.value()\n\n        fig = plot_confusion_matrix(\n            cm,\n            figsize=(6 + num_classes // 3, 6 + num_classes // 3),\n            class_names=class_names,\n            normalize=True,\n            noshow=True,\n        )\n        fig = render_figure_to_tensor(fig)\n\n        logger = get_tensorboard_logger(state)\n        logger.add_image(f""{self.prefix}/epoch"", fig, global_step=state.step)\n\n\nclass MacroF1Callback(Callback):\n    """"""\n    Compute F1-macro metric\n    """"""\n\n    def __init__(\n        self, input_key: str = ""targets"", output_key: str = ""logits"", prefix: str = ""macro_f1"", ignore_index=None\n    ):\n        """"""\n        :param input_key: input key to use for precision calculation;\n            specifies our `y_true`.\n        :param output_key: output key to use for precision calculation;\n            specifies our `y_pred`.\n        """"""\n        super().__init__(CallbackOrder.Metric)\n        self.metric_fn = lambda outputs, targets: f1_score(targets, outputs, average=""macro"")\n        self.prefix = prefix\n        self.output_key = output_key\n        self.input_key = input_key\n        self.outputs = []\n        self.targets = []\n        self.ignore_index = ignore_index\n\n    @torch.no_grad()\n    def on_batch_end(self, state: RunnerState):\n        outputs = to_numpy(state.output[self.output_key])\n        targets = to_numpy(state.input[self.input_key])\n\n        num_classes = outputs.shape[1]\n        outputs = np.argmax(outputs, axis=1)\n\n        if self.ignore_index is not None:\n            mask = targets != self.ignore_index\n            outputs = outputs[mask]\n            targets = targets[mask]\n\n        outputs = [np.eye(num_classes)[y] for y in outputs]\n        targets = [np.eye(num_classes)[y] for y in targets]\n\n        self.outputs.extend(outputs)\n        self.targets.extend(targets)\n\n        # metric = self.metric_fn(self.targets, self.outputs)\n        # state.metrics.add_batch_value(name=self.prefix, value=metric)\n\n    def on_loader_start(self, state):\n        self.outputs = []\n        self.targets = []\n\n    def on_loader_end(self, state):\n        metric_name = self.prefix\n        targets = np.array(self.targets)\n        outputs = np.array(self.outputs)\n\n        metric = self.metric_fn(outputs, targets)\n        state.metrics.epoch_values[state.loader_name][metric_name] = metric\n\n\ndef binary_dice_iou_score(\n    y_pred: torch.Tensor,\n    y_true: torch.Tensor,\n    mode=""dice"",\n    threshold=None,\n    nan_score_on_empty=False,\n    eps=1e-7,\n    ignore_index=None,\n) -> float:\n    """"""\n    Compute IoU score between two image tensors\n    :param y_pred: Input image tensor of any shape\n    :param y_true: Target image of any shape (must match size of y_pred)\n    :param mode: Metric to compute (dice, iou)\n    :param threshold: Optional binarization threshold to apply on @y_pred\n    :param nan_score_on_empty: If true, return np.nan if target has no positive pixels;\n        If false, return 1. if both target and input are empty, and 0 otherwise.\n    :param eps: Small value to add to denominator for numerical stability\n    :param ignore_index:\n    :return: Float scalar\n    """"""\n    assert mode in {""dice"", ""iou""}\n\n    # Make binary predictions\n    if threshold is not None:\n        y_pred = (y_pred > threshold).to(y_true.dtype)\n\n    if ignore_index is not None:\n        mask = (y_true != ignore_index).to(y_true.dtype)\n        y_true = y_true * mask\n        y_pred = y_pred * mask\n\n    intersection = torch.sum(y_pred * y_true).item()\n    cardinality = (torch.sum(y_pred) + torch.sum(y_true)).item()\n\n    if mode == ""dice"":\n        score = (2.0 * intersection) / (cardinality + eps)\n    else:\n        score = intersection / (cardinality - intersection + eps)\n\n    has_targets = torch.sum(y_true) > 0\n    has_predicted = torch.sum(y_pred) > 0\n\n    if not has_targets:\n        if nan_score_on_empty:\n            score = np.nan\n        else:\n            score = float(not has_predicted)\n    return score\n\n\ndef multiclass_dice_iou_score(\n    y_pred: torch.Tensor,\n    y_true: torch.Tensor,\n    mode=""dice"",\n    threshold=None,\n    eps=1e-7,\n    nan_score_on_empty=False,\n    classes_of_interest=None,\n):\n    ious = []\n    num_classes = y_pred.size(0)\n    y_pred = y_pred.argmax(dim=0)\n\n    if classes_of_interest is None:\n        classes_of_interest = range(num_classes)\n\n    for class_index in classes_of_interest:\n        iou = binary_dice_iou_score(\n            y_pred=(y_pred == class_index).float(),\n            y_true=(y_true == class_index).float(),\n            mode=mode,\n            nan_score_on_empty=nan_score_on_empty,\n            threshold=threshold,\n            eps=eps,\n        )\n        ious.append(iou)\n\n    return ious\n\n\ndef multilabel_dice_iou_score(\n    y_true: torch.Tensor,\n    y_pred: torch.Tensor,\n    mode=""dice"",\n    threshold=None,\n    eps=1e-7,\n    nan_score_on_empty=False,\n    classes_of_interest=None,\n    ignore_index=None,\n):\n    ious = []\n    num_classes = y_pred.size(0)\n\n    if classes_of_interest is None:\n        classes_of_interest = range(num_classes)\n\n    for class_index in classes_of_interest:\n        iou = binary_dice_iou_score(\n            y_pred=y_pred[class_index],\n            y_true=y_true[class_index],\n            mode=mode,\n            threshold=threshold,\n            nan_score_on_empty=nan_score_on_empty,\n            eps=eps,\n            ignore_index=ignore_index,\n        )\n        ious.append(iou)\n\n    return ious\n\n\nclass IoUMetricsCallback(Callback):\n    """"""\n    A metric callback for computing either Dice or Jaccard metric\n    which is computed across whole epoch, not per-batch.\n    """"""\n\n    def __init__(\n        self,\n        mode: str,\n        metric=""dice"",\n        class_names=None,\n        classes_of_interest=None,\n        input_key: str = ""targets"",\n        output_key: str = ""logits"",\n        nan_score_on_empty=True,\n        prefix: str = None,\n        ignore_index=None,\n    ):\n        """"""\n        :param mode: One of: \'binary\', \'multiclass\', \'multilabel\'.\n        :param input_key: input key to use for precision calculation; specifies our `y_true`.\n        :param output_key: output key to use for precision calculation; specifies our `y_pred`.\n        """"""\n        super().__init__(CallbackOrder.Metric)\n        assert mode in {BINARY_MODE, MULTILABEL_MODE, MULTICLASS_MODE}\n\n        if prefix is None:\n            prefix = metric\n\n        if classes_of_interest is not None:\n            if classes_of_interest.dtype == np.bool:\n                num_classes = len(classes_of_interest)\n                classes_of_interest = np.arange(num_classes)[classes_of_interest]\n\n            if class_names is not None:\n                if len(class_names) != len(classes_of_interest):\n                    raise ValueError(\n                        ""Length of \'classes_of_interest\' must be equal to length of \'classes_of_interest\'""\n                    )\n\n        self.mode = mode\n        self.prefix = prefix\n        self.output_key = output_key\n        self.input_key = input_key\n        self.class_names = class_names\n        self.classes_of_interest = classes_of_interest\n        self.scores = []\n\n        if self.mode == BINARY_MODE:\n            self.score_fn = partial(\n                binary_dice_iou_score,\n                threshold=0.0,\n                nan_score_on_empty=nan_score_on_empty,\n                mode=metric,\n                ignore_index=ignore_index,\n            )\n\n        if self.mode == MULTICLASS_MODE:\n            if ignore_index is not None:\n                warnings.warn(\n                    f""Use of ignore_index on {self.__class__.__name__} with {self.mode} ""\n                    ""is not needed as this implementation will ignore all target values outside [0..num_classes) range.""\n                )\n            self.score_fn = partial(\n                multiclass_dice_iou_score,\n                mode=metric,\n                threshold=0.0,\n                nan_score_on_empty=nan_score_on_empty,\n                classes_of_interest=self.classes_of_interest,\n            )\n\n        if self.mode == MULTILABEL_MODE:\n            self.score_fn = partial(\n                multilabel_dice_iou_score,\n                mode=metric,\n                threshold=0.0,\n                nan_score_on_empty=nan_score_on_empty,\n                classes_of_interest=self.classes_of_interest,\n                ignore_index=ignore_index,\n            )\n\n    def on_loader_start(self, state):\n        self.scores = []\n\n    @torch.no_grad()\n    def on_batch_end(self, state: RunnerState):\n        outputs = state.output[self.output_key].detach()\n        targets = state.input[self.input_key].detach()\n\n        batch_size = targets.size(0)\n        score_per_image = []\n        for image_index in range(batch_size):\n            score_per_class = self.score_fn(y_pred=outputs[image_index], y_true=targets[image_index])\n            score_per_image.append(score_per_class)\n\n        mean_score = np.nanmean(score_per_image)\n        state.metrics.add_batch_value(self.prefix, float(mean_score))\n        self.scores.extend(score_per_image)\n\n    def on_loader_end(self, state):\n        scores = np.array(self.scores)\n        mean_score = np.nanmean(scores)\n\n        state.metrics.epoch_values[state.loader_name][self.prefix] = float(mean_score)\n\n        # Log additional IoU scores per class\n        if self.mode in {MULTICLASS_MODE, MULTILABEL_MODE}:\n            num_classes = scores.shape[1]\n            class_names = self.class_names\n            if class_names is None:\n                class_names = [f""class_{i}"" for i in range(num_classes)]\n\n            scores_per_class = np.nanmean(scores, axis=0)\n            for class_name, score_per_class in zip(class_names, scores_per_class):\n                state.metrics.epoch_values[state.loader_name][self.prefix + ""_"" + class_name] = float(score_per_class)\n'"
pytorch_toolbelt/utils/catalyst/opl.py,0,"b'import numpy as np\nfrom catalyst.dl import Callback, CallbackOrder, RunnerState\n\nfrom ..torch_utils import to_numpy\n\n__all__ = [""MulticlassOnlinePseudolabelingCallback"", ""BCEOnlinePseudolabelingCallback"", ""PseudolabelDatasetMixin""]\n\n\nclass PseudolabelDatasetMixin:\n    def set_target(self, index: int, value):\n        raise NotImplementedError\n\n\nclass MulticlassOnlinePseudolabelingCallback(Callback):\n    """"""\n    Online pseudo-labeling callback for multi-class problem.\n\n    >>> unlabeled_train = get_test_dataset(\n    >>>     data_dir, image_size=image_size, augmentation=augmentations\n    >>> )\n    >>> unlabeled_eval = get_test_dataset(\n    >>>     data_dir, image_size=image_size\n    >>> )\n    >>>\n    >>> callbacks += [\n    >>>     MulticlassOnlinePseudolabelingCallback(\n    >>>         unlabeled_train.targets,\n    >>>         pseudolabel_loader=""label"",\n    >>>         prob_threshold=0.9)\n    >>> ]\n    >>> train_ds = train_ds + unlabeled_train\n    >>>\n    >>> loaders = collections.OrderedDict()\n    >>> loaders[""train""] = DataLoader(train_ds)\n    >>> loaders[""valid""] = DataLoader(valid_ds)\n    >>> loaders[""label""] = DataLoader(unlabeled_eval, shuffle=False) # ! shuffle=False is important !\n    """"""\n\n    def __init__(\n        self,\n        unlabeled_ds: PseudolabelDatasetMixin,\n        pseudolabel_loader=""label"",\n        prob_threshold=0.9,\n        prob_ratio=None,\n        output_key=""logits"",\n        unlabeled_class=-100,\n    ):\n        super().__init__(CallbackOrder.Other)\n        self.unlabeled_ds = unlabeled_ds\n        self.pseudolabel_loader = pseudolabel_loader\n        self.prob_threshold = prob_threshold\n        self.prob_ratio = prob_ratio\n        self.predictions = []\n        self.output_key = output_key\n        self.unlabeled_class = unlabeled_class\n\n    def on_epoch_start(self, state: RunnerState):\n        pass\n\n    def on_loader_start(self, state: RunnerState):\n        if state.loader_name == self.pseudolabel_loader:\n            self.predictions = []\n\n    def get_probabilities(self, state: RunnerState):\n        probs = state.output[self.output_key].detach().softmax(dim=1)\n        return to_numpy(probs)\n\n    def on_batch_end(self, state: RunnerState):\n        if state.loader_name == self.pseudolabel_loader:\n            probs = self.get_probabilities(state)\n            self.predictions.extend(probs)\n\n    def on_loader_end(self, state: RunnerState):\n        if state.loader_name == self.pseudolabel_loader:\n            predictions = np.array(self.predictions)\n            max_pred = np.argmax(predictions, axis=1)\n            max_score = np.amax(predictions, axis=1)\n            confident_mask = max_score > self.prob_threshold\n            num_samples = len(predictions)\n\n            for index, predicted_target, score in zip(range(num_samples, max_pred, max_score)):\n                target = predicted_target if score > self.prob_threshold else self.unlabeled_class\n                self.unlabeled_ds.set_target(index, target)\n\n            num_confident_samples = confident_mask.sum()\n            state.metrics.epoch_values[state.loader_name][""pseudolabeling/confident_samples""] = num_confident_samples\n            state.metrics.epoch_values[state.loader_name][""pseudolabeling/confident_samples_mean_score""] = max_score[\n                confident_mask\n            ].mean()\n\n            state.metrics.epoch_values[state.loader_name][""pseudolabeling/unconfident_samples""] = (\n                len(predictions) - num_confident_samples\n            )\n            state.metrics.epoch_values[state.loader_name][""pseudolabeling/unconfident_samples_mean_score""] = max_score[\n                ~confident_mask\n            ].mean()\n\n    def on_epoch_end(self, state: RunnerState):\n        pass\n\n\nclass BCEOnlinePseudolabelingCallback(MulticlassOnlinePseudolabelingCallback):\n    def get_probabilities(self, state: RunnerState):\n        probs = state.output[self.output_key].detach().sigmoid()\n        return to_numpy(probs)\n'"
pytorch_toolbelt/utils/catalyst/utils.py,2,"b'import os\nfrom collections import OrderedDict\nfrom pathlib import Path\nfrom typing import Dict\n\nimport safitty\nimport torch\nfrom catalyst import utils\nfrom catalyst.dl import RunnerState, Callback, CallbackOrder\nfrom catalyst.dl.callbacks.checkpoint import BaseCheckpointCallback\n\n__all__ = [""clean_checkpoint"", ""report_checkpoint"", ""BestMetricCheckpointCallback"", ""HyperParametersCallback""]\n\nfrom pytorch_toolbelt.utils.catalyst import get_tensorboard_logger\n\n\ndef clean_checkpoint(src_fname, dst_fname):\n    """"""\n    Remove optimizer, scheduler and criterion states from checkpoint\n    :param src_fname: Source checkpoint filename\n    :param dst_fname: Target checkpoint filename (can be same)\n    """"""\n    checkpoint = torch.load(src_fname, map_location=""cpu"")\n\n    keys = [""criterion_state_dict"", ""optimizer_state_dict"", ""scheduler_state_dict""]\n\n    for key in keys:\n        if key in checkpoint:\n            del checkpoint[key]\n\n    torch.save(checkpoint, dst_fname)\n\n\ndef report_checkpoint(checkpoint: Dict):\n    """"""\n    Print checkpoint metrics and epoch number\n    :param checkpoint:\n    """"""\n    print(""Epoch          :"", checkpoint[""epoch""])\n\n    skip_fields = [\n        ""_base/lr"",\n        ""_base/momentum"",\n        ""_timers/data_time"",\n        ""_timers/model_time"",\n        ""_timers/batch_time"",\n        ""_timers/_fps"",\n    ]\n    print(\n        ""Metrics (Train):"", [(k, v) for k, v, in checkpoint[""epoch_metrics""][""train""].items() if k not in skip_fields]\n    )\n    print(\n        ""Metrics (Valid):"", [(k, v) for k, v, in checkpoint[""epoch_metrics""][""valid""].items() if k not in skip_fields]\n    )\n\n\nclass BestMetricCheckpointCallback(BaseCheckpointCallback):\n    """"""\n    Checkpoint callback to save model weights based on user-defined metric value.\n    """"""\n\n    def __init__(\n        self,\n        target_metric: str,\n        target_metric_minimize=False,\n        save_n_best: int = 3,\n        checkpoints_dir=None,\n        metric_filename: str = ""_metrics.json"",\n    ):\n        """"""\n        Args:\n            target_metric (str): name of the target metric to monitor.\n            target_metric_minimize (bool): define whether metric is minimized.\n            save_n_best (int): number of best checkpoint to keep\n            checkpoints_dir (str): path to directory where checkpoints will be saved\n            metric_filename (str): filename to save metrics\n                in checkpoint folder. Must ends on ``.json`` or ``.yml``\n        """"""\n        if checkpoints_dir is None:\n            checkpoints_dir = ""checkpoints_"" + target_metric\n\n        super().__init__(metric_filename)\n        self.main_metric = target_metric\n        self.minimize_metric = target_metric_minimize\n        self.save_n_best = save_n_best\n        self.top_best_metrics = []\n        self.epochs_metrics = []\n        self.checkpoints_dir = checkpoints_dir\n        self.best_main_metric_value = None\n\n    def get_checkpoint_suffix(self, checkpoint: dict) -> str:\n        result = f""{checkpoint[\'stage\']}.{checkpoint[\'epoch\']}""\n        return result\n\n    def get_metric(self, last_valid_metrics) -> Dict:\n        top_best_checkpoints = [\n            (Path(filepath).stem, valid_metric) for (filepath, _, valid_metric) in self.top_best_metrics\n        ]\n        all_epochs_metrics = [\n            (f""epoch_{order_index}"", valid_metric) for (order_index, valid_metric) in enumerate(self.epochs_metrics)\n        ]\n        best_valid_metrics = top_best_checkpoints[0][1]\n        metrics = OrderedDict(\n            [(""best"", best_valid_metrics)] + [(""last"", last_valid_metrics)] + top_best_checkpoints + all_epochs_metrics\n        )\n\n        self.metrics = metrics\n        return self.metrics\n\n    def truncate_checkpoints(self, minimize_metric: bool) -> None:\n        self.top_best_metrics = sorted(self.top_best_metrics, key=lambda x: x[1], reverse=not minimize_metric)\n        if len(self.top_best_metrics) > self.save_n_best:\n            last_item = self.top_best_metrics.pop(-1)\n            last_filepath = Path(last_item[0])\n            last_filepaths = last_filepath.parent.glob(last_filepath.name.replace("".pth"", ""*""))\n            for filepath in last_filepaths:\n                os.remove(filepath)\n\n    def process_checkpoint(\n        self, logdir: str, checkpoint: Dict, is_best: bool, main_metric: str = ""loss"", minimize_metric: bool = True\n    ):\n        suffix = self.get_checkpoint_suffix(checkpoint)\n\n        exclude = [""criterion"", ""optimizer"", ""scheduler""]\n        checkpoint = {key: value for key, value in checkpoint.items() if all(z not in key for z in exclude)}\n        filepath = utils.save_checkpoint(\n            checkpoint=checkpoint,\n            logdir=Path(logdir) / Path(self.checkpoints_dir),\n            suffix=suffix,\n            is_best=is_best,\n            is_last=True,\n        )\n\n        valid_metrics = checkpoint[""valid_metrics""]\n        checkpoint_metric = valid_metrics[main_metric]\n        metrics_record = (filepath, checkpoint_metric, valid_metrics)\n        self.top_best_metrics.append(metrics_record)\n        self.epochs_metrics.append(metrics_record)\n        self.truncate_checkpoints(minimize_metric=minimize_metric)\n        metrics = self.get_metric(valid_metrics)\n        self.save_metric(logdir, metrics)\n\n    def on_stage_start(self, state: RunnerState):\n        self.best_main_metric_value: float = float(""+inf"") if self.minimize_metric else float(""-inf"")\n\n    def on_epoch_end(self, state: RunnerState):\n        if state.stage.startswith(""infer""):\n            return\n\n        valid_metrics = dict(state.metrics.valid_values)\n        epoch_metrics = dict(state.metrics.epoch_values)\n\n        checkpoint = utils.pack_checkpoint(\n            model=state.model,\n            criterion=state.criterion,\n            optimizer=state.optimizer,\n            scheduler=state.scheduler,\n            epoch_metrics=epoch_metrics,\n            valid_metrics=valid_metrics,\n            stage=state.stage,\n            epoch=state.epoch_log,\n            checkpoint_data=state.checkpoint_data,\n        )\n\n        main_metric_value = valid_metrics[self.main_metric]\n        if self.minimize_metric:\n            is_best = main_metric_value < self.best_main_metric_value\n        else:\n            is_best = main_metric_value > self.best_main_metric_value\n\n        if is_best:\n            self.best_main_metric_value = main_metric_value\n\n        self.process_checkpoint(\n            logdir=state.logdir,\n            checkpoint=checkpoint,\n            is_best=is_best,\n            main_metric=self.main_metric,\n            minimize_metric=self.minimize_metric,\n        )\n\n    def on_stage_end(self, state: RunnerState):\n        print(""Top best models:"")\n        top_best_metrics_str = ""\\n"".join(\n            [\n                ""{filepath}\\t{metric:3.4f}"".format(filepath=filepath, metric=checkpoint_metric)\n                for filepath, checkpoint_metric, _ in self.top_best_metrics\n            ]\n        )\n        print(top_best_metrics_str)\n\n    def save_metric(self, logdir: str, metrics: Dict) -> None:\n        safitty.save(metrics, f""{logdir}/{self.checkpoints_dir}/{self.metric_filename}"")\n\n    def on_exception(self, state: RunnerState):\n        exception = state.exception\n        if not utils.is_exception(exception):\n            return\n\n        try:\n            valid_metrics = state.metrics.valid_values\n            epoch_metrics = state.metrics.epoch_values\n            checkpoint = utils.pack_checkpoint(\n                model=state.model,\n                criterion=state.criterion,\n                optimizer=state.optimizer,\n                scheduler=state.scheduler,\n                epoch_metrics=epoch_metrics,\n                valid_metrics=valid_metrics,\n                stage=state.stage,\n                epoch=state.epoch_log,\n                checkpoint_data=state.checkpoint_data,\n            )\n            suffix = self.get_checkpoint_suffix(checkpoint)\n            suffix = f""{suffix}.exception_{exception.__class__.__name__}""\n            utils.save_checkpoint(\n                logdir=Path(f""{state.logdir}/{self.checkpoints_dir}/""),\n                checkpoint=checkpoint,\n                suffix=suffix,\n                is_best=False,\n                is_last=False,\n            )\n            metrics = self.metrics\n            metrics[suffix] = valid_metrics\n            self.save_metric(state.logdir, metrics)\n        except Exception:\n            pass\n\n\nclass HyperParametersCallback(Callback):\n    """"""\n    Callback that logs hyper-parameters for training session and target metric value.\n    Useful for evaluation of several runs in Tensorboard.\n    """"""\n\n    def __init__(self, hparam_dict: Dict):\n        if ""stage"" in hparam_dict:\n            raise KeyError(""Key \'stage\' is reserved"")\n\n        super().__init__(CallbackOrder.Metric)\n        self.hparam_dict = hparam_dict\n\n    def on_stage_end(self, state: RunnerState):\n        logger = get_tensorboard_logger(state)\n\n        hparam_dict = self.hparam_dict.copy()\n        hparam_dict[""stage""] = state.stage\n\n        logger.add_hparams(\n            hparam_dict=self.hparam_dict,\n            metric_dict={""best_"" + state.main_metric: state.metrics.best_main_metric_value},\n        )\n'"
pytorch_toolbelt/utils/catalyst/visualization.py,6,"b'import warnings\nfrom typing import Callable, Optional, List, Union\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom catalyst.dl import Callback, RunnerState, CallbackOrder\nfrom catalyst.dl.callbacks import TensorboardLogger\nfrom catalyst.utils.tensorboard import SummaryWriter\n\nfrom ..torch_utils import rgb_image_from_tensor, to_numpy\nfrom ..torch_utils import tensor_from_rgb_image\n\n__all__ = [\n    ""get_tensorboard_logger"",\n    ""ShowPolarBatchesCallback"",\n    ""ShowEmbeddingsCallback"",\n    ""draw_binary_segmentation_predictions"",\n    ""draw_semantic_segmentation_predictions"",\n]\n\n\ndef get_tensorboard_logger(state: RunnerState) -> SummaryWriter:\n    for logger_name, logger in state.loggers.items():\n        if isinstance(logger, TensorboardLogger):\n            return logger.loggers[state.loader_name]\n    raise RuntimeError(f""Cannot find Tensorboard logger for loader {state.loader_name}"")\n\n\nclass ShowPolarBatchesCallback(Callback):\n    """"""\n    Visualize best and worst batch based in metric in Tensorboard\n    """"""\n\n    def __init__(\n        self,\n        visualize_batch: Callable,\n        metric: str = ""loss"",\n        minimize: bool = True,\n        min_delta: float = 1e-6,\n        targets=""tensorboard"",\n    ):\n        """"""\n\n        :param visualize_batch: Visualization function that must return list of images.\n               It\'s takes two arguments: (batch input, predicted output).\n        :param metric:\n        :param minimize:\n        :param min_delta:\n        :param targets: Str \'tensorboard\' or \'matplotlib, or [\'tensorboard\', \'matplotlib\']\n        """"""\n        super().__init__(CallbackOrder.Other)\n        assert isinstance(targets, (list, str))\n\n        self.best_score = None\n        self.best_input = None\n        self.best_output = None\n\n        self.worst_score = None\n        self.worst_input = None\n        self.worst_output = None\n\n        self.target_metric = metric\n        self.num_bad_epochs = 0\n        self.is_better = None\n        self.visualize_batch = visualize_batch\n        self.targets = [targets] if isinstance(targets, str) else targets\n\n        if minimize:\n            self.is_better = lambda score, best: score <= (best - min_delta)\n            self.is_worse = lambda score, worst: score >= (worst + min_delta)\n        else:\n            self.is_better = lambda score, best: score >= (best + min_delta)\n            self.is_worse = lambda score, worst: score <= (worst - min_delta)\n\n    def to_cpu(self, data):\n        if isinstance(data, dict):\n            return dict((key, self.to_cpu(value)) for (key, value) in data.items())\n        if isinstance(data, torch.Tensor):\n            return data.detach().cpu()\n        if isinstance(data, (list, tuple)):\n            return [self.to_cpu(value) for value in data]\n        return data\n\n    def on_loader_start(self, state):\n        self.best_score = None\n        self.best_input = None\n        self.best_output = None\n\n        self.worst_score = None\n        self.worst_input = None\n        self.worst_output = None\n\n    def on_batch_end(self, state: RunnerState):\n        value = state.metrics.batch_values.get(self.target_metric, None)\n        if value is None:\n            warnings.warn(f""Metric value for {self.target_metric} is not available in state.metrics.batch_values"")\n            return\n\n        if self.best_score is None or self.is_better(value, self.best_score):\n            self.best_score = value\n            self.best_input = self.to_cpu(state.input)\n            self.best_output = self.to_cpu(state.output)\n\n        if self.worst_score is None or self.is_worse(value, self.worst_score):\n            self.worst_score = value\n            self.worst_input = self.to_cpu(state.input)\n            self.worst_output = self.to_cpu(state.output)\n\n    def on_loader_end(self, state: RunnerState) -> None:\n        logger = get_tensorboard_logger(state)\n\n        if self.best_score is not None:\n            best_samples = self.visualize_batch(self.best_input, self.best_output)\n            self._log_samples(best_samples, ""best"", logger, state.step)\n\n        if self.worst_score is not None:\n            worst_samples = self.visualize_batch(self.worst_input, self.worst_output)\n            self._log_samples(worst_samples, ""worst"", logger, state.step)\n\n    def _log_samples(self, samples, name, logger, step):\n        if ""tensorboard"" in self.targets:\n            for i, image in enumerate(samples):\n                logger.add_image(f""{self.target_metric}/{name}/{i}"", tensor_from_rgb_image(image), step)\n\n        if ""matplotlib"" in self.targets:\n            for i, image in enumerate(samples):\n                plt.figure()\n                plt.imshow(image)\n                plt.tight_layout()\n                plt.axis(""off"")\n                plt.show()\n\n\nclass ShowEmbeddingsCallback(Callback):\n    def __init__(\n        self,\n        embedding_key,\n        input_key,\n        targets_key,\n        prefix=""embedding"",\n        mean=(0.485, 0.456, 0.406),\n        std=(0.229, 0.224, 0.225),\n    ):\n        super().__init__(CallbackOrder.Other)\n        self.prefix = prefix\n        self.embedding_key = embedding_key\n        self.input_key = input_key\n        self.targets_key = targets_key\n        self.mean = torch.tensor(mean).view((1, 3, 1, 1))\n        self.std = torch.tensor(std).view((1, 3, 1, 1))\n\n        self.embeddings = []\n        self.images = []\n        self.targets = []\n\n    def on_loader_start(self, state: RunnerState):\n        self.embeddings = []\n        self.images = []\n        self.targets = []\n\n    def on_loader_end(self, state: RunnerState):\n        logger = get_tensorboard_logger(state)\n        logger.add_embedding(\n            mat=torch.cat(self.embeddings, dim=0),\n            metadata=self.targets,\n            label_img=torch.cat(self.images, dim=0),\n            global_step=state.epoch,\n            tag=self.prefix,\n        )\n\n    def on_batch_end(self, state: RunnerState):\n        embedding = state.output[self.embedding_key].detach().cpu()\n        image = state.input[self.input_key].detach().cpu()\n        targets = state.input[self.targets_key].detach().cpu().tolist()\n\n        image = F.interpolate(image, size=(256, 256))\n        image = image * self.std + self.mean\n\n        self.images.append(image)\n        self.embeddings.append(embedding)\n        self.targets.extend(targets)\n\n\ndef draw_binary_segmentation_predictions(\n    input: dict,\n    output: dict,\n    image_key=""features"",\n    image_id_key: Optional[str] = ""image_id"",\n    targets_key=""targets"",\n    outputs_key=""logits"",\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225),\n    max_images=None,\n    targets_threshold=0.5,\n    logits_threshold=0,\n    image_format: Union[str, Callable] = ""rgb"",\n) -> List[np.ndarray]:\n    """"""\n    Render visualization of model\'s prediction for binary segmentation problem.\n    This function draws a color-coded overlay on top of the image, with color codes meaning:\n        - green: True positives\n        - red: False-negatives\n        - yellow: False-positives\n\n    :param input: Input batch (model\'s input batch)\n    :param output: Output batch (model predictions)\n    :param image_key: Key for getting image\n    :param image_id_key: Key for getting image id/fname\n    :param targets_key: Key for getting ground-truth mask\n    :param outputs_key: Key for getting model logits for predicted mask\n    :param mean: Mean vector user during normalization\n    :param std: Std vector user during normalization\n    :param max_images: Maximum number of images to visualize from batch\n        (If you have huge batch, saving hundreds of images may make TensorBoard slow)\n    :param targets_threshold: Threshold to convert target values to binary.\n        Default value 0.5 is safe for both smoothed and hard labels.\n    :param logits_threshold: Threshold to convert model predictions (raw logits) values to binary.\n        Default value 0.0 is equivalent to 0.5 after applying sigmoid activation\n    :param image_format: Source format of the image tensor to conver to RGB representation.\n        Can be string (""gray"", ""rgb"", ""brg"") or function `convert(np.ndarray)->nd.ndarray`.\n    :return: List of images\n    """"""\n    images = []\n    num_samples = len(input[image_key])\n    if max_images is not None:\n        num_samples = min(num_samples, max_images)\n\n    assert output[outputs_key].size(1) == 1, ""Mask must be single-channel tensor of shape [Nx1xHxW]""\n\n    for i in range(num_samples):\n        image = rgb_image_from_tensor(input[image_key][i], mean, std)\n\n        if image_format == ""rgb"":\n            pass\n        elif image_format == ""bgr"":\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        elif image_format == ""gray"":\n            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n        elif isinstance(image_format, callable):\n            image = image_format(image)\n\n        target = to_numpy(input[targets_key][i]).squeeze(0)\n        logits = to_numpy(output[outputs_key][i]).squeeze(0)\n\n        overlay = image.copy()\n        true_mask = target > targets_threshold\n        pred_mask = logits > logits_threshold\n\n        overlay[true_mask & pred_mask] = np.array(\n            [0, 250, 0], dtype=overlay.dtype\n        )  # Correct predictions (Hits) painted with green\n        overlay[true_mask & ~pred_mask] = np.array([250, 0, 0], dtype=overlay.dtype)  # Misses painted with red\n        overlay[~true_mask & pred_mask] = np.array(\n            [250, 250, 0], dtype=overlay.dtype\n        )  # False alarm painted with yellow\n        overlay = cv2.addWeighted(image, 0.5, overlay, 0.5, 0, dtype=cv2.CV_8U)\n\n        if image_id_key is not None and image_id_key in input:\n            image_id = input[image_id_key][i]\n            cv2.putText(overlay, str(image_id), (10, 15), cv2.FONT_HERSHEY_PLAIN, 1, (250, 250, 250))\n\n        images.append(overlay)\n    return images\n\n\ndef draw_semantic_segmentation_predictions(\n    input: dict,\n    output: dict,\n    class_colors: List,\n    mode=""overlay"",\n    image_key=""features"",\n    image_id_key=""image_id"",\n    targets_key=""targets"",\n    outputs_key=""logits"",\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225),\n    max_images=None,\n    image_format: Union[str, Callable] = ""rgb"",\n) -> List[np.ndarray]:\n    """"""\n    Render visualization of model\'s prediction for binary segmentation problem.\n    This function draws a color-coded overlay on top of the image, with color codes meaning:\n        - green: True positives\n        - red: False-negatives\n        - yellow: False-positives\n\n    :param input: Input batch (model\'s input batch)\n    :param output: Output batch (model predictions)\n    :param class_colors:\n    :param mode:\n    :param image_key: Key for getting image\n    :param image_id_key: Key for getting image id/fname\n    :param targets_key: Key for getting ground-truth mask\n    :param outputs_key: Key for getting model logits for predicted mask\n    :param mean: Mean vector user during normalization\n    :param std: Std vector user during normalization\n    :param max_images: Maximum number of images to visualize from batch\n        (If you have huge batch, saving hundreds of images may make TensorBoard slow)\n    :param image_format: Source format of the image tensor to conver to RGB representation.\n        Can be string (""gray"", ""rgb"", ""brg"") or function `convert(np.ndarray)->nd.ndarray`.\n    :return: List of images\n    """"""\n    assert mode in {""overlay"", ""side-by-side""}\n\n    images = []\n    num_samples = len(input[image_key])\n    if max_images is not None:\n        num_samples = min(num_samples, max_images)\n\n    for i in range(num_samples):\n        image = rgb_image_from_tensor(input[image_key][i], mean, std)\n\n        if image_format == ""rgb"":\n            pass\n        elif image_format == ""bgr"":\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        elif image_format == ""gray"":\n            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n        elif isinstance(image_format, callable):\n            image = image_format(image)\n\n        target = to_numpy(input[targets_key][i])\n        logits = to_numpy(output[outputs_key][i]).argmax(axis=0)\n\n        if mode == ""overlay"":\n            overlay = image.copy()\n            for class_index, class_color in enumerate(class_colors):\n                overlay[logits == class_index, :] = class_color\n\n            overlay = cv2.addWeighted(image, 0.5, overlay, 0.5, 0, dtype=cv2.CV_8U)\n        elif mode == ""side-by-side"":\n            true_mask = np.zeros_like(image)\n            for class_index, class_color in enumerate(class_colors):\n                true_mask[target == class_index, :] = class_color\n\n            pred_mask = np.zeros_like(image)\n            for class_index, class_color in enumerate(class_colors):\n                pred_mask[logits == class_index, :] = class_color\n\n            overlay = np.hstack((image, true_mask, pred_mask))\n        else:\n            raise ValueError(mode)\n\n        if image_id_key is not None and image_id_key in input:\n            image_id = input[image_id_key][i]\n            cv2.putText(overlay, str(image_id), (10, 15), cv2.FONT_HERSHEY_PLAIN, 1, (250, 250, 250))\n\n        images.append(overlay)\n\n    return images\n\n\ndef draw_multilabel_segmentation_predictions(\n    input: dict,\n    output: dict,\n    class_colors: List,\n    mode=""side-by-side"",\n    image_key=""features"",\n    image_id_key=""image_id"",\n    targets_key=""targets"",\n    outputs_key=""logits"",\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225),\n    max_images=None,\n    targets_threshold=0.5,\n    logits_threshold=0,\n    image_format: Union[str, Callable] = ""rgb"",\n) -> List[np.ndarray]:\n    """"""\n    Render visualization of model\'s prediction for binary segmentation problem.\n    This function draws a color-coded overlay on top of the image, with color codes meaning:\n        - green: True positives\n        - red: False-negatives\n        - yellow: False-positives\n\n    :param input: Input batch (model\'s input batch)\n    :param output: Output batch (model predictions)\n    :param class_colors:\n    :param mode:\n    :param image_key: Key for getting image\n    :param image_id_key: Key for getting image id/fname\n    :param targets_key: Key for getting ground-truth mask\n    :param outputs_key: Key for getting model logits for predicted mask\n    :param mean: Mean vector user during normalization\n    :param std: Std vector user during normalization\n    :param max_images: Maximum number of images to visualize from batch\n        (If you have huge batch, saving hundreds of images may make TensorBoard slow)\n    :param image_format: Source format of the image tensor to conver to RGB representation.\n        Can be string (""gray"", ""rgb"", ""brg"") or function `convert(np.ndarray)->nd.ndarray`.\n    :return: List of images\n    """"""\n    assert mode in {""overlay"", ""side-by-side""}\n\n    images = []\n    num_samples = len(input[image_key])\n    if max_images is not None:\n        num_samples = min(num_samples, max_images)\n\n    for i in range(num_samples):\n        image = rgb_image_from_tensor(input[image_key][i], mean, std)\n\n        if image_format == ""rgb"":\n            pass\n        elif image_format == ""bgr"":\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        elif image_format == ""gray"":\n            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n        elif isinstance(image_format, callable):\n            image = image_format(image)\n\n        target = to_numpy(input[targets_key][i]) > targets_threshold\n        logits = to_numpy(output[outputs_key][i]) > logits_threshold\n\n        if mode == ""overlay"":\n            overlay = image.copy()\n            for class_index, class_color in enumerate(class_colors):\n                overlay[logits[class_index], :] = class_color\n\n            overlay = cv2.addWeighted(image, 0.5, overlay, 0.5, 0, dtype=cv2.CV_8U)\n        elif mode == ""side-by-side"":\n            true_mask = image.copy()\n            for class_index, class_color in enumerate(class_colors):\n                true_mask[target[class_index], :] = class_color\n\n            pred_mask = image.copy()\n            for class_index, class_color in enumerate(class_colors):\n                pred_mask[logits[class_index], :] = class_color\n\n            true_mask = cv2.addWeighted(image, 0.5, true_mask, 0.5, 0, dtype=cv2.CV_8U)\n            pred_mask = cv2.addWeighted(image, 0.5, pred_mask, 0.5, 0, dtype=cv2.CV_8U)\n            overlay = np.hstack((true_mask, pred_mask))\n        else:\n            raise ValueError(mode)\n\n        if image_id_key is not None and image_id_key in input:\n            image_id = input[image_id_key][i]\n            cv2.putText(overlay, str(image_id), (10, 15), cv2.FONT_HERSHEY_PLAIN, 1, (250, 250, 250))\n\n        images.append(overlay)\n\n    return images\n'"
