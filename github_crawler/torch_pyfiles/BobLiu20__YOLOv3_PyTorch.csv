file_path,api_count,code
common/__init__.py,0,b''
common/coco_dataset.py,2,"b'import os\nimport numpy as np\nimport logging\nimport cv2\n\nimport torch\nfrom torch.utils.data import Dataset\n\nfrom . import data_transforms\n\n\nclass COCODataset(Dataset):\n    def __init__(self, list_path, img_size, is_training, is_debug=False):\n        self.img_files = []\n        self.label_files = []\n        for path in open(list_path, \'r\'):\n            label_path = path.replace(\'images\', \'labels\').replace(\'.png\', \'.txt\').replace(\n                \'.jpg\', \'.txt\').strip()\n            if os.path.isfile(label_path):\n                self.img_files.append(path)\n                self.label_files.append(label_path)\n            else:\n                logging.info(""no label found. skip it: {}"".format(path))\n        logging.info(""Total images: {}"".format(len(self.img_files)))\n        self.img_size = img_size  # (w, h)\n        self.max_objects = 50\n        self.is_debug = is_debug\n\n        #  transforms and augmentation\n        self.transforms = data_transforms.Compose()\n        if is_training:\n            self.transforms.add(data_transforms.ImageBaseAug())\n        # self.transforms.add(data_transforms.KeepAspect())\n        self.transforms.add(data_transforms.ResizeImage(self.img_size))\n        self.transforms.add(data_transforms.ToTensor(self.max_objects, self.is_debug))\n\n    def __getitem__(self, index):\n        img_path = self.img_files[index % len(self.img_files)].rstrip()\n        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        if img is None:\n            raise Exception(""Read image error: {}"".format(img_path))\n        ori_h, ori_w = img.shape[:2]\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        label_path = self.label_files[index % len(self.img_files)].rstrip()\n        if os.path.exists(label_path):\n            labels = np.loadtxt(label_path).reshape(-1, 5)\n        else:\n            logging.info(""label does not exist: {}"".format(label_path))\n            labels = np.zeros((1, 5), np.float32)\n\n        sample = {\'image\': img, \'label\': labels}\n        if self.transforms is not None:\n            sample = self.transforms(sample)\n        sample[""image_path""] = img_path\n        sample[""origin_size""] = str([ori_w, ori_h])\n        return sample\n\n    def __len__(self):\n        return len(self.img_files)\n\n\n#  use for test dataloader\nif __name__ == ""__main__"":\n    dataloader = torch.utils.data.DataLoader(COCODataset(""../data/coco/trainvalno5k.txt"",\n                                                         (416, 416), True, is_debug=True),\n                                             batch_size=2,\n                                             shuffle=False, num_workers=1, pin_memory=False)\n    for step, sample in enumerate(dataloader):\n        for i, (image, label) in enumerate(zip(sample[\'image\'], sample[\'label\'])):\n            image = image.numpy()\n            h, w = image.shape[:2]\n            for l in label:\n                if l.sum() == 0:\n                    continue\n                x1 = int((l[1] - l[3] / 2) * w)\n                y1 = int((l[2] - l[4] / 2) * h)\n                x2 = int((l[1] + l[3] / 2) * w)\n                y2 = int((l[2] + l[4] / 2) * h)\n                cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255))\n            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n            cv2.imwrite(""step{}_{}.jpg"".format(step, i), image)\n        # only one batch\n        break\n'"
common/data_transforms.py,1,"b'import numpy as np\nimport cv2\nimport torch\n\n#  pip install imgaug\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\n\n\nclass Compose(object):\n    """"""Composes several transforms together.\n    Args:\n        transforms (list of ``Transform`` objects): list of transforms to compose.\n    """"""\n    def __init__(self, transforms=[]):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        for t in self.transforms:\n            img = t(img)\n        return img\n\n    def add(self, transform):\n        self.transforms.append(transform)\n\n\nclass ToTensor(object):\n    def __init__(self, max_objects=50, is_debug=False):\n        self.max_objects = max_objects\n        self.is_debug = is_debug\n\n    def __call__(self, sample):\n        image, labels = sample[\'image\'], sample[\'label\']\n        if self.is_debug == False:\n            image = image.astype(np.float32)\n            image /= 255.0\n            image = np.transpose(image, (2, 0, 1))\n            image = image.astype(np.float32)\n\n        filled_labels = np.zeros((self.max_objects, 5), np.float32)\n        filled_labels[range(len(labels))[:self.max_objects]] = labels[:self.max_objects]\n        return {\'image\': torch.from_numpy(image), \'label\': torch.from_numpy(filled_labels)}\n\nclass KeepAspect(object):\n    def __init__(self):\n        pass\n\n    def __call__(self, sample):\n        image, label = sample[\'image\'], sample[\'label\']\n\n        h, w, _ = image.shape\n        dim_diff = np.abs(h - w)\n        # Upper (left) and lower (right) padding\n        pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n        # Determine padding\n        pad = ((pad1, pad2), (0, 0), (0, 0)) if h <= w else ((0, 0), (pad1, pad2), (0, 0))\n        # Add padding\n        image_new = np.pad(image, pad, \'constant\', constant_values=128)\n        padded_h, padded_w, _ = image_new.shape\n\n        # Extract coordinates for unpadded + unscaled image\n        x1 = w * (label[:, 1] - label[:, 3]/2)\n        y1 = h * (label[:, 2] - label[:, 4]/2)\n        x2 = w * (label[:, 1] + label[:, 3]/2)\n        y2 = h * (label[:, 2] + label[:, 4]/2)\n        # Adjust for added padding\n        x1 += pad[1][0]\n        y1 += pad[0][0]\n        x2 += pad[1][0]\n        y2 += pad[0][0]\n        # Calculate ratios from coordinates\n        label[:, 1] = ((x1 + x2) / 2) / padded_w\n        label[:, 2] = ((y1 + y2) / 2) / padded_h\n        label[:, 3] *= w / padded_w\n        label[:, 4] *= h / padded_h\n\n        return {\'image\': image_new, \'label\': label}\n\nclass ResizeImage(object):\n    def __init__(self, new_size, interpolation=cv2.INTER_LINEAR):\n        self.new_size = tuple(new_size) #  (w, h)\n        self.interpolation = interpolation\n\n    def __call__(self, sample):\n        image, label = sample[\'image\'], sample[\'label\']\n        image = cv2.resize(image, self.new_size, interpolation=self.interpolation)\n        return {\'image\': image, \'label\': label}\n\nclass ImageBaseAug(object):\n    def __init__(self):\n        sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n        self.seq = iaa.Sequential(\n            [\n                # Blur each image with varying strength using\n                # gaussian blur (sigma between 0 and 3.0),\n                # average/uniform blur (kernel size between 2x2 and 7x7)\n                # median blur (kernel size between 3x3 and 11x11).\n                iaa.OneOf([\n                    iaa.GaussianBlur((0, 3.0)),\n                    iaa.AverageBlur(k=(2, 7)),\n                    iaa.MedianBlur(k=(3, 11)),\n                ]),\n                # Sharpen each image, overlay the result with the original\n                # image using an alpha between 0 (no sharpening) and 1\n                # (full sharpening effect).\n                sometimes(iaa.Sharpen(alpha=(0, 0.5), lightness=(0.75, 1.5))),\n                # Add gaussian noise to some images.\n                sometimes(iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5)),\n                # Add a value of -5 to 5 to each pixel.\n                sometimes(iaa.Add((-5, 5), per_channel=0.5)),\n                # Change brightness of images (80-120% of original value).\n                sometimes(iaa.Multiply((0.8, 1.2), per_channel=0.5)),\n                # Improve or worsen the contrast of images.\n                sometimes(iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5)),\n            ],\n            # do all of the above augmentations in random order\n            random_order=True\n        )\n\n    def __call__(self, sample):\n        seq_det = self.seq.to_deterministic()\n        image, label = sample[\'image\'], sample[\'label\']\n        image = seq_det.augment_images([image])[0]\n        return {\'image\': image, \'label\': label}\n'"
common/utils.py,14,"b'from __future__ import division\nimport math\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\n\n\ndef bbox_iou(box1, box2, x1y1x2y2=True):\n    """"""\n    Returns the IoU of two bounding boxes\n    """"""\n    if not x1y1x2y2:\n        # Transform from center and width to exact coordinates\n        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n    else:\n        # Get the coordinates of bounding boxes\n        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]\n        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]\n\n    # get the corrdinates of the intersection rectangle\n    inter_rect_x1 =  torch.max(b1_x1, b2_x1)\n    inter_rect_y1 =  torch.max(b1_y1, b2_y1)\n    inter_rect_x2 =  torch.min(b1_x2, b2_x2)\n    inter_rect_y2 =  torch.min(b1_y2, b2_y2)\n    # Intersection area\n    inter_area =    torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * \\\n                    torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)\n    # Union Area\n    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n\n    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n\n    return iou\n\n\ndef non_max_suppression(prediction, num_classes, conf_thres=0.5, nms_thres=0.4):\n    """"""\n    Removes detections with lower object confidence score than \'conf_thres\' and performs\n    Non-Maximum Suppression to further filter detections.\n    Returns detections with shape:\n        (x1, y1, x2, y2, object_conf, class_score, class_pred)\n    """"""\n\n    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n    box_corner = prediction.new(prediction.shape)\n    box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n    box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n    box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n    box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n    prediction[:, :, :4] = box_corner[:, :, :4]\n\n    output = [None for _ in range(len(prediction))]\n    for image_i, image_pred in enumerate(prediction):\n        # Filter out confidence scores below threshold\n        conf_mask = (image_pred[:, 4] >= conf_thres).squeeze()\n        image_pred = image_pred[conf_mask]\n        # If none are remaining => process next image\n        if not image_pred.size(0):\n            continue\n        # Get score and class with highest confidence\n        class_conf, class_pred = torch.max(image_pred[:, 5:5 + num_classes], 1,  keepdim=True)\n        # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)\n        detections = torch.cat((image_pred[:, :5], class_conf.float(), class_pred.float()), 1)\n        # Iterate through all predicted classes\n        unique_labels = detections[:, -1].cpu().unique()\n        if prediction.is_cuda:\n            unique_labels = unique_labels.cuda()\n        for c in unique_labels:\n            # Get the detections with the particular class\n            detections_class = detections[detections[:, -1] == c]\n            # Sort the detections by maximum objectness confidence\n            _, conf_sort_index = torch.sort(detections_class[:, 4], descending=True)\n            detections_class = detections_class[conf_sort_index]\n            # Perform non-maximum suppression\n            max_detections = []\n            while detections_class.size(0):\n                # Get detection with highest confidence and save as max detection\n                max_detections.append(detections_class[0].unsqueeze(0))\n                # Stop if we\'re at the last detection\n                if len(detections_class) == 1:\n                    break\n                # Get the IOUs for all boxes with lower confidence\n                ious = bbox_iou(max_detections[-1], detections_class[1:])\n                # Remove detections with IoU >= NMS threshold\n                detections_class = detections_class[1:][ious < nms_thres]\n\n            max_detections = torch.cat(max_detections).data\n            # Add max detections to outputs\n            output[image_i] = max_detections if output[image_i] is None else torch.cat((output[image_i], max_detections))\n\n    return output\n'"
evaluate/eval.py,7,"b'# coding=\'utf-8\'\nimport os\nimport sys\nimport numpy as np\nimport time\nimport datetime\nimport json\nimport importlib\nimport logging\nimport shutil\n\nimport torch\nimport torch.nn as nn\n\n\nMY_DIRNAME = os.path.dirname(os.path.abspath(__file__))\nsys.path.insert(0, os.path.join(MY_DIRNAME, \'..\'))\nfrom nets.model_main import ModelMain\nfrom nets.yolo_loss import YOLOLoss\nfrom common.coco_dataset import COCODataset\nfrom common.utils import non_max_suppression, bbox_iou\n\n\ndef evaluate(config):\n    is_training = False\n    # Load and initialize network\n    net = ModelMain(config, is_training=is_training)\n    net.train(is_training)\n\n    # Set data parallel\n    net = nn.DataParallel(net)\n    net = net.cuda()\n\n    # Restore pretrain model\n    if config[""pretrain_snapshot""]:\n        state_dict = torch.load(config[""pretrain_snapshot""])\n        net.load_state_dict(state_dict)\n    else:\n        logging.warning(""missing pretrain_snapshot!!!"")\n\n    # YOLO loss with 3 scales\n    yolo_losses = []\n    for i in range(3):\n        yolo_losses.append(YOLOLoss(config[""yolo""][""anchors""][i],\n                                    config[""yolo""][""classes""], (config[""img_w""], config[""img_h""])))\n\n    # DataLoader\n    dataloader = torch.utils.data.DataLoader(COCODataset(config[""val_path""],\n                                                         (config[""img_w""], config[""img_h""]),\n                                                         is_training=False),\n                                             batch_size=config[""batch_size""],\n                                             shuffle=False, num_workers=16, pin_memory=False)\n\n    # Start the eval loop\n    logging.info(""Start eval."")\n    n_gt = 0\n    correct = 0\n    for step, samples in enumerate(dataloader):\n        images, labels = samples[""image""], samples[""label""]\n        labels = labels.cuda()\n        with torch.no_grad():\n            outputs = net(images)\n            output_list = []\n            for i in range(3):\n                output_list.append(yolo_losses[i](outputs[i]))\n            output = torch.cat(output_list, 1)\n            output = non_max_suppression(output, config[""yolo""][""classes""], conf_thres=0.2)\n            #  calculate\n            for sample_i in range(labels.size(0)):\n                # Get labels for sample where width is not zero (dummies)\n                target_sample = labels[sample_i, labels[sample_i, :, 3] != 0]\n                for obj_cls, tx, ty, tw, th in target_sample:\n                    # Get rescaled gt coordinates\n                    tx1, tx2 = config[""img_w""] * (tx - tw / 2), config[""img_w""] * (tx + tw / 2)\n                    ty1, ty2 = config[""img_h""] * (ty - th / 2), config[""img_h""] * (ty + th / 2)\n                    n_gt += 1\n                    box_gt = torch.cat([coord.unsqueeze(0) for coord in [tx1, ty1, tx2, ty2]]).view(1, -1)\n                    sample_pred = output[sample_i]\n                    if sample_pred is not None:\n                        # Iterate through predictions where the class predicted is same as gt\n                        for x1, y1, x2, y2, conf, obj_conf, obj_pred in sample_pred[sample_pred[:, 6] == obj_cls]:\n                            box_pred = torch.cat([coord.unsqueeze(0) for coord in [x1, y1, x2, y2]]).view(1, -1)\n                            iou = bbox_iou(box_pred, box_gt)\n                            if iou >= config[""iou_thres""]:\n                                correct += 1\n                                break\n        if n_gt:\n            logging.info(\'Batch [%d/%d] mAP: %.5f\' % (step, len(dataloader), float(correct / n_gt)))\n\n    logging.info(\'Mean Average Precision: %.5f\' % float(correct / n_gt))\n\ndef main():\n    logging.basicConfig(level=logging.DEBUG,\n                        format=""[%(asctime)s %(filename)s] %(message)s"")\n\n    if len(sys.argv) != 2:\n        logging.error(""Usage: python eval.py params.py"")\n        sys.exit()\n    params_path = sys.argv[1]\n    if not os.path.isfile(params_path):\n        logging.error(""no params file found! path: {}"".format(params_path))\n        sys.exit()\n    config = importlib.import_module(params_path[:-3]).TRAINING_PARAMS\n    config[""batch_size""] *= len(config[""parallels""])\n\n    # Start training\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \',\'.join(map(str, config[""parallels""]))\n    evaluate(config)\n\nif __name__ == ""__main__"":\n    main()\n'"
evaluate/eval_coco.py,5,"b'# coding=\'utf-8\'\nimport os\nimport json\nfrom json import encoder\nencoder.FLOAT_REPR = lambda o: format(o, \'.2f\')\nimport sys\nimport numpy as np\nimport time\nimport datetime\nimport importlib\nimport logging\nimport shutil\n\nimport matplotlib\nmatplotlib.use(\'Agg\') #  disable display\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nimport torch\nimport torch.nn as nn\n\n\nMY_DIRNAME = os.path.dirname(os.path.abspath(__file__))\nsys.path.insert(0, os.path.join(MY_DIRNAME, \'..\'))\nfrom nets.model_main import ModelMain\nfrom nets.yolo_loss import YOLOLoss\nfrom common.coco_dataset import COCODataset\nfrom common.utils import non_max_suppression\n\n\ndef evaluate(config):\n    is_training = False\n    # Load and initialize network\n    net = ModelMain(config, is_training=is_training)\n    net.train(is_training)\n\n    # Set data parallel\n    net = nn.DataParallel(net)\n    net = net.cuda()\n\n    # Restore pretrain model\n    if config[""pretrain_snapshot""]:\n        logging.info(""Load checkpoint: {}"".format(config[""pretrain_snapshot""]))\n        state_dict = torch.load(config[""pretrain_snapshot""])\n        net.load_state_dict(state_dict)\n    else:\n        logging.warning(""missing pretrain_snapshot!!!"")\n\n    # YOLO loss with 3 scales\n    yolo_losses = []\n    for i in range(3):\n        yolo_losses.append(YOLOLoss(config[""yolo""][""anchors""][i],\n                                    config[""yolo""][""classes""], (config[""img_w""], config[""img_h""])))\n\n    # DataLoader.\n    dataloader = torch.utils.data.DataLoader(COCODataset(config[""val_path""],\n                                                         (config[""img_w""], config[""img_h""]),\n                                                         is_training=False),\n                                             batch_size=config[""batch_size""],\n                                             shuffle=False, num_workers=8, pin_memory=False)\n\n    # Coco Prepare.\n    index2category = json.load(open(""coco_index2category.json""))\n\n    # Start the eval loop\n    logging.info(""Start eval."")\n    coco_results = []\n    coco_img_ids= set([])\n    for step, samples in enumerate(dataloader):\n        images, labels = samples[""image""], samples[""label""]\n        image_paths, origin_sizes = samples[""image_path""], samples[""origin_size""]\n        with torch.no_grad():\n            outputs = net(images)\n            output_list = []\n            for i in range(3):\n                output_list.append(yolo_losses[i](outputs[i]))\n            output = torch.cat(output_list, 1)\n            batch_detections = non_max_suppression(output, config[""yolo""][""classes""],\n                                                   conf_thres=0.01,\n                                                   nms_thres=0.45)\n        for idx, detections in enumerate(batch_detections):\n            image_id = int(os.path.basename(image_paths[idx])[-16:-4])\n            coco_img_ids.add(image_id)\n            if detections is not None:\n                origin_size = eval(origin_sizes[idx])\n                detections = detections.cpu().numpy()\n                for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n                    x1 = x1 / config[""img_w""] * origin_size[0]\n                    x2 = x2 / config[""img_w""] * origin_size[0]\n                    y1 = y1 / config[""img_h""] * origin_size[1]\n                    y2 = y2 / config[""img_h""] * origin_size[1]\n                    w = x2 - x1\n                    h = y2 - y1\n                    coco_results.append({\n                        ""image_id"": image_id,\n                        ""category_id"": index2category[str(int(cls_pred.item()))],\n                        ""bbox"": (float(x1), float(y1), float(w), float(h)),\n                        ""score"": float(conf),\n                    })\n        logging.info(""Now {}/{}"".format(step, len(dataloader)))\n    save_results_path = ""coco_results.json""\n    with open(save_results_path, ""w"") as f:\n        json.dump(coco_results, f, sort_keys=True, indent=4, separators=(\',\', \':\'))\n    logging.info(""Save coco format results to {}"".format(save_results_path))\n\n    #  COCO api\n    logging.info(""Using coco-evaluate tools to evaluate."")\n    cocoGt = COCO(config[""annotation_path""])\n    cocoDt = cocoGt.loadRes(save_results_path)\n    cocoEval = COCOeval(cocoGt, cocoDt, ""bbox"")\n    cocoEval.params.imgIds  = list(coco_img_ids) # real imgIds\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    cocoEval.summarize()\n\n\ndef main():\n    logging.basicConfig(level=logging.DEBUG,\n                        format=""[%(asctime)s %(filename)s] %(message)s"")\n\n    if len(sys.argv) != 2:\n        logging.error(""Usage: python eval_coco.py params.py"")\n        sys.exit()\n    params_path = sys.argv[1]\n    if not os.path.isfile(params_path):\n        logging.error(""no params file found! path: {}"".format(params_path))\n        sys.exit()\n    config = importlib.import_module(params_path[:-3]).TRAINING_PARAMS\n    config[""batch_size""] *= len(config[""parallels""])\n\n    # Start training\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \',\'.join(map(str, config[""parallels""]))\n    evaluate(config)\n\nif __name__ == ""__main__"":\n    main()\n'"
evaluate/params.py,1,"b'TRAINING_PARAMS = \\\n{\n    ""model_params"": {\n        ""backbone_name"": ""darknet_53"",\n        ""backbone_pretrained"": """",\n    },\n    ""yolo"": {\n        ""anchors"": [[[116, 90], [156, 198], [373, 326]],\n                    [[30, 61], [62, 45], [59, 119]],\n                    [[10, 13], [16, 30], [33, 23]]],\n        ""classes"": 80,\n    },\n    ""batch_size"": 16,\n    ""iou_thres"": 0.5,\n    ""val_path"": ""../data/coco/5k.txt"",\n    ""annotation_path"": ""../data/coco/annotations/instances_val2014.json"",\n    ""img_h"": 416,\n    ""img_w"": 416,\n    ""parallels"": [0],\n    ""pretrain_snapshot"": ""../weights/official_yolov3_weights_pytorch.pth"",\n}\n'"
nets/__init__.py,0,b''
nets/model_main.py,11,"b'import torch\nimport torch.nn as nn\nfrom collections import OrderedDict\n\nfrom .backbone import backbone_fn\n\n\nclass ModelMain(nn.Module):\n    def __init__(self, config, is_training=True):\n        super(ModelMain, self).__init__()\n        self.config = config\n        self.training = is_training\n        self.model_params = config[""model_params""]\n        #  backbone\n        _backbone_fn = backbone_fn[self.model_params[""backbone_name""]]\n        self.backbone = _backbone_fn(self.model_params[""backbone_pretrained""])\n        _out_filters = self.backbone.layers_out_filters\n        #  embedding0\n        final_out_filter0 = len(config[""yolo""][""anchors""][0]) * (5 + config[""yolo""][""classes""])\n        self.embedding0 = self._make_embedding([512, 1024], _out_filters[-1], final_out_filter0)\n        #  embedding1\n        final_out_filter1 = len(config[""yolo""][""anchors""][1]) * (5 + config[""yolo""][""classes""])\n        self.embedding1_cbl = self._make_cbl(512, 256, 1)\n        self.embedding1_upsample = nn.Upsample(scale_factor=2, mode=\'nearest\')\n        self.embedding1 = self._make_embedding([256, 512], _out_filters[-2] + 256, final_out_filter1)\n        #  embedding2\n        final_out_filter2 = len(config[""yolo""][""anchors""][2]) * (5 + config[""yolo""][""classes""])\n        self.embedding2_cbl = self._make_cbl(256, 128, 1)\n        self.embedding2_upsample = nn.Upsample(scale_factor=2, mode=\'nearest\')\n        self.embedding2 = self._make_embedding([128, 256], _out_filters[-3] + 128, final_out_filter2)\n\n    def _make_cbl(self, _in, _out, ks):\n        \'\'\' cbl = conv + batch_norm + leaky_relu\n        \'\'\'\n        pad = (ks - 1) // 2 if ks else 0\n        return nn.Sequential(OrderedDict([\n            (""conv"", nn.Conv2d(_in, _out, kernel_size=ks, stride=1, padding=pad, bias=False)),\n            (""bn"", nn.BatchNorm2d(_out)),\n            (""relu"", nn.LeakyReLU(0.1)),\n        ]))\n\n    def _make_embedding(self, filters_list, in_filters, out_filter):\n        m = nn.ModuleList([\n            self._make_cbl(in_filters, filters_list[0], 1),\n            self._make_cbl(filters_list[0], filters_list[1], 3),\n            self._make_cbl(filters_list[1], filters_list[0], 1),\n            self._make_cbl(filters_list[0], filters_list[1], 3),\n            self._make_cbl(filters_list[1], filters_list[0], 1),\n            self._make_cbl(filters_list[0], filters_list[1], 3)])\n        m.add_module(""conv_out"", nn.Conv2d(filters_list[1], out_filter, kernel_size=1,\n                                           stride=1, padding=0, bias=True))\n        return m\n\n    def forward(self, x):\n        def _branch(_embedding, _in):\n            for i, e in enumerate(_embedding):\n                _in = e(_in)\n                if i == 4:\n                    out_branch = _in\n            return _in, out_branch\n        #  backbone\n        x2, x1, x0 = self.backbone(x)\n        #  yolo branch 0\n        out0, out0_branch = _branch(self.embedding0, x0)\n        #  yolo branch 1\n        x1_in = self.embedding1_cbl(out0_branch)\n        x1_in = self.embedding1_upsample(x1_in)\n        x1_in = torch.cat([x1_in, x1], 1)\n        out1, out1_branch = _branch(self.embedding1, x1_in)\n        #  yolo branch 2\n        x2_in = self.embedding2_cbl(out1_branch)\n        x2_in = self.embedding2_upsample(x2_in)\n        x2_in = torch.cat([x2_in, x2], 1)\n        out2, out2_branch = _branch(self.embedding2, x2_in)\n        return out0, out1, out2\n\n    def load_darknet_weights(self, weights_path):\n        import numpy as np\n        #Open the weights file\n        fp = open(weights_path, ""rb"")\n        header = np.fromfile(fp, dtype=np.int32, count=5)   # First five are header values\n        # Needed to write header when saving weights\n        weights = np.fromfile(fp, dtype=np.float32)         # The rest are weights\n        print (""total len weights = "", weights.shape)\n        fp.close()\n\n        ptr = 0\n        all_dict = self.state_dict()\n        all_keys = self.state_dict().keys()\n        print (all_keys)\n        last_bn_weight = None\n        last_conv = None\n        for i, (k, v) in enumerate(all_dict.items()):\n            if \'bn\' in k:\n                if \'weight\' in k:\n                    last_bn_weight = v\n                elif \'bias\' in k:\n                    num_b = v.numel()\n                    vv = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(v)\n                    v.copy_(vv)\n                    print (""bn_bias: "", ptr, num_b, k)\n                    ptr += num_b\n                    # weight\n                    v = last_bn_weight\n                    num_b = v.numel()\n                    vv = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(v)\n                    v.copy_(vv)\n                    print (""bn_weight: "", ptr, num_b, k)\n                    ptr += num_b\n                    last_bn_weight = None\n                elif \'running_mean\' in k:\n                    num_b = v.numel()\n                    vv = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(v)\n                    v.copy_(vv)\n                    print (""bn_mean: "", ptr, num_b, k)\n                    ptr += num_b\n                elif \'running_var\' in k:\n                    num_b = v.numel()\n                    vv = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(v)\n                    v.copy_(vv)\n                    print (""bn_var: "", ptr, num_b, k)\n                    ptr += num_b\n                    # conv\n                    v = last_conv\n                    num_b = v.numel()\n                    vv = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(v)\n                    v.copy_(vv)\n                    print (""conv wight: "", ptr, num_b, k)\n                    ptr += num_b\n                    last_conv = None\n                else:\n                    raise Exception(""Error for bn"")\n            elif \'conv\' in k:\n                if \'weight\' in k:\n                    last_conv = v\n                else:\n                    num_b = v.numel()\n                    vv = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(v)\n                    v.copy_(vv)\n                    print (""conv bias: "", ptr, num_b, k)\n                    ptr += num_b\n                    # conv\n                    v = last_conv\n                    num_b = v.numel()\n                    vv = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(v)\n                    v.copy_(vv)\n                    print (""conv wight: "", ptr, num_b, k)\n                    ptr += num_b\n                    last_conv = None\n        print(""Total ptr = "", ptr)\n        print(""real size = "", weights.shape)\n\n\nif __name__ == ""__main__"":\n    config = {""model_params"": {""backbone_name"": ""darknet_53""}}\n    m = ModelMain(config)\n    x = torch.randn(1, 3, 416, 416)\n    y0, y1, y2 = m(x)\n    print(y0.size())\n    print(y1.size())\n    print(y2.size())\n\n'"
nets/yolo_loss.py,23,"b'import torch\nimport torch.nn as nn\nimport numpy as np\nimport math\n\nfrom common.utils import bbox_iou\n\n\nclass YOLOLoss(nn.Module):\n    def __init__(self, anchors, num_classes, img_size):\n        super(YOLOLoss, self).__init__()\n        self.anchors = anchors\n        self.num_anchors = len(anchors)\n        self.num_classes = num_classes\n        self.bbox_attrs = 5 + num_classes\n        self.img_size = img_size\n\n        self.ignore_threshold = 0.5\n        self.lambda_xy = 2.5\n        self.lambda_wh = 2.5\n        self.lambda_conf = 1.0\n        self.lambda_cls = 1.0\n\n        self.mse_loss = nn.MSELoss()\n        self.bce_loss = nn.BCELoss()\n\n    def forward(self, input, targets=None):\n        bs = input.size(0)\n        in_h = input.size(2)\n        in_w = input.size(3)\n        stride_h = self.img_size[1] / in_h\n        stride_w = self.img_size[0] / in_w\n        scaled_anchors = [(a_w / stride_w, a_h / stride_h) for a_w, a_h in self.anchors]\n\n        prediction = input.view(bs,  self.num_anchors,\n                                self.bbox_attrs, in_h, in_w).permute(0, 1, 3, 4, 2).contiguous()\n\n        # Get outputs\n        x = torch.sigmoid(prediction[..., 0])          # Center x\n        y = torch.sigmoid(prediction[..., 1])          # Center y\n        w = prediction[..., 2]                         # Width\n        h = prediction[..., 3]                         # Height\n        conf = torch.sigmoid(prediction[..., 4])       # Conf\n        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.\n\n        if targets is not None:\n            #  build target\n            mask, noobj_mask, tx, ty, tw, th, tconf, tcls = self.get_target(targets, scaled_anchors,\n                                                                           in_w, in_h,\n                                                                           self.ignore_threshold)\n            mask, noobj_mask = mask.cuda(), noobj_mask.cuda()\n            tx, ty, tw, th = tx.cuda(), ty.cuda(), tw.cuda(), th.cuda()\n            tconf, tcls = tconf.cuda(), tcls.cuda()\n            #  losses.\n            loss_x = self.bce_loss(x * mask, tx * mask)\n            loss_y = self.bce_loss(y * mask, ty * mask)\n            loss_w = self.mse_loss(w * mask, tw * mask)\n            loss_h = self.mse_loss(h * mask, th * mask)\n            loss_conf = self.bce_loss(conf * mask, mask) + \\\n                0.5 * self.bce_loss(conf * noobj_mask, noobj_mask * 0.0)\n            loss_cls = self.bce_loss(pred_cls[mask == 1], tcls[mask == 1])\n            #  total loss = losses * weight\n            loss = loss_x * self.lambda_xy + loss_y * self.lambda_xy + \\\n                loss_w * self.lambda_wh + loss_h * self.lambda_wh + \\\n                loss_conf * self.lambda_conf + loss_cls * self.lambda_cls\n\n            return loss, loss_x.item(), loss_y.item(), loss_w.item(),\\\n                loss_h.item(), loss_conf.item(), loss_cls.item()\n        else:\n            FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n            LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n            # Calculate offsets for each grid\n            grid_x = torch.linspace(0, in_w-1, in_w).repeat(in_w, 1).repeat(\n                bs * self.num_anchors, 1, 1).view(x.shape).type(FloatTensor)\n            grid_y = torch.linspace(0, in_h-1, in_h).repeat(in_h, 1).t().repeat(\n                bs * self.num_anchors, 1, 1).view(y.shape).type(FloatTensor)\n            # Calculate anchor w, h\n            anchor_w = FloatTensor(scaled_anchors).index_select(1, LongTensor([0]))\n            anchor_h = FloatTensor(scaled_anchors).index_select(1, LongTensor([1]))\n            anchor_w = anchor_w.repeat(bs, 1).repeat(1, 1, in_h * in_w).view(w.shape)\n            anchor_h = anchor_h.repeat(bs, 1).repeat(1, 1, in_h * in_w).view(h.shape)\n            # Add offset and scale with anchors\n            pred_boxes = FloatTensor(prediction[..., :4].shape)\n            pred_boxes[..., 0] = x.data + grid_x\n            pred_boxes[..., 1] = y.data + grid_y\n            pred_boxes[..., 2] = torch.exp(w.data) * anchor_w\n            pred_boxes[..., 3] = torch.exp(h.data) * anchor_h\n            # Results\n            _scale = torch.Tensor([stride_w, stride_h] * 2).type(FloatTensor)\n            output = torch.cat((pred_boxes.view(bs, -1, 4) * _scale,\n                                conf.view(bs, -1, 1), pred_cls.view(bs, -1, self.num_classes)), -1)\n            return output.data\n\n    def get_target(self, target, anchors, in_w, in_h, ignore_threshold):\n        bs = target.size(0)\n\n        mask = torch.zeros(bs, self.num_anchors, in_h, in_w, requires_grad=False)\n        noobj_mask = torch.ones(bs, self.num_anchors, in_h, in_w, requires_grad=False)\n        tx = torch.zeros(bs, self.num_anchors, in_h, in_w, requires_grad=False)\n        ty = torch.zeros(bs, self.num_anchors, in_h, in_w, requires_grad=False)\n        tw = torch.zeros(bs, self.num_anchors, in_h, in_w, requires_grad=False)\n        th = torch.zeros(bs, self.num_anchors, in_h, in_w, requires_grad=False)\n        tconf = torch.zeros(bs, self.num_anchors, in_h, in_w, requires_grad=False)\n        tcls = torch.zeros(bs, self.num_anchors, in_h, in_w, self.num_classes, requires_grad=False)\n        for b in range(bs):\n            for t in range(target.shape[1]):\n                if target[b, t].sum() == 0:\n                    continue\n                # Convert to position relative to box\n                gx = target[b, t, 1] * in_w\n                gy = target[b, t, 2] * in_h\n                gw = target[b, t, 3] * in_w\n                gh = target[b, t, 4] * in_h\n                # Get grid box indices\n                gi = int(gx)\n                gj = int(gy)\n                # Get shape of gt box\n                gt_box = torch.FloatTensor(np.array([0, 0, gw, gh])).unsqueeze(0)\n                # Get shape of anchor box\n                anchor_shapes = torch.FloatTensor(np.concatenate((np.zeros((self.num_anchors, 2)),\n                                                                  np.array(anchors)), 1))\n                # Calculate iou between gt and anchor shapes\n                anch_ious = bbox_iou(gt_box, anchor_shapes)\n                # Where the overlap is larger than threshold set mask to zero (ignore)\n                noobj_mask[b, anch_ious > ignore_threshold, gj, gi] = 0\n                # Find the best matching anchor box\n                best_n = np.argmax(anch_ious)\n\n                # Masks\n                mask[b, best_n, gj, gi] = 1\n                # Coordinates\n                tx[b, best_n, gj, gi] = gx - gi\n                ty[b, best_n, gj, gi] = gy - gj\n                # Width and height\n                tw[b, best_n, gj, gi] = math.log(gw/anchors[best_n][0] + 1e-16)\n                th[b, best_n, gj, gi] = math.log(gh/anchors[best_n][1] + 1e-16)\n                # object\n                tconf[b, best_n, gj, gi] = 1\n                # One-hot encoding of label\n                tcls[b, best_n, gj, gi, int(target[b, t, 0])] = 1\n\n        return mask, noobj_mask, tx, ty, tw, th, tconf, tcls\n'"
test/params.py,1,"b'TRAINING_PARAMS = \\\n{\n    ""model_params"": {\n        ""backbone_name"": ""darknet_53"",\n        ""backbone_pretrained"": """",\n    },\n    ""yolo"": {\n        ""anchors"": [[[116, 90], [156, 198], [373, 326]],\n                    [[30, 61], [62, 45], [59, 119]],\n                    [[10, 13], [16, 30], [33, 23]]],\n        ""classes"": 80,\n    },\n    ""batch_size"": 16,\n    ""confidence_threshold"": 0.5,\n    ""images_path"": ""./images/"",\n    ""classes_names_path"": ""../data/coco.names"",\n    ""img_h"": 416,\n    ""img_w"": 416,\n    ""parallels"": [0],\n    ""pretrain_snapshot"": ""../weights/official_yolov3_weights_pytorch.pth"",\n}\n'"
test/test_fps.py,6,"b'# coding=\'utf-8\'\nimport os\nimport sys\nimport numpy as np\nimport time\nimport datetime\nimport json\nimport importlib\nimport logging\nimport shutil\nimport cv2\nimport random\n\nimport torch\nimport torch.nn as nn\n\n\nMY_DIRNAME = os.path.dirname(os.path.abspath(__file__))\nsys.path.insert(0, os.path.join(MY_DIRNAME, \'..\'))\nfrom nets.model_main import ModelMain\nfrom nets.yolo_loss import YOLOLoss\nfrom common.utils import non_max_suppression, bbox_iou\n\n\ndef test(config):\n    is_training = False\n    # Load and initialize network\n    net = ModelMain(config, is_training=is_training)\n    net.train(is_training)\n\n    # Set data parallel\n    net = nn.DataParallel(net)\n    net = net.cuda()\n\n    # Restore pretrain model\n    if config[""pretrain_snapshot""]:\n        logging.info(""load checkpoint from {}"".format(config[""pretrain_snapshot""]))\n        state_dict = torch.load(config[""pretrain_snapshot""])\n        net.load_state_dict(state_dict)\n    else:\n        raise Exception(""missing pretrain_snapshot!!!"")\n\n    # YOLO loss with 3 scales\n    yolo_losses = []\n    for i in range(3):\n        yolo_losses.append(YOLOLoss(config[""yolo""][""anchors""][i],\n                                    config[""yolo""][""classes""], (config[""img_w""], config[""img_h""])))\n\n    # prepare images path\n    images_name = os.listdir(config[""images_path""])\n    images_path = [os.path.join(config[""images_path""], name) for name in images_name]\n    if len(images_path) == 0:\n        raise Exception(""no image found in {}"".format(config[""images_path""]))\n\n    # Start testing FPS of different batch size\n    for batch_size in range(1, 10):\n        # preprocess\n        images = []\n        for path in images_path[: batch_size]:\n            image = cv2.imread(path, cv2.IMREAD_COLOR)\n            if image is None:\n                logging.error(""read path error: {}. skip it."".format(path))\n                continue\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = cv2.resize(image, (config[""img_w""], config[""img_h""]),\n                               interpolation=cv2.INTER_LINEAR)\n            image = image.astype(np.float32)\n            image /= 255.0\n            image = np.transpose(image, (2, 0, 1))\n            image = image.astype(np.float32)\n            images.append(image)\n        for i in range(batch_size-len(images)):\n            images.append(images[0]) #  fill len to batch_sze\n        images = np.asarray(images)\n        images = torch.from_numpy(images).cuda()\n        # inference in 30 times and calculate average\n        inference_times = []\n        for i in range(30):\n            start_time = time.time()\n            with torch.no_grad():\n                outputs = net(images)\n                output_list = []\n                for i in range(3):\n                    output_list.append(yolo_losses[i](outputs[i]))\n                output = torch.cat(output_list, 1)\n                batch_detections = non_max_suppression(output, config[""yolo""][""classes""],\n                                                       conf_thres=config[""confidence_threshold""])\n                torch.cuda.synchronize() #  wait all done.\n            end_time = time.time()\n            inference_times.append(end_time - start_time)\n        inference_time = sum(inference_times) / len(inference_times) / batch_size\n        fps = 1.0 / inference_time\n        logging.info(""Batch_Size: {}, Inference_Time: {:.5f} s/image, FPS: {}"".format(batch_size,\n                                                                                     inference_time,\n                                                                                     fps))\n\n\n\ndef main():\n    logging.basicConfig(level=logging.DEBUG,\n                        format=""[%(asctime)s %(filename)s] %(message)s"")\n\n    if len(sys.argv) != 2:\n        logging.error(""Usage: python test_images.py params.py"")\n        sys.exit()\n    params_path = sys.argv[1]\n    if not os.path.isfile(params_path):\n        logging.error(""no params file found! path: {}"".format(params_path))\n        sys.exit()\n    config = importlib.import_module(params_path[:-3]).TRAINING_PARAMS\n    config[""batch_size""] *= len(config[""parallels""])\n\n    # Start training\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \',\'.join(map(str, config[""parallels""]))\n    test(config)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
test/test_images.py,5,"b'# coding=\'utf-8\'\nimport os\nimport sys\nimport numpy as np\nimport time\nimport datetime\nimport json\nimport importlib\nimport logging\nimport shutil\nimport cv2\nimport random\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.ticker import NullLocator\n\nimport torch\nimport torch.nn as nn\n\n\nMY_DIRNAME = os.path.dirname(os.path.abspath(__file__))\nsys.path.insert(0, os.path.join(MY_DIRNAME, \'..\'))\nfrom nets.model_main import ModelMain\nfrom nets.yolo_loss import YOLOLoss\nfrom common.utils import non_max_suppression, bbox_iou\n\ncmap = plt.get_cmap(\'tab20b\')\ncolors = [cmap(i) for i in np.linspace(0, 1, 20)]\n\n\ndef test(config):\n    is_training = False\n    # Load and initialize network\n    net = ModelMain(config, is_training=is_training)\n    net.train(is_training)\n\n    # Set data parallel\n    net = nn.DataParallel(net)\n    net = net.cuda()\n\n    # Restore pretrain model\n    if config[""pretrain_snapshot""]:\n        logging.info(""load checkpoint from {}"".format(config[""pretrain_snapshot""]))\n        state_dict = torch.load(config[""pretrain_snapshot""])\n        net.load_state_dict(state_dict)\n    else:\n        raise Exception(""missing pretrain_snapshot!!!"")\n\n    # YOLO loss with 3 scales\n    yolo_losses = []\n    for i in range(3):\n        yolo_losses.append(YOLOLoss(config[""yolo""][""anchors""][i],\n                                    config[""yolo""][""classes""], (config[""img_w""], config[""img_h""])))\n\n    # prepare images path\n    images_name = os.listdir(config[""images_path""])\n    images_path = [os.path.join(config[""images_path""], name) for name in images_name]\n    if len(images_path) == 0:\n        raise Exception(""no image found in {}"".format(config[""images_path""]))\n\n    # Start inference\n    batch_size = config[""batch_size""]\n    for step in range(0, len(images_path), batch_size):\n        # preprocess\n        images = []\n        images_origin = []\n        for path in images_path[step*batch_size: (step+1)*batch_size]:\n            logging.info(""processing: {}"".format(path))\n            image = cv2.imread(path, cv2.IMREAD_COLOR)\n            if image is None:\n                logging.error(""read path error: {}. skip it."".format(path))\n                continue\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            images_origin.append(image)  # keep for save result\n            image = cv2.resize(image, (config[""img_w""], config[""img_h""]),\n                               interpolation=cv2.INTER_LINEAR)\n            image = image.astype(np.float32)\n            image /= 255.0\n            image = np.transpose(image, (2, 0, 1))\n            image = image.astype(np.float32)\n            images.append(image)\n        images = np.asarray(images)\n        images = torch.from_numpy(images).cuda()\n        # inference\n        with torch.no_grad():\n            outputs = net(images)\n            output_list = []\n            for i in range(3):\n                output_list.append(yolo_losses[i](outputs[i]))\n            output = torch.cat(output_list, 1)\n            batch_detections = non_max_suppression(output, config[""yolo""][""classes""],\n                                                   conf_thres=config[""confidence_threshold""],\n                                                   nms_thres=0.45)\n\n        # write result images. Draw bounding boxes and labels of detections\n        classes = open(config[""classes_names_path""], ""r"").read().split(""\\n"")[:-1]\n        if not os.path.isdir(""./output/""):\n            os.makedirs(""./output/"")\n        for idx, detections in enumerate(batch_detections):\n            plt.figure()\n            fig, ax = plt.subplots(1)\n            ax.imshow(images_origin[idx])\n            if detections is not None:\n                unique_labels = detections[:, -1].cpu().unique()\n                n_cls_preds = len(unique_labels)\n                bbox_colors = random.sample(colors, n_cls_preds)\n                for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n                    color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n                    # Rescale coordinates to original dimensions\n                    ori_h, ori_w = images_origin[idx].shape[:2]\n                    pre_h, pre_w = config[""img_h""], config[""img_w""]\n                    box_h = ((y2 - y1) / pre_h) * ori_h\n                    box_w = ((x2 - x1) / pre_w) * ori_w\n                    y1 = (y1 / pre_h) * ori_h\n                    x1 = (x1 / pre_w) * ori_w\n                    # Create a Rectangle patch\n                    bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2,\n                                             edgecolor=color,\n                                             facecolor=\'none\')\n                    # Add the bbox to the plot\n                    ax.add_patch(bbox)\n                    # Add label\n                    plt.text(x1, y1, s=classes[int(cls_pred)], color=\'white\',\n                             verticalalignment=\'top\',\n                             bbox={\'color\': color, \'pad\': 0})\n            # Save generated image with detections\n            plt.axis(\'off\')\n            plt.gca().xaxis.set_major_locator(NullLocator())\n            plt.gca().yaxis.set_major_locator(NullLocator())\n            plt.savefig(\'output/{}_{}.jpg\'.format(step, idx), bbox_inches=\'tight\', pad_inches=0.0)\n            plt.close()\n    logging.info(""Save all results to ./output/"")    \n\n\ndef main():\n    logging.basicConfig(level=logging.DEBUG,\n                        format=""[%(asctime)s %(filename)s] %(message)s"")\n\n    if len(sys.argv) != 2:\n        logging.error(""Usage: python test_images.py params.py"")\n        sys.exit()\n    params_path = sys.argv[1]\n    if not os.path.isfile(params_path):\n        logging.error(""no params file found! path: {}"".format(params_path))\n        sys.exit()\n    config = importlib.import_module(params_path[:-3]).TRAINING_PARAMS\n    config[""batch_size""] *= len(config[""parallels""])\n\n    # Start training\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \',\'.join(map(str, config[""parallels""]))\n    test(config)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
training/params.py,1,"b'TRAINING_PARAMS = \\\n{\n    ""model_params"": {\n        ""backbone_name"": ""darknet_53"",\n        ""backbone_pretrained"": ""../weights/darknet53_weights_pytorch.pth"", #  set empty to disable\n    },\n    ""yolo"": {\n        ""anchors"": [[[116, 90], [156, 198], [373, 326]],\n                    [[30, 61], [62, 45], [59, 119]],\n                    [[10, 13], [16, 30], [33, 23]]],\n        ""classes"": 80,\n    },\n    ""lr"": {\n        ""backbone_lr"": 0.001,\n        ""other_lr"": 0.01,\n        ""freeze_backbone"": False,   #  freeze backbone wegiths to finetune\n        ""decay_gamma"": 0.1,\n        ""decay_step"": 20,           #  decay lr in every ? epochs\n    },\n    ""optimizer"": {\n        ""type"": ""sgd"",\n        ""weight_decay"": 4e-05,\n    },\n    ""batch_size"": 16,\n    ""train_path"": ""../data/coco/trainvalno5k.txt"",\n    ""epochs"": 100,\n    ""img_h"": 416,\n    ""img_w"": 416,\n    ""parallels"": [0,1,2,3],                         #  config GPU device\n    ""working_dir"": ""YOUR_WORKING_DIR"",              #  replace with your working dir\n    ""pretrain_snapshot"": """",                        #  load checkpoint\n    ""evaluate_type"": """", \n    ""try"": 0,\n    ""export_onnx"": False,\n}\n'"
training/training.py,9,"b'# coding=\'utf-8\'\nimport os\nimport sys\nimport numpy as np\nimport time\nimport datetime\nimport json\nimport importlib\nimport logging\nimport shutil\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom tensorboardX import SummaryWriter\n\nMY_DIRNAME = os.path.dirname(os.path.abspath(__file__))\nsys.path.insert(0, os.path.join(MY_DIRNAME, \'..\'))\n# sys.path.insert(0, os.path.join(MY_DIRNAME, \'..\', \'evaluate\'))\nfrom nets.model_main import ModelMain\nfrom nets.yolo_loss import YOLOLoss\nfrom common.coco_dataset import COCODataset\n\n\ndef train(config):\n    config[""global_step""] = config.get(""start_step"", 0)\n    is_training = False if config.get(""export_onnx"") else True\n\n    # Load and initialize network\n    net = ModelMain(config, is_training=is_training)\n    net.train(is_training)\n\n    # Optimizer and learning rate\n    optimizer = _get_optimizer(config, net)\n    lr_scheduler = optim.lr_scheduler.StepLR(\n        optimizer,\n        step_size=config[""lr""][""decay_step""],\n        gamma=config[""lr""][""decay_gamma""])\n\n    # Set data parallel\n    net = nn.DataParallel(net)\n    net = net.cuda()\n\n    # Restore pretrain model\n    if config[""pretrain_snapshot""]:\n        logging.info(""Load pretrained weights from {}"".format(config[""pretrain_snapshot""]))\n        state_dict = torch.load(config[""pretrain_snapshot""])\n        net.load_state_dict(state_dict)\n\n    # Only export onnx\n    # if config.get(""export_onnx""):\n        # real_model = net.module\n        # real_model.eval()\n        # dummy_input = torch.randn(8, 3, config[""img_h""], config[""img_w""]).cuda()\n        # save_path = os.path.join(config[""sub_working_dir""], ""pytorch.onnx"")\n        # logging.info(""Exporting onnx to {}"".format(save_path))\n        # torch.onnx.export(real_model, dummy_input, save_path, verbose=False)\n        # logging.info(""Done. Exiting now."")\n        # sys.exit()\n\n    # Evaluate interface\n    # if config[""evaluate_type""]:\n        # logging.info(""Using {} to evaluate model."".format(config[""evaluate_type""]))\n        # evaluate_func = importlib.import_module(config[""evaluate_type""]).run_eval\n        # config[""online_net""] = net\n\n    # YOLO loss with 3 scales\n    yolo_losses = []\n    for i in range(3):\n        yolo_losses.append(YOLOLoss(config[""yolo""][""anchors""][i],\n                                    config[""yolo""][""classes""], (config[""img_w""], config[""img_h""])))\n\n    # DataLoader\n    dataloader = torch.utils.data.DataLoader(COCODataset(config[""train_path""],\n                                                         (config[""img_w""], config[""img_h""]),\n                                                         is_training=True),\n                                             batch_size=config[""batch_size""],\n                                             shuffle=True, num_workers=32, pin_memory=True)\n\n    # Start the training loop\n    logging.info(""Start training."")\n    for epoch in range(config[""epochs""]):\n        for step, samples in enumerate(dataloader):\n            images, labels = samples[""image""], samples[""label""]\n            start_time = time.time()\n            config[""global_step""] += 1\n\n            # Forward and backward\n            optimizer.zero_grad()\n            outputs = net(images)\n            losses_name = [""total_loss"", ""x"", ""y"", ""w"", ""h"", ""conf"", ""cls""]\n            losses = []\n            for _ in range(len(losses_name)):\n                losses.append([])\n            for i in range(3):\n                _loss_item = yolo_losses[i](outputs[i], labels)\n                for j, l in enumerate(_loss_item):\n                    losses[j].append(l)\n            losses = [sum(l) for l in losses]\n            loss = losses[0]\n            loss.backward()\n            optimizer.step()\n\n            if step > 0 and step % 10 == 0:\n                _loss = loss.item()\n                duration = float(time.time() - start_time)\n                example_per_second = config[""batch_size""] / duration\n                lr = optimizer.param_groups[0][\'lr\']\n                logging.info(\n                    ""epoch [%.3d] iter = %d loss = %.2f example/sec = %.3f lr = %.5f ""%\n                    (epoch, step, _loss, example_per_second, lr)\n                )\n                config[""tensorboard_writer""].add_scalar(""lr"",\n                                                        lr,\n                                                        config[""global_step""])\n                config[""tensorboard_writer""].add_scalar(""example/sec"",\n                                                        example_per_second,\n                                                        config[""global_step""])\n                for i, name in enumerate(losses_name):\n                    value = _loss if i == 0 else losses[i]\n                    config[""tensorboard_writer""].add_scalar(name,\n                                                            value,\n                                                            config[""global_step""])\n\n            if step > 0 and step % 1000 == 0:\n                # net.train(False)\n                _save_checkpoint(net.state_dict(), config)\n                # net.train(True)\n\n        lr_scheduler.step()\n\n    # net.train(False)\n    _save_checkpoint(net.state_dict(), config)\n    # net.train(True)\n    logging.info(""Bye~"")\n\n# best_eval_result = 0.0\ndef _save_checkpoint(state_dict, config, evaluate_func=None):\n    # global best_eval_result\n    checkpoint_path = os.path.join(config[""sub_working_dir""], ""model.pth"")\n    torch.save(state_dict, checkpoint_path)\n    logging.info(""Model checkpoint saved to %s"" % checkpoint_path)\n    # eval_result = evaluate_func(config)\n    # if eval_result > best_eval_result:\n        # best_eval_result = eval_result\n        # logging.info(""New best result: {}"".format(best_eval_result))\n        # best_checkpoint_path = os.path.join(config[""sub_working_dir""], \'model_best.pth\')\n        # shutil.copyfile(checkpoint_path, best_checkpoint_path)\n        # logging.info(""Best checkpoint saved to {}"".format(best_checkpoint_path))\n    # else:\n        # logging.info(""Best result: {}"".format(best_eval_result))\n\n\ndef _get_optimizer(config, net):\n    optimizer = None\n\n    # Assign different lr for each layer\n    params = None\n    base_params = list(\n        map(id, net.backbone.parameters())\n    )\n    logits_params = filter(lambda p: id(p) not in base_params, net.parameters())\n\n    if not config[""lr""][""freeze_backbone""]:\n        params = [\n            {""params"": logits_params, ""lr"": config[""lr""][""other_lr""]},\n            {""params"": net.backbone.parameters(), ""lr"": config[""lr""][""backbone_lr""]},\n        ]\n    else:\n        logging.info(""freeze backbone\'s parameters."")\n        for p in net.backbone.parameters():\n            p.requires_grad = False\n        params = [\n            {""params"": logits_params, ""lr"": config[""lr""][""other_lr""]},\n        ]\n\n    # Initialize optimizer class\n    if config[""optimizer""][""type""] == ""adam"":\n        optimizer = optim.Adam(params, weight_decay=config[""optimizer""][""weight_decay""])\n    elif config[""optimizer""][""type""] == ""amsgrad"":\n        optimizer = optim.Adam(params, weight_decay=config[""optimizer""][""weight_decay""],\n                               amsgrad=True)\n    elif config[""optimizer""][""type""] == ""rmsprop"":\n        optimizer = optim.RMSprop(params, weight_decay=config[""optimizer""][""weight_decay""])\n    else:\n        # Default to sgd\n        logging.info(""Using SGD optimizer."")\n        optimizer = optim.SGD(params, momentum=0.9,\n                              weight_decay=config[""optimizer""][""weight_decay""],\n                              nesterov=(config[""optimizer""][""type""] == ""nesterov""))\n\n    return optimizer\n\ndef main():\n    logging.basicConfig(level=logging.DEBUG,\n                        format=""[%(asctime)s %(filename)s] %(message)s"")\n\n    if len(sys.argv) != 2:\n        logging.error(""Usage: python training.py params.py"")\n        sys.exit()\n    params_path = sys.argv[1]\n    if not os.path.isfile(params_path):\n        logging.error(""no params file found! path: {}"".format(params_path))\n        sys.exit()\n    config = importlib.import_module(params_path[:-3]).TRAINING_PARAMS\n    config[""batch_size""] *= len(config[""parallels""])\n\n    # Create sub_working_dir\n    sub_working_dir = \'{}/{}/size{}x{}_try{}/{}\'.format(\n        config[\'working_dir\'], config[\'model_params\'][\'backbone_name\'], \n        config[\'img_w\'], config[\'img_h\'], config[\'try\'],\n        time.strftime(""%Y%m%d%H%M%S"", time.localtime()))\n    if not os.path.exists(sub_working_dir):\n        os.makedirs(sub_working_dir)\n    config[""sub_working_dir""] = sub_working_dir\n    logging.info(""sub working dir: %s"" % sub_working_dir)\n\n    # Creat tf_summary writer\n    config[""tensorboard_writer""] = SummaryWriter(sub_working_dir)\n    logging.info(""Please using \'python -m tensorboard.main --logdir={}\'"".format(sub_working_dir))\n\n    # Start training\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \',\'.join(map(str, config[""parallels""]))\n    train(config)\n\nif __name__ == ""__main__"":\n    main()\n'"
nets/backbone/__init__.py,0,"b'from . import darknet\n\nbackbone_fn = {\n    ""darknet_21"": darknet.darknet21,\n    ""darknet_53"": darknet.darknet53,\n}\n'"
nets/backbone/darknet.py,3,"b'import torch\nimport torch.nn as nn\nimport math\nfrom collections import OrderedDict\n\n__all__ = [\'darknet21\', \'darknet53\']\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, inplanes, planes):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes[0], kernel_size=1,\n                               stride=1, padding=0, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes[0])\n        self.relu1 = nn.LeakyReLU(0.1)\n        self.conv2 = nn.Conv2d(planes[0], planes[1], kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes[1])\n        self.relu2 = nn.LeakyReLU(0.1)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu1(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu2(out)\n\n        out += residual\n        return out\n\n\nclass DarkNet(nn.Module):\n    def __init__(self, layers):\n        super(DarkNet, self).__init__()\n        self.inplanes = 32\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.inplanes)\n        self.relu1 = nn.LeakyReLU(0.1)\n\n        self.layer1 = self._make_layer([32, 64], layers[0])\n        self.layer2 = self._make_layer([64, 128], layers[1])\n        self.layer3 = self._make_layer([128, 256], layers[2])\n        self.layer4 = self._make_layer([256, 512], layers[3])\n        self.layer5 = self._make_layer([512, 1024], layers[4])\n\n        self.layers_out_filters = [64, 128, 256, 512, 1024]\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, planes, blocks):\n        layers = []\n        #  downsample\n        layers.append((""ds_conv"", nn.Conv2d(self.inplanes, planes[1], kernel_size=3,\n                                stride=2, padding=1, bias=False)))\n        layers.append((""ds_bn"", nn.BatchNorm2d(planes[1])))\n        layers.append((""ds_relu"", nn.LeakyReLU(0.1)))\n        #  blocks\n        self.inplanes = planes[1]\n        for i in range(0, blocks):\n            layers.append((""residual_{}"".format(i), BasicBlock(self.inplanes, planes)))\n        return nn.Sequential(OrderedDict(layers))\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu1(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        out3 = self.layer3(x)\n        out4 = self.layer4(out3)\n        out5 = self.layer5(out4)\n\n        return out3, out4, out5\n\ndef darknet21(pretrained, **kwargs):\n    """"""Constructs a darknet-21 model.\n    """"""\n    model = DarkNet([1, 1, 2, 2, 1])\n    if pretrained:\n        if isinstance(pretrained, str):\n            model.load_state_dict(torch.load(pretrained))\n        else:\n            raise Exception(""darknet request a pretrained path. got [{}]"".format(pretrained))\n    return model\n\ndef darknet53(pretrained, **kwargs):\n    """"""Constructs a darknet-53 model.\n    """"""\n    model = DarkNet([1, 2, 8, 8, 4])\n    if pretrained:\n        if isinstance(pretrained, str):\n            model.load_state_dict(torch.load(pretrained))\n        else:\n            raise Exception(""darknet request a pretrained path. got [{}]"".format(pretrained))\n    return model\n'"
