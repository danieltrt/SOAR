file_path,api_count,code
setup.py,0,"b""# Copyright (c) 2018-present, Royal Bank of Canada and other authors.\n# See the AUTHORS.txt file for a list of contributors.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nimport os\nfrom setuptools import setup\nfrom setuptools import find_packages\n\n\nwith open(os.path.join(os.path.dirname(__file__), 'advertorch/VERSION')) as f:\n    version = f.read().strip()\n\n\nsetup(name='advertorch',\n      version=version,\n      url='https://github.com/BorealisAI/advertorch',\n      package_data={'advertorch_examples': ['*.ipynb', 'trained_models/*.pt']},\n      install_requires=[],\n      include_package_data=True,\n      packages=find_packages())\n"""
advertorch/__init__.py,0,"b""# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\n\nimport os\n\nwith open(os.path.join(os.path.dirname(__file__), 'VERSION')) as f:\n    __version__ = f.read().strip()\n\nfrom . import attacks  # noqa: F401\nfrom . import defenses  # noqa: F401\n"""
advertorch/bpda.py,8,"b'# Copyright (c) 2018-present, Royal Bank of Canada and other authors.\n# See the AUTHORS.txt file for a list of contributors.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\n# BPDA stands for Backward Pass Differentiable Approximation\n# See:\n# Athalye, A., Carlini, N. & Wagner, D.. (2018). Obfuscated Gradients Give a\n# False Sense of Security: Circumventing Defenses to Adversarial Examples.\n# Proceedings of the 35th International Conference on Machine Learning,\n# in PMLR 80:274-283\n\nimport torch\nimport torch.nn as nn\n\n__all__ = [\'BPDAWrapper\']\n\n\nclass FunctionWrapper(nn.Module):\n    """"""`nn.Module` wrapping a `torch.autograd.Function`.""""""\n\n    def __init__(self, func):\n        """"""Wraps the provided function `func`.\n\n        :param func: the `torch.autograd.Function` to be wrapped.\n        """"""\n        super(FunctionWrapper, self).__init__()\n        self.func = func\n\n    def forward(self, *inputs):\n        """"""Wraps the `forward` method of `func`.""""""\n        return self.func.apply(*inputs)\n\n\nclass BPDAWrapper(FunctionWrapper):\n    """"""Backward Pass Differentiable Approximation.\n\n    The module should be provided a `forward` method and a `backward`\n    method that approximates the derivatives of `forward`.\n\n    The `forward` function is called in the forward pass, and the\n    `backward` function is used to find gradients in the backward pass.\n\n    The `backward` function can be implicitly provided-by providing\n    `forwardsub` - an alternative forward pass function, which its\n    gradient will be used in the backward pass.\n\n    If not `backward` nor `forwardsub` are provided, the `backward`\n    function will be assumed to be the identity.\n\n    :param forward: `forward(*inputs)` - the forward function for BPDA.\n    :param forwardsub: (Optional) a substitute forward function, for the\n                       gradients approximation of `forward`.\n    :param backward: (Optional) `backward(inputs, grad_outputs)` the\n                     backward pass function for BPDA.\n    """"""\n\n    def __init__(self, forward, forwardsub=None, backward=None):\n        func = self._create_func(forward, backward, forwardsub)\n        super(BPDAWrapper, self).__init__(func)\n\n    @classmethod\n    def _create_func(cls, forward_fn, backward_fn, forwardsub_fn):\n        if backward_fn is not None:\n            return cls._create_func_backward(forward_fn, backward_fn)\n\n        if forwardsub_fn is not None:\n            return cls._create_func_forwardsub(forward_fn, forwardsub_fn)\n\n        return cls._create_func_forward_only(forward_fn)\n\n    @classmethod\n    def _create_func_forward_only(cls, forward_fn):\n        """"""Creates a differentiable `Function` given the forward function,\n        and the identity as backward function.""""""\n\n        class Func(torch.autograd.Function):\n\n            @staticmethod\n            def forward(ctx, *inputs, **kwargs):\n                ctx.save_for_backward(*inputs)\n                return forward_fn(*inputs, **kwargs)\n\n            @staticmethod\n            def backward(ctx, *grad_outputs):\n                inputs = ctx.saved_tensors\n                if len(grad_outputs) == len(inputs):\n                    return grad_outputs\n                elif len(grad_outputs) == 1:\n                    return tuple([grad_outputs[0] for _ in inputs])\n\n                raise ValueError(""Expected %d gradients but got %d"" %\n                                 (len(inputs), len(grad_outputs)))\n\n\n        return Func\n\n    @classmethod\n    def _create_func_forwardsub(cls, forward_fn, forwardsub_fn):\n        """"""Creates a differentiable `Function` given the forward function,\n        and a substitute forward function.\n\n        The substitute forward function is used to approximate the gradients\n        in the backward pass.\n        """"""\n\n        class Func(torch.autograd.Function):\n\n            @staticmethod\n            def forward(ctx, *inputs, **kwargs):\n                ctx.save_for_backward(*inputs)\n                return forward_fn(*inputs, **kwargs)\n\n            @staticmethod\n            @torch.enable_grad()  # enables grad in the method\'s scope\n            def backward(ctx, *grad_outputs):\n                inputs = ctx.saved_tensors\n                inputs = [x.detach().clone().requires_grad_() for x in inputs]\n                outputs = forwardsub_fn(*inputs)\n                return torch.autograd.grad(outputs, inputs, grad_outputs)\n\n        return Func\n\n    @classmethod\n    def _create_func_backward(cls, forward_fn, backward_fn):\n        """"""Creates a differentiable `Function` given the forward and backward\n        functions.""""""\n\n        class Func(torch.autograd.Function):\n\n            @staticmethod\n            def forward(ctx, *inputs, **kwargs):\n                ctx.save_for_backward(*inputs)\n                return forward_fn(*inputs, **kwargs)\n\n            @staticmethod\n            def backward(ctx, *grad_outputs):\n                inputs = ctx.saved_tensors\n                return backward_fn(inputs, grad_outputs)\n\n        return Func\n'"
advertorch/context.py,0,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom contextlib import contextmanager\n\n\nclass ctx_noparamgrad(object):\n    def __init__(self, module):\n        self.prev_grad_state = get_param_grad_state(module)\n        self.module = module\n        set_param_grad_off(module)\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, *args):\n        set_param_grad_state(self.module, self.prev_grad_state)\n        return False\n\n\nclass ctx_eval(object):\n    def __init__(self, module):\n        self.prev_training_state = get_module_training_state(module)\n        self.module = module\n        set_module_training_off(module)\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, *args):\n        set_module_training_state(self.module, self.prev_training_state)\n        return False\n\n\n@contextmanager\ndef ctx_noparamgrad_and_eval(module):\n    with ctx_noparamgrad(module) as a, ctx_eval(module) as b:\n        yield (a, b)\n\n\ndef get_module_training_state(module):\n    return {mod: mod.training for mod in module.modules()}\n\n\ndef set_module_training_state(module, training_state):\n    for mod in module.modules():\n        mod.training = training_state[mod]\n\n\ndef set_module_training_off(module):\n    for mod in module.modules():\n        mod.training = False\n\n\ndef get_param_grad_state(module):\n    return {param: param.requires_grad for param in module.parameters()}\n\n\ndef set_param_grad_state(module, grad_state):\n    for param in module.parameters():\n        param.requires_grad = grad_state[param]\n\n\ndef set_param_grad_off(module):\n    for param in module.parameters():\n        param.requires_grad = False\n'"
advertorch/functional.py,4,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\ntry:\n    from cStringIO import StringIO as BytesIO\nexcept ImportError:\n    from io import BytesIO\n\nimport torch\nfrom torchvision import transforms\nfrom PIL import Image\n\n_to_pil_image = transforms.ToPILImage()\n_to_tensor = transforms.ToTensor()\n\n\nclass FloatToIntSqueezing(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, max_int, vmin, vmax):\n        # here assuming 0 =< x =< 1\n        x = (x - vmin) / (vmax - vmin)\n        x = torch.round(x * max_int) / max_int\n        return x * (vmax - vmin) + vmin\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        raise NotImplementedError(\n            ""backward not implemented"", FloatToIntSqueezing)\n\n\n\nclass JPEGEncodingDecoding(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, quality):\n        lst_img = []\n        for img in x:\n            img = _to_pil_image(img.detach().clone().cpu())\n            virtualpath = BytesIO()\n            img.save(virtualpath, \'JPEG\', quality=quality)\n            lst_img.append(_to_tensor(Image.open(virtualpath)))\n        return x.new_tensor(torch.stack(lst_img))\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        raise NotImplementedError(\n            ""backward not implemented"", JPEGEncodingDecoding)\n'"
advertorch/loss.py,9,"b'import torch\nfrom torch.nn.modules.loss import _Loss\nfrom advertorch.utils import clamp\n\n\nclass ZeroOneLoss(_Loss):\n    """"""Zero-One Loss""""""\n\n    def __init__(self, size_average=None, reduce=None,\n                 reduction=\'elementwise_mean\'):\n        super(ZeroOneLoss, self).__init__(size_average, reduce, reduction)\n\n    def forward(self, input, target):\n        return logit_margin_loss(input, target, reduction=self.reduction)\n\n\n\nclass LogitMarginLoss(_Loss):\n    """"""Logit Margin Loss""""""\n\n    def __init__(self, size_average=None, reduce=None,\n                 reduction=\'elementwise_mean\', offset=0.):\n        super(LogitMarginLoss, self).__init__(size_average, reduce, reduction)\n        self.offset = offset\n\n    def forward(self, input, target):\n        return logit_margin_loss(\n            input, target, reduction=self.reduction, offset=self.offset)\n\n\nclass CWLoss(_Loss):\n    """"""CW Loss""""""\n    # TODO: combine with the CWLoss in advertorch.utils\n\n    def __init__(self, size_average=None, reduce=None,\n                 reduction=\'elementwise_mean\'):\n        super(CWLoss, self).__init__(size_average, reduce, reduction)\n\n    def forward(self, input, target):\n        return cw_loss(input, target, reduction=self.reduction)\n\n\nclass SoftLogitMarginLoss(_Loss):\n    """"""Soft Logit Margin Loss""""""\n\n    def __init__(self, size_average=None, reduce=None,\n                 reduction=\'elementwise_mean\', offset=0.):\n        super(SoftLogitMarginLoss, self).__init__(\n            size_average, reduce, reduction)\n        self.offset = offset\n\n    def forward(self, logits, targets):\n        return soft_logit_margin_loss(\n            logits, targets, reduction=self.reduction, offset=self.offset)\n\n\ndef zero_one_loss(input, target, reduction=\'elementwise_mean\'):\n    loss = (input != target)\n    return _reduce_loss(loss, reduction)\n\n\ndef elementwise_margin(logits, label):\n    batch_size = logits.size(0)\n    topval, topidx = logits.topk(2, dim=1)\n    maxelse = ((label != topidx[:, 0]).float() * topval[:, 0]\n               + (label == topidx[:, 0]).float() * topval[:, 1])\n    return maxelse - logits[torch.arange(batch_size), label]\n\n\ndef logit_margin_loss(input, target, reduction=\'elementwise_mean\', offset=0.):\n    loss = elementwise_margin(input, target)\n    return _reduce_loss(loss, reduction) + offset\n\n\ndef cw_loss(input, target, reduction=\'elementwise_mean\'):\n    loss = clamp(elementwise_margin(input, target) + 50, 0.)\n    return _reduce_loss(loss, reduction)\n\n\ndef _reduce_loss(loss, reduction):\n    if reduction == \'none\':\n        return loss\n    elif reduction == \'elementwise_mean\':\n        return loss.mean()\n    elif reduction == \'sum\':\n        return loss.sum()\n    else:\n        raise ValueError(reduction + "" is not valid"")\n\n\ndef soft_logit_margin_loss(\n        logits, targets, reduction=\'elementwise_mean\', offset=0.):\n    batch_size = logits.size(0)\n    num_class = logits.size(1)\n    mask = torch.ones_like(logits).byte()\n    # TODO: need to cover different versions of torch\n    # mask = torch.ones_like(logits).bool()\n    mask[torch.arange(batch_size), targets] = 0\n    logits_true_label = logits[torch.arange(batch_size), targets]\n    logits_other_label = logits[mask].reshape(batch_size, num_class - 1)\n    loss = torch.logsumexp(logits_other_label, dim=1) - logits_true_label\n    return _reduce_loss(loss, reduction) + offset\n'"
advertorch/test_utils.py,33,"b'# Copyright (c) 2018-present, Royal Bank of Canada and other authors.\n# See the AUTHORS.txt file for a list of contributors.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom advertorch.attacks import LocalSearchAttack\nfrom advertorch.attacks import SinglePixelAttack\nfrom advertorch.attacks import SpatialTransformAttack\nfrom advertorch.attacks import JacobianSaliencyMapAttack\nfrom advertorch.attacks import LBFGSAttack\nfrom advertorch.attacks import CarliniWagnerL2Attack\nfrom advertorch.attacks import DDNL2Attack\nfrom advertorch.attacks import FastFeatureAttack\nfrom advertorch.attacks import MomentumIterativeAttack\nfrom advertorch.attacks import LinfPGDAttack\nfrom advertorch.attacks import SparseL1DescentAttack\nfrom advertorch.attacks import L1PGDAttack\nfrom advertorch.attacks import L2BasicIterativeAttack\nfrom advertorch.attacks import GradientAttack\nfrom advertorch.attacks import LinfBasicIterativeAttack\nfrom advertorch.attacks import GradientSignAttack\nfrom advertorch.attacks import ElasticNetL1Attack\nfrom advertorch.attacks import LinfSPSAAttack\nfrom advertorch.attacks import LinfFABAttack\nfrom advertorch.attacks import L2FABAttack\nfrom advertorch.attacks import L1FABAttack\nfrom advertorch.defenses import JPEGFilter\nfrom advertorch.defenses import BitSqueezing\nfrom advertorch.defenses import MedianSmoothing2D\nfrom advertorch.defenses import AverageSmoothing2D\nfrom advertorch.defenses import GaussianSmoothing2D\nfrom advertorch.defenses import BinaryFilter\n\n\nDIM_INPUT = 15\nNUM_CLASS = 5\nBATCH_SIZE = 16\n\nIMAGE_SIZE = 16\nCOLOR_CHANNEL = 3\n\n\n# ###########################################################\n# model definitions for testing\n\n\nclass SimpleModel(nn.Module):\n    def __init__(self, dim_input=DIM_INPUT, num_classes=NUM_CLASS):\n        super(SimpleModel, self).__init__()\n        self.fc1 = nn.Linear(dim_input, 10)\n        self.fc2 = nn.Linear(10, num_classes)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        return x\n\n\nclass SimpleImageModel(nn.Module):\n\n    def __init__(self, num_classes=NUM_CLASS):\n        super(SimpleImageModel, self).__init__()\n        self.num_classes = NUM_CLASS\n        self.conv1 = nn.Conv2d(\n            COLOR_CHANNEL, 8, kernel_size=3, padding=1, stride=1)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.maxpool1 = nn.MaxPool2d(4)\n        self.linear1 = nn.Linear(4 * 4 * 8, self.num_classes)\n\n    def forward(self, x):\n        out = self.maxpool1(self.relu1(self.conv1(x)))\n        out = out.view(out.size(0), -1)\n        out = self.linear1(out)\n        return out\n\n\nclass LeNet5(nn.Module):\n\n    def __init__(self):\n        super(LeNet5, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1, stride=1)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.maxpool1 = nn.MaxPool2d(2)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=1)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.maxpool2 = nn.MaxPool2d(2)\n        self.linear1 = nn.Linear(7 * 7 * 64, 200)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.linear2 = nn.Linear(200, 10)\n\n    def forward(self, x):\n        out = self.maxpool1(self.relu1(self.conv1(x)))\n        out = self.maxpool2(self.relu2(self.conv2(out)))\n        out = out.view(out.size(0), -1)\n        out = self.relu3(self.linear1(out))\n        out = self.linear2(out)\n        return out\n\n\nclass MLP(nn.Module):\n    # MLP-300-100\n\n    def __init__(self):\n        super(MLP, self).__init__()\n        self.linear1 = nn.Linear(28 * 28, 300)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.linear2 = nn.Linear(300, 100)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.linear3 = nn.Linear(100, 10)\n\n    def forward(self, x):\n        out = x.view(x.size(0), -1)\n        out = self.linear1(out)\n        out = self.relu1(out)\n        out = self.linear2(out)\n        out = self.relu2(out)\n        out = self.linear3(out)\n        return out\n\n\n# ###########################################################\n# model and data generation functions for testing\n\n\ndef generate_random_toy_data(clip_min=0., clip_max=1.):\n    data = torch.Tensor(BATCH_SIZE, DIM_INPUT).uniform_(clip_min, clip_max)\n    label = torch.LongTensor(BATCH_SIZE).random_(NUM_CLASS)\n    return data, label\n\n\ndef generate_random_image_toy_data(clip_min=0., clip_max=1.):\n    data = torch.Tensor(BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE).uniform_(\n        clip_min, clip_max)\n    label = torch.LongTensor(BATCH_SIZE).random_(NUM_CLASS)\n    return data, label\n\n\ndef generate_data_model_on_vec():\n    data, label = generate_random_toy_data()\n    model = SimpleModel()\n    model.eval()\n    return data, label, model\n\n\ndef generate_data_model_on_img():\n    data, label = generate_random_image_toy_data()\n    model = SimpleImageModel()\n    model.eval()\n    return data, label, model\n\n\n# ###########################################################\n# construct data needed for testing\nvecdata, veclabel, vecmodel = generate_data_model_on_vec()\nimgdata, imglabel, imgmodel = generate_data_model_on_img()\n\n\n# ###########################################################\n# construct groups and configs needed for testing defenses\n\n\ndefense_kwargs = {\n    BinaryFilter: {},\n    BitSqueezing: {""bit_depth"": 4},\n    MedianSmoothing2D: {},\n    GaussianSmoothing2D: {""sigma"": 3, ""channels"": COLOR_CHANNEL},\n    AverageSmoothing2D: {""kernel_size"": 5, ""channels"": COLOR_CHANNEL},\n    JPEGFilter: {},\n}\n\ndefenses = defense_kwargs.keys()\n\n# store one suitable data for test\ndefense_data = {\n    BinaryFilter: vecdata,\n    BitSqueezing: vecdata,\n    MedianSmoothing2D: imgdata,\n    GaussianSmoothing2D: imgdata,\n    AverageSmoothing2D: imgdata,\n    JPEGFilter: imgdata,\n}\n\nnograd_defenses = [\n    BinaryFilter,\n    BitSqueezing,\n    JPEGFilter,\n]\n\nwithgrad_defenses = [\n    MedianSmoothing2D,\n    GaussianSmoothing2D,\n    AverageSmoothing2D,\n]\n\nimage_only_defenses = [\n    MedianSmoothing2D,\n    GaussianSmoothing2D,\n    AverageSmoothing2D,\n    JPEGFilter,\n]\n\n# as opposed to image-only\ngeneral_input_defenses = [\n    BitSqueezing,\n    BinaryFilter,\n]\n\n\n# ###########################################################\n# construct groups and configs needed for testing attacks\n\n\n# as opposed to image-only\ngeneral_input_attacks = [\n    GradientSignAttack,\n    LinfBasicIterativeAttack,\n    GradientAttack,\n    L2BasicIterativeAttack,\n    LinfPGDAttack,\n    MomentumIterativeAttack,\n    FastFeatureAttack,\n    CarliniWagnerL2Attack,\n    ElasticNetL1Attack,\n    LBFGSAttack,\n    JacobianSaliencyMapAttack,\n    SinglePixelAttack,\n    DDNL2Attack,\n    SparseL1DescentAttack,\n    L1PGDAttack,\n    LinfSPSAAttack,\n    LinfFABAttack,\n    L2FABAttack,\n    L1FABAttack,\n]\n\nimage_only_attacks = [\n    SpatialTransformAttack,\n    LocalSearchAttack,\n]\n\nlabel_attacks = [\n    GradientSignAttack,\n    LinfBasicIterativeAttack,\n    GradientAttack,\n    L2BasicIterativeAttack,\n    LinfPGDAttack,\n    MomentumIterativeAttack,\n    CarliniWagnerL2Attack,\n    ElasticNetL1Attack,\n    LBFGSAttack,\n    JacobianSaliencyMapAttack,\n    SpatialTransformAttack,\n    DDNL2Attack,\n    SparseL1DescentAttack,\n    L1PGDAttack,\n    LinfSPSAAttack,\n    LinfFABAttack,\n    L2FABAttack,\n    L1FABAttack,\n]\n\nfeature_attacks = [\n    FastFeatureAttack,\n]\n\nbatch_consistent_attacks = [\n    GradientSignAttack,\n    LinfBasicIterativeAttack,\n    GradientAttack,\n    L2BasicIterativeAttack,\n    LinfPGDAttack,\n    MomentumIterativeAttack,\n    FastFeatureAttack,\n    JacobianSaliencyMapAttack,\n    DDNL2Attack,\n    SparseL1DescentAttack,\n    L1PGDAttack,\n    LinfSPSAAttack,\n    # FABAttack,\n    # CarliniWagnerL2Attack,  # XXX: not exactly sure: test says no\n    # LBFGSAttack,  # XXX: not exactly sure: test says no\n    # SpatialTransformAttack,  # XXX: not exactly sure: test says no\n]\n\n\ntargeted_only_attacks = [\n    JacobianSaliencyMapAttack,\n]\n\n\n# ###########################################################\n# helper functions\n\n\ndef merge2dicts(x, y):\n    z = x.copy()\n    z.update(y)\n    return z\n'"
advertorch/utils.py,42,"b'# Copyright (c) 2018-present, Royal Bank of Canada and other authors.\n# See the AUTHORS.txt file for a list of contributors.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef torch_allclose(x, y, rtol=1.e-5, atol=1.e-8):\n    """"""\n    Wrap on numpy\'s allclose. Input x and y are both tensors of equal shape\n\n    Original numpy documentation:\n    https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.allclose.html\n\n    Notes:\n    If the following equation is element-wise True, then allclose returns\n    True.\n\n     absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))\n\n    :param x: (torch tensor)\n    :param y: (torch tensor)\n    :param rtol: (float) the relative tolerance parameter\n    :param atol: (float) the absolute tolerance parameter\n    :return: (bool) if x and y are all close\n    """"""\n    import numpy as np\n    return np.allclose(x.detach().cpu().numpy(), y.detach().cpu().numpy(),\n                       rtol=rtol, atol=atol)\n\n\ndef single_dim_flip(x, dim):\n    dim = x.dim() + dim if dim < 0 else dim\n    indices = torch.arange(\n        x.size(dim) - 1, -1, -1,\n        dtype=torch.long, device=x.device, requires_grad=x.requires_grad)\n    # TODO: do we need requires_grad???\n    return x.index_select(dim, indices)\n\n\ndef torch_flip(x, dims):\n    for dim in dims:\n        x = single_dim_flip(x, dim)\n    return x\n\n\ndef replicate_input(x):\n    return x.detach().clone()\n\n\ndef replicate_input_withgrad(x):\n    return x.detach().clone().requires_grad_()\n\n\ndef calc_l2distsq(x, y):\n    d = (x - y)**2\n    return d.view(d.shape[0], -1).sum(dim=1)\n\n\ndef calc_l1dist(x, y):\n    d = torch.abs(x - y)\n    return d.view(d.shape[0], -1).sum(dim=1)\n\n\ndef tanh_rescale(x, x_min=-1., x_max=1.):\n    return (torch.tanh(x)) * 0.5 * (x_max - x_min) + (x_max + x_min) * 0.5\n\n\ndef torch_arctanh(x, eps=1e-6):\n    return (torch.log((1 + x) / (1 - x))) * 0.5\n\n\ndef clamp(input, min=None, max=None):\n    ndim = input.ndimension()\n    if min is None:\n        pass\n    elif isinstance(min, (float, int)):\n        input = torch.clamp(input, min=min)\n    elif isinstance(min, torch.Tensor):\n        if min.ndimension() == ndim - 1 and min.shape == input.shape[1:]:\n            input = torch.max(input, min.view(1, *min.shape))\n        else:\n            assert min.shape == input.shape\n            input = torch.max(input, min)\n    else:\n        raise ValueError(""min can only be None | float | torch.Tensor"")\n\n    if max is None:\n        pass\n    elif isinstance(max, (float, int)):\n        input = torch.clamp(input, max=max)\n    elif isinstance(max, torch.Tensor):\n        if max.ndimension() == ndim - 1 and max.shape == input.shape[1:]:\n            input = torch.min(input, max.view(1, *max.shape))\n        else:\n            assert max.shape == input.shape\n            input = torch.min(input, max)\n    else:\n        raise ValueError(""max can only be None | float | torch.Tensor"")\n    return input\n\n\n\n\ndef to_one_hot(y, num_classes=10):\n    """"""\n    Take a batch of label y with n dims and convert it to\n    1-hot representation with n+1 dims.\n    Link: https://discuss.pytorch.org/t/convert-int-into-one-hot-format/507/24\n    """"""\n    y = replicate_input(y).view(-1, 1)\n    y_one_hot = y.new_zeros((y.size()[0], num_classes)).scatter_(1, y, 1)\n    return y_one_hot\n\n\nclass CarliniWagnerLoss(nn.Module):\n    """"""\n    Carlini-Wagner Loss: objective function #6.\n    Paper: https://arxiv.org/pdf/1608.04644.pdf\n    """"""\n\n    def __init__(self):\n        super(CarliniWagnerLoss, self).__init__()\n\n    def forward(self, input, target):\n        """"""\n        :param input: pre-softmax/logits.\n        :param target: true labels.\n        :return: CW loss value.\n        """"""\n        num_classes = input.size(1)\n        label_mask = to_one_hot(target, num_classes=num_classes).float()\n        correct_logit = torch.sum(label_mask * input, dim=1)\n        wrong_logit = torch.max((1. - label_mask) * input, dim=1)[0]\n        loss = -F.relu(correct_logit - wrong_logit + 50.).sum()\n        return loss\n\n\ndef _batch_multiply_tensor_by_vector(vector, batch_tensor):\n    """"""Equivalent to the following\n    for ii in range(len(vector)):\n        batch_tensor.data[ii] *= vector[ii]\n    return batch_tensor\n    """"""\n    return (\n        batch_tensor.transpose(0, -1) * vector).transpose(0, -1).contiguous()\n\n\ndef _batch_clamp_tensor_by_vector(vector, batch_tensor):\n    """"""Equivalent to the following\n    for ii in range(len(vector)):\n        batch_tensor[ii] = clamp(\n            batch_tensor[ii], -vector[ii], vector[ii])\n    """"""\n    return torch.min(\n        torch.max(batch_tensor.transpose(0, -1), -vector), vector\n    ).transpose(0, -1).contiguous()\n\n\ndef batch_multiply(float_or_vector, tensor):\n    if isinstance(float_or_vector, torch.Tensor):\n        assert len(float_or_vector) == len(tensor)\n        tensor = _batch_multiply_tensor_by_vector(float_or_vector, tensor)\n    elif isinstance(float_or_vector, float):\n        tensor *= float_or_vector\n    else:\n        raise TypeError(""Value has to be float or torch.Tensor"")\n    return tensor\n\n\ndef batch_clamp(float_or_vector, tensor):\n    if isinstance(float_or_vector, torch.Tensor):\n        assert len(float_or_vector) == len(tensor)\n        tensor = _batch_clamp_tensor_by_vector(float_or_vector, tensor)\n        return tensor\n    elif isinstance(float_or_vector, float):\n        tensor = clamp(tensor, -float_or_vector, float_or_vector)\n    else:\n        raise TypeError(""Value has to be float or torch.Tensor"")\n    return tensor\n\n\ndef _get_norm_batch(x, p):\n    batch_size = x.size(0)\n    return x.abs().pow(p).view(batch_size, -1).sum(dim=1).pow(1. / p)\n\n\ndef _thresh_by_magnitude(theta, x):\n    return torch.relu(torch.abs(x) - theta) * x.sign()\n\n\ndef batch_l1_proj_flat(x, z=1):\n    """"""\n    Implementation of L1 ball projection from:\n\n    https://stanford.edu/~jduchi/projects/DuchiShSiCh08.pdf\n\n    inspired from:\n\n    https://gist.github.com/daien/1272551/edd95a6154106f8e28209a1c7964623ef8397246\n\n    :param x: input data\n    :param eps: l1 radius\n\n    :return: tensor containing the projection.\n    """"""\n\n    # Computing the l1 norm of v\n    v = torch.abs(x)\n    v = v.sum(dim=1)\n\n    # Getting the elements to project in the batch\n    indexes_b = torch.nonzero(v > z).view(-1)\n    x_b = x[indexes_b]\n    batch_size_b = x_b.size(0)\n\n    # If all elements are in the l1-ball, return x\n    if batch_size_b == 0:\n        return x\n\n    # make the projection on l1 ball for elements outside the ball\n    view = x_b\n    view_size = view.size(1)\n    mu = view.abs().sort(1, descending=True)[0]\n    vv = torch.arange(view_size).float().to(x.device)\n    st = (mu.cumsum(1) - z) / (vv + 1)\n    u = (mu - st) > 0\n    rho = (1 - u).cumsum(dim=1).eq(0).sum(1) - 1\n    theta = st.gather(1, rho.unsqueeze(1))\n    proj_x_b = _thresh_by_magnitude(theta, x_b)\n\n    # gather all the projected batch\n    proj_x = x.detach().clone()\n    proj_x[indexes_b] = proj_x_b\n    return proj_x\n\n\ndef batch_l1_proj(x, eps):\n    batch_size = x.size(0)\n    view = x.view(batch_size, -1)\n    proj_flat = batch_l1_proj_flat(view, z=eps)\n    return proj_flat.view_as(x)\n\n\ndef clamp_by_pnorm(x, p, r):\n    assert isinstance(p, float) or isinstance(p, int)\n    norm = _get_norm_batch(x, p)\n    if isinstance(r, torch.Tensor):\n        assert norm.size() == r.size()\n    else:\n        assert isinstance(r, float)\n    factor = torch.min(r / norm, torch.ones_like(norm))\n    return batch_multiply(factor, x)\n\n\ndef is_float_or_torch_tensor(x):\n    return isinstance(x, torch.Tensor) or isinstance(x, float)\n\n\ndef normalize_by_pnorm(x, p=2, small_constant=1e-6):\n    """"""\n    Normalize gradients for gradient (not gradient sign) attacks.\n    # TODO: move this function to utils\n\n    :param x: tensor containing the gradients on the input.\n    :param p: (optional) order of the norm for the normalization (1 or 2).\n    :param small_constant: (optional float) to avoid dividing by zero.\n    :return: normalized gradients.\n    """"""\n    # loss is averaged over the batch so need to multiply the batch\n    # size to find the actual gradient of each input sample\n\n    assert isinstance(p, float) or isinstance(p, int)\n    norm = _get_norm_batch(x, p)\n    norm = torch.max(norm, torch.ones_like(norm) * small_constant)\n    return batch_multiply(1. / norm, x)\n\n\ndef jacobian(model, x, output_class):\n    """"""\n    Compute the output_class\'th row of a Jacobian matrix. In other words,\n    compute the gradient wrt to the output_class.\n\n    :param model: forward pass function.\n    :param x: input tensor.\n    :param output_class: the output class we want to compute the gradients.\n    :return: output_class\'th row of the Jacobian matrix wrt x.\n    """"""\n    xvar = replicate_input_withgrad(x)\n    scores = model(xvar)\n\n    # compute gradients for the class output_class wrt the input x\n    # using backpropagation\n    torch.sum(scores[:, output_class]).backward()\n\n    return xvar.grad.detach().clone()\n\n\nMNIST_MEAN = (0.1307,)\nMNIST_STD = (0.3081,)\n\nCIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\nCIFAR10_STD = (0.2023, 0.1994, 0.2010)\n\n\nclass NormalizeByChannelMeanStd(nn.Module):\n    def __init__(self, mean, std):\n        super(NormalizeByChannelMeanStd, self).__init__()\n        if not isinstance(mean, torch.Tensor):\n            mean = torch.tensor(mean)\n        if not isinstance(std, torch.Tensor):\n            std = torch.tensor(std)\n        self.register_buffer(""mean"", mean)\n        self.register_buffer(""std"", std)\n\n    def forward(self, tensor):\n        return normalize_fn(tensor, self.mean, self.std)\n\n    def extra_repr(self):\n        return \'mean={}, std={}\'.format(self.mean, self.std)\n\n\ndef normalize_fn(tensor, mean, std):\n    """"""Differentiable version of torchvision.functional.normalize""""""\n    # here we assume the color channel is in at dim=1\n    mean = mean[None, :, None, None]\n    std = std[None, :, None, None]\n    return tensor.sub(mean).div(std)\n\n\ndef batch_per_image_standardization(imgs):\n    # replicate tf.image.per_image_standardization, but in batch\n    assert imgs.ndimension() == 4\n    mean = imgs.view(imgs.shape[0], -1).mean(dim=1).view(\n        imgs.shape[0], 1, 1, 1)\n    return (imgs - mean) / batch_adjusted_stddev(imgs)\n\n\ndef batch_adjusted_stddev(imgs):\n    # for batch_per_image_standardization\n    std = imgs.view(imgs.shape[0], -1).std(dim=1).view(imgs.shape[0], 1, 1, 1)\n    std_min = 1. / imgs.new_tensor(imgs.shape[1:]).prod().float().sqrt()\n    return torch.max(std, std_min)\n\n\nclass PerImageStandardize(nn.Module):\n    def __init__(self):\n        super(PerImageStandardize, self).__init__()\n\n    def forward(self, tensor):\n        return batch_per_image_standardization(tensor)\n\n\ndef predict_from_logits(logits, dim=1):\n    return logits.max(dim=dim, keepdim=False)[1]\n\n\ndef get_accuracy(pred, target):\n    return pred.eq(target).float().mean().item()\n\n\ndef set_torch_deterministic():\n    import torch.backends.cudnn as cudnn\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n\n\ndef set_seed(seed=None):\n    import torch\n    import numpy as np\n    import random\n    if seed is not None:\n        torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n'"
advertorch_examples/__init__.py,0,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n'"
advertorch_examples/benchmark_utils.py,3,"b'# Copyright (c) 2018-present, Royal Bank of Canada and other authors.\n# See the AUTHORS.txt file for a list of contributors.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nimport os\nimport sys\n\nimport torch\nimport torchvision\n\nimport advertorch\nfrom advertorch.attacks.utils import multiple_mini_batch_attack\n\n\ndef get_benchmark_sys_info():\n    rval = ""#\\n#\\n""\n    rval += (""# Automatically generated benchmark report ""\n             ""(screen print of running this file)\\n#\\n"")\n    uname = os.uname()\n    rval += ""# sysname: {}\\n"".format(uname.sysname)\n    rval += ""# release: {}\\n"".format(uname.release)\n    rval += ""# version: {}\\n"".format(uname.version)\n    rval += ""# machine: {}\\n"".format(uname.machine)\n    rval += ""# python: {}.{}.{}\\n"".format(\n        sys.version_info.major,\n        sys.version_info.minor,\n        sys.version_info.micro)\n    rval += ""# torch: {}\\n"".format(torch.__version__)\n    rval += ""# torchvision: {}\\n"".format(torchvision.__version__)\n    rval += ""# advertorch: {}\\n"".format(advertorch.__version__)\n    return rval\n\n\ndef _calculate_benchmark_results(\n        model, loader, attack_class, attack_kwargs, norm, device, num_batch):\n    adversary = attack_class(model, **attack_kwargs)\n    label, pred, advpred, dist = multiple_mini_batch_attack(\n        adversary, loader, device=device, norm=norm, num_batch=num_batch)\n    accuracy = 100. * (label == pred).sum().item() / len(label)\n    attack_success_rate = 100. * (label != advpred).sum().item() / len(label)\n    dist = None if dist is None else dist[(label != advpred) & (label == pred)]\n    return len(label), accuracy, attack_success_rate, dist\n\n\ndef _generate_basic_benchmark_str(\n        model, loader, attack_class, attack_kwargs, num, accuracy,\n        attack_success_rate):\n    rval = """"\n    rval += ""# attack type: {}\\n"".format(attack_class.__name__)\n\n    prefix = "" attack kwargs: ""\n    count = 0\n    for key in attack_kwargs:\n        this_prefix = prefix if count == 0 else "" "" * len(prefix)\n        count += 1\n        rval += ""#{}{}={}\\n"".format(this_prefix, key, attack_kwargs[key])\n\n    rval += ""# data: {}, {} samples\\n"".format(loader.name, num)\n    rval += ""# model: {}\\n"".format(model.name)\n    rval += ""# accuracy: {}%\\n"".format(accuracy)\n    rval += ""# attack success rate: {}%\\n"".format(attack_success_rate)\n    return rval\n\n\ndef benchmark_attack_success_rate(\n        model, loader, attack_class, attack_kwargs,\n        device=""cuda"", num_batch=None):\n    num, accuracy, attack_success_rate, _ = _calculate_benchmark_results(\n        model, loader, attack_class, attack_kwargs, None, device, num_batch)\n    rval = _generate_basic_benchmark_str(\n        model, loader, attack_class, attack_kwargs, num, accuracy,\n        attack_success_rate)\n    return rval\n\n\ndef benchmark_margin(\n        model, loader, attack_class, attack_kwargs, norm,\n        device=""cuda"", num_batch=None):\n\n    num, accuracy, attack_success_rate, dist = _calculate_benchmark_results(\n        model, loader, attack_class, attack_kwargs, norm, device, num_batch)\n    rval = _generate_basic_benchmark_str(\n        model, loader, attack_class, attack_kwargs, num, accuracy,\n        attack_success_rate)\n\n    rval += ""# Among successful attacks ({} norm) "".format(norm) + \\\n        ""on correctly classified examples:\\n""\n    rval += ""#    minimum distance: {:.4}\\n"".format(dist.min().item())\n    rval += ""#    median distance: {:.4}\\n"".format(dist.median().item())\n    rval += ""#    maximum distance: {:.4}\\n"".format(dist.max().item())\n    rval += ""#    average distance: {:.4}\\n"".format(dist.mean().item())\n    rval += ""#    distance standard deviation: {:.4}\\n"".format(\n        dist.std().item())\n\n    return rval\n'"
advertorch_examples/instantiate_adversary_from_attackconfig_class.py,4,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nimport torch.nn as nn\n\nfrom advertorch.attacks import LinfPGDAttack\nfrom advertorch.attacks.utils import AttackConfig\n\n\nclass PGDLinfMadryTrainMnist(AttackConfig):\n    AttackClass = LinfPGDAttack\n    eps = 0.3\n    eps_iter = 0.01\n    nb_iter = 40\n    loss_fn = nn.CrossEntropyLoss(reduction=""sum"")\n    rand_init = True\n    clip_min = 0.0\n    clip_max = 1.0\n\n\nclass PGDLinfMadryTestMnist(PGDLinfMadryTrainMnist):\n    # only modify the entry that is changed from PGDLinfMadryTrainMnist\n    nb_iter = 100\n\n\nif __name__ == \'__main__\':\n    from advertorch.test_utils import LeNet5\n\n    model = LeNet5()\n\n    train_adversary = PGDLinfMadryTrainMnist()(model)\n    test_adversary = PGDLinfMadryTestMnist()(model)\n'"
advertorch_examples/models.py,4,"b'import math\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass LeNet5Madry(nn.Module):\n    # model replicated from\n    #   https://github.com/MadryLab/mnist_challenge/blob/\n    #   2527d24c4c34e511a12b8a9d7cf6b949aae6fc1b/model.py\n    # TODO: combine with the model in advertorch.test_utils\n\n    def __init__(\n            self, nb_filters=(1, 32, 64), kernel_sizes=(5, 5),\n            paddings=(2, 2), strides=(1, 1), pool_sizes=(2, 2),\n            nb_hiddens=(7 * 7 * 64, 1024), nb_classes=10):\n        super(LeNet5Madry, self).__init__()\n        self.conv1 = nn.Conv2d(\n            nb_filters[0], nb_filters[1], kernel_size=kernel_sizes[0],\n            padding=paddings[0], stride=strides[0])\n        self.relu1 = nn.ReLU(inplace=True)\n        self.maxpool1 = nn.MaxPool2d(pool_sizes[0])\n        self.conv2 = nn.Conv2d(\n            nb_filters[1], nb_filters[2], kernel_size=kernel_sizes[1],\n            padding=paddings[0], stride=strides[0])\n        self.relu2 = nn.ReLU(inplace=True)\n        self.maxpool2 = nn.MaxPool2d(pool_sizes[1])\n        self.linear1 = nn.Linear(nb_hiddens[0], nb_hiddens[1])\n        self.relu3 = nn.ReLU(inplace=True)\n        self.linear2 = nn.Linear(nb_hiddens[1], nb_classes)\n\n    def forward(self, x):\n        out = self.maxpool1(self.relu1(self.conv1(x)))\n        out = self.maxpool2(self.relu2(self.conv2(out)))\n        out = out.view(out.size(0), -1)\n        out = self.relu3(self.linear1(out))\n        out = self.linear2(out)\n        return out\n\n\ndef get_lenet5madry_with_width(widen_factor):\n    return LeNet5Madry(\n        nb_filters=(1, int(widen_factor * 32), int(widen_factor * 64)),\n        nb_hiddens=(7 * 7 * int(widen_factor * 64), int(widen_factor * 1024)))\n\n\n# WideResNet related code adapted from\n#   https://github.com/xternalz/WideResNet-pytorch/blob/\n#   ae12d25bdf273010bd4a54971948a6c796cb95ed/wideresnet.py\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, drop_rate=0.0):\n        super(BasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(\n            in_planes, out_planes, kernel_size=3, stride=stride,\n            padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(\n            out_planes, out_planes, kernel_size=3, stride=1,\n            padding=1, bias=False)\n        self.drop_rate = drop_rate\n        self.in_out_equal = (in_planes == out_planes)\n\n        if not self.in_out_equal:\n            self.conv_shortcut = nn.Conv2d(\n                in_planes, out_planes, kernel_size=1, stride=stride,\n                padding=0, bias=False)\n\n    def forward(self, x):\n        out = self.relu1(self.bn1(x))\n        if not self.in_out_equal:\n            x = self.conv_shortcut(out)\n        out = self.relu2(self.bn2(self.conv1(out)))\n        if self.drop_rate > 0:\n            out = F.dropout(out, p=self.drop_rate, training=self.training)\n        out = self.conv2(out)\n        out += x\n        return out\n\n\nclass ConvGroup(nn.Module):\n    def __init__(\n            self, num_blocks, in_planes, out_planes, block, stride,\n            drop_rate=0.0):\n        super(ConvGroup, self).__init__()\n        self.layer = self._make_layer(\n            block, in_planes, out_planes, num_blocks, stride, drop_rate)\n\n    def _make_layer(\n            self, block, in_planes, out_planes, num_blocks, stride, drop_rate):\n        layers = []\n        for i in range(int(num_blocks)):\n            layers.append(\n                block(in_planes=in_planes if i == 0 else out_planes,\n                      out_planes=out_planes,\n                      stride=stride if i == 0 else 1,\n                      drop_rate=drop_rate)\n            )\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layer(x)\n\n\nclass WideResNet(nn.Module):\n    def __init__(self, depth, num_classes, widen_factor=1, drop_rate=0.0,\n                 color_channels=3, block=BasicBlock):\n        super(WideResNet, self).__init__()\n        num_channels = [\n            16, int(16 * widen_factor),\n            int(32 * widen_factor), int(64 * widen_factor)]\n        assert((depth - 4) % 6 == 0)\n        num_blocks = (depth - 4) / 6\n\n        self.conv1 = nn.Conv2d(\n            color_channels, num_channels[0], kernel_size=3, stride=1,\n            padding=1, bias=False)\n        self.convgroup1 = ConvGroup(\n            num_blocks, num_channels[0], num_channels[1], block, 1, drop_rate)\n        self.convgroup2 = ConvGroup(\n            num_blocks, num_channels[1], num_channels[2], block, 2, drop_rate)\n        self.convgroup3 = ConvGroup(\n            num_blocks, num_channels[2], num_channels[3], block, 2, drop_rate)\n        # global average pooling and classifier\n        self.bn1 = nn.BatchNorm2d(num_channels[3])\n        self.relu = nn.ReLU(inplace=True)\n        self.fc = nn.Linear(num_channels[3], num_classes)\n        self.num_channels = num_channels[3]\n\n        for mod in self.modules():\n            if isinstance(mod, nn.Conv2d):\n                n = mod.kernel_size[0] * mod.kernel_size[1] * mod.out_channels\n                mod.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(mod, nn.BatchNorm2d):\n                mod.weight.data.fill_(1)\n                mod.bias.data.zero_()\n            elif isinstance(mod, nn.Linear):\n                mod.bias.data.zero_()\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.convgroup1(out)\n        out = self.convgroup2(out)\n        out = self.convgroup3(out)\n        out = self.relu(self.bn1(out))\n        out = out.mean(dim=-1).mean(dim=-1)\n        out = self.fc(out)\n        return out\n\n\ndef get_cifar10_wrn28_widen_factor(widen_factor):\n    from advertorch.utils import PerImageStandardize\n    model = WideResNet(28, 10, widen_factor)\n    model = nn.Sequential(PerImageStandardize(), model)\n    return model\n'"
advertorch_examples/tutorial_train_mnist.py,12,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom __future__ import print_function\n\nimport os\nimport argparse\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom advertorch.context import ctx_noparamgrad_and_eval\nfrom advertorch.test_utils import LeNet5\nfrom advertorch_examples.utils import get_mnist_train_loader\nfrom advertorch_examples.utils import get_mnist_test_loader\nfrom advertorch_examples.utils import TRAINED_MODEL_PATH\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Train MNIST\')\n    parser.add_argument(\'--seed\', default=0, type=int)\n    parser.add_argument(\'--mode\', default=""cln"", help=""cln | adv"")\n    parser.add_argument(\'--train_batch_size\', default=50, type=int)\n    parser.add_argument(\'--test_batch_size\', default=1000, type=int)\n    parser.add_argument(\'--log_interval\', default=200, type=int)\n    args = parser.parse_args()\n\n    torch.manual_seed(args.seed)\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(""cuda"" if use_cuda else ""cpu"")\n    if args.mode == ""cln"":\n        flag_advtrain = False\n        nb_epoch = 10\n        model_filename = ""mnist_lenet5_clntrained.pt""\n    elif args.mode == ""adv"":\n        flag_advtrain = True\n        nb_epoch = 90\n        model_filename = ""mnist_lenet5_advtrained.pt""\n    else:\n        raise\n\n    train_loader = get_mnist_train_loader(\n        batch_size=args.train_batch_size, shuffle=True)\n    test_loader = get_mnist_test_loader(\n        batch_size=args.test_batch_size, shuffle=False)\n\n    model = LeNet5()\n    model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n    if flag_advtrain:\n        from advertorch.attacks import LinfPGDAttack\n        adversary = LinfPGDAttack(\n            model, loss_fn=nn.CrossEntropyLoss(reduction=""sum""), eps=0.3,\n            nb_iter=40, eps_iter=0.01, rand_init=True, clip_min=0.0,\n            clip_max=1.0, targeted=False)\n\n    for epoch in range(nb_epoch):\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device)\n            ori = data\n            if flag_advtrain:\n                # when performing attack, the model needs to be in eval mode\n                # also the parameters should NOT be accumulating gradients\n                with ctx_noparamgrad_and_eval(model):\n                    data = adversary.perturb(data, target)\n\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.cross_entropy(\n                output, target, reduction=\'elementwise_mean\')\n            loss.backward()\n            optimizer.step()\n            if batch_idx % args.log_interval == 0:\n                print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                    epoch, batch_idx *\n                    len(data), len(train_loader.dataset),\n                    100. * batch_idx / len(train_loader), loss.item()))\n\n        model.eval()\n        test_clnloss = 0\n        clncorrect = 0\n\n        if flag_advtrain:\n            test_advloss = 0\n            advcorrect = 0\n\n        for clndata, target in test_loader:\n            clndata, target = clndata.to(device), target.to(device)\n            with torch.no_grad():\n                output = model(clndata)\n            test_clnloss += F.cross_entropy(\n                output, target, reduction=\'sum\').item()\n            pred = output.max(1, keepdim=True)[1]\n            clncorrect += pred.eq(target.view_as(pred)).sum().item()\n\n            if flag_advtrain:\n                advdata = adversary.perturb(clndata, target)\n                with torch.no_grad():\n                    output = model(advdata)\n                test_advloss += F.cross_entropy(\n                    output, target, reduction=\'sum\').item()\n                pred = output.max(1, keepdim=True)[1]\n                advcorrect += pred.eq(target.view_as(pred)).sum().item()\n\n        test_clnloss /= len(test_loader.dataset)\n        print(\'\\nTest set: avg cln loss: {:.4f},\'\n              \' cln acc: {}/{} ({:.0f}%)\\n\'.format(\n                  test_clnloss, clncorrect, len(test_loader.dataset),\n                  100. * clncorrect / len(test_loader.dataset)))\n        if flag_advtrain:\n            test_advloss /= len(test_loader.dataset)\n            print(\'Test set: avg adv loss: {:.4f},\'\n                  \' adv acc: {}/{} ({:.0f}%)\\n\'.format(\n                      test_advloss, advcorrect, len(test_loader.dataset),\n                      100. * advcorrect / len(test_loader.dataset)))\n\n    torch.save(\n        model.state_dict(),\n        os.path.join(TRAINED_MODEL_PATH, model_filename))\n'"
advertorch_examples/utils.py,11,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport os\nimport sys\nimport pathlib\nfrom torch.utils.data.dataset import Subset\n\nimport numpy as np\nimport torch\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nfrom advertorch.test_utils import LeNet5\n\n# TODO: need to refactor path to keep a single copy of file\n\nROOT_PATH = os.path.expanduser(""~/.advertorch"")\nDATA_PATH = os.path.join(ROOT_PATH, ""data"")\n\npath_of_this_module = os.path.dirname(sys.modules[__name__].__file__)\nTRAINED_MODEL_PATH = os.path.join(path_of_this_module, ""trained_models"")\n\n\ndef mkdir(directory):\n    pathlib.Path(directory).mkdir(parents=True, exist_ok=True)\n\n\ndef get_mnist_train_loader(batch_size, shuffle=True):\n    loader = torch.utils.data.DataLoader(\n        datasets.MNIST(DATA_PATH, train=True, download=True,\n                       transform=transforms.ToTensor()),\n        batch_size=batch_size, shuffle=shuffle)\n    loader.name = ""mnist_train""\n    return loader\n\n\ndef get_mnist_test_loader(batch_size, shuffle=False):\n    loader = torch.utils.data.DataLoader(\n        datasets.MNIST(DATA_PATH, train=False, download=True,\n                       transform=transforms.ToTensor()),\n        batch_size=batch_size, shuffle=shuffle)\n    loader.name = ""mnist_test""\n    return loader\n\n\ndef get_cifar10_train_loader(batch_size, shuffle=True):\n    loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(DATA_PATH, train=True, download=True,\n                         transform=transforms.ToTensor()),\n        batch_size=batch_size, shuffle=shuffle)\n    loader.name = ""cifar10_train""\n    return loader\n\n\ndef get_cifar10_test_loader(batch_size, shuffle=False):\n    loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(DATA_PATH, train=False, download=True,\n                         transform=transforms.ToTensor()),\n        batch_size=batch_size, shuffle=shuffle)\n    loader.name = ""cifar10_test""\n    return loader\n\n\ndef get_mnist_lenet5_clntrained():\n    filename = ""mnist_lenet5_clntrained.pt""\n    model = LeNet5()\n    model.load_state_dict(\n        torch.load(os.path.join(TRAINED_MODEL_PATH, filename)))\n    model.eval()\n    model.name = ""MNIST LeNet5 standard training""\n    # TODO: also described where can you find this model, and how is it trained\n    return model\n\n\ndef get_mnist_lenet5_advtrained():\n    filename = ""mnist_lenet5_advtrained.pt""\n    model = LeNet5()\n    model.load_state_dict(\n        torch.load(os.path.join(TRAINED_MODEL_PATH, filename)))\n    model.eval()\n    model.name = ""MNIST LeNet 5 PGD training according to Madry et al. 2018""\n    # TODO: also described where can you find this model, and how is it trained\n    return model\n\n\ndef get_madry_et_al_cifar10_train_transform():\n    return transforms.Compose([\n        transforms.Pad(4, padding_mode=""reflect""),\n        transforms.RandomCrop(32),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n    ])\n\n\n\ndef get_train_val_loaders(\n        dataset, datapath=DATA_PATH,\n        train_size=None, val_size=5000,\n        train_batch_size=100, val_batch_size=1000,\n        kwargs=None, train_transform=None, val_transform=None,\n        train_shuffle=True, val_shuffle=False):\n    """"""Support MNIST and CIFAR10""""""\n    if kwargs is None:\n        kwargs = {}\n    if train_transform is None:\n        train_transform = transforms.ToTensor()\n    if val_transform is None:\n        val_transform = transforms.ToTensor()\n\n    datapath = os.path.join(datapath, dataset)\n\n    trainset = datasets.__dict__[dataset](\n        datapath, train=True, download=True, transform=train_transform)\n\n    if train_size is not None:\n        assert train_size + val_size <= len(trainset)\n\n    if val_size > 0:\n        indices = list(range(len(trainset)))\n        trainset = Subset(trainset, indices[val_size:])\n\n        valset = datasets.__dict__[dataset](\n            datapath, train=True, download=True, transform=val_transform)\n        valset = Subset(valset, indices[:val_size])\n        val_loader = torch.utils.data.DataLoader(\n            valset, batch_size=val_batch_size, shuffle=val_shuffle, **kwargs)\n\n    else:\n        val_loader = None\n\n    if train_size is not None:\n        trainset = Subset(trainset, list(range(train_size)))\n\n    train_loader = torch.utils.data.DataLoader(\n        trainset, batch_size=train_batch_size, shuffle=train_shuffle, **kwargs)\n\n    return train_loader, val_loader\n\n\ndef get_test_loader(\n        dataset, datapath=DATA_PATH, test_size=None, batch_size=1000,\n        transform=None, kwargs=None, shuffle=False):\n    """"""Support MNIST and CIFAR10""""""\n    if kwargs is None:\n        kwargs = {}\n    if transform is None:\n        transform = transforms.ToTensor()\n\n    datapath = os.path.join(datapath, dataset)\n\n    testset = datasets.__dict__[dataset](\n        datapath, train=False, download=True, transform=transform)\n\n    if test_size is not None:\n        testset = Subset(testset, list(range(test_size)))\n\n    test_loader = torch.utils.data.DataLoader(\n        testset, batch_size=batch_size, shuffle=shuffle, **kwargs)\n    return test_loader\n\n\ndef bchw2bhwc(x):\n    if isinstance(x, np.ndarray):\n        pass\n    else:\n        raise\n\n    if x.ndim == 3:\n        return np.moveaxis(x, 0, 2)\n    if x.ndim == 4:\n        return np.moveaxis(x, 1, 3)\n\n\ndef bhwc2bchw(x):\n    if isinstance(x, np.ndarray):\n        pass\n    else:\n        raise\n\n    if x.ndim == 3:\n        return np.moveaxis(x, 2, 0)\n    if x.ndim == 4:\n        return np.moveaxis(x, 3, 1)\n\n\ndef _imshow(img):\n    import matplotlib.pyplot as plt\n    img = bchw2bhwc(img.detach().cpu().numpy())\n    if img.shape[2] == 1:\n        img = np.repeat(img, 3, axis=2)\n    plt.imshow(img, vmin=0, vmax=1)\n    plt.axis(""off"")\n\n\nclass ImageNetClassNameLookup(object):\n\n    def _load_list(self):\n        import json\n        with open(self.json_path) as f:\n            class_idx = json.load(f)\n        self.label2classname = [\n            class_idx[str(k)][1] for k in range(len(class_idx))]\n\n    def __init__(self):\n        self.json_url = (""https://s3.amazonaws.com/deep-learning-models/""\n                         ""image-models/imagenet_class_index.json"")\n        self.json_path = os.path.join(DATA_PATH, ""imagenet_class_index.json"")\n        if os.path.exists(self.json_path):\n            self._load_list()\n        else:\n            import urllib\n            urllib.request.urlretrieve(self.json_url, self.json_path)\n            self._load_list()\n\n\n    def __call__(self, label):\n        return self.label2classname[label]\n\n\ndef get_panda_image():\n    img_path = os.path.join(DATA_PATH, ""panda.jpg"")\n    img_url = ""https://farm1.static.flickr.com/230/524562325_fb0a11d1e1.jpg""\n\n    def _load_panda_image():\n        from skimage.io import imread\n        return imread(img_path) / 255.\n\n    if os.path.exists(img_path):\n        return _load_panda_image()\n    else:\n        import urllib\n        urllib.request.urlretrieve(img_url, img_path)\n        return _load_panda_image()\n\n\nmkdir(ROOT_PATH)\nmkdir(DATA_PATH)\n'"
docs/conf.py,23,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\nimport os\nimport shutil\nif os.path.exists(""_tutorials""):\n    shutil.rmtree(""_tutorials"")\nos.makedirs(""_tutorials"")\nos.symlink(\n    ""../../advertorch_examples/tutorial_attack_defense_bpda_mnist.ipynb"",\n    ""_tutorials/tutorial_attack_defense_bpda_mnist.ipynb"")\nimport sys  # noqa: F401\nsys.path.insert(0, os.path.abspath(\'..\'))\n\n\n# autodoc_mock_imports = [\n# \'numpy\',\n# \'numpy.linalg\',\n# \'scipy\',\n# \'scipy.optimize\',\n# \'scipy.interpolate\',\n# \'scipy.ndimage\',\n# \'scipy.ndimage.filters\',\n# \'tensorflow\',\n# \'theano\',\n# \'theano.tensor\',\n# # \'torch\',\n# \'torch.nn\',\n# \'torch.nn.functional\',\n# \'torch.optim\',\n# \'torch.nn.modules\',\n# \'torch.nn.modules.utils\',\n# \'torch.utils\',\n# \'torch.utils.model_zoo\',\n# \'torch.nn.init\',\n# \'torch.utils.data\',\n# \'randomstate\',\n# \'scipy._lib\',\n# ]\n\nfrom unittest.mock import Mock  # noqa: F401\n# from sphinx.ext.autodoc.importer import _MockObject as Mock\nMock.Module = object\nsys.modules[\'torch\'] = Mock()\nsys.modules[\'numpy\'] = Mock()\nsys.modules[\'numpy.linalg\'] = Mock()\nsys.modules[\'scipy\'] = Mock()\nsys.modules[\'scipy.optimize\'] = Mock()\nsys.modules[\'scipy.interpolate\'] = Mock()\nsys.modules[\'scipy.ndimage\'] = Mock()\nsys.modules[\'scipy.ndimage.filters\'] = Mock()\nsys.modules[\'tensorflow\'] = Mock()\nsys.modules[\'theano\'] = Mock()\nsys.modules[\'theano.tensor\'] = Mock()\nsys.modules[\'torch\'] = Mock()\nsys.modules[\'torch.autograd\'] = Mock()\nsys.modules[\'torch.autograd.gradcheck\'] = Mock()\nsys.modules[\'torch.distributions\'] = Mock()\nsys.modules[\'torch.nn\'] = Mock()\nsys.modules[\'torch.nn.functional\'] = Mock()\nsys.modules[\'torch.optim\'] = Mock()\nsys.modules[\'torch.nn.modules\'] = Mock()\nsys.modules[\'torch.nn.modules.utils\'] = Mock()\nsys.modules[\'torch.nn.modules.loss\'] = Mock()\nsys.modules[\'torch.utils\'] = Mock()\nsys.modules[\'torch.utils.model_zoo\'] = Mock()\nsys.modules[\'torch.nn.init\'] = Mock()\nsys.modules[\'torch.utils.data\'] = Mock()\nsys.modules[\'torchvision\'] = Mock()\nsys.modules[\'randomstate\'] = Mock()\nsys.modules[\'scipy._lib\'] = Mock()\n\n# XXX: This import has to be after mock\nimport advertorch  # noqa: F401\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'advertorch\'\ncopyright = \'2018-present, Royal Bank of Canada.\'\nauthor = \'\'\n\n# The short X.Y version\nversion = \'\'\n# The full version, including alpha/beta/rc tags\nrelease = \'\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.linkcode\',\n    \'nbsphinx\',\n    \'numpydoc\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\nnumpydoc_show_class_members = False\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\ntodo_include_todos = False\n\n\n# Resolve function for the linkcode extension.\ndef linkcode_resolve(domain, info):\n    def find_source():\n        # try to find the file and line number, based on code from numpy:\n        # https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L286\n        obj = sys.modules[info[\'module\']]\n        for part in info[\'fullname\'].split(\'.\'):\n            obj = getattr(obj, part)\n        import inspect\n        import os\n        fn = inspect.getsourcefile(obj)\n        fn = os.path.relpath(fn, start=os.path.dirname(advertorch.__file__))\n        source, lineno = inspect.getsourcelines(obj)\n        return fn, lineno, lineno + len(source) - 1\n\n    if domain != \'py\' or not info[\'module\']:\n        return None\n    try:\n        filename = \'advertorch/%s#L%d-L%d\' % find_source()\n    except Exception:\n        filename = info[\'module\'].replace(\'.\', \'/\') + \'.py\'\n    tag = \'master\'\n    url = ""https://github.com/BorealisAI/advertorch/blob/%s/%s""\n    return url % (tag, filename)\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\n# html_theme = \'alabaster\'\n\nif os.environ.get(\'READTHEDOCS\') != \'True\':\n    try:\n        import sphinx_rtd_theme\n    except ImportError:\n        pass  # assume we have sphinx >= 1.3\n    else:\n        html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n    html_theme = \'sphinx_rtd_theme\'\n\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\n# html_theme = \'alabaster\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\n# html_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'advertorch_testdoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\n# latex_documents = [\n#     (master_doc, \'advertorch_test.tex\', \'advertorch\\\\_test Documentation\',\n#      \'tracy\', \'manual\'),\n# ]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\n# man_pages = [\n#     (master_doc, \'advertorch_test\', \'advertorch_test Documentation\',\n#      [author], 1)\n# ]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\n# texinfo_documents = [\n#     (master_doc, \'advertorch_test\', \'advertorch_test Documentation\',\n#      author, \'advertorch_test\', \'One line description of project.\',\n#      \'Miscellaneous\'),\n# ]\n\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for intersphinx extension ---------------------------------------\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\'https://docs.python.org/\': None}\n\n# -- Options for todo extension ----------------------------------------------\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n'"
external_tests/test_attacks_on_cleverhans.py,23,"b'# Copyright (c) 2018-present, Royal Bank of Canada and other authors.\n# See the AUTHORS.txt file for a list of contributors.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport warnings\nimport pytest\nimport random\n\nimport numpy as np\nimport torch\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data\nimport tensorflow as tf\n\nfrom cleverhans.attacks import CarliniWagnerL2\nfrom cleverhans.attacks import ElasticNetMethod\nfrom cleverhans.attacks import FastGradientMethod\nfrom cleverhans.attacks import MomentumIterativeMethod\nfrom cleverhans.attacks import MadryEtAl\nfrom cleverhans.attacks import FastFeatureAdversaries\nfrom cleverhans.attacks import BasicIterativeMethod\nfrom cleverhans.attacks import LBFGS\nfrom cleverhans.attacks import SaliencyMapMethod\nfrom cleverhans.model import Model as ClModel\n\nfrom advertorch.attacks import CarliniWagnerL2Attack\nfrom advertorch.attacks import ElasticNetL1Attack\nfrom advertorch.attacks import GradientAttack\nfrom advertorch.attacks import GradientSignAttack\nfrom advertorch.attacks import L2MomentumIterativeAttack\nfrom advertorch.attacks import LinfMomentumIterativeAttack\nfrom advertorch.attacks import LinfPGDAttack\nfrom advertorch.attacks import FastFeatureAttack\nfrom advertorch.attacks import LinfBasicIterativeAttack\nfrom advertorch.attacks import L2BasicIterativeAttack\nfrom advertorch.attacks import LBFGSAttack\nfrom advertorch.attacks import JacobianSaliencyMapAttack\nfrom advertorch.test_utils import SimpleModel\nfrom advertorch.test_utils import merge2dicts\n\n\nBATCH_SIZE = 9\nDIM_INPUT = 15\nNUM_CLASS = 5\nEPS = 0.08\n\nATOL = 1e-4\nRTOL = 1e-4\nNB_ITER = 5\n\n# XXX: carlini still doesn\'t pass sometimes under certain random seed\nseed = 66666\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\ntf.set_random_seed(seed)\ninputs = np.random.uniform(0, 1, size=(BATCH_SIZE, DIM_INPUT))\ntargets = np.random.randint(0, NUM_CLASS, size=BATCH_SIZE)\n\n\ntargets_onehot = np.zeros((BATCH_SIZE, NUM_CLASS), dtype=\'int\')\ntargets_onehot[np.arange(BATCH_SIZE), targets] = 1\n\n\nclass SimpleModelTf(ClModel):\n\n    def __init__(self, dim_input, num_classes, session=None):\n        import keras\n        self.sess = session\n        model = keras.models.Sequential()\n        model.add(keras.layers.Dense(10, input_shape=(dim_input, )))\n        model.add(keras.layers.Activation(\'relu\'))\n        model.add(keras.layers.Dense(num_classes))\n        self.model = model\n        self.flag_weight_set = False\n\n    def set_weights(self, weights):\n        self.model.set_weights(weights)\n        self.flag_weight_set = True\n\n    def load_state_dict(self, w):\n        self.set_weights([\n            w[\'fc1.weight\'].cpu().numpy().transpose(),\n            w[\'fc1.bias\'].cpu().numpy(),\n            w[\'fc2.weight\'].cpu().numpy().transpose(),\n            w[\'fc2.bias\'].cpu().numpy(),\n        ])\n\n    def get_logits(self, data):\n        assert self.flag_weight_set, ""Weight Not Set!!!""\n        return self.model(data)\n\n    def get_probs(self, data):\n        assert self.flag_weight_set, ""Weight Not Set!!!""\n        return tf.nn.softmax(logits=self.model(data))\n\n\ndef load_weights_pt(model_pt, layers):\n    w = model_pt.state_dict()\n    layers[0].W = tf.Variable(tf.convert_to_tensor(\n        w[\'fc1.weight\'].cpu().numpy().transpose(), tf.float32))\n    layers[0].b = tf.Variable(tf.convert_to_tensor(\n        w[\'fc1.bias\'].cpu().numpy(), tf.float32))\n    layers[2].W = tf.Variable(tf.convert_to_tensor(\n        w[\'fc2.weight\'].cpu().numpy().transpose(), tf.float32))\n    layers[2].b = tf.Variable(tf.convert_to_tensor(\n        w[\'fc2.bias\'].cpu().numpy(), tf.float32))\n\n\ndef setup_simple_model_tf(model_pt, input_shape):\n    from cleverhans_tutorials.tutorial_models import MLP, Linear, ReLU\n    layers = [Linear(10),\n              ReLU(),\n              Linear(10)]\n    layers[0].name = \'fc1\'\n    layers[1].name = \'relu\'\n    layers[2].name = \'fc2\'\n    model = MLP(layers, input_shape)\n    load_weights_pt(model_pt, layers)\n    return model\n\n\n\n# kwargs for attacks to be tested\nattack_kwargs = {\n    GradientSignAttack: {\n        ""cl_class"": FastGradientMethod,\n        ""kwargs"": dict(\n            eps=EPS,\n            clip_min=0.0,\n            clip_max=1.0,\n        ),\n        ""at_kwargs"": dict(\n        ),\n        ""cl_kwargs"": dict(\n            ord=np.inf,\n        ),\n        ""thresholds"": dict(\n            atol=ATOL,\n            rtol=RTOL,\n        ),\n    },\n    GradientAttack: {\n        ""cl_class"": FastGradientMethod,\n        ""kwargs"": dict(\n            eps=EPS,\n            clip_min=0.0,\n            clip_max=1.0,\n        ),\n        ""at_kwargs"": dict(\n        ),\n        ""cl_kwargs"": dict(\n            ord=2,\n        ),\n        ""thresholds"": dict(\n            atol=ATOL,\n            rtol=RTOL,\n        ),\n    },\n    LinfPGDAttack: {\n        ""cl_class"": MadryEtAl,\n        ""kwargs"": dict(\n            eps=EPS,\n            eps_iter=0.01,\n            clip_min=0.0,\n            clip_max=1.0,\n            rand_init=False,\n            nb_iter=NB_ITER,\n        ),\n        ""at_kwargs"": dict(\n            targeted=True,\n        ),\n        ""cl_kwargs"": dict(\n            ord=np.inf,\n        ),\n        ""thresholds"": dict(\n            atol=ATOL,\n            rtol=RTOL,\n        ),\n    },\n    L2MomentumIterativeAttack: {\n        ""cl_class"": MomentumIterativeMethod,\n        ""kwargs"": dict(\n            eps=EPS,\n            eps_iter=0.01,\n            clip_min=0.0,\n            clip_max=1.0,\n            decay_factor=1.,\n            nb_iter=NB_ITER,\n        ),\n        ""at_kwargs"": dict(\n        ),\n        ""cl_kwargs"": dict(\n            ord=2,\n        ),\n        ""thresholds"": dict(\n            atol=ATOL,\n            rtol=RTOL,\n        ),\n    },\n    LinfMomentumIterativeAttack: {\n        ""cl_class"": MomentumIterativeMethod,\n        ""kwargs"": dict(\n            eps=EPS,\n            eps_iter=0.01,\n            clip_min=0.0,\n            clip_max=1.0,\n            decay_factor=1.,\n            nb_iter=NB_ITER,\n        ),\n        ""at_kwargs"": dict(\n        ),\n        ""cl_kwargs"": dict(\n            ord=np.inf,\n        ),\n        ""thresholds"": dict(\n            atol=ATOL,\n            rtol=RTOL,\n        ),\n    },\n    CarliniWagnerL2Attack: {\n        ""cl_class"": CarliniWagnerL2,\n        ""kwargs"": dict(\n            max_iterations=100,\n            clip_min=0,\n            clip_max=1,\n            binary_search_steps=9,\n            learning_rate=0.1,\n            confidence=0.1,\n        ),\n        ""at_kwargs"": dict(\n            num_classes=NUM_CLASS,\n        ),\n        ""cl_kwargs"": dict(\n            batch_size=BATCH_SIZE,\n            initial_const=1e-3,\n        ),\n        ""thresholds"": dict(\n            atol=ATOL,\n            rtol=RTOL,\n        ),\n    },\n    ElasticNetL1Attack: {\n        ""cl_class"": ElasticNetMethod,\n        ""kwargs"": dict(\n            max_iterations=100,\n            clip_min=0,\n            clip_max=1,\n            binary_search_steps=9,\n            learning_rate=0.1,\n            confidence=0.1,\n        ),\n        ""at_kwargs"": dict(\n            num_classes=NUM_CLASS,\n        ),\n        ""cl_kwargs"": dict(\n            batch_size=BATCH_SIZE,\n            initial_const=1e-3,\n        ),\n        ""thresholds"": dict(\n            atol=ATOL,\n            rtol=RTOL,\n        ),\n    },\n    FastFeatureAttack: {\n        ""cl_class"": FastFeatureAdversaries,\n        ""kwargs"": dict(\n            nb_iter=NB_ITER,\n            clip_min=0,\n            clip_max=1,\n            eps_iter=0.05,\n            eps=0.3,\n        ),\n        ""at_kwargs"": dict(\n        ),\n        ""cl_kwargs"": dict(\n            layer=\'logits\',\n        ),\n        ""thresholds"": dict(\n            atol=ATOL,\n            rtol=RTOL,\n        ),\n    },\n    LinfBasicIterativeAttack: {\n        ""cl_class"": BasicIterativeMethod,\n        ""kwargs"": dict(\n            clip_min=0,\n            clip_max=1,\n            eps_iter=0.05,\n            eps=0.1,\n            nb_iter=NB_ITER,\n        ),\n        ""at_kwargs"": dict(\n        ),\n        ""cl_kwargs"": dict(\n            ord=np.inf,\n        ),\n        ""thresholds"": dict(\n            atol=ATOL,\n            rtol=RTOL,\n        ),\n    },\n    L2BasicIterativeAttack: {\n        ""cl_class"": BasicIterativeMethod,\n        ""kwargs"": dict(\n            clip_min=0,\n            clip_max=1,\n            eps_iter=0.05,\n            eps=0.1,\n            nb_iter=NB_ITER,\n        ),\n        ""at_kwargs"": dict(\n        ),\n        ""cl_kwargs"": dict(\n            ord=2,\n        ),\n        ""thresholds"": dict(\n            atol=ATOL,\n            rtol=RTOL,\n        ),\n    },\n    LBFGSAttack: {\n        ""cl_class"": LBFGS,\n        ""kwargs"": dict(\n            clip_min=0.,\n            clip_max=1.,\n            # set binary search step = 3, which can successfully create\n            # adversarial images and the difference between advertorch\n            # and cleverhans is within the threshold\n            # the difference of the two results are very small at first\n            # because of some rounding and calculating difference\n            # with tensors and numpy arrays, the difference gets larger\n            # with more iterations\n            binary_search_steps=3,\n            max_iterations=50,\n            initial_const=1e-3,\n            batch_size=BATCH_SIZE,\n        ),\n        ""at_kwargs"": dict(\n            num_classes=NUM_CLASS,\n        ),\n        ""cl_kwargs"": dict(\n        ),\n        ""thresholds"": dict(\n            atol=ATOL,\n            rtol=RTOL,\n        ),\n    },\n    JacobianSaliencyMapAttack: {\n        ""cl_class"": SaliencyMapMethod,\n        ""kwargs"": dict(\n            clip_min=0.0,\n            clip_max=1.0,\n            theta=1.0,\n            gamma=1.0,\n        ),\n        ""at_kwargs"": dict(\n            num_classes=NUM_CLASS,\n            comply_cleverhans=True,\n        ),\n        ""cl_kwargs"": dict(\n            # nb_classes=NUM_CLASS,\n        ),\n        ""thresholds"": dict(\n            atol=ATOL,\n            rtol=RTOL,\n        ),\n    },\n}\n\n\ndef overwrite_fastfeature(attack, x, g, eta, **kwargs):\n    # overwrite cleverhans generate function for fastfeatureattack to\n    # allow eta as an input\n    from cleverhans.utils_tf import clip_eta\n\n    # Parse and save attack-specific parameters\n    assert attack.parse_params(**kwargs)\n\n    g_feat = attack.model.get_layer(g, attack.layer)\n\n    # Initialize loop variables\n    eta = tf.Variable(tf.convert_to_tensor(eta, np.float32))\n    eta = clip_eta(eta, attack.ord, attack.eps)\n\n    for i in range(attack.nb_iter):\n        eta = attack.attack_single_step(x, eta, g_feat)\n\n    # Define adversarial example (and clip if necessary)\n    adv_x = x + eta\n    if attack.clip_min is not None and attack.clip_max is not None:\n        adv_x = tf.clip_by_value(adv_x, attack.clip_min, attack.clip_max)\n\n    return adv_x\n\n\ndef genenerate_ptb_pt(adversary, inputs, targets, delta=None):\n    if inputs.ndim == 4:\n        # TODO: move the transpose to a better place\n        input_t = torch.from_numpy(inputs.transpose(0, 3, 1, 2))\n    else:\n        input_t = torch.from_numpy(inputs)\n    input_t = input_t.float()\n\n    if targets is None:\n        adversary.targeted = False\n        adv_pt = adversary.perturb(input_t, None)\n    else:\n        target_t = torch.from_numpy(targets)\n        if isinstance(adversary, FastFeatureAttack):\n            adv_pt = adversary.perturb(input_t, target_t,\n                                       delta=torch.from_numpy(delta))\n        else:\n            adversary.targeted = True\n            adv_pt = adversary.perturb(input_t, target_t)\n\n    adv_pt = adv_pt.cpu().detach().numpy()\n\n    if inputs.ndim == 4:\n        # TODO: move the transpose to a better place\n        adv_pt = adv_pt.transpose(0, 2, 3, 1)\n    return adv_pt - inputs\n\n\ndef compare_at_cl(ptb_at, ptb_cl, atol, rtol):\n    assert np.allclose(ptb_at, ptb_cl, atol=atol, rtol=rtol), \\\n        (np.abs(ptb_at - ptb_cl).max())\n\n\ndef compare_attacks(key, item, targeted=False):\n    AdvertorchAttack = key\n    CleverhansAttack = item[""cl_class""]\n    cl_kwargs = merge2dicts(item[""kwargs""], item[""cl_kwargs""])\n    at_kwargs = merge2dicts(item[""kwargs""], item[""at_kwargs""])\n    thresholds = item[""thresholds""]\n    seed = 6666\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\n    # WARNING: don\'t use tf.InteractiveSession() here\n    # It causes that fastfeature attack has to be the last test for some reason\n    with tf.Session() as sess:\n        model_pt = SimpleModel(DIM_INPUT, NUM_CLASS)\n        model_tf = SimpleModelTf(DIM_INPUT, NUM_CLASS)\n        model_tf.load_state_dict(model_pt.state_dict())\n        adversary = AdvertorchAttack(model_pt, **at_kwargs)\n\n        if AdvertorchAttack is FastFeatureAttack:\n            model_tf_fastfeature = setup_simple_model_tf(\n                model_pt, inputs.shape)\n            delta = np.random.uniform(\n                -item[""kwargs""][\'eps\'], item[""kwargs""][\'eps\'],\n                size=inputs.shape).astype(\'float32\')\n            inputs_guide = np.random.uniform(\n                0, 1, size=(BATCH_SIZE, DIM_INPUT)).astype(\'float32\')\n            inputs_tf = tf.convert_to_tensor(inputs, np.float32)\n            inputs_guide_tf = tf.convert_to_tensor(inputs_guide, np.float32)\n            attack = CleverhansAttack(model_tf_fastfeature)\n            cl_result = overwrite_fastfeature(attack,\n                                              x=inputs_tf,\n                                              g=inputs_guide_tf,\n                                              eta=delta,\n                                              **cl_kwargs)\n            init = tf.global_variables_initializer()\n            sess.run(init)\n            ptb_cl = sess.run(cl_result) - inputs\n            ptb_at = genenerate_ptb_pt(\n                adversary, inputs, inputs_guide, delta=delta)\n\n        else:\n            attack = CleverhansAttack(model_tf, sess=sess)\n            if targeted:\n                with warnings.catch_warnings():\n                    warnings.simplefilter(""ignore"")\n                    ptb_cl = attack.generate_np(\n                        inputs, y_target=targets_onehot, **cl_kwargs) - inputs\n                ptb_at = genenerate_ptb_pt(adversary, inputs, targets=targets)\n            else:\n                with warnings.catch_warnings():\n                    warnings.simplefilter(""ignore"")\n                    ptb_cl = attack.generate_np(\n                        inputs, y=None, **cl_kwargs) - inputs\n                ptb_at = genenerate_ptb_pt(adversary, inputs, targets=None)\n\n        if AdvertorchAttack is CarliniWagnerL2Attack:\n            assert np.sum(np.abs(ptb_at)) > 0 and np.sum(np.abs(ptb_cl)) > 0, \\\n                (""Both advertorch and cleverhans returns zero perturbation""\n                 "" of CarliniWagnerL2Attack, ""\n                 ""the test results are not reliable,""\n                 "" Adjust your testing parameters to avoid this.""\n                 )\n        compare_at_cl(ptb_at, ptb_cl, **thresholds)\n\n\n@pytest.mark.parametrize(""targeted"", [False, True])\ndef test_fgsm_attack(targeted):\n    compare_attacks(\n        GradientSignAttack,\n        attack_kwargs[GradientSignAttack],\n        targeted)\n\n\n@pytest.mark.parametrize(""targeted"", [False, True])\ndef test_fgm_attack(targeted):\n    compare_attacks(\n        GradientAttack,\n        attack_kwargs[GradientAttack],\n        targeted)\n\n\n@pytest.mark.parametrize(""targeted"", [False, True])\ndef test_l2_momentum_iterative_attack(targeted):\n    compare_attacks(\n        L2MomentumIterativeAttack,\n        attack_kwargs[L2MomentumIterativeAttack],\n        targeted)\n\n\n@pytest.mark.parametrize(""targeted"", [False, True])\ndef test_linf_momentum_iterative_attack(targeted):\n    compare_attacks(\n        LinfMomentumIterativeAttack,\n        attack_kwargs[LinfMomentumIterativeAttack],\n        targeted)\n\n\n@pytest.mark.skip(reason=""XXX: temporary"")\ndef test_fastfeature_attack():\n    compare_attacks(\n        FastFeatureAttack,\n        attack_kwargs[FastFeatureAttack])\n\n\n@pytest.mark.parametrize(""targeted"", [False, True])\ndef test_pgd_attack(targeted):\n    compare_attacks(\n        LinfPGDAttack,\n        attack_kwargs[LinfPGDAttack],\n        targeted)\n\n\n@pytest.mark.parametrize(""targeted"", [False, True])\ndef test_iterative_sign_attack(targeted):\n    compare_attacks(\n        LinfBasicIterativeAttack,\n        attack_kwargs[LinfBasicIterativeAttack],\n        targeted)\n\n\n@pytest.mark.parametrize(""targeted"", [False, True])\ndef test_iterative_attack(targeted):\n    compare_attacks(\n        L2BasicIterativeAttack,\n        attack_kwargs[L2BasicIterativeAttack],\n        targeted)\n\n\n@pytest.mark.parametrize(""targeted"", [False, True])\ndef test_carlini_l2_attack(targeted):\n    compare_attacks(\n        CarliniWagnerL2Attack,\n        attack_kwargs[CarliniWagnerL2Attack],\n        targeted)\n\n\n@pytest.mark.parametrize(""targeted"", [False, True])\ndef test_elasticnet_l1_attack(targeted):\n    compare_attacks(\n        ElasticNetL1Attack,\n        attack_kwargs[ElasticNetL1Attack],\n        targeted)\n\n\ndef test_lbfgs_attack():\n    compare_attacks(\n        LBFGSAttack,\n        attack_kwargs[LBFGSAttack],\n        True)\n\n\n@pytest.mark.skip(reason=""XXX: temporary"")\ndef test_jsma():\n    compare_attacks(\n        JacobianSaliencyMapAttack,\n        attack_kwargs[JacobianSaliencyMapAttack],\n        True)\n\n\nif __name__ == \'__main__\':\n    # pass\n    test_iterative_attack(False)\n    test_iterative_attack(True)\n'"
external_tests/test_attacks_on_foolbox.py,8,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport warnings\n\nimport numpy as np\nimport torch\n\nfrom advertorch.attacks import SinglePixelAttack\nfrom advertorch.attacks import LocalSearchAttack\nfrom advertorch.utils import predict_from_logits\nfrom advertorch.test_utils import merge2dicts\nfrom advertorch.test_utils import MLP\n\nfrom advertorch_examples.utils import TRAINED_MODEL_PATH\nfrom advertorch_examples.utils import get_mnist_test_loader\n\nimport foolbox\nfrom foolbox.attacks.localsearch import SinglePixelAttack as SPAfb\nfrom foolbox.attacks.localsearch import LocalSearchAttack as LSAfb\n\n\nNUM_CLASS = 10\nBATCH_SIZE = 10\n# TODO: need to make sure these precisions are enough\nATOL = 1e-4\nRTOL = 1e-4\n\nloader_test = get_mnist_test_loader(BATCH_SIZE)\n\ndata_iter = iter(loader_test)\nimg_batch, label_batch = data_iter.next()\n\n# Setup the test MLP model\nmodel = MLP()\nmodel.eval()\nmodel.load_state_dict(\n    torch.load(os.path.join(TRAINED_MODEL_PATH, \'mlp.pkl\'),\n               map_location=\'cpu\'))\nmodel.to(""cpu"")\n\n# foolbox single pixel attack do not succeed on this model\n#   therefore using mlp.pkl\n# from advertorch.test_utils import LeNet5\n# model = LeNet5()\n# model.eval()\n# model.load_state_dict(\n#     torch.load(os.path.join(TRAINED_MODEL_PATH,\n#                             \'mnist_lenet5_advtrained.pt\')))\n# model.to(""cpu"")\n\n\n\n\nattack_kwargs = {\n    SinglePixelAttack: {\n        ""fb_class"": SPAfb,\n        ""kwargs"": dict(\n            max_pixels=50,\n        ),\n        ""at_kwargs"": dict(\n            clip_min=0.0,\n            clip_max=1.0,\n            comply_with_foolbox=True,\n        ),\n        ""fb_kwargs"": dict(\n            unpack=True,\n        ),\n        ""thresholds"": dict(\n            atol=ATOL,\n            rtol=RTOL,\n        ),\n    },\n    LocalSearchAttack: {\n        ""fb_class"": LSAfb,\n        ""kwargs"": dict(\n            p=1.,\n            r=1.5,\n            d=10,\n            t=100,\n        ),\n        ""at_kwargs"": dict(\n            clip_min=0.0,\n            clip_max=1.0,\n            k=1,\n            round_ub=100,\n            comply_with_foolbox=True,\n        ),\n        ""fb_kwargs"": dict(\n            R=100,\n            unpack=True,\n        ),\n        ""thresholds"": dict(\n            atol=ATOL,\n            rtol=RTOL,\n        ),\n    },\n}\n\n\n\ndef compare_at_fb(ptb_at, ptb_fb, atol, rtol):\n    assert np.allclose(ptb_at, ptb_fb, atol=atol, rtol=rtol), \\\n        (np.abs(ptb_at - ptb_fb).max())\n\n\ndef compare_attacks(key, item):\n    AdvertorchAttack = key\n    fmodel = foolbox.models.PyTorchModel(\n        model, bounds=(0, 1),\n        num_classes=NUM_CLASS,\n        cuda=False,\n    )\n    fb_adversary = item[""fb_class""](fmodel)\n    fb_kwargs = merge2dicts(item[""kwargs""], item[""fb_kwargs""])\n    at_kwargs = merge2dicts(item[""kwargs""], item[""at_kwargs""])\n    thresholds = item[""thresholds""]\n    at_adversary = AdvertorchAttack(model, **at_kwargs)\n    x_at = at_adversary.perturb(img_batch, label_batch)\n    y_logits = model(img_batch)\n    y_at_logits = model(x_at)\n    y_pred = predict_from_logits(y_logits)\n    y_at_pred = predict_from_logits(y_at_logits)\n\n    fb_successed_once = False\n    for i, (x_i, y_i) in enumerate(zip(img_batch, label_batch)):\n        # rule out when classification is wrong or attack is\n        # unsuccessful (we test if foolbox attacks fails here)\n        if y_i != y_pred[i:i + 1][0]:\n            continue\n        if y_i == y_at_pred[i:i + 1][0]:\n            continue\n        np.random.seed(233333)\n        with warnings.catch_warnings():\n            warnings.simplefilter(""ignore"")\n            x_fb = fb_adversary(\n                x_i.cpu().numpy(), label=int(y_i), **fb_kwargs)\n        if x_fb is not None:\n            compare_at_fb(x_at[i].cpu().numpy(), x_fb, **thresholds)\n            fb_successed_once = True\n\n    if not fb_successed_once:\n        raise RuntimeError(\n            ""Foolbox never succeed, change your testing parameters!!!"")\n\n\ndef test_single_pixel():\n    compare_attacks(\n        SinglePixelAttack,\n        attack_kwargs[SinglePixelAttack],\n    )\n\n\ndef test_local_search():\n    compare_attacks(\n        LocalSearchAttack,\n        attack_kwargs[LocalSearchAttack],\n    )\n\n\nif __name__ == \'__main__\':\n    pass\n'"
tests/test_attacks_running.py,41,"b'# Copyright (c) 2018-present, Royal Bank of Canada and other authors.\n# See the AUTHORS.txt file for a list of contributors.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport pytest\nimport itertools\n\nimport torch\nimport torch.nn as nn\n\nfrom advertorch.attacks import GradientSignAttack\nfrom advertorch.attacks import LinfBasicIterativeAttack\nfrom advertorch.attacks import GradientAttack\nfrom advertorch.attacks import L2BasicIterativeAttack\nfrom advertorch.attacks import LinfPGDAttack\nfrom advertorch.attacks import L1PGDAttack\nfrom advertorch.attacks import SparseL1DescentAttack\nfrom advertorch.attacks import MomentumIterativeAttack\nfrom advertorch.attacks import FastFeatureAttack\nfrom advertorch.attacks import CarliniWagnerL2Attack\nfrom advertorch.attacks import DDNL2Attack\nfrom advertorch.attacks import ElasticNetL1Attack\nfrom advertorch.attacks import LBFGSAttack\nfrom advertorch.attacks import JacobianSaliencyMapAttack\nfrom advertorch.attacks import SpatialTransformAttack\nfrom advertorch.attacks import LinfSPSAAttack\nfrom advertorch.attacks import LinfFABAttack\nfrom advertorch.attacks import L2FABAttack\nfrom advertorch.attacks import L1FABAttack\nfrom advertorch.utils import CarliniWagnerLoss\nfrom advertorch.utils import torch_allclose\n\nfrom advertorch.test_utils import NUM_CLASS\nfrom advertorch.test_utils import BATCH_SIZE\nfrom advertorch.test_utils import batch_consistent_attacks\nfrom advertorch.test_utils import general_input_attacks\nfrom advertorch.test_utils import image_only_attacks\nfrom advertorch.test_utils import label_attacks\nfrom advertorch.test_utils import feature_attacks\nfrom advertorch.test_utils import targeted_only_attacks\n\nfrom advertorch.test_utils import vecdata\nfrom advertorch.test_utils import veclabel\nfrom advertorch.test_utils import vecmodel\nfrom advertorch.test_utils import imgdata\nfrom advertorch.test_utils import imglabel\nfrom advertorch.test_utils import imgmodel\n\n\nxent_loss = nn.CrossEntropyLoss(reduction=""sum"")\ncw_loss = CarliniWagnerLoss()\nmse_loss = nn.MSELoss(reduction=""sum"")\nsmoothl1_loss = nn.SmoothL1Loss(reduction=""sum"")\n\nlabel_criteria = (xent_loss, cw_loss)\nfeature_criteria = (smoothl1_loss, mse_loss)\n\ncuda = ""cuda""\ncpu = ""cpu""\n\ndevices = (cpu, cuda) if torch.cuda.is_available() else (cpu, )\n\nattack_kwargs = {\n    GradientSignAttack: {},\n    GradientAttack: {},\n    LinfBasicIterativeAttack: {""nb_iter"": 5},\n    L2BasicIterativeAttack: {""nb_iter"": 5},\n    LinfPGDAttack: {""rand_init"": False, ""nb_iter"": 5},\n    MomentumIterativeAttack: {""nb_iter"": 5},\n    CarliniWagnerL2Attack: {""num_classes"": NUM_CLASS, ""max_iterations"": 10},\n    ElasticNetL1Attack: {""num_classes"": NUM_CLASS, ""max_iterations"": 10},\n    FastFeatureAttack: {""rand_init"": False, ""nb_iter"": 5},\n    LBFGSAttack: {""num_classes"": NUM_CLASS},\n    JacobianSaliencyMapAttack: {""num_classes"": NUM_CLASS, ""gamma"": 0.01},\n    SpatialTransformAttack: {""num_classes"": NUM_CLASS},\n    DDNL2Attack: {""nb_iter"": 5},\n    SparseL1DescentAttack: {""rand_init"": False, ""nb_iter"": 5},\n    L1PGDAttack: {""rand_init"": False, ""nb_iter"": 5},\n    LinfSPSAAttack: {""eps"": 0.3, ""max_batch_size"": 63},\n    LinfFABAttack: {""n_iter"": 5},\n    L2FABAttack: {""n_iter"": 5},\n    L1FABAttack: {""n_iter"": 5},\n}\n\n\ndef _run_and_assert_original_data_untouched(adversary, data, label):\n    data_clone = data.clone()\n    adversary.perturb(data, label)\n    assert (data_clone == data).all()\n\n    for Attack in targeted_only_attacks:\n        if isinstance(adversary, Attack):\n            return\n\n    adversary.perturb(data)\n    assert (data_clone == data).all()\n\n    adversary.targeted = True\n    adversary.perturb(data, label)\n    assert (data_clone == data).all()\n\n\ndef _run_data_model_criterion_label_attack(\n        data, label, model, criterion, attack, device):\n    model.to(device)\n    adversary = attack(\n        predict=model, loss_fn=criterion, **attack_kwargs[attack])\n    data, label = data.to(device), label.to(device)\n    _run_and_assert_original_data_untouched(adversary, data, label)\n\n\n@pytest.mark.parametrize(\n    ""device, criterion, att_cls"", itertools.product(\n        devices, label_criteria,\n        set(label_attacks).intersection(general_input_attacks)))\ndef test_running_label_attacks_on_vec(device, criterion, att_cls):\n    _run_data_model_criterion_label_attack(\n        vecdata, veclabel, vecmodel, criterion, att_cls, device)\n\n\n@pytest.mark.parametrize(\n    ""device, criterion, att_cls"", itertools.product(\n        devices, label_criteria,\n        set(label_attacks).intersection(\n            image_only_attacks + general_input_attacks)))\ndef test_running_label_attacks_on_img(device, criterion, att_cls):\n    _run_data_model_criterion_label_attack(\n        imgdata, imglabel, imgmodel, criterion, att_cls, device)\n\n\ndef _run_data_model_criterion_feature_attack(\n        data, model, criterion, attack, device):\n    model.to(device)\n    adversary = attack(\n        predict=model, loss_fn=criterion, **attack_kwargs[attack])\n    guide = data.detach().clone()[torch.randperm(len(data))]\n    source, guide = data.to(device), guide.to(device)\n    source_clone = source.clone()\n    adversary.perturb(source, guide)\n    assert (source_clone == source).all()\n\n\n@pytest.mark.parametrize(\n    ""device, criterion, att_cls"", itertools.product(\n        devices, feature_criteria,\n        set(feature_attacks).intersection(general_input_attacks)))\ndef test_running_feature_attacks_on_vec(device, criterion, att_cls):\n    _run_data_model_criterion_feature_attack(\n        vecdata, vecmodel, criterion, att_cls, device)\n\n\n@pytest.mark.parametrize(\n    ""device, criterion, att_cls"", itertools.product(\n        devices, feature_criteria,\n        set(feature_attacks).intersection(\n            image_only_attacks + general_input_attacks)))\ndef test_running_feature_attacks_on_img(device, criterion, att_cls):\n    _run_data_model_criterion_feature_attack(\n        imgdata, imgmodel, criterion, att_cls, device)\n\n\ndef _run_batch_consistent(data, label, model, att_cls, idx):\n    if att_cls in feature_attacks:\n        guide = data.detach().clone()[torch.randperm(len(data))]\n        data, guide = data.to(cpu), guide.to(cpu)\n        label_or_guide = guide\n    else:\n        label_or_guide = label\n    model.to(cpu)\n    data, label_or_guide = data.to(cpu), label_or_guide.to(cpu)\n    adversary = att_cls(model, **attack_kwargs[att_cls])\n    torch.manual_seed(0)\n    a = adversary.perturb(data, label_or_guide)[idx:idx + 1]\n    torch.manual_seed(0)\n    b = adversary.perturb(data[idx:idx + 1], label_or_guide[idx:idx + 1])\n    assert torch_allclose(a, b)\n\n\n@pytest.mark.parametrize(\n    ""idx, att_cls"", itertools.product(\n        [0, BATCH_SIZE // 2, BATCH_SIZE - 1], batch_consistent_attacks))\ndef test_batch_consistent_on_vec(idx, att_cls):\n    _run_batch_consistent(vecdata, veclabel, vecmodel, att_cls, idx)\n\n\n@pytest.mark.parametrize(\n    ""idx, att_cls"", itertools.product(\n        [0, BATCH_SIZE // 2, BATCH_SIZE - 1], batch_consistent_attacks))\ndef test_batch_consistent_on_img(idx, att_cls):\n    _run_batch_consistent(imgdata, imglabel, imgmodel, att_cls, idx)\n\n\nif __name__ == \'__main__\':\n    pass\n'"
tests/test_bpda.py,17,"b'# Copyright (c) 2018-present, Royal Bank of Canada and other authors.\n# See the AUTHORS.txt file for a list of contributors.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nimport itertools\n\nimport pytest\nimport torch\nimport torch.nn as nn\n\nfrom advertorch.bpda import BPDAWrapper\nfrom advertorch.utils import torch_allclose\nfrom advertorch.test_utils import withgrad_defenses\nfrom advertorch.test_utils import nograd_defenses\nfrom advertorch.test_utils import defense_kwargs\nfrom advertorch.test_utils import defense_data\nfrom advertorch.test_utils import vecdata\n\n\ncuda = ""cuda""\ncpu = ""cpu""\ndevices = (cpu, cuda) if torch.cuda.is_available() else (cpu, )\n\n\ndef _identity(x):\n    return x\n\n\ndef _straight_through_backward(grad_output, x):\n    return grad_output\n\n\ndef _calc_datagrad_on_defense(defense, data):\n    data = data.detach().clone().requires_grad_()\n    loss = defense(data).sum()\n    loss.backward()\n    return data.grad.detach().clone()\n\n\n@pytest.mark.parametrize(\n    ""device, def_cls"", itertools.product(devices, nograd_defenses))\ndef test_bpda_on_nograd_defense(device, def_cls):\n    defense = def_cls(**defense_kwargs[def_cls])\n\n    defense = BPDAWrapper(defense, forwardsub=_identity)\n    _calc_datagrad_on_defense(defense, defense_data[def_cls])\n\n    defense = BPDAWrapper(defense, backward=_straight_through_backward)\n    _calc_datagrad_on_defense(defense, defense_data[def_cls])\n\n\n@pytest.mark.parametrize(\n    ""device, def_cls"", itertools.product(devices, withgrad_defenses))\ndef test_bpda_on_withgrad_defense(device, def_cls):\n    defense = def_cls(**defense_kwargs[def_cls])\n\n    grad_from_self = _calc_datagrad_on_defense(\n        defense, defense_data[def_cls])\n\n    defense_with_idenity_backward = BPDAWrapper(defense, forwardsub=_identity)\n    grad_from_identity_backward = _calc_datagrad_on_defense(\n        defense_with_idenity_backward, defense_data[def_cls])\n\n    defense_with_self_backward = BPDAWrapper(defense, forwardsub=defense)\n    grad_from_self_backward = _calc_datagrad_on_defense(\n        defense_with_self_backward, defense_data[def_cls])\n\n    assert not torch_allclose(grad_from_identity_backward, grad_from_self)\n    assert torch_allclose(grad_from_self_backward, grad_from_self)\n\n\n@pytest.mark.parametrize(\n    ""device, func"", itertools.product(\n        devices, [torch.sigmoid, torch.tanh, torch.relu]))\ndef test_bpda_on_activations(device, func):\n    data = vecdata.detach().clone()\n    data = data - data.mean()\n\n    grad_from_self = _calc_datagrad_on_defense(func, data)\n\n    func_with_idenity_backward = BPDAWrapper(func, forwardsub=_identity)\n    grad_from_identity_backward = _calc_datagrad_on_defense(\n        func_with_idenity_backward, data)\n\n    func_with_self_backward = BPDAWrapper(func, forwardsub=func)\n    grad_from_self_backward = _calc_datagrad_on_defense(\n        func_with_self_backward, data)\n\n    assert not torch_allclose(grad_from_identity_backward, grad_from_self)\n    assert torch_allclose(grad_from_self_backward, grad_from_self)\n\n\n@pytest.mark.parametrize(\n    ""device, func"", itertools.product(\n        devices, [nn.Sigmoid(), nn.Tanh(), nn.ReLU()]))\ndef test_bpda_nograd_on_multi_input(device, func):\n\n    class MultiInputFunc(nn.Module):\n        def forward(self, x, y):\n            return 2.0 * x - 1.0 * y\n\n    class DummyNet(nn.Module):\n        def __init__(self):\n            super(DummyNet, self).__init__()\n            self.linear = nn.Linear(1200, 10)\n\n        def forward(self, x):\n            x = x.view(x.shape[0], -1)\n            return self.linear(x)\n\n    bpda = BPDAWrapper(forward=MultiInputFunc())\n\n    with torch.enable_grad():\n        x = torch.rand(size=(10, 3, 20, 20), device=device,\n                       requires_grad=True)\n        y = torch.rand_like(x, requires_grad=True)\n        z = bpda(x, y)\n        z_ = z.detach().requires_grad_()\n\n    net = nn.Sequential(func, DummyNet())\n\n    with torch.enable_grad():\n        loss_ = net(z_).sum()\n        loss = net(z).sum()\n    grad_z, = torch.autograd.grad(loss_, [z_])\n    grad_x, grad_y = torch.autograd.grad(loss, [x, y])\n\n    assert torch_allclose(grad_x, grad_z)\n    assert torch_allclose(grad_y, grad_z)\n\n\nif __name__ == \'__main__\':\n    from advertorch.defenses import AverageSmoothing2D\n    test_bpda_on_withgrad_defense(cpu, AverageSmoothing2D)\n'"
tests/test_context.py,9,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nimport pytest\n\nfrom advertorch.context import ctx_eval\nfrom advertorch.context import ctx_noparamgrad\nfrom advertorch.context import ctx_noparamgrad_and_eval\nfrom advertorch.context import get_param_grad_state\nfrom advertorch.context import get_module_training_state\nfrom advertorch.context import set_param_grad_off\nfrom advertorch.utils import torch_allclose\nfrom advertorch.test_utils import SimpleModel\nfrom advertorch.test_utils import vecdata\n\n\ndef _generate_models():\n    mix_model = SimpleModel()\n    mix_model.fc1.training = True\n    mix_model.fc2.training = False\n    mix_model.fc1.weight.requires_grad = False\n    mix_model.fc2.bias.requires_grad = False\n\n    trainon_model = SimpleModel()\n    trainon_model.train()\n\n    trainoff_model = SimpleModel()\n    trainoff_model.eval()\n\n    gradon_model = SimpleModel()\n\n    gradoff_model = SimpleModel()\n    set_param_grad_off(gradoff_model)\n\n    return (\n        mix_model, gradon_model, gradoff_model, trainon_model, trainoff_model\n    )\n\n\nmix_model, gradon_model, gradoff_model, trainon_model, trainoff_model = \\\n    _generate_models()\n\n\ndef _assert_grad_off(module):\n    for param in module.parameters():\n        assert not param.requires_grad\n\n\ndef _assert_grad_on(module):\n    for param in module.parameters():\n        assert param.requires_grad\n\n\ndef _assert_training_off(module):\n    for mod in module.modules():\n        assert not mod.training\n\n\ndef _assert_training_on(module):\n    for mod in module.modules():\n        assert mod.training\n\n\ndef _run_one_assert_val(ctxmgr, model, assert_inside, assert_outside):\n    output = model(vecdata)\n    assert_outside(model)\n    with ctxmgr(model):\n        assert_inside(model)\n        assert torch_allclose(output, model(vecdata))\n    assert_outside(model)\n    assert torch_allclose(output, model(vecdata))\n\n\ndef _run_one_assert_consistent(ctxmgr, model, get_state_fn, assert_inside):\n    dct = get_state_fn(mix_model)\n    output = model(vecdata)\n    with ctxmgr(model):\n        assert_inside(model)\n        assert torch_allclose(output, model(vecdata))\n    newdct = get_state_fn(model)\n    assert dct is not newdct\n    assert dct == newdct\n    assert torch_allclose(output, model(vecdata))\n\n\n@pytest.mark.parametrize(\n    ""ctxmgr"", (ctx_noparamgrad, ctx_noparamgrad_and_eval))\ndef test_noparamgrad(ctxmgr):\n    _run_one_assert_consistent(ctxmgr, mix_model,\n                               get_state_fn=get_param_grad_state,\n                               assert_inside=_assert_grad_off)\n\n    _run_one_assert_val(ctxmgr, gradon_model,\n                        assert_inside=_assert_grad_off,\n                        assert_outside=_assert_grad_on)\n\n    _run_one_assert_val(ctxmgr, gradoff_model,\n                        assert_inside=_assert_grad_off,\n                        assert_outside=_assert_grad_off)\n\n\n@pytest.mark.parametrize(\n    ""ctxmgr"", (ctx_eval, ctx_noparamgrad_and_eval))\ndef test_eval(ctxmgr):\n    _run_one_assert_consistent(ctxmgr, mix_model,\n                               get_state_fn=get_module_training_state,\n                               assert_inside=_assert_training_off)\n\n    _run_one_assert_val(ctxmgr, trainon_model,\n                        assert_inside=_assert_training_off,\n                        assert_outside=_assert_training_on)\n\n    _run_one_assert_val(ctxmgr, trainoff_model,\n                        assert_inside=_assert_training_off,\n                        assert_outside=_assert_training_off)\n\n\nif __name__ == \'__main__\':\n    pass\n'"
tests/test_defenses_correct.py,4,"b""# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nimport numpy as np\nfrom scipy import ndimage\n\nfrom advertorch.test_utils import generate_data_model_on_img\nfrom advertorch.utils import torch_allclose\nfrom advertorch.defenses import BinaryFilter\nfrom advertorch.defenses import MedianSmoothing2D\n\ndata, label, model = generate_data_model_on_img()\n\n\ndef test_binary_filter():\n    assert torch_allclose(BinaryFilter()(data), data > 0.5)\n\n\ndef test_median_filter():\n    # XXX: doesn't pass when kernel_size is even\n    # XXX: when kernel_size is odd, pixels on the boundaries are different\n    kernel_size = 3\n    padding = kernel_size // 2\n    rval_scipy = ndimage.filters.median_filter(\n        data.detach().numpy(), size=(1, 1, kernel_size, kernel_size))\n    rval = MedianSmoothing2D(kernel_size=kernel_size)(data).detach().numpy()\n    assert np.allclose(rval_scipy[:, :, padding:-padding, padding:-padding],\n                       rval[:, :, padding:-padding, padding:-padding])\n\n\n# TODO: correctness test of GaussianSmoothing2D and AverageSmoothing2D\n\nif __name__ == '__main__':\n    test_binary_filter()\n    test_median_filter()\n"""
tests/test_defenses_running.py,12,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nimport itertools\n\nimport pytest\nimport torch\nimport torch.nn as nn\n\nfrom advertorch.test_utils import vecdata\nfrom advertorch.test_utils import vecmodel\nfrom advertorch.test_utils import imgdata\nfrom advertorch.test_utils import imgmodel\nfrom advertorch.test_utils import general_input_defenses\nfrom advertorch.test_utils import image_only_defenses\nfrom advertorch.test_utils import withgrad_defenses\nfrom advertorch.test_utils import nograd_defenses\nfrom advertorch.test_utils import defense_kwargs\nfrom advertorch.test_utils import defense_data\n\ncuda = ""cuda""\ncpu = ""cpu""\ndevices = (cpu, cuda) if torch.cuda.is_available() else (cpu, )\n\n\ndef _run_data_model_defense(data, model, defense, device):\n    defended_model = nn.Sequential(defense, model)\n    defended_model.to(device)\n    data = data.to(device)\n    defended_model(data)\n\n\n@pytest.mark.parametrize(\n    ""device, def_cls"", itertools.product(devices, general_input_defenses))\ndef test_running_on_vec(device, def_cls):\n    _run_data_model_defense(\n        vecdata, vecmodel, def_cls(**defense_kwargs[def_cls]), device)\n\n\n@pytest.mark.parametrize(\n    ""device, def_cls"",\n    itertools.product(devices, general_input_defenses + image_only_defenses))\ndef test_running_on_img(device, def_cls):\n    _run_data_model_defense(\n        imgdata, imgmodel, def_cls(**defense_kwargs[def_cls]), device)\n\n\n@pytest.mark.parametrize(\n    ""device, def_cls"",\n    itertools.product(devices, withgrad_defenses))\ndef test_withgrad(device, def_cls):\n    defense = def_cls(**defense_kwargs[def_cls])\n    data = defense_data[def_cls]\n    data.requires_grad_()\n    loss = defense(data).sum()\n    loss.backward()\n\n\n@pytest.mark.parametrize(\n    ""device, def_cls"",\n    itertools.product(devices, nograd_defenses))\ndef test_defenses_nograd(device, def_cls):\n    with pytest.raises(NotImplementedError):\n        defense = def_cls(**defense_kwargs[def_cls])\n        data = defense_data[def_cls]\n        data.requires_grad_()\n        loss = defense(data).sum()\n        loss.backward()\n\n\nif __name__ == \'__main__\':\n    pass\n'"
tests/test_utilities.py,25,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nimport warnings\n\nimport numpy as np\nimport torch\nimport torchvision.transforms.functional as F\n\nfrom advertorch.utils import torch_allclose\nfrom advertorch.utils import clamp\nfrom advertorch.utils import CIFAR10_MEAN\nfrom advertorch.utils import CIFAR10_STD\nfrom advertorch.utils import MNIST_MEAN\nfrom advertorch.utils import MNIST_STD\nfrom advertorch.utils import NormalizeByChannelMeanStd\nfrom advertorch.utils import PerImageStandardize\nfrom advertorch.utils import torch_flip\nfrom advertorch_examples.utils import bchw2bhwc\nfrom advertorch_examples.utils import bhwc2bchw\n\n\ndef test_mnist_normalize():\n    # MNIST\n    tensor = torch.rand((16, 1, 28, 28))\n    normalize = NormalizeByChannelMeanStd(MNIST_MEAN, MNIST_STD)\n\n    assert torch_allclose(\n        torch.stack([F.normalize(t, MNIST_MEAN, MNIST_STD)\n                     for t in tensor.clone()]),\n        normalize(tensor))\n\n\ndef test_cifar10_normalize():\n    # CIFAR10\n    tensor = torch.rand((16, 3, 32, 32))\n    normalize = NormalizeByChannelMeanStd(CIFAR10_MEAN, CIFAR10_STD)\n\n    assert torch_allclose(\n        torch.stack([F.normalize(t, CIFAR10_MEAN, CIFAR10_STD)\n                     for t in tensor.clone()]),\n        normalize(tensor))\n\n\ndef test_grad_through_normalize():\n    tensor = torch.rand((2, 1, 28, 28))\n    tensor.requires_grad_()\n    mean = torch.tensor((0.,))\n    std = torch.tensor((1.,))\n    normalize = NormalizeByChannelMeanStd(mean, std)\n\n    loss = (normalize(tensor) ** 2).sum()\n    loss.backward()\n\n    assert torch_allclose(2 * tensor, tensor.grad)\n\n\ndef _run_tf_per_image_standardization(imgs):\n    import tensorflow as tf\n    import tensorflow.image  # noqa: F401\n\n    imgs = bchw2bhwc(imgs)\n    placeholder = tf.placeholder(tf.float32, shape=imgs.shape)\n    var_scaled = tf.map_fn(\n        lambda img: tf.image.per_image_standardization(img), placeholder)\n\n    with tf.Session() as sess:\n        tf_scaled = sess.run(var_scaled, feed_dict={placeholder: imgs})\n    return bhwc2bchw(tf_scaled)\n\n\ndef test_per_image_standardization():\n    imgs = np.random.normal(\n        scale=1. / (3072 ** 0.5), size=(10, 3, 32, 32)).astype(np.float32)\n    per_image_standardize = PerImageStandardize()\n    pt_scaled = per_image_standardize(torch.tensor(imgs)).numpy()\n    with warnings.catch_warnings():\n        warnings.simplefilter(""ignore"")\n        tf_scaled = _run_tf_per_image_standardization(imgs)\n    assert np.abs(pt_scaled - tf_scaled).max() < 0.001\n\n\ndef test_clamp():\n\n    def _convert_to_float(x):\n        return float(x) if x is not None else None\n\n    def _convert_to_batch_tensor(x, data):\n        return x * torch.ones_like(data) if x is not None else None\n\n    def _convert_to_single_tensor(x, data):\n        return x * torch.ones_like(data[0]) if x is not None else None\n\n\n    for min, max in [(-1, None), (None, 1), (-1, 1)]:\n\n        data = 3 * torch.randn((11, 12, 13))\n        case1 = clamp(data, min, max)\n        case2 = clamp(data, _convert_to_float(min), _convert_to_float(max))\n        case3 = clamp(data, _convert_to_batch_tensor(min, data),\n                      _convert_to_batch_tensor(max, data))\n        case4 = clamp(data, _convert_to_single_tensor(min, data),\n                      _convert_to_single_tensor(max, data))\n\n        assert torch.all(case1 == case2)\n        assert torch.all(case2 == case3)\n        assert torch.all(case3 == case4)\n\n\ndef test_flip():\n    x = torch.randn(4, 5, 6, 7)\n    assert (torch_flip(x, dims=(1, 2)) == torch.flip(x, dims=(1, 2))).all()\n'"
advertorch/attacks/__init__.py,0,"b'# Copyright (c) 2018-present, Royal Bank of Canada and other authors.\n# See the AUTHORS.txt file for a list of contributors.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\n# flake8: noqa\n\nfrom .base import Attack\nfrom .base import LabelMixin\n\nfrom .one_step_gradient import GradientAttack\nfrom .one_step_gradient import GradientSignAttack\nfrom .one_step_gradient import FGM\nfrom .one_step_gradient import FGSM\n\nfrom .iterative_projected_gradient import FastFeatureAttack\nfrom .iterative_projected_gradient import L2BasicIterativeAttack\nfrom .iterative_projected_gradient import LinfBasicIterativeAttack\nfrom .iterative_projected_gradient import PGDAttack\nfrom .iterative_projected_gradient import LinfPGDAttack\nfrom .iterative_projected_gradient import L2PGDAttack\nfrom .iterative_projected_gradient import L1PGDAttack\nfrom .iterative_projected_gradient import SparseL1DescentAttack\nfrom .iterative_projected_gradient import MomentumIterativeAttack\nfrom .iterative_projected_gradient import L2MomentumIterativeAttack\nfrom .iterative_projected_gradient import LinfMomentumIterativeAttack\n\nfrom .carlini_wagner import CarliniWagnerL2Attack\nfrom .ead import ElasticNetL1Attack\n\nfrom .decoupled_direction_norm import DDNL2Attack\n\nfrom .lbfgs import LBFGSAttack\n\nfrom .localsearch import SinglePixelAttack\nfrom .localsearch import LocalSearchAttack\n\nfrom .spatial import SpatialTransformAttack\n\nfrom .jsma import JacobianSaliencyMapAttack\nfrom .jsma import JSMA\n\nfrom .spsa import LinfSPSAAttack\nfrom .fast_adaptive_boundary import FABAttack\nfrom .fast_adaptive_boundary import LinfFABAttack\nfrom .fast_adaptive_boundary import L2FABAttack\nfrom .fast_adaptive_boundary import L1FABAttack\n\nfrom .utils import ChooseBestAttack\n'"
advertorch/attacks/base.py,3,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom abc import ABCMeta\n\nimport torch\n\nfrom advertorch.utils import replicate_input\n\n\nclass Attack(object):\n    """"""\n    Abstract base class for all attack classes.\n\n    :param predict: forward pass function.\n    :param loss_fn: loss function that takes .\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n\n    """"""\n\n    __metaclass__ = ABCMeta\n\n    def __init__(self, predict, loss_fn, clip_min, clip_max):\n        """"""Create an Attack instance.""""""\n        self.predict = predict\n        self.loss_fn = loss_fn\n        self.clip_min = clip_min\n        self.clip_max = clip_max\n\n    def perturb(self, x, **kwargs):\n        """"""Virtual method for generating the adversarial examples.\n\n        :param x: the model\'s input tensor.\n        :param **kwargs: optional parameters used by child classes.\n        :return: adversarial examples.\n        """"""\n        error = ""Sub-classes must implement perturb.""\n        raise NotImplementedError(error)\n\n    def __call__(self, *args, **kwargs):\n        return self.perturb(*args, **kwargs)\n\n\nclass LabelMixin(object):\n    def _get_predicted_label(self, x):\n        """"""\n        Compute predicted labels given x. Used to prevent label leaking\n        during adversarial training.\n\n        :param x: the model\'s input tensor.\n        :return: tensor containing predicted labels.\n        """"""\n        with torch.no_grad():\n            outputs = self.predict(x)\n        _, y = torch.max(outputs, dim=1)\n        return y\n\n    def _verify_and_process_inputs(self, x, y):\n        if self.targeted:\n            assert y is not None\n\n        if not self.targeted:\n            if y is None:\n                y = self._get_predicted_label(x)\n\n        x = replicate_input(x)\n        y = replicate_input(y)\n        return x, y\n'"
advertorch/attacks/carlini_wagner.py,20,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom advertorch.utils import calc_l2distsq\nfrom advertorch.utils import tanh_rescale\nfrom advertorch.utils import torch_arctanh\nfrom advertorch.utils import clamp\nfrom advertorch.utils import to_one_hot\nfrom advertorch.utils import replicate_input\n\nfrom .base import Attack\nfrom .base import LabelMixin\nfrom .utils import is_successful\n\n\nCARLINI_L2DIST_UPPER = 1e10\nCARLINI_COEFF_UPPER = 1e10\nINVALID_LABEL = -1\nREPEAT_STEP = 10\nONE_MINUS_EPS = 0.999999\nUPPER_CHECK = 1e9\nPREV_LOSS_INIT = 1e6\nTARGET_MULT = 10000.0\nNUM_CHECKS = 10\n\n\nclass CarliniWagnerL2Attack(Attack, LabelMixin):\n    """"""\n    The Carlini and Wagner L2 Attack, https://arxiv.org/abs/1608.04644\n\n    :param predict: forward pass function.\n    :param num_classes: number of clasess.\n    :param confidence: confidence of the adversarial examples.\n    :param targeted: if the attack is targeted.\n    :param learning_rate: the learning rate for the attack algorithm\n    :param binary_search_steps: number of binary search times to find the\n        optimum\n    :param max_iterations: the maximum number of iterations\n    :param abort_early: if set to true, abort early if getting stuck in local\n        min\n    :param initial_const: initial value of the constant c\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param loss_fn: loss function\n    """"""\n\n    def __init__(self, predict, num_classes, confidence=0,\n                 targeted=False, learning_rate=0.01,\n                 binary_search_steps=9, max_iterations=10000,\n                 abort_early=True, initial_const=1e-3,\n                 clip_min=0., clip_max=1., loss_fn=None):\n        """"""Carlini Wagner L2 Attack implementation in pytorch.""""""\n        if loss_fn is not None:\n            import warnings\n            warnings.warn(\n                ""This Attack currently do not support a different loss""\n                "" function other than the default. Setting loss_fn manually""\n                "" is not effective.""\n            )\n\n        loss_fn = None\n\n        super(CarliniWagnerL2Attack, self).__init__(\n            predict, loss_fn, clip_min, clip_max)\n\n        self.learning_rate = learning_rate\n        self.max_iterations = max_iterations\n        self.binary_search_steps = binary_search_steps\n        self.abort_early = abort_early\n        self.confidence = confidence\n        self.initial_const = initial_const\n        self.num_classes = num_classes\n        # The last iteration (if we run many steps) repeat the search once.\n        self.repeat = binary_search_steps >= REPEAT_STEP\n        self.targeted = targeted\n\n    def _loss_fn(self, output, y_onehot, l2distsq, const):\n        # TODO: move this out of the class and make this the default loss_fn\n        #   after having targeted tests implemented\n        real = (y_onehot * output).sum(dim=1)\n\n        # TODO: make loss modular, write a loss class\n        other = ((1.0 - y_onehot) * output - (y_onehot * TARGET_MULT)\n                 ).max(1)[0]\n        # - (y_onehot * TARGET_MULT) is for the true label not to be selected\n\n        if self.targeted:\n            loss1 = clamp(other - real + self.confidence, min=0.)\n        else:\n            loss1 = clamp(real - other + self.confidence, min=0.)\n        loss2 = (l2distsq).sum()\n        loss1 = torch.sum(const * loss1)\n        loss = loss1 + loss2\n        return loss\n\n    def _is_successful(self, output, label, is_logits):\n        # determine success, see if confidence-adjusted logits give the right\n        #   label\n\n        if is_logits:\n            output = output.detach().clone()\n            if self.targeted:\n                output[torch.arange(len(label)).long(),\n                       label] -= self.confidence\n            else:\n                output[torch.arange(len(label)).long(),\n                       label] += self.confidence\n            pred = torch.argmax(output, dim=1)\n        else:\n            pred = output\n            if pred == INVALID_LABEL:\n                return pred.new_zeros(pred.shape).byte()\n\n        return is_successful(pred, label, self.targeted)\n\n\n    def _forward_and_update_delta(\n            self, optimizer, x_atanh, delta, y_onehot, loss_coeffs):\n\n        optimizer.zero_grad()\n        adv = tanh_rescale(delta + x_atanh, self.clip_min, self.clip_max)\n        transimgs_rescale = tanh_rescale(x_atanh, self.clip_min, self.clip_max)\n        output = self.predict(adv)\n        l2distsq = calc_l2distsq(adv, transimgs_rescale)\n        loss = self._loss_fn(output, y_onehot, l2distsq, loss_coeffs)\n        loss.backward()\n        optimizer.step()\n\n        return loss.item(), l2distsq.data, output.data, adv.data\n\n\n    def _get_arctanh_x(self, x):\n        result = clamp((x - self.clip_min) / (self.clip_max - self.clip_min),\n                       min=0., max=1.) * 2 - 1\n        return torch_arctanh(result * ONE_MINUS_EPS)\n\n    def _update_if_smaller_dist_succeed(\n            self, adv_img, labs, output, l2distsq, batch_size,\n            cur_l2distsqs, cur_labels,\n            final_l2distsqs, final_labels, final_advs):\n\n        target_label = labs\n        output_logits = output\n        _, output_label = torch.max(output_logits, 1)\n\n        mask = (l2distsq < cur_l2distsqs) & self._is_successful(\n            output_logits, target_label, True)\n\n        cur_l2distsqs[mask] = l2distsq[mask]  # redundant\n        cur_labels[mask] = output_label[mask]\n\n        mask = (l2distsq < final_l2distsqs) & self._is_successful(\n            output_logits, target_label, True)\n        final_l2distsqs[mask] = l2distsq[mask]\n        final_labels[mask] = output_label[mask]\n        final_advs[mask] = adv_img[mask]\n\n    def _update_loss_coeffs(\n            self, labs, cur_labels, batch_size, loss_coeffs,\n            coeff_upper_bound, coeff_lower_bound):\n\n        # TODO: remove for loop, not significant, since only called during each\n        # binary search step\n        for ii in range(batch_size):\n            cur_labels[ii] = int(cur_labels[ii])\n            if self._is_successful(cur_labels[ii], labs[ii], False):\n                coeff_upper_bound[ii] = min(\n                    coeff_upper_bound[ii], loss_coeffs[ii])\n\n                if coeff_upper_bound[ii] < UPPER_CHECK:\n                    loss_coeffs[ii] = (\n                        coeff_lower_bound[ii] + coeff_upper_bound[ii]) / 2\n            else:\n                coeff_lower_bound[ii] = max(\n                    coeff_lower_bound[ii], loss_coeffs[ii])\n                if coeff_upper_bound[ii] < UPPER_CHECK:\n                    loss_coeffs[ii] = (\n                        coeff_lower_bound[ii] + coeff_upper_bound[ii]) / 2\n                else:\n                    loss_coeffs[ii] *= 10\n\n\n    def perturb(self, x, y=None):\n        x, y = self._verify_and_process_inputs(x, y)\n\n        # Initialization\n        if y is None:\n            y = self._get_predicted_label(x)\n        x = replicate_input(x)\n        batch_size = len(x)\n        coeff_lower_bound = x.new_zeros(batch_size)\n        coeff_upper_bound = x.new_ones(batch_size) * CARLINI_COEFF_UPPER\n        loss_coeffs = torch.ones_like(y).float() * self.initial_const\n        final_l2distsqs = [CARLINI_L2DIST_UPPER] * batch_size\n        final_labels = [INVALID_LABEL] * batch_size\n        final_advs = x\n        x_atanh = self._get_arctanh_x(x)\n        y_onehot = to_one_hot(y, self.num_classes).float()\n\n        final_l2distsqs = torch.FloatTensor(final_l2distsqs).to(x.device)\n        final_labels = torch.LongTensor(final_labels).to(x.device)\n\n        # Start binary search\n        for outer_step in range(self.binary_search_steps):\n            delta = nn.Parameter(torch.zeros_like(x))\n            optimizer = optim.Adam([delta], lr=self.learning_rate)\n            cur_l2distsqs = [CARLINI_L2DIST_UPPER] * batch_size\n            cur_labels = [INVALID_LABEL] * batch_size\n            cur_l2distsqs = torch.FloatTensor(cur_l2distsqs).to(x.device)\n            cur_labels = torch.LongTensor(cur_labels).to(x.device)\n            prevloss = PREV_LOSS_INIT\n\n            if (self.repeat and outer_step == (self.binary_search_steps - 1)):\n                loss_coeffs = coeff_upper_bound\n            for ii in range(self.max_iterations):\n                loss, l2distsq, output, adv_img = \\\n                    self._forward_and_update_delta(\n                        optimizer, x_atanh, delta, y_onehot, loss_coeffs)\n                if self.abort_early:\n                    if ii % (self.max_iterations // NUM_CHECKS or 1) == 0:\n                        if loss > prevloss * ONE_MINUS_EPS:\n                            break\n                        prevloss = loss\n\n                self._update_if_smaller_dist_succeed(\n                    adv_img, y, output, l2distsq, batch_size,\n                    cur_l2distsqs, cur_labels,\n                    final_l2distsqs, final_labels, final_advs)\n\n            self._update_loss_coeffs(\n                y, cur_labels, batch_size,\n                loss_coeffs, coeff_upper_bound, coeff_lower_bound)\n\n        return final_advs\n'"
advertorch/attacks/decoupled_direction_norm.py,9,"b'# Copyright (c) 2019-present, J\xc3\xa9r\xc3\xb4me Rony.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom .base import Attack\nfrom .base import LabelMixin\n\n\nclass DDNL2Attack(Attack, LabelMixin):\n    """"""\n    The decoupled direction and norm attack (Rony et al, 2018).\n    Paper: https://arxiv.org/abs/1811.09600\n\n    :param predict: forward pass function.\n    :param nb_iter: number of iterations.\n    :param gamma: factor to modify the norm at each iteration.\n    :param init_norm: initial norm of the perturbation.\n    :param quantize: perform quantization at each iteration.\n    :param levels: number of quantization levels (e.g. 256 for 8 bit images).\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param targeted: if the attack is targeted.\n    :param loss_fn: loss function.\n    """"""\n\n    def __init__(\n            self, predict, nb_iter=100, gamma=0.05, init_norm=1.,\n            quantize=True, levels=256, clip_min=0., clip_max=1.,\n            targeted=False, loss_fn=None):\n        """"""\n        Decoupled Direction and Norm L2 Attack implementation in pytorch.\n        """"""\n        if loss_fn is not None:\n            import warnings\n            warnings.warn(\n                ""This Attack currently does not support a different loss""\n                "" function other than the default. Setting loss_fn manually""\n                "" is not effective.""\n            )\n\n        loss_fn = nn.CrossEntropyLoss(reduction=""sum"")\n\n        super(DDNL2Attack, self).__init__(predict, loss_fn, clip_min, clip_max)\n\n        self.nb_iter = nb_iter\n        self.gamma = gamma\n        self.init_norm = init_norm\n        self.quantize = quantize\n        self.levels = levels\n        self.targeted = targeted\n\n    def perturb(self, x, y=None):\n        """"""\n        Given examples (x, y), returns their adversarial counterparts with\n        an attack length of eps.\n\n        :param x: input tensor.\n        :param y: label tensor.\n                  - if None and self.targeted=False, compute y as predicted\n                    labels.\n                  - if self.targeted=True, then y must be the targeted labels.\n        :return: tensor containing perturbed inputs.\n        """"""\n        x, y = self._verify_and_process_inputs(x, y)\n\n        s = self.clip_max - self.clip_min\n        multiplier = 1 if self.targeted else -1\n        batch_size = x.shape[0]\n        data_dims = (1,) * (x.dim() - 1)\n        norm = torch.full((batch_size,), s * self.init_norm,\n                          device=x.device, dtype=torch.float)\n        worst_norm = torch.max(\n            x - self.clip_min, self.clip_max - x).flatten(1).norm(p=2, dim=1)\n\n        # setup variable and optimizer\n        delta = torch.zeros_like(x, requires_grad=True)\n        optimizer = optim.SGD([delta], lr=1)\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=self.nb_iter, eta_min=0.01)\n\n        best_l2 = worst_norm.clone()\n        best_delta = torch.zeros_like(x)\n\n        for i in range(self.nb_iter):\n            scheduler.step()\n\n            l2 = delta.data.flatten(1).norm(p=2, dim=1)\n            logits = self.predict(x + delta)\n            pred_labels = logits.argmax(1)\n            ce_loss = self.loss_fn(logits, y)\n            loss = multiplier * ce_loss\n\n            is_adv = (pred_labels == y) if self.targeted else (\n                pred_labels != y)\n            is_smaller = l2 < best_l2\n            is_both = is_adv * is_smaller\n            best_l2[is_both] = l2[is_both]\n            best_delta[is_both] = delta.data[is_both]\n\n            optimizer.zero_grad()\n            loss.backward()\n\n            # renorming gradient\n            grad_norms = s * delta.grad.flatten(1).norm(p=2, dim=1)\n            delta.grad.div_(grad_norms.view(-1, *data_dims))\n            # avoid nan or inf if gradient is 0\n            if (grad_norms == 0).any():\n                delta.grad[grad_norms == 0] = torch.randn_like(\n                    delta.grad[grad_norms == 0])\n\n            optimizer.step()\n\n            norm.mul_(1 - (2 * is_adv.float() - 1) * self.gamma)\n\n            delta.data.mul_((norm / delta.data.flatten(1).norm(\n                p=2, dim=1)).view(-1, *data_dims))\n            delta.data.add_(x)\n            if self.quantize:\n                delta.data.sub_(self.clip_min).div_(s)\n                delta.data.mul_(self.levels - 1).round_().div_(self.levels - 1)\n                delta.data.mul_(s).add_(self.clip_min)\n            delta.data.clamp_(self.clip_min, self.clip_max).sub_(x)\n\n        return x + best_delta\n'"
advertorch/attacks/ead.py,19,"b'# Copyright (c) 2019-present, Alexandre Araujo.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport torch\nimport torch.nn as nn\n\nfrom advertorch.utils import calc_l2distsq\nfrom advertorch.utils import calc_l1dist\nfrom advertorch.utils import clamp\nfrom advertorch.utils import to_one_hot\nfrom advertorch.utils import replicate_input\n\nfrom .base import Attack\nfrom .base import LabelMixin\nfrom .utils import is_successful\n\n\nDIST_UPPER = 1e10\nCOEFF_UPPER = 1e10\nINVALID_LABEL = -1\nREPEAT_STEP = 10\nONE_MINUS_EPS = 0.999999\nUPPER_CHECK = 1e9\nPREV_LOSS_INIT = 1e6\nTARGET_MULT = 10000\nNUM_CHECKS = 10\n\n\nclass ElasticNetL1Attack(Attack, LabelMixin):\n    """"""\n    The ElasticNet L1 Attack, https://arxiv.org/abs/1709.04114\n\n    :param predict: forward pass function.\n    :param num_classes: number of clasess.\n    :param confidence: confidence of the adversarial examples.\n    :param targeted: if the attack is targeted.\n    :param learning_rate: the learning rate for the attack algorithm\n    :param binary_search_steps: number of binary search times to find the\n        optimum\n    :param max_iterations: the maximum number of iterations\n    :param abort_early: if set to true, abort early if getting stuck in local\n        min\n    :param initial_const: initial value of the constant c\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param beta: hyperparameter trading off L2 minimization for L1 minimization\n    :param decision_rule: EN or L1. Select final adversarial example from\n                          all successful examples based on the least\n                          elastic-net or L1 distortion criterion.\n    :param loss_fn: loss function\n    """"""\n\n    def __init__(self, predict, num_classes, confidence=0,\n                 targeted=False, learning_rate=1e-2,\n                 binary_search_steps=9, max_iterations=10000,\n                 abort_early=False, initial_const=1e-3,\n                 clip_min=0., clip_max=1., beta=1e-2, decision_rule=\'EN\',\n                 loss_fn=None):\n        """"""ElasticNet L1 Attack implementation in pytorch.""""""\n        if loss_fn is not None:\n            import warnings\n            warnings.warn(\n                ""This Attack currently do not support a different loss""\n                "" function other than the default. Setting loss_fn manually""\n                "" is not effective.""\n            )\n\n        loss_fn = None\n\n        super(ElasticNetL1Attack, self).__init__(\n            predict, loss_fn, clip_min, clip_max)\n\n        self.learning_rate = learning_rate\n        self.init_learning_rate = learning_rate\n        self.max_iterations = max_iterations\n        self.binary_search_steps = binary_search_steps\n        self.abort_early = abort_early\n        self.confidence = confidence\n        self.initial_const = initial_const\n        self.num_classes = num_classes\n        self.beta = beta\n        # The last iteration (if we run many steps) repeat the search once.\n        self.repeat = binary_search_steps >= REPEAT_STEP\n        self.targeted = targeted\n        self.decision_rule = decision_rule\n\n\n    def _loss_fn(self, output, y_onehot, l1dist, l2distsq, const, opt=False):\n\n        real = (y_onehot * output).sum(dim=1)\n        other = ((1.0 - y_onehot) * output -\n                 (y_onehot * TARGET_MULT)).max(1)[0]\n\n        if self.targeted:\n            loss_logits = clamp(other - real + self.confidence, min=0.)\n        else:\n            loss_logits = clamp(real - other + self.confidence, min=0.)\n        loss_logits = torch.sum(const * loss_logits)\n\n        loss_l2 = l2distsq.sum()\n\n        if opt:\n            loss = loss_logits + loss_l2\n        else:\n            loss_l1 = self.beta * l1dist.sum()\n            loss = loss_logits + loss_l2 + loss_l1\n        return loss\n\n\n    def _is_successful(self, output, label, is_logits):\n        # determine success, see if confidence-adjusted logits give the right\n        #   label\n        if is_logits:\n            output = output.detach().clone()\n            if self.targeted:\n                output[torch.arange(len(label)).long(),\n                       label] -= self.confidence\n            else:\n                output[torch.arange(len(label)).long(),\n                       label] += self.confidence\n            pred = torch.argmax(output, dim=1)\n        else:\n            pred = output\n            if pred == INVALID_LABEL:\n                return pred.new_zeros(pred.shape).byte()\n\n        return is_successful(pred, label, self.targeted)\n\n\n    def _fast_iterative_shrinkage_thresholding(self, x, yy_k, xx_k):\n\n        zt = self.global_step / (self.global_step + 3)\n\n        upper = clamp(yy_k - self.beta, max=self.clip_max)\n        lower = clamp(yy_k + self.beta, min=self.clip_min)\n\n        diff = yy_k - x\n        cond1 = (diff > self.beta).float()\n        cond2 = (torch.abs(diff) <= self.beta).float()\n        cond3 = (diff < -self.beta).float()\n\n        xx_k_p_1 = (cond1 * upper) + (cond2 * x) + (cond3 * lower)\n        yy_k.data = xx_k_p_1 + (zt * (xx_k_p_1 - xx_k))\n        return yy_k, xx_k_p_1\n\n\n    def _update_if_smaller_dist_succeed(\n            self, adv_img, labs, output, dist, batch_size,\n            cur_dist, cur_labels,\n            final_dist, final_labels, final_advs):\n\n        target_label = labs\n        output_logits = output\n        _, output_label = torch.max(output_logits, 1)\n\n        mask = (dist < cur_dist) & self._is_successful(\n            output_logits, target_label, True)\n\n        cur_dist[mask] = dist[mask]  # redundant\n        cur_labels[mask] = output_label[mask]\n\n        mask = (dist < final_dist) & self._is_successful(\n            output_logits, target_label, True)\n        final_dist[mask] = dist[mask]\n        final_labels[mask] = output_label[mask]\n        final_advs[mask] = adv_img[mask]\n\n\n    def _update_loss_coeffs(\n            self, labs, cur_labels, batch_size, loss_coeffs,\n            coeff_upper_bound, coeff_lower_bound):\n\n        # TODO: remove for loop, not significant, since only called during each\n        # binary search step\n        for ii in range(batch_size):\n            cur_labels[ii] = int(cur_labels[ii])\n            if self._is_successful(cur_labels[ii], labs[ii], False):\n                coeff_upper_bound[ii] = min(\n                    coeff_upper_bound[ii], loss_coeffs[ii])\n\n                if coeff_upper_bound[ii] < UPPER_CHECK:\n                    loss_coeffs[ii] = (\n                        coeff_lower_bound[ii] + coeff_upper_bound[ii]) / 2\n            else:\n                coeff_lower_bound[ii] = max(\n                    coeff_lower_bound[ii], loss_coeffs[ii])\n                if coeff_upper_bound[ii] < UPPER_CHECK:\n                    loss_coeffs[ii] = (\n                        coeff_lower_bound[ii] + coeff_upper_bound[ii]) / 2\n                else:\n                    loss_coeffs[ii] *= 10\n\n\n    def perturb(self, x, y=None):\n\n        x, y = self._verify_and_process_inputs(x, y)\n\n        # Initialization\n        if y is None:\n            y = self._get_predicted_label(x)\n\n        x = replicate_input(x)\n        batch_size = len(x)\n        coeff_lower_bound = x.new_zeros(batch_size)\n        coeff_upper_bound = x.new_ones(batch_size) * COEFF_UPPER\n        loss_coeffs = torch.ones_like(y).float() * self.initial_const\n\n        final_dist = [DIST_UPPER] * batch_size\n        final_labels = [INVALID_LABEL] * batch_size\n\n        final_advs = x.clone()\n        y_onehot = to_one_hot(y, self.num_classes).float()\n\n        final_dist = torch.FloatTensor(final_dist).to(x.device)\n        final_labels = torch.LongTensor(final_labels).to(x.device)\n\n        # Start binary search\n        for outer_step in range(self.binary_search_steps):\n\n            self.global_step = 0\n\n            # slack vector from the paper\n            yy_k = nn.Parameter(x.clone())\n            xx_k = x.clone()\n\n            cur_dist = [DIST_UPPER] * batch_size\n            cur_labels = [INVALID_LABEL] * batch_size\n\n            cur_dist = torch.FloatTensor(cur_dist).to(x.device)\n            cur_labels = torch.LongTensor(cur_labels).to(x.device)\n\n            prevloss = PREV_LOSS_INIT\n\n            if (self.repeat and outer_step == (self.binary_search_steps - 1)):\n                loss_coeffs = coeff_upper_bound\n\n            lr = self.learning_rate\n\n            for ii in range(self.max_iterations):\n\n                # reset gradient\n                if yy_k.grad is not None:\n                    yy_k.grad.detach_()\n                    yy_k.grad.zero_()\n\n                # loss over yy_k with only L2 same as C&W\n                # we don\'t update L1 loss with SGD because we use ISTA\n                output = self.predict(yy_k)\n                l2distsq = calc_l2distsq(yy_k, x)\n                loss_opt = self._loss_fn(\n                    output, y_onehot, None, l2distsq, loss_coeffs, opt=True)\n                loss_opt.backward()\n\n                # gradient step\n                yy_k.data.add_(-lr, yy_k.grad.data)\n                self.global_step += 1\n\n                # ploynomial decay of learning rate\n                lr = self.init_learning_rate * \\\n                    (1 - self.global_step / self.max_iterations)**0.5\n\n                yy_k, xx_k = self._fast_iterative_shrinkage_thresholding(\n                    x, yy_k, xx_k)\n\n                # loss ElasticNet or L1 over xx_k\n                with torch.no_grad():\n                    output = self.predict(xx_k)\n                    l2distsq = calc_l2distsq(xx_k, x)\n                    l1dist = calc_l1dist(xx_k, x)\n\n                    if self.decision_rule == \'EN\':\n                        dist = l2distsq + (l1dist * self.beta)\n                    elif self.decision_rule == \'L1\':\n                        dist = l1dist\n                    loss = self._loss_fn(\n                        output, y_onehot, l1dist, l2distsq, loss_coeffs)\n\n                    if self.abort_early:\n                        if ii % (self.max_iterations // NUM_CHECKS or 1) == 0:\n                            if loss > prevloss * ONE_MINUS_EPS:\n                                break\n                            prevloss = loss\n\n                    self._update_if_smaller_dist_succeed(\n                        xx_k.data, y, output, dist, batch_size,\n                        cur_dist, cur_labels,\n                        final_dist, final_labels, final_advs)\n\n            self._update_loss_coeffs(\n                y, cur_labels, batch_size,\n                loss_coeffs, coeff_upper_bound, coeff_lower_bound)\n\n        return final_advs\n'"
advertorch/attacks/fast_adaptive_boundary.py,87,"b'# Copyright (c) 2019-present, Francesco Croce\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport torch\nfrom torch.autograd.gradcheck import zero_gradients\nimport time\n\ntry:\n    from torch import flip\nexcept ImportError:\n    from advertorch.utils import torch_flip as flip\n\nfrom advertorch.utils import replicate_input\n\nfrom .base import Attack\nfrom .base import LabelMixin\n\nDEFAULT_EPS_DICT_BY_NORM = {\'Linf\': .3, \'L2\': 1., \'L1\': 5.0}\n\n\nclass FABAttack(Attack, LabelMixin):\n    """"""\n    Fast Adaptive Boundary Attack (Linf, L2, L1)\n    https://arxiv.org/abs/1907.02044\n\n    :param predict:       forward pass function\n    :param norm:          Lp-norm to minimize (\'Linf\', \'L2\', \'L1\' supported)\n    :param n_restarts:    number of random restarts\n    :param n_iter:        number of iterations\n    :param eps:           epsilon for the random restarts\n    :param alpha_max:     alpha_max\n    :param eta:           overshooting\n    :param beta:          backward step\n    :param device:        device to use (\'cuda\' or \'cpu\')\n    """"""\n\n    def __init__(\n            self,\n            predict,\n            norm=\'Linf\',\n            n_restarts=1,\n            n_iter=100,\n            eps=None,\n            alpha_max=0.1,\n            eta=1.05,\n            beta=0.9,\n            loss_fn=None,\n            verbose=False,\n    ):\n        """""" FAB-attack implementation in pytorch """"""\n\n        super(FABAttack, self).__init__(\n            predict, loss_fn=None, clip_min=0., clip_max=1.)\n\n        self.norm = norm\n        self.n_restarts = n_restarts\n        self.n_iter = n_iter\n        self.eps = eps if eps is not None else DEFAULT_EPS_DICT_BY_NORM[norm]\n        self.alpha_max = alpha_max\n        self.eta = eta\n        self.beta = beta\n        self.targeted = False\n        self.verbose = verbose\n\n    def check_shape(self, x):\n        return x if len(x.shape) > 0 else x.unsqueeze(0)\n\n    def get_diff_logits_grads_batch(self, imgs, la):\n        im = imgs.clone().requires_grad_()\n        with torch.enable_grad():\n            y = self.predict(im)\n\n        g2 = torch.zeros([y.shape[-1], *imgs.size()]).to(self.device)\n        grad_mask = torch.zeros_like(y)\n        for counter in range(y.shape[-1]):\n            zero_gradients(im)\n            grad_mask[:, counter] = 1.0\n            y.backward(grad_mask, retain_graph=True)\n            grad_mask[:, counter] = 0.0\n            g2[counter] = im.grad.data\n\n        g2 = torch.transpose(g2, 0, 1).detach()\n        y2 = self.predict(imgs).detach()\n        df = y2 - y2[torch.arange(imgs.shape[0]), la].unsqueeze(1)\n        dg = g2 - g2[torch.arange(imgs.shape[0]), la].unsqueeze(1)\n        df[torch.arange(imgs.shape[0]), la] = 1e10\n\n        return df, dg\n\n    def projection_linf(self, points_to_project, w_hyperplane, b_hyperplane):\n        t = points_to_project.clone()\n        w = w_hyperplane.clone()\n        b = b_hyperplane.clone()\n\n        ind2 = ((w * t).sum(1) - b < 0).nonzero().squeeze()\n        ind2 = self.check_shape(ind2)\n        w[ind2] *= -1\n        b[ind2] *= -1\n\n        c5 = (w < 0).float()\n        a = torch.ones(t.shape).to(self.device)\n        d = (a * c5 - t) * (w != 0).float()\n        a -= a * (1 - c5)\n\n        p = torch.ones(t.shape).to(self.device) * c5 - t * (2 * c5 - 1)\n        _, indp = torch.sort(p, dim=1)\n\n        b = b - (w * t).sum(1)\n        b0 = (w * d).sum(1)\n        b1 = b0.clone()\n\n        counter = 0\n        indp2 = flip(indp.unsqueeze(-1), dims=(1, 2)).squeeze()\n        u = torch.arange(0, w.shape[0])\n        ws = w[u.unsqueeze(1), indp2]\n        bs2 = - ws * d[u.unsqueeze(1), indp2]\n\n        s = torch.cumsum(ws.abs(), dim=1)\n        sb = torch.cumsum(bs2, dim=1) + b0.unsqueeze(1)\n\n        c = b - b1 > 0\n        b2 = sb[u, -1] - s[u, -1] * p[u, indp[u, 0]]\n        c_l = (b - b2 > 0).nonzero().squeeze()\n        c2 = ((b - b1 > 0) * (b - b2 <= 0)).nonzero().squeeze()\n        c_l = self.check_shape(c_l)\n        c2 = self.check_shape(c2)\n\n        lb = torch.zeros(c2.shape[0])\n        ub = torch.ones(c2.shape[0]) * (w.shape[1] - 1)\n        nitermax = torch.ceil(torch.log2(torch.tensor(w.shape[1]).float()))\n        counter2 = torch.zeros(lb.shape).long()\n\n        while counter < nitermax:\n            counter4 = torch.floor((lb + ub) / 2)\n            counter2 = counter4.long()\n            indcurr = indp[c2, -counter2 - 1]\n            b2 = sb[c2, counter2] - s[c2, counter2] * p[c2, indcurr]\n            c = b[c2] - b2 > 0\n            ind3 = c.nonzero().squeeze()\n            ind32 = (~c).nonzero().squeeze()\n            ind3 = self.check_shape(ind3)\n            ind32 = self.check_shape(ind32)\n            lb[ind3] = counter4[ind3]\n            ub[ind32] = counter4[ind32]\n            counter += 1\n\n        lb = lb.long()\n        counter2 = 0\n\n        if c_l.nelement() != 0:\n            lmbd_opt = (torch.max((b[c_l] - sb[c_l, -1]) / (-s[c_l, -1]),\n                                  torch.zeros(sb[c_l, -1].shape)\n                                  .to(self.device))).unsqueeze(-1)\n            d[c_l] = (2 * a[c_l] - 1) * lmbd_opt\n\n        lmbd_opt = (torch.max((b[c2] - sb[c2, lb]) / (-s[c2, lb]),\n                              torch.zeros(sb[c2, lb].shape)\n                              .to(self.device))).unsqueeze(-1)\n        d[c2] = torch.min(lmbd_opt, d[c2]) * c5[c2]\\\n            + torch.max(-lmbd_opt, d[c2]) * (1 - c5[c2])\n\n        return d * (w != 0).float()\n\n    def projection_l2(self, points_to_project, w_hyperplane, b_hyperplane):\n        t = points_to_project.clone()\n        w = w_hyperplane.clone()\n        b = b_hyperplane.clone()\n\n        c = (w * t).sum(1) - b\n        ind2 = (c < 0).nonzero().squeeze()\n        ind2 = self.check_shape(ind2)\n        w[ind2] *= -1\n        c[ind2] *= -1\n\n        u = torch.arange(0, w.shape[0]).unsqueeze(1)\n\n        r = torch.max(t / w, (t - 1) / w)\n        u2 = torch.ones(r.shape).to(self.device)\n        r = torch.min(r, 1e12 * u2)\n        r = torch.max(r, -1e12 * u2)\n        r[w.abs() < 1e-8] = 1e12\n        r[r == -1e12] = -r[r == -1e12]\n        rs, indr = torch.sort(r, dim=1)\n        rs2 = torch.cat((rs[:, 1:],\n                         torch.zeros(rs.shape[0], 1).to(self.device)), 1)\n        rs[rs == 1e12] = 0\n        rs2[rs2 == 1e12] = 0\n\n        w3 = w ** 2\n        w3s = w3[u, indr]\n        w5 = w3s.sum(dim=1, keepdim=True)\n        ws = w5 - torch.cumsum(w3s, dim=1)\n        d = -(r * w).clone()\n        d = d * (w.abs() > 1e-8).float()\n        s = torch.cat(((-w5.squeeze() * rs[:, 0]).unsqueeze(1),\n                       torch.cumsum((-rs2 + rs) * ws, dim=1) -\n                       w5 * rs[:, 0].unsqueeze(-1)), 1)\n\n        c4 = (s[:, 0] + c < 0)\n        c3 = ((d * w).sum(dim=1) + c > 0)\n        c6 = c4.nonzero().squeeze()\n        c2 = ((1 - c4.float()) * (1 - c3.float())).nonzero().squeeze()\n        c6 = self.check_shape(c6)\n        c2 = self.check_shape(c2)\n\n        counter = 0\n        lb = torch.zeros(c2.shape[0])\n        ub = torch.ones(c2.shape[0]) * (w.shape[1] - 1)\n        nitermax = torch.ceil(torch.log2(torch.tensor(w.shape[1]).float()))\n        counter2 = torch.zeros(lb.shape).long()\n\n        while counter < nitermax:\n            counter4 = torch.floor((lb + ub) / 2)\n            counter2 = counter4.long()\n            c3 = s[c2, counter2] + c[c2] > 0\n            ind3 = c3.nonzero().squeeze()\n            ind32 = (~c3).nonzero().squeeze()\n            ind3 = self.check_shape(ind3)\n            ind32 = self.check_shape(ind32)\n            lb[ind3] = counter4[ind3]\n            ub[ind32] = counter4[ind32]\n            counter += 1\n\n        lb = lb.long()\n        alpha = torch.zeros([1])\n\n        if c6.nelement() != 0:\n            alpha = c[c6] / w5[c6].squeeze(-1)\n            d[c6] = -alpha.unsqueeze(-1) * w[c6]\n\n        if c2.nelement() != 0:\n            alpha = (s[c2, lb] + c[c2]) / ws[c2, lb] + rs[c2, lb]\n            if torch.sum(ws[c2, lb] == 0) > 0:\n                ind = (ws[c2, lb] == 0).nonzero().squeeze().long()\n                ind = self.check_shape(ind)\n                alpha[ind] = 0\n            c5 = (alpha.unsqueeze(-1) > r[c2]).float()\n            d[c2] = d[c2] * c5 - alpha.unsqueeze(-1) * w[c2] * (1 - c5)\n\n        return d * (w.abs() > 1e-8).float()\n\n    def projection_l1(self, points_to_project, w_hyperplane, b_hyperplane):\n        t = points_to_project.clone()\n        w = w_hyperplane.clone()\n        b = b_hyperplane.clone()\n\n        c = (w * t).sum(1) - b\n        ind2 = (c < 0).nonzero().squeeze()\n        ind2 = self.check_shape(ind2)\n        w[ind2] *= -1\n        c[ind2] *= -1\n\n        r = torch.max(1 / w, -1 / w)\n        r = torch.min(r, 1e12 * torch.ones(r.shape).to(self.device))\n        rs, indr = torch.sort(r, dim=1)\n        _, indr_rev = torch.sort(indr)\n\n        u = torch.arange(0, w.shape[0]).unsqueeze(1)\n        u2 = torch.arange(0, w.shape[1]).repeat(w.shape[0], 1)\n        c6 = (w < 0).float()\n        d = (-t + c6) * (w != 0).float()\n        d2 = torch.min(-w * t, w * (1 - t))\n        ds = d2[u, indr]\n        ds2 = torch.cat((c.unsqueeze(-1), ds), 1)\n        s = torch.cumsum(ds2, dim=1)\n\n        c4 = s[:, -1] < 0\n        c2 = c4.nonzero().squeeze(-1)\n        c2 = self.check_shape(c2)\n\n        counter = 0\n        lb = torch.zeros(c2.shape[0])\n        ub = torch.ones(c2.shape[0]) * (s.shape[1])\n        nitermax = torch.ceil(torch.log2(torch.tensor(s.shape[1]).float()))\n        counter2 = torch.zeros(lb.shape).long()\n\n        while counter < nitermax:\n            counter4 = torch.floor((lb + ub) / 2)\n            counter2 = counter4.long()\n            c3 = s[c2, counter2] > 0\n            ind3 = c3.nonzero().squeeze()\n            ind32 = (~c3).nonzero().squeeze()\n            ind3 = self.check_shape(ind3)\n            ind32 = self.check_shape(ind32)\n            lb[ind3] = counter4[ind3]\n            ub[ind32] = counter4[ind32]\n            counter += 1\n\n        lb2 = lb.long()\n\n        if c2.nelement() != 0:\n            alpha = -s[c2, lb2] / w[c2, indr[c2, lb2]]\n            c5 = u2[c2].float() < lb.unsqueeze(-1).float()\n            u3 = c5[u[:c5.shape[0]], indr_rev[c2]]\n            d[c2] = d[c2] * u3.float().to(self.device)\n            d[c2, indr[c2, lb2]] = alpha\n\n        return d * (w.abs() > 1e-8).float()\n\n    def perturb(self, x, y=None):\n        """"""\n        :param x:    clean images\n        :param y:    clean labels, if None we use the predicted labels\n        """"""\n\n        self.device = x.device\n        self.orig_dim = list(x.shape[1:])\n        self.ndims = len(self.orig_dim)\n\n        x = x.detach().clone().float().to(self.device)\n        # assert next(self.predict.parameters()).device == x.device\n\n        y_pred = self._get_predicted_label(x)\n        if y is None:\n            y = y_pred.detach().clone().long().to(self.device)\n        else:\n            y = y.detach().clone().long().to(self.device)\n        pred = y_pred == y\n        corr_classified = pred.float().sum()\n        if self.verbose:\n            print(\'Clean accuracy: {:.2%}\'.format(pred.float().mean()))\n        if pred.sum() == 0:\n            return x\n        pred = self.check_shape(pred.nonzero().squeeze())\n\n        startt = time.time()\n        # runs the attack only on correctly classified points\n        im2 = replicate_input(x[pred])\n        la2 = replicate_input(y[pred])\n        if len(im2.shape) == self.ndims:\n            im2 = im2.unsqueeze(0)\n        bs = im2.shape[0]\n        u1 = torch.arange(bs)\n        adv = im2.clone()\n        adv_c = x.clone()\n        res2 = 1e10 * torch.ones([bs]).to(self.device)\n        res_c = torch.zeros([x.shape[0]]).to(self.device)\n        x1 = im2.clone()\n        x0 = im2.clone().reshape([bs, -1])\n        counter_restarts = 0\n\n        while counter_restarts < self.n_restarts:\n            if counter_restarts > 0:\n                if self.norm == \'Linf\':\n                    t = 2 * torch.rand(x1.shape).to(self.device) - 1\n                    x1 = im2 + (\n                        torch.min(\n                            res2,\n                            self.eps * torch.ones(res2.shape).to(self.device)\n                        ).reshape([-1, *([1] * self.ndims)])\n                    ) * t / (t.reshape([t.shape[0], -1]).abs()\n                             .max(dim=1, keepdim=True)[0]\n                             .reshape([-1, *([1] * self.ndims)])) * .5\n                elif self.norm == \'L2\':\n                    t = torch.randn(x1.shape).to(self.device)\n                    x1 = im2 + (\n                        torch.min(\n                            res2,\n                            self.eps * torch.ones(res2.shape).to(self.device)\n                        ).reshape([-1, *([1] * self.ndims)])\n                    ) * t / ((t ** 2)\n                             .view(t.shape[0], -1)\n                             .sum(dim=-1)\n                             .sqrt()\n                             .view(t.shape[0], *([1] * self.ndims))) * .5\n                elif self.norm == \'L1\':\n                    t = torch.randn(x1.shape).to(self.device)\n                    x1 = im2 + (torch.min(\n                        res2,\n                        self.eps * torch.ones(res2.shape).to(self.device)\n                    ).reshape([-1, *([1] * self.ndims)])\n                    ) * t / (t.abs().view(t.shape[0], -1)\n                             .sum(dim=-1)\n                             .view(t.shape[0], *([1] * self.ndims))) / 2\n\n                x1 = x1.clamp(0.0, 1.0)\n\n            counter_iter = 0\n            while counter_iter < self.n_iter:\n                with torch.no_grad():\n                    df, dg = self.get_diff_logits_grads_batch(x1, la2)\n                    if self.norm == \'Linf\':\n                        dist1 = df.abs() / (1e-12 +\n                                            dg.abs()\n                                            .view(dg.shape[0], dg.shape[1], -1)\n                                            .sum(dim=-1))\n                    elif self.norm == \'L2\':\n                        dist1 = df.abs() / (1e-12 + (dg ** 2)\n                                            .view(dg.shape[0], dg.shape[1], -1)\n                                            .sum(dim=-1).sqrt())\n                    elif self.norm == \'L1\':\n                        dist1 = df.abs() / (1e-12 + dg.abs().reshape(\n                            [df.shape[0], df.shape[1], -1]).max(dim=2)[0])\n                    else:\n                        raise ValueError(\'norm not supported\')\n                    ind = dist1.min(dim=1)[1]\n                    dg2 = dg[u1, ind]\n                    b = (- df[u1, ind] +\n                         (dg2 * x1).view(x1.shape[0], -1).sum(dim=-1))\n                    w = dg2.reshape([bs, -1])\n\n                    if self.norm == \'Linf\':\n                        d3 = self.projection_linf(\n                            torch.cat((x1.reshape([bs, -1]), x0), 0),\n                            torch.cat((w, w), 0),\n                            torch.cat((b, b), 0))\n                    elif self.norm == \'L2\':\n                        d3 = self.projection_l2(\n                            torch.cat((x1.reshape([bs, -1]), x0), 0),\n                            torch.cat((w, w), 0),\n                            torch.cat((b, b), 0))\n                    elif self.norm == \'L1\':\n                        d3 = self.projection_l1(\n                            torch.cat((x1.reshape([bs, -1]), x0), 0),\n                            torch.cat((w, w), 0),\n                            torch.cat((b, b), 0))\n                    d1 = torch.reshape(d3[:bs], x1.shape)\n                    d2 = torch.reshape(d3[-bs:], x1.shape)\n                    if self.norm == \'Linf\':\n                        a0 = d3.abs().max(dim=1, keepdim=True)[0]\\\n                            .view(-1, *([1] * self.ndims))\n                    elif self.norm == \'L2\':\n                        a0 = (d3 ** 2).sum(dim=1, keepdim=True).sqrt()\\\n                            .view(-1, *([1] * self.ndims))\n                    elif self.norm == \'L1\':\n                        a0 = d3.abs().sum(dim=1, keepdim=True)\\\n                            .view(-1, *([1] * self.ndims))\n                    a0 = torch.max(a0, 1e-8 * torch.ones(\n                        a0.shape).to(self.device))\n                    a1 = a0[:bs]\n                    a2 = a0[-bs:]\n                    alpha = torch.min(torch.max(a1 / (a1 + a2),\n                                                torch.zeros(a1.shape)\n                                                .to(self.device))[0],\n                                      self.alpha_max * torch.ones(a1.shape)\n                                      .to(self.device))\n                    x1 = ((x1 + self.eta * d1) * (1 - alpha) +\n                          (im2 + d2 * self.eta) * alpha).clamp(0.0, 1.0)\n\n                    is_adv = self._get_predicted_label(x1) != la2\n\n                    if is_adv.sum() > 0:\n                        ind_adv = is_adv.nonzero().squeeze()\n                        ind_adv = self.check_shape(ind_adv)\n                        if self.norm == \'Linf\':\n                            t = (x1[ind_adv] - im2[ind_adv]).reshape(\n                                [ind_adv.shape[0], -1]).abs().max(dim=1)[0]\n                        elif self.norm == \'L2\':\n                            t = ((x1[ind_adv] - im2[ind_adv]) ** 2)\\\n                                .view(ind_adv.shape[0], -1).sum(dim=-1).sqrt()\n                        elif self.norm == \'L1\':\n                            t = (x1[ind_adv] - im2[ind_adv])\\\n                                .abs().view(ind_adv.shape[0], -1).sum(dim=-1)\n                        adv[ind_adv] = x1[ind_adv] * (t < res2[ind_adv]).\\\n                            float().reshape([-1, *([1] * self.ndims)]) \\\n                            + adv[ind_adv]\\\n                            * (t >= res2[ind_adv]).float().reshape(\n                            [-1, *([1] * self.ndims)])\n                        res2[ind_adv] = t * (t < res2[ind_adv]).float()\\\n                            + res2[ind_adv] * (t >= res2[ind_adv]).float()\n                        x1[ind_adv] = im2[ind_adv] + (\n                            x1[ind_adv] - im2[ind_adv]) * self.beta\n\n                    counter_iter += 1\n\n            counter_restarts += 1\n\n        ind_succ = res2 < 1e10\n        if self.verbose:\n            print(\'success rate: {:.0f}/{:.0f}\'\n                  .format(ind_succ.float().sum(), corr_classified) +\n                  \' (on correctly classified points) in {:.1f} s\'\n                  .format(time.time() - startt))\n\n        res_c[pred] = res2 * ind_succ.float() + 1e10 * (1 - ind_succ.float())\n        ind_succ = self.check_shape(ind_succ.nonzero().squeeze())\n        adv_c[pred[ind_succ]] = adv[ind_succ].clone()\n\n        return adv_c\n\n\nclass LinfFABAttack(FABAttack):\n    """"""\n    Linf - Fast Adaptive Boundary Attack\n    https://arxiv.org/abs/1907.02044\n\n    :param predict:       forward pass function\n    :param n_restarts:    number of random restarts\n    :param n_iter:        number of iterations\n    :param eps:           epsilon for the random restarts\n    :param alpha_max:     alpha_max\n    :param eta:           overshooting\n    :param beta:          backward step\n    :param device:        device to use (\'cuda\' or \'cpu\')\n    """"""\n\n    def __init__(\n            self,\n            predict,\n            n_restarts=1,\n            n_iter=100,\n            eps=None,\n            alpha_max=0.1,\n            eta=1.05,\n            beta=0.9,\n            loss_fn=None,\n            verbose=False,\n    ):\n        norm = \'Linf\'\n        super(LinfFABAttack, self).__init__(\n            predict=predict, norm=norm, n_restarts=n_restarts,\n            n_iter=n_iter, eps=eps, alpha_max=alpha_max, eta=eta, beta=beta,\n            loss_fn=loss_fn, verbose=verbose)\n\n\nclass L2FABAttack(FABAttack):\n    """"""\n    L2 - Fast Adaptive Boundary Attack\n    https://arxiv.org/abs/1907.02044\n\n    :param predict:       forward pass function\n    :param n_restarts:    number of random restarts\n    :param n_iter:        number of iterations\n    :param eps:           epsilon for the random restarts\n    :param alpha_max:     alpha_max\n    :param eta:           overshooting\n    :param beta:          backward step\n    :param device:        device to use (\'cuda\' or \'cpu\')\n    """"""\n\n    def __init__(\n            self,\n            predict,\n            n_restarts=1,\n            n_iter=100,\n            eps=None,\n            alpha_max=0.1,\n            eta=1.05,\n            beta=0.9,\n            loss_fn=None,\n            verbose=False,\n    ):\n        norm = \'L2\'\n        super(L2FABAttack, self).__init__(\n            predict=predict, norm=norm, n_restarts=n_restarts,\n            n_iter=n_iter, eps=eps, alpha_max=alpha_max, eta=eta, beta=beta,\n            loss_fn=loss_fn, verbose=verbose)\n\n\nclass L1FABAttack(FABAttack):\n    """"""\n    L1 - Fast Adaptive Boundary Attack\n    https://arxiv.org/abs/1907.02044\n\n    :param predict:       forward pass function\n    :param n_restarts:    number of random restarts\n    :param n_iter:        number of iterations\n    :param eps:           epsilon for the random restarts\n    :param alpha_max:     alpha_max\n    :param eta:           overshooting\n    :param beta:          backward step\n    :param device:        device to use (\'cuda\' or \'cpu\')\n    """"""\n\n    def __init__(\n            self,\n            predict,\n            n_restarts=1,\n            n_iter=100,\n            eps=None,\n            alpha_max=0.1,\n            eta=1.05,\n            beta=0.9,\n            loss_fn=None,\n            verbose=False,\n    ):\n        norm = \'L1\'\n        super(L1FABAttack, self).__init__(\n            predict=predict, norm=norm, n_restarts=n_restarts,\n            n_iter=n_iter, eps=eps, alpha_max=alpha_max, eta=eta, beta=beta,\n            loss_fn=loss_fn, verbose=verbose)\n'"
advertorch/attacks/iterative_projected_gradient.py,17,"b'# Copyright (c) 2018-present, Royal Bank of Canada and other authors.\n# See the AUTHORS.txt file for a list of contributors.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom advertorch.utils import clamp\nfrom advertorch.utils import normalize_by_pnorm\nfrom advertorch.utils import clamp_by_pnorm\nfrom advertorch.utils import is_float_or_torch_tensor\nfrom advertorch.utils import batch_multiply\nfrom advertorch.utils import batch_clamp\nfrom advertorch.utils import replicate_input\nfrom advertorch.utils import batch_l1_proj\n\nfrom .base import Attack\nfrom .base import LabelMixin\nfrom .utils import rand_init_delta\n\n\ndef perturb_iterative(xvar, yvar, predict, nb_iter, eps, eps_iter, loss_fn,\n                      delta_init=None, minimize=False, ord=np.inf,\n                      clip_min=0.0, clip_max=1.0,\n                      l1_sparsity=None):\n    """"""\n    Iteratively maximize the loss over the input. It is a shared method for\n    iterative attacks including IterativeGradientSign, LinfPGD, etc.\n\n    :param xvar: input data.\n    :param yvar: input labels.\n    :param predict: forward pass function.\n    :param nb_iter: number of iterations.\n    :param eps: maximum distortion.\n    :param eps_iter: attack step size.\n    :param loss_fn: loss function.\n    :param delta_init: (optional) tensor contains the random initialization.\n    :param minimize: (optional bool) whether to minimize or maximize the loss.\n    :param ord: (optional) the order of maximum distortion (inf or 2).\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param l1_sparsity: sparsity value for L1 projection.\n                  - if None, then perform regular L1 projection.\n                  - if float value, then perform sparse L1 descent from\n                    Algorithm 1 in https://arxiv.org/pdf/1904.13000v1.pdf\n    :return: tensor containing the perturbed input.\n    """"""\n    if delta_init is not None:\n        delta = delta_init\n    else:\n        delta = torch.zeros_like(xvar)\n\n    delta.requires_grad_()\n    for ii in range(nb_iter):\n        outputs = predict(xvar + delta)\n        loss = loss_fn(outputs, yvar)\n        if minimize:\n            loss = -loss\n\n        loss.backward()\n        if ord == np.inf:\n            grad_sign = delta.grad.data.sign()\n            delta.data = delta.data + batch_multiply(eps_iter, grad_sign)\n            delta.data = batch_clamp(eps, delta.data)\n            delta.data = clamp(xvar.data + delta.data, clip_min, clip_max\n                               ) - xvar.data\n\n        elif ord == 2:\n            grad = delta.grad.data\n            grad = normalize_by_pnorm(grad)\n            delta.data = delta.data + batch_multiply(eps_iter, grad)\n            delta.data = clamp(xvar.data + delta.data, clip_min, clip_max\n                               ) - xvar.data\n            if eps is not None:\n                delta.data = clamp_by_pnorm(delta.data, ord, eps)\n\n        elif ord == 1:\n            grad = delta.grad.data\n            abs_grad = torch.abs(grad)\n\n            batch_size = grad.size(0)\n            view = abs_grad.view(batch_size, -1)\n            view_size = view.size(1)\n            if l1_sparsity is None:\n                vals, idx = view.topk(1)\n            else:\n                vals, idx = view.topk(\n                    int(np.round((1 - l1_sparsity) * view_size)))\n\n            out = torch.zeros_like(view).scatter_(1, idx, vals)\n            out = out.view_as(grad)\n            grad = grad.sign() * (out > 0).float()\n            grad = normalize_by_pnorm(grad, p=1)\n            delta.data = delta.data + batch_multiply(eps_iter, grad)\n\n            delta.data = batch_l1_proj(delta.data.cpu(), eps)\n            if xvar.is_cuda:\n                delta.data = delta.data.cuda()\n            delta.data = clamp(xvar.data + delta.data, clip_min, clip_max\n                               ) - xvar.data\n        else:\n            error = ""Only ord = inf, ord = 1 and ord = 2 have been implemented""\n            raise NotImplementedError(error)\n        delta.grad.data.zero_()\n\n    x_adv = clamp(xvar + delta, clip_min, clip_max)\n    return x_adv\n\n\nclass PGDAttack(Attack, LabelMixin):\n    """"""\n    The projected gradient descent attack (Madry et al, 2017).\n    The attack performs nb_iter steps of size eps_iter, while always staying\n    within eps from the initial point.\n    Paper: https://arxiv.org/pdf/1706.06083.pdf\n\n    :param predict: forward pass function.\n    :param loss_fn: loss function.\n    :param eps: maximum distortion.\n    :param nb_iter: number of iterations.\n    :param eps_iter: attack step size.\n    :param rand_init: (optional bool) random initialization.\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param ord: (optional) the order of maximum distortion (inf or 2).\n    :param targeted: if the attack is targeted.\n    """"""\n\n    def __init__(\n            self, predict, loss_fn=None, eps=0.3, nb_iter=40,\n            eps_iter=0.01, rand_init=True, clip_min=0., clip_max=1.,\n            ord=np.inf, l1_sparsity=None, targeted=False):\n        """"""\n        Create an instance of the PGDAttack.\n\n        """"""\n        super(PGDAttack, self).__init__(\n            predict, loss_fn, clip_min, clip_max)\n        self.eps = eps\n        self.nb_iter = nb_iter\n        self.eps_iter = eps_iter\n        self.rand_init = rand_init\n        self.ord = ord\n        self.targeted = targeted\n        if self.loss_fn is None:\n            self.loss_fn = nn.CrossEntropyLoss(reduction=""sum"")\n        self.l1_sparsity = l1_sparsity\n        assert is_float_or_torch_tensor(self.eps_iter)\n        assert is_float_or_torch_tensor(self.eps)\n\n    def perturb(self, x, y=None):\n        """"""\n        Given examples (x, y), returns their adversarial counterparts with\n        an attack length of eps.\n\n        :param x: input tensor.\n        :param y: label tensor.\n                  - if None and self.targeted=False, compute y as predicted\n                    labels.\n                  - if self.targeted=True, then y must be the targeted labels.\n        :return: tensor containing perturbed inputs.\n        """"""\n        x, y = self._verify_and_process_inputs(x, y)\n\n        delta = torch.zeros_like(x)\n        delta = nn.Parameter(delta)\n        if self.rand_init:\n            rand_init_delta(\n                delta, x, self.ord, self.eps, self.clip_min, self.clip_max)\n            delta.data = clamp(\n                x + delta.data, min=self.clip_min, max=self.clip_max) - x\n\n        rval = perturb_iterative(\n            x, y, self.predict, nb_iter=self.nb_iter,\n            eps=self.eps, eps_iter=self.eps_iter,\n            loss_fn=self.loss_fn, minimize=self.targeted,\n            ord=self.ord, clip_min=self.clip_min,\n            clip_max=self.clip_max, delta_init=delta,\n            l1_sparsity=self.l1_sparsity,\n        )\n\n        return rval.data\n\n\nclass LinfPGDAttack(PGDAttack):\n    """"""\n    PGD Attack with order=Linf\n\n    :param predict: forward pass function.\n    :param loss_fn: loss function.\n    :param eps: maximum distortion.\n    :param nb_iter: number of iterations.\n    :param eps_iter: attack step size.\n    :param rand_init: (optional bool) random initialization.\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param targeted: if the attack is targeted.\n    """"""\n\n    def __init__(\n            self, predict, loss_fn=None, eps=0.3, nb_iter=40,\n            eps_iter=0.01, rand_init=True, clip_min=0., clip_max=1.,\n            targeted=False):\n        ord = np.inf\n        super(LinfPGDAttack, self).__init__(\n            predict=predict, loss_fn=loss_fn, eps=eps, nb_iter=nb_iter,\n            eps_iter=eps_iter, rand_init=rand_init, clip_min=clip_min,\n            clip_max=clip_max, targeted=targeted,\n            ord=ord)\n\n\nclass L2PGDAttack(PGDAttack):\n    """"""\n    PGD Attack with order=L2\n\n    :param predict: forward pass function.\n    :param loss_fn: loss function.\n    :param eps: maximum distortion.\n    :param nb_iter: number of iterations.\n    :param eps_iter: attack step size.\n    :param rand_init: (optional bool) random initialization.\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param targeted: if the attack is targeted.\n    """"""\n\n    def __init__(\n            self, predict, loss_fn=None, eps=0.3, nb_iter=40,\n            eps_iter=0.01, rand_init=True, clip_min=0., clip_max=1.,\n            targeted=False):\n        ord = 2\n        super(L2PGDAttack, self).__init__(\n            predict=predict, loss_fn=loss_fn, eps=eps, nb_iter=nb_iter,\n            eps_iter=eps_iter, rand_init=rand_init, clip_min=clip_min,\n            clip_max=clip_max, targeted=targeted,\n            ord=ord)\n\n\nclass L1PGDAttack(PGDAttack):\n    """"""\n    PGD Attack with order=L1\n\n    :param predict: forward pass function.\n    :param loss_fn: loss function.\n    :param eps: maximum distortion.\n    :param nb_iter: number of iterations.\n    :param eps_iter: attack step size.\n    :param rand_init: (optional bool) random initialization.\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param targeted: if the attack is targeted.\n    """"""\n\n    def __init__(\n            self, predict, loss_fn=None, eps=10., nb_iter=40,\n            eps_iter=0.01, rand_init=True, clip_min=0., clip_max=1.,\n            targeted=False):\n        ord = 1\n        super(L1PGDAttack, self).__init__(\n            predict=predict, loss_fn=loss_fn, eps=eps, nb_iter=nb_iter,\n            eps_iter=eps_iter, rand_init=rand_init, clip_min=clip_min,\n            clip_max=clip_max, targeted=targeted,\n            ord=ord, l1_sparsity=None)\n\n\nclass SparseL1DescentAttack(PGDAttack):\n    """"""\n    SparseL1Descent Attack\n\n    :param predict: forward pass function.\n    :param loss_fn: loss function.\n    :param eps: maximum distortion.\n    :param nb_iter: number of iterations.\n    :param eps_iter: attack step size.\n    :param rand_init: (optional bool) random initialization.\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param targeted: if the attack is targeted.\n    :param l1_sparsity: proportion of zeros in gradient updates\n    """"""\n\n    def __init__(\n            self, predict, loss_fn=None, eps=0.3, nb_iter=40,\n            eps_iter=0.01, rand_init=False, clip_min=0., clip_max=1.,\n            l1_sparsity=0.95, targeted=False):\n        ord = 1\n        super(SparseL1DescentAttack, self).__init__(\n            predict=predict, loss_fn=loss_fn, eps=eps, nb_iter=nb_iter,\n            eps_iter=eps_iter, rand_init=rand_init, clip_min=clip_min,\n            clip_max=clip_max, targeted=targeted,\n            ord=ord, l1_sparsity=l1_sparsity)\n\n\nclass L2BasicIterativeAttack(PGDAttack):\n    """"""Like GradientAttack but with several steps for each epsilon.\n\n    :param predict: forward pass function.\n    :param loss_fn: loss function.\n    :param eps: maximum distortion.\n    :param nb_iter: number of iterations.\n    :param eps_iter: attack step size.\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param targeted: if the attack is targeted.\n    """"""\n\n    def __init__(self, predict, loss_fn=None, eps=0.1, nb_iter=10,\n                 eps_iter=0.05, clip_min=0., clip_max=1., targeted=False):\n        ord = 2\n        rand_init = False\n        l1_sparsity = None\n        super(L2BasicIterativeAttack, self).__init__(\n            predict, loss_fn, eps, nb_iter, eps_iter, rand_init,\n            clip_min, clip_max, ord, l1_sparsity, targeted)\n\n\nclass LinfBasicIterativeAttack(PGDAttack):\n    """"""\n    Like GradientSignAttack but with several steps for each epsilon.\n    Aka Basic Iterative Attack.\n    Paper: https://arxiv.org/pdf/1611.01236.pdf\n\n    :param predict: forward pass function.\n    :param loss_fn: loss function.\n    :param eps: maximum distortion.\n    :param nb_iter: number of iterations.\n    :param eps_iter: attack step size.\n    :param rand_init: (optional bool) random initialization.\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param targeted: if the attack is targeted.\n    """"""\n\n    def __init__(self, predict, loss_fn=None, eps=0.1, nb_iter=10,\n                 eps_iter=0.05, clip_min=0., clip_max=1., targeted=False):\n        ord = np.inf\n        rand_init = False\n        l1_sparsity = None\n        super(LinfBasicIterativeAttack, self).__init__(\n            predict, loss_fn, eps, nb_iter, eps_iter, rand_init,\n            clip_min, clip_max, ord, l1_sparsity, targeted)\n\n\nclass MomentumIterativeAttack(Attack, LabelMixin):\n    """"""\n    The Momentum Iterative Attack (Dong et al. 2017).\n\n    The attack performs nb_iter steps of size eps_iter, while always staying\n    within eps from the initial point. The optimization is performed with\n    momentum.\n    Paper: https://arxiv.org/pdf/1710.06081.pdf\n\n    :param predict: forward pass function.\n    :param loss_fn: loss function.\n    :param eps: maximum distortion.\n    :param nb_iter: number of iterations\n    :param decay_factor: momentum decay factor.\n    :param eps_iter: attack step size.\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param targeted: if the attack is targeted.\n    :param ord: the order of maximum distortion (inf or 2).\n    """"""\n\n    def __init__(\n            self, predict, loss_fn=None, eps=0.3, nb_iter=40, decay_factor=1.,\n            eps_iter=0.01, clip_min=0., clip_max=1., targeted=False,\n            ord=np.inf):\n        """"""Create an instance of the MomentumIterativeAttack.""""""\n        super(MomentumIterativeAttack, self).__init__(\n            predict, loss_fn, clip_min, clip_max)\n        self.eps = eps\n        self.nb_iter = nb_iter\n        self.decay_factor = decay_factor\n        self.eps_iter = eps_iter\n        self.targeted = targeted\n        self.ord = ord\n        if self.loss_fn is None:\n            self.loss_fn = nn.CrossEntropyLoss(reduction=""sum"")\n\n    def perturb(self, x, y=None):\n        """"""\n        Given examples (x, y), returns their adversarial counterparts with\n        an attack length of eps.\n\n        :param x: input tensor.\n        :param y: label tensor.\n                  - if None and self.targeted=False, compute y as predicted\n                    labels.\n                  - if self.targeted=True, then y must be the targeted labels.\n        :return: tensor containing perturbed inputs.\n        """"""\n        x, y = self._verify_and_process_inputs(x, y)\n\n        delta = torch.zeros_like(x)\n        g = torch.zeros_like(x)\n\n        delta = nn.Parameter(delta)\n\n        for i in range(self.nb_iter):\n\n            if delta.grad is not None:\n                delta.grad.detach_()\n                delta.grad.zero_()\n\n            imgadv = x + delta\n            outputs = self.predict(imgadv)\n            loss = self.loss_fn(outputs, y)\n            if self.targeted:\n                loss = -loss\n            loss.backward()\n\n            g = self.decay_factor * g + normalize_by_pnorm(\n                delta.grad.data, p=1)\n            # according to the paper it should be .sum(), but in their\n            #   implementations (both cleverhans and the link from the paper)\n            #   it is .mean(), but actually it shouldn\'t matter\n            if self.ord == np.inf:\n                delta.data += self.eps_iter * torch.sign(g)\n                delta.data = clamp(\n                    delta.data, min=-self.eps, max=self.eps)\n                delta.data = clamp(\n                    x + delta.data, min=self.clip_min, max=self.clip_max) - x\n            elif self.ord == 2:\n                delta.data += self.eps_iter * normalize_by_pnorm(g, p=2)\n                delta.data *= clamp(\n                    (self.eps * normalize_by_pnorm(delta.data, p=2) /\n                        delta.data),\n                    max=1.)\n                delta.data = clamp(\n                    x + delta.data, min=self.clip_min, max=self.clip_max) - x\n            else:\n                error = ""Only ord = inf and ord = 2 have been implemented""\n                raise NotImplementedError(error)\n\n        rval = x + delta.data\n        return rval\n\n\nclass L2MomentumIterativeAttack(MomentumIterativeAttack):\n    """"""\n    The L2 Momentum Iterative Attack\n    Paper: https://arxiv.org/pdf/1710.06081.pdf\n\n    :param predict: forward pass function.\n    :param loss_fn: loss function.\n    :param eps: maximum distortion.\n    :param nb_iter: number of iterations\n    :param decay_factor: momentum decay factor.\n    :param eps_iter: attack step size.\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param targeted: if the attack is targeted.\n    """"""\n\n    def __init__(\n            self, predict, loss_fn=None, eps=0.3, nb_iter=40, decay_factor=1.,\n            eps_iter=0.01, clip_min=0., clip_max=1., targeted=False):\n        """"""Create an instance of the MomentumIterativeAttack.""""""\n        ord = 2\n        super(L2MomentumIterativeAttack, self).__init__(\n            predict, loss_fn, eps, nb_iter, decay_factor,\n            eps_iter, clip_min, clip_max, targeted, ord)\n\n\nclass LinfMomentumIterativeAttack(MomentumIterativeAttack):\n    """"""\n    The Linf Momentum Iterative Attack\n    Paper: https://arxiv.org/pdf/1710.06081.pdf\n\n    :param predict: forward pass function.\n    :param loss_fn: loss function.\n    :param eps: maximum distortion.\n    :param nb_iter: number of iterations\n    :param decay_factor: momentum decay factor.\n    :param eps_iter: attack step size.\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param targeted: if the attack is targeted.\n    """"""\n\n    def __init__(\n            self, predict, loss_fn=None, eps=0.3, nb_iter=40, decay_factor=1.,\n            eps_iter=0.01, clip_min=0., clip_max=1., targeted=False):\n        """"""Create an instance of the MomentumIterativeAttack.""""""\n        ord = np.inf\n        super(LinfMomentumIterativeAttack, self).__init__(\n            predict, loss_fn, eps, nb_iter, decay_factor,\n            eps_iter, clip_min, clip_max, targeted, ord)\n\n\nclass FastFeatureAttack(Attack):\n    """"""\n    Fast attack against a target internal representation of a model using\n    gradient descent (Sabour et al. 2016).\n    Paper: https://arxiv.org/abs/1511.05122\n\n    :param predict: forward pass function.\n    :param loss_fn: loss function.\n    :param eps: maximum distortion.\n    :param eps_iter: attack step size.\n    :param nb_iter: number of iterations\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    """"""\n\n    def __init__(self, predict, loss_fn=None, eps=0.3, eps_iter=0.05,\n                 nb_iter=10, rand_init=True, clip_min=0., clip_max=1.):\n        """"""Create an instance of the FastFeatureAttack.""""""\n        super(FastFeatureAttack, self).__init__(\n            predict, loss_fn, clip_min, clip_max)\n        self.eps = eps\n        self.eps_iter = eps_iter\n        self.nb_iter = nb_iter\n        self.rand_init = rand_init\n        self.clip_min = clip_min\n        self.clip_max = clip_max\n        if self.loss_fn is None:\n            self.loss_fn = nn.MSELoss(reduction=""sum"")\n\n    def perturb(self, source, guide, delta=None):\n        """"""\n        Given source, returns their adversarial counterparts\n        with representations close to that of the guide.\n\n        :param source: input tensor which we want to perturb.\n        :param guide: targeted input.\n        :param delta: tensor contains the random initialization.\n        :return: tensor containing perturbed inputs.\n        """"""\n        # Initialization\n        if delta is None:\n            delta = torch.zeros_like(source)\n            if self.rand_init:\n                delta = delta.uniform_(-self.eps, self.eps)\n        else:\n            delta = delta.detach()\n\n        delta.requires_grad_()\n\n        source = replicate_input(source)\n        guide = replicate_input(guide)\n        guide_ftr = self.predict(guide).detach()\n\n        xadv = perturb_iterative(source, guide_ftr, self.predict,\n                                 self.nb_iter, eps_iter=self.eps_iter,\n                                 loss_fn=self.loss_fn, minimize=True,\n                                 ord=np.inf, eps=self.eps,\n                                 clip_min=self.clip_min,\n                                 clip_max=self.clip_max,\n                                 delta_init=delta)\n\n        xadv = clamp(xadv, self.clip_min, self.clip_max)\n\n        return xadv.data\n'"
advertorch/attacks/jsma.py,8,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\n\nimport numpy as np\nimport torch\n\nfrom advertorch.utils import clamp\nfrom advertorch.utils import jacobian\n\nfrom .base import Attack\nfrom .base import LabelMixin\n\n\nclass JacobianSaliencyMapAttack(Attack, LabelMixin):\n    """"""\n    Jacobian Saliency Map Attack\n    This includes Algorithm 1 and 3 in v1, https://arxiv.org/abs/1511.07528v1\n\n    :param predict: forward pass function.\n    :param num_classes: number of clasess.\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param gamma: highest percentage of pixels can be modified\n    :param theta: perturb length, range is either [theta, 0], [0, theta]\n\n    """"""\n\n    def __init__(self, predict, num_classes,\n                 clip_min=0.0, clip_max=1.0, loss_fn=None,\n                 theta=1.0, gamma=1.0, comply_cleverhans=False):\n        super(JacobianSaliencyMapAttack, self).__init__(\n            predict, loss_fn, clip_min, clip_max)\n        self.num_classes = num_classes\n        self.theta = theta\n        self.gamma = gamma\n        self.comply_cleverhans = comply_cleverhans\n        self.targeted = True\n\n\n    def _compute_forward_derivative(self, xadv, y):\n        jacobians = torch.stack([jacobian(self.predict, xadv, yadv)\n                                 for yadv in range(self.num_classes)])\n        grads = jacobians.view((jacobians.shape[0], jacobians.shape[1], -1))\n        grads_target = grads[y, range(len(y)), :]\n        grads_other = grads.sum(dim=0) - grads_target\n        return grads_target, grads_other\n\n\n    def _sum_pair(self, grads, dim_x):\n        return grads.view(-1, dim_x, 1) + grads.view(-1, 1, dim_x)\n\n    def _and_pair(self, cond, dim_x):\n        return cond.view(-1, dim_x, 1) & cond.view(-1, 1, dim_x)\n\n    def _saliency_map(self, search_space, grads_target, grads_other, y):\n\n        dim_x = search_space.shape[1]\n\n        # alpha in Algorithm 3 line 2\n        gradsum_target = self._sum_pair(grads_target, dim_x)\n        # alpha in Algorithm 3 line 3\n        gradsum_other = self._sum_pair(grads_other, dim_x)\n\n        if self.theta > 0:\n            scores_mask = (\n                torch.gt(gradsum_target, 0) & torch.lt(gradsum_other, 0))\n        else:\n            scores_mask = (\n                torch.lt(gradsum_target, 0) & torch.gt(gradsum_other, 0))\n\n        scores_mask &= self._and_pair(search_space.ne(0), dim_x)\n        scores_mask[:, range(dim_x), range(dim_x)] = 0\n\n        if self.comply_cleverhans:\n            valid = torch.ones(scores_mask.shape[0]).byte()\n        else:\n            valid = scores_mask.view(-1, dim_x * dim_x).any(dim=1)\n\n        scores = scores_mask.float() * (-gradsum_target * gradsum_other)\n        best = torch.max(scores.view(-1, dim_x * dim_x), 1)[1]\n        p1 = torch.remainder(best, dim_x)\n        p2 = (best / dim_x).long()\n        return p1, p2, valid\n\n    def _modify_xadv(self, xadv, batch_size, cond, p1, p2):\n        ori_shape = xadv.shape\n        xadv = xadv.view(batch_size, -1)\n        for idx in range(batch_size):\n            if cond[idx] != 0:\n                xadv[idx, p1[idx]] += self.theta\n                xadv[idx, p2[idx]] += self.theta\n        xadv = clamp(xadv, min=self.clip_min, max=self.clip_max)\n        xadv = xadv.view(ori_shape)\n        return xadv\n\n    def _update_search_space(self, search_space, p1, p2, cond):\n        for idx in range(len(cond)):\n            if cond[idx] != 0:\n                search_space[idx, p1[idx]] -= 1\n                search_space[idx, p2[idx]] -= 1\n\n    def perturb(self, x, y=None):\n        x, y = self._verify_and_process_inputs(x, y)\n        xadv = x\n        batch_size = x.shape[0]\n        dim_x = int(np.prod(x.shape[1:]))\n        max_iters = int(dim_x * self.gamma / 2)\n        search_space = x.new_ones(batch_size, dim_x).int()\n        curr_step = 0\n        yadv = self._get_predicted_label(xadv)\n\n        # Algorithm 1\n        while ((y != yadv).any() and curr_step < max_iters):\n\n            grads_target, grads_other = self._compute_forward_derivative(\n                xadv, y)\n\n            # Algorithm 3\n            p1, p2, valid = self._saliency_map(\n                search_space, grads_target, grads_other, y)\n\n            cond = (y != yadv) & valid\n\n            self._update_search_space(search_space, p1, p2, cond)\n\n            xadv = self._modify_xadv(xadv, batch_size, cond, p1, p2)\n            yadv = self._get_predicted_label(xadv)\n\n            curr_step += 1\n\n        xadv = clamp(xadv, min=self.clip_min, max=self.clip_max)\n        return xadv\n\n\nJSMA = JacobianSaliencyMapAttack\n'"
advertorch/attacks/lbfgs.py,8,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom advertorch.utils import calc_l2distsq\n\nfrom .base import Attack\nfrom .base import LabelMixin\n\n\nL2DIST_UPPER = 1e10\nCOEFF_UPPER = 1e10\nINVALID_LABEL = -1\nUPPER_CHECK = 1e9\n\n\nclass LBFGSAttack(Attack, LabelMixin):\n    """"""\n    The attack that uses L-BFGS to minimize the distance of the original\n    and perturbed images\n\n    :param predict: forward pass function.\n    :param num_classes: number of clasess.\n    :param batch_size: number of samples in the batch\n    :param binary_search_steps: number of binary search times to find the\n        optimum\n    :param max_iterations: the maximum number of iterations\n    :param initial_const: initial value of the constant c\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param loss_fn: loss function\n    :param targeted: if the attack is targeted.\n    """"""\n\n    def __init__(self, predict, num_classes, batch_size=1,\n                 binary_search_steps=9, max_iterations=100,\n                 initial_const=1e-2,\n                 clip_min=0, clip_max=1, loss_fn=None, targeted=False):\n        super(LBFGSAttack, self).__init__(\n            predict, loss_fn, clip_min, clip_max)\n        # XXX: should combine the input loss function with other things\n        self.num_classes = num_classes\n        self.batch_size = batch_size\n        self.binary_search_steps = binary_search_steps\n        self.max_iterations = max_iterations\n        self.initial_const = initial_const\n        self.targeted = targeted\n\n\n    def _update_if_better(\n            self, adv_img, labs, output, dist, batch_size,\n            final_l2dists, final_labels, final_advs):\n        for ii in range(batch_size):\n            target_label = labs[ii]\n            output_logits = output[ii]\n            _, output_label = torch.max(output_logits, 0)\n            di = dist[ii]\n            if (di < final_l2dists[ii] and\n                    output_label.item() == target_label):\n                final_l2dists[ii] = di\n                final_labels[ii] = output_label\n                final_advs[ii] = adv_img[ii]\n\n\n    def _update_loss_coeffs(\n            self, labs, batch_size,\n            loss_coeffs, coeff_upper_bound, coeff_lower_bound, output):\n        for ii in range(batch_size):\n            _, cur_label = torch.max(output[ii], 0)\n            if cur_label.item() == int(labs[ii]):\n                coeff_upper_bound[ii] = min(\n                    coeff_upper_bound[ii], loss_coeffs[ii])\n\n                if coeff_upper_bound[ii] < UPPER_CHECK:\n                    loss_coeffs[ii] = (\n                        coeff_lower_bound[ii] + coeff_upper_bound[ii]) / 2\n            else:\n                coeff_lower_bound[ii] = max(\n                    coeff_lower_bound[ii], loss_coeffs[ii])\n                if coeff_upper_bound[ii] < UPPER_CHECK:\n                    loss_coeffs[ii] = (\n                        coeff_lower_bound[ii] + coeff_upper_bound[ii]) / 2\n                else:\n                    loss_coeffs[ii] *= 10\n\n\n    def perturb(self, x, y=None):\n\n        from scipy.optimize import fmin_l_bfgs_b\n\n        def _loss_fn(adv_x_np, self, x, target, const):\n            adv_x = torch.from_numpy(\n                adv_x_np.reshape(x.shape)).float().to(\n                x.device).requires_grad_()\n            output = self.predict(adv_x)\n            loss2 = torch.sum((x - adv_x) ** 2)\n            loss_fn = F.cross_entropy(output, target, reduction=\'none\')\n            loss1 = torch.sum(const * loss_fn)\n            loss = loss1 + loss2\n            loss.backward()\n            grad_ret = adv_x.grad.data.cpu().numpy().flatten().astype(float)\n            loss = loss.data.cpu().numpy().flatten().astype(float)\n            if not self.targeted:\n                loss = -loss\n            return loss, grad_ret\n\n        x, y = self._verify_and_process_inputs(x, y)\n        batch_size = len(x)\n        coeff_lower_bound = x.new_zeros(batch_size)\n        coeff_upper_bound = x.new_ones(batch_size) * COEFF_UPPER\n        loss_coeffs = x.new_ones(batch_size) * self.initial_const\n        final_l2dists = [L2DIST_UPPER] * batch_size\n        final_labels = [INVALID_LABEL] * batch_size\n        final_advs = x.clone()\n        clip_min = self.clip_min * np.ones(x.shape[:]).astype(float)\n        clip_max = self.clip_max * np.ones(x.shape[:]).astype(float)\n        clip_bound = list(zip(clip_min.flatten(), clip_max.flatten()))\n\n        for outer_step in range(self.binary_search_steps):\n            init_guess = x.clone().cpu().numpy().flatten().astype(float)\n            adv_x, f, _ = fmin_l_bfgs_b(_loss_fn,\n                                        init_guess,\n                                        args=(self, x.clone(), y, loss_coeffs),\n                                        bounds=clip_bound,\n                                        maxiter=self.max_iterations,\n                                        iprint=0)\n\n            adv_x = torch.from_numpy(\n                adv_x.reshape(x.shape)).float().to(x.device)\n            l2s = calc_l2distsq(x, adv_x)\n            output = self.predict(adv_x)\n            self._update_if_better(\n                adv_x, y, output.data, l2s, batch_size,\n                final_l2dists, final_labels, final_advs)\n            self._update_loss_coeffs(\n                y, batch_size,\n                loss_coeffs, coeff_upper_bound, coeff_lower_bound,\n                output.data)\n        return final_advs\n'"
advertorch/attacks/localsearch.py,17,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom advertorch.utils import clamp\nfrom advertorch.utils import replicate_input\n\nfrom .base import Attack\nfrom .base import LabelMixin\nfrom .utils import is_successful\n\n\nclass SinglePixelAttack(Attack, LabelMixin):\n    """"""\n    Single Pixel Attack\n    Algorithm 1 in https://arxiv.org/pdf/1612.06299.pdf\n\n    :param predict: forward pass function.\n    :param max_pixels: max number of pixels to perturb.\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param loss_fn: loss function\n    :param targeted: if the attack is targeted.\n    """"""\n\n    def __init__(self, predict, max_pixels=100, clip_min=0.,\n                 loss_fn=None, clip_max=1., comply_with_foolbox=False,\n                 targeted=False):\n        super(SinglePixelAttack, self).__init__(\n            predict=predict, loss_fn=None,\n            clip_min=clip_min, clip_max=clip_max)\n        self.max_pixels = max_pixels\n        self.clip_min = clip_min\n        self.clip_max = clip_max\n        self.comply_with_foolbox = comply_with_foolbox\n        self.targeted = targeted\n\n\n    def perturb_single(self, x, y):\n        # x shape [C * H * W]\n        if self.comply_with_foolbox is True:\n            np.random.seed(233333)\n            rand_np = np.random.permutation(x.shape[1] * x.shape[2])\n            pixels = torch.from_numpy(rand_np)\n        else:\n            pixels = torch.randperm(x.shape[1] * x.shape[2])\n        pixels = pixels.to(x.device)\n        pixels = pixels[:self.max_pixels]\n        for ii in range(self.max_pixels):\n            row = pixels[ii] % x.shape[2]\n            col = pixels[ii] // x.shape[2]\n            for val in [self.clip_min, self.clip_max]:\n                adv = replicate_input(x)\n                for mm in range(x.shape[0]):\n                    adv[mm, row, col] = val\n                out_label = self._get_predicted_label(adv.unsqueeze(0))\n                if self.targeted is True:\n                    if int(out_label[0]) == int(y):\n                        return adv\n                else:\n                    if int(out_label[0]) != int(y):\n                        return adv\n        return x\n\n    def perturb(self, x, y=None):\n        x, y = self._verify_and_process_inputs(x, y)\n        return _perturb_batch(self.perturb_single, x, y)\n\n\nclass LocalSearchAttack(Attack, LabelMixin):\n    """"""\n    Local Search Attack\n    Algorithm 3 in https://arxiv.org/pdf/1612.06299.pdf\n\n    :param predict: forward pass function.\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param p: parameter controls pixel complexity\n    :param r: perturbation value\n    :param loss_fn: loss function\n    :param d: the half side length of the neighbourhood square\n    :param t: the number of pixels perturbed at each round\n    :param k: the threshold for k-misclassification\n    :param round_ub: an upper bound on the number of rounds\n    """"""\n\n    def __init__(self, predict, clip_min=0., clip_max=1., p=1., r=1.5,\n                 loss_fn=None, d=5, t=5, k=1, round_ub=10, seed_ratio=0.1,\n                 max_nb_seeds=128, comply_with_foolbox=False, targeted=False):\n        super(LocalSearchAttack, self).__init__(\n            predict=predict, clip_max=clip_max,\n            clip_min=clip_min, loss_fn=None)\n        self.p = p\n        self.r = r\n        self.d = d\n        self.t = t\n        self.k = k\n        self.round_ub = round_ub\n        self.seed_ratio = seed_ratio\n        self.max_nb_seeds = max_nb_seeds\n        self.comply_with_foolbox = comply_with_foolbox\n        self.targeted = targeted\n\n        if clip_min is None or clip_max is None:\n            raise ValueError(""{} {}"".format(\n                LocalSearchAttack,\n                ""must have clip_min and clip_max specified as scalar values.""))\n\n    def perturb_single(self, x, y):\n        # x shape C * H * W\n        rescaled_x = replicate_input(x)\n        best_img = None\n        best_dist = np.inf\n        rescaled_x, lb, ub = self._rescale_to_m0d5_to_0d5(\n            rescaled_x, vmin=self.clip_min, vmax=self.clip_max)\n\n        if self.comply_with_foolbox is True:\n            np.random.seed(233333)\n            init_rand = np.random.permutation(x.shape[1] * x.shape[2])\n        else:\n            init_rand = None\n\n        # Algorithm 3 in v1\n\n        pxy = self._random_sample_seeds(\n            x.shape[1], x.shape[2], seed_ratio=self.seed_ratio,\n            max_nb_seeds=self.max_nb_seeds, init_rand=init_rand)\n        pxy = pxy.to(x.device)\n        ii = 0\n        if self.comply_with_foolbox:\n            adv = rescaled_x\n        while ii < self.round_ub:\n            if not self.comply_with_foolbox:\n                adv = replicate_input(rescaled_x)\n            # Computing the function g using the neighbourhood\n            if self.comply_with_foolbox:\n                rand_np = np.random.permutation(len(pxy))[:self.max_nb_seeds]\n                pxy = pxy[torch.from_numpy(rand_np)]\n            else:\n                pxy = pxy[torch.randperm(len(pxy))[:self.max_nb_seeds]]\n\n            pert_lst = [\n                self._perturb_seed_pixel(\n                    adv, self.p, int(row), int(col)) for row, col in pxy]\n            # Compute the score for each pert in the list\n            scores, curr_best_img, curr_best_dist = self._rescale_x_score(\n                self.predict, pert_lst, y, x, best_dist)\n            if curr_best_img is not None:\n                best_img = curr_best_img\n                best_dist = curr_best_dist\n            _, indices = torch.sort(scores)\n            indices = indices[:self.t]\n            pxy_star = pxy[indices.data.cpu()]\n            # Generation of the perturbed image adv\n            for row, col in pxy_star:\n                for b in range(x.shape[0]):\n                    adv[b, int(row), int(col)] = self._cyclic(\n                        self.r, lb, ub, adv[b, int(row), int(col)])\n            # Check whether the perturbed image is an adversarial image\n            revert_adv = self._revert_rescale(adv)\n            curr_lb = self._get_predicted_label(revert_adv.unsqueeze(0))\n            curr_dist = torch.sum((x - revert_adv) ** 2)\n            if (is_successful(int(curr_lb), y, self.targeted) and\n                    curr_dist < best_dist):\n                best_img = revert_adv\n                best_dist = curr_dist\n                return best_img\n            elif is_successful(curr_lb, y, self.targeted):\n                return best_img\n            pxy = [\n                (row, col)\n                for rowcenter, colcenter in pxy_star\n                for row in range(\n                    int(rowcenter) - self.d, int(rowcenter) + self.d + 1)\n                for col in range(\n                    int(colcenter) - self.d, int(colcenter) + self.d + 1)]\n            pxy = list(set((row, col) for row, col in pxy if (\n                0 <= row < x.shape[2] and 0 <= col < x.shape[1])))\n            pxy = torch.FloatTensor(pxy)\n            ii += 1\n        if best_img is None:\n            return x\n        return best_img\n\n    def perturb(self, x, y=None):\n        x, y = self._verify_and_process_inputs(x, y)\n        return _perturb_batch(self.perturb_single, x, y)\n\n    def _rescale_to_m0d5_to_0d5(self, x, vmin=0., vmax=1.):\n        x = x - (vmin + vmax) / 2\n        x = x / (vmax - vmin)\n        return x, -0.5, 0.5\n\n\n    def _revert_rescale(self, x, vmin=0., vmax=1.):\n        x_revert = x.clone()\n        x_revert = x_revert * (vmax - vmin)\n        x_revert = x_revert + (vmin + vmax) / 2\n        return x_revert\n\n\n    def _random_sample_seeds(self, h, w, seed_ratio, max_nb_seeds, init_rand):\n        n = int(seed_ratio * h * w)\n        n = min(n, max_nb_seeds)\n        if init_rand is not None:\n            locations = torch.from_numpy(init_rand)[:n]\n        else:\n            locations = torch.randperm(h * w)[:n]\n        p_x = locations.int() % w\n        p_y = locations.int() / w\n        pxy = list(zip(p_x, p_y))\n        pxy = torch.Tensor(pxy)\n        return pxy\n\n\n    def _perturb_seed_pixel(self, x, p, row, col):\n        x_pert = replicate_input(x)\n        for ii in range(x.shape[0]):\n            if x[ii, row, col] > 0:\n                x_pert[ii, row, col] = p\n            elif x[ii, row, col] < 0:\n                x_pert[ii, row, col] = -1 * p\n            else:\n                x_pert[ii, row, col] = 0\n        return x_pert\n\n\n    def _cyclic(self, r, lower_bound, upper_bound, i_bxy):\n        # Algorithm 2 in v1\n        result = r * i_bxy\n        if result < lower_bound:\n            result = result + (upper_bound - lower_bound)\n        elif result > upper_bound:\n            result = result - (upper_bound - lower_bound)\n        return result\n\n\n    def _rescale_x_score(self, predict, x, y, ori, best_dist):\n        x = torch.stack(x)\n        x = self._revert_rescale(x)\n\n        batch_logits = predict(x)\n        scores = nn.Softmax(dim=1)(batch_logits)[:, y]\n\n        if not self.comply_with_foolbox:\n            x = clamp(x, self.clip_min, self.clip_max)\n            batch_logits = predict(x)\n\n        _, bests = torch.max(batch_logits, dim=1)\n        best_img = None\n        for ii in range(len(bests)):\n            curr_dist = torch.sum((x[ii] - ori) ** 2)\n            if (is_successful(\n                    int(bests[ii]), y, self.targeted) and\n                    curr_dist < best_dist):\n                best_img = x[ii]\n                best_dist = curr_dist\n        scores = nn.Softmax(dim=1)(batch_logits)[:, y]\n        return scores, best_img, best_dist\n\n\ndef _perturb_batch(perturb_single, x, y):\n    for ii in range(len(x)):\n        temp = perturb_single(x[ii], y[ii])[None, :, :, :]\n        if ii == 0:\n            result = temp\n        else:\n            result = torch.cat((result, temp))\n    return result\n'"
advertorch/attacks/one_step_gradient.py,3,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport torch.nn as nn\n\nfrom advertorch.utils import clamp\nfrom advertorch.utils import normalize_by_pnorm\n\nfrom .base import Attack\nfrom .base import LabelMixin\n\n\nclass GradientSignAttack(Attack, LabelMixin):\n    """"""\n    One step fast gradient sign method (Goodfellow et al, 2014).\n    Paper: https://arxiv.org/abs/1412.6572\n\n    :param predict: forward pass function.\n    :param loss_fn: loss function.\n    :param eps: attack step size.\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param targeted: indicate if this is a targeted attack.\n    """"""\n\n    def __init__(self, predict, loss_fn=None, eps=0.3, clip_min=0.,\n                 clip_max=1., targeted=False):\n        """"""\n        Create an instance of the GradientSignAttack.\n        """"""\n        super(GradientSignAttack, self).__init__(\n            predict, loss_fn, clip_min, clip_max)\n\n        self.eps = eps\n        self.targeted = targeted\n        if self.loss_fn is None:\n            self.loss_fn = nn.CrossEntropyLoss(reduction=""sum"")\n\n    def perturb(self, x, y=None):\n        """"""\n        Given examples (x, y), returns their adversarial counterparts with\n        an attack length of eps.\n\n        :param x: input tensor.\n        :param y: label tensor.\n                  - if None and self.targeted=False, compute y as predicted\n                    labels.\n                  - if self.targeted=True, then y must be the targeted labels.\n        :return: tensor containing perturbed inputs.\n        """"""\n\n        x, y = self._verify_and_process_inputs(x, y)\n        xadv = x.requires_grad_()\n        outputs = self.predict(xadv)\n\n        loss = self.loss_fn(outputs, y)\n        if self.targeted:\n            loss = -loss\n        loss.backward()\n        grad_sign = xadv.grad.detach().sign()\n\n        xadv = xadv + self.eps * grad_sign\n\n        xadv = clamp(xadv, self.clip_min, self.clip_max)\n\n        return xadv.detach()\n\n\nFGSM = GradientSignAttack\n\n\nclass GradientAttack(Attack, LabelMixin):\n    """"""\n    Perturbs the input with gradient (not gradient sign) of the loss wrt the\n    input.\n\n    :param predict: forward pass function.\n    :param loss_fn: loss function.\n    :param eps: attack step size.\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param targeted: indicate if this is a targeted attack.\n    """"""\n\n    def __init__(self, predict, loss_fn=None, eps=0.3,\n                 clip_min=0., clip_max=1., targeted=False):\n        """"""\n        Create an instance of the GradientAttack.\n        """"""\n        super(GradientAttack, self).__init__(\n            predict, loss_fn, clip_min, clip_max)\n\n        self.eps = eps\n        self.targeted = targeted\n        if self.loss_fn is None:\n            self.loss_fn = nn.CrossEntropyLoss(reduction=""sum"")\n\n    def perturb(self, x, y=None):\n        """"""\n        Given examples (x, y), returns their adversarial counterparts with\n        an attack length of eps.\n\n        :param x: input tensor.\n        :param y: label tensor.\n                  - if None and self.targeted=False, compute y as predicted\n                    labels.\n                  - if self.targeted=True, then y must be the targeted labels.\n        :return: tensor containing perturbed inputs.\n        """"""\n        x, y = self._verify_and_process_inputs(x, y)\n        xadv = x.requires_grad_()\n        outputs = self.predict(xadv)\n\n        loss = self.loss_fn(outputs, y)\n        if self.targeted:\n            loss = -loss\n        loss.backward()\n        grad = normalize_by_pnorm(xadv.grad)\n        xadv = xadv + self.eps * grad\n        xadv = clamp(xadv, self.clip_min, self.clip_max)\n\n        return xadv.detach()\n\n\nFGM = GradientAttack\n'"
advertorch/attacks/spatial.py,12,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom advertorch.utils import calc_l2distsq\nfrom advertorch.utils import clamp\nfrom advertorch.utils import to_one_hot\n\nfrom .base import Attack\nfrom .base import LabelMixin\nfrom .utils import is_successful\n\nL2DIST_UPPER = 1e10\nTARGET_MULT = 10000.0\nINVALID_LABEL = -1\n\n\nclass SpatialTransformAttack(Attack, LabelMixin):\n    """"""\n    Spatially Transformed Attack (Xiao et al. 2018)\n    https://openreview.net/forum?id=HyydRMZC-\n\n    :param predict: forward pass function.\n    :param num_classes: number of clasess.\n    :param confidence: confidence of the adversarial examples.\n    :param initial_const: initial value of the constant c\n    :param max_iterations: the maximum number of iterations\n    :param search_steps: number of search times to find the optimum\n    :param loss_fn: loss function\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param abort_early: if set to true, abort early if getting stuck in local\n        min\n    :param targeted: if the attack is targeted\n    """"""\n\n    def __init__(self, predict, num_classes, confidence=0,\n                 initial_const=1, max_iterations=1000,\n                 search_steps=1, loss_fn=None,\n                 clip_min=0.0, clip_max=1.0,\n                 abort_early=True, targeted=False):\n        super(SpatialTransformAttack, self).__init__(\n            predict, loss_fn, clip_min, clip_max)\n        self.num_classes = num_classes\n        self.confidence = confidence\n        self.initial_const = initial_const\n        self.max_iterations = max_iterations\n        self.search_steps = search_steps\n        self.abort_early = abort_early\n        self.targeted = targeted\n\n    def _loss_fn_spatial(self, grid, x, y, const, grid_ori):\n        imgs = x.clone()\n        grid = torch.from_numpy(\n            grid.reshape(grid_ori.shape)).float().to(\n            x.device).requires_grad_()\n        delta = grid_ori - grid\n\n        adv_img = F.grid_sample(imgs, grid)\n        output = self.predict(adv_img)\n        real = (y * output).sum(dim=1)\n        other = (\n            (1.0 - y) * output - (y * TARGET_MULT)).max(1)[0]\n        if self.targeted:\n            loss1 = clamp(other - real + self.confidence, min=0.)\n        else:\n            loss1 = clamp(real - other + self.confidence, min=0.)\n        loss2 = self.initial_const * (\n            torch.sqrt((((\n                delta[:, :, 1:] - delta[:, :, :-1] + 1e-10) ** 2)).view(\n                    delta.shape[0], -1).sum(1)) +\n            torch.sqrt(((\n                delta[:, 1:, :] - delta[:, :-1, :] + 1e-10) ** 2).view(\n                    delta.shape[0], -1).sum(1)))\n        loss = torch.sum(loss1) + torch.sum(loss2)\n        loss.backward()\n        grad_ret = grid.grad.data.cpu().numpy().flatten().astype(float)\n        grid.grad.data.zero_()\n        return loss.data.cpu().numpy().astype(float), grad_ret\n\n    def _update_if_better(\n            self, adv_img, labs, output, dist, batch_size,\n            final_l2dists, final_labels, final_advs, step, final_step):\n\n        for ii in range(batch_size):\n            target_label = labs[ii]\n            output_logits = output[ii]\n            _, output_label = torch.max(output_logits, 0)\n            di = dist[ii]\n            if (di < final_l2dists[ii] and\n                    is_successful(\n                    int(output_label.item()), int(target_label),\n                    self.targeted)):\n                final_l2dists[ii] = di\n                final_labels[ii] = output_label\n                final_advs[ii] = adv_img[ii]\n                final_step[ii] = step\n\n    def perturb(self, x, y=None):\n        x, y = self._verify_and_process_inputs(x, y)\n        batch_size = len(x)\n        loss_coeffs = x.new_ones(batch_size) * self.initial_const\n        final_l2dists = [L2DIST_UPPER] * batch_size\n        final_labels = [INVALID_LABEL] * batch_size\n        final_step = [INVALID_LABEL] * batch_size\n        final_advs = torch.zeros_like(x)\n\n        # TODO: refactor the theta generation\n        theta = torch.tensor([[[1., 0., 0.],\n                               [0., 1., 0.]]]).to(x.device)\n        theta = theta.repeat((x.shape[0], 1, 1))\n\n\n        grid = F.affine_grid(theta, x.size())\n\n        grid_ori = grid.clone()\n        y_onehot = to_one_hot(y, self.num_classes).float()\n\n        clip_min = np.ones(grid_ori.shape[:]) * -1\n        clip_max = np.ones(grid_ori.shape[:]) * 1\n        clip_bound = list(zip(clip_min.flatten(), clip_max.flatten()))\n        grid_ret = grid.clone().data.cpu().numpy().flatten().astype(float)\n        from scipy.optimize import fmin_l_bfgs_b\n        for outer_step in range(self.search_steps):\n            grid_ret, f, d = fmin_l_bfgs_b(\n                self._loss_fn_spatial,\n                grid_ret,\n                args=(\n                    x.clone().detach(),\n                    y_onehot, loss_coeffs,\n                    grid_ori.clone().detach()),\n                maxiter=self.max_iterations,\n                bounds=clip_bound,\n                iprint=0,\n                maxls=100,\n            )\n            grid = torch.from_numpy(\n                grid_ret.reshape(grid_ori.shape)).float().to(x.device)\n            adv_x = F.grid_sample(x.clone(), grid)\n            l2s = calc_l2distsq(grid.data, grid_ori.data)\n            output = self.predict(adv_x)\n            self._update_if_better(\n                adv_x.data, y, output.data, l2s, batch_size,\n                final_l2dists, final_labels, final_advs,\n                outer_step, final_step)\n\n        return final_advs\n'"
advertorch/attacks/spsa.py,8,"b'# Copyright (c) 2018-present, Royal Bank of Canada and other authors.\n# See the AUTHORS.txt file for a list of contributors.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport warnings\n\nimport torch\n\nfrom .base import Attack\nfrom .base import LabelMixin\nfrom .utils import MarginalLoss\nfrom ..utils import is_float_or_torch_tensor\n\n__all__ = [\'LinfSPSAAttack\', \'spsa_grad\', \'spsa_perturb\']\n\n\ndef linf_clamp_(dx, x, eps, clip_min, clip_max):\n    """"""Clamps perturbation `dx` to fit L_inf norm and image bounds.\n\n    Limit the L_inf norm of `dx` to be <= `eps`, and the bounds of `x + dx`\n    to be in `[clip_min, clip_max]`.\n\n    :param dx: perturbation to be clamped (inplace).\n    :param x: the image.\n    :param eps: maximum possible L_inf.\n    :param clip_min: upper bound of image values.\n    :param clip_max: lower bound of image values.\n\n    :return: the clamped perturbation `dx`.\n    """"""\n\n    dx_clamped = torch.clamp(dx, -eps, eps)\n    x_adv = torch.clamp(x + dx_clamped, clip_min, clip_max)\n    # `dx` is changed *inplace* so the optimizer will keep\n    # tracking it. the simplest mechanism for inplace was\n    # adding the difference between the new value `x_adv - x`\n    # and the old value `dx`.\n    dx += x_adv - x - dx\n    return dx\n\n\ndef _get_batch_sizes(n, max_batch_size):\n    batches = [max_batch_size for _ in range(n // max_batch_size)]\n    if n % max_batch_size > 0:\n        batches.append(n % max_batch_size)\n    return batches\n\n\n@torch.no_grad()\ndef spsa_grad(predict, loss_fn, x, y, delta, nb_sample, max_batch_size):\n    """"""Uses SPSA method to apprixmate gradient w.r.t `x`.\n\n    Use the SPSA method to approximate the gradient of `loss_fn(predict(x), y)`\n    with respect to `x`, based on the nonce `v`.\n\n    :param predict: predict function (single argument: input).\n    :param loss_fn: loss function (dual arguments: output, target).\n    :param x: input argument for function `predict`.\n    :param y: target argument for function `loss_fn`.\n    :param v: perturbations of `x`.\n    :param delta: scaling parameter of SPSA.\n    :param reduction: how to reduce the gradients of the different samples.\n\n    :return: return the approximated gradient of `loss_fn(predict(x), y)`\n             with respect to `x`.\n    """"""\n\n    grad = torch.zeros_like(x)\n    x = x.unsqueeze(0)\n    y = y.unsqueeze(0)\n\n    def f(xvar, yvar):\n        return loss_fn(predict(xvar), yvar)\n    x = x.expand(max_batch_size, *x.shape[1:]).contiguous()\n    y = y.expand(max_batch_size, *y.shape[1:]).contiguous()\n    v = torch.empty_like(x[:, :1, ...])\n\n    for batch_size in _get_batch_sizes(nb_sample, max_batch_size):\n        x_ = x[:batch_size]\n        y_ = y[:batch_size]\n        vb = v[:batch_size]\n        vb = vb.bernoulli_().mul_(2.0).sub_(1.0)\n        v_ = vb.expand_as(x_).contiguous()\n        x_shape = x_.shape\n        x_ = x_.view(-1, *x.shape[2:])\n        y_ = y_.view(-1, *y.shape[2:])\n        v_ = v_.view(-1, *v.shape[2:])\n        df = f(x_ + delta * v_, y_) - f(x_ - delta * v_, y_)\n        df = df.view(-1, *[1 for _ in v_.shape[1:]])\n        grad_ = df / (2. * delta * v_)\n        grad_ = grad_.view(x_shape)\n        grad_ = grad_.sum(dim=0, keepdim=False)\n        grad += grad_\n    grad /= nb_sample\n\n    return grad\n\n\ndef spsa_perturb(predict, loss_fn, x, y, eps, delta, lr, nb_iter,\n                 nb_sample, max_batch_size, clip_min=0.0, clip_max=1.0):\n    """"""Perturbs the input `x` based on SPSA attack.\n\n    :param predict: predict function (single argument: input).\n    :param loss_fn: loss function (dual arguments: output, target).\n    :param x: input argument for function `predict`.\n    :param y: target argument for function `loss_fn`.\n    :param eps: the L_inf budget of the attack.\n    :param delta: scaling parameter of SPSA.\n    :param lr: the learning rate of the `Adam` optimizer.\n    :param nb_iter: number of iterations of the attack.\n    :param nb_sample: number of samples for the SPSA gradient approximation.\n    :param max_batch_size: maximum batch size to be evaluated at once.\n    :param clip_min: upper bound of image values.\n    :param clip_max: lower bound of image values.\n\n    :return: the perturbated input.\n    """"""\n\n    dx = torch.zeros_like(x)\n    dx.grad = torch.zeros_like(dx)\n    optimizer = torch.optim.Adam([dx], lr=lr)\n    for _ in range(nb_iter):\n        optimizer.zero_grad()\n        dx.grad = spsa_grad(\n            predict, loss_fn, x + dx, y, delta, nb_sample, max_batch_size)\n        optimizer.step()\n        dx = linf_clamp_(dx, x, eps, clip_min, clip_max)\n    x_adv = x + dx\n\n    return x_adv\n\n\nclass LinfSPSAAttack(Attack, LabelMixin):\n    """"""SPSA Attack (Uesato et al. 2018).\n    Based on: https://arxiv.org/abs/1802.05666\n\n    :param predict: predict function (single argument: input).\n    :param eps: the L_inf budget of the attack.\n    :param delta: scaling parameter of SPSA.\n    :param lr: the learning rate of the `Adam` optimizer.\n    :param nb_iter: number of iterations of the attack.\n    :param nb_sample: number of samples for SPSA gradient approximation.\n    :param max_batch_size: maximum batch size to be evaluated at once.\n    :param targeted: [description]\n    :param loss_fn: loss function (dual arguments: output, target).\n    :param clip_min: upper bound of image values.\n    :param clip_max: lower bound of image values.\n    """"""\n\n    def __init__(self, predict, eps, delta=0.01, lr=0.01, nb_iter=1,\n                 nb_sample=128, max_batch_size=64, targeted=False,\n                 loss_fn=None, clip_min=0.0, clip_max=1.0):\n\n        if loss_fn is None:\n            loss_fn = MarginalLoss(reduction=""none"")\n        elif hasattr(loss_fn, ""reduction"") and \\\n                getattr(loss_fn, ""reduction"") != ""none"":\n            warnings.warn(""`loss_fn` is recommended to have ""\n                          ""reduction=\'none\' when used in SPSA attack"")\n\n        super(LinfSPSAAttack, self).__init__(predict, loss_fn,\n                                             clip_min, clip_max)\n\n        assert is_float_or_torch_tensor(eps)\n        assert is_float_or_torch_tensor(delta)\n        assert is_float_or_torch_tensor(lr)\n\n        self.eps = float(eps)\n        self.delta = float(delta)\n        self.lr = float(lr)\n        self.nb_iter = int(nb_iter)\n        self.nb_sample = int(nb_sample)\n        self.max_batch_size = int(max_batch_size)\n        self.targeted = bool(targeted)\n\n    def perturb(self, x, y=None):  # pylint: disable=arguments-differ\n        """"""Perturbs the input `x` based on SPSA attack.\n\n        :param x: input tensor.\n        :param y: label tensor (default=`None`). if `self.targeted` is `False`,\n                  `y` is the ground-truth label. if it\'s `None`, then `y` is\n                  computed as the predicted label of `x`.\n                  if `self.targeted` is `True`, `y` is the target label.\n\n        :return: the perturbated input.\n        """"""\n\n        x, y = self._verify_and_process_inputs(x, y)\n\n        if self.targeted:\n            def loss_fn(*args):\n                return self.loss_fn(*args)\n\n        else:\n            def loss_fn(*args):\n                return -self.loss_fn(*args)\n\n        return spsa_perturb(self.predict, loss_fn, x, y, self.eps, self.delta,\n                            self.lr, self.nb_iter, self.nb_sample,\n                            self.max_batch_size, self.clip_min, self.clip_max)\n'"
advertorch/attacks/utils.py,22,"b'# Copyright (c) 2018-present, Royal Bank of Canada and other authors.\n# See the AUTHORS.txt file for a list of contributors.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport torch\n\nfrom torch.distributions import laplace\nfrom torch.distributions import uniform\nfrom torch.nn.modules.loss import _Loss\n\nfrom advertorch.utils import clamp\nfrom advertorch.utils import clamp_by_pnorm\nfrom advertorch.utils import batch_multiply\nfrom advertorch.utils import normalize_by_pnorm\nfrom advertorch.utils import predict_from_logits\nfrom advertorch.loss import ZeroOneLoss\nfrom advertorch.attacks import Attack, LabelMixin\n\n\n\ndef rand_init_delta(delta, x, ord, eps, clip_min, clip_max):\n    # TODO: Currently only considered one way of ""uniform"" sampling\n    # for Linf, there are 3 ways:\n    #   1) true uniform sampling by first calculate the rectangle then sample\n    #   2) uniform in eps box then truncate using data domain (implemented)\n    #   3) uniform sample in data domain then truncate with eps box\n    # for L2, true uniform sampling is hard, since it requires uniform sampling\n    #   inside a intersection of cube and ball, so there are 2 ways:\n    #   1) uniform sample in the data domain, then truncate using the L2 ball\n    #       (implemented)\n    #   2) uniform sample in the L2 ball, then truncate using the data domain\n    # for L1: uniform l1 ball init, then truncate using the data domain\n\n    if isinstance(eps, torch.Tensor):\n        assert len(eps) == len(delta)\n\n    if ord == np.inf:\n        delta.data.uniform_(-1, 1)\n        delta.data = batch_multiply(eps, delta.data)\n    elif ord == 2:\n        delta.data.uniform_(clip_min, clip_max)\n        delta.data = delta.data - x\n        delta.data = clamp_by_pnorm(delta.data, ord, eps)\n    elif ord == 1:\n        ini = laplace.Laplace(\n            loc=delta.new_tensor(0), scale=delta.new_tensor(1))\n        delta.data = ini.sample(delta.data.shape)\n        delta.data = normalize_by_pnorm(delta.data, p=1)\n        ray = uniform.Uniform(0, eps).sample()\n        delta.data *= ray\n        delta.data = clamp(x.data + delta.data, clip_min, clip_max) - x.data\n    else:\n        error = ""Only ord = inf, ord = 1 and ord = 2 have been implemented""\n        raise NotImplementedError(error)\n\n    delta.data = clamp(\n        x + delta.data, min=clip_min, max=clip_max) - x\n    return delta.data\n\n\ndef is_successful(y1, y2, targeted):\n    if targeted is True:\n        return y1 == y2\n    else:\n        return y1 != y2\n\n\nclass AttackConfig(object):\n    # a convenient class for generate an attack/adversary instance\n\n    def __init__(self):\n        self.kwargs = {}\n\n        for mro in reversed(self.__class__.__mro__):\n            if mro in (AttackConfig, object):\n                continue\n            for kwarg in mro.__dict__:\n                if kwarg in self.AttackClass.__init__.__code__.co_varnames:\n                    self.kwargs[kwarg] = mro.__dict__[kwarg]\n                else:\n                    # make sure we don\'t specify wrong kwargs\n                    assert kwarg in [""__module__"", ""AttackClass"", ""__doc__""]\n\n    def __call__(self, *args):\n        adversary = self.AttackClass(*args, **self.kwargs)\n        print(self.AttackClass, args, self.kwargs)\n        return adversary\n\n\ndef multiple_mini_batch_attack(\n        adversary, loader, device=""cuda"", save_adv=False,\n        norm=None, num_batch=None):\n    lst_label = []\n    lst_pred = []\n    lst_advpred = []\n    lst_dist = []\n\n    _norm_convert_dict = {""Linf"": ""inf"", ""L2"": 2, ""L1"": 1}\n    if norm in _norm_convert_dict:\n        norm = _norm_convert_dict[norm]\n\n    if norm == ""inf"":\n        def dist_func(x, y):\n            return (x - y).view(x.size(0), -1).max(dim=1)[0]\n    elif norm == 1 or norm == 2:\n        from advertorch.utils import _get_norm_batch\n\n        def dist_func(x, y):\n            return _get_norm_batch(x - y, norm)\n    else:\n        assert norm is None\n\n\n    idx_batch = 0\n\n    for data, label in loader:\n        data, label = data.to(device), label.to(device)\n        adv = adversary.perturb(data, label)\n        advpred = predict_from_logits(adversary.predict(adv))\n        pred = predict_from_logits(adversary.predict(data))\n        lst_label.append(label)\n        lst_pred.append(pred)\n        lst_advpred.append(advpred)\n        if norm is not None:\n            lst_dist.append(dist_func(data, adv))\n\n        idx_batch += 1\n        if idx_batch == num_batch:\n            break\n\n    return torch.cat(lst_label), torch.cat(lst_pred), torch.cat(lst_advpred), \\\n        torch.cat(lst_dist) if norm is not None else None\n\n\nclass MarginalLoss(_Loss):\n    # TODO: move this to advertorch.loss\n\n    def forward(self, logits, targets):  # pylint: disable=arguments-differ\n        assert logits.shape[-1] >= 2\n        top_logits, top_classes = torch.topk(logits, 2, dim=-1)\n        target_logits = logits[torch.arange(logits.shape[0]), targets]\n        max_nontarget_logits = torch.where(\n            top_classes[..., 0] == targets,\n            top_logits[..., 1],\n            top_logits[..., 0],\n        )\n\n        loss = max_nontarget_logits - target_logits\n        if self.reduction == ""none"":\n            pass\n        elif self.reduction == ""sum"":\n            loss = loss.sum()\n        elif self.reduction == ""mean"":\n            loss = loss.mean()\n        else:\n            raise ValueError(""unknown reduction: \'%s\'"" % (self.recution,))\n\n        return loss\n\n\nclass ChooseBestAttack(Attack, LabelMixin):\n    def __init__(self, predict, base_adversaries, loss_fn=None,\n                 targeted=False):\n        self.predict = predict\n        self.base_adversaries = base_adversaries\n        self.loss_fn = loss_fn\n        self.targeted = targeted\n\n        if self.loss_fn is None:\n            self.loss_fn = ZeroOneLoss(reduction=""none"")\n        else:\n            assert self.loss_fn.reduction == ""none""\n\n        for adversary in self.base_adversaries:\n            assert self.targeted == adversary.targeted\n\n    def perturb(self, x, y=None):\n        # TODO: might want to also retain the list of all attacks\n\n        x, y = self._verify_and_process_inputs(x, y)\n\n        with torch.no_grad():\n            maxloss = self.loss_fn(self.predict(x), y)\n        final_adv = torch.zeros_like(x)\n        for adversary in self.base_adversaries:\n            adv = adversary.perturb(x, y)\n            loss = self.loss_fn(self.predict(adv), y)\n            to_replace = maxloss < loss\n            final_adv[to_replace] = adv[to_replace]\n            maxloss[to_replace] = loss[to_replace]\n\n        return final_adv\n\n\ndef attack_whole_dataset(adversary, loader, device=""cuda""):\n    lst_adv = []\n    lst_label = []\n    lst_pred = []\n    lst_advpred = []\n    for data, label in loader:\n        data, label = data.to(device), label.to(device)\n        pred = predict_from_logits(adversary.predict(data))\n        adv = adversary.perturb(data, label)\n        advpred = predict_from_logits(adversary.predict(adv))\n        lst_label.append(label)\n        lst_pred.append(pred)\n        lst_advpred.append(advpred)\n        lst_adv.append(adv)\n    return torch.cat(lst_adv), torch.cat(lst_label), torch.cat(lst_pred), \\\n        torch.cat(lst_advpred)\n'"
advertorch/defenses/__init__.py,0,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\n# flake8: noqa\n\nfrom .base import Processor\n\nfrom .smoothing import ConvSmoothing2D\nfrom .smoothing import AverageSmoothing2D\nfrom .smoothing import GaussianSmoothing2D\nfrom .smoothing import MedianSmoothing2D\n\nfrom .jpeg import JPEGFilter\n\nfrom .bitsqueezing import BitSqueezing\nfrom .bitsqueezing import BinaryFilter\n'"
advertorch/defenses/base.py,1,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport torch.nn as nn\n\n\nclass Processor(nn.Module):\n    """"""\n    Processor\n    """"""\n    def __init__(self):\n        super(Processor, self).__init__()\n\n    def forward(self, x):\n        return x\n\n    def extra_repr(self):\n        return \'EmptyDefense (Identity)\'\n'"
advertorch/defenses/bitsqueezing.py,1,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom advertorch.functional import FloatToIntSqueezing\n\nfrom .base import Processor\n\n\nclass BitSqueezing(Processor):\n    """"""\n    Bit Squeezing.\n\n    :param bit_depth: bit depth.\n    :param vmin: min value.\n    :param vmax: max value.\n    """"""\n\n    def __init__(self, bit_depth, vmin=0., vmax=1.):\n        super(BitSqueezing, self).__init__()\n\n        self.bit_depth = bit_depth\n        self.max_int = 2 ** self.bit_depth - 1\n        self.vmin = vmin\n        self.vmax = vmax\n\n\n    def forward(self, x):\n        return FloatToIntSqueezing.apply(\n            x, self.max_int, self.vmin, self.vmax)\n\n\nclass BinaryFilter(BitSqueezing):\n    """"""\n    Binary Filter.\n\n    :param vmin: min value.\n    :param vmax: max value.\n    """"""\n\n    def __init__(self, vmin=0., vmax=1.):\n        super(BinaryFilter, self).__init__(bit_depth=1, vmin=vmin, vmax=vmax)\n'"
advertorch/defenses/jpeg.py,1,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom advertorch.functional import JPEGEncodingDecoding\n\nfrom .base import Processor\n\n\nclass JPEGFilter(Processor):\n    """"""\n    JPEG Filter.\n\n    :param quality: quality of the output.\n    """"""\n    def __init__(self, quality=75):\n        super(JPEGFilter, self).__init__()\n        self.quality = quality\n\n    def forward(self, x):\n        return JPEGEncodingDecoding.apply(x, self.quality)\n'"
advertorch/defenses/smoothing.py,9,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.utils import _quadruple\n\nfrom .base import Processor\n\n\nclass MedianSmoothing2D(Processor):\n    """"""\n    Median Smoothing 2D.\n\n    :param kernel_size: aperture linear size; must be odd and greater than 1.\n    :param stride: stride of the convolution.\n    """"""\n\n    def __init__(self, kernel_size=3, stride=1):\n        super(MedianSmoothing2D, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride\n        padding = int(kernel_size) // 2\n        if _is_even(kernel_size):\n            # both ways of padding should be fine here\n            # self.padding = (padding, 0, padding, 0)\n            self.padding = (0, padding, 0, padding)\n        else:\n            self.padding = _quadruple(padding)\n\n\n    def forward(self, x):\n        x = F.pad(x, pad=self.padding, mode=""reflect"")\n        x = x.unfold(2, self.kernel_size, self.stride)\n        x = x.unfold(3, self.kernel_size, self.stride)\n        x = x.contiguous().view(x.shape[:4] + (-1, )).median(dim=-1)[0]\n        return x\n\n\nclass ConvSmoothing2D(Processor):\n    """"""\n    Conv Smoothing 2D.\n\n    :param kernel_size: size of the convolving kernel.\n    """"""\n\n    def __init__(self, kernel):\n        super(ConvSmoothing2D, self).__init__()\n        self.filter = _generate_conv2d_from_smoothing_kernel(kernel)\n\n    def forward(self, x):\n        return self.filter(x)\n\n\nclass GaussianSmoothing2D(ConvSmoothing2D):\n    """"""\n    Gaussian Smoothing 2D.\n\n    :param sigma: sigma of the Gaussian.\n    :param channels: number of channels in the output.\n    :param kernel_size: aperture size.\n    """"""\n\n    def __init__(self, sigma, channels, kernel_size=None):\n        kernel = _generate_gaussian_kernel(sigma, channels, kernel_size)\n        super(GaussianSmoothing2D, self).__init__(kernel)\n\n\nclass AverageSmoothing2D(ConvSmoothing2D):\n    """"""\n    Average Smoothing 2D.\n\n    :param channels: number of channels in the output.\n    :param kernel_size: aperture size.\n    """"""\n\n    def __init__(self, channels, kernel_size):\n        kernel = torch.ones((channels, 1, kernel_size, kernel_size)) / (\n            kernel_size * kernel_size)\n        super(AverageSmoothing2D, self).__init__(kernel)\n\n\ndef _generate_conv2d_from_smoothing_kernel(kernel):\n    channels = kernel.shape[0]\n    kernel_size = kernel.shape[-1]\n\n    if _is_even(kernel_size):\n        raise NotImplementedError(\n            ""Even number kernel size not supported yet, kernel_size={}"".format(\n                kernel_size))\n\n    filter_ = nn.Conv2d(\n        in_channels=channels, out_channels=channels, kernel_size=kernel_size,\n        groups=channels, padding=kernel_size // 2, bias=False)\n\n    filter_.weight.data = kernel\n    filter_.weight.requires_grad = False\n    return filter_\n\n\ndef _generate_gaussian_kernel(sigma, channels, kernel_size=None):\n\n    if kernel_size is None:\n        kernel_size = _round_to_odd(2 * 2 * sigma)\n\n    vecx = torch.arange(kernel_size).float()\n    vecy = torch.arange(kernel_size).float()\n    gridxy = _meshgrid(vecx, vecy)\n    mean = (kernel_size - 1) / 2.\n    var = sigma ** 2\n\n    gaussian_kernel = (\n        1. / (2. * math.pi * var) *\n        torch.exp(-(gridxy - mean).pow(2).sum(dim=0) / (2 * var))\n    )\n\n    gaussian_kernel /= torch.sum(gaussian_kernel)\n\n    gaussian_kernel = gaussian_kernel.repeat(channels, 1, 1, 1)\n\n    return gaussian_kernel\n\n\ndef _round_to_odd(f):\n    return math.ceil(f) // 2 * 2 + 1\n\n\ndef _meshgrid(vecx, vecy):\n    gridx = vecx.repeat(len(vecy), 1)\n    gridy = vecy.repeat(len(vecx), 1).t()\n    return torch.stack([gridx, gridy])\n\n\ndef _is_even(x):\n    return int(x) % 2 == 0\n'"
advertorch_examples/attack_benchmarks/benchmark_decoupled_direction_norm.py,1,"b'# Copyright (c) 2018-present, Royal Bank of Canada and other authors.\n# See the AUTHORS.txt file for a list of contributors.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n#\n#\n# Automatically generated benchmark report (screen print of running this file)\n#\n# sysname: Linux\n# release: 4.4.0-140-generic\n# version: #166-Ubuntu SMP Wed Nov 14 20:09:47 UTC 2018\n# machine: x86_64\n# python: 3.7.3\n# torch: 1.1.0\n# torchvision: 0.3.0\n# advertorch: 0.1.5\n\n# attack type: DDNL2Attack\n# attack kwargs: nb_iter=1000\n#                gamma=0.05\n#                init_norm=1.0\n#                quantize=True\n#                levels=256\n#                clip_min=0.0\n#                clip_max=1.0\n#                targeted=False\n# data: mnist_test\n# model: MNIST LeNet5 standard training\n# accuracy: 98.89%\n# attack success rate: 100.0%\n# Among successful attacks (L2 norm) on correctly classified examples:\n#    minimum distance: 0.006792\n#    median distance: 1.388\n#    maximum distance: 3.3\n#    average distance: 1.38\n#    distance standard deviation: 0.4716\n\n# attack type: DDNL2Attack\n# attack kwargs: nb_iter=1000\n#                gamma=0.05\n#                init_norm=1.0\n#                quantize=True\n#                levels=256\n#                clip_min=0.0\n#                clip_max=1.0\n#                targeted=False\n# data: mnist_test\n# model: MNIST LeNet 5 PGD training according to Madry et al. 2018\n# accuracy: 98.64%\n# attack success rate: 100.0%\n# Among successful attacks (L2 norm) on correctly classified examples:\n#    minimum distance: 0.005546\n#    median distance: 1.872\n#    maximum distance: 20.45\n#    average distance: 1.917\n#    distance standard deviation: 0.733\n\n\nfrom advertorch_examples.utils import get_mnist_test_loader\nfrom advertorch_examples.utils import get_mnist_lenet5_clntrained\nfrom advertorch_examples.utils import get_mnist_lenet5_advtrained\nfrom advertorch_examples.benchmark_utils import get_benchmark_sys_info\n\nfrom advertorch.attacks import DDNL2Attack\n\nfrom advertorch_examples.benchmark_utils import benchmark_margin\n\nbatch_size = 100\ndevice = ""cuda""\n\nlst_attack = [\n    (DDNL2Attack, dict(\n        nb_iter=1000, gamma=0.05, init_norm=1., quantize=True, levels=256,\n        clip_min=0., clip_max=1., targeted=False)),\n]  # each element in the list is the tuple (attack_class, attack_kwargs)\n\nmnist_clntrained_model = get_mnist_lenet5_clntrained().to(device)\nmnist_advtrained_model = get_mnist_lenet5_advtrained().to(device)\nmnist_test_loader = get_mnist_test_loader(batch_size=batch_size)\n\nlst_setting = [\n    (mnist_clntrained_model, mnist_test_loader),\n    (mnist_advtrained_model, mnist_test_loader),\n]\n\n\ninfo = get_benchmark_sys_info()\n\nlst_benchmark = []\nfor model, loader in lst_setting:\n    for attack_class, attack_kwargs in lst_attack:\n        lst_benchmark.append(benchmark_margin(\n            model, loader, attack_class, attack_kwargs, norm=2, device=""cuda""))\n\nprint(info)\nfor item in lst_benchmark:\n    print(item)\n'"
advertorch_examples/attack_benchmarks/benchmark_fast_adaptive_boundary.py,1,"b'# Copyright (c) 2018-present, Royal Bank of Canada and other authors.\n# See the AUTHORS.txt file for a list of contributors.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n#\n#\n# Automatically generated benchmark report (screen print of running this file)\n#\n# sysname: Linux\n# release: 4.4.0-140-generic\n# version: #166-Ubuntu SMP Wed Nov 14 20:09:47 UTC 2018\n# machine: x86_64\n# python: 3.7.3\n# torch: 1.1.0\n# torchvision: 0.3.0\n# advertorch: 0.1.5\n\n# attack type: FABAttack\n# attack kwargs: norm=Linf\n#                n_restarts=1\n#                n_iter=20\n#                alpha_max=0.1\n#                eta=1.05\n#                beta=0.9\n#                loss_fn=None\n# data: mnist_test\n# model: MNIST LeNet5 standard training\n# accuracy: 98.89%\n# attack success rate: 100.0%\n# Among successful attacks (Linf norm) on correctly classified examples:\n#    minimum distance: 0.0001396\n#    median distance: 0.112\n#    maximum distance: 0.2155\n#    average distance: 0.1092\n#    distance standard deviation: 0.03498\n\n# attack type: FABAttack\n# attack kwargs: norm=L2\n#                n_restarts=1\n#                n_iter=20\n#                alpha_max=0.1\n#                eta=1.05\n#                beta=0.9\n#                loss_fn=None\n# data: mnist_test\n# model: MNIST LeNet5 standard training\n# accuracy: 98.89%\n# attack success rate: 100.0%\n# Among successful attacks (L2 norm) on correctly classified examples:\n#    minimum distance: 0.001726\n#    median distance: 1.423\n#    maximum distance: 3.01\n#    average distance: 1.412\n#    distance standard deviation: 0.4805\n\n# attack type: FABAttack\n# attack kwargs: norm=L1\n#                n_restarts=1\n#                n_iter=20\n#                alpha_max=0.1\n#                eta=1.05\n#                beta=0.9\n#                loss_fn=None\n# data: mnist_test\n# model: MNIST LeNet5 standard training\n# accuracy: 98.89%\n# attack success rate: 99.55%\n# Among successful attacks (L1 norm) on correctly classified examples:\n#    minimum distance: 0.007688\n#    median distance: 7.61\n#    maximum distance: 36.06\n#    average distance: 8.365\n#    distance standard deviation: 4.42\n\n# attack type: FABAttack\n# attack kwargs: norm=Linf\n#                n_restarts=1\n#                n_iter=20\n#                alpha_max=0.1\n#                eta=1.05\n#                beta=0.9\n#                loss_fn=None\n# data: mnist_test\n# model: MNIST LeNet 5 PGD training according to Madry et al. 2018\n# accuracy: 98.64%\n# attack success rate: 99.86%\n# Among successful attacks (Linf norm) on correctly classified examples:\n#    minimum distance: 0.001405\n#    median distance: 0.3509\n#    maximum distance: 0.6404\n#    average distance: 0.3476\n#    distance standard deviation: 0.05255\n\n# attack type: FABAttack\n# attack kwargs: norm=L2\n#                n_restarts=1\n#                n_iter=20\n#                alpha_max=0.1\n#                eta=1.05\n#                beta=0.9\n#                loss_fn=None\n# data: mnist_test\n# model: MNIST LeNet 5 PGD training according to Madry et al. 2018\n# accuracy: 98.64%\n# attack success rate: 98.35%\n# Among successful attacks (L2 norm) on correctly classified examples:\n#    minimum distance: 0.003942\n#    median distance: 3.04\n#    maximum distance: 19.92\n#    average distance: 3.205\n#    distance standard deviation: 1.311\n\n# attack type: FABAttack\n# attack kwargs: norm=L1\n#                n_restarts=1\n#                n_iter=20\n#                alpha_max=0.1\n#                eta=1.05\n#                beta=0.9\n#                loss_fn=None\n# data: mnist_test\n# model: MNIST LeNet 5 PGD training according to Madry et al. 2018\n# accuracy: 98.64%\n# attack success rate: 94.33%\n# Among successful attacks (L1 norm) on correctly classified examples:\n#    minimum distance: 0.00622\n#    median distance: 112.8\n#    maximum distance: 441.9\n#    average distance: 114.6\n#    distance standard deviation: 52.85\n\n\n\nfrom advertorch_examples.utils import get_mnist_test_loader\nfrom advertorch_examples.utils import get_mnist_lenet5_clntrained\nfrom advertorch_examples.utils import get_mnist_lenet5_advtrained\nfrom advertorch_examples.benchmark_utils import get_benchmark_sys_info\n\nfrom advertorch.attacks import FABAttack\n\nfrom advertorch_examples.benchmark_utils import benchmark_margin\n\nbatch_size = 100\ndevice = ""cuda""\n\nlst_attack = [\n    (FABAttack, dict(\n        norm=\'Linf\',\n        n_restarts=1,\n        n_iter=20,\n        alpha_max=0.1,\n        eta=1.05,\n        beta=0.9,\n        loss_fn=None)),\n    (FABAttack, dict(\n        norm=\'L2\',\n        n_restarts=1,\n        n_iter=20,\n        alpha_max=0.1,\n        eta=1.05,\n        beta=0.9,\n        loss_fn=None)),\n    (FABAttack, dict(\n        norm=\'L1\',\n        n_restarts=1,\n        n_iter=20,\n        alpha_max=0.1,\n        eta=1.05,\n        beta=0.9,\n        loss_fn=None)),\n]  # each element in the list is the tuple (attack_class, attack_kwargs)\n\nmnist_clntrained_model = get_mnist_lenet5_clntrained().to(device)\nmnist_advtrained_model = get_mnist_lenet5_advtrained().to(device)\nmnist_test_loader = get_mnist_test_loader(batch_size=batch_size)\n\nlst_setting = [\n    (mnist_clntrained_model, mnist_test_loader),\n    (mnist_advtrained_model, mnist_test_loader),\n]\n\n\ninfo = get_benchmark_sys_info()\n\nlst_benchmark = []\nfor model, loader in lst_setting:\n    for attack_class, attack_kwargs in lst_attack:\n        lst_benchmark.append(benchmark_margin(\n            model, loader, attack_class, attack_kwargs,\n            norm=attack_kwargs[""norm""]))\n\nprint(info)\nfor item in lst_benchmark:\n    print(item)\n'"
advertorch_examples/attack_benchmarks/benchmark_iterative_projected_gradient.py,12,"b'# Copyright (c) 2018-present, Royal Bank of Canada and other authors.\n# See the AUTHORS.txt file for a list of contributors.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n#\n#\n# Automatically generated benchmark report (screen print of running this file)\n#\n# sysname: Linux\n# release: 4.4.0-140-generic\n# version: #166-Ubuntu SMP Wed Nov 14 20:09:47 UTC 2018\n# machine: x86_64\n# python: 3.7.3\n# torch: 1.1.0\n# torchvision: 0.3.0\n# advertorch: 0.1.5\n\n# attack type: LinfPGDAttack\n# attack kwargs: loss_fn=CrossEntropyLoss()\n#                eps=0.3\n#                nb_iter=40\n#                eps_iter=0.01\n#                rand_init=False\n#                clip_min=0.0\n#                clip_max=1.0\n#                targeted=False\n# data: mnist_test\n# model: MNIST LeNet5 standard training\n# accuracy: 98.89%\n# attack success rate: 100.0%\n\n# attack type: LinfPGDAttack\n# attack kwargs: loss_fn=CrossEntropyLoss()\n#                eps=0.3\n#                nb_iter=40\n#                eps_iter=0.01\n#                rand_init=False\n#                clip_min=0.0\n#                clip_max=1.0\n#                targeted=False\n# data: mnist_test\n# model: MNIST LeNet 5 PGD training according to Madry et al. 2018\n# accuracy: 98.64%\n# attack success rate: 6.8%\n\n\nimport torch.nn as nn\n\nfrom advertorch_examples.utils import get_mnist_test_loader\nfrom advertorch_examples.utils import get_mnist_lenet5_clntrained\nfrom advertorch_examples.utils import get_mnist_lenet5_advtrained\nfrom advertorch_examples.benchmark_utils import get_benchmark_sys_info\n\nfrom advertorch.attacks import LinfPGDAttack\n# TODO: from advertorch.attacks import L2BasicIterativeAttack\n# TODO: from advertorch.attacks import LinfBasicIterativeAttack\n# TODO: from advertorch.attacks import PGDAttack\n# TODO: from advertorch.attacks import L2PGDAttack\n# TODO: from advertorch.attacks import L1PGDAttack\n# TODO: from advertorch.attacks import SparseL1DescentAttack\n# TODO: from advertorch.attacks import MomentumIterativeAttack\n# TODO: from advertorch.attacks import L2MomentumIterativeAttack\n# TODO: from advertorch.attacks import LinfMomentumIterativeAttack\n# TODO: from advertorch.attacks import FastFeatureAttack\n\nfrom advertorch_examples.benchmark_utils import benchmark_attack_success_rate\n\nbatch_size = 100\ndevice = ""cuda""\n\nlst_attack = [\n    (LinfPGDAttack, dict(\n        loss_fn=nn.CrossEntropyLoss(reduction=""sum""), eps=0.3,\n        nb_iter=40, eps_iter=0.01, rand_init=False,\n        clip_min=0.0, clip_max=1.0, targeted=False)),\n]  # each element in the list is the tuple (attack_class, attack_kwargs)\n\nmnist_clntrained_model = get_mnist_lenet5_clntrained().to(device)\nmnist_advtrained_model = get_mnist_lenet5_advtrained().to(device)\nmnist_test_loader = get_mnist_test_loader(batch_size=batch_size)\n\nlst_setting = [\n    (mnist_clntrained_model, mnist_test_loader),\n    (mnist_advtrained_model, mnist_test_loader),\n]\n\n\ninfo = get_benchmark_sys_info()\n\nlst_benchmark = []\nfor model, loader in lst_setting:\n    for attack_class, attack_kwargs in lst_attack:\n        lst_benchmark.append(benchmark_attack_success_rate(\n            model, loader, attack_class, attack_kwargs, device=""cuda""))\n\nprint(info)\nfor item in lst_benchmark:\n    print(item)\n'"
advertorch_examples/attack_benchmarks/benchmark_spsa_attack.py,1,"b'# Copyright (c) 2018-present, Royal Bank of Canada and other authors.\n# See the AUTHORS.txt file for a list of contributors.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n#\n# Automatically generated benchmark report (screen print of running this file)\n#\n# sysname: Linux\n# release: 4.4.0-140-generic\n# version: #166-Ubuntu SMP Wed Nov 14 20:09:47 UTC 2018\n# machine: x86_64\n# python: 3.7.3\n# torch: 1.1.0\n# torchvision: 0.3.0\n# advertorch: 0.1.5\n\n# attack type: LinfSPSAAttack\n# attack kwargs: eps=0.3\n#                delta=0.01\n#                lr=0.01\n#                nb_iter=1000\n#                nb_sample=128\n#                max_batch_size=64\n#                targeted=False\n#                loss_fn=None\n#                clip_min=0.0\n#                clip_max=1.0\n# data: mnist_test, 100 samples\n# model: MNIST LeNet5 standard training\n# accuracy: 99.0%\n# attack success rate: 100.0%\n\n# attack type: LinfSPSAAttack\n# attack kwargs: eps=0.3\n#                delta=0.01\n#                lr=0.01\n#                nb_iter=100\n#                nb_sample=8192\n#                max_batch_size=64\n#                targeted=False\n#                loss_fn=None\n#                clip_min=0.0\n#                clip_max=1.0\n# data: mnist_test, 100 samples\n# model: MNIST LeNet5 standard training\n# accuracy: 99.0%\n# attack success rate: 100.0%\n\n# attack type: LinfSPSAAttack\n# attack kwargs: eps=0.3\n#                delta=0.01\n#                lr=0.01\n#                nb_iter=1000\n#                nb_sample=128\n#                max_batch_size=64\n#                targeted=False\n#                loss_fn=None\n#                clip_min=0.0\n#                clip_max=1.0\n# data: mnist_test, 100 samples\n# model: MNIST LeNet 5 PGD training according to Madry et al. 2018\n# accuracy: 100.0%\n# attack success rate: 10.0%\n\n# attack type: LinfSPSAAttack\n# attack kwargs: eps=0.3\n#                delta=0.01\n#                lr=0.01\n#                nb_iter=100\n#                nb_sample=8192\n#                max_batch_size=64\n#                targeted=False\n#                loss_fn=None\n#                clip_min=0.0\n#                clip_max=1.0\n# data: mnist_test, 100 samples\n# model: MNIST LeNet 5 PGD training according to Madry et al. 2018\n# accuracy: 100.0%\n# attack success rate: 6.0%\n\n\n\nfrom advertorch_examples.utils import get_mnist_test_loader\nfrom advertorch_examples.utils import get_mnist_lenet5_clntrained\nfrom advertorch_examples.utils import get_mnist_lenet5_advtrained\nfrom advertorch_examples.benchmark_utils import get_benchmark_sys_info\n\nfrom advertorch.attacks import LinfSPSAAttack\n\nfrom advertorch_examples.benchmark_utils import benchmark_attack_success_rate\n\nbatch_size = 10\nnum_batch = 10\ndevice = ""cuda""\n\nlst_attack = [\n    (LinfSPSAAttack, dict(\n        eps=0.3, delta=0.01, lr=0.01, nb_iter=1000, nb_sample=128,\n        max_batch_size=64, targeted=False,\n        loss_fn=None,\n        clip_min=0.0, clip_max=1.0)),\n    (LinfSPSAAttack, dict(\n        eps=0.3, delta=0.01, lr=0.01, nb_iter=100, nb_sample=8192,\n        max_batch_size=64, targeted=False,\n        loss_fn=None,\n        clip_min=0.0, clip_max=1.0)),\n]  # each element in the list is the tuple (attack_class, attack_kwargs)\n\nmnist_clntrained_model = get_mnist_lenet5_clntrained().to(device)\nmnist_advtrained_model = get_mnist_lenet5_advtrained().to(device)\nmnist_test_loader = get_mnist_test_loader(batch_size=batch_size)\n\nlst_setting = [\n    (mnist_clntrained_model, mnist_test_loader),\n    (mnist_advtrained_model, mnist_test_loader),\n]\n\n\ninfo = get_benchmark_sys_info()\n\nlst_benchmark = []\nfor model, loader in lst_setting:\n    for attack_class, attack_kwargs in lst_attack:\n        lst_benchmark.append(benchmark_attack_success_rate(\n            model, loader, attack_class, attack_kwargs,\n            device=device, num_batch=num_batch))\n\nprint(info)\nfor item in lst_benchmark:\n    print(item)\n'"
advertorch_examples/attack_madry_et_al_models/attack_cifar10_challenge.py,3,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nimport torch.nn as nn\n\nfrom advertorch.attacks import LinfPGDAttack\nfrom advertorch.attacks.utils import multiple_mini_batch_attack\nfrom advertorch_examples.utils import get_cifar10_test_loader\n\nfrom madry_et_al_utils import get_madry_et_al_tf_model\n\nmodel = get_madry_et_al_tf_model(""CIFAR10"")\nloader = get_cifar10_test_loader(batch_size=100)\nadversary = LinfPGDAttack(\n    model, loss_fn=nn.CrossEntropyLoss(reduction=""sum""), eps=8. / 255,\n    nb_iter=20, eps_iter=2. / 255, rand_init=False, clip_min=0.0, clip_max=1.0,\n    targeted=False)\n\nlabel, pred, advpred, _ = multiple_mini_batch_attack(\n    adversary, loader, device=""cuda"")\n\nprint(""Accuracy: {:.2f}%, Robust Accuracy: {:.2f}%"".format(\n    100. * (label == pred).sum().item() / len(label),\n    100. * (label == advpred).sum().item() / len(label)))\n\n# Accuracy: 87.14%, Robust Accuracy: 45.69%\n'"
advertorch_examples/attack_madry_et_al_models/attack_mnist_challenge.py,3,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nimport torch.nn as nn\n\nfrom advertorch.attacks import LinfPGDAttack\nfrom advertorch.attacks.utils import multiple_mini_batch_attack\nfrom advertorch_examples.utils import get_mnist_test_loader\n\nfrom madry_et_al_utils import get_madry_et_al_tf_model\n\nmodel = get_madry_et_al_tf_model(""MNIST"")\nloader = get_mnist_test_loader(batch_size=100)\nadversary = LinfPGDAttack(\n    model, loss_fn=nn.CrossEntropyLoss(reduction=""sum""), eps=0.3,\n    nb_iter=100, eps_iter=0.01, rand_init=False, clip_min=0.0, clip_max=1.0,\n    targeted=False)\n\nlabel, pred, advpred, _ = multiple_mini_batch_attack(\n    adversary, loader, device=""cuda"")\n\nprint(""Accuracy: {:.2f}%, Robust Accuracy: {:.2f}%"".format(\n    100. * (label == pred).sum().item() / len(label),\n    100. * (label == advpred).sum().item() / len(label)))\n\n# Accuracy: 98.53%, Robust Accuracy: 92.51%\n'"
advertorch_examples/attack_madry_et_al_models/madry_et_al_utils.py,2,"b'# Copyright (c) 2018-present, Royal Bank of Canada.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nimport os\nimport sys\nfrom pathlib import Path\n\nimport tensorflow as tf\nimport torch\n\nfrom advertorch.bpda import BPDAWrapper\nfrom advertorch_examples.utils import ROOT_PATH, mkdir\n\nMODEL_PATH = os.path.join(ROOT_PATH, ""madry_et_al_models"")\nmkdir(MODEL_PATH)\nPath(os.path.join(MODEL_PATH, ""__init__.py"")).touch()\nsys.path.append(MODEL_PATH)\n\n\nclass WrappedTfModel(object):\n\n    def __init__(self, weights_path, model_class):\n\n        model = model_class()\n\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        sess = tf.Session(config=config).__enter__()\n        saver = tf.train.Saver()\n        checkpoint = tf.train.latest_checkpoint(weights_path)\n        saver.restore(sess, checkpoint)\n\n        self.inputs = model.x_input\n        self.logits = model.pre_softmax\n\n        self.session = tf.get_default_session()\n        assert self.session.graph == self.inputs.graph\n\n        with self.session.graph.as_default():\n            self.bw_gradient_pre = tf.placeholder(\n                tf.float32, self.logits.shape)\n            bw_loss = tf.reduce_sum(self.logits * self.bw_gradient_pre)\n            self.bw_gradients = tf.gradients(bw_loss, self.inputs)[0]\n\n    def backward(self, inputs_val, logits_grad_val):\n        inputs_grad_val = self.session.run(\n            self.bw_gradients,\n            feed_dict={\n                self.inputs: inputs_val,\n                self.bw_gradient_pre: logits_grad_val,\n            })\n        return inputs_grad_val\n\n    def forward(self, inputs_val):\n        logits_val = self.session.run(\n            self.logits,\n            feed_dict={\n                self.inputs: inputs_val,\n            })\n        return logits_val\n\n\nclass TorchWrappedModel(object):\n\n    def __init__(self, tfmodel, device):\n        self.tfmodel = tfmodel\n        self.device = device\n\n    def _to_numpy(self, val):\n        return val.cpu().detach().numpy()\n\n    def _to_torch(self, val):\n        return torch.from_numpy(val).float().to(self.device)\n\n    def forward(self, inputs_val):\n        rval = self.tfmodel.forward(self._to_numpy(inputs_val))\n        return self._to_torch(rval)\n\n    def backward(self, inputs_val, logits_grad_val):\n        rval = self.tfmodel.backward(\n            self._to_numpy(inputs_val),\n            self._to_numpy(logits_grad_val),\n        )\n        return self._to_torch(rval)\n\n\n\ndef get_madry_et_al_tf_model(dataname, device=""cuda""):\n    if dataname == ""MNIST"":\n        weights_path = os.path.join(\n            MODEL_PATH, \'mnist_challenge/models/secret\')\n\n        try:\n            from mnist_challenge.model import Model\n            print(""mnist_challenge found and imported"")\n        except (ImportError, ModuleNotFoundError):\n            print(""mnist_challenge not found, downloading ..."")\n            os.system(""bash download_mnist_challenge.sh {}"".format(MODEL_PATH))\n            from mnist_challenge.model import Model\n            print(""mnist_challenge found and imported"")\n\n        def _process_inputs_val(val):\n            return val.view(val.shape[0], 784)\n\n        def _process_grads_val(val):\n            return val.view(val.shape[0], 1, 28, 28)\n\n\n    elif dataname == ""CIFAR10"":\n        weights_path = os.path.join(\n            MODEL_PATH, \'cifar10_challenge/models/model_0\')\n\n        try:\n            from cifar10_challenge.model import Model\n            print(""cifar10_challenge found and imported"")\n        except (ImportError, ModuleNotFoundError):\n            print(""cifar10_challenge not found, downloading ..."")\n            os.system(\n                ""bash download_cifar10_challenge.sh {}"".format(MODEL_PATH))\n            from cifar10_challenge.model import Model\n            print(""cifar10_challenge found and imported"")\n\n        from functools import partial\n        Model = partial(Model, mode=""eval"")\n\n        def _process_inputs_val(val):\n            return 255. * val.permute(0, 2, 3, 1)\n\n        def _process_grads_val(val):\n            return val.permute(0, 3, 1, 2) / 255.\n\n    else:\n        raise ValueError(dataname)\n\n\n    def _wrap_forward(forward):\n        def new_forward(inputs_val):\n            return forward(_process_inputs_val(inputs_val))\n        return new_forward\n\n    def _wrap_backward(backward):\n        def new_backward(inputs_val, logits_grad_val):\n            return _process_grads_val(backward(\n                _process_inputs_val(*inputs_val), *logits_grad_val))\n        return new_backward\n\n\n    ptmodel = TorchWrappedModel(\n        WrappedTfModel(weights_path, Model), device)\n    model = BPDAWrapper(\n        forward=_wrap_forward(ptmodel.forward),\n        backward=_wrap_backward(ptmodel.backward)\n    )\n\n    return model\n'"
