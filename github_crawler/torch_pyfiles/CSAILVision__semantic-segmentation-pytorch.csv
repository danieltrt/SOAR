file_path,api_count,code
dataset.py,8,"b'import os\nimport json\nimport torch\nfrom torchvision import transforms\nimport numpy as np\nfrom PIL import Image\n\n\ndef imresize(im, size, interp=\'bilinear\'):\n    if interp == \'nearest\':\n        resample = Image.NEAREST\n    elif interp == \'bilinear\':\n        resample = Image.BILINEAR\n    elif interp == \'bicubic\':\n        resample = Image.BICUBIC\n    else:\n        raise Exception(\'resample method undefined!\')\n\n    return im.resize(size, resample)\n\n\nclass BaseDataset(torch.utils.data.Dataset):\n    def __init__(self, odgt, opt, **kwargs):\n        # parse options\n        self.imgSizes = opt.imgSizes\n        self.imgMaxSize = opt.imgMaxSize\n        # max down sampling rate of network to avoid rounding during conv or pooling\n        self.padding_constant = opt.padding_constant\n\n        # parse the input list\n        self.parse_input_list(odgt, **kwargs)\n\n        # mean and std\n        self.normalize = transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225])\n\n    def parse_input_list(self, odgt, max_sample=-1, start_idx=-1, end_idx=-1):\n        if isinstance(odgt, list):\n            self.list_sample = odgt\n        elif isinstance(odgt, str):\n            self.list_sample = [json.loads(x.rstrip()) for x in open(odgt, \'r\')]\n\n        if max_sample > 0:\n            self.list_sample = self.list_sample[0:max_sample]\n        if start_idx >= 0 and end_idx >= 0:     # divide file list\n            self.list_sample = self.list_sample[start_idx:end_idx]\n\n        self.num_sample = len(self.list_sample)\n        assert self.num_sample > 0\n        print(\'# samples: {}\'.format(self.num_sample))\n\n    def img_transform(self, img):\n        # 0-255 to 0-1\n        img = np.float32(np.array(img)) / 255.\n        img = img.transpose((2, 0, 1))\n        img = self.normalize(torch.from_numpy(img.copy()))\n        return img\n\n    def segm_transform(self, segm):\n        # to tensor, -1 to 149\n        segm = torch.from_numpy(np.array(segm)).long() - 1\n        return segm\n\n    # Round x to the nearest multiple of p and x\' >= x\n    def round2nearest_multiple(self, x, p):\n        return ((x - 1) // p + 1) * p\n\n\nclass TrainDataset(BaseDataset):\n    def __init__(self, root_dataset, odgt, opt, batch_per_gpu=1, **kwargs):\n        super(TrainDataset, self).__init__(odgt, opt, **kwargs)\n        self.root_dataset = root_dataset\n        # down sampling rate of segm labe\n        self.segm_downsampling_rate = opt.segm_downsampling_rate\n        self.batch_per_gpu = batch_per_gpu\n\n        # classify images into two classes: 1. h > w and 2. h <= w\n        self.batch_record_list = [[], []]\n\n        # override dataset length when trainig with batch_per_gpu > 1\n        self.cur_idx = 0\n        self.if_shuffled = False\n\n    def _get_sub_batch(self):\n        while True:\n            # get a sample record\n            this_sample = self.list_sample[self.cur_idx]\n            if this_sample[\'height\'] > this_sample[\'width\']:\n                self.batch_record_list[0].append(this_sample) # h > w, go to 1st class\n            else:\n                self.batch_record_list[1].append(this_sample) # h <= w, go to 2nd class\n\n            # update current sample pointer\n            self.cur_idx += 1\n            if self.cur_idx >= self.num_sample:\n                self.cur_idx = 0\n                np.random.shuffle(self.list_sample)\n\n            if len(self.batch_record_list[0]) == self.batch_per_gpu:\n                batch_records = self.batch_record_list[0]\n                self.batch_record_list[0] = []\n                break\n            elif len(self.batch_record_list[1]) == self.batch_per_gpu:\n                batch_records = self.batch_record_list[1]\n                self.batch_record_list[1] = []\n                break\n        return batch_records\n\n    def __getitem__(self, index):\n        # NOTE: random shuffle for the first time. shuffle in __init__ is useless\n        if not self.if_shuffled:\n            np.random.seed(index)\n            np.random.shuffle(self.list_sample)\n            self.if_shuffled = True\n\n        # get sub-batch candidates\n        batch_records = self._get_sub_batch()\n\n        # resize all images\' short edges to the chosen size\n        if isinstance(self.imgSizes, list) or isinstance(self.imgSizes, tuple):\n            this_short_size = np.random.choice(self.imgSizes)\n        else:\n            this_short_size = self.imgSizes\n\n        # calculate the BATCH\'s height and width\n        # since we concat more than one samples, the batch\'s h and w shall be larger than EACH sample\n        batch_widths = np.zeros(self.batch_per_gpu, np.int32)\n        batch_heights = np.zeros(self.batch_per_gpu, np.int32)\n        for i in range(self.batch_per_gpu):\n            img_height, img_width = batch_records[i][\'height\'], batch_records[i][\'width\']\n            this_scale = min(\n                this_short_size / min(img_height, img_width), \\\n                self.imgMaxSize / max(img_height, img_width))\n            batch_widths[i] = img_width * this_scale\n            batch_heights[i] = img_height * this_scale\n\n        # Here we must pad both input image and segmentation map to size h\' and w\' so that p | h\' and p | w\'\n        batch_width = np.max(batch_widths)\n        batch_height = np.max(batch_heights)\n        batch_width = int(self.round2nearest_multiple(batch_width, self.padding_constant))\n        batch_height = int(self.round2nearest_multiple(batch_height, self.padding_constant))\n\n        assert self.padding_constant >= self.segm_downsampling_rate, \\\n            \'padding constant must be equal or large than segm downsamping rate\'\n        batch_images = torch.zeros(\n            self.batch_per_gpu, 3, batch_height, batch_width)\n        batch_segms = torch.zeros(\n            self.batch_per_gpu,\n            batch_height // self.segm_downsampling_rate,\n            batch_width // self.segm_downsampling_rate).long()\n\n        for i in range(self.batch_per_gpu):\n            this_record = batch_records[i]\n\n            # load image and label\n            image_path = os.path.join(self.root_dataset, this_record[\'fpath_img\'])\n            segm_path = os.path.join(self.root_dataset, this_record[\'fpath_segm\'])\n\n            img = Image.open(image_path).convert(\'RGB\')\n            segm = Image.open(segm_path)\n            assert(segm.mode == ""L"")\n            assert(img.size[0] == segm.size[0])\n            assert(img.size[1] == segm.size[1])\n\n            # random_flip\n            if np.random.choice([0, 1]):\n                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n                segm = segm.transpose(Image.FLIP_LEFT_RIGHT)\n\n            # note that each sample within a mini batch has different scale param\n            img = imresize(img, (batch_widths[i], batch_heights[i]), interp=\'bilinear\')\n            segm = imresize(segm, (batch_widths[i], batch_heights[i]), interp=\'nearest\')\n\n            # further downsample seg label, need to avoid seg label misalignment\n            segm_rounded_width = self.round2nearest_multiple(segm.size[0], self.segm_downsampling_rate)\n            segm_rounded_height = self.round2nearest_multiple(segm.size[1], self.segm_downsampling_rate)\n            segm_rounded = Image.new(\'L\', (segm_rounded_width, segm_rounded_height), 0)\n            segm_rounded.paste(segm, (0, 0))\n            segm = imresize(\n                segm_rounded,\n                (segm_rounded.size[0] // self.segm_downsampling_rate, \\\n                 segm_rounded.size[1] // self.segm_downsampling_rate), \\\n                interp=\'nearest\')\n\n            # image transform, to torch float tensor 3xHxW\n            img = self.img_transform(img)\n\n            # segm transform, to torch long tensor HxW\n            segm = self.segm_transform(segm)\n\n            # put into batch arrays\n            batch_images[i][:, :img.shape[1], :img.shape[2]] = img\n            batch_segms[i][:segm.shape[0], :segm.shape[1]] = segm\n\n        output = dict()\n        output[\'img_data\'] = batch_images\n        output[\'seg_label\'] = batch_segms\n        return output\n\n    def __len__(self):\n        return int(1e10) # It\'s a fake length due to the trick that every loader maintains its own list\n        #return self.num_sampleclass\n\n\nclass ValDataset(BaseDataset):\n    def __init__(self, root_dataset, odgt, opt, **kwargs):\n        super(ValDataset, self).__init__(odgt, opt, **kwargs)\n        self.root_dataset = root_dataset\n\n    def __getitem__(self, index):\n        this_record = self.list_sample[index]\n        # load image and label\n        image_path = os.path.join(self.root_dataset, this_record[\'fpath_img\'])\n        segm_path = os.path.join(self.root_dataset, this_record[\'fpath_segm\'])\n        img = Image.open(image_path).convert(\'RGB\')\n        segm = Image.open(segm_path)\n        assert(segm.mode == ""L"")\n        assert(img.size[0] == segm.size[0])\n        assert(img.size[1] == segm.size[1])\n\n        ori_width, ori_height = img.size\n\n        img_resized_list = []\n        for this_short_size in self.imgSizes:\n            # calculate target height and width\n            scale = min(this_short_size / float(min(ori_height, ori_width)),\n                        self.imgMaxSize / float(max(ori_height, ori_width)))\n            target_height, target_width = int(ori_height * scale), int(ori_width * scale)\n\n            # to avoid rounding in network\n            target_width = self.round2nearest_multiple(target_width, self.padding_constant)\n            target_height = self.round2nearest_multiple(target_height, self.padding_constant)\n\n            # resize images\n            img_resized = imresize(img, (target_width, target_height), interp=\'bilinear\')\n\n            # image transform, to torch float tensor 3xHxW\n            img_resized = self.img_transform(img_resized)\n            img_resized = torch.unsqueeze(img_resized, 0)\n            img_resized_list.append(img_resized)\n\n        # segm transform, to torch long tensor HxW\n        segm = self.segm_transform(segm)\n        batch_segms = torch.unsqueeze(segm, 0)\n\n        output = dict()\n        output[\'img_ori\'] = np.array(img)\n        output[\'img_data\'] = [x.contiguous() for x in img_resized_list]\n        output[\'seg_label\'] = batch_segms.contiguous()\n        output[\'info\'] = this_record[\'fpath_img\']\n        return output\n\n    def __len__(self):\n        return self.num_sample\n\n\nclass TestDataset(BaseDataset):\n    def __init__(self, odgt, opt, **kwargs):\n        super(TestDataset, self).__init__(odgt, opt, **kwargs)\n\n    def __getitem__(self, index):\n        this_record = self.list_sample[index]\n        # load image\n        image_path = this_record[\'fpath_img\']\n        img = Image.open(image_path).convert(\'RGB\')\n\n        ori_width, ori_height = img.size\n\n        img_resized_list = []\n        for this_short_size in self.imgSizes:\n            # calculate target height and width\n            scale = min(this_short_size / float(min(ori_height, ori_width)),\n                        self.imgMaxSize / float(max(ori_height, ori_width)))\n            target_height, target_width = int(ori_height * scale), int(ori_width * scale)\n\n            # to avoid rounding in network\n            target_width = self.round2nearest_multiple(target_width, self.padding_constant)\n            target_height = self.round2nearest_multiple(target_height, self.padding_constant)\n\n            # resize images\n            img_resized = imresize(img, (target_width, target_height), interp=\'bilinear\')\n\n            # image transform, to torch float tensor 3xHxW\n            img_resized = self.img_transform(img_resized)\n            img_resized = torch.unsqueeze(img_resized, 0)\n            img_resized_list.append(img_resized)\n\n        output = dict()\n        output[\'img_ori\'] = np.array(img)\n        output[\'img_data\'] = [x.contiguous() for x in img_resized_list]\n        output[\'info\'] = this_record[\'fpath_img\']\n        return output\n\n    def __len__(self):\n        return self.num_sample\n'"
eval.py,9,"b'# System libs\nimport os\nimport time\nimport argparse\nfrom distutils.version import LooseVersion\n# Numerical libs\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom scipy.io import loadmat\n# Our libs\nfrom config import cfg\nfrom dataset import ValDataset\nfrom models import ModelBuilder, SegmentationModule\nfrom utils import AverageMeter, colorEncode, accuracy, intersectionAndUnion, setup_logger\nfrom lib.nn import user_scattered_collate, async_copy_to\nfrom lib.utils import as_numpy\nfrom PIL import Image\nfrom tqdm import tqdm\n\ncolors = loadmat(\'data/color150.mat\')[\'colors\']\n\n\ndef visualize_result(data, pred, dir_result):\n    (img, seg, info) = data\n\n    # segmentation\n    seg_color = colorEncode(seg, colors)\n\n    # prediction\n    pred_color = colorEncode(pred, colors)\n\n    # aggregate images and save\n    im_vis = np.concatenate((img, seg_color, pred_color),\n                            axis=1).astype(np.uint8)\n\n    img_name = info.split(\'/\')[-1]\n    Image.fromarray(im_vis).save(os.path.join(dir_result, img_name.replace(\'.jpg\', \'.png\')))\n\n\ndef evaluate(segmentation_module, loader, cfg, gpu):\n    acc_meter = AverageMeter()\n    intersection_meter = AverageMeter()\n    union_meter = AverageMeter()\n    time_meter = AverageMeter()\n\n    segmentation_module.eval()\n\n    pbar = tqdm(total=len(loader))\n    for batch_data in loader:\n        # process data\n        batch_data = batch_data[0]\n        seg_label = as_numpy(batch_data[\'seg_label\'][0])\n        img_resized_list = batch_data[\'img_data\']\n\n        torch.cuda.synchronize()\n        tic = time.perf_counter()\n        with torch.no_grad():\n            segSize = (seg_label.shape[0], seg_label.shape[1])\n            scores = torch.zeros(1, cfg.DATASET.num_class, segSize[0], segSize[1])\n            scores = async_copy_to(scores, gpu)\n\n            for img in img_resized_list:\n                feed_dict = batch_data.copy()\n                feed_dict[\'img_data\'] = img\n                del feed_dict[\'img_ori\']\n                del feed_dict[\'info\']\n                feed_dict = async_copy_to(feed_dict, gpu)\n\n                # forward pass\n                scores_tmp = segmentation_module(feed_dict, segSize=segSize)\n                scores = scores + scores_tmp / len(cfg.DATASET.imgSizes)\n\n            _, pred = torch.max(scores, dim=1)\n            pred = as_numpy(pred.squeeze(0).cpu())\n\n        torch.cuda.synchronize()\n        time_meter.update(time.perf_counter() - tic)\n\n        # calculate accuracy\n        acc, pix = accuracy(pred, seg_label)\n        intersection, union = intersectionAndUnion(pred, seg_label, cfg.DATASET.num_class)\n        acc_meter.update(acc, pix)\n        intersection_meter.update(intersection)\n        union_meter.update(union)\n\n        # visualization\n        if cfg.VAL.visualize:\n            visualize_result(\n                (batch_data[\'img_ori\'], seg_label, batch_data[\'info\']),\n                pred,\n                os.path.join(cfg.DIR, \'result\')\n            )\n\n        pbar.update(1)\n\n    # summary\n    iou = intersection_meter.sum / (union_meter.sum + 1e-10)\n    for i, _iou in enumerate(iou):\n        print(\'class [{}], IoU: {:.4f}\'.format(i, _iou))\n\n    print(\'[Eval Summary]:\')\n    print(\'Mean IoU: {:.4f}, Accuracy: {:.2f}%, Inference Time: {:.4f}s\'\n          .format(iou.mean(), acc_meter.average()*100, time_meter.average()))\n\n\ndef main(cfg, gpu):\n    torch.cuda.set_device(gpu)\n\n    # Network Builders\n    net_encoder = ModelBuilder.build_encoder(\n        arch=cfg.MODEL.arch_encoder.lower(),\n        fc_dim=cfg.MODEL.fc_dim,\n        weights=cfg.MODEL.weights_encoder)\n    net_decoder = ModelBuilder.build_decoder(\n        arch=cfg.MODEL.arch_decoder.lower(),\n        fc_dim=cfg.MODEL.fc_dim,\n        num_class=cfg.DATASET.num_class,\n        weights=cfg.MODEL.weights_decoder,\n        use_softmax=True)\n\n    crit = nn.NLLLoss(ignore_index=-1)\n\n    segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n\n    # Dataset and Loader\n    dataset_val = ValDataset(\n        cfg.DATASET.root_dataset,\n        cfg.DATASET.list_val,\n        cfg.DATASET)\n    loader_val = torch.utils.data.DataLoader(\n        dataset_val,\n        batch_size=cfg.VAL.batch_size,\n        shuffle=False,\n        collate_fn=user_scattered_collate,\n        num_workers=5,\n        drop_last=True)\n\n    segmentation_module.cuda()\n\n    # Main loop\n    evaluate(segmentation_module, loader_val, cfg, gpu)\n\n    print(\'Evaluation Done!\')\n\n\nif __name__ == \'__main__\':\n    assert LooseVersion(torch.__version__) >= LooseVersion(\'0.4.0\'), \\\n        \'PyTorch>=0.4.0 is required\'\n\n    parser = argparse.ArgumentParser(\n        description=""PyTorch Semantic Segmentation Validation""\n    )\n    parser.add_argument(\n        ""--cfg"",\n        default=""config/ade20k-resnet50dilated-ppm_deepsup.yaml"",\n        metavar=""FILE"",\n        help=""path to config file"",\n        type=str,\n    )\n    parser.add_argument(\n        ""--gpu"",\n        default=0,\n        help=""gpu to use""\n    )\n    parser.add_argument(\n        ""opts"",\n        help=""Modify config options using the command-line"",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n    args = parser.parse_args()\n\n    cfg.merge_from_file(args.cfg)\n    cfg.merge_from_list(args.opts)\n    # cfg.freeze()\n\n    logger = setup_logger(distributed_rank=0)   # TODO\n    logger.info(""Loaded configuration file {}"".format(args.cfg))\n    logger.info(""Running with config:\\n{}"".format(cfg))\n\n    # absolute paths of model weights\n    cfg.MODEL.weights_encoder = os.path.join(\n        cfg.DIR, \'encoder_\' + cfg.VAL.checkpoint)\n    cfg.MODEL.weights_decoder = os.path.join(\n        cfg.DIR, \'decoder_\' + cfg.VAL.checkpoint)\n    assert os.path.exists(cfg.MODEL.weights_encoder) and \\\n        os.path.exists(cfg.MODEL.weights_decoder), ""checkpoint does not exitst!""\n\n    if not os.path.isdir(os.path.join(cfg.DIR, ""result"")):\n        os.makedirs(os.path.join(cfg.DIR, ""result""))\n\n    main(cfg, args.gpu)\n'"
eval_multipro.py,7,"b'# System libs\nimport os\nimport argparse\nfrom distutils.version import LooseVersion\nfrom multiprocessing import Queue, Process\n# Numerical libs\nimport numpy as np\nimport math\nimport torch\nimport torch.nn as nn\nfrom scipy.io import loadmat\n# Our libs\nfrom config import cfg\nfrom dataset import ValDataset\nfrom models import ModelBuilder, SegmentationModule\nfrom utils import AverageMeter, colorEncode, accuracy, intersectionAndUnion, parse_devices, setup_logger\nfrom lib.nn import user_scattered_collate, async_copy_to\nfrom lib.utils import as_numpy\nfrom PIL import Image\nfrom tqdm import tqdm\n\ncolors = loadmat(\'data/color150.mat\')[\'colors\']\n\n\ndef visualize_result(data, pred, dir_result):\n    (img, seg, info) = data\n\n    # segmentation\n    seg_color = colorEncode(seg, colors)\n\n    # prediction\n    pred_color = colorEncode(pred, colors)\n\n    # aggregate images and save\n    im_vis = np.concatenate((img, seg_color, pred_color),\n                            axis=1).astype(np.uint8)\n\n    img_name = info.split(\'/\')[-1]\n    Image.fromarray(im_vis).save(os.path.join(dir_result, img_name.replace(\'.jpg\', \'.png\')))\n\n\ndef evaluate(segmentation_module, loader, cfg, gpu_id, result_queue):\n    segmentation_module.eval()\n\n    for batch_data in loader:\n        # process data\n        batch_data = batch_data[0]\n        seg_label = as_numpy(batch_data[\'seg_label\'][0])\n        img_resized_list = batch_data[\'img_data\']\n\n        with torch.no_grad():\n            segSize = (seg_label.shape[0], seg_label.shape[1])\n            scores = torch.zeros(1, cfg.DATASET.num_class, segSize[0], segSize[1])\n            scores = async_copy_to(scores, gpu_id)\n\n            for img in img_resized_list:\n                feed_dict = batch_data.copy()\n                feed_dict[\'img_data\'] = img\n                del feed_dict[\'img_ori\']\n                del feed_dict[\'info\']\n                feed_dict = async_copy_to(feed_dict, gpu_id)\n\n                # forward pass\n                scores_tmp = segmentation_module(feed_dict, segSize=segSize)\n                scores = scores + scores_tmp / len(cfg.DATASET.imgSizes)\n\n            _, pred = torch.max(scores, dim=1)\n            pred = as_numpy(pred.squeeze(0).cpu())\n\n        # calculate accuracy and SEND THEM TO MASTER\n        acc, pix = accuracy(pred, seg_label)\n        intersection, union = intersectionAndUnion(pred, seg_label, cfg.DATASET.num_class)\n        result_queue.put_nowait((acc, pix, intersection, union))\n\n        # visualization\n        if cfg.VAL.visualize:\n            visualize_result(\n                (batch_data[\'img_ori\'], seg_label, batch_data[\'info\']),\n                pred,\n                os.path.join(cfg.DIR, \'result\')\n            )\n\n\ndef worker(cfg, gpu_id, start_idx, end_idx, result_queue):\n    torch.cuda.set_device(gpu_id)\n\n    # Dataset and Loader\n    dataset_val = ValDataset(\n        cfg.DATASET.root_dataset,\n        cfg.DATASET.list_val,\n        cfg.DATASET,\n        start_idx=start_idx, end_idx=end_idx)\n    loader_val = torch.utils.data.DataLoader(\n        dataset_val,\n        batch_size=cfg.VAL.batch_size,\n        shuffle=False,\n        collate_fn=user_scattered_collate,\n        num_workers=2)\n\n    # Network Builders\n    net_encoder = ModelBuilder.build_encoder(\n        arch=cfg.MODEL.arch_encoder.lower(),\n        fc_dim=cfg.MODEL.fc_dim,\n        weights=cfg.MODEL.weights_encoder)\n    net_decoder = ModelBuilder.build_decoder(\n        arch=cfg.MODEL.arch_decoder.lower(),\n        fc_dim=cfg.MODEL.fc_dim,\n        num_class=cfg.DATASET.num_class,\n        weights=cfg.MODEL.weights_decoder,\n        use_softmax=True)\n\n    crit = nn.NLLLoss(ignore_index=-1)\n\n    segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n\n    segmentation_module.cuda()\n\n    # Main loop\n    evaluate(segmentation_module, loader_val, cfg, gpu_id, result_queue)\n\n\ndef main(cfg, gpus):\n    with open(cfg.DATASET.list_val, \'r\') as f:\n        lines = f.readlines()\n        num_files = len(lines)\n\n    num_files_per_gpu = math.ceil(num_files / len(gpus))\n\n    pbar = tqdm(total=num_files)\n\n    acc_meter = AverageMeter()\n    intersection_meter = AverageMeter()\n    union_meter = AverageMeter()\n\n    result_queue = Queue(500)\n    procs = []\n    for idx, gpu_id in enumerate(gpus):\n        start_idx = idx * num_files_per_gpu\n        end_idx = min(start_idx + num_files_per_gpu, num_files)\n        proc = Process(target=worker, args=(cfg, gpu_id, start_idx, end_idx, result_queue))\n        print(\'gpu:{}, start_idx:{}, end_idx:{}\'.format(gpu_id, start_idx, end_idx))\n        proc.start()\n        procs.append(proc)\n\n    # master fetches results\n    processed_counter = 0\n    while processed_counter < num_files:\n        if result_queue.empty():\n            continue\n        (acc, pix, intersection, union) = result_queue.get()\n        acc_meter.update(acc, pix)\n        intersection_meter.update(intersection)\n        union_meter.update(union)\n        processed_counter += 1\n        pbar.update(1)\n\n    for p in procs:\n        p.join()\n\n    # summary\n    iou = intersection_meter.sum / (union_meter.sum + 1e-10)\n    for i, _iou in enumerate(iou):\n        print(\'class [{}], IoU: {:.4f}\'.format(i, _iou))\n\n    print(\'[Eval Summary]:\')\n    print(\'Mean IoU: {:.4f}, Accuracy: {:.2f}%\'\n          .format(iou.mean(), acc_meter.average()*100))\n\n    print(\'Evaluation Done!\')\n\n\nif __name__ == \'__main__\':\n    assert LooseVersion(torch.__version__) >= LooseVersion(\'0.4.0\'), \\\n        \'PyTorch>=0.4.0 is required\'\n\n    parser = argparse.ArgumentParser(\n        description=""PyTorch Semantic Segmentation Validation""\n    )\n    parser.add_argument(\n        ""--cfg"",\n        default=""config/ade20k-resnet50dilated-ppm_deepsup.yaml"",\n        metavar=""FILE"",\n        help=""path to config file"",\n        type=str,\n    )\n    parser.add_argument(\n        ""--gpus"",\n        default=""0-3"",\n        help=""gpus to use, e.g. 0-3 or 0,1,2,3""\n    )\n    parser.add_argument(\n        ""opts"",\n        help=""Modify config options using the command-line"",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n    args = parser.parse_args()\n\n    cfg.merge_from_file(args.cfg)\n    cfg.merge_from_list(args.opts)\n    # cfg.freeze()\n\n    logger = setup_logger(distributed_rank=0)   # TODO\n    logger.info(""Loaded configuration file {}"".format(args.cfg))\n    logger.info(""Running with config:\\n{}"".format(cfg))\n\n    # absolute paths of model weights\n    cfg.MODEL.weights_encoder = os.path.join(\n        cfg.DIR, \'encoder_\' + cfg.VAL.checkpoint)\n    cfg.MODEL.weights_decoder = os.path.join(\n        cfg.DIR, \'decoder_\' + cfg.VAL.checkpoint)\n    assert os.path.exists(cfg.MODEL.weights_encoder) and \\\n        os.path.exists(cfg.MODEL.weights_decoder), ""checkpoint does not exitst!""\n\n    if not os.path.isdir(os.path.join(cfg.DIR, ""result"")):\n        os.makedirs(os.path.join(cfg.DIR, ""result""))\n\n    # Parse gpu ids\n    gpus = parse_devices(args.gpus)\n    gpus = [x.replace(\'gpu\', \'\') for x in gpus]\n    gpus = [int(x) for x in gpus]\n\n    main(cfg, gpus)\n'"
test.py,7,"b'# System libs\nimport os\nimport argparse\nfrom distutils.version import LooseVersion\n# Numerical libs\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom scipy.io import loadmat\nimport csv\n# Our libs\nfrom dataset import TestDataset\nfrom models import ModelBuilder, SegmentationModule\nfrom utils import colorEncode, find_recursive, setup_logger\nfrom lib.nn import user_scattered_collate, async_copy_to\nfrom lib.utils import as_numpy\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom config import cfg\n\ncolors = loadmat(\'data/color150.mat\')[\'colors\']\nnames = {}\nwith open(\'data/object150_info.csv\') as f:\n    reader = csv.reader(f)\n    next(reader)\n    for row in reader:\n        names[int(row[0])] = row[5].split("";"")[0]\n\n\ndef visualize_result(data, pred, cfg):\n    (img, info) = data\n\n    # print predictions in descending order\n    pred = np.int32(pred)\n    pixs = pred.size\n    uniques, counts = np.unique(pred, return_counts=True)\n    print(""Predictions in [{}]:"".format(info))\n    for idx in np.argsort(counts)[::-1]:\n        name = names[uniques[idx] + 1]\n        ratio = counts[idx] / pixs * 100\n        if ratio > 0.1:\n            print(""  {}: {:.2f}%"".format(name, ratio))\n\n    # colorize prediction\n    pred_color = colorEncode(pred, colors).astype(np.uint8)\n\n    # aggregate images and save\n    im_vis = np.concatenate((img, pred_color), axis=1)\n\n    img_name = info.split(\'/\')[-1]\n    Image.fromarray(im_vis).save(\n        os.path.join(cfg.TEST.result, img_name.replace(\'.jpg\', \'.png\')))\n\n\ndef test(segmentation_module, loader, gpu):\n    segmentation_module.eval()\n\n    pbar = tqdm(total=len(loader))\n    for batch_data in loader:\n        # process data\n        batch_data = batch_data[0]\n        segSize = (batch_data[\'img_ori\'].shape[0],\n                   batch_data[\'img_ori\'].shape[1])\n        img_resized_list = batch_data[\'img_data\']\n\n        with torch.no_grad():\n            scores = torch.zeros(1, cfg.DATASET.num_class, segSize[0], segSize[1])\n            scores = async_copy_to(scores, gpu)\n\n            for img in img_resized_list:\n                feed_dict = batch_data.copy()\n                feed_dict[\'img_data\'] = img\n                del feed_dict[\'img_ori\']\n                del feed_dict[\'info\']\n                feed_dict = async_copy_to(feed_dict, gpu)\n\n                # forward pass\n                pred_tmp = segmentation_module(feed_dict, segSize=segSize)\n                scores = scores + pred_tmp / len(cfg.DATASET.imgSizes)\n\n            _, pred = torch.max(scores, dim=1)\n            pred = as_numpy(pred.squeeze(0).cpu())\n\n        # visualization\n        visualize_result(\n            (batch_data[\'img_ori\'], batch_data[\'info\']),\n            pred,\n            cfg\n        )\n\n        pbar.update(1)\n\n\ndef main(cfg, gpu):\n    torch.cuda.set_device(gpu)\n\n    # Network Builders\n    net_encoder = ModelBuilder.build_encoder(\n        arch=cfg.MODEL.arch_encoder,\n        fc_dim=cfg.MODEL.fc_dim,\n        weights=cfg.MODEL.weights_encoder)\n    net_decoder = ModelBuilder.build_decoder(\n        arch=cfg.MODEL.arch_decoder,\n        fc_dim=cfg.MODEL.fc_dim,\n        num_class=cfg.DATASET.num_class,\n        weights=cfg.MODEL.weights_decoder,\n        use_softmax=True)\n\n    crit = nn.NLLLoss(ignore_index=-1)\n\n    segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n\n    # Dataset and Loader\n    dataset_test = TestDataset(\n        cfg.list_test,\n        cfg.DATASET)\n    loader_test = torch.utils.data.DataLoader(\n        dataset_test,\n        batch_size=cfg.TEST.batch_size,\n        shuffle=False,\n        collate_fn=user_scattered_collate,\n        num_workers=5,\n        drop_last=True)\n\n    segmentation_module.cuda()\n\n    # Main loop\n    test(segmentation_module, loader_test, gpu)\n\n    print(\'Inference done!\')\n\n\nif __name__ == \'__main__\':\n    assert LooseVersion(torch.__version__) >= LooseVersion(\'0.4.0\'), \\\n        \'PyTorch>=0.4.0 is required\'\n\n    parser = argparse.ArgumentParser(\n        description=""PyTorch Semantic Segmentation Testing""\n    )\n    parser.add_argument(\n        ""--imgs"",\n        required=True,\n        type=str,\n        help=""an image paths, or a directory name""\n    )\n    parser.add_argument(\n        ""--cfg"",\n        default=""config/ade20k-resnet50dilated-ppm_deepsup.yaml"",\n        metavar=""FILE"",\n        help=""path to config file"",\n        type=str,\n    )\n    parser.add_argument(\n        ""--gpu"",\n        default=0,\n        type=int,\n        help=""gpu id for evaluation""\n    )\n    parser.add_argument(\n        ""opts"",\n        help=""Modify config options using the command-line"",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n    args = parser.parse_args()\n\n    cfg.merge_from_file(args.cfg)\n    cfg.merge_from_list(args.opts)\n    # cfg.freeze()\n\n    logger = setup_logger(distributed_rank=0)   # TODO\n    logger.info(""Loaded configuration file {}"".format(args.cfg))\n    logger.info(""Running with config:\\n{}"".format(cfg))\n\n    cfg.MODEL.arch_encoder = cfg.MODEL.arch_encoder.lower()\n    cfg.MODEL.arch_decoder = cfg.MODEL.arch_decoder.lower()\n\n    # absolute paths of model weights\n    cfg.MODEL.weights_encoder = os.path.join(\n        cfg.DIR, \'encoder_\' + cfg.TEST.checkpoint)\n    cfg.MODEL.weights_decoder = os.path.join(\n        cfg.DIR, \'decoder_\' + cfg.TEST.checkpoint)\n\n    assert os.path.exists(cfg.MODEL.weights_encoder) and \\\n        os.path.exists(cfg.MODEL.weights_decoder), ""checkpoint does not exitst!""\n\n    # generate testing image list\n    if os.path.isdir(args.imgs[0]):\n        imgs = find_recursive(args.imgs[0])\n    else:\n        imgs = [args.imgs]\n    assert len(imgs), ""imgs should be a path to image (.jpg) or directory.""\n    cfg.list_test = [{\'fpath_img\': x} for x in imgs]\n\n    if not os.path.isdir(cfg.TEST.result):\n        os.makedirs(cfg.TEST.result)\n\n    main(cfg, args.gpu)\n'"
train.py,9,"b'# System libs\nimport os\nimport time\n# import math\nimport random\nimport argparse\nfrom distutils.version import LooseVersion\n# Numerical libs\nimport torch\nimport torch.nn as nn\n# Our libs\nfrom config import cfg\nfrom dataset import TrainDataset\nfrom models import ModelBuilder, SegmentationModule\nfrom utils import AverageMeter, parse_devices, setup_logger\nfrom lib.nn import UserScatteredDataParallel, user_scattered_collate, patch_replication_callback\n\n\n# train one epoch\ndef train(segmentation_module, iterator, optimizers, history, epoch, cfg):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    ave_total_loss = AverageMeter()\n    ave_acc = AverageMeter()\n\n    segmentation_module.train(not cfg.TRAIN.fix_bn)\n\n    # main loop\n    tic = time.time()\n    for i in range(cfg.TRAIN.epoch_iters):\n        # load a batch of data\n        batch_data = next(iterator)\n        data_time.update(time.time() - tic)\n        segmentation_module.zero_grad()\n\n        # adjust learning rate\n        cur_iter = i + (epoch - 1) * cfg.TRAIN.epoch_iters\n        adjust_learning_rate(optimizers, cur_iter, cfg)\n\n        # forward pass\n        loss, acc = segmentation_module(batch_data)\n        loss = loss.mean()\n        acc = acc.mean()\n\n        # Backward\n        loss.backward()\n        for optimizer in optimizers:\n            optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - tic)\n        tic = time.time()\n\n        # update average loss and acc\n        ave_total_loss.update(loss.data.item())\n        ave_acc.update(acc.data.item()*100)\n\n        # calculate accuracy, and display\n        if i % cfg.TRAIN.disp_iter == 0:\n            print(\'Epoch: [{}][{}/{}], Time: {:.2f}, Data: {:.2f}, \'\n                  \'lr_encoder: {:.6f}, lr_decoder: {:.6f}, \'\n                  \'Accuracy: {:4.2f}, Loss: {:.6f}\'\n                  .format(epoch, i, cfg.TRAIN.epoch_iters,\n                          batch_time.average(), data_time.average(),\n                          cfg.TRAIN.running_lr_encoder, cfg.TRAIN.running_lr_decoder,\n                          ave_acc.average(), ave_total_loss.average()))\n\n            fractional_epoch = epoch - 1 + 1. * i / cfg.TRAIN.epoch_iters\n            history[\'train\'][\'epoch\'].append(fractional_epoch)\n            history[\'train\'][\'loss\'].append(loss.data.item())\n            history[\'train\'][\'acc\'].append(acc.data.item())\n\n\ndef checkpoint(nets, history, cfg, epoch):\n    print(\'Saving checkpoints...\')\n    (net_encoder, net_decoder, crit) = nets\n\n    dict_encoder = net_encoder.state_dict()\n    dict_decoder = net_decoder.state_dict()\n\n    torch.save(\n        history,\n        \'{}/history_epoch_{}.pth\'.format(cfg.DIR, epoch))\n    torch.save(\n        dict_encoder,\n        \'{}/encoder_epoch_{}.pth\'.format(cfg.DIR, epoch))\n    torch.save(\n        dict_decoder,\n        \'{}/decoder_epoch_{}.pth\'.format(cfg.DIR, epoch))\n\n\ndef group_weight(module):\n    group_decay = []\n    group_no_decay = []\n    for m in module.modules():\n        if isinstance(m, nn.Linear):\n            group_decay.append(m.weight)\n            if m.bias is not None:\n                group_no_decay.append(m.bias)\n        elif isinstance(m, nn.modules.conv._ConvNd):\n            group_decay.append(m.weight)\n            if m.bias is not None:\n                group_no_decay.append(m.bias)\n        elif isinstance(m, nn.modules.batchnorm._BatchNorm):\n            if m.weight is not None:\n                group_no_decay.append(m.weight)\n            if m.bias is not None:\n                group_no_decay.append(m.bias)\n\n    assert len(list(module.parameters())) == len(group_decay) + len(group_no_decay)\n    groups = [dict(params=group_decay), dict(params=group_no_decay, weight_decay=.0)]\n    return groups\n\n\ndef create_optimizers(nets, cfg):\n    (net_encoder, net_decoder, crit) = nets\n    optimizer_encoder = torch.optim.SGD(\n        group_weight(net_encoder),\n        lr=cfg.TRAIN.lr_encoder,\n        momentum=cfg.TRAIN.beta1,\n        weight_decay=cfg.TRAIN.weight_decay)\n    optimizer_decoder = torch.optim.SGD(\n        group_weight(net_decoder),\n        lr=cfg.TRAIN.lr_decoder,\n        momentum=cfg.TRAIN.beta1,\n        weight_decay=cfg.TRAIN.weight_decay)\n    return (optimizer_encoder, optimizer_decoder)\n\n\ndef adjust_learning_rate(optimizers, cur_iter, cfg):\n    scale_running_lr = ((1. - float(cur_iter) / cfg.TRAIN.max_iters) ** cfg.TRAIN.lr_pow)\n    cfg.TRAIN.running_lr_encoder = cfg.TRAIN.lr_encoder * scale_running_lr\n    cfg.TRAIN.running_lr_decoder = cfg.TRAIN.lr_decoder * scale_running_lr\n\n    (optimizer_encoder, optimizer_decoder) = optimizers\n    for param_group in optimizer_encoder.param_groups:\n        param_group[\'lr\'] = cfg.TRAIN.running_lr_encoder\n    for param_group in optimizer_decoder.param_groups:\n        param_group[\'lr\'] = cfg.TRAIN.running_lr_decoder\n\n\ndef main(cfg, gpus):\n    # Network Builders\n    net_encoder = ModelBuilder.build_encoder(\n        arch=cfg.MODEL.arch_encoder.lower(),\n        fc_dim=cfg.MODEL.fc_dim,\n        weights=cfg.MODEL.weights_encoder)\n    net_decoder = ModelBuilder.build_decoder(\n        arch=cfg.MODEL.arch_decoder.lower(),\n        fc_dim=cfg.MODEL.fc_dim,\n        num_class=cfg.DATASET.num_class,\n        weights=cfg.MODEL.weights_decoder)\n\n    crit = nn.NLLLoss(ignore_index=-1)\n\n    if cfg.MODEL.arch_decoder.endswith(\'deepsup\'):\n        segmentation_module = SegmentationModule(\n            net_encoder, net_decoder, crit, cfg.TRAIN.deep_sup_scale)\n    else:\n        segmentation_module = SegmentationModule(\n            net_encoder, net_decoder, crit)\n\n    # Dataset and Loader\n    dataset_train = TrainDataset(\n        cfg.DATASET.root_dataset,\n        cfg.DATASET.list_train,\n        cfg.DATASET,\n        batch_per_gpu=cfg.TRAIN.batch_size_per_gpu)\n\n    loader_train = torch.utils.data.DataLoader(\n        dataset_train,\n        batch_size=len(gpus),  # we have modified data_parallel\n        shuffle=False,  # we do not use this param\n        collate_fn=user_scattered_collate,\n        num_workers=cfg.TRAIN.workers,\n        drop_last=True,\n        pin_memory=True)\n    print(\'1 Epoch = {} iters\'.format(cfg.TRAIN.epoch_iters))\n\n    # create loader iterator\n    iterator_train = iter(loader_train)\n\n    # load nets into gpu\n    if len(gpus) > 1:\n        segmentation_module = UserScatteredDataParallel(\n            segmentation_module,\n            device_ids=gpus)\n        # For sync bn\n        patch_replication_callback(segmentation_module)\n    segmentation_module.cuda()\n\n    # Set up optimizers\n    nets = (net_encoder, net_decoder, crit)\n    optimizers = create_optimizers(nets, cfg)\n\n    # Main loop\n    history = {\'train\': {\'epoch\': [], \'loss\': [], \'acc\': []}}\n\n    for epoch in range(cfg.TRAIN.start_epoch, cfg.TRAIN.num_epoch):\n        train(segmentation_module, iterator_train, optimizers, history, epoch+1, cfg)\n\n        # checkpointing\n        checkpoint(nets, history, cfg, epoch+1)\n\n    print(\'Training Done!\')\n\n\nif __name__ == \'__main__\':\n    assert LooseVersion(torch.__version__) >= LooseVersion(\'0.4.0\'), \\\n        \'PyTorch>=0.4.0 is required\'\n\n    parser = argparse.ArgumentParser(\n        description=""PyTorch Semantic Segmentation Training""\n    )\n    parser.add_argument(\n        ""--cfg"",\n        default=""config/ade20k-resnet50dilated-ppm_deepsup.yaml"",\n        metavar=""FILE"",\n        help=""path to config file"",\n        type=str,\n    )\n    parser.add_argument(\n        ""--gpus"",\n        default=""0-3"",\n        help=""gpus to use, e.g. 0-3 or 0,1,2,3""\n    )\n    parser.add_argument(\n        ""opts"",\n        help=""Modify config options using the command-line"",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n    args = parser.parse_args()\n\n    cfg.merge_from_file(args.cfg)\n    cfg.merge_from_list(args.opts)\n    # cfg.freeze()\n\n    logger = setup_logger(distributed_rank=0)   # TODO\n    logger.info(""Loaded configuration file {}"".format(args.cfg))\n    logger.info(""Running with config:\\n{}"".format(cfg))\n\n    # Output directory\n    if not os.path.isdir(cfg.DIR):\n        os.makedirs(cfg.DIR)\n    logger.info(""Outputing checkpoints to: {}"".format(cfg.DIR))\n    with open(os.path.join(cfg.DIR, \'config.yaml\'), \'w\') as f:\n        f.write(""{}"".format(cfg))\n\n    # Start from checkpoint\n    if cfg.TRAIN.start_epoch > 0:\n        cfg.MODEL.weights_encoder = os.path.join(\n            cfg.DIR, \'encoder_epoch_{}.pth\'.format(cfg.TRAIN.start_epoch))\n        cfg.MODEL.weights_decoder = os.path.join(\n            cfg.DIR, \'decoder_epoch_{}.pth\'.format(cfg.TRAIN.start_epoch))\n        assert os.path.exists(cfg.MODEL.weights_encoder) and \\\n            os.path.exists(cfg.MODEL.weights_decoder), ""checkpoint does not exitst!""\n\n    # Parse gpu ids\n    gpus = parse_devices(args.gpus)\n    gpus = [x.replace(\'gpu\', \'\') for x in gpus]\n    gpus = [int(x) for x in gpus]\n    num_gpus = len(gpus)\n    cfg.TRAIN.batch_size = num_gpus * cfg.TRAIN.batch_size_per_gpu\n\n    cfg.TRAIN.max_iters = cfg.TRAIN.epoch_iters * cfg.TRAIN.num_epoch\n    cfg.TRAIN.running_lr_encoder = cfg.TRAIN.lr_encoder\n    cfg.TRAIN.running_lr_decoder = cfg.TRAIN.lr_decoder\n\n    random.seed(cfg.TRAIN.seed)\n    torch.manual_seed(cfg.TRAIN.seed)\n\n    main(cfg, gpus)\n'"
utils.py,0,"b'import sys\nimport os\nimport logging\nimport re\nimport functools\nimport fnmatch\nimport numpy as np\n\n\ndef setup_logger(distributed_rank=0, filename=""log.txt""):\n    logger = logging.getLogger(""Logger"")\n    logger.setLevel(logging.DEBUG)\n    # don\'t log results for the non-master process\n    if distributed_rank > 0:\n        return logger\n    ch = logging.StreamHandler(stream=sys.stdout)\n    ch.setLevel(logging.DEBUG)\n    fmt = ""[%(asctime)s %(levelname)s %(filename)s line %(lineno)d %(process)d] %(message)s""\n    ch.setFormatter(logging.Formatter(fmt))\n    logger.addHandler(ch)\n\n    return logger\n\n\ndef find_recursive(root_dir, ext=\'.jpg\'):\n    files = []\n    for root, dirnames, filenames in os.walk(root_dir):\n        for filename in fnmatch.filter(filenames, \'*\' + ext):\n            files.append(os.path.join(root, filename))\n    return files\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.initialized = False\n        self.val = None\n        self.avg = None\n        self.sum = None\n        self.count = None\n\n    def initialize(self, val, weight):\n        self.val = val\n        self.avg = val\n        self.sum = val * weight\n        self.count = weight\n        self.initialized = True\n\n    def update(self, val, weight=1):\n        if not self.initialized:\n            self.initialize(val, weight)\n        else:\n            self.add(val, weight)\n\n    def add(self, val, weight):\n        self.val = val\n        self.sum += val * weight\n        self.count += weight\n        self.avg = self.sum / self.count\n\n    def value(self):\n        return self.val\n\n    def average(self):\n        return self.avg\n\n\ndef unique(ar, return_index=False, return_inverse=False, return_counts=False):\n    ar = np.asanyarray(ar).flatten()\n\n    optional_indices = return_index or return_inverse\n    optional_returns = optional_indices or return_counts\n\n    if ar.size == 0:\n        if not optional_returns:\n            ret = ar\n        else:\n            ret = (ar,)\n            if return_index:\n                ret += (np.empty(0, np.bool),)\n            if return_inverse:\n                ret += (np.empty(0, np.bool),)\n            if return_counts:\n                ret += (np.empty(0, np.intp),)\n        return ret\n    if optional_indices:\n        perm = ar.argsort(kind=\'mergesort\' if return_index else \'quicksort\')\n        aux = ar[perm]\n    else:\n        ar.sort()\n        aux = ar\n    flag = np.concatenate(([True], aux[1:] != aux[:-1]))\n\n    if not optional_returns:\n        ret = aux[flag]\n    else:\n        ret = (aux[flag],)\n        if return_index:\n            ret += (perm[flag],)\n        if return_inverse:\n            iflag = np.cumsum(flag) - 1\n            inv_idx = np.empty(ar.shape, dtype=np.intp)\n            inv_idx[perm] = iflag\n            ret += (inv_idx,)\n        if return_counts:\n            idx = np.concatenate(np.nonzero(flag) + ([ar.size],))\n            ret += (np.diff(idx),)\n    return ret\n\n\ndef colorEncode(labelmap, colors, mode=\'RGB\'):\n    labelmap = labelmap.astype(\'int\')\n    labelmap_rgb = np.zeros((labelmap.shape[0], labelmap.shape[1], 3),\n                            dtype=np.uint8)\n    for label in unique(labelmap):\n        if label < 0:\n            continue\n        labelmap_rgb += (labelmap == label)[:, :, np.newaxis] * \\\n            np.tile(colors[label],\n                    (labelmap.shape[0], labelmap.shape[1], 1))\n\n    if mode == \'BGR\':\n        return labelmap_rgb[:, :, ::-1]\n    else:\n        return labelmap_rgb\n\n\ndef accuracy(preds, label):\n    valid = (label >= 0)\n    acc_sum = (valid * (preds == label)).sum()\n    valid_sum = valid.sum()\n    acc = float(acc_sum) / (valid_sum + 1e-10)\n    return acc, valid_sum\n\n\ndef intersectionAndUnion(imPred, imLab, numClass):\n    imPred = np.asarray(imPred).copy()\n    imLab = np.asarray(imLab).copy()\n\n    imPred += 1\n    imLab += 1\n    # Remove classes from unlabeled pixels in gt image.\n    # We should not penalize detections in unlabeled portions of the image.\n    imPred = imPred * (imLab > 0)\n\n    # Compute area intersection:\n    intersection = imPred * (imPred == imLab)\n    (area_intersection, _) = np.histogram(\n        intersection, bins=numClass, range=(1, numClass))\n\n    # Compute area union:\n    (area_pred, _) = np.histogram(imPred, bins=numClass, range=(1, numClass))\n    (area_lab, _) = np.histogram(imLab, bins=numClass, range=(1, numClass))\n    area_union = area_pred + area_lab - area_intersection\n\n    return (area_intersection, area_union)\n\n\nclass NotSupportedCliException(Exception):\n    pass\n\n\ndef process_range(xpu, inp):\n    start, end = map(int, inp)\n    if start > end:\n        end, start = start, end\n    return map(lambda x: \'{}{}\'.format(xpu, x), range(start, end+1))\n\n\nREGEX = [\n    (re.compile(r\'^gpu(\\d+)$\'), lambda x: [\'gpu%s\' % x[0]]),\n    (re.compile(r\'^(\\d+)$\'), lambda x: [\'gpu%s\' % x[0]]),\n    (re.compile(r\'^gpu(\\d+)-(?:gpu)?(\\d+)$\'),\n     functools.partial(process_range, \'gpu\')),\n    (re.compile(r\'^(\\d+)-(\\d+)$\'),\n     functools.partial(process_range, \'gpu\')),\n]\n\n\ndef parse_devices(input_devices):\n\n    """"""Parse user\'s devices input str to standard format.\n    e.g. [gpu0, gpu1, ...]\n\n    """"""\n    ret = []\n    for d in input_devices.split(\',\'):\n        for regex, func in REGEX:\n            m = regex.match(d.lower().strip())\n            if m:\n                tmp = func(m.groups())\n                # prevent duplicate\n                for x in tmp:\n                    if x not in ret:\n                        ret.append(x)\n                break\n        else:\n            raise NotSupportedCliException(\n                \'Can not recognize device: ""{}""\'.format(d))\n    return ret\n'"
config/__init__.py,0,b'from .defaults import _C as cfg\n'
config/defaults.py,0,"b'from yacs.config import CfgNode as CN\n\n# -----------------------------------------------------------------------------\n# Config definition\n# -----------------------------------------------------------------------------\n\n_C = CN()\n_C.DIR = ""ckpt/ade20k-resnet50dilated-ppm_deepsup""\n\n# -----------------------------------------------------------------------------\n# Dataset\n# -----------------------------------------------------------------------------\n_C.DATASET = CN()\n_C.DATASET.root_dataset = ""./data/""\n_C.DATASET.list_train = ""./data/training.odgt""\n_C.DATASET.list_val = ""./data/validation.odgt""\n_C.DATASET.num_class = 150\n# multiscale train/test, size of short edge (int or tuple)\n_C.DATASET.imgSizes = (300, 375, 450, 525, 600)\n# maximum input image size of long edge\n_C.DATASET.imgMaxSize = 1000\n# maxmimum downsampling rate of the network\n_C.DATASET.padding_constant = 8\n# downsampling rate of the segmentation label\n_C.DATASET.segm_downsampling_rate = 8\n# randomly horizontally flip images when train/test\n_C.DATASET.random_flip = True\n\n# -----------------------------------------------------------------------------\n# Model\n# -----------------------------------------------------------------------------\n_C.MODEL = CN()\n# architecture of net_encoder\n_C.MODEL.arch_encoder = ""resnet50dilated""\n# architecture of net_decoder\n_C.MODEL.arch_decoder = ""ppm_deepsup""\n# weights to finetune net_encoder\n_C.MODEL.weights_encoder = """"\n# weights to finetune net_decoder\n_C.MODEL.weights_decoder = """"\n# number of feature channels between encoder and decoder\n_C.MODEL.fc_dim = 2048\n\n# -----------------------------------------------------------------------------\n# Training\n# -----------------------------------------------------------------------------\n_C.TRAIN = CN()\n_C.TRAIN.batch_size_per_gpu = 2\n# epochs to train for\n_C.TRAIN.num_epoch = 20\n# epoch to start training. useful if continue from a checkpoint\n_C.TRAIN.start_epoch = 0\n# iterations of each epoch (irrelevant to batch size)\n_C.TRAIN.epoch_iters = 5000\n\n_C.TRAIN.optim = ""SGD""\n_C.TRAIN.lr_encoder = 0.02\n_C.TRAIN.lr_decoder = 0.02\n# power in poly to drop LR\n_C.TRAIN.lr_pow = 0.9\n# momentum for sgd, beta1 for adam\n_C.TRAIN.beta1 = 0.9\n# weights regularizer\n_C.TRAIN.weight_decay = 1e-4\n# the weighting of deep supervision loss\n_C.TRAIN.deep_sup_scale = 0.4\n# fix bn params, only under finetuning\n_C.TRAIN.fix_bn = False\n# number of data loading workers\n_C.TRAIN.workers = 16\n\n# frequency to display\n_C.TRAIN.disp_iter = 20\n# manual seed\n_C.TRAIN.seed = 304\n\n# -----------------------------------------------------------------------------\n# Validation\n# -----------------------------------------------------------------------------\n_C.VAL = CN()\n# currently only supports 1\n_C.VAL.batch_size = 1\n# output visualization during validation\n_C.VAL.visualize = False\n# the checkpoint to evaluate on\n_C.VAL.checkpoint = ""epoch_20.pth""\n\n# -----------------------------------------------------------------------------\n# Testing\n# -----------------------------------------------------------------------------\n_C.TEST = CN()\n# currently only supports 1\n_C.TEST.batch_size = 1\n# the checkpoint to test on\n_C.TEST.checkpoint = ""epoch_20.pth""\n# folder to output visualization results\n_C.TEST.result = ""./""\n'"
models/__init__.py,0,"b'from .models import ModelBuilder, SegmentationModule\n'"
models/hrnet.py,3,"b'""""""\nThis HRNet implementation is modified from the following repository:\nhttps://github.com/HRNet/HRNet-Semantic-Segmentation\n""""""\n\nimport logging\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .utils import load_url\nfrom lib.nn import SynchronizedBatchNorm2d\n\nBatchNorm2d = SynchronizedBatchNorm2d\nBN_MOMENTUM = 0.1\nlogger = logging.getLogger(__name__)\n\n\n__all__ = [\'hrnetv2\']\n\n\nmodel_urls = {\n    \'hrnetv2\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/hrnetv2_w48-imagenet.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n                               bias=False)\n        self.bn3 = BatchNorm2d(planes * self.expansion,\n                               momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass HighResolutionModule(nn.Module):\n    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n                 num_channels, fuse_method, multi_scale_output=True):\n        super(HighResolutionModule, self).__init__()\n        self._check_branches(\n            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n\n        self.num_inchannels = num_inchannels\n        self.fuse_method = fuse_method\n        self.num_branches = num_branches\n\n        self.multi_scale_output = multi_scale_output\n\n        self.branches = self._make_branches(\n            num_branches, blocks, num_blocks, num_channels)\n        self.fuse_layers = self._make_fuse_layers()\n        self.relu = nn.ReLU(inplace=True)\n\n    def _check_branches(self, num_branches, blocks, num_blocks,\n                        num_inchannels, num_channels):\n        if num_branches != len(num_blocks):\n            error_msg = \'NUM_BRANCHES({}) <> NUM_BLOCKS({})\'.format(\n                num_branches, len(num_blocks))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_channels):\n            error_msg = \'NUM_BRANCHES({}) <> NUM_CHANNELS({})\'.format(\n                num_branches, len(num_channels))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_inchannels):\n            error_msg = \'NUM_BRANCHES({}) <> NUM_INCHANNELS({})\'.format(\n                num_branches, len(num_inchannels))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n                         stride=1):\n        downsample = None\n        if stride != 1 or \\\n           self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.num_inchannels[branch_index],\n                          num_channels[branch_index] * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm2d(num_channels[branch_index] * block.expansion,\n                            momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(self.num_inchannels[branch_index],\n                            num_channels[branch_index], stride, downsample))\n        self.num_inchannels[branch_index] = \\\n            num_channels[branch_index] * block.expansion\n        for i in range(1, num_blocks[branch_index]):\n            layers.append(block(self.num_inchannels[branch_index],\n                                num_channels[branch_index]))\n\n        return nn.Sequential(*layers)\n\n    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n        branches = []\n\n        for i in range(num_branches):\n            branches.append(\n                self._make_one_branch(i, block, num_blocks, num_channels))\n\n        return nn.ModuleList(branches)\n\n    def _make_fuse_layers(self):\n        if self.num_branches == 1:\n            return None\n\n        num_branches = self.num_branches\n        num_inchannels = self.num_inchannels\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(nn.Sequential(\n                        nn.Conv2d(num_inchannels[j],\n                                  num_inchannels[i],\n                                  1,\n                                  1,\n                                  0,\n                                  bias=False),\n                        BatchNorm2d(num_inchannels[i], momentum=BN_MOMENTUM)))\n                elif j == i:\n                    fuse_layer.append(None)\n                else:\n                    conv3x3s = []\n                    for k in range(i-j):\n                        if k == i - j - 1:\n                            num_outchannels_conv3x3 = num_inchannels[i]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j],\n                                          num_outchannels_conv3x3,\n                                          3, 2, 1, bias=False),\n                                BatchNorm2d(num_outchannels_conv3x3,\n                                            momentum=BN_MOMENTUM)))\n                        else:\n                            num_outchannels_conv3x3 = num_inchannels[j]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j],\n                                          num_outchannels_conv3x3,\n                                          3, 2, 1, bias=False),\n                                BatchNorm2d(num_outchannels_conv3x3,\n                                            momentum=BN_MOMENTUM),\n                                nn.ReLU(inplace=True)))\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\n            fuse_layers.append(nn.ModuleList(fuse_layer))\n\n        return nn.ModuleList(fuse_layers)\n\n    def get_num_inchannels(self):\n        return self.num_inchannels\n\n    def forward(self, x):\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i in range(self.num_branches):\n            x[i] = self.branches[i](x[i])\n\n        x_fuse = []\n        for i in range(len(self.fuse_layers)):\n            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n            for j in range(1, self.num_branches):\n                if i == j:\n                    y = y + x[j]\n                elif j > i:\n                    width_output = x[i].shape[-1]\n                    height_output = x[i].shape[-2]\n                    y = y + F.interpolate(\n                        self.fuse_layers[i][j](x[j]),\n                        size=(height_output, width_output),\n                        mode=\'bilinear\',\n                        align_corners=False)\n                else:\n                    y = y + self.fuse_layers[i][j](x[j])\n            x_fuse.append(self.relu(y))\n\n        return x_fuse\n\n\nblocks_dict = {\n    \'BASIC\': BasicBlock,\n    \'BOTTLENECK\': Bottleneck\n}\n\n\nclass HRNetV2(nn.Module):\n    def __init__(self, n_class, **kwargs):\n        super(HRNetV2, self).__init__()\n        extra = {\n            \'STAGE2\': {\'NUM_MODULES\': 1, \'NUM_BRANCHES\': 2, \'BLOCK\': \'BASIC\', \'NUM_BLOCKS\': (4, 4), \'NUM_CHANNELS\': (48, 96), \'FUSE_METHOD\': \'SUM\'},\n            \'STAGE3\': {\'NUM_MODULES\': 4, \'NUM_BRANCHES\': 3, \'BLOCK\': \'BASIC\', \'NUM_BLOCKS\': (4, 4, 4), \'NUM_CHANNELS\': (48, 96, 192), \'FUSE_METHOD\': \'SUM\'},\n            \'STAGE4\': {\'NUM_MODULES\': 3, \'NUM_BRANCHES\': 4, \'BLOCK\': \'BASIC\', \'NUM_BLOCKS\': (4, 4, 4, 4), \'NUM_CHANNELS\': (48, 96, 192, 384), \'FUSE_METHOD\': \'SUM\'},\n            \'FINAL_CONV_KERNEL\': 1\n            }\n\n        # stem net\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn1 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn2 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.layer1 = self._make_layer(Bottleneck, 64, 64, 4)\n\n        self.stage2_cfg = extra[\'STAGE2\']\n        num_channels = self.stage2_cfg[\'NUM_CHANNELS\']\n        block = blocks_dict[self.stage2_cfg[\'BLOCK\']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition1 = self._make_transition_layer([256], num_channels)\n        self.stage2, pre_stage_channels = self._make_stage(\n            self.stage2_cfg, num_channels)\n\n        self.stage3_cfg = extra[\'STAGE3\']\n        num_channels = self.stage3_cfg[\'NUM_CHANNELS\']\n        block = blocks_dict[self.stage3_cfg[\'BLOCK\']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition2 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage3, pre_stage_channels = self._make_stage(\n            self.stage3_cfg, num_channels)\n\n        self.stage4_cfg = extra[\'STAGE4\']\n        num_channels = self.stage4_cfg[\'NUM_CHANNELS\']\n        block = blocks_dict[self.stage4_cfg[\'BLOCK\']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition3 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage4, pre_stage_channels = self._make_stage(\n            self.stage4_cfg, num_channels, multi_scale_output=True)\n\n    def _make_transition_layer(\n            self, num_channels_pre_layer, num_channels_cur_layer):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(nn.Sequential(\n                        nn.Conv2d(num_channels_pre_layer[i],\n                                  num_channels_cur_layer[i],\n                                  3,\n                                  1,\n                                  1,\n                                  bias=False),\n                        BatchNorm2d(\n                            num_channels_cur_layer[i], momentum=BN_MOMENTUM),\n                        nn.ReLU(inplace=True)))\n                else:\n                    transition_layers.append(None)\n            else:\n                conv3x3s = []\n                for j in range(i+1-num_branches_pre):\n                    inchannels = num_channels_pre_layer[-1]\n                    outchannels = num_channels_cur_layer[i] \\\n                        if j == i-num_branches_pre else inchannels\n                    conv3x3s.append(nn.Sequential(\n                        nn.Conv2d(\n                            inchannels, outchannels, 3, 2, 1, bias=False),\n                        BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n                        nn.ReLU(inplace=True)))\n                transition_layers.append(nn.Sequential(*conv3x3s))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(inplanes, planes, stride, downsample))\n        inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_stage(self, layer_config, num_inchannels,\n                    multi_scale_output=True):\n        num_modules = layer_config[\'NUM_MODULES\']\n        num_branches = layer_config[\'NUM_BRANCHES\']\n        num_blocks = layer_config[\'NUM_BLOCKS\']\n        num_channels = layer_config[\'NUM_CHANNELS\']\n        block = blocks_dict[layer_config[\'BLOCK\']]\n        fuse_method = layer_config[\'FUSE_METHOD\']\n\n        modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            if not multi_scale_output and i == num_modules - 1:\n                reset_multi_scale_output = False\n            else:\n                reset_multi_scale_output = True\n            modules.append(\n                HighResolutionModule(\n                    num_branches,\n                    block,\n                    num_blocks,\n                    num_inchannels,\n                    num_channels,\n                    fuse_method,\n                    reset_multi_scale_output)\n            )\n            num_inchannels = modules[-1].get_num_inchannels()\n\n        return nn.Sequential(*modules), num_inchannels\n\n    def forward(self, x, return_feature_maps=False):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n\n        x_list = []\n        for i in range(self.stage2_cfg[\'NUM_BRANCHES\']):\n            if self.transition1[i] is not None:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(self.stage3_cfg[\'NUM_BRANCHES\']):\n            if self.transition2[i] is not None:\n                x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(self.stage4_cfg[\'NUM_BRANCHES\']):\n            if self.transition3[i] is not None:\n                x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        x = self.stage4(x_list)\n\n        # Upsampling\n        x0_h, x0_w = x[0].size(2), x[0].size(3)\n        x1 = F.interpolate(\n            x[1], size=(x0_h, x0_w), mode=\'bilinear\', align_corners=False)\n        x2 = F.interpolate(\n            x[2], size=(x0_h, x0_w), mode=\'bilinear\', align_corners=False)\n        x3 = F.interpolate(\n            x[3], size=(x0_h, x0_w), mode=\'bilinear\', align_corners=False)\n\n        x = torch.cat([x[0], x1, x2, x3], 1)\n\n        # x = self.last_layer(x)\n        return [x]\n\n\ndef hrnetv2(pretrained=False, **kwargs):\n    model = HRNetV2(n_class=1000, **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'hrnetv2\']), strict=False)\n\n    return model\n'"
models/mobilenet.py,1,"b'""""""\nThis MobileNetV2 implementation is modified from the following repository:\nhttps://github.com/tonylins/pytorch-mobilenet-v2\n""""""\n\nimport torch.nn as nn\nimport math\nfrom .utils import load_url\nfrom lib.nn import SynchronizedBatchNorm2d\n\nBatchNorm2d = SynchronizedBatchNorm2d\n\n\n__all__ = [\'mobilenetv2\']\n\n\nmodel_urls = {\n    \'mobilenetv2\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/mobilenet_v2.pth.tar\',\n}\n\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n        interverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        assert input_size % 32 == 0\n        input_channel = int(input_channel * width_mult)\n        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n        self.features = [conv_bn(3, input_channel, 2)]\n        # building inverted residual blocks\n        for t, c, n, s in interverted_residual_setting:\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                if i == 0:\n                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n                else:\n                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, n_class),\n        )\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.mean(3).mean(2)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n\ndef mobilenetv2(pretrained=False, **kwargs):\n    """"""Constructs a MobileNet_V2 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = MobileNetV2(n_class=1000, **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'mobilenetv2\']), strict=False)\n    return model\n'"
models/models.py,10,"b'import torch\nimport torch.nn as nn\nimport torchvision\nfrom . import resnet, resnext, mobilenet, hrnet\nfrom lib.nn import SynchronizedBatchNorm2d\nBatchNorm2d = SynchronizedBatchNorm2d\n\n\nclass SegmentationModuleBase(nn.Module):\n    def __init__(self):\n        super(SegmentationModuleBase, self).__init__()\n\n    def pixel_acc(self, pred, label):\n        _, preds = torch.max(pred, dim=1)\n        valid = (label >= 0).long()\n        acc_sum = torch.sum(valid * (preds == label).long())\n        pixel_sum = torch.sum(valid)\n        acc = acc_sum.float() / (pixel_sum.float() + 1e-10)\n        return acc\n\n\nclass SegmentationModule(SegmentationModuleBase):\n    def __init__(self, net_enc, net_dec, crit, deep_sup_scale=None):\n        super(SegmentationModule, self).__init__()\n        self.encoder = net_enc\n        self.decoder = net_dec\n        self.crit = crit\n        self.deep_sup_scale = deep_sup_scale\n\n    def forward(self, feed_dict, *, segSize=None):\n        # training\n        if segSize is None:\n            if self.deep_sup_scale is not None: # use deep supervision technique\n                (pred, pred_deepsup) = self.decoder(self.encoder(feed_dict[\'img_data\'], return_feature_maps=True))\n            else:\n                pred = self.decoder(self.encoder(feed_dict[\'img_data\'], return_feature_maps=True))\n\n            loss = self.crit(pred, feed_dict[\'seg_label\'])\n            if self.deep_sup_scale is not None:\n                loss_deepsup = self.crit(pred_deepsup, feed_dict[\'seg_label\'])\n                loss = loss + loss_deepsup * self.deep_sup_scale\n\n            acc = self.pixel_acc(pred, feed_dict[\'seg_label\'])\n            return loss, acc\n        # inference\n        else:\n            pred = self.decoder(self.encoder(feed_dict[\'img_data\'], return_feature_maps=True), segSize=segSize)\n            return pred\n\n\nclass ModelBuilder:\n    # custom weights initialization\n    @staticmethod\n    def weights_init(m):\n        classname = m.__class__.__name__\n        if classname.find(\'Conv\') != -1:\n            nn.init.kaiming_normal_(m.weight.data)\n        elif classname.find(\'BatchNorm\') != -1:\n            m.weight.data.fill_(1.)\n            m.bias.data.fill_(1e-4)\n        #elif classname.find(\'Linear\') != -1:\n        #    m.weight.data.normal_(0.0, 0.0001)\n\n    @staticmethod\n    def build_encoder(arch=\'resnet50dilated\', fc_dim=512, weights=\'\'):\n        pretrained = True if len(weights) == 0 else False\n        arch = arch.lower()\n        if arch == \'mobilenetv2dilated\':\n            orig_mobilenet = mobilenet.__dict__[\'mobilenetv2\'](pretrained=pretrained)\n            net_encoder = MobileNetV2Dilated(orig_mobilenet, dilate_scale=8)\n        elif arch == \'resnet18\':\n            orig_resnet = resnet.__dict__[\'resnet18\'](pretrained=pretrained)\n            net_encoder = Resnet(orig_resnet)\n        elif arch == \'resnet18dilated\':\n            orig_resnet = resnet.__dict__[\'resnet18\'](pretrained=pretrained)\n            net_encoder = ResnetDilated(orig_resnet, dilate_scale=8)\n        elif arch == \'resnet34\':\n            raise NotImplementedError\n            orig_resnet = resnet.__dict__[\'resnet34\'](pretrained=pretrained)\n            net_encoder = Resnet(orig_resnet)\n        elif arch == \'resnet34dilated\':\n            raise NotImplementedError\n            orig_resnet = resnet.__dict__[\'resnet34\'](pretrained=pretrained)\n            net_encoder = ResnetDilated(orig_resnet, dilate_scale=8)\n        elif arch == \'resnet50\':\n            orig_resnet = resnet.__dict__[\'resnet50\'](pretrained=pretrained)\n            net_encoder = Resnet(orig_resnet)\n        elif arch == \'resnet50dilated\':\n            orig_resnet = resnet.__dict__[\'resnet50\'](pretrained=pretrained)\n            net_encoder = ResnetDilated(orig_resnet, dilate_scale=8)\n        elif arch == \'resnet101\':\n            orig_resnet = resnet.__dict__[\'resnet101\'](pretrained=pretrained)\n            net_encoder = Resnet(orig_resnet)\n        elif arch == \'resnet101dilated\':\n            orig_resnet = resnet.__dict__[\'resnet101\'](pretrained=pretrained)\n            net_encoder = ResnetDilated(orig_resnet, dilate_scale=8)\n        elif arch == \'resnext101\':\n            orig_resnext = resnext.__dict__[\'resnext101\'](pretrained=pretrained)\n            net_encoder = Resnet(orig_resnext) # we can still use class Resnet\n        elif arch == \'hrnetv2\':\n            net_encoder = hrnet.__dict__[\'hrnetv2\'](pretrained=pretrained)\n        else:\n            raise Exception(\'Architecture undefined!\')\n\n        # encoders are usually pretrained\n        # net_encoder.apply(ModelBuilder.weights_init)\n        if len(weights) > 0:\n            print(\'Loading weights for net_encoder\')\n            net_encoder.load_state_dict(\n                torch.load(weights, map_location=lambda storage, loc: storage), strict=False)\n        return net_encoder\n\n    @staticmethod\n    def build_decoder(arch=\'ppm_deepsup\',\n                      fc_dim=512, num_class=150,\n                      weights=\'\', use_softmax=False):\n        arch = arch.lower()\n        if arch == \'c1_deepsup\':\n            net_decoder = C1DeepSup(\n                num_class=num_class,\n                fc_dim=fc_dim,\n                use_softmax=use_softmax)\n        elif arch == \'c1\':\n            net_decoder = C1(\n                num_class=num_class,\n                fc_dim=fc_dim,\n                use_softmax=use_softmax)\n        elif arch == \'ppm\':\n            net_decoder = PPM(\n                num_class=num_class,\n                fc_dim=fc_dim,\n                use_softmax=use_softmax)\n        elif arch == \'ppm_deepsup\':\n            net_decoder = PPMDeepsup(\n                num_class=num_class,\n                fc_dim=fc_dim,\n                use_softmax=use_softmax)\n        elif arch == \'upernet_lite\':\n            net_decoder = UPerNet(\n                num_class=num_class,\n                fc_dim=fc_dim,\n                use_softmax=use_softmax,\n                fpn_dim=256)\n        elif arch == \'upernet\':\n            net_decoder = UPerNet(\n                num_class=num_class,\n                fc_dim=fc_dim,\n                use_softmax=use_softmax,\n                fpn_dim=512)\n        else:\n            raise Exception(\'Architecture undefined!\')\n\n        net_decoder.apply(ModelBuilder.weights_init)\n        if len(weights) > 0:\n            print(\'Loading weights for net_decoder\')\n            net_decoder.load_state_dict(\n                torch.load(weights, map_location=lambda storage, loc: storage), strict=False)\n        return net_decoder\n\n\ndef conv3x3_bn_relu(in_planes, out_planes, stride=1):\n    ""3x3 convolution + BN + relu""\n    return nn.Sequential(\n            nn.Conv2d(in_planes, out_planes, kernel_size=3,\n                      stride=stride, padding=1, bias=False),\n            BatchNorm2d(out_planes),\n            nn.ReLU(inplace=True),\n            )\n\n\nclass Resnet(nn.Module):\n    def __init__(self, orig_resnet):\n        super(Resnet, self).__init__()\n\n        # take pretrained resnet, except AvgPool and FC\n        self.conv1 = orig_resnet.conv1\n        self.bn1 = orig_resnet.bn1\n        self.relu1 = orig_resnet.relu1\n        self.conv2 = orig_resnet.conv2\n        self.bn2 = orig_resnet.bn2\n        self.relu2 = orig_resnet.relu2\n        self.conv3 = orig_resnet.conv3\n        self.bn3 = orig_resnet.bn3\n        self.relu3 = orig_resnet.relu3\n        self.maxpool = orig_resnet.maxpool\n        self.layer1 = orig_resnet.layer1\n        self.layer2 = orig_resnet.layer2\n        self.layer3 = orig_resnet.layer3\n        self.layer4 = orig_resnet.layer4\n\n    def forward(self, x, return_feature_maps=False):\n        conv_out = []\n\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x); conv_out.append(x);\n        x = self.layer2(x); conv_out.append(x);\n        x = self.layer3(x); conv_out.append(x);\n        x = self.layer4(x); conv_out.append(x);\n\n        if return_feature_maps:\n            return conv_out\n        return [x]\n\n\nclass ResnetDilated(nn.Module):\n    def __init__(self, orig_resnet, dilate_scale=8):\n        super(ResnetDilated, self).__init__()\n        from functools import partial\n\n        if dilate_scale == 8:\n            orig_resnet.layer3.apply(\n                partial(self._nostride_dilate, dilate=2))\n            orig_resnet.layer4.apply(\n                partial(self._nostride_dilate, dilate=4))\n        elif dilate_scale == 16:\n            orig_resnet.layer4.apply(\n                partial(self._nostride_dilate, dilate=2))\n\n        # take pretrained resnet, except AvgPool and FC\n        self.conv1 = orig_resnet.conv1\n        self.bn1 = orig_resnet.bn1\n        self.relu1 = orig_resnet.relu1\n        self.conv2 = orig_resnet.conv2\n        self.bn2 = orig_resnet.bn2\n        self.relu2 = orig_resnet.relu2\n        self.conv3 = orig_resnet.conv3\n        self.bn3 = orig_resnet.bn3\n        self.relu3 = orig_resnet.relu3\n        self.maxpool = orig_resnet.maxpool\n        self.layer1 = orig_resnet.layer1\n        self.layer2 = orig_resnet.layer2\n        self.layer3 = orig_resnet.layer3\n        self.layer4 = orig_resnet.layer4\n\n    def _nostride_dilate(self, m, dilate):\n        classname = m.__class__.__name__\n        if classname.find(\'Conv\') != -1:\n            # the convolution with stride\n            if m.stride == (2, 2):\n                m.stride = (1, 1)\n                if m.kernel_size == (3, 3):\n                    m.dilation = (dilate//2, dilate//2)\n                    m.padding = (dilate//2, dilate//2)\n            # other convoluions\n            else:\n                if m.kernel_size == (3, 3):\n                    m.dilation = (dilate, dilate)\n                    m.padding = (dilate, dilate)\n\n    def forward(self, x, return_feature_maps=False):\n        conv_out = []\n\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x); conv_out.append(x);\n        x = self.layer2(x); conv_out.append(x);\n        x = self.layer3(x); conv_out.append(x);\n        x = self.layer4(x); conv_out.append(x);\n\n        if return_feature_maps:\n            return conv_out\n        return [x]\n\n\nclass MobileNetV2Dilated(nn.Module):\n    def __init__(self, orig_net, dilate_scale=8):\n        super(MobileNetV2Dilated, self).__init__()\n        from functools import partial\n\n        # take pretrained mobilenet features\n        self.features = orig_net.features[:-1]\n\n        self.total_idx = len(self.features)\n        self.down_idx = [2, 4, 7, 14]\n\n        if dilate_scale == 8:\n            for i in range(self.down_idx[-2], self.down_idx[-1]):\n                self.features[i].apply(\n                    partial(self._nostride_dilate, dilate=2)\n                )\n            for i in range(self.down_idx[-1], self.total_idx):\n                self.features[i].apply(\n                    partial(self._nostride_dilate, dilate=4)\n                )\n        elif dilate_scale == 16:\n            for i in range(self.down_idx[-1], self.total_idx):\n                self.features[i].apply(\n                    partial(self._nostride_dilate, dilate=2)\n                )\n\n    def _nostride_dilate(self, m, dilate):\n        classname = m.__class__.__name__\n        if classname.find(\'Conv\') != -1:\n            # the convolution with stride\n            if m.stride == (2, 2):\n                m.stride = (1, 1)\n                if m.kernel_size == (3, 3):\n                    m.dilation = (dilate//2, dilate//2)\n                    m.padding = (dilate//2, dilate//2)\n            # other convoluions\n            else:\n                if m.kernel_size == (3, 3):\n                    m.dilation = (dilate, dilate)\n                    m.padding = (dilate, dilate)\n\n    def forward(self, x, return_feature_maps=False):\n        if return_feature_maps:\n            conv_out = []\n            for i in range(self.total_idx):\n                x = self.features[i](x)\n                if i in self.down_idx:\n                    conv_out.append(x)\n            conv_out.append(x)\n            return conv_out\n\n        else:\n            return [self.features(x)]\n\n\n# last conv, deep supervision\nclass C1DeepSup(nn.Module):\n    def __init__(self, num_class=150, fc_dim=2048, use_softmax=False):\n        super(C1DeepSup, self).__init__()\n        self.use_softmax = use_softmax\n\n        self.cbr = conv3x3_bn_relu(fc_dim, fc_dim // 4, 1)\n        self.cbr_deepsup = conv3x3_bn_relu(fc_dim // 2, fc_dim // 4, 1)\n\n        # last conv\n        self.conv_last = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)\n        self.conv_last_deepsup = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)\n\n    def forward(self, conv_out, segSize=None):\n        conv5 = conv_out[-1]\n\n        x = self.cbr(conv5)\n        x = self.conv_last(x)\n\n        if self.use_softmax:  # is True during inference\n            x = nn.functional.interpolate(\n                x, size=segSize, mode=\'bilinear\', align_corners=False)\n            x = nn.functional.softmax(x, dim=1)\n            return x\n\n        # deep sup\n        conv4 = conv_out[-2]\n        _ = self.cbr_deepsup(conv4)\n        _ = self.conv_last_deepsup(_)\n\n        x = nn.functional.log_softmax(x, dim=1)\n        _ = nn.functional.log_softmax(_, dim=1)\n\n        return (x, _)\n\n\n# last conv\nclass C1(nn.Module):\n    def __init__(self, num_class=150, fc_dim=2048, use_softmax=False):\n        super(C1, self).__init__()\n        self.use_softmax = use_softmax\n\n        self.cbr = conv3x3_bn_relu(fc_dim, fc_dim // 4, 1)\n\n        # last conv\n        self.conv_last = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)\n\n    def forward(self, conv_out, segSize=None):\n        conv5 = conv_out[-1]\n        x = self.cbr(conv5)\n        x = self.conv_last(x)\n\n        if self.use_softmax: # is True during inference\n            x = nn.functional.interpolate(\n                x, size=segSize, mode=\'bilinear\', align_corners=False)\n            x = nn.functional.softmax(x, dim=1)\n        else:\n            x = nn.functional.log_softmax(x, dim=1)\n\n        return x\n\n\n# pyramid pooling\nclass PPM(nn.Module):\n    def __init__(self, num_class=150, fc_dim=4096,\n                 use_softmax=False, pool_scales=(1, 2, 3, 6)):\n        super(PPM, self).__init__()\n        self.use_softmax = use_softmax\n\n        self.ppm = []\n        for scale in pool_scales:\n            self.ppm.append(nn.Sequential(\n                nn.AdaptiveAvgPool2d(scale),\n                nn.Conv2d(fc_dim, 512, kernel_size=1, bias=False),\n                BatchNorm2d(512),\n                nn.ReLU(inplace=True)\n            ))\n        self.ppm = nn.ModuleList(self.ppm)\n\n        self.conv_last = nn.Sequential(\n            nn.Conv2d(fc_dim+len(pool_scales)*512, 512,\n                      kernel_size=3, padding=1, bias=False),\n            BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.1),\n            nn.Conv2d(512, num_class, kernel_size=1)\n        )\n\n    def forward(self, conv_out, segSize=None):\n        conv5 = conv_out[-1]\n\n        input_size = conv5.size()\n        ppm_out = [conv5]\n        for pool_scale in self.ppm:\n            ppm_out.append(nn.functional.interpolate(\n                pool_scale(conv5),\n                (input_size[2], input_size[3]),\n                mode=\'bilinear\', align_corners=False))\n        ppm_out = torch.cat(ppm_out, 1)\n\n        x = self.conv_last(ppm_out)\n\n        if self.use_softmax:  # is True during inference\n            x = nn.functional.interpolate(\n                x, size=segSize, mode=\'bilinear\', align_corners=False)\n            x = nn.functional.softmax(x, dim=1)\n        else:\n            x = nn.functional.log_softmax(x, dim=1)\n        return x\n\n\n# pyramid pooling, deep supervision\nclass PPMDeepsup(nn.Module):\n    def __init__(self, num_class=150, fc_dim=4096,\n                 use_softmax=False, pool_scales=(1, 2, 3, 6)):\n        super(PPMDeepsup, self).__init__()\n        self.use_softmax = use_softmax\n\n        self.ppm = []\n        for scale in pool_scales:\n            self.ppm.append(nn.Sequential(\n                nn.AdaptiveAvgPool2d(scale),\n                nn.Conv2d(fc_dim, 512, kernel_size=1, bias=False),\n                BatchNorm2d(512),\n                nn.ReLU(inplace=True)\n            ))\n        self.ppm = nn.ModuleList(self.ppm)\n        self.cbr_deepsup = conv3x3_bn_relu(fc_dim // 2, fc_dim // 4, 1)\n\n        self.conv_last = nn.Sequential(\n            nn.Conv2d(fc_dim+len(pool_scales)*512, 512,\n                      kernel_size=3, padding=1, bias=False),\n            BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.1),\n            nn.Conv2d(512, num_class, kernel_size=1)\n        )\n        self.conv_last_deepsup = nn.Conv2d(fc_dim // 4, num_class, 1, 1, 0)\n        self.dropout_deepsup = nn.Dropout2d(0.1)\n\n    def forward(self, conv_out, segSize=None):\n        conv5 = conv_out[-1]\n\n        input_size = conv5.size()\n        ppm_out = [conv5]\n        for pool_scale in self.ppm:\n            ppm_out.append(nn.functional.interpolate(\n                pool_scale(conv5),\n                (input_size[2], input_size[3]),\n                mode=\'bilinear\', align_corners=False))\n        ppm_out = torch.cat(ppm_out, 1)\n\n        x = self.conv_last(ppm_out)\n\n        if self.use_softmax:  # is True during inference\n            x = nn.functional.interpolate(\n                x, size=segSize, mode=\'bilinear\', align_corners=False)\n            x = nn.functional.softmax(x, dim=1)\n            return x\n\n        # deep sup\n        conv4 = conv_out[-2]\n        _ = self.cbr_deepsup(conv4)\n        _ = self.dropout_deepsup(_)\n        _ = self.conv_last_deepsup(_)\n\n        x = nn.functional.log_softmax(x, dim=1)\n        _ = nn.functional.log_softmax(_, dim=1)\n\n        return (x, _)\n\n\n# upernet\nclass UPerNet(nn.Module):\n    def __init__(self, num_class=150, fc_dim=4096,\n                 use_softmax=False, pool_scales=(1, 2, 3, 6),\n                 fpn_inplanes=(256, 512, 1024, 2048), fpn_dim=256):\n        super(UPerNet, self).__init__()\n        self.use_softmax = use_softmax\n\n        # PPM Module\n        self.ppm_pooling = []\n        self.ppm_conv = []\n\n        for scale in pool_scales:\n            self.ppm_pooling.append(nn.AdaptiveAvgPool2d(scale))\n            self.ppm_conv.append(nn.Sequential(\n                nn.Conv2d(fc_dim, 512, kernel_size=1, bias=False),\n                BatchNorm2d(512),\n                nn.ReLU(inplace=True)\n            ))\n        self.ppm_pooling = nn.ModuleList(self.ppm_pooling)\n        self.ppm_conv = nn.ModuleList(self.ppm_conv)\n        self.ppm_last_conv = conv3x3_bn_relu(fc_dim + len(pool_scales)*512, fpn_dim, 1)\n\n        # FPN Module\n        self.fpn_in = []\n        for fpn_inplane in fpn_inplanes[:-1]:   # skip the top layer\n            self.fpn_in.append(nn.Sequential(\n                nn.Conv2d(fpn_inplane, fpn_dim, kernel_size=1, bias=False),\n                BatchNorm2d(fpn_dim),\n                nn.ReLU(inplace=True)\n            ))\n        self.fpn_in = nn.ModuleList(self.fpn_in)\n\n        self.fpn_out = []\n        for i in range(len(fpn_inplanes) - 1):  # skip the top layer\n            self.fpn_out.append(nn.Sequential(\n                conv3x3_bn_relu(fpn_dim, fpn_dim, 1),\n            ))\n        self.fpn_out = nn.ModuleList(self.fpn_out)\n\n        self.conv_last = nn.Sequential(\n            conv3x3_bn_relu(len(fpn_inplanes) * fpn_dim, fpn_dim, 1),\n            nn.Conv2d(fpn_dim, num_class, kernel_size=1)\n        )\n\n    def forward(self, conv_out, segSize=None):\n        conv5 = conv_out[-1]\n\n        input_size = conv5.size()\n        ppm_out = [conv5]\n        for pool_scale, pool_conv in zip(self.ppm_pooling, self.ppm_conv):\n            ppm_out.append(pool_conv(nn.functional.interpolate(\n                pool_scale(conv5),\n                (input_size[2], input_size[3]),\n                mode=\'bilinear\', align_corners=False)))\n        ppm_out = torch.cat(ppm_out, 1)\n        f = self.ppm_last_conv(ppm_out)\n\n        fpn_feature_list = [f]\n        for i in reversed(range(len(conv_out) - 1)):\n            conv_x = conv_out[i]\n            conv_x = self.fpn_in[i](conv_x) # lateral branch\n\n            f = nn.functional.interpolate(\n                f, size=conv_x.size()[2:], mode=\'bilinear\', align_corners=False) # top-down branch\n            f = conv_x + f\n\n            fpn_feature_list.append(self.fpn_out[i](f))\n\n        fpn_feature_list.reverse() # [P2 - P5]\n        output_size = fpn_feature_list[0].size()[2:]\n        fusion_list = [fpn_feature_list[0]]\n        for i in range(1, len(fpn_feature_list)):\n            fusion_list.append(nn.functional.interpolate(\n                fpn_feature_list[i],\n                output_size,\n                mode=\'bilinear\', align_corners=False))\n        fusion_out = torch.cat(fusion_list, 1)\n        x = self.conv_last(fusion_out)\n\n        if self.use_softmax:  # is True during inference\n            x = nn.functional.interpolate(\n                x, size=segSize, mode=\'bilinear\', align_corners=False)\n            x = nn.functional.softmax(x, dim=1)\n            return x\n\n        x = nn.functional.log_softmax(x, dim=1)\n\n        return x\n'"
models/resnet.py,1,"b'import torch.nn as nn\nimport math\nfrom .utils import load_url\nfrom lib.nn import SynchronizedBatchNorm2d\nBatchNorm2d = SynchronizedBatchNorm2d\n\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet50\', \'resnet101\'] # resnet101 is coming soon!\n\n\nmodel_urls = {\n    \'resnet18\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet18-imagenet.pth\',\n    \'resnet50\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet50-imagenet.pth\',\n    \'resnet101\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet101-imagenet.pth\'\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 128\n        super(ResNet, self).__init__()\n        self.conv1 = conv3x3(3, 64, stride=2)\n        self.bn1 = BatchNorm2d(64)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(64, 64)\n        self.bn2 = BatchNorm2d(64)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = conv3x3(64, 128)\n        self.bn3 = BatchNorm2d(128)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnet18\']))\n    return model\n\n\'\'\'\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnet34\']))\n    return model\n\'\'\'\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnet50\']), strict=False)\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnet101\']), strict=False)\n    return model\n\n# def resnet152(pretrained=False, **kwargs):\n#     """"""Constructs a ResNet-152 model.\n#\n#     Args:\n#         pretrained (bool): If True, returns a model pre-trained on ImageNet\n#     """"""\n#     model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n#     if pretrained:\n#         model.load_state_dict(load_url(model_urls[\'resnet152\']))\n#     return model\n'"
models/resnext.py,1,"b'import torch.nn as nn\nimport math\nfrom .utils import load_url\nfrom lib.nn import SynchronizedBatchNorm2d\nBatchNorm2d = SynchronizedBatchNorm2d\n\n\n__all__ = [\'ResNeXt\', \'resnext101\'] # support resnext 101\n\n\nmodel_urls = {\n    #\'resnext50\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnext50-imagenet.pth\',\n    \'resnext101\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnext101-imagenet.pth\'\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass GroupBottleneck(nn.Module):\n    expansion = 2\n\n    def __init__(self, inplanes, planes, stride=1, groups=1, downsample=None):\n        super(GroupBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 2, kernel_size=1, bias=False)\n        self.bn3 = BatchNorm2d(planes * 2)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNeXt(nn.Module):\n\n    def __init__(self, block, layers, groups=32, num_classes=1000):\n        self.inplanes = 128\n        super(ResNeXt, self).__init__()\n        self.conv1 = conv3x3(3, 64, stride=2)\n        self.bn1 = BatchNorm2d(64)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(64, 64)\n        self.bn2 = BatchNorm2d(64)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = conv3x3(64, 128)\n        self.bn3 = BatchNorm2d(128)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 128, layers[0], groups=groups)\n        self.layer2 = self._make_layer(block, 256, layers[1], stride=2, groups=groups)\n        self.layer3 = self._make_layer(block, 512, layers[2], stride=2, groups=groups)\n        self.layer4 = self._make_layer(block, 1024, layers[3], stride=2, groups=groups)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(1024 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels // m.groups\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, groups=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, groups, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=groups))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\n\'\'\'\ndef resnext50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNeXt(GroupBottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnext50\']), strict=False)\n    return model\n\'\'\'\n\n\ndef resnext101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNeXt(GroupBottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnext101\']), strict=False)\n    return model\n\n\n# def resnext152(pretrained=False, **kwargs):\n#     """"""Constructs a ResNeXt-152 model.\n#\n#     Args:\n#         pretrained (bool): If True, returns a model pre-trained on Places\n#     """"""\n#     model = ResNeXt(GroupBottleneck, [3, 8, 36, 3], **kwargs)\n#     if pretrained:\n#         model.load_state_dict(load_url(model_urls[\'resnext152\']))\n#     return model\n'"
models/utils.py,1,"b'import sys\nimport os\ntry:\n    from urllib import urlretrieve\nexcept ImportError:\n    from urllib.request import urlretrieve\nimport torch\n\n\ndef load_url(url, model_dir=\'./pretrained\', map_location=None):\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    filename = url.split(\'/\')[-1]\n    cached_file = os.path.join(model_dir, filename)\n    if not os.path.exists(cached_file):\n        sys.stderr.write(\'Downloading: ""{}"" to {}\\n\'.format(url, cached_file))\n        urlretrieve(url, cached_file)\n    return torch.load(cached_file, map_location=map_location)\n'"
lib/nn/__init__.py,0,"b'from .modules import *\nfrom .parallel import UserScatteredDataParallel, user_scattered_collate, async_copy_to\n'"
lib/utils/__init__.py,0,b'from .th import *\n'
lib/utils/th.py,3,"b""import torch\nfrom torch.autograd import Variable\nimport numpy as np\nimport collections\n\n__all__ = ['as_variable', 'as_numpy', 'mark_volatile']\n\ndef as_variable(obj):\n    if isinstance(obj, Variable):\n        return obj\n    if isinstance(obj, collections.Sequence):\n        return [as_variable(v) for v in obj]\n    elif isinstance(obj, collections.Mapping):\n        return {k: as_variable(v) for k, v in obj.items()}\n    else:\n        return Variable(obj)\n\ndef as_numpy(obj):\n    if isinstance(obj, collections.Sequence):\n        return [as_numpy(v) for v in obj]\n    elif isinstance(obj, collections.Mapping):\n        return {k: as_numpy(v) for k, v in obj.items()}\n    elif isinstance(obj, Variable):\n        return obj.data.cpu().numpy()\n    elif torch.is_tensor(obj):\n        return obj.cpu().numpy()\n    else:\n        return np.array(obj)\n\ndef mark_volatile(obj):\n    if torch.is_tensor(obj):\n        obj = Variable(obj)\n    if isinstance(obj, Variable):\n        obj.no_grad = True\n        return obj\n    elif isinstance(obj, collections.Mapping):\n        return {k: mark_volatile(o) for k, o in obj.items()}\n    elif isinstance(obj, collections.Sequence):\n        return [mark_volatile(o) for o in obj]\n    else:\n        return obj\n"""
lib/nn/modules/__init__.py,0,"b'# -*- coding: utf-8 -*-\n# File   : __init__.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nfrom .batchnorm import SynchronizedBatchNorm1d, SynchronizedBatchNorm2d, SynchronizedBatchNorm3d\nfrom .replicate import DataParallelWithCallback, patch_replication_callback\n'"
lib/nn/modules/batchnorm.py,9,"b'# -*- coding: utf-8 -*-\n# File   : batchnorm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport collections\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch.nn.modules.batchnorm import _BatchNorm\nfrom torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\n\nfrom .comm import SyncMaster\n\n__all__ = [\'SynchronizedBatchNorm1d\', \'SynchronizedBatchNorm2d\', \'SynchronizedBatchNorm3d\']\n\n\ndef _sum_ft(tensor):\n    """"""sum over the first and last dimention""""""\n    return tensor.sum(dim=0).sum(dim=-1)\n\n\ndef _unsqueeze_ft(tensor):\n    """"""add new dementions at the front and the tail""""""\n    return tensor.unsqueeze(0).unsqueeze(-1)\n\n\n_ChildMessage = collections.namedtuple(\'_ChildMessage\', [\'sum\', \'ssum\', \'sum_size\'])\n_MasterMessage = collections.namedtuple(\'_MasterMessage\', [\'sum\', \'inv_std\'])\n\n\nclass _SynchronizedBatchNorm(_BatchNorm):\n    def __init__(self, num_features, eps=1e-5, momentum=0.001, affine=True):\n        super(_SynchronizedBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine)\n\n        self._sync_master = SyncMaster(self._data_parallel_master)\n\n        self._is_parallel = False\n        self._parallel_id = None\n        self._slave_pipe = None\n\n        # customed batch norm statistics\n        self._moving_average_fraction = 1. - momentum\n        self.register_buffer(\'_tmp_running_mean\', torch.zeros(self.num_features))\n        self.register_buffer(\'_tmp_running_var\', torch.ones(self.num_features))\n        self.register_buffer(\'_running_iter\', torch.ones(1))\n        self._tmp_running_mean = self.running_mean.clone() * self._running_iter\n        self._tmp_running_var = self.running_var.clone() * self._running_iter\n\n    def forward(self, input):\n        # If it is not parallel computation or is in evaluation mode, use PyTorch\'s implementation.\n        if not (self._is_parallel and self.training):\n            return F.batch_norm(\n                input, self.running_mean, self.running_var, self.weight, self.bias,\n                self.training, self.momentum, self.eps)\n\n        # Resize the input to (B, C, -1).\n        input_shape = input.size()\n        input = input.view(input.size(0), self.num_features, -1)\n\n        # Compute the sum and square-sum.\n        sum_size = input.size(0) * input.size(2)\n        input_sum = _sum_ft(input)\n        input_ssum = _sum_ft(input ** 2)\n\n        # Reduce-and-broadcast the statistics.\n        if self._parallel_id == 0:\n            mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\n        else:\n            mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n\n        # Compute the output.\n        if self.affine:\n            # MJY:: Fuse the multiplication for speed.\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n        else:\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n\n        # Reshape it.\n        return output.view(input_shape)\n\n    def __data_parallel_replicate__(self, ctx, copy_id):\n        self._is_parallel = True\n        self._parallel_id = copy_id\n\n        # parallel_id == 0 means master device.\n        if self._parallel_id == 0:\n            ctx.sync_master = self._sync_master\n        else:\n            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n\n    def _data_parallel_master(self, intermediates):\n        """"""Reduce the sum and square-sum, compute the statistics, and broadcast it.""""""\n        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n\n        to_reduce = [i[1][:2] for i in intermediates]\n        to_reduce = [j for i in to_reduce for j in i]  # flatten\n        target_gpus = [i[1].sum.get_device() for i in intermediates]\n\n        sum_size = sum([i[1].sum_size for i in intermediates])\n        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n\n        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n\n        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n\n        outputs = []\n        for i, rec in enumerate(intermediates):\n            outputs.append((rec[0], _MasterMessage(*broadcasted[i*2:i*2+2])))\n\n        return outputs\n\n    def _add_weighted(self, dest, delta, alpha=1, beta=1, bias=0):\n        """"""return *dest* by `dest := dest*alpha + delta*beta + bias`""""""\n        return dest * alpha + delta * beta + bias\n\n    def _compute_mean_std(self, sum_, ssum, size):\n        """"""Compute the mean and standard-deviation with sum and square-sum. This method\n        also maintains the moving average on the master device.""""""\n        assert size > 1, \'BatchNorm computes unbiased standard-deviation, which requires size > 1.\'\n        mean = sum_ / size\n        sumvar = ssum - sum_ * mean\n        unbias_var = sumvar / (size - 1)\n        bias_var = sumvar / size\n\n        self._tmp_running_mean = self._add_weighted(self._tmp_running_mean, mean.data, alpha=self._moving_average_fraction)\n        self._tmp_running_var = self._add_weighted(self._tmp_running_var, unbias_var.data, alpha=self._moving_average_fraction)\n        self._running_iter = self._add_weighted(self._running_iter, 1, alpha=self._moving_average_fraction)\n\n        self.running_mean = self._tmp_running_mean / self._running_iter\n        self.running_var = self._tmp_running_var / self._running_iter\n\n        return mean, bias_var.clamp(self.eps) ** -0.5\n\n\nclass SynchronizedBatchNorm1d(_SynchronizedBatchNorm):\n    r""""""Applies Synchronized Batch Normalization over a 2d or 3d input that is seen as a\n    mini-batch.\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm1d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n    \n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, L)` slices, it\'s common terminology to call this Temporal BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of size\n            `batch_size x num_features [x width]`\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape:\n        - Input: :math:`(N, C)` or :math:`(N, C, L)`\n        - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm1d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm1d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 2 and input.dim() != 3:\n            raise ValueError(\'expected 2D or 3D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm1d, self)._check_input_dim(input)\n\n\nclass SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n    r""""""Applies Batch Normalization over a 4d input that is seen as a mini-batch\n    of 3d inputs\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n    \n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, H, W)` slices, it\'s common terminology to call this Spatial BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape:\n        - Input: :math:`(N, C, H, W)`\n        - Output: :math:`(N, C, H, W)` (same shape as input)\n\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm2d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 4:\n            raise ValueError(\'expected 4D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm2d, self)._check_input_dim(input)\n\n\nclass SynchronizedBatchNorm3d(_SynchronizedBatchNorm):\n    r""""""Applies Batch Normalization over a 5d input that is seen as a mini-batch\n    of 4d inputs\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm3d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n    \n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, D, H, W)` slices, it\'s common terminology to call this Volumetric BatchNorm\n    or Spatio-temporal BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x depth x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape:\n        - Input: :math:`(N, C, D, H, W)`\n        - Output: :math:`(N, C, D, H, W)` (same shape as input)\n\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm3d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm3d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45, 10))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 5:\n            raise ValueError(\'expected 5D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm3d, self)._check_input_dim(input)\n'"
lib/nn/modules/comm.py,0,"b'# -*- coding: utf-8 -*-\n# File   : comm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport queue\nimport collections\nimport threading\n\n__all__ = [\'FutureResult\', \'SlavePipe\', \'SyncMaster\']\n\n\nclass FutureResult(object):\n    """"""A thread-safe future implementation. Used only as one-to-one pipe.""""""\n\n    def __init__(self):\n        self._result = None\n        self._lock = threading.Lock()\n        self._cond = threading.Condition(self._lock)\n\n    def put(self, result):\n        with self._lock:\n            assert self._result is None, \'Previous result has\\\'t been fetched.\'\n            self._result = result\n            self._cond.notify()\n\n    def get(self):\n        with self._lock:\n            if self._result is None:\n                self._cond.wait()\n\n            res = self._result\n            self._result = None\n            return res\n\n\n_MasterRegistry = collections.namedtuple(\'MasterRegistry\', [\'result\'])\n_SlavePipeBase = collections.namedtuple(\'_SlavePipeBase\', [\'identifier\', \'queue\', \'result\'])\n\n\nclass SlavePipe(_SlavePipeBase):\n    """"""Pipe for master-slave communication.""""""\n\n    def run_slave(self, msg):\n        self.queue.put((self.identifier, msg))\n        ret = self.result.get()\n        self.queue.put(True)\n        return ret\n\n\nclass SyncMaster(object):\n    """"""An abstract `SyncMaster` object.\n\n    - During the replication, as the data parallel will trigger an callback of each module, all slave devices should\n    call `register(id)` and obtain an `SlavePipe` to communicate with the master.\n    - During the forward pass, master device invokes `run_master`, all messages from slave devices will be collected,\n    and passed to a registered callback.\n    - After receiving the messages, the master device should gather the information and determine to message passed\n    back to each slave devices.\n    """"""\n\n    def __init__(self, master_callback):\n        """"""\n\n        Args:\n            master_callback: a callback to be invoked after having collected messages from slave devices.\n        """"""\n        self._master_callback = master_callback\n        self._queue = queue.Queue()\n        self._registry = collections.OrderedDict()\n        self._activated = False\n\n    def register_slave(self, identifier):\n        """"""\n        Register an slave device.\n\n        Args:\n            identifier: an identifier, usually is the device id.\n\n        Returns: a `SlavePipe` object which can be used to communicate with the master device.\n\n        """"""\n        if self._activated:\n            assert self._queue.empty(), \'Queue is not clean before next initialization.\'\n            self._activated = False\n            self._registry.clear()\n        future = FutureResult()\n        self._registry[identifier] = _MasterRegistry(future)\n        return SlavePipe(identifier, self._queue, future)\n\n    def run_master(self, master_msg):\n        """"""\n        Main entry for the master device in each forward pass.\n        The messages were first collected from each devices (including the master device), and then\n        an callback will be invoked to compute the message to be sent back to each devices\n        (including the master device).\n\n        Args:\n            master_msg: the message that the master want to send to itself. This will be placed as the first\n            message when calling `master_callback`. For detailed usage, see `_SynchronizedBatchNorm` for an example.\n\n        Returns: the message to be sent back to the master device.\n\n        """"""\n        self._activated = True\n\n        intermediates = [(0, master_msg)]\n        for i in range(self.nr_slaves):\n            intermediates.append(self._queue.get())\n\n        results = self._master_callback(intermediates)\n        assert results[0][0] == 0, \'The first result should belongs to the master.\'\n\n        for i, res in results:\n            if i == 0:\n                continue\n            self._registry[i].result.put(res)\n\n        for i in range(self.nr_slaves):\n            assert self._queue.get() is True\n\n        return results[0][1]\n\n    @property\n    def nr_slaves(self):\n        return len(self._registry)\n'"
lib/nn/modules/replicate.py,1,"b'# -*- coding: utf-8 -*-\n# File   : replicate.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport functools\n\nfrom torch.nn.parallel.data_parallel import DataParallel\n\n__all__ = [\n    \'CallbackContext\',\n    \'execute_replication_callbacks\',\n    \'DataParallelWithCallback\',\n    \'patch_replication_callback\'\n]\n\n\nclass CallbackContext(object):\n    pass\n\n\ndef execute_replication_callbacks(modules):\n    """"""\n    Execute an replication callback `__data_parallel_replicate__` on each module created by original replication.\n\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n\n    Note that, as all modules are isomorphism, we assign each sub-module with a context\n    (shared among multiple copies of this module on different devices).\n    Through this context, different copies can share some information.\n\n    We guarantee that the callback on the master copy (the first copy) will be called ahead of calling the callback\n    of any slave copies.\n    """"""\n    master_copy = modules[0]\n    nr_modules = len(list(master_copy.modules()))\n    ctxs = [CallbackContext() for _ in range(nr_modules)]\n\n    for i, module in enumerate(modules):\n        for j, m in enumerate(module.modules()):\n            if hasattr(m, \'__data_parallel_replicate__\'):\n                m.__data_parallel_replicate__(ctxs[j], i)\n\n\nclass DataParallelWithCallback(DataParallel):\n    """"""\n    Data Parallel with a replication callback.\n\n    An replication callback `__data_parallel_replicate__` of each module will be invoked after being created by\n    original `replicate` function.\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n\n    Examples:\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n        # sync_bn.__data_parallel_replicate__ will be invoked.\n    """"""\n\n    def replicate(self, module, device_ids):\n        modules = super(DataParallelWithCallback, self).replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules\n\n\ndef patch_replication_callback(data_parallel):\n    """"""\n    Monkey-patch an existing `DataParallel` object. Add the replication callback.\n    Useful when you have customized `DataParallel` implementation.\n\n    Examples:\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallel(sync_bn, device_ids=[0, 1])\n        > patch_replication_callback(sync_bn)\n        # this is equivalent to\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n    """"""\n\n    assert isinstance(data_parallel, DataParallel)\n\n    old_replicate = data_parallel.replicate\n\n    @functools.wraps(old_replicate)\n    def new_replicate(module, device_ids):\n        modules = old_replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules\n\n    data_parallel.replicate = new_replicate\n'"
lib/nn/modules/unittest.py,1,"b""# -*- coding: utf-8 -*-\n# File   : unittest.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport unittest\n\nimport numpy as np\nfrom torch.autograd import Variable\n\n\ndef as_numpy(v):\n    if isinstance(v, Variable):\n        v = v.data\n    return v.cpu().numpy()\n\n\nclass TorchTestCase(unittest.TestCase):\n    def assertTensorClose(self, a, b, atol=1e-3, rtol=1e-3):\n        npa, npb = as_numpy(a), as_numpy(b)\n        self.assertTrue(\n                np.allclose(npa, npb, atol=atol),\n                'Tensor close check failed\\n{}\\n{}\\nadiff={}, rdiff={}'.format(a, b, np.abs(npa - npb).max(), np.abs((npa - npb) / np.fmax(npa, 1e-5)).max())\n        )\n"""
lib/nn/parallel/__init__.py,0,"b'from .data_parallel import UserScatteredDataParallel, user_scattered_collate, async_copy_to\n'"
lib/nn/parallel/data_parallel.py,5,"b'# -*- coding: utf8 -*-\n\nimport torch.cuda as cuda\nimport torch.nn as nn\nimport torch\nimport collections\nfrom torch.nn.parallel._functions import Gather\n\n\n__all__ = [\'UserScatteredDataParallel\', \'user_scattered_collate\', \'async_copy_to\']\n\n\ndef async_copy_to(obj, dev, main_stream=None):\n    if torch.is_tensor(obj):\n        v = obj.cuda(dev, non_blocking=True)\n        if main_stream is not None:\n            v.data.record_stream(main_stream)\n        return v\n    elif isinstance(obj, collections.Mapping):\n        return {k: async_copy_to(o, dev, main_stream) for k, o in obj.items()}\n    elif isinstance(obj, collections.Sequence):\n        return [async_copy_to(o, dev, main_stream) for o in obj]\n    else:\n        return obj\n\n\ndef dict_gather(outputs, target_device, dim=0):\n    """"""\n    Gathers variables from different GPUs on a specified device\n      (-1 means the CPU), with dictionary support.\n    """"""\n    def gather_map(outputs):\n        out = outputs[0]\n        if torch.is_tensor(out):\n            # MJY(20180330) HACK:: force nr_dims > 0\n            if out.dim() == 0:\n                outputs = [o.unsqueeze(0) for o in outputs]\n            return Gather.apply(target_device, dim, *outputs)\n        elif out is None:\n            return None\n        elif isinstance(out, collections.Mapping):\n            return {k: gather_map([o[k] for o in outputs]) for k in out}\n        elif isinstance(out, collections.Sequence):\n            return type(out)(map(gather_map, zip(*outputs)))\n    return gather_map(outputs)\n\n\nclass DictGatherDataParallel(nn.DataParallel):\n    def gather(self, outputs, output_device):\n        return dict_gather(outputs, output_device, dim=self.dim)\n\n\nclass UserScatteredDataParallel(DictGatherDataParallel):\n    def scatter(self, inputs, kwargs, device_ids):\n        assert len(inputs) == 1\n        inputs = inputs[0]\n        inputs = _async_copy_stream(inputs, device_ids)\n        inputs = [[i] for i in inputs]\n        assert len(kwargs) == 0\n        kwargs = [{} for _ in range(len(inputs))]\n\n        return inputs, kwargs\n\n\ndef user_scattered_collate(batch):\n    return batch\n\n\ndef _async_copy(inputs, device_ids):\n    nr_devs = len(device_ids)\n    assert type(inputs) in (tuple, list)\n    assert len(inputs) == nr_devs\n\n    outputs = []\n    for i, dev in zip(inputs, device_ids):\n        with cuda.device(dev):\n            outputs.append(async_copy_to(i, dev))\n\n    return tuple(outputs)\n\n\ndef _async_copy_stream(inputs, device_ids):\n    nr_devs = len(device_ids)\n    assert type(inputs) in (tuple, list)\n    assert len(inputs) == nr_devs\n\n    outputs = []\n    streams = [_get_stream(d) for d in device_ids]\n    for i, dev, stream in zip(inputs, device_ids, streams):\n        with cuda.device(dev):\n            main_stream = cuda.current_stream()\n            with cuda.stream(stream):\n                outputs.append(async_copy_to(i, dev, main_stream=main_stream))\n            main_stream.wait_stream(stream)\n\n    return outputs\n\n\n""""""Adapted from: torch/nn/parallel/_functions.py""""""\n# background streams used for copying\n_streams = None\n\n\ndef _get_stream(device):\n    """"""Gets a background stream for copying between CPU and GPU""""""\n    global _streams\n    if device == -1:\n        return None\n    if _streams is None:\n        _streams = [None] * cuda.device_count()\n    if _streams[device] is None: _streams[device] = cuda.Stream(device)\n    return _streams[device]\n'"
lib/utils/data/__init__.py,0,"b'\nfrom .dataset import Dataset, TensorDataset, ConcatDataset\nfrom .dataloader import DataLoader\n'"
lib/utils/data/dataloader.py,26,"b'import torch\nimport torch.multiprocessing as multiprocessing\nfrom torch._C import _set_worker_signal_handlers, \\\n    _remove_worker_pids, _error_if_any_worker_fails\ntry:\n    from torch._C import _set_worker_pids\nexcept:\n    from torch._C import _update_worker_pids as _set_worker_pids\nfrom .sampler import SequentialSampler, RandomSampler, BatchSampler\nimport signal\nimport collections\nimport re\nimport sys\nimport threading\nimport traceback\nfrom torch._six import string_classes, int_classes\nimport numpy as np\n\nif sys.version_info[0] == 2:\n    import Queue as queue\nelse:\n    import queue\n\n\nclass ExceptionWrapper(object):\n    r""Wraps an exception plus traceback to communicate across threads""\n\n    def __init__(self, exc_info):\n        self.exc_type = exc_info[0]\n        self.exc_msg = """".join(traceback.format_exception(*exc_info))\n\n\n_use_shared_memory = False\n""""""Whether to use shared memory in default_collate""""""\n\n\ndef _worker_loop(dataset, index_queue, data_queue, collate_fn, seed, init_fn, worker_id):\n    global _use_shared_memory\n    _use_shared_memory = True\n\n    # Intialize C side signal handlers for SIGBUS and SIGSEGV. Python signal\n    # module\'s handlers are executed after Python returns from C low-level\n    # handlers, likely when the same fatal signal happened again already.\n    # https://docs.python.org/3/library/signal.html Sec. 18.8.1.1\n    _set_worker_signal_handlers()\n\n    torch.set_num_threads(1)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\n    if init_fn is not None:\n        init_fn(worker_id)\n\n    while True:\n        r = index_queue.get()\n        if r is None:\n            break\n        idx, batch_indices = r\n        try:\n            samples = collate_fn([dataset[i] for i in batch_indices])\n        except Exception:\n            data_queue.put((idx, ExceptionWrapper(sys.exc_info())))\n        else:\n            data_queue.put((idx, samples))\n\n\ndef _worker_manager_loop(in_queue, out_queue, done_event, pin_memory, device_id):\n    if pin_memory:\n        torch.cuda.set_device(device_id)\n\n    while True:\n        try:\n            r = in_queue.get()\n        except Exception:\n            if done_event.is_set():\n                return\n            raise\n        if r is None:\n            break\n        if isinstance(r[1], ExceptionWrapper):\n            out_queue.put(r)\n            continue\n        idx, batch = r\n        try:\n            if pin_memory:\n                batch = pin_memory_batch(batch)\n        except Exception:\n            out_queue.put((idx, ExceptionWrapper(sys.exc_info())))\n        else:\n            out_queue.put((idx, batch))\n\nnumpy_type_map = {\n    \'float64\': torch.DoubleTensor,\n    \'float32\': torch.FloatTensor,\n    \'float16\': torch.HalfTensor,\n    \'int64\': torch.LongTensor,\n    \'int32\': torch.IntTensor,\n    \'int16\': torch.ShortTensor,\n    \'int8\': torch.CharTensor,\n    \'uint8\': torch.ByteTensor,\n}\n\n\ndef default_collate(batch):\n    ""Puts each data field into a tensor with outer dimension batch size""\n\n    error_msg = ""batch must contain tensors, numbers, dicts or lists; found {}""\n    elem_type = type(batch[0])\n    if torch.is_tensor(batch[0]):\n        out = None\n        if _use_shared_memory:\n            # If we\'re in a background process, concatenate directly into a\n            # shared memory tensor to avoid an extra copy\n            numel = sum([x.numel() for x in batch])\n            storage = batch[0].storage()._new_shared(numel)\n            out = batch[0].new(storage)\n        return torch.stack(batch, 0, out=out)\n    elif elem_type.__module__ == \'numpy\' and elem_type.__name__ != \'str_\' \\\n            and elem_type.__name__ != \'string_\':\n        elem = batch[0]\n        if elem_type.__name__ == \'ndarray\':\n            # array of string classes and object\n            if re.search(\'[SaUO]\', elem.dtype.str) is not None:\n                raise TypeError(error_msg.format(elem.dtype))\n\n            return torch.stack([torch.from_numpy(b) for b in batch], 0)\n        if elem.shape == ():  # scalars\n            py_type = float if elem.dtype.name.startswith(\'float\') else int\n            return numpy_type_map[elem.dtype.name](list(map(py_type, batch)))\n    elif isinstance(batch[0], int_classes):\n        return torch.LongTensor(batch)\n    elif isinstance(batch[0], float):\n        return torch.DoubleTensor(batch)\n    elif isinstance(batch[0], string_classes):\n        return batch\n    elif isinstance(batch[0], collections.Mapping):\n        return {key: default_collate([d[key] for d in batch]) for key in batch[0]}\n    elif isinstance(batch[0], collections.Sequence):\n        transposed = zip(*batch)\n        return [default_collate(samples) for samples in transposed]\n\n    raise TypeError((error_msg.format(type(batch[0]))))\n\n\ndef pin_memory_batch(batch):\n    if torch.is_tensor(batch):\n        return batch.pin_memory()\n    elif isinstance(batch, string_classes):\n        return batch\n    elif isinstance(batch, collections.Mapping):\n        return {k: pin_memory_batch(sample) for k, sample in batch.items()}\n    elif isinstance(batch, collections.Sequence):\n        return [pin_memory_batch(sample) for sample in batch]\n    else:\n        return batch\n\n\n_SIGCHLD_handler_set = False\n""""""Whether SIGCHLD handler is set for DataLoader worker failures. Only one\nhandler needs to be set for all DataLoaders in a process.""""""\n\n\ndef _set_SIGCHLD_handler():\n    # Windows doesn\'t support SIGCHLD handler\n    if sys.platform == \'win32\':\n        return\n    # can\'t set signal in child threads\n    if not isinstance(threading.current_thread(), threading._MainThread):\n        return\n    global _SIGCHLD_handler_set\n    if _SIGCHLD_handler_set:\n        return\n    previous_handler = signal.getsignal(signal.SIGCHLD)\n    if not callable(previous_handler):\n        previous_handler = None\n\n    def handler(signum, frame):\n        # This following call uses `waitid` with WNOHANG from C side. Therefore,\n        # Python can still get and update the process status successfully.\n        _error_if_any_worker_fails()\n        if previous_handler is not None:\n            previous_handler(signum, frame)\n\n    signal.signal(signal.SIGCHLD, handler)\n    _SIGCHLD_handler_set = True\n\n\nclass DataLoaderIter(object):\n    ""Iterates once over the DataLoader\'s dataset, as specified by the sampler""\n\n    def __init__(self, loader):\n        self.dataset = loader.dataset\n        self.collate_fn = loader.collate_fn\n        self.batch_sampler = loader.batch_sampler\n        self.num_workers = loader.num_workers\n        self.pin_memory = loader.pin_memory and torch.cuda.is_available()\n        self.timeout = loader.timeout\n        self.done_event = threading.Event()\n\n        self.sample_iter = iter(self.batch_sampler)\n\n        if self.num_workers > 0:\n            self.worker_init_fn = loader.worker_init_fn\n            self.index_queue = multiprocessing.SimpleQueue()\n            self.worker_result_queue = multiprocessing.SimpleQueue()\n            self.batches_outstanding = 0\n            self.worker_pids_set = False\n            self.shutdown = False\n            self.send_idx = 0\n            self.rcvd_idx = 0\n            self.reorder_dict = {}\n\n            base_seed = torch.LongTensor(1).random_(0, 2**31-1)[0]\n            self.workers = [\n                multiprocessing.Process(\n                    target=_worker_loop,\n                    args=(self.dataset, self.index_queue, self.worker_result_queue, self.collate_fn,\n                          base_seed + i, self.worker_init_fn, i))\n                for i in range(self.num_workers)]\n\n            if self.pin_memory or self.timeout > 0:\n                self.data_queue = queue.Queue()\n                if self.pin_memory:\n                    maybe_device_id = torch.cuda.current_device()\n                else:\n                    # do not initialize cuda context if not necessary\n                    maybe_device_id = None\n                self.worker_manager_thread = threading.Thread(\n                    target=_worker_manager_loop,\n                    args=(self.worker_result_queue, self.data_queue, self.done_event, self.pin_memory,\n                          maybe_device_id))\n                self.worker_manager_thread.daemon = True\n                self.worker_manager_thread.start()\n            else:\n                self.data_queue = self.worker_result_queue\n\n            for w in self.workers:\n                w.daemon = True  # ensure that the worker exits on process exit\n                w.start()\n\n            _set_worker_pids(id(self), tuple(w.pid for w in self.workers))\n            _set_SIGCHLD_handler()\n            self.worker_pids_set = True\n\n            # prime the prefetch loop\n            for _ in range(2 * self.num_workers):\n                self._put_indices()\n\n    def __len__(self):\n        return len(self.batch_sampler)\n\n    def _get_batch(self):\n        if self.timeout > 0:\n            try:\n                return self.data_queue.get(timeout=self.timeout)\n            except queue.Empty:\n                raise RuntimeError(\'DataLoader timed out after {} seconds\'.format(self.timeout))\n        else:\n            return self.data_queue.get()\n\n    def __next__(self):\n        if self.num_workers == 0:  # same-process loading\n            indices = next(self.sample_iter)  # may raise StopIteration\n            batch = self.collate_fn([self.dataset[i] for i in indices])\n            if self.pin_memory:\n                batch = pin_memory_batch(batch)\n            return batch\n\n        # check if the next sample has already been generated\n        if self.rcvd_idx in self.reorder_dict:\n            batch = self.reorder_dict.pop(self.rcvd_idx)\n            return self._process_next_batch(batch)\n\n        if self.batches_outstanding == 0:\n            self._shutdown_workers()\n            raise StopIteration\n\n        while True:\n            assert (not self.shutdown and self.batches_outstanding > 0)\n            idx, batch = self._get_batch()\n            self.batches_outstanding -= 1\n            if idx != self.rcvd_idx:\n                # store out-of-order samples\n                self.reorder_dict[idx] = batch\n                continue\n            return self._process_next_batch(batch)\n\n    next = __next__  # Python 2 compatibility\n\n    def __iter__(self):\n        return self\n\n    def _put_indices(self):\n        assert self.batches_outstanding < 2 * self.num_workers\n        indices = next(self.sample_iter, None)\n        if indices is None:\n            return\n        self.index_queue.put((self.send_idx, indices))\n        self.batches_outstanding += 1\n        self.send_idx += 1\n\n    def _process_next_batch(self, batch):\n        self.rcvd_idx += 1\n        self._put_indices()\n        if isinstance(batch, ExceptionWrapper):\n            raise batch.exc_type(batch.exc_msg)\n        return batch\n\n    def __getstate__(self):\n        # TODO: add limited pickling support for sharing an iterator\n        # across multiple threads for HOGWILD.\n        # Probably the best way to do this is by moving the sample pushing\n        # to a separate thread and then just sharing the data queue\n        # but signalling the end is tricky without a non-blocking API\n        raise NotImplementedError(""DataLoaderIterator cannot be pickled"")\n\n    def _shutdown_workers(self):\n        try:\n            if not self.shutdown:\n                self.shutdown = True\n                self.done_event.set()\n                # if worker_manager_thread is waiting to put\n                while not self.data_queue.empty():\n                    self.data_queue.get()\n                for _ in self.workers:\n                    self.index_queue.put(None)\n                # done_event should be sufficient to exit worker_manager_thread,\n                # but be safe here and put another None\n                self.worker_result_queue.put(None)\n        finally:\n            # removes pids no matter what\n            if self.worker_pids_set:\n                _remove_worker_pids(id(self))\n                self.worker_pids_set = False\n\n    def __del__(self):\n        if self.num_workers > 0:\n            self._shutdown_workers()\n\n\nclass DataLoader(object):\n    """"""\n    Data loader. Combines a dataset and a sampler, and provides\n    single- or multi-process iterators over the dataset.\n\n    Arguments:\n        dataset (Dataset): dataset from which to load the data.\n        batch_size (int, optional): how many samples per batch to load\n            (default: 1).\n        shuffle (bool, optional): set to ``True`` to have the data reshuffled\n            at every epoch (default: False).\n        sampler (Sampler, optional): defines the strategy to draw samples from\n            the dataset. If specified, ``shuffle`` must be False.\n        batch_sampler (Sampler, optional): like sampler, but returns a batch of\n            indices at a time. Mutually exclusive with batch_size, shuffle,\n            sampler, and drop_last.\n        num_workers (int, optional): how many subprocesses to use for data\n            loading. 0 means that the data will be loaded in the main process.\n            (default: 0)\n        collate_fn (callable, optional): merges a list of samples to form a mini-batch.\n        pin_memory (bool, optional): If ``True``, the data loader will copy tensors\n            into CUDA pinned memory before returning them.\n        drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n            if the dataset size is not divisible by the batch size. If ``False`` and\n            the size of dataset is not divisible by the batch size, then the last batch\n            will be smaller. (default: False)\n        timeout (numeric, optional): if positive, the timeout value for collecting a batch\n            from workers. Should always be non-negative. (default: 0)\n        worker_init_fn (callable, optional): If not None, this will be called on each\n            worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n            input, after seeding and before data loading. (default: None)\n\n    .. note:: By default, each worker will have its PyTorch seed set to\n              ``base_seed + worker_id``, where ``base_seed`` is a long generated\n              by main process using its RNG. You may use ``torch.initial_seed()`` to access\n              this value in :attr:`worker_init_fn`, which can be used to set other seeds\n              (e.g. NumPy) before data loading.\n\n    .. warning:: If ``spawn\'\' start method is used, :attr:`worker_init_fn` cannot be an\n                 unpicklable object, e.g., a lambda function.\n    """"""\n\n    def __init__(self, dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None,\n                 num_workers=0, collate_fn=default_collate, pin_memory=False, drop_last=False,\n                 timeout=0, worker_init_fn=None):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.collate_fn = collate_fn\n        self.pin_memory = pin_memory\n        self.drop_last = drop_last\n        self.timeout = timeout\n        self.worker_init_fn = worker_init_fn\n\n        if timeout < 0:\n            raise ValueError(\'timeout option should be non-negative\')\n\n        if batch_sampler is not None:\n            if batch_size > 1 or shuffle or sampler is not None or drop_last:\n                raise ValueError(\'batch_sampler is mutually exclusive with \'\n                                 \'batch_size, shuffle, sampler, and drop_last\')\n\n        if sampler is not None and shuffle:\n            raise ValueError(\'sampler is mutually exclusive with shuffle\')\n\n        if self.num_workers < 0:\n            raise ValueError(\'num_workers cannot be negative; \'\n                             \'use num_workers=0 to disable multiprocessing.\')\n\n        if batch_sampler is None:\n            if sampler is None:\n                if shuffle:\n                    sampler = RandomSampler(dataset)\n                else:\n                    sampler = SequentialSampler(dataset)\n            batch_sampler = BatchSampler(sampler, batch_size, drop_last)\n\n        self.sampler = sampler\n        self.batch_sampler = batch_sampler\n\n    def __iter__(self):\n        return DataLoaderIter(self)\n\n    def __len__(self):\n        return len(self.batch_sampler)\n'"
lib/utils/data/dataset.py,1,"b'import bisect\nimport warnings\n\nfrom torch._utils import _accumulate\nfrom torch import randperm\n\n\nclass Dataset(object):\n    """"""An abstract class representing a Dataset.\n\n    All other datasets should subclass it. All subclasses should override\n    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n    supporting integer indexing in range from 0 to len(self) exclusive.\n    """"""\n\n    def __getitem__(self, index):\n        raise NotImplementedError\n\n    def __len__(self):\n        raise NotImplementedError\n\n    def __add__(self, other):\n        return ConcatDataset([self, other])\n\n\nclass TensorDataset(Dataset):\n    """"""Dataset wrapping data and target tensors.\n\n    Each sample will be retrieved by indexing both tensors along the first\n    dimension.\n\n    Arguments:\n        data_tensor (Tensor): contains sample data.\n        target_tensor (Tensor): contains sample targets (labels).\n    """"""\n\n    def __init__(self, data_tensor, target_tensor):\n        assert data_tensor.size(0) == target_tensor.size(0)\n        self.data_tensor = data_tensor\n        self.target_tensor = target_tensor\n\n    def __getitem__(self, index):\n        return self.data_tensor[index], self.target_tensor[index]\n\n    def __len__(self):\n        return self.data_tensor.size(0)\n\n\nclass ConcatDataset(Dataset):\n    """"""\n    Dataset to concatenate multiple datasets.\n    Purpose: useful to assemble different existing datasets, possibly\n    large-scale datasets as the concatenation operation is done in an\n    on-the-fly manner.\n\n    Arguments:\n        datasets (iterable): List of datasets to be concatenated\n    """"""\n\n    @staticmethod\n    def cumsum(sequence):\n        r, s = [], 0\n        for e in sequence:\n            l = len(e)\n            r.append(l + s)\n            s += l\n        return r\n\n    def __init__(self, datasets):\n        super(ConcatDataset, self).__init__()\n        assert len(datasets) > 0, \'datasets should not be an empty iterable\'\n        self.datasets = list(datasets)\n        self.cumulative_sizes = self.cumsum(self.datasets)\n\n    def __len__(self):\n        return self.cumulative_sizes[-1]\n\n    def __getitem__(self, idx):\n        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n        if dataset_idx == 0:\n            sample_idx = idx\n        else:\n            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n        return self.datasets[dataset_idx][sample_idx]\n\n    @property\n    def cummulative_sizes(self):\n        warnings.warn(""cummulative_sizes attribute is renamed to ""\n                      ""cumulative_sizes"", DeprecationWarning, stacklevel=2)\n        return self.cumulative_sizes\n\n\nclass Subset(Dataset):\n    def __init__(self, dataset, indices):\n        self.dataset = dataset\n        self.indices = indices\n\n    def __getitem__(self, idx):\n        return self.dataset[self.indices[idx]]\n\n    def __len__(self):\n        return len(self.indices)\n\n\ndef random_split(dataset, lengths):\n    """"""\n    Randomly split a dataset into non-overlapping new datasets of given lengths\n    ds\n\n    Arguments:\n        dataset (Dataset): Dataset to be split\n        lengths (iterable): lengths of splits to be produced\n    """"""\n    if sum(lengths) != len(dataset):\n        raise ValueError(""Sum of input lengths does not equal the length of the input dataset!"")\n\n    indices = randperm(sum(lengths))\n    return [Subset(dataset, indices[offset - length:offset]) for offset, length in zip(_accumulate(lengths), lengths)]\n'"
lib/utils/data/distributed.py,4,"b'import math\nimport torch\nfrom .sampler import Sampler\nfrom torch.distributed import get_world_size, get_rank\n\n\nclass DistributedSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n\n    .. note::\n        Dataset is assumed to be of constant size.\n\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    """"""\n\n    def __init__(self, dataset, num_replicas=None, rank=None):\n        if num_replicas is None:\n            num_replicas = get_world_size()\n        if rank is None:\n            rank = get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        g = torch.Generator()\n        g.manual_seed(self.epoch)\n        indices = list(torch.randperm(len(self.dataset), generator=g))\n\n        # add extra samples to make it evenly divisible\n        indices += indices[:(self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset:offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n'"
lib/utils/data/sampler.py,4,"b'import torch\n\n\nclass Sampler(object):\n    """"""Base class for all Samplers.\n\n    Every Sampler subclass has to provide an __iter__ method, providing a way\n    to iterate over indices of dataset elements, and a __len__ method that\n    returns the length of the returned iterators.\n    """"""\n\n    def __init__(self, data_source):\n        pass\n\n    def __iter__(self):\n        raise NotImplementedError\n\n    def __len__(self):\n        raise NotImplementedError\n\n\nclass SequentialSampler(Sampler):\n    """"""Samples elements sequentially, always in the same order.\n\n    Arguments:\n        data_source (Dataset): dataset to sample from\n    """"""\n\n    def __init__(self, data_source):\n        self.data_source = data_source\n\n    def __iter__(self):\n        return iter(range(len(self.data_source)))\n\n    def __len__(self):\n        return len(self.data_source)\n\n\nclass RandomSampler(Sampler):\n    """"""Samples elements randomly, without replacement.\n\n    Arguments:\n        data_source (Dataset): dataset to sample from\n    """"""\n\n    def __init__(self, data_source):\n        self.data_source = data_source\n\n    def __iter__(self):\n        return iter(torch.randperm(len(self.data_source)).long())\n\n    def __len__(self):\n        return len(self.data_source)\n\n\nclass SubsetRandomSampler(Sampler):\n    """"""Samples elements randomly from a given list of indices, without replacement.\n\n    Arguments:\n        indices (list): a list of indices\n    """"""\n\n    def __init__(self, indices):\n        self.indices = indices\n\n    def __iter__(self):\n        return (self.indices[i] for i in torch.randperm(len(self.indices)))\n\n    def __len__(self):\n        return len(self.indices)\n\n\nclass WeightedRandomSampler(Sampler):\n    """"""Samples elements from [0,..,len(weights)-1] with given probabilities (weights).\n\n    Arguments:\n        weights (list)   : a list of weights, not necessary summing up to one\n        num_samples (int): number of samples to draw\n        replacement (bool): if ``True``, samples are drawn with replacement.\n            If not, they are drawn without replacement, which means that when a\n            sample index is drawn for a row, it cannot be drawn again for that row.\n    """"""\n\n    def __init__(self, weights, num_samples, replacement=True):\n        self.weights = torch.DoubleTensor(weights)\n        self.num_samples = num_samples\n        self.replacement = replacement\n\n    def __iter__(self):\n        return iter(torch.multinomial(self.weights, self.num_samples, self.replacement))\n\n    def __len__(self):\n        return self.num_samples\n\n\nclass BatchSampler(object):\n    """"""Wraps another sampler to yield a mini-batch of indices.\n\n    Args:\n        sampler (Sampler): Base sampler.\n        batch_size (int): Size of mini-batch.\n        drop_last (bool): If ``True``, the sampler will drop the last batch if\n            its size would be less than ``batch_size``\n\n    Example:\n        >>> list(BatchSampler(range(10), batch_size=3, drop_last=False))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n        >>> list(BatchSampler(range(10), batch_size=3, drop_last=True))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    """"""\n\n    def __init__(self, sampler, batch_size, drop_last):\n        self.sampler = sampler\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n\n    def __iter__(self):\n        batch = []\n        for idx in self.sampler:\n            batch.append(idx)\n            if len(batch) == self.batch_size:\n                yield batch\n                batch = []\n        if len(batch) > 0 and not self.drop_last:\n            yield batch\n\n    def __len__(self):\n        if self.drop_last:\n            return len(self.sampler) // self.batch_size\n        else:\n            return (len(self.sampler) + self.batch_size - 1) // self.batch_size\n'"
lib/nn/modules/tests/test_numeric_batchnorm.py,5,"b""# -*- coding: utf-8 -*-\n# File   : test_numeric_batchnorm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n\nimport unittest\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom sync_batchnorm.unittest import TorchTestCase\n\n\ndef handy_var(a, unbias=True):\n    n = a.size(0)\n    asum = a.sum(dim=0)\n    as_sum = (a ** 2).sum(dim=0)  # a square sum\n    sumvar = as_sum - asum * asum / n\n    if unbias:\n        return sumvar / (n - 1)\n    else:\n        return sumvar / n\n\n\nclass NumericTestCase(TorchTestCase):\n    def testNumericBatchNorm(self):\n        a = torch.rand(16, 10)\n        bn = nn.BatchNorm2d(10, momentum=1, eps=1e-5, affine=False)\n        bn.train()\n\n        a_var1 = Variable(a, requires_grad=True)\n        b_var1 = bn(a_var1)\n        loss1 = b_var1.sum()\n        loss1.backward()\n\n        a_var2 = Variable(a, requires_grad=True)\n        a_mean2 = a_var2.mean(dim=0, keepdim=True)\n        a_std2 = torch.sqrt(handy_var(a_var2, unbias=False).clamp(min=1e-5))\n        # a_std2 = torch.sqrt(a_var2.var(dim=0, keepdim=True, unbiased=False) + 1e-5)\n        b_var2 = (a_var2 - a_mean2) / a_std2\n        loss2 = b_var2.sum()\n        loss2.backward()\n\n        self.assertTensorClose(bn.running_mean, a.mean(dim=0))\n        self.assertTensorClose(bn.running_var, handy_var(a))\n        self.assertTensorClose(a_var1.data, a_var2.data)\n        self.assertTensorClose(b_var1.data, b_var2.data)\n        self.assertTensorClose(a_var1.grad, a_var2.grad)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
lib/nn/modules/tests/test_sync_batchnorm.py,7,"b'# -*- coding: utf-8 -*-\n# File   : test_sync_batchnorm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n# \n# This file is part of Synchronized-BatchNorm-PyTorch.\n\nimport unittest\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom sync_batchnorm import SynchronizedBatchNorm1d, SynchronizedBatchNorm2d, DataParallelWithCallback\nfrom sync_batchnorm.unittest import TorchTestCase\n\n\ndef handy_var(a, unbias=True):\n    n = a.size(0)\n    asum = a.sum(dim=0)\n    as_sum = (a ** 2).sum(dim=0)  # a square sum\n    sumvar = as_sum - asum * asum / n\n    if unbias:\n        return sumvar / (n - 1)\n    else:\n        return sumvar / n\n\n\ndef _find_bn(module):\n    for m in module.modules():\n        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, SynchronizedBatchNorm1d, SynchronizedBatchNorm2d)):\n            return m\n\n\nclass SyncTestCase(TorchTestCase):\n    def _syncParameters(self, bn1, bn2):\n        bn1.reset_parameters()\n        bn2.reset_parameters()\n        if bn1.affine and bn2.affine:\n            bn2.weight.data.copy_(bn1.weight.data)\n            bn2.bias.data.copy_(bn1.bias.data)\n\n    def _checkBatchNormResult(self, bn1, bn2, input, is_train, cuda=False):\n        """"""Check the forward and backward for the customized batch normalization.""""""\n        bn1.train(mode=is_train)\n        bn2.train(mode=is_train)\n\n        if cuda:\n            input = input.cuda()\n\n        self._syncParameters(_find_bn(bn1), _find_bn(bn2))\n\n        input1 = Variable(input, requires_grad=True)\n        output1 = bn1(input1)\n        output1.sum().backward()\n        input2 = Variable(input, requires_grad=True)\n        output2 = bn2(input2)\n        output2.sum().backward()\n\n        self.assertTensorClose(input1.data, input2.data)\n        self.assertTensorClose(output1.data, output2.data)\n        self.assertTensorClose(input1.grad, input2.grad)\n        self.assertTensorClose(_find_bn(bn1).running_mean, _find_bn(bn2).running_mean)\n        self.assertTensorClose(_find_bn(bn1).running_var, _find_bn(bn2).running_var)\n\n    def testSyncBatchNormNormalTrain(self):\n        bn = nn.BatchNorm1d(10)\n        sync_bn = SynchronizedBatchNorm1d(10)\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10), True)\n\n    def testSyncBatchNormNormalEval(self):\n        bn = nn.BatchNorm1d(10)\n        sync_bn = SynchronizedBatchNorm1d(10)\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10), False)\n\n    def testSyncBatchNormSyncTrain(self):\n        bn = nn.BatchNorm1d(10, eps=1e-5, affine=False)\n        sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n\n        bn.cuda()\n        sync_bn.cuda()\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10), True, cuda=True)\n\n    def testSyncBatchNormSyncEval(self):\n        bn = nn.BatchNorm1d(10, eps=1e-5, affine=False)\n        sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n\n        bn.cuda()\n        sync_bn.cuda()\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10), False, cuda=True)\n\n    def testSyncBatchNorm2DSyncTrain(self):\n        bn = nn.BatchNorm2d(10)\n        sync_bn = SynchronizedBatchNorm2d(10)\n        sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n\n        bn.cuda()\n        sync_bn.cuda()\n\n        self._checkBatchNormResult(bn, sync_bn, torch.rand(16, 10, 16, 16), True, cuda=True)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
