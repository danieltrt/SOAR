file_path,api_count,code
make_gif.py,0,"b'import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nfrom pathlib import Path\nfrom skimage import io\n\nimport matplotlib.animation as ani\nfrom IPython.display import HTML\nimport matplotlib\n\nsource_dir = Path(\'./data/source/test_img\')\ntarget_dir = Path(\'./results/target/test_latest/images\')\nlabel_dir = Path(\'./data/source/test_label_ori\')\n\nsource_img_paths = sorted(source_dir.iterdir())\ntarget_synth_paths = sorted(target_dir.glob(\'*synthesized*\'))\ntarget_label_paths = sorted(label_dir.iterdir())\n\n\ndef animate(nframe):\n    ax1.clear()\n    ax2.clear()\n    ax3.clear()\n\n    source_img = io.imread(source_img_paths[nframe])\n    ax1.imshow(source_img)\n    ax1.set_xticks([])\n    ax1.set_yticks([])\n\n    target_label = io.imread(target_label_paths[nframe])\n    ax2.imshow(target_label)\n    ax2.set_xticks([])\n    ax2.set_yticks([])\n\n    target_synth = io.imread(target_synth_paths[nframe])\n    ax3.imshow(target_synth)\n    ax3.set_xticks([])\n    ax3.set_yticks([])\n\n\nfig = plt.figure(figsize=(12, 6))\nax1 = fig.add_subplot(131)\nax2 = fig.add_subplot(132)\nax3 = fig.add_subplot(133)\n\nanim = ani.FuncAnimation(fig, animate, frames=len(target_label_paths), interval=1000 / 24)\nplt.close()\n\njs_anim = HTML(anim.to_jshtml())\n \n\nanim.save(""output.gif"", writer=""imagemagick"")\n'"
make_source.py,7,"b'\'\'\'Download and extract video\'\'\'\nimport cv2\nfrom pathlib import Path\nimport os\nimport torch\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = ""0""\n\ntorch.multiprocessing.set_sharing_strategy(\'file_system\')\ntorch.backends.cudnn.benchmark = True\ntorch.cuda.set_device(0)\n\nsave_dir = Path(\'./data/source/\')\nsave_dir.mkdir(exist_ok=True)\n\nimg_dir = save_dir.joinpath(\'images\')\nimg_dir.mkdir(exist_ok=True)\n\nif len(os.listdir(\'./data/source/images\'))<100:\n    cap = cv2.VideoCapture(str(save_dir.joinpath(\'mv.mp4\')))\n    i = 0\n    while (cap.isOpened()):\n        flag, frame = cap.read()\n        if flag == False or i >= 1000:\n            break\n        cv2.imwrite(str(img_dir.joinpath(\'{:05}.png\'.format(i))), frame)\n        if i%100 == 0:\n            print(\'Has generated %d picetures\'%i)\n        i += 1\n\n\'\'\'Pose estimation (OpenPose)\'\'\'\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom tqdm import tqdm\n\nopenpose_dir = Path(\'./src/PoseEstimation/\')\n\nimport sys\nsys.path.append(str(openpose_dir))\nsys.path.append(\'./src/utils\')\n\n\n# openpose\n#from network.rtpose_vgg import gopenpose_diret_model\nfrom evaluate.coco_eval import get_multiplier, get_outputs\nfrom network.rtpose_vgg import get_model\n# utils\nfrom openpose_utils import remove_noise, get_pose\n\n\nweight_name = \'./src/PoseEstimation/network/weight/pose_model.pth\'\n\nmodel = get_model(\'vgg19\')\nmodel.load_state_dict(torch.load(weight_name))\nmodel = torch.nn.DataParallel(model).cuda()\nmodel.float()\nmodel.eval()\n\n\'\'\'make label images for pix2pix\'\'\'\ntest_img_dir = save_dir.joinpath(\'test_img\')\ntest_img_dir.mkdir(exist_ok=True)\ntest_label_dir = save_dir.joinpath(\'test_label_ori\')\ntest_label_dir.mkdir(exist_ok=True)\ntest_head_dir = save_dir.joinpath(\'test_head_ori\')\ntest_head_dir.mkdir(exist_ok=True)\n\npose_cords = []\nfor idx in tqdm(range(len(os.listdir(str(img_dir))))):\n    img_path = img_dir.joinpath(\'{:05}.png\'.format(idx))\n    img = cv2.imread(str(img_path))\n    shape_dst = np.min(img.shape[:2])\n    oh = (img.shape[0] - shape_dst) // 2\n    ow = (img.shape[1] - shape_dst) // 2\n\n    img = img[oh:oh + shape_dst, ow:ow + shape_dst]\n    img = cv2.resize(img, (512, 512))\n    multiplier = get_multiplier(img)\n    with torch.no_grad():\n        paf, heatmap = get_outputs(multiplier, img, model, \'rtpose\')\n    r_heatmap = np.array([remove_noise(ht)\n                          for ht in heatmap.transpose(2, 0, 1)[:-1]]) \\\n        .transpose(1, 2, 0)\n    heatmap[:, :, :-1] = r_heatmap\n    param = {\'thre1\': 0.1, \'thre2\': 0.05, \'thre3\': 0.5}\n    label, cord = get_pose(param, heatmap, paf)\n    index = 13\n    crop_size = 25\n    try:\n        head_cord = cord[index]\n    except:\n        head_cord = pose_cords[-1] # if there is not head point in picture, use last frame\n\n    pose_cords.append(head_cord)\n    head = img[int(head_cord[1] - crop_size): int(head_cord[1] + crop_size),\n           int(head_cord[0] - crop_size): int(head_cord[0] + crop_size), :]\n    plt.imshow(head)\n    plt.savefig(str(test_head_dir.joinpath(\'pose_{}.jpg\'.format(idx))))\n    plt.clf()\n    cv2.imwrite(str(test_img_dir.joinpath(\'{:05}.png\'.format(idx))), img)\n    cv2.imwrite(str(test_label_dir.joinpath(\'{:05}.png\'.format(idx))), label)\n    if idx % 100 == 0 and idx != 0:\n        pose_cords_arr = np.array(pose_cords, dtype=np.int)\n        np.save(str((save_dir.joinpath(\'pose_source.npy\'))), pose_cords_arr)\n\npose_cords_arr = np.array(pose_cords, dtype=np.int)\nnp.save(str((save_dir.joinpath(\'pose_source.npy\'))), pose_cords_arr)\ntorch.cuda.empty_cache()\n'"
make_target.py,4,"b'import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport torch\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport os\nimport warnings\nwarnings.filterwarnings(\'ignore\')\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = ""0""\n\nopenpose_dir = Path(\'./src/PoseEstimation/\')\n\n\nsave_dir = Path(\'./data/target/\')\nsave_dir.mkdir(exist_ok=True)\n\nimg_dir = save_dir.joinpath(\'images\')\nimg_dir.mkdir(exist_ok=True)\n\nif len(os.listdir(\'./data/target/images\'))<100:\n    cap = cv2.VideoCapture(str(save_dir.joinpath(\'mv.mp4\')))\n    i = 0\n    while (cap.isOpened()):\n        flag, frame = cap.read()\n        if flag == False :\n            break\n        cv2.imwrite(str(img_dir.joinpath(\'{:05}.png\'.format(i))), frame)\n        if i%100 == 0:\n            print(\'Has generated %d picetures\'%i)\n        i += 1\n\nimport sys\nsys.path.append(str(openpose_dir))\nsys.path.append(\'./src/utils\')\n# openpose\nfrom network.rtpose_vgg import get_model\nfrom evaluate.coco_eval import get_multiplier, get_outputs\n\n# utils\nfrom openpose_utils import remove_noise, get_pose\n\nweight_name = \'./src/PoseEstimation/network/weight/pose_model.pth\'\nprint(\'load model...\')\nmodel = get_model(\'vgg19\')\nmodel.load_state_dict(torch.load(weight_name))\nmodel = torch.nn.DataParallel(model).cuda()\nmodel.float()\nmodel.eval()\npass\n\nsave_dir = Path(\'./data/target/\')\nsave_dir.mkdir(exist_ok=True)\n\nimg_dir = save_dir.joinpath(\'images\')\nimg_dir.mkdir(exist_ok=True)\n\n\n\'\'\'make label images for pix2pix\'\'\'\ntrain_dir = save_dir.joinpath(\'train\')\ntrain_dir.mkdir(exist_ok=True)\n\ntrain_img_dir = train_dir.joinpath(\'train_img\')\ntrain_img_dir.mkdir(exist_ok=True)\ntrain_label_dir = train_dir.joinpath(\'train_label\')\ntrain_label_dir.mkdir(exist_ok=True)\ntrain_head_dir = train_dir.joinpath(\'head_img\')\ntrain_head_dir.mkdir(exist_ok=True)\n\npose_cords = []\nfor idx in tqdm(range(len(os.listdir(str(img_dir))))):\n    img_path = img_dir.joinpath(\'{:05}.png\'.format(idx))\n    img = cv2.imread(str(img_path))\n    shape_dst = np.min(img.shape[:2])\n    oh = (img.shape[0] - shape_dst) // 2\n    ow = (img.shape[1] - shape_dst) // 2\n\n    img = img[oh:oh + shape_dst, ow:ow + shape_dst]\n    img = cv2.resize(img, (512, 512))\n    multiplier = get_multiplier(img)\n    with torch.no_grad():\n        paf, heatmap = get_outputs(multiplier, img, model, \'rtpose\')\n    r_heatmap = np.array([remove_noise(ht)\n                          for ht in heatmap.transpose(2, 0, 1)[:-1]]).transpose(1, 2, 0)\n    heatmap[:, :, :-1] = r_heatmap\n    param = {\'thre1\': 0.1, \'thre2\': 0.05, \'thre3\': 0.5}\n    #TODO get_pose\n    label, cord = get_pose(param, heatmap, paf)\n    index = 13\n    crop_size = 25\n    try:\n        head_cord = cord[index]\n    except:\n        head_cord = pose_cords[-1] # if there is not head point in picture, use last frame\n\n    pose_cords.append(head_cord)\n    head = img[int(head_cord[1] - crop_size): int(head_cord[1] + crop_size),\n           int(head_cord[0] - crop_size): int(head_cord[0] + crop_size), :]\n    plt.imshow(head)\n    plt.savefig(str(train_head_dir.joinpath(\'pose_{}.jpg\'.format(idx))))\n    plt.clf()\n    cv2.imwrite(str(train_img_dir.joinpath(\'{:05}.png\'.format(idx))), img)\n    cv2.imwrite(str(train_label_dir.joinpath(\'{:05}.png\'.format(idx))), label)\n\npose_cords = np.array(pose_cords, dtype=np.int)\nnp.save(str((save_dir.joinpath(\'pose.npy\'))), pose_cords)\ntorch.cuda.empty_cache()\n'"
normalization.py,0,"b""# *_*coding:utf-8 *_*\nfrom tqdm import tqdm\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nfrom pathlib import Path\n\n# for idx in range(200,400):\n#     img = cv2.imread('./data/source/test_label_ori/label_{}.png'.format(idx))\n#     cv2.imwrite('./data/source/test_label_ori/{:05d}.png'.format(idx), img)\n\n\ntarget_img = cv2.imread('./data/target/train/train_label/00001.png')[:,:,0]\ntarget_img_rgb = cv2.imread('./data/target/train/train_img/00001.png')\nsource_img = cv2.imread('./data/target/train/train_label/00001.png')[:,:,0]\nsource_img_rgb = cv2.imread('./data/target/train/train_img/00001.png')\n\npath = './data/source/test_label_ori/'\nsave_dir = Path('./data/source/')\noutput = save_dir.joinpath('test_label')\noutput.mkdir(exist_ok=True)\nhead_dir = save_dir.joinpath('test_head')\nhead_dir.mkdir(exist_ok=True)\npose_dir = Path('./data/source/pose_source.npy')\npose_cord = np.load(str(pose_dir))\n\nplt.subplot(222)\nplt.imshow(target_img)\nplt.subplot(221)\nplt.imshow(target_img_rgb)\nplt.subplot(224)\nplt.imshow(source_img)\nplt.subplot(223)\nplt.imshow(source_img_rgb)\nplt.savefig('norm.png')\nplt.show()\n\n\ndef get_scale(label_img):\n    any1 = label_img.any(axis=1)\n    linspace1 = np.arange(len(any1))\n    head_x, height = linspace1[list(any1)][0], len(linspace1[list(any1)])\n    any0 = label_img[head_x, :] != 0\n    linspace2 = np.arange(len(any0))\n    head_y = int(np.mean(linspace2[list(any0)]))\n    return (head_x,head_y),height\n\ntarget_head,target_height = get_scale(target_img)\ntarget_head_x = target_head[0]\ntarget_head_y = target_head[1]\n\nsource_head,source_height = get_scale(source_img)\n\n\nnew_head_pose = []\n\nfor img_idx in tqdm(range(len(os.listdir(path)))):\n    img = cv2.imread(path+'{:05}.png'.format(img_idx))\n    source_rsize = cv2.resize(img,\n                              (int(img.shape[0] * target_height / source_height),\n                               int(img.shape[1] * target_height / source_height)))\n\n    source_pad = np.pad(source_rsize, ((1000, 1000), (1000, 1000),(0,0)), mode='edge')\n\n    source_head_rs, source_height_rs = get_scale(source_pad[:,:,0])\n    source_head_rs_x = source_head_rs[0]\n    source_head_rs_y = source_head_rs[1]\n\n    new_source = source_pad[\n                 (source_head_rs_x - target_head_x):(source_head_rs_x + (target_img.shape[0] - target_head_x)),\n                 int((source_pad.shape[1] - target_img.shape[1])/2):int((source_pad.shape[1]-(source_pad.shape[1] - target_img.shape[1])/2))\n                 ]\n    new_source_head, _ = get_scale(new_source[:,:,0])\n\n    source_head_x, source_head_y = source_head\n    source_cord_y, source_cord_x = pose_cord[0]\n\n    new_head_y = int(new_source_head[1] - (source_head_y - source_cord_y))\n    new_head_x = int(new_source_head[0] - (source_head_x - source_cord_x) * (target_height / source_height))\n\n    crop_size = 50\n    new_head_pose.append([new_head_y,new_head_x])\n    head = img[int(new_head_x - crop_size): int(new_head_x + crop_size),\n           int(new_head_y - crop_size): int(new_head_y + crop_size), :]\n    plt.imshow(head)\n    plt.savefig(str(head_dir.joinpath('pose_{}.jpg'.format(img_idx))))\n\n\n    cv2.imwrite(str(output) + '/{:05}.png'.format(img_idx),new_source)\n\npose_cords_arr = np.array(new_head_pose, dtype=np.int)\nnp.save(str((save_dir.joinpath('pose_source_norm.npy'))), pose_cords_arr)\n"""
train_pose2vid.py,5,"b'import os\nimport numpy as np\nimport torch\nimport time\nimport sys\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nfrom pathlib import Path\nimport warnings\n\nwarnings.filterwarnings(\'ignore\')\nmainpath = os.getcwd()\npix2pixhd_dir = Path(mainpath+\'/src/pix2pixHD/\')\nsys.path.append(str(pix2pixhd_dir))\n\n\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nimport util.util as util\nfrom util.visualizer import Visualizer\nimport src.config.train_opt as opt\n\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = ""0""\ntorch.multiprocessing.set_sharing_strategy(\'file_system\')\ntorch.backends.cudnn.benchmark = True\n\n\ndef main():\n    iter_path = os.path.join(opt.checkpoints_dir, opt.name, \'iter.txt\')\n    data_loader = CreateDataLoader(opt)\n    dataset = data_loader.load_data()\n    dataset_size = len(data_loader)\n    print(\'#training images = %d\' % dataset_size)\n\n    start_epoch, epoch_iter = 1, 0\n    total_steps = (start_epoch - 1) * dataset_size + epoch_iter\n    display_delta = total_steps % opt.display_freq\n    print_delta = total_steps % opt.print_freq\n    save_delta = total_steps % opt.save_latest_freq\n\n    model = create_model(opt)\n    model = model.cuda()\n    visualizer = Visualizer(opt)\n\n    for epoch in range(start_epoch, opt.niter + opt.niter_decay + 1):\n        epoch_start_time = time.time()\n        if epoch != start_epoch:\n            epoch_iter = epoch_iter % dataset_size\n        for i, data in enumerate(dataset, start=epoch_iter):\n            iter_start_time = time.time()\n            total_steps += opt.batchSize\n            epoch_iter += opt.batchSize\n\n            # whether to collect output images\n            save_fake = total_steps % opt.display_freq == display_delta\n\n            ############## Forward Pass ######################\n            losses, generated = model(Variable(data[\'label\']), Variable(data[\'inst\']),\n                                      Variable(data[\'image\']), Variable(data[\'feat\']), infer=save_fake)\n\n            # sum per device losses\n            losses = [torch.mean(x) if not isinstance(x, int) else x for x in losses]\n            loss_dict = dict(zip(model.loss_names, losses))\n\n            # calculate final loss scalar\n            loss_D = (loss_dict[\'D_fake\'] + loss_dict[\'D_real\']) * 0.5\n            loss_G = loss_dict[\'G_GAN\'] + loss_dict.get(\'G_GAN_Feat\', 0) + loss_dict.get(\'G_VGG\', 0)\n\n            ############### Backward Pass ####################\n            # update generator weights\n            model.optimizer_G.zero_grad()\n            loss_G.backward()\n            model.optimizer_G.step()\n\n            # update discriminator weights\n            model.optimizer_D.zero_grad()\n            loss_D.backward()\n            model.optimizer_D.step()\n\n\n            ############## Display results and errors ##########\n            ### print out errors\n            if total_steps % opt.print_freq == print_delta:\n                errors = {k: v.data[0] if not isinstance(v, int) else v for k, v in loss_dict.items()}\n                t = (time.time() - iter_start_time) / opt.batchSize\n                visualizer.print_current_errors(epoch, epoch_iter, errors, t)\n                visualizer.plot_current_errors(errors, total_steps)\n\n            ### display output images\n            if save_fake:\n                visuals = OrderedDict([(\'input_label\', util.tensor2label(data[\'label\'][0], opt.label_nc)),\n                                       (\'synthesized_image\', util.tensor2im(generated.data[0])),\n                                       (\'real_image\', util.tensor2im(data[\'image\'][0]))])\n                visualizer.display_current_results(visuals, epoch, total_steps)\n\n            ### save latest model\n            if total_steps % opt.save_latest_freq == save_delta:\n                print(\'saving the latest model (epoch %d, total_steps %d)\' % (epoch, total_steps))\n                model.save(\'latest\')\n                np.savetxt(iter_path, (epoch, epoch_iter), delimiter=\',\', fmt=\'%d\')\n\n            if epoch_iter >= dataset_size:\n                break\n\n        # end of epoch\n        print(\'End of epoch %d / %d \\t Time Taken: %d sec\' %\n              (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n\n        ### save model for this epoch\n        if epoch % opt.save_epoch_freq == 0:\n            print(\'saving the model at the end of epoch %d, iters %d\' % (epoch, total_steps))\n            model.save(\'latest\')\n            model.save(epoch)\n            np.savetxt(iter_path, (epoch + 1, 0), delimiter=\',\', fmt=\'%d\')\n\n        ### instead of only training the local enhancer, train the entire network after certain iterations\n        if (opt.niter_fix_global != 0) and (epoch == opt.niter_fix_global):\n            model.update_fixed_params()\n\n        ### linearly decay learning rate after certain iterations\n        if epoch > opt.niter:\n            model.update_learning_rate()\n\n    torch.cuda.empty_cache()\n\nif __name__ == \'__main__\':\n    main()\n'"
transfer.py,1,"b'import os\nimport torch\nfrom collections import OrderedDict\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport sys\npix2pixhd_dir = Path(\'./src/pix2pixHD/\')\nsys.path.append(str(pix2pixhd_dir))\n\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nimport util.util as util\nfrom util.visualizer import Visualizer\nfrom util import html\nimport src.config.test_opt as opt\n\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = ""0""\niter_path = os.path.join(opt.checkpoints_dir, opt.name, \'iter.txt\')\n\ndata_loader = CreateDataLoader(opt)\ndataset = data_loader.load_data()\nvisualizer = Visualizer(opt)\n\nweb_dir = os.path.join(opt.results_dir, opt.name, \'%s_%s\' % (opt.phase, opt.which_epoch))\nwebpage = html.HTML(web_dir, \'Experiment = %s, Phase = %s, Epoch = %s\' % (opt.name, opt.phase, opt.which_epoch))\n\nmodel = create_model(opt)\n\nfor data in tqdm(dataset):\n    minibatch = 1\n    generated = model.inference(data[\'label\'], data[\'inst\'])\n\n    visuals = OrderedDict([(\'input_label\', util.tensor2label(data[\'label\'][0], opt.label_nc)),\n                           (\'synthesized_image\', util.tensor2im(generated.data[0]))])\n    img_path = data[\'path\']\n    visualizer.save_images(webpage, visuals, img_path)\nwebpage.save()\ntorch.cuda.empty_cache()\n'"
face_enhancer/dataset.py,1,"b""import os\nimport pickle\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom skimage.io import imread\n\nclass ImageFolderDataset(Dataset):\n    def __init__(self, root, cache=None, is_test=False):\n        self.is_test = is_test\n        # if cache is not None and os.path.isfile(cache):\n        if cache is not None and os.path.isfile(cache):\n            with open(cache, 'rb') as f:\n                self.root, self.images, self.size = pickle.load(f)\n        else:\n            self.images = sorted(os.listdir(os.path.join(root, 'test_real')))\n            self.root = root\n            tmp = imread(os.path.join(self.root, 'test_real', self.images[0]))\n            self.size = tmp.shape[:-1]\n            if cache is not None:\n                with open(cache, 'wb') as f:\n                    pickle.dump((self.root, self.images, self.size), f)\n\n    # TODO: rewrite this part\n    def __getitem__(self, item):\n        name = self.images[item]\n        real_img = None if self.is_test else imread(os.path.join(self.root, 'test_real', name))\n        fake_img = imread(os.path.join(self.root, 'test_sync', name))\n        return real_img, fake_img\n\n    def __len__(self):\n        return len(self.images)\n\n\nclass FaceCropDataset(Dataset): #TODO FaceCropDataset\n    def __init__(self, image_dataset, pose_file, transform, crop_size=96):\n        self.image_dataset = image_dataset\n        self.transform = transform\n        self.crop_size = crop_size\n\n        if not os.path.isfile(pose_file):\n            raise(FileNotFoundError('Cannot find pose data...'))\n        self.poses = np.load(pose_file)\n\n    def get_full_sample(self, item):\n        # skip over bad items\n        while True:\n            real_img, fake_img = self.image_dataset[item]\n            head_pos = self.poses[item]\n            if head_pos[0] == -1 or head_pos[1] == -1:\n                item = (item + 1) % len(self.image_dataset)\n            else:\n                break\n\n        # crop head image\n        size = self.image_dataset.size\n        left = int(head_pos[0] - self.crop_size / 2)  # don't suppose left will go out of bound eh?\n        left = left if left >= 0 else 0\n        left = size[1] - self.crop_size if left + self.crop_size > size[1] else left\n\n        top = int(head_pos[1] - self.crop_size / 2)\n        top = top if top >= 0 else 0\n        top = size[0] - self.crop_size if top + self.crop_size > size[0] else top\n\n\n        real_head = None if self.image_dataset.is_test else \\\n                    self.transform(real_img[top: top + self.crop_size, left: left + self.crop_size,  :])\n        fake_head = self.transform(fake_img[top: top + self.crop_size, left: left + self.crop_size,  :])\n\n        # from matplotlib.pyplot import imshow, show\n        # imshow(real_head.numpy().transpose((2,1,0)))\n        # show()\n        # imshow(fake_head.numpy().transpose((2,1,0)))\n        # show()\n\n        # keep full fake image to visualize enhancement result\n        return real_head, fake_head, \\\n               top, top + self.crop_size, \\\n               left, left + self.crop_size, \\\n               real_img, fake_img\n\n    def __getitem__(self, item):\n        real_head, fake_head, _, _, _, _, _, _ = self.get_full_sample(item)\n        return {'real_heads': real_head, 'fake_heads': fake_head}\n\n    def __len__(self):\n        return len(self.image_dataset)\n\n"""
face_enhancer/enhance.py,5,"b'import model\nimport dataset\nimport cv2\nfrom trainer import Trainer\nimport os\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nimport numpy as np\nfrom PIL import Image\nfrom skimage.io import imsave\nfrom imageio import get_writer\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = ""0""\nimage_transforms = transforms.Compose([\n        Image.fromarray,\n        transforms.ToTensor(),\n        transforms.Normalize([.5, .5, .5], [.5, .5, .5]),\n    ])\n    \ndevice = torch.device(\'cuda\')\n\n\ndef load_models(directory):\n    generator = model.GlobalGenerator(n_downsampling=2, n_blocks=6)\n    gen_name = os.path.join(directory, \'final_generator.pth\')\n\n    if os.path.isfile(gen_name):\n        gen_dict = torch.load(gen_name)\n        generator.load_state_dict(gen_dict)\n        \n    return generator.to(device)\n    \ndef torch2numpy(tensor):\n        generated = tensor.detach().cpu().permute(1, 2, 0).numpy()\n        generated[generated < -1] = -1\n        generated[generated > 1] = 1\n        generated = (generated + 1) / 2 * 255\n        return generated.astype(np.uint8)\n    \n    \nif __name__ == \'__main__\':\n    torch.backends.cudnn.benchmark = True\n    dataset_dir = \'../data/face\'   # save test_sync in this folder\n    pose_name = \'../data/source/pose_source_norm.npy\' # coordinate save every heads\n    ckpt_dir = \'../checkpoints/face\'\n    result_dir = \'./results\'\n    save_dir = dataset_dir+\'/full_fake/\'\n\n    if not os.path.exists(save_dir):\n        print(\'generate %s\'%save_dir)\n        os.mkdir(save_dir)\n    else:\n        print(save_dir, \'is existing...\')\n\n\n    image_folder = dataset.ImageFolderDataset(dataset_dir, cache=os.path.join(dataset_dir, \'local.db\'), is_test=True)\n    face_dataset = dataset.FaceCropDataset(image_folder, pose_name, image_transforms, crop_size=48)\n    length = len(face_dataset)\n    print(\'Picture number\',length)\n\n    generator = load_models(os.path.join(ckpt_dir))\n\n    for i in tqdm(range(length)):\n        _, fake_head, top, bottom, left, right, real_full, fake_full \\\n            = face_dataset.get_full_sample(i)\n\n        with torch.no_grad():\n            fake_head.unsqueeze_(0)\n            fake_head = fake_head.to(device)\n            residual = generator(fake_head)\n            enhanced = fake_head + residual\n\n        enhanced.squeeze_()\n        enhanced = torch2numpy(enhanced)\n        fake_full_old = fake_full.copy()\n        fake_full[top: bottom, left: right, :] = enhanced\n\n        b, g, r = cv2.split(fake_full)\n        fake_full = cv2.merge([r, g, b])\n        cv2.imwrite(save_dir+ \'{:05}.png\'.format(i),fake_full)\n\n'"
face_enhancer/main.py,4,"b'import model\nimport dataset\nfrom trainer import Trainer\n\nimport os\n\nimport torch\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nfrom torch.backends import cudnn\nfrom PIL import Image\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = ""0""\nimage_transforms = transforms.Compose([\n        Image.fromarray,\n        transforms.ToTensor(),\n        transforms.Normalize([.5, .5, .5], [.5, .5, .5]),\n    ])\n\n\ndef load_models(directory, batch_num):\n    # 20180924: smaller network.\n    generator = model.GlobalGenerator(n_downsampling=2, n_blocks=6)\n    discriminator = model.NLayerDiscriminator(input_nc=3, n_layers=3)  # 48 input\n    gen_name = os.path.join(directory, \'%05d_generator.pth\' % batch_num)\n    dis_name = os.path.join(directory, \'%05d_discriminator.pth\' % batch_num)\n\n    if os.path.isfile(gen_name) and os.path.isfile(dis_name):\n        gen_dict = torch.load(gen_name)\n        dis_dict = torch.load(dis_name)\n        generator.load_state_dict(gen_dict)\n        discriminator.load_state_dict(dis_dict)\n        print(\'Models loaded, resume training from batch %05d...\' % batch_num)\n    else:\n        print(\'Cannot find saved models, start training from scratch...\')\n        batch_num = 0\n\n    return generator, discriminator, batch_num\n\n\ndef main(is_debug):\n    # configs\n    import os\n\n    dataset_dir = \'../data/face\'\n    pose_name = \'../data/target/pose.npy\'\n    ckpt_dir = \'../checkpoints/face\'\n    log_dir = \'../checkpoints/face/logs\'\n    batch_num = 10\n    batch_size = 10\n\n    image_folder = dataset.ImageFolderDataset(dataset_dir, cache=os.path.join(dataset_dir, \'local.db\'))\n    face_dataset = dataset.FaceCropDataset(image_folder, pose_name, image_transforms, crop_size=48)  # 48 for 512-frame, 96 for HD frame\n    data_loader = DataLoader(face_dataset, batch_size=batch_size,\n                             drop_last=True, num_workers=4, shuffle=True)\n\n    generator, discriminator, batch_num = load_models(ckpt_dir, batch_num)\n\n    if is_debug:\n        trainer = Trainer(ckpt_dir, log_dir, face_dataset, data_loader, log_every=1, save_every=1)\n    else:\n        trainer = Trainer(ckpt_dir, log_dir, face_dataset, data_loader)\n    trainer.train(generator, discriminator, batch_num)\n\n\nif __name__ == \'__main__\':\n    cudnn.enabled = True\n    is_debug=True\n    main(is_debug)\n'"
face_enhancer/model.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.parallel\nfrom torch.nn import functional\nimport torch.utils.data\nimport numpy as np\nfrom utils.spectral_norm import spectral_norm\n\n\n# Define a resnet block\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, padding_type, norm_layer, activation=nn.ReLU(True), use_dropout=False):\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, activation, use_dropout)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, activation, use_dropout):\n        conv_block = []\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n                       norm_layer(dim),\n                       activation]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n                       norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n        return out\n\n\n# transported from pix2pixHD project\nclass GlobalGenerator(nn.Module):\n    def __init__(self, input_nc=3, output_nc=3, ngf=64, n_downsampling=3, n_blocks=9, norm_layer=nn.BatchNorm2d,\n                 padding_type=\'reflect\'):\n        assert (n_blocks >= 0)\n        super(GlobalGenerator, self).__init__()\n        activation = nn.ReLU(True)\n\n        model = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0), norm_layer(ngf), activation]\n        # downsample\n        for i in range(n_downsampling):\n            mult = 2 ** i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1),\n                      norm_layer(ngf * mult * 2), activation]\n\n        # resnet blocks\n        mult = 2 ** n_downsampling\n        for i in range(n_blocks):\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, activation=activation, norm_layer=norm_layer)]\n\n        # upsample\n        for i in range(n_downsampling):\n            mult = 2 ** (n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1,\n                                         output_padding=1),\n                      norm_layer(int(ngf * mult / 2)), activation]\n        model += [nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0), nn.Tanh()]\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        return self.model(x)\n\n\n""""""\n  Use CycleGAN\'s NLayerDiscriminator as a starting point...\n  https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py\n\n  The discriminator is fixed to process 64*64 input.\n  params:\n    @ n_layers: You can change this param to control the receptive field size\n        n_layers  output  receptive field size\n            4     13*13           34\n            5      5*5            70\n            6      1*1            256\n\n  P.S. This implementation doesn\'t use sigmoid, so it must be trained with\n  nn.BCEWithLogitLoss() instead of nn.BCELoss()!       \n""""""\n\n\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=4,\n                 norm_layer=nn.BatchNorm2d,\n                 use_sigmoid=True, use_bias=False):\n        super(NLayerDiscriminator, self).__init__()\n\n        kw = 4\n        padw = 1\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2 ** n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                spectral_norm(norm_layer(ndf * nf_mult)),\n                nn.LeakyReLU(0.2, True),\n            ]\n\n        # modify this part for feature extraction\n        self.model = nn.Sequential(*sequence)  # Extract feature here\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2 ** n_layers, 8)\n        sequence = [  # building a new sequence, not adding modules!\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            spectral_norm(norm_layer(ndf * nf_mult)),\n            nn.LeakyReLU(0.2, True),\n            # nn.Dropout(0.1),\n            nn.Conv2d(ndf * nf_mult, 1, kernel_size=3, stride=1, padding=0)\n        ]\n\n        if use_sigmoid:\n            sequence += [nn.Sigmoid()]\n\n        self.predictor = nn.Sequential(*sequence)\n\n    # single label prediction\n    def forward(self, x):\n        return self.predictor(self.model(x)).squeeze()\n\n    # high-level feature matching\n    def extract_features(self, x):\n        return self.model(x).squeeze()\n'"
face_enhancer/prepare.py,1,"b'import os\nfrom pathlib import Path\nimport cv2\nfrom tqdm import tqdm\n\nface_sync_dir = Path(\'../data/face/ \')\nface_sync_dir.mkdir(exist_ok=True)\ntest_sync_dir = Path(\'../data/face/test_sync/ \')\ntest_sync_dir.mkdir(exist_ok=True)\ntest_real_dir = Path(\'../data/face/test_real/ \')\ntest_real_dir.mkdir(exist_ok=True)\ntest_img = Path(\'../data/target/test_img/ \')\ntest_img.mkdir(exist_ok=True)\ntest_label = Path(\'../data/target/test_label/ \')\ntest_label.mkdir(exist_ok=True)\n\ntrain_dir = \'../data/target/train/train_img/\'\nlabel_dir = \'../data/target/train/train_label/\'\n\nprint(\'Prepare test_real....\')\nfor img_idx in tqdm(range(len(os.listdir(train_dir)))):\n    img = cv2.imread(train_dir+\'{:05}.png\'.format(img_idx))\n    label = cv2.imread(label_dir+\'{:05}.png\'.format(img_idx))\n    cv2.imwrite(str(test_real_dir)+\'{:05}.png\'.format(img_idx),img)\n    cv2.imwrite(str(test_img)+\'{:05}.png\'.format(img_idx),img)\n    cv2.imwrite(str(test_label)+\'{:05}.png\'.format(img_idx),label)\n\nprint(\'Prepare test_sync....\')\nimport os\nimport torch\nfrom collections import OrderedDict\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport sys\npix2pixhd_dir = Path(\'../src/pix2pixHD/\')\nsys.path.append(str(pix2pixhd_dir))\n\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nimport util.util as util\nfrom util.visualizer import Visualizer\nfrom util import html\nimport src.config.test_opt as opt\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = ""0""\nopt.checkpoints_dir = \'../checkpoints/\'\nopt.dataroot=\'../data/target/\'\nopt.name=\'target\'\nopt.nThreads=0\nopt.results_dir=\'./prepare/\'\n\niter_path = os.path.join(opt.checkpoints_dir, opt.name, \'iter.txt\')\n\ndata_loader = CreateDataLoader(opt)\ndataset = data_loader.load_data()\nvisualizer = Visualizer(opt)\n\nweb_dir = os.path.join(opt.results_dir, opt.name, \'%s_%s\' % (opt.phase, opt.which_epoch))\nwebpage = html.HTML(web_dir, \'Experiment = %s, Phase = %s, Epoch = %s\' % (opt.name, opt.phase, opt.which_epoch))\n\nmodel = create_model(opt)\n\nfor data in tqdm(dataset):\n    minibatch = 1\n    generated = model.inference(data[\'label\'], data[\'inst\'])\n\n    visuals = OrderedDict([(\'synthesized_image\', util.tensor2im(generated.data[0]))])\n    img_path = data[\'path\']\n    visualizer.save_images(webpage, visuals, img_path)\nwebpage.save()\ntorch.cuda.empty_cache()\n\nprint(\'Copy the synthesized images...\')\nsynthesized_image_dir = \'./prepare/target/test_latest/images/\'\nfor img_idx in tqdm(range(len(os.listdir(synthesized_image_dir)))):\n    img = cv2.imread(synthesized_image_dir+\' {:05}_synthesized_image.jpg\'.format(img_idx))\n    cv2.imwrite(str(test_sync_dir) + \'{:05}.png\'.format(img_idx), img)\n\n\n'"
face_enhancer/trainer.py,11,"b'import os\nimport time\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom skimage.io import imsave\n\nclass Trainer(object):\n    def __init__(self, ckpt_dir, log_dir, dataset, dataloader,\n                 log_every=10, save_every=500,\n                 max_batches=40000, gen_lr=1e-4, dis_lr=1e-4):\n\n        if not os.path.isdir(ckpt_dir):\n            os.mkdir(ckpt_dir)\n        if not os.path.isdir(log_dir):\n            os.mkdir(log_dir)\n\n        self.ckpt_dir = ckpt_dir\n        self.log_dir = log_dir\n        self.log_every = log_every\n        self.save_every = save_every\n        self.max_batches = max_batches\n        self.gen_lr = gen_lr\n        self.dis_lr = dis_lr\n\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n        self.dataset = dataset\n        self.dataset_len = len(dataset)\n        self.sampler = dataloader\n        self.enumerator = None\n\n        from face_enhancer.utils.perceptual_loss import VGG_perceptual_loss\n        self.gen_loss = nn.MSELoss()\n        self.recon_loss = VGG_perceptual_loss(pretrained=True, device=self.device)\n        self.dis_loss = nn.MSELoss()  # LSGAN\n\n        np.random.seed(233)\n\n    # only keep samples\n    def get_batch(self):\n        if self.enumerator is None:\n            self.enumerator = enumerate(self.sampler)\n\n        batch_idx, batch = next(self.enumerator)\n        b = {}\n        for k, v in batch.items():\n            b[k] = v.to(self.device)\n\n        if batch_idx == len(self.sampler) - 1:\n            self.enumerator = enumerate(self.sampler)\n\n        return b\n\n    @staticmethod\n    def image2numpy(tensor):\n        generated = tensor.detach().cpu().permute(1, 2, 0).numpy()\n        generated[generated < -1] = -1\n        generated[generated > 1] = 1\n        generated = (generated + 1) / 2 * 255\n        return generated.astype(np.uint8)\n\n    @staticmethod\n    def _init_logs():\n        return {\'gen_loss\': 0, \'dis_loss\': 0}\n\n    def train_generator(self, g, d, g_opt):\n        batch = self.get_batch()\n        real_heads = batch[\'real_heads\']\n        fake_heads = batch[\'fake_heads\']\n\n        g_opt.zero_grad()\n\n        residuals = g(fake_heads)\n        enhanced_heads = fake_heads + residuals\n        fake_features = d.extract_features(enhanced_heads)\n        with torch.no_grad():\n            real_features = d.extract_features(real_heads)\n        gen_loss = self.gen_loss(fake_features, real_features)\n\n        recon_loss = self.recon_loss(enhanced_heads, real_heads)\n        # 20180924 change recon_loss weight\n        # 20180929 ablation study on recon_loss weight\n        gen_loss = gen_loss + 10 * recon_loss\n        gen_loss_val = gen_loss.item()\n        gen_loss.backward()\n        g_opt.step()\n\n        return gen_loss_val\n\n    def train_discriminator(self, g, d, d_opt):\n        batch = self.get_batch()\n        real_heads = batch[\'real_heads\']\n        fake_heads = batch[\'fake_heads\']\n\n        d_opt.zero_grad()\n        real_labels = d(real_heads)\n        with torch.no_grad():\n            ones = torch.ones_like(real_labels)\n            zeros = torch.zeros_like(real_labels)\n\n            # one sided label smoothing for vanilla gan\n            ones.uniform_(.9, 1.1)\n            zeros.uniform_(-.1, .1)\n\n        dis_real_loss = self.dis_loss(real_labels, ones)\n        dis_real_loss_val = dis_real_loss.item()\n\n        residual = g(fake_heads)\n        enhanced_heads = fake_heads + residual\n        fake_labels = d(enhanced_heads)\n        dis_fake_loss = self.dis_loss(fake_labels, zeros)\n        dis_fake_loss_val = dis_fake_loss.item()\n\n        dis_loss = dis_real_loss + dis_fake_loss\n        dis_loss.backward()\n        d_opt.step()\n\n        return dis_real_loss_val + dis_fake_loss_val\n\n    def save_models(self, generator, discriminator, batch_num):\n        torch.save(generator.state_dict(), os.path.join(self.ckpt_dir, \'%05d_generator.pth\' % batch_num))\n        torch.save(discriminator.state_dict(), os.path.join(self.ckpt_dir, \'%05d_discriminator.pth\' % batch_num))\n        print(\'Model saved... Batch Num: %05d\' % batch_num)\n\n    def validate_and_save(self, generator, batch_num):\n        idx = np.random.randint(self.dataset_len, size=(1,))[0]\n        #print(idx)\n        real_head, fake_head, top, bottom, left, right, real_full, fake_full \\\n            = self.dataset.get_full_sample(idx)\n\n        with torch.no_grad():\n            fake_head.unsqueeze_(0)\n            real_head = real_head.to(self.device)\n            fake_head = fake_head.to(self.device)\n            residual = generator(fake_head)\n            enhanced = fake_head + residual\n\n        fake_head.squeeze_()\n        enhanced.squeeze_()\n        residual.squeeze_()\n        image = torch.cat((real_head, fake_head, residual, enhanced), dim=2)\n        image = self.image2numpy(image)\n        imsave(os.path.join(self.log_dir, \'%05d_enhanced_head.png\' % batch_num), image)\n        fake_full_old = fake_full.copy()\n        fake_full[top: bottom, left: right, :] = self.image2numpy(enhanced)\n        imsave(os.path.join(self.log_dir, \'%05d_enhanced_full.png\' % batch_num),\n               np.concatenate((real_full, fake_full_old, fake_full), axis=1))\n\n    def train(self, generator, discriminator, batch):\n        generator = generator.to(self.device)\n        discriminator = discriminator.to(self.device)\n        generator.train()\n        discriminator.train()\n\n        opt_generator = optim.Adam(generator.parameters(),\n                                   lr=self.gen_lr, betas=(0.5, 0.999),\n                                   weight_decay=1e-5)\n        opt_discriminator = optim.Adam(discriminator.parameters(),\n                                       lr=self.dis_lr, betas=(0.5, 0.999),\n                                       weight_decay=1e-5)\n\n        logs = self._init_logs()\n        start = time.time()\n        while batch <= self.max_batches:\n            batch += 1\n            gen_loss = self.train_generator(generator, discriminator, opt_generator)\n            dis_loss = self.train_discriminator(generator, discriminator, opt_discriminator)\n\n            logs[\'gen_loss\'] += gen_loss\n            logs[\'dis_loss\'] += dis_loss\n\n            if batch % self.log_every == 0:\n                log_string = ""Batch %d"" % batch\n                for k, v in logs.items():\n                    log_string += "" [%s] %5.3f"" % (k, v)\n\n                log_string += "". Took %5.2f"" % (time.time() - start)\n                print(log_string)\n                logs = self._init_logs()\n                start = time.time()\n\n            if batch % self.save_every == 0:\n                self.validate_and_save(generator, batch)\n                self.save_models(generator, discriminator, batch)\n                torch.cuda.empty_cache()  # just for safety\n'"
face_enhancer/utils/configs.py,2,"b'""""""\nCopyright (C) 2017 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-ND 4.0 license (https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode).\n\n""""""\n\nimport torch\n\nclass Config():\n    def __init__(self, **kwargs):\n        self.__dict__.update(kwargs)\n\ndef debug(num=0):\n    return Config(\n        ngf=64,\n        from_batch = num,\n        # video_batch_size = 1,\n        # video_length = 3,\n        # clip_size=3,\n        image_batch_size = 3,\n        log_interval = 1,\n        save_interval = 1,\n        train_batches = 100000,\n        dataset = \'D:/data/ntu_image_skeleton\',\n        log_folder=\'logs_debug\',\n        image_height = 256,\n        image_width = None,\n        device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\'),\n        is_debug = True,\n        enable_visdom = False,\n        random_flip = True,\n        use_dropout = True,\n        recon_loss_weight = 2,\n        gen_lr = 2e-5,\n        gen_loss_type = \'feature_matching_loss\',\n        # gen_loss_type = \'gan_loss\',\n\t    # recon_loss_type=\'vgg_perceptual_loss\',\n\t    recon_loss_type=\'L1_loss\',\n        dis_lr = 1e-5,\n        dis_loss_type = \'least_square_loss\',\n        # dis_loss_type = \'gan_loss\',\n\n    )\n\ndef train(num=0):\n    return Config(\n        ngf=64,\n        from_batch=num,\n        # video_batch_size = 1,\n        # video_length = 3,\n        # clip_size=3,\n        image_batch_size=3,\n        log_interval=10,\n        save_interval=200,\n        train_batches=100000,\n        dataset=\'D:/data/ntu_image_skeleton_clean_bg\',\n        log_folder=\'logs_train_ntu_256_clean_bg_#13\',\n        image_height=256,\n        image_width=None,\n        device=torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\'),\n        is_debug=False,\n        enable_visdom=True,\n        random_flip=True,\n        use_dropout=True,\n        recon_loss_weight=2,\n        gen_lr = 1e-5,\n        gen_loss_type=\'feature_matching_loss\',\n\t    # gen_loss_type = \'gan_loss\',\n\t    # recon_loss_type=\'vgg_perceptual_loss\',\n\t    recon_loss_type=\'L1_loss\',\n\t    dis_lr = 1e-5,\n        dis_loss_type = \'least_square_loss\',\n        # dis_loss_type = \'gan_loss\',\n        changes_upon_last_version=[\n            \'Try spectral norm\',\n            \'Dang it I\\\'m out... (for now)?\'\n        ]\n    )\n'"
face_enhancer/utils/perceptual_loss.py,4,"b'from torchvision.models import vgg\nimport torch\nfrom torch import nn\nimport os\n""""""\n  TODO: VGG perceptual loss module\n  something is wrong? Must double check this time...\n""""""\nclass VGG_perceptual_loss(nn.Module):\n    def __init__(self, pretrained=False, device=\'cuda\'):\n        super(VGG_perceptual_loss, self).__init__()\n        self.device = device\n        self.loss_function = nn.L1Loss()\n        self.vgg_features = vgg.make_layers(vgg.cfg[\'D\'])\n        if pretrained:\n            self.vgg_features.load_state_dict(\n                torch.load(\'vgg16_pretrained_features.pth\'))\n        self.vgg_features.to(device)\n        # freeze parameter update\n        for params in self.vgg_features.parameters():\n            params.requires_grad = False\n        self.layer_name_mapping = {\n            \'3\': ""relu1_2"",\n            \'8\': ""relu2_2"",\n            \'15\': ""relu3_3"",\n            \'22\': ""relu4_3""\n        }\n\n    def forward(self, input, target):\n        # TODO: extract 16 layers of activations and return weighted L1-loss.\n        loss = torch.tensor(0.).to(self.device)\n        for name, module in self.vgg_features._modules.items():\n            input = module(input)\n            target = module(target)\n            if name in self.layer_name_mapping:\n                loss += self.loss_function(input, target)\n        return 0.1 * loss  # recon loss should be on the same level as gen loss...\n\n\n#\n# if __name__ == \'__main__\':\n# \t# from torchvision.models import vgg16\n# \t# vggnet = vgg16()\n# \t# vggnet.load_state_dict(torch.load(\'vgg16-397923af.pth\'))\n# \t# torch.save(vggnet.features.state_dict(), \'vgg16_pretrained_features.pth\')\n# \tvg = VGG_perceptual_loss(pretrained=True)\n# \tprint(vg)'"
face_enhancer/utils/show_skeleton_on_RGB.py,0,"b""import imageio\nfrom scipy.io import loadmat\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntestVideo = 'D:/data/ntu_processed_256/S001C001P001R001A001_rgb.avi'\ntestSkeleton = 'D:/data/ntu_processed_256/S001C001P001R001A001.skeleton'\noutputVideo = 'D:/data/out_256.avi'\n\ndef render_frame(frame, skeleton):\n\tconnecting_joint = np.array(\n\t\t[2, 1, 21, 3, 21, 5, 6, 7, 21, 9, 10, 11, 1, 13, 14, 15, 1, 17, 18, 19, 2, 8, 8, 12, 12]) - 1\n\n\tfor i in range(25):\n\t\tdx = int(skeleton[i, 0])\n\t\tdy = int(skeleton[i, 1])\n\n\t\t# if skeletons[n][i, 2] == 0:\n\t\t#\tcontinue\n\n\t\trv = 255\n\t\tgv = 0\n\t\tbv = 0\n\n\t\tk = connecting_joint[i]\n\n\t\tdx2 = int(skeleton[k, 0])\n\t\tdy2 = int(skeleton[k, 1])\n\n\t\txdist = abs(dx - dx2)\n\t\tydist = abs(dy - dy2)\n\n\t\tif xdist > ydist:\n\t\t\txrange = np.linspace(dx, dx2, xdist, endpoint=False)\n\t\t\tyrange = np.linspace(dy, dy2, xdist, endpoint=False)\n\t\telse:\n\t\t\tyrange = np.linspace(dy, dy2, ydist, endpoint=False)\n\t\t\txrange = np.linspace(dx, dx2, ydist, endpoint=False)\n\n\t\tfor i in range(len(xrange)):\n\t\t\tdx = int(round(xrange[i]))\n\t\t\tdy = int(round(yrange[i]))\n\t\t\tframe[dy - 1: dy + 1, dx - 1: dx + 1, 0] = rv\n\t\t\tframe[dy - 1: dy + 1, dx - 1: dx + 1, 1] = gv\n\t\t\tframe[dy - 1: dy + 1, dx - 1: dx + 1, 2] = bv\n\n\t\trv = 0\n\t\tgv = 255\n\t\tbv = 0\n\t\tframe[dy - 2: dy + 2, dx - 2: dx + 2, 0] = rv\n\t\tframe[dy - 2: dy + 2, dx - 2: dx + 2, 1] = gv\n\t\tframe[dy - 2: dy + 2, dx - 2: dx + 2, 2] = bv\n\n\treturn frame\n\n\nif __name__ == '__main__':\n\tvideoreader = imageio.get_reader(testVideo)\n\tfps = videoreader._meta['fps']\n\tframes = [im for im in videoreader]\n\tskeletons = loadmat(testSkeleton)\n\tskeletons = skeletons['joint_coordinates']\n\tassert (len(frames) == skeletons.shape[0])\n\n\tvideowriter = imageio.get_writer(outputVideo, fps=fps)\n\n\tfor n in range(len(frames)):\n\t\tvideowriter.append_data(render_frame(frames[n], skeletons[n, ...]))\n\n\tvideowriter.close()\n"""
face_enhancer/utils/spectral_norm.py,8,"b'# https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/spectral_norm.py\n#\n\nimport torch\nfrom torch.nn.functional import normalize\nfrom torch.nn.parameter import Parameter\n\n""""""\nSpectral Normalization from https://arxiv.org/abs/1802.05957\n""""""\nclass SpectralNorm(object):\n\n    def __init__(self, name=\'weight\', n_power_iterations=1, eps=1e-12):\n        self.name = name\n        self.n_power_iterations = n_power_iterations\n        self.eps = eps\n\n    def compute_weight(self, module):\n        weight = getattr(module, self.name + \'_org\')\n        u = getattr(module, self.name + \'_u\')\n        height = weight.size(0)\n        weight_mat = weight.view(height, -1)\n        with torch.no_grad():\n            for _ in range(self.n_power_iterations):\n                # Spectral norm of weight equals to `u^T W v`, where `u` and `v`\n                # are the first left and right singular vectors.\n                # This power iteration produces approximations of `u` and `v`.\n                v = normalize(torch.matmul(weight_mat.t(), u), dim=0, eps=self.eps)\n                u = normalize(torch.matmul(weight_mat, v), dim=0, eps=self.eps)\n\n            sigma = torch.dot(u, torch.matmul(weight_mat, v))\n        weight = weight / sigma\n        return weight, u\n\n    def remove(self, module):\n        weight = module._parameters[self.name + \'_org\']\n        delattr(module, self.name)\n        delattr(module, self.name + \'_u\')\n        delattr(module, self.name + \'_org\')\n        module.register_parameter(self.name, weight)\n\n    def __call__(self, module, inputs):\n        weight, u = self.compute_weight(module)\n        setattr(module, self.name, weight)\n        with torch.no_grad():\n            getattr(module, self.name).copy_(weight)\n\n    @staticmethod\n    def apply(module, name, n_power_iterations, eps):\n        fn = SpectralNorm(name, n_power_iterations, eps)\n        weight = module._parameters[name]\n        height = weight.size(0)\n\n        u = normalize(weight.new_empty(height).normal_(0, 1), dim=0, eps=fn.eps)\n        delattr(module, fn.name)\n        module.register_parameter(fn.name + ""_org"", weight)\n        module.register_buffer(fn.name, weight)\n        module.register_buffer(fn.name + ""_u"", u)\n\n        module.register_forward_pre_hook(fn)\n        return fn\n\n\ndef spectral_norm(module, name=\'weight\', n_power_iterations=1, eps=1e-12):\n    r""""""Applies spectral normalization to a parameter in the given module.\n    .. math::\n         \\mathbf{W} &= \\dfrac{\\mathbf{W}}{\\sigma(\\mathbf{W})} \\\\\n         \\sigma(\\mathbf{W}) &= \\max_{\\mathbf{h}: \\mathbf{h} \\ne 0} \\dfrac{\\|\\mathbf{W} \\mathbf{h}\\|_2}{\\|\\mathbf{h}\\|_2}\n    Spectral normalization stabilizes the training of discriminators (critics)\n    in Generative Adversarial Networks (GANs) by rescaling the weight tensor\n    with spectral norm :math:`\\sigma` of the weight matrix calculated using\n    power iteration method. If the dimension of the weight tensor is greater\n    than 2, it is reshaped to 2D in power iteration method to get spectral\n    norm. This is implemented via a hook that calculates spectral norm and\n    rescales weight before every :meth:`~Module.forward` call.\n    See `Spectral Normalization for Generative Adversarial Networks`_ .\n    .. _`Spectral Normalization for Generative Adversarial Networks`: https://arxiv.org/abs/1802.05957\n    Args:\n        module (nn.Module): containing module\n        name (str, optional): name of weight parameter\n        n_power_iterations (int, optional): number of power iterations to\n            calculate spectral norm\n        eps (float, optional): epsilon for numerical stability in\n            calculating norms\n    Returns:\n        The original module with the spectral norm hook\n    Example::\n        >>> m = spectral_norm(nn.Linear(20, 40))\n        Linear (20 -> 40)\n        >>> m.weight_u.size()\n        torch.Size([20])\n    """"""\n    SpectralNorm.apply(module, name, n_power_iterations, eps)\n    return module\n\n\ndef remove_spectral_norm(module, name=\'weight\'):\n    r""""""Removes the spectral normalization reparameterization from a module.\n    Args:\n        module (nn.Module): containing module\n        name (str, optional): name of weight parameter\n    Example:\n        >>> m = spectral_norm(nn.Linear(40, 10))\n        >>> remove_spectral_norm(m)\n    """"""\n    for k, hook in module._forward_pre_hooks.items():\n        if isinstance(hook, SpectralNorm) and hook.name == name:\n            hook.remove(module)\n            del module._forward_pre_hooks[k]\n            return module\n\n    raise ValueError(""spectral_norm of \'{}\' not found in {}"".format(\n        name, module))'"
src/PoseEstimation/train_SH.py,9,"b'import argparse\nimport time\nimport os\nimport numpy as np\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n#import encoding\nfrom network.rtpose_vgg import get_model, use_vgg\nfrom network import rtpose_shufflenetV2\nimport network.rtpose_hourglass as hourglass\nfrom training.datasets.coco import get_loader\n\n# Hyper-params\nparser = argparse.ArgumentParser(description=\'PyTorch rtpose Training\')\nparser.add_argument(\'--data_dir\', default=\'/data/coco/images\', type=str, metavar=\'DIR\',\n                    help=\'path to where coco images stored\') \nparser.add_argument(\'--mask_dir\', default=\'/data/coco/\', type=str, metavar=\'DIR\',\n                    help=\'path to where coco images stored\')    \nparser.add_argument(\'--logdir\', default=\'/extra/tensorboy\', type=str, metavar=\'DIR\',\n                    help=\'path to where tensorboard log restore\')                                       \nparser.add_argument(\'--json_path\', default=\'/data/coco/COCO.json\', type=str, metavar=\'PATH\',\n                    help=\'path to where coco images stored\')                                      \n\nparser.add_argument(\'--model_path\', default=\'./network/weight/\', type=str, metavar=\'DIR\',\n                    help=\'path to where the model saved\') \n                    \nparser.add_argument(\'--lr\', \'--learning-rate\', default=1., type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\n\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\n \nparser.add_argument(\'--epochs\', default=200, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\n                    \nparser.add_argument(\'--weight-decay\', \'--wd\', default=0.000, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')  \nparser.add_argument(\'--nesterov\', dest=\'nesterov\', action=\'store_true\')     \n                                                   \nparser.add_argument(\'-o\', \'--optim\', default=\'sgd\', type=str)\n#Device options\nparser.add_argument(\'--gpu_ids\', dest=\'gpu_ids\', help=\'which gpu to use\', nargs=""+"",\n                    default=[0,1,2,3], type=int)\n                    \nparser.add_argument(\'-b\', \'--batch_size\', default=80, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 256)\')\n\nparser.add_argument(\'--print_freq\', default=20, type=int, metavar=\'N\',\n                    help=\'number of iterations to print the training statistics\')\nfrom tensorboardX import SummaryWriter      \nargs = parser.parse_args()  \n               \nos.environ[\'CUDA_VISIBLE_DEVICES\'] = \',\'.join(str(e) for e in args.gpu_ids)\n\nparams_transform = dict()\nparams_transform[\'mode\'] = 5\n# === aug_scale ===\nparams_transform[\'scale_min\'] = 0.5\nparams_transform[\'scale_max\'] = 1.1\nparams_transform[\'scale_prob\'] = 1\nparams_transform[\'target_dist\'] = 0.6\n# === aug_rotate ===\nparams_transform[\'max_rotate_degree\'] = 40\n\n# ===\nparams_transform[\'center_perterb_max\'] = 40\n\n# === aug_flip ===\nparams_transform[\'flip_prob\'] = 0.5\n\nparams_transform[\'np\'] = 56\nparams_transform[\'sigma\'] = 4.416\nparams_transform[\'limb_width\'] = 1.289\n\n\ndef get_loss(saved_for_loss, heat_temp, heat_weight,\n               vec_temp, vec_weight):\n\n    saved_for_log = OrderedDict()\n    criterion = nn.MSELoss(size_average=False).cuda()\n    batch_size = heat_temp.size(0)\n    \n    total_loss = 0\n\n    pred1 = saved_for_loss[0] * vec_weight\n    """"""\n    print(""pred1 sizes"")\n    print(saved_for_loss[2*j].data.size())\n    print(vec_weight.data.size())\n    print(vec_temp.data.size())\n    """"""\n    gt1 = vec_temp * vec_weight\n\n    pred2 = saved_for_loss[1] * heat_weight\n    gt2 = heat_weight * heat_temp\n    """"""\n    print(""pred2 sizes"")\n    print(saved_for_loss[2*j+1].data.size())\n    print(heat_weight.data.size())\n    print(heat_temp.data.size())\n    """"""\n\n    # Compute losses\n    loss1 = criterion(pred1, gt1)/(2*batch_size)\n    loss2 = criterion(pred2, gt2)/(2*batch_size)\n\n    total_loss += loss1\n    total_loss += loss2\n    # print(total_loss)\n\n    # Get value from Variable and save for log\n    saved_for_log[\'paf\'] = loss1.item()\n    saved_for_log[\'heatmap\'] = loss2.item()\n\n    saved_for_log[\'max_ht\'] = torch.max(\n        saved_for_loss[-1].data[:, 0:-1, :, :]).item()\n    saved_for_log[\'min_ht\'] = torch.min(\n        saved_for_loss[-1].data[:, 0:-1, :, :]).item()\n    saved_for_log[\'max_paf\'] = torch.max(saved_for_loss[-2].data).item()\n    saved_for_log[\'min_paf\'] = torch.min(saved_for_loss[-2].data).item()\n\n    return total_loss, saved_for_log\n         \n\ndef train(train_loader, model, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    \n    meter_dict = {}\n\n    meter_dict[\'paf\'] = AverageMeter()\n    meter_dict[\'heatmap\'] = AverageMeter()     \n    meter_dict[\'max_ht\'] = AverageMeter()\n    meter_dict[\'min_ht\'] = AverageMeter()    \n    meter_dict[\'max_paf\'] = AverageMeter()    \n    meter_dict[\'min_paf\'] = AverageMeter()\n    \n    \n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (img, heatmap_target, heat_mask, paf_target, paf_mask) in enumerate(train_loader):\n        # measure data loading time\n        #writer.add_text(\'Text\', \'text logged at step:\' + str(i), i)\n        \n        #for name, param in model.named_parameters():\n        #    writer.add_histogram(name, param.clone().cpu().data.numpy(),i)        \n        data_time.update(time.time() - end)\n\n        img = img.cuda()\n        heatmap_target = heatmap_target.cuda()\n        heat_mask = heat_mask.cuda()\n        paf_target = paf_target.cuda()\n        paf_mask = paf_mask.cuda()\n        \n        # compute output\n        _,saved_for_loss = model(img)\n        \n        total_loss, saved_for_log = get_loss(saved_for_loss, heatmap_target, heat_mask,\n               paf_target, paf_mask)\n        \n        for name,_ in meter_dict.items():\n            meter_dict[name].update(saved_for_log[name], img.size(0))\n        losses.update(total_loss, img.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        total_loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if i % args.print_freq == 0:\n            print_string = \'Epoch: [{0}][{1}/{2}]\\t\'.format(epoch, i, len(train_loader))\n            print_string +=\'Data time {data_time.val:.3f} ({data_time.avg:.3f})\\t\'.format( data_time=data_time)\n            print_string += \'Loss {loss.val:.4f} ({loss.avg:.4f})\'.format(loss=losses)\n\n            for name, value in meter_dict.items():\n                print_string+=\'{name}: {loss.val:.4f} ({loss.avg:.4f})\\t\'.format(name=name, loss=value)\n            print(print_string)\n    return losses.avg  \n        \ndef validate(val_loader, model, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    \n    meter_dict = {}\n    meter_dict[\'paf\'] = AverageMeter()\n    meter_dict[\'heatmap\'] = AverageMeter()    \n    meter_dict[\'max_ht\'] = AverageMeter()\n    meter_dict[\'min_ht\'] = AverageMeter()    \n    meter_dict[\'max_paf\'] = AverageMeter()    \n    meter_dict[\'min_paf\'] = AverageMeter()\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (img, heatmap_target, heat_mask, paf_target, paf_mask) in enumerate(val_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        img = img.cuda()\n        heatmap_target = heatmap_target.cuda()\n        heat_mask = heat_mask.cuda()\n        paf_target = paf_target.cuda()\n        paf_mask = paf_mask.cuda()\n        \n        # compute output\n        _,saved_for_loss = model(img)\n        \n        total_loss, saved_for_log = get_loss(saved_for_loss, heatmap_target, heat_mask,\n               paf_target, paf_mask)\n               \n        #for name,_ in meter_dict.items():\n        #    meter_dict[name].update(saved_for_log[name], img.size(0))\n            \n        losses.update(total_loss.item(), img.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        total_loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()  \n        if i % args.print_freq == 0:\n            print_string = \'Epoch: [{0}][{1}/{2}]\\t\'.format(epoch, i, len(val_loader))\n            print_string +=\'Data time {data_time.val:.3f} ({data_time.avg:.3f})\\t\'.format( data_time=data_time)\n            print_string += \'Loss {loss.val:.4f} ({loss.avg:.4f})\'.format(loss=losses)\n\n            for name, value in meter_dict.items():\n                print_string+=\'{name}: {loss.val:.4f} ({loss.avg:.4f})\\t\'.format(name=name, loss=value)\n            print(print_string)\n                \n    return losses.avg\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nprint(""Loading dataset..."")\n# load data\ntrain_data = get_loader(args.json_path, args.data_dir,\n                        args.mask_dir, 256, 4,\n                        \'rtpose\', args.batch_size,\n                        shuffle=True, params_transform=params_transform, training=True, num_workers=16)\nprint(\'train dataset len: {}\'.format(len(train_data.dataset)))\n\n# validation data\nvalid_data = get_loader(args.json_path, args.data_dir, args.mask_dir, 256,\n                            4, preprocess=\'rtpose\', params_transform=params_transform, training=False,\n                            batch_size=args.batch_size, shuffle=False, num_workers=4)\nprint(\'val dataset len: {}\'.format(len(valid_data.dataset)))\n\n# model\nmodel = hourglass.hg(num_stacks=8, num_blocks=1, paf_classes=38, ht_classes=19)\n#model = encoding.nn.DataParallelModel(model, device_ids=args.gpu_ids)\nmodel = torch.nn.DataParallel(model).cuda()\n \nwriter = SummaryWriter(log_dir=args.logdir)                                                      \n\ntrainable_vars = [param for param in model.parameters() if param.requires_grad]\noptimizer = torch.optim.SGD(trainable_vars, lr=args.lr,\n                           momentum=args.momentum,\n                           weight_decay=args.weight_decay,\n                           nesterov=args.nesterov)          \n                                                    \nlr_scheduler = ReduceLROnPlateau(optimizer, mode=\'min\', factor=0.8, patience=5, verbose=True, threshold=0.0001, threshold_mode=\'rel\', cooldown=3, min_lr=0, eps=1e-08)\n\nbest_val_loss = np.inf\n\n\nmodel_save_filename = \'./network/weight/best_pose_SH.pth\'\nfor epoch in range(args.epochs):\n\n    # train for one epoch\n    train_loss = train(train_data, model, optimizer, epoch)\n\n    # evaluate on validation set\n    val_loss = validate(valid_data, model, epoch)   \n    \n    writer.add_scalars(\'data/scalar_group\', {\'train loss\': train_loss,\n                                             \'val loss\': val_loss}, epoch)\n    lr_scheduler.step(val_loss)                        \n    \n    is_best = val_loss<best_val_loss\n    best_val_loss = max(val_loss, best_val_loss)\n    if is_best:\n        torch.save(model.state_dict(), model_save_filename)      \n        \nwriter.export_scalars_to_json(os.path.join(args.model_path,""tensorboard/all_scalars.json""))\nwriter.close()    \n'"
src/PoseEstimation/train_ShuffleNetV2.py,9,"b'import argparse\nimport time\nimport os\nimport numpy as np\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n#import encoding\nfrom network.rtpose_vgg import get_model, use_vgg\nfrom network import rtpose_shufflenetV2\nfrom training.datasets.coco import get_loader\n\n# Hyper-params\nparser = argparse.ArgumentParser(description=\'PyTorch rtpose Training\')\nparser.add_argument(\'--data_dir\', default=\'/data/coco/images\', type=str, metavar=\'DIR\',\n                    help=\'path to where coco images stored\') \nparser.add_argument(\'--mask_dir\', default=\'/data/coco/\', type=str, metavar=\'DIR\',\n                    help=\'path to where coco images stored\')    \nparser.add_argument(\'--logdir\', default=\'/extra/tensorboy\', type=str, metavar=\'DIR\',\n                    help=\'path to where tensorboard log restore\')                                       \nparser.add_argument(\'--json_path\', default=\'/data/coco/COCO.json\', type=str, metavar=\'PATH\',\n                    help=\'path to where coco images stored\')                                      \n\nparser.add_argument(\'--model_path\', default=\'./network/weight/\', type=str, metavar=\'DIR\',\n                    help=\'path to where the model saved\') \n                    \nparser.add_argument(\'--lr\', \'--learning-rate\', default=1., type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\n\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\n \nparser.add_argument(\'--epochs\', default=200, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\n                    \nparser.add_argument(\'--weight-decay\', \'--wd\', default=0.000, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')  \nparser.add_argument(\'--nesterov\', dest=\'nesterov\', action=\'store_true\')     \n                                                   \nparser.add_argument(\'-o\', \'--optim\', default=\'sgd\', type=str)\n#Device options\nparser.add_argument(\'--gpu_ids\', dest=\'gpu_ids\', help=\'which gpu to use\', nargs=""+"",\n                    default=[0,1,2,3], type=int)\n                    \nparser.add_argument(\'-b\', \'--batch_size\', default=80, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 256)\')\n\nparser.add_argument(\'--print_freq\', default=20, type=int, metavar=\'N\',\n                    help=\'number of iterations to print the training statistics\')\nfrom tensorboardX import SummaryWriter      \nargs = parser.parse_args()  \n               \nos.environ[\'CUDA_VISIBLE_DEVICES\'] = \',\'.join(str(e) for e in args.gpu_ids)\n\nparams_transform = dict()\nparams_transform[\'mode\'] = 5\n# === aug_scale ===\nparams_transform[\'scale_min\'] = 0.5\nparams_transform[\'scale_max\'] = 1.1\nparams_transform[\'scale_prob\'] = 1\nparams_transform[\'target_dist\'] = 0.6\n# === aug_rotate ===\nparams_transform[\'max_rotate_degree\'] = 40\n\n# ===\nparams_transform[\'center_perterb_max\'] = 40\n\n# === aug_flip ===\nparams_transform[\'flip_prob\'] = 0.5\n\nparams_transform[\'np\'] = 56\nparams_transform[\'sigma\'] = 7.0\n\ndef get_loss(saved_for_loss, heat_temp, heat_weight,\n               vec_temp, vec_weight):\n\n    saved_for_log = OrderedDict()\n    criterion = nn.MSELoss(size_average=True).cuda()\n    #criterion = encoding.nn.DataParallelCriterion(criterion, device_ids=args.gpu_ids)\n    total_loss = 0\n\n\n    pred1 = saved_for_loss[0] * vec_weight\n    """"""\n    print(""pred1 sizes"")\n    print(saved_for_loss[2*j].data.size())\n    print(vec_weight.data.size())\n    print(vec_temp.data.size())\n    """"""\n    gt1 = vec_temp * vec_weight\n\n    pred2 = saved_for_loss[1] * heat_weight\n    gt2 = heat_weight * heat_temp\n    """"""\n    print(""pred2 sizes"")\n    print(saved_for_loss[2*j+1].data.size())\n    print(heat_weight.data.size())\n    print(heat_temp.data.size())\n    """"""\n\n    # Compute losses\n    loss1 = criterion(pred1, gt1)\n    loss2 = criterion(pred2, gt2) \n\n    total_loss += loss1\n    total_loss += loss2\n    # print(total_loss)\n\n    # Get value from Variable and save for log\n    saved_for_log[\'paf\'] = loss1.item()\n    saved_for_log[\'heatmap\'] = loss2.item()\n\n    saved_for_log[\'max_ht\'] = torch.max(\n        saved_for_loss[-1].data[:, 0:-1, :, :]).item()\n    saved_for_log[\'min_ht\'] = torch.min(\n        saved_for_loss[-1].data[:, 0:-1, :, :]).item()\n    saved_for_log[\'max_paf\'] = torch.max(saved_for_loss[-2].data).item()\n    saved_for_log[\'min_paf\'] = torch.min(saved_for_loss[-2].data).item()\n\n    return total_loss, saved_for_log\n         \n\ndef train(train_loader, model, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    \n    meter_dict = {}\n\n    meter_dict[\'paf\'] = AverageMeter()\n    meter_dict[\'heatmap\'] = AverageMeter()     \n    meter_dict[\'max_ht\'] = AverageMeter()\n    meter_dict[\'min_ht\'] = AverageMeter()    \n    meter_dict[\'max_paf\'] = AverageMeter()    \n    meter_dict[\'min_paf\'] = AverageMeter()\n    \n    \n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (img, heatmap_target, heat_mask, paf_target, paf_mask) in enumerate(train_loader):\n        # measure data loading time\n        #writer.add_text(\'Text\', \'text logged at step:\' + str(i), i)\n        \n        #for name, param in model.named_parameters():\n        #    writer.add_histogram(name, param.clone().cpu().data.numpy(),i)        \n        data_time.update(time.time() - end)\n\n        img = img.cuda()\n        heatmap_target = heatmap_target.cuda()\n        heat_mask = heat_mask.cuda()\n        paf_target = paf_target.cuda()\n        paf_mask = paf_mask.cuda()\n        \n        # compute output\n        _,saved_for_loss = model(img)\n        \n        total_loss, saved_for_log = get_loss(saved_for_loss, heatmap_target, heat_mask,\n               paf_target, paf_mask)\n        \n        for name,_ in meter_dict.items():\n            meter_dict[name].update(saved_for_log[name], img.size(0))\n        losses.update(total_loss, img.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        total_loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if i % args.print_freq == 0:\n            print_string = \'Epoch: [{0}][{1}/{2}]\\t\'.format(epoch, i, len(train_loader))\n            print_string +=\'Data time {data_time.val:.3f} ({data_time.avg:.3f})\\t\'.format( data_time=data_time)\n            print_string += \'Loss {loss.val:.4f} ({loss.avg:.4f})\'.format(loss=losses)\n\n            for name, value in meter_dict.items():\n                print_string+=\'{name}: {loss.val:.4f} ({loss.avg:.4f})\\t\'.format(name=name, loss=value)\n            print(print_string)\n    return losses.avg  \n        \ndef validate(val_loader, model, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    \n    meter_dict = {}\n    meter_dict[\'paf\'] = AverageMeter()\n    meter_dict[\'heatmap\'] = AverageMeter()    \n    meter_dict[\'max_ht\'] = AverageMeter()\n    meter_dict[\'min_ht\'] = AverageMeter()    \n    meter_dict[\'max_paf\'] = AverageMeter()    \n    meter_dict[\'min_paf\'] = AverageMeter()\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (img, heatmap_target, heat_mask, paf_target, paf_mask) in enumerate(val_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        img = img.cuda()\n        heatmap_target = heatmap_target.cuda()\n        heat_mask = heat_mask.cuda()\n        paf_target = paf_target.cuda()\n        paf_mask = paf_mask.cuda()\n        \n        # compute output\n        _,saved_for_loss = model(img)\n        \n        total_loss, saved_for_log = get_loss(saved_for_loss, heatmap_target, heat_mask,\n               paf_target, paf_mask)\n               \n        #for name,_ in meter_dict.items():\n        #    meter_dict[name].update(saved_for_log[name], img.size(0))\n            \n        losses.update(total_loss.item(), img.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        total_loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()  \n        if i % args.print_freq == 0:\n            print_string = \'Epoch: [{0}][{1}/{2}]\\t\'.format(epoch, i, len(val_loader))\n            print_string +=\'Data time {data_time.val:.3f} ({data_time.avg:.3f})\\t\'.format( data_time=data_time)\n            print_string += \'Loss {loss.val:.4f} ({loss.avg:.4f})\'.format(loss=losses)\n\n            for name, value in meter_dict.items():\n                print_string+=\'{name}: {loss.val:.4f} ({loss.avg:.4f})\\t\'.format(name=name, loss=value)\n            print(print_string)\n                \n    return losses.avg\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nprint(""Loading dataset..."")\n# load data\ntrain_data = get_loader(args.json_path, args.data_dir,\n                        args.mask_dir, 368, 8,\n                        \'rtpose\', args.batch_size, params_transform = params_transform, \n                        shuffle=True, training=True, num_workers=16)\nprint(\'train dataset len: {}\'.format(len(train_data.dataset)))\n\n# validation data\nvalid_data = get_loader(args.json_path, args.data_dir, args.mask_dir, 368,\n                            8, preprocess=\'rtpose\', training=False,\n                            batch_size=args.batch_size,  params_transform = params_transform, \n                            shuffle=False, num_workers=4)\nprint(\'val dataset len: {}\'.format(len(valid_data.dataset)))\n\n# model\nmodel = rtpose_shufflenetV2.Network(width_multiplier=1.0)\n#model = encoding.nn.DataParallelModel(model, device_ids=args.gpu_ids)\nmodel = torch.nn.DataParallel(model).cuda()\n\n \nwriter = SummaryWriter(log_dir=args.logdir)       \n                                                                                          \n\ntrainable_vars = [param for param in model.parameters() if param.requires_grad]\noptimizer = torch.optim.SGD(trainable_vars, lr=args.lr,\n                           momentum=args.momentum,\n                           weight_decay=args.weight_decay,\n                           nesterov=args.nesterov)          \n                                                    \nlr_scheduler = ReduceLROnPlateau(optimizer, mode=\'min\', factor=0.8, patience=5, verbose=True, threshold=0.0001, threshold_mode=\'rel\', cooldown=3, min_lr=0, eps=1e-08)\n\nbest_val_loss = np.inf\n\n\nmodel_save_filename = \'./network/weight/best_pose_ShuffleNetV2.pth\'\nfor epoch in range(args.epochs):\n\n    # train for one epoch\n    train_loss = train(train_data, model, optimizer, epoch)\n\n    # evaluate on validation set\n    val_loss = validate(valid_data, model, epoch)   \n    \n    writer.add_scalars(\'data/scalar_group\', {\'train loss\': train_loss,\n                                             \'val loss\': val_loss}, epoch)\n    lr_scheduler.step(val_loss)                        \n    \n    is_best = val_loss<best_val_loss\n    best_val_loss = max(val_loss, best_val_loss)\n    if is_best:\n        torch.save(model.state_dict(), model_save_filename)      \n        \nwriter.export_scalars_to_json(os.path.join(args.model_path,""tensorboard/all_scalars.json""))\nwriter.close()    \n'"
src/PoseEstimation/train_VGG19.py,10,"b'import argparse\nimport time\nimport os\nimport numpy as np\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n#import encoding\nfrom network.rtpose_vgg import get_model, use_vgg\nfrom training.datasets.coco import get_loader\n\n# Hyper-params\nparser = argparse.ArgumentParser(description=\'PyTorch rtpose Training\')\nparser.add_argument(\'--data_dir\', default=\'/data/coco/images\', type=str, metavar=\'DIR\',\n                    help=\'path to where coco images stored\') \nparser.add_argument(\'--mask_dir\', default=\'/data/coco/\', type=str, metavar=\'DIR\',\n                    help=\'path to where coco images stored\')    \nparser.add_argument(\'--logdir\', default=\'/extra/tensorboy\', type=str, metavar=\'DIR\',\n                    help=\'path to where tensorboard log restore\')                                       \nparser.add_argument(\'--json_path\', default=\'/data/coco/COCO.json\', type=str, metavar=\'PATH\',\n                    help=\'path to where coco images stored\')                                      \n\nparser.add_argument(\'--model_path\', default=\'./network/weight/\', type=str, metavar=\'DIR\',\n                    help=\'path to where the model saved\') \n                    \nparser.add_argument(\'--lr\', \'--learning-rate\', default=1., type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\n\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\n \nparser.add_argument(\'--epochs\', default=200, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\n                    \nparser.add_argument(\'--weight-decay\', \'--wd\', default=0.000, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')  \nparser.add_argument(\'--nesterov\', dest=\'nesterov\', action=\'store_true\')     \n                                                   \nparser.add_argument(\'-o\', \'--optim\', default=\'sgd\', type=str)\n#Device options\nparser.add_argument(\'--gpu_ids\', dest=\'gpu_ids\', help=\'which gpu to use\', nargs=""+"",\n                    default=[0,1,2,3], type=int)\n                    \nparser.add_argument(\'-b\', \'--batch_size\', default=80, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 256)\')\n\nparser.add_argument(\'--print_freq\', default=20, type=int, metavar=\'N\',\n                    help=\'number of iterations to print the training statistics\')\nfrom tensorboardX import SummaryWriter      \nargs = parser.parse_args()  \n               \nos.environ[\'CUDA_VISIBLE_DEVICES\'] = \',\'.join(str(e) for e in args.gpu_ids)\n\nparams_transform = dict()\nparams_transform[\'mode\'] = 5\n# === aug_scale ===\nparams_transform[\'scale_min\'] = 0.5\nparams_transform[\'scale_max\'] = 1.1\nparams_transform[\'scale_prob\'] = 1\nparams_transform[\'target_dist\'] = 0.6\n# === aug_rotate ===\nparams_transform[\'max_rotate_degree\'] = 40\n\n# ===\nparams_transform[\'center_perterb_max\'] = 40\n\n# === aug_flip ===\nparams_transform[\'flip_prob\'] = 0.5\n\nparams_transform[\'np\'] = 56\nparams_transform[\'sigma\'] = 7.0\nparams_transform[\'limb_width\'] = 1.\n\ndef build_names():\n    names = []\n\n    for j in range(1, 7):\n        for k in range(1, 3):\n            names.append(\'loss_stage%d_L%d\' % (j, k))\n    return names\n\n\ndef get_loss(saved_for_loss, heat_temp, heat_weight,\n               vec_temp, vec_weight):\n\n    names = build_names()\n    saved_for_log = OrderedDict()\n    criterion = nn.MSELoss(size_average=True).cuda()\n    #criterion = encoding.nn.DataParallelCriterion(criterion, device_ids=args.gpu_ids)\n    total_loss = 0\n\n    for j in range(6):\n        pred1 = saved_for_loss[2 * j] * vec_weight\n        """"""\n        print(""pred1 sizes"")\n        print(saved_for_loss[2*j].data.size())\n        print(vec_weight.data.size())\n        print(vec_temp.data.size())\n        """"""\n        gt1 = vec_temp * vec_weight\n\n        pred2 = saved_for_loss[2 * j + 1] * heat_weight\n        gt2 = heat_weight * heat_temp\n        """"""\n        print(""pred2 sizes"")\n        print(saved_for_loss[2*j+1].data.size())\n        print(heat_weight.data.size())\n        print(heat_temp.data.size())\n        """"""\n\n        # Compute losses\n        loss1 = criterion(pred1, gt1)\n        loss2 = criterion(pred2, gt2) \n\n        total_loss += loss1\n        total_loss += loss2\n        # print(total_loss)\n\n        # Get value from Variable and save for log\n        saved_for_log[names[2 * j]] = loss1.item()\n        saved_for_log[names[2 * j + 1]] = loss2.item()\n\n    saved_for_log[\'max_ht\'] = torch.max(\n        saved_for_loss[-1].data[:, 0:-1, :, :]).item()\n    saved_for_log[\'min_ht\'] = torch.min(\n        saved_for_loss[-1].data[:, 0:-1, :, :]).item()\n    saved_for_log[\'max_paf\'] = torch.max(saved_for_loss[-2].data).item()\n    saved_for_log[\'min_paf\'] = torch.min(saved_for_loss[-2].data).item()\n\n    return total_loss, saved_for_log\n         \n\ndef train(train_loader, model, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    \n    meter_dict = {}\n    for name in build_names():\n        meter_dict[name] = AverageMeter()\n    meter_dict[\'max_ht\'] = AverageMeter()\n    meter_dict[\'min_ht\'] = AverageMeter()    \n    meter_dict[\'max_paf\'] = AverageMeter()    \n    meter_dict[\'min_paf\'] = AverageMeter()\n    \n    \n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (img, heatmap_target, heat_mask, paf_target, paf_mask) in enumerate(train_loader):\n        # measure data loading time\n        #writer.add_text(\'Text\', \'text logged at step:\' + str(i), i)\n        \n        #for name, param in model.named_parameters():\n        #    writer.add_histogram(name, param.clone().cpu().data.numpy(),i)        \n        data_time.update(time.time() - end)\n\n        img = img.cuda()\n        heatmap_target = heatmap_target.cuda()\n        heat_mask = heat_mask.cuda()\n        paf_target = paf_target.cuda()\n        paf_mask = paf_mask.cuda()\n        \n        # compute output\n        _,saved_for_loss = model(img)\n        \n        total_loss, saved_for_log = get_loss(saved_for_loss, heatmap_target, heat_mask,\n               paf_target, paf_mask)\n        \n        for name,_ in meter_dict.items():\n            meter_dict[name].update(saved_for_log[name], img.size(0))\n        losses.update(total_loss, img.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        total_loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if i % args.print_freq == 0:\n            print_string = \'Epoch: [{0}][{1}/{2}]\\t\'.format(epoch, i, len(train_loader))\n            print_string +=\'Data time {data_time.val:.3f} ({data_time.avg:.3f})\\t\'.format( data_time=data_time)\n            print_string += \'Loss {loss.val:.4f} ({loss.avg:.4f})\'.format(loss=losses)\n\n            for name, value in meter_dict.items():\n                print_string+=\'{name}: {loss.val:.4f} ({loss.avg:.4f})\\t\'.format(name=name, loss=value)\n            print(print_string)\n    return losses.avg  \n        \ndef validate(val_loader, model, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    \n    meter_dict = {}\n    for name in build_names():\n        meter_dict[name] = AverageMeter()\n    meter_dict[\'max_ht\'] = AverageMeter()\n    meter_dict[\'min_ht\'] = AverageMeter()    \n    meter_dict[\'max_paf\'] = AverageMeter()    \n    meter_dict[\'min_paf\'] = AverageMeter()\n    # switch to train mode\n    model.eval()\n\n    end = time.time()\n    for i, (img, heatmap_target, heat_mask, paf_target, paf_mask) in enumerate(val_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        img = img.cuda()\n        heatmap_target = heatmap_target.cuda()\n        heat_mask = heat_mask.cuda()\n        paf_target = paf_target.cuda()\n        paf_mask = paf_mask.cuda()\n        \n        # compute output\n        _,saved_for_loss = model(img)\n        \n        total_loss, saved_for_log = get_loss(saved_for_loss, heatmap_target, heat_mask,\n               paf_target, paf_mask)\n               \n        #for name,_ in meter_dict.items():\n        #    meter_dict[name].update(saved_for_log[name], img.size(0))\n            \n        losses.update(total_loss.item(), img.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()  \n        if i % args.print_freq == 0:\n            print_string = \'Epoch: [{0}][{1}/{2}]\\t\'.format(epoch, i, len(val_loader))\n            print_string +=\'Data time {data_time.val:.3f} ({data_time.avg:.3f})\\t\'.format( data_time=data_time)\n            print_string += \'Loss {loss.val:.4f} ({loss.avg:.4f})\'.format(loss=losses)\n\n            for name, value in meter_dict.items():\n                print_string+=\'{name}: {loss.val:.4f} ({loss.avg:.4f})\\t\'.format(name=name, loss=value)\n            print(print_string)\n                \n    return losses.avg\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nprint(""Loading dataset..."")\n# load data\ntrain_data = get_loader(args.json_path, args.data_dir,\n                        args.mask_dir, 368, 8,\n                        \'vgg\', args.batch_size, params_transform = params_transform, \n                        shuffle=True, training=True, num_workers=8)\nprint(\'train dataset len: {}\'.format(len(train_data.dataset)))\n\n# validation data\nvalid_data = get_loader(args.json_path, args.data_dir, args.mask_dir, 368,\n                            8, preprocess=\'vgg\', training=False,\n                            batch_size=args.batch_size, params_transform = params_transform, shuffle=False, num_workers=4)\nprint(\'val dataset len: {}\'.format(len(valid_data.dataset)))\n\n# model\nmodel = get_model(trunk=\'vgg19\')\n#model = encoding.nn.DataParallelModel(model, device_ids=args.gpu_ids)\nmodel = torch.nn.DataParallel(model).cuda()\n# load pretrained\nuse_vgg(model, args.model_path, \'vgg19\')\n\n\n# Fix the VGG weights first, and then the weights will be released\nfor i in range(20):\n    for param in model.module.model0[i].parameters():\n        param.requires_grad = False\n\ntrainable_vars = [param for param in model.parameters() if param.requires_grad]\noptimizer = torch.optim.SGD(trainable_vars, lr=args.lr,\n                           momentum=args.momentum,\n                           weight_decay=args.weight_decay,\n                           nesterov=args.nesterov)\n \nwriter = SummaryWriter(log_dir=args.logdir)       \n                                                                                          \nfor epoch in range(5):\n    # train for one epoch\n    train_loss = train(train_data, model, optimizer, epoch)\n\n    # evaluate on validation set\n    val_loss = validate(valid_data, model, epoch)  \n                                 \n    writer.add_scalars(\'data/scalar_group\', {\'train loss\': train_loss,\n                                             \'val loss\': val_loss}, epoch)            \n# Release all weights                                   \nfor param in model.module.parameters():\n    param.requires_grad = True\n\ntrainable_vars = [param for param in model.parameters() if param.requires_grad]\noptimizer = torch.optim.SGD(trainable_vars, lr=args.lr,\n                           momentum=args.momentum,\n                           weight_decay=args.weight_decay,\n                           nesterov=args.nesterov)          \n                                                    \nlr_scheduler = ReduceLROnPlateau(optimizer, mode=\'min\', factor=0.8, patience=5, verbose=True, threshold=0.0001, threshold_mode=\'rel\', cooldown=3, min_lr=0, eps=1e-08)\n\nbest_val_loss = np.inf\n\n\nmodel_save_filename = \'./network/weight/best_pose.pth\'\nfor epoch in range(5, args.epochs):\n\n    # train for one epoch\n    train_loss = train(train_data, model, optimizer, epoch)\n\n    # evaluate on validation set\n    val_loss = validate(valid_data, model, epoch)   \n    \n    writer.add_scalars(\'data/scalar_group\', {\'train loss\': train_loss,\n                                             \'val loss\': val_loss}, epoch)\n    lr_scheduler.step(val_loss)                        \n    \n    is_best = val_loss<best_val_loss\n    best_val_loss = min(val_loss, best_val_loss)\n    if is_best:\n        torch.save(model.state_dict(), model_save_filename)      \n        \nwriter.export_scalars_to_json(os.path.join(args.model_path,""tensorboard/all_scalars.json""))\nwriter.close()    \n'"
src/config/test_opt.py,0,"b""aspect_ratio=1.0\nbatchSize=1\ncheckpoints_dir='./checkpoints/'\ncluster_path='features_clustered_010.npy'\ndata_type=32\ndataroot='./data/source/'\ndisplay_winsize=512\nengine=None\nexport_onnx=None\nfeat_num=3\nfineSize=512\nfine_size=480\nhow_many=50\ninput_nc=3\ninstance_feat=False\nisTrain=False\nlabel_feat=False\nlabel_nc=18\nloadSize=512\nload_features=False\nmax_dataset_size=100000\nmodel='pix2pixHD'\nnThreads=1\nn_blocks_global=9\nn_blocks_local=3\nn_clusters=10\nn_downsample_E=4\nn_downsample_global=4\nn_local_enhancers=1\nname='target'\nnef=16\nnetG='global'\nngf=64\nniter_fix_global=0\nno_flip=True\nno_instance=True\nnorm='instance'\nntest=100000\nonnx=None,\noutput_nc=3\nphase='test'\nresize_or_crop='scale_width'\nresults_dir='./results/'\nserial_batches=True\ntf_log=False\nuse_dropout=False\nverbose=False\nwhich_epoch='latest'\ngpu_ids = [0]"""
src/config/train_opt.py,0,"b""batchSize=1\nbeta1=0.5\ncheckpoints_dir='./checkpoints/'\ncontinue_train=False\ndata_type=32\ndataroot='./data/target/train'\n# load_pretrain = './checkpoints/target/' # use this if you want to continue last training\ndebug=False\ndisplay_freq=640\ndisplay_winsize=512\nfeat_num=3\nfineSize=512\nfine_size=480\ninput_nc=3\ninstance_feat=False\nisTrain=True\nlabel_feat=False\nlabel_nc=18\nlambda_feat=10.0\nloadSize=512\nload_features=False\nload_pretrain=''\nlr=0.0002\nmax_dataset_size=100000\nmodel='pix2pixHD'\nnThreads=2\nn_blocks_global=9\nn_blocks_local=3\nn_clusters=10\nn_downsample_E=4\nn_downsample_global=4\nn_layers_D=3\nn_local_enhancers=1\nname='target'\nndf=64\nnef=16\nnetG='global'\nngf=64\nniter=20\nniter_decay=20\nniter_fix_global=0\nno_flip=False\nno_ganFeat_loss=False\nno_html=False\nno_instance=True\nno_lsgan=False\nno_vgg_loss=False\nnorm='instance'\nnum_D=2\noutput_nc=3\nphase='train'\npool_size=0\nprint_freq=640\nresize_or_crop='scale_width'\nsave_epoch_freq=10\nsave_latest_freq=640\nserial_batches=False\ntf_log=True\nuse_dropout=False\nverbose=False\nwhich_epoch='latest'\ngpu_ids=[0]"""
src/pix2pixHD/encode_features.py,0,"b""### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nfrom options.train_options import TrainOptions\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nimport numpy as np\nimport os\n\nopt = TrainOptions().parse()\nopt.nThreads = 1\nopt.batchSize = 1 \nopt.serial_batches = True \nopt.no_flip = True\nopt.instance_feat = True\n\nname = 'features'\nsave_path = os.path.join(opt.checkpoints_dir, opt.name)\n\n############ Initialize #########\ndata_loader = CreateDataLoader(opt)\ndataset = data_loader.load_data()\ndataset_size = len(data_loader)\nmodel = create_model(opt)\n\n########### Encode features ###########\nreencode = True\nif reencode:\n\tfeatures = {}\n\tfor label in range(opt.label_nc):\n\t\tfeatures[label] = np.zeros((0, opt.feat_num+1))\n\tfor i, data in enumerate(dataset):    \n\t    feat = model.module.encode_features(data['image'], data['inst'])\n\t    for label in range(opt.label_nc):\n\t    \tfeatures[label] = np.append(features[label], feat[label], axis=0) \n\t        \n\t    print('%d / %d images' % (i+1, dataset_size))    \n\tsave_name = os.path.join(save_path, name + '.npy')\n\tnp.save(save_name, features)\n\n############## Clustering ###########\nn_clusters = opt.n_clusters\nload_name = os.path.join(save_path, name + '.npy')\nfeatures = np.load(load_name).item()\nfrom sklearn.cluster import KMeans\ncenters = {}\nfor label in range(opt.label_nc):\n\tfeat = features[label]\n\tfeat = feat[feat[:,-1] > 0.5, :-1]\t\t\n\tif feat.shape[0]:\n\t\tn_clusters = min(feat.shape[0], opt.n_clusters)\n\t\tkmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(feat)\n\t\tcenters[label] = kmeans.cluster_centers_\nsave_name = os.path.join(save_path, name + '_clustered_%03d.npy' % opt.n_clusters)\nnp.save(save_name, centers)\nprint('saving to %s' % save_name)"""
src/pix2pixHD/precompute_feature_maps.py,2,"b""### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nfrom options.train_options import TrainOptions\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nimport os\nimport util.util as util\nfrom torch.autograd import Variable\nimport torch.nn as nn\n\nopt = TrainOptions().parse()\nopt.nThreads = 1\nopt.batchSize = 1 \nopt.serial_batches = True \nopt.no_flip = True\nopt.instance_feat = True\n\nname = 'features'\nsave_path = os.path.join(opt.checkpoints_dir, opt.name)\n\n############ Initialize #########\ndata_loader = CreateDataLoader(opt)\ndataset = data_loader.load_data()\ndataset_size = len(data_loader)\nmodel = create_model(opt)\nutil.mkdirs(os.path.join(opt.dataroot, opt.phase + '_feat'))\n\n######## Save precomputed feature maps for 1024p training #######\nfor i, data in enumerate(dataset):\n\tprint('%d / %d images' % (i+1, dataset_size)) \n\tfeat_map = model.module.netE.forward(Variable(data['image'].cuda(), volatile=True), data['inst'].cuda())\n\tfeat_map = nn.Upsample(scale_factor=2, mode='nearest')(feat_map)\n\timage_numpy = util.tensor2im(feat_map.data[0])\n\tsave_path = data['path'][0].replace('/train_label/', '/train_feat/')\n\tutil.save_image(image_numpy, save_path)"""
src/pix2pixHD/run_engine.py,0,"b'import os\nimport sys\nfrom random import randint\nimport numpy as np\nimport tensorrt\n\ntry:\n    from PIL import Image\n    import pycuda.driver as cuda\n    import pycuda.gpuarray as gpuarray\n    import pycuda.autoinit\n    import argparse\nexcept ImportError as err:\n    sys.stderr.write(""""""ERROR: failed to import module ({})\nPlease make sure you have pycuda and the example dependencies installed.\nhttps://wiki.tiker.net/PyCuda/Installation/Linux\npip(3) install tensorrt[examples]\n"""""".format(err))\n    exit(1)\n\ntry:\n    import tensorrt as trt\n    from tensorrt.parsers import caffeparser\n    from tensorrt.parsers import onnxparser    \nexcept ImportError as err:\n    sys.stderr.write(""""""ERROR: failed to import module ({})\nPlease make sure you have the TensorRT Library installed\nand accessible in your LD_LIBRARY_PATH\n"""""".format(err))\n    exit(1)\n\n\nG_LOGGER = trt.infer.ConsoleLogger(trt.infer.LogSeverity.INFO)\n\nclass Profiler(trt.infer.Profiler):\n    """"""\n    Example Implimentation of a Profiler\n    Is identical to the Profiler class in trt.infer so it is possible\n    to just use that instead of implementing this if further\n    functionality is not needed\n    """"""\n    def __init__(self, timing_iter):\n        trt.infer.Profiler.__init__(self)\n        self.timing_iterations = timing_iter\n        self.profile = []\n\n    def report_layer_time(self, layerName, ms):\n        record = next((r for r in self.profile if r[0] == layerName), (None, None))\n        if record == (None, None):\n            self.profile.append((layerName, ms))\n        else:\n            self.profile[self.profile.index(record)] = (record[0], record[1] + ms)\n\n    def print_layer_times(self):\n        totalTime = 0\n        for i in range(len(self.profile)):\n            print(""{:40.40} {:4.3f}ms"".format(self.profile[i][0], self.profile[i][1] / self.timing_iterations))\n            totalTime += self.profile[i][1]\n        print(""Time over all layers: {:4.2f} ms per iteration"".format(totalTime / self.timing_iterations))\n\n\ndef get_input_output_names(trt_engine):\n    nbindings = trt_engine.get_nb_bindings();\n    maps = []\n\n    for b in range(0, nbindings):\n        dims = trt_engine.get_binding_dimensions(b).to_DimsCHW()\n        name = trt_engine.get_binding_name(b)\n        type = trt_engine.get_binding_data_type(b)\n        \n        if (trt_engine.binding_is_input(b)):\n            maps.append(name)\n            print(""Found input: "", name)\n        else:\n            maps.append(name)\n            print(""Found output: "", name)\n\n        print(""shape="" + str(dims.C()) + "" , "" + str(dims.H()) + "" , "" + str(dims.W()))\n        print(""dtype="" + str(type))\n    return maps\n\ndef create_memory(engine, name,  buf, mem, batchsize, inp, inp_idx):\n    binding_idx = engine.get_binding_index(name)\n    if binding_idx == -1:\n        raise AttributeError(""Not a valid binding"")\n    print(""Binding: name={}, bindingIndex={}"".format(name, str(binding_idx)))\n    dims = engine.get_binding_dimensions(binding_idx).to_DimsCHW()\n    eltCount = dims.C() * dims.H() * dims.W() * batchsize\n\n    if engine.binding_is_input(binding_idx):\n        h_mem = inp[inp_idx]\n        inp_idx = inp_idx + 1\n    else:\n        h_mem = np.random.uniform(0.0, 255.0, eltCount).astype(np.dtype(\'f4\'))\n\n    d_mem = cuda.mem_alloc(eltCount * 4)\n    cuda.memcpy_htod(d_mem, h_mem)\n    buf.insert(binding_idx, int(d_mem))\n    mem.append(d_mem)\n    return inp_idx\n\n\n#Run inference on device\ndef time_inference(engine, batch_size, inp):\n    bindings = []\n    mem = []\n    inp_idx = 0\n    for io in get_input_output_names(engine):\n        inp_idx = create_memory(engine, io,  bindings, mem,\n                                batch_size, inp, inp_idx)\n\n    context = engine.create_execution_context()\n    g_prof = Profiler(500)\n    context.set_profiler(g_prof)\n    for i in range(iter):\n        context.execute(batch_size, bindings)\n    g_prof.print_layer_times()\n    \n    context.destroy() \n    return\n\n\ndef convert_to_datatype(v):\n    if v==8:\n        return trt.infer.DataType.INT8\n    elif v==16:\n        return trt.infer.DataType.HALF\n    elif v==32:\n        return trt.infer.DataType.FLOAT\n    else:\n        print(""ERROR: Invalid model data type bit depth: "" + str(v))\n        return trt.infer.DataType.INT8\n\ndef run_trt_engine(engine_file, bs, it):\n    engine = trt.utils.load_engine(G_LOGGER, engine_file)\n    time_inference(engine, bs, it)\n\ndef run_onnx(onnx_file, data_type, bs, inp):\n    # Create onnx_config\n    apex = onnxparser.create_onnxconfig()\n    apex.set_model_file_name(onnx_file)\n    apex.set_model_dtype(convert_to_datatype(data_type))\n\n     # create parser\n    trt_parser = onnxparser.create_onnxparser(apex)\n    assert(trt_parser)\n    data_type = apex.get_model_dtype()\n    onnx_filename = apex.get_model_file_name()\n    trt_parser.parse(onnx_filename, data_type)\n    trt_parser.report_parsing_info()\n    trt_parser.convert_to_trtnetwork()\n    trt_network = trt_parser.get_trtnetwork()\n    assert(trt_network)\n\n    # create infer builder\n    trt_builder = trt.infer.create_infer_builder(G_LOGGER)\n    trt_builder.set_max_batch_size(max_batch_size)\n    trt_builder.set_max_workspace_size(max_workspace_size)\n    \n    if (apex.get_model_dtype() == trt.infer.DataType_kHALF):\n        print(""-------------------  Running FP16 -----------------------------"")\n        trt_builder.set_half2_mode(True)\n    elif (apex.get_model_dtype() == trt.infer.DataType_kINT8): \n        print(""-------------------  Running INT8 -----------------------------"")\n        trt_builder.set_int8_mode(True)\n    else:\n        print(""-------------------  Running FP32 -----------------------------"")\n        \n    print(""----- Builder is Done -----"")\n    print(""----- Creating Engine -----"")\n    trt_engine = trt_builder.build_cuda_engine(trt_network)\n    print(""----- Engine is built -----"")\n    time_inference(engine, bs, inp)\n'"
src/pix2pixHD/test.py,3,"b'### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport os\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nfrom options.test_options import TestOptions\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nimport util.util as util\nfrom util.visualizer import Visualizer\nfrom util import html\nimport torch\n\nopt = TestOptions().parse(save=False)\nopt.nThreads = 1   # test code only supports nThreads = 1\nopt.batchSize = 1  # test code only supports batchSize = 1\nopt.serial_batches = True  # no shuffle\nopt.no_flip = True  # no flip\n\ndata_loader = CreateDataLoader(opt)\ndataset = data_loader.load_data()\nvisualizer = Visualizer(opt)\n# create website\nweb_dir = os.path.join(opt.results_dir, opt.name, \'%s_%s\' % (opt.phase, opt.which_epoch))\nwebpage = html.HTML(web_dir, \'Experiment = %s, Phase = %s, Epoch = %s\' % (opt.name, opt.phase, opt.which_epoch))\n\n# test\nif not opt.engine and not opt.onnx:\n    model = create_model(opt)\n    if opt.data_type == 16:\n        model.half()\n    elif opt.data_type == 8:\n        model.type(torch.uint8)\n            \n    if opt.verbose:\n        print(model)\nelse:\n    from run_engine import run_trt_engine, run_onnx\n    \nfor i, data in enumerate(dataset):\n    if i >= opt.how_many:\n        break\n    if opt.data_type == 16:\n        data[\'label\'] = data[\'label\'].half()\n        data[\'inst\']  = data[\'inst\'].half()\n    elif opt.data_type == 8:\n        data[\'label\'] = data[\'label\'].uint8()\n        data[\'inst\']  = data[\'inst\'].uint8()\n    if opt.export_onnx:\n        print (""Exporting to ONNX: "", opt.export_onnx)\n        assert opt.export_onnx.endswith(""onnx""), ""Export model file should end with .onnx""\n        torch.onnx.export(model, [data[\'label\'], data[\'inst\']],\n                          opt.export_onnx, verbose=True)\n        exit(0)\n    minibatch = 1 \n    if opt.engine:\n        generated = run_trt_engine(opt.engine, minibatch, [data[\'label\'], data[\'inst\']])\n    elif opt.onnx:\n        generated = run_onnx(opt.onnx, opt.data_type, minibatch, [data[\'label\'], data[\'inst\']])\n    else:        \n        generated = model.inference(data[\'label\'], data[\'inst\'])\n        \n    visuals = OrderedDict([(\'input_label\', util.tensor2label(data[\'label\'][0], opt.label_nc)),\n                           (\'synthesized_image\', util.tensor2im(generated.data[0]))])\n    img_path = data[\'path\']\n    print(\'process image... %s\' % img_path)\n    visualizer.save_images(webpage, visuals, img_path)\n\nwebpage.save()\n'"
src/pix2pixHD/train.py,2,"b'### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport time\nfrom collections import OrderedDict\nfrom options.train_options import TrainOptions\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nimport util.util as util\nfrom util.visualizer import Visualizer\nimport os\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\n\nopt = TrainOptions().parse()\niter_path = os.path.join(opt.checkpoints_dir, opt.name, \'iter.txt\')\nif opt.continue_train:\n    try:\n        start_epoch, epoch_iter = np.loadtxt(iter_path , delimiter=\',\', dtype=int)\n    except:\n        start_epoch, epoch_iter = 1, 0\n    print(\'Resuming from epoch %d at iteration %d\' % (start_epoch, epoch_iter))        \nelse:    \n    start_epoch, epoch_iter = 1, 0\n\nif opt.debug:\n    opt.display_freq = 1\n    opt.print_freq = 1\n    opt.niter = 1\n    opt.niter_decay = 0\n    opt.max_dataset_size = 10\n\ndata_loader = CreateDataLoader(opt)\ndataset = data_loader.load_data()\ndataset_size = len(data_loader)\nprint(\'#training images = %d\' % dataset_size)\n\nmodel = create_model(opt)\nvisualizer = Visualizer(opt)\n\ntotal_steps = (start_epoch-1) * dataset_size + epoch_iter\n\ndisplay_delta = total_steps % opt.display_freq\nprint_delta = total_steps % opt.print_freq\nsave_delta = total_steps % opt.save_latest_freq\n\nfor epoch in range(start_epoch, opt.niter + opt.niter_decay + 1):\n    epoch_start_time = time.time()\n    if epoch != start_epoch:\n        epoch_iter = epoch_iter % dataset_size\n    for i, data in enumerate(dataset, start=epoch_iter):\n        iter_start_time = time.time()\n        total_steps += opt.batchSize\n        epoch_iter += opt.batchSize\n\n        # whether to collect output images\n        save_fake = total_steps % opt.display_freq == display_delta\n\n        ############## Forward Pass ######################\n        losses, generated = model(Variable(data[\'label\']), Variable(data[\'inst\']), \n            Variable(data[\'image\']), Variable(data[\'feat\']), infer=save_fake)\n\n        # sum per device losses\n        losses = [ torch.mean(x) if not isinstance(x, int) else x for x in losses ]\n        loss_dict = dict(zip(model.module.loss_names, losses))\n\n        # calculate final loss scalar\n        loss_D = (loss_dict[\'D_fake\'] + loss_dict[\'D_real\']) * 0.5\n        loss_G = loss_dict[\'G_GAN\'] + loss_dict.get(\'G_GAN_Feat\',0) + loss_dict.get(\'G_VGG\',0)\n\n        ############### Backward Pass ####################\n        # update generator weights\n        model.module.optimizer_G.zero_grad()\n        loss_G.backward()\n        model.module.optimizer_G.step()\n\n        # update discriminator weights\n        model.module.optimizer_D.zero_grad()\n        loss_D.backward()\n        model.module.optimizer_D.step()\n\n        #call([""nvidia-smi"", ""--format=csv"", ""--query-gpu=memory.used,memory.free""]) \n\n        ############## Display results and errors ##########\n        ### print out errors\n        if total_steps % opt.print_freq == print_delta:\n            errors = {k: v.data[0] if not isinstance(v, int) else v for k, v in loss_dict.items()}\n            t = (time.time() - iter_start_time) / opt.batchSize\n            visualizer.print_current_errors(epoch, epoch_iter, errors, t)\n            visualizer.plot_current_errors(errors, total_steps)\n\n        ### display output images\n        if save_fake:\n            visuals = OrderedDict([(\'input_label\', util.tensor2label(data[\'label\'][0], opt.label_nc)),\n                                   (\'synthesized_image\', util.tensor2im(generated.data[0])),\n                                   (\'real_image\', util.tensor2im(data[\'image\'][0]))])\n            visualizer.display_current_results(visuals, epoch, total_steps)\n\n        ### save latest model\n        if total_steps % opt.save_latest_freq == save_delta:\n            print(\'saving the latest model (epoch %d, total_steps %d)\' % (epoch, total_steps))\n            model.module.save(\'latest\')            \n            np.savetxt(iter_path, (epoch, epoch_iter), delimiter=\',\', fmt=\'%d\')\n\n        if epoch_iter >= dataset_size:\n            break\n       \n    # end of epoch \n    iter_end_time = time.time()\n    print(\'End of epoch %d / %d \\t Time Taken: %d sec\' %\n          (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n\n    ### save model for this epoch\n    if epoch % opt.save_epoch_freq == 0:\n        print(\'saving the model at the end of epoch %d, iters %d\' % (epoch, total_steps))        \n        model.module.save(\'latest\')\n        model.module.save(epoch)\n        np.savetxt(iter_path, (epoch+1, 0), delimiter=\',\', fmt=\'%d\')\n\n    ### instead of only training the local enhancer, train the entire network after certain iterations\n    if (opt.niter_fix_global != 0) and (epoch == opt.niter_fix_global):\n        model.module.update_fixed_params()\n\n    ### linearly decay learning rate after certain iterations\n    if epoch > opt.niter:\n        model.module.update_learning_rate()\n'"
src/utils/openpose_utils.py,0,"b""import numpy as np\nimport math\nimport cv2\nfrom skimage import filters\nfrom scipy import ndimage\n\n# openpose\nimport sys\nsys.path.append('./PoseEstimation')\n\nfrom network.post import *\n\ndef remove_noise(img):\n    th = filters.threshold_otsu(img)\n    bin_img = img > th\n    regions, num = ndimage.label(bin_img)\n    areas = []\n    for i in range(num):\n        areas.append(np.sum(regions == i+1))\n    img[regions != np.argmax(areas)+1] = 0\n    return img\n\n\ndef create_label(shape, joint_list, person_to_joint_assoc):\n    label = np.zeros(shape, dtype=np.uint8)\n    cord_list = []\n    for limb_type in range(17):\n        for person_joint_info in person_to_joint_assoc:\n            joint_indices = person_joint_info[joint_to_limb_heatmap_relationship[limb_type]].astype(int)\n            if -1 in joint_indices:\n                continue\n            joint_coords = joint_list[joint_indices, :2]\n            coords_center = tuple(np.round(np.mean(joint_coords, 0)).astype(int))\n            cord_list.append(joint_coords[0])\n            limb_dir = joint_coords[0, :] - joint_coords[1, :]\n            limb_length = np.linalg.norm(limb_dir)\n            angle = math.degrees(math.atan2(limb_dir[1], limb_dir[0]))\n            polygon = cv2.ellipse2Poly(coords_center, (int(limb_length / 2), 4), int(angle), 0, 360, 1)\n            cv2.fillConvexPoly(label, polygon, limb_type+1)\n    return label,cord_list\n\n\ndef get_pose(param, heatmaps, pafs):\n    shape = heatmaps.shape[:2]\n    # Bottom-up approach:\n    # Step 1: find all joints in the image (organized by joint type: [0]=nose,\n    # [1]=neck...)\n    joint_list_per_joint_type = NMS(param, heatmaps)\n    # joint_list is an unravel'd version of joint_list_per_joint, where we add\n    # a 5th column to indicate the joint_type (0=nose, 1=neck...)\n    joint_list = np.array([tuple(peak) + (joint_type,) for joint_type,\n                           joint_peaks in enumerate(joint_list_per_joint_type) for peak in joint_peaks])\n\n    # Step 2: find which joints go together to form limbs (which wrists go\n    # with which elbows)\n    paf_upsamp = cv2.resize(pafs, shape, interpolation=cv2.INTER_CUBIC)\n    connected_limbs = find_connected_joints(param, paf_upsamp, joint_list_per_joint_type)\n\n    # Step 3: associate limbs that belong to the same person\n    person_to_joint_assoc = group_limbs_of_same_person(connected_limbs, joint_list)\n\n    # (Step 4): plot results\n    label,cord_list = create_label(shape, joint_list, person_to_joint_assoc)\n\n    return label, cord_list\n"""
src/utils/save_img.py,0,"b""import numpy as np\nimport cv2\n\ndef main():\n    vc = cv2.VideoCapture(0)\n    if vc.isOpened():\n        is_capturing, _ = vc.read()\n    else:\n        is_capturing = False\n\n    cnt = 0\n    while is_capturing:\n        is_capturing, img = vc.read()\n        # img = img[:, 80:80+480]\n        cv2.imwrite('../../data/target/images/img_%d.png'%cnt, img)\n        cv2.imshow('video', img)\n        k = cv2.waitKey(30) & 0xff\n        if k == 27: # press 'ESC' to quit\n            break\n        cnt += 1\n    vc.release()\n    cv2.destroyAllWindows()\n\nmain()\n"""
src/PoseEstimation/demo/picture_demo.py,6,"b""import os\nimport re\nimport sys\nimport cv2\nimport math\nimport time\nimport scipy\nimport argparse\nimport matplotlib\nimport numpy as np\nimport pylab as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom scipy.ndimage.filters import gaussian_filter\nfrom network.rtpose_vgg import get_model\nfrom network.post import decode_pose\nfrom training.datasets.coco_data.preprocessing import (inception_preprocess,\n                                              rtpose_preprocess,\n                                              ssd_preprocess, vgg_preprocess)\nfrom network import im_transform\nfrom evaluate.coco_eval import get_multiplier, get_outputs, handle_paf_and_heat\n\n\nweight_name = './network/weight/pose_model.pth'\n\nmodel = get_model('vgg19')     \nmodel.load_state_dict(torch.load(weight_name))\nmodel = torch.nn.DataParallel(model).cuda()\nmodel.float()\nmodel.eval()\n\n\n\ntest_image = './readme/ski.jpg'\noriImg = cv2.imread(test_image) # B,G,R order\nshape_dst = np.min(oriImg.shape[0:2])\n\n# Get results of original image\nmultiplier = get_multiplier(oriImg)\n\nwith torch.no_grad():\n    orig_paf, orig_heat = get_outputs(\n        multiplier, oriImg, model,  'rtpose')\n          \n    # Get results of flipped image\n    swapped_img = oriImg[:, ::-1, :]\n    flipped_paf, flipped_heat = get_outputs(multiplier, swapped_img,\n                                            model, 'rtpose')\n\n    # compute averaged heatmap and paf\n    paf, heatmap = handle_paf_and_heat(\n        orig_heat, flipped_heat, orig_paf, flipped_paf)\n            \nparam = {'thre1': 0.1, 'thre2': 0.05, 'thre3': 0.5}\ncanvas, to_plot, candidate, subset = decode_pose(\n    oriImg, param, heatmap, paf)\n \ncv2.imwrite('result.png',to_plot)   \n\n"""
src/PoseEstimation/demo/web_demo.py,5,"b'import os\nimport re\nimport sys\nimport cv2\nimport math\nimport time\nimport scipy\nimport argparse\nimport matplotlib\nimport numpy as np\nimport pylab as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom config_reader import config_reader\nfrom scipy.ndimage.filters import gaussian_filter\nfrom network.rtpose_vgg import get_model\nfrom network.post import decode_pose\nfrom training.datasets.coco_data.preprocessing import (inception_preprocess,\n                                              rtpose_preprocess,\n                                              ssd_preprocess, vgg_preprocess)\nfrom network import im_transform\nfrom evaluate.coco_eval import get_multiplier, get_outputs\n#parser = argparse.ArgumentParser()\n#parser.add_argument(\'--t7_file\', required=True)\n#parser.add_argument(\'--pth_file\', required=True)\n#args = parser.parse_args()\n\nweight_name = \'./network/weight/pose_model.pth\'\nmodel = get_model(\'vgg19\')     \nmodel.load_state_dict(torch.load(weight_name))\nmodel.cuda()\nmodel.float()\nmodel.eval()\n\nif __name__ == ""__main__"":\n    \n    video_capture = cv2.VideoCapture(0)\n\n    while True:\n        # Capture frame-by-frame\n        ret, oriImg = video_capture.read()\n        \n        shape_dst = np.min(oriImg.shape[0:2])\n\n        # Get results of original image\n        multiplier = get_multiplier(oriImg)\n\n        with torch.no_grad():\n            paf, heatmap = get_outputs(\n                multiplier, oriImg, model,  \'rtpose\')\n                  \n\n        param = {\'thre1\': 0.1, \'thre2\': 0.05, \'thre3\': 0.5}\n        canvas, to_plot, candidate, subset = decode_pose(\n            oriImg, param, heatmap, paf)\n\n        # Display the resulting frame\n        cv2.imshow(\'Video\', to_plot)\n\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n            break\n\n    # When everything is done, release the capture\n    video_capture.release()\n    cv2.destroyAllWindows()\n'"
src/PoseEstimation/evaluate/__init__.py,0,b''
src/PoseEstimation/evaluate/coco_eval.py,1,"b'import os\nimport time\n\nimport cv2\nimport numpy as np\nimport json\nimport pandas as pd\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nimport torch\nfrom training.datasets.coco_data.preprocessing import (inception_preprocess,\n                                              rtpose_preprocess,\n                                              ssd_preprocess, vgg_preprocess)\nfrom network.post import decode_pose\nfrom network import im_transform\n\n\'\'\'\nMS COCO annotation order:\n0: nose\t   \t\t1: l eye\t\t2: r eye\t3: l ear\t4: r ear\n5: l shoulder\t6: r shoulder\t7: l elbow\t8: r elbow\n9: l wrist\t\t10: r wrist\t\t11: l hip\t12: r hip\t13: l knee\n14: r knee\t\t15: l ankle\t\t16: r ankle\n\nThe order in this work:\n(0-\'nose\'\t1-\'neck\' 2-\'right_shoulder\' 3-\'right_elbow\' 4-\'right_wrist\'\n5-\'left_shoulder\' 6-\'left_elbow\'\t    7-\'left_wrist\'  8-\'right_hip\'\n9-\'right_knee\'\t 10-\'right_ankle\'\t11-\'left_hip\'   12-\'left_knee\'\n13-\'left_ankle\'\t 14-\'right_eye\'\t    15-\'left_eye\'   16-\'right_ear\'\n17-\'left_ear\' )\n\'\'\'\n\nORDER_COCO = [0, 15, 14, 17, 16, 5, 2, 6, 3, 7, 4, 11, 8, 12, 9, 13, 10]\n\nMID_1 = [1, 8,  9, 1,  11, 12, 1, 2, 3,\n         2,  1, 5, 6, 5,  1, 0,  0,  14, 15]\n\nMID_2 = [8, 9, 10, 11, 12, 13, 2, 3, 4,\n         16, 5, 6, 7, 17, 0, 14, 15, 16, 17]\n\n\ndef eval_coco(outputs, dataDir, imgIds):\n    """"""Evaluate images on Coco test set\n    :param outputs: list of dictionaries, the models\' processed outputs\n    :param dataDir: string, path to the MSCOCO data directory\n    :param imgIds: list, all the image ids in the validation set\n    :returns : float, the mAP score\n    """"""\n    with open(\'results.json\', \'w\') as f:\n        json.dump(outputs, f)  \n    annType = \'keypoints\'\n    prefix = \'person_keypoints\'\n\n    # initialize COCO ground truth api\n    dataType = \'val2014\'\n    annFile = \'%s/annotations/%s_%s.json\' % (dataDir, prefix, dataType)\n    cocoGt = COCO(annFile)  # load annotations\n    cocoDt = cocoGt.loadRes(\'results.json\')  # load model outputs\n\n    # running evaluation\n    cocoEval = COCOeval(cocoGt, cocoDt, annType)\n    cocoEval.params.imgIds = imgIds\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    cocoEval.summarize()\n    os.remove(\'results.json\')\n    # return Average Precision\n    return cocoEval.stats[0]\n\n\ndef get_multiplier(img):\n    """"""Computes the sizes of image at different scales\n    :param img: numpy array, the current image\n    :returns : list of float. The computed scales\n    """"""\n    scale_search = [0.5, 1., 1.5, 2, 2.5]\n    return [x * 368. / float(img.shape[0]) for x in scale_search]\n\n\ndef get_coco_val(file_path):\n    """"""Reads MSCOCO validation informatio\n    :param file_path: string, the path to the MSCOCO validation file\n    :returns : list of image ids, list of image file paths, list of widths,\n               list of heights\n    """"""\n    val_coco = pd.read_csv(file_path, sep=\'\\s+\', header=None)\n    image_ids = list(val_coco[1])\n    file_paths = list(val_coco[2])\n    heights = list(val_coco[3])\n    widths = list(val_coco[4])\n\n    return image_ids, file_paths, heights, widths\n\n\ndef get_outputs(multiplier, img, model, preprocess):\n    """"""Computes the averaged heatmap and paf for the given image\n    :param multiplier:\n    :param origImg: numpy array, the image being processed\n    :param model: pytorch model\n    :returns: numpy arrays, the averaged paf and heatmap\n    """"""\n\n    heatmap_avg = np.zeros((img.shape[0], img.shape[1], 19))\n    paf_avg = np.zeros((img.shape[0], img.shape[1], 38))\n    max_scale = multiplier[-1]\n    max_size = max_scale * img.shape[0]\n    # padding\n    max_cropped, _, _ = im_transform.crop_with_factor(\n        img, max_size, factor=8, is_ceil=True)\n    batch_images = np.zeros(\n        (len(multiplier), 3, max_cropped.shape[0], max_cropped.shape[1]))\n\n    for m in range(len(multiplier)):\n        scale = multiplier[m]\n        inp_size = scale * img.shape[0]\n\n        # padding\n        im_croped, im_scale, real_shape = im_transform.crop_with_factor(\n            img, inp_size, factor=8, is_ceil=True)\n\n        if preprocess == \'rtpose\':\n            im_data = rtpose_preprocess(im_croped)\n\n        elif preprocess == \'vgg\':\n            im_data = vgg_preprocess(im_croped)\n\n        elif preprocess == \'inception\':\n            im_data = inception_preprocess(im_croped)\n\n        elif preprocess == \'ssd\':\n            im_data = ssd_preprocess(im_croped)\n\n        batch_images[m, :, :im_data.shape[1], :im_data.shape[2]] = im_data\n\n    # several scales as a batch\n    batch_var = torch.from_numpy(batch_images).cuda().float()\n    predicted_outputs, _ = model(batch_var)\n    output1, output2 = predicted_outputs[-2], predicted_outputs[-1]\n    heatmaps = output2.cpu().data.numpy().transpose(0, 2, 3, 1)\n    pafs = output1.cpu().data.numpy().transpose(0, 2, 3, 1)\n\n    for m in range(len(multiplier)):\n        scale = multiplier[m]\n        inp_size = scale * img.shape[0]\n\n        # padding\n        im_cropped, im_scale, real_shape = im_transform.crop_with_factor(\n            img, inp_size, factor=8, is_ceil=True)\n        heatmap = heatmaps[m, :int(im_cropped.shape[0] /\n                           8), :int(im_cropped.shape[1] / 8), :]\n        heatmap = cv2.resize(heatmap, None, fx=8, fy=8,\n                             interpolation=cv2.INTER_CUBIC)\n        heatmap = heatmap[0:real_shape[0], 0:real_shape[1], :]\n        heatmap = cv2.resize(\n            heatmap, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_CUBIC)\n\n        paf = pafs[m, :int(im_cropped.shape[0] / 8), :int(im_cropped.shape[1] / 8), :]\n        paf = cv2.resize(paf, None, fx=8, fy=8, interpolation=cv2.INTER_CUBIC)\n        paf = paf[0:real_shape[0], 0:real_shape[1], :]\n        paf = cv2.resize(\n            paf, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_CUBIC)\n\n        heatmap_avg = heatmap_avg + heatmap / len(multiplier)\n        paf_avg = paf_avg + paf / len(multiplier)\n\n    return paf_avg, heatmap_avg\n\n\ndef append_result(image_id, person_to_joint_assoc, joint_list, outputs):\n    """"""Build the outputs to be evaluated\n    :param image_id: int, the id of the current image\n    :param person_to_joint_assoc: numpy array of joints associations\n    :param joint_list: list, list of joints\n    :param outputs: list of dictionaries with the following keys: image_id,\n                    category_id, keypoints, score\n    """"""\n\n    for ridxPred in range(len(person_to_joint_assoc)):\n        one_result = {\n            ""image_id"": 0,\n            ""category_id"": 1,\n            ""keypoints"": [],\n            ""score"": 0\n        }\n\n        one_result[""image_id""] = image_id\n        keypoints = np.zeros((17, 3))\n\n        for part in range(17):\n            ind = ORDER_COCO[part]\n            index = int(person_to_joint_assoc[ridxPred, ind])\n\n            if -1 == index:\n                keypoints[part, 0] = 0\n                keypoints[part, 1] = 0\n                keypoints[part, 2] = 0\n\n            else:\n                keypoints[part, 0] = joint_list[index, 0] + 0.5\n                keypoints[part, 1] = joint_list[index, 1] + 0.5\n                keypoints[part, 2] = 1\n\n        one_result[""score""] = person_to_joint_assoc[ridxPred, -2] * \\\n            person_to_joint_assoc[ridxPred, -1]\n        one_result[""keypoints""] = list(keypoints.reshape(51))\n\n        outputs.append(one_result)\n\n\ndef handle_paf_and_heat(normal_heat, flipped_heat, normal_paf, flipped_paf):\n    """"""Compute the average of normal and flipped heatmap and paf\n    :param normal_heat: numpy array, the normal heatmap\n    :param normal_paf: numpy array, the normal paf\n    :param flipped_heat: numpy array, the flipped heatmap\n    :param flipped_paf: numpy array, the flipped  paf\n    :returns: numpy arrays, the averaged paf and heatmap\n    """"""\n\n    # The order to swap left and right of heatmap\n    swap_heat = np.array((0, 1, 5, 6, 7, 2, 3, 4, 11, 12,\n                          13, 8, 9, 10, 15, 14, 17, 16, 18))\n\n    # paf\'s order\n    # 0,1 2,3 4,5\n    # neck to right_hip, right_hip to right_knee, right_knee to right_ankle\n\n    # 6,7 8,9, 10,11\n    # neck to left_hip, left_hip to left_knee, left_knee to left_ankle\n\n    # 12,13 14,15, 16,17, 18, 19\n    # neck to right_shoulder, right_shoulder to right_elbow, right_elbow to\n    # right_wrist, right_shoulder to right_ear\n\n    # 20,21 22,23, 24,25 26,27\n    # neck to left_shoulder, left_shoulder to left_elbow, left_elbow to\n    # left_wrist, left_shoulder to left_ear\n\n    # 28,29, 30,31, 32,33, 34,35 36,37\n    # neck to nose, nose to right_eye, nose to left_eye, right_eye to\n    # right_ear, left_eye to left_ear So the swap of paf should be:\n    swap_paf = np.array((6, 7, 8, 9, 10, 11, 0, 1, 2, 3, 4, 5, 20, 21, 22, 23,\n                         24, 25, 26, 27, 12, 13, 14, 15, 16, 17, 18, 19, 28,\n                         29, 32, 33, 30, 31, 36, 37, 34, 35))\n\n    flipped_paf = flipped_paf[:, ::-1, :]\n\n    # The pafs are unit vectors, The x will change direction after flipped.\n    # not easy to understand, you may try visualize it.\n    flipped_paf[:, :, swap_paf[1::2]] = flipped_paf[:, :, swap_paf[1::2]]\n    flipped_paf[:, :, swap_paf[::2]] = -flipped_paf[:, :, swap_paf[::2]]\n    averaged_paf = (normal_paf + flipped_paf[:, :, swap_paf]) / 2.\n    averaged_heatmap = (\n        normal_heat + flipped_heat[:, ::-1, :][:, :, swap_heat]) / 2.\n\n    return averaged_paf, averaged_heatmap\n\n        \ndef run_eval(image_dir, anno_dir, vis_dir, image_list_txt, model, preprocess):\n    """"""Run the evaluation on the test set and report mAP score\n    :param model: the model to test\n    :returns: float, the reported mAP score\n    """"""\n    # This txt file is fount in the caffe_rtpose repository:\n    # https://github.com/CMU-Perceptual-Computing-Lab/caffe_rtpose/blob/master\n    img_ids, img_paths, img_heights, img_widths = get_coco_val(\n        image_list_txt)\n    # img_ids = img_ids[81:82]\n    # img_paths = img_paths[81:82]\n    print(""Total number of validation images {}"".format(len(img_ids)))\n\n    # iterate all val images\n    outputs = []\n    print(""Processing Images in validation set"")\n    for i in range(len(img_ids)):\n        if i % 10 == 0 and i != 0:\n            print(""Processed {} images"".format(i))\n\n        oriImg = cv2.imread(os.path.join(image_dir, \'val2014/\' + img_paths[i]))\n        # Get the shortest side of the image (either height or width)\n        shape_dst = np.min(oriImg.shape[0:2])\n\n        # Get results of original image\n        multiplier = get_multiplier(oriImg)\n        orig_paf, orig_heat = get_outputs(\n            multiplier, oriImg, model,  preprocess)\n\n        # Get results of flipped image\n        swapped_img = oriImg[:, ::-1, :]\n        flipped_paf, flipped_heat = get_outputs(multiplier, swapped_img,\n                                                model, preprocess)\n\n        # compute averaged heatmap and paf\n        paf, heatmap = handle_paf_and_heat(\n            orig_heat, flipped_heat, orig_paf, flipped_paf)\n\n        # choose which post-processing to use, our_post_processing\n        # got slightly higher AP but is slow.\n        param = {\'thre1\': 0.1, \'thre2\': 0.05, \'thre3\': 0.5}\n        canvas, to_plot, candidate, subset = decode_pose(\n            oriImg, param, heatmap, paf)\n            \n        vis_path = os.path.join(vis_dir, img_paths[i])\n        cv2.imwrite(vis_path, to_plot)\n        # subset indicated how many peoples foun in this image.\n        append_result(img_ids[i], subset, candidate, outputs)\n\n\n        # cv2.imshow(\'test\', canvas)\n        # cv2.waitKey(0)\n    # Eval and show the final result!\n    return eval_coco(outputs=outputs, dataDir=anno_dir, imgIds=img_ids)\n'"
src/PoseEstimation/evaluate/evaluation.py,3,"b""import unittest\nimport torch\nfrom evaluate.coco_eval import run_eval\nfrom network.rtpose_vgg import get_model, use_vgg\nfrom torch import load\n\n#Notice, if you using the \nwith torch.autograd.no_grad():\n    # this path is with respect to the root of the project\n    weight_name = './network/weight/best_pose.pth'\n    state_dict = torch.load(weight_name)\n    model = get_model(trunk='vgg19')\n    \n    model = torch.nn.DataParallel(model).cuda()\n    model.load_state_dict(state_dict)\n    model.eval()\n    model.float()\n    model = model.cuda()\n    \n    # The choice of image preprocessing include: 'rtpose', 'inception', 'vgg' and 'ssd'.\n    # If you use the converted model from caffe, it is 'rtpose' preprocess, the model trained in \n    # this repo used 'vgg' preprocess\n    run_eval(image_dir= '/data/coco/images/', anno_dir = '/data/coco', vis_dir = '/data/coco/vis',\n        image_list_txt='./evaluate/image_info_val2014_1k.txt', \n        model=model, preprocess='vgg')\n\n\n"""
src/PoseEstimation/network/__init__.py,0,b''
src/PoseEstimation/network/im_transform.py,0,"b'import numpy as np\nimport cv2\n\n\ndef resize(frame, desired_size):\n    old_size = frame.shape[:2]\n    ratio = float(desired_size) / max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n\n    frame = cv2.resize(frame, (new_size[1], new_size[0]))\n    delta_w = desired_size - new_size[1]\n    delta_h = desired_size - new_size[0]\n    top, bottom = delta_h//2, delta_h-(delta_h//2)\n    left, right = delta_w//2, delta_w-(delta_w//2)\n    color = [0, 0, 0]\n    frame = cv2.copyMakeBorder(\n        frame, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n\n    if left == 0:\n        scale = float(old_size[1]) / desired_size\n    else:\n        scale = float(old_size[0]) / desired_size\n    return frame, left, top, scale\n\n\ndef imcv2_recolor(im, a=.1):\n    # t = [np.random.uniform()]\n    # t += [np.random.uniform()]\n    # t += [np.random.uniform()]\n    # t = np.array(t) * 2. - 1.\n    t = np.random.uniform(-1, 1, 3)\n\n    # random amplify each channel\n    im = im.astype(np.float)\n    im *= (1 + t * a)\n    mx = 255. * (1 + a)\n    up = np.random.uniform(-1, 1)\n    im = np.power(im / mx, 1. + up * .5)\n    # return np.array(im * 255., np.uint8)\n    return im\n\n\ndef imcv2_affine_trans(im, flip=None, im_shape=None, rotate=False, max_scale=1.5):\n    # Scale and translate\n    h, w = im.shape[:2] if im_shape is None else im_shape[:2]\n    scale = np.random.uniform(1., max_scale)\n\n    degree = np.random.uniform(-5, 5) if rotate else None\n\n    max_offx = (scale - 1.) * w\n    max_offy = (scale - 1.) * h\n    offx = int(np.random.uniform() * max_offx)\n    offy = int(np.random.uniform() * max_offy)\n\n    flip_ = np.random.uniform() > 0.5 if flip is None else flip\n\n    if im is not None:\n        im = apply_affine(im, scale, [offx, offy, degree], flip_, im_shape)\n\n    return im, [scale, [offx, offy, degree], flip_, im_shape]\n\n\ndef apply_affine(im, scale, offs, flip, im_shape=None):\n    offx, offy, degree = offs\n    h, w = im.shape[:2] if im_shape is None else im_shape[:2]\n\n    im = cv2.resize(im, (0, 0), fx=scale, fy=scale)\n    if degree is not None:\n        retval = cv2.getRotationMatrix2D((w // 2, h // 2), degree, 1)\n        im = cv2.warpAffine(im, retval, (w, h))\n    im = im[offy: (offy + h), offx: (offx + w)]\n    if flip:\n        im = cv2.flip(im, 1)\n\n    return im\n\n\ndef offset_boxes(boxes, scale, offs, flip, im_shape):\n    if len(boxes) == 0:\n        return boxes\n\n    boxes = np.asarray(boxes, dtype=np.float)\n    expand = False\n    if boxes.ndim == 1:\n        expand = True\n        boxes = np.expand_dims(boxes, 0)\n\n    boxes *= scale\n    boxes[:, 0::2] -= offs[0]\n    boxes[:, 1::2] -= offs[1]\n\n    is_box = boxes.shape[-1] % 4 == 0\n    # if is_box:\n    #     boxes = clip_boxes(boxes, im_shape)\n\n    if flip:\n        boxes[:, 0::2] = im_shape[1] - boxes[:, 0::2]\n        if is_box:\n            for i in range(boxes.shape[-1] // 4):\n                tmp = boxes[:, i].copy()\n                boxes[:, i] = boxes[:, i + 2]\n                boxes[:, i + 2] = tmp\n\n        # boxes_x = np.copy(boxes[:, 0])\n        # boxes[:, 0] = im_shape[1] - boxes[:, 2]\n        # boxes[:, 2] = im_shape[1] - boxes_x\n\n    if expand:\n        boxes = boxes[0]\n    return boxes\n\n\ndef _factor_closest(num, factor, is_ceil=True):\n    num = np.ceil(float(num) / factor) if is_ceil else np.floor(float(num) / factor)\n    num = int(num) * factor\n    return num\n\n\ndef crop_with_factor(im, dest_size=None, factor=32, is_ceil=True):\n    im_shape = im.shape\n    im_size_min = np.min(im_shape[0:2])\n    im_size_max = np.max(im_shape[0:2])\n    # im_scale = 1.\n    # if max_size is not None and im_size_min > max_size:\n    im_scale = float(dest_size) / im_size_min\n    im = cv2.resize(im, None, fx=im_scale, fy=im_scale)\n\n    h, w, c = im.shape\n    new_h = _factor_closest(h, factor=factor, is_ceil=is_ceil)\n    new_w = _factor_closest(w, factor=factor, is_ceil=is_ceil)\n    im_croped = np.zeros([new_h, new_w, c], dtype=im.dtype)\n    im_croped[0:h, 0:w, :] = im\n\n    return im_croped, im_scale, im.shape\n    \n'"
src/PoseEstimation/network/post.py,0,"b'import math\n\nimport cv2\nimport matplotlib.cm\nimport numpy as np\nfrom scipy.ndimage.filters import gaussian_filter, maximum_filter\nfrom scipy.ndimage.morphology import generate_binary_structure\n\n# It is better to use 0.1 as threshold when evaluation, but 0.3 for demo\n# purpose.\ncmap = matplotlib.cm.get_cmap(\'hsv\')\n\n# Heatmap indices to find each limb (joint connection). Eg: limb_type=1 is\n# Neck->LShoulder, so joint_to_limb_heatmap_relationship[1] represents the\n# indices of heatmaps to look for joints: neck=1, LShoulder=5\njoint_to_limb_heatmap_relationship = [\n    [1, 2], [1, 5], [2, 3], [3, 4], [5, 6], [6, 7], [1, 8], [8, 9], [9, 10],\n    [1, 11], [11, 12], [12, 13], [1, 0], [0, 14], [14, 16], [0, 15], [15, 17],\n    [2, 16], [5, 17]]\n\n# PAF indices containing the x and y coordinates of the PAF for a given limb.\n# Eg: limb_type=1 is Neck->LShoulder, so\n# PAFneckLShoulder_x=paf_xy_coords_per_limb[1][0] and\n# PAFneckLShoulder_y=paf_xy_coords_per_limb[1][1]\npaf_xy_coords_per_limb = [\n    [12, 13], [20, 21], [14, 15], [16, 17], [22, 23],\n    [24, 25], [0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [28, 29],\n    [30, 31], [34, 35], [32, 33], [36, 37], [18, 19], [26, 27]]\n\n# Color code used to plot different joints and limbs (eg: joint_type=3 and\n# limb_type=3 will use colors[3])\ncolors = [\n    [255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0],\n    [85, 255, 0], [0, 255, 0], [0, 255, 85], [0, 255, 170], [0, 255, 255],\n    [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255], [170, 0, 255],\n    [255, 0, 255], [255, 0, 170], [255, 0, 85], [255, 0, 0]]\n\nNUM_JOINTS = 18\nNUM_LIMBS = len(joint_to_limb_heatmap_relationship)\n\n\ndef find_peaks(param, img):\n    """"""\n    Given a (grayscale) image, find local maxima whose value is above a given\n    threshold (param[\'thre1\'])\n    :param img: Input image (2d array) where we want to find peaks\n    :return: 2d np.array containing the [x,y] coordinates of each peak found\n    in the image\n    """"""\n\n    peaks_binary = (maximum_filter(img, footprint=generate_binary_structure(\n        2, 1)) == img) * (img > param[\'thre1\'])\n    # Note reverse ([::-1]): we return [[x y], [x y]...] instead of [[y x], [y\n    # x]...]\n    return np.array(np.nonzero(peaks_binary)[::-1]).T\n\n\ndef compute_resized_coords(coords, resizeFactor):\n    """"""\n    Given the index/coordinates of a cell in some input array (e.g. image),\n    provides the new coordinates if that array was resized by making it\n    resizeFactor times bigger.\n    E.g.: image of size 3x3 is resized to 6x6 (resizeFactor=2), we\'d like to\n    know the new coordinates of cell [1,2] -> Function would return [2.5,4.5]\n    :param coords: Coordinates (indices) of a cell in some input array\n    :param resizeFactor: Resize coefficient = shape_dest/shape_source. E.g.:\n    resizeFactor=2 means the destination array is twice as big as the\n    original one\n    :return: Coordinates in an array of size\n    shape_dest=resizeFactor*shape_source, expressing the array indices of the\n    closest point to \'coords\' if an image of size shape_source was resized to\n    shape_dest\n    """"""\n\n    # 1) Add 0.5 to coords to get coordinates of center of the pixel (e.g.\n    # index [0,0] represents the pixel at location [0.5,0.5])\n    # 2) Transform those coordinates to shape_dest, by multiplying by resizeFactor\n    # 3) That number represents the location of the pixel center in the new array,\n    # so subtract 0.5 to get coordinates of the array index/indices (revert\n    # step 1)\n    return (np.array(coords, dtype=float) + 0.5) * resizeFactor - 0.5\n\n\ndef NMS(param, heatmaps, upsampFactor=1., bool_refine_center=True, bool_gaussian_filt=False):\n    """"""\n    NonMaximaSuppression: find peaks (local maxima) in a set of grayscale images\n    :param heatmaps: set of grayscale images on which to find local maxima (3d np.array,\n    with dimensions image_height x image_width x num_heatmaps)\n    :param upsampFactor: Size ratio between CPM heatmap output and the input image size.\n    Eg: upsampFactor=16 if original image was 480x640 and heatmaps are 30x40xN\n    :param bool_refine_center: Flag indicating whether:\n     - False: Simply return the low-res peak found upscaled by upsampFactor (subject to grid-snap)\n     - True: (Recommended, very accurate) Upsample a small patch around each low-res peak and\n     fine-tune the location of the peak at the resolution of the original input image\n    :param bool_gaussian_filt: Flag indicating whether to apply a 1d-GaussianFilter (smoothing)\n    to each upsampled patch before fine-tuning the location of each peak.\n    :return: a NUM_JOINTS x 4 np.array where each row represents a joint type (0=nose, 1=neck...)\n    and the columns indicate the {x,y} position, the score (probability) and a unique id (counter)\n    """"""\n    # MODIFIED BY CARLOS: Instead of upsampling the heatmaps to heatmap_avg and\n    # then performing NMS to find peaks, this step can be sped up by ~25-50x by:\n    # (9-10ms [with GaussFilt] or 5-6ms [without GaussFilt] vs 250-280ms on RoG\n    # 1. Perform NMS at (low-res) CPM\'s output resolution\n    # 1.1. Find peaks using scipy.ndimage.filters.maximum_filter\n    # 2. Once a peak is found, take a patch of 5x5 centered around the peak, upsample it, and\n    # fine-tune the position of the actual maximum.\n    #  \'-> That\'s equivalent to having found the peak on heatmap_avg, but much faster because we only\n    #      upsample and scan the 5x5 patch instead of the full (e.g.) 480x640\n\n    joint_list_per_joint_type = []\n    cnt_total_joints = 0\n\n    # For every peak found, win_size specifies how many pixels in each\n    # direction from the peak we take to obtain the patch that will be\n    # upsampled. Eg: win_size=1 -> patch is 3x3; win_size=2 -> 5x5\n    # (for BICUBIC interpolation to be accurate, win_size needs to be >=2!)\n    win_size = 2\n\n    for joint in range(NUM_JOINTS):\n        map_orig = heatmaps[:, :, joint]\n        peak_coords = find_peaks(param, map_orig)\n        peaks = np.zeros((len(peak_coords), 4))\n        for i, peak in enumerate(peak_coords):\n            if bool_refine_center:\n                x_min, y_min = np.maximum(0, peak - win_size)\n                x_max, y_max = np.minimum(\n                    np.array(map_orig.T.shape) - 1, peak + win_size)\n\n                # Take a small patch around each peak and only upsample that\n                # tiny region\n                patch = map_orig[y_min:y_max + 1, x_min:x_max + 1]\n                map_upsamp = cv2.resize(\n                    patch, None, fx=upsampFactor, fy=upsampFactor, interpolation=cv2.INTER_CUBIC)\n\n                # Gaussian filtering takes an average of 0.8ms/peak (and there might be\n                # more than one peak per joint!) -> For now, skip it (it\'s\n                # accurate enough)\n                map_upsamp = gaussian_filter(\n                    map_upsamp, sigma=3) if bool_gaussian_filt else map_upsamp\n\n                # Obtain the coordinates of the maximum value in the patch\n                location_of_max = np.unravel_index(\n                    map_upsamp.argmax(), map_upsamp.shape)\n                # Remember that peaks indicates [x,y] -> need to reverse it for\n                # [y,x]\n                location_of_patch_center = compute_resized_coords(\n                    peak[::-1] - [y_min, x_min], upsampFactor)\n                # Calculate the offset wrt to the patch center where the actual\n                # maximum is\n                refined_center = (location_of_max - location_of_patch_center)\n                peak_score = map_upsamp[location_of_max]\n            else:\n                refined_center = [0, 0]\n                # Flip peak coordinates since they are [x,y] instead of [y,x]\n                peak_score = map_orig[tuple(peak[::-1])]\n            peaks[i, :] = tuple([int(round(x)) for x in compute_resized_coords(\n                peak_coords[i], upsampFactor) + refined_center[::-1]]) + (peak_score, cnt_total_joints)\n            cnt_total_joints += 1\n        joint_list_per_joint_type.append(peaks)\n\n    return joint_list_per_joint_type\n\n\ndef find_connected_joints(param, paf_upsamp, joint_list_per_joint_type, num_intermed_pts=10):\n    """"""\n    For every type of limb (eg: forearm, shin, etc.), look for every potential\n    pair of joints (eg: every wrist-elbow combination) and evaluate the PAFs to\n    determine which pairs are indeed body limbs.\n    :param paf_upsamp: PAFs upsampled to the original input image resolution\n    :param joint_list_per_joint_type: See \'return\' doc of NMS()\n    :param num_intermed_pts: Int indicating how many intermediate points to take\n    between joint_src and joint_dst, at which the PAFs will be evaluated\n    :return: List of NUM_LIMBS rows. For every limb_type (a row) we store\n    a list of all limbs of that type found (eg: all the right forearms).\n    For each limb (each item in connected_limbs[limb_type]), we store 5 cells:\n    # {joint_src_id,joint_dst_id}: a unique number associated with each joint,\n    # limb_score_penalizing_long_dist: a score of how good a connection\n    of the joints is, penalized if the limb length is too long\n    # {joint_src_index,joint_dst_index}: the index of the joint within\n    all the joints of that type found (eg: the 3rd right elbow found)\n    """"""\n    connected_limbs = []\n\n    # Auxiliary array to access paf_upsamp quickly\n    limb_intermed_coords = np.empty((4, num_intermed_pts), dtype=np.intp)\n    for limb_type in range(NUM_LIMBS):\n        # List of all joints of type A found, where A is specified by limb_type\n        # (eg: a right forearm starts in a right elbow)\n        joints_src = joint_list_per_joint_type[joint_to_limb_heatmap_relationship[limb_type][0]]\n        # List of all joints of type B found, where B is specified by limb_type\n        # (eg: a right forearm ends in a right wrist)\n        joints_dst = joint_list_per_joint_type[joint_to_limb_heatmap_relationship[limb_type][1]]\n        if len(joints_src) == 0 or len(joints_dst) == 0:\n            # No limbs of this type found (eg: no right forearms found because\n            # we didn\'t find any right wrists or right elbows)\n            connected_limbs.append([])\n        else:\n            connection_candidates = []\n            # Specify the paf index that contains the x-coord of the paf for\n            # this limb\n            limb_intermed_coords[2, :] = paf_xy_coords_per_limb[limb_type][0]\n            # And the y-coord paf index\n            limb_intermed_coords[3, :] = paf_xy_coords_per_limb[limb_type][1]\n            for i, joint_src in enumerate(joints_src):\n                # Try every possible joints_src[i]-joints_dst[j] pair and see\n                # if it\'s a feasible limb\n                for j, joint_dst in enumerate(joints_dst):\n                    # Subtract the position of both joints to obtain the\n                    # direction of the potential limb\n                    limb_dir = joint_dst[:2] - joint_src[:2]\n                    # Compute the distance/length of the potential limb (norm\n                    # of limb_dir)\n                    limb_dist = np.sqrt(np.sum(limb_dir**2)) + 1e-8\n                    limb_dir = limb_dir / limb_dist  # Normalize limb_dir to be a unit vector\n\n                    # Linearly distribute num_intermed_pts points from the x\n                    # coordinate of joint_src to the x coordinate of joint_dst\n                    limb_intermed_coords[1, :] = np.round(np.linspace(\n                        joint_src[0], joint_dst[0], num=num_intermed_pts))\n                    limb_intermed_coords[0, :] = np.round(np.linspace(\n                        joint_src[1], joint_dst[1], num=num_intermed_pts))  # Same for the y coordinate\n                    intermed_paf = paf_upsamp[limb_intermed_coords[0, :],\n                                              limb_intermed_coords[1, :], limb_intermed_coords[2:4, :]].T\n\n                    score_intermed_pts = intermed_paf.dot(limb_dir)\n                    score_penalizing_long_dist = score_intermed_pts.mean(\n                    ) + min(0.5 * paf_upsamp.shape[0] / limb_dist - 1, 0)\n                    # Criterion 1: At least 80% of the intermediate points have\n                    # a score higher than thre2\n                    criterion1 = (np.count_nonzero(\n                        score_intermed_pts > param[\'thre2\']) > 0.8 * num_intermed_pts)\n                    # Criterion 2: Mean score, penalized for large limb\n                    # distances (larger than half the image height), is\n                    # positive\n                    criterion2 = (score_penalizing_long_dist > 0)\n                    if criterion1 and criterion2:\n                        # Last value is the combined paf(+limb_dist) + heatmap\n                        # scores of both joints\n                        connection_candidates.append(\n                            [i, j, score_penalizing_long_dist, score_penalizing_long_dist + joint_src[2] + joint_dst[2]])\n\n            # Sort connection candidates based on their\n            # score_penalizing_long_dist\n            connection_candidates = sorted(\n                connection_candidates, key=lambda x: x[2], reverse=True)\n            connections = np.empty((0, 5))\n            # There can only be as many limbs as the smallest number of source\n            # or destination joints (eg: only 2 forearms if there\'s 5 wrists\n            # but 2 elbows)\n            max_connections = min(len(joints_src), len(joints_dst))\n            # Traverse all potential joint connections (sorted by their score)\n            for potential_connection in connection_candidates:\n                i, j, s = potential_connection[0:3]\n                # Make sure joints_src[i] or joints_dst[j] haven\'t already been\n                # connected to other joints_dst or joints_src\n                if i not in connections[:, 3] and j not in connections[:, 4]:\n                    # [joint_src_id, joint_dst_id, limb_score_penalizing_long_dist, joint_src_index, joint_dst_index]\n                    connections = np.vstack(\n                        [connections, [joints_src[i][3], joints_dst[j][3], s, i, j]])\n                    # Exit if we\'ve already established max_connections\n                    # connections (each joint can\'t be connected to more than\n                    # one joint)\n                    if len(connections) >= max_connections:\n                        break\n            connected_limbs.append(connections)\n\n    return connected_limbs\n\n\ndef group_limbs_of_same_person(connected_limbs, joint_list):\n    """"""\n    Associate limbs belonging to the same person together.\n    :param connected_limbs: See \'return\' doc of find_connected_joints()\n    :param joint_list: unravel\'d version of joint_list_per_joint [See \'return\' doc of NMS()]\n    :return: 2d np.array of size num_people x (NUM_JOINTS+2). For each person found:\n    # First NUM_JOINTS columns contain the index (in joint_list) of the joints associated\n    with that person (or -1 if their i-th joint wasn\'t found)\n    # 2nd-to-last column: Overall score of the joints+limbs that belong to this person\n    # Last column: Total count of joints found for this person\n    """"""\n    person_to_joint_assoc = []\n\n    for limb_type in range(NUM_LIMBS):\n        joint_src_type, joint_dst_type = joint_to_limb_heatmap_relationship[limb_type]\n\n        for limb_info in connected_limbs[limb_type]:\n            person_assoc_idx = []\n            for person, person_limbs in enumerate(person_to_joint_assoc):\n                if person_limbs[joint_src_type] == limb_info[0] or person_limbs[joint_dst_type] == limb_info[1]:\n                    person_assoc_idx.append(person)\n\n            # If one of the joints has been associated to a person, and either\n            # the other joint is also associated with the same person or not\n            # associated to anyone yet:\n            if len(person_assoc_idx) == 1:\n                person_limbs = person_to_joint_assoc[person_assoc_idx[0]]\n                # If the other joint is not associated to anyone yet,\n                if person_limbs[joint_dst_type] != limb_info[1]:\n                    # Associate it with the current person\n                    person_limbs[joint_dst_type] = limb_info[1]\n                    # Increase the number of limbs associated to this person\n                    person_limbs[-1] += 1\n                    # And update the total score (+= heatmap score of joint_dst\n                    # + score of connecting joint_src with joint_dst)\n                    person_limbs[-2] += joint_list[limb_info[1]\n                                                   .astype(int), 2] + limb_info[2]\n            elif len(person_assoc_idx) == 2:  # if found 2 and disjoint, merge them\n                person1_limbs = person_to_joint_assoc[person_assoc_idx[0]]\n                person2_limbs = person_to_joint_assoc[person_assoc_idx[1]]\n                membership = ((person1_limbs >= 0) & (person2_limbs >= 0))[:-2]\n                if not membership.any():  # If both people have no same joints connected, merge them into a single person\n                    # Update which joints are connected\n                    person1_limbs[:-2] += (person2_limbs[:-2] + 1)\n                    # Update the overall score and total count of joints\n                    # connected by summing their counters\n                    person1_limbs[-2:] += person2_limbs[-2:]\n                    # Add the score of the current joint connection to the\n                    # overall score\n                    person1_limbs[-2] += limb_info[2]\n                    person_to_joint_assoc.pop(person_assoc_idx[1])\n                else:  # Same case as len(person_assoc_idx)==1 above\n                    person1_limbs[joint_dst_type] = limb_info[1]\n                    person1_limbs[-1] += 1\n                    person1_limbs[-2] += joint_list[limb_info[1]\n                                                    .astype(int), 2] + limb_info[2]\n            else:  # No person has claimed any of these joints, create a new person\n                # Initialize person info to all -1 (no joint associations)\n                row = -1 * np.ones(20)\n                # Store the joint info of the new connection\n                row[joint_src_type] = limb_info[0]\n                row[joint_dst_type] = limb_info[1]\n                # Total count of connected joints for this person: 2\n                row[-1] = 2\n                # Compute overall score: score joint_src + score joint_dst + score connection\n                # {joint_src,joint_dst}\n                row[-2] = sum(joint_list[limb_info[:2].astype(int), 2]\n                              ) + limb_info[2]\n                person_to_joint_assoc.append(row)\n\n    # Delete people who have very few parts connected\n    people_to_delete = []\n    for person_id, person_info in enumerate(person_to_joint_assoc):\n        if person_info[-1] < 3 or person_info[-2] / person_info[-1] < 0.2:\n            people_to_delete.append(person_id)\n    # Traverse the list in reverse order so we delete indices starting from the\n    # last one (otherwise, removing item for example 0 would modify the indices of\n    # the remaining people to be deleted!)\n    for index in people_to_delete[::-1]:\n        person_to_joint_assoc.pop(index)\n\n    # Appending items to a np.array can be very costly (allocating new memory, copying over the array, then adding new row)\n    # Instead, we treat the set of people as a list (fast to append items) and\n    # only convert to np.array at the end\n    return np.array(person_to_joint_assoc)\n\n\ndef plot_pose(img_orig, joint_list, person_to_joint_assoc, bool_fast_plot=True, plot_ear_to_shoulder=False):\n    canvas = img_orig.copy()  # Make a copy so we don\'t modify the original image\n\n    # to_plot is the location of all joints found overlaid on top of the\n    # original image\n    to_plot = canvas.copy() if bool_fast_plot else cv2.addWeighted(\n        img_orig, 0.3, canvas, 0.7, 0)\n\n    limb_thickness = 4\n    # Last 2 limbs connect ears with shoulders and this looks very weird.\n    # Disabled by default to be consistent with original rtpose output\n    which_limbs_to_plot = NUM_LIMBS if plot_ear_to_shoulder else NUM_LIMBS - 2\n    for limb_type in range(which_limbs_to_plot):\n        for person_joint_info in person_to_joint_assoc:\n            joint_indices = person_joint_info[joint_to_limb_heatmap_relationship[limb_type]].astype(\n                int)\n            if -1 in joint_indices:\n                # Only draw actual limbs (connected joints), skip if not\n                # connected\n                continue\n            # joint_coords[:,0] represents Y coords of both joints;\n            # joint_coords[:,1], X coords\n            joint_coords = joint_list[joint_indices, 0:2]\n            \n            for joint in joint_coords:  # Draw circles at every joint\n                cv2.circle(canvas, tuple(joint[0:2].astype(\n                    int)), 4, (255,255,255), thickness=-1)            \n            # mean along the axis=0 computes meanYcoord and meanXcoord -> Round\n            # and make int to avoid errors\n            coords_center = tuple(\n                np.round(np.mean(joint_coords, 0)).astype(int))\n            # joint_coords[0,:] is the coords of joint_src; joint_coords[1,:]\n            # is the coords of joint_dst\n            limb_dir = joint_coords[0, :] - joint_coords[1, :]\n            limb_length = np.linalg.norm(limb_dir)\n            # Get the angle of limb_dir in degrees using atan2(limb_dir_x,\n            # limb_dir_y)\n            angle = math.degrees(math.atan2(limb_dir[1], limb_dir[0]))\n\n            # For faster plotting, just plot over canvas instead of constantly\n            # copying it\n            cur_canvas = canvas if bool_fast_plot else canvas.copy()\n            polygon = cv2.ellipse2Poly(\n                coords_center, (int(limb_length / 2), limb_thickness), int(angle), 0, 360, 1)\n            cv2.fillConvexPoly(cur_canvas, polygon, colors[limb_type])\n            if not bool_fast_plot:\n                canvas = cv2.addWeighted(canvas, 0.4, cur_canvas, 0.6, 0)\n\n    return to_plot, canvas\n\n\ndef decode_pose(img_orig, param, heatmaps, pafs):\n    # Bottom-up approach:\n    # Step 1: find all joints in the image (organized by joint type: [0]=nose,\n    # [1]=neck...)\n    joint_list_per_joint_type = NMS(param,\n                                    heatmaps, img_orig.shape[0] / float(heatmaps.shape[0]))\n    # joint_list is an unravel\'d version of joint_list_per_joint, where we add\n    # a 5th column to indicate the joint_type (0=nose, 1=neck...)\n    joint_list = np.array([tuple(peak) + (joint_type,) for joint_type,\n                           joint_peaks in enumerate(joint_list_per_joint_type) for peak in joint_peaks])\n\n    # Step 2: find which joints go together to form limbs (which wrists go\n    # with which elbows)\n    paf_upsamp = cv2.resize(\n        pafs, (img_orig.shape[1], img_orig.shape[0]), interpolation=cv2.INTER_CUBIC)\n    connected_limbs = find_connected_joints(param,\n                                            paf_upsamp, joint_list_per_joint_type)\n\n    # Step 3: associate limbs that belong to the same person\n    person_to_joint_assoc = group_limbs_of_same_person(\n        connected_limbs, joint_list)\n\n    # (Step 4): plot results\n    to_plot, canvas = plot_pose(img_orig, joint_list, person_to_joint_assoc)\n\n    return to_plot, canvas, joint_list, person_to_joint_assoc\n'"
src/PoseEstimation/network/rtpose_cu_net.py,9,"b""# Zhiqiang Tang, Feb 2017\nimport torch\nimport torch.nn as nn\nimport math\nfrom collections import OrderedDict\nimport torch.utils.checkpoint as cp\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom torch.nn.parameter import Parameter\n\ndef _bn_function_factory(norm, relu, conv):\n    def bn_function(*inputs):\n        concated_features = torch.cat(inputs, 1)\n        bottleneck_output = conv(relu(norm(concated_features)))\n        return bottleneck_output\n\n    return bn_function\n\nclass _Adapter(nn.Module):\n    def __init__(self, num_input_features, num_output_features, efficient):\n        super(_Adapter, self).__init__()\n        self.add_module('adapter_norm', nn.BatchNorm2d(num_input_features))\n        self.add_module('adapter_relu', nn.ReLU(inplace=True))\n        self.add_module('adapter_conv', nn.Conv2d(num_input_features, num_output_features,\n                                                  kernel_size=1, stride=1, bias=False))\n        self.efficient = efficient\n    def forward(self, prev_features):\n        bn_function = _bn_function_factory(self.adapter_norm, self.adapter_relu,\n                                           self.adapter_conv)\n        if self.efficient and any(prev_feature.requires_grad for prev_feature in prev_features):\n            adapter_output = cp.checkpoint(bn_function, *prev_features)\n        else:\n            adapter_output = bn_function(*prev_features)\n\n        return adapter_output\n\n\nclass _DenseLayer(nn.Module):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, efficient=False):\n        super(_DenseLayer, self).__init__()\n        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n        self.add_module('relu1', nn.ReLU(inplace=True)),\n        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n                        growth_rate, kernel_size=1, stride=1, bias=False)),\n        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module('relu2', nn.ReLU(inplace=True)),\n        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                        kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = drop_rate\n        self.efficient = efficient\n\n    def forward(self, prev_features):\n        bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1)\n        # print type(prev_features), type(prev_features[0]), type(prev_features[0][0])\n        # for prev_feature in prev_features:\n        #     print 'prev_feature type: ', type(prev_feature)\n        #     print 'prev_feature size: ', prev_feature.size()\n        if self.efficient and any(prev_fea.requires_grad for prev_fea in prev_features):\n            bottleneck_output = cp.checkpoint(bn_function, *prev_features)\n        else:\n            bottleneck_output = bn_function(*prev_features)\n        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return new_features\n\nclass _DenseBlock(nn.Module):\n    def __init__(self, in_num, growth_rate, neck_size, layer_num, max_link,\n                 drop_rate=0, efficient=True, requires_skip=True, is_up=False):\n\n        self.saved_features = []\n        self.max_link = max_link\n        self.requires_skip = requires_skip\n        super(_DenseBlock, self).__init__()\n        max_in_num = in_num + max_link * growth_rate\n        self.final_num_features = max_in_num\n        self.layers = nn.ModuleList()\n        print('layer number is %d' % layer_num)\n        for i in range(0, layer_num):\n            if i < max_link:\n                tmp_in_num = in_num + i * growth_rate\n            else:\n                tmp_in_num = max_in_num\n            print('layer %d input channel number is %d' % (i, tmp_in_num))\n            self.layers.append(_DenseLayer(tmp_in_num, growth_rate=growth_rate,\n                                           bn_size=neck_size, drop_rate=drop_rate,\n                                           efficient=efficient))\n        # self.layers = nn.ModuleList(self.layers)\n\n        self.adapters_ahead = nn.ModuleList()\n        adapter_in_nums = []\n        adapter_out_num = in_num\n        if is_up:\n            adapter_out_num = adapter_out_num / 2\n        for i in range(0, layer_num):\n            if i < max_link:\n                tmp_in_num = in_num + (i+1) * growth_rate\n            else:\n                tmp_in_num = max_in_num + growth_rate\n            adapter_in_nums.append(tmp_in_num)\n            print('adapter %d input channel number is %d' % (i, adapter_in_nums[i]))\n            self.adapters_ahead.append(_Adapter(adapter_in_nums[i], adapter_out_num,\n                                                efficient=efficient))\n        # self.adapters_ahead = nn.ModuleList(self.adapters_ahead)\n        print('adapter output channel number is %d' % adapter_out_num)\n\n        if requires_skip:\n            print('creating skip layers ...')\n            self.adapters_skip = nn.ModuleList()\n            for i in range(0, layer_num):\n                self.adapters_skip.append(_Adapter(adapter_in_nums[i], adapter_out_num,\n                                                   efficient=efficient))\n            # self.adapters_skip = nn.ModuleList(self.adapters_skip)\n\n    def forward(self, x, i):\n        if i == 0:\n            self.saved_features = []\n\n        if type(x) is torch.Tensor:\n            x = [x]\n        if type(x) is not list:\n            raise Exception('type(x) should be list, but it is: ', type(x))\n        # for t_x in x:\n        #     print 't_x type: ', type(t_x)\n        #     print 't_x size: ', t_x.size()\n\n        x = x + self.saved_features\n        # for t_x in x:\n        #     print 't_x type: ', type(t_x)\n        #     print 't_x size: ', t_x.size()\n\n        out = self.layers[i](x)\n        if i < self.max_link:\n            self.saved_features.append(out)\n        elif len(self.saved_features) != 0:\n            self.saved_features.pop(0)\n            self.saved_features.append(out)\n        x.append(out)\n        out_ahead = self.adapters_ahead[i](x)\n        if self.requires_skip:\n            out_skip = self.adapters_skip[i](x)\n            return out_ahead, out_skip\n        else:\n            return out_ahead\n\nclass _IntermediaBlock(nn.Module):\n    def __init__(self, in_num, out_num, layer_num, max_link, efficient=True):\n\n        max_in_num = in_num + max_link * out_num\n        self.final_num_features = max_in_num\n        self.saved_features = []\n        self.max_link = max_link\n        super(_IntermediaBlock, self).__init__()\n        print('creating intermedia block ...')\n        self.adapters = nn.ModuleList()\n        for i in range(0, layer_num-1):\n            if i < max_link:\n                tmp_in_num = in_num + (i+1) * out_num\n            else:\n                tmp_in_num = max_in_num\n            print('intermedia layer %d input channel number is %d' % (i, tmp_in_num))\n            self.adapters.append(_Adapter(tmp_in_num, out_num, efficient=efficient))\n        # self.adapters = nn.ModuleList(self.adapters)\n        print('intermedia layer output channel number is %d' % out_num)\n\n    def forward(self, x, i):\n        if i == 0:\n            self.saved_features = []\n            if type(x) is torch.Tensor:\n                if self.max_link != 0:\n                    self.saved_features.append(x)\n            elif type(x) is list:\n                if self.max_link != 0:\n                    self.saved_features = self.saved_features + x\n            return x\n\n        if type(x) is torch.Tensor:\n            x = [x]\n        if type(x) is not list:\n            raise Exception('type(x) should be list, but it is: ', type(x))\n\n        x = x + self.saved_features\n        out = self.adapters[i-1](x)\n        if i < self.max_link:\n            self.saved_features.append(out)\n        elif len(self.saved_features) != 0:\n            self.saved_features.pop(0)\n            self.saved_features.append(out)\n        # print('middle list length is %d' % len(self.saved_features))\n        return out\n\nclass _Bn_Relu_Conv1x1(nn.Sequential):\n    def __init__(self, in_num, out_num):\n        super(_Bn_Relu_Conv1x1, self).__init__()\n        self.add_module('norm', nn.BatchNorm2d(in_num))\n        self.add_module('relu', nn.ReLU(inplace=True))\n        self.add_module('conv', nn.Conv2d(in_num, out_num, kernel_size=1,\n                                          stride=1, bias=False))\n\n# class _TransitionDown(nn.Module):\n#     def __init__(self, in_num_list, out_num, num_units):\n#         super(_TransitionDown, self).__init__()\n#         self.adapters = []\n#         for i in range(0, num_units):\n#             self.adapters.append(_Bn_Relu_Conv1x1(in_num=in_num_list[i], out_num=out_num))\n#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n#\n#     def forward(self, x, i):\n#         x = self.adapters[i](x)\n#         out = self.pool(x)\n#         return out\n#\n# class _TransitionUp(nn.Module):\n#     def __init__(self, in_num_list, out_num_list, num_units):\n#         super(_TransitionUp, self).__init__()\n#         self.adapters = []\n#         for i in range(0, num_units):\n#             self.adapters.append(_Bn_Relu_Conv1x1(in_num=in_num_list[i], out_num=out_num_list[i]))\n#         self.upsample = nn.UpsamplingNearest2d(scale_factor=2)\n#\n#     def forward(self, x, i):\n#         x = self.adapters[i](x)\n#         out = self.upsample(x)\n#         return out\n\n\nclass _CU_Net(nn.Module):\n    def __init__(self, in_num, neck_size, growth_rate, layer_num, max_link):\n        super(_CU_Net, self).__init__()\n        self.down_blocks = nn.ModuleList()\n        self.up_blocks = nn.ModuleList()\n        self.num_blocks = 4\n        print('creating hg ...')\n        for i in range(0, self.num_blocks):\n            print('creating down block %d ...' % i)\n            self.down_blocks.append(_DenseBlock(in_num=in_num, neck_size=neck_size,\n                                    growth_rate=growth_rate, layer_num=layer_num,\n                                    max_link=max_link, requires_skip=True))\n            print('creating up block %d ...' % i)\n            self.up_blocks.append(_DenseBlock(in_num=in_num*2, neck_size=neck_size,\n                                  growth_rate=growth_rate, layer_num=layer_num,\n                                  max_link=max_link, requires_skip=False, is_up=True))\n        # self.down_blocks = nn.ModuleList(self.down_blocks)\n        # self.up_blocks = nn.ModuleList(self.up_blocks)\n        print('creating neck block ...')\n        self.neck_block = _DenseBlock(in_num=in_num, neck_size=neck_size,\n                                      growth_rate=growth_rate, layer_num=layer_num,\n                                      max_link=max_link, requires_skip=False)\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.upsample = nn.Upsample(scale_factor=2)\n\n    def forward(self, x, i):\n        skip_list = [None] * self.num_blocks\n        # print 'input x size is ', x.size()\n        for j in range(0, self.num_blocks):\n            # print('using down block %d ...' % j)\n            x, skip_list[j] = self.down_blocks[j](x, i)\n            # print 'output size is ', x.size()\n            # print 'skip size is ', skip_list[j].size()\n            x = self.maxpool(x)\n        # print('using neck block ...')\n        x = self.neck_block(x, i)\n        # print 'output size is ', x.size()\n        for j in list(reversed(range(0, self.num_blocks))):\n            x = self.upsample(x)\n            # print('using up block %d ...' % j)\n            x = self.up_blocks[j]([x, skip_list[j]], i)\n            # print 'output size is ', x.size()\n        return x\n\nclass _CU_Net_Wrapper(nn.Module):\n    def __init__(self, init_chan_num, neck_size, growth_rate,\n                 class_num, layer_num, order, loss_num):\n        assert loss_num <= layer_num and loss_num >= 1\n        loss_every = float(layer_num) / float(loss_num)\n        self.loss_achors = []\n        for i in range(0, loss_num):\n            tmp_achor = int(round(loss_every * (i+1)))\n            if tmp_achor <= layer_num:\n                self.loss_achors.append(tmp_achor)\n\n        assert layer_num in self.loss_achors\n        assert loss_num == len(self.loss_achors)\n\n        if order >= layer_num:\n            print 'order is larger than the layer number.'\n            exit()\n        print('layer number is %d' % layer_num)\n        print('loss number is %d' % loss_num)\n        print('loss achors are: ', self.loss_achors)\n        print('order is %d' % order)\n        print('growth rate is %d' % growth_rate)\n        print('neck size is %d' % neck_size)\n        print('class number is %d' % class_num)\n        print('initial channel number is %d' % init_chan_num)\n        num_chans = init_chan_num\n        super(_CU_Net_Wrapper, self).__init__()\n        self.layer_num = layer_num\n        self.features = nn.Sequential(OrderedDict([\n            ('conv0', nn.Conv2d(3, init_chan_num, kernel_size=7, stride=2, padding=3, bias=False)),\n            ('norm0', nn.BatchNorm2d(init_chan_num)),\n            ('relu0', nn.ReLU(inplace=True)),\n            ('pool0', nn.MaxPool2d(kernel_size=2, stride=2)),\n        ]))\n        # self.denseblock0 = _DenseBlock(layer_num=4, in_num=init_chan_num,\n        #                                neck_size=neck_size, growth_rate=growth_rate)\n        # hg_in_num = init_chan_num + growth_rate * 4\n        print('channel number is %d' % num_chans)\n        self.hg = _CU_Net(in_num=num_chans, neck_size=neck_size, growth_rate=growth_rate,\n                             layer_num=layer_num, max_link=order)\n\n        self.linears = nn.ModuleList()\n        for i in range(0, layer_num):\n            self.linears.append(_Bn_Relu_Conv1x1(in_num=num_chans, out_num=class_num))\n        # self.linears = nn.ModuleList(self.linears)\n        # intermedia_in_nums = []\n        # for i in range(0, num_units-1):\n        #     intermedia_in_nums.append(num_chans * (i+2))\n        self.intermedia = _IntermediaBlock(in_num=num_chans, out_num=num_chans,\n                                           layer_num=layer_num, max_link=order)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n                stdv = 1/math.sqrt(n)\n                m.weight.data.uniform_(-stdv, stdv)\n                # m.weight.data.zero_()\n                if m.bias is not None:\n                    m.bias.data.uniform_(-stdv, stdv)\n                    # m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.uniform_()\n                # m.weight.data.zero_()\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        # print(x.size())\n        x = self.features(x)\n        # print(x.size())\n        # x = self.denseblock0(x)\n        # print 'x size is', x.size()\n        out = []\n        # middle = []\n        # middle.append(x)\n        for i in range(0, self.layer_num):\n            # print('using intermedia layer %d ...' % i)\n            x = self.intermedia(x, i)\n            # print 'x size after intermedia layer is ', x.size()\n            # print('using hg %d ...' % i)\n            x = self.hg(x, i)\n            # print 'x size after hg is ', x.size()\n            # middle.append(x)\n            if (i+1) in self.loss_achors:\n                tmp_out = self.linears[i](x)\n                # print 'tmp output size is ', tmp_out.size()\n                out.append(tmp_out)\n            # if i < self.num_units-1:\n        # exit()\n        assert len(self.loss_achors) == len(out)\n        return out\n\ndef create_cu_net(neck_size, growth_rate, init_chan_num,\n                  class_num, layer_num, order, loss_num):\n\n    net = _CU_Net_Wrapper(init_chan_num=init_chan_num, neck_size=neck_size,\n                          growth_rate=growth_rate, class_num=class_num,\n                          layer_num=layer_num, order=order, loss_num=loss_num)\n    return net\n\n"""
src/PoseEstimation/network/rtpose_hourglass.py,3,"b""import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\nNUM_JOINTS = 18\nNUM_LIMBS = 38\nclass Bottleneck(nn.Module):\n    expansion = 2\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=True)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=True)\n        self.bn3 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 2, kernel_size=1, bias=True)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        out = self.bn3(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n\n        return out\n\n\nclass Hourglass(nn.Module):\n    def __init__(self, block, num_blocks, planes, depth):\n        super(Hourglass, self).__init__()\n        self.depth = depth\n        self.block = block\n        self.upsample = nn.Upsample(scale_factor=2)\n        self.hg = self._make_hour_glass(block, num_blocks, planes, depth)\n\n    def _make_residual(self, block, num_blocks, planes):\n        layers = []\n        for i in range(0, num_blocks):\n            layers.append(block(planes * block.expansion, planes))\n        return nn.Sequential(*layers)\n\n    def _make_hour_glass(self, block, num_blocks, planes, depth):\n        hg = []\n        for i in range(depth):\n            res = []\n            for j in range(3):\n                res.append(self._make_residual(block, num_blocks, planes))\n            if i == 0:\n                res.append(self._make_residual(block, num_blocks, planes))\n            hg.append(nn.ModuleList(res))\n        return nn.ModuleList(hg)\n\n    def _hour_glass_forward(self, n, x):\n        up1 = self.hg[n - 1][0](x)\n        low1 = F.max_pool2d(x, 2, stride=2)\n        low1 = self.hg[n - 1][1](low1)\n\n        if n > 1:\n            low2 = self._hour_glass_forward(n - 1, low1)\n        else:\n            low2 = self.hg[n - 1][3](low1)\n        low3 = self.hg[n - 1][2](low2)\n        up2 = self.upsample(low3)\n        out = up1 + up2\n        return out\n\n    def forward(self, x):\n        return self._hour_glass_forward(self.depth, x)\n\n\nclass HourglassNet(nn.Module):\n    '''Hourglass model from Newell et al ECCV 2016'''\n\n    def __init__(self, block, num_stacks=2, num_blocks=4, paf_classes=NUM_LIMBS*2, ht_classes=NUM_JOINTS+1):\n        super(HourglassNet, self).__init__()\n\n        self.inplanes = 64\n        self.num_feats = 128\n        self.num_stacks = num_stacks\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=True)\n        self.bn1 = nn.BatchNorm2d(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_residual(block, self.inplanes, 1)\n        self.layer2 = self._make_residual(block, self.inplanes, 1)\n        self.layer3 = self._make_residual(block, self.num_feats, 1)\n        self.maxpool = nn.MaxPool2d(2, stride=2)\n\n        # build hourglass modules\n        ch = self.num_feats * block.expansion\n        hg, res, fc, score_paf, score_ht, fc_, paf_score_, ht_score_ = \\\n        [], [], [], [], [], [], [], []\n        for i in range(num_stacks):\n            hg.append(Hourglass(block, num_blocks, self.num_feats, 4))\n            res.append(self._make_residual(block, self.num_feats, num_blocks))\n            fc.append(self._make_fc(ch, ch))\n            score_paf.append(nn.Conv2d(ch, paf_classes, kernel_size=1, bias=True))\n            score_ht.append(nn.Conv2d(ch, ht_classes, kernel_size=1, bias=True))            \n            if i < num_stacks - 1:\n                fc_.append(nn.Conv2d(ch, ch, kernel_size=1, bias=True))\n                paf_score_.append(nn.Conv2d(paf_classes, ch,\n                                        kernel_size=1, bias=True))\n                ht_score_.append(nn.Conv2d(ht_classes, ch,\n                                        kernel_size=1, bias=True))                                        \n        self.hg = nn.ModuleList(hg)\n        self.res = nn.ModuleList(res)\n        self.fc = nn.ModuleList(fc)\n        self.score_ht = nn.ModuleList(score_ht)\n        self.score_paf = nn.ModuleList(score_paf)        \n        self.fc_ = nn.ModuleList(fc_)\n        self.paf_score_ = nn.ModuleList(paf_score_)\n        self.ht_score_ = nn.ModuleList(ht_score_)\n        \n        self._initialize_weights_norm()        \n        \n    def _make_residual(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=True),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_fc(self, inplanes, outplanes):\n        bn = nn.BatchNorm2d(inplanes)\n        conv = nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=True)\n        return nn.Sequential(\n            conv,\n            bn,\n            self.relu,\n        )\n\n    def forward(self, x):\n        saved_for_loss = []    \n        out = []\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.maxpool(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        for i in range(self.num_stacks):\n            y = self.hg[i](x)\n            y = self.res[i](y)\n            y = self.fc[i](y)\n            score_paf = self.score_paf[i](y)\n            score_ht = self.score_ht[i](y)            \n            if i < self.num_stacks - 1:\n                fc_ = self.fc_[i](y)\n                paf_score_ = self.paf_score_[i](score_paf)\n                ht_score_ = self.ht_score_[i](score_ht)                \n                x = x + fc_ + paf_score_ + ht_score_\n                \n        saved_for_loss.append(score_paf)\n        saved_for_loss.append(score_ht)\n\n        return (score_paf, score_ht), saved_for_loss\n\n    def _initialize_weights_norm(self):        \n       for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.normal_(m.weight, std=0.01)\n                if m.bias is not None:  # mobilenet conv2d doesn't add bias\n                    init.constant_(m.bias, 0.0) \n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\ndef hg(**kwargs):\n    model = HourglassNet(Bottleneck, num_stacks=kwargs['num_stacks'], \n    num_blocks=kwargs['num_blocks'], paf_classes=kwargs['paf_classes'], \n    ht_classes=kwargs['ht_classes'])\n    return model\n"""
src/PoseEstimation/network/rtpose_mobilenetV2.py,1,"b'import torch.nn as nn\nimport math\n\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n        interverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        assert input_size % 32 == 0\n        input_channel = int(input_channel * width_mult)\n        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n        self.features = [conv_bn(3, input_channel, 2)]\n        # building inverted residual blocks\n        for t, c, n, s in interverted_residual_setting:\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                if i == 0:\n                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n                else:\n                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, n_class),\n        )\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.mean(3).mean(2)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n'"
src/PoseEstimation/network/rtpose_shufflenetV2.py,9,"b""#!/usr/bin/env python\n# encoding: utf-8\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\n\ntry:\n    import caffe\n    from caffe import layers as L\n    from caffe import params as P\nexcept ImportError:\n    pass\n\nfrom network import slim\nfrom network.slim import g_name\n\n\nclass BasicBlock(nn.Module):\n\n    def __init__(self, name, in_channels, out_channels, stride, downsample, dilation):\n        super(BasicBlock, self).__init__()\n        self.g_name = name\n        self.in_channels = in_channels\n        self.stride = stride\n        self.downsample = downsample\n        channels = out_channels//2\n        if not self.downsample and self.stride==1:\n            assert in_channels == out_channels\n            self.conv = nn.Sequential(\n                slim.conv_bn_relu(name + '/conv1', channels, channels, 1),\n                slim.conv_bn(name + '/conv2', \n                    channels, channels, 3, stride=stride, \n                    dilation=dilation, padding=dilation, groups=channels),\n                slim.conv_bn_relu(name + '/conv3', channels, channels, 1),\n            )\n        else:\n            self.conv = nn.Sequential(\n                slim.conv_bn_relu(name + '/conv1', in_channels, channels, 1),\n                slim.conv_bn(name + '/conv2', \n                    channels, channels, 3, stride=stride, \n                    dilation=dilation, padding=dilation, groups=channels),\n                slim.conv_bn_relu(name + '/conv3', channels, channels, 1),\n            )\n            self.conv0 = nn.Sequential(\n                slim.conv_bn(name + '/conv4', \n                    in_channels, in_channels, 3, stride=stride, \n                    dilation=dilation, padding=dilation, groups=in_channels),\n                slim.conv_bn_relu(name + '/conv5', in_channels, channels, 1),\n            )\n        self.shuffle = slim.channel_shuffle(name + '/shuffle', 2)\n\n    def forward(self, x):\n        if not self.downsample:\n            x1 = x[:, :(x.shape[1]//2), :, :]\n            x2 = x[:, (x.shape[1]//2):, :, :]\n            x = torch.cat((x1, self.conv(x2)), 1)\n        else:\n            x = torch.cat((self.conv0(x), self.conv(x)), 1)\n        return self.shuffle(x)\n\n    def generate_caffe_prototxt(self, caffe_net, layer):\n        if self.stride == 1:\n            layer_x1, layer_x2 = L.Slice(layer, ntop=2, axis=1, slice_point=[self.in_channels//2])\n            caffe_net[self.g_name + '/slice1'] = layer_x1\n            caffe_net[self.g_name + '/slice2'] = layer_x2\n            layer_x2 = slim.generate_caffe_prototxt(self.conv, caffe_net, layer_x2)\n        else:\n            layer_x1 = slim.generate_caffe_prototxt(self.conv0, caffe_net, layer)\n            layer_x2 = slim.generate_caffe_prototxt(self.conv, caffe_net, layer)\n        layer = L.Concat(layer_x1, layer_x2, axis=1)\n        caffe_net[self.g_name + '/concat'] = layer\n        layer = slim.generate_caffe_prototxt(self.shuffle, caffe_net, layer)\n        return layer\n\n\nclass Network(nn.Module):\n\n    def __init__(self, width_multiplier):\n        super(Network, self).__init__()\n        width_config = {\n            0.25: (24, 48, 96, 512),\n            0.33: (32, 64, 128, 512),\n            0.5: (48, 96, 192, 1024),\n            1.0: (116, 232, 464, 1024),\n            1.5: (176, 352, 704, 1024),\n            2.0: (244, 488, 976, 2048),\n        }\n        width_config = width_config[width_multiplier]\n        in_channels = 24\n\n        # outputs, stride, dilation, blocks, type\n        self.network_config = [\n            g_name('data/bn', nn.BatchNorm2d(3)),\n            slim.conv_bn_relu('stage1/conv', 3, in_channels, 3, 2, 1),\n            g_name('stage1/pool', nn.MaxPool2d(3, 2, 0, ceil_mode=True)),\n            (width_config[0], 2, 1, 4, 'b'),\n            (width_config[1], 1, 1, 8, 'b'), # x16\n            (width_config[2], 1, 1, 4, 'b'), # x32\n            slim.conv_bn_relu('conv5', width_config[2], width_config[3], 1)\n        ]\n        self.paf = nn.Conv2d(width_config[3], 38, 1)\n        self.heatmap = nn.Conv2d(width_config[3], 19, 1)\n        self.network = []\n        for i, config in enumerate(self.network_config):\n            if isinstance(config, nn.Module):\n                self.network.append(config)\n                continue\n            out_channels, stride, dilation, num_blocks, stage_type = config\n            if stride==2:\n                downsample=True\n            stage_prefix = 'stage_{}'.format(i - 1)\n            blocks = [BasicBlock(stage_prefix + '_1', in_channels, \n                out_channels, stride, downsample, dilation)]\n            for i in range(1, num_blocks):\n                blocks.append(BasicBlock(stage_prefix + '_{}'.format(i + 1), \n                    out_channels, out_channels, 1, False, dilation))\n            self.network += [nn.Sequential(*blocks)]\n\n            in_channels = out_channels\n        self.network = nn.Sequential(*self.network)\n\n        for name, m in self.named_modules():\n            if any(map(lambda x: isinstance(m, x), [nn.Linear, nn.Conv1d, nn.Conv2d])):\n                nn.init.kaiming_uniform_(m.weight, mode='fan_in')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def trainable_parameters(self):\n        parameters = [\n            {'params': self.cls_head_list.parameters(), 'lr_mult': 1.0},\n            {'params': self.loc_head_list.parameters(), 'lr_mult': 1.0}\n        ]\n        for i in range(len(self.network)):\n            lr_mult = 0.1 if i in (0, 1, 2, 3, 4, 5) else 1\n            parameters.append(\n                {'params': self.network[i].parameters(), 'lr_mult': lr_mult}\n            )\n        return parameters\n\n    def forward(self, x):\n        x = self.network(x)\n        PAF = self.paf(x)\n        HEAT = self.heatmap(x)\n        return [PAF, HEAT], [PAF, HEAT]\n\n    def generate_caffe_prototxt(self, caffe_net, layer):\n        data_layer = layer\n        network = slim.generate_caffe_prototxt(self.network, caffe_net, data_layer)\n        return network\n\n    def convert_to_caffe(self, name):\n        caffe_net = caffe.NetSpec()\n        layer = L.Input(shape=dict(dim=[1, 3, args.image_hw, args.image_hw]))\n        caffe_net.tops['data'] = layer\n        slim.generate_caffe_prototxt(self, caffe_net, layer)\n        print(caffe_net.to_proto())\n        with open(name + '.prototxt', 'wb') as f:\n            f.write(str(caffe_net.to_proto()).encode())\n        caffe_net = caffe.Net(name + '.prototxt', caffe.TEST)\n        slim.convert_pytorch_to_caffe(self, caffe_net)\n        caffe_net.save(name + '.caffemodel')\n\n\nif __name__ == '__main__':\n    import sys\n    import argparse\n    import PIL.Image\n    import torchvision\n    import numpy as np\n\n    def assert_diff(a, b):\n        if isinstance(a, torch.Tensor):\n            a = a.detach().cpu().numpy()\n        if isinstance(b, torch.Tensor):\n            b = b.detach().cpu().numpy()\n        print(a.shape, b.shape)\n        a = a.reshape(-1)\n        b = b.reshape(-1)\n        assert a.shape == b.shape\n        diff = np.abs(a - b)\n        print('mean diff = %f' % diff.mean())\n        assert diff.mean() < 0.001\n        print('max diff = %f' % diff.max())\n        assert diff.max() < 0.001\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--image_hw', type=int, default=224)\n    parser.add_argument('--num_classes', type=int, default=1000)\n    parser.add_argument('--model_width', type=float, default=0.5)\n    parser.add_argument('--load_pytorch', type=str)\n    parser.add_argument('--save_pytorch', type=str)\n    parser.add_argument('--save_caffe', type=str)\n    parser.add_argument('--test', type=str)\n    args = parser.parse_args()\n\n    if args.test is None:\n        img = np.random.rand(1, 3, args.image_hw, args.image_hw)\n        # img = np.ones((1, 3, args.image_hw, args.image_hw))\n    else:\n        img = PIL.Image.open(args.test).convert('RGB')\n        img = torchvision.transforms.functional.resize(img, (args.image_hw, args.image_hw))\n        img = torchvision.transforms.functional.to_tensor(img).unsqueeze(0).numpy()\n\n    ##############################################\n    # Initilize a PyTorch model.\n    net = Network(args.num_classes, args.model_width).train(False)\n    print(net)\n    if args.load_pytorch is not None:\n        net.load_state_dict(torch.load(args.load_pytorch, map_location=lambda storage, loc: storage))\n    x = torch.tensor(img.copy(), dtype=torch.float32)\n    with torch.no_grad():\n        cls_results = net(x)\n    print(cls_results.shape)\n    if args.save_pytorch is not None:\n        torch.save(net.state_dict(), args.save_pytorch + '.pth')\n\n    ##############################################\n    # Caffe model generation and converting.\n    if args.save_caffe is not None:\n        net.convert_to_caffe(args.save_caffe)\n        caffe_net = caffe.Net(args.save_caffe + '.prototxt', caffe.TEST, \n            weights=(args.save_caffe + '.caffemodel'))\n        caffe_net.blobs['data'].data[...] = img.copy()\n        caffe_results = caffe_net.forward(blobs=['fc'])\n        cls_results_caffe = caffe_results['fc']\n        print(cls_results_caffe.shape)\n        assert_diff(cls_results, cls_results_caffe)\n"""
src/PoseEstimation/network/rtpose_vgg.py,14,"b'""""""CPM Pytorch Implementation""""""\n\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport torch.utils.model_zoo as model_zoo\nfrom torch.autograd import Variable\nfrom torch.nn import init\n\ndef make_stages(cfg_dict):\n    """"""Builds CPM stages from a dictionary\n    Args:\n        cfg_dict: a dictionary\n    """"""\n    layers = []\n    for i in range(len(cfg_dict) - 1):\n        one_ = cfg_dict[i]\n        for k, v in one_.items():\n            if \'pool\' in k:\n                layers += [nn.MaxPool2d(kernel_size=v[0], stride=v[1],\n                                        padding=v[2])]\n            else:\n                conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1],\n                                   kernel_size=v[2], stride=v[3],\n                                   padding=v[4])\n                layers += [conv2d, nn.ReLU(inplace=True)]\n    one_ = list(cfg_dict[-1].keys())\n    k = one_[0]\n    v = cfg_dict[-1][k]\n    conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1],\n                       kernel_size=v[2], stride=v[3], padding=v[4])\n    layers += [conv2d]\n    return nn.Sequential(*layers)\n\n\ndef make_vgg19_block(block):\n    """"""Builds a vgg19 block from a dictionary\n    Args:\n        block: a dictionary\n    """"""\n    layers = []\n    for i in range(len(block)):\n        one_ = block[i]\n        for k, v in one_.items():\n            if \'pool\' in k:\n                layers += [nn.MaxPool2d(kernel_size=v[0], stride=v[1],\n                                        padding=v[2])]\n            else:\n                conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1],\n                                   kernel_size=v[2], stride=v[3],\n                                   padding=v[4])\n                layers += [conv2d, nn.ReLU(inplace=True)]\n    return nn.Sequential(*layers)\n\n\n\ndef get_model(trunk=\'vgg19\'):\n    """"""Creates the whole CPM model\n    Args:\n        trunk: string, \'vgg19\' or \'mobilenet\'\n    Returns: Module, the defined model\n    """"""\n    blocks = {}\n    # block0 is the preprocessing stage\n    if trunk == \'vgg19\':\n        block0 = [{\'conv1_1\': [3, 64, 3, 1, 1]},\n                  {\'conv1_2\': [64, 64, 3, 1, 1]},\n                  {\'pool1_stage1\': [2, 2, 0]},\n                  {\'conv2_1\': [64, 128, 3, 1, 1]},\n                  {\'conv2_2\': [128, 128, 3, 1, 1]},\n                  {\'pool2_stage1\': [2, 2, 0]},\n                  {\'conv3_1\': [128, 256, 3, 1, 1]},\n                  {\'conv3_2\': [256, 256, 3, 1, 1]},\n                  {\'conv3_3\': [256, 256, 3, 1, 1]},\n                  {\'conv3_4\': [256, 256, 3, 1, 1]},\n                  {\'pool3_stage1\': [2, 2, 0]},\n                  {\'conv4_1\': [256, 512, 3, 1, 1]},\n                  {\'conv4_2\': [512, 512, 3, 1, 1]},\n                  {\'conv4_3_CPM\': [512, 256, 3, 1, 1]},\n                  {\'conv4_4_CPM\': [256, 128, 3, 1, 1]}]\n\n    elif trunk == \'mobilenet\':\n        block0 = [{\'conv_bn\': [3, 32, 2]},  # out: 3, 32, 184, 184\n                  {\'conv_dw1\': [32, 64, 1]},  # out: 32, 64, 184, 184\n                  {\'conv_dw2\': [64, 128, 2]},  # out: 64, 128, 92, 92\n                  {\'conv_dw3\': [128, 128, 1]},  # out: 128, 256, 92, 92\n                  {\'conv_dw4\': [128, 256, 2]},  # out: 256, 256, 46, 46\n                  {\'conv4_3_CPM\': [256, 256, 1, 3, 1]},\n                  {\'conv4_4_CPM\': [256, 128, 1, 3, 1]}]\n\n    # Stage 1\n    blocks[\'block1_1\'] = [{\'conv5_1_CPM_L1\': [128, 128, 3, 1, 1]},\n                          {\'conv5_2_CPM_L1\': [128, 128, 3, 1, 1]},\n                          {\'conv5_3_CPM_L1\': [128, 128, 3, 1, 1]},\n                          {\'conv5_4_CPM_L1\': [128, 512, 1, 1, 0]},\n                          {\'conv5_5_CPM_L1\': [512, 38, 1, 1, 0]}]\n\n    blocks[\'block1_2\'] = [{\'conv5_1_CPM_L2\': [128, 128, 3, 1, 1]},\n                          {\'conv5_2_CPM_L2\': [128, 128, 3, 1, 1]},\n                          {\'conv5_3_CPM_L2\': [128, 128, 3, 1, 1]},\n                          {\'conv5_4_CPM_L2\': [128, 512, 1, 1, 0]},\n                          {\'conv5_5_CPM_L2\': [512, 19, 1, 1, 0]}]\n\n    # Stages 2 - 6\n    for i in range(2, 7):\n        blocks[\'block%d_1\' % i] = [\n            {\'Mconv1_stage%d_L1\' % i: [185, 128, 7, 1, 3]},\n            {\'Mconv2_stage%d_L1\' % i: [128, 128, 7, 1, 3]},\n            {\'Mconv3_stage%d_L1\' % i: [128, 128, 7, 1, 3]},\n            {\'Mconv4_stage%d_L1\' % i: [128, 128, 7, 1, 3]},\n            {\'Mconv5_stage%d_L1\' % i: [128, 128, 7, 1, 3]},\n            {\'Mconv6_stage%d_L1\' % i: [128, 128, 1, 1, 0]},\n            {\'Mconv7_stage%d_L1\' % i: [128, 38, 1, 1, 0]}\n        ]\n\n        blocks[\'block%d_2\' % i] = [\n            {\'Mconv1_stage%d_L2\' % i: [185, 128, 7, 1, 3]},\n            {\'Mconv2_stage%d_L2\' % i: [128, 128, 7, 1, 3]},\n            {\'Mconv3_stage%d_L2\' % i: [128, 128, 7, 1, 3]},\n            {\'Mconv4_stage%d_L2\' % i: [128, 128, 7, 1, 3]},\n            {\'Mconv5_stage%d_L2\' % i: [128, 128, 7, 1, 3]},\n            {\'Mconv6_stage%d_L2\' % i: [128, 128, 1, 1, 0]},\n            {\'Mconv7_stage%d_L2\' % i: [128, 19, 1, 1, 0]}\n        ]\n\n    models = {}\n\n    if trunk == \'vgg19\':\n        print(""Bulding VGG19"")\n        models[\'block0\'] = make_vgg19_block(block0)\n\n    for k, v in blocks.items():\n        models[k] = make_stages(list(v))\n\n    class rtpose_model(nn.Module):\n        def __init__(self, model_dict):\n            super(rtpose_model, self).__init__()\n            self.model0 = model_dict[\'block0\']\n            self.model1_1 = model_dict[\'block1_1\']\n            self.model2_1 = model_dict[\'block2_1\']\n            self.model3_1 = model_dict[\'block3_1\']\n            self.model4_1 = model_dict[\'block4_1\']\n            self.model5_1 = model_dict[\'block5_1\']\n            self.model6_1 = model_dict[\'block6_1\']\n\n            self.model1_2 = model_dict[\'block1_2\']\n            self.model2_2 = model_dict[\'block2_2\']\n            self.model3_2 = model_dict[\'block3_2\']\n            self.model4_2 = model_dict[\'block4_2\']\n            self.model5_2 = model_dict[\'block5_2\']\n            self.model6_2 = model_dict[\'block6_2\']\n\n            self._initialize_weights_norm()\n\n        def forward(self, x):\n\n            saved_for_loss = []\n            out1 = self.model0(x)\n\n            out1_1 = self.model1_1(out1)\n            out1_2 = self.model1_2(out1)\n            out2 = torch.cat([out1_1, out1_2, out1], 1)\n            saved_for_loss.append(out1_1)\n            saved_for_loss.append(out1_2)\n\n            out2_1 = self.model2_1(out2)\n            out2_2 = self.model2_2(out2)\n            out3 = torch.cat([out2_1, out2_2, out1], 1)\n            saved_for_loss.append(out2_1)\n            saved_for_loss.append(out2_2)\n\n            out3_1 = self.model3_1(out3)\n            out3_2 = self.model3_2(out3)\n            out4 = torch.cat([out3_1, out3_2, out1], 1)\n            saved_for_loss.append(out3_1)\n            saved_for_loss.append(out3_2)\n\n            out4_1 = self.model4_1(out4)\n            out4_2 = self.model4_2(out4)\n            out5 = torch.cat([out4_1, out4_2, out1], 1)\n            saved_for_loss.append(out4_1)\n            saved_for_loss.append(out4_2)\n\n            out5_1 = self.model5_1(out5)\n            out5_2 = self.model5_2(out5)\n            out6 = torch.cat([out5_1, out5_2, out1], 1)\n            saved_for_loss.append(out5_1)\n            saved_for_loss.append(out5_2)\n\n            out6_1 = self.model6_1(out6)\n            out6_2 = self.model6_2(out6)\n            saved_for_loss.append(out6_1)\n            saved_for_loss.append(out6_2)\n\n            return (out6_1, out6_2), saved_for_loss\n\n        def _initialize_weights_norm(self):\n\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    init.normal(m.weight, std=0.01)\n                    if m.bias is not None:  # mobilenet conv2d doesn\'t add bias\n                        init.constant(m.bias, 0.0)\n\n            # last layer of these block don\'t have Relu\n            init.normal(self.model1_1[8].weight, std=0.01)\n            init.normal(self.model1_2[8].weight, std=0.01)\n\n            init.normal(self.model2_1[12].weight, std=0.01)\n            init.normal(self.model3_1[12].weight, std=0.01)\n            init.normal(self.model4_1[12].weight, std=0.01)\n            init.normal(self.model5_1[12].weight, std=0.01)\n            init.normal(self.model6_1[12].weight, std=0.01)\n\n            init.normal(self.model2_2[12].weight, std=0.01)\n            init.normal(self.model3_2[12].weight, std=0.01)\n            init.normal(self.model4_2[12].weight, std=0.01)\n            init.normal(self.model5_2[12].weight, std=0.01)\n            init.normal(self.model6_2[12].weight, std=0.01)\n\n    model = rtpose_model(models)\n    return model\n\n\n""""""Load pretrained model on Imagenet\n:param model, the PyTorch nn.Module which will train.\n:param model_path, the directory which load the pretrained model, will download one if not have.\n:param trunk, the feature extractor network of model.               \n""""""\n\n\ndef use_vgg(model, model_path, trunk):\n    model_urls = {\n        \'vgg16\': \'https://download.pytorch.org/models/vgg16-397923af.pth\',\n        \'ssd\': \'https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth\',\n        \'vgg19\': \'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\'}\n\n    number_weight = {\n        \'vgg16\': 18,\n        \'ssd\': 18,\n        \'vgg19\': 20}\n\n    url = model_urls[trunk]\n\n    if trunk == \'ssd\':\n        urllib.urlretrieve(\'https://s3.amazonaws.com/amdegroot-models/ssd300_mAP_77.43_v2.pth\',\n                           os.path.join(model_path, \'ssd.pth\'))\n        vgg_state_dict = torch.load(os.path.join(model_path, \'ssd.pth\'))\n        print(\'loading SSD\')\n    else:\n        vgg_state_dict = model_zoo.load_url(url, model_dir=model_path)\n    vgg_keys = vgg_state_dict.keys()\n\n    # load weights of vgg\n    weights_load = {}\n    # weight+bias,weight+bias.....(repeat 10 times)\n    for i in range(number_weight[trunk]):\n        weights_load[list(model.state_dict().keys())[i]\n                     ] = vgg_state_dict[list(vgg_keys)[i]]\n\n    state = model.state_dict()\n    state.update(weights_load)\n    model.load_state_dict(state)\n    print(\'load imagenet pretrained model: {}\'.format(model_path))\n'"
src/PoseEstimation/network/slim.py,1,"b'#!/usr/bin/env python\n# encoding: utf-8\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\n\ntry:\n    import caffe\n    from caffe import layers as L\n    from caffe import params as P\nexcept ImportError:\n    pass\n\n\ndef g_name(g_name, m):\n    m.g_name = g_name\n    return m\n\n\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n\n    def forward(self, x):\n        x = x.reshape(x.shape[0], self.groups, x.shape[1] // self.groups, x.shape[2], x.shape[3])\n        x = x.permute(0, 2, 1, 3, 4)\n        x = x.reshape(x.shape[0], -1, x.shape[3], x.shape[4])\n        return x\n\n    def generate_caffe_prototxt(self, caffe_net, layer):\n        layer = L.ShuffleChannel(layer, group=self.groups)\n        caffe_net[self.g_name] = layer\n        return layer\n\n\ndef channel_shuffle(name, groups):\n    return g_name(name, ChannelShuffle(groups))\n\n\nclass Permute(nn.Module):\n    def __init__(self, order):\n        super(Permute, self).__init__()\n        self.order = order\n\n    def forward(self, x):\n        x = x.permute(*self.order).contiguous()\n        return x\n\n    def generate_caffe_prototxt(self, caffe_net, layer):\n        layer = L.Permute(layer, order=list(self.order))\n        caffe_net[self.g_name] = layer\n        return layer\n\n\ndef permute(name, order):\n    return g_name(name, Permute(order))\n\n\nclass Flatten(nn.Module):\n    def __init__(self, axis):\n        super(Flatten, self).__init__()\n        self.axis = axis\n\n    def forward(self, x):\n        assert self.axis == 1\n        x = x.reshape(x.shape[0], -1)\n        return x\n\n    def generate_caffe_prototxt(self, caffe_net, layer):\n        layer = L.Flatten(layer, axis=self.axis)\n        caffe_net[self.g_name] = layer\n        return layer\n\n\ndef flatten(name, axis):\n    return g_name(name, Flatten(axis))\n\n\ndef generate_caffe_prototxt(m, caffe_net, layer):\n    if hasattr(m, \'generate_caffe_prototxt\'):\n        return m.generate_caffe_prototxt(caffe_net, layer)\n\n    if isinstance(m, nn.Sequential):\n        for module in m:\n            layer = generate_caffe_prototxt(module, caffe_net, layer)\n        return layer\n\n    if isinstance(m, nn.Conv2d):\n        if m.bias is None:\n            param=[dict(lr_mult=1, decay_mult=1)]\n        else:\n            param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=1, decay_mult=0)]\n        assert m.dilation[0] == m.dilation[1]\n        convolution_param=dict(\n            num_output=m.out_channels,\n            group=m.groups, bias_term=(m.bias is not None),\n            weight_filler=dict(type=\'msra\'),\n            dilation=m.dilation[0],\n        )\n        if m.kernel_size[0] == m.kernel_size[1]:\n            convolution_param[\'kernel_size\'] = m.kernel_size[0]\n        else:\n            convolution_param[\'kernel_h\'] = m.kernel_size[0]\n            convolution_param[\'kernel_w\'] = m.kernel_size[1]\n        if m.stride[0] == m.stride[1]:\n            convolution_param[\'stride\'] = m.stride[0]\n        else:\n            convolution_param[\'stride_h\'] = m.stride[0]\n            convolution_param[\'stride_w\'] = m.stride[1]\n        if m.padding[0] == m.padding[1]:\n            convolution_param[\'pad\'] = m.padding[0]\n        else:\n            convolution_param[\'pad_h\'] = m.padding[0]\n            convolution_param[\'pad_w\'] = m.padding[1]\n        layer = L.Convolution(\n            layer,\n            param=param,\n            convolution_param=convolution_param,\n        )\n        caffe_net.tops[m.g_name] = layer\n        return layer\n\n    if isinstance(m, nn.ConvTranspose2d):\n        if m.bias is None:\n            param=[dict(lr_mult=1, decay_mult=1)]\n        else:\n            param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=1, decay_mult=0)]\n        assert m.dilation[0] == m.dilation[1]\n        convolution_param=dict(\n            num_output=m.out_channels,\n            group=m.groups, bias_term=(m.bias is not None),\n            weight_filler=dict(type=\'msra\'),\n            dilation=m.dilation[0],\n        )\n        if m.kernel_size[0] == m.kernel_size[1]:\n            convolution_param[\'kernel_size\'] = m.kernel_size[0]\n        else:\n            convolution_param[\'kernel_h\'] = m.kernel_size[0]\n            convolution_param[\'kernel_w\'] = m.kernel_size[1]\n        if m.stride[0] == m.stride[1]:\n            convolution_param[\'stride\'] = m.stride[0]\n        else:\n            convolution_param[\'stride_h\'] = m.stride[0]\n            convolution_param[\'stride_w\'] = m.stride[1]\n        if m.padding[0] == m.padding[1]:\n            convolution_param[\'pad\'] = m.padding[0]\n        else:\n            convolution_param[\'pad_h\'] = m.padding[0]\n            convolution_param[\'pad_w\'] = m.padding[1]\n        layer = L.Deconvolution(\n            layer,\n            param=param,\n            convolution_param=convolution_param,\n        )\n        caffe_net.tops[m.g_name] = layer\n        return layer\n\n    if isinstance(m, nn.BatchNorm2d):\n        layer = L.BatchNorm(\n            layer, in_place=True,\n            param=[dict(lr_mult=0, decay_mult=0), dict(lr_mult=0, decay_mult=0), dict(lr_mult=0, decay_mult=0)],\n        )\n        caffe_net[m.g_name] = layer\n        if m.affine:\n            layer = L.Scale(\n                layer, in_place=True, bias_term=True,\n                filler=dict(type=\'constant\', value=1), bias_filler=dict(type=\'constant\', value=0),\n                param=[dict(lr_mult=1, decay_mult=0), dict(lr_mult=1, decay_mult=0)],\n            )\n            caffe_net[m.g_name + \'/scale\'] = layer\n        return layer\n\n    if isinstance(m, nn.ReLU):\n        layer = L.ReLU(layer, in_place=True)\n        caffe_net.tops[m.g_name] = layer\n        return layer\n\n    if isinstance(m, nn.PReLU):\n        layer = L.PReLU(layer)\n        caffe_net.tops[m.g_name] = layer\n        return layer\n\n    if isinstance(m, nn.AvgPool2d) or isinstance(m, nn.MaxPool2d):\n        if isinstance(m, nn.AvgPool2d):\n            pooling_param = dict(pool=P.Pooling.AVE)\n        else:\n            pooling_param = dict(pool=P.Pooling.MAX)\n        if isinstance(m.kernel_size, tuple) or isinstance(m.kernel_size, list):\n            pooling_param[\'kernel_h\'] = m.kernel_size[0]\n            pooling_param[\'kernel_w\'] = m.kernel_size[1]\n        else:\n            pooling_param[\'kernel_size\'] = m.kernel_size\n        if isinstance(m.stride, tuple) or isinstance(m.stride, list):\n            pooling_param[\'stride_h\'] = m.stride[0]\n            pooling_param[\'stride_w\'] = m.stride[1]\n        else:\n            pooling_param[\'stride\'] = m.stride\n        if isinstance(m.padding, tuple) or isinstance(m.padding, list):\n            pooling_param[\'pad_h\'] = m.padding[0]\n            pooling_param[\'pad_w\'] = m.padding[1]\n        else:\n            pooling_param[\'pad\'] = m.padding\n        layer = L.Pooling(layer, pooling_param=pooling_param)\n        caffe_net.tops[m.g_name] = layer\n        return layer\n    raise Exception(""Unknow module \'%s\' to generate caffe prototxt."" % m)\n\n\ndef convert_pytorch_to_caffe(torch_net, caffe_net):\n    for name, m in torch_net.named_modules():\n        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n            print(\'convert conv:\', name, m.g_name, m)\n            caffe_net.params[m.g_name][0].data[...] = m.weight.data.cpu().numpy()\n            if m.bias is not None:\n                caffe_net.params[m.g_name][1].data[...] = m.bias.data.cpu().numpy()\n        if isinstance(m, nn.BatchNorm2d):\n            print(\'convert bn:\', name, m.g_name, m)\n            caffe_net.params[m.g_name][0].data[...] = m.running_mean.cpu().numpy()\n            caffe_net.params[m.g_name][1].data[...] = m.running_var.cpu().numpy()\n            caffe_net.params[m.g_name][2].data[...] = 1\n            if m.affine:\n                caffe_net.params[m.g_name + \'/scale\'][0].data[...] = m.weight.data.cpu().numpy()\n                caffe_net.params[m.g_name + \'/scale\'][1].data[...] = m.bias.data.cpu().numpy()\n\n\ndef conv_bn_relu(name, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1):\n    return nn.Sequential(\n        g_name(name, nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, False)),\n        g_name(name + \'/bn\', nn.BatchNorm2d(out_channels)),\n        g_name(name + \'/relu\', nn.ReLU(inplace=True)),\n    )\n\n\ndef conv_bn(name, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1):\n    return nn.Sequential(\n        g_name(name, nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, False)),\n        g_name(name + \'/bn\', nn.BatchNorm2d(out_channels)),\n    )\n\n\ndef conv(name, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1):\n    return g_name(name, nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, True))\n\n\ndef conv_relu(name, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1):\n    return nn.Sequential(\n        g_name(name, nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, True)),\n        g_name(name + \'/relu\', nn.ReLU()),\n    )\n\ndef conv_prelu(name, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1):\n    return nn.Sequential(\n        g_name(name, nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, True)),\n        g_name(name + \'/prelu\', nn.PReLU()),\n    )\n    \n\nif __name__ == \'__main__\':\n\n    class BasicBlock(nn.Module):\n\n        def __init__(self, name, in_channels, middle_channels, out_channels, stride, residual):\n            super(BasicBlock, self).__init__()\n            self.g_name = name\n            self.residual = residual\n            self.conv = [\n                conv_bn(name + \'/conv1\', \n                    in_channels, in_channels, 3, stride=stride, padding=1, groups=in_channels),\n                conv_bn_relu(name + \'/conv2\', in_channels, middle_channels, 1),\n                conv_bn(name + \'/conv3\', middle_channels, out_channels, 1),\n            ]\n            self.conv = nn.Sequential(*self.conv)\n            # self.relu = g_name(name + \'/relu\', nn.ReLU(inplace=True))\n\n        def forward(self, x):\n            x = x + self.conv(x) if self.residual else self.conv(x)\n            # x = self.relu(x)\n            return x\n\n        def generate_caffe_prototxt(self, caffe_net, layer):\n            residual_layer = layer\n            layer = generate_caffe_prototxt(self.conv, caffe_net, layer)\n            if self.residual:\n                layer = L.Eltwise(residual_layer, layer, operation=P.Eltwise.SUM)\n                caffe_net[self.g_name + \'/sum\'] = layer\n            # layer = generate_caffe_prototxt(self.relu, caffe_net, layer)\n            return layer\n\n\n    class Network(nn.Module):\n\n        def __init__(self, num_outputs, width_multiplier=32):\n            super(Network, self).__init__()\n\n            assert width_multiplier >= 0 and width_multiplier <= 256\n            # assert width_multiplier % 2 == 0\n\n            self.network = [\n                g_name(\'data/bn\', nn.BatchNorm2d(3)),\n                conv_bn_relu(\'stage1/conv\', 3, 32, 3, 2, 1),\n                # g_name(\'stage1/pool\', nn.MaxPool2d(3, 2, 0, ceil_mode=True)),\n            ]\n            channel = lambda i: (2**i) * int(width_multiplier)\n            network_parameters = [\n                (32,         channel(2) * 4, channel(2), 2, 2),\n                (channel(2), channel(2) * 4, channel(2), 2, 4),\n                (channel(2), channel(3) * 4, channel(3), 2, 8),\n                (channel(3), channel(4) * 4, channel(4), 2, 4),\n            ]\n            for i, parameters in enumerate(network_parameters):\n                in_channels, middle_channels, out_channels, stride, num_blocks = parameters\n                self.network += [self._generate_stage(\'stage_{}\'.format(i + 2), \n                    in_channels, middle_channels, out_channels, stride, num_blocks)]\n            self.network += [\n                conv_bn_relu(\'unsqueeze\', out_channels, out_channels * 4, 1),\n                g_name(\'pool_fc\', nn.AvgPool2d(7)),\n                g_name(\'fc\', nn.Conv2d(out_channels * 4, num_outputs, 1)),\n            ]\n            self.network = nn.Sequential(*self.network)\n\n            for name, m in self.named_modules():\n                if any(map(lambda x: isinstance(m, x), [nn.Linear, nn.Conv1d, nn.Conv2d])):\n                    nn.init.kaiming_normal(m.weight, mode=\'fan_out\')\n                    if m.bias is not None:\n                        nn.init.constant(m.bias, 0)\n\n        def _generate_stage(self, name, in_channels, middle_channels, out_channels, stride, num_blocks):\n            blocks = [BasicBlock(name + \'_1\', in_channels, middle_channels, out_channels, 2, False)]\n            for i in range(1, num_blocks):\n                blocks.append(BasicBlock(name + \'_{}\'.format(i + 1), \n                    out_channels, middle_channels, out_channels, 1, True))\n            return nn.Sequential(*blocks)\n\n        def forward(self, x):\n            return self.network(x).view(x.size(0), -1)\n        \n        def generate_caffe_prototxt(self, caffe_net, layer):\n            return generate_caffe_prototxt(self.network, caffe_net, layer)\n\n        def convert_to_caffe(self, name):\n            caffe_net = caffe.NetSpec()\n            layer = L.Input(shape=dict(dim=[1, 3, 224, 224]))\n            caffe_net.tops[\'data\'] = layer\n            generate_caffe_prototxt(self, caffe_net, layer)\n            print(caffe_net.to_proto())\n            with open(name + \'.prototxt\', \'wb\') as f:\n                f.write(str(caffe_net.to_proto()))\n            caffe_net = caffe.Net(name + \'.prototxt\', caffe.TEST)\n            convert_pytorch_to_caffe(self, caffe_net)\n            caffe_net.save(name + \'.caffemodel\')\n\n\n    network = Network(1000, 8)\n    print(network)\n    network.convert_to_caffe(\'net\')\n'"
src/PoseEstimation/tmp/Canny_to_detect_dege_after_conv.py,0,"b""import os\nimport cv2\nimport numpy as np\n\n#The directory where all intermediate features are stored.\nsource_dir = '/media/data2/aifi/all_yoga_features/'\n\n#Where the detected edges are stores.\nsave_dir   = '/data/coco/all_edges/'\n\n# The operation on output of which layer\n\nconv_type  = 'conv1_1'\n\nimage_list = os.listdir(source_dir)\n\nfor one_path in image_list:\n\n    #Trival operations to find images.\n    image_name = one_path\n        \n    image_path = source_dir +'/'+image_name+'/'+conv_type+'/'\n    \n    \n    images = os.listdir(image_path)\n\n    # The convolution kernel for morphologyEx algorithm\n    kernel = np.ones((3,3),np.uint8)\n    \n    #Store all detected edges after conv_type, final edge will average that.\n    all_edged = []\n    \n    for path in images:\n    \n        #Get an image afeter conv_type\n        a_path = image_path+path\n\n        im = cv2.imread(a_path)\n\n        #Make image gray\n        blurred = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n        \n        # Core part of this algorithm, learned it from internet. \n        # Didn't know well for the theoretical behind it yet. :)\n        auto = cv2.Canny(blurred,40,255)\n        \n        auto = cv2.morphologyEx(auto, cv2.MORPH_CLOSE, kernel)\n\n        all_edged.append(auto)                \n\n    #Average and save it.\n    edges = np.mean(all_edged,axis=0).astype(np.uint8)\n\n    edges = cv2.cvtColor(edges,cv2.COLOR_GRAY2RGB)\n    \n    cv2.imwrite('./all_edges/'+image_name+'_'+conv_type+'.png',edges)\n"""
src/PoseEstimation/tmp/get_features_yoga.py,13,"b'import copy\nimport json\nimport math\n# general package\nimport os\nimport random\nimport struct\nimport sys\nimport time\nfrom collections import OrderedDict\nfrom math import sqrt\n\nimport cv2\nimport matplotlib.cm\nimport numpy as np\nimport pandas as pd\nimport scipy\nimport scipy.io as sio\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom scipy.ndimage.filters import gaussian_filter\n\n# torch package\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\n#from network.Ying_model import get_ying_model\nfrom network.rtpose_vgg import get_model\n\n#import utils\nfrom training.datasets.coco_data.preprocessing import (inception_preprocess,\n                                              rtpose_preprocess,\n                                              ssd_preprocess, vgg_preprocess)\nfrom network import im_transform\nfrom evaluate.coco_eval import get_multiplier, get_outputs, handle_paf_and_heat\n\nblocks = {}\nblock0 = [\'conv1_1\',\n          \'conv1_2\',\n          \'pool1_stage1\',\n          \'conv2_1\',\n          \'conv2_2\',\n          \'pool2_stage1\',\n          \'conv3_1\',\n          \'conv3_2\',\n          \'conv3_3\',\n          \'conv3_4\',\n          \'pool3_stage1\',\n          \'conv4_1\',\n          \'conv4_2\',\n          \'conv4_3_CPM\',\n          \'conv4_4_CPM\']\n\nblocks[\'block0\']=[]\nfor i, item in enumerate(block0):\n    if \'conv\' in item:\n        blocks[\'block0\'].append(item)\n        blocks[\'block0\'].append(item+\'_relu\')        \n    elif \'pool\' in item:\n        blocks[\'block0\'].append(item)         \n\n\n# Stage 1\nblock1_1 = [\'conv5_1_CPM_L1\',\n                      \'conv5_2_CPM_L1\',\n                      \'conv5_3_CPM_L1\',\n                      \'conv5_4_CPM_L1\',\n                      \'conv5_5_CPM_L1\']\n\nblock1_2 = [\'conv5_1_CPM_L2\', \n                      \'conv5_2_CPM_L2\', \n                      \'conv5_3_CPM_L2\',\n                      \'conv5_4_CPM_L2\', \n                      \'conv5_5_CPM_L2\']     \n\nblocks[\'block1_1\']=[]                                       \nfor i in range(len(block1_1)):\n    item = block1_1[i]\n    blocks[\'block1_1\'].append(item)\n    \n    if i==(len(block1_1)-1):\n        break\n        \n    blocks[\'block1_1\'].append(item+\'_relu\')        \n  \nblocks[\'block1_2\']=[]                                       \nfor i in range(len(block1_2)):\n    item = block1_2[i]\n    blocks[\'block1_2\'].append(item)\n    \n    if i==(len(block1_2)-1):\n        break\n        \n    blocks[\'block1_2\'].append(item+\'_relu\')   \n    \n   \n# Stages 2 - 6\nfor i in range(2, 7):\n    block1 = [\n        \'Mconv1_stage%d_L1\' % i,\n        \'Mconv2_stage%d_L1\' % i,\n        \'Mconv3_stage%d_L1\' % i,\n        \'Mconv4_stage%d_L1\' % i,\n        \'Mconv5_stage%d_L1\' % i,\n        \'Mconv6_stage%d_L1\' % i,\n        \'Mconv7_stage%d_L1\' % i]\n\n    block2 = [\n        \'Mconv1_stage%d_L2\' % i,\n        \'Mconv2_stage%d_L2\' % i,\n        \'Mconv3_stage%d_L2\' % i,\n        \'Mconv4_stage%d_L2\' % i,\n        \'Mconv5_stage%d_L2\' % i,\n        \'Mconv6_stage%d_L2\' % i,\n        \'Mconv7_stage%d_L2\' % i\n    ]\n\n    blocks[\'block%d_1\' % i] = []\n    blocks[\'block%d_2\' % i] = []\n    \n    for k in range(len(block1)):\n        item = block1[k]\n        blocks[\'block%d_1\' % i].append(item)\n        \n        if k==(len(block1)-1):\n            break\n            \n        blocks[\'block%d_1\' % i].append(item+\'_relu\')        \n      \n                                     \n    for k in range(len(block2)):\n        item = block2[k]\n        blocks[\'block%d_2\' % i].append(item)\n        \n        if k==(len(block2)-1):\n            break\n            \n        blocks[\'block%d_2\' % i].append(item+\'_relu\')     \n\n\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self, submodule):\n        super(FeatureExtractor, self).__init__()\n        self.submodule = submodule\n\n    def forward(self, x):\n        outputs = []\n        for name, module in self.submodule._modules.items():\n        \n            x = module(x)\n            \n            z = x.data.cpu().numpy()\n            outputs += [z]\n            \n        return outputs, x\n\nfeature_save_path = \'../all_yoga_features_our_best_model/\'\n\nif __name__ == ""__main__"":\n    \'\'\'\n    MS COCO annotation order:\n    0: nose\t   \t\t1: l eye\t\t2: r eye\t3: l ear\t4: r ear\n    5: l shoulder\t6: r shoulder\t7: l elbow\t8: r elbow\n    9: l wrist\t\t10: r wrist\t\t11: l hip\t12: r hip\t13: l knee\n    14: r knee\t\t15: l ankle\t\t16: r ankle\n\n    The order in this work:\n    (0-\'nose\'\t1-\'neck\' 2-\'right_shoulder\' 3-\'right_elbow\' 4-\'right_wrist\'\n    5-\'left_shoulder\' 6-\'left_elbow\'\t    7-\'left_wrist\'  8-\'right_hip\'  \n    9-\'right_knee\'\t 10-\'right_ankle\'\t11-\'left_hip\'   12-\'left_knee\' \n    13-\'left_ankle\'\t 14-\'right_eye\'\t    15-\'left_eye\'   16-\'right_ear\' \n    17-\'left_ear\' )\n    \'\'\'\n    orderCOCO = [0, 15, 14, 17, 16, 5, 2, 6, 3, 7, 4, 11, 8, 12, 9, 13, 10]\n\n    mid_1 = [1, 8,  9, 1,  11, 12, 1, 2, 3,\n             2,  1, 5, 6, 5,  1, 0,  0,  14, 15]\n\n    mid_2 = [8, 9, 10, 11, 12, 13, 2, 3, 4,\n             16, 5, 6, 7, 17, 0, 14, 15, 16, 17]\n\n    # This txt file is get at the caffe_rtpose repository:\n    # https://github.com/CMU-Perceptual-Computing-Lab/caffe_rtpose/blob/master/image_info_val2014_1k.txt\n\n    image_dir = \'/data/coco/val2014/\'\n    save_dir  = \'/data/coco/val2014_features/\'\n\n    model = get_model(\'vgg19\')\n    model = torch.nn.DataParallel(model).cuda()\n#    model = get_ying_model(stages=5, have_bn=True, have_bias=False)\n#    Load our model\n    weight_name = \'./network/weight/pose_model_scratch.pth\'\n    model.load_state_dict(torch.load(weight_name))\n    model = model.module\n#    model.load_state_dict(torch.load(\'pose_model.pth\'))\n#    model.load_state_dict(torch.load(\'../caffe_model/dilated3_5stage_merged.pth\'))\n#    model.load_state_dict(torch.load(\'../caffe_model/dilated3_remove_stage1_test.pth\'))\n\n    model.eval()\n    model.float()\n    model.cuda()\n    \n    feature_extractor = FeatureExtractor(model.model0)\n    \n    feature_extractor1_1 = FeatureExtractor(model.model1_1)\n    feature_extractor1_2 = FeatureExtractor(model.model1_2)\n    \n    feature_extractor2_1 = FeatureExtractor(model.model2_1)\n    feature_extractor2_2 = FeatureExtractor(model.model2_2)\n    \n    feature_extractor3_1 = FeatureExtractor(model.model3_1)\n    feature_extractor3_2 = FeatureExtractor(model.model3_2)\n    \n    feature_extractor4_1 = FeatureExtractor(model.model4_1)\n    feature_extractor4_2 = FeatureExtractor(model.model4_2)\n    \n    feature_extractor5_1 = FeatureExtractor(model.model5_1)\n    feature_extractor5_2 = FeatureExtractor(model.model5_2)\n    \n    feature_extractor6_1 = FeatureExtractor(model.model6_1)\n    feature_extractor6_2 = FeatureExtractor(model.model6_2)    \n                                 \n    \n    images = os.listdir(image_dir)\n    # iterate all val images\n    feed_size = 368\n    for a_path in images:\n        one_path = image_dir + a_path\n        print(one_path)\n\n        oriImg = cv2.imread(one_path)\n      \n        im_croped, im_scale, real_shape = im_transform.crop_with_factor(\n            oriImg, feed_size, factor=8, is_ceil=True)\n        print(\'size of im crop\', im_croped.shape)\n            \n        im_data = vgg_preprocess(im_croped)\n\n        #im_data = im_data.transpose([2, 0, 1]).astype(np.float32)        \n        \n        batch_images = np.expand_dims(im_data,0)\n       \n        batch_var = torch.from_numpy(batch_images).cuda().float()\n         \n        all_outputs = []\n         \n        outputs0, out1 = feature_extractor(batch_var)\n        \n        outputs1_1, out1_1 = feature_extractor1_1(out1)\n        outputs1_2, out1_2 = feature_extractor1_2(out1)\n        out2 = torch.cat([out1_1, out1_2, out1], 1)\n\n        outputs2_1, out2_1 = feature_extractor2_1(out2)\n        outputs2_2, out2_2 = feature_extractor2_2(out2)\n        out3 = torch.cat([out2_1, out2_2, out1], 1)\n\n\n        outputs3_1, out3_1 = feature_extractor3_1(out3)\n        outputs3_2, out3_2 = feature_extractor3_2(out3)\n        out4 = torch.cat([out3_1, out3_2, out1], 1)\n\n\n        outputs4_1,  out4_1 = feature_extractor4_1(out4)\n        outputs4_2, out4_2 = feature_extractor4_2(out4)\n        out5 = torch.cat([out4_1, out4_2, out1], 1)\n\n        outputs5_1, out5_1 = feature_extractor5_1(out5)\n        outputs5_2,  out5_2 = feature_extractor5_2(out5)\n        out6 = torch.cat([out5_1, out5_2, out1], 1)\n\n        outputs6_1,  out6_1 = feature_extractor6_1(out6)\n        outputs6_2,  out6_2 = feature_extractor6_2(out6)\n\n        all_outputs = outputs0 +outputs1_1 + outputs1_2 + outputs2_1 + outputs2_2 + \\\n        outputs3_1  + outputs3_2  + outputs4_1  + outputs4_2  + \\\n        outputs5_1  + outputs5_2  + outputs6_1  + outputs6_2 \\\n        \n        all_names = blocks[\'block0\']+blocks[\'block1_1\']+blocks[\'block1_2\']+blocks[\'block2_1\']\\\n        +blocks[\'block2_2\']+blocks[\'block3_1\']+blocks[\'block3_2\']+blocks[\'block4_1\']+blocks[\'block4_2\']\\\n        +blocks[\'block5_1\']+blocks[\'block5_2\']+blocks[\'block6_1\']+blocks[\'block6_2\']\n\n        for name,tensor in zip(all_names,all_outputs):\n            tensor = tensor[0]\n            for n in range(tensor.shape[0]):\n                one_save_dir = os.path.join(save_dir, a_path.split(\'.\')[0], name)\n                try:\n                    os.makedirs(one_save_dir)\n                except OSError:\n                    pass\n                a_tensor = tensor[n]                \n                a_tensor = (a_tensor-np.min(a_tensor))/(np.max(a_tensor)-np.min(a_tensor))\n                a_image = np.clip(a_tensor*255,0,255).astype(np.uint8)\n                print(a_image.shape)\n                one_save_path = os.path.join(one_save_dir, str(n)+\'.jpg\') \n                cv2.imwrite(one_save_path, a_image)\n'"
src/PoseEstimation/training/__init__.py,0,b''
src/pix2pixHD/data/__init__.py,0,b''
src/pix2pixHD/data/aligned_dataset.py,0,"b""### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport os.path\nfrom data.base_dataset import BaseDataset, get_params, get_transform, normalize\nfrom data.image_folder import make_dataset\nfrom PIL import Image\n\nclass AlignedDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot    \n\n        ### input A (label maps)\n        dir_A = '_A' if self.opt.label_nc == 0 else '_label'\n        self.dir_A = os.path.join(opt.dataroot, opt.phase + dir_A)\n        self.A_paths = sorted(make_dataset(self.dir_A))\n\n        ### input B (real images)\n        if opt.isTrain:\n            dir_B = '_B' if self.opt.label_nc == 0 else '_img'\n            self.dir_B = os.path.join(opt.dataroot, opt.phase + dir_B)  \n            self.B_paths = sorted(make_dataset(self.dir_B))\n\n        ### instance maps\n        if not opt.no_instance:\n            self.dir_inst = os.path.join(opt.dataroot, opt.phase + '_inst')\n            self.inst_paths = sorted(make_dataset(self.dir_inst))\n\n        ### load precomputed instance-wise encoded features\n        if opt.load_features:                              \n            self.dir_feat = os.path.join(opt.dataroot, opt.phase + '_feat')\n            print('----------- loading features from %s ----------' % self.dir_feat)\n            self.feat_paths = sorted(make_dataset(self.dir_feat))\n\n        self.dataset_size = len(self.A_paths) \n      \n    def __getitem__(self, index):        \n        ### input A (label maps)\n        A_path = self.A_paths[index]              \n        A = Image.open(A_path)        \n        params = get_params(self.opt, A.size)\n        if self.opt.label_nc == 0:\n            transform_A = get_transform(self.opt, params)\n            A_tensor = transform_A(A.convert('RGB'))\n        else:\n            transform_A = get_transform(self.opt, params, method=Image.NEAREST, normalize=False)\n            A_tensor = transform_A(A) * 255.0\n\n        B_tensor = inst_tensor = feat_tensor = 0\n        ### input B (real images)\n        if self.opt.isTrain:\n            B_path = self.B_paths[index]   \n            B = Image.open(B_path).convert('RGB')\n            transform_B = get_transform(self.opt, params)      \n            B_tensor = transform_B(B)\n\n        ### if using instance maps        \n        if not self.opt.no_instance:\n            inst_path = self.inst_paths[index]\n            inst = Image.open(inst_path)\n            inst_tensor = transform_A(inst)\n\n            if self.opt.load_features:\n                feat_path = self.feat_paths[index]            \n                feat = Image.open(feat_path).convert('RGB')\n                norm = normalize()\n                feat_tensor = norm(transform_A(feat))                            \n\n        input_dict = {'label': A_tensor, 'inst': inst_tensor, 'image': B_tensor, \n                      'feat': feat_tensor, 'path': A_path}\n\n        return input_dict\n\n    def __len__(self):\n        return len(self.A_paths) // self.opt.batchSize * self.opt.batchSize\n\n    def name(self):\n        return 'AlignedDataset'"""
src/pix2pixHD/data/base_data_loader.py,0,"b'\nclass BaseDataLoader():\n    def __init__(self):\n        pass\n    \n    def initialize(self, opt):\n        self.opt = opt\n        pass\n\n    def load_data():\n        return None\n\n        \n        \n'"
src/pix2pixHD/data/base_dataset.py,1,"b""### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport torch.utils.data as data\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport numpy as np\nimport random\n\nclass BaseDataset(data.Dataset):\n    def __init__(self):\n        super(BaseDataset, self).__init__()\n\n    def name(self):\n        return 'BaseDataset'\n\n    def initialize(self, opt):\n        pass\n\ndef get_params(opt, size):\n    w, h = size\n    new_h = h\n    new_w = w\n    if opt.resize_or_crop == 'resize_and_crop':\n        new_h = new_w = opt.loadSize            \n    elif opt.resize_or_crop == 'scale_width_and_crop':\n        new_w = opt.loadSize\n        new_h = opt.loadSize * h // w\n\n    x = random.randint(0, np.maximum(0, new_w - opt.fineSize))\n    y = random.randint(0, np.maximum(0, new_h - opt.fineSize))\n    \n    flip = random.random() > 0.5\n    return {'crop_pos': (x, y), 'flip': flip}\n\ndef get_transform(opt, params, method=Image.BICUBIC, normalize=True):\n    transform_list = []\n    if 'resize' in opt.resize_or_crop:\n        osize = [opt.loadSize, opt.loadSize]\n        transform_list.append(transforms.Scale(osize, method))   \n    elif 'scale_width' in opt.resize_or_crop:\n        transform_list.append(transforms.Lambda(lambda img: __scale_width(img, opt.loadSize, method)))\n        \n    if 'crop' in opt.resize_or_crop:\n        transform_list.append(transforms.Lambda(lambda img: __crop(img, params['crop_pos'], opt.fineSize)))\n\n    if opt.resize_or_crop == 'none':\n        base = float(2 ** opt.n_downsample_global)\n        if opt.netG == 'local':\n            base *= (2 ** opt.n_local_enhancers)\n        transform_list.append(transforms.Lambda(lambda img: __make_power_2(img, base, method)))\n\n    if opt.isTrain and not opt.no_flip:\n        transform_list.append(transforms.Lambda(lambda img: __flip(img, params['flip'])))\n\n    transform_list += [transforms.ToTensor()]\n\n    if normalize:\n        transform_list += [transforms.Normalize((0.5, 0.5, 0.5),\n                                                (0.5, 0.5, 0.5))]\n    return transforms.Compose(transform_list)\n\ndef normalize():    \n    return transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n\ndef __make_power_2(img, base, method=Image.BICUBIC):\n    ow, oh = img.size        \n    h = int(round(oh / base) * base)\n    w = int(round(ow / base) * base)\n    if (h == oh) and (w == ow):\n        return img\n    return img.resize((w, h), method)\n\ndef __scale_width(img, target_width, method=Image.BICUBIC):\n    ow, oh = img.size\n    if (ow == target_width):\n        return img    \n    w = target_width\n    h = int(target_width * oh / ow)    \n    return img.resize((w, h), method)\n\ndef __crop(img, pos, size):\n    ow, oh = img.size\n    x1, y1 = pos\n    tw = th = size\n    if (ow > tw or oh > th):        \n        return img.crop((x1, y1, x1 + tw, y1 + th))\n    return img\n\ndef __flip(img, flip):\n    if flip:\n        return img.transpose(Image.FLIP_LEFT_RIGHT)\n    return img\n"""
src/pix2pixHD/data/custom_dataset_data_loader.py,2,"b'import torch.utils.data\nfrom data.base_data_loader import BaseDataLoader\n\n\ndef CreateDataset(opt):\n    dataset = None\n    from data.aligned_dataset import AlignedDataset\n    dataset = AlignedDataset()\n\n    print(""dataset [%s] was created"" % (dataset.name()))\n    dataset.initialize(opt)\n    return dataset\n\nclass CustomDatasetDataLoader(BaseDataLoader):\n    def name(self):\n        return \'CustomDatasetDataLoader\'\n\n    def initialize(self, opt):\n        BaseDataLoader.initialize(self, opt)\n        self.dataset = CreateDataset(opt)\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=opt.batchSize,\n            shuffle=not opt.serial_batches,\n            num_workers=int(opt.nThreads))\n\n    def load_data(self):\n        return self.dataloader\n\n    def __len__(self):\n        return min(len(self.dataset), self.opt.max_dataset_size)\n'"
src/pix2pixHD/data/data_loader.py,0,b'\ndef CreateDataLoader(opt):\n    from data.custom_dataset_data_loader import CustomDatasetDataLoader\n    data_loader = CustomDatasetDataLoader()\n    print(data_loader.name())\n    data_loader.initialize(opt)\n    return data_loader\n'
src/pix2pixHD/data/image_folder.py,1,"b'###############################################################################\n# Code from\n# https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n# Modified the original code so that it also loads images from the current\n# directory as well as the subdirectories\n###############################################################################\nimport torch.utils.data as data\nfrom PIL import Image\nimport os\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\', \'.tiff\'\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef make_dataset(dir):\n    images = []\n    assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n\n    for root, _, fnames in sorted(os.walk(dir)):\n        for fname in fnames:\n            if is_image_file(fname):\n                path = os.path.join(root, fname)\n                images.append(path)\n\n    return images\n\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\n\nclass ImageFolder(data.Dataset):\n\n    def __init__(self, root, transform=None, return_paths=False,\n                 loader=default_loader):\n        imgs = make_dataset(root)\n        if len(imgs) == 0:\n            raise(RuntimeError(""Found 0 images in: "" + root + ""\\n""\n                               ""Supported image extensions are: "" +\n                               "","".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.imgs = imgs\n        self.transform = transform\n        self.return_paths = return_paths\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.return_paths:\n            return img, path\n        else:\n            return img\n\n    def __len__(self):\n        return len(self.imgs)\n'"
src/pix2pixHD/models/__init__.py,0,b''
src/pix2pixHD/models/base_model.py,7,"b""### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport os\nimport torch\nimport sys\n\nclass BaseModel(torch.nn.Module):\n    def name(self):\n        return 'BaseModel'\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.gpu_ids = 0\n        self.isTrain = opt.isTrain\n        self.Tensor = torch.cuda.FloatTensor if self.gpu_ids else torch.Tensor\n        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n\n    def set_input(self, input):\n        self.input = input\n\n    def forward(self):\n        pass\n\n    # used in test time, no backprop\n    def test(self):\n        pass\n\n    def get_image_paths(self):\n        pass\n\n    def optimize_parameters(self):\n        pass\n\n    def get_current_visuals(self):\n        return self.input\n\n    def get_current_errors(self):\n        return {}\n\n    def save(self, label):\n        pass\n\n    # helper saving function that can be used by subclasses\n    def save_network(self, network, network_label, epoch_label, gpu_ids):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(self.save_dir, save_filename)\n        torch.save(network.cpu().state_dict(), save_path)\n        if len(gpu_ids) and torch.cuda.is_available():\n            network.cuda()\n\n    # helper loading function that can be used by subclasses\n    def load_network(self, network, network_label, epoch_label, save_dir=''):        \n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        if not save_dir:\n            save_dir = self.save_dir\n        save_path = os.path.join(save_dir, save_filename)        \n        if not os.path.isfile(save_path):\n            print('%s not exists yet!' % save_path)\n            if network_label == 'G':\n                raise('Generator must exist!')\n        else:\n            #network.load_state_dict(torch.load(save_path))\n            try:\n                network.load_state_dict(torch.load(save_path))\n            except:   \n                pretrained_dict = torch.load(save_path)                \n                model_dict = network.state_dict()\n                try:\n                    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}                    \n                    network.load_state_dict(pretrained_dict)\n                    if self.opt.verbose:\n                        print('Pretrained network %s has excessive layers; Only loading layers that are used' % network_label)\n                except:\n                    print('Pretrained network %s has fewer layers; The following are not initialized:' % network_label)\n                    for k, v in pretrained_dict.items():                      \n                        if v.size() == model_dict[k].size():\n                            model_dict[k] = v\n\n                    if sys.version_info >= (3,0):\n                        not_initialized = set()\n                    else:\n                        from sets import Set\n                        not_initialized = Set()                    \n\n                    for k, v in model_dict.items():\n                        if k not in pretrained_dict or v.size() != pretrained_dict[k].size():\n                            not_initialized.add(k.split('.')[0])\n                    \n                    print(sorted(not_initialized))\n                    network.load_state_dict(model_dict)                  \n\n    def update_learning_rate():\n        pass\n"""
src/pix2pixHD/models/models.py,1,"b'### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport torch\n\ndef create_model(opt):\n    if opt.model == \'pix2pixHD\':\n        from .pix2pixHD_model import Pix2PixHDModel, InferenceModel\n        if opt.isTrain:\n            model = Pix2PixHDModel()\n        else:\n            model = InferenceModel()\n    else:\n        from .ui_model import UIModel\n        model = UIModel()\n    model.initialize(opt)\n    if opt.verbose:\n        print(""model [%s] was created"" % (model.name()))\n\n    # if opt.isTrain and len(opt.gpu_ids):\n    #     model = torch.nn.DataParallel(model, device_ids=opt.gpu_ids)\n\n    return model\n'"
src/pix2pixHD/models/networks.py,12,"b""### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport torch\nimport torch.nn as nn\nimport functools\nfrom torch.autograd import Variable\nimport numpy as np\n\n###############################################################################\n# Functions\n###############################################################################\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm2d') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n\ndef get_norm_layer(norm_type='instance'):\n    if norm_type == 'batch':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n    elif norm_type == 'instance':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\n    else:\n        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n    return norm_layer\n\ndef define_G(input_nc, output_nc, ngf, netG, n_downsample_global=3, n_blocks_global=9, n_local_enhancers=1, \n             n_blocks_local=3, norm='instance', gpu_ids=[]):    \n    norm_layer = get_norm_layer(norm_type=norm)     \n    if netG == 'global':    \n        netG = GlobalGenerator(input_nc, output_nc, ngf, n_downsample_global, n_blocks_global, norm_layer)       \n    elif netG == 'local':        \n        netG = LocalEnhancer(input_nc, output_nc, ngf, n_downsample_global, n_blocks_global, \n                                  n_local_enhancers, n_blocks_local, norm_layer)\n    elif netG == 'encoder':\n        netG = Encoder(input_nc, output_nc, ngf, n_downsample_global, norm_layer)\n    else:\n        raise('generator not implemented!')\n    print(netG)\n    if len(gpu_ids) > 0:\n        assert(torch.cuda.is_available())   \n        netG.cuda(gpu_ids[0])\n    netG.apply(weights_init)\n    return netG\n\ndef define_D(input_nc, ndf, n_layers_D, norm='instance', use_sigmoid=False, num_D=1, getIntermFeat=False, gpu_ids=[]):        \n    norm_layer = get_norm_layer(norm_type=norm)   \n    netD = MultiscaleDiscriminator(input_nc, ndf, n_layers_D, norm_layer, use_sigmoid, num_D, getIntermFeat)   \n    print(netD)\n    if len(gpu_ids) > 0:\n        assert(torch.cuda.is_available())\n        netD.cuda(gpu_ids[0])\n    netD.apply(weights_init)\n    return netD\n\ndef print_network(net):\n    if isinstance(net, list):\n        net = net[0]\n    num_params = 0\n    for param in net.parameters():\n        num_params += param.numel()\n    print(net)\n    print('Total number of parameters: %d' % num_params)\n\n##############################################################################\n# Losses\n##############################################################################\nclass GANLoss(nn.Module):\n    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n                 tensor=torch.FloatTensor):\n        super(GANLoss, self).__init__()\n        self.real_label = target_real_label\n        self.fake_label = target_fake_label\n        self.real_label_var = None\n        self.fake_label_var = None\n        self.Tensor = tensor\n        if use_lsgan:\n            self.loss = nn.MSELoss()\n        else:\n            self.loss = nn.BCELoss()\n\n    def get_target_tensor(self, input, target_is_real):\n        target_tensor = None\n        if target_is_real:\n            create_label = ((self.real_label_var is None) or\n                            (self.real_label_var.numel() != input.numel()))\n            if create_label:\n                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n                self.real_label_var = Variable(real_tensor, requires_grad=False)\n            target_tensor = self.real_label_var\n        else:\n            create_label = ((self.fake_label_var is None) or\n                            (self.fake_label_var.numel() != input.numel()))\n            if create_label:\n                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n            target_tensor = self.fake_label_var\n        return target_tensor\n\n    def __call__(self, input, target_is_real):\n        if isinstance(input[0], list):\n            loss = 0\n            for input_i in input:\n                pred = input_i[-1].cuda()\n                target_tensor = self.get_target_tensor(pred, target_is_real).cuda()\n                ##\xe6\x8a\x8a\xe5\x8a\xa0cuda()##\n                loss += self.loss(pred, target_tensor)\n            return loss\n        else:            \n            target_tensor = self.get_target_tensor(input[-1], target_is_real)\n            return self.loss(input[-1], target_tensor)\n\nclass VGGLoss(nn.Module):\n    def __init__(self, gpu_ids):\n        super(VGGLoss, self).__init__()        \n        self.vgg = Vgg19().cuda()\n        self.criterion = nn.L1Loss()\n        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]        \n\n    def forward(self, x, y):              \n        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n        loss = 0\n        for i in range(len(x_vgg)):\n            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())        \n        return loss\n\n##############################################################################\n# Generator\n##############################################################################\nclass LocalEnhancer(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=32, n_downsample_global=3, n_blocks_global=9, \n                 n_local_enhancers=1, n_blocks_local=3, norm_layer=nn.BatchNorm2d, padding_type='reflect'):        \n        super(LocalEnhancer, self).__init__()\n        self.n_local_enhancers = n_local_enhancers\n        \n        ###### global generator model #####           \n        ngf_global = ngf * (2**n_local_enhancers)\n        model_global = GlobalGenerator(input_nc, output_nc, ngf_global, n_downsample_global, n_blocks_global, norm_layer).model        \n        model_global = [model_global[i] for i in range(len(model_global)-3)] # get rid of final convolution layers        \n        self.model = nn.Sequential(*model_global)                \n\n        ###### local enhancer layers #####\n        for n in range(1, n_local_enhancers+1):\n            ### downsample            \n            ngf_global = ngf * (2**(n_local_enhancers-n))\n            model_downsample = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf_global, kernel_size=7, padding=0), \n                                norm_layer(ngf_global), nn.ReLU(True),\n                                nn.Conv2d(ngf_global, ngf_global * 2, kernel_size=3, stride=2, padding=1), \n                                norm_layer(ngf_global * 2), nn.ReLU(True)]\n            ### residual blocks\n            model_upsample = []\n            for i in range(n_blocks_local):\n                model_upsample += [ResnetBlock(ngf_global * 2, padding_type=padding_type, norm_layer=norm_layer)]\n\n            ### upsample\n            model_upsample += [nn.ConvTranspose2d(ngf_global * 2, ngf_global, kernel_size=3, stride=2, padding=1, output_padding=1), \n                               norm_layer(ngf_global), nn.ReLU(True)]      \n\n            ### final convolution\n            if n == n_local_enhancers:                \n                model_upsample += [nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0), nn.Tanh()]                       \n            \n            setattr(self, 'model'+str(n)+'_1', nn.Sequential(*model_downsample))\n            setattr(self, 'model'+str(n)+'_2', nn.Sequential(*model_upsample))                  \n        \n        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n\n    def forward(self, input): \n        ### create input pyramid\n        input_downsampled = [input]\n        for i in range(self.n_local_enhancers):\n            input_downsampled.append(self.downsample(input_downsampled[-1]))\n\n        ### output at coarest level\n        output_prev = self.model(input_downsampled[-1])        \n        ### build up one layer at a time\n        for n_local_enhancers in range(1, self.n_local_enhancers+1):\n            model_downsample = getattr(self, 'model'+str(n_local_enhancers)+'_1')\n            model_upsample = getattr(self, 'model'+str(n_local_enhancers)+'_2')            \n            input_i = input_downsampled[self.n_local_enhancers-n_local_enhancers]            \n            output_prev = model_upsample(model_downsample(input_i) + output_prev)\n        return output_prev\n\nclass GlobalGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, n_downsampling=3, n_blocks=9, norm_layer=nn.BatchNorm2d, \n                 padding_type='reflect'):\n        assert(n_blocks >= 0)\n        super(GlobalGenerator, self).__init__()        \n        activation = nn.ReLU(True)        \n\n        model = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0), norm_layer(ngf), activation]\n        ### downsample\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1),\n                      norm_layer(ngf * mult * 2), activation]\n\n        ### resnet blocks\n        mult = 2**n_downsampling\n        for i in range(n_blocks):\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, activation=activation, norm_layer=norm_layer)]\n        \n        ### upsample         \n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n                       norm_layer(int(ngf * mult / 2)), activation]\n        model += [nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0), nn.Tanh()]        \n        self.model = nn.Sequential(*model)\n            \n    def forward(self, input):\n        return self.model(input)             \n        \n# Define a resnet block\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, padding_type, norm_layer, activation=nn.ReLU(True), use_dropout=False):\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, activation, use_dropout)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, activation, use_dropout):\n        conv_block = []\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n                       norm_layer(dim),\n                       activation]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n                       norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n        return out\n\nclass Encoder(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=32, n_downsampling=4, norm_layer=nn.BatchNorm2d):\n        super(Encoder, self).__init__()        \n        self.output_nc = output_nc        \n\n        model = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0), \n                 norm_layer(ngf), nn.ReLU(True)]             \n        ### downsample\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1),\n                      norm_layer(ngf * mult * 2), nn.ReLU(True)]\n\n        ### upsample         \n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n                       norm_layer(int(ngf * mult / 2)), nn.ReLU(True)]        \n\n        model += [nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0), nn.Tanh()]\n        self.model = nn.Sequential(*model) \n\n    def forward(self, input, inst):\n        outputs = self.model(input)\n\n        # instance-wise average pooling\n        outputs_mean = outputs.clone()\n        inst_list = np.unique(inst.cpu().numpy().astype(int))        \n        for i in inst_list:\n            for b in range(input.size()[0]):\n                indices = (inst[b:b+1] == int(i)).nonzero() # n x 4            \n                for j in range(self.output_nc):\n                    output_ins = outputs[indices[:,0] + b, indices[:,1] + j, indices[:,2], indices[:,3]]                    \n                    mean_feat = torch.mean(output_ins).expand_as(output_ins)                                        \n                    outputs_mean[indices[:,0] + b, indices[:,1] + j, indices[:,2], indices[:,3]] = mean_feat                       \n        return outputs_mean\n\nclass MultiscaleDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, \n                 use_sigmoid=False, num_D=3, getIntermFeat=False):\n        super(MultiscaleDiscriminator, self).__init__()\n        self.num_D = num_D\n        self.n_layers = n_layers\n        self.getIntermFeat = getIntermFeat\n     \n        for i in range(num_D):\n            netD = NLayerDiscriminator(input_nc, ndf, n_layers, norm_layer, use_sigmoid, getIntermFeat)\n            if getIntermFeat:                                \n                for j in range(n_layers+2):\n                    setattr(self, 'scale'+str(i)+'_layer'+str(j), getattr(netD, 'model'+str(j)))                                   \n            else:\n                setattr(self, 'layer'+str(i), netD.model)\n\n        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n\n    def singleD_forward(self, model, input):\n        if self.getIntermFeat:\n            result = [input]\n            for i in range(len(model)):\n                result.append(model[i](result[-1]))\n            return result[1:]\n        else:\n            return [model(input)]\n\n    def forward(self, input):        \n        num_D = self.num_D\n        result = []\n        input_downsampled = input\n        for i in range(num_D):\n            if self.getIntermFeat:\n                model = [getattr(self, 'scale'+str(num_D-1-i)+'_layer'+str(j)) for j in range(self.n_layers+2)]\n            else:\n                model = getattr(self, 'layer'+str(num_D-1-i))\n            result.append(self.singleD_forward(model, input_downsampled))\n            if i != (num_D-1):\n                input_downsampled = self.downsample(input_downsampled)\n        return result\n        \n# Defines the PatchGAN discriminator with the specified arguments.\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, getIntermFeat=False):\n        super(NLayerDiscriminator, self).__init__()\n        self.getIntermFeat = getIntermFeat\n        self.n_layers = n_layers\n\n        kw = 4\n        padw = int(np.ceil((kw-1.0)/2))\n        sequence = [[nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]]\n\n        nf = ndf\n        for n in range(1, n_layers):\n            nf_prev = nf\n            nf = min(nf * 2, 512)\n            sequence += [[\n                nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=2, padding=padw),\n                norm_layer(nf), nn.LeakyReLU(0.2, True)\n            ]]\n\n        nf_prev = nf\n        nf = min(nf * 2, 512)\n        sequence += [[\n            nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=1, padding=padw),\n            norm_layer(nf),\n            nn.LeakyReLU(0.2, True)\n        ]]\n\n        sequence += [[nn.Conv2d(nf, 1, kernel_size=kw, stride=1, padding=padw)]]\n\n        if use_sigmoid:\n            sequence += [[nn.Sigmoid()]]\n\n        if getIntermFeat:\n            for n in range(len(sequence)):\n                setattr(self, 'model'+str(n), nn.Sequential(*sequence[n]))\n        else:\n            sequence_stream = []\n            for n in range(len(sequence)):\n                sequence_stream += sequence[n]\n            self.model = nn.Sequential(*sequence_stream)\n\n    def forward(self, input):\n        if self.getIntermFeat:\n            res = [input]\n            for n in range(self.n_layers+2):\n                model = getattr(self, 'model'+str(n))\n                res.append(model(res[-1]))\n            return res[1:]\n        else:\n            return self.model(input)        \n\nfrom torchvision import models\nclass Vgg19(torch.nn.Module):\n    def __init__(self, requires_grad=False):\n        super(Vgg19, self).__init__()\n        vgg_pretrained_features = models.vgg19(pretrained=True).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        for x in range(2):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(2, 7):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(7, 12):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(12, 21):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(21, 30):\n            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, X):\n        h_relu1 = self.slice1(X)\n        h_relu2 = self.slice2(h_relu1)        \n        h_relu3 = self.slice3(h_relu2)        \n        h_relu4 = self.slice4(h_relu3)        \n        h_relu5 = self.slice5(h_relu4)                \n        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n        return out\n"""
src/pix2pixHD/models/pix2pixHD_model.py,15,"b'### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport numpy as np\nimport torch\nimport os\nfrom torch.autograd import Variable\nfrom util.image_pool import ImagePool\nfrom .base_model import BaseModel\nfrom . import networks\n\nclass Pix2PixHDModel(BaseModel):\n    def name(self):\n        return \'Pix2PixHDModel\'\n    \n    def init_loss_filter(self, use_gan_feat_loss, use_vgg_loss):\n        flags = (True, use_gan_feat_loss, use_vgg_loss, True, True)\n        def loss_filter(g_gan, g_gan_feat, g_vgg, d_real, d_fake):\n            return [l for (l,f) in zip((g_gan,g_gan_feat,g_vgg,d_real,d_fake),flags) if f]\n        return loss_filter\n    \n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n        if opt.resize_or_crop != \'none\' or not opt.isTrain: # when training at full res this causes OOM\n            torch.backends.cudnn.benchmark = True\n        self.isTrain = opt.isTrain\n        self.use_features = opt.instance_feat or opt.label_feat\n        self.gen_features = self.use_features and not self.opt.load_features\n        \n        ######extra add\n        self.gpu_ids = opt.gpu_ids\n        ####################\n        input_nc = opt.label_nc if opt.label_nc != 0 else opt.input_nc\n\n        ##### define networks        \n        # Generator network\n        netG_input_nc = input_nc        \n        if not opt.no_instance:\n            netG_input_nc += 1\n        if self.use_features:\n            netG_input_nc += opt.feat_num                  \n        self.netG = networks.define_G(netG_input_nc, opt.output_nc, opt.ngf, opt.netG, \n                                      opt.n_downsample_global, opt.n_blocks_global, opt.n_local_enhancers, \n                                      opt.n_blocks_local, opt.norm, gpu_ids=self.gpu_ids)        \n\n        # Discriminator network\n        if self.isTrain:\n            use_sigmoid = opt.no_lsgan\n            netD_input_nc = input_nc + opt.output_nc\n            if not opt.no_instance:\n                netD_input_nc += 1\n            self.netD = networks.define_D(netD_input_nc, opt.ndf, opt.n_layers_D, opt.norm, use_sigmoid, \n                                          opt.num_D, not opt.no_ganFeat_loss, gpu_ids=self.gpu_ids)\n\n        ### Encoder network\n        if self.gen_features:          \n            self.netE = networks.define_G(opt.output_nc, opt.feat_num, opt.nef, \'encoder\', \n                                          opt.n_downsample_E, norm=opt.norm, gpu_ids=self.gpu_ids)  \n        if self.opt.verbose:\n                print(\'---------- Networks initialized -------------\')\n\n        # load networks\n        if not self.isTrain or opt.continue_train or opt.load_pretrain:\n            pretrained_path = \'\' if not self.isTrain else opt.load_pretrain\n            self.load_network(self.netG, \'G\', opt.which_epoch, pretrained_path)            \n            if self.isTrain:\n                self.load_network(self.netD, \'D\', opt.which_epoch, pretrained_path)  \n            if self.gen_features:\n                self.load_network(self.netE, \'E\', opt.which_epoch, pretrained_path)              \n\n        # set loss functions and optimizers\n        if self.isTrain:\n            if opt.pool_size > 0 and (len(self.gpu_ids)) > 1:\n                raise NotImplementedError(""Fake Pool Not Implemented for MultiGPU"")\n            self.fake_pool = ImagePool(opt.pool_size)\n            self.old_lr = opt.lr\n\n            # define loss functions\n            self.loss_filter = self.init_loss_filter(not opt.no_ganFeat_loss, not opt.no_vgg_loss)\n            \n            self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)   \n            self.criterionFeat = torch.nn.L1Loss()\n            if not opt.no_vgg_loss:             \n                self.criterionVGG = networks.VGGLoss(self.gpu_ids)\n                \n        \n            # Names so we can breakout loss\n            self.loss_names = self.loss_filter(\'G_GAN\',\'G_GAN_Feat\',\'G_VGG\',\'D_real\', \'D_fake\')\n\n            # initialize optimizers\n            # optimizer G\n            if opt.niter_fix_global > 0:                \n                import sys\n                if sys.version_info >= (3,0):\n                    finetune_list = set()\n                else:\n                    from sets import Set\n                    finetune_list = Set()\n\n                params_dict = dict(self.netG.named_parameters())\n                params = []\n                for key, value in params_dict.items():       \n                    if key.startswith(\'model\' + str(opt.n_local_enhancers)):                    \n                        params += [value]\n                        finetune_list.add(key.split(\'.\')[0])  \n                print(\'------------- Only training the local enhancer network (for %d epochs) ------------\' % opt.niter_fix_global)\n                print(\'The layers that are finetuned are \', sorted(finetune_list))                         \n            else:\n                params = list(self.netG.parameters())\n            if self.gen_features:              \n                params += list(self.netE.parameters())         \n            self.optimizer_G = torch.optim.Adam(params, lr=opt.lr, betas=(opt.beta1, 0.999))                            \n\n            # optimizer D                        \n            params = list(self.netD.parameters())    \n            self.optimizer_D = torch.optim.Adam(params, lr=opt.lr, betas=(opt.beta1, 0.999))\n\n    def encode_input(self, label_map, inst_map=None, real_image=None, feat_map=None, infer=False):             \n        if self.opt.label_nc == 0:\n            input_label = label_map.data.cuda()\n        else:\n            # create one-hot vector for label map \n            size = label_map.size()\n            oneHot_size = (size[0], self.opt.label_nc, size[2], size[3])\n            input_label = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n            input_label = input_label.scatter_(1, label_map.data.long().cuda(), 1.0)\n            if self.opt.data_type == 16:\n                input_label = input_label.half()\n\n        # get edges from instance map\n        if not self.opt.no_instance:\n            inst_map = inst_map.data.cuda()\n            edge_map = self.get_edges(inst_map)\n            input_label = torch.cat((input_label, edge_map), dim=1) \n        input_label = Variable(input_label, volatile=infer)\n\n        # real images for training\n        if real_image is not None:\n            real_image = Variable(real_image.data.cuda())\n\n        # instance map for feature encoding\n        if self.use_features:\n            # get precomputed feature maps\n            if self.opt.load_features:\n                feat_map = Variable(feat_map.data.cuda())\n\n        return input_label, inst_map, real_image, feat_map\n\n    def discriminate(self, input_label, test_image, use_pool=False):\n        input_concat = torch.cat((input_label, test_image.detach()), dim=1)\n        if use_pool:            \n            fake_query = self.fake_pool.query(input_concat)\n            return self.netD.forward(fake_query)\n        else:\n            return self.netD.forward(input_concat)\n\n    def forward(self, label, inst, image, feat, infer=False):\n        # Encode Inputs\n        input_label, inst_map, real_image, feat_map = self.encode_input(label, inst, image, feat)  \n\n        # Fake Generation\n        if self.use_features:\n            if not self.opt.load_features:\n                feat_map = self.netE.forward(real_image, inst_map)                     \n            input_concat = torch.cat((input_label, feat_map), dim=1)                        \n        else:\n            input_concat = input_label\n        # TODO----------------------#    \n        fake_image = self.netG.forward(input_concat.float())\n\n        # Fake Detection and Loss\n        pred_fake_pool = self.discriminate(input_label, fake_image, use_pool=True)\n        loss_D_fake = self.criterionGAN(pred_fake_pool, False)        \n\n        # Real Detection and Loss        \n        pred_real = self.discriminate(input_label, real_image)\n        loss_D_real = self.criterionGAN(pred_real, True)\n\n        # GAN loss (Fake Passability Loss)        \n        pred_fake = self.netD.forward(torch.cat((input_label, fake_image), dim=1))        \n        loss_G_GAN = self.criterionGAN(pred_fake, True)               \n        \n        # GAN feature matching loss\n        loss_G_GAN_Feat = 0\n        if not self.opt.no_ganFeat_loss:\n            feat_weights = 4.0 / (self.opt.n_layers_D + 1)\n            D_weights = 1.0 / self.opt.num_D\n            for i in range(self.opt.num_D):\n                for j in range(len(pred_fake[i])-1):\n                    loss_G_GAN_Feat += D_weights * feat_weights * \\\n                        self.criterionFeat(pred_fake[i][j], pred_real[i][j].detach()) * self.opt.lambda_feat\n                   \n        # VGG feature matching loss\n        loss_G_VGG = 0\n        if not self.opt.no_vgg_loss:\n            loss_G_VGG = self.criterionVGG(fake_image, real_image) * self.opt.lambda_feat\n        \n        # Only return the fake_B image if necessary to save BW\n        return [ self.loss_filter( loss_G_GAN, loss_G_GAN_Feat, loss_G_VGG, loss_D_real, loss_D_fake ), None if not infer else fake_image ]\n\n    def inference(self, label, inst):\n        # Encode Inputs        \n        input_label, inst_map, _, _ = self.encode_input(Variable(label), Variable(inst), infer=True)\n\n        # Fake Generation\n        if self.use_features:       \n            # sample clusters from precomputed features             \n            feat_map = self.sample_features(inst_map)\n            input_concat = torch.cat((input_label, feat_map), dim=1)                        \n        else:\n            input_concat = input_label        \n           \n        if torch.__version__.startswith(\'0.4\'):\n            with torch.no_grad():\n                fake_image = self.netG.forward(input_concat)\n        else:\n            fake_image = self.netG.forward(input_concat)\n        return fake_image\n\n    def sample_features(self, inst): \n        # read precomputed feature clusters \n        cluster_path = os.path.join(self.opt.checkpoints_dir, self.opt.name, self.opt.cluster_path)        \n        features_clustered = np.load(cluster_path).item()\n\n        # randomly sample from the feature clusters\n        inst_np = inst.cpu().numpy().astype(int)                                      \n        feat_map = self.Tensor(inst.size()[0], self.opt.feat_num, inst.size()[2], inst.size()[3])\n        for i in np.unique(inst_np):    \n            label = i if i < 1000 else i//1000\n            if label in features_clustered:\n                feat = features_clustered[label]\n                cluster_idx = np.random.randint(0, feat.shape[0]) \n                                            \n                idx = (inst == int(i)).nonzero()\n                for k in range(self.opt.feat_num):                                    \n                    feat_map[idx[:,0], idx[:,1] + k, idx[:,2], idx[:,3]] = feat[cluster_idx, k]\n        if self.opt.data_type==16:\n            feat_map = feat_map.half()\n        return feat_map\n\n    def encode_features(self, image, inst):\n        image = Variable(image.cuda(), volatile=True)\n        feat_num = self.opt.feat_num\n        h, w = inst.size()[2], inst.size()[3]\n        block_num = 32\n        feat_map = self.netE.forward(image, inst.cuda())\n        inst_np = inst.cpu().numpy().astype(int)\n        feature = {}\n        for i in range(self.opt.label_nc):\n            feature[i] = np.zeros((0, feat_num+1))\n        for i in np.unique(inst_np):\n            label = i if i < 1000 else i//1000\n            idx = (inst == int(i)).nonzero()\n            num = idx.size()[0]\n            idx = idx[num//2,:]\n            val = np.zeros((1, feat_num+1))                        \n            for k in range(feat_num):\n                val[0, k] = feat_map[idx[0], idx[1] + k, idx[2], idx[3]].data[0]            \n            val[0, feat_num] = float(num) / (h * w // block_num)\n            feature[label] = np.append(feature[label], val, axis=0)\n        return feature\n\n    def get_edges(self, t):\n        edge = torch.cuda.ByteTensor(t.size()).zero_()\n        edge[:,:,:,1:] = edge[:,:,:,1:] | (t[:,:,:,1:] != t[:,:,:,:-1])\n        edge[:,:,:,:-1] = edge[:,:,:,:-1] | (t[:,:,:,1:] != t[:,:,:,:-1])\n        edge[:,:,1:,:] = edge[:,:,1:,:] | (t[:,:,1:,:] != t[:,:,:-1,:])\n        edge[:,:,:-1,:] = edge[:,:,:-1,:] | (t[:,:,1:,:] != t[:,:,:-1,:])\n        if self.opt.data_type==16:\n            return edge.half()\n        else:\n            return edge.float()\n\n    def save(self, which_epoch):\n        self.save_network(self.netG, \'G\', which_epoch, self.gpu_ids)\n        self.save_network(self.netD, \'D\', which_epoch, self.gpu_ids)\n        if self.gen_features:\n            self.save_network(self.netE, \'E\', which_epoch, self.gpu_ids)\n\n    def update_fixed_params(self):\n        # after fixing the global generator for a number of iterations, also start finetuning it\n        params = list(self.netG.parameters())\n        if self.gen_features:\n            params += list(self.netE.parameters())           \n        self.optimizer_G = torch.optim.Adam(params, lr=self.opt.lr, betas=(self.opt.beta1, 0.999))\n        if self.opt.verbose:\n            print(\'------------ Now also finetuning global generator -----------\')\n\n    def update_learning_rate(self):\n        lrd = self.opt.lr / self.opt.niter_decay\n        lr = self.old_lr - lrd        \n        for param_group in self.optimizer_D.param_groups:\n            param_group[\'lr\'] = lr\n        for param_group in self.optimizer_G.param_groups:\n            param_group[\'lr\'] = lr\n        if self.opt.verbose:\n            print(\'update learning rate: %f -> %f\' % (self.old_lr, lr))\n        self.old_lr = lr\n\nclass InferenceModel(Pix2PixHDModel):\n    def forward(self, inp):\n        label, inst = inp\n        return self.inference(label, inst)\n\n        \n'"
src/pix2pixHD/models/ui_model.py,10,"b'### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport torch\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nimport numpy as np\nimport os\nfrom PIL import Image\nimport util.util as util\nfrom .base_model import BaseModel\nfrom . import networks\n\nclass UIModel(BaseModel):\n    def name(self):\n        return \'UIModel\'\n\n    def initialize(self, opt):\n        assert(not opt.isTrain)\n        BaseModel.initialize(self, opt)\n        self.use_features = opt.instance_feat or opt.label_feat\n\n        netG_input_nc = opt.label_nc\n        if not opt.no_instance:\n            netG_input_nc += 1            \n        if self.use_features:   \n            netG_input_nc += opt.feat_num           \n\n        self.netG = networks.define_G(netG_input_nc, opt.output_nc, opt.ngf, opt.netG, \n                                      opt.n_downsample_global, opt.n_blocks_global, opt.n_local_enhancers, \n                                      opt.n_blocks_local, opt.norm, gpu_ids=self.gpu_ids)            \n        self.load_network(self.netG, \'G\', opt.which_epoch)\n\n        print(\'---------- Networks initialized -------------\')\n\n    def toTensor(self, img, normalize=False):\n        tensor = torch.from_numpy(np.array(img, np.int32, copy=False))\n        tensor = tensor.view(1, img.size[1], img.size[0], len(img.mode))    \n        tensor = tensor.transpose(1, 2).transpose(1, 3).contiguous()\n        if normalize:\n            return (tensor.float()/255.0 - 0.5) / 0.5        \n        return tensor.float()\n\n    def load_image(self, label_path, inst_path, feat_path):\n        opt = self.opt\n        # read label map\n        label_img = Image.open(label_path)    \n        if label_path.find(\'face\') != -1:\n            label_img = label_img.convert(\'L\')\n        ow, oh = label_img.size    \n        w = opt.loadSize\n        h = int(w * oh / ow)    \n        label_img = label_img.resize((w, h), Image.NEAREST)\n        label_map = self.toTensor(label_img)           \n        \n        # onehot vector input for label map\n        self.label_map = label_map.cuda()\n        oneHot_size = (1, opt.label_nc, h, w)\n        input_label = self.Tensor(torch.Size(oneHot_size)).zero_()\n        self.input_label = input_label.scatter_(1, label_map.long().cuda(), 1.0)\n\n        # read instance map\n        if not opt.no_instance:\n            inst_img = Image.open(inst_path)        \n            inst_img = inst_img.resize((w, h), Image.NEAREST)            \n            self.inst_map = self.toTensor(inst_img).cuda()\n            self.edge_map = self.get_edges(self.inst_map)          \n            self.net_input = Variable(torch.cat((self.input_label, self.edge_map), dim=1), volatile=True)\n        else:\n            self.net_input = Variable(self.input_label, volatile=True)  \n        \n        self.features_clustered = np.load(feat_path).item()\n        self.object_map = self.inst_map if opt.instance_feat else self.label_map \n                       \n        object_np = self.object_map.cpu().numpy().astype(int) \n        self.feat_map = self.Tensor(1, opt.feat_num, h, w).zero_()                 \n        self.cluster_indices = np.zeros(self.opt.label_nc, np.uint8)\n        for i in np.unique(object_np):    \n            label = i if i < 1000 else i//1000\n            if label in self.features_clustered:\n                feat = self.features_clustered[label]\n                np.random.seed(i+1)\n                cluster_idx = np.random.randint(0, feat.shape[0])\n                self.cluster_indices[label] = cluster_idx\n                idx = (self.object_map == i).nonzero()                    \n                self.set_features(idx, feat, cluster_idx)\n\n        self.net_input_original = self.net_input.clone()        \n        self.label_map_original = self.label_map.clone()\n        self.feat_map_original = self.feat_map.clone()\n        if not opt.no_instance:\n            self.inst_map_original = self.inst_map.clone()        \n\n    def reset(self):\n        self.net_input = self.net_input_prev = self.net_input_original.clone()        \n        self.label_map = self.label_map_prev = self.label_map_original.clone()\n        self.feat_map = self.feat_map_prev = self.feat_map_original.clone()\n        if not self.opt.no_instance:\n            self.inst_map = self.inst_map_prev = self.inst_map_original.clone()\n        self.object_map = self.inst_map if self.opt.instance_feat else self.label_map \n\n    def undo(self):        \n        self.net_input = self.net_input_prev\n        self.label_map = self.label_map_prev\n        self.feat_map = self.feat_map_prev\n        if not self.opt.no_instance:\n            self.inst_map = self.inst_map_prev\n        self.object_map = self.inst_map if self.opt.instance_feat else self.label_map \n            \n    # get boundary map from instance map\n    def get_edges(self, t):\n        edge = torch.cuda.ByteTensor(t.size()).zero_()\n        edge[:,:,:,1:] = edge[:,:,:,1:] | (t[:,:,:,1:] != t[:,:,:,:-1])\n        edge[:,:,:,:-1] = edge[:,:,:,:-1] | (t[:,:,:,1:] != t[:,:,:,:-1])\n        edge[:,:,1:,:] = edge[:,:,1:,:] | (t[:,:,1:,:] != t[:,:,:-1,:])\n        edge[:,:,:-1,:] = edge[:,:,:-1,:] | (t[:,:,1:,:] != t[:,:,:-1,:])\n        return edge.float()\n\n    # change the label at the source position to the label at the target position\n    def change_labels(self, click_src, click_tgt): \n        y_src, x_src = click_src[0], click_src[1]\n        y_tgt, x_tgt = click_tgt[0], click_tgt[1]\n        label_src = int(self.label_map[0, 0, y_src, x_src])\n        inst_src = self.inst_map[0, 0, y_src, x_src]\n        label_tgt = int(self.label_map[0, 0, y_tgt, x_tgt])\n        inst_tgt = self.inst_map[0, 0, y_tgt, x_tgt]\n\n        idx_src = (self.inst_map == inst_src).nonzero()         \n        # need to change 3 things: label map, instance map, and feature map\n        if idx_src.shape:\n            # backup current maps\n            self.backup_current_state() \n\n            # change both the label map and the network input\n            self.label_map[idx_src[:,0], idx_src[:,1], idx_src[:,2], idx_src[:,3]] = label_tgt\n            self.net_input[idx_src[:,0], idx_src[:,1] + label_src, idx_src[:,2], idx_src[:,3]] = 0\n            self.net_input[idx_src[:,0], idx_src[:,1] + label_tgt, idx_src[:,2], idx_src[:,3]] = 1                                    \n            \n            # update the instance map (and the network input)\n            if inst_tgt > 1000:\n                # if different instances have different ids, give the new object a new id\n                tgt_indices = (self.inst_map > label_tgt * 1000) & (self.inst_map < (label_tgt+1) * 1000)\n                inst_tgt = self.inst_map[tgt_indices].max() + 1\n            self.inst_map[idx_src[:,0], idx_src[:,1], idx_src[:,2], idx_src[:,3]] = inst_tgt\n            self.net_input[:,-1,:,:] = self.get_edges(self.inst_map)\n\n            # also copy the source features to the target position      \n            idx_tgt = (self.inst_map == inst_tgt).nonzero()    \n            if idx_tgt.shape:\n                self.copy_features(idx_src, idx_tgt[0,:])\n\n        self.fake_image = util.tensor2im(self.single_forward(self.net_input, self.feat_map))\n\n    # add strokes of target label in the image\n    def add_strokes(self, click_src, label_tgt, bw, save):\n        # get the region of the new strokes (bw is the brush width)        \n        size = self.net_input.size()\n        h, w = size[2], size[3]\n        idx_src = torch.LongTensor(bw**2, 4).fill_(0)\n        for i in range(bw):\n            idx_src[i*bw:(i+1)*bw, 2] = min(h-1, max(0, click_src[0]-bw//2 + i))\n            for j in range(bw):\n                idx_src[i*bw+j, 3] = min(w-1, max(0, click_src[1]-bw//2 + j))\n        idx_src = idx_src.cuda()\n        \n        # again, need to update 3 things\n        if idx_src.shape:\n            # backup current maps\n            if save:\n                self.backup_current_state()\n\n            # update the label map (and the network input) in the stroke region            \n            self.label_map[idx_src[:,0], idx_src[:,1], idx_src[:,2], idx_src[:,3]] = label_tgt\n            for k in range(self.opt.label_nc):\n                self.net_input[idx_src[:,0], idx_src[:,1] + k, idx_src[:,2], idx_src[:,3]] = 0\n            self.net_input[idx_src[:,0], idx_src[:,1] + label_tgt, idx_src[:,2], idx_src[:,3]] = 1                 \n\n            # update the instance map (and the network input)\n            self.inst_map[idx_src[:,0], idx_src[:,1], idx_src[:,2], idx_src[:,3]] = label_tgt\n            self.net_input[:,-1,:,:] = self.get_edges(self.inst_map)\n            \n            # also update the features if available\n            if self.opt.instance_feat:                                            \n                feat = self.features_clustered[label_tgt]\n                #np.random.seed(label_tgt+1)   \n                #cluster_idx = np.random.randint(0, feat.shape[0])\n                cluster_idx = self.cluster_indices[label_tgt]\n                self.set_features(idx_src, feat, cluster_idx)                                                  \n        \n        self.fake_image = util.tensor2im(self.single_forward(self.net_input, self.feat_map))\n\n    # add an object to the clicked position with selected style\n    def add_objects(self, click_src, label_tgt, mask, style_id=0):\n        y, x = click_src[0], click_src[1]\n        mask = np.transpose(mask, (2, 0, 1))[np.newaxis,...]        \n        idx_src = torch.from_numpy(mask).cuda().nonzero()        \n        idx_src[:,2] += y\n        idx_src[:,3] += x\n\n        # backup current maps\n        self.backup_current_state()\n\n        # update label map\n        self.label_map[idx_src[:,0], idx_src[:,1], idx_src[:,2], idx_src[:,3]] = label_tgt        \n        for k in range(self.opt.label_nc):\n            self.net_input[idx_src[:,0], idx_src[:,1] + k, idx_src[:,2], idx_src[:,3]] = 0\n        self.net_input[idx_src[:,0], idx_src[:,1] + label_tgt, idx_src[:,2], idx_src[:,3]] = 1            \n\n        # update instance map\n        self.inst_map[idx_src[:,0], idx_src[:,1], idx_src[:,2], idx_src[:,3]] = label_tgt\n        self.net_input[:,-1,:,:] = self.get_edges(self.inst_map)\n                \n        # update feature map\n        self.set_features(idx_src, self.feat, style_id)                \n        \n        self.fake_image = util.tensor2im(self.single_forward(self.net_input, self.feat_map))\n\n    def single_forward(self, net_input, feat_map):\n        net_input = torch.cat((net_input, feat_map), dim=1)\n        fake_image = self.netG.forward(net_input)\n\n        if fake_image.size()[0] == 1:\n            return fake_image.data[0]        \n        return fake_image.data\n\n\n    # generate all outputs for different styles\n    def style_forward(self, click_pt, style_id=-1):           \n        if click_pt is None:            \n            self.fake_image = util.tensor2im(self.single_forward(self.net_input, self.feat_map))\n            self.crop = None\n            self.mask = None        \n        else:                       \n            instToChange = int(self.object_map[0, 0, click_pt[0], click_pt[1]])\n            self.instToChange = instToChange\n            label = instToChange if instToChange < 1000 else instToChange//1000        \n            self.feat = self.features_clustered[label]\n            self.fake_image = []\n            self.mask = self.object_map == instToChange\n            idx = self.mask.nonzero()\n            self.get_crop_region(idx)            \n            if idx.size():                \n                if style_id == -1:\n                    (min_y, min_x, max_y, max_x) = self.crop\n                    ### original\n                    for cluster_idx in range(self.opt.multiple_output):\n                        self.set_features(idx, self.feat, cluster_idx)\n                        fake_image = self.single_forward(self.net_input, self.feat_map)\n                        fake_image = util.tensor2im(fake_image[:,min_y:max_y,min_x:max_x])\n                        self.fake_image.append(fake_image)    \n                    """"""### To speed up previewing different style results, either crop or downsample the label maps\n                    if instToChange > 1000:\n                        (min_y, min_x, max_y, max_x) = self.crop                                                \n                        ### crop                                                \n                        _, _, h, w = self.net_input.size()\n                        offset = 512\n                        y_start, x_start = max(0, min_y-offset), max(0, min_x-offset)\n                        y_end, x_end = min(h, (max_y + offset)), min(w, (max_x + offset))\n                        y_region = slice(y_start, y_start+(y_end-y_start)//16*16)\n                        x_region = slice(x_start, x_start+(x_end-x_start)//16*16)\n                        net_input = self.net_input[:,:,y_region,x_region]                    \n                        for cluster_idx in range(self.opt.multiple_output):  \n                            self.set_features(idx, self.feat, cluster_idx)\n                            fake_image = self.single_forward(net_input, self.feat_map[:,:,y_region,x_region])                            \n                            fake_image = util.tensor2im(fake_image[:,min_y-y_start:max_y-y_start,min_x-x_start:max_x-x_start])\n                            self.fake_image.append(fake_image)\n                    else:\n                        ### downsample\n                        (min_y, min_x, max_y, max_x) = [crop//2 for crop in self.crop]                    \n                        net_input = self.net_input[:,:,::2,::2]                    \n                        size = net_input.size()\n                        net_input_batch = net_input.expand(self.opt.multiple_output, size[1], size[2], size[3])             \n                        for cluster_idx in range(self.opt.multiple_output):  \n                            self.set_features(idx, self.feat, cluster_idx)\n                            feat_map = self.feat_map[:,:,::2,::2]\n                            if cluster_idx == 0:\n                                feat_map_batch = feat_map\n                            else:\n                                feat_map_batch = torch.cat((feat_map_batch, feat_map), dim=0)\n                        fake_image_batch = self.single_forward(net_input_batch, feat_map_batch)\n                        for i in range(self.opt.multiple_output):\n                            self.fake_image.append(util.tensor2im(fake_image_batch[i,:,min_y:max_y,min_x:max_x]))""""""\n                                        \n                else:\n                    self.set_features(idx, self.feat, style_id)\n                    self.cluster_indices[label] = style_id\n                    self.fake_image = util.tensor2im(self.single_forward(self.net_input, self.feat_map))        \n\n    def backup_current_state(self):\n        self.net_input_prev = self.net_input.clone()\n        self.label_map_prev = self.label_map.clone() \n        self.inst_map_prev = self.inst_map.clone() \n        self.feat_map_prev = self.feat_map.clone() \n\n    # crop the ROI and get the mask of the object\n    def get_crop_region(self, idx):\n        size = self.net_input.size()\n        h, w = size[2], size[3]\n        min_y, min_x = idx[:,2].min(), idx[:,3].min()\n        max_y, max_x = idx[:,2].max(), idx[:,3].max()             \n        crop_min = 128\n        if max_y - min_y < crop_min:\n            min_y = max(0, (max_y + min_y) // 2 - crop_min // 2)\n            max_y = min(h-1, min_y + crop_min)\n        if max_x - min_x < crop_min:\n            min_x = max(0, (max_x + min_x) // 2 - crop_min // 2)\n            max_x = min(w-1, min_x + crop_min)\n        self.crop = (min_y, min_x, max_y, max_x)           \n        self.mask = self.mask[:,:, min_y:max_y, min_x:max_x]\n\n    # update the feature map once a new object is added or the label is changed\n    def update_features(self, cluster_idx, mask=None, click_pt=None):        \n        self.feat_map_prev = self.feat_map.clone()\n        # adding a new object\n        if mask is not None:\n            y, x = click_pt[0], click_pt[1]\n            mask = np.transpose(mask, (2,0,1))[np.newaxis,...]        \n            idx = torch.from_numpy(mask).cuda().nonzero()        \n            idx[:,2] += y\n            idx[:,3] += x    \n        # changing the label of an existing object \n        else:            \n            idx = (self.object_map == self.instToChange).nonzero()              \n\n        # update feature map\n        self.set_features(idx, self.feat, cluster_idx)        \n\n    # set the class features to the target feature\n    def set_features(self, idx, feat, cluster_idx):        \n        for k in range(self.opt.feat_num):\n            self.feat_map[idx[:,0], idx[:,1] + k, idx[:,2], idx[:,3]] = feat[cluster_idx, k] \n\n    # copy the features at the target position to the source position\n    def copy_features(self, idx_src, idx_tgt):        \n        for k in range(self.opt.feat_num):\n            val = self.feat_map[idx_tgt[0], idx_tgt[1] + k, idx_tgt[2], idx_tgt[3]]\n            self.feat_map[idx_src[:,0], idx_src[:,1] + k, idx_src[:,2], idx_src[:,3]] = val \n\n    def get_current_visuals(self, getLabel=False):                              \n        mask = self.mask     \n        if self.mask is not None:\n            mask = np.transpose(self.mask[0].cpu().float().numpy(), (1,2,0)).astype(np.uint8)        \n\n        dict_list = [(\'fake_image\', self.fake_image), (\'mask\', mask)]\n\n        if getLabel: # only output label map if needed to save bandwidth\n            label = util.tensor2label(self.net_input.data[0], self.opt.label_nc)                    \n            dict_list += [(\'label\', label)]\n\n        return OrderedDict(dict_list)'"
src/pix2pixHD/options/__init__.py,0,b''
src/pix2pixHD/options/base_options.py,1,"b'### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport argparse\nimport os\nfrom util import util\nimport torch\n\nclass BaseOptions():\n    def __init__(self):\n        self.parser = argparse.ArgumentParser()\n        self.initialized = False\n\n    def initialize(self):    \n        # experiment specifics\n        self.parser.add_argument(\'--name\', type=str, default=\'label2city\', help=\'name of the experiment. It decides where to store samples and models\')        \n        self.parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU\')\n        self.parser.add_argument(\'--checkpoints_dir\', type=str, default=\'./checkpoints\', help=\'models are saved here\')\n        self.parser.add_argument(\'--model\', type=str, default=\'pix2pixHD\', help=\'which model to use\')\n        self.parser.add_argument(\'--norm\', type=str, default=\'instance\', help=\'instance normalization or batch normalization\')        \n        self.parser.add_argument(\'--use_dropout\', action=\'store_true\', help=\'use dropout for the generator\')\n        self.parser.add_argument(\'--data_type\', default=32, type=int, choices=[8, 16, 32], help=""Supported data type i.e. 8, 16, 32 bit"")\n        self.parser.add_argument(\'--verbose\', action=\'store_true\', default=False, help=\'toggles verbose\')\n\n        # input/output sizes       \n        self.parser.add_argument(\'--batchSize\', type=int, default=1, help=\'input batch size\')\n        self.parser.add_argument(\'--loadSize\', type=int, default=1024, help=\'scale images to this size\')\n        self.parser.add_argument(\'--fineSize\', type=int, default=512, help=\'then crop to this size\')\n        self.parser.add_argument(\'--label_nc\', type=int, default=35, help=\'# of input label channels\')\n        self.parser.add_argument(\'--input_nc\', type=int, default=3, help=\'# of input image channels\')\n        self.parser.add_argument(\'--output_nc\', type=int, default=3, help=\'# of output image channels\')\n\n        # for setting inputs\n        self.parser.add_argument(\'--dataroot\', type=str, default=\'./datasets/cityscapes/\') \n        self.parser.add_argument(\'--resize_or_crop\', type=str, default=\'scale_width\', help=\'scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop]\')\n        self.parser.add_argument(\'--serial_batches\', action=\'store_true\', help=\'if true, takes images in order to make batches, otherwise takes them randomly\')        \n        self.parser.add_argument(\'--no_flip\', action=\'store_true\', help=\'if specified, do not flip the images for data argumentation\') \n        self.parser.add_argument(\'--nThreads\', default=2, type=int, help=\'# threads for loading data\')                \n        self.parser.add_argument(\'--max_dataset_size\', type=int, default=float(""inf""), help=\'Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.\')\n\n        # for displays\n        self.parser.add_argument(\'--display_winsize\', type=int, default=512,  help=\'display window size\')\n        self.parser.add_argument(\'--tf_log\', action=\'store_true\', help=\'if specified, use tensorboard logging. Requires tensorflow installed\')\n\n        # for generator\n        self.parser.add_argument(\'--netG\', type=str, default=\'global\', help=\'selects model to use for netG\')\n        self.parser.add_argument(\'--ngf\', type=int, default=64, help=\'# of gen filters in first conv layer\')\n        self.parser.add_argument(\'--n_downsample_global\', type=int, default=4, help=\'number of downsampling layers in netG\') \n        self.parser.add_argument(\'--n_blocks_global\', type=int, default=9, help=\'number of residual blocks in the global generator network\')\n        self.parser.add_argument(\'--n_blocks_local\', type=int, default=3, help=\'number of residual blocks in the local enhancer network\')\n        self.parser.add_argument(\'--n_local_enhancers\', type=int, default=1, help=\'number of local enhancers to use\')        \n        self.parser.add_argument(\'--niter_fix_global\', type=int, default=0, help=\'number of epochs that we only train the outmost local enhancer\')        \n\n        # for instance-wise features\n        self.parser.add_argument(\'--no_instance\', action=\'store_true\', help=\'if specified, do *not* add instance map as input\')        \n        self.parser.add_argument(\'--instance_feat\', action=\'store_true\', help=\'if specified, add encoded instance features as input\')\n        self.parser.add_argument(\'--label_feat\', action=\'store_true\', help=\'if specified, add encoded label features as input\')        \n        self.parser.add_argument(\'--feat_num\', type=int, default=3, help=\'vector length for encoded features\')        \n        self.parser.add_argument(\'--load_features\', action=\'store_true\', help=\'if specified, load precomputed feature maps\')\n        self.parser.add_argument(\'--n_downsample_E\', type=int, default=4, help=\'# of downsampling layers in encoder\') \n        self.parser.add_argument(\'--nef\', type=int, default=16, help=\'# of encoder filters in the first conv layer\')        \n        self.parser.add_argument(\'--n_clusters\', type=int, default=10, help=\'number of clusters for features\')        \n\n        self.initialized = True\n\n    def parse(self, save=True):\n        if not self.initialized:\n            self.initialize()\n        self.opt = self.parser.parse_args()\n        self.opt.isTrain = self.isTrain   # train or test\n\n        str_ids = self.opt.gpu_ids.split(\',\')\n        self.opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                self.opt.gpu_ids.append(id)\n        \n        # set gpu ids\n        if len(self.opt.gpu_ids) > 0:\n            torch.cuda.set_device(self.opt.gpu_ids[0])\n\n        args = vars(self.opt)\n\n        print(\'------------ Options -------------\')\n        for k, v in sorted(args.items()):\n            print(\'%s: %s\' % (str(k), str(v)))\n        print(\'-------------- End ----------------\')\n\n        # save to the disk        \n        expr_dir = os.path.join(self.opt.checkpoints_dir, self.opt.name)\n        util.mkdirs(expr_dir)\n        if save and not self.opt.continue_train:\n            file_name = os.path.join(expr_dir, \'opt.txt\')\n            with open(file_name, \'wt\') as opt_file:\n                opt_file.write(\'------------ Options -------------\\n\')\n                for k, v in sorted(args.items()):\n                    opt_file.write(\'%s: %s\\n\' % (str(k), str(v)))\n                opt_file.write(\'-------------- End ----------------\\n\')\n        return self.opt\n'"
src/pix2pixHD/options/test_options.py,0,"b'### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nfrom .base_options import BaseOptions\n\nclass TestOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        self.parser.add_argument(\'--ntest\', type=int, default=float(""inf""), help=\'# of test examples.\')\n        self.parser.add_argument(\'--results_dir\', type=str, default=\'./results/\', help=\'saves results here.\')\n        self.parser.add_argument(\'--aspect_ratio\', type=float, default=1.0, help=\'aspect ratio of result images\')\n        self.parser.add_argument(\'--phase\', type=str, default=\'test\', help=\'train, val, test, etc\')\n        self.parser.add_argument(\'--which_epoch\', type=str, default=\'latest\', help=\'which epoch to load? set to latest to use latest cached model\')\n        self.parser.add_argument(\'--how_many\', type=int, default=50, help=\'how many test images to run\')       \n        self.parser.add_argument(\'--cluster_path\', type=str, default=\'features_clustered_010.npy\', help=\'the path for clustered results of encoded features\')\n        self.parser.add_argument(""--export_onnx"", type=str, help=""export ONNX model to a given file"")\n        self.parser.add_argument(""--engine"", type=str, help=""run serialized TRT engine"")\n        self.parser.add_argument(""--onnx"", type=str, help=""run ONNX model via TRT"")        \n        self.isTrain = False\n'"
src/pix2pixHD/options/train_options.py,0,"b""### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nfrom .base_options import BaseOptions\n\nclass TrainOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        # for displays\n        self.parser.add_argument('--display_freq', type=int, default=100, help='frequency of showing training results on screen')\n        self.parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n        self.parser.add_argument('--save_latest_freq', type=int, default=1000, help='frequency of saving the latest results')\n        self.parser.add_argument('--save_epoch_freq', type=int, default=10, help='frequency of saving checkpoints at the end of epochs')        \n        self.parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n        self.parser.add_argument('--debug', action='store_true', help='only do one epoch and displays at each iteration')\n\n        # for training\n        self.parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n        self.parser.add_argument('--load_pretrain', type=str, default='', help='load the pretrained model from the specified location')\n        self.parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n        self.parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n        self.parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate')\n        self.parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero')\n        self.parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n        self.parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n\n        # for discriminators        \n        self.parser.add_argument('--num_D', type=int, default=2, help='number of discriminators to use')\n        self.parser.add_argument('--n_layers_D', type=int, default=3, help='only used if which_model_netD==n_layers')\n        self.parser.add_argument('--ndf', type=int, default=64, help='# of discrim filters in first conv layer')    \n        self.parser.add_argument('--lambda_feat', type=float, default=10.0, help='weight for feature matching loss')                \n        self.parser.add_argument('--no_ganFeat_loss', action='store_true', help='if specified, do *not* use discriminator feature matching loss')\n        self.parser.add_argument('--no_vgg_loss', action='store_true', help='if specified, do *not* use VGG feature matching loss')        \n        self.parser.add_argument('--no_lsgan', action='store_true', help='do *not* use least square GAN, if false, use vanilla GAN')\n        self.parser.add_argument('--pool_size', type=int, default=0, help='the size of image buffer that stores previously generated images')\n\n        self.isTrain = True\n"""
src/pix2pixHD/util/__init__.py,0,b''
src/pix2pixHD/util/html.py,0,"b'import dominate\nfrom dominate.tags import *\nimport os\n\n\nclass HTML:\n    def __init__(self, web_dir, title, refresh=0):\n        self.title = title\n        self.web_dir = web_dir\n        self.img_dir = os.path.join(self.web_dir, \'images\')\n        if not os.path.exists(self.web_dir):\n            os.makedirs(self.web_dir)\n        if not os.path.exists(self.img_dir):\n            os.makedirs(self.img_dir)\n\n        self.doc = dominate.document(title=title)\n        if refresh > 0:\n            with self.doc.head:\n                meta(http_equiv=""refresh"", content=str(refresh))\n\n    def get_image_dir(self):\n        return self.img_dir\n\n    def add_header(self, str):\n        with self.doc:\n            h3(str)\n\n    def add_table(self, border=1):\n        self.t = table(border=border, style=""table-layout: fixed;"")\n        self.doc.add(self.t)\n\n    def add_images(self, ims, txts, links, width=512):\n        self.add_table()\n        with self.t:\n            with tr():\n                for im, txt, link in zip(ims, txts, links):\n                    with td(style=""word-wrap: break-word;"", halign=""center"", valign=""top""):\n                        with p():\n                            with a(href=os.path.join(\'images\', link)):\n                                img(style=""width:%dpx"" % (width), src=os.path.join(\'images\', im))\n                            br()\n                            p(txt)\n\n    def save(self):\n        html_file = \'%s/index.html\' % self.web_dir\n        f = open(html_file, \'wt\')\n        f.write(self.doc.render())\n        f.close()\n\n\nif __name__ == \'__main__\':\n    html = HTML(\'web/\', \'test_html\')\n    html.add_header(\'hello world\')\n\n    ims = []\n    txts = []\n    links = []\n    for n in range(4):\n        ims.append(\'image_%d.jpg\' % n)\n        txts.append(\'text_%d\' % n)\n        links.append(\'image_%d.jpg\' % n)\n    html.add_images(ims, txts, links)\n    html.save()\n'"
src/pix2pixHD/util/image_pool.py,3,"b'import random\nimport torch\nfrom torch.autograd import Variable\nclass ImagePool():\n    def __init__(self, pool_size):\n        self.pool_size = pool_size\n        if self.pool_size > 0:\n            self.num_imgs = 0\n            self.images = []\n\n    def query(self, images):\n        if self.pool_size == 0:\n            return images\n        return_images = []\n        for image in images.data:\n            image = torch.unsqueeze(image, 0)\n            if self.num_imgs < self.pool_size:\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n                return_images.append(image)\n            else:\n                p = random.uniform(0, 1)\n                if p > 0.5:\n                    random_id = random.randint(0, self.pool_size-1)\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image\n                    return_images.append(tmp)\n                else:\n                    return_images.append(image)\n        return_images = Variable(torch.cat(return_images, 0))\n        return return_images\n'"
src/pix2pixHD/util/util.py,2,"b'from __future__ import print_function\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport numpy as np\nimport os\n\n# Converts a Tensor into a Numpy array\n# |imtype|: the desired type of the converted numpy array\ndef tensor2im(image_tensor, imtype=np.uint8, normalize=True):\n    if isinstance(image_tensor, list):\n        image_numpy = []\n        for i in range(len(image_tensor)):\n            image_numpy.append(tensor2im(image_tensor[i], imtype, normalize))\n        return image_numpy\n    image_numpy = image_tensor.cpu().float().numpy()\n    if normalize:\n        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n    else:\n        image_numpy = np.transpose(image_numpy, (1, 2, 0)) * 255.0      \n    image_numpy = np.clip(image_numpy, 0, 255)\n    if image_numpy.shape[2] == 1 or image_numpy.shape[2] > 3:        \n        image_numpy = image_numpy[:,:,0]\n    return image_numpy.astype(imtype)\n\n# Converts a one-hot tensor into a colorful label map\ndef tensor2label(label_tensor, n_label, imtype=np.uint8):\n    if n_label == 0:\n        return tensor2im(label_tensor, imtype)\n    label_tensor = label_tensor.cpu().float()    \n    if label_tensor.size()[0] > 1:\n        label_tensor = label_tensor.max(0, keepdim=True)[1]\n    label_tensor = Colorize(n_label)(label_tensor)\n    label_numpy = np.transpose(label_tensor.numpy(), (1, 2, 0))\n    return label_numpy.astype(imtype)\n\ndef save_image(image_numpy, image_path):\n    image_pil = Image.fromarray(image_numpy)\n    image_pil.save(image_path)\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n###############################################################################\n# Code from\n# https://github.com/ycszen/pytorch-seg/blob/master/transform.py\n# Modified so it complies with the Citscape label map colors\n###############################################################################\ndef uint82bin(n, count=8):\n    """"""returns the binary of integer n, count refers to amount of bits""""""\n    return \'\'.join([str((n >> y) & 1) for y in range(count-1, -1, -1)])\n\ndef labelcolormap(N):\n    if N == 35: # cityscape\n        cmap = np.array([(  0,  0,  0), (  0,  0,  0), (  0,  0,  0), (  0,  0,  0), (  0,  0,  0), (111, 74,  0), ( 81,  0, 81),\n                     (128, 64,128), (244, 35,232), (250,170,160), (230,150,140), ( 70, 70, 70), (102,102,156), (190,153,153),\n                     (180,165,180), (150,100,100), (150,120, 90), (153,153,153), (153,153,153), (250,170, 30), (220,220,  0),\n                     (107,142, 35), (152,251,152), ( 70,130,180), (220, 20, 60), (255,  0,  0), (  0,  0,142), (  0,  0, 70),\n                     (  0, 60,100), (  0,  0, 90), (  0,  0,110), (  0, 80,100), (  0,  0,230), (119, 11, 32), (  0,  0,142)], \n                     dtype=np.uint8)\n    else:\n        cmap = np.zeros((N, 3), dtype=np.uint8)\n        for i in range(N):\n            r, g, b = 0, 0, 0\n            id = i\n            for j in range(7):\n                str_id = uint82bin(id)\n                r = r ^ (np.uint8(str_id[-1]) << (7-j))\n                g = g ^ (np.uint8(str_id[-2]) << (7-j))\n                b = b ^ (np.uint8(str_id[-3]) << (7-j))\n                id = id >> 3\n            cmap[i, 0] = r\n            cmap[i, 1] = g\n            cmap[i, 2] = b\n    return cmap\n\nclass Colorize(object):\n    def __init__(self, n=35):\n        self.cmap = labelcolormap(n)\n        self.cmap = torch.from_numpy(self.cmap[:n])\n\n    def __call__(self, gray_image):\n        size = gray_image.size()\n        color_image = torch.ByteTensor(3, size[1], size[2]).fill_(0)\n\n        for label in range(0, len(self.cmap)):\n            mask = (label == gray_image[0]).cpu()\n            color_image[0][mask] = self.cmap[label][0]\n            color_image[1][mask] = self.cmap[label][1]\n            color_image[2][mask] = self.cmap[label][2]\n\n        return color_image\n'"
src/pix2pixHD/util/visualizer.py,0,"b'### Copyright (C) 2017 NVIDIA Corporation. All rights reserved.\n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport numpy as np\nimport os\nimport ntpath\nimport time\nfrom . import util\nfrom . import html\nimport scipy.misc\ntry:\n    from StringIO import StringIO  # Python 2.7\nexcept ImportError:\n    from io import BytesIO         # Python 3.x\n\nclass Visualizer():\n    def __init__(self, opt):\n        # self.opt = opt\n        self.tf_log = opt.tf_log\n        self.use_html = opt.isTrain and not opt.no_html\n        self.win_size = opt.display_winsize\n        self.name = opt.name\n        if self.tf_log:\n            import tensorflow as tf\n            self.tf = tf\n            self.log_dir = os.path.join(opt.checkpoints_dir, opt.name, \'logs\')\n            self.writer = tf.summary.FileWriter(self.log_dir)\n\n        if self.use_html:\n            self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, \'web\')\n            self.img_dir = os.path.join(self.web_dir, \'images\')\n            print(\'create web directory %s...\' % self.web_dir)\n            util.mkdirs([self.web_dir, self.img_dir])\n        self.log_name = os.path.join(opt.checkpoints_dir, opt.name, \'loss_log.txt\')\n        with open(self.log_name, ""a"") as log_file:\n            now = time.strftime(""%c"")\n            log_file.write(\'================ Training Loss (%s) ================\\n\' % now)\n\n    # |visuals|: dictionary of images to display or save\n    def display_current_results(self, visuals, epoch, step):\n        if self.tf_log: # show images in tensorboard output\n            img_summaries = []\n            for label, image_numpy in visuals.items():\n                # Write the image to a string\n                try:\n                    s = StringIO()\n                except:\n                    s = BytesIO()\n                print(image_numpy)\n                scipy.misc.toimage(image_numpy).save(s, format=""jpeg"")\n                # Create an Image object\n                img_sum = self.tf.Summary.Image(encoded_image_string=s.getvalue(), height=image_numpy.shape[0], width=image_numpy.shape[1])\n                # Create a Summary value\n                img_summaries.append(self.tf.Summary.Value(tag=label, image=img_sum))\n\n            # Create and write Summary\n            summary = self.tf.Summary(value=img_summaries)\n            self.writer.add_summary(summary, step)\n\n        if self.use_html: # save images to a html file\n            for label, image_numpy in visuals.items():\n                if isinstance(image_numpy, list):\n                    for i in range(len(image_numpy)):\n                        img_path = os.path.join(self.img_dir, \'epoch%.3d_%s_%d.jpg\' % (epoch, label, i))\n                        util.save_image(image_numpy[i], img_path)\n                else:\n                    img_path = os.path.join(self.img_dir, \'epoch%.3d_%s.jpg\' % (epoch, label))\n                    util.save_image(image_numpy, img_path)\n\n            # update website\n            webpage = html.HTML(self.web_dir, \'Experiment name = %s\' % self.name, refresh=30)\n            for n in range(epoch, 0, -1):\n                webpage.add_header(\'epoch [%d]\' % n)\n                ims = []\n                txts = []\n                links = []\n\n                for label, image_numpy in visuals.items():\n                    if isinstance(image_numpy, list):\n                        for i in range(len(image_numpy)):\n                            img_path = \'epoch%.3d_%s_%d.jpg\' % (n, label, i)\n                            ims.append(img_path)\n                            txts.append(label+str(i))\n                            links.append(img_path)\n                    else:\n                        img_path = \'epoch%.3d_%s.jpg\' % (n, label)\n                        ims.append(img_path)\n                        txts.append(label)\n                        links.append(img_path)\n                if len(ims) < 10:\n                    webpage.add_images(ims, txts, links, width=self.win_size)\n                else:\n                    num = int(round(len(ims)/2.0))\n                    webpage.add_images(ims[:num], txts[:num], links[:num], width=self.win_size)\n                    webpage.add_images(ims[num:], txts[num:], links[num:], width=self.win_size)\n            webpage.save()\n\n    # errors: dictionary of error labels and values\n    def plot_current_errors(self, errors, step):\n        if self.tf_log:\n            for tag, value in errors.items():\n                summary = self.tf.Summary(value=[self.tf.Summary.Value(tag=tag, simple_value=value)])\n                self.writer.add_summary(summary, step)\n\n    # errors: same format as |errors| of plotCurrentErrors\n    def print_current_errors(self, epoch, i, errors, t):\n        message = \'(epoch: %d, iters: %d, time: %.3f) \' % (epoch, i, t)\n        for k, v in errors.items():\n            if v != 0:\n                message += \'%s: %.3f \' % (k, v)\n\n        print(message)\n        with open(self.log_name, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n\n    # save image to the disk\n    def save_images(self, webpage, visuals, image_path):\n        image_dir = webpage.get_image_dir()\n        short_path = ntpath.basename(image_path[0])\n        name = os.path.splitext(short_path)[0]\n\n        webpage.add_header(name)\n        ims = []\n        txts = []\n        links = []\n\n        for label, image_numpy in visuals.items():\n            image_name = \'%s_%s.jpg\' % (name, label)\n            save_path = os.path.join(image_dir, image_name)\n            util.save_image(image_numpy, save_path)\n\n            ims.append(image_name)\n            txts.append(label)\n            links.append(image_name)\n        webpage.add_images(ims, txts, links, width=self.win_size)\n'"
src/PoseEstimation/network/caffe_to_pytorch/__init__.py,0,b''
src/PoseEstimation/network/caffe_to_pytorch/caffe_demo.py,0,"b'import os\nimport cv2\nimport numpy as np\nimport scipy\nimport PIL.Image\nimport math\nimport caffe\nimport time\nfrom config_reader import config_reader\nimport util\nimport copy\nimport matplotlib\nimport pylab as plt\nfrom scipy.ndimage.filters import gaussian_filter\nimport scipy\nfrom numpy import ma\n\n\nparam, model = config_reader()\n\n\n#if param[\'use_gpu\']: \n#    caffe.set_mode_gpu()\n#    caffe.set_device(param[\'GPUdeviceNumber\']) # set to your device!\n#else:\ncaffe.set_mode_cpu()\nnet = caffe.Net(model[\'deployFile\'], model[\'caffemodel\'], caffe.TEST)\n\n# find connection in the specified sequence, center 29 is in the position 15\nlimbSeq = [[2,3], [2,6], [3,4], [4,5], [6,7], [7,8], [2,9], [9,10], \\\n           [10,11], [2,12], [12,13], [13,14], [2,1], [1,15], [15,17], \\\n           [1,16], [16,18], [3,17], [6,18]]\n           \n# the middle joints heatmap correpondence\nmapIdx = [[31,32], [39,40], [33,34], [35,36], [41,42], [43,44], [19,20], [21,22], \\\n          [23,24], [25,26], [27,28], [29,30], [47,48], [49,50], [53,54], [51,52], \\\n          [55,56], [37,38], [45,46]]\n \n# visualize\ncolors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0], \\\n          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255], \\\n          [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]\n                           \ndef do_work(test_image,save_name):\n    oriImg = cv2.imread(test_image) # B,G,R order\n    multiplier = [x * model[\'boxsize\'] / oriImg.shape[0] for x in param[\'scale_search\']]\n    heatmap_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 19))\n    paf_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 38))\n\n    print \'len of multipler\'\n    print len(multiplier)\n    for m in range(len(multiplier)):\n        scale = multiplier[m]\n        print scale\n        imageToTest = cv2.resize(oriImg, (0,0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n        imageToTest_padded, pad = util.padRightDownCorner(imageToTest, model[\'stride\'], model[\'padValue\'])\n        print imageToTest_padded.shape\n        \n        net.blobs[\'data\'].reshape(*(1, 3, imageToTest_padded.shape[0], imageToTest_padded.shape[1]))\n        #net.forward() # dry run\n        net.blobs[\'data\'].data[...] = np.transpose(np.float32(imageToTest_padded[:,:,:,np.newaxis]), (3,2,0,1))/256 - 0.5;\n        start_time = time.time()\n        output_blobs = net.forward()\n        print(\'At scale %d, The CNN took %.2f ms.\' % (m, 1000 * (time.time() - start_time)))\n        start_time = time.time()\n        # extract outputs, resize, and remove padding\n        heatmap = np.transpose(np.squeeze(net.blobs[output_blobs.keys()[1]].data), (1,2,0)) # output 1 is heatmaps\n        heatmap = cv2.resize(heatmap, (0,0), fx=model[\'stride\'], fy=model[\'stride\'], interpolation=cv2.INTER_CUBIC)\n        heatmap = heatmap[:imageToTest_padded.shape[0]-pad[2], :imageToTest_padded.shape[1]-pad[3], :]\n        heatmap = cv2.resize(heatmap, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)\n        \n        paf = np.transpose(np.squeeze(net.blobs[output_blobs.keys()[0]].data), (1,2,0)) # output 0 is PAFs\n        paf = cv2.resize(paf, (0,0), fx=model[\'stride\'], fy=model[\'stride\'], interpolation=cv2.INTER_CUBIC)\n        paf = paf[:imageToTest_padded.shape[0]-pad[2], :imageToTest_padded.shape[1]-pad[3], :]\n        paf = cv2.resize(paf, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)    \n        \n        heatmap_avg = heatmap_avg + heatmap / len(multiplier)\n        paf_avg = paf_avg + paf / len(multiplier)\n        print(\'2 At scale %d, The CNN took %.2f ms.\' % (m, 1000 * (time.time() - start_time)))\n\n    start_time = time.time()\n    all_peaks = []\n    peak_counter = 0\n\n    for part in range(19-1):\n        x_list = []\n        y_list = []\n        map_ori = heatmap_avg[:,:,part]\n        map = gaussian_filter(map_ori, sigma=3)\n        \n        map_left = np.zeros(map.shape)\n        map_left[1:,:] = map[:-1,:]\n        map_right = np.zeros(map.shape)\n        map_right[:-1,:] = map[1:,:]\n        map_up = np.zeros(map.shape)\n        map_up[:,1:] = map[:,:-1]\n        map_down = np.zeros(map.shape)\n        map_down[:,:-1] = map[:,1:]\n        \n        peaks_binary = np.logical_and.reduce((map>=map_left, map>=map_right, map>=map_up, map>=map_down, map > param[\'thre1\']))\n        peaks = zip(np.nonzero(peaks_binary)[1], np.nonzero(peaks_binary)[0]) # note reverse\n        peaks_with_score = [x + (map_ori[x[1],x[0]],) for x in peaks]\n        id = range(peak_counter, peak_counter + len(peaks))\n        peaks_with_score_and_id = [peaks_with_score[i] + (id[i],) for i in range(len(id))]\n\n        all_peaks.append(peaks_with_score_and_id)\n        peak_counter += len(peaks)\n        \n\n              \n    connection_all = []\n    special_k = []\n    mid_num = 10\n\n    for k in range(len(mapIdx)):\n        score_mid = paf_avg[:,:,[x-19 for x in mapIdx[k]]]\n        candA = all_peaks[limbSeq[k][0]-1]\n        candB = all_peaks[limbSeq[k][1]-1]\n        nA = len(candA)\n        nB = len(candB)\n        indexA, indexB = limbSeq[k]\n        if(nA != 0 and nB != 0):\n            connection_candidate = []\n            for i in range(nA):\n                for j in range(nB):\n                    vec = np.subtract(candB[j][:2], candA[i][:2])\n                    norm = math.sqrt(vec[0]*vec[0] + vec[1]*vec[1])\n                    vec = np.divide(vec, norm)\n                    \n                    startend = zip(np.linspace(candA[i][0], candB[j][0], num=mid_num), \\\n                                   np.linspace(candA[i][1], candB[j][1], num=mid_num))\n                    \n                    vec_x = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 0] \\\n                                      for I in range(len(startend))])\n                    vec_y = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 1] \\\n                                      for I in range(len(startend))])\n\n                    score_midpts = np.multiply(vec_x, vec[0]) + np.multiply(vec_y, vec[1])\n                    score_with_dist_prior = sum(score_midpts)/len(score_midpts) + min(0.5*oriImg.shape[0]/norm-1, 0)\n                    criterion1 = len(np.nonzero(score_midpts > param[\'thre2\'])[0]) > 0.8 * len(score_midpts)\n                    criterion2 = score_with_dist_prior > 0\n                    if criterion1 and criterion2:\n                        connection_candidate.append([i, j, score_with_dist_prior, score_with_dist_prior+candA[i][2]+candB[j][2]])\n\n            connection_candidate = sorted(connection_candidate, key=lambda x: x[2], reverse=True)\n            connection = np.zeros((0,5))\n            for c in range(len(connection_candidate)):\n                i,j,s = connection_candidate[c][0:3]\n                if(i not in connection[:,3] and j not in connection[:,4]):\n                    connection = np.vstack([connection, [candA[i][3], candB[j][3], s, i, j]])\n                    if(len(connection) >= min(nA, nB)):\n                        break\n\n            connection_all.append(connection)\n        else:\n            special_k.append(k)\n            connection_all.append([])\n            \n    # last number in each row is the total parts number of that person\n    # the second last number in each row is the score of the overall configuration\n    subset = -1 * np.ones((0, 20))\n    candidate = np.array([item for sublist in all_peaks for item in sublist])\n\n    for k in range(len(mapIdx)):\n        if k not in special_k:\n            partAs = connection_all[k][:,0]\n            partBs = connection_all[k][:,1]\n            indexA, indexB = np.array(limbSeq[k]) - 1\n\n            for i in range(len(connection_all[k])): #= 1:size(temp,1)\n                found = 0\n                subset_idx = [-1, -1]\n                for j in range(len(subset)): #1:size(subset,1):\n                    if subset[j][indexA] == partAs[i] or subset[j][indexB] == partBs[i]:\n                        subset_idx[found] = j\n                        found += 1\n                \n                if found == 1:\n                    j = subset_idx[0]\n                    if(subset[j][indexB] != partBs[i]):\n                        subset[j][indexB] = partBs[i]\n                        subset[j][-1] += 1\n                        subset[j][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n                elif found == 2: # if found 2 and disjoint, merge them\n                    j1, j2 = subset_idx\n                    print ""found = 2""\n                    membership = ((subset[j1]>=0).astype(int) + (subset[j2]>=0).astype(int))[:-2]\n                    if len(np.nonzero(membership == 2)[0]) == 0: #merge\n                        subset[j1][:-2] += (subset[j2][:-2] + 1)\n                        subset[j1][-2:] += subset[j2][-2:]\n                        subset[j1][-2] += connection_all[k][i][2]\n                        subset = np.delete(subset, j2, 0)\n                    else: # as like found == 1\n                        subset[j1][indexB] = partBs[i]\n                        subset[j1][-1] += 1\n                        subset[j1][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n\n                # if find no partA in the subset, create a new subset\n                elif not found and k < 17:\n                    row = -1 * np.ones(20)\n                    row[indexA] = partAs[i]\n                    row[indexB] = partBs[i]\n                    row[-1] = 2\n                    row[-2] = sum(candidate[connection_all[k][i,:2].astype(int), 2]) + connection_all[k][i][2]\n                    subset = np.vstack([subset, row])\n\n    # delete some rows of subset which has few parts occur\n    deleteIdx = [];\n    for i in range(len(subset)):\n        if subset[i][-1] < 4 or subset[i][-2]/subset[i][-1] < 0.4:\n            deleteIdx.append(i)\n    subset = np.delete(subset, deleteIdx, axis=0)\n\n\n    cmap = matplotlib.cm.get_cmap(\'hsv\')\n\n    canvas = cv2.imread(test_image) # B,G,R order\n\n    for i in range(18):\n        rgba = np.array(cmap(1 - i/18. - 1./36))\n        rgba[0:3] *= 255\n        for j in range(len(all_peaks[i])):\n            cv2.circle(canvas, all_peaks[i][j][0:2], 4, colors[i], thickness=-1)\n    # visualize 2\n    stickwidth = 4\n\n    for i in range(17):\n        for n in range(len(subset)):\n            index = subset[n][np.array(limbSeq[i])-1]\n            if -1 in index:\n                continue\n            cur_canvas = canvas.copy()\n            Y = candidate[index.astype(int), 0]\n            X = candidate[index.astype(int), 1]\n            mX = np.mean(X)\n            mY = np.mean(Y)\n            length = ((X[0] - X[1]) ** 2 + (Y[0] - Y[1]) ** 2) ** 0.5\n            angle = math.degrees(math.atan2(X[0] - X[1], Y[0] - Y[1]))\n            polygon = cv2.ellipse2Poly((int(mY),int(mX)), (int(length/2), stickwidth), int(angle), 0, 360, 1)\n            cv2.fillConvexPoly(cur_canvas, polygon, colors[i])\n            canvas = cv2.addWeighted(canvas, 0.4, cur_canvas, 0.6, 0)\n    cv2.imwrite(save_name,canvas)        \n    print(\'3 At scale %d, The CNN took %.2f ms.\' % (m, 1000 * (time.time() - start_time)))\n\n\ndo_work(\'../sample_image/ski.jpg\',\'caffe_result.jpg\')\n\n#path = \'../sample_image/pictures/\'\n#save_path = \'../sample_image/result2/\'\n#image_names = sorted(os.listdir(path))\n#for k in range(len(image_names)):\n#    one_path =path+image_names[k]\n#    save_path = save_path +image_names[k]\n#    do_work(one_path,save_path)\n'"
src/PoseEstimation/network/caffe_to_pytorch/config_reader.py,0,"b'import numpy as np\n\nfrom configobj import ConfigObj\n\n\ndef config_reader():\n    config = ConfigObj(\'config\')\n\n    param = config[\'param\']\n    model_id = param[\'modelID\']\n    model = config[\'models\'][model_id]\n    model[\'boxsize\'] = int(model[\'boxsize\'])\n    model[\'stride\'] = int(model[\'stride\'])\n    model[\'padValue\'] = int(model[\'padValue\'])\n    #param[\'starting_range\'] = float(param[\'starting_range\'])\n    #param[\'ending_range\'] = float(param[\'ending_range\'])\n    param[\'octave\'] = int(param[\'octave\'])\n    param[\'use_gpu\'] = int(param[\'use_gpu\'])\n    param[\'starting_range\'] = float(param[\'starting_range\'])\n    param[\'ending_range\'] = float(param[\'ending_range\'])\n    param[\'scale_search\'] = map(float, param[\'scale_search\'])\n    param[\'thre1\'] = float(param[\'thre1\'])\n    param[\'thre2\'] = float(param[\'thre2\'])\n    param[\'thre3\'] = float(param[\'thre3\'])\n    param[\'mid_num\'] = int(param[\'mid_num\'])\n    param[\'min_num\'] = int(param[\'min_num\'])\n    param[\'crop_ratio\'] = float(param[\'crop_ratio\'])\n    param[\'bbox_ratio\'] = float(param[\'bbox_ratio\'])\n    param[\'GPUdeviceNumber\'] = int(param[\'GPUdeviceNumber\'])\n\n    return param, model\n\nif __name__ == ""__main__"":\n    config_reader()\n'"
src/PoseEstimation/network/caffe_to_pytorch/convert.py,12,"b""import math\nimport os\nimport re\nimport sys\nimport time\nfrom collections import OrderedDict\n\nimport numpy as np\nimport skimage.io\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom torch.autograd import Variable\n\nimport caffe\nimport cv2\nimport util\nfrom config_reader import config_reader\n\ncaffemodel = '../model/_trained_COCO/pose_iter_440000.caffemodel'\ndeployFile = '../model/_trained_COCO/pose_deploy.prototxt'\ncaffe.set_mode_cpu()\nnet = caffe.Net(deployFile, caffemodel, caffe.TEST)\n\n#layers_caffe = dict(zip(list(net._layer_names), net.layers))\n#print 'Number of layers: %i' % len(layers_caffe.keys())\n\nblocks = {}\n\nblock0  = [{'conv1_1':[3,64,3,1,1]},{'conv1_2':[64,64,3,1,1]},{'pool1_stage1':[2,2,0]},{'conv2_1':[64,128,3,1,1]},{'conv2_2':[128,128,3,1,1]},{'pool2_stage1':[2,2,0]},{'conv3_1':[128,256,3,1,1]},{'conv3_2':[256,256,3,1,1]},{'conv3_3':[256,256,3,1,1]},{'conv3_4':[256,256,3,1,1]},{'pool3_stage1':[2,2,0]},{'conv4_1':[256,512,3,1,1]},{'conv4_2':[512,512,3,1,1]},{'conv4_3_CPM':[512,256,3,1,1]},{'conv4_4_CPM':[256,128,3,1,1]}]\n\nblocks['block1_1']  = [{'conv5_1_CPM_L1':[128,128,3,1,1]},{'conv5_2_CPM_L1':[128,128,3,1,1]},{'conv5_3_CPM_L1':[128,128,3,1,1]},{'conv5_4_CPM_L1':[128,512,1,1,0]},{'conv5_5_CPM_L1':[512,38,1,1,0]}]\n\nblocks['block1_2']  = [{'conv5_1_CPM_L2':[128,128,3,1,1]},{'conv5_2_CPM_L2':[128,128,3,1,1]},{'conv5_3_CPM_L2':[128,128,3,1,1]},{'conv5_4_CPM_L2':[128,512,1,1,0]},{'conv5_5_CPM_L2':[512,19,1,1,0]}]\n\nfor i in range(2,7):\n    blocks['block%d_1'%i]  = [{'Mconv1_stage%d_L1'%i:[185,128,7,1,3]},{'Mconv2_stage%d_L1'%i:[128,128,7,1,3]},{'Mconv3_stage%d_L1'%i:[128,128,7,1,3]},{'Mconv4_stage%d_L1'%i:[128,128,7,1,3]},\n{'Mconv5_stage%d_L1'%i:[128,128,7,1,3]},{'Mconv6_stage%d_L1'%i:[128,128,1,1,0]},{'Mconv7_stage%d_L1'%i:[128,38,1,1,0]}]\n    blocks['block%d_2'%i]  = [{'Mconv1_stage%d_L2'%i:[185,128,7,1,3]},{'Mconv2_stage%d_L2'%i:[128,128,7,1,3]},{'Mconv3_stage%d_L2'%i:[128,128,7,1,3]},{'Mconv4_stage%d_L2'%i:[128,128,7,1,3]},\n{'Mconv5_stage%d_L2'%i:[128,128,7,1,3]},{'Mconv6_stage%d_L2'%i:[128,128,1,1,0]},{'Mconv7_stage%d_L2'%i:[128,19,1,1,0]}]\n\n             \ndef make_layers(cfg_dict):\n    layers = []\n    for i in range(len(cfg_dict)-1):\n        one_ = cfg_dict[i]\n        for k,v in one_.iteritems():      \n            if 'pool' in k:\n                layers += [nn.MaxPool2d(kernel_size=v[0], stride=v[1], padding=v[2] )]\n            else:\n                conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1], kernel_size=v[2], stride = v[3], padding=v[4])\n                layers += [conv2d, nn.ReLU(inplace=True)]\n    one_ = cfg_dict[-1].keys()\n    k = one_[0]\n    v = cfg_dict[-1][k]\n    conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1], kernel_size=v[2], stride = v[3], padding=v[4])\n    layers += [conv2d]\n    return nn.Sequential(*layers)\n    \nlayers = []\nfor i in range(len(block0)):\n    one_ = block0[i]\n    for k,v in one_.iteritems():      \n        if 'pool' in k:\n            layers += [nn.MaxPool2d(kernel_size=v[0], stride=v[1], padding=v[2] )]\n        else:\n            conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1], kernel_size=v[2], stride = v[3], padding=v[4])\n            layers += [conv2d, nn.ReLU(inplace=True)]  \n       \nmodels = {}           \nmodels['block0']=nn.Sequential(*layers)        \n\nfor k,v in blocks.iteritems():\n    models[k] = make_layers(v)\n                \n    \nclass pose_model(nn.Module):\n    def __init__(self,model_dict,transform_input=False):\n        super(pose_model, self).__init__()\n        self.model0   = model_dict['block0']\n        self.model1_1 = model_dict['block1_1']        \n        self.model2_1 = model_dict['block2_1']  \n        self.model3_1 = model_dict['block3_1']  \n        self.model4_1 = model_dict['block4_1']  \n        self.model5_1 = model_dict['block5_1']  \n        self.model6_1 = model_dict['block6_1']  \n        \n        self.model1_2 = model_dict['block1_2']        \n        self.model2_2 = model_dict['block2_2']  \n        self.model3_2 = model_dict['block3_2']  \n        self.model4_2 = model_dict['block4_2']  \n        self.model5_2 = model_dict['block5_2']  \n        self.model6_2 = model_dict['block6_2']\n        \n        \n    def forward(self, x):    \n        out1 = self.model0(x)\n        \n        out1_1 = self.model1_1(out1)\n        out1_2 = self.model1_2(out1)\n        out2  = torch.cat([out1_1,out1_2,out1],1)\n        \n        out2_1 = self.model2_1(out2)\n        out2_2 = self.model2_2(out2)\n        out3   = torch.cat([out2_1,out2_2,out1],1)\n        \n        out3_1 = self.model3_1(out3)\n        out3_2 = self.model3_2(out3)\n        out4   = torch.cat([out3_1,out3_2,out1],1)\n\n        out4_1 = self.model4_1(out4)\n        out4_2 = self.model4_2(out4)\n        out5   = torch.cat([out4_1,out4_2,out1],1)  \n        \n        out5_1 = self.model5_1(out5)\n        out5_2 = self.model5_2(out5)\n        out6   = torch.cat([out5_1,out5_2,out1],1)         \n              \n        out6_1 = self.model6_1(out6)\n        out6_2 = self.model6_2(out6)\n        \n        return out6_1,out6_2        \n\nmodel = pose_model(models)     \nmodel.eval()\n\npose_model_keys = model.state_dict().keys() \n\nmatch_dict = {}  \nmatch_dict['model0.0']  = 'conv1_1'\nmatch_dict['model0.2']  = 'conv1_2'\nmatch_dict['model0.5']  = 'conv2_1'\nmatch_dict['model0.7'] = 'conv2_2'\nmatch_dict['model0.10'] = 'conv3_1'\nmatch_dict['model0.12'] = 'conv3_2'\nmatch_dict['model0.14'] = 'conv3_3'\nmatch_dict['model0.16'] = 'conv3_4'\nmatch_dict['model0.19'] = 'conv4_1'\nmatch_dict['model0.21'] = 'conv4_2'\nmatch_dict['model0.23'] = 'conv4_3_CPM'\nmatch_dict['model0.25'] = 'conv4_4_CPM'\n\n\nfor i in range(1,7):\n    for j in range(1,3):\n        caffe_block = blocks['block%d_%d'%(i,j)]\n        caffe_keys =[]\n        for m in range(len(caffe_block)):\n            caffe_keys+=caffe_block[m].keys()\n        pytorch_block = []\n        for item in pose_model_keys:\n            if 'model%d_%d'%(i,j) in item:\n                pytorch_block.append(item)\n        for k in range(len(pytorch_block)):\n            pytorch_block[k]=pytorch_block[k].rsplit('.',1)[0]\n        pytorch_block = list(set(pytorch_block))\n        pytorch_block = sorted(pytorch_block,key=lambda x: int(x.rsplit('.',1)[-1]))\n        match_dict.update(dict(zip(pytorch_block,caffe_keys)))\n        \n#from copy import deepcopy\n#new_state_dict = deepcopy(model.state_dict())\nnew_state_dict = OrderedDict()\n\nfor var_name in model.state_dict().keys():\n    print var_name\n    if 'weight' in var_name:\n        name_in_caffe = match_dict[var_name.rsplit('.',1)[0]]\n        data = net.params[name_in_caffe][0].data\n    elif 'bias' in var_name:\n        name_in_caffe = match_dict[var_name.rsplit('.',1)[0]]\n        data = net.params[name_in_caffe][1].data  \n    else:\n        print 'bad'\n    new_state_dict[var_name] = torch.from_numpy(data).float()\n    \nmodel.load_state_dict(new_state_dict)   \n#model.cuda()\nmodel.eval()\n\n\n\nparam_, model_ = config_reader()\n\ntest_image = '../sample_image/ski.jpg'\n#test_image = 'a.jpg'\noriImg = cv2.imread(test_image) # B,G,R order\n\nmultiplier = [x * model_['boxsize'] / oriImg.shape[0] for x in param_['scale_search']]\n\n#heatmap_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 19))\n#paf_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 38))\n\nfor m in range(len(multiplier)):\n    scale = multiplier[m]\n    imageToTest = cv2.resize(oriImg, (0,0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n    imageToTest_padded, pad = util.padRightDownCorner(imageToTest, model_['stride'], model_['padValue'])\n\n    feed = np.transpose(np.float32(imageToTest_padded[:,:,:,np.newaxis]), (3,2,0,1))/256 - 0.5\n   \n    output1,output2 = model(Variable(torch.from_numpy(feed).float(), volatile=True))\n\n    output1 = output1.cpu().data.numpy()\n    output2 = output2.cpu().data.numpy()\n    \n    net.blobs['data'].reshape(*(1, 3, imageToTest_padded.shape[0], imageToTest_padded.shape[1]))\n    net.blobs['data'].data[...] = feed\n    \n    output_blobs = net.forward()\n    \n    output1_ = net.blobs[output_blobs.keys()[0]].data\n    output2_ = net.blobs[output_blobs.keys()[1]].data\n    \n    print 'output1 have %10.10f%% relative error'%(np.linalg.norm(output1-output1_)/np.linalg.norm(output1_)*100)\n    print 'output2 have %10.10f%% relative error'%(np.linalg.norm(output2-output2_)/np.linalg.norm(output2_)*100)\n    \n\n\ntorch.save(model.state_dict(), '../model/pose_model.pth')    \n"""
src/PoseEstimation/network/caffe_to_pytorch/draw_caffe_net.py,0,"b'#!/usr/bin/env python\n""""""\nDraw a graph of the net architecture.\n""""""\nfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\nfrom google.protobuf import text_format\n\nimport caffe\nimport caffe.draw\nfrom caffe.proto import caffe_pb2\n\n\ndef parse_args():\n    """"""Parse input arguments\n    """"""\n\n    parser = ArgumentParser(description=__doc__,\n                            formatter_class=ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\'--input_net_proto_file\',\n                        help=\'Input network prototxt file\')\n    parser.add_argument(\'--output_image_file\',\n                        help=\'Output image file\')\n    parser.add_argument(\'--rankdir\',\n                        help=(\'One of TB (top-bottom, i.e., vertical), \'\n                              \'RL (right-left, i.e., horizontal), or another \'\n                              \'valid dot option; see \'\n                              \'http://www.graphviz.org/doc/info/\'\n                              \'attrs.html#k:rankdir\'),\n                        default=\'LR\')\n    parser.add_argument(\'--phase\',\n                        help=(\'Which network phase to draw: can be TRAIN, \'\n                              \'TEST, or ALL.  If ALL, then all layers are drawn \'\n                              \'regardless of phase.\'),\n                        default=""ALL"")\n\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    net = caffe_pb2.NetParameter()\n    text_format.Merge(open(args.input_net_proto_file).read(), net)\n    print(\'Drawing net to %s\' % args.output_image_file)\n    phase=None;\n    if args.phase == ""TRAIN"":\n        phase = caffe.TRAIN\n    elif args.phase == ""TEST"":\n        phase = caffe.TEST\n    elif args.phase != ""ALL"":\n        raise ValueError(""Unknown phase: "" + args.phase)\n    caffe.draw.draw_net_to_file(net, args.output_image_file, args.rankdir,\n                                phase)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
src/PoseEstimation/network/caffe_to_pytorch/util.py,0,"b'from cStringIO import StringIO\n\nimport numpy as np\nimport PIL.Image\nfrom IPython.display import Image, display\n\n\ndef showBGRimage(a, fmt=\'jpeg\'):\n    a = np.uint8(np.clip(a, 0, 255))\n    a[:,:,[0,2]] = a[:,:,[2,0]] # for B,G,R order\n    f = StringIO()\n    PIL.Image.fromarray(a).save(f, fmt)\n    display(Image(data=f.getvalue()))\n\ndef showmap(a, fmt=\'png\'):\n    a = np.uint8(np.clip(a, 0, 255))\n    f = StringIO()\n    PIL.Image.fromarray(a).save(f, fmt)\n    display(Image(data=f.getvalue()))\n\n#def checkparam(param):\n#    octave = param[\'octave\']\n#    starting_range = param[\'starting_range\']\n#    ending_range = param[\'ending_range\']\n#    assert starting_range <= ending_range, \'starting ratio should <= ending ratio\'\n#    assert octave >= 1, \'octave should >= 1\'\n#    return starting_range, ending_range, octave\n\ndef getJetColor(v, vmin, vmax):\n    c = np.zeros((3))\n    if (v < vmin):\n        v = vmin\n    if (v > vmax):\n        v = vmax\n    dv = vmax - vmin\n    if (v < (vmin + 0.125 * dv)): \n        c[0] = 256 * (0.5 + (v * 4)) #B: 0.5 ~ 1\n    elif (v < (vmin + 0.375 * dv)):\n        c[0] = 255\n        c[1] = 256 * (v - 0.125) * 4 #G: 0 ~ 1\n    elif (v < (vmin + 0.625 * dv)):\n        c[0] = 256 * (-4 * v + 2.5)  #B: 1 ~ 0\n        c[1] = 255\n        c[2] = 256 * (4 * (v - 0.375)) #R: 0 ~ 1\n    elif (v < (vmin + 0.875 * dv)):\n        c[1] = 256 * (-4 * v + 3.5)  #G: 1 ~ 0\n        c[2] = 255\n    else:\n        c[2] = 256 * (-4 * v + 4.5) #R: 1 ~ 0.5                      \n    return c\n\ndef colorize(gray_img):\n    out = np.zeros(gray_img.shape + (3,))\n    for y in range(out.shape[0]):\n        for x in range(out.shape[1]):\n            out[y,x,:] = getJetColor(gray_img[y,x], 0, 1)\n    return out\n\ndef padRightDownCorner(img, stride, padValue):\n    h = img.shape[0]\n    w = img.shape[1]\n\n    pad = 4 * [None]\n    pad[0] = 0 # up\n    pad[1] = 0 # left\n    pad[2] = 0 if (h%stride==0) else stride - (h % stride) # down\n    pad[3] = 0 if (w%stride==0) else stride - (w % stride) # right\n\n    img_padded = img\n    pad_up = np.tile(img_padded[0:1,:,:]*0 + padValue, (pad[0], 1, 1))\n    img_padded = np.concatenate((pad_up, img_padded), axis=0)\n    pad_left = np.tile(img_padded[:,0:1,:]*0 + padValue, (1, pad[1], 1))\n    img_padded = np.concatenate((pad_left, img_padded), axis=1)\n    pad_down = np.tile(img_padded[-2:-1,:,:]*0 + padValue, (pad[2], 1, 1))\n    img_padded = np.concatenate((img_padded, pad_down), axis=0)\n    pad_right = np.tile(img_padded[:,-2:-1,:]*0 + padValue, (1, pad[3], 1))\n    img_padded = np.concatenate((img_padded, pad_right), axis=1)\n\n    return img_padded, pad\n\n#if __name__ == ""__main__"":\n#    config_reader()\n'"
src/PoseEstimation/training/datasets/__init__.py,0,b''
src/PoseEstimation/training/datasets/coco.py,0,"b'""""""MSCOCO Dataloader\n   Thanks to @tensorboy @shuangliu\n""""""\n\ntry:\n    import ujson as json\nexcept ImportError:\n    import json\n\nfrom torchvision.transforms import ToTensor\nfrom training.datasets.coco_data.COCO_data_pipeline import Cocokeypoints\nfrom training.datasets.dataloader import sDataLoader\n\n\ndef get_loader(json_path, data_dir, mask_dir, inp_size, feat_stride, preprocess,\n               batch_size, params_transform, training=True, shuffle=True, num_workers=3):\n    """""" Build a COCO dataloader\n    :param json_path: string, path to jso file\n    :param datadir: string, path to coco data\n    :returns : the data_loader\n    """"""\n    with open(json_path) as data_file:\n        data_this = json.load(data_file)\n        data = data_this[\'root\']\n\n    num_samples = len(data)\n    train_indexes = []\n    val_indexes = []\n    for count in range(num_samples):\n        if data[count][\'isValidation\'] != 0.:\n            val_indexes.append(count)\n        else:\n            train_indexes.append(count)\n\n    coco_data = Cocokeypoints(root=data_dir, mask_dir=mask_dir,\n                              index_list=train_indexes if training else val_indexes,\n                              data=data, inp_size=inp_size, feat_stride=feat_stride,\n                              preprocess=preprocess, transform=ToTensor(), params_transform=params_transform)\n\n    data_loader = sDataLoader(coco_data, batch_size=batch_size,\n                              shuffle=shuffle, num_workers=num_workers)\n\n    return data_loader\n'"
src/PoseEstimation/training/datasets/dataloader.py,1,"b'from typing import Generator\nfrom torch.utils.data.dataloader import DataLoader\nimport logging \n\nlogger = logging.getLogger(__name__)\n\nclass sDataLoader(DataLoader):\n    def get_stream(self):\n        """"""\n        Return a generate that can yield endless data.\n        :Example:\n        stream = get_stream()\n        for i in range(100):\n            batch = next(stream)\n\n        :return: stream\n        :rtype: Generator\n        """"""\n        while True:\n            for data in iter(self):\n                yield data\n\n    @staticmethod\n    def copy(loader):\n        """"""\n        Init a sDataloader from an existing Dataloader\n        :param loader: an instance of Dataloader\n        :type loader: DataLoader\n        :return: a new instance of sDataloader\n        :rtype: sDataLoader\n        """"""\n        if not isinstance(loader, DataLoader):\n            logger.warning(\'loader should be an instance of Dataloader, but got {}\'.format(type(loader)))\n            return loader\n\n        new_loader = sDataLoader(loader.dataset)\n        for k, v in loader.__dict__.items():\n            setattr(new_loader, k, v)\n        return new_loader\n'"
src/PoseEstimation/training/datasets/coco_data/COCO_data_pipeline.py,6,"b'# coding=utf-8\nimport os\n\nimport cv2\nimport numpy as np\n\nimport torch\nfrom training.datasets.coco_data.heatmap import putGaussianMaps\nfrom training.datasets.coco_data.ImageAugmentation import (aug_croppad, aug_flip,\n                                                  aug_rotate, aug_scale)\nfrom training.datasets.coco_data.paf import putVecMaps\nfrom training.datasets.coco_data.preprocessing import (inception_preprocess,\n                                              rtpose_preprocess,\n                                              ssd_preprocess, vgg_preprocess)\nfrom torch.utils.data import DataLoader, Dataset\n\n\'\'\'\ntrain2014  : 82783 simages\nval2014    : 40504 images\n\nfirst 2644 of val2014 marked by \'isValidation = 1\', as our minval dataset.\nSo all training data have 82783+40504-2644 = 120643 samples\n\'\'\'\n\nclass Cocokeypoints(Dataset):\n    def __init__(self, root, mask_dir, index_list, data, inp_size, feat_stride, preprocess=\'rtpose\', transform=None,\n                 target_transform=None, params_transform=None):\n\n        self.params_transform = params_transform\n        self.params_transform[\'crop_size_x\'] = inp_size\n        self.params_transform[\'crop_size_y\'] = inp_size\n        self.params_transform[\'stride\'] = feat_stride\n\n        # add preprocessing as a choice, so we don\'t modify it manually.\n        self.preprocess = preprocess\n        self.data = data\n        self.mask_dir = mask_dir\n        self.numSample = len(index_list)\n        self.index_list = index_list\n        self.root = root\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def get_anno(self, meta_data):\n        """"""\n        get meta information\n        """"""\n        anno = dict()\n        anno[\'dataset\'] = meta_data[\'dataset\']\n        anno[\'img_height\'] = int(meta_data[\'img_height\'])\n        anno[\'img_width\'] = int(meta_data[\'img_width\'])\n\n        anno[\'isValidation\'] = meta_data[\'isValidation\']\n        anno[\'people_index\'] = int(meta_data[\'people_index\'])\n        anno[\'annolist_index\'] = int(meta_data[\'annolist_index\'])\n\n        # (b) objpos_x (float), objpos_y (float)\n        anno[\'objpos\'] = np.array(meta_data[\'objpos\'])\n        anno[\'scale_provided\'] = meta_data[\'scale_provided\']\n        anno[\'joint_self\'] = np.array(meta_data[\'joint_self\'])\n\n        anno[\'numOtherPeople\'] = int(meta_data[\'numOtherPeople\'])\n        anno[\'num_keypoints_other\'] = np.array(\n            meta_data[\'num_keypoints_other\'])\n        anno[\'joint_others\'] = np.array(meta_data[\'joint_others\'])\n        anno[\'objpos_other\'] = np.array(meta_data[\'objpos_other\'])\n        anno[\'scale_provided_other\'] = meta_data[\'scale_provided_other\']\n        anno[\'bbox_other\'] = meta_data[\'bbox_other\']\n        anno[\'segment_area_other\'] = meta_data[\'segment_area_other\']\n\n        if anno[\'numOtherPeople\'] == 1:\n            anno[\'joint_others\'] = np.expand_dims(anno[\'joint_others\'], 0)\n            anno[\'objpos_other\'] = np.expand_dims(anno[\'objpos_other\'], 0)\n        return anno\n\n    def add_neck(self, meta):\n        \'\'\'\n        MS COCO annotation order:\n        0: nose\t   \t\t1: l eye\t\t2: r eye\t3: l ear\t4: r ear\n        5: l shoulder\t6: r shoulder\t7: l elbow\t8: r elbow\n        9: l wrist\t\t10: r wrist\t\t11: l hip\t12: r hip\t13: l knee\n        14: r knee\t\t15: l ankle\t\t16: r ankle\n\n        The order in this work:\n        (0-\'nose\'\t1-\'neck\' 2-\'right_shoulder\' 3-\'right_elbow\' 4-\'right_wrist\'\n        5-\'left_shoulder\' 6-\'left_elbow\'\t    7-\'left_wrist\'  8-\'right_hip\'\n        9-\'right_knee\'\t 10-\'right_ankle\'\t11-\'left_hip\'   12-\'left_knee\'\n        13-\'left_ankle\'\t 14-\'right_eye\'\t    15-\'left_eye\'   16-\'right_ear\'\n        17-\'left_ear\' )\n        \'\'\'\n        our_order = [0, 17, 6, 8, 10, 5, 7, 9,\n                     12, 14, 16, 11, 13, 15, 2, 1, 4, 3]\n        # Index 6 is right shoulder and Index 5 is left shoulder\n        right_shoulder = meta[\'joint_self\'][6, :]\n        left_shoulder = meta[\'joint_self\'][5, :]\n        neck = (right_shoulder + left_shoulder) / 2\n        if right_shoulder[2] == 2 or left_shoulder[2] == 2:\n            neck[2] = 2\n        elif right_shoulder[2] == 1 or left_shoulder[2] == 1:\n            neck[2] = 1\n        else:\n            neck[2] = right_shoulder[2] * left_shoulder[2]\n\n        neck = neck.reshape(1, len(neck))\n        neck = np.round(neck)\n        meta[\'joint_self\'] = np.vstack((meta[\'joint_self\'], neck))\n        meta[\'joint_self\'] = meta[\'joint_self\'][our_order, :]\n        temp = []\n\n        for i in range(meta[\'numOtherPeople\']):\n            right_shoulder = meta[\'joint_others\'][i, 6, :]\n            left_shoulder = meta[\'joint_others\'][i, 5, :]\n            neck = (right_shoulder + left_shoulder) / 2\n            if (right_shoulder[2] == 2 or left_shoulder[2] == 2):\n                neck[2] = 2\n            elif (right_shoulder[2] == 1 or left_shoulder[2] == 1):\n                neck[2] = 1\n            else:\n                neck[2] = right_shoulder[2] * left_shoulder[2]\n            neck = neck.reshape(1, len(neck))\n            neck = np.round(neck)\n            single_p = np.vstack((meta[\'joint_others\'][i], neck))\n            single_p = single_p[our_order, :]\n            temp.append(single_p)\n        meta[\'joint_others\'] = np.array(temp)\n\n        return meta\n\n    def remove_illegal_joint(self, meta):\n        crop_x = int(self.params_transform[\'crop_size_x\'])\n        crop_y = int(self.params_transform[\'crop_size_y\'])\n        mask = np.logical_or.reduce((meta[\'joint_self\'][:, 0] >= crop_x,\n                                     meta[\'joint_self\'][:, 0] < 0,\n                                     meta[\'joint_self\'][:, 1] >= crop_y,\n                                     meta[\'joint_self\'][:, 1] < 0))\n        # out_bound = np.nonzero(mask)\n        # print(mask.shape)\n        meta[\'joint_self\'][mask == True, :] = (1, 1, 2)\n        if (meta[\'numOtherPeople\'] != 0):\n            mask = np.logical_or.reduce((meta[\'joint_others\'][:, :, 0] >= crop_x,\n                                         meta[\'joint_others\'][:, :, 0] < 0,\n                                         meta[\'joint_others\'][:,\n                                                              :, 1] >= crop_y,\n                                         meta[\'joint_others\'][:, :, 1] < 0))\n            meta[\'joint_others\'][mask == True, :] = (1, 1, 2)\n\n        return meta\n\n    def get_ground_truth(self, meta, mask_miss):\n\n        stride = self.params_transform[\'stride\']\n        mode = self.params_transform[\'mode\']\n        crop_size_y = self.params_transform[\'crop_size_y\']\n        crop_size_x = self.params_transform[\'crop_size_x\']\n        num_parts = self.params_transform[\'np\']\n        nop = meta[\'numOtherPeople\']\n        grid_y = crop_size_y / stride\n        grid_x = crop_size_x / stride\n        channels = (num_parts + 1) * 2\n        heatmaps = np.zeros((int(grid_y), int(grid_x), 19))\n        pafs = np.zeros((int(grid_y), int(grid_x), 38))\n\n        mask_miss = cv2.resize(mask_miss, (0, 0), fx=1.0 / stride, fy=1.0 /\n                               stride, interpolation=cv2.INTER_CUBIC).astype(\n            np.float32)\n        mask_miss = mask_miss / 255.\n        mask_miss = np.expand_dims(mask_miss, axis=2)\n\n        heat_mask = np.repeat(mask_miss, 19, axis=2)\n        paf_mask = np.repeat(mask_miss, 38, axis=2)\n\n        # confidance maps for body parts\n        for i in range(18):\n            if (meta[\'joint_self\'][i, 2] <= 1):\n                center = meta[\'joint_self\'][i, :2]\n                gaussian_map = heatmaps[:, :, i]\n                heatmaps[:, :, i] = putGaussianMaps(\n                    center, gaussian_map, params_transform=self.params_transform)\n            for j in range(nop):\n                if (meta[\'joint_others\'][j, i, 2] <= 1):\n                    center = meta[\'joint_others\'][j, i, :2]\n                    gaussian_map = heatmaps[:, :, i]\n                    heatmaps[:, :, i] = putGaussianMaps(\n                        center, gaussian_map, params_transform=self.params_transform)\n        # pafs\n        mid_1 = [2, 9, 10, 2, 12, 13, 2, 3, 4,\n                 3, 2, 6, 7, 6, 2, 1, 1, 15, 16]\n\n        mid_2 = [9, 10, 11, 12, 13, 14, 3, 4, 5,\n                 17, 6, 7, 8, 18, 1, 15, 16, 17, 18]\n\n        thre = 1\n        for i in range(19):\n            # limb\n\n            count = np.zeros((int(grid_y), int(grid_x)), dtype=np.uint32)\n            if (meta[\'joint_self\'][mid_1[i] - 1, 2] <= 1 and meta[\'joint_self\'][mid_2[i] - 1, 2] <= 1):\n                centerA = meta[\'joint_self\'][mid_1[i] - 1, :2]\n                centerB = meta[\'joint_self\'][mid_2[i] - 1, :2]\n                vec_map = pafs[:, :, 2 * i:2 * i + 2]\n                #                    print vec_map.shape\n                pafs[:, :, 2 * i:2 * i + 2], count = putVecMaps(centerA=centerA,\n                                                                centerB=centerB,\n                                                                accumulate_vec_map=vec_map,\n                                                                count=count, params_transform=self.params_transform)\n            for j in range(nop):\n                if (meta[\'joint_others\'][j, mid_1[i] - 1, 2] <= 1 and meta[\'joint_others\'][j, mid_2[i] - 1, 2] <= 1):\n                    centerA = meta[\'joint_others\'][j, mid_1[i] - 1, :2]\n                    centerB = meta[\'joint_others\'][j, mid_2[i] - 1, :2]\n                    vec_map = pafs[:, :, 2 * i:2 * i + 2]\n                    pafs[:, :, 2 * i:2 * i + 2], count = putVecMaps(centerA=centerA,\n                                                                    centerB=centerB,\n                                                                    accumulate_vec_map=vec_map,\n                                                                    count=count, params_transform=self.params_transform)\n        # background\n        heatmaps[:, :, -\n                 1] = np.maximum(1 - np.max(heatmaps[:, :, :18], axis=2), 0.)\n\n        return heat_mask, heatmaps, paf_mask, pafs\n\n    def __getitem__(self, index):\n        idx = self.index_list[index]\n        img = cv2.imread(os.path.join(self.root, self.data[idx][\'img_paths\']))\n        img_idx = self.data[idx][\'img_paths\'][-16:-3]\n#        print img.shape\n        if ""COCO_val"" in self.data[idx][\'dataset\']:\n            mask_miss = cv2.imread(\n                self.mask_dir + \'mask2014/val2014_mask_miss_\' + img_idx + \'png\', 0)\n        elif ""COCO"" in self.data[idx][\'dataset\']:\n            mask_miss = cv2.imread(\n                self.mask_dir + \'mask2014/train2014_mask_miss_\' + img_idx + \'png\', 0)\n#        print self.root + \'mask2014/val2014_mask_miss_\' + img_idx + \'png\'\n        meta_data = self.get_anno(self.data[idx])\n\n        meta_data = self.add_neck(meta_data)\n\n        meta_data, img, mask_miss = aug_scale(\n            meta_data, img, mask_miss, self.params_transform)\n\n        meta_data, img, mask_miss = aug_rotate(\n            meta_data, img, mask_miss, self.params_transform)\n\n        meta_data, img, mask_miss = aug_croppad(\n            meta_data, img, mask_miss, self.params_transform)\n\n        meta_data, img, mask_miss = aug_flip(\n            meta_data, img, mask_miss, self.params_transform)\n\n        meta_data = self.remove_illegal_joint(meta_data)\n\n        heat_mask, heatmaps, paf_mask, pafs = self.get_ground_truth(\n            meta_data, mask_miss)\n\n        # image preprocessing, which comply the model\n        # trianed on Imagenet dataset\n        if self.preprocess == \'rtpose\':\n            img = rtpose_preprocess(img)\n\n        elif self.preprocess == \'vgg\':\n            img = vgg_preprocess(img)\n\n        elif self.preprocess == \'inception\':\n            img = inception_preprocess(img)\n\n        elif self.preprocess == \'ssd\':\n            img = ssd_preprocess(img)\n\n        img = torch.from_numpy(img)\n        heatmaps = torch.from_numpy(\n            heatmaps.transpose((2, 0, 1)).astype(np.float32))\n        heat_mask = torch.from_numpy(\n            heat_mask.transpose((2, 0, 1)).astype(np.float32))\n        pafs = torch.from_numpy(pafs.transpose((2, 0, 1)).astype(np.float32))\n        paf_mask = torch.from_numpy(\n            paf_mask.transpose((2, 0, 1)).astype(np.float32))\n\n        return img, heatmaps, heat_mask, pafs, paf_mask\n\n    def __len__(self):\n        return self.numSample\n'"
src/PoseEstimation/training/datasets/coco_data/ImageAugmentation.py,0,"b'# coding=utf-8\n\nimport random\nimport sys\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import misc, ndimage\n\n\n""""""The purpose of Augmentor is to automate image augmentation \n   in order to expand datasets as input for our algorithms.\n:aut_scale : Scales them by dice2 (<1, so it is zoom out). \n:aug_croppad centerB: int with shape (2,), centerB will point to centerA.\n:aug_flip: Mirrors the image around a vertical line running through its center.\n:aug_rotate: Rotates the image. The angle of rotation, in degrees, \n             is specified by a random integer value that is included\n             in the transform argument.\n             \n:param params_transform: store the value of stride and crop_szie_y, crop_size_x                 \n""""""\n\n\ndef aug_scale(meta, img, mask_miss, params_transform):\n    dice = random.random()  # (0,1)\n    if (dice > params_transform[\'scale_prob\']):\n\n        scale_multiplier = 1\n    else:\n        dice2 = random.random()\n        # linear shear into [scale_min, scale_max]\n        scale_multiplier = (\n            params_transform[\'scale_max\'] - params_transform[\'scale_min\']) * dice2 + \\\n            params_transform[\'scale_min\']\n    scale_abs = params_transform[\'target_dist\'] / meta[\'scale_provided\']\n    scale = scale_abs * scale_multiplier\n    img = cv2.resize(img, (0, 0), fx=scale, fy=scale,\n                     interpolation=cv2.INTER_CUBIC)\n\n    mask_miss = cv2.resize(mask_miss, (0, 0), fx=scale,\n                           fy=scale, interpolation=cv2.INTER_CUBIC)\n\n    # modify meta data\n    meta[\'objpos\'] *= scale\n    meta[\'joint_self\'][:, :2] *= scale\n    if (meta[\'numOtherPeople\'] != 0):\n        meta[\'objpos_other\'] *= scale\n        meta[\'joint_others\'][:, :, :2] *= scale\n    return meta, img, mask_miss\n\n\ndef aug_croppad(meta, img, mask_miss, params_transform):\n    dice_x = random.random()\n    dice_y = random.random()\n    crop_x = int(params_transform[\'crop_size_x\'])\n    crop_y = int(params_transform[\'crop_size_y\'])\n    x_offset = int((dice_x - 0.5) * 2 *\n                   params_transform[\'center_perterb_max\'])\n    y_offset = int((dice_y - 0.5) * 2 *\n                   params_transform[\'center_perterb_max\'])\n\n    center = meta[\'objpos\'] + np.array([x_offset, y_offset])\n    center = center.astype(int)\n\n    # pad up and down\n    pad_v = np.ones((crop_y, img.shape[1], 3), dtype=np.uint8) * 128\n    pad_v_mask_miss = np.ones(\n        (crop_y, mask_miss.shape[1]), dtype=np.uint8) * 255\n\n    img = np.concatenate((pad_v, img, pad_v), axis=0)\n    mask_miss = np.concatenate(\n        (pad_v_mask_miss, mask_miss, pad_v_mask_miss), axis=0)\n\n    # pad right and left\n    pad_h = np.ones((img.shape[0], crop_x, 3), dtype=np.uint8) * 128\n    pad_h_mask_miss = np.ones(\n        (mask_miss.shape[0], crop_x), dtype=np.uint8) * 255\n\n    img = np.concatenate((pad_h, img, pad_h), axis=1)\n    mask_miss = np.concatenate(\n        (pad_h_mask_miss, mask_miss, pad_h_mask_miss), axis=1)\n\n    img = img[int(center[1] + crop_y / 2):int(center[1] + crop_y / 2 + crop_y),\n              int(center[0] + crop_x / 2):int(center[0] + crop_x / 2 + crop_x), :]\n\n    mask_miss = mask_miss[int(center[1] + crop_y / 2):int(center[1] + crop_y / 2 +\n                          crop_y + 1), int(center[0] + crop_x / 2):int(center[0] + crop_x / 2 + crop_x + 1)]\n\n    offset_left = crop_x / 2 - center[0]\n    offset_up = crop_y / 2 - center[1]\n\n    offset = np.array([offset_left, offset_up])\n    meta[\'objpos\'] += offset\n    meta[\'joint_self\'][:, :2] += offset\n    mask = np.logical_or.reduce((meta[\'joint_self\'][:, 0] >= crop_x,\n                                 meta[\'joint_self\'][:, 0] < 0,\n                                 meta[\'joint_self\'][:, 1] >= crop_y,\n                                 meta[\'joint_self\'][:, 1] < 0))\n\n    meta[\'joint_self\'][mask == True, 2] = 2\n    if (meta[\'numOtherPeople\'] != 0):\n        meta[\'objpos_other\'] += offset\n        meta[\'joint_others\'][:, :, :2] += offset\n        mask = np.logical_or.reduce((meta[\'joint_others\'][:, :, 0] >= crop_x,\n                                     meta[\'joint_others\'][:, :, 0] < 0,\n                                     meta[\'joint_others\'][:, :, 1] >= crop_y,\n                                     meta[\'joint_others\'][:, :, 1] < 0))\n\n        meta[\'joint_others\'][mask == True, 2] = 2\n\n    return meta, img, mask_miss\n\n\ndef aug_flip(meta, img, mask_miss, params_transform):\n    mode = params_transform[\'mode\']\n    num_other_people = meta[\'numOtherPeople\']\n    dice = random.random()\n    doflip = dice <= params_transform[\'flip_prob\']\n\n    if doflip:\n        img = img.copy()\n        cv2.flip(src=img, flipCode=1, dst=img)\n        w = img.shape[1]\n\n        mask_miss = mask_miss.copy()\n        cv2.flip(src=mask_miss, flipCode=1, dst=mask_miss)\n\n        \'\'\'\n        The order in this work:\n            (0-\'nose\'   1-\'neck\' 2-\'right_shoulder\' 3-\'right_elbow\' 4-\'right_wrist\'\n            5-\'left_shoulder\' 6-\'left_elbow\'        7-\'left_wrist\'  8-\'right_hip\'  \n            9-\'right_knee\'   10-\'right_ankle\'   11-\'left_hip\'   12-\'left_knee\' \n            13-\'left_ankle\'  14-\'right_eye\'     15-\'left_eye\'   16-\'right_ear\' \n            17-\'left_ear\' )\n        \'\'\'\n        meta[\'objpos\'][0] = w - 1 - meta[\'objpos\'][0]\n        meta[\'joint_self\'][:, 0] = w - 1 - meta[\'joint_self\'][:, 0]\n        # print meta[\'joint_self\']\n        meta[\'joint_self\'] = meta[\'joint_self\'][[0, 1, 5, 6,\n                                                 7, 2, 3, 4, 11, 12, 13, 8, 9, 10, 15, 14, 17, 16]]\n        if (num_other_people != 0):\n            meta[\'objpos_other\'][:, 0] = w - 1 - meta[\'objpos_other\'][:, 0]\n            meta[\'joint_others\'][:, :, 0] = w - \\\n                1 - meta[\'joint_others\'][:, :, 0]\n            for i in range(num_other_people):\n                meta[\'joint_others\'][i] = meta[\'joint_others\'][i][[\n                    0, 1, 5, 6, 7, 2, 3, 4, 11, 12, 13, 8, 9, 10, 15, 14, 17, 16]]\n\n    return meta, img, mask_miss\n\n\ndef rotatepoint(p, R):\n    point = np.zeros((3, 1))\n    point[0] = p[0]\n    point[1] = p[1]\n    point[2] = 1\n\n    new_point = R.dot(point)\n\n    p[0] = new_point[0]\n\n    p[1] = new_point[1]\n    return p\n\n\n# The correct way to rotation an image\n# http://www.pyimagesearch.com/2017/01/02/rotate-images-correctly-with-opencv-and-python/\n\n\ndef rotate_bound(image, angle, bordervalue):\n    # grab the dimensions of the image and then determine the\n    # center\n    (h, w) = image.shape[:2]\n    (cX, cY) = (w // 2, h // 2)\n\n    # grab the rotation matrix (applying the negative of the\n    # angle to rotate clockwise), then grab the sine and cosine\n    # (i.e., the rotation components of the matrix)\n    M = cv2.getRotationMatrix2D((cX, cY), -angle, 1.0)\n    cos = np.abs(M[0, 0])\n    sin = np.abs(M[0, 1])\n\n    # compute the new bounding dimensions of the image\n    nW = int((h * sin) + (w * cos))\n    nH = int((h * cos) + (w * sin))\n\n    # adjust the rotation matrix to take into account translation\n    M[0, 2] += (nW / 2) - cX\n    M[1, 2] += (nH / 2) - cY\n\n    # perform the actual rotation and return the image\n    return cv2.warpAffine(image, M, (nW, nH), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_CONSTANT,\n                          borderValue=bordervalue), M\n\n\ndef aug_rotate(meta, img, mask_miss, params_transform, type=""random"", input=0, fillType=""nearest"", constant=0):\n    dice = random.random()\n    degree = (dice - 0.5) * 2 * \\\n        params_transform[\'max_rotate_degree\']  # degree [-40,40]\n\n    img_rot, R = rotate_bound(img, np.copy(degree), (128, 128, 128))\n\n    # Not sure it will cause mask_miss to rotate rightly, just avoid it fails\n    # by np.copy().\n    mask_miss_rot, _ = rotate_bound(mask_miss, np.copy(degree), (255))\n\n    # modify meta data\n    meta[\'objpos\'] = rotatepoint(meta[\'objpos\'], R)\n\n    for i in range(18):\n        meta[\'joint_self\'][i, :] = rotatepoint(meta[\'joint_self\'][i, :], R)\n\n    for j in range(meta[\'numOtherPeople\']):\n\n        meta[\'objpos_other\'][j, :] = rotatepoint(meta[\'objpos_other\'][j, :], R)\n\n        for i in range(18):\n            meta[\'joint_others\'][j, i, :] = rotatepoint(\n                meta[\'joint_others\'][j, i, :], R)\n\n    return meta, img_rot, mask_miss_rot\n'"
src/PoseEstimation/training/datasets/coco_data/__init__.py,0,b'\n# from . import COCO_data_pipeline\n# from . import heatmap\n# from . import ImageAugmentation\n# from . import paf\n# from . import preprocessing\n'
src/PoseEstimation/training/datasets/coco_data/heatmap.py,0,"b'# coding=utf-8\n\nimport random\nimport sys\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import misc, ndimage\n\n\n""""""Implement the generate of every channel of ground truth heatmap.\n:param centerA: int with shape (2,), every coordinate of person\'s keypoint.\n:param accumulate_confid_map: one channel of heatmap, which is accumulated, \n       np.log(100) is the max value of heatmap.\n:param params_transform: store the value of stride and crop_szie_y, crop_size_x                 \n""""""\n\n\ndef putGaussianMaps(center, accumulate_confid_map, params_transform):\n    crop_size_y = params_transform[\'crop_size_y\']\n    crop_size_x = params_transform[\'crop_size_x\']\n    stride = params_transform[\'stride\']\n    sigma = params_transform[\'sigma\']\n\n    grid_y = crop_size_y / stride\n    grid_x = crop_size_x / stride\n    start = stride / 2.0 - 0.5\n    y_range = [i for i in range(int(grid_y))]\n    x_range = [i for i in range(int(grid_x))]\n    xx, yy = np.meshgrid(x_range, y_range)\n    xx = xx * stride + start\n    yy = yy * stride + start\n    d2 = (xx - center[0]) ** 2 + (yy - center[1]) ** 2\n    exponent = d2 / 2.0 / sigma / sigma\n    mask = exponent <= 4.6052\n    cofid_map = np.exp(-exponent)\n    cofid_map = np.multiply(mask, cofid_map)\n    accumulate_confid_map += cofid_map\n    accumulate_confid_map[accumulate_confid_map > 1.0] = 1.0\n    return accumulate_confid_map\n'"
src/PoseEstimation/training/datasets/coco_data/paf.py,0,"b'# coding=utf-8\n""""""Implement Part Affinity Fields\n:param centerA: int with shape (2,), centerA will pointed by centerB.\n:param centerB: int with shape (2,), centerB will point to centerA.\n:param accumulate_vec_map: one channel of paf.\n:param count: store how many pafs overlaped in one coordinate of accumulate_vec_map.\n:param params_transform: store the value of stride and crop_szie_y, crop_size_x                 \n""""""\nimport random\nimport sys\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import misc, ndimage\n\n\ndef putVecMaps(centerA, centerB, accumulate_vec_map, count, params_transform):\n    centerA = centerA.astype(float)\n    centerB = centerB.astype(float)\n\n    stride = params_transform[\'stride\']\n    crop_size_y = params_transform[\'crop_size_y\']\n    crop_size_x = params_transform[\'crop_size_x\']\n    grid_y = crop_size_y / stride\n    grid_x = crop_size_x / stride\n    thre = params_transform[\'limb_width\']   # limb width\n    centerB = centerB / stride\n    centerA = centerA / stride\n\n    limb_vec = centerB - centerA\n    norm = np.linalg.norm(limb_vec)\n    if (norm == 0.0):\n        # print \'limb is too short, ignore it...\'\n        return accumulate_vec_map, count\n    limb_vec_unit = limb_vec / norm\n    # print \'limb unit vector: {}\'.format(limb_vec_unit)\n\n    # To make sure not beyond the border of this two points\n    min_x = max(int(round(min(centerA[0], centerB[0]) - thre)), 0)\n    max_x = min(int(round(max(centerA[0], centerB[0]) + thre)), grid_x)\n    min_y = max(int(round(min(centerA[1], centerB[1]) - thre)), 0)\n    max_y = min(int(round(max(centerA[1], centerB[1]) + thre)), grid_y)\n\n    range_x = list(range(int(min_x), int(max_x), 1))\n    range_y = list(range(int(min_y), int(max_y), 1))\n    xx, yy = np.meshgrid(range_x, range_y)\n    ba_x = xx - centerA[0]  # the vector from (x,y) to centerA\n    ba_y = yy - centerA[1]\n    limb_width = np.abs(ba_x * limb_vec_unit[1] - ba_y * limb_vec_unit[0])\n    mask = limb_width < thre  # mask is 2D\n\n    vec_map = np.copy(accumulate_vec_map) * 0.0\n    vec_map[yy, xx] = np.repeat(mask[:, :, np.newaxis], 2, axis=2)\n    vec_map[yy, xx] *= limb_vec_unit[np.newaxis, np.newaxis, :]\n\n    mask = np.logical_or.reduce(\n        (np.abs(vec_map[:, :, 0]) > 0, np.abs(vec_map[:, :, 1]) > 0))\n\n    accumulate_vec_map = np.multiply(\n        accumulate_vec_map, count[:, :, np.newaxis])\n    accumulate_vec_map += vec_map\n    count[mask == True] += 1\n\n    mask = count == 0\n\n    count[mask == True] = 1\n\n    accumulate_vec_map = np.divide(accumulate_vec_map, count[:, :, np.newaxis])\n    count[mask == True] = 0\n\n    return accumulate_vec_map, count\n'"
src/PoseEstimation/training/datasets/coco_data/preprocessing.py,0,"b'""""""\nProvides different utilities to preprocess images.\nArgs:\nimage: A np.array representing an image of (h,w,3).\n\nReturns:\nA preprocessed image. which dtype is np.float32\nand transposed to (3,h,w).\n\n""""""\n\nimport cv2\nimport numpy as np\n\n\ndef rtpose_preprocess(image):\n    image = image.astype(np.float32)\n    image = image / 256. - 0.5\n    image = image.transpose((2, 0, 1)).astype(np.float32)\n\n    return image\n\n\ndef vgg_preprocess(image):\n    image = image.astype(np.float32) / 255.\n    means = [0.485, 0.456, 0.406]\n    stds = [0.229, 0.224, 0.225]\n\n    preprocessed_img = image.copy()[:, :, ::-1]\n    for i in range(3):\n        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - means[i]\n        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / stds[i]\n\n    preprocessed_img = preprocessed_img.transpose((2, 0, 1)).astype(np.float32)\n    return preprocessed_img\n\n\ndef inception_preprocess(image):\n    image = image.copy()[:, :, ::-1]\n    image = image.astype(np.float32)\n    image = image / 128. - 1.\n    image = image.transpose((2, 0, 1)).astype(np.float32)\n\n    return image\n\n\ndef ssd_preprocess(image):\n    image = image.astype(np.float32)\n    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    rgb_image -= (104.0, 117.0, 123.0)\n\n    processed_img = rgb_image.astype(np.float32)\n    processed_img = processed_img[:, :, ::-1].copy()\n    processed_img = processed_img.transpose((2, 0, 1)).astype(np.float32)\n\n    return processed_img\n'"
