file_path,api_count,code
setup.py,0,"b'from __future__ import print_function, division, absolute_import\n""""""A setuptools based setup module.\n\nSee:\nhttps://packaging.python.org/en/latest/distributing.html\nhttps://github.com/pypa/sampleproject\n""""""\n\n# Always prefer setuptools over distutils\nfrom setuptools import setup, find_packages\n# To use a consistent encoding\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\n# Arguments marked as ""Required"" below must be included for upload to PyPI.\n# Fields marked as ""Optional"" may be commented out.\n\n# https://stackoverflow.com/questions/458550/standard-way-to-embed-version-into-python-package/16084844#16084844\nexec(open(\'pretrainedmodels/version.py\').read())\nsetup(\n    # This is the name of your project. The first time you publish this\n    # package, this name will be registered for you. It will determine how\n    # users can install this project, e.g.:\n    #\n    # $ pip install sampleproject\n    #\n    # And where it will live on PyPI: https://pypi.org/project/sampleproject/\n    #\n    # There are some restrictions on what makes a valid project name\n    # specification here:\n    # https://packaging.python.org/specifications/core-metadata/#name\n    name=\'pretrainedmodels\',  # Required\n\n    # Versions should comply with PEP 440:\n    # https://www.python.org/dev/peps/pep-0440/\n    #\n    # For a discussion on single-sourcing the version across setup.py and the\n    # project code, see\n    # https://packaging.python.org/en/latest/single_source_version.html\n    version=__version__,  # Required\n\n    # This is a one-line description or tagline of what your project does. This\n    # corresponds to the ""Summary"" metadata field:\n    # https://packaging.python.org/specifications/core-metadata/#summary\n    description=\'Pretrained models for Pytorch\',  # Required\n\n    # This is an optional longer description of your project that represents\n    # the body of text which users will see when they visit PyPI.\n    #\n    # Often, this is the same as your README, so you can just read it in from\n    # that file directly (as we have already done above)\n    #\n    # This field corresponds to the ""Description"" metadata field:\n    # https://packaging.python.org/specifications/core-metadata/#description-optional\n    long_description=long_description,  # Optional\n\n    # This should be a valid link to your project\'s main homepage.\n    #\n    # This field corresponds to the ""Home-Page"" metadata field:\n    # https://packaging.python.org/specifications/core-metadata/#home-page-optional\n    url=\'https://github.com/cadene/pretrained-models.pytorch\',  # Optional\n\n    # This should be your name or the name of the organization which owns the\n    # project.\n    author=\'Remi Cadene\',  # Optional\n\n    # This should be a valid email address corresponding to the author listed\n    # above.\n    author_email=\'remi.cadene@icloud.com\',  # Optional\n\n    # Classifiers help users find your project by categorizing it.\n    #\n    # For a list of valid classifiers, see\n    # https://pypi.python.org/pypi?%3Aaction=list_classifiers\n    classifiers=[  # Optional\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        \'Development Status :: 3 - Alpha\',\n\n        # Indicate who your project is intended for\n        \'Intended Audience :: Developers\',\n        \'Topic :: Software Development :: Build Tools\',\n\n        # Pick your license as you wish\n        \'License :: OSI Approved :: MIT License\',\n\n        # Specify the Python versions you support here. In particular, ensure\n        # that you indicate whether you support Python 2, Python 3 or both.\n        \'Programming Language :: Python :: 3.6\',\n    ],\n\n    # This field adds keywords for your project which will appear on the\n    # project page. What does your project relate to?\n    #\n    # Note that this is a string of words separated by whitespace, not a list.\n    keywords=\'pytorch pretrained models deep learning\',  # Optional\n\n    # You can just specify package directories manually here if your project is\n    # simple. Or you can use find_packages().\n    #\n    # Alternatively, if you just want to distribute a single Python file, use\n    # the `py_modules` argument instead as follows, which will expect a file\n    # called `my_module.py` to exist:\n    #\n    #   py_modules=[""my_module""],\n    #\n    packages=find_packages(exclude=[\'data\', \'examples\']),  # Required\n\n    # This field lists other packages that your project depends on to run.\n    # Any package you put here will be installed by pip when your project is\n    # installed, so they must be valid existing projects.\n    #\n    # For an analysis of ""install_requires"" vs pip\'s requirements files see:\n    # https://packaging.python.org/en/latest/requirements.html\n    install_requires=[\'torch\', \'torchvision\', \'munch\', \'tqdm\'],  # Optional\n\n    # List additional groups of dependencies here (e.g. development\n    # dependencies). Users will be able to install these using the ""extras""\n    # syntax, for example:\n    #\n    #   $ pip install sampleproject[dev]\n    #\n    # Similar to `install_requires` above, these must be valid existing\n    # projects.\n    # extras_require={  # Optional\n    #     \'dev\': [\'check-manifest\'],\n    #     \'test\': [\'coverage\'],\n    # },\n\n    # If there are data files included in your packages that need to be\n    # installed, specify them here.\n    #\n    # If using Python 2.6 or earlier, then these have to be included in\n    # MANIFEST.in as well.\n    # package_data={  # Optional\n    #     \'sample\': [\'package_data.dat\'],\n    # },\n\n    # Although \'package_data\' is the preferred approach, in some case you may\n    # need to place data files outside of your packages. See:\n    # http://docs.python.org/3.4/distutils/setupscript.html#installing-additional-files\n    #\n    # In this case, \'data_file\' will be installed into \'<sys.prefix>/my_data\'\n    #data_files=[(\'my_data\', [\'data/data_file\'])],  # Optional\n\n    # To provide executable scripts, use entry points in preference to the\n    # ""scripts"" keyword. Entry points provide cross-platform support and allow\n    # `pip` to create the appropriate form of executable for the target\n    # platform.\n    #\n    # For example, the following would provide a command called `sample` which\n    # executes the function `main` from this package when invoked:\n    # entry_points={  # Optional\n    #     \'console_scripts\': [\n    #         \'sample=sample:main\',\n    #     ],\n    # },\n)\n'"
examples/imagenet_eval.py,14,"b'from __future__ import print_function, division, absolute_import\nimport argparse\nimport os\nimport shutil\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nimport sys\n\nsys.path.append(\'.\')\nimport pretrainedmodels\nimport pretrainedmodels.utils\n\nmodel_names = sorted(name for name in pretrainedmodels.__dict__\n                     if not name.startswith(""__"")\n                     and name.islower()\n                     and callable(pretrainedmodels.__dict__[name]))\n\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\nparser.add_argument(\'--data\', metavar=\'DIR\', default=""path_to_imagenet"",\n                    help=\'path to dataset\')\nparser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'nasnetamobile\',\n                    choices=model_names,\n                    help=\'model architecture: \' +\n                         \' | \'.join(model_names) +\n                         \' (default: fbresnet152)\')\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\nparser.add_argument(\'--epochs\', default=90, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=1256, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 256)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.1, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--print-freq\', \'-p\', default=10, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 10)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', default=True,\n                    action=\'store_true\', help=\'evaluate model on validation set\')\nparser.add_argument(\'--pretrained\', default=\'imagenet\', help=\'use pre-trained model\')\nparser.add_argument(\'--do-not-preserve-aspect-ratio\',\n                    dest=\'preserve_aspect_ratio\',\n                    help=\'do not preserve the aspect ratio when resizing an image\',\n                    action=\'store_false\')\nparser.set_defaults(preserve_aspect_ratio=True)\nbest_prec1 = 0\n\n\ndef main():\n    global args, best_prec1\n    args = parser.parse_args()\n\n    # create model\n    print(""=> creating model \'{}\'"".format(args.arch))\n    if args.pretrained.lower() not in [\'false\', \'none\', \'not\', \'no\', \'0\']:\n        print(""=> using pre-trained parameters \'{}\'"".format(args.pretrained))\n        model = pretrainedmodels.__dict__[args.arch](num_classes=1000,\n                                                     pretrained=args.pretrained)\n    else:\n        model = pretrainedmodels.__dict__[args.arch]()\n\n    # optionally resume from a checkpoint\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_prec1 = checkpoint[\'best_prec1\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.resume, checkpoint[\'epoch\']))\n        else:\n            print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n\n    cudnn.benchmark = True\n\n    # Data loading code\n    # traindir = os.path.join(args.data, \'train\')\n    valdir = os.path.join(args.data, \'val\')\n\n    # train_loader = torch.utils.data.DataLoader(\n    #     datasets.ImageFolder(traindir, transforms.Compose([\n    #         transforms.RandomSizedCrop(max(model.input_size)),\n    #         transforms.RandomHorizontalFlip(),\n    #         transforms.ToTensor(),\n    #         normalize,\n    #     ])),\n    #     batch_size=args.batch_size, shuffle=True,\n    #     num_workers=args.workers, pin_memory=True)\n\n\n\n    # if \'scale\' in pretrainedmodels.pretrained_settings[args.arch][args.pretrained]:\n    #     scale = pretrainedmodels.pretrained_settings[args.arch][args.pretrained][\'scale\']\n    # else:\n    #     scale = 0.875\n    scale = 0.875\n\n    print(\'Images transformed from size {} to {}\'.format(\n        int(round(max(model.input_size) / scale)),\n        model.input_size))\n\n    val_tf = pretrainedmodels.utils.TransformImage(\n        model,\n        scale=scale,\n        preserve_aspect_ratio=args.preserve_aspect_ratio\n    )\n\n    val_loader = torch.utils.data.DataLoader(\n        datasets.ImageFolder(valdir, val_tf),\n        batch_size=args.batch_size, shuffle=False,\n        num_workers=args.workers, pin_memory=True)\n\n    # define loss function (criterion) and optimizer\n    criterion = nn.CrossEntropyLoss().cuda()\n\n    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n\n    model = torch.nn.DataParallel(model).cuda()\n\n    if args.evaluate:\n        validate(val_loader, model, criterion)\n        return\n\n    for epoch in range(args.start_epoch, args.epochs):\n        adjust_learning_rate(optimizer, epoch)\n\n        # train for one epoch\n        train(train_loader, model, criterion, optimizer, epoch)\n\n        # evaluate on validation set\n        prec1 = validate(val_loader, model, criterion)\n\n        # remember best prec@1 and save checkpoint\n        is_best = prec1 > best_prec1\n        best_prec1 = max(prec1, best_prec1)\n        save_checkpoint({\n            \'epoch\': epoch + 1,\n            \'arch\': args.arch,\n            \'state_dict\': model.state_dict(),\n            \'best_prec1\': best_prec1,\n        }, is_best)\n\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (input, target) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        target = target.cuda()\n        input_var = torch.autograd.Variable(input)\n        target_var = torch.autograd.Variable(target)\n\n        # compute output\n        output = model(input_var)\n        loss = criterion(output, target_var)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n        losses.update(loss.data[0], input.size(0))\n        top1.update(prec1[0], input.size(0))\n        top5.update(prec5[0], input.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print(\'Epoch: [{0}][{1}/{2}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                  \'Acc@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                epoch, i, len(train_loader), batch_time=batch_time,\n                data_time=data_time, loss=losses, top1=top1, top5=top5))\n\n\ndef validate(val_loader, model, criterion):\n    with torch.no_grad():\n        batch_time = AverageMeter()\n        losses = AverageMeter()\n        top1 = AverageMeter()\n        top5 = AverageMeter()\n\n        # switch to evaluate mode\n        model.eval()\n\n        end = time.time()\n        for i, (input, target) in enumerate(val_loader):\n            target = target.cuda()\n            input = input.cuda()\n\n            # compute output\n            output = model(input)\n            loss = criterion(output, target)\n\n            # measure accuracy and record loss\n            prec1, prec5 = accuracy(output.data, target.data, topk=(1, 5))\n            losses.update(loss.data.item(), input.size(0))\n            top1.update(prec1.item(), input.size(0))\n            top5.update(prec5.item(), input.size(0))\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % args.print_freq == 0:\n                print(\'Test: [{0}/{1}]\\t\'\n                      \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                      \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                      \'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                      \'Acc@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                       i, len(val_loader), batch_time=batch_time, loss=losses,\n                       top1=top1, top5=top5))\n\n        print(\' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}\'\n              .format(top1=top1, top5=top5))\n\n        return top1.avg, top5.avg\n\n\ndef save_checkpoint(state, is_best, filename=\'checkpoint.pth.tar\'):\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, \'model_best.pth.tar\')\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef adjust_learning_rate(optimizer, epoch):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n    lr = args.lr * (0.1 ** (epoch // 30))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\n\nif __name__ == \'__main__\':\n    main()'"
examples/imagenet_logits.py,1,"b'from __future__ import print_function, division, absolute_import\nimport argparse\n\nfrom PIL import Image\nimport torch\nimport torchvision.transforms as transforms\n\nimport sys\nsys.path.append(\'.\')\nimport pretrainedmodels\nimport pretrainedmodels.utils as utils\n\nmodel_names = sorted(name for name in pretrainedmodels.__dict__\n    if not name.startswith(""__"")\n    and name.islower()\n    and callable(pretrainedmodels.__dict__[name]))\n\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\nparser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'nasnetalarge\',\n                    choices=model_names,\n                    help=\'model architecture: \' +\n                        \' | \'.join(model_names) +\n                        \' (default: nasnetalarge)\',\n                    nargs=\'+\')\nparser.add_argument(\'--path_img\', type=str, default=\'data/cat.jpg\')\n\ndef main():\n    global args\n    args = parser.parse_args()\n\n    for arch in args.arch:\n        # Load Model\n        model = pretrainedmodels.__dict__[arch](num_classes=1000,\n                                                pretrained=\'imagenet\')\n        model.eval()\n\n        path_img = args.path_img\n        # Load and Transform one input image\n        load_img = utils.LoadImage()\n        tf_img = utils.TransformImage(model)\n\n        input_data = load_img(args.path_img) # 3x400x225\n        input_data = tf_img(input_data)      # 3x299x299\n        input_data = input_data.unsqueeze(0) # 1x3x299x299\n        input = torch.autograd.Variable(input_data)\n\n        # Load Imagenet Synsets\n        with open(\'data/imagenet_synsets.txt\', \'r\') as f:\n            synsets = f.readlines()\n\n        # len(synsets)==1001\n        # sysnets[0] == background\n        synsets = [x.strip() for x in synsets]\n        splits = [line.split(\' \') for line in synsets]\n        key_to_classname = {spl[0]:\' \'.join(spl[1:]) for spl in splits}\n\n        with open(\'data/imagenet_classes.txt\', \'r\') as f:\n            class_id_to_key = f.readlines()\n\n        class_id_to_key = [x.strip() for x in class_id_to_key]\n\n        # Make predictions\n        output = model(input) # size(1, 1000)\n        max, argmax = output.data.squeeze().max(0)\n        class_id = argmax[0]\n        class_key = class_id_to_key[class_id]\n        classname = key_to_classname[class_key]\n\n        print(""\'{}\': \'{}\' is a \'{}\'"".format(arch, path_img, classname))\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/visu_arch.py,16,"b'from __future__ import print_function, division, absolute_import\nimport os\n\nimport torch # http://pytorch.org/about/\nfrom torch.autograd import Variable\nfrom torch.utils import model_zoo\n\nimport torchvision # https://github.com/pytorch/vision\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\nfrom lib.voc import Voc2007Classification\nfrom lib.util import load_imagenet_classes\n\nmodel_urls = {\n    # Alexnet\n    # Paper: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n    # https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py\n    \'alexnet\': \'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\',\n    # VGG\n    #\xc2\xa0Paper: https://arxiv.org/abs/1409.1556\n    #\xc2\xa0https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n    \'vgg16\': \'https://download.pytorch.org/models/vgg16-397923af.pth\',\n    # VGG BatchNorm\n    # Paper: https://arxiv.org/abs/1502.03167\n    #\xc2\xa0https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n    \'vgg16_bn\': \'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\',\n    # Inception\n    # Paper: https://arxiv.org/abs/1602.07261\n    # https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py\n    \'inception_v3\': \'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\',\n    # Resnet\n    # Paper: https://arxiv.org/abs/1512.03385\n    #\xc2\xa0https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\'\n}\n\nif __name__ == \'__main__\':\n\n    model_name = \'alexnet\'\n\n    dir_datasets = \'/home/sasl/shared/EI-SE5-CS/datasets\' # \'/tmp/torch/datasets\'\n    dir_models = \'/home/sasl/shared/EI-SE5-CS/models\' # \'/tmp/torch/models\'\n    dir_outputs = \'/tmp/outputs/\' + model_name\n\n    print(\'Create network\')\n    model = models.__dict__[model_name]() # https://stackoverflow.com/questions/19907442/python-explain-dict-attribute\n    model.eval() # http://pytorch.org/docs/master/nn.html?highlight=eval#torch.nn.Module.eval\n    print(\'\')\n\n    ##########################################################################\n\n    print(\'Display modules\')\n    print(model)\n    print(\'\')\n\n    ##########################################################################\n\n    print(\'Display parameters\')\n    state_dict = model.state_dict() # http://pytorch.org/docs/master/_modules/torch/nn/modules/module.html#Module.state_dict\n    for key, value in state_dict.items():\n        print(key, value.size())\n    print(\'\')\n\n    print(\'Display features.0.weight\')\n    print(state_dict[\'features.0.weight\'])\n    print(\'\')\n\n    ##########################################################################\n\n    print(\'Display inputs/outputs\')\n\n    def print_info(self, input, output):\n        print(\'Inside \'+ self.__class__.__name__+ \' forward\')\n        print(\'input size\', input[0].size())\n        print(\'output size\', output.data.size())\n        print(\'\')\n\n    handles = []\n    for m in model.features:\n        handles.append(m.register_forward_hook(print_info)) # http://pytorch.org/docs/master/_modules/torch/nn/modules/module.html#Module.register_forward_pre_hook\n\n    for m in model.classifier:\n        handles.append(m.register_forward_hook(print_info))\n\n    input = Variable(torch.randn(1,3,224,224).float(), requires_grad=False) # http://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py\n    output = model(input) # model(input) calls model.__call__(input) which calls model.forward(hook) and then calls the hooks\n\n    for h in handles:\n        h.remove() # to remove the hooks\n\n    print(\'\')\n\n    ##########################################################################\n\n    print(\'Load dataset Voc2007\')\n\n    train_data = Voc2007Classification(dir_datasets, \'train\') # or val, test, trainval\n\n    print(\'Voc2007 trainset has {} images\'.format(len(train_data)))\n\n    print(\'Voc2007 has {} classes\'.format(len(train_data.classes)))\n    print(train_data.classes)\n\n    item = train_data[0] # train_data contains a list of items (image, name, target)\n    img_data = item[0] # PIL.Image.Image\n    img_name = item[1] # string\n    target = item[2] #  torch.Tensor of size=20 (=nb_classes), contains 3 values: -1 (absence of class), 1 (presence of class), 0 (hard example)\n\n    os.system(\'mkdir -p \' + dir_outputs) # create a directory\n    path_img = os.path.join(dir_outputs, img_name+\'.png\')\n    img_data.save(path_img) # save image using PIL\n\n    print(\'Write image to \' + path_img)\n    for class_id, has_class in enumerate(target):\n        if has_class == 1:\n            print(\'image {} has object of class {}\'.format(img_name, train_data.classes[class_id]))\n\n    ##########################################################################\n\n    print(\'Load pretrained model on Imagenet\')\n    model.load_state_dict(model_zoo.load_url(model_urls[model_name],\n                                   model_dir=dir_models))\n\n    print(\'Display predictions\')\n\n    tf = transforms.Compose([\n        transforms.Scale(224), # rescale an RGB image to size 224^ (not a square)\n        transforms.CenterCrop(224), # extract a square of size 224 at the center of the image\n        transforms.ToTensor(), # convert the PIL.Image into a torch.Tensor\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406], # mean pixel value per channel\n            std=[0.229, 0.224, 0.225] # standard deviation value per channel\n        )\n    ])\n\n    input_data = tf(img_data)\n    input_data = input_data.unsqueeze(0) # (3,224,224) -> (1,3,224,224)\n    print(\'input size\', input_data.size())\n    print(input_data)\n\n    input = Variable(input_data, requires_grad=False)\n    output = model(input)\n\n    print(\'output size\', output.data.size())\n    print(output.data)\n\n    # Load Imagenet Synsets\n    imagenet_classes = load_imagenet_classes()\n    print(\'Imagenet has {} classes\'.format(imagenet_classes))\n\n    max, argmax = output.data.squeeze().max(0)\n    class_id = argmax[0]\n    print(\'Image {} is of class ""{}""\'.format(img_name, imagenet_classes[class_id]))\n\n    #############################################################################\n\n    print(\'Save normalized input as RGB image\')\n\n    dir_activations = os.path.join(dir_outputs,\'activations\')\n    os.system(\'mkdir -p \' + dir_activations)\n\n    path_img_input = os.path.join(dir_activations, \'input.png\')\n    print(\'save input activation to \' + path_img_input)\n    transforms.ToPILImage()(input_data[0]).save(path_img_input) # save image using PIL\n\n    print(\'\')\n\n    #############################################################################\n\n    print(\'Save activations as Gray images\')\n\n    layer_id = 0\n\n    def save_activation(self, input, output):\n        global layer_id\n\n        for i in range(10):#output.data.size(1)):\n            path_img_output = os.path.join(dir_activations, \'layer{}_{}_channel{}.png\'.format(layer_id, self.__class__.__name__, i))\n            print(\'save output activation to \' + path_img_output)\n            torchvision.utils.save_image(output.data.squeeze(0)[i], path_img_output) # save image (of type Tensor) using torchvision\n\n        layer_id += 1\n\n    handles = []\n    for m in model.features:\n        handles.append(m.register_forward_hook(save_activation))\n\n    input = Variable(input_data, requires_grad=False)\n    output = model(input)\n\n    for h in handles:\n        h.remove()\n\n    print(\'\')\n\n    #############################################################################\n\n    dir_parameters = os.path.join(dir_outputs, \'parameters\')\n    os.system(\'mkdir -p \' + dir_parameters)\n    state_dict = model.state_dict()\n\n    print(\'Save first layer parameters as RGB images\')\n\n    weight = state_dict[\'features.0.weight\']\n    for filter_id in range(weight.size(0)):\n        path_param = os.path.join(dir_parameters, \'features.0.weight_filter{}.png\'.format(filter_id))\n        print(\'save \' + path_param)\n        torchvision.utils.save_image(weight[filter_id], path_param)\n\n    print(\'\')\n\n\n    print(\'Save other layer parameters as Gray images\')\n\n    for key in state_dict:\n        if \'features\' in key and \'weight\' in key:\n            for filter_id in range(3):\n                for channel_id in range(3):\n                    path_param = os.path.join(dir_parameters, \'{}_filter{}_channel{}.png\'.format(key, filter_id, channel_id))\n                    print(\'save \' + path_param)\n                    torchvision.utils.save_image(state_dict[key][filter_id][channel_id], path_param)\n\n    print(\'\')\n\n'"
examples/voc2007_extract.py,17,"b'from __future__ import print_function, division, absolute_import\nimport os\nimport argparse\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.autograd import Variable\nfrom torch.utils import model_zoo\n\n# http://scikit-learn.org\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\n\nimport sys\nsys.path.append(\'.\')\nimport pretrainedmodels\nimport pretrainedmodels.utils\nimport pretrainedmodels.datasets\n\nmodel_names = sorted(name for name in pretrainedmodels.__dict__\n    if not name.startswith(""__"")\n    and name.islower()\n    and callable(pretrainedmodels.__dict__[name]))\n\ndef extract_features_targets(model, features_size, loader, path_data, cuda=False):\n    if os.path.isfile(path_data):\n        print(\'Load features from {}\'.format(path_data))\n        return torch.load(path_data)\n\n    print(\'\\nExtract features on {}set\'.format(loader.dataset.set))\n\n    features = torch.Tensor(len(loader.dataset), features_size)\n    targets = torch.Tensor(len(loader.dataset), len(loader.dataset.classes))\n\n    for batch_id, batch in enumerate(tqdm(loader)):\n        img = batch[0]\n        target = batch[2]\n        current_bsize = img.size(0)\n        from_ = int(batch_id * loader.batch_size)\n        to_ = int(from_ + current_bsize)\n\n        if cuda:\n            img = img.cuda(async=True)\n\n        input = Variable(img, requires_grad=False)\n        output = model(input)\n\n        features[from_:to_] = output.data.cpu()\n        targets[from_:to_] = target\n\n    os.system(\'mkdir -p {}\'.format(os.path.dirname(path_data)))\n    print(\'save \' + path_data)\n    torch.save((features, targets), path_data)\n    print(\'\')\n    return features, targets\n\ndef train_multilabel(features, targets, classes, train_split, test_split, C=1.0, ignore_hard_examples=True, after_ReLU=False, normalize_L2=False):\n    print(\'\\nHyperparameters:\\n - C: {}\\n - after_ReLU: {}\\n - normL2: {}\'.format(C, after_ReLU, normalize_L2))\n    train_APs = []\n    test_APs = []\n    for class_id in range(len(classes)):\n\n        classifier = SVC(C=C, kernel=\'linear\') # http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n\n        if ignore_hard_examples:\n            train_masks = (targets[train_split][:,class_id] != 0).view(-1, 1)\n            train_features = torch.masked_select(features[train_split], train_masks.expand_as(features[train_split])).view(-1,features[train_split].size(1))\n            train_targets = torch.masked_select(targets[train_split], train_masks.expand_as(targets[train_split])).view(-1,targets[train_split].size(1))\n            test_masks = (targets[test_split][:,class_id] != 0).view(-1, 1)\n            test_features = torch.masked_select(features[test_split], test_masks.expand_as(features[test_split])).view(-1,features[test_split].size(1))\n            test_targets = torch.masked_select(targets[test_split], test_masks.expand_as(targets[test_split])).view(-1,targets[test_split].size(1))\n        else:\n            train_features = features[train_split]\n            train_targets = targets[train_split]\n            test_features = features[test_split]\n            test_targets = features[test_split]\n\n        if after_ReLU:\n            train_features[train_features < 0] = 0\n            test_features[test_features < 0] = 0\n\n        if normalize_L2:\n            train_norm = torch.norm(train_features, p=2, dim=1).unsqueeze(1)\n            train_features = train_features.div(train_norm.expand_as(train_features))\n            test_norm = torch.norm(test_features, p=2, dim=1).unsqueeze(1)\n            test_features = test_features.div(test_norm.expand_as(test_features))\n\n        train_X = train_features.numpy()\n        train_y = (train_targets[:,class_id] != -1).numpy() # uses hard examples if not ignored\n\n        test_X = test_features.numpy()\n        test_y = (test_targets[:,class_id] != -1).numpy()\n\n        classifier.fit(train_X, train_y) # train parameters of the classifier\n\n        train_preds = classifier.predict(train_X)\n        train_acc = accuracy_score(train_y, train_preds) * 100\n        train_AP = average_precision_score(train_y, train_preds) * 100\n        train_APs.append(train_AP)\n\n        test_preds = classifier.predict(test_X)\n        test_acc = accuracy_score(test_y, test_preds) * 100\n        test_AP = average_precision_score(test_y, test_preds) * 100\n        test_APs.append(test_AP)\n\n        print(\'class ""{}"" ({}/{}):\'.format(classes[class_id], test_y.sum(), test_y.shape[0]))\n        print(\'  - {:8}: acc {:.2f}, AP {:.2f}\'.format(train_split, train_acc, train_AP))\n        print(\'  - {:8}: acc {:.2f}, AP {:.2f}\'.format(test_split, test_acc, test_AP))\n\n    print(\'all classes:\')\n    print(\'  - {:8}: mAP {:.4f}\'.format(train_split, sum(train_APs)/len(classes)))\n    print(\'  - {:8}: mAP {:.4f}\'.format(test_split, sum(test_APs)/len(classes)))\n\n##########################################################################\n# main\n##########################################################################\n\nparser = argparse.ArgumentParser(\n    description=\'Train/Evaluate models\',\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--dir_outputs\', default=\'/tmp/outputs\', type=str, help=\'\')\nparser.add_argument(\'--dir_datasets\', default=\'/tmp/datasets\', type=str, help=\'\')\nparser.add_argument(\'--C\', default=1, type=float, help=\'\')\nparser.add_argument(\'-b\', \'--batch_size\', default=50, type=float, help=\'\')\nparser.add_argument(\'-a\', \'--arch\', default=\'alexnet\', choices=model_names,\n                    help=\'model architecture: \' +\n                        \' | \'.join(model_names) +\n                        \' (default: alexnet)\')\nparser.add_argument(\'--train_split\', default=\'train\', type=str, help=\'\')\nparser.add_argument(\'--test_split\', default=\'val\', type=str, help=\'\')\nparser.add_argument(\'--cuda\', const=True, nargs=\'?\', type=bool, help=\'\')\n\ndef main ():\n    global args\n    args = parser.parse_args()\n    print(\'\\nCUDA status: {}\'.format(args.cuda))\n\n    print(\'\\nLoad pretrained model on Imagenet\')\n    model = pretrainedmodels.__dict__[args.arch](num_classes=1000, pretrained=\'imagenet\')\n    model.eval()\n    if args.cuda:\n        model.cuda()\n\n    features_size = model.last_linear.in_features\n    model.last_linear = pretrainedmodels.utils.Identity() # Trick to get inputs (features) from last_linear\n\n    print(\'\\nLoad datasets\')\n    tf_img = pretrainedmodels.utils.TransformImage(model)\n    train_set = pretrainedmodels.datasets.Voc2007Classification(args.dir_datasets, \'train\', transform=tf_img)\n    val_set = pretrainedmodels.datasets.Voc2007Classification(args.dir_datasets, \'val\', transform=tf_img)\n    test_set = pretrainedmodels.datasets.Voc2007Classification(args.dir_datasets, \'test\', transform=tf_img)\n\n    train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size, shuffle=False, num_workers=2)\n    val_loader = torch.utils.data.DataLoader(val_set, batch_size=args.batch_size, shuffle=False, num_workers=2)\n    test_loader = torch.utils.data.DataLoader(test_set, batch_size=args.batch_size, shuffle=False, num_workers=2)\n\n    print(\'\\nLoad features\')\n    dir_features = os.path.join(args.dir_outputs, \'data/{}\'.format(args.arch))\n    path_train_data = \'{}/{}set.pth\'.format(dir_features, \'train\')\n    path_val_data = \'{}/{}set.pth\'.format(dir_features, \'val\')\n    path_test_data = \'{}/{}set.pth\'.format(dir_features, \'test\')\n\n    features = {}\n    targets = {}\n    features[\'train\'], targets[\'train\'] = extract_features_targets(model, features_size, train_loader, path_train_data, args.cuda)\n    features[\'val\'], targets[\'val\'] = extract_features_targets(model, features_size, val_loader, path_val_data, args.cuda)\n    features[\'test\'], targets[\'test\'] = extract_features_targets(model, features_size, test_loader, path_test_data, args.cuda)\n    features[\'trainval\'] = torch.cat([features[\'train\'], features[\'val\']], 0)\n    targets[\'trainval\'] = torch.cat([targets[\'train\'], targets[\'val\']], 0)\n\n    print(\'\\nTrain Support Vector Machines\')\n    if args.train_split == \'train\' and args.test_split == \'val\':\n        print(\'\\nHyperparameters search: train multilabel classifiers (on-versus-all) on train/val\')\n    elif args.train_split == \'trainval\' and args.test_split == \'test\':\n        print(\'\\nEvaluation: train a multilabel classifier on trainval/test\')\n    else:\n        raise ValueError(\'Trying to train on {} and eval on {}\'.format(args.train_split, args.test_split))\n\n    train_multilabel(features, targets, train_set.classes, args.train_split, args.test_split, C=args.C)\n\n\nif __name__ == \'__main__\':\n    main()'"
pretrainedmodels/__init__.py,0,"b""from .version import __version__\n\nfrom . import models\nfrom . import datasets\n\nfrom .models.utils import pretrained_settings\nfrom .models.utils import model_names\n\n# to support pretrainedmodels.__dict__['nasnetalarge']\n# but depreciated\nfrom .models.fbresnet import fbresnet152\nfrom .models.cafferesnet import cafferesnet101\nfrom .models.bninception import bninception\nfrom .models.resnext import resnext101_32x4d\nfrom .models.resnext import resnext101_64x4d\nfrom .models.inceptionv4 import inceptionv4\nfrom .models.inceptionresnetv2 import inceptionresnetv2\nfrom .models.nasnet import nasnetalarge\nfrom .models.nasnet_mobile import nasnetamobile\nfrom .models.torchvision_models import alexnet\nfrom .models.torchvision_models import densenet121\nfrom .models.torchvision_models import densenet169\nfrom .models.torchvision_models import densenet201\nfrom .models.torchvision_models import densenet161\nfrom .models.torchvision_models import resnet18\nfrom .models.torchvision_models import resnet34\nfrom .models.torchvision_models import resnet50\nfrom .models.torchvision_models import resnet101\nfrom .models.torchvision_models import resnet152\nfrom .models.torchvision_models import inceptionv3\nfrom .models.torchvision_models import squeezenet1_0\nfrom .models.torchvision_models import squeezenet1_1\nfrom .models.torchvision_models import vgg11\nfrom .models.torchvision_models import vgg11_bn\nfrom .models.torchvision_models import vgg13\nfrom .models.torchvision_models import vgg13_bn\nfrom .models.torchvision_models import vgg16\nfrom .models.torchvision_models import vgg16_bn\nfrom .models.torchvision_models import vgg19_bn\nfrom .models.torchvision_models import vgg19\nfrom .models.dpn import dpn68\nfrom .models.dpn import dpn68b\nfrom .models.dpn import dpn92\nfrom .models.dpn import dpn98\nfrom .models.dpn import dpn131\nfrom .models.dpn import dpn107\nfrom .models.xception import xception\nfrom .models.senet import senet154\nfrom .models.senet import se_resnet50\nfrom .models.senet import se_resnet101\nfrom .models.senet import se_resnet152\nfrom .models.senet import se_resnext50_32x4d\nfrom .models.senet import se_resnext101_32x4d\nfrom .models.pnasnet import pnasnet5large\nfrom .models.polynet import polynet\n"""
pretrainedmodels/utils.py,1,"b""from __future__ import print_function, division, absolute_import\nimport math\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom munch import munchify\n\nclass ToSpaceBGR(object):\n\n    def __init__(self, is_bgr):\n        self.is_bgr = is_bgr\n\n    def __call__(self, tensor):\n        if self.is_bgr:\n            new_tensor = tensor.clone()\n            new_tensor[0] = tensor[2]\n            new_tensor[2] = tensor[0]\n            tensor = new_tensor\n        return tensor\n\n\nclass ToRange255(object):\n\n    def __init__(self, is_255):\n        self.is_255 = is_255\n\n    def __call__(self, tensor):\n        if self.is_255:\n            tensor.mul_(255)\n        return tensor\n\n\nclass TransformImage(object):\n\n    def __init__(self, opts, scale=0.875, random_crop=False,\n                 random_hflip=False, random_vflip=False,\n                 preserve_aspect_ratio=True):\n        if type(opts) == dict:\n            opts = munchify(opts)\n        self.input_size = opts.input_size\n        self.input_space = opts.input_space\n        self.input_range = opts.input_range\n        self.mean = opts.mean\n        self.std = opts.std\n\n        # https://github.com/tensorflow/models/blob/master/research/inception/inception/image_processing.py#L294\n        self.scale = scale\n        self.random_crop = random_crop\n        self.random_hflip = random_hflip\n        self.random_vflip = random_vflip\n\n        tfs = []\n        if preserve_aspect_ratio:\n            tfs.append(transforms.Resize(int(math.floor(max(self.input_size)/self.scale))))\n        else:\n            height = int(self.input_size[1] / self.scale)\n            width = int(self.input_size[2] / self.scale)\n            tfs.append(transforms.Resize((height, width)))\n\n        if random_crop:\n            tfs.append(transforms.RandomCrop(max(self.input_size)))\n        else:\n            tfs.append(transforms.CenterCrop(max(self.input_size)))\n\n        if random_hflip:\n            tfs.append(transforms.RandomHorizontalFlip())\n\n        if random_vflip:\n            tfs.append(transforms.RandomVerticalFlip())\n\n        tfs.append(transforms.ToTensor())\n        tfs.append(ToSpaceBGR(self.input_space=='BGR'))\n        tfs.append(ToRange255(max(self.input_range)==255))\n        tfs.append(transforms.Normalize(mean=self.mean, std=self.std))\n\n        self.tf = transforms.Compose(tfs)\n\n    def __call__(self, img):\n        tensor = self.tf(img)\n        return tensor\n\n\nclass LoadImage(object):\n\n    def __init__(self, space='RGB'):\n        self.space = space\n\n    def __call__(self, path_img):\n        with open(path_img, 'rb') as f:\n            with Image.open(f) as img:\n                img = img.convert(self.space)\n        return img\n\n\nclass LoadTransformImage(object):\n\n    def __init__(self, model, scale=0.875):\n        self.load = LoadImage()\n        self.tf = TransformImage(model, scale=scale)\n\n    def __call__(self, path_img):\n        img = self.load(path_img)\n        tensor = self.tf(img)\n        return tensor\n\n\nclass Identity(nn.Module):\n\n    def __init__(self):\n        super(Identity, self).__init__()\n\n    def forward(self, x):\n        return x"""
pretrainedmodels/version.py,0,"b""from __future__ import print_function, division, absolute_import\n__version__ = '0.7.4'\n"""
tests/test_pm_imagenet.py,7,"b'import pytest\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport pretrainedmodels as pm\nimport pretrainedmodels.utils as utils\n\n# torch 1.0.x\nset_grad_enabled = getattr(torch.autograd, \'set_grad_enabled\', None)\n\npm_args = []\nfor model_name in pm.model_names:\n    for pretrained in pm.pretrained_settings[model_name]:\n        if pretrained in [\'imagenet\', \'imagenet+5k\']:\n            pm_args.append((model_name, pretrained))\n\nimg = utils.LoadImage()(\'data/cat.jpg\')\n\n\ndef equal(x,y):\n    return torch.all(torch.lt(torch.abs(torch.add(x, -y)), 1e-12))\n\n@pytest.mark.parametrize(\'model_name, pretrained\', pm_args)\ndef test_pm_imagenet(model_name, pretrained):\n    if set_grad_enabled: set_grad_enabled(False)\n\n    print(\'test_pm_imagenet(""{}"")\'.format(model_name))\n    net = pm.__dict__[model_name](\n        num_classes=1000,\n        pretrained=pretrained)\n    net.eval()\n\n    tensor = utils.TransformImage(net)(img)\n    tensor = tensor.unsqueeze(0)\n    x = Variable(tensor, requires_grad=False)\n\n    out_logits = net(x)\n    if \'squeezenet\' in model_name:\n        # Conv2d without view at the end\n        assert out_logits.shape == torch.Size([1,1000,1,1])\n        return\n\n    assert out_logits.shape == torch.Size([1,1000])\n\n    out_feats = net.features(x)\n    out_logits_2 = net.logits(out_feats)\n    assert equal(out_logits, out_logits_2)\n\n    if \'dpn\' in model_name:\n        # Conv2d instead of Linear\n        return\n    net.last_linear = nn.Linear(\n        net.last_linear.in_features,\n        10)\n\n    out_logits_3 = net.logits(out_feats)\n    assert out_logits_3.shape == torch.Size([1,10])\n\n    if set_grad_enabled: set_grad_enabled(True)\n'"
tests/test_torch_save.py,1,"b'import pytest\nimport torch\nimport pretrainedmodels as pm\n\n\n# TODO: put ""pm_args"" into fixture to share with all tests?\npm_args = []\nfor model_name in pm.model_names:    \n    for pretrained in pm.pretrained_settings[model_name]:\n        if pretrained in [\'imagenet\', \'imagenet+5k\']:\n            pm_args.append((model_name, pretrained))\n            \n\n@pytest.mark.parametrize(\'model_name, pretrained\', pm_args)\ndef test_torch_save(model_name, pretrained, tmp_path):\n    print(\'test_torch_save(""{}"")\'.format(model_name))\n    net = pm.__dict__[model_name](\n        num_classes=1000,\n        pretrained=pretrained)\n    \n    tmp_file = tmp_path/\'{}.pkl\'.format(model_name)\n    torch.save(net, tmp_file.open(\'wb\'))\n    tmp_file.unlink()\n'"
pretrainedmodels/datasets/__init__.py,0,"b'from __future__ import print_function, division, absolute_import\nfrom .voc import Voc2007Classification'"
pretrainedmodels/datasets/utils.py,9,"b'from __future__ import print_function, division, absolute_import\nimport math\nfrom six.moves.urllib.request import urlretrieve\n\nimport torch\nfrom PIL import Image\nfrom tqdm import tqdm\n\ndef load_imagenet_classes(path_synsets=\'data/imagenet_synsets.txt\',\n                          path_classes=\'data/imagenet_classes.txt\'):\n    with open(path_synsets, \'r\') as f:\n        synsets = f.readlines()\n\n    synsets = [x.strip() for x in synsets]\n    splits = [line.split(\' \') for line in synsets]\n    key_to_classname = {spl[0]:\' \'.join(spl[1:]) for spl in splits}\n\n    with open(path_classes, \'r\') as f:\n        class_id_to_key = f.readlines()\n\n    class_id_to_key = [x.strip() for x in class_id_to_key]\n\n    cid_to_cname = []\n    for i in range(len(class_id_to_key)):\n        key = class_id_to_key[i]\n        cname = key_to_classname[key]\n        cid_to_cname.append(cname)\n\n    return cid_to_cname\n\n\nclass Warp(object):\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = int(size)\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        return img.resize((self.size, self.size), self.interpolation)\n\n    def __str__(self):\n        return self.__class__.__name__ + \' (size={size}, interpolation={interpolation})\'.format(size=self.size,\n                                                                                                interpolation=self.interpolation)\n\n\ndef download_url(url, destination=None, progress_bar=True):\n    """"""Download a URL to a local file.\n\n    Parameters\n    ----------\n    url : str\n        The URL to download.\n    destination : str, None\n        The destination of the file. If None is given the file is saved to a temporary directory.\n    progress_bar : bool\n        Whether to show a command-line progress bar while downloading.\n\n    Returns\n    -------\n    filename : str\n        The location of the downloaded file.\n\n    Notes\n    -----\n    Progress bar use/example adapted from tqdm documentation: https://github.com/tqdm/tqdm\n    """"""\n\n    def my_hook(t):\n        last_b = [0]\n\n        def inner(b=1, bsize=1, tsize=None):\n            if tsize is not None:\n                t.total = tsize\n            if b > 0:\n                t.update((b - last_b[0]) * bsize)\n            last_b[0] = b\n\n        return inner\n\n    if progress_bar:\n        with tqdm(unit=\'B\', unit_scale=True, miniters=1, desc=url.split(\'/\')[-1]) as t:\n            filename, _ = urlretrieve(url, filename=destination, reporthook=my_hook(t))\n    else:\n        filename, _ = urlretrieve(url, filename=destination)\n\n\nclass AveragePrecisionMeter(object):\n    """"""\n    The APMeter measures the average precision per class.\n    The APMeter is designed to operate on `NxK` Tensors `output` and\n    `target`, and optionally a `Nx1` Tensor weight where (1) the `output`\n    contains model output scores for `N` examples and `K` classes that ought to\n    be higher when the model is more convinced that the example should be\n    positively labeled, and smaller when the model believes the example should\n    be negatively labeled (for instance, the output of a sigmoid function); (2)\n    the `target` contains only values 0 (for negative examples) and 1\n    (for positive examples); and (3) the `weight` ( > 0) represents weight for\n    each sample.\n    """"""\n\n    def __init__(self, difficult_examples=False):\n        super(AveragePrecisionMeter, self).__init__()\n        self.reset()\n        self.difficult_examples = difficult_examples\n\n    def reset(self):\n        """"""Resets the meter with empty member variables""""""\n        self.scores = torch.FloatTensor(torch.FloatStorage())\n        self.targets = torch.LongTensor(torch.LongStorage())\n\n    def add(self, output, target):\n        """"""\n        Args:\n            output (Tensor): NxK tensor that for each of the N examples\n                indicates the probability of the example belonging to each of\n                the K classes, according to the model. The probabilities should\n                sum to one over all classes\n            target (Tensor): binary NxK tensort that encodes which of the K\n                classes are associated with the N-th input\n                    (eg: a row [0, 1, 0, 1] indicates that the example is\n                         associated with classes 2 and 4)\n            weight (optional, Tensor): Nx1 tensor representing the weight for\n                each example (each weight > 0)\n        """"""\n        if not torch.is_tensor(output):\n            output = torch.from_numpy(output)\n        if not torch.is_tensor(target):\n            target = torch.from_numpy(target)\n\n        if output.dim() == 1:\n            output = output.view(-1, 1)\n        else:\n            assert output.dim() == 2, \\\n                \'wrong output size (should be 1D or 2D with one column \\\n                per class)\'\n        if target.dim() == 1:\n            target = target.view(-1, 1)\n        else:\n            assert target.dim() == 2, \\\n                \'wrong target size (should be 1D or 2D with one column \\\n                per class)\'\n        if self.scores.numel() > 0:\n            assert target.size(1) == self.targets.size(1), \\\n                \'dimensions for output should match previously added examples.\'\n\n        # make sure storage is of sufficient size\n        if self.scores.storage().size() < self.scores.numel() + output.numel():\n            new_size = math.ceil(self.scores.storage().size() * 1.5)\n            self.scores.storage().resize_(int(new_size + output.numel()))\n            self.targets.storage().resize_(int(new_size + output.numel()))\n\n        # store scores and targets\n        offset = self.scores.size(0) if self.scores.dim() > 0 else 0\n        self.scores.resize_(offset + output.size(0), output.size(1))\n        self.targets.resize_(offset + target.size(0), target.size(1))\n        self.scores.narrow(0, offset, output.size(0)).copy_(output)\n        self.targets.narrow(0, offset, target.size(0)).copy_(target)\n\n    def value(self):\n        """"""Returns the model\'s average precision for each class\n        Return:\n            ap (FloatTensor): 1xK tensor, with avg precision for each class k\n        """"""\n\n        if self.scores.numel() == 0:\n            return 0\n        ap = torch.zeros(self.scores.size(1))\n        rg = torch.arange(1, self.scores.size(0)).float()\n\n        # compute average precision for each class\n        for k in range(self.scores.size(1)):\n            # sort scores\n            scores = self.scores[:, k]\n            targets = self.targets[:, k]\n\n            # compute average precision\n            ap[k] = AveragePrecisionMeter.average_precision(scores, targets, self.difficult_examples)\n        return ap\n\n    @staticmethod\n    def average_precision(output, target, difficult_examples=True):\n\n        # sort examples\n        sorted, indices = torch.sort(output, dim=0, descending=True)\n\n        # Computes prec@i\n        pos_count = 0.\n        total_count = 0.\n        precision_at_i = 0.\n        for i in indices:\n            label = target[i]\n            if difficult_examples and label == 0:\n                continue\n            if label == 1:\n                pos_count += 1\n            total_count += 1\n            if label == 1:\n                precision_at_i += pos_count / total_count\n        precision_at_i /= pos_count\n        return precision_at_i'"
pretrainedmodels/datasets/voc.py,2,"b'from __future__ import print_function, division, absolute_import\nimport csv\nimport os\nimport os.path\nimport tarfile\nfrom six.moves.urllib.parse import urlparse\n\nimport numpy as np\nimport torch\nimport torch.utils.data as data\nfrom PIL import Image\n\nfrom . import utils\n\nobject_categories = [\'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n                     \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n                     \'cow\', \'diningtable\', \'dog\', \'horse\',\n                     \'motorbike\', \'person\', \'pottedplant\',\n                     \'sheep\', \'sofa\', \'train\', \'tvmonitor\']\n\nurls = {\n    \'devkit\': \'http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCdevkit_18-May-2011.tar\',\n    \'trainval_2007\': \'http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\',\n    \'test_images_2007\': \'http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\',\n    \'test_anno_2007\': \'http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtestnoimgs_06-Nov-2007.tar\',\n}\n\n\ndef read_image_label(file):\n    print(\'[dataset] read \' + file)\n    data = dict()\n    with open(file, \'r\') as f:\n        for line in f:\n            tmp = line.split(\' \')\n            name = tmp[0]\n            label = int(tmp[-1])\n            data[name] = label\n            # data.append([name, label])\n            # print(\'%s  %d\' % (name, label))\n    return data\n\n\ndef read_object_labels(root, dataset, set):\n    path_labels = os.path.join(root, \'VOCdevkit\', dataset, \'ImageSets\', \'Main\')\n    labeled_data = dict()\n    num_classes = len(object_categories)\n\n    for i in range(num_classes):\n        file = os.path.join(path_labels, object_categories[i] + \'_\' + set + \'.txt\')\n        data = read_image_label(file)\n\n        if i == 0:\n            for (name, label) in data.items():\n                labels = np.zeros(num_classes)\n                labels[i] = label\n                labeled_data[name] = labels\n        else:\n            for (name, label) in data.items():\n                labeled_data[name][i] = label\n\n    return labeled_data\n\n\ndef write_object_labels_csv(file, labeled_data):\n    # write a csv file\n    print(\'[dataset] write file %s\' % file)\n    with open(file, \'w\') as csvfile:\n        fieldnames = [\'name\']\n        fieldnames.extend(object_categories)\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for (name, labels) in labeled_data.items():\n            example = {\'name\': name}\n            for i in range(20):\n                example[fieldnames[i + 1]] = int(labels[i])\n            writer.writerow(example)\n\n    csvfile.close()\n\n\ndef read_object_labels_csv(file, header=True):\n    images = []\n    num_categories = 0\n    print(\'[dataset] read\', file)\n    with open(file, \'r\') as f:\n        reader = csv.reader(f)\n        rownum = 0\n        for row in reader:\n            if header and rownum == 0:\n                header = row\n            else:\n                if num_categories == 0:\n                    num_categories = len(row) - 1\n                name = row[0]\n                labels = (np.asarray(row[1:num_categories + 1])).astype(np.float32)\n                labels = torch.from_numpy(labels)\n                item = (name, labels)\n                images.append(item)\n            rownum += 1\n    return images\n\n\ndef find_images_classification(root, dataset, set):\n    path_labels = os.path.join(root, \'VOCdevkit\', dataset, \'ImageSets\', \'Main\')\n    images = []\n    file = os.path.join(path_labels, set + \'.txt\')\n    with open(file, \'r\') as f:\n        for line in f:\n            images.append(line)\n    return images\n\n\ndef download_voc2007(root):\n    path_devkit = os.path.join(root, \'VOCdevkit\')\n    path_images = os.path.join(root, \'VOCdevkit\', \'VOC2007\', \'JPEGImages\')\n    tmpdir = os.path.join(root, \'tmp\')\n\n    # create directory\n    if not os.path.exists(root):\n        os.makedirs(root)\n\n    if not os.path.exists(path_devkit):\n\n        if not os.path.exists(tmpdir):\n            os.makedirs(tmpdir)\n\n        parts = urlparse(urls[\'devkit\'])\n        filename = os.path.basename(parts.path)\n        cached_file = os.path.join(tmpdir, filename)\n\n        if not os.path.exists(cached_file):\n            print(\'Downloading: ""{}"" to {}\\n\'.format(urls[\'devkit\'], cached_file))\n            utils.download_url(urls[\'devkit\'], cached_file)\n\n        # extract file\n        print(\'[dataset] Extracting tar file {file} to {path}\'.format(file=cached_file, path=root))\n        cwd = os.getcwd()\n        tar = tarfile.open(cached_file, ""r"")\n        os.chdir(root)\n        tar.extractall()\n        tar.close()\n        os.chdir(cwd)\n        print(\'[dataset] Done!\')\n\n    # train/val images/annotations\n    if not os.path.exists(path_images):\n\n        # download train/val images/annotations\n        parts = urlparse(urls[\'trainval_2007\'])\n        filename = os.path.basename(parts.path)\n        cached_file = os.path.join(tmpdir, filename)\n\n        if not os.path.exists(cached_file):\n            print(\'Downloading: ""{}"" to {}\\n\'.format(urls[\'trainval_2007\'], cached_file))\n            utils.download_url(urls[\'trainval_2007\'], cached_file)\n\n        # extract file\n        print(\'[dataset] Extracting tar file {file} to {path}\'.format(file=cached_file, path=root))\n        cwd = os.getcwd()\n        tar = tarfile.open(cached_file, ""r"")\n        os.chdir(root)\n        tar.extractall()\n        tar.close()\n        os.chdir(cwd)\n        print(\'[dataset] Done!\')\n\n    # test annotations\n    test_anno = os.path.join(path_devkit, \'VOC2007/ImageSets/Main/aeroplane_test.txt\')\n    if not os.path.exists(test_anno):\n\n        # download test annotations\n        parts = urlparse(urls[\'test_images_2007\'])\n        filename = os.path.basename(parts.path)\n        cached_file = os.path.join(tmpdir, filename)\n\n        if not os.path.exists(cached_file):\n            print(\'Downloading: ""{}"" to {}\\n\'.format(urls[\'test_images_2007\'], cached_file))\n            utils.download_url(urls[\'test_images_2007\'], cached_file)\n\n        # extract file\n        print(\'[dataset] Extracting tar file {file} to {path}\'.format(file=cached_file, path=root))\n        cwd = os.getcwd()\n        tar = tarfile.open(cached_file, ""r"")\n        os.chdir(root)\n        tar.extractall()\n        tar.close()\n        os.chdir(cwd)\n        print(\'[dataset] Done!\')\n\n    # test images\n    test_image = os.path.join(path_devkit, \'VOC2007/JPEGImages/000001.jpg\')\n    if not os.path.exists(test_image):\n\n        # download test images\n        parts = urlparse(urls[\'test_anno_2007\'])\n        filename = os.path.basename(parts.path)\n        cached_file = os.path.join(tmpdir, filename)\n\n        if not os.path.exists(cached_file):\n            print(\'Downloading: ""{}"" to {}\\n\'.format(urls[\'test_anno_2007\'], cached_file))\n            utils.download_url(urls[\'test_anno_2007\'], cached_file)\n\n        # extract file\n        print(\'[dataset] Extracting tar file {file} to {path}\'.format(file=cached_file, path=root))\n        cwd = os.getcwd()\n        tar = tarfile.open(cached_file, ""r"")\n        os.chdir(root)\n        tar.extractall()\n        tar.close()\n        os.chdir(cwd)\n        print(\'[dataset] Done!\')\n\n\nclass Voc2007Classification(data.Dataset):\n\n    def __init__(self, root, set, transform=None, target_transform=None):\n        self.root = root\n        self.path_devkit = os.path.join(root, \'VOCdevkit\')\n        self.path_images = os.path.join(root, \'VOCdevkit\', \'VOC2007\', \'JPEGImages\')\n        self.set = set\n        self.transform = transform\n        self.target_transform = target_transform\n\n        # download dataset\n        download_voc2007(self.root)\n\n        # define path of csv file\n        path_csv = os.path.join(self.root, \'files\', \'VOC2007\')\n        # define filename of csv file\n        file_csv = os.path.join(path_csv, \'classification_\' + set + \'.csv\')\n\n        # create the csv file if necessary\n        if not os.path.exists(file_csv):\n            if not os.path.exists(path_csv):  # create dir if necessary\n                os.makedirs(path_csv)\n            # generate csv file\n            labeled_data = read_object_labels(self.root, \'VOC2007\', self.set)\n            # write csv file\n            write_object_labels_csv(file_csv, labeled_data)\n\n        self.classes = object_categories\n        self.images = read_object_labels_csv(file_csv)\n\n        print(\'[dataset] VOC 2007 classification set=%s number of classes=%d  number of images=%d\' % (\n            set, len(self.classes), len(self.images)))\n\n    def __getitem__(self, index):\n        path, target = self.images[index]\n        img = Image.open(os.path.join(self.path_images, path + \'.jpg\')).convert(\'RGB\')\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        return img, path, target\n\n    def __len__(self):\n        return len(self.images)\n\n    def get_number_classes(self):\n        return len(self.classes)\n'"
pretrainedmodels/models/__init__.py,0,"b'from __future__ import print_function, division, absolute_import\nfrom .fbresnet import fbresnet152\n\nfrom .cafferesnet import cafferesnet101\n\nfrom .bninception import bninception\n\nfrom .resnext import resnext101_32x4d\nfrom .resnext import resnext101_64x4d\n\nfrom .inceptionv4 import inceptionv4\n\nfrom .inceptionresnetv2 import inceptionresnetv2\n\nfrom .nasnet import nasnetalarge\n\nfrom .nasnet_mobile import nasnetamobile\n\nfrom .torchvision_models import alexnet\nfrom .torchvision_models import densenet121\nfrom .torchvision_models import densenet169\nfrom .torchvision_models import densenet201\nfrom .torchvision_models import densenet161\nfrom .torchvision_models import resnet18\nfrom .torchvision_models import resnet34\nfrom .torchvision_models import resnet50\nfrom .torchvision_models import resnet101\nfrom .torchvision_models import resnet152\nfrom .torchvision_models import inceptionv3\nfrom .torchvision_models import squeezenet1_0\nfrom .torchvision_models import squeezenet1_1\nfrom .torchvision_models import vgg11\nfrom .torchvision_models import vgg11_bn\nfrom .torchvision_models import vgg13\nfrom .torchvision_models import vgg13_bn\nfrom .torchvision_models import vgg16\nfrom .torchvision_models import vgg16_bn\nfrom .torchvision_models import vgg19_bn\nfrom .torchvision_models import vgg19\n\nfrom .dpn import dpn68\nfrom .dpn import dpn68b\nfrom .dpn import dpn92\nfrom .dpn import dpn98\nfrom .dpn import dpn131\nfrom .dpn import dpn107\n\nfrom .xception import xception\n\nfrom .senet import senet154\nfrom .senet import se_resnet50\nfrom .senet import se_resnet101\nfrom .senet import se_resnet152\nfrom .senet import se_resnext50_32x4d\nfrom .senet import se_resnext101_32x4d\n\nfrom .pnasnet import pnasnet5large\nfrom .polynet import polynet\n'"
pretrainedmodels/models/bninception.py,13,"b'from __future__ import print_function, division, absolute_import\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nimport os\nimport sys\n\n__all__ = [\'BNInception\', \'bninception\']\n\npretrained_settings = {\n    \'bninception\': {\n        \'imagenet\': {\n            # Was ported using python2 (may trigger warning)\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/bn_inception-52deb4733.pth\',\n            # \'url\': \'http://yjxiong.me/others/bn_inception-9f5701afb96c8044.pth\',\n            \'input_space\': \'BGR\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 255],\n            \'mean\': [104, 117, 128],\n            \'std\': [1, 1, 1],\n            \'num_classes\': 1000\n        }\n    }\n}\n\nclass BNInception(nn.Module):\n\n    def __init__(self, num_classes=1000):\n        super(BNInception, self).__init__()\n        inplace = True\n        self.conv1_7x7_s2 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n        self.conv1_7x7_s2_bn = nn.BatchNorm2d(64, affine=True)\n        self.conv1_relu_7x7 = nn.ReLU (inplace)\n        self.pool1_3x3_s2 = nn.MaxPool2d ((3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n        self.conv2_3x3_reduce = nn.Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.conv2_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n        self.conv2_relu_3x3_reduce = nn.ReLU (inplace)\n        self.conv2_3x3 = nn.Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv2_3x3_bn = nn.BatchNorm2d(192, affine=True)\n        self.conv2_relu_3x3 = nn.ReLU (inplace)\n        self.pool2_3x3_s2 = nn.MaxPool2d ((3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n        self.inception_3a_1x1 = nn.Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3a_1x1_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_3a_relu_1x1 = nn.ReLU (inplace)\n        self.inception_3a_3x3_reduce = nn.Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3a_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_3a_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_3a_3x3 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3a_3x3_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_3a_relu_3x3 = nn.ReLU (inplace)\n        self.inception_3a_double_3x3_reduce = nn.Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3a_double_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_3a_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_3a_double_3x3_1 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3a_double_3x3_1_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_3a_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_3a_double_3x3_2 = nn.Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3a_double_3x3_2_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_3a_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_3a_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_3a_pool_proj = nn.Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3a_pool_proj_bn = nn.BatchNorm2d(32, affine=True)\n        self.inception_3a_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_3b_1x1 = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3b_1x1_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_3b_relu_1x1 = nn.ReLU (inplace)\n        self.inception_3b_3x3_reduce = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3b_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_3b_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_3b_3x3 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3b_3x3_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_3b_relu_3x3 = nn.ReLU (inplace)\n        self.inception_3b_double_3x3_reduce = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3b_double_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_3b_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_3b_double_3x3_1 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3b_double_3x3_1_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_3b_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_3b_double_3x3_2 = nn.Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3b_double_3x3_2_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_3b_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_3b_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_3b_pool_proj = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3b_pool_proj_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_3b_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_3c_3x3_reduce = nn.Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3c_3x3_reduce_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_3c_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_3c_3x3 = nn.Conv2d(128, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.inception_3c_3x3_bn = nn.BatchNorm2d(160, affine=True)\n        self.inception_3c_relu_3x3 = nn.ReLU (inplace)\n        self.inception_3c_double_3x3_reduce = nn.Conv2d(320, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3c_double_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_3c_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_3c_double_3x3_1 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3c_double_3x3_1_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_3c_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_3c_double_3x3_2 = nn.Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.inception_3c_double_3x3_2_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_3c_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_3c_pool = nn.MaxPool2d ((3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n        self.inception_4a_1x1 = nn.Conv2d(576, 224, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4a_1x1_bn = nn.BatchNorm2d(224, affine=True)\n        self.inception_4a_relu_1x1 = nn.ReLU (inplace)\n        self.inception_4a_3x3_reduce = nn.Conv2d(576, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4a_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n        self.inception_4a_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4a_3x3 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4a_3x3_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_4a_relu_3x3 = nn.ReLU (inplace)\n        self.inception_4a_double_3x3_reduce = nn.Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4a_double_3x3_reduce_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_4a_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4a_double_3x3_1 = nn.Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4a_double_3x3_1_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4a_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_4a_double_3x3_2 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4a_double_3x3_2_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4a_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_4a_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_4a_pool_proj = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4a_pool_proj_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4a_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_4b_1x1 = nn.Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4b_1x1_bn = nn.BatchNorm2d(192, affine=True)\n        self.inception_4b_relu_1x1 = nn.ReLU (inplace)\n        self.inception_4b_3x3_reduce = nn.Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4b_3x3_reduce_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_4b_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4b_3x3 = nn.Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4b_3x3_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4b_relu_3x3 = nn.ReLU (inplace)\n        self.inception_4b_double_3x3_reduce = nn.Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4b_double_3x3_reduce_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_4b_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4b_double_3x3_1 = nn.Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4b_double_3x3_1_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4b_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_4b_double_3x3_2 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4b_double_3x3_2_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4b_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_4b_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_4b_pool_proj = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4b_pool_proj_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4b_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_4c_1x1 = nn.Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4c_1x1_bn = nn.BatchNorm2d(160, affine=True)\n        self.inception_4c_relu_1x1 = nn.ReLU (inplace)\n        self.inception_4c_3x3_reduce = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4c_3x3_reduce_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4c_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4c_3x3 = nn.Conv2d(128, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4c_3x3_bn = nn.BatchNorm2d(160, affine=True)\n        self.inception_4c_relu_3x3 = nn.ReLU (inplace)\n        self.inception_4c_double_3x3_reduce = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4c_double_3x3_reduce_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4c_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4c_double_3x3_1 = nn.Conv2d(128, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4c_double_3x3_1_bn = nn.BatchNorm2d(160, affine=True)\n        self.inception_4c_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_4c_double_3x3_2 = nn.Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4c_double_3x3_2_bn = nn.BatchNorm2d(160, affine=True)\n        self.inception_4c_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_4c_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_4c_pool_proj = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4c_pool_proj_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4c_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_4d_1x1 = nn.Conv2d(608, 96, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4d_1x1_bn = nn.BatchNorm2d(96, affine=True)\n        self.inception_4d_relu_1x1 = nn.ReLU (inplace)\n        self.inception_4d_3x3_reduce = nn.Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4d_3x3_reduce_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4d_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4d_3x3 = nn.Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4d_3x3_bn = nn.BatchNorm2d(192, affine=True)\n        self.inception_4d_relu_3x3 = nn.ReLU (inplace)\n        self.inception_4d_double_3x3_reduce = nn.Conv2d(608, 160, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4d_double_3x3_reduce_bn = nn.BatchNorm2d(160, affine=True)\n        self.inception_4d_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4d_double_3x3_1 = nn.Conv2d(160, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4d_double_3x3_1_bn = nn.BatchNorm2d(192, affine=True)\n        self.inception_4d_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_4d_double_3x3_2 = nn.Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4d_double_3x3_2_bn = nn.BatchNorm2d(192, affine=True)\n        self.inception_4d_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_4d_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_4d_pool_proj = nn.Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4d_pool_proj_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4d_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_4e_3x3_reduce = nn.Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4e_3x3_reduce_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_4e_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4e_3x3 = nn.Conv2d(128, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.inception_4e_3x3_bn = nn.BatchNorm2d(192, affine=True)\n        self.inception_4e_relu_3x3 = nn.ReLU (inplace)\n        self.inception_4e_double_3x3_reduce = nn.Conv2d(608, 192, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4e_double_3x3_reduce_bn = nn.BatchNorm2d(192, affine=True)\n        self.inception_4e_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4e_double_3x3_1 = nn.Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4e_double_3x3_1_bn = nn.BatchNorm2d(256, affine=True)\n        self.inception_4e_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_4e_double_3x3_2 = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.inception_4e_double_3x3_2_bn = nn.BatchNorm2d(256, affine=True)\n        self.inception_4e_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_4e_pool = nn.MaxPool2d ((3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n        self.inception_5a_1x1 = nn.Conv2d(1056, 352, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5a_1x1_bn = nn.BatchNorm2d(352, affine=True)\n        self.inception_5a_relu_1x1 = nn.ReLU (inplace)\n        self.inception_5a_3x3_reduce = nn.Conv2d(1056, 192, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5a_3x3_reduce_bn = nn.BatchNorm2d(192, affine=True)\n        self.inception_5a_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_5a_3x3 = nn.Conv2d(192, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_5a_3x3_bn = nn.BatchNorm2d(320, affine=True)\n        self.inception_5a_relu_3x3 = nn.ReLU (inplace)\n        self.inception_5a_double_3x3_reduce = nn.Conv2d(1056, 160, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5a_double_3x3_reduce_bn = nn.BatchNorm2d(160, affine=True)\n        self.inception_5a_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_5a_double_3x3_1 = nn.Conv2d(160, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_5a_double_3x3_1_bn = nn.BatchNorm2d(224, affine=True)\n        self.inception_5a_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_5a_double_3x3_2 = nn.Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_5a_double_3x3_2_bn = nn.BatchNorm2d(224, affine=True)\n        self.inception_5a_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_5a_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_5a_pool_proj = nn.Conv2d(1056, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5a_pool_proj_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_5a_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_5b_1x1 = nn.Conv2d(1024, 352, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5b_1x1_bn = nn.BatchNorm2d(352, affine=True)\n        self.inception_5b_relu_1x1 = nn.ReLU (inplace)\n        self.inception_5b_3x3_reduce = nn.Conv2d(1024, 192, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5b_3x3_reduce_bn = nn.BatchNorm2d(192, affine=True)\n        self.inception_5b_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_5b_3x3 = nn.Conv2d(192, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_5b_3x3_bn = nn.BatchNorm2d(320, affine=True)\n        self.inception_5b_relu_3x3 = nn.ReLU (inplace)\n        self.inception_5b_double_3x3_reduce = nn.Conv2d(1024, 192, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5b_double_3x3_reduce_bn = nn.BatchNorm2d(192, affine=True)\n        self.inception_5b_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_5b_double_3x3_1 = nn.Conv2d(192, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_5b_double_3x3_1_bn = nn.BatchNorm2d(224, affine=True)\n        self.inception_5b_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_5b_double_3x3_2 = nn.Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_5b_double_3x3_2_bn = nn.BatchNorm2d(224, affine=True)\n        self.inception_5b_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_5b_pool = nn.MaxPool2d ((3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), ceil_mode=True)\n        self.inception_5b_pool_proj = nn.Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5b_pool_proj_bn = nn.BatchNorm2d(128, affine=True)\n        self.inception_5b_relu_pool_proj = nn.ReLU (inplace)\n        self.last_linear = nn.Linear (1024, num_classes)\n\n    def features(self, input):\n        conv1_7x7_s2_out = self.conv1_7x7_s2(input)\n        conv1_7x7_s2_bn_out = self.conv1_7x7_s2_bn(conv1_7x7_s2_out)\n        conv1_relu_7x7_out = self.conv1_relu_7x7(conv1_7x7_s2_bn_out)\n        pool1_3x3_s2_out = self.pool1_3x3_s2(conv1_relu_7x7_out)\n        conv2_3x3_reduce_out = self.conv2_3x3_reduce(pool1_3x3_s2_out)\n        conv2_3x3_reduce_bn_out = self.conv2_3x3_reduce_bn(conv2_3x3_reduce_out)\n        conv2_relu_3x3_reduce_out = self.conv2_relu_3x3_reduce(conv2_3x3_reduce_bn_out)\n        conv2_3x3_out = self.conv2_3x3(conv2_relu_3x3_reduce_out)\n        conv2_3x3_bn_out = self.conv2_3x3_bn(conv2_3x3_out)\n        conv2_relu_3x3_out = self.conv2_relu_3x3(conv2_3x3_bn_out)\n        pool2_3x3_s2_out = self.pool2_3x3_s2(conv2_relu_3x3_out)\n        inception_3a_1x1_out = self.inception_3a_1x1(pool2_3x3_s2_out)\n        inception_3a_1x1_bn_out = self.inception_3a_1x1_bn(inception_3a_1x1_out)\n        inception_3a_relu_1x1_out = self.inception_3a_relu_1x1(inception_3a_1x1_bn_out)\n        inception_3a_3x3_reduce_out = self.inception_3a_3x3_reduce(pool2_3x3_s2_out)\n        inception_3a_3x3_reduce_bn_out = self.inception_3a_3x3_reduce_bn(inception_3a_3x3_reduce_out)\n        inception_3a_relu_3x3_reduce_out = self.inception_3a_relu_3x3_reduce(inception_3a_3x3_reduce_bn_out)\n        inception_3a_3x3_out = self.inception_3a_3x3(inception_3a_relu_3x3_reduce_out)\n        inception_3a_3x3_bn_out = self.inception_3a_3x3_bn(inception_3a_3x3_out)\n        inception_3a_relu_3x3_out = self.inception_3a_relu_3x3(inception_3a_3x3_bn_out)\n        inception_3a_double_3x3_reduce_out = self.inception_3a_double_3x3_reduce(pool2_3x3_s2_out)\n        inception_3a_double_3x3_reduce_bn_out = self.inception_3a_double_3x3_reduce_bn(inception_3a_double_3x3_reduce_out)\n        inception_3a_relu_double_3x3_reduce_out = self.inception_3a_relu_double_3x3_reduce(inception_3a_double_3x3_reduce_bn_out)\n        inception_3a_double_3x3_1_out = self.inception_3a_double_3x3_1(inception_3a_relu_double_3x3_reduce_out)\n        inception_3a_double_3x3_1_bn_out = self.inception_3a_double_3x3_1_bn(inception_3a_double_3x3_1_out)\n        inception_3a_relu_double_3x3_1_out = self.inception_3a_relu_double_3x3_1(inception_3a_double_3x3_1_bn_out)\n        inception_3a_double_3x3_2_out = self.inception_3a_double_3x3_2(inception_3a_relu_double_3x3_1_out)\n        inception_3a_double_3x3_2_bn_out = self.inception_3a_double_3x3_2_bn(inception_3a_double_3x3_2_out)\n        inception_3a_relu_double_3x3_2_out = self.inception_3a_relu_double_3x3_2(inception_3a_double_3x3_2_bn_out)\n        inception_3a_pool_out = self.inception_3a_pool(pool2_3x3_s2_out)\n        inception_3a_pool_proj_out = self.inception_3a_pool_proj(inception_3a_pool_out)\n        inception_3a_pool_proj_bn_out = self.inception_3a_pool_proj_bn(inception_3a_pool_proj_out)\n        inception_3a_relu_pool_proj_out = self.inception_3a_relu_pool_proj(inception_3a_pool_proj_bn_out)\n        inception_3a_output_out = torch.cat([inception_3a_relu_1x1_out,inception_3a_relu_3x3_out,inception_3a_relu_double_3x3_2_out ,inception_3a_relu_pool_proj_out], 1)\n        inception_3b_1x1_out = self.inception_3b_1x1(inception_3a_output_out)\n        inception_3b_1x1_bn_out = self.inception_3b_1x1_bn(inception_3b_1x1_out)\n        inception_3b_relu_1x1_out = self.inception_3b_relu_1x1(inception_3b_1x1_bn_out)\n        inception_3b_3x3_reduce_out = self.inception_3b_3x3_reduce(inception_3a_output_out)\n        inception_3b_3x3_reduce_bn_out = self.inception_3b_3x3_reduce_bn(inception_3b_3x3_reduce_out)\n        inception_3b_relu_3x3_reduce_out = self.inception_3b_relu_3x3_reduce(inception_3b_3x3_reduce_bn_out)\n        inception_3b_3x3_out = self.inception_3b_3x3(inception_3b_relu_3x3_reduce_out)\n        inception_3b_3x3_bn_out = self.inception_3b_3x3_bn(inception_3b_3x3_out)\n        inception_3b_relu_3x3_out = self.inception_3b_relu_3x3(inception_3b_3x3_bn_out)\n        inception_3b_double_3x3_reduce_out = self.inception_3b_double_3x3_reduce(inception_3a_output_out)\n        inception_3b_double_3x3_reduce_bn_out = self.inception_3b_double_3x3_reduce_bn(inception_3b_double_3x3_reduce_out)\n        inception_3b_relu_double_3x3_reduce_out = self.inception_3b_relu_double_3x3_reduce(inception_3b_double_3x3_reduce_bn_out)\n        inception_3b_double_3x3_1_out = self.inception_3b_double_3x3_1(inception_3b_relu_double_3x3_reduce_out)\n        inception_3b_double_3x3_1_bn_out = self.inception_3b_double_3x3_1_bn(inception_3b_double_3x3_1_out)\n        inception_3b_relu_double_3x3_1_out = self.inception_3b_relu_double_3x3_1(inception_3b_double_3x3_1_bn_out)\n        inception_3b_double_3x3_2_out = self.inception_3b_double_3x3_2(inception_3b_relu_double_3x3_1_out)\n        inception_3b_double_3x3_2_bn_out = self.inception_3b_double_3x3_2_bn(inception_3b_double_3x3_2_out)\n        inception_3b_relu_double_3x3_2_out = self.inception_3b_relu_double_3x3_2(inception_3b_double_3x3_2_bn_out)\n        inception_3b_pool_out = self.inception_3b_pool(inception_3a_output_out)\n        inception_3b_pool_proj_out = self.inception_3b_pool_proj(inception_3b_pool_out)\n        inception_3b_pool_proj_bn_out = self.inception_3b_pool_proj_bn(inception_3b_pool_proj_out)\n        inception_3b_relu_pool_proj_out = self.inception_3b_relu_pool_proj(inception_3b_pool_proj_bn_out)\n        inception_3b_output_out = torch.cat([inception_3b_relu_1x1_out,inception_3b_relu_3x3_out,inception_3b_relu_double_3x3_2_out,inception_3b_relu_pool_proj_out], 1)\n        inception_3c_3x3_reduce_out = self.inception_3c_3x3_reduce(inception_3b_output_out)\n        inception_3c_3x3_reduce_bn_out = self.inception_3c_3x3_reduce_bn(inception_3c_3x3_reduce_out)\n        inception_3c_relu_3x3_reduce_out = self.inception_3c_relu_3x3_reduce(inception_3c_3x3_reduce_bn_out)\n        inception_3c_3x3_out = self.inception_3c_3x3(inception_3c_relu_3x3_reduce_out)\n        inception_3c_3x3_bn_out = self.inception_3c_3x3_bn(inception_3c_3x3_out)\n        inception_3c_relu_3x3_out = self.inception_3c_relu_3x3(inception_3c_3x3_bn_out)\n        inception_3c_double_3x3_reduce_out = self.inception_3c_double_3x3_reduce(inception_3b_output_out)\n        inception_3c_double_3x3_reduce_bn_out = self.inception_3c_double_3x3_reduce_bn(inception_3c_double_3x3_reduce_out)\n        inception_3c_relu_double_3x3_reduce_out = self.inception_3c_relu_double_3x3_reduce(inception_3c_double_3x3_reduce_bn_out)\n        inception_3c_double_3x3_1_out = self.inception_3c_double_3x3_1(inception_3c_relu_double_3x3_reduce_out)\n        inception_3c_double_3x3_1_bn_out = self.inception_3c_double_3x3_1_bn(inception_3c_double_3x3_1_out)\n        inception_3c_relu_double_3x3_1_out = self.inception_3c_relu_double_3x3_1(inception_3c_double_3x3_1_bn_out)\n        inception_3c_double_3x3_2_out = self.inception_3c_double_3x3_2(inception_3c_relu_double_3x3_1_out)\n        inception_3c_double_3x3_2_bn_out = self.inception_3c_double_3x3_2_bn(inception_3c_double_3x3_2_out)\n        inception_3c_relu_double_3x3_2_out = self.inception_3c_relu_double_3x3_2(inception_3c_double_3x3_2_bn_out)\n        inception_3c_pool_out = self.inception_3c_pool(inception_3b_output_out)\n        inception_3c_output_out = torch.cat([inception_3c_relu_3x3_out,inception_3c_relu_double_3x3_2_out,inception_3c_pool_out], 1)\n        inception_4a_1x1_out = self.inception_4a_1x1(inception_3c_output_out)\n        inception_4a_1x1_bn_out = self.inception_4a_1x1_bn(inception_4a_1x1_out)\n        inception_4a_relu_1x1_out = self.inception_4a_relu_1x1(inception_4a_1x1_bn_out)\n        inception_4a_3x3_reduce_out = self.inception_4a_3x3_reduce(inception_3c_output_out)\n        inception_4a_3x3_reduce_bn_out = self.inception_4a_3x3_reduce_bn(inception_4a_3x3_reduce_out)\n        inception_4a_relu_3x3_reduce_out = self.inception_4a_relu_3x3_reduce(inception_4a_3x3_reduce_bn_out)\n        inception_4a_3x3_out = self.inception_4a_3x3(inception_4a_relu_3x3_reduce_out)\n        inception_4a_3x3_bn_out = self.inception_4a_3x3_bn(inception_4a_3x3_out)\n        inception_4a_relu_3x3_out = self.inception_4a_relu_3x3(inception_4a_3x3_bn_out)\n        inception_4a_double_3x3_reduce_out = self.inception_4a_double_3x3_reduce(inception_3c_output_out)\n        inception_4a_double_3x3_reduce_bn_out = self.inception_4a_double_3x3_reduce_bn(inception_4a_double_3x3_reduce_out)\n        inception_4a_relu_double_3x3_reduce_out = self.inception_4a_relu_double_3x3_reduce(inception_4a_double_3x3_reduce_bn_out)\n        inception_4a_double_3x3_1_out = self.inception_4a_double_3x3_1(inception_4a_relu_double_3x3_reduce_out)\n        inception_4a_double_3x3_1_bn_out = self.inception_4a_double_3x3_1_bn(inception_4a_double_3x3_1_out)\n        inception_4a_relu_double_3x3_1_out = self.inception_4a_relu_double_3x3_1(inception_4a_double_3x3_1_bn_out)\n        inception_4a_double_3x3_2_out = self.inception_4a_double_3x3_2(inception_4a_relu_double_3x3_1_out)\n        inception_4a_double_3x3_2_bn_out = self.inception_4a_double_3x3_2_bn(inception_4a_double_3x3_2_out)\n        inception_4a_relu_double_3x3_2_out = self.inception_4a_relu_double_3x3_2(inception_4a_double_3x3_2_bn_out)\n        inception_4a_pool_out = self.inception_4a_pool(inception_3c_output_out)\n        inception_4a_pool_proj_out = self.inception_4a_pool_proj(inception_4a_pool_out)\n        inception_4a_pool_proj_bn_out = self.inception_4a_pool_proj_bn(inception_4a_pool_proj_out)\n        inception_4a_relu_pool_proj_out = self.inception_4a_relu_pool_proj(inception_4a_pool_proj_bn_out)\n        inception_4a_output_out = torch.cat([inception_4a_relu_1x1_out,inception_4a_relu_3x3_out,inception_4a_relu_double_3x3_2_out,inception_4a_relu_pool_proj_out], 1)\n        inception_4b_1x1_out = self.inception_4b_1x1(inception_4a_output_out)\n        inception_4b_1x1_bn_out = self.inception_4b_1x1_bn(inception_4b_1x1_out)\n        inception_4b_relu_1x1_out = self.inception_4b_relu_1x1(inception_4b_1x1_bn_out)\n        inception_4b_3x3_reduce_out = self.inception_4b_3x3_reduce(inception_4a_output_out)\n        inception_4b_3x3_reduce_bn_out = self.inception_4b_3x3_reduce_bn(inception_4b_3x3_reduce_out)\n        inception_4b_relu_3x3_reduce_out = self.inception_4b_relu_3x3_reduce(inception_4b_3x3_reduce_bn_out)\n        inception_4b_3x3_out = self.inception_4b_3x3(inception_4b_relu_3x3_reduce_out)\n        inception_4b_3x3_bn_out = self.inception_4b_3x3_bn(inception_4b_3x3_out)\n        inception_4b_relu_3x3_out = self.inception_4b_relu_3x3(inception_4b_3x3_bn_out)\n        inception_4b_double_3x3_reduce_out = self.inception_4b_double_3x3_reduce(inception_4a_output_out)\n        inception_4b_double_3x3_reduce_bn_out = self.inception_4b_double_3x3_reduce_bn(inception_4b_double_3x3_reduce_out)\n        inception_4b_relu_double_3x3_reduce_out = self.inception_4b_relu_double_3x3_reduce(inception_4b_double_3x3_reduce_bn_out)\n        inception_4b_double_3x3_1_out = self.inception_4b_double_3x3_1(inception_4b_relu_double_3x3_reduce_out)\n        inception_4b_double_3x3_1_bn_out = self.inception_4b_double_3x3_1_bn(inception_4b_double_3x3_1_out)\n        inception_4b_relu_double_3x3_1_out = self.inception_4b_relu_double_3x3_1(inception_4b_double_3x3_1_bn_out)\n        inception_4b_double_3x3_2_out = self.inception_4b_double_3x3_2(inception_4b_relu_double_3x3_1_out)\n        inception_4b_double_3x3_2_bn_out = self.inception_4b_double_3x3_2_bn(inception_4b_double_3x3_2_out)\n        inception_4b_relu_double_3x3_2_out = self.inception_4b_relu_double_3x3_2(inception_4b_double_3x3_2_bn_out)\n        inception_4b_pool_out = self.inception_4b_pool(inception_4a_output_out)\n        inception_4b_pool_proj_out = self.inception_4b_pool_proj(inception_4b_pool_out)\n        inception_4b_pool_proj_bn_out = self.inception_4b_pool_proj_bn(inception_4b_pool_proj_out)\n        inception_4b_relu_pool_proj_out = self.inception_4b_relu_pool_proj(inception_4b_pool_proj_bn_out)\n        inception_4b_output_out = torch.cat([inception_4b_relu_1x1_out,inception_4b_relu_3x3_out,inception_4b_relu_double_3x3_2_out,inception_4b_relu_pool_proj_out], 1)\n        inception_4c_1x1_out = self.inception_4c_1x1(inception_4b_output_out)\n        inception_4c_1x1_bn_out = self.inception_4c_1x1_bn(inception_4c_1x1_out)\n        inception_4c_relu_1x1_out = self.inception_4c_relu_1x1(inception_4c_1x1_bn_out)\n        inception_4c_3x3_reduce_out = self.inception_4c_3x3_reduce(inception_4b_output_out)\n        inception_4c_3x3_reduce_bn_out = self.inception_4c_3x3_reduce_bn(inception_4c_3x3_reduce_out)\n        inception_4c_relu_3x3_reduce_out = self.inception_4c_relu_3x3_reduce(inception_4c_3x3_reduce_bn_out)\n        inception_4c_3x3_out = self.inception_4c_3x3(inception_4c_relu_3x3_reduce_out)\n        inception_4c_3x3_bn_out = self.inception_4c_3x3_bn(inception_4c_3x3_out)\n        inception_4c_relu_3x3_out = self.inception_4c_relu_3x3(inception_4c_3x3_bn_out)\n        inception_4c_double_3x3_reduce_out = self.inception_4c_double_3x3_reduce(inception_4b_output_out)\n        inception_4c_double_3x3_reduce_bn_out = self.inception_4c_double_3x3_reduce_bn(inception_4c_double_3x3_reduce_out)\n        inception_4c_relu_double_3x3_reduce_out = self.inception_4c_relu_double_3x3_reduce(inception_4c_double_3x3_reduce_bn_out)\n        inception_4c_double_3x3_1_out = self.inception_4c_double_3x3_1(inception_4c_relu_double_3x3_reduce_out)\n        inception_4c_double_3x3_1_bn_out = self.inception_4c_double_3x3_1_bn(inception_4c_double_3x3_1_out)\n        inception_4c_relu_double_3x3_1_out = self.inception_4c_relu_double_3x3_1(inception_4c_double_3x3_1_bn_out)\n        inception_4c_double_3x3_2_out = self.inception_4c_double_3x3_2(inception_4c_relu_double_3x3_1_out)\n        inception_4c_double_3x3_2_bn_out = self.inception_4c_double_3x3_2_bn(inception_4c_double_3x3_2_out)\n        inception_4c_relu_double_3x3_2_out = self.inception_4c_relu_double_3x3_2(inception_4c_double_3x3_2_bn_out)\n        inception_4c_pool_out = self.inception_4c_pool(inception_4b_output_out)\n        inception_4c_pool_proj_out = self.inception_4c_pool_proj(inception_4c_pool_out)\n        inception_4c_pool_proj_bn_out = self.inception_4c_pool_proj_bn(inception_4c_pool_proj_out)\n        inception_4c_relu_pool_proj_out = self.inception_4c_relu_pool_proj(inception_4c_pool_proj_bn_out)\n        inception_4c_output_out = torch.cat([inception_4c_relu_1x1_out,inception_4c_relu_3x3_out,inception_4c_relu_double_3x3_2_out,inception_4c_relu_pool_proj_out], 1)\n        inception_4d_1x1_out = self.inception_4d_1x1(inception_4c_output_out)\n        inception_4d_1x1_bn_out = self.inception_4d_1x1_bn(inception_4d_1x1_out)\n        inception_4d_relu_1x1_out = self.inception_4d_relu_1x1(inception_4d_1x1_bn_out)\n        inception_4d_3x3_reduce_out = self.inception_4d_3x3_reduce(inception_4c_output_out)\n        inception_4d_3x3_reduce_bn_out = self.inception_4d_3x3_reduce_bn(inception_4d_3x3_reduce_out)\n        inception_4d_relu_3x3_reduce_out = self.inception_4d_relu_3x3_reduce(inception_4d_3x3_reduce_bn_out)\n        inception_4d_3x3_out = self.inception_4d_3x3(inception_4d_relu_3x3_reduce_out)\n        inception_4d_3x3_bn_out = self.inception_4d_3x3_bn(inception_4d_3x3_out)\n        inception_4d_relu_3x3_out = self.inception_4d_relu_3x3(inception_4d_3x3_bn_out)\n        inception_4d_double_3x3_reduce_out = self.inception_4d_double_3x3_reduce(inception_4c_output_out)\n        inception_4d_double_3x3_reduce_bn_out = self.inception_4d_double_3x3_reduce_bn(inception_4d_double_3x3_reduce_out)\n        inception_4d_relu_double_3x3_reduce_out = self.inception_4d_relu_double_3x3_reduce(inception_4d_double_3x3_reduce_bn_out)\n        inception_4d_double_3x3_1_out = self.inception_4d_double_3x3_1(inception_4d_relu_double_3x3_reduce_out)\n        inception_4d_double_3x3_1_bn_out = self.inception_4d_double_3x3_1_bn(inception_4d_double_3x3_1_out)\n        inception_4d_relu_double_3x3_1_out = self.inception_4d_relu_double_3x3_1(inception_4d_double_3x3_1_bn_out)\n        inception_4d_double_3x3_2_out = self.inception_4d_double_3x3_2(inception_4d_relu_double_3x3_1_out)\n        inception_4d_double_3x3_2_bn_out = self.inception_4d_double_3x3_2_bn(inception_4d_double_3x3_2_out)\n        inception_4d_relu_double_3x3_2_out = self.inception_4d_relu_double_3x3_2(inception_4d_double_3x3_2_bn_out)\n        inception_4d_pool_out = self.inception_4d_pool(inception_4c_output_out)\n        inception_4d_pool_proj_out = self.inception_4d_pool_proj(inception_4d_pool_out)\n        inception_4d_pool_proj_bn_out = self.inception_4d_pool_proj_bn(inception_4d_pool_proj_out)\n        inception_4d_relu_pool_proj_out = self.inception_4d_relu_pool_proj(inception_4d_pool_proj_bn_out)\n        inception_4d_output_out = torch.cat([inception_4d_relu_1x1_out,inception_4d_relu_3x3_out,inception_4d_relu_double_3x3_2_out,inception_4d_relu_pool_proj_out], 1)\n        inception_4e_3x3_reduce_out = self.inception_4e_3x3_reduce(inception_4d_output_out)\n        inception_4e_3x3_reduce_bn_out = self.inception_4e_3x3_reduce_bn(inception_4e_3x3_reduce_out)\n        inception_4e_relu_3x3_reduce_out = self.inception_4e_relu_3x3_reduce(inception_4e_3x3_reduce_bn_out)\n        inception_4e_3x3_out = self.inception_4e_3x3(inception_4e_relu_3x3_reduce_out)\n        inception_4e_3x3_bn_out = self.inception_4e_3x3_bn(inception_4e_3x3_out)\n        inception_4e_relu_3x3_out = self.inception_4e_relu_3x3(inception_4e_3x3_bn_out)\n        inception_4e_double_3x3_reduce_out = self.inception_4e_double_3x3_reduce(inception_4d_output_out)\n        inception_4e_double_3x3_reduce_bn_out = self.inception_4e_double_3x3_reduce_bn(inception_4e_double_3x3_reduce_out)\n        inception_4e_relu_double_3x3_reduce_out = self.inception_4e_relu_double_3x3_reduce(inception_4e_double_3x3_reduce_bn_out)\n        inception_4e_double_3x3_1_out = self.inception_4e_double_3x3_1(inception_4e_relu_double_3x3_reduce_out)\n        inception_4e_double_3x3_1_bn_out = self.inception_4e_double_3x3_1_bn(inception_4e_double_3x3_1_out)\n        inception_4e_relu_double_3x3_1_out = self.inception_4e_relu_double_3x3_1(inception_4e_double_3x3_1_bn_out)\n        inception_4e_double_3x3_2_out = self.inception_4e_double_3x3_2(inception_4e_relu_double_3x3_1_out)\n        inception_4e_double_3x3_2_bn_out = self.inception_4e_double_3x3_2_bn(inception_4e_double_3x3_2_out)\n        inception_4e_relu_double_3x3_2_out = self.inception_4e_relu_double_3x3_2(inception_4e_double_3x3_2_bn_out)\n        inception_4e_pool_out = self.inception_4e_pool(inception_4d_output_out)\n        inception_4e_output_out = torch.cat([inception_4e_relu_3x3_out,inception_4e_relu_double_3x3_2_out,inception_4e_pool_out], 1)\n        inception_5a_1x1_out = self.inception_5a_1x1(inception_4e_output_out)\n        inception_5a_1x1_bn_out = self.inception_5a_1x1_bn(inception_5a_1x1_out)\n        inception_5a_relu_1x1_out = self.inception_5a_relu_1x1(inception_5a_1x1_bn_out)\n        inception_5a_3x3_reduce_out = self.inception_5a_3x3_reduce(inception_4e_output_out)\n        inception_5a_3x3_reduce_bn_out = self.inception_5a_3x3_reduce_bn(inception_5a_3x3_reduce_out)\n        inception_5a_relu_3x3_reduce_out = self.inception_5a_relu_3x3_reduce(inception_5a_3x3_reduce_bn_out)\n        inception_5a_3x3_out = self.inception_5a_3x3(inception_5a_relu_3x3_reduce_out)\n        inception_5a_3x3_bn_out = self.inception_5a_3x3_bn(inception_5a_3x3_out)\n        inception_5a_relu_3x3_out = self.inception_5a_relu_3x3(inception_5a_3x3_bn_out)\n        inception_5a_double_3x3_reduce_out = self.inception_5a_double_3x3_reduce(inception_4e_output_out)\n        inception_5a_double_3x3_reduce_bn_out = self.inception_5a_double_3x3_reduce_bn(inception_5a_double_3x3_reduce_out)\n        inception_5a_relu_double_3x3_reduce_out = self.inception_5a_relu_double_3x3_reduce(inception_5a_double_3x3_reduce_bn_out)\n        inception_5a_double_3x3_1_out = self.inception_5a_double_3x3_1(inception_5a_relu_double_3x3_reduce_out)\n        inception_5a_double_3x3_1_bn_out = self.inception_5a_double_3x3_1_bn(inception_5a_double_3x3_1_out)\n        inception_5a_relu_double_3x3_1_out = self.inception_5a_relu_double_3x3_1(inception_5a_double_3x3_1_bn_out)\n        inception_5a_double_3x3_2_out = self.inception_5a_double_3x3_2(inception_5a_relu_double_3x3_1_out)\n        inception_5a_double_3x3_2_bn_out = self.inception_5a_double_3x3_2_bn(inception_5a_double_3x3_2_out)\n        inception_5a_relu_double_3x3_2_out = self.inception_5a_relu_double_3x3_2(inception_5a_double_3x3_2_bn_out)\n        inception_5a_pool_out = self.inception_5a_pool(inception_4e_output_out)\n        inception_5a_pool_proj_out = self.inception_5a_pool_proj(inception_5a_pool_out)\n        inception_5a_pool_proj_bn_out = self.inception_5a_pool_proj_bn(inception_5a_pool_proj_out)\n        inception_5a_relu_pool_proj_out = self.inception_5a_relu_pool_proj(inception_5a_pool_proj_bn_out)\n        inception_5a_output_out = torch.cat([inception_5a_relu_1x1_out,inception_5a_relu_3x3_out,inception_5a_relu_double_3x3_2_out,inception_5a_relu_pool_proj_out], 1)\n        inception_5b_1x1_out = self.inception_5b_1x1(inception_5a_output_out)\n        inception_5b_1x1_bn_out = self.inception_5b_1x1_bn(inception_5b_1x1_out)\n        inception_5b_relu_1x1_out = self.inception_5b_relu_1x1(inception_5b_1x1_bn_out)\n        inception_5b_3x3_reduce_out = self.inception_5b_3x3_reduce(inception_5a_output_out)\n        inception_5b_3x3_reduce_bn_out = self.inception_5b_3x3_reduce_bn(inception_5b_3x3_reduce_out)\n        inception_5b_relu_3x3_reduce_out = self.inception_5b_relu_3x3_reduce(inception_5b_3x3_reduce_bn_out)\n        inception_5b_3x3_out = self.inception_5b_3x3(inception_5b_relu_3x3_reduce_out)\n        inception_5b_3x3_bn_out = self.inception_5b_3x3_bn(inception_5b_3x3_out)\n        inception_5b_relu_3x3_out = self.inception_5b_relu_3x3(inception_5b_3x3_bn_out)\n        inception_5b_double_3x3_reduce_out = self.inception_5b_double_3x3_reduce(inception_5a_output_out)\n        inception_5b_double_3x3_reduce_bn_out = self.inception_5b_double_3x3_reduce_bn(inception_5b_double_3x3_reduce_out)\n        inception_5b_relu_double_3x3_reduce_out = self.inception_5b_relu_double_3x3_reduce(inception_5b_double_3x3_reduce_bn_out)\n        inception_5b_double_3x3_1_out = self.inception_5b_double_3x3_1(inception_5b_relu_double_3x3_reduce_out)\n        inception_5b_double_3x3_1_bn_out = self.inception_5b_double_3x3_1_bn(inception_5b_double_3x3_1_out)\n        inception_5b_relu_double_3x3_1_out = self.inception_5b_relu_double_3x3_1(inception_5b_double_3x3_1_bn_out)\n        inception_5b_double_3x3_2_out = self.inception_5b_double_3x3_2(inception_5b_relu_double_3x3_1_out)\n        inception_5b_double_3x3_2_bn_out = self.inception_5b_double_3x3_2_bn(inception_5b_double_3x3_2_out)\n        inception_5b_relu_double_3x3_2_out = self.inception_5b_relu_double_3x3_2(inception_5b_double_3x3_2_bn_out)\n        inception_5b_pool_out = self.inception_5b_pool(inception_5a_output_out)\n        inception_5b_pool_proj_out = self.inception_5b_pool_proj(inception_5b_pool_out)\n        inception_5b_pool_proj_bn_out = self.inception_5b_pool_proj_bn(inception_5b_pool_proj_out)\n        inception_5b_relu_pool_proj_out = self.inception_5b_relu_pool_proj(inception_5b_pool_proj_bn_out)\n        inception_5b_output_out = torch.cat([inception_5b_relu_1x1_out,inception_5b_relu_3x3_out,inception_5b_relu_double_3x3_2_out,inception_5b_relu_pool_proj_out], 1)\n        return inception_5b_output_out\n\n    def logits(self, features):\n        adaptiveAvgPoolWidth = features.shape[2]\n        x = F.avg_pool2d(features, kernel_size=adaptiveAvgPoolWidth)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\ndef bninception(num_classes=1000, pretrained=\'imagenet\'):\n    r""""""BNInception model architecture from <https://arxiv.org/pdf/1502.03167.pdf>`_ paper.\n    """"""\n    model = BNInception(num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'bninception\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    return model\n\n\nif __name__ == \'__main__\':\n\n    model = bninception()\n'"
pretrainedmodels/models/cafferesnet.py,3,"b'from __future__ import print_function, division, absolute_import\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\npretrained_settings = {\n    \'cafferesnet101\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/cafferesnet101-9d633cc0.pth\',\n            \'input_space\': \'BGR\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 255],\n            \'mean\': [102.9801, 115.9465, 122.7717],\n            \'std\': [1, 1, 1],\n            \'num_classes\': 1000\n        }\n    }\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n  ""3x3 convolution with padding""\n  return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n           padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n  expansion = 1\n\n  def __init__(self, inplanes, planes, stride=1, downsample=None):\n    super(BasicBlock, self).__init__()\n    self.conv1 = conv3x3(inplanes, planes, stride)\n    self.bn1 = nn.BatchNorm2d(planes)\n    self.relu = nn.ReLU(inplace=True)\n    self.conv2 = conv3x3(planes, planes)\n    self.bn2 = nn.BatchNorm2d(planes)\n    self.downsample = downsample\n    self.stride = stride\n\n  def forward(self, x):\n    residual = x\n\n    out = self.conv1(x)\n    out = self.bn1(out)\n    out = self.relu(out)\n\n    out = self.conv2(out)\n    out = self.bn2(out)\n\n    if self.downsample is not None:\n      residual = self.downsample(x)\n\n    out += residual\n    out = self.relu(out)\n\n    return out\n\n\nclass Bottleneck(nn.Module):\n  expansion = 4\n\n  def __init__(self, inplanes, planes, stride=1, downsample=None):\n    super(Bottleneck, self).__init__()\n    self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False) # change\n    self.bn1 = nn.BatchNorm2d(planes)\n    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, # change\n                 padding=1, bias=False)\n    self.bn2 = nn.BatchNorm2d(planes)\n    self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n    self.bn3 = nn.BatchNorm2d(planes * 4)\n    self.relu = nn.ReLU(inplace=True)\n    self.downsample = downsample\n    self.stride = stride\n\n  def forward(self, x):\n    residual = x\n\n    out = self.conv1(x)\n    out = self.bn1(out)\n    out = self.relu(out)\n\n    out = self.conv2(out)\n    out = self.bn2(out)\n    out = self.relu(out)\n\n    out = self.conv3(out)\n    out = self.bn3(out)\n\n    if self.downsample is not None:\n      residual = self.downsample(x)\n\n    out += residual\n    out = self.relu(out)\n\n    return out\n\n\nclass ResNet(nn.Module):\n\n  def __init__(self, block, layers, num_classes=1000):\n    self.inplanes = 64\n    super(ResNet, self).__init__()\n    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                 bias=False)\n    self.bn1 = nn.BatchNorm2d(64)\n    self.relu = nn.ReLU(inplace=True)\n    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True) # change\n    self.layer1 = self._make_layer(block, 64, layers[0])\n    self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n    self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n    self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n    # it is slightly better whereas slower to set stride = 1\n    # self.layer4 = self._make_layer(block, 512, layers[3], stride=1)\n    self.avgpool = nn.AvgPool2d(7)\n    self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    for m in self.modules():\n      if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n      elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n\n  def _make_layer(self, block, planes, blocks, stride=1):\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n      downsample = nn.Sequential(\n        nn.Conv2d(self.inplanes, planes * block.expansion,\n              kernel_size=1, stride=stride, bias=False),\n        nn.BatchNorm2d(planes * block.expansion),\n      )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n      layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n  def features(self, x):\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.relu(x)\n    x = self.maxpool(x)\n\n    x = self.layer1(x)\n    x = self.layer2(x)\n    x = self.layer3(x)\n    x = self.layer4(x)\n    return x\n\n  def logits(self, x):\n    x = self.avgpool(x)\n    x = x.view(x.size(0), -1)\n    x = self.last_linear(x)\n    return x\n\n  def forward(self, x):\n    x = self.features(x)\n    x = self.logits(x)\n    return x\n\n\ndef cafferesnet101(num_classes=1000, pretrained=\'imagenet\'):\n    """"""Constructs a ResNet-101 model.\n    Args:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'cafferesnet101\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    return model'"
pretrainedmodels/models/dpn.py,10,"b'"""""" PyTorch implementation of DualPathNetworks\nPorted to PyTorch by [Ross Wightman](https://github.com/rwightman/pytorch-dpn-pretrained)\n\nBased on original MXNet implementation https://github.com/cypw/DPNs with\nmany ideas from another PyTorch implementation https://github.com/oyam/pytorch-DPNs.\n\nThis implementation is compatible with the pretrained weights\nfrom cypw\'s MXNet implementation.\n""""""\nfrom __future__ import print_function, division, absolute_import\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom collections import OrderedDict\n\n__all__ = [\'DPN\', \'dpn68\', \'dpn68b\', \'dpn92\', \'dpn98\', \'dpn131\', \'dpn107\']\n\npretrained_settings = {\n    \'dpn68\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/dpn68-4af7d88d2.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [124 / 255, 117 / 255, 104 / 255],\n            \'std\': [1 / (.0167 * 255)] * 3,\n            \'num_classes\': 1000\n        }\n    },\n    \'dpn68b\': {\n        \'imagenet+5k\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/dpn68b_extra-363ab9c19.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [124 / 255, 117 / 255, 104 / 255],\n            \'std\': [1 / (.0167 * 255)] * 3,\n            \'num_classes\': 1000\n        }\n    },\n    \'dpn92\': {\n        # \'imagenet\': {\n        #     \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/dpn68-66bebafa7.pth\',\n        #     \'input_space\': \'RGB\',\n        #     \'input_size\': [3, 224, 224],\n        #     \'input_range\': [0, 1],\n        #     \'mean\': [124 / 255, 117 / 255, 104 / 255],\n        #     \'std\': [1 / (.0167 * 255)] * 3,\n        #     \'num_classes\': 1000\n        # },\n        \'imagenet+5k\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/dpn92_extra-fda993c95.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [124 / 255, 117 / 255, 104 / 255],\n            \'std\': [1 / (.0167 * 255)] * 3,\n            \'num_classes\': 1000\n        }\n    },\n    \'dpn98\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/dpn98-722954780.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [124 / 255, 117 / 255, 104 / 255],\n            \'std\': [1 / (.0167 * 255)] * 3,\n            \'num_classes\': 1000\n        }\n    },\n    \'dpn131\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/dpn131-7af84be88.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [124 / 255, 117 / 255, 104 / 255],\n            \'std\': [1 / (.0167 * 255)] * 3,\n            \'num_classes\': 1000\n        }\n    },\n    \'dpn107\': {\n        \'imagenet+5k\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/dpn107_extra-b7f9f4cc9.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [124 / 255, 117 / 255, 104 / 255],\n            \'std\': [1 / (.0167 * 255)] * 3,\n            \'num_classes\': 1000\n        }\n    }\n}\n\ndef dpn68(num_classes=1000, pretrained=\'imagenet\'):\n    model = DPN(\n        small=True, num_init_features=10, k_r=128, groups=32,\n        k_sec=(3, 4, 12, 3), inc_sec=(16, 32, 32, 64),\n        num_classes=num_classes, test_time_pool=True)\n    if pretrained:\n        settings = pretrained_settings[\'dpn68\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    return model\n\ndef dpn68b(num_classes=1000, pretrained=\'imagenet+5k\'):\n    model = DPN(\n        small=True, num_init_features=10, k_r=128, groups=32,\n        b=True, k_sec=(3, 4, 12, 3), inc_sec=(16, 32, 32, 64),\n        num_classes=num_classes, test_time_pool=True)\n    if pretrained:\n        settings = pretrained_settings[\'dpn68b\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    return model\n\ndef dpn92(num_classes=1000, pretrained=\'imagenet+5k\'):\n    model = DPN(\n        num_init_features=64, k_r=96, groups=32,\n        k_sec=(3, 4, 20, 3), inc_sec=(16, 32, 24, 128),\n        num_classes=num_classes, test_time_pool=True)\n    if pretrained:\n        settings = pretrained_settings[\'dpn92\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    return model\n\ndef dpn98(num_classes=1000, pretrained=\'imagenet\'):\n    model = DPN(\n        num_init_features=96, k_r=160, groups=40,\n        k_sec=(3, 6, 20, 3), inc_sec=(16, 32, 32, 128),\n        num_classes=num_classes, test_time_pool=True)\n    if pretrained:\n        settings = pretrained_settings[\'dpn98\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    return model\n\ndef dpn131(num_classes=1000, pretrained=\'imagenet\'):\n    model = DPN(\n        num_init_features=128, k_r=160, groups=40,\n        k_sec=(4, 8, 28, 3), inc_sec=(16, 32, 32, 128),\n        num_classes=num_classes, test_time_pool=True)\n    if pretrained:\n        settings = pretrained_settings[\'dpn131\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    return model\n\ndef dpn107(num_classes=1000, pretrained=\'imagenet+5k\'):\n    model = DPN(\n        num_init_features=128, k_r=200, groups=50,\n        k_sec=(4, 8, 20, 3), inc_sec=(20, 64, 64, 128),\n        num_classes=num_classes, test_time_pool=True)\n    if pretrained:\n        settings = pretrained_settings[\'dpn107\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    return model\n\n\nclass CatBnAct(nn.Module):\n    def __init__(self, in_chs, activation_fn=nn.ReLU(inplace=True)):\n        super(CatBnAct, self).__init__()\n        self.bn = nn.BatchNorm2d(in_chs, eps=0.001)\n        self.act = activation_fn\n\n    def forward(self, x):\n        x = torch.cat(x, dim=1) if isinstance(x, tuple) else x\n        return self.act(self.bn(x))\n\n\nclass BnActConv2d(nn.Module):\n    def __init__(self, in_chs, out_chs, kernel_size, stride,\n                 padding=0, groups=1, activation_fn=nn.ReLU(inplace=True)):\n        super(BnActConv2d, self).__init__()\n        self.bn = nn.BatchNorm2d(in_chs, eps=0.001)\n        self.act = activation_fn\n        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size, stride, padding, groups=groups, bias=False)\n\n    def forward(self, x):\n        return self.conv(self.act(self.bn(x)))\n\n\nclass InputBlock(nn.Module):\n    def __init__(self, num_init_features, kernel_size=7,\n                 padding=3, activation_fn=nn.ReLU(inplace=True)):\n        super(InputBlock, self).__init__()\n        self.conv = nn.Conv2d(\n            3, num_init_features, kernel_size=kernel_size, stride=2, padding=padding, bias=False)\n        self.bn = nn.BatchNorm2d(num_init_features, eps=0.001)\n        self.act = activation_fn\n        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.act(x)\n        x = self.pool(x)\n        return x\n\n\nclass DualPathBlock(nn.Module):\n    def __init__(\n            self, in_chs, num_1x1_a, num_3x3_b, num_1x1_c, inc, groups, block_type=\'normal\', b=False):\n        super(DualPathBlock, self).__init__()\n        self.num_1x1_c = num_1x1_c\n        self.inc = inc\n        self.b = b\n        if block_type is \'proj\':\n            self.key_stride = 1\n            self.has_proj = True\n        elif block_type is \'down\':\n            self.key_stride = 2\n            self.has_proj = True\n        else:\n            assert block_type is \'normal\'\n            self.key_stride = 1\n            self.has_proj = False\n\n        if self.has_proj:\n            # Using different member names here to allow easier parameter key matching for conversion\n            if self.key_stride == 2:\n                self.c1x1_w_s2 = BnActConv2d(\n                    in_chs=in_chs, out_chs=num_1x1_c + 2 * inc, kernel_size=1, stride=2)\n            else:\n                self.c1x1_w_s1 = BnActConv2d(\n                    in_chs=in_chs, out_chs=num_1x1_c + 2 * inc, kernel_size=1, stride=1)\n        self.c1x1_a = BnActConv2d(in_chs=in_chs, out_chs=num_1x1_a, kernel_size=1, stride=1)\n        self.c3x3_b = BnActConv2d(\n            in_chs=num_1x1_a, out_chs=num_3x3_b, kernel_size=3,\n            stride=self.key_stride, padding=1, groups=groups)\n        if b:\n            self.c1x1_c = CatBnAct(in_chs=num_3x3_b)\n            self.c1x1_c1 = nn.Conv2d(num_3x3_b, num_1x1_c, kernel_size=1, bias=False)\n            self.c1x1_c2 = nn.Conv2d(num_3x3_b, inc, kernel_size=1, bias=False)\n        else:\n            self.c1x1_c = BnActConv2d(in_chs=num_3x3_b, out_chs=num_1x1_c + inc, kernel_size=1, stride=1)\n\n    def forward(self, x):\n        x_in = torch.cat(x, dim=1) if isinstance(x, tuple) else x\n        if self.has_proj:\n            if self.key_stride == 2:\n                x_s = self.c1x1_w_s2(x_in)\n            else:\n                x_s = self.c1x1_w_s1(x_in)\n            x_s1 = x_s[:, :self.num_1x1_c, :, :]\n            x_s2 = x_s[:, self.num_1x1_c:, :, :]\n        else:\n            x_s1 = x[0]\n            x_s2 = x[1]\n        x_in = self.c1x1_a(x_in)\n        x_in = self.c3x3_b(x_in)\n        if self.b:\n            x_in = self.c1x1_c(x_in)\n            out1 = self.c1x1_c1(x_in)\n            out2 = self.c1x1_c2(x_in)\n        else:\n            x_in = self.c1x1_c(x_in)\n            out1 = x_in[:, :self.num_1x1_c, :, :]\n            out2 = x_in[:, self.num_1x1_c:, :, :]\n        resid = x_s1 + out1\n        dense = torch.cat([x_s2, out2], dim=1)\n        return resid, dense\n\n\nclass DPN(nn.Module):\n    def __init__(self, small=False, num_init_features=64, k_r=96, groups=32,\n                 b=False, k_sec=(3, 4, 20, 3), inc_sec=(16, 32, 24, 128),\n                 num_classes=1000, test_time_pool=False):\n        super(DPN, self).__init__()\n        self.test_time_pool = test_time_pool\n        self.b = b\n        bw_factor = 1 if small else 4\n\n        blocks = OrderedDict()\n\n        # conv1\n        if small:\n            blocks[\'conv1_1\'] = InputBlock(num_init_features, kernel_size=3, padding=1)\n        else:\n            blocks[\'conv1_1\'] = InputBlock(num_init_features, kernel_size=7, padding=3)\n\n        # conv2\n        bw = 64 * bw_factor\n        inc = inc_sec[0]\n        r = (k_r * bw) // (64 * bw_factor)\n        blocks[\'conv2_1\'] = DualPathBlock(num_init_features, r, r, bw, inc, groups, \'proj\', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[0] + 1):\n            blocks[\'conv2_\' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'normal\', b)\n            in_chs += inc\n\n        # conv3\n        bw = 128 * bw_factor\n        inc = inc_sec[1]\n        r = (k_r * bw) // (64 * bw_factor)\n        blocks[\'conv3_1\'] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'down\', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[1] + 1):\n            blocks[\'conv3_\' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'normal\', b)\n            in_chs += inc\n\n        # conv4\n        bw = 256 * bw_factor\n        inc = inc_sec[2]\n        r = (k_r * bw) // (64 * bw_factor)\n        blocks[\'conv4_1\'] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'down\', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[2] + 1):\n            blocks[\'conv4_\' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'normal\', b)\n            in_chs += inc\n\n        # conv5\n        bw = 512 * bw_factor\n        inc = inc_sec[3]\n        r = (k_r * bw) // (64 * bw_factor)\n        blocks[\'conv5_1\'] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'down\', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[3] + 1):\n            blocks[\'conv5_\' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'normal\', b)\n            in_chs += inc\n        blocks[\'conv5_bn_ac\'] = CatBnAct(in_chs)\n\n        self.features = nn.Sequential(blocks)\n\n        # Using 1x1 conv for the FC layer to allow the extra pooling scheme\n        self.last_linear = nn.Conv2d(in_chs, num_classes, kernel_size=1, bias=True)\n\n    def logits(self, features):\n        if not self.training and self.test_time_pool:\n            x = F.avg_pool2d(features, kernel_size=7, stride=1)\n            out = self.last_linear(x)\n            # The extra test time pool should be pooling an img_size//32 - 6 size patch\n            out = adaptive_avgmax_pool2d(out, pool_type=\'avgmax\')\n        else:\n            x = adaptive_avgmax_pool2d(features, pool_type=\'avg\')\n            out = self.last_linear(x)\n        return out.view(out.size(0), -1)\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n"""""" PyTorch selectable adaptive pooling\nAdaptive pooling with the ability to select the type of pooling from:\n    * \'avg\' - Average pooling\n    * \'max\' - Max pooling\n    * \'avgmax\' - Sum of average and max pooling re-scaled by 0.5\n    * \'avgmaxc\' - Concatenation of average and max pooling along feature dim, doubles feature dim\n\nBoth a functional and a nn.Module version of the pooling is provided.\n\nAuthor: Ross Wightman (rwightman)\n""""""\n\ndef pooling_factor(pool_type=\'avg\'):\n    return 2 if pool_type == \'avgmaxc\' else 1\n\n\ndef adaptive_avgmax_pool2d(x, pool_type=\'avg\', padding=0, count_include_pad=False):\n    """"""Selectable global pooling function with dynamic input kernel size\n    """"""\n    if pool_type == \'avgmaxc\':\n        x = torch.cat([\n            F.avg_pool2d(\n                x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad),\n            F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n        ], dim=1)\n    elif pool_type == \'avgmax\':\n        x_avg = F.avg_pool2d(\n                x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad)\n        x_max = F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n        x = 0.5 * (x_avg + x_max)\n    elif pool_type == \'max\':\n        x = F.max_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=padding)\n    else:\n        if pool_type != \'avg\':\n            print(\'Invalid pool type %s specified. Defaulting to average pooling.\' % pool_type)\n        x = F.avg_pool2d(\n            x, kernel_size=(x.size(2), x.size(3)), padding=padding, count_include_pad=count_include_pad)\n    return x\n\n\nclass AdaptiveAvgMaxPool2d(torch.nn.Module):\n    """"""Selectable global pooling layer with dynamic input kernel size\n    """"""\n    def __init__(self, output_size=1, pool_type=\'avg\'):\n        super(AdaptiveAvgMaxPool2d, self).__init__()\n        self.output_size = output_size\n        self.pool_type = pool_type\n        if pool_type == \'avgmaxc\' or pool_type == \'avgmax\':\n            self.pool = nn.ModuleList([nn.AdaptiveAvgPool2d(output_size), nn.AdaptiveMaxPool2d(output_size)])\n        elif pool_type == \'max\':\n            self.pool = nn.AdaptiveMaxPool2d(output_size)\n        else:\n            if pool_type != \'avg\':\n                print(\'Invalid pool type %s specified. Defaulting to average pooling.\' % pool_type)\n            self.pool = nn.AdaptiveAvgPool2d(output_size)\n\n    def forward(self, x):\n        if self.pool_type == \'avgmaxc\':\n            x = torch.cat([p(x) for p in self.pool], dim=1)\n        elif self.pool_type == \'avgmax\':\n            x = 0.5 * torch.sum(torch.stack([p(x) for p in self.pool]), 0).squeeze(dim=0)\n        else:\n            x = self.pool(x)\n        return x\n\n    def factor(self):\n        return pooling_factor(self.pool_type)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \' (\' \\\n               + \'output_size=\' + str(self.output_size) \\\n               + \', pool_type=\' + self.pool_type + \')\''"
pretrainedmodels/models/fbresnet.py,3,"b'from __future__ import print_function, division, absolute_import\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport torch.utils.model_zoo as model_zoo\n\n\n__all__ = [\'FBResNet\',\n           #\'fbresnet18\', \'fbresnet34\', \'fbresnet50\', \'fbresnet101\',\n           \'fbresnet152\']\n\npretrained_settings = {\n    \'fbresnet152\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/fbresnet152-2e20f6b4.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    }\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=True)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=True)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=True)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=True)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass FBResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        # Special attributs\n        self.input_space = None\n        self.input_size = (299, 299, 3)\n        self.mean = None\n        self.std = None\n        super(FBResNet, self).__init__()\n        # Modules\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                                bias=True)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=True),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def features(self, input):\n        x = self.conv1(input)\n        self.conv1_input = x.clone()\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, features):\n        adaptiveAvgPoolWidth = features.shape[2]\n        x = F.avg_pool2d(features, kernel_size=adaptiveAvgPoolWidth)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\ndef fbresnet18(num_classes=1000):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = FBResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n    return model\n\n\ndef fbresnet34(num_classes=1000):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = FBResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n    return model\n\n\ndef fbresnet50(num_classes=1000):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = FBResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes)\n    return model\n\n\ndef fbresnet101(num_classes=1000):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = FBResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes)\n    return model\n\n\ndef fbresnet152(num_classes=1000, pretrained=\'imagenet\'):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = FBResNet(Bottleneck, [3, 8, 36, 3], num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'fbresnet152\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    return model\n\n\n'"
pretrainedmodels/models/inceptionresnetv2.py,8,"b'from __future__ import print_function, division, absolute_import\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport os\nimport sys\n\n__all__ = [\'InceptionResNetV2\', \'inceptionresnetv2\']\n\npretrained_settings = {\n    \'inceptionresnetv2\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/inceptionresnetv2-520b38e4.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 299, 299],\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1000\n        },\n        \'imagenet+background\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/inceptionresnetv2-520b38e4.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 299, 299],\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1001\n        }\n    }\n}\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes,\n                              kernel_size=kernel_size, stride=stride,\n                              padding=padding, bias=False) # verify bias false\n        self.bn = nn.BatchNorm2d(out_planes,\n                                 eps=0.001, # value found in tensorflow\n                                 momentum=0.1, # default pytorch value\n                                 affine=True)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass Mixed_5b(nn.Module):\n\n    def __init__(self):\n        super(Mixed_5b, self).__init__()\n\n        self.branch0 = BasicConv2d(192, 96, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(192, 48, kernel_size=1, stride=1),\n            BasicConv2d(48, 64, kernel_size=5, stride=1, padding=2)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(192, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(192, 64, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Block35(nn.Module):\n\n    def __init__(self, scale=1.0):\n        super(Block35, self).__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(320, 32, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(320, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(320, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 48, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(48, 64, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.conv2d = nn.Conv2d(128, 320, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\n\nclass Mixed_6a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_6a, self).__init__()\n\n        self.branch0 = BasicConv2d(320, 384, kernel_size=3, stride=2)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(320, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Block17(nn.Module):\n\n    def __init__(self, scale=1.0):\n        super(Block17, self).__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(1088, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1088, 128, kernel_size=1, stride=1),\n            BasicConv2d(128, 160, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(160, 192, kernel_size=(7,1), stride=1, padding=(3,0))\n        )\n\n        self.conv2d = nn.Conv2d(384, 1088, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\n\nclass Mixed_7a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_7a, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(288, 320, kernel_size=3, stride=2)\n        )\n\n        self.branch3 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Block8(nn.Module):\n\n    def __init__(self, scale=1.0, noReLU=False):\n        super(Block8, self).__init__()\n\n        self.scale = scale\n        self.noReLU = noReLU\n\n        self.branch0 = BasicConv2d(2080, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(2080, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=(1,3), stride=1, padding=(0,1)),\n            BasicConv2d(224, 256, kernel_size=(3,1), stride=1, padding=(1,0))\n        )\n\n        self.conv2d = nn.Conv2d(448, 2080, kernel_size=1, stride=1)\n        if not self.noReLU:\n            self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        if not self.noReLU:\n            out = self.relu(out)\n        return out\n\n\nclass InceptionResNetV2(nn.Module):\n\n    def __init__(self, num_classes=1001):\n        super(InceptionResNetV2, self).__init__()\n        # Special attributs\n        self.input_space = None\n        self.input_size = (299, 299, 3)\n        self.mean = None\n        self.std = None\n        # Modules\n        self.conv2d_1a = BasicConv2d(3, 32, kernel_size=3, stride=2)\n        self.conv2d_2a = BasicConv2d(32, 32, kernel_size=3, stride=1)\n        self.conv2d_2b = BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.maxpool_3a = nn.MaxPool2d(3, stride=2)\n        self.conv2d_3b = BasicConv2d(64, 80, kernel_size=1, stride=1)\n        self.conv2d_4a = BasicConv2d(80, 192, kernel_size=3, stride=1)\n        self.maxpool_5a = nn.MaxPool2d(3, stride=2)\n        self.mixed_5b = Mixed_5b()\n        self.repeat = nn.Sequential(\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17)\n        )\n        self.mixed_6a = Mixed_6a()\n        self.repeat_1 = nn.Sequential(\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10)\n        )\n        self.mixed_7a = Mixed_7a()\n        self.repeat_2 = nn.Sequential(\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20)\n        )\n        self.block8 = Block8(noReLU=True)\n        self.conv2d_7b = BasicConv2d(2080, 1536, kernel_size=1, stride=1)\n        self.avgpool_1a = nn.AvgPool2d(8, count_include_pad=False)\n        self.last_linear = nn.Linear(1536, num_classes)\n\n    def features(self, input):\n        x = self.conv2d_1a(input)\n        x = self.conv2d_2a(x)\n        x = self.conv2d_2b(x)\n        x = self.maxpool_3a(x)\n        x = self.conv2d_3b(x)\n        x = self.conv2d_4a(x)\n        x = self.maxpool_5a(x)\n        x = self.mixed_5b(x)\n        x = self.repeat(x)\n        x = self.mixed_6a(x)\n        x = self.repeat_1(x)\n        x = self.mixed_7a(x)\n        x = self.repeat_2(x)\n        x = self.block8(x)\n        x = self.conv2d_7b(x)\n        return x\n\n    def logits(self, features):\n        x = self.avgpool_1a(features)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\ndef inceptionresnetv2(num_classes=1000, pretrained=\'imagenet\'):\n    r""""""InceptionResNetV2 model architecture from the\n    `""InceptionV4, Inception-ResNet..."" <https://arxiv.org/abs/1602.07261>`_ paper.\n    """"""\n    if pretrained:\n        settings = pretrained_settings[\'inceptionresnetv2\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n\n        # both \'imagenet\'&\'imagenet+background\' are loaded from same parameters\n        model = InceptionResNetV2(num_classes=1001)\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n\n        if pretrained == \'imagenet\':\n            new_last_linear = nn.Linear(1536, 1000)\n            new_last_linear.weight.data = model.last_linear.weight.data[1:]\n            new_last_linear.bias.data = model.last_linear.bias.data[1:]\n            model.last_linear = new_last_linear\n\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    else:\n        model = InceptionResNetV2(num_classes=num_classes)\n    return model\n\n\'\'\'\nTEST\nRun this code with:\n```\ncd $HOME/pretrained-models.pytorch\npython -m pretrainedmodels.inceptionresnetv2\n```\n\'\'\'\nif __name__ == \'__main__\':\n\n    assert inceptionresnetv2(num_classes=10, pretrained=None)\n    print(\'success\')\n    assert inceptionresnetv2(num_classes=1000, pretrained=\'imagenet\')\n    print(\'success\')\n    assert inceptionresnetv2(num_classes=1001, pretrained=\'imagenet+background\')\n    print(\'success\')\n\n    # fail\n    assert inceptionresnetv2(num_classes=1001, pretrained=\'imagenet\')'"
pretrainedmodels/models/inceptionv4.py,13,"b'from __future__ import print_function, division, absolute_import\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nimport os\nimport sys\n\n__all__ = [\'InceptionV4\', \'inceptionv4\']\n\npretrained_settings = {\n    \'inceptionv4\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 299, 299],\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1000\n        },\n        \'imagenet+background\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 299, 299],\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1001\n        }\n    }\n}\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes,\n                              kernel_size=kernel_size, stride=stride,\n                              padding=padding, bias=False) # verify bias false\n        self.bn = nn.BatchNorm2d(out_planes,\n                                 eps=0.001, # value found in tensorflow\n                                 momentum=0.1, # default pytorch value\n                                 affine=True)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass Mixed_3a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_3a, self).__init__()\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n        self.conv = BasicConv2d(64, 96, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        x0 = self.maxpool(x)\n        x1 = self.conv(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Mixed_4a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_4a, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 64, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(64, 64, kernel_size=(7,1), stride=1, padding=(3,0)),\n            BasicConv2d(64, 96, kernel_size=(3,3), stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Mixed_5a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_5a, self).__init__()\n        self.conv = BasicConv2d(192, 192, kernel_size=3, stride=2)\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.conv(x)\n        x1 = self.maxpool(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Inception_A(nn.Module):\n\n    def __init__(self):\n        super(Inception_A, self).__init__()\n        self.branch0 = BasicConv2d(384, 96, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(384, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(384, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(384, 96, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Reduction_A(nn.Module):\n\n    def __init__(self):\n        super(Reduction_A, self).__init__()\n        self.branch0 = BasicConv2d(384, 384, kernel_size=3, stride=2)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(384, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(224, 256, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Inception_B(nn.Module):\n\n    def __init__(self):\n        super(Inception_B, self).__init__()\n        self.branch0 = BasicConv2d(1024, 384, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(224, 256, kernel_size=(7,1), stride=1, padding=(3,0))\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 192, kernel_size=(7,1), stride=1, padding=(3,0)),\n            BasicConv2d(192, 224, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(224, 224, kernel_size=(7,1), stride=1, padding=(3,0)),\n            BasicConv2d(224, 256, kernel_size=(1,7), stride=1, padding=(0,3))\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(1024, 128, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Reduction_B(nn.Module):\n\n    def __init__(self):\n        super(Reduction_B, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 192, kernel_size=3, stride=2)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1024, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(256, 320, kernel_size=(7,1), stride=1, padding=(3,0)),\n            BasicConv2d(320, 320, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Inception_C(nn.Module):\n\n    def __init__(self):\n        super(Inception_C, self).__init__()\n\n        self.branch0 = BasicConv2d(1536, 256, kernel_size=1, stride=1)\n\n        self.branch1_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)\n        self.branch1_1a = BasicConv2d(384, 256, kernel_size=(1,3), stride=1, padding=(0,1))\n        self.branch1_1b = BasicConv2d(384, 256, kernel_size=(3,1), stride=1, padding=(1,0))\n\n        self.branch2_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)\n        self.branch2_1 = BasicConv2d(384, 448, kernel_size=(3,1), stride=1, padding=(1,0))\n        self.branch2_2 = BasicConv2d(448, 512, kernel_size=(1,3), stride=1, padding=(0,1))\n        self.branch2_3a = BasicConv2d(512, 256, kernel_size=(1,3), stride=1, padding=(0,1))\n        self.branch2_3b = BasicConv2d(512, 256, kernel_size=(3,1), stride=1, padding=(1,0))\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(1536, 256, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n\n        x1_0 = self.branch1_0(x)\n        x1_1a = self.branch1_1a(x1_0)\n        x1_1b = self.branch1_1b(x1_0)\n        x1 = torch.cat((x1_1a, x1_1b), 1)\n\n        x2_0 = self.branch2_0(x)\n        x2_1 = self.branch2_1(x2_0)\n        x2_2 = self.branch2_2(x2_1)\n        x2_3a = self.branch2_3a(x2_2)\n        x2_3b = self.branch2_3b(x2_2)\n        x2 = torch.cat((x2_3a, x2_3b), 1)\n\n        x3 = self.branch3(x)\n\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass InceptionV4(nn.Module):\n\n    def __init__(self, num_classes=1001):\n        super(InceptionV4, self).__init__()\n        # Special attributs\n        self.input_space = None\n        self.input_size = (299, 299, 3)\n        self.mean = None\n        self.std = None\n        # Modules\n        self.features = nn.Sequential(\n            BasicConv2d(3, 32, kernel_size=3, stride=2),\n            BasicConv2d(32, 32, kernel_size=3, stride=1),\n            BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            Mixed_3a(),\n            Mixed_4a(),\n            Mixed_5a(),\n            Inception_A(),\n            Inception_A(),\n            Inception_A(),\n            Inception_A(),\n            Reduction_A(), # Mixed_6a\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Reduction_B(), # Mixed_7a\n            Inception_C(),\n            Inception_C(),\n            Inception_C()\n        )\n        self.last_linear = nn.Linear(1536, num_classes)\n\n    def logits(self, features):\n        #Allows image of any size to be processed\n        adaptiveAvgPoolWidth = features.shape[2]\n        x = F.avg_pool2d(features, kernel_size=adaptiveAvgPoolWidth)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\ndef inceptionv4(num_classes=1000, pretrained=\'imagenet\'):\n    if pretrained:\n        settings = pretrained_settings[\'inceptionv4\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n\n        # both \'imagenet\'&\'imagenet+background\' are loaded from same parameters\n        model = InceptionV4(num_classes=1001)\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n\n        if pretrained == \'imagenet\':\n            new_last_linear = nn.Linear(1536, 1000)\n            new_last_linear.weight.data = model.last_linear.weight.data[1:]\n            new_last_linear.bias.data = model.last_linear.bias.data[1:]\n            model.last_linear = new_last_linear\n\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    else:\n        model = InceptionV4(num_classes=num_classes)\n    return model\n\n\n\'\'\'\nTEST\nRun this code with:\n```\ncd $HOME/pretrained-models.pytorch\npython -m pretrainedmodels.inceptionv4\n```\n\'\'\'\nif __name__ == \'__main__\':\n\n    assert inceptionv4(num_classes=10, pretrained=None)\n    print(\'success\')\n    assert inceptionv4(num_classes=1000, pretrained=\'imagenet\')\n    print(\'success\')\n    assert inceptionv4(num_classes=1001, pretrained=\'imagenet+background\')\n    print(\'success\')\n\n    # fail\n    assert inceptionv4(num_classes=1001, pretrained=\'imagenet\')\n'"
pretrainedmodels/models/nasnet.py,13,"b'from __future__ import print_function, division, absolute_import\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom torch.autograd import Variable\n\npretrained_settings = {\n    \'nasnetalarge\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 331, 331], # resize 354\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1000\n        },\n        \'imagenet+background\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 331, 331], # resize 354\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1001\n        }\n    }\n}\n\n\nclass MaxPoolPad(nn.Module):\n\n    def __init__(self):\n        super(MaxPoolPad, self).__init__()\n        self.pad = nn.ZeroPad2d((1, 0, 1, 0))\n        self.pool = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x):\n        x = self.pad(x)\n        x = self.pool(x)\n        x = x[:, :, 1:, 1:]\n        return x\n\n\nclass AvgPoolPad(nn.Module):\n\n    def __init__(self, stride=2, padding=1):\n        super(AvgPoolPad, self).__init__()\n        self.pad = nn.ZeroPad2d((1, 0, 1, 0))\n        self.pool = nn.AvgPool2d(3, stride=stride, padding=padding, count_include_pad=False)\n\n    def forward(self, x):\n        x = self.pad(x)\n        x = self.pool(x)\n        x = x[:, :, 1:, 1:]\n        return x\n\n\nclass SeparableConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, dw_kernel, dw_stride, dw_padding, bias=False):\n        super(SeparableConv2d, self).__init__()\n        self.depthwise_conv2d = nn.Conv2d(in_channels, in_channels, dw_kernel,\n                                          stride=dw_stride,\n                                          padding=dw_padding,\n                                          bias=bias,\n                                          groups=in_channels)\n        self.pointwise_conv2d = nn.Conv2d(in_channels, out_channels, 1, stride=1, bias=bias)\n\n    def forward(self, x):\n        x = self.depthwise_conv2d(x)\n        x = self.pointwise_conv2d(x)\n        return x\n\n\nclass BranchSeparables(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=False):\n        super(BranchSeparables, self).__init__()\n        self.relu = nn.ReLU()\n        self.separable_1 = SeparableConv2d(in_channels, in_channels, kernel_size, stride, padding, bias=bias)\n        self.bn_sep_1 = nn.BatchNorm2d(in_channels, eps=0.001, momentum=0.1, affine=True)\n        self.relu1 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(in_channels, out_channels, kernel_size, 1, padding, bias=bias)\n        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.separable_1(x)\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass BranchSeparablesStem(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=False):\n        super(BranchSeparablesStem, self).__init__()\n        self.relu = nn.ReLU()\n        self.separable_1 = SeparableConv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n        self.bn_sep_1 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n        self.relu1 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(out_channels, out_channels, kernel_size, 1, padding, bias=bias)\n        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.separable_1(x)\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass BranchSeparablesReduction(BranchSeparables):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, z_padding=1, bias=False):\n        BranchSeparables.__init__(self, in_channels, out_channels, kernel_size, stride, padding, bias)\n        self.padding = nn.ZeroPad2d((z_padding, 0, z_padding, 0))\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.padding(x)\n        x = self.separable_1(x)\n        x = x[:, :, 1:, 1:].contiguous()\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass CellStem0(nn.Module):\n    def __init__(self, stem_filters, num_filters=42):\n        super(CellStem0, self).__init__()\n        self.num_filters = num_filters\n        self.stem_filters = stem_filters\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(self.stem_filters, self.num_filters, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(self.num_filters, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparables(self.num_filters, self.num_filters, 5, 2, 2)\n        self.comb_iter_0_right = BranchSeparablesStem(self.stem_filters, self.num_filters, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_1_right = BranchSeparablesStem(self.stem_filters, self.num_filters, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n        self.comb_iter_2_right = BranchSeparablesStem(self.stem_filters, self.num_filters, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(self.num_filters, self.num_filters, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x):\n        x1 = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x1)\n        x_comb_iter_0_right = self.comb_iter_0_right(x)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x1)\n        x_comb_iter_1_right = self.comb_iter_1_right(x)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x1)\n        x_comb_iter_2_right = self.comb_iter_2_right(x)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x1)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass CellStem1(nn.Module):\n\n    def __init__(self, stem_filters, num_filters):\n        super(CellStem1, self).__init__()\n        self.num_filters = num_filters\n        self.stem_filters = stem_filters\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(2*self.num_filters, self.num_filters, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(self.num_filters, eps=0.001, momentum=0.1, affine=True))\n\n        self.relu = nn.ReLU()\n        self.path_1 = nn.Sequential()\n        self.path_1.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_1.add_module(\'conv\', nn.Conv2d(self.stem_filters, self.num_filters//2, 1, stride=1, bias=False))\n        self.path_2 = nn.ModuleList()\n        self.path_2.add_module(\'pad\', nn.ZeroPad2d((0, 1, 0, 1)))\n        self.path_2.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_2.add_module(\'conv\', nn.Conv2d(self.stem_filters, self.num_filters//2, 1, stride=1, bias=False))\n\n        self.final_path_bn = nn.BatchNorm2d(self.num_filters, eps=0.001, momentum=0.1, affine=True)\n\n        self.comb_iter_0_left = BranchSeparables(self.num_filters, self.num_filters, 5, 2, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(self.num_filters, self.num_filters, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_1_right = BranchSeparables(self.num_filters, self.num_filters, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n        self.comb_iter_2_right = BranchSeparables(self.num_filters, self.num_filters, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(self.num_filters, self.num_filters, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x_conv0, x_stem_0):\n        x_left = self.conv_1x1(x_stem_0)\n\n        x_relu = self.relu(x_conv0)\n        # path 1\n        x_path1 = self.path_1(x_relu)\n        # path 2\n        x_path2 = self.path_2.pad(x_relu)\n        x_path2 = x_path2[:, :, 1:, 1:]\n        x_path2 = self.path_2.avgpool(x_path2)\n        x_path2 = self.path_2.conv(x_path2)\n        # final path\n        x_right = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_left)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_right)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_right)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_left)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_left)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass FirstCell(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(FirstCell, self).__init__()\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.relu = nn.ReLU()\n        self.path_1 = nn.Sequential()\n        self.path_1.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.path_2 = nn.ModuleList()\n        self.path_2.add_module(\'pad\', nn.ZeroPad2d((0, 1, 0, 1)))\n        self.path_2.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_2.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n\n        self.final_path_bn = nn.BatchNorm2d(out_channels_left * 2, eps=0.001, momentum=0.1, affine=True)\n\n        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n        self.comb_iter_1_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n        self.comb_iter_1_right = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_3_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n    def forward(self, x, x_prev):\n        x_relu = self.relu(x_prev)\n        # path 1\n        x_path1 = self.path_1(x_relu)\n        # path 2\n        x_path2 = self.path_2.pad(x_relu)\n        x_path2 = x_path2[:, :, 1:, 1:]\n        x_path2 = self.path_2.avgpool(x_path2)\n        x_path2 = self.path_2.conv(x_path2)\n        # final path\n        x_left = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_left\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_right\n\n        x_out = torch.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass NormalCell(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(NormalCell, self).__init__()\n        self.conv_prev_1x1 = nn.Sequential()\n        self.conv_prev_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_prev_1x1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.conv_prev_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(out_channels_left, out_channels_left, 3, 1, 1, bias=False)\n\n        self.comb_iter_1_left = BranchSeparables(out_channels_left, out_channels_left, 5, 1, 2, bias=False)\n        self.comb_iter_1_right = BranchSeparables(out_channels_left, out_channels_left, 3, 1, 1, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_3_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_left\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_right\n\n        x_out = torch.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass ReductionCell0(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(ReductionCell0, self).__init__()\n        self.conv_prev_1x1 = nn.Sequential()\n        self.conv_prev_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_prev_1x1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.conv_prev_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparablesReduction(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = MaxPoolPad()\n        self.comb_iter_1_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = AvgPoolPad()\n        self.comb_iter_2_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparablesReduction(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = MaxPoolPad()\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass ReductionCell1(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(ReductionCell1, self).__init__()\n        self.conv_prev_1x1 = nn.Sequential()\n        self.conv_prev_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_prev_1x1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.conv_prev_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_1_right = BranchSeparables(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n        self.comb_iter_2_right = BranchSeparables(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass NASNetALarge(nn.Module):\n    """"""NASNetALarge (6 @ 4032) """"""\n\n    def __init__(self, num_classes=1001, stem_filters=96, penultimate_filters=4032, filters_multiplier=2):\n        super(NASNetALarge, self).__init__()\n        self.num_classes = num_classes\n        self.stem_filters = stem_filters\n        self.penultimate_filters = penultimate_filters\n        self.filters_multiplier = filters_multiplier\n\n        filters = self.penultimate_filters // 24\n        # 24 is default value for the architecture\n\n        self.conv0 = nn.Sequential()\n        self.conv0.add_module(\'conv\', nn.Conv2d(in_channels=3, out_channels=self.stem_filters, kernel_size=3, padding=0, stride=2,\n                                                bias=False))\n        self.conv0.add_module(\'bn\', nn.BatchNorm2d(self.stem_filters, eps=0.001, momentum=0.1, affine=True))\n\n        self.cell_stem_0 = CellStem0(self.stem_filters, num_filters=filters // (filters_multiplier ** 2))\n        self.cell_stem_1 = CellStem1(self.stem_filters, num_filters=filters // filters_multiplier)\n\n        self.cell_0 = FirstCell(in_channels_left=filters, out_channels_left=filters//2,\n                                in_channels_right=2*filters, out_channels_right=filters)\n        self.cell_1 = NormalCell(in_channels_left=2*filters, out_channels_left=filters,\n                                 in_channels_right=6*filters, out_channels_right=filters)\n        self.cell_2 = NormalCell(in_channels_left=6*filters, out_channels_left=filters,\n                                 in_channels_right=6*filters, out_channels_right=filters)\n        self.cell_3 = NormalCell(in_channels_left=6*filters, out_channels_left=filters,\n                                 in_channels_right=6*filters, out_channels_right=filters)\n        self.cell_4 = NormalCell(in_channels_left=6*filters, out_channels_left=filters,\n                                 in_channels_right=6*filters, out_channels_right=filters)\n        self.cell_5 = NormalCell(in_channels_left=6*filters, out_channels_left=filters,\n                                 in_channels_right=6*filters, out_channels_right=filters)\n\n        self.reduction_cell_0 = ReductionCell0(in_channels_left=6*filters, out_channels_left=2*filters,\n                                               in_channels_right=6*filters, out_channels_right=2*filters)\n\n        self.cell_6 = FirstCell(in_channels_left=6*filters, out_channels_left=filters,\n                                in_channels_right=8*filters, out_channels_right=2*filters)\n        self.cell_7 = NormalCell(in_channels_left=8*filters, out_channels_left=2*filters,\n                                 in_channels_right=12*filters, out_channels_right=2*filters)\n        self.cell_8 = NormalCell(in_channels_left=12*filters, out_channels_left=2*filters,\n                                 in_channels_right=12*filters, out_channels_right=2*filters)\n        self.cell_9 = NormalCell(in_channels_left=12*filters, out_channels_left=2*filters,\n                                 in_channels_right=12*filters, out_channels_right=2*filters)\n        self.cell_10 = NormalCell(in_channels_left=12*filters, out_channels_left=2*filters,\n                                  in_channels_right=12*filters, out_channels_right=2*filters)\n        self.cell_11 = NormalCell(in_channels_left=12*filters, out_channels_left=2*filters,\n                                  in_channels_right=12*filters, out_channels_right=2*filters)\n\n        self.reduction_cell_1 = ReductionCell1(in_channels_left=12*filters, out_channels_left=4*filters,\n                                               in_channels_right=12*filters, out_channels_right=4*filters)\n\n        self.cell_12 = FirstCell(in_channels_left=12*filters, out_channels_left=2*filters,\n                                 in_channels_right=16*filters, out_channels_right=4*filters)\n        self.cell_13 = NormalCell(in_channels_left=16*filters, out_channels_left=4*filters,\n                                  in_channels_right=24*filters, out_channels_right=4*filters)\n        self.cell_14 = NormalCell(in_channels_left=24*filters, out_channels_left=4*filters,\n                                  in_channels_right=24*filters, out_channels_right=4*filters)\n        self.cell_15 = NormalCell(in_channels_left=24*filters, out_channels_left=4*filters,\n                                  in_channels_right=24*filters, out_channels_right=4*filters)\n        self.cell_16 = NormalCell(in_channels_left=24*filters, out_channels_left=4*filters,\n                                  in_channels_right=24*filters, out_channels_right=4*filters)\n        self.cell_17 = NormalCell(in_channels_left=24*filters, out_channels_left=4*filters,\n                                  in_channels_right=24*filters, out_channels_right=4*filters)\n\n        self.relu = nn.ReLU()\n        self.avg_pool = nn.AvgPool2d(11, stride=1, padding=0)\n        self.dropout = nn.Dropout()\n        self.last_linear = nn.Linear(24*filters, self.num_classes)\n\n    def features(self, input):\n        x_conv0 = self.conv0(input)\n        x_stem_0 = self.cell_stem_0(x_conv0)\n        x_stem_1 = self.cell_stem_1(x_conv0, x_stem_0)\n\n        x_cell_0 = self.cell_0(x_stem_1, x_stem_0)\n        x_cell_1 = self.cell_1(x_cell_0, x_stem_1)\n        x_cell_2 = self.cell_2(x_cell_1, x_cell_0)\n        x_cell_3 = self.cell_3(x_cell_2, x_cell_1)\n        x_cell_4 = self.cell_4(x_cell_3, x_cell_2)\n        x_cell_5 = self.cell_5(x_cell_4, x_cell_3)\n\n        x_reduction_cell_0 = self.reduction_cell_0(x_cell_5, x_cell_4)\n\n        x_cell_6 = self.cell_6(x_reduction_cell_0, x_cell_4)\n        x_cell_7 = self.cell_7(x_cell_6, x_reduction_cell_0)\n        x_cell_8 = self.cell_8(x_cell_7, x_cell_6)\n        x_cell_9 = self.cell_9(x_cell_8, x_cell_7)\n        x_cell_10 = self.cell_10(x_cell_9, x_cell_8)\n        x_cell_11 = self.cell_11(x_cell_10, x_cell_9)\n\n        x_reduction_cell_1 = self.reduction_cell_1(x_cell_11, x_cell_10)\n\n        x_cell_12 = self.cell_12(x_reduction_cell_1, x_cell_10)\n        x_cell_13 = self.cell_13(x_cell_12, x_reduction_cell_1)\n        x_cell_14 = self.cell_14(x_cell_13, x_cell_12)\n        x_cell_15 = self.cell_15(x_cell_14, x_cell_13)\n        x_cell_16 = self.cell_16(x_cell_15, x_cell_14)\n        x_cell_17 = self.cell_17(x_cell_16, x_cell_15)\n        return x_cell_17\n\n    def logits(self, features):\n        x = self.relu(features)\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\ndef nasnetalarge(num_classes=1001, pretrained=\'imagenet\'):\n    r""""""NASNetALarge model architecture from the\n    `""NASNet"" <https://arxiv.org/abs/1707.07012>`_ paper.\n    """"""\n    if pretrained:\n        settings = pretrained_settings[\'nasnetalarge\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n\n        # both \'imagenet\'&\'imagenet+background\' are loaded from same parameters\n        model = NASNetALarge(num_classes=1001)\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n\n        if pretrained == \'imagenet\':\n            new_last_linear = nn.Linear(model.last_linear.in_features, 1000)\n            new_last_linear.weight.data = model.last_linear.weight.data[1:]\n            new_last_linear.bias.data = model.last_linear.bias.data[1:]\n            model.last_linear = new_last_linear\n\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    else:\n        model = NASNetALarge(num_classes=num_classes)\n    return model\n\n\nif __name__ == ""__main__"":\n\n    model = NASNetALarge()\n    input = Variable(torch.randn(2, 3, 331, 331))\n\n    output = model(input)\n    print(output.size())\n\n\n'"
pretrainedmodels/models/nasnet_mobile.py,13,"b'""""""\nNASNet Mobile\nThanks to Anastasiia (https://github.com/DagnyT) for the great help, support and motivation!\n\n\n------------------------------------------------------------------------------------\n      Architecture       | Top-1 Acc | Top-5 Acc |  Multiply-Adds |  Params (M)\n------------------------------------------------------------------------------------\n|   NASNet-A (4 @ 1056)  |   74.08%  |   91.74%  |       564 M    |     5.3        |\n------------------------------------------------------------------------------------\n# References:\n - [Learning Transferable Architectures for Scalable Image Recognition]\n    (https://arxiv.org/abs/1707.07012)\n""""""\nfrom __future__ import print_function, division, absolute_import\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom torch.autograd import Variable\nimport numpy as np\n\npretrained_settings = {\n    \'nasnetamobile\': {\n        \'imagenet\': {\n            #\'url\': \'https://github.com/veronikayurchuk/pretrained-models.pytorch/releases/download/v1.0/nasnetmobile-7e03cead.pth.tar\',\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/nasnetamobile-7e03cead.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224], # resize 256\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1000\n        },\n        # \'imagenet+background\': {\n        #     # \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth\',\n        #     \'input_space\': \'RGB\',\n        #     \'input_size\': [3, 224, 224], # resize 256\n        #     \'input_range\': [0, 1],\n        #     \'mean\': [0.5, 0.5, 0.5],\n        #     \'std\': [0.5, 0.5, 0.5],\n        #     \'num_classes\': 1001\n        # }\n    }\n}\n\n\nclass MaxPoolPad(nn.Module):\n\n    def __init__(self):\n        super(MaxPoolPad, self).__init__()\n        self.pad = nn.ZeroPad2d((1, 0, 1, 0))\n        self.pool = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x):\n        x = self.pad(x)\n        x = self.pool(x)\n        x = x[:, :, 1:, 1:].contiguous()\n        return x\n\n\nclass AvgPoolPad(nn.Module):\n\n    def __init__(self, stride=2, padding=1):\n        super(AvgPoolPad, self).__init__()\n        self.pad = nn.ZeroPad2d((1, 0, 1, 0))\n        self.pool = nn.AvgPool2d(3, stride=stride, padding=padding, count_include_pad=False)\n\n    def forward(self, x):\n        x = self.pad(x)\n        x = self.pool(x)\n        x = x[:, :, 1:, 1:].contiguous()\n        return x\n\n\nclass SeparableConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, dw_kernel, dw_stride, dw_padding, bias=False):\n        super(SeparableConv2d, self).__init__()\n        self.depthwise_conv2d = nn.Conv2d(in_channels, in_channels, dw_kernel,\n                                          stride=dw_stride,\n                                          padding=dw_padding,\n                                          bias=bias,\n                                          groups=in_channels)\n        self.pointwise_conv2d = nn.Conv2d(in_channels, out_channels, 1, stride=1, bias=bias)\n\n    def forward(self, x):\n        x = self.depthwise_conv2d(x)\n        x = self.pointwise_conv2d(x)\n        return x\n\n\nclass BranchSeparables(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, name=None, bias=False):\n        super(BranchSeparables, self).__init__()\n        self.relu = nn.ReLU()\n        self.separable_1 = SeparableConv2d(in_channels, in_channels, kernel_size, stride, padding, bias=bias)\n        self.bn_sep_1 = nn.BatchNorm2d(in_channels, eps=0.001, momentum=0.1, affine=True)\n        self.relu1 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(in_channels, out_channels, kernel_size, 1, padding, bias=bias)\n        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n        self.name = name\n\n    def forward(self, x):\n        x = self.relu(x)\n        if self.name == \'specific\':\n            x = nn.ZeroPad2d((1, 0, 1, 0))(x)\n        x = self.separable_1(x)\n        if self.name == \'specific\':\n            x = x[:, :, 1:, 1:].contiguous()\n\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass BranchSeparablesStem(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=False):\n        super(BranchSeparablesStem, self).__init__()\n        self.relu = nn.ReLU()\n        self.separable_1 = SeparableConv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n        self.bn_sep_1 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n        self.relu1 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(out_channels, out_channels, kernel_size, 1, padding, bias=bias)\n        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.separable_1(x)\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass BranchSeparablesReduction(BranchSeparables):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, z_padding=1, bias=False):\n        BranchSeparables.__init__(self, in_channels, out_channels, kernel_size, stride, padding, bias)\n        self.padding = nn.ZeroPad2d((z_padding, 0, z_padding, 0))\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.padding(x)\n        x = self.separable_1(x)\n        x = x[:, :, 1:, 1:].contiguous()\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass CellStem0(nn.Module):\n    def __init__(self, stem_filters, num_filters=42):\n        super(CellStem0, self).__init__()\n        self.num_filters = num_filters\n        self.stem_filters = stem_filters\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(self.stem_filters, self.num_filters, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(self.num_filters, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparables(self.num_filters, self.num_filters, 5, 2, 2)\n        self.comb_iter_0_right = BranchSeparablesStem(self.stem_filters, self.num_filters, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_1_right = BranchSeparablesStem(self.stem_filters, self.num_filters, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n        self.comb_iter_2_right = BranchSeparablesStem(self.stem_filters, self.num_filters, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(self.num_filters, self.num_filters, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x):\n        x1 = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x1)\n        x_comb_iter_0_right = self.comb_iter_0_right(x)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x1)\n        x_comb_iter_1_right = self.comb_iter_1_right(x)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x1)\n        x_comb_iter_2_right = self.comb_iter_2_right(x)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x1)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass CellStem1(nn.Module):\n\n    def __init__(self, stem_filters, num_filters):\n        super(CellStem1, self).__init__()\n        self.num_filters = num_filters\n        self.stem_filters = stem_filters\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(2*self.num_filters, self.num_filters, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(self.num_filters, eps=0.001, momentum=0.1, affine=True))\n\n        self.relu = nn.ReLU()\n        self.path_1 = nn.Sequential()\n        self.path_1.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_1.add_module(\'conv\', nn.Conv2d(self.stem_filters, self.num_filters//2, 1, stride=1, bias=False))\n        self.path_2 = nn.ModuleList()\n        self.path_2.add_module(\'pad\', nn.ZeroPad2d((0, 1, 0, 1)))\n        self.path_2.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_2.add_module(\'conv\', nn.Conv2d(self.stem_filters, self.num_filters//2, 1, stride=1, bias=False))\n\n        self.final_path_bn = nn.BatchNorm2d(self.num_filters, eps=0.001, momentum=0.1, affine=True)\n\n        self.comb_iter_0_left = BranchSeparables(self.num_filters, self.num_filters, 5, 2, 2, name=\'specific\', bias=False)\n        self.comb_iter_0_right = BranchSeparables(self.num_filters, self.num_filters, 7, 2, 3, name=\'specific\', bias=False)\n\n        # self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_1_left = MaxPoolPad()\n        self.comb_iter_1_right = BranchSeparables(self.num_filters, self.num_filters, 7, 2, 3, name=\'specific\', bias=False)\n\n        # self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n        self.comb_iter_2_left = AvgPoolPad()\n        self.comb_iter_2_right = BranchSeparables(self.num_filters, self.num_filters, 5, 2, 2, name=\'specific\', bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(self.num_filters, self.num_filters, 3, 1, 1, name=\'specific\', bias=False)\n        # self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_4_right = MaxPoolPad()\n\n    def forward(self, x_conv0, x_stem_0):\n        x_left = self.conv_1x1(x_stem_0)\n\n        x_relu = self.relu(x_conv0)\n        # path 1\n        x_path1 = self.path_1(x_relu)\n        # path 2\n        x_path2 = self.path_2.pad(x_relu)\n        x_path2 = x_path2[:, :, 1:, 1:]\n        x_path2 = self.path_2.avgpool(x_path2)\n        x_path2 = self.path_2.conv(x_path2)\n        # final path\n        x_right = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_left)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_right)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_right)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_left)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_left)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass FirstCell(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(FirstCell, self).__init__()\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.relu = nn.ReLU()\n        self.path_1 = nn.Sequential()\n        self.path_1.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.path_2 = nn.ModuleList()\n        self.path_2.add_module(\'pad\', nn.ZeroPad2d((0, 1, 0, 1)))\n        self.path_2.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_2.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n\n        self.final_path_bn = nn.BatchNorm2d(out_channels_left * 2, eps=0.001, momentum=0.1, affine=True)\n\n        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n        self.comb_iter_1_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n        self.comb_iter_1_right = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_3_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n    def forward(self, x, x_prev):\n        x_relu = self.relu(x_prev)\n        # path 1\n        x_path1 = self.path_1(x_relu)\n        # path 2\n        x_path2 = self.path_2.pad(x_relu)\n        x_path2 = x_path2[:, :, 1:, 1:]\n        x_path2 = self.path_2.avgpool(x_path2)\n        x_path2 = self.path_2.conv(x_path2)\n        # final path\n        x_left = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_left\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_right\n\n        x_out = torch.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass NormalCell(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(NormalCell, self).__init__()\n        self.conv_prev_1x1 = nn.Sequential()\n        self.conv_prev_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_prev_1x1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.conv_prev_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(out_channels_left, out_channels_left, 3, 1, 1, bias=False)\n\n        self.comb_iter_1_left = BranchSeparables(out_channels_left, out_channels_left, 5, 1, 2, bias=False)\n        self.comb_iter_1_right = BranchSeparables(out_channels_left, out_channels_left, 3, 1, 1, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_3_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_left\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_right\n\n        x_out = torch.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass ReductionCell0(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(ReductionCell0, self).__init__()\n        self.conv_prev_1x1 = nn.Sequential()\n        self.conv_prev_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_prev_1x1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.conv_prev_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparablesReduction(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = MaxPoolPad()\n        self.comb_iter_1_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = AvgPoolPad()\n        self.comb_iter_2_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparablesReduction(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = MaxPoolPad()\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass ReductionCell1(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(ReductionCell1, self).__init__()\n        self.conv_prev_1x1 = nn.Sequential()\n        self.conv_prev_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_prev_1x1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.conv_prev_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 2, 2, name=\'specific\', bias=False)\n        self.comb_iter_0_right = BranchSeparables(out_channels_right, out_channels_right, 7, 2, 3, name=\'specific\', bias=False)\n\n        # self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_1_left = MaxPoolPad()\n        self.comb_iter_1_right = BranchSeparables(out_channels_right, out_channels_right, 7, 2, 3, name=\'specific\', bias=False)\n\n        # self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n        self.comb_iter_2_left = AvgPoolPad()\n        self.comb_iter_2_right = BranchSeparables(out_channels_right, out_channels_right, 5, 2, 2, name=\'specific\', bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, name=\'specific\', bias=False)\n        # self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_4_right =MaxPoolPad()\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass NASNetAMobile(nn.Module):\n    """"""NASNetAMobile (4 @ 1056) """"""\n\n    def __init__(self, num_classes=1000, stem_filters=32, penultimate_filters=1056, filters_multiplier=2):\n        super(NASNetAMobile, self).__init__()\n        self.num_classes = num_classes\n        self.stem_filters = stem_filters\n        self.penultimate_filters = penultimate_filters\n        self.filters_multiplier = filters_multiplier\n\n        filters = self.penultimate_filters // 24\n        # 24 is default value for the architecture\n\n        self.conv0 = nn.Sequential()\n        self.conv0.add_module(\'conv\', nn.Conv2d(in_channels=3, out_channels=self.stem_filters, kernel_size=3, padding=0, stride=2,\n                                                bias=False))\n        self.conv0.add_module(\'bn\', nn.BatchNorm2d(self.stem_filters, eps=0.001, momentum=0.1, affine=True))\n\n        self.cell_stem_0 = CellStem0(self.stem_filters, num_filters=filters // (filters_multiplier ** 2))\n        self.cell_stem_1 = CellStem1(self.stem_filters, num_filters=filters // filters_multiplier)\n\n        self.cell_0 = FirstCell(in_channels_left=filters, out_channels_left=filters//2, # 1, 0.5\n                                in_channels_right=2*filters, out_channels_right=filters) # 2, 1\n        self.cell_1 = NormalCell(in_channels_left=2*filters, out_channels_left=filters, # 2, 1\n                                 in_channels_right=6*filters, out_channels_right=filters) # 6, 1\n        self.cell_2 = NormalCell(in_channels_left=6*filters, out_channels_left=filters, # 6, 1\n                                 in_channels_right=6*filters, out_channels_right=filters) # 6, 1\n        self.cell_3 = NormalCell(in_channels_left=6*filters, out_channels_left=filters, # 6, 1\n                                 in_channels_right=6*filters, out_channels_right=filters) # 6, 1\n\n        self.reduction_cell_0 = ReductionCell0(in_channels_left=6*filters, out_channels_left=2*filters, # 6, 2\n                                               in_channels_right=6*filters, out_channels_right=2*filters) # 6, 2\n\n        self.cell_6 = FirstCell(in_channels_left=6*filters, out_channels_left=filters, # 6, 1\n                                in_channels_right=8*filters, out_channels_right=2*filters) # 8, 2\n        self.cell_7 = NormalCell(in_channels_left=8*filters, out_channels_left=2*filters, # 8, 2\n                                 in_channels_right=12*filters, out_channels_right=2*filters) # 12, 2\n        self.cell_8 = NormalCell(in_channels_left=12*filters, out_channels_left=2*filters, # 12, 2\n                                 in_channels_right=12*filters, out_channels_right=2*filters) # 12, 2\n        self.cell_9 = NormalCell(in_channels_left=12*filters, out_channels_left=2*filters, # 12, 2\n                                 in_channels_right=12*filters, out_channels_right=2*filters) # 12, 2\n\n        self.reduction_cell_1 = ReductionCell1(in_channels_left=12*filters, out_channels_left=4*filters, # 12, 4\n                                               in_channels_right=12*filters, out_channels_right=4*filters) # 12, 4\n\n        self.cell_12 = FirstCell(in_channels_left=12*filters, out_channels_left=2*filters, # 12, 2\n                                 in_channels_right=16*filters, out_channels_right=4*filters) # 16, 4\n        self.cell_13 = NormalCell(in_channels_left=16*filters, out_channels_left=4*filters, # 16, 4\n                                  in_channels_right=24*filters, out_channels_right=4*filters) # 24, 4\n        self.cell_14 = NormalCell(in_channels_left=24*filters, out_channels_left=4*filters, # 24, 4\n                                  in_channels_right=24*filters, out_channels_right=4*filters) # 24, 4\n        self.cell_15 = NormalCell(in_channels_left=24*filters, out_channels_left=4*filters, # 24, 4\n                                  in_channels_right=24*filters, out_channels_right=4*filters) # 24, 4\n\n        self.relu = nn.ReLU()\n        self.avg_pool = nn.AvgPool2d(7, stride=1, padding=0)\n        self.dropout = nn.Dropout()\n        self.last_linear = nn.Linear(24*filters, self.num_classes)\n\n    def features(self, input):\n        x_conv0 = self.conv0(input)\n        x_stem_0 = self.cell_stem_0(x_conv0)\n        x_stem_1 = self.cell_stem_1(x_conv0, x_stem_0)\n\n        x_cell_0 = self.cell_0(x_stem_1, x_stem_0)\n        x_cell_1 = self.cell_1(x_cell_0, x_stem_1)\n        x_cell_2 = self.cell_2(x_cell_1, x_cell_0)\n        x_cell_3 = self.cell_3(x_cell_2, x_cell_1)\n\n        x_reduction_cell_0 = self.reduction_cell_0(x_cell_3, x_cell_2)\n\n        x_cell_6 = self.cell_6(x_reduction_cell_0, x_cell_3)\n        x_cell_7 = self.cell_7(x_cell_6, x_reduction_cell_0)\n        x_cell_8 = self.cell_8(x_cell_7, x_cell_6)\n        x_cell_9 = self.cell_9(x_cell_8, x_cell_7)\n\n        x_reduction_cell_1 = self.reduction_cell_1(x_cell_9, x_cell_8)\n\n        x_cell_12 = self.cell_12(x_reduction_cell_1, x_cell_9)\n        x_cell_13 = self.cell_13(x_cell_12, x_reduction_cell_1)\n        x_cell_14 = self.cell_14(x_cell_13, x_cell_12)\n        x_cell_15 = self.cell_15(x_cell_14, x_cell_13)\n        return x_cell_15\n\n    def logits(self, features):\n        x = self.relu(features)\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\ndef nasnetamobile(num_classes=1000, pretrained=\'imagenet\'):\n    r""""""NASNetALarge model architecture from the\n    `""NASNet"" <https://arxiv.org/abs/1707.07012>`_ paper.\n    """"""\n    if pretrained:\n        settings = pretrained_settings[\'nasnetamobile\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n\n        # both \'imagenet\'&\'imagenet+background\' are loaded from same parameters\n        model = NASNetAMobile(num_classes=num_classes)\n        model.load_state_dict(model_zoo.load_url(settings[\'url\'], map_location=None))\n\n       # if pretrained == \'imagenet\':\n       #     new_last_linear = nn.Linear(model.last_linear.in_features, 1000)\n       #     new_last_linear.weight.data = model.last_linear.weight.data[1:]\n       #     new_last_linear.bias.data = model.last_linear.bias.data[1:]\n       #     model.last_linear = new_last_linear\n\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    else:\n        settings = pretrained_settings[\'nasnetamobile\'][\'imagenet\']\n        model = NASNetAMobile(num_classes=num_classes)\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    return model\n\n\nif __name__ == ""__main__"":\n\n    model = NASNetAMobile()\n    input = Variable(torch.randn(2, 3, 224, 224))\n    output = model(input)\n\n    print(output.size())\n'"
pretrainedmodels/models/pnasnet.py,4,"b'from __future__ import print_function, division, absolute_import\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n\npretrained_settings = {\n    \'pnasnet5large\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/pnasnet5large-bf079911.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 331, 331],\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1000\n        },\n        \'imagenet+background\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/pnasnet5large-bf079911.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 331, 331],\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1001\n        }\n    }\n}\n\n\nclass MaxPool(nn.Module):\n\n    def __init__(self, kernel_size, stride=1, padding=1, zero_pad=False):\n        super(MaxPool, self).__init__()\n        self.zero_pad = nn.ZeroPad2d((1, 0, 1, 0)) if zero_pad else None\n        self.pool = nn.MaxPool2d(kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x):\n        if self.zero_pad:\n            x = self.zero_pad(x)\n        x = self.pool(x)\n        if self.zero_pad:\n            x = x[:, :, 1:, 1:]\n        return x\n\n\nclass SeparableConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, dw_kernel_size, dw_stride,\n                 dw_padding):\n        super(SeparableConv2d, self).__init__()\n        self.depthwise_conv2d = nn.Conv2d(in_channels, in_channels,\n                                          kernel_size=dw_kernel_size,\n                                          stride=dw_stride, padding=dw_padding,\n                                          groups=in_channels, bias=False)\n        self.pointwise_conv2d = nn.Conv2d(in_channels, out_channels,\n                                          kernel_size=1, bias=False)\n\n    def forward(self, x):\n        x = self.depthwise_conv2d(x)\n        x = self.pointwise_conv2d(x)\n        return x\n\n\nclass BranchSeparables(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 stem_cell=False, zero_pad=False):\n        super(BranchSeparables, self).__init__()\n        padding = kernel_size // 2\n        middle_channels = out_channels if stem_cell else in_channels\n        self.zero_pad = nn.ZeroPad2d((1, 0, 1, 0)) if zero_pad else None\n        self.relu_1 = nn.ReLU()\n        self.separable_1 = SeparableConv2d(in_channels, middle_channels,\n                                           kernel_size, dw_stride=stride,\n                                           dw_padding=padding)\n        self.bn_sep_1 = nn.BatchNorm2d(middle_channels, eps=0.001)\n        self.relu_2 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(middle_channels, out_channels,\n                                           kernel_size, dw_stride=1,\n                                           dw_padding=padding)\n        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x):\n        x = self.relu_1(x)\n        if self.zero_pad:\n            x = self.zero_pad(x)\n        x = self.separable_1(x)\n        if self.zero_pad:\n            x = x[:, :, 1:, 1:].contiguous()\n        x = self.bn_sep_1(x)\n        x = self.relu_2(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass ReluConvBn(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n        super(ReluConvBn, self).__init__()\n        self.relu = nn.ReLU()\n        self.conv = nn.Conv2d(in_channels, out_channels,\n                              kernel_size=kernel_size, stride=stride,\n                              bias=False)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass FactorizedReduction(nn.Module):\n\n    def __init__(self, in_channels, out_channels):\n        super(FactorizedReduction, self).__init__()\n        self.relu = nn.ReLU()\n        self.path_1 = nn.Sequential(OrderedDict([\n            (\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False)),\n            (\'conv\', nn.Conv2d(in_channels, out_channels // 2,\n                               kernel_size=1, bias=False)),\n        ]))\n        self.path_2 = nn.Sequential(OrderedDict([\n            (\'pad\', nn.ZeroPad2d((0, 1, 0, 1))),\n            (\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False)),\n            (\'conv\', nn.Conv2d(in_channels, out_channels // 2,\n                               kernel_size=1, bias=False)),\n        ]))\n        self.final_path_bn = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x):\n        x = self.relu(x)\n\n        x_path1 = self.path_1(x)\n\n        x_path2 = self.path_2.pad(x)\n        x_path2 = x_path2[:, :, 1:, 1:]\n        x_path2 = self.path_2.avgpool(x_path2)\n        x_path2 = self.path_2.conv(x_path2)\n\n        out = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n        return out\n\n\nclass CellBase(nn.Module):\n\n    def cell_forward(self, x_left, x_right):\n        x_comb_iter_0_left = self.comb_iter_0_left(x_left)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_right)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_comb_iter_2)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_right)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_left)\n        if self.comb_iter_4_right:\n            x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        else:\n            x_comb_iter_4_right = x_right\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat(\n            [x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3,\n             x_comb_iter_4], 1)\n        return x_out\n\n\nclass CellStem0(CellBase):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right,\n                 out_channels_right):\n        super(CellStem0, self).__init__()\n        self.conv_1x1 = ReluConvBn(in_channels_right, out_channels_right,\n                                   kernel_size=1)\n        self.comb_iter_0_left = BranchSeparables(in_channels_left,\n                                                 out_channels_left,\n                                                 kernel_size=5, stride=2,\n                                                 stem_cell=True)\n        self.comb_iter_0_right = nn.Sequential(OrderedDict([\n            (\'max_pool\', MaxPool(3, stride=2)),\n            (\'conv\', nn.Conv2d(in_channels_left, out_channels_left,\n                               kernel_size=1, bias=False)),\n            (\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001)),\n        ]))\n        self.comb_iter_1_left = BranchSeparables(out_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=7, stride=2)\n        self.comb_iter_1_right = MaxPool(3, stride=2)\n        self.comb_iter_2_left = BranchSeparables(out_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=5, stride=2)\n        self.comb_iter_2_right = BranchSeparables(out_channels_right,\n                                                  out_channels_right,\n                                                  kernel_size=3, stride=2)\n        self.comb_iter_3_left = BranchSeparables(out_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=3)\n        self.comb_iter_3_right = MaxPool(3, stride=2)\n        self.comb_iter_4_left = BranchSeparables(in_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=3, stride=2,\n                                                 stem_cell=True)\n        self.comb_iter_4_right = ReluConvBn(out_channels_right,\n                                            out_channels_right,\n                                            kernel_size=1, stride=2)\n\n    def forward(self, x_left):\n        x_right = self.conv_1x1(x_left)\n        x_out = self.cell_forward(x_left, x_right)\n        return x_out\n\n\nclass Cell(CellBase):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right,\n                 out_channels_right, is_reduction=False, zero_pad=False,\n                 match_prev_layer_dimensions=False):\n        super(Cell, self).__init__()\n\n        # If `is_reduction` is set to `True` stride 2 is used for\n        # convolutional and pooling layers to reduce the spatial size of\n        # the output of a cell approximately by a factor of 2.\n        stride = 2 if is_reduction else 1\n\n        # If `match_prev_layer_dimensions` is set to `True`\n        # `FactorizedReduction` is used to reduce the spatial size\n        # of the left input of a cell approximately by a factor of 2.\n        self.match_prev_layer_dimensions = match_prev_layer_dimensions\n        if match_prev_layer_dimensions:\n            self.conv_prev_1x1 = FactorizedReduction(in_channels_left,\n                                                     out_channels_left)\n        else:\n            self.conv_prev_1x1 = ReluConvBn(in_channels_left,\n                                            out_channels_left, kernel_size=1)\n\n        self.conv_1x1 = ReluConvBn(in_channels_right, out_channels_right,\n                                   kernel_size=1)\n        self.comb_iter_0_left = BranchSeparables(out_channels_left,\n                                                 out_channels_left,\n                                                 kernel_size=5, stride=stride,\n                                                 zero_pad=zero_pad)\n        self.comb_iter_0_right = MaxPool(3, stride=stride, zero_pad=zero_pad)\n        self.comb_iter_1_left = BranchSeparables(out_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=7, stride=stride,\n                                                 zero_pad=zero_pad)\n        self.comb_iter_1_right = MaxPool(3, stride=stride, zero_pad=zero_pad)\n        self.comb_iter_2_left = BranchSeparables(out_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=5, stride=stride,\n                                                 zero_pad=zero_pad)\n        self.comb_iter_2_right = BranchSeparables(out_channels_right,\n                                                  out_channels_right,\n                                                  kernel_size=3, stride=stride,\n                                                  zero_pad=zero_pad)\n        self.comb_iter_3_left = BranchSeparables(out_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=3)\n        self.comb_iter_3_right = MaxPool(3, stride=stride, zero_pad=zero_pad)\n        self.comb_iter_4_left = BranchSeparables(out_channels_left,\n                                                 out_channels_left,\n                                                 kernel_size=3, stride=stride,\n                                                 zero_pad=zero_pad)\n        if is_reduction:\n            self.comb_iter_4_right = ReluConvBn(out_channels_right,\n                                                out_channels_right,\n                                                kernel_size=1, stride=stride)\n        else:\n            self.comb_iter_4_right = None\n\n    def forward(self, x_left, x_right):\n        x_left = self.conv_prev_1x1(x_left)\n        x_right = self.conv_1x1(x_right)\n        x_out = self.cell_forward(x_left, x_right)\n        return x_out\n\n\nclass PNASNet5Large(nn.Module):\n    def __init__(self, num_classes=1001):\n        super(PNASNet5Large, self).__init__()\n        self.num_classes = num_classes\n        self.conv_0 = nn.Sequential(OrderedDict([\n            (\'conv\', nn.Conv2d(3, 96, kernel_size=3, stride=2, bias=False)),\n            (\'bn\', nn.BatchNorm2d(96, eps=0.001))\n        ]))\n        self.cell_stem_0 = CellStem0(in_channels_left=96, out_channels_left=54,\n                                     in_channels_right=96,\n                                     out_channels_right=54)\n        self.cell_stem_1 = Cell(in_channels_left=96, out_channels_left=108,\n                                in_channels_right=270, out_channels_right=108,\n                                match_prev_layer_dimensions=True,\n                                is_reduction=True)\n        self.cell_0 = Cell(in_channels_left=270, out_channels_left=216,\n                           in_channels_right=540, out_channels_right=216,\n                           match_prev_layer_dimensions=True)\n        self.cell_1 = Cell(in_channels_left=540, out_channels_left=216,\n                           in_channels_right=1080, out_channels_right=216)\n        self.cell_2 = Cell(in_channels_left=1080, out_channels_left=216,\n                           in_channels_right=1080, out_channels_right=216)\n        self.cell_3 = Cell(in_channels_left=1080, out_channels_left=216,\n                           in_channels_right=1080, out_channels_right=216)\n        self.cell_4 = Cell(in_channels_left=1080, out_channels_left=432,\n                           in_channels_right=1080, out_channels_right=432,\n                           is_reduction=True, zero_pad=True)\n        self.cell_5 = Cell(in_channels_left=1080, out_channels_left=432,\n                           in_channels_right=2160, out_channels_right=432,\n                           match_prev_layer_dimensions=True)\n        self.cell_6 = Cell(in_channels_left=2160, out_channels_left=432,\n                           in_channels_right=2160, out_channels_right=432)\n        self.cell_7 = Cell(in_channels_left=2160, out_channels_left=432,\n                           in_channels_right=2160, out_channels_right=432)\n        self.cell_8 = Cell(in_channels_left=2160, out_channels_left=864,\n                           in_channels_right=2160, out_channels_right=864,\n                           is_reduction=True)\n        self.cell_9 = Cell(in_channels_left=2160, out_channels_left=864,\n                           in_channels_right=4320, out_channels_right=864,\n                           match_prev_layer_dimensions=True)\n        self.cell_10 = Cell(in_channels_left=4320, out_channels_left=864,\n                            in_channels_right=4320, out_channels_right=864)\n        self.cell_11 = Cell(in_channels_left=4320, out_channels_left=864,\n                            in_channels_right=4320, out_channels_right=864)\n        self.relu = nn.ReLU()\n        self.avg_pool = nn.AvgPool2d(11, stride=1, padding=0)\n        self.dropout = nn.Dropout(0.5)\n        self.last_linear = nn.Linear(4320, num_classes)\n\n    def features(self, x):\n        x_conv_0 = self.conv_0(x)\n        x_stem_0 = self.cell_stem_0(x_conv_0)\n        x_stem_1 = self.cell_stem_1(x_conv_0, x_stem_0)\n        x_cell_0 = self.cell_0(x_stem_0, x_stem_1)\n        x_cell_1 = self.cell_1(x_stem_1, x_cell_0)\n        x_cell_2 = self.cell_2(x_cell_0, x_cell_1)\n        x_cell_3 = self.cell_3(x_cell_1, x_cell_2)\n        x_cell_4 = self.cell_4(x_cell_2, x_cell_3)\n        x_cell_5 = self.cell_5(x_cell_3, x_cell_4)\n        x_cell_6 = self.cell_6(x_cell_4, x_cell_5)\n        x_cell_7 = self.cell_7(x_cell_5, x_cell_6)\n        x_cell_8 = self.cell_8(x_cell_6, x_cell_7)\n        x_cell_9 = self.cell_9(x_cell_7, x_cell_8)\n        x_cell_10 = self.cell_10(x_cell_8, x_cell_9)\n        x_cell_11 = self.cell_11(x_cell_9, x_cell_10)\n        return x_cell_11\n\n    def logits(self, features):\n        x = self.relu(features)\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\ndef pnasnet5large(num_classes=1001, pretrained=\'imagenet\'):\n    r""""""PNASNet-5 model architecture from the\n    `""Progressive Neural Architecture Search""\n    <https://arxiv.org/abs/1712.00559>`_ paper.\n    """"""\n    if pretrained:\n        settings = pretrained_settings[\'pnasnet5large\'][pretrained]\n        assert num_classes == settings[\n            \'num_classes\'], \'num_classes should be {}, but is {}\'.format(\n            settings[\'num_classes\'], num_classes)\n\n        # both \'imagenet\'&\'imagenet+background\' are loaded from same parameters\n        model = PNASNet5Large(num_classes=1001)\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n\n        if pretrained == \'imagenet\':\n            new_last_linear = nn.Linear(model.last_linear.in_features, 1000)\n            new_last_linear.weight.data = model.last_linear.weight.data[1:]\n            new_last_linear.bias.data = model.last_linear.bias.data[1:]\n            model.last_linear = new_last_linear\n\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    else:\n        model = PNASNet5Large(num_classes=num_classes)\n    return model\n'"
pretrainedmodels/models/polynet.py,12,"b'from __future__ import print_function, division, absolute_import\nimport torch\nimport torch.nn as nn\nfrom torch.utils import model_zoo\n\n__all__ = [\'PolyNet\', \'polynet\']\n\npretrained_settings = {\n    \'polynet\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/polynet-f71d82a5.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 331, 331],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        },\n    }\n}\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0,\n                 output_relu=True):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size,\n                              stride=stride, padding=padding, bias=False)\n        self.bn = nn.BatchNorm2d(out_planes)\n        self.relu = nn.ReLU() if output_relu else None\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        if self.relu:\n            x = self.relu(x)\n        return x\n\n\nclass PolyConv2d(nn.Module):\n    """"""A block that is used inside poly-N (poly-2, poly-3, and so on) modules.\n    The Convolution layer is shared between all Inception blocks inside\n    a poly-N module. BatchNorm layers are not shared between Inception blocks\n    and therefore the number of BatchNorm layers is equal to the number of\n    Inception blocks inside a poly-N module.\n    """"""\n\n    def __init__(self, in_planes, out_planes, kernel_size, num_blocks,\n                 stride=1, padding=0):\n        super(PolyConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size,\n                              stride=stride, padding=padding, bias=False)\n        self.bn_blocks = nn.ModuleList([\n            nn.BatchNorm2d(out_planes) for _ in range(num_blocks)\n        ])\n        self.relu = nn.ReLU()\n\n    def forward(self, x, block_index):\n        x = self.conv(x)\n        bn = self.bn_blocks[block_index]\n        x = bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass Stem(nn.Module):\n\n    def __init__(self):\n        super(Stem, self).__init__()\n        self.conv1 = nn.Sequential(\n            BasicConv2d(3, 32, kernel_size=3, stride=2),\n            BasicConv2d(32, 32, kernel_size=3),\n            BasicConv2d(32, 64, kernel_size=3, padding=1),\n        )\n        self.conv1_pool_branch = nn.MaxPool2d(3, stride=2)\n        self.conv1_branch = BasicConv2d(64, 96, kernel_size=3, stride=2)\n        self.conv2_short = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1),\n            BasicConv2d(64, 96, kernel_size=3),\n        )\n        self.conv2_long = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1),\n            BasicConv2d(64, 64, kernel_size=(7, 1), padding=(3, 0)),\n            BasicConv2d(64, 64, kernel_size=(1, 7), padding=(0, 3)),\n            BasicConv2d(64, 96, kernel_size=3),\n        )\n        self.conv2_pool_branch = nn.MaxPool2d(3, stride=2)\n        self.conv2_branch = BasicConv2d(192, 192, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        x = self.conv1(x)\n\n        x0 = self.conv1_pool_branch(x)\n        x1 = self.conv1_branch(x)\n        x = torch.cat((x0, x1), 1)\n\n        x0 = self.conv2_short(x)\n        x1 = self.conv2_long(x)\n        x = torch.cat((x0, x1), 1)\n\n        x0 = self.conv2_pool_branch(x)\n        x1 = self.conv2_branch(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass BlockA(nn.Module):\n    """"""Inception-ResNet-A block.""""""\n\n    def __init__(self):\n        super(BlockA, self).__init__()\n        self.path0 = nn.Sequential(\n            BasicConv2d(384, 32, kernel_size=1),\n            BasicConv2d(32, 48, kernel_size=3, padding=1),\n            BasicConv2d(48, 64, kernel_size=3, padding=1),\n        )\n        self.path1 = nn.Sequential(\n            BasicConv2d(384, 32, kernel_size=1),\n            BasicConv2d(32, 32, kernel_size=3, padding=1),\n        )\n        self.path2 = BasicConv2d(384, 32, kernel_size=1)\n        self.conv2d = BasicConv2d(128, 384, kernel_size=1, output_relu=False)\n\n    def forward(self, x):\n        x0 = self.path0(x)\n        x1 = self.path1(x)\n        x2 = self.path2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        out = self.conv2d(out)\n        return out\n\n\nclass BlockB(nn.Module):\n    """"""Inception-ResNet-B block.""""""\n\n    def __init__(self):\n        super(BlockB, self).__init__()\n        self.path0 = nn.Sequential(\n            BasicConv2d(1152, 128, kernel_size=1),\n            BasicConv2d(128, 160, kernel_size=(1, 7), padding=(0, 3)),\n            BasicConv2d(160, 192, kernel_size=(7, 1), padding=(3, 0)),\n        )\n        self.path1 = BasicConv2d(1152, 192, kernel_size=1)\n        self.conv2d = BasicConv2d(384, 1152, kernel_size=1, output_relu=False)\n\n    def forward(self, x):\n        x0 = self.path0(x)\n        x1 = self.path1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        return out\n\n\nclass BlockC(nn.Module):\n    """"""Inception-ResNet-C block.""""""\n\n    def __init__(self):\n        super(BlockC, self).__init__()\n        self.path0 = nn.Sequential(\n            BasicConv2d(2048, 192, kernel_size=1),\n            BasicConv2d(192, 224, kernel_size=(1, 3), padding=(0, 1)),\n            BasicConv2d(224, 256, kernel_size=(3, 1), padding=(1, 0)),\n        )\n        self.path1 = BasicConv2d(2048, 192, kernel_size=1)\n        self.conv2d = BasicConv2d(448, 2048, kernel_size=1, output_relu=False)\n\n    def forward(self, x):\n        x0 = self.path0(x)\n        x1 = self.path1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        return out\n\n\nclass ReductionA(nn.Module):\n    """"""A dimensionality reduction block that is placed after stage-a\n    Inception-ResNet blocks.\n    """"""\n\n    def __init__(self):\n        super(ReductionA, self).__init__()\n        self.path0 = nn.Sequential(\n            BasicConv2d(384, 256, kernel_size=1),\n            BasicConv2d(256, 256, kernel_size=3, padding=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2),\n        )\n        self.path1 = BasicConv2d(384, 384, kernel_size=3, stride=2)\n        self.path2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.path0(x)\n        x1 = self.path1(x)\n        x2 = self.path2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass ReductionB(nn.Module):\n    """"""A dimensionality reduction block that is placed after stage-b\n    Inception-ResNet blocks.\n    """"""\n    def __init__(self):\n        super(ReductionB, self).__init__()\n        self.path0 = nn.Sequential(\n            BasicConv2d(1152, 256, kernel_size=1),\n            BasicConv2d(256, 256, kernel_size=3, padding=1),\n            BasicConv2d(256, 256, kernel_size=3, stride=2),\n        )\n        self.path1 = nn.Sequential(\n            BasicConv2d(1152, 256, kernel_size=1),\n            BasicConv2d(256, 256, kernel_size=3, stride=2),\n        )\n        self.path2 = nn.Sequential(\n            BasicConv2d(1152, 256, kernel_size=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2),\n        )\n        self.path3 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.path0(x)\n        x1 = self.path1(x)\n        x2 = self.path2(x)\n        x3 = self.path3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass InceptionResNetBPoly(nn.Module):\n    """"""Base class for constructing poly-N Inception-ResNet-B modules.\n    When `num_blocks` is equal to 1, a module will have only a first-order path\n    and will be equal to a standard Inception-ResNet-B block.\n    When `num_blocks` is equal to 2, a module will have first-order and\n    second-order paths and will be called Inception-ResNet-B poly-2 module.\n    Increasing value of the `num_blocks` parameter will produce a higher order\n    Inception-ResNet-B poly-N modules.\n    """"""\n\n    def __init__(self, scale, num_blocks):\n        super(InceptionResNetBPoly, self).__init__()\n        assert num_blocks >= 1, \'num_blocks should be greater or equal to 1\'\n        self.scale = scale\n        self.num_blocks = num_blocks\n        self.path0_1x1 = PolyConv2d(1152, 128, kernel_size=1,\n                                    num_blocks=self.num_blocks)\n        self.path0_1x7 = PolyConv2d(128, 160, kernel_size=(1, 7),\n                                    num_blocks=self.num_blocks, padding=(0, 3))\n        self.path0_7x1 = PolyConv2d(160, 192, kernel_size=(7, 1),\n                                    num_blocks=self.num_blocks, padding=(3, 0))\n        self.path1 = PolyConv2d(1152, 192, kernel_size=1,\n                                num_blocks=self.num_blocks)\n        # conv2d blocks are not shared between Inception-ResNet-B blocks\n        self.conv2d_blocks = nn.ModuleList([\n            BasicConv2d(384, 1152, kernel_size=1, output_relu=False)\n            for _ in range(self.num_blocks)\n        ])\n        self.relu = nn.ReLU()\n\n    def forward_block(self, x, block_index):\n        x0 = self.path0_1x1(x, block_index)\n        x0 = self.path0_1x7(x0, block_index)\n        x0 = self.path0_7x1(x0, block_index)\n        x1 = self.path1(x, block_index)\n        out = torch.cat((x0, x1), 1)\n        conv2d_block = self.conv2d_blocks[block_index]\n        out = conv2d_block(out)\n        return out\n\n    def forward(self, x):\n        out = x\n        for block_index in range(self.num_blocks):\n            x = self.forward_block(x, block_index)\n            out = out + x * self.scale\n            x = self.relu(x)\n        out = self.relu(out)\n        return out\n\n\nclass InceptionResNetCPoly(nn.Module):\n    """"""Base class for constructing poly-N Inception-ResNet-C modules.\n    When `num_blocks` is equal to 1, a module will have only a first-order path\n    and will be equal to a standard Inception-ResNet-C block.\n    When `num_blocks` is equal to 2, a module will have first-order and\n    second-order paths and will be called Inception-ResNet-C poly-2 module.\n    Increasing value of the `num_blocks` parameter will produce a higher order\n    Inception-ResNet-C poly-N modules.\n    """"""\n\n    def __init__(self, scale, num_blocks):\n        super(InceptionResNetCPoly, self).__init__()\n        assert num_blocks >= 1, \'num_blocks should be greater or equal to 1\'\n        self.scale = scale\n        self.num_blocks = num_blocks\n        self.path0_1x1 = PolyConv2d(2048, 192, kernel_size=1,\n                                    num_blocks=self.num_blocks)\n        self.path0_1x3 = PolyConv2d(192, 224, kernel_size=(1, 3),\n                                    num_blocks=self.num_blocks, padding=(0, 1))\n        self.path0_3x1 = PolyConv2d(224, 256, kernel_size=(3, 1),\n                                    num_blocks=self.num_blocks, padding=(1, 0))\n        self.path1 = PolyConv2d(2048, 192, kernel_size=1,\n                                num_blocks=self.num_blocks)\n        # conv2d blocks are not shared between Inception-ResNet-C blocks\n        self.conv2d_blocks = nn.ModuleList([\n            BasicConv2d(448, 2048, kernel_size=1, output_relu=False)\n            for _ in range(self.num_blocks)\n        ])\n        self.relu = nn.ReLU()\n\n    def forward_block(self, x, block_index):\n        x0 = self.path0_1x1(x, block_index)\n        x0 = self.path0_1x3(x0, block_index)\n        x0 = self.path0_3x1(x0, block_index)\n        x1 = self.path1(x, block_index)\n        out = torch.cat((x0, x1), 1)\n        conv2d_block = self.conv2d_blocks[block_index]\n        out = conv2d_block(out)\n        return out\n\n    def forward(self, x):\n        out = x\n        for block_index in range(self.num_blocks):\n            x = self.forward_block(x, block_index)\n            out = out + x * self.scale\n            x = self.relu(x)\n        out = self.relu(out)\n        return out\n\n\nclass MultiWay(nn.Module):\n    """"""Base class for constructing N-way modules (2-way, 3-way, and so on).""""""\n\n    def __init__(self, scale, block_cls, num_blocks):\n        super(MultiWay, self).__init__()\n        assert num_blocks >= 1, \'num_blocks should be greater or equal to 1\'\n        self.scale = scale\n        self.blocks = nn.ModuleList([block_cls() for _ in range(num_blocks)])\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = x\n        for block in self.blocks:\n            out = out + block(x) * self.scale\n        out = self.relu(out)\n        return out\n\n\n# Some helper classes to simplify the construction of PolyNet\n\nclass InceptionResNetA2Way(MultiWay):\n\n    def __init__(self, scale):\n        super(InceptionResNetA2Way, self).__init__(scale, block_cls=BlockA,\n                                                   num_blocks=2)\n\n\nclass InceptionResNetB2Way(MultiWay):\n\n    def __init__(self, scale):\n        super(InceptionResNetB2Way, self).__init__(scale, block_cls=BlockB,\n                                                   num_blocks=2)\n\n\nclass InceptionResNetC2Way(MultiWay):\n\n    def __init__(self, scale):\n        super(InceptionResNetC2Way, self).__init__(scale, block_cls=BlockC,\n                                                   num_blocks=2)\n\n\nclass InceptionResNetBPoly3(InceptionResNetBPoly):\n\n    def __init__(self, scale):\n        super(InceptionResNetBPoly3, self).__init__(scale, num_blocks=3)\n\n\nclass InceptionResNetCPoly3(InceptionResNetCPoly):\n\n    def __init__(self, scale):\n        super(InceptionResNetCPoly3, self).__init__(scale, num_blocks=3)\n\n\nclass PolyNet(nn.Module):\n\n    def __init__(self, num_classes=1000):\n        super(PolyNet, self).__init__()\n        self.stem = Stem()\n        self.stage_a = nn.Sequential(\n            InceptionResNetA2Way(scale=1),\n            InceptionResNetA2Way(scale=0.992308),\n            InceptionResNetA2Way(scale=0.984615),\n            InceptionResNetA2Way(scale=0.976923),\n            InceptionResNetA2Way(scale=0.969231),\n            InceptionResNetA2Way(scale=0.961538),\n            InceptionResNetA2Way(scale=0.953846),\n            InceptionResNetA2Way(scale=0.946154),\n            InceptionResNetA2Way(scale=0.938462),\n            InceptionResNetA2Way(scale=0.930769),\n        )\n        self.reduction_a = ReductionA()\n        self.stage_b = nn.Sequential(\n            InceptionResNetBPoly3(scale=0.923077),\n            InceptionResNetB2Way(scale=0.915385),\n            InceptionResNetBPoly3(scale=0.907692),\n            InceptionResNetB2Way(scale=0.9),\n            InceptionResNetBPoly3(scale=0.892308),\n            InceptionResNetB2Way(scale=0.884615),\n            InceptionResNetBPoly3(scale=0.876923),\n            InceptionResNetB2Way(scale=0.869231),\n            InceptionResNetBPoly3(scale=0.861538),\n            InceptionResNetB2Way(scale=0.853846),\n            InceptionResNetBPoly3(scale=0.846154),\n            InceptionResNetB2Way(scale=0.838462),\n            InceptionResNetBPoly3(scale=0.830769),\n            InceptionResNetB2Way(scale=0.823077),\n            InceptionResNetBPoly3(scale=0.815385),\n            InceptionResNetB2Way(scale=0.807692),\n            InceptionResNetBPoly3(scale=0.8),\n            InceptionResNetB2Way(scale=0.792308),\n            InceptionResNetBPoly3(scale=0.784615),\n            InceptionResNetB2Way(scale=0.776923),\n        )\n        self.reduction_b = ReductionB()\n        self.stage_c = nn.Sequential(\n            InceptionResNetCPoly3(scale=0.769231),\n            InceptionResNetC2Way(scale=0.761538),\n            InceptionResNetCPoly3(scale=0.753846),\n            InceptionResNetC2Way(scale=0.746154),\n            InceptionResNetCPoly3(scale=0.738462),\n            InceptionResNetC2Way(scale=0.730769),\n            InceptionResNetCPoly3(scale=0.723077),\n            InceptionResNetC2Way(scale=0.715385),\n            InceptionResNetCPoly3(scale=0.707692),\n            InceptionResNetC2Way(scale=0.7),\n        )\n        self.avg_pool = nn.AvgPool2d(9, stride=1)\n        self.dropout = nn.Dropout(0.2)\n        self.last_linear = nn.Linear(2048, num_classes)\n\n    def features(self, x):\n        x = self.stem(x)\n        x = self.stage_a(x)\n        x = self.reduction_a(x)\n        x = self.stage_b(x)\n        x = self.reduction_b(x)\n        x = self.stage_c(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef polynet(num_classes=1000, pretrained=\'imagenet\'):\n    """"""PolyNet architecture from the paper\n    \'PolyNet: A Pursuit of Structural Diversity in Very Deep Networks\'\n    https://arxiv.org/abs/1611.05725\n    """"""\n    if pretrained:\n        settings = pretrained_settings[\'polynet\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            \'num_classes should be {}, but is {}\'.format(\n                settings[\'num_classes\'], num_classes)\n        model = PolyNet(num_classes=num_classes)\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    else:\n        model = PolyNet(num_classes=num_classes)\n    return model\n'"
pretrainedmodels/models/resnext.py,2,"b'from __future__ import print_function, division, absolute_import\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nfrom .resnext_features import resnext101_32x4d_features\nfrom .resnext_features import resnext101_64x4d_features\n\n__all__ = [\'ResNeXt101_32x4d\', \'resnext101_32x4d\',\n           \'ResNeXt101_64x4d\', \'resnext101_64x4d\']\n\npretrained_settings = {\n    \'resnext101_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/resnext101_32x4d-29e315fa.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'resnext101_64x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/resnext101_64x4d-e77a0586.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    }\n}\n\nclass ResNeXt101_32x4d(nn.Module):\n\n    def __init__(self, num_classes=1000):\n        super(ResNeXt101_32x4d, self).__init__()\n        self.num_classes = num_classes\n        self.features = resnext101_32x4d_features\n        self.avg_pool = nn.AvgPool2d((7, 7), (1, 1))\n        self.last_linear = nn.Linear(2048, num_classes)\n\n    def logits(self, input):\n        x = self.avg_pool(input)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\nclass ResNeXt101_64x4d(nn.Module):\n\n    def __init__(self, num_classes=1000):\n        super(ResNeXt101_64x4d, self).__init__()\n        self.num_classes = num_classes\n        self.features = resnext101_64x4d_features\n        self.avg_pool = nn.AvgPool2d((7, 7), (1, 1))\n        self.last_linear = nn.Linear(2048, num_classes)\n\n    def logits(self, input):\n        x = self.avg_pool(input)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\ndef resnext101_32x4d(num_classes=1000, pretrained=\'imagenet\'):\n    model = ResNeXt101_32x4d(num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'resnext101_32x4d\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    return model\n\ndef resnext101_64x4d(num_classes=1000, pretrained=\'imagenet\'):\n    model = ResNeXt101_64x4d(num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'resnext101_64x4d\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    return model\n'"
pretrainedmodels/models/senet.py,2,"b'""""""\nResNet code gently borrowed from\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n""""""\nfrom __future__ import print_function, division, absolute_import\nfrom collections import OrderedDict\nimport math\n\nimport torch.nn as nn\nfrom torch.utils import model_zoo\n\n__all__ = [\'SENet\', \'senet154\', \'se_resnet50\', \'se_resnet101\', \'se_resnet152\',\n           \'se_resnext50_32x4d\', \'se_resnext101_32x4d\']\n\npretrained_settings = {\n    \'senet154\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet50\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet101\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet152\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnext50_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnext101_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    """"""\n    Base class for bottlenecks that implements `forward()` method.\n    """"""\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    """"""\n    Bottleneck for SENet154.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    """"""\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n                               stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    """"""\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width / 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n                               stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        """"""\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        """"""\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n                                    bias=False)),\n                (\'bn1\', nn.BatchNorm2d(64)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n                (\'conv2\', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n                                    bias=False)),\n                (\'bn2\', nn.BatchNorm2d(64)),\n                (\'relu2\', nn.ReLU(inplace=True)),\n                (\'conv3\', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n                                    bias=False)),\n                (\'bn3\', nn.BatchNorm2d(inplanes)),\n                (\'relu3\', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3, bias=False)),\n                (\'bn1\', nn.BatchNorm2d(inplanes)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append((\'pool\', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings[\'num_classes\'], \\\n        \'num_classes should be {}, but is {}\'.format(\n            settings[\'num_classes\'], num_classes)\n    model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n    model.input_space = settings[\'input_space\']\n    model.input_size = settings[\'input_size\']\n    model.input_range = settings[\'input_range\']\n    model.mean = settings[\'mean\']\n    model.std = settings[\'std\']\n\n\ndef senet154(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n                  dropout_p=0.2, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'senet154\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet50(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet50\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet101(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet101\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet152(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet152\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext50_32x4d(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnext50_32x4d\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext101_32x4d(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnext101_32x4d\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n'"
pretrainedmodels/models/torchvision_models.py,19,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function, division, absolute_import\nimport torchvision.models as models\nimport torch.utils.model_zoo as model_zoo\nimport torch.nn.functional as F\nimport types\nimport re\n\n#################################################################\n# You can find the definitions of those models here:\n# https://github.com/pytorch/vision/blob/master/torchvision/models\n#\n# To fit the API, we usually added/redefined some methods and\n# renamed some attributs (see below for each models).\n#\n# However, you usually do not need to see the original model\n# definition from torchvision. Just use `print(model)` to see\n# the modules and see bellow the `model.features` and\n# `model.classifier` definitions.\n#################################################################\n\n__all__ = [\n    \'alexnet\',\n    \'densenet121\', \'densenet169\', \'densenet201\', \'densenet161\',\n    \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\', \'resnet152\',\n    \'inceptionv3\',\n    \'squeezenet1_0\', \'squeezenet1_1\',\n    \'vgg11\', \'vgg11_bn\', \'vgg13\', \'vgg13_bn\', \'vgg16\', \'vgg16_bn\',\n    \'vgg19_bn\', \'vgg19\'\n]\n\nmodel_urls = {\n    \'alexnet\': \'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\',\n    \'densenet121\': \'http://data.lip6.fr/cadene/pretrainedmodels/densenet121-fbdb23505.pth\',\n    \'densenet169\': \'http://data.lip6.fr/cadene/pretrainedmodels/densenet169-f470b90a4.pth\',\n    \'densenet201\': \'http://data.lip6.fr/cadene/pretrainedmodels/densenet201-5750cbb1e.pth\',\n    \'densenet161\': \'http://data.lip6.fr/cadene/pretrainedmodels/densenet161-347e6b360.pth\',\n    \'inceptionv3\': \'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\',\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n    \'squeezenet1_0\': \'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth\',\n    \'squeezenet1_1\': \'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth\',\n    \'vgg11\': \'https://download.pytorch.org/models/vgg11-bbd30ac9.pth\',\n    \'vgg13\': \'https://download.pytorch.org/models/vgg13-c768596a.pth\',\n    \'vgg16\': \'https://download.pytorch.org/models/vgg16-397923af.pth\',\n    \'vgg19\': \'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\',\n    \'vgg11_bn\': \'https://download.pytorch.org/models/vgg11_bn-6002323d.pth\',\n    \'vgg13_bn\': \'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth\',\n    \'vgg16_bn\': \'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\',\n    \'vgg19_bn\': \'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\',\n    # \'vgg16_caffe\': \'https://s3-us-west-2.amazonaws.com/jcjohns-models/vgg16-00b39a1b.pth\',\n    # \'vgg19_caffe\': \'https://s3-us-west-2.amazonaws.com/jcjohns-models/vgg19-d01eb7cb.pth\'\n}\n\ninput_sizes = {}\nmeans = {}\nstds = {}\n\nfor model_name in __all__:\n    input_sizes[model_name] = [3, 224, 224]\n    means[model_name] = [0.485, 0.456, 0.406]\n    stds[model_name] = [0.229, 0.224, 0.225]\n\nfor model_name in [\'inceptionv3\']:\n    input_sizes[model_name] = [3, 299, 299]\n    means[model_name] = [0.5, 0.5, 0.5]\n    stds[model_name] = [0.5, 0.5, 0.5]\n\npretrained_settings = {}\n\nfor model_name in __all__:\n    pretrained_settings[model_name] = {\n        \'imagenet\': {\n            \'url\': model_urls[model_name],\n            \'input_space\': \'RGB\',\n            \'input_size\': input_sizes[model_name],\n            \'input_range\': [0, 1],\n            \'mean\': means[model_name],\n            \'std\': stds[model_name],\n            \'num_classes\': 1000\n        }\n    }\n\n# for model_name in [\'vgg16\', \'vgg19\']:\n#     pretrained_settings[model_name][\'imagenet_caffe\'] = {\n#         \'url\': model_urls[model_name + \'_caffe\'],\n#         \'input_space\': \'BGR\',\n#         \'input_size\': input_sizes[model_name],\n#         \'input_range\': [0, 255],\n#         \'mean\': [103.939, 116.779, 123.68],\n#         \'std\': [1., 1., 1.],\n#         \'num_classes\': 1000\n#     }\n\ndef update_state_dict(state_dict):\n    # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\n    # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n    # They are also in the checkpoints in model_urls. This pattern is used\n    # to find such keys.\n    pattern = re.compile(\n        r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n    for key in list(state_dict.keys()):\n        res = pattern.match(key)\n        if res:\n            new_key = res.group(1) + res.group(2)\n            state_dict[new_key] = state_dict[key]\n            del state_dict[key]\n    return state_dict\n\ndef load_pretrained(model, num_classes, settings):\n    assert num_classes == settings[\'num_classes\'], \\\n        ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n    state_dict = model_zoo.load_url(settings[\'url\'])\n    state_dict = update_state_dict(state_dict)\n    model.load_state_dict(state_dict)\n    model.input_space = settings[\'input_space\']\n    model.input_size = settings[\'input_size\']\n    model.input_range = settings[\'input_range\']\n    model.mean = settings[\'mean\']\n    model.std = settings[\'std\']\n    return model\n\n#################################################################\n# AlexNet\n\ndef modify_alexnet(model):\n    # Modify attributs\n    model._features = model.features\n    del model.features\n    model.dropout0 = model.classifier[0]\n    model.linear0 = model.classifier[1]\n    model.relu0 = model.classifier[2]\n    model.dropout1 = model.classifier[3]\n    model.linear1 = model.classifier[4]\n    model.relu1 = model.classifier[5]\n    model.last_linear = model.classifier[6]\n    del model.classifier\n\n    def features(self, input):\n        x = self._features(input)\n        x = x.view(x.size(0), 256 * 6 * 6)\n        x = self.dropout0(x)\n        x = self.linear0(x)\n        x = self.relu0(x)\n        x = self.dropout1(x)\n        x = self.linear1(x)\n        return x\n\n    def logits(self, features):\n        x = self.relu1(features)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n    # Modify methods\n    model.features = types.MethodType(features, model)\n    model.logits = types.MethodType(logits, model)\n    model.forward = types.MethodType(forward, model)\n    return model\n\ndef alexnet(num_classes=1000, pretrained=\'imagenet\'):\n    r""""""AlexNet model architecture from the\n    `""One weird trick..."" <https://arxiv.org/abs/1404.5997>`_ paper.\n    """"""\n    # https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py\n    model = models.alexnet(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'alexnet\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_alexnet(model)\n    return model\n\n###############################################################\n#\xc2\xa0DenseNets\n\ndef modify_densenets(model):\n    # Modify attributs\n    model.last_linear = model.classifier\n    del model.classifier\n\n    def logits(self, features):\n        x = F.relu(features, inplace=True)\n        x = F.avg_pool2d(x, kernel_size=7, stride=1)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n    # Modify methods\n    model.logits = types.MethodType(logits, model)\n    model.forward = types.MethodType(forward, model)\n    return model\n\ndef densenet121(num_classes=1000, pretrained=\'imagenet\'):\n    r""""""Densenet-121 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`\n    """"""\n    model = models.densenet121(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'densenet121\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_densenets(model)\n    return model\n\ndef densenet169(num_classes=1000, pretrained=\'imagenet\'):\n    r""""""Densenet-169 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`\n    """"""\n    model = models.densenet169(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'densenet169\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_densenets(model)\n    return model\n\ndef densenet201(num_classes=1000, pretrained=\'imagenet\'):\n    r""""""Densenet-201 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`\n    """"""\n    model = models.densenet201(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'densenet201\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_densenets(model)\n    return model\n\ndef densenet161(num_classes=1000, pretrained=\'imagenet\'):\n    r""""""Densenet-161 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`\n    """"""\n    model = models.densenet161(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'densenet161\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_densenets(model)\n    return model\n\n###############################################################\n#\xc2\xa0InceptionV3\n\ndef inceptionv3(num_classes=1000, pretrained=\'imagenet\'):\n    r""""""Inception v3 model architecture from\n    `""Rethinking the Inception Architecture for Computer Vision"" <http://arxiv.org/abs/1512.00567>`_.\n    """"""\n    model = models.inception_v3(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'inceptionv3\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n\n    # Modify attributs\n    model.last_linear = model.fc\n    del model.fc\n\n    def features(self, input):\n        # 299 x 299 x 3\n        x = self.Conv2d_1a_3x3(input) # 149 x 149 x 32\n        x = self.Conv2d_2a_3x3(x) # 147 x 147 x 32\n        x = self.Conv2d_2b_3x3(x) # 147 x 147 x 64\n        x = F.max_pool2d(x, kernel_size=3, stride=2) # 73 x 73 x 64\n        x = self.Conv2d_3b_1x1(x) # 73 x 73 x 80\n        x = self.Conv2d_4a_3x3(x) # 71 x 71 x 192\n        x = F.max_pool2d(x, kernel_size=3, stride=2) # 35 x 35 x 192\n        x = self.Mixed_5b(x) # 35 x 35 x 256\n        x = self.Mixed_5c(x) # 35 x 35 x 288\n        x = self.Mixed_5d(x) # 35 x 35 x 288\n        x = self.Mixed_6a(x) # 17 x 17 x 768\n        x = self.Mixed_6b(x) # 17 x 17 x 768\n        x = self.Mixed_6c(x) # 17 x 17 x 768\n        x = self.Mixed_6d(x) # 17 x 17 x 768\n        x = self.Mixed_6e(x) # 17 x 17 x 768\n        if self.training and self.aux_logits:\n            self._out_aux = self.AuxLogits(x) # 17 x 17 x 768\n        x = self.Mixed_7a(x) # 8 x 8 x 1280\n        x = self.Mixed_7b(x) # 8 x 8 x 2048\n        x = self.Mixed_7c(x) # 8 x 8 x 2048\n        return x\n\n    def logits(self, features):\n        x = F.avg_pool2d(features, kernel_size=8) # 1 x 1 x 2048\n        x = F.dropout(x, training=self.training) # 1 x 1 x 2048\n        x = x.view(x.size(0), -1) # 2048\n        x = self.last_linear(x) # 1000 (num_classes)\n        if self.training and self.aux_logits:\n            aux = self._out_aux\n            self._out_aux = None\n            return x, aux\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n    # Modify methods\n    model.features = types.MethodType(features, model)\n    model.logits = types.MethodType(logits, model)\n    model.forward = types.MethodType(forward, model)\n    return model\n\n###############################################################\n#\xc2\xa0ResNets\n\ndef modify_resnets(model):\n    # Modify attributs\n    model.last_linear = model.fc\n    model.fc = None\n\n    def features(self, input):\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, features):\n        x = self.avgpool(features)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n    # Modify methods\n    model.features = types.MethodType(features, model)\n    model.logits = types.MethodType(logits, model)\n    model.forward = types.MethodType(forward, model)\n    return model\n\ndef resnet18(num_classes=1000, pretrained=\'imagenet\'):\n    """"""Constructs a ResNet-18 model.\n    """"""\n    model = models.resnet18(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'resnet18\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_resnets(model)\n    return model\n\ndef resnet34(num_classes=1000, pretrained=\'imagenet\'):\n    """"""Constructs a ResNet-34 model.\n    """"""\n    model = models.resnet34(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'resnet34\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_resnets(model)\n    return model\n\ndef resnet50(num_classes=1000, pretrained=\'imagenet\'):\n    """"""Constructs a ResNet-50 model.\n    """"""\n    model = models.resnet50(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'resnet50\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_resnets(model)\n    return model\n\ndef resnet101(num_classes=1000, pretrained=\'imagenet\'):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = models.resnet101(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'resnet101\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_resnets(model)\n    return model\n\ndef resnet152(num_classes=1000, pretrained=\'imagenet\'):\n    """"""Constructs a ResNet-152 model.\n    """"""\n    model = models.resnet152(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'resnet152\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_resnets(model)\n    return model\n\n###############################################################\n#\xc2\xa0SqueezeNets\n\ndef modify_squeezenets(model):\n    # /!\\ Beware squeezenets do not have any last_linear module\n\n    # Modify attributs\n    model.dropout = model.classifier[0]\n    model.last_conv = model.classifier[1]\n    model.relu = model.classifier[2]\n    model.avgpool = model.classifier[3]\n    del model.classifier\n\n    def logits(self, features):\n        x = self.dropout(features)\n        x = self.last_conv(x)\n        x = self.relu(x)\n        x = self.avgpool(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n    # Modify methods\n    model.logits = types.MethodType(logits, model)\n    model.forward = types.MethodType(forward, model)\n    return model\n\ndef squeezenet1_0(num_classes=1000, pretrained=\'imagenet\'):\n    r""""""SqueezeNet model architecture from the `""SqueezeNet: AlexNet-level\n    accuracy with 50x fewer parameters and <0.5MB model size""\n    <https://arxiv.org/abs/1602.07360>`_ paper.\n    """"""\n    model = models.squeezenet1_0(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'squeezenet1_0\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_squeezenets(model)\n    return model\n\ndef squeezenet1_1(num_classes=1000, pretrained=\'imagenet\'):\n    r""""""SqueezeNet 1.1 model from the `official SqueezeNet repo\n    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n    than SqueezeNet 1.0, without sacrificing accuracy.\n    """"""\n    model = models.squeezenet1_1(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'squeezenet1_1\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_squeezenets(model)\n    return model\n\n###############################################################\n#\xc2\xa0VGGs\n\ndef modify_vggs(model):\n    # Modify attributs\n    model._features = model.features\n    del model.features\n    model.linear0 = model.classifier[0]\n    model.relu0 = model.classifier[1]\n    model.dropout0 = model.classifier[2]\n    model.linear1 = model.classifier[3]\n    model.relu1 = model.classifier[4]\n    model.dropout1 = model.classifier[5]\n    model.last_linear = model.classifier[6]\n    del model.classifier\n\n    def features(self, input):\n        x = self._features(input)\n        x = x.view(x.size(0), -1)\n        x = self.linear0(x)\n        x = self.relu0(x)\n        x = self.dropout0(x)\n        x = self.linear1(x)\n        return x\n\n    def logits(self, features):\n        x = self.relu1(features)\n        x = self.dropout1(x)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n    # Modify methods\n    model.features = types.MethodType(features, model)\n    model.logits = types.MethodType(logits, model)\n    model.forward = types.MethodType(forward, model)\n    return model\n\ndef vgg11(num_classes=1000, pretrained=\'imagenet\'):\n    """"""VGG 11-layer model (configuration ""A"")\n    """"""\n    model = models.vgg11(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'vgg11\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_vggs(model)\n    return model\n\ndef vgg11_bn(num_classes=1000, pretrained=\'imagenet\'):\n    """"""VGG 11-layer model (configuration ""A"") with batch normalization\n    """"""\n    model = models.vgg11_bn(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'vgg11_bn\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_vggs(model)\n    return model\n\ndef vgg13(num_classes=1000, pretrained=\'imagenet\'):\n    """"""VGG 13-layer model (configuration ""B"")\n    """"""\n    model = models.vgg13(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'vgg13\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_vggs(model)\n    return model\n\ndef vgg13_bn(num_classes=1000, pretrained=\'imagenet\'):\n    """"""VGG 13-layer model (configuration ""B"") with batch normalization\n    """"""\n    model = models.vgg13_bn(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'vgg13_bn\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_vggs(model)\n    return model\n\ndef vgg16(num_classes=1000, pretrained=\'imagenet\'):\n    """"""VGG 16-layer model (configuration ""D"")\n    """"""\n    model = models.vgg16(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'vgg16\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_vggs(model)\n    return model\n\ndef vgg16_bn(num_classes=1000, pretrained=\'imagenet\'):\n    """"""VGG 16-layer model (configuration ""D"") with batch normalization\n    """"""\n    model = models.vgg16_bn(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'vgg16_bn\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_vggs(model)\n    return model\n\ndef vgg19(num_classes=1000, pretrained=\'imagenet\'):\n    """"""VGG 19-layer model (configuration ""E"")\n    """"""\n    model = models.vgg19(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'vgg19\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_vggs(model)\n    return model\n\ndef vgg19_bn(num_classes=1000, pretrained=\'imagenet\'):\n    """"""VGG 19-layer model (configuration \'E\') with batch normalization\n    """"""\n    model = models.vgg19_bn(pretrained=False)\n    if pretrained is not None:\n        settings = pretrained_settings[\'vgg19_bn\'][pretrained]\n        model = load_pretrained(model, num_classes, settings)\n    model = modify_vggs(model)\n    return model\n'"
pretrainedmodels/models/utils.py,0,"b'from __future__ import print_function, division, absolute_import\nfrom .fbresnet import pretrained_settings as fbresnet_settings\nfrom .bninception import pretrained_settings as bninception_settings\nfrom .resnext import pretrained_settings as resnext_settings\nfrom .inceptionv4 import pretrained_settings as inceptionv4_settings\nfrom .inceptionresnetv2 import pretrained_settings as inceptionresnetv2_settings\nfrom .torchvision_models import pretrained_settings as torchvision_models_settings\nfrom .nasnet_mobile import pretrained_settings as nasnet_mobile_settings\nfrom .nasnet import pretrained_settings as nasnet_settings\nfrom .dpn import pretrained_settings as dpn_settings\nfrom .xception import pretrained_settings as xception_settings\nfrom .senet import pretrained_settings as senet_settings\nfrom .cafferesnet import pretrained_settings as cafferesnet_settings\nfrom .pnasnet import pretrained_settings as pnasnet_settings\nfrom .polynet import pretrained_settings as polynet_settings\n\nall_settings = [\n    fbresnet_settings,\n    bninception_settings,\n    resnext_settings,\n    inceptionv4_settings,\n    inceptionresnetv2_settings,\n    torchvision_models_settings,\n    nasnet_mobile_settings,\n    nasnet_settings,\n    dpn_settings,\n    xception_settings,\n    senet_settings,\n    cafferesnet_settings,\n    pnasnet_settings,\n    polynet_settings\n]\n\nmodel_names = []\npretrained_settings = {}\nfor settings in all_settings:\n    for model_name, model_settings in settings.items():\n        pretrained_settings[model_name] = model_settings\n        model_names.append(model_name)\n'"
pretrainedmodels/models/vggm.py,4,"b'from __future__ import print_function, division, absolute_import\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n#from torch.legacy import nn as nnl\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [\'vggm\']\n\npretrained_settings = {\n    \'vggm\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/vggm-786f2434.pth\',\n            \'input_space\': \'BGR\',\n            \'input_size\': [3, 221, 221],\n            \'input_range\': [0, 255],\n            \'mean\': [123.68, 116.779, 103.939],\n            \'std\': [1, 1, 1],\n            \'num_classes\': 1000\n        }\n    }\n}\n\nclass SpatialCrossMapLRN(nn.Module):\n    def __init__(self, local_size=1, alpha=1.0, beta=0.75, k=1, ACROSS_CHANNELS=True):\n        super(SpatialCrossMapLRN, self).__init__()\n        self.ACROSS_CHANNELS = ACROSS_CHANNELS\n        if ACROSS_CHANNELS:\n            self.average=nn.AvgPool3d(kernel_size=(local_size, 1, 1),\n                    stride=1,\n                    padding=(int((local_size-1.0)/2), 0, 0))\n        else:\n            self.average=nn.AvgPool2d(kernel_size=local_size,\n                    stride=1,\n                    padding=int((local_size-1.0)/2))\n        self.alpha = alpha\n        self.beta = beta\n        self.k = k\n\n    def forward(self, x):\n        if self.ACROSS_CHANNELS:\n            div = x.pow(2).unsqueeze(1)\n            div = self.average(div).squeeze(1)\n            div = div.mul(self.alpha).add(self.k).pow(self.beta)\n        else:\n            div = x.pow(2)\n            div = self.average(div)\n            div = div.mul(self.alpha).add(self.k).pow(self.beta)\n        x = x.div(div)\n        return x\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, fn, *args):\n        super(LambdaBase, self).__init__(*args)\n        self.lambda_func = fn\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\nclass Lambda(LambdaBase):\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\nclass VGGM(nn.Module):\n\n    def __init__(self, num_classes=1000):\n        super(VGGM, self).__init__()\n        self.num_classes = num_classes\n        self.features = nn.Sequential(\n            nn.Conv2d(3,96,(7, 7),(2, 2)),\n            nn.ReLU(),\n            SpatialCrossMapLRN(5, 0.0005, 0.75, 2),\n            nn.MaxPool2d((3, 3),(2, 2),(0, 0),ceil_mode=True),\n            nn.Conv2d(96,256,(5, 5),(2, 2),(1, 1)),\n            nn.ReLU(),\n            SpatialCrossMapLRN(5, 0.0005, 0.75, 2),\n            nn.MaxPool2d((3, 3),(2, 2),(0, 0),ceil_mode=True),\n            nn.Conv2d(256,512,(3, 3),(1, 1),(1, 1)),\n            nn.ReLU(),\n            nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1)),\n            nn.ReLU(),\n            nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1)),\n            nn.ReLU(),\n            nn.MaxPool2d((3, 3),(2, 2),(0, 0),ceil_mode=True)\n        )\n        self.classif = nn.Sequential(\n            nn.Linear(18432,4096),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(4096,4096),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(4096,num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classif(x)\n        return x\n\ndef vggm(num_classes=1000, pretrained=\'imagenet\'):\n    if pretrained:\n        settings = pretrained_settings[\'vggm\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n\n        model = VGGM(num_classes=1000)\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n    else:\n        model = VGGM(num_classes=num_classes)\n    return model'"
pretrainedmodels/models/wideresnet.py,3,"b'from __future__ import print_function, division, absolute_import\nimport os\nfrom os.path import expanduser\nimport hickle as hkl\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n__all__ = [\'wideresnet50\']\n\nmodel_urls = {\n    \'wideresnet152\': \'https://s3.amazonaws.com/pytorch/h5models/wide-resnet-50-2-export.hkl\'\n}\n\ndef define_model(params):\n    def conv2d(input, params, base, stride=1, pad=0):\n        return F.conv2d(input, params[base + \'.weight\'],\n                        params[base + \'.bias\'], stride, pad)\n\n    def group(input, params, base, stride, n):\n        o = input\n        for i in range(0,n):\n            b_base = (\'%s.block%d.conv\') % (base, i)\n            x = o\n            o = conv2d(x, params, b_base + \'0\')\n            o = F.relu(o)\n            o = conv2d(o, params, b_base + \'1\', stride=i==0 and stride or 1, pad=1)\n            o = F.relu(o)\n            o = conv2d(o, params, b_base + \'2\')\n            if i == 0:\n                o += conv2d(x, params, b_base + \'_dim\', stride=stride)\n            else:\n                o += x\n            o = F.relu(o)\n        return o\n\n    # determine network size by parameters\n    blocks = [sum([re.match(\'group%d.block\\d+.conv0.weight\'%j, k) is not None\n                   for k in params.keys()]) for j in range(4)]\n\n    def f(input, params, pooling_classif=True):\n        o = F.conv2d(input, params[\'conv0.weight\'], params[\'conv0.bias\'], 2, 3)\n        o = F.relu(o)\n        o = F.max_pool2d(o, 3, 2, 1)\n        o_g0 = group(o, params, \'group0\', 1, blocks[0])\n        o_g1 = group(o_g0, params, \'group1\', 2, blocks[1])\n        o_g2 = group(o_g1, params, \'group2\', 2, blocks[2])\n        o_g3 = group(o_g2, params, \'group3\', 2, blocks[3])\n        if pooling_classif:\n            o = F.avg_pool2d(o_g3, 7, 1, 0)\n            o = o.view(o.size(0), -1)\n            o = F.linear(o, params[\'fc.weight\'], params[\'fc.bias\'])\n        return o\n\n    return f\n\n\nclass WideResNet(nn.Module):\n\n    def __init__(self, pooling):\n        super(WideResNet, self).__init__()\n        self.pooling = pooling\n        self.params = params\n\n    def forward(self, x):\n        x = f(x, self.params, self.pooling)\n        return x\n\n\ndef wideresnet50(pooling):\n    dir_models = os.path.join(expanduser(""~""), \'.torch/wideresnet\')\n    path_hkl = os.path.join(dir_models, \'wideresnet50.hkl\')\n    if os.path.isfile(path_hkl):\n        params = hkl.load(path_hkl)\n        # convert numpy arrays to torch Variables\n        for k,v in sorted(params.items()):\n            print(k, v.shape)\n            params[k] = Variable(torch.from_numpy(v), requires_grad=True)\n    else:\n        os.system(\'mkdir -p \' + dir_models)\n        os.system(\'wget {} -O {}\'.format(model_urls[\'wideresnet50\'], path_hkl))\n    f = define_model(params)\n    model = WideResNet(pooling)\n    return model\n\n\n'"
pretrainedmodels/models/xception.py,4,"b'""""""\nPorted to pytorch thanks to [tstandley](https://github.com/tstandley/Xception-PyTorch)\n\n@author: tstandley\nAdapted by cadene\n\nCreates an Xception Model as defined in:\n\nFrancois Chollet\nXception: Deep Learning with Depthwise Separable Convolutions\nhttps://arxiv.org/pdf/1610.02357.pdf\n\nThis weights ported from the Keras implementation. Achieves the following performance on the validation set:\n\nLoss:0.9173 Prec@1:78.892 Prec@5:94.292\n\nREMEMBER to set your image size to 3x299x299 for both test and validation\n\nnormalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                  std=[0.5, 0.5, 0.5])\n\nThe resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n""""""\nfrom __future__ import print_function, division, absolute_import\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom torch.nn import init\n\n__all__ = [\'xception\']\n\npretrained_settings = {\n    \'xception\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/xception-43020ad28.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 299, 299],\n            \'input_range\': [0, 1],\n            \'mean\': [0.5, 0.5, 0.5],\n            \'std\': [0.5, 0.5, 0.5],\n            \'num_classes\': 1000,\n            \'scale\': 0.8975 # The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n        }\n    }\n}\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n        super(SeparableConv2d,self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n\n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(self,in_filters,out_filters,reps,strides=1,start_with_relu=True,grow_first=True):\n        super(Block, self).__init__()\n\n        if out_filters != in_filters or strides!=1:\n            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n            self.skipbn = nn.BatchNorm2d(out_filters)\n        else:\n            self.skip=None\n\n        rep=[]\n\n        filters=in_filters\n        if grow_first:\n            rep.append(nn.ReLU(inplace=True))\n            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n            rep.append(nn.BatchNorm2d(out_filters))\n            filters = out_filters\n\n        for i in range(reps-1):\n            rep.append(nn.ReLU(inplace=True))\n            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n            rep.append(nn.BatchNorm2d(filters))\n\n        if not grow_first:\n            rep.append(nn.ReLU(inplace=True))\n            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n            rep.append(nn.BatchNorm2d(out_filters))\n\n        if not start_with_relu:\n            rep = rep[1:]\n        else:\n            rep[0] = nn.ReLU(inplace=False)\n\n        if strides != 1:\n            rep.append(nn.MaxPool2d(3,strides,1))\n        self.rep = nn.Sequential(*rep)\n\n    def forward(self,inp):\n        x = self.rep(inp)\n\n        if self.skip is not None:\n            skip = self.skip(inp)\n            skip = self.skipbn(skip)\n        else:\n            skip = inp\n\n        x+=skip\n        return x\n\n\nclass Xception(nn.Module):\n    """"""\n    Xception optimized for the ImageNet dataset, as specified in\n    https://arxiv.org/pdf/1610.02357.pdf\n    """"""\n    def __init__(self, num_classes=1000):\n        """""" Constructor\n        Args:\n            num_classes: number of classes\n        """"""\n        super(Xception, self).__init__()\n        self.num_classes = num_classes\n\n        self.conv1 = nn.Conv2d(3, 32, 3,2, 0, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.relu1 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.relu2 = nn.ReLU(inplace=True)\n        #do relu here\n\n        self.block1=Block(64,128,2,2,start_with_relu=False,grow_first=True)\n        self.block2=Block(128,256,2,2,start_with_relu=True,grow_first=True)\n        self.block3=Block(256,728,2,2,start_with_relu=True,grow_first=True)\n\n        self.block4=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block5=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block6=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block7=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n\n        self.block8=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block9=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block10=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block11=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n\n        self.block12=Block(728,1024,2,2,start_with_relu=True,grow_first=False)\n\n        self.conv3 = SeparableConv2d(1024,1536,3,1,1)\n        self.bn3 = nn.BatchNorm2d(1536)\n        self.relu3 = nn.ReLU(inplace=True)\n\n        #do relu here\n        self.conv4 = SeparableConv2d(1536,2048,3,1,1)\n        self.bn4 = nn.BatchNorm2d(2048)\n\n        self.fc = nn.Linear(2048, num_classes)\n\n        # #------- init weights --------\n        # for m in self.modules():\n        #     if isinstance(m, nn.Conv2d):\n        #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        #         m.weight.data.normal_(0, math.sqrt(2. / n))\n        #     elif isinstance(m, nn.BatchNorm2d):\n        #         m.weight.data.fill_(1)\n        #         m.bias.data.zero_()\n        # #-----------------------------\n\n    def features(self, input):\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.relu1(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.block6(x)\n        x = self.block7(x)\n        x = self.block8(x)\n        x = self.block9(x)\n        x = self.block10(x)\n        x = self.block11(x)\n        x = self.block12(x)\n\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu3(x)\n\n        x = self.conv4(x)\n        x = self.bn4(x)\n        return x\n\n    def logits(self, features):\n        x = nn.ReLU(inplace=True)(features)\n\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\ndef xception(num_classes=1000, pretrained=\'imagenet\'):\n    model = Xception(num_classes=num_classes)\n    if pretrained:\n        settings = pretrained_settings[\'xception\'][pretrained]\n        assert num_classes == settings[\'num_classes\'], \\\n            ""num_classes should be {}, but is {}"".format(settings[\'num_classes\'], num_classes)\n\n        model = Xception(num_classes=num_classes)\n        model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n\n        model.input_space = settings[\'input_space\']\n        model.input_size = settings[\'input_size\']\n        model.input_range = settings[\'input_range\']\n        model.mean = settings[\'mean\']\n        model.std = settings[\'std\']\n\n    # TODO: ugly\n    model.last_linear = model.fc\n    del model.fc\n    return model\n'"
pretrainedmodels/models/fbresnet/resnet152_load.py,23,"b'from __future__ import print_function, division, absolute_import\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\n\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=True)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=True)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=True)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=True)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nfrom torch.legacy import nn as nnl\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                                bias=True)\n        #self.conv1 = nnl.SpatialConvolution(3, 64, 7, 7, 2, 2, 3, 3)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=True),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        self.conv1_input = x.clone()\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model\n\nimport torchfile\nfrom torch.utils.serialization import load_lua\nimport torch\nnetparams = torchfile.load(\'data/resnet152/netparams.t7\')\n#netparams2 = load_lua(\'data/resnet152/netparams.t7\')\n#import ipdb; ipdb.set_trace()\nnetoutputs = []\nfor i in range(1, 12):\n    path = \'data/resnet152/output{}.t7\'.format(i)\n    out = load_lua(path)\n    #print(out.size())\n    if out.dim()==4:\n        pass#out.transpose_(2, 3)\n    netoutputs.append(out)\n\nnet = resnet152()\nstate_dict = net.state_dict()\n\nimport collections\ns = collections.OrderedDict()\n\n\ni=0\nfor key in state_dict.keys():\n    new = torch.from_numpy(netparams[i])\n    s[key] = new\n    if s[key].dim() == 4:\n        pass#s[key].transpose_(2,3)\n    i += 1\n\nnet.load_state_dict(s)\n\nnet.conv1.register_forward_hook(lambda self, input, output: \\\n    print(\'conv1\', torch.dist(output.data, netoutputs[0])))\nnet.bn1.register_forward_hook(lambda self, input, output: \\\n    print(\'bn1\', torch.dist(output.data, netoutputs[1])))\nnet.relu.register_forward_hook(lambda self, input, output: \\\n    print(\'relu\', torch.dist(output.data, netoutputs[2])))\nnet.maxpool.register_forward_hook(lambda self, input, output: \\\n    print(\'maxpool\', torch.dist(output.data, netoutputs[3])))\nnet.layer1.register_forward_hook(lambda self, input, output: \\\n    print(\'layer1\', torch.dist(output.data, netoutputs[4])))\nnet.layer2.register_forward_hook(lambda self, input, output: \\\n    print(\'layer2\', torch.dist(output.data, netoutputs[5])))\nnet.layer3.register_forward_hook(lambda self, input, output: \\\n    print(\'layer3\', torch.dist(output.data, netoutputs[6])))\nnet.layer4.register_forward_hook(lambda self, input, output: \\\n    print(\'layer4\', torch.dist(output.data, netoutputs[7])))\nnet.avgpool.register_forward_hook(lambda self, input, output: \\\n    print(\'avgpool\', torch.dist(output.data, netoutputs[8])))\nnet.fc.register_forward_hook(lambda self, input, output: \\\n    print(\'fc\', torch.dist(output.data, netoutputs[10])))\n\nnet.eval()\ninput_data = torch.ones(1,3,224,224)\ninput_data[0][0][0][0] = -1\nfrom PIL import Image\nimport torchvision.transforms as transforms\ninput_data[0] = transforms.ToTensor()(Image.open(\'data/cat_224.png\'))\nprint(\'cat sum\', input_data.sum())\ninput = torch.autograd.Variable(input_data)\noutput = net.forward(input)\n\ntorch.save(s, \'data/resnet152.pth\')\n\n\n'"
pretrainedmodels/models/resnext_features/__init__.py,0,"b'from __future__ import print_function, division, absolute_import\nfrom .resnext101_32x4d_features import resnext101_32x4d_features\nfrom .resnext101_64x4d_features import resnext101_64x4d_features'"
pretrainedmodels/models/resnext_features/resnext101_32x4d_features.py,2,"b'from __future__ import print_function, division, absolute_import\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom functools import reduce\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, *args):\n        super(LambdaBase, self).__init__(*args)\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\nclass Lambda(LambdaBase):\n    def __init__(self, *args):\n        super(Lambda, self).__init__(*args)\n        self.lambda_func = identity\n\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\nclass LambdaMap(LambdaBase):\n    def __init__(self, *args):\n        super(LambdaMap, self).__init__(*args)\n        self.lambda_func = identity\n\n    def forward(self, input):\n        return list(map(self.lambda_func,self.forward_prepare(input)))\n\nclass LambdaReduce(LambdaBase):\n    def __init__(self, *args):\n        super(LambdaReduce, self).__init__(*args)\n        self.lambda_func = add\n\n    def forward(self, input):\n        return reduce(self.lambda_func,self.forward_prepare(input))\n\ndef identity(x): return x\n\ndef add(x, y): return x + y\n\nresnext101_32x4d_features = nn.Sequential( # Sequential,\n    nn.Conv2d(3,64,(7, 7),(2, 2),(3, 3),1,1,bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(),\n    nn.MaxPool2d((3, 3),(2, 2),(1, 1)),\n    nn.Sequential( # Sequential,\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(64,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(),\n                        nn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(256),\n                ),\n                nn.Sequential( # Sequential,\n                    nn.Conv2d(64,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(256),\n                ),\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(256,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(),\n                        nn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(256),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(256,128,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(),\n                        nn.Conv2d(128,128,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(128,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(256),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n    ),\n    nn.Sequential( # Sequential,\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(256,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                        nn.Conv2d(256,256,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(512),\n                ),\n                nn.Sequential( # Sequential,\n                    nn.Conv2d(256,512,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(512),\n                ),\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                        nn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(512),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                        nn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(512),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(512,256,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                        nn.Conv2d(256,256,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(256,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(512),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n    ),\n    nn.Sequential( # Sequential,\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(512,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                nn.Sequential( # Sequential,\n                    nn.Conv2d(512,1024,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,512,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512,512,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n    ),\n    nn.Sequential( # Sequential,\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(1024,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024,1024,(3, 3),(2, 2),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(2048),\n                ),\n                nn.Sequential( # Sequential,\n                    nn.Conv2d(1024,2048,(1, 1),(2, 2),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(2048),\n                ),\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(2048,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(2048),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential( # Sequential,\n            LambdaMap( # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Sequential( # Sequential,\n                        nn.Conv2d(2048,1024,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024,1024,(3, 3),(1, 1),(1, 1),1,32,bias=False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024,2048,(1, 1),(1, 1),(0, 0),1,1,bias=False),\n                    nn.BatchNorm2d(2048),\n                ),\n                Lambda(), # Identity,\n            ),\n            LambdaReduce(), # CAddTable,\n            nn.ReLU(),\n        ),\n    )\n)'"
pretrainedmodels/models/resnext_features/resnext101_64x4d_features.py,2,"b'from __future__ import print_function, division, absolute_import\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom functools import reduce\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, *args):\n        super(LambdaBase, self).__init__(*args)\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\nclass Lambda(LambdaBase):\n    def __init__(self, *args):\n        super(Lambda, self).__init__(*args)\n        self.lambda_func = identity\n\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\nclass LambdaMap(LambdaBase):\n    def __init__(self, *args):\n        super(LambdaMap, self).__init__(*args)\n        self.lambda_func = identity\n\n    def forward(self, input):\n        return list(map(self.lambda_func,self.forward_prepare(input)))\n\nclass LambdaReduce(LambdaBase):\n    def __init__(self, *args):\n        super(LambdaReduce, self).__init__(*args)\n        self.lambda_func = add\n\n    def forward(self, input):\n        return reduce(self.lambda_func,self.forward_prepare(input))\n\ndef identity(x): return x\n\ndef add(x, y): return x + y\n\nresnext101_64x4d_features = nn.Sequential(#Sequential,\n    nn.Conv2d(3, 64, (7, 7), (2, 2), (3, 3), 1, 1, bias = False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(),\n    nn.MaxPool2d((3, 3), (2, 2), (1, 1)),\n    nn.Sequential(#Sequential,\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(64, 256, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                        nn.Conv2d(256, 256, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(256, 256, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(256),\n                ),\n                nn.Sequential(#Sequential,\n                    nn.Conv2d(64, 256, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(256),\n                ),\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(256, 256, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                        nn.Conv2d(256, 256, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(256, 256, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(256),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(256, 256, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                        nn.Conv2d(256, 256, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(256, 256, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(256),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n    ),\n    nn.Sequential(#Sequential,\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(256, 512, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512, 512, (3, 3), (2, 2), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512, 512, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(512),\n                ),\n                nn.Sequential(#Sequential,\n                    nn.Conv2d(256, 512, (1, 1), (2, 2), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(512),\n                ),\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(512, 512, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512, 512, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512, 512, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(512),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(512, 512, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512, 512, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512, 512, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(512),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(512, 512, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                        nn.Conv2d(512, 512, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(512, 512, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(512),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n    ),\n    nn.Sequential(#Sequential,\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(512, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (2, 2), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                nn.Sequential(#Sequential,\n                    nn.Conv2d(512, 1024, (1, 1), (2, 2), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                        nn.Conv2d(1024, 1024, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(1024),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(1024, 1024, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(1024),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n    ),\n    nn.Sequential(#Sequential,\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(1024, 2048, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(2048),\n                        nn.ReLU(),\n                        nn.Conv2d(2048, 2048, (3, 3), (2, 2), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(2048),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(2048, 2048, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(2048),\n                ),\n                nn.Sequential(#Sequential,\n                    nn.Conv2d(1024, 2048, (1, 1), (2, 2), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(2048),\n                ),\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(2048, 2048, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(2048),\n                        nn.ReLU(),\n                        nn.Conv2d(2048, 2048, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(2048),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(2048, 2048, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(2048),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n        nn.Sequential(#Sequential,\n            LambdaMap( #ConcatTable,\n                nn.Sequential(#Sequential,\n                    nn.Sequential(#Sequential,\n                        nn.Conv2d(2048, 2048, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                        nn.BatchNorm2d(2048),\n                        nn.ReLU(),\n                        nn.Conv2d(2048, 2048, (3, 3), (1, 1), (1, 1), 1, 64, bias = False),\n                        nn.BatchNorm2d(2048),\n                        nn.ReLU(),\n                    ),\n                    nn.Conv2d(2048, 2048, (1, 1), (1, 1), (0, 0), 1, 1, bias = False),\n                    nn.BatchNorm2d(2048),\n                ),\n                Lambda(), #Identity,\n            ),\n            LambdaReduce(), #CAddTable,\n            nn.ReLU(),\n        ),\n    )\n)'"
