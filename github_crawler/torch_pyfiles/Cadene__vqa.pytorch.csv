file_path,api_count,code
demo_server.py,4,"b'import os\nimport time\nimport yaml\nimport json\nimport argparse\nimport re\nimport base64\nimport torch\nfrom torch.autograd import Variable\nfrom PIL import Image\nfrom io import BytesIO\nfrom pprint import pprint\n\nfrom werkzeug.wrappers import Request, Response\nfrom werkzeug.serving import run_simple\n\nimport torchvision.transforms as transforms\nimport vqa.lib.utils as utils\nimport vqa.datasets as datasets\nimport vqa.models as models\nimport vqa.models.convnets as convnets\nfrom vqa.datasets.vqa_processed import tokenize_mcb\nfrom train import load_checkpoint\n\nparser = argparse.ArgumentParser(\n    description=\'Demo server\',\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--dir_logs\', type=str,\n    #default=\'logs/vqa2/blocmutan_noatt_fbresnet152torchported_save_all\',\n    default=\'logs/vqa2/mutan_att_train\',\n    help=\'dir logs\')\nparser.add_argument(\'--path_opt\', type=str,\n    #default=\'logs/vqa2/blocmutan_noatt_fbresnet152torchported_save_all/blocmutan_noatt.yaml\',\n    default=\'logs/vqa2/mutan_att_train/mutan_att_train.yaml\',\n    help=\'path to a yaml options file\')\nparser.add_argument(\'--resume\', type=str,\n    default=\'best\',\n    help=\'path to latest checkpoint\')\nparser.add_argument(\'--cuda\', type=bool,\n    const=True,\n    nargs=\'?\',\n    help=\'path to latest checkpoint\')\n\n@Request.application\ndef application(request):\n    print(\'\')\n    if \'visual\' in request.form and \'question\' in request.form:\n        visual = process_visual(request.form[\'visual\'])\n        question = process_question(request.form[\'question\'])\n        answer = process_answer(model(visual, question))\n        response = Response(answer)\n    \n    elif \'question\' not in request.form:\n        response = Response(\'Question missing\')\n\n    elif \'visual\' not in request.form:\n        response = Response(\'Image missing\')\n\n    else:\n        response = Response(\'what?\')\n\n    response.headers.add(\'Access-Control-Allow-Origin\', \'*\')\n    response.headers.add(\'Access-Control-Allow-Methods\', \'GET,PUT,POST,DELETE,PATCH\')\n    response.headers.add(\'Access-Control-Allow-Headers\', \'Content-Type, Authorization\')\n    response.headers.add(\'X-XSS-Protection\', \'0\')    \n    return response\n\ndef process_visual(visual_strb64):\n    visual_strb64 = re.sub(\'^data:image/.+;base64,\', \'\', visual_strb64)\n    visual_PIL = Image.open(BytesIO(base64.b64decode(visual_strb64)))\n    visual_tensor = transform(visual_PIL)\n    visual_data = torch.FloatTensor(1, 3,\n                                       visual_tensor.size(1),\n                                       visual_tensor.size(2))\n    visual_data[0][0] = visual_tensor[0]\n    visual_data[0][1] = visual_tensor[1]\n    visual_data[0][2] = visual_tensor[2]\n    print(\'visual\', visual_data.size(), visual_data.mean())\n    if args.cuda:\n        visual_data = visual_data.cuda(async=True)\n    visual_input = Variable(visual_data, volatile=True)\n    visual_features = cnn(visual_input)\n    if \'NoAtt\' in options[\'model\'][\'arch\']:\n        nb_regions = visual_features.size(2) * visual_features.size(3)\n        visual_features = visual_features.sum(3).sum(2).div(nb_regions).view(-1, 2048)\n    return visual_features\n\ndef process_question(question_str):\n    question_tokens = tokenize_mcb(question_str)\n    question_data = torch.LongTensor(1, len(question_tokens))\n    for i, word in enumerate(question_tokens):\n        if word in trainset.word_to_wid:\n            question_data[0][i] = trainset.word_to_wid[word]\n        else:\n            question_data[0][i] = trainset.word_to_wid[\'UNK\']\n    if args.cuda:\n        question_data = question_data.cuda(async=True)\n    question_input = Variable(question_data, volatile=True)\n    print(\'question\', question_str, question_tokens, question_data)\n\n    return question_input\n\ndef process_answer(answer_var):\n    answer_sm = torch.nn.functional.softmax(answer_var.data[0].cpu())\n    max_, aid = answer_sm.topk(5, 0, True, True)\n    ans = []\n    val = []\n    for i in range(5):\n        ans.append(trainset.aid_to_ans[aid.data[i]])\n        val.append(max_.data[i])\n\n    att = []\n    for x_att in model.list_att:\n        img = x_att.view(1,14,14).cpu()\n        img = transforms.ToPILImage()(img)\n        buffer_ = BytesIO()\n        img.save(buffer_, format=""PNG"")\n        img_str = base64.b64encode(buffer_.getvalue()).decode()\n        img_str = \'data:image/png;base64,\'+img_str\n        att.append(img_str)\n\n    answer = {\'ans\':ans,\'val\':val,\'att\':att}\n    answer_str = json.dumps(answer)\n\n    return answer_str\n\ndef main():\n    global args, options, model, cnn, transform, trainset\n    args = parser.parse_args()\n\n    options = {\n        \'logs\': {\n            \'dir_logs\': args.dir_logs\n        }\n    }\n    if args.path_opt is not None:\n        with open(args.path_opt, \'r\') as handle:\n            options_yaml = yaml.load(handle)\n        options = utils.update_values(options, options_yaml)\n    print(\'## args\'); pprint(vars(args))\n    print(\'## options\'); pprint(options)\n\n    trainset = datasets.factory_VQA(options[\'vqa\'][\'trainsplit\'],\n                                    options[\'vqa\'])\n                                    #options[\'coco\'])\n\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n    transform = transforms.Compose([\n        transforms.Scale(options[\'coco\'][\'size\']),\n        transforms.CenterCrop(options[\'coco\'][\'size\']),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n    opt_factory_cnn = {\n        \'arch\': options[\'coco\'][\'arch\']\n    }\n    cnn = convnets.factory(opt_factory_cnn, cuda=args.cuda, data_parallel=False)\n    model = models.factory(options[\'model\'],\n                           trainset.vocab_words(),\n                           trainset.vocab_answers(),\n                           cuda=args.cuda,\n                           data_parallel=False)\n    model.eval()\n    start_epoch, best_acc1, _ = load_checkpoint(model, None,\n            os.path.join(options[\'logs\'][\'dir_logs\'], args.resume))\n\n    my_local_ip = \'192.168.0.32\'\n    my_local_port = 3456\n    run_simple(my_local_ip, my_local_port, application)\n\nif __name__ == \'__main__\':\n    main()\n\n'"
eval_res.py,0,"b'import argparse\nimport json\nimport random\nimport os\nfrom os.path import join\nimport sys\n#import pickle\nhelperDir = \'vqa/external/VQA/\'\nsys.path.insert(0, \'%s/PythonHelperTools/vqaTools\' %(helperDir))\nsys.path.insert(0, \'%s/PythonEvaluationTools/vqaEvaluation\' %(helperDir))\nfrom vqa import VQA\nfrom vqaEval import VQAEval\n\n\nif __name__==""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--dir_vqa\',   type=str, default=\'/local/cadene/data/vqa\')\n    parser.add_argument(\'--dir_epoch\', type=str, default=\'logs/16_12_13_20:39:55/epoch,1\')\n    parser.add_argument(\'--subtype\',  type=str, default=\'train2014\')\n    args = parser.parse_args()\n\n    diranno  = join(args.dir_vqa, \'raw\', \'annotations\')\n    annFile  = join(diranno, \'mscoco_%s_annotations.json\' % (args.subtype))\n    quesFile = join(diranno, \'OpenEnded_mscoco_%s_questions.json\' % (args.subtype))\n    vqa = VQA(annFile, quesFile)\n    \n    taskType    = \'OpenEnded\'\n    dataType    = \'mscoco\'\n    dataSubType = args.subtype\n    resultType  = \'model\'\n    fileTypes = [\'results\', \'accuracy\', \'evalQA\', \'evalQuesType\', \'evalAnsType\'] \n    \n    [resFile, accuracyFile, evalQAFile, evalQuesTypeFile, evalAnsTypeFile] = \\\n        [\'%s/%s_%s_%s_%s_%s.json\' % (args.dir_epoch, taskType, dataType,\n            dataSubType, resultType, fileType) for fileType in fileTypes] \n    vqaRes = vqa.loadRes(resFile, quesFile)\n    vqaEval = VQAEval(vqa, vqaRes, n=2)\n\n    quesIds = [int(d[\'question_id\']) for d in json.loads(open(resFile).read())]\n    vqaEval.evaluate(quesIds=quesIds)\n    \n    json.dump(vqaEval.accuracy,     open(accuracyFile,     \'w\'))\n    #json.dump(vqaEval.evalQA,       open(evalQAFile,       \'w\'))\n    #json.dump(vqaEval.evalQuesType, open(evalQuesTypeFile, \'w\'))\n    #json.dump(vqaEval.evalAnsType,  open(evalAnsTypeFile,  \'w\'))'"
extract.py,6,"b'import argparse\nimport os\nimport time\nimport h5py\nimport numpy\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\n\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nimport vqa.models.convnets as convnets\nimport vqa.datasets as datasets\nfrom vqa.lib.dataloader import DataLoader\nfrom vqa.lib.logger import AvgMeter\n\nparser = argparse.ArgumentParser(description=\'Extract\')\nparser.add_argument(\'--dataset\', default=\'coco\',\n                    choices=[\'coco\', \'vgenome\'],\n                    help=\'dataset type: coco (default) | vgenome\')\nparser.add_argument(\'--dir_data\', default=\'data/coco\',\n                    help=\'dir dataset to download or/and load images\')\nparser.add_argument(\'--data_split\', default=\'train\', type=str,\n                    help=\'Options: (default) train | val | test\')\nparser.add_argument(\'--arch\', \'-a\', default=\'fbresnet152\',\n                    choices=convnets.model_names,\n                    help=\'model architecture: \' +\n                        \' | \'.join(convnets.model_names) +\n                        \' (default: fbresnet152)\')\nparser.add_argument(\'--workers\', default=4, type=int,\n                    help=\'number of data loading workers (default: 4)\')\nparser.add_argument(\'--batch_size\', \'-b\', default=80, type=int,\n                    help=\'mini-batch size (default: 80)\')\nparser.add_argument(\'--mode\', default=\'both\', type=str,\n                    help=\'Options: att | noatt |\xc2\xa0(default) both\')\nparser.add_argument(\'--size\', default=448, type=int,\n                    help=\'Image size (448 for noatt := avg pooling to get 224) (default:448)\')\n\n\ndef main():\n    global args\n    args = parser.parse_args()\n\n    print(""=> using pre-trained model \'{}\'"".format(args.arch))\n    model = convnets.factory({\'arch\':args.arch}, cuda=True, data_parallel=True)\n\n    extract_name = \'arch,{}_size,{}\'.format(args.arch, args.size)\n\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n\n    if args.dataset == \'coco\':\n        if \'coco\' not in args.dir_data:\n            raise ValueError(\'""coco"" string not in dir_data\')\n        dataset = datasets.COCOImages(args.data_split, dict(dir=args.dir_data),\n            transform=transforms.Compose([\n                transforms.Scale(args.size),\n                transforms.CenterCrop(args.size),\n                transforms.ToTensor(),\n                normalize,\n            ]))\n    elif args.dataset == \'vgenome\':\n        if args.data_split != \'train\':\n            raise ValueError(\'train split is required for vgenome\')\n        if \'vgenome\' not in args.dir_data:\n            raise ValueError(\'""vgenome"" string not in dir_data\')\n        dataset = datasets.VisualGenomeImages(args.data_split, dict(dir=args.dir_data),\n            transform=transforms.Compose([\n                transforms.Scale(args.size),\n                transforms.CenterCrop(args.size),\n                transforms.ToTensor(),\n                normalize,\n            ]))\n\n    data_loader = DataLoader(dataset,\n        batch_size=args.batch_size, shuffle=False,\n        num_workers=args.workers, pin_memory=True)\n\n    dir_extract = os.path.join(args.dir_data, \'extract\', extract_name)\n    path_file = os.path.join(dir_extract, args.data_split + \'set\')\n    os.system(\'mkdir -p \' + dir_extract)\n\n    extract(data_loader, model, path_file, args.mode)\n\n\ndef extract(data_loader, model, path_file, mode):\n    path_hdf5 = path_file + \'.hdf5\'\n    path_txt = path_file + \'.txt\'\n    hdf5_file = h5py.File(path_hdf5, \'w\')\n\n    # estimate output shapes\n    output = model(Variable(torch.ones(1, 3, args.size, args.size),\n                            volatile=True))\n\n    nb_images = len(data_loader.dataset)\n    if mode == \'both\' or mode == \'att\':\n        shape_att = (nb_images, output.size(1), output.size(2), output.size(3))\n        print(\'Warning: shape_att={}\'.format(shape_att))\n        hdf5_att = hdf5_file.create_dataset(\'att\', shape_att,\n                                            dtype=\'f\')#, compression=\'gzip\')\n    if mode == \'both\' or mode == \'noatt\':\n        shape_noatt = (nb_images, output.size(1))\n        print(\'Warning: shape_noatt={}\'.format(shape_noatt))\n        hdf5_noatt = hdf5_file.create_dataset(\'noatt\', shape_noatt,\n                                              dtype=\'f\')#, compression=\'gzip\')\n\n    model.eval()\n\n    batch_time = AvgMeter()\n    data_time  = AvgMeter()\n    begin = time.time()\n    end = time.time()\n\n    idx = 0\n    for i, input in enumerate(data_loader):\n        input_var = Variable(input[\'visual\'], volatile=True)\n        output_att = model(input_var)\n\n        nb_regions = output_att.size(2) * output_att.size(3)\n        output_noatt = output_att.sum(3).sum(2).div(nb_regions).view(-1, 2048)\n\n        batch_size = output_att.size(0)\n        if mode == \'both\' or mode == \'att\':\n            hdf5_att[idx:idx+batch_size]   = output_att.data.cpu().numpy()\n        if mode == \'both\' or mode == \'noatt\':\n            hdf5_noatt[idx:idx+batch_size] = output_noatt.data.cpu().numpy()\n        idx += batch_size\n\n        torch.cuda.synchronize()\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % 1 == 0:\n            print(\'Extract: [{0}/{1}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\'.format(\n                   i, len(data_loader),\n                   batch_time=batch_time,\n                   data_time=data_time,))\n\n    hdf5_file.close()\n\n    # Saving image names in the same order than extraction\n    with open(path_txt, \'w\') as handle:\n        for name in data_loader.dataset.dataset.imgs:\n            handle.write(name + \'\\n\')\n\n    end = time.time() - begin\n    print(\'Finished in {}m and {}s\'.format(int(end/60), int(end%60)))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
train.py,12,"b'import argparse\nimport os\nimport shutil\nimport yaml\nimport json\nimport click\nfrom pprint import pprint\n\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\n\nimport vqa.lib.engine as engine\nimport vqa.lib.utils as utils\nimport vqa.lib.logger as logger\nimport vqa.lib.criterions as criterions\nimport vqa.datasets as datasets\nimport vqa.models as models\n\nparser = argparse.ArgumentParser(\n    description=\'Train/Evaluate models\',\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n##################################################\n#\xc2\xa0yaml options file contains all default choices #\nparser.add_argument(\'--path_opt\', default=\'options/vqa/default.yaml\', type=str, \n                    help=\'path to a yaml options file\')\n################################################\n# change cli options to modify default choices #\n# logs options\nparser.add_argument(\'--dir_logs\', type=str, help=\'dir logs\')\n# data options\nparser.add_argument(\'--vqa_trainsplit\', type=str, choices=[\'train\',\'trainval\'])\n# model options\nparser.add_argument(\'--arch\', choices=models.model_names,\n                    help=\'vqa model architecture: \' +\n                        \' | \'.join(models.model_names))\nparser.add_argument(\'--st_type\',\n                    help=\'skipthoughts type\')\nparser.add_argument(\'--st_dropout\', type=float)\nparser.add_argument(\'--st_fixed_emb\', default=None, type=utils.str2bool,\n                    help=\'backprop on embedding\')\n# optim options\nparser.add_argument(\'-lr\', \'--learning_rate\', type=float,\n                    help=\'initial learning rate\')\nparser.add_argument(\'-b\', \'--batch_size\', type=int,\n                    help=\'mini-batch size\')\nparser.add_argument(\'--epochs\', type=int,\n                    help=\'number of total epochs to run\')\n# options not in yaml file          \nparser.add_argument(\'--start_epoch\', default=0, type=int,\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--resume\', default=\'\', type=str,\n                    help=\'path to latest checkpoint\')\nparser.add_argument(\'--save_model\', default=True, type=utils.str2bool,\n                    help=\'able or disable save model and optim state\')\nparser.add_argument(\'--save_all_from\', type=int,\n                    help=\'\'\'delete the preceding checkpoint until an epoch,\'\'\'\n                         \'\'\' then keep all (useful to save disk space)\')\'\'\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                    help=\'evaluate model on validation and test set\')\nparser.add_argument(\'-j\', \'--workers\', default=2, type=int,\n                    help=\'number of data loading workers\')\nparser.add_argument(\'--print_freq\', \'-p\', default=10, type=int,\n                    help=\'print frequency\')\n################################################\nparser.add_argument(\'-ho\', \'--help_opt\', dest=\'help_opt\', action=\'store_true\',\n                    help=\'show selected options before running\')\n\nbest_acc1 = 0\n\ndef main():\n    global args, best_acc1\n    args = parser.parse_args()\n\n    #########################################################################################\n    # Create options\n    #########################################################################################\n\n    options = {\n        \'vqa\' : {\n            \'trainsplit\': args.vqa_trainsplit\n        },\n        \'logs\': {\n            \'dir_logs\': args.dir_logs\n        },\n        \'model\': {\n            \'arch\': args.arch,\n            \'seq2vec\': {\n                \'type\': args.st_type,\n                \'dropout\': args.st_dropout,\n                \'fixed_emb\': args.st_fixed_emb\n            }\n        },\n        \'optim\': {\n            \'lr\': args.learning_rate,\n            \'batch_size\': args.batch_size,\n            \'epochs\': args.epochs\n        }\n    }\n    if args.path_opt is not None:\n        with open(args.path_opt, \'r\') as handle:\n            options_yaml = yaml.load(handle)\n        options = utils.update_values(options, options_yaml)\n    print(\'## args\'); pprint(vars(args))\n    print(\'## options\'); pprint(options)\n    if args.help_opt:\n        return\n\n    # Set datasets options\n    if \'vgenome\' not in options:\n        options[\'vgenome\'] = None\n\n    #########################################################################################\n    # Create needed datasets\n    #########################################################################################\n\n    trainset = datasets.factory_VQA(options[\'vqa\'][\'trainsplit\'],\n                                    options[\'vqa\'],\n                                    options[\'coco\'],\n                                    options[\'vgenome\'])\n    train_loader = trainset.data_loader(batch_size=options[\'optim\'][\'batch_size\'],\n                                        num_workers=args.workers,\n                                        shuffle=True)                                      \n\n    if options[\'vqa\'][\'trainsplit\'] == \'train\':\n        valset = datasets.factory_VQA(\'val\', options[\'vqa\'], options[\'coco\'])\n        val_loader = valset.data_loader(batch_size=options[\'optim\'][\'batch_size\'],\n                                        num_workers=args.workers)\n\n    if options[\'vqa\'][\'trainsplit\'] == \'trainval\' or args.evaluate:\n        testset = datasets.factory_VQA(\'test\', options[\'vqa\'], options[\'coco\'])\n        test_loader = testset.data_loader(batch_size=options[\'optim\'][\'batch_size\'],\n                                          num_workers=args.workers)\n    \n    #########################################################################################\n    # Create model, criterion and optimizer\n    #########################################################################################\n    \n    model = models.factory(options[\'model\'],\n                           trainset.vocab_words(), trainset.vocab_answers(),\n                           cuda=True, data_parallel=True)\n    criterion = criterions.factory(options[\'vqa\'], cuda=True)\n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n                            options[\'optim\'][\'lr\'])\n    \n    #########################################################################################\n    # args.resume: resume from a checkpoint OR create logs directory\n    #########################################################################################\n\n    exp_logger = None\n    if args.resume:\n        args.start_epoch, best_acc1, exp_logger = load_checkpoint(model.module, optimizer,\n            os.path.join(options[\'logs\'][\'dir_logs\'], args.resume))\n    else:\n        # Or create logs directory\n        if os.path.isdir(options[\'logs\'][\'dir_logs\']):\n            if click.confirm(\'Logs directory already exists in {}. Erase?\'\n                .format(options[\'logs\'][\'dir_logs\'], default=False)):\n                os.system(\'rm -r \' + options[\'logs\'][\'dir_logs\'])\n            else:\n                return\n        os.system(\'mkdir -p \' + options[\'logs\'][\'dir_logs\'])\n        path_new_opt = os.path.join(options[\'logs\'][\'dir_logs\'],\n                       os.path.basename(args.path_opt))\n        path_args = os.path.join(options[\'logs\'][\'dir_logs\'], \'args.yaml\')\n        with open(path_new_opt, \'w\') as f:\n            yaml.dump(options, f, default_flow_style=False)\n        with open(path_args, \'w\') as f:\n            yaml.dump(vars(args), f, default_flow_style=False)\n        \n    if exp_logger is None:\n        #\xc2\xa0Set loggers\n        exp_name = os.path.basename(options[\'logs\'][\'dir_logs\']) # add timestamp\n        exp_logger = logger.Experiment(exp_name, options)\n        exp_logger.add_meters(\'train\', make_meters())\n        exp_logger.add_meters(\'test\', make_meters())\n        if options[\'vqa\'][\'trainsplit\'] == \'train\':\n            exp_logger.add_meters(\'val\', make_meters())\n        exp_logger.info[\'model_params\'] = utils.params_count(model)\n        print(\'Model has {} parameters\'.format(exp_logger.info[\'model_params\']))\n\n    #########################################################################################\n    #\xc2\xa0args.evaluate: on valset OR/AND on testset\n    #########################################################################################\n\n    if args.evaluate:\n        path_logger_json = os.path.join(options[\'logs\'][\'dir_logs\'], \'logger.json\')\n\n        if options[\'vqa\'][\'trainsplit\'] == \'train\':\n            acc1, val_results = engine.validate(val_loader, model, criterion,\n                                                exp_logger, args.start_epoch, args.print_freq)\n            # save results and compute OpenEnd accuracy\n            exp_logger.to_json(path_logger_json)\n            save_results(val_results, args.start_epoch, valset.split_name(),\n                         options[\'logs\'][\'dir_logs\'], options[\'vqa\'][\'dir\'])\n        \n        test_results, testdev_results = engine.test(test_loader, model, exp_logger,\n                                                    args.start_epoch, args.print_freq)\n        # save results and DOES NOT compute OpenEnd accuracy\n        exp_logger.to_json(path_logger_json)\n        save_results(test_results, args.start_epoch, testset.split_name(),\n                     options[\'logs\'][\'dir_logs\'], options[\'vqa\'][\'dir\'])\n        save_results(testdev_results, args.start_epoch, testset.split_name(testdev=True),\n                     options[\'logs\'][\'dir_logs\'], options[\'vqa\'][\'dir\'])\n        return\n\n    #########################################################################################\n    #\xc2\xa0Begin training on train/val or trainval/test\n    #########################################################################################\n\n    for epoch in range(args.start_epoch+1, options[\'optim\'][\'epochs\']):\n        #adjust_learning_rate(optimizer, epoch)\n\n        # train for one epoch\n        engine.train(train_loader, model, criterion, optimizer, \n                     exp_logger, epoch, args.print_freq)\n        \n        if options[\'vqa\'][\'trainsplit\'] == \'train\':\n            # evaluate on validation set\n            acc1, val_results = engine.validate(val_loader, model, criterion,\n                                                exp_logger, epoch, args.print_freq)\n            # remember best prec@1 and save checkpoint\n            is_best = acc1 > best_acc1\n            best_acc1 = max(acc1, best_acc1)\n            save_checkpoint({\n                    \'epoch\': epoch,\n                    \'arch\': options[\'model\'][\'arch\'],\n                    \'best_acc1\': best_acc1,\n                    \'exp_logger\': exp_logger\n                },\n                model.module.state_dict(),\n                optimizer.state_dict(),\n                options[\'logs\'][\'dir_logs\'],\n                args.save_model,\n                args.save_all_from,\n                is_best)\n\n            # save results and compute OpenEnd accuracy\n            save_results(val_results, epoch, valset.split_name(),\n                         options[\'logs\'][\'dir_logs\'], options[\'vqa\'][\'dir\'])\n        else:\n            test_results, testdev_results = engine.test(test_loader, model, exp_logger,\n                                                        epoch, args.print_freq)\n\n            # save checkpoint at every timestep\n            save_checkpoint({\n                    \'epoch\': epoch,\n                    \'arch\': options[\'model\'][\'arch\'],\n                    \'best_acc1\': best_acc1,\n                    \'exp_logger\': exp_logger\n                },\n                model.module.state_dict(),\n                optimizer.state_dict(),\n                options[\'logs\'][\'dir_logs\'],\n                args.save_model,\n                args.save_all_from)\n\n            # save results and DOES NOT compute OpenEnd accuracy\n            save_results(test_results, epoch, testset.split_name(),\n                         options[\'logs\'][\'dir_logs\'], options[\'vqa\'][\'dir\'])\n            save_results(testdev_results, epoch, testset.split_name(testdev=True),\n                         options[\'logs\'][\'dir_logs\'], options[\'vqa\'][\'dir\'])\n    \n\ndef make_meters():  \n    meters_dict = {\n        \'loss\': logger.AvgMeter(),\n        \'acc1\': logger.AvgMeter(),\n        \'acc5\': logger.AvgMeter(),\n        \'batch_time\': logger.AvgMeter(),\n        \'data_time\': logger.AvgMeter(),\n        \'epoch_time\': logger.SumMeter()\n    }\n    return meters_dict\n\ndef save_results(results, epoch, split_name, dir_logs, dir_vqa):\n    dir_epoch = os.path.join(dir_logs, \'epoch_\' + str(epoch))\n    name_json = \'OpenEnded_mscoco_{}_model_results.json\'.format(split_name)\n    # TODO: simplify formating\n    if \'test\' in split_name:\n        name_json = \'vqa_\' + name_json\n    path_rslt = os.path.join(dir_epoch, name_json)\n    os.system(\'mkdir -p \' + dir_epoch)\n    with open(path_rslt, \'w\') as handle:\n        json.dump(results, handle)\n    if not \'test\' in split_name:\n        os.system(\'python2 eval_res.py --dir_vqa {} --dir_epoch {} --subtype {} &\'\n                  .format(dir_vqa, dir_epoch, split_name))\n\ndef save_checkpoint(info, model, optim, dir_logs, save_model, save_all_from=None, is_best=True):\n    os.system(\'mkdir -p \' + dir_logs)\n    if save_all_from is None:\n        path_ckpt_info  = os.path.join(dir_logs, \'ckpt_info.pth.tar\')\n        path_ckpt_model = os.path.join(dir_logs, \'ckpt_model.pth.tar\')\n        path_ckpt_optim = os.path.join(dir_logs, \'ckpt_optim.pth.tar\')\n        path_best_info  = os.path.join(dir_logs, \'best_info.pth.tar\')\n        path_best_model = os.path.join(dir_logs, \'best_model.pth.tar\')\n        path_best_optim = os.path.join(dir_logs, \'best_optim.pth.tar\')\n        # save info & logger\n        path_logger = os.path.join(dir_logs, \'logger.json\')\n        info[\'exp_logger\'].to_json(path_logger)\n        torch.save(info, path_ckpt_info)\n        if is_best:\n            shutil.copyfile(path_ckpt_info, path_best_info)\n        #\xc2\xa0save model state & optim state\n        if save_model:\n            torch.save(model, path_ckpt_model)\n            torch.save(optim, path_ckpt_optim)\n            if is_best:\n                shutil.copyfile(path_ckpt_model, path_best_model)\n                shutil.copyfile(path_ckpt_optim, path_best_optim)\n    else:\n        is_best = False # because we don\'t know the test accuracy\n        path_ckpt_info  = os.path.join(dir_logs, \'ckpt_epoch,{}_info.pth.tar\')\n        path_ckpt_model = os.path.join(dir_logs, \'ckpt_epoch,{}_model.pth.tar\')\n        path_ckpt_optim = os.path.join(dir_logs, \'ckpt_epoch,{}_optim.pth.tar\')\n        # save info & logger\n        path_logger = os.path.join(dir_logs, \'logger.json\')\n        info[\'exp_logger\'].to_json(path_logger)\n        torch.save(info, path_ckpt_info.format(info[\'epoch\']))\n        #\xc2\xa0save model state & optim state\n        if save_model:\n            torch.save(model, path_ckpt_model.format(info[\'epoch\']))\n            torch.save(optim, path_ckpt_optim.format(info[\'epoch\']))\n        if  info[\'epoch\'] > 1 and info[\'epoch\'] < save_all_from + 1:\n            os.system(\'rm \' + path_ckpt_info.format(info[\'epoch\'] - 1))\n            os.system(\'rm \' + path_ckpt_model.format(info[\'epoch\'] - 1))\n            os.system(\'rm \' + path_ckpt_optim.format(info[\'epoch\'] - 1))\n    if not save_model:\n        print(\'Warning train.py: checkpoint not saved\')\n\ndef load_checkpoint(model, optimizer, path_ckpt):\n    path_ckpt_info  = path_ckpt + \'_info.pth.tar\'\n    path_ckpt_model = path_ckpt + \'_model.pth.tar\'\n    path_ckpt_optim = path_ckpt + \'_optim.pth.tar\'\n    if os.path.isfile(path_ckpt_info):\n        info = torch.load(path_ckpt_info)\n        start_epoch = 0\n        best_acc1   = 0\n        exp_logger  = None\n        if \'epoch\' in info:\n            start_epoch = info[\'epoch\']\n        else:\n            print(\'Warning train.py: no epoch to resume\')\n        if \'best_acc1\' in info:\n            best_acc1 = info[\'best_acc1\']\n        else:\n            print(\'Warning train.py: no best_acc1 to resume\')\n        if \'exp_logger\' in info:\n            exp_logger = info[\'exp_logger\']\n        else:\n            print(\'Warning train.py: no exp_logger to resume\')\n    else:\n        print(""Warning train.py: no info checkpoint found at \'{}\'"".format(path_ckpt_info))\n    if os.path.isfile(path_ckpt_model):\n        model_state = torch.load(path_ckpt_model)\n        model.load_state_dict(model_state)\n    else:\n        print(""Warning train.py: no model checkpoint found at \'{}\'"".format(path_ckpt_model))\n    if optimizer is not None and os.path.isfile(path_ckpt_optim):\n        optim_state = torch.load(path_ckpt_optim)\n        optimizer.load_state_dict(optim_state)\n    else:\n        print(""Warning train.py: no optim checkpoint found at \'{}\'"".format(path_ckpt_optim))\n    print(""=> loaded checkpoint \'{}\' (epoch {}, best_acc1 {})""\n              .format(path_ckpt, start_epoch, best_acc1))\n    return start_epoch, best_acc1, exp_logger\n\nif __name__ == \'__main__\':\n    main()\n'"
visu.py,0,"b""import os\nimport time\nimport argparse\nimport json\nimport random\nimport numpy as np\nimport colorlover as cl\n\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, plot\nfrom plotly import tools\n\nfrom vqa.lib.logger import Experiment\n\ndef load_accs_oe(path_logger):\n    dir_xp = os.path.dirname(path_logger)\n    epochs = []\n    for name in os.listdir(dir_xp):\n        if name.startswith('epoch'):\n            epochs.append(name)\n    epochs = sorted(epochs, key=lambda x: float(x.split('_')[1]))\n    accs = {}\n    for i, epoch in enumerate(epochs):\n        epoch_id = i+1\n        path_acc = os.path.join(dir_xp, epoch, 'OpenEnded_mscoco_val2014_model_accuracy.json')\n        if os.path.exists(path_acc):\n            with open(path_acc, 'r') as f:\n                data = json.load(f)\n                accs[epoch_id] = data['overall']\n    return accs\n\ndef sort(dict_):\n    return [v for k,v in sorted(dict_.items(), \\\n                           key=lambda x: float(x[0]))]\n\ndef reduce(list_, num=15):\n    tmp = []\n    for i, val in enumerate(list_):\n        if i < num:\n            tmp.append(val)\n    return tmp\n\n# Display accuracy & loss of one exp\ndef visu_one_exp(path_logger, path_visu, auto_open=True):\n    xp = Experiment.from_json(path_logger)\n    xp.logged['val']['acc1_oe'] = load_accs_oe(path_logger)\n    train_acc1 = sort(xp.logged['train']['acc1'])\n    val_acc1   = sort(xp.logged['val']['acc1_oe'])\n    train_loss = sort(xp.logged['train']['loss'])\n    val_loss   = sort(xp.logged['val']['loss'])\n    train_data_x = list(range(1, len(train_acc1)+1))\n    val_data_x   = list(range(1, len(val_acc1)+1))\n  \n    fig = tools.make_subplots(rows=1,\n                              cols=2,\n                              subplot_titles=('Accuracy top1', 'Loss'))\n    # blue rgb(31, 119, 180)\n    # orange rgb(255, 127, 14)\n\n    train_acc1_trace = go.Scatter(\n        x=train_data_x, \n        y=train_acc1,\n        name='train accuracy top1'\n    )\n    val_acc1_trace = go.Scatter(\n        x=val_data_x, \n        y=val_acc1,\n        name='val accuracy top1',\n        line = dict(\n            color = ('rgb(255, 127, 14)'),\n        )\n    )\n    best_val_acc1_trace = go.Scatter(\n        x=[np.argmax(val_acc1)+1], \n        y=[max(val_acc1)],\n        mode='markers',\n        name='best val accuracy top1',\n        marker = dict(\n            color = 'rgb(255, 127, 14)',\n            size = 10\n        )\n    )\n\n    val_loss_trace = go.Scatter(\n        x=val_data_x, \n        y=val_loss,\n        name='val loss'\n    )\n    train_loss_trace = go.Scatter(\n        x=train_data_x, \n        y=train_loss,\n        name='train loss'\n    )\n\n    fig.append_trace(train_acc1_trace, 1, 1)\n    fig.append_trace(val_acc1_trace, 1, 1)\n    fig.append_trace(best_val_acc1_trace, 1, 1)\n    \n    fig.append_trace(train_loss_trace, 1, 2)\n    fig.append_trace(val_loss_trace, 1, 2)\n\n    plot(fig, filename=path_visu, auto_open=auto_open)\n\n    return train_acc1, val_acc1\n\n# Display accuracy & loss of one exp\ndef visu_exps(list_path_logger, path_visu, auto_open=True):\n    fig = tools.make_subplots(rows=2,\n                          cols=2,\n                          subplot_titles=('Val accuracy top1',\n                                          'Val loss',\n                                          'Train accuracy top1',\n                                          'Train loss'))\n    num_xp = len(list_path_logger)\n    if num_xp < 3: # cl.scales not accept\n        num_xp = 3\n    list_color = cl.scales[str(num_xp)]['qual']['Paired']\n\n    for i, path_logger in enumerate(list_path_logger):\n        name = path_logger.split('/')[-2]\n\n        xp = Experiment.from_json(path_logger)\n        xp.logged['val']['acc1_oe'] = load_accs_oe(path_logger)\n        train_acc1 = sort(xp.logged['train']['acc1'])\n        val_acc1   = sort(xp.logged['val']['acc1_oe'])\n        train_loss = sort(xp.logged['train']['loss'])\n        val_loss   = sort(xp.logged['val']['loss'])\n        train_data_x = list(range(1, len(train_acc1)+1))\n        val_data_x   = list(range(1, len(val_acc1)+1))\n\n        train_acc1_trace = go.Scatter(\n            x=train_data_x, \n            y=train_acc1,\n            name='train acc: '+name,\n            line=dict(\n                color=list_color[i]\n            )\n        )\n        val_acc1_trace = go.Scatter(\n            x=val_data_x, \n            y=val_acc1,\n            name='val acc: '+name,\n            line=dict(\n                color=list_color[i]\n            )\n        )\n        best_val_acc1_trace = go.Scatter(\n            x=[np.argmax(val_acc1)+1], \n            y=[max(val_acc1)],\n            mode='markers',\n            name='best val acc: '+name,\n            marker = dict(\n                color = list_color[i],\n                size = 10\n            )\n        )\n\n        val_loss_trace = go.Scatter(\n            x=val_data_x, \n            y=val_loss,\n            name='val loss: '+name,\n            line=dict(\n                color=list_color[i]\n            )\n        )\n        train_loss_trace = go.Scatter(\n            x=train_data_x, \n            y=train_loss,\n            name='train loss: '+name,\n            line=dict(\n                color=list_color[i]\n            )\n        )\n\n        fig.append_trace(val_acc1_trace, 1, 1)\n        fig.append_trace(best_val_acc1_trace, 1, 1)\n        fig.append_trace(train_acc1_trace, 2, 1)\n\n        fig.append_trace(val_loss_trace, 1, 2)\n        fig.append_trace(train_loss_trace, 2, 2)\n\n    plot(fig, filename=path_visu, auto_open=auto_open)\n\ndef main_one_exp(dir_logs, path_visu=None, refresh_freq=60):\n    if path_visu is None:\n        path_visu = os.path.join(dir_logs, 'visu.html')\n    \n    path_logger = os.path.join(dir_logs, 'logger.json')\n\n    i = 1\n    print('Create visu to ' + path_visu)\n    while True:\n        train_acc1, val_acc1 = visu_one_exp(path_logger, path_visu, auto_open=(i==1))\n        print('# Visu iteration (refresh every {} sec): {}'.format(refresh_freq, i))\n        print('Max Val OpenEnded-Accuracy Top1: {}'.format(max(val_acc1)))\n        print('Max Train Accuracy Top1: {}'.format(max(train_acc1)))\n        i += 1\n        time.sleep(refresh_freq)\n\ndef main_exps(list_dir_logs, path_visu=None, refresh_freq=60):\n    if path_visu is None:\n        path_visu = os.path.join(os.path.dirname(list_dir_logs[0]), 'visu.html')\n\n    list_path_logger = []\n    for dir_logs in list_dir_logs:\n        list_path_logger.append(os.path.join(dir_logs, 'logger.json'))\n\n    i = 1\n    print('Create visu to ' + path_visu)\n    while True:\n        visu_exps(list_path_logger, path_visu, auto_open=(i==1))\n        print('# Visu iteration (refresh every {} sec): {}'.format(refresh_freq, i))\n        i += 1\n        time.sleep(refresh_freq)\n\n\n##########################################################################\n# Main\n##########################################################################\n\nparser = argparse.ArgumentParser(\n    description='Create html visu files',\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\nparser.add_argument('--dir_logs', type=str,\n                    help='''First mode: dir to logs of an experiment (ex: logs/vqa/mutan)'''\n                         '''Second mode: add several dirs to create a comparativ visualisation (ex: logs/vqa/mutan,logs/vqa/mlb)''')\nparser.add_argument('--refresh_freq', '-f', default=60, type=int,\n                    help='refresh frequency in seconds')\nparser.add_argument('--path_visu', default=None,\n                    help='path to the html file (default: visu.html in dir_logs)')\n\ndef main():\n    global args\n    args = parser.parse_args()\n\n    list_dir_logs = args.dir_logs.split(',')\n\n    if len(list_dir_logs) == 1:\n        main_one_exp(args.dir_logs,\n                     path_visu=args.path_visu,\n                     refresh_freq=args.refresh_freq)\n    else:\n        main_exps(list_dir_logs,\n                  path_visu=args.path_visu,\n                  refresh_freq=args.refresh_freq)\n\nif __name__ == '__main__':\n    main()\n"""
vqa/__init__.py,0,b''
vqa/datasets/__init__.py,0,b'from .vqa import factory as factory_VQA\nfrom .coco import COCOImages\nfrom .vgenome import VisualGenomeImages'
vqa/datasets/coco.py,1,"b""import os\n# from mpi4py import MPI\nimport numpy as np\nimport h5py\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\n\nfrom ..lib import utils\nfrom .images import ImagesFolder, AbstractImagesDataset, default_loader\nfrom .features import FeaturesDataset\n\ndef split_name(data_split):\n    if data_split in ['train', 'val']:\n        return data_split + '2014'\n    elif data_split == 'test':\n        return data_split + '2015'\n    else:\n        assert False, 'data_split {} not exists'.format(data_split)\n\n\nclass COCOImages(AbstractImagesDataset):\n\n    def __init__(self, data_split, opt, transform=None, loader=default_loader):\n        self.split_name = split_name(data_split)\n        super(COCOImages, self).__init__(data_split, opt, transform, loader)\n        self.dir_split = self.get_dir_data()\n        self.dataset = ImagesFolder(self.dir_split, transform=self.transform, loader=self.loader)\n        self.name_to_index = self._load_name_to_index()\n\n    def get_dir_data(self):\n        return os.path.join(self.dir_raw, self.split_name)\n\n    def _raw(self):\n        if self.data_split in ['train', 'val']:\n            os.system('wget http://msvocds.blob.core.windows.net/coco2014/{}.zip -P {}'.format(self.split_name, self.dir_raw))\n        elif self.data_split == 'test':\n            os.system('wget http://msvocds.blob.core.windows.net/coco2015/test2015.zip -P '+self.dir_raw)\n        else:\n            assert False, 'data_split {} not exists'.format(self.data_split)\n        os.system('unzip '+os.path.join(self.dir_raw, self.split_name+'.zip')+' -d '+self.dir_raw)\n\n    def _load_name_to_index(self):\n        self.name_to_index = {name:index for index, name in enumerate(self.dataset.imgs)}\n        return self.name_to_index\n\n    def __getitem__(self, index):\n        item = self.dataset[index]\n        item['name'] = os.path.join(self.split_name, item['name'])\n        return item\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nclass COCOTrainval(data.Dataset):\n\n    def __init__(self, trainset, valset):\n        self.trainset = trainset\n        self.valset = valset\n\n    def __getitem__(self, index):\n        if index < len(self.trainset):\n            item = self.trainset[index]\n        else:\n            item = self.valset[index - len(self.trainset)]\n        return item\n\n    def get_by_name(self, image_name):\n        if image_name in self.trainset.name_to_index:\n            index = self.trainset.name_to_index[image_name]\n            item = self.trainset[index]\n            return item\n        elif image_name in self.valset.name_to_index:\n            index = self.valset.name_to_index[image_name]\n            item = self.valset[index]\n            return item\n        else:\n            raise ValueError\n\n    def __len__(self):\n        return len(self.trainset) + len(self.valset)\n\n\ndef default_transform(size):\n    transform = transforms.Compose([\n        transforms.Scale(size),\n        transforms.CenterCrop(size),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], # resnet imagnet\n                             std=[0.229, 0.224, 0.225])\n    ])\n    return transform\n\ndef factory(data_split, opt, transform=None):\n    if data_split == 'trainval':\n        trainset = factory('train', opt, transform)\n        valset = factory('val', opt, transform)\n        return COCOTrainval(trainset, valset)\n    elif data_split in ['train', 'val', 'test']:\n        if opt['mode'] == 'img':\n            if transform is None:\n                transform = default_transform(opt['size'])\n            return COCOImages(data_split, opt, transform)\n        elif opt['mode'] in ['noatt', 'att']:\n            return FeaturesDataset(data_split, opt)\n        else:\n            raise ValueError\n    else:\n        raise ValueError\n"""
vqa/datasets/features.py,3,"b""import os\nimport numpy as np\nimport h5py\nimport torch\nimport torch.utils.data as data\n\nclass FeaturesDataset(data.Dataset):\n\n    def __init__(self, data_split, opt):\n        self.data_split = data_split\n        self.opt = opt\n        self.dir_extract = os.path.join(self.opt['dir'],\n                                      'extract',\n                                      'arch,' + self.opt['arch'])\n        if 'size' in opt:\n            self.dir_extract += '_size,' + str(opt['size'])\n        self.path_hdf5 = os.path.join(self.dir_extract,\n                                      data_split + 'set.hdf5')\n        assert os.path.isfile(self.path_hdf5), \\\n               'File not found in {}, you must extract the features first with extract.py'.format(self.path_hdf5)\n        self.hdf5_file = h5py.File(self.path_hdf5, 'r')#, driver='mpio', comm=MPI.COMM_WORLD)\n        self.dataset_features = self.hdf5_file[self.opt['mode']]\n        self.index_to_name, self.name_to_index = self._load_dicts()\n\n    def _load_dicts(self):\n        self.path_fname = os.path.join(self.dir_extract,\n                                       self.data_split + 'set.txt')\n        with open(self.path_fname, 'r') as handle:\n            self.index_to_name = handle.readlines()\n        self.index_to_name = [name[:-1] for name in self.index_to_name] # remove char '\\n'\n        self.name_to_index = {name:index for index,name in enumerate(self.index_to_name)}\n        return self.index_to_name, self.name_to_index\n\n    def __getitem__(self, index):\n        item = {}\n        item['name'] = self.index_to_name[index]\n        item['visual'] = self.get_features(index)\n        #item = torch.Tensor(self.get_features(index))\n        return item\n\n    def get_features(self, index):\n        return torch.Tensor(self.dataset_features[index])\n\n    def get_features_old(self, index):\n        try:\n            self.features_array\n        except AttributeError:\n            if self.opt['mode'] == 'att':\n                self.features_array = np.zeros((2048,14,14), dtype='f')\n            elif self.opt['mode'] == 'noatt':\n                self.features_array = np.zeros((2048), dtype='f')\n\n        if self.opt['mode'] == 'att':\n            self.dataset_features.read_direct(self.features_array,\n                                              np.s_[index,:2048,:14,:14],\n                                              np.s_[:2048,:14,:14])\n        elif self.opt['mode'] == 'noatt':\n            self.dataset_features.read_direct(self.features_array,\n                                              np.s_[index,:2048],\n                                              np.s_[:2048])\n        return self.features_array\n        \n\n    def get_by_name(self, image_name):\n        index = self.name_to_index[image_name]\n        return self[index]\n\n    def __len__(self):\n        return self.dataset_features.shape[0]\n"""
vqa/datasets/images.py,1,"b'import torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n]\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\ndef make_dataset(dir):\n    images = []\n    for fname in os.listdir(dir):\n        if is_image_file(fname):\n            images.append(fname)\n    return images\n\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\n\nclass ImagesFolder(data.Dataset):\n\n    def __init__(self, root, transform=None, loader=default_loader):\n        imgs = make_dataset(root)\n        if len(imgs) == 0:\n            raise(RuntimeError(""Found 0 images in subfolders of: "" + root + ""\\n""\n                               ""Supported image extensions are: "" + "","".join(IMG_EXTENSIONS)))\n        self.root = root\n        self.imgs = imgs\n        self.transform = transform\n        self.loader = loader\n\n    def __getitem__(self, index):\n        item = {}\n        item[\'name\'] = self.imgs[index]\n        item[\'path\'] = os.path.join(self.root, item[\'name\'])\n        if self.loader is not None:\n            item[\'visual\']  = self.loader(item[\'path\'])\n            if self.transform is not None:\n                item[\'visual\'] = self.transform(item[\'visual\'])\n        return item\n\n    def __len__(self):\n        return len(self.imgs)\n\n\nclass AbstractImagesDataset(data.Dataset):\n\n    def __init__(self, data_split, opt, transform=None, loader=default_loader):\n        self.data_split = data_split\n        self.opt = opt\n        self.transform = transform\n        self.loader = loader\n        self.dir_raw = os.path.join(self.opt[\'dir\'], \'raw\')\n\n        if not os.path.exists(self.get_dir_data()):\n            self._raw()\n\n    def get_dir_data(self):\n        return self.dir_raw\n\n    def get_by_name(self, image_name):\n        index = self.name_to_index[image_name]\n        return self[index]\n\n    def _raw(self):\n        raise NotImplementedError\n\n    def __getitem__(self, index):\n        raise NotImplementedError\n\n    def __len__(self):\n        raise NotImplementedError\n'"
vqa/datasets/utils.py,1,"b""import os\nimport pickle\nimport torch\nimport torch.utils.data as data\nimport copy\n\nclass AbstractVQADataset(data.Dataset):\n\n    def __init__(self, data_split, opt, dataset_img=None):\n        self.data_split = data_split\n        self.opt = copy.copy(opt)\n        self.dataset_img = dataset_img\n\n        self.dir_raw = os.path.join(self.opt['dir'], 'raw')\n        if not os.path.exists(self.dir_raw):\n            self._raw()\n  \n        self.dir_interim = os.path.join(self.opt['dir'], 'interim')\n        if not os.path.exists(self.dir_interim):\n            self._interim()\n  \n        self.dir_processed = os.path.join(self.opt['dir'], 'processed')\n        self.subdir_processed = self.subdir_processed()\n        if not os.path.exists(self.subdir_processed):\n            self._processed()\n\n        path_wid_to_word = os.path.join(self.subdir_processed, 'wid_to_word.pickle')\n        path_word_to_wid = os.path.join(self.subdir_processed, 'word_to_wid.pickle')\n        path_aid_to_ans  = os.path.join(self.subdir_processed, 'aid_to_ans.pickle')\n        path_ans_to_aid  = os.path.join(self.subdir_processed, 'ans_to_aid.pickle')\n        path_dataset     = os.path.join(self.subdir_processed, self.data_split+'set.pickle')\n        \n        with open(path_wid_to_word, 'rb') as handle:\n            self.wid_to_word = pickle.load(handle)\n  \n        with open(path_word_to_wid, 'rb') as handle:\n            self.word_to_wid = pickle.load(handle)\n  \n        with open(path_aid_to_ans, 'rb') as handle:\n            self.aid_to_ans = pickle.load(handle)\n  \n        with open(path_ans_to_aid, 'rb') as handle:\n            self.ans_to_aid = pickle.load(handle)\n \n        with open(path_dataset, 'rb') as handle:\n            self.dataset = pickle.load(handle)\n\n    def _raw(self):\n        raise NotImplementedError\n\n    def _interim(self):\n        raise NotImplementedError\n\n    def _processed(self):\n        raise NotImplementedError\n\n    def __getitem__(self, index):\n        raise NotImplementedError\n\n    def subdir_processed(self):\n        subdir = 'nans,' + str(self.opt['nans']) \\\n              + '_maxlength,' + str(self.opt['maxlength']) \\\n              + '_minwcount,' + str(self.opt['minwcount']) \\\n              + '_nlp,' + self.opt['nlp'] \\\n              + '_pad,' + self.opt['pad'] \\\n              + '_trainsplit,' + self.opt['trainsplit']\n        subdir = os.path.join(self.dir_processed, subdir)\n        return subdir"""
vqa/datasets/vgenome.py,2,"b'import os\nimport torch\nimport torch.utils.data as data\nimport copy\n\nfrom .images import ImagesFolder, AbstractImagesDataset, default_loader\nfrom .features import FeaturesDataset\nfrom .vgenome_interim import vgenome_interim\nfrom .vgenome_processed import vgenome_processed\nfrom .coco import default_transform\nfrom .utils import AbstractVQADataset\n\ndef raw(dir_raw):\n    dir_img = os.path.join(dir_raw, \'images\')\n    os.system(\'wget http://visualgenome.org/static/data/dataset/image_data.json.zip -P \'+dir_raw)\n    os.system(\'wget http://visualgenome.org/static/data/dataset/question_answers.json.zip -P \'+dir_raw)\n    os.system(\'wget https://cs.stanford.edu/people/rak248/VG_100K_2/images.zip -P \'+dir_raw)\n    os.system(\'wget https://cs.stanford.edu/people/rak248/VG_100K_2/images2.zip -P \'+dir_raw)\n\n    os.system(\'unzip \'+os.path.join(dir_raw, \'image_data.json.zip\')+\' -d \'+dir_raw)\n    os.system(\'unzip \'+os.path.join(dir_raw, \'question_answers.json.zip\')+\' -d \'+dir_raw)\n    os.system(\'unzip \'+os.path.join(dir_raw, \'images.zip\')+\' -d \'+dir_raw)\n    os.system(\'unzip \'+os.path.join(dir_raw, \'images2.zip\')+\' -d \'+dir_raw)\n\n    os.system(\'mv \'+os.path.join(dir_raw, \'VG_100K\')+\' \'+dir_img)\n\n    #os.system(\'mv \'+os.path.join(dir_raw, \'VG_100K_2\', \'*.jpg\')+\' \'+dir_img)\n    os.system(\'find \'+os.path.join(dir_raw, \'VG_100K_2\')+\' -type f -name \\\'*\\\' -exec mv {} \'+dir_img+\' \\\\;\')\n    os.system(\'rm -rf \'+os.path.join(dir_raw, \'VG_100K_2\'))\n\n    # remove images with 0 octet in a ugly but efficient way :\')\n    #print(\'for f in $(ls -lh \'+dir_img+\' | grep "" 0 "" | cut -s -f14 --delimiter="" ""); do rm \'+dir_img+\'/${f}; done;\')\n    os.system(\'for f in $(ls -lh \'+dir_img+\' | grep "" 0 "" | cut -s -f14 --delimiter="" ""); do echo \'+dir_img+\'/${f}; done;\')\n    os.system(\'for f in $(ls -lh \'+dir_img+\' | grep "" 0 "" | cut -s -f14 --delimiter="" ""); do rm \'+dir_img+\'/${f}; done;\')\n\n\nclass VisualGenome(AbstractVQADataset):\n\n    def __init__(self, data_split, opt, dataset_img=None):\n        super(VisualGenome, self).__init__(data_split, opt, dataset_img)\n\n    def __getitem__(self, index):\n        item_qa = self.dataset[index]\n        item = {}\n        if self.dataset_img is not None:\n            item_img = self.dataset_img.get_by_name(item_qa[\'image_name\'])\n            item[\'visual\'] = item_img[\'visual\']\n            # DEBUG\n            #item[\'visual_debug\'] = item_qa[\'image_name\']\n        item[\'question\'] = torch.LongTensor(item_qa[\'question_wids\'])\n        # DEBUG\n        #item[\'question_debug\'] = item_qa[\'question\']\n        item[\'question_id\'] = item_qa[\'question_id\']\n        item[\'answer\'] = item_qa[\'answer_aid\']\n        # DEBUG\n        #item[\'answer_debug\'] = item_qa[\'answer\']\n        return item\n\n    def _raw(self):\n        raw(self.dir_raw)\n\n    def _interim(self):\n        vgenome_interim(self.opt)\n\n    def _processed(self):\n        vgenome_processed(self.opt)\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nclass VisualGenomeImages(AbstractImagesDataset):\n\n    def __init__(self, data_split, opt, transform=None, loader=default_loader):\n        super(VisualGenomeImages, self).__init__(data_split, opt, transform, loader)\n        self.dir_img = os.path.join(self.dir_raw, \'images\')\n        self.dataset = ImagesFolder(self.dir_img, transform=self.transform, loader=self.loader)\n        self.name_to_index = self._load_name_to_index()\n\n    def _raw(self):\n        raw(self.dir_raw)\n\n    def _load_name_to_index(self):\n        self.name_to_index = {name:index for index, name in enumerate(self.dataset.imgs)}\n        return self.name_to_index\n\n    def __getitem__(self, index):\n        item = self.dataset[index]\n        return item\n\n    def __len__(self):\n        return len(self.dataset)\n\n\ndef factory(opt, vqa=False, transform=None):\n\n    if vqa:\n        dataset_img = factory(opt, vqa=False, transform=transform)\n        return VisualGenome(\'train\', opt, dataset_img)\n\n    if opt[\'mode\'] == \'img\':\n        if transform is None:\n            transform = default_transform(opt[\'size\'])\n\n    elif opt[\'mode\'] in [\'noatt\', \'att\']:\n        return FeaturesDataset(\'train\', opt)\n        \n    else:\n        raise ValueError\n\n\n'"
vqa/datasets/vgenome_interim.py,0,"b'import json\nimport os\nimport argparse\n\n# def get_image_path(subtype=\'train2014\', image_id=\'1\', format=\'%s/COCO_%s_%012d.jpg\'):\n#     return format%(subtype, subtype, image_id)\n\ndef interim(questions_annotations):\n    data = []\n    for i in range(len(questions_annotations)):\n        qa_img = questions_annotations[i]\n        qa_img_id = qa_img[\'id\']\n        for j in range(len(qa_img[\'qas\'])):\n            qa = qa_img[\'qas\'][j]\n            row = {}\n            row[\'question_id\'] = qa[\'qa_id\']\n            row[\'image_id\'] = qa_img_id\n            row[\'image_name\'] = str(qa_img_id) + \'.jpg\'\n            row[\'question\'] = qa[\'question\']\n            row[\'answer\'] = qa[\'answer\']\n            data.append(row)\n    return data\n\ndef vgenome_interim(params):\n    \'\'\'\n    Put the VisualGenomme VQA data into single json file in data/interim\n    or train, val, trainval : [[question_id, image_id, question, answer] ... ]\n    \'\'\'\n    path_qa = os.path.join(params[\'dir\'], \'interim\', \'questions_annotations.json\')\n    os.system(\'mkdir -p \' + os.path.join(params[\'dir\'], \'interim\'))\n\n    print(\'Loading annotations and questions...\')\n    questions_annotations = json.load(open(os.path.join(params[\'dir\'], \'raw\', \'question_answers.json\'), \'r\'))\n    \n    data = interim(questions_annotations)\n    print(\'Questions number %d\'%len(data))\n    print(\'Write\', path_qa)\n    json.dump(data, open(path_qa, \'w\'))\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--dir_vg\', default=\'data/visualgenome\', type=str, help=\'Path to visual genome data directory\')\n    args = parser.parse_args()\n    params = vars(args)\n    vgenome_interim(params)\n'"
vqa/datasets/vgenome_processed.py,0,"b'""""""\nPreprocess an interim json data files\ninto one preprocess hdf5/json data files.\nCaption: Use nltk, or mcb, or split function to get tokens. \n""""""\nfrom random import shuffle, seed\nimport sys\nimport os.path\nimport argparse\nimport numpy as np\nimport scipy.io\nimport pdb\nimport h5py\nfrom nltk.tokenize import word_tokenize\nimport json\nimport csv\nimport re\nimport math\nimport pickle\n\nfrom .vqa_processed import get_top_answers, remove_examples, tokenize, tokenize_mcb, \\\n                           preprocess_questions, remove_long_tail_train, \\\n                           encode_question, encode_answer\n\ndef preprocess_answers(examples, nlp=\'nltk\'):\n    print(\'Example of modified answers after preprocessing:\')\n    for i, ex in enumerate(examples):\n        s = ex[\'answer\']\n        if nlp == \'nltk\':\n            ex[\'answer\'] = "" "".join(word_tokenize(str(s).lower()))\n        elif nlp == \'mcb\':\n            ex[\'answer\'] = "" "".join(tokenize_mcb(s))\n        else:\n            ex[\'answer\'] = "" "".join(tokenize(s))\n        if i < 10: print(s, \'became\', ""->""+ex[\'answer\']+""<-"")\n        if i>0 and i % 1000 == 0:\n            sys.stdout.write(""processing %d/%d (%.2f%% done)   \\r"" %  (i, len(examples), i*100.0/len(examples)) )\n            sys.stdout.flush() \n    return examples\n\ndef build_csv(path, examples, split=\'train\', delimiter_col=\'~\', delimiter_number=\'|\'):\n    with open(path, \'wb\') as f:\n        writer = csv.writer(f, delimiter=delimiter_col)\n        for ex in examples:\n            import ipdb; ipdb.set_trace()\n            row = []\n            row.append(ex[\'question_id\'])\n            row.append(ex[\'question\'])\n            row.append(delimiter_number.join(ex[\'question_words_UNK\']))\n            row.append(delimiter_number.join(ex[\'question_wids\']))\n\n            row.append(ex[\'image_id\'])\n\n            if split in [\'train\',\'val\',\'trainval\']:\n                row.append(ex[\'answer_aid\'])\n                row.append(ex[\'answer\'])\n            writer.writerow(row)\n\ndef vgenome_processed(params):\n    \n    #####################################################\n    ## Read input files\n    #####################################################\n\n    path_train = os.path.join(params[\'dir\'], \'interim\', \'questions_annotations.json\')\n \n    # An example is a tuple (question, image, answer)\n    # /!\\ test and test-dev have no answer\n    trainset = json.load(open(path_train, \'r\'))\n\n    #####################################################\n    ## Preprocess examples (questions and answers)\n    #####################################################\n\n    trainset = preprocess_answers(trainset, params[\'nlp\'])\n\n    top_answers = get_top_answers(trainset, params[\'nans\'])\n    aid_to_ans = {i+1:w for i,w in enumerate(top_answers)}\n    ans_to_aid = {w:i+1 for i,w in enumerate(top_answers)}\n\n    # Remove examples if answer is not in top answers\n    #trainset = remove_examples(trainset, ans_to_aid)\n\n    # Add \'question_words\' to the initial tuple\n    trainset = preprocess_questions(trainset, params[\'nlp\'])\n\n    # Also process top_words which contains a UNK char\n    trainset, top_words = remove_long_tail_train(trainset, params[\'minwcount\'])\n    wid_to_word = {i+1:w for i,w in enumerate(top_words)}\n    word_to_wid = {w:i+1 for i,w in enumerate(top_words)}\n\n    #examples_test = remove_long_tail_test(examples_test, word_to_wid)\n\n    trainset = encode_question(trainset, word_to_wid, params[\'maxlength\'], params[\'pad\'])\n\n    trainset = encode_answer(trainset, ans_to_aid)\n\n    #####################################################\n    ## Write output files\n    #####################################################\n\n    # Paths to output files\n    # Ex: data/vqa/preprocess/nans,3000_maxlength,15_..._trainsplit,train_testsplit,val/id_to_word.json\n    subdirname = \'nans,\'+str(params[\'nans\'])\n    for param in [\'maxlength\', \'minwcount\', \'nlp\', \'pad\', \'trainsplit\']:\n        subdirname += \'_\' + param + \',\' + str(params[param])\n    os.system(\'mkdir -p \' + os.path.join(params[\'dir\'], \'processed\', subdirname))\n\n    path_wid_to_word = os.path.join(params[\'dir\'], \'processed\', subdirname, \'wid_to_word.pickle\')\n    path_word_to_wid = os.path.join(params[\'dir\'], \'processed\', subdirname, \'word_to_wid.pickle\')\n    path_aid_to_ans  = os.path.join(params[\'dir\'], \'processed\', subdirname, \'aid_to_ans.pickle\')\n    path_ans_to_aid  = os.path.join(params[\'dir\'], \'processed\', subdirname, \'ans_to_aid.pickle\')\n    #path_csv_train   = os.path.join(params[\'dir\'], \'processed\', subdirname, \'train.csv\')\n    path_trainset = os.path.join(params[\'dir\'], \'processed\', subdirname, \'trainset.pickle\')\n\n    print(\'Write wid_to_word to\', path_wid_to_word)\n    with open(path_wid_to_word, \'wb\') as handle:\n        pickle.dump(wid_to_word, handle)\n\n    print(\'Write word_to_wid to\', path_word_to_wid)\n    with open(path_word_to_wid, \'wb\') as handle:\n        pickle.dump(word_to_wid, handle)\n\n    print(\'Write aid_to_ans to\', path_aid_to_ans)\n    with open(path_aid_to_ans, \'wb\') as handle:\n        pickle.dump(aid_to_ans, handle)\n\n    print(\'Write ans_to_aid to\', path_ans_to_aid)\n    with open(path_ans_to_aid, \'wb\') as handle:\n        pickle.dump(ans_to_aid, handle)\n\n    print(\'Write trainset to\', path_trainset)\n    with open(path_trainset, \'wb\') as handle:\n        pickle.dump(trainset, handle)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--dir_vg\',\n        default=\'data/visualgenome\',\n        type=str,\n        help=\'Root directory containing raw, interim and processed directories\'\n    )\n    parser.add_argument(\'--nans\',\n        default=10000,\n        type=int,\n        help=\'Number of top answers for the final classifications\'\n    )\n    parser.add_argument(\'--maxlength\',\n        default=26,\n        type=int,\n        help=\'Max number of words in a caption. Captions longer get clipped\'\n    )\n    parser.add_argument(\'--minwcount\',\n        default=0,\n        type=int,\n        help=\'Words that occur less than that are removed from vocab\'\n    )\n    parser.add_argument(\'--nlp\',\n        default=\'mcb\',\n        type=str,\n        help=\'Token method ; Options: nltk | mcb | naive\'\n    )\n    parser.add_argument(\'--pad\',\n        default=\'left\',\n        type=str,\n        help=\'Padding ; Options: right (finish by zeros) | left (begin by zeros)\'\n    )\n    args = parser.parse_args()\n    params = vars(args)\n    vgenome_processed(params)'"
vqa/datasets/vqa.py,2,"b'import os\nimport pickle\nimport torch\nimport torch.utils.data as data\nimport copy\nimport numpy as np\n\nfrom ..lib import utils\nfrom ..lib.dataloader import DataLoader\nfrom .utils import AbstractVQADataset\nfrom .vqa_interim import vqa_interim\nfrom .vqa2_interim import vqa_interim as vqa2_interim\nfrom .vqa_processed import vqa_processed\nfrom . import coco\nfrom . import vgenome\n\n\nclass AbstractVQA(AbstractVQADataset):\n\n    def __init__(self, data_split, opt, dataset_img=None):\n        super(AbstractVQA, self).__init__(data_split, opt, dataset_img)\n\n        if \'train\' not in self.data_split: # means self.data_split is \'val\' or \'test\'\n            self.opt[\'samplingans\'] = False\n        assert \'samplingans\' in self.opt, \\\n               ""opt[\'vqa\'] does not have \'samplingans\' ""\\\n               ""entry. Set it to True or False.""\n  \n        if self.data_split == \'test\':\n            path_testdevset = os.path.join(self.subdir_processed, \'testdevset.pickle\')\n            with open(path_testdevset, \'rb\') as handle:\n                self.testdevset_vqa = pickle.load(handle)\n            self.is_qid_testdev = {}\n            for i in range(len(self.testdevset_vqa)):\n                qid = self.testdevset_vqa[i][\'question_id\']\n                self.is_qid_testdev[qid] = True\n\n    def _raw(self):\n        raise NotImplementedError\n\n    def _interim(self):\n        raise NotImplementedError\n\n    def _processed(self):\n        raise NotImplementedError\n\n    def __getitem__(self, index):\n        item = {}\n        # TODO: better handle cascade of dict items\n        item_vqa = self.dataset[index]\n\n        # Process Visual (image or features)\n        if self.dataset_img is not None:\n            item_img = self.dataset_img.get_by_name(item_vqa[\'image_name\'])\n            item[\'visual\'] = item_img[\'visual\']\n        \n        # Process Question (word token)\n        item[\'question_id\'] = item_vqa[\'question_id\']\n        item[\'question\'] = torch.LongTensor(item_vqa[\'question_wids\'])\n\n        if self.data_split == \'test\':\n            if item[\'question_id\'] in self.is_qid_testdev:\n                item[\'is_testdev\'] = True\n            else:\n                item[\'is_testdev\'] = False\n        else:\n        ## Process Answer if exists\n            if self.opt[\'samplingans\']:\n                proba = item_vqa[\'answers_count\']\n                proba = proba / np.sum(proba)\n                item[\'answer\'] = int(np.random.choice(item_vqa[\'answers_aid\'], p=proba))\n            else:\n                item[\'answer\'] = item_vqa[\'answer_aid\']\n\n        return item\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def num_classes(self):\n        return len(self.aid_to_ans)\n\n    def vocab_words(self):\n        return list(self.wid_to_word.values())\n\n    def vocab_answers(self):\n        return self.aid_to_ans\n\n    def data_loader(self, batch_size=10, num_workers=4, shuffle=False):\n        return DataLoader(self,\n            batch_size=batch_size, shuffle=shuffle,\n            num_workers=num_workers, pin_memory=True)\n \n    def split_name(self, testdev=False):\n        if testdev:\n            return \'test-dev2015\'\n        if self.data_split in [\'train\', \'val\']:\n            return self.data_split+\'2014\'\n        elif self.data_split == \'test\':\n            return self.data_split+\'2015\'\n        elif self.data_split == \'testdev\':\n            return \'test-dev2015\'\n        else:\n            assert False, \'Wrong data_split: {}\'.format(self.data_split)\n \n    def subdir_processed(self):\n        subdir = \'nans,\' + str(self.opt[\'nans\']) \\\n              + \'_maxlength,\' + str(self.opt[\'maxlength\']) \\\n              + \'_minwcount,\' + str(self.opt[\'minwcount\']) \\\n              + \'_nlp,\' + self.opt[\'nlp\'] \\\n              + \'_pad,\' + self.opt[\'pad\'] \\\n              + \'_trainsplit,\' + self.opt[\'trainsplit\']\n        subdir = os.path.join(self.dir_processed, subdir)\n        return subdir\n\n\nclass VQA(AbstractVQA):\n\n    def __init__(self, data_split, opt, dataset_img=None):\n        super(VQA, self).__init__(data_split, opt, dataset_img)\n\n    def _raw(self):\n        dir_zip = os.path.join(self.dir_raw, \'zip\')\n        dir_ann = os.path.join(self.dir_raw, \'annotations\')\n        os.system(\'mkdir -p \'+dir_zip)\n        os.system(\'mkdir -p \'+dir_ann)\n        os.system(\'wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/Questions_Train_mscoco.zip -P \'+dir_zip)\n        os.system(\'wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/Questions_Val_mscoco.zip -P \'+dir_zip)\n        os.system(\'wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/Questions_Test_mscoco.zip -P \'+dir_zip)\n        os.system(\'wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/Annotations_Train_mscoco.zip -P \'+dir_zip)\n        os.system(\'wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/Annotations_Val_mscoco.zip -P \'+dir_zip)\n        os.system(\'unzip \'+os.path.join(dir_zip, \'Questions_Train_mscoco.zip\')+\' -d \'+dir_ann)\n        os.system(\'unzip \'+os.path.join(dir_zip, \'Questions_Val_mscoco.zip\')+\' -d \'+dir_ann)\n        os.system(\'unzip \'+os.path.join(dir_zip, \'Questions_Test_mscoco.zip\')+\' -d \'+dir_ann)\n        os.system(\'unzip \'+os.path.join(dir_zip, \'Annotations_Train_mscoco.zip\')+\' -d \'+dir_ann)\n        os.system(\'unzip \'+os.path.join(dir_zip, \'Annotations_Val_mscoco.zip\')+\' -d \'+dir_ann)\n\n    def _interim(self):\n        vqa_interim(self.opt[\'dir\'])\n\n    def _processed(self):\n        vqa_processed(self.opt)\n\n\nclass VQA2(AbstractVQA):\n\n    def __init__(self, data_split, opt, dataset_img=None):\n        super(VQA2, self).__init__(data_split, opt, dataset_img)\n\n    def _raw(self):\n        dir_zip = os.path.join(self.dir_raw, \'zip\')\n        dir_ann = os.path.join(self.dir_raw, \'annotations\')\n        os.system(\'mkdir -p \'+dir_zip)\n        os.system(\'mkdir -p \'+dir_ann)\n        os.system(\'wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip -P \'+dir_zip)\n        os.system(\'wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip -P \'+dir_zip)\n        os.system(\'wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Test_mscoco.zip -P \'+dir_zip)\n        os.system(\'wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip -P \'+dir_zip)\n        os.system(\'wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip -P \'+dir_zip)\n        os.system(\'unzip \'+os.path.join(dir_zip, \'v2_Questions_Train_mscoco.zip\')+\' -d \'+dir_ann)\n        os.system(\'unzip \'+os.path.join(dir_zip, \'v2_Questions_Val_mscoco.zip\')+\' -d \'+dir_ann)\n        os.system(\'unzip \'+os.path.join(dir_zip, \'v2_Questions_Test_mscoco.zip\')+\' -d \'+dir_ann)\n        os.system(\'unzip \'+os.path.join(dir_zip, \'v2_Annotations_Train_mscoco.zip\')+\' -d \'+dir_ann)\n        os.system(\'unzip \'+os.path.join(dir_zip, \'v2_Annotations_Val_mscoco.zip\')+\' -d \'+dir_ann)\n        os.system(\'mv \'+os.path.join(dir_ann, \'v2_mscoco_train2014_annotations.json\')+\' \'\n                       +os.path.join(dir_ann, \'mscoco_train2014_annotations.json\'))\n        os.system(\'mv \'+os.path.join(dir_ann, \'v2_mscoco_val2014_annotations.json\')+\' \'\n                       +os.path.join(dir_ann, \'mscoco_val2014_annotations.json\'))\n        os.system(\'mv \'+os.path.join(dir_ann, \'v2_OpenEnded_mscoco_train2014_questions.json\')+\' \'\n                       +os.path.join(dir_ann, \'OpenEnded_mscoco_train2014_questions.json\'))\n        os.system(\'mv \'+os.path.join(dir_ann, \'v2_OpenEnded_mscoco_val2014_questions.json\')+\' \'\n                       +os.path.join(dir_ann, \'OpenEnded_mscoco_val2014_questions.json\'))\n        os.system(\'mv \'+os.path.join(dir_ann, \'v2_OpenEnded_mscoco_test2015_questions.json\')+\' \'\n                       +os.path.join(dir_ann, \'OpenEnded_mscoco_test2015_questions.json\'))\n        os.system(\'mv \'+os.path.join(dir_ann, \'v2_OpenEnded_mscoco_test-dev2015_questions.json\')+\' \'\n                       +os.path.join(dir_ann, \'OpenEnded_mscoco_test-dev2015_questions.json\'))\n\n    def _interim(self):\n        vqa2_interim(self.opt[\'dir\'])\n\n    def _processed(self):\n        vqa_processed(self.opt)\n\n\nclass VQAVisualGenome(data.Dataset):\n\n    def __init__(self, dataset_vqa, dataset_vgenome):\n        self.dataset_vqa = dataset_vqa\n        self.dataset_vgenome = dataset_vgenome\n        self._filter_dataset_vgenome()\n\n    def _filter_dataset_vgenome(self):\n        print(\'-> Filtering dataset vgenome\')\n        data_vg = self.dataset_vgenome.dataset\n        ans_to_aid = self.dataset_vqa.ans_to_aid\n        word_to_wid = self.dataset_vqa.word_to_wid\n        data_vg_new = []\n        not_in = 0\n        for i in range(len(data_vg)):\n            if data_vg[i][\'answer\'] not in ans_to_aid:\n                not_in += 1\n            else:\n                data_vg[i][\'answer_aid\'] = ans_to_aid[data_vg[i][\'answer\']]\n                for j in range(data_vg[i][\'seq_length\']):\n                    word = data_vg[i][\'question_words_UNK\'][j]\n                    if word in word_to_wid:\n                        wid = word_to_wid[word]\n                    else:\n                        wid = word_to_wid[\'UNK\']\n                    data_vg[i][\'question_wids\'][j] = wid\n                data_vg_new.append(data_vg[i])\n        print(\'-> {} / {} items removed\'.format(not_in, len(data_vg)))\n        self.dataset_vgenome.dataset = data_vg_new\n        print(\'-> {} items left in visual genome\'.format(len(self.dataset_vgenome)))\n        print(\'-> {} items total in vqa+vg\'.format(len(self)))\n                \n\n    def __getitem__(self, index):\n        if index < len(self.dataset_vqa):\n            item = self.dataset_vqa[index]\n            #print(\'vqa\')\n        else:\n            item = self.dataset_vgenome[index - len(self.dataset_vqa)]\n            #print(\'vg\')\n        #import ipdb; ipdb.set_trace()\n        return item\n\n    def __len__(self):\n        return len(self.dataset_vqa) + len(self.dataset_vgenome)\n\n    def num_classes(self):\n        return self.dataset_vqa.num_classes()\n\n    def vocab_words(self):\n        return self.dataset_vqa.vocab_words()\n\n    def vocab_answers(self):\n        return self.dataset_vqa.vocab_answers()\n\n    def data_loader(self, batch_size=10, num_workers=4, shuffle=False):\n        return DataLoader(self,\n            batch_size=batch_size, shuffle=shuffle,\n            num_workers=num_workers, pin_memory=True)\n\n    def split_name(self, testdev=False):\n        return self.dataset_vqa.split_name(testdev=testdev)\n\n\ndef factory(data_split, opt, opt_coco=None, opt_vgenome=None):\n    dataset_img = None\n\n    if opt_coco is not None:\n        dataset_img = coco.factory(data_split, opt_coco)\n\n    if opt[\'dataset\'] == \'VQA\' and \'2\' not in opt[\'dir\']: # sanity check\n        dataset_vqa = VQA(data_split, opt, dataset_img)\n    elif opt[\'dataset\'] == \'VQA2\' and \'2\' in opt[\'dir\']: # sanity check\n        dataset_vqa = VQA2(data_split, opt, dataset_img)\n    else:\n        raise ValueError\n\n    if opt_vgenome is not None:\n        dataset_vgenome = vgenome.factory(opt_vgenome, vqa=True)\n        return VQAVisualGenome(dataset_vqa, dataset_vgenome)\n    else:\n        return dataset_vqa\n\n'"
vqa/datasets/vqa2_interim.py,0,"b'import json\nimport os\nimport argparse\nfrom collections import Counter\n\ndef get_subtype(split=\'train\'):\n    if split in [\'train\', \'val\']:\n        return split + \'2014\'\n    else:\n        return \'test2015\'\n\ndef get_image_name_old(subtype=\'train2014\', image_id=\'1\', format=\'%s/COCO_%s_%012d.jpg\'):\n    return format%(subtype, subtype, image_id)\n\ndef get_image_name(subtype=\'train2014\', image_id=\'1\', format=\'COCO_%s_%012d.jpg\'):\n    return format%(subtype, image_id)\n\ndef interim(questions, split=\'train\', annotations=[]):\n    print(\'Interim\', split)\n    data = []\n    for i in range(len(questions)):\n        row = {}\n        row[\'question_id\'] = questions[i][\'question_id\']\n        row[\'image_name\'] = get_image_name(get_subtype(split), questions[i][\'image_id\'])\n        row[\'question\'] = questions[i][\'question\']\n        #row[\'MC_answer\'] = questions[i][\'multiple_choices\']\n        if split in [\'train\', \'val\', \'trainval\']:\n            row[\'answer\'] = annotations[i][\'multiple_choice_answer\']\n            answers = []\n            for ans in annotations[i][\'answers\']:\n                answers.append(ans[\'answer\'])\n            row[\'answers_occurence\'] = Counter(answers).most_common()\n        data.append(row)\n    return data\n\ndef vqa_interim(dir_vqa):\n    \'\'\'\n    Put the VQA data into single json file in data/interim\n    or train, val, trainval : [[question_id, image_name, question, MC_answer, answer] ... ]\n    or test, test-dev :       [[question_id, image_name, question, MC_answer] ... ]\n    \'\'\'\n\n    path_train_qa    = os.path.join(dir_vqa, \'interim\', \'train_questions_annotations.json\')\n    path_val_qa      = os.path.join(dir_vqa, \'interim\', \'val_questions_annotations.json\')\n    path_trainval_qa = os.path.join(dir_vqa, \'interim\', \'trainval_questions_annotations.json\')\n    path_test_q      = os.path.join(dir_vqa, \'interim\', \'test_questions.json\')\n    path_testdev_q   = os.path.join(dir_vqa, \'interim\', \'testdev_questions.json\')\n\n    os.system(\'mkdir -p \' + os.path.join(dir_vqa, \'interim\'))\n\n    print(\'Loading annotations and questions...\')\n    annotations_train = json.load(open(os.path.join(dir_vqa, \'raw\', \'annotations\', \'mscoco_train2014_annotations.json\'), \'r\'))\n    annotations_val   = json.load(open(os.path.join(dir_vqa, \'raw\', \'annotations\', \'mscoco_val2014_annotations.json\'), \'r\'))\n    questions_train   = json.load(open(os.path.join(dir_vqa, \'raw\', \'annotations\', \'OpenEnded_mscoco_train2014_questions.json\'), \'r\'))\n    questions_val     = json.load(open(os.path.join(dir_vqa, \'raw\', \'annotations\', \'OpenEnded_mscoco_val2014_questions.json\'), \'r\'))\n    questions_test    = json.load(open(os.path.join(dir_vqa, \'raw\', \'annotations\', \'OpenEnded_mscoco_test2015_questions.json\'), \'r\'))\n    questions_testdev = json.load(open(os.path.join(dir_vqa, \'raw\', \'annotations\', \'OpenEnded_mscoco_test-dev2015_questions.json\'), \'r\'))\n\n    data_train = interim(questions_train[\'questions\'], \'train\', annotations_train[\'annotations\'])\n    print(\'Train size %d\'%len(data_train))\n    print(\'Write\', path_train_qa)\n    json.dump(data_train, open(path_train_qa, \'w\'))\n\n    data_val = interim(questions_val[\'questions\'], \'val\', annotations_val[\'annotations\'])\n    print(\'Val size %d\'%len(data_val))\n    print(\'Write\', path_val_qa)\n    json.dump(data_val, open(path_val_qa, \'w\'))\n\n    print(\'Concat. train and val\')\n    data_trainval = data_train + data_val\n    print(\'Trainval size %d\'%len(data_trainval))\n    print(\'Write\', path_trainval_qa)\n    json.dump(data_trainval, open(path_trainval_qa, \'w\'))\n\n    data_testdev = interim(questions_testdev[\'questions\'], \'testdev\')\n    print(\'Testdev size %d\'%len(data_testdev))\n    print(\'Write\', path_testdev_q)\n    json.dump(data_testdev, open(path_testdev_q, \'w\'))\n\n    data_test = interim(questions_test[\'questions\'], \'test\')\n    print(\'Test size %d\'%len(data_test))\n    print(\'Write\', path_test_q)\n    json.dump(data_test, open(path_test_q, \'w\'))\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--dir_vqa\', default=\'data/vqa\', type=str, help=\'Path to vqa data directory\')\n    args = parser.parse_args()\n    vqa_interim(args.dir_vqa)\n'"
vqa/datasets/vqa_interim.py,0,"b'import json\nimport os\nimport argparse\nfrom collections import Counter\n\ndef get_subtype(split=\'train\'):\n    if split in [\'train\', \'val\']:\n        return split + \'2014\'\n    else:\n        return \'test2015\'\n\ndef get_image_name_old(subtype=\'train2014\', image_id=\'1\', format=\'%s/COCO_%s_%012d.jpg\'):\n    return format%(subtype, subtype, image_id)\n\ndef get_image_name(subtype=\'train2014\', image_id=\'1\', format=\'COCO_%s_%012d.jpg\'):\n    return format%(subtype, image_id)\n\ndef interim(questions, split=\'train\', annotations=[]):\n    print(\'Interim\', split)\n    data = []\n    for i in range(len(questions)):\n        row = {}\n        row[\'question_id\'] = questions[i][\'question_id\']\n        row[\'image_name\'] = get_image_name(get_subtype(split), questions[i][\'image_id\'])\n        row[\'question\'] = questions[i][\'question\']\n        row[\'MC_answer\'] = questions[i][\'multiple_choices\']\n        if split in [\'train\', \'val\', \'trainval\']:\n            row[\'answer\'] = annotations[i][\'multiple_choice_answer\']\n            answers = []\n            for ans in annotations[i][\'answers\']:\n                answers.append(ans[\'answer\'])\n            row[\'answers_occurence\'] = Counter(answers).most_common()\n        data.append(row)\n    return data\n\ndef vqa_interim(dir_vqa):\n    \'\'\'\n    Put the VQA data into single json file in data/interim\n    or train, val, trainval : [[question_id, image_name, question, MC_answer, answer] ... ]\n    or test, test-dev :       [[question_id, image_name, question, MC_answer] ... ]\n    \'\'\'\n\n    path_train_qa    = os.path.join(dir_vqa, \'interim\', \'train_questions_annotations.json\')\n    path_val_qa      = os.path.join(dir_vqa, \'interim\', \'val_questions_annotations.json\')\n    path_trainval_qa = os.path.join(dir_vqa, \'interim\', \'trainval_questions_annotations.json\')\n    path_test_q      = os.path.join(dir_vqa, \'interim\', \'test_questions.json\')\n    path_testdev_q   = os.path.join(dir_vqa, \'interim\', \'testdev_questions.json\')\n\n    os.system(\'mkdir -p \' + os.path.join(dir_vqa, \'interim\'))\n\n    print(\'Loading annotations and questions...\')\n    annotations_train = json.load(open(os.path.join(dir_vqa, \'raw\', \'annotations\', \'mscoco_train2014_annotations.json\'), \'r\'))\n    annotations_val   = json.load(open(os.path.join(dir_vqa, \'raw\', \'annotations\', \'mscoco_val2014_annotations.json\'), \'r\'))\n    questions_train   = json.load(open(os.path.join(dir_vqa, \'raw\', \'annotations\', \'MultipleChoice_mscoco_train2014_questions.json\'), \'r\'))\n    questions_val     = json.load(open(os.path.join(dir_vqa, \'raw\', \'annotations\', \'MultipleChoice_mscoco_val2014_questions.json\'), \'r\'))\n    questions_test    = json.load(open(os.path.join(dir_vqa, \'raw\', \'annotations\', \'MultipleChoice_mscoco_test2015_questions.json\'), \'r\'))\n    questions_testdev = json.load(open(os.path.join(dir_vqa, \'raw\', \'annotations\', \'MultipleChoice_mscoco_test-dev2015_questions.json\'), \'r\'))\n\n    data_train = interim(questions_train[\'questions\'], \'train\', annotations_train[\'annotations\'])\n    print(\'Train size %d\'%len(data_train))\n    print(\'Write\', path_train_qa)\n    json.dump(data_train, open(path_train_qa, \'w\'))\n\n    data_val = interim(questions_val[\'questions\'], \'val\', annotations_val[\'annotations\'])\n    print(\'Val size %d\'%len(data_val))\n    print(\'Write\', path_val_qa)\n    json.dump(data_val, open(path_val_qa, \'w\'))\n\n    print(\'Concat. train and val\')\n    data_trainval = data_train + data_val\n    print(\'Trainval size %d\'%len(data_trainval))\n    print(\'Write\', path_trainval_qa)\n    json.dump(data_trainval, open(path_trainval_qa, \'w\'))\n\n    data_testdev = interim(questions_testdev[\'questions\'], \'testdev\')\n    print(\'Testdev size %d\'%len(data_testdev))\n    print(\'Write\', path_testdev_q)\n    json.dump(data_testdev, open(path_testdev_q, \'w\'))\n\n    data_test = interim(questions_test[\'questions\'], \'test\')\n    print(\'Test size %d\'%len(data_test))\n    print(\'Write\', path_test_q)\n    json.dump(data_test, open(path_test_q, \'w\'))\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--dir_vqa\', default=\'data/vqa\', type=str, help=\'Path to vqa data directory\')\n    args = parser.parse_args()\n    vqa_interim(args.dir_vqa)\n'"
vqa/datasets/vqa_processed.py,0,"b'""""""\nPreprocess a train/test pair of interim json data files.\nCaption: Use NLTK or split function to get tokens. \n""""""\nfrom random import shuffle, seed\nimport sys\nimport os.path\nimport argparse\nimport numpy as np\nimport scipy.io\nimport pdb\nimport json\nimport csv\nimport re\nimport math\nimport pickle\n#import pprint\n\ndef get_top_answers(examples, nans=3000):\n    counts = {}\n    for ex in examples:\n        ans = ex[\'answer\'] \n        counts[ans] = counts.get(ans, 0) + 1\n\n    cw = sorted([(count,w) for w,count in counts.items()], reverse=True)\n    print(\'Top answer and their counts:\'    )\n    print(\'\\n\'.join(map(str,cw[:20])))\n\n    vocab = []\n    for i in range(nans):\n        vocab.append(cw[i][1])\n    return vocab[:nans]\n\ndef remove_examples(examples, ans_to_aid):\n    new_examples = []\n    for i, ex in enumerate(examples):\n        if ex[\'answer\'] in ans_to_aid:\n            new_examples.append(ex)\n    print(\'Number of examples reduced from %d to %d \'%(len(examples), len(new_examples)))\n    return new_examples\n\ndef tokenize(sentence):\n    return [i for i in re.split(r""([-.\\""\',:? !\\$#@~()*&\\^%;\\[\\]/\\\\\\+<>\\n=])"", sentence) if i!=\'\' and i!=\' \' and i!=\'\\n\'];\n\ndef tokenize_mcb(s):\n    t_str = s.lower()\n    for i in [r\'\\?\',r\'\\!\',r\'\\\'\',r\'\\""\',r\'\\$\',r\'\\:\',r\'\\@\',r\'\\(\',r\'\\)\',r\'\\,\',r\'\\.\',r\'\\;\']:\n        t_str = re.sub( i, \'\', t_str)\n    for i in [r\'\\-\',r\'\\/\']:\n        t_str = re.sub( i, \' \', t_str)\n    q_list = re.sub(r\'\\?\',\'\',t_str.lower()).split(\' \')\n    q_list = list(filter(lambda x: len(x) > 0, q_list))\n    return q_list\n\ndef preprocess_questions(examples, nlp=\'nltk\'):\n    if nlp == \'nltk\':\n        from nltk.tokenize import word_tokenize\n    print(\'Example of generated tokens after preprocessing some questions:\')\n    for i, ex in enumerate(examples):\n        s = ex[\'question\']\n        if nlp == \'nltk\':\n            ex[\'question_words\'] = word_tokenize(str(s).lower())\n        elif nlp == \'mcb\':\n            ex[\'question_words\'] = tokenize_mcb(s)\n        else:\n            ex[\'question_words\'] = tokenize(s)\n        if i < 10:\n            print(ex[\'question_words\'])\n        if i % 1000 == 0:\n            sys.stdout.write(""processing %d/%d (%.2f%% done)   \\r"" %  (i, len(examples), i*100.0/len(examples)) )\n            sys.stdout.flush() \n    return examples\n\ndef remove_long_tail_train(examples, minwcount=0):\n    # Replace words which are in the long tail (counted less than \'minwcount\' times) by the UNK token.\n    # Also create vocab, a list of the final words.\n\n    # count up the number of words\n    counts = {}\n    for ex in examples:\n        for w in ex[\'question_words\']:\n            counts[w] = counts.get(w, 0) + 1\n    cw = sorted([(count,w) for w, count in counts.items()], reverse=True)\n    print(\'Top words and their counts:\')\n    print(\'\\n\'.join(map(str,cw[:20])))\n\n    total_words = sum(counts.values())\n    print(\'Total words:\', total_words)\n    bad_words = [w for w,n in counts.items() if n <= minwcount]\n    vocab     = [w for w,n in counts.items() if n > minwcount]\n    bad_count = sum(counts[w] for w in bad_words)\n    print(\'Number of bad words: %d/%d = %.2f%%\' % (len(bad_words), len(counts), len(bad_words)*100.0/len(counts)))\n    print(\'Number of words in vocab would be %d\' % (len(vocab), ))\n    print(\'Number of UNKs: %d/%d = %.2f%%\' % (bad_count, total_words, bad_count*100.0/total_words))\n\n    print(\'Insert the special UNK token\')\n    vocab.append(\'UNK\')\n    for ex in examples:\n        words = ex[\'question_words\']\n        question = [w if counts.get(w,0) > minwcount else \'UNK\' for w in words]\n        ex[\'question_words_UNK\'] = question\n\n    return examples, vocab\n\ndef remove_long_tail_test(examples, word_to_wid):\n    for ex in examples:\n        ex[\'question_words_UNK\'] = [w if w in word_to_wid else \'UNK\' for w in ex[\'question_words\']]\n    return examples\n\ndef encode_question(examples, word_to_wid, maxlength=15, pad=\'left\'):\n    # Add to tuple question_wids and question_length\n    for i, ex in enumerate(examples):\n        ex[\'question_length\'] = min(maxlength, len(ex[\'question_words_UNK\'])) # record the length of this sequence\n        ex[\'question_wids\'] = [0]*maxlength\n        for k, w in enumerate(ex[\'question_words_UNK\']):\n            if k < maxlength:\n                if pad == \'right\':\n                    ex[\'question_wids\'][k] = word_to_wid[w]\n                else:   #[\'pad\'] == \'left\'\n                    new_k = k + maxlength - len(ex[\'question_words_UNK\'])\n                    ex[\'question_wids\'][new_k] = word_to_wid[w]\n                ex[\'seq_length\'] = len(ex[\'question_words_UNK\'])\n    return examples\n\ndef encode_answer(examples, ans_to_aid):\n    print(\'Warning: aid of answer not in vocab is 1999\')\n    for i, ex in enumerate(examples):\n        ex[\'answer_aid\'] = ans_to_aid.get(ex[\'answer\'], 1999) # -1 means answer not in vocab\n    return examples\n\ndef encode_answers_occurence(examples, ans_to_aid):\n    for i, ex in enumerate(examples):\n        answers = []\n        answers_aid = []\n        answers_count = []\n        for ans in ex[\'answers_occurence\']:\n            aid = ans_to_aid.get(ans[0], -1) # -1 means answer not in vocab\n            if aid != -1:\n                answers.append(ans[0])\n                answers_aid.append(aid) \n                answers_count.append(ans[1])\n        ex[\'answers\']       = answers\n        ex[\'answers_aid\']   = answers_aid\n        ex[\'answers_count\'] = answers_count\n    return examples\n\ndef vqa_processed(params):\n    \n    #####################################################\n    ## Read input files\n    #####################################################\n\n    path_train = os.path.join(params[\'dir\'], \'interim\', params[\'trainsplit\']+\'_questions_annotations.json\')\n    if params[\'trainsplit\'] == \'train\':\n        path_val = os.path.join(params[\'dir\'], \'interim\', \'val_questions_annotations.json\')\n    path_test    = os.path.join(params[\'dir\'], \'interim\', \'test_questions.json\')\n    path_testdev = os.path.join(params[\'dir\'], \'interim\', \'testdev_questions.json\')\n \n    # An example is a tuple (question, image, answer)\n    # /!\\ test and test-dev have no answer\n    trainset = json.load(open(path_train, \'r\'))\n    if params[\'trainsplit\'] == \'train\':\n        valset = json.load(open(path_val, \'r\'))\n    testset    = json.load(open(path_test, \'r\'))\n    testdevset = json.load(open(path_testdev, \'r\'))\n\n    #####################################################\n    ## Preprocess examples (questions and answers)\n    #####################################################\n\n    top_answers = get_top_answers(trainset, params[\'nans\'])\n    aid_to_ans = [a for i,a in enumerate(top_answers)]\n    ans_to_aid = {a:i for i,a in enumerate(top_answers)}\n    # Remove examples if answer is not in top answers\n    trainset = remove_examples(trainset, ans_to_aid)\n\n    # Add \'question_words\' to the initial tuple\n    trainset = preprocess_questions(trainset, params[\'nlp\'])\n    if params[\'trainsplit\'] == \'train\':\n        valset = preprocess_questions(valset, params[\'nlp\'])\n    testset    = preprocess_questions(testset, params[\'nlp\'])\n    testdevset = preprocess_questions(testdevset, params[\'nlp\'])\n\n    # Also process top_words which contains a UNK char\n    trainset, top_words = remove_long_tail_train(trainset, params[\'minwcount\'])\n    wid_to_word = {i+1:w for i,w in enumerate(top_words)}\n    word_to_wid = {w:i+1 for i,w in enumerate(top_words)}\n\n    if params[\'trainsplit\'] == \'train\':\n        valset = remove_long_tail_test(valset, word_to_wid)\n    testset    = remove_long_tail_test(testset, word_to_wid)\n    testdevset = remove_long_tail_test(testdevset, word_to_wid)\n\n    trainset = encode_question(trainset, word_to_wid, params[\'maxlength\'], params[\'pad\'])\n    if params[\'trainsplit\'] == \'train\':\n        valset = encode_question(valset, word_to_wid, params[\'maxlength\'], params[\'pad\'])\n    testset    = encode_question(testset, word_to_wid, params[\'maxlength\'], params[\'pad\'])\n    testdevset = encode_question(testdevset, word_to_wid, params[\'maxlength\'], params[\'pad\'])\n\n    trainset = encode_answer(trainset, ans_to_aid)\n    trainset = encode_answers_occurence(trainset, ans_to_aid)\n    if params[\'trainsplit\'] == \'train\':\n        valset = encode_answer(valset, ans_to_aid)\n        valset = encode_answers_occurence(valset, ans_to_aid)\n\n    #####################################################\n    ## Write output files\n    #####################################################\n\n    # Paths to output files\n    # Ex: data/vqa/processed/nans,3000_maxlength,15_..._trainsplit,train_testsplit,val/id_to_word.json\n    subdirname = \'nans,\'+str(params[\'nans\'])\n    for param in [\'maxlength\', \'minwcount\', \'nlp\', \'pad\', \'trainsplit\']:\n        subdirname += \'_\' + param + \',\' + str(params[param])\n    os.system(\'mkdir -p \' + os.path.join(params[\'dir\'], \'processed\', subdirname))\n\n    path_wid_to_word = os.path.join(params[\'dir\'], \'processed\', subdirname, \'wid_to_word.pickle\')\n    path_word_to_wid = os.path.join(params[\'dir\'], \'processed\', subdirname, \'word_to_wid.pickle\')\n    path_aid_to_ans  = os.path.join(params[\'dir\'], \'processed\', subdirname, \'aid_to_ans.pickle\')\n    path_ans_to_aid  = os.path.join(params[\'dir\'], \'processed\', subdirname, \'ans_to_aid.pickle\')\n    if params[\'trainsplit\'] == \'train\':\n        path_trainset = os.path.join(params[\'dir\'], \'processed\', subdirname, \'trainset.pickle\')\n        path_valset   = os.path.join(params[\'dir\'], \'processed\', subdirname, \'valset.pickle\')\n    elif params[\'trainsplit\'] == \'trainval\':\n        path_trainset = os.path.join(params[\'dir\'], \'processed\', subdirname, \'trainvalset.pickle\')\n    path_testset     = os.path.join(params[\'dir\'], \'processed\', subdirname, \'testset.pickle\')\n    path_testdevset  = os.path.join(params[\'dir\'], \'processed\', subdirname, \'testdevset.pickle\')\n\n    print(\'Write wid_to_word to\', path_wid_to_word)\n    with open(path_wid_to_word, \'wb\') as handle:\n        pickle.dump(wid_to_word, handle)\n\n    print(\'Write word_to_wid to\', path_word_to_wid)\n    with open(path_word_to_wid, \'wb\') as handle:\n        pickle.dump(word_to_wid, handle)\n\n    print(\'Write aid_to_ans to\', path_aid_to_ans)\n    with open(path_aid_to_ans, \'wb\') as handle:\n        pickle.dump(aid_to_ans, handle)\n\n    print(\'Write ans_to_aid to\', path_ans_to_aid)\n    with open(path_ans_to_aid, \'wb\') as handle:\n        pickle.dump(ans_to_aid, handle)\n\n    print(\'Write trainset to\', path_trainset)\n    with open(path_trainset, \'wb\') as handle:\n        pickle.dump(trainset, handle)\n\n    if params[\'trainsplit\'] == \'train\':\n        print(\'Write valset to\', path_valset)\n        with open(path_valset, \'wb\') as handle:\n            pickle.dump(valset, handle)\n\n    print(\'Write testset to\', path_testset)\n    with open(path_testset, \'wb\') as handle:\n        pickle.dump(testset, handle)\n\n    print(\'Write testdevset to\', path_testdevset)\n    with open(path_testdevset, \'wb\') as handle:\n        pickle.dump(testdevset, handle)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--dirname\',\n        default=\'data/vqa\',\n        type=str,\n        help=\'Root directory containing raw, interim and processed directories\'\n    )\n    parser.add_argument(\'--trainsplit\',\n        default=\'train\',\n        type=str,\n        help=\'Options: train | trainval\'\n    )\n    parser.add_argument(\'--nans\',\n        default=2000,\n        type=int,\n        help=\'Number of top answers for the final classifications\'\n    )\n    parser.add_argument(\'--maxlength\',\n        default=26,\n        type=int,\n        help=\'Max number of words in a caption. Captions longer get clipped\'\n    )\n    parser.add_argument(\'--minwcount\',\n        default=0,\n        type=int,\n        help=\'Words that occur less than that are removed from vocab\'\n    )\n    parser.add_argument(\'--nlp\',\n        default=\'mcb\',\n        type=str,\n        help=\'Token method ; Options: nltk | mcb | naive\'\n    )\n    parser.add_argument(\'--pad\',\n        default=\'left\',\n        type=str,\n        help=\'Padding ; Options: right (finish by zeros) | left (begin by zeros)\'\n    )\n    args = parser.parse_args()\n    opt_vqa = vars(args)\n    vqa_processed(opt_vqa)'"
vqa/lib/__init__.py,0,b''
vqa/lib/criterions.py,1,"b'import torch\nimport torch.nn as nn\n\ndef factory(opt, cuda=True):\n    criterion = nn.CrossEntropyLoss()\n    if cuda:\n        criterion = criterion.cuda()\n    return criterion'"
vqa/lib/dataloader.py,8,"b'import torch\nimport torch.multiprocessing as multiprocessing\nfrom .sampler import SequentialSampler, RandomSampler\nimport collections\nimport math\nimport sys\nimport traceback\nimport threading\nif sys.version_info[0] == 2:\n    import Queue as queue\nelse:\n    import queue\n\n\nclass ExceptionWrapper(object):\n    ""Wraps an exception plus traceback to communicate across threads""\n\n    def __init__(self, exc_info):\n        self.exc_type = exc_info[0]\n        self.exc_msg = """".join(traceback.format_exception(*exc_info))\n\n\ndef _worker_loop(dataset, index_queue, data_queue, collate_fn):\n    torch.set_num_threads(1)\n    while True:\n        r = index_queue.get()\n        if r is None:\n            data_queue.put(None)\n            break\n        idx, batch_indices = r\n        try:\n            samples = collate_fn([dataset[i] for i in batch_indices])\n        except Exception:\n            data_queue.put((idx, ExceptionWrapper(sys.exc_info())))\n        else:\n            data_queue.put((idx, samples))\n\n\ndef _pin_memory_loop(in_queue, out_queue, done_event):\n    while True:\n        try:\n            r = in_queue.get()\n        except:\n            if done_event.is_set():\n                return\n            raise\n        if r is None:\n            break\n        if isinstance(r[1], ExceptionWrapper):\n            out_queue.put(r)\n            continue\n        idx, batch = r\n        try:\n            batch = pin_memory_batch(batch)\n        except Exception:\n            out_queue.put((idx, ExceptionWrapper(sys.exc_info())))\n        else:\n            out_queue.put((idx, batch))\n\n\ndef default_collate(batch):\n    string_classes = (str, bytes)\n    ""Puts each data field into a tensor with outer dimension batch size""\n    if torch.is_tensor(batch[0]):\n        return torch.stack(batch, 0)\n    elif type(batch[0]).__module__ == \'numpy\' and type(batch[0]).__name__ == \'ndarray\':\n        return torch.stack([torch.from_numpy(b) for b in batch], 0)\n    elif isinstance(batch[0], int):\n        return torch.LongTensor(batch)\n    elif isinstance(batch[0], float):\n        return torch.DoubleTensor(batch)\n    elif isinstance(batch[0], string_classes):\n        return batch\n    elif isinstance(batch[0], dict):\n        # ~ added by Cadene ~\n        # if each batch element is a dict with same keys,\n        # then it should be a dict of collated elements\n        keys = batch[0].keys()\n        new_dict = {}\n        for key in keys:\n            new_dict[key] = []\n        for sample in batch:\n            for key in keys:\n                new_dict[key].append(sample[key])\n        return {key:default_collate(samples) for key, samples in new_dict.items()}\n    elif isinstance(batch[0], collections.Iterable):\n        # if each batch element is not a tensor, then it should be a tuple\n        # of tensors; in that case we collate each element in the tuple\n        transposed = zip(*batch)\n        return [default_collate(samples) for samples in transposed]\n    raise TypeError((""batch must contain tensors, numbers, or lists; found {}""\n                     .format(type(batch[0]))))\n\n\ndef pin_memory_batch(batch):\n    if torch.is_tensor(batch):\n        return batch.pin_memory()\n    elif isinstance(batch, dict):\n        # ~ added by Cadene ~\n        return {key:pin_memory_batch(sample) for key,sample in batch.items()}\n    elif isinstance(batch[0], str):\n        # ~ added by Cadene ~\n        return batch\n    elif isinstance(batch, collections.Iterable):\n        return [pin_memory_batch(sample) for sample in batch]\n    else:\n        return batch\n\n\nclass DataLoaderIter(object):\n    ""Iterates once over the DataLoader\'s dataset, as specified by the sampler""\n\n    def __init__(self, loader):\n        self.dataset = loader.dataset\n        self.batch_size = loader.batch_size\n        self.collate_fn = loader.collate_fn\n        self.sampler = loader.sampler\n        self.num_workers = loader.num_workers\n        self.pin_memory = loader.pin_memory\n        self.done_event = threading.Event()\n\n        self.samples_remaining = len(self.sampler)\n        self.sample_iter = iter(self.sampler)\n\n        if self.num_workers > 0:\n            self.index_queue = multiprocessing.SimpleQueue()\n            self.data_queue = multiprocessing.SimpleQueue()\n            self.batches_outstanding = 0\n            self.shutdown = False\n            self.send_idx = 0\n            self.rcvd_idx = 0\n            self.reorder_dict = {}\n\n            self.workers = [\n                multiprocessing.Process(\n                    target=_worker_loop,\n                    args=(self.dataset, self.index_queue, self.data_queue, self.collate_fn))\n                for _ in range(self.num_workers)]\n\n            for w in self.workers:\n                w.daemon = True  # ensure that the worker exits on process exit\n                w.start()\n\n            if self.pin_memory:\n                in_data = self.data_queue\n                self.data_queue = queue.Queue()\n                self.pin_thread = threading.Thread(\n                    target=_pin_memory_loop,\n                    args=(in_data, self.data_queue, self.done_event))\n                self.pin_thread.daemon = True\n                self.pin_thread.start()\n\n            # prime the prefetch loop\n            for _ in range(2 * self.num_workers):\n                self._put_indices()\n\n    def __len__(self):\n        return int(math.ceil(len(self.sampler) / float(self.batch_size)))\n\n    def __next__(self):\n        if self.num_workers == 0:\n            # same-process loading\n            if self.samples_remaining == 0:\n                raise StopIteration\n            indices = self._next_indices()\n            batch = self.collate_fn([self.dataset[i] for i in indices])\n            if self.pin_memory:\n                batch = pin_memory_batch(batch)\n            return batch\n\n        # check if the next sample has already been generated\n        if self.rcvd_idx in self.reorder_dict:\n            batch = self.reorder_dict.pop(self.rcvd_idx)\n            return self._process_next_batch(batch)\n\n        if self.batches_outstanding == 0:\n            self._shutdown_workers()\n            raise StopIteration\n\n        while True:\n            assert (not self.shutdown and self.batches_outstanding > 0)\n            idx, batch = self.data_queue.get()\n            self.batches_outstanding -= 1\n            if idx != self.rcvd_idx:\n                # store out-of-order samples\n                self.reorder_dict[idx] = batch\n                continue\n            return self._process_next_batch(batch)\n\n    next = __next__  # Python 2 compatibility\n\n    def __iter__(self):\n        return self\n\n    def _next_indices(self):\n        batch_size = min(self.samples_remaining, self.batch_size)\n        batch = [next(self.sample_iter) for _ in range(batch_size)]\n        self.samples_remaining -= len(batch)\n        return batch\n\n    def _put_indices(self):\n        assert self.batches_outstanding < 2 * self.num_workers\n        if self.samples_remaining > 0:\n            self.index_queue.put((self.send_idx, self._next_indices()))\n            self.batches_outstanding += 1\n            self.send_idx += 1\n\n    def _process_next_batch(self, batch):\n        self.rcvd_idx += 1\n        self._put_indices()\n        if isinstance(batch, ExceptionWrapper):\n            raise batch.exc_type(batch.exc_msg)\n        return batch\n\n    def __getstate__(self):\n        # TODO: add limited pickling support for sharing an iterator\n        # across multiple threads for HOGWILD.\n        # Probably the best way to do this is by moving the sample pushing\n        # to a separate thread and then just sharing the data queue\n        # but signalling the end is tricky without a non-blocking API\n        raise NotImplementedError(""DataLoaderIterator cannot be pickled"")\n\n    def _shutdown_workers(self):\n        if not self.shutdown:\n            self.shutdown = True\n            self.done_event.set()\n            for _ in self.workers:\n                self.index_queue.put(None)\n\n    def __del__(self):\n        if self.num_workers > 0:\n            self._shutdown_workers()\n\n\nclass DataLoader(object):\n    """"""\n    Data loader. Combines a dataset and a sampler, and provides\n    single- or multi-process iterators over the dataset.\n\n    Arguments:\n        dataset (Dataset): dataset from which to load the data.\n        batch_size (int, optional): how many samples per batch to load\n            (default: 1).\n        shuffle (bool, optional): set to ``True`` to have the data reshuffled\n            at every epoch (default: False).\n        sampler (Sampler, optional): defines the strategy to draw samples from\n            the dataset. If specified, the ``shuffle`` argument is ignored.\n        num_workers (int, optional): how many subprocesses to use for data\n            loading. 0 means that the data will be loaded in the main process\n            (default: 0)\n        collate_fn (callable, optional)\n        pin_memory (bool, optional)\n    """"""\n\n    def __init__(self, dataset, batch_size=1, shuffle=False, sampler=None,\n                 num_workers=0, collate_fn=default_collate, pin_memory=False):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.collate_fn = collate_fn\n        self.pin_memory = pin_memory\n\n        if sampler is not None:\n            self.sampler = sampler\n        elif shuffle:\n            self.sampler = RandomSampler(dataset)\n        elif not shuffle:\n            self.sampler = SequentialSampler(dataset)\n\n    def __iter__(self):\n        return DataLoaderIter(self)\n\n    def __len__(self):\n        return int(math.ceil(len(self.sampler) / float(self.batch_size)))\n'"
vqa/lib/engine.py,4,"b'import time\nimport torch\nfrom torch.autograd import Variable\nimport vqa.lib.utils as utils\n\ndef train(loader, model, criterion, optimizer, logger, epoch, print_freq=10):\n    # switch to train mode\n    model.train()\n    meters = logger.reset_meters(\'train\')\n\n    end = time.time()\n    for i, sample in enumerate(loader):\n        batch_size = sample[\'visual\'].size(0)\n\n        # measure data loading time\n        meters[\'data_time\'].update(time.time() - end, n=batch_size)\n\n        input_visual   = Variable(sample[\'visual\'])\n        input_question = Variable(sample[\'question\'])\n        target_answer  = Variable(sample[\'answer\'].cuda(async=True))\n\n        # compute output\n        output = model(input_visual, input_question)\n        torch.cuda.synchronize()\n        loss = criterion(output, target_answer)\n        meters[\'loss\'].update(loss.data[0], n=batch_size)\n\n        # measure accuracy \n        acc1, acc5 = utils.accuracy(output.data, target_answer.data, topk=(1, 5))\n        meters[\'acc1\'].update(acc1[0], n=batch_size)\n        meters[\'acc5\'].update(acc5[0], n=batch_size)\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        torch.cuda.synchronize()\n        optimizer.step()\n        torch.cuda.synchronize()\n\n        # measure elapsed time\n        meters[\'batch_time\'].update(time.time() - end, n=batch_size)\n        end = time.time()\n\n        if i % print_freq == 0:\n            print(\'Epoch: [{0}][{1}/{2}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Acc@1 {acc1.val:.3f} ({acc1.avg:.3f})\\t\'\n                  \'Acc@5 {acc5.val:.3f} ({acc5.avg:.3f})\'.format(\n                   epoch, i, len(loader),\n                   batch_time=meters[\'batch_time\'], data_time=meters[\'data_time\'],\n                   loss=meters[\'loss\'], acc1=meters[\'acc1\'], acc5=meters[\'acc5\']))\n\n    logger.log_meters(\'train\', n=epoch)\n\n# def adjust_learning_rate(optimizer, epoch):\n#     """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n#     lr = args.lr * (0.1 ** (epoch // 30))\n#     for param_group in optimizer.param_groups:\n#         param_group[\'lr\'] = lr\n\n\ndef validate(loader, model, criterion, logger, epoch=0, print_freq=10):\n    results = []\n\n    # switch to evaluate mode\n    model.eval()\n    meters = logger.reset_meters(\'val\')\n\n    end = time.time()\n    for i, sample in enumerate(loader):\n        batch_size = sample[\'visual\'].size(0)\n        input_visual   = Variable(sample[\'visual\'].cuda(async=True), volatile=True)\n        input_question = Variable(sample[\'question\'].cuda(async=True), volatile=True)\n        target_answer  = Variable(sample[\'answer\'].cuda(async=True), volatile=True)\n\n        # compute output\n        output = model(input_visual, input_question)\n        loss = criterion(output, target_answer)\n        meters[\'loss\'].update(loss.data[0], n=batch_size)\n\n        # measure accuracy and record loss\n        acc1, acc5 = utils.accuracy(output.data, target_answer.data, topk=(1, 5))\n        meters[\'acc1\'].update(acc1[0], n=batch_size)\n        meters[\'acc5\'].update(acc5[0], n=batch_size)\n\n        # compute predictions for OpenEnded accuracy\n        _, pred = output.data.cpu().max(1)\n        pred.squeeze_()\n        for j in range(batch_size):\n            results.append({\'question_id\': sample[\'question_id\'][j],\n                            \'answer\': loader.dataset.aid_to_ans[pred[j]]})\n\n        # measure elapsed time\n        meters[\'batch_time\'].update(time.time() - end, n=batch_size)\n        end = time.time()\n\n        if i % print_freq == 0:\n            print(\'Val: [{0}/{1}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Acc@1 {acc1.val:.3f} ({acc1.avg:.3f})\\t\'\n                  \'Acc@5 {acc5.val:.3f} ({acc5.avg:.3f})\'.format(\n                   i, len(loader), batch_time=meters[\'batch_time\'],\n                   data_time=meters[\'data_time\'], loss=meters[\'loss\'],\n                   acc1=meters[\'acc1\'], acc5=meters[\'acc5\']))\n\n    print(\' * Acc@1 {acc1.avg:.3f} Acc@5 {acc5.avg:.3f}\'\n          .format(acc1=meters[\'acc1\'], acc5=meters[\'acc1\']))\n\n    logger.log_meters(\'val\', n=epoch)\n    return meters[\'acc1\'].avg, results\n\n\ndef test(loader, model, logger, epoch=0, print_freq=10):\n    results = []\n    testdev_results = []\n\n    model.eval()\n    meters = logger.reset_meters(\'test\')\n\n    end = time.time()\n    for i, sample in enumerate(loader):\n        batch_size = sample[\'visual\'].size(0)\n        input_visual   = Variable(sample[\'visual\'].cuda(async=True), volatile=True)\n        input_question = Variable(sample[\'question\'].cuda(async=True), volatile=True)\n\n        # compute output\n        output = model(input_visual, input_question)\n\n        # compute predictions for OpenEnded accuracy\n        _, pred = output.data.cpu().max(1)\n        pred.squeeze_()\n        for j in range(batch_size):\n            item = {\'question_id\': sample[\'question_id\'][j],\n                    \'answer\': loader.dataset.aid_to_ans[pred[j]]}\n            results.append(item)\n            if sample[\'is_testdev\'][j]:\n                testdev_results.append(item)\n\n        # measure elapsed time\n        meters[\'batch_time\'].update(time.time() - end, n=batch_size)\n        end = time.time()\n\n        if i % print_freq == 0:\n            print(\'Test: [{0}/{1}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\'.format(\n                   i, len(loader), batch_time=meters[\'batch_time\']))\n\n    logger.log_meters(\'test\', n=epoch)\n    return results, testdev_results\n'"
vqa/lib/logger.py,0,"b'import copy\nimport time\nimport json\nimport numpy as np\nimport os\nfrom collections import defaultdict\n\nclass Experiment(object):\n\n    def __init__(self, name, options=dict()):\n        """""" Create an experiment\n        """"""\n        super(Experiment, self).__init__()\n\n        self.name = name\n        self.options = options\n        self.date_and_time = time.strftime(\'%d-%m-%Y--%H-%M-%S\')\n\n        self.info = defaultdict(dict)\n        self.logged = defaultdict(dict)\n        self.meters = defaultdict(dict)\n\n    def add_meters(self, tag, meters_dict):\n        assert tag not in (self.meters.keys())\n        for name, meter in meters_dict.items():\n            self.add_meter(tag, name, meter)\n\n    def add_meter(self, tag, name, meter):\n        assert name not in list(self.meters[tag].keys()), \\\n            ""meter with tag {} and name {} already exists"".format(tag, name)\n        self.meters[tag][name] = meter\n\n    def update_options(self, options_dict):\n        self.options.update(options_dict)\n\n    def log_meter(self, tag, name, n=1):\n        meter = self.get_meter(tag, name)\n        if name not in self.logged[tag]:\n            self.logged[tag][name] = {}\n        self.logged[tag][name][n] = meter.value()\n\n    def log_meters(self, tag, n=1):\n        for name, meter in self.get_meters(tag).items():\n            self.log_meter(tag, name, n=n)\n\n    def reset_meters(self, tag):\n        meters = self.get_meters(tag)\n        for name, meter in meters.items():\n            meter.reset()\n        return meters\n\n    def get_meters(self, tag):\n        assert tag in list(self.meters.keys())\n        return self.meters[tag]\n\n    def get_meter(self, tag, name):\n        assert tag in list(self.meters.keys())\n        assert name in list(self.meters[tag].keys())\n        return self.meters[tag][name]\n\n    def to_json(self, filename):\n        os.system(\'mkdir -p \' + os.path.dirname(filename))\n        var_dict = copy.copy(vars(self))\n        var_dict.pop(\'meters\')\n        for key in (\'viz\', \'viz_dict\'):\n            if key in list(var_dict.keys()):\n                var_dict.pop(key)\n        with open(filename, \'w\') as f:\n            json.dump(var_dict, f)\n\n    def from_json(filename):\n        with open(filename, \'r\') as f:\n            var_dict = json.load(f)\n        xp = Experiment(\'\')\n        xp.date_and_time = var_dict[\'date_and_time\']\n        xp.logged        = var_dict[\'logged\']\n        # TODO: Remove\n        if \'info\' in var_dict:\n            xp.info          = var_dict[\'info\']\n        xp.options       = var_dict[\'options\']\n        xp.name          = var_dict[\'name\']\n        return xp\n\n\nclass AvgMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def value(self):\n        return self.avg\n\n\nclass SumMeter(object):\n    """"""Computes and stores the sum and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n\n    def value(self):\n        return self.sum\n\n\nclass ValueMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n\n    def update(self, val):\n        self.val = val\n\n    def value(self):\n        return self.val'"
vqa/lib/sampler.py,1,"b'import torch\n\n\nclass Sampler(object):\n    """"""Base class for all Samplers.\n    Every Sampler subclass has to provide an __iter__ method, providing a way\n    to iterate over indices of dataset elements, and a __len__ method that\n    returns the length of the returned iterators.\n    """"""\n\n    def __init__(self, data_source):\n        pass\n\n    def __iter__(self):\n        raise NotImplementedError\n\n    def __len__(self):\n        raise NotImplementedError\n\n\nclass SequentialSampler(Sampler):\n    """"""Samples elements sequentially, always in the same order.\n    Arguments:\n        data_source (Dataset): dataset to sample from\n    """"""\n\n    def __init__(self, data_source):\n        self.num_samples = len(data_source)\n\n    def __iter__(self):\n        return iter(range(self.num_samples))\n\n    def __len__(self):\n        return self.num_samples\n\n\nclass RandomSampler(Sampler):\n    """"""Samples elements randomly, without replacement.\n    Arguments:\n        data_source (Dataset): dataset to sample from\n    """"""\n\n    def __init__(self, data_source):\n        self.num_samples = len(data_source)\n\n    def __iter__(self):\n        return iter(torch.randperm(self.num_samples).long())\n\n    def __len__(self):\n        return self.num_samples'"
vqa/lib/utils.py,2,"b'import itertools\nimport collections\nimport torch\nimport numpy as np\n\ndef update_values(dict_from, dict_to):\n    for key, value in dict_from.items():\n        if isinstance(value, dict):\n            update_values(dict_from[key], dict_to[key])\n        elif value is not None:\n            dict_to[key] = dict_from[key] \n    return dict_to\n\ndef merge_dict(a, b):\n    if isinstance(a, dict) and isinstance(b, dict):\n        d = dict(a)\n        d.update({k: merge_dict(a.get(k, None), b[k]) for k in b})\n    if isinstance(a, list) and isinstance(b, list):\n        return b\n        #return [merge_dict(x, y) for x, y in itertools.zip_longest(a, b)]\n    return a if b is None else b\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    if target.dim() == 2: # multians option\n        _, target = torch.max(target, 1)\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\ndef params_count(model):\n    count = 0\n    for p in model.parameters():\n        c = 1\n        for i in range(p.dim()):\n            c *= p.size(i)\n        count += c\n    return count\n\ndef str2bool(v):\n    if v is None:\n        return v\n    elif type(v) == bool:\n        return v\n    elif type(v) == str:\n        if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n            return True\n        if v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n            return False\n    raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\ndef create_n_hot(idxs, N):\n    out = np.zeros(N)\n    for i in idxs:\n        out[i] += 1\n    return torch.Tensor(out/out.sum())\n\n'"
vqa/models/__init__.py,0,"b'from .noatt import MLBNoAtt, MutanNoAtt\nfrom .att import MLBAtt, MutanAtt\nfrom .utils import factory\nfrom .utils import model_names'"
vqa/models/att.py,7,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport copy\n\nfrom vqa.lib import utils\nfrom vqa.models import seq2vec\nfrom vqa.models import fusion\n\nclass AbstractAtt(nn.Module):\n\n    def __init__(self, opt={}, vocab_words=[], vocab_answers=[]):\n        super(AbstractAtt, self).__init__()\n        self.opt = opt\n        self.vocab_words = vocab_words\n        self.vocab_answers = vocab_answers\n        self.num_classes = len(self.vocab_answers)\n        # Modules\n        self.seq2vec = seq2vec.factory(self.vocab_words, self.opt['seq2vec'])\n        # Modules for attention\n        self.conv_v_att = nn.Conv2d(self.opt['dim_v'],\n                                    self.opt['attention']['dim_v'], 1, 1)\n        self.linear_q_att = nn.Linear(self.opt['dim_q'],\n                                      self.opt['attention']['dim_q'])\n        self.conv_att = nn.Conv2d(self.opt['attention']['dim_mm'],\n                                  self.opt['attention']['nb_glimpses'], 1, 1)\n        #\xc2\xa0Modules for classification\n        self.list_linear_v_fusion = None\n        self.linear_q_fusion = None\n        self.linear_classif = None\n\n    def _fusion_att(self, x_v, x_q):\n        raise NotImplementedError\n\n    def _fusion_classif(self, x_v, x_q):\n        raise NotImplementedError\n\n    def _attention(self, input_v, x_q_vec):\n        batch_size = input_v.size(0)\n        width = input_v.size(2)\n        height = input_v.size(3)\n\n        # Process visual before fusion\n        #x_v = input_v.view(batch_size*width*height, dim_features)\n        x_v = input_v\n        x_v = F.dropout(x_v,\n                        p=self.opt['attention']['dropout_v'],\n                        training=self.training)\n        x_v = self.conv_v_att(x_v)\n        if 'activation_v' in self.opt['attention']:\n            x_v = getattr(F, self.opt['attention']['activation_v'])(x_v)\n        x_v = x_v.view(batch_size,\n                       self.opt['attention']['dim_v'],\n                       width * height)\n        x_v = x_v.transpose(1,2)\n\n        # Process question before fusion\n        x_q = F.dropout(x_q_vec, p=self.opt['attention']['dropout_q'],\n                           training=self.training)\n        x_q = self.linear_q_att(x_q)\n        if 'activation_q' in self.opt['attention']:\n            x_q = getattr(F, self.opt['attention']['activation_q'])(x_q)\n        x_q = x_q.view(batch_size,\n                       1,\n                       self.opt['attention']['dim_q'])\n        x_q = x_q.expand(batch_size,\n                         width * height,\n                         self.opt['attention']['dim_q'])\n\n        # First multimodal fusion\n        x_att = self._fusion_att(x_v, x_q)\n\n        if 'activation_mm' in self.opt['attention']:\n            x_att = getattr(F, self.opt['attention']['activation_mm'])(x_att)\n\n        # Process attention vectors\n        x_att = F.dropout(x_att,\n                          p=self.opt['attention']['dropout_mm'],\n                          training=self.training)\n        # can be optim to avoid two views and transposes\n        x_att = x_att.view(batch_size,\n                           width,\n                           height,\n                           self.opt['attention']['dim_mm']) \n        x_att = x_att.transpose(2,3).transpose(1,2)\n        x_att = self.conv_att(x_att)\n        x_att = x_att.view(batch_size,\n                           self.opt['attention']['nb_glimpses'],\n                           width * height)\n        list_att_split = torch.split(x_att, 1, dim=1)\n        list_att = []\n        for x_att in list_att_split:\n            x_att = x_att.contiguous()\n            x_att = x_att.view(batch_size, width*height)\n            x_att = F.softmax(x_att)\n            list_att.append(x_att)\n\n        self.list_att = [x_att.data for x_att in list_att]\n\n        # Apply attention vectors to input_v\n        x_v = input_v.view(batch_size, self.opt['dim_v'], width * height)\n        x_v = x_v.transpose(1,2)\n\n        list_v_att = []\n        for i, x_att in enumerate(list_att):\n            x_att = x_att.view(batch_size,\n                               width * height,\n                               1)\n            x_att = x_att.expand(batch_size,\n                                 width * height,\n                                 self.opt['dim_v'])\n            x_v_att = torch.mul(x_att, x_v)\n            x_v_att = x_v_att.sum(1)\n            x_v_att = x_v_att.view(batch_size, self.opt['dim_v'])\n            list_v_att.append(x_v_att)\n\n        return list_v_att\n\n    def _fusion_glimpses(self, list_v_att, x_q_vec):\n        # Process visual for each glimpses\n        list_v = []\n        for glimpse_id, x_v_att in enumerate(list_v_att):\n            x_v = F.dropout(x_v_att,\n                            p=self.opt['fusion']['dropout_v'],\n                            training=self.training)\n            x_v = self.list_linear_v_fusion[glimpse_id](x_v)\n            if 'activation_v' in self.opt['fusion']:\n                x_v = getattr(F, self.opt['fusion']['activation_v'])(x_v)\n            list_v.append(x_v)\n        x_v = torch.cat(list_v, 1)\n\n        # Process question\n        x_q = F.dropout(x_q_vec,\n                        p=self.opt['fusion']['dropout_q'],\n                        training=self.training)\n        x_q = self.linear_q_fusion(x_q)\n        if 'activation_q' in self.opt['fusion']:\n            x_q = getattr(F, self.opt['fusion']['activation_q'])(x_q)\n\n        # Second multimodal fusion\n        x = self._fusion_classif(x_v, x_q)\n        return x\n\n    def _classif(self, x):\n\n        if 'activation' in self.opt['classif']:\n            x = getattr(F, self.opt['classif']['activation'])(x)\n        x = F.dropout(x,\n                      p=self.opt['classif']['dropout'],\n                      training=self.training)\n        x = self.linear_classif(x)\n        return x\n\n    def forward(self, input_v, input_q):\n        if input_v.dim() != 4 and input_q.dim() != 2:\n            raise ValueError\n\n        x_q_vec = self.seq2vec(input_q)\n        list_v_att = self._attention(input_v, x_q_vec)\n        x = self._fusion_glimpses(list_v_att, x_q_vec)\n        x = self._classif(x)\n        return x\n\n\nclass MLBAtt(AbstractAtt):\n\n    def __init__(self, opt={}, vocab_words=[], vocab_answers=[]):\n        # TODO: deep copy ?\n        opt['attention']['dim_v']  = opt['attention']['dim_h']\n        opt['attention']['dim_q']  = opt['attention']['dim_h']\n        opt['attention']['dim_mm'] = opt['attention']['dim_h']\n        super(MLBAtt, self).__init__(opt, vocab_words, vocab_answers)\n        # Modules for classification\n        self.list_linear_v_fusion = nn.ModuleList([\n            nn.Linear(self.opt['dim_v'],\n                      self.opt['fusion']['dim_h'])\n            for i in range(self.opt['attention']['nb_glimpses'])])\n        self.linear_q_fusion = nn.Linear(self.opt['dim_q'],\n                                         self.opt['fusion']['dim_h']\n                                         * self.opt['attention']['nb_glimpses'])\n        self.linear_classif = nn.Linear(self.opt['fusion']['dim_h']\n                                        * self.opt['attention']['nb_glimpses'],\n                                        self.num_classes)\n\n    def _fusion_att(self, x_v, x_q):\n        x_att = torch.mul(x_v, x_q)\n        return x_att\n\n    def _fusion_classif(self, x_v, x_q):\n        x_mm = torch.mul(x_v, x_q)\n        return x_mm\n\n\nclass MutanAtt(AbstractAtt):\n\n    def __init__(self, opt={}, vocab_words=[], vocab_answers=[]):\n        # TODO: deep copy ?\n        opt['attention']['dim_v'] = opt['attention']['dim_hv']\n        opt['attention']['dim_q'] = opt['attention']['dim_hq']\n        super(MutanAtt, self).__init__(opt, vocab_words, vocab_answers)\n        # Modules for classification\n        self.fusion_att = fusion.MutanFusion2d(self.opt['attention'],\n                                               visual_embedding=False,\n                                               question_embedding=False)\n        self.list_linear_v_fusion = nn.ModuleList([\n            nn.Linear(self.opt['dim_v'],\n                      int(self.opt['fusion']['dim_hv']\n                          / opt['attention']['nb_glimpses']))\n            for i in range(self.opt['attention']['nb_glimpses'])])\n        self.linear_q_fusion = nn.Linear(self.opt['dim_q'],\n                                         self.opt['fusion']['dim_hq'])\n        self.linear_classif = nn.Linear(self.opt['fusion']['dim_mm'],\n                                        self.num_classes)\n        self.fusion_classif = fusion.MutanFusion(self.opt['fusion'],\n                                                 visual_embedding=False,\n                                                 question_embedding=False)\n\n    def _fusion_att(self, x_v, x_q):\n        return self.fusion_att(x_v, x_q)\n\n    def _fusion_classif(self, x_v, x_q):\n        return self.fusion_classif(x_v, x_q)\n"""
vqa/models/convnets.py,1,"b'import copy\nimport torch\nimport torch.nn as nn\nimport torchvision.models as pytorch_models\nimport sys\nsys.path.append(\'vqa/external/pretrained-models.pytorch\')\nimport pretrainedmodels as torch7_models\n\npytorch_resnet_names = sorted(name for name in pytorch_models.__dict__\n    if name.islower()\n    and name.startswith(""resnet"")\n    and callable(pytorch_models.__dict__[name]))\n\ntorch7_resnet_names = sorted(name for name in torch7_models.__dict__\n    if name.islower()\n    and callable(torch7_models.__dict__[name]))\n\nmodel_names = pytorch_resnet_names + torch7_resnet_names\n\ndef factory(opt, cuda=True, data_parallel=True):\n    opt = copy.copy(opt)\n\n    class WrapperModule(nn.Module):\n        def __init__(self, net, forward_fn):\n            super(WrapperModule, self).__init__()\n            self.net = net\n            self.forward_fn = forward_fn\n\n        def forward(self, x):\n            return self.forward_fn(self.net, x)\n\n        def __getattr__(self, attr):\n            try:\n                return super(WrapperModule, self).__getattr__(attr)\n            except AttributeError:\n                return getattr(self.net, attr)\n\n    def forward_resnet(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        if \'pooling\' in opt and opt[\'pooling\']:\n            x = self.avgpool(x)\n            div = x.size(3) + x.size(2)\n            x = x.sum(3)\n            x = x.sum(2)\n            x = x.view(x.size(0), -1)\n            x = x.div(div)\n\n        return x\n\n    def forward_resnext(self, x):\n        x = self.features(x)\n\n        if \'pooling\' in opt and opt[\'pooling\']:\n            x = self.avgpool(x)\n            div = x.size(3) + x.size(2)\n            x = x.sum(3)\n            x = x.sum(2)\n            x = x.view(x.size(0), -1)\n            x = x.div(div)\n\n        return x\n\n    if opt[\'arch\'] in pytorch_resnet_names:\n        model = pytorch_models.__dict__[opt[\'arch\']](pretrained=True)\n\n        model = WrapperModule(model, forward_resnet) #\xc2\xa0ugly hack in case of DataParallel wrapping\n\n    elif opt[\'arch\'] == \'fbresnet152\':\n        model = torch7_models.__dict__[opt[\'arch\']](num_classes=1000,\n                                                    pretrained=\'imagenet\')\n\n        model = WrapperModule(model, forward_resnet) #\xc2\xa0ugly hack in case of DataParallel wrapping\n\n    elif opt[\'arch\'] in torch7_resnet_names:\n        model = torch7_models.__dict__[opt[\'arch\']](num_classes=1000,\n                                                    pretrained=\'imagenet\')\n\n        model = WrapperModule(model, forward_resnext) #\xc2\xa0ugly hack in case of DataParallel wrapping\n\n    else:\n        raise ValueError\n\n    if data_parallel:\n        model = nn.DataParallel(model).cuda()\n        if not cuda:\n            raise ValueError\n\n    if cuda:\n        model.cuda()\n\n    return model\n'"
vqa/models/fusion.py,6,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass AbstractFusion(nn.Module):\n\n    def __init__(self, opt={}):\n        super(AbstractFusion, self).__init__()\n        self.opt = opt\n\n    def forward(self, input_v, input_q):\n        raise NotImplementedError\n\n\nclass MLBFusion(AbstractFusion):\n\n    def __init__(self, opt):\n        super(MLBFusion, self).__init__(opt)\n        # Modules\n        if 'dim_v' in self.opt:\n            self.linear_v = nn.Linear(self.opt['dim_v'], self.opt['dim_h'])\n        else:\n            print('Warning fusion.py: no visual embedding before fusion')\n\n        if 'dim_q' in self.opt:\n            self.linear_q = nn.Linear(self.opt['dim_q'], self.opt['dim_h'])\n        else:\n            print('Warning fusion.py: no question embedding before fusion')\n        \n    def forward(self, input_v, input_q):\n        # visual (cnn features)\n        if 'dim_v' in self.opt:\n            x_v = F.dropout(input_v, p=self.opt['dropout_v'], training=self.training)\n            x_v = self.linear_v(x_v)\n            if 'activation_v' in self.opt:\n                x_v = getattr(F, self.opt['activation_v'])(x_v)\n        else:\n            x_v = input_v\n        # question (rnn features)\n        if 'dim_q' in self.opt:\n            x_q = F.dropout(input_q, p=self.opt['dropout_q'], training=self.training)\n            x_q = self.linear_q(x_q)\n            if 'activation_q' in self.opt:\n                x_q = getattr(F, self.opt['activation_q'])(x_q)\n        else:\n            x_q = input_q\n        #\xc2\xa0hadamard product\n        x_mm = torch.mul(x_q, x_v)\n        return x_mm\n\n\nclass MutanFusion(AbstractFusion):\n\n    def __init__(self, opt, visual_embedding=True, question_embedding=True):\n        super(MutanFusion, self).__init__(opt)\n        self.visual_embedding = visual_embedding\n        self.question_embedding = question_embedding\n        # Modules\n        if self.visual_embedding:\n            self.linear_v = nn.Linear(self.opt['dim_v'], self.opt['dim_hv'])\n        else:\n            print('Warning fusion.py: no visual embedding before fusion')\n\n        if self.question_embedding:\n            self.linear_q = nn.Linear(self.opt['dim_q'], self.opt['dim_hq'])\n        else:\n            print('Warning fusion.py: no question embedding before fusion')\n        \n        self.list_linear_hv = nn.ModuleList([\n            nn.Linear(self.opt['dim_hv'], self.opt['dim_mm'])\n            for i in range(self.opt['R'])])\n\n        self.list_linear_hq = nn.ModuleList([\n            nn.Linear(self.opt['dim_hq'], self.opt['dim_mm'])\n            for i in range(self.opt['R'])])\n\n    def forward(self, input_v, input_q):\n        if input_v.dim() != input_q.dim() and input_v.dim() != 2:\n            raise ValueError\n        batch_size = input_v.size(0)\n\n        if self.visual_embedding:\n            x_v = F.dropout(input_v, p=self.opt['dropout_v'], training=self.training)\n            x_v = self.linear_v(x_v)\n            if 'activation_v' in self.opt:\n                    x_v = getattr(F, self.opt['activation_v'])(x_v)\n        else:\n            x_v = input_v\n\n        if self.question_embedding:\n            x_q = F.dropout(input_q, p=self.opt['dropout_q'], training=self.training)\n            x_q = self.linear_q(x_q)\n            if 'activation_q' in self.opt:\n                    x_q = getattr(F, self.opt['activation_q'])(x_q)\n        else:\n            x_q = input_q\n\n        x_mm = []\n        for i in range(self.opt['R']):\n\n            x_hv = F.dropout(x_v, p=self.opt['dropout_hv'], training=self.training)\n            x_hv = self.list_linear_hv[i](x_hv)\n            if 'activation_hv' in self.opt:\n                x_hv = getattr(F, self.opt['activation_hv'])(x_hv)\n\n            x_hq = F.dropout(x_q, p=self.opt['dropout_hq'], training=self.training)\n            x_hq = self.list_linear_hq[i](x_hq)\n            if 'activation_hq' in self.opt:\n                x_hq = getattr(F, self.opt['activation_hq'])(x_hq)\n\n            x_mm.append(torch.mul(x_hq, x_hv))\n\n        x_mm = torch.stack(x_mm, dim=1)\n        x_mm = x_mm.sum(1).view(batch_size, self.opt['dim_mm'])\n\n        if 'activation_mm' in self.opt:\n            x_mm = getattr(F, self.opt['activation_mm'])(x_mm)\n\n        return x_mm\n\n\nclass MutanFusion2d(MutanFusion):\n\n    def __init__(self, opt, visual_embedding=True, question_embedding=True):\n        super(MutanFusion2d, self).__init__(opt,\n                                            visual_embedding,\n                                            question_embedding)\n\n    def forward(self, input_v, input_q):\n        if input_v.dim() != input_q.dim() and input_v.dim() != 3:\n            raise ValueError\n        batch_size = input_v.size(0)\n        weight_height = input_v.size(1)\n        dim_hv = input_v.size(2)\n        dim_hq = input_q.size(2)\n        if not input_v.is_contiguous():\n            input_v = input_v.contiguous()\n        if not input_q.is_contiguous():\n            input_q = input_q.contiguous()\n        x_v = input_v.view(batch_size * weight_height, self.opt['dim_hv'])\n        x_q = input_q.view(batch_size * weight_height, self.opt['dim_hq'])\n        x_mm = super().forward(x_v, x_q)\n        x_mm = x_mm.view(batch_size, weight_height, self.opt['dim_mm'])\n        return x_mm\n\n"""
vqa/models/noatt.py,2,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom vqa.lib import utils\nfrom vqa.models import fusion\nfrom vqa.models import seq2vec\n\nclass AbstractNoAtt(nn.Module):\n\n    def __init__(self, opt={}, vocab_words=[], vocab_answers=[]):\n        super(AbstractNoAtt, self).__init__()\n        self.opt = opt\n        self.vocab_words = vocab_words\n        self.vocab_answers = vocab_answers\n        self.num_classes = len(self.vocab_answers)\n        # Modules\n        self.seq2vec = seq2vec.factory(self.vocab_words, self.opt['seq2vec'])\n        self.linear_classif = nn.Linear(self.opt['fusion']['dim_h'], self.num_classes)\n\n    def _fusion(self, input_v, input_q):\n        raise NotImplementedError\n\n    def _classif(self, x):\n        if 'activation' in self.opt['classif']:\n            x = getattr(F, self.opt['classif']['activation'])(x)\n        x = F.dropout(x, p=self.opt['classif']['dropout'], training=self.training)\n        x = self.linear_classif(x)\n        return x\n\n    def forward(self, input_v, input_q):\n        x_q = self.seq2vec(input_q)\n        x = self._fusion(input_v, x_q)\n        x = self._classif(x)\n        return x\n\n\nclass MLBNoAtt(AbstractNoAtt):\n\n    def __init__(self, opt={}, vocab_words=[], vocab_answers=[]):\n        super(MLBNoAtt, self).__init__(opt, vocab_words, vocab_answers)\n        self.fusion = fusion.MLBFusion(self.opt['fusion'])\n\n    def _fusion(self, input_v, input_q):\n        x = self.fusion(input_v, input_q)\n        return x\n\n\nclass MutanNoAtt(AbstractNoAtt):\n\n    def __init__(self, opt={}, vocab_words=[], vocab_answers=[]):\n        opt['fusion']['dim_h'] = opt['fusion']['dim_mm']\n        super(MutanNoAtt, self).__init__(opt, vocab_words, vocab_answers)\n        self.fusion = fusion.MutanFusion(self.opt['fusion'])\n\n    def _fusion(self, input_v, input_q):\n        x = self.fusion(input_v, input_q)\n        return x\n\n"""
vqa/models/seq2vec.py,5,"b""import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nimport sys\nsys.path.append('vqa/external/skip-thoughts.torch/pytorch')\nimport skipthoughts\n\n\ndef process_lengths(input):\n    max_length = input.size(1)\n    lengths = list(max_length - input.data.eq(0).sum(1).squeeze())\n    return lengths\n\ndef select_last(x, lengths):\n    batch_size = x.size(0)\n    seq_length = x.size(1)\n    mask = x.data.new().resize_as_(x.data).fill_(0)\n    for i in range(batch_size):\n        mask[i][lengths[i]-1].fill_(1)\n    mask = Variable(mask)\n    x = x.mul(mask)\n    x = x.sum(1).view(batch_size, x.size(2))\n    return x\n\nclass LSTM(nn.Module):\n\n    def __init__(self, vocab, emb_size, hidden_size, num_layers):\n        super(LSTM, self).__init__()\n        self.vocab = vocab\n        self.emb_size = emb_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.embedding = nn.Embedding(num_embeddings=len(self.vocab)+1,\n                                      embedding_dim=emb_size,\n                                      padding_idx=0)\n        self.rnn = nn.LSTM(input_size=emb_size, hidden_size=hidden_size, num_layers=num_layers)\n\n    def forward(self, input):\n        lengths = process_lengths(input)\n        x = self.embedding(input) # seq2seq\n        output, hn = self.rnn(x)\n        output = select_last(output, lengths)\n        return output\n\n\nclass TwoLSTM(nn.Module):\n\n    def __init__(self, vocab, emb_size, hidden_size):\n        super(TwoLSTM, self).__init__()\n        self.vocab = vocab\n        self.emb_size = emb_size\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(num_embeddings=len(self.vocab)+1,\n                                      embedding_dim=emb_size,\n                                      padding_idx=0)\n        self.rnn_0 = nn.LSTM(input_size=emb_size, hidden_size=hidden_size, num_layers=1)\n        self.rnn_1 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=1)\n\n    def forward(self, input):\n        lengths = process_lengths(input)\n        x = self.embedding(input) # seq2seq\n        x = getattr(F, 'tanh')(x)\n        x_0, hn = self.rnn_0(x)\n        vec_0 = select_last(x_0, lengths)\n\n        # x_1 = F.dropout(x_0, p=0.3, training=self.training)\n        # print(x_1.size())\n        x_1, hn = self.rnn_1(x_0)\n        vec_1 = select_last(x_1, lengths)\n        \n        vec_0 = F.dropout(vec_0, p=0.3, training=self.training)\n        vec_1 = F.dropout(vec_1, p=0.3, training=self.training)\n        output = torch.cat((vec_0, vec_1), 1)\n        return output\n        \n\ndef factory(vocab_words, opt):\n    if opt['arch'] == 'skipthoughts':\n        st_class = getattr(skipthoughts, opt['type'])\n        seq2vec = st_class(opt['dir_st'],\n                           vocab_words,\n                           dropout=opt['dropout'],\n                           fixed_emb=opt['fixed_emb'])\n    elif opt['arch'] == '2-lstm':\n        seq2vec = TwoLSTM(vocab_words,\n                          opt['emb_size'],\n                          opt['hidden_size'])\n    elif opt['arch'] == 'lstm':\n        seq2vec = TwoLSTM(vocab_words,\n                          opt['emb_size'],\n                          opt['hidden_size'],\n                          opt['num_layers'])\n    else:\n        raise NotImplementedError\n    return seq2vec\n\n\nif __name__ == '__main__':\n\n    vocab = ['robots', 'are', 'very', 'cool', '<eos>', 'BiDiBu']\n    lstm = TwoLSTM(vocab, 300, 1024)\n\n    input = Variable(torch.LongTensor([\n        [1,2,3,4,5,0,0],\n        [6,1,2,3,3,4,5],\n        [6,1,2,3,3,4,5]\n    ]))\n    output = lstm(input)\n"""
vqa/models/utils.py,1,"b'import sys\nimport copy\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nfrom .noatt import MLBNoAtt, MutanNoAtt\nfrom .att import MLBAtt, MutanAtt\n\nmodel_names = sorted(name for name in sys.modules[__name__].__dict__\n    if not name.startswith(""__""))# and \'Att\' in name)\n\ndef factory(opt, vocab_words, vocab_answers, cuda=True, data_parallel=True):\n    opt = copy.copy(opt)\n\n    if opt[\'arch\'] in model_names:\n        model = getattr(sys.modules[__name__], opt[\'arch\'])(opt, vocab_words, vocab_answers)\n    else:\n        raise ValueError\n\n    if data_parallel:\n        model = nn.DataParallel(model).cuda()\n        if not cuda:\n            raise ValueError\n\n    if cuda:\n        model.cuda()\n\n    return model'"
