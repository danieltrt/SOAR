file_path,api_count,code
decode_baselines.py,3,"b'"""""" run decoding of X-ext (+ abs)""""""\nimport argparse\nimport json\nimport os\nfrom os.path import join\nfrom datetime import timedelta\nfrom time import time\n\nfrom cytoolz import identity\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom data.batcher import tokenize\n\nfrom decoding import Abstractor, Extractor, DecodeDataset\nfrom decoding import make_html_safe\n\n\nMAX_ABS_NUM = 6  # need to set max sentences to extract for non-RL extractor\n\n\ndef decode(save_path, abs_dir, ext_dir, split, batch_size, max_len, cuda):\n    start = time()\n    # setup model\n    if abs_dir is None:\n        # NOTE: if no abstractor is provided then\n        #       the whole model would be extractive summarization\n        abstractor = identity\n    else:\n        abstractor = Abstractor(abs_dir, max_len, cuda)\n    if ext_dir is None:\n        # NOTE: if no abstractor is provided then\n        #       it would be  the lead-N extractor\n        extractor = lambda art_sents: list(range(len(art_sents)))[:MAX_ABS_NUM]\n    else:\n        extractor = Extractor(ext_dir, max_ext=MAX_ABS_NUM, cuda=cuda)\n\n    # setup loader\n    def coll(batch):\n        articles = list(filter(bool, batch))\n        return articles\n    dataset = DecodeDataset(split)\n\n    n_data = len(dataset)\n    loader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=False, num_workers=4,\n        collate_fn=coll\n    )\n\n    # prepare save paths and logs\n    for i in range(MAX_ABS_NUM):\n        os.makedirs(join(save_path, \'output_{}\'.format(i)))\n    dec_log = {}\n    dec_log[\'abstractor\'] = (None if abs_dir is None\n                             else json.load(open(join(abs_dir, \'meta.json\'))))\n    dec_log[\'extractor\'] = (None if ext_dir is None\n                            else json.load(open(join(ext_dir, \'meta.json\'))))\n    dec_log[\'rl\'] = False\n    dec_log[\'split\'] = split\n    dec_log[\'beam\'] = 1  # greedy decoding only\n    with open(join(save_path, \'log.json\'), \'w\') as f:\n        json.dump(dec_log, f, indent=4)\n\n    # Decoding\n    i = 0\n    with torch.no_grad():\n        for i_debug, raw_article_batch in enumerate(loader):\n            tokenized_article_batch = map(tokenize(None), raw_article_batch)\n            ext_arts = []\n            ext_inds = []\n            for raw_art_sents in tokenized_article_batch:\n                ext = extractor(raw_art_sents)\n                ext_inds += [(len(ext_arts), len(ext))]\n                ext_arts += list(map(lambda i: raw_art_sents[i], ext))\n            dec_outs = abstractor(ext_arts)\n            assert i == batch_size*i_debug\n            for j, n in ext_inds:\n                decoded_sents = [\' \'.join(dec) for dec in dec_outs[j:j+n]]\n                for k, dec_str in enumerate(decoded_sents):\n                    with open(join(save_path, \'output_{}/{}.dec\'.format(k, i)),\n                              \'w\') as f:\n                        f.write(make_html_safe(dec_str))\n\n                i += 1\n                print(\'{}/{} ({:.2f}%) decoded in {} seconds\\r\'.format(\n                    i, n_data, i/n_data*100, timedelta(seconds=int(time()-start))\n                ), end=\'\')\n    print()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n        description=(\'combine an extractor and an abstractor \'\n                     \'to decode summary and evaluate on the \'\n                     \'CNN/Daily Mail dataset\')\n    )\n    parser.add_argument(\'--path\', required=True, help=\'path to store/eval\')\n    parser.add_argument(\'--abs_dir\', help=\'root of the abstractor model\')\n    parser.add_argument(\'--ext_dir\', help=\'root of the extractor model\')\n\n    # dataset split\n    data = parser.add_mutually_exclusive_group(required=True)\n    data.add_argument(\'--val\', action=\'store_true\', help=\'use validation set\')\n    data.add_argument(\'--test\', action=\'store_true\', help=\'use test set\')\n\n    # decode options\n    parser.add_argument(\'--batch\', type=int, action=\'store\', default=32,\n                        help=\'batch size of faster decoding\')\n    parser.add_argument(\'--n_ext\', type=int, action=\'store\', default=4,\n                        help=\'number of sents to be extracted\')\n    parser.add_argument(\'--max_dec_word\', type=int, action=\'store\', default=30,\n                        help=\'maximun words to be decoded for the abstractor\')\n\n    parser.add_argument(\'--no-cuda\', action=\'store_true\',\n                        help=\'disable GPU training\')\n    args = parser.parse_args()\n    args.cuda = torch.cuda.is_available() and not args.no_cuda\n\n    data_split = \'test\' if args.test else \'val\'\n    decode(args.path, args.abs_dir, args.ext_dir,\n           data_split, args.batch, args.max_dec_word, args.cuda)\n'"
decode_full_model.py,3,"b'"""""" run decoding of rnn-ext + abs + RL (+ rerank)""""""\nimport argparse\nimport json\nimport os\nfrom os.path import join\nfrom datetime import timedelta\nfrom time import time\nfrom collections import Counter, defaultdict\nfrom itertools import product\nfrom functools import reduce\nimport operator as op\n\nfrom cytoolz import identity, concat, curry\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import multiprocessing as mp\n\nfrom data.batcher import tokenize\n\nfrom decoding import Abstractor, RLExtractor, DecodeDataset, BeamAbstractor\nfrom decoding import make_html_safe\n\n\ndef decode(save_path, model_dir, split, batch_size,\n           beam_size, diverse, max_len, cuda):\n    start = time()\n    # setup model\n    with open(join(model_dir, \'meta.json\')) as f:\n        meta = json.loads(f.read())\n    if meta[\'net_args\'][\'abstractor\'] is None:\n        # NOTE: if no abstractor is provided then\n        #       the whole model would be extractive summarization\n        assert beam_size == 1\n        abstractor = identity\n    else:\n        if beam_size == 1:\n            abstractor = Abstractor(join(model_dir, \'abstractor\'),\n                                    max_len, cuda)\n        else:\n            abstractor = BeamAbstractor(join(model_dir, \'abstractor\'),\n                                        max_len, cuda)\n    extractor = RLExtractor(model_dir, cuda=cuda)\n\n    # setup loader\n    def coll(batch):\n        articles = list(filter(bool, batch))\n        return articles\n    dataset = DecodeDataset(split)\n\n    n_data = len(dataset)\n    loader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=False, num_workers=4,\n        collate_fn=coll\n    )\n\n    # prepare save paths and logs\n    os.makedirs(join(save_path, \'output\'))\n    dec_log = {}\n    dec_log[\'abstractor\'] = meta[\'net_args\'][\'abstractor\']\n    dec_log[\'extractor\'] = meta[\'net_args\'][\'extractor\']\n    dec_log[\'rl\'] = True\n    dec_log[\'split\'] = split\n    dec_log[\'beam\'] = beam_size\n    dec_log[\'diverse\'] = diverse\n    with open(join(save_path, \'log.json\'), \'w\') as f:\n        json.dump(dec_log, f, indent=4)\n\n    # Decoding\n    i = 0\n    with torch.no_grad():\n        for i_debug, raw_article_batch in enumerate(loader):\n            tokenized_article_batch = map(tokenize(None), raw_article_batch)\n            ext_arts = []\n            ext_inds = []\n            for raw_art_sents in tokenized_article_batch:\n                ext = extractor(raw_art_sents)[:-1]  # exclude EOE\n                if not ext:\n                    # use top-5 if nothing is extracted\n                    # in some rare cases rnn-ext does not extract at all\n                    ext = list(range(5))[:len(raw_art_sents)]\n                else:\n                    ext = [i.item() for i in ext]\n                ext_inds += [(len(ext_arts), len(ext))]\n                ext_arts += [raw_art_sents[i] for i in ext]\n            if beam_size > 1:\n                all_beams = abstractor(ext_arts, beam_size, diverse)\n                dec_outs = rerank_mp(all_beams, ext_inds)\n            else:\n                dec_outs = abstractor(ext_arts)\n            assert i == batch_size*i_debug\n            for j, n in ext_inds:\n                decoded_sents = [\' \'.join(dec) for dec in dec_outs[j:j+n]]\n                with open(join(save_path, \'output/{}.dec\'.format(i)),\n                          \'w\') as f:\n                    f.write(make_html_safe(\'\\n\'.join(decoded_sents)))\n                i += 1\n                print(\'{}/{} ({:.2f}%) decoded in {} seconds\\r\'.format(\n                    i, n_data, i/n_data*100,\n                    timedelta(seconds=int(time()-start))\n                ), end=\'\')\n    print()\n\n_PRUNE = defaultdict(\n    lambda: 2,\n    {1:5, 2:5, 3:5, 4:5, 5:5, 6:4, 7:3, 8:3}\n)\n\ndef rerank(all_beams, ext_inds):\n    beam_lists = (all_beams[i: i+n] for i, n in ext_inds if n > 0)\n    return list(concat(map(rerank_one, beam_lists)))\n\ndef rerank_mp(all_beams, ext_inds):\n    beam_lists = [all_beams[i: i+n] for i, n in ext_inds if n > 0]\n    with mp.Pool(8) as pool:\n        reranked = pool.map(rerank_one, beam_lists)\n    return list(concat(reranked))\n\ndef rerank_one(beams):\n    @curry\n    def process_beam(beam, n):\n        for b in beam[:n]:\n            b.gram_cnt = Counter(_make_n_gram(b.sequence))\n        return beam[:n]\n    beams = map(process_beam(n=_PRUNE[len(beams)]), beams)\n    best_hyps = max(product(*beams), key=_compute_score)\n    dec_outs = [h.sequence for h in best_hyps]\n    return dec_outs\n\ndef _make_n_gram(sequence, n=2):\n    return (tuple(sequence[i:i+n]) for i in range(len(sequence)-(n-1)))\n\ndef _compute_score(hyps):\n    all_cnt = reduce(op.iadd, (h.gram_cnt for h in hyps), Counter())\n    repeat = sum(c-1 for g, c in all_cnt.items() if c > 1)\n    lp = sum(h.logprob for h in hyps) / sum(len(h.sequence) for h in hyps)\n    return (-repeat, lp)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n        description=\'run decoding of the full model (RL)\')\n    parser.add_argument(\'--path\', required=True, help=\'path to store/eval\')\n    parser.add_argument(\'--model_dir\', help=\'root of the full model\')\n\n    # dataset split\n    data = parser.add_mutually_exclusive_group(required=True)\n    data.add_argument(\'--val\', action=\'store_true\', help=\'use validation set\')\n    data.add_argument(\'--test\', action=\'store_true\', help=\'use test set\')\n\n    # decode options\n    parser.add_argument(\'--batch\', type=int, action=\'store\', default=32,\n                        help=\'batch size of faster decoding\')\n    parser.add_argument(\'--beam\', type=int, action=\'store\', default=1,\n                        help=\'beam size for beam-search (reranking included)\')\n    parser.add_argument(\'--div\', type=float, action=\'store\', default=1.0,\n                        help=\'diverse ratio for the diverse beam-search\')\n    parser.add_argument(\'--max_dec_word\', type=int, action=\'store\', default=30,\n                        help=\'maximun words to be decoded for the abstractor\')\n\n    parser.add_argument(\'--no-cuda\', action=\'store_true\',\n                        help=\'disable GPU training\')\n    args = parser.parse_args()\n    args.cuda = torch.cuda.is_available() and not args.no_cuda\n\n    data_split = \'test\' if args.test else \'val\'\n    decode(args.path, args.model_dir,\n           data_split, args.batch, args.beam, args.div,\n           args.max_dec_word, args.cuda)\n'"
decoding.py,6,"b'"""""" decoding utilities""""""\nimport json\nimport re\nimport os\nfrom os.path import join\nimport pickle as pkl\nfrom itertools import starmap\n\nfrom cytoolz import curry\n\nimport torch\n\nfrom utils import PAD, UNK, START, END\nfrom model.copy_summ import CopySumm\nfrom model.extract import ExtractSumm, PtrExtractSumm\nfrom model.rl import ActorCritic\nfrom data.batcher import conver2id, pad_batch_tensorize\nfrom data.data import CnnDmDataset\n\n\ntry:\n    DATASET_DIR = os.environ[\'DATA\']\nexcept KeyError:\n    print(\'please use environment variable to specify data directories\')\n\nclass DecodeDataset(CnnDmDataset):\n    """""" get the article sentences only (for decoding use)""""""\n    def __init__(self, split):\n        assert split in [\'val\', \'test\']\n        super().__init__(split, DATASET_DIR)\n\n    def __getitem__(self, i):\n        js_data = super().__getitem__(i)\n        art_sents = js_data[\'article\']\n        return art_sents\n\n\ndef make_html_safe(s):\n    """"""Rouge use html, has to make output html safe""""""\n    return s.replace(""<"", ""&lt;"").replace("">"", ""&gt;"")\n\n\ndef load_best_ckpt(model_dir, reverse=False):\n    """""" reverse=False->loss, reverse=True->reward/score""""""\n    ckpts = os.listdir(join(model_dir, \'ckpt\'))\n    ckpt_matcher = re.compile(\'^ckpt-.*-[0-9]*\')\n    ckpts = sorted([c for c in ckpts if ckpt_matcher.match(c)],\n                   key=lambda c: float(c.split(\'-\')[1]), reverse=reverse)\n    print(\'loading checkpoint {}...\'.format(ckpts[0]))\n    ckpt = torch.load(\n        join(model_dir, \'ckpt/{}\'.format(ckpts[0]))\n    )[\'state_dict\']\n    return ckpt\n\n\nclass Abstractor(object):\n    def __init__(self, abs_dir, max_len=30, cuda=True):\n        abs_meta = json.load(open(join(abs_dir, \'meta.json\')))\n        assert abs_meta[\'net\'] == \'base_abstractor\'\n        abs_args = abs_meta[\'net_args\']\n        abs_ckpt = load_best_ckpt(abs_dir)\n        word2id = pkl.load(open(join(abs_dir, \'vocab.pkl\'), \'rb\'))\n        abstractor = CopySumm(**abs_args)\n        abstractor.load_state_dict(abs_ckpt)\n        self._device = torch.device(\'cuda\' if cuda else \'cpu\')\n        self._net = abstractor.to(self._device)\n        self._word2id = word2id\n        self._id2word = {i: w for w, i in word2id.items()}\n        self._max_len = max_len\n\n    def _prepro(self, raw_article_sents):\n        ext_word2id = dict(self._word2id)\n        ext_id2word = dict(self._id2word)\n        for raw_words in raw_article_sents:\n            for w in raw_words:\n                if not w in ext_word2id:\n                    ext_word2id[w] = len(ext_word2id)\n                    ext_id2word[len(ext_id2word)] = w\n        articles = conver2id(UNK, self._word2id, raw_article_sents)\n        art_lens = [len(art) for art in articles]\n        article = pad_batch_tensorize(articles, PAD, cuda=False\n                                     ).to(self._device)\n        extend_arts = conver2id(UNK, ext_word2id, raw_article_sents)\n        extend_art = pad_batch_tensorize(extend_arts, PAD, cuda=False\n                                        ).to(self._device)\n        extend_vsize = len(ext_word2id)\n        dec_args = (article, art_lens, extend_art, extend_vsize,\n                    START, END, UNK, self._max_len)\n        return dec_args, ext_id2word\n\n    def __call__(self, raw_article_sents):\n        self._net.eval()\n        dec_args, id2word = self._prepro(raw_article_sents)\n        decs, attns = self._net.batch_decode(*dec_args)\n        def argmax(arr, keys):\n            return arr[max(range(len(arr)), key=lambda i: keys[i].item())]\n        dec_sents = []\n        for i, raw_words in enumerate(raw_article_sents):\n            dec = []\n            for id_, attn in zip(decs, attns):\n                if id_[i] == END:\n                    break\n                elif id_[i] == UNK:\n                    dec.append(argmax(raw_words, attn[i]))\n                else:\n                    dec.append(id2word[id_[i].item()])\n            dec_sents.append(dec)\n        return dec_sents\n\n\nclass BeamAbstractor(Abstractor):\n    def __call__(self, raw_article_sents, beam_size=5, diverse=1.0):\n        self._net.eval()\n        dec_args, id2word = self._prepro(raw_article_sents)\n        dec_args = (*dec_args, beam_size, diverse)\n        all_beams = self._net.batched_beamsearch(*dec_args)\n        all_beams = list(starmap(_process_beam(id2word),\n                                 zip(all_beams, raw_article_sents)))\n        return all_beams\n\n@curry\ndef _process_beam(id2word, beam, art_sent):\n    def process_hyp(hyp):\n        seq = []\n        for i, attn in zip(hyp.sequence[1:], hyp.attns[:-1]):\n            if i == UNK:\n                copy_word = art_sent[max(range(len(art_sent)),\n                                         key=lambda j: attn[j].item())]\n                seq.append(copy_word)\n            else:\n                seq.append(id2word[i])\n        hyp.sequence = seq\n        del hyp.hists\n        del hyp.attns\n        return hyp\n    return list(map(process_hyp, beam))\n\n\nclass Extractor(object):\n    def __init__(self, ext_dir, max_ext=5, cuda=True):\n        ext_meta = json.load(open(join(ext_dir, \'meta.json\')))\n        if ext_meta[\'net\'] == \'ml_ff_extractor\':\n            ext_cls = ExtractSumm\n        elif ext_meta[\'net\'] == \'ml_rnn_extractor\':\n            ext_cls = PtrExtractSumm\n        else:\n            raise ValueError()\n        ext_ckpt = load_best_ckpt(ext_dir)\n        ext_args = ext_meta[\'net_args\']\n        extractor = ext_cls(**ext_args)\n        extractor.load_state_dict(ext_ckpt)\n        word2id = pkl.load(open(join(ext_dir, \'vocab.pkl\'), \'rb\'))\n        self._device = torch.device(\'cuda\' if cuda else \'cpu\')\n        self._net = extractor.to(self._device)\n        self._word2id = word2id\n        self._id2word = {i: w for w, i in word2id.items()}\n        self._max_ext = max_ext\n\n    def __call__(self, raw_article_sents):\n        self._net.eval()\n        n_art = len(raw_article_sents)\n        articles = conver2id(UNK, self._word2id, raw_article_sents)\n        article = pad_batch_tensorize(articles, PAD, cuda=False\n                                     ).to(self._device)\n        indices = self._net.extract([article], k=min(n_art, self._max_ext))\n        return indices\n\n\nclass ArticleBatcher(object):\n    def __init__(self, word2id, cuda=True):\n        self._device = torch.device(\'cuda\' if cuda else \'cpu\')\n        self._word2id = word2id\n        self._device = torch.device(\'cuda\' if cuda else \'cpu\')\n\n    def __call__(self, raw_article_sents):\n        articles = conver2id(UNK, self._word2id, raw_article_sents)\n        article = pad_batch_tensorize(articles, PAD, cuda=False\n                                     ).to(self._device)\n        return article\n\nclass RLExtractor(object):\n    def __init__(self, ext_dir, cuda=True):\n        ext_meta = json.load(open(join(ext_dir, \'meta.json\')))\n        assert ext_meta[\'net\'] == \'rnn-ext_abs_rl\'\n        ext_args = ext_meta[\'net_args\'][\'extractor\'][\'net_args\']\n        word2id = pkl.load(open(join(ext_dir, \'agent_vocab.pkl\'), \'rb\'))\n        extractor = PtrExtractSumm(**ext_args)\n        agent = ActorCritic(extractor._sent_enc,\n                            extractor._art_enc,\n                            extractor._extractor,\n                            ArticleBatcher(word2id, cuda))\n        ext_ckpt = load_best_ckpt(ext_dir, reverse=True)\n        agent.load_state_dict(ext_ckpt)\n        self._device = torch.device(\'cuda\' if cuda else \'cpu\')\n        self._net = agent.to(self._device)\n        self._word2id = word2id\n        self._id2word = {i: w for w, i in word2id.items()}\n\n    def __call__(self, raw_article_sents):\n        self._net.eval()\n        indices = self._net(raw_article_sents)\n        return indices\n'"
eval_acl.py,0,"b'"""""" Evaluate the output files to get the numbers reported in ACL18""""""\nimport argparse\nfrom os.path import join, abspath, dirname, exists\n\nfrom evaluate import eval_meteor, eval_rouge\n\n\n_REF_DIR = join(abspath(dirname(__file__)), \'acl18_results\')\n\n\ndef main(args):\n    dec_dir = args.decode_dir\n    ref_dir = join(_REF_DIR, \'reference\')\n    if args.rouge:\n        dec_pattern = r\'(\\d+).dec\'\n        ref_pattern = \'#ID#.ref\'\n        output = eval_rouge(dec_pattern, dec_dir, ref_pattern, ref_dir)\n    else:\n        dec_pattern = \'[0-9]+.dec\'\n        ref_pattern = \'[0-9]+.ref\'\n        output = eval_meteor(dec_pattern, dec_dir, ref_pattern, ref_dir)\n    print(output)\n\n\nif __name__ == \'__main__\':\n    assert exists(_REF_DIR)\n    parser = argparse.ArgumentParser(\n        description=\'Evaluate the output files to get the numbers reported\'\n                    \' as in the ACL paper\'\n    )\n\n    # choose metric to evaluate\n    metric_opt = parser.add_mutually_exclusive_group(required=True)\n    metric_opt.add_argument(\'--rouge\', action=\'store_true\',\n                            help=\'ROUGE evaluation\')\n    metric_opt.add_argument(\'--meteor\', action=\'store_true\',\n                            help=\'METEOR evaluation\')\n\n    parser.add_argument(\'--decode_dir\', action=\'store\', required=True,\n                        help=\'directory of decoded summaries\')\n\n    args = parser.parse_args()\n    main(args)\n'"
eval_baselines.py,0,"b'"""""" Evaluate the baselines ont ROUGE/METEOR""""""\nimport argparse\nimport json\nimport re\nimport os\nfrom os.path import join, exists\n\nfrom evaluate import eval_meteor, eval_rouge\n\n\ntry:\n    _DATA_DIR = os.environ[\'DATA\']\nexcept KeyError:\n    print(\'please use environment variable to specify data directories\')\n\ndef make_summaries(decode_dir, n_ext):\n    out_dir = join(decode_dir, \'output_top{}\'.format(args.n_ext))\n    os.makedirs(out_dir)\n    decs = os.listdir(join(decode_dir, \'output_0\'))\n    dec_matcher = re.compile(\'[0-9]*.dec\')\n    decs = sorted([d for d in decs if dec_matcher.match(d)],\n                   key=lambda d: float(d.split(\'.\')[0]))\n    for d in decs:\n        sents = []\n        for i in range(n_ext):\n            fname = join(decode_dir, \'output_{}/{}\'.format(i, d))\n            if exists(fname):\n                with open(fname, \'r\') as f:\n                    sents.append(f.read())\n        with open(join(out_dir, d), \'w\') as f:\n            f.write(\'\\n\'.join(sents))\n\n\ndef main(args):\n    dec_dir = join(args.decode_dir, \'output_top{}\'.format(args.n_ext))\n    if not exists(dec_dir):\n        make_summaries(args.decode_dir, args.n_ext)\n    with open(join(args.decode_dir, \'log.json\')) as f:\n        split = json.loads(f.read())[\'split\']\n    ref_dir = join(_DATA_DIR, \'refs\', split)\n    assert exists(ref_dir)\n\n    if args.rouge:\n        dec_pattern = r\'(\\d+).dec\'\n        ref_pattern = \'#ID#.ref\'\n        output = eval_rouge(dec_pattern, dec_dir, ref_pattern, ref_dir)\n        metric = \'rouge\'\n    else:\n        dec_pattern = \'[0-9]+.dec\'\n        ref_pattern = \'[0-9]+.ref\'\n        output = eval_meteor(dec_pattern, dec_dir, ref_pattern, ref_dir)\n        metric = \'meteor\'\n    print(output)\n    with open(join(args.decode_dir,\n                   \'top{}_{}.txt\'.format(args.n_ext, metric)), \'w\') as f:\n        f.write(output)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n        description=\'Evaluate the output files for the baseline models\')\n\n    # choose metric to evaluate\n    metric_opt = parser.add_mutually_exclusive_group(required=True)\n    metric_opt.add_argument(\'--rouge\', action=\'store_true\',\n                            help=\'ROUGE evaluation\')\n    metric_opt.add_argument(\'--meteor\', action=\'store_true\',\n                            help=\'METEOR evaluation\')\n\n    parser.add_argument(\'--n_ext\', type=int, action=\'store\', required=True,\n                        help=\'number of sentences to extract\')\n    parser.add_argument(\'--decode_dir\', action=\'store\', required=True,\n                        help=\'directory of decoded summaries\')\n\n    args = parser.parse_args()\n    main(args)\n'"
eval_full_model.py,0,"b'"""""" Evaluate the baselines ont ROUGE/METEOR""""""\nimport argparse\nimport json\nimport os\nfrom os.path import join, exists\n\nfrom evaluate import eval_meteor, eval_rouge\n\n\ntry:\n    _DATA_DIR = os.environ[\'DATA\']\nexcept KeyError:\n    print(\'please use environment variable to specify data directories\')\n\n\ndef main(args):\n    dec_dir = join(args.decode_dir, \'output\')\n    with open(join(args.decode_dir, \'log.json\')) as f:\n        split = json.loads(f.read())[\'split\']\n    ref_dir = join(_DATA_DIR, \'refs\', split)\n    assert exists(ref_dir)\n\n    if args.rouge:\n        dec_pattern = r\'(\\d+).dec\'\n        ref_pattern = \'#ID#.ref\'\n        output = eval_rouge(dec_pattern, dec_dir, ref_pattern, ref_dir)\n        metric = \'rouge\'\n    else:\n        dec_pattern = \'[0-9]+.dec\'\n        ref_pattern = \'[0-9]+.ref\'\n        output = eval_meteor(dec_pattern, dec_dir, ref_pattern, ref_dir)\n        metric = \'meteor\'\n    print(output)\n    with open(join(args.decode_dir, \'{}.txt\'.format(metric)), \'w\') as f:\n        f.write(output)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n        description=\'Evaluate the output files for the RL full models\')\n\n    # choose metric to evaluate\n    metric_opt = parser.add_mutually_exclusive_group(required=True)\n    metric_opt.add_argument(\'--rouge\', action=\'store_true\',\n                            help=\'ROUGE evaluation\')\n    metric_opt.add_argument(\'--meteor\', action=\'store_true\',\n                            help=\'METEOR evaluation\')\n\n    parser.add_argument(\'--decode_dir\', action=\'store\', required=True,\n                        help=\'directory of decoded summaries\')\n\n    args = parser.parse_args()\n    main(args)\n'"
evaluate.py,0,"b'"""""" evaluation scripts""""""\nimport re\nimport os\nfrom os.path import join\nimport logging\nimport tempfile\nimport subprocess as sp\n\nfrom cytoolz import curry\n\nfrom pyrouge import Rouge155\nfrom pyrouge.utils import log\n\n\ntry:\n    _ROUGE_PATH = os.environ[\'ROUGE\']\nexcept KeyError:\n    print(\'Warning: ROUGE is not configured\')\n    _ROUGE_PATH = None\ndef eval_rouge(dec_pattern, dec_dir, ref_pattern, ref_dir,\n               cmd=\'-c 95 -r 1000 -n 2 -m\', system_id=1):\n    """""" evaluate by original Perl implementation""""""\n    # silence pyrouge logging\n    assert _ROUGE_PATH is not None\n    log.get_global_console_logger().setLevel(logging.WARNING)\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        Rouge155.convert_summaries_to_rouge_format(\n            dec_dir, join(tmp_dir, \'dec\'))\n        Rouge155.convert_summaries_to_rouge_format(\n            ref_dir, join(tmp_dir, \'ref\'))\n        Rouge155.write_config_static(\n            join(tmp_dir, \'dec\'), dec_pattern,\n            join(tmp_dir, \'ref\'), ref_pattern,\n            join(tmp_dir, \'settings.xml\'), system_id\n        )\n        cmd = (join(_ROUGE_PATH, \'ROUGE-1.5.5.pl\')\n               + \' -e {} \'.format(join(_ROUGE_PATH, \'data\'))\n               + cmd\n               + \' -a {}\'.format(join(tmp_dir, \'settings.xml\')))\n        output = sp.check_output(cmd.split(\' \'), universal_newlines=True)\n    return output\n\n\ntry:\n    _METEOR_PATH = os.environ[\'METEOR\']\nexcept KeyError:\n    print(\'Warning: METEOR is not configured\')\n    _METEOR_PATH = None\ndef eval_meteor(dec_pattern, dec_dir, ref_pattern, ref_dir):\n    """""" METEOR evaluation""""""\n    assert _METEOR_PATH is not None\n    ref_matcher = re.compile(ref_pattern)\n    refs = sorted([r for r in os.listdir(ref_dir) if ref_matcher.match(r)],\n                  key=lambda name: int(name.split(\'.\')[0]))\n    dec_matcher = re.compile(dec_pattern)\n    decs = sorted([d for d in os.listdir(dec_dir) if dec_matcher.match(d)],\n                  key=lambda name: int(name.split(\'.\')[0]))\n    @curry\n    def read_file(file_dir, file_name):\n        with open(join(file_dir, file_name)) as f:\n            return \' \'.join(f.read().split())\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        with open(join(tmp_dir, \'ref.txt\'), \'w\') as ref_f,\\\n             open(join(tmp_dir, \'dec.txt\'), \'w\') as dec_f:\n            ref_f.write(\'\\n\'.join(map(read_file(ref_dir), refs)) + \'\\n\')\n            dec_f.write(\'\\n\'.join(map(read_file(dec_dir), decs)) + \'\\n\')\n\n        cmd = \'java -Xmx2G -jar {} {} {} -l en -norm\'.format(\n            _METEOR_PATH, join(tmp_dir, \'dec.txt\'), join(tmp_dir, \'ref.txt\'))\n        output = sp.check_output(cmd.split(\' \'), universal_newlines=True)\n    return output\n'"
make_eval_references.py,0,"b'"""""" make reference text files needed for ROUGE evaluation """"""\nimport json\nimport os\nfrom os.path import join, exists\nfrom time import time\nfrom datetime import timedelta\n\nfrom utils import count_data\nfrom decoding import make_html_safe\n\ntry:\n    DATA_DIR = os.environ[\'DATA\']\nexcept KeyError:\n    print(\'please use environment variable to specify data directories\')\n\n\ndef dump(split):\n    start = time()\n    print(\'start processing {} split...\'.format(split))\n    data_dir = join(DATA_DIR, split)\n    dump_dir = join(DATA_DIR, \'refs\', split)\n    n_data = count_data(data_dir)\n    for i in range(n_data):\n        print(\'processing {}/{} ({:.2f}%%)\\r\'.format(i, n_data, 100*i/n_data),\n              end=\'\')\n        with open(join(data_dir, \'{}.json\'.format(i))) as f:\n            data = json.loads(f.read())\n        abs_sents = data[\'abstract\']\n        with open(join(dump_dir, \'{}.ref\'.format(i)), \'w\') as f:\n            f.write(make_html_safe(\'\\n\'.join(abs_sents)))\n    print(\'finished in {}\'.format(timedelta(seconds=time()-start)))\n\ndef main():\n    for split in [\'val\', \'test\']:  # evaluation of train data takes too long\n        if not exists(join(DATA_DIR, \'refs\', split)):\n            os.makedirs(join(DATA_DIR, \'refs\', split))\n        dump(split)\n\nif __name__ == \'__main__\':\n    main()\n'"
make_extraction_labels.py,0,"b'""""""produce the dataset with (psudo) extraction label""""""\nimport os\nfrom os.path import exists, join\nimport json\nfrom time import time\nfrom datetime import timedelta\nimport multiprocessing as mp\n\nfrom cytoolz import curry, compose\n\nfrom utils import count_data\nfrom metric import compute_rouge_l\n\n\ntry:\n    DATA_DIR = os.environ[\'DATA\']\nexcept KeyError:\n    print(\'please use environment variable to specify data directories\')\n\n\ndef _split_words(texts):\n    return map(lambda t: t.split(), texts)\n\n\ndef get_extract_label(art_sents, abs_sents):\n    """""" greedily match summary sentences to article sentences""""""\n    extracted = []\n    scores = []\n    indices = list(range(len(art_sents)))\n    for abst in abs_sents:\n        rouges = list(map(compute_rouge_l(reference=abst, mode=\'r\'),\n                          art_sents))\n        ext = max(indices, key=lambda i: rouges[i])\n        indices.remove(ext)\n        extracted.append(ext)\n        scores.append(rouges[ext])\n        if not indices:\n            break\n    return extracted, scores\n\n@curry\ndef process(split, i):\n    data_dir = join(DATA_DIR, split)\n    with open(join(data_dir, \'{}.json\'.format(i))) as f:\n        data = json.loads(f.read())\n    tokenize = compose(list, _split_words)\n    art_sents = tokenize(data[\'article\'])\n    abs_sents = tokenize(data[\'abstract\'])\n    if art_sents and abs_sents: # some data contains empty article/abstract\n        extracted, scores = get_extract_label(art_sents, abs_sents)\n    else:\n        extracted, scores = [], []\n    data[\'extracted\'] = extracted\n    data[\'score\'] = scores\n    with open(join(data_dir, \'{}.json\'.format(i)), \'w\') as f:\n        json.dump(data, f, indent=4)\n\ndef label_mp(split):\n    """""" process the data split with multi-processing""""""\n    start = time()\n    print(\'start processing {} split...\'.format(split))\n    data_dir = join(DATA_DIR, split)\n    n_data = count_data(data_dir)\n    with mp.Pool() as pool:\n        list(pool.imap_unordered(process(split),\n                                 list(range(n_data)), chunksize=1024))\n    print(\'finished in {}\'.format(timedelta(seconds=time()-start)))\n\ndef label(split):\n    start = time()\n    print(\'start processing {} split...\'.format(split))\n    data_dir = join(DATA_DIR, split)\n    n_data = count_data(data_dir)\n    for i in range(n_data):\n        print(\'processing {}/{} ({:.2f}%%)\\r\'.format(i, n_data, 100*i/n_data),\n              end=\'\')\n        with open(join(data_dir, \'{}.json\'.format(i))) as f:\n            data = json.loads(f.read())\n        tokenize = compose(list, _split_words)\n        art_sents = tokenize(data[\'article\'])\n        abs_sents = tokenize(data[\'abstract\'])\n        extracted, scores = get_extract_label(art_sents, abs_sents)\n        data[\'extracted\'] = extracted\n        data[\'score\'] = scores\n        with open(join(data_dir, \'{}.json\'.format(i)), \'w\') as f:\n            json.dump(data, f, indent=4)\n    print(\'finished in {}\'.format(timedelta(seconds=time()-start)))\n\n\ndef main():\n    for split in [\'val\', \'train\']:  # no need of extraction label when testing\n        label_mp(split)\n\nif __name__ == \'__main__\':\n    main()\n'"
metric.py,0,"b'"""""" ROUGE utils""""""\nimport os\nimport threading\nimport subprocess as sp\nfrom collections import Counter, deque\n\nfrom cytoolz import concat, curry\n\n\ndef make_n_grams(seq, n):\n    """""" return iterator """"""\n    ngrams = (tuple(seq[i:i+n]) for i in range(len(seq)-n+1))\n    return ngrams\n\ndef _n_gram_match(summ, ref, n):\n    summ_grams = Counter(make_n_grams(summ, n))\n    ref_grams = Counter(make_n_grams(ref, n))\n    grams = min(summ_grams, ref_grams, key=len)\n    count = sum(min(summ_grams[g], ref_grams[g]) for g in grams)\n    return count\n\n@curry\ndef compute_rouge_n(output, reference, n=1, mode=\'f\'):\n    """""" compute ROUGE-N for a single pair of summary and reference""""""\n    assert mode in list(\'fpr\')  # F-1, precision, recall\n    match = _n_gram_match(reference, output, n)\n    if match == 0:\n        score = 0.0\n    else:\n        precision = match / len(output)\n        recall = match / len(reference)\n        f_score = 2 * (precision * recall) / (precision + recall)\n        if mode == \'p\':\n            score = precision\n        elif mode == \'r\':\n            score = recall\n        else:\n            score = f_score\n    return score\n\n\ndef _lcs_dp(a, b):\n    """""" compute the len dp of lcs""""""\n    dp = [[0 for _ in range(0, len(b)+1)]\n          for _ in range(0, len(a)+1)]\n    # dp[i][j]: lcs_len(a[:i], b[:j])\n    for i in range(1, len(a)+1):\n        for j in range(1, len(b)+1):\n            if a[i-1] == b[j-1]:\n                dp[i][j] = dp[i-1][j-1] + 1\n            else:\n                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n    return dp\n\ndef _lcs_len(a, b):\n    """""" compute the length of longest common subsequence between a and b""""""\n    dp = _lcs_dp(a, b)\n    return dp[-1][-1]\n\n@curry\ndef compute_rouge_l(output, reference, mode=\'f\'):\n    """""" compute ROUGE-L for a single pair of summary and reference\n    output, reference are list of words\n    """"""\n    assert mode in list(\'fpr\')  # F-1, precision, recall\n    lcs = _lcs_len(output, reference)\n    if lcs == 0:\n        score = 0.0\n    else:\n        precision = lcs / len(output)\n        recall = lcs / len(reference)\n        f_score = 2 * (precision * recall) / (precision + recall)\n        if mode == \'p\':\n            score = precision\n        if mode == \'r\':\n            score = recall\n        else:\n            score = f_score\n    return score\n\n\ndef _lcs(a, b):\n    """""" compute the longest common subsequence between a and b""""""\n    dp = _lcs_dp(a, b)\n    i = len(a)\n    j = len(b)\n    lcs = deque()\n    while (i > 0 and j > 0):\n        if a[i-1] == b[j-1]:\n            lcs.appendleft(a[i-1])\n            i -= 1\n            j -= 1\n        elif dp[i-1][j] >= dp[i][j-1]:\n            i -= 1\n        else:\n            j -= 1\n    assert len(lcs) == dp[-1][-1]\n    return lcs\n\ndef compute_rouge_l_summ(summs, refs, mode=\'f\'):\n    """""" summary level ROUGE-L""""""\n    assert mode in list(\'fpr\')  # F-1, precision, recall\n    tot_hit = 0\n    ref_cnt = Counter(concat(refs))\n    summ_cnt = Counter(concat(summs))\n    for ref in refs:\n        for summ in summs:\n            lcs = _lcs(summ, ref)\n            for gram in lcs:\n                if ref_cnt[gram] > 0 and summ_cnt[gram] > 0:\n                    tot_hit += 1\n                ref_cnt[gram] -= 1\n                summ_cnt[gram] -= 1\n    if tot_hit == 0:\n        score = 0.0\n    else:\n        precision = tot_hit / sum((len(s) for s in summs))\n        recall = tot_hit / sum((len(r) for r in refs))\n        f_score = 2 * (precision * recall) / (precision + recall)\n        if mode == \'p\':\n            score = precision\n        if mode == \'r\':\n            score = recall\n        else:\n            score = f_score\n    return score\n\n\ntry:\n    _METEOR_PATH = os.environ[\'METEOR\']\nexcept KeyError:\n    print(\'Warning: METEOR is not configured\')\n    _METEOR_PATH = None\nclass Meteor(object):\n    def __init__(self):\n        assert _METEOR_PATH is not None\n        cmd = \'java -Xmx2G -jar {} - - -l en -norm -stdio\'.format(_METEOR_PATH)\n        self._meteor_proc = sp.Popen(\n            cmd.split(),\n            stdin=sp.PIPE, stdout=sp.PIPE, stderr=sp.PIPE,\n            universal_newlines=True, encoding=\'utf-8\', bufsize=1\n        )\n        self._lock = threading.Lock()\n\n    def __call__(self, summ, ref):\n        self._lock.acquire()\n        score_line = \'SCORE ||| {} ||| {}\\n\'.format(\n            \' \'.join(ref), \' \'.join(summ))\n        self._meteor_proc.stdin.write(score_line)\n        stats = self._meteor_proc.stdout.readline().strip()\n        eval_line = \'EVAL ||| {}\\n\'.format(stats)\n        self._meteor_proc.stdin.write(eval_line)\n        score = float(self._meteor_proc.stdout.readline().strip())\n        self._lock.release()\n        return score\n\n    def __del__(self):\n        self._lock.acquire()\n        self._meteor_proc.stdin.close()\n        self._meteor_proc.kill()\n        self._meteor_proc.wait()\n        self._lock.release()\n'"
rl.py,7,"b'"""""" RL training utilities""""""\nimport math\nfrom time import time\nfrom datetime import timedelta\n\nfrom toolz.sandbox.core import unzip\nfrom cytoolz import concat\n\nimport numpy as np\nimport torch\nfrom torch.nn import functional as F\nfrom torch import autograd\nfrom torch.nn.utils import clip_grad_norm_\n\nfrom metric import compute_rouge_l, compute_rouge_n\nfrom training import BasicPipeline\n\n\ndef a2c_validate(agent, abstractor, loader):\n    agent.eval()\n    start = time()\n    print(\'start running validation...\', end=\'\')\n    avg_reward = 0\n    i = 0\n    with torch.no_grad():\n        for art_batch, abs_batch in loader:\n            ext_sents = []\n            ext_inds = []\n            for raw_arts in art_batch:\n                indices = agent(raw_arts)\n                ext_inds += [(len(ext_sents), len(indices)-1)]\n                ext_sents += [raw_arts[idx.item()]\n                              for idx in indices if idx.item() < len(raw_arts)]\n            all_summs = abstractor(ext_sents)\n            for (j, n), abs_sents in zip(ext_inds, abs_batch):\n                summs = all_summs[j:j+n]\n                # python ROUGE-1 (not official evaluation)\n                avg_reward += compute_rouge_n(list(concat(summs)),\n                                              list(concat(abs_sents)), n=1)\n                i += 1\n    avg_reward /= (i/100)\n    print(\'finished in {}! avg reward: {:.2f}\'.format(\n        timedelta(seconds=int(time()-start)), avg_reward))\n    return {\'reward\': avg_reward}\n\n\ndef a2c_train_step(agent, abstractor, loader, opt, grad_fn,\n                   gamma=0.99, reward_fn=compute_rouge_l,\n                   stop_reward_fn=compute_rouge_n(n=1), stop_coeff=1.0):\n    opt.zero_grad()\n    indices = []\n    probs = []\n    baselines = []\n    ext_sents = []\n    art_batch, abs_batch = next(loader)\n    for raw_arts in art_batch:\n        (inds, ms), bs = agent(raw_arts)\n        baselines.append(bs)\n        indices.append(inds)\n        probs.append(ms)\n        ext_sents += [raw_arts[idx.item()]\n                      for idx in inds if idx.item() < len(raw_arts)]\n    with torch.no_grad():\n        summaries = abstractor(ext_sents)\n    i = 0\n    rewards = []\n    avg_reward = 0\n    for inds, abss in zip(indices, abs_batch):\n        rs = ([reward_fn(summaries[i+j], abss[j])\n              for j in range(min(len(inds)-1, len(abss)))]\n              + [0 for _ in range(max(0, len(inds)-1-len(abss)))]\n              + [stop_coeff*stop_reward_fn(\n                  list(concat(summaries[i:i+len(inds)-1])),\n                  list(concat(abss)))])\n        assert len(rs) == len(inds)\n        avg_reward += rs[-1]/stop_coeff\n        i += len(inds)-1\n        # compute discounted rewards\n        R = 0\n        disc_rs = []\n        for r in rs[::-1]:\n            R = r + gamma * R\n            disc_rs.insert(0, R)\n        rewards += disc_rs\n    indices = list(concat(indices))\n    probs = list(concat(probs))\n    baselines = list(concat(baselines))\n    # standardize rewards\n    reward = torch.Tensor(rewards).to(baselines[0].device)\n    reward = (reward - reward.mean()) / (\n        reward.std() + float(np.finfo(np.float32).eps))\n    baseline = torch.cat(baselines).squeeze()\n    avg_advantage = 0\n    losses = []\n    for action, p, r, b in zip(indices, probs, reward, baseline):\n        advantage = r - b\n        avg_advantage += advantage\n        losses.append(-p.log_prob(action)\n                      * (advantage/len(indices))) # divide by T*B\n    critic_loss = F.mse_loss(baseline, reward)\n    # backprop and update\n    autograd.backward(\n        [critic_loss] + losses,\n        [torch.ones(1).to(critic_loss.device)]*(1+len(losses))\n    )\n    grad_log = grad_fn()\n    opt.step()\n    log_dict = {}\n    log_dict.update(grad_log)\n    log_dict[\'reward\'] = avg_reward/len(art_batch)\n    log_dict[\'advantage\'] = avg_advantage.item()/len(indices)\n    log_dict[\'mse\'] = critic_loss.item()\n    assert not math.isnan(log_dict[\'grad_norm\'])\n    return log_dict\n\n\ndef get_grad_fn(agent, clip_grad, max_grad=1e2):\n    """""" monitor gradient for each sub-component""""""\n    params = [p for p in agent.parameters()]\n    def f():\n        grad_log = {}\n        for n, m in agent.named_children():\n            tot_grad = 0\n            for p in m.parameters():\n                if p.grad is not None:\n                    tot_grad += p.grad.norm(2) ** 2\n            tot_grad = tot_grad ** (1/2)\n            grad_log[\'grad_norm\'+n] = tot_grad.item()\n        grad_norm = clip_grad_norm_(\n            [p for p in params if p.requires_grad], clip_grad)\n        grad_norm = grad_norm.item()\n        if max_grad is not None and grad_norm >= max_grad:\n            print(\'WARNING: Exploding Gradients {:.2f}\'.format(grad_norm))\n            grad_norm = max_grad\n        grad_log[\'grad_norm\'] = grad_norm\n        return grad_log\n    return f\n\n\nclass A2CPipeline(BasicPipeline):\n    def __init__(self, name,\n                 net, abstractor,\n                 train_batcher, val_batcher,\n                 optim, grad_fn,\n                 reward_fn, gamma,\n                 stop_reward_fn, stop_coeff):\n        self.name = name\n        self._net = net\n        self._train_batcher = train_batcher\n        self._val_batcher = val_batcher\n        self._opt = optim\n        self._grad_fn = grad_fn\n\n        self._abstractor = abstractor\n        self._gamma = gamma\n        self._reward_fn = reward_fn\n        self._stop_reward_fn = stop_reward_fn\n        self._stop_coeff = stop_coeff\n\n        self._n_epoch = 0  # epoch not very useful?\n\n    def batches(self):\n        raise NotImplementedError(\'A2C does not use batcher\')\n\n    def train_step(self):\n        # forward pass of model\n        self._net.train()\n        log_dict = a2c_train_step(\n            self._net, self._abstractor,\n            self._train_batcher,\n            self._opt, self._grad_fn,\n            self._gamma, self._reward_fn,\n            self._stop_reward_fn, self._stop_coeff\n        )\n        return log_dict\n\n    def validate(self):\n        return a2c_validate(self._net, self._abstractor, self._val_batcher)\n\n    def checkpoint(self, *args, **kwargs):\n        # explicitly use inherited function in case I forgot :)\n        return super().checkpoint(*args, **kwargs)\n\n    def terminate(self):\n        pass  # No extra processs so do nothing\n'"
train_abstractor.py,4,"b'"""""" train the abstractor""""""\nimport argparse\nimport json\nimport os\nfrom os.path import join, exists\nimport pickle as pkl\n\nfrom cytoolz import compose\n\nimport torch\nfrom torch import optim\nfrom torch.nn import functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\n\nfrom model.copy_summ import CopySumm\nfrom model.util import sequence_loss\nfrom training import get_basic_grad_fn, basic_validate\nfrom training import BasicPipeline, BasicTrainer\n\nfrom data.data import CnnDmDataset\nfrom data.batcher import coll_fn, prepro_fn\nfrom data.batcher import convert_batch_copy, batchify_fn_copy\nfrom data.batcher import BucketedGenerater\n\nfrom utils import PAD, UNK, START, END\nfrom utils import make_vocab, make_embedding\n\n# NOTE: bucket size too large may sacrifice randomness,\n#       to low may increase # of PAD tokens\nBUCKET_SIZE = 6400\n\ntry:\n    DATA_DIR = os.environ[\'DATA\']\nexcept KeyError:\n    print(\'please use environment variable to specify data directories\')\n\nclass MatchDataset(CnnDmDataset):\n    """""" single article sentence -> single abstract sentence\n    (dataset created by greedily matching ROUGE)\n    """"""\n    def __init__(self, split):\n        super().__init__(split, DATA_DIR)\n\n    def __getitem__(self, i):\n        js_data = super().__getitem__(i)\n        art_sents, abs_sents, extracts = (\n            js_data[\'article\'], js_data[\'abstract\'], js_data[\'extracted\'])\n        matched_arts = [art_sents[i] for i in extracts]\n        return matched_arts, abs_sents[:len(extracts)]\n\n\ndef configure_net(vocab_size, emb_dim,\n                  n_hidden, bidirectional, n_layer):\n    net_args = {}\n    net_args[\'vocab_size\']    = vocab_size\n    net_args[\'emb_dim\']       = emb_dim\n    net_args[\'n_hidden\']      = n_hidden\n    net_args[\'bidirectional\'] = bidirectional\n    net_args[\'n_layer\']       = n_layer\n\n    net = CopySumm(**net_args)\n    return net, net_args\n\n\ndef configure_training(opt, lr, clip_grad, lr_decay, batch_size):\n    """""" supports Adam optimizer only""""""\n    assert opt in [\'adam\']\n    opt_kwargs = {}\n    opt_kwargs[\'lr\'] = lr\n\n    train_params = {}\n    train_params[\'optimizer\']      = (opt, opt_kwargs)\n    train_params[\'clip_grad_norm\'] = clip_grad\n    train_params[\'batch_size\']     = batch_size\n    train_params[\'lr_decay\']       = lr_decay\n\n    nll = lambda logit, target: F.nll_loss(logit, target, reduce=False)\n    def criterion(logits, targets):\n        return sequence_loss(logits, targets, nll, pad_idx=PAD)\n\n    return criterion, train_params\n\ndef build_batchers(word2id, cuda, debug):\n    prepro = prepro_fn(args.max_art, args.max_abs)\n    def sort_key(sample):\n        src, target = sample\n        return (len(target), len(src))\n    batchify = compose(\n        batchify_fn_copy(PAD, START, END, cuda=cuda),\n        convert_batch_copy(UNK, word2id)\n    )\n\n    train_loader = DataLoader(\n        MatchDataset(\'train\'), batch_size=BUCKET_SIZE,\n        shuffle=not debug,\n        num_workers=4 if cuda and not debug else 0,\n        collate_fn=coll_fn\n    )\n    train_batcher = BucketedGenerater(train_loader, prepro, sort_key, batchify,\n                                      single_run=False, fork=not debug)\n\n    val_loader = DataLoader(\n        MatchDataset(\'val\'), batch_size=BUCKET_SIZE,\n        shuffle=False, num_workers=4 if cuda and not debug else 0,\n        collate_fn=coll_fn\n    )\n    val_batcher = BucketedGenerater(val_loader, prepro, sort_key, batchify,\n                                    single_run=True, fork=not debug)\n    return train_batcher, val_batcher\n\ndef main(args):\n    # create data batcher, vocabulary\n    # batcher\n    with open(join(DATA_DIR, \'vocab_cnt.pkl\'), \'rb\') as f:\n        wc = pkl.load(f)\n    word2id = make_vocab(wc, args.vsize)\n    train_batcher, val_batcher = build_batchers(word2id,\n                                                args.cuda, args.debug)\n\n    # make net\n    net, net_args = configure_net(len(word2id), args.emb_dim,\n                                  args.n_hidden, args.bi, args.n_layer)\n    if args.w2v:\n        # NOTE: the pretrained embedding having the same dimension\n        #       as args.emb_dim should already be trained\n        embedding, _ = make_embedding(\n            {i: w for w, i in word2id.items()}, args.w2v)\n        net.set_embedding(embedding)\n\n    # configure training setting\n    criterion, train_params = configure_training(\n        \'adam\', args.lr, args.clip, args.decay, args.batch\n    )\n\n    # save experiment setting\n    if not exists(args.path):\n        os.makedirs(args.path)\n    with open(join(args.path, \'vocab.pkl\'), \'wb\') as f:\n        pkl.dump(word2id, f, pkl.HIGHEST_PROTOCOL)\n    meta = {}\n    meta[\'net\']           = \'base_abstractor\'\n    meta[\'net_args\']      = net_args\n    meta[\'traing_params\'] = train_params\n    with open(join(args.path, \'meta.json\'), \'w\') as f:\n        json.dump(meta, f, indent=4)\n\n    # prepare trainer\n    val_fn = basic_validate(net, criterion)\n    grad_fn = get_basic_grad_fn(net, args.clip)\n    optimizer = optim.Adam(net.parameters(), **train_params[\'optimizer\'][1])\n    scheduler = ReduceLROnPlateau(optimizer, \'min\', verbose=True,\n                                  factor=args.decay, min_lr=0,\n                                  patience=args.lr_p)\n\n    if args.cuda:\n        net = net.cuda()\n    pipeline = BasicPipeline(meta[\'net\'], net,\n                             train_batcher, val_batcher, args.batch, val_fn,\n                             criterion, optimizer, grad_fn)\n    trainer = BasicTrainer(pipeline, args.path,\n                           args.ckpt_freq, args.patience, scheduler)\n\n    print(\'start training with the following hyper-parameters:\')\n    print(meta)\n    trainer.train()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n        description=\'training of the abstractor (ML)\'\n    )\n    parser.add_argument(\'--path\', required=True, help=\'root of the model\')\n\n\n    parser.add_argument(\'--vsize\', type=int, action=\'store\', default=30000,\n                        help=\'vocabulary size\')\n    parser.add_argument(\'--emb_dim\', type=int, action=\'store\', default=128,\n                        help=\'the dimension of word embedding\')\n    parser.add_argument(\'--w2v\', action=\'store\',\n                        help=\'use pretrained word2vec embedding\')\n    parser.add_argument(\'--n_hidden\', type=int, action=\'store\', default=256,\n                        help=\'the number of hidden units of LSTM\')\n    parser.add_argument(\'--n_layer\', type=int, action=\'store\', default=1,\n                        help=\'the number of layers of LSTM\')\n    parser.add_argument(\'--no-bi\', action=\'store_true\',\n                        help=\'disable bidirectional LSTM encoder\')\n\n    # length limit\n    parser.add_argument(\'--max_art\', type=int, action=\'store\', default=100,\n                        help=\'maximun words in a single article sentence\')\n    parser.add_argument(\'--max_abs\', type=int, action=\'store\', default=30,\n                        help=\'maximun words in a single abstract sentence\')\n    # training options\n    parser.add_argument(\'--lr\', type=float, action=\'store\', default=1e-3,\n                        help=\'learning rate\')\n    parser.add_argument(\'--decay\', type=float, action=\'store\', default=0.5,\n                        help=\'learning rate decay ratio\')\n    parser.add_argument(\'--lr_p\', type=int, action=\'store\', default=0,\n                        help=\'patience for learning rate decay\')\n    parser.add_argument(\'--clip\', type=float, action=\'store\', default=2.0,\n                        help=\'gradient clipping\')\n    parser.add_argument(\'--batch\', type=int, action=\'store\', default=32,\n                        help=\'the training batch size\')\n    parser.add_argument(\n        \'--ckpt_freq\', type=int, action=\'store\', default=3000,\n        help=\'number of update steps for checkpoint and validation\'\n    )\n    parser.add_argument(\'--patience\', type=int, action=\'store\', default=5,\n                        help=\'patience for early stopping\')\n\n    parser.add_argument(\'--debug\', action=\'store_true\',\n                        help=\'run in debugging mode\')\n    parser.add_argument(\'--no-cuda\', action=\'store_true\',\n                        help=\'disable GPU training\')\n    args = parser.parse_args()\n    args.bi = not args.no_bi\n    args.cuda = torch.cuda.is_available() and not args.no_cuda\n\n    main(args)\n'"
train_extractor_ml.py,4,"b'"""""" train extractor (ML)""""""\nimport argparse\nimport json\nimport os\nfrom os.path import join, exists\nimport pickle as pkl\n\nfrom cytoolz import compose\n\nimport torch\nfrom torch import optim\nfrom torch.nn import functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\n\nfrom model.extract import ExtractSumm, PtrExtractSumm\nfrom model.util import sequence_loss\nfrom training import get_basic_grad_fn, basic_validate\nfrom training import BasicPipeline, BasicTrainer\n\nfrom utils import PAD, UNK\nfrom utils import make_vocab, make_embedding\n\nfrom data.data import CnnDmDataset\nfrom data.batcher import coll_fn_extract, prepro_fn_extract\nfrom data.batcher import convert_batch_extract_ff, batchify_fn_extract_ff\nfrom data.batcher import convert_batch_extract_ptr, batchify_fn_extract_ptr\nfrom data.batcher import BucketedGenerater\n\n\nBUCKET_SIZE = 6400\n\ntry:\n    DATA_DIR = os.environ[\'DATA\']\nexcept KeyError:\n    print(\'please use environment variable to specify data directories\')\n\nclass ExtractDataset(CnnDmDataset):\n    """""" article sentences -> extraction indices\n    (dataset created by greedily matching ROUGE)\n    """"""\n    def __init__(self, split):\n        super().__init__(split, DATA_DIR)\n\n    def __getitem__(self, i):\n        js_data = super().__getitem__(i)\n        art_sents, extracts = js_data[\'article\'], js_data[\'extracted\']\n        return art_sents, extracts\n\n\ndef build_batchers(net_type, word2id, cuda, debug):\n    assert net_type in [\'ff\', \'rnn\']\n    prepro = prepro_fn_extract(args.max_word, args.max_sent)\n    def sort_key(sample):\n        src_sents, _ = sample\n        return len(src_sents)\n    batchify_fn = (batchify_fn_extract_ff if net_type == \'ff\'\n                   else batchify_fn_extract_ptr)\n    convert_batch = (convert_batch_extract_ff if net_type == \'ff\'\n                     else convert_batch_extract_ptr)\n    batchify = compose(batchify_fn(PAD, cuda=cuda),\n                       convert_batch(UNK, word2id))\n\n    train_loader = DataLoader(\n        ExtractDataset(\'train\'), batch_size=BUCKET_SIZE,\n        shuffle=not debug,\n        num_workers=4 if cuda and not debug else 0,\n        collate_fn=coll_fn_extract\n    )\n    train_batcher = BucketedGenerater(train_loader, prepro, sort_key, batchify,\n                                      single_run=False, fork=not debug)\n\n    val_loader = DataLoader(\n        ExtractDataset(\'val\'), batch_size=BUCKET_SIZE,\n        shuffle=False, num_workers=4 if cuda and not debug else 0,\n        collate_fn=coll_fn_extract\n    )\n    val_batcher = BucketedGenerater(val_loader, prepro, sort_key, batchify,\n                                    single_run=True, fork=not debug)\n    return train_batcher, val_batcher\n\n\ndef configure_net(net_type, vocab_size, emb_dim, conv_hidden,\n                  lstm_hidden, lstm_layer, bidirectional):\n    assert net_type in [\'ff\', \'rnn\']\n    net_args = {}\n    net_args[\'vocab_size\']    = vocab_size\n    net_args[\'emb_dim\']       = emb_dim\n    net_args[\'conv_hidden\']   = conv_hidden\n    net_args[\'lstm_hidden\']   = lstm_hidden\n    net_args[\'lstm_layer\']    = lstm_layer\n    net_args[\'bidirectional\'] = bidirectional\n\n    net = (ExtractSumm(**net_args) if net_type == \'ff\'\n           else PtrExtractSumm(**net_args))\n    return net, net_args\n\n\ndef configure_training(net_type, opt, lr, clip_grad, lr_decay, batch_size):\n    """""" supports Adam optimizer only""""""\n    assert opt in [\'adam\']\n    assert net_type in [\'ff\', \'rnn\']\n    opt_kwargs = {}\n    opt_kwargs[\'lr\'] = lr\n\n    train_params = {}\n    train_params[\'optimizer\']      = (opt, opt_kwargs)\n    train_params[\'clip_grad_norm\'] = clip_grad\n    train_params[\'batch_size\']     = batch_size\n    train_params[\'lr_decay\']       = lr_decay\n\n    if net_type == \'ff\':\n        criterion = lambda logit, target: F.binary_cross_entropy_with_logits(\n            logit, target, reduce=False)\n    else:\n        ce = lambda logit, target: F.cross_entropy(logit, target, reduce=False)\n        def criterion(logits, targets):\n            return sequence_loss(logits, targets, ce, pad_idx=-1)\n\n    return criterion, train_params\n\n\ndef main(args):\n    assert args.net_type in [\'ff\', \'rnn\']\n    # create data batcher, vocabulary\n    # batcher\n    with open(join(DATA_DIR, \'vocab_cnt.pkl\'), \'rb\') as f:\n        wc = pkl.load(f)\n    word2id = make_vocab(wc, args.vsize)\n    train_batcher, val_batcher = build_batchers(args.net_type, word2id,\n                                                args.cuda, args.debug)\n\n    # make net\n    net, net_args = configure_net(args.net_type,\n                                  len(word2id), args.emb_dim, args.conv_hidden,\n                                  args.lstm_hidden, args.lstm_layer, args.bi)\n    if args.w2v:\n        # NOTE: the pretrained embedding having the same dimension\n        #       as args.emb_dim should already be trained\n        embedding, _ = make_embedding(\n            {i: w for w, i in word2id.items()}, args.w2v)\n        net.set_embedding(embedding)\n\n    # configure training setting\n    criterion, train_params = configure_training(\n        args.net_type, \'adam\', args.lr, args.clip, args.decay, args.batch\n    )\n\n    # save experiment setting\n    if not exists(args.path):\n        os.makedirs(args.path)\n    with open(join(args.path, \'vocab.pkl\'), \'wb\') as f:\n        pkl.dump(word2id, f, pkl.HIGHEST_PROTOCOL)\n    meta = {}\n    meta[\'net\']           = \'ml_{}_extractor\'.format(args.net_type)\n    meta[\'net_args\']      = net_args\n    meta[\'traing_params\'] = train_params\n    with open(join(args.path, \'meta.json\'), \'w\') as f:\n        json.dump(meta, f, indent=4)\n\n    # prepare trainer\n    val_fn = basic_validate(net, criterion)\n    grad_fn = get_basic_grad_fn(net, args.clip)\n    optimizer = optim.Adam(net.parameters(), **train_params[\'optimizer\'][1])\n    scheduler = ReduceLROnPlateau(optimizer, \'min\', verbose=True,\n                                  factor=args.decay, min_lr=0,\n                                  patience=args.lr_p)\n\n    if args.cuda:\n        net = net.cuda()\n    pipeline = BasicPipeline(meta[\'net\'], net,\n                             train_batcher, val_batcher, args.batch, val_fn,\n                             criterion, optimizer, grad_fn)\n    trainer = BasicTrainer(pipeline, args.path,\n                           args.ckpt_freq, args.patience, scheduler)\n\n    print(\'start training with the following hyper-parameters:\')\n    print(meta)\n    trainer.train()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n        description=\'training of the feed-forward extractor (ff-ext, ML)\'\n    )\n    parser.add_argument(\'--path\', required=True, help=\'root of the model\')\n\n    # model options\n    parser.add_argument(\'--net-type\', action=\'store\', default=\'rnn\',\n                        help=\'model type of the extractor (ff/rnn)\')\n    parser.add_argument(\'--vsize\', type=int, action=\'store\', default=30000,\n                        help=\'vocabulary size\')\n    parser.add_argument(\'--emb_dim\', type=int, action=\'store\', default=128,\n                        help=\'the dimension of word embedding\')\n    parser.add_argument(\'--w2v\', action=\'store\',\n                        help=\'use pretrained word2vec embedding\')\n    parser.add_argument(\'--conv_hidden\', type=int, action=\'store\', default=100,\n                        help=\'the number of hidden units of Conv\')\n    parser.add_argument(\'--lstm_hidden\', type=int, action=\'store\', default=256,\n                        help=\'the number of hidden units of lSTM\')\n    parser.add_argument(\'--lstm_layer\', type=int, action=\'store\', default=1,\n                        help=\'the number of layers of LSTM Encoder\')\n    parser.add_argument(\'--no-bi\', action=\'store_true\',\n                        help=\'disable bidirectional LSTM encoder\')\n\n    # length limit\n    parser.add_argument(\'--max_word\', type=int, action=\'store\', default=100,\n                        help=\'maximun words in a single article sentence\')\n    parser.add_argument(\'--max_sent\', type=int, action=\'store\', default=60,\n                        help=\'maximun sentences in an article article\')\n    # training options\n    parser.add_argument(\'--lr\', type=float, action=\'store\', default=1e-3,\n                        help=\'learning rate\')\n    parser.add_argument(\'--decay\', type=float, action=\'store\', default=0.5,\n                        help=\'learning rate decay ratio\')\n    parser.add_argument(\'--lr_p\', type=int, action=\'store\', default=0,\n                        help=\'patience for learning rate decay\')\n    parser.add_argument(\'--clip\', type=float, action=\'store\', default=2.0,\n                        help=\'gradient clipping\')\n    parser.add_argument(\'--batch\', type=int, action=\'store\', default=32,\n                        help=\'the training batch size\')\n    parser.add_argument(\n        \'--ckpt_freq\', type=int, action=\'store\', default=3000,\n        help=\'number of update steps for checkpoint and validation\'\n    )\n    parser.add_argument(\'--patience\', type=int, action=\'store\', default=5,\n                        help=\'patience for early stopping\')\n\n    parser.add_argument(\'--debug\', action=\'store_true\',\n                        help=\'run in debugging mode\')\n    parser.add_argument(\'--no-cuda\', action=\'store_true\',\n                        help=\'disable GPU training\')\n    args = parser.parse_args()\n    args.bi = not args.no_bi\n    args.cuda = torch.cuda.is_available() and not args.no_cuda\n\n    main(args)\n'"
train_full_rl.py,4,"b'"""""" full training (train rnn-ext + abs + RL) """"""\nimport argparse\nimport json\nimport pickle as pkl\nimport os\nfrom os.path import join, exists\nfrom itertools import cycle\n\nfrom toolz.sandbox.core import unzip\nfrom cytoolz import identity\n\nimport torch\nfrom torch import optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\n\nfrom data.data import CnnDmDataset\nfrom data.batcher import tokenize\n\nfrom model.rl import ActorCritic\nfrom model.extract import PtrExtractSumm\n\nfrom training import BasicTrainer\nfrom rl import get_grad_fn\nfrom rl import A2CPipeline\nfrom decoding import load_best_ckpt\nfrom decoding import Abstractor, ArticleBatcher\nfrom metric import compute_rouge_l, compute_rouge_n\n\n\nMAX_ABS_LEN = 30\n\ntry:\n    DATA_DIR = os.environ[\'DATA\']\nexcept KeyError:\n    print(\'please use environment variable to specify data directories\')\n\n\nclass RLDataset(CnnDmDataset):\n    """""" get the article sentences only (for decoding use)""""""\n    def __init__(self, split):\n        super().__init__(split, DATA_DIR)\n\n    def __getitem__(self, i):\n        js_data = super().__getitem__(i)\n        art_sents = js_data[\'article\']\n        abs_sents = js_data[\'abstract\']\n        return art_sents, abs_sents\n\ndef load_ext_net(ext_dir):\n    ext_meta = json.load(open(join(ext_dir, \'meta.json\')))\n    assert ext_meta[\'net\'] == \'ml_rnn_extractor\'\n    ext_ckpt = load_best_ckpt(ext_dir)\n    ext_args = ext_meta[\'net_args\']\n    vocab = pkl.load(open(join(ext_dir, \'vocab.pkl\'), \'rb\'))\n    ext = PtrExtractSumm(**ext_args)\n    ext.load_state_dict(ext_ckpt)\n    return ext, vocab\n\n\ndef configure_net(abs_dir, ext_dir, cuda):\n    """""" load pretrained sub-modules and build the actor-critic network""""""\n    # load pretrained abstractor model\n    if abs_dir is not None:\n        abstractor = Abstractor(abs_dir, MAX_ABS_LEN, cuda)\n    else:\n        abstractor = identity\n\n    # load ML trained extractor net and buiild RL agent\n    extractor, agent_vocab = load_ext_net(ext_dir)\n    agent = ActorCritic(extractor._sent_enc,\n                        extractor._art_enc,\n                        extractor._extractor,\n                        ArticleBatcher(agent_vocab, cuda))\n    if cuda:\n        agent = agent.cuda()\n\n    net_args = {}\n    net_args[\'abstractor\'] = (None if abs_dir is None\n                              else json.load(open(join(abs_dir, \'meta.json\'))))\n    net_args[\'extractor\'] = json.load(open(join(ext_dir, \'meta.json\')))\n\n    return agent, agent_vocab, abstractor, net_args\n\n\ndef configure_training(opt, lr, clip_grad, lr_decay, batch_size,\n                       gamma, reward, stop_coeff, stop_reward):\n    assert opt in [\'adam\']\n    opt_kwargs = {}\n    opt_kwargs[\'lr\'] = lr\n\n    train_params = {}\n    train_params[\'optimizer\']      = (opt, opt_kwargs)\n    train_params[\'clip_grad_norm\'] = clip_grad\n    train_params[\'batch_size\']     = batch_size\n    train_params[\'lr_decay\']       = lr_decay\n    train_params[\'gamma\']          = gamma\n    train_params[\'reward\']         = reward\n    train_params[\'stop_coeff\']     = stop_coeff\n    train_params[\'stop_reward\']    = stop_reward\n\n    return train_params\n\ndef build_batchers(batch_size):\n    def coll(batch):\n        art_batch, abs_batch = unzip(batch)\n        art_sents = list(filter(bool, map(tokenize(None), art_batch)))\n        abs_sents = list(filter(bool, map(tokenize(None), abs_batch)))\n        return art_sents, abs_sents\n    loader = DataLoader(\n        RLDataset(\'train\'), batch_size=batch_size,\n        shuffle=True, num_workers=4,\n        collate_fn=coll\n    )\n    val_loader = DataLoader(\n        RLDataset(\'val\'), batch_size=batch_size,\n        shuffle=False, num_workers=4,\n        collate_fn=coll\n    )\n    return cycle(loader), val_loader\n\n\ndef train(args):\n    if not exists(args.path):\n        os.makedirs(args.path)\n\n    # make net\n    agent, agent_vocab, abstractor, net_args = configure_net(\n        args.abs_dir, args.ext_dir, args.cuda)\n\n    # configure training setting\n    assert args.stop > 0\n    train_params = configure_training(\n        \'adam\', args.lr, args.clip, args.decay, args.batch,\n        args.gamma, args.reward, args.stop, \'rouge-1\'\n    )\n    train_batcher, val_batcher = build_batchers(args.batch)\n    # TODO different reward\n    reward_fn = compute_rouge_l\n    stop_reward_fn = compute_rouge_n(n=1)\n\n    # save abstractor binary\n    if args.abs_dir is not None:\n        abs_ckpt = {}\n        abs_ckpt[\'state_dict\'] = load_best_ckpt(args.abs_dir)\n        abs_vocab = pkl.load(open(join(args.abs_dir, \'vocab.pkl\'), \'rb\'))\n        abs_dir = join(args.path, \'abstractor\')\n        os.makedirs(join(abs_dir, \'ckpt\'))\n        with open(join(abs_dir, \'meta.json\'), \'w\') as f:\n            json.dump(net_args[\'abstractor\'], f, indent=4)\n        torch.save(abs_ckpt, join(abs_dir, \'ckpt/ckpt-0-0\'))\n        with open(join(abs_dir, \'vocab.pkl\'), \'wb\') as f:\n            pkl.dump(abs_vocab, f)\n    # save configuration\n    meta = {}\n    meta[\'net\']           = \'rnn-ext_abs_rl\'\n    meta[\'net_args\']      = net_args\n    meta[\'train_params\']  = train_params\n    with open(join(args.path, \'meta.json\'), \'w\') as f:\n        json.dump(meta, f, indent=4)\n    with open(join(args.path, \'agent_vocab.pkl\'), \'wb\') as f:\n        pkl.dump(agent_vocab, f)\n\n    # prepare trainer\n    grad_fn = get_grad_fn(agent, args.clip)\n    optimizer = optim.Adam(agent.parameters(), **train_params[\'optimizer\'][1])\n    scheduler = ReduceLROnPlateau(optimizer, \'max\', verbose=True,\n                                  factor=args.decay, min_lr=0,\n                                  patience=args.lr_p)\n\n    pipeline = A2CPipeline(meta[\'net\'], agent, abstractor,\n                           train_batcher, val_batcher,\n                           optimizer, grad_fn,\n                           reward_fn, args.gamma,\n                           stop_reward_fn, args.stop)\n    trainer = BasicTrainer(pipeline, args.path,\n                           args.ckpt_freq, args.patience, scheduler,\n                           val_mode=\'score\')\n\n    print(\'start training with the following hyper-parameters:\')\n    print(meta)\n    trainer.train()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n        description=\'program to demo a Seq2Seq model\'\n    )\n    parser.add_argument(\'--path\', required=True, help=\'root of the model\')\n\n\n    # model options\n    parser.add_argument(\'--abs_dir\', action=\'store\',\n                        help=\'pretrained summarizer model root path\')\n    parser.add_argument(\'--ext_dir\', action=\'store\',\n                        help=\'root of the extractor model\')\n    parser.add_argument(\'--ckpt\', type=int, action=\'store\', default=None,\n                        help=\'ckeckpoint used decode\')\n\n    # training options\n    parser.add_argument(\'--reward\', action=\'store\', default=\'rouge-l\',\n                        help=\'reward function for RL\')\n    parser.add_argument(\'--lr\', type=float, action=\'store\', default=1e-4,\n                        help=\'learning rate\')\n    parser.add_argument(\'--decay\', type=float, action=\'store\', default=0.5,\n                        help=\'learning rate decay ratio\')\n    parser.add_argument(\'--lr_p\', type=int, action=\'store\', default=0,\n                        help=\'patience for learning rate decay\')\n    parser.add_argument(\'--gamma\', type=float, action=\'store\', default=0.95,\n                        help=\'discount factor of RL\')\n    parser.add_argument(\'--stop\', type=float, action=\'store\', default=1.0,\n                        help=\'stop coefficient for rouge-1\')\n    parser.add_argument(\'--clip\', type=float, action=\'store\', default=2.0,\n                        help=\'gradient clipping\')\n    parser.add_argument(\'--batch\', type=int, action=\'store\', default=32,\n                        help=\'the training batch size\')\n    parser.add_argument(\n        \'--ckpt_freq\', type=int, action=\'store\', default=1000,\n        help=\'number of update steps for checkpoint and validation\'\n    )\n    parser.add_argument(\'--patience\', type=int, action=\'store\', default=3,\n                        help=\'patience for early stopping\')\n    parser.add_argument(\'--no-cuda\', action=\'store_true\',\n                        help=\'disable GPU training\')\n    args = parser.parse_args()\n    args.cuda = torch.cuda.is_available() and not args.no_cuda\n\n    train(args)\n'"
train_word2vec.py,0,"b'"""""" pretrain a word2vec on the corpus""""""\nimport argparse\nimport json\nimport logging\nimport os\nfrom os.path import join, exists\nfrom time import time\nfrom datetime import timedelta\n\nfrom cytoolz import concatv\nimport gensim\n\nfrom utils import count_data\n\n\ntry:\n    DATA_DIR = os.environ[\'DATA\']\nexcept KeyError:\n    print(\'please use environment variable to specify data directories\')\n\nclass Sentences(object):\n    """""" needed for gensim word2vec training""""""\n    def __init__(self):\n        self._path = join(DATA_DIR, \'train\')\n        self._n_data = count_data(self._path)\n\n    def __iter__(self):\n        for i in range(self._n_data):\n            with open(join(self._path, \'{}.json\'.format(i))) as f:\n                data = json.loads(f.read())\n            for s in concatv(data[\'article\'], data[\'abstract\']):\n                yield [\'<s>\'] + s.lower().split() + [r\'<\\s>\']\n\n\ndef main(args):\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\',\n                        level=logging.INFO)\n    start = time()\n    save_dir = args.path\n    if not exists(save_dir):\n        os.makedirs(save_dir)\n\n    sentences = Sentences()\n    model = gensim.models.Word2Vec(\n        size=args.dim, min_count=5, workers=16, sg=1)\n    model.build_vocab(sentences)\n    print(\'vocab built in {}\'.format(timedelta(seconds=time()-start)))\n    model.train(sentences,\n                total_examples=model.corpus_count, epochs=model.iter)\n\n    model.save(join(save_dir, \'word2vec.{}d.{}k.bin\'.format(\n        args.dim, len(model.wv.vocab)//1000)))\n    model.wv.save_word2vec_format(join(\n        save_dir,\n        \'word2vec.{}d.{}k.w2v\'.format(args.dim, len(model.wv.vocab)//1000)\n    ))\n\n    print(\'word2vec trained in {}\'.format(timedelta(seconds=time()-start)))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n        description=\'train word2vec embedding used for model initialization\'\n    )\n    parser.add_argument(\'--path\', required=True, help=\'root of the model\')\n    parser.add_argument(\'--dim\', action=\'store\', type=int, default=128)\n    args = parser.parse_args()\n\n    main(args)\n'"
training.py,4,"b'"""""" module providing basic training utilities""""""\nimport os\nfrom os.path import join\nfrom time import time\nfrom datetime import timedelta\nfrom itertools import starmap\n\nfrom cytoolz import curry, reduce\n\nimport torch\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport tensorboardX\n\n\ndef get_basic_grad_fn(net, clip_grad, max_grad=1e2):\n    def f():\n        grad_norm = clip_grad_norm_(\n            [p for p in net.parameters() if p.requires_grad], clip_grad)\n        grad_norm = grad_norm.item()\n        if max_grad is not None and grad_norm >= max_grad:\n            print(\'WARNING: Exploding Gradients {:.2f}\'.format(grad_norm))\n            grad_norm = max_grad\n        grad_log = {}\n        grad_log[\'grad_norm\'] = grad_norm\n        return grad_log\n    return f\n\n@curry\ndef compute_loss(net, criterion, fw_args, loss_args):\n    loss = criterion(*((net(*fw_args),) + loss_args))\n    return loss\n\n@curry\ndef val_step(loss_step, fw_args, loss_args):\n    loss = loss_step(fw_args, loss_args)\n    return loss.size(0), loss.sum().item()\n\n@curry\ndef basic_validate(net, criterion, val_batches):\n    print(\'running validation ... \', end=\'\')\n    net.eval()\n    start = time()\n    with torch.no_grad():\n        validate_fn = val_step(compute_loss(net, criterion))\n        n_data, tot_loss = reduce(\n            lambda a, b: (a[0]+b[0], a[1]+b[1]),\n            starmap(validate_fn, val_batches),\n            (0, 0)\n        )\n    val_loss = tot_loss / n_data\n    print(\n        \'validation finished in {}                                    \'.format(\n            timedelta(seconds=int(time()-start)))\n    )\n    print(\'validation loss: {:.4f} ... \'.format(val_loss))\n    return {\'loss\': val_loss}\n\n\nclass BasicPipeline(object):\n    def __init__(self, name, net,\n                 train_batcher, val_batcher, batch_size,\n                 val_fn, criterion, optim, grad_fn=None):\n        self.name = name\n        self._net = net\n        self._train_batcher = train_batcher\n        self._val_batcher = val_batcher\n        self._criterion = criterion\n        self._opt = optim\n        # grad_fn is calleble without input args that modifyies gradient\n        # it should return a dictionary of logging values\n        self._grad_fn = grad_fn\n        self._val_fn = val_fn\n\n        self._n_epoch = 0  # epoch not very useful?\n        self._batch_size = batch_size\n        self._batches = self.batches()\n\n    def batches(self):\n        while True:\n            for fw_args, bw_args in self._train_batcher(self._batch_size):\n                yield fw_args, bw_args\n            self._n_epoch += 1\n\n    def get_loss_args(self, net_out, bw_args):\n        if isinstance(net_out, tuple):\n            loss_args = net_out + bw_args\n        else:\n            loss_args = (net_out, ) + bw_args\n        return loss_args\n\n    def train_step(self):\n        # forward pass of model\n        self._net.train()\n        fw_args, bw_args = next(self._batches)\n        net_out = self._net(*fw_args)\n\n        # get logs and output for logging, backward\n        log_dict = {}\n        loss_args = self.get_loss_args(net_out, bw_args)\n\n        # backward and update ( and optional gradient monitoring )\n        loss = self._criterion(*loss_args).mean()\n        loss.backward()\n        log_dict[\'loss\'] = loss.item()\n        if self._grad_fn is not None:\n            log_dict.update(self._grad_fn())\n        self._opt.step()\n        self._net.zero_grad()\n\n        return log_dict\n\n    def validate(self):\n        return self._val_fn(self._val_batcher(self._batch_size))\n\n    def checkpoint(self, save_path, step, val_metric=None):\n        save_dict = {}\n        if val_metric is not None:\n            name = \'ckpt-{:6f}-{}\'.format(val_metric, step)\n            save_dict[\'val_metric\'] = val_metric\n        else:\n            name = \'ckpt-{}\'.format(step)\n\n        save_dict[\'state_dict\'] = self._net.state_dict()\n        save_dict[\'optimizer\'] = self._opt.state_dict()\n        torch.save(save_dict, join(save_path, name))\n\n    def terminate(self):\n        self._train_batcher.terminate()\n        self._val_batcher.terminate()\n\n\nclass BasicTrainer(object):\n    """""" Basic trainer with minimal function and early stopping""""""\n    def __init__(self, pipeline, save_dir, ckpt_freq, patience,\n                 scheduler=None, val_mode=\'loss\'):\n        assert isinstance(pipeline, BasicPipeline)\n        assert val_mode in [\'loss\', \'score\']\n        self._pipeline = pipeline\n        self._save_dir = save_dir\n        self._logger = tensorboardX.SummaryWriter(join(save_dir, \'log\'))\n        os.makedirs(join(save_dir, \'ckpt\'))\n\n        self._ckpt_freq = ckpt_freq\n        self._patience = patience\n        self._sched = scheduler\n        self._val_mode = val_mode\n\n        self._step = 0\n        self._running_loss = None\n        # state vars for early stopping\n        self._current_p = 0\n        self._best_val = None\n\n    def log(self, log_dict):\n        loss = log_dict[\'loss\'] if \'loss\' in log_dict else log_dict[\'reward\']\n        if self._running_loss is not None:\n            self._running_loss = 0.99*self._running_loss + 0.01*loss\n        else:\n            self._running_loss = loss\n        print(\'train step: {}, {}: {:.4f}\\r\'.format(\n            self._step,\n            \'loss\' if \'loss\' in log_dict else \'reward\',\n            self._running_loss), end=\'\')\n        for key, value in log_dict.items():\n            self._logger.add_scalar(\n                \'{}_{}\'.format(key, self._pipeline.name), value, self._step)\n\n    def validate(self):\n        print()\n        val_log = self._pipeline.validate()\n        for key, value in val_log.items():\n            self._logger.add_scalar(\n                \'val_{}_{}\'.format(key, self._pipeline.name),\n                value, self._step\n            )\n        if \'reward\' in val_log:\n            val_metric = val_log[\'reward\']\n        else:\n            val_metric = (val_log[\'loss\'] if self._val_mode == \'loss\'\n                          else val_log[\'score\'])\n        return val_metric\n\n    def checkpoint(self):\n        val_metric = self.validate()\n        self._pipeline.checkpoint(\n            join(self._save_dir, \'ckpt\'), self._step, val_metric)\n        if isinstance(self._sched, ReduceLROnPlateau):\n            self._sched.step(val_metric)\n        else:\n            self._sched.step()\n        stop = self.check_stop(val_metric)\n        return stop\n\n    def check_stop(self, val_metric):\n        if self._best_val is None:\n            self._best_val = val_metric\n        elif ((val_metric < self._best_val and self._val_mode == \'loss\')\n              or (val_metric > self._best_val and self._val_mode == \'score\')):\n            self._current_p = 0\n            self._best_val = val_metric\n        else:\n            self._current_p += 1\n        return self._current_p >= self._patience\n\n    def train(self):\n        try:\n            start = time()\n            print(\'Start training\')\n            while True:\n                log_dict = self._pipeline.train_step()\n                self._step += 1\n                self.log(log_dict)\n\n                if self._step % self._ckpt_freq == 0:\n                    stop = self.checkpoint()\n                    if stop:\n                        break\n            print(\'Training finised in \', timedelta(seconds=time()-start))\n        finally:\n            self._pipeline.terminate()\n'"
utils.py,4,"b'"""""" utility functions""""""\nimport re\nimport os\nfrom os.path import basename\n\nimport gensim\nimport torch\nfrom torch import nn\n\n\ndef count_data(path):\n    """""" count number of data in the given path""""""\n    matcher = re.compile(r\'[0-9]+\\.json\')\n    match = lambda name: bool(matcher.match(name))\n    names = os.listdir(path)\n    n_data = len(list(filter(match, names)))\n    return n_data\n\n\nPAD = 0\nUNK = 1\nSTART = 2\nEND = 3\ndef make_vocab(wc, vocab_size):\n    word2id, id2word = {}, {}\n    word2id[\'<pad>\'] = PAD\n    word2id[\'<unk>\'] = UNK\n    word2id[\'<start>\'] = START\n    word2id[\'<end>\'] = END\n    for i, (w, _) in enumerate(wc.most_common(vocab_size), 4):\n        word2id[w] = i\n    return word2id\n\n\ndef make_embedding(id2word, w2v_file, initializer=None):\n    attrs = basename(w2v_file).split(\'.\')  #word2vec.{dim}d.{vsize}k.bin\n    w2v = gensim.models.Word2Vec.load(w2v_file).wv\n    vocab_size = len(id2word)\n    emb_dim = int(attrs[-3][:-1])\n    embedding = nn.Embedding(vocab_size, emb_dim).weight\n    if initializer is not None:\n        initializer(embedding)\n\n    oovs = []\n    with torch.no_grad():\n        for i in range(len(id2word)):\n            # NOTE: id2word can be list or dict\n            if i == START:\n                embedding[i, :] = torch.Tensor(w2v[\'<s>\'])\n            elif i == END:\n                embedding[i, :] = torch.Tensor(w2v[r\'<\\s>\'])\n            elif id2word[i] in w2v:\n                embedding[i, :] = torch.Tensor(w2v[id2word[i]])\n            else:\n                oovs.append(i)\n    return embedding, oovs\n'"
data/__init__.py,0,b''
data/batcher.py,3,"b'"""""" batching """"""\nimport random\nfrom collections import defaultdict\n\nfrom toolz.sandbox import unzip\nfrom cytoolz import curry, concat, compose\nfrom cytoolz import curried\n\nimport torch\nimport torch.multiprocessing as mp\n\n\n# Batching functions\ndef coll_fn(data):\n    source_lists, target_lists = unzip(data)\n    # NOTE: independent filtering works because\n    #       source and targets are matched properly by the Dataset\n    sources = list(filter(bool, concat(source_lists)))\n    targets = list(filter(bool, concat(target_lists)))\n    assert all(sources) and all(targets)\n    return sources, targets\n\ndef coll_fn_extract(data):\n    def is_good_data(d):\n        """""" make sure data is not empty""""""\n        source_sents, extracts = d\n        return source_sents and extracts\n    batch = list(filter(is_good_data, data))\n    assert all(map(is_good_data, batch))\n    return batch\n\n@curry\ndef tokenize(max_len, texts):\n    return [t.lower().split()[:max_len] for t in texts]\n\ndef conver2id(unk, word2id, words_list):\n    word2id = defaultdict(lambda: unk, word2id)\n    return [[word2id[w] for w in words] for words in words_list]\n\n@curry\ndef prepro_fn(max_src_len, max_tgt_len, batch):\n    sources, targets = batch\n    sources = tokenize(max_src_len, sources)\n    targets = tokenize(max_tgt_len, targets)\n    batch = list(zip(sources, targets))\n    return batch\n\n@curry\ndef prepro_fn_extract(max_src_len, max_src_num, batch):\n    def prepro_one(sample):\n        source_sents, extracts = sample\n        tokenized_sents = tokenize(max_src_len, source_sents)[:max_src_num]\n        cleaned_extracts = list(filter(lambda e: e < len(tokenized_sents),\n                                       extracts))\n        return tokenized_sents, cleaned_extracts\n    batch = list(map(prepro_one, batch))\n    return batch\n\n@curry\ndef convert_batch(unk, word2id, batch):\n    sources, targets = unzip(batch)\n    sources = conver2id(unk, word2id, sources)\n    targets = conver2id(unk, word2id, targets)\n    batch = list(zip(sources, targets))\n    return batch\n\n@curry\ndef convert_batch_copy(unk, word2id, batch):\n    sources, targets = map(list, unzip(batch))\n    ext_word2id = dict(word2id)\n    for source in sources:\n        for word in source:\n            if word not in ext_word2id:\n                ext_word2id[word] = len(ext_word2id)\n    src_exts = conver2id(unk, ext_word2id, sources)\n    sources = conver2id(unk, word2id, sources)\n    tar_ins = conver2id(unk, word2id, targets)\n    targets = conver2id(unk, ext_word2id, targets)\n    batch = list(zip(sources, src_exts, tar_ins, targets))\n    return batch\n\n@curry\ndef convert_batch_extract_ptr(unk, word2id, batch):\n    def convert_one(sample):\n        source_sents, extracts = sample\n        id_sents = conver2id(unk, word2id, source_sents)\n        return id_sents, extracts\n    batch = list(map(convert_one, batch))\n    return batch\n\n@curry\ndef convert_batch_extract_ff(unk, word2id, batch):\n    def convert_one(sample):\n        source_sents, extracts = sample\n        id_sents = conver2id(unk, word2id, source_sents)\n        binary_extracts = [0] * len(source_sents)\n        for ext in extracts:\n            binary_extracts[ext] = 1\n        return id_sents, binary_extracts\n    batch = list(map(convert_one, batch))\n    return batch\n\n\n@curry\ndef pad_batch_tensorize(inputs, pad, cuda=True):\n    """"""pad_batch_tensorize\n\n    :param inputs: List of size B containing torch tensors of shape [T, ...]\n    :type inputs: List[np.ndarray]\n    :rtype: TorchTensor of size (B, T, ...)\n    """"""\n    tensor_type = torch.cuda.LongTensor if cuda else torch.LongTensor\n    batch_size = len(inputs)\n    max_len = max(len(ids) for ids in inputs)\n    tensor_shape = (batch_size, max_len)\n    tensor = tensor_type(*tensor_shape)\n    tensor.fill_(pad)\n    for i, ids in enumerate(inputs):\n        tensor[i, :len(ids)] = tensor_type(ids)\n    return tensor\n\n@curry\ndef batchify_fn(pad, start, end, data, cuda=True):\n    sources, targets = tuple(map(list, unzip(data)))\n\n    src_lens = [len(src) for src in sources]\n    tar_ins = [[start] + tgt for tgt in targets]\n    targets = [tgt + [end] for tgt in targets]\n\n    source = pad_batch_tensorize(sources, pad, cuda)\n    tar_in = pad_batch_tensorize(tar_ins, pad, cuda)\n    target = pad_batch_tensorize(targets, pad, cuda)\n\n    fw_args = (source, src_lens, tar_in)\n    loss_args = (target, )\n    return fw_args, loss_args\n\n\n@curry\ndef batchify_fn_copy(pad, start, end, data, cuda=True):\n    sources, ext_srcs, tar_ins, targets = tuple(map(list, unzip(data)))\n\n    src_lens = [len(src) for src in sources]\n    sources = [src for src in sources]\n    ext_srcs = [ext for ext in ext_srcs]\n\n    tar_ins = [[start] + tgt for tgt in tar_ins]\n    targets = [tgt + [end] for tgt in targets]\n\n    source = pad_batch_tensorize(sources, pad, cuda)\n    tar_in = pad_batch_tensorize(tar_ins, pad, cuda)\n    target = pad_batch_tensorize(targets, pad, cuda)\n    ext_src = pad_batch_tensorize(ext_srcs, pad, cuda)\n\n    ext_vsize = ext_src.max().item() + 1\n    fw_args = (source, src_lens, tar_in, ext_src, ext_vsize)\n    loss_args = (target, )\n    return fw_args, loss_args\n\n\n@curry\ndef batchify_fn_extract_ptr(pad, data, cuda=True):\n    source_lists, targets = tuple(map(list, unzip(data)))\n\n    src_nums = list(map(len, source_lists))\n    sources = list(map(pad_batch_tensorize(pad=pad, cuda=cuda), source_lists))\n\n    # PAD is -1 (dummy extraction index) for using sequence loss\n    target = pad_batch_tensorize(targets, pad=-1, cuda=cuda)\n    remove_last = lambda tgt: tgt[:-1]\n    tar_in = pad_batch_tensorize(\n        list(map(remove_last, targets)),\n        pad=-0, cuda=cuda # use 0 here for feeding first conv sentence repr.\n    )\n\n    fw_args = (sources, src_nums, tar_in)\n    loss_args = (target, )\n    return fw_args, loss_args\n\n@curry\ndef batchify_fn_extract_ff(pad, data, cuda=True):\n    source_lists, targets = tuple(map(list, unzip(data)))\n\n    src_nums = list(map(len, source_lists))\n    sources = list(map(pad_batch_tensorize(pad=pad, cuda=cuda), source_lists))\n\n    tensor_type = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n    target = tensor_type(list(concat(targets)))\n\n    fw_args = (sources, src_nums)\n    loss_args = (target, )\n    return fw_args, loss_args\n\n\ndef _batch2q(loader, prepro, q, single_run=True):\n    epoch = 0\n    while True:\n        for batch in loader:\n            q.put(prepro(batch))\n        if single_run:\n            break\n        epoch += 1\n        q.put(epoch)\n    q.put(None)\n\nclass BucketedGenerater(object):\n    def __init__(self, loader, prepro,\n                 sort_key, batchify,\n                 single_run=True, queue_size=8, fork=True):\n        self._loader = loader\n        self._prepro = prepro\n        self._sort_key = sort_key\n        self._batchify = batchify\n        self._single_run = single_run\n        if fork:\n            ctx = mp.get_context(\'forkserver\')\n            self._queue = ctx.Queue(queue_size)\n        else:\n            # for easier debugging\n            self._queue = None\n        self._process = None\n\n    def __call__(self, batch_size: int):\n        def get_batches(hyper_batch):\n            indexes = list(range(0, len(hyper_batch), batch_size))\n            if not self._single_run:\n                # random shuffle for training batches\n                random.shuffle(hyper_batch)\n                random.shuffle(indexes)\n            hyper_batch.sort(key=self._sort_key)\n            for i in indexes:\n                batch = self._batchify(hyper_batch[i:i+batch_size])\n                yield batch\n\n        if self._queue is not None:\n            ctx = mp.get_context(\'forkserver\')\n            self._process = ctx.Process(\n                target=_batch2q,\n                args=(self._loader, self._prepro,\n                      self._queue, self._single_run)\n            )\n            self._process.start()\n            while True:\n                d = self._queue.get()\n                if d is None:\n                    break\n                if isinstance(d, int):\n                    print(\'\\nepoch {} done\'.format(d))\n                    continue\n                yield from get_batches(d)\n            self._process.join()\n        else:\n            i = 0\n            while True:\n                for batch in self._loader:\n                    yield from get_batches(self._prepro(batch))\n                if self._single_run:\n                    break\n                i += 1\n                print(\'\\nepoch {} done\'.format(i))\n\n    def terminate(self):\n        if self._process is not None:\n            self._process.terminate()\n            self._process.join()\n'"
data/data.py,1,"b'"""""" CNN/DM dataset""""""\nimport json\nimport re\nimport os\nfrom os.path import join\n\nfrom torch.utils.data import Dataset\n\n\nclass CnnDmDataset(Dataset):\n    def __init__(self, split: str, path: str) -> None:\n        assert split in [\'train\', \'val\', \'test\']\n        self._data_path = join(path, split)\n        self._n_data = _count_data(self._data_path)\n\n    def __len__(self) -> int:\n        return self._n_data\n\n    def __getitem__(self, i: int):\n        with open(join(self._data_path, \'{}.json\'.format(i))) as f:\n            js = json.loads(f.read())\n        return js\n\n\ndef _count_data(path):\n    """""" count number of data in the given path""""""\n    matcher = re.compile(r\'[0-9]+\\.json\')\n    match = lambda name: bool(matcher.match(name))\n    names = os.listdir(path)\n    n_data = len(list(filter(match, names)))\n    return n_data\n'"
model/__init__.py,0,b''
model/attention.py,1,"b'"""""" attention functions """"""\nfrom torch.nn import functional as F\n\n\ndef dot_attention_score(key, query):\n    """"""[B, Tk, D], [(Bs), B, Tq, D] -> [(Bs), B, Tq, Tk]""""""\n    return query.matmul(key.transpose(1, 2))\n\ndef prob_normalize(score, mask):\n    """""" [(...), T]\n    user should handle mask shape""""""\n    score = score.masked_fill(mask == 0, -1e18)\n    norm_score = F.softmax(score, dim=-1)\n    return norm_score\n\ndef attention_aggregate(value, score):\n    """"""[B, Tv, D], [(Bs), B, Tq, Tv] -> [(Bs), B, Tq, D]""""""\n    output = score.matmul(value)\n    return output\n\n\ndef step_attention(query, key, value, mem_mask=None):\n    """""" query[(Bs), B, D], key[B, T, D], value[B, T, D]""""""\n    score = dot_attention_score(key, query.unsqueeze(-2))\n    if mem_mask is None:\n        norm_score = F.softmax(score, dim=-1)\n    else:\n        norm_score = prob_normalize(score, mem_mask)\n    output = attention_aggregate(value, norm_score)\n    return output.squeeze(-2), norm_score.squeeze(-2)\n'"
model/beam_search.py,2,"b'"""""" beam-search utilities""""""\nfrom collections import Counter\n\nfrom cytoolz import concat\n\nimport torch\n\n\nclass _Hypothesis(object):\n    def __init__(self, sequence, logprob, hists, attns=[]):\n        """"""\n        seqence: list of int tokens\n        logprob: current log probability\n        hists: history of prevous convolution list(n_layers)/\n               prev_states and output of lstm ((H, C), out)\n        """"""\n        self.sequence = sequence\n        self.logprob = logprob\n        self.hists = hists\n        self.attns = attns  # for unk replacement\n\n    def extend_k(self, topk, logprobs, hists, attn=None, diverse=1.0):\n        if attn is None:\n            attns = []\n        else:\n            attns = self.attns + [attn]\n        return [_Hypothesis(self.sequence+[t.item()],\n                            self.logprob+lp.item()-diverse*i, hists, attns)\n                for i, (t, lp) in enumerate(zip(topk, logprobs))]\n\n    def __lt__(self, other):\n        return (other.logprob/len(other.sequence)\n                < self.logprob/len(self.sequence))\n\n\ndef init_beam(start, hists):\n    """""" get a initial beam to start beam search""""""\n    return [_Hypothesis([start], 0, hists)]\n\n\ndef create_beam(tok, lp, hists):\n    """""" initailiza a beam with top k token""""""\n    k = tok.size(0)\n    return [_Hypothesis([tok[i].item()], lp[i].item(), hists)\n            for i in range(k)]\n\n\ndef pack_beam(hyps, device):\n    """"""pack a list of hypothesis to decoder input batches""""""\n    token = torch.LongTensor([h.sequence[-1] for h in hyps])\n\n    hists = tuple(torch.stack([hyp.hists[i] for hyp in hyps], dim=d)\n                  for i, d in enumerate([1, 1, 0]))\n    token = token.to(device)\n    states = ((hists[0], hists[1]), hists[2])\n    return token, states\n\n\ndef next_search_beam(beam, beam_size, finished,\n                     end, topk, lp, hists, attn=None, diverse=1.0):\n    """"""generate the next beam(K-best hyps)""""""\n    topks, lps, hists_list, attns = _unpack_topk(topk, lp, hists, attn)\n    hyps_lists = [h.extend_k(topks[i], lps[i],\n                             hists_list[i], attns[i], diverse)\n                  for i, h in enumerate(beam)]\n    hyps = list(concat(hyps_lists))\n    finished, beam = _clean_beam(finished, hyps, end, beam_size)\n\n    return finished, beam\n\n\ndef best_sequence(finished, beam=None):\n    """""" return the sequence with the highest prob(normalized by length)""""""\n    if beam is None:  # not empty\n        best_beam = finished[0]\n    else:\n        if finished and beam[0] < finished[0]:\n            best_beam = finished[0]\n        else:\n            best_beam = beam[0]\n\n    best_seq = best_beam.sequence[1:]\n    if best_beam.attns:\n        return best_seq, best_beam.attns\n    else:\n        return best_seq\n\n\ndef _unpack_topk(topk, lp, hists, attn=None):\n    """"""unpack the decoder output""""""\n    beam, _ = topk.size()\n    topks = [t for t in topk]\n    lps = [l for l in lp]\n    k_hists = [(hists[0][:, i, :], hists[1][:, i, :], hists[2][i, :])\n               for i in range(beam)]\n\n    if attn is None:\n        return topks, lps, k_hists\n    else:\n        attns = [attn[i] for i in range(beam)]\n        return topks, lps, k_hists, attns\n\n\ndef _clean_beam(finished, beam, end_tok, beam_size, remove_tri=True):\n    """""" remove completed sequence from beam """"""\n    new_beam = []\n    for h in sorted(beam, reverse=True,\n                    key=lambda h: h.logprob/len(h.sequence)):\n        if remove_tri and _has_repeat_tri(h.sequence):\n            h.logprob = -1e9\n        if h.sequence[-1] == end_tok:\n            finished_hyp = _Hypothesis(h.sequence[:-1], # remove EOS\n                                       h.logprob, h.hists, h.attns)\n            finished.append(finished_hyp)\n        else:\n            new_beam.append(h)\n        if len(new_beam) == beam_size:\n            break\n    else:\n        # ensure beam size\n        while len(new_beam) < beam_size:\n            new_beam.append(new_beam[0])\n\n    finished = sorted(finished, reverse=True,\n                      key=lambda h: h.logprob/len(h.sequence))\n    return finished, new_beam\n\n\ndef _has_repeat_tri(grams):\n    tri_grams = [tuple(grams[i:i+3]) for i in range(len(grams)-2)]\n    cnt = Counter(tri_grams)\n    return not all((cnt[g] <= 1 for g in cnt))\n'"
model/copy_summ.py,30,"b'import torch\nfrom torch import nn\nfrom torch.nn import init\nfrom torch.nn import functional as F\n\nfrom .attention import step_attention\nfrom .util import len_mask\nfrom .summ import Seq2SeqSumm, AttentionalLSTMDecoder\nfrom . import beam_search as bs\n\n\nINIT = 1e-2\n\n\nclass _CopyLinear(nn.Module):\n    def __init__(self, context_dim, state_dim, input_dim, bias=True):\n        super().__init__()\n        self._v_c = nn.Parameter(torch.Tensor(context_dim))\n        self._v_s = nn.Parameter(torch.Tensor(state_dim))\n        self._v_i = nn.Parameter(torch.Tensor(input_dim))\n        init.uniform_(self._v_c, -INIT, INIT)\n        init.uniform_(self._v_s, -INIT, INIT)\n        init.uniform_(self._v_i, -INIT, INIT)\n        if bias:\n            self._b = nn.Parameter(torch.zeros(1))\n        else:\n            self.register_parameter(None, \'_b\')\n\n    def forward(self, context, state, input_):\n        output = (torch.matmul(context, self._v_c.unsqueeze(1))\n                  + torch.matmul(state, self._v_s.unsqueeze(1))\n                  + torch.matmul(input_, self._v_i.unsqueeze(1)))\n        if self._b is not None:\n            output = output + self._b.unsqueeze(0)\n        return output\n\n\nclass CopySumm(Seq2SeqSumm):\n    def __init__(self, vocab_size, emb_dim,\n                 n_hidden, bidirectional, n_layer, dropout=0.0):\n        super().__init__(vocab_size, emb_dim,\n                         n_hidden, bidirectional, n_layer, dropout)\n        self._copy = _CopyLinear(n_hidden, n_hidden, 2*emb_dim)\n        self._decoder = CopyLSTMDecoder(\n            self._copy, self._embedding, self._dec_lstm,\n            self._attn_wq, self._projection\n        )\n\n    def forward(self, article, art_lens, abstract, extend_art, extend_vsize):\n        attention, init_dec_states = self.encode(article, art_lens)\n        mask = len_mask(art_lens, attention.device).unsqueeze(-2)\n        logit = self._decoder(\n            (attention, mask, extend_art, extend_vsize),\n            init_dec_states, abstract\n        )\n        return logit\n\n    def batch_decode(self, article, art_lens, extend_art, extend_vsize,\n                     go, eos, unk, max_len):\n        """""" greedy decode support batching""""""\n        batch_size = len(art_lens)\n        vsize = self._embedding.num_embeddings\n        attention, init_dec_states = self.encode(article, art_lens)\n        mask = len_mask(art_lens, attention.device).unsqueeze(-2)\n        attention = (attention, mask, extend_art, extend_vsize)\n        tok = torch.LongTensor([go]*batch_size).to(article.device)\n        outputs = []\n        attns = []\n        states = init_dec_states\n        for i in range(max_len):\n            tok, states, attn_score = self._decoder.decode_step(\n                tok, states, attention)\n            attns.append(attn_score)\n            outputs.append(tok[:, 0].clone())\n            tok.masked_fill_(tok >= vsize, unk)\n        return outputs, attns\n\n    def decode(self, article, extend_art, extend_vsize, go, eos, unk, max_len):\n        vsize = self._embedding.num_embeddings\n        attention, init_dec_states = self.encode(article)\n        attention = (attention, None, extend_art, extend_vsize)\n        tok = torch.LongTensor([go]).to(article.device)\n        outputs = []\n        attns = []\n        states = init_dec_states\n        for i in range(max_len):\n            tok, states, attn_score = self._decoder.decode_step(\n                tok, states, attention)\n            if tok[0, 0].item() == eos:\n                break\n            outputs.append(tok[0, 0].item())\n            attns.append(attn_score.squeeze(0))\n            if tok[0, 0].item() >= vsize:\n                tok[0, 0] = unk\n        return outputs, attns\n\n    def batched_beamsearch(self, article, art_lens,\n                           extend_art, extend_vsize,\n                           go, eos, unk, max_len, beam_size, diverse=1.0):\n        batch_size = len(art_lens)\n        vsize = self._embedding.num_embeddings\n        attention, init_dec_states = self.encode(article, art_lens)\n        mask = len_mask(art_lens, attention.device).unsqueeze(-2)\n        all_attention = (attention, mask, extend_art, extend_vsize)\n        attention = all_attention\n        (h, c), prev = init_dec_states\n        all_beams = [bs.init_beam(go, (h[:, i, :], c[:, i, :], prev[i]))\n                     for i in range(batch_size)]\n        finished_beams = [[] for _ in range(batch_size)]\n        outputs = [None for _ in range(batch_size)]\n        for t in range(max_len):\n            toks = []\n            all_states = []\n            for beam in filter(bool, all_beams):\n                token, states = bs.pack_beam(beam, article.device)\n                toks.append(token)\n                all_states.append(states)\n            token = torch.stack(toks, dim=1)\n            states = ((torch.stack([h for (h, _), _ in all_states], dim=2),\n                       torch.stack([c for (_, c), _ in all_states], dim=2)),\n                      torch.stack([prev for _, prev in all_states], dim=1))\n            token.masked_fill_(token >= vsize, unk)\n\n            topk, lp, states, attn_score = self._decoder.topk_step(\n                token, states, attention, beam_size)\n\n            batch_i = 0\n            for i, (beam, finished) in enumerate(zip(all_beams,\n                                                     finished_beams)):\n                if not beam:\n                    continue\n                finished, new_beam = bs.next_search_beam(\n                    beam, beam_size, finished, eos,\n                    topk[:, batch_i, :], lp[:, batch_i, :],\n                    (states[0][0][:, :, batch_i, :],\n                     states[0][1][:, :, batch_i, :],\n                     states[1][:, batch_i, :]),\n                    attn_score[:, batch_i, :],\n                    diverse\n                )\n                batch_i += 1\n                if len(finished) >= beam_size:\n                    all_beams[i] = []\n                    outputs[i] = finished[:beam_size]\n                    # exclude finished inputs\n                    (attention, mask, extend_art, extend_vsize\n                    ) = all_attention\n                    masks = [mask[j] for j, o in enumerate(outputs)\n                             if o is None]\n                    ind = [j for j, o in enumerate(outputs) if o is None]\n                    ind = torch.LongTensor(ind).to(attention.device)\n                    attention, extend_art = map(\n                        lambda v: v.index_select(dim=0, index=ind),\n                        [attention, extend_art]\n                    )\n                    if masks:\n                        mask = torch.stack(masks, dim=0)\n                    else:\n                        mask = None\n                    attention = (\n                        attention, mask, extend_art, extend_vsize)\n                else:\n                    all_beams[i] = new_beam\n                    finished_beams[i] = finished\n            if all(outputs):\n                break\n        else:\n            for i, (o, f, b) in enumerate(zip(outputs,\n                                              finished_beams, all_beams)):\n                if o is None:\n                    outputs[i] = (f+b)[:beam_size]\n        return outputs\n\n\nclass CopyLSTMDecoder(AttentionalLSTMDecoder):\n    def __init__(self, copy, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._copy = copy\n\n    def _step(self, tok, states, attention):\n        prev_states, prev_out = states\n        lstm_in = torch.cat(\n            [self._embedding(tok).squeeze(1), prev_out],\n            dim=1\n        )\n        states = self._lstm(lstm_in, prev_states)\n        lstm_out = states[0][-1]\n        query = torch.mm(lstm_out, self._attn_w)\n        attention, attn_mask, extend_src, extend_vsize = attention\n        context, score = step_attention(\n            query, attention, attention, attn_mask)\n        dec_out = self._projection(torch.cat([lstm_out, context], dim=1))\n\n        # extend generation prob to extended vocabulary\n        gen_prob = self._compute_gen_prob(dec_out, extend_vsize)\n        # compute the probabilty of each copying\n        copy_prob = torch.sigmoid(self._copy(context, states[0][-1], lstm_in))\n        # add the copy prob to existing vocab distribution\n        lp = torch.log(\n            ((-copy_prob + 1) * gen_prob\n            ).scatter_add(\n                dim=1,\n                index=extend_src.expand_as(score),\n                source=score * copy_prob\n        ) + 1e-8)  # numerical stability for log\n        return lp, (states, dec_out), score\n\n\n    def topk_step(self, tok, states, attention, k):\n        """"""tok:[BB, B], states ([L, BB, B, D]*2, [BB, B, D])""""""\n        (h, c), prev_out = states\n\n        # lstm is not bemable\n        nl, _, _, d = h.size()\n        beam, batch = tok.size()\n        lstm_in_beamable = torch.cat(\n            [self._embedding(tok), prev_out], dim=-1)\n        lstm_in = lstm_in_beamable.contiguous().view(beam*batch, -1)\n        prev_states = (h.contiguous().view(nl, -1, d),\n                       c.contiguous().view(nl, -1, d))\n        h, c = self._lstm(lstm_in, prev_states)\n        states = (h.contiguous().view(nl, beam, batch, -1),\n                  c.contiguous().view(nl, beam, batch, -1))\n        lstm_out = states[0][-1]\n\n        # attention is beamable\n        query = torch.matmul(lstm_out, self._attn_w)\n        attention, attn_mask, extend_src, extend_vsize = attention\n        context, score = step_attention(\n            query, attention, attention, attn_mask)\n        dec_out = self._projection(torch.cat([lstm_out, context], dim=-1))\n\n        # copy mechanism is not beamable\n        gen_prob = self._compute_gen_prob(\n            dec_out.contiguous().view(batch*beam, -1), extend_vsize)\n        copy_prob = torch.sigmoid(\n            self._copy(context, lstm_out, lstm_in_beamable)\n        ).contiguous().view(-1, 1)\n        lp = torch.log(\n            ((-copy_prob + 1) * gen_prob\n            ).scatter_add(\n                dim=1,\n                index=extend_src.expand_as(score).contiguous().view(\n                    beam*batch, -1),\n                source=score.contiguous().view(beam*batch, -1) * copy_prob\n        ) + 1e-8).contiguous().view(beam, batch, -1)\n\n        k_lp, k_tok = lp.topk(k=k, dim=-1)\n        return k_tok, k_lp, (states, dec_out), score\n\n    def _compute_gen_prob(self, dec_out, extend_vsize, eps=1e-6):\n        logit = torch.mm(dec_out, self._embedding.weight.t())\n        bsize, vsize = logit.size()\n        if extend_vsize > vsize:\n            ext_logit = torch.Tensor(bsize, extend_vsize-vsize\n                                    ).to(logit.device)\n            ext_logit.fill_(eps)\n            gen_logit = torch.cat([logit, ext_logit], dim=1)\n        else:\n            gen_logit = logit\n        gen_prob = F.softmax(gen_logit, dim=-1)\n        return gen_prob\n\n    def _compute_copy_activation(self, context, state, input_, score):\n        copy = self._copy(context, state, input_) * score\n        return copy\n'"
model/extract.py,31,"b'import torch\nfrom torch import nn\nfrom torch.nn import init\nfrom torch.nn import functional as F\n\nfrom .rnn import MultiLayerLSTMCells\nfrom .rnn import lstm_encoder\nfrom .util import sequence_mean, len_mask\nfrom .attention import prob_normalize\n\nINI = 1e-2\n\nclass ConvSentEncoder(nn.Module):\n    """"""\n    Convolutional word-level sentence encoder\n    w/ max-over-time pooling, [3, 4, 5] kernel sizes, ReLU activation\n    """"""\n    def __init__(self, vocab_size, emb_dim, n_hidden, dropout):\n        super().__init__()\n        self._embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self._convs = nn.ModuleList([nn.Conv1d(emb_dim, n_hidden, i)\n                                     for i in range(3, 6)])\n        self._dropout = dropout\n        self._grad_handle = None\n\n    def forward(self, input_):\n        emb_input = self._embedding(input_)\n        conv_in = F.dropout(emb_input.transpose(1, 2),\n                            self._dropout, training=self.training)\n        output = torch.cat([F.relu(conv(conv_in)).max(dim=2)[0]\n                            for conv in self._convs], dim=1)\n        return output\n\n    def set_embedding(self, embedding):\n        """"""embedding is the weight matrix""""""\n        assert self._embedding.weight.size() == embedding.size()\n        self._embedding.weight.data.copy_(embedding)\n\n\nclass LSTMEncoder(nn.Module):\n    def __init__(self, input_dim, n_hidden, n_layer, dropout, bidirectional):\n        super().__init__()\n        self._init_h = nn.Parameter(\n            torch.Tensor(n_layer*(2 if bidirectional else 1), n_hidden))\n        self._init_c = nn.Parameter(\n            torch.Tensor(n_layer*(2 if bidirectional else 1), n_hidden))\n        init.uniform_(self._init_h, -INI, INI)\n        init.uniform_(self._init_c, -INI, INI)\n        self._lstm = nn.LSTM(input_dim, n_hidden, n_layer,\n                             dropout=dropout, bidirectional=bidirectional)\n\n    def forward(self, input_, in_lens=None):\n        """""" [batch_size, max_num_sent, input_dim] Tensor""""""\n        size = (self._init_h.size(0), input_.size(0), self._init_h.size(1))\n        init_states = (self._init_h.unsqueeze(1).expand(*size),\n                       self._init_c.unsqueeze(1).expand(*size))\n        lstm_out, _ = lstm_encoder(\n            input_, self._lstm, in_lens, init_states)\n        return lstm_out.transpose(0, 1)\n\n    @property\n    def input_size(self):\n        return self._lstm.input_size\n\n    @property\n    def hidden_size(self):\n        return self._lstm.hidden_size\n\n    @property\n    def num_layers(self):\n        return self._lstm.num_layers\n\n    @property\n    def bidirectional(self):\n        return self._lstm.bidirectional\n\n\nclass ExtractSumm(nn.Module):\n    """""" ff-ext """"""\n    def __init__(self, vocab_size, emb_dim,\n                 conv_hidden, lstm_hidden, lstm_layer,\n                 bidirectional, dropout=0.0):\n        super().__init__()\n        self._sent_enc = ConvSentEncoder(\n            vocab_size, emb_dim, conv_hidden, dropout)\n        self._art_enc = LSTMEncoder(\n            3*conv_hidden, lstm_hidden, lstm_layer,\n            dropout=dropout, bidirectional=bidirectional\n        )\n\n        lstm_out_dim = lstm_hidden * (2 if bidirectional else 1)\n        self._sent_linear = nn.Linear(lstm_out_dim, 1)\n        self._art_linear = nn.Linear(lstm_out_dim, lstm_out_dim)\n\n    def forward(self, article_sents, sent_nums):\n        enc_sent, enc_art = self._encode(article_sents, sent_nums)\n        saliency = torch.matmul(enc_sent, enc_art.unsqueeze(2))\n        saliency = torch.cat(\n            [s[:n] for s, n in zip(saliency, sent_nums)], dim=0)\n        content = self._sent_linear(\n            torch.cat([s[:n] for s, n in zip(enc_sent, sent_nums)], dim=0)\n        )\n        logit = (content + saliency).squeeze(1)\n        return logit\n\n    def extract(self, article_sents, sent_nums=None, k=4):\n        """""" extract top-k scored sentences from article (eval only)""""""\n        enc_sent, enc_art = self._encode(article_sents, sent_nums)\n        saliency = torch.matmul(enc_sent, enc_art.unsqueeze(2))\n        content = self._sent_linear(enc_sent)\n        logit = (content + saliency).squeeze(2)\n        if sent_nums is None:  # test-time extract only\n            assert len(article_sents) == 1\n            n_sent = logit.size(1)\n            extracted = logit[0].topk(\n                k if k < n_sent else n_sent, sorted=False  # original order\n            )[1].tolist()\n        else:\n            extracted = [l[:n].topk(k if k < n else n)[1].tolist()\n                         for n, l in zip(sent_nums, logit)]\n        return extracted\n\n    def _encode(self, article_sents, sent_nums):\n        if sent_nums is None:  # test-time extract only\n            enc_sent = self._sent_enc(article_sents[0]).unsqueeze(0)\n        else:\n            max_n = max(sent_nums)\n            enc_sents = [self._sent_enc(art_sent)\n                         for art_sent in article_sents]\n            def zero(n, device):\n                z = torch.zeros(n, self._art_enc.input_size).to(device)\n                return z\n            enc_sent = torch.stack(\n                [torch.cat([s, zero(max_n-n, s.device)],\n                           dim=0) if n != max_n\n                 else s\n                 for s, n in zip(enc_sents, sent_nums)],\n                dim=0\n            )\n        lstm_out = self._art_enc(enc_sent, sent_nums)\n        enc_art = F.tanh(\n            self._art_linear(sequence_mean(lstm_out, sent_nums, dim=1)))\n        return lstm_out, enc_art\n\n    def set_embedding(self, embedding):\n        self._sent_enc.set_embedding(embedding)\n\n\nclass LSTMPointerNet(nn.Module):\n    """"""Pointer network as in Vinyals et al """"""\n    def __init__(self, input_dim, n_hidden, n_layer,\n                 dropout, n_hop):\n        super().__init__()\n        self._init_h = nn.Parameter(torch.Tensor(n_layer, n_hidden))\n        self._init_c = nn.Parameter(torch.Tensor(n_layer, n_hidden))\n        self._init_i = nn.Parameter(torch.Tensor(input_dim))\n        init.uniform_(self._init_h, -INI, INI)\n        init.uniform_(self._init_c, -INI, INI)\n        init.uniform_(self._init_i, -0.1, 0.1)\n        self._lstm = nn.LSTM(\n            input_dim, n_hidden, n_layer,\n            bidirectional=False, dropout=dropout\n        )\n        self._lstm_cell = None\n\n        # attention parameters\n        self._attn_wm = nn.Parameter(torch.Tensor(input_dim, n_hidden))\n        self._attn_wq = nn.Parameter(torch.Tensor(n_hidden, n_hidden))\n        self._attn_v = nn.Parameter(torch.Tensor(n_hidden))\n        init.xavier_normal_(self._attn_wm)\n        init.xavier_normal_(self._attn_wq)\n        init.uniform_(self._attn_v, -INI, INI)\n\n        # hop parameters\n        self._hop_wm = nn.Parameter(torch.Tensor(input_dim, n_hidden))\n        self._hop_wq = nn.Parameter(torch.Tensor(n_hidden, n_hidden))\n        self._hop_v = nn.Parameter(torch.Tensor(n_hidden))\n        init.xavier_normal_(self._hop_wm)\n        init.xavier_normal_(self._hop_wq)\n        init.uniform_(self._hop_v, -INI, INI)\n        self._n_hop = n_hop\n\n    def forward(self, attn_mem, mem_sizes, lstm_in):\n        """"""atten_mem: Tensor of size [batch_size, max_sent_num, input_dim]""""""\n        attn_feat, hop_feat, lstm_states, init_i = self._prepare(attn_mem)\n        lstm_in = torch.cat([init_i, lstm_in], dim=1).transpose(0, 1)\n        query, final_states = self._lstm(lstm_in, lstm_states)\n        query = query.transpose(0, 1)\n        for _ in range(self._n_hop):\n            query = LSTMPointerNet.attention(\n                hop_feat, query, self._hop_v, self._hop_wq, mem_sizes)\n        output = LSTMPointerNet.attention_score(\n            attn_feat, query, self._attn_v, self._attn_wq)\n        return output  # unormalized extraction logit\n\n    def extract(self, attn_mem, mem_sizes, k):\n        """"""extract k sentences, decode only, batch_size==1""""""\n        attn_feat, hop_feat, lstm_states, lstm_in = self._prepare(attn_mem)\n        lstm_in = lstm_in.squeeze(1)\n        if self._lstm_cell is None:\n            self._lstm_cell = MultiLayerLSTMCells.convert(\n                self._lstm).to(attn_mem.device)\n        extracts = []\n        for _ in range(k):\n            h, c = self._lstm_cell(lstm_in, lstm_states)\n            query = h[-1]\n            for _ in range(self._n_hop):\n                query = LSTMPointerNet.attention(\n                    hop_feat, query, self._hop_v, self._hop_wq, mem_sizes)\n            score = LSTMPointerNet.attention_score(\n                attn_feat, query, self._attn_v, self._attn_wq)\n            score = score.squeeze()\n            for e in extracts:\n                score[e] = -1e6\n            ext = score.max(dim=0)[1].item()\n            extracts.append(ext)\n            lstm_states = (h, c)\n            lstm_in = attn_mem[:, ext, :]\n        return extracts\n\n    def _prepare(self, attn_mem):\n        attn_feat = torch.matmul(attn_mem, self._attn_wm.unsqueeze(0))\n        hop_feat = torch.matmul(attn_mem, self._hop_wm.unsqueeze(0))\n        bs = attn_mem.size(0)\n        n_l, d = self._init_h.size()\n        size = (n_l, bs, d)\n        lstm_states = (self._init_h.unsqueeze(1).expand(*size).contiguous(),\n                       self._init_c.unsqueeze(1).expand(*size).contiguous())\n        d = self._init_i.size(0)\n        init_i = self._init_i.unsqueeze(0).unsqueeze(1).expand(bs, 1, d)\n        return attn_feat, hop_feat, lstm_states, init_i\n\n    @staticmethod\n    def attention_score(attention, query, v, w):\n        """""" unnormalized attention score""""""\n        sum_ = attention.unsqueeze(1) + torch.matmul(\n            query, w.unsqueeze(0)\n        ).unsqueeze(2)  # [B, Nq, Ns, D]\n        score = torch.matmul(\n            F.tanh(sum_), v.unsqueeze(0).unsqueeze(1).unsqueeze(3)\n        ).squeeze(3)  # [B, Nq, Ns]\n        return score\n\n    @staticmethod\n    def attention(attention, query, v, w, mem_sizes):\n        """""" attention context vector""""""\n        score = LSTMPointerNet.attention_score(attention, query, v, w)\n        if mem_sizes is None:\n            norm_score = F.softmax(score, dim=-1)\n        else:\n            mask = len_mask(mem_sizes, score.device).unsqueeze(-2)\n            norm_score = prob_normalize(score, mask)\n        output = torch.matmul(norm_score, attention)\n        return output\n\n\nclass PtrExtractSumm(nn.Module):\n    """""" rnn-ext""""""\n    def __init__(self, emb_dim, vocab_size, conv_hidden,\n                 lstm_hidden, lstm_layer, bidirectional,\n                 n_hop=1, dropout=0.0):\n        super().__init__()\n        self._sent_enc = ConvSentEncoder(\n            vocab_size, emb_dim, conv_hidden, dropout)\n        self._art_enc = LSTMEncoder(\n            3*conv_hidden, lstm_hidden, lstm_layer,\n            dropout=dropout, bidirectional=bidirectional\n        )\n        enc_out_dim = lstm_hidden * (2 if bidirectional else 1)\n        self._extractor = LSTMPointerNet(\n            enc_out_dim, lstm_hidden, lstm_layer,\n            dropout, n_hop\n        )\n\n    def forward(self, article_sents, sent_nums, target):\n        enc_out = self._encode(article_sents, sent_nums)\n        bs, nt = target.size()\n        d = enc_out.size(2)\n        ptr_in = torch.gather(\n            enc_out, dim=1, index=target.unsqueeze(2).expand(bs, nt, d)\n        )\n        output = self._extractor(enc_out, sent_nums, ptr_in)\n        return output\n\n    def extract(self, article_sents, sent_nums=None, k=4):\n        enc_out = self._encode(article_sents, sent_nums)\n        output = self._extractor.extract(enc_out, sent_nums, k)\n        return output\n\n    def _encode(self, article_sents, sent_nums):\n        if sent_nums is None:  # test-time excode only\n            enc_sent = self._sent_enc(article_sents[0]).unsqueeze(0)\n        else:\n            max_n = max(sent_nums)\n            enc_sents = [self._sent_enc(art_sent)\n                         for art_sent in article_sents]\n            def zero(n, device):\n                z = torch.zeros(n, self._art_enc.input_size).to(device)\n                return z\n            enc_sent = torch.stack(\n                [torch.cat([s, zero(max_n-n, s.device)], dim=0)\n                   if n != max_n\n                 else s\n                 for s, n in zip(enc_sents, sent_nums)],\n                dim=0\n            )\n        lstm_out = self._art_enc(enc_sent, sent_nums)\n        return lstm_out\n\n    def set_embedding(self, embedding):\n        self._sent_enc.set_embedding(embedding)\n'"
model/rl.py,18,"b'import torch\nfrom torch import nn\nfrom torch.nn import init\nfrom torch.nn import functional as F\n\nfrom .rnn import MultiLayerLSTMCells\nfrom .extract import LSTMPointerNet\n\n\nINI = 1e-2\n\n# FIXME eccessing \'private members\' of pointer net module is bad\n\nclass PtrExtractorRL(nn.Module):\n    """""" works only on single sample in RL setting""""""\n    def __init__(self, ptr_net):\n        super().__init__()\n        assert isinstance(ptr_net, LSTMPointerNet)\n        self._init_h = nn.Parameter(ptr_net._init_h.clone())\n        self._init_c = nn.Parameter(ptr_net._init_c.clone())\n        self._init_i = nn.Parameter(ptr_net._init_i.clone())\n        self._lstm_cell = MultiLayerLSTMCells.convert(ptr_net._lstm)\n\n        # attention parameters\n        self._attn_wm = nn.Parameter(ptr_net._attn_wm.clone())\n        self._attn_wq = nn.Parameter(ptr_net._attn_wq.clone())\n        self._attn_v = nn.Parameter(ptr_net._attn_v.clone())\n\n        # hop parameters\n        self._hop_wm = nn.Parameter(ptr_net._hop_wm.clone())\n        self._hop_wq = nn.Parameter(ptr_net._hop_wq.clone())\n        self._hop_v = nn.Parameter(ptr_net._hop_v.clone())\n        self._n_hop = ptr_net._n_hop\n\n    def forward(self, attn_mem, n_step):\n        """"""atten_mem: Tensor of size [num_sents, input_dim]""""""\n        attn_feat = torch.mm(attn_mem, self._attn_wm)\n        hop_feat = torch.mm(attn_mem, self._hop_wm)\n        outputs = []\n        lstm_in = self._init_i.unsqueeze(0)\n        lstm_states = (self._init_h.unsqueeze(1), self._init_c.unsqueeze(1))\n        for _ in range(n_step):\n            h, c = self._lstm_cell(lstm_in, lstm_states)\n            query = h[:, -1, :]\n            for _ in range(self._n_hop):\n                query = PtrExtractorRL.attention(hop_feat, query,\n                                                self._hop_v, self._hop_wq)\n            score = PtrExtractorRL.attention_score(\n                attn_feat, query, self._attn_v, self._attn_wq)\n            if self.training:\n                prob = F.softmax(score, dim=-1)\n                out = torch.distributions.Categorical(prob)\n            else:\n                for o in outputs:\n                    score[0, o[0, 0].item()][0] = -1e18\n                out = score.max(dim=1, keepdim=True)[1]\n            outputs.append(out)\n            lstm_in = attn_mem[out[0, 0].item()].unsqueeze(0)\n            lstm_states = (h, c)\n        return outputs\n\n    @staticmethod\n    def attention_score(attention, query, v, w):\n        """""" unnormalized attention score""""""\n        sum_ = attention + torch.mm(query, w)\n        score = torch.mm(F.tanh(sum_), v.unsqueeze(1)).t()\n        return score\n\n    @staticmethod\n    def attention(attention, query, v, w):\n        """""" attention context vector""""""\n        score = F.softmax(\n            PtrExtractorRL.attention_score(attention, query, v, w), dim=-1)\n        output = torch.mm(score, attention)\n        return output\n\n\nclass PtrExtractorRLStop(PtrExtractorRL):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if args:\n            ptr_net = args[0]\n        else:\n            ptr_net = kwargs[\'ptr_net\']\n        assert isinstance(ptr_net, LSTMPointerNet)\n        self._stop = nn.Parameter(\n            torch.Tensor(self._lstm_cell.input_size))\n        init.uniform_(self._stop, -INI, INI)\n\n    def forward(self, attn_mem, n_ext=None):\n        """"""atten_mem: Tensor of size [num_sents, input_dim]""""""\n        if n_ext is not None:\n            return super().forward(attn_mem, n_ext)\n        max_step = attn_mem.size(0)\n        attn_mem = torch.cat([attn_mem, self._stop.unsqueeze(0)], dim=0)\n        attn_feat = torch.mm(attn_mem, self._attn_wm)\n        hop_feat = torch.mm(attn_mem, self._hop_wm)\n        outputs = []\n        dists = []\n        lstm_in = self._init_i.unsqueeze(0)\n        lstm_states = (self._init_h.unsqueeze(1), self._init_c.unsqueeze(1))\n        while True:\n            h, c = self._lstm_cell(lstm_in, lstm_states)\n            query = h[:, -1, :]\n            for _ in range(self._n_hop):\n                query = PtrExtractorRL.attention(hop_feat, query,\n                                                self._hop_v, self._hop_wq)\n            score = PtrExtractorRL.attention_score(\n                attn_feat, query, self._attn_v, self._attn_wq)\n            for o in outputs:\n                score[0, o.item()] = -1e18\n            if self.training:\n                prob = F.softmax(score, dim=-1)\n                m = torch.distributions.Categorical(prob)\n                dists.append(m)\n                out = m.sample()\n            else:\n                out = score.max(dim=1, keepdim=True)[1]\n            outputs.append(out)\n            if out.item() == max_step:\n                break\n            lstm_in = attn_mem[out.item()].unsqueeze(0)\n            lstm_states = (h, c)\n        if dists:\n            # return distributions only when not empty (trining)\n            return outputs, dists\n        else:\n            return outputs\n\n\nclass PtrScorer(nn.Module):\n    """""" to be used as critic (predicts a scalar baseline reward)""""""\n    def __init__(self, ptr_net):\n        super().__init__()\n        assert isinstance(ptr_net, LSTMPointerNet)\n        self._init_h = nn.Parameter(ptr_net._init_h.clone())\n        self._init_c = nn.Parameter(ptr_net._init_c.clone())\n        self._init_i = nn.Parameter(ptr_net._init_i.clone())\n        self._lstm_cell = MultiLayerLSTMCells.convert(ptr_net._lstm)\n\n        # attention parameters\n        self._attn_wm = nn.Parameter(ptr_net._attn_wm.clone())\n        self._attn_wq = nn.Parameter(ptr_net._attn_wq.clone())\n        self._attn_v = nn.Parameter(ptr_net._attn_v.clone())\n\n        # hop parameters\n        self._hop_wm = nn.Parameter(ptr_net._hop_wm.clone())\n        self._hop_wq = nn.Parameter(ptr_net._hop_wq.clone())\n        self._hop_v = nn.Parameter(ptr_net._hop_v.clone())\n        self._n_hop = ptr_net._n_hop\n\n        # regression layer\n        self._score_linear = nn.Linear(self._lstm_cell.input_size, 1)\n\n    def forward(self, attn_mem, n_step):\n        """"""atten_mem: Tensor of size [num_sents, input_dim]""""""\n        attn_feat = torch.mm(attn_mem, self._attn_wm)\n        hop_feat = torch.mm(attn_mem, self._hop_wm)\n        scores = []\n        lstm_in = self._init_i.unsqueeze(0)\n        lstm_states = (self._init_h.unsqueeze(1), self._init_c.unsqueeze(1))\n        for _ in range(n_step):\n            h, c = self._lstm_cell(lstm_in, lstm_states)\n            query = h[:, -1, :]\n            for _ in range(self._n_hop):\n                query = PtrScorer.attention(hop_feat, hop_feat, query,\n                                            self._hop_v, self._hop_wq)\n            output = PtrScorer.attention(\n                attn_mem, attn_feat, query, self._attn_v, self._attn_wq)\n            score = self._score_linear(output)\n            scores.append(score)\n            lstm_in = output\n        return scores\n\n    @staticmethod\n    def attention(attention, attention_feat, query, v, w):\n        """""" attention context vector""""""\n        sum_ = attention_feat + torch.mm(query, w)\n        score = F.softmax(torch.mm(F.tanh(sum_), v.unsqueeze(1)).t(), dim=-1)\n        output = torch.mm(score, attention)\n        return output\n\n\nclass ActorCritic(nn.Module):\n    """""" shared encoder between actor/critic""""""\n    def __init__(self, sent_encoder, art_encoder,\n                 extractor, art_batcher):\n        super().__init__()\n        self._sent_enc = sent_encoder\n        self._art_enc = art_encoder\n        self._ext = PtrExtractorRLStop(extractor)\n        self._scr = PtrScorer(extractor)\n        self._batcher = art_batcher\n\n    def forward(self, raw_article_sents, n_abs=None):\n        article_sent = self._batcher(raw_article_sents)\n        enc_sent = self._sent_enc(article_sent).unsqueeze(0)\n        enc_art = self._art_enc(enc_sent).squeeze(0)\n        if n_abs is not None and not self.training:\n            n_abs = min(len(raw_article_sents), n_abs)\n        if n_abs is None:\n            outputs = self._ext(enc_art)\n        else:\n            outputs = self._ext(enc_art, n_abs)\n        if self.training:\n            if n_abs is None:\n                n_abs = len(outputs[0])\n            scores = self._scr(enc_art, n_abs)\n            return outputs, scores\n        else:\n            return outputs\n'"
model/rnn.py,8,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.nn import init\n\nfrom .util import reorder_sequence, reorder_lstm_states\n\n\ndef lstm_encoder(sequence, lstm,\n                 seq_lens=None, init_states=None, embedding=None):\n    """""" functional LSTM encoder (sequence is [b, t]/[b, t, d],\n    lstm should be rolled lstm)""""""\n    batch_size = sequence.size(0)\n    if not lstm.batch_first:\n        sequence = sequence.transpose(0, 1)\n        emb_sequence = (embedding(sequence) if embedding is not None\n                        else sequence)\n    if seq_lens:\n        assert batch_size == len(seq_lens)\n        sort_ind = sorted(range(len(seq_lens)),\n                          key=lambda i: seq_lens[i], reverse=True)\n        seq_lens = [seq_lens[i] for i in sort_ind]\n        emb_sequence = reorder_sequence(emb_sequence, sort_ind,\n                                        lstm.batch_first)\n\n    if init_states is None:\n        device = sequence.device\n        init_states = init_lstm_states(lstm, batch_size, device)\n    else:\n        init_states = (init_states[0].contiguous(),\n                       init_states[1].contiguous())\n\n    if seq_lens:\n        packed_seq = nn.utils.rnn.pack_padded_sequence(emb_sequence,\n                                                       seq_lens)\n        packed_out, final_states = lstm(packed_seq, init_states)\n        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out)\n\n        back_map = {ind: i for i, ind in enumerate(sort_ind)}\n        reorder_ind = [back_map[i] for i in range(len(seq_lens))]\n        lstm_out = reorder_sequence(lstm_out, reorder_ind, lstm.batch_first)\n        final_states = reorder_lstm_states(final_states, reorder_ind)\n    else:\n        lstm_out, final_states = lstm(emb_sequence, init_states)\n\n    return lstm_out, final_states\n\n\ndef init_lstm_states(lstm, batch_size, device):\n    n_layer = lstm.num_layers*(2 if lstm.bidirectional else 1)\n    n_hidden = lstm.hidden_size\n\n    states = (torch.zeros(n_layer, batch_size, n_hidden).to(device),\n              torch.zeros(n_layer, batch_size, n_hidden).to(device))\n    return states\n\n\nclass StackedLSTMCells(nn.Module):\n    """""" stack multiple LSTM Cells""""""\n    def __init__(self, cells, dropout=0.0):\n        super().__init__()\n        self._cells = nn.ModuleList(cells)\n        self._dropout = dropout\n\n    def forward(self, input_, state):\n        """"""\n        Arguments:\n            input_: FloatTensor (batch, input_size)\n            states: tuple of the H, C LSTM states\n                FloatTensor (num_layers, batch, hidden_size)\n        Returns:\n            LSTM states\n            new_h: (num_layers, batch, hidden_size)\n            new_c: (num_layers, batch, hidden_size)\n        """"""\n        hs = []\n        cs = []\n        for i, cell in enumerate(self._cells):\n            s = (state[0][i, :, :], state[1][i, :, :])\n            h, c = cell(input_, s)\n            hs.append(h)\n            cs.append(c)\n            input_ = F.dropout(h, p=self._dropout, training=self.training)\n\n        new_h = torch.stack(hs, dim=0)\n        new_c = torch.stack(cs, dim=0)\n\n        return new_h, new_c\n\n    @property\n    def hidden_size(self):\n        return self._cells[0].hidden_size\n\n    @property\n    def input_size(self):\n        return self._cells[0].input_size\n\n    @property\n    def num_layers(self):\n        return len(self._cells)\n\n    @property\n    def bidirectional(self):\n        return self._cells[0].bidirectional\n\n\nclass MultiLayerLSTMCells(StackedLSTMCells):\n    """"""\n    This class is a one-step version of the cudnn LSTM\n    , or multi-layer version of LSTMCell\n    """"""\n    def __init__(self, input_size, hidden_size, num_layers,\n                 bias=True, dropout=0.0):\n        """""" same as nn.LSTM but without (bidirectional)""""""\n        cells = []\n        cells.append(nn.LSTMCell(input_size, hidden_size, bias))\n        for _ in range(num_layers-1):\n            cells.append(nn.LSTMCell(hidden_size, hidden_size, bias))\n        super().__init__(cells, dropout)\n\n    @property\n    def bidirectional(self):\n        return False\n\n    def reset_parameters(self):\n        for cell in self._cells:\n            # xavier initilization\n            gate_size = self.hidden_size / 4\n            for weight in [cell.weight_ih, cell.weight_hh]:\n                for w in torch.chunk(weight, 4, dim=0):\n                    init.xavier_normal_(w)\n            #forget bias = 1\n            for bias in [cell.bias_ih, cell.bias_hh]:\n                torch.chunk(bias, 4, dim=0)[1].data.fill_(1)\n\n    @staticmethod\n    def convert(lstm):\n        """""" convert from a cudnn LSTM""""""\n        lstm_cell = MultiLayerLSTMCells(\n            lstm.input_size, lstm.hidden_size,\n            lstm.num_layers, dropout=lstm.dropout)\n        for i, cell in enumerate(lstm_cell._cells):\n            cell.weight_ih.data.copy_(getattr(lstm, \'weight_ih_l{}\'.format(i)))\n            cell.weight_hh.data.copy_(getattr(lstm, \'weight_hh_l{}\'.format(i)))\n            cell.bias_ih.data.copy_(getattr(lstm, \'bias_ih_l{}\'.format(i)))\n            cell.bias_hh.data.copy_(getattr(lstm, \'bias_hh_l{}\'.format(i)))\n        return lstm_cell\n'"
model/summ.py,19,"b'import torch\nfrom torch import nn\nfrom torch.nn import init\n\nfrom .rnn import lstm_encoder\nfrom .rnn import MultiLayerLSTMCells\nfrom .attention import step_attention\nfrom .util import sequence_mean, len_mask\n\n\nINIT = 1e-2\n\n\nclass Seq2SeqSumm(nn.Module):\n    def __init__(self, vocab_size, emb_dim,\n                 n_hidden, bidirectional, n_layer, dropout=0.0):\n        super().__init__()\n        # embedding weight parameter is shared between encoder, decoder,\n        # and used as final projection layer to vocab logit\n        # can initialize with pretrained word vectors\n        self._embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self._enc_lstm = nn.LSTM(\n            emb_dim, n_hidden, n_layer,\n            bidirectional=bidirectional, dropout=dropout\n        )\n        # initial encoder LSTM states are learned parameters\n        state_layer = n_layer * (2 if bidirectional else 1)\n        self._init_enc_h = nn.Parameter(\n            torch.Tensor(state_layer, n_hidden)\n        )\n        self._init_enc_c = nn.Parameter(\n            torch.Tensor(state_layer, n_hidden)\n        )\n        init.uniform_(self._init_enc_h, -INIT, INIT)\n        init.uniform_(self._init_enc_c, -INIT, INIT)\n\n        # vanillat lstm / LNlstm\n        self._dec_lstm = MultiLayerLSTMCells(\n            2*emb_dim, n_hidden, n_layer, dropout=dropout\n        )\n        # project encoder final states to decoder initial states\n        enc_out_dim = n_hidden * (2 if bidirectional else 1)\n        self._dec_h = nn.Linear(enc_out_dim, n_hidden, bias=False)\n        self._dec_c = nn.Linear(enc_out_dim, n_hidden, bias=False)\n        # multiplicative attention\n        self._attn_wm = nn.Parameter(torch.Tensor(enc_out_dim, n_hidden))\n        self._attn_wq = nn.Parameter(torch.Tensor(n_hidden, n_hidden))\n        init.xavier_normal_(self._attn_wm)\n        init.xavier_normal_(self._attn_wq)\n        # project decoder output to emb_dim, then\n        # apply weight matrix from embedding layer\n        self._projection = nn.Sequential(\n            nn.Linear(2*n_hidden, n_hidden),\n            nn.Tanh(),\n            nn.Linear(n_hidden, emb_dim, bias=False)\n        )\n        # functional object for easier usage\n        self._decoder = AttentionalLSTMDecoder(\n            self._embedding, self._dec_lstm,\n            self._attn_wq, self._projection\n        )\n\n    def forward(self, article, art_lens, abstract):\n        attention, init_dec_states = self.encode(article, art_lens)\n        mask = len_mask(art_lens, attention.device).unsqueeze(-2)\n        logit = self._decoder((attention, mask), init_dec_states, abstract)\n        return logit\n\n    def encode(self, article, art_lens=None):\n        size = (\n            self._init_enc_h.size(0),\n            len(art_lens) if art_lens else 1,\n            self._init_enc_h.size(1)\n        )\n        init_enc_states = (\n            self._init_enc_h.unsqueeze(1).expand(*size),\n            self._init_enc_c.unsqueeze(1).expand(*size)\n        )\n        enc_art, final_states = lstm_encoder(\n            article, self._enc_lstm, art_lens,\n            init_enc_states, self._embedding\n        )\n        if self._enc_lstm.bidirectional:\n            h, c = final_states\n            final_states = (\n                torch.cat(h.chunk(2, dim=0), dim=2),\n                torch.cat(c.chunk(2, dim=0), dim=2)\n            )\n        init_h = torch.stack([self._dec_h(s)\n                              for s in final_states[0]], dim=0)\n        init_c = torch.stack([self._dec_c(s)\n                              for s in final_states[1]], dim=0)\n        init_dec_states = (init_h, init_c)\n        attention = torch.matmul(enc_art, self._attn_wm).transpose(0, 1)\n        init_attn_out = self._projection(torch.cat(\n            [init_h[-1], sequence_mean(attention, art_lens, dim=1)], dim=1\n        ))\n        return attention, (init_dec_states, init_attn_out)\n\n    def batch_decode(self, article, art_lens, go, eos, max_len):\n        """""" greedy decode support batching""""""\n        batch_size = len(art_lens)\n        attention, init_dec_states = self.encode(article, art_lens)\n        mask = len_mask(art_lens, attention.device).unsqueeze(-2)\n        attention = (attention, mask)\n        tok = torch.LongTensor([go]*batch_size).to(article.device)\n        outputs = []\n        attns = []\n        states = init_dec_states\n        for i in range(max_len):\n            tok, states, attn_score = self._decoder.decode_step(\n                tok, states, attention)\n            outputs.append(tok[:, 0])\n            attns.append(attn_score)\n        return outputs, attns\n\n    def decode(self, article, go, eos, max_len):\n        attention, init_dec_states = self.encode(article)\n        attention = (attention, None)\n        tok = torch.LongTensor([go]).to(article.device)\n        outputs = []\n        attns = []\n        states = init_dec_states\n        for i in range(max_len):\n            tok, states, attn_score = self._decoder.decode_step(\n                tok, states, attention)\n            if tok[0, 0].item() == eos:\n                break\n            outputs.append(tok[0, 0].item())\n            attns.append(attn_score.squeeze(0))\n        return outputs, attns\n\n    def set_embedding(self, embedding):\n        """"""embedding is the weight matrix""""""\n        assert self._embedding.weight.size() == embedding.size()\n        self._embedding.weight.data.copy_(embedding)\n\n\nclass AttentionalLSTMDecoder(object):\n    def __init__(self, embedding, lstm, attn_w, projection):\n        super().__init__()\n        self._embedding = embedding\n        self._lstm = lstm\n        self._attn_w = attn_w\n        self._projection = projection\n\n    def __call__(self, attention, init_states, target):\n        max_len = target.size(1)\n        states = init_states\n        logits = []\n        for i in range(max_len):\n            tok = target[:, i:i+1]\n            logit, states, _ = self._step(tok, states, attention)\n            logits.append(logit)\n        logit = torch.stack(logits, dim=1)\n        return logit\n\n    def _step(self, tok, states, attention):\n        prev_states, prev_out = states\n        lstm_in = torch.cat(\n            [self._embedding(tok).squeeze(1), prev_out],\n            dim=1\n        )\n        states = self._lstm(lstm_in, prev_states)\n        lstm_out = states[0][-1]\n        query = torch.mm(lstm_out, self._attn_w)\n        attention, attn_mask = attention\n        context, score = step_attention(\n            query, attention, attention, attn_mask)\n        dec_out = self._projection(torch.cat([lstm_out, context], dim=1))\n        states = (states, dec_out)\n        logit = torch.mm(dec_out, self._embedding.weight.t())\n        return logit, states, score\n\n    def decode_step(self, tok, states, attention):\n        logit, states, score = self._step(tok, states, attention)\n        out = torch.max(logit, dim=1, keepdim=True)[1]\n        return out, states, score\n'"
model/util.py,7,"b'import math\n\nimport torch\nfrom torch.nn import functional as F\n\n\n#################### general sequence helper #########################\ndef len_mask(lens, device):\n    """""" users are resposible for shaping\n    Return: tensor_type [B, T]\n    """"""\n    max_len = max(lens)\n    batch_size = len(lens)\n    mask = torch.ByteTensor(batch_size, max_len).to(device)\n    mask.fill_(0)\n    for i, l in enumerate(lens):\n        mask[i, :l].fill_(1)\n    return mask\n\ndef sequence_mean(sequence, seq_lens, dim=1):\n    if seq_lens:\n        assert sequence.size(0) == len(seq_lens)   # batch_size\n        sum_ = torch.sum(sequence, dim=dim, keepdim=False)\n        mean = torch.stack([s/l for s, l in zip(sum_, seq_lens)], dim=0)\n    else:\n        mean = torch.mean(sequence, dim=dim, keepdim=False)\n    return mean\n\ndef sequence_loss(logits, targets, xent_fn=None, pad_idx=0):\n    """""" functional interface of SequenceLoss""""""\n    assert logits.size()[:-1] == targets.size()\n\n    mask = targets != pad_idx\n    target = targets.masked_select(mask)\n    logit = logits.masked_select(\n        mask.unsqueeze(2).expand_as(logits)\n    ).contiguous().view(-1, logits.size(-1))\n    if xent_fn:\n        loss = xent_fn(logit, target)\n    else:\n        loss = F.cross_entropy(logit, target)\n    assert (not math.isnan(loss.mean().item())\n            and not math.isinf(loss.mean().item()))\n    return loss\n\n\n#################### LSTM helper #########################\n\ndef reorder_sequence(sequence_emb, order, batch_first=False):\n    """"""\n    sequence_emb: [T, B, D] if not batch_first\n    order: list of sequence length\n    """"""\n    batch_dim = 0 if batch_first else 1\n    assert len(order) == sequence_emb.size()[batch_dim]\n\n    order = torch.LongTensor(order).to(sequence_emb.device)\n    sorted_ = sequence_emb.index_select(index=order, dim=batch_dim)\n\n    return sorted_\n\ndef reorder_lstm_states(lstm_states, order):\n    """"""\n    lstm_states: (H, C) of tensor [layer, batch, hidden]\n    order: list of sequence length\n    """"""\n    assert isinstance(lstm_states, tuple)\n    assert len(lstm_states) == 2\n    assert lstm_states[0].size() == lstm_states[1].size()\n    assert len(order) == lstm_states[0].size()[1]\n\n    order = torch.LongTensor(order).to(lstm_states[0].device)\n    sorted_states = (lstm_states[0].index_select(index=order, dim=1),\n                     lstm_states[1].index_select(index=order, dim=1))\n\n    return sorted_states\n'"
