file_path,api_count,code
A2C.py,1,"b'\nimport torch as th\nfrom torch import nn\nfrom torch.optim import Adam, RMSprop\n\nimport numpy as np\n\nfrom common.Agent import Agent\nfrom common.Model import ActorNetwork, CriticNetwork\nfrom common.utils import entropy, index_to_one_hot, to_tensor_var\n\n\nclass A2C(Agent):\n    """"""\n    An agent learned with Advantage Actor-Critic\n    - Actor takes state as input\n    - Critic takes both state and action as input\n    - agent interact with environment to collect experience\n    - agent training with experience to update policy\n    """"""\n    def __init__(self, env, state_dim, action_dim,\n                 memory_capacity=10000, max_steps=None,\n                 roll_out_n_steps=10,\n                 reward_gamma=0.99, reward_scale=1., done_penalty=None,\n                 actor_hidden_size=32, critic_hidden_size=32,\n                 actor_output_act=nn.functional.log_softmax, critic_loss=""mse"",\n                 actor_lr=0.001, critic_lr=0.001,\n                 optimizer_type=""rmsprop"", entropy_reg=0.01,\n                 max_grad_norm=0.5, batch_size=100, episodes_before_train=100,\n                 epsilon_start=0.9, epsilon_end=0.01, epsilon_decay=200,\n                 use_cuda=True):\n        super(A2C, self).__init__(env, state_dim, action_dim,\n                 memory_capacity, max_steps,\n                 reward_gamma, reward_scale, done_penalty,\n                 actor_hidden_size, critic_hidden_size,\n                 actor_output_act, critic_loss,\n                 actor_lr, critic_lr,\n                 optimizer_type, entropy_reg,\n                 max_grad_norm, batch_size, episodes_before_train,\n                 epsilon_start, epsilon_end, epsilon_decay,\n                 use_cuda)\n\n        self.roll_out_n_steps = roll_out_n_steps\n\n        self.actor = ActorNetwork(self.state_dim, self.actor_hidden_size,\n                                  self.action_dim, self.actor_output_act)\n        self.critic = CriticNetwork(self.state_dim, self.action_dim,\n                                    self.critic_hidden_size, 1)\n        if self.optimizer_type == ""adam"":\n            self.actor_optimizer = Adam(self.actor.parameters(), lr=self.actor_lr)\n            self.critic_optimizer = Adam(self.critic.parameters(), lr=self.critic_lr)\n        elif self.optimizer_type == ""rmsprop"":\n            self.actor_optimizer = RMSprop(self.actor.parameters(), lr=self.actor_lr)\n            self.critic_optimizer = RMSprop(self.critic.parameters(), lr=self.critic_lr)\n        if self.use_cuda:\n            self.actor.cuda()\n\n    # agent interact with the environment to collect experience\n    def interact(self):\n        super(A2C, self)._take_n_steps()\n\n    # train on a roll out batch\n    def train(self):\n        if self.n_episodes <= self.episodes_before_train:\n            pass\n\n        batch = self.memory.sample(self.batch_size)\n        states_var = to_tensor_var(batch.states, self.use_cuda).view(-1, self.state_dim)\n        one_hot_actions = index_to_one_hot(batch.actions, self.action_dim)\n        actions_var = to_tensor_var(one_hot_actions, self.use_cuda).view(-1, self.action_dim)\n        rewards_var = to_tensor_var(batch.rewards, self.use_cuda).view(-1, 1)\n\n        # update actor network\n        self.actor_optimizer.zero_grad()\n        action_log_probs = self.actor(states_var)\n        entropy_loss = th.mean(entropy(th.exp(action_log_probs)))\n        action_log_probs = th.sum(action_log_probs * actions_var, 1)\n        values = self.critic(states_var, actions_var)\n        advantages = rewards_var - values.detach()\n        pg_loss = -th.mean(action_log_probs * advantages)\n        actor_loss = pg_loss - entropy_loss * self.entropy_reg\n        actor_loss.backward()\n        if self.max_grad_norm is not None:\n            nn.utils.clip_grad_norm(self.actor.parameters(), self.max_grad_norm)\n        self.actor_optimizer.step()\n\n        # update critic network\n        self.critic_optimizer.zero_grad()\n        target_values = rewards_var\n        if self.critic_loss == ""huber"":\n            critic_loss = nn.functional.smooth_l1_loss(values, target_values)\n        else:\n            critic_loss = nn.MSELoss()(values, target_values)\n        critic_loss.backward()\n        if self.max_grad_norm is not None:\n            nn.utils.clip_grad_norm(self.critic.parameters(), self.max_grad_norm)\n        self.critic_optimizer.step()\n\n    # predict softmax action based on state\n    def _softmax_action(self, state):\n        state_var = to_tensor_var([state], self.use_cuda)\n        softmax_action_var = th.exp(self.actor(state_var))\n        if self.use_cuda:\n            softmax_action = softmax_action_var.data.cpu().numpy()[0]\n        else:\n            softmax_action = softmax_action_var.data.numpy()[0]\n        return softmax_action\n\n    # choose an action based on state with random noise added for exploration in training\n    def exploration_action(self, state):\n        softmax_action = self._softmax_action(state)\n        epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n                                  np.exp(-1. * self.n_steps / self.epsilon_decay)\n        if np.random.rand() < epsilon:\n            action = np.random.choice(self.action_dim)\n        else:\n            action = np.argmax(softmax_action)\n        return action\n\n    # choose an action based on state for execution\n    def action(self, state):\n        softmax_action = self._softmax_action(state)\n        action = np.argmax(softmax_action)\n        return action\n\n    # evaluate value for a state-action pair\n    def value(self, state, action):\n        state_var = to_tensor_var([state], self.use_cuda)\n        action = index_to_one_hot(action, self.action_dim)\n        action_var = to_tensor_var([action], self.use_cuda)\n        value_var = self.critic(state_var, action_var)\n        if self.use_cuda:\n            value = value_var.data.cpu().numpy()[0]\n        else:\n            value = value_var.data.numpy()[0]\n        return value\n'"
ACKTR.py,0,"b'\nimport torch as th\nfrom torch import nn\n\nimport numpy as np\n\nfrom A2C import A2C\nfrom common.Model import ActorCriticNetwork\nfrom common.kfac import KFACOptimizer\nfrom common.utils import index_to_one_hot, entropy, to_tensor_var\n\n\nclass DisjointACKTR(A2C):\n    """"""\n    An agent learned with ACKTR\n    Using disjoint actor and critic results in instability in training.\n    """"""\n    def __init__(self, env, state_dim, action_dim,\n                 memory_capacity=10000, max_steps=None,\n                 roll_out_n_steps=10,\n                 reward_gamma=0.99, reward_scale=1., done_penalty=None,\n                 actor_hidden_size=32, critic_hidden_size=32,\n                 actor_output_act=nn.functional.log_softmax, critic_loss=""mse"",\n                 actor_lr=0.001, critic_lr=0.001, vf_fisher_coef=0.5,\n                 optimizer_type=""rmsprop"", entropy_reg=0.01,\n                 max_grad_norm=0.5, batch_size=100, episodes_before_train=100,\n                 epsilon_start=0.9, epsilon_end=0.01, epsilon_decay=200,\n                 use_cuda=True):\n        super(DisjointACKTR, self).__init__(env, state_dim, action_dim,\n                 memory_capacity, max_steps, roll_out_n_steps,\n                 reward_gamma, reward_scale, done_penalty,\n                 actor_hidden_size, critic_hidden_size,\n                 actor_output_act, critic_loss,\n                 actor_lr, critic_lr,\n                 optimizer_type, entropy_reg,\n                 max_grad_norm, batch_size, episodes_before_train,\n                 epsilon_start, epsilon_end, epsilon_decay,\n                 use_cuda)\n\n        self.actor_optimizer = KFACOptimizer(self.actor, lr=self.actor_lr)\n        self.critic_optimizer = KFACOptimizer(self.critic, lr=self.critic_lr)\n        self.vf_fisher_coef = vf_fisher_coef\n\n    # train on a roll out batch\n    def train(self):\n        if self.n_episodes <= self.episodes_before_train:\n            pass\n\n        batch = self.memory.sample(self.batch_size)\n        states_var = to_tensor_var(batch.states, self.use_cuda).view(-1, self.state_dim)\n        one_hot_actions = index_to_one_hot(batch.actions, self.action_dim)\n        actions_var = to_tensor_var(one_hot_actions, self.use_cuda).view(-1, self.action_dim)\n        rewards_var = to_tensor_var(batch.rewards, self.use_cuda).view(-1, 1)\n\n        # update actor network\n        action_log_probs = self.actor(states_var)\n        entropy_loss = th.mean(entropy(th.exp(action_log_probs)))\n        action_log_probs = th.sum(action_log_probs * actions_var, 1)\n        # fisher loss\n        if self.actor_optimizer.steps % self.actor_optimizer.Ts == 0:\n            self.actor.zero_grad()\n            pg_fisher_loss = th.mean(action_log_probs)\n            self.actor_optimizer.acc_stats = True\n            pg_fisher_loss.backward(retain_graph=True)\n            self.actor_optimizer.acc_stats = False\n        self.actor_optimizer.zero_grad()\n        # actor loss\n        values = self.critic(states_var, actions_var).detach()\n        advantages = rewards_var - values\n        pg_loss = -th.mean(action_log_probs * advantages)\n        actor_loss = pg_loss - entropy_loss * self.entropy_reg\n        actor_loss.backward()\n        if self.max_grad_norm is not None:\n            nn.utils.clip_grad_norm(self.actor.parameters(), self.max_grad_norm)\n        self.actor_optimizer.step()\n\n        # update critic network\n        target_values = rewards_var\n        values = self.critic(states_var, actions_var)\n        # fisher loss\n        if self.critic_optimizer.steps % self.critic_optimizer.Ts == 0:\n            self.critic.zero_grad()\n            values_noise = to_tensor_var(np.random.randn(values.size()[0]), self.use_cuda)\n            sample_values = (values + values_noise.view(-1, 1)).detach()\n            if self.critic_loss == ""huber"":\n                vf_fisher_loss = - self.vf_fisher_coef * nn.functional.smooth_l1_loss(values, sample_values)\n            else:\n                vf_fisher_loss = - self.vf_fisher_coef * nn.MSELoss()(values, sample_values)\n            self.critic_optimizer.acc_stats = True\n            vf_fisher_loss.backward(retain_graph=True)\n            self.critic_optimizer.acc_stats = False\n        self.critic_optimizer.zero_grad()\n        # critic loss\n        if self.critic_loss == ""huber"":\n            critic_loss = nn.functional.smooth_l1_loss(values, target_values)\n        else:\n            critic_loss = nn.MSELoss()(values, target_values)\n        critic_loss.backward()\n        if self.max_grad_norm is not None:\n            nn.utils.clip_grad_norm(self.critic.parameters(), self.max_grad_norm)\n        self.critic_optimizer.step()\n\nclass JointACKTR(A2C):\n    """"""\n    An agent learned with ACKTR\n    Compared with DisjointACKTR, JointACKTR is more stable.\n    """"""\n    def __init__(self, env, state_dim, action_dim,\n                 memory_capacity=10000, max_steps=None,\n                 roll_out_n_steps=10,\n                 reward_gamma=0.99, reward_scale=1., done_penalty=None,\n                 actor_hidden_size=32, critic_hidden_size=32,\n                 actor_output_act=nn.functional.log_softmax, critic_loss=""mse"",\n                 actor_lr=0.001, critic_lr=0.001, vf_coef=0.5, vf_fisher_coef=1.0,\n                 optimizer_type=""rmsprop"", entropy_reg=0.01,\n                 max_grad_norm=0.5, batch_size=100, episodes_before_train=100,\n                 epsilon_start=0.9, epsilon_end=0.01, epsilon_decay=200,\n                 use_cuda=True):\n        super(JointACKTR, self).__init__(env, state_dim, action_dim,\n                 memory_capacity, max_steps, roll_out_n_steps,\n                 reward_gamma, reward_scale, done_penalty,\n                 actor_hidden_size, critic_hidden_size,\n                 actor_output_act, critic_loss,\n                 actor_lr, critic_lr,\n                 optimizer_type, entropy_reg,\n                 max_grad_norm, batch_size, episodes_before_train,\n                 epsilon_start, epsilon_end, epsilon_decay,\n                 use_cuda)\n\n        self.actor_critic = ActorCriticNetwork(self.state_dim, self.action_dim,\n                                               min(self.actor_hidden_size, self.critic_hidden_size),\n                                               self.actor_output_act)\n        self.optimizer = KFACOptimizer(self.actor_critic, lr=min(self.actor_lr, self.critic_lr))\n        self.vf_coef = vf_coef\n        self.vf_fisher_coef = vf_fisher_coef\n\n    # train on a roll out batch\n    def train(self):\n        if self.n_episodes <= self.episodes_before_train:\n            pass\n\n        batch = self.memory.sample(self.batch_size)\n        states_var = to_tensor_var(batch.states, self.use_cuda).view(-1, self.state_dim)\n        one_hot_actions = index_to_one_hot(batch.actions, self.action_dim)\n        actions_var = to_tensor_var(one_hot_actions, self.use_cuda).view(-1, self.action_dim)\n        rewards_var = to_tensor_var(batch.rewards, self.use_cuda).view(-1, 1)\n\n        # update actor network\n        action_log_probs, values = self.actor_critic(states_var)\n        entropy_loss = th.mean(entropy(th.exp(action_log_probs)))\n        action_log_probs = th.sum(action_log_probs * actions_var, 1)\n        # fisher loss\n        if self.optimizer.steps % self.optimizer.Ts == 0:\n            self.actor_critic.zero_grad()\n            pg_fisher_loss = th.mean(action_log_probs)\n            values_noise = to_tensor_var(np.random.randn(values.size()[0]), self.use_cuda)\n            sample_values = (values + values_noise.view(-1, 1)).detach()\n            if self.critic_loss == ""huber"":\n                vf_fisher_loss = - nn.functional.smooth_l1_loss(values, sample_values)\n            else:\n                vf_fisher_loss = - nn.MSELoss()(values, sample_values)\n            joint_fisher_loss = pg_fisher_loss + self.vf_fisher_coef * vf_fisher_loss\n            self.optimizer.acc_stats = True\n            joint_fisher_loss.backward(retain_graph=True)\n            self.optimizer.acc_stats = False\n        self.optimizer.zero_grad()\n        # actor loss\n        advantages = rewards_var - values.detach()\n        pg_loss = -th.mean(action_log_probs * advantages)\n        actor_loss = pg_loss - entropy_loss * self.entropy_reg\n        # critic loss\n        target_values = rewards_var\n        if self.critic_loss == ""huber"":\n            critic_loss = nn.functional.smooth_l1_loss(values, target_values)\n        else:\n            critic_loss = nn.MSELoss()(values, target_values)\n        loss = actor_loss + critic_loss\n        loss.backward()\n        if self.max_grad_norm is not None:\n            nn.utils.clip_grad_norm(self.actor_critic.parameters(), self.max_grad_norm)\n        self.optimizer.step()\n\n    # predict softmax action based on state\n    def _softmax_action(self, state):\n        state_var = to_tensor_var([state], self.use_cuda)\n        softmax_action_var = th.exp(self.actor_critic(state_var)[0])\n        if self.use_cuda:\n            softmax_action = softmax_action_var.data.cpu().numpy()[0]\n        else:\n            softmax_action = softmax_action_var.data.numpy()[0]\n        return softmax_action\n\n    # evaluate value for a state-action pair\n    def value(self, state, action):\n        state_var = to_tensor_var([state], self.use_cuda)\n        value_var = self.actor_critic(state_var)[1]\n        if self.use_cuda:\n            value = value_var.data.cpu().numpy()[0]\n        else:\n            value = value_var.data.numpy()[0]\n        return value\n'"
DDPG.py,2,"b'\nimport torch.nn as nn\nfrom torch.optim import Adam, RMSprop\n\nimport numpy as np\nfrom copy import deepcopy\n\nfrom common.Agent import Agent\nfrom common.Model import ActorNetwork, CriticNetwork\nfrom common.utils import to_tensor_var\n\n\nclass DDPG(Agent):\n    """"""\n    An agent learned with Deep Deterministic Policy Gradient using Actor-Critic framework\n    - Actor takes state as input\n    - Critic takes both state and action as input\n    - Critic uses gradient temporal-difference learning\n    """"""\n    def __init__(self, env, state_dim, action_dim,\n                 memory_capacity=10000, max_steps=None,\n                 target_tau=0.01, target_update_steps=5,\n                 reward_gamma=0.99, reward_scale=1., done_penalty=None,\n                 actor_hidden_size=32, critic_hidden_size=32,\n                 actor_output_act=nn.functional.tanh, critic_loss=""mse"",\n                 actor_lr=0.001, critic_lr=0.001,\n                 optimizer_type=""adam"", entropy_reg=0.01,\n                 max_grad_norm=0.5, batch_size=100, episodes_before_train=100,\n                 epsilon_start=0.9, epsilon_end=0.01, epsilon_decay=200,\n                 use_cuda=True):\n        super(DDPG, self).__init__(env, state_dim, action_dim,\n                 memory_capacity, max_steps,\n                 reward_gamma, reward_scale, done_penalty,\n                 actor_hidden_size, critic_hidden_size,\n                 actor_output_act, critic_loss,\n                 actor_lr, critic_lr,\n                 optimizer_type, entropy_reg,\n                 max_grad_norm, batch_size, episodes_before_train,\n                 epsilon_start, epsilon_end, epsilon_decay,\n                 use_cuda)\n\n        self.target_tau = target_tau\n        self.target_update_steps = target_update_steps\n\n        self.actor = ActorNetwork(self.state_dim, self.actor_hidden_size, self.action_dim, self.actor_output_act)\n        self.critic = CriticNetwork(self.state_dim, self.action_dim, self.critic_hidden_size, 1)\n        # to ensure target network and learning network has the same weights\n        self.actor_target = deepcopy(self.actor)\n        self.critic_target = deepcopy(self.critic)\n\n        if self.optimizer_type == ""adam"":\n            self.actor_optimizer = Adam(self.actor.parameters(), lr=self.actor_lr)\n            self.critic_optimizer = Adam(self.critic.parameters(), lr=self.critic_lr)\n        elif self.optimizer_type == ""rmsprop"":\n            self.actor_optimizer = RMSprop(self.actor.parameters(), lr=self.actor_lr)\n            self.critic_optimizer = RMSprop(self.critic.parameters(), lr=self.critic_lr)\n\n        if self.use_cuda:\n            self.actor.cuda()\n            self.critic.cuda()\n            self.actor_target.cuda()\n            self.critic_target.cuda()\n\n    # agent interact with the environment to collect experience\n    def interact(self):\n        super(DDPG, self)._take_one_step()\n\n    # train on a sample batch\n    def train(self):\n        # do not train until exploration is enough\n        if self.n_episodes <= self.episodes_before_train:\n            pass\n\n        batch = self.memory.sample(self.batch_size)\n        state_var = to_tensor_var(batch.states, self.use_cuda).view(-1, self.state_dim)\n        action_var = to_tensor_var(batch.actions, self.use_cuda).view(-1, self.action_dim)\n        reward_var = to_tensor_var(batch.rewards, self.use_cuda).view(-1, 1)\n        next_state_var = to_tensor_var(batch.next_states, self.use_cuda).view(-1, self.state_dim)\n        done_var = to_tensor_var(batch.dones, self.use_cuda).view(-1, 1)\n\n        # estimate the target q with actor_target network and critic_target network\n        next_action_var = self.actor_target(next_state_var)\n        next_q = self.critic_target(next_state_var, next_action_var).detach()\n        target_q = self.reward_scale * reward_var + self.reward_gamma * next_q * (1. - done_var)\n\n        # update critic network\n        self.critic_optimizer.zero_grad()\n        # current Q values\n        current_q = self.critic(state_var, action_var)\n        # rewards is target Q values\n        if self.critic_loss == ""huber"":\n            critic_loss = nn.functional.smooth_l1_loss(current_q, target_q)\n        else:\n            critic_loss = nn.MSELoss()(current_q, target_q)\n        critic_loss.backward()\n        if self.max_grad_norm is not None:\n            nn.utils.clip_grad_norm(self.critic.parameters(), self.max_grad_norm)\n        self.critic_optimizer.step()\n\n        # update actor network\n        self.actor_optimizer.zero_grad()\n        # the accurate action prediction\n        action = self.actor(state_var)\n        # actor_loss is used to maximize the Q value for the predicted action\n        actor_loss = - self.critic(state_var, action)\n        actor_loss = actor_loss.mean()\n        actor_loss.backward()\n        if self.max_grad_norm is not None:\n            nn.utils.clip_grad_norm(self.actor.parameters(), self.max_grad_norm)\n        self.actor_optimizer.step()\n\n        # update actor target network and critic target network\n        if self.n_steps % self.target_update_steps == 0 and self.n_steps > 0:\n            super(DDPG, self)._soft_update_target(self.critic_target, self.critic)\n            super(DDPG, self)._soft_update_target(self.actor_target, self.actor)\n\n    # choose an action based on state with random noise added for exploration in training\n    def exploration_action(self, state):\n        action = self.action(state)\n        epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n                                  np.exp(-1. * self.n_steps / self.epsilon_decay)\n        # add noise\n        noise = np.random.randn(self.action_dim) * epsilon\n        action += noise\n        return action\n\n    # choose an action based on state for execution\n    def action(self, state):\n        action_var = self.actor(to_tensor_var([state], self.use_cuda))\n        if self.use_cuda:\n            action = action_var.data.cpu().numpy()[0]\n        else:\n            action = action_var.data.numpy()[0]\n        return action\n'"
DQN.py,1,"b'\nimport torch as th\nfrom torch import nn\nfrom torch.optim import Adam, RMSprop\n\nimport numpy as np\n\nfrom common.Agent import Agent\nfrom common.Model import ActorNetwork\nfrom common.utils import identity, to_tensor_var\n\n\nclass DQN(Agent):\n    """"""\n    An agent learned with DQN using replay memory and temporal difference\n    - use a value network to estimate the state-action value\n    """"""\n    def __init__(self, env, state_dim, action_dim,\n                 memory_capacity=10000, max_steps=10000,\n                 reward_gamma=0.99, reward_scale=1., done_penalty=None,\n                 actor_hidden_size=32, critic_hidden_size=32,\n                 actor_output_act=identity, critic_loss=""mse"",\n                 actor_lr=0.001, critic_lr=0.001,\n                 optimizer_type=""rmsprop"", entropy_reg=0.01,\n                 max_grad_norm=0.5, batch_size=100, episodes_before_train=100,\n                 epsilon_start=0.9, epsilon_end=0.01, epsilon_decay=200,\n                 use_cuda=True):\n        super(DQN, self).__init__(env, state_dim, action_dim,\n                 memory_capacity, max_steps,\n                 reward_gamma, reward_scale, done_penalty,\n                 actor_hidden_size, critic_hidden_size,\n                 actor_output_act, critic_loss,\n                 actor_lr, critic_lr,\n                 optimizer_type, entropy_reg,\n                 max_grad_norm, batch_size, episodes_before_train,\n                 epsilon_start, epsilon_end, epsilon_decay,\n                 use_cuda)\n\n        self.actor = ActorNetwork(self.state_dim, self.actor_hidden_size,\n                                          self.action_dim, self.actor_output_act)\n        if self.optimizer_type == ""adam"":\n            self.actor_optimizer = Adam(self.actor.parameters(), lr=self.actor_lr)\n        elif self.optimizer_type == ""rmsprop"":\n            self.actor_optimizer = RMSprop(self.actor.parameters(), lr=self.actor_lr)\n        if self.use_cuda:\n            self.actor.cuda()\n\n    # agent interact with the environment to collect experience\n    def interact(self):\n        super(DQN, self)._take_one_step()\n\n    # train on a sample batch\n    def train(self):\n        if self.n_episodes <= self.episodes_before_train:\n            pass\n\n        batch = self.memory.sample(self.batch_size)\n        states_var = to_tensor_var(batch.states, self.use_cuda).view(-1, self.state_dim)\n        actions_var = to_tensor_var(batch.actions, self.use_cuda, ""long"").view(-1, 1)\n        rewards_var = to_tensor_var(batch.rewards, self.use_cuda).view(-1, 1)\n        next_states_var = to_tensor_var(batch.next_states, self.use_cuda).view(-1, self.state_dim)\n        dones_var = to_tensor_var(batch.dones, self.use_cuda).view(-1, 1)\n\n        # compute Q(s_t, a) - the model computes Q(s_t), then we select the\n        # columns of actions taken\n        current_q = self.actor(states_var).gather(1, actions_var)\n\n        # compute V(s_{t+1}) for all next states and all actions,\n        # and we then take max_a { V(s_{t+1}) }\n        next_state_action_values = self.actor(next_states_var).detach()\n        next_q = th.max(next_state_action_values, 1)[0].view(-1, 1)\n        # compute target q by: r + gamma * max_a { V(s_{t+1}) }\n        target_q = self.reward_scale * rewards_var + self.reward_gamma * next_q * (1. - dones_var)\n\n        # update value network\n        self.actor_optimizer.zero_grad()\n        if self.critic_loss == ""huber"":\n            loss = th.nn.functional.smooth_l1_loss(current_q, target_q)\n        else:\n            loss = th.nn.MSELoss()(current_q, target_q)\n        loss.backward()\n        if self.max_grad_norm is not None:\n            nn.utils.clip_grad_norm(self.actor.parameters(), self.max_grad_norm)\n        self.actor_optimizer.step()\n\n    # choose an action based on state with random noise added for exploration in training\n    def exploration_action(self, state):\n        epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n                                  np.exp(-1. * self.n_steps / self.epsilon_decay)\n        if np.random.rand() < epsilon:\n            action = np.random.choice(self.action_dim)\n        else:\n            action = self.action(state)\n        return action\n\n    # choose an action based on state for execution\n    def action(self, state):\n        state_var = to_tensor_var([state], self.use_cuda)\n        state_action_value_var = self.actor(state_var)\n        if self.use_cuda:\n            state_action_value = state_action_value_var.data.cpu().numpy()[0]\n        else:\n            state_action_value = state_action_value_var.data.numpy()[0]\n        action = np.argmax(state_action_value)\n        return action\n'"
MAA2C.py,1,"b'\nimport torch as th\nfrom torch import nn\nfrom torch.optim import Adam, RMSprop\n\nimport numpy as np\n\nfrom common.Agent import Agent\nfrom common.Model import ActorNetwork, CriticNetwork\nfrom common.utils import entropy, index_to_one_hot, to_tensor_var\n\n\nclass MAA2C(Agent):\n    """"""\n    An multi-agent learned with Advantage Actor-Critic\n    - Actor takes its local observations as input\n    - agent interact with environment to collect experience\n    - agent training with experience to update policy\n\n    Parameters\n    - training_strategy:\n        - cocurrent\n            - each agent learns its own individual policy which is independent\n            - multiple policies are optimized simultaneously\n        - centralized (see MADDPG in [1] for details)\n            - centralized training and decentralized execution\n            - decentralized actor map it\'s local observations to action using individual policy\n            - centralized critic takes both state and action from all agents as input, each actor\n                has its own critic for estimating the value function, which allows each actor has\n                different reward structure, e.g., cooperative, competitive, mixed task\n    - actor_parameter_sharing:\n        - True: all actors share a single policy which enables parameters and experiences sharing,\n            this is mostly useful where the agents are homogeneous. Please see Sec. 4.3 in [2] and\n            Sec. 4.1 & 4.2 in [3] for details.\n        - False: each actor use independent policy\n    - critic_parameter_sharing:\n        - True: all actors share a single critic which enables parameters and experiences sharing,\n            this is mostly useful where the agents are homogeneous and reward sharing holds. Please\n            see Sec. 4.1 in [3] for details.\n        - False: each actor use independent critic (though each critic can take other agents actions\n            as input, see MADDPG in [1] for details)\n\n    Reference:\n    [1] Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments\n    [2] Cooperative Multi-Agent Control Using Deep Reinforcement Learning\n    [3] Parameter Sharing Deep Deterministic Policy Gradient for Cooperative Multi-agent Reinforcement Learning\n\n    """"""\n    def __init__(self, env, n_agents, state_dim, action_dim,\n                 memory_capacity=10000, max_steps=None,\n                 roll_out_n_steps=10,\n                 reward_gamma=0.99, reward_scale=1., done_penalty=None,\n                 actor_hidden_size=32, critic_hidden_size=32,\n                 actor_output_act=nn.functional.log_softmax, critic_loss=""mse"",\n                 actor_lr=0.001, critic_lr=0.001,\n                 optimizer_type=""rmsprop"", entropy_reg=0.01,\n                 max_grad_norm=0.5, batch_size=100, episodes_before_train=100,\n                 epsilon_start=0.9, epsilon_end=0.01, epsilon_decay=200,\n                 use_cuda=True, training_strategy=""cocurrent"",\n                 actor_parameter_sharing=False, critic_parameter_sharing=False):\n        super(MAA2C, self).__init__(env, state_dim, action_dim,\n                 memory_capacity, max_steps,\n                 reward_gamma, reward_scale, done_penalty,\n                 actor_hidden_size, critic_hidden_size,\n                 actor_output_act, critic_loss,\n                 actor_lr, critic_lr,\n                 optimizer_type, entropy_reg,\n                 max_grad_norm, batch_size, episodes_before_train,\n                 epsilon_start, epsilon_end, epsilon_decay,\n                 use_cuda)\n\n        assert training_strategy in [""cocurrent"", ""centralized""]\n\n        self.n_agents = n_agents\n        self.roll_out_n_steps = roll_out_n_steps\n        self.training_strategy = training_strategy\n        self.actor_parameter_sharing = actor_parameter_sharing\n        self.critic_parameter_sharing = critic_parameter_sharing\n\n        self.actors = [ActorNetwork(self.state_dim, self.actor_hidden_size, self.action_dim, self.actor_output_act)] * self.n_agents\n        if self.training_strategy == ""cocurrent"":\n            self.critics = [CriticNetwork(self.state_dim, self.action_dim, self.critic_hidden_size, 1)] * self.n_agents\n        elif self.training_strategy == ""centralized"":\n            critic_state_dim = self.n_agents * self.state_dim\n            critic_action_dim = self.n_agents * self.action_dim\n            self.critics = [CriticNetwork(critic_state_dim, critic_action_dim, self.critic_hidden_size, 1)] * self.n_agents\n        if optimizer_type == ""adam"":\n            self.actor_optimizers = [Adam(a.parameters(), lr=self.actor_lr) for a in self.actors]\n            self.critic_optimizers = [Adam(c.parameters(), lr=self.critic_lr) for c in self.critics]\n        elif optimizer_type == ""rmsprop"":\n            self.actor_optimizers = [RMSprop(a.parameters(), lr=self.actor_lr) for a in self.actors]\n            self.critic_optimizers = [RMSprop(c.parameters(), lr=self.critic_lr) for c in self.critics]\n\n        # tricky and memory consumed implementation of parameter sharing\n        if self.actor_parameter_sharing:\n            for agent_id in range(1, self.n_agents):\n                self.actors[agent_id] = self.actors[0]\n                self.actor_optimizers[agent_id] = self.actor_optimizers[0]\n        if self.critic_parameter_sharing:\n            for agent_id in range(1, self.n_agents):\n                self.critics[agent_id] = self.critics[0]\n                self.critic_optimizers[agent_id] = self.critic_optimizers[0]\n\n        if self.use_cuda:\n            for a in self.actors:\n                a.cuda()\n            for c in self.critics:\n                c.cuda()\n\n    # agent interact with the environment to collect experience\n    def interact(self):\n        if (self.max_steps is not None) and (self.n_steps >= self.max_steps):\n            self.env_state = self.env.reset()\n            self.n_steps = 0\n        states = []\n        actions = []\n        rewards = []\n        # take n steps\n        for i in range(self.roll_out_n_steps):\n            states.append(self.env_state)\n            action = self.exploration_action(self.env_state)\n            next_state, reward, done, _ = self.env.step(action)\n            done = done[0]\n            actions.append([index_to_one_hot(a, self.action_dim) for a in action])\n            rewards.append(reward)\n            final_state = next_state\n            self.env_state = next_state\n            if done:\n                self.env_state = self.env.reset()\n                break\n        # discount reward\n        if done:\n            final_r = [0.0] * self.n_agents\n            self.n_episodes += 1\n            self.episode_done = True\n        else:\n            self.episode_done = False\n            final_action = self.action(final_state)\n            one_hot_action = [index_to_one_hot(a, self.action_dim) for a in final_action]\n            final_r = self.value(final_state, one_hot_action)\n\n        rewards = np.array(rewards)\n        for agent_id in range(self.n_agents):\n            rewards[:,agent_id] = self._discount_reward(rewards[:,agent_id], final_r[agent_id])\n        rewards = rewards.tolist()\n        self.n_steps += 1\n        self.memory.push(states, actions, rewards)\n\n    # train on a roll out batch\n    def train(self):\n        if self.n_episodes <= self.episodes_before_train:\n            pass\n\n        batch = self.memory.sample(self.batch_size)\n        states_var = to_tensor_var(batch.states, self.use_cuda).view(-1, self.n_agents, self.state_dim)\n        actions_var = to_tensor_var(batch.actions, self.use_cuda).view(-1, self.n_agents, self.action_dim)\n        rewards_var = to_tensor_var(batch.rewards, self.use_cuda).view(-1, self.n_agents, 1)\n        whole_states_var = states_var.view(-1, self.n_agents*self.state_dim)\n        whole_actions_var = actions_var.view(-1, self.n_agents*self.action_dim)\n\n        for agent_id in range(self.n_agents):\n            # update actor network\n            self.actor_optimizers[agent_id].zero_grad()\n            action_log_probs = self.actors[agent_id](states_var[:,agent_id,:])\n            entropy_loss = th.mean(entropy(th.exp(action_log_probs)))\n            action_log_probs = th.sum(action_log_probs * actions_var[:,agent_id,:], 1)\n            if self.training_strategy == ""cocurrent"":\n                values = self.critics[agent_id](states_var[:,agent_id,:], actions_var[:,agent_id,:])\n            elif self.training_strategy == ""centralized"":\n                values = self.critics[agent_id](whole_states_var, whole_actions_var)\n            advantages = rewards_var[:,agent_id,:] - values.detach()\n            pg_loss = -th.mean(action_log_probs * advantages)\n            actor_loss = pg_loss - entropy_loss * self.entropy_reg\n            actor_loss.backward()\n            if self.max_grad_norm is not None:\n                nn.utils.clip_grad_norm(self.actors[agent_id].parameters(), self.max_grad_norm)\n            self.actor_optimizers[agent_id].step()\n\n            # update critic network\n            self.critic_optimizers[agent_id].zero_grad()\n            target_values = rewards_var[:,agent_id,:]\n            if self.critic_loss == ""huber"":\n                critic_loss = nn.functional.smooth_l1_loss(values, target_values)\n            else:\n                critic_loss = nn.MSELoss()(values, target_values)\n            critic_loss.backward()\n            if self.max_grad_norm is not None:\n                nn.utils.clip_grad_norm(self.critics[agent_id].parameters(), self.max_grad_norm)\n            self.critic_optimizers[agent_id].step()\n\n    # predict softmax action based on state\n    def _softmax_action(self, state):\n        state_var = to_tensor_var([state], self.use_cuda)\n        softmax_action = np.zeros((self.n_agents, self.action_dim), dtype=np.float64)\n        for agent_id in range(self.n_agents):\n            softmax_action_var = th.exp(self.actors[agent_id](state_var[:,agent_id,:]))\n            if self.use_cuda:\n                softmax_action[agent_id] = softmax_action_var.data.cpu().numpy()[0]\n            else:\n                softmax_action[agent_id] = softmax_action_var.data.numpy()[0]\n        return softmax_action\n\n    # predict action based on state, added random noise for exploration in training\n    def exploration_action(self, state):\n        softmax_action = self._softmax_action(state)\n        actions = [0]*self.n_agents\n        epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n                                     np.exp(-1. * self.n_steps / self.epsilon_decay)\n        for agent_id in range(self.n_agents):\n            if np.random.rand() < epsilon:\n                actions[agent_id] = np.random.choice(self.action_dim)\n            else:\n                actions[agent_id] = np.argmax(softmax_action[agent_id])\n        return actions\n\n    # predict action based on state for execution\n    def action(self, state):\n        softmax_actions = self._softmax_action(state)\n        actions = np.argmax(softmax_actions, axis=1)\n        return actions\n\n    # evaluate value\n    def value(self, state, action):\n        state_var = to_tensor_var([state], self.use_cuda)\n        action_var = to_tensor_var([action], self.use_cuda)\n        whole_state_var = state_var.view(-1, self.n_agents*self.state_dim)\n        whole_action_var = action_var.view(-1, self.n_agents*self.action_dim)\n        values = [0]*self.n_agents\n        for agent_id in range(self.n_agents):\n            if self.training_strategy == ""cocurrent"":\n                value_var = self.critics[agent_id](state_var[:,agent_id,:], action_var[:,agent_id,:])\n            elif self.training_strategy == ""centralized"":\n                value_var = self.critics[agent_id](whole_state_var, whole_action_var)\n            if self.use_cuda:\n                values[agent_id] = value_var.data.cpu().numpy()[0]\n            else:\n                values[agent_id] = value_var.data.numpy()[0]\n        return values\n'"
MADDPG.py,2,"b'\nimport torch as th\nimport torch.nn as nn\nfrom torch.optim import Adam, RMSprop\n\nimport numpy as np\nfrom copy import deepcopy\n\nfrom common.Memory import ReplayMemory\nfrom common.Model import ActorNetwork, CriticNetwork\nfrom common.utils import to_tensor_var\n\n\nclass MADDPG(object):\n    """"""\n    An multi-agent learned with Deep Deterministic Policy Gradient using Actor-Critic framework\n    - Actor takes state as input\n    - Critic takes both state and action as input\n    """"""\n    def __init__(self):\n        """"""\n        TODO\n        """"""\n        pass\n'"
MADQN.py,1,"b'\nimport torch as th\nfrom torch import nn\nfrom torch.optim import Adam, RMSprop\n\nimport numpy as np\n\nfrom common.Memory import ReplayMemory\nfrom common.Model import ActorNetwork\nfrom common.utils import to_tensor_var\n\n\nclass MADQN(object):\n    """"""\n    An multi-agent learned with DQN using replay memory and temporal difference\n    - use a value network to estimate the state-action value\n    """"""\n    def __init__(self):\n        """"""\n        TODO\n        """"""\n        pass\n'"
PPO.py,1,"b'\nimport torch as th\nfrom torch import nn\nfrom torch.optim import Adam, RMSprop\n\nimport numpy as np\nfrom copy import deepcopy\n\nfrom common.Agent import Agent\nfrom common.Model import ActorNetwork, CriticNetwork\nfrom common.utils import index_to_one_hot, to_tensor_var\n\n\nclass PPO(Agent):\n    """"""\n    An agent learned with PPO using Advantage Actor-Critic framework\n    - Actor takes state as input\n    - Critic takes both state and action as input\n    - agent interact with environment to collect experience\n    - agent training with experience to update policy\n    - adam seems better than rmsprop for ppo\n    """"""\n    def __init__(self, env, state_dim, action_dim,\n                 memory_capacity=10000, max_steps=None,\n                 roll_out_n_steps=1, target_tau=1.,\n                 target_update_steps=5, clip_param=0.2,\n                 reward_gamma=0.99, reward_scale=1., done_penalty=None,\n                 actor_hidden_size=32, critic_hidden_size=32,\n                 actor_output_act=nn.functional.log_softmax, critic_loss=""mse"",\n                 actor_lr=0.001, critic_lr=0.001,\n                 optimizer_type=""adam"", entropy_reg=0.01,\n                 max_grad_norm=0.5, batch_size=100, episodes_before_train=100,\n                 epsilon_start=0.9, epsilon_end=0.01, epsilon_decay=200,\n                 use_cuda=True):\n        super(PPO, self).__init__(env, state_dim, action_dim,\n                 memory_capacity, max_steps,\n                 reward_gamma, reward_scale, done_penalty,\n                 actor_hidden_size, critic_hidden_size,\n                 actor_output_act, critic_loss,\n                 actor_lr, critic_lr,\n                 optimizer_type, entropy_reg,\n                 max_grad_norm, batch_size, episodes_before_train,\n                 epsilon_start, epsilon_end, epsilon_decay,\n                 use_cuda)\n\n        self.roll_out_n_steps = roll_out_n_steps\n        self.target_tau = target_tau\n        self.target_update_steps = target_update_steps\n        self.clip_param = clip_param\n\n        self.actor = ActorNetwork(self.state_dim, self.actor_hidden_size,\n                                  self.action_dim, self.actor_output_act)\n        self.critic = CriticNetwork(self.state_dim, self.action_dim, self.critic_hidden_size, 1)\n        # to ensure target network and learning network has the same weights\n        self.actor_target = deepcopy(self.actor)\n        self.critic_target = deepcopy(self.critic)\n\n        if self.optimizer_type == ""adam"":\n            self.actor_optimizer = Adam(self.actor.parameters(), lr=self.actor_lr)\n            self.critic_optimizer = Adam(self.critic.parameters(), lr=self.critic_lr)\n        elif self.optimizer_type == ""rmsprop"":\n            self.actor_optimizer = RMSprop(self.actor.parameters(), lr=self.actor_lr)\n            self.critic_optimizer = RMSprop(self.critic.parameters(), lr=self.critic_lr)\n\n        if self.use_cuda:\n            self.actor.cuda()\n            self.critic.cuda()\n            self.actor_target.cuda()\n            self.critic_target.cuda()\n\n    # agent interact with the environment to collect experience\n    def interact(self):\n        super(PPO, self)._take_n_steps()\n\n    # train on a roll out batch\n    def train(self):\n        if self.n_episodes <= self.episodes_before_train:\n            pass\n\n        batch = self.memory.sample(self.batch_size)\n        states_var = to_tensor_var(batch.states, self.use_cuda).view(-1, self.state_dim)\n        one_hot_actions = index_to_one_hot(batch.actions, self.action_dim)\n        actions_var = to_tensor_var(one_hot_actions, self.use_cuda).view(-1, self.action_dim)\n        rewards_var = to_tensor_var(batch.rewards, self.use_cuda).view(-1, 1)\n\n        # update actor network\n        self.actor_optimizer.zero_grad()\n        values = self.critic_target(states_var, actions_var).detach()\n        advantages = rewards_var - values\n        # # normalizing advantages seems not working correctly here\n        # advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-5)\n        action_log_probs = self.actor(states_var)\n        action_log_probs = th.sum(action_log_probs * actions_var, 1)\n        old_action_log_probs = self.actor_target(states_var).detach()\n        old_action_log_probs = th.sum(old_action_log_probs * actions_var, 1)\n        ratio = th.exp(action_log_probs - old_action_log_probs)\n        surr1 = ratio * advantages\n        surr2 = th.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * advantages\n        # PPO\'s pessimistic surrogate (L^CLIP)\n        actor_loss = -th.mean(th.min(surr1, surr2))\n        actor_loss.backward()\n        if self.max_grad_norm is not None:\n            nn.utils.clip_grad_norm(self.actor.parameters(), self.max_grad_norm)\n        self.actor_optimizer.step()\n\n        # update critic network\n        self.critic_optimizer.zero_grad()\n        target_values = rewards_var\n        values = self.critic(states_var, actions_var)\n        if self.critic_loss == ""huber"":\n            critic_loss = nn.functional.smooth_l1_loss(values, target_values)\n        else:\n            critic_loss = nn.MSELoss()(values, target_values)\n        critic_loss.backward()\n        if self.max_grad_norm is not None:\n            nn.utils.clip_grad_norm(self.critic.parameters(), self.max_grad_norm)\n        self.critic_optimizer.step()\n\n        # update actor target network and critic target network\n        if self.n_steps % self.target_update_steps == 0 and self.n_steps > 0:\n            super(PPO, self)._soft_update_target(self.actor_target, self.actor)\n            super(PPO, self)._soft_update_target(self.critic_target, self.critic)\n\n    # predict softmax action based on state\n    def _softmax_action(self, state):\n        state_var = to_tensor_var([state], self.use_cuda)\n        softmax_action_var = th.exp(self.actor(state_var))\n        if self.use_cuda:\n            softmax_action = softmax_action_var.data.cpu().numpy()[0]\n        else:\n            softmax_action = softmax_action_var.data.numpy()[0]\n        return softmax_action\n\n    # choose an action based on state with random noise added for exploration in training\n    def exploration_action(self, state):\n        softmax_action = self._softmax_action(state)\n        epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n                                  np.exp(-1. * self.n_steps / self.epsilon_decay)\n        if np.random.rand() < epsilon:\n            action = np.random.choice(self.action_dim)\n        else:\n            action = np.argmax(softmax_action)\n        return action\n\n    # choose an action based on state for execution\n    def action(self, state):\n        softmax_action = self._softmax_action(state)\n        action = np.argmax(softmax_action)\n        return action\n\n    # evaluate value for a state-action pair\n    def value(self, state, action):\n        state_var = to_tensor_var([state], self.use_cuda)\n        action = index_to_one_hot(action, self.action_dim)\n        action_var = to_tensor_var([action], self.use_cuda)\n        value_var = self.critic(state_var, action_var)\n        if self.use_cuda:\n            value = value_var.data.cpu().numpy()[0]\n        else:\n            value = value_var.data.numpy()[0]\n        return value\n'"
run_a2c.py,0,"b'\nfrom A2C import A2C\nfrom common.utils import agg_double_list\n\nimport sys\nimport gym\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nMAX_EPISODES = 5000\nEPISODES_BEFORE_TRAIN = 0\nEVAL_EPISODES = 10\nEVAL_INTERVAL = 100\n\n# roll out n steps\nROLL_OUT_N_STEPS = 10\n# only remember the latest ROLL_OUT_N_STEPS\nMEMORY_CAPACITY = ROLL_OUT_N_STEPS\n# only use the latest ROLL_OUT_N_STEPS for training A2C\nBATCH_SIZE = ROLL_OUT_N_STEPS\n\nREWARD_DISCOUNTED_GAMMA = 0.99\nENTROPY_REG = 0.00\n#\nDONE_PENALTY = -10.\n\nCRITIC_LOSS = ""mse""\nMAX_GRAD_NORM = None\n\nEPSILON_START = 0.99\nEPSILON_END = 0.05\nEPSILON_DECAY = 500\n\nRANDOM_SEED = 2017\n\n\ndef run(env_id=""CartPole-v0""):\n\n    env = gym.make(env_id)\n    env.seed(RANDOM_SEED)\n    env_eval = gym.make(env_id)\n    env_eval.seed(RANDOM_SEED)\n    state_dim = env.observation_space.shape[0]\n    if len(env.action_space.shape) > 1:\n        action_dim = env.action_space.shape[0]\n    else:\n        action_dim = env.action_space.n\n\n    a2c = A2C(env=env, memory_capacity=MEMORY_CAPACITY,\n              state_dim=state_dim, action_dim=action_dim,\n              batch_size=BATCH_SIZE, entropy_reg=ENTROPY_REG,\n              done_penalty=DONE_PENALTY, roll_out_n_steps=ROLL_OUT_N_STEPS,\n              reward_gamma=REWARD_DISCOUNTED_GAMMA,\n              epsilon_start=EPSILON_START, epsilon_end=EPSILON_END,\n              epsilon_decay=EPSILON_DECAY, max_grad_norm=MAX_GRAD_NORM,\n              episodes_before_train=EPISODES_BEFORE_TRAIN,\n              critic_loss=CRITIC_LOSS)\n\n    episodes =[]\n    eval_rewards =[]\n    while a2c.n_episodes < MAX_EPISODES:\n        a2c.interact()\n        if a2c.n_episodes >= EPISODES_BEFORE_TRAIN:\n            a2c.train()\n        if a2c.episode_done and ((a2c.n_episodes+1)%EVAL_INTERVAL == 0):\n            rewards, _ = a2c.evaluation(env_eval, EVAL_EPISODES)\n            rewards_mu, rewards_std = agg_double_list(rewards)\n            print(""Episode %d, Average Reward %.2f"" % (a2c.n_episodes+1, rewards_mu))\n            episodes.append(a2c.n_episodes+1)\n            eval_rewards.append(rewards_mu)\n\n    episodes = np.array(episodes)\n    eval_rewards = np.array(eval_rewards)\n    np.savetxt(""./output/%s_a2c_episodes.txt""%env_id, episodes)\n    np.savetxt(""./output/%s_a2c_eval_rewards.txt""%env_id, eval_rewards)\n\n    plt.figure()\n    plt.plot(episodes, eval_rewards)\n    plt.title(""%s""%env_id)\n    plt.xlabel(""Episode"")\n    plt.ylabel(""Average Reward"")\n    plt.legend([""A2C""])\n    plt.savefig(""./output/%s_a2c.png""%env_id)\n\n\nif __name__ == ""__main__"":\n    if len(sys.argv) >= 2:\n        run(sys.argv[1])\n    else:\n        run()\n'"
run_acktr.py,0,"b'\nfrom ACKTR import DisjointACKTR as ACKTR\nfrom ACKTR import JointACKTR as ACKTR\nfrom common.utils import agg_double_list\n\nimport sys\nimport gym\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nMAX_EPISODES = 5000\nEPISODES_BEFORE_TRAIN = 0\nEVAL_EPISODES = 10\nEVAL_INTERVAL = 100\n\n# roll out n steps\nROLL_OUT_N_STEPS = 10\n# only remember the latest ROLL_OUT_N_STEPS\nMEMORY_CAPACITY = ROLL_OUT_N_STEPS\n# only use the latest ROLL_OUT_N_STEPS for training A2C\nBATCH_SIZE = ROLL_OUT_N_STEPS\n\nREWARD_DISCOUNTED_GAMMA = 0.99\nENTROPY_REG = 0.00\n#\nDONE_PENALTY = -10.\n\nCRITIC_LOSS = ""mse""\nMAX_GRAD_NORM = None\n\nEPSILON_START = 0.99\nEPSILON_END = 0.05\nEPSILON_DECAY = 500\n\nRANDOM_SEED = 2017\n\n\ndef run(env_id=""CartPole-v0""):\n\n    env = gym.make(env_id)\n    env.seed(RANDOM_SEED)\n    env_eval = gym.make(env_id)\n    env_eval.seed(RANDOM_SEED)\n    state_dim = env.observation_space.shape[0]\n    if len(env.action_space.shape) > 1:\n        action_dim = env.action_space.shape[0]\n    else:\n        action_dim = env.action_space.n\n\n    acktr = ACKTR(env=env, memory_capacity=MEMORY_CAPACITY,\n              state_dim=state_dim, action_dim=action_dim,\n              batch_size=BATCH_SIZE, entropy_reg=ENTROPY_REG,\n              done_penalty=DONE_PENALTY, roll_out_n_steps=ROLL_OUT_N_STEPS,\n              reward_gamma=REWARD_DISCOUNTED_GAMMA,\n              epsilon_start=EPSILON_START, epsilon_end=EPSILON_END,\n              epsilon_decay=EPSILON_DECAY, max_grad_norm=MAX_GRAD_NORM,\n              episodes_before_train=EPISODES_BEFORE_TRAIN,\n              critic_loss=CRITIC_LOSS)\n\n    episodes =[]\n    eval_rewards =[]\n    while acktr.n_episodes < MAX_EPISODES:\n        acktr.interact()\n        if acktr.n_episodes >= EPISODES_BEFORE_TRAIN:\n            acktr.train()\n        if acktr.episode_done and ((acktr.n_episodes+1)%EVAL_INTERVAL == 0):\n            rewards, _ = acktr.evaluation(env_eval, EVAL_EPISODES)\n            rewards_mu, rewards_std = agg_double_list(rewards)\n            print(""Episode %d, Average Reward %.2f"" % (acktr.n_episodes+1, rewards_mu))\n            episodes.append(acktr.n_episodes+1)\n            eval_rewards.append(rewards_mu)\n\n    episodes = np.array(episodes)\n    eval_rewards = np.array(eval_rewards)\n    np.savetxt(""./output/%s_acktr_episodes.txt""%env_id, episodes)\n    np.savetxt(""./output/%s_acktr_eval_rewards.txt""%env_id, eval_rewards)\n\n    plt.figure()\n    plt.plot(episodes, eval_rewards)\n    plt.title(""%s"" % env_id)\n    plt.xlabel(""Episode"")\n    plt.ylabel(""Average Reward"")\n    plt.legend([""ACKTR""])\n    plt.savefig(""./output/%s_acktr.png""%env_id)\n\n\nif __name__ == ""__main__"":\n    if len(sys.argv) >= 2:\n        run(sys.argv[1])\n    else:\n        run()\n'"
run_ddpg.py,0,"b'\nfrom DDPG import DDPG\nfrom common.utils import agg_double_list\n\nimport gym\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nMAX_EPISODES = 5000\nEPISODES_BEFORE_TRAIN = 100\nEVAL_EPISODES = 10\nEVAL_INTERVAL = 100\n\n# max steps in each episode, prevent from running too long\nMAX_STEPS = 10000 # None\n\nMEMORY_CAPACITY = 10000\nBATCH_SIZE = 100\nCRITIC_LOSS = ""mse""\nMAX_GRAD_NORM = None\n\nTARGET_UPDATE_STEPS = 5\nTARGET_TAU = 0.01\n\nREWARD_DISCOUNTED_GAMMA = 0.99\n\nEPSILON_START = 0.99\nEPSILON_END = 0.05\nEPSILON_DECAY = 500\n\nDONE_PENALTY = None\n\nRANDOM_SEED = 2017\n\n\ndef run(env_id=""Pendulum-v0""):\n\n    env = gym.make(env_id)\n    env.seed(RANDOM_SEED)\n    env_eval = gym.make(env_id)\n    env_eval.seed(RANDOM_SEED)\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.shape[0]\n\n    ddpg = DDPG(env=env, memory_capacity=MEMORY_CAPACITY,\n                state_dim=state_dim, action_dim=action_dim,\n                batch_size=BATCH_SIZE, max_steps=MAX_STEPS,\n                done_penalty=DONE_PENALTY,\n                target_update_steps=TARGET_UPDATE_STEPS, target_tau=TARGET_TAU,\n                reward_gamma=REWARD_DISCOUNTED_GAMMA, critic_loss=CRITIC_LOSS,\n                epsilon_start=EPSILON_START, epsilon_end=EPSILON_END,\n                epsilon_decay=EPSILON_DECAY, max_grad_norm=MAX_GRAD_NORM,\n                episodes_before_train=EPISODES_BEFORE_TRAIN)\n\n    episodes =[]\n    eval_rewards =[]\n    while ddpg.n_episodes < MAX_EPISODES:\n        ddpg.interact()\n        if ddpg.n_episodes >= EPISODES_BEFORE_TRAIN:\n            ddpg.train()\n        if ddpg.episode_done and ((ddpg.n_episodes+1)%EVAL_INTERVAL == 0):\n            rewards, _ = ddpg.evaluation(env_eval, EVAL_EPISODES)\n            rewards_mu, rewards_std = agg_double_list(rewards)\n            print(""Episode: %d, Average Reward: %.5f"" % (ddpg.n_episodes+1, rewards_mu))\n            episodes.append(ddpg.n_episodes+1)\n            eval_rewards.append(rewards_mu)\n\n    episodes = np.array(episodes)\n    eval_rewards = np.array(eval_rewards)\n    np.savetxt(""./output/%s_ddpg_episodes.txt""%env_id, episodes)\n    np.savetxt(""./output/%s_ddpg_eval_rewards.txt""%env_id, eval_rewards)\n\n    plt.figure()\n    plt.plot(episodes, eval_rewards)\n    plt.title(""%s"" % env_id)\n    plt.xlabel(""Episode"")\n    plt.ylabel(""Average Reward"")\n    plt.legend([""DDPG""])\n    plt.savefig(""./output/%s_ddpg.png""%env_id)\n\n\nif __name__ == ""__main__"":\n    if len(sys.argv) >= 2:\n        run(sys.argv[1])\n    else:\n        run()\n'"
run_dqn.py,0,"b'\nfrom DQN import DQN\nfrom common.utils import agg_double_list\n\nimport sys\nimport gym\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nMAX_EPISODES = 5000\nEPISODES_BEFORE_TRAIN = 0\nEVAL_EPISODES = 10\nEVAL_INTERVAL = 100\n\n# max steps in each episode, prevent from running too long\nMAX_STEPS = 10000 # None\n\nMEMORY_CAPACITY = 10000\nBATCH_SIZE = 100\nCRITIC_LOSS = ""mse""\nMAX_GRAD_NORM = None\n\nREWARD_DISCOUNTED_GAMMA = 0.99\n\nEPSILON_START = 0.99\nEPSILON_END = 0.05\nEPSILON_DECAY = 500\n\nDONE_PENALTY = -10.\n\nRANDOM_SEED = 2017\n\n\ndef run(env_id=""CartPole-v0""):\n\n    env = gym.make(env_id)\n    env.seed(RANDOM_SEED)\n    env_eval = gym.make(env_id)\n    env_eval.seed(RANDOM_SEED)\n    state_dim = env.observation_space.shape[0]\n    if len(env.action_space.shape) > 1:\n        action_dim = env.action_space.shape[0]\n    else:\n        action_dim = env.action_space.n\n\n    dqn = DQN(env=env, memory_capacity=MEMORY_CAPACITY,\n              state_dim=state_dim, action_dim=action_dim,\n              batch_size=BATCH_SIZE, max_steps=MAX_STEPS,\n              done_penalty=DONE_PENALTY, critic_loss=CRITIC_LOSS,\n              reward_gamma=REWARD_DISCOUNTED_GAMMA,\n              epsilon_start=EPSILON_START, epsilon_end=EPSILON_END,\n              epsilon_decay=EPSILON_DECAY, max_grad_norm=MAX_GRAD_NORM,\n              episodes_before_train=EPISODES_BEFORE_TRAIN)\n\n    episodes =[]\n    eval_rewards =[]\n    while dqn.n_episodes < MAX_EPISODES:\n        dqn.interact()\n        if dqn.n_episodes >= EPISODES_BEFORE_TRAIN:\n            dqn.train()\n        if dqn.episode_done and ((dqn.n_episodes+1)%EVAL_INTERVAL == 0):\n            rewards, _ = dqn.evaluation(env_eval, EVAL_EPISODES)\n            rewards_mu, rewards_std = agg_double_list(rewards)\n            print(""Episode %d, Average Reward %.2f"" % (dqn.n_episodes+1, rewards_mu))\n            episodes.append(dqn.n_episodes+1)\n            eval_rewards.append(rewards_mu)\n\n    episodes = np.array(episodes)\n    eval_rewards = np.array(eval_rewards)\n    np.savetxt(""./output/%s_dqn_episodes.txt""%env_id, episodes)\n    np.savetxt(""./output/%s_dqn_eval_rewards.txt""%env_id, eval_rewards)\n\n    plt.figure()\n    plt.plot(episodes, eval_rewards)\n    plt.title(""%s"" % env_id)\n    plt.xlabel(""Episode"")\n    plt.ylabel(""Average Reward"")\n    plt.legend([""DQN""])\n    plt.savefig(""./output/%s_dqn.png""%env_id)\n\n\nif __name__ == ""__main__"":\n    if len(sys.argv) >= 2:\n        run(sys.argv[1])\n    else:\n        run()\n'"
run_ppo.py,0,"b'\nfrom PPO import PPO\nfrom common.utils import agg_double_list\n\nimport sys\nimport gym\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nMAX_EPISODES = 5000\nEPISODES_BEFORE_TRAIN = 0\nEVAL_EPISODES = 10\nEVAL_INTERVAL = 100\n\n# roll out n steps\nROLL_OUT_N_STEPS = 10\n# only remember the latest ROLL_OUT_N_STEPS\nMEMORY_CAPACITY = ROLL_OUT_N_STEPS\n# only use the latest ROLL_OUT_N_STEPS for training PPO\nBATCH_SIZE = ROLL_OUT_N_STEPS\n\nTARGET_UPDATE_STEPS = 5\nTARGET_TAU = 1.0\n\nREWARD_DISCOUNTED_GAMMA = 0.99\nENTROPY_REG = 0.00\n#\nDONE_PENALTY = -10.\n\nCRITIC_LOSS = ""mse""\nMAX_GRAD_NORM = None\n\nEPSILON_START = 0.99\nEPSILON_END = 0.05\nEPSILON_DECAY = 500\n\nRANDOM_SEED = 2017\n\n\ndef run(env_id=""CartPole-v0""):\n\n    env = gym.make(env_id)\n    env.seed(RANDOM_SEED)\n    env_eval = gym.make(env_id)\n    env_eval.seed(RANDOM_SEED)\n    state_dim = env.observation_space.shape[0]\n    if len(env.action_space.shape) > 1:\n        action_dim = env.action_space.shape[0]\n    else:\n        action_dim = env.action_space.n\n\n    ppo = PPO(env=env, memory_capacity=MEMORY_CAPACITY,\n              state_dim=state_dim, action_dim=action_dim,\n              batch_size=BATCH_SIZE, entropy_reg=ENTROPY_REG,\n              done_penalty=DONE_PENALTY, roll_out_n_steps=ROLL_OUT_N_STEPS,\n              target_update_steps=TARGET_UPDATE_STEPS, target_tau=TARGET_TAU,\n              reward_gamma=REWARD_DISCOUNTED_GAMMA,\n              epsilon_start=EPSILON_START, epsilon_end=EPSILON_END,\n              epsilon_decay=EPSILON_DECAY, max_grad_norm=MAX_GRAD_NORM,\n              episodes_before_train=EPISODES_BEFORE_TRAIN,\n              critic_loss=CRITIC_LOSS)\n\n    episodes =[]\n    eval_rewards =[]\n    while ppo.n_episodes < MAX_EPISODES:\n        ppo.interact()\n        if ppo.n_episodes >= EPISODES_BEFORE_TRAIN:\n            ppo.train()\n        if ppo.episode_done and ((ppo.n_episodes+1)%EVAL_INTERVAL == 0):\n            rewards, _ = ppo.evaluation(env_eval, EVAL_EPISODES)\n            rewards_mu, rewards_std = agg_double_list(rewards)\n            print(""Episode %d, Average Reward %.2f"" % (ppo.n_episodes+1, rewards_mu))\n            episodes.append(ppo.n_episodes+1)\n            eval_rewards.append(rewards_mu)\n\n    episodes = np.array(episodes)\n    eval_rewards = np.array(eval_rewards)\n    np.savetxt(""./output/%s_ppo_episodes.txt""%env_id, episodes)\n    np.savetxt(""./output/%s_ppo_eval_rewards.txt""%env_id, eval_rewards)\n\n    plt.figure()\n    plt.plot(episodes, eval_rewards)\n    plt.title(""%s"" % env_id)\n    plt.xlabel(""Episode"")\n    plt.ylabel(""Average Reward"")\n    plt.legend([""PPO""])\n    plt.savefig(""./output/%s_ppo.png""%env_id)\n\n\nif __name__ == ""__main__"":\n    if len(sys.argv) >= 2:\n        run(sys.argv[1])\n    else:\n        run()\n'"
common/Agent.py,0,"b'\nimport torch as th\n\nimport numpy as np\n\nfrom common.Memory import ReplayMemory\nfrom common.utils import identity\n\n\nclass Agent(object):\n    """"""\n    A unified agent interface:\n    - interact: interact with the environment to collect experience\n        - _take_one_step: take one step\n        - _take_n_steps: take n steps\n        - _discount_reward: discount roll out rewards\n    - train: train on a sample batch\n        - _soft_update_target: soft update the target network\n    - exploration_action: choose an action based on state with random noise\n                            added for exploration in training\n    - action: choose an action based on state for execution\n    - value: evaluate value for a state-action pair\n    - evaluation: evaluation a learned agent\n    """"""\n    def __init__(self, env, state_dim, action_dim,\n                 memory_capacity=10000, max_steps=10000,\n                 reward_gamma=0.99, reward_scale=1., done_penalty=None,\n                 actor_hidden_size=32, critic_hidden_size=32,\n                 actor_output_act=identity, critic_loss=""mse"",\n                 actor_lr=0.01, critic_lr=0.01,\n                 optimizer_type=""rmsprop"", entropy_reg=0.01,\n                 max_grad_norm=0.5, batch_size=100, episodes_before_train=100,\n                 epsilon_start=0.9, epsilon_end=0.01, epsilon_decay=200,\n                 use_cuda=True):\n\n        self.env = env\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.env_state = self.env.reset()\n        self.n_episodes = 0\n        self.n_steps = 0\n        self.max_steps = max_steps\n        self.roll_out_n_steps = 1\n\n        self.reward_gamma = reward_gamma\n        self.reward_scale = reward_scale\n        self.done_penalty = done_penalty\n\n        self.memory = ReplayMemory(memory_capacity)\n        self.actor_hidden_size = actor_hidden_size\n        self.critic_hidden_size = critic_hidden_size\n        self.actor_output_act = actor_output_act\n        self.critic_loss = critic_loss\n        self.actor_lr = actor_lr\n        self.critic_lr = critic_lr\n        self.optimizer_type = optimizer_type\n        self.entropy_reg = entropy_reg\n        self.max_grad_norm = max_grad_norm\n        self.batch_size = batch_size\n        self.episodes_before_train = episodes_before_train\n        self.target_tau = 0.01\n\n        # params for epsilon greedy\n        self.epsilon_start = epsilon_start\n        self.epsilon_end = epsilon_end\n        self.epsilon_decay = epsilon_decay\n\n        self.use_cuda = use_cuda and th.cuda.is_available()\n\n    # agent interact with the environment to collect experience\n    def interact(self):\n        pass\n\n    # take one step\n    def _take_one_step(self):\n        if (self.max_steps is not None) and (self.n_steps >= self.max_steps):\n            self.env_state = self.env.reset()\n            self.n_steps = 0\n        state = self.env_state\n        action = self.exploration_action(self.env_state)\n        next_state, reward, done, _ = self.env.step(action)\n        if done:\n            if self.done_penalty is not None:\n                reward = self.done_penalty\n            next_state = [0] * len(state)\n            self.env_state = self.env.reset()\n            self.n_episodes += 1\n            self.episode_done = True\n        else:\n            self.env_state = next_state\n            self.episode_done = False\n        self.n_steps += 1\n        self.memory.push(state, action, reward, next_state, done)\n\n    # take n steps\n    def _take_n_steps(self):\n        if (self.max_steps is not None) and (self.n_steps >= self.max_steps):\n            self.env_state = self.env.reset()\n            self.n_steps = 0\n        states = []\n        actions = []\n        rewards = []\n        # take n steps\n        for i in range(self.roll_out_n_steps):\n            states.append(self.env_state)\n            action = self.exploration_action(self.env_state)\n            next_state, reward, done, _ = self.env.step(action)\n            actions.append(action)\n            if done and self.done_penalty is not None:\n                reward = self.done_penalty\n            rewards.append(reward)\n            final_state = next_state\n            self.env_state = next_state\n            if done:\n                self.env_state = self.env.reset()\n                break\n        # discount reward\n        if done:\n            final_value = 0.0\n            self.n_episodes += 1\n            self.episode_done = True\n        else:\n            self.episode_done = False\n            final_action = self.action(final_state)\n            final_value = self.value(final_state, final_action)\n        rewards = self._discount_reward(rewards, final_value)\n        self.n_steps += 1\n        self.memory.push(states, actions, rewards)\n\n    # discount roll out rewards\n    def _discount_reward(self, rewards, final_value):\n        discounted_r = np.zeros_like(rewards)\n        running_add = final_value\n        for t in reversed(range(0, len(rewards))):\n            running_add = running_add * self.reward_gamma + rewards[t]\n            discounted_r[t] = running_add\n        return discounted_r\n\n    # soft update the actor target network or critic target network\n    def _soft_update_target(self, target, source):\n        for t, s in zip(target.parameters(), source.parameters()):\n            t.data.copy_(\n                (1. - self.target_tau) * t.data + self.target_tau * s.data)\n\n    # train on a sample batch\n    def train(self):\n        pass\n\n    # choose an action based on state with random noise added for exploration in training\n    def exploration_action(self, state):\n        pass\n\n    # choose an action based on state for execution\n    def action(self, state):\n        pass\n\n    # evaluate value for a state-action pair\n    def value(self, state, action):\n        pass\n\n    # evaluation the learned agent\n    def evaluation(self, env, eval_episodes=10):\n        rewards = []\n        infos = []\n        for i in range(eval_episodes):\n            rewards_i = []\n            infos_i = []\n            state = env.reset()\n            action = self.action(state)\n            state, reward, done, info = env.step(action)\n            done = done[0] if isinstance(done, list) else done\n            rewards_i.append(reward)\n            infos_i.append(info)\n            while not done:\n                action = self.action(state)\n                state, reward, done, info = env.step(action)\n                done = done[0] if isinstance(done, list) else done\n                rewards_i.append(reward)\n                infos_i.append(info)\n            rewards.append(rewards_i)\n            infos.append(infos_i)\n        return rewards, infos\n'"
common/Memory.py,0,"b'\nimport random\nfrom collections import namedtuple\n\n\nExperience = namedtuple(""Experience"",\n                        (""states"", ""actions"", ""rewards"", ""next_states"", ""dones""))\n\n\nclass ReplayMemory(object):\n    """"""\n    Replay memory buffer\n    """"""\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.memory = []\n        self.position = 0\n\n    def _push_one(self, state, action, reward, next_state=None, done=None):\n        if len(self.memory) < self.capacity:\n            self.memory.append(None)\n        self.memory[self.position] = Experience(state, action, reward, next_state, done)\n        self.position = (self.position + 1) % self.capacity\n\n    def push(self, states, actions, rewards, next_states=None, dones=None):\n        if isinstance(states, list):\n            if next_states is not None and len(next_states) > 0:\n                for s,a,r,n_s,d in zip(states, actions, rewards, next_states, dones):\n                    self._push_one(s, a, r, n_s, d)\n            else:\n                for s,a,r in zip(states, actions, rewards):\n                    self._push_one(s, a, r)\n        else:\n            self._push_one(states, actions, rewards, next_states, dones)\n\n    def sample(self, batch_size):\n        if batch_size > len(self.memory):\n            batch_size = len(self.memory)\n        transitions = random.sample(self.memory, batch_size)\n        batch = Experience(*zip(*transitions))\n        return batch\n\n    def __len__(self):\n        return len(self.memory)\n'"
common/Model.py,0,"b'\nimport torch as th\nfrom torch import nn\n\n\nclass ActorNetwork(nn.Module):\n    """"""\n    A network for actor\n    """"""\n    def __init__(self, state_dim, hidden_size, output_size, output_act):\n        super(ActorNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_dim, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, output_size)\n        # activation function for the output\n        self.output_act = output_act\n\n    def __call__(self, state):\n        out = nn.functional.relu(self.fc1(state))\n        out = nn.functional.relu(self.fc2(out))\n        out = self.output_act(self.fc3(out))\n        return out\n\n\nclass CriticNetwork(nn.Module):\n    """"""\n    A network for critic\n    """"""\n    def __init__(self, state_dim, action_dim, hidden_size, output_size=1):\n        super(CriticNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_dim, hidden_size)\n        self.fc2 = nn.Linear(hidden_size + action_dim, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, output_size)\n\n    def __call__(self, state, action):\n        out = nn.functional.relu(self.fc1(state))\n        out = th.cat([out, action], 1)\n        out = nn.functional.relu(self.fc2(out))\n        out = self.fc3(out)\n        return out\n\nclass ActorCriticNetwork(nn.Module):\n    """"""\n    An actor-critic network that shared lower-layer representations but\n    have distinct output layers\n    """"""\n    def __init__(self, state_dim, action_dim, hidden_size,\n                 actor_output_act, critic_output_size=1):\n        super(ActorCriticNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_dim, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.actor_linear = nn.Linear(hidden_size, action_dim)\n        self.critic_linear = nn.Linear(hidden_size, critic_output_size)\n        self.actor_output_act = actor_output_act\n\n    def __call__(self, state):\n        out = nn.functional.relu(self.fc1(state))\n        out = nn.functional.relu(self.fc2(out))\n        act = self.actor_output_act(self.actor_linear(out))\n        val = self.critic_linear(out)\n        return act, val\n'"
common/utils.py,1,"b'\nimport torch as th\nfrom torch.autograd import Variable\nimport numpy as np\n\n\ndef identity(x):\n    return x\n\n\ndef entropy(p):\n    return -th.sum(p * th.log(p), 1)\n\n\ndef kl_log_probs(log_p1, log_p2):\n    return -th.sum(th.exp(log_p1)*(log_p2 - log_p1), 1)\n\n\ndef index_to_one_hot(index, dim):\n    if isinstance(index, np.int) or isinstance(index, np.int64):\n        one_hot = np.zeros(dim)\n        one_hot[index] = 1.\n    else:\n        one_hot = np.zeros((len(index), dim))\n        one_hot[np.arange(len(index)), index] = 1.\n    return one_hot\n\n\ndef to_tensor_var(x, use_cuda=True, dtype=""float""):\n    FloatTensor = th.cuda.FloatTensor if use_cuda else th.FloatTensor\n    LongTensor = th.cuda.LongTensor if use_cuda else th.LongTensor\n    ByteTensor = th.cuda.ByteTensor if use_cuda else th.ByteTensor\n    if dtype == ""float"":\n        x = np.array(x, dtype=np.float64).tolist()\n        return Variable(FloatTensor(x))\n    elif dtype == ""long"":\n        x = np.array(x, dtype=np.long).tolist()\n        return Variable(LongTensor(x))\n    elif dtype == ""byte"":\n        x = np.array(x, dtype=np.byte).tolist()\n        return Variable(ByteTensor(x))\n    else:\n        x = np.array(x, dtype=np.float64).tolist()\n        return Variable(FloatTensor(x))\n\n\ndef agg_double_list(l):\n    # l: [ [...], [...], [...] ]\n    # l_i: result of each step in the i-th episode\n    s = [np.sum(np.array(l_i), 0) for l_i in l]\n    s_mu = np.mean(np.array(s), 0)\n    s_std = np.std(np.array(s), 0)\n    return s_mu, s_std\n'"
