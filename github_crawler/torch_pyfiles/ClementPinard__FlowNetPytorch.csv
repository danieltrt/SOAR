file_path,api_count,code
flow_transforms.py,2,"b'from __future__ import division\nimport torch\nimport random\nimport numpy as np\nimport numbers\nimport types\nimport scipy.ndimage as ndimage\n\n\'\'\'Set of tranform random routines that takes both input and target as arguments,\nin order to have random but coherent transformations.\ninputs are PIL Image pairs and targets are ndarrays\'\'\'\n\n\nclass Compose(object):\n    """""" Composes several co_transforms together.\n    For example:\n    >>> co_transforms.Compose([\n    >>>     co_transforms.CenterCrop(10),\n    >>>     co_transforms.ToTensor(),\n    >>>  ])\n    """"""\n\n    def __init__(self, co_transforms):\n        self.co_transforms = co_transforms\n\n    def __call__(self, input, target):\n        for t in self.co_transforms:\n            input,target = t(input,target)\n        return input,target\n\n\nclass ArrayToTensor(object):\n    """"""Converts a numpy.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W).""""""\n\n    def __call__(self, array):\n        assert(isinstance(array, np.ndarray))\n        array = np.transpose(array, (2, 0, 1))\n        # handle numpy array\n        tensor = torch.from_numpy(array)\n        # put it from HWC to CHW format\n        return tensor.float()\n\n\nclass Lambda(object):\n    """"""Applies a lambda as a transform""""""\n\n    def __init__(self, lambd):\n        assert isinstance(lambd, types.LambdaType)\n        self.lambd = lambd\n\n    def __call__(self, input,target):\n        return self.lambd(input,target)\n\n\nclass CenterCrop(object):\n    """"""Crops the given inputs and target arrays at the center to have a region of\n    the given size. size can be a tuple (target_height, target_width)\n    or an integer, in which case the target will be of a square shape (size, size)\n    Careful, img1 and img2 may not be the same size\n    """"""\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, inputs, target):\n        h1, w1, _ = inputs[0].shape\n        h2, w2, _ = inputs[1].shape\n        th, tw = self.size\n        x1 = int(round((w1 - tw) / 2.))\n        y1 = int(round((h1 - th) / 2.))\n        x2 = int(round((w2 - tw) / 2.))\n        y2 = int(round((h2 - th) / 2.))\n\n        inputs[0] = inputs[0][y1: y1 + th, x1: x1 + tw]\n        inputs[1] = inputs[1][y2: y2 + th, x2: x2 + tw]\n        target = target[y1: y1 + th, x1: x1 + tw]\n        return inputs,target\n\n\nclass Scale(object):\n    """""" Rescales the inputs and target arrays to the given \'size\'.\n    \'size\' will be the size of the smaller edge.\n    For example, if height > width, then image will be\n    rescaled to (size * height / width, size)\n    size: size of the smaller edge\n    interpolation order: Default: 2 (bilinear)\n    """"""\n\n    def __init__(self, size, order=2):\n        self.size = size\n        self.order = order\n\n    def __call__(self, inputs, target):\n        h, w, _ = inputs[0].shape\n        if (w <= h and w == self.size) or (h <= w and h == self.size):\n            return inputs,target\n        if w < h:\n            ratio = self.size/w\n        else:\n            ratio = self.size/h\n\n        inputs[0] = ndimage.interpolation.zoom(inputs[0], ratio, order=self.order)\n        inputs[1] = ndimage.interpolation.zoom(inputs[1], ratio, order=self.order)\n\n        target = ndimage.interpolation.zoom(target, ratio, order=self.order)\n        target *= ratio\n        return inputs, target\n\n\nclass RandomCrop(object):\n    """"""Crops the given PIL.Image at a random location to have a region of\n    the given size. size can be a tuple (target_height, target_width)\n    or an integer, in which case the target will be of a square shape (size, size)\n    """"""\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, inputs,target):\n        h, w, _ = inputs[0].shape\n        th, tw = self.size\n        if w == tw and h == th:\n            return inputs,target\n\n        x1 = random.randint(0, w - tw)\n        y1 = random.randint(0, h - th)\n        inputs[0] = inputs[0][y1: y1 + th,x1: x1 + tw]\n        inputs[1] = inputs[1][y1: y1 + th,x1: x1 + tw]\n        return inputs, target[y1: y1 + th,x1: x1 + tw]\n\n\nclass RandomHorizontalFlip(object):\n    """"""Randomly horizontally flips the given PIL.Image with a probability of 0.5\n    """"""\n\n    def __call__(self, inputs, target):\n        if random.random() < 0.5:\n            inputs[0] = np.copy(np.fliplr(inputs[0]))\n            inputs[1] = np.copy(np.fliplr(inputs[1]))\n            target = np.copy(np.fliplr(target))\n            target[:,:,0] *= -1\n        return inputs,target\n\n\nclass RandomVerticalFlip(object):\n    """"""Randomly horizontally flips the given PIL.Image with a probability of 0.5\n    """"""\n\n    def __call__(self, inputs, target):\n        if random.random() < 0.5:\n            inputs[0] = np.copy(np.flipud(inputs[0]))\n            inputs[1] = np.copy(np.flipud(inputs[1]))\n            target = np.copy(np.flipud(target))\n            target[:,:,1] *= -1\n        return inputs,target\n\n\nclass RandomRotate(object):\n    """"""Random rotation of the image from -angle to angle (in degrees)\n    This is useful for dataAugmentation, especially for geometric problems such as FlowEstimation\n    angle: max angle of the rotation\n    interpolation order: Default: 2 (bilinear)\n    reshape: Default: false. If set to true, image size will be set to keep every pixel in the image.\n    diff_angle: Default: 0. Must stay less than 10 degrees, or linear approximation of flowmap will be off.\n    """"""\n\n    def __init__(self, angle, diff_angle=0, order=2, reshape=False):\n        self.angle = angle\n        self.reshape = reshape\n        self.order = order\n        self.diff_angle = diff_angle\n\n    def __call__(self, inputs,target):\n        applied_angle = random.uniform(-self.angle,self.angle)\n        diff = random.uniform(-self.diff_angle,self.diff_angle)\n        angle1 = applied_angle - diff/2\n        angle2 = applied_angle + diff/2\n        angle1_rad = angle1*np.pi/180\n\n        h, w, _ = target.shape\n\n        def rotate_flow(i,j,k):\n            return -k*(j-w/2)*(diff*np.pi/180) + (1-k)*(i-h/2)*(diff*np.pi/180)\n\n        rotate_flow_map = np.fromfunction(rotate_flow, target.shape)\n        target += rotate_flow_map\n\n        inputs[0] = ndimage.interpolation.rotate(inputs[0], angle1, reshape=self.reshape, order=self.order)\n        inputs[1] = ndimage.interpolation.rotate(inputs[1], angle2, reshape=self.reshape, order=self.order)\n        target = ndimage.interpolation.rotate(target, angle1, reshape=self.reshape, order=self.order)\n        # flow vectors must be rotated too! careful about Y flow which is upside down\n        target_ = np.copy(target)\n        target[:,:,0] = np.cos(angle1_rad)*target_[:,:,0] + np.sin(angle1_rad)*target_[:,:,1]\n        target[:,:,1] = -np.sin(angle1_rad)*target_[:,:,0] + np.cos(angle1_rad)*target_[:,:,1]\n        return inputs,target\n\n\nclass RandomTranslate(object):\n    def __init__(self, translation):\n        if isinstance(translation, numbers.Number):\n            self.translation = (int(translation), int(translation))\n        else:\n            self.translation = translation\n\n    def __call__(self, inputs,target):\n        h, w, _ = inputs[0].shape\n        th, tw = self.translation\n        tw = random.randint(-tw, tw)\n        th = random.randint(-th, th)\n        if tw == 0 and th == 0:\n            return inputs, target\n        # compute x1,x2,y1,y2 for img1 and target, and x3,x4,y3,y4 for img2\n        x1,x2,x3,x4 = max(0,tw), min(w+tw,w), max(0,-tw), min(w-tw,w)\n        y1,y2,y3,y4 = max(0,th), min(h+th,h), max(0,-th), min(h-th,h)\n\n        inputs[0] = inputs[0][y1:y2,x1:x2]\n        inputs[1] = inputs[1][y3:y4,x3:x4]\n        target = target[y1:y2,x1:x2]\n        target[:,:,0] += tw\n        target[:,:,1] += th\n\n        return inputs, target\n\n\nclass RandomColorWarp(object):\n    def __init__(self, mean_range=0, std_range=0):\n        self.mean_range = mean_range\n        self.std_range = std_range\n\n    def __call__(self, inputs, target):\n        random_std = np.random.uniform(-self.std_range, self.std_range, 3)\n        random_mean = np.random.uniform(-self.mean_range, self.mean_range, 3)\n        random_order = np.random.permutation(3)\n\n        inputs[0] *= (1 + random_std)\n        inputs[0] += random_mean\n\n        inputs[1] *= (1 + random_std)\n        inputs[1] += random_mean\n\n        inputs[0] = inputs[0][:,:,random_order]\n        inputs[1] = inputs[1][:,:,random_order]\n\n        return inputs, target\n'"
main.py,17,"b'import argparse\nimport os\nimport time\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport flow_transforms\nimport models\nimport datasets\nfrom multiscaleloss import multiscaleEPE, realEPE\nimport datetime\nfrom tensorboardX import SummaryWriter\nfrom util import flow2rgb, AverageMeter, save_checkpoint\n\nmodel_names = sorted(name for name in models.__dict__\n                     if name.islower() and not name.startswith(""__""))\ndataset_names = sorted(name for name in datasets.__all__)\n\nparser = argparse.ArgumentParser(description=\'PyTorch FlowNet Training on several datasets\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'data\', metavar=\'DIR\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--dataset\', metavar=\'DATASET\', default=\'flying_chairs\',\n                    choices=dataset_names,\n                    help=\'dataset type : \' +\n                    \' | \'.join(dataset_names))\ngroup = parser.add_mutually_exclusive_group()\ngroup.add_argument(\'-s\', \'--split-file\', default=None, type=str,\n                   help=\'test-val split file\')\ngroup.add_argument(\'--split-value\', default=0.8, type=float,\n                   help=\'test-val split proportion between 0 (only test) and 1 (only train), \'\n                        \'will be overwritten if a split file is set\')\nparser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'flownets\',\n                    choices=model_names,\n                    help=\'model architecture, overwritten if pretrained is specified: \' +\n                    \' | \'.join(model_names))\nparser.add_argument(\'--solver\', default=\'adam\',choices=[\'adam\',\'sgd\'],\n                    help=\'solver algorithms\')\nparser.add_argument(\'-j\', \'--workers\', default=8, type=int, metavar=\'N\',\n                    help=\'number of data loading workers\')\nparser.add_argument(\'--epochs\', default=300, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--epoch-size\', default=1000, type=int, metavar=\'N\',\n                    help=\'manual epoch size (will match dataset size if set to 0)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=8, type=int,\n                    metavar=\'N\', help=\'mini-batch size\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.0001, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum for sgd, alpha parameter for adam\')\nparser.add_argument(\'--beta\', default=0.999, type=float, metavar=\'M\',\n                    help=\'beta parameter for adam\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=4e-4, type=float,\n                    metavar=\'W\', help=\'weight decay\')\nparser.add_argument(\'--bias-decay\', default=0, type=float,\n                    metavar=\'B\', help=\'bias decay\')\nparser.add_argument(\'--multiscale-weights\', \'-w\', default=[0.005,0.01,0.02,0.08,0.32], type=float, nargs=5,\n                    help=\'training weight for each scale, from highest resolution (flow2) to lowest (flow6)\',\n                    metavar=(\'W2\', \'W3\', \'W4\', \'W5\', \'W6\'))\nparser.add_argument(\'--sparse\', action=\'store_true\',\n                    help=\'look for NaNs in target flow when computing EPE, avoid if flow is garantied to be dense,\'\n                    \'automatically seleted when choosing a KITTIdataset\')\nparser.add_argument(\'--print-freq\', \'-p\', default=10, type=int,\n                    metavar=\'N\', help=\'print frequency\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                    help=\'evaluate model on validation set\')\nparser.add_argument(\'--pretrained\', dest=\'pretrained\', default=None,\n                    help=\'path to pre-trained model\')\nparser.add_argument(\'--no-date\', action=\'store_true\',\n                    help=\'don\\\'t append date timestamp to folder\' )\nparser.add_argument(\'--div-flow\', default=20,\n                    help=\'value by which flow will be divided. Original value is 20 but 1 with batchNorm gives good results\')\nparser.add_argument(\'--milestones\', default=[100,150,200], metavar=\'N\', nargs=\'*\', help=\'epochs at which learning rate is divided by 2\')\n\nbest_EPE = -1\nn_iter = 0\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n\ndef main():\n    global args, best_EPE\n    args = parser.parse_args()\n    save_path = \'{},{},{}epochs{},b{},lr{}\'.format(\n        args.arch,\n        args.solver,\n        args.epochs,\n        \',epochSize\'+str(args.epoch_size) if args.epoch_size > 0 else \'\',\n        args.batch_size,\n        args.lr)\n    if not args.no_date:\n        timestamp = datetime.datetime.now().strftime(""%m-%d-%H:%M"")\n        save_path = os.path.join(timestamp,save_path)\n    save_path = os.path.join(args.dataset,save_path)\n    print(\'=> will save everything to {}\'.format(save_path))\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n\n    train_writer = SummaryWriter(os.path.join(save_path,\'train\'))\n    test_writer = SummaryWriter(os.path.join(save_path,\'test\'))\n    output_writers = []\n    for i in range(3):\n        output_writers.append(SummaryWriter(os.path.join(save_path,\'test\',str(i))))\n\n    # Data loading code\n    input_transform = transforms.Compose([\n        flow_transforms.ArrayToTensor(),\n        transforms.Normalize(mean=[0,0,0], std=[255,255,255]),\n        transforms.Normalize(mean=[0.45,0.432,0.411], std=[1,1,1])\n    ])\n    target_transform = transforms.Compose([\n        flow_transforms.ArrayToTensor(),\n        transforms.Normalize(mean=[0,0],std=[args.div_flow,args.div_flow])\n    ])\n\n    if \'KITTI\' in args.dataset:\n        args.sparse = True\n    if args.sparse:\n        co_transform = flow_transforms.Compose([\n            flow_transforms.RandomCrop((320,448)),\n            flow_transforms.RandomVerticalFlip(),\n            flow_transforms.RandomHorizontalFlip()\n        ])\n    else:\n        co_transform = flow_transforms.Compose([\n            flow_transforms.RandomTranslate(10),\n            flow_transforms.RandomRotate(10,5),\n            flow_transforms.RandomCrop((320,448)),\n            flow_transforms.RandomVerticalFlip(),\n            flow_transforms.RandomHorizontalFlip()\n        ])\n\n    print(""=> fetching img pairs in \'{}\'"".format(args.data))\n    train_set, test_set = datasets.__dict__[args.dataset](\n        args.data,\n        transform=input_transform,\n        target_transform=target_transform,\n        co_transform=co_transform,\n        split=args.split_file if args.split_file else args.split_value\n    )\n    print(\'{} samples found, {} train samples and {} test samples \'.format(len(test_set)+len(train_set),\n                                                                           len(train_set),\n                                                                           len(test_set)))\n    train_loader = torch.utils.data.DataLoader(\n        train_set, batch_size=args.batch_size,\n        num_workers=args.workers, pin_memory=True, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(\n        test_set, batch_size=args.batch_size,\n        num_workers=args.workers, pin_memory=True, shuffle=False)\n\n    # create model\n    if args.pretrained:\n        network_data = torch.load(args.pretrained)\n        args.arch = network_data[\'arch\']\n        print(""=> using pre-trained model \'{}\'"".format(args.arch))\n    else:\n        network_data = None\n        print(""=> creating model \'{}\'"".format(args.arch))\n\n    model = models.__dict__[args.arch](network_data).cuda()\n    model = torch.nn.DataParallel(model).cuda()\n    cudnn.benchmark = True\n\n    assert(args.solver in [\'adam\', \'sgd\'])\n    print(\'=> setting {} solver\'.format(args.solver))\n    param_groups = [{\'params\': model.module.bias_parameters(), \'weight_decay\': args.bias_decay},\n                    {\'params\': model.module.weight_parameters(), \'weight_decay\': args.weight_decay}]\n    if args.solver == \'adam\':\n        optimizer = torch.optim.Adam(param_groups, args.lr,\n                                     betas=(args.momentum, args.beta))\n    elif args.solver == \'sgd\':\n        optimizer = torch.optim.SGD(param_groups, args.lr,\n                                    momentum=args.momentum)\n\n    if args.evaluate:\n        best_EPE = validate(val_loader, model, 0, output_writers)\n        return\n\n    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.milestones, gamma=0.5)\n\n    for epoch in range(args.start_epoch, args.epochs):\n        scheduler.step()\n\n        # train for one epoch\n        train_loss, train_EPE = train(train_loader, model, optimizer, epoch, train_writer)\n        train_writer.add_scalar(\'mean EPE\', train_EPE, epoch)\n\n        # evaluate on validation set\n\n        with torch.no_grad():\n            EPE = validate(val_loader, model, epoch, output_writers)\n        test_writer.add_scalar(\'mean EPE\', EPE, epoch)\n\n        if best_EPE < 0:\n            best_EPE = EPE\n\n        is_best = EPE < best_EPE\n        best_EPE = min(EPE, best_EPE)\n        save_checkpoint({\n            \'epoch\': epoch + 1,\n            \'arch\': args.arch,\n            \'state_dict\': model.module.state_dict(),\n            \'best_EPE\': best_EPE,\n            \'div_flow\': args.div_flow\n        }, is_best, save_path)\n\n\ndef train(train_loader, model, optimizer, epoch, train_writer):\n    global n_iter, args\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    flow2_EPEs = AverageMeter()\n\n    epoch_size = len(train_loader) if args.epoch_size == 0 else min(len(train_loader), args.epoch_size)\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n\n    for i, (input, target) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n        target = target.to(device)\n        input = torch.cat(input,1).to(device)\n\n        # compute output\n        output = model(input)\n        if args.sparse:\n            # Since Target pooling is not very precise when sparse,\n            # take the highest resolution prediction and upsample it instead of downsampling target\n            h, w = target.size()[-2:]\n            output = [F.interpolate(output[0], (h,w)), *output[1:]]\n\n        loss = multiscaleEPE(output, target, weights=args.multiscale_weights, sparse=args.sparse)\n        flow2_EPE = args.div_flow * realEPE(output[0], target, sparse=args.sparse)\n        # record loss and EPE\n        losses.update(loss.item(), target.size(0))\n        train_writer.add_scalar(\'train_loss\', loss.item(), n_iter)\n        flow2_EPEs.update(flow2_EPE.item(), target.size(0))\n\n        # compute gradient and do optimization step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print(\'Epoch: [{0}][{1}/{2}]\\t Time {3}\\t Data {4}\\t Loss {5}\\t EPE {6}\'\n                  .format(epoch, i, epoch_size, batch_time,\n                          data_time, losses, flow2_EPEs))\n        n_iter += 1\n        if i >= epoch_size:\n            break\n\n    return losses.avg, flow2_EPEs.avg\n\n\ndef validate(val_loader, model, epoch, output_writers):\n    global args\n\n    batch_time = AverageMeter()\n    flow2_EPEs = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    for i, (input, target) in enumerate(val_loader):\n        target = target.to(device)\n        input = torch.cat(input,1).to(device)\n\n        # compute output\n        output = model(input)\n        flow2_EPE = args.div_flow*realEPE(output, target, sparse=args.sparse)\n        # record EPE\n        flow2_EPEs.update(flow2_EPE.item(), target.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i < len(output_writers):  # log first output of first batches\n            if epoch == 0:\n                mean_values = torch.tensor([0.45,0.432,0.411], dtype=input.dtype).view(3,1,1)\n                output_writers[i].add_image(\'GroundTruth\', flow2rgb(args.div_flow * target[0], max_value=10), 0)\n                output_writers[i].add_image(\'Inputs\', (input[0,:3].cpu() + mean_values).clamp(0,1), 0)\n                output_writers[i].add_image(\'Inputs\', (input[0,3:].cpu() + mean_values).clamp(0,1), 1)\n            output_writers[i].add_image(\'FlowNet Outputs\', flow2rgb(args.div_flow * output[0], max_value=10), epoch)\n\n        if i % args.print_freq == 0:\n            print(\'Test: [{0}/{1}]\\t Time {2}\\t EPE {3}\'\n                  .format(i, len(val_loader), batch_time, flow2_EPEs))\n\n    print(\' * EPE {:.3f}\'.format(flow2_EPEs.avg))\n\n    return flow2_EPEs.avg\n\n\nif __name__ == \'__main__\':\n    main()\n'"
multiscaleloss.py,2,"b'import torch\nimport torch.nn.functional as F\n\n\ndef EPE(input_flow, target_flow, sparse=False, mean=True):\n    EPE_map = torch.norm(target_flow-input_flow,2,1)\n    batch_size = EPE_map.size(0)\n    if sparse:\n        # invalid flow is defined with both flow coordinates to be exactly 0\n        mask = (target_flow[:,0] == 0) & (target_flow[:,1] == 0)\n\n        EPE_map = EPE_map[~mask]\n    if mean:\n        return EPE_map.mean()\n    else:\n        return EPE_map.sum()/batch_size\n\n\ndef sparse_max_pool(input, size):\n    \'\'\'Downsample the input by considering 0 values as invalid.\n\n    Unfortunately, no generic interpolation mode can resize a sparse map correctly,\n    the strategy here is to use max pooling for positive values and ""min pooling""\n    for negative values, the two results are then summed.\n    This technique allows sparsity to be minized, contrary to nearest interpolation,\n    which could potentially lose information for isolated data points.\'\'\'\n\n    positive = (input > 0).float()\n    negative = (input < 0).float()\n    output = F.adaptive_max_pool2d(input * positive, size) - F.adaptive_max_pool2d(-input * negative, size)\n    return output\n\n\ndef multiscaleEPE(network_output, target_flow, weights=None, sparse=False):\n    def one_scale(output, target, sparse):\n\n        b, _, h, w = output.size()\n\n        if sparse:\n            target_scaled = sparse_max_pool(target, (h, w))\n        else:\n            target_scaled = F.interpolate(target, (h, w), mode=\'area\')\n        return EPE(output, target_scaled, sparse, mean=False)\n\n    if type(network_output) not in [tuple, list]:\n        network_output = [network_output]\n    if weights is None:\n        weights = [0.005, 0.01, 0.02, 0.08, 0.32]  # as in original article\n    assert(len(weights) == len(network_output))\n\n    loss = 0\n    for output, weight in zip(network_output, weights):\n        loss += weight * one_scale(output, target_flow, sparse)\n    return loss\n\n\ndef realEPE(output, target, sparse=False):\n    b, _, h, w = target.size()\n    upsampled_output = F.interpolate(output, (h,w), mode=\'bilinear\', align_corners=False)\n    return EPE(upsampled_output, target, sparse, mean=True)\n'"
run_inference.py,8,"b'import argparse\nfrom path import Path\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn.functional as F\nimport models\nfrom tqdm import tqdm\n\nimport torchvision.transforms as transforms\nimport flow_transforms\nfrom imageio import imread, imwrite\nimport numpy as np\nfrom util import flow2rgb\n\nmodel_names = sorted(name for name in models.__dict__\n                     if name.islower() and not name.startswith(""__""))\n\n\nparser = argparse.ArgumentParser(description=\'PyTorch FlowNet inference on a folder of img pairs\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'data\', metavar=\'DIR\',\n                    help=\'path to images folder, image names must match \\\'[name]0.[ext]\\\' and \\\'[name]1.[ext]\\\'\')\nparser.add_argument(\'pretrained\', metavar=\'PTH\', help=\'path to pre-trained model\')\nparser.add_argument(\'--output\', \'-o\', metavar=\'DIR\', default=None,\n                    help=\'path to output folder. If not set, will be created in data folder\')\nparser.add_argument(\'--output-value\', \'-v\', choices=[\'raw\', \'vis\', \'both\'], default=\'both\',\n                    help=\'which value to output, between raw input (as a npy file) and color vizualisation (as an image file).\'\n                    \' If not set, will output both\')\nparser.add_argument(\'--div-flow\', default=20, type=float,\n                    help=\'value by which flow will be divided. overwritten if stored in pretrained file\')\nparser.add_argument(""--img-exts"", metavar=\'EXT\', default=[\'png\', \'jpg\', \'bmp\', \'ppm\'], nargs=\'*\', type=str,\n                    help=""images extensions to glob"")\nparser.add_argument(\'--max_flow\', default=None, type=float,\n                    help=\'max flow value. Flow map color is saturated above this value. If not set, will use flow map\\\'s max value\')\nparser.add_argument(\'--upsampling\', \'-u\', choices=[\'nearest\', \'bilinear\'], default=None, help=\'if not set, will output FlowNet raw input,\'\n                    \'which is 4 times downsampled. If set, will output full resolution flow map, with selected upsampling\')\nparser.add_argument(\'--bidirectional\', action=\'store_true\', help=\'if set, will output invert flow (from 1 to 0) along with regular flow\')\n\ndevice = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"")\n\n\n@torch.no_grad()\ndef main():\n    global args, save_path\n    args = parser.parse_args()\n\n    if args.output_value == \'both\':\n        output_string = ""raw output and RGB visualization""\n    elif args.output_value == \'raw\':\n        output_string = ""raw output""\n    elif args.output_value == \'vis\':\n        output_string = ""RGB visualization""\n    print(""=> will save "" + output_string)\n    data_dir = Path(args.data)\n    print(""=> fetching img pairs in \'{}\'"".format(args.data))\n    if args.output is None:\n        save_path = data_dir/\'flow\'\n    else:\n        save_path = Path(args.output)\n    print(\'=> will save everything to {}\'.format(save_path))\n    save_path.makedirs_p()\n    # Data loading code\n    input_transform = transforms.Compose([\n        flow_transforms.ArrayToTensor(),\n        transforms.Normalize(mean=[0,0,0], std=[255,255,255]),\n        transforms.Normalize(mean=[0.411,0.432,0.45], std=[1,1,1])\n    ])\n\n    img_pairs = []\n    for ext in args.img_exts:\n        test_files = data_dir.files(\'*1.{}\'.format(ext))\n        for file in test_files:\n            img_pair = file.parent / (file.namebase[:-1] + \'2.{}\'.format(ext))\n            if img_pair.isfile():\n                img_pairs.append([file, img_pair])\n\n    print(\'{} samples found\'.format(len(img_pairs)))\n    # create model\n    network_data = torch.load(args.pretrained)\n    print(""=> using pre-trained model \'{}\'"".format(network_data[\'arch\']))\n    model = models.__dict__[network_data[\'arch\']](network_data).to(device)\n    model.eval()\n    cudnn.benchmark = True\n\n    if \'div_flow\' in network_data.keys():\n        args.div_flow = network_data[\'div_flow\']\n\n    for (img1_file, img2_file) in tqdm(img_pairs):\n\n        img1 = input_transform(imread(img1_file))\n        img2 = input_transform(imread(img2_file))\n        input_var = torch.cat([img1, img2]).unsqueeze(0)\n\n        if args.bidirectional:\n            # feed inverted pair along with normal pair\n            inverted_input_var = torch.cat([img2, img1]).unsqueeze(0)\n            input_var = torch.cat([input_var, inverted_input_var])\n\n        input_var = input_var.to(device)\n        # compute output\n        output = model(input_var)\n        if args.upsampling is not None:\n            output = F.interpolate(output, size=img1.size()[-2:], mode=args.upsampling, align_corners=False)\n        for suffix, flow_output in zip([\'flow\', \'inv_flow\'], output):\n            filename = save_path/\'{}{}\'.format(img1_file.namebase[:-1], suffix)\n            if args.output_value in[\'vis\', \'both\']:\n                rgb_flow = flow2rgb(args.div_flow * flow_output, max_value=args.max_flow)\n                to_save = (rgb_flow * 255).astype(np.uint8).transpose(1,2,0)\n                imwrite(filename + \'.png\', to_save)\n            if args.output_value in [\'raw\', \'both\']:\n                # Make the flow map a HxWx2 array as in .flo files\n                to_save = (args.div_flow*flow_output).cpu().numpy().transpose(1,2,0)\n                np.save(filename + \'.npy\', to_save)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
util.py,1,"b'import os\nimport numpy as np\nimport shutil\nimport torch\n\n\ndef save_checkpoint(state, is_best, save_path, filename=\'checkpoint.pth.tar\'):\n    torch.save(state, os.path.join(save_path,filename))\n    if is_best:\n        shutil.copyfile(os.path.join(save_path,filename), os.path.join(save_path,\'model_best.pth.tar\'))\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __repr__(self):\n        return \'{:.3f} ({:.3f})\'.format(self.val, self.avg)\n\n\ndef flow2rgb(flow_map, max_value):\n    flow_map_np = flow_map.detach().cpu().numpy()\n    _, h, w = flow_map_np.shape\n    flow_map_np[:,(flow_map_np[0] == 0) & (flow_map_np[1] == 0)] = float(\'nan\')\n    rgb_map = np.ones((3,h,w)).astype(np.float32)\n    if max_value is not None:\n        normalized_flow_map = flow_map_np / max_value\n    else:\n        normalized_flow_map = flow_map_np / (np.abs(flow_map_np).max())\n    rgb_map[0] += normalized_flow_map[0]\n    rgb_map[1] -= 0.5*(normalized_flow_map[0] + normalized_flow_map[1])\n    rgb_map[2] += normalized_flow_map[1]\n    return rgb_map.clip(0,1)'"
datasets/KITTI.py,0,"b'from __future__ import division\nimport os.path\nimport glob\nfrom .listdataset import ListDataset\nfrom .util import split2list\nimport numpy as np\nimport flow_transforms\n\ntry:\n    import cv2\nexcept ImportError as e:\n    import warnings\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""default"", category=ImportWarning)\n        warnings.warn(""failed to load openCV, which is needed""\n                      ""for KITTI which uses 16bit PNG images"", ImportWarning)\n\n\'\'\'\nDataset routines for KITTI_flow, 2012 and 2015.\nhttp://www.cvlibs.net/datasets/kitti/eval_flow.php\nThe dataset is not very big, you might want to only finetune on it for flownet\nEPE are not representative in this dataset because of the sparsity of the GT.\nOpenCV is needed to load 16bit png images\n\'\'\'\n\n\ndef load_flow_from_png(png_path):\n    # The -1 is here to specify not to change the image depth (16bit), and is compatible\n    # with both OpenCV2 and OpenCV3\n    flo_file = cv2.imread(png_path, -1)\n    flo_img = flo_file[:,:,2:0:-1].astype(np.float32)\n    invalid = (flo_file[:,:,0] == 0)\n    flo_img = flo_img - 32768\n    flo_img = flo_img / 64\n    flo_img[np.abs(flo_img) < 1e-10] = 1e-10\n    flo_img[invalid, :] = 0\n    return(flo_img)\n\n\ndef make_dataset(dir, split, occ=True):\n    \'\'\'Will search in training folder for folders \'flow_noc\' or \'flow_occ\'\n       and \'colored_0\' (KITTI 2012) or \'image_2\' (KITTI 2015) \'\'\'\n    flow_dir = \'flow_occ\' if occ else \'flow_noc\'\n    assert(os.path.isdir(os.path.join(dir, flow_dir)))\n    img_dir = \'colored_0\'\n    if not os.path.isdir(os.path.join(dir, img_dir)):\n        img_dir = \'image_2\'\n    assert(os.path.isdir(os.path.join(dir, img_dir)))\n\n    images = []\n    for flow_map in glob.iglob(os.path.join(dir, flow_dir, \'*.png\')):\n        flow_map = os.path.basename(flow_map)\n        root_filename = flow_map[:-7]\n        flow_map = os.path.join(flow_dir, flow_map)\n        img1 = os.path.join(img_dir, root_filename+\'_10.png\')\n        img2 = os.path.join(img_dir, root_filename+\'_11.png\')\n        if not (os.path.isfile(os.path.join(dir, img1)) or os.path.isfile(os.path.join(dir, img2))):\n            continue\n        images.append([[img1, img2], flow_map])\n\n    return split2list(images, split, default_split=0.9)\n\n\ndef KITTI_loader(root,path_imgs, path_flo):\n    imgs = [os.path.join(root,path) for path in path_imgs]\n    flo = os.path.join(root,path_flo)\n    return [cv2.imread(img)[:,:,::-1].astype(np.float32) for img in imgs],load_flow_from_png(flo)\n\n\ndef KITTI_occ(root, transform=None, target_transform=None,\n              co_transform=None, split=None):\n    train_list, test_list = make_dataset(root, split, True)\n    train_dataset = ListDataset(root, train_list, transform,\n                                target_transform, co_transform,\n                                loader=KITTI_loader)\n    # All test sample are cropped to lowest possible size of KITTI images\n    test_dataset = ListDataset(root, test_list, transform,\n                               target_transform, flow_transforms.CenterCrop((370,1224)),\n                               loader=KITTI_loader)\n\n    return train_dataset, test_dataset\n\n\ndef KITTI_noc(root, transform=None, target_transform=None,\n              co_transform=None, split=None):\n    train_list, test_list = make_dataset(root, split, False)\n    train_dataset = ListDataset(root, train_list, transform, target_transform, co_transform, loader=KITTI_loader)\n    # All test sample are cropped to lowest possible size of KITTI images\n    test_dataset = ListDataset(root, test_list, transform, target_transform, flow_transforms.CenterCrop((370,1224)), loader=KITTI_loader)\n\n    return train_dataset, test_dataset\n'"
datasets/__init__.py,0,"b""from .flyingchairs import flying_chairs\nfrom .KITTI import KITTI_occ,KITTI_noc\nfrom .mpisintel import mpi_sintel_clean,mpi_sintel_final,mpi_sintel_both\n\n__all__ = ('flying_chairs','KITTI_occ','KITTI_noc','mpi_sintel_clean','mpi_sintel_final','mpi_sintel_both')\n"""
datasets/flyingchairs.py,0,"b""import os.path\nimport glob\nfrom .listdataset import ListDataset\nfrom .util import split2list\n\n\ndef make_dataset(dir, split=None):\n    '''Will search for triplets that go by the pattern '[name]_img1.ppm  [name]_img2.ppm    [name]_flow.flo' '''\n    images = []\n    for flow_map in sorted(glob.glob(os.path.join(dir,'*_flow.flo'))):\n        flow_map = os.path.basename(flow_map)\n        root_filename = flow_map[:-9]\n        img1 = root_filename+'_img1.ppm'\n        img2 = root_filename+'_img2.ppm'\n        if not (os.path.isfile(os.path.join(dir,img1)) and os.path.isfile(os.path.join(dir,img2))):\n            continue\n\n        images.append([[img1,img2],flow_map])\n\n    return split2list(images, split, default_split=0.97)\n\n\ndef flying_chairs(root, transform=None, target_transform=None,\n                  co_transform=None, split=None):\n    train_list, test_list = make_dataset(root,split)\n    train_dataset = ListDataset(root, train_list, transform, target_transform, co_transform)\n    test_dataset = ListDataset(root, test_list, transform, target_transform)\n\n    return train_dataset, test_dataset\n"""
datasets/listdataset.py,1,"b""import torch.utils.data as data\nimport os\nimport os.path\nfrom imageio import imread\nimport numpy as np\n\n\ndef load_flo(path):\n    with open(path, 'rb') as f:\n        magic = np.fromfile(f, np.float32, count=1)\n        assert(202021.25 == magic),'Magic number incorrect. Invalid .flo file'\n        h = np.fromfile(f, np.int32, count=1)[0]\n        w = np.fromfile(f, np.int32, count=1)[0]\n        data = np.fromfile(f, np.float32, count=2*w*h)\n    # Reshape data into 3D array (columns, rows, bands)\n    data2D = np.resize(data, (w, h, 2))\n    return data2D\n\n\ndef default_loader(root, path_imgs, path_flo):\n    imgs = [os.path.join(root,path) for path in path_imgs]\n    flo = os.path.join(root,path_flo)\n    return [imread(img).astype(np.float32) for img in imgs],load_flo(flo)\n\n\nclass ListDataset(data.Dataset):\n    def __init__(self, root, path_list, transform=None, target_transform=None,\n                 co_transform=None, loader=default_loader):\n\n        self.root = root\n        self.path_list = path_list\n        self.transform = transform\n        self.target_transform = target_transform\n        self.co_transform = co_transform\n        self.loader = loader\n\n    def __getitem__(self, index):\n        inputs, target = self.path_list[index]\n\n        inputs, target = self.loader(self.root, inputs, target)\n        if self.co_transform is not None:\n            inputs, target = self.co_transform(inputs, target)\n        if self.transform is not None:\n            inputs[0] = self.transform(inputs[0])\n            inputs[1] = self.transform(inputs[1])\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        return inputs, target\n\n    def __len__(self):\n        return len(self.path_list)\n"""
datasets/mpisintel.py,0,"b""import os.path\nimport glob\nfrom .listdataset import ListDataset\nfrom .util import split2list\nimport flow_transforms\n\n'''\nDataset routines for MPI Sintel.\nhttp://sintel.is.tue.mpg.de/\nclean version imgs are without shaders, final version imgs are fully rendered\nThe dataset is not very big, you might want to only pretrain on it for flownet\n'''\n\n\ndef make_dataset(dataset_dir, split, dataset_type='clean'):\n    flow_dir = 'flow'\n    assert(os.path.isdir(os.path.join(dataset_dir,flow_dir)))\n    img_dir = dataset_type\n    assert(os.path.isdir(os.path.join(dataset_dir,img_dir)))\n\n    images = []\n    for flow_map in sorted(glob.glob(os.path.join(dataset_dir,flow_dir,'*','*.flo'))):\n        flow_map = os.path.relpath(flow_map,os.path.join(dataset_dir,flow_dir))\n\n        scene_dir, filename = os.path.split(flow_map)\n        no_ext_filename = os.path.splitext(filename)[0]\n        prefix, frame_nb = no_ext_filename.split('_')\n        frame_nb = int(frame_nb)\n        img1 = os.path.join(img_dir, scene_dir, '{}_{:04d}.png'.format(prefix, frame_nb))\n        img2 = os.path.join(img_dir, scene_dir, '{}_{:04d}.png'.format(prefix, frame_nb + 1))\n        flow_map = os.path.join(flow_dir,flow_map)\n        if not (os.path.isfile(os.path.join(dataset_dir,img1)) and os.path.isfile(os.path.join(dataset_dir,img2))):\n            continue\n        images.append([[img1,img2],flow_map])\n\n    return split2list(images, split, default_split=0.87)\n\n\ndef mpi_sintel_clean(root, transform=None, target_transform=None,\n                     co_transform=None, split=None):\n    train_list, test_list = make_dataset(root, split, 'clean')\n    train_dataset = ListDataset(root, train_list, transform, target_transform, co_transform)\n    test_dataset = ListDataset(root, test_list, transform, target_transform, flow_transforms.CenterCrop((384,1024)))\n\n    return train_dataset, test_dataset\n\n\ndef mpi_sintel_final(root, transform=None, target_transform=None,\n                     co_transform=None, split=None):\n    train_list, test_list = make_dataset(root, split, 'final')\n    train_dataset = ListDataset(root, train_list, transform, target_transform, co_transform)\n    test_dataset = ListDataset(root, test_list, transform, target_transform, flow_transforms.CenterCrop((384,1024)))\n\n    return train_dataset, test_dataset\n\n\ndef mpi_sintel_both(root, transform=None, target_transform=None,\n                    co_transform=None, split=None):\n    '''load images from both clean and final folders.\n    We cannot shuffle input, because it would very likely cause data snooping\n    for the clean and final frames are not that different'''\n    assert(isinstance(split, str)), 'To avoid data snooping, you must provide a static list of train/val when dealing with both clean and final.'\n    ' Look at Sintel_train_val.txt for an example'\n    train_list1, test_list1 = make_dataset(root, split, 'clean')\n    train_list2, test_list2 = make_dataset(root, split, 'final')\n    train_dataset = ListDataset(root, train_list1 + train_list2, transform, target_transform, co_transform)\n    test_dataset = ListDataset(root, test_list1 + test_list2, transform, target_transform, flow_transforms.CenterCrop((384,1024)))\n\n    return train_dataset, test_dataset\n"""
datasets/util.py,0,"b'import numpy as np\n\n\ndef split2list(images, split, default_split=0.9):\n    if isinstance(split, str):\n        with open(split) as f:\n            split_values = [x.strip() == \'1\' for x in f.readlines()]\n        assert(len(images) == len(split_values))\n    elif split is None:\n        split_values = np.random.uniform(0,1,len(images)) < default_split\n    else:\n        try:\n            split = float(split)\n        except TypeError:\n            print(""Invalid Split value, it must be either a filepath or a float"")\n            raise\n        split_values = np.random.uniform(0,1,len(images)) < split\n    train_samples = [sample for sample, split in zip(images, split_values) if split]\n    test_samples = [sample for sample, split in zip(images, split_values) if not split]\n    return train_samples, test_samples\n'"
models/FlowNetC.py,7,"b'import torch\nimport torch.nn as nn\nfrom torch.nn.init import kaiming_normal_, constant_\nfrom .util import conv, predict_flow, deconv, crop_like, correlate\n\n__all__ = [\n    \'flownetc\', \'flownetc_bn\'\n]\n\n\nclass FlowNetC(nn.Module):\n    expansion = 1\n\n    def __init__(self,batchNorm=True):\n        super(FlowNetC,self).__init__()\n\n        self.batchNorm = batchNorm\n        self.conv1      = conv(self.batchNorm,   3,   64, kernel_size=7, stride=2)\n        self.conv2      = conv(self.batchNorm,  64,  128, kernel_size=5, stride=2)\n        self.conv3      = conv(self.batchNorm, 128,  256, kernel_size=5, stride=2)\n        self.conv_redir = conv(self.batchNorm, 256,   32, kernel_size=1, stride=1)\n\n        self.conv3_1 = conv(self.batchNorm, 473,  256)\n        self.conv4   = conv(self.batchNorm, 256,  512, stride=2)\n        self.conv4_1 = conv(self.batchNorm, 512,  512)\n        self.conv5   = conv(self.batchNorm, 512,  512, stride=2)\n        self.conv5_1 = conv(self.batchNorm, 512,  512)\n        self.conv6   = conv(self.batchNorm, 512, 1024, stride=2)\n        self.conv6_1 = conv(self.batchNorm,1024, 1024)\n\n        self.deconv5 = deconv(1024,512)\n        self.deconv4 = deconv(1026,256)\n        self.deconv3 = deconv(770,128)\n        self.deconv2 = deconv(386,64)\n\n        self.predict_flow6 = predict_flow(1024)\n        self.predict_flow5 = predict_flow(1026)\n        self.predict_flow4 = predict_flow(770)\n        self.predict_flow3 = predict_flow(386)\n        self.predict_flow2 = predict_flow(194)\n\n        self.upsampled_flow6_to_5 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n        self.upsampled_flow5_to_4 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n        self.upsampled_flow4_to_3 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n        self.upsampled_flow3_to_2 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n                kaiming_normal_(m.weight, 0.1)\n                if m.bias is not None:\n                    constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                constant_(m.weight, 1)\n                constant_(m.bias, 0)\n\n    def forward(self, x):\n        x1 = x[:,:3]\n        x2 = x[:,3:]\n\n        out_conv1a = self.conv1(x1)\n        out_conv2a = self.conv2(out_conv1a)\n        out_conv3a = self.conv3(out_conv2a)\n\n        out_conv1b = self.conv1(x2)\n        out_conv2b = self.conv2(out_conv1b)\n        out_conv3b = self.conv3(out_conv2b)\n\n        out_conv_redir = self.conv_redir(out_conv3a)\n        out_correlation = correlate(out_conv3a,out_conv3b)\n\n        in_conv3_1 = torch.cat([out_conv_redir, out_correlation], dim=1)\n\n        out_conv3 = self.conv3_1(in_conv3_1)\n        out_conv4 = self.conv4_1(self.conv4(out_conv3))\n        out_conv5 = self.conv5_1(self.conv5(out_conv4))\n        out_conv6 = self.conv6_1(self.conv6(out_conv5))\n\n        flow6       = self.predict_flow6(out_conv6)\n        flow6_up    = crop_like(self.upsampled_flow6_to_5(flow6), out_conv5)\n        out_deconv5 = crop_like(self.deconv5(out_conv6), out_conv5)\n\n        concat5 = torch.cat((out_conv5,out_deconv5,flow6_up),1)\n        flow5       = self.predict_flow5(concat5)\n        flow5_up    = crop_like(self.upsampled_flow5_to_4(flow5), out_conv4)\n        out_deconv4 = crop_like(self.deconv4(concat5), out_conv4)\n\n        concat4 = torch.cat((out_conv4,out_deconv4,flow5_up),1)\n        flow4       = self.predict_flow4(concat4)\n        flow4_up    = crop_like(self.upsampled_flow4_to_3(flow4), out_conv3)\n        out_deconv3 = crop_like(self.deconv3(concat4), out_conv3)\n\n        concat3 = torch.cat((out_conv3,out_deconv3,flow4_up),1)\n        flow3       = self.predict_flow3(concat3)\n        flow3_up    = crop_like(self.upsampled_flow3_to_2(flow3), out_conv2a)\n        out_deconv2 = crop_like(self.deconv2(concat3), out_conv2a)\n\n        concat2 = torch.cat((out_conv2a,out_deconv2,flow3_up),1)\n        flow2 = self.predict_flow2(concat2)\n\n        if self.training:\n            return flow2,flow3,flow4,flow5,flow6\n        else:\n            return flow2\n\n    def weight_parameters(self):\n        return [param for name, param in self.named_parameters() if \'weight\' in name]\n\n    def bias_parameters(self):\n        return [param for name, param in self.named_parameters() if \'bias\' in name]\n\n\ndef flownetc(data=None):\n    """"""FlowNetS model architecture from the\n    ""Learning Optical Flow with Convolutional Networks"" paper (https://arxiv.org/abs/1504.06852)\n\n    Args:\n        data : pretrained weights of the network. will create a new one if not set\n    """"""\n    model = FlowNetC(batchNorm=False)\n    if data is not None:\n        model.load_state_dict(data[\'state_dict\'])\n    return model\n\n\ndef flownetc_bn(data=None):\n    """"""FlowNetS model architecture from the\n    ""Learning Optical Flow with Convolutional Networks"" paper (https://arxiv.org/abs/1504.06852)\n\n    Args:\n        data : pretrained weights of the network. will create a new one if not set\n    """"""\n    model = FlowNetC(batchNorm=True)\n    if data is not None:\n        model.load_state_dict(data[\'state_dict\'])\n    return model\n'"
models/FlowNetS.py,6,"b'import torch\nimport torch.nn as nn\nfrom torch.nn.init import kaiming_normal_, constant_\nfrom .util import conv, predict_flow, deconv, crop_like\n\n__all__ = [\n    \'flownets\', \'flownets_bn\'\n]\n\n\nclass FlowNetS(nn.Module):\n    expansion = 1\n\n    def __init__(self,batchNorm=True):\n        super(FlowNetS,self).__init__()\n\n        self.batchNorm = batchNorm\n        self.conv1   = conv(self.batchNorm,   6,   64, kernel_size=7, stride=2)\n        self.conv2   = conv(self.batchNorm,  64,  128, kernel_size=5, stride=2)\n        self.conv3   = conv(self.batchNorm, 128,  256, kernel_size=5, stride=2)\n        self.conv3_1 = conv(self.batchNorm, 256,  256)\n        self.conv4   = conv(self.batchNorm, 256,  512, stride=2)\n        self.conv4_1 = conv(self.batchNorm, 512,  512)\n        self.conv5   = conv(self.batchNorm, 512,  512, stride=2)\n        self.conv5_1 = conv(self.batchNorm, 512,  512)\n        self.conv6   = conv(self.batchNorm, 512, 1024, stride=2)\n        self.conv6_1 = conv(self.batchNorm,1024, 1024)\n\n        self.deconv5 = deconv(1024,512)\n        self.deconv4 = deconv(1026,256)\n        self.deconv3 = deconv(770,128)\n        self.deconv2 = deconv(386,64)\n\n        self.predict_flow6 = predict_flow(1024)\n        self.predict_flow5 = predict_flow(1026)\n        self.predict_flow4 = predict_flow(770)\n        self.predict_flow3 = predict_flow(386)\n        self.predict_flow2 = predict_flow(194)\n\n        self.upsampled_flow6_to_5 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n        self.upsampled_flow5_to_4 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n        self.upsampled_flow4_to_3 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n        self.upsampled_flow3_to_2 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n                kaiming_normal_(m.weight, 0.1)\n                if m.bias is not None:\n                    constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                constant_(m.weight, 1)\n                constant_(m.bias, 0)\n\n    def forward(self, x):\n        out_conv2 = self.conv2(self.conv1(x))\n        out_conv3 = self.conv3_1(self.conv3(out_conv2))\n        out_conv4 = self.conv4_1(self.conv4(out_conv3))\n        out_conv5 = self.conv5_1(self.conv5(out_conv4))\n        out_conv6 = self.conv6_1(self.conv6(out_conv5))\n\n        flow6       = self.predict_flow6(out_conv6)\n        flow6_up    = crop_like(self.upsampled_flow6_to_5(flow6), out_conv5)\n        out_deconv5 = crop_like(self.deconv5(out_conv6), out_conv5)\n\n        concat5 = torch.cat((out_conv5,out_deconv5,flow6_up),1)\n        flow5       = self.predict_flow5(concat5)\n        flow5_up    = crop_like(self.upsampled_flow5_to_4(flow5), out_conv4)\n        out_deconv4 = crop_like(self.deconv4(concat5), out_conv4)\n\n        concat4 = torch.cat((out_conv4,out_deconv4,flow5_up),1)\n        flow4       = self.predict_flow4(concat4)\n        flow4_up    = crop_like(self.upsampled_flow4_to_3(flow4), out_conv3)\n        out_deconv3 = crop_like(self.deconv3(concat4), out_conv3)\n\n        concat3 = torch.cat((out_conv3,out_deconv3,flow4_up),1)\n        flow3       = self.predict_flow3(concat3)\n        flow3_up    = crop_like(self.upsampled_flow3_to_2(flow3), out_conv2)\n        out_deconv2 = crop_like(self.deconv2(concat3), out_conv2)\n\n        concat2 = torch.cat((out_conv2,out_deconv2,flow3_up),1)\n        flow2 = self.predict_flow2(concat2)\n\n        if self.training:\n            return flow2,flow3,flow4,flow5,flow6\n        else:\n            return flow2\n\n    def weight_parameters(self):\n        return [param for name, param in self.named_parameters() if \'weight\' in name]\n\n    def bias_parameters(self):\n        return [param for name, param in self.named_parameters() if \'bias\' in name]\n\n\ndef flownets(data=None):\n    """"""FlowNetS model architecture from the\n    ""Learning Optical Flow with Convolutional Networks"" paper (https://arxiv.org/abs/1504.06852)\n\n    Args:\n        data : pretrained weights of the network. will create a new one if not set\n    """"""\n    model = FlowNetS(batchNorm=False)\n    if data is not None:\n        model.load_state_dict(data[\'state_dict\'])\n    return model\n\n\ndef flownets_bn(data=None):\n    """"""FlowNetS model architecture from the\n    ""Learning Optical Flow with Convolutional Networks"" paper (https://arxiv.org/abs/1504.06852)\n\n    Args:\n        data : pretrained weights of the network. will create a new one if not set\n    """"""\n    model = FlowNetS(batchNorm=True)\n    if data is not None:\n        model.load_state_dict(data[\'state_dict\'])\n    return model\n'"
models/__init__.py,0,b'from .FlowNetS import *\nfrom .FlowNetC import *'
models/util.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\n\ntry:\n    from spatial_correlation_sampler import spatial_correlation_sample\nexcept ImportError as e:\n    import warnings\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""default"", category=ImportWarning)\n        warnings.warn(""failed to load custom correlation module""\n                      ""which is needed for FlowNetC"", ImportWarning)\n\n\ndef conv(batchNorm, in_planes, out_planes, kernel_size=3, stride=1):\n    if batchNorm:\n        return nn.Sequential(\n            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=False),\n            nn.BatchNorm2d(out_planes),\n            nn.LeakyReLU(0.1,inplace=True)\n        )\n    else:\n        return nn.Sequential(\n            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=True),\n            nn.LeakyReLU(0.1,inplace=True)\n        )\n\n\ndef predict_flow(in_planes):\n    return nn.Conv2d(in_planes,2,kernel_size=3,stride=1,padding=1,bias=False)\n\n\ndef deconv(in_planes, out_planes):\n    return nn.Sequential(\n        nn.ConvTranspose2d(in_planes, out_planes, kernel_size=4, stride=2, padding=1, bias=False),\n        nn.LeakyReLU(0.1,inplace=True)\n    )\n\n\ndef correlate(input1, input2):\n    out_corr = spatial_correlation_sample(input1,\n                                          input2,\n                                          kernel_size=1,\n                                          patch_size=21,\n                                          stride=1,\n                                          padding=0,\n                                          dilation_patch=2)\n    # collate dimensions 1 and 2 in order to be treated as a\n    # regular 4D tensor\n    b, ph, pw, h, w = out_corr.size()\n    out_corr = out_corr.view(b, ph * pw, h, w)/input1.size(1)\n    return F.leaky_relu_(out_corr, 0.1)\n\n\ndef crop_like(input, target):\n    if input.size()[2:] == target.size()[2:]:\n        return input\n    else:\n        return input[:, :, :target.size(2), :target.size(3)]\n'"
