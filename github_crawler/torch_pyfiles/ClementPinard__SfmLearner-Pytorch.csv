file_path,api_count,code
custom_transforms.py,2,"b'from __future__ import division\nimport torch\nimport random\nimport numpy as np\nfrom PIL import Image\n\n\'\'\'Set of tranform random routines that takes list of inputs as arguments,\nin order to have random but coherent transformations.\'\'\'\n\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, images, intrinsics):\n        for t in self.transforms:\n            images, intrinsics = t(images, intrinsics)\n        return images, intrinsics\n\n\nclass Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, images, intrinsics):\n        for tensor in images:\n            for t, m, s in zip(tensor, self.mean, self.std):\n                t.sub_(m).div_(s)\n        return images, intrinsics\n\n\nclass ArrayToTensor(object):\n    """"""Converts a list of numpy.ndarray (H x W x C) along with a intrinsics matrix to a list of torch.FloatTensor of shape (C x H x W) with a intrinsics tensor.""""""\n\n    def __call__(self, images, intrinsics):\n        tensors = []\n        for im in images:\n            # put it from HWC to CHW format\n            im = np.transpose(im, (2, 0, 1))\n            # handle numpy array\n            tensors.append(torch.from_numpy(im).float()/255)\n        return tensors, intrinsics\n\n\nclass RandomHorizontalFlip(object):\n    """"""Randomly horizontally flips the given numpy array with a probability of 0.5""""""\n\n    def __call__(self, images, intrinsics):\n        assert intrinsics is not None\n        if random.random() < 0.5:\n            output_intrinsics = np.copy(intrinsics)\n            output_images = [np.copy(np.fliplr(im)) for im in images]\n            w = output_images[0].shape[1]\n            output_intrinsics[0,2] = w - output_intrinsics[0,2]\n        else:\n            output_images = images\n            output_intrinsics = intrinsics\n        return output_images, output_intrinsics\n\n\nclass RandomScaleCrop(object):\n    """"""Randomly zooms images up to 15% and crop them to keep same size as before.""""""\n\n    def __call__(self, images, intrinsics):\n        assert intrinsics is not None\n        output_intrinsics = np.copy(intrinsics)\n\n        in_h, in_w, _ = images[0].shape\n        x_scaling, y_scaling = np.random.uniform(1,1.15,2)\n        scaled_h, scaled_w = int(in_h * y_scaling), int(in_w * x_scaling)\n\n        output_intrinsics[0] *= x_scaling\n        output_intrinsics[1] *= y_scaling\n        scaled_images = [np.array(Image.fromarray(im.astype(np.uint8)).resize((scaled_w, scaled_h))).astype(np.float32) for im in images]\n\n        offset_y = np.random.randint(scaled_h - in_h + 1)\n        offset_x = np.random.randint(scaled_w - in_w + 1)\n        cropped_images = [im[offset_y:offset_y + in_h, offset_x:offset_x + in_w] for im in scaled_images]\n\n        output_intrinsics[0,2] -= offset_x\n        output_intrinsics[1,2] -= offset_y\n\n        return cropped_images, output_intrinsics\n'"
inverse_warp.py,18,"b'from __future__ import division\nimport torch\nimport torch.nn.functional as F\n\npixel_coords = None\n\n\ndef set_id_grid(depth):\n    global pixel_coords\n    b, h, w = depth.size()\n    i_range = torch.arange(0, h).view(1, h, 1).expand(1,h,w).type_as(depth)  # [1, H, W]\n    j_range = torch.arange(0, w).view(1, 1, w).expand(1,h,w).type_as(depth)  # [1, H, W]\n    ones = torch.ones(1,h,w).type_as(depth)\n\n    pixel_coords = torch.stack((j_range, i_range, ones), dim=1)  # [1, 3, H, W]\n\n\ndef check_sizes(input, input_name, expected):\n    condition = [input.ndimension() == len(expected)]\n    for i,size in enumerate(expected):\n        if size.isdigit():\n            condition.append(input.size(i) == int(size))\n    assert(all(condition)), ""wrong size for {}, expected {}, got  {}"".format(input_name, \'x\'.join(expected), list(input.size()))\n\n\ndef pixel2cam(depth, intrinsics_inv):\n    global pixel_coords\n    """"""Transform coordinates in the pixel frame to the camera frame.\n    Args:\n        depth: depth maps -- [B, H, W]\n        intrinsics_inv: intrinsics_inv matrix for each element of batch -- [B, 3, 3]\n    Returns:\n        array of (u,v,1) cam coordinates -- [B, 3, H, W]\n    """"""\n    b, h, w = depth.size()\n    if (pixel_coords is None) or pixel_coords.size(2) < h:\n        set_id_grid(depth)\n    current_pixel_coords = pixel_coords[:,:,:h,:w].expand(b,3,h,w).reshape(b, 3, -1)  # [B, 3, H*W]\n    cam_coords = (intrinsics_inv @ current_pixel_coords).reshape(b, 3, h, w)\n    return cam_coords * depth.unsqueeze(1)\n\n\ndef cam2pixel(cam_coords, proj_c2p_rot, proj_c2p_tr):\n    """"""Transform coordinates in the camera frame to the pixel frame.\n    Args:\n        cam_coords: pixel coordinates defined in the first camera coordinates system -- [B, 4, H, W]\n        proj_c2p_rot: rotation matrix of cameras -- [B, 3, 4]\n        proj_c2p_tr: translation vectors of cameras -- [B, 3, 1]\n    Returns:\n        array of [-1,1] coordinates -- [B, 2, H, W]\n    """"""\n    b, _, h, w = cam_coords.size()\n    cam_coords_flat = cam_coords.reshape(b, 3, -1)  # [B, 3, H*W]\n    if proj_c2p_rot is not None:\n        pcoords = proj_c2p_rot @ cam_coords_flat\n    else:\n        pcoords = cam_coords_flat\n\n    if proj_c2p_tr is not None:\n        pcoords = pcoords + proj_c2p_tr  # [B, 3, H*W]\n    X = pcoords[:, 0]\n    Y = pcoords[:, 1]\n    Z = pcoords[:, 2].clamp(min=1e-3)\n\n    X_norm = 2*(X / Z)/(w-1) - 1  # Normalized, -1 if on extreme left, 1 if on extreme right (x = w-1) [B, H*W]\n    Y_norm = 2*(Y / Z)/(h-1) - 1  # Idem [B, H*W]\n\n    pixel_coords = torch.stack([X_norm, Y_norm], dim=2)  # [B, H*W, 2]\n    return pixel_coords.reshape(b,h,w,2)\n\n\ndef euler2mat(angle):\n    """"""Convert euler angles to rotation matrix.\n\n     Reference: https://github.com/pulkitag/pycaffe-utils/blob/master/rot_utils.py#L174\n\n    Args:\n        angle: rotation angle along 3 axis (in radians) -- size = [B, 3]\n    Returns:\n        Rotation matrix corresponding to the euler angles -- size = [B, 3, 3]\n    """"""\n    B = angle.size(0)\n    x, y, z = angle[:,0], angle[:,1], angle[:,2]\n\n    cosz = torch.cos(z)\n    sinz = torch.sin(z)\n\n    zeros = z.detach()*0\n    ones = zeros.detach()+1\n    zmat = torch.stack([cosz, -sinz, zeros,\n                        sinz,  cosz, zeros,\n                        zeros, zeros,  ones], dim=1).reshape(B, 3, 3)\n\n    cosy = torch.cos(y)\n    siny = torch.sin(y)\n\n    ymat = torch.stack([cosy, zeros,  siny,\n                        zeros,  ones, zeros,\n                        -siny, zeros,  cosy], dim=1).reshape(B, 3, 3)\n\n    cosx = torch.cos(x)\n    sinx = torch.sin(x)\n\n    xmat = torch.stack([ones, zeros, zeros,\n                        zeros,  cosx, -sinx,\n                        zeros,  sinx,  cosx], dim=1).reshape(B, 3, 3)\n\n    rotMat = xmat @ ymat @ zmat\n    return rotMat\n\n\ndef quat2mat(quat):\n    """"""Convert quaternion coefficients to rotation matrix.\n\n    Args:\n        quat: first three coeff of quaternion of rotation. fourht is then computed to have a norm of 1 -- size = [B, 3]\n    Returns:\n        Rotation matrix corresponding to the quaternion -- size = [B, 3, 3]\n    """"""\n    norm_quat = torch.cat([quat[:,:1].detach()*0 + 1, quat], dim=1)\n    norm_quat = norm_quat/norm_quat.norm(p=2, dim=1, keepdim=True)\n    w, x, y, z = norm_quat[:,0], norm_quat[:,1], norm_quat[:,2], norm_quat[:,3]\n\n    B = quat.size(0)\n\n    w2, x2, y2, z2 = w.pow(2), x.pow(2), y.pow(2), z.pow(2)\n    wx, wy, wz = w*x, w*y, w*z\n    xy, xz, yz = x*y, x*z, y*z\n\n    rotMat = torch.stack([w2 + x2 - y2 - z2, 2*xy - 2*wz, 2*wy + 2*xz,\n                          2*wz + 2*xy, w2 - x2 + y2 - z2, 2*yz - 2*wx,\n                          2*xz - 2*wy, 2*wx + 2*yz, w2 - x2 - y2 + z2], dim=1).reshape(B, 3, 3)\n    return rotMat\n\n\ndef pose_vec2mat(vec, rotation_mode=\'euler\'):\n    """"""\n    Convert 6DoF parameters to transformation matrix.\n\n    Args:s\n        vec: 6DoF parameters in the order of tx, ty, tz, rx, ry, rz -- [B, 6]\n    Returns:\n        A transformation matrix -- [B, 3, 4]\n    """"""\n    translation = vec[:, :3].unsqueeze(-1)  # [B, 3, 1]\n    rot = vec[:,3:]\n    if rotation_mode == \'euler\':\n        rot_mat = euler2mat(rot)  # [B, 3, 3]\n    elif rotation_mode == \'quat\':\n        rot_mat = quat2mat(rot)  # [B, 3, 3]\n    transform_mat = torch.cat([rot_mat, translation], dim=2)  # [B, 3, 4]\n    return transform_mat\n\n\ndef inverse_warp(img, depth, pose, intrinsics, rotation_mode=\'euler\', padding_mode=\'zeros\'):\n    """"""\n    Inverse warp a source image to the target image plane.\n\n    Args:\n        img: the source image (where to sample pixels) -- [B, 3, H, W]\n        depth: depth map of the target image -- [B, H, W]\n        pose: 6DoF pose parameters from target to source -- [B, 6]\n        intrinsics: camera intrinsic matrix -- [B, 3, 3]\n    Returns:\n        projected_img: Source image warped to the target image plane\n        valid_points: Boolean array indicating point validity\n    """"""\n    check_sizes(img, \'img\', \'B3HW\')\n    check_sizes(depth, \'depth\', \'BHW\')\n    check_sizes(pose, \'pose\', \'B6\')\n    check_sizes(intrinsics, \'intrinsics\', \'B33\')\n\n    batch_size, _, img_height, img_width = img.size()\n\n    cam_coords = pixel2cam(depth, intrinsics.inverse())  # [B,3,H,W]\n\n    pose_mat = pose_vec2mat(pose, rotation_mode)  # [B,3,4]\n\n    # Get projection matrix for tgt camera frame to source pixel frame\n    proj_cam_to_src_pixel = intrinsics @ pose_mat  # [B, 3, 4]\n\n    rot, tr = proj_cam_to_src_pixel[:,:,:3], proj_cam_to_src_pixel[:,:,-1:]\n    src_pixel_coords = cam2pixel(cam_coords, rot, tr)  # [B,H,W,2]\n    projected_img = F.grid_sample(img, src_pixel_coords, padding_mode=padding_mode, align_corners=True)\n\n    valid_points = src_pixel_coords.abs().max(dim=-1)[0] <= 1\n\n    return projected_img, valid_points\n'"
logger.py,0,"b'from blessings import Terminal\nimport progressbar\nimport sys\n\n\nclass TermLogger(object):\n    def __init__(self, n_epochs, train_size, valid_size):\n        self.n_epochs = n_epochs\n        self.train_size = train_size\n        self.valid_size = valid_size\n        self.t = Terminal()\n        s = 10\n        e = 1   # epoch bar position\n        tr = 3  # train bar position\n        ts = 6  # valid bar position\n        h = self.t.height\n\n        for i in range(10):\n            print(\'\')\n        self.epoch_bar = progressbar.ProgressBar(max_value=n_epochs, fd=Writer(self.t, (0, h-s+e)))\n\n        self.train_writer = Writer(self.t, (0, h-s+tr))\n        self.train_bar_writer = Writer(self.t, (0, h-s+tr+1))\n\n        self.valid_writer = Writer(self.t, (0, h-s+ts))\n        self.valid_bar_writer = Writer(self.t, (0, h-s+ts+1))\n\n        self.reset_train_bar()\n        self.reset_valid_bar()\n\n    def reset_train_bar(self, train_size=None):\n        self.train_bar = progressbar.ProgressBar(max_value=train_size if train_size is not None else self.train_size,\n                                                 fd=self.train_bar_writer)\n\n    def reset_valid_bar(self):\n        self.valid_bar = progressbar.ProgressBar(max_value=self.valid_size, fd=self.valid_bar_writer)\n\n\nclass Writer(object):\n    """"""Create an object with a write method that writes to a\n    specific place on the screen, defined at instantiation.\n\n    This is the glue between blessings and progressbar.\n    """"""\n\n    def __init__(self, t, location):\n        """"""\n        Input: location - tuple of ints (x, y), the position\n                        of the bar in the terminal\n        """"""\n        self.location = location\n        self.t = t\n\n    def write(self, string):\n        with self.t.location(*self.location):\n            sys.stdout.write(""\\033[K"")\n            print(string)\n\n    def flush(self):\n        return\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self, i=1, precision=3):\n        self.meters = i\n        self.precision = precision\n        self.reset(self.meters)\n\n    def reset(self, i):\n        self.val = [0]*i\n        self.avg = [0]*i\n        self.sum = [0]*i\n        self.count = 0\n\n    def update(self, val, n=1):\n        if not isinstance(val, list):\n            val = [val]\n        assert(len(val) == self.meters)\n        self.count += n\n        for i,v in enumerate(val):\n            self.val[i] = v\n            self.sum[i] += v * n\n            self.avg[i] = self.sum[i] / self.count\n\n    def __repr__(self):\n        val = \' \'.join([\'{:.{}f}\'.format(v, self.precision) for v in self.val])\n        avg = \' \'.join([\'{:.{}f}\'.format(a, self.precision) for a in self.avg])\n        return \'{} ({})\'.format(val, avg)\n'"
loss_functions.py,9,"b""from __future__ import division\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom inverse_warp import inverse_warp\n\n\ndef photometric_reconstruction_loss(tgt_img, ref_imgs, intrinsics,\n                                    depth, explainability_mask, pose,\n                                    rotation_mode='euler', padding_mode='zeros'):\n    def one_scale(depth, explainability_mask):\n        assert(explainability_mask is None or depth.size()[2:] == explainability_mask.size()[2:])\n        assert(pose.size(1) == len(ref_imgs))\n\n        reconstruction_loss = 0\n        b, _, h, w = depth.size()\n        downscale = tgt_img.size(2)/h\n\n        tgt_img_scaled = F.interpolate(tgt_img, (h, w), mode='area')\n        ref_imgs_scaled = [F.interpolate(ref_img, (h, w), mode='area') for ref_img in ref_imgs]\n        intrinsics_scaled = torch.cat((intrinsics[:, 0:2]/downscale, intrinsics[:, 2:]), dim=1)\n\n        warped_imgs = []\n        diff_maps = []\n\n        for i, ref_img in enumerate(ref_imgs_scaled):\n            current_pose = pose[:, i]\n\n            ref_img_warped, valid_points = inverse_warp(ref_img, depth[:,0], current_pose,\n                                                        intrinsics_scaled,\n                                                        rotation_mode, padding_mode)\n            diff = (tgt_img_scaled - ref_img_warped) * valid_points.unsqueeze(1).float()\n\n            if explainability_mask is not None:\n                diff = diff * explainability_mask[:,i:i+1].expand_as(diff)\n\n            reconstruction_loss += diff.abs().mean()\n            assert((reconstruction_loss == reconstruction_loss).item() == 1)\n\n            warped_imgs.append(ref_img_warped[0])\n            diff_maps.append(diff[0])\n\n        return reconstruction_loss, warped_imgs, diff_maps\n\n    warped_results, diff_results = [], []\n    if type(explainability_mask) not in [tuple, list]:\n        explainability_mask = [explainability_mask]\n    if type(depth) not in [list, tuple]:\n        depth = [depth]\n\n    total_loss = 0\n    for d, mask in zip(depth, explainability_mask):\n        loss, warped, diff = one_scale(d, mask)\n        total_loss += loss\n        warped_results.append(warped)\n        diff_results.append(diff)\n    return total_loss, warped_results, diff_results\n\n\ndef explainability_loss(mask):\n    if type(mask) not in [tuple, list]:\n        mask = [mask]\n    loss = 0\n    for mask_scaled in mask:\n        ones_var = torch.ones_like(mask_scaled)\n        loss += nn.functional.binary_cross_entropy(mask_scaled, ones_var)\n    return loss\n\n\ndef smooth_loss(pred_map):\n    def gradient(pred):\n        D_dy = pred[:, :, 1:] - pred[:, :, :-1]\n        D_dx = pred[:, :, :, 1:] - pred[:, :, :, :-1]\n        return D_dx, D_dy\n\n    if type(pred_map) not in [tuple, list]:\n        pred_map = [pred_map]\n\n    loss = 0\n    weight = 1.\n\n    for scaled_map in pred_map:\n        dx, dy = gradient(scaled_map)\n        dx2, dxdy = gradient(dx)\n        dydx, dy2 = gradient(dy)\n        loss += (dx2.abs().mean() + dxdy.abs().mean() + dydx.abs().mean() + dy2.abs().mean())*weight\n        weight /= 2.3  # don't ask me why it works better\n    return loss\n\n\n@torch.no_grad()\ndef compute_errors(gt, pred, crop=True):\n    abs_diff, abs_rel, sq_rel, a1, a2, a3 = 0,0,0,0,0,0\n    batch_size = gt.size(0)\n\n    '''\n    crop used by Garg ECCV16 to reprocude Eigen NIPS14 results\n    construct a mask of False values, with the same size as target\n    and then set to True values inside the crop\n    '''\n    if crop:\n        crop_mask = gt[0] != gt[0]\n        y1,y2 = int(0.40810811 * gt.size(1)), int(0.99189189 * gt.size(1))\n        x1,x2 = int(0.03594771 * gt.size(2)), int(0.96405229 * gt.size(2))\n        crop_mask[y1:y2,x1:x2] = 1\n\n    for current_gt, current_pred in zip(gt, pred):\n        valid = (current_gt > 0) & (current_gt < 80)\n        if crop:\n            valid = valid & crop_mask\n\n        valid_gt = current_gt[valid]\n        valid_pred = current_pred[valid].clamp(1e-3, 80)\n\n        valid_pred = valid_pred * torch.median(valid_gt)/torch.median(valid_pred)\n\n        thresh = torch.max((valid_gt / valid_pred), (valid_pred / valid_gt))\n        a1 += (thresh < 1.25).float().mean()\n        a2 += (thresh < 1.25 ** 2).float().mean()\n        a3 += (thresh < 1.25 ** 3).float().mean()\n\n        abs_diff += torch.mean(torch.abs(valid_gt - valid_pred))\n        abs_rel += torch.mean(torch.abs(valid_gt - valid_pred) / valid_gt)\n\n        sq_rel += torch.mean(((valid_gt - valid_pred)**2) / valid_gt)\n\n    return [metric.item() / batch_size for metric in [abs_diff, abs_rel, sq_rel, a1, a2, a3]]\n"""
run_inference.py,4,"b'import torch\n\nfrom imageio import imread, imsave\nfrom PIL import Image\nimport numpy as np\nfrom path import Path\nimport argparse\nfrom tqdm import tqdm\n\nfrom models import DispNetS\nfrom utils import tensor2array\n\nparser = argparse.ArgumentParser(description=\'Inference script for DispNet learned with \\\n                                 Structure from Motion Learner inference on KITTI and CityScapes Dataset\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(""--output-disp"", action=\'store_true\', help=""save disparity img"")\nparser.add_argument(""--output-depth"", action=\'store_true\', help=""save depth img"")\nparser.add_argument(""--pretrained"", required=True, type=str, help=""pretrained DispNet path"")\nparser.add_argument(""--img-height"", default=128, type=int, help=""Image height"")\nparser.add_argument(""--img-width"", default=416, type=int, help=""Image width"")\nparser.add_argument(""--no-resize"", action=\'store_true\', help=""no resizing is done"")\n\nparser.add_argument(""--dataset-list"", default=None, type=str, help=""Dataset list file"")\nparser.add_argument(""--dataset-dir"", default=\'.\', type=str, help=""Dataset directory"")\nparser.add_argument(""--output-dir"", default=\'output\', type=str, help=""Output directory"")\n\nparser.add_argument(""--img-exts"", default=[\'png\', \'jpg\', \'bmp\'], nargs=\'*\', type=str, help=""images extensions to glob"")\n\ndevice = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"")\n\n\n@torch.no_grad()\ndef main():\n    args = parser.parse_args()\n    if not(args.output_disp or args.output_depth):\n        print(\'You must at least output one value !\')\n        return\n\n    disp_net = DispNetS().to(device)\n    weights = torch.load(args.pretrained)\n    disp_net.load_state_dict(weights[\'state_dict\'])\n    disp_net.eval()\n\n    dataset_dir = Path(args.dataset_dir)\n    output_dir = Path(args.output_dir)\n    output_dir.makedirs_p()\n\n    if args.dataset_list is not None:\n        with open(args.dataset_list, \'r\') as f:\n            test_files = [dataset_dir/file for file in f.read().splitlines()]\n    else:\n        test_files = sum([list(dataset_dir.walkfiles(\'*.{}\'.format(ext))) for ext in args.img_exts], [])\n\n    print(\'{} files to test\'.format(len(test_files)))\n\n    for file in tqdm(test_files):\n\n        img = imread(file)\n\n        h,w,_ = img.shape\n        if (not args.no_resize) and (h != args.img_height or w != args.img_width):\n            img = np.array(Image.fromarray(img).imresize((args.img_height, args.img_width)))\n        img = np.transpose(img, (2, 0, 1))\n\n        tensor_img = torch.from_numpy(img.astype(np.float32)).unsqueeze(0)\n        tensor_img = ((tensor_img/255 - 0.5)/0.5).to(device)\n\n        output = disp_net(tensor_img)[0]\n\n        file_path, file_ext = file.relpath(args.dataset_dir).splitext()\n        print(file_path)\n        print(file_path.splitall())\n        file_name = \'-\'.join(file_path.splitall()[1:])\n        print(file_name)\n\n        if args.output_disp:\n            disp = (255*tensor2array(output, max_value=None, colormap=\'bone\')).astype(np.uint8)\n            imsave(output_dir/\'{}_disp{}\'.format(file_name, file_ext), np.transpose(disp, (1,2,0)))\n        if args.output_depth:\n            depth = 1/output\n            depth = (255*tensor2array(depth, max_value=10, colormap=\'rainbow\')).astype(np.uint8)\n            imsave(output_dir/\'{}_depth{}\'.format(file_name, file_ext), np.transpose(depth, (1,2,0)))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
test_disp.py,6,"b'import torch\n\nfrom scipy.misc import imresize\nfrom scipy.ndimage.interpolation import zoom\nimport numpy as np\nfrom path import Path\nimport argparse\nfrom tqdm import tqdm\n\nfrom models import DispNetS, PoseExpNet\n\n\nparser = argparse.ArgumentParser(description=\'Script for DispNet testing with corresponding groundTruth\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(""--pretrained-dispnet"", required=True, type=str, help=""pretrained DispNet path"")\nparser.add_argument(""--pretrained-posenet"", default=None, type=str, help=""pretrained PoseNet path (for scale factor)"")\nparser.add_argument(""--img-height"", default=128, type=int, help=""Image height"")\nparser.add_argument(""--img-width"", default=416, type=int, help=""Image width"")\nparser.add_argument(""--no-resize"", action=\'store_true\', help=""no resizing is done"")\nparser.add_argument(""--min-depth"", default=1e-3)\nparser.add_argument(""--max-depth"", default=80)\n\nparser.add_argument(""--dataset-dir"", default=\'.\', type=str, help=""Dataset directory"")\nparser.add_argument(""--dataset-list"", default=None, type=str, help=""Dataset list file"")\nparser.add_argument(""--output-dir"", default=None, type=str, help=""Output directory for saving predictions in a big 3D numpy file"")\n\nparser.add_argument(""--gt-type"", default=\'KITTI\', type=str, help=""GroundTruth data type"", choices=[\'npy\', \'png\', \'KITTI\', \'stillbox\'])\nparser.add_argument(""--gps"", \'-g\', action=\'store_true\',\n                    help=\'if selected, will get displacement from GPS for KITTI. Otherwise, will integrate speed\')\nparser.add_argument(""--img-exts"", default=[\'png\', \'jpg\', \'bmp\'], nargs=\'*\', type=str, help=""images extensions to glob"")\n\ndevice = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"")\n\n\n@torch.no_grad()\ndef main():\n    args = parser.parse_args()\n    if args.gt_type == \'KITTI\':\n        from kitti_eval.depth_evaluation_utils import test_framework_KITTI as test_framework\n    elif args.gt_type == \'stillbox\':\n        from stillbox_eval.depth_evaluation_utils import test_framework_stillbox as test_framework\n\n    disp_net = DispNetS().to(device)\n    weights = torch.load(args.pretrained_dispnet)\n    disp_net.load_state_dict(weights[\'state_dict\'])\n    disp_net.eval()\n\n    if args.pretrained_posenet is None:\n        print(\'no PoseNet specified, scale_factor will be determined by median ratio, which is kiiinda cheating\\\n            (but consistent with original paper)\')\n        seq_length = 1\n    else:\n        weights = torch.load(args.pretrained_posenet)\n        seq_length = int(weights[\'state_dict\'][\'conv1.0.weight\'].size(1)/3)\n        pose_net = PoseExpNet(nb_ref_imgs=seq_length - 1, output_exp=False).to(device)\n        pose_net.load_state_dict(weights[\'state_dict\'], strict=False)\n\n    dataset_dir = Path(args.dataset_dir)\n    if args.dataset_list is not None:\n        with open(args.dataset_list, \'r\') as f:\n            test_files = list(f.read().splitlines())\n    else:\n        test_files = [file.relpathto(dataset_dir) for file in sum([dataset_dir.files(\'*.{}\'.format(ext)) for ext in args.img_exts], [])]\n\n    framework = test_framework(dataset_dir, test_files, seq_length,\n                               args.min_depth, args.max_depth,\n                               use_gps=args.gps)\n\n    print(\'{} files to test\'.format(len(test_files)))\n    errors = np.zeros((2, 9, len(test_files)), np.float32)\n    if args.output_dir is not None:\n        output_dir = Path(args.output_dir)\n        output_dir.makedirs_p()\n\n    for j, sample in enumerate(tqdm(framework)):\n        tgt_img = sample[\'tgt\']\n\n        ref_imgs = sample[\'ref\']\n\n        h,w,_ = tgt_img.shape\n        if (not args.no_resize) and (h != args.img_height or w != args.img_width):\n            tgt_img = imresize(tgt_img, (args.img_height, args.img_width)).astype(np.float32)\n            ref_imgs = [imresize(img, (args.img_height, args.img_width)).astype(np.float32) for img in ref_imgs]\n\n        tgt_img = np.transpose(tgt_img, (2, 0, 1))\n        ref_imgs = [np.transpose(img, (2,0,1)) for img in ref_imgs]\n\n        tgt_img = torch.from_numpy(tgt_img).unsqueeze(0)\n        tgt_img = ((tgt_img/255 - 0.5)/0.5).to(device)\n\n        for i, img in enumerate(ref_imgs):\n            img = torch.from_numpy(img).unsqueeze(0)\n            img = ((img/255 - 0.5)/0.5).to(device)\n            ref_imgs[i] = img\n\n        pred_disp = disp_net(tgt_img).cpu().numpy()[0,0]\n\n        if args.output_dir is not None:\n            if j == 0:\n                predictions = np.zeros((len(test_files), *pred_disp.shape))\n            predictions[j] = 1/pred_disp\n\n        gt_depth = sample[\'gt_depth\']\n\n        pred_depth = 1/pred_disp\n        pred_depth_zoomed = zoom(pred_depth,\n                                 (gt_depth.shape[0]/pred_depth.shape[0],\n                                  gt_depth.shape[1]/pred_depth.shape[1])\n                                 ).clip(args.min_depth, args.max_depth)\n        if sample[\'mask\'] is not None:\n            pred_depth_zoomed = pred_depth_zoomed[sample[\'mask\']]\n            gt_depth = gt_depth[sample[\'mask\']]\n\n        if seq_length > 1:\n            # Reorganize ref_imgs : tgt is middle frame but not necessarily the one used in DispNetS\n            # (in case sample to test was in end or beginning of the image sequence)\n            middle_index = seq_length//2\n            tgt = ref_imgs[middle_index]\n            reorganized_refs = ref_imgs[:middle_index] + ref_imgs[middle_index + 1:]\n            _, poses = pose_net(tgt, reorganized_refs)\n            displacement_magnitudes = poses[0,:,:3].norm(2,1).cpu().numpy()\n\n            scale_factor = np.mean(sample[\'displacements\'] / displacement_magnitudes)\n            errors[0,:,j] = compute_errors(gt_depth, pred_depth_zoomed*scale_factor)\n\n        scale_factor = np.median(gt_depth)/np.median(pred_depth_zoomed)\n        errors[1,:,j] = compute_errors(gt_depth, pred_depth_zoomed*scale_factor)\n\n    mean_errors = errors.mean(2)\n    error_names = [\'abs_diff\', \'abs_rel\',\'sq_rel\',\'rms\',\'log_rms\', \'abs_log\', \'a1\',\'a2\',\'a3\']\n    if args.pretrained_posenet:\n        print(""Results with scale factor determined by PoseNet : "")\n        print(""{:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}"".format(*error_names))\n        print(""{:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}"".format(*mean_errors[0]))\n\n    print(""Results with scale factor determined by GT/prediction ratio (like the original paper) : "")\n    print(""{:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}"".format(*error_names))\n    print(""{:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}"".format(*mean_errors[1]))\n\n    if args.output_dir is not None:\n        np.save(output_dir/\'predictions.npy\', predictions)\n\n\ndef compute_errors(gt, pred):\n    thresh = np.maximum((gt / pred), (pred / gt))\n    a1 = (thresh < 1.25   ).mean()\n    a2 = (thresh < 1.25 ** 2).mean()\n    a3 = (thresh < 1.25 ** 3).mean()\n\n    rmse = (gt - pred) ** 2\n    rmse = np.sqrt(rmse.mean())\n\n    rmse_log = (np.log(gt) - np.log(pred)) ** 2\n    rmse_log = np.sqrt(rmse_log.mean())\n\n    abs_log = np.mean(np.abs(np.log(gt) - np.log(pred)))\n\n    abs_rel = np.mean(np.abs(gt - pred) / gt)\n    abs_diff = np.mean(np.abs(gt - pred))\n\n    sq_rel = np.mean(((gt - pred)**2) / gt)\n\n    return abs_diff, abs_rel, sq_rel, rmse, rmse_log, abs_log, a1, a2, a3\n\n\nif __name__ == \'__main__\':\n    main()\n'"
test_pose.py,6,"b'import torch\nfrom torch.autograd import Variable\n\nfrom scipy.misc import imresize\nimport numpy as np\nfrom path import Path\nimport argparse\nfrom tqdm import tqdm\n\nfrom models import PoseExpNet\nfrom inverse_warp import pose_vec2mat\n\n\nparser = argparse.ArgumentParser(description=\'Script for PoseNet testing with corresponding groundTruth from KITTI Odometry\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(""pretrained_posenet"", type=str, help=""pretrained PoseNet path"")\nparser.add_argument(""--img-height"", default=128, type=int, help=""Image height"")\nparser.add_argument(""--img-width"", default=416, type=int, help=""Image width"")\nparser.add_argument(""--no-resize"", action=\'store_true\', help=""no resizing is done"")\nparser.add_argument(""--min-depth"", default=1e-3)\nparser.add_argument(""--max-depth"", default=80)\n\nparser.add_argument(""--dataset-dir"", default=\'.\', type=str, help=""Dataset directory"")\nparser.add_argument(""--sequences"", default=[\'09\'], type=str, nargs=\'*\', help=""sequences to test"")\nparser.add_argument(""--output-dir"", default=None, type=str, help=""Output directory for saving predictions in a big 3D numpy file"")\nparser.add_argument(""--img-exts"", default=[\'png\', \'jpg\', \'bmp\'], nargs=\'*\', type=str, help=""images extensions to glob"")\nparser.add_argument(""--rotation-mode"", default=\'euler\', choices=[\'euler\', \'quat\'], type=str)\n\ndevice = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"")\n\n\n@torch.no_grad()\ndef main():\n    args = parser.parse_args()\n    from kitti_eval.pose_evaluation_utils import test_framework_KITTI as test_framework\n\n    weights = torch.load(args.pretrained_posenet)\n    seq_length = int(weights[\'state_dict\'][\'conv1.0.weight\'].size(1)/3)\n    pose_net = PoseExpNet(nb_ref_imgs=seq_length - 1, output_exp=False).to(device)\n    pose_net.load_state_dict(weights[\'state_dict\'], strict=False)\n\n    dataset_dir = Path(args.dataset_dir)\n    framework = test_framework(dataset_dir, args.sequences, seq_length)\n\n    print(\'{} snippets to test\'.format(len(framework)))\n    errors = np.zeros((len(framework), 2), np.float32)\n    if args.output_dir is not None:\n        output_dir = Path(args.output_dir)\n        output_dir.makedirs_p()\n        predictions_array = np.zeros((len(framework), seq_length, 3, 4))\n\n    for j, sample in enumerate(tqdm(framework)):\n        imgs = sample[\'imgs\']\n\n        h,w,_ = imgs[0].shape\n        if (not args.no_resize) and (h != args.img_height or w != args.img_width):\n            imgs = [imresize(img, (args.img_height, args.img_width)).astype(np.float32) for img in imgs]\n\n        imgs = [np.transpose(img, (2,0,1)) for img in imgs]\n\n        ref_imgs = []\n        for i, img in enumerate(imgs):\n            img = torch.from_numpy(img).unsqueeze(0)\n            img = ((img/255 - 0.5)/0.5).to(device)\n            if i == len(imgs)//2:\n                tgt_img = img\n            else:\n                ref_imgs.append(img)\n\n        _, poses = pose_net(tgt_img, ref_imgs)\n\n        poses = poses.cpu()[0]\n        poses = torch.cat([poses[:len(imgs)//2], torch.zeros(1,6).float(), poses[len(imgs)//2:]])\n\n        inv_transform_matrices = pose_vec2mat(poses, rotation_mode=args.rotation_mode).numpy().astype(np.float64)\n\n        rot_matrices = np.linalg.inv(inv_transform_matrices[:,:,:3])\n        tr_vectors = -rot_matrices @ inv_transform_matrices[:,:,-1:]\n\n        transform_matrices = np.concatenate([rot_matrices, tr_vectors], axis=-1)\n\n        first_inv_transform = inv_transform_matrices[0]\n        final_poses = first_inv_transform[:,:3] @ transform_matrices\n        final_poses[:,:,-1:] += first_inv_transform[:,-1:]\n\n        if args.output_dir is not None:\n            predictions_array[j] = final_poses\n\n        ATE, RE = compute_pose_error(sample[\'poses\'], final_poses)\n        errors[j] = ATE, RE\n\n    mean_errors = errors.mean(0)\n    std_errors = errors.std(0)\n    error_names = [\'ATE\',\'RE\']\n    print(\'\')\n    print(""Results"")\n    print(""\\t {:>10}, {:>10}"".format(*error_names))\n    print(""mean \\t {:10.4f}, {:10.4f}"".format(*mean_errors))\n    print(""std \\t {:10.4f}, {:10.4f}"".format(*std_errors))\n\n    if args.output_dir is not None:\n        np.save(output_dir/\'predictions.npy\', predictions_array)\n\n\ndef compute_pose_error(gt, pred):\n    RE = 0\n    snippet_length = gt.shape[0]\n    scale_factor = np.sum(gt[:,:,-1] * pred[:,:,-1])/np.sum(pred[:,:,-1] ** 2)\n    ATE = np.linalg.norm((gt[:,:,-1] - scale_factor * pred[:,:,-1]).reshape(-1))\n    for gt_pose, pred_pose in zip(gt, pred):\n        # Residual matrix to which we compute angle\'s sin and cos\n        R = gt_pose[:,:3] @ np.linalg.inv(pred_pose[:,:3])\n        s = np.linalg.norm([R[0,1]-R[1,0],\n                            R[1,2]-R[2,1],\n                            R[0,2]-R[2,0]])\n        c = np.trace(R) - 1\n        # Note: we actually compute double of cos and sin, but arctan2 is invariant to scale\n        RE += np.arctan2(s,c)\n\n    return ATE/snippet_length, RE/snippet_length\n\n\nif __name__ == \'__main__\':\n    main()\n'"
train.py,15,"b'import argparse\nimport time\nimport csv\n\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport custom_transforms\nimport models\nfrom utils import tensor2array, save_checkpoint, save_path_formatter, log_output_tensorboard\n\nfrom loss_functions import photometric_reconstruction_loss, explainability_loss, smooth_loss, compute_errors\nfrom logger import TermLogger, AverageMeter\nfrom tensorboardX import SummaryWriter\n\nparser = argparse.ArgumentParser(description=\'Structure from Motion Learner training on KITTI and CityScapes Dataset\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\nparser.add_argument(\'data\', metavar=\'DIR\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--dataset-format\', default=\'sequential\', metavar=\'STR\',\n                    help=\'dataset format, stacked: stacked frames (from original TensorFlow code) \\\n                    sequential: sequential folders (easier to convert to with a non KITTI/Cityscape dataset\')\nparser.add_argument(\'--sequence-length\', type=int, metavar=\'N\', help=\'sequence length for training\', default=3)\nparser.add_argument(\'--rotation-mode\', type=str, choices=[\'euler\', \'quat\'], default=\'euler\',\n                    help=\'rotation mode for PoseExpnet : euler (yaw,pitch,roll) or quaternion (last 3 coefficients)\')\nparser.add_argument(\'--padding-mode\', type=str, choices=[\'zeros\', \'border\'], default=\'zeros\',\n                    help=\'padding mode for image warping : this is important for photometric differenciation when going outside target image.\'\n                         \' zeros will null gradients outside target image.\'\n                         \' border will only null gradients of the coordinate outside (x or y)\')\nparser.add_argument(\'--with-gt\', action=\'store_true\', help=\'use ground truth for validation. \\\n                    You need to store it in npy 2D arrays see data/kitti_raw_loader.py for an example\')\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                    help=\'number of data loading workers\')\nparser.add_argument(\'--epochs\', default=200, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--epoch-size\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch size (will match dataset size if not set)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=4, type=int,\n                    metavar=\'N\', help=\'mini-batch size\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=2e-4, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum for sgd, alpha parameter for adam\')\nparser.add_argument(\'--beta\', default=0.999, type=float, metavar=\'M\',\n                    help=\'beta parameters for adam\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=0, type=float,\n                    metavar=\'W\', help=\'weight decay\')\nparser.add_argument(\'--print-freq\', default=10, type=int,\n                    metavar=\'N\', help=\'print frequency\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                    help=\'evaluate model on validation set\')\nparser.add_argument(\'--pretrained-disp\', dest=\'pretrained_disp\', default=None, metavar=\'PATH\',\n                    help=\'path to pre-trained dispnet model\')\nparser.add_argument(\'--pretrained-exppose\', dest=\'pretrained_exp_pose\', default=None, metavar=\'PATH\',\n                    help=\'path to pre-trained Exp Pose net model\')\nparser.add_argument(\'--seed\', default=0, type=int, help=\'seed for random functions, and network initialization\')\nparser.add_argument(\'--log-summary\', default=\'progress_log_summary.csv\', metavar=\'PATH\',\n                    help=\'csv where to save per-epoch train and valid stats\')\nparser.add_argument(\'--log-full\', default=\'progress_log_full.csv\', metavar=\'PATH\',\n                    help=\'csv where to save per-gradient descent train stats\')\nparser.add_argument(\'-p\', \'--photo-loss-weight\', type=float, help=\'weight for photometric loss\', metavar=\'W\', default=1)\nparser.add_argument(\'-m\', \'--mask-loss-weight\', type=float, help=\'weight for explainabilty mask loss\', metavar=\'W\', default=0)\nparser.add_argument(\'-s\', \'--smooth-loss-weight\', type=float, help=\'weight for disparity smoothness loss\', metavar=\'W\', default=0.1)\nparser.add_argument(\'--log-output\', action=\'store_true\', help=\'will log dispnet outputs and warped imgs at validation step\')\nparser.add_argument(\'-f\', \'--training-output-freq\', type=int,\n                    help=\'frequence for outputting dispnet outputs and warped imgs at training for all scales if 0 will not output\',\n                    metavar=\'N\', default=0)\n\nbest_error = -1\nn_iter = 0\ndevice = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"")\n\n\ndef main():\n    global best_error, n_iter, device\n    args = parser.parse_args()\n    if args.dataset_format == \'stacked\':\n        from datasets.stacked_sequence_folders import SequenceFolder\n    elif args.dataset_format == \'sequential\':\n        from datasets.sequence_folders import SequenceFolder\n    save_path = save_path_formatter(args, parser)\n    args.save_path = \'checkpoints\'/save_path\n    print(\'=> will save everything to {}\'.format(args.save_path))\n    args.save_path.makedirs_p()\n    torch.manual_seed(args.seed)\n    if args.evaluate:\n        args.epochs = 0\n\n    tb_writer = SummaryWriter(args.save_path)\n    # Data loading code\n    normalize = custom_transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                            std=[0.5, 0.5, 0.5])\n    train_transform = custom_transforms.Compose([\n        custom_transforms.RandomHorizontalFlip(),\n        custom_transforms.RandomScaleCrop(),\n        custom_transforms.ArrayToTensor(),\n        normalize\n    ])\n\n    valid_transform = custom_transforms.Compose([custom_transforms.ArrayToTensor(), normalize])\n\n    print(""=> fetching scenes in \'{}\'"".format(args.data))\n    train_set = SequenceFolder(\n        args.data,\n        transform=train_transform,\n        seed=args.seed,\n        train=True,\n        sequence_length=args.sequence_length\n    )\n\n    # if no Groundtruth is avalaible, Validation set is the same type as training set to measure photometric loss from warping\n    if args.with_gt:\n        from datasets.validation_folders import ValidationSet\n        val_set = ValidationSet(\n            args.data,\n            transform=valid_transform\n        )\n    else:\n        val_set = SequenceFolder(\n            args.data,\n            transform=valid_transform,\n            seed=args.seed,\n            train=False,\n            sequence_length=args.sequence_length,\n        )\n    print(\'{} samples found in {} train scenes\'.format(len(train_set), len(train_set.scenes)))\n    print(\'{} samples found in {} valid scenes\'.format(len(val_set), len(val_set.scenes)))\n    train_loader = torch.utils.data.DataLoader(\n        train_set, batch_size=args.batch_size, shuffle=True,\n        num_workers=args.workers, pin_memory=True)\n    val_loader = torch.utils.data.DataLoader(\n        val_set, batch_size=args.batch_size, shuffle=False,\n        num_workers=args.workers, pin_memory=True)\n\n    if args.epoch_size == 0:\n        args.epoch_size = len(train_loader)\n\n    # create model\n    print(""=> creating model"")\n\n    disp_net = models.DispNetS().to(device)\n    output_exp = args.mask_loss_weight > 0\n    if not output_exp:\n        print(""=> no mask loss, PoseExpnet will only output pose"")\n    pose_exp_net = models.PoseExpNet(nb_ref_imgs=args.sequence_length - 1, output_exp=args.mask_loss_weight > 0).to(device)\n\n    if args.pretrained_exp_pose:\n        print(""=> using pre-trained weights for explainabilty and pose net"")\n        weights = torch.load(args.pretrained_exp_pose)\n        pose_exp_net.load_state_dict(weights[\'state_dict\'], strict=False)\n    else:\n        pose_exp_net.init_weights()\n\n    if args.pretrained_disp:\n        print(""=> using pre-trained weights for Dispnet"")\n        weights = torch.load(args.pretrained_disp)\n        disp_net.load_state_dict(weights[\'state_dict\'])\n    else:\n        disp_net.init_weights()\n\n    cudnn.benchmark = True\n    disp_net = torch.nn.DataParallel(disp_net)\n    pose_exp_net = torch.nn.DataParallel(pose_exp_net)\n\n    print(\'=> setting adam solver\')\n\n    optim_params = [\n        {\'params\': disp_net.parameters(), \'lr\': args.lr},\n        {\'params\': pose_exp_net.parameters(), \'lr\': args.lr}\n    ]\n    optimizer = torch.optim.Adam(optim_params,\n                                 betas=(args.momentum, args.beta),\n                                 weight_decay=args.weight_decay)\n\n    with open(args.save_path/args.log_summary, \'w\') as csvfile:\n        writer = csv.writer(csvfile, delimiter=\'\\t\')\n        writer.writerow([\'train_loss\', \'validation_loss\'])\n\n    with open(args.save_path/args.log_full, \'w\') as csvfile:\n        writer = csv.writer(csvfile, delimiter=\'\\t\')\n        writer.writerow([\'train_loss\', \'photo_loss\', \'explainability_loss\', \'smooth_loss\'])\n\n    logger = TermLogger(n_epochs=args.epochs, train_size=min(len(train_loader), args.epoch_size), valid_size=len(val_loader))\n    logger.epoch_bar.start()\n\n    if args.pretrained_disp or args.evaluate:\n        logger.reset_valid_bar()\n        if args.with_gt:\n            errors, error_names = validate_with_gt(args, val_loader, disp_net, 0, logger, tb_writer)\n        else:\n            errors, error_names = validate_without_gt(args, val_loader, disp_net, pose_exp_net, 0, logger, tb_writer)\n        for error, name in zip(errors, error_names):\n            tb_writer.add_scalar(name, error, 0)\n        error_string = \', \'.join(\'{} : {:.3f}\'.format(name, error) for name, error in zip(error_names[2:9], errors[2:9]))\n        logger.valid_writer.write(\' * Avg {}\'.format(error_string))\n\n    for epoch in range(args.epochs):\n        logger.epoch_bar.update(epoch)\n\n        # train for one epoch\n        logger.reset_train_bar()\n        train_loss = train(args, train_loader, disp_net, pose_exp_net, optimizer, args.epoch_size, logger, tb_writer)\n        logger.train_writer.write(\' * Avg Loss : {:.3f}\'.format(train_loss))\n\n        # evaluate on validation set\n        logger.reset_valid_bar()\n        if args.with_gt:\n            errors, error_names = validate_with_gt(args, val_loader, disp_net, epoch, logger, tb_writer)\n        else:\n            errors, error_names = validate_without_gt(args, val_loader, disp_net, pose_exp_net, epoch, logger, tb_writer)\n        error_string = \', \'.join(\'{} : {:.3f}\'.format(name, error) for name, error in zip(error_names, errors))\n        logger.valid_writer.write(\' * Avg {}\'.format(error_string))\n\n        for error, name in zip(errors, error_names):\n            tb_writer.add_scalar(name, error, epoch)\n\n        # Up to you to chose the most relevant error to measure your model\'s performance, careful some measures are to maximize (such as a1,a2,a3)\n        decisive_error = errors[1]\n        if best_error < 0:\n            best_error = decisive_error\n\n        # remember lowest error and save checkpoint\n        is_best = decisive_error < best_error\n        best_error = min(best_error, decisive_error)\n        save_checkpoint(\n            args.save_path, {\n                \'epoch\': epoch + 1,\n                \'state_dict\': disp_net.module.state_dict()\n            }, {\n                \'epoch\': epoch + 1,\n                \'state_dict\': pose_exp_net.module.state_dict()\n            },\n            is_best)\n\n        with open(args.save_path/args.log_summary, \'a\') as csvfile:\n            writer = csv.writer(csvfile, delimiter=\'\\t\')\n            writer.writerow([train_loss, decisive_error])\n    logger.epoch_bar.finish()\n\n\ndef train(args, train_loader, disp_net, pose_exp_net, optimizer, epoch_size, logger, tb_writer):\n    global n_iter, device\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter(precision=4)\n    w1, w2, w3 = args.photo_loss_weight, args.mask_loss_weight, args.smooth_loss_weight\n\n    # switch to train mode\n    disp_net.train()\n    pose_exp_net.train()\n\n    end = time.time()\n    logger.train_bar.update(0)\n\n    for i, (tgt_img, ref_imgs, intrinsics, intrinsics_inv) in enumerate(train_loader):\n        log_losses = i > 0 and n_iter % args.print_freq == 0\n        log_output = args.training_output_freq > 0 and n_iter % args.training_output_freq == 0\n\n        # measure data loading time\n        data_time.update(time.time() - end)\n        tgt_img = tgt_img.to(device)\n        ref_imgs = [img.to(device) for img in ref_imgs]\n        intrinsics = intrinsics.to(device)\n\n        # compute output\n        disparities = disp_net(tgt_img)\n        depth = [1/disp for disp in disparities]\n        explainability_mask, pose = pose_exp_net(tgt_img, ref_imgs)\n\n        loss_1, warped, diff = photometric_reconstruction_loss(tgt_img, ref_imgs, intrinsics,\n                                                               depth, explainability_mask, pose,\n                                                               args.rotation_mode, args.padding_mode)\n        if w2 > 0:\n            loss_2 = explainability_loss(explainability_mask)\n        else:\n            loss_2 = 0\n        loss_3 = smooth_loss(depth)\n\n        loss = w1*loss_1 + w2*loss_2 + w3*loss_3\n\n        if log_losses:\n            tb_writer.add_scalar(\'photometric_error\', loss_1.item(), n_iter)\n            if w2 > 0:\n                tb_writer.add_scalar(\'explanability_loss\', loss_2.item(), n_iter)\n            tb_writer.add_scalar(\'disparity_smoothness_loss\', loss_3.item(), n_iter)\n            tb_writer.add_scalar(\'total_loss\', loss.item(), n_iter)\n\n        if log_output:\n            tb_writer.add_image(\'train Input\', tensor2array(tgt_img[0]), n_iter)\n            for k, scaled_maps in enumerate(zip(depth, disparities, warped, diff, explainability_mask)):\n                log_output_tensorboard(tb_writer, ""train"", 0, "" {}"".format(k), n_iter, *scaled_maps)\n\n        # record loss and EPE\n        losses.update(loss.item(), args.batch_size)\n\n        # compute gradient and do Adam step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        with open(args.save_path/args.log_full, \'a\') as csvfile:\n            writer = csv.writer(csvfile, delimiter=\'\\t\')\n            writer.writerow([loss.item(), loss_1.item(), loss_2.item() if w2 > 0 else 0, loss_3.item()])\n        logger.train_bar.update(i+1)\n        if i % args.print_freq == 0:\n            logger.train_writer.write(\'Train: Time {} Data {} Loss {}\'.format(batch_time, data_time, losses))\n        if i >= epoch_size - 1:\n            break\n\n        n_iter += 1\n\n    return losses.avg[0]\n\n\n@torch.no_grad()\ndef validate_without_gt(args, val_loader, disp_net, pose_exp_net, epoch, logger, tb_writer, sample_nb_to_log=3):\n    global device\n    batch_time = AverageMeter()\n    losses = AverageMeter(i=3, precision=4)\n    log_outputs = sample_nb_to_log > 0\n    w1, w2, w3 = args.photo_loss_weight, args.mask_loss_weight, args.smooth_loss_weight\n    poses = np.zeros(((len(val_loader)-1) * args.batch_size * (args.sequence_length-1),6))\n    disp_values = np.zeros(((len(val_loader)-1) * args.batch_size * 3))\n\n    # switch to evaluate mode\n    disp_net.eval()\n    pose_exp_net.eval()\n\n    end = time.time()\n    logger.valid_bar.update(0)\n    for i, (tgt_img, ref_imgs, intrinsics, intrinsics_inv) in enumerate(val_loader):\n        tgt_img = tgt_img.to(device)\n        ref_imgs = [img.to(device) for img in ref_imgs]\n        intrinsics = intrinsics.to(device)\n        intrinsics_inv = intrinsics_inv.to(device)\n\n        # compute output\n        disp = disp_net(tgt_img)\n        depth = 1/disp\n        explainability_mask, pose = pose_exp_net(tgt_img, ref_imgs)\n\n        loss_1, warped, diff = photometric_reconstruction_loss(tgt_img, ref_imgs,\n                                                               intrinsics, depth,\n                                                               explainability_mask, pose,\n                                                               args.rotation_mode, args.padding_mode)\n        loss_1 = loss_1.item()\n        if w2 > 0:\n            loss_2 = explainability_loss(explainability_mask).item()\n        else:\n            loss_2 = 0\n        loss_3 = smooth_loss(depth).item()\n\n        if log_outputs and i < sample_nb_to_log - 1:  # log first output of first batches\n            if epoch == 0:\n                for j,ref in enumerate(ref_imgs):\n                    tb_writer.add_image(\'val Input {}/{}\'.format(j, i), tensor2array(tgt_img[0]), 0)\n                    tb_writer.add_image(\'val Input {}/{}\'.format(j, i), tensor2array(ref[0]), 1)\n\n            log_output_tensorboard(tb_writer, \'val\', i, \'\', epoch, 1./disp, disp, warped[0], diff[0], explainability_mask)\n\n        if log_outputs and i < len(val_loader)-1:\n            step = args.batch_size*(args.sequence_length-1)\n            poses[i * step:(i+1) * step] = pose.cpu().view(-1,6).numpy()\n            step = args.batch_size * 3\n            disp_unraveled = disp.cpu().view(args.batch_size, -1)\n            disp_values[i * step:(i+1) * step] = torch.cat([disp_unraveled.min(-1)[0],\n                                                            disp_unraveled.median(-1)[0],\n                                                            disp_unraveled.max(-1)[0]]).numpy()\n\n        loss = w1*loss_1 + w2*loss_2 + w3*loss_3\n        losses.update([loss, loss_1, loss_2])\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        logger.valid_bar.update(i+1)\n        if i % args.print_freq == 0:\n            logger.valid_writer.write(\'valid: Time {} Loss {}\'.format(batch_time, losses))\n    if log_outputs:\n        prefix = \'valid poses\'\n        coeffs_names = [\'tx\', \'ty\', \'tz\']\n        if args.rotation_mode == \'euler\':\n            coeffs_names.extend([\'rx\', \'ry\', \'rz\'])\n        elif args.rotation_mode == \'quat\':\n            coeffs_names.extend([\'qx\', \'qy\', \'qz\'])\n        for i in range(poses.shape[1]):\n            tb_writer.add_histogram(\'{} {}\'.format(prefix, coeffs_names[i]), poses[:,i], epoch)\n        tb_writer.add_histogram(\'disp_values\', disp_values, epoch)\n    logger.valid_bar.update(len(val_loader))\n    return losses.avg, [\'Validation Total loss\', \'Validation Photo loss\', \'Validation Exp loss\']\n\n\n@torch.no_grad()\ndef validate_with_gt(args, val_loader, disp_net, epoch, logger, tb_writer, sample_nb_to_log=3):\n    global device\n    batch_time = AverageMeter()\n    error_names = [\'abs_diff\', \'abs_rel\', \'sq_rel\', \'a1\', \'a2\', \'a3\']\n    errors = AverageMeter(i=len(error_names))\n    log_outputs = sample_nb_to_log > 0\n\n    # switch to evaluate mode\n    disp_net.eval()\n\n    end = time.time()\n    logger.valid_bar.update(0)\n    for i, (tgt_img, depth) in enumerate(val_loader):\n        tgt_img = tgt_img.to(device)\n        depth = depth.to(device)\n\n        # compute output\n        output_disp = disp_net(tgt_img)\n        output_depth = 1/output_disp[:,0]\n\n        if log_outputs and i < sample_nb_to_log:\n            if epoch == 0:\n                tb_writer.add_image(\'val Input/{}\'.format(i), tensor2array(tgt_img[0]), 0)\n                depth_to_show = depth[0]\n                tb_writer.add_image(\'val target Depth Normalized/{}\'.format(i),\n                                    tensor2array(depth_to_show, max_value=None),\n                                    epoch)\n                depth_to_show[depth_to_show == 0] = 1000\n                disp_to_show = (1/depth_to_show).clamp(0,10)\n                tb_writer.add_image(\'val target Disparity Normalized/{}\'.format(i),\n                                    tensor2array(disp_to_show, max_value=None, colormap=\'magma\'),\n                                    epoch)\n\n            tb_writer.add_image(\'val Dispnet Output Normalized/{}\'.format(i),\n                                tensor2array(output_disp[0], max_value=None, colormap=\'magma\'),\n                                epoch)\n            tb_writer.add_image(\'val Depth Output Normalized/{}\'.format(i),\n                                tensor2array(output_depth[0], max_value=None),\n                                epoch)\n\n        errors.update(compute_errors(depth, output_depth))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        logger.valid_bar.update(i+1)\n        if i % args.print_freq == 0:\n            logger.valid_writer.write(\'valid: Time {} Abs Error {:.4f} ({:.4f})\'.format(batch_time, errors.val[0], errors.avg[0]))\n    logger.valid_bar.update(len(val_loader))\n    return errors.avg, error_names\n\n\nif __name__ == \'__main__\':\n    main()\n'"
train_flexible_shifts.py,15,"b'import time\nimport csv\n\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport custom_transforms\nimport models\nfrom utils import save_checkpoint,save_path_formatter\nfrom logger import TermLogger, AverageMeter\nfrom itertools import chain\nfrom tensorboardX import SummaryWriter\nfrom datasets.shifted_sequence_folders import ShiftedSequenceFolder\nfrom datasets.sequence_folders import SequenceFolder\nfrom train import train, validate_with_gt, validate_without_gt, parser\n\nparser.add_argument(\'-d\', \'--target-displacement\', type=float, help=\'displacement to aim at when adjustting shifts, regarding posenet output\',\n                    metavar=\'D\', default=0.05)\n\nbest_error = -1\nn_iter = 0\ndevice = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"")\n\n\ndef main():\n    global args, best_error, n_iter, device\n    args = parser.parse_args()\n    save_path = save_path_formatter(args, parser)\n    args.save_path = \'checkpoints_shifted\'/save_path\n    print(\'=> will save everything to {}\'.format(args.save_path))\n    args.save_path.makedirs_p()\n    torch.manual_seed(args.seed)\n\n    tb_writer = SummaryWriter(args.save_path)\n\n    # Data loading code\n    normalize = custom_transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                            std=[0.5, 0.5, 0.5])\n    train_transform = custom_transforms.Compose([\n        custom_transforms.RandomHorizontalFlip(),\n        custom_transforms.RandomScaleCrop(),\n        custom_transforms.ArrayToTensor(),\n        normalize\n    ])\n\n    valid_transform = custom_transforms.Compose([custom_transforms.ArrayToTensor(), normalize])\n\n    print(""=> fetching scenes in \'{}\'"".format(args.data))\n    train_set = ShiftedSequenceFolder(\n        args.data,\n        transform=train_transform,\n        seed=args.seed,\n        train=True,\n        sequence_length=args.sequence_length,\n        target_displacement=args.target_displacement\n    )\n\n    # if no Groundtruth is avalaible, Validation set is the same type as training set to measure photometric loss from warping\n    if args.with_gt:\n        from datasets.validation_folders import ValidationSet\n        val_set = ValidationSet(\n            args.data,\n            transform=valid_transform\n        )\n    else:\n        val_set = SequenceFolder(\n            args.data,\n            transform=valid_transform,\n            seed=args.seed,\n            train=False,\n            sequence_length=args.sequence_length,\n        )\n    print(\'{} samples found in {} train scenes\'.format(len(train_set), len(train_set.scenes)))\n    print(\'{} samples found in {} valid scenes\'.format(len(val_set), len(val_set.scenes)))\n    train_loader = torch.utils.data.DataLoader(\n        train_set, batch_size=args.batch_size, shuffle=True,\n        num_workers=args.workers, pin_memory=True)\n    adjust_loader = torch.utils.data.DataLoader(\n        train_set, batch_size=args.batch_size, shuffle=False,\n        num_workers=0, pin_memory=True)  # workers is set to 0 to avoid multiple instances to be modified at the same time\n    val_loader = torch.utils.data.DataLoader(\n        val_set, batch_size=args.batch_size, shuffle=False,\n        num_workers=args.workers, pin_memory=True)\n\n    if args.epoch_size == 0:\n        args.epoch_size = len(train_loader)\n\n    train.args = args\n    # create model\n    print(""=> creating model"")\n\n    disp_net = models.DispNetS().cuda()\n    output_exp = args.mask_loss_weight > 0\n    if not output_exp:\n        print(""=> no mask loss, PoseExpnet will only output pose"")\n    pose_exp_net = models.PoseExpNet(nb_ref_imgs=args.sequence_length - 1, output_exp=args.mask_loss_weight > 0).to(device)\n\n    if args.pretrained_exp_pose:\n        print(""=> using pre-trained weights for explainabilty and pose net"")\n        weights = torch.load(args.pretrained_exp_pose)\n        pose_exp_net.load_state_dict(weights[\'state_dict\'], strict=False)\n    else:\n        pose_exp_net.init_weights()\n\n    if args.pretrained_disp:\n        print(""=> using pre-trained weights for Dispnet"")\n        weights = torch.load(args.pretrained_disp)\n        disp_net.load_state_dict(weights[\'state_dict\'])\n    else:\n        disp_net.init_weights()\n\n    cudnn.benchmark = True\n    disp_net = torch.nn.DataParallel(disp_net)\n    pose_exp_net = torch.nn.DataParallel(pose_exp_net)\n\n    print(\'=> setting adam solver\')\n\n    parameters = chain(disp_net.parameters(), pose_exp_net.parameters())\n    optimizer = torch.optim.Adam(parameters, args.lr,\n                                 betas=(args.momentum, args.beta),\n                                 weight_decay=args.weight_decay)\n\n    with open(args.save_path/args.log_summary, \'w\') as csvfile:\n        writer = csv.writer(csvfile, delimiter=\'\\t\')\n        writer.writerow([\'train_loss\', \'validation_loss\'])\n\n    with open(args.save_path/args.log_full, \'w\') as csvfile:\n        writer = csv.writer(csvfile, delimiter=\'\\t\')\n        writer.writerow([\'train_loss\', \'photo_loss\', \'explainability_loss\', \'smooth_loss\'])\n\n    logger = TermLogger(n_epochs=args.epochs, train_size=min(len(train_loader), args.epoch_size), valid_size=len(val_loader))\n    logger.epoch_bar.start()\n\n    for epoch in range(args.epochs):\n        logger.epoch_bar.update(epoch)\n\n        # train for one epoch\n        logger.reset_train_bar()\n        train_loss = train(args, train_loader, disp_net, pose_exp_net, optimizer, args.epoch_size, logger, tb_writer)\n        logger.train_writer.write(\' * Avg Loss : {:.3f}\'.format(train_loss))\n\n        if (epoch + 1) % 5 == 0:\n            train_set.adjust = True\n            logger.reset_train_bar(len(adjust_loader))\n            average_shifts = adjust_shifts(args, train_set, adjust_loader, pose_exp_net, epoch, logger, tb_writer)\n            shifts_string = \' \'.join([\'{:.3f}\'.format(s) for s in average_shifts])\n            logger.train_writer.write(\' * adjusted shifts, average shifts are now : {}\'.format(shifts_string))\n            for i, shift in enumerate(average_shifts):\n                tb_writer.add_scalar(\'shifts{}\'.format(i), shift, epoch)\n            train_set.adjust = False\n\n        # evaluate on validation set\n        logger.reset_valid_bar()\n        if args.with_gt:\n            errors, error_names = validate_with_gt(args, val_loader, disp_net, epoch, logger, tb_writer)\n        else:\n            errors, error_names = validate_without_gt(args, val_loader, disp_net, pose_exp_net, epoch, logger, tb_writer)\n        error_string = \', \'.join(\'{} : {:.3f}\'.format(name, error) for name, error in zip(error_names, errors))\n        logger.valid_writer.write(\' * Avg {}\'.format(error_string))\n\n        for error, name in zip(errors, error_names):\n            tb_writer.add_scalar(name, error, epoch)\n\n        # Up to you to chose the most relevant error to measure your model\'s performance, careful some measures are to maximize (such as a1,a2,a3)\n        decisive_error = errors[0]\n        if best_error < 0:\n            best_error = decisive_error\n\n        # remember lowest error and save checkpoint\n        is_best = decisive_error < best_error\n        best_error = min(best_error, decisive_error)\n        save_checkpoint(\n            args.save_path, {\n                \'epoch\': epoch + 1,\n                \'state_dict\': disp_net.module.state_dict()\n            }, {\n                \'epoch\': epoch + 1,\n                \'state_dict\': pose_exp_net.module.state_dict()\n            },\n            is_best)\n\n        with open(args.save_path/args.log_summary, \'a\') as csvfile:\n            writer = csv.writer(csvfile, delimiter=\'\\t\')\n            writer.writerow([train_loss, decisive_error])\n    logger.epoch_bar.finish()\n\n\n@torch.no_grad()\ndef adjust_shifts(args, train_set, adjust_loader, pose_exp_net, epoch, logger, tb_writer):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    new_shifts = AverageMeter(args.sequence_length-1)\n    pose_exp_net.train()\n    poses = np.zeros(((len(adjust_loader)-1) * args.batch_size * (args.sequence_length-1),6))\n\n    mid_index = (args.sequence_length - 1)//2\n\n    target_values = np.abs(np.arange(-mid_index, mid_index + 1)) * (args.target_displacement)\n    target_values = np.concatenate([target_values[:mid_index], target_values[mid_index + 1:]])\n\n    end = time.time()\n\n    for i, (indices, tgt_img, ref_imgs, intrinsics, intrinsics_inv) in enumerate(adjust_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n        tgt_img = tgt_img.to(device)\n        ref_imgs = [img.to(device) for img in ref_imgs]\n\n        # compute output\n        explainability_mask, pose_batch = pose_exp_net(tgt_img, ref_imgs)\n\n        if i < len(adjust_loader)-1:\n            step = args.batch_size*(args.sequence_length-1)\n            poses[i * step:(i+1) * step] = pose_batch.cpu().reshape(-1,6).numpy()\n\n        for index, pose in zip(indices, pose_batch):\n            displacements = pose[:,:3].norm(p=2, dim=1).cpu().numpy()\n            ratio = target_values / displacements\n\n            train_set.reset_shifts(index, ratio[:mid_index], ratio[mid_index:])\n            new_shifts.update(train_set.get_shifts(index))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        logger.train_bar.update(i)\n        if i % args.print_freq == 0:\n            logger.train_writer.write(\'Adjustement:\'\n                                      \'Time {} Data {} shifts {}\'.format(batch_time, data_time, new_shifts))\n\n    prefix = \'train poses\'\n    coeffs_names = [\'tx\', \'ty\', \'tz\']\n    if args.rotation_mode == \'euler\':\n        coeffs_names.extend([\'rx\', \'ry\', \'rz\'])\n    elif args.rotation_mode == \'quat\':\n        coeffs_names.extend([\'qx\', \'qy\', \'qz\'])\n    for i in range(poses.shape[1]):\n        tb_writer.add_histogram(\'{} {}\'.format(prefix, coeffs_names[i]), poses[:,i], epoch)\n\n    return new_shifts.avg\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils.py,1,"b'from __future__ import division\nimport shutil\nimport numpy as np\nimport torch\nfrom path import Path\nimport datetime\nfrom collections import OrderedDict\nfrom matplotlib import cm\nfrom matplotlib.colors import ListedColormap, LinearSegmentedColormap\n\n\ndef save_path_formatter(args, parser):\n    def is_default(key, value):\n        return value == parser.get_default(key)\n    args_dict = vars(args)\n    data_folder_name = str(Path(args_dict[\'data\']).normpath().name)\n    folder_string = [data_folder_name]\n    if not is_default(\'epochs\', args_dict[\'epochs\']):\n        folder_string.append(\'{}epochs\'.format(args_dict[\'epochs\']))\n    keys_with_prefix = OrderedDict()\n    keys_with_prefix[\'epoch_size\'] = \'epoch_size\'\n    keys_with_prefix[\'sequence_length\'] = \'seq\'\n    keys_with_prefix[\'rotation_mode\'] = \'rot_\'\n    keys_with_prefix[\'padding_mode\'] = \'padding_\'\n    keys_with_prefix[\'batch_size\'] = \'b\'\n    keys_with_prefix[\'lr\'] = \'lr\'\n    keys_with_prefix[\'photo_loss_weight\'] = \'p\'\n    keys_with_prefix[\'mask_loss_weight\'] = \'m\'\n    keys_with_prefix[\'smooth_loss_weight\'] = \'s\'\n\n    for key, prefix in keys_with_prefix.items():\n        value = args_dict[key]\n        if not is_default(key, value):\n            folder_string.append(\'{}{}\'.format(prefix, value))\n    save_path = Path(\',\'.join(folder_string))\n    timestamp = datetime.datetime.now().strftime(""%m-%d-%H:%M"")\n    return save_path/timestamp\n\n\ndef high_res_colormap(low_res_cmap, resolution=1000, max_value=1):\n    # Construct the list colormap, with interpolated values for higer resolution\n    # For a linear segmented colormap, you can just specify the number of point in\n    # cm.get_cmap(name, lutsize) with the parameter lutsize\n    x = np.linspace(0,1,low_res_cmap.N)\n    low_res = low_res_cmap(x)\n    new_x = np.linspace(0,max_value,resolution)\n    high_res = np.stack([np.interp(new_x, x, low_res[:,i]) for i in range(low_res.shape[1])], axis=1)\n    return ListedColormap(high_res)\n\n\ndef opencv_rainbow(resolution=1000):\n    # Construct the opencv equivalent of Rainbow\n    opencv_rainbow_data = (\n        (0.000, (1.00, 0.00, 0.00)),\n        (0.400, (1.00, 1.00, 0.00)),\n        (0.600, (0.00, 1.00, 0.00)),\n        (0.800, (0.00, 0.00, 1.00)),\n        (1.000, (0.60, 0.00, 1.00))\n    )\n\n    return LinearSegmentedColormap.from_list(\'opencv_rainbow\', opencv_rainbow_data, resolution)\n\n\nCOLORMAPS = {\'rainbow\': opencv_rainbow(),\n             \'magma\': high_res_colormap(cm.get_cmap(\'magma\')),\n             \'bone\': cm.get_cmap(\'bone\', 10000)}\n\n\ndef log_output_tensorboard(writer, prefix, index, suffix, n_iter, depth, disp, warped, diff, mask):\n    disp_to_show = tensor2array(disp[0], max_value=None, colormap=\'magma\')\n    depth_to_show = tensor2array(depth[0], max_value=None)\n    writer.add_image(\'{} Dispnet Output Normalized{}/{}\'.format(prefix, suffix, index), disp_to_show, n_iter)\n    writer.add_image(\'{} Depth Output Normalized{}/{}\'.format(prefix, suffix, index), depth_to_show, n_iter)\n    # log warped images along with explainability mask\n    for j, (warped_j, diff_j) in enumerate(zip(warped, diff)):\n        whole_suffix = \'{} {}/{}\'.format(suffix, j, index)\n        warped_to_show = tensor2array(warped_j)\n        diff_to_show = tensor2array(0.5*diff_j)\n        writer.add_image(\'{} Warped Outputs {}\'.format(prefix, whole_suffix), warped_to_show, n_iter)\n        writer.add_image(\'{} Diff Outputs {}\'.format(prefix, whole_suffix), diff_to_show, n_iter)\n        if mask is not None:\n            mask_to_show = tensor2array(mask[0,j], max_value=1, colormap=\'bone\')\n            writer.add_image(\'{} Exp mask Outputs {}\'.format(prefix, whole_suffix), mask_to_show, n_iter)\n\n\ndef tensor2array(tensor, max_value=None, colormap=\'rainbow\'):\n    tensor = tensor.detach().cpu()\n    if max_value is None:\n        max_value = tensor.max().item()\n    if tensor.ndimension() == 2 or tensor.size(0) == 1:\n        norm_array = tensor.squeeze().numpy()/max_value\n        array = COLORMAPS[colormap](norm_array).astype(np.float32)\n        array = array.transpose(2, 0, 1)[:3]\n\n    elif tensor.ndimension() == 3:\n        assert(tensor.size(0) == 3)\n        array = 0.5 + tensor.numpy()*0.5\n    return array\n\n\ndef save_checkpoint(save_path, dispnet_state, exp_pose_state, is_best, filename=\'checkpoint.pth.tar\'):\n    file_prefixes = [\'dispnet\', \'exp_pose\']\n    states = [dispnet_state, exp_pose_state]\n    for (prefix, state) in zip(file_prefixes, states):\n        torch.save(state, save_path/\'{}_{}\'.format(prefix,filename))\n\n    if is_best:\n        for prefix in file_prefixes:\n            shutil.copyfile(save_path/\'{}_{}\'.format(prefix,filename), save_path/\'{}_model_best.pth.tar\'.format(prefix))\n'"
data/cityscapes_loader.py,0,"b'from __future__ import division\nimport json\nimport numpy as np\nimport scipy.misc\nfrom path import Path\nfrom tqdm import tqdm\nfrom imageio import imread\nfrom skimage.transform import resize as imresize\n\nclass cityscapes_loader(object):\n    def __init__(self,\n                 dataset_dir,\n                 split=\'train\',\n                 crop_bottom=True,  # Get rid of the car logo\n                 img_height=171,\n                 img_width=416):\n        self.dataset_dir = Path(dataset_dir)\n        self.split = split\n        # Crop out the bottom 25% of the image to remove the car logo\n        self.crop_bottom = crop_bottom\n        self.img_height = img_height\n        self.img_width = img_width\n        self.min_speed = 2\n        self.scenes = (self.dataset_dir/\'leftImg8bit_sequence\'/split).dirs()\n        print(\'Total scenes collected: {}\'.format(len(self.scenes)))\n\n    def collect_scenes(self, city):\n        img_files = sorted(city.files(\'*.png\'))\n        scenes = {}\n        connex_scenes = {}\n        connex_scene_data_list = []\n        for f in img_files:\n            frame_id, scene_id = f.basename().split(\'_\')[1:3]\n            if scene_id not in scenes.keys():\n                scenes[scene_id] = []\n            scenes[scene_id].append(frame_id)\n\n        # divide scenes into connexe sequences\n        for scene_id in scenes.keys():\n            previous = None\n            connex_scenes[scene_id] = []\n            for id in scenes[scene_id]:\n                if previous is None or int(id) - int(previous) > 1:\n                    current_list = []\n                    connex_scenes[scene_id].append(current_list)\n                current_list.append(id)\n                previous = id\n\n        # create scene data dicts, and subsample scene every two frames\n        for scene_id in connex_scenes.keys():\n            intrinsics = self.load_intrinsics(city, scene_id)\n            for subscene in connex_scenes[scene_id]:\n                frame_speeds = [self.load_speed(city, scene_id, frame_id) for frame_id in subscene]\n                connex_scene_data_list.append({\'city\':city,\n                                               \'scene_id\': scene_id,\n                                               \'rel_path\': city.basename()+\'_\'+scene_id+\'_\'+subscene[0]+\'_0\',\n                                               \'intrinsics\': intrinsics,\n                                               \'frame_ids\':subscene[0::2],\n                                               \'speeds\':frame_speeds[0::2]})\n                connex_scene_data_list.append({\'city\':city,\n                                               \'scene_id\': scene_id,\n                                               \'rel_path\': city.basename()+\'_\'+scene_id+\'_\'+subscene[0]+\'_1\',\n                                               \'intrinsics\': intrinsics,\n                                               \'frame_ids\': subscene[1::2],\n                                               \'speeds\': frame_speeds[1::2]})\n        return connex_scene_data_list\n\n    def load_intrinsics(self, city, scene_id):\n        city_name = city.basename()\n        camera_folder = self.dataset_dir/\'camera\'/self.split/city_name\n        camera_file = camera_folder.files(\'{}_*_{}_camera.json\'.format(city_name, scene_id))[0]\n        frame_id = camera_file.split(\'_\')[1]\n        frame_path = city/\'{}_{}_{}_leftImg8bit.png\'.format(city_name, frame_id, scene_id)\n\n        with open(camera_file, \'r\') as f:\n            camera = json.load(f)\n        fx = camera[\'intrinsic\'][\'fx\']\n        fy = camera[\'intrinsic\'][\'fy\']\n        u0 = camera[\'intrinsic\'][\'u0\']\n        v0 = camera[\'intrinsic\'][\'v0\']\n        intrinsics = np.array([[fx, 0, u0],\n                               [0, fy, v0],\n                               [0,  0,  1]])\n\n        img = imread(frame_path)\n        h,w,_ = img.shape\n        zoom_y = self.img_height/h\n        zoom_x = self.img_width/w\n\n        intrinsics[0] *= zoom_x\n        intrinsics[1] *= zoom_y\n        return intrinsics\n\n    def load_speed(self, city, scene_id, frame_id):\n        city_name = city.basename()\n        vehicle_folder = self.dataset_dir/\'vehicle_sequence\'/self.split/city_name\n        vehicle_file = vehicle_folder/\'{}_{}_{}_vehicle.json\'.format(city_name, frame_id, scene_id)\n        with open(vehicle_file, \'r\') as f:\n            vehicle = json.load(f)\n        return vehicle[\'speed\']\n\n    def get_scene_imgs(self, scene_data):\n        cum_speed = np.zeros(3)\n        #print(scene_data[\'city\'].basename(), scene_data[\'scene_id\'], scene_data[\'frame_ids\'])\n        for i,frame_id in enumerate(scene_data[\'frame_ids\']):\n            cum_speed += scene_data[\'speeds\'][i]\n            speed_mag = np.linalg.norm(cum_speed)\n            if speed_mag > self.min_speed:\n                yield {""img"":self.load_image(scene_data[\'city\'], scene_data[\'scene_id\'], frame_id),\n                       ""id"":frame_id}\n                cum_speed *= 0\n\n    def load_image(self, city, scene_id, frame_id):\n        img_file = city/\'{}_{}_{}_leftImg8bit.png\'.format(city.basename(),\n                                                          frame_id,\n                                                          scene_id)\n        if not img_file.isfile():\n            return None\n        img = imread(img_file)\n        img = imresize(img, (self.img_height, self.img_width))[:int(self.img_height*0.75)]\n\n        # workaround for skimage (float [0 .. 1]) and imageio (uint8 [0 .. 255]) interoperability\n        img = (img * 255).astype(np.uint8)\n\n        return img\n'"
data/kitti_raw_loader.py,0,"b'from __future__ import division\nimport numpy as np\nfrom path import Path\nimport scipy.misc\nfrom collections import Counter\nfrom imageio import imread, imwrite\nfrom skimage.transform import resize as imresize\n\ndef rotx(t):\n    """"""Rotation about the x-axis.""""""\n    c = np.cos(t)\n    s = np.sin(t)\n    return np.array([[1,  0,  0],\n                     [0,  c, -s],\n                     [0,  s,  c]])\n\n\ndef roty(t):\n    """"""Rotation about the y-axis.""""""\n    c = np.cos(t)\n    s = np.sin(t)\n    return np.array([[c,  0,  s],\n                     [0,  1,  0],\n                     [-s, 0,  c]])\n\n\ndef rotz(t):\n    """"""Rotation about the z-axis.""""""\n    c = np.cos(t)\n    s = np.sin(t)\n    return np.array([[c, -s,  0],\n                     [s,  c,  0],\n                     [0,  0,  1]])\n\n\ndef pose_from_oxts_packet(metadata, scale):\n\n    lat, lon, alt, roll, pitch, yaw = metadata\n    """"""Helper method to compute a SE(3) pose matrix from an OXTS packet.\n    Taken from https://github.com/utiasSTARS/pykitti\n    """"""\n\n    er = 6378137.  # earth radius (approx.) in meters\n    # Use a Mercator projection to get the translation vector\n    ty = lat * np.pi * er / 180.\n\n    tx = scale * lon * np.pi * er / 180.\n    # ty = scale * er * \\\n    #     np.log(np.tan((90. + lat) * np.pi / 360.))\n    tz = alt\n    t = np.array([tx, ty, tz]).reshape(-1,1)\n\n    # Use the Euler angles to get the rotation matrix\n    Rx = rotx(roll)\n    Ry = roty(pitch)\n    Rz = rotz(yaw)\n    R = Rz.dot(Ry.dot(Rx))\n    return transform_from_rot_trans(R, t)\n\n\ndef read_calib_file(path):\n    # taken from https://github.com/hunse/kitti\n    float_chars = set(""0123456789.e+- "")\n    data = {}\n    with open(path, \'r\') as f:\n        for line in f.readlines():\n            key, value = line.split(\':\', 1)\n            value = value.strip()\n            data[key] = value\n            if float_chars.issuperset(value):\n                # try to cast to float array\n                try:\n                    data[key] = np.array(list(map(float, value.split(\' \'))))\n                except ValueError:\n                    # casting error: data[key] already eq. value, so pass\n                    pass\n\n    return data\n\n\ndef transform_from_rot_trans(R, t):\n    """"""Transforation matrix from rotation matrix and translation vector.""""""\n    R = R.reshape(3, 3)\n    t = t.reshape(3, 1)\n    return np.vstack((np.hstack([R, t]), [0, 0, 0, 1]))\n\n\nclass KittiRawLoader(object):\n    def __init__(self,\n                 dataset_dir,\n                 static_frames_file=None,\n                 img_height=128,\n                 img_width=416,\n                 min_speed=2,\n                 get_depth=False,\n                 get_pose=False,\n                 depth_size_ratio=1):\n        dir_path = Path(__file__).realpath().dirname()\n        test_scene_file = dir_path/\'test_scenes.txt\'\n\n        self.from_speed = static_frames_file is None\n        if static_frames_file is not None:\n            static_frames_file = Path(static_frames_file)\n            self.collect_static_frames(static_frames_file)\n\n        with open(test_scene_file, \'r\') as f:\n            test_scenes = f.readlines()\n        self.test_scenes = [t[:-1] for t in test_scenes]\n        self.dataset_dir = Path(dataset_dir)\n        self.img_height = img_height\n        self.img_width = img_width\n        self.cam_ids = [\'02\', \'03\']\n        self.date_list = [\'2011_09_26\', \'2011_09_28\', \'2011_09_29\', \'2011_09_30\', \'2011_10_03\']\n        self.min_speed = min_speed\n        self.get_depth = get_depth\n        self.get_pose = get_pose\n        self.depth_size_ratio = depth_size_ratio\n        self.collect_train_folders()\n\n    def collect_static_frames(self, static_frames_file):\n        with open(static_frames_file, \'r\') as f:\n            frames = f.readlines()\n        self.static_frames = {}\n        for fr in frames:\n            if fr == \'\\n\':\n                continue\n            date, drive, frame_id = fr.split(\' \')\n            curr_fid = \'%.10d\' % (np.int(frame_id[:-1]))\n            if drive not in self.static_frames.keys():\n                self.static_frames[drive] = []\n            self.static_frames[drive].append(curr_fid)\n\n    def collect_train_folders(self):\n        self.scenes = []\n        for date in self.date_list:\n            drive_set = (self.dataset_dir/date).dirs()\n            for dr in drive_set:\n                if dr.name[:-5] not in self.test_scenes:\n                    self.scenes.append(dr)\n\n    def collect_scenes(self, drive):\n        train_scenes = []\n        for c in self.cam_ids:\n            oxts = sorted((drive/\'oxts\'/\'data\').files(\'*.txt\'))\n            scene_data = {\'cid\': c, \'dir\': drive, \'speed\': [], \'frame_id\': [], \'pose\':[], \'rel_path\': drive.name + \'_\' + c}\n            scale = None\n            origin = None\n            imu2velo = read_calib_file(drive.parent/\'calib_imu_to_velo.txt\')\n            velo2cam = read_calib_file(drive.parent/\'calib_velo_to_cam.txt\')\n            cam2cam = read_calib_file(drive.parent/\'calib_cam_to_cam.txt\')\n\n            velo2cam_mat = transform_from_rot_trans(velo2cam[\'R\'], velo2cam[\'T\'])\n            imu2velo_mat = transform_from_rot_trans(imu2velo[\'R\'], imu2velo[\'T\'])\n            cam_2rect_mat = transform_from_rot_trans(cam2cam[\'R_rect_00\'], np.zeros(3))\n\n            imu2cam = cam_2rect_mat @ velo2cam_mat @ imu2velo_mat\n\n            for n, f in enumerate(oxts):\n                metadata = np.genfromtxt(f)\n                speed = metadata[8:11]\n                scene_data[\'speed\'].append(speed)\n                scene_data[\'frame_id\'].append(\'{:010d}\'.format(n))\n                lat = metadata[0]\n\n                if scale is None:\n                    scale = np.cos(lat * np.pi / 180.)\n\n                pose_matrix = pose_from_oxts_packet(metadata[:6], scale)\n                if origin is None:\n                    origin = pose_matrix\n\n                odo_pose = imu2cam @ np.linalg.inv(origin) @ pose_matrix @ np.linalg.inv(imu2cam)\n                scene_data[\'pose\'].append(odo_pose[:3])\n\n            sample = self.load_image(scene_data, 0)\n            if sample is None:\n                return []\n            scene_data[\'P_rect\'] = self.get_P_rect(scene_data, sample[1], sample[2])\n            scene_data[\'intrinsics\'] = scene_data[\'P_rect\'][:,:3]\n\n            train_scenes.append(scene_data)\n        return train_scenes\n\n    def get_scene_imgs(self, scene_data):\n        def construct_sample(scene_data, i, frame_id):\n            sample = {""img"":self.load_image(scene_data, i)[0], ""id"":frame_id}\n\n            if self.get_depth:\n                sample[\'depth\'] = self.generate_depth_map(scene_data, i)\n            if self.get_pose:\n                sample[\'pose\'] = scene_data[\'pose\'][i]\n            return sample\n\n        if self.from_speed:\n            cum_speed = np.zeros(3)\n            for i, speed in enumerate(scene_data[\'speed\']):\n                cum_speed += speed\n                speed_mag = np.linalg.norm(cum_speed)\n                if speed_mag > self.min_speed:\n                    frame_id = scene_data[\'frame_id\'][i]\n                    yield construct_sample(scene_data, i, frame_id)\n                    cum_speed *= 0\n        else:  # from static frame file\n            drive = str(scene_data[\'dir\'].name)\n            for (i,frame_id) in enumerate(scene_data[\'frame_id\']):\n                if (drive not in self.static_frames.keys()) or (frame_id not in self.static_frames[drive]):\n                    yield construct_sample(scene_data, i, frame_id)\n\n    def get_P_rect(self, scene_data, zoom_x, zoom_y):\n        calib_file = scene_data[\'dir\'].parent/\'calib_cam_to_cam.txt\'\n\n        filedata = self.read_raw_calib_file(calib_file)\n        P_rect = np.reshape(filedata[\'P_rect_\' + scene_data[\'cid\']], (3, 4))\n        P_rect[0] *= zoom_x\n        P_rect[1] *= zoom_y\n        return P_rect\n\n    def load_image(self, scene_data, tgt_idx):\n        img_file = scene_data[\'dir\']/\'image_{}\'.format(scene_data[\'cid\'])/\'data\'/scene_data[\'frame_id\'][tgt_idx]+\'.png\'\n        if not img_file.isfile():\n            return None\n        img = imread(img_file)\n        zoom_y = self.img_height/img.shape[0]\n        zoom_x = self.img_width/img.shape[1]\n        img = imresize(img, (self.img_height, self.img_width))\n\n        # workaround for skimage (float [0 .. 1]) and imageio (uint8 [0 .. 255]) interoperability  \n        img = (img * 255).astype(np.uint8)\n\n        return img, zoom_x, zoom_y\n\n    def read_raw_calib_file(self, filepath):\n        # From https://github.com/utiasSTARS/pykitti/blob/master/pykitti/utils.py\n        """"""Read in a calibration file and parse into a dictionary.""""""\n        data = {}\n\n        with open(filepath, \'r\') as f:\n            for line in f.readlines():\n                key, value = line.split(\':\', 1)\n                # The only non-float values in these files are dates, which\n                # we don\'t care about anyway\n                try:\n                        data[key] = np.array([float(x) for x in value.split()])\n                except ValueError:\n                        pass\n        return data\n\n    def generate_depth_map(self, scene_data, tgt_idx):\n        # compute projection matrix velodyne->image plane\n\n        def sub2ind(matrixSize, rowSub, colSub):\n            m, n = matrixSize\n            return rowSub * (n-1) + colSub - 1\n\n        R_cam2rect = np.eye(4)\n\n        calib_dir = scene_data[\'dir\'].parent\n        cam2cam = self.read_raw_calib_file(calib_dir/\'calib_cam_to_cam.txt\')\n        velo2cam = self.read_raw_calib_file(calib_dir/\'calib_velo_to_cam.txt\')\n        velo2cam = np.hstack((velo2cam[\'R\'].reshape(3,3), velo2cam[\'T\'][..., np.newaxis]))\n        velo2cam = np.vstack((velo2cam, np.array([0, 0, 0, 1.0])))\n        P_rect = np.copy(scene_data[\'P_rect\'])\n        P_rect[0] /= self.depth_size_ratio\n        P_rect[1] /= self.depth_size_ratio\n\n        R_cam2rect[:3,:3] = cam2cam[\'R_rect_00\'].reshape(3,3)\n\n        P_velo2im = np.dot(np.dot(P_rect, R_cam2rect), velo2cam)\n\n        velo_file_name = scene_data[\'dir\']/\'velodyne_points\'/\'data\'/\'{}.bin\'.format(scene_data[\'frame_id\'][tgt_idx])\n\n        # load velodyne points and remove all behind image plane (approximation)\n        # each row of the velodyne data is forward, left, up, reflectance\n        velo = np.fromfile(velo_file_name, dtype=np.float32).reshape(-1, 4)\n        velo[:,3] = 1\n        velo = velo[velo[:, 0] >= 0, :]\n\n        # project the points to the camera\n        velo_pts_im = np.dot(P_velo2im, velo.T).T\n        velo_pts_im[:, :2] = velo_pts_im[:,:2] / velo_pts_im[:,-1:]\n\n        # check if in bounds\n        # use minus 1 to get the exact same value as KITTI matlab code\n        velo_pts_im[:, 0] = np.round(velo_pts_im[:,0]) - 1\n        velo_pts_im[:, 1] = np.round(velo_pts_im[:,1]) - 1\n\n        val_inds = (velo_pts_im[:, 0] >= 0) & (velo_pts_im[:, 1] >= 0)\n        val_inds = val_inds & (velo_pts_im[:,0] < self.img_width/self.depth_size_ratio)\n        val_inds = val_inds & (velo_pts_im[:,1] < self.img_height/self.depth_size_ratio)\n        velo_pts_im = velo_pts_im[val_inds, :]\n\n        # project to image\n        depth = np.zeros((self.img_height // self.depth_size_ratio, self.img_width // self.depth_size_ratio)).astype(np.float32)\n        depth[velo_pts_im[:, 1].astype(np.int), velo_pts_im[:, 0].astype(np.int)] = velo_pts_im[:, 2]\n\n        # find the duplicate points and choose the closest depth\n        inds = sub2ind(depth.shape, velo_pts_im[:, 1], velo_pts_im[:, 0])\n        dupe_inds = [item for item, count in Counter(inds).items() if count > 1]\n        for dd in dupe_inds:\n            pts = np.where(inds == dd)[0]\n            x_loc = int(velo_pts_im[pts[0], 0])\n            y_loc = int(velo_pts_im[pts[0], 1])\n            depth[y_loc, x_loc] = velo_pts_im[pts, 2].min()\n        depth[depth < 0] = 0\n        return depth\n'"
data/prepare_train_data.py,0,"b'import argparse\nimport scipy.misc\nimport numpy as np\nfrom pebble import ProcessPool\nimport sys\nfrom tqdm import tqdm\nfrom path import Path\nfrom imageio import imwrite\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""dataset_dir"", metavar=\'DIR\',\n                    help=\'path to original dataset\')\nparser.add_argument(""--dataset-format"", type=str, default=\'kitti\', choices=[""kitti"", ""cityscapes""])\nparser.add_argument(""--static-frames"", default=None,\n                    help=""list of imgs to discard for being static, if not set will discard them based on speed \\\n                    (careful, on KITTI some frames have incorrect speed)"")\nparser.add_argument(""--with-depth"", action=\'store_true\',\n                    help=""If available (e.g. with KITTI), will store depth ground truth along with images, for validation"")\nparser.add_argument(""--with-pose"", action=\'store_true\',\n                    help=""If available (e.g. with KITTI), will store pose ground truth along with images, for validation"")\nparser.add_argument(""--no-train-gt"", action=\'store_true\',\n                    help=""If selected, will delete ground truth depth to save space"")\nparser.add_argument(""--dump-root"", type=str, default=\'dump\', help=""Where to dump the data"")\nparser.add_argument(""--height"", type=int, default=128, help=""image height"")\nparser.add_argument(""--width"", type=int, default=416, help=""image width"")\nparser.add_argument(""--depth-size-ratio"", type=int, default=1, help=""will divide depth size by that ratio"")\nparser.add_argument(""--num-threads"", type=int, default=4, help=""number of threads to use"")\n\nargs = parser.parse_args()\n\n\ndef dump_example(args, scene):\n    scene_list = data_loader.collect_scenes(scene)\n    # print(""scene list"", scene_list, "" for scene "", scene)\n    for scene_data in scene_list:\n        dump_dir = args.dump_root/scene_data[\'rel_path\']\n        dump_dir.makedirs_p()\n        intrinsics = scene_data[\'intrinsics\']\n\n        dump_cam_file = dump_dir/\'cam.txt\'\n\n        np.savetxt(dump_cam_file, intrinsics)\n        poses_file = dump_dir/\'poses.txt\'\n        poses = []\n\n        for sample in data_loader.get_scene_imgs(scene_data):\n            img, frame_nb = sample[""img""], sample[""id""]\n            dump_img_file = dump_dir/\'{}.jpg\'.format(frame_nb)\n            dump_img_file.parent.makedirs_p()\n            imwrite(dump_img_file, img)\n\n            if ""pose"" in sample.keys():\n                poses.append(sample[""pose""].tolist())\n            if ""depth"" in sample.keys():\n                dump_depth_file = dump_dir/\'{}.npy\'.format(frame_nb)\n                np.save(dump_depth_file, sample[""depth""])\n        if len(poses) != 0:\n            np.savetxt(poses_file, np.array(poses).reshape(-1, 12), fmt=\'%.6e\')\n\n        if len(dump_dir.files(\'*.jpg\')) < 3:\n            dump_dir.rmtree()\n\n\ndef main():\n    args.dump_root = Path(args.dump_root)\n    args.dump_root.mkdir_p()\n\n    global data_loader\n\n    if args.dataset_format == \'kitti\':\n        from kitti_raw_loader import KittiRawLoader\n        data_loader = KittiRawLoader(args.dataset_dir,\n                                     static_frames_file=args.static_frames,\n                                     img_height=args.height,\n                                     img_width=args.width,\n                                     get_depth=args.with_depth,\n                                     get_pose=args.with_pose,\n                                     depth_size_ratio=args.depth_size_ratio)\n\n    if args.dataset_format == \'cityscapes\':\n        from cityscapes_loader import cityscapes_loader\n        data_loader = cityscapes_loader(args.dataset_dir,\n                                        img_height=args.height,\n                                        img_width=args.width)\n\n    n_scenes = len(data_loader.scenes)\n    print(\'Found {} potential scenes\'.format(n_scenes))\n    print(\'Retrieving frames\')\n    if args.num_threads == 1:\n        for scene in tqdm(data_loader.scenes):\n            dump_example(args, scene)\n    else:\n        with ProcessPool(max_workers=args.num_threads) as pool:\n            tasks = pool.map(dump_example, [args]*n_scenes, data_loader.scenes)\n            try:\n                for _ in tqdm(tasks.result(), total=n_scenes):\n                    pass\n            except KeyboardInterrupt as e:\n                tasks.cancel()\n                raise e\n\n    print(\'Generating train val lists\')\n    np.random.seed(8964)\n    # to avoid data snooping, we will make two cameras of the same scene to fall in the same set, train or val\n    subdirs = args.dump_root.dirs()\n    canonic_prefixes = set([subdir.basename()[:-2] for subdir in subdirs])\n    with open(args.dump_root / \'train.txt\', \'w\') as tf:\n        with open(args.dump_root / \'val.txt\', \'w\') as vf:\n            for pr in tqdm(canonic_prefixes):\n                corresponding_dirs = args.dump_root.dirs(\'{}*\'.format(pr))\n                if np.random.random() < 0.1:\n                    for s in corresponding_dirs:\n                        vf.write(\'{}\\n\'.format(s.name))\n                else:\n                    for s in corresponding_dirs:\n                        tf.write(\'{}\\n\'.format(s.name))\n                        if args.with_depth and args.no_train_gt:\n                            for gt_file in s.files(\'*.npy\'):\n                                gt_file.remove_p()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
datasets/sequence_folders.py,1,"b'import torch.utils.data as data\nimport numpy as np\nfrom imageio import imread\nfrom path import Path\nimport random\n\n\ndef load_as_float(path):\n    return imread(path).astype(np.float32)\n\n\nclass SequenceFolder(data.Dataset):\n    """"""A sequence data loader where the files are arranged in this way:\n        root/scene_1/0000000.jpg\n        root/scene_1/0000001.jpg\n        ..\n        root/scene_1/cam.txt\n        root/scene_2/0000000.jpg\n        .\n\n        transform functions must take in a list a images and a numpy array (usually intrinsics matrix)\n    """"""\n\n    def __init__(self, root, seed=None, train=True, sequence_length=3, transform=None, target_transform=None):\n        np.random.seed(seed)\n        random.seed(seed)\n        self.root = Path(root)\n        scene_list_path = self.root/\'train.txt\' if train else self.root/\'val.txt\'\n        self.scenes = [self.root/folder[:-1] for folder in open(scene_list_path)]\n        self.transform = transform\n        self.crawl_folders(sequence_length)\n\n    def crawl_folders(self, sequence_length):\n        sequence_set = []\n        demi_length = (sequence_length-1)//2\n        shifts = list(range(-demi_length, demi_length + 1))\n        shifts.pop(demi_length)\n        for scene in self.scenes:\n            intrinsics = np.genfromtxt(scene/\'cam.txt\').astype(np.float32).reshape((3, 3))\n            imgs = sorted(scene.files(\'*.jpg\'))\n            if len(imgs) < sequence_length:\n                continue\n            for i in range(demi_length, len(imgs)-demi_length):\n                sample = {\'intrinsics\': intrinsics, \'tgt\': imgs[i], \'ref_imgs\': []}\n                for j in shifts:\n                    sample[\'ref_imgs\'].append(imgs[i+j])\n                sequence_set.append(sample)\n        random.shuffle(sequence_set)\n        self.samples = sequence_set\n\n    def __getitem__(self, index):\n        sample = self.samples[index]\n        tgt_img = load_as_float(sample[\'tgt\'])\n        ref_imgs = [load_as_float(ref_img) for ref_img in sample[\'ref_imgs\']]\n        if self.transform is not None:\n            imgs, intrinsics = self.transform([tgt_img] + ref_imgs, np.copy(sample[\'intrinsics\']))\n            tgt_img = imgs[0]\n            ref_imgs = imgs[1:]\n        else:\n            intrinsics = np.copy(sample[\'intrinsics\'])\n        return tgt_img, ref_imgs, intrinsics, np.linalg.inv(intrinsics)\n\n    def __len__(self):\n        return len(self.samples)\n'"
datasets/shifted_sequence_folders.py,0,"b'import numpy as np\nfrom .sequence_folders import SequenceFolder, load_as_float\nimport random\nimport json\n\n\nclass ShiftedSequenceFolder(SequenceFolder):\n    """"""A sequence data loader where the files are arranged in this way:\n        root/scene_1/0000000.jpg\n        root/scene_1/0000001.jpg\n        ..\n        root/scene_1/cam.txt\n        (optional) root/scene_1/shifts.json\n        root/scene_2/0000000.jpg\n        .\n\n        transform functions must take in a list a images and a numpy array (usually intrinsics matrix)\n    """"""\n\n    def __init__(self, root, seed=None, train=True, sequence_length=3, target_displacement=0.02, transform=None, target_transform=None):\n        super().__init__(root, seed, train, sequence_length, transform, target_transform)\n        self.target_displacement = target_displacement\n        self.max_shift = 50\n        self.adjust = False\n\n    def crawl_folders(self, sequence_length):\n        sequence_set = []\n        img_sequences = []\n        demi_length = (sequence_length-1)//2\n        for scene in self.scenes:\n            imgs = sorted(scene.files(\'*.jpg\'))\n            if len(imgs) < sequence_length:\n                continue\n\n            shifts_file = scene/\'shifts.json\'\n            if shifts_file.isfile():\n                with open(shifts_file, \'r\') as f:\n                    shifts = json.load(f)\n            else:\n                prior_shifts = list(range(-demi_length, 0))\n                post_shifts = list(range(1, sequence_length - demi_length))\n                shifts = [[prior_shifts[:], post_shifts[:]] for i in imgs]\n\n            img_sequences.append(imgs)\n            sequence_index = len(img_sequences) - 1\n            intrinsics = np.genfromtxt(scene/\'cam.txt\').astype(np.float32).reshape((3, 3))\n            for i in range(demi_length, len(imgs)-demi_length):\n                sample = {\'intrinsics\': intrinsics,\n                          \'tgt\': i,\n                          \'prior_shifts\': shifts[i][0],\n                          \'post_shifts\': shifts[i][1],\n                          \'sequence_index\': sequence_index}\n                sequence_set.append(sample)\n        random.shuffle(sequence_set)\n        self.samples = sequence_set\n        self.img_sequences = img_sequences\n\n    def __getitem__(self, index):\n        sample = self.samples[index]\n        imgs = self.img_sequences[sample[\'sequence_index\']]\n        tgt_index = sample[\'tgt\']\n        tgt_img = load_as_float(imgs[tgt_index])\n        try:\n            prior_imgs = [load_as_float(imgs[tgt_index + i]) for i in sample[\'prior_shifts\']]\n            post_imgs = [load_as_float(imgs[tgt_index + i]) for i in sample[\'post_shifts\']]\n            imgs = [tgt_img] + prior_imgs + post_imgs\n        except Exception as e:\n            print(index, sample[\'tgt\'], sample[\'prior_shifts\'], sample[\'post_shifts\'], len(imgs))\n            raise e\n        if self.transform is not None:\n            imgs, intrinsics = self.transform(imgs, sample[\'intrinsics\'])\n            tgt_img = imgs[0]\n            ref_imgs = imgs[1:]\n        else:\n            intrinsics = sample[\'intrinsics\']\n        sample = (tgt_img, ref_imgs, intrinsics, np.linalg.inv(intrinsics))\n\n        if self.adjust:\n            return (index, *sample)\n        else:\n            return sample\n\n    def reset_shifts(self, index, prior_ratio, post_ratio):\n        sample = self.samples[index]\n        assert(len(sample[\'prior_shifts\']) == len(prior_ratio))\n        assert(len(sample[\'post_shifts\']) == len(post_ratio))\n        imgs = self.img_sequences[sample[\'sequence_index\']]\n        tgt_index = sample[\'tgt\']\n\n        for j, r in enumerate(prior_ratio[::-1]):\n\n            shift_index = len(prior_ratio) - 1 - j\n            old_shift = sample[\'prior_shifts\'][shift_index]\n            new_shift = old_shift * r\n            assert(new_shift < 0), ""shift must be negative: {:.3f}, {}, {:.3f}"".format(new_shift, old_shift, r)\n            new_shift = round(new_shift)\n            \'\'\' Here is how bounds work for prior shifts:\n            prior shifts must be negative in a strict ascending order in the original list\n            max_shift (in magnitude) is either tgt (to keep index inside list) or self.max_shift\n            Let\'s say you have 2 anterior shifts, which means seq_length is 5\n            1st shift can be -max_shift but cannot be 0 as it would mean that 2nd would not be higher than 1st and above 0\n            2nd shift cannot be -max_shift as 1st shift would have to be less than -max_shift - 1.\n            More generally, shift must be clipped within -max_shift + its index and upper shift - 1\n            Note that priority is given for shifts closer to tgt_index, they are free to choose the value they want, at the risk of\n            constraining outside shifts to one only valid value\n            \'\'\'\n\n            max_shift = min(tgt_index, self.max_shift)\n\n            lower_bound = -max_shift + shift_index\n            upper_bound = -1 if shift_index == len(prior_ratio) - 1 else sample[\'prior_shifts\'][shift_index + 1] - 1\n\n            sample[\'prior_shifts\'][shift_index] = int(np.clip(new_shift, lower_bound, upper_bound))\n\n        for j, r in enumerate(post_ratio):\n            shift_index = j\n            old_shift = sample[\'post_shifts\'][shift_index]\n            new_shift = old_shift * r\n            assert(new_shift > 0), ""shift must be positive: {:.3f}, {}, {}"".format(new_shift, old_shift, r)\n            new_shift = round(new_shift)\n            \'\'\'For posterior shifts :\n            must be postive in a strict descending order\n            max_shift is either len(imgs) - tgt or self.max_shift\n            shift must be clipped within upper shift + 1 and max_shift - seq_length + its index\n            \'\'\'\n\n            max_shift = min(len(imgs) - tgt_index - 1, self.max_shift)\n\n            lower_bound = 1 if shift_index == 0 else sample[\'post_shifts\'][shift_index - 1] + 1\n            upper_bound = max_shift + shift_index - len(post_ratio) + 1\n\n            sample[\'post_shifts\'][shift_index] = int(np.clip(new_shift, lower_bound, upper_bound))\n\n    def get_shifts(self, index):\n        sample = self.samples[index]\n        prior = sample[\'prior_shifts\']\n        post = sample[\'post_shifts\']\n        return prior + post\n\n    def __len__(self):\n        return len(self.samples)\n'"
datasets/stacked_sequence_folders.py,1,"b'import torch.utils.data as data\nimport numpy as np\nfrom imageio import imread\nfrom path import Path\nimport random\n\n\ndef load_as_float(path, sequence_length):\n    stack = imread(path).astype(np.float32)\n    h,w,_ = stack.shape\n    w_img = int(w/(sequence_length))\n    imgs = [stack[:,i*w_img:(i+1)*w_img] for i in range(sequence_length)]\n    tgt_index = sequence_length//2\n    return([imgs[tgt_index]] + imgs[:tgt_index] + imgs[tgt_index+1:])\n\n\nclass SequenceFolder(data.Dataset):\n    """"""A sequence data loader where the images are arranged in this way:\n        root/scene_1/0000000.jpg\n        root/scene_1/0000000_cam.txt\n        root/scene_1/0000001.jpg\n        root/scene_1/0000001_cam.txt\n        .\n        root/scene_2/0000000.jpg\n        root/scene_2/0000000_cam.txt\n    """"""\n\n    def __init__(self, root, seed=None, train=True, sequence_length=3, transform=None, target_transform=None):\n        np.random.seed(seed)\n        random.seed(seed)\n        self.root = Path(root)\n        self.samples = []\n        frames_list_path = self.root/\'train.txt\' if train else self.root/\'val.txt\'\n        self.scenes = self.root.dirs()\n        self.sequence_length = sequence_length\n        for frame_path in open(frames_list_path):\n            a,b = frame_path[:-1].split(\' \')\n            base_path = (self.root/a)/b\n            intrinsics = np.genfromtxt(base_path+\'_cam.txt\', delimiter=\',\').astype(np.float32).reshape((3, 3))\n            sample = {\'intrinsics\': intrinsics, \'img_stack\': base_path+\'.jpg\'}\n            self.samples.append(sample)\n        self.transform = transform\n\n    def __getitem__(self, index):\n        sample = self.samples[index]\n        imgs = load_as_float(sample[\'img_stack\'], self.sequence_length)\n        if self.transform is not None:\n            imgs, intrinsics = self.transform(imgs, np.copy(sample[\'intrinsics\']))\n        else:\n            intrinsics = sample[\'intrinsics\']\n        return imgs[0], imgs[1:], intrinsics, np.linalg.inv(intrinsics)\n\n    def __len__(self):\n        return len(self.samples)\n'"
datasets/validation_folders.py,1,"b'import torch.utils.data as data\nimport numpy as np\nfrom imageio import imread\nfrom path import Path\n\n\ndef crawl_folders(folders_list):\n        imgs = []\n        depth = []\n        for folder in folders_list:\n            current_imgs = sorted(folder.files(\'*.jpg\'))\n            current_depth = []\n            for img in current_imgs:\n                d = img.dirname()/(img.name[:-4] + \'.npy\')\n                assert(d.isfile()), ""depth file {} not found"".format(str(d))\n                depth.append(d)\n            imgs.extend(current_imgs)\n            depth.extend(current_depth)\n        return imgs, depth\n\n\ndef load_as_float(path):\n    return imread(path).astype(np.float32)\n\n\nclass ValidationSet(data.Dataset):\n    """"""A sequence data loader where the files are arranged in this way:\n        root/scene_1/0000000.jpg\n        root/scene_1/0000000.npy\n        root/scene_1/0000001.jpg\n        root/scene_1/0000001.npy\n        ..\n        root/scene_2/0000000.jpg\n        root/scene_2/0000000.npy\n        .\n\n        transform functions must take in a list a images and a numpy array which can be None\n    """"""\n\n    def __init__(self, root, transform=None):\n        self.root = Path(root)\n        scene_list_path = self.root/\'val.txt\'\n        self.scenes = [self.root/folder[:-1] for folder in open(scene_list_path)]\n        self.imgs, self.depth = crawl_folders(self.scenes)\n        self.transform = transform\n\n    def __getitem__(self, index):\n        img = load_as_float(self.imgs[index])\n        depth = np.load(self.depth[index]).astype(np.float32)\n        if self.transform is not None:\n            img, _ = self.transform([img], None)\n            img = img[0]\n        return img, depth\n\n    def __len__(self):\n        return len(self.imgs)\n'"
kitti_eval/depth_evaluation_utils.py,0,"b'# Mostly based on the code written by Clement Godard:\n# https://github.com/mrharicot/monodepth/blob/master/utils/evaluation_utils.py\nimport numpy as np\nfrom collections import Counter\nfrom path import Path\nfrom scipy.misc import imread\nfrom tqdm import tqdm\nimport datetime\n\n\nclass test_framework_KITTI(object):\n    def __init__(self, root, test_files, seq_length=3, min_depth=1e-3, max_depth=100, step=1, use_gps=True):\n        self.root = root\n        self.min_depth, self.max_depth = min_depth, max_depth\n        self.use_gps = use_gps\n        self.calib_dirs, self.gt_files, self.img_files, self.displacements, self.cams = read_scene_data(self.root,\n                                                                                                        test_files,\n                                                                                                        seq_length,\n                                                                                                        step,\n                                                                                                        self.use_gps)\n\n    def __getitem__(self, i):\n        tgt = imread(self.img_files[i][0]).astype(np.float32)\n        depth = generate_depth_map(self.calib_dirs[i], self.gt_files[i], tgt.shape[:2], self.cams[i])\n        return {\'tgt\': tgt,\n                \'ref\': [imread(img).astype(np.float32) for img in self.img_files[i][1]],\n                \'path\':self.img_files[i][0],\n                \'gt_depth\': depth,\n                \'displacements\': np.array(self.displacements[i]),\n                \'mask\': generate_mask(depth, self.min_depth, self.max_depth)\n                }\n\n    def __len__(self):\n        return len(self.img_files)\n\n\n###############################################################################\n#  EIGEN\n\ndef getXYZ(lat, lon, alt):\n    """"""Helper method to compute a R(3) pose vector from an OXTS packet.\n    Unlike KITTI official devkit, we use sinusoidal projection (https://en.wikipedia.org/wiki/Sinusoidal_projection)\n    instead of mercator as it is much simpler.\n    Initially Mercator was used because it renders nicely for Odometry vizualisation, but we don\'t need that here.\n    In order to avoid problems for potential other runs closer to the pole in the future,\n    we stick to sinusoidal which keeps the distances cleaner than mercator (and that\'s the only thing we want here)\n    See https://github.com/utiasSTARS/pykitti/issues/24\n    """"""\n    er = 6378137.  # earth radius (approx.) in meters\n    scale = np.cos(lat * np.pi / 180.)\n    tx = scale * lon * np.pi * er / 180.\n    ty = er * lat * np.pi / 180.\n    tz = alt\n    t = np.array([tx, ty, tz])\n    return t\n\n\ndef get_displacements_from_GPS(root, date, scene, indices, tgt_index, precision_warning_threshold=2):\n    """"""gets displacement magntidues between middle frame and other frames, this is, to a scaling factor\n    the mean output PoseNet should have for translation. Since the scaling is the same factor for depth maps and\n    for translations, it will be used to determine how much predicted depth should be multiplied to.""""""\n\n    first_pose = None\n    displacements = []\n    oxts_root = root/date/scene/\'oxts\'\n    if len(indices) == 0:\n        return 0\n    reordered_indices = [indices[tgt_index]] + [*indices[:tgt_index]] + [*indices[tgt_index + 1:]]\n    already_warned = False\n    for index in reordered_indices:\n        oxts_data = np.genfromtxt(oxts_root/\'data\'/\'{:010d}.txt\'.format(index))\n\n        if not already_warned:\n            position_precision = oxts_data[23]\n            if position_precision > precision_warning_threshold:\n                print(""Warning for scene {} frame {} : bad position precision from oxts ({:.2f}m). ""\n                      ""You might want to get displacements from speed"".format(scene, index, position_precision))\n            already_warned = True\n\n        lat, lon, alt = oxts_data[:3]\n        pose = getXYZ(lat, lon, alt)\n        if first_pose is None:\n            first_pose = pose\n        else:\n            displacements.append(np.linalg.norm(pose - first_pose))\n    return displacements\n\n\ndef get_displacements_from_speed(root, date, scene, indices, tgt_index):\n    """"""get displacement magnitudes by integrating over speed values.\n    Might be a good alternative if the GPS is not good enough""""""\n    if len(indices) == 0:\n        return []\n    oxts_root = root/date/scene/\'oxts\'\n    with open(oxts_root/\'timestamps.txt\') as f:\n        timestamps = np.array([datetime.datetime.strptime(ts[:-3], ""%Y-%m-%d %H:%M:%S.%f"").timestamp() for ts in f.read().splitlines()])\n    speeds = np.zeros((len(indices), 3))\n    for i, index in enumerate(indices):\n        oxts_data = np.genfromtxt(oxts_root/\'data\'/\'{:010d}.txt\'.format(index))\n        speeds[i] = oxts_data[[6,7,10]]\n    displacements = np.zeros((len(indices), 3))\n    # Perform the integration operation, using trapezoidal method\n    for i0, (i1, i2) in enumerate(zip(indices, indices[1:])):\n        displacements[i0 + 1] = displacements[i0] + 0.5*(speeds[i0] + speeds[i0 + 1]) * (timestamps[i1] - timestamps[i2])\n    # Set the origin of displacements at tgt_index\n    displacements -= displacements[tgt_index]\n    # Finally, get the displacement magnitude relative to tgt and discard the middle value (which is supposed to be 0)\n    displacements_mag = np.linalg.norm(displacements, axis=1)\n    return np.concatenate([displacements_mag[:tgt_index], displacements_mag[tgt_index + 1:]])\n\n\ndef read_scene_data(data_root, test_list, seq_length=3, step=1, use_gps=True):\n    data_root = Path(data_root)\n    gt_files = []\n    calib_dirs = []\n    im_files = []\n    cams = []\n    displacements = []\n    demi_length = (seq_length - 1) // 2\n    shift_range = step * np.arange(-demi_length, demi_length + 1)\n\n    print(\'getting test metadata ... \')\n    for sample in tqdm(test_list):\n        tgt_img_path = data_root/sample\n        date, scene, cam_id, _, index = sample[:-4].split(\'/\')\n\n        scene_length = len(tgt_img_path.parent.files(\'*.png\'))\n\n        ref_indices = shift_range + np.clip(int(index), step*demi_length, scene_length - step*demi_length - 1)\n\n        ref_imgs_path = [tgt_img_path.dirname()/\'{:010d}.png\'.format(i) for i in ref_indices]\n        vel_path = data_root/date/scene/\'velodyne_points\'/\'data\'/\'{}.bin\'.format(index[:10])\n\n        if tgt_img_path.isfile():\n            gt_files.append(vel_path)\n            calib_dirs.append(data_root/date)\n            im_files.append([tgt_img_path,ref_imgs_path])\n            cams.append(int(cam_id[-2:]))\n\n            args = (data_root, date, scene, ref_indices, demi_length)\n            if use_gps:\n                displacements.append(get_displacements_from_GPS(*args))\n            else:\n                displacements.append(get_displacements_from_speed(*args))\n        else:\n            print(\'{} missing\'.format(tgt_img_path))\n\n    return calib_dirs, gt_files, im_files, displacements, cams\n\n\ndef load_velodyne_points(file_name):\n    # adapted from https://github.com/hunse/kitti\n    points = np.fromfile(file_name, dtype=np.float32).reshape(-1, 4)\n    points[:,3] = 1\n    return points\n\n\ndef read_calib_file(path):\n    # taken from https://github.com/hunse/kitti\n    float_chars = set(""0123456789.e+- "")\n    data = {}\n    with open(path, \'r\') as f:\n        for line in f.readlines():\n            key, value = line.split(\':\', 1)\n            value = value.strip()\n            data[key] = value\n            if float_chars.issuperset(value):\n                # try to cast to float array\n                try:\n                    data[key] = np.array(list(map(float, value.split(\' \'))))\n                except ValueError:\n                    # casting error: data[key] already eq. value, so pass\n                    pass\n\n    return data\n\n\ndef sub2ind(matrixSize, rowSub, colSub):\n    m, n = matrixSize\n    return rowSub * (n-1) + colSub - 1\n\n\ndef generate_depth_map(calib_dir, velo_file_name, im_shape, cam=2):\n    # load calibration files\n    cam2cam = read_calib_file(calib_dir/\'calib_cam_to_cam.txt\')\n    velo2cam = read_calib_file(calib_dir/\'calib_velo_to_cam.txt\')\n    velo2cam = np.hstack((velo2cam[\'R\'].reshape(3,3), velo2cam[\'T\'][..., np.newaxis]))\n    velo2cam = np.vstack((velo2cam, np.array([0, 0, 0, 1.0])))\n\n    # compute projection matrix velodyne->image plane\n    R_cam2rect = np.eye(4)\n    R_cam2rect[:3,:3] = cam2cam[\'R_rect_00\'].reshape(3,3)\n    P_rect = cam2cam[\'P_rect_0\'+str(cam)].reshape(3,4)\n    P_velo2im = np.dot(np.dot(P_rect, R_cam2rect), velo2cam)\n\n    # load velodyne points and remove all behind image plane (approximation)\n    # each row of the velodyne data is forward, left, up, reflectance\n    velo = load_velodyne_points(velo_file_name)\n    velo = velo[velo[:, 0] >= 0, :]\n\n    # project the points to the camera\n    velo_pts_im = np.dot(P_velo2im, velo.T).T\n    velo_pts_im[:, :2] = velo_pts_im[:,:2] / velo_pts_im[:,-1:]\n\n    # check if in bounds\n    # use minus 1 to get the exact same value as KITTI matlab code\n    velo_pts_im[:, 0] = np.round(velo_pts_im[:,0]) - 1\n    velo_pts_im[:, 1] = np.round(velo_pts_im[:,1]) - 1\n    val_inds = (velo_pts_im[:, 0] >= 0) & (velo_pts_im[:, 1] >= 0)\n    val_inds = val_inds & (velo_pts_im[:,0] < im_shape[1]) & (velo_pts_im[:,1] < im_shape[0])\n    velo_pts_im = velo_pts_im[val_inds, :]\n\n    # project to image\n    depth = np.zeros((im_shape))\n    depth[velo_pts_im[:, 1].astype(np.int), velo_pts_im[:, 0].astype(np.int)] = velo_pts_im[:, 2]\n\n    # find the duplicate points and choose the closest depth\n    inds = sub2ind(depth.shape, velo_pts_im[:, 1], velo_pts_im[:, 0])\n    dupe_inds = [item for item, count in Counter(inds).items() if count > 1]\n    for dd in dupe_inds:\n        pts = np.where(inds == dd)[0]\n        x_loc = int(velo_pts_im[pts[0], 0])\n        y_loc = int(velo_pts_im[pts[0], 1])\n        depth[y_loc, x_loc] = velo_pts_im[pts, 2].min()\n    depth[depth < 0] = 0\n    return depth\n\n\ndef generate_mask(gt_depth, min_depth, max_depth):\n    mask = np.logical_and(gt_depth > min_depth,\n                          gt_depth < max_depth)\n    # crop used by Garg ECCV16 to reprocude Eigen NIPS14 results\n    # if used on gt_size 370x1224 produces a crop of [-218, -3, 44, 1180]\n    gt_height, gt_width = gt_depth.shape\n    crop = np.array([0.40810811 * gt_height, 0.99189189 * gt_height,\n                     0.03594771 * gt_width,  0.96405229 * gt_width]).astype(np.int32)\n\n    crop_mask = np.zeros(mask.shape)\n    crop_mask[crop[0]:crop[1],crop[2]:crop[3]] = 1\n    mask = np.logical_and(mask, crop_mask)\n    return mask\n'"
kitti_eval/pose_evaluation_utils.py,0,"b""# Mostly based on the code written by Clement Godard:\n# https://github.com/mrharicot/monodepth/blob/master/utils/evaluation_utils.py\nimport numpy as np\n# import pandas as pd\nfrom path import Path\nfrom scipy.misc import imread\nfrom tqdm import tqdm\n\n\nclass test_framework_KITTI(object):\n    def __init__(self, root, sequence_set, seq_length=3, step=1):\n        self.root = root\n        self.img_files, self.poses, self.sample_indices = read_scene_data(self.root, sequence_set, seq_length, step)\n\n    def generator(self):\n        for img_list, pose_list, sample_list in zip(self.img_files, self.poses, self.sample_indices):\n            for snippet_indices in sample_list:\n                imgs = [imread(img_list[i]).astype(np.float32) for i in snippet_indices]\n\n                poses = np.stack(pose_list[i] for i in snippet_indices)\n                first_pose = poses[0]\n                poses[:,:,-1] -= first_pose[:,-1]\n                compensated_poses = np.linalg.inv(first_pose[:,:3]) @ poses\n\n                yield {'imgs': imgs,\n                       'path': img_list[0],\n                       'poses': compensated_poses\n                       }\n\n    def __iter__(self):\n        return self.generator()\n\n    def __len__(self):\n        return sum(len(imgs) for imgs in self.img_files)\n\n\ndef read_scene_data(data_root, sequence_set, seq_length=3, step=1):\n    data_root = Path(data_root)\n    im_sequences = []\n    poses_sequences = []\n    indices_sequences = []\n    demi_length = (seq_length - 1) // 2\n    shift_range = np.array([step*i for i in range(-demi_length, demi_length + 1)]).reshape(1, -1)\n\n    sequences = set()\n    for seq in sequence_set:\n        corresponding_dirs = set((data_root/'sequences').dirs(seq))\n        sequences = sequences | corresponding_dirs\n\n    print('getting test metadata for theses sequences : {}'.format(sequences))\n    for sequence in tqdm(sequences):\n        poses = np.genfromtxt(data_root/'poses'/'{}.txt'.format(sequence.name)).astype(np.float64).reshape(-1, 3, 4)\n        imgs = sorted((sequence/'image_2').files('*.png'))\n        # construct 5-snippet sequences\n        tgt_indices = np.arange(demi_length, len(imgs) - demi_length).reshape(-1, 1)\n        snippet_indices = shift_range + tgt_indices\n        im_sequences.append(imgs)\n        poses_sequences.append(poses)\n        indices_sequences.append(snippet_indices)\n    return im_sequences, poses_sequences, indices_sequences"""
models/DispNetS.py,10,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.init import xavier_uniform_, zeros_\n\n\ndef downsample_conv(in_planes, out_planes, kernel_size=3):\n    return nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=2, padding=(kernel_size-1)//2),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out_planes, out_planes, kernel_size=kernel_size, padding=(kernel_size-1)//2),\n        nn.ReLU(inplace=True)\n    )\n\n\ndef predict_disp(in_planes):\n    return nn.Sequential(\n        nn.Conv2d(in_planes, 1, kernel_size=3, padding=1),\n        nn.Sigmoid()\n    )\n\n\ndef conv(in_planes, out_planes):\n    return nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True)\n    )\n\n\ndef upconv(in_planes, out_planes):\n    return nn.Sequential(\n        nn.ConvTranspose2d(in_planes, out_planes, kernel_size=3, stride=2, padding=1, output_padding=1),\n        nn.ReLU(inplace=True)\n    )\n\n\ndef crop_like(input, ref):\n    assert(input.size(2) >= ref.size(2) and input.size(3) >= ref.size(3))\n    return input[:, :, :ref.size(2), :ref.size(3)]\n\n\nclass DispNetS(nn.Module):\n\n    def __init__(self, alpha=10, beta=0.01):\n        super(DispNetS, self).__init__()\n\n        self.alpha = alpha\n        self.beta = beta\n\n        conv_planes = [32, 64, 128, 256, 512, 512, 512]\n        self.conv1 = downsample_conv(3,              conv_planes[0], kernel_size=7)\n        self.conv2 = downsample_conv(conv_planes[0], conv_planes[1], kernel_size=5)\n        self.conv3 = downsample_conv(conv_planes[1], conv_planes[2])\n        self.conv4 = downsample_conv(conv_planes[2], conv_planes[3])\n        self.conv5 = downsample_conv(conv_planes[3], conv_planes[4])\n        self.conv6 = downsample_conv(conv_planes[4], conv_planes[5])\n        self.conv7 = downsample_conv(conv_planes[5], conv_planes[6])\n\n        upconv_planes = [512, 512, 256, 128, 64, 32, 16]\n        self.upconv7 = upconv(conv_planes[6],   upconv_planes[0])\n        self.upconv6 = upconv(upconv_planes[0], upconv_planes[1])\n        self.upconv5 = upconv(upconv_planes[1], upconv_planes[2])\n        self.upconv4 = upconv(upconv_planes[2], upconv_planes[3])\n        self.upconv3 = upconv(upconv_planes[3], upconv_planes[4])\n        self.upconv2 = upconv(upconv_planes[4], upconv_planes[5])\n        self.upconv1 = upconv(upconv_planes[5], upconv_planes[6])\n\n        self.iconv7 = conv(upconv_planes[0] + conv_planes[5], upconv_planes[0])\n        self.iconv6 = conv(upconv_planes[1] + conv_planes[4], upconv_planes[1])\n        self.iconv5 = conv(upconv_planes[2] + conv_planes[3], upconv_planes[2])\n        self.iconv4 = conv(upconv_planes[3] + conv_planes[2], upconv_planes[3])\n        self.iconv3 = conv(1 + upconv_planes[4] + conv_planes[1], upconv_planes[4])\n        self.iconv2 = conv(1 + upconv_planes[5] + conv_planes[0], upconv_planes[5])\n        self.iconv1 = conv(1 + upconv_planes[6], upconv_planes[6])\n\n        self.predict_disp4 = predict_disp(upconv_planes[3])\n        self.predict_disp3 = predict_disp(upconv_planes[4])\n        self.predict_disp2 = predict_disp(upconv_planes[5])\n        self.predict_disp1 = predict_disp(upconv_planes[6])\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n                xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    zeros_(m.bias)\n\n    def forward(self, x):\n        out_conv1 = self.conv1(x)\n        out_conv2 = self.conv2(out_conv1)\n        out_conv3 = self.conv3(out_conv2)\n        out_conv4 = self.conv4(out_conv3)\n        out_conv5 = self.conv5(out_conv4)\n        out_conv6 = self.conv6(out_conv5)\n        out_conv7 = self.conv7(out_conv6)\n\n        out_upconv7 = crop_like(self.upconv7(out_conv7), out_conv6)\n        concat7 = torch.cat((out_upconv7, out_conv6), 1)\n        out_iconv7 = self.iconv7(concat7)\n\n        out_upconv6 = crop_like(self.upconv6(out_iconv7), out_conv5)\n        concat6 = torch.cat((out_upconv6, out_conv5), 1)\n        out_iconv6 = self.iconv6(concat6)\n\n        out_upconv5 = crop_like(self.upconv5(out_iconv6), out_conv4)\n        concat5 = torch.cat((out_upconv5, out_conv4), 1)\n        out_iconv5 = self.iconv5(concat5)\n\n        out_upconv4 = crop_like(self.upconv4(out_iconv5), out_conv3)\n        concat4 = torch.cat((out_upconv4, out_conv3), 1)\n        out_iconv4 = self.iconv4(concat4)\n        disp4 = self.alpha * self.predict_disp4(out_iconv4) + self.beta\n\n        out_upconv3 = crop_like(self.upconv3(out_iconv4), out_conv2)\n        disp4_up = crop_like(F.interpolate(disp4, scale_factor=2, mode='bilinear', align_corners=False), out_conv2)\n        concat3 = torch.cat((out_upconv3, out_conv2, disp4_up), 1)\n        out_iconv3 = self.iconv3(concat3)\n        disp3 = self.alpha * self.predict_disp3(out_iconv3) + self.beta\n\n        out_upconv2 = crop_like(self.upconv2(out_iconv3), out_conv1)\n        disp3_up = crop_like(F.interpolate(disp3, scale_factor=2, mode='bilinear', align_corners=False), out_conv1)\n        concat2 = torch.cat((out_upconv2, out_conv1, disp3_up), 1)\n        out_iconv2 = self.iconv2(concat2)\n        disp2 = self.alpha * self.predict_disp2(out_iconv2) + self.beta\n\n        out_upconv1 = crop_like(self.upconv1(out_iconv2), x)\n        disp2_up = crop_like(F.interpolate(disp2, scale_factor=2, mode='bilinear', align_corners=False), x)\n        concat1 = torch.cat((out_upconv1, disp2_up), 1)\n        out_iconv1 = self.iconv1(concat1)\n        disp1 = self.alpha * self.predict_disp1(out_iconv1) + self.beta\n\n        if self.training:\n            return disp1, disp2, disp3, disp4\n        else:\n            return disp1\n"""
models/PoseExpNet.py,3,"b'import torch\nimport torch.nn as nn\nfrom torch import sigmoid\nfrom torch.nn.init import xavier_uniform_, zeros_\n\n\ndef conv(in_planes, out_planes, kernel_size=3):\n    return nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, padding=(kernel_size-1)//2, stride=2),\n        nn.ReLU(inplace=True)\n    )\n\n\ndef upconv(in_planes, out_planes):\n    return nn.Sequential(\n        nn.ConvTranspose2d(in_planes, out_planes, kernel_size=4, stride=2, padding=1),\n        nn.ReLU(inplace=True)\n    )\n\n\nclass PoseExpNet(nn.Module):\n\n    def __init__(self, nb_ref_imgs=2, output_exp=False):\n        super(PoseExpNet, self).__init__()\n        self.nb_ref_imgs = nb_ref_imgs\n        self.output_exp = output_exp\n\n        conv_planes = [16, 32, 64, 128, 256, 256, 256]\n        self.conv1 = conv(3*(1+self.nb_ref_imgs), conv_planes[0], kernel_size=7)\n        self.conv2 = conv(conv_planes[0], conv_planes[1], kernel_size=5)\n        self.conv3 = conv(conv_planes[1], conv_planes[2])\n        self.conv4 = conv(conv_planes[2], conv_planes[3])\n        self.conv5 = conv(conv_planes[3], conv_planes[4])\n        self.conv6 = conv(conv_planes[4], conv_planes[5])\n        self.conv7 = conv(conv_planes[5], conv_planes[6])\n\n        self.pose_pred = nn.Conv2d(conv_planes[6], 6*self.nb_ref_imgs, kernel_size=1, padding=0)\n\n        if self.output_exp:\n            upconv_planes = [256, 128, 64, 32, 16]\n            self.upconv5 = upconv(conv_planes[4],   upconv_planes[0])\n            self.upconv4 = upconv(upconv_planes[0], upconv_planes[1])\n            self.upconv3 = upconv(upconv_planes[1], upconv_planes[2])\n            self.upconv2 = upconv(upconv_planes[2], upconv_planes[3])\n            self.upconv1 = upconv(upconv_planes[3], upconv_planes[4])\n\n            self.predict_mask4 = nn.Conv2d(upconv_planes[1], self.nb_ref_imgs, kernel_size=3, padding=1)\n            self.predict_mask3 = nn.Conv2d(upconv_planes[2], self.nb_ref_imgs, kernel_size=3, padding=1)\n            self.predict_mask2 = nn.Conv2d(upconv_planes[3], self.nb_ref_imgs, kernel_size=3, padding=1)\n            self.predict_mask1 = nn.Conv2d(upconv_planes[4], self.nb_ref_imgs, kernel_size=3, padding=1)\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n                xavier_uniform_(m.weight.data)\n                if m.bias is not None:\n                    zeros_(m.bias)\n\n    def forward(self, target_image, ref_imgs):\n        assert(len(ref_imgs) == self.nb_ref_imgs)\n        input = [target_image]\n        input.extend(ref_imgs)\n        input = torch.cat(input, 1)\n        out_conv1 = self.conv1(input)\n        out_conv2 = self.conv2(out_conv1)\n        out_conv3 = self.conv3(out_conv2)\n        out_conv4 = self.conv4(out_conv3)\n        out_conv5 = self.conv5(out_conv4)\n        out_conv6 = self.conv6(out_conv5)\n        out_conv7 = self.conv7(out_conv6)\n\n        pose = self.pose_pred(out_conv7)\n        pose = pose.mean(3).mean(2)\n        pose = 0.01 * pose.view(pose.size(0), self.nb_ref_imgs, 6)\n\n        if self.output_exp:\n            out_upconv5 = self.upconv5(out_conv5  )[:, :, 0:out_conv4.size(2), 0:out_conv4.size(3)]\n            out_upconv4 = self.upconv4(out_upconv5)[:, :, 0:out_conv3.size(2), 0:out_conv3.size(3)]\n            out_upconv3 = self.upconv3(out_upconv4)[:, :, 0:out_conv2.size(2), 0:out_conv2.size(3)]\n            out_upconv2 = self.upconv2(out_upconv3)[:, :, 0:out_conv1.size(2), 0:out_conv1.size(3)]\n            out_upconv1 = self.upconv1(out_upconv2)[:, :, 0:input.size(2), 0:input.size(3)]\n\n            exp_mask4 = sigmoid(self.predict_mask4(out_upconv4))\n            exp_mask3 = sigmoid(self.predict_mask3(out_upconv3))\n            exp_mask2 = sigmoid(self.predict_mask2(out_upconv2))\n            exp_mask1 = sigmoid(self.predict_mask1(out_upconv1))\n        else:\n            exp_mask4 = None\n            exp_mask3 = None\n            exp_mask2 = None\n            exp_mask1 = None\n\n        if self.training:\n            return [exp_mask1, exp_mask2, exp_mask3, exp_mask4], pose\n        else:\n            return exp_mask1, pose\n'"
models/__init__.py,0,b'from .DispNetS import DispNetS\nfrom .PoseExpNet import PoseExpNet\n'
stillbox_eval/depth_evaluation_utils.py,0,"b'import numpy as np\nimport json\nfrom path import Path\nfrom scipy.misc import imread\nfrom tqdm import tqdm\n\n\nclass test_framework_stillbox(object):\n    def __init__(self, root, test_files, seq_length=3, min_depth=1e-3, max_depth=80, step=1, **kwargs):\n        self.root = root\n        self.min_depth, self.max_depth = min_depth, max_depth\n        self.gt_files, self.img_files, self.displacements = read_scene_data(self.root, test_files, seq_length, step)\n\n    def __getitem__(self, i):\n        tgt = imread(self.img_files[i][0]).astype(np.float32)\n        depth = np.load(self.gt_files[i])\n        return {\'tgt\': tgt,\n                \'ref\': [imread(img).astype(np.float32) for img in self.img_files[i][1]],\n                \'path\':self.img_files[i][0],\n                \'gt_depth\': depth,\n                \'displacements\': np.array(self.displacements[i]),\n                \'mask\': generate_mask(depth, self.min_depth, self.max_depth)\n                }\n\n    def __len__(self):\n        return len(self.img_files)\n\n\ndef get_displacements(scene, middle_index, ref_indices):\n    assert(all(i < scene[\'length\'] and i >= 0 for i in ref_indices)), str(ref_indices)\n    atomic_movement = np.linalg.norm(scene[\'speed\'])*scene[\'time_step\']\n    """"""in Still box, movements are rectilinear so magnitude adds up.\n    I mean, this is very convenient, I wonder who is the genius who came with such a dataset""""""\n    displacements = np.abs(atomic_movement * (np.array(ref_indices) - ref_indices[middle_index]))\n    return [*displacements[:middle_index], *displacements[middle_index+1:]]\n\n\ndef read_scene_data(data_root, test_list, seq_length=3, step=1):\n    data_root = Path(data_root)\n    metadata_files = {}\n    for folder in data_root.dirs():\n        with open(folder/\'metadata.json\', \'r\') as f:\n            metadata_files[str(folder.name)] = json.load(f)\n    gt_files = []\n    im_files = []\n    displacements = []\n    demi_length = (seq_length - 1) // 2\n    shift_range = step * np.arange(-demi_length, demi_length + 1)\n\n    print(\'getting test metadata ... \')\n    for sample in tqdm(test_list):\n        folder, file = sample.split(\'/\')\n        _, scene_index, index = file[:-4].split(\'_\')  # filename is in the form \'RGB_XXXX_XX.jpg\'\n        index = int(index)\n        scene = metadata_files[folder][\'scenes\'][int(scene_index)]\n        tgt_img_path = data_root/sample\n        folder_path = data_root/folder\n        if tgt_img_path.isfile():\n            ref_indices = shift_range + np.clip(index, step*demi_length, scene[\'length\'] - step * demi_length - 1)\n            ref_imgs_path = [folder_path/\'{}\'.format(scene[\'imgs\'][ref_index]) for ref_index in ref_indices]\n\n            gt_files.append(folder_path/\'{}\'.format(scene[\'depth\'][index]))\n            im_files.append([tgt_img_path,ref_imgs_path])\n            displacements.append(get_displacements(scene, demi_length, ref_indices))\n        else:\n            print(\'{} missing\'.format(tgt_img_path))\n\n    return gt_files, im_files, displacements\n\n\ndef generate_mask(gt_depth, min_depth, max_depth):\n    mask = np.logical_and(gt_depth > min_depth,\n                          gt_depth < max_depth)\n    # crop gt to exclude border values\n    # if used on gt_size 100x100 produces a crop of [-95, -5, 5, 95]\n    gt_height, gt_width = gt_depth.shape\n    crop = np.array([0.05 * gt_height, 0.95 * gt_height,\n                     0.05 * gt_width,  0.95 * gt_width]).astype(np.int32)\n\n    crop_mask = np.zeros(mask.shape)\n    crop_mask[crop[0]:crop[1],crop[2]:crop[3]] = 1\n    mask = np.logical_and(mask, crop_mask)\n    return mask\n'"
