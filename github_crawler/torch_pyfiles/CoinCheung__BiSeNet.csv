file_path,api_count,code
cityscapes.py,1,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport torch\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\n\nimport os.path as osp\nimport os\nfrom PIL import Image\nimport numpy as np\nimport json\n\nfrom transform import *\n\n\n\nclass CityScapes(Dataset):\n    def __init__(self, rootpth, cropsize=(640, 480), mode=\'train\', *args, **kwargs):\n        super(CityScapes, self).__init__(*args, **kwargs)\n        assert mode in (\'train\', \'val\', \'test\')\n        self.mode = mode\n        self.ignore_lb = 255\n\n        with open(\'./cityscapes_info.json\', \'r\') as fr:\n            labels_info = json.load(fr)\n        self.lb_map = {el[\'id\']: el[\'trainId\'] for el in labels_info}\n\n        ## parse img directory\n        self.imgs = {}\n        imgnames = []\n        impth = osp.join(rootpth, \'leftImg8bit\', mode)\n        folders = os.listdir(impth)\n        for fd in folders:\n            fdpth = osp.join(impth, fd)\n            im_names = os.listdir(fdpth)\n            names = [el.replace(\'_leftImg8bit.png\', \'\') for el in im_names]\n            impths = [osp.join(fdpth, el) for el in im_names]\n            imgnames.extend(names)\n            self.imgs.update(dict(zip(names, impths)))\n\n        ## parse gt directory\n        self.labels = {}\n        gtnames = []\n        gtpth = osp.join(rootpth, \'gtFine\', mode)\n        folders = os.listdir(gtpth)\n        for fd in folders:\n            fdpth = osp.join(gtpth, fd)\n            lbnames = os.listdir(fdpth)\n            lbnames = [el for el in lbnames if \'labelIds\' in el]\n            names = [el.replace(\'_gtFine_labelIds.png\', \'\') for el in lbnames]\n            lbpths = [osp.join(fdpth, el) for el in lbnames]\n            gtnames.extend(names)\n            self.labels.update(dict(zip(names, lbpths)))\n\n        self.imnames = imgnames\n        self.len = len(self.imnames)\n        assert set(imgnames) == set(gtnames)\n        assert set(self.imnames) == set(self.imgs.keys())\n        assert set(self.imnames) == set(self.labels.keys())\n\n        ## pre-processing\n        self.to_tensor = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n            ])\n        self.trans_train = Compose([\n            ColorJitter(\n                brightness = 0.5,\n                contrast = 0.5,\n                saturation = 0.5),\n            HorizontalFlip(),\n            RandomScale((0.75, 1.0, 1.25, 1.5, 1.75, 2.0)),\n            RandomCrop(cropsize)\n            ])\n\n\n    def __getitem__(self, idx):\n        fn  = self.imnames[idx]\n        impth = self.imgs[fn]\n        lbpth = self.labels[fn]\n        img = Image.open(impth).convert(\'RGB\')\n        label = Image.open(lbpth)\n        if self.mode == \'train\':\n            im_lb = dict(im = img, lb = label)\n            im_lb = self.trans_train(im_lb)\n            img, label = im_lb[\'im\'], im_lb[\'lb\']\n        img = self.to_tensor(img)\n        label = np.array(label).astype(np.int64)[np.newaxis, :]\n        label = self.convert_labels(label)\n        return img, label\n\n\n    def __len__(self):\n        return self.len\n\n\n    def convert_labels(self, label):\n        for k, v in self.lb_map.items():\n            label[label == k] = v\n        return label\n\n\n\nif __name__ == ""__main__"":\n    from tqdm import tqdm\n    ds = CityScapes(\'./data/\', n_classes=19, mode=\'val\')\n    uni = []\n    for im, lb in tqdm(ds):\n        lb_uni = np.unique(lb).tolist()\n        uni.extend(lb_uni)\n    print(uni)\n    print(set(uni))\n\n'"
demo.py,2,"b""\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport cv2\n\nfrom fp16.model import BiSeNet\n\n\n# args\nparse = argparse.ArgumentParser()\nparse.add_argument(\n        '--ckpt',\n        dest='ckpt',\n        type=str,\n        default='./res/model_final.pth',)\nparse.add_argument(\n        '--img_path',\n        dest='img_path',\n        type=str,\n        default='./pic.jpg',)\nargs = parse.parse_args()\n\n\n# define model\nnet = BiSeNet(n_classes=19)\nnet.load_state_dict(torch.load(args.ckpt, map_location='cpu'))\nnet.eval()\nnet.cuda()\n\n# prepare data\nto_tensor = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ])\nim = to_tensor(Image.open(args.img_path).convert('RGB')).unsqueeze(0).cuda()\n\n# inference\nout = net(im)[0].argmax(dim=1).squeeze().detach().cpu().numpy()\ncv2.imwrite('./res.jpg', out)\n"""
evaluate.py,12,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nfrom logger import setup_logger\nfrom model import BiSeNet\nfrom cityscapes import CityScapes\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torch.distributed as dist\n\nimport os\nimport os.path as osp\nimport logging\nimport time\nimport numpy as np\nfrom tqdm import tqdm\nimport math\n\n\n\nclass MscEval(object):\n    def __init__(self,\n            model,\n            dataloader,\n            scales = [0.5, 0.75, 1, 1.25, 1.5, 1.75],\n            n_classes = 19,\n            lb_ignore = 255,\n            cropsize = 1024,\n            flip = True,\n            *args, **kwargs):\n        self.scales = scales\n        self.n_classes = n_classes\n        self.lb_ignore = lb_ignore\n        self.flip = flip\n        self.cropsize = cropsize\n        ## dataloader\n        self.dl = dataloader\n        self.net = model\n\n\n    def pad_tensor(self, inten, size):\n        N, C, H, W = inten.size()\n        outten = torch.zeros(N, C, size[0], size[1]).cuda()\n        outten.requires_grad = False\n        margin_h, margin_w = size[0]-H, size[1]-W\n        hst, hed = margin_h//2, margin_h//2+H\n        wst, wed = margin_w//2, margin_w//2+W\n        outten[:, :, hst:hed, wst:wed] = inten\n        return outten, [hst, hed, wst, wed]\n\n\n    def eval_chip(self, crop):\n        with torch.no_grad():\n            out = self.net(crop)[0]\n            prob = F.softmax(out, 1)\n            if self.flip:\n                crop = torch.flip(crop, dims=(3,))\n                out = self.net(crop)[0]\n                out = torch.flip(out, dims=(3,))\n                prob += F.softmax(out, 1)\n            prob = torch.exp(prob)\n        return prob\n\n\n    def crop_eval(self, im):\n        cropsize = self.cropsize\n        stride_rate = 5/6.\n        N, C, H, W = im.size()\n        long_size, short_size = (H,W) if H>W else (W,H)\n        if long_size < cropsize:\n            im, indices = self.pad_tensor(im, (cropsize, cropsize))\n            prob = self.eval_chip(im)\n            prob = prob[:, :, indices[0]:indices[1], indices[2]:indices[3]]\n        else:\n            stride = math.ceil(cropsize*stride_rate)\n            if short_size < cropsize:\n                if H < W:\n                    im, indices = self.pad_tensor(im, (cropsize, W))\n                else:\n                    im, indices = self.pad_tensor(im, (H, cropsize))\n            N, C, H, W = im.size()\n            n_x = math.ceil((W-cropsize)/stride)+1\n            n_y = math.ceil((H-cropsize)/stride)+1\n            prob = torch.zeros(N, self.n_classes, H, W).cuda()\n            prob.requires_grad = False\n            for iy in range(n_y):\n                for ix in range(n_x):\n                    hed, wed = min(H, stride*iy+cropsize), min(W, stride*ix+cropsize)\n                    hst, wst = hed-cropsize, wed-cropsize\n                    chip = im[:, :, hst:hed, wst:wed]\n                    prob_chip = self.eval_chip(chip)\n                    prob[:, :, hst:hed, wst:wed] += prob_chip\n            if short_size < cropsize:\n                prob = prob[:, :, indices[0]:indices[1], indices[2]:indices[3]]\n        return prob\n\n\n    def scale_crop_eval(self, im, scale):\n        N, C, H, W = im.size()\n        new_hw = [int(H*scale), int(W*scale)]\n        im = F.interpolate(im, new_hw, mode=\'bilinear\', align_corners=True)\n        prob = self.crop_eval(im)\n        prob = F.interpolate(prob, (H, W), mode=\'bilinear\', align_corners=True)\n        return prob\n\n\n    def compute_hist(self, pred, lb):\n        n_classes = self.n_classes\n        ignore_idx = self.lb_ignore\n        keep = np.logical_not(lb==ignore_idx)\n        merge = pred[keep] * n_classes + lb[keep]\n        hist = np.bincount(merge, minlength=n_classes**2)\n        hist = hist.reshape((n_classes, n_classes))\n        return hist\n\n\n    def evaluate(self):\n        ## evaluate\n        n_classes = self.n_classes\n        hist = np.zeros((n_classes, n_classes), dtype=np.float32)\n        dloader = tqdm(self.dl)\n        if dist.is_initialized() and not dist.get_rank()==0:\n            dloader = self.dl\n        for i, (imgs, label) in enumerate(dloader):\n            N, _, H, W = label.shape\n            probs = torch.zeros((N, self.n_classes, H, W))\n            probs.requires_grad = False\n            imgs = imgs.cuda()\n            for sc in self.scales:\n                prob = self.scale_crop_eval(imgs, sc)\n                probs += prob.detach().cpu()\n            probs = probs.data.numpy()\n            preds = np.argmax(probs, axis=1)\n\n            hist_once = self.compute_hist(preds, label.data.numpy().squeeze(1))\n            hist = hist + hist_once\n        IOUs = np.diag(hist) / (np.sum(hist, axis=0)+np.sum(hist, axis=1)-np.diag(hist))\n        mIOU = np.mean(IOUs)\n        return mIOU\n\n\ndef evaluate(respth=\'./res\', dspth=\'./data\'):\n    ## logger\n    logger = logging.getLogger()\n\n    ## model\n    logger.info(\'\\n\')\n    logger.info(\'====\'*20)\n    logger.info(\'evaluating the model ...\\n\')\n    logger.info(\'setup and restore model\')\n    n_classes = 19\n    net = BiSeNet(n_classes=n_classes)\n    save_pth = osp.join(respth, \'model_final.pth\')\n    net.load_state_dict(torch.load(save_pth))\n    net.cuda()\n    net.eval()\n\n    ## dataset\n    batchsize = 5\n    n_workers = 2\n    dsval = CityScapes(dspth, mode=\'val\')\n    dl = DataLoader(dsval,\n                    batch_size = batchsize,\n                    shuffle = False,\n                    num_workers = n_workers,\n                    drop_last = False)\n\n    ## evaluator\n    logger.info(\'compute the mIOU\')\n    evaluator = MscEval(net, dl)\n\n    ## eval\n    mIOU = evaluator.evaluate()\n    logger.info(\'mIOU is: {:.6f}\'.format(mIOU))\n\n\n\nif __name__ == ""__main__"":\n    setup_logger(\'./res\')\n    evaluate()\n'"
logger.py,1,"b""#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport os.path as osp\nimport time\nimport sys\nimport logging\n\nimport torch.distributed as dist\n\n\ndef setup_logger(logpth):\n    logfile = 'BiSeNet-{}.log'.format(time.strftime('%Y-%m-%d-%H-%M-%S'))\n    logfile = osp.join(logpth, logfile)\n    FORMAT = '%(levelname)s %(filename)s(%(lineno)d): %(message)s'\n    log_level = logging.INFO\n    if dist.is_initialized() and not dist.get_rank()==0:\n        log_level = logging.ERROR\n    logging.basicConfig(level=log_level, format=FORMAT, filename=logfile)\n    logging.root.addHandler(logging.StreamHandler())\n\n\n"""
loss.py,10,"b""#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\n\n\nclass OhemCELoss(nn.Module):\n    def __init__(self, thresh, n_min, ignore_lb=255, *args, **kwargs):\n        super(OhemCELoss, self).__init__()\n        self.thresh = -torch.log(torch.tensor(thresh, dtype=torch.float)).cuda()\n        self.n_min = n_min\n        self.ignore_lb = ignore_lb\n        self.criteria = nn.CrossEntropyLoss(ignore_index=ignore_lb, reduction='none')\n\n    def forward(self, logits, labels):\n        N, C, H, W = logits.size()\n        loss = self.criteria(logits, labels).view(-1)\n        loss, _ = torch.sort(loss, descending=True)\n        if loss[self.n_min] > self.thresh:\n            loss = loss[loss>self.thresh]\n        else:\n            loss = loss[:self.n_min]\n        return torch.mean(loss)\n\n\nclass SoftmaxFocalLoss(nn.Module):\n    def __init__(self, gamma, ignore_lb=255, *args, **kwargs):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.nll = nn.NLLLoss(ignore_index=ignore_lb)\n\n    def forward(self, logits, labels):\n        scores = F.softmax(logits, dim=1)\n        factor = torch.pow(1.-scores, self.gamma)\n        log_score = F.log_softmax(logits, dim=1)\n        log_score = factor * log_score\n        loss = self.nll(log_score, labels)\n        return loss\n\n\nif __name__ == '__main__':\n    torch.manual_seed(15)\n    criteria1 = OhemCELoss(thresh=0.7, n_min=16*20*20//16).cuda()\n    criteria2 = OhemCELoss(thresh=0.7, n_min=16*20*20//16).cuda()\n    net1 = nn.Sequential(\n        nn.Conv2d(3, 19, kernel_size=3, stride=2, padding=1),\n    )\n    net1.cuda()\n    net1.train()\n    net2 = nn.Sequential(\n        nn.Conv2d(3, 19, kernel_size=3, stride=2, padding=1),\n    )\n    net2.cuda()\n    net2.train()\n\n    with torch.no_grad():\n        inten = torch.randn(16, 3, 20, 20).cuda()\n        lbs = torch.randint(0, 19, [16, 20, 20]).cuda()\n        lbs[1, :, :] = 255\n\n    logits1 = net1(inten)\n    logits1 = F.interpolate(logits1, inten.size()[2:], mode='bilinear')\n    logits2 = net2(inten)\n    logits2 = F.interpolate(logits2, inten.size()[2:], mode='bilinear')\n\n    loss1 = criteria1(logits1, lbs)\n    loss2 = criteria2(logits2, lbs)\n    loss = loss1 + loss2\n    print(loss.detach().cpu())\n    loss.backward()\n"""
model.py,6,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\n\nfrom resnet import Resnet18\nfrom modules.bn import InPlaceABNSync as BatchNorm2d\n\n\nclass ConvBNReLU(nn.Module):\n    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1, *args, **kwargs):\n        super(ConvBNReLU, self).__init__()\n        self.conv = nn.Conv2d(in_chan,\n                out_chan,\n                kernel_size = ks,\n                stride = stride,\n                padding = padding,\n                bias = False)\n        self.bn = BatchNorm2d(out_chan)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n\nclass BiSeNetOutput(nn.Module):\n    def __init__(self, in_chan, mid_chan, n_classes, *args, **kwargs):\n        super(BiSeNetOutput, self).__init__()\n        self.conv = ConvBNReLU(in_chan, mid_chan, ks=3, stride=1, padding=1)\n        self.conv_out = nn.Conv2d(mid_chan, n_classes, kernel_size=1, bias=False)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.conv_out(x)\n        return x\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass AttentionRefinementModule(nn.Module):\n    def __init__(self, in_chan, out_chan, *args, **kwargs):\n        super(AttentionRefinementModule, self).__init__()\n        self.conv = ConvBNReLU(in_chan, out_chan, ks=3, stride=1, padding=1)\n        self.conv_atten = nn.Conv2d(out_chan, out_chan, kernel_size= 1, bias=False)\n        self.bn_atten = BatchNorm2d(out_chan, activation=\'none\')\n        self.sigmoid_atten = nn.Sigmoid()\n        self.init_weight()\n\n    def forward(self, x):\n        feat = self.conv(x)\n        atten = F.avg_pool2d(feat, feat.size()[2:])\n        atten = self.conv_atten(atten)\n        atten = self.bn_atten(atten)\n        atten = self.sigmoid_atten(atten)\n        out = torch.mul(feat, atten)\n        return out\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n\nclass ContextPath(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(ContextPath, self).__init__()\n        self.resnet = Resnet18()\n        self.arm16 = AttentionRefinementModule(256, 128)\n        self.arm32 = AttentionRefinementModule(512, 128)\n        self.conv_head32 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n        self.conv_head16 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n        self.conv_avg = ConvBNReLU(512, 128, ks=1, stride=1, padding=0)\n\n        self.init_weight()\n\n    def forward(self, x):\n        H0, W0 = x.size()[2:]\n        feat8, feat16, feat32 = self.resnet(x)\n        H8, W8 = feat8.size()[2:]\n        H16, W16 = feat16.size()[2:]\n        H32, W32 = feat32.size()[2:]\n\n        avg = F.avg_pool2d(feat32, feat32.size()[2:])\n        avg = self.conv_avg(avg)\n        avg_up = F.interpolate(avg, (H32, W32), mode=\'nearest\')\n\n        feat32_arm = self.arm32(feat32)\n        feat32_sum = feat32_arm + avg_up\n        feat32_up = F.interpolate(feat32_sum, (H16, W16), mode=\'nearest\')\n        feat32_up = self.conv_head32(feat32_up)\n\n        feat16_arm = self.arm16(feat16)\n        feat16_sum = feat16_arm + feat32_up\n        feat16_up = F.interpolate(feat16_sum, (H8, W8), mode=\'nearest\')\n        feat16_up = self.conv_head16(feat16_up)\n\n        return feat16_up, feat32_up # x8, x16\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass SpatialPath(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(SpatialPath, self).__init__()\n        self.conv1 = ConvBNReLU(3, 64, ks=7, stride=2, padding=3)\n        self.conv2 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n        self.conv3 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n        self.conv_out = ConvBNReLU(64, 128, ks=1, stride=1, padding=0)\n        self.init_weight()\n\n    def forward(self, x):\n        feat = self.conv1(x)\n        feat = self.conv2(feat)\n        feat = self.conv3(feat)\n        feat = self.conv_out(feat)\n        return feat\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass FeatureFusionModule(nn.Module):\n    def __init__(self, in_chan, out_chan, *args, **kwargs):\n        super(FeatureFusionModule, self).__init__()\n        self.convblk = ConvBNReLU(in_chan, out_chan, ks=1, stride=1, padding=0)\n        self.conv1 = nn.Conv2d(out_chan,\n                out_chan//4,\n                kernel_size = 1,\n                stride = 1,\n                padding = 0,\n                bias = False)\n        self.conv2 = nn.Conv2d(out_chan//4,\n                out_chan,\n                kernel_size = 1,\n                stride = 1,\n                padding = 0,\n                bias = False)\n        self.relu = nn.ReLU(inplace=True)\n        self.sigmoid = nn.Sigmoid()\n        self.init_weight()\n\n    def forward(self, fsp, fcp):\n        fcat = torch.cat([fsp, fcp], dim=1)\n        feat = self.convblk(fcat)\n        atten = F.avg_pool2d(feat, feat.size()[2:])\n        atten = self.conv1(atten)\n        atten = self.relu(atten)\n        atten = self.conv2(atten)\n        atten = self.sigmoid(atten)\n        feat_atten = torch.mul(feat, atten)\n        feat_out = feat_atten + feat\n        return feat_out\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass BiSeNet(nn.Module):\n    def __init__(self, n_classes, *args, **kwargs):\n        super(BiSeNet, self).__init__()\n        self.cp = ContextPath()\n        self.sp = SpatialPath()\n        self.ffm = FeatureFusionModule(256, 256)\n        self.conv_out = BiSeNetOutput(256, 256, n_classes)\n        self.conv_out16 = BiSeNetOutput(128, 64, n_classes)\n        self.conv_out32 = BiSeNetOutput(128, 64, n_classes)\n        self.init_weight()\n\n    def forward(self, x):\n        H, W = x.size()[2:]\n        feat_cp8, feat_cp16 = self.cp(x)\n        feat_sp = self.sp(x)\n        feat_fuse = self.ffm(feat_sp, feat_cp8)\n\n        feat_out = self.conv_out(feat_fuse)\n        feat_out16 = self.conv_out16(feat_cp8)\n        feat_out32 = self.conv_out32(feat_cp16)\n\n        feat_out = F.interpolate(feat_out, (H, W), mode=\'bilinear\', align_corners=True)\n        feat_out16 = F.interpolate(feat_out16, (H, W), mode=\'bilinear\', align_corners=True)\n        feat_out32 = F.interpolate(feat_out32, (H, W), mode=\'bilinear\', align_corners=True)\n        return feat_out, feat_out16, feat_out32\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = [], [], [], []\n        for name, child in self.named_children():\n            child_wd_params, child_nowd_params = child.get_params()\n            if isinstance(child, (FeatureFusionModule, BiSeNetOutput)):\n                lr_mul_wd_params += child_wd_params\n                lr_mul_nowd_params += child_nowd_params\n            else:\n                wd_params += child_wd_params\n                nowd_params += child_nowd_params\n        return wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params\n\n\nif __name__ == ""__main__"":\n    net = BiSeNet(19)\n    net.cuda()\n    net.eval()\n    in_ten = torch.randn(16, 3, 640, 480).cuda()\n    out, out16, out32 = net(in_ten)\n    print(out.shape)\n\n    net.get_params()\n'"
optimizer.py,1,"b""#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport torch\nimport logging\n\nlogger = logging.getLogger()\n\nclass Optimizer(object):\n    def __init__(self,\n                model,\n                lr0,\n                momentum,\n                wd,\n                warmup_steps,\n                warmup_start_lr,\n                max_iter,\n                power,\n                *args, **kwargs):\n        self.warmup_steps = warmup_steps\n        self.warmup_start_lr = warmup_start_lr\n        self.lr0 = lr0\n        self.lr = self.lr0\n        self.max_iter = float(max_iter)\n        self.power = power\n        self.it = 0\n        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = model.get_params()\n        param_list = [\n                {'params': wd_params},\n                {'params': nowd_params, 'weight_decay': 0},\n                {'params': lr_mul_wd_params, 'lr_mul': True},\n                {'params': lr_mul_nowd_params, 'weight_decay': 0, 'lr_mul': True}]\n        self.optim = torch.optim.SGD(\n                param_list,\n                lr = lr0,\n                momentum = momentum,\n                weight_decay = wd)\n        self.warmup_factor = (self.lr0/self.warmup_start_lr)**(1./self.warmup_steps)\n\n\n    def get_lr(self):\n        if self.it <= self.warmup_steps:\n            lr = self.warmup_start_lr*(self.warmup_factor**self.it)\n        else:\n            factor = (1-(self.it-self.warmup_steps)/(self.max_iter-self.warmup_steps))**self.power\n            lr = self.lr0 * factor\n        return lr\n\n\n    def step(self):\n        self.lr = self.get_lr()\n        for pg in self.optim.param_groups:\n            if pg.get('lr_mul', False):\n                pg['lr'] = self.lr * 10\n            else:\n                pg['lr'] = self.lr\n        if self.optim.defaults.get('lr_mul', False):\n            self.optim.defaults['lr'] = self.lr * 10\n        else:\n            self.optim.defaults['lr'] = self.lr\n        self.it += 1\n        self.optim.step()\n        if self.it == self.warmup_steps+2:\n            logger.info('==> warmup done, start to implement poly lr strategy')\n\n    def zero_grad(self):\n        self.optim.zero_grad()\n\n"""
resnet.py,5,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as modelzoo\n\nfrom modules.bn import InPlaceABNSync as BatchNorm2d\n\nresnet18_url = \'https://download.pytorch.org/models/resnet18-5c106cde.pth\'\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_chan, out_chan, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(in_chan, out_chan, stride)\n        self.bn1 = BatchNorm2d(out_chan)\n        self.conv2 = conv3x3(out_chan, out_chan)\n        self.bn2 = BatchNorm2d(out_chan, activation=\'none\')\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = None\n        if in_chan != out_chan or stride != 1:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_chan, out_chan,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm2d(out_chan, activation=\'none\'),\n                )\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = self.bn1(residual)\n        residual = self.conv2(residual)\n        residual = self.bn2(residual)\n\n        shortcut = x\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n\n        out = shortcut + residual\n        out = self.relu(out)\n        return out\n\n\ndef create_layer_basic(in_chan, out_chan, bnum, stride=1):\n    layers = [BasicBlock(in_chan, out_chan, stride=stride)]\n    for i in range(bnum-1):\n        layers.append(BasicBlock(out_chan, out_chan, stride=1))\n    return nn.Sequential(*layers)\n\n\nclass Resnet18(nn.Module):\n    def __init__(self):\n        super(Resnet18, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = BatchNorm2d(64)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = create_layer_basic(64, 64, bnum=2, stride=1)\n        self.layer2 = create_layer_basic(64, 128, bnum=2, stride=2)\n        self.layer3 = create_layer_basic(128, 256, bnum=2, stride=2)\n        self.layer4 = create_layer_basic(256, 512, bnum=2, stride=2)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        feat8 = self.layer2(x) # 1/8\n        feat16 = self.layer3(feat8) # 1/16\n        feat32 = self.layer4(feat16) # 1/32\n        return feat8, feat16, feat32\n\n    def init_weight(self):\n        state_dict = modelzoo.load_url(resnet18_url)\n        self_state_dict = self.state_dict()\n        for k, v in state_dict.items():\n            if \'fc\' in k: continue\n            self_state_dict.update({k: v})\n        self.load_state_dict(self_state_dict)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, (BatchNorm2d, nn.BatchNorm2d)):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nif __name__ == ""__main__"":\n    net = Resnet18()\n    x = torch.randn(16, 3, 224, 224)\n    out = net(x)\n    print(out[0].size())\n    print(out[1].size())\n    print(out[2].size())\n    net.get_params()\n'"
train.py,10,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nfrom logger import setup_logger\nfrom model import BiSeNet\nfrom cityscapes import CityScapes\nfrom loss import OhemCELoss\nfrom evaluate import evaluate\nfrom optimizer import Optimizer\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torch.distributed as dist\n\nimport os\nimport os.path as osp\nimport logging\nimport time\nimport datetime\nimport argparse\n\n\nrespth = \'./res\'\nif not osp.exists(respth): os.makedirs(respth)\nlogger = logging.getLogger()\n\n\ndef parse_args():\n    parse = argparse.ArgumentParser()\n    parse.add_argument(\n            \'--local_rank\',\n            dest = \'local_rank\',\n            type = int,\n            default = -1,\n            )\n    parse.add_argument(\n            \'--ckpt\',\n            dest = \'ckpt\',\n            type = str,\n            default = None,\n            )\n    return parse.parse_args()\n\n\ndef train():\n    args = parse_args()\n    torch.cuda.set_device(args.local_rank)\n    dist.init_process_group(\n                backend = \'nccl\',\n                init_method = \'tcp://127.0.0.1:33271\',\n                world_size = torch.cuda.device_count(),\n                rank=args.local_rank\n                )\n    setup_logger(respth)\n\n    ## dataset\n    n_classes = 19\n    n_img_per_gpu = 8\n    n_workers = 4\n    cropsize = [1024, 1024]\n    ds = CityScapes(\'./data\', cropsize=cropsize, mode=\'train\')\n    sampler = torch.utils.data.distributed.DistributedSampler(ds)\n    dl = DataLoader(ds,\n                    batch_size = n_img_per_gpu,\n                    shuffle = False,\n                    sampler = sampler,\n                    num_workers = n_workers,\n                    pin_memory = True,\n                    drop_last = True)\n\n    ## model\n    ignore_idx = 255\n    net = BiSeNet(n_classes=n_classes)\n    if not args.ckpt is None:\n        net.load_state_dict(torch.load(args.ckpt, map_location=\'cpu\'))\n    net.cuda()\n    net.train()\n    net = nn.parallel.DistributedDataParallel(net,\n            device_ids = [args.local_rank, ],\n            output_device = args.local_rank\n            )\n    score_thres = 0.7\n    n_min = n_img_per_gpu*cropsize[0]*cropsize[1]//16\n    criteria_p = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n    criteria_16 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n    criteria_32 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n\n    ## optimizer\n    momentum = 0.9\n    weight_decay = 5e-4\n    lr_start = 1e-2\n    max_iter = 80000\n    power = 0.9\n    warmup_steps = 1000\n    warmup_start_lr = 1e-5\n    optim = Optimizer(\n            model = net.module,\n            lr0 = lr_start,\n            momentum = momentum,\n            wd = weight_decay,\n            warmup_steps = warmup_steps,\n            warmup_start_lr = warmup_start_lr,\n            max_iter = max_iter,\n            power = power)\n\n    ## train loop\n    msg_iter = 50\n    loss_avg = []\n    st = glob_st = time.time()\n    diter = iter(dl)\n    epoch = 0\n    for it in range(max_iter):\n        try:\n            im, lb = next(diter)\n            if not im.size()[0]==n_img_per_gpu: raise StopIteration\n        except StopIteration:\n            epoch += 1\n            sampler.set_epoch(epoch)\n            diter = iter(dl)\n            im, lb = next(diter)\n        im = im.cuda()\n        lb = lb.cuda()\n        H, W = im.size()[2:]\n        lb = torch.squeeze(lb, 1)\n\n        optim.zero_grad()\n        out, out16, out32 = net(im)\n        lossp = criteria_p(out, lb)\n        loss2 = criteria_16(out16, lb)\n        loss3 = criteria_32(out32, lb)\n        loss = lossp + loss2 + loss3\n        loss.backward()\n        optim.step()\n\n        loss_avg.append(loss.item())\n        ## print training log message\n        if (it+1)%msg_iter==0:\n            loss_avg = sum(loss_avg) / len(loss_avg)\n            lr = optim.lr\n            ed = time.time()\n            t_intv, glob_t_intv = ed - st, ed - glob_st\n            eta = int((max_iter - it) * (glob_t_intv / it))\n            eta = str(datetime.timedelta(seconds=eta))\n            msg = \', \'.join([\n                    \'it: {it}/{max_it}\',\n                    \'lr: {lr:4f}\',\n                    \'loss: {loss:.4f}\',\n                    \'eta: {eta}\',\n                    \'time: {time:.4f}\',\n                ]).format(\n                    it = it+1,\n                    max_it = max_iter,\n                    lr = lr,\n                    loss = loss_avg,\n                    time = t_intv,\n                    eta = eta\n                )\n            logger.info(msg)\n            loss_avg = []\n            st = ed\n\n    ## dump the final model\n    save_pth = osp.join(respth, \'model_final.pth\')\n    net.cpu()\n    state = net.module.state_dict() if hasattr(net, \'module\') else net.state_dict()\n    if dist.get_rank()==0: torch.save(state, save_pth)\n    logger.info(\'training done, model saved to: {}\'.format(save_pth))\n\n\nif __name__ == ""__main__"":\n    train()\n    evaluate()\n'"
transform.py,0,"b""#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nfrom PIL import Image\nimport PIL.ImageEnhance as ImageEnhance\nimport random\n\n\nclass RandomCrop(object):\n    def __init__(self, size, *args, **kwargs):\n        self.size = size\n\n    def __call__(self, im_lb):\n        im = im_lb['im']\n        lb = im_lb['lb']\n        assert im.size == lb.size\n        W, H = self.size\n        w, h = im.size\n\n        if (W, H) == (w, h): return dict(im=im, lb=lb)\n        if w < W or h < H:\n            scale = float(W) / w if w < h else float(H) / h\n            w, h = int(scale * w + 1), int(scale * h + 1)\n            im = im.resize((w, h), Image.BILINEAR)\n            lb = lb.resize((w, h), Image.NEAREST)\n        sw, sh = random.random() * (w - W), random.random() * (h - H)\n        crop = int(sw), int(sh), int(sw) + W, int(sh) + H\n        return dict(\n                im = im.crop(crop),\n                lb = lb.crop(crop)\n                    )\n\n\nclass HorizontalFlip(object):\n    def __init__(self, p=0.5, *args, **kwargs):\n        self.p = p\n\n    def __call__(self, im_lb):\n        if random.random() > self.p:\n            return im_lb\n        else:\n            im = im_lb['im']\n            lb = im_lb['lb']\n            return dict(im = im.transpose(Image.FLIP_LEFT_RIGHT),\n                        lb = lb.transpose(Image.FLIP_LEFT_RIGHT),\n                    )\n\n\nclass RandomScale(object):\n    def __init__(self, scales=(1, ), *args, **kwargs):\n        self.scales = scales\n\n    def __call__(self, im_lb):\n        im = im_lb['im']\n        lb = im_lb['lb']\n        W, H = im.size\n        scale = random.choice(self.scales)\n        w, h = int(W * scale), int(H * scale)\n        return dict(im = im.resize((w, h), Image.BILINEAR),\n                    lb = lb.resize((w, h), Image.NEAREST),\n                )\n\n\nclass ColorJitter(object):\n    def __init__(self, brightness=None, contrast=None, saturation=None, *args, **kwargs):\n        if not brightness is None and brightness>0:\n            self.brightness = [max(1-brightness, 0), 1+brightness]\n        if not contrast is None and contrast>0:\n            self.contrast = [max(1-contrast, 0), 1+contrast]\n        if not saturation is None and saturation>0:\n            self.saturation = [max(1-saturation, 0), 1+saturation]\n\n    def __call__(self, im_lb):\n        im = im_lb['im']\n        lb = im_lb['lb']\n        r_brightness = random.uniform(self.brightness[0], self.brightness[1])\n        r_contrast = random.uniform(self.contrast[0], self.contrast[1])\n        r_saturation = random.uniform(self.saturation[0], self.saturation[1])\n        im = ImageEnhance.Brightness(im).enhance(r_brightness)\n        im = ImageEnhance.Contrast(im).enhance(r_contrast)\n        im = ImageEnhance.Color(im).enhance(r_saturation)\n        return dict(im = im,\n                    lb = lb,\n                )\n\n\nclass MultiScale(object):\n    def __init__(self, scales):\n        self.scales = scales\n\n    def __call__(self, img):\n        W, H = img.size\n        sizes = [(int(W*ratio), int(H*ratio)) for ratio in self.scales]\n        imgs = []\n        [imgs.append(img.resize(size, Image.BILINEAR)) for size in sizes]\n        return imgs\n\n\nclass Compose(object):\n    def __init__(self, do_list):\n        self.do_list = do_list\n\n    def __call__(self, im_lb):\n        for comp in self.do_list:\n            im_lb = comp(im_lb)\n        return im_lb\n\n\n\n\nif __name__ == '__main__':\n    flip = HorizontalFlip(p = 1)\n    crop = RandomCrop((321, 321))\n    rscales = RandomScale((0.75, 1.0, 1.5, 1.75, 2.0))\n    img = Image.open('data/img.jpg')\n    lb = Image.open('data/label.png')\n"""
bisenetv2/__init__.py,0,b''
bisenetv2/bisenetv2.py,16,"b'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ConvBNReLU(nn.Module):\n\n    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1,\n                 dilation=1, groups=1, bias=False):\n        super(ConvBNReLU, self).__init__()\n        self.conv = nn.Conv2d(\n                in_chan, out_chan, kernel_size=ks, stride=stride,\n                padding=padding, dilation=dilation,\n                groups=groups, bias=bias)\n        self.bn = nn.BatchNorm2d(out_chan)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        feat = self.conv(x)\n        feat = self.bn(feat)\n        feat = self.relu(feat)\n        return feat\n\n\nclass DetailBranch(nn.Module):\n\n    def __init__(self):\n        super(DetailBranch, self).__init__()\n        self.S1 = nn.Sequential(\n            ConvBNReLU(3, 64, 3, stride=2),\n            ConvBNReLU(64, 64, 3, stride=1),\n        )\n        self.S2 = nn.Sequential(\n            ConvBNReLU(64, 64, 3, stride=2),\n            ConvBNReLU(64, 64, 3, stride=1),\n            ConvBNReLU(64, 64, 3, stride=1),\n        )\n        self.S3 = nn.Sequential(\n            ConvBNReLU(64, 128, 3, stride=2),\n            ConvBNReLU(128, 128, 3, stride=1),\n            ConvBNReLU(128, 128, 3, stride=1),\n        )\n\n    def forward(self, x):\n        feat = self.S1(x)\n        feat = self.S2(feat)\n        feat = self.S3(feat)\n        return feat\n\n\nclass StemBlock(nn.Module):\n\n    def __init__(self):\n        super(StemBlock, self).__init__()\n        self.conv = ConvBNReLU(3, 16, 3, stride=2)\n        self.left = nn.Sequential(\n            ConvBNReLU(16, 8, 1, stride=1, padding=0),\n            ConvBNReLU(8, 16, 3, stride=2),\n        )\n        self.right = nn.MaxPool2d(\n            kernel_size=3, stride=2, padding=1, ceil_mode=False)\n        self.fuse = ConvBNReLU(32, 16, 3, stride=1)\n\n    def forward(self, x):\n        feat = self.conv(x)\n        feat_left = self.left(feat)\n        feat_right = self.right(feat)\n        feat = torch.cat([feat_left, feat_right], dim=1)\n        feat = self.fuse(feat)\n        return feat\n\n\nclass CEBlock(nn.Module):\n\n    def __init__(self):\n        super(CEBlock, self).__init__()\n        self.bn = nn.BatchNorm2d(128)\n        self.conv_gap = ConvBNReLU(128, 128, 1, stride=1, padding=0)\n        #TODO: in paper here is naive conv2d, no bn-relu\n        self.conv_last = ConvBNReLU(128, 128, 3, stride=1)\n\n    def forward(self, x):\n        feat = torch.mean(x, dim=(2, 3), keepdim=True)\n        feat = self.bn(feat)\n        feat = self.conv_gap(feat)\n        feat = feat + x\n        feat = self.conv_last(feat)\n        return feat\n\n\nclass GELayerS1(nn.Module):\n\n    def __init__(self, in_chan, out_chan, exp_ratio=6):\n        super(GELayerS1, self).__init__()\n        mid_chan = in_chan * exp_ratio\n        self.conv1 = ConvBNReLU(in_chan, in_chan, 3, stride=1)\n        self.dwconv = nn.Sequential(\n            nn.Conv2d(\n                in_chan, mid_chan, kernel_size=3, stride=1,\n                padding=1, groups=in_chan, bias=False),\n            nn.BatchNorm2d(mid_chan),\n            nn.ReLU(inplace=True), # not shown in paper\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(\n                mid_chan, out_chan, kernel_size=1, stride=1,\n                padding=0, bias=False),\n            nn.BatchNorm2d(out_chan),\n        )\n        self.conv2[1].last_bn = True\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        feat = self.conv1(x)\n        feat = self.dwconv(feat)\n        feat = self.conv2(feat)\n        feat = feat + x\n        feat = self.relu(feat)\n        return feat\n\n\nclass GELayerS2(nn.Module):\n\n    def __init__(self, in_chan, out_chan, exp_ratio=6):\n        super(GELayerS2, self).__init__()\n        mid_chan = in_chan * exp_ratio\n        self.conv1 = ConvBNReLU(in_chan, in_chan, 3, stride=1)\n        self.dwconv1 = nn.Sequential(\n            nn.Conv2d(\n                in_chan, mid_chan, kernel_size=3, stride=2,\n                padding=1, groups=in_chan, bias=False),\n            nn.BatchNorm2d(mid_chan),\n        )\n        self.dwconv2 = nn.Sequential(\n            nn.Conv2d(\n                mid_chan, mid_chan, kernel_size=3, stride=1,\n                padding=1, groups=mid_chan, bias=False),\n            nn.BatchNorm2d(mid_chan),\n            nn.ReLU(inplace=True), # not shown in paper\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(\n                mid_chan, out_chan, kernel_size=1, stride=1,\n                padding=0, bias=False),\n            nn.BatchNorm2d(out_chan),\n        )\n        self.conv2[1].last_bn = True\n        self.shortcut = nn.Sequential(\n                nn.Conv2d(\n                    in_chan, in_chan, kernel_size=3, stride=2,\n                    padding=1, groups=in_chan, bias=False),\n                nn.BatchNorm2d(in_chan),\n                nn.Conv2d(\n                    in_chan, out_chan, kernel_size=1, stride=1,\n                    padding=0, bias=False),\n                nn.BatchNorm2d(out_chan),\n        )\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        feat = self.conv1(x)\n        feat = self.dwconv1(feat)\n        feat = self.dwconv2(feat)\n        feat = self.conv2(feat)\n        shortcut = self.shortcut(x)\n        feat = feat + shortcut\n        feat = self.relu(feat)\n        return feat\n\n\nclass SegmentBranch(nn.Module):\n\n    def __init__(self):\n        super(SegmentBranch, self).__init__()\n        self.S1S2 = StemBlock()\n        self.S3 = nn.Sequential(\n            GELayerS2(16, 32),\n            GELayerS1(32, 32),\n        )\n        self.S4 = nn.Sequential(\n            GELayerS2(32, 64),\n            GELayerS1(64, 64),\n        )\n        self.S5_4 = nn.Sequential(\n            GELayerS2(64, 128),\n            GELayerS1(128, 128),\n            GELayerS1(128, 128),\n            GELayerS1(128, 128),\n        )\n        self.S5_5 = CEBlock()\n\n    def forward(self, x):\n        feat2 = self.S1S2(x)\n        feat3 = self.S3(feat2)\n        feat4 = self.S4(feat3)\n        feat5_4 = self.S5_4(feat4)\n        feat5_5 = self.S5_5(feat5_4)\n        return feat2, feat3, feat4, feat5_4, feat5_5\n\n\nclass BGALayer(nn.Module):\n\n    def __init__(self):\n        super(BGALayer, self).__init__()\n        self.left1 = nn.Sequential(\n            nn.Conv2d(\n                128, 128, kernel_size=3, stride=1,\n                padding=1, groups=128, bias=False),\n            nn.BatchNorm2d(128),\n            nn.Conv2d(\n                128, 128, kernel_size=1, stride=1,\n                padding=0, bias=False),\n        )\n        self.left2 = nn.Sequential(\n            nn.Conv2d(\n                128, 128, kernel_size=3, stride=2,\n                padding=1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.AvgPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=False)\n        )\n        self.right1 = nn.Sequential(\n            nn.Conv2d(\n                128, 128, kernel_size=3, stride=1,\n                padding=1, bias=False),\n            nn.BatchNorm2d(128),\n        )\n        self.right2 = nn.Sequential(\n            nn.Conv2d(\n                128, 128, kernel_size=3, stride=1,\n                padding=1, groups=128, bias=False),\n            nn.BatchNorm2d(128),\n            nn.Conv2d(\n                128, 128, kernel_size=1, stride=1,\n                padding=0, bias=False),\n        )\n        ##TODO: does this really has no relu?\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                128, 128, kernel_size=3, stride=1,\n                padding=1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True), # not shown in paper\n        )\n\n    def forward(self, x_d, x_s):\n        dsize = x_d.size()[2:]\n        left1 = self.left1(x_d)\n        left2 = self.left2(x_d)\n        right1 = self.right1(x_s)\n        right2 = self.right2(x_s)\n        right1 = F.interpolate(\n            right1, size=dsize, mode=\'bilinear\', align_corners=True)\n        left = left1 * torch.sigmoid(right1)\n        right = left2 * torch.sigmoid(right2)\n        right = F.interpolate(\n            right, size=dsize, mode=\'bilinear\', align_corners=True)\n        out = self.conv(left + right)\n        return out\n\n\n\nclass SegmentHead(nn.Module):\n\n    def __init__(self, in_chan, mid_chan, n_classes):\n        super(SegmentHead, self).__init__()\n        self.conv = ConvBNReLU(in_chan, mid_chan, 3, stride=1)\n        self.drop = nn.Dropout(0.1)\n        self.conv_out = nn.Conv2d(\n                mid_chan, n_classes, kernel_size=1, stride=1,\n                padding=0, bias=True)\n\n    def forward(self, x, size=None):\n        feat = self.conv(x)\n        feat = self.drop(feat)\n        feat = self.conv_out(feat)\n        if not size is None:\n            feat = F.interpolate(feat, size=size,\n                mode=\'bilinear\', align_corners=True)\n        return feat\n\n\nclass BiSeNetV2(nn.Module):\n\n    def __init__(self, n_classes):\n        super(BiSeNetV2, self).__init__()\n        self.detail = DetailBranch()\n        self.segment = SegmentBranch()\n        self.bga = BGALayer()\n\n        ## TODO: what is the number of mid chan ?\n        self.head = SegmentHead(128, 1024, n_classes)\n        self.aux2 = SegmentHead(16, 128, n_classes)\n        self.aux3 = SegmentHead(32, 128, n_classes)\n        self.aux4 = SegmentHead(64, 128, n_classes)\n        self.aux5_4 = SegmentHead(128, 128, n_classes)\n\n        self.init_weights()\n\n    def forward(self, x):\n        size = x.size()[2:]\n        feat_d = self.detail(x)\n        feat2, feat3, feat4, feat5_4, feat_s = self.segment(x)\n        feat_head = self.bga(feat_d, feat_s)\n\n        logits = self.head(feat_head, size)\n        logits_aux2 = self.aux2(feat2, size)\n        logits_aux3 = self.aux3(feat3, size)\n        logits_aux4 = self.aux4(feat4, size)\n        logits_aux5_4 = self.aux5_4(feat5_4, size)\n        return logits, logits_aux2, logits_aux3, logits_aux4, logits_aux5_4\n\n    def init_weights(self):\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Conv2d, nn.Linear)):\n                nn.init.kaiming_normal_(module.weight, mode=\'fan_out\')\n                if not module.bias is None: nn.init.constant_(module.bias, 0)\n            elif isinstance(module, nn.modules.batchnorm._BatchNorm):\n                if hasattr(module, \'last_bn\') and module.last_bn:\n                    nn.init.zeros_(module.weight)\n                else:\n                    nn.init.ones_(module.weight)\n                nn.init.zeros_(module.bias)\n\n\nif __name__ == ""__main__"":\n    #  x = torch.randn(16, 3, 1024, 2048)\n    #  detail = DetailBranch()\n    #  feat = detail(x)\n    #  print(\'detail\', feat.size())\n    #\n    #  x = torch.randn(16, 3, 1024, 2048)\n    #  stem = StemBlock()\n    #  feat = stem(x)\n    #  print(\'stem\', feat.size())\n    #\n    #  x = torch.randn(16, 128, 16, 32)\n    #  ceb = CEBlock()\n    #  feat = ceb(x)\n    #  print(feat.size())\n    #\n    #  x = torch.randn(16, 32, 16, 32)\n    #  ge1 = GELayerS1(32, 32)\n    #  feat = ge1(x)\n    #  print(feat.size())\n    #\n    #  x = torch.randn(16, 16, 16, 32)\n    #  ge2 = GELayerS2(16, 32)\n    #  feat = ge2(x)\n    #  print(feat.size())\n    #\n    #  left = torch.randn(16, 128, 64, 128)\n    #  right = torch.randn(16, 128, 16, 32)\n    #  bga = BGALayer()\n    #  feat = bga(left, right)\n    #  print(feat.size())\n    #\n    #  x = torch.randn(16, 128, 64, 128)\n    #  head = SegmentHead(128, 128, 19)\n    #  logits = head(x)\n    #  print(logits.size())\n    #\n    #  x = torch.randn(16, 3, 1024, 2048)\n    #  segment = SegmentBranch()\n    #  feat = segment(x)[0]\n    #  print(feat.size())\n    #\n    x = torch.randn(16, 3, 512, 1024)\n    model = BiSeNetV2(n_classes=19)\n    logits = model(x)[0]\n    print(logits.size())\n\n    for name, param in model.named_parameters():\n        if len(param.size()) == 1:\n            print(name)\n'"
bisenetv2/cityscapes_cv2.py,5,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nimport os\nimport os.path as osp\nimport json\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.distributed as dist\nimport cv2\nimport numpy as np\n\nimport bisenetv2.transform_cv2 as T\nfrom sampler import RepeatedDistSampler\n\n\nlabels_info = [\n    {""hasInstances"": False, ""category"": ""void"", ""catid"": 0, ""name"": ""unlabeled"", ""ignoreInEval"": True, ""id"": 0, ""color"": [0, 0, 0], ""trainId"": 255},\n    {""hasInstances"": False, ""category"": ""void"", ""catid"": 0, ""name"": ""ego vehicle"", ""ignoreInEval"": True, ""id"": 1, ""color"": [0, 0, 0], ""trainId"": 255},\n    {""hasInstances"": False, ""category"": ""void"", ""catid"": 0, ""name"": ""rectification border"", ""ignoreInEval"": True, ""id"": 2, ""color"": [0, 0, 0], ""trainId"": 255},\n    {""hasInstances"": False, ""category"": ""void"", ""catid"": 0, ""name"": ""out of roi"", ""ignoreInEval"": True, ""id"": 3, ""color"": [0, 0, 0], ""trainId"": 255},\n    {""hasInstances"": False, ""category"": ""void"", ""catid"": 0, ""name"": ""static"", ""ignoreInEval"": True, ""id"": 4, ""color"": [0, 0, 0], ""trainId"": 255},\n    {""hasInstances"": False, ""category"": ""void"", ""catid"": 0, ""name"": ""dynamic"", ""ignoreInEval"": True, ""id"": 5, ""color"": [111, 74, 0], ""trainId"": 255},\n    {""hasInstances"": False, ""category"": ""void"", ""catid"": 0, ""name"": ""ground"", ""ignoreInEval"": True, ""id"": 6, ""color"": [81, 0, 81], ""trainId"": 255},\n    {""hasInstances"": False, ""category"": ""flat"", ""catid"": 1, ""name"": ""road"", ""ignoreInEval"": False, ""id"": 7, ""color"": [128, 64, 128], ""trainId"": 0},\n    {""hasInstances"": False, ""category"": ""flat"", ""catid"": 1, ""name"": ""sidewalk"", ""ignoreInEval"": False, ""id"": 8, ""color"": [244, 35, 232], ""trainId"": 1},\n    {""hasInstances"": False, ""category"": ""flat"", ""catid"": 1, ""name"": ""parking"", ""ignoreInEval"": True, ""id"": 9, ""color"": [250, 170, 160], ""trainId"": 255},\n    {""hasInstances"": False, ""category"": ""flat"", ""catid"": 1, ""name"": ""rail track"", ""ignoreInEval"": True, ""id"": 10, ""color"": [230, 150, 140], ""trainId"": 255},\n    {""hasInstances"": False, ""category"": ""construction"", ""catid"": 2, ""name"": ""building"", ""ignoreInEval"": False, ""id"": 11, ""color"": [70, 70, 70], ""trainId"": 2},\n    {""hasInstances"": False, ""category"": ""construction"", ""catid"": 2, ""name"": ""wall"", ""ignoreInEval"": False, ""id"": 12, ""color"": [102, 102, 156], ""trainId"": 3},\n    {""hasInstances"": False, ""category"": ""construction"", ""catid"": 2, ""name"": ""fence"", ""ignoreInEval"": False, ""id"": 13, ""color"": [190, 153, 153], ""trainId"": 4},\n    {""hasInstances"": False, ""category"": ""construction"", ""catid"": 2, ""name"": ""guard rail"", ""ignoreInEval"": True, ""id"": 14, ""color"": [180, 165, 180], ""trainId"": 255},\n    {""hasInstances"": False, ""category"": ""construction"", ""catid"": 2, ""name"": ""bridge"", ""ignoreInEval"": True, ""id"": 15, ""color"": [150, 100, 100], ""trainId"": 255},\n    {""hasInstances"": False, ""category"": ""construction"", ""catid"": 2, ""name"": ""tunnel"", ""ignoreInEval"": True, ""id"": 16, ""color"": [150, 120, 90], ""trainId"": 255},\n    {""hasInstances"": False, ""category"": ""object"", ""catid"": 3, ""name"": ""pole"", ""ignoreInEval"": False, ""id"": 17, ""color"": [153, 153, 153], ""trainId"": 5},\n    {""hasInstances"": False, ""category"": ""object"", ""catid"": 3, ""name"": ""polegroup"", ""ignoreInEval"": True, ""id"": 18, ""color"": [153, 153, 153], ""trainId"": 255},\n    {""hasInstances"": False, ""category"": ""object"", ""catid"": 3, ""name"": ""traffic light"", ""ignoreInEval"": False, ""id"": 19, ""color"": [250, 170, 30], ""trainId"": 6},\n    {""hasInstances"": False, ""category"": ""object"", ""catid"": 3, ""name"": ""traffic sign"", ""ignoreInEval"": False, ""id"": 20, ""color"": [220, 220, 0], ""trainId"": 7},\n    {""hasInstances"": False, ""category"": ""nature"", ""catid"": 4, ""name"": ""vegetation"", ""ignoreInEval"": False, ""id"": 21, ""color"": [107, 142, 35], ""trainId"": 8},\n    {""hasInstances"": False, ""category"": ""nature"", ""catid"": 4, ""name"": ""terrain"", ""ignoreInEval"": False, ""id"": 22, ""color"": [152, 251, 152], ""trainId"": 9},\n    {""hasInstances"": False, ""category"": ""sky"", ""catid"": 5, ""name"": ""sky"", ""ignoreInEval"": False, ""id"": 23, ""color"": [70, 130, 180], ""trainId"": 10},\n    {""hasInstances"": True, ""category"": ""human"", ""catid"": 6, ""name"": ""person"", ""ignoreInEval"": False, ""id"": 24, ""color"": [220, 20, 60], ""trainId"": 11},\n    {""hasInstances"": True, ""category"": ""human"", ""catid"": 6, ""name"": ""rider"", ""ignoreInEval"": False, ""id"": 25, ""color"": [255, 0, 0], ""trainId"": 12},\n    {""hasInstances"": True, ""category"": ""vehicle"", ""catid"": 7, ""name"": ""car"", ""ignoreInEval"": False, ""id"": 26, ""color"": [0, 0, 142], ""trainId"": 13},\n    {""hasInstances"": True, ""category"": ""vehicle"", ""catid"": 7, ""name"": ""truck"", ""ignoreInEval"": False, ""id"": 27, ""color"": [0, 0, 70], ""trainId"": 14},\n    {""hasInstances"": True, ""category"": ""vehicle"", ""catid"": 7, ""name"": ""bus"", ""ignoreInEval"": False, ""id"": 28, ""color"": [0, 60, 100], ""trainId"": 15},\n    {""hasInstances"": True, ""category"": ""vehicle"", ""catid"": 7, ""name"": ""caravan"", ""ignoreInEval"": True, ""id"": 29, ""color"": [0, 0, 90], ""trainId"": 255},\n    {""hasInstances"": True, ""category"": ""vehicle"", ""catid"": 7, ""name"": ""trailer"", ""ignoreInEval"": True, ""id"": 30, ""color"": [0, 0, 110], ""trainId"": 255},\n    {""hasInstances"": True, ""category"": ""vehicle"", ""catid"": 7, ""name"": ""train"", ""ignoreInEval"": False, ""id"": 31, ""color"": [0, 80, 100], ""trainId"": 16},\n    {""hasInstances"": True, ""category"": ""vehicle"", ""catid"": 7, ""name"": ""motorcycle"", ""ignoreInEval"": False, ""id"": 32, ""color"": [0, 0, 230], ""trainId"": 17},\n    {""hasInstances"": True, ""category"": ""vehicle"", ""catid"": 7, ""name"": ""bicycle"", ""ignoreInEval"": False, ""id"": 33, ""color"": [119, 11, 32], ""trainId"": 18},\n    {""hasInstances"": False, ""category"": ""vehicle"", ""catid"": 7, ""name"": ""license plate"", ""ignoreInEval"": True, ""id"": -1, ""color"": [0, 0, 142], ""trainId"": -1}\n]\n\n\nclass CityScapes(Dataset):\n    \'\'\'\n    \'\'\'\n    def __init__(self, datapth, trans_func=None, mode=\'train\'):\n        super(CityScapes, self).__init__()\n        assert mode in (\'train\', \'val\', \'test\')\n        self.mode = mode\n        self.trans_func = trans_func\n        self.n_cats = 19\n        self.lb_ignore = 255\n\n        self.lb_map = np.arange(256).astype(np.uint8)\n        for el in labels_info:\n            self.lb_map[el[\'id\']] = el[\'trainId\']\n\n        ## parse img directory\n        self.imgs = {}\n        imgnames = []\n        impth = osp.join(datapth, \'leftImg8bit\', mode)\n        folders = os.listdir(impth)\n        for fd in folders:\n            fdpth = osp.join(impth, fd)\n            im_names = os.listdir(fdpth)\n            names = [el.replace(\'_leftImg8bit.png\', \'\') for el in im_names]\n            impths = [osp.join(fdpth, el) for el in im_names]\n            imgnames.extend(names)\n            self.imgs.update(dict(zip(names, impths)))\n\n        ## parse gt directory\n        self.labels = {}\n        gtnames = []\n        gtpth = osp.join(datapth, \'gtFine\', mode)\n        folders = os.listdir(gtpth)\n        for fd in folders:\n            fdpth = osp.join(gtpth, fd)\n            lbnames = os.listdir(fdpth)\n            lbnames = [el for el in lbnames if \'labelIds\' in el]\n            names = [el.replace(\'_gtFine_labelIds.png\', \'\') for el in lbnames]\n            lbpths = [osp.join(fdpth, el) for el in lbnames]\n            gtnames.extend(names)\n            self.labels.update(dict(zip(names, lbpths)))\n\n        self.imnames = imgnames\n        self.len = len(self.imnames)\n        assert set(imgnames) == set(gtnames)\n        assert set(self.imnames) == set(self.imgs.keys())\n        assert set(self.imnames) == set(self.labels.keys())\n\n        self.to_tensor = T.ToTensor(\n            mean=(0.3257, 0.3690, 0.3223), # city, rgb\n            std=(0.2112, 0.2148, 0.2115),\n        )\n\n    def __getitem__(self, idx):\n        fn  = self.imnames[idx]\n        impth, lbpth = self.imgs[fn], self.labels[fn]\n        img, label = cv2.imread(impth), cv2.imread(lbpth, 0)\n        label = self.lb_map[label]\n        im_lb = dict(im=img, lb=label)\n        if not self.trans_func is None:\n            im_lb = self.trans_func(im_lb)\n        im_lb = self.to_tensor(im_lb)\n        img, label = im_lb[\'im\'], im_lb[\'lb\']\n        return img.detach(), label.unsqueeze(0).detach()\n\n    def __len__(self):\n        return self.len\n\n\nclass TransformationTrain(object):\n\n    def __init__(self):\n        self.trans_func = T.Compose([\n            T.RandomResizedCrop([0.375, 1.], [512, 1024]),\n            T.RandomHorizontalFlip(),\n            T.ColorJitter(\n                brightness=0.4,\n                contrast=0.4,\n                saturation=0.4\n            ),\n        ])\n\n    def __call__(self, im_lb):\n        im_lb = self.trans_func(im_lb)\n        return im_lb\n\n\nclass TransformationVal(object):\n\n    def __call__(self, im_lb):\n        im, lb = im_lb[\'im\'], im_lb[\'lb\']\n        im = cv2.resize(im, (1024, 512))\n        return dict(im=im, lb=lb)\n\n\ndef get_data_loader(datapth, ims_per_gpu, max_iter=None, mode=\'train\', distributed=True):\n    train_trans_func = TransformationTrain()\n    val_trans_func = TransformationVal()\n    if mode == \'train\':\n        trans_func = train_trans_func\n        batchsize = ims_per_gpu\n        shuffle = True\n        drop_last = True\n    elif mode == \'val\':\n        trans_func = val_trans_func\n        batchsize = ims_per_gpu * 2\n        shuffle = False\n        drop_last = False\n\n    ds = CityScapes(datapth, trans_func=trans_func, mode=mode)\n\n    if distributed:\n        assert dist.is_available(), ""dist should be initialzed""\n        if mode == \'train\':\n            assert not max_iter is None\n            n_train_imgs = ims_per_gpu * dist.get_world_size() * max_iter\n            sampler = RepeatedDistSampler(ds, n_train_imgs, shuffle=shuffle)\n        else:\n            sampler = torch.utils.data.distributed.DistributedSampler(\n                ds, shuffle=shuffle)\n        batchsampler = torch.utils.data.sampler.BatchSampler(\n            sampler, batchsize, drop_last=drop_last\n        )\n        dl = DataLoader(\n            ds,\n            batch_sampler=batchsampler,\n            num_workers=4,\n            pin_memory=True,\n        )\n    else:\n        dl = DataLoader(\n            ds,\n            batch_size=batchsize,\n            shuffle=shuffle,\n            drop_last=drop_last,\n            num_workers=4,\n            pin_memory=True,\n        )\n    return dl\n\n\n\nif __name__ == ""__main__"":\n    from tqdm import tqdm\n    from torch.utils.data import DataLoader\n    ds = CityScapes(\'./data/\', mode=\'val\')\n    dl = DataLoader(ds,\n                    batch_size = 4,\n                    shuffle = True,\n                    num_workers = 4,\n                    drop_last = True)\n    for imgs, label in dl:\n        print(len(imgs))\n        for el in imgs:\n            print(el.size())\n        break\n'"
bisenetv2/evaluatev2.py,11,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nimport sys\nsys.path.insert(0, \'.\')\nimport os\nimport os.path as osp\nimport logging\nimport argparse\nimport math\n\nfrom tqdm import tqdm\nimport numpy as np\nimport cv2\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.distributed as dist\n\nfrom bisenetv2.bisenetv2 import BiSeNetV2\nfrom bisenetv2.logger import setup_logger\nfrom bisenetv2.cityscapes_cv2 import get_data_loader\n\n\n\n\nclass MscEvalV0(object):\n\n    def __init__(self, ignore_label=255):\n        self.ignore_label = ignore_label\n\n    def __call__(self, net, dl, n_classes):\n        ## evaluate\n        hist = torch.zeros(n_classes, n_classes).cuda().detach()\n        if dist.is_initialized() and dist.get_rank() != 0:\n            diter = enumerate(dl)\n        else:\n            diter = enumerate(tqdm(dl))\n        for i, (imgs, label) in diter:\n            N, _, H, W = label.shape\n            label = label.squeeze(1).cuda()\n            size = label.size()[-2:]\n            imgs = imgs.cuda()\n            logits = net(imgs)[0]\n            logits = F.interpolate(logits, size=size,\n                    mode=\'bilinear\', align_corners=True)\n            probs = torch.softmax(logits, dim=1)\n            preds = torch.argmax(probs, dim=1)\n            keep = label != self.ignore_label\n            hist += torch.bincount(\n                label[keep] * n_classes + preds[keep],\n                minlength=n_classes ** 2\n                ).view(n_classes, n_classes)\n        if dist.is_initialized():\n            dist.all_reduce(hist, dist.ReduceOp.SUM)\n        ious = hist.diag() / (hist.sum(dim=0) + hist.sum(dim=1) - hist.diag())\n        miou = ious.mean()\n        return miou.item()\n\n\n\ndef eval_model(net, ims_per_gpu):\n    is_dist = dist.is_initialized()\n    dl = get_data_loader(\'./data\', ims_per_gpu, mode=\'val\', distributed=is_dist)\n    net.eval()\n\n    with torch.no_grad():\n        single_scale = MscEvalV0()\n        mIOU = single_scale(net, dl, 19)\n    logger = logging.getLogger()\n    logger.info(\'mIOU is: %s\\n\', mIOU)\n\n\ndef evaluate(weight_pth):\n    logger = logging.getLogger()\n\n    ## model\n    logger.info(\'setup and restore model\')\n    net = BiSeNetV2(19)\n    net.load_state_dict(torch.load(weight_pth))\n    net.cuda()\n\n    is_dist = dist.is_initialized()\n    if is_dist:\n        local_rank = dist.get_rank()\n        net = nn.parallel.DistributedDataParallel(\n            net,\n            device_ids=[local_rank, ],\n            output_device=local_rank\n        )\n\n    ## evaluator\n    eval_model(net, 4)\n\n\ndef parse_args():\n    parse = argparse.ArgumentParser()\n    parse.add_argument(\'--local_rank\', dest=\'local_rank\',\n                       type=int, default=-1,)\n    parse.add_argument(\'--weight-path\', dest=\'weight_pth\', type=str,\n                       default=\'model_final.pth\',)\n    parse.add_argument(\'--port\', dest=\'port\', type=int, default=44553,)\n    parse.add_argument(\'--respth\', dest=\'respth\', type=str, default=\'./res\',)\n    return parse.parse_args()\n\n\ndef main():\n    args = parse_args()\n    if not args.local_rank == -1:\n        torch.cuda.set_device(args.local_rank)\n        dist.init_process_group(backend=\'nccl\',\n        init_method=\'tcp://127.0.0.1:{}\'.format(args.port),\n        world_size=torch.cuda.device_count(),\n        rank=args.local_rank\n    )\n    if not osp.exists(args.respth): os.makedirs(args.respth)\n    setup_logger(\'BiSeNetV2-eval\', args.respth)\n    evaluate(args.weight_pth)\n\n\n\nif __name__ == ""__main__"":\n    main()\n'"
bisenetv2/logger.py,1,"b""#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport os.path as osp\nimport time\nimport logging\n\nimport torch.distributed as dist\n\n\ndef setup_logger(name, logpth):\n    logfile = '{}-{}.log'.format(name, time.strftime('%Y-%m-%d-%H-%M-%S'))\n    logfile = osp.join(logpth, logfile)\n    FORMAT = '%(levelname)s %(filename)s(%(lineno)d): %(message)s'\n    log_level = logging.INFO\n    if dist.is_initialized() and dist.get_rank() != 0:\n        log_level = logging.WARNING\n    logging.basicConfig(level=log_level, format=FORMAT, filename=logfile)\n    logging.root.addHandler(logging.StreamHandler())\n\n\ndef print_log_msg(it, max_iter, lr, time_meter, loss_meter, loss_pre_meter,\n        loss_aux_meters):\n    t_intv, eta = time_meter.get()\n    loss_avg, _ = loss_meter.get()\n    loss_pre_avg, _ = loss_pre_meter.get()\n    loss_aux_avg = ', '.join(['{}: {:.4f}'.format(el.name, el.get()[0]) for el in loss_aux_meters])\n    msg = ', '.join([\n        'iter: {it}/{max_it}',\n        'lr: {lr:4f}',\n        'eta: {eta}',\n        'time: {time:.2f}',\n        'loss: {loss:.4f}',\n        'loss_pre: {loss_pre:.4f}',\n    ]).format(\n        it=it+1,\n        max_it=max_iter,\n        lr=lr,\n        time=t_intv,\n        eta=eta,\n        loss=loss_avg,\n        loss_pre=loss_pre_avg,\n        )\n    msg += ', ' + loss_aux_avg\n    logger = logging.getLogger()\n    logger.info(msg)\n"""
bisenetv2/lr_scheduler.py,3,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nimport math\nfrom bisect import bisect_right\nimport torch\n\n\nclass WarmupLrScheduler(torch.optim.lr_scheduler._LRScheduler):\n\n    def __init__(\n            self,\n            optimizer,\n            warmup_iter=500,\n            warmup_ratio=5e-4,\n            warmup=\'exp\',\n            last_epoch=-1,\n    ):\n        self.warmup_iter = warmup_iter\n        self.warmup_ratio = warmup_ratio\n        self.warmup = warmup\n        super(WarmupLrScheduler, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        ratio = self.get_lr_ratio()\n        lrs = [ratio * lr for lr in self.base_lrs]\n        return lrs\n\n    def get_lr_ratio(self):\n        if self.last_epoch < self.warmup_iter:\n            ratio = self.get_warmup_ratio()\n        else:\n            ratio = self.get_main_ratio()\n        return ratio\n\n    def get_main_ratio(self):\n        raise NotImplementedError\n\n    def get_warmup_ratio(self):\n        assert self.warmup in (\'linear\', \'exp\')\n        alpha = self.last_epoch / self.warmup_iter\n        if self.warmup == \'linear\':\n            ratio = self.warmup_ratio + (1 - self.warmup_ratio) * alpha\n        elif self.warmup == \'exp\':\n            ratio = self.warmup_ratio ** (1. - alpha)\n        return ratio\n\n\nclass WarmupPolyLrScheduler(WarmupLrScheduler):\n\n    def __init__(\n            self,\n            optimizer,\n            power,\n            max_iter,\n            warmup_iter=500,\n            warmup_ratio=5e-4,\n            warmup=\'exp\',\n            last_epoch=-1,\n    ):\n        self.power = power\n        self.max_iter = max_iter\n        super(WarmupPolyLrScheduler, self).__init__(\n            optimizer, warmup_iter, warmup_ratio, warmup, last_epoch)\n\n    def get_main_ratio(self):\n        real_iter = self.last_epoch - self.warmup_iter\n        real_max_iter = self.max_iter - self.warmup_iter\n        alpha = real_iter / real_max_iter\n        ratio = (1 - alpha) ** self.power\n        return ratio\n\n\nclass WarmupExpLrScheduler(WarmupLrScheduler):\n\n    def __init__(\n            self,\n            optimizer,\n            gamma,\n            interval=1,\n            warmup_iter=500,\n            warmup_ratio=5e-4,\n            warmup=\'exp\',\n            last_epoch=-1,\n    ):\n        self.gamma = gamma\n        self.interval = interval\n        super(WarmupExpLrScheduler, self).__init__(\n            optimizer, warmup_iter, warmup_ratio, warmup, last_epoch)\n\n    def get_main_ratio(self):\n        real_iter = self.last_epoch - self.warmup_iter\n        ratio = self.gamma ** (real_iter // self.interval)\n        return ratio\n\n\nclass WarmupCosineLrScheduler(WarmupLrScheduler):\n\n    def __init__(\n            self,\n            optimizer,\n            max_iter,\n            eta_ratio=0,\n            warmup_iter=500,\n            warmup_ratio=5e-4,\n            warmup=\'exp\',\n            last_epoch=-1,\n    ):\n        self.eta_ratio = eta_ratio\n        self.max_iter = max_iter\n        super(WarmupCosineLrScheduler, self).__init__(\n            optimizer, warmup_iter, warmup_ratio, warmup, last_epoch)\n\n    def get_main_ratio(self):\n        real_iter = self.last_epoch - self.warmup_iter\n        real_max_iter = self.max_iter - self.warmup_iter\n        return self.eta_ratio + (1 - self.eta_ratio) * (\n                1 + math.cos(math.pi * self.last_epoch / real_max_iter)) / 2\n\n\nclass WarmupStepLrScheduler(WarmupLrScheduler):\n\n    def __init__(\n            self,\n            optimizer,\n            milestones: list,\n            gamma=0.1,\n            warmup_iter=500,\n            warmup_ratio=5e-4,\n            warmup=\'exp\',\n            last_epoch=-1,\n    ):\n        self.milestones = milestones\n        self.gamma = gamma\n        super(WarmupStepLrScheduler, self).__init__(\n            optimizer, warmup_iter, warmup_ratio, warmup, last_epoch)\n\n    def get_main_ratio(self):\n        real_iter = self.last_epoch - self.warmup_iter\n        ratio = self.gamma ** bisect_right(self.milestones, real_iter)\n        return ratio\n\n\nif __name__ == ""__main__"":\n    model = torch.nn.Conv2d(3, 16, 3, 1, 1)\n    optim = torch.optim.SGD(model.parameters(), lr=1e-3)\n\n    max_iter = 20000\n    lr_scheduler = WarmupPolyLrScheduler(optim, 0.9, max_iter, 200, 0.1, \'linear\', -1)\n    lrs = []\n    for _ in range(max_iter):\n        lr = lr_scheduler.get_lr()[0]\n        print(lr)\n        lrs.append(lr)\n        lr_scheduler.step()\n    import matplotlib\n    import matplotlib.pyplot as plt\n    import numpy as np\n    lrs = np.array(lrs)\n    n_lrs = len(lrs)\n    plt.plot(np.arange(n_lrs), lrs)\n    plt.grid()\n    plt.show()\n\n\n'"
bisenetv2/meters.py,0,"b'\nimport time\nimport datetime\n\nclass TimeMeter(object):\n\n    def __init__(self, max_iter):\n        self.iter = 0\n        self.max_iter = max_iter\n        self.st = time.time()\n        self.global_st = self.st\n        self.curr = self.st\n\n    def update(self):\n        self.iter += 1\n\n    def get(self):\n        self.curr = time.time()\n        interv = self.curr - self.st\n        global_interv = self.curr - self.global_st\n        eta = int((self.max_iter-self.iter) * (global_interv / (self.iter+1)))\n        eta = str(datetime.timedelta(seconds=eta))\n        self.st = self.curr\n        return interv, eta\n\n\nclass AvgMeter(object):\n\n    def __init__(self, name):\n        self.name = name\n        self.seq = []\n        self.global_seq = []\n\n    def update(self, val):\n        self.seq.append(val)\n        self.global_seq.append(val)\n\n    def get(self):\n        avg = sum(self.seq) / len(self.seq)\n        global_avg = sum(self.global_seq) / len(self.global_seq)\n        self.seq = []\n        return avg, global_avg\n\n'"
bisenetv2/ohem_ce_loss.py,4,"b""#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n\nclass OhemCELoss(nn.Module):\n\n    def __init__(self, thresh, ignore_lb=255):\n        super(OhemCELoss, self).__init__()\n        self.thresh = -torch.log(torch.tensor(thresh, requires_grad=False, dtype=torch.float)).cuda()\n        self.ignore_lb = ignore_lb\n        self.criteria = nn.CrossEntropyLoss(ignore_index=ignore_lb, reduction='none')\n\n    def forward(self, logits, labels):\n        n_min = labels[labels != self.ignore_lb].numel() // 16\n        loss = self.criteria(logits, labels).view(-1)\n        loss_hard = loss[loss > self.thresh]\n        if loss_hard.numel() < n_min:\n            loss_hard, _ = loss.topk(n_min)\n        return torch.mean(loss_hard)\n\n\nif __name__ == '__main__':\n    pass\n    #  criteria1 = OhemCELoss(thresh=0.7, n_min=16*20*20//16).cuda()\n    #  criteria2 = OhemCELoss(thresh=0.7, n_min=16*20*20//16).cuda()\n\n"""
bisenetv2/sampler.py,5,"b'\nimport math\nimport torch\nfrom torch.utils.data.sampler import Sampler\nimport torch.distributed as dist\n\n\nclass RepeatedDistSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n\n    .. note::\n        Dataset is assumed to be of constant size.\n\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n        shuffle (optional): If true (default), sampler will shuffle the indices\n    """"""\n\n    def __init__(self, dataset, num_imgs, num_replicas=None, rank=None, shuffle=True):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.num_imgs_rank = int(math.ceil(num_imgs * 1.0 / self.num_replicas))\n        self.total_size = self.num_imgs_rank * self.num_replicas\n        self.num_imgs = num_imgs\n        self.shuffle = shuffle\n\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        g = torch.Generator()\n        n_repeats = self.num_imgs // len(self.dataset) + 1\n        indices = []\n        for n in range(n_repeats):\n            if self.shuffle:\n                g.manual_seed(n)\n                indices += torch.randperm(len(self.dataset), generator=g).tolist()\n            else:\n                indices += [i for i in range(len(self.dataset))]\n\n        # add extra samples to make it evenly divisible\n        indices = indices[:self.total_size]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        assert len(indices) == self.num_imgs_rank\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_imgs_rank\n\n'"
bisenetv2/train.py,16,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nimport sys\nsys.path.insert(0, \'.\')\nimport os\nimport os.path as osp\nimport random\nimport logging\nimport time\nimport argparse\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.distributed as dist\nfrom torch.utils.data import DataLoader\n\nfrom bisenetv2.bisenetv2 import BiSeNetV2\nfrom bisenetv2.cityscapes_cv2 import get_data_loader\nfrom bisenetv2.evaluatev2 import eval_model\nfrom bisenetv2.ohem_ce_loss import OhemCELoss\nfrom bisenetv2.lr_scheduler import WarmupPolyLrScheduler\nfrom bisenetv2.meters import TimeMeter, AvgMeter\nfrom bisenetv2.logger import setup_logger, print_log_msg\n\n# apex\nhas_apex = True\ntry:\n    from apex import amp, parallel\nexcept ImportError:\n    has_apex = False\n\n\n## fix all random seeds\ntorch.manual_seed(123)\ntorch.cuda.manual_seed(123)\nnp.random.seed(123)\nrandom.seed(123)\ntorch.backends.cudnn.deterministic = True\n#  torch.backends.cudnn.benchmark = True\n#  torch.multiprocessing.set_sharing_strategy(\'file_system\')\n\nlr_start = 5e-2\nwarmup_iters = 1000\nmax_iter = 150000  + warmup_iters\nims_per_gpu = 8\n\n\ndef parse_args():\n    parse = argparse.ArgumentParser()\n    parse.add_argument(\'--local_rank\', dest=\'local_rank\', type=int, default=-1,)\n    parse.add_argument(\'--sync-bn\', dest=\'use_sync_bn\', action=\'store_true\',)\n    parse.add_argument(\'--fp16\', dest=\'use_fp16\', action=\'store_true\',)\n    parse.add_argument(\'--port\', dest=\'port\', type=int, default=44554,)\n    parse.add_argument(\'--respth\', dest=\'respth\', type=str, default=\'./res\',)\n    return parse.parse_args()\n\nargs = parse_args()\n\n\n\ndef set_model():\n    net = BiSeNetV2(19)\n    if args.use_sync_bn: net = set_syncbn(net)\n    net.cuda()\n    net.train()\n    criteria_pre = OhemCELoss(0.7)\n    criteria_aux = [OhemCELoss(0.7) for _ in range(4)]\n    return net, criteria_pre, criteria_aux\n\ndef set_syncbn(net):\n    if has_apex:\n        net = parallel.convert_syncbn_model(net)\n    else:\n        net = nn.SyncBatchNorm.convert_sync_batchnorm(net)\n    return net\n\n\ndef set_optimizer(model):\n    wd_params, non_wd_params = [], []\n    for name, param in model.named_parameters():\n        if param.dim() == 1:\n            non_wd_params.append(param)\n        elif param.dim() == 2 or param.dim() == 4:\n            wd_params.append(param)\n    params_list = [\n        {\'params\': wd_params, },\n        {\'params\': non_wd_params, \'weight_decay\': 0},\n    ]\n    optim = torch.optim.SGD(\n        params_list,\n        lr=lr_start,\n        momentum=0.9,\n        weight_decay=5e-4,\n    )\n    return optim\n\n\ndef set_model_dist(net):\n    if has_apex:\n        net = parallel.DistributedDataParallel(net, delay_allreduce=True)\n    else:\n        local_rank = dist.get_rank()\n        net = nn.parallel.DistributedDataParallel(\n            net,\n            device_ids=[local_rank, ],\n            output_device=local_rank)\n    return net\n\n\ndef set_meters():\n    time_meter = TimeMeter(max_iter)\n    loss_meter = AvgMeter(\'loss\')\n    loss_pre_meter = AvgMeter(\'loss_prem\')\n    loss_aux_meters = [AvgMeter(\'loss_aux{}\'.format(i)) for i in range(4)]\n    return time_meter, loss_meter, loss_pre_meter, loss_aux_meters\n\n\ndef save_model(states, save_pth):\n    logger = logging.getLogger()\n    logger.info(\'\\nsave models to {}\'.format(save_pth))\n    for name, state in states.items():\n        save_name = \'model_final_{}.pth\'.format(name)\n        modelpth = osp.join(save_pth, save_name)\n        if dist.is_initialized() and dist.get_rank() == 0:\n            torch.save(state, modelpth)\n\n\ndef train():\n    logger = logging.getLogger()\n    is_dist = dist.is_initialized()\n\n    ## dataset\n    dl = get_data_loader(\'./data/\', ims_per_gpu, max_iter,\n            mode=\'train\', distributed=is_dist)\n\n    ## model\n    net, criteria_pre, criteria_aux = set_model()\n\n    ## optimizer\n    optim = set_optimizer(net)\n\n    ## fp16\n    if has_apex and args.use_fp16:\n        net, optim = amp.initialize(net, optim, opt_level=\'O1\')\n\n    ## ddp training\n    net = set_model_dist(net)\n\n    ## meters\n    time_meter, loss_meter, loss_pre_meter, loss_aux_meters = set_meters()\n\n    ## lr scheduler\n    lr_schdr = WarmupPolyLrScheduler(optim, power=0.9,\n        max_iter=max_iter, warmup_iter=warmup_iters,\n        warmup_ratio=0.1, warmup=\'exp\', last_epoch=-1,)\n\n    ## train loop\n    for it, (im, lb) in enumerate(dl):\n        im = im.cuda()\n        lb = lb.cuda()\n\n        lb = torch.squeeze(lb, 1)\n\n        optim.zero_grad()\n        logits, *logits_aux = net(im)\n        loss_pre = criteria_pre(logits, lb)\n        loss_aux = [crit(lgt, lb) for crit, lgt in zip(criteria_aux, logits_aux)]\n        loss = loss_pre + sum(loss_aux)\n        if has_apex:\n            with amp.scale_loss(loss, optim) as scaled_loss:\n                scaled_loss.backward()\n        else:\n            loss.backward()\n        optim.step()\n        torch.cuda.synchronize()\n        lr_schdr.step()\n\n        time_meter.update()\n        loss_meter.update(loss.item())\n        loss_pre_meter.update(loss_pre.item())\n        _ = [mter.update(lss.item()) for mter, lss in zip(loss_aux_meters, loss_aux)]\n\n        ## print training log message\n        if (it + 1) % 100 == 0:\n            lr = lr_schdr.get_lr()\n            lr = sum(lr) / len(lr)\n            print_log_msg(\n                it, max_iter, lr, time_meter, loss_meter,\n                loss_pre_meter, loss_aux_meters)\n\n    ## dump the final model and evaluate the result\n    save_pth = osp.join(args.respth, \'model_final.pth\')\n    logger.info(\'\\nsave models to {}\'.format(save_pth))\n    state = net.module.state_dict()\n    if dist.get_rank() == 0: torch.save(state, save_pth)\n\n    logger.info(\'\\nevaluating the final model\')\n    torch.cuda.empty_cache()\n    eval_model(net, 4)\n\n    return\n\n\ndef main():\n    torch.cuda.set_device(args.local_rank)\n    dist.init_process_group(\n        backend=\'nccl\',\n        init_method=\'tcp://127.0.0.1:{}\'.format(args.port),\n        world_size=torch.cuda.device_count(),\n        rank=args.local_rank\n    )\n    if not osp.exists(args.respth): os.makedirs(args.respth)\n    setup_logger(\'BiSeNetV2-train\', args.respth)\n    train()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
bisenetv2/transform_cv2.py,4,"b""#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport random\nimport math\n\nimport numpy as np\nimport cv2\nimport torch\n\n\n\nclass RandomResizedCrop(object):\n    '''\n    size should be a tuple of (H, W)\n    '''\n    def __init__(self, scales=(0.5, 1.), size=(384, 384)):\n        self.scales = scales\n        self.size = size\n\n    def __call__(self, im_lb):\n        if self.size is None:\n            return im_lb\n\n        im, lb = im_lb['im'], im_lb['lb']\n        assert im.shape[:2] == lb.shape[:2]\n\n        crop_h, crop_w = self.size\n        scale = np.random.uniform(min(self.scales), max(self.scales))\n        im_h, im_w = [math.ceil(el * scale) for el in im.shape[:2]]\n        im = cv2.resize(im, (im_w, im_h))\n        lb = cv2.resize(lb, (im_w, im_h), interpolation=cv2.INTER_NEAREST)\n\n        if (im_h, im_w) == (crop_h, crop_w): return dict(im=im, lb=lb)\n        pad_h, pad_w = 0, 0\n        if im_h < crop_h:\n            pad_h = (crop_h - im_h) // 2 + 1\n        if im_w < crop_w:\n            pad_w = (crop_w - im_w) // 2 + 1\n        if pad_h > 0 or pad_w > 0:\n            im = np.pad(im, ((pad_h, pad_h), (pad_w, pad_w), (0, 0)))\n            lb = np.pad(lb, ((pad_h, pad_h), (pad_w, pad_w)), 'constant', constant_values=255)\n\n        im_h, im_w, _ = im.shape\n        sh, sw = np.random.random(2)\n        sh, sw = int(sh * (im_h - crop_h)), int(sw * (im_w - crop_w))\n        return dict(\n            im=im[sh:sh+crop_h, sw:sw+crop_w, :].copy(),\n            lb=lb[sh:sh+crop_h, sw:sw+crop_w].copy()\n        )\n\n\n\nclass RandomHorizontalFlip(object):\n\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, im_lb):\n        if np.random.random() < self.p:\n            return im_lb\n        im, lb = im_lb['im'], im_lb['lb']\n        assert im.shape[:2] == lb.shape[:2]\n        return dict(\n            im=im[:, ::-1, :],\n            lb=lb[:, ::-1],\n        )\n\n\n\nclass ColorJitter(object):\n\n    def __init__(self, brightness=None, contrast=None, saturation=None):\n        if not brightness is None and brightness >= 0:\n            self.brightness = [max(1-brightness, 0), 1+brightness]\n        if not contrast is None and contrast >= 0:\n            self.contrast = [max(1-contrast, 0), 1+contrast]\n        if not saturation is None and saturation >= 0:\n            self.saturation = [max(1-saturation, 0), 1+saturation]\n\n    def __call__(self, im_lb):\n        im, lb = im_lb['im'], im_lb['lb']\n        assert im.shape[:2] == lb.shape[:2]\n        if not self.brightness is None:\n            rate = np.random.uniform(*self.brightness)\n            im = self.adj_brightness(im, rate)\n        if not self.contrast is None:\n            rate = np.random.uniform(*self.contrast)\n            im = self.adj_contrast(im, rate)\n        if not self.saturation is None:\n            rate = np.random.uniform(*self.saturation)\n            im = self.adj_saturation(im, rate)\n        return dict(im=im, lb=lb,)\n\n    def adj_saturation(self, im, rate):\n        M = np.float32([\n            [1+2*rate, 1-rate, 1-rate],\n            [1-rate, 1+2*rate, 1-rate],\n            [1-rate, 1-rate, 1+2*rate]\n        ])\n        shape = im.shape\n        im = np.matmul(im.reshape(-1, 3), M).reshape(shape)/3\n        im = np.clip(im, 0, 255).astype(np.uint8)\n        return im\n\n    def adj_brightness(self, im, rate):\n        table = np.array([\n            i * rate for i in range(256)\n        ]).clip(0, 255).astype(np.uint8)\n        return table[im]\n\n    def adj_contrast(self, im, rate):\n        table = np.array([\n            74 + (i - 74) * rate for i in range(256)\n        ]).clip(0, 255).astype(np.uint8)\n        return table[im]\n\n\n\n\nclass ToTensor(object):\n    '''\n    mean and std should be of the channel order 'bgr'\n    '''\n    def __init__(self, mean=(0, 0, 0), std=(1., 1., 1.)):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, im_lb):\n        im, lb = im_lb['im'], im_lb['lb']\n        im = im[:, :, ::-1].transpose(2, 0, 1).astype(np.float32) # to rgb order\n        im = torch.from_numpy(im).div_(255)\n        dtype, device = im.dtype, im.device\n        mean = torch.as_tensor(self.mean, dtype=dtype, device=device)[:, None, None]\n        std = torch.as_tensor(self.std, dtype=dtype, device=device)[:, None, None]\n        im = im.sub_(mean).div_(std).clone()\n        lb = torch.from_numpy(lb.astype(np.int64).copy()).clone()\n        return dict(im=im, lb=lb)\n\n\nclass Compose(object):\n\n    def __init__(self, do_list):\n        self.do_list = do_list\n\n    def __call__(self, im_lb):\n        for comp in self.do_list:\n            im_lb = comp(im_lb)\n        return im_lb\n\n\n\n\nif __name__ == '__main__':\n    #  from PIL import Image\n    #  im = Image.open(imgpth)\n    #  lb = Image.open(lbpth)\n    #  print(lb.size)\n    #  im.show()\n    #  lb.show()\n    import cv2\n    im = cv2.imread(imgpth)\n    lb = cv2.imread(lbpth, 0)\n    lb = lb * 10\n\n    trans = Compose([\n        RandomHorizontalFlip(),\n        RandomShear(p=0.5, rate=3),\n        RandomRotate(p=0.5, degree=5),\n        RandomScale([0.5, 0.7]),\n        RandomCrop((768, 768)),\n        RandomErasing(p=1, size=(36, 36)),\n        ChannelShuffle(p=1),\n        ColorJitter(\n            brightness=0.3,\n            contrast=0.3,\n            saturation=0.5\n        ),\n        #  RandomEqualize(p=0.1),\n    ])\n    #  inten = dict(im=im, lb=lb)\n    #  out = trans(inten)\n    #  im = out['im']\n    #  lb = out['lb']\n    #  cv2.imshow('lb', lb)\n    #  cv2.imshow('org', im)\n    #  cv2.waitKey(0)\n\n\n    ### try merge rotate and shear here\n    im = cv2.imread(imgpth)\n    lb = cv2.imread(lbpth, 0)\n    im = cv2.resize(im, (1024, 512))\n    lb = cv2.resize(lb, (1024, 512), interpolation=cv2.INTER_NEAREST)\n    lb = lb * 10\n    inten = dict(im=im, lb=lb)\n    trans1 = Compose([\n        RandomShear(p=1, rate=0.15),\n        #  RandomRotate(p=1, degree=10),\n    ])\n    trans2 = Compose([\n        #  RandomShearRotate(p_shear=1, p_rot=0, rate_shear=0.1, rot_degree=9),\n        RandomHFlipShearRotate(p_flip=0.5, p_shear=1, p_rot=0, rate_shear=0.1, rot_degree=9),\n    ])\n    out1 = trans1(inten)\n    im1 = out1['im']\n    lb1 = out1['lb']\n    #  cv2.imshow('lb', lb1)\n    cv2.imshow('org1', im1)\n    out2 = trans2(inten)\n    im2 = out2['im']\n    lb2 = out2['lb']\n    #  cv2.imshow('lb', lb1)\n    #  cv2.imshow('org2', im2)\n    cv2.waitKey(0)\n    print(np.sum(im1-im2))\n    print('====')\n    ####\n\n\n    totensor = ToTensor(\n        mean=(0.406, 0.456, 0.485),\n        std=(0.225, 0.224, 0.229)\n    )\n    #  print(im[0, :2, :2])\n    print(lb[:2, :2])\n    out = totensor(out)\n    im = out['im']\n    lb = out['lb']\n    print(im.size())\n    #  print(im[0, :2, :2])\n    #  print(lb[:2, :2])\n\n    out = totensor(inten)\n    im = out['im']\n    print(im.size())\n    print(im[0, 502:504, 766:768])\n\n"""
diss/__init__.py,0,b''
diss/evaluate.py,12,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nfrom logger import setup_logger\nfrom diss.model import BiSeNet\nfrom cityscapes import CityScapes\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torch.distributed as dist\n\nimport os\nimport os.path as osp\nimport logging\nimport time\nimport numpy as np\nfrom tqdm import tqdm\nimport math\n\n\n\nclass MscEval(object):\n    def __init__(self,\n            model,\n            dataloader,\n            scales = [0.5, 0.75, 1, 1.25, 1.5, 1.75],\n            n_classes = 19,\n            lb_ignore = 255,\n            cropsize = 1024,\n            flip = True,\n            *args, **kwargs):\n        self.scales = scales\n        self.n_classes = n_classes\n        self.lb_ignore = lb_ignore\n        self.flip = flip\n        self.cropsize = cropsize\n        ## dataloader\n        self.dl = dataloader\n        self.net = model\n\n\n    def pad_tensor(self, inten, size):\n        N, C, H, W = inten.size()\n        outten = torch.zeros(N, C, size[0], size[1]).cuda()\n        outten.requires_grad = False\n        margin_h, margin_w = size[0]-H, size[1]-W\n        hst, hed = margin_h//2, margin_h//2+H\n        wst, wed = margin_w//2, margin_w//2+W\n        outten[:, :, hst:hed, wst:wed] = inten\n        return outten, [hst, hed, wst, wed]\n\n\n    def eval_chip(self, crop):\n        with torch.no_grad():\n            out = self.net(crop)[0]\n            prob = F.softmax(out, 1)\n            if self.flip:\n                crop = torch.flip(crop, dims=(3,))\n                out = self.net(crop)[0]\n                out = torch.flip(out, dims=(3,))\n                prob += F.softmax(out, 1)\n            prob = torch.exp(prob)\n        return prob\n\n\n    def crop_eval(self, im):\n        cropsize = self.cropsize\n        stride_rate = 5/6.\n        N, C, H, W = im.size()\n        long_size, short_size = (H,W) if H>W else (W,H)\n        if long_size < cropsize:\n            im, indices = self.pad_tensor(im, (cropsize, cropsize))\n            prob = self.eval_chip(im)\n            prob = prob[:, :, indices[0]:indices[1], indices[2]:indices[3]]\n        else:\n            stride = math.ceil(cropsize*stride_rate)\n            if short_size < cropsize:\n                if H < W:\n                    im, indices = self.pad_tensor(im, (cropsize, W))\n                else:\n                    im, indices = self.pad_tensor(im, (H, cropsize))\n            N, C, H, W = im.size()\n            n_x = math.ceil((W-cropsize)/stride)+1\n            n_y = math.ceil((H-cropsize)/stride)+1\n            prob = torch.zeros(N, self.n_classes, H, W).cuda()\n            prob.requires_grad = False\n            for iy in range(n_y):\n                for ix in range(n_x):\n                    hed, wed = min(H, stride*iy+cropsize), min(W, stride*ix+cropsize)\n                    hst, wst = hed-cropsize, wed-cropsize\n                    chip = im[:, :, hst:hed, wst:wed]\n                    prob_chip = self.eval_chip(chip)\n                    prob[:, :, hst:hed, wst:wed] += prob_chip\n            if short_size < cropsize:\n                prob = prob[:, :, indices[0]:indices[1], indices[2]:indices[3]]\n        return prob\n\n\n    def scale_crop_eval(self, im, scale):\n        N, C, H, W = im.size()\n        new_hw = [int(H*scale), int(W*scale)]\n        im = F.interpolate(im, new_hw, mode=\'bilinear\', align_corners=True)\n        prob = self.crop_eval(im)\n        prob = F.interpolate(prob, (H, W), mode=\'bilinear\', align_corners=True)\n        return prob\n\n\n    def compute_hist(self, pred, lb):\n        n_classes = self.n_classes\n        ignore_idx = self.lb_ignore\n        keep = np.logical_not(lb==ignore_idx)\n        merge = pred[keep] * n_classes + lb[keep]\n        hist = np.bincount(merge, minlength=n_classes**2)\n        hist = hist.reshape((n_classes, n_classes))\n        return hist\n\n\n    def evaluate(self):\n        ## evaluate\n        n_classes = self.n_classes\n        hist = np.zeros((n_classes, n_classes), dtype=np.float32)\n        dloader = tqdm(self.dl)\n        if dist.is_initialized() and not dist.get_rank()==0:\n            dloader = self.dl\n        for i, (imgs, label) in enumerate(dloader):\n            N, _, H, W = label.shape\n            probs = torch.zeros((N, self.n_classes, H, W))\n            probs.requires_grad = False\n            imgs = imgs.cuda()\n            for sc in self.scales:\n                prob = self.scale_crop_eval(imgs, sc)\n                probs += prob.detach().cpu()\n            probs = probs.data.numpy()\n            preds = np.argmax(probs, axis=1)\n\n            hist_once = self.compute_hist(preds, label.data.numpy().squeeze(1))\n            hist = hist + hist_once\n        IOUs = np.diag(hist) / (np.sum(hist, axis=0)+np.sum(hist, axis=1)-np.diag(hist))\n        mIOU = np.mean(IOUs)\n        return mIOU\n\n\ndef evaluate(respth=\'./res\', dspth=\'./data\'):\n    ## logger\n    logger = logging.getLogger()\n\n    ## model\n    logger.info(\'\\n\')\n    logger.info(\'====\'*20)\n    logger.info(\'evaluating the model ...\\n\')\n    logger.info(\'setup and restore model\')\n    n_classes = 19\n    net = BiSeNet(n_classes=n_classes)\n    save_pth = osp.join(respth, \'model_final_diss.pth\')\n    net.load_state_dict(torch.load(save_pth))\n    net.cuda()\n    net.eval()\n\n    ## dataset\n    batchsize = 5\n    n_workers = 2\n    dsval = CityScapes(dspth, mode=\'val\')\n    dl = DataLoader(dsval,\n                    batch_size = batchsize,\n                    shuffle = False,\n                    num_workers = n_workers,\n                    drop_last = False)\n\n    ## evaluator\n    logger.info(\'compute the mIOU\')\n    evaluator = MscEval(net, dl)\n\n    ## eval\n    mIOU = evaluator.evaluate()\n    logger.info(\'mIOU is: {:.6f}\'.format(mIOU))\n\n\n\nif __name__ == ""__main__"":\n    setup_logger(\'./res\')\n    evaluate()\n'"
diss/model.py,6,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\n\nfrom resnet import Resnet18\nfrom modules.bn import InPlaceABNSync as BatchNorm2d\n\n\nclass ConvBNReLU(nn.Module):\n    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1, *args, **kwargs):\n        super(ConvBNReLU, self).__init__()\n        self.conv = nn.Conv2d(in_chan,\n                out_chan,\n                kernel_size = ks,\n                stride = stride,\n                padding = padding,\n                bias = False)\n        self.bn = BatchNorm2d(out_chan)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n\nclass BiSeNetOutput(nn.Module):\n    def __init__(self, in_chan, mid_chan, n_classes, *args, **kwargs):\n        super(BiSeNetOutput, self).__init__()\n        self.conv = ConvBNReLU(in_chan, mid_chan, ks=3, stride=1, padding=1)\n        self.conv_out = nn.Conv2d(mid_chan, n_classes, kernel_size=1, bias=False)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.conv_out(x)\n        return x\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass AttentionRefinementModule(nn.Module):\n    def __init__(self, in_chan, out_chan, *args, **kwargs):\n        super(AttentionRefinementModule, self).__init__()\n        self.conv = ConvBNReLU(in_chan, out_chan, ks=3, stride=1, padding=1)\n        self.conv_atten = nn.Conv2d(out_chan, out_chan, kernel_size= 1, bias=False)\n        self.bn_atten = BatchNorm2d(out_chan, activation=\'none\')\n        self.sigmoid_atten = nn.Sigmoid()\n        self.init_weight()\n\n    def forward(self, x):\n        feat = self.conv(x)\n        atten = F.avg_pool2d(feat, feat.size()[2:])\n        atten = self.conv_atten(atten)\n        atten = self.bn_atten(atten)\n        atten = self.sigmoid_atten(atten)\n        out = torch.mul(feat, atten)\n        return out\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n\nclass ContextPath(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(ContextPath, self).__init__()\n        self.resnet = Resnet18()\n        self.arm16 = AttentionRefinementModule(256, 128)\n        self.arm32 = AttentionRefinementModule(512, 128)\n        self.conv_head32 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n        self.conv_head16 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n        self.conv_avg = ConvBNReLU(512, 128, ks=1, stride=1, padding=0)\n\n        self.init_weight()\n\n    def forward(self, x):\n        H0, W0 = x.size()[2:]\n        feat8, feat16, feat32 = self.resnet(x)\n        H8, W8 = feat8.size()[2:]\n        H16, W16 = feat16.size()[2:]\n        H32, W32 = feat32.size()[2:]\n\n        avg = F.avg_pool2d(feat32, feat32.size()[2:])\n        avg = self.conv_avg(avg)\n        avg_up = F.interpolate(avg, (H32, W32), mode=\'nearest\')\n\n        feat32_arm = self.arm32(feat32)\n        feat32_sum = feat32_arm + avg_up\n        feat32_up = F.interpolate(feat32_sum, (H16, W16), mode=\'nearest\')\n        feat32_up = self.conv_head32(feat32_up)\n\n        feat16_arm = self.arm16(feat16)\n        feat16_sum = feat16_arm + feat32_up\n        feat16_up = F.interpolate(feat16_sum, (H8, W8), mode=\'nearest\')\n        feat16_up = self.conv_head16(feat16_up)\n\n        return feat8, feat16_up, feat32_up # x8, x8, x16\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\n### This is not used, since I replace this with the resnet feature with the same size\nclass SpatialPath(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(SpatialPath, self).__init__()\n        self.conv1 = ConvBNReLU(3, 64, ks=7, stride=2, padding=3)\n        self.conv2 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n        self.conv3 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n        self.conv_out = ConvBNReLU(64, 128, ks=1, stride=1, padding=0)\n        self.init_weight()\n\n    def forward(self, x):\n        feat = self.conv1(x)\n        feat = self.conv2(feat)\n        feat = self.conv3(feat)\n        feat = self.conv_out(feat)\n        return feat\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass FeatureFusionModule(nn.Module):\n    def __init__(self, in_chan, out_chan, *args, **kwargs):\n        super(FeatureFusionModule, self).__init__()\n        self.convblk = ConvBNReLU(in_chan, out_chan, ks=1, stride=1, padding=0)\n        self.conv1 = nn.Conv2d(out_chan,\n                out_chan//4,\n                kernel_size = 1,\n                stride = 1,\n                padding = 0,\n                bias = False)\n        self.conv2 = nn.Conv2d(out_chan//4,\n                out_chan,\n                kernel_size = 1,\n                stride = 1,\n                padding = 0,\n                bias = False)\n        self.relu = nn.ReLU(inplace=True)\n        self.sigmoid = nn.Sigmoid()\n        self.init_weight()\n\n    def forward(self, fsp, fcp):\n        fcat = torch.cat([fsp, fcp], dim=1)\n        feat = self.convblk(fcat)\n        atten = F.avg_pool2d(feat, feat.size()[2:])\n        atten = self.conv1(atten)\n        atten = self.relu(atten)\n        atten = self.conv2(atten)\n        atten = self.sigmoid(atten)\n        feat_atten = torch.mul(feat, atten)\n        feat_out = feat_atten + feat\n        return feat_out\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass BiSeNet(nn.Module):\n    def __init__(self, n_classes, *args, **kwargs):\n        super(BiSeNet, self).__init__()\n        self.cp = ContextPath()\n        ## here self.sp is deleted\n        self.ffm = FeatureFusionModule(256, 256)\n        self.conv_out = BiSeNetOutput(256, 256, n_classes)\n        self.conv_out16 = BiSeNetOutput(128, 64, n_classes)\n        self.conv_out32 = BiSeNetOutput(128, 64, n_classes)\n        self.init_weight()\n\n    def forward(self, x):\n        H, W = x.size()[2:]\n        feat_res8, feat_cp8, feat_cp16 = self.cp(x) # here return res3b1 feature\n        feat_sp = feat_res8 # use res3b1 feature to replace spatial path feature\n        feat_fuse = self.ffm(feat_sp, feat_cp8)\n\n        feat_out = self.conv_out(feat_fuse)\n        feat_out16 = self.conv_out16(feat_cp8)\n        feat_out32 = self.conv_out32(feat_cp16)\n\n        feat_out = F.interpolate(feat_out, (H, W), mode=\'bilinear\', align_corners=True)\n        feat_out16 = F.interpolate(feat_out16, (H, W), mode=\'bilinear\', align_corners=True)\n        feat_out32 = F.interpolate(feat_out32, (H, W), mode=\'bilinear\', align_corners=True)\n        return feat_out, feat_out16, feat_out32\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = [], [], [], []\n        for name, child in self.named_children():\n            child_wd_params, child_nowd_params = child.get_params()\n            if isinstance(child, FeatureFusionModule) or isinstance(child, BiSeNetOutput):\n                lr_mul_wd_params += child_wd_params\n                lr_mul_nowd_params += child_nowd_params\n            else:\n                wd_params += child_wd_params\n                nowd_params += child_nowd_params\n        return wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params\n\n\nif __name__ == ""__main__"":\n    net = BiSeNet(19)\n    net.cuda()\n    net.eval()\n    in_ten = torch.randn(16, 3, 640, 480).cuda()\n    out, out16, out32 = net(in_ten)\n    print(out.shape)\n\n    net.get_params()\n'"
diss/train.py,9,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport sys\nsys.path.insert(0, \'.\')\nfrom logger import setup_logger\nfrom diss.model import BiSeNet\nfrom cityscapes import CityScapes\nfrom loss import OhemCELoss\nfrom diss.evaluate import evaluate\nfrom optimizer import Optimizer\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torch.distributed as dist\n\nimport os\nimport os.path as osp\nimport logging\nimport time\nimport datetime\nimport argparse\n\n\nrespth = \'./res\'\nif not osp.exists(respth): os.makedirs(respth)\nlogger = logging.getLogger()\n\n\ndef parse_args():\n    parse = argparse.ArgumentParser()\n    parse.add_argument(\n            \'--local_rank\',\n            dest = \'local_rank\',\n            type = int,\n            default = -1,\n            )\n    return parse.parse_args()\n\n\ndef train():\n    args = parse_args()\n    torch.cuda.set_device(args.local_rank)\n    dist.init_process_group(\n                backend = \'nccl\',\n                init_method = \'tcp://127.0.0.1:33241\',\n                world_size = torch.cuda.device_count(),\n                rank=args.local_rank\n                )\n    setup_logger(respth)\n\n    ## dataset\n    n_classes = 19\n    n_img_per_gpu = 8\n    n_workers = 4\n    cropsize = [1024, 1024]\n    ds = CityScapes(\'./data\', cropsize=cropsize, mode=\'train\')\n    sampler = torch.utils.data.distributed.DistributedSampler(ds)\n    dl = DataLoader(ds,\n                    batch_size = n_img_per_gpu,\n                    shuffle = False,\n                    sampler = sampler,\n                    num_workers = n_workers,\n                    pin_memory = True,\n                    drop_last = True)\n\n    ## model\n    ignore_idx = 255\n    net = BiSeNet(n_classes=n_classes)\n    net.cuda()\n    net.train()\n    net = nn.parallel.DistributedDataParallel(net,\n            device_ids = [args.local_rank, ],\n            output_device = args.local_rank\n            )\n    score_thres = 0.7\n    n_min = n_img_per_gpu*cropsize[0]*cropsize[1]//16\n    LossP = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n    Loss2 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n    Loss3 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n\n    ## optimizer\n    momentum = 0.9\n    weight_decay = 5e-4\n    lr_start = 1e-2\n    max_iter = 80000\n    power = 0.9\n    warmup_steps = 1000\n    warmup_start_lr = 1e-5\n    optim = Optimizer(\n            model = net.module,\n            lr0 = lr_start,\n            momentum = momentum,\n            wd = weight_decay,\n            warmup_steps = warmup_steps,\n            warmup_start_lr = warmup_start_lr,\n            max_iter = max_iter,\n            power = power)\n\n    ## train loop\n    msg_iter = 50\n    loss_avg = []\n    st = glob_st = time.time()\n    diter = iter(dl)\n    epoch = 0\n    for it in range(max_iter):\n        try:\n            im, lb = next(diter)\n            if not im.size()[0]==n_img_per_gpu: raise StopIteration\n        except StopIteration:\n            epoch += 1\n            sampler.set_epoch(epoch)\n            diter = iter(dl)\n            im, lb = next(diter)\n        im = im.cuda()\n        lb = lb.cuda()\n        H, W = im.size()[2:]\n        lb = torch.squeeze(lb, 1)\n\n        optim.zero_grad()\n        out, out16, out32 = net(im)\n        lossp = LossP(out, lb)\n        loss2 = Loss2(out16, lb)\n        loss3 = Loss3(out32, lb)\n        loss = lossp + loss2 + loss3\n        loss.backward()\n        optim.step()\n\n        loss_avg.append(loss.item())\n        ## print training log message\n        if (it+1)%msg_iter==0:\n            loss_avg = sum(loss_avg) / len(loss_avg)\n            lr = optim.lr\n            ed = time.time()\n            t_intv, glob_t_intv = ed - st, ed - glob_st\n            eta = int((max_iter - it) * (glob_t_intv / it))\n            eta = str(datetime.timedelta(seconds=eta))\n            msg = \', \'.join([\n                    \'it: {it}/{max_it}\',\n                    \'lr: {lr:4f}\',\n                    \'loss: {loss:.4f}\',\n                    \'eta: {eta}\',\n                    \'time: {time:.4f}\',\n                ]).format(\n                    it = it+1,\n                    max_it = max_iter,\n                    lr = lr,\n                    loss = loss_avg,\n                    time = t_intv,\n                    eta = eta\n                )\n            logger.info(msg)\n            loss_avg = []\n            st = ed\n\n    ## dump the final model\n    save_pth = osp.join(respth, \'model_final_diss.pth\')\n    net.cpu()\n    state = net.module.state_dict() if hasattr(net, \'module\') else net.state_dict()\n    if dist.get_rank()==0: torch.save(state, save_pth)\n    logger.info(\'training done, model saved to: {}\'.format(save_pth))\n\n\nif __name__ == ""__main__"":\n    train()\n    evaluate()\n'"
fp16/__init__.py,0,b''
fp16/evaluate.py,12,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nfrom logger import setup_logger\nfrom fp16.model import BiSeNet\nfrom cityscapes import CityScapes\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torch.distributed as dist\n\nimport os\nimport os.path as osp\nimport logging\nimport time\nimport numpy as np\nfrom tqdm import tqdm\nimport math\n\n\n\nclass MscEval(object):\n    def __init__(self,\n            model,\n            dataloader,\n            scales = [0.5, 0.75, 1, 1.25, 1.5, 1.75],\n            n_classes = 19,\n            lb_ignore = 255,\n            cropsize = 1024,\n            flip = True,\n            *args, **kwargs):\n        self.scales = scales\n        self.n_classes = n_classes\n        self.lb_ignore = lb_ignore\n        self.flip = flip\n        self.cropsize = cropsize\n        ## dataloader\n        self.dl = dataloader\n        self.net = model\n\n\n    def pad_tensor(self, inten, size):\n        N, C, H, W = inten.size()\n        outten = torch.zeros(N, C, size[0], size[1]).cuda()\n        outten.requires_grad = False\n        margin_h, margin_w = size[0]-H, size[1]-W\n        hst, hed = margin_h//2, margin_h//2+H\n        wst, wed = margin_w//2, margin_w//2+W\n        outten[:, :, hst:hed, wst:wed] = inten\n        return outten, [hst, hed, wst, wed]\n\n\n    def eval_chip(self, crop):\n        with torch.no_grad():\n            out = self.net(crop)[0]\n            prob = F.softmax(out, 1)\n            if self.flip:\n                crop = torch.flip(crop, dims=(3,))\n                out = self.net(crop)[0]\n                out = torch.flip(out, dims=(3,))\n                prob += F.softmax(out, 1)\n            prob = torch.exp(prob)\n        return prob\n\n\n    def crop_eval(self, im):\n        cropsize = self.cropsize\n        stride_rate = 5/6.\n        N, C, H, W = im.size()\n        long_size, short_size = (H,W) if H>W else (W,H)\n        if long_size < cropsize:\n            im, indices = self.pad_tensor(im, (cropsize, cropsize))\n            prob = self.eval_chip(im)\n            prob = prob[:, :, indices[0]:indices[1], indices[2]:indices[3]]\n        else:\n            stride = math.ceil(cropsize*stride_rate)\n            if short_size < cropsize:\n                if H < W:\n                    im, indices = self.pad_tensor(im, (cropsize, W))\n                else:\n                    im, indices = self.pad_tensor(im, (H, cropsize))\n            N, C, H, W = im.size()\n            n_x = math.ceil((W-cropsize)/stride)+1\n            n_y = math.ceil((H-cropsize)/stride)+1\n            prob = torch.zeros(N, self.n_classes, H, W).cuda()\n            prob.requires_grad = False\n            for iy in range(n_y):\n                for ix in range(n_x):\n                    hed, wed = min(H, stride*iy+cropsize), min(W, stride*ix+cropsize)\n                    hst, wst = hed-cropsize, wed-cropsize\n                    chip = im[:, :, hst:hed, wst:wed]\n                    prob_chip = self.eval_chip(chip)\n                    prob[:, :, hst:hed, wst:wed] += prob_chip\n            if short_size < cropsize:\n                prob = prob[:, :, indices[0]:indices[1], indices[2]:indices[3]]\n        return prob\n\n\n    def scale_crop_eval(self, im, scale):\n        N, C, H, W = im.size()\n        new_hw = [int(H*scale), int(W*scale)]\n        im = F.interpolate(im, new_hw, mode=\'bilinear\', align_corners=True)\n        prob = self.crop_eval(im)\n        prob = F.interpolate(prob, (H, W), mode=\'bilinear\', align_corners=True)\n        return prob\n\n\n    def compute_hist(self, pred, lb):\n        n_classes = self.n_classes\n        ignore_idx = self.lb_ignore\n        keep = np.logical_not(lb==ignore_idx)\n        merge = pred[keep] * n_classes + lb[keep]\n        hist = np.bincount(merge, minlength=n_classes**2)\n        hist = hist.reshape((n_classes, n_classes))\n        return hist\n\n\n    def evaluate(self):\n        ## evaluate\n        n_classes = self.n_classes\n        hist = np.zeros((n_classes, n_classes), dtype=np.float32)\n        dloader = tqdm(self.dl)\n        if dist.is_initialized() and not dist.get_rank()==0:\n            dloader = self.dl\n        for i, (imgs, label) in enumerate(dloader):\n            N, _, H, W = label.shape\n            probs = torch.zeros((N, self.n_classes, H, W))\n            probs.requires_grad = False\n            imgs = imgs.cuda()\n            for sc in self.scales:\n                prob = self.scale_crop_eval(imgs, sc)\n                probs += prob.detach().cpu()\n            probs = probs.data.numpy()\n            preds = np.argmax(probs, axis=1)\n\n            hist_once = self.compute_hist(preds, label.data.numpy().squeeze(1))\n            hist = hist + hist_once\n        IOUs = np.diag(hist) / (np.sum(hist, axis=0)+np.sum(hist, axis=1)-np.diag(hist))\n        mIOU = np.mean(IOUs)\n        return mIOU\n\n\ndef evaluate(respth=\'./res\', dspth=\'./data\'):\n    ## logger\n    logger = logging.getLogger()\n\n    ## model\n    logger.info(\'\\n\')\n    logger.info(\'====\'*20)\n    logger.info(\'evaluating the model ...\\n\')\n    logger.info(\'setup and restore model\')\n    n_classes = 19\n    net = BiSeNet(n_classes=n_classes)\n    save_pth = osp.join(respth, \'model_final.pth\')\n    net.load_state_dict(torch.load(save_pth))\n    net.cuda()\n    net.eval()\n\n    ## dataset\n    batchsize = 5\n    n_workers = 2\n    dsval = CityScapes(dspth, mode=\'val\')\n    dl = DataLoader(dsval,\n                    batch_size = batchsize,\n                    shuffle = False,\n                    num_workers = n_workers,\n                    drop_last = False)\n\n    ## evaluator\n    logger.info(\'compute the mIOU\')\n    evaluator = MscEval(net, dl)\n\n    ## eval\n    mIOU = evaluator.evaluate()\n    logger.info(\'mIOU is: {:.6f}\'.format(mIOU))\n\n\n\nif __name__ == ""__main__"":\n    setup_logger(\'./res\')\n    evaluate()\n'"
fp16/model.py,7,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\n\nfrom .resnet import Resnet18\n\nfrom torch.nn import BatchNorm2d\n\n\nclass ConvBNReLU(nn.Module):\n    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1, *args, **kwargs):\n        super(ConvBNReLU, self).__init__()\n        self.conv = nn.Conv2d(in_chan,\n                out_chan,\n                kernel_size = ks,\n                stride = stride,\n                padding = padding,\n                bias = False)\n        self.bn = BatchNorm2d(out_chan)\n        self.relu = nn.ReLU(inplace=True)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n\nclass BiSeNetOutput(nn.Module):\n    def __init__(self, in_chan, mid_chan, n_classes, *args, **kwargs):\n        super(BiSeNetOutput, self).__init__()\n        self.conv = ConvBNReLU(in_chan, mid_chan, ks=3, stride=1, padding=1)\n        self.conv_out = nn.Conv2d(mid_chan, n_classes, kernel_size=1, bias=False)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.conv_out(x)\n        return x\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.modules.batchnorm._BatchNorm):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass AttentionRefinementModule(nn.Module):\n    def __init__(self, in_chan, out_chan, *args, **kwargs):\n        super(AttentionRefinementModule, self).__init__()\n        self.conv = ConvBNReLU(in_chan, out_chan, ks=3, stride=1, padding=1)\n        self.conv_atten = nn.Conv2d(out_chan, out_chan, kernel_size= 1, bias=False)\n        self.bn_atten = BatchNorm2d(out_chan)\n        self.sigmoid_atten = nn.Sigmoid()\n        self.init_weight()\n\n    def forward(self, x):\n        feat = self.conv(x)\n        atten = F.avg_pool2d(feat, feat.size()[2:])\n        atten = self.conv_atten(atten)\n        atten = self.bn_atten(atten)\n        atten = self.sigmoid_atten(atten)\n        out = torch.mul(feat, atten)\n        return out\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n\nclass ContextPath(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(ContextPath, self).__init__()\n        self.resnet = Resnet18()\n        self.arm16 = AttentionRefinementModule(256, 128)\n        self.arm32 = AttentionRefinementModule(512, 128)\n        self.conv_head32 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n        self.conv_head16 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n        self.conv_avg = ConvBNReLU(512, 128, ks=1, stride=1, padding=0)\n\n        self.init_weight()\n\n    def forward(self, x):\n        H0, W0 = x.size()[2:]\n        feat8, feat16, feat32 = self.resnet(x)\n        H8, W8 = feat8.size()[2:]\n        H16, W16 = feat16.size()[2:]\n        H32, W32 = feat32.size()[2:]\n\n        avg = F.avg_pool2d(feat32, feat32.size()[2:])\n        avg = self.conv_avg(avg)\n        avg_up = F.interpolate(avg, (H32, W32), mode=\'nearest\')\n\n        feat32_arm = self.arm32(feat32)\n        feat32_sum = feat32_arm + avg_up\n        feat32_up = F.interpolate(feat32_sum, (H16, W16), mode=\'nearest\')\n        feat32_up = self.conv_head32(feat32_up)\n\n        feat16_arm = self.arm16(feat16)\n        feat16_sum = feat16_arm + feat32_up\n        feat16_up = F.interpolate(feat16_sum, (H8, W8), mode=\'nearest\')\n        feat16_up = self.conv_head16(feat16_up)\n\n        return feat16_up, feat32_up # x8, x16\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.modules.batchnorm._BatchNorm):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass SpatialPath(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(SpatialPath, self).__init__()\n        self.conv1 = ConvBNReLU(3, 64, ks=7, stride=2, padding=3)\n        self.conv2 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n        self.conv3 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n        self.conv_out = ConvBNReLU(64, 128, ks=1, stride=1, padding=0)\n        self.init_weight()\n\n    def forward(self, x):\n        feat = self.conv1(x)\n        feat = self.conv2(feat)\n        feat = self.conv3(feat)\n        feat = self.conv_out(feat)\n        return feat\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.modules.batchnorm._BatchNorm):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass FeatureFusionModule(nn.Module):\n    def __init__(self, in_chan, out_chan, *args, **kwargs):\n        super(FeatureFusionModule, self).__init__()\n        self.convblk = ConvBNReLU(in_chan, out_chan, ks=1, stride=1, padding=0)\n        self.conv1 = nn.Conv2d(out_chan,\n                out_chan//4,\n                kernel_size = 1,\n                stride = 1,\n                padding = 0,\n                bias = False)\n        self.conv2 = nn.Conv2d(out_chan//4,\n                out_chan,\n                kernel_size = 1,\n                stride = 1,\n                padding = 0,\n                bias = False)\n        self.relu = nn.ReLU(inplace=True)\n        self.sigmoid = nn.Sigmoid()\n        self.init_weight()\n\n    def forward(self, fsp, fcp):\n        fcat = torch.cat([fsp, fcp], dim=1)\n        feat = self.convblk(fcat)\n        atten = F.avg_pool2d(feat, feat.size()[2:])\n        atten = self.conv1(atten)\n        atten = self.relu(atten)\n        atten = self.conv2(atten)\n        atten = self.sigmoid(atten)\n        feat_atten = torch.mul(feat, atten)\n        feat_out = feat_atten + feat\n        return feat_out\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.modules.batchnorm._BatchNorm):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass BiSeNet(nn.Module):\n    def __init__(self, n_classes, *args, **kwargs):\n        super(BiSeNet, self).__init__()\n        self.cp = ContextPath()\n        self.sp = SpatialPath()\n        self.ffm = FeatureFusionModule(256, 256)\n        self.conv_out = BiSeNetOutput(256, 256, n_classes)\n        self.conv_out16 = BiSeNetOutput(128, 64, n_classes)\n        self.conv_out32 = BiSeNetOutput(128, 64, n_classes)\n        self.init_weight()\n\n    def forward(self, x):\n        H, W = x.size()[2:]\n        feat_cp8, feat_cp16 = self.cp(x)\n        feat_sp = self.sp(x)\n        feat_fuse = self.ffm(feat_sp, feat_cp8)\n\n        feat_out = self.conv_out(feat_fuse)\n        feat_out16 = self.conv_out16(feat_cp8)\n        feat_out32 = self.conv_out32(feat_cp16)\n\n        feat_out = F.interpolate(feat_out, (H, W), mode=\'bilinear\', align_corners=True)\n        feat_out16 = F.interpolate(feat_out16, (H, W), mode=\'bilinear\', align_corners=True)\n        feat_out32 = F.interpolate(feat_out32, (H, W), mode=\'bilinear\', align_corners=True)\n        return feat_out, feat_out16, feat_out32\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = [], [], [], []\n        for name, child in self.named_children():\n            child_wd_params, child_nowd_params = child.get_params()\n            if isinstance(child, (FeatureFusionModule, BiSeNetOutput)):\n                lr_mul_wd_params += child_wd_params\n                lr_mul_nowd_params += child_nowd_params\n            else:\n                wd_params += child_wd_params\n                nowd_params += child_nowd_params\n        return wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params\n\n\nif __name__ == ""__main__"":\n    net = BiSeNet(19)\n    net.cuda()\n    net.eval()\n    in_ten = torch.randn(16, 3, 640, 480).cuda()\n    out, out16, out32 = net(in_ten)\n    print(out.shape)\n\n    net.get_params()\n'"
fp16/resnet.py,6,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as modelzoo\n\nresnet18_url = \'https://download.pytorch.org/models/resnet18-5c106cde.pth\'\n\n#  from torch.nn import BatchNorm2d\ndef BatchNorm2d(out_chan):\n    return nn.SyncBatchNorm.convert_sync_batchnorm(nn.BatchNorm2d(out_chan))\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_chan, out_chan, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(in_chan, out_chan, stride)\n        self.bn1 = BatchNorm2d(out_chan)\n        self.conv2 = conv3x3(out_chan, out_chan)\n        self.bn2 = BatchNorm2d(out_chan)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = None\n        if in_chan != out_chan or stride != 1:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_chan, out_chan,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm2d(out_chan),\n                )\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = self.bn1(residual)\n        residual = self.relu(residual)\n        residual = self.conv2(residual)\n        residual = self.bn2(residual)\n\n        shortcut = x\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n\n        out = shortcut + residual\n        out = self.relu(out)\n        return out\n\n\ndef create_layer_basic(in_chan, out_chan, bnum, stride=1):\n    layers = [BasicBlock(in_chan, out_chan, stride=stride)]\n    for i in range(bnum-1):\n        layers.append(BasicBlock(out_chan, out_chan, stride=1))\n    return nn.Sequential(*layers)\n\n\nclass Resnet18(nn.Module):\n    def __init__(self):\n        super(Resnet18, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = create_layer_basic(64, 64, bnum=2, stride=1)\n        self.layer2 = create_layer_basic(64, 128, bnum=2, stride=2)\n        self.layer3 = create_layer_basic(128, 256, bnum=2, stride=2)\n        self.layer4 = create_layer_basic(256, 512, bnum=2, stride=2)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        feat8 = self.layer2(x) # 1/8\n        feat16 = self.layer3(feat8) # 1/16\n        feat32 = self.layer4(feat16) # 1/32\n        return feat8, feat16, feat32\n\n    def init_weight(self):\n        state_dict = modelzoo.load_url(resnet18_url)\n        self_state_dict = self.state_dict()\n        for k, v in state_dict.items():\n            if \'fc\' in k: continue\n            self_state_dict.update({k: v})\n        self.load_state_dict(self_state_dict)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.modules.batchnorm._BatchNorm):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nif __name__ == ""__main__"":\n    net = Resnet18()\n    x = torch.randn(16, 3, 224, 224)\n    out = net(x)\n    print(out[0].size())\n    print(out[1].size())\n    print(out[2].size())\n    net.get_params()\n'"
fp16/train.py,9,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nimport sys\nsys.path.insert(0, \'.\')\nfrom logger import setup_logger\nfrom fp16.model import BiSeNet\nfrom cityscapes import CityScapes\nfrom loss import OhemCELoss\nfrom fp16.evaluate import evaluate\nfrom optimizer import Optimizer\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torch.distributed as dist\nfrom apex import amp\n\nimport os\nimport os.path as osp\nimport logging\nimport time\nimport datetime\nimport argparse\n\n\nrespth = \'./res\'\nif not osp.exists(respth): os.makedirs(respth)\nlogger = logging.getLogger()\n\n\ndef parse_args():\n    parse = argparse.ArgumentParser()\n    parse.add_argument(\n            \'--local_rank\',\n            dest = \'local_rank\',\n            type = int,\n            default = -1,\n            )\n    return parse.parse_args()\n\n\ndef train():\n    args = parse_args()\n    torch.cuda.set_device(args.local_rank)\n    dist.init_process_group(\n                backend = \'nccl\',\n                init_method = \'tcp://127.0.0.1:33271\',\n                world_size = torch.cuda.device_count(),\n                rank=args.local_rank\n                )\n    setup_logger(respth)\n\n    ## dataset\n    n_classes = 19\n    n_img_per_gpu = 8\n    n_workers = 4\n    cropsize = [1024, 1024]\n    ds = CityScapes(\'./data\', cropsize=cropsize, mode=\'train\')\n    sampler = torch.utils.data.distributed.DistributedSampler(ds)\n    dl = DataLoader(ds,\n                    batch_size = n_img_per_gpu,\n                    shuffle = False,\n                    sampler = sampler,\n                    num_workers = n_workers,\n                    pin_memory = True,\n                    drop_last = True)\n\n    ## model\n    ignore_idx = 255\n    net = BiSeNet(n_classes=n_classes)\n    net = nn.SyncBatchNorm.convert_sync_batchnorm(net)\n    net.cuda()\n    net.train()\n    score_thres = 0.7\n    n_min = n_img_per_gpu*cropsize[0]*cropsize[1]//16\n    criteria_p = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n    criteria_16 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n    criteria_32 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n\n    ## optimizer\n    momentum = 0.9\n    weight_decay = 5e-4\n    lr_start = 1e-2\n    max_iter = 80000\n    power = 0.9\n    warmup_steps = 1000\n    warmup_start_lr = 1e-5\n    optim = Optimizer(\n            #  model = net.module,\n            model = net,\n            lr0 = lr_start,\n            momentum = momentum,\n            wd = weight_decay,\n            warmup_steps = warmup_steps,\n            warmup_start_lr = warmup_start_lr,\n            max_iter = max_iter,\n            power = power)\n\n    ## fp16\n    net, opt = amp.initialize(net, optim.optim, opt_level=\'O1\')\n    optim.optim = opt\n\n    ## set dist\n    net = nn.parallel.DistributedDataParallel(net,\n            device_ids = [args.local_rank, ],\n            output_device = args.local_rank\n            )\n\n    ## train loop\n    msg_iter = 50\n    loss_avg = []\n    st = glob_st = time.time()\n    diter = iter(dl)\n    epoch = 0\n    for it in range(max_iter):\n        try:\n            im, lb = next(diter)\n            if not im.size()[0]==n_img_per_gpu: raise StopIteration\n        except StopIteration:\n            epoch += 1\n            sampler.set_epoch(epoch)\n            diter = iter(dl)\n            im, lb = next(diter)\n        im = im.cuda()\n        lb = lb.cuda()\n        H, W = im.size()[2:]\n        lb = torch.squeeze(lb, 1)\n\n        optim.zero_grad()\n        out, out16, out32 = net(im)\n        lossp = criteria_p(out, lb)\n        loss2 = criteria_16(out16, lb)\n        loss3 = criteria_32(out32, lb)\n        loss = lossp + loss2 + loss3\n        #  loss.backward()\n        with amp.scale_loss(loss, opt) as scaled_loss:\n            scaled_loss.backward()\n        optim.step()\n\n        loss_avg.append(loss.item())\n        ## print training log message\n        if (it+1)%msg_iter==0:\n            loss_avg = sum(loss_avg) / len(loss_avg)\n            lr = optim.lr\n            ed = time.time()\n            t_intv, glob_t_intv = ed - st, ed - glob_st\n            eta = int((max_iter - it) * (glob_t_intv / it))\n            eta = str(datetime.timedelta(seconds=eta))\n            msg = \', \'.join([\n                    \'it: {it}/{max_it}\',\n                    \'lr: {lr:4f}\',\n                    \'loss: {loss:.4f}\',\n                    \'eta: {eta}\',\n                    \'time: {time:.4f}\',\n                ]).format(\n                    it = it+1,\n                    max_it = max_iter,\n                    lr = lr,\n                    loss = loss_avg,\n                    time = t_intv,\n                    eta = eta\n                )\n            logger.info(msg)\n            loss_avg = []\n            st = ed\n\n    ## dump the final model\n    save_pth = osp.join(respth, \'model_final.pth\')\n    net.cpu()\n    state = net.module.state_dict() if hasattr(net, \'module\') else net.state_dict()\n    if dist.get_rank()==0: torch.save(state, save_pth)\n    logger.info(\'training done, model saved to: {}\'.format(save_pth))\n\n\nif __name__ == ""__main__"":\n    train()\n    evaluate()\n'"
modules/__init__.py,0,"b'from .bn import ABN, InPlaceABN, InPlaceABNSync\nfrom .functions import ACT_RELU, ACT_LEAKY_RELU, ACT_ELU, ACT_NONE\nfrom .misc import GlobalAvgPool2d, SingleGPU\nfrom .residual import IdentityResidualBlock\nfrom .dense import DenseModule\n'"
modules/bn.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as functional\n\ntry:\n    from queue import Queue\nexcept ImportError:\n    from Queue import Queue\n\nfrom .functions import *\n\n\nclass ABN(nn.Module):\n    """"""Activated Batch Normalization\n\n    This gathers a `BatchNorm2d` and an activation function in a single module\n    """"""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, activation=""leaky_relu"", slope=0.01):\n        """"""Creates an Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        eps : float\n            Small constant to prevent numerical issues.\n        momentum : float\n            Momentum factor applied to compute running statistics as.\n        affine : bool\n            If `True` apply learned scale and shift transformation after normalization.\n        activation : str\n            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n        """"""\n        super(ABN, self).__init__()\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n        self.momentum = momentum\n        self.activation = activation\n        self.slope = slope\n        if self.affine:\n            self.weight = nn.Parameter(torch.ones(num_features))\n            self.bias = nn.Parameter(torch.zeros(num_features))\n        else:\n            self.register_parameter(\'weight\', None)\n            self.register_parameter(\'bias\', None)\n        self.register_buffer(\'running_mean\', torch.zeros(num_features))\n        self.register_buffer(\'running_var\', torch.ones(num_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.constant_(self.running_mean, 0)\n        nn.init.constant_(self.running_var, 1)\n        if self.affine:\n            nn.init.constant_(self.weight, 1)\n            nn.init.constant_(self.bias, 0)\n\n    def forward(self, x):\n        x = functional.batch_norm(x, self.running_mean, self.running_var, self.weight, self.bias,\n                                  self.training, self.momentum, self.eps)\n\n        if self.activation == ACT_RELU:\n            return functional.relu(x, inplace=True)\n        elif self.activation == ACT_LEAKY_RELU:\n            return functional.leaky_relu(x, negative_slope=self.slope, inplace=True)\n        elif self.activation == ACT_ELU:\n            return functional.elu(x, inplace=True)\n        else:\n            return x\n\n    def __repr__(self):\n        rep = \'{name}({num_features}, eps={eps}, momentum={momentum},\' \\\n              \' affine={affine}, activation={activation}\'\n        if self.activation == ""leaky_relu"":\n            rep += \', slope={slope})\'\n        else:\n            rep += \')\'\n        return rep.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass InPlaceABN(ABN):\n    """"""InPlace Activated Batch Normalization""""""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, activation=""leaky_relu"", slope=0.01):\n        """"""Creates an InPlace Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        eps : float\n            Small constant to prevent numerical issues.\n        momentum : float\n            Momentum factor applied to compute running statistics as.\n        affine : bool\n            If `True` apply learned scale and shift transformation after normalization.\n        activation : str\n            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n        """"""\n        super(InPlaceABN, self).__init__(num_features, eps, momentum, affine, activation, slope)\n\n    def forward(self, x):\n        return inplace_abn(x, self.weight, self.bias, self.running_mean, self.running_var,\n                           self.training, self.momentum, self.eps, self.activation, self.slope)\n\n\nclass InPlaceABNSync(ABN):\n    """"""InPlace Activated Batch Normalization with cross-GPU synchronization\n    This assumes that it will be replicated across GPUs using the same mechanism as in `nn.DistributedDataParallel`.\n    """"""\n\n    def forward(self, x):\n        return inplace_abn_sync(x, self.weight, self.bias, self.running_mean, self.running_var,\n                                   self.training, self.momentum, self.eps, self.activation, self.slope)\n\n    def __repr__(self):\n        rep = \'{name}({num_features}, eps={eps}, momentum={momentum},\' \\\n              \' affine={affine}, activation={activation}\'\n        if self.activation == ""leaky_relu"":\n            rep += \', slope={slope})\'\n        else:\n            rep += \')\'\n        return rep.format(name=self.__class__.__name__, **self.__dict__)\n\n\n'"
modules/deeplab.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as functional\n\nfrom models._util import try_index\nfrom .bn import ABN\n\n\nclass DeeplabV3(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 hidden_channels=256,\n                 dilations=(12, 24, 36),\n                 norm_act=ABN,\n                 pooling_size=None):\n        super(DeeplabV3, self).__init__()\n        self.pooling_size = pooling_size\n\n        self.map_convs = nn.ModuleList([\n            nn.Conv2d(in_channels, hidden_channels, 1, bias=False),\n            nn.Conv2d(in_channels, hidden_channels, 3, bias=False, dilation=dilations[0], padding=dilations[0]),\n            nn.Conv2d(in_channels, hidden_channels, 3, bias=False, dilation=dilations[1], padding=dilations[1]),\n            nn.Conv2d(in_channels, hidden_channels, 3, bias=False, dilation=dilations[2], padding=dilations[2])\n        ])\n        self.map_bn = norm_act(hidden_channels * 4)\n\n        self.global_pooling_conv = nn.Conv2d(in_channels, hidden_channels, 1, bias=False)\n        self.global_pooling_bn = norm_act(hidden_channels)\n\n        self.red_conv = nn.Conv2d(hidden_channels * 4, out_channels, 1, bias=False)\n        self.pool_red_conv = nn.Conv2d(hidden_channels, out_channels, 1, bias=False)\n        self.red_bn = norm_act(out_channels)\n\n        self.reset_parameters(self.map_bn.activation, self.map_bn.slope)\n\n    def reset_parameters(self, activation, slope):\n        gain = nn.init.calculate_gain(activation, slope)\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight.data, gain)\n                if hasattr(m, ""bias"") and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, ABN):\n                if hasattr(m, ""weight"") and m.weight is not None:\n                    nn.init.constant_(m.weight, 1)\n                if hasattr(m, ""bias"") and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        # Map convolutions\n        out = torch.cat([m(x) for m in self.map_convs], dim=1)\n        out = self.map_bn(out)\n        out = self.red_conv(out)\n\n        # Global pooling\n        pool = self._global_pooling(x)\n        pool = self.global_pooling_conv(pool)\n        pool = self.global_pooling_bn(pool)\n        pool = self.pool_red_conv(pool)\n        if self.training or self.pooling_size is None:\n            pool = pool.repeat(1, 1, x.size(2), x.size(3))\n\n        out += pool\n        out = self.red_bn(out)\n        return out\n\n    def _global_pooling(self, x):\n        if self.training or self.pooling_size is None:\n            pool = x.view(x.size(0), x.size(1), -1).mean(dim=-1)\n            pool = pool.view(x.size(0), x.size(1), 1, 1)\n        else:\n            pooling_size = (min(try_index(self.pooling_size, 0), x.shape[2]),\n                            min(try_index(self.pooling_size, 1), x.shape[3]))\n            padding = (\n                (pooling_size[1] - 1) // 2,\n                (pooling_size[1] - 1) // 2 if pooling_size[1] % 2 == 1 else (pooling_size[1] - 1) // 2 + 1,\n                (pooling_size[0] - 1) // 2,\n                (pooling_size[0] - 1) // 2 if pooling_size[0] % 2 == 1 else (pooling_size[0] - 1) // 2 + 1\n            )\n\n            pool = functional.avg_pool2d(x, pooling_size, stride=1)\n            pool = functional.pad(pool, pad=padding, mode=""replicate"")\n        return pool\n'"
modules/dense.py,3,"b'from collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\n\nfrom .bn import ABN\n\n\nclass DenseModule(nn.Module):\n    def __init__(self, in_channels, growth, layers, bottleneck_factor=4, norm_act=ABN, dilation=1):\n        super(DenseModule, self).__init__()\n        self.in_channels = in_channels\n        self.growth = growth\n        self.layers = layers\n\n        self.convs1 = nn.ModuleList()\n        self.convs3 = nn.ModuleList()\n        for i in range(self.layers):\n            self.convs1.append(nn.Sequential(OrderedDict([\n                (""bn"", norm_act(in_channels)),\n                (""conv"", nn.Conv2d(in_channels, self.growth * bottleneck_factor, 1, bias=False))\n            ])))\n            self.convs3.append(nn.Sequential(OrderedDict([\n                (""bn"", norm_act(self.growth * bottleneck_factor)),\n                (""conv"", nn.Conv2d(self.growth * bottleneck_factor, self.growth, 3, padding=dilation, bias=False,\n                                   dilation=dilation))\n            ])))\n            in_channels += self.growth\n\n    @property\n    def out_channels(self):\n        return self.in_channels + self.growth * self.layers\n\n    def forward(self, x):\n        inputs = [x]\n        for i in range(self.layers):\n            x = torch.cat(inputs, dim=1)\n            x = self.convs1[i](x)\n            x = self.convs3[i](x)\n            inputs += [x]\n\n        return torch.cat(inputs, dim=1)\n'"
modules/functions.py,6,"b'from os import path\nimport torch \nimport torch.distributed as dist\nimport torch.autograd as autograd\nimport torch.cuda.comm as comm\nfrom torch.autograd.function import once_differentiable\nfrom torch.utils.cpp_extension import load\n\n_src_path = path.join(path.dirname(path.abspath(__file__)), ""src"")\n_backend = load(name=""inplace_abn"",\n                extra_cflags=[""-O3""],\n                sources=[path.join(_src_path, f) for f in [\n                    ""inplace_abn.cpp"",\n                    ""inplace_abn_cpu.cpp"",\n                    ""inplace_abn_cuda.cu"",\n                    ""inplace_abn_cuda_half.cu""\n                ]],\n                extra_cuda_cflags=[""--expt-extended-lambda""])\n\n# Activation names\nACT_RELU = ""relu""\nACT_LEAKY_RELU = ""leaky_relu""\nACT_ELU = ""elu""\nACT_NONE = ""none""\n\n\ndef _check(fn, *args, **kwargs):\n    success = fn(*args, **kwargs)\n    if not success:\n        raise RuntimeError(""CUDA Error encountered in {}"".format(fn))\n\n\ndef _broadcast_shape(x):\n    out_size = []\n    for i, s in enumerate(x.size()):\n        if i != 1:\n            out_size.append(1)\n        else:\n            out_size.append(s)\n    return out_size\n\n\ndef _reduce(x):\n    if len(x.size()) == 2:\n        return x.sum(dim=0)\n    else:\n        n, c = x.size()[0:2]\n        return x.contiguous().view((n, c, -1)).sum(2).sum(0)\n\n\ndef _count_samples(x):\n    count = 1\n    for i, s in enumerate(x.size()):\n        if i != 1:\n            count *= s\n    return count\n\n\ndef _act_forward(ctx, x):\n    if ctx.activation == ACT_LEAKY_RELU:\n        _backend.leaky_relu_forward(x, ctx.slope)\n    elif ctx.activation == ACT_ELU:\n        _backend.elu_forward(x)\n    elif ctx.activation == ACT_NONE:\n        pass\n\n\ndef _act_backward(ctx, x, dx):\n    if ctx.activation == ACT_LEAKY_RELU:\n        _backend.leaky_relu_backward(x, dx, ctx.slope)\n    elif ctx.activation == ACT_ELU:\n        _backend.elu_backward(x, dx)\n    elif ctx.activation == ACT_NONE:\n        pass\n\n\nclass InPlaceABN(autograd.Function):\n    @staticmethod\n    def forward(ctx, x, weight, bias, running_mean, running_var,\n                training=True, momentum=0.1, eps=1e-05, activation=ACT_LEAKY_RELU, slope=0.01):\n        # Save context\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n        ctx.affine = weight is not None and bias is not None\n\n        # Prepare inputs\n        count = _count_samples(x)\n        x = x.contiguous()\n        weight = weight.contiguous() if ctx.affine else x.new_empty(0)\n        bias = bias.contiguous() if ctx.affine else x.new_empty(0)\n\n        if ctx.training:\n            mean, var = _backend.mean_var(x)\n\n            # Update running stats\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * var * count / (count - 1))\n\n            # Mark in-place modified tensors\n            ctx.mark_dirty(x, running_mean, running_var)\n        else:\n            mean, var = running_mean.contiguous(), running_var.contiguous()\n            ctx.mark_dirty(x)\n\n        # BN forward + activation\n        _backend.forward(x, mean, var, weight, bias, ctx.affine, ctx.eps)\n        _act_forward(ctx, x)\n\n        # Output\n        ctx.var = var\n        ctx.save_for_backward(x, var, weight, bias)\n        return x\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        z, var, weight, bias = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # Undo activation\n        _act_backward(ctx, z, dz)\n\n        if ctx.training:\n            edz, eydz = _backend.edz_eydz(z, dz, weight, bias, ctx.affine, ctx.eps)\n        else:\n            # TODO: implement simplified CUDA backward for inference mode\n            edz = dz.new_zeros(dz.size(1))\n            eydz = dz.new_zeros(dz.size(1))\n\n        dx = _backend.backward(z, dz, var, weight, bias, edz, eydz, ctx.affine, ctx.eps)\n        dweight = eydz * weight.sign() if ctx.affine else None\n        dbias = edz if ctx.affine else None\n\n        return dx, dweight, dbias, None, None, None, None, None, None, None\n\nclass InPlaceABNSync(autograd.Function):\n    @classmethod\n    def forward(cls, ctx, x, weight, bias, running_mean, running_var,\n                training=True, momentum=0.1, eps=1e-05, activation=ACT_LEAKY_RELU, slope=0.01, equal_batches=True):\n        # Save context\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n        ctx.affine = weight is not None and bias is not None\n\n        # Prepare inputs\n        ctx.world_size = dist.get_world_size() if dist.is_initialized() else 1\n\n        #count = _count_samples(x)\n        batch_size = x.new_tensor([x.shape[0]],dtype=torch.long)\n\n        x = x.contiguous()\n        weight = weight.contiguous() if ctx.affine else x.new_empty(0)\n        bias = bias.contiguous() if ctx.affine else x.new_empty(0)\n\n        if ctx.training:\n            mean, var = _backend.mean_var(x)\n            if ctx.world_size>1:\n                # get global batch size\n                if equal_batches:\n                    batch_size *= ctx.world_size\n                else:\n                    dist.all_reduce(batch_size, dist.ReduceOp.SUM)\n\n                ctx.factor = x.shape[0]/float(batch_size.item())\n\n                mean_all = mean.clone() * ctx.factor\n                dist.all_reduce(mean_all, dist.ReduceOp.SUM)\n\n                var_all = (var + (mean - mean_all) ** 2) * ctx.factor\n                dist.all_reduce(var_all, dist.ReduceOp.SUM)\n\n                mean = mean_all\n                var = var_all\n\n            # Update running stats\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\n            count = batch_size.item() * x.view(x.shape[0],x.shape[1],-1).shape[-1]\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * var * (float(count) / (count - 1)))\n\n            # Mark in-place modified tensors\n            ctx.mark_dirty(x, running_mean, running_var)\n        else:\n            mean, var = running_mean.contiguous(), running_var.contiguous()\n            ctx.mark_dirty(x)\n\n        # BN forward + activation\n        _backend.forward(x, mean, var, weight, bias, ctx.affine, ctx.eps)\n        _act_forward(ctx, x)\n\n        # Output\n        ctx.var = var\n        ctx.save_for_backward(x, var, weight, bias)\n        return x\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        z, var, weight, bias = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # Undo activation\n        _act_backward(ctx, z, dz)\n\n        if ctx.training:\n            edz, eydz = _backend.edz_eydz(z, dz, weight, bias, ctx.affine, ctx.eps)\n            edz_local = edz.clone()\n            eydz_local = eydz.clone()\n\n            if ctx.world_size>1:\n                edz *= ctx.factor\n                dist.all_reduce(edz, dist.ReduceOp.SUM)\n\n                eydz *= ctx.factor\n                dist.all_reduce(eydz, dist.ReduceOp.SUM)\n        else:\n            edz_local = edz = dz.new_zeros(dz.size(1))\n            eydz_local = eydz = dz.new_zeros(dz.size(1))\n\n        dx = _backend.backward(z, dz, var, weight, bias, edz, eydz, ctx.affine, ctx.eps)\n        dweight = eydz_local * weight.sign() if ctx.affine else None\n        dbias = edz_local if ctx.affine else None\n\n        return dx, dweight, dbias, None, None, None, None, None, None, None\n\ninplace_abn = InPlaceABN.apply\ninplace_abn_sync = InPlaceABNSync.apply\n\n__all__ = [""inplace_abn"", ""inplace_abn_sync"", ""ACT_RELU"", ""ACT_LEAKY_RELU"", ""ACT_ELU"", ""ACT_NONE""]\n'"
modules/misc.py,2,"b'import torch.nn as nn\nimport torch\nimport torch.distributed as dist\n\nclass GlobalAvgPool2d(nn.Module):\n    def __init__(self):\n        """"""Global average pooling over the input\'s spatial dimensions""""""\n        super(GlobalAvgPool2d, self).__init__()\n\n    def forward(self, inputs):\n        in_size = inputs.size()\n        return inputs.view((in_size[0], in_size[1], -1)).mean(dim=2)\n\nclass SingleGPU(nn.Module):\n    def __init__(self, module):\n        super(SingleGPU, self).__init__()\n        self.module=module\n\n    def forward(self, input):\n        return self.module(input.cuda(non_blocking=True))\n\n'"
modules/residual.py,1,"b'from collections import OrderedDict\n\nimport torch.nn as nn\n\nfrom .bn import ABN\n\n\nclass IdentityResidualBlock(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 channels,\n                 stride=1,\n                 dilation=1,\n                 groups=1,\n                 norm_act=ABN,\n                 dropout=None):\n        """"""Configurable identity-mapping residual block\n\n        Parameters\n        ----------\n        in_channels : int\n            Number of input channels.\n        channels : list of int\n            Number of channels in the internal feature maps. Can either have two or three elements: if three construct\n            a residual block with two `3 x 3` convolutions, otherwise construct a bottleneck block with `1 x 1`, then\n            `3 x 3` then `1 x 1` convolutions.\n        stride : int\n            Stride of the first `3 x 3` convolution\n        dilation : int\n            Dilation to apply to the `3 x 3` convolutions.\n        groups : int\n            Number of convolution groups. This is used to create ResNeXt-style blocks and is only compatible with\n            bottleneck blocks.\n        norm_act : callable\n            Function to create normalization / activation Module.\n        dropout: callable\n            Function to create Dropout Module.\n        """"""\n        super(IdentityResidualBlock, self).__init__()\n\n        # Check parameters for inconsistencies\n        if len(channels) != 2 and len(channels) != 3:\n            raise ValueError(""channels must contain either two or three values"")\n        if len(channels) == 2 and groups != 1:\n            raise ValueError(""groups > 1 are only valid if len(channels) == 3"")\n\n        is_bottleneck = len(channels) == 3\n        need_proj_conv = stride != 1 or in_channels != channels[-1]\n\n        self.bn1 = norm_act(in_channels)\n        if not is_bottleneck:\n            layers = [\n                (""conv1"", nn.Conv2d(in_channels, channels[0], 3, stride=stride, padding=dilation, bias=False,\n                                    dilation=dilation)),\n                (""bn2"", norm_act(channels[0])),\n                (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,\n                                    dilation=dilation))\n            ]\n            if dropout is not None:\n                layers = layers[0:2] + [(""dropout"", dropout())] + layers[2:]\n        else:\n            layers = [\n                (""conv1"", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),\n                (""bn2"", norm_act(channels[0])),\n                (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,\n                                    groups=groups, dilation=dilation)),\n                (""bn3"", norm_act(channels[1])),\n                (""conv3"", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False))\n            ]\n            if dropout is not None:\n                layers = layers[0:4] + [(""dropout"", dropout())] + layers[4:]\n        self.convs = nn.Sequential(OrderedDict(layers))\n\n        if need_proj_conv:\n            self.proj_conv = nn.Conv2d(in_channels, channels[-1], 1, stride=stride, padding=0, bias=False)\n\n    def forward(self, x):\n        if hasattr(self, ""proj_conv""):\n            bn1 = self.bn1(x)\n            shortcut = self.proj_conv(bn1)\n        else:\n            shortcut = x.clone()\n            bn1 = self.bn1(x)\n\n        out = self.convs(bn1)\n        out.add_(shortcut)\n\n        return out\n'"
