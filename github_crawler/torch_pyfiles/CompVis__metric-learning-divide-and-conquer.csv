file_path,api_count,code
browse_results.py,0,"b""import shelve\nfrom collections import defaultdict\nimport sys\nimport os\nimport numpy as np\nimport pandas as pd\nimport time\nimport glob\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('exp_dir', type = str)\nparser.add_argument('-cw', '--col-width', type=int, default=100)\nargs = parser.parse_args()\nprint(args)\n\nprint('exp_dir=', args.exp_dir)\n\nfiles = sorted(list(map(lambda x: x[:-4], glob.glob(os.path.join(args.exp_dir, '*.dat')))))\n\n\nresults = defaultdict(list)\nks = [1, 2, 4, 8, 10, 20, 30, 50]\ncolumns=[\n        'epoch',\n         *['R@{}'.format(i) for i in ks],\n        ]\n\nlast_modified = None\n\nfor p in files:\n    try:\n        db = shelve.open(p)\n        log_path = p + '.log'\n        assert os.path.exists(log_path), log_path\n        last_modified = (time.time() - os.path.getmtime(p + '.log')) / 60\n    except:\n        print('Failed to open', p)\n    try:\n        p = os.path.basename(p)\n        cur_results_t = np.array([(epoch, *d['score']['recall'])\n                             for (epoch, d) in db['metrics'].items()])\n        cur_results = np.zeros((cur_results_t.shape[0], 1 + len(ks)), dtype=float)\n        cur_results[:, :] = np.nan\n        cur_results[:, :2] = cur_results_t[:, :2]\n        # TODO: maybe rename args to config\n        if db['config']['dataset_selected'] == 'inshop':\n           cur_results[:, 5:] = cur_results_t[:, 2:]\n        else:\n           cur_results[:, 2:5] = cur_results_t[:, 2:]\n\n    except Exception as e:\n        print(p, e)\n        print(db['config'])\n\n    idx_max_recall = cur_results[:, 1].argmax()\n    best_epoch_results = cur_results[idx_max_recall]\n    max_epoch = cur_results[:, 0].max()\n    best_epoch_results = best_epoch_results.tolist()\n    best_epoch_results[0] = '{:02}/{:02}'.format(int(best_epoch_results[0]), int(max_epoch))\n    assert len(best_epoch_results) == len(columns)\n\n    for i, col_name in enumerate(columns):\n        results[col_name].append(best_epoch_results[i])\n\n    # if the file was last modified < 10 minute ago; than print Running status\n    if last_modified is None:\n        results['S'].append('?')\n    elif last_modified > 10:\n        results['S'].append('-')\n    else:\n        results['S'].append('[R]')\n\n\ndf = pd.DataFrame(index=list(map(os.path.basename, files)),\n                  data=results)\n\npd.set_option('display.max_rows', 10000)\npd.set_option('display.max_columns', 10000)\npd.set_option('display.max_colwidth', args.col_width)\npd.set_option('display.width', 1000000)\ndf.sort_values(by=['R@1'], inplace=True)\nprint(df)\n"""
experiment.py,0,"b""from __future__ import print_function\n\nimport argparse\nimport math\nimport matplotlib\nimport sys\n\nimport train\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--nb-clusters', required = True, type = int)\n    parser.add_argument('--dataset', dest = 'dataset_selected',\n        choices=['sop', 'inshop', 'vid'], required = True\n    )\n    parser.add_argument('--nb-epochs', type = int, default=200)\n    parser.add_argument('--finetune-epoch', type = int, default=190)\n    parser.add_argument('--mod-epoch', type = int, default=2)\n    parser.add_argument('--num-workers', default=4, type=int)\n    parser.add_argument('--sz-batch', type=int, default=80)\n    parser.add_argument('--sz-embedding', default=128, type=int)\n    parser.add_argument('--cuda-device', default = 0, type = int)\n    parser.add_argument('--exp', default='0', type=str, help='experiment identifier')\n    parser.add_argument('--dir', default='default', type=str)\n    parser.add_argument('--backend', default='faiss',\n        choices=('torch+sklearn', 'faiss', 'faiss-gpu')\n    )\n    parser.add_argument('--random-seed', default = 0, type = int)\n    parser.add_argument('--backbone-wd', default=1e-4, type=float)\n    parser.add_argument('--backbone-lr', default=1e-5, type=float)\n    parser.add_argument('--embedding-lr', default=1e-5, type=float)\n    parser.add_argument('--embedding-wd', default=1e-4, type=float)\n    parser.add_argument('--verbose', action = 'store_true')\n    args = vars(parser.parse_args())\n\n    config = train.load_config(config_name = 'config.json')\n\n    config['dataloader']['batch_size'] = args.pop('sz_batch')\n    config['dataloader']['num_workers'] = args.pop('num_workers')\n    config['recluster']['mod_epoch'] = args.pop('mod_epoch')\n    config['opt']['backbone']['lr'] = args.pop('backbone_lr')\n    config['opt']['backbone']['weight_decay'] = args.pop('backbone_wd')\n    config['opt']['embedding']['lr'] = args.pop('embedding_lr')\n    config['opt']['embedding']['weight_decay'] = args.pop('embedding_wd')\n\n    for k in args:\n        if k in config:\n            config[k] = args[k]\n\n    if config['nb_clusters'] == 1:\n        config['recluster']['enabled'] = False\n\n    config['log'] = {\n        'name': '{}-K-{}-M-{}-exp-{}'.format(\n            config['dataset_selected'],\n            config['nb_clusters'],\n            config['recluster']['mod_epoch'],\n            args['exp']\n        ),\n        'path': 'log/{}'.format(args['dir'])\n    }\n\n    # tkinter not installed on this system, use non-GUI backend\n    matplotlib.use('agg')\n    train.start(config)\n\n"""
train.py,6,"b'from __future__ import print_function\nfrom __future__ import division\n\nimport collections\nimport os\nimport matplotlib\nimport numpy as np\nimport logging\nimport torch\nimport time\nimport json\nimport random\nimport shelve\nfrom tqdm import tqdm\nimport lib\nfrom lib.clustering import make_clustered_dataloaders\nimport warnings\n\n\nwarnings.simplefilter(""ignore"", category=PendingDeprecationWarning)\nos.putenv(""OMP_NUM_THREADS"", ""8"")\n\n\ndef load_config(config_name):\n    with open(config_name, \'r\') as f:\n        config = json.load(f)\n    # config = json.load(open(config_name))\n    def eval_json(config):\n        for k in config:\n            if type(config[k]) != dict:\n                if type(config[k]) is str:\n                    # if python types, then evaluate str expressions\n                    if config[k][:5] in [\'range\', \'float\']:\n                        config[k] = eval(config[k])\n            else:\n                eval_json(config[k])\n    eval_json(config)\n    return config\n\n\ndef json_dumps(**kwargs):\n    # __repr__ may contain `\\n`, json replaces it by `\\\\n` + indent\n    return json.dumps(**kwargs).replace(\'\\\\n\', \'\\n    \')\n\n\nclass JSONEncoder(json.JSONEncoder):\n    def default(self, x):\n        # add encoding for other types if necessary\n        if isinstance(x, range):\n            return \'range({}, {})\'.format(x.start, x.stop)\n        if not isinstance(x, (int, str, list, float, bool)):\n            return repr(x)\n        return json.JSONEncoder.default(self, x)\n\n\ndef evaluate(model, dataloaders, logging, backend=\'faiss\', config = None):\n    if config is not None and config[\'dataset_selected\'] == \'inshop\':\n        dl_query = lib.data.loader.make(config, model,\n            \'eval\', inshop_type = \'query\')\n        dl_gallery = lib.data.loader.make(config, model,\n            \'eval\', inshop_type = \'gallery\')\n        score = lib.utils.evaluate_in_shop(\n            model,\n            dl_query = dl_query,\n            dl_gallery = dl_gallery,\n            use_penultimate = False,\n            backend = backend)\n    else:\n        score = lib.utils.evaluate(\n            model,\n            dataloaders[\'eval\'],\n            use_penultimate = False,\n            backend=backend\n        )\n    return score\n\n\ndef train_batch(model, criterion, opt, config, batch, dset, epoch):\n    X = batch[0].cuda(non_blocking=True) # images\n    T = batch[1].cuda(non_blocking=True) # class labels\n    I = batch[2] # image ids\n\n    opt.zero_grad()\n    M = model(X)\n\n    if epoch >= config[\'finetune_epoch\'] * 8 / 19:\n        pass\n    else:\n        M = M.split(config[\'sz_embedding\'] // config[\'nb_clusters\'], dim = 1)\n        M = M[dset.id]\n\n    M = torch.nn.functional.normalize(M, p=2, dim=1)\n    loss = criterion[dset.id](M, T)\n    loss.backward()\n    opt.step()\n    return loss.item()\n\n\ndef get_criterion(config):\n    name = \'margin\'\n    ds_name = config[\'dataset_selected\']\n    nb_classes = len(\n        config[\'dataset\'][ds_name][\'classes\'][\'train\']\n    )\n    logging.debug(\'Create margin loss. #classes={}\'.format(nb_classes))\n    criterion = [\n        lib.loss.MarginLoss(\n            nb_classes,\n        ).cuda() for i in range(config[\'nb_clusters\'])\n    ]\n    return criterion\n\n\ndef get_optimizer(config, model, criterion):\n\n    opt = torch.optim.Adam([\n        {\n            \'params\': model.parameters_dict[\'backbone\'],\n            **config[\'opt\'][\'backbone\']\n        },\n        {\n            \'params\': model.parameters_dict[\'embedding\'],\n            **config[\'opt\'][\'embedding\']\n        }\n    ])\n\n    return opt\n\n\ndef start(config):\n\n    """"""\n    Import `plt` after setting `matplotlib` backend to `agg`, because `tkinter`\n    missing. If `agg` set, when this module is imported, then plots can not\n    be displayed in jupyter notebook, because backend can be set only once.\n    """"""\n    import matplotlib.pyplot as plt\n\n    metrics = {}\n\n    # reserve GPU memory for faiss if faiss-gpu used\n    faiss_reserver = lib.faissext.MemoryReserver()\n\n    # create logging directory\n    os.makedirs(config[\'log\'][\'path\'], exist_ok = True)\n\n    # warn if log file exists already and append underscore\n    import warnings\n    _fpath = os.path.join(config[\'log\'][\'path\'], config[\'log\'][\'name\'])\n    if os.path.exists(_fpath):\n        warnings.warn(\'Log file exists already: {}\'.format(_fpath))\n        print(\'Appending underscore to log file and database\')\n        config[\'log\'][\'name\'] += \'_\'\n\n    # initialize logger\n    logging.basicConfig(\n        format = ""%(asctime)s %(message)s"",\n        level = logging.DEBUG if config[\'verbose\'] else logging.INFO,\n        handlers = [\n            logging.FileHandler(\n                ""{0}/{1}.log"".format(\n                    config[\'log\'][\'path\'],\n                    config[\'log\'][\'name\']\n                )\n            ),\n            logging.StreamHandler()\n        ]\n    )\n\n    # print summary of config\n    logging.info(\n        json_dumps(obj = config, indent=4, cls = JSONEncoder, sort_keys = True)\n    )\n\n    torch.cuda.set_device(config[\'cuda_device\'])\n\n    if not os.path.isdir(config[\'log\'][\'path\']):\n        os.mkdir(config[\'log\'][\'path\'])\n\n    # set random seed for all gpus\n    seed = config[\'random_seed\']\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    faiss_reserver.lock(config[\'backend\'])\n\n    model = lib.model.make(config).cuda()\n\n    start_epoch = 0\n    best_epoch = -1\n    best_recall = 0\n\n    # create init and eval dataloaders; init used for creating clustered DLs\n    dataloaders = {}\n    for dl_type in [\'init\', \'eval\']:\n        if config[\'dataset_selected\'] == \'inshop\':\n            # query and gallery initialized in `make_clustered_dataloaders`\n            if dl_type == \'init\':\n                dataloaders[dl_type] = lib.data.loader.make(config, model,\n                    dl_type, inshop_type = \'train\')\n        else:\n            dataloaders[dl_type] = lib.data.loader.make(config, model,\n                dl_type)\n\n    criterion = get_criterion(config)\n    opt = get_optimizer(config, model, criterion)\n\n    faiss_reserver.release()\n    logging.info(""Evaluating initial model..."")\n    metrics[-1] = {\n        \'score\': evaluate(model, dataloaders, logging,\n                        backend = config[\'backend\'],\n                        config = config)}\n\n    dataloaders[\'train\'], C, T, I = make_clustered_dataloaders(model,\n            dataloaders[\'init\'], config, reassign = False, logging = logging)\n    faiss_reserver.lock(config[\'backend\'])\n\n    metrics[-1].update({\'C\': C, \'T\': T, \'I\': I})\n\n    if config[\'verbose\']:\n        print(\'Printing only first 200 classes (because of SOProducts)\')\n        for c in range(config[\'nb_clusters\']):\n            print(\n                np.bincount(\n                    np.array(dataloaders[\'train\'][c].dataset.ys)\n                )[:200]\n            )\n            plt.hist(np.array(dataloaders[\'train\'][c].dataset.ys), bins = 100)\n            plt.show()\n\n    logging.info(""Training for {} epochs."".format(config[\'nb_epochs\']))\n    losses = []\n    t1 = time.time()\n\n    for e in range(start_epoch, config[\'nb_epochs\']):\n        is_best = False\n\n        metrics[e] = {}\n        time_per_epoch_1 = time.time()\n        losses_per_epoch = []\n\n        if e >= config[\'finetune_epoch\']:\n            if e == config[\'finetune_epoch\'] or e == start_epoch:\n                logging.info(\'Starting to finetune model...\')\n                config[\'nb_clusters\'] = 1\n                logging.debug(\n                    ""config[\'nb_clusters\']: {})"".format(config[\'nb_clusters\']))\n                faiss_reserver.release()\n                dataloaders[\'train\'], C, T, I = make_clustered_dataloaders(\n                    model, dataloaders[\'init\'], config, logging = logging)\n                assert len(dataloaders[\'train\']) == 1\n        elif e > 0 and config[\'recluster\'][\'enabled\'] and \\\n                config[\'nb_clusters\'] > 0:\n            if e % config[\'recluster\'][\'mod_epoch\'] == 0:\n                logging.info(""Reclustering dataloaders..."")\n                faiss_reserver.release()\n                dataloaders[\'train\'], C, T, I = make_clustered_dataloaders(\n                    model, dataloaders[\'init\'], config, reassign = True,\n                    C_prev = C, I_prev = I, logging = logging)\n                faiss_reserver.lock(config[\'backend\'])\n                if config[\'verbose\']:\n                    for c in range(config[\'nb_clusters\']):\n                        print(\n                            np.bincount(\n                                np.array(\n                                    dataloaders[\'train\'][c].dataset.ys)\n                                )[:200]\n                            )\n\n                metrics[e].update({\'C\': C, \'T\': T, \'I\': I})\n\n\n        # merge dataloaders (created from clusters) into one dataloader\n        mdl = lib.data.loader.merge(dataloaders[\'train\'])\n\n        # calculate number of batches for tqdm\n        max_len_dataloaders = max([len(dl) for dl in dataloaders[\'train\']])\n        num_batches_approx = max_len_dataloaders * len(dataloaders[\'train\'])\n\n        for batch, dset in tqdm(\n            mdl,\n            total = num_batches_approx,\n            disable = num_batches_approx < 100,\n            desc = \'Train epoch {}.\'.format(e)\n        ):\n            loss = train_batch(model, criterion, opt, config, batch, dset, e)\n            losses_per_epoch.append(loss)\n\n        time_per_epoch_2 = time.time()\n        losses.append(np.mean(losses_per_epoch[-20:]))\n        logging.info(\n            ""Epoch: {}, loss: {}, time (seconds): {:.2f}."".format(\n                e,\n                losses[-1],\n                time_per_epoch_2 - time_per_epoch_1\n            )\n        )\n\n        faiss_reserver.release()\n        tic = time.time()\n        metrics[e].update({\n            \'score\': evaluate(model, dataloaders, logging,\n                        backend=config[\'backend\'],\n                        config = config),\n            \'loss\': {\n                \'train\': losses[-1]\n            }\n        })\n        logging.debug(\n            \'Evaluation total elapsed time: {:.2f} s\'.format(\n                time.time() - tic\n            )\n        )\n        faiss_reserver.lock(config[\'backend\'])\n\n        recall_curr = metrics[e][\'score\'][\'recall\'][0] # take R@1\n        if recall_curr > best_recall:\n            best_recall = recall_curr\n            best_epoch = e\n            is_best = True\n            logging.info(\'Best epoch!\')\n\n        model.current_epoch = e\n\n        # save metrics etc. to shelve file\n        with shelve.open(\n            os.path.join(\n                config[\'log\'][\'path\'], config[\'log\'][\'name\']),\n            writeback = True\n        ) as _f:\n            if \'config\' not in _f:\n                _f[\'config\'] = config\n            if \'metrics\' not in _f:\n                _f[\'metrics\'] = {}\n                # if initial model evaluated, append metrics\n                if -1 in metrics:\n                    _f[\'metrics\'][-1] = metrics[-1]\n            _f[\'metrics\'][e] = metrics[e]\n\n        if config[\'save_model\'] and is_best:\n            save_suff = \'.pt\'\n            torch.save(\n                model.state_dict(),\n                os.path.join(\n                    config[\'log\'][\'path\'], config[\'log\'][\'name\'] + save_suff\n                )\n            )\n            logging.info(\'Save the checkpoint!\')\n    t2 = time.time()\n    logging.info(\n        ""Total training time (minutes): {:.2f}."".format(\n            (t2 - t1) / 60\n        )\n    )\n    logging.info(""Best R@1 = {} at epoch {}."".format(best_recall, best_epoch))\n\n'"
lib/__init__.py,0,b'from . import loss\nfrom . import utils\nfrom . import evaluation\nfrom . import similarity\nfrom . import model\nfrom . import data\nfrom . import clustering\n\n'
lib/clustering.py,2,"b""from __future__ import print_function\nfrom __future__ import division\n\nimport torch\nimport logging\nimport numpy as np\nimport sklearn.cluster\nfrom . import evaluation\nfrom . import faissext\nfrom . import data\nfrom . import utils\n\ndef get_cluster_labels(model, data_loader, use_penultimate, nb_clusters,\n       gpu_id=None, backend='faiss'):\n    is_dry_run = (nb_clusters == 1)\n    if not is_dry_run:\n        if not use_penultimate:\n            logging.debug('Using the final layer for clustering')\n        X_all, T_all, I_all = utils.predict_batchwise(\n            model=model,\n            dataloader=data_loader,\n            use_penultimate=use_penultimate,\n            is_dry_run=is_dry_run\n        )\n        perm = np.argsort(I_all)\n        X_all = X_all[perm]\n        I_all = I_all[perm]\n        T_all = T_all[perm]\n        if backend == 'torch+sklearn':\n            clustering_algorithm = sklearn.cluster.KMeans(\n                n_clusters=nb_clusters)\n            C = clustering_algorithm.fit(X_all).labels_\n        else:\n            C = faissext.do_clustering(\n                X_all,\n                num_clusters = nb_clusters,\n                gpu_ids = None if backend != 'faiss-gpu'\n                    else torch.cuda.current_device(),\n                niter=100,\n                nredo=5,\n                verbose=0\n            )\n    else:\n        T_all = np.array(data_loader.dataset.ys)\n        I_all = np.array(data_loader.dataset.I)\n        C = np.zeros(len(T_all), dtype=int)\n    return C, T_all, I_all\n\n\ndef make_clustered_dataloaders(model, dataloader_init, config,\n        reassign = False, I_prev = None, C_prev = None, logging = None):\n\n    def correct_indices(I):\n        return torch.sort(torch.LongTensor(I))[1]\n\n    C, T, I = get_cluster_labels(\n        model,\n        dataloader_init,\n        use_penultimate = True,\n        nb_clusters = config['nb_clusters'],\n        backend=config['backend']\n    )\n\n    if reassign == True:\n\n        # get correct indices for samples by sorting them and return arg sort\n        I_correct = correct_indices(I)\n        I = I[I_correct]\n        T = T[I_correct]\n        C = C[I_correct]\n\n        # also use the same indices of sorted samples for previous data\n        I_prev_correct = correct_indices(I_prev)\n        I_prev = I_prev[I_prev_correct]\n        C_prev = C_prev[I_prev_correct]\n\n        logging.debug('Reassigning clusters...')\n        logging.debug('Calculating NMI for consecutive cluster assignments...')\n        logging.debug(str(\n            evaluation.calc_normalized_mutual_information(\n            C[I],\n            C_prev[I_prev]\n        )))\n\n        # assign s.t. least costs w.r.t. L1 norm\n        C, costs = data.loader.reassign_clusters(C_prev = C_prev,\n                C_curr = C, I_prev = I_prev, I_curr = I)\n        logging.debug('Costs before reassignment')\n        logging.debug(str(costs))\n        _, costs = data.loader.reassign_clusters(C_prev = C_prev,\n                C_curr = C, I_prev = I_prev, I_curr = I)\n        # after printing out the costs now, the trace of matrix should\n        # have lower numbers than other entries in matrix\n        logging.debug('Costs after reassignment')\n        logging.debug(str(costs))\n\n    #  remove labels s.t. minimum 2 samples per class per cluster\n    for c in range(config['nb_clusters']):\n        for t in np.unique(T[C == c]):\n            if (T[C == c] == t).sum().item() == 1:\n                # assign to cluster -1 if only one sample from class\n                C[(T == t) & (C == c)] = -1\n\n    dls = data.loader.make_from_clusters(\n        C = C, subset_indices = I, model = model, config = config\n    )\n\n    return dls, C, T, I\n"""
lib/faissext.py,0,"b'from __future__ import print_function\nfrom __future__ import division\n\nimport numpy as np\nimport faiss\nimport sys\nimport time\nimport warnings\nimport logging\n\n\nif not sys.warnoptions:\n    # suppress pesky PIL EXIF warnings\n    warnings.simplefilter(""once"")\n    warnings.filterwarnings(""ignore"", message=""(Possibly )?corrupt EXIF data.*"")\n    warnings.filterwarnings(""ignore"", message=""numpy.dtype size changed.*"")\n    warnings.filterwarnings(""ignore"", message=""numpy.ufunc size changed.*"")\n\n\ndef reserve_faiss_gpu_memory(gpu_id=0):\n    """"""\n    Reserves around 2.4 Gb memory on Titan Xp.\n    `r = reserve_faiss_gpu_memory()`\n    To release the memory run `del r`\n\n    Something like 200 Mb will still be hold afterwards.\n    """"""\n    res = faiss.StandardGpuResources()\n    cfg = faiss.GpuIndexFlatConfig()\n    cfg.useFloat16 = False\n    cfg.device = gpu_id\n    index = faiss.GpuIndexFlatL2(res, 2048, cfg)\n    return index, res\n\n\nclass MemoryReserver():\n    """"""\n    Faiss memory manager. If not used and another process takes up memory of\n    currently used GPU, then the program will crash.\n    """"""\n    def __init__(self):\n        self.memory_holder = None\n\n    def lock(self, backend):\n        # reserve memory for faiss if backend is faiss-gpu\n        if backend == \'faiss-gpu\':\n            logging.debug(\'Reserve some memory for FAISS\')\n            self.memory_holder = reserve_faiss_gpu_memory(gpu_id=0)\n        else:\n            self.memory_holder = None\n\n    def release(self):\n        if self.memory_holder is not None:\n            logging.debug(\'Release memory for FAISS\')\n            self.memory_holder = None\n\n\ndef preprocess_features(x, x2=None, d=256):\n    """"""\n    Calculate PCA + Whitening + L2 normalization for each vector\n\n    Args:\n        x (ndarray): N x D, where N is number of vectors, D - dimensionality\n        x2 (ndarray): optional, if not None apply PCA+Whitening learned on x to x2.\n        d (int): number of output dimensions (how many principal components to use).\n    Returns:\n        transformed [N x d] matrix xt .\n    """"""\n    n, orig_d = x.shape\n    pcaw = faiss.PCAMatrix(d_in=orig_d, d_out=d, eigen_power=-0.5, random_rotation=False)\n    pcaw.train(x)\n    assert pcaw.is_trained\n    print(\'Performing PCA + whitening\')\n    x = pcaw.apply_py(x)\n    print(\'x.shape after PCA + whitening:\', x.shape)\n    l2normalization = faiss.NormalizationTransform(d, 2.0)\n    print(\'Performing L2 normalization\')\n    x = l2normalization.apply_py(x)\n    if x2 is not None:\n        print(\'Perform PCA + whitening for x2\')\n        x2 = pcaw.apply_py(x2)\n        x2 = l2normalization.apply_py(x2)\n        return x, x2\n    else:\n        return x\n\n\ndef train_kmeans(x, num_clusters=1000, gpu_ids=None, niter=100, nredo=1, verbose=0):\n    """"""\n    Runs k-means clustering on one or several GPUs\n    """"""\n    assert np.all(~np.isnan(x)), \'x contains NaN\'\n    assert np.all(np.isfinite(x)), \'x contains Inf\'\n    if isinstance(gpu_ids, int):\n        gpu_ids = [gpu_ids]\n    assert gpu_ids is None or len(gpu_ids)\n\n    d = x.shape[1]\n    kmeans = faiss.Clustering(d, num_clusters)\n    kmeans.verbose = bool(verbose)\n    kmeans.niter = niter\n    kmeans.nredo = nredo\n\n    # otherwise the kmeans implementation sub-samples the training set\n    kmeans.max_points_per_centroid = 10000000\n\n    if gpu_ids is not None:\n        res = [faiss.StandardGpuResources() for i in gpu_ids]\n\n        flat_config = []\n        for i in gpu_ids:\n            cfg = faiss.GpuIndexFlatConfig()\n            cfg.useFloat16 = False\n            cfg.device = i\n            flat_config.append(cfg)\n\n        if len(gpu_ids) == 1:\n            index = faiss.GpuIndexFlatL2(res[0], d, flat_config[0])\n        else:\n            indexes = [faiss.GpuIndexFlatL2(res[i], d, flat_config[i])\n                       for i in range(len(gpu_ids))]\n            index = faiss.IndexProxy()\n            for sub_index in indexes:\n                index.addIndex(sub_index)\n    else:\n        index = faiss.IndexFlatL2(d)\n\n    # perform the training\n    kmeans.train(x, index)\n    centroids = faiss.vector_float_to_array(kmeans.centroids)\n\n    objective = faiss.vector_float_to_array(kmeans.obj)\n    #logging.debug(""Final objective: %.4g"" % objective[-1])\n\n    return centroids.reshape(num_clusters, d)\n\n\ndef compute_cluster_assignment(centroids, x):\n    assert centroids is not None, ""should train before assigning""\n    d = centroids.shape[1]\n    index = faiss.IndexFlatL2(d)\n    index.add(centroids)\n    distances, labels = index.search(x, 1)\n    return labels.ravel()\n\n\ndef do_clustering(features, num_clusters, gpu_ids=None,\n                  num_pca_components=None, niter=100, nredo=1, verbose=0):\n    logging.debug(\'FAISS: using GPUs {}\'.format(gpu_ids))\n    features = np.asarray(features.reshape(features.shape[0], -1), dtype=np.float32)\n\n    if num_pca_components is not None:\n        features = preprocess_features(features, d=num_pca_components,\n                                       niter=niter, nredo=nredo, verbose=verbose)\n\n    logging.debug(\'FAISS: clustering...\')\n    t0 = time.time()\n    centroids = train_kmeans(features, num_clusters, gpu_ids=gpu_ids, verbose=1)\n    labels = compute_cluster_assignment(centroids, features)\n    t1 = time.time()\n    logging.debug(""FAISS: Clustering total elapsed time: %.3f m"" % ((t1 - t0) / 60.0))\n    return labels\n\n\ndef find_nearest_neighbors(x, queries=None, k=5, gpu_id=None):\n    """"""\n    Find k nearest neighbors for each of the n examples.\n    Distances are computed using Squared Euclidean distance metric.\n\n    Arguments:\n    ----------\n    queries\n    x (ndarray): N examples to search within. [N x d].\n    gpu_id (int): use CPU if None else use GPU with the specified id.\n    queries (ndarray): find nearest neigbor for each query example. [M x d] matrix\n        If None than find k nearest neighbors for each row of x\n        (excluding self exampels).\n    k (int): number of nearest neighbors to find.\n\n    Return\n    I (ndarray): Indices of the nearest neighnpors. [M x k]\n    distances (ndarray): Distances to the nearest neighbors. [M x k]\n\n    """"""\n    if gpu_id is not None and not isinstance(gpu_id, int):\n        raise ValueError(\'gpu_id must be None or int\')\n    x = np.asarray(x.reshape(x.shape[0], -1), dtype=np.float32)\n    remove_self = False # will we have queries in the search results?\n    if queries is None:\n        remove_self = True\n        queries = x\n        k += 1\n\n    d = x.shape[1]\n\n    tic = time.time()\n    if gpu_id is None:\n        logging.debug(\'FAISS: cpu::find {} nearest neighbors\'\\\n                     .format(k - int(remove_self)))\n        index = faiss.IndexFlatL2(d)\n    else:\n        logging.debug(\'FAISS: gpu[{}]::find {} nearest neighbors\'\\\n                     .format(gpu_id, k - int(remove_self)))\n        cfg = faiss.GpuIndexFlatConfig()\n        cfg.useFloat16 = False\n        cfg.device = gpu_id\n\n        flat_config = [cfg]\n        resources = [faiss.StandardGpuResources()]\n        index = faiss.GpuIndexFlatL2(resources[0], d, flat_config[0])\n    index.add(x)\n    distances, nns = index.search(queries, k)\n    if remove_self:\n        for i in range(len(nns)):\n            indices = np.nonzero(nns[i, :] != i)[0]\n            indices.sort()\n            if len(indices) > k - 1:\n                indices = indices[:-1]\n            nns[i, :-1] = nns[i, indices]\n            distances[i, :-1] = distances[i, indices]\n        nns = nns[:, :-1]\n        distances = distances[:, :-1]\n    logging.debug(\'FAISS: Neighbors search total elapsed time: {:.2f} sec\'.format(time.time() - tic))\n    return nns, distances\n\n\ndef example(size=30000, k=10, num_pca_components=256):\n    gpu_ids = [0]\n\n    x = np.random.rand(size, 512)\n    print(""reshape"")\n    x = x.reshape(x.shape[0], -1).astype(\'float32\')\n    x, _ = preprocess_features(x, x, d=num_pca_components)\n\n    print(""run"")\n    t0 = time.time()\n    centroids = train_kmeans(x, k, gpu_ids=gpu_ids)\n    print(\'compute_cluster_assignment\')\n    labels = compute_cluster_assignment(centroids, x)\n    print(\'centroids.shape:\', centroids.shape)\n    print(\'labels.type:\', labels.__class__, labels.dtype)\n    print(\'labels.shape:\', labels.shape)\n    t1 = time.time()\n\n    print(""total runtime: %.2f s"" % (t1 - t0))\n\n\ndef test_knn_search(size=10000, gpu_id=None):\n    x = np.random.rand(size, 512)\n    x = x.reshape(x.shape[0], -1).astype(\'float32\')\n    d = x.shape[1]\n\n    tic = time.time()\n    if gpu_id is None:\n        index = faiss.IndexFlatL2(d)\n    else:\n        cfg = faiss.GpuIndexFlatConfig()\n        cfg.useFloat16 = False\n        cfg.device = gpu_id\n\n        flat_config = [cfg]\n        resources = [faiss.StandardGpuResources()]\n        index = faiss.GpuIndexFlatL2(resources[0], d, flat_config[0])\n    index.add(x)\n    print(\'Index built in {} sec\'.format(time.time() - tic))\n    distances, I = index.search(x, 21)\n    print(\'Searched in {} sec\'.format(time.time() - tic))\n    print(distances.shape)\n    print(I.shape)\n    print(distances[:5])\n    print(I[:5])\n\n\nif __name__ == \'__main__\':\n    #example(size=100000, k=3, num_pca_components=32)\n    test_knn_search(size=100000, gpu_id=5)\n'"
lib/model.py,10,"b'\nimport torchvision\nimport torch\nfrom math import ceil\nimport logging\nimport torch\nimport numpy as np\nimport torchvision\nimport torch\nfrom torch.nn import Linear, Dropout, AvgPool2d, MaxPool2d\nfrom torch.nn.init import xavier_normal_\n\n\ndef resnet50(pretrained = True):\n    model = torchvision.models.resnet50(pretrained = pretrained)\n\n    model.features = torch.nn.Sequential(\n        model.conv1, model.bn1, model.relu, model.maxpool,\n        model.layer1, model.layer2, model.layer3, model.layer4\n    )\n\n    model.sz_features_output = 2048\n\n    for module in filter(\n        lambda m: type(m) == torch.nn.BatchNorm2d, model.modules()\n    ):\n        module.eval()\n        module.train = lambda _: None\n\n    return model\n\n\ndef make_parameters_dict(model, filter_module_names):\n    """"""\n    Separates model parameters into \'backbone\' and other modules whose names\n    are given in as list in `filter_module_names`, e.g. [\'embedding_layer\'].\n    """"""\n\n    # init parameters dict\n    D = {k: [] for k in [\'backbone\', *filter_module_names]}\n    for name, param in model.named_parameters():\n        name = name.split(\'.\')[0]\n        if name not in filter_module_names:\n            D[\'backbone\'] += [param]\n        else:\n            D[name] += [param]\n\n    # verify that D contains same number of parameters as in model\n    nb_total = len(list(model.parameters()))\n    nb_dict_params = sum([len(D[d]) for d in D])\n    assert nb_total == nb_dict_params\n    return D\n\n\ndef init_splitted(layer, nb_clusters, sz_embedding):\n    # initialize splitted embedding parts separately\n    from math import ceil\n    for c in range(nb_clusters):\n        i = torch.arange(\n            c * ceil(sz_embedding / nb_clusters),\n            # cut off remaining indices, e.g. if > embedding size\n            min(\n                (c + 1) * ceil(\n                    sz_embedding / nb_clusters\n                ),\n                sz_embedding\n            )\n        ).long()\n        _layer = torch.nn.Linear(layer.weight.shape[1], len(i))\n        layer.weight.data[i] = xavier_normal_(_layer.weight.data, gain = 1)\n        layer.bias.data[i] = _layer.bias.data\n\n\ndef embed_model(model, config, sz_embedding, normalize_output=True):\n\n    model.features_pooling = AvgPool2d(7,\n        stride=1, padding=0, ceil_mode=True, count_include_pad=True\n    )\n    model.features_dropout = Dropout(0.01)\n\n    # choose arbitrary parameter for selecting GPU/CPU\n    dev = list(model.parameters())[0].device\n    if type(model) != torchvision.models.ResNet:\n        model.sz_features_output = _sz_features[type(model)]\n    torch.random.manual_seed(config[\'random_seed\'] + 1)\n    model.embedding = Linear(model.sz_features_output, sz_embedding).to(dev)\n\n    # for fair comparison between different cluster sizes\n    torch.random.manual_seed(config[\'random_seed\'] + 1)\n    np.random.seed(config[\'random_seed\'] + 1)\n\n    init_splitted(\n        model.embedding, config[\'nb_clusters\'], config[\'sz_embedding\']\n    )\n\n    features_parameters = model.features.parameters()\n\n    model.parameters_dict = make_parameters_dict(\n        model = model,\n        filter_module_names = [\'embedding\']\n    )\n\n    assert normalize_output\n\n    nb_clusters = config[\'nb_clusters\']\n\n    learner_neurons = [None] * nb_clusters\n    for c in range(nb_clusters):\n        learner_neurons[c] = np.arange(\n            c * ceil(sz_embedding / nb_clusters),\n            # cut off remaining indices, e.g. if > embedding size\n            min(\n                (c + 1) * ceil(\n                    sz_embedding / nb_clusters\n                ),\n                sz_embedding\n            )\n        )\n    model.learner_neurons = learner_neurons\n\n    def forward(x, use_penultimate=False):\n        x = model.features(x)\n        x = model.features_pooling(x)\n        x = model.features_dropout(x)\n        x = x.view(x.size(0), -1)\n        if not use_penultimate:\n            x = model.embedding(x)\n            for idxs in model.learner_neurons:\n                x[:, idxs] = torch.nn.functional.normalize(\n                    x[:, idxs], p=2, dim=1\n                )\n        else:\n            # normalize the entire penultimate layer\n            x = torch.nn.functional.normalize(x, p=2, dim=1)\n        return x\n    model.forward = forward\n\n\ndef make(config):\n    model = resnet50(pretrained = True)\n    embed_model(\n        model = model,\n        config = config,\n        sz_embedding = config[\'sz_embedding\'],\n        normalize_output = True\n    )\n    return model\n'"
lib/similarity.py,12,"b'from __future__ import print_function\nfrom __future__ import division\n\nimport logging\nimport random\nimport torch\nimport sklearn\nimport time\nimport numpy as np\n\nfrom . import utils\nfrom . import faissext\n\n__all__ = [\n    \'pairwise_distance\',\n    \'assign_by_euclidian_at_k\'\n]\n\n\nTORCH_SKLEARN_BACKEND = \'torch+sklearn\'\nFAISS_BACKEND = \'faiss\'\nFAISS_GPU_BACKEND = \'faiss-gpu\'\n_DEFAULT_BACKEND_ = FAISS_GPU_BACKEND\n_backends_ = [TORCH_SKLEARN_BACKEND, FAISS_BACKEND, FAISS_GPU_BACKEND]\n\n\ndef pairwise_distance(a, squared=False):\n    """"""Computes the pairwise distance matrix with numerical stability.\n    output[i, j] = || feature[i, :] - feature[j, :] ||_2\n    Args:\n    feature: 2-D Tensor of size [number of data, feature dimension].\n    squared: Boolean, whether or not to square the pairwise distances.\n    Returns:\n    pairwise_distances: 2-D Tensor of size [number of data, number of data].\n    """"""\n    a = torch.as_tensor(np.atleast_2d(a))\n    pairwise_distances_squared = torch.add(\n        a.pow(2).sum(dim=1, keepdim=True).expand(a.size(0), -1),\n        torch.t(a).pow(2).sum(dim=0, keepdim=True).expand(a.size(0), -1)\n    ) - 2 * (\n        torch.mm(a, torch.t(a))\n    )\n\n    # Deal with numerical inaccuracies. Set small negatives to zero.\n    pairwise_distances_squared = torch.clamp(\n        pairwise_distances_squared, min=0.0\n    )\n\n    # Get the mask where the zero distances are at.\n    error_mask = torch.le(pairwise_distances_squared, 0.0)\n\n    # Optionally take the sqrt.\n    if squared:\n        pairwise_distances = pairwise_distances_squared\n    else:\n        pairwise_distances = torch.sqrt(\n            pairwise_distances_squared + error_mask.float() * 1e-16\n        )\n\n    # Undo conditionally adding 1e-16.\n    pairwise_distances = torch.mul(\n        pairwise_distances,\n        (error_mask == False).float()\n    )\n\n    # Explicitly set diagonals to zero.\n    mask_offdiagonals = 1 - torch.eye(\n        *pairwise_distances.size(),\n        device=pairwise_distances.device\n    )\n    pairwise_distances = torch.mul(pairwise_distances, mask_offdiagonals).data.cpu().numpy()\n\n    return pairwise_distances\n\n\ndef assign_by_euclidian_at_k(X, T, k, gpu_id=None, backend=_DEFAULT_BACKEND_):\n    """"""\n    X : [nb_samples x nb_features], e.g. 100 x 64 (embeddings)\n    k : for each sample, assign target labels of k nearest points\n    """"""\n    if backend == TORCH_SKLEARN_BACKEND:\n        distances = sklearn.metrics.pairwise.pairwise_distances(X)\n        # get nearest points\n        nns = np.argsort(distances, axis = 1)[:, :k + 1]\n        for i in range(len(nns)):\n            indices = np.nonzero(nns[i, :] != i)[0]\n            if len(indices) > k:\n                indices = indices[:-1]\n            nns[i, :-1] = nns[i, indices]\n        nns = nns[:, :-1]\n        assert nns.shape[1] == k, nns.shape\n    else:\n        nns, _ = faissext.find_nearest_neighbors(X,\n                                                 k=k,\n                                                 gpu_id=None if backend != FAISS_GPU_BACKEND\n                                                    else torch.cuda.current_device()\n        )\n    return np.array([[T[i] for i in ii] for ii in nns])\n\n\ndef cluster_by_kmeans(X, nb_clusters, gpu_id=None, backend=_DEFAULT_BACKEND_):\n    """"""\n    xs : embeddings with shape [nb_samples, nb_features]\n    nb_clusters : in this case, must be equal to number of classes\n    """"""\n    if backend == TORCH_SKLEARN_BACKEND:\n        C = sklearn.cluster.KMeans(nb_clusters).fit(X).labels_\n    else:\n        C = faissext.do_clustering(\n            X,\n            num_clusters = nb_clusters,\n            gpu_ids = None if backend != FAISS_GPU_BACKEND\n                else torch.cuda.current_device(),\n            niter=100,\n            nredo=5,\n            verbose=1\n        )\n    return C\n\n'"
lib/utils.py,6,"b'from __future__ import print_function\nfrom __future__ import division\n\nfrom . import evaluation\nfrom . import similarity\nimport numpy as np\nimport torch\nimport logging\nfrom tqdm import tqdm\n\n\ndef predict_batchwise(model, dataloader, use_penultimate, is_dry_run=False):\n    # list with N lists, where N = |{image, label, index}|\n    model_is_training = model.training\n    model.eval()\n    ds = dataloader.dataset\n    A = [[] for i in range(len(ds[0]))]\n    with torch.no_grad():\n\n        # use tqdm when the dataset is large (SOProducts)\n        is_verbose = len(dataloader.dataset) > 0\n\n        # extract batches (A becomes list of samples)\n        for batch in tqdm(dataloader, desc=\'predict\', disable=not is_verbose):\n            for i, J in enumerate(batch):\n                # i = 0: sz_batch * images\n                # i = 1: sz_batch * labels\n                # i = 2: sz_batch * indices\n                if i == 0:\n                    if not is_dry_run:\n                        # move images to device of model (approximate device)\n                        J = J.to(list(model.parameters())[0].device)\n                        # predict model output for image\n                        J = model(J, use_penultimate).data.cpu().numpy()\n                        # take only subset of resulting embedding w.r.t dataset\n                    else:\n                        # just a placeholder not to break existing code\n                        J = np.array([-1])\n                for j in J:\n                    A[i].append(np.asarray(j))\n        result = [np.stack(A[i]) for i in range(len(A))]\n    model.train()\n    model.train(model_is_training) # revert to previous training state\n    if is_dry_run:\n        # do not return features if is_dry_run\n        return [None, *result[1:]]\n    else:\n        return result\n\n\ndef evaluate_in_shop(model, dl_query, dl_gallery, use_penultimate, backend,\n        K = [1, 10, 20, 30, 50], with_nmi = False):\n\n    # calculate embeddings with model and get targets\n    X_query, T_query, _ = predict_batchwise(\n        model, dl_query, use_penultimate)\n    X_gallery, T_gallery, _ = predict_batchwise(\n        model, dl_gallery, use_penultimate)\n\n    nb_classes = dl_query.dataset.nb_classes()\n    assert nb_classes == len(set(T_query))\n\n    # calculate full similarity matrix, choose only first `len(X_query)` rows\n    # and only last columns corresponding to the column\n    T_eval = torch.cat(\n        [torch.from_numpy(T_query), torch.from_numpy(T_gallery)])\n    X_eval = torch.cat(\n        [torch.from_numpy(X_query), torch.from_numpy(X_gallery)])\n    D = similarity.pairwise_distance(X_eval)[:len(X_query), len(X_query):]\n\n    D = torch.from_numpy(D)\n    # get top k labels with smallest (`largest = False`) distance\n    Y = T_gallery[D.topk(k = max(K), dim = 1, largest = False)[1]]\n\n    scores = {}\n\n    recall = []\n    for k in K:\n        r_at_k = evaluation.calc_recall_at_k(T_query, Y, k)\n        recall.append(r_at_k)\n        logging.info(""R@{} : {:.3f}"".format(k, 100 * r_at_k))\n\n    scores[\'recall\'] = recall\n\n    if with_nmi:\n        # calculate NMI with kmeans clustering\n        nmi = evaluation.calc_normalized_mutual_information(\n            T_eval.numpy(),\n            similarity.cluster_by_kmeans(\n                X_eval.numpy(), nb_classes, backend=backend\n            )\n        )\n        logging.info(""NMI: {:.3f}"".format(nmi * 100))\n        scores[\'nmi\'] = nmi\n\n    return scores\n\n\ndef evaluate(model, dataloader, use_penultimate, backend,\n        K = [1, 2, 4, 8], with_nmi = False):\n    nb_classes = dataloader.dataset.nb_classes()\n\n    # calculate embeddings with model and get targets\n    X, T, _ = predict_batchwise(model, dataloader, use_penultimate)\n\n    scores = {}\n\n    # calculate NMI with kmeans clustering\n    if with_nmi:\n        nmi = evaluation.calc_normalized_mutual_information(\n            T,\n            similarity.cluster_by_kmeans(\n                X, nb_classes, backend=backend\n            )\n        )\n        logging.info(""NMI: {:.3f}"".format(nmi * 100))\n        scores[\'nmi\'] = nmi\n\n    # get predictions by assigning nearest 8 neighbors with euclidian\n    assert np.max(K) <= 8, (""Sorry, this is hardcoded here.""\n                "" You would need to retrieve > 8 nearest neighbors""\n                            "" to calculate R@k with k > 8"")\n    Y = similarity.assign_by_euclidian_at_k(X, T, 8, backend=backend)\n\n    # calculate recall @ 1, 2, 4, 8\n    recall = []\n    for k in K:\n        r_at_k = evaluation.calc_recall_at_k(T, Y, k)\n        recall.append(r_at_k)\n        logging.info(""R@{} : {:.3f}"".format(k, 100 * r_at_k))\n\n    scores[\'recall\'] = recall\n\n    return scores\n\n'"
lib/data/__init__.py,0,b'\nfrom . import set\nfrom . import loader\n'
lib/evaluation/__init__.py,0,b'\nfrom .normalized_mutual_information  import calc_normalized_mutual_information\nfrom .recall import calc_recall_at_k\n\n'
lib/evaluation/normalized_mutual_information.py,0,"b'import sklearn.cluster\nimport sklearn.metrics.cluster\nimport torch\n\ndef calc_normalized_mutual_information(ys, xs_clustered):\n    return sklearn.metrics.cluster.normalized_mutual_info_score(xs_clustered, ys)\n'"
lib/evaluation/recall.py,0,"b'from __future__ import division\n\nimport numpy as np\nimport torch\nimport sklearn.metrics.pairwise\nfrom .. import faissext\n\n\ndef calc_recall_at_k(T, Y, k):\n    """"""\n    T : [nb_samples] (target labels)\n    Y : [nb_samples x k] (k predicted labels/neighbours)\n    """"""\n    s = sum([1 for t, y in zip(T, Y) if t in y[:k]])\n    return s / (1. * len(T))\n\n'"
lib/loss/__init__.py,0,b'from .margin_loss import MarginLoss\n'
lib/loss/margin_loss.py,10,"b'\n\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom .sampler import Sampler\n\n\nclass MarginLoss(torch.nn.Module):\n    """"""Margin based loss.\n\n    Parameters\n    ----------\n    nb_classes: int\n        Number of classes in the train dataset.\n        Used to initialize class-specific boundaries beta.\n    margin : float\n        Margin between positive and negative pairs.\n    nu : float\n        Regularization parameter for beta.\n    class_specific_beta : bool\n        Are class-specific boundaries beind used?\n\n    Inputs:\n        - anchors: sampled anchor embeddings.\n        - positives: sampled positive embeddings.\n        - negatives: sampled negative embeddings.\n        - anchor_classes: labels of anchors. Used to get class-specific beta.\n\n    Outputs:\n        Loss value.\n    """"""\n\n    def __init__(self, nb_classes, beta=1.2, margin=0.2, nu=0.0,\n \t\t class_specific_beta=False, **kwargs):\n        super(MarginLoss, self).__init__()\n\n        self.nb_classes = nb_classes\n        self.class_specific_beta = class_specific_beta\n        if class_specific_beta:\n            assert nb_classes is not None\n            beta = torch.ones(nb_classes, dtype=torch.float32) * beta\n        else:\n            beta = torch.tensor([beta], dtype=torch.float32)\n        self.beta = torch.nn.Parameter(beta)\n        self.margin = margin\n        self.nu = nu\n        self.sampler = Sampler()\n\n    # def forward(self, anchors, positives, negatives, anchor_classes=None):\n    def forward(self, E, T):\n\n        # anchors, positives, negatives, anchor_classes = self.sampler(E, T)\n        anchor_idx, anchors, positives, negatives = self.sampler(E, T)\n        anchor_classes = T[anchor_idx]\n\n        if anchor_classes is not None:\n            if self.class_specific_beta:\n                # select beta for every sample according to the class label\n                beta = self.beta[anchor_classes]\n            else:\n                beta = self.beta\n            beta_regularization_loss = torch.norm(beta, p=1) * self.nu\n        else:\n            beta = self.beta\n            beta_regularization_loss = 0.0\n        try:\n            d_ap = ((positives - anchors)**2).sum(dim=1) + 1e-8\n        except Exception as e:\n            print(e)\n            print(positives.shape, anchors.shape)\n            raise e\n        d_ap = torch.sqrt(d_ap)\n        d_an = ((negatives - anchors)**2).sum(dim=1) + 1e-8\n        d_an = torch.sqrt(d_an)\n\n        pos_loss = F.relu(d_ap - beta + self.margin)\n        neg_loss = F.relu(beta - d_an + self.margin)\n\n        pair_cnt = torch.sum((pos_loss > 0.0) + (neg_loss > 0.0)).type_as(pos_loss)\n        loss = torch.sum(pos_loss + neg_loss)\n        if pair_cnt > 0.0:\n            # Normalize based on the number of pairs.\n            loss = (loss + beta_regularization_loss) / pair_cnt\n        return loss\n\n'"
lib/loss/sampler.py,6,"b'\nfrom __future__ import print_function\nfrom __future__ import division\n\n\nimport logging\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef pdist(A, squared=False, eps=1e-4):\n    prod = torch.mm(A, A.t())\n    norm = prod.diag().unsqueeze(1).expand_as(prod)\n    res = (norm + norm.t() - 2 * prod).clamp(min=0)\n    return res if squared else res.clamp(min=eps).sqrt()\n\n\ndef topk_mask(input, dim, K=10, **kwargs):\n    index = input.topk(max(1, min(K, input.size(dim))), dim=dim, **kwargs)[1]\n    return torch.zeros_like(input.data).scatter(dim, index, 1.0)\n\n\nclass Sampler(nn.Module):\n    """"""\n    Sample for each anchor negative examples\n        are K closest points on the distance >= cutoff\n\n    Inputs:\n        - **data**: input tensor with shape (batch_size, embed_dim).\n        Here we assume the consecutive batch_k examples are of the same class.\n        For example, if batch_k = 5, the first 5 examples belong to the same class,\n        6th-10th examples belong to another class, etc.\n\n    Outputs:\n        - a_indices: indices of anchors.\n        - x[a_indices]: sampled anchor embeddings.\n        - x[p_indices]: sampled positive embeddings.\n        - x[n_indices]: sampled negative embeddings.\n        - x: embeddings of the input batch.\n    """"""\n\n    def __init__(self, cutoff=0.5, infinity=1e6, eps=1e-6):\n        super(Sampler, self).__init__()\n        self.cutoff = cutoff\n        self.infinity = infinity\n        self.eps = eps\n\n    def forward(self, x, labels):\n        """"""\n        x: input tensor of shape (batch_size, embed_dim)\n        labels: tensor of class labels of shape (batch_size,)\n        """"""\n        d = pdist(x)\n        pos = torch.eq(\n            *[labels.unsqueeze(dim).expand_as(d) for dim in [0, 1]]\n        ).type_as(d) - (torch.eye( len(d))).type_as(d)\n        num_neg = int(pos.data.sum()) // len(pos)\n        neg = topk_mask(\n            d + self.infinity * ((pos > 0) + (d < self.cutoff)).type_as(d),\n            dim=1,\n            largest=False,\n            K=num_neg\n        )\n\n        a_indices = []\n        p_indices = []\n        n_indices = []\n\n        for i in range(len(d)):\n            a_indices.extend([i] * num_neg)\n            p_indices.extend(\n                np.atleast_1d(pos[i].nonzero().squeeze().cpu().numpy())\n            )\n            n_indices.extend(\n                np.atleast_1d(neg[i].nonzero().squeeze().cpu().numpy())\n            )\n\n            if len(a_indices) != len(p_indices) or len(a_indices) != len(n_indices):\n                logging.warning(\n                    \'Probably too many positives, because of lacking classes in\' +\n                    \' the current cluster.\' +\n                    \' n_anchors={}, n_pos={}, n_neg= {}\'.format(\n                        *map(len, [a_indices, p_indices, n_indices])\n                    )\n                )\n                min_len = min(map(len, [a_indices, p_indices, n_indices]))\n                a_indices = a_indices[:min_len]\n                p_indices = p_indices[:min_len]\n                n_indices = n_indices[:min_len]\n\n        assert len(a_indices) == len(p_indices) == len(n_indices), \\\n                \'{}, {}, {}\'.format(\n                    *map(len, [a_indices, p_indices, n_indices])\n                )\n\n        return a_indices, x[a_indices], x[p_indices], x[n_indices]\n'"
lib/data/loader/__init__.py,0,"b'\nfrom .reassignment import reassign_clusters\nfrom .sampler import ClassBalancedSampler\nfrom .utils import make, merge, make_from_clusters\n'"
lib/data/loader/reassignment.py,4,"b""\nimport torch\nfrom scipy.optimize import linear_sum_assignment\nimport numpy as np\n\n\ndef reassign_clusters(C_prev, C_curr, I_prev, I_curr):\n    nb_clusters = max(C_prev).item() + 1 # cluster ids start from 0\n    assert set(\n        i.item() for i in np.unique(I_prev)\n    ) == set(i.item() for i in np.unique(I_curr))\n    I_max = max(I_curr).item() + 1\n    I_all = {\n        'prev': torch.zeros(nb_clusters, I_max),\n        'curr': torch.zeros(nb_clusters, I_max)\n    }\n    I = {'prev': I_prev, 'curr': I_curr}\n    C = {'prev': C_prev, 'curr': C_curr}\n\n    for e in ['prev', 'curr']:\n        for c in range(nb_clusters):\n            _C = C[e]\n            _I = I[e]\n            I_all[e][c, _I[_C == c]] = 1\n\n    costs = torch.zeros(nb_clusters, nb_clusters)\n    for i in range(nb_clusters):\n        for j in range(nb_clusters):\n            costs[i, j] = torch.norm(\n                I_all['curr'][i] - I_all['prev'][j],\n                p = 1\n            )\n\n    reassign_prev, reassign_curr = linear_sum_assignment(costs)\n\n    C_reassigned = C['curr'].copy()\n\n    for a_prev, a_curr in zip(reassign_prev, reassign_curr):\n        C_reassigned[C['curr'] == int(a_curr)] = int(a_prev)\n\n    return C_reassigned, costs\n"""
lib/data/loader/sampler.py,1,"b'from __future__ import print_function\nfrom __future__ import division\n\nimport logging\nimport numpy as np\nfrom torch.utils.data.sampler import BatchSampler\n\n\nclass ClassBalancedSampler(BatchSampler):\n    """"""\n    Sampler that generates class balanced indices with classes chosen randomly.\n    For example, choosing batch_size = 32 and nun_samples_per_class = 8\n    will result in\n    32 indices, which point to 8 samples from 32/8=4 randomly picked classes.\n    """"""\n\n    def __init__(self, dataset, batch_size=80, num_samples_per_class=4):\n        self.duplicate_small_classes = True\n        assert batch_size % num_samples_per_class == 0, \\\n                ""batch size must be divisable by num_samples_per_class""\n        self.targets = np.array(dataset.ys)\n        self.C = list(set(self.targets))\n        self.C_index = {\n            c: np.where(self.targets == c)[0] for c in self.C}\n        for c in self.C:\n            np.random.shuffle(self.C_index[c])\n        self.C_count = {c: 0 for c in self.C}\n        self.count = 0\n        self.num_classes = batch_size // num_samples_per_class\n        self.num_samples_per_class = num_samples_per_class\n        self.dataset = dataset\n        self.batch_size = batch_size\n\n    def __iter__(self):\n        self.count = 0\n        is_not_enough_classes = len(self.C) < self.num_classes\n        if is_not_enough_classes:\n            logging.warn((\'Not enough classes to sample batches: have={},\'\n                         \'required={}\').format(len(self.C), self.num_classes))\n        while self.count + self.batch_size < len(self.dataset):\n            C = np.random.choice(\n                self.C, self.num_classes, replace=is_not_enough_classes\n            )\n            indices = []\n            for class_ in C:\n                if self.C_count[class_] + self.num_samples_per_class\\\n                   > len( self.C_index[class_]):\n                    indices.extend(\n                        np.random.choice(\n                            self.C_index[class_],\n                            self.num_samples_per_class,\n                            replace = len(\n                                self.C_index[class_] < \\\n                                        self.num_samples_per_class\n                                )\n                            )\n                        )\n                else:\n                    indices.extend(\n                        self.C_index[class_][self.C_count[class_]:\n                        self.C_count[class_] + self.num_samples_per_class]\n                    )\n                self.C_count[class_] += self.num_samples_per_class\n                if self.C_count[class_] + self.num_samples_per_class \\\n                        > len( self.C_index[class_]):\n                    np.random.shuffle(self.C_index[class_])\n                    self.C_count[class_] = 0\n            yield indices\n            self.count += self.num_classes * self.num_samples_per_class\n\n    def __len__(self):\n        return len(self.dataset) // self.batch_size\n\n'"
lib/data/loader/utils.py,2,"b'\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport random\n\nimport torch\nimport numpy as np\nfrom ..set import VehicleID, InShop, SOProducts\nfrom ..set import transform\nfrom .sampler import ClassBalancedSampler\n\n\ndatasets = {\n    \'sop\': SOProducts,\n    \'inshop\': InShop,\n    \'vid\': VehicleID\n}\n\n\ndef make(config, model, type, subset_indices = None, inshop_type = None):\n    """"""\n    subset_indices: indices for selecting subset of dataset, for creating\n        clustered dataloaders.\n    type: \'init\', \'eval\' or \'train\'.\n    """"""\n    # inshop_types: train, query, gallery; basically instead of labels/classes\n    ds_name = config[\'dataset_selected\']\n    if ds_name == \'inshop\':\n        ds = datasets[ds_name](\n            root = config[\'dataset\'][ds_name][\'root\'],\n            dset_type = inshop_type,\n            transform = transform.make(\n                **config[\'transform_parameters\'],\n                is_train = True if type == \'train\' else False\n            )\n        )\n    else:\n        ds = datasets[ds_name](\n            root = config[\'dataset\'][ds_name][\'root\'],\n            classes = config[\'dataset\'][ds_name][\'classes\'][type],\n            transform = transform.make(\n                **config[\'transform_parameters\'],\n                is_train = True if type == \'train\' else False\n            )\n        )\n    if type == \'train\':\n        ds.set_subset(subset_indices)\n        _c = config[\'dataloader\']\n        dl = torch.utils.data.DataLoader(\n            ds,\n            # ignore batch_size, since batch_sampler enabled\n            **{k: _c[k] for k in _c if k != \'batch_size\'},\n            batch_size = -1,\n            batch_sampler = ClassBalancedSampler(\n                ds,\n                batch_size = config[\'dataloader\'][\'batch_size\'],\n                num_samples_per_class = 4\n            )\n        )\n    else:\n        # else init or eval loader\n        dl = torch.utils.data.DataLoader(ds, **config[\'dataloader\'])\n    return dl\n\n\ndef make_from_clusters(C, subset_indices, model, config):\n    import numpy as np\n    from math import ceil\n    dataloaders = [[None] for c in range(config[\'nb_clusters\'])]\n    for c in range(config[\'nb_clusters\']):\n        dataloaders[c] = make(\n            config = config, model = model, type = \'train\', subset_indices = subset_indices[C == c],\n            inshop_type = \'train\')\n        dataloaders[c].dataset.id = c\n    return dataloaders\n\n\ndef merge(dls_non_iter):\n\n    nb_batches_per_dl = [len(dl) for dl in dls_non_iter]\n    nb_batches = max(nb_batches_per_dl)\n    I = range(len(dls_non_iter))\n    length = len(dls_non_iter)\n    dls = [iter(dl) for dl in dls_non_iter]\n\n    for j in range(nb_batches):\n        for i in I:\n            b = next(dls[i], None)\n            if b == None:\n                # initialize new dataloader in case no batches left\n                dls[i] = iter(dls_non_iter[i])\n                b = next(dls[i])\n            yield b, dls[i].dataset\n\n'"
lib/data/set/__init__.py,0,b'\nfrom . import transform\nfrom .base import BaseDataset\nfrom .sop import SOProducts\nfrom .inshop import InShop\nfrom .vid import VehicleID\n'
lib/data/set/base.py,1,"b""\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os\nimport torch\nimport torchvision\nimport numpy as np\nimport PIL.Image\n\n\nclass BaseDataset(torch.utils.data.Dataset):\n    def __init__(self, root, classes, transform = None):\n        self.classes = classes\n        self.root = root\n        self.transform = transform\n        self.ys, self.im_paths, self.I = [], [], []\n\n    def nb_classes(self):\n        assert set(self.ys) == set(self.classes)\n        return len(self.classes)\n\n    def __len__(self):\n        return len(self.ys)\n\n    def __getitem__(self, index):\n        im = PIL.Image.open(self.im_paths[index])\n        # convert gray to rgb\n        if len(list(im.split())) == 1 : im = im.convert('RGB')\n        if self.transform is not None:\n            im = self.transform(im)\n        return im, self.ys[index], index\n\n    def get_label(self, index):\n        return self.ys[index]\n\n    def set_subset(self, I):\n        self.ys = [self.ys[i] for i in I]\n        self.I = [self.I[i] for i in I]\n        self.im_paths = [self.im_paths[i] for i in I]\n"""
lib/data/set/inshop.py,2,"b'\n""""""NOTE: I\'ve checked the images for correctnes manually, i.e. each image\nof gallery should have a corresponding pair in query (i.e. should look the\nsame) and also have the same label.""""""\n\nimport PIL\nimport torch\nimport os\n\nclass InShop(torch.utils.data.Dataset):\n    """"""\n    For the In-Shop Clothes Retrieval dataset, we use the predefined\n    25, 882 training images of 3,997 classes for training. The test\n    set is partitioned into a query set (14,218 images of 3,985 classes)\n    and a gallery set (12, 612 images of 3, 985 classes)\n    """"""\n    def __init__(self, root, classes = None, dset_type = \'train\',\n            transform = None):\n        with open(\n            os.path.join(\n                root, \'Eval/list_eval_partition.txt\'\n            ), \'r\'\n        ) as f:\n            lines = f.readlines()\n\n        self.transform = transform\n\n        # store for using later \'__getitem__\'\n        self.dset_type = dset_type\n\n        nb_samples = int(lines[0].strip(\'\\n\'))\n        assert nb_samples == 52712\n\n        torch.utils.data.Dataset.__init__(self)\n        self.im_paths = {\'train\': [], \'query\': [], \'gallery\': []}\n        self.ys = {\'train\': [], \'query\': [], \'gallery\': []}\n        I_ = {\'train\': 0, \'query\': 0, \'gallery\': 0}\n        self.I = {\'train\': [], \'query\': [], \'gallery\': []}\n        # start from second line, since 0th and 1st contain meta-data\n        for line in lines[2:]:\n            im_path, im_id, eval_type = [\n                l for l in line.split(\' \') if l != \'\' and l != \'\\n\']\n            y = int(im_id.split(\'_\')[1])\n            self.im_paths[eval_type] += [os.path.join(root, im_path)]\n            self.ys[eval_type] += [y]\n            self.I[eval_type] += [I_[eval_type]]\n            I_[eval_type] += 1\n\n        nb_samples_counted = len(self.im_paths[\'train\']) + \\\n                len(self.im_paths[\'gallery\']) + len(self.im_paths[\'query\'])\n        assert nb_samples_counted == nb_samples\n\n        # verify that labels are sorted for next step\n        self.ys[\'query\'] == sorted(self.ys[\'query\'])\n        self.ys[\'gallery\'] == sorted(self.ys[\'gallery\'])\n\n        assert len(self.ys[\'train\']) == 25882\n        assert len(self.ys[\'query\']) == 14218\n        assert len(self.ys[\'gallery\']) == 12612\n\n        # verify that query and gallery have same labels\n        assert set(self.ys[\'query\']) == set(self.ys[\'gallery\'])\n\n        # labels of query and gallery are like [1, 1, 7, 7, 8, 11, ...]\n        # condense them such that ordered without spaces,\n        # i.e. 1 -> 1, 7 -> 2, ...\n        idx_to_class = {idx: i for i, idx in enumerate(\n            sorted(set(self.ys[\'query\']))\n        )}\n        for _type in [\'query\', \'gallery\']:\n            self.ys[_type] = list(\n                map(lambda x: idx_to_class[x], self.ys[_type]))\n\n        # same thing for train labels\n        idx_to_class = {idx: i for i, idx in enumerate(\n            sorted(set(self.ys[\'train\']))\n        )}\n        self.ys[\'train\'] = list(\n            map(lambda x: idx_to_class[x], self.ys[\'train\']))\n\n        # should be 3997 classes for training, 3985 for query/gallery\n        assert len(set(self.ys[\'train\'])) == 3997\n        assert len(set(self.ys[\'query\'])) == 3985\n        assert len(set(self.ys[\'gallery\'])) == 3985\n\n        self.im_paths = self.im_paths[dset_type]\n        self.ys = self.ys[dset_type]\n        self.I = self.I[dset_type]\n\n    def __len__(self):\n        return len(self.ys)\n\n    def nb_classes(self):\n        return len(set(self.ys))\n\n    def __getitem__(self, index):\n        im = PIL.Image.open(self.im_paths[index])\n        # convert gray to rgb\n        if len(list(im.split())) == 1 : im = im.convert(\'RGB\')\n        if self.transform is not None:\n            im = self.transform(im)\n        return im, self.ys[index], index\n\n    def get_label(self, index):\n        return self.ys[index]\n\n    def set_subset(self, I):\n        self.ys = [self.ys[i] for i in I]\n        self.I = [self.I[i] for i in I]\n        self.im_paths = [\n            self.im_paths[i] for i in I]\n\n'"
lib/data/set/sop.py,0,"b""from .base import *\n\nclass SOProducts(BaseDataset):\n    nb_train_all = 59551\n    nb_test_all = 60502\n    def __init__(self, root, classes, transform=None):\n        BaseDataset.__init__(self, root, classes, transform)\n\n        classes_train = range(0, 11318)\n        classes_test = range(11318, 22634)\n\n        if classes.start in classes_train:\n            if classes.stop - 1 in classes_train:\n                train = True\n\n        if classes.start in classes_test:\n            if classes.stop - 1 in classes_test:\n                train = False\n\n        with open(\n            os.path.join(\n            root,\n            'Ebay_{}.txt'.format('train' if train else 'test')\n            )\n        ) as f:\n\n            f.readline()\n            index = 0\n            nb_images = 0\n\n            for (image_id, class_id, _, path) in map(str.split, f):\n                nb_images += 1\n                if int(class_id) - 1 in classes:\n                    self.im_paths.append(os.path.join(root, path))\n                    self.ys.append(int(class_id) - 1)\n                    self.I += [index]\n                    index += 1\n\n            if train:\n                assert nb_images == type(self).nb_train_all\n            else:\n                assert nb_images == type(self).nb_test_all\n"""
lib/data/set/transform.py,0,"b'from __future__ import print_function\nfrom __future__ import division\n\nimport logging\nimport torchvision\nfrom torchvision import transforms\nimport PIL.Image\nimport torch\n\n\nclass Identity(): # used for skipping transforms\n    def __call__(self, im):\n        return im\n\n\nclass RGBToBGR():\n    def __call__(self, im):\n        assert im.mode == \'RGB\'\n        r, g, b = [im.getchannel(i) for i in range(3)]\n        # RGB mode also for BGR, `3x8-bit pixels, true color`, see PIL doc\n        im = PIL.Image.merge(\'RGB\', [b, g, r])\n        return im\n\n\nclass ScaleIntensities():\n    def __init__(self, in_range, out_range):\n        """""" Scales intensities. For example [-1, 1] -> [0, 255].""""""\n        self.in_range = in_range\n        self.out_range = out_range\n\n    def __oldcall__(self, tensor):\n        tensor.mul_(255)\n        return tensor\n\n    def __call__(self, tensor):\n        tensor = (\n            tensor - self.in_range[0]\n        ) / (\n            self.in_range[1] - self.in_range[0]\n        ) * (\n            self.out_range[1] - self.out_range[0]\n        ) + self.out_range[0]\n        return tensor\n\n\ndef make(sz_resize = 256, sz_crop = 227, mean = [104, 117, 128],\n        std = [1, 1, 1], rgb_to_bgr = True, is_train = True,\n        intensity_scale = None):\n    return transforms.Compose([\n        RGBToBGR() if rgb_to_bgr else Identity(),\n        transforms.RandomResizedCrop(sz_crop) if is_train else Identity(),\n        transforms.Resize(sz_resize) if not is_train else Identity(),\n        transforms.CenterCrop(sz_crop) if not is_train else Identity(),\n        transforms.RandomHorizontalFlip() if is_train else Identity(),\n        transforms.ToTensor(),\n        ScaleIntensities(\n            *intensity_scale) if intensity_scale is not None else Identity(),\n        transforms.Normalize(\n            mean=mean,\n            std=std,\n        )\n    ])\n\n'"
lib/data/set/vid.py,0,"b""from .base import *\n\nclass VehicleID(BaseDataset):\n    def __init__(self, root, classes, transform = None):\n        BaseDataset.__init__(self, root, classes, transform)\n        # amount of images deviates slightly from what's reported online\n\n        if classes == range(0, 13164):\n            fname = 'train_list.txt'\n        elif classes == range(13164, 13164 + 800):\n            fname = 'test_list_800.txt'\n        elif classes == range(13164, 13164 + 1600):\n            fname = 'test_list_1600.txt'\n        elif classes == range(13164, 13164 + 2400):\n            fname = 'test_list_2400.txt'\n        else:\n            print('Unknown range for classes selected')\n            input()\n\n        with open(\n            os.path.join(root, 'train_test_split', fname), 'r'\n        ) as f:\n            lines = [l.strip('\\n').split(' ') for l in f.readlines()]\n\n        i = 0\n        for l in lines:\n            self.im_paths += [os.path.join(root, 'image', l[0] + '.jpg')]\n            self.ys += [int(l[1])]\n            self.I += [i]\n            i += 1\n\n        idx_to_class = {idx: i for i, idx in enumerate(\n            sorted(set(self.ys))\n        )}\n        self.ys = list(\n            map(lambda x: idx_to_class[x], self.ys))\n\n    def nb_classes(self):\n        assert len(set(self.ys)) == len(set(self.classes))\n        return len(self.classes)\n"""
