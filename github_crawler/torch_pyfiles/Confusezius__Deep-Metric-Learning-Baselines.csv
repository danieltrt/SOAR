file_path,api_count,code
Standard_Training.py,10,"b'# Copyright 2019 Karsten Roth and Biagio Brattoli\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\n#################### LIBRARIES ########################\nimport warnings\nwarnings.filterwarnings(""ignore"")\n\nimport os, sys, numpy as np, argparse, imp, datetime, time, pickle as pkl, random, json\nos.chdir(os.path.dirname(os.path.realpath(__file__)))\n\nimport matplotlib\nmatplotlib.use(\'agg\')\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nimport pandas as pd\n\nimport torch, torch.nn as nn\nimport auxiliaries as aux\nimport datasets as data\n\nimport netlib as netlib\nimport losses as losses\nimport evaluate as eval\n\nimport torch.multiprocessing\ntorch.multiprocessing.set_sharing_strategy(\'file_system\')\n\n\n\n################### INPUT ARGUMENTS ###################\nparser = argparse.ArgumentParser()\n\n####### Main Parameter: Dataset to use for Training\nparser.add_argument(\'--dataset\',      default=\'cub200\',   type=str, help=\'Dataset to use.\')\n\n### General Training Parameters\nparser.add_argument(\'--lr\',                default=0.00001,  type=float, help=\'Learning Rate for network parameters.\')\nparser.add_argument(\'--fc_lr_mul\',         default=0,        type=float, help=\'OPTIONAL: Multiply the embedding layer learning rate by this value. If set to 0, the embedding layer shares the same learning rate.\')\nparser.add_argument(\'--n_epochs\',          default=70,       type=int,   help=\'Number of training epochs.\')\nparser.add_argument(\'--kernels\',           default=8,        type=int,   help=\'Number of workers for pytorch dataloader.\')\nparser.add_argument(\'--bs\',                default=112 ,     type=int,   help=\'Mini-Batchsize to use.\')\nparser.add_argument(\'--samples_per_class\', default=4,        type=int,   help=\'Number of samples in one class drawn before choosing the next class. Set to >1 for losses other than ProxyNCA.\')\nparser.add_argument(\'--seed\',              default=1,        type=int,   help=\'Random seed for reproducibility.\')\nparser.add_argument(\'--scheduler\',         default=\'step\',   type=str,   help=\'Type of learning rate scheduling. Currently: step & exp.\')\nparser.add_argument(\'--gamma\',             default=0.3,      type=float, help=\'Learning rate reduction after tau epochs.\')\nparser.add_argument(\'--decay\',             default=0.0004,   type=float, help=\'Weight decay for optimizer.\')\nparser.add_argument(\'--tau\',               default=[30,55],nargs=\'+\',type=int,help=\'Stepsize(s) before reducing learning rate.\')\n\n##### Loss-specific Settings\nparser.add_argument(\'--loss\',         default=\'marginloss\', type=str,   help=\'loss options: marginloss, triplet, npair, proxynca\')\nparser.add_argument(\'--sampling\',     default=\'distance\',   type=str,   help=\'For triplet-based losses: Modes of Sampling: random, semihard, distance.\')\n### MarginLoss\nparser.add_argument(\'--margin\',       default=0.2,          type=float, help=\'TRIPLET/MARGIN: Margin for Triplet-based Losses\')\nparser.add_argument(\'--beta_lr\',      default=0.0005,       type=float, help=\'MARGIN: Learning Rate for class margin parameters in MarginLoss\')\nparser.add_argument(\'--beta\',         default=1.2,          type=float, help=\'MARGIN: Initial Class Margin Parameter in Margin Loss\')\nparser.add_argument(\'--nu\',           default=0,            type=float, help=\'MARGIN: Regularisation value on betas in Margin Loss.\')\nparser.add_argument(\'--beta_constant\',                      action=\'store_true\', help=\'MARGIN: Use constant, un-trained beta.\')\n### ProxyNCA\nparser.add_argument(\'--proxy_lr\',     default=0.00001,     type=float, help=\'PROXYNCA: Learning Rate for Proxies in ProxyNCALoss.\')\n### NPair L2 Penalty\nparser.add_argument(\'--l2npair\',      default=0.02,        type=float, help=\'NPAIR: Penalty-value for non-normalized N-PAIR embeddings.\')\n\n##### Evaluation Settings\nparser.add_argument(\'--k_vals\',       nargs=\'+\', default=[1,2,4,8], type=int, help=\'Recall @ Values.\')\n\n##### Network parameters\nparser.add_argument(\'--embed_dim\',    default=128,         type=int,   help=\'Embedding dimensionality of the network. Note: in literature, dim=128 is used for ResNet50 and dim=512 for GoogLeNet.\')\nparser.add_argument(\'--arch\',         default=\'resnet50\',  type=str,   help=\'Network backend choice: resnet50, googlenet.\')\nparser.add_argument(\'--not_pretrained\',                    action=\'store_true\', help=\'If added, the network will be trained WITHOUT ImageNet-pretrained weights.\')\nparser.add_argument(\'--grad_measure\',                      action=\'store_true\', help=\'If added, gradients passed from embedding layer to the last conv-layer are stored in each iteration.\')\nparser.add_argument(\'--dist_measure\',                      action=\'store_true\', help=\'If added, the ratio between intra- and interclass distances is stored after each epoch.\')\n\n##### Setup Parameters\nparser.add_argument(\'--gpu\',          default=0,           type=int,   help=\'GPU-id for GPU to use.\')\nparser.add_argument(\'--savename\',     default=\'\',          type=str,   help=\'Save folder name if any special information is to be included.\')\n\n### Paths to datasets and storage folder\nparser.add_argument(\'--source_path\',  default=os.getcwd()+\'/Datasets\',         type=str, help=\'Path to training data.\')\nparser.add_argument(\'--save_path\',    default=os.getcwd()+\'/Training_Results\', type=str, help=\'Where to save everything.\')\n\n##### Read in parameters\nopt = parser.parse_args()\n\n\n""""""============================================================================""""""\nopt.source_path += \'/\'+opt.dataset\nopt.save_path   += \'/\'+opt.dataset\n\nif opt.dataset==\'online_products\':\n    opt.k_vals = [1,10,100,1000]\nif opt.dataset==\'in-shop\':\n    opt.k_vals = [1,10,20,30,50]\nif opt.dataset==\'vehicle_id\':\n    opt.k_vals = [1,5]\n\nif opt.loss == \'proxynca\':\n    opt.samples_per_class = 1\nelse:\n    assert not opt.bs%opt.samples_per_class, \'Batchsize needs to fit number of samples per class for distance sampling and margin/triplet loss!\'\n\nif opt.loss == \'npair\' or opt.loss == \'proxynca\': opt.sampling = \'None\'\n\nopt.pretrained = not opt.not_pretrained\n\n\n""""""============================================================================""""""\n################### GPU SETTINGS ###########################\nos.environ[""CUDA_DEVICE_ORDER""]   =""PCI_BUS_ID""\nos.environ[""CUDA_VISIBLE_DEVICES""]= str(opt.gpu)\n\n\n""""""============================================================================""""""\n#################### SEEDS FOR REPROD. #####################\ntorch.backends.cudnn.deterministic=True\nnp.random.seed(opt.seed); random.seed(opt.seed)\ntorch.manual_seed(opt.seed); torch.cuda.manual_seed(opt.seed); torch.cuda.manual_seed_all(opt.seed)\n\n\n""""""============================================================================""""""\n##################### NETWORK SETUP ##################\nopt.device = torch.device(\'cuda\')\n#Depending on the choice opt.arch, networkselect() returns the respective network model\nmodel      = netlib.networkselect(opt)\n\nprint(\'{} Setup for {} with {} sampling on {} complete with #weights: {}\'.format(opt.loss.upper(), opt.arch.upper(), opt.sampling.upper(), opt.dataset.upper(), aux.gimme_params(model)))\n\n#Push to Device\n_          = model.to(opt.device)\n#Place trainable parameter in list of parameters to train:\nif \'fc_lr_mul\' in vars(opt).keys() and opt.fc_lr_mul!=0:\n    all_but_fc_params = list(filter(lambda x: \'last_linear\' not in x[0],model.named_parameters()))\n    fc_params         = model.model.last_linear.parameters()\n    to_optim          = [{\'params\':all_but_fc_params,\'lr\':opt.lr,\'weight_decay\':opt.decay},\n                         {\'params\':fc_params,\'lr\':opt.lr*opt.fc_lr_mul,\'weight_decay\':opt.decay}]\nelse:\n    to_optim   = [{\'params\':model.parameters(),\'lr\':opt.lr,\'weight_decay\':opt.decay}]\n\n\n\n\n""""""============================================================================""""""\n#################### DATALOADER SETUPS ##################\n#Returns a dictionary containing \'training\', \'testing\', and \'evaluation\' dataloaders.\n#The \'testing\'-dataloader corresponds to the validation set, and the \'evaluation\'-dataloader\n#Is simply using the training set, however running under the same rules as \'testing\' dataloader,\n#i.e. no shuffling and no random cropping.\ndataloaders      = data.give_dataloaders(opt.dataset, opt)\n#Because the number of supervised classes is dataset dependent, we store them after\n#initializing the dataloader\nopt.num_classes  = len(dataloaders[\'training\'].dataset.avail_classes)\n\n\n\n\n""""""============================================================================""""""\n#################### CREATE LOGGING FILES ###############\n#Each dataset usually has a set of standard metrics to log. aux.metrics_to_examine()\n#returns a dict which lists metrics to log for training (\'train\') and validation/testing (\'val\')\n\nmetrics_to_log = aux.metrics_to_examine(opt.dataset, opt.k_vals)\n# example output: {\'train\': [\'Epochs\', \'Time\', \'Train Loss\', \'Time\'],\n#                  \'val\': [\'Epochs\',\'Time\',\'NMI\',\'F1\', \'Recall @ 1\',\'Recall @ 2\',\'Recall @ 4\',\'Recall @ 8\']}\n\n#Using the provided metrics of interest, we generate a LOGGER instance.\n#Note that \'start_new\' denotes that a new folder should be made in which everything will be stored.\n#This includes network weights as well.\nLOG = aux.LOGGER(opt, metrics_to_log, name=\'Base\', start_new=True)\n#If graphviz is installed on the system, a computational graph of the underlying\n#network will be made as well.\ntry:\n    aux.save_graph(opt, model)\nexcept:\n    print(\'Cannot generate graph!\')\n\n\n\n""""""============================================================================""""""\n##################### OPTIONAL EVALUATIONS #####################\n#Store the averaged gradients returned from the embedding to the last conv. layer.\nif opt.grad_measure:\n    grad_measure = eval.GradientMeasure(opt, name=\'baseline\')\n#Store the relative distances between average intra- and inter-class distance.\nif opt.dist_measure:\n    #Add a distance measure for training distance ratios\n    distance_measure = eval.DistanceMeasure(dataloaders[\'evaluation\'], opt, name=\'Train\', update_epochs=1)\n    # #If uncommented: Do the same for the test set\n    # distance_measure_test = eval.DistanceMeasure(dataloaders[\'testing\'], opt, name=\'Train\', update_epochs=1)\n\n\n""""""============================================================================""""""\n#################### LOSS SETUP ####################\n#Depending on opt.loss and opt.sampling, the respective criterion is returned,\n#and if the loss has trainable parameters, to_optim is appended.\ncriterion, to_optim = losses.loss_select(opt.loss, opt, to_optim)\n_ = criterion.to(opt.device)\n\n""""""============================================================================""""""\n#################### OPTIM SETUP ####################\n#As optimizer, Adam with standard parameters is used.\noptimizer    = torch.optim.Adam(to_optim)\n\nif opt.scheduler==\'exp\':\n    scheduler    = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=opt.gamma)\nelif opt.scheduler==\'step\':\n    scheduler    = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=opt.tau, gamma=opt.gamma)\nelif opt.scheduler==\'none\':\n    print(\'No scheduling used!\')\nelse:\n    raise Exception(\'No scheduling option for input: {}\'.format(opt.scheduler))\n\n\n\n""""""============================================================================""""""\n#################### TRAINER FUNCTION ############################\ndef train_one_epoch(train_dataloader, model, optimizer, criterion, opt, epoch):\n    """"""\n    This function is called every epoch to perform training of the network over one full\n    (randomized) iteration of the dataset.\n\n    Args:\n        train_dataloader: torch.utils.data.DataLoader, returns (augmented) training data.\n        model:            Network to train.\n        optimizer:        Optimizer to use for training.\n        criterion:        criterion to use during training.\n        opt:              argparse.Namespace, Contains all relevant parameters.\n        epoch:            int, Current epoch.\n\n    Returns:\n        Nothing!\n    """"""\n    loss_collect = []\n\n    start = time.time()\n\n    data_iterator = tqdm(train_dataloader, desc=\'Epoch {} Training...\'.format(epoch))\n    for i,(class_labels, input) in enumerate(data_iterator):\n        #Compute embeddings for input batch.\n        features  = model(input.to(opt.device))\n        #Compute loss.\n        loss      = criterion(features, class_labels)\n\n        #Ensure gradients are set to zero at beginning\n        optimizer.zero_grad()\n        #Compute gradients.\n        loss.backward()\n\n        if opt.grad_measure:\n            #If desired, save computed gradients.\n            grad_measure.include(model.model.last_linear)\n\n        #Update weights using comp. gradients.\n        optimizer.step()\n\n        #Store loss per iteration.\n        loss_collect.append(loss.item())\n        if i==len(train_dataloader)-1: data_iterator.set_description(\'Epoch (Train) {0}: Mean Loss [{1:.4f}]\'.format(epoch, np.mean(loss_collect)))\n\n    #Save metrics\n    LOG.log(\'train\', LOG.metrics_to_log[\'train\'], [epoch, np.round(time.time()-start,4), np.mean(loss_collect)])\n\n    if opt.grad_measure:\n        #Dump stored gradients to Pickle-File.\n        grad_measure.dump(epoch)\n\n\n\n\n""""""============================================================================""""""\n""""""========================== MAIN TRAINING PART ==============================""""""\n""""""============================================================================""""""\n################### SCRIPT MAIN ##########################\nprint(\'\\n-----\\n\')\nfor epoch in range(opt.n_epochs):\n    ### Print current learning rates for all parameters\n    if opt.scheduler!=\'none\': print(\'Running with learning rates {}...\'.format(\' | \'.join(\'{}\'.format(x) for x in scheduler.get_lr())))\n\n    ### Train one epoch\n    _ = model.train()\n    train_one_epoch(dataloaders[\'training\'], model, optimizer, criterion, opt, epoch)\n\n    ### Evaluate\n    _ = model.eval()\n    #Each dataset requires slightly different dataloaders.\n    if opt.dataset in [\'cars196\', \'cub200\', \'online_products\']:\n        eval_params = {\'dataloader\':dataloaders[\'testing\'], \'model\':model, \'opt\':opt, \'epoch\':epoch}\n    elif opt.dataset==\'in-shop\':\n        eval_params = {\'query_dataloader\':dataloaders[\'testing_query\'], \'gallery_dataloader\':dataloaders[\'testing_gallery\'], \'model\':model, \'opt\':opt, \'epoch\':epoch}\n    elif opt.dataset==\'vehicle_id\':\n        eval_params = {\'dataloaders\':[dataloaders[\'testing_set1\'], dataloaders[\'testing_set2\'], dataloaders[\'testing_set3\']], \'model\':model, \'opt\':opt, \'epoch\':epoch}\n\n    #Compute Evaluation metrics, print them and store in LOG.\n    eval.evaluate(opt.dataset, LOG, save=True, **eval_params)\n\n    #Update the Metric Plot and save it.\n    LOG.update_info_plot()\n\n    #(optional) compute ratio of intra- to interdistances.\n    if opt.dist_measure:\n        distance_measure.measure(model, epoch)\n        # distance_measure_test.measure(model, epoch)\n\n    ### Learning Rate Scheduling Step\n    if opt.scheduler != \'none\':\n        scheduler.step()\n\n    print(\'\\n-----\\n\')\n'"
auxiliaries.py,15,"b'# Copyright 2019 Karsten Roth and Biagio Brattoli\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n################## LIBRARIES ##############################\nimport warnings\nwarnings.filterwarnings(""ignore"")\n\nimport numpy as np, os, sys, pandas as pd, csv, random, datetime\n\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport pickle as pkl\n\nfrom sklearn import metrics\nfrom sklearn import cluster\n\nimport faiss\n\nimport losses as losses\n\n\n\n""""""=============================================================================================================""""""\n################# ACQUIRE NUMBER OF WEIGHTS #################\ndef gimme_params(model):\n    """"""\n    Provide number of trainable parameters (i.e. those requiring gradient computation) for input network.\n\n    Args:\n        model: PyTorch Network\n    Returns:\n        int, number of parameters.\n    """"""\n    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    return params\n\n\n################# SAVE TRAINING PARAMETERS IN NICE STRING #################\ndef gimme_save_string(opt):\n    """"""\n    Taking the set of parameters and convert it to easy-to-read string, which can be stored later.\n\n    Args:\n        opt: argparse.Namespace, contains all training-specific parameters.\n    Returns:\n        string, returns string summary of parameters.\n    """"""\n    varx = vars(opt)\n    base_str = \'\'\n    for key in varx:\n        base_str += str(key)\n        if isinstance(varx[key],dict):\n            for sub_key, sub_item in varx[key].items():\n                base_str += \'\\n\\t\'+str(sub_key)+\': \'+str(sub_item)\n        else:\n            base_str += \'\\n\\t\'+str(varx[key])\n        base_str+=\'\\n\\n\'\n    return base_str\n\n\n\ndef f1_score(model_generated_cluster_labels, target_labels, feature_coll, computed_centroids):\n    """"""\n    NOTE: MOSTLY ADAPTED FROM https://github.com/wzzheng/HDML on Hardness-Aware Deep Metric Learning.\n\n    Args:\n        model_generated_cluster_labels: np.ndarray [n_samples x 1], Cluster labels computed on top of data embeddings.\n        target_labels:                  np.ndarray [n_samples x 1], ground truth labels for each data sample.\n        feature_coll:                   np.ndarray [n_samples x embed_dim], total data embedding made by network.\n        computed_centroids:             np.ndarray [num_cluster=num_classes x embed_dim], cluster coordinates\n    Returns:\n        float, F1-score\n    """"""\n    from scipy.special import comb\n\n    d = np.zeros(len(feature_coll))\n    for i in range(len(feature_coll)):\n        d[i] = np.linalg.norm(feature_coll[i,:] - computed_centroids[model_generated_cluster_labels[i],:])\n\n    labels_pred = np.zeros(len(feature_coll))\n    for i in np.unique(model_generated_cluster_labels):\n        index = np.where(model_generated_cluster_labels == i)[0]\n        ind = np.argmin(d[index])\n        cid = index[ind]\n        labels_pred[index] = cid\n\n\n    N = len(target_labels)\n\n    #Cluster n_labels\n    avail_labels = np.unique(target_labels)\n    n_labels     = len(avail_labels)\n\n    #Count the number of objects in each cluster\n    count_cluster = np.zeros(n_labels)\n    for i in range(n_labels):\n        count_cluster[i] = len(np.where(target_labels == avail_labels[i])[0])\n\n    #Build a mapping from item_id to item index\n    keys     = np.unique(labels_pred)\n    num_item = len(keys)\n    values   = range(num_item)\n    item_map = dict()\n    for i in range(len(keys)):\n        item_map.update([(keys[i], values[i])])\n\n\n    #Count the number of objects of each item\n    count_item = np.zeros(num_item)\n    for i in range(N):\n        index = item_map[labels_pred[i]]\n        count_item[index] = count_item[index] + 1\n\n    #Compute True Positive (TP) plus False Positive (FP) count\n    tp_fp = 0\n    for k in range(n_labels):\n        if count_cluster[k] > 1:\n            tp_fp = tp_fp + comb(count_cluster[k], 2)\n\n    #Compute True Positive (TP) count\n    tp = 0\n    for k in range(n_labels):\n        member = np.where(target_labels == avail_labels[k])[0]\n        member_ids = labels_pred[member]\n\n        count = np.zeros(num_item)\n        for j in range(len(member)):\n            index = item_map[member_ids[j]]\n            count[index] = count[index] + 1\n\n        for i in range(num_item):\n            if count[i] > 1:\n                tp = tp + comb(count[i], 2)\n\n    #Compute  False Positive (FP) count\n    fp = tp_fp - tp\n\n    #Compute False Negative (FN) count\n    count = 0\n    for j in range(num_item):\n        if count_item[j] > 1:\n            count = count + comb(count_item[j], 2)\n    fn = count - tp\n\n    # compute F measure\n    beta = 1\n    P  = tp / (tp + fp)\n    R  = tp / (tp + fn)\n    F1 = (beta*beta + 1) * P * R / (beta*beta * P + R)\n\n    return F1\n\n\n\n\n""""""=============================================================================================================""""""\ndef eval_metrics_one_dataset(model, test_dataloader, device, k_vals, opt):\n    """"""\n    Compute evaluation metrics on test-dataset, e.g. NMI, F1 and Recall @ k.\n\n    Args:\n        model:              PyTorch network, network to compute evaluation metrics for.\n        test_dataloader:    PyTorch Dataloader, dataloader for test dataset, should have no shuffling and correct processing.\n        device:             torch.device, Device to run inference on.\n        k_vals:             list of int, Recall values to compute\n        opt:                argparse.Namespace, contains all training-specific parameters.\n    Returns:\n        F1 score (float), NMI score (float), recall_at_k (list of float), data embedding (np.ndarray)\n    """"""\n    torch.cuda.empty_cache()\n\n    _ = model.eval()\n    n_classes = len(test_dataloader.dataset.avail_classes)\n\n    with torch.no_grad():\n        ### For all test images, extract features\n        target_labels, feature_coll = [],[]\n        final_iter = tqdm(test_dataloader, desc=\'Computing Evaluation Metrics...\')\n        image_paths= [x[0] for x in test_dataloader.dataset.image_list]\n        for idx,inp in enumerate(final_iter):\n            input_img,target = inp[-1], inp[0]\n            target_labels.extend(target.numpy().tolist())\n            out = model(input_img.to(device))\n            feature_coll.extend(out.cpu().detach().numpy().tolist())\n\n        target_labels = np.hstack(target_labels).reshape(-1,1)\n        feature_coll  = np.vstack(feature_coll).astype(\'float32\')\n\n        torch.cuda.empty_cache()\n\n        ### Set Faiss CPU Cluster index\n        cpu_cluster_index = faiss.IndexFlatL2(feature_coll.shape[-1])\n        kmeans            = faiss.Clustering(feature_coll.shape[-1], n_classes)\n        kmeans.niter = 20\n        kmeans.min_points_per_centroid = 1\n        kmeans.max_points_per_centroid = 1000000000\n\n        ### Train Kmeans\n        kmeans.train(feature_coll, cpu_cluster_index)\n        computed_centroids = faiss.vector_float_to_array(kmeans.centroids).reshape(n_classes, feature_coll.shape[-1])\n\n        ### Assign feature points to clusters\n        faiss_search_index = faiss.IndexFlatL2(computed_centroids.shape[-1])\n        faiss_search_index.add(computed_centroids)\n        _, model_generated_cluster_labels = faiss_search_index.search(feature_coll, 1)\n\n        ### Compute NMI\n        NMI = metrics.cluster.normalized_mutual_info_score(model_generated_cluster_labels.reshape(-1), target_labels.reshape(-1))\n\n\n        ### Recover max(k_vals) nearest neighbours to use for recall computation\n        faiss_search_index  = faiss.IndexFlatL2(feature_coll.shape[-1])\n        faiss_search_index.add(feature_coll)\n        _, k_closest_points = faiss_search_index.search(feature_coll, int(np.max(k_vals)+1))\n        k_closest_classes   = target_labels.reshape(-1)[k_closest_points[:,1:]]\n\n        ### Compute Recall\n        recall_all_k = []\n        for k in k_vals:\n            recall_at_k = np.sum([1 for target, recalled_predictions in zip(target_labels, k_closest_classes) if target in recalled_predictions[:k]])/len(target_labels)\n            recall_all_k.append(recall_at_k)\n\n        ### Compute F1 Score\n        F1 = f1_score(model_generated_cluster_labels, target_labels, feature_coll, computed_centroids)\n\n    return F1, NMI, recall_all_k, feature_coll\n\n\n\ndef eval_metrics_query_and_gallery_dataset(model, query_dataloader, gallery_dataloader, device, k_vals, opt):\n    """"""\n    Compute evaluation metrics on test-dataset, e.g. NMI, F1 and Recall @ k.\n\n    Args:\n        model:               PyTorch network, network to compute evaluation metrics for.\n        query_dataloader:    PyTorch Dataloader, dataloader for query dataset, for which nearest neighbours in the gallery dataset are retrieved.\n        gallery_dataloader:  PyTorch Dataloader, dataloader for gallery dataset, provides target samples which are to be retrieved in correspondance to the query dataset.\n        device:              torch.device, Device to run inference on.\n        k_vals:              list of int, Recall values to compute\n        opt:                 argparse.Namespace, contains all training-specific parameters.\n    Returns:\n        F1 score (float), NMI score (float), recall_at_ks (list of float), query data embedding (np.ndarray), gallery data embedding (np.ndarray)\n    """"""\n    torch.cuda.empty_cache()\n\n    _ = model.eval()\n    n_classes = len(query_dataloader.dataset.avail_classes)\n\n    with torch.no_grad():\n        ### For all query test images, extract features\n        query_target_labels, query_feature_coll     = [],[]\n        query_image_paths   = [x[0] for x in query_dataloader.dataset.image_list]\n        query_iter = tqdm(query_dataloader, desc=\'Extraction Query Features\')\n        for idx,inp in enumerate(query_iter):\n            input_img,target = inp[-1], inp[0]\n            query_target_labels.extend(target.numpy().tolist())\n            out = model(input_img.to(device))\n            query_feature_coll.extend(out.cpu().detach().numpy().tolist())\n\n        ### For all gallery test images, extract features\n        gallery_target_labels, gallery_feature_coll = [],[]\n        gallery_image_paths = [x[0] for x in gallery_dataloader.dataset.image_list]\n        gallery_iter = tqdm(gallery_dataloader, desc=\'Extraction Gallery Features\')\n        for idx,inp in enumerate(gallery_iter):\n            input_img,target = inp[-1], inp[0]\n            gallery_target_labels.extend(target.numpy().tolist())\n            out = model(input_img.to(device))\n            gallery_feature_coll.extend(out.cpu().detach().numpy().tolist())\n\n\n        query_target_labels, query_feature_coll     = np.hstack(query_target_labels).reshape(-1,1), np.vstack(query_feature_coll).astype(\'float32\')\n        gallery_target_labels, gallery_feature_coll = np.hstack(gallery_target_labels).reshape(-1,1), np.vstack(gallery_feature_coll).astype(\'float32\')\n\n        torch.cuda.empty_cache()\n\n        ### Set CPU Cluster index\n        stackset    = np.concatenate([query_feature_coll, gallery_feature_coll],axis=0)\n        stacklabels = np.concatenate([query_target_labels, gallery_target_labels],axis=0)\n        cpu_cluster_index = faiss.IndexFlatL2(stackset.shape[-1])\n        kmeans            = faiss.Clustering(stackset.shape[-1], n_classes)\n        kmeans.niter = 20\n        kmeans.min_points_per_centroid = 1\n        kmeans.max_points_per_centroid = 1000000000\n\n        ### Train Kmeans\n        kmeans.train(stackset, cpu_cluster_index)\n        computed_centroids = faiss.vector_float_to_array(kmeans.centroids).reshape(n_classes, stackset.shape[-1])\n\n        ### Assign feature points to clusters\n        faiss_search_index = faiss.IndexFlatL2(computed_centroids.shape[-1])\n        faiss_search_index.add(computed_centroids)\n        _, model_generated_cluster_labels = faiss_search_index.search(stackset, 1)\n\n        ### Compute NMI\n        NMI = metrics.cluster.normalized_mutual_info_score(model_generated_cluster_labels.reshape(-1), stacklabels.reshape(-1))\n\n        ### Recover max(k_vals) nearest neighbours to use for recall computation\n        faiss_search_index  = faiss.IndexFlatL2(gallery_feature_coll.shape[-1])\n        faiss_search_index.add(gallery_feature_coll)\n        _, k_closest_points = faiss_search_index.search(query_feature_coll, int(np.max(k_vals)))\n        k_closest_classes   = gallery_target_labels.reshape(-1)[k_closest_points]\n\n        ### Compute Recall\n        recall_all_k = []\n        for k in k_vals:\n            recall_at_k = np.sum([1 for target, recalled_predictions in zip(query_target_labels, k_closest_classes) if target in recalled_predictions[:k]])/len(query_target_labels)\n            recall_all_k.append(recall_at_k)\n        recall_str = \', \'.join(\'@{0}: {1:.4f}\'.format(k,rec) for k,rec in zip(k_vals, recall_all_k))\n\n        ### Compute F1 score\n        F1 = f1_score(model_generated_cluster_labels, stacklabels, stackset, computed_centroids)\n\n    return F1, NMI, recall_all_k, query_feature_coll, gallery_feature_coll\n\n\n\n""""""=============================================================================================================""""""\n####### RECOVER CLOSEST EXAMPLE IMAGES #######\ndef recover_closest_one_dataset(feature_matrix_all, image_paths, save_path, n_image_samples=10, n_closest=3):\n    """"""\n    Provide sample recoveries.\n\n    Args:\n        feature_matrix_all: np.ndarray [n_samples x embed_dim], full data embedding of test samples.\n        image_paths:        list [n_samples], list of datapaths corresponding to <feature_matrix_all>\n        save_path:          str, where to store sample image.\n        n_image_samples:    Number of sample recoveries.\n        n_closest:          Number of closest recoveries to show.\n    Returns:\n        Nothing!\n    """"""\n    image_paths = np.array([x[0] for x in image_paths])\n    sample_idxs = np.random.choice(np.arange(len(feature_matrix_all)), n_image_samples)\n\n    faiss_search_index = faiss.IndexFlatL2(feature_matrix_all.shape[-1])\n    faiss_search_index.add(feature_matrix_all)\n    _, closest_feature_idxs = faiss_search_index.search(feature_matrix_all, n_closest+1)\n\n    sample_paths = image_paths[closest_feature_idxs][sample_idxs]\n\n    f,axes = plt.subplots(n_image_samples, n_closest+1)\n    for i,(ax,plot_path) in enumerate(zip(axes.reshape(-1), sample_paths.reshape(-1))):\n        ax.imshow(np.array(Image.open(plot_path)))\n        ax.set_xticks([])\n        ax.set_yticks([])\n        if i%(n_closest+1):\n            ax.axvline(x=0, color=\'g\', linewidth=13)\n        else:\n            ax.axvline(x=0, color=\'r\', linewidth=13)\n    f.set_size_inches(10,20)\n    f.tight_layout()\n    f.savefig(save_path)\n    plt.close()\n\n\n####### RECOVER CLOSEST EXAMPLE IMAGES #######\ndef recover_closest_inshop(query_feature_matrix_all, gallery_feature_matrix_all, query_image_paths, gallery_image_paths, save_path, n_image_samples=10, n_closest=3):\n    """"""\n    Provide sample recoveries.\n\n    Args:\n        query_feature_matrix_all:   np.ndarray [n_query_samples x embed_dim], full data embedding of query samples.\n        gallery_feature_matrix_all: np.ndarray [n_gallery_samples x embed_dim], full data embedding of gallery samples.\n        query_image_paths:          list [n_samples], list of datapaths corresponding to <query_feature_matrix_all>\n        gallery_image_paths:        list [n_samples], list of datapaths corresponding to <gallery_feature_matrix_all>\n        save_path:          str, where to store sample image.\n        n_image_samples:    Number of sample recoveries.\n        n_closest:          Number of closest recoveries to show.\n    Returns:\n        Nothing!\n    """"""\n    query_image_paths, gallery_image_paths   = np.array(query_image_paths), np.array(gallery_image_paths)\n    sample_idxs = np.random.choice(np.arange(len(query_feature_matrix_all)), n_image_samples)\n\n    faiss_search_index = faiss.IndexFlatL2(gallery_feature_matrix_all.shape[-1])\n    faiss_search_index.add(gallery_feature_matrix_all)\n    _, closest_feature_idxs = faiss_search_index.search(query_feature_matrix_all, n_closest)\n\n    image_paths  = gallery_image_paths[closest_feature_idxs]\n    image_paths  = np.concatenate([query_image_paths.reshape(-1,1), image_paths],axis=-1)\n\n    sample_paths = image_paths[closest_feature_idxs][sample_idxs]\n\n    f,axes = plt.subplots(n_image_samples, n_closest+1)\n    for i,(ax,plot_path) in enumerate(zip(axes.reshape(-1), sample_paths.reshape(-1))):\n        ax.imshow(np.array(Image.open(plot_path)))\n        ax.set_xticks([])\n        ax.set_yticks([])\n        if i%(n_closest+1):\n            ax.axvline(x=0, color=\'g\', linewidth=13)\n        else:\n            ax.axvline(x=0, color=\'r\', linewidth=13)\n    f.set_size_inches(10,20)\n    f.tight_layout()\n    f.savefig(save_path)\n    plt.close()\n\n\n\n""""""=============================================================================================================""""""\n################## SET NETWORK TRAINING CHECKPOINT #####################\ndef set_checkpoint(model, opt, progress_saver, savepath):\n    """"""\n    Store relevant parameters (model and progress saver, as well as parameter-namespace).\n    Can be easily extend for other stuff.\n\n    Args:\n        model:          PyTorch network, network whose parameters are to be saved.\n        opt:            argparse.Namespace, includes all training-specific parameters\n        progress_saver: subclass of LOGGER-class, contains a running memory of all training metrics.\n        savepath:       str, where to save checkpoint.\n    Returns:\n        Nothing!\n    """"""\n    torch.save({\'state_dict\':model.state_dict(), \'opt\':opt,\n                \'progress\':progress_saver}, savepath)\n\n\n\n\n""""""=============================================================================================================""""""\n################## WRITE TO CSV FILE #####################\nclass CSV_Writer():\n    """"""\n    Class to append newly compute training metrics to a csv file\n    for data logging.\n    Is used together with the LOGGER class.\n    """"""\n    def __init__(self, save_path, columns):\n        """"""\n        Args:\n            save_path: str, where to store the csv file\n            columns:   list of str, name of csv columns under which the resp. metrics are stored.\n        Returns:\n            Nothing!\n        """"""\n        self.save_path = save_path\n        self.columns   = columns\n\n        with open(self.save_path, ""a"") as csv_file:\n            writer = csv.writer(csv_file, delimiter="","")\n            writer.writerow(self.columns)\n\n    def log(self, inputs):\n        """"""\n        log one set of entries to the csv.\n\n        Args:\n            inputs: [list of int/str/float], values to append to the csv. Has to be of the same length as self.columns.\n        Returns:\n            Nothing!\n        """"""\n        with open(self.save_path, ""a"") as csv_file:\n            writer = csv.writer(csv_file, delimiter=\',\')\n            writer.writerow(inputs)\n\n\n\n################## PLOT SUMMARY IMAGE #####################\nclass InfoPlotter():\n    """"""\n    Plotter class to visualize training progression by showing\n    different metrics.\n    """"""\n    def __init__(self, save_path, title=\'Training Log\', figsize=(20,15)):\n        """"""\n        Args:\n            save_path: str, where to store the create plot.\n            title:     placeholder title of plot\n            figsize:   base size of saved figure\n        Returns:\n            Nothing!\n        """"""\n        self.save_path = save_path\n        self.title     = title\n        self.figsize   = figsize\n        #Colors for validation lines\n        self.v_colors    = [\'r\',\'g\',\'b\',\'y\',\'m\',\'k\',\'c\']\n        #Colors for training lines\n        self.t_colors    = [\'k\',\'b\',\'r\',\'g\']\n\n    def make_plot(self, t_epochs, v_epochs, t_metrics, v_metrics, t_labels, v_labels, appendix=None):\n        """"""\n        Given a list of iterated epochs, visualize the progression of various training/testing metrics.\n\n        Args:\n            t_epochs:  [list of int/float], list of epochs for which training metrics were collected (e.g. Training Loss)\n            v_epochs:  [list of int/float], list of epochs for which validation metrics were collected (e.g. Recall @ k)\n            t_metrics: [list of float], list of training metrics per epoch\n            v_metrics: [list of list of int/float], contains all computed validation metrics\n            t_labels, v_labels: [list of str], names for each metric that is plotted.\n        Returns:\n            Nothing!\n        """"""\n        plt.style.use(\'ggplot\')\n\n        f,axes = plt.subplots(1,2)\n\n        #Visualize Training Loss\n        for i in range(len(t_metrics)):\n            axes[0].plot(t_epochs, t_metrics[i], \'-{}\'.format(self.t_colors[i]), linewidth=1, label=t_labels[i])\n        axes[0].set_title(\'Training Performance\', fontsize=19)\n\n        axes[0].legend(fontsize=16)\n\n        axes[0].tick_params(axis=\'both\', which=\'major\', labelsize=16)\n        axes[0].tick_params(axis=\'both\', which=\'minor\', labelsize=16)\n\n        #Visualize Validation metrics\n        for i in range(len(v_metrics)):\n            axes[1].plot(v_epochs, v_metrics[i], \'-{}\'.format(self.v_colors[i]), linewidth=1, label=v_labels[i])\n        axes[1].set_title(self.title, fontsize=19)\n\n        axes[1].legend(fontsize=16)\n\n        axes[1].tick_params(axis=\'both\', which=\'major\', labelsize=16)\n        axes[1].tick_params(axis=\'both\', which=\'minor\', labelsize=16)\n\n        f.set_size_inches(2*self.figsize[0], self.figsize[1])\n\n        savepath = self.save_path\n        f.savefig(self.save_path, bbox_inches=\'tight\')\n\n        plt.close()\n\n\n################## GENERATE LOGGING FOLDER/FILES #######################\ndef set_logging(opt):\n    """"""\n    Generate the folder in which everything is saved.\n    If opt.savename is given, folder will take on said name.\n    If not, a name based on the start time is provided.\n    If the folder already exists, it will by iterated until it can be created without\n    deleting existing data.\n    The current opt.save_path will be extended to account for the new save_folder name.\n\n    Args:\n        opt: argparse.Namespace, contains all training-specific parameters.\n    Returns:\n        Nothing!\n    """"""\n    checkfolder = opt.save_path+\'/\'+opt.savename\n\n    #Create start-time-based name if opt.savename is not give.\n    if opt.savename == \'\':\n        date = datetime.datetime.now()\n        time_string = \'{}-{}-{}-{}-{}-{}\'.format(date.year, date.month, date.day, date.hour, date.minute, date.second)\n        checkfolder = opt.save_path+\'/{}_{}_\'.format(opt.dataset.upper(), opt.arch.upper())+time_string\n\n    #If folder already exists, iterate over it until is doesn\'t.\n    counter     = 1\n    while os.path.exists(checkfolder):\n        checkfolder = opt.save_path+\'/\'+opt.savename+\'_\'+str(counter)\n        counter += 1\n\n    #Create Folder\n    os.makedirs(checkfolder)\n    opt.save_path = checkfolder\n\n    #Store training parameters as text and pickle in said folder.\n    with open(opt.save_path+\'/Parameter_Info.txt\',\'w\') as f:\n        f.write(gimme_save_string(opt))\n    pkl.dump(opt,open(opt.save_path+""/hypa.pkl"",""wb""))\n\n\nclass LOGGER():\n    """"""\n    This class provides a collection of logging properties that are useful for training.\n    These include setting the save folder, in which progression of training/testing metrics is visualized,\n    csv log-files are stored, sample recoveries are plotted and an internal data saver.\n    """"""\n    def __init__(self, opt, metrics_to_log, name=\'Basic\', start_new=True):\n        """"""\n        Args:\n            opt:               argparse.Namespace, contains all training-specific parameters.\n            metrics_to_log:    dict, dictionary which shows in what structure the data should be saved.\n                               is given as the output of aux.metrics_to_examine. Example:\n                               {\'train\': [\'Epochs\', \'Time\', \'Train Loss\', \'Time\'],\n                                \'val\': [\'Epochs\',\'Time\',\'NMI\',\'F1\', \'Recall @ 1\',\'Recall @ 2\',\'Recall @ 4\',\'Recall @ 8\']}\n            name:              Name of this logger. Will be used to distinguish logged files from other LOGGER instances.\n            start_new:         If set to true, a new save folder will be created initially.\n        Returns:\n            Nothing!\n        """"""\n        self.prop           = opt\n        self.metrics_to_log = metrics_to_log\n\n        ### Make Logging Directories\n        if start_new: set_logging(opt)\n\n        ### Set INFO-PLOTS\n        if self.prop.dataset != \'vehicle_id\':\n            self.info_plot = InfoPlotter(opt.save_path+\'/InfoPlot_{}.svg\'.format(name))\n        else:\n            self.info_plot = {\'Set {}\'.format(i): InfoPlotter(opt.save_path+\'/InfoPlot_{}_Set{}.svg\'.format(name,i+1)) for i in range(3)}\n\n        ### Set Progress Saver Dict\n        self.progress_saver = self.provide_progress_saver(metrics_to_log)\n\n        ### Set CSV Writters\n        self.csv_loggers= {mode:CSV_Writer(opt.save_path+\'/log_\'+mode+\'_\'+name+\'.csv\', lognames) for mode, lognames in metrics_to_log.items()}\n\n\n    def provide_progress_saver(self, metrics_to_log):\n        """"""\n        Provide Progress Saver dictionary.\n\n        Args:\n            metrics_to_log: see __init__(). Describes the structure of Progress_Saver.\n        """"""\n        Progress_Saver = {key:{sub_key:[] for sub_key in metrics_to_log[key]} for key in metrics_to_log.keys()}\n        return Progress_Saver\n\n    def log(self, main_keys, metric_keys, values):\n        """"""\n        Actually log new values in csv and Progress Saver dict internally.\n        Args:\n            main_keys:      Main key in which data will be stored. Normally is either \'train\' for training metrics or \'val\' for validation metrics.\n            metric_keys:    Needs to follow the list length of self.progress_saver[main_key(s)]. List of metric keys that are extended with new values.\n            values:         Needs to be a list of the same structure as metric_keys. Actual values that are appended.\n        """"""\n        if not isinstance(main_keys, list):   main_keys = [main_keys]\n        if not isinstance(metric_keys, list): metric_keys = [metric_keys]\n        if not isinstance(values, list):      values = [values]\n\n        #Log data to progress saver dict.\n        for main_key in main_keys:\n            for value, metric_key in zip(values, metric_keys):\n                self.progress_saver[main_key][metric_key].append(value)\n\n        #Append data to csv.\n        self.csv_loggers[main_key].log(values)\n\n    def update_info_plot(self):\n        """"""\n        Create a new updated version of training/metric progression plot.\n\n        Args:\n            None\n        Returns:\n            Nothing!\n        """"""\n        t_epochs         = self.progress_saver[\'val\'][\'Epochs\']\n        t_loss_list      = [self.progress_saver[\'train\'][\'Train Loss\']]\n        t_legend_handles = [\'Train Loss\']\n\n        v_epochs         = self.progress_saver[\'val\'][\'Epochs\']\n        #Because Vehicle-ID normally uses three different test sets, a distinction has to be made.\n        if self.prop.dataset != \'vehicle_id\':\n            title = \' | \'.join(key+\': {0:3.3f}\'.format(np.max(item)) for key,item in self.progress_saver[\'val\'].items() if key not in [\'Time\', \'Epochs\'])\n            self.info_plot.title = title\n            v_metric_list    = [self.progress_saver[\'val\'][key] for key in self.progress_saver[\'val\'].keys() if key not in [\'Time\', \'Epochs\']]\n            v_legend_handles = [key for key in self.progress_saver[\'val\'].keys() if key not in [\'Time\', \'Epochs\']]\n\n            self.info_plot.make_plot(t_epochs, v_epochs, t_loss_list, v_metric_list, t_legend_handles, v_legend_handles)\n        else:\n            #Iterate over all test sets.\n            for i in range(3):\n                title = \' | \'.join(key+\': {0:3.3f}\'.format(np.max(item)) for key,item in self.progress_saver[\'val\'].items() if key not in [\'Time\', \'Epochs\'] and \'Set {}\'.format(i) in key)\n                self.info_plot[\'Set {}\'.format(i)].title = title\n                v_metric_list    = [self.progress_saver[\'val\'][key] for key in self.progress_saver[\'val\'].keys() if key not in [\'Time\', \'Epochs\'] and \'Set {}\'.format(i) in key]\n                v_legend_handles = [key for key in self.progress_saver[\'val\'].keys() if key not in [\'Time\', \'Epochs\'] and \'Set {}\'.format(i) in key]\n                self.info_plot[\'Set {}\'.format(i)].make_plot(t_epochs, v_epochs, t_loss_list, v_metric_list, t_legend_handles, v_legend_handles, appendix=\'set_{}\'.format(i))\n\ndef metrics_to_examine(dataset, k_vals):\n    """"""\n    Please only use either of the following keys:\n    -> Epochs, Time, Train Loss for training\n    -> Epochs, Time, NMI, F1 & Recall @ k for validation\n\n    Args:\n        dataset: str, dataset for which a storing structure for LOGGER.progress_saver is to be made.\n        k_vals:  list of int, Recall @ k - values.\n    Returns:\n        metric_dict: Dictionary representing the storing structure for LOGGER.progress_saver. See LOGGER.__init__() for an example.\n    """"""\n    metric_dict        = {\'train\':[\'Epochs\',\'Time\',\'Train Loss\']}\n\n    if dataset==\'vehicle_id\':\n        metric_dict[\'val\'] = [\'Epochs\',\'Time\']\n        #Vehicle_ID uses three test sets\n        for i in range(3):\n            metric_dict[\'val\'] += [\'Set {} NMI\'.format(i), \'Set {} F1\'.format(i)]\n            for k in k_vals:\n                metric_dict[\'val\'] += [\'Set {} Recall @ {}\'.format(i,k)]\n    else:\n        metric_dict[\'val\'] = [\'Epochs\',\'Time\',\'NMI\', \'F1\']\n        metric_dict[\'val\'] += [\'Recall @ {}\'.format(k) for k in k_vals]\n\n    return metric_dict\n\n\n\n""""""=================================================================================================""""""\ndef run_kmeans(features, n_cluster):\n    """"""\n    Run kmeans on a set of features to find <n_cluster> cluster.\n\n    Args:\n        features:  np.ndarrary [n_samples x embed_dim], embedding training/testing samples for which kmeans should be performed.\n        n_cluster: int, number of cluster.\n    Returns:\n        cluster_assignments: np.ndarray [n_samples x 1], per sample provide the respective cluster label it belongs to.\n    """"""\n    n_samples, dim = features.shape\n    kmeans = faiss.Kmeans(dim, n_cluster)\n    kmeans.n_iter, kmeans.min_points_per_centroid, kmeans.max_points_per_centroid = 20,5,1000000000\n    kmeans.train(features)\n    _, cluster_assignments = kmeans.index.search(features,1)\n    return cluster_assignments\n\n\n\n""""""=============================================================================================================""""""\ndef save_graph(opt, model):\n    """"""\n    Generate Network Graph.\n    NOTE: Requires the installation of the graphviz library on you system.\n\n    Args:\n        opt:   argparse.Namespace, contains all training-specific parameters.\n        model: PyTorch Network, network for which the computational graph should be visualized.\n    Returns:\n        Nothing!\n    """"""\n    inp = torch.randn((1,3,224,224)).to(opt.device)\n    network_output = model(inp)\n    if isinstance(network_output, dict): network_output = network_output[\'Class\']\n\n    from graphviz import Digraph\n    def make_dot(var, savename, params=None):\n        """"""\n        Generate a symbolic representation of the network graph.\n        """"""\n        if params is not None:\n            assert all(isinstance(p, Variable) for p in params.values())\n            param_map = {id(v): k for k, v in params.items()}\n\n        node_attr = dict(style=\'filled\',\n                         shape=\'box\',\n                         align=\'left\',\n                         fontsize=\'6\',\n                         ranksep=\'0.1\',\n                         height=\'0.6\',\n                         width=\'1\')\n        dot  = Digraph(node_attr=node_attr, format=\'svg\', graph_attr=dict(size=""40,10"", rankdir=\'LR\', rank=\'same\'))\n        seen = set()\n\n        def size_to_str(size):\n            return \'(\'+(\', \').join([\'%d\' % v for v in size])+\')\'\n\n        def add_nodes(var):\n            replacements  = [\'Backward\', \'Th\', \'Cudnn\']\n            color_assigns = {\'Convolution\':\'orange\',\n                             \'ConvolutionTranspose\': \'lightblue\',\n                             \'Add\': \'red\',\n                             \'Cat\': \'green\',\n                             \'Softmax\': \'yellow\',\n                             \'Sigmoid\': \'yellow\',\n                             \'Copys\':   \'yellow\'}\n            if var not in seen:\n                op1 = torch.is_tensor(var)\n                op2 = not torch.is_tensor(var) and str(type(var).__name__)!=\'AccumulateGrad\'\n\n                text = str(type(var).__name__)\n                for rep in replacements:\n                    text = text.replace(rep, \'\')\n                color = color_assigns[text] if text in color_assigns.keys() else \'gray\'\n\n                if \'Pool\' in text: color = \'lightblue\'\n\n                if op1 or op2:\n                    if hasattr(var, \'next_functions\'):\n                        count = 0\n                        for i, u in enumerate(var.next_functions):\n                            if str(type(u[0]).__name__)==\'AccumulateGrad\':\n                                if count==0: attr_text = \'\\nParameter Sizes:\\n\'\n                                attr_text += size_to_str(u[0].variable.size())\n                                count += 1\n                                attr_text += \' \'\n                        if count>0: text += attr_text\n\n\n                if op1:\n                    dot.node(str(id(var)), size_to_str(var.size()), fillcolor=\'orange\')\n                if op2:\n                    dot.node(str(id(var)), text, fillcolor=color)\n\n                seen.add(var)\n\n                if op1 or op2:\n                    if hasattr(var, \'next_functions\'):\n                        for u in var.next_functions:\n                            if u[0] is not None:\n                                if str(type(u[0]).__name__)!=\'AccumulateGrad\':\n                                    dot.edge(str(id(u[0])), str(id(var)))\n                                    add_nodes(u[0])\n                    if hasattr(var, \'saved_tensors\'):\n                        for t in var.saved_tensors:\n                            dot.edge(str(id(t)), str(id(var)))\n                            add_nodes(t)\n\n        add_nodes(var.grad_fn)\n        dot.save(savename)\n        return dot\n\n    if not os.path.exists(opt.save_path):\n        raise Exception(\'No save folder {} available!\'.format(opt.save_path))\n\n    viz_graph = make_dot(network_output, opt.save_path+""/Network_Graphs""+""/{}_network_graph"".format(opt.arch))\n    viz_graph.format = \'svg\'\n    viz_graph.render()\n\n    torch.cuda.empty_cache()\n'"
auxiliaries_nofaiss.py,15,"b'# Copyright 2019 Karsten Roth and Biagio Brattoli\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n################## LIBRARIES ##############################\nimport warnings\nwarnings.filterwarnings(""ignore"")\n\nimport numpy as np, os, sys, pandas as pd, csv, random, datetime\n\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport pickle as pkl\n\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import squareform, pdist, cdist\n\n# import faiss\n\nimport losses as losses\n\n\n\n""""""=============================================================================================================""""""\n################# ACQUIRE NUMBER OF WEIGHTS #################\ndef gimme_params(model):\n    """"""\n    Provide number of trainable parameters (i.e. those requiring gradient computation) for input network.\n\n    Args:\n        model: PyTorch Network\n    Returns:\n        int, number of parameters.\n    """"""\n    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    return params\n\n\n################# SAVE TRAINING PARAMETERS IN NICE STRING #################\ndef gimme_save_string(opt):\n    """"""\n    Taking the set of parameters and convert it to easy-to-read string, which can be stored later.\n\n    Args:\n        opt: argparse.Namespace, contains all training-specific parameters.\n    Returns:\n        string, returns string summary of parameters.\n    """"""\n    varx = vars(opt)\n    base_str = \'\'\n    for key in varx:\n        base_str += str(key)\n        if isinstance(varx[key],dict):\n            for sub_key, sub_item in varx[key].items():\n                base_str += \'\\n\\t\'+str(sub_key)+\': \'+str(sub_item)\n        else:\n            base_str += \'\\n\\t\'+str(varx[key])\n        base_str+=\'\\n\\n\'\n    return base_str\n\n\n\ndef f1_score(model_generated_cluster_labels, target_labels, feature_coll, computed_centroids):\n    """"""\n    NOTE: MOSTLY ADAPTED FROM https://github.com/wzzheng/HDML on Hardness-Aware Deep Metric Learning.\n\n    Args:\n        model_generated_cluster_labels: np.ndarray [n_samples x 1], Cluster labels computed on top of data embeddings.\n        target_labels:                  np.ndarray [n_samples x 1], ground truth labels for each data sample.\n        feature_coll:                   np.ndarray [n_samples x embed_dim], total data embedding made by network.\n        computed_centroids:             np.ndarray [num_cluster=num_classes x embed_dim], cluster coordinates\n    Returns:\n        float, F1-score\n    """"""\n    from scipy.special import comb\n\n    d = np.zeros(len(feature_coll))\n    for i in range(len(feature_coll)):\n        d[i] = np.linalg.norm(feature_coll[i,:] - computed_centroids[model_generated_cluster_labels[i],:])\n\n    labels_pred = np.zeros(len(feature_coll))\n    for i in np.unique(model_generated_cluster_labels):\n        index = np.where(model_generated_cluster_labels == i)[0]\n        ind = np.argmin(d[index])\n        cid = index[ind]\n        labels_pred[index] = cid\n\n\n    N = len(target_labels)\n\n    #Cluster n_labels\n    avail_labels = np.unique(target_labels)\n    n_labels     = len(avail_labels)\n\n    #Count the number of objects in each cluster\n    count_cluster = np.zeros(n_labels)\n    for i in range(n_labels):\n        count_cluster[i] = len(np.where(target_labels == avail_labels[i])[0])\n\n    #Build a mapping from item_id to item index\n    keys     = np.unique(labels_pred)\n    num_item = len(keys)\n    values   = range(num_item)\n    item_map = dict()\n    for i in range(len(keys)):\n        item_map.update([(keys[i], values[i])])\n\n\n    #Count the number of objects of each item\n    count_item = np.zeros(num_item)\n    for i in range(N):\n        index = item_map[labels_pred[i]]\n        count_item[index] = count_item[index] + 1\n\n    #Compute True Positive (TP) plus False Positive (FP) count\n    tp_fp = 0\n    for k in range(n_labels):\n        if count_cluster[k] > 1:\n            tp_fp = tp_fp + comb(count_cluster[k], 2)\n\n    #Compute True Positive (TP) count\n    tp = 0\n    for k in range(n_labels):\n        member = np.where(target_labels == avail_labels[k])[0]\n        member_ids = labels_pred[member]\n\n        count = np.zeros(num_item)\n        for j in range(len(member)):\n            index = item_map[member_ids[j]]\n            count[index] = count[index] + 1\n\n        for i in range(num_item):\n            if count[i] > 1:\n                tp = tp + comb(count[i], 2)\n\n    #Compute  False Positive (FP) count\n    fp = tp_fp - tp\n\n    #Compute False Negative (FN) count\n    count = 0\n    for j in range(num_item):\n        if count_item[j] > 1:\n            count = count + comb(count_item[j], 2)\n    fn = count - tp\n\n    # compute F measure\n    beta = 1\n    P  = tp / (tp + fp)\n    R  = tp / (tp + fn)\n    F1 = (beta*beta + 1) * P * R / (beta*beta * P + R)\n\n    return F1\n\n\n\n\n""""""=============================================================================================================""""""\ndef eval_metrics_one_dataset(model, test_dataloader, device, k_vals, opt):\n    """"""\n    Compute evaluation metrics on test-dataset, e.g. NMI, F1 and Recall @ k.\n\n    Args:\n        model:              PyTorch network, network to compute evaluation metrics for.\n        test_dataloader:    PyTorch Dataloader, dataloader for test dataset, should have no shuffling and correct processing.\n        device:             torch.device, Device to run inference on.\n        k_vals:             list of int, Recall values to compute\n        opt:                argparse.Namespace, contains all training-specific parameters.\n    Returns:\n        F1 score (float), NMI score (float), recall_at_k (list of float), data embedding (np.ndarray)\n    """"""\n    torch.cuda.empty_cache()\n\n    _ = model.eval()\n    n_classes = len(test_dataloader.dataset.avail_classes)\n\n    with torch.no_grad():\n        ### For all test images, extract features\n        target_labels, feature_coll = [],[]\n        final_iter = tqdm(test_dataloader, desc=\'Computing Evaluation Metrics...\')\n        image_paths= [x[0] for x in test_dataloader.dataset.image_list]\n        for idx,inp in enumerate(final_iter):\n            input_img,target = inp[-1], inp[0]\n            target_labels.extend(target.numpy().tolist())\n            out = model(input_img.to(device))\n            feature_coll.extend(out.cpu().detach().numpy().tolist())\n\n        target_labels = np.hstack(target_labels).reshape(-1,1)\n        feature_coll  = np.vstack(feature_coll).astype(\'float32\')\n\n        torch.cuda.empty_cache()\n\n        ### Set Faiss CPU Cluster index\n        # cpu_cluster_index = faiss.IndexFlatL2(feature_coll.shape[-1])\n        # kmeans            = faiss.Clustering(feature_coll.shape[-1], n_classes)\n        # kmeans.niter = 20\n        # kmeans.min_points_per_centroid = 1\n        # kmeans.max_points_per_centroid = 1000000000\n\n        ### Train Kmeans\n        # kmeans.train(feature_coll, cpu_cluster_index)\n        # computed_centroids = faiss.vector_float_to_array(kmeans.centroids).reshape(n_classes, feature_coll.shape[-1])\n\n        ### Assign feature points to clusters\n        # faiss_search_index = faiss.IndexFlatL2(computed_centroids.shape[-1])\n        # faiss_search_index.add(computed_centroids)\n        # _, model_generated_cluster_labels = faiss_search_index.search(feature_coll, 1)\n\n        kmeans = KMeans(n_clusters=n_classes, random_state=0).fit(feature_coll)\n        model_generated_cluster_labels = kmeans.labels_\n        computed_centroids = kmeans.cluster_centers_\n\n        ### Compute NMI\n        NMI = metrics.cluster.normalized_mutual_info_score(model_generated_cluster_labels.reshape(-1), target_labels.reshape(-1))\n\n\n        ### Recover max(k_vals) nearest neighbours to use for recall computation\n        # faiss_search_index  = faiss.IndexFlatL2(feature_coll.shape[-1])\n        # faiss_search_index.add(feature_coll)\n        # _, k_closest_points = faiss_search_index.search(feature_coll, int(np.max(k_vals)+1))\n\n        k_closest_points  = squareform(pdist(feature_coll)).argsort(1)[:, :int(np.max(k_vals)+1)]\n        k_closest_classes = target_labels.reshape(-1)[k_closest_points[:, 1:]]\n\n        ### Compute Recall\n        recall_all_k = []\n        for k in k_vals:\n            recall_at_k = np.sum([1 for target, recalled_predictions in zip(target_labels, k_closest_classes) if target in recalled_predictions[:k]])/len(target_labels)\n            recall_all_k.append(recall_at_k)\n\n        ### Compute F1 Score\n        F1 = f1_score(model_generated_cluster_labels, target_labels, feature_coll, computed_centroids)\n\n    return F1, NMI, recall_all_k, feature_coll\n\n\n\ndef eval_metrics_query_and_gallery_dataset(model, query_dataloader, gallery_dataloader, device, k_vals, opt):\n    """"""\n    Compute evaluation metrics on test-dataset, e.g. NMI, F1 and Recall @ k.\n\n    Args:\n        model:               PyTorch network, network to compute evaluation metrics for.\n        query_dataloader:    PyTorch Dataloader, dataloader for query dataset, for which nearest neighbours in the gallery dataset are retrieved.\n        gallery_dataloader:  PyTorch Dataloader, dataloader for gallery dataset, provides target samples which are to be retrieved in correspondance to the query dataset.\n        device:              torch.device, Device to run inference on.\n        k_vals:              list of int, Recall values to compute\n        opt:                 argparse.Namespace, contains all training-specific parameters.\n    Returns:\n        F1 score (float), NMI score (float), recall_at_ks (list of float), query data embedding (np.ndarray), gallery data embedding (np.ndarray)\n    """"""\n    torch.cuda.empty_cache()\n\n    _ = model.eval()\n    n_classes = len(query_dataloader.dataset.avail_classes)\n\n    with torch.no_grad():\n        ### For all query test images, extract features\n        query_target_labels, query_feature_coll     = [],[]\n        query_image_paths   = [x[0] for x in query_dataloader.dataset.image_list]\n        query_iter = tqdm(query_dataloader, desc=\'Extraction Query Features\')\n        for idx,inp in enumerate(query_iter):\n            input_img,target = inp[-1], inp[0]\n            query_target_labels.extend(target.numpy().tolist())\n            out = model(input_img.to(device))\n            query_feature_coll.extend(out.cpu().detach().numpy().tolist())\n\n        ### For all gallery test images, extract features\n        gallery_target_labels, gallery_feature_coll = [],[]\n        gallery_image_paths = [x[0] for x in gallery_dataloader.dataset.image_list]\n        gallery_iter = tqdm(gallery_dataloader, desc=\'Extraction Gallery Features\')\n        for idx,inp in enumerate(gallery_iter):\n            input_img,target = inp[-1], inp[0]\n            gallery_target_labels.extend(target.numpy().tolist())\n            out = model(input_img.to(device))\n            gallery_feature_coll.extend(out.cpu().detach().numpy().tolist())\n\n\n        query_target_labels, query_feature_coll     = np.hstack(query_target_labels).reshape(-1,1), np.vstack(query_feature_coll).astype(\'float32\')\n        gallery_target_labels, gallery_feature_coll = np.hstack(gallery_target_labels).reshape(-1,1), np.vstack(gallery_feature_coll).astype(\'float32\')\n\n        torch.cuda.empty_cache()\n\n        ### Set CPU Cluster index\n        stackset    = np.concatenate([query_feature_coll, gallery_feature_coll],axis=0)\n        stacklabels = np.concatenate([query_target_labels, gallery_target_labels],axis=0)\n        cpu_cluster_index = faiss.IndexFlatL2(stackset.shape[-1])\n        kmeans            = faiss.Clustering(stackset.shape[-1], n_classes)\n        kmeans.niter = 20\n        kmeans.min_points_per_centroid = 1\n        kmeans.max_points_per_centroid = 1000000000\n\n        ### Train Kmeans\n        kmeans.train(stackset, cpu_cluster_index)\n        computed_centroids = faiss.vector_float_to_array(kmeans.centroids).reshape(n_classes, stackset.shape[-1])\n\n        ### Assign feature points to clusters\n        faiss_search_index = faiss.IndexFlatL2(computed_centroids.shape[-1])\n        faiss_search_index.add(computed_centroids)\n        _, model_generated_cluster_labels = faiss_search_index.search(stackset, 1)\n\n        ### Compute NMI\n        NMI = metrics.cluster.normalized_mutual_info_score(model_generated_cluster_labels.reshape(-1), stacklabels.reshape(-1))\n\n        ### Recover max(k_vals) nearest neighbours to use for recall computation\n        faiss_search_index  = faiss.IndexFlatL2(gallery_feature_coll.shape[-1])\n        faiss_search_index.add(gallery_feature_coll)\n        _, k_closest_points = faiss_search_index.search(query_feature_coll, int(np.max(k_vals)))\n        k_closest_classes   = gallery_target_labels.reshape(-1)[k_closest_points]\n\n        ### Compute Recall\n        recall_all_k = []\n        for k in k_vals:\n            recall_at_k = np.sum([1 for target, recalled_predictions in zip(query_target_labels, k_closest_classes) if target in recalled_predictions[:k]])/len(query_target_labels)\n            recall_all_k.append(recall_at_k)\n        recall_str = \', \'.join(\'@{0}: {1:.4f}\'.format(k,rec) for k,rec in zip(k_vals, recall_all_k))\n\n        ### Compute F1 score\n        F1 = f1_score(model_generated_cluster_labels, stacklabels, stackset, computed_centroids)\n\n    return F1, NMI, recall_all_k, query_feature_coll, gallery_feature_coll\n\n\n\n""""""=============================================================================================================""""""\n####### RECOVER CLOSEST EXAMPLE IMAGES #######\ndef recover_closest_one_dataset(feature_matrix_all, image_paths, save_path, n_image_samples=10, n_closest=3):\n    """"""\n    Provide sample recoveries.\n\n    Args:\n        feature_matrix_all: np.ndarray [n_samples x embed_dim], full data embedding of test samples.\n        image_paths:        list [n_samples], list of datapaths corresponding to <feature_matrix_all>\n        save_path:          str, where to store sample image.\n        n_image_samples:    Number of sample recoveries.\n        n_closest:          Number of closest recoveries to show.\n    Returns:\n        Nothing!\n    """"""\n    image_paths = np.array([x[0] for x in image_paths])\n    sample_idxs = np.random.choice(np.arange(len(feature_matrix_all)), n_image_samples)\n\n    # faiss_search_index = faiss.IndexFlatL2(feature_matrix_all.shape[-1])\n    # faiss_search_index.add(feature_matrix_all)\n    # _, closest_feature_idxs = faiss_search_index.search(feature_matrix_all, n_closest+1)\n\n    closest_feature_idxs  = squareform(pdist(feature_matrix_all)).argsort(1)[:,:n_closest+1]\n\n    sample_paths = image_paths[closest_feature_idxs][sample_idxs]\n\n    f,axes = plt.subplots(n_image_samples, n_closest+1)\n    for i,(ax,plot_path) in enumerate(zip(axes.reshape(-1), sample_paths.reshape(-1))):\n        ax.imshow(np.array(Image.open(plot_path)))\n        ax.set_xticks([])\n        ax.set_yticks([])\n        if i%(n_closest+1):\n            ax.axvline(x=0, color=\'g\', linewidth=13)\n        else:\n            ax.axvline(x=0, color=\'r\', linewidth=13)\n    f.set_size_inches(10,20)\n    f.tight_layout()\n    f.savefig(save_path)\n    plt.close()\n\n\n####### RECOVER CLOSEST EXAMPLE IMAGES #######\ndef recover_closest_inshop(query_feature_matrix_all, gallery_feature_matrix_all, query_image_paths, gallery_image_paths, save_path, n_image_samples=10, n_closest=3):\n    """"""\n    Provide sample recoveries.\n\n    Args:\n        query_feature_matrix_all:   np.ndarray [n_query_samples x embed_dim], full data embedding of query samples.\n        gallery_feature_matrix_all: np.ndarray [n_gallery_samples x embed_dim], full data embedding of gallery samples.\n        query_image_paths:          list [n_samples], list of datapaths corresponding to <query_feature_matrix_all>\n        gallery_image_paths:        list [n_samples], list of datapaths corresponding to <gallery_feature_matrix_all>\n        save_path:          str, where to store sample image.\n        n_image_samples:    Number of sample recoveries.\n        n_closest:          Number of closest recoveries to show.\n    Returns:\n        Nothing!\n    """"""\n    query_image_paths, gallery_image_paths   = np.array(query_image_paths), np.array(gallery_image_paths)\n    sample_idxs = np.random.choice(np.arange(len(query_feature_matrix_all)), n_image_samples)\n\n    faiss_search_index = faiss.IndexFlatL2(gallery_feature_matrix_all.shape[-1])\n    faiss_search_index.add(gallery_feature_matrix_all)\n    _, closest_feature_idxs = faiss_search_index.search(query_feature_matrix_all, n_closest)\n\n    image_paths  = gallery_image_paths[closest_feature_idxs]\n    image_paths  = np.concatenate([query_image_paths.reshape(-1,1), image_paths],axis=-1)\n\n    sample_paths = image_paths[closest_feature_idxs][sample_idxs]\n\n    f,axes = plt.subplots(n_image_samples, n_closest+1)\n    for i,(ax,plot_path) in enumerate(zip(axes.reshape(-1), sample_paths.reshape(-1))):\n        ax.imshow(np.array(Image.open(plot_path)))\n        ax.set_xticks([])\n        ax.set_yticks([])\n        if i%(n_closest+1):\n            ax.axvline(x=0, color=\'g\', linewidth=13)\n        else:\n            ax.axvline(x=0, color=\'r\', linewidth=13)\n    f.set_size_inches(10,20)\n    f.tight_layout()\n    f.savefig(save_path)\n    plt.close()\n\n\n\n""""""=============================================================================================================""""""\n################## SET NETWORK TRAINING CHECKPOINT #####################\ndef set_checkpoint(model, opt, progress_saver, savepath):\n    """"""\n    Store relevant parameters (model and progress saver, as well as parameter-namespace).\n    Can be easily extend for other stuff.\n\n    Args:\n        model:          PyTorch network, network whose parameters are to be saved.\n        opt:            argparse.Namespace, includes all training-specific parameters\n        progress_saver: subclass of LOGGER-class, contains a running memory of all training metrics.\n        savepath:       str, where to save checkpoint.\n    Returns:\n        Nothing!\n    """"""\n    torch.save({\'state_dict\':model.state_dict(), \'opt\':opt,\n                \'progress\':progress_saver}, savepath)\n\n\n\n\n""""""=============================================================================================================""""""\n################## WRITE TO CSV FILE #####################\nclass CSV_Writer():\n    """"""\n    Class to append newly compute training metrics to a csv file\n    for data logging.\n    Is used together with the LOGGER class.\n    """"""\n    def __init__(self, save_path, columns):\n        """"""\n        Args:\n            save_path: str, where to store the csv file\n            columns:   list of str, name of csv columns under which the resp. metrics are stored.\n        Returns:\n            Nothing!\n        """"""\n        self.save_path = save_path\n        self.columns   = columns\n\n        with open(self.save_path, ""a"") as csv_file:\n            writer = csv.writer(csv_file, delimiter="","")\n            writer.writerow(self.columns)\n\n    def log(self, inputs):\n        """"""\n        log one set of entries to the csv.\n\n        Args:\n            inputs: [list of int/str/float], values to append to the csv. Has to be of the same length as self.columns.\n        Returns:\n            Nothing!\n        """"""\n        with open(self.save_path, ""a"") as csv_file:\n            writer = csv.writer(csv_file, delimiter=\',\')\n            writer.writerow(inputs)\n\n\n\n################## PLOT SUMMARY IMAGE #####################\nclass InfoPlotter():\n    """"""\n    Plotter class to visualize training progression by showing\n    different metrics.\n    """"""\n    def __init__(self, save_path, title=\'Training Log\', figsize=(20,15)):\n        """"""\n        Args:\n            save_path: str, where to store the create plot.\n            title:     placeholder title of plot\n            figsize:   base size of saved figure\n        Returns:\n            Nothing!\n        """"""\n        self.save_path = save_path\n        self.title     = title\n        self.figsize   = figsize\n        #Colors for validation lines\n        self.v_colors    = [\'r\',\'g\',\'b\',\'y\',\'m\',\'k\',\'c\']\n        #Colors for training lines\n        self.t_colors    = [\'k\',\'b\',\'r\',\'g\']\n\n    def make_plot(self, t_epochs, v_epochs, t_metrics, v_metrics, t_labels, v_labels, appendix=None):\n        """"""\n        Given a list of iterated epochs, visualize the progression of various training/testing metrics.\n\n        Args:\n            t_epochs:  [list of int/float], list of epochs for which training metrics were collected (e.g. Training Loss)\n            v_epochs:  [list of int/float], list of epochs for which validation metrics were collected (e.g. Recall @ k)\n            t_metrics: [list of float], list of training metrics per epoch\n            v_metrics: [list of list of int/float], contains all computed validation metrics\n            t_labels, v_labels: [list of str], names for each metric that is plotted.\n        Returns:\n            Nothing!\n        """"""\n        plt.style.use(\'ggplot\')\n\n        f,axes = plt.subplots(1,2)\n\n        #Visualize Training Loss\n        for i in range(len(t_metrics)):\n            axes[0].plot(t_epochs, t_metrics[i], \'-{}\'.format(self.t_colors[i]), linewidth=1, label=t_labels[i])\n        axes[0].set_title(\'Training Performance\', fontsize=19)\n\n        axes[0].legend(fontsize=16)\n\n        axes[0].tick_params(axis=\'both\', which=\'major\', labelsize=16)\n        axes[0].tick_params(axis=\'both\', which=\'minor\', labelsize=16)\n\n        #Visualize Validation metrics\n        for i in range(len(v_metrics)):\n            axes[1].plot(v_epochs, v_metrics[i], \'-{}\'.format(self.v_colors[i]), linewidth=1, label=v_labels[i])\n        axes[1].set_title(self.title, fontsize=19)\n\n        axes[1].legend(fontsize=16)\n\n        axes[1].tick_params(axis=\'both\', which=\'major\', labelsize=16)\n        axes[1].tick_params(axis=\'both\', which=\'minor\', labelsize=16)\n\n        f.set_size_inches(2*self.figsize[0], self.figsize[1])\n\n        savepath = self.save_path\n        f.savefig(self.save_path, bbox_inches=\'tight\')\n\n        plt.close()\n\n\n################## GENERATE LOGGING FOLDER/FILES #######################\ndef set_logging(opt):\n    """"""\n    Generate the folder in which everything is saved.\n    If opt.savename is given, folder will take on said name.\n    If not, a name based on the start time is provided.\n    If the folder already exists, it will by iterated until it can be created without\n    deleting existing data.\n    The current opt.save_path will be extended to account for the new save_folder name.\n\n    Args:\n        opt: argparse.Namespace, contains all training-specific parameters.\n    Returns:\n        Nothing!\n    """"""\n    checkfolder = opt.save_path+\'/\'+opt.savename\n\n    #Create start-time-based name if opt.savename is not give.\n    if opt.savename == \'\':\n        date = datetime.datetime.now()\n        time_string = \'{}-{}-{}-{}-{}-{}\'.format(date.year, date.month, date.day, date.hour, date.minute, date.second)\n        checkfolder = opt.save_path+\'/{}_{}_\'.format(opt.dataset.upper(), opt.arch.upper())+time_string\n\n    #If folder already exists, iterate over it until is doesn\'t.\n    counter     = 1\n    while os.path.exists(checkfolder):\n        checkfolder = opt.save_path+\'/\'+opt.savename+\'_\'+str(counter)\n        counter += 1\n\n    #Create Folder\n    os.makedirs(checkfolder)\n    opt.save_path = checkfolder\n\n    #Store training parameters as text and pickle in said folder.\n    with open(opt.save_path+\'/Parameter_Info.txt\',\'w\') as f:\n        f.write(gimme_save_string(opt))\n    pkl.dump(opt,open(opt.save_path+""/hypa.pkl"",""wb""))\n\n\nclass LOGGER():\n    """"""\n    This class provides a collection of logging properties that are useful for training.\n    These include setting the save folder, in which progression of training/testing metrics is visualized,\n    csv log-files are stored, sample recoveries are plotted and an internal data saver.\n    """"""\n    def __init__(self, opt, metrics_to_log, name=\'Basic\', start_new=True):\n        """"""\n        Args:\n            opt:               argparse.Namespace, contains all training-specific parameters.\n            metrics_to_log:    dict, dictionary which shows in what structure the data should be saved.\n                               is given as the output of aux.metrics_to_examine. Example:\n                               {\'train\': [\'Epochs\', \'Time\', \'Train Loss\', \'Time\'],\n                                \'val\': [\'Epochs\',\'Time\',\'NMI\',\'F1\', \'Recall @ 1\',\'Recall @ 2\',\'Recall @ 4\',\'Recall @ 8\']}\n            name:              Name of this logger. Will be used to distinguish logged files from other LOGGER instances.\n            start_new:         If set to true, a new save folder will be created initially.\n        Returns:\n            Nothing!\n        """"""\n        self.prop           = opt\n        self.metrics_to_log = metrics_to_log\n\n        ### Make Logging Directories\n        if start_new: set_logging(opt)\n\n        ### Set INFO-PLOTS\n        if self.prop.dataset != \'vehicle_id\':\n            self.info_plot = InfoPlotter(opt.save_path+\'/InfoPlot_{}.svg\'.format(name))\n        else:\n            self.info_plot = {\'Set {}\'.format(i): InfoPlotter(opt.save_path+\'/InfoPlot_{}_Set{}.svg\'.format(name,i+1)) for i in range(3)}\n\n        ### Set Progress Saver Dict\n        self.progress_saver = self.provide_progress_saver(metrics_to_log)\n\n        ### Set CSV Writters\n        self.csv_loggers= {mode:CSV_Writer(opt.save_path+\'/log_\'+mode+\'_\'+name+\'.csv\', lognames) for mode, lognames in metrics_to_log.items()}\n\n\n    def provide_progress_saver(self, metrics_to_log):\n        """"""\n        Provide Progress Saver dictionary.\n\n        Args:\n            metrics_to_log: see __init__(). Describes the structure of Progress_Saver.\n        """"""\n        Progress_Saver = {key:{sub_key:[] for sub_key in metrics_to_log[key]} for key in metrics_to_log.keys()}\n        return Progress_Saver\n\n    def log(self, main_keys, metric_keys, values):\n        """"""\n        Actually log new values in csv and Progress Saver dict internally.\n        Args:\n            main_keys:      Main key in which data will be stored. Normally is either \'train\' for training metrics or \'val\' for validation metrics.\n            metric_keys:    Needs to follow the list length of self.progress_saver[main_key(s)]. List of metric keys that are extended with new values.\n            values:         Needs to be a list of the same structure as metric_keys. Actual values that are appended.\n        """"""\n        if not isinstance(main_keys, list):   main_keys = [main_keys]\n        if not isinstance(metric_keys, list): metric_keys = [metric_keys]\n        if not isinstance(values, list):      values = [values]\n\n        #Log data to progress saver dict.\n        for main_key in main_keys:\n            for value, metric_key in zip(values, metric_keys):\n                self.progress_saver[main_key][metric_key].append(value)\n\n        #Append data to csv.\n        self.csv_loggers[main_key].log(values)\n\n    def update_info_plot(self):\n        """"""\n        Create a new updated version of training/metric progression plot.\n\n        Args:\n            None\n        Returns:\n            Nothing!\n        """"""\n        t_epochs         = self.progress_saver[\'val\'][\'Epochs\']\n        t_loss_list      = [self.progress_saver[\'train\'][\'Train Loss\']]\n        t_legend_handles = [\'Train Loss\']\n\n        v_epochs         = self.progress_saver[\'val\'][\'Epochs\']\n        #Because Vehicle-ID normally uses three different test sets, a distinction has to be made.\n        if self.prop.dataset != \'vehicle_id\':\n            title = \' | \'.join(key+\': {0:3.3f}\'.format(np.max(item)) for key,item in self.progress_saver[\'val\'].items() if key not in [\'Time\', \'Epochs\'])\n            self.info_plot.title = title\n            v_metric_list    = [self.progress_saver[\'val\'][key] for key in self.progress_saver[\'val\'].keys() if key not in [\'Time\', \'Epochs\']]\n            v_legend_handles = [key for key in self.progress_saver[\'val\'].keys() if key not in [\'Time\', \'Epochs\']]\n\n            self.info_plot.make_plot(t_epochs, v_epochs, t_loss_list, v_metric_list, t_legend_handles, v_legend_handles)\n        else:\n            #Iterate over all test sets.\n            for i in range(3):\n                title = \' | \'.join(key+\': {0:3.3f}\'.format(np.max(item)) for key,item in self.progress_saver[\'val\'].items() if key not in [\'Time\', \'Epochs\'] and \'Set {}\'.format(i) in key)\n                self.info_plot[\'Set {}\'.format(i)].title = title\n                v_metric_list    = [self.progress_saver[\'val\'][key] for key in self.progress_saver[\'val\'].keys() if key not in [\'Time\', \'Epochs\'] and \'Set {}\'.format(i) in key]\n                v_legend_handles = [key for key in self.progress_saver[\'val\'].keys() if key not in [\'Time\', \'Epochs\'] and \'Set {}\'.format(i) in key]\n                self.info_plot[\'Set {}\'.format(i)].make_plot(t_epochs, v_epochs, t_loss_list, v_metric_list, t_legend_handles, v_legend_handles, appendix=\'set_{}\'.format(i))\n\ndef metrics_to_examine(dataset, k_vals):\n    """"""\n    Please only use either of the following keys:\n    -> Epochs, Time, Train Loss for training\n    -> Epochs, Time, NMI, F1 & Recall @ k for validation\n\n    Args:\n        dataset: str, dataset for which a storing structure for LOGGER.progress_saver is to be made.\n        k_vals:  list of int, Recall @ k - values.\n    Returns:\n        metric_dict: Dictionary representing the storing structure for LOGGER.progress_saver. See LOGGER.__init__() for an example.\n    """"""\n    metric_dict        = {\'train\':[\'Epochs\',\'Time\',\'Train Loss\']}\n\n    if dataset==\'vehicle_id\':\n        metric_dict[\'val\'] = [\'Epochs\',\'Time\']\n        #Vehicle_ID uses three test sets\n        for i in range(3):\n            metric_dict[\'val\'] += [\'Set {} NMI\'.format(i), \'Set {} F1\'.format(i)]\n            for k in k_vals:\n                metric_dict[\'val\'] += [\'Set {} Recall @ {}\'.format(i,k)]\n    else:\n        metric_dict[\'val\'] = [\'Epochs\',\'Time\',\'NMI\', \'F1\']\n        metric_dict[\'val\'] += [\'Recall @ {}\'.format(k) for k in k_vals]\n\n    return metric_dict\n\n\n\n""""""=================================================================================================""""""\ndef run_kmeans(features, n_cluster):\n    """"""\n    Run kmeans on a set of features to find <n_cluster> cluster.\n\n    Args:\n        features:  np.ndarrary [n_samples x embed_dim], embedding training/testing samples for which kmeans should be performed.\n        n_cluster: int, number of cluster.\n    Returns:\n        cluster_assignments: np.ndarray [n_samples x 1], per sample provide the respective cluster label it belongs to.\n    """"""\n    n_samples, dim = features.shape\n    kmeans = faiss.Kmeans(dim, n_cluster)\n    kmeans.n_iter, kmeans.min_points_per_centroid, kmeans.max_points_per_centroid = 20,5,1000000000\n    kmeans.train(features)\n    _, cluster_assignments = kmeans.index.search(features,1)\n    return cluster_assignments\n\n\n\n""""""=============================================================================================================""""""\ndef save_graph(opt, model):\n    """"""\n    Generate Network Graph.\n    NOTE: Requires the installation of the graphviz library on you system.\n\n    Args:\n        opt:   argparse.Namespace, contains all training-specific parameters.\n        model: PyTorch Network, network for which the computational graph should be visualized.\n    Returns:\n        Nothing!\n    """"""\n    inp = torch.randn((1,3,224,224)).to(opt.device)\n    network_output = model(inp)\n    if isinstance(network_output, dict): network_output = network_output[\'Class\']\n\n    from graphviz import Digraph\n    def make_dot(var, savename, params=None):\n        """"""\n        Generate a symbolic representation of the network graph.\n        """"""\n        if params is not None:\n            assert all(isinstance(p, Variable) for p in params.values())\n            param_map = {id(v): k for k, v in params.items()}\n\n        node_attr = dict(style=\'filled\',\n                         shape=\'box\',\n                         align=\'left\',\n                         fontsize=\'6\',\n                         ranksep=\'0.1\',\n                         height=\'0.6\',\n                         width=\'1\')\n        dot  = Digraph(node_attr=node_attr, format=\'svg\', graph_attr=dict(size=""40,10"", rankdir=\'LR\', rank=\'same\'))\n        seen = set()\n\n        def size_to_str(size):\n            return \'(\'+(\', \').join([\'%d\' % v for v in size])+\')\'\n\n        def add_nodes(var):\n            replacements  = [\'Backward\', \'Th\', \'Cudnn\']\n            color_assigns = {\'Convolution\':\'orange\',\n                             \'ConvolutionTranspose\': \'lightblue\',\n                             \'Add\': \'red\',\n                             \'Cat\': \'green\',\n                             \'Softmax\': \'yellow\',\n                             \'Sigmoid\': \'yellow\',\n                             \'Copys\':   \'yellow\'}\n            if var not in seen:\n                op1 = torch.is_tensor(var)\n                op2 = not torch.is_tensor(var) and str(type(var).__name__)!=\'AccumulateGrad\'\n\n                text = str(type(var).__name__)\n                for rep in replacements:\n                    text = text.replace(rep, \'\')\n                color = color_assigns[text] if text in color_assigns.keys() else \'gray\'\n\n                if \'Pool\' in text: color = \'lightblue\'\n\n                if op1 or op2:\n                    if hasattr(var, \'next_functions\'):\n                        count = 0\n                        for i, u in enumerate(var.next_functions):\n                            if str(type(u[0]).__name__)==\'AccumulateGrad\':\n                                if count==0: attr_text = \'\\nParameter Sizes:\\n\'\n                                attr_text += size_to_str(u[0].variable.size())\n                                count += 1\n                                attr_text += \' \'\n                        if count>0: text += attr_text\n\n\n                if op1:\n                    dot.node(str(id(var)), size_to_str(var.size()), fillcolor=\'orange\')\n                if op2:\n                    dot.node(str(id(var)), text, fillcolor=color)\n\n                seen.add(var)\n\n                if op1 or op2:\n                    if hasattr(var, \'next_functions\'):\n                        for u in var.next_functions:\n                            if u[0] is not None:\n                                if str(type(u[0]).__name__)!=\'AccumulateGrad\':\n                                    dot.edge(str(id(u[0])), str(id(var)))\n                                    add_nodes(u[0])\n                    if hasattr(var, \'saved_tensors\'):\n                        for t in var.saved_tensors:\n                            dot.edge(str(id(t)), str(id(var)))\n                            add_nodes(t)\n\n        add_nodes(var.grad_fn)\n        dot.save(savename)\n        return dot\n\n    if not os.path.exists(opt.save_path):\n        raise Exception(\'No save folder {} available!\'.format(opt.save_path))\n\n    viz_graph = make_dot(network_output, opt.save_path+""/Network_Graphs""+""/{}_network_graph"".format(opt.arch))\n    viz_graph.format = \'svg\'\n    viz_graph.render()\n\n    torch.cuda.empty_cache()\n'"
datasets.py,4,"b'# Copyright 2019 Karsten Roth and Biagio Brattoli\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n################# LIBRARIES ###############################\nimport warnings\nwarnings.filterwarnings(""ignore"")\n\nimport numpy as np, os, sys, pandas as pd, csv, copy\nimport torch, torch.nn as nn, matplotlib.pyplot as plt, random\n\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom torchvision import transforms\nfrom tqdm import tqdm\n\nimport pretrainedmodels.utils as utils\nimport auxiliaries as aux\n\n\n""""""============================================================================""""""\n################ FUNCTION TO RETURN ALL DATALOADERS NECESSARY ####################\ndef give_dataloaders(dataset, opt):\n    """"""\n    Args:\n        dataset: string, name of dataset for which the dataloaders should be returned.\n        opt:     argparse.Namespace, contains all training-specific parameters.\n    Returns:\n        dataloaders: dict of dataloaders for training, testing and evaluation on training.\n    """"""\n    #Dataset selection\n    if opt.dataset==\'cub200\':\n        datasets = give_CUB200_datasets(opt)\n    elif opt.dataset==\'cars196\':\n        datasets = give_CARS196_datasets(opt)\n    elif opt.dataset==\'online_products\':\n        datasets = give_OnlineProducts_datasets(opt)\n    elif opt.dataset==\'in-shop\':\n        datasets = give_InShop_datasets(opt)\n    elif opt.dataset==\'vehicle_id\':\n        datasets = give_VehicleID_datasets(opt)\n    else:\n        raise Exception(\'No Dataset >{}< available!\'.format(dataset))\n\n    #Move datasets to dataloaders.\n    dataloaders = {}\n    for key,dataset in datasets.items():\n        is_val = dataset.is_validation\n        dataloaders[key] = torch.utils.data.DataLoader(dataset, batch_size=opt.bs, num_workers=opt.kernels, shuffle=not is_val, pin_memory=True, drop_last=not is_val)\n\n    return dataloaders\n\n\n""""""============================================================================""""""\n################# FUNCTIONS TO RETURN TRAIN/VAL PYTORCH DATASETS FOR CUB200, CARS196, STANFORD ONLINE PRODUCTS, IN-SHOP CLOTHES, PKU VEHICLE-ID ####################################\ndef give_CUB200_datasets(opt):\n    """"""\n    This function generates a training, testing and evaluation dataloader for Metric Learning on the CUB-200-2011 dataset.\n    For Metric Learning, the dataset classes are sorted by name, and the first half used for training while the last half is used for testing.\n    So no random shuffling of classes.\n\n    Args:\n        opt: argparse.Namespace, contains all traininig-specific parameters.\n    Returns:\n        dict of PyTorch datasets for training, testing and evaluation.\n    """"""\n    image_sourcepath  = opt.source_path+\'/images\'\n    #Find available data classes.\n    image_classes = sorted([x for x in os.listdir(image_sourcepath) if \'._\' not in x], key=lambda x: int(x.split(\'.\')[0]))\n    #Make a index-to-labelname conversion dict.\n    conversion    = {int(x.split(\'.\')[0]):x.split(\'.\')[-1] for x in image_classes}\n    #Generate a list of tuples (class_label, image_path)\n    image_list    = {int(key.split(\'.\')[0]):sorted([image_sourcepath+\'/\'+key+\'/\'+x for x in os.listdir(image_sourcepath+\'/\'+key) if \'._\' not in x]) for key in image_classes}\n    image_list    = [[(key,img_path) for img_path in image_list[key]] for key in image_list.keys()]\n    image_list    = [x for y in image_list for x in y]\n\n    #Image-dict of shape {class_idx:[list of paths to images belong to this class] ...}\n    image_dict    = {}\n    for key, img_path in image_list:\n        key = key-1\n        if not key in image_dict.keys():\n            image_dict[key] = []\n        image_dict[key].append(img_path)\n\n    keys = sorted(list(image_dict.keys()))\n\n    #Following ""Deep Metric Learning via Lifted Structured Feature Embedding"", we use the first half of classes for training.\n    train,test = keys[:len(keys)//2], keys[len(keys)//2:]\n    train_image_dict, val_image_dict = {key:image_dict[key] for key in train},{key:image_dict[key] for key in test}\n\n\n    train_dataset = BaseTripletDataset(train_image_dict, opt, samples_per_class=opt.samples_per_class)\n    val_dataset   = BaseTripletDataset(val_image_dict,   opt, is_validation=True)\n    eval_dataset  = BaseTripletDataset(train_image_dict, opt, is_validation=True)\n\n    train_dataset.conversion = conversion\n    val_dataset.conversion   = conversion\n    eval_dataset.conversion  = conversion\n\n    return {\'training\':train_dataset, \'testing\':val_dataset, \'evaluation\':eval_dataset}\n\n\ndef give_CARS196_datasets(opt):\n    """"""\n    This function generates a training, testing and evaluation dataloader for Metric Learning on the CARS196 dataset.\n    For Metric Learning, the dataset classes are sorted by name, and the first half used for training while the last half is used for testing.\n    So no random shuffling of classes.\n\n    Args:\n        opt: argparse.Namespace, contains all traininig-specific parameters.\n    Returns:\n        dict of PyTorch datasets for training, testing and evaluation.\n    """"""\n    image_sourcepath  = opt.source_path+\'/images\'\n    #Find available data classes.\n    image_classes = sorted([x for x in os.listdir(image_sourcepath)])\n    #Make a index-to-labelname conversion dict.\n    conversion    = {i:x for i,x in enumerate(image_classes)}\n    #Generate a list of tuples (class_label, image_path)\n    image_list    = {i:sorted([image_sourcepath+\'/\'+key+\'/\'+x for x in os.listdir(image_sourcepath+\'/\'+key)]) for i,key in enumerate(image_classes)}\n    image_list    = [[(key,img_path) for img_path in image_list[key]] for key in image_list.keys()]\n    image_list    = [x for y in image_list for x in y]\n\n    #Image-dict of shape {class_idx:[list of paths to images belong to this class] ...}\n    image_dict    = {}\n    for key, img_path in image_list:\n        key = key\n        # key = key-1\n        if not key in image_dict.keys():\n            image_dict[key] = []\n        image_dict[key].append(img_path)\n\n    keys = sorted(list(image_dict.keys()))\n\n    #Following ""Deep Metric Learning via Lifted Structured Feature Embedding"", we use the first half of classes for training.\n    train,test = keys[:len(keys)//2], keys[len(keys)//2:]\n    train_image_dict, val_image_dict = {key:image_dict[key] for key in train},{key:image_dict[key] for key in test}\n\n    train_dataset = BaseTripletDataset(train_image_dict, opt, samples_per_class=opt.samples_per_class)\n    val_dataset   = BaseTripletDataset(val_image_dict,   opt, is_validation=True)\n    eval_dataset  = BaseTripletDataset(train_image_dict, opt, is_validation=True)\n\n    train_dataset.conversion = conversion\n    val_dataset.conversion   = conversion\n    eval_dataset.conversion  = conversion\n\n    return {\'training\':train_dataset, \'testing\':val_dataset, \'evaluation\':eval_dataset}\n\n\ndef give_OnlineProducts_datasets(opt):\n    """"""\n    This function generates a training, testing and evaluation dataloader for Metric Learning on the Online-Products dataset.\n    For Metric Learning, training and test sets are provided by given text-files, Ebay_train.txt & Ebay_test.txt.\n    So no random shuffling of classes.\n\n    Args:\n        opt: argparse.Namespace, contains all traininig-specific parameters.\n    Returns:\n        dict of PyTorch datasets for training, testing and evaluation.\n    """"""\n    image_sourcepath  = opt.source_path+\'/images\'\n    #Load text-files containing classes and imagepaths.\n    training_files = pd.read_table(opt.source_path+\'/Info_Files/Ebay_train.txt\', header=0, delimiter=\' \')\n    test_files     = pd.read_table(opt.source_path+\'/Info_Files/Ebay_test.txt\', header=0, delimiter=\' \')\n\n    #Generate Conversion dict.\n    conversion = {}\n    for class_id, path in zip(training_files[\'class_id\'],training_files[\'path\']):\n        conversion[class_id] = path.split(\'/\')[0]\n    for class_id, path in zip(test_files[\'class_id\'],test_files[\'path\']):\n        conversion[class_id] = path.split(\'/\')[0]\n\n    #Generate image_dicts of shape {class_idx:[list of paths to images belong to this class] ...}\n    train_image_dict, val_image_dict  = {},{}\n    for key, img_path in zip(training_files[\'class_id\'],training_files[\'path\']):\n        key = key-1\n        if not key in train_image_dict.keys():\n            train_image_dict[key] = []\n        train_image_dict[key].append(image_sourcepath+\'/\'+img_path)\n\n    for key, img_path in zip(test_files[\'class_id\'],test_files[\'path\']):\n        key = key-1\n        if not key in val_image_dict.keys():\n            val_image_dict[key] = []\n        val_image_dict[key].append(image_sourcepath+\'/\'+img_path)\n\n    ### Uncomment this if super-labels should be used to generate resp.datasets\n    # super_conversion = {}\n    # for super_class_id, path in zip(training_files[\'super_class_id\'],training_files[\'path\']):\n    #     conversion[super_class_id] = path.split(\'/\')[0]\n    # for key, img_path in zip(training_files[\'super_class_id\'],training_files[\'path\']):\n    #     key = key-1\n    #     if not key in super_train_image_dict.keys():\n    #         super_train_image_dict[key] = []\n    #     super_train_image_dict[key].append(image_sourcepath+\'/\'+img_path)\n    # super_train_dataset = BaseTripletDataset(super_train_image_dict, opt, is_validation=True)\n    # super_train_dataset.conversion = super_conversion\n\n\n    train_dataset       = BaseTripletDataset(train_image_dict, opt, samples_per_class=opt.samples_per_class)\n    val_dataset         = BaseTripletDataset(val_image_dict,   opt, is_validation=True)\n    eval_dataset        = BaseTripletDataset(train_image_dict, opt, is_validation=True)\n\n    train_dataset.conversion       = conversion\n    val_dataset.conversion         = conversion\n    eval_dataset.conversion        = conversion\n\n    return {\'training\':train_dataset, \'testing\':val_dataset, \'evaluation\':eval_dataset}\n    # return {\'training\':train_dataset, \'testing\':val_dataset, \'evaluation\':eval_dataset, \'super_evaluation\':super_train_dataset}\n\n\ndef give_InShop_datasets(opt):\n    """"""\n    This function generates a training, testing and evaluation dataloader for Metric Learning on the In-Shop Clothes dataset.\n    For Metric Learning, training and test sets are provided by one text file, list_eval_partition.txt.\n    So no random shuffling of classes.\n\n    Args:\n        opt: argparse.Namespace, contains all traininig-specific parameters.\n    Returns:\n        dict of PyTorch datasets for training, testing (by query and gallery separation) and evaluation.\n    """"""\n    #Load train-test-partition text file.\n    data_info = np.array(pd.read_table(opt.source_path+\'/Eval/list_eval_partition.txt\', header=1, delim_whitespace=True))[1:,:]\n    #Separate into training dataset and query/gallery dataset for testing.\n    train, query, gallery   = data_info[data_info[:,2]==\'train\'][:,:2], data_info[data_info[:,2]==\'query\'][:,:2], data_info[data_info[:,2]==\'gallery\'][:,:2]\n\n    #Generate conversions\n    lab_conv = {x:i for i,x in enumerate(np.unique(np.array([int(x.split(\'_\')[-1]) for x in train[:,1]])))}\n    train[:,1] = np.array([lab_conv[int(x.split(\'_\')[-1])] for x in train[:,1]])\n\n    lab_conv = {x:i for i,x in enumerate(np.unique(np.array([int(x.split(\'_\')[-1]) for x in np.concatenate([query[:,1], gallery[:,1]])])))}\n    query[:,1]   = np.array([lab_conv[int(x.split(\'_\')[-1])] for x in query[:,1]])\n    gallery[:,1] = np.array([lab_conv[int(x.split(\'_\')[-1])] for x in gallery[:,1]])\n\n    #Generate Image-Dicts for training, query and gallery of shape {class_idx:[list of paths to images belong to this class] ...}\n    train_image_dict    = {}\n    for img_path, key in train:\n        if not key in train_image_dict.keys():\n            train_image_dict[key] = []\n        train_image_dict[key].append(opt.source_path+\'/\'+img_path)\n\n    query_image_dict    = {}\n    for img_path, key in query:\n        if not key in query_image_dict.keys():\n            query_image_dict[key] = []\n        query_image_dict[key].append(opt.source_path+\'/\'+img_path)\n\n    gallery_image_dict    = {}\n    for img_path, key in gallery:\n        if not key in gallery_image_dict.keys():\n            gallery_image_dict[key] = []\n        gallery_image_dict[key].append(opt.source_path+\'/\'+img_path)\n\n    ### Uncomment this if super-labels should be used to generate resp.datasets\n    # super_train_image_dict, counter, super_assign = {},0,{}\n    # for img_path, _ in train:\n    #     key = \'_\'.join(img_path.split(\'/\')[1:3])\n    #     if key not in super_assign.keys():\n    #         super_assign[key] = counter\n    #         counter += 1\n    #     key = super_assign[key]\n    #\n    #     if not key in super_train_image_dict.keys():\n    #         super_train_image_dict[key] = []\n    #     super_train_image_dict[key].append(opt.source_path+\'/\'+img_path)\n    # super_train_dataset = BaseTripletDataset(super_train_image_dict, opt, is_validation=True)\n\n    train_dataset     = BaseTripletDataset(train_image_dict, opt,   samples_per_class=opt.samples_per_class)\n    eval_dataset      = BaseTripletDataset(train_image_dict, opt,   is_validation=True)\n    query_dataset     = BaseTripletDataset(query_image_dict, opt,   is_validation=True)\n    gallery_dataset   = BaseTripletDataset(gallery_image_dict, opt, is_validation=True)\n\n    return {\'training\':train_dataset, \'testing_query\':query_dataset, \'evaluation\':eval_dataset, \'testing_gallery\':gallery_dataset}\n    # return {\'training\':train_dataset, \'testing_query\':query_dataset, \'evaluation\':eval_dataset, \'testing_gallery\':gallery_dataset, \'super_evaluation\':super_train_dataset}\n\n\ndef give_VehicleID_datasets(opt):\n    """"""\n    This function generates a training, testing and evaluation dataloader for Metric Learning on the PKU Vehicle dataset.\n    For Metric Learning, training and (multiple) test sets are provided by separate text files, train_list and test_list_<n_classes_2_test>.txt.\n    So no random shuffling of classes.\n\n    Args:\n        opt: argparse.Namespace, contains all traininig-specific parameters.\n    Returns:\n        dict of PyTorch datasets for training, testing and evaluation.\n    """"""\n    #Load respective text-files\n    train       = np.array(pd.read_table(opt.source_path+\'/train_test_split/train_list.txt\', header=None, delim_whitespace=True))\n    small_test  = np.array(pd.read_table(opt.source_path+\'/train_test_split/test_list_800.txt\', header=None, delim_whitespace=True))\n    medium_test = np.array(pd.read_table(opt.source_path+\'/train_test_split/test_list_1600.txt\', header=None, delim_whitespace=True))\n    big_test    = np.array(pd.read_table(opt.source_path+\'/train_test_split/test_list_2400.txt\', header=None, delim_whitespace=True))\n\n    #Generate conversions\n    lab_conv = {x:i for i,x in enumerate(np.unique(train[:,1]))}\n    train[:,1] = np.array([lab_conv[x] for x in train[:,1]])\n    lab_conv = {x:i for i,x in enumerate(np.unique(np.concatenate([small_test[:,1], medium_test[:,1], big_test[:,1]])))}\n    small_test[:,1]  = np.array([lab_conv[x] for x in small_test[:,1]])\n    medium_test[:,1] = np.array([lab_conv[x] for x in medium_test[:,1]])\n    big_test[:,1]    = np.array([lab_conv[x] for x in big_test[:,1]])\n\n    #Generate Image-Dicts for training and different testings of shape {class_idx:[list of paths to images belong to this class] ...}\n    train_image_dict    = {}\n    for img_path, key in train:\n        if not key in train_image_dict.keys():\n            train_image_dict[key] = []\n        train_image_dict[key].append(opt.source_path+\'/image/{:07d}.jpg\'.format(img_path))\n\n    small_test_dict = {}\n    for img_path, key in small_test:\n        if not key in small_test_dict.keys():\n            small_test_dict[key] = []\n        small_test_dict[key].append(opt.source_path+\'/image/{:07d}.jpg\'.format(img_path))\n\n    medium_test_dict    = {}\n    for img_path, key in medium_test:\n        if not key in medium_test_dict.keys():\n            medium_test_dict[key] = []\n        medium_test_dict[key].append(opt.source_path+\'/image/{:07d}.jpg\'.format(img_path))\n\n    big_test_dict    = {}\n    for img_path, key in big_test:\n        if not key in big_test_dict.keys():\n            big_test_dict[key] = []\n        big_test_dict[key].append(opt.source_path+\'/image/{:07d}.jpg\'.format(img_path))\n\n    train_dataset = BaseTripletDataset(train_image_dict, opt, samples_per_class=opt.samples_per_class)\n    eval_dataset  = BaseTripletDataset(train_image_dict, opt,    is_validation=True)\n    val_small_dataset     = BaseTripletDataset(small_test_dict, opt,  is_validation=True)\n    val_medium_dataset    = BaseTripletDataset(medium_test_dict, opt, is_validation=True)\n    val_big_dataset       = BaseTripletDataset(big_test_dict, opt,    is_validation=True)\n\n    return {\'training\':train_dataset, \'testing_set1\':val_small_dataset, \'testing_set2\':val_medium_dataset, \\\n            \'testing_set3\':val_big_dataset, \'evaluation\':eval_dataset}\n\n\n\n\n\n################## BASIC PYTORCH DATASET USED FOR ALL DATASETS ##################################\nclass BaseTripletDataset(Dataset):\n    """"""\n    Dataset class to provide (augmented) correctly prepared training samples corresponding to standard DML literature.\n    This includes normalizing to ImageNet-standards, and Random & Resized cropping of shapes 224 for ResNet50 and 227 for\n    GoogLeNet during Training. During validation, only resizing to 256 or center cropping to 224/227 is performed.\n    """"""\n    def __init__(self, image_dict, opt, samples_per_class=8, is_validation=False):\n        """"""\n        Dataset Init-Function.\n\n        Args:\n            image_dict:         dict, Dictionary of shape {class_idx:[list of paths to images belong to this class] ...} providing all the training paths and classes.\n            opt:                argparse.Namespace, contains all training-specific parameters.\n            samples_per_class:  Number of samples to draw from one class before moving to the next when filling the batch.\n            is_validation:      If is true, dataset properties for validation/testing are used instead of ones for training.\n        Returns:\n            Nothing!\n        """"""\n        #Define length of dataset\n        self.n_files     = np.sum([len(image_dict[key]) for key in image_dict.keys()])\n\n        self.is_validation = is_validation\n\n        self.pars        = opt\n        self.image_dict  = image_dict\n\n        self.avail_classes    = sorted(list(self.image_dict.keys()))\n\n        #Convert image dictionary from classname:content to class_idx:content, because the initial indices are not necessarily from 0 - <n_classes>.\n        self.image_dict    = {i:self.image_dict[key] for i,key in enumerate(self.avail_classes)}\n        self.avail_classes = sorted(list(self.image_dict.keys()))\n\n        #Init. properties that are used when filling up batches.\n        if not self.is_validation:\n            self.samples_per_class = samples_per_class\n            #Select current class to sample images from up to <samples_per_class>\n            self.current_class   = np.random.randint(len(self.avail_classes))\n            self.classes_visited = [self.current_class, self.current_class]\n            self.n_samples_drawn = 0\n\n        #Data augmentation/processing methods.\n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n        transf_list = []\n        if not self.is_validation:\n            transf_list.extend([transforms.RandomResizedCrop(size=224) if opt.arch==\'resnet50\' else transforms.RandomResizedCrop(size=227),\n                                transforms.RandomHorizontalFlip(0.5)])\n        else:\n            transf_list.extend([transforms.Resize(256),\n                                transforms.CenterCrop(224) if opt.arch==\'resnet50\' else transforms.CenterCrop(227)])\n\n        transf_list.extend([transforms.ToTensor(), normalize])\n        self.transform = transforms.Compose(transf_list)\n\n        #Convert Image-Dict to list of (image_path, image_class). Allows for easier direct sampling.\n        self.image_list = [[(x,key) for x in self.image_dict[key]] for key in self.image_dict.keys()]\n        self.image_list = [x for y in self.image_list for x in y]\n\n        #Flag that denotes if dataset is called for the first time.\n        self.is_init = True\n\n\n    def ensure_3dim(self, img):\n        """"""\n        Function that ensures that the input img is three-dimensional.\n\n        Args:\n            img: PIL.Image, image which is to be checked for three-dimensionality (i.e. if some images are black-and-white in an otherwise coloured dataset).\n        Returns:\n            Checked PIL.Image img.\n        """"""\n        if len(img.size)==2:\n            img = img.convert(\'RGB\')\n        return img\n\n\n    def __getitem__(self, idx):\n        """"""\n        Args:\n            idx: Sample idx for training sample\n        Returns:\n            tuple of form (sample_class, torch.Tensor() of input image)\n        """"""\n        if self.is_init:\n            self.current_class = self.avail_classes[idx%len(self.avail_classes)]\n            self.is_init = False\n\n        if not self.is_validation:\n            if self.samples_per_class==1:\n                return self.image_list[idx][-1], self.transform(self.ensure_3dim(Image.open(self.image_list[idx][0])))\n\n            if self.n_samples_drawn==self.samples_per_class:\n                #Once enough samples per class have been drawn, we choose another class to draw samples from.\n                #Note that we ensure with self.classes_visited that no class is chosen if it had been chosen\n                #previously or one before that.\n                counter = copy.deepcopy(self.avail_classes)\n                for prev_class in self.classes_visited:\n                    if prev_class in counter: counter.remove(prev_class)\n\n                self.current_class   = counter[idx%len(counter)]\n                self.classes_visited = self.classes_visited[1:]+[self.current_class]\n                self.n_samples_drawn = 0\n\n            class_sample_idx = idx%len(self.image_dict[self.current_class])\n            self.n_samples_drawn += 1\n\n            out_img = self.transform(self.ensure_3dim(Image.open(self.image_dict[self.current_class][class_sample_idx])))\n            return self.current_class,out_img\n        else:\n            return self.image_list[idx][-1], self.transform(self.ensure_3dim(Image.open(self.image_list[idx][0])))\n\n    def __len__(self):\n        return self.n_files\n'"
evaluate.py,8,"b'# Copyright 2019 Karsten Roth and Biagio Brattoli\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\n##################################### LIBRARIES ###########################################\nimport warnings\nwarnings.filterwarnings(""ignore"")\n\nimport os, sys, numpy as np, argparse, imp, datetime, time, pickle as pkl, random, json, csv\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom scipy.spatial import distance\nfrom sklearn.preprocessing import normalize\n\nfrom tqdm import tqdm\nimport pandas as pd\n\nimport torch, torch.nn as nn\nimport auxiliaries as aux\nimport datasets as data\n\nimport netlib\nimport losses as losses\n\nimport torch.multiprocessing\ntorch.multiprocessing.set_sharing_strategy(\'file_system\')\n\n\n""""""==================================================================================================================""""""\n""""""==================================================================================================================""""""\n""""""=========================================================""""""\ndef evaluate(dataset, LOG, **kwargs):\n    """"""\n    Given a dataset name, applies the correct evaluation function.\n\n    Args:\n        dataset: str, name of dataset.\n        LOG:     aux.LOGGER instance, main logging class.\n        **kwargs: Input Argument Dict, depends on dataset.\n    Returns:\n        (optional) Computed metrics. Are normally written directly to LOG and printed.\n    """"""\n    if dataset in [\'cars196\', \'cub200\', \'online_products\']:\n        ret = evaluate_one_dataset(LOG, **kwargs)\n    elif dataset in [\'in-shop\']:\n        ret = evaluate_query_and_gallery_dataset(LOG, **kwargs)\n    elif dataset in [\'vehicle_id\']:\n        ret = evaluate_multiple_datasets(LOG, **kwargs)\n    else:\n        raise Exception(\'No implementation for dataset {} available!\')\n\n    return ret\n\n\n""""""=========================================================""""""\nclass DistanceMeasure():\n    """"""\n    Container class to run and log the change of distance ratios\n    between intra-class distances and inter-class distances.\n    """"""\n    def __init__(self, checkdata, opt, name=\'Train\', update_epochs=1):\n        """"""\n        Args:\n            checkdata: PyTorch DataLoader, data to check distance progression.\n            opt:       argparse.Namespace, contains all training-specific parameters.\n            name:      str, Name of instance. Important for savenames.\n            update_epochs:  int, Only compute distance ratios every said epoch.\n        Returns:\n            Nothing!\n        """"""\n        self.update_epochs = update_epochs\n        self.pars          = opt\n        self.save_path = opt.save_path\n\n        self.name          = name\n        self.csv_file      = opt.save_path+\'/distance_measures_{}.csv\'.format(self.name)\n        with open(self.csv_file,\'a\') as csv_file:\n            writer = csv.writer(csv_file, delimiter=\',\')\n            writer.writerow([\'Rel. Intra/Inter Distance\'])\n\n        self.checkdata     = checkdata\n\n        self.mean_class_dists = []\n        self.epochs           = []\n\n\n    def measure(self, model, epoch):\n        """"""\n        Compute distance ratios of intra- and interclass distance.\n\n        Args:\n            model: PyTorch Network, network that produces the resp. embeddings.\n            epoch: Current epoch.\n        Returns:\n            Nothing!\n        """"""\n        if epoch%self.update_epochs: return\n\n        self.epochs.append(epoch)\n\n        torch.cuda.empty_cache()\n\n        _ = model.eval()\n\n        #Compute Embeddings\n        with torch.no_grad():\n            feature_coll, target_coll = [],[]\n            data_iter = tqdm(self.checkdata, desc=\'Estimating Data Distances...\')\n            for idx, data in enumerate(data_iter):\n                input_img, target = data[1], data[0]\n                features = model(input_img.to(self.pars.device))\n                feature_coll.extend(features.cpu().detach().numpy().tolist())\n                target_coll.extend(target.numpy().tolist())\n\n        feature_coll = np.vstack(feature_coll).astype(\'float32\')\n        target_coll   = np.hstack(target_coll).reshape(-1)\n        avail_labels  = np.unique(target_coll)\n\n        #Compute indixes of embeddings for each class.\n        class_positions = []\n        for lab in avail_labels:\n            class_positions.append(np.where(target_coll==lab)[0])\n\n        #Compute average intra-class distance and center of mass.\n        com_class, dists_class = [],[]\n        for class_pos in class_positions:\n            dists = distance.cdist(feature_coll[class_pos],feature_coll[class_pos],\'cosine\')\n            dists = np.sum(dists)/(len(dists)**2-len(dists))\n            # dists = np.linalg.norm(np.std(feature_coll_aux[class_pos],axis=0).reshape(1,-1)).reshape(-1)\n            com   = normalize(np.mean(feature_coll[class_pos],axis=0).reshape(1,-1)).reshape(-1)\n            dists_class.append(dists)\n            com_class.append(com)\n\n        #Compute mean inter-class distances by the class-coms.\n        mean_inter_dist = distance.cdist(np.array(com_class), np.array(com_class), \'cosine\')\n        mean_inter_dist = np.sum(mean_inter_dist)/(len(mean_inter_dist)**2-len(mean_inter_dist))\n\n        #Compute distance ratio\n        mean_class_dist = np.mean(np.array(dists_class)/mean_inter_dist)\n        self.mean_class_dists.append(mean_class_dist)\n\n        self.update(mean_class_dist)\n\n\n    def update(self, mean_class_dist):\n        """"""\n        Update Loggers.\n\n        Args:\n            mean_class_dist: float, Distance Ratio\n        Returns:\n            Nothing!\n        """"""\n        self.update_csv(mean_class_dist)\n        self.update_plot()\n\n\n    def update_csv(self, mean_class_dist):\n        """"""\n        Update CSV.\n\n        Args:\n            mean_class_dist: float, Distance Ratio\n        Returns:\n            Nothing!\n        """"""\n        with open(self.csv_file, \'a\') as csv_file:\n            writer = csv.writer(csv_file, delimiter=\',\')\n            writer.writerow([mean_class_dist])\n\n\n    def update_plot(self):\n        """"""\n        Update progression plot.\n\n        Args:\n            None.\n        Returns:\n            Nothing!\n        """"""\n        plt.style.use(\'ggplot\')\n        f,ax = plt.subplots(1)\n        ax.set_title(\'Mean Intra- over Interclassdistances\')\n        ax.plot(self.epochs, self.mean_class_dists, label=\'Class\')\n        f.legend()\n        f.set_size_inches(15,8)\n        f.savefig(self.save_path+\'/distance_measures_{}.svg\'.format(self.name))\n\n\n\n\nclass GradientMeasure():\n    """"""\n    Container for gradient measure functionalities.\n    Measure the gradients coming from the embedding layer to the final conv. layer\n    to examine learning signal.\n    """"""\n    def __init__(self, opt, name=\'class-it\'):\n        """"""\n        Args:\n            opt:   argparse.Namespace, contains all training-specific parameters.\n            name:  Name of class instance. Important for the savename.\n        Returns:\n            Nothing!\n        """"""\n        self.pars  = opt\n        self.name  = name\n        self.saver = {\'grad_normal_mean\':[], \'grad_normal_std\':[], \'grad_abs_mean\':[], \'grad_abs_std\':[]}\n\n\n    def include(self, params):\n        """"""\n        Include the gradients for a set of parameters, normally the final embedding layer.\n\n        Args:\n            params: PyTorch Network layer after .backward() was called.\n        Returns:\n            Nothing!\n        """"""\n        gradients = [params.weight.grad.detach().cpu().numpy()]\n\n        for grad in gradients:\n            ### Shape: 128 x 2048\n            self.saver[\'grad_normal_mean\'].append(np.mean(grad,axis=0))\n            self.saver[\'grad_normal_std\'].append(np.std(grad,axis=0))\n            self.saver[\'grad_abs_mean\'].append(np.mean(np.abs(grad),axis=0))\n            self.saver[\'grad_abs_std\'].append(np.std(np.abs(grad),axis=0))\n\n    def dump(self, epoch):\n        """"""\n        Append all gradients to a pickle file.\n\n        Args:\n            epoch: Current epoch\n        Returns:\n            Nothing!\n        """"""\n        with open(self.pars.save_path+\'/grad_dict_{}.pkl\'.format(self.name),\'ab\') as f:\n            pkl.dump([self.saver], f)\n        self.saver = {\'grad_normal_mean\':[], \'grad_normal_std\':[], \'grad_abs_mean\':[], \'grad_abs_std\':[]}\n\n\n\n\n""""""=========================================================""""""\ndef evaluate_one_dataset(LOG, dataloader, model, opt, save=True, give_return=False, epoch=0):\n    """"""\n    Compute evaluation metrics, update LOGGER and print results.\n\n    Args:\n        LOG:         aux.LOGGER-instance. Main Logging Functionality.\n        dataloader:  PyTorch Dataloader, Testdata to be evaluated.\n        model:       PyTorch Network, Network to evaluate.\n        opt:         argparse.Namespace, contains all training-specific parameters.\n        save:        bool, if True, Checkpoints are saved when testing metrics (specifically Recall @ 1) improve.\n        give_return: bool, if True, return computed metrics.\n        epoch:       int, current epoch, required for logger.\n    Returns:\n        (optional) Computed metrics. Are normally written directly to LOG and printed.\n    """"""\n    start = time.time()\n    image_paths = np.array(dataloader.dataset.image_list)\n\n    with torch.no_grad():\n        #Compute Metrics\n        F1, NMI, recall_at_ks, feature_matrix_all = aux.eval_metrics_one_dataset(model, dataloader, device=opt.device, k_vals=opt.k_vals, opt=opt)\n        #Make printable summary string.\n        result_str = \', \'.join(\'@{0}: {1:.4f}\'.format(k,rec) for k,rec in zip(opt.k_vals, recall_at_ks))\n        result_str = \'Epoch (Test) {0}: NMI [{1:.4f}] | F1 [{2:.4f}] | Recall [{3}]\'.format(epoch, NMI, F1, result_str)\n\n        if LOG is not None:\n            if save:\n                if not len(LOG.progress_saver[\'val\'][\'Recall @ 1\']) or recall_at_ks[0]>np.max(LOG.progress_saver[\'val\'][\'Recall @ 1\']):\n                    #Save Checkpoint\n                    aux.set_checkpoint(model, opt, LOG.progress_saver, LOG.prop.save_path+\'/checkpoint.pth.tar\')\n                    aux.recover_closest_one_dataset(feature_matrix_all, image_paths, LOG.prop.save_path+\'/sample_recoveries.png\')\n            #Update logs.\n            LOG.log(\'val\', LOG.metrics_to_log[\'val\'], [epoch, np.round(time.time()-start), NMI, F1]+recall_at_ks)\n\n    print(result_str)\n    if give_return:\n        return recall_at_ks, NMI, F1\n    else:\n        None\n\n\n\n\n""""""=========================================================""""""\ndef evaluate_query_and_gallery_dataset(LOG, query_dataloader, gallery_dataloader, model, opt, save=True, give_return=False, epoch=0):\n    """"""\n    Compute evaluation metrics, update LOGGER and print results, specifically for In-Shop Clothes.\n\n    Args:\n        LOG:         aux.LOGGER-instance. Main Logging Functionality.\n        query_dataloader:    PyTorch Dataloader, Query-testdata to be evaluated.\n        gallery_dataloader:  PyTorch Dataloader, Gallery-testdata to be evaluated.\n        model:       PyTorch Network, Network to evaluate.\n        opt:         argparse.Namespace, contains all training-specific parameters.\n        save:        bool, if True, Checkpoints are saved when testing metrics (specifically Recall @ 1) improve.\n        give_return: bool, if True, return computed metrics.\n        epoch:       int, current epoch, required for logger.\n    Returns:\n        (optional) Computed metrics. Are normally written directly to LOG and printed.\n    """"""\n    start = time.time()\n    query_image_paths   = np.array([x[0] for x in query_dataloader.dataset.image_list])\n    gallery_image_paths = np.array([x[0] for x in gallery_dataloader.dataset.image_list])\n\n    with torch.no_grad():\n        #Compute Metrics.\n        F1, NMI, recall_at_ks, query_feature_matrix_all, gallery_feature_matrix_all = aux.eval_metrics_query_and_gallery_dataset(model, query_dataloader, gallery_dataloader, device=opt.device, k_vals = opt.k_vals, opt=opt)\n        #Generate printable summary string.\n        result_str = \', \'.join(\'@{0}: {1:.4f}\'.format(k,rec) for k,rec in zip(opt.k_vals, recall_at_ks))\n        result_str = \'Epoch (Test) {0}: NMI [{1:.4f}] | F1 [{2:.4f}] | Recall [{3}]\'.format(epoch, NMI, F1, result_str)\n\n        if LOG is not None:\n            if save:\n                if not len(LOG.progress_saver[\'val\'][\'Recall @ 1\']) or recall_at_ks[0]>np.max(LOG.progress_saver[\'val\'][\'Recall @ 1\']):\n                    #Save Checkpoint\n                    aux.set_checkpoint(model, opt, LOG.progress_saver, LOG.prop.save_path+\'/checkpoint.pth.tar\')\n                    aux.recover_closest_inshop(query_feature_matrix_all, gallery_feature_matrix_all, query_image_paths, gallery_image_paths, LOG.prop.save_path+\'/sample_recoveries.png\')\n            #Update logs.\n            LOG.log(\'val\', LOG.metrics_to_log[\'val\'], [epoch, np.round(time.time()-start), NMI, F1]+recall_at_ks)\n\n    print(result_str)\n    if give_return:\n        return recall_at_ks, NMI, F1\n    else:\n        None\n\n\n\n\n""""""=========================================================""""""\ndef evaluate_multiple_datasets(LOG, dataloaders, model, opt, save=True, give_return=False, epoch=0):\n    """"""\n    Compute evaluation metrics, update LOGGER and print results, specifically for Multi-test datasets s.a. PKU Vehicle ID.\n\n    Args:\n        LOG:         aux.LOGGER-instance. Main Logging Functionality.\n        dataloaders: List of PyTorch Dataloaders, test-dataloaders to evaluate.\n        model:       PyTorch Network, Network to evaluate.\n        opt:         argparse.Namespace, contains all training-specific parameters.\n        save:        bool, if True, Checkpoints are saved when testing metrics (specifically Recall @ 1) improve.\n        give_return: bool, if True, return computed metrics.\n        epoch:       int, current epoch, required for logger.\n    Returns:\n        (optional) Computed metrics. Are normally written directly to LOG and printed.\n    """"""\n    start = time.time()\n\n    csv_data = [epoch]\n\n    with torch.no_grad():\n        for i,dataloader in enumerate(dataloaders):\n            print(\'Working on Set {}/{}\'.format(i+1, len(dataloaders)))\n            image_paths = np.array(dataloader.dataset.image_list)\n            #Compute Metrics for specific testset.\n            F1, NMI, recall_at_ks, feature_matrix_all = aux.eval_metrics_one_dataset(model, dataloader, device=opt.device, k_vals=opt.k_vals, opt=opt)\n            #Generate printable summary string.\n            result_str = \', \'.join(\'@{0}: {1:.4f}\'.format(k,rec) for k,rec in zip(opt.k_vals, recall_at_ks))\n            result_str = \'SET {0}: Epoch (Test) {1}: NMI [{2:.4f}] | F1 {3:.4f}| Recall [{4}]\'.format(i+1, epoch, NMI, F1, result_str)\n\n            if LOG is not None:\n                if save:\n                    if not len(LOG.progress_saver[\'val\'][\'Set {} Recall @ 1\'.format(i)]) or recall_at_ks[0]>np.max(LOG.progress_saver[\'val\'][\'Set {} Recall @ 1\'.format(i)]):\n                        #Save Checkpoint for specific test set.\n                        aux.set_checkpoint(model, opt, LOG.progress_saver, LOG.prop.save_path+\'/checkpoint_set{}.pth.tar\'.format(i+1))\n                        aux.recover_closest_one_dataset(feature_matrix_all, image_paths, LOG.prop.save_path+\'/sample_recoveries_set{}.png\'.format(i+1))\n\n                csv_data += [NMI, F1]+recall_at_ks\n            print(result_str)\n\n    csv_data.insert(0, np.round(time.time()-start))\n    #Update logs.\n    LOG.log(\'val\', LOG.metrics_to_log[\'val\'], csv_data)\n\n\n    if give_return:\n        return csv_data[2:]\n    else:\n        None\n'"
googlenet.py,11,"b'### NOTE: THIS IMPLEMENTATION IS COPIED FROM THE OFFICIAL PYTORCH REPO!\n### NOTE: THIS IMPLEMENTATION IS COPIED FROM THE OFFICIAL PYTORCH REPO!\n### NOTE: THIS IMPLEMENTATION IS COPIED FROM THE OFFICIAL PYTORCH REPO!\n### NOTE: THIS IMPLEMENTATION IS COPIED FROM THE OFFICIAL PYTORCH REPO!\n\n\nimport warnings\nfrom collections import namedtuple\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils import model_zoo\n\n__all__ = [\'GoogLeNet\', \'googlenet\']\n\nmodel_urls = {\n    # GoogLeNet ported from TensorFlow\n    \'googlenet\': \'https://download.pytorch.org/models/googlenet-1378be20.pth\',\n}\n\n_GoogLeNetOuputs = namedtuple(\'GoogLeNetOuputs\', [\'logits\', \'aux_logits2\', \'aux_logits1\'])\n\n\ndef googlenet(pretrained=False, **kwargs):\n    r""""""GoogLeNet (Inception v1) model architecture from\n    `""Going Deeper with Convolutions"" <http://arxiv.org/abs/1409.4842>`_.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        aux_logits (bool): If True, adds two auxiliary branches that can improve training.\n            Default: *False* when pretrained is True otherwise *True*\n        transform_input (bool): If True, preprocesses the input according to the method with which it\n            was trained on ImageNet. Default: *False*\n    """"""\n    if pretrained:\n        if \'transform_input\' not in kwargs:\n            kwargs[\'transform_input\'] = True\n        if \'aux_logits\' not in kwargs:\n            kwargs[\'aux_logits\'] = False\n        if kwargs[\'aux_logits\']:\n            warnings.warn(\'auxiliary heads in the pretrained googlenet model are NOT pretrained, \'\n                          \'so make sure to train them\')\n        original_aux_logits = kwargs[\'aux_logits\']\n        kwargs[\'aux_logits\'] = True\n        kwargs[\'init_weights\'] = False\n        model = GoogLeNet(**kwargs)\n        model.load_state_dict(model_zoo.load_url(model_urls[\'googlenet\']))\n        if not original_aux_logits:\n            model.aux_logits = False\n            del model.aux1, model.aux2\n        return model\n\n    return GoogLeNet(**kwargs)\n\n\nclass GoogLeNet(nn.Module):\n\n    def __init__(self, num_classes=1000, aux_logits=True, transform_input=False, init_weights=True):\n        super(GoogLeNet, self).__init__()\n        self.aux_logits = aux_logits\n        self.transform_input = transform_input\n\n        self.conv1 = BasicConv2d(3, 64, kernel_size=7, stride=2, padding=3)\n        self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n        self.conv2 = BasicConv2d(64, 64, kernel_size=1)\n        self.conv3 = BasicConv2d(64, 192, kernel_size=3, padding=1)\n        self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n\n        self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)\n        self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)\n        self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n\n        self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)\n        self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)\n        self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)\n        self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)\n        self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)\n        self.maxpool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n\n        self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)\n        self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)\n\n        if aux_logits:\n            self.aux1 = InceptionAux(512, num_classes)\n            self.aux2 = InceptionAux(528, num_classes)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(0.2)\n        self.fc = nn.Linear(1024, num_classes)\n\n        if init_weights:\n            self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                import scipy.stats as stats\n                X = stats.truncnorm(-2, 2, scale=0.01)\n                values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n                values = values.view(m.weight.size())\n                with torch.no_grad():\n                    m.weight.copy_(values)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        if self.transform_input:\n            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n\n        # N x 3 x 224 x 224\n        x = self.conv1(x)\n        # N x 64 x 112 x 112\n        x = self.maxpool1(x)\n        # N x 64 x 56 x 56\n        x = self.conv2(x)\n        # N x 64 x 56 x 56\n        x = self.conv3(x)\n        # N x 192 x 56 x 56\n        x = self.maxpool2(x)\n\n        # N x 192 x 28 x 28\n        x = self.inception3a(x)\n        # N x 256 x 28 x 28\n        x = self.inception3b(x)\n        # N x 480 x 28 x 28\n        x = self.maxpool3(x)\n        # N x 480 x 14 x 14\n        x = self.inception4a(x)\n        # N x 512 x 14 x 14\n        if self.training and self.aux_logits:\n            aux1 = self.aux1(x)\n\n        x = self.inception4b(x)\n        # N x 512 x 14 x 14\n        x = self.inception4c(x)\n        # N x 512 x 14 x 14\n        x = self.inception4d(x)\n        # N x 528 x 14 x 14\n        if self.training and self.aux_logits:\n            aux2 = self.aux2(x)\n\n        x = self.inception4e(x)\n        # N x 832 x 14 x 14\n        x = self.maxpool4(x)\n        # N x 832 x 7 x 7\n        x = self.inception5a(x)\n        # N x 832 x 7 x 7\n        x = self.inception5b(x)\n        # N x 1024 x 7 x 7\n\n        x = self.avgpool(x)\n        # N x 1024 x 1 x 1\n        x = x.view(x.size(0), -1)\n        # N x 1024\n        x = self.dropout(x)\n        x = self.fc(x)\n        # N x 1000 (num_classes)\n        if self.training and self.aux_logits:\n            return _GoogLeNetOuputs(x, aux2, aux1)\n        return x\n\n\nclass Inception(nn.Module):\n\n    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\n        super(Inception, self).__init__()\n\n        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1)\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(in_channels, ch3x3red, kernel_size=1),\n            BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)\n        )\n\n        self.branch3 = nn.Sequential(\n            BasicConv2d(in_channels, ch5x5red, kernel_size=1),\n            BasicConv2d(ch5x5red, ch5x5, kernel_size=3, padding=1)\n        )\n\n        self.branch4 = nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),\n            BasicConv2d(in_channels, pool_proj, kernel_size=1)\n        )\n\n    def forward(self, x):\n        branch1 = self.branch1(x)\n        branch2 = self.branch2(x)\n        branch3 = self.branch3(x)\n        branch4 = self.branch4(x)\n\n        outputs = [branch1, branch2, branch3, branch4]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionAux(nn.Module):\n\n    def __init__(self, in_channels, num_classes):\n        super(InceptionAux, self).__init__()\n        self.conv = BasicConv2d(in_channels, 128, kernel_size=1)\n\n        self.fc1 = nn.Linear(2048, 1024)\n        self.fc2 = nn.Linear(1024, num_classes)\n\n    def forward(self, x):\n        # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14\n        x = F.adaptive_avg_pool2d(x, (4, 4))\n        # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4\n        x = self.conv(x)\n        # N x 128 x 4 x 4\n        x = x.view(x.size(0), -1)\n        # N x 2048\n        x = F.relu(self.fc1(x), inplace=True)\n        # N x 2048\n        x = F.dropout(x, 0.7, training=self.training)\n        # N x 2048\n        x = self.fc2(x)\n        # N x 1024\n\n        return x\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return F.relu(x, inplace=True)\n'"
losses.py,70,"b'# Copyright 2019 Karsten Roth and Biagio Brattoli\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n###################### LIBRARIES #################################################\nimport warnings\nwarnings.filterwarnings(""ignore"")\n\nimport torch, random, itertools as it, numpy as np, faiss, random\nfrom tqdm import tqdm\n\nfrom scipy.spatial.distance import cdist\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import normalize\n\nfrom PIL import Image\n\n\n\n""""""=================================================================================================""""""\n############ LOSS SELECTION FUNCTION #####################\ndef loss_select(loss, opt, to_optim):\n    """"""\n    Selection function which returns the respective criterion while appending to list of trainable parameters if required.\n\n    Args:\n        loss:     str, name of loss function to return.\n        opt:      argparse.Namespace, contains all training-specific parameters.\n        to_optim: list of trainable parameters. Is extend if loss function contains those as well.\n    Returns:\n        criterion (torch.nn.Module inherited), to_optim (optionally appended)\n    """"""\n    if loss==\'triplet\':\n        loss_params  = {\'margin\':opt.margin, \'sampling_method\':opt.sampling}\n        criterion    = TripletLoss(**loss_params)\n    elif loss==\'npair\':\n        loss_params  = {\'l2\':opt.l2npair}\n        criterion    = NPairLoss(**loss_params)\n    elif loss==\'marginloss\':\n        loss_params  = {\'margin\':opt.margin, \'nu\': opt.nu, \'beta\':opt.beta, \'n_classes\':opt.num_classes, \'sampling_method\':opt.sampling}\n        criterion    = MarginLoss(**loss_params)\n        to_optim    += [{\'params\':criterion.parameters(), \'lr\':opt.beta_lr, \'weight_decay\':0}]\n    elif loss==\'proxynca\':\n        loss_params  = {\'num_proxies\':opt.num_classes, \'embedding_dim\':opt.classembed if \'num_cluster\' in vars(opt).keys() else opt.embed_dim}\n        criterion    = ProxyNCALoss(**loss_params)\n        to_optim    += [{\'params\':criterion.parameters(), \'lr\':opt.proxy_lr}]\n    elif loss==\'crossentropy\':\n        loss_params  = {\'n_classes\':opt.num_classes, \'inp_dim\':opt.embed_dim}\n        criterion    = CEClassLoss(**loss_params)\n        to_optim    += [{\'params\':criterion.parameters(), \'lr\':opt.lr, \'weight_decay\':0}]\n    else:\n        raise Exception(\'Loss {} not available!\'.format(loss))\n\n    return criterion, to_optim\n\n\n\n\n\n""""""=================================================================================================""""""\n######### MAIN SAMPLER CLASS #################################\nclass TupleSampler():\n    """"""\n    Container for all sampling methods that can be used in conjunction with the respective loss functions.\n    Based on batch-wise sampling, i.e. given a batch of training data, sample useful data tuples that are\n    used to train the network more efficiently.\n    """"""\n    def __init__(self, method=\'random\'):\n        """"""\n        Args:\n            method: str, name of sampling method to use.\n        Returns:\n            Nothing!\n        """"""\n        self.method = method\n        if method==\'semihard\':\n            self.give = self.semihardsampling\n        if method==\'softhard\':\n            self.give = self.softhardsampling\n        elif method==\'distance\':\n            self.give = self.distanceweightedsampling\n        elif method==\'npair\':\n            self.give = self.npairsampling\n        elif method==\'random\':\n            self.give = self.randomsampling\n\n    def randomsampling(self, batch, labels):\n        """"""\n        This methods finds all available triplets in a batch given by the classes provided in labels, and randomly\n        selects <len(batch)> triplets.\n\n        Args:\n            batch:  np.ndarray or torch.Tensor, batch-wise embedded training samples.\n            labels: np.ndarray or torch.Tensor, ground truth labels corresponding to batch.\n        Returns:\n            list of sampled data tuples containing reference indices to the position IN THE BATCH.\n        """"""\n        if isinstance(labels, torch.Tensor): labels = labels.detach().numpy()\n        unique_classes = np.unique(labels)\n        indices        = np.arange(len(batch))\n        class_dict     = {i:indices[labels==i] for i in unique_classes}\n\n        sampled_triplets = [list(it.product([x],[x],[y for y in unique_classes if x!=y])) for x in unique_classes]\n        sampled_triplets = [x for y in sampled_triplets for x in y]\n\n        sampled_triplets = [[x for x in list(it.product(*[class_dict[j] for j in i])) if x[0]!=x[1]] for i in sampled_triplets]\n        sampled_triplets = [x for y in sampled_triplets for x in y]\n\n        #NOTE: The number of possible triplets is given by #unique_classes*(2*(samples_per_class-1)!)*(#unique_classes-1)*samples_per_class\n        sampled_triplets = random.sample(sampled_triplets, batch.shape[0])\n        return sampled_triplets\n\n\n    def semihardsampling(self, batch, labels, margin=0.2):\n        if isinstance(labels, torch.Tensor):\n            labels = labels.detach().numpy()\n        bs = batch.size(0)\n        #Return distance matrix for all elements in batch (BSxBS)\n        distances = self.pdist(batch.detach()).detach().cpu().numpy()\n\n        positives, negatives = [], []\n        anchors = []\n        for i in range(bs):\n            l, d = labels[i], distances[i]\n            neg = labels!=l; pos = labels==l\n\n            anchors.append(i)\n            pos[i] = False\n            p      = np.random.choice(np.where(pos)[0])\n            positives.append(p)\n\n            #Find negatives that violate tripet constraint semi-negatives\n            neg_mask = np.logical_and(neg,d>d[p])\n            neg_mask = np.logical_and(neg_mask,d<margin+d[p])\n            if neg_mask.sum()>0:\n                negatives.append(np.random.choice(np.where(neg_mask)[0]))\n            else:\n                negatives.append(np.random.choice(np.where(neg)[0]))\n\n        sampled_triplets = [[a, p, n] for a, p, n in zip(anchors, positives, negatives)]\n        return sampled_triplets\n\n    def softhardsampling(self, batch, labels):\n        """"""\n        This methods finds all available triplets in a batch given by the classes provided in labels, and select\n        triplets based on semihard sampling introduced in \'https://arxiv.org/pdf/1503.03832.pdf\'.\n\n        Args:\n            batch:  np.ndarray or torch.Tensor, batch-wise embedded training samples.\n            labels: np.ndarray or torch.Tensor, ground truth labels corresponding to batch.\n        Returns:\n            list of sampled data tuples containing reference indices to the position IN THE BATCH.\n        """"""\n        if isinstance(labels, torch.Tensor): labels = labels.detach().numpy()\n        bs = batch.size(0)\n        #Return distance matrix for all elements in batch (BSxBS)\n        distances = self.pdist(batch.detach()).detach().cpu().numpy()\n\n        positives, negatives = [], []\n        anchors = []\n        for i in range(bs):\n            l, d = labels[i], distances[i]\n            anchors.append(i)\n            #1 for batchelements with label l\n            neg = labels!=l; pos = labels==l\n            #0 for current anchor\n            pos[i] = False\n\n            #Find negatives that violate triplet constraint semi-negatives\n            neg_mask = np.logical_and(neg,d<d[np.where(pos)[0]].max())\n            #Find positives that violate triplet constraint semi-hardly\n            pos_mask = np.logical_and(pos,d>d[np.where(neg)[0]].min())\n\n            if pos_mask.sum()>0:\n                positives.append(np.random.choice(np.where(pos_mask)[0]))\n            else:\n                positives.append(np.random.choice(np.where(pos)[0]))\n\n            if neg_mask.sum()>0:\n                negatives.append(np.random.choice(np.where(neg_mask)[0]))\n            else:\n                negatives.append(np.random.choice(np.where(neg)[0]))\n\n        sampled_triplets = [[a, p, n] for a, p, n in zip(anchors, positives, negatives)]\n        return sampled_triplets\n\n\n    def distanceweightedsampling(self, batch, labels, lower_cutoff=0.5, upper_cutoff=1.4):\n        """"""\n        This methods finds all available triplets in a batch given by the classes provided in labels, and select\n        triplets based on distance sampling introduced in \'Sampling Matters in Deep Embedding Learning\'.\n\n        Args:\n            batch:  np.ndarray or torch.Tensor, batch-wise embedded training samples.\n            labels: np.ndarray or torch.Tensor, ground truth labels corresponding to batch.\n            lower_cutoff: float, lower cutoff value for negatives that are too close to anchor embeddings. Set to literature value. They will be assigned a zero-sample probability.\n            upper_cutoff: float, upper cutoff value for positives that are too far away from the anchor embeddings. Set to literature value. They will be assigned a zero-sample probability.\n        Returns:\n            list of sampled data tuples containing reference indices to the position IN THE BATCH.\n        """"""\n        if isinstance(labels, torch.Tensor): labels = labels.detach().cpu().numpy()\n        bs = batch.shape[0]\n\n        distances    = self.pdist(batch.detach()).clamp(min=lower_cutoff)\n\n\n\n        positives, negatives = [],[]\n        labels_visited = []\n        anchors = []\n\n        for i in range(bs):\n            neg = labels!=labels[i]; pos = labels==labels[i]\n            q_d_inv = self.inverse_sphere_distances(batch, distances[i], labels, labels[i])\n            #Sample positives randomly\n            pos[i] = 0\n            positives.append(np.random.choice(np.where(pos)[0]))\n            #Sample negatives by distance\n            negatives.append(np.random.choice(bs,p=q_d_inv))\n\n        sampled_triplets = [[a,p,n] for a,p,n in zip(list(range(bs)), positives, negatives)]\n        return sampled_triplets\n\n\n    def npairsampling(self, batch, labels):\n        """"""\n        This methods finds N-Pairs in a batch given by the classes provided in labels in the\n        creation fashion proposed in \'Improved Deep Metric Learning with Multi-class N-pair Loss Objective\'.\n\n        Args:\n            batch:  np.ndarray or torch.Tensor, batch-wise embedded training samples.\n            labels: np.ndarray or torch.Tensor, ground truth labels corresponding to batch.\n        Returns:\n            list of sampled data tuples containing reference indices to the position IN THE BATCH.\n        """"""\n        if isinstance(labels, torch.Tensor):    labels = labels.detach().cpu().numpy()\n\n        label_set, count = np.unique(labels, return_counts=True)\n        label_set  = label_set[count>=2]\n        pos_pairs  = np.array([np.random.choice(np.where(labels==x)[0], 2, replace=False) for x in label_set])\n        neg_tuples = []\n\n        for idx in range(len(pos_pairs)):\n            neg_tuples.append(pos_pairs[np.delete(np.arange(len(pos_pairs)),idx),1])\n\n        neg_tuples = np.array(neg_tuples)\n\n        sampled_npairs = [[a,p,*list(neg)] for (a,p),neg in zip(pos_pairs, neg_tuples)]\n        return sampled_npairs\n\n\n    def pdist(self, A):\n        """"""\n        Efficient function to compute the distance matrix for a matrix A.\n\n        Args:\n            A:   Matrix/Tensor for which the distance matrix is to be computed.\n            eps: float, minimal distance/clampling value to ensure no zero values.\n        Returns:\n            distance_matrix, clamped to ensure no zero values are passed.\n        """"""\n        prod = torch.mm(A, A.t())\n        norm = prod.diag().unsqueeze(1).expand_as(prod)\n        res = (norm + norm.t() - 2 * prod).clamp(min = 0)\n        return res.clamp(min = 0).sqrt()\n\n\n    def inverse_sphere_distances(self, batch, dist, labels, anchor_label):\n        """"""\n        Function to utilise the distances of batch samples to compute their\n        probability of occurence, and using the inverse to sample actual negatives to the resp. anchor.\n\n        Args:\n            batch:        torch.Tensor(), batch for which the sampling probabilities w.r.t to the anchor are computed. Used only to extract the shape.\n            dist:         torch.Tensor(), computed distances between anchor to all batch samples.\n            labels:       np.ndarray, labels for each sample for which distances were computed in dist.\n            anchor_label: float, anchor label\n        Returns:\n            distance_matrix, clamped to ensure no zero values are passed.\n        """"""\n        bs,dim       = len(dist),batch.shape[-1]\n\n        #negated log-distribution of distances of unit sphere in dimension <dim>\n        log_q_d_inv = ((2.0 - float(dim)) * torch.log(dist) - (float(dim-3) / 2) * torch.log(1.0 - 0.25 * (dist.pow(2))))\n        #Set sampling probabilities of positives to zero\n        log_q_d_inv[np.where(labels==anchor_label)[0]] = 0\n\n        q_d_inv     = torch.exp(log_q_d_inv - torch.max(log_q_d_inv)) # - max(log) for stability\n        #Set sampling probabilities of positives to zero\n        q_d_inv[np.where(labels==anchor_label)[0]] = 0\n\n        ### NOTE: Cutting of values with high distances made the results slightly worse.\n        # q_d_inv[np.where(dist>upper_cutoff)[0]]    = 0\n\n        #Normalize inverted distance for probability distr.\n        q_d_inv = q_d_inv/q_d_inv.sum()\n        return q_d_inv.detach().cpu().numpy()\n\n\n\n\n""""""=================================================================================================""""""\n### Standard Triplet Loss, finds triplets in Mini-batches.\nclass TripletLoss(torch.nn.Module):\n    def __init__(self, margin=1, sampling_method=\'random\'):\n        """"""\n        Basic Triplet Loss as proposed in \'FaceNet: A Unified Embedding for Face Recognition and Clustering\'\n        Args:\n            margin:             float, Triplet Margin - Ensures that positives aren\'t placed arbitrarily close to the anchor.\n                                Similarl, negatives should not be placed arbitrarily far away.\n            sampling_method:    Method to use for sampling training triplets. Used for the TupleSampler-class.\n        """"""\n        super(TripletLoss, self).__init__()\n        self.margin             = margin\n        self.sampler            = TupleSampler(method=sampling_method)\n\n    def triplet_distance(self, anchor, positive, negative):\n        """"""\n        Compute triplet loss.\n\n        Args:\n            anchor, positive, negative: torch.Tensor(), resp. embeddings for anchor, positive and negative samples.\n        Returns:\n            triplet loss (torch.Tensor())\n        """"""\n        return torch.nn.functional.relu((anchor-positive).pow(2).sum()-(anchor-negative).pow(2).sum()+self.margin)\n\n    def forward(self, batch, labels):\n        """"""\n        Args:\n            batch:   torch.Tensor() [(BS x embed_dim)], batch of embeddings\n            labels:  np.ndarray [(BS x 1)], for each element of the batch assigns a class [0,...,C-1]\n        Returns:\n            triplet loss (torch.Tensor(), batch-averaged)\n        """"""\n        #Sample triplets to use for training.\n        sampled_triplets = self.sampler.give(batch, labels)\n        #Compute triplet loss\n        loss             = torch.stack([self.triplet_distance(batch[triplet[0],:],batch[triplet[1],:],batch[triplet[2],:]) for triplet in sampled_triplets])\n\n        return torch.mean(loss)\n\n\n\n""""""=================================================================================================""""""\n### Standard N-Pair Loss.\nclass NPairLoss(torch.nn.Module):\n    def __init__(self, l2=0.02):\n        """"""\n        Basic N-Pair Loss as proposed in \'Improved Deep Metric Learning with Multi-class N-pair Loss Objective\'\n\n        Args:\n            l2: float, weighting parameter for weight penality due to embeddings not being normalized.\n        Returns:\n            Nothing!\n        """"""\n        super(NPairLoss, self).__init__()\n        self.sampler = TupleSampler(method=\'npair\')\n        self.l2      = l2\n\n    def npair_distance(self, anchor, positive, negatives):\n        """"""\n        Compute basic N-Pair loss.\n\n        Args:\n            anchor, positive, negative: torch.Tensor(), resp. embeddings for anchor, positive and negative samples.\n        Returns:\n            n-pair loss (torch.Tensor())\n        """"""\n        return torch.log(1+torch.sum(torch.exp(anchor.mm((negatives-positive).transpose(0,1)))))\n\n    def weightsum(self, anchor, positive):\n        """"""\n        Compute weight penalty.\n        NOTE: Only need to penalize anchor and positive since the negatives are created based on these.\n\n        Args:\n            anchor, positive: torch.Tensor(), resp. embeddings for anchor and positive samples.\n        Returns:\n            torch.Tensor(), Weight penalty\n        """"""\n        return torch.sum(anchor**2+positive**2)\n\n    def forward(self, batch, labels):\n        """"""\n        Args:\n            batch:   torch.Tensor() [(BS x embed_dim)], batch of embeddings\n            labels:  np.ndarray [(BS x 1)], for each element of the batch assigns a class [0,...,C-1]\n        Returns:\n            n-pair loss (torch.Tensor(), batch-averaged)\n        """"""\n        #Sample N-Pairs\n        sampled_npairs = self.sampler.give(batch, labels)\n        #Compute basic n=pair loss\n        loss           = torch.stack([self.npair_distance(batch[npair[0]:npair[0]+1,:],batch[npair[1]:npair[1]+1,:],batch[npair[2:],:]) for npair in sampled_npairs])\n        #Include weight penalty\n        loss           = loss + self.l2*torch.mean(torch.stack([self.weightsum(batch[npair[0],:], batch[npair[1],:]) for npair in sampled_npairs]))\n\n        return torch.mean(loss)\n\n\n\n\n""""""=================================================================================================""""""\n### MarginLoss with trainable class separation margin beta. Runs on Mini-batches as well.\nclass MarginLoss(torch.nn.Module):\n    def __init__(self, margin=0.2, nu=0, beta=1.2, n_classes=100, beta_constant=False, sampling_method=\'distance\'):\n        """"""\n        Basic Margin Loss as proposed in \'Sampling Matters in Deep Embedding Learning\'.\n\n        Args:\n            margin:          float, fixed triplet margin (see also TripletLoss).\n            nu:              float, regularisation weight for beta. Zero by default (in literature as well).\n            beta:            float, initial value for trainable class margins. Set to default literature value.\n            n_classes:       int, number of target class. Required because it dictates the number of trainable class margins.\n            beta_constant:   bool, set to True if betas should not be trained.\n            sampling_method: str, sampling method to use to generate training triplets.\n        Returns:\n            Nothing!\n        """"""\n        super(MarginLoss, self).__init__()\n        self.margin             = margin\n        self.n_classes          = n_classes\n        self.beta_constant     = beta_constant\n\n        self.beta_val = beta\n        self.beta     = beta if beta_constant else torch.nn.Parameter(torch.ones(n_classes)*beta)\n\n        self.nu                 = nu\n\n        self.sampling_method    = sampling_method\n        self.sampler            = TupleSampler(method=sampling_method)\n\n\n    def forward(self, batch, labels):\n        """"""\n        Args:\n            batch:   torch.Tensor() [(BS x embed_dim)], batch of embeddings\n            labels:  np.ndarray [(BS x 1)], for each element of the batch assigns a class [0,...,C-1]\n        Returns:\n            margin loss (torch.Tensor(), batch-averaged)\n        """"""\n        if isinstance(labels, torch.Tensor): labels = labels.detach().cpu().numpy()\n\n        sampled_triplets = self.sampler.give(batch, labels)\n\n        #Compute distances between anchor-positive and anchor-negative.\n        d_ap, d_an = [],[]\n        for triplet in sampled_triplets:\n            train_triplet = {\'Anchor\': batch[triplet[0],:], \'Positive\':batch[triplet[1],:], \'Negative\':batch[triplet[2]]}\n\n            pos_dist = ((train_triplet[\'Anchor\']-train_triplet[\'Positive\']).pow(2).sum()+1e-8).pow(1/2)\n            neg_dist = ((train_triplet[\'Anchor\']-train_triplet[\'Negative\']).pow(2).sum()+1e-8).pow(1/2)\n\n            d_ap.append(pos_dist)\n            d_an.append(neg_dist)\n        d_ap, d_an = torch.stack(d_ap), torch.stack(d_an)\n\n        #Group betas together by anchor class in sampled triplets (as each beta belongs to one class).\n        if self.beta_constant:\n            beta = self.beta\n        else:\n            beta = torch.stack([self.beta[labels[triplet[0]]] for triplet in sampled_triplets]).type(torch.cuda.FloatTensor)\n\n        #Compute actual margin postive and margin negative loss\n        pos_loss = torch.nn.functional.relu(d_ap-beta+self.margin)\n        neg_loss = torch.nn.functional.relu(beta-d_an+self.margin)\n\n        #Compute normalization constant\n        pair_count = torch.sum((pos_loss>0.)+(neg_loss>0.)).type(torch.cuda.FloatTensor)\n\n        #Actual Margin Loss\n        loss = torch.sum(pos_loss+neg_loss) if pair_count==0. else torch.sum(pos_loss+neg_loss)/pair_count\n\n        #(Optional) Add regularization penalty on betas.\n        if self.nu: loss = loss + beta_regularisation_loss.type(torch.cuda.FloatTensor)\n\n        return loss\n\n\n\n\n\n""""""=================================================================================================""""""\n### ProxyNCALoss containing trainable class proxies. Works independent of batch size.\nclass ProxyNCALoss(torch.nn.Module):\n    def __init__(self, num_proxies, embedding_dim):\n        """"""\n        Basic ProxyNCA Loss as proposed in \'No Fuss Distance Metric Learning using Proxies\'.\n\n        Args:\n            num_proxies:     int, number of proxies to use to estimate data groups. Usually set to number of classes.\n            embedding_dim:   int, Required to generate initial proxies which are the same size as the actual data embeddings.\n        Returns:\n            Nothing!\n        """"""\n        super(ProxyNCALoss, self).__init__()\n        self.num_proxies   = num_proxies\n        self.embedding_dim = embedding_dim\n        self.PROXIES = torch.nn.Parameter(torch.randn(num_proxies, self.embedding_dim) / 8)\n        self.all_classes = torch.arange(num_proxies)\n\n\n    def forward(self, batch, labels):\n        """"""\n        Args:\n            batch:   torch.Tensor() [(BS x embed_dim)], batch of embeddings\n            labels:  np.ndarray [(BS x 1)], for each element of the batch assigns a class [0,...,C-1]\n        Returns:\n            proxynca loss (torch.Tensor(), batch-averaged)\n        """"""\n        #Normalize batch in case it is not normalized (which should never be the case for ProxyNCA, but still).\n        #Same for the PROXIES. Note that the multiplication by 3 seems arbitrary, but helps the actual training.\n        batch       = 3*torch.nn.functional.normalize(batch, dim=1)\n        PROXIES     = 3*torch.nn.functional.normalize(self.PROXIES, dim=1)\n        #Group required proxies\n        pos_proxies = torch.stack([PROXIES[pos_label:pos_label+1,:] for pos_label in labels])\n        neg_proxies = torch.stack([torch.cat([self.all_classes[:class_label],self.all_classes[class_label+1:]]) for class_label in labels])\n        neg_proxies = torch.stack([PROXIES[neg_labels,:] for neg_labels in neg_proxies])\n        #Compute Proxy-distances\n        dist_to_neg_proxies = torch.sum((batch[:,None,:]-neg_proxies).pow(2),dim=-1)\n        dist_to_pos_proxies = torch.sum((batch[:,None,:]-pos_proxies).pow(2),dim=-1)\n        #Compute final proxy-based NCA loss\n        negative_log_proxy_nca_loss = torch.mean(dist_to_pos_proxies[:,0] + torch.logsumexp(-dist_to_neg_proxies, dim=1))\n        return negative_log_proxy_nca_loss\n\n\n\n\n\n""""""=================================================================================================""""""\nclass CEClassLoss(torch.nn.Module):\n    def __init__(self, inp_dim, n_classes):\n        """"""\n        Basic Cross Entropy Loss for reference. Can be useful.\n        Contains its own mapping network, so the actual network can remain untouched.\n\n        Args:\n            inp_dim:   int, embedding dimension of network.\n            n_classes: int, number of target classes.\n        Returns:\n            Nothing!\n        """"""\n        super(CEClassLoss, self).__init__()\n        self.mapper  = torch.nn.Sequential(torch.nn.Linear(inp_dim, n_classes))\n        self.ce_loss = torch.nn.CrossEntropyLoss()\n\n    def forward(self, batch, labels):\n        """"""\n        Args:\n            batch:   torch.Tensor() [(BS x embed_dim)], batch of embeddings\n            labels:  np.ndarray [(BS x 1)], for each element of the batch assigns a class [0,...,C-1]\n        Returns:\n            cross-entropy loss (torch.Tensor(), batch-averaged by default)\n        """"""\n        return self.ce_loss(self.mapper(batch), labels.type(torch.cuda.LongTensor))\n'"
netlib.py,5,"b'# Copyright 2019 Karsten Roth and Biagio Brattoli\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\n############################ LIBRARIES ######################################\nimport torch, os, numpy as np\n\nimport torch.nn as nn\nimport pretrainedmodels as ptm\n\nimport pretrainedmodels.utils as utils\nimport torchvision.models as models\n\nimport googlenet\n\n\n\n""""""=============================================================""""""\ndef initialize_weights(model):\n    """"""\n    Function to initialize network weights.\n    NOTE: NOT USED IN MAIN SCRIPT.\n\n    Args:\n        model: PyTorch Network\n    Returns:\n        Nothing!\n    """"""\n    for idx,module in enumerate(model.modules()):\n        if isinstance(module, nn.Conv2d):\n            nn.init.kaiming_normal_(module.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n        elif isinstance(module, nn.BatchNorm2d):\n            nn.init.constant_(module.weight, 1)\n            nn.init.constant_(module.bias, 0)\n        elif isinstance(module, nn.Linear):\n            module.weight.data.normal_(0,0.01)\n            module.bias.data.zero_()\n\n\n\n""""""==================================================================================================================================""""""\n### ATTRIBUTE CHANGE HELPER\ndef rename_attr(model, attr, name):\n    """"""\n    Rename attribute in a class. Simply helper function.\n\n    Args:\n        model:  General Class for which attributes should be renamed.\n        attr:   str, Name of target attribute.\n        name:   str, New attribute name.\n    """"""\n    setattr(model, name, getattr(model, attr))\n    delattr(model, attr)\n\n\n""""""==================================================================================================================================""""""\n### NETWORK SELECTION FUNCTION\ndef networkselect(opt):\n    """"""\n    Selection function for available networks.\n\n    Args:\n        opt: argparse.Namespace, contains all training-specific training parameters.\n    Returns:\n        Network of choice\n    """"""\n    if opt.arch == \'googlenet\':\n        network =  GoogLeNet(opt)\n    elif opt.arch == \'resnet50\':\n        network =  ResNet50(opt)\n    else:\n        raise Exception(\'Network {} not available!\'.format(opt.arch))\n    return network\n\n\n\n\n""""""==================================================================================================================================""""""\nclass GoogLeNet(nn.Module):\n    """"""\n    Container for GoogLeNet s.t. it can be used for metric learning.\n    The Network has been broken down to allow for higher modularity, if one wishes\n    to target specific layers/blocks directly.\n    """"""\n    def __init__(self, opt):\n        """"""\n        Args:\n            opt: argparse.Namespace, contains all training-specific parameters.\n        Returns:\n            Nothing!\n        """"""\n        super(GoogLeNet, self).__init__()\n\n        self.pars       = opt\n\n        self.model = googlenet.googlenet(num_classes=1000, pretrained=\'imagenet\' if not opt.not_pretrained else False)\n\n        for module in filter(lambda m: type(m) == nn.BatchNorm2d, self.model.modules()):\n            module.eval()\n            module.train = lambda _: None\n\n        rename_attr(self.model, \'fc\', \'last_linear\')\n\n        self.layer_blocks = nn.ModuleList([self.model.inception3a, self.model.inception3b, self.model.maxpool3,\n                                           self.model.inception4a, self.model.inception4b, self.model.inception4c,\n                                           self.model.inception4d, self.model.inception4e, self.model.maxpool4,\n                                           self.model.inception5a, self.model.inception5b, self.model.avgpool])\n\n        self.model.last_linear = torch.nn.Linear(self.model.last_linear.in_features, opt.embed_dim)\n\n\n    def forward(self, x):\n        ### Initial Conv Layers\n        x = self.model.conv3(self.model.conv2(self.model.maxpool1(self.model.conv1(x))))\n        x = self.model.maxpool2(x)\n\n        ### Inception Blocks\n        for layerblock in self.layer_blocks:\n            x = layerblock(x)\n\n        x = x.view(x.size(0), -1)\n        x = self.model.dropout(x)\n\n        mod_x = self.model.last_linear(x)\n\n        #No Normalization is used if N-Pair Loss is the target criterion.\n        return mod_x if self.pars.loss==\'npair\' else torch.nn.functional.normalize(mod_x, dim=-1)\n\n\n\n""""""=============================================================""""""\nclass ResNet50(nn.Module):\n    """"""\n    Container for ResNet50 s.t. it can be used for metric learning.\n    The Network has been broken down to allow for higher modularity, if one wishes\n    to target specific layers/blocks directly.\n    """"""\n    def __init__(self, opt, list_style=False, no_norm=False):\n        super(ResNet50, self).__init__()\n\n        self.pars = opt\n\n        if not opt.not_pretrained:\n            print(\'Getting pretrained weights...\')\n            self.model = ptm.__dict__[\'resnet50\'](num_classes=1000, pretrained=\'imagenet\')\n            print(\'Done.\')\n        else:\n            print(\'Not utilizing pretrained weights!\')\n            self.model = ptm.__dict__[\'resnet50\'](num_classes=1000, pretrained=None)\n\n        for module in filter(lambda m: type(m) == nn.BatchNorm2d, self.model.modules()):\n            module.eval()\n            module.train = lambda _: None\n\n        self.model.last_linear = torch.nn.Linear(self.model.last_linear.in_features, opt.embed_dim)\n\n        self.layer_blocks = nn.ModuleList([self.model.layer1, self.model.layer2, self.model.layer3, self.model.layer4])\n\n    def forward(self, x, is_init_cluster_generation=False):\n        x = self.model.maxpool(self.model.relu(self.model.bn1(self.model.conv1(x))))\n\n        for layerblock in self.layer_blocks:\n            x = layerblock(x)\n\n        x = self.model.avgpool(x)\n        x = x.view(x.size(0),-1)\n\n        mod_x = self.model.last_linear(x)\n        #No Normalization is used if N-Pair Loss is the target criterion.\n        return mod_x if self.pars.loss==\'npair\' else torch.nn.functional.normalize(mod_x, dim=-1)\n'"
