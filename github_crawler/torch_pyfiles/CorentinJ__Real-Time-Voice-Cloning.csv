file_path,api_count,code
demo_cli.py,4,"b'from encoder.params_model import model_embedding_size as speaker_embedding_size\nfrom utils.argutils import print_args\nfrom synthesizer.inference import Synthesizer\nfrom encoder import inference as encoder\nfrom vocoder import inference as vocoder\nfrom pathlib import Path\nimport numpy as np\nimport librosa\nimport argparse\nimport torch\nimport sys\n\n\nif __name__ == \'__main__\':\n    ## Info & args\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(""-e"", ""--enc_model_fpath"", type=Path, \n                        default=""encoder/saved_models/pretrained.pt"",\n                        help=""Path to a saved encoder"")\n    parser.add_argument(""-s"", ""--syn_model_dir"", type=Path, \n                        default=""synthesizer/saved_models/logs-pretrained/"",\n                        help=""Directory containing the synthesizer model"")\n    parser.add_argument(""-v"", ""--voc_model_fpath"", type=Path, \n                        default=""vocoder/saved_models/pretrained/pretrained.pt"",\n                        help=""Path to a saved vocoder"")\n    parser.add_argument(""--low_mem"", action=""store_true"", help=\\\n        ""If True, the memory used by the synthesizer will be freed after each use. Adds large ""\n        ""overhead but allows to save some GPU memory for lower-end GPUs."")\n    parser.add_argument(""--no_sound"", action=""store_true"", help=\\\n        ""If True, audio won\'t be played."")\n    args = parser.parse_args()\n    print_args(args, parser)\n    if not args.no_sound:\n        import sounddevice as sd\n        \n    \n    ## Print some environment information (for debugging purposes)\n    print(""Running a test of your configuration...\\n"")\n    if not torch.cuda.is_available():\n        print(""Your PyTorch installation is not configured to use CUDA. If you have a GPU ready ""\n              ""for deep learning, ensure that the drivers are properly installed, and that your ""\n              ""CUDA version matches your PyTorch installation. CPU-only inference is currently ""\n              ""not supported."", file=sys.stderr)\n        quit(-1)\n    device_id = torch.cuda.current_device()\n    gpu_properties = torch.cuda.get_device_properties(device_id)\n    print(""Found %d GPUs available. Using GPU %d (%s) of compute capability %d.%d with ""\n          ""%.1fGb total memory.\\n"" % \n          (torch.cuda.device_count(),\n           device_id,\n           gpu_properties.name,\n           gpu_properties.major,\n           gpu_properties.minor,\n           gpu_properties.total_memory / 1e9))\n    \n    \n    ## Load the models one by one.\n    print(""Preparing the encoder, the synthesizer and the vocoder..."")\n    encoder.load_model(args.enc_model_fpath)\n    synthesizer = Synthesizer(args.syn_model_dir.joinpath(""taco_pretrained""), low_mem=args.low_mem)\n    vocoder.load_model(args.voc_model_fpath)\n    \n    \n    ## Run a test\n    print(""Testing your configuration with small inputs."")\n    # Forward an audio waveform of zeroes that lasts 1 second. Notice how we can get the encoder\'s\n    # sampling rate, which may differ.\n    # If you\'re unfamiliar with digital audio, know that it is encoded as an array of floats \n    # (or sometimes integers, but mostly floats in this projects) ranging from -1 to 1.\n    # The sampling rate is the number of values (samples) recorded per second, it is set to\n    # 16000 for the encoder. Creating an array of length <sampling_rate> will always correspond \n    # to an audio of 1 second.\n    print(""\\tTesting the encoder..."")\n    encoder.embed_utterance(np.zeros(encoder.sampling_rate))\n    \n    # Create a dummy embedding. You would normally use the embedding that encoder.embed_utterance\n    # returns, but here we\'re going to make one ourselves just for the sake of showing that it\'s\n    # possible.\n    embed = np.random.rand(speaker_embedding_size)\n    # Embeddings are L2-normalized (this isn\'t important here, but if you want to make your own \n    # embeddings it will be).\n    embed /= np.linalg.norm(embed)\n    # The synthesizer can handle multiple inputs with batching. Let\'s create another embedding to \n    # illustrate that\n    embeds = [embed, np.zeros(speaker_embedding_size)]\n    texts = [""test 1"", ""test 2""]\n    print(""\\tTesting the synthesizer... (loading the model will output a lot of text)"")\n    mels = synthesizer.synthesize_spectrograms(texts, embeds)\n    \n    # The vocoder synthesizes one waveform at a time, but it\'s more efficient for long ones. We \n    # can concatenate the mel spectrograms to a single one.\n    mel = np.concatenate(mels, axis=1)\n    # The vocoder can take a callback function to display the generation. More on that later. For \n    # now we\'ll simply hide it like this:\n    no_action = lambda *args: None\n    print(""\\tTesting the vocoder..."")\n    # For the sake of making this test short, we\'ll pass a short target length. The target length \n    # is the length of the wav segments that are processed in parallel. E.g. for audio sampled \n    # at 16000 Hertz, a target length of 8000 means that the target audio will be cut in chunks of\n    # 0.5 seconds which will all be generated together. The parameters here are absurdly short, and \n    # that has a detrimental effect on the quality of the audio. The default parameters are \n    # recommended in general.\n    vocoder.infer_waveform(mel, target=200, overlap=50, progress_callback=no_action)\n    \n    print(""All test passed! You can now synthesize speech.\\n\\n"")\n    \n    \n    ## Interactive speech generation\n    print(""This is a GUI-less example of interface to SV2TTS. The purpose of this script is to ""\n          ""show how you can interface this project easily with your own. See the source code for ""\n          ""an explanation of what is happening.\\n"")\n    \n    print(""Interactive generation loop"")\n    num_generated = 0\n    while True:\n        try:\n            # Get the reference audio filepath\n            message = ""Reference voice: enter an audio filepath of a voice to be cloned (mp3, "" \\\n                      ""wav, m4a, flac, ...):\\n""\n            in_fpath = Path(input(message).replace(""\\"""", """").replace(""\\\'"", """"))\n            \n            \n            ## Computing the embedding\n            # First, we load the wav using the function that the speaker encoder provides. This is \n            # important: there is preprocessing that must be applied.\n            \n            # The following two methods are equivalent:\n            # - Directly load from the filepath:\n            preprocessed_wav = encoder.preprocess_wav(in_fpath)\n            # - If the wav is already loaded:\n            original_wav, sampling_rate = librosa.load(in_fpath)\n            preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)\n            print(""Loaded file succesfully"")\n            \n            # Then we derive the embedding. There are many functions and parameters that the \n            # speaker encoder interfaces. These are mostly for in-depth research. You will typically\n            # only use this function (with its default parameters):\n            embed = encoder.embed_utterance(preprocessed_wav)\n            print(""Created the embedding"")\n            \n            \n            ## Generating the spectrogram\n            text = input(""Write a sentence (+-20 words) to be synthesized:\\n"")\n            \n            # The synthesizer works in batch, so you need to put your data in a list or numpy array\n            texts = [text]\n            embeds = [embed]\n            # If you know what the attention layer alignments are, you can retrieve them here by\n            # passing return_alignments=True\n            specs = synthesizer.synthesize_spectrograms(texts, embeds)\n            spec = specs[0]\n            print(""Created the mel spectrogram"")\n            \n            \n            ## Generating the waveform\n            print(""Synthesizing the waveform:"")\n            # Synthesizing the waveform is fairly straightforward. Remember that the longer the\n            # spectrogram, the more time-efficient the vocoder.\n            generated_wav = vocoder.infer_waveform(spec)\n            \n            \n            ## Post-generation\n            # There\'s a bug with sounddevice that makes the audio cut one second earlier, so we\n            # pad it.\n            generated_wav = np.pad(generated_wav, (0, synthesizer.sample_rate), mode=""constant"")\n            \n            # Play the audio (non-blocking)\n            if not args.no_sound:\n                sd.stop()\n                sd.play(generated_wav, synthesizer.sample_rate)\n                \n            # Save it on the disk\n            fpath = ""demo_output_%02d.wav"" % num_generated\n            print(generated_wav.dtype)\n            librosa.output.write_wav(fpath, generated_wav.astype(np.float32), \n                                     synthesizer.sample_rate)\n            num_generated += 1\n            print(""\\nSaved output as %s\\n\\n"" % fpath)\n            \n            \n        except Exception as e:\n            print(""Caught exception: %s"" % repr(e))\n            print(""Restarting\\n"")\n        '"
demo_toolbox.py,0,"b'from pathlib import Path\nfrom toolbox import Toolbox\nfrom utils.argutils import print_args\nimport argparse\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n        description=""Runs the toolbox"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    \n    parser.add_argument(""-d"", ""--datasets_root"", type=Path, help= \\\n        ""Path to the directory containing your datasets. See toolbox/__init__.py for a list of ""\n        ""supported datasets. You can add your own data by created a directory named UserAudio ""\n        ""in your datasets root. Supported formats are mp3, flac, wav and m4a. Each speaker should ""\n        ""be inside a directory, e.g. <datasets_root>/UserAudio/speaker_01/audio_01.wav."",\n                        default=None)\n    parser.add_argument(""-e"", ""--enc_models_dir"", type=Path, default=""encoder/saved_models"", \n                        help=""Directory containing saved encoder models"")\n    parser.add_argument(""-s"", ""--syn_models_dir"", type=Path, default=""synthesizer/saved_models"", \n                        help=""Directory containing saved synthesizer models"")\n    parser.add_argument(""-v"", ""--voc_models_dir"", type=Path, default=""vocoder/saved_models"", \n                        help=""Directory containing saved vocoder models"")\n    parser.add_argument(""--low_mem"", action=""store_true"", help=\\\n        ""If True, the memory used by the synthesizer will be freed after each use. Adds large ""\n        ""overhead but allows to save some GPU memory for lower-end GPUs."")\n    args = parser.parse_args()\n\n    # Launch the toolbox\n    print_args(args, parser)\n    Toolbox(**vars(args))\n    '"
encoder_preprocess.py,0,"b'from encoder.preprocess import preprocess_librispeech, preprocess_voxceleb1, preprocess_voxceleb2\nfrom utils.argutils import print_args\nfrom pathlib import Path\nimport argparse\n\n\nif __name__ == ""__main__"":\n    class MyFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawDescriptionHelpFormatter):\n        pass\n    \n    parser = argparse.ArgumentParser(\n        description=""Preprocesses audio files from datasets, encodes them as mel spectrograms and ""\n                    ""writes them to the disk. This will allow you to train the encoder. The ""\n                    ""datasets required are at least one of VoxCeleb1, VoxCeleb2 and LibriSpeech. ""\n                    ""Ideally, you should have all three. You should extract them as they are ""\n                    ""after having downloaded them and put them in a same directory, e.g.:\\n""\n                    ""-[datasets_root]\\n""\n                    ""  -LibriSpeech\\n""\n                    ""    -train-other-500\\n""\n                    ""  -VoxCeleb1\\n""\n                    ""    -wav\\n""\n                    ""    -vox1_meta.csv\\n""\n                    ""  -VoxCeleb2\\n""\n                    ""    -dev"",\n        formatter_class=MyFormatter\n    )\n    parser.add_argument(""datasets_root"", type=Path, help=\\\n        ""Path to the directory containing your LibriSpeech/TTS and VoxCeleb datasets."")\n    parser.add_argument(""-o"", ""--out_dir"", type=Path, default=argparse.SUPPRESS, help=\\\n        ""Path to the output directory that will contain the mel spectrograms. If left out, ""\n        ""defaults to <datasets_root>/SV2TTS/encoder/"")\n    parser.add_argument(""-d"", ""--datasets"", type=str, \n                        default=""librispeech_other,voxceleb1,voxceleb2"", help=\\\n        ""Comma-separated list of the name of the datasets you want to preprocess. Only the train ""\n        ""set of these datasets will be used. Possible names: librispeech_other, voxceleb1, ""\n        ""voxceleb2."")\n    parser.add_argument(""-s"", ""--skip_existing"", action=""store_true"", help=\\\n        ""Whether to skip existing output files with the same name. Useful if this script was ""\n        ""interrupted."")\n    args = parser.parse_args()\n\n    # Process the arguments\n    args.datasets = args.datasets.split("","")\n    if not hasattr(args, ""out_dir""):\n        args.out_dir = args.datasets_root.joinpath(""SV2TTS"", ""encoder"")\n    assert args.datasets_root.exists()\n    args.out_dir.mkdir(exist_ok=True, parents=True)\n    \n    # Preprocess the datasets\n    print_args(args, parser)\n    preprocess_func = {\n        ""librispeech_other"": preprocess_librispeech,\n        ""voxceleb1"": preprocess_voxceleb1,\n        ""voxceleb2"": preprocess_voxceleb2,\n    }\n    args = vars(args)\n    for dataset in args.pop(""datasets""):\n        print(""Preprocessing %s"" % dataset)\n        preprocess_func[dataset](**args)\n'"
encoder_train.py,0,"b'from utils.argutils import print_args\nfrom encoder.train import train\nfrom pathlib import Path\nimport argparse\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(\n        description=""Trains the speaker encoder. You must have run encoder_preprocess.py first."",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    \n    parser.add_argument(""run_id"", type=str, help= \\\n        ""Name for this model instance. If a model state from the same run ID was previously ""\n        ""saved, the training will restart from there. Pass -f to overwrite saved states and ""\n        ""restart from scratch."")\n    parser.add_argument(""clean_data_root"", type=Path, help= \\\n        ""Path to the output directory of encoder_preprocess.py. If you left the default ""\n        ""output directory when preprocessing, it should be <datasets_root>/SV2TTS/encoder/."")\n    parser.add_argument(""-m"", ""--models_dir"", type=Path, default=""encoder/saved_models/"", help=\\\n        ""Path to the output directory that will contain the saved model weights, as well as ""\n        ""backups of those weights and plots generated during training."")\n    parser.add_argument(""-v"", ""--vis_every"", type=int, default=10, help= \\\n        ""Number of steps between updates of the loss and the plots."")\n    parser.add_argument(""-u"", ""--umap_every"", type=int, default=100, help= \\\n        ""Number of steps between updates of the umap projection. Set to 0 to never update the ""\n        ""projections."")\n    parser.add_argument(""-s"", ""--save_every"", type=int, default=500, help= \\\n        ""Number of steps between updates of the model on the disk. Set to 0 to never save the ""\n        ""model."")\n    parser.add_argument(""-b"", ""--backup_every"", type=int, default=7500, help= \\\n        ""Number of steps between backups of the model. Set to 0 to never make backups of the ""\n        ""model."")\n    parser.add_argument(""-f"", ""--force_restart"", action=""store_true"", help= \\\n        ""Do not load any saved model."")\n    parser.add_argument(""--visdom_server"", type=str, default=""http://localhost"")\n    parser.add_argument(""--no_visdom"", action=""store_true"", help= \\\n        ""Disable visdom."")\n    args = parser.parse_args()\n    \n    # Process the arguments\n    args.models_dir.mkdir(exist_ok=True)\n    \n    # Run the training\n    print_args(args, parser)\n    train(**vars(args))\n    '"
synthesizer_preprocess_audio.py,0,"b'from synthesizer.preprocess import preprocess_librispeech\nfrom synthesizer.hparams import hparams\nfrom utils.argutils import print_args\nfrom pathlib import Path\nimport argparse\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(\n        description=""Preprocesses audio files from datasets, encodes them as mel spectrograms ""\n                    ""and writes them to  the disk. Audio files are also saved, to be used by the ""\n                    ""vocoder for training."",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(""datasets_root"", type=Path, help=\\\n        ""Path to the directory containing your LibriSpeech/TTS datasets."")\n    parser.add_argument(""-o"", ""--out_dir"", type=Path, default=argparse.SUPPRESS, help=\\\n        ""Path to the output directory that will contain the mel spectrograms, the audios and the ""\n        ""embeds. Defaults to <datasets_root>/SV2TTS/synthesizer/"")\n    parser.add_argument(""-n"", ""--n_processes"", type=int, default=None, help=\\\n        ""Number of processes in parallel."")\n    parser.add_argument(""-s"", ""--skip_existing"", action=""store_true"", help=\\\n        ""Whether to overwrite existing files with the same name. Useful if the preprocessing was ""\n        ""interrupted."")\n    parser.add_argument(""--hparams"", type=str, default="""", help=\\\n        ""Hyperparameter overrides as a comma-separated list of name-value pairs"")\n    args = parser.parse_args()\n    \n    # Process the arguments\n    if not hasattr(args, ""out_dir""):\n        args.out_dir = args.datasets_root.joinpath(""SV2TTS"", ""synthesizer"")\n\n    # Create directories\n    assert args.datasets_root.exists()\n    args.out_dir.mkdir(exist_ok=True, parents=True)\n\n    # Preprocess the dataset\n    print_args(args, parser)\n    args.hparams = hparams.parse(args.hparams)\n    preprocess_librispeech(**vars(args))    \n'"
synthesizer_preprocess_embeds.py,0,"b'from synthesizer.preprocess import create_embeddings\nfrom utils.argutils import print_args\nfrom pathlib import Path\nimport argparse\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(\n        description=""Creates embeddings for the synthesizer from the LibriSpeech utterances."",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(""synthesizer_root"", type=Path, help=\\\n        ""Path to the synthesizer training data that contains the audios and the train.txt file. ""\n        ""If you let everything as default, it should be <datasets_root>/SV2TTS/synthesizer/."")\n    parser.add_argument(""-e"", ""--encoder_model_fpath"", type=Path, \n                        default=""encoder/saved_models/pretrained.pt"", help=\\\n        ""Path your trained encoder model."")\n    parser.add_argument(""-n"", ""--n_processes"", type=int, default=4, help= \\\n        ""Number of parallel processes. An encoder is created for each, so you may need to lower ""\n        ""this value on GPUs with low memory. Set it to 1 if CUDA is unhappy."")\n    args = parser.parse_args()\n    \n    # Preprocess the dataset\n    print_args(args, parser)\n    create_embeddings(**vars(args))    \n'"
synthesizer_train.py,0,"b'from synthesizer.hparams import hparams\nfrom synthesizer.train import tacotron_train\nfrom utils.argutils import print_args\nfrom synthesizer import infolog\nimport argparse\nimport os\n\n\ndef prepare_run(args):\n    modified_hp = hparams.parse(args.hparams)\n    os.environ[""TF_CPP_MIN_LOG_LEVEL""] = str(args.tf_log_level)\n    run_name = args.name\n    log_dir = os.path.join(args.models_dir, ""logs-{}"".format(run_name))\n    os.makedirs(log_dir, exist_ok=True)\n    infolog.init(os.path.join(log_dir, ""Terminal_train_log""), run_name, args.slack_url)\n    return log_dir, modified_hp\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""name"", help=""Name of the run and of the logging directory."")\n    parser.add_argument(""synthesizer_root"", type=str, help=\\\n        ""Path to the synthesizer training data that contains the audios and the train.txt file. ""\n        ""If you let everything as default, it should be <datasets_root>/SV2TTS/synthesizer/."")\n    parser.add_argument(""-m"", ""--models_dir"", type=str, default=""synthesizer/saved_models/"", help=\\\n        ""Path to the output directory that will contain the saved model weights and the logs."")\n    parser.add_argument(""--mode"", default=""synthesis"",\n                        help=""mode for synthesis of tacotron after training"")\n    parser.add_argument(""--GTA"", default=""True"",\n                        help=""Ground truth aligned synthesis, defaults to True, only considered ""\n\t\t\t\t\t\t\t ""in Tacotron synthesis mode"")\n    parser.add_argument(""--restore"", type=bool, default=True,\n                        help=""Set this to False to do a fresh training"")\n    parser.add_argument(""--summary_interval"", type=int, default=2500,\n                        help=""Steps between running summary ops"")\n    parser.add_argument(""--embedding_interval"", type=int, default=10000,\n                        help=""Steps between updating embeddings projection visualization"")\n    parser.add_argument(""--checkpoint_interval"", type=int, default=2000, # Was 5000\n                        help=""Steps between writing checkpoints"")\n    parser.add_argument(""--eval_interval"", type=int, default=100000, # Was 10000\n                        help=""Steps between eval on test data"")\n    parser.add_argument(""--tacotron_train_steps"", type=int, default=2000000, # Was 100000\n                        help=""total number of tacotron training steps"")\n    parser.add_argument(""--tf_log_level"", type=int, default=1, help=""Tensorflow C++ log level."")\n    parser.add_argument(""--slack_url"", default=None,\n                        help=""slack webhook notification destination link"")\n    parser.add_argument(""--hparams"", default="""",\n                        help=""Hyperparameter overrides as a comma-separated list of name=value ""\n\t\t\t\t\t\t\t ""pairs"")\n    args = parser.parse_args()\n    print_args(args, parser)\n    \n    log_dir, hparams = prepare_run(args)\n    \n    tacotron_train(args, log_dir, hparams)\n'"
vocoder_preprocess.py,0,"b'from synthesizer.synthesize import run_synthesis\nfrom synthesizer.hparams import hparams\nfrom utils.argutils import print_args\nimport argparse\nimport os\n\n\nif __name__ == ""__main__"":\n    class MyFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawDescriptionHelpFormatter):\n        pass\n    \n    parser = argparse.ArgumentParser(\n        description=""Creates ground-truth aligned (GTA) spectrograms from the vocoder."",\n        formatter_class=MyFormatter\n    )\n    parser.add_argument(""datasets_root"", type=str, help=\\\n        ""Path to the directory containing your SV2TTS directory. If you specify both --in_dir and ""\n        ""--out_dir, this argument won\'t be used."")\n    parser.add_argument(""--model_dir"", type=str, \n                        default=""synthesizer/saved_models/logs-pretrained/"", help=\\\n        ""Path to the pretrained model directory."")\n    parser.add_argument(""-i"", ""--in_dir"", type=str, default=argparse.SUPPRESS, help= \\\n        ""Path to the synthesizer directory that contains the mel spectrograms, the wavs and the ""\n        ""embeds. Defaults to  <datasets_root>/SV2TTS/synthesizer/."")\n    parser.add_argument(""-o"", ""--out_dir"", type=str, default=argparse.SUPPRESS, help= \\\n        ""Path to the output vocoder directory that will contain the ground truth aligned mel ""\n        ""spectrograms. Defaults to <datasets_root>/SV2TTS/vocoder/."")\n    parser.add_argument(""--hparams"", default="""",\n                        help=""Hyperparameter overrides as a comma-separated list of name=value ""\n                             ""pairs"")\n    args = parser.parse_args()\n    print_args(args, parser)\n    modified_hp = hparams.parse(args.hparams)\n    \n    if not hasattr(args, ""in_dir""):\n        args.in_dir = os.path.join(args.datasets_root, ""SV2TTS"", ""synthesizer"")\n    if not hasattr(args, ""out_dir""):\n        args.out_dir = os.path.join(args.datasets_root, ""SV2TTS"", ""vocoder"")\n    \n    run_synthesis(args.in_dir, args.out_dir, args.model_dir, modified_hp)\n    '"
vocoder_train.py,0,"b'from utils.argutils import print_args\nfrom vocoder.train import train\nfrom pathlib import Path\nimport argparse\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(\n        description=""Trains the vocoder from the synthesizer audios and the GTA synthesized mels, ""\n                    ""or ground truth mels."",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    \n    parser.add_argument(""run_id"", type=str, help= \\\n        ""Name for this model instance. If a model state from the same run ID was previously ""\n        ""saved, the training will restart from there. Pass -f to overwrite saved states and ""\n        ""restart from scratch."")\n    parser.add_argument(""datasets_root"", type=str, help= \\\n        ""Path to the directory containing your SV2TTS directory. Specifying --syn_dir or --voc_dir ""\n        ""will take priority over this argument."")\n    parser.add_argument(""--syn_dir"", type=str, default=argparse.SUPPRESS, help= \\\n        ""Path to the synthesizer directory that contains the ground truth mel spectrograms, ""\n        ""the wavs and the embeds. Defaults to <datasets_root>/SV2TTS/synthesizer/."")\n    parser.add_argument(""--voc_dir"", type=str, default=argparse.SUPPRESS, help= \\\n        ""Path to the vocoder directory that contains the GTA synthesized mel spectrograms. ""\n        ""Defaults to <datasets_root>/SV2TTS/vocoder/. Unused if --ground_truth is passed."")\n    parser.add_argument(""-m"", ""--models_dir"", type=str, default=""vocoder/saved_models/"", help=\\\n        ""Path to the directory that will contain the saved model weights, as well as backups ""\n        ""of those weights and wavs generated during training."")\n    parser.add_argument(""-g"", ""--ground_truth"", action=""store_true"", help= \\\n        ""Train on ground truth spectrograms (<datasets_root>/SV2TTS/synthesizer/mels)."")\n    parser.add_argument(""-s"", ""--save_every"", type=int, default=1000, help= \\\n        ""Number of steps between updates of the model on the disk. Set to 0 to never save the ""\n        ""model."")\n    parser.add_argument(""-b"", ""--backup_every"", type=int, default=25000, help= \\\n        ""Number of steps between backups of the model. Set to 0 to never make backups of the ""\n        ""model."")\n    parser.add_argument(""-f"", ""--force_restart"", action=""store_true"", help= \\\n        ""Do not load any saved model and restart from scratch."")\n    args = parser.parse_args()\n\n    # Process the arguments\n    if not hasattr(args, ""syn_dir""):\n        args.syn_dir = Path(args.datasets_root, ""SV2TTS"", ""synthesizer"")\n    args.syn_dir = Path(args.syn_dir)\n    if not hasattr(args, ""voc_dir""):\n        args.voc_dir = Path(args.datasets_root, ""SV2TTS"", ""vocoder"")\n    args.voc_dir = Path(args.voc_dir)\n    del args.datasets_root\n    args.models_dir = Path(args.models_dir)\n    args.models_dir.mkdir(exist_ok=True)\n\n    # Run the training\n    print_args(args, parser)\n    train(**vars(args))\n    '"
encoder/__init__.py,0,b''
encoder/audio.py,0,"b'from scipy.ndimage.morphology import binary_dilation\nfrom encoder.params_data import *\nfrom pathlib import Path\nfrom typing import Optional, Union\nimport numpy as np\nimport webrtcvad\nimport librosa\nimport struct\n\nint16_max = (2 ** 15) - 1\n\n\ndef preprocess_wav(fpath_or_wav: Union[str, Path, np.ndarray],\n                   source_sr: Optional[int] = None):\n    """"""\n    Applies the preprocessing operations used in training the Speaker Encoder to a waveform \n    either on disk or in memory. The waveform will be resampled to match the data hyperparameters.\n\n    :param fpath_or_wav: either a filepath to an audio file (many extensions are supported, not \n    just .wav), either the waveform as a numpy array of floats.\n    :param source_sr: if passing an audio waveform, the sampling rate of the waveform before \n    preprocessing. After preprocessing, the waveform\'s sampling rate will match the data \n    hyperparameters. If passing a filepath, the sampling rate will be automatically detected and \n    this argument will be ignored.\n    """"""\n    # Load the wav from disk if needed\n    if isinstance(fpath_or_wav, str) or isinstance(fpath_or_wav, Path):\n        wav, source_sr = librosa.load(fpath_or_wav, sr=None)\n    else:\n        wav = fpath_or_wav\n    \n    # Resample the wav if needed\n    if source_sr is not None and source_sr != sampling_rate:\n        wav = librosa.resample(wav, source_sr, sampling_rate)\n        \n    # Apply the preprocessing: normalize volume and shorten long silences \n    wav = normalize_volume(wav, audio_norm_target_dBFS, increase_only=True)\n    wav = trim_long_silences(wav)\n    \n    return wav\n\n\ndef wav_to_mel_spectrogram(wav):\n    """"""\n    Derives a mel spectrogram ready to be used by the encoder from a preprocessed audio waveform.\n    Note: this not a log-mel spectrogram.\n    """"""\n    frames = librosa.feature.melspectrogram(\n        wav,\n        sampling_rate,\n        n_fft=int(sampling_rate * mel_window_length / 1000),\n        hop_length=int(sampling_rate * mel_window_step / 1000),\n        n_mels=mel_n_channels\n    )\n    return frames.astype(np.float32).T\n\n\ndef trim_long_silences(wav):\n    """"""\n    Ensures that segments without voice in the waveform remain no longer than a \n    threshold determined by the VAD parameters in params.py.\n\n    :param wav: the raw waveform as a numpy array of floats \n    :return: the same waveform with silences trimmed away (length <= original wav length)\n    """"""\n    # Compute the voice detection window size\n    samples_per_window = (vad_window_length * sampling_rate) // 1000\n    \n    # Trim the end of the audio to have a multiple of the window size\n    wav = wav[:len(wav) - (len(wav) % samples_per_window)]\n    \n    # Convert the float waveform to 16-bit mono PCM\n    pcm_wave = struct.pack(""%dh"" % len(wav), *(np.round(wav * int16_max)).astype(np.int16))\n    \n    # Perform voice activation detection\n    voice_flags = []\n    vad = webrtcvad.Vad(mode=3)\n    for window_start in range(0, len(wav), samples_per_window):\n        window_end = window_start + samples_per_window\n        voice_flags.append(vad.is_speech(pcm_wave[window_start * 2:window_end * 2],\n                                         sample_rate=sampling_rate))\n    voice_flags = np.array(voice_flags)\n    \n    # Smooth the voice detection with a moving average\n    def moving_average(array, width):\n        array_padded = np.concatenate((np.zeros((width - 1) // 2), array, np.zeros(width // 2)))\n        ret = np.cumsum(array_padded, dtype=float)\n        ret[width:] = ret[width:] - ret[:-width]\n        return ret[width - 1:] / width\n    \n    audio_mask = moving_average(voice_flags, vad_moving_average_width)\n    audio_mask = np.round(audio_mask).astype(np.bool)\n    \n    # Dilate the voiced regions\n    audio_mask = binary_dilation(audio_mask, np.ones(vad_max_silence_length + 1))\n    audio_mask = np.repeat(audio_mask, samples_per_window)\n    \n    return wav[audio_mask == True]\n\n\ndef normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False):\n    if increase_only and decrease_only:\n        raise ValueError(""Both increase only and decrease only are set"")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n'"
encoder/config.py,0,"b'librispeech_datasets = {\n    ""train"": {\n        ""clean"": [""LibriSpeech/train-clean-100"", ""LibriSpeech/train-clean-360""],\n        ""other"": [""LibriSpeech/train-other-500""]\n    },\n    ""test"": {\n        ""clean"": [""LibriSpeech/test-clean""],\n        ""other"": [""LibriSpeech/test-other""]\n    },\n    ""dev"": {\n        ""clean"": [""LibriSpeech/dev-clean""],\n        ""other"": [""LibriSpeech/dev-other""]\n    },\n}\nlibritts_datasets = {\n    ""train"": {\n        ""clean"": [""LibriTTS/train-clean-100"", ""LibriTTS/train-clean-360""],\n        ""other"": [""LibriTTS/train-other-500""]\n    },\n    ""test"": {\n        ""clean"": [""LibriTTS/test-clean""],\n        ""other"": [""LibriTTS/test-other""]\n    },\n    ""dev"": {\n        ""clean"": [""LibriTTS/dev-clean""],\n        ""other"": [""LibriTTS/dev-other""]\n    },\n}\nvoxceleb_datasets = {\n    ""voxceleb1"" : {\n        ""train"": [""VoxCeleb1/wav""],\n        ""test"": [""VoxCeleb1/test_wav""]\n    },\n    ""voxceleb2"" : {\n        ""train"": [""VoxCeleb2/dev/aac""],\n        ""test"": [""VoxCeleb2/test_wav""]\n    }\n}\n\nother_datasets = [\n    ""LJSpeech-1.1"",\n    ""VCTK-Corpus/wav48"",\n]\n\nanglophone_nationalites = [""australia"", ""canada"", ""ireland"", ""uk"", ""usa""]\n'"
encoder/inference.py,6,"b'from encoder.params_data import *\nfrom encoder.model import SpeakerEncoder\nfrom encoder.audio import preprocess_wav   # We want to expose this function from here\nfrom matplotlib import cm\nfrom encoder import audio\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\n_model = None # type: SpeakerEncoder\n_device = None # type: torch.device\n\n\ndef load_model(weights_fpath: Path, device=None):\n    """"""\n    Loads the model in memory. If this function is not explicitely called, it will be run on the \n    first call to embed_frames() with the default weights file.\n    \n    :param weights_fpath: the path to saved model weights.\n    :param device: either a torch device or the name of a torch device (e.g. ""cpu"", ""cuda""). The \n    model will be loaded and will run on this device. Outputs will however always be on the cpu. \n    If None, will default to your GPU if it""s available, otherwise your CPU.\n    """"""\n    # TODO: I think the slow loading of the encoder might have something to do with the device it\n    #   was saved on. Worth investigating.\n    global _model, _device\n    if device is None:\n        _device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    elif isinstance(device, str):\n        _device = torch.device(device)\n    _model = SpeakerEncoder(_device, torch.device(""cpu""))\n    checkpoint = torch.load(weights_fpath)\n    _model.load_state_dict(checkpoint[""model_state""])\n    _model.eval()\n    print(""Loaded encoder \\""%s\\"" trained to step %d"" % (weights_fpath.name, checkpoint[""step""]))\n    \n    \ndef is_loaded():\n    return _model is not None\n\n\ndef embed_frames_batch(frames_batch):\n    """"""\n    Computes embeddings for a batch of mel spectrogram.\n    \n    :param frames_batch: a batch mel of spectrogram as a numpy array of float32 of shape \n    (batch_size, n_frames, n_channels)\n    :return: the embeddings as a numpy array of float32 of shape (batch_size, model_embedding_size)\n    """"""\n    if _model is None:\n        raise Exception(""Model was not loaded. Call load_model() before inference."")\n    \n    frames = torch.from_numpy(frames_batch).to(_device)\n    embed = _model.forward(frames).detach().cpu().numpy()\n    return embed\n\n\ndef compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5):\n    """"""\n    Computes where to split an utterance waveform and its corresponding mel spectrogram to obtain \n    partial utterances of <partial_utterance_n_frames> each. Both the waveform and the mel \n    spectrogram slices are returned, so as to make each partial utterance waveform correspond to \n    its spectrogram. This function assumes that the mel spectrogram parameters used are those \n    defined in params_data.py.\n    \n    The returned ranges may be indexing further than the length of the waveform. It is \n    recommended that you pad the waveform with zeros up to wave_slices[-1].stop.\n    \n    :param n_samples: the number of samples in the waveform\n    :param partial_utterance_n_frames: the number of mel spectrogram frames in each partial \n    utterance\n    :param min_pad_coverage: when reaching the last partial utterance, it may or may not have \n    enough frames. If at least <min_pad_coverage> of <partial_utterance_n_frames> are present, \n    then the last partial utterance will be considered, as if we padded the audio. Otherwise, \n    it will be discarded, as if we trimmed the audio. If there aren\'t enough frames for 1 partial \n    utterance, this parameter is ignored so that the function always returns at least 1 slice.\n    :param overlap: by how much the partial utterance should overlap. If set to 0, the partial \n    utterances are entirely disjoint. \n    :return: the waveform slices and mel spectrogram slices as lists of array slices. Index \n    respectively the waveform and the mel spectrogram with these slices to obtain the partial \n    utterances.\n    """"""\n    assert 0 <= overlap < 1\n    assert 0 < min_pad_coverage <= 1\n    \n    samples_per_frame = int((sampling_rate * mel_window_step / 1000))\n    n_frames = int(np.ceil((n_samples + 1) / samples_per_frame))\n    frame_step = max(int(np.round(partial_utterance_n_frames * (1 - overlap))), 1)\n\n    # Compute the slices\n    wav_slices, mel_slices = [], []\n    steps = max(1, n_frames - partial_utterance_n_frames + frame_step + 1)\n    for i in range(0, steps, frame_step):\n        mel_range = np.array([i, i + partial_utterance_n_frames])\n        wav_range = mel_range * samples_per_frame\n        mel_slices.append(slice(*mel_range))\n        wav_slices.append(slice(*wav_range))\n        \n    # Evaluate whether extra padding is warranted or not\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n    \n    return wav_slices, mel_slices\n\n\ndef embed_utterance(wav, using_partials=True, return_partials=False, **kwargs):\n    """"""\n    Computes an embedding for a single utterance.\n    \n    # TODO: handle multiple wavs to benefit from batching on GPU\n    :param wav: a preprocessed (see audio.py) utterance waveform as a numpy array of float32\n    :param using_partials: if True, then the utterance is split in partial utterances of \n    <partial_utterance_n_frames> frames and the utterance embedding is computed from their \n    normalized average. If False, the utterance is instead computed from feeding the entire \n    spectogram to the network.\n    :param return_partials: if True, the partial embeddings will also be returned along with the \n    wav slices that correspond to the partial embeddings.\n    :param kwargs: additional arguments to compute_partial_splits()\n    :return: the embedding as a numpy array of float32 of shape (model_embedding_size,). If \n    <return_partials> is True, the partial utterances as a numpy array of float32 of shape \n    (n_partials, model_embedding_size) and the wav partials as a list of slices will also be \n    returned. If <using_partials> is simultaneously set to False, both these values will be None \n    instead.\n    """"""\n    # Process the entire utterance if not using partials\n    if not using_partials:\n        frames = audio.wav_to_mel_spectrogram(wav)\n        embed = embed_frames_batch(frames[None, ...])[0]\n        if return_partials:\n            return embed, None, None\n        return embed\n    \n    # Compute where to split the utterance into partials and pad if necessary\n    wave_slices, mel_slices = compute_partial_slices(len(wav), **kwargs)\n    max_wave_length = wave_slices[-1].stop\n    if max_wave_length >= len(wav):\n        wav = np.pad(wav, (0, max_wave_length - len(wav)), ""constant"")\n    \n    # Split the utterance into partials\n    frames = audio.wav_to_mel_spectrogram(wav)\n    frames_batch = np.array([frames[s] for s in mel_slices])\n    partial_embeds = embed_frames_batch(frames_batch)\n    \n    # Compute the utterance embedding from the partial embeddings\n    raw_embed = np.mean(partial_embeds, axis=0)\n    embed = raw_embed / np.linalg.norm(raw_embed, 2)\n    \n    if return_partials:\n        return embed, partial_embeds, wave_slices\n    return embed\n\n\ndef embed_speaker(wavs, **kwargs):\n    raise NotImplemented()\n\n\ndef plot_embedding_as_heatmap(embed, ax=None, title="""", shape=None, color_range=(0, 0.30)):\n    if ax is None:\n        ax = plt.gca()\n    \n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        shape = (height, -1)\n    embed = embed.reshape(shape)\n    \n    cmap = cm.get_cmap()\n    mappable = ax.imshow(embed, cmap=cmap)\n    cbar = plt.colorbar(mappable, ax=ax, fraction=0.046, pad=0.04)\n    cbar.set_clim(*color_range)\n    \n    ax.set_xticks([]), ax.set_yticks([])\n    ax.set_title(title)\n'"
encoder/model.py,13,"b'from encoder.params_model import *\nfrom encoder.params_data import *\nfrom scipy.interpolate import interp1d\nfrom sklearn.metrics import roc_curve\nfrom torch.nn.utils import clip_grad_norm_\nfrom scipy.optimize import brentq\nfrom torch import nn\nimport numpy as np\nimport torch\n\n\nclass SpeakerEncoder(nn.Module):\n    def __init__(self, device, loss_device):\n        super().__init__()\n        self.loss_device = loss_device\n        \n        # Network defition\n        self.lstm = nn.LSTM(input_size=mel_n_channels,\n                            hidden_size=model_hidden_size, \n                            num_layers=model_num_layers, \n                            batch_first=True).to(device)\n        self.linear = nn.Linear(in_features=model_hidden_size, \n                                out_features=model_embedding_size).to(device)\n        self.relu = torch.nn.ReLU().to(device)\n        \n        # Cosine similarity scaling (with fixed initial parameter values)\n        self.similarity_weight = nn.Parameter(torch.tensor([10.])).to(loss_device)\n        self.similarity_bias = nn.Parameter(torch.tensor([-5.])).to(loss_device)\n\n        # Loss\n        self.loss_fn = nn.CrossEntropyLoss().to(loss_device)\n        \n    def do_gradient_ops(self):\n        # Gradient scale\n        self.similarity_weight.grad *= 0.01\n        self.similarity_bias.grad *= 0.01\n            \n        # Gradient clipping\n        clip_grad_norm_(self.parameters(), 3, norm_type=2)\n    \n    def forward(self, utterances, hidden_init=None):\n        """"""\n        Computes the embeddings of a batch of utterance spectrograms.\n        \n        :param utterances: batch of mel-scale filterbanks of same duration as a tensor of shape \n        (batch_size, n_frames, n_channels) \n        :param hidden_init: initial hidden state of the LSTM as a tensor of shape (num_layers, \n        batch_size, hidden_size). Will default to a tensor of zeros if None.\n        :return: the embeddings as a tensor of shape (batch_size, embedding_size)\n        """"""\n        # Pass the input through the LSTM layers and retrieve all outputs, the final hidden state\n        # and the final cell state.\n        out, (hidden, cell) = self.lstm(utterances, hidden_init)\n        \n        # We take only the hidden state of the last layer\n        embeds_raw = self.relu(self.linear(hidden[-1]))\n        \n        # L2-normalize it\n        embeds = embeds_raw / torch.norm(embeds_raw, dim=1, keepdim=True)\n        \n        return embeds\n    \n    def similarity_matrix(self, embeds):\n        """"""\n        Computes the similarity matrix according the section 2.1 of GE2E.\n\n        :param embeds: the embeddings as a tensor of shape (speakers_per_batch, \n        utterances_per_speaker, embedding_size)\n        :return: the similarity matrix as a tensor of shape (speakers_per_batch,\n        utterances_per_speaker, speakers_per_batch)\n        """"""\n        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n        \n        # Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n        centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n        centroids_incl = centroids_incl.clone() / torch.norm(centroids_incl, dim=2, keepdim=True)\n\n        # Exclusive centroids (1 per utterance)\n        centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\n        centroids_excl /= (utterances_per_speaker - 1)\n        centroids_excl = centroids_excl.clone() / torch.norm(centroids_excl, dim=2, keepdim=True)\n\n        # Similarity matrix. The cosine similarity of already 2-normed vectors is simply the dot\n        # product of these vectors (which is just an element-wise multiplication reduced by a sum).\n        # We vectorize the computation for efficiency.\n        sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker,\n                                 speakers_per_batch).to(self.loss_device)\n        mask_matrix = 1 - np.eye(speakers_per_batch, dtype=np.int)\n        for j in range(speakers_per_batch):\n            mask = np.where(mask_matrix[j])[0]\n            sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2)\n            sim_matrix[j, :, j] = (embeds[j] * centroids_excl[j]).sum(dim=1)\n        \n        ## Even more vectorized version (slower maybe because of transpose)\n        # sim_matrix2 = torch.zeros(speakers_per_batch, speakers_per_batch, utterances_per_speaker\n        #                           ).to(self.loss_device)\n        # eye = np.eye(speakers_per_batch, dtype=np.int)\n        # mask = np.where(1 - eye)\n        # sim_matrix2[mask] = (embeds[mask[0]] * centroids_incl[mask[1]]).sum(dim=2)\n        # mask = np.where(eye)\n        # sim_matrix2[mask] = (embeds * centroids_excl).sum(dim=2)\n        # sim_matrix2 = sim_matrix2.transpose(1, 2)\n        \n        sim_matrix = sim_matrix * self.similarity_weight + self.similarity_bias\n        return sim_matrix\n    \n    def loss(self, embeds):\n        """"""\n        Computes the softmax loss according the section 2.1 of GE2E.\n        \n        :param embeds: the embeddings as a tensor of shape (speakers_per_batch, \n        utterances_per_speaker, embedding_size)\n        :return: the loss and the EER for this batch of embeddings.\n        """"""\n        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n        \n        # Loss\n        sim_matrix = self.similarity_matrix(embeds)\n        sim_matrix = sim_matrix.reshape((speakers_per_batch * utterances_per_speaker, \n                                         speakers_per_batch))\n        ground_truth = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n        target = torch.from_numpy(ground_truth).long().to(self.loss_device)\n        loss = self.loss_fn(sim_matrix, target)\n        \n        # EER (not backpropagated)\n        with torch.no_grad():\n            inv_argmax = lambda i: np.eye(1, speakers_per_batch, i, dtype=np.int)[0]\n            labels = np.array([inv_argmax(i) for i in ground_truth])\n            preds = sim_matrix.detach().cpu().numpy()\n\n            # Snippet from https://yangcha.github.io/EER-ROC/\n            fpr, tpr, thresholds = roc_curve(labels.flatten(), preds.flatten())           \n            eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n            \n        return loss, eer'"
encoder/params_data.py,0,"b'\n## Mel-filterbank\nmel_window_length = 25  # In milliseconds\nmel_window_step = 10    # In milliseconds\nmel_n_channels = 40\n\n\n## Audio\nsampling_rate = 16000\n# Number of spectrogram frames in a partial utterance\npartials_n_frames = 160     # 1600 ms\n# Number of spectrogram frames at inference\ninference_n_frames = 80     #  800 ms\n\n\n## Voice Activation Detection\n# Window size of the VAD. Must be either 10, 20 or 30 milliseconds.\n# This sets the granularity of the VAD. Should not need to be changed.\nvad_window_length = 30  # In milliseconds\n# Number of frames to average together when performing the moving average smoothing.\n# The larger this value, the larger the VAD variations must be to not get smoothed out. \nvad_moving_average_width = 8\n# Maximum number of consecutive silent frames a segment can have.\nvad_max_silence_length = 6\n\n\n## Audio volume normalization\naudio_norm_target_dBFS = -30\n\n'"
encoder/params_model.py,0,b'\n## Model parameters\nmodel_hidden_size = 256\nmodel_embedding_size = 256\nmodel_num_layers = 3\n\n\n## Training parameters\nlearning_rate_init = 1e-4\nspeakers_per_batch = 64\nutterances_per_speaker = 10\n'
encoder/preprocess.py,0,"b'from multiprocess.pool import ThreadPool\nfrom encoder.params_data import *\nfrom encoder.config import librispeech_datasets, anglophone_nationalites\nfrom datetime import datetime\nfrom encoder import audio\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\n\n\nclass DatasetLog:\n    """"""\n    Registers metadata about the dataset in a text file.\n    """"""\n    def __init__(self, root, name):\n        self.text_file = open(Path(root, ""Log_%s.txt"" % name.replace(""/"", ""_"")), ""w"")\n        self.sample_data = dict()\n        \n        start_time = str(datetime.now().strftime(""%A %d %B %Y at %H:%M""))\n        self.write_line(""Creating dataset %s on %s"" % (name, start_time))\n        self.write_line(""-----"")\n        self._log_params()\n        \n    def _log_params(self):\n        from encoder import params_data\n        self.write_line(""Parameter values:"")\n        for param_name in (p for p in dir(params_data) if not p.startswith(""__"")):\n            value = getattr(params_data, param_name)\n            self.write_line(""\\t%s: %s"" % (param_name, value))\n        self.write_line(""-----"")\n    \n    def write_line(self, line):\n        self.text_file.write(""%s\\n"" % line)\n        \n    def add_sample(self, **kwargs):\n        for param_name, value in kwargs.items():\n            if not param_name in self.sample_data:\n                self.sample_data[param_name] = []\n            self.sample_data[param_name].append(value)\n            \n    def finalize(self):\n        self.write_line(""Statistics:"")\n        for param_name, values in self.sample_data.items():\n            self.write_line(""\\t%s:"" % param_name)\n            self.write_line(""\\t\\tmin %.3f, max %.3f"" % (np.min(values), np.max(values)))\n            self.write_line(""\\t\\tmean %.3f, median %.3f"" % (np.mean(values), np.median(values)))\n        self.write_line(""-----"")\n        end_time = str(datetime.now().strftime(""%A %d %B %Y at %H:%M""))\n        self.write_line(""Finished on %s"" % end_time)\n        self.text_file.close()\n       \n        \ndef _init_preprocess_dataset(dataset_name, datasets_root, out_dir) -> (Path, DatasetLog):\n    dataset_root = datasets_root.joinpath(dataset_name)\n    if not dataset_root.exists():\n        print(""Couldn\\\'t find %s, skipping this dataset."" % dataset_root)\n        return None, None\n    return dataset_root, DatasetLog(out_dir, dataset_name)\n\n\ndef _preprocess_speaker_dirs(speaker_dirs, dataset_name, datasets_root, out_dir, extension,\n                             skip_existing, logger):\n    print(""%s: Preprocessing data for %d speakers."" % (dataset_name, len(speaker_dirs)))\n    \n    # Function to preprocess utterances for one speaker\n    def preprocess_speaker(speaker_dir: Path):\n        # Give a name to the speaker that includes its dataset\n        speaker_name = ""_"".join(speaker_dir.relative_to(datasets_root).parts)\n        \n        # Create an output directory with that name, as well as a txt file containing a \n        # reference to each source file.\n        speaker_out_dir = out_dir.joinpath(speaker_name)\n        speaker_out_dir.mkdir(exist_ok=True)\n        sources_fpath = speaker_out_dir.joinpath(""_sources.txt"")\n        \n        # There\'s a possibility that the preprocessing was interrupted earlier, check if \n        # there already is a sources file.\n        if sources_fpath.exists():\n            try:\n                with sources_fpath.open(""r"") as sources_file:\n                    existing_fnames = {line.split("","")[0] for line in sources_file}\n            except:\n                existing_fnames = {}\n        else:\n            existing_fnames = {}\n        \n        # Gather all audio files for that speaker recursively\n        sources_file = sources_fpath.open(""a"" if skip_existing else ""w"")\n        for in_fpath in speaker_dir.glob(""**/*.%s"" % extension):\n            # Check if the target output file already exists\n            out_fname = ""_"".join(in_fpath.relative_to(speaker_dir).parts)\n            out_fname = out_fname.replace("".%s"" % extension, "".npy"")\n            if skip_existing and out_fname in existing_fnames:\n                continue\n                \n            # Load and preprocess the waveform\n            wav = audio.preprocess_wav(in_fpath)\n            if len(wav) == 0:\n                continue\n            \n            # Create the mel spectrogram, discard those that are too short\n            frames = audio.wav_to_mel_spectrogram(wav)\n            if len(frames) < partials_n_frames:\n                continue\n            \n            out_fpath = speaker_out_dir.joinpath(out_fname)\n            np.save(out_fpath, frames)\n            logger.add_sample(duration=len(wav) / sampling_rate)\n            sources_file.write(""%s,%s\\n"" % (out_fname, in_fpath))\n        \n        sources_file.close()\n    \n    # Process the utterances for each speaker\n    with ThreadPool(8) as pool:\n        list(tqdm(pool.imap(preprocess_speaker, speaker_dirs), dataset_name, len(speaker_dirs),\n                  unit=""speakers""))\n    logger.finalize()\n    print(""Done preprocessing %s.\\n"" % dataset_name)\n\n\ndef preprocess_librispeech(datasets_root: Path, out_dir: Path, skip_existing=False):\n    for dataset_name in librispeech_datasets[""train""][""other""]:\n        # Initialize the preprocessing\n        dataset_root, logger = _init_preprocess_dataset(dataset_name, datasets_root, out_dir)\n        if not dataset_root:\n            return \n        \n        # Preprocess all speakers\n        speaker_dirs = list(dataset_root.glob(""*""))\n        _preprocess_speaker_dirs(speaker_dirs, dataset_name, datasets_root, out_dir, ""flac"",\n                                 skip_existing, logger)\n\n\ndef preprocess_voxceleb1(datasets_root: Path, out_dir: Path, skip_existing=False):\n    # Initialize the preprocessing\n    dataset_name = ""VoxCeleb1""\n    dataset_root, logger = _init_preprocess_dataset(dataset_name, datasets_root, out_dir)\n    if not dataset_root:\n        return\n\n    # Get the contents of the meta file\n    with dataset_root.joinpath(""vox1_meta.csv"").open(""r"") as metafile:\n        metadata = [line.split(""\\t"") for line in metafile][1:]\n    \n    # Select the ID and the nationality, filter out non-anglophone speakers\n    nationalities = {line[0]: line[3] for line in metadata}\n    keep_speaker_ids = [speaker_id for speaker_id, nationality in nationalities.items() if \n                        nationality.lower() in anglophone_nationalites]\n    print(""VoxCeleb1: using samples from %d (presumed anglophone) speakers out of %d."" % \n          (len(keep_speaker_ids), len(nationalities)))\n    \n    # Get the speaker directories for anglophone speakers only\n    speaker_dirs = dataset_root.joinpath(""wav"").glob(""*"")\n    speaker_dirs = [speaker_dir for speaker_dir in speaker_dirs if\n                    speaker_dir.name in keep_speaker_ids]\n    print(""VoxCeleb1: found %d anglophone speakers on the disk, %d missing (this is normal)."" % \n          (len(speaker_dirs), len(keep_speaker_ids) - len(speaker_dirs)))\n\n    # Preprocess all speakers\n    _preprocess_speaker_dirs(speaker_dirs, dataset_name, datasets_root, out_dir, ""wav"",\n                             skip_existing, logger)\n\n\ndef preprocess_voxceleb2(datasets_root: Path, out_dir: Path, skip_existing=False):\n    # Initialize the preprocessing\n    dataset_name = ""VoxCeleb2""\n    dataset_root, logger = _init_preprocess_dataset(dataset_name, datasets_root, out_dir)\n    if not dataset_root:\n        return\n    \n    # Get the speaker directories\n    # Preprocess all speakers\n    speaker_dirs = list(dataset_root.joinpath(""dev"", ""aac"").glob(""*""))\n    _preprocess_speaker_dirs(speaker_dirs, dataset_name, datasets_root, out_dir, ""m4a"",\n                             skip_existing, logger)\n'"
encoder/train.py,10,"b'from encoder.visualizations import Visualizations\nfrom encoder.data_objects import SpeakerVerificationDataLoader, SpeakerVerificationDataset\nfrom encoder.params_model import *\nfrom encoder.model import SpeakerEncoder\nfrom utils.profiler import Profiler\nfrom pathlib import Path\nimport torch\n\ndef sync(device: torch.device):\n    # FIXME\n    return \n    # For correct profiling (cuda operations are async)\n    if device.type == ""cuda"":\n        torch.cuda.synchronize(device)\n\ndef train(run_id: str, clean_data_root: Path, models_dir: Path, umap_every: int, save_every: int,\n          backup_every: int, vis_every: int, force_restart: bool, visdom_server: str,\n          no_visdom: bool):\n    # Create a dataset and a dataloader\n    dataset = SpeakerVerificationDataset(clean_data_root)\n    loader = SpeakerVerificationDataLoader(\n        dataset,\n        speakers_per_batch,\n        utterances_per_speaker,\n        num_workers=8,\n    )\n    \n    # Setup the device on which to run the forward pass and the loss. These can be different, \n    # because the forward pass is faster on the GPU whereas the loss is often (depending on your\n    # hyperparameters) faster on the CPU.\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    # FIXME: currently, the gradient is None if loss_device is cuda\n    loss_device = torch.device(""cpu"")\n    \n    # Create the model and the optimizer\n    model = SpeakerEncoder(device, loss_device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate_init)\n    init_step = 1\n    \n    # Configure file path for the model\n    state_fpath = models_dir.joinpath(run_id + "".pt"")\n    backup_dir = models_dir.joinpath(run_id + ""_backups"")\n\n    # Load any existing model\n    if not force_restart:\n        if state_fpath.exists():\n            print(""Found existing model \\""%s\\"", loading it and resuming training."" % run_id)\n            checkpoint = torch.load(state_fpath)\n            init_step = checkpoint[""step""]\n            model.load_state_dict(checkpoint[""model_state""])\n            optimizer.load_state_dict(checkpoint[""optimizer_state""])\n            optimizer.param_groups[0][""lr""] = learning_rate_init\n        else:\n            print(""No model \\""%s\\"" found, starting training from scratch."" % run_id)\n    else:\n        print(""Starting the training from scratch."")\n    model.train()\n    \n    # Initialize the visualization environment\n    vis = Visualizations(run_id, vis_every, server=visdom_server, disabled=no_visdom)\n    vis.log_dataset(dataset)\n    vis.log_params()\n    device_name = str(torch.cuda.get_device_name(0) if torch.cuda.is_available() else ""CPU"")\n    vis.log_implementation({""Device"": device_name})\n    \n    # Training loop\n    profiler = Profiler(summarize_every=10, disabled=False)\n    for step, speaker_batch in enumerate(loader, init_step):\n        profiler.tick(""Blocking, waiting for batch (threaded)"")\n        \n        # Forward pass\n        inputs = torch.from_numpy(speaker_batch.data).to(device)\n        sync(device)\n        profiler.tick(""Data to %s"" % device)\n        embeds = model(inputs)\n        sync(device)\n        profiler.tick(""Forward pass"")\n        embeds_loss = embeds.view((speakers_per_batch, utterances_per_speaker, -1)).to(loss_device)\n        loss, eer = model.loss(embeds_loss)\n        sync(loss_device)\n        profiler.tick(""Loss"")\n\n        # Backward pass\n        model.zero_grad()\n        loss.backward()\n        profiler.tick(""Backward pass"")\n        model.do_gradient_ops()\n        optimizer.step()\n        profiler.tick(""Parameter update"")\n        \n        # Update visualizations\n        # learning_rate = optimizer.param_groups[0][""lr""]\n        vis.update(loss.item(), eer, step)\n        \n        # Draw projections and save them to the backup folder\n        if umap_every != 0 and step % umap_every == 0:\n            print(""Drawing and saving projections (step %d)"" % step)\n            backup_dir.mkdir(exist_ok=True)\n            projection_fpath = backup_dir.joinpath(""%s_umap_%06d.png"" % (run_id, step))\n            embeds = embeds.detach().cpu().numpy()\n            vis.draw_projections(embeds, utterances_per_speaker, step, projection_fpath)\n            vis.save()\n\n        # Overwrite the latest version of the model\n        if save_every != 0 and step % save_every == 0:\n            print(""Saving the model (step %d)"" % step)\n            torch.save({\n                ""step"": step + 1,\n                ""model_state"": model.state_dict(),\n                ""optimizer_state"": optimizer.state_dict(),\n            }, state_fpath)\n            \n        # Make a backup\n        if backup_every != 0 and step % backup_every == 0:\n            print(""Making a backup (step %d)"" % step)\n            backup_dir.mkdir(exist_ok=True)\n            backup_fpath = backup_dir.joinpath(""%s_bak_%06d.pt"" % (run_id, step))\n            torch.save({\n                ""step"": step + 1,\n                ""model_state"": model.state_dict(),\n                ""optimizer_state"": optimizer.state_dict(),\n            }, backup_fpath)\n            \n        profiler.tick(""Extras (visualizations, saving)"")\n        '"
encoder/visualizations.py,0,"b'from encoder.data_objects.speaker_verification_dataset import SpeakerVerificationDataset\nfrom datetime import datetime\nfrom time import perf_counter as timer\nimport matplotlib.pyplot as plt\nimport numpy as np\n# import webbrowser\nimport visdom\nimport umap\n\ncolormap = np.array([\n    [76, 255, 0],\n    [0, 127, 70],\n    [255, 0, 0],\n    [255, 217, 38],\n    [0, 135, 255],\n    [165, 0, 165],\n    [255, 167, 255],\n    [0, 255, 255],\n    [255, 96, 38],\n    [142, 76, 0],\n    [33, 0, 127],\n    [0, 0, 0],\n    [183, 183, 183],\n], dtype=np.float) / 255 \n\n\nclass Visualizations:\n    def __init__(self, env_name=None, update_every=10, server=""http://localhost"", disabled=False):\n        # Tracking data\n        self.last_update_timestamp = timer()\n        self.update_every = update_every\n        self.step_times = []\n        self.losses = []\n        self.eers = []\n        print(""Updating the visualizations every %d steps."" % update_every)\n        \n        # If visdom is disabled TODO: use a better paradigm for that\n        self.disabled = disabled    \n        if self.disabled:\n            return \n        \n        # Set the environment name\n        now = str(datetime.now().strftime(""%d-%m %Hh%M""))\n        if env_name is None:\n            self.env_name = now\n        else:\n            self.env_name = ""%s (%s)"" % (env_name, now)\n        \n        # Connect to visdom and open the corresponding window in the browser\n        try:\n            self.vis = visdom.Visdom(server, env=self.env_name, raise_exceptions=True)\n        except ConnectionError:\n            raise Exception(""No visdom server detected. Run the command \\""visdom\\"" in your CLI to ""\n                            ""start it."")\n        # webbrowser.open(""http://localhost:8097/env/"" + self.env_name)\n        \n        # Create the windows\n        self.loss_win = None\n        self.eer_win = None\n        # self.lr_win = None\n        self.implementation_win = None\n        self.projection_win = None\n        self.implementation_string = """"\n        \n    def log_params(self):\n        if self.disabled:\n            return \n        from encoder import params_data\n        from encoder import params_model\n        param_string = ""<b>Model parameters</b>:<br>""\n        for param_name in (p for p in dir(params_model) if not p.startswith(""__"")):\n            value = getattr(params_model, param_name)\n            param_string += ""\\t%s: %s<br>"" % (param_name, value)\n        param_string += ""<b>Data parameters</b>:<br>""\n        for param_name in (p for p in dir(params_data) if not p.startswith(""__"")):\n            value = getattr(params_data, param_name)\n            param_string += ""\\t%s: %s<br>"" % (param_name, value)\n        self.vis.text(param_string, opts={""title"": ""Parameters""})\n        \n    def log_dataset(self, dataset: SpeakerVerificationDataset):\n        if self.disabled:\n            return \n        dataset_string = """"\n        dataset_string += ""<b>Speakers</b>: %s\\n"" % len(dataset.speakers)\n        dataset_string += ""\\n"" + dataset.get_logs()\n        dataset_string = dataset_string.replace(""\\n"", ""<br>"")\n        self.vis.text(dataset_string, opts={""title"": ""Dataset""})\n        \n    def log_implementation(self, params):\n        if self.disabled:\n            return \n        implementation_string = """"\n        for param, value in params.items():\n            implementation_string += ""<b>%s</b>: %s\\n"" % (param, value)\n            implementation_string = implementation_string.replace(""\\n"", ""<br>"")\n        self.implementation_string = implementation_string\n        self.implementation_win = self.vis.text(\n            implementation_string, \n            opts={""title"": ""Training implementation""}\n        )\n\n    def update(self, loss, eer, step):\n        # Update the tracking data\n        now = timer()\n        self.step_times.append(1000 * (now - self.last_update_timestamp))\n        self.last_update_timestamp = now\n        self.losses.append(loss)\n        self.eers.append(eer)\n        print(""."", end="""")\n        \n        # Update the plots every <update_every> steps\n        if step % self.update_every != 0:\n            return\n        time_string = ""Step time:  mean: %5dms  std: %5dms"" % \\\n                      (int(np.mean(self.step_times)), int(np.std(self.step_times)))\n        print(""\\nStep %6d   Loss: %.4f   EER: %.4f   %s"" %\n              (step, np.mean(self.losses), np.mean(self.eers), time_string))\n        if not self.disabled:\n            self.loss_win = self.vis.line(\n                [np.mean(self.losses)],\n                [step],\n                win=self.loss_win,\n                update=""append"" if self.loss_win else None,\n                opts=dict(\n                    legend=[""Avg. loss""],\n                    xlabel=""Step"",\n                    ylabel=""Loss"",\n                    title=""Loss"",\n                )\n            )\n            self.eer_win = self.vis.line(\n                [np.mean(self.eers)],\n                [step],\n                win=self.eer_win,\n                update=""append"" if self.eer_win else None,\n                opts=dict(\n                    legend=[""Avg. EER""],\n                    xlabel=""Step"",\n                    ylabel=""EER"",\n                    title=""Equal error rate""\n                )\n            )\n            if self.implementation_win is not None:\n                self.vis.text(\n                    self.implementation_string + (""<b>%s</b>"" % time_string), \n                    win=self.implementation_win,\n                    opts={""title"": ""Training implementation""},\n                )\n\n        # Reset the tracking\n        self.losses.clear()\n        self.eers.clear()\n        self.step_times.clear()\n        \n    def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None,\n                         max_speakers=10):\n        max_speakers = min(max_speakers, len(colormap))\n        embeds = embeds[:max_speakers * utterances_per_speaker]\n        \n        n_speakers = len(embeds) // utterances_per_speaker\n        ground_truth = np.repeat(np.arange(n_speakers), utterances_per_speaker)\n        colors = [colormap[i] for i in ground_truth]\n        \n        reducer = umap.UMAP()\n        projected = reducer.fit_transform(embeds)\n        plt.scatter(projected[:, 0], projected[:, 1], c=colors)\n        plt.gca().set_aspect(""equal"", ""datalim"")\n        plt.title(""UMAP projection (step %d)"" % step)\n        if not self.disabled:\n            self.projection_win = self.vis.matplot(plt, win=self.projection_win)\n        if out_fpath is not None:\n            plt.savefig(out_fpath)\n        plt.clf()\n        \n    def save(self):\n        if not self.disabled:\n            self.vis.save([self.env_name])\n        '"
synthesizer/__init__.py,0,b'#'
synthesizer/audio.py,0,"b'import librosa\nimport librosa.filters\nimport numpy as np\nimport tensorflow as tf\nfrom scipy import signal\nfrom scipy.io import wavfile\n\n\ndef load_wav(path, sr):\n    return librosa.core.load(path, sr=sr)[0]\n\ndef save_wav(wav, path, sr):\n    wav *= 32767 / max(0.01, np.max(np.abs(wav)))\n    #proposed by @dsmiller\n    wavfile.write(path, sr, wav.astype(np.int16))\n\ndef save_wavenet_wav(wav, path, sr):\n    librosa.output.write_wav(path, wav, sr=sr)\n\ndef preemphasis(wav, k, preemphasize=True):\n    if preemphasize:\n        return signal.lfilter([1, -k], [1], wav)\n    return wav\n\ndef inv_preemphasis(wav, k, inv_preemphasize=True):\n    if inv_preemphasize:\n        return signal.lfilter([1], [1, -k], wav)\n    return wav\n\n#From https://github.com/r9y9/wavenet_vocoder/blob/master/audio.py\ndef start_and_end_indices(quantized, silence_threshold=2):\n    for start in range(quantized.size):\n        if abs(quantized[start] - 127) > silence_threshold:\n            break\n    for end in range(quantized.size - 1, 1, -1):\n        if abs(quantized[end] - 127) > silence_threshold:\n            break\n    \n    assert abs(quantized[start] - 127) > silence_threshold\n    assert abs(quantized[end] - 127) > silence_threshold\n    \n    return start, end\n\ndef get_hop_size(hparams):\n    hop_size = hparams.hop_size\n    if hop_size is None:\n        assert hparams.frame_shift_ms is not None\n        hop_size = int(hparams.frame_shift_ms / 1000 * hparams.sample_rate)\n    return hop_size\n\ndef linearspectrogram(wav, hparams):\n    D = _stft(preemphasis(wav, hparams.preemphasis, hparams.preemphasize), hparams)\n    S = _amp_to_db(np.abs(D), hparams) - hparams.ref_level_db\n    \n    if hparams.signal_normalization:\n        return _normalize(S, hparams)\n    return S\n\ndef melspectrogram(wav, hparams):\n    D = _stft(preemphasis(wav, hparams.preemphasis, hparams.preemphasize), hparams)\n    S = _amp_to_db(_linear_to_mel(np.abs(D), hparams), hparams) - hparams.ref_level_db\n    \n    if hparams.signal_normalization:\n        return _normalize(S, hparams)\n    return S\n\ndef inv_linear_spectrogram(linear_spectrogram, hparams):\n    """"""Converts linear spectrogram to waveform using librosa""""""\n    if hparams.signal_normalization:\n        D = _denormalize(linear_spectrogram, hparams)\n    else:\n        D = linear_spectrogram\n    \n    S = _db_to_amp(D + hparams.ref_level_db) #Convert back to linear\n    \n    if hparams.use_lws:\n        processor = _lws_processor(hparams)\n        D = processor.run_lws(S.astype(np.float64).T ** hparams.power)\n        y = processor.istft(D).astype(np.float32)\n        return inv_preemphasis(y, hparams.preemphasis, hparams.preemphasize)\n    else:\n        return inv_preemphasis(_griffin_lim(S ** hparams.power, hparams), hparams.preemphasis, hparams.preemphasize)\n\ndef inv_mel_spectrogram(mel_spectrogram, hparams):\n    """"""Converts mel spectrogram to waveform using librosa""""""\n    if hparams.signal_normalization:\n        D = _denormalize(mel_spectrogram, hparams)\n    else:\n        D = mel_spectrogram\n    \n    S = _mel_to_linear(_db_to_amp(D + hparams.ref_level_db), hparams)  # Convert back to linear\n    \n    if hparams.use_lws:\n        processor = _lws_processor(hparams)\n        D = processor.run_lws(S.astype(np.float64).T ** hparams.power)\n        y = processor.istft(D).astype(np.float32)\n        return inv_preemphasis(y, hparams.preemphasis, hparams.preemphasize)\n    else:\n        return inv_preemphasis(_griffin_lim(S ** hparams.power, hparams), hparams.preemphasis, hparams.preemphasize)\n\ndef _lws_processor(hparams):\n    import lws\n    return lws.lws(hparams.n_fft, get_hop_size(hparams), fftsize=hparams.win_size, mode=""speech"")\n\ndef _griffin_lim(S, hparams):\n    """"""librosa implementation of Griffin-Lim\n    Based on https://github.com/librosa/librosa/issues/434\n    """"""\n    angles = np.exp(2j * np.pi * np.random.rand(*S.shape))\n    S_complex = np.abs(S).astype(np.complex)\n    y = _istft(S_complex * angles, hparams)\n    for i in range(hparams.griffin_lim_iters):\n        angles = np.exp(1j * np.angle(_stft(y, hparams)))\n        y = _istft(S_complex * angles, hparams)\n    return y\n\ndef _stft(y, hparams):\n    if hparams.use_lws:\n        return _lws_processor(hparams).stft(y).T\n    else:\n        return librosa.stft(y=y, n_fft=hparams.n_fft, hop_length=get_hop_size(hparams), win_length=hparams.win_size)\n\ndef _istft(y, hparams):\n    return librosa.istft(y, hop_length=get_hop_size(hparams), win_length=hparams.win_size)\n\n##########################################################\n#Those are only correct when using lws!!! (This was messing with Wavenet quality for a long time!)\ndef num_frames(length, fsize, fshift):\n    """"""Compute number of time frames of spectrogram\n    """"""\n    pad = (fsize - fshift)\n    if length % fshift == 0:\n        M = (length + pad * 2 - fsize) // fshift + 1\n    else:\n        M = (length + pad * 2 - fsize) // fshift + 2\n    return M\n\n\ndef pad_lr(x, fsize, fshift):\n    """"""Compute left and right padding\n    """"""\n    M = num_frames(len(x), fsize, fshift)\n    pad = (fsize - fshift)\n    T = len(x) + 2 * pad\n    r = (M - 1) * fshift + fsize - T\n    return pad, pad + r\n##########################################################\n#Librosa correct padding\ndef librosa_pad_lr(x, fsize, fshift):\n    return 0, (x.shape[0] // fshift + 1) * fshift - x.shape[0]\n\n# Conversions\n_mel_basis = None\n_inv_mel_basis = None\n\ndef _linear_to_mel(spectogram, hparams):\n    global _mel_basis\n    if _mel_basis is None:\n        _mel_basis = _build_mel_basis(hparams)\n    return np.dot(_mel_basis, spectogram)\n\ndef _mel_to_linear(mel_spectrogram, hparams):\n    global _inv_mel_basis\n    if _inv_mel_basis is None:\n        _inv_mel_basis = np.linalg.pinv(_build_mel_basis(hparams))\n    return np.maximum(1e-10, np.dot(_inv_mel_basis, mel_spectrogram))\n\ndef _build_mel_basis(hparams):\n    assert hparams.fmax <= hparams.sample_rate // 2\n    return librosa.filters.mel(hparams.sample_rate, hparams.n_fft, n_mels=hparams.num_mels,\n                               fmin=hparams.fmin, fmax=hparams.fmax)\n\ndef _amp_to_db(x, hparams):\n    min_level = np.exp(hparams.min_level_db / 20 * np.log(10))\n    return 20 * np.log10(np.maximum(min_level, x))\n\ndef _db_to_amp(x):\n    return np.power(10.0, (x) * 0.05)\n\ndef _normalize(S, hparams):\n    if hparams.allow_clipping_in_normalization:\n        if hparams.symmetric_mels:\n            return np.clip((2 * hparams.max_abs_value) * ((S - hparams.min_level_db) / (-hparams.min_level_db)) - hparams.max_abs_value,\n                           -hparams.max_abs_value, hparams.max_abs_value)\n        else:\n            return np.clip(hparams.max_abs_value * ((S - hparams.min_level_db) / (-hparams.min_level_db)), 0, hparams.max_abs_value)\n    \n    assert S.max() <= 0 and S.min() - hparams.min_level_db >= 0\n    if hparams.symmetric_mels:\n        return (2 * hparams.max_abs_value) * ((S - hparams.min_level_db) / (-hparams.min_level_db)) - hparams.max_abs_value\n    else:\n        return hparams.max_abs_value * ((S - hparams.min_level_db) / (-hparams.min_level_db))\n\ndef _denormalize(D, hparams):\n    if hparams.allow_clipping_in_normalization:\n        if hparams.symmetric_mels:\n            return (((np.clip(D, -hparams.max_abs_value,\n                              hparams.max_abs_value) + hparams.max_abs_value) * -hparams.min_level_db / (2 * hparams.max_abs_value))\n                    + hparams.min_level_db)\n        else:\n            return ((np.clip(D, 0, hparams.max_abs_value) * -hparams.min_level_db / hparams.max_abs_value) + hparams.min_level_db)\n    \n    if hparams.symmetric_mels:\n        return (((D + hparams.max_abs_value) * -hparams.min_level_db / (2 * hparams.max_abs_value)) + hparams.min_level_db)\n    else:\n        return ((D * -hparams.min_level_db / hparams.max_abs_value) + hparams.min_level_db)\n'"
synthesizer/feeder.py,0,"b'from sklearn.model_selection import train_test_split\nfrom synthesizer.utils.text import text_to_sequence\nfrom synthesizer.infolog import log\nimport tensorflow as tf\nimport numpy as np\nimport threading\nimport time\nimport os\n\n_batches_per_group = 64\n\nclass Feeder:\n\t""""""\n\t\tFeeds batches of data into queue on a background thread.\n\t""""""\n\n\tdef __init__(self, coordinator, metadata_filename, hparams):\n\t\tsuper(Feeder, self).__init__()\n\t\tself._coord = coordinator\n\t\tself._hparams = hparams\n\t\tself._cleaner_names = [x.strip() for x in hparams.cleaners.split("","")]\n\t\tself._train_offset = 0\n\t\tself._test_offset = 0\n\n\t\t# Load metadata\n\t\tself._mel_dir = os.path.join(os.path.dirname(metadata_filename), ""mels"")\n\t\tself._embed_dir = os.path.join(os.path.dirname(metadata_filename), ""embeds"")\n\t\twith open(metadata_filename, encoding=""utf-8"") as f:\n\t\t\tself._metadata = [line.strip().split(""|"") for line in f]\n\t\t\tframe_shift_ms = hparams.hop_size / hparams.sample_rate\n\t\t\thours = sum([int(x[4]) for x in self._metadata]) * frame_shift_ms / (3600)\n\t\t\tlog(""Loaded metadata for {} examples ({:.2f} hours)"".format(len(self._metadata), hours))\n\n\t\t#Train test split\n\t\tif hparams.tacotron_test_size is None:\n\t\t\tassert hparams.tacotron_test_batches is not None\n\n\t\ttest_size = (hparams.tacotron_test_size if hparams.tacotron_test_size is not None\n\t\t\telse hparams.tacotron_test_batches * hparams.tacotron_batch_size)\n\t\tindices = np.arange(len(self._metadata))\n\t\ttrain_indices, test_indices = train_test_split(indices,\n\t\t\ttest_size=test_size, random_state=hparams.tacotron_data_random_state)\n\n\t\t#Make sure test_indices is a multiple of batch_size else round up\n\t\tlen_test_indices = self._round_down(len(test_indices), hparams.tacotron_batch_size)\n\t\textra_test = test_indices[len_test_indices:]\n\t\ttest_indices = test_indices[:len_test_indices]\n\t\ttrain_indices = np.concatenate([train_indices, extra_test])\n\n\t\tself._train_meta = list(np.array(self._metadata)[train_indices])\n\t\tself._test_meta = list(np.array(self._metadata)[test_indices])\n\n\t\tself.test_steps = len(self._test_meta) // hparams.tacotron_batch_size\n\n\t\tif hparams.tacotron_test_size is None:\n\t\t\tassert hparams.tacotron_test_batches == self.test_steps\n\n\t\t#pad input sequences with the <pad_token> 0 ( _ )\n\t\tself._pad = 0\n\t\t#explicitely setting the padding to a value that doesn""t originally exist in the spectogram\n\t\t#to avoid any possible conflicts, without affecting the output range of the model too much\n\t\tif hparams.symmetric_mels:\n\t\t\tself._target_pad = -hparams.max_abs_value\n\t\telse:\n\t\t\tself._target_pad = 0.\n\t\t#Mark finished sequences with 1s\n\t\tself._token_pad = 1.\n\n\t\twith tf.device(""/cpu:0""):\n\t\t\t# Create placeholders for inputs and targets. Don""t specify batch size because we want\n\t\t\t# to be able to feed different batch sizes at eval time.\n\t\t\tself._placeholders = [\n\t\t\t\ttf.placeholder(tf.int32, shape=(None, None), name=""inputs""),\n\t\t\t\ttf.placeholder(tf.int32, shape=(None, ), name=""input_lengths""),\n\t\t\t\ttf.placeholder(tf.float32, shape=(None, None, hparams.num_mels), \n\t\t\t\t\t\t\t   name=""mel_targets""),\n\t\t\t\ttf.placeholder(tf.float32, shape=(None, None), name=""token_targets""),\n\t\t\t\ttf.placeholder(tf.int32, shape=(None, ), name=""targets_lengths""),\n\t\t\t\ttf.placeholder(tf.int32, shape=(hparams.tacotron_num_gpus, None), \n\t\t\t\t\t\t\t   name=""split_infos""),\n\t\t\t\t\n\t\t\t\t# SV2TTS\n\t\t\t\ttf.placeholder(tf.float32, shape=(None, hparams.speaker_embedding_size), \n\t\t\t\t\t\t\t   name=""speaker_embeddings"")\n\t\t\t]\n\n\t\t\t# Create queue for buffering data\n\t\t\tqueue = tf.FIFOQueue(8, [tf.int32, tf.int32, tf.float32, tf.float32, \n\t\t\t\t\t\t\t\t\t tf.int32, tf.int32, tf.float32], name=""input_queue"")\n\t\t\tself._enqueue_op = queue.enqueue(self._placeholders)\n\t\t\tself.inputs, self.input_lengths, self.mel_targets, self.token_targets, \\\n\t\t\t\tself.targets_lengths, self.split_infos, self.speaker_embeddings = queue.dequeue()\n\n\t\t\tself.inputs.set_shape(self._placeholders[0].shape)\n\t\t\tself.input_lengths.set_shape(self._placeholders[1].shape)\n\t\t\tself.mel_targets.set_shape(self._placeholders[2].shape)\n\t\t\tself.token_targets.set_shape(self._placeholders[3].shape)\n\t\t\tself.targets_lengths.set_shape(self._placeholders[4].shape)\n\t\t\tself.split_infos.set_shape(self._placeholders[5].shape)\n\t\t\tself.speaker_embeddings.set_shape(self._placeholders[6].shape)\n\n\t\t\t# Create eval queue for buffering eval data\n\t\t\teval_queue = tf.FIFOQueue(1, [tf.int32, tf.int32, tf.float32, tf.float32,  \n\t\t\t\t\t\t\t\t\t\t  tf.int32, tf.int32, tf.float32], name=""eval_queue"")\n\t\t\tself._eval_enqueue_op = eval_queue.enqueue(self._placeholders)\n\t\t\tself.eval_inputs, self.eval_input_lengths, self.eval_mel_targets, \\\n\t\t\t\tself.eval_token_targets, self.eval_targets_lengths, \\\n\t\t\t\tself.eval_split_infos, self.eval_speaker_embeddings = eval_queue.dequeue()\n\n\t\t\tself.eval_inputs.set_shape(self._placeholders[0].shape)\n\t\t\tself.eval_input_lengths.set_shape(self._placeholders[1].shape)\n\t\t\tself.eval_mel_targets.set_shape(self._placeholders[2].shape)\n\t\t\tself.eval_token_targets.set_shape(self._placeholders[3].shape)\n\t\t\tself.eval_targets_lengths.set_shape(self._placeholders[4].shape)\n\t\t\tself.eval_split_infos.set_shape(self._placeholders[5].shape)\n\t\t\tself.eval_speaker_embeddings.set_shape(self._placeholders[6].shape)\n\n\n\tdef start_threads(self, session):\n\t\tself._session = session\n\t\tthread = threading.Thread(name=""background"", target=self._enqueue_next_train_group)\n\t\tthread.daemon = True #Thread will close when parent quits\n\t\tthread.start()\n\n\t\tthread = threading.Thread(name=""background"", target=self._enqueue_next_test_group)\n\t\tthread.daemon = True #Thread will close when parent quits\n\t\tthread.start()\n\n\tdef _get_test_groups(self):\n\t\tmeta = self._test_meta[self._test_offset]\n\t\tself._test_offset += 1\n\n\t\ttext = meta[5]\n\n\t\tinput_data = np.asarray(text_to_sequence(text, self._cleaner_names), dtype=np.int32)\n\t\tmel_target = np.load(os.path.join(self._mel_dir, meta[1]))\n\t\t#Create parallel sequences containing zeros to represent a non finished sequence\n\t\ttoken_target = np.asarray([0.] * (len(mel_target) - 1))\n\t\tembed_target = np.load(os.path.join(self._embed_dir, meta[2]))\n\t\treturn input_data, mel_target, token_target, embed_target, len(mel_target)\n\t\n\tdef make_test_batches(self):\n\t\tstart = time.time()\n\n\t\t# Read a group of examples\n\t\tn = self._hparams.tacotron_batch_size\n\t\tr = self._hparams.outputs_per_step\n\n\t\t#Test on entire test set\n\t\texamples = [self._get_test_groups() for i in range(len(self._test_meta))]\n\n\t\t# Bucket examples based on similar output sequence length for efficiency\n\t\texamples.sort(key=lambda x: x[-1])\n\t\tbatches = [examples[i: i+n] for i in range(0, len(examples), n)]\n\t\tnp.random.shuffle(batches)\n\n\t\tlog(""\\nGenerated %d test batches of size %d in %.3f sec"" % (len(batches), n, time.time() - start))\n\t\treturn batches, r\n\n\tdef _enqueue_next_train_group(self):\n\t\twhile not self._coord.should_stop():\n\t\t\tstart = time.time()\n\n\t\t\t# Read a group of examples\n\t\t\tn = self._hparams.tacotron_batch_size\n\t\t\tr = self._hparams.outputs_per_step\n\t\t\texamples = [self._get_next_example() for i in range(n * _batches_per_group)]\n\n\t\t\t# Bucket examples based on similar output sequence length for efficiency\n\t\t\texamples.sort(key=lambda x: x[-1])\n\t\t\tbatches = [examples[i: i+n] for i in range(0, len(examples), n)]\n\t\t\tnp.random.shuffle(batches)\n\n\t\t\tlog(""\\nGenerated {} train batches of size {} in {:.3f} sec"".format(len(batches), n, time.time() - start))\n\t\t\tfor batch in batches:\n\t\t\t\tfeed_dict = dict(zip(self._placeholders, self._prepare_batch(batch, r)))\n\t\t\t\tself._session.run(self._enqueue_op, feed_dict=feed_dict)\n\n\tdef _enqueue_next_test_group(self):\n\t\t#Create test batches once and evaluate on them for all test steps\n\t\ttest_batches, r = self.make_test_batches()\n\t\twhile not self._coord.should_stop():\n\t\t\tfor batch in test_batches:\n\t\t\t\tfeed_dict = dict(zip(self._placeholders, self._prepare_batch(batch, r)))\n\t\t\t\tself._session.run(self._eval_enqueue_op, feed_dict=feed_dict)\n\n\tdef _get_next_example(self):\n\t\t""""""Gets a single example (input, mel_target, token_target, linear_target, mel_length) from_ disk\n\t\t""""""\n\t\tif self._train_offset >= len(self._train_meta):\n\t\t\tself._train_offset = 0\n\t\t\tnp.random.shuffle(self._train_meta)\n\n\t\tmeta = self._train_meta[self._train_offset]\n\t\tself._train_offset += 1\n\n\t\ttext = meta[5]\n\n\t\tinput_data = np.asarray(text_to_sequence(text, self._cleaner_names), dtype=np.int32)\n\t\tmel_target = np.load(os.path.join(self._mel_dir, meta[1]))\n\t\t#Create parallel sequences containing zeros to represent a non finished sequence\n\t\ttoken_target = np.asarray([0.] * (len(mel_target) - 1))\n\t\tembed_target = np.load(os.path.join(self._embed_dir, meta[2]))\n\t\treturn input_data, mel_target, token_target, embed_target, len(mel_target)\n\n\tdef _prepare_batch(self, batches, outputs_per_step):\n\t\tassert 0 == len(batches) % self._hparams.tacotron_num_gpus\n\t\tsize_per_device = int(len(batches) / self._hparams.tacotron_num_gpus)\n\t\tnp.random.shuffle(batches)\n\n\t\tinputs = None\n\t\tmel_targets = None\n\t\ttoken_targets = None\n\t\ttargets_lengths = None\n\t\tsplit_infos = []\n\n\t\ttargets_lengths = np.asarray([x[-1] for x in batches], dtype=np.int32) #Used to mask loss\n\t\tinput_lengths = np.asarray([len(x[0]) for x in batches], dtype=np.int32)\n\t\t\n\t\tfor i in range(self._hparams.tacotron_num_gpus):\n\t\t\tbatch = batches[size_per_device*i:size_per_device*(i+1)]\n\t\t\tinput_cur_device, input_max_len = self._prepare_inputs([x[0] for x in batch])\n\t\t\tinputs = np.concatenate((inputs, input_cur_device), axis=1) if inputs is not None else input_cur_device\n\t\t\tmel_target_cur_device, mel_target_max_len = self._prepare_targets([x[1] for x in batch], outputs_per_step)\n\t\t\tmel_targets = np.concatenate(( mel_targets, mel_target_cur_device), axis=1) if mel_targets is not None else mel_target_cur_device\n\n\t\t\t#Pad sequences with 1 to infer that the sequence is done\n\t\t\ttoken_target_cur_device, token_target_max_len = self._prepare_token_targets([x[2] for x in batch], outputs_per_step)\n\t\t\ttoken_targets = np.concatenate((token_targets, token_target_cur_device),axis=1) if token_targets is not None else token_target_cur_device\n\t\t\tsplit_infos.append([input_max_len, mel_target_max_len, token_target_max_len])\n\n\t\tsplit_infos = np.asarray(split_infos, dtype=np.int32)\n\t\t\n\t\t### SV2TTS ###\n\t\t\n\t\tembed_targets = np.asarray([x[3] for x in batches])\n\t\t\n\t\t##############\n\t\t\n\t\treturn inputs, input_lengths, mel_targets, token_targets, targets_lengths, \\\n\t\t\t   split_infos, embed_targets\n\n\tdef _prepare_inputs(self, inputs):\n\t\tmax_len = max([len(x) for x in inputs])\n\t\treturn np.stack([self._pad_input(x, max_len) for x in inputs]), max_len\n\n\tdef _prepare_targets(self, targets, alignment):\n\t\tmax_len = max([len(t) for t in targets])\n\t\tdata_len = self._round_up(max_len, alignment)\n\t\treturn np.stack([self._pad_target(t, data_len) for t in targets]), data_len\n\n\tdef _prepare_token_targets(self, targets, alignment):\n\t\tmax_len = max([len(t) for t in targets]) + 1\n\t\tdata_len = self._round_up(max_len, alignment)\n\t\treturn np.stack([self._pad_token_target(t, data_len) for t in targets]), data_len\n\n\tdef _pad_input(self, x, length):\n\t\treturn np.pad(x, (0, length - x.shape[0]), mode=""constant"", constant_values=self._pad)\n\n\tdef _pad_target(self, t, length):\n\t\treturn np.pad(t, [(0, length - t.shape[0]), (0, 0)], mode=""constant"", constant_values=self._target_pad)\n\n\tdef _pad_token_target(self, t, length):\n\t\treturn np.pad(t, (0, length - t.shape[0]), mode=""constant"", constant_values=self._token_pad)\n\n\tdef _round_up(self, x, multiple):\n\t\tremainder = x % multiple\n\t\treturn x if remainder == 0 else x + multiple - remainder\n\n\tdef _round_down(self, x, multiple):\n\t\tremainder = x % multiple\n\t\treturn x if remainder == 0 else x - remainder\n'"
synthesizer/hparams.py,0,"b'from tensorflow.contrib.training import HParams\n\n# Default hyperparameters\nhparams = HParams(\n    # Comma-separated list of cleaners to run on text prior to training and eval. For non-English\n    # text, you may want to use ""basic_cleaners"" or ""transliteration_cleaners"".\n    cleaners=""english_cleaners"",\n    \n    # If you only have 1 GPU or want to use only one GPU, please set num_gpus=0 and specify the \n    # GPU idx on run. example:\n    # expample 1 GPU of index 2 (train on ""/gpu2"" only): CUDA_VISIBLE_DEVICES=2 python train.py \n    # --model=""Tacotron"" --hparams=""tacotron_gpu_start_idx=2""\n    # If you want to train on multiple GPUs, simply specify the number of GPUs available, \n    # and the idx of the first GPU to use. example:\n    # example 4 GPUs starting from index 0 (train on ""/gpu0""->""/gpu3""): python train.py \n    # --model=""Tacotron"" --hparams=""tacotron_num_gpus=4, tacotron_gpu_start_idx=0""\n    # The hparams arguments can be directly modified on this hparams.py file instead of being \n    # specified on run if preferred!\n    \n    # If one wants to train both Tacotron and WaveNet in parallel (provided WaveNet will be \n    # trained on True mel spectrograms), one needs to specify different GPU idxes.\n    # example Tacotron+WaveNet on a machine with 4 or plus GPUs. Two GPUs for each model: \n    # CUDA_VISIBLE_DEVICES=0,1 python train.py --model=""Tacotron"" \n\t# --hparams=""tacotron_gpu_start_idx=0, tacotron_num_gpus=2""\n    # Cuda_VISIBLE_DEVICES=2,3 python train.py --model=""WaveNet"" \n\t# --hparams=""wavenet_gpu_start_idx=2; wavenet_num_gpus=2""\n    \n    # IMPORTANT NOTE: If using N GPUs, please multiply the tacotron_batch_size by N below in the \n    # hparams! (tacotron_batch_size = 32 * N)\n    # Never use lower batch size than 32 on a single GPU!\n    # Same applies for Wavenet: wavenet_batch_size = 8 * N (wavenet_batch_size can be smaller than\n    #  8 if GPU is having OOM, minimum 2)\n    # Please also apply the synthesis batch size modification likewise. (if N GPUs are used for \n    # synthesis, minimal batch size must be N, minimum of 1 sample per GPU)\n    # We did not add an automatic multi-GPU batch size computation to avoid confusion in the \n    # user""s mind and to provide more control to the user for\n    # resources related decisions.\n    \n    # Acknowledgement:\n    #\tMany thanks to @MlWoo for his awesome work on multi-GPU Tacotron which showed to work a \n\t# little faster than the original\n    #\tpipeline for a single GPU as well. Great work!\n    \n    # Hardware setup: Default supposes user has only one GPU: ""/gpu:0"" (Tacotron only for now! \n    # WaveNet does not support multi GPU yet, WIP)\n    # Synthesis also uses the following hardware parameters for multi-GPU parallel synthesis.\n    tacotron_gpu_start_idx=0,  # idx of the first GPU to be used for Tacotron training.\n    tacotron_num_gpus=1,  # Determines the number of gpus in use for Tacotron training.\n    split_on_cpu=True,\n    # Determines whether to split data on CPU or on first GPU. This is automatically True when \n\t# more than 1 GPU is used.\n    ###########################################################################################################################################\n    \n    # Audio\n    # Audio parameters are the most important parameters to tune when using this work on your \n    # personal data. Below are the beginner steps to adapt\n    # this work to your personal data:\n    #\t1- Determine my data sample rate: First you need to determine your audio sample_rate (how \n\t# many samples are in a second of audio). This can be done using sox: ""sox --i <filename>""\n    #\t\t(For this small tuto, I will consider 24kHz (24000 Hz), and defaults are 22050Hz, \n\t# so there are plenty of examples to refer to)\n    #\t2- set sample_rate parameter to your data correct sample rate\n    #\t3- Fix win_size and and hop_size accordingly: (Supposing you will follow our advice: 50ms \n\t# window_size, and 12.5ms frame_shift(hop_size))\n    #\t\ta- win_size = 0.05 * sample_rate. In the tuto example, 0.05 * 24000 = 1200\n    #\t\tb- hop_size = 0.25 * win_size. Also equal to 0.0125 * sample_rate. In the tuto \n\t# example, 0.25 * 1200 = 0.0125 * 24000 = 300 (Can set frame_shift_ms=12.5 instead)\n    #\t4- Fix n_fft, num_freq and upsample_scales parameters accordingly.\n    #\t\ta- n_fft can be either equal to win_size or the first power of 2 that comes after \n\t# win_size. I usually recommend using the latter\n    #\t\t\tto be more consistent with signal processing friends. No big difference to be seen\n\t#  however. For the tuto example: n_fft = 2048 = 2**11\n    #\t\tb- num_freq = (n_fft / 2) + 1. For the tuto example: num_freq = 2048 / 2 + 1 = 1024 + \n\t# 1 = 1025.\n    #\t\tc- For WaveNet, upsample_scales products must be equal to hop_size. For the tuto \n\t# example: upsample_scales=[15, 20] where 15 * 20 = 300\n    #\t\t\tit is also possible to use upsample_scales=[3, 4, 5, 5] instead. One must only \n\t# keep in mind that upsample_kernel_size[0] = 2*upsample_scales[0]\n    #\t\t\tso the training segments should be long enough (2.8~3x upsample_scales[0] * \n\t# hop_size or longer) so that the first kernel size can see the middle \n    #\t\t\tof the samples efficiently. The length of WaveNet training segments is under the \n\t# parameter ""max_time_steps"".\n    #\t5- Finally comes the silence trimming. This very much data dependent, so I suggest trying \n\t# preprocessing (or part of it, ctrl-C to stop), then use the\n    #\t\t.ipynb provided in the repo to listen to some inverted mel/linear spectrograms. That \n\t# will first give you some idea about your above parameters, and\n    #\t\tit will also give you an idea about trimming. If silences persist, try reducing \n\t# trim_top_db slowly. If samples are trimmed mid words, try increasing it.\n    #\t6- If audio quality is too metallic or fragmented (or if linear spectrogram plots are \n\t# showing black silent regions on top), then restart from step 2.\n    num_mels=80,  # Number of mel-spectrogram channels and local conditioning dimensionality\n    #  network\n    rescale=True,  # Whether to rescale audio prior to preprocessing\n    rescaling_max=0.9,  # Rescaling value\n    # Whether to clip silence in Audio (at beginning and end of audio only, not the middle)\n    # train samples of lengths between 3sec and 14sec are more than enough to make a model capable\n    # of good parallelization.\n    clip_mels_length=True,\n    # For cases of OOM (Not really recommended, only use if facing unsolvable OOM errors, \n\t# also consider clipping your samples to smaller chunks)\n    max_mel_frames=900,\n    # Only relevant when clip_mels_length = True, please only use after trying output_per_steps=3\n\t#  and still getting OOM errors.\n    \n    # Use LWS (https://github.com/Jonathan-LeRoux/lws) for STFT and phase reconstruction\n    # It""s preferred to set True to use with https://github.com/r9y9/wavenet_vocoder\n    # Does not work if n_ffit is not multiple of hop_size!!\n    use_lws=False,\n    # Only used to set as True if using WaveNet, no difference in performance is observed in \n    # either cases.\n    silence_threshold=2,  # silence threshold used for sound trimming for wavenet preprocessing\n    \n    # Mel spectrogram  \n    n_fft=800,  # Extra window size is filled with 0 paddings to match this parameter\n    hop_size=200,  # For 16000Hz, 200 = 12.5 ms (0.0125 * sample_rate)\n    win_size=800,  # For 16000Hz, 800 = 50 ms (If None, win_size = n_fft) (0.05 * sample_rate)\n    sample_rate=16000,  # 16000Hz (corresponding to librispeech) (sox --i <filename>)\n    \n    frame_shift_ms=None,  # Can replace hop_size parameter. (Recommended: 12.5)\n    \n    # M-AILABS (and other datasets) trim params (these parameters are usually correct for any \n\t# data, but definitely must be tuned for specific speakers)\n    trim_fft_size=512,\n    trim_hop_size=128,\n    trim_top_db=23,\n    \n    # Mel and Linear spectrograms normalization/scaling and clipping\n    signal_normalization=True,\n    # Whether to normalize mel spectrograms to some predefined range (following below parameters)\n    allow_clipping_in_normalization=True,  # Only relevant if mel_normalization = True\n    symmetric_mels=True,\n    # Whether to scale the data to be symmetric around 0. (Also multiplies the output range by 2, \n    # faster and cleaner convergence)\n    max_abs_value=4.,\n    # max absolute value of data. If symmetric, data will be [-max, max] else [0, max] (Must not \n    # be too big to avoid gradient explosion, \n    # not too small for fast convergence)\n    normalize_for_wavenet=True,\n    # whether to rescale to [0, 1] for wavenet. (better audio quality)\n    clip_for_wavenet=True,\n    # whether to clip [-max, max] before training/synthesizing with wavenet (better audio quality)\n    \n    # Contribution by @begeekmyfriend\n    # Spectrogram Pre-Emphasis (Lfilter: Reduce spectrogram noise and helps model certitude \n\t# levels. Also allows for better G&L phase reconstruction)\n    preemphasize=True,  # whether to apply filter\n    preemphasis=0.97,  # filter coefficient.\n    \n    # Limits\n    min_level_db=-100,\n    ref_level_db=20,\n    fmin=55,\n    # Set this to 55 if your speaker is male! if female, 95 should help taking off noise. (To \n\t# test depending on dataset. Pitch info: male~[65, 260], female~[100, 525])\n    fmax=7600,  # To be increased/reduced depending on data.\n    \n    # Griffin Lim\n    power=1.5,\n    # Only used in G&L inversion, usually values between 1.2 and 1.5 are a good choice.\n    griffin_lim_iters=60,\n    # Number of G&L iterations, typically 30 is enough but we use 60 to ensure convergence.\n    ###########################################################################################################################################\n    \n    # Tacotron\n    outputs_per_step=2, # Was 1\n    # number of frames to generate at each decoding step (increase to speed up computation and \n    # allows for higher batch size, decreases G&L audio quality)\n    stop_at_any=True,\n    # Determines whether the decoder should stop when predicting <stop> to any frame or to all of \n    # them (True works pretty well)\n    \n    embedding_dim=512,  # dimension of embedding space (these are NOT the speaker embeddings)\n    \n    # Encoder parameters\n    enc_conv_num_layers=3,  # number of encoder convolutional layers\n    enc_conv_kernel_size=(5,),  # size of encoder convolution filters for each layer\n    enc_conv_channels=512,  # number of encoder convolutions filters for each layer\n    encoder_lstm_units=256,  # number of lstm units for each direction (forward and backward)\n    \n    # Attention mechanism\n    smoothing=False,  # Whether to smooth the attention normalization function\n    attention_dim=128,  # dimension of attention space\n    attention_filters=32,  # number of attention convolution filters\n    attention_kernel=(31,),  # kernel size of attention convolution\n    cumulative_weights=True,\n    # Whether to cumulate (sum) all previous attention weights or simply feed previous weights (\n    # Recommended: True)\n    \n    # Decoder\n    prenet_layers=[256, 256],  # number of layers and number of units of prenet\n    decoder_layers=2,  # number of decoder lstm layers\n    decoder_lstm_units=1024,  # number of decoder lstm units on each layer\n    max_iters=2000,\n    # Max decoder steps during inference (Just for safety from infinite loop cases)\n    \n    # Residual postnet\n    postnet_num_layers=5,  # number of postnet convolutional layers\n    postnet_kernel_size=(5,),  # size of postnet convolution filters for each layer\n    postnet_channels=512,  # number of postnet convolution filters for each layer\n    \n    # CBHG mel->linear postnet\n    cbhg_kernels=8,\n    # All kernel sizes from 1 to cbhg_kernels will be used in the convolution bank of CBHG to act\n    #  as ""K-grams""\n    cbhg_conv_channels=128,  # Channels of the convolution bank\n    cbhg_pool_size=2,  # pooling size of the CBHG\n    cbhg_projection=256,\n    # projection channels of the CBHG (1st projection, 2nd is automatically set to num_mels)\n    cbhg_projection_kernel_size=3,  # kernel_size of the CBHG projections\n    cbhg_highwaynet_layers=4,  # Number of HighwayNet layers\n    cbhg_highway_units=128,  # Number of units used in HighwayNet fully connected layers\n    cbhg_rnn_units=128,\n    # Number of GRU units used in bidirectional RNN of CBHG block. CBHG output is 2x rnn_units in \n    # shape\n    \n    # Loss params\n    mask_encoder=True,\n    # whether to mask encoder padding while computing attention. Set to True for better prosody \n    # but slower convergence.\n    mask_decoder=False,\n    # Whether to use loss mask for padded sequences (if False, <stop_token> loss function will not\n    #  be weighted, else recommended pos_weight = 20)\n    cross_entropy_pos_weight=20,\n    # Use class weights to reduce the stop token classes imbalance (by adding more penalty on \n    # False Negatives (FN)) (1 = disabled)\n    predict_linear=False,\n    # Whether to add a post-processing network to the Tacotron to predict linear spectrograms (\n\t# True mode Not tested!!)\n    ###########################################################################################################################################\n\n    # Tacotron Training\n    # Reproduction seeds\n    tacotron_random_seed=5339,\n    # Determines initial graph and operations (i.e: model) random state for reproducibility\n    tacotron_data_random_state=1234,  # random state for train test split repeatability\n    \n    # performance parameters\n    tacotron_swap_with_cpu=False,\n    # Whether to use cpu as support to gpu for decoder computation (Not recommended: may cause \n    # major slowdowns! Only use when critical!)\n    \n    # train/test split ratios, mini-batches sizes\n    tacotron_batch_size=36,  # number of training samples on each training steps (was 32)\n    # Tacotron Batch synthesis supports ~16x the training batch size (no gradients during \n    # testing). \n    # Training Tacotron with unmasked paddings makes it aware of them, which makes synthesis times\n    #  different from training. We thus recommend masking the encoder.\n    tacotron_synthesis_batch_size=128,\n    # DO NOT MAKE THIS BIGGER THAN 1 IF YOU DIDN""T TRAIN TACOTRON WITH ""mask_encoder=True""!!\n    tacotron_test_size=0.05,\n    # % of data to keep as test data, if None, tacotron_test_batches must be not None. (5% is \n\t# enough to have a good idea about overfit)\n    tacotron_test_batches=None,  # number of test batches.\n    \n    # Learning rate schedule\n    tacotron_decay_learning_rate=True,\n    # boolean, determines if the learning rate will follow an exponential decay\n    tacotron_start_decay=50000,  # Step at which learning decay starts\n    tacotron_decay_steps=50000,  # Determines the learning rate decay slope (UNDER TEST)\n    tacotron_decay_rate=0.5,  # learning rate decay rate (UNDER TEST)\n    tacotron_initial_learning_rate=1e-3,  # starting learning rate\n    tacotron_final_learning_rate=1e-5,  # minimal learning rate\n    \n    # Optimization parameters\n    tacotron_adam_beta1=0.9,  # AdamOptimizer beta1 parameter\n    tacotron_adam_beta2=0.999,  # AdamOptimizer beta2 parameter\n    tacotron_adam_epsilon=1e-6,  # AdamOptimizer Epsilon parameter\n    \n    # Regularization parameters\n    tacotron_reg_weight=1e-7,  # regularization weight (for L2 regularization)\n    tacotron_scale_regularization=False,\n    # Whether to rescale regularization weight to adapt for outputs range (used when reg_weight is\n    #  high and biasing the model)\n    tacotron_zoneout_rate=0.1,  # zoneout rate for all LSTM cells in the network\n    tacotron_dropout_rate=0.5,  # dropout rate for all convolutional layers + prenet\n    tacotron_clip_gradients=True,  # whether to clip gradients\n    \n    # Evaluation parameters\n    natural_eval=False,\n    # Whether to use 100% natural eval (to evaluate Curriculum Learning performance) or with same\n\t#  teacher-forcing ratio as in training (just for overfit)\n    \n    # Decoder RNN learning can take be done in one of two ways:\n    #\tTeacher Forcing: vanilla teacher forcing (usually with ratio = 1). mode=""constant""\n    #\tCurriculum Learning Scheme: From Teacher-Forcing to sampling from previous outputs is \n    # function of global step. (teacher forcing ratio decay) mode=""scheduled""\n    # The second approach is inspired by:\n    # Bengio et al. 2015: Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks.\n    # Can be found under: https://arxiv.org/pdf/1506.03099.pdf\n    tacotron_teacher_forcing_mode=""constant"",\n    # Can be (""constant"" or ""scheduled""). ""scheduled"" mode applies a cosine teacher forcing ratio \n    # decay. (Preference: scheduled)\n    tacotron_teacher_forcing_ratio=1.,\n    # Value from [0., 1.], 0.=0%, 1.=100%, determines the % of times we force next decoder \n\t# inputs, Only relevant if mode=""constant""\n    tacotron_teacher_forcing_init_ratio=1.,\n    # initial teacher forcing ratio. Relevant if mode=""scheduled""\n    tacotron_teacher_forcing_final_ratio=0.,\n    # final teacher forcing ratio. Relevant if mode=""scheduled""\n    tacotron_teacher_forcing_start_decay=10000,\n    # starting point of teacher forcing ratio decay. Relevant if mode=""scheduled""\n    tacotron_teacher_forcing_decay_steps=280000,\n    # Determines the teacher forcing ratio decay slope. Relevant if mode=""scheduled""\n    tacotron_teacher_forcing_decay_alpha=0.,\n    # teacher forcing ratio decay rate. Relevant if mode=""scheduled""\n    ###########################################################################################################################################\n \n    # Tacotron-2 integration parameters\n    train_with_GTA=False,\n    # Whether to use GTA mels to train WaveNet instead of ground truth mels.\n    ###########################################################################################################################################\n    \n    # Eval sentences (if no eval text file was specified during synthesis, these sentences are \n\t# used for eval)\n    sentences=[\n        # From July 8, 2017 New York Times:\n        ""Scientists at the CERN laboratory say they have discovered a new particle."",\n        ""There\\""s a way to measure the acute emotional intelligence that has never gone out of ""\n\t\t""style."",\n        ""President Trump met with other leaders at the Group of 20 conference."",\n        ""The Senate\\""s bill to repeal and replace the Affordable Care Act is now imperiled."",\n        # From Google""s Tacotron example page:\n        ""Generative adversarial network or variational auto-encoder."",\n        ""Basilar membrane and otolaryngology are not auto-correlations."",\n        ""He has read the whole thing."",\n        ""He reads books."",\n        ""He thought it was time to present the present."",\n        ""Thisss isrealy awhsome."",\n        ""Punctuation sensitivity, is working."",\n        ""Punctuation sensitivity is working."",\n        ""Peter Piper picked a peck of pickled peppers. How many pickled peppers did Peter Piper pick?"",\n        ""She sells sea-shells on the sea-shore. The shells she sells are sea-shells I\'m sure."",\n        ""Tajima Airport serves Toyooka."",\n        # From The web (random long utterance)\n        ""Sequence to sequence models have enjoyed great success in a variety of tasks such as machine translation, speech recognition, and text summarization.\\\n        This project covers a sequence to sequence model trained to predict a speech representation from an input sequence of characters. We show that\\\n        the adopted architecture is able to perform this task with wild success."",\n        ""Thank you so much for your support!"",\n    ],\n    \n    \n    ### SV2TTS ###\n    speaker_embedding_size=256,\n    silence_min_duration_split=0.4, # Duration in seconds of a silence for an utterance to be split\n    utterance_min_duration=1.6,     # Duration in seconds below which utterances are discarded\n    \n)\n\n\ndef hparams_debug_string():\n    values = hparams.values()\n    hp = [""  %s: %s"" % (name, values[name]) for name in sorted(values) if name != ""sentences""]\n    return ""Hyperparameters:\\n"" + ""\\n"".join(hp)\n'"
synthesizer/inference.py,0,"b'from synthesizer.tacotron2 import Tacotron2\nfrom synthesizer.hparams import hparams\nfrom multiprocess.pool import Pool  # You\'re free to use either one\n#from multiprocessing import Pool   # \nfrom synthesizer import audio\nfrom pathlib import Path\nfrom typing import Union, List\nimport tensorflow as tf\nimport numpy as np\nimport numba.cuda\nimport librosa\n\n\nclass Synthesizer:\n    sample_rate = hparams.sample_rate\n    hparams = hparams\n    \n    def __init__(self, checkpoints_dir: Path, verbose=True, low_mem=False):\n        """"""\n        Creates a synthesizer ready for inference. The actual model isn\'t loaded in memory until\n        needed or until load() is called.\n        \n        :param checkpoints_dir: path to the directory containing the checkpoint file as well as the\n        weight files (.data, .index and .meta files)\n        :param verbose: if False, only tensorflow\'s output will be printed TODO: suppress them too\n        :param low_mem: if True, the model will be loaded in a separate process and its resources \n        will be released after each usage. Adds a large overhead, only recommended if your GPU \n        memory is low (<= 2gb)\n        """"""\n        self.verbose = verbose\n        self._low_mem = low_mem\n        \n        # Prepare the model\n        self._model = None  # type: Tacotron2\n        checkpoint_state = tf.train.get_checkpoint_state(checkpoints_dir)\n        if checkpoint_state is None:\n            raise Exception(""Could not find any synthesizer weights under %s"" % checkpoints_dir)\n        self.checkpoint_fpath = checkpoint_state.model_checkpoint_path\n        if verbose:\n            model_name = checkpoints_dir.parent.name.replace(""logs-"", """")\n            step = int(self.checkpoint_fpath[self.checkpoint_fpath.rfind(\'-\') + 1:])\n            print(""Found synthesizer \\""%s\\"" trained to step %d"" % (model_name, step))\n     \n    def is_loaded(self):\n        """"""\n        Whether the model is loaded in GPU memory.\n        """"""\n        return self._model is not None\n    \n    def load(self):\n        """"""\n        Effectively loads the model to GPU memory given the weights file that was passed in the\n        constructor.\n        """"""\n        if self._low_mem:\n            raise Exception(""Cannot load the synthesizer permanently in low mem mode"")\n        tf.reset_default_graph()\n        self._model = Tacotron2(self.checkpoint_fpath, hparams)\n            \n    def synthesize_spectrograms(self, texts: List[str],\n                                embeddings: Union[np.ndarray, List[np.ndarray]],\n                                return_alignments=False):\n        """"""\n        Synthesizes mel spectrograms from texts and speaker embeddings.\n\n        :param texts: a list of N text prompts to be synthesized\n        :param embeddings: a numpy array or list of speaker embeddings of shape (N, 256) \n        :param return_alignments: if True, a matrix representing the alignments between the \n        characters\n        and each decoder output step will be returned for each spectrogram\n        :return: a list of N melspectrograms as numpy arrays of shape (80, Mi), where Mi is the \n        sequence length of spectrogram i, and possibly the alignments.\n        """"""\n        if not self._low_mem:\n            # Usual inference mode: load the model on the first request and keep it loaded.\n            if not self.is_loaded():\n                self.load()\n            specs, alignments = self._model.my_synthesize(embeddings, texts)\n        else:\n            # Low memory inference mode: load the model upon every request. The model has to be \n            # loaded in a separate process to be able to release GPU memory (a simple workaround \n            # to tensorflow\'s intricacies)\n            specs, alignments = Pool(1).starmap(Synthesizer._one_shot_synthesize_spectrograms, \n                                                [(self.checkpoint_fpath, embeddings, texts)])[0]\n    \n        return (specs, alignments) if return_alignments else specs\n\n    @staticmethod\n    def _one_shot_synthesize_spectrograms(checkpoint_fpath, embeddings, texts):\n        # Load the model and forward the inputs\n        tf.reset_default_graph()\n        model = Tacotron2(checkpoint_fpath, hparams)\n        specs, alignments = model.my_synthesize(embeddings, texts)\n        \n        # Detach the outputs (not doing so will cause the process to hang)\n        specs, alignments = [spec.copy() for spec in specs], alignments.copy()\n        \n        # Close cuda for this process\n        model.session.close()\n        numba.cuda.select_device(0)\n        numba.cuda.close()\n        \n        return specs, alignments\n\n    @staticmethod\n    def load_preprocess_wav(fpath):\n        """"""\n        Loads and preprocesses an audio file under the same conditions the audio files were used to\n        train the synthesizer. \n        """"""\n        wav = librosa.load(fpath, hparams.sample_rate)[0]\n        if hparams.rescale:\n            wav = wav / np.abs(wav).max() * hparams.rescaling_max\n        return wav\n\n    @staticmethod\n    def make_spectrogram(fpath_or_wav: Union[str, Path, np.ndarray]):\n        """"""\n        Creates a mel spectrogram from an audio file in the same manner as the mel spectrograms that \n        were fed to the synthesizer when training.\n        """"""\n        if isinstance(fpath_or_wav, str) or isinstance(fpath_or_wav, Path):\n            wav = Synthesizer.load_preprocess_wav(fpath_or_wav)\n        else:\n            wav = fpath_or_wav\n        \n        mel_spectrogram = audio.melspectrogram(wav, hparams).astype(np.float32)\n        return mel_spectrogram\n    \n    @staticmethod\n    def griffin_lim(mel):\n        """"""\n        Inverts a mel spectrogram using Griffin-Lim. The mel spectrogram is expected to have been built\n        with the same parameters present in hparams.py.\n        """"""\n        return audio.inv_mel_spectrogram(mel, hparams)\n    '"
synthesizer/infolog.py,0,"b'import atexit\nimport json\nfrom datetime import datetime\nfrom threading import Thread\nfrom urllib.request import Request, urlopen\n\n_format = ""%Y-%m-%d %H:%M:%S.%f""\n_file = None\n_run_name = None\n_slack_url = None\n\n\ndef init(filename, run_name, slack_url=None):\n\tglobal _file, _run_name, _slack_url\n\t_close_logfile()\n\t_file = open(filename, ""a"")\n\t_file = open(filename, ""a"")\n\t_file.write(""\\n-----------------------------------------------------------------\\n"")\n\t_file.write(""Starting new {} training run\\n"".format(run_name))\n\t_file.write(""-----------------------------------------------------------------\\n"")\n\t_run_name = run_name\n\t_slack_url = slack_url\n\n\ndef log(msg, end=""\\n"", slack=False):\n\tprint(msg, end=end)\n\tif _file is not None:\n\t\t_file.write(""[%s]  %s\\n"" % (datetime.now().strftime(_format)[:-3], msg))\n\tif slack and _slack_url is not None:\n\t\tThread(target=_send_slack, args=(msg,)).start()\n\n\ndef _close_logfile():\n\tglobal _file\n\tif _file is not None:\n\t\t_file.close()\n\t\t_file = None\n\n\ndef _send_slack(msg):\n\treq = Request(_slack_url)\n\treq.add_header(""Content-Type"", ""application/json"")\n\turlopen(req, json.dumps({\n\t\t""username"": ""tacotron"",\n\t\t""icon_emoji"": "":taco:"",\n\t\t""text"": ""*%s*: %s"" % (_run_name, msg)\n\t}).encode())\n\n\natexit.register(_close_logfile)\n'"
synthesizer/preprocess.py,0,"b'from multiprocessing.pool import Pool \nfrom synthesizer import audio\nfrom functools import partial\nfrom itertools import chain\nfrom encoder import inference as encoder\nfrom pathlib import Path\nfrom utils import logmmse\nfrom tqdm import tqdm\nimport numpy as np\nimport librosa\n\n\ndef preprocess_librispeech(datasets_root: Path, out_dir: Path, n_processes: int, \n                           skip_existing: bool, hparams):\n    # Gather the input directories\n    dataset_root = datasets_root.joinpath(""LibriSpeech"")\n    input_dirs = [dataset_root.joinpath(""train-clean-100""), \n                  dataset_root.joinpath(""train-clean-360"")]\n    print(""\\n    "".join(map(str, [""Using data from:""] + input_dirs)))\n    assert all(input_dir.exists() for input_dir in input_dirs)\n    \n    # Create the output directories for each output file type\n    out_dir.joinpath(""mels"").mkdir(exist_ok=True)\n    out_dir.joinpath(""audio"").mkdir(exist_ok=True)\n    \n    # Create a metadata file\n    metadata_fpath = out_dir.joinpath(""train.txt"")\n    metadata_file = metadata_fpath.open(""a"" if skip_existing else ""w"", encoding=""utf-8"")\n\n    # Preprocess the dataset\n    speaker_dirs = list(chain.from_iterable(input_dir.glob(""*"") for input_dir in input_dirs))\n    func = partial(preprocess_speaker, out_dir=out_dir, skip_existing=skip_existing, \n                   hparams=hparams)\n    job = Pool(n_processes).imap(func, speaker_dirs)\n    for speaker_metadata in tqdm(job, ""LibriSpeech"", len(speaker_dirs), unit=""speakers""):\n        for metadatum in speaker_metadata:\n            metadata_file.write(""|"".join(str(x) for x in metadatum) + ""\\n"")\n    metadata_file.close()\n\n    # Verify the contents of the metadata file\n    with metadata_fpath.open(""r"", encoding=""utf-8"") as metadata_file:\n        metadata = [line.split(""|"") for line in metadata_file]\n    mel_frames = sum([int(m[4]) for m in metadata])\n    timesteps = sum([int(m[3]) for m in metadata])\n    sample_rate = hparams.sample_rate\n    hours = (timesteps / sample_rate) / 3600\n    print(""The dataset consists of %d utterances, %d mel frames, %d audio timesteps (%.2f hours)."" %\n          (len(metadata), mel_frames, timesteps, hours))\n    print(""Max input length (text chars): %d"" % max(len(m[5]) for m in metadata))\n    print(""Max mel frames length: %d"" % max(int(m[4]) for m in metadata))\n    print(""Max audio timesteps length: %d"" % max(int(m[3]) for m in metadata))\n\n\ndef preprocess_speaker(speaker_dir, out_dir: Path, skip_existing: bool, hparams):\n    metadata = []\n    for book_dir in speaker_dir.glob(""*""):\n        # Gather the utterance audios and texts\n        try:\n            alignments_fpath = next(book_dir.glob(""*.alignment.txt""))\n            with alignments_fpath.open(""r"") as alignments_file:\n                alignments = [line.rstrip().split("" "") for line in alignments_file]\n        except StopIteration:\n            # A few alignment files will be missing\n            continue\n        \n        # Iterate over each entry in the alignments file\n        for wav_fname, words, end_times in alignments:\n            wav_fpath = book_dir.joinpath(wav_fname + "".flac"")\n            assert wav_fpath.exists()\n            words = words.replace(""\\"""", """").split("","")\n            end_times = list(map(float, end_times.replace(""\\"""", """").split("","")))\n            \n            # Process each sub-utterance\n            wavs, texts = split_on_silences(wav_fpath, words, end_times, hparams)\n            for i, (wav, text) in enumerate(zip(wavs, texts)):\n                sub_basename = ""%s_%02d"" % (wav_fname, i)\n                metadata.append(process_utterance(wav, text, out_dir, sub_basename, \n                                                  skip_existing, hparams))\n    \n    return [m for m in metadata if m is not None]\n\n\ndef split_on_silences(wav_fpath, words, end_times, hparams):\n    # Load the audio waveform\n    wav, _ = librosa.load(wav_fpath, hparams.sample_rate)\n    if hparams.rescale:\n        wav = wav / np.abs(wav).max() * hparams.rescaling_max\n    \n    words = np.array(words)\n    start_times = np.array([0.0] + end_times[:-1])\n    end_times = np.array(end_times)\n    assert len(words) == len(end_times) == len(start_times)\n    assert words[0] == """" and words[-1] == """"\n    \n    # Find pauses that are too long\n    mask = (words == """") & (end_times - start_times >= hparams.silence_min_duration_split)\n    mask[0] = mask[-1] = True\n    breaks = np.where(mask)[0]\n\n    # Profile the noise from the silences and perform noise reduction on the waveform\n    silence_times = [[start_times[i], end_times[i]] for i in breaks]\n    silence_times = (np.array(silence_times) * hparams.sample_rate).astype(np.int)\n    noisy_wav = np.concatenate([wav[stime[0]:stime[1]] for stime in silence_times])\n    if len(noisy_wav) > hparams.sample_rate * 0.02:\n        profile = logmmse.profile_noise(noisy_wav, hparams.sample_rate)\n        wav = logmmse.denoise(wav, profile, eta=0)\n    \n    # Re-attach segments that are too short\n    segments = list(zip(breaks[:-1], breaks[1:]))\n    segment_durations = [start_times[end] - end_times[start] for start, end in segments]\n    i = 0\n    while i < len(segments) and len(segments) > 1:\n        if segment_durations[i] < hparams.utterance_min_duration:\n            # See if the segment can be re-attached with the right or the left segment\n            left_duration = float(""inf"") if i == 0 else segment_durations[i - 1]\n            right_duration = float(""inf"") if i == len(segments) - 1 else segment_durations[i + 1]\n            joined_duration = segment_durations[i] + min(left_duration, right_duration)\n\n            # Do not re-attach if it causes the joined utterance to be too long\n            if joined_duration > hparams.hop_size * hparams.max_mel_frames / hparams.sample_rate:\n                i += 1\n                continue\n\n            # Re-attach the segment with the neighbour of shortest duration\n            j = i - 1 if left_duration <= right_duration else i\n            segments[j] = (segments[j][0], segments[j + 1][1])\n            segment_durations[j] = joined_duration\n            del segments[j + 1], segment_durations[j + 1]\n        else:\n            i += 1\n    \n    # Split the utterance\n    segment_times = [[end_times[start], start_times[end]] for start, end in segments]\n    segment_times = (np.array(segment_times) * hparams.sample_rate).astype(np.int)\n    wavs = [wav[segment_time[0]:segment_time[1]] for segment_time in segment_times]\n    texts = ["" "".join(words[start + 1:end]).replace(""  "", "" "") for start, end in segments]\n    \n    # # DEBUG: play the audio segments (run with -n=1)\n    # import sounddevice as sd\n    # if len(wavs) > 1:\n    #     print(""This sentence was split in %d segments:"" % len(wavs))\n    # else:\n    #     print(""There are no silences long enough for this sentence to be split:"")\n    # for wav, text in zip(wavs, texts):\n    #     # Pad the waveform with 1 second of silence because sounddevice tends to cut them early\n    #     # when playing them. You shouldn\'t need to do that in your parsers.\n    #     wav = np.concatenate((wav, [0] * 16000))\n    #     print(""\\t%s"" % text)\n    #     sd.play(wav, 16000, blocking=True)\n    # print("""")\n    \n    return wavs, texts\n    \n    \ndef process_utterance(wav: np.ndarray, text: str, out_dir: Path, basename: str, \n                      skip_existing: bool, hparams):\n    ## FOR REFERENCE:\n    # For you not to lose your head if you ever wish to change things here or implement your own\n    # synthesizer.\n    # - Both the audios and the mel spectrograms are saved as numpy arrays\n    # - There is no processing done to the audios that will be saved to disk beyond volume  \n    #   normalization (in split_on_silences)\n    # - However, pre-emphasis is applied to the audios before computing the mel spectrogram. This\n    #   is why we re-apply it on the audio on the side of the vocoder.\n    # - Librosa pads the waveform before computing the mel spectrogram. Here, the waveform is saved\n    #   without extra padding. This means that you won\'t have an exact relation between the length\n    #   of the wav and of the mel spectrogram. See the vocoder data loader.\n    \n    \n    # Skip existing utterances if needed\n    mel_fpath = out_dir.joinpath(""mels"", ""mel-%s.npy"" % basename)\n    wav_fpath = out_dir.joinpath(""audio"", ""audio-%s.npy"" % basename)\n    if skip_existing and mel_fpath.exists() and wav_fpath.exists():\n        return None\n    \n    # Skip utterances that are too short\n    if len(wav) < hparams.utterance_min_duration * hparams.sample_rate:\n        return None\n    \n    # Compute the mel spectrogram\n    mel_spectrogram = audio.melspectrogram(wav, hparams).astype(np.float32)\n    mel_frames = mel_spectrogram.shape[1]\n    \n    # Skip utterances that are too long\n    if mel_frames > hparams.max_mel_frames and hparams.clip_mels_length:\n        return None\n    \n    # Write the spectrogram, embed and audio to disk\n    np.save(mel_fpath, mel_spectrogram.T, allow_pickle=False)\n    np.save(wav_fpath, wav, allow_pickle=False)\n    \n    # Return a tuple describing this training example\n    return wav_fpath.name, mel_fpath.name, ""embed-%s.npy"" % basename, len(wav), mel_frames, text\n \n \ndef embed_utterance(fpaths, encoder_model_fpath):\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n    \n \ndef create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int):\n    wav_dir = synthesizer_root.joinpath(""audio"")\n    metadata_fpath = synthesizer_root.joinpath(""train.txt"")\n    assert wav_dir.exists() and metadata_fpath.exists()\n    embed_dir = synthesizer_root.joinpath(""embeds"")\n    embed_dir.mkdir(exist_ok=True)\n    \n    # Gather the input wave filepath and the target output embed filepath\n    with metadata_fpath.open(""r"") as metadata_file:\n        metadata = [line.split(""|"") for line in metadata_file]\n        fpaths = [(wav_dir.joinpath(m[0]), embed_dir.joinpath(m[2])) for m in metadata]\n        \n    # TODO: improve on the multiprocessing, it\'s terrible. Disk I/O is the bottleneck here.\n    # Embed the utterances in separate threads\n    func = partial(embed_utterance, encoder_model_fpath=encoder_model_fpath)\n    job = Pool(n_processes).imap(func, fpaths)\n    list(tqdm(job, ""Embedding"", len(fpaths), unit=""utterances""))\n    '"
synthesizer/synthesize.py,0,"b'from synthesizer.tacotron2 import Tacotron2\nfrom synthesizer.hparams import hparams_debug_string\nfrom synthesizer.infolog import log\nimport tensorflow as tf\nfrom tqdm import tqdm\nimport time\nimport os\n\n\ndef run_eval(args, checkpoint_path, output_dir, hparams, sentences):\n    eval_dir = os.path.join(output_dir, ""eval"")\n    log_dir = os.path.join(output_dir, ""logs-eval"")\n    \n    #Create output path if it doesn""t exist\n    os.makedirs(eval_dir, exist_ok=True)\n    os.makedirs(log_dir, exist_ok=True)\n    os.makedirs(os.path.join(log_dir, ""wavs""), exist_ok=True)\n    os.makedirs(os.path.join(log_dir, ""plots""), exist_ok=True)\n    \n    log(hparams_debug_string())\n    synth = Tacotron2(checkpoint_path, hparams)\n    \n    #Set inputs batch wise\n    sentences = [sentences[i: i+hparams.tacotron_synthesis_batch_size] for i \n                 in range(0, len(sentences), hparams.tacotron_synthesis_batch_size)]\n    \n    log(""Starting Synthesis"")\n    with open(os.path.join(eval_dir, ""map.txt""), ""w"") as file:\n        for i, texts in enumerate(tqdm(sentences)):\n            start = time.time()\n            basenames = [""batch_{}_sentence_{}"".format(i, j) for j in range(len(texts))]\n            mel_filenames, speaker_ids = synth.synthesize(texts, basenames, eval_dir, log_dir, None)\n            \n            for elems in zip(texts, mel_filenames, speaker_ids):\n                file.write(""|"".join([str(x) for x in elems]) + ""\\n"")\n    log(""synthesized mel spectrograms at {}"".format(eval_dir))\n    return eval_dir\n\ndef run_synthesis(in_dir, out_dir, model_dir, hparams):\n    synth_dir = os.path.join(out_dir, ""mels_gta"")\n    os.makedirs(synth_dir, exist_ok=True)\n    metadata_filename = os.path.join(in_dir, ""train.txt"")\n    print(hparams_debug_string())\n    \n    # Load the model in memory\n    weights_dir = os.path.join(model_dir, ""taco_pretrained"")\n    checkpoint_fpath = tf.train.get_checkpoint_state(weights_dir).model_checkpoint_path\n    synth = Tacotron2(checkpoint_fpath, hparams, gta=True)\n    \n    # Load the metadata\n    with open(metadata_filename, encoding=""utf-8"") as f:\n        metadata = [line.strip().split(""|"") for line in f]\n        frame_shift_ms = hparams.hop_size / hparams.sample_rate\n        hours = sum([int(x[4]) for x in metadata]) * frame_shift_ms / 3600\n        print(""Loaded metadata for {} examples ({:.2f} hours)"".format(len(metadata), hours))\n        \n    #Set inputs batch wise\n    metadata = [metadata[i: i + hparams.tacotron_synthesis_batch_size] for i in\n                range(0, len(metadata), hparams.tacotron_synthesis_batch_size)]\n    # TODO: come on big boy, fix this\n    # Quick and dirty fix to make sure that all batches have the same size \n    metadata = metadata[:-1]\n    \n    print(""Starting Synthesis"")\n    mel_dir = os.path.join(in_dir, ""mels"")\n    embed_dir = os.path.join(in_dir, ""embeds"")\n    meta_out_fpath = os.path.join(out_dir, ""synthesized.txt"")\n    with open(meta_out_fpath, ""w"") as file:\n        for i, meta in enumerate(tqdm(metadata)):\n            texts = [m[5] for m in meta]\n            mel_filenames = [os.path.join(mel_dir, m[1]) for m in meta]\n            embed_filenames = [os.path.join(embed_dir, m[2]) for m in meta]\n            basenames = [os.path.basename(m).replace("".npy"", """").replace(""mel-"", """") \n                         for m in mel_filenames]\n            synth.synthesize(texts, basenames, synth_dir, None, mel_filenames, embed_filenames)\n            \n            for elems in meta:\n                file.write(""|"".join([str(x) for x in elems]) + ""\\n"")\n                \n    print(""Synthesized mel spectrograms at {}"".format(synth_dir))\n    return meta_out_fpath\n\n'"
synthesizer/tacotron2.py,0,"b'from synthesizer.utils.text import text_to_sequence\nfrom synthesizer.infolog import log\nfrom synthesizer.models import create_model\nfrom synthesizer.utils import plot\nfrom synthesizer import audio\nimport tensorflow as tf\nimport numpy as np\nimport os\n\n\nclass Tacotron2:\n    def __init__(self, checkpoint_path, hparams, gta=False, model_name=""Tacotron""):\n        log(""Constructing model: %s"" % model_name)\n        #Force the batch size to be known in order to use attention masking in batch synthesis\n        inputs = tf.placeholder(tf.int32, (None, None), name=""inputs"")\n        input_lengths = tf.placeholder(tf.int32, (None,), name=""input_lengths"")\n        speaker_embeddings = tf.placeholder(tf.float32, (None, hparams.speaker_embedding_size),\n                                            name=""speaker_embeddings"")\n        targets = tf.placeholder(tf.float32, (None, None, hparams.num_mels), name=""mel_targets"")\n        split_infos = tf.placeholder(tf.int32, shape=(hparams.tacotron_num_gpus, None), name=""split_infos"")\n        with tf.variable_scope(""Tacotron_model"") as scope:\n            self.model = create_model(model_name, hparams)\n            if gta:\n                self.model.initialize(inputs, input_lengths, speaker_embeddings, targets, gta=gta,\n                                      split_infos=split_infos)\n            else:\n                self.model.initialize(inputs, input_lengths, speaker_embeddings,\n                                      split_infos=split_infos)\n            \n            self.mel_outputs = self.model.tower_mel_outputs\n            self.linear_outputs = self.model.tower_linear_outputs if (hparams.predict_linear and not gta) else None\n            self.alignments = self.model.tower_alignments\n            self.stop_token_prediction = self.model.tower_stop_token_prediction\n            self.targets = targets\n        \n        self.gta = gta\n        self._hparams = hparams\n        #pad input sequences with the <pad_token> 0 ( _ )\n        self._pad = 0\n        #explicitely setting the padding to a value that doesn""t originally exist in the spectogram\n        #to avoid any possible conflicts, without affecting the output range of the model too much\n        if hparams.symmetric_mels:\n            self._target_pad = -hparams.max_abs_value\n        else:\n            self._target_pad = 0.\n        \n        self.inputs = inputs\n        self.input_lengths = input_lengths\n        self.speaker_embeddings = speaker_embeddings\n        self.targets = targets\n        self.split_infos = split_infos\n        \n        log(""Loading checkpoint: %s"" % checkpoint_path)\n        #Memory allocation on the GPUs as needed\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        config.allow_soft_placement = True\n        \n        self.session = tf.Session(config=config)\n        self.session.run(tf.global_variables_initializer())\n        \n        saver = tf.train.Saver()\n        saver.restore(self.session, checkpoint_path)\n    \n    def my_synthesize(self, speaker_embeds, texts):\n        """"""\n        Lighter synthesis function that directly returns the mel spectrograms.\n        """"""\n        \n        # Prepare the input\n        cleaner_names = [x.strip() for x in self._hparams.cleaners.split("","")]\n        seqs = [np.asarray(text_to_sequence(text, cleaner_names)) for text in texts]\n        input_lengths = [len(seq) for seq in seqs]\n        input_seqs, max_seq_len = self._prepare_inputs(seqs)\n        split_infos = [[max_seq_len, 0, 0, 0]]\n        feed_dict = {\n            self.inputs: input_seqs,\n            self.input_lengths: np.asarray(input_lengths, dtype=np.int32),\n            self.split_infos: np.asarray(split_infos, dtype=np.int32),\n            self.speaker_embeddings: speaker_embeds\n        }\n        \n        # Forward it\n        mels, alignments, stop_tokens = self.session.run(\n            [self.mel_outputs, self.alignments, self.stop_token_prediction],\n            feed_dict=feed_dict)\n        mels, alignments, stop_tokens = list(mels[0]), alignments[0], stop_tokens[0]\n        \n        # Trim the output\n        for i in range(len(mels)):\n            try:\n                target_length = list(np.round(stop_tokens[i])).index(1)\n                mels[i] = mels[i][:target_length, :]\n            except ValueError:\n                # If no token is generated, we simply do not trim the output\n                continue\n        \n        return [mel.T for mel in mels], alignments\n    \n    def synthesize(self, texts, basenames, out_dir, log_dir, mel_filenames, embed_filenames):\n        hparams = self._hparams\n        cleaner_names = [x.strip() for x in hparams.cleaners.split("","")]\n              \n        assert 0 == len(texts) % self._hparams.tacotron_num_gpus\n        seqs = [np.asarray(text_to_sequence(text, cleaner_names)) for text in texts]\n        input_lengths = [len(seq) for seq in seqs]\n        \n        size_per_device = len(seqs) // self._hparams.tacotron_num_gpus\n        \n        #Pad inputs according to each GPU max length\n        input_seqs = None\n        split_infos = []\n        for i in range(self._hparams.tacotron_num_gpus):\n            device_input = seqs[size_per_device*i: size_per_device*(i+1)]\n            device_input, max_seq_len = self._prepare_inputs(device_input)\n            input_seqs = np.concatenate((input_seqs, device_input), axis=1) if input_seqs is not None else device_input\n            split_infos.append([max_seq_len, 0, 0, 0])\n        \n        feed_dict = {\n            self.inputs: input_seqs,\n            self.input_lengths: np.asarray(input_lengths, dtype=np.int32),\n        }\n        \n        if self.gta:\n            np_targets = [np.load(mel_filename) for mel_filename in mel_filenames]\n            target_lengths = [len(np_target) for np_target in np_targets]\n            \n            #pad targets according to each GPU max length\n            target_seqs = None\n            for i in range(self._hparams.tacotron_num_gpus):\n                device_target = np_targets[size_per_device*i: size_per_device*(i+1)]\n                device_target, max_target_len = self._prepare_targets(device_target, self._hparams.outputs_per_step)\n                target_seqs = np.concatenate((target_seqs, device_target), axis=1) if target_seqs is not None else device_target\n                split_infos[i][1] = max_target_len #Not really used but setting it in case for future development maybe?\n            \n            feed_dict[self.targets] = target_seqs\n            assert len(np_targets) == len(texts)\n        \n        feed_dict[self.split_infos] = np.asarray(split_infos, dtype=np.int32)\n        feed_dict[self.speaker_embeddings] = [np.load(f) for f in embed_filenames]\n        \n        if self.gta or not hparams.predict_linear:\n            mels, alignments, stop_tokens = self.session.run(\n                [self.mel_outputs, self.alignments, self.stop_token_prediction],\n                feed_dict=feed_dict)\n            #Linearize outputs (1D arrays)\n            mels = [mel for gpu_mels in mels for mel in gpu_mels]\n            alignments = [align for gpu_aligns in alignments for align in gpu_aligns]\n            stop_tokens = [token for gpu_token in stop_tokens for token in gpu_token]\n            \n            if not self.gta:\n                #Natural batch synthesis\n                #Get Mel lengths for the entire batch from stop_tokens predictions\n                target_lengths = self._get_output_lengths(stop_tokens)\n            \n            #Take off the batch wise padding\n            mels = [mel[:target_length, :] for mel, target_length in zip(mels, target_lengths)]\n            assert len(mels) == len(texts)\n        \n        else:\n            linears, mels, alignments, stop_tokens = self.session.run(\n                [self.linear_outputs, self.mel_outputs, self.alignments,\n                 self.stop_token_prediction],\n                feed_dict=feed_dict)\n            #Linearize outputs (1D arrays)\n            linears = [linear for gpu_linear in linears for linear in gpu_linear]\n            mels = [mel for gpu_mels in mels for mel in gpu_mels]\n            alignments = [align for gpu_aligns in alignments for align in gpu_aligns]\n            stop_tokens = [token for gpu_token in stop_tokens for token in gpu_token]\n            \n            #Natural batch synthesis\n            #Get Mel/Linear lengths for the entire batch from stop_tokens predictions\n            # target_lengths = self._get_output_lengths(stop_tokens)\n            target_lengths = [9999]\n            \n            #Take off the batch wise padding\n            mels = [mel[:target_length, :] for mel, target_length in zip(mels, target_lengths)]\n            linears = [linear[:target_length, :] for linear, target_length in zip(linears, target_lengths)]\n            assert len(mels) == len(linears) == len(texts)\n        \n        if basenames is None:\n            raise NotImplemented()\n        \n        saved_mels_paths = []\n        for i, mel in enumerate(mels):\n            # Write the spectrogram to disk\n            # Note: outputs mel-spectrogram files and target ones have same names, just different folders\n            mel_filename = os.path.join(out_dir, ""mel-{}.npy"".format(basenames[i]))\n            np.save(mel_filename, mel, allow_pickle=False)\n            saved_mels_paths.append(mel_filename)\n            \n            if log_dir is not None:\n                #save wav (mel -> wav)\n                wav = audio.inv_mel_spectrogram(mel.T, hparams)\n                audio.save_wav(wav, os.path.join(log_dir, ""wavs/wav-{}-mel.wav"".format(basenames[i])), sr=hparams.sample_rate)\n                \n                #save alignments\n                plot.plot_alignment(alignments[i], os.path.join(log_dir, ""plots/alignment-{}.png"".format(basenames[i])),\n                                    title=""{}"".format(texts[i]), split_title=True, max_len=target_lengths[i])\n                \n                #save mel spectrogram plot\n                plot.plot_spectrogram(mel, os.path.join(log_dir, ""plots/mel-{}.png"".format(basenames[i])),\n                                      title=""{}"".format(texts[i]), split_title=True)\n                \n                if hparams.predict_linear:\n                    #save wav (linear -> wav)\n                    wav = audio.inv_linear_spectrogram(linears[i].T, hparams)\n                    audio.save_wav(wav, os.path.join(log_dir, ""wavs/wav-{}-linear.wav"".format(basenames[i])), sr=hparams.sample_rate)\n                    \n                    #save linear spectrogram plot\n                    plot.plot_spectrogram(linears[i], os.path.join(log_dir, ""plots/linear-{}.png"".format(basenames[i])),\n                                          title=""{}"".format(texts[i]), split_title=True, auto_aspect=True)\n        \n        return saved_mels_paths\n    \n    def _round_up(self, x, multiple):\n        remainder = x % multiple\n        return x if remainder == 0 else x + multiple - remainder\n    \n    def _prepare_inputs(self, inputs):\n        max_len = max([len(x) for x in inputs])\n        return np.stack([self._pad_input(x, max_len) for x in inputs]), max_len\n    \n    def _pad_input(self, x, length):\n        return np.pad(x, (0, length - x.shape[0]), mode=""constant"", constant_values=self._pad)\n    \n    def _prepare_targets(self, targets, alignment):\n        max_len = max([len(t) for t in targets])\n        data_len = self._round_up(max_len, alignment)\n        return np.stack([self._pad_target(t, data_len) for t in targets]), data_len\n    \n    def _pad_target(self, t, length):\n        return np.pad(t, [(0, length - t.shape[0]), (0, 0)], mode=""constant"", constant_values=self._target_pad)\n    \n    def _get_output_lengths(self, stop_tokens):\n        #Determine each mel length by the stop token predictions. (len = first occurence of 1 in stop_tokens row wise)\n        output_lengths = [row.index(1) for row in np.round(stop_tokens).tolist()]\n        return output_lengths\n'"
synthesizer/train.py,0,"b'from synthesizer.utils.symbols import symbols\nfrom synthesizer.utils.text import sequence_to_text\nfrom synthesizer.hparams import hparams_debug_string\nfrom synthesizer.feeder import Feeder\nfrom synthesizer.models import create_model\nfrom synthesizer.utils import ValueWindow, plot\nfrom synthesizer import infolog, audio\nfrom datetime import datetime\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport numpy as np\nimport traceback\nimport time\nimport os\n\nlog = infolog.log\n\n\ndef add_embedding_stats(summary_writer, embedding_names, paths_to_meta, checkpoint_path):\n    # Create tensorboard projector\n    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n    config.model_checkpoint_path = checkpoint_path\n    \n    for embedding_name, path_to_meta in zip(embedding_names, paths_to_meta):\n        # Initialize config\n        embedding = config.embeddings.add()\n        # Specifiy the embedding variable and the metadata\n        embedding.tensor_name = embedding_name\n        embedding.metadata_path = path_to_meta\n    \n    # Project the embeddings to space dimensions for visualization\n    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(summary_writer, config)\n\n\ndef add_train_stats(model, hparams):\n    with tf.variable_scope(""stats"") as scope:\n        for i in range(hparams.tacotron_num_gpus):\n            tf.summary.histogram(""mel_outputs %d"" % i, model.tower_mel_outputs[i])\n            tf.summary.histogram(""mel_targets %d"" % i, model.tower_mel_targets[i])\n        tf.summary.scalar(""before_loss"", model.before_loss)\n        tf.summary.scalar(""after_loss"", model.after_loss)\n        \n        if hparams.predict_linear:\n            tf.summary.scalar(""linear_loss"", model.linear_loss)\n            for i in range(hparams.tacotron_num_gpus):\n                tf.summary.histogram(""mel_outputs %d"" % i, model.tower_linear_outputs[i])\n                tf.summary.histogram(""mel_targets %d"" % i, model.tower_linear_targets[i])\n        \n        tf.summary.scalar(""regularization_loss"", model.regularization_loss)\n        tf.summary.scalar(""stop_token_loss"", model.stop_token_loss)\n        tf.summary.scalar(""loss"", model.loss)\n        tf.summary.scalar(""learning_rate"", model.learning_rate)  # Control learning rate decay speed\n        if hparams.tacotron_teacher_forcing_mode == ""scheduled"":\n            tf.summary.scalar(""teacher_forcing_ratio"", model.ratio)  # Control teacher forcing \n        # ratio decay when mode = ""scheduled""\n        gradient_norms = [tf.norm(grad) for grad in model.gradients]\n        tf.summary.histogram(""gradient_norm"", gradient_norms)\n        tf.summary.scalar(""max_gradient_norm"", tf.reduce_max(gradient_norms))  # visualize \n        # gradients (in case of explosion)\n        return tf.summary.merge_all()\n\n\ndef add_eval_stats(summary_writer, step, linear_loss, before_loss, after_loss, stop_token_loss,\n                   loss):\n    values = [\n        tf.Summary.Value(tag=""Tacotron_eval_model/eval_stats/eval_before_loss"",\n                         simple_value=before_loss),\n        tf.Summary.Value(tag=""Tacotron_eval_model/eval_stats/eval_after_loss"",\n                         simple_value=after_loss),\n        tf.Summary.Value(tag=""Tacotron_eval_model/eval_stats/stop_token_loss"",\n                         simple_value=stop_token_loss),\n        tf.Summary.Value(tag=""Tacotron_eval_model/eval_stats/eval_loss"", simple_value=loss),\n    ]\n    if linear_loss is not None:\n        values.append(tf.Summary.Value(tag=""Tacotron_eval_model/eval_stats/eval_linear_loss"",\n                                       simple_value=linear_loss))\n    test_summary = tf.Summary(value=values)\n    summary_writer.add_summary(test_summary, step)\n\n\ndef time_string():\n    return datetime.now().strftime(""%Y-%m-%d %H:%M"")\n\n\ndef model_train_mode(args, feeder, hparams, global_step):\n    with tf.variable_scope(""Tacotron_model"", reuse=tf.AUTO_REUSE) as scope:\n        model = create_model(""Tacotron"", hparams)\n        model.initialize(feeder.inputs, feeder.input_lengths, feeder.speaker_embeddings, \n                         feeder.mel_targets, feeder.token_targets,\n                         targets_lengths=feeder.targets_lengths, global_step=global_step,\n                         is_training=True, split_infos=feeder.split_infos)\n        model.add_loss()\n        model.add_optimizer(global_step)\n        stats = add_train_stats(model, hparams)\n        return model, stats\n\n\ndef model_test_mode(args, feeder, hparams, global_step):\n    with tf.variable_scope(""Tacotron_model"", reuse=tf.AUTO_REUSE) as scope:\n        model = create_model(""Tacotron"", hparams)\n        model.initialize(feeder.eval_inputs, feeder.eval_input_lengths, \n                         feeder.eval_speaker_embeddings, feeder.eval_mel_targets,\n                         feeder.eval_token_targets, targets_lengths=feeder.eval_targets_lengths, \n                         global_step=global_step, is_training=False, is_evaluating=True,\n                         split_infos=feeder.eval_split_infos)\n        model.add_loss()\n        return model\n\n\ndef train(log_dir, args, hparams):\n    save_dir = os.path.join(log_dir, ""taco_pretrained"")\n    plot_dir = os.path.join(log_dir, ""plots"")\n    wav_dir = os.path.join(log_dir, ""wavs"")\n    mel_dir = os.path.join(log_dir, ""mel-spectrograms"")\n    eval_dir = os.path.join(log_dir, ""eval-dir"")\n    eval_plot_dir = os.path.join(eval_dir, ""plots"")\n    eval_wav_dir = os.path.join(eval_dir, ""wavs"")\n    tensorboard_dir = os.path.join(log_dir, ""tacotron_events"")\n    meta_folder = os.path.join(log_dir, ""metas"")\n    os.makedirs(save_dir, exist_ok=True)\n    os.makedirs(plot_dir, exist_ok=True)\n    os.makedirs(wav_dir, exist_ok=True)\n    os.makedirs(mel_dir, exist_ok=True)\n    os.makedirs(eval_dir, exist_ok=True)\n    os.makedirs(eval_plot_dir, exist_ok=True)\n    os.makedirs(eval_wav_dir, exist_ok=True)\n    os.makedirs(tensorboard_dir, exist_ok=True)\n    os.makedirs(meta_folder, exist_ok=True)\n    \n    checkpoint_fpath = os.path.join(save_dir, ""tacotron_model.ckpt"")\n    metadat_fpath = os.path.join(args.synthesizer_root, ""train.txt"")\n    \n    log(""Checkpoint path: {}"".format(checkpoint_fpath))\n    log(""Loading training data from: {}"".format(metadat_fpath))\n    log(""Using model: Tacotron"")\n    log(hparams_debug_string())\n    \n    # Start by setting a seed for repeatability\n    tf.set_random_seed(hparams.tacotron_random_seed)\n    \n    # Set up data feeder\n    coord = tf.train.Coordinator()\n    with tf.variable_scope(""datafeeder"") as scope:\n        feeder = Feeder(coord, metadat_fpath, hparams)\n    \n    # Set up model:\n    global_step = tf.Variable(0, name=""global_step"", trainable=False)\n    model, stats = model_train_mode(args, feeder, hparams, global_step)\n    eval_model = model_test_mode(args, feeder, hparams, global_step)\n    \n    # Embeddings metadata\n    char_embedding_meta = os.path.join(meta_folder, ""CharacterEmbeddings.tsv"")\n    if not os.path.isfile(char_embedding_meta):\n        with open(char_embedding_meta, ""w"", encoding=""utf-8"") as f:\n            for symbol in symbols:\n                if symbol == "" "":\n                    symbol = ""\\\\s""  # For visual purposes, swap space with \\s\n                \n                f.write(""{}\\n"".format(symbol))\n    \n    char_embedding_meta = char_embedding_meta.replace(log_dir, "".."")\n    \n    # Book keeping\n    step = 0\n    time_window = ValueWindow(100)\n    loss_window = ValueWindow(100)\n    saver = tf.train.Saver(max_to_keep=5)\n    \n    log(""Tacotron training set to a maximum of {} steps"".format(args.tacotron_train_steps))\n    \n    # Memory allocation on the GPU as needed\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.allow_soft_placement = True\n    \n    # Train\n    with tf.Session(config=config) as sess:\n        try:\n            summary_writer = tf.summary.FileWriter(tensorboard_dir, sess.graph)\n            \n            sess.run(tf.global_variables_initializer())\n            \n            # saved model restoring\n            if args.restore:\n                # Restore saved model if the user requested it, default = True\n                try:\n                    checkpoint_state = tf.train.get_checkpoint_state(save_dir)\n                    \n                    if checkpoint_state and checkpoint_state.model_checkpoint_path:\n                        log(""Loading checkpoint {}"".format(checkpoint_state.model_checkpoint_path),\n                            slack=True)\n                        saver.restore(sess, checkpoint_state.model_checkpoint_path)\n                    \n                    else:\n                        log(""No model to load at {}"".format(save_dir), slack=True)\n                        saver.save(sess, checkpoint_fpath, global_step=global_step)\n                \n                except tf.errors.OutOfRangeError as e:\n                    log(""Cannot restore checkpoint: {}"".format(e), slack=True)\n            else:\n                log(""Starting new training!"", slack=True)\n                saver.save(sess, checkpoint_fpath, global_step=global_step)\n            \n            # initializing feeder\n            feeder.start_threads(sess)\n            \n            # Training loop\n            while not coord.should_stop() and step < args.tacotron_train_steps:\n                start_time = time.time()\n                step, loss, opt = sess.run([global_step, model.loss, model.optimize])\n                time_window.append(time.time() - start_time)\n                loss_window.append(loss)\n                message = ""Step {:7d} [{:.3f} sec/step, loss={:.5f}, avg_loss={:.5f}]"".format(\n                    step, time_window.average, loss, loss_window.average)\n                log(message, end=""\\r"", slack=(step % args.checkpoint_interval == 0))\n                print(message)\n                \n                if loss > 100 or np.isnan(loss):\n                    log(""Loss exploded to {:.5f} at step {}"".format(loss, step))\n                    raise Exception(""Loss exploded"")\n                \n                if step % args.summary_interval == 0:\n                    log(""\\nWriting summary at step {}"".format(step))\n                    summary_writer.add_summary(sess.run(stats), step)\n                \n                if step % args.eval_interval == 0:\n                    # Run eval and save eval stats\n                    log(""\\nRunning evaluation at step {}"".format(step))\n                    \n                    eval_losses = []\n                    before_losses = []\n                    after_losses = []\n                    stop_token_losses = []\n                    linear_losses = []\n                    linear_loss = None\n                    \n                    if hparams.predict_linear:\n                        for i in tqdm(range(feeder.test_steps)):\n                            eloss, before_loss, after_loss, stop_token_loss, linear_loss, mel_p, \\\n\t\t\t\t\t\t\tmel_t, t_len, align, lin_p, lin_t = sess.run(\n                                [\n                                    eval_model.tower_loss[0], eval_model.tower_before_loss[0],\n                                    eval_model.tower_after_loss[0],\n                                    eval_model.tower_stop_token_loss[0],\n                                    eval_model.tower_linear_loss[0],\n                                    eval_model.tower_mel_outputs[0][0],\n                                    eval_model.tower_mel_targets[0][0],\n                                    eval_model.tower_targets_lengths[0][0],\n                                    eval_model.tower_alignments[0][0],\n                                    eval_model.tower_linear_outputs[0][0],\n                                    eval_model.tower_linear_targets[0][0],\n                                ])\n                            eval_losses.append(eloss)\n                            before_losses.append(before_loss)\n                            after_losses.append(after_loss)\n                            stop_token_losses.append(stop_token_loss)\n                            linear_losses.append(linear_loss)\n                        linear_loss = sum(linear_losses) / len(linear_losses)\n                        \n                        wav = audio.inv_linear_spectrogram(lin_p.T, hparams)\n                        audio.save_wav(wav, os.path.join(eval_wav_dir,\n                                                         ""step-{}-eval-wave-from-linear.wav"".format(\n                                                             step)), sr=hparams.sample_rate)\n                    \n                    else:\n                        for i in tqdm(range(feeder.test_steps)):\n                            eloss, before_loss, after_loss, stop_token_loss, mel_p, mel_t, t_len,\\\n\t\t\t\t\t\t\talign = sess.run(\n                                [\n                                    eval_model.tower_loss[0], eval_model.tower_before_loss[0],\n                                    eval_model.tower_after_loss[0],\n                                    eval_model.tower_stop_token_loss[0],\n                                    eval_model.tower_mel_outputs[0][0],\n                                    eval_model.tower_mel_targets[0][0],\n                                    eval_model.tower_targets_lengths[0][0],\n                                    eval_model.tower_alignments[0][0]\n                                ])\n                            eval_losses.append(eloss)\n                            before_losses.append(before_loss)\n                            after_losses.append(after_loss)\n                            stop_token_losses.append(stop_token_loss)\n                    \n                    eval_loss = sum(eval_losses) / len(eval_losses)\n                    before_loss = sum(before_losses) / len(before_losses)\n                    after_loss = sum(after_losses) / len(after_losses)\n                    stop_token_loss = sum(stop_token_losses) / len(stop_token_losses)\n                    \n                    log(""Saving eval log to {}.."".format(eval_dir))\n                    # Save some log to monitor model improvement on same unseen sequence\n                    wav = audio.inv_mel_spectrogram(mel_p.T, hparams)\n                    audio.save_wav(wav, os.path.join(eval_wav_dir,\n                                                     ""step-{}-eval-wave-from-mel.wav"".format(step)),\n                                   sr=hparams.sample_rate)\n                    \n                    plot.plot_alignment(align, os.path.join(eval_plot_dir,\n                                                            ""step-{}-eval-align.png"".format(step)),\n                                        title=""{}, {}, step={}, loss={:.5f}"".format(""Tacotron"",\n                                                                                    time_string(),\n                                                                                    step,\n                                                                                    eval_loss),\n                                        max_len=t_len // hparams.outputs_per_step)\n                    plot.plot_spectrogram(mel_p, os.path.join(eval_plot_dir,\n                                                              ""step-{""\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  ""}-eval-mel-spectrogram.png"".format(\n                                                                  step)),\n                                          title=""{}, {}, step={}, loss={:.5f}"".format(""Tacotron"",\n                                                                                      time_string(),\n                                                                                      step,\n                                                                                      eval_loss),\n                                          target_spectrogram=mel_t,\n                                          max_len=t_len)\n                    \n                    if hparams.predict_linear:\n                        plot.plot_spectrogram(lin_p, os.path.join(eval_plot_dir,\n                                                                  ""step-{}-eval-linear-spectrogram.png"".format(\n                                                                      step)),\n                                              title=""{}, {}, step={}, loss={:.5f}"".format(\n                                                  ""Tacotron"", time_string(), step, eval_loss),\n                                              target_spectrogram=lin_t,\n                                              max_len=t_len, auto_aspect=True)\n                    \n                    log(""Eval loss for global step {}: {:.3f}"".format(step, eval_loss))\n                    log(""Writing eval summary!"")\n                    add_eval_stats(summary_writer, step, linear_loss, before_loss, after_loss,\n                                   stop_token_loss, eval_loss)\n                \n                if step % args.checkpoint_interval == 0 or step == args.tacotron_train_steps or \\\n                        step == 300:\n                    # Save model and current global step\n                    saver.save(sess, checkpoint_fpath, global_step=global_step)\n                    \n                    log(""\\nSaving alignment, Mel-Spectrograms and griffin-lim inverted waveform.."")\n                    input_seq, mel_prediction, alignment, target, target_length = sess.run([\n                        model.tower_inputs[0][0],\n                        model.tower_mel_outputs[0][0],\n                        model.tower_alignments[0][0],\n                        model.tower_mel_targets[0][0],\n                        model.tower_targets_lengths[0][0],\n                    ])\n                    \n                    # save predicted mel spectrogram to disk (debug)\n                    mel_filename = ""mel-prediction-step-{}.npy"".format(step)\n                    np.save(os.path.join(mel_dir, mel_filename), mel_prediction.T,\n                            allow_pickle=False)\n                    \n                    # save griffin lim inverted wav for debug (mel -> wav)\n                    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n                    audio.save_wav(wav,\n                                   os.path.join(wav_dir, ""step-{}-wave-from-mel.wav"".format(step)),\n                                   sr=hparams.sample_rate)\n                    \n                    # save alignment plot to disk (control purposes)\n                    plot.plot_alignment(alignment,\n                                        os.path.join(plot_dir, ""step-{}-align.png"".format(step)),\n                                        title=""{}, {}, step={}, loss={:.5f}"".format(""Tacotron"",\n                                                                                    time_string(),\n                                                                                    step, loss),\n                                        max_len=target_length // hparams.outputs_per_step)\n                    # save real and predicted mel-spectrogram plot to disk (control purposes)\n                    plot.plot_spectrogram(mel_prediction, os.path.join(plot_dir,\n                                                                       ""step-{}-mel-spectrogram.png"".format(\n                                                                           step)),\n                                          title=""{}, {}, step={}, loss={:.5f}"".format(""Tacotron"",\n                                                                                      time_string(),\n                                                                                      step, loss),\n                                          target_spectrogram=target,\n                                          max_len=target_length)\n                    log(""Input at step {}: {}"".format(step, sequence_to_text(input_seq)))\n                \n                if step % args.embedding_interval == 0 or step == args.tacotron_train_steps or step == 1:\n                    # Get current checkpoint state\n                    checkpoint_state = tf.train.get_checkpoint_state(save_dir)\n                    \n                    # Update Projector\n                    log(""\\nSaving Model Character Embeddings visualization.."")\n                    add_embedding_stats(summary_writer, [model.embedding_table.name],\n                                        [char_embedding_meta],\n                                        checkpoint_state.model_checkpoint_path)\n                    log(""Tacotron Character embeddings have been updated on tensorboard!"")\n            \n            log(""Tacotron training complete after {} global steps!"".format(\n                args.tacotron_train_steps), slack=True)\n            return save_dir\n        \n        except Exception as e:\n            log(""Exiting due to exception: {}"".format(e), slack=True)\n            traceback.print_exc()\n            coord.request_stop(e)\n\n\ndef tacotron_train(args, log_dir, hparams):\n    return train(log_dir, args, hparams)\n'"
toolbox/__init__.py,0,"b'from toolbox.ui import UI\nfrom encoder import inference as encoder\nfrom synthesizer.inference import Synthesizer\nfrom vocoder import inference as vocoder\nfrom pathlib import Path\nfrom time import perf_counter as timer\nfrom toolbox.utterance import Utterance\nimport numpy as np\nimport traceback\nimport sys\n\n\n# Use this directory structure for your datasets, or modify it to fit your needs\nrecognized_datasets = [\n    ""LibriSpeech/dev-clean"",\n    ""LibriSpeech/dev-other"",\n    ""LibriSpeech/test-clean"",\n    ""LibriSpeech/test-other"",\n    ""LibriSpeech/train-clean-100"",\n    ""LibriSpeech/train-clean-360"",\n    ""LibriSpeech/train-other-500"",\n    ""LibriTTS/dev-clean"",\n    ""LibriTTS/dev-other"",\n    ""LibriTTS/test-clean"",\n    ""LibriTTS/test-other"",\n    ""LibriTTS/train-clean-100"",\n    ""LibriTTS/train-clean-360"",\n    ""LibriTTS/train-other-500"",\n    ""LJSpeech-1.1"",\n    ""VoxCeleb1/wav"",\n    ""VoxCeleb1/test_wav"",\n    ""VoxCeleb2/dev/aac"",\n    ""VoxCeleb2/test/aac"",\n    ""VCTK-Corpus/wav48"",\n]\n\nclass Toolbox:\n    def __init__(self, datasets_root, enc_models_dir, syn_models_dir, voc_models_dir, low_mem):\n        sys.excepthook = self.excepthook\n        self.datasets_root = datasets_root\n        self.low_mem = low_mem\n        self.utterances = set()\n        self.current_generated = (None, None, None, None) # speaker_name, spec, breaks, wav\n        \n        self.synthesizer = None # type: Synthesizer\n        \n        # Initialize the events and the interface\n        self.ui = UI()\n        self.reset_ui(enc_models_dir, syn_models_dir, voc_models_dir)\n        self.setup_events()\n        self.ui.start()\n        \n    def excepthook(self, exc_type, exc_value, exc_tb):\n        traceback.print_exception(exc_type, exc_value, exc_tb)\n        self.ui.log(""Exception: %s"" % exc_value)\n        \n    def setup_events(self):\n        # Dataset, speaker and utterance selection\n        self.ui.browser_load_button.clicked.connect(lambda: self.load_from_browser())\n        random_func = lambda level: lambda: self.ui.populate_browser(self.datasets_root,\n                                                                     recognized_datasets,\n                                                                     level)\n        self.ui.random_dataset_button.clicked.connect(random_func(0))\n        self.ui.random_speaker_button.clicked.connect(random_func(1))\n        self.ui.random_utterance_button.clicked.connect(random_func(2))\n        self.ui.dataset_box.currentIndexChanged.connect(random_func(1))\n        self.ui.speaker_box.currentIndexChanged.connect(random_func(2))\n        \n        # Model selection\n        self.ui.encoder_box.currentIndexChanged.connect(self.init_encoder)\n        def func(): \n            self.synthesizer = None\n        self.ui.synthesizer_box.currentIndexChanged.connect(func)\n        self.ui.vocoder_box.currentIndexChanged.connect(self.init_vocoder)\n        \n        # Utterance selection\n        func = lambda: self.load_from_browser(self.ui.browse_file())\n        self.ui.browser_browse_button.clicked.connect(func)\n        func = lambda: self.ui.draw_utterance(self.ui.selected_utterance, ""current"")\n        self.ui.utterance_history.currentIndexChanged.connect(func)\n        func = lambda: self.ui.play(self.ui.selected_utterance.wav, Synthesizer.sample_rate)\n        self.ui.play_button.clicked.connect(func)\n        self.ui.stop_button.clicked.connect(self.ui.stop)\n        self.ui.record_button.clicked.connect(self.record)\n        \n        # Generation\n        func = lambda: self.synthesize() or self.vocode()\n        self.ui.generate_button.clicked.connect(func)\n        self.ui.synthesize_button.clicked.connect(self.synthesize)\n        self.ui.vocode_button.clicked.connect(self.vocode)\n        \n        # UMAP legend\n        self.ui.clear_button.clicked.connect(self.clear_utterances)\n\n    def reset_ui(self, encoder_models_dir, synthesizer_models_dir, vocoder_models_dir):\n        self.ui.populate_browser(self.datasets_root, recognized_datasets, 0, True)\n        self.ui.populate_models(encoder_models_dir, synthesizer_models_dir, vocoder_models_dir)\n        \n    def load_from_browser(self, fpath=None):\n        if fpath is None:\n            fpath = Path(self.datasets_root,\n                         self.ui.current_dataset_name,\n                         self.ui.current_speaker_name,\n                         self.ui.current_utterance_name)\n            name = str(fpath.relative_to(self.datasets_root))\n            speaker_name = self.ui.current_dataset_name + \'_\' + self.ui.current_speaker_name\n            \n            # Select the next utterance\n            if self.ui.auto_next_checkbox.isChecked():\n                self.ui.browser_select_next()\n        elif fpath == """":\n            return \n        else:\n            name = fpath.name\n            speaker_name = fpath.parent.name\n        \n        # Get the wav from the disk. We take the wav with the vocoder/synthesizer format for\n        # playback, so as to have a fair comparison with the generated audio\n        wav = Synthesizer.load_preprocess_wav(fpath)\n        self.ui.log(""Loaded %s"" % name)\n\n        self.add_real_utterance(wav, name, speaker_name)\n        \n    def record(self):\n        wav = self.ui.record_one(encoder.sampling_rate, 5)\n        if wav is None:\n            return \n        self.ui.play(wav, encoder.sampling_rate)\n\n        speaker_name = ""user01""\n        name = speaker_name + ""_rec_%05d"" % np.random.randint(100000)\n        self.add_real_utterance(wav, name, speaker_name)\n        \n    def add_real_utterance(self, wav, name, speaker_name):\n        # Compute the mel spectrogram\n        spec = Synthesizer.make_spectrogram(wav)\n        self.ui.draw_spec(spec, ""current"")\n\n        # Compute the embedding\n        if not encoder.is_loaded():\n            self.init_encoder()\n        encoder_wav = encoder.preprocess_wav(wav)\n        embed, partial_embeds, _ = encoder.embed_utterance(encoder_wav, return_partials=True)\n\n        # Add the utterance\n        utterance = Utterance(name, speaker_name, wav, spec, embed, partial_embeds, False)\n        self.utterances.add(utterance)\n        self.ui.register_utterance(utterance)\n\n        # Plot it\n        self.ui.draw_embed(embed, name, ""current"")\n        self.ui.draw_umap_projections(self.utterances)\n        \n    def clear_utterances(self):\n        self.utterances.clear()\n        self.ui.draw_umap_projections(self.utterances)\n        \n    def synthesize(self):\n        self.ui.log(""Generating the mel spectrogram..."")\n        self.ui.set_loading(1)\n        \n        # Synthesize the spectrogram\n        if self.synthesizer is None:\n            model_dir = self.ui.current_synthesizer_model_dir\n            checkpoints_dir = model_dir.joinpath(""taco_pretrained"")\n            self.synthesizer = Synthesizer(checkpoints_dir, low_mem=self.low_mem)\n        if not self.synthesizer.is_loaded():\n            self.ui.log(""Loading the synthesizer %s"" % self.synthesizer.checkpoint_fpath)\n        \n        texts = self.ui.text_prompt.toPlainText().split(""\\n"")\n        embed = self.ui.selected_utterance.embed\n        embeds = np.stack([embed] * len(texts))\n        specs = self.synthesizer.synthesize_spectrograms(texts, embeds)\n        breaks = [spec.shape[1] for spec in specs]\n        spec = np.concatenate(specs, axis=1)\n        \n        self.ui.draw_spec(spec, ""generated"")\n        self.current_generated = (self.ui.selected_utterance.speaker_name, spec, breaks, None)\n        self.ui.set_loading(0)\n\n    def vocode(self):\n        speaker_name, spec, breaks, _ = self.current_generated\n        assert spec is not None\n\n        # Synthesize the waveform\n        if not vocoder.is_loaded():\n            self.init_vocoder()\n        def vocoder_progress(i, seq_len, b_size, gen_rate):\n            real_time_factor = (gen_rate / Synthesizer.sample_rate) * 1000\n            line = ""Waveform generation: %d/%d (batch size: %d, rate: %.1fkHz - %.2fx real time)"" \\\n                   % (i * b_size, seq_len * b_size, b_size, gen_rate, real_time_factor)\n            self.ui.log(line, ""overwrite"")\n            self.ui.set_loading(i, seq_len)\n        if self.ui.current_vocoder_fpath is not None:\n            self.ui.log("""")\n            wav = vocoder.infer_waveform(spec, progress_callback=vocoder_progress)\n        else:\n            self.ui.log(""Waveform generation with Griffin-Lim... "")\n            wav = Synthesizer.griffin_lim(spec)\n        self.ui.set_loading(0)\n        self.ui.log("" Done!"", ""append"")\n        \n        # Add breaks\n        b_ends = np.cumsum(np.array(breaks) * Synthesizer.hparams.hop_size)\n        b_starts = np.concatenate(([0], b_ends[:-1]))\n        wavs = [wav[start:end] for start, end, in zip(b_starts, b_ends)]\n        breaks = [np.zeros(int(0.15 * Synthesizer.sample_rate))] * len(breaks)\n        wav = np.concatenate([i for w, b in zip(wavs, breaks) for i in (w, b)])\n\n        # Play it\n        wav = wav / np.abs(wav).max() * 0.97\n        self.ui.play(wav, Synthesizer.sample_rate)\n\n        # Compute the embedding\n        # TODO: this is problematic with different sampling rates, gotta fix it\n        if not encoder.is_loaded():\n            self.init_encoder()\n        encoder_wav = encoder.preprocess_wav(wav)\n        embed, partial_embeds, _ = encoder.embed_utterance(encoder_wav, return_partials=True)\n        \n        # Add the utterance\n        name = speaker_name + ""_gen_%05d"" % np.random.randint(100000)\n        utterance = Utterance(name, speaker_name, wav, spec, embed, partial_embeds, True)\n        self.utterances.add(utterance)\n        \n        # Plot it\n        self.ui.draw_embed(embed, name, ""generated"")\n        self.ui.draw_umap_projections(self.utterances)\n        \n    def init_encoder(self):\n        model_fpath = self.ui.current_encoder_fpath\n        \n        self.ui.log(""Loading the encoder %s... "" % model_fpath)\n        self.ui.set_loading(1)\n        start = timer()\n        encoder.load_model(model_fpath)\n        self.ui.log(""Done (%dms)."" % int(1000 * (timer() - start)), ""append"")\n        self.ui.set_loading(0)\n           \n    def init_vocoder(self):\n        model_fpath = self.ui.current_vocoder_fpath\n        # Case of Griffin-lim\n        if model_fpath is None:\n            return \n    \n        self.ui.log(""Loading the vocoder %s... "" % model_fpath)\n        self.ui.set_loading(1)\n        start = timer()\n        vocoder.load_model(model_fpath)\n        self.ui.log(""Done (%dms)."" % int(1000 * (timer() - start)), ""append"")\n        self.ui.set_loading(0)\n'"
toolbox/ui.py,0,"b'from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas\nfrom matplotlib.figure import Figure\nfrom PyQt5.QtCore import Qt\nfrom PyQt5.QtWidgets import *\nfrom encoder.inference import plot_embedding_as_heatmap\nfrom toolbox.utterance import Utterance\nfrom pathlib import Path\nfrom typing import List, Set\nimport sounddevice as sd\nimport matplotlib.pyplot as plt\nimport numpy as np\n# from sklearn.manifold import TSNE         # You can try with TSNE if you like, I prefer UMAP \nfrom time import sleep\nimport umap\nimport sys\nfrom warnings import filterwarnings\nfilterwarnings(""ignore"")\n\n\ncolormap = np.array([\n    [0, 127, 70],\n    [255, 0, 0],\n    [255, 217, 38],\n    [0, 135, 255],\n    [165, 0, 165],\n    [255, 167, 255],\n    [97, 142, 151],\n    [0, 255, 255],\n    [255, 96, 38],\n    [142, 76, 0],\n    [33, 0, 127],\n    [0, 0, 0],\n    [183, 183, 183],\n    [76, 255, 0],\n], dtype=np.float) / 255 \n\ndefault_text = \\\n    ""Welcome to the toolbox! To begin, load an utterance from your datasets or record one "" \\\n    ""yourself.\\nOnce its embedding has been created, you can synthesize any text written here.\\n"" \\\n    ""With the current synthesizer model, punctuation and special characters will be ignored.\\n"" \\\n    ""The synthesizer expects to generate "" \\\n    ""outputs that are somewhere between 5 and 12 seconds.\\nTo mark breaks, write a new line. "" \\\n    ""Each line will be treated separately.\\nThen, they are joined together to make the final "" \\\n    ""spectrogram. Use the vocoder to generate audio.\\nThe vocoder generates almost in constant "" \\\n    ""time, so it will be more time efficient for longer inputs like this one.\\nOn the left you "" \\\n    ""have the embedding projections. Load or record more utterances to see them.\\nIf you have "" \\\n    ""at least 2 or 3 utterances from a same speaker, a cluster should form.\\nSynthesized "" \\\n    ""utterances are of the same color as the speaker whose voice was used, but they\'re "" \\\n    ""represented with a cross.""\n\n   \nclass UI(QDialog):\n    min_umap_points = 4\n    max_log_lines = 5\n    max_saved_utterances = 20\n    \n    def draw_utterance(self, utterance: Utterance, which):\n        self.draw_spec(utterance.spec, which)\n        self.draw_embed(utterance.embed, utterance.name, which)\n    \n    def draw_embed(self, embed, name, which):\n        embed_ax, _ = self.current_ax if which == ""current"" else self.gen_ax\n        embed_ax.figure.suptitle("""" if embed is None else name)\n        \n        ## Embedding\n        # Clear the plot\n        if len(embed_ax.images) > 0:\n            embed_ax.images[0].colorbar.remove()\n        embed_ax.clear()\n        \n        # Draw the embed\n        if embed is not None:\n            plot_embedding_as_heatmap(embed, embed_ax)\n            embed_ax.set_title(""embedding"")\n        embed_ax.set_aspect(""equal"", ""datalim"")\n        embed_ax.set_xticks([])\n        embed_ax.set_yticks([])\n        embed_ax.figure.canvas.draw()\n\n    def draw_spec(self, spec, which):\n        _, spec_ax = self.current_ax if which == ""current"" else self.gen_ax\n\n        ## Spectrogram\n        # Draw the spectrogram\n        spec_ax.clear()\n        if spec is not None:\n            im = spec_ax.imshow(spec, aspect=""auto"", interpolation=""none"")\n            # spec_ax.figure.colorbar(mappable=im, shrink=0.65, orientation=""horizontal"", \n            # spec_ax=spec_ax)\n            spec_ax.set_title(""mel spectrogram"")\n    \n        spec_ax.set_xticks([])\n        spec_ax.set_yticks([])\n        spec_ax.figure.canvas.draw()\n        if which != ""current"":\n            self.vocode_button.setDisabled(spec is None)\n\n    def draw_umap_projections(self, utterances: Set[Utterance]):\n        self.umap_ax.clear()\n\n        speakers = np.unique([u.speaker_name for u in utterances])\n        colors = {speaker_name: colormap[i] for i, speaker_name in enumerate(speakers)}\n        embeds = [u.embed for u in utterances]\n\n        # Display a message if there aren\'t enough points\n        if len(utterances) < self.min_umap_points:\n            self.umap_ax.text(.5, .5, ""Add %d more points to\\ngenerate the projections"" % \n                              (self.min_umap_points - len(utterances)), \n                              horizontalalignment=\'center\', fontsize=15)\n            self.umap_ax.set_title("""")\n            \n        # Compute the projections\n        else:\n            if not self.umap_hot:\n                self.log(\n                    ""Drawing UMAP projections for the first time, this will take a few seconds."")\n                self.umap_hot = True\n            \n            reducer = umap.UMAP(int(np.ceil(np.sqrt(len(embeds)))), metric=""cosine"")\n            # reducer = TSNE()\n            projections = reducer.fit_transform(embeds)\n            \n            speakers_done = set()\n            for projection, utterance in zip(projections, utterances):\n                color = colors[utterance.speaker_name]\n                mark = ""x"" if ""_gen_"" in utterance.name else ""o""\n                label = None if utterance.speaker_name in speakers_done else utterance.speaker_name\n                speakers_done.add(utterance.speaker_name)\n                self.umap_ax.scatter(projection[0], projection[1], c=[color], marker=mark,\n                                     label=label)\n            # self.umap_ax.set_title(""UMAP projections"")\n            self.umap_ax.legend(prop={\'size\': 10})\n\n        # Draw the plot\n        self.umap_ax.set_aspect(""equal"", ""datalim"")\n        self.umap_ax.set_xticks([])\n        self.umap_ax.set_yticks([])\n        self.umap_ax.figure.canvas.draw()\n        \n    def play(self, wav, sample_rate):\n        sd.stop()\n        sd.play(wav, sample_rate)\n        \n    def stop(self):\n        sd.stop()\n\n    def record_one(self, sample_rate, duration):\n        self.record_button.setText(""Recording..."")\n        self.record_button.setDisabled(True)\n        \n        self.log(""Recording %d seconds of audio"" % duration)\n        sd.stop()\n        try:\n            wav = sd.rec(duration * sample_rate, sample_rate, 1)\n        except Exception as e:\n            print(e)\n            self.log(""Could not record anything. Is your recording device enabled?"")\n            self.log(""Your device must be connected before you start the toolbox."")\n            return None\n        \n        for i in np.arange(0, duration, 0.1):\n            self.set_loading(i, duration)\n            sleep(0.1)\n        self.set_loading(duration, duration)\n        sd.wait()\n        \n        self.log(""Done recording."")\n        self.record_button.setText(""Record one"")\n        self.record_button.setDisabled(False)\n        \n        return wav.squeeze()\n\n    @property        \n    def current_dataset_name(self):\n        return self.dataset_box.currentText()\n\n    @property\n    def current_speaker_name(self):\n        return self.speaker_box.currentText()\n    \n    @property\n    def current_utterance_name(self):\n        return self.utterance_box.currentText()\n    \n    def browse_file(self):\n        fpath = QFileDialog().getOpenFileName(\n            parent=self,\n            caption=""Select an audio file"",\n            filter=""Audio Files (*.mp3 *.flac *.wav *.m4a)""\n        )\n        return Path(fpath[0]) if fpath[0] != """" else """"\n    \n    @staticmethod\n    def repopulate_box(box, items, random=False):\n        """"""\n        Resets a box and adds a list of items. Pass a list of (item, data) pairs instead to join \n        data to the items\n        """"""\n        box.blockSignals(True)\n        box.clear()\n        for item in items:\n            item = list(item) if isinstance(item, tuple) else [item]\n            box.addItem(str(item[0]), *item[1:])\n        if len(items) > 0:\n            box.setCurrentIndex(np.random.randint(len(items)) if random else 0)\n        box.setDisabled(len(items) == 0)\n        box.blockSignals(False)\n    \n    def populate_browser(self, datasets_root: Path, recognized_datasets: List, level: int,\n                         random=True):\n        # Select a random dataset\n        if level <= 0:\n            if datasets_root is not None:\n                datasets = [datasets_root.joinpath(d) for d in recognized_datasets]\n                datasets = [d.relative_to(datasets_root) for d in datasets if d.exists()]\n                self.browser_load_button.setDisabled(len(datasets) == 0)\n            if datasets_root is None or len(datasets) == 0:\n                msg = ""Warning: you d"" + (""id not pass a root directory for datasets as argument"" \\\n                    if datasets_root is None else ""o not have any of the recognized datasets"" \\\n                                                  "" in %s"" % datasets_root) \n                self.log(msg)\n                msg += "".\\nThe recognized datasets are:\\n\\t%s\\nFeel free to add your own. You "" \\\n                       ""can still use the toolbox by recording samples yourself."" % \\\n                       (""\\n\\t"".join(recognized_datasets))\n                print(msg, file=sys.stderr)\n                \n                self.random_utterance_button.setDisabled(True)\n                self.random_speaker_button.setDisabled(True)\n                self.random_dataset_button.setDisabled(True)\n                self.utterance_box.setDisabled(True)\n                self.speaker_box.setDisabled(True)\n                self.dataset_box.setDisabled(True)\n                return \n            self.repopulate_box(self.dataset_box, datasets, random)\n    \n        # Select a random speaker\n        if level <= 1:\n            speakers_root = datasets_root.joinpath(self.current_dataset_name)\n            speaker_names = [d.stem for d in speakers_root.glob(""*"") if d.is_dir()]\n            self.repopulate_box(self.speaker_box, speaker_names, random)\n    \n        # Select a random utterance\n        if level <= 2:\n            utterances_root = datasets_root.joinpath(\n                self.current_dataset_name, \n                self.current_speaker_name\n            )\n            utterances = []\n            for extension in [\'mp3\', \'flac\', \'wav\', \'m4a\']:\n                utterances.extend(Path(utterances_root).glob(""**/*.%s"" % extension))\n            utterances = [fpath.relative_to(utterances_root) for fpath in utterances]\n            self.repopulate_box(self.utterance_box, utterances, random)\n            \n    def browser_select_next(self):\n        index = (self.utterance_box.currentIndex() + 1) % len(self.utterance_box)\n        self.utterance_box.setCurrentIndex(index)\n\n    @property\n    def current_encoder_fpath(self):\n        return self.encoder_box.itemData(self.encoder_box.currentIndex())\n    \n    @property\n    def current_synthesizer_model_dir(self):\n        return self.synthesizer_box.itemData(self.synthesizer_box.currentIndex())\n    \n    @property\n    def current_vocoder_fpath(self):\n        return self.vocoder_box.itemData(self.vocoder_box.currentIndex())\n\n    def populate_models(self, encoder_models_dir: Path, synthesizer_models_dir: Path, \n                        vocoder_models_dir: Path):\n        # Encoder\n        encoder_fpaths = list(encoder_models_dir.glob(""*.pt""))\n        if len(encoder_fpaths) == 0:\n            raise Exception(""No encoder models found in %s"" % encoder_models_dir)\n        self.repopulate_box(self.encoder_box, [(f.stem, f) for f in encoder_fpaths])\n        \n        # Synthesizer\n        synthesizer_model_dirs = list(synthesizer_models_dir.glob(""*""))\n        synthesizer_items = [(f.name.replace(""logs-"", """"), f) for f in synthesizer_model_dirs]\n        if len(synthesizer_model_dirs) == 0:\n            raise Exception(""No synthesizer models found in %s. For the synthesizer, the expected ""\n                            ""structure is <syn_models_dir>/logs-<model_name>/taco_pretrained/""\n                            ""checkpoint"" % synthesizer_models_dir)\n        self.repopulate_box(self.synthesizer_box, synthesizer_items)\n\n        # Vocoder\n        vocoder_fpaths = list(vocoder_models_dir.glob(""**/*.pt""))\n        vocoder_items = [(f.stem, f) for f in vocoder_fpaths] + [(""Griffin-Lim"", None)]\n        self.repopulate_box(self.vocoder_box, vocoder_items)\n        \n    @property\n    def selected_utterance(self):\n        return self.utterance_history.itemData(self.utterance_history.currentIndex())\n        \n    def register_utterance(self, utterance: Utterance):\n        self.utterance_history.blockSignals(True)\n        self.utterance_history.insertItem(0, utterance.name, utterance)\n        self.utterance_history.setCurrentIndex(0)\n        self.utterance_history.blockSignals(False)\n        \n        if len(self.utterance_history) > self.max_saved_utterances:\n            self.utterance_history.removeItem(self.max_saved_utterances)\n\n        self.play_button.setDisabled(False)\n        self.generate_button.setDisabled(False)\n        self.synthesize_button.setDisabled(False)\n\n    def log(self, line, mode=""newline""):\n        if mode == ""newline"":\n            self.logs.append(line)\n            if len(self.logs) > self.max_log_lines:\n                del self.logs[0]\n        elif mode == ""append"":\n            self.logs[-1] += line\n        elif mode == ""overwrite"":\n            self.logs[-1] = line\n        log_text = \'\\n\'.join(self.logs)\n        \n        self.log_window.setText(log_text)\n        self.app.processEvents()\n\n    def set_loading(self, value, maximum=1):\n        self.loading_bar.setValue(value * 100)\n        self.loading_bar.setMaximum(maximum * 100)\n        self.loading_bar.setTextVisible(value != 0)\n        self.app.processEvents()\n\n    def reset_interface(self):\n        self.draw_embed(None, None, ""current"")\n        self.draw_embed(None, None, ""generated"")\n        self.draw_spec(None, ""current"")\n        self.draw_spec(None, ""generated"")\n        self.draw_umap_projections(set())\n        self.set_loading(0)\n        self.play_button.setDisabled(True)\n        self.generate_button.setDisabled(True)\n        self.synthesize_button.setDisabled(True)\n        self.vocode_button.setDisabled(True)\n        [self.log("""") for _ in range(self.max_log_lines)]\n\n    def __init__(self):\n        ## Initialize the application\n        self.app = QApplication(sys.argv)\n        super().__init__(None)\n        self.setWindowTitle(""SV2TTS toolbox"")\n        \n        \n        ## Main layouts\n        # Root\n        root_layout = QGridLayout()\n        self.setLayout(root_layout)\n        \n        # Browser\n        browser_layout = QGridLayout()\n        root_layout.addLayout(browser_layout, 0, 1)\n        \n        # Visualizations\n        vis_layout = QVBoxLayout()\n        root_layout.addLayout(vis_layout, 1, 1, 2, 3)\n        \n        # Generation\n        gen_layout = QVBoxLayout()\n        root_layout.addLayout(gen_layout, 0, 2)\n        \n        # Projections\n        self.projections_layout = QVBoxLayout()\n        root_layout.addLayout(self.projections_layout, 1, 0)\n\n\n        ## Projections\n        # UMap\n        fig, self.umap_ax = plt.subplots(figsize=(4, 4), facecolor=""#F0F0F0"")\n        fig.subplots_adjust(left=0.02, bottom=0.02, right=0.98, top=0.98)\n        self.projections_layout.addWidget(FigureCanvas(fig))\n        self.umap_hot = False\n        self.clear_button = QPushButton(""Clear"")\n        self.projections_layout.addWidget(self.clear_button)\n\n\n        ## Browser\n        # Dataset, speaker and utterance selection\n        i = 0\n        self.dataset_box = QComboBox()\n        browser_layout.addWidget(QLabel(""<b>Dataset</b>""), i, 0)\n        browser_layout.addWidget(self.dataset_box, i + 1, 0)\n        self.speaker_box = QComboBox()\n        browser_layout.addWidget(QLabel(""<b>Speaker</b>""), i, 1)\n        browser_layout.addWidget(self.speaker_box, i + 1, 1)\n        self.utterance_box = QComboBox()\n        browser_layout.addWidget(QLabel(""<b>Utterance</b>""), i, 2)\n        browser_layout.addWidget(self.utterance_box, i + 1, 2)\n        self.browser_browse_button = QPushButton(""Browse"")\n        browser_layout.addWidget(self.browser_browse_button, i, 3)\n        self.browser_load_button = QPushButton(""Load"")\n        browser_layout.addWidget(self.browser_load_button, i + 1, 3)\n        i += 2\n        \n        # Random buttons\n        self.random_dataset_button = QPushButton(""Random"")\n        browser_layout.addWidget(self.random_dataset_button, i, 0)\n        self.random_speaker_button = QPushButton(""Random"")\n        browser_layout.addWidget(self.random_speaker_button, i, 1)\n        self.random_utterance_button = QPushButton(""Random"")\n        browser_layout.addWidget(self.random_utterance_button, i, 2)\n        self.auto_next_checkbox = QCheckBox(""Auto select next"")\n        self.auto_next_checkbox.setChecked(True)\n        browser_layout.addWidget(self.auto_next_checkbox, i, 3)\n        i += 1\n        \n        # Utterance box\n        browser_layout.addWidget(QLabel(""<b>Use embedding from:</b>""), i, 0)\n        i += 1\n        \n        # Random & next utterance buttons\n        self.utterance_history = QComboBox()\n        browser_layout.addWidget(self.utterance_history, i, 0, 1, 3)\n        i += 1\n        \n        # Random & next utterance buttons\n        self.take_generated_button = QPushButton(""Take generated"")\n        browser_layout.addWidget(self.take_generated_button, i, 0)\n        self.record_button = QPushButton(""Record"")\n        browser_layout.addWidget(self.record_button, i, 1)\n        self.play_button = QPushButton(""Play"")\n        browser_layout.addWidget(self.play_button, i, 2)\n        self.stop_button = QPushButton(""Stop"")\n        browser_layout.addWidget(self.stop_button, i, 3)\n        i += 2\n\n        # Model selection\n        self.encoder_box = QComboBox()\n        browser_layout.addWidget(QLabel(""<b>Encoder</b>""), i, 0)\n        browser_layout.addWidget(self.encoder_box, i + 1, 0)\n        self.synthesizer_box = QComboBox()\n        browser_layout.addWidget(QLabel(""<b>Synthesizer</b>""), i, 1)\n        browser_layout.addWidget(self.synthesizer_box, i + 1, 1)\n        self.vocoder_box = QComboBox()\n        browser_layout.addWidget(QLabel(""<b>Vocoder</b>""), i, 2)\n        browser_layout.addWidget(self.vocoder_box, i + 1, 2)\n        i += 2\n\n\n        ## Embed & spectrograms\n        vis_layout.addStretch()\n\n        gridspec_kw = {""width_ratios"": [1, 4]}\n        fig, self.current_ax = plt.subplots(1, 2, figsize=(10, 2.25), facecolor=""#F0F0F0"", \n                                            gridspec_kw=gridspec_kw)\n        fig.subplots_adjust(left=0, bottom=0.1, right=1, top=0.8)\n        vis_layout.addWidget(FigureCanvas(fig))\n\n        fig, self.gen_ax = plt.subplots(1, 2, figsize=(10, 2.25), facecolor=""#F0F0F0"", \n                                        gridspec_kw=gridspec_kw)\n        fig.subplots_adjust(left=0, bottom=0.1, right=1, top=0.8)\n        vis_layout.addWidget(FigureCanvas(fig))\n\n        for ax in self.current_ax.tolist() + self.gen_ax.tolist():\n            ax.set_facecolor(""#F0F0F0"")\n            for side in [""top"", ""right"", ""bottom"", ""left""]:\n                ax.spines[side].set_visible(False)\n        \n        \n        ## Generation\n        self.text_prompt = QPlainTextEdit(default_text)\n        gen_layout.addWidget(self.text_prompt, stretch=1)\n        \n        self.generate_button = QPushButton(""Synthesize and vocode"")\n        gen_layout.addWidget(self.generate_button)\n        \n        layout = QHBoxLayout()\n        self.synthesize_button = QPushButton(""Synthesize only"")\n        layout.addWidget(self.synthesize_button)\n        self.vocode_button = QPushButton(""Vocode only"")\n        layout.addWidget(self.vocode_button)\n        gen_layout.addLayout(layout)\n\n        self.loading_bar = QProgressBar()\n        gen_layout.addWidget(self.loading_bar)\n        \n        self.log_window = QLabel()\n        self.log_window.setAlignment(Qt.AlignBottom | Qt.AlignLeft)\n        gen_layout.addWidget(self.log_window)\n        self.logs = []\n        gen_layout.addStretch()\n\n        \n        ## Set the size of the window and of the elements\n        max_size = QDesktopWidget().availableGeometry(self).size() * 0.8\n        self.resize(max_size)\n        \n        ## Finalize the display\n        self.reset_interface()\n        self.show()\n\n    def start(self):\n        self.app.exec_()\n'"
toolbox/utterance.py,0,"b'from collections import namedtuple\n\nUtterance = namedtuple(""Utterance"", ""name speaker_name wav spec embed partial_embeds synth"")\nUtterance.__eq__ = lambda x, y: x.name == y.name\nUtterance.__hash__ = lambda x: hash(x.name)\n'"
utils/__init__.py,0,b''
utils/argutils.py,0,"b'from pathlib import Path\nimport numpy as np\nimport argparse\n\n_type_priorities = [    # In decreasing order\n    Path,\n    str,\n    int,\n    float,\n    bool,\n]\n\ndef _priority(o):\n    p = next((i for i, t in enumerate(_type_priorities) if type(o) is t), None) \n    if p is not None:\n        return p\n    p = next((i for i, t in enumerate(_type_priorities) if isinstance(o, t)), None) \n    if p is not None:\n        return p\n    return len(_type_priorities)\n\ndef print_args(args: argparse.Namespace, parser=None):\n    args = vars(args)\n    if parser is None:\n        priorities = list(map(_priority, args.values()))\n    else:\n        all_params = [a.dest for g in parser._action_groups for a in g._group_actions ]\n        priority = lambda p: all_params.index(p) if p in all_params else len(all_params)\n        priorities = list(map(priority, args.keys()))\n    \n    pad = max(map(len, args.keys())) + 3\n    indices = np.lexsort((list(args.keys()), priorities))\n    items = list(args.items())\n    \n    print(""Arguments:"")\n    for i in indices:\n        param, value = items[i]\n        print(""    {0}:{1}{2}"".format(param, \' \' * (pad - len(param)), value))\n    print("""")\n    '"
utils/logmmse.py,0,"b'# The MIT License (MIT)\n# \n# Copyright (c) 2015 braindead\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n#\n# This code was extracted from the logmmse package (https://pypi.org/project/logmmse/) and I\n# simply modified the interface to meet my needs.\n\n\nimport numpy as np\nimport math\nfrom scipy.special import expn\nfrom collections import namedtuple\n\nNoiseProfile = namedtuple(""NoiseProfile"", ""sampling_rate window_size len1 len2 win n_fft noise_mu2"")\n\n\ndef profile_noise(noise, sampling_rate, window_size=0):\n    """"""\n    Creates a profile of the noise in a given waveform.\n    \n    :param noise: a waveform containing noise ONLY, as a numpy array of floats or ints. \n    :param sampling_rate: the sampling rate of the audio\n    :param window_size: the size of the window the logmmse algorithm operates on. A default value \n    will be picked if left as 0.\n    :return: a NoiseProfile object\n    """"""\n    noise, dtype = to_float(noise)\n    noise += np.finfo(np.float64).eps\n\n    if window_size == 0:\n        window_size = int(math.floor(0.02 * sampling_rate))\n\n    if window_size % 2 == 1:\n        window_size = window_size + 1\n    \n    perc = 50\n    len1 = int(math.floor(window_size * perc / 100))\n    len2 = int(window_size - len1)\n\n    win = np.hanning(window_size)\n    win = win * len2 / np.sum(win)\n    n_fft = 2 * window_size\n\n    noise_mean = np.zeros(n_fft)\n    n_frames = len(noise) // window_size\n    for j in range(0, window_size * n_frames, window_size):\n        noise_mean += np.absolute(np.fft.fft(win * noise[j:j + window_size], n_fft, axis=0))\n    noise_mu2 = (noise_mean / n_frames) ** 2\n    \n    return NoiseProfile(sampling_rate, window_size, len1, len2, win, n_fft, noise_mu2)\n\n\ndef denoise(wav, noise_profile: NoiseProfile, eta=0.15):\n    """"""\n    Cleans the noise from a speech waveform given a noise profile. The waveform must have the \n    same sampling rate as the one used to create the noise profile. \n    \n    :param wav: a speech waveform as a numpy array of floats or ints.\n    :param noise_profile: a NoiseProfile object that was created from a similar (or a segment of \n    the same) waveform.\n    :param eta: voice threshold for noise update. While the voice activation detection value is \n    below this threshold, the noise profile will be continuously updated throughout the audio. \n    Set to 0 to disable updating the noise profile.\n    :return: the clean wav as a numpy array of floats or ints of the same length.\n    """"""\n    wav, dtype = to_float(wav)\n    wav += np.finfo(np.float64).eps\n    p = noise_profile\n    \n    nframes = int(math.floor(len(wav) / p.len2) - math.floor(p.window_size / p.len2))\n    x_final = np.zeros(nframes * p.len2)\n\n    aa = 0.98\n    mu = 0.98\n    ksi_min = 10 ** (-25 / 10)\n    \n    x_old = np.zeros(p.len1)\n    xk_prev = np.zeros(p.len1)\n    noise_mu2 = p.noise_mu2\n    for k in range(0, nframes * p.len2, p.len2):\n        insign = p.win * wav[k:k + p.window_size]\n\n        spec = np.fft.fft(insign, p.n_fft, axis=0)\n        sig = np.absolute(spec)\n        sig2 = sig ** 2\n\n        gammak = np.minimum(sig2 / noise_mu2, 40)\n\n        if xk_prev.all() == 0:\n            ksi = aa + (1 - aa) * np.maximum(gammak - 1, 0)\n        else:\n            ksi = aa * xk_prev / noise_mu2 + (1 - aa) * np.maximum(gammak - 1, 0)\n            ksi = np.maximum(ksi_min, ksi)\n\n        log_sigma_k = gammak * ksi/(1 + ksi) - np.log(1 + ksi)\n        vad_decision = np.sum(log_sigma_k) / p.window_size\n        if vad_decision < eta:\n            noise_mu2 = mu * noise_mu2 + (1 - mu) * sig2\n\n        a = ksi / (1 + ksi)\n        vk = a * gammak\n        ei_vk = 0.5 * expn(1, np.maximum(vk, 1e-8))\n        hw = a * np.exp(ei_vk)\n        sig = sig * hw\n        xk_prev = sig ** 2\n        xi_w = np.fft.ifft(hw * spec, p.n_fft, axis=0)\n        xi_w = np.real(xi_w)\n\n        x_final[k:k + p.len2] = x_old + xi_w[0:p.len1]\n        x_old = xi_w[p.len1:p.window_size]\n\n    output = from_float(x_final, dtype)\n    output = np.pad(output, (0, len(wav) - len(output)), mode=""constant"")\n    return output\n\n\n## Alternative VAD algorithm to webrctvad. It has the advantage of not requiring to install that \n## darn package and it also works for any sampling rate. Maybe I\'ll eventually use it instead of \n## webrctvad\n# def vad(wav, sampling_rate, eta=0.15, window_size=0):\n#     """"""\n#     TODO: fix doc\n#     Creates a profile of the noise in a given waveform.\n# \n#     :param wav: a waveform containing noise ONLY, as a numpy array of floats or ints. \n#     :param sampling_rate: the sampling rate of the audio\n#     :param window_size: the size of the window the logmmse algorithm operates on. A default value \n#     will be picked if left as 0.\n#     :param eta: voice threshold for noise update. While the voice activation detection value is \n#     below this threshold, the noise profile will be continuously updated throughout the audio. \n#     Set to 0 to disable updating the noise profile.\n#     """"""\n#     wav, dtype = to_float(wav)\n#     wav += np.finfo(np.float64).eps\n#     \n#     if window_size == 0:\n#         window_size = int(math.floor(0.02 * sampling_rate))\n#     \n#     if window_size % 2 == 1:\n#         window_size = window_size + 1\n#     \n#     perc = 50\n#     len1 = int(math.floor(window_size * perc / 100))\n#     len2 = int(window_size - len1)\n#     \n#     win = np.hanning(window_size)\n#     win = win * len2 / np.sum(win)\n#     n_fft = 2 * window_size\n#     \n#     wav_mean = np.zeros(n_fft)\n#     n_frames = len(wav) // window_size\n#     for j in range(0, window_size * n_frames, window_size):\n#         wav_mean += np.absolute(np.fft.fft(win * wav[j:j + window_size], n_fft, axis=0))\n#     noise_mu2 = (wav_mean / n_frames) ** 2\n#     \n#     wav, dtype = to_float(wav)\n#     wav += np.finfo(np.float64).eps\n#     \n#     nframes = int(math.floor(len(wav) / len2) - math.floor(window_size / len2))\n#     vad = np.zeros(nframes * len2, dtype=np.bool)\n# \n#     aa = 0.98\n#     mu = 0.98\n#     ksi_min = 10 ** (-25 / 10)\n#     \n#     xk_prev = np.zeros(len1)\n#     noise_mu2 = noise_mu2\n#     for k in range(0, nframes * len2, len2):\n#         insign = win * wav[k:k + window_size]\n#         \n#         spec = np.fft.fft(insign, n_fft, axis=0)\n#         sig = np.absolute(spec)\n#         sig2 = sig ** 2\n#         \n#         gammak = np.minimum(sig2 / noise_mu2, 40)\n#         \n#         if xk_prev.all() == 0:\n#             ksi = aa + (1 - aa) * np.maximum(gammak - 1, 0)\n#         else:\n#             ksi = aa * xk_prev / noise_mu2 + (1 - aa) * np.maximum(gammak - 1, 0)\n#             ksi = np.maximum(ksi_min, ksi)\n#         \n#         log_sigma_k = gammak * ksi / (1 + ksi) - np.log(1 + ksi)\n#         vad_decision = np.sum(log_sigma_k) / window_size\n#         if vad_decision < eta:\n#             noise_mu2 = mu * noise_mu2 + (1 - mu) * sig2\n#         print(vad_decision)\n#         \n#         a = ksi / (1 + ksi)\n#         vk = a * gammak\n#         ei_vk = 0.5 * expn(1, np.maximum(vk, 1e-8))\n#         hw = a * np.exp(ei_vk)\n#         sig = sig * hw\n#         xk_prev = sig ** 2\n#         \n#         vad[k:k + len2] = vad_decision >= eta\n#         \n#     vad = np.pad(vad, (0, len(wav) - len(vad)), mode=""constant"")\n#     return vad\n\n\ndef to_float(_input):\n    if _input.dtype == np.float64:\n        return _input, _input.dtype\n    elif _input.dtype == np.float32:\n        return _input.astype(np.float64), _input.dtype\n    elif _input.dtype == np.uint8:\n        return (_input - 128) / 128., _input.dtype\n    elif _input.dtype == np.int16:\n        return _input / 32768., _input.dtype\n    elif _input.dtype == np.int32:\n        return _input / 2147483648., _input.dtype\n    raise ValueError(\'Unsupported wave file format\')\n\n\ndef from_float(_input, dtype):\n    if dtype == np.float64:\n        return _input, np.float64\n    elif dtype == np.float32:\n        return _input.astype(np.float32)\n    elif dtype == np.uint8:\n        return ((_input * 128) + 128).astype(np.uint8)\n    elif dtype == np.int16:\n        return (_input * 32768).astype(np.int16)\n    elif dtype == np.int32:\n        print(_input)\n        return (_input * 2147483648).astype(np.int32)\n    raise ValueError(\'Unsupported wave file format\')\n'"
utils/profiler.py,0,"b'from time import perf_counter as timer\nfrom collections import OrderedDict\nimport numpy as np\n\n\nclass Profiler:\n    def __init__(self, summarize_every=5, disabled=False):\n        self.last_tick = timer()\n        self.logs = OrderedDict()\n        self.summarize_every = summarize_every\n        self.disabled = disabled\n    \n    def tick(self, name):\n        if self.disabled:\n            return\n        \n        # Log the time needed to execute that function\n        if not name in self.logs:\n            self.logs[name] = []\n        if len(self.logs[name]) >= self.summarize_every:\n            self.summarize()\n            self.purge_logs()\n        self.logs[name].append(timer() - self.last_tick)\n        \n        self.reset_timer()\n        \n    def purge_logs(self):\n        for name in self.logs:\n            self.logs[name].clear()\n    \n    def reset_timer(self):\n        self.last_tick = timer()\n    \n    def summarize(self):\n        n = max(map(len, self.logs.values()))\n        assert n == self.summarize_every\n        print(""\\nAverage execution time over %d steps:"" % n)\n\n        name_msgs = [""%s (%d/%d):"" % (name, len(deltas), n) for name, deltas in self.logs.items()]\n        pad = max(map(len, name_msgs))\n        for name_msg, deltas in zip(name_msgs, self.logs.values()):\n            print(""  %s  mean: %4.0fms   std: %4.0fms"" % \n                  (name_msg.ljust(pad), np.mean(deltas) * 1000, np.std(deltas) * 1000))\n        print("""", flush=True)    \n        '"
vocoder/audio.py,0,"b'import math\nimport numpy as np\nimport librosa\nimport vocoder.hparams as hp\nfrom scipy.signal import lfilter\n\n\ndef label_2_float(x, bits) :\n    return 2 * x / (2**bits - 1.) - 1.\n\n\ndef float_2_label(x, bits) :\n    assert abs(x).max() <= 1.0\n    x = (x + 1.) * (2**bits - 1) / 2\n    return x.clip(0, 2**bits - 1)\n\n\ndef load_wav(path) :\n    return librosa.load(path, sr=hp.sample_rate)[0]\n\n\ndef save_wav(x, path) :\n    librosa.output.write_wav(path, x.astype(np.float32), sr=hp.sample_rate)\n\n\ndef split_signal(x) :\n    unsigned = x + 2**15\n    coarse = unsigned // 256\n    fine = unsigned % 256\n    return coarse, fine\n\n\ndef combine_signal(coarse, fine) :\n    return coarse * 256 + fine - 2**15\n\n\ndef encode_16bits(x) :\n    return np.clip(x * 2**15, -2**15, 2**15 - 1).astype(np.int16)\n\n\nmel_basis = None\n\n\ndef linear_to_mel(spectrogram):\n    global mel_basis\n    if mel_basis is None:\n        mel_basis = build_mel_basis()\n    return np.dot(mel_basis, spectrogram)\n\n\ndef build_mel_basis():\n    return librosa.filters.mel(hp.sample_rate, hp.n_fft, n_mels=hp.num_mels, fmin=hp.fmin)\n\n\ndef normalize(S):\n    return np.clip((S - hp.min_level_db) / -hp.min_level_db, 0, 1)\n\n\ndef denormalize(S):\n    return (np.clip(S, 0, 1) * -hp.min_level_db) + hp.min_level_db\n\n\ndef amp_to_db(x):\n    return 20 * np.log10(np.maximum(1e-5, x))\n\n\ndef db_to_amp(x):\n    return np.power(10.0, x * 0.05)\n\n\ndef spectrogram(y):\n    D = stft(y)\n    S = amp_to_db(np.abs(D)) - hp.ref_level_db\n    return normalize(S)\n\n\ndef melspectrogram(y):\n    D = stft(y)\n    S = amp_to_db(linear_to_mel(np.abs(D)))\n    return normalize(S)\n\n\ndef stft(y):\n    return librosa.stft(y=y, n_fft=hp.n_fft, hop_length=hp.hop_length, win_length=hp.win_length)\n\n\ndef pre_emphasis(x):\n    return lfilter([1, -hp.preemphasis], [1], x)\n\n\ndef de_emphasis(x):\n    return lfilter([1], [1, -hp.preemphasis], x)\n\n\ndef encode_mu_law(x, mu) :\n    mu = mu - 1\n    fx = np.sign(x) * np.log(1 + mu * np.abs(x)) / np.log(1 + mu)\n    return np.floor((fx + 1) / 2 * mu + 0.5)\n\n\ndef decode_mu_law(y, mu, from_labels=True) :\n    if from_labels: \n        y = label_2_float(y, math.log2(mu))\n    mu = mu - 1\n    x = np.sign(y) / mu * ((1 + mu) ** np.abs(y) - 1)\n    return x\n\n'"
vocoder/display.py,0,"b'import matplotlib.pyplot as plt\nimport time\nimport numpy as np\nimport sys\n\n\ndef progbar(i, n, size=16):\n    done = (i * size) // n\n    bar = \'\'\n    for i in range(size):\n        bar += \'\xe2\x96\x88\' if i <= done else \'\xe2\x96\x91\'\n    return bar\n\n\ndef stream(message) :\n    sys.stdout.write(""\\r{%s}"" % message)\n\n\ndef simple_table(item_tuples) :\n\n    border_pattern = \'+---------------------------------------\'\n    whitespace = \'                                            \'\n\n    headings, cells, = [], []\n\n    for item in item_tuples :\n\n        heading, cell = str(item[0]), str(item[1])\n\n        pad_head = True if len(heading) < len(cell) else False\n\n        pad = abs(len(heading) - len(cell))\n        pad = whitespace[:pad]\n\n        pad_left = pad[:len(pad)//2]\n        pad_right = pad[len(pad)//2:]\n\n        if pad_head :\n            heading = pad_left + heading + pad_right\n        else :\n            cell = pad_left + cell + pad_right\n\n        headings += [heading]\n        cells += [cell]\n\n    border, head, body = \'\', \'\', \'\'\n\n    for i in range(len(item_tuples)) :\n\n        temp_head = f\'| {headings[i]} \'\n        temp_body = f\'| {cells[i]} \'\n\n        border += border_pattern[:len(temp_head)]\n        head += temp_head\n        body += temp_body\n\n        if i == len(item_tuples) - 1 :\n            head += \'|\'\n            body += \'|\'\n            border += \'+\'\n\n    print(border)\n    print(head)\n    print(border)\n    print(body)\n    print(border)\n    print(\' \')\n\n\ndef time_since(started) :\n    elapsed = time.time() - started\n    m = int(elapsed // 60)\n    s = int(elapsed % 60)\n    if m >= 60 :\n        h = int(m // 60)\n        m = m % 60\n        return f\'{h}h {m}m {s}s\'\n    else :\n        return f\'{m}m {s}s\'\n\n\ndef save_attention(attn, path) :\n    fig = plt.figure(figsize=(12, 6))\n    plt.imshow(attn.T, interpolation=\'nearest\', aspect=\'auto\')\n    fig.savefig(f\'{path}.png\', bbox_inches=\'tight\')\n    plt.close(fig)\n\n\ndef save_spectrogram(M, path, length=None) :\n    M = np.flip(M, axis=0)\n    if length : M = M[:, :length]\n    fig = plt.figure(figsize=(12, 6))\n    plt.imshow(M, interpolation=\'nearest\', aspect=\'auto\')\n    fig.savefig(f\'{path}.png\', bbox_inches=\'tight\')\n    plt.close(fig)\n\n\ndef plot(array) : \n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color(\'grey\')\n    ax.yaxis.label.set_color(\'grey\')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis=\'x\', colors=\'grey\', labelsize=23)\n    ax.tick_params(axis=\'y\', colors=\'grey\', labelsize=23)\n    plt.plot(array)\n\n\ndef plot_spec(M) :\n    M = np.flip(M, axis=0)\n    plt.figure(figsize=(18,4))\n    plt.imshow(M, interpolation=\'nearest\', aspect=\'auto\')\n    plt.show()\n\n'"
vocoder/distribution.py,17,"b'import numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\ndef log_sum_exp(x):\n    """""" numerically stable log_sum_exp implementation that prevents overflow """"""\n    # TF ordering\n    axis = len(x.size()) - 1\n    m, _ = torch.max(x, dim=axis)\n    m2, _ = torch.max(x, dim=axis, keepdim=True)\n    return m + torch.log(torch.sum(torch.exp(x - m2), dim=axis))\n\n\n# It is adapted from https://github.com/r9y9/wavenet_vocoder/blob/master/wavenet_vocoder/mixture.py\ndef discretized_mix_logistic_loss(y_hat, y, num_classes=65536,\n                                  log_scale_min=None, reduce=True):\n    if log_scale_min is None:\n        log_scale_min = float(np.log(1e-14))\n    y_hat = y_hat.permute(0,2,1)\n    assert y_hat.dim() == 3\n    assert y_hat.size(1) % 3 == 0\n    nr_mix = y_hat.size(1) // 3\n\n    # (B x T x C)\n    y_hat = y_hat.transpose(1, 2)\n\n    # unpack parameters. (B, T, num_mixtures) x 3\n    logit_probs = y_hat[:, :, :nr_mix]\n    means = y_hat[:, :, nr_mix:2 * nr_mix]\n    log_scales = torch.clamp(y_hat[:, :, 2 * nr_mix:3 * nr_mix], min=log_scale_min)\n\n    # B x T x 1 -> B x T x num_mixtures\n    y = y.expand_as(means)\n\n    centered_y = y - means\n    inv_stdv = torch.exp(-log_scales)\n    plus_in = inv_stdv * (centered_y + 1. / (num_classes - 1))\n    cdf_plus = torch.sigmoid(plus_in)\n    min_in = inv_stdv * (centered_y - 1. / (num_classes - 1))\n    cdf_min = torch.sigmoid(min_in)\n\n    # log probability for edge case of 0 (before scaling)\n    # equivalent: torch.log(F.sigmoid(plus_in))\n    log_cdf_plus = plus_in - F.softplus(plus_in)\n\n    # log probability for edge case of 255 (before scaling)\n    # equivalent: (1 - F.sigmoid(min_in)).log()\n    log_one_minus_cdf_min = -F.softplus(min_in)\n\n    # probability for all other cases\n    cdf_delta = cdf_plus - cdf_min\n\n    mid_in = inv_stdv * centered_y\n    # log probability in the center of the bin, to be used in extreme cases\n    # (not actually used in our code)\n    log_pdf_mid = mid_in - log_scales - 2. * F.softplus(mid_in)\n\n    # tf equivalent\n    """"""\n    log_probs = tf.where(x < -0.999, log_cdf_plus,\n                         tf.where(x > 0.999, log_one_minus_cdf_min,\n                                  tf.where(cdf_delta > 1e-5,\n                                           tf.log(tf.maximum(cdf_delta, 1e-12)),\n                                           log_pdf_mid - np.log(127.5))))\n    """"""\n    # TODO: cdf_delta <= 1e-5 actually can happen. How can we choose the value\n    # for num_classes=65536 case? 1e-7? not sure..\n    inner_inner_cond = (cdf_delta > 1e-5).float()\n\n    inner_inner_out = inner_inner_cond * \\\n        torch.log(torch.clamp(cdf_delta, min=1e-12)) + \\\n        (1. - inner_inner_cond) * (log_pdf_mid - np.log((num_classes - 1) / 2))\n    inner_cond = (y > 0.999).float()\n    inner_out = inner_cond * log_one_minus_cdf_min + (1. - inner_cond) * inner_inner_out\n    cond = (y < -0.999).float()\n    log_probs = cond * log_cdf_plus + (1. - cond) * inner_out\n\n    log_probs = log_probs + F.log_softmax(logit_probs, -1)\n\n    if reduce:\n        return -torch.mean(log_sum_exp(log_probs))\n    else:\n        return -log_sum_exp(log_probs).unsqueeze(-1)\n\n\ndef sample_from_discretized_mix_logistic(y, log_scale_min=None):\n    """"""\n    Sample from discretized mixture of logistic distributions\n    Args:\n        y (Tensor): B x C x T\n        log_scale_min (float): Log scale minimum value\n    Returns:\n        Tensor: sample in range of [-1, 1].\n    """"""\n    if log_scale_min is None:\n        log_scale_min = float(np.log(1e-14))\n    assert y.size(1) % 3 == 0\n    nr_mix = y.size(1) // 3\n\n    # B x T x C\n    y = y.transpose(1, 2)\n    logit_probs = y[:, :, :nr_mix]\n\n    # sample mixture indicator from softmax\n    temp = logit_probs.data.new(logit_probs.size()).uniform_(1e-5, 1.0 - 1e-5)\n    temp = logit_probs.data - torch.log(- torch.log(temp))\n    _, argmax = temp.max(dim=-1)\n\n    # (B, T) -> (B, T, nr_mix)\n    one_hot = to_one_hot(argmax, nr_mix)\n    # select logistic parameters\n    means = torch.sum(y[:, :, nr_mix:2 * nr_mix] * one_hot, dim=-1)\n    log_scales = torch.clamp(torch.sum(\n        y[:, :, 2 * nr_mix:3 * nr_mix] * one_hot, dim=-1), min=log_scale_min)\n    # sample from logistic & clip to interval\n    # we don\'t actually round to the nearest 8bit value when sampling\n    u = means.data.new(means.size()).uniform_(1e-5, 1.0 - 1e-5)\n    x = means + torch.exp(log_scales) * (torch.log(u) - torch.log(1. - u))\n\n    x = torch.clamp(torch.clamp(x, min=-1.), max=1.)\n\n    return x\n\n\ndef to_one_hot(tensor, n, fill_with=1.):\n    # we perform one hot encore with respect to the last axis\n    one_hot = torch.FloatTensor(tensor.size() + (n,)).zero_()\n    if tensor.is_cuda:\n        one_hot = one_hot.cuda()\n    one_hot.scatter_(len(tensor.size()), tensor.unsqueeze(-1), fill_with)\n    return one_hot\n'"
vocoder/gen_wavernn.py,0,"b'from vocoder.models.fatchord_version import  WaveRNN\nfrom vocoder.audio import *\n\n\ndef gen_testset(model: WaveRNN, test_set, samples, batched, target, overlap, save_path):\n    k = model.get_step() // 1000\n\n    for i, (m, x) in enumerate(test_set, 1):\n        if i > samples: \n            break\n\n        print(\'\\n| Generating: %i/%i\' % (i, samples))\n\n        x = x[0].numpy()\n\n        bits = 16 if hp.voc_mode == \'MOL\' else hp.bits\n\n        if hp.mu_law and hp.voc_mode != \'MOL\' :\n            x = decode_mu_law(x, 2**bits, from_labels=True)\n        else :\n            x = label_2_float(x, bits)\n\n        save_wav(x, save_path.joinpath(""%dk_steps_%d_target.wav"" % (k, i)))\n        \n        batch_str = ""gen_batched_target%d_overlap%d"" % (target, overlap) if batched else \\\n            ""gen_not_batched""\n        save_str = save_path.joinpath(""%dk_steps_%d_%s.wav"" % (k, i, batch_str))\n\n        wav = model.generate(m, batched, target, overlap, hp.mu_law)\n        save_wav(wav, save_str)\n\n'"
vocoder/hparams.py,0,"b""from synthesizer.hparams import hparams as _syn_hp\n\n\n# Audio settings------------------------------------------------------------------------\n# Match the values of the synthesizer\nsample_rate = _syn_hp.sample_rate\nn_fft = _syn_hp.n_fft\nnum_mels = _syn_hp.num_mels\nhop_length = _syn_hp.hop_size\nwin_length = _syn_hp.win_size\nfmin = _syn_hp.fmin\nmin_level_db = _syn_hp.min_level_db\nref_level_db = _syn_hp.ref_level_db\nmel_max_abs_value = _syn_hp.max_abs_value\npreemphasis = _syn_hp.preemphasis\napply_preemphasis = _syn_hp.preemphasize\n\nbits = 9                            # bit depth of signal\nmu_law = True                       # Recommended to suppress noise if using raw bits in hp.voc_mode\n                                    # below\n\n\n# WAVERNN / VOCODER --------------------------------------------------------------------------------\nvoc_mode = 'RAW'                    # either 'RAW' (softmax on raw bits) or 'MOL' (sample from \n# mixture of logistics)\nvoc_upsample_factors = (5, 5, 8)    # NB - this needs to correctly factorise hop_length\nvoc_rnn_dims = 512\nvoc_fc_dims = 512\nvoc_compute_dims = 128\nvoc_res_out_dims = 128\nvoc_res_blocks = 10\n\n# Training\nvoc_batch_size = 100\nvoc_lr = 1e-4\nvoc_gen_at_checkpoint = 5           # number of samples to generate at each checkpoint\nvoc_pad = 2                         # this will pad the input so that the resnet can 'see' wider \n                                    # than input length\nvoc_seq_len = hop_length * 5        # must be a multiple of hop_length\n\n# Generating / Synthesizing\nvoc_gen_batched = True              # very fast (realtime+) single utterance batched generation\nvoc_target = 8000                   # target number of samples to be generated in each batch entry\nvoc_overlap = 400                   # number of samples for crossfading between batches\n"""
vocoder/inference.py,2,"b'from vocoder.models.fatchord_version import WaveRNN\nfrom vocoder import hparams as hp\nimport torch\n\n\n_model = None   # type: WaveRNN\n\ndef load_model(weights_fpath, verbose=True):\n    global _model\n    \n    if verbose:\n        print(""Building Wave-RNN"")\n    _model = WaveRNN(\n        rnn_dims=hp.voc_rnn_dims,\n        fc_dims=hp.voc_fc_dims,\n        bits=hp.bits,\n        pad=hp.voc_pad,\n        upsample_factors=hp.voc_upsample_factors,\n        feat_dims=hp.num_mels,\n        compute_dims=hp.voc_compute_dims,\n        res_out_dims=hp.voc_res_out_dims,\n        res_blocks=hp.voc_res_blocks,\n        hop_length=hp.hop_length,\n        sample_rate=hp.sample_rate,\n        mode=hp.voc_mode\n    ).cuda()\n    \n    if verbose:\n        print(""Loading model weights at %s"" % weights_fpath)\n    checkpoint = torch.load(weights_fpath)\n    _model.load_state_dict(checkpoint[\'model_state\'])\n    _model.eval()\n\n\ndef is_loaded():\n    return _model is not None\n\n\ndef infer_waveform(mel, normalize=True,  batched=True, target=8000, overlap=800, \n                   progress_callback=None):\n    """"""\n    Infers the waveform of a mel spectrogram output by the synthesizer (the format must match \n    that of the synthesizer!)\n    \n    :param normalize:  \n    :param batched: \n    :param target: \n    :param overlap: \n    :return: \n    """"""\n    if _model is None:\n        raise Exception(""Please load Wave-RNN in memory before using it"")\n    \n    if normalize:\n        mel = mel / hp.mel_max_abs_value\n    mel = torch.from_numpy(mel[None, ...])\n    wav = _model.generate(mel, batched, target, overlap, hp.mu_law, progress_callback)\n    return wav\n'"
vocoder/train.py,2,"b'from vocoder.models.fatchord_version import WaveRNN\nfrom vocoder.vocoder_dataset import VocoderDataset, collate_vocoder\nfrom vocoder.distribution import discretized_mix_logistic_loss\nfrom vocoder.display import stream, simple_table\nfrom vocoder.gen_wavernn import gen_testset\nfrom torch.utils.data import DataLoader\nfrom pathlib import Path\nfrom torch import optim\nimport torch.nn.functional as F\nimport vocoder.hparams as hp\nimport numpy as np\nimport time\n\n\ndef train(run_id: str, syn_dir: Path, voc_dir: Path, models_dir: Path, ground_truth: bool,\n          save_every: int, backup_every: int, force_restart: bool):\n    # Check to make sure the hop length is correctly factorised\n    assert np.cumprod(hp.voc_upsample_factors)[-1] == hp.hop_length\n    \n    # Instantiate the model\n    print(""Initializing the model..."")\n    model = WaveRNN(\n        rnn_dims=hp.voc_rnn_dims,\n        fc_dims=hp.voc_fc_dims,\n        bits=hp.bits,\n        pad=hp.voc_pad,\n        upsample_factors=hp.voc_upsample_factors,\n        feat_dims=hp.num_mels,\n        compute_dims=hp.voc_compute_dims,\n        res_out_dims=hp.voc_res_out_dims,\n        res_blocks=hp.voc_res_blocks,\n        hop_length=hp.hop_length,\n        sample_rate=hp.sample_rate,\n        mode=hp.voc_mode\n    ).cuda()\n       \n    # Initialize the optimizer\n    optimizer = optim.Adam(model.parameters())\n    for p in optimizer.param_groups: \n        p[""lr""] = hp.voc_lr\n    loss_func = F.cross_entropy if model.mode == ""RAW"" else discretized_mix_logistic_loss\n\n    # Load the weights\n    model_dir = models_dir.joinpath(run_id)\n    model_dir.mkdir(exist_ok=True)\n    weights_fpath = model_dir.joinpath(run_id + "".pt"")\n    if force_restart or not weights_fpath.exists():\n        print(""\\nStarting the training of WaveRNN from scratch\\n"")\n        model.save(weights_fpath, optimizer)\n    else:\n        print(""\\nLoading weights at %s"" % weights_fpath)\n        model.load(weights_fpath, optimizer)\n        print(""WaveRNN weights loaded from step %d"" % model.step)\n    \n    # Initialize the dataset\n    metadata_fpath = syn_dir.joinpath(""train.txt"") if ground_truth else \\\n        voc_dir.joinpath(""synthesized.txt"")\n    mel_dir = syn_dir.joinpath(""mels"") if ground_truth else voc_dir.joinpath(""mels_gta"")\n    wav_dir = syn_dir.joinpath(""audio"")\n    dataset = VocoderDataset(metadata_fpath, mel_dir, wav_dir)\n    test_loader = DataLoader(dataset,\n                             batch_size=1,\n                             shuffle=True,\n                             pin_memory=True)\n\n    # Begin the training\n    simple_table([(\'Batch size\', hp.voc_batch_size),\n                  (\'LR\', hp.voc_lr),\n                  (\'Sequence Len\', hp.voc_seq_len)])\n    \n    for epoch in range(1, 350):\n        data_loader = DataLoader(dataset,\n                                 collate_fn=collate_vocoder,\n                                 batch_size=hp.voc_batch_size,\n                                 num_workers=2,\n                                 shuffle=True,\n                                 pin_memory=True)\n        start = time.time()\n        running_loss = 0.\n\n        for i, (x, y, m) in enumerate(data_loader, 1):\n            x, m, y = x.cuda(), m.cuda(), y.cuda()\n            \n            # Forward pass\n            y_hat = model(x, m)\n            if model.mode == \'RAW\':\n                y_hat = y_hat.transpose(1, 2).unsqueeze(-1)\n            elif model.mode == \'MOL\':\n                y = y.float()\n            y = y.unsqueeze(-1)\n            \n            # Backward pass\n            loss = loss_func(y_hat, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            speed = i / (time.time() - start)\n            avg_loss = running_loss / i\n\n            step = model.get_step()\n            k = step // 1000\n\n            if backup_every != 0 and step % backup_every == 0 :\n                model.checkpoint(model_dir, optimizer)\n                \n            if save_every != 0 and step % save_every == 0 :\n                model.save(weights_fpath, optimizer)\n\n            msg = f""| Epoch: {epoch} ({i}/{len(data_loader)}) | "" \\\n                f""Loss: {avg_loss:.4f} | {speed:.1f} "" \\\n                f""steps/s | Step: {k}k | ""\n            stream(msg)\n\n\n        gen_testset(model, test_loader, hp.voc_gen_at_checkpoint, hp.voc_gen_batched,\n                    hp.voc_target, hp.voc_overlap, model_dir)\n        print("""")\n'"
vocoder/vocoder_dataset.py,3,"b'from torch.utils.data import Dataset\nfrom pathlib import Path\nfrom vocoder import audio\nimport vocoder.hparams as hp\nimport numpy as np\nimport torch\n\n\nclass VocoderDataset(Dataset):\n    def __init__(self, metadata_fpath: Path, mel_dir: Path, wav_dir: Path):\n        print(""Using inputs from:\\n\\t%s\\n\\t%s\\n\\t%s"" % (metadata_fpath, mel_dir, wav_dir))\n        \n        with metadata_fpath.open(""r"") as metadata_file:\n            metadata = [line.split(""|"") for line in metadata_file]\n        \n        gta_fnames = [x[1] for x in metadata if int(x[4])]\n        gta_fpaths = [mel_dir.joinpath(fname) for fname in gta_fnames]\n        wav_fnames = [x[0] for x in metadata if int(x[4])]\n        wav_fpaths = [wav_dir.joinpath(fname) for fname in wav_fnames]\n        self.samples_fpaths = list(zip(gta_fpaths, wav_fpaths))\n        \n        print(""Found %d samples"" % len(self.samples_fpaths))\n    \n    def __getitem__(self, index):  \n        mel_path, wav_path = self.samples_fpaths[index]\n        \n        # Load the mel spectrogram and adjust its range to [-1, 1]\n        mel = np.load(mel_path).T.astype(np.float32) / hp.mel_max_abs_value\n        \n        # Load the wav\n        wav = np.load(wav_path)\n        if hp.apply_preemphasis:\n            wav = audio.pre_emphasis(wav)\n        wav = np.clip(wav, -1, 1)\n        \n        # Fix for missing padding   # TODO: settle on whether this is any useful\n        r_pad =  (len(wav) // hp.hop_length + 1) * hp.hop_length - len(wav)\n        wav = np.pad(wav, (0, r_pad), mode=\'constant\')\n        assert len(wav) >= mel.shape[1] * hp.hop_length\n        wav = wav[:mel.shape[1] * hp.hop_length]\n        assert len(wav) % hp.hop_length == 0\n        \n        # Quantize the wav\n        if hp.voc_mode == \'RAW\':\n            if hp.mu_law:\n                quant = audio.encode_mu_law(wav, mu=2 ** hp.bits)\n            else:\n                quant = audio.float_2_label(wav, bits=hp.bits)\n        elif hp.voc_mode == \'MOL\':\n            quant = audio.float_2_label(wav, bits=16)\n            \n        return mel.astype(np.float32), quant.astype(np.int64)\n\n    def __len__(self):\n        return len(self.samples_fpaths)\n        \n        \ndef collate_vocoder(batch):\n    mel_win = hp.voc_seq_len // hp.hop_length + 2 * hp.voc_pad\n    max_offsets = [x[0].shape[-1] -2 - (mel_win + 2 * hp.voc_pad) for x in batch]\n    mel_offsets = [np.random.randint(0, offset) for offset in max_offsets]\n    sig_offsets = [(offset + hp.voc_pad) * hp.hop_length for offset in mel_offsets]\n\n    mels = [x[0][:, mel_offsets[i]:mel_offsets[i] + mel_win] for i, x in enumerate(batch)]\n\n    labels = [x[1][sig_offsets[i]:sig_offsets[i] + hp.voc_seq_len + 1] for i, x in enumerate(batch)]\n\n    mels = np.stack(mels).astype(np.float32)\n    labels = np.stack(labels).astype(np.int64)\n\n    mels = torch.tensor(mels)\n    labels = torch.tensor(labels).long()\n\n    x = labels[:, :hp.voc_seq_len]\n    y = labels[:, 1:]\n\n    bits = 16 if hp.voc_mode == \'MOL\' else hp.bits\n\n    x = audio.label_2_float(x.float(), bits)\n\n    if hp.voc_mode == \'MOL\' :\n        y = audio.label_2_float(y.float(), bits)\n\n    return x, y, mels'"
encoder/data_objects/__init__.py,0,b'from encoder.data_objects.speaker_verification_dataset import SpeakerVerificationDataset\nfrom encoder.data_objects.speaker_verification_dataset import SpeakerVerificationDataLoader\n'
encoder/data_objects/random_cycler.py,0,"b'import random\n\nclass RandomCycler:\n    """"""\n    Creates an internal copy of a sequence and allows access to its items in a constrained random \n    order. For a source sequence of n items and one or several consecutive queries of a total \n    of m items, the following guarantees hold (one implies the other):\n        - Each item will be returned between m // n and ((m - 1) // n) + 1 times.\n        - Between two appearances of the same item, there may be at most 2 * (n - 1) other items.\n    """"""\n    \n    def __init__(self, source):\n        if len(source) == 0:\n            raise Exception(""Can\'t create RandomCycler from an empty collection"")\n        self.all_items = list(source)\n        self.next_items = []\n    \n    def sample(self, count: int):\n        shuffle = lambda l: random.sample(l, len(l))\n        \n        out = []\n        while count > 0:\n            if count >= len(self.all_items):\n                out.extend(shuffle(list(self.all_items)))\n                count -= len(self.all_items)\n                continue\n            n = min(count, len(self.next_items))\n            out.extend(self.next_items[:n])\n            count -= n\n            self.next_items = self.next_items[n:]\n            if len(self.next_items) == 0:\n                self.next_items = shuffle(list(self.all_items))\n        return out\n    \n    def __next__(self):\n        return self.sample(1)[0]\n\n'"
encoder/data_objects/speaker.py,0,"b'from encoder.data_objects.random_cycler import RandomCycler\nfrom encoder.data_objects.utterance import Utterance\nfrom pathlib import Path\n\n# Contains the set of utterances of a single speaker\nclass Speaker:\n    def __init__(self, root: Path):\n        self.root = root\n        self.name = root.name\n        self.utterances = None\n        self.utterance_cycler = None\n        \n    def _load_utterances(self):\n        with self.root.joinpath(""_sources.txt"").open(""r"") as sources_file:\n            sources = [l.split("","") for l in sources_file]\n        sources = {frames_fname: wave_fpath for frames_fname, wave_fpath in sources}\n        self.utterances = [Utterance(self.root.joinpath(f), w) for f, w in sources.items()]\n        self.utterance_cycler = RandomCycler(self.utterances)\n               \n    def random_partial(self, count, n_frames):\n        """"""\n        Samples a batch of <count> unique partial utterances from the disk in a way that all \n        utterances come up at least once every two cycles and in a random order every time.\n        \n        :param count: The number of partial utterances to sample from the set of utterances from \n        that speaker. Utterances are guaranteed not to be repeated if <count> is not larger than \n        the number of utterances available.\n        :param n_frames: The number of frames in the partial utterance.\n        :return: A list of tuples (utterance, frames, range) where utterance is an Utterance, \n        frames are the frames of the partial utterances and range is the range of the partial \n        utterance with regard to the complete utterance.\n        """"""\n        if self.utterances is None:\n            self._load_utterances()\n\n        utterances = self.utterance_cycler.sample(count)\n\n        a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n        return a\n'"
encoder/data_objects/speaker_batch.py,0,"b'import numpy as np\nfrom typing import List\nfrom encoder.data_objects.speaker import Speaker\n\nclass SpeakerBatch:\n    def __init__(self, speakers: List[Speaker], utterances_per_speaker: int, n_frames: int):\n        self.speakers = speakers\n        self.partials = {s: s.random_partial(utterances_per_speaker, n_frames) for s in speakers}\n        \n        # Array of shape (n_speakers * n_utterances, n_frames, mel_n), e.g. for 3 speakers with\n        # 4 utterances each of 160 frames of 40 mel coefficients: (12, 160, 40)\n        self.data = np.array([frames for s in speakers for _, frames, _ in self.partials[s]])\n'"
encoder/data_objects/speaker_verification_dataset.py,1,"b'from encoder.data_objects.random_cycler import RandomCycler\nfrom encoder.data_objects.speaker_batch import SpeakerBatch\nfrom encoder.data_objects.speaker import Speaker\nfrom encoder.params_data import partials_n_frames\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\n\n# TODO: improve with a pool of speakers for data efficiency\n\nclass SpeakerVerificationDataset(Dataset):\n    def __init__(self, datasets_root: Path):\n        self.root = datasets_root\n        speaker_dirs = [f for f in self.root.glob(""*"") if f.is_dir()]\n        if len(speaker_dirs) == 0:\n            raise Exception(""No speakers found. Make sure you are pointing to the directory ""\n                            ""containing all preprocessed speaker directories."")\n        self.speakers = [Speaker(speaker_dir) for speaker_dir in speaker_dirs]\n        self.speaker_cycler = RandomCycler(self.speakers)\n\n    def __len__(self):\n        return int(1e10)\n        \n    def __getitem__(self, index):\n        return next(self.speaker_cycler)\n    \n    def get_logs(self):\n        log_string = """"\n        for log_fpath in self.root.glob(""*.txt""):\n            with log_fpath.open(""r"") as log_file:\n                log_string += """".join(log_file.readlines())\n        return log_string\n    \n    \nclass SpeakerVerificationDataLoader(DataLoader):\n    def __init__(self, dataset, speakers_per_batch, utterances_per_speaker, sampler=None, \n                 batch_sampler=None, num_workers=0, pin_memory=False, timeout=0, \n                 worker_init_fn=None):\n        self.utterances_per_speaker = utterances_per_speaker\n\n        super().__init__(\n            dataset=dataset, \n            batch_size=speakers_per_batch, \n            shuffle=False, \n            sampler=sampler, \n            batch_sampler=batch_sampler, \n            num_workers=num_workers,\n            collate_fn=self.collate, \n            pin_memory=pin_memory, \n            drop_last=False, \n            timeout=timeout, \n            worker_init_fn=worker_init_fn\n        )\n\n    def collate(self, speakers):\n        return SpeakerBatch(speakers, self.utterances_per_speaker, partials_n_frames) \n    '"
encoder/data_objects/utterance.py,0,"b'import numpy as np\n\n\nclass Utterance:\n    def __init__(self, frames_fpath, wave_fpath):\n        self.frames_fpath = frames_fpath\n        self.wave_fpath = wave_fpath\n        \n    def get_frames(self):\n        return np.load(self.frames_fpath)\n\n    def random_partial(self, n_frames):\n        """"""\n        Crops the frames into a partial utterance of n_frames\n        \n        :param n_frames: The number of frames of the partial utterance\n        :return: the partial utterance frames and a tuple indicating the start and end of the \n        partial utterance in the complete utterance.\n        """"""\n        frames = self.get_frames()\n        if frames.shape[0] == n_frames:\n            start = 0\n        else:\n            start = np.random.randint(0, frames.shape[0] - n_frames)\n        end = start + n_frames\n        return frames[start:end], (start, end)'"
synthesizer/models/__init__.py,0,"b'from .tacotron import Tacotron\n\n\ndef create_model(name, hparams):\n  if name == ""Tacotron"":\n    return Tacotron(hparams)\n  else:\n    raise Exception(""Unknown model: "" + name)\n'"
synthesizer/models/architecture_wrappers.py,0,"b'""""""A set of wrappers useful for tacotron 2 architecture\nAll notations and variable names were used in concordance with originial tensorflow implementation\n""""""\nimport collections\nimport tensorflow as tf\nfrom synthesizer.models.attention import _compute_attention\nfrom tensorflow.contrib.rnn import RNNCell\nfrom tensorflow.python.framework import ops, tensor_shape\nfrom tensorflow.python.ops import array_ops, check_ops, rnn_cell_impl, tensor_array_ops\nfrom tensorflow.python.util import nest\n\n_zero_state_tensors = rnn_cell_impl._zero_state_tensors\n\n\n\nclass TacotronEncoderCell(RNNCell):\n\t""""""Tacotron 2 Encoder Cell\n\tPasses inputs through a stack of convolutional layers then through a bidirectional LSTM\n\tlayer to predict the hidden representation vector (or memory)\n\t""""""\n\n\tdef __init__(self, convolutional_layers, lstm_layer):\n\t\t""""""Initialize encoder parameters\n\n\t\tArgs:\n\t\t\tconvolutional_layers: Encoder convolutional block class\n\t\t\tlstm_layer: encoder bidirectional lstm layer class\n\t\t""""""\n\t\tsuper(TacotronEncoderCell, self).__init__()\n\t\t#Initialize encoder layers\n\t\tself._convolutions = convolutional_layers\n\t\tself._cell = lstm_layer\n\n\tdef __call__(self, inputs, input_lengths=None):\n\t\t#Pass input sequence through a stack of convolutional layers\n\t\tconv_output = self._convolutions(inputs)\n\n\t\t#Extract hidden representation from encoder lstm cells\n\t\thidden_representation = self._cell(conv_output, input_lengths)\n\n\t\t#For shape visualization\n\t\tself.conv_output_shape = conv_output.shape\n\t\treturn hidden_representation\n\n\nclass TacotronDecoderCellState(\n\tcollections.namedtuple(""TacotronDecoderCellState"",\n\t (""cell_state"", ""attention"", ""time"", ""alignments"",\n\t  ""alignment_history""))):\n\t""""""`namedtuple` storing the state of a `TacotronDecoderCell`.\n\tContains:\n\t  - `cell_state`: The state of the wrapped `RNNCell` at the previous time\n\t\tstep.\n\t  - `attention`: The attention emitted at the previous time step.\n\t  - `time`: int32 scalar containing the current time step.\n\t  - `alignments`: A single or tuple of `Tensor`(s) containing the alignments\n\t\t emitted at the previous time step for each attention mechanism.\n\t  - `alignment_history`: a single or tuple of `TensorArray`(s)\n\t\t containing alignment matrices from all time steps for each attention\n\t\t mechanism. Call `stack()` on each to convert to a `Tensor`.\n\t""""""\n\tdef replace(self, **kwargs):\n\t\t""""""Clones the current state while overwriting components provided by kwargs.\n\t\t""""""\n\t\treturn super(TacotronDecoderCellState, self)._replace(**kwargs)\n\nclass TacotronDecoderCell(RNNCell):\n\t""""""Tactron 2 Decoder Cell\n\tDecodes encoder output and previous mel frames into next r frames\n\n\tDecoder Step i:\n\t\t1) Prenet to compress last output information\n\t\t2) Concat compressed inputs with previous context vector (input feeding) *\n\t\t3) Decoder RNN (actual decoding) to predict current state s_{i} *\n\t\t4) Compute new context vector c_{i} based on s_{i} and a cumulative sum of previous alignments *\n\t\t5) Predict new output y_{i} using s_{i} and c_{i} (concatenated)\n\t\t6) Predict <stop_token> output ys_{i} using s_{i} and c_{i} (concatenated)\n\n\t* : This is typically taking a vanilla LSTM, wrapping it using tensorflow""s attention wrapper,\n\tand wrap that with the prenet before doing an input feeding, and with the prediction layer\n\tthat uses RNN states to project on output space. Actions marked with (*) can be replaced with\n\ttensorflow""s attention wrapper call if it was using cumulative alignments instead of previous alignments only.\n\t""""""\n\n\tdef __init__(self, prenet, attention_mechanism, rnn_cell, frame_projection, stop_projection):\n\t\t""""""Initialize decoder parameters\n\n\t\tArgs:\n\t\t    prenet: A tensorflow fully connected layer acting as the decoder pre-net\n\t\t    attention_mechanism: A _BaseAttentionMechanism instance, usefull to\n\t\t\t    learn encoder-decoder alignments\n\t\t    rnn_cell: Instance of RNNCell, main body of the decoder\n\t\t    frame_projection: tensorflow fully connected layer with r * num_mels output units\n\t\t    stop_projection: tensorflow fully connected layer, expected to project to a scalar\n\t\t\t    and through a sigmoid activation\n\t\t\tmask_finished: Boolean, Whether to mask decoder frames after the <stop_token>\n\t\t""""""\n\t\tsuper(TacotronDecoderCell, self).__init__()\n\t\t#Initialize decoder layers\n\t\tself._prenet = prenet\n\t\tself._attention_mechanism = attention_mechanism\n\t\tself._cell = rnn_cell\n\t\tself._frame_projection = frame_projection\n\t\tself._stop_projection = stop_projection\n\n\t\tself._attention_layer_size = self._attention_mechanism.values.get_shape()[-1].value\n\n\tdef _batch_size_checks(self, batch_size, error_message):\n\t\treturn [check_ops.assert_equal(batch_size,\n\t\t  self._attention_mechanism.batch_size,\n\t\t  message=error_message)]\n\n\t@property\n\tdef output_size(self):\n\t\treturn self._frame_projection.shape\n\n\t@property\n\tdef state_size(self):\n\t\t""""""The `state_size` property of `TacotronDecoderCell`.\n\n\t\tReturns:\n\t\t  An `TacotronDecoderCell` tuple containing shapes used by this object.\n\t\t""""""\n\t\treturn TacotronDecoderCellState(\n\t\t\tcell_state=self._cell._cell.state_size,\n\t\t\ttime=tensor_shape.TensorShape([]),\n\t\t\tattention=self._attention_layer_size,\n\t\t\talignments=self._attention_mechanism.alignments_size,\n\t\t\talignment_history=())\n\n\tdef zero_state(self, batch_size, dtype):\n\t\t""""""Return an initial (zero) state tuple for this `AttentionWrapper`.\n\n\t\tArgs:\n\t\t  batch_size: `0D` integer tensor: the batch size.\n\t\t  dtype: The internal state data type.\n\t\tReturns:\n\t\t  An `TacotronDecoderCellState` tuple containing zeroed out tensors and,\n\t\t  possibly, empty `TensorArray` objects.\n\t\tRaises:\n\t\t  ValueError: (or, possibly at runtime, InvalidArgument), if\n\t\t\t`batch_size` does not match the output size of the encoder passed\n\t\t\tto the wrapper object at initialization time.\n\t\t""""""\n\t\twith ops.name_scope(type(self).__name__ + ""ZeroState"", values=[batch_size]):\n\t\t\tcell_state = self._cell._cell.zero_state(batch_size, dtype)\n\t\t\terror_message = (\n\t\t\t\t""When calling zero_state of TacotronDecoderCell %s: "" % self._base_name +\n\t\t\t\t""Non-matching batch sizes between the memory ""\n\t\t\t\t""(encoder output) and the requested batch size."")\n\t\t\twith ops.control_dependencies(\n\t\t\t\tself._batch_size_checks(batch_size, error_message)):\n\t\t\t\tcell_state = nest.map_structure(\n\t\t\t\t\tlambda s: array_ops.identity(s, name=""checked_cell_state""),\n\t\t\t\t\tcell_state)\n\t\t\treturn TacotronDecoderCellState(\n\t\t\t\tcell_state=cell_state,\n\t\t\t\ttime=array_ops.zeros([], dtype=tf.int32),\n\t\t\t\tattention=_zero_state_tensors(self._attention_layer_size, batch_size,\n\t\t\t\t  dtype),\n\t\t\t\talignments=self._attention_mechanism.initial_alignments(batch_size, dtype),\n\t\t\t\talignment_history=tensor_array_ops.TensorArray(dtype=dtype, size=0,\n\t\t\t\tdynamic_size=True))\n\n\tdef __call__(self, inputs, state):\n\t\t#Information bottleneck (essential for learning attention)\n\t\tprenet_output = self._prenet(inputs)\n\n\t\t#Concat context vector and prenet output to form LSTM cells input (input feeding)\n\t\tLSTM_input = tf.concat([prenet_output, state.attention], axis=-1)\n\n\t\t#Unidirectional LSTM layers\n\t\tLSTM_output, next_cell_state = self._cell(LSTM_input, state.cell_state)\n\n\n\t\t#Compute the attention (context) vector and alignments using\n\t\t#the new decoder cell hidden state as query vector\n\t\t#and cumulative alignments to extract location features\n\t\t#The choice of the new cell hidden state (s_{i}) of the last\n\t\t#decoder RNN Cell is based on Luong et Al. (2015):\n\t\t#https://arxiv.org/pdf/1508.04025.pdf\n\t\tprevious_alignments = state.alignments\n\t\tprevious_alignment_history = state.alignment_history\n\t\tcontext_vector, alignments, cumulated_alignments = _compute_attention(self._attention_mechanism,\n\t\t\tLSTM_output,\n\t\t\tprevious_alignments,\n\t\t\tattention_layer=None)\n\n\t\t#Concat LSTM outputs and context vector to form projections inputs\n\t\tprojections_input = tf.concat([LSTM_output, context_vector], axis=-1)\n\n\t\t#Compute predicted frames and predicted <stop_token>\n\t\tcell_outputs = self._frame_projection(projections_input)\n\t\tstop_tokens = self._stop_projection(projections_input)\n\n\t\t#Save alignment history\n\t\talignment_history = previous_alignment_history.write(state.time, alignments)\n\n\t\t#Prepare next decoder state\n\t\tnext_state = TacotronDecoderCellState(\n\t\t\ttime=state.time + 1,\n\t\t\tcell_state=next_cell_state,\n\t\t\tattention=context_vector,\n\t\t\talignments=cumulated_alignments,\n\t\t\talignment_history=alignment_history)\n\n\t\treturn (cell_outputs, stop_tokens), next_state\n'"
synthesizer/models/attention.py,0,"b'""""""Attention file for location based attention (compatible with tensorflow attention wrapper)""""""\n\nimport tensorflow as tf\nfrom tensorflow.contrib.seq2seq.python.ops.attention_wrapper import BahdanauAttention\nfrom tensorflow.python.layers import core as layers_core\nfrom tensorflow.python.ops import array_ops, math_ops, nn_ops, variable_scope\n\n\n#From https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\ndef _compute_attention(attention_mechanism, cell_output, attention_state,\n\t\t\t\t\t   attention_layer):\n\t""""""Computes the attention and alignments for a given attention_mechanism.""""""\n\talignments, next_attention_state = attention_mechanism(\n\t\tcell_output, state=attention_state)\n\n\t# Reshape from [batch_size, memory_time] to [batch_size, 1, memory_time]\n\texpanded_alignments = array_ops.expand_dims(alignments, 1)\n\t# Context is the inner product of alignments and values along the\n\t# memory time dimension.\n\t# alignments shape is\n\t#   [batch_size, 1, memory_time]\n\t# attention_mechanism.values shape is\n\t#   [batch_size, memory_time, memory_size]\n\t# the batched matmul is over memory_time, so the output shape is\n\t#   [batch_size, 1, memory_size].\n\t# we then squeeze out the singleton dim.\n\tcontext = math_ops.matmul(expanded_alignments, attention_mechanism.values)\n\tcontext = array_ops.squeeze(context, [1])\n\n\tif attention_layer is not None:\n\t\tattention = attention_layer(array_ops.concat([cell_output, context], 1))\n\telse:\n\t\tattention = context\n\n\treturn attention, alignments, next_attention_state\n\n\ndef _location_sensitive_score(W_query, W_fil, W_keys):\n\t""""""Impelements Bahdanau-style (cumulative) scoring function.\n\tThis attention is described in:\n\t\tJ. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Ben-\n\t  gio, \xe2\x80\x9cAttention-based models for speech recognition,\xe2\x80\x9d in Ad-\n\t  vances in Neural Information Processing Systems, 2015, pp.\n\t  577\xe2\x80\x93585.\n\n\t#############################################################################\n\t\t\t  hybrid attention (content-based + location-based)\n\t\t\t\t\t\t\t   f = F * \xce\xb1_{i-1}\n\t   energy = dot(v_a, tanh(W_keys(h_enc) + W_query(h_dec) + W_fil(f) + b_a))\n\t#############################################################################\n\n\tArgs:\n\t\tW_query: Tensor, shape ""[batch_size, 1, attention_dim]"" to compare to location features.\n\t\tW_location: processed previous alignments into location features, shape ""[batch_size, max_time, attention_dim]""\n\t\tW_keys: Tensor, shape ""[batch_size, max_time, attention_dim]"", typically the encoder outputs.\n\tReturns:\n\t\tA ""[batch_size, max_time]"" attention score (energy)\n\t""""""\n\t# Get the number of hidden units from the trailing dimension of keys\n\tdtype = W_query.dtype\n\tnum_units = W_keys.shape[-1].value or array_ops.shape(W_keys)[-1]\n\n\tv_a = tf.get_variable(\n\t\t""attention_variable_projection"", shape=[num_units], dtype=dtype,\n\t\tinitializer=tf.contrib.layers.xavier_initializer())\n\tb_a = tf.get_variable(\n\t\t""attention_bias"", shape=[num_units], dtype=dtype,\n\t\tinitializer=tf.zeros_initializer())\n\n\treturn tf.reduce_sum(v_a * tf.tanh(W_keys + W_query + W_fil + b_a), [2])\n\ndef _smoothing_normalization(e):\n\t""""""Applies a smoothing normalization function instead of softmax\n\tIntroduced in:\n\t\tJ. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Ben-\n\t  gio, \xe2\x80\x9cAttention-based models for speech recognition,\xe2\x80\x9d in Ad-\n\t  vances in Neural Information Processing Systems, 2015, pp.\n\t  577\xe2\x80\x93585.\n\n\t############################################################################\n\t\t\t\t\t\tSmoothing normalization function\n\t\t\t\ta_{i, j} = sigmoid(e_{i, j}) / sum_j(sigmoid(e_{i, j}))\n\t############################################################################\n\n\tArgs:\n\t\te: matrix [batch_size, max_time(memory_time)]: expected to be energy (score)\n\t\t\tvalues of an attention mechanism\n\tReturns:\n\t\tmatrix [batch_size, max_time]: [0, 1] normalized alignments with possible\n\t\t\tattendance to multiple memory time steps.\n\t""""""\n\treturn tf.nn.sigmoid(e) / tf.reduce_sum(tf.nn.sigmoid(e), axis=-1, keepdims=True)\n\n\nclass LocationSensitiveAttention(BahdanauAttention):\n\t""""""Impelements Bahdanau-style (cumulative) scoring function.\n\tUsually referred to as ""hybrid"" attention (content-based + location-based)\n\tExtends the additive attention described in:\n\t""D. Bahdanau, K. Cho, and Y. Bengio, \xe2\x80\x9cNeural machine transla-\n  tion by jointly learning to align and translate,\xe2\x80\x9d in Proceedings\n  of ICLR, 2015.""\n\tto use previous alignments as additional location features.\n\n\tThis attention is described in:\n\tJ. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Ben-\n  gio, \xe2\x80\x9cAttention-based models for speech recognition,\xe2\x80\x9d in Ad-\n  vances in Neural Information Processing Systems, 2015, pp.\n  577\xe2\x80\x93585.\n\t""""""\n\n\tdef __init__(self,\n\t\t\t\t num_units,\n\t\t\t\t memory,\n\t\t\t\t hparams,\n\t\t\t\t mask_encoder=True,\n\t\t\t\t memory_sequence_length=None,\n\t\t\t\t smoothing=False,\n\t\t\t\t cumulate_weights=True,\n\t\t\t\t name=""LocationSensitiveAttention""):\n\t\t""""""Construct the Attention mechanism.\n\t\tArgs:\n\t\t\tnum_units: The depth of the query mechanism.\n\t\t\tmemory: The memory to query; usually the output of an RNN encoder.  This\n\t\t\t\ttensor should be shaped `[batch_size, max_time, ...]`.\n\t\t\tmask_encoder (optional): Boolean, whether to mask encoder paddings.\n\t\t\tmemory_sequence_length (optional): Sequence lengths for the batch entries\n\t\t\t\tin memory.  If provided, the memory tensor rows are masked with zeros\n\t\t\t\tfor values past the respective sequence lengths. Only relevant if mask_encoder = True.\n\t\t\tsmoothing (optional): Boolean. Determines which normalization function to use.\n\t\t\t\tDefault normalization function (probablity_fn) is softmax. If smoothing is\n\t\t\t\tenabled, we replace softmax with:\n\t\t\t\t\t\ta_{i, j} = sigmoid(e_{i, j}) / sum_j(sigmoid(e_{i, j}))\n\t\t\t\tIntroduced in:\n\t\t\t\t\tJ. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Ben-\n\t\t\t\t  gio, \xe2\x80\x9cAttention-based models for speech recognition,\xe2\x80\x9d in Ad-\n\t\t\t\t  vances in Neural Information Processing Systems, 2015, pp.\n\t\t\t\t  577\xe2\x80\x93585.\n\t\t\t\tThis is mainly used if the model wants to attend to multiple input parts\n\t\t\t\tat the same decoding step. We probably won""t be using it since multiple sound\n\t\t\t\tframes may depend on the same character/phone, probably not the way around.\n\t\t\t\tNote:\n\t\t\t\t\tWe still keep it implemented in case we want to test it. They used it in the\n\t\t\t\t\tpaper in the context of speech recognition, where one phoneme may depend on\n\t\t\t\t\tmultiple subsequent sound frames.\n\t\t\tname: Name to use when creating ops.\n\t\t""""""\n\t\t#Create normalization function\n\t\t#Setting it to None defaults in using softmax\n\t\tnormalization_function = _smoothing_normalization if (smoothing == True) else None\n\t\tmemory_length = memory_sequence_length if (mask_encoder==True) else None\n\t\tsuper(LocationSensitiveAttention, self).__init__(\n\t\t\t\tnum_units=num_units,\n\t\t\t\tmemory=memory,\n\t\t\t\tmemory_sequence_length=memory_length,\n\t\t\t\tprobability_fn=normalization_function,\n\t\t\t\tname=name)\n\n\t\tself.location_convolution = tf.layers.Conv1D(filters=hparams.attention_filters,\n\t\t\tkernel_size=hparams.attention_kernel, padding=""same"", use_bias=True,\n\t\t\tbias_initializer=tf.zeros_initializer(), name=""location_features_convolution"")\n\t\tself.location_layer = tf.layers.Dense(units=num_units, use_bias=False,\n\t\t\tdtype=tf.float32, name=""location_features_layer"")\n\t\tself._cumulate = cumulate_weights\n\n\tdef __call__(self, query, state):\n\t\t""""""Score the query based on the keys and values.\n\t\tArgs:\n\t\t\tquery: Tensor of dtype matching `self.values` and shape\n\t\t\t\t`[batch_size, query_depth]`.\n\t\t\tstate (previous alignments): Tensor of dtype matching `self.values` and shape\n\t\t\t\t`[batch_size, alignments_size]`\n\t\t\t\t(`alignments_size` is memory""s `max_time`).\n\t\tReturns:\n\t\t\talignments: Tensor of dtype matching `self.values` and shape\n\t\t\t\t`[batch_size, alignments_size]` (`alignments_size` is memory\'s\n\t\t\t\t`max_time`).\n\t\t""""""\n\t\tprevious_alignments = state\n\t\twith variable_scope.variable_scope(None, ""Location_Sensitive_Attention"", [query]):\n\n\t\t\t# processed_query shape [batch_size, query_depth] -> [batch_size, attention_dim]\n\t\t\tprocessed_query = self.query_layer(query) if self.query_layer else query\n\t\t\t# -> [batch_size, 1, attention_dim]\n\t\t\tprocessed_query = tf.expand_dims(processed_query, 1)\n\n\t\t\t# processed_location_features shape [batch_size, max_time, attention dimension]\n\t\t\t# [batch_size, max_time] -> [batch_size, max_time, 1]\n\t\t\texpanded_alignments = tf.expand_dims(previous_alignments, axis=2)\n\t\t\t# location features [batch_size, max_time, filters]\n\t\t\tf = self.location_convolution(expanded_alignments)\n\t\t\t# Projected location features [batch_size, max_time, attention_dim]\n\t\t\tprocessed_location_features = self.location_layer(f)\n\n\t\t\t# energy shape [batch_size, max_time]\n\t\t\tenergy = _location_sensitive_score(processed_query, processed_location_features, self.keys)\n\n\n\t\t# alignments shape = energy shape = [batch_size, max_time]\n\t\talignments = self._probability_fn(energy, previous_alignments)\n\n\t\t# Cumulate alignments\n\t\tif self._cumulate:\n\t\t\tnext_state = alignments + previous_alignments\n\t\telse:\n\t\t\tnext_state = alignments\n\n\t\treturn alignments, next_state\n'"
synthesizer/models/custom_decoder.py,0,"b'from __future__ import absolute_import, division, print_function\nimport collections\nimport tensorflow as tf\nfrom synthesizer.models.helpers import TacoTestHelper, TacoTrainingHelper\nfrom tensorflow.contrib.seq2seq.python.ops import decoder\nfrom tensorflow.contrib.seq2seq.python.ops import helper as helper_py\nfrom tensorflow.python.framework import ops, tensor_shape\nfrom tensorflow.python.layers import base as layers_base\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.util import nest\n\n\nclass CustomDecoderOutput(\n\t\tcollections.namedtuple(""CustomDecoderOutput"", (""rnn_output"", ""token_output"", ""sample_id""))):\n\tpass\n\n\nclass CustomDecoder(decoder.Decoder):\n\t""""""Custom sampling decoder.\n\n\tAllows for stop token prediction at inference time\n\tand returns equivalent loss in training time.\n\n\tNote:\n\tOnly use this decoder with Tacotron 2 as it only accepts tacotron custom helpers\n\t""""""\n\n\tdef __init__(self, cell, helper, initial_state, output_layer=None):\n\t\t""""""Initialize CustomDecoder.\n\t\tArgs:\n\t\t\tcell: An `RNNCell` instance.\n\t\t\thelper: A `Helper` instance.\n\t\t\tinitial_state: A (possibly nested tuple of...) tensors and TensorArrays.\n\t\t\t\tThe initial state of the RNNCell.\n\t\t\toutput_layer: (Optional) An instance of `tf.layers.Layer`, i.e.,\n\t\t\t\t`tf.layers.Dense`. Optional layer to apply to the RNN output prior\n\t\t\t\tto storing the result or sampling.\n\t\tRaises:\n\t\t\tTypeError: if `cell`, `helper` or `output_layer` have an incorrect type.\n\t\t""""""\n\t\trnn_cell_impl.assert_like_rnncell(type(cell), cell)\n\t\tif not isinstance(helper, helper_py.Helper):\n\t\t\traise TypeError(""helper must be a Helper, received: %s"" % type(helper))\n\t\tif (output_layer is not None\n\t\t\t\tand not isinstance(output_layer, layers_base.Layer)):\n\t\t\traise TypeError(\n\t\t\t\t\t""output_layer must be a Layer, received: %s"" % type(output_layer))\n\t\tself._cell = cell\n\t\tself._helper = helper\n\t\tself._initial_state = initial_state\n\t\tself._output_layer = output_layer\n\n\t@property\n\tdef batch_size(self):\n\t\treturn self._helper.batch_size\n\n\tdef _rnn_output_size(self):\n\t\tsize = self._cell.output_size\n\t\tif self._output_layer is None:\n\t\t\treturn size\n\t\telse:\n\t\t\t# To use layer""s compute_output_shape, we need to convert the\n\t\t\t# RNNCell""s output_size entries into shapes with an unknown\n\t\t\t# batch size.  We then pass this through the layer""s\n\t\t\t# compute_output_shape and read off all but the first (batch)\n\t\t\t# dimensions to get the output size of the rnn with the layer\n\t\t\t# applied to the top.\n\t\t\toutput_shape_with_unknown_batch = nest.map_structure(\n\t\t\t\t\tlambda s: tensor_shape.TensorShape([None]).concatenate(s),\n\t\t\t\t\tsize)\n\t\t\tlayer_output_shape = self._output_layer._compute_output_shape(  # pylint: disable=protected-access\n\t\t\t\t\toutput_shape_with_unknown_batch)\n\t\t\treturn nest.map_structure(lambda s: s[1:], layer_output_shape)\n\n\t@property\n\tdef output_size(self):\n\t\t# Return the cell output and the id\n\t\treturn CustomDecoderOutput(\n\t\t\t\trnn_output=self._rnn_output_size(),\n\t\t\t\ttoken_output=self._helper.token_output_size,\n\t\t\t\tsample_id=self._helper.sample_ids_shape)\n\n\t@property\n\tdef output_dtype(self):\n\t\t# Assume the dtype of the cell is the output_size structure\n\t\t# containing the input_state""s first component\'s dtype.\n\t\t# Return that structure and the sample_ids_dtype from the helper.\n\t\tdtype = nest.flatten(self._initial_state)[0].dtype\n\t\treturn CustomDecoderOutput(\n\t\t\t\tnest.map_structure(lambda _: dtype, self._rnn_output_size()),\n\t\t\t\ttf.float32,\n\t\t\t\tself._helper.sample_ids_dtype)\n\n\tdef initialize(self, name=None):\n\t\t""""""Initialize the decoder.\n\t\tArgs:\n\t\t\tname: Name scope for any created operations.\n\t\tReturns:\n\t\t\t`(finished, first_inputs, initial_state)`.\n\t\t""""""\n\t\treturn self._helper.initialize() + (self._initial_state,)\n\n\tdef step(self, time, inputs, state, name=None):\n\t\t""""""Perform a custom decoding step.\n\t\tEnables for dyanmic <stop_token> prediction\n\t\tArgs:\n\t\t\ttime: scalar `int32` tensor.\n\t\t\tinputs: A (structure of) input tensors.\n\t\t\tstate: A (structure of) state tensors and TensorArrays.\n\t\t\tname: Name scope for any created operations.\n\t\tReturns:\n\t\t\t`(outputs, next_state, next_inputs, finished)`.\n\t\t""""""\n\t\twith ops.name_scope(name, ""CustomDecoderStep"", (time, inputs, state)):\n\t\t\t#Call outputprojection wrapper cell\n\t\t\t(cell_outputs, stop_token), cell_state = self._cell(inputs, state)\n\n\t\t\t#apply output_layer (if existant)\n\t\t\tif self._output_layer is not None:\n\t\t\t\tcell_outputs = self._output_layer(cell_outputs)\n\t\t\tsample_ids = self._helper.sample(\n\t\t\t\t\ttime=time, outputs=cell_outputs, state=cell_state)\n\n\t\t\t(finished, next_inputs, next_state) = self._helper.next_inputs(\n\t\t\t\t\ttime=time,\n\t\t\t\t\toutputs=cell_outputs,\n\t\t\t\t\tstate=cell_state,\n\t\t\t\t\tsample_ids=sample_ids,\n\t\t\t\t\tstop_token_prediction=stop_token)\n\n\t\toutputs = CustomDecoderOutput(cell_outputs, stop_token, sample_ids)\n\t\treturn (outputs, next_state, next_inputs, finished)\n'"
synthesizer/models/helpers.py,0,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.seq2seq import Helper\n\n\nclass TacoTestHelper(Helper):\n\tdef __init__(self, batch_size, hparams):\n\t\twith tf.name_scope(""TacoTestHelper""):\n\t\t\tself._batch_size = batch_size\n\t\t\tself._output_dim = hparams.num_mels\n\t\t\tself._reduction_factor = hparams.outputs_per_step\n\t\t\tself.stop_at_any = hparams.stop_at_any\n\n\t@property\n\tdef batch_size(self):\n\t\treturn self._batch_size\n\n\t@property\n\tdef token_output_size(self):\n\t\treturn self._reduction_factor\n\n\t@property\n\tdef sample_ids_shape(self):\n\t\treturn tf.TensorShape([])\n\n\t@property\n\tdef sample_ids_dtype(self):\n\t\treturn np.int32\n\n\tdef initialize(self, name=None):\n\t\treturn (tf.tile([False], [self._batch_size]), _go_frames(self._batch_size, self._output_dim))\n\n\tdef sample(self, time, outputs, state, name=None):\n\t\treturn tf.tile([0], [self._batch_size])  # Return all 0; we ignore them\n\n\tdef next_inputs(self, time, outputs, state, sample_ids, stop_token_prediction, name=None):\n\t\t""""""Stop on EOS. Otherwise, pass the last output as the next input and pass through state.""""""\n\t\twith tf.name_scope(""TacoTestHelper""):\n\t\t\t#A sequence is finished when the output probability is > 0.5\n\t\t\tfinished = tf.cast(tf.round(stop_token_prediction), tf.bool)\n\n\t\t\t#Since we are predicting r frames at each step, two modes are\n\t\t\t#then possible:\n\t\t\t#\tStop when the model outputs a p > 0.5 for any frame between r frames (Recommended)\n\t\t\t#\tStop when the model outputs a p > 0.5 for all r frames (Safer)\n\t\t\t#Note:\n\t\t\t#\tWith enough training steps, the model should be able to predict when to stop correctly\n\t\t\t#\tand the use of stop_at_any = True would be recommended. If however the model didn""t\n\t\t\t#\tlearn to stop correctly yet, (stops too soon) one could choose to use the safer option\n\t\t\t#\tto get a correct synthesis\n\t\t\tif self.stop_at_any:\n\t\t\t\tfinished = tf.reduce_any(tf.reduce_all(finished, axis=0)) #Recommended\n\t\t\telse:\n\t\t\t\tfinished = tf.reduce_all(tf.reduce_all(finished, axis=0)) #Safer option\n\n\t\t\t# Feed last output frame as next input. outputs is [N, output_dim * r]\n\t\t\tnext_inputs = outputs[:, -self._output_dim:]\n\t\t\tnext_state = state\n\t\t\treturn (finished, next_inputs, next_state)\n\n\nclass TacoTrainingHelper(Helper):\n\tdef __init__(self, batch_size, targets, hparams, gta, evaluating, global_step):\n\t\t# inputs is [N, T_in], targets is [N, T_out, D]\n\t\twith tf.name_scope(""TacoTrainingHelper""):\n\t\t\tself._batch_size = batch_size\n\t\t\tself._output_dim = hparams.num_mels\n\t\t\tself._reduction_factor = hparams.outputs_per_step\n\t\t\tself._ratio = tf.convert_to_tensor(hparams.tacotron_teacher_forcing_ratio)\n\t\t\tself.gta = gta\n\t\t\tself.eval = evaluating\n\t\t\tself._hparams = hparams\n\t\t\tself.global_step = global_step\n\n\t\t\tr = self._reduction_factor\n\t\t\t# Feed every r-th target frame as input\n\t\t\tself._targets = targets[:, r-1::r, :]\n\n\t\t\t#Maximal sequence length\n\t\t\tself._lengths = tf.tile([tf.shape(self._targets)[1]], [self._batch_size])\n\n\t@property\n\tdef batch_size(self):\n\t\treturn self._batch_size\n\n\t@property\n\tdef token_output_size(self):\n\t\treturn self._reduction_factor\n\n\t@property\n\tdef sample_ids_shape(self):\n\t\treturn tf.TensorShape([])\n\n\t@property\n\tdef sample_ids_dtype(self):\n\t\treturn np.int32\n\n\tdef initialize(self, name=None):\n\t\t#Compute teacher forcing ratio for this global step.\n\t\t#In GTA mode, override teacher forcing scheme to work with full teacher forcing\n\t\tif self.gta:\n\t\t\tself._ratio = tf.convert_to_tensor(1.) #Force GTA model to always feed ground-truth\n\t\telif self.eval and self._hparams.natural_eval:\n\t\t\tself._ratio = tf.convert_to_tensor(0.) #Force eval model to always feed predictions\n\t\telse:\n\t\t\tif self._hparams.tacotron_teacher_forcing_mode == ""scheduled"":\n\t\t\t\tself._ratio = _teacher_forcing_ratio_decay(self._hparams.tacotron_teacher_forcing_init_ratio,\n\t\t\t\t\tself.global_step, self._hparams)\n\n\t\treturn (tf.tile([False], [self._batch_size]), _go_frames(self._batch_size, self._output_dim))\n\n\tdef sample(self, time, outputs, state, name=None):\n\t\treturn tf.tile([0], [self._batch_size])  # Return all 0; we ignore them\n\n\tdef next_inputs(self, time, outputs, state, sample_ids, stop_token_prediction, name=None):\n\t\twith tf.name_scope(name or ""TacoTrainingHelper""):\n\t\t\t#synthesis stop (we let the model see paddings as we mask them when computing loss functions)\n\t\t\tfinished = (time + 1 >= self._lengths)\n\n\t\t\t#Pick previous outputs randomly with respect to teacher forcing ratio\n\t\t\tnext_inputs = tf.cond(\n\t\t\t\ttf.less(tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32), self._ratio),\n\t\t\t\tlambda: self._targets[:, time, :], #Teacher-forcing: return true frame\n\t\t\t\tlambda: outputs[:,-self._output_dim:])\n\n\t\t\t#Pass on state\n\t\t\tnext_state = state\n\t\t\treturn (finished, next_inputs, next_state)\n\n\ndef _go_frames(batch_size, output_dim):\n\t""""""Returns all-zero <GO> frames for a given batch size and output dimension""""""\n\treturn tf.tile([[0.0]], [batch_size, output_dim])\n\ndef _teacher_forcing_ratio_decay(init_tfr, global_step, hparams):\n\t\t#################################################################\n\t\t# Narrow Cosine Decay:\n\n\t\t# Phase 1: tfr = 1\n\t\t# We only start learning rate decay after 10k steps\n\n\t\t# Phase 2: tfr in ]0, 1[\n\t\t# decay reach minimal value at step ~280k\n\n\t\t# Phase 3: tfr = 0\n\t\t# clip by minimal teacher forcing ratio value (step >~ 280k)\n\t\t#################################################################\n\t\t#Compute natural cosine decay\n\t\ttfr = tf.train.cosine_decay(init_tfr,\n\t\t\tglobal_step=global_step - hparams.tacotron_teacher_forcing_start_decay, #tfr = 1 at step 10k\n\t\t\tdecay_steps=hparams.tacotron_teacher_forcing_decay_steps, #tfr = 0 at step ~280k\n\t\t\talpha=hparams.tacotron_teacher_forcing_decay_alpha, #tfr = 0% of init_tfr as final value\n\t\t\tname=""tfr_cosine_decay"")\n\n\t\t#force teacher forcing ratio to take initial value when global step < start decay step.\n\t\tnarrow_tfr = tf.cond(\n\t\t\ttf.less(global_step, tf.convert_to_tensor(hparams.tacotron_teacher_forcing_start_decay)),\n\t\t\tlambda: tf.convert_to_tensor(init_tfr),\n\t\t\tlambda: tfr)\n\n\t\treturn narrow_tfr'"
synthesizer/models/modules.py,0,"b'import tensorflow as tf\n\n\nclass HighwayNet:\n    def __init__(self, units, name=None):\n        self.units = units\n        self.scope = ""HighwayNet"" if name is None else name\n        \n        self.H_layer = tf.layers.Dense(units=self.units, activation=tf.nn.relu, name=""H"")\n        self.T_layer = tf.layers.Dense(units=self.units, activation=tf.nn.sigmoid, name=""T"",\n                                       bias_initializer=tf.constant_initializer(-1.))\n    \n    def __call__(self, inputs):\n        with tf.variable_scope(self.scope):\n            H = self.H_layer(inputs)\n            T = self.T_layer(inputs)\n            return H * T + inputs * (1. - T)\n\n\nclass CBHG:\n    def __init__(self, K, conv_channels, pool_size, projections, projection_kernel_size,\n                 n_highwaynet_layers, highway_units, rnn_units, is_training, name=None):\n        self.K = K\n        self.conv_channels = conv_channels\n        self.pool_size = pool_size\n        \n        self.projections = projections\n        self.projection_kernel_size = projection_kernel_size\n        \n        self.is_training = is_training\n        self.scope = ""CBHG"" if name is None else name\n        \n        self.highway_units = highway_units\n        self.highwaynet_layers = [\n            HighwayNet(highway_units, name=""{}_highwaynet_{}"".format(self.scope, i + 1)) for i in\n            range(n_highwaynet_layers)]\n        self._fw_cell = tf.nn.rnn_cell.GRUCell(rnn_units, name=""{}_forward_RNN"".format(self.scope))\n        self._bw_cell = tf.nn.rnn_cell.GRUCell(rnn_units, name=""{}_backward_RNN"".format(self.scope))\n    \n    def __call__(self, inputs, input_lengths):\n        with tf.variable_scope(self.scope):\n            with tf.variable_scope(""conv_bank""):\n                # Convolution bank: concatenate on the last axis to stack channels from all \n                # convolutions\n                # The convolution bank uses multiple different kernel sizes to have many insights \n                # of the input sequence\n                # This makes one of the strengths of the CBHG block on sequences.\n                conv_outputs = tf.concat(\n                    [conv1d(inputs, k, self.conv_channels, tf.nn.relu, self.is_training, 0.,\n                            ""conv1d_{}"".format(k)) for k in range(1, self.K + 1)],\n                    axis=-1\n                )\n            \n            # Maxpooling (dimension reduction, Using max instead of average helps finding ""Edges"" \n\t\t\t# in mels)\n            maxpool_output = tf.layers.max_pooling1d(\n                conv_outputs,\n                pool_size=self.pool_size,\n                strides=1,\n                padding=""same"")\n            \n            # Two projection layers\n            proj1_output = conv1d(maxpool_output, self.projection_kernel_size, self.projections[0],\n                                  tf.nn.relu, self.is_training, 0., ""proj1"")\n            proj2_output = conv1d(proj1_output, self.projection_kernel_size, self.projections[1],\n                                  lambda _: _, self.is_training, 0., ""proj2"")\n            \n            # Residual connection\n            highway_input = proj2_output + inputs\n            \n            # Additional projection in case of dimension mismatch (for HighwayNet ""residual"" \n\t\t\t# connection)\n            if highway_input.shape[2] != self.highway_units:\n                highway_input = tf.layers.dense(highway_input, self.highway_units)\n            \n            # 4-layer HighwayNet\n            for highwaynet in self.highwaynet_layers:\n                highway_input = highwaynet(highway_input)\n            rnn_input = highway_input\n            \n            # Bidirectional RNN\n            outputs, states = tf.nn.bidirectional_dynamic_rnn(\n                self._fw_cell,\n                self._bw_cell,\n                rnn_input,\n                sequence_length=input_lengths,\n                dtype=tf.float32)\n            return tf.concat(outputs, axis=2)  # Concat forward and backward outputs\n\n\nclass ZoneoutLSTMCell(tf.nn.rnn_cell.RNNCell):\n    """"""Wrapper for tf LSTM to create Zoneout LSTM Cell\n\n    inspired by:\n    https://github.com/teganmaharaj/zoneout/blob/master/zoneout_tensorflow.py\n\n    Published by one of ""https://arxiv.org/pdf/1606.01305.pdf"" paper writers.\n\n    Many thanks to @Ondal90 for pointing this out. You sir are a hero!\n    """"""\n    \n    def __init__(self, num_units, is_training, zoneout_factor_cell=0., zoneout_factor_output=0.,\n                 state_is_tuple=True, name=None):\n        """"""Initializer with possibility to set different zoneout values for cell/hidden states.\n        """"""\n        zm = min(zoneout_factor_output, zoneout_factor_cell)\n        zs = max(zoneout_factor_output, zoneout_factor_cell)\n        \n        if zm < 0. or zs > 1.:\n            raise ValueError(""One/both provided Zoneout factors are not in [0, 1]"")\n        \n        self._cell = tf.nn.rnn_cell.LSTMCell(num_units, state_is_tuple=state_is_tuple, name=name)\n        self._zoneout_cell = zoneout_factor_cell\n        self._zoneout_outputs = zoneout_factor_output\n        self.is_training = is_training\n        self.state_is_tuple = state_is_tuple\n    \n    @property\n    def state_size(self):\n        return self._cell.state_size\n    \n    @property\n    def output_size(self):\n        return self._cell.output_size\n    \n    def __call__(self, inputs, state, scope=None):\n        """"""Runs vanilla LSTM Cell and applies zoneout.\n        """"""\n        # Apply vanilla LSTM\n        output, new_state = self._cell(inputs, state, scope)\n        \n        if self.state_is_tuple:\n            (prev_c, prev_h) = state\n            (new_c, new_h) = new_state\n        else:\n            num_proj = self._cell._num_units if self._cell._num_proj is None else \\\n\t\t\t\tself._cell._num_proj\n            prev_c = tf.slice(state, [0, 0], [-1, self._cell._num_units])\n            prev_h = tf.slice(state, [0, self._cell._num_units], [-1, num_proj])\n            new_c = tf.slice(new_state, [0, 0], [-1, self._cell._num_units])\n            new_h = tf.slice(new_state, [0, self._cell._num_units], [-1, num_proj])\n        \n        # Apply zoneout\n        if self.is_training:\n            # nn.dropout takes keep_prob (probability to keep activations) not drop_prob (\n\t\t\t# probability to mask activations)!\n            c = (1 - self._zoneout_cell) * tf.nn.dropout(new_c - prev_c,\n                                                         (1 - self._zoneout_cell)) + prev_c\n            h = (1 - self._zoneout_outputs) * tf.nn.dropout(new_h - prev_h,\n                                                            (1 - self._zoneout_outputs)) + prev_h\n        \n        else:\n            c = (1 - self._zoneout_cell) * new_c + self._zoneout_cell * prev_c\n            h = (1 - self._zoneout_outputs) * new_h + self._zoneout_outputs * prev_h\n        \n        new_state = tf.nn.rnn_cell.LSTMStateTuple(c, h) if self.state_is_tuple else tf.concat(1, [c,\n                                                                                                  h])\n        \n        return output, new_state\n\n\nclass EncoderConvolutions:\n    """"""Encoder convolutional layers used to find local dependencies in inputs characters.\n    """"""\n    \n    def __init__(self, is_training, hparams, activation=tf.nn.relu, scope=None):\n        """"""\n        Args:\n            is_training: Boolean, determines if the model is training or in inference to control \n            dropout\n            kernel_size: tuple or integer, The size of convolution kernels\n            channels: integer, number of convolutional kernels\n            activation: callable, postnet activation function for each convolutional layer\n            scope: Postnet scope.\n        """"""\n        super(EncoderConvolutions, self).__init__()\n        self.is_training = is_training\n        \n        self.kernel_size = hparams.enc_conv_kernel_size\n        self.channels = hparams.enc_conv_channels\n        self.activation = activation\n        self.scope = ""enc_conv_layers"" if scope is None else scope\n        self.drop_rate = hparams.tacotron_dropout_rate\n        self.enc_conv_num_layers = hparams.enc_conv_num_layers\n    \n    def __call__(self, inputs):\n        with tf.variable_scope(self.scope):\n            x = inputs\n            for i in range(self.enc_conv_num_layers):\n                x = conv1d(x, self.kernel_size, self.channels, self.activation,\n                           self.is_training, self.drop_rate,\n                           ""conv_layer_{}_"".format(i + 1) + self.scope)\n        return x\n\n\nclass EncoderRNN:\n    """"""Encoder bidirectional one layer LSTM\n    """"""\n    \n    def __init__(self, is_training, size=256, zoneout=0.1, scope=None):\n        """"""\n        Args:\n            is_training: Boolean, determines if the model is training or in inference to control \n            zoneout\n            size: integer, the number of LSTM units for each direction\n            zoneout: the zoneout factor\n            scope: EncoderRNN scope.\n        """"""\n        super(EncoderRNN, self).__init__()\n        self.is_training = is_training\n        \n        self.size = size\n        self.zoneout = zoneout\n        self.scope = ""encoder_LSTM"" if scope is None else scope\n        \n        # Create forward LSTM Cell\n        self._fw_cell = ZoneoutLSTMCell(size, is_training,\n                                        zoneout_factor_cell=zoneout,\n                                        zoneout_factor_output=zoneout,\n                                        name=""encoder_fw_LSTM"")\n        \n        # Create backward LSTM Cell\n        self._bw_cell = ZoneoutLSTMCell(size, is_training,\n                                        zoneout_factor_cell=zoneout,\n                                        zoneout_factor_output=zoneout,\n                                        name=""encoder_bw_LSTM"")\n    \n    def __call__(self, inputs, input_lengths):\n        with tf.variable_scope(self.scope):\n            outputs, (fw_state, bw_state) = tf.nn.bidirectional_dynamic_rnn(\n                self._fw_cell,\n                self._bw_cell,\n                inputs,\n                sequence_length=input_lengths,\n                dtype=tf.float32,\n                swap_memory=True)\n            \n            return tf.concat(outputs, axis=2)  # Concat and return forward + backward outputs\n\n\nclass Prenet:\n    """"""Two fully connected layers used as an information bottleneck for the attention.\n    """"""\n    \n    def __init__(self, is_training, layers_sizes=[256, 256], drop_rate=0.5, activation=tf.nn.relu,\n                 scope=None):\n        """"""\n        Args:\n            layers_sizes: list of integers, the length of the list represents the number of pre-net\n                layers and the list values represent the layers number of units\n            activation: callable, activation functions of the prenet layers.\n            scope: Prenet scope.\n        """"""\n        super(Prenet, self).__init__()\n        self.drop_rate = drop_rate\n        \n        self.layers_sizes = layers_sizes\n        self.activation = activation\n        self.is_training = is_training\n        \n        self.scope = ""prenet"" if scope is None else scope\n    \n    def __call__(self, inputs):\n        x = inputs\n        \n        with tf.variable_scope(self.scope):\n            for i, size in enumerate(self.layers_sizes):\n                dense = tf.layers.dense(x, units=size, activation=self.activation,\n                                        name=""dense_{}"".format(i + 1))\n                # The paper discussed introducing diversity in generation at inference time\n                # by using a dropout of 0.5 only in prenet layers (in both training and inference).\n                x = tf.layers.dropout(dense, rate=self.drop_rate, training=True,\n                                      name=""dropout_{}"".format(i + 1) + self.scope)\n        return x\n\n\nclass DecoderRNN:\n    """"""Decoder two uni directional LSTM Cells\n    """"""\n    \n    def __init__(self, is_training, layers=2, size=1024, zoneout=0.1, scope=None):\n        """"""\n        Args:\n            is_training: Boolean, determines if the model is in training or inference to control \n            zoneout\n            layers: integer, the number of LSTM layers in the decoder\n            size: integer, the number of LSTM units in each layer\n            zoneout: the zoneout factor\n        """"""\n        super(DecoderRNN, self).__init__()\n        self.is_training = is_training\n        \n        self.layers = layers\n        self.size = size\n        self.zoneout = zoneout\n        self.scope = ""decoder_rnn"" if scope is None else scope\n        \n        # Create a set of LSTM layers\n        self.rnn_layers = [ZoneoutLSTMCell(size, is_training,\n                                           zoneout_factor_cell=zoneout,\n                                           zoneout_factor_output=zoneout,\n                                           name=""decoder_LSTM_{}"".format(i + 1)) for i in\n                           range(layers)]\n        \n        self._cell = tf.contrib.rnn.MultiRNNCell(self.rnn_layers, state_is_tuple=True)\n    \n    def __call__(self, inputs, states):\n        with tf.variable_scope(self.scope):\n            return self._cell(inputs, states)\n\n\nclass FrameProjection:\n    """"""Projection layer to r * num_mels dimensions or num_mels dimensions\n    """"""\n    \n    def __init__(self, shape=80, activation=None, scope=None):\n        """"""\n        Args:\n            shape: integer, dimensionality of output space (r*n_mels for decoder or n_mels for \n            postnet)\n            activation: callable, activation function\n            scope: FrameProjection scope.\n        """"""\n        super(FrameProjection, self).__init__()\n        \n        self.shape = shape\n        self.activation = activation\n        \n        self.scope = ""Linear_projection"" if scope is None else scope\n        self.dense = tf.layers.Dense(units=shape, activation=activation,\n                                     name=""projection_{}"".format(self.scope))\n    \n    def __call__(self, inputs):\n        with tf.variable_scope(self.scope):\n            # If activation==None, this returns a simple Linear projection\n            # else the projection will be passed through an activation function\n            # output = tf.layers.dense(inputs, units=self.shape, activation=self.activation,\n            # \tname=""projection_{}"".format(self.scope))\n            output = self.dense(inputs)\n            \n            return output\n\n\nclass StopProjection:\n    """"""Projection to a scalar and through a sigmoid activation\n    """"""\n    \n    def __init__(self, is_training, shape=1, activation=tf.nn.sigmoid, scope=None):\n        """"""\n        Args:\n            is_training: Boolean, to control the use of sigmoid function as it is useless to use it\n                during training since it is integrate inside the sigmoid_crossentropy loss\n            shape: integer, dimensionality of output space. Defaults to 1 (scalar)\n            activation: callable, activation function. only used during inference\n            scope: StopProjection scope.\n        """"""\n        super(StopProjection, self).__init__()\n        self.is_training = is_training\n        \n        self.shape = shape\n        self.activation = activation\n        self.scope = ""stop_token_projection"" if scope is None else scope\n    \n    def __call__(self, inputs):\n        with tf.variable_scope(self.scope):\n            output = tf.layers.dense(inputs, units=self.shape,\n                                     activation=None, name=""projection_{}"".format(self.scope))\n            \n            # During training, don""t use activation as it is integrated inside the \n\t\t\t# sigmoid_cross_entropy loss function\n            if self.is_training:\n                return output\n            return self.activation(output)\n\n\nclass Postnet:\n    """"""Postnet that takes final decoder output and fine tunes it (using vision on past and future \n    frames)\n    """"""\n    \n    def __init__(self, is_training, hparams, activation=tf.nn.tanh, scope=None):\n        """"""\n        Args:\n            is_training: Boolean, determines if the model is training or in inference to control \n            dropout\n            kernel_size: tuple or integer, The size of convolution kernels\n            channels: integer, number of convolutional kernels\n            activation: callable, postnet activation function for each convolutional layer\n            scope: Postnet scope.\n        """"""\n        super(Postnet, self).__init__()\n        self.is_training = is_training\n        \n        self.kernel_size = hparams.postnet_kernel_size\n        self.channels = hparams.postnet_channels\n        self.activation = activation\n        self.scope = ""postnet_convolutions"" if scope is None else scope\n        self.postnet_num_layers = hparams.postnet_num_layers\n        self.drop_rate = hparams.tacotron_dropout_rate\n    \n    def __call__(self, inputs):\n        with tf.variable_scope(self.scope):\n            x = inputs\n            for i in range(self.postnet_num_layers - 1):\n                x = conv1d(x, self.kernel_size, self.channels, self.activation,\n                           self.is_training, self.drop_rate,\n                           ""conv_layer_{}_"".format(i + 1) + self.scope)\n            x = conv1d(x, self.kernel_size, self.channels, lambda _: _, self.is_training,\n                       self.drop_rate,\n                       ""conv_layer_{}_"".format(5) + self.scope)\n        return x\n\n\ndef conv1d(inputs, kernel_size, channels, activation, is_training, drop_rate, scope):\n    with tf.variable_scope(scope):\n        conv1d_output = tf.layers.conv1d(\n            inputs,\n            filters=channels,\n            kernel_size=kernel_size,\n            activation=None,\n            padding=""same"")\n        batched = tf.layers.batch_normalization(conv1d_output, training=is_training)\n        activated = activation(batched)\n        return tf.layers.dropout(activated, rate=drop_rate, training=is_training,\n                                 name=""dropout_{}"".format(scope))\n\n\ndef _round_up_tf(x, multiple):\n    # Tf version of remainder = x % multiple\n    remainder = tf.mod(x, multiple)\n    # Tf version of return x if remainder == 0 else x + multiple - remainder\n    x_round = tf.cond(tf.equal(remainder, tf.zeros(tf.shape(remainder), dtype=tf.int32)),\n                      lambda: x,\n                      lambda: x + multiple - remainder)\n    \n    return x_round\n\n\ndef sequence_mask(lengths, r, expand=True):\n    """"""Returns a 2-D or 3-D tensorflow sequence mask depending on the argument ""expand""\n    """"""\n    max_len = tf.reduce_max(lengths)\n    max_len = _round_up_tf(max_len, tf.convert_to_tensor(r))\n    if expand:\n        return tf.expand_dims(tf.sequence_mask(lengths, maxlen=max_len, dtype=tf.float32), axis=-1)\n    return tf.sequence_mask(lengths, maxlen=max_len, dtype=tf.float32)\n\n\ndef MaskedMSE(targets, outputs, targets_lengths, hparams, mask=None):\n    """"""Computes a masked Mean Squared Error\n    """"""\n    \n    # [batch_size, time_dimension, 1]\n    # example:\n    # sequence_mask([1, 3, 2], 5) = [[[1., 0., 0., 0., 0.]],\n    #\t\t\t\t\t\t\t    [[1., 1., 1., 0., 0.]],\n    #\t\t\t\t\t\t\t    [[1., 1., 0., 0., 0.]]]\n    # Note the maxlen argument that ensures mask shape is compatible with r>1\n    # This will by default mask the extra paddings caused by r>1\n    if mask is None:\n        mask = sequence_mask(targets_lengths, hparams.outputs_per_step, True)\n    \n    # [batch_size, time_dimension, channel_dimension(mels)]\n    ones = tf.ones(shape=[tf.shape(mask)[0], tf.shape(mask)[1], tf.shape(targets)[-1]],\n                   dtype=tf.float32)\n    mask_ = mask * ones\n    \n    with tf.control_dependencies([tf.assert_equal(tf.shape(targets), tf.shape(mask_))]):\n        return tf.losses.mean_squared_error(labels=targets, predictions=outputs, weights=mask_)\n\n\ndef MaskedSigmoidCrossEntropy(targets, outputs, targets_lengths, hparams, mask=None):\n    """"""Computes a masked SigmoidCrossEntropy with logits\n    """"""\n    \n    # [batch_size, time_dimension]\n    # example:\n    # sequence_mask([1, 3, 2], 5) = [[1., 0., 0., 0., 0.],\n    #\t\t\t\t\t\t\t    [1., 1., 1., 0., 0.],\n    #\t\t\t\t\t\t\t    [1., 1., 0., 0., 0.]]\n    # Note the maxlen argument that ensures mask shape is compatible with r>1\n    # This will by default mask the extra paddings caused by r>1\n    if mask is None:\n        mask = sequence_mask(targets_lengths, hparams.outputs_per_step, False)\n    \n    with tf.control_dependencies([tf.assert_equal(tf.shape(targets), tf.shape(mask))]):\n        # Use a weighted sigmoid cross entropy to measure the <stop_token> loss. Set \n        # hparams.cross_entropy_pos_weight to 1\n        # will have the same effect as  vanilla tf.nn.sigmoid_cross_entropy_with_logits.\n        losses = tf.nn.weighted_cross_entropy_with_logits(targets=targets, logits=outputs,\n                                                          pos_weight=hparams.cross_entropy_pos_weight)\n    \n    with tf.control_dependencies([tf.assert_equal(tf.shape(mask), tf.shape(losses))]):\n        masked_loss = losses * mask\n    \n    return tf.reduce_sum(masked_loss) / tf.count_nonzero(masked_loss, dtype=tf.float32)\n\n\ndef MaskedLinearLoss(targets, outputs, targets_lengths, hparams, mask=None):\n    """"""Computes a masked MAE loss with priority to low frequencies\n    """"""\n    \n    # [batch_size, time_dimension, 1]\n    # example:\n    # sequence_mask([1, 3, 2], 5) = [[[1., 0., 0., 0., 0.]],\n    #\t\t\t\t\t\t\t    [[1., 1., 1., 0., 0.]],\n    #\t\t\t\t\t\t\t    [[1., 1., 0., 0., 0.]]]\n    # Note the maxlen argument that ensures mask shape is compatible with r>1\n    # This will by default mask the extra paddings caused by r>1\n    if mask is None:\n        mask = sequence_mask(targets_lengths, hparams.outputs_per_step, True)\n    \n    # [batch_size, time_dimension, channel_dimension(freq)]\n    ones = tf.ones(shape=[tf.shape(mask)[0], tf.shape(mask)[1], tf.shape(targets)[-1]],\n                   dtype=tf.float32)\n    mask_ = mask * ones\n    \n    l1 = tf.abs(targets - outputs)\n    n_priority_freq = int(2000 / (hparams.sample_rate * 0.5) * hparams.num_freq)\n    \n    with tf.control_dependencies([tf.assert_equal(tf.shape(targets), tf.shape(mask_))]):\n        masked_l1 = l1 * mask_\n        masked_l1_low = masked_l1[:, :, 0:n_priority_freq]\n    \n    mean_l1 = tf.reduce_sum(masked_l1) / tf.reduce_sum(mask_)\n    mean_l1_low = tf.reduce_sum(masked_l1_low) / tf.reduce_sum(mask_)\n    \n    return 0.5 * mean_l1 + 0.5 * mean_l1_low\n'"
synthesizer/models/tacotron.py,0,"b'import tensorflow as tf\nfrom synthesizer.utils.symbols import symbols\nfrom synthesizer.infolog import log\nfrom synthesizer.models.helpers import TacoTrainingHelper, TacoTestHelper\nfrom synthesizer.models.modules import *\nfrom tensorflow.contrib.seq2seq import dynamic_decode\nfrom synthesizer.models.architecture_wrappers import TacotronEncoderCell, TacotronDecoderCell\nfrom synthesizer.models.custom_decoder import CustomDecoder\nfrom synthesizer.models.attention import LocationSensitiveAttention\n\nimport numpy as np\n\n\ndef split_func(x, split_pos):\n    rst = []\n    start = 0\n    # x will be a numpy array with the contents of the placeholder below\n    for i in range(split_pos.shape[0]):\n        rst.append(x[:, start:start + split_pos[i]])\n        start += split_pos[i]\n    return rst\n\n\nclass Tacotron():\n    """"""Tacotron-2 Feature prediction Model.\n    """"""\n    \n    def __init__(self, hparams):\n        self._hparams = hparams\n    \n    def initialize(self, inputs, input_lengths, embed_targets, mel_targets=None, \n                   stop_token_targets=None, linear_targets=None, targets_lengths=None, gta=False,\n                   global_step=None, is_training=False, is_evaluating=False, split_infos=None):\n        """"""\n        Initializes the model for inference sets ""mel_outputs"" and ""alignments"" fields.\n        Args:\n            - inputs: int32 Tensor with shape [N, T_in] where N is batch size, T_in is number of\n              steps in the input time series, and values are character IDs\n            - input_lengths: int32 Tensor with shape [N] where N is batch size and values are the \n            lengths of each sequence in inputs.\n            - embed_targets: float32 Tensor with shape [N, E] where E is the speaker \n            embedding size.\n            - mel_targets: float32 Tensor with shape [N, T_out, M] where N is batch size, \n            T_out is number of steps in the output time series, M is num_mels, and values are \n            entries in the mel spectrogram. Only needed for training.\n        """"""\n        if mel_targets is None and stop_token_targets is not None:\n            raise ValueError(""no multi targets were provided but token_targets were given"")\n        if mel_targets is not None and stop_token_targets is None and not gta:\n            raise ValueError(""Mel targets are provided without corresponding token_targets"")\n        if not gta and self._hparams.predict_linear == True and linear_targets is None and \\\n\t\t\t\tis_training:\n            raise ValueError(\n                ""Model is set to use post processing to predict linear spectrograms in training ""\n\t\t\t\t""but no linear targets given!"")\n        if gta and linear_targets is not None:\n            raise ValueError(""Linear spectrogram prediction is not supported in GTA mode!"")\n        if is_training and self._hparams.mask_decoder and targets_lengths is None:\n            raise RuntimeError(\n                ""Model set to mask paddings but no targets lengths provided for the mask!"")\n        if is_training and is_evaluating:\n            raise RuntimeError(\n                ""Model can not be in training and evaluation modes at the same time!"")\n        \n        split_device = ""/cpu:0"" if self._hparams.tacotron_num_gpus > 1 or \\\n\t\t\t\t\t\t\t\t   self._hparams.split_on_cpu else ""/gpu:{}"".format(\n            self._hparams.tacotron_gpu_start_idx)\n        with tf.device(split_device):\n            hp = self._hparams\n            lout_int = [tf.int32] * hp.tacotron_num_gpus\n            lout_float = [tf.float32] * hp.tacotron_num_gpus\n            \n            tower_input_lengths = tf.split(input_lengths, num_or_size_splits=hp.tacotron_num_gpus,\n                                           axis=0)\n            tower_targets_lengths = \\\n                tf.split(targets_lengths, num_or_size_splits=hp.tacotron_num_gpus, axis=0) if \\\n                    targets_lengths is not None else targets_lengths\n            \n            ### SV2TTS ###\n            \n            tower_embed_targets = tf.split(embed_targets, num_or_size_splits=hp.tacotron_num_gpus,\n                                           axis=0)\n            \n            ##############\n            \n            p_inputs = tf.py_func(split_func, [inputs, split_infos[:, 0]], lout_int)\n            p_mel_targets = tf.py_func(split_func, [mel_targets, split_infos[:, 1]],\n                                       lout_float) if mel_targets is not None else mel_targets\n            p_stop_token_targets = tf.py_func(split_func, [stop_token_targets, split_infos[:, 2]],\n                                              lout_float) if stop_token_targets is not None else \\\n\t\t\t\tstop_token_targets\n            \n            tower_inputs = []\n            tower_mel_targets = []\n            tower_stop_token_targets = []\n            \n            batch_size = tf.shape(inputs)[0]\n            mel_channels = hp.num_mels\n            for i in range(hp.tacotron_num_gpus):\n                tower_inputs.append(tf.reshape(p_inputs[i], [batch_size, -1]))\n                if p_mel_targets is not None:\n                    tower_mel_targets.append(\n                        tf.reshape(p_mel_targets[i], [batch_size, -1, mel_channels]))\n                if p_stop_token_targets is not None:\n                    tower_stop_token_targets.append(\n                        tf.reshape(p_stop_token_targets[i], [batch_size, -1]))\n        \n        self.tower_decoder_output = []\n        self.tower_alignments = []\n        self.tower_stop_token_prediction = []\n        self.tower_mel_outputs = []\n        \n        tower_embedded_inputs = []\n        tower_enc_conv_output_shape = []\n        tower_encoder_cond_outputs = []\n        tower_residual = []\n        tower_projected_residual = []\n        \n        # 1. Declare GPU Devices\n        gpus = [""/gpu:{}"".format(i) for i in\n                range(hp.tacotron_gpu_start_idx, hp.tacotron_gpu_start_idx + hp.tacotron_num_gpus)]\n        for i in range(hp.tacotron_num_gpus):\n            with tf.device(tf.train.replica_device_setter(ps_tasks=1, ps_device=""/cpu:0"",\n                                                          worker_device=gpus[i])):\n                with tf.variable_scope(""inference"") as scope:\n                    assert hp.tacotron_teacher_forcing_mode in (""constant"", ""scheduled"")\n                    if hp.tacotron_teacher_forcing_mode == ""scheduled"" and is_training:\n                        assert global_step is not None\n                    \n                    # GTA is only used for predicting mels to train Wavenet vocoder, so we ommit \n                    # post processing when doing GTA synthesis\n                    post_condition = hp.predict_linear and not gta\n                    \n                    # Embeddings ==> [batch_size, sequence_length, embedding_dim]\n                    self.embedding_table = tf.get_variable(\n                        ""inputs_embedding"", [len(symbols), hp.embedding_dim], dtype=tf.float32)\n                    embedded_inputs = tf.nn.embedding_lookup(self.embedding_table, tower_inputs[i])\n                    \n                    # Encoder Cell ==> [batch_size, encoder_steps, encoder_lstm_units]\n                    encoder_cell = TacotronEncoderCell(\n                        EncoderConvolutions(is_training, hparams=hp, scope=""encoder_convolutions""),\n                        EncoderRNN(is_training, size=hp.encoder_lstm_units,\n                                   zoneout=hp.tacotron_zoneout_rate, scope=""encoder_LSTM""))\n                    \n                    encoder_outputs = encoder_cell(embedded_inputs, tower_input_lengths[i])\n                    \n                    # For shape visualization purpose\n                    enc_conv_output_shape = encoder_cell.conv_output_shape\n                    \n                    \n                    ### SV2TT2 ###\n                    \n                    # Append the speaker embedding to the encoder output at each timestep\n                    tileable_shape = [-1, 1, self._hparams.speaker_embedding_size]\n                    tileable_embed_targets = tf.reshape(tower_embed_targets[i], tileable_shape)\n                    tiled_embed_targets = tf.tile(tileable_embed_targets, \n                                                       [1, tf.shape(encoder_outputs)[1], 1])\n                    encoder_cond_outputs = tf.concat((encoder_outputs, tiled_embed_targets), 2)\n                    \n                    ##############\n                    \n                    \n                    # Decoder Parts\n                    # Attention Decoder Prenet\n                    prenet = Prenet(is_training, layers_sizes=hp.prenet_layers,\n                                    drop_rate=hp.tacotron_dropout_rate, scope=""decoder_prenet"")\n                    # Attention Mechanism\n                    attention_mechanism = LocationSensitiveAttention(hp.attention_dim,\n                                                                     encoder_cond_outputs, \n                                                                     hparams=hp,\n                                                                     mask_encoder=hp.mask_encoder,\n                                                                     memory_sequence_length=tf.reshape(\n                                                                         tower_input_lengths[i],\n                                                                         [-1]),\n                                                                     smoothing=hp.smoothing,\n                                                                     cumulate_weights=hp.cumulative_weights)\n                    # Decoder LSTM Cells\n                    decoder_lstm = DecoderRNN(is_training, layers=hp.decoder_layers,\n                                              size=hp.decoder_lstm_units,\n                                              zoneout=hp.tacotron_zoneout_rate,\n                                              scope=""decoder_LSTM"")\n                    # Frames Projection layer\n                    frame_projection = FrameProjection(hp.num_mels * hp.outputs_per_step,\n                                                       scope=""linear_transform_projection"")\n                    # <stop_token> projection layer\n                    stop_projection = StopProjection(is_training or is_evaluating, shape=hp\n                                                     .outputs_per_step,\n                                                     scope=""stop_token_projection"")\n                    \n                    # Decoder Cell ==> [batch_size, decoder_steps, num_mels * r] (after decoding)\n                    decoder_cell = TacotronDecoderCell(\n                        prenet,\n                        attention_mechanism,\n                        decoder_lstm,\n                        frame_projection,\n                        stop_projection)\n                    \n                    # Define the helper for our decoder\n                    if is_training or is_evaluating or gta:\n                        self.helper = TacoTrainingHelper(batch_size, tower_mel_targets[i], hp, gta,\n                                                         is_evaluating, global_step)\n                    else:\n                        self.helper = TacoTestHelper(batch_size, hp)\n                    \n                    # initial decoder state\n                    decoder_init_state = decoder_cell.zero_state(batch_size=batch_size,\n                                                                 dtype=tf.float32)\n                    \n                    # Only use max iterations at synthesis time\n                    max_iters = hp.max_iters if not (is_training or is_evaluating) else None\n                    \n                    # Decode\n                    (frames_prediction, stop_token_prediction,\n                     _), final_decoder_state, _ = dynamic_decode(\n                        CustomDecoder(decoder_cell, self.helper, decoder_init_state),\n                        impute_finished=False,\n                        maximum_iterations=max_iters,\n                        swap_memory=hp.tacotron_swap_with_cpu)\n                    \n                    # Reshape outputs to be one output per entry \n                    # ==> [batch_size, non_reduced_decoder_steps (decoder_steps * r), num_mels]\n                    decoder_output = tf.reshape(frames_prediction, [batch_size, -1, hp.num_mels])\n                    stop_token_prediction = tf.reshape(stop_token_prediction, [batch_size, -1])\n                    \n                    # Postnet\n                    postnet = Postnet(is_training, hparams=hp, scope=""postnet_convolutions"")\n                    \n                    # Compute residual using post-net ==> [batch_size, decoder_steps * r, \n                    # postnet_channels]\n                    residual = postnet(decoder_output)\n                    \n                    # Project residual to same dimension as mel spectrogram \n                    # ==> [batch_size, decoder_steps * r, num_mels]\n                    residual_projection = FrameProjection(hp.num_mels, scope=""postnet_projection"")\n                    projected_residual = residual_projection(residual)\n                    \n                    # Compute the mel spectrogram\n                    mel_outputs = decoder_output + projected_residual\n                    \n                    if post_condition:\n                        # Add post-processing CBHG. This does a great job at extracting features \n\t\t\t\t\t\t# from mels before projection to Linear specs.\n                        post_cbhg = CBHG(hp.cbhg_kernels, hp.cbhg_conv_channels, hp.cbhg_pool_size,\n                                         [hp.cbhg_projection, hp.num_mels],\n                                         hp.cbhg_projection_kernel_size, hp.cbhg_highwaynet_layers,\n                                         hp.cbhg_highway_units, hp.cbhg_rnn_units, is_training,\n                                         name=""CBHG_postnet"")\n                        \n                        # [batch_size, decoder_steps(mel_frames), cbhg_channels]\n                        post_outputs = post_cbhg(mel_outputs, None)\n                        \n                        # Linear projection of extracted features to make linear spectrogram\n                        linear_specs_projection = FrameProjection(hp.num_freq,\n                                                                  scope=""cbhg_linear_specs_projection"")\n                        \n                        # [batch_size, decoder_steps(linear_frames), num_freq]\n                        linear_outputs = linear_specs_projection(post_outputs)\n                    \n                    # Grab alignments from the final decoder state\n                    alignments = tf.transpose(final_decoder_state.alignment_history.stack(),\n                                              [1, 2, 0])\n                    \n                    self.tower_decoder_output.append(decoder_output)\n                    self.tower_alignments.append(alignments)\n                    self.tower_stop_token_prediction.append(stop_token_prediction)\n                    self.tower_mel_outputs.append(mel_outputs)\n                    tower_embedded_inputs.append(embedded_inputs)\n                    tower_enc_conv_output_shape.append(enc_conv_output_shape)\n                    tower_encoder_cond_outputs.append(encoder_cond_outputs)\n                    tower_residual.append(residual)\n                    tower_projected_residual.append(projected_residual)\n                    \n                    if post_condition:\n                        self.tower_linear_outputs.append(linear_outputs)\n            log(""initialisation done {}"".format(gpus[i]))\n        \n        if is_training:\n            self.ratio = self.helper._ratio\n        self.tower_inputs = tower_inputs\n        self.tower_input_lengths = tower_input_lengths\n        self.tower_mel_targets = tower_mel_targets\n        # self.tower_linear_targets = tower_linear_targets\n        self.tower_targets_lengths = tower_targets_lengths\n        self.tower_stop_token_targets = tower_stop_token_targets\n        \n        self.all_vars = tf.trainable_variables()\n        \n        log(""Initialized Tacotron model. Dimensions (? = dynamic shape): "")\n        log(""  Train mode:               {}"".format(is_training))\n        log(""  Eval mode:                {}"".format(is_evaluating))\n        log(""  GTA mode:                 {}"".format(gta))\n        log(""  Synthesis mode:           {}"".format(not (is_training or is_evaluating)))\n        log(""  Input:                    {}"".format(inputs.shape))\n        for i in range(hp.tacotron_num_gpus + hp.tacotron_gpu_start_idx):\n            log(""  device:                   {}"".format(i))\n            log(""  embedding:                {}"".format(tower_embedded_inputs[i].shape))\n            log(""  enc conv out:             {}"".format(tower_enc_conv_output_shape[i]))\n            log(""  encoder out (cond):       {}"".format(tower_encoder_cond_outputs[i].shape))\n            log(""  decoder out:              {}"".format(self.tower_decoder_output[i].shape))\n            log(""  residual out:             {}"".format(tower_residual[i].shape))\n            log(""  projected residual out:   {}"".format(tower_projected_residual[i].shape))\n            log(""  mel out:                  {}"".format(self.tower_mel_outputs[i].shape))\n            if post_condition:\n                log(""  linear out:               {}"".format(self.tower_linear_outputs[i].shape))\n            log(""  <stop_token> out:         {}"".format(self.tower_stop_token_prediction[i].shape))\n            \n            # 1_000_000 is causing syntax problems for some people?! Python please :)\n            log(""  Tacotron Parameters       {:.3f} Million."".format(\n                np.sum([np.prod(v.get_shape().as_list()) for v in self.all_vars]) / 1000000))\n    \n    \n    def add_loss(self):\n        """"""Adds loss to the model. Sets ""loss"" field. initialize must have been called.""""""\n        hp = self._hparams\n        \n        self.tower_before_loss = []\n        self.tower_after_loss = []\n        self.tower_stop_token_loss = []\n        self.tower_regularization_loss = []\n        self.tower_linear_loss = []\n        self.tower_loss = []\n        \n        total_before_loss = 0\n        total_after_loss = 0\n        total_stop_token_loss = 0\n        total_regularization_loss = 0\n        total_linear_loss = 0\n        total_loss = 0\n\n        gpus = [""/gpu:{}"".format(i) for i in\n                range(hp.tacotron_gpu_start_idx, hp.tacotron_gpu_start_idx + hp.tacotron_num_gpus)]\n        \n        for i in range(hp.tacotron_num_gpus):\n            with tf.device(tf.train.replica_device_setter(ps_tasks=1, ps_device=""/cpu:0"",\n                                                          worker_device=gpus[i])):\n                with tf.variable_scope(""loss"") as scope:\n                    if hp.mask_decoder:\n                        # Compute loss of predictions before postnet\n                        before = MaskedMSE(self.tower_mel_targets[i], self.tower_decoder_output[i],\n                                           self.tower_targets_lengths[i],\n                                           hparams=self._hparams)\n                        # Compute loss after postnet\n                        after = MaskedMSE(self.tower_mel_targets[i], self.tower_mel_outputs[i],\n                                          self.tower_targets_lengths[i],\n                                          hparams=self._hparams)\n                        # Compute <stop_token> loss (for learning dynamic generation stop)\n                        stop_token_loss = MaskedSigmoidCrossEntropy(\n                            self.tower_stop_token_targets[i],\n                            self.tower_stop_token_prediction[i], self.tower_targets_lengths[i],\n                            hparams=self._hparams)\n                        # SV2TTS extra L1 loss (disabled for now)\n                        # linear_loss = MaskedLinearLoss(self.tower_mel_targets[i],\n                        #                                self.tower_decoder_output[i],\n                        #                                self.tower_targets_lengths[i],\n                        #                                hparams=self._hparams)\n                        linear_loss = 0.\n                    else:\n                        # Compute loss of predictions before postnet\n                        before = tf.losses.mean_squared_error(self.tower_mel_targets[i],\n                                                              self.tower_decoder_output[i])\n                        # Compute loss after postnet\n                        after = tf.losses.mean_squared_error(self.tower_mel_targets[i],\n                                                             self.tower_mel_outputs[i])\n                        # Compute <stop_token> loss (for learning dynamic generation stop)\n                        stop_token_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n                            labels=self.tower_stop_token_targets[i],\n                            logits=self.tower_stop_token_prediction[i]))\n                        \n                        # SV2TTS extra L1 loss\n                        l1 = tf.abs(self.tower_mel_targets[i] - self.tower_decoder_output[i])\n                        linear_loss = tf.reduce_mean(l1)\n\n                        # if hp.predict_linear:\n                        #     # Compute linear loss\n                        #     # From https://github.com/keithito/tacotron/blob/tacotron2-work-in\n\t\t\t\t\t\t# \t# -progress/models/tacotron.py\n                        #     # Prioritize loss for frequencies under 2000 Hz.\n                        #     l1 = tf.abs(self.tower_linear_targets[i] - self.tower_linear_outputs[i])\n                        #     n_priority_freq = int(2000 / (hp.sample_rate * 0.5) * hp.num_freq)\n                        #     linear_loss = 0.5 * tf.reduce_mean(l1) + 0.5 * tf.reduce_mean(\n                        #         l1[:, :, 0:n_priority_freq])\n                        # else:\n                        #     linear_loss = 0.\n                    \n                    # Compute the regularization weight\n                    if hp.tacotron_scale_regularization:\n                        reg_weight_scaler = 1. / (\n                                    2 * hp.max_abs_value) if hp.symmetric_mels else 1. / (\n                            hp.max_abs_value)\n                        reg_weight = hp.tacotron_reg_weight * reg_weight_scaler\n                    else:\n                        reg_weight = hp.tacotron_reg_weight\n                    \n                    # Regularize variables\n                    # Exclude all types of bias, RNN (Bengio et al. On the difficulty of training recurrent neural networks), embeddings and prediction projection layers.\n                    # Note that we consider attention mechanism v_a weights as a prediction projection layer and we don""t regularize it. (This gave better stability)\n                    regularization = tf.add_n([tf.nn.l2_loss(v) for v in self.all_vars\n                                               if not (\n                                    ""bias"" in v.name or ""Bias"" in v.name or ""_projection"" in v.name or ""inputs_embedding"" in v.name\n                                    or ""RNN"" in v.name or ""LSTM"" in v.name)]) * reg_weight\n                    \n                    # Compute final loss term\n                    self.tower_before_loss.append(before)\n                    self.tower_after_loss.append(after)\n                    self.tower_stop_token_loss.append(stop_token_loss)\n                    self.tower_regularization_loss.append(regularization)\n                    self.tower_linear_loss.append(linear_loss)\n                    \n                    loss = before + after + stop_token_loss + regularization + linear_loss\n                    self.tower_loss.append(loss)\n        \n        for i in range(hp.tacotron_num_gpus):\n            total_before_loss += self.tower_before_loss[i]\n            total_after_loss += self.tower_after_loss[i]\n            total_stop_token_loss += self.tower_stop_token_loss[i]\n            total_regularization_loss += self.tower_regularization_loss[i]\n            total_linear_loss += self.tower_linear_loss[i]\n            total_loss += self.tower_loss[i]\n        \n        self.before_loss = total_before_loss / hp.tacotron_num_gpus\n        self.after_loss = total_after_loss / hp.tacotron_num_gpus\n        self.stop_token_loss = total_stop_token_loss / hp.tacotron_num_gpus\n        self.regularization_loss = total_regularization_loss / hp.tacotron_num_gpus\n        self.linear_loss = total_linear_loss / hp.tacotron_num_gpus\n        self.loss = total_loss / hp.tacotron_num_gpus\n    \n    def add_optimizer(self, global_step):\n        """"""Adds optimizer. Sets ""gradients"" and ""optimize"" fields. add_loss must have been called.\n        Args:\n            global_step: int32 scalar Tensor representing current global step in training\n        """"""\n        hp = self._hparams\n        tower_gradients = []\n        \n        # 1. Declare GPU Devices\n        gpus = [""/gpu:{}"".format(i) for i in\n                range(hp.tacotron_gpu_start_idx, hp.tacotron_gpu_start_idx + hp.tacotron_num_gpus)]\n        \n        grad_device = ""/cpu:0"" if hp.tacotron_num_gpus > 1 else gpus[0]\n        \n        with tf.device(grad_device):\n            with tf.variable_scope(""optimizer"") as scope:\n                if hp.tacotron_decay_learning_rate:\n                    self.decay_steps = hp.tacotron_decay_steps\n                    self.decay_rate = hp.tacotron_decay_rate\n                    self.learning_rate = self._learning_rate_decay(\n                        hp.tacotron_initial_learning_rate, global_step)\n                else:\n                    self.learning_rate = tf.convert_to_tensor(hp.tacotron_initial_learning_rate)\n                \n                optimizer = tf.train.AdamOptimizer(self.learning_rate, hp.tacotron_adam_beta1,\n                                                   hp.tacotron_adam_beta2, hp.tacotron_adam_epsilon)\n        \n        # 2. Compute Gradient\n        for i in range(hp.tacotron_num_gpus):\n            #  Device placement\n            with tf.device(tf.train.replica_device_setter(ps_tasks=1, ps_device=""/cpu:0"",\n                                                          worker_device=gpus[i])):\n                # agg_loss += self.tower_loss[i]\n                with tf.variable_scope(""optimizer"") as scope:\n                    gradients = optimizer.compute_gradients(self.tower_loss[i])\n                    tower_gradients.append(gradients)\n        \n        # 3. Average Gradient\n        with tf.device(grad_device):\n            avg_grads = []\n            vars = []\n            for grad_and_vars in zip(*tower_gradients):\n                # grads_vars = [(grad1, var), (grad2, var), ...]\n                grads = []\n                for g, _ in grad_and_vars:\n                    expanded_g = tf.expand_dims(g, 0)\n                    # Append on a ""tower"" dimension which we will average over below.\n                    grads.append(expanded_g)\n                # Average over the ""tower"" dimension.\n                grad = tf.concat(axis=0, values=grads)\n                grad = tf.reduce_mean(grad, 0)\n                \n                v = grad_and_vars[0][1]\n                avg_grads.append(grad)\n                vars.append(v)\n            \n            self.gradients = avg_grads\n            # Just for causion\n            # https://github.com/Rayhane-mamah/Tacotron-2/issues/11\n            if hp.tacotron_clip_gradients:\n                clipped_gradients, _ = tf.clip_by_global_norm(avg_grads, 1.)  # __mark 0.5 refer\n            else:\n                clipped_gradients = avg_grads\n            \n            # Add dependency on UPDATE_OPS; otherwise batchnorm won""t work correctly. See:\n            # https://github.com/tensorflow/tensorflow/issues/1122\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                self.optimize = optimizer.apply_gradients(zip(clipped_gradients, vars),\n                                                          global_step=global_step)\n    \n    def _learning_rate_decay(self, init_lr, global_step):\n        #################################################################\n        # Narrow Exponential Decay:\n        \n        # Phase 1: lr = 1e-3\n        # We only start learning rate decay after 50k steps\n        \n        # Phase 2: lr in ]1e-5, 1e-3[\n        # decay reach minimal value at step 310k\n        \n        # Phase 3: lr = 1e-5\n        # clip by minimal learning rate value (step > 310k)\n        #################################################################\n        hp = self._hparams\n        \n        # Compute natural exponential decay\n        lr = tf.train.exponential_decay(init_lr,\n                                        global_step - hp.tacotron_start_decay,\n                                        # lr = 1e-3 at step 50k\n                                        self.decay_steps,\n                                        self.decay_rate,  # lr = 1e-5 around step 310k\n                                        name=""lr_exponential_decay"")\n        \n        # clip learning rate by max and min values (initial and final values)\n        return tf.minimum(tf.maximum(lr, hp.tacotron_final_learning_rate), init_lr)\n'"
synthesizer/utils/__init__.py,0,"b'class ValueWindow():\n  def __init__(self, window_size=100):\n    self._window_size = window_size\n    self._values = []\n\n  def append(self, x):\n    self._values = self._values[-(self._window_size - 1):] + [x]\n\n  @property\n  def sum(self):\n    return sum(self._values)\n\n  @property\n  def count(self):\n    return len(self._values)\n\n  @property\n  def average(self):\n    return self.sum / max(1, self.count)\n\n  def reset(self):\n    self._values = []\n'"
synthesizer/utils/_cmudict.py,0,"b'import re\n\nvalid_symbols = [\n  ""AA"", ""AA0"", ""AA1"", ""AA2"", ""AE"", ""AE0"", ""AE1"", ""AE2"", ""AH"", ""AH0"", ""AH1"", ""AH2"",\n  ""AO"", ""AO0"", ""AO1"", ""AO2"", ""AW"", ""AW0"", ""AW1"", ""AW2"", ""AY"", ""AY0"", ""AY1"", ""AY2"",\n  ""B"", ""CH"", ""D"", ""DH"", ""EH"", ""EH0"", ""EH1"", ""EH2"", ""ER"", ""ER0"", ""ER1"", ""ER2"", ""EY"",\n  ""EY0"", ""EY1"", ""EY2"", ""F"", ""G"", ""HH"", ""IH"", ""IH0"", ""IH1"", ""IH2"", ""IY"", ""IY0"", ""IY1"",\n  ""IY2"", ""JH"", ""K"", ""L"", ""M"", ""N"", ""NG"", ""OW"", ""OW0"", ""OW1"", ""OW2"", ""OY"", ""OY0"",\n  ""OY1"", ""OY2"", ""P"", ""R"", ""S"", ""SH"", ""T"", ""TH"", ""UH"", ""UH0"", ""UH1"", ""UH2"", ""UW"",\n  ""UW0"", ""UW1"", ""UW2"", ""V"", ""W"", ""Y"", ""Z"", ""ZH""\n]\n\n_valid_symbol_set = set(valid_symbols)\n\n\nclass CMUDict:\n  """"""Thin wrapper around CMUDict data. http://www.speech.cs.cmu.edu/cgi-bin/cmudict""""""\n  def __init__(self, file_or_path, keep_ambiguous=True):\n    if isinstance(file_or_path, str):\n      with open(file_or_path, encoding=""latin-1"") as f:\n        entries = _parse_cmudict(f)\n    else:\n      entries = _parse_cmudict(file_or_path)\n    if not keep_ambiguous:\n      entries = {word: pron for word, pron in entries.items() if len(pron) == 1}\n    self._entries = entries\n\n\n  def __len__(self):\n    return len(self._entries)\n\n\n  def lookup(self, word):\n    """"""Returns list of ARPAbet pronunciations of the given word.""""""\n    return self._entries.get(word.upper())\n\n\n\n_alt_re = re.compile(r""\\([0-9]+\\)"")\n\n\ndef _parse_cmudict(file):\n  cmudict = {}\n  for line in file:\n    if len(line) and (line[0] >= ""A"" and line[0] <= ""Z"" or line[0] == ""\'""):\n      parts = line.split(""  "")\n      word = re.sub(_alt_re, """", parts[0])\n      pronunciation = _get_pronunciation(parts[1])\n      if pronunciation:\n        if word in cmudict:\n          cmudict[word].append(pronunciation)\n        else:\n          cmudict[word] = [pronunciation]\n  return cmudict\n\n\ndef _get_pronunciation(s):\n  parts = s.strip().split("" "")\n  for part in parts:\n    if part not in _valid_symbol_set:\n      return None\n  return "" "".join(parts)\n'"
synthesizer/utils/cleaners.py,0,"b'""""""\nCleaners are transformations that run over the input text at both training and eval time.\n\nCleaners can be selected by passing a comma-delimited list of cleaner names as the ""cleaners""\nhyperparameter. Some cleaners are English-specific. You""ll typically want to use:\n  1. ""english_cleaners"" for English text\n  2. ""transliteration_cleaners"" for non-English text that can be transliterated to ASCII using\n     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n  3. ""basic_cleaners"" if you do not want to transliterate (in this case, you should also update\n     the symbols in symbols.py to match your data).\n""""""\n\nimport re\nfrom unidecode import unidecode\nfrom .numbers import normalize_numbers\n\n# Regular expression matching whitespace:\n_whitespace_re = re.compile(r""\\s+"")\n\n# List of (regular expression, replacement) pairs for abbreviations:\n_abbreviations = [(re.compile(""\\\\b%s\\\\."" % x[0], re.IGNORECASE), x[1]) for x in [\n  (""mrs"", ""misess""),\n  (""mr"", ""mister""),\n  (""dr"", ""doctor""),\n  (""st"", ""saint""),\n  (""co"", ""company""),\n  (""jr"", ""junior""),\n  (""maj"", ""major""),\n  (""gen"", ""general""),\n  (""drs"", ""doctors""),\n  (""rev"", ""reverend""),\n  (""lt"", ""lieutenant""),\n  (""hon"", ""honorable""),\n  (""sgt"", ""sergeant""),\n  (""capt"", ""captain""),\n  (""esq"", ""esquire""),\n  (""ltd"", ""limited""),\n  (""col"", ""colonel""),\n  (""ft"", ""fort""),\n]]\n\n\ndef expand_abbreviations(text):\n  for regex, replacement in _abbreviations:\n    text = re.sub(regex, replacement, text)\n  return text\n\n\ndef expand_numbers(text):\n  return normalize_numbers(text)\n\n\ndef lowercase(text):\n  """"""lowercase input tokens.""""""\n  return text.lower()\n\n\ndef collapse_whitespace(text):\n  return re.sub(_whitespace_re, "" "", text)\n\n\ndef convert_to_ascii(text):\n  return unidecode(text)\n\n\ndef basic_cleaners(text):\n  """"""Basic pipeline that lowercases and collapses whitespace without transliteration.""""""\n  text = lowercase(text)\n  text = collapse_whitespace(text)\n  return text\n\n\ndef transliteration_cleaners(text):\n  """"""Pipeline for non-English text that transliterates to ASCII.""""""\n  text = convert_to_ascii(text)\n  text = lowercase(text)\n  text = collapse_whitespace(text)\n  return text\n\n\ndef english_cleaners(text):\n  """"""Pipeline for English text, including number and abbreviation expansion.""""""\n  text = convert_to_ascii(text)\n  text = lowercase(text)\n  text = expand_numbers(text)\n  text = expand_abbreviations(text)\n  text = collapse_whitespace(text)\n  return text\n'"
synthesizer/utils/numbers.py,0,"b'import re\nimport inflect\n\n_inflect = inflect.engine()\n_comma_number_re = re.compile(r""([0-9][0-9\\,]+[0-9])"")\n_decimal_number_re = re.compile(r""([0-9]+\\.[0-9]+)"")\n_pounds_re = re.compile(r""\xc2\xa3([0-9\\,]*[0-9]+)"")\n_dollars_re = re.compile(r""\\$([0-9\\.\\,]*[0-9]+)"")\n_ordinal_re = re.compile(r""[0-9]+(st|nd|rd|th)"")\n_number_re = re.compile(r""[0-9]+"")\n\n\ndef _remove_commas(m):\n  return m.group(1).replace("","", """")\n\n\ndef _expand_decimal_point(m):\n  return m.group(1).replace(""."", "" point "")\n\n\ndef _expand_dollars(m):\n  match = m.group(1)\n  parts = match.split(""."")\n  if len(parts) > 2:\n    return match + "" dollars""  # Unexpected format\n  dollars = int(parts[0]) if parts[0] else 0\n  cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n  if dollars and cents:\n    dollar_unit = ""dollar"" if dollars == 1 else ""dollars""\n    cent_unit = ""cent"" if cents == 1 else ""cents""\n    return ""%s %s, %s %s"" % (dollars, dollar_unit, cents, cent_unit)\n  elif dollars:\n    dollar_unit = ""dollar"" if dollars == 1 else ""dollars""\n    return ""%s %s"" % (dollars, dollar_unit)\n  elif cents:\n    cent_unit = ""cent"" if cents == 1 else ""cents""\n    return ""%s %s"" % (cents, cent_unit)\n  else:\n    return ""zero dollars""\n\n\ndef _expand_ordinal(m):\n  return _inflect.number_to_words(m.group(0))\n\n\ndef _expand_number(m):\n  num = int(m.group(0))\n  if num > 1000 and num < 3000:\n    if num == 2000:\n      return ""two thousand""\n    elif num > 2000 and num < 2010:\n      return ""two thousand "" + _inflect.number_to_words(num % 100)\n    elif num % 100 == 0:\n      return _inflect.number_to_words(num // 100) + "" hundred""\n    else:\n      return _inflect.number_to_words(num, andword="""", zero=""oh"", group=2).replace("", "", "" "")\n  else:\n    return _inflect.number_to_words(num, andword="""")\n\n\ndef normalize_numbers(text):\n  text = re.sub(_comma_number_re, _remove_commas, text)\n  text = re.sub(_pounds_re, r""\\1 pounds"", text)\n  text = re.sub(_dollars_re, _expand_dollars, text)\n  text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n  text = re.sub(_ordinal_re, _expand_ordinal, text)\n  text = re.sub(_number_re, _expand_number, text)\n  return text\n'"
synthesizer/utils/plot.py,0,"b'import matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef split_title_line(title_text, max_words=5):\n\t""""""\n\tA function that splits any string based on specific character\n\t(returning it with the string), with maximum number of words on it\n\t""""""\n\tseq = title_text.split()\n\treturn ""\\n"".join(["" "".join(seq[i:i + max_words]) for i in range(0, len(seq), max_words)])\n\ndef plot_alignment(alignment, path, title=None, split_title=False, max_len=None):\n\tif max_len is not None:\n\t\talignment = alignment[:, :max_len]\n\n\tfig = plt.figure(figsize=(8, 6))\n\tax = fig.add_subplot(111)\n\n\tim = ax.imshow(\n\t\talignment,\n\t\taspect=""auto"",\n\t\torigin=""lower"",\n\t\tinterpolation=""none"")\n\tfig.colorbar(im, ax=ax)\n\txlabel = ""Decoder timestep""\n\n\tif split_title:\n\t\ttitle = split_title_line(title)\n\n\tplt.xlabel(xlabel)\n\tplt.title(title)\n\tplt.ylabel(""Encoder timestep"")\n\tplt.tight_layout()\n\tplt.savefig(path, format=""png"")\n\tplt.close()\n\n\ndef plot_spectrogram(pred_spectrogram, path, title=None, split_title=False, target_spectrogram=None, max_len=None, auto_aspect=False):\n\tif max_len is not None:\n\t\ttarget_spectrogram = target_spectrogram[:max_len]\n\t\tpred_spectrogram = pred_spectrogram[:max_len]\n\n\tif split_title:\n\t\ttitle = split_title_line(title)\n\n\tfig = plt.figure(figsize=(10, 8))\n\t# Set common labels\n\tfig.text(0.5, 0.18, title, horizontalalignment=""center"", fontsize=16)\n\n\t#target spectrogram subplot\n\tif target_spectrogram is not None:\n\t\tax1 = fig.add_subplot(311)\n\t\tax2 = fig.add_subplot(312)\n\n\t\tif auto_aspect:\n\t\t\tim = ax1.imshow(np.rot90(target_spectrogram), aspect=""auto"", interpolation=""none"")\n\t\telse:\n\t\t\tim = ax1.imshow(np.rot90(target_spectrogram), interpolation=""none"")\n\t\tax1.set_title(""Target Mel-Spectrogram"")\n\t\tfig.colorbar(mappable=im, shrink=0.65, orientation=""horizontal"", ax=ax1)\n\t\tax2.set_title(""Predicted Mel-Spectrogram"")\n\telse:\n\t\tax2 = fig.add_subplot(211)\n\n\tif auto_aspect:\n\t\tim = ax2.imshow(np.rot90(pred_spectrogram), aspect=""auto"", interpolation=""none"")\n\telse:\n\t\tim = ax2.imshow(np.rot90(pred_spectrogram), interpolation=""none"")\n\tfig.colorbar(mappable=im, shrink=0.65, orientation=""horizontal"", ax=ax2)\n\n\tplt.tight_layout()\n\tplt.savefig(path, format=""png"")\n\tplt.close()\n'"
synthesizer/utils/symbols.py,0,"b'""""""\nDefines the set of symbols used in text input to the model.\n\nThe default is a set of ASCII characters that works well for English or text that has been run\nthrough Unidecode. For other data, you can modify _characters. See TRAINING_DATA.md for details.\n""""""\n# from . import cmudict\n\n_pad        = ""_""\n_eos        = ""~""\n_characters = ""ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!\\\'\\""(),-.:;? ""\n\n# Prepend ""@"" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n#_arpabet = [""@\' + s for s in cmudict.valid_symbols]\n\n# Export all symbols:\nsymbols = [_pad, _eos] + list(_characters) #+ _arpabet\n'"
synthesizer/utils/text.py,0,"b'from .symbols import symbols\nfrom . import cleaners\nimport re\n\n# Mappings from symbol to numeric ID and vice versa:\n_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n\n# Regular expression matching text enclosed in curly braces:\n_curly_re = re.compile(r""(.*?)\\{(.+?)\\}(.*)"")\n\n\ndef text_to_sequence(text, cleaner_names):\n  """"""Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n\n    The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n    in it. For example, ""Turn left on {HH AW1 S S T AH0 N} Street.""\n\n    Args:\n      text: string to convert to a sequence\n      cleaner_names: names of the cleaner functions to run the text through\n\n    Returns:\n      List of integers corresponding to the symbols in the text\n  """"""\n  sequence = []\n\n  # Check for curly braces and treat their contents as ARPAbet:\n  while len(text):\n    m = _curly_re.match(text)\n    if not m:\n      sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n      break\n    sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n    sequence += _arpabet_to_sequence(m.group(2))\n    text = m.group(3)\n\n  # Append EOS token\n  sequence.append(_symbol_to_id[""~""])\n  return sequence\n\n\ndef sequence_to_text(sequence):\n  """"""Converts a sequence of IDs back to a string""""""\n  result = """"\n  for symbol_id in sequence:\n    if symbol_id in _id_to_symbol:\n      s = _id_to_symbol[symbol_id]\n      # Enclose ARPAbet back in curly braces:\n      if len(s) > 1 and s[0] == ""@"":\n        s = ""{%s}"" % s[1:]\n      result += s\n  return result.replace(""}{"", "" "")\n\n\ndef _clean_text(text, cleaner_names):\n  for name in cleaner_names:\n    cleaner = getattr(cleaners, name)\n    if not cleaner:\n      raise Exception(""Unknown cleaner: %s"" % name)\n    text = cleaner(text)\n  return text\n\n\ndef _symbols_to_sequence(symbols):\n  return [_symbol_to_id[s] for s in symbols if _should_keep_symbol(s)]\n\n\ndef _arpabet_to_sequence(text):\n  return _symbols_to_sequence([""@"" + s for s in text.split()])\n\n\ndef _should_keep_symbol(s):\n  return s in _symbol_to_id and s not in (""_"", ""~"")\n'"
vocoder/models/deepmind_version.py,31,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom utils.display import *\nfrom utils.dsp import *\n\n\nclass WaveRNN(nn.Module) :\n    def __init__(self, hidden_size=896, quantisation=256) :\n        super(WaveRNN, self).__init__()\n        \n        self.hidden_size = hidden_size\n        self.split_size = hidden_size // 2\n        \n        # The main matmul\n        self.R = nn.Linear(self.hidden_size, 3 * self.hidden_size, bias=False)\n        \n        # Output fc layers\n        self.O1 = nn.Linear(self.split_size, self.split_size)\n        self.O2 = nn.Linear(self.split_size, quantisation)\n        self.O3 = nn.Linear(self.split_size, self.split_size)\n        self.O4 = nn.Linear(self.split_size, quantisation)\n        \n        # Input fc layers\n        self.I_coarse = nn.Linear(2, 3 * self.split_size, bias=False)\n        self.I_fine = nn.Linear(3, 3 * self.split_size, bias=False)\n\n        # biases for the gates\n        self.bias_u = nn.Parameter(torch.zeros(self.hidden_size))\n        self.bias_r = nn.Parameter(torch.zeros(self.hidden_size))\n        self.bias_e = nn.Parameter(torch.zeros(self.hidden_size))\n        \n        # display num params\n        self.num_params()\n\n        \n    def forward(self, prev_y, prev_hidden, current_coarse) :\n        \n        # Main matmul - the projection is split 3 ways\n        R_hidden = self.R(prev_hidden)\n        R_u, R_r, R_e, = torch.split(R_hidden, self.hidden_size, dim=1)\n        \n        # Project the prev input \n        coarse_input_proj = self.I_coarse(prev_y)\n        I_coarse_u, I_coarse_r, I_coarse_e = \\\n            torch.split(coarse_input_proj, self.split_size, dim=1)\n        \n        # Project the prev input and current coarse sample\n        fine_input = torch.cat([prev_y, current_coarse], dim=1)\n        fine_input_proj = self.I_fine(fine_input)\n        I_fine_u, I_fine_r, I_fine_e = \\\n            torch.split(fine_input_proj, self.split_size, dim=1)\n        \n        # concatenate for the gates\n        I_u = torch.cat([I_coarse_u, I_fine_u], dim=1)\n        I_r = torch.cat([I_coarse_r, I_fine_r], dim=1)\n        I_e = torch.cat([I_coarse_e, I_fine_e], dim=1)\n        \n        # Compute all gates for coarse and fine \n        u = F.sigmoid(R_u + I_u + self.bias_u)\n        r = F.sigmoid(R_r + I_r + self.bias_r)\n        e = F.tanh(r * R_e + I_e + self.bias_e)\n        hidden = u * prev_hidden + (1. - u) * e\n        \n        # Split the hidden state\n        hidden_coarse, hidden_fine = torch.split(hidden, self.split_size, dim=1)\n        \n        # Compute outputs \n        out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n        out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n\n        return out_coarse, out_fine, hidden\n    \n        \n    def generate(self, seq_len):\n        with torch.no_grad():\n            # First split up the biases for the gates \n            b_coarse_u, b_fine_u = torch.split(self.bias_u, self.split_size)\n            b_coarse_r, b_fine_r = torch.split(self.bias_r, self.split_size)\n            b_coarse_e, b_fine_e = torch.split(self.bias_e, self.split_size)\n\n            # Lists for the two output seqs\n            c_outputs, f_outputs = [], []\n\n            # Some initial inputs\n            out_coarse = torch.LongTensor([0]).cuda()\n            out_fine = torch.LongTensor([0]).cuda()\n\n            # We'll meed a hidden state\n            hidden = self.init_hidden()\n\n            # Need a clock for display\n            start = time.time()\n\n            # Loop for generation\n            for i in range(seq_len) :\n\n                # Split into two hidden states\n                hidden_coarse, hidden_fine = \\\n                    torch.split(hidden, self.split_size, dim=1)\n\n                # Scale and concat previous predictions\n                out_coarse = out_coarse.unsqueeze(0).float() / 127.5 - 1.\n                out_fine = out_fine.unsqueeze(0).float() / 127.5 - 1.\n                prev_outputs = torch.cat([out_coarse, out_fine], dim=1)\n\n                # Project input \n                coarse_input_proj = self.I_coarse(prev_outputs)\n                I_coarse_u, I_coarse_r, I_coarse_e = \\\n                    torch.split(coarse_input_proj, self.split_size, dim=1)\n\n                # Project hidden state and split 6 ways\n                R_hidden = self.R(hidden)\n                R_coarse_u , R_fine_u, \\\n                R_coarse_r, R_fine_r, \\\n                R_coarse_e, R_fine_e = torch.split(R_hidden, self.split_size, dim=1)\n\n                # Compute the coarse gates\n                u = F.sigmoid(R_coarse_u + I_coarse_u + b_coarse_u)\n                r = F.sigmoid(R_coarse_r + I_coarse_r + b_coarse_r)\n                e = F.tanh(r * R_coarse_e + I_coarse_e + b_coarse_e)\n                hidden_coarse = u * hidden_coarse + (1. - u) * e\n\n                # Compute the coarse output\n                out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n                posterior = F.softmax(out_coarse, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_coarse = distrib.sample()\n                c_outputs.append(out_coarse)\n\n                # Project the [prev outputs and predicted coarse sample]\n                coarse_pred = out_coarse.float() / 127.5 - 1.\n                fine_input = torch.cat([prev_outputs, coarse_pred.unsqueeze(0)], dim=1)\n                fine_input_proj = self.I_fine(fine_input)\n                I_fine_u, I_fine_r, I_fine_e = \\\n                    torch.split(fine_input_proj, self.split_size, dim=1)\n\n                # Compute the fine gates\n                u = F.sigmoid(R_fine_u + I_fine_u + b_fine_u)\n                r = F.sigmoid(R_fine_r + I_fine_r + b_fine_r)\n                e = F.tanh(r * R_fine_e + I_fine_e + b_fine_e)\n                hidden_fine = u * hidden_fine + (1. - u) * e\n\n                # Compute the fine output\n                out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n                posterior = F.softmax(out_fine, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_fine = distrib.sample()\n                f_outputs.append(out_fine)\n\n                # Put the hidden state back together\n                hidden = torch.cat([hidden_coarse, hidden_fine], dim=1)\n\n                # Display progress\n                speed = (i + 1) / (time.time() - start)\n                stream('Gen: %i/%i -- Speed: %i',  (i + 1, seq_len, speed))\n\n            coarse = torch.stack(c_outputs).squeeze(1).cpu().data.numpy()\n            fine = torch.stack(f_outputs).squeeze(1).cpu().data.numpy()        \n            output = combine_signal(coarse, fine)\n        \n        return output, coarse, fine\n\n    def init_hidden(self, batch_size=1) :\n        return torch.zeros(batch_size, self.hidden_size).cuda()\n    \n    def num_params(self) :\n        parameters = filter(lambda p: p.requires_grad, self.parameters())\n        parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n        print('Trainable Parameters: %.3f million' % parameters)"""
vocoder/models/fatchord_version.py,24,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom vocoder.distribution import sample_from_discretized_mix_logistic\nfrom vocoder.display import *\nfrom vocoder.audio import *\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, dims):\n        super().__init__()\n        self.conv1 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n        self.conv2 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n        self.batch_norm1 = nn.BatchNorm1d(dims)\n        self.batch_norm2 = nn.BatchNorm1d(dims)\n\n    def forward(self, x):\n        residual = x\n        x = self.conv1(x)\n        x = self.batch_norm1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = self.batch_norm2(x)\n        return x + residual\n\n\nclass MelResNet(nn.Module):\n    def __init__(self, res_blocks, in_dims, compute_dims, res_out_dims, pad):\n        super().__init__()\n        k_size = pad * 2 + 1\n        self.conv_in = nn.Conv1d(in_dims, compute_dims, kernel_size=k_size, bias=False)\n        self.batch_norm = nn.BatchNorm1d(compute_dims)\n        self.layers = nn.ModuleList()\n        for i in range(res_blocks):\n            self.layers.append(ResBlock(compute_dims))\n        self.conv_out = nn.Conv1d(compute_dims, res_out_dims, kernel_size=1)\n\n    def forward(self, x):\n        x = self.conv_in(x)\n        x = self.batch_norm(x)\n        x = F.relu(x)\n        for f in self.layers: x = f(x)\n        x = self.conv_out(x)\n        return x\n\n\nclass Stretch2d(nn.Module):\n    def __init__(self, x_scale, y_scale):\n        super().__init__()\n        self.x_scale = x_scale\n        self.y_scale = y_scale\n\n    def forward(self, x):\n        b, c, h, w = x.size()\n        x = x.unsqueeze(-1).unsqueeze(3)\n        x = x.repeat(1, 1, 1, self.y_scale, 1, self.x_scale)\n        return x.view(b, c, h * self.y_scale, w * self.x_scale)\n\n\nclass UpsampleNetwork(nn.Module):\n    def __init__(self, feat_dims, upsample_scales, compute_dims,\n                 res_blocks, res_out_dims, pad):\n        super().__init__()\n        total_scale = np.cumproduct(upsample_scales)[-1]\n        self.indent = pad * total_scale\n        self.resnet = MelResNet(res_blocks, feat_dims, compute_dims, res_out_dims, pad)\n        self.resnet_stretch = Stretch2d(total_scale, 1)\n        self.up_layers = nn.ModuleList()\n        for scale in upsample_scales:\n            k_size = (1, scale * 2 + 1)\n            padding = (0, scale)\n            stretch = Stretch2d(scale, 1)\n            conv = nn.Conv2d(1, 1, kernel_size=k_size, padding=padding, bias=False)\n            conv.weight.data.fill_(1. / k_size[1])\n            self.up_layers.append(stretch)\n            self.up_layers.append(conv)\n\n    def forward(self, m):\n        aux = self.resnet(m).unsqueeze(1)\n        aux = self.resnet_stretch(aux)\n        aux = aux.squeeze(1)\n        m = m.unsqueeze(1)\n        for f in self.up_layers: m = f(m)\n        m = m.squeeze(1)[:, :, self.indent:-self.indent]\n        return m.transpose(1, 2), aux.transpose(1, 2)\n\n\nclass WaveRNN(nn.Module):\n    def __init__(self, rnn_dims, fc_dims, bits, pad, upsample_factors,\n                 feat_dims, compute_dims, res_out_dims, res_blocks,\n                 hop_length, sample_rate, mode=\'RAW\'):\n        super().__init__()\n        self.mode = mode\n        self.pad = pad\n        if self.mode == \'RAW\' :\n            self.n_classes = 2 ** bits\n        elif self.mode == \'MOL\' :\n            self.n_classes = 30\n        else :\n            RuntimeError(""Unknown model mode value - "", self.mode)\n\n        self.rnn_dims = rnn_dims\n        self.aux_dims = res_out_dims // 4\n        self.hop_length = hop_length\n        self.sample_rate = sample_rate\n\n        self.upsample = UpsampleNetwork(feat_dims, upsample_factors, compute_dims, res_blocks, res_out_dims, pad)\n        self.I = nn.Linear(feat_dims + self.aux_dims + 1, rnn_dims)\n        self.rnn1 = nn.GRU(rnn_dims, rnn_dims, batch_first=True)\n        self.rnn2 = nn.GRU(rnn_dims + self.aux_dims, rnn_dims, batch_first=True)\n        self.fc1 = nn.Linear(rnn_dims + self.aux_dims, fc_dims)\n        self.fc2 = nn.Linear(fc_dims + self.aux_dims, fc_dims)\n        self.fc3 = nn.Linear(fc_dims, self.n_classes)\n\n        self.step = nn.Parameter(torch.zeros(1).long(), requires_grad=False)\n        self.num_params()\n\n    def forward(self, x, mels):\n        self.step += 1\n        bsize = x.size(0)\n        h1 = torch.zeros(1, bsize, self.rnn_dims).cuda()\n        h2 = torch.zeros(1, bsize, self.rnn_dims).cuda()\n        mels, aux = self.upsample(mels)\n\n        aux_idx = [self.aux_dims * i for i in range(5)]\n        a1 = aux[:, :, aux_idx[0]:aux_idx[1]]\n        a2 = aux[:, :, aux_idx[1]:aux_idx[2]]\n        a3 = aux[:, :, aux_idx[2]:aux_idx[3]]\n        a4 = aux[:, :, aux_idx[3]:aux_idx[4]]\n\n        x = torch.cat([x.unsqueeze(-1), mels, a1], dim=2)\n        x = self.I(x)\n        res = x\n        x, _ = self.rnn1(x, h1)\n\n        x = x + res\n        res = x\n        x = torch.cat([x, a2], dim=2)\n        x, _ = self.rnn2(x, h2)\n\n        x = x + res\n        x = torch.cat([x, a3], dim=2)\n        x = F.relu(self.fc1(x))\n\n        x = torch.cat([x, a4], dim=2)\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\n    def generate(self, mels, batched, target, overlap, mu_law, progress_callback=None):\n        mu_law = mu_law if self.mode == \'RAW\' else False\n        progress_callback = progress_callback or self.gen_display\n\n        self.eval()\n        output = []\n        start = time.time()\n        rnn1 = self.get_gru_cell(self.rnn1)\n        rnn2 = self.get_gru_cell(self.rnn2)\n\n        with torch.no_grad():\n            mels = mels.cuda()\n            wave_len = (mels.size(-1) - 1) * self.hop_length\n            mels = self.pad_tensor(mels.transpose(1, 2), pad=self.pad, side=\'both\')\n            mels, aux = self.upsample(mels.transpose(1, 2))\n\n            if batched:\n                mels = self.fold_with_overlap(mels, target, overlap)\n                aux = self.fold_with_overlap(aux, target, overlap)\n\n            b_size, seq_len, _ = mels.size()\n\n            h1 = torch.zeros(b_size, self.rnn_dims).cuda()\n            h2 = torch.zeros(b_size, self.rnn_dims).cuda()\n            x = torch.zeros(b_size, 1).cuda()\n\n            d = self.aux_dims\n            aux_split = [aux[:, :, d * i:d * (i + 1)] for i in range(4)]\n\n            for i in range(seq_len):\n\n                m_t = mels[:, i, :]\n\n                a1_t, a2_t, a3_t, a4_t = (a[:, i, :] for a in aux_split)\n\n                x = torch.cat([x, m_t, a1_t], dim=1)\n                x = self.I(x)\n                h1 = rnn1(x, h1)\n\n                x = x + h1\n                inp = torch.cat([x, a2_t], dim=1)\n                h2 = rnn2(inp, h2)\n\n                x = x + h2\n                x = torch.cat([x, a3_t], dim=1)\n                x = F.relu(self.fc1(x))\n\n                x = torch.cat([x, a4_t], dim=1)\n                x = F.relu(self.fc2(x))\n\n                logits = self.fc3(x)\n\n                if self.mode == \'MOL\':\n                    sample = sample_from_discretized_mix_logistic(logits.unsqueeze(0).transpose(1, 2))\n                    output.append(sample.view(-1))\n                    # x = torch.FloatTensor([[sample]]).cuda()\n                    x = sample.transpose(0, 1).cuda()\n\n                elif self.mode == \'RAW\' :\n                    posterior = F.softmax(logits, dim=1)\n                    distrib = torch.distributions.Categorical(posterior)\n\n                    sample = 2 * distrib.sample().float() / (self.n_classes - 1.) - 1.\n                    output.append(sample)\n                    x = sample.unsqueeze(-1)\n                else:\n                    raise RuntimeError(""Unknown model mode value - "", self.mode)\n\n                if i % 100 == 0:\n                    gen_rate = (i + 1) / (time.time() - start) * b_size / 1000\n                    progress_callback(i, seq_len, b_size, gen_rate)\n\n        output = torch.stack(output).transpose(0, 1)\n        output = output.cpu().numpy()\n        output = output.astype(np.float64)\n        \n        if batched:\n            output = self.xfade_and_unfold(output, target, overlap)\n        else:\n            output = output[0]\n\n        if mu_law:\n            output = decode_mu_law(output, self.n_classes, False)\n        if hp.apply_preemphasis:\n            output = de_emphasis(output)\n\n        # Fade-out at the end to avoid signal cutting out suddenly\n        fade_out = np.linspace(1, 0, 20 * self.hop_length)\n        output = output[:wave_len]\n        output[-20 * self.hop_length:] *= fade_out\n        \n        self.train()\n\n        return output\n\n\n    def gen_display(self, i, seq_len, b_size, gen_rate):\n        pbar = progbar(i, seq_len)\n        msg = f\'| {pbar} {i*b_size}/{seq_len*b_size} | Batch Size: {b_size} | Gen Rate: {gen_rate:.1f}kHz | \'\n        stream(msg)\n\n    def get_gru_cell(self, gru):\n        gru_cell = nn.GRUCell(gru.input_size, gru.hidden_size)\n        gru_cell.weight_hh.data = gru.weight_hh_l0.data\n        gru_cell.weight_ih.data = gru.weight_ih_l0.data\n        gru_cell.bias_hh.data = gru.bias_hh_l0.data\n        gru_cell.bias_ih.data = gru.bias_ih_l0.data\n        return gru_cell\n\n    def pad_tensor(self, x, pad, side=\'both\'):\n        # NB - this is just a quick method i need right now\n        # i.e., it won\'t generalise to other shapes/dims\n        b, t, c = x.size()\n        total = t + 2 * pad if side == \'both\' else t + pad\n        padded = torch.zeros(b, total, c).cuda()\n        if side == \'before\' or side == \'both\':\n            padded[:, pad:pad + t, :] = x\n        elif side == \'after\':\n            padded[:, :t, :] = x\n        return padded\n\n    def fold_with_overlap(self, x, target, overlap):\n\n        \'\'\' Fold the tensor with overlap for quick batched inference.\n            Overlap will be used for crossfading in xfade_and_unfold()\n\n        Args:\n            x (tensor)    : Upsampled conditioning features.\n                            shape=(1, timesteps, features)\n            target (int)  : Target timesteps for each index of batch\n            overlap (int) : Timesteps for both xfade and rnn warmup\n\n        Return:\n            (tensor) : shape=(num_folds, target + 2 * overlap, features)\n\n        Details:\n            x = [[h1, h2, ... hn]]\n\n            Where each h is a vector of conditioning features\n\n            Eg: target=2, overlap=1 with x.size(1)=10\n\n            folded = [[h1, h2, h3, h4],\n                      [h4, h5, h6, h7],\n                      [h7, h8, h9, h10]]\n        \'\'\'\n\n        _, total_len, features = x.size()\n\n        # Calculate variables needed\n        num_folds = (total_len - overlap) // (target + overlap)\n        extended_len = num_folds * (overlap + target) + overlap\n        remaining = total_len - extended_len\n\n        # Pad if some time steps poking out\n        if remaining != 0:\n            num_folds += 1\n            padding = target + 2 * overlap - remaining\n            x = self.pad_tensor(x, padding, side=\'after\')\n\n        folded = torch.zeros(num_folds, target + 2 * overlap, features).cuda()\n\n        # Get the values for the folded tensor\n        for i in range(num_folds):\n            start = i * (target + overlap)\n            end = start + target + 2 * overlap\n            folded[i] = x[:, start:end, :]\n\n        return folded\n\n    def xfade_and_unfold(self, y, target, overlap):\n\n        \'\'\' Applies a crossfade and unfolds into a 1d array.\n\n        Args:\n            y (ndarry)    : Batched sequences of audio samples\n                            shape=(num_folds, target + 2 * overlap)\n                            dtype=np.float64\n            overlap (int) : Timesteps for both xfade and rnn warmup\n\n        Return:\n            (ndarry) : audio samples in a 1d array\n                       shape=(total_len)\n                       dtype=np.float64\n\n        Details:\n            y = [[seq1],\n                 [seq2],\n                 [seq3]]\n\n            Apply a gain envelope at both ends of the sequences\n\n            y = [[seq1_in, seq1_target, seq1_out],\n                 [seq2_in, seq2_target, seq2_out],\n                 [seq3_in, seq3_target, seq3_out]]\n\n            Stagger and add up the groups of samples:\n\n            [seq1_in, seq1_target, (seq1_out + seq2_in), seq2_target, ...]\n\n        \'\'\'\n\n        num_folds, length = y.shape\n        target = length - 2 * overlap\n        total_len = num_folds * (target + overlap) + overlap\n\n        # Need some silence for the rnn warmup\n        silence_len = overlap // 2\n        fade_len = overlap - silence_len\n        silence = np.zeros((silence_len), dtype=np.float64)\n\n        # Equal power crossfade\n        t = np.linspace(-1, 1, fade_len, dtype=np.float64)\n        fade_in = np.sqrt(0.5 * (1 + t))\n        fade_out = np.sqrt(0.5 * (1 - t))\n\n        # Concat the silence to the fades\n        fade_in = np.concatenate([silence, fade_in])\n        fade_out = np.concatenate([fade_out, silence])\n\n        # Apply the gain to the overlap samples\n        y[:, :overlap] *= fade_in\n        y[:, -overlap:] *= fade_out\n\n        unfolded = np.zeros((total_len), dtype=np.float64)\n\n        # Loop to add up all the samples\n        for i in range(num_folds):\n            start = i * (target + overlap)\n            end = start + target + 2 * overlap\n            unfolded[start:end] += y[i]\n\n        return unfolded\n\n    def get_step(self) :\n        return self.step.data.item()\n\n    def checkpoint(self, model_dir, optimizer) :\n        k_steps = self.get_step() // 1000\n        self.save(model_dir.joinpath(""checkpoint_%dk_steps.pt"" % k_steps), optimizer)\n\n    def log(self, path, msg) :\n        with open(path, \'a\') as f:\n            print(msg, file=f)\n\n    def load(self, path, optimizer) :\n        checkpoint = torch.load(path)\n        if ""optimizer_state"" in checkpoint:\n            self.load_state_dict(checkpoint[""model_state""])\n            optimizer.load_state_dict(checkpoint[""optimizer_state""])\n        else:\n            # Backwards compatibility\n            self.load_state_dict(checkpoint)\n\n    def save(self, path, optimizer) :\n        torch.save({\n            ""model_state"": self.state_dict(),\n            ""optimizer_state"": optimizer.state_dict(),\n        }, path)\n\n    def num_params(self, print_out=True):\n        parameters = filter(lambda p: p.requires_grad, self.parameters())\n        parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n        if print_out :\n            print(\'Trainable Parameters: %.3fM\' % parameters)\n'"
