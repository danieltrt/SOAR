file_path,api_count,code
pytorch/main.py,16,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nimport re\nimport argparse\nimport os\nimport shutil\nimport time\nimport math\nimport logging\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\nimport torchvision.datasets\n\nfrom mean_teacher import architectures, datasets, data, losses, ramps, cli\nfrom mean_teacher.run_context import RunContext\nfrom mean_teacher.data import NO_LABEL\nfrom mean_teacher.utils import *\n\n\nLOG = logging.getLogger(\'main\')\n\nargs = None\nbest_prec1 = 0\nglobal_step = 0\n\n\ndef main(context):\n    global global_step\n    global best_prec1\n\n    checkpoint_path = context.transient_dir\n    training_log = context.create_train_log(""training"")\n    validation_log = context.create_train_log(""validation"")\n    ema_validation_log = context.create_train_log(""ema_validation"")\n\n    dataset_config = datasets.__dict__[args.dataset]()\n    num_classes = dataset_config.pop(\'num_classes\')\n    train_loader, eval_loader = create_data_loaders(**dataset_config, args=args)\n\n    def create_model(ema=False):\n        LOG.info(""=> creating {pretrained}{ema}model \'{arch}\'"".format(\n            pretrained=\'pre-trained \' if args.pretrained else \'\',\n            ema=\'EMA \' if ema else \'\',\n            arch=args.arch))\n\n        model_factory = architectures.__dict__[args.arch]\n        model_params = dict(pretrained=args.pretrained, num_classes=num_classes)\n        model = model_factory(**model_params)\n        model = nn.DataParallel(model).cuda()\n\n        if ema:\n            for param in model.parameters():\n                param.detach_()\n\n        return model\n\n    model = create_model()\n    ema_model = create_model(ema=True)\n\n    LOG.info(parameters_string(model))\n\n    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay,\n                                nesterov=args.nesterov)\n\n    # optionally resume from a checkpoint\n    if args.resume:\n        assert os.path.isfile(args.resume), ""=> no checkpoint found at \'{}\'"".format(args.resume)\n        LOG.info(""=> loading checkpoint \'{}\'"".format(args.resume))\n        checkpoint = torch.load(args.resume)\n        args.start_epoch = checkpoint[\'epoch\']\n        global_step = checkpoint[\'global_step\']\n        best_prec1 = checkpoint[\'best_prec1\']\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        ema_model.load_state_dict(checkpoint[\'ema_state_dict\'])\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        LOG.info(""=> loaded checkpoint \'{}\' (epoch {})"".format(args.resume, checkpoint[\'epoch\']))\n\n    cudnn.benchmark = True\n\n    if args.evaluate:\n        LOG.info(""Evaluating the primary model:"")\n        validate(eval_loader, model, validation_log, global_step, args.start_epoch)\n        LOG.info(""Evaluating the EMA model:"")\n        validate(eval_loader, ema_model, ema_validation_log, global_step, args.start_epoch)\n        return\n\n    for epoch in range(args.start_epoch, args.epochs):\n        start_time = time.time()\n        # train for one epoch\n        train(train_loader, model, ema_model, optimizer, epoch, training_log)\n        LOG.info(""--- training epoch in %s seconds ---"" % (time.time() - start_time))\n\n        if args.evaluation_epochs and (epoch + 1) % args.evaluation_epochs == 0:\n            start_time = time.time()\n            LOG.info(""Evaluating the primary model:"")\n            prec1 = validate(eval_loader, model, validation_log, global_step, epoch + 1)\n            LOG.info(""Evaluating the EMA model:"")\n            ema_prec1 = validate(eval_loader, ema_model, ema_validation_log, global_step, epoch + 1)\n            LOG.info(""--- validation in %s seconds ---"" % (time.time() - start_time))\n            is_best = ema_prec1 > best_prec1\n            best_prec1 = max(ema_prec1, best_prec1)\n        else:\n            is_best = False\n\n        if args.checkpoint_epochs and (epoch + 1) % args.checkpoint_epochs == 0:\n            save_checkpoint({\n                \'epoch\': epoch + 1,\n                \'global_step\': global_step,\n                \'arch\': args.arch,\n                \'state_dict\': model.state_dict(),\n                \'ema_state_dict\': ema_model.state_dict(),\n                \'best_prec1\': best_prec1,\n                \'optimizer\' : optimizer.state_dict(),\n            }, is_best, checkpoint_path, epoch + 1)\n\n\ndef parse_dict_args(**kwargs):\n    global args\n\n    def to_cmdline_kwarg(key, value):\n        if len(key) == 1:\n            key = ""-{}"".format(key)\n        else:\n            key = ""--{}"".format(re.sub(r""_"", ""-"", key))\n        value = str(value)\n        return key, value\n\n    kwargs_pairs = (to_cmdline_kwarg(key, value)\n                    for key, value in kwargs.items())\n    cmdline_args = list(sum(kwargs_pairs, ()))\n    args = parser.parse_args(cmdline_args)\n\n\ndef create_data_loaders(train_transformation,\n                        eval_transformation,\n                        datadir,\n                        args):\n    traindir = os.path.join(datadir, args.train_subdir)\n    evaldir = os.path.join(datadir, args.eval_subdir)\n\n    assert_exactly_one([args.exclude_unlabeled, args.labeled_batch_size])\n\n    dataset = torchvision.datasets.ImageFolder(traindir, train_transformation)\n\n    if args.labels:\n        with open(args.labels) as f:\n            labels = dict(line.split(\' \') for line in f.read().splitlines())\n        labeled_idxs, unlabeled_idxs = data.relabel_dataset(dataset, labels)\n\n    if args.exclude_unlabeled:\n        sampler = SubsetRandomSampler(labeled_idxs)\n        batch_sampler = BatchSampler(sampler, args.batch_size, drop_last=True)\n    elif args.labeled_batch_size:\n        batch_sampler = data.TwoStreamBatchSampler(\n            unlabeled_idxs, labeled_idxs, args.batch_size, args.labeled_batch_size)\n    else:\n        assert False, ""labeled batch size {}"".format(args.labeled_batch_size)\n\n    train_loader = torch.utils.data.DataLoader(dataset,\n                                               batch_sampler=batch_sampler,\n                                               num_workers=args.workers,\n                                               pin_memory=True)\n\n    eval_loader = torch.utils.data.DataLoader(\n        torchvision.datasets.ImageFolder(evaldir, eval_transformation),\n        batch_size=args.batch_size,\n        shuffle=False,\n        num_workers=2 * args.workers,  # Needs images twice as fast\n        pin_memory=True,\n        drop_last=False)\n\n    return train_loader, eval_loader\n\n\ndef update_ema_variables(model, ema_model, alpha, global_step):\n    # Use the true average until the exponential average is more correct\n    alpha = min(1 - 1 / (global_step + 1), alpha)\n    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n\n\ndef train(train_loader, model, ema_model, optimizer, epoch, log):\n    global global_step\n\n    class_criterion = nn.CrossEntropyLoss(size_average=False, ignore_index=NO_LABEL).cuda()\n    if args.consistency_type == \'mse\':\n        consistency_criterion = losses.softmax_mse_loss\n    elif args.consistency_type == \'kl\':\n        consistency_criterion = losses.softmax_kl_loss\n    else:\n        assert False, args.consistency_type\n    residual_logit_criterion = losses.symmetric_mse_loss\n\n    meters = AverageMeterSet()\n\n    # switch to train mode\n    model.train()\n    ema_model.train()\n\n    end = time.time()\n    for i, ((input, ema_input), target) in enumerate(train_loader):\n        # measure data loading time\n        meters.update(\'data_time\', time.time() - end)\n\n        adjust_learning_rate(optimizer, epoch, i, len(train_loader))\n        meters.update(\'lr\', optimizer.param_groups[0][\'lr\'])\n\n        input_var = torch.autograd.Variable(input)\n        ema_input_var = torch.autograd.Variable(ema_input, volatile=True)\n        target_var = torch.autograd.Variable(target.cuda(async=True))\n\n        minibatch_size = len(target_var)\n        labeled_minibatch_size = target_var.data.ne(NO_LABEL).sum()\n        assert labeled_minibatch_size > 0\n        meters.update(\'labeled_minibatch_size\', labeled_minibatch_size)\n\n        ema_model_out = ema_model(ema_input_var)\n        model_out = model(input_var)\n\n        if isinstance(model_out, Variable):\n            assert args.logit_distance_cost < 0\n            logit1 = model_out\n            ema_logit = ema_model_out\n        else:\n            assert len(model_out) == 2\n            assert len(ema_model_out) == 2\n            logit1, logit2 = model_out\n            ema_logit, _ = ema_model_out\n\n        ema_logit = Variable(ema_logit.detach().data, requires_grad=False)\n\n        if args.logit_distance_cost >= 0:\n            class_logit, cons_logit = logit1, logit2\n            res_loss = args.logit_distance_cost * residual_logit_criterion(class_logit, cons_logit) / minibatch_size\n            meters.update(\'res_loss\', res_loss.data[0])\n        else:\n            class_logit, cons_logit = logit1, logit1\n            res_loss = 0\n\n        class_loss = class_criterion(class_logit, target_var) / minibatch_size\n        meters.update(\'class_loss\', class_loss.data[0])\n\n        ema_class_loss = class_criterion(ema_logit, target_var) / minibatch_size\n        meters.update(\'ema_class_loss\', ema_class_loss.data[0])\n\n        if args.consistency:\n            consistency_weight = get_current_consistency_weight(epoch)\n            meters.update(\'cons_weight\', consistency_weight)\n            consistency_loss = consistency_weight * consistency_criterion(cons_logit, ema_logit) / minibatch_size\n            meters.update(\'cons_loss\', consistency_loss.data[0])\n        else:\n            consistency_loss = 0\n            meters.update(\'cons_loss\', 0)\n\n        loss = class_loss + consistency_loss + res_loss\n        assert not (np.isnan(loss.data[0]) or loss.data[0] > 1e5), \'Loss explosion: {}\'.format(loss.data[0])\n        meters.update(\'loss\', loss.data[0])\n\n        prec1, prec5 = accuracy(class_logit.data, target_var.data, topk=(1, 5))\n        meters.update(\'top1\', prec1[0], labeled_minibatch_size)\n        meters.update(\'error1\', 100. - prec1[0], labeled_minibatch_size)\n        meters.update(\'top5\', prec5[0], labeled_minibatch_size)\n        meters.update(\'error5\', 100. - prec5[0], labeled_minibatch_size)\n\n        ema_prec1, ema_prec5 = accuracy(ema_logit.data, target_var.data, topk=(1, 5))\n        meters.update(\'ema_top1\', ema_prec1[0], labeled_minibatch_size)\n        meters.update(\'ema_error1\', 100. - ema_prec1[0], labeled_minibatch_size)\n        meters.update(\'ema_top5\', ema_prec5[0], labeled_minibatch_size)\n        meters.update(\'ema_error5\', 100. - ema_prec5[0], labeled_minibatch_size)\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        global_step += 1\n        update_ema_variables(model, ema_model, args.ema_decay, global_step)\n\n        # measure elapsed time\n        meters.update(\'batch_time\', time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            LOG.info(\n                \'Epoch: [{0}][{1}/{2}]\\t\'\n                \'Time {meters[batch_time]:.3f}\\t\'\n                \'Data {meters[data_time]:.3f}\\t\'\n                \'Class {meters[class_loss]:.4f}\\t\'\n                \'Cons {meters[cons_loss]:.4f}\\t\'\n                \'Prec@1 {meters[top1]:.3f}\\t\'\n                \'Prec@5 {meters[top5]:.3f}\'.format(\n                    epoch, i, len(train_loader), meters=meters))\n            log.record(epoch + i / len(train_loader), {\n                \'step\': global_step,\n                **meters.values(),\n                **meters.averages(),\n                **meters.sums()\n            })\n\n\ndef validate(eval_loader, model, log, global_step, epoch):\n    class_criterion = nn.CrossEntropyLoss(size_average=False, ignore_index=NO_LABEL).cuda()\n    meters = AverageMeterSet()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    for i, (input, target) in enumerate(eval_loader):\n        meters.update(\'data_time\', time.time() - end)\n\n        input_var = torch.autograd.Variable(input, volatile=True)\n        target_var = torch.autograd.Variable(target.cuda(async=True), volatile=True)\n\n        minibatch_size = len(target_var)\n        labeled_minibatch_size = target_var.data.ne(NO_LABEL).sum()\n        assert labeled_minibatch_size > 0\n        meters.update(\'labeled_minibatch_size\', labeled_minibatch_size)\n\n        # compute output\n        output1, output2 = model(input_var)\n        softmax1, softmax2 = F.softmax(output1, dim=1), F.softmax(output2, dim=1)\n        class_loss = class_criterion(output1, target_var) / minibatch_size\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output1.data, target_var.data, topk=(1, 5))\n        meters.update(\'class_loss\', class_loss.data[0], labeled_minibatch_size)\n        meters.update(\'top1\', prec1[0], labeled_minibatch_size)\n        meters.update(\'error1\', 100.0 - prec1[0], labeled_minibatch_size)\n        meters.update(\'top5\', prec5[0], labeled_minibatch_size)\n        meters.update(\'error5\', 100.0 - prec5[0], labeled_minibatch_size)\n\n        # measure elapsed time\n        meters.update(\'batch_time\', time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            LOG.info(\n                \'Test: [{0}/{1}]\\t\'\n                \'Time {meters[batch_time]:.3f}\\t\'\n                \'Data {meters[data_time]:.3f}\\t\'\n                \'Class {meters[class_loss]:.4f}\\t\'\n                \'Prec@1 {meters[top1]:.3f}\\t\'\n                \'Prec@5 {meters[top5]:.3f}\'.format(\n                    i, len(eval_loader), meters=meters))\n\n    LOG.info(\' * Prec@1 {top1.avg:.3f}\\tPrec@5 {top5.avg:.3f}\'\n          .format(top1=meters[\'top1\'], top5=meters[\'top5\']))\n    log.record(epoch, {\n        \'step\': global_step,\n        **meters.values(),\n        **meters.averages(),\n        **meters.sums()\n    })\n\n    return meters[\'top1\'].avg\n\n\ndef save_checkpoint(state, is_best, dirpath, epoch):\n    filename = \'checkpoint.{}.ckpt\'.format(epoch)\n    checkpoint_path = os.path.join(dirpath, filename)\n    best_path = os.path.join(dirpath, \'best.ckpt\')\n    torch.save(state, checkpoint_path)\n    LOG.info(""--- checkpoint saved to %s ---"" % checkpoint_path)\n    if is_best:\n        shutil.copyfile(checkpoint_path, best_path)\n        LOG.info(""--- checkpoint copied to %s ---"" % best_path)\n\n\ndef adjust_learning_rate(optimizer, epoch, step_in_epoch, total_steps_in_epoch):\n    lr = args.lr\n    epoch = epoch + step_in_epoch / total_steps_in_epoch\n\n    # LR warm-up to handle large minibatch sizes from https://arxiv.org/abs/1706.02677\n    lr = ramps.linear_rampup(epoch, args.lr_rampup) * (args.lr - args.initial_lr) + args.initial_lr\n\n    # Cosine LR rampdown from https://arxiv.org/abs/1608.03983 (but one cycle only)\n    if args.lr_rampdown_epochs:\n        assert args.lr_rampdown_epochs >= args.epochs\n        lr *= ramps.cosine_rampdown(epoch, args.lr_rampdown_epochs)\n\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef get_current_consistency_weight(epoch):\n    # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n    return args.consistency * ramps.sigmoid_rampup(epoch, args.consistency_rampup)\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    labeled_minibatch_size = max(target.ne(NO_LABEL).sum(), 1e-8)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / labeled_minibatch_size))\n    return res\n\n\nif __name__ == \'__main__\':\n    logging.basicConfig(level=logging.INFO)\n    args = cli.parse_commandline_args()\n    main(RunContext(__file__, 0))\n'"
tensorflow/train_cifar10.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""Train ConvNet Mean Teacher on CIFAR-10 training set and evaluate against a validation set\n\nThis runner converges quickly to a fairly good accuracy.\nOn the other hand, the runner experiments/cifar10_final_eval.py\ncontains the hyperparameters used in the paper, and converges\nmuch more slowly but possibly to a slightly better accuracy.\n""""""\n\nimport logging\nfrom datetime import datetime\n\nfrom experiments.run_context import RunContext\nfrom datasets import Cifar10ZCA\nfrom mean_teacher.model import Model\nfrom mean_teacher import minibatching\n\n\nlogging.basicConfig(level=logging.INFO)\nLOG = logging.getLogger(\'main\')\n\n\ndef run(data_seed=0):\n    n_labeled = 4000\n\n    model = Model(RunContext(__file__, 0))\n    model[\'flip_horizontally\'] = True\n    model[\'normalize_input\'] = False  # Keep ZCA information\n    model[\'rampdown_length\'] = 0\n    model[\'rampup_length\'] = 5000\n    model[\'training_length\'] = 40000\n    model[\'max_consistency_cost\'] = 50.0\n\n    tensorboard_dir = model.save_tensorboard_graph()\n    LOG.info(""Saved tensorboard graph to %r"", tensorboard_dir)\n\n    cifar = Cifar10ZCA(data_seed, n_labeled)\n    training_batches = minibatching.training_batches(cifar.training, n_labeled_per_batch=50)\n    evaluation_batches_fn = minibatching.evaluation_epoch_generator(cifar.evaluation)\n\n    model.train(training_batches, evaluation_batches_fn)\n\n\nif __name__ == ""__main__"":\n    run()\n'"
tensorflow/train_svhn.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""Train ConvNet Mean Teacher on SVHN training set and evaluate against a validation set\n\nThis runner converges quickly to a fairly good accuracy.\nOn the other hand, the runner experiments/svhn_final_eval.py\ncontains the hyperparameters used in the paper, and converges\nmuch more slowly but possibly to a slightly better accuracy.\n""""""\n\nimport logging\nfrom datetime import datetime\n\nfrom experiments.run_context import RunContext\nfrom datasets import SVHN\nfrom mean_teacher.model import Model\nfrom mean_teacher import minibatching\n\n\nlogging.basicConfig(level=logging.INFO)\nLOG = logging.getLogger(\'main\')\n\n\ndef run(data_seed=0):\n    n_labeled = 500\n    n_extra_unlabeled = 0\n\n    model = Model(RunContext(__file__, 0))\n    model[\'rampdown_length\'] = 0\n    model[\'rampup_length\'] = 5000\n    model[\'training_length\'] = 40000\n    model[\'max_consistency_cost\'] = 50.0\n\n    tensorboard_dir = model.save_tensorboard_graph()\n    LOG.info(""Saved tensorboard graph to %r"", tensorboard_dir)\n\n    svhn = SVHN(data_seed, n_labeled, n_extra_unlabeled)\n    training_batches = minibatching.training_batches(svhn.training, n_labeled_per_batch=50)\n    evaluation_batches_fn = minibatching.evaluation_epoch_generator(svhn.evaluation)\n\n    model.train(training_batches, evaluation_batches_fn)\n\n\nif __name__ == ""__main__"":\n    run()\n'"
pytorch/experiments/__init__.py,0,b''
pytorch/experiments/cifar10_test.py,1,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""Train CIFAR-10 with 1000 or 4000 labels and all training images. Evaluate against test set.""""""\n\nimport sys\nimport logging\n\nimport torch\n\nimport main\nfrom mean_teacher.cli import parse_dict_args\nfrom mean_teacher.run_context import RunContext\n\n\nLOG = logging.getLogger(\'runner\')\n\n\ndef parameters():\n    defaults = {\n        # Technical details\n        \'workers\': 2,\n        \'checkpoint_epochs\': 20,\n\n        # Data\n        \'dataset\': \'cifar10\',\n        \'train_subdir\': \'train+val\',\n        \'eval_subdir\': \'test\',\n\n        # Data sampling\n        \'base_batch_size\': 128,\n        \'base_labeled_batch_size\': 31,\n\n        # Architecture\n        \'arch\': \'cifar_shakeshake26\',\n\n        # Costs\n        \'consistency_type\': \'mse\',\n        \'consistency_rampup\': 5,\n        \'consistency\': 100.0,\n        \'logit_distance_cost\': .01,\n        \'weight_decay\': 2e-4,\n\n        # Optimization\n        \'lr_rampup\': 0,\n        \'base_lr\': 0.05,\n        \'nesterov\': True,\n    }\n\n    # 4000 labels:\n    for data_seed in range(10, 20):\n        yield {\n            **defaults,\n            \'title\': \'4000-label cifar-10\',\n            \'n_labels\': 4000,\n            \'data_seed\': data_seed,\n            \'epochs\': 300,\n            \'lr_rampdown_epochs\': 350,\n            \'ema_decay\': 0.99,\n        }\n\n    # 1000 labels:\n    for data_seed in range(10, 20):\n        yield {\n            **defaults,\n            \'title\': \'1000-label cifar-10\',\n            \'n_labels\': 1000,\n            \'data_seed\': data_seed,\n            \'epochs\': 180,\n            \'lr_rampdown_epochs\': 210,\n            \'ema_decay\': 0.97,\n        }\n\n\ndef run(title, base_batch_size, base_labeled_batch_size, base_lr, n_labels, data_seed, **kwargs):\n    LOG.info(\'run title: %s, data seed: %d\', title, data_seed)\n\n    ngpu = torch.cuda.device_count()\n    assert ngpu > 0, ""Expecting at least one GPU, found none.""\n\n    adapted_args = {\n        \'batch_size\': base_batch_size * ngpu,\n        \'labeled_batch_size\': base_labeled_batch_size * ngpu,\n        \'lr\': base_lr * ngpu,\n        \'labels\': \'data-local/labels/cifar10/{}_balanced_labels/{:02d}.txt\'.format(n_labels, data_seed),\n    }\n    context = RunContext(__file__, ""{}_{}"".format(n_labels, data_seed))\n    main.args = parse_dict_args(**adapted_args, **kwargs)\n    main.main(context)\n\n\nif __name__ == ""__main__"":\n    for run_params in parameters():\n        run(**run_params)\n'"
pytorch/experiments/imagenet_valid.py,1,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""Train ImageNet with 10% of the labels and evaluate against the validation set""""""\n\nimport sys\nimport logging\n\nimport torch\n\nimport main\nfrom mean_teacher.cli import parse_dict_args\nfrom mean_teacher.run_context import RunContext\n\n\nLOG = logging.getLogger(\'runner\')\n\n\ndef parameters():\n    defaults = {\n        # Technical details\n        \'workers\': 20,\n        \'checkpoint_epochs\': 1,\n        \'evaluation_epochs\': 1,\n\n        # Data\n        \'dataset\': \'imagenet\',\n        \'exclude_unlabeled\': False,\n\n        # Data sampling\n        \'base_batch_size\': 40,\n        \'base_labeled_batch_size\': 20,\n\n        # Architecture\n        \'arch\': \'resnext152\',\n        \'ema_decay\': .9997,\n\n        # Costs\n        \'consistency_type\': \'kl\',\n        \'consistency\': 10.0,\n        \'consistency_rampup\': 5,\n        \'logit_distance_cost\': 0.01,\n        \'weight_decay\': 5e-5,\n\n        # Optimization\n        \'epochs\': 60,\n        \'lr_rampdown_epochs\': 75,\n        \'lr_rampup\': 2,\n        \'initial_lr\': 0.1,\n        \'base_lr\': 0.025,\n        \'nesterov\': True,\n    }\n\n    for data_seed in range(10, 12):\n        yield {\n            **defaults,\n            \'title\': \'mean teacher r-152 eval\',\n            \'data_seed\': 0\n        }\n\n\ndef run(title, base_batch_size, base_labeled_batch_size, base_lr, data_seed, **kwargs):\n    LOG.info(\'run title: %s\', title)\n    ngpu = torch.cuda.device_count()\n    adapted_args = {\n        \'batch_size\': base_batch_size * ngpu,\n        \'labeled_batch_size\': base_labeled_batch_size * ngpu,\n        \'lr\': base_lr * ngpu,\n        \'labels\': \'data-local/labels/ilsvrc2012/128000_balanced_labels/{:02d}.txt\'.format(data_seed),\n    }\n    context = RunContext(__file__, data_seed)\n    main.args = parse_dict_args(**adapted_args, **kwargs)\n    main.main(context)\n\n\nif __name__ == ""__main__"":\n    for run_params in parameters():\n        run(**run_params)\n'"
pytorch/mean_teacher/__init__.py,0,b''
pytorch/mean_teacher/architectures.py,3,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nimport sys\nimport math\nimport itertools\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable, Function\n\nfrom .utils import export, parameter_count\n\n\n@export\ndef cifar_shakeshake26(pretrained=False, **kwargs):\n    assert not pretrained\n    model = ResNet32x32(ShakeShakeBlock,\n                        layers=[4, 4, 4],\n                        channels=96,\n                        downsample=\'shift_conv\', **kwargs)\n    return model\n\n\n@export\ndef resnext152(pretrained=False, **kwargs):\n    assert not pretrained\n    model = ResNet224x224(BottleneckBlock,\n                          layers=[3, 8, 36, 3],\n                          channels=32 * 4,\n                          groups=32,\n                          downsample=\'basic\', **kwargs)\n    return model\n\n\n\nclass ResNet224x224(nn.Module):\n    def __init__(self, block, layers, channels, groups=1, num_classes=1000, downsample=\'basic\'):\n        super().__init__()\n        assert len(layers) == 4\n        self.downsample_mode = downsample\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, channels, groups, layers[0])\n        self.layer2 = self._make_layer(\n            block, channels * 2, groups, layers[1], stride=2)\n        self.layer3 = self._make_layer(\n            block, channels * 4, groups, layers[2], stride=2)\n        self.layer4 = self._make_layer(\n            block, channels * 8, groups, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7)\n        self.fc1 = nn.Linear(block.out_channels(\n            channels * 8, groups), num_classes)\n        self.fc2 = nn.Linear(block.out_channels(\n            channels * 8, groups), num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, groups, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != block.out_channels(planes, groups):\n            if self.downsample_mode == \'basic\' or stride == 1:\n                downsample = nn.Sequential(\n                    nn.Conv2d(self.inplanes, block.out_channels(planes, groups),\n                              kernel_size=1, stride=stride, bias=False),\n                    nn.BatchNorm2d(block.out_channels(planes, groups)),\n                )\n            elif self.downsample_mode == \'shift_conv\':\n                downsample = ShiftConvDownsample(in_channels=self.inplanes,\n                                                 out_channels=block.out_channels(planes, groups))\n            else:\n                assert False\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, stride, downsample))\n        self.inplanes = block.out_channels(planes, groups)\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        return self.fc1(x), self.fc2(x)\n\n\nclass ResNet32x32(nn.Module):\n    def __init__(self, block, layers, channels, groups=1, num_classes=1000, downsample=\'basic\'):\n        super().__init__()\n        assert len(layers) == 3\n        self.downsample_mode = downsample\n        self.inplanes = 16\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.layer1 = self._make_layer(block, channels, groups, layers[0])\n        self.layer2 = self._make_layer(\n            block, channels * 2, groups, layers[1], stride=2)\n        self.layer3 = self._make_layer(\n            block, channels * 4, groups, layers[2], stride=2)\n        self.avgpool = nn.AvgPool2d(8)\n        self.fc1 = nn.Linear(block.out_channels(\n            channels * 4, groups), num_classes)\n        self.fc2 = nn.Linear(block.out_channels(\n            channels * 4, groups), num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, groups, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != block.out_channels(planes, groups):\n            if self.downsample_mode == \'basic\' or stride == 1:\n                downsample = nn.Sequential(\n                    nn.Conv2d(self.inplanes, block.out_channels(planes, groups),\n                              kernel_size=1, stride=stride, bias=False),\n                    nn.BatchNorm2d(block.out_channels(planes, groups)),\n                )\n            elif self.downsample_mode == \'shift_conv\':\n                downsample = ShiftConvDownsample(in_channels=self.inplanes,\n                                                 out_channels=block.out_channels(planes, groups))\n            else:\n                assert False\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, stride, downsample))\n        self.inplanes = block.out_channels(planes, groups)\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        return self.fc1(x), self.fc2(x)\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BottleneckBlock(nn.Module):\n    @classmethod\n    def out_channels(cls, planes, groups):\n        if groups > 1:\n            return 2 * planes\n        else:\n            return 4 * planes\n\n    def __init__(self, inplanes, planes, groups, stride=1, downsample=None):\n        super().__init__()\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv_a1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn_a1 = nn.BatchNorm2d(planes)\n        self.conv_a2 = nn.Conv2d(\n            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False, groups=groups)\n        self.bn_a2 = nn.BatchNorm2d(planes)\n        self.conv_a3 = nn.Conv2d(planes, self.out_channels(\n            planes, groups), kernel_size=1, bias=False)\n        self.bn_a3 = nn.BatchNorm2d(self.out_channels(planes, groups))\n\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        a, residual = x, x\n\n        a = self.conv_a1(a)\n        a = self.bn_a1(a)\n        a = self.relu(a)\n        a = self.conv_a2(a)\n        a = self.bn_a2(a)\n        a = self.relu(a)\n        a = self.conv_a3(a)\n        a = self.bn_a3(a)\n\n        if self.downsample is not None:\n            residual = self.downsample(residual)\n\n        return self.relu(residual + a)\n\n\nclass ShakeShakeBlock(nn.Module):\n    @classmethod\n    def out_channels(cls, planes, groups):\n        assert groups == 1\n        return planes\n\n    def __init__(self, inplanes, planes, groups, stride=1, downsample=None):\n        super().__init__()\n        assert groups == 1\n        self.conv_a1 = conv3x3(inplanes, planes, stride)\n        self.bn_a1 = nn.BatchNorm2d(planes)\n        self.conv_a2 = conv3x3(planes, planes)\n        self.bn_a2 = nn.BatchNorm2d(planes)\n\n        self.conv_b1 = conv3x3(inplanes, planes, stride)\n        self.bn_b1 = nn.BatchNorm2d(planes)\n        self.conv_b2 = conv3x3(planes, planes)\n        self.bn_b2 = nn.BatchNorm2d(planes)\n\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        a, b, residual = x, x, x\n\n        a = F.relu(a, inplace=False)\n        a = self.conv_a1(a)\n        a = self.bn_a1(a)\n        a = F.relu(a, inplace=True)\n        a = self.conv_a2(a)\n        a = self.bn_a2(a)\n\n        b = F.relu(b, inplace=False)\n        b = self.conv_b1(b)\n        b = self.bn_b1(b)\n        b = F.relu(b, inplace=True)\n        b = self.conv_b2(b)\n        b = self.bn_b2(b)\n\n        ab = shake(a, b, training=self.training)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        return residual + ab\n\n\nclass Shake(Function):\n    @classmethod\n    def forward(cls, ctx, inp1, inp2, training):\n        assert inp1.size() == inp2.size()\n        gate_size = [inp1.size()[0], *itertools.repeat(1, inp1.dim() - 1)]\n        gate = inp1.new(*gate_size)\n        if training:\n            gate.uniform_(0, 1)\n        else:\n            gate.fill_(0.5)\n        return inp1 * gate + inp2 * (1. - gate)\n\n    @classmethod\n    def backward(cls, ctx, grad_output):\n        grad_inp1 = grad_inp2 = grad_training = None\n        gate_size = [grad_output.size()[0], *itertools.repeat(1,\n                                                              grad_output.dim() - 1)]\n        gate = Variable(grad_output.data.new(*gate_size).uniform_(0, 1))\n        if ctx.needs_input_grad[0]:\n            grad_inp1 = grad_output * gate\n        if ctx.needs_input_grad[1]:\n            grad_inp2 = grad_output * (1 - gate)\n        assert not ctx.needs_input_grad[2]\n        return grad_inp1, grad_inp2, grad_training\n\n\ndef shake(inp1, inp2, training=False):\n    return Shake.apply(inp1, inp2, training)\n\n\nclass ShiftConvDownsample(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.conv = nn.Conv2d(in_channels=2 * in_channels,\n                              out_channels=out_channels,\n                              kernel_size=1,\n                              groups=2)\n        self.bn = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        x = torch.cat((x[:, :, 0::2, 0::2],\n                       x[:, :, 1::2, 1::2]), dim=1)\n        x = self.relu(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n'"
pytorch/mean_teacher/cli.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.`\n\nimport re\nimport argparse\nimport logging\n\nfrom . import architectures, datasets\n\n\nLOG = logging.getLogger(\'main\')\n\n__all__ = [\'parse_cmd_args\', \'parse_dict_args\']\n\n\ndef create_parser():\n    parser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\n    parser.add_argument(\'--dataset\', metavar=\'DATASET\', default=\'imagenet\',\n                        choices=datasets.__all__,\n                        help=\'dataset: \' +\n                            \' | \'.join(datasets.__all__) +\n                            \' (default: imagenet)\')\n    parser.add_argument(\'--train-subdir\', type=str, default=\'train\',\n                        help=\'the subdirectory inside the data directory that contains the training data\')\n    parser.add_argument(\'--eval-subdir\', type=str, default=\'val\',\n                        help=\'the subdirectory inside the data directory that contains the evaluation data\')\n    parser.add_argument(\'--labels\', default=None, type=str, metavar=\'FILE\',\n                        help=\'list of image labels (default: based on directory structure)\')\n    parser.add_argument(\'--exclude-unlabeled\', default=False, type=str2bool, metavar=\'BOOL\',\n                        help=\'exclude unlabeled examples from the training set\')\n    parser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'resnet18\',\n                        choices=architectures.__all__,\n                        help=\'model architecture: \' +\n                            \' | \'.join(architectures.__all__))\n    parser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                        help=\'number of data loading workers (default: 4)\')\n    parser.add_argument(\'--epochs\', default=90, type=int, metavar=\'N\',\n                        help=\'number of total epochs to run\')\n    parser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                        help=\'manual epoch number (useful on restarts)\')\n    parser.add_argument(\'-b\', \'--batch-size\', default=256, type=int,\n                        metavar=\'N\', help=\'mini-batch size (default: 256)\')\n    parser.add_argument(\'--labeled-batch-size\', default=None, type=int,\n                        metavar=\'N\', help=""labeled examples per minibatch (default: no constrain)"")\n    parser.add_argument(\'--lr\', \'--learning-rate\', default=0.1, type=float,\n                        metavar=\'LR\', help=\'max learning rate\')\n    parser.add_argument(\'--initial-lr\', default=0.0, type=float,\n                        metavar=\'LR\', help=\'initial learning rate when using linear rampup\')\n    parser.add_argument(\'--lr-rampup\', default=0, type=int, metavar=\'EPOCHS\',\n                        help=\'length of learning rate rampup in the beginning\')\n    parser.add_argument(\'--lr-rampdown-epochs\', default=None, type=int, metavar=\'EPOCHS\',\n                        help=\'length of learning rate cosine rampdown (>= length of training)\')\n    parser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                        help=\'momentum\')\n    parser.add_argument(\'--nesterov\', default=False, type=str2bool,\n                        help=\'use nesterov momentum\', metavar=\'BOOL\')\n    parser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,\n                        metavar=\'W\', help=\'weight decay (default: 1e-4)\')\n    parser.add_argument(\'--ema-decay\', default=0.999, type=float, metavar=\'ALPHA\',\n                        help=\'ema variable decay rate (default: 0.999)\')\n    parser.add_argument(\'--consistency\', default=None, type=float, metavar=\'WEIGHT\',\n                        help=\'use consistency loss with given weight (default: None)\')\n    parser.add_argument(\'--consistency-type\', default=""mse"", type=str, metavar=\'TYPE\',\n                        choices=[\'mse\', \'kl\'],\n                        help=\'consistency loss type to use\')\n    parser.add_argument(\'--consistency-rampup\', default=30, type=int, metavar=\'EPOCHS\',\n                        help=\'length of the consistency loss ramp-up\')\n    parser.add_argument(\'--logit-distance-cost\', default=-1, type=float, metavar=\'WEIGHT\',\n                        help=\'let the student model have two outputs and use an MSE loss between the logits with the given weight (default: only have one output)\')\n    parser.add_argument(\'--checkpoint-epochs\', default=1, type=int,\n                        metavar=\'EPOCHS\', help=\'checkpoint frequency in epochs, 0 to turn checkpointing off (default: 1)\')\n    parser.add_argument(\'--evaluation-epochs\', default=1, type=int,\n                        metavar=\'EPOCHS\', help=\'evaluation frequency in epochs, 0 to turn evaluation off (default: 1)\')\n    parser.add_argument(\'--print-freq\', \'-p\', default=10, type=int,\n                        metavar=\'N\', help=\'print frequency (default: 10)\')\n    parser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                        help=\'path to latest checkpoint (default: none)\')\n    parser.add_argument(\'-e\', \'--evaluate\', type=str2bool,\n                        help=\'evaluate model on evaluation set\')\n    parser.add_argument(\'--pretrained\', dest=\'pretrained\', action=\'store_true\',\n                        help=\'use pre-trained model\')\n    return parser\n\n\ndef parse_commandline_args():\n    return create_parser().parse_args()\n\n\ndef parse_dict_args(**kwargs):\n    def to_cmdline_kwarg(key, value):\n        if len(key) == 1:\n            key = ""-{}"".format(key)\n        else:\n            key = ""--{}"".format(re.sub(r""_"", ""-"", key))\n        value = str(value)\n        return key, value\n\n    kwargs_pairs = (to_cmdline_kwarg(key, value)\n                    for key, value in kwargs.items())\n    cmdline_args = list(sum(kwargs_pairs, ()))\n\n    LOG.info(""Using these command line args: %s"", "" "".join(cmdline_args))\n\n    return create_parser().parse_args(cmdline_args)\n\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\n\ndef str2epochs(v):\n    try:\n        if len(v) == 0:\n            epochs = []\n        else:\n            epochs = [int(string) for string in v.split("","")]\n    except:\n        raise argparse.ArgumentTypeError(\n            \'Expected comma-separated list of integers, got ""{}""\'.format(v))\n    if not all(0 < epoch1 < epoch2 for epoch1, epoch2 in zip(epochs[:-1], epochs[1:])):\n        raise argparse.ArgumentTypeError(\n            \'Expected the epochs to be listed in increasing order\')\n    return epochs\n'"
pytorch/mean_teacher/data.py,1,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""Functions to load data from folders and augment it""""""\n\nimport itertools\nimport logging\nimport os.path\n\nfrom PIL import Image\nimport numpy as np\nfrom torch.utils.data.sampler import Sampler\n\n\nLOG = logging.getLogger(\'main\')\nNO_LABEL = -1\n\n\n\n\nclass RandomTranslateWithReflect:\n    """"""Translate image randomly\n\n    Translate vertically and horizontally by n pixels where\n    n is integer drawn uniformly independently for each axis\n    from [-max_translation, max_translation].\n\n    Fill the uncovered blank area with reflect padding.\n    """"""\n\n    def __init__(self, max_translation):\n        self.max_translation = max_translation\n\n    def __call__(self, old_image):\n        xtranslation, ytranslation = np.random.randint(-self.max_translation,\n                                                       self.max_translation + 1,\n                                                       size=2)\n        xpad, ypad = abs(xtranslation), abs(ytranslation)\n        xsize, ysize = old_image.size\n\n        flipped_lr = old_image.transpose(Image.FLIP_LEFT_RIGHT)\n        flipped_tb = old_image.transpose(Image.FLIP_TOP_BOTTOM)\n        flipped_both = old_image.transpose(Image.ROTATE_180)\n\n        new_image = Image.new(""RGB"", (xsize + 2 * xpad, ysize + 2 * ypad))\n\n        new_image.paste(old_image, (xpad, ypad))\n\n        new_image.paste(flipped_lr, (xpad + xsize - 1, ypad))\n        new_image.paste(flipped_lr, (xpad - xsize + 1, ypad))\n\n        new_image.paste(flipped_tb, (xpad, ypad + ysize - 1))\n        new_image.paste(flipped_tb, (xpad, ypad - ysize + 1))\n\n        new_image.paste(flipped_both, (xpad - xsize + 1, ypad - ysize + 1))\n        new_image.paste(flipped_both, (xpad + xsize - 1, ypad - ysize + 1))\n        new_image.paste(flipped_both, (xpad - xsize + 1, ypad + ysize - 1))\n        new_image.paste(flipped_both, (xpad + xsize - 1, ypad + ysize - 1))\n\n        new_image = new_image.crop((xpad - xtranslation,\n                                    ypad - ytranslation,\n                                    xpad + xsize - xtranslation,\n                                    ypad + ysize - ytranslation))\n\n        return new_image\n\n\nclass TransformTwice:\n    def __init__(self, transform):\n        self.transform = transform\n\n    def __call__(self, inp):\n        out1 = self.transform(inp)\n        out2 = self.transform(inp)\n        return out1, out2\n\n\ndef relabel_dataset(dataset, labels):\n    unlabeled_idxs = []\n    for idx in range(len(dataset.imgs)):\n        path, _ = dataset.imgs[idx]\n        filename = os.path.basename(path)\n        if filename in labels:\n            label_idx = dataset.class_to_idx[labels[filename]]\n            dataset.imgs[idx] = path, label_idx\n            del labels[filename]\n        else:\n            dataset.imgs[idx] = path, NO_LABEL\n            unlabeled_idxs.append(idx)\n\n    if len(labels) != 0:\n        message = ""List of unlabeled contains {} unknown files: {}, ...""\n        some_missing = \', \'.join(list(labels.keys())[:5])\n        raise LookupError(message.format(len(labels), some_missing))\n\n    labeled_idxs = sorted(set(range(len(dataset.imgs))) - set(unlabeled_idxs))\n\n    return labeled_idxs, unlabeled_idxs\n\n\nclass TwoStreamBatchSampler(Sampler):\n    """"""Iterate two sets of indices\n\n    An \'epoch\' is one iteration through the primary indices.\n    During the epoch, the secondary indices are iterated through\n    as many times as needed.\n    """"""\n    def __init__(self, primary_indices, secondary_indices, batch_size, secondary_batch_size):\n        self.primary_indices = primary_indices\n        self.secondary_indices = secondary_indices\n        self.secondary_batch_size = secondary_batch_size\n        self.primary_batch_size = batch_size - secondary_batch_size\n\n        assert len(self.primary_indices) >= self.primary_batch_size > 0\n        assert len(self.secondary_indices) >= self.secondary_batch_size > 0\n\n    def __iter__(self):\n        primary_iter = iterate_once(self.primary_indices)\n        secondary_iter = iterate_eternally(self.secondary_indices)\n        return (\n            primary_batch + secondary_batch\n            for (primary_batch, secondary_batch)\n            in  zip(grouper(primary_iter, self.primary_batch_size),\n                    grouper(secondary_iter, self.secondary_batch_size))\n        )\n\n    def __len__(self):\n        return len(self.primary_indices) // self.primary_batch_size\n\n\ndef iterate_once(iterable):\n    return np.random.permutation(iterable)\n\n\ndef iterate_eternally(indices):\n    def infinite_shuffles():\n        while True:\n            yield np.random.permutation(indices)\n    return itertools.chain.from_iterable(infinite_shuffles())\n\n\ndef grouper(iterable, n):\n    ""Collect data into fixed-length chunks or blocks""\n    # grouper(\'ABCDEFG\', 3) --> ABC DEF""\n    args = [iter(iterable)] * n\n    return zip(*args)\n'"
pytorch/mean_teacher/datasets.py,0,"b""# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nimport torchvision.transforms as transforms\n\nfrom . import data\nfrom .utils import export\n\n\n@export\ndef imagenet():\n    channel_stats = dict(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n    train_transformation = data.TransformTwice(transforms.Compose([\n        transforms.RandomRotation(10),\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n        transforms.ToTensor(),\n        transforms.Normalize(**channel_stats)\n    ]))\n    eval_transformation = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(**channel_stats)\n    ])\n\n    return {\n        'train_transformation': train_transformation,\n        'eval_transformation': eval_transformation,\n        'datadir': 'data-local/images/ilsvrc2012/',\n        'num_classes': 1000\n    }\n\n\n@export\ndef cifar10():\n    channel_stats = dict(mean=[0.4914, 0.4822, 0.4465],\n                         std=[0.2470,  0.2435,  0.2616])\n    train_transformation = data.TransformTwice(transforms.Compose([\n        data.RandomTranslateWithReflect(4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(**channel_stats)\n    ]))\n    eval_transformation = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(**channel_stats)\n    ])\n\n    return {\n        'train_transformation': train_transformation,\n        'eval_transformation': eval_transformation,\n        'datadir': 'data-local/images/cifar/cifar10/by-image',\n        'num_classes': 10\n    }\n"""
pytorch/mean_teacher/losses.py,3,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""Custom loss functions""""""\n\nimport torch\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable\n\n\ndef softmax_mse_loss(input_logits, target_logits):\n    """"""Takes softmax on both sides and returns MSE loss\n\n    Note:\n    - Returns the sum over all examples. Divide by the batch size afterwards\n      if you want the mean.\n    - Sends gradients to inputs but not the targets.\n    """"""\n    assert input_logits.size() == target_logits.size()\n    input_softmax = F.softmax(input_logits, dim=1)\n    target_softmax = F.softmax(target_logits, dim=1)\n    num_classes = input_logits.size()[1]\n    return F.mse_loss(input_softmax, target_softmax, size_average=False) / num_classes\n\n\ndef softmax_kl_loss(input_logits, target_logits):\n    """"""Takes softmax on both sides and returns KL divergence\n\n    Note:\n    - Returns the sum over all examples. Divide by the batch size afterwards\n      if you want the mean.\n    - Sends gradients to inputs but not the targets.\n    """"""\n    assert input_logits.size() == target_logits.size()\n    input_log_softmax = F.log_softmax(input_logits, dim=1)\n    target_softmax = F.softmax(target_logits, dim=1)\n    return F.kl_div(input_log_softmax, target_softmax, size_average=False)\n\n\ndef symmetric_mse_loss(input1, input2):\n    """"""Like F.mse_loss but sends gradients to both directions\n\n    Note:\n    - Returns the sum over all examples. Divide by the batch size afterwards\n      if you want the mean.\n    - Sends gradients to both input1 and input2.\n    """"""\n    assert input1.size() == input2.size()\n    num_classes = input1.size()[1]\n    return torch.sum((input1 - input2)**2) / num_classes\n'"
pytorch/mean_teacher/ramps.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""Functions for ramping hyperparameters up or down\n\nEach function takes the current training step or epoch, and the\nramp length in the same format, and returns a multiplier between\n0 and 1.\n""""""\n\n\nimport numpy as np\n\n\ndef sigmoid_rampup(current, rampup_length):\n    """"""Exponential rampup from https://arxiv.org/abs/1610.02242""""""\n    if rampup_length == 0:\n        return 1.0\n    else:\n        current = np.clip(current, 0.0, rampup_length)\n        phase = 1.0 - current / rampup_length\n        return float(np.exp(-5.0 * phase * phase))\n\n\ndef linear_rampup(current, rampup_length):\n    """"""Linear rampup""""""\n    assert current >= 0 and rampup_length >= 0\n    if current >= rampup_length:\n        return 1.0\n    else:\n        return current / rampup_length\n\n\ndef cosine_rampdown(current, rampdown_length):\n    """"""Cosine rampdown from https://arxiv.org/abs/1608.03983""""""\n    assert 0 <= current <= rampdown_length\n    return float(.5 * (np.cos(np.pi * current / rampdown_length) + 1))\n'"
pytorch/mean_teacher/run_context.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nfrom datetime import datetime\nfrom collections import defaultdict\nimport threading\nimport time\nimport logging\nimport os\n\nfrom pandas import DataFrame\nfrom collections import defaultdict\n\n\nclass TrainLog:\n    """"""Saves training logs in Pandas msgpacks""""""\n\n    INCREMENTAL_UPDATE_TIME = 300\n\n    def __init__(self, directory, name):\n        self.log_file_path = ""{}/{}.msgpack"".format(directory, name)\n        self._log = defaultdict(dict)\n        self._log_lock = threading.RLock()\n        self._last_update_time = time.time() - self.INCREMENTAL_UPDATE_TIME\n\n    def record_single(self, step, column, value):\n        self._record(step, {column: value})\n\n    def record(self, step, col_val_dict):\n        self._record(step, col_val_dict)\n\n    def save(self):\n        df = self._as_dataframe()\n        df.to_msgpack(self.log_file_path, compress=\'zlib\')\n\n    def _record(self, step, col_val_dict):\n        with self._log_lock:\n            self._log[step].update(col_val_dict)\n            if time.time() - self._last_update_time >= self.INCREMENTAL_UPDATE_TIME:\n                self._last_update_time = time.time()\n                self.save()\n\n    def _as_dataframe(self):\n        with self._log_lock:\n            return DataFrame.from_dict(self._log, orient=\'index\')\n\n\nclass RunContext:\n    """"""Creates directories and files for the run""""""\n\n    def __init__(self, runner_file, run_idx):\n        logging.basicConfig(level=logging.INFO, format=\'%(message)s\')\n        runner_name = os.path.basename(runner_file).split(""."")[0]\n        self.result_dir = ""{root}/{runner_name}/{date:%Y-%m-%d_%H:%M:%S}/{run_idx}"".format(\n            root=\'results\',\n            runner_name=runner_name,\n            date=datetime.now(),\n            run_idx=run_idx\n        )\n        self.transient_dir = self.result_dir + ""/transient""\n        os.makedirs(self.result_dir)\n        os.makedirs(self.transient_dir)\n\n    def create_train_log(self, name):\n        return TrainLog(self.result_dir, name)\n'"
pytorch/mean_teacher/utils.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""Utility functions and classes""""""\n\nimport sys\n\n\ndef parameters_string(module):\n    lines = [\n        """",\n        ""List of model parameters:"",\n        ""========================="",\n    ]\n\n    row_format = ""{name:<40} {shape:>20} ={total_size:>12,d}""\n    params = list(module.named_parameters())\n    for name, param in params:\n        lines.append(row_format.format(\n            name=name,\n            shape="" * "".join(str(p) for p in param.size()),\n            total_size=param.numel()\n        ))\n    lines.append(""="" * 75)\n    lines.append(row_format.format(\n        name=""all parameters"",\n        shape=""sum of above"",\n        total_size=sum(int(param.numel()) for name, param in params)\n    ))\n    lines.append("""")\n    return ""\\n"".join(lines)\n\n\ndef assert_exactly_one(lst):\n    assert sum(int(bool(el)) for el in lst) == 1, "", "".join(str(el)\n                                                            for el in lst)\n\n\nclass AverageMeterSet:\n    def __init__(self):\n        self.meters = {}\n\n    def __getitem__(self, key):\n        return self.meters[key]\n\n    def update(self, name, value, n=1):\n        if not name in self.meters:\n            self.meters[name] = AverageMeter()\n        self.meters[name].update(value, n)\n\n    def reset(self):\n        for meter in self.meters.values():\n            meter.reset()\n\n    def values(self, postfix=\'\'):\n        return {name + postfix: meter.val for name, meter in self.meters.items()}\n\n    def averages(self, postfix=\'/avg\'):\n        return {name + postfix: meter.avg for name, meter in self.meters.items()}\n\n    def sums(self, postfix=\'/sum\'):\n        return {name + postfix: meter.sum for name, meter in self.meters.items()}\n\n    def counts(self, postfix=\'/count\'):\n        return {name + postfix: meter.count for name, meter in self.meters.items()}\n\n\nclass AverageMeter:\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __format__(self, format):\n        return ""{self.val:{format}} ({self.avg:{format}})"".format(self=self, format=format)\n\n\ndef export(fn):\n    mod = sys.modules[fn.__module__]\n    if hasattr(mod, \'__all__\'):\n        mod.__all__.append(fn.__name__)\n    else:\n        mod.__all__ = [fn.__name__]\n    return fn\n\n\ndef parameter_count(module):\n    return sum(int(param.numel()) for param in module.parameters())\n'"
tensorflow/datasets/__init__.py,0,b'from .svhn import SVHN\nfrom .cifar10 import Cifar10ZCA\n'
tensorflow/datasets/cifar10.py,0,"b""# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nimport os\n\nimport numpy as np\n\nfrom .utils import random_balanced_partitions, random_partitions\n\n\nclass Cifar10ZCA:\n    DATA_PATH = os.path.join('data', 'images', 'cifar', 'cifar10', 'cifar10_gcn_zca_v2.npz')\n    VALIDATION_SET_SIZE = 5000  # 10% of the training set\n    UNLABELED = -1\n\n    def __init__(self, data_seed=0, n_labeled='all', test_phase=False):\n        random = np.random.RandomState(seed=data_seed)\n        self._load()\n\n        if test_phase:\n            self.evaluation, self.training = self._test_and_training()\n        else:\n            self.evaluation, self.training = self._validation_and_training(random)\n\n        if n_labeled != 'all':\n            self.training = self._unlabel(self.training, n_labeled, random)\n\n    def _load(self):\n        file_data = np.load(self.DATA_PATH)\n        self._train_data = self._data_array(50000, file_data['train_x'], file_data['train_y'])\n        self._test_data = self._data_array(10000, file_data['test_x'], file_data['test_y'])\n\n    def _data_array(self, expected_n, x_data, y_data):\n        array = np.zeros(expected_n, dtype=[\n            ('x', np.float32, (32, 32, 3)),\n            ('y', np.int32, ())  # We will be using -1 for unlabeled\n        ])\n        array['x'] = x_data\n        array['y'] = y_data\n        return array\n\n    def _validation_and_training(self, random):\n        return random_partitions(self._train_data, self.VALIDATION_SET_SIZE, random)\n\n    def _test_and_training(self):\n        return self._test_data, self._train_data\n\n    def _unlabel(self, data, n_labeled, random):\n        labeled, unlabeled = random_balanced_partitions(\n            data, n_labeled, labels=data['y'], random=random)\n        unlabeled['y'] = self.UNLABELED\n        return np.concatenate([labeled, unlabeled])\n"""
tensorflow/datasets/preprocess_cifar10.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""\n    Creates static contrast normalized and ZCA-whitened dataset using the following parameters\n    - global contrast normalization using Goodfellow scale factor 55.\n    - ZCA using filter_bias=0.1\n""""""\n\nimport os\nimport numpy as np\nfrom scipy.io import loadmat\n\n\nDIR = os.path.join(\'data\', \'images\', \'cifar\', \'cifar10\')\n\n\ndef assert_not_exists(path):\n    assert not os.path.exists(path), """"\n\n\ndef cifar10_orig_train():\n    return load_batch_files([os.path.join(DIR, ""data_batch_{}.mat"".format(i)) for i in range(1, 6)])\n\n\ndef cifar10_orig_test():\n    return load_batch_file(os.path.join(DIR, ""test_batch.mat""))\n\n\ndef load_batch_files(batch_files):\n    data_batches, label_batches = zip(*[load_batch_file(batch_file) for batch_file in batch_files])\n    x = np.concatenate(data_batches, axis=0)\n    y = np.concatenate(label_batches, axis=0)\n    return x, y\n\n\ndef load_batch_file(path):\n    d = loadmat(path)\n    x = d[\'data\'].astype(np.uint8)\n    y = d[\'labels\'].astype(np.uint8).flatten()\n    return x, y\n\n\ndef to_channel_rgb(x):\n    return np.transpose(np.reshape(x, (x.shape[0], 3, 32, 32)), [0, 2, 3, 1])\n\n\ndef global_contrast_normalize(X, scale=55., min_divisor=1e-8):\n    X = X - X.mean(axis=1)[:, np.newaxis]\n\n    normalizers = np.sqrt((X ** 2).sum(axis=1)) / scale\n    normalizers[normalizers < min_divisor] = 1.\n\n    X /= normalizers[:, np.newaxis]\n\n    return X\n\n\ndef create_zca(imgs, filter_bias=0.1):\n    meanX = np.mean(imgs, axis=0)\n\n    covX = np.cov(imgs.T)\n    D, E = np.linalg.eigh(covX + filter_bias * np.eye(covX.shape[0], covX.shape[1]))\n\n    assert not np.isnan(D).any()\n    assert not np.isnan(E).any()\n    assert D.min() > 0\n\n    D **= -.5\n\n    W = np.dot(E, np.dot(np.diag(D), E.T))\n\n    def transform(images):\n        return np.dot(images - meanX, W)\n\n    return transform\n\n\ndef do():\n    train_x_orig, train_y = cifar10_orig_train()\n    test_x_orig, test_y = cifar10_orig_test()\n\n    train_x_gcn = global_contrast_normalize(train_x_orig)\n    zca = create_zca(train_x_gcn)\n    train_x = to_channel_rgb(zca(train_x_gcn))\n    test_x = to_channel_rgb(zca(global_contrast_normalize(test_x_orig)))\n    p = os.path.join(DIR, ""cifar10_gcn_zca_v2.npz"")\n    assert_not_exists(p)\n    np.savez(p, train_x=train_x, train_y=train_y, test_x=test_x, test_y=test_y)\n\n\nif __name__ == ""__main__"":\n    do()\n'"
tensorflow/datasets/svhn.py,0,"b""# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nimport os\n\nimport numpy as np\nimport scipy.io\n\nfrom .utils import random_balanced_partitions, random_partitions\n\n\nclass Datafile:\n    def __init__(self, path, n_examples):\n        self.path = path\n        self.n_examples = n_examples\n        self._data = None\n\n    @property\n    def data(self):\n        if self._data is None:\n            self._load()\n        return self._data\n\n    def _load(self):\n        data = np.zeros(self.n_examples, dtype=[\n            ('x', np.uint8, (32, 32, 3)),\n            ('y', np.int32, ())  # We will be using -1 for unlabeled\n        ])\n        dictionary = scipy.io.loadmat(self.path)\n        data['x'] = np.transpose(dictionary['X'], [3, 0, 1, 2])\n        data['y'] = dictionary['y'].reshape((-1))\n        data['y'][data['y'] == 10] = 0  # Use label 0 for zeros\n        self._data = data\n\n\nclass SVHN:\n    DIR = os.path.join('data', 'images', 'svhn')\n    FILES = {\n        'train': Datafile(os.path.join(DIR, 'train_32x32.mat'), 73257),\n        'extra': Datafile(os.path.join(DIR, 'extra_32x32.mat'), 531131),\n        'test': Datafile(os.path.join(DIR, 'test_32x32.mat'), 26032),\n    }\n    VALIDATION_SET_SIZE = 7325  # 10% of the training set\n    UNLABELED = -1\n\n    def __init__(self, data_seed=0, n_labeled='all', n_extra_unlabeled=0, test_phase=False):\n        random = np.random.RandomState(seed=data_seed)\n\n        if test_phase:\n            self.evaluation, self.training = self._test_and_training()\n        else:\n            self.evaluation, self.training = self._validation_and_training(random)\n\n        if n_labeled != 'all':\n            self.training = self._unlabel(self.training, n_labeled, random)\n\n        if n_extra_unlabeled > 0:\n            self.training = self._add_extra_unlabeled(self.training, n_extra_unlabeled, random)\n\n    def _validation_and_training(self, random):\n        return random_partitions(self.FILES['train'].data, self.VALIDATION_SET_SIZE, random)\n\n    def _test_and_training(self):\n        return self.FILES['test'].data, self.FILES['train'].data\n\n    def _unlabel(self, data, n_labeled, random):\n        labeled, unlabeled = random_balanced_partitions(\n            data, n_labeled, labels=data['y'], random=random)\n        unlabeled['y'] = self.UNLABELED\n        return np.concatenate([labeled, unlabeled])\n\n    def _add_extra_unlabeled(self, data, n_extra_unlabeled, random):\n        extra_unlabeled, _ = random_partitions(self.FILES['extra'].data, n_extra_unlabeled, random)\n        extra_unlabeled['y'] = self.UNLABELED\n        return np.concatenate([data, extra_unlabeled])\n"""
tensorflow/datasets/utils.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nimport numpy as np\n\n\ndef random_partitions(data, first_size, random):\n    """"""Split data into two random partitions of sizes n and len(data) - n\n\n    Args:\n        data (ndarray): data to be split\n        first_size (int): size of the first partition\n        random (RandomState): source of randomness\n\n    Return:\n        tuple of two ndarrays\n    """"""\n    mask = np.zeros(len(data), dtype=bool)\n    mask[:first_size] = True\n    random.shuffle(mask)\n    return data[mask], data[~mask]\n\n\ndef random_balanced_partitions(data, first_size, labels, random=np.random):\n    """"""Split data into a balanced random partition and the rest\n\n    Partition the `data` array into two random partitions, using\n    the `labels` array (of equal size) to guide the choice of\n    elements of the first returned array.\n\n    Example:\n        random_balanced_partition([\'a\', \'b\', \'c\'], 2, [3, 5, 5])\n        # Both labels 3 and 5 need to be presented once, so\n        # the result can be either ([\'a\', \'b\'], [\'c\']) or\n        # ([\'a\', \'c\'], [\'b\']) but not ([\'b\', \'c\'], [\'a\']).\n\n    Args:\n        data (ndarray): data to be split\n        first_size (int): size of the first partition\n        balance (ndarray): according to which balancing is done\n        random (RandomState): source of randomness\n\n    Return:\n        tuple of two ndarrays\n    """"""\n    assert len(data) == len(labels)\n\n    classes, class_counts = np.unique(labels, return_counts=True)\n    assert len(classes) <= 10000, ""surprisingly many classes: {}"".format(len(classes))\n    assert first_size % len(classes) == 0, ""not divisible: {}/{}"".format(first_size, len(classes))\n    assert np.all(class_counts >= first_size // len(classes)), ""not enough examples of some class""\n\n    idxs_per_class = [np.nonzero(labels == klass)[0] for klass in classes]\n    chosen_idxs_per_class = [\n        random.choice(idxs, first_size // len(classes), replace=False)\n        for idxs in idxs_per_class\n    ]\n    first_idxs = np.concatenate(chosen_idxs_per_class)\n    second_idxs = np.setdiff1d(np.arange(len(labels)), first_idxs)\n\n    assert first_idxs.shape == (first_size,)\n    assert second_idxs.shape == (len(data) - first_size,)\n    return data[first_idxs], data[second_idxs]\n'"
tensorflow/experiments/__init__.py,0,b''
tensorflow/experiments/cifar10_final_eval.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""CIFAR-10 final evaluation""""""\n\nimport logging\nimport sys\n\nfrom experiments.run_context import RunContext\nimport tensorflow as tf\n\nfrom datasets import Cifar10ZCA\nfrom mean_teacher.model import Model\nfrom mean_teacher import minibatching\n\n\nLOG = logging.getLogger(\'main\')\n\n\ndef parameters():\n    test_phase = True\n    for n_labeled in [1000, 2000, 4000, \'all\']:\n        for model_type in [\'mean_teacher\', \'pi\']:\n            if n_labeled == \'all\':\n                n_runs = 4\n            else:\n                n_runs = 10\n            for data_seed in range(2000, 2000 + n_runs):\n                yield {\n                    \'test_phase\': test_phase,\n                    \'model_type\': model_type,\n                    \'n_labeled\': n_labeled,\n                    \'data_seed\': data_seed\n                }\n\n\ndef model_hyperparameters(model_type, n_labeled):\n    assert model_type in [\'mean_teacher\', \'pi\']\n    if n_labeled == \'all\':\n        return {\n            \'n_labeled_per_batch\': 100,\n            \'max_consistency_cost\': 100.0,\n            \'apply_consistency_to_labeled\': True,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    elif isinstance(n_labeled, int):\n        return {\n            \'n_labeled_per_batch\': \'vary\',\n            \'max_consistency_cost\': 100.0 * n_labeled / 50000,\n            \'apply_consistency_to_labeled\': True,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    else:\n        msg = ""Unexpected combination: {model_type}, {n_labeled}""\n        assert False, msg.format(locals())\n\n\ndef run(test_phase, n_labeled, data_seed, model_type):\n    minibatch_size = 100\n    hyperparams = model_hyperparameters(model_type, n_labeled)\n\n    tf.reset_default_graph()\n    model = Model(RunContext(__file__, data_seed))\n\n    cifar = Cifar10ZCA(n_labeled=n_labeled,\n                       data_seed=data_seed,\n                       test_phase=test_phase)\n\n    model[\'flip_horizontally\'] = True\n    model[\'ema_consistency\'] = hyperparams[\'ema_consistency\']\n    model[\'max_consistency_cost\'] = hyperparams[\'max_consistency_cost\']\n    model[\'apply_consistency_to_labeled\'] = hyperparams[\'apply_consistency_to_labeled\']\n    model[\'adam_beta_2_during_rampup\'] = 0.999\n    model[\'ema_decay_during_rampup\'] = 0.999\n    model[\'normalize_input\'] = False  # Keep ZCA information\n    model[\'rampdown_length\'] = 25000\n    model[\'training_length\'] = 150000\n\n    training_batches = minibatching.training_batches(cifar.training,\n                                                     minibatch_size,\n                                                     hyperparams[\'n_labeled_per_batch\'])\n    evaluation_batches_fn = minibatching.evaluation_epoch_generator(cifar.evaluation,\n                                                                    minibatch_size)\n\n    tensorboard_dir = model.save_tensorboard_graph()\n    LOG.info(""Saved tensorboard graph to %r"", tensorboard_dir)\n\n    model.train(training_batches, evaluation_batches_fn)\n\n\nif __name__ == ""__main__"":\n    for run_params in parameters():\n        run(**run_params)\n'"
tensorflow/experiments/cifar10_no_augmentation_final_eval.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""CIFAR-10 final evaluation without augmentation""""""\n\nimport logging\nimport sys\n\nfrom .run_context import RunContext\nimport tensorflow as tf\n\nfrom datasets import Cifar10ZCA\nfrom mean_teacher.model import Model\nfrom mean_teacher import minibatching\n\n\nLOG = logging.getLogger(\'main\')\n\n\ndef parameters():\n    test_phase = True\n    for n_labeled in [4000, \'all\', 1000, 2000]:\n        for model_type in [\'mean_teacher\', \'pi\']:\n            if n_labeled == \'all\':\n                n_runs = 4\n            else:\n                n_runs = 10\n            for data_seed in range(2000, 2000 + n_runs):\n                yield {\n                    \'test_phase\': test_phase,\n                    \'model_type\': model_type,\n                    \'n_labeled\': n_labeled,\n                    \'data_seed\': data_seed\n                }\n\n\ndef model_hyperparameters(model_type, n_labeled):\n    assert model_type in [\'mean_teacher\', \'pi\']\n    if n_labeled == \'all\':\n        return {\n            \'n_labeled_per_batch\': 100,\n            \'max_consistency_cost\': 100.0,\n            \'apply_consistency_to_labeled\': True,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    elif isinstance(n_labeled, int):\n        return {\n            \'n_labeled_per_batch\': \'vary\',\n            \'max_consistency_cost\': 100.0 * n_labeled / 50000,\n            \'apply_consistency_to_labeled\': True,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    else:\n        msg = ""Unexpected combination: {model_type}, {n_labeled}""\n        assert False, msg.format(locals())\n\n\ndef run(test_phase, n_labeled, data_seed, model_type):\n    minibatch_size = 100\n    hyperparams = model_hyperparameters(model_type, n_labeled)\n\n    tf.reset_default_graph()\n    model = Model(RunContext(__file__, data_seed))\n\n    cifar = Cifar10ZCA(n_labeled=n_labeled,\n                       data_seed=data_seed,\n                       test_phase=test_phase)\n\n    model[\'ema_consistency\'] = hyperparams[\'ema_consistency\']\n    model[\'max_consistency_cost\'] = hyperparams[\'max_consistency_cost\']\n    model[\'apply_consistency_to_labeled\'] = hyperparams[\'apply_consistency_to_labeled\']\n    model[\'adam_beta_2_during_rampup\'] = 0.999\n    model[\'ema_decay_during_rampup\'] = 0.999\n    model[\'normalize_input\'] = False  # Keep ZCA information\n    model[\'rampdown_length\'] = 25000\n    model[\'training_length\'] = 150000\n\n    # Turn off augmentation\n    model[\'translate\'] = False\n    model[\'flip_horizontally\'] = False\n\n    training_batches = minibatching.training_batches(cifar.training,\n                                                     minibatch_size,\n                                                     hyperparams[\'n_labeled_per_batch\'])\n    evaluation_batches_fn = minibatching.evaluation_epoch_generator(cifar.evaluation,\n                                                                    minibatch_size)\n\n    tensorboard_dir = model.save_tensorboard_graph()\n    LOG.info(""Saved tensorboard graph to %r"", tensorboard_dir)\n\n    model.train(training_batches, evaluation_batches_fn)\n\n\nif __name__ == ""__main__"":\n    for run_params in parameters():\n        run(**run_params)\n'"
tensorflow/experiments/cifar10_supervised_final_eval.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""CIFAR-10 supervised baselines. Train with all training data, evaluate against test set.""""""\n\nimport logging\nimport sys\n\nfrom .run_context import RunContext\nimport tensorflow as tf\n\nfrom datasets import Cifar10ZCA\nfrom mean_teacher.model import Model\nfrom mean_teacher import minibatching\n\n\nLOG = logging.getLogger(\'main\')\n\n\ndef parameters():\n    test_phase = True\n    for n_labeled in [4000, 2000, 1000]:\n        for data_seed in range(10):\n            epochs = 750\n            rampdown_epochs = 100\n            yield {\n                \'test_phase\': test_phase,\n                \'n_labeled\': n_labeled,\n                \'data_seed\': data_seed,\n                \'training_length\': epochs * n_labeled / 100,\n                \'rampdown_length\': rampdown_epochs * n_labeled / 100\n            }\n\n    for data_seed in range(4):\n        yield {\n            \'test_phase\': test_phase,\n            \'n_labeled\': \'all\',\n            \'data_seed\': data_seed,\n            \'training_length\': 150000,\n            \'rampdown_length\': 25000\n        }\n\n\ndef run(test_phase, data_seed, n_labeled, training_length, rampdown_length):\n    minibatch_size = 100\n    n_labeled_per_batch = 100\n\n    tf.reset_default_graph()\n    model = Model(RunContext(__file__, data_seed))\n\n    cifar = Cifar10ZCA(n_labeled=n_labeled,\n                       data_seed=data_seed,\n                       test_phase=test_phase)\n\n    model[\'flip_horizontally\'] = True\n    model[\'ema_consistency\'] = True\n    model[\'max_consistency_cost\'] = 0.0\n    model[\'apply_consistency_to_labeled\'] = False\n    model[\'adam_beta_2_during_rampup\'] = 0.999\n    model[\'ema_decay_during_rampup\'] = 0.999\n    model[\'normalize_input\'] = False  # Keep ZCA information\n    model[\'rampdown_length\'] = rampdown_length\n    model[\'training_length\'] = training_length\n\n    training_batches = minibatching.training_batches(cifar.training,\n                                                     minibatch_size,\n                                                     n_labeled_per_batch)\n    evaluation_batches_fn = minibatching.evaluation_epoch_generator(cifar.evaluation,\n                                                                    minibatch_size)\n\n    tensorboard_dir = model.save_tensorboard_graph()\n    LOG.info(""Saved tensorboard graph to %r"", tensorboard_dir)\n\n    model.train(training_batches, evaluation_batches_fn)\n\n\nif __name__ == ""__main__"":\n    for run_params in parameters():\n        run(**run_params)\n'"
tensorflow/experiments/cifar10_supervised_no_augmentation_final_eval.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""CIFAR-10 supervised baselines. Train with all training data, evaluate against test set.""""""\n\nimport logging\nimport sys\n\nfrom .run_context import RunContext\nimport tensorflow as tf\n\nfrom datasets import Cifar10ZCA\nfrom mean_teacher.model import Model\nfrom mean_teacher import minibatching\n\n\nLOG = logging.getLogger(\'main\')\n\n\ndef parameters():\n    test_phase = True\n    for n_labeled in [4000, 2000, 1000]:\n        for data_seed in range(10):\n            epochs = 750\n            rampdown_epochs = 100\n            yield {\n                \'test_phase\': test_phase,\n                \'n_labeled\': n_labeled,\n                \'data_seed\': data_seed,\n                \'training_length\': epochs * n_labeled / 100,\n                \'rampdown_length\': rampdown_epochs * n_labeled / 100\n            }\n\n    for data_seed in range(4):\n        yield {\n            \'test_phase\': test_phase,\n            \'n_labeled\': \'all\',\n            \'data_seed\': data_seed,\n            \'training_length\': 150000,\n            \'rampdown_length\': 25000\n        }\n\n\ndef run(test_phase, data_seed, n_labeled, training_length, rampdown_length):\n    minibatch_size = 100\n    n_labeled_per_batch = 100\n\n    tf.reset_default_graph()\n    model = Model(RunContext(__file__, data_seed))\n\n    cifar = Cifar10ZCA(n_labeled=n_labeled,\n                       data_seed=data_seed,\n                       test_phase=test_phase)\n\n    model[\'flip_horizontally\'] = True\n    model[\'ema_consistency\'] = True\n    model[\'max_consistency_cost\'] = 0.0\n    model[\'apply_consistency_to_labeled\'] = False\n    model[\'adam_beta_2_during_rampup\'] = 0.999\n    model[\'ema_decay_during_rampup\'] = 0.999\n    model[\'normalize_input\'] = False  # Keep ZCA information\n    model[\'rampdown_length\'] = rampdown_length\n    model[\'training_length\'] = training_length\n\n    # Turn off augmentation\n    model[\'translate\'] = False\n    model[\'flip_horizontally\'] = False\n\n    training_batches = minibatching.training_batches(cifar.training,\n                                                     minibatch_size,\n                                                     n_labeled_per_batch)\n    evaluation_batches_fn = minibatching.evaluation_epoch_generator(cifar.evaluation,\n                                                                    minibatch_size)\n\n    tensorboard_dir = model.save_tensorboard_graph()\n    LOG.info(""Saved tensorboard graph to %r"", tensorboard_dir)\n\n    model.train(training_batches, evaluation_batches_fn)\n\n\nif __name__ == ""__main__"":\n    for run_params in parameters():\n        run(**run_params)\n'"
tensorflow/experiments/run_context.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nfrom datetime import datetime\nfrom collections import defaultdict\nimport threading\nimport time\nimport logging\nimport os\n\nfrom pandas import DataFrame\nfrom collections import defaultdict\n\n\nclass TrainLog:\n    """"""Saves training logs in Pandas msgpacks""""""\n\n    INCREMENTAL_UPDATE_TIME = 300\n\n    def __init__(self, directory, name):\n        self.log_file_path = ""{}/{}.msgpack"".format(directory, name)\n        self._log = defaultdict(dict)\n        self._log_lock = threading.RLock()\n        self._last_update_time = time.time() - self.INCREMENTAL_UPDATE_TIME\n\n    def record_single(self, step, column, value):\n        self._record(step, {column: value})\n\n    def record(self, step, col_val_dict):\n        self._record(step, col_val_dict)\n\n    def save(self):\n        df = self._as_dataframe()\n        df.to_msgpack(self.log_file_path, compress=\'zlib\')\n\n    def _record(self, step, col_val_dict):\n        with self._log_lock:\n            self._log[step].update(col_val_dict)\n            if time.time() - self._last_update_time >= self.INCREMENTAL_UPDATE_TIME:\n                self._last_update_time = time.time()\n                self.save()\n\n    def _as_dataframe(self):\n        with self._log_lock:\n            return DataFrame.from_dict(self._log, orient=\'index\')\n\n\nclass RunContext:\n    """"""Creates directories and files for the run""""""\n\n    def __init__(self, runner_file, run_idx):\n        logging.basicConfig(level=logging.INFO, format=\'%(message)s\')\n        runner_name = os.path.basename(runner_file).split(""."")[0]\n        self.result_dir = ""{root}/{runner_name}/{date:%Y-%m-%d_%H:%M:%S}/{run_idx}"".format(\n            root=\'results\',\n            runner_name=runner_name,\n            date=datetime.now(),\n            run_idx=run_idx\n        )\n        self.transient_dir = self.result_dir + ""/transient""\n        os.makedirs(self.result_dir)\n        os.makedirs(self.transient_dir)\n\n    def create_train_log(self, name):\n        return TrainLog(self.result_dir, name)\n'"
tensorflow/experiments/svhn_250_vary_consistency_cost.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""Vary ema decay parameter on 250-label SVHN for the NIPS paper""""""\n\nimport logging\nimport sys\n\nfrom .run_context import RunContext\nimport tensorflow as tf\n\nfrom datasets import SVHN\nfrom mean_teacher.model import Model\nfrom mean_teacher import minibatching\n\n\nLOG = logging.getLogger(\'main\')\n\n\ndef parameters():\n    n_runs = 4\n    for data_seed in range(1000, 1000 + n_runs):\n        for max_consistency_cost in [0, 0.1, 0.3, 1, 3, 10, 30]:\n            yield {\n                \'data_seed\': data_seed,\n                \'max_consistency_cost\': max_consistency_cost\n            }\n\n\ndef model_hyperparameters(model_type, n_labeled, n_extra_unlabeled):\n    assert model_type in [\'mean_teacher\', \'pi\']\n    training_length = {\n        0: 180000,\n        100000: 400000,\n        500000: 600000,\n    }\n    if n_labeled == \'all\':\n        return {\n            \'training_length\': training_length[n_extra_unlabeled],\n            \'n_labeled_per_batch\': 100,\n            \'max_consistency_cost\': 100.0,\n            \'apply_consistency_to_labeled\': True,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    elif isinstance(n_labeled, int):\n        return {\n            \'training_length\': training_length[n_extra_unlabeled],\n            \'n_labeled_per_batch\': 1,\n            \'max_consistency_cost\': 1.0,\n            \'apply_consistency_to_labeled\': False,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    else:\n        msg = ""Unexpected combination: {model_type}, {n_labeled}, {n_extra_unlabeled}""\n        assert False, msg.format(locals())\n\n\ndef run(data_seed, max_consistency_cost,\n        test_phase=False, n_labeled=250, n_extra_unlabeled=0, model_type=\'mean_teacher\'):\n    minibatch_size = 100\n    hyperparams = model_hyperparameters(model_type, n_labeled, n_extra_unlabeled)\n\n    tf.reset_default_graph()\n    model = Model(RunContext(__file__, data_seed))\n\n    svhn = SVHN(n_labeled=n_labeled,\n                n_extra_unlabeled=n_extra_unlabeled,\n                data_seed=data_seed,\n                test_phase=test_phase)\n\n    model[\'ema_consistency\'] = hyperparams[\'ema_consistency\']\n    model[\'apply_consistency_to_labeled\'] = hyperparams[\'apply_consistency_to_labeled\']\n    model[\'training_length\'] = hyperparams[\'training_length\']\n    #model[\'max_consistency_cost\'] = hyperparams[\'max_consistency_cost\']\n    model[\'max_consistency_cost\'] = max_consistency_cost\n\n    training_batches = minibatching.training_batches(svhn.training,\n                                                     minibatch_size,\n                                                     hyperparams[\'n_labeled_per_batch\'])\n    evaluation_batches_fn = minibatching.evaluation_epoch_generator(svhn.evaluation,\n                                                                    minibatch_size)\n\n    tensorboard_dir = model.save_tensorboard_graph()\n    LOG.info(""Saved tensorboard graph to %r"", tensorboard_dir)\n\n    model.train(training_batches, evaluation_batches_fn)\n\n\nif __name__ == ""__main__"":\n    for run_params in parameters():\n        run(**run_params)\n'"
tensorflow/experiments/svhn_250_vary_dropout.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""Vary dropout parameter on 250-label SVHN for the NIPS paper""""""\n\nimport logging\nimport sys\n\nfrom .run_context import RunContext\nimport tensorflow as tf\n\nfrom datasets import SVHN\nfrom mean_teacher.model import Model\nfrom mean_teacher import minibatching\n\n\nLOG = logging.getLogger(\'main\')\n\n\ndef all_parameters():\n    n_runs = 4\n    for data_seed in range(1000, 1000 + n_runs):\n        for student_dropout_probability in [0, 0.25, 0.5, 0.75]:\n            for teacher_dropout_probability in [0, 0.25, 0.5, 0.75]:\n                if student_dropout_probability in [teacher_dropout_probability, 0.5]:\n                    yield {\n                        \'data_seed\': data_seed,\n                        \'student_dropout_probability\': student_dropout_probability,\n                        \'teacher_dropout_probability\': teacher_dropout_probability,\n                        \'model_type\': \'pi\'\n                    }\n\n\ndef parameters():\n    for idx, param in enumerate(all_parameters()):\n        if idx >= 21:\n            yield param\n\n\ndef model_hyperparameters(model_type, n_labeled, n_extra_unlabeled):\n    assert model_type in [\'mean_teacher\', \'pi\']\n    training_length = {\n        0: 180000,\n        100000: 400000,\n        500000: 600000,\n    }\n    if n_labeled == \'all\':\n        return {\n            \'training_length\': training_length[n_extra_unlabeled],\n            \'n_labeled_per_batch\': 100,\n            \'max_consistency_cost\': 100.0,\n            \'apply_consistency_to_labeled\': True,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    elif isinstance(n_labeled, int):\n        return {\n            \'training_length\': training_length[n_extra_unlabeled],\n            \'n_labeled_per_batch\': 1,\n            \'max_consistency_cost\': 1.0,\n            \'apply_consistency_to_labeled\': False,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    else:\n        msg = ""Unexpected combination: {model_type}, {n_labeled}, {n_extra_unlabeled}""\n        assert False, msg.format(locals())\n\n\ndef run(data_seed, student_dropout_probability, teacher_dropout_probability,\n        test_phase=False, n_labeled=250, n_extra_unlabeled=0, model_type=\'mean_teacher\'):\n    minibatch_size = 100\n    hyperparams = model_hyperparameters(model_type, n_labeled, n_extra_unlabeled)\n\n    tf.reset_default_graph()\n    model = Model(RunContext(__file__, data_seed))\n\n    svhn = SVHN(n_labeled=n_labeled,\n                n_extra_unlabeled=n_extra_unlabeled,\n                data_seed=data_seed,\n                test_phase=test_phase)\n\n    model[\'ema_consistency\'] = hyperparams[\'ema_consistency\']\n    model[\'max_consistency_cost\'] = hyperparams[\'max_consistency_cost\']\n    model[\'apply_consistency_to_labeled\'] = hyperparams[\'apply_consistency_to_labeled\']\n    model[\'training_length\'] = hyperparams[\'training_length\']\n    model[\'student_dropout_probability\'] = student_dropout_probability\n    model[\'teacher_dropout_probability\'] = teacher_dropout_probability\n\n    training_batches = minibatching.training_batches(svhn.training,\n                                                     minibatch_size,\n                                                     hyperparams[\'n_labeled_per_batch\'])\n    evaluation_batches_fn = minibatching.evaluation_epoch_generator(svhn.evaluation,\n                                                                    minibatch_size)\n\n    tensorboard_dir = model.save_tensorboard_graph()\n    LOG.info(""Saved tensorboard graph to %r"", tensorboard_dir)\n\n    model.train(training_batches, evaluation_batches_fn)\n\n\nif __name__ == ""__main__"":\n    for run_params in parameters():\n        run(**run_params)\n'"
tensorflow/experiments/svhn_250_vary_ema_decay.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""Vary ema decay parameter on 250-label SVHN for the NIPS paper""""""\n\nimport logging\nimport sys\n\nfrom .run_context import RunContext\nimport tensorflow as tf\n\nfrom datasets import SVHN\nfrom mean_teacher.model import Model\nfrom mean_teacher import minibatching\n\n\nLOG = logging.getLogger(\'main\')\n\n\ndef parameters():\n    n_runs = 4\n    for data_seed in range(1000, 1000 + n_runs):\n        for ema_decay in [0, 0.9, 0.97, 0.99, 0.997, 0.999, 0.9997, 0.9999]:\n            yield {\n                \'data_seed\': data_seed,\n                \'ema_decay_during_rampup\': ema_decay,\n                \'ema_decay_after_rampup\': ema_decay\n            }\n\n        yield {\n            \'data_seed\': data_seed,\n            \'ema_decay_during_rampup\': 0.99,\n            \'ema_decay_after_rampup\': 0.999,\n        }\n\n\ndef model_hyperparameters(model_type, n_labeled, n_extra_unlabeled):\n    assert model_type in [\'mean_teacher\', \'pi\']\n    training_length = {\n        0: 180000,\n        100000: 400000,\n        500000: 600000,\n    }\n    if n_labeled == \'all\':\n        return {\n            \'training_length\': training_length[n_extra_unlabeled],\n            \'n_labeled_per_batch\': 100,\n            \'max_consistency_cost\': 100.0,\n            \'apply_consistency_to_labeled\': True,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    elif isinstance(n_labeled, int):\n        return {\n            \'training_length\': training_length[n_extra_unlabeled],\n            \'n_labeled_per_batch\': 1,\n            \'max_consistency_cost\': 1.0,\n            \'apply_consistency_to_labeled\': False,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    else:\n        msg = ""Unexpected combination: {model_type}, {n_labeled}, {n_extra_unlabeled}""\n        assert False, msg.format(locals())\n\n\ndef run(data_seed, ema_decay_during_rampup, ema_decay_after_rampup,\n        test_phase=False, n_labeled=250, n_extra_unlabeled=0, model_type=\'mean_teacher\'):\n    minibatch_size = 100\n    hyperparams = model_hyperparameters(model_type, n_labeled, n_extra_unlabeled)\n\n    tf.reset_default_graph()\n    model = Model(RunContext(__file__, data_seed))\n\n    svhn = SVHN(n_labeled=n_labeled,\n                n_extra_unlabeled=n_extra_unlabeled,\n                data_seed=data_seed,\n                test_phase=test_phase)\n\n    model[\'ema_consistency\'] = hyperparams[\'ema_consistency\']\n    model[\'max_consistency_cost\'] = hyperparams[\'max_consistency_cost\']\n    model[\'apply_consistency_to_labeled\'] = hyperparams[\'apply_consistency_to_labeled\']\n    model[\'training_length\'] = hyperparams[\'training_length\']\n    model[\'ema_decay_during_rampup\'] = ema_decay_during_rampup\n    model[\'ema_decay_after_rampup\'] = ema_decay_after_rampup\n\n    training_batches = minibatching.training_batches(svhn.training,\n                                                     minibatch_size,\n                                                     hyperparams[\'n_labeled_per_batch\'])\n    evaluation_batches_fn = minibatching.evaluation_epoch_generator(svhn.evaluation,\n                                                                    minibatch_size)\n\n    tensorboard_dir = model.save_tensorboard_graph()\n    LOG.info(""Saved tensorboard graph to %r"", tensorboard_dir)\n\n    model.train(training_batches, evaluation_batches_fn)\n\n\nif __name__ == ""__main__"":\n    for run_params in parameters():\n        run(**run_params)\n'"
tensorflow/experiments/svhn_250_vary_logit_distance_cost.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""Vary trust parameter on 250-label SVHN for the NIPS paper""""""\n\nimport logging\nimport sys\n\nfrom .run_context import RunContext\nimport tensorflow as tf\n\nfrom datasets import SVHN\nfrom mean_teacher.model import Model\nfrom mean_teacher import minibatching\n\n\nLOG = logging.getLogger(\'main\')\n\n\ndef parameters():\n    n_runs = 4\n    for data_seed in range(1000, 1000 + n_runs):\n        for logit_distance_cost in [0.0, 0.001, 0.01, 0.1, 1.0]:\n            yield {\n                \'data_seed\': data_seed,\n                \'num_logits\': 2,\n                \'logit_distance_cost\': logit_distance_cost\n            }\n\n        yield {\n            \'data_seed\': data_seed,\n            \'num_logits\': 1,\n            \'logit_distance_cost\': 0.0\n        }\n\n\ndef model_hyperparameters(model_type, n_labeled, n_extra_unlabeled):\n    assert model_type in [\'mean_teacher\', \'pi\']\n    training_length = {\n        0: 180000,\n        100000: 400000,\n        500000: 600000,\n    }\n    if n_labeled == \'all\':\n        return {\n            \'training_length\': training_length[n_extra_unlabeled],\n            \'n_labeled_per_batch\': 100,\n            \'max_consistency_cost\': 100.0,\n            \'apply_consistency_to_labeled\': True,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    elif isinstance(n_labeled, int):\n        return {\n            \'training_length\': training_length[n_extra_unlabeled],\n            \'n_labeled_per_batch\': 1,\n            \'max_consistency_cost\': 1.0,\n            \'apply_consistency_to_labeled\': False,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    else:\n        msg = ""Unexpected combination: {model_type}, {n_labeled}, {n_extra_unlabeled}""\n        assert False, msg.format(locals())\n\n\ndef run(data_seed, num_logits, logit_distance_cost, test_phase=False, n_labeled=500, n_extra_unlabeled=0, model_type=\'mean_teacher\'):\n    minibatch_size = 100\n    hyperparams = model_hyperparameters(model_type, n_labeled, n_extra_unlabeled)\n\n    tf.reset_default_graph()\n    model = Model(RunContext(__file__, data_seed))\n\n    svhn = SVHN(n_labeled=n_labeled,\n                n_extra_unlabeled=n_extra_unlabeled,\n                data_seed=data_seed,\n                test_phase=test_phase)\n\n    model[\'ema_consistency\'] = hyperparams[\'ema_consistency\']\n    model[\'max_consistency_cost\'] = hyperparams[\'max_consistency_cost\']\n    model[\'apply_consistency_to_labeled\'] = hyperparams[\'apply_consistency_to_labeled\']\n    model[\'training_length\'] = hyperparams[\'training_length\']\n    model[\'num_logits\'] = num_logits\n    model[\'logit_distance_cost\'] = logit_distance_cost\n\n    training_batches = minibatching.training_batches(svhn.training,\n                                                     minibatch_size,\n                                                     hyperparams[\'n_labeled_per_batch\'])\n    evaluation_batches_fn = minibatching.evaluation_epoch_generator(svhn.evaluation,\n                                                                    minibatch_size)\n\n    tensorboard_dir = model.save_tensorboard_graph()\n    LOG.info(""Saved tensorboard graph to %r"", tensorboard_dir)\n\n    model.train(training_batches, evaluation_batches_fn)\n\n\nif __name__ == ""__main__"":\n    for run_params in parameters():\n        run(**run_params)\n'"
tensorflow/experiments/svhn_250_vary_perturbation.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""Vary different perturbations on 250-label SVHN for the NIPS paper""""""\n\nimport logging\nimport sys\n\nfrom .run_context import RunContext\nimport tensorflow as tf\n\nfrom datasets import SVHN\nfrom mean_teacher.model import Model\nfrom mean_teacher import minibatching\n\n\nLOG = logging.getLogger(\'main\')\n\n\ndef all_parameters():\n    n_runs = 4\n    for data_seed in range(1000, 1000 + n_runs):\n        for dropout in [0.5, 0.]:\n            for input_noise in [0.15, 0.]:\n                for augmentation in [True, False]:\n                    yield {\n                        \'data_seed\': data_seed,\n                        \'dropout\': dropout,\n                        \'input_noise\': input_noise,\n                        \'augmentation\': augmentation,\n                    }\n\n\ndef parameters():\n    for idx, params in enumerate(all_parameters()):\n        if idx >= 6:\n            yield params\n\n\ndef model_hyperparameters(model_type, n_labeled, n_extra_unlabeled):\n    assert model_type in [\'mean_teacher\', \'pi\']\n    training_length = {\n        0: 180000,\n        100000: 400000,\n        500000: 600000,\n    }\n    if n_labeled == \'all\':\n        return {\n            \'training_length\': training_length[n_extra_unlabeled],\n            \'n_labeled_per_batch\': 100,\n            \'max_consistency_cost\': 100.0,\n            \'apply_consistency_to_labeled\': True,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    elif isinstance(n_labeled, int):\n        return {\n            \'training_length\': training_length[n_extra_unlabeled],\n            \'n_labeled_per_batch\': 1,\n            \'max_consistency_cost\': 1.0,\n            \'apply_consistency_to_labeled\': False,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    else:\n        msg = ""Unexpected combination: {model_type}, {n_labeled}, {n_extra_unlabeled}""\n        assert False, msg.format(locals())\n\n\ndef run(data_seed, dropout, input_noise, augmentation,\n        test_phase=False, n_labeled=250, n_extra_unlabeled=0, model_type=\'mean_teacher\'):\n    minibatch_size = 100\n    hyperparams = model_hyperparameters(model_type, n_labeled, n_extra_unlabeled)\n\n    tf.reset_default_graph()\n    model = Model(RunContext(__file__, data_seed))\n\n    svhn = SVHN(n_labeled=n_labeled,\n                n_extra_unlabeled=n_extra_unlabeled,\n                data_seed=data_seed,\n                test_phase=test_phase)\n\n    model[\'ema_consistency\'] = hyperparams[\'ema_consistency\']\n    model[\'max_consistency_cost\'] = hyperparams[\'max_consistency_cost\']\n    model[\'apply_consistency_to_labeled\'] = hyperparams[\'apply_consistency_to_labeled\']\n    model[\'training_length\'] = hyperparams[\'training_length\']\n    model[\'student_dropout_probability\'] = dropout\n    model[\'teacher_dropout_probability\'] = dropout\n    model[\'input_noise\'] = input_noise\n    model[\'translate\'] = augmentation\n\n    training_batches = minibatching.training_batches(svhn.training,\n                                                     minibatch_size,\n                                                     hyperparams[\'n_labeled_per_batch\'])\n    evaluation_batches_fn = minibatching.evaluation_epoch_generator(svhn.evaluation,\n                                                                    minibatch_size)\n\n    tensorboard_dir = model.save_tensorboard_graph()\n    LOG.info(""Saved tensorboard graph to %r"", tensorboard_dir)\n\n    model.train(training_batches, evaluation_batches_fn)\n\n\nif __name__ == ""__main__"":\n    for run_params in parameters():\n        run(**run_params)\n'"
tensorflow/experiments/svhn_250_vary_trust.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""Vary trust parameter on 250-label SVHN for the NIPS paper""""""\n\nimport logging\nimport sys\n\nfrom .run_context import RunContext\nimport tensorflow as tf\n\nfrom datasets import SVHN\nfrom mean_teacher.model import Model\nfrom mean_teacher import minibatching\n\n\nLOG = logging.getLogger(\'main\')\n\n\ndef all_parameters():\n    n_runs = 4\n    for data_seed in range(1000, 1000 + n_runs):\n        for consistency_trust in [0.0, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99, 1.0]:\n            yield {\n                \'data_seed\': data_seed,\n                \'consistency_trust\': consistency_trust\n            }\n\n\ndef parameters():\n    for idx, param in enumerate(all_parameters()):\n        if idx > 22:\n            yield param\n\n\ndef model_hyperparameters(model_type, n_labeled, n_extra_unlabeled):\n    assert model_type in [\'mean_teacher\', \'pi\']\n    training_length = {\n        0: 180000,\n        100000: 400000,\n        500000: 600000,\n    }\n    if n_labeled == \'all\':\n        return {\n            \'training_length\': training_length[n_extra_unlabeled],\n            \'n_labeled_per_batch\': 100,\n            \'max_consistency_cost\': 100.0,\n            \'apply_consistency_to_labeled\': True,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    elif isinstance(n_labeled, int):\n        return {\n            \'training_length\': training_length[n_extra_unlabeled],\n            \'n_labeled_per_batch\': 1,\n            \'max_consistency_cost\': 1.0,\n            \'apply_consistency_to_labeled\': False,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    else:\n        msg = ""Unexpected combination: {model_type}, {n_labeled}, {n_extra_unlabeled}""\n        assert False, msg.format(locals())\n\n\ndef run(data_seed, consistency_trust, test_phase=False, n_labeled=500, n_extra_unlabeled=0, model_type=\'mean_teacher\'):\n    minibatch_size = 100\n    hyperparams = model_hyperparameters(model_type, n_labeled, n_extra_unlabeled)\n\n    tf.reset_default_graph()\n    model = Model(RunContext(__file__, data_seed))\n\n    svhn = SVHN(n_labeled=n_labeled,\n                n_extra_unlabeled=n_extra_unlabeled,\n                data_seed=data_seed,\n                test_phase=test_phase)\n\n    model[\'ema_consistency\'] = hyperparams[\'ema_consistency\']\n    model[\'max_consistency_cost\'] = hyperparams[\'max_consistency_cost\']\n    model[\'apply_consistency_to_labeled\'] = hyperparams[\'apply_consistency_to_labeled\']\n    model[\'training_length\'] = hyperparams[\'training_length\']\n    model[\'consistency_trust\'] = consistency_trust\n\n    training_batches = minibatching.training_batches(svhn.training,\n                                                     minibatch_size,\n                                                     hyperparams[\'n_labeled_per_batch\'])\n    evaluation_batches_fn = minibatching.evaluation_epoch_generator(svhn.evaluation,\n                                                                    minibatch_size)\n\n    tensorboard_dir = model.save_tensorboard_graph()\n    LOG.info(""Saved tensorboard graph to %r"", tensorboard_dir)\n\n    model.train(training_batches, evaluation_batches_fn)\n\n\nif __name__ == ""__main__"":\n    for run_params in parameters():\n        run(**run_params)\n'"
tensorflow/experiments/svhn_final_eval.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""SVHN final evaluation before ICML author feedback and ICLR workshop""""""\n\nimport logging\nimport sys\n\nfrom .run_context import RunContext\nimport tensorflow as tf\n\nfrom datasets import SVHN\nfrom mean_teacher.model import Model\nfrom mean_teacher import minibatching\n\n\nLOG = logging.getLogger(\'main\')\n\n\ndef parameters():\n    test_phase = True\n    for n_labeled in [500, 250, 1000, \'all\', 100]:\n        for n_extra_unlabeled in [500000, 100000, 0]:\n            for model_type in [\'mean_teacher\', \'pi\']:\n                if n_extra_unlabeled > 0 and n_labeled != 500:\n                    continue\n                if n_labeled == \'all\':\n                    n_runs = 4\n                else:\n                    n_runs = 10\n                for data_seed in range(2000, 2000 + n_runs):\n                    yield {\n                        \'test_phase\': test_phase,\n                        \'model_type\': model_type,\n                        \'n_labeled\': n_labeled,\n                        \'n_extra_unlabeled\': n_extra_unlabeled,\n                        \'data_seed\': data_seed\n                    }\n\n\ndef model_hyperparameters(model_type, n_labeled, n_extra_unlabeled):\n    assert model_type in [\'mean_teacher\', \'pi\']\n    training_length = {\n        0: 180000,\n        100000: 400000,\n        500000: 600000,\n    }\n    if n_labeled == \'all\':\n        return {\n            \'training_length\': training_length[n_extra_unlabeled],\n            \'n_labeled_per_batch\': 100,\n            \'max_consistency_cost\': 100.0,\n            \'apply_consistency_to_labeled\': True,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    elif isinstance(n_labeled, int):\n        return {\n            \'training_length\': training_length[n_extra_unlabeled],\n            \'n_labeled_per_batch\': 1,\n            \'max_consistency_cost\': 1.0,\n            \'apply_consistency_to_labeled\': False,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    else:\n        msg = ""Unexpected combination: {model_type}, {n_labeled}, {n_extra_unlabeled}""\n        assert False, msg.format(locals())\n\n\ndef run(test_phase, n_labeled, n_extra_unlabeled, data_seed, model_type):\n    minibatch_size = 100\n    hyperparams = model_hyperparameters(model_type, n_labeled, n_extra_unlabeled)\n\n    tf.reset_default_graph()\n    model = Model(RunContext(__file__, data_seed))\n\n    svhn = SVHN(n_labeled=n_labeled,\n                n_extra_unlabeled=n_extra_unlabeled,\n                data_seed=data_seed,\n                test_phase=test_phase)\n\n    model[\'ema_consistency\'] = hyperparams[\'ema_consistency\']\n    model[\'max_consistency_cost\'] = hyperparams[\'max_consistency_cost\']\n    model[\'apply_consistency_to_labeled\'] = hyperparams[\'apply_consistency_to_labeled\']\n    model[\'training_length\'] = hyperparams[\'training_length\']\n\n    training_batches = minibatching.training_batches(svhn.training,\n                                                     minibatch_size,\n                                                     hyperparams[\'n_labeled_per_batch\'])\n    evaluation_batches_fn = minibatching.evaluation_epoch_generator(svhn.evaluation,\n                                                                    minibatch_size)\n\n    tensorboard_dir = model.save_tensorboard_graph()\n    LOG.info(""Saved tensorboard graph to %r"", tensorboard_dir)\n\n    model.train(training_batches, evaluation_batches_fn)\n\n\nif __name__ == ""__main__"":\n    for run_params in parameters():\n        run(**run_params)\n'"
tensorflow/experiments/svhn_no_augmentation_final_eval.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""SVHN final evaluation without augmentation""""""\n\nimport logging\nimport sys\n\nfrom .run_context import RunContext\nimport tensorflow as tf\n\nfrom datasets import SVHN\nfrom mean_teacher.model import Model\nfrom mean_teacher import minibatching\n\n\nLOG = logging.getLogger(\'main\')\n\n\ndef parameters():\n    test_phase = True\n    for n_labeled in [500, 250, 1000, \'all\', 100]:\n        for n_extra_unlabeled in [0]:\n            for model_type in [\'mean_teacher\', \'pi\']:\n                if n_extra_unlabeled > 0 and n_labeled != 500:\n                    continue\n                if n_labeled == \'all\':\n                    n_runs = 4\n                else:\n                    n_runs = 10\n                for data_seed in range(2000, 2000 + n_runs):\n                    yield {\n                        \'test_phase\': test_phase,\n                        \'model_type\': model_type,\n                        \'n_labeled\': n_labeled,\n                        \'n_extra_unlabeled\': n_extra_unlabeled,\n                        \'data_seed\': data_seed\n                    }\n\n\ndef model_hyperparameters(model_type, n_labeled, n_extra_unlabeled):\n    assert model_type in [\'mean_teacher\', \'pi\']\n    training_length = {\n        0: 180000,\n        100000: 400000,\n        500000: 600000,\n    }\n    if n_labeled == \'all\':\n        return {\n            \'training_length\': training_length[n_extra_unlabeled],\n            \'n_labeled_per_batch\': 100,\n            \'max_consistency_cost\': 100.0,\n            \'apply_consistency_to_labeled\': True,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    elif isinstance(n_labeled, int):\n        return {\n            \'training_length\': training_length[n_extra_unlabeled],\n            \'n_labeled_per_batch\': 1,\n            \'max_consistency_cost\': 1.0,\n            \'apply_consistency_to_labeled\': False,\n            \'ema_consistency\': model_type == \'mean_teacher\'\n        }\n    else:\n        msg = ""Unexpected combination: {model_type}, {n_labeled}, {n_extra_unlabeled}""\n        assert False, msg.format(locals())\n\n\ndef run(test_phase, n_labeled, n_extra_unlabeled, data_seed, model_type):\n    minibatch_size = 100\n    hyperparams = model_hyperparameters(model_type, n_labeled, n_extra_unlabeled)\n\n    tf.reset_default_graph()\n    model = Model(RunContext(__file__, data_seed))\n\n    svhn = SVHN(n_labeled=n_labeled,\n                n_extra_unlabeled=n_extra_unlabeled,\n                data_seed=data_seed,\n                test_phase=test_phase)\n\n    model[\'ema_consistency\'] = hyperparams[\'ema_consistency\']\n    model[\'max_consistency_cost\'] = hyperparams[\'max_consistency_cost\']\n    model[\'apply_consistency_to_labeled\'] = hyperparams[\'apply_consistency_to_labeled\']\n    model[\'training_length\'] = hyperparams[\'training_length\']\n\n    # Turn off augmentation\n    model[\'translate\'] = False\n    model[\'flip_horizontally\'] = False\n\n    training_batches = minibatching.training_batches(svhn.training,\n                                                     minibatch_size,\n                                                     hyperparams[\'n_labeled_per_batch\'])\n    evaluation_batches_fn = minibatching.evaluation_epoch_generator(svhn.evaluation,\n                                                                    minibatch_size)\n\n    tensorboard_dir = model.save_tensorboard_graph()\n    LOG.info(""Saved tensorboard graph to %r"", tensorboard_dir)\n\n    model.train(training_batches, evaluation_batches_fn)\n\n\nif __name__ == ""__main__"":\n    for run_params in parameters():\n        run(**run_params)\n'"
tensorflow/experiments/svhn_supervised_final_eval.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""SVHN supervised evaluation""""""\n\nimport logging\nimport sys\n\nfrom .run_context import RunContext\nimport tensorflow as tf\n\nfrom datasets import SVHN\nfrom mean_teacher.model import Model\nfrom mean_teacher import minibatching\n\n\nLOG = logging.getLogger(\'main\')\n\n\ndef parameters():\n    test_phase = True\n    for n_labeled in [250, 500, 1000]:\n        for data_seed in range(10):\n            yield {\n                \'test_phase\': test_phase,\n                \'n_labeled\': n_labeled,\n                \'data_seed\': data_seed,\n                \'training_length\': 40000,\n                \'rampdown_length\': 10000\n            }\n\n    for data_seed in range(4):\n        yield {\n            \'test_phase\': test_phase,\n            \'n_labeled\': \'all\',\n            \'data_seed\': data_seed,\n            \'training_length\': 180000,\n            \'rampdown_length\': 25000\n        }\n\n\n\ndef run(test_phase, data_seed, n_labeled, training_length, rampdown_length):\n    minibatch_size = 100\n    n_labeled_per_batch = 100\n\n    tf.reset_default_graph()\n    model = Model(RunContext(__file__, data_seed))\n\n    cifar = SVHN(n_labeled=n_labeled,\n                 data_seed=data_seed,\n                 test_phase=test_phase)\n\n    model[\'ema_consistency\'] = True\n    model[\'max_consistency_cost\'] = 0.0\n    model[\'apply_consistency_to_labeled\'] = False\n    model[\'rampdown_length\'] = rampdown_length\n    model[\'training_length\'] = training_length\n\n    training_batches = minibatching.training_batches(cifar.training,\n                                                     minibatch_size,\n                                                     n_labeled_per_batch)\n    evaluation_batches_fn = minibatching.evaluation_epoch_generator(cifar.evaluation,\n                                                                    minibatch_size)\n\n    tensorboard_dir = model.save_tensorboard_graph()\n    LOG.info(""Saved tensorboard graph to %r"", tensorboard_dir)\n\n    model.train(training_batches, evaluation_batches_fn)\n\n\nif __name__ == ""__main__"":\n    for run_params in parameters():\n        run(**run_params)\n'"
tensorflow/experiments/svhn_supervised_no_augmentation_final_eval.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""SVHN supervised evaluation""""""\n\nimport logging\nimport sys\n\nfrom .run_context import RunContext\nimport tensorflow as tf\n\nfrom datasets import SVHN\nfrom mean_teacher.model import Model\nfrom mean_teacher import minibatching\n\n\nLOG = logging.getLogger(\'main\')\n\n\ndef parameters():\n    test_phase = True\n    for n_labeled in [250, 500, 1000]:\n        for data_seed in range(10):\n            yield {\n                \'test_phase\': test_phase,\n                \'n_labeled\': n_labeled,\n                \'data_seed\': data_seed,\n                \'training_length\': 40000,\n                \'rampdown_length\': 10000\n            }\n\n    for data_seed in range(4):\n        yield {\n            \'test_phase\': test_phase,\n            \'n_labeled\': \'all\',\n            \'data_seed\': data_seed,\n            \'training_length\': 180000,\n            \'rampdown_length\': 25000\n        }\n\n\n\ndef run(test_phase, data_seed, n_labeled, training_length, rampdown_length):\n    minibatch_size = 100\n    n_labeled_per_batch = 100\n\n    tf.reset_default_graph()\n    model = Model(RunContext(__file__, data_seed))\n\n    cifar = SVHN(n_labeled=n_labeled,\n                 data_seed=data_seed,\n                 test_phase=test_phase)\n\n    model[\'ema_consistency\'] = True\n    model[\'max_consistency_cost\'] = 0.0\n    model[\'apply_consistency_to_labeled\'] = False\n    model[\'rampdown_length\'] = rampdown_length\n    model[\'training_length\'] = training_length\n\n    # Turn off augmentation\n    model[\'translate\'] = False\n    model[\'flip_horizontally\'] = False\n\n    training_batches = minibatching.training_batches(cifar.training,\n                                                     minibatch_size,\n                                                     n_labeled_per_batch)\n    evaluation_batches_fn = minibatching.evaluation_epoch_generator(cifar.evaluation,\n                                                                    minibatch_size)\n\n    tensorboard_dir = model.save_tensorboard_graph()\n    LOG.info(""Saved tensorboard graph to %r"", tensorboard_dir)\n\n    model.train(training_batches, evaluation_batches_fn)\n\n\nif __name__ == ""__main__"":\n    for run_params in parameters():\n        run(**run_params)\n'"
tensorflow/mean_teacher/__init__.py,0,b''
tensorflow/mean_teacher/framework.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""Tools for building Tensorflow graphs""\n\nfrom contextlib import contextmanager\n\nimport tensorflow as tf\n\n\nclass HyperparamVariables:\n    def __init__(self, hyperparams, name_or_scope=None):\n        self.variables = {}\n        self.placeholders = {}\n        self.assign_ops = {}\n\n        with tf.variable_scope(name_or_scope, ""hyperparams""):\n            for name, default in hyperparams.items():\n                variable = tf.Variable(default, name=name, trainable=False)\n                tf.add_to_collection(""hyperparams"", variable)\n                placeholder = tf.placeholder(dtype=variable.dtype,\n                                             shape=variable.get_shape(),\n                                             name=(name + ""/placeholder""))\n                assign_op = tf.assign(variable, placeholder, name=(name + ""/assign""))\n\n                assert name not in self.variables\n                self.variables[name] = variable\n                self.placeholders[name] = placeholder\n                self.assign_ops[name] = assign_op\n\n    def __getitem__(self, name):\n        ""Get the TF tensor representing the hyperparameter""\n        return self.variables[name]\n\n    def get(self, session, name):\n        ""Get the current value of the given hyperparameter in the given session""\n        return session.run(self.variables[name])\n\n    def assign(self, session, name, value):\n        ""Change the value of the given hyperparameter in the given session""\n        return session.run(self.assign_ops[name], {self.placeholders[name]: value})\n\n\n@contextmanager\ndef name_variable_scope(name_scope_name,\n                        var_scope_or_var_scope_name,\n                        *var_scope_args,\n                        **var_scope_kwargs):\n    """"""A combination of name_scope and variable_scope with different names\n\n    The tf.variable_scope function creates both a name_scope and a variable_scope\n    with identical names. But the naming would often be clearer if the names\n    of operations didn\'t inherit the scope name of the (reused) variables.\n    So use this function to make shorter and more logical scope names in these cases.\n    """"""\n    with tf.name_scope(name_scope_name) as outer_name_scope:\n        with tf.variable_scope(var_scope_or_var_scope_name,\n                               *var_scope_args,\n                               **var_scope_kwargs) as var_scope:\n            with tf.name_scope(outer_name_scope) as inner_name_scope:\n                yield inner_name_scope, var_scope\n\n\n@contextmanager\ndef ema_variable_scope(name_scope_name, var_scope, decay=0.999):\n    """"""Scope that replaces trainable variables with their exponential moving averages\n\n    We capture only trainable variables. There\'s no reason we couldn\'t support\n    other types of variables, but the assumed use case is for trainable variables.\n    """"""\n    with tf.name_scope(name_scope_name + ""/ema_variables""):\n        original_trainable_vars = {\n            tensor.op.name: tensor\n            for tensor\n            in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=var_scope.name)\n        }\n        ema = tf.train.ExponentialMovingAverage(decay)\n        update_op = ema.apply(original_trainable_vars.values())\n        tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_op)\n\n    def use_ema_variables(getter, name, *_, **__):\n        #pylint: disable=unused-argument\n        assert name in original_trainable_vars, ""Unknown variable {}."".format(name)\n        return ema.average(original_trainable_vars[name])\n\n    with name_variable_scope(name_scope_name,\n                             var_scope,\n                             custom_getter=use_ema_variables) as (name_scope, var_scope):\n        yield name_scope, var_scope\n\n\ndef assert_shape(tensor, expected_shape):\n    tensor_shape = tensor.get_shape().as_list()\n    error_message = ""tensor {name} shape {actual} != {expected}""\n    assert tensor_shape == expected_shape, error_message.format(\n        name=tensor.name, actual=tensor_shape, expected=expected_shape)\n'"
tensorflow/mean_teacher/minibatching.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nfrom itertools import islice, chain\n\nimport numpy as np\n\n\ndef evaluation_epoch_generator(data, batch_size=100):\n    def generate():\n        for idx in range(0, len(data), batch_size):\n            yield data[idx:(idx + batch_size)]\n    return generate\n\n\ndef training_batches(data, batch_size=100, n_labeled_per_batch=\'vary\', random=np.random):\n    if n_labeled_per_batch == \'vary\':\n        return eternal_batches(data, batch_size, random)\n    elif n_labeled_per_batch == batch_size:\n        labeled_data, _ = split_labeled(data)\n        return eternal_batches(labeled_data, batch_size, random)\n    else:\n        assert 0 < n_labeled_per_batch < batch_size\n        n_unlabeled_per_batch = batch_size - n_labeled_per_batch\n        labeled_data, _ = split_labeled(data)\n        return combine_batches(\n            eternal_batches(labeled_data, n_labeled_per_batch, random),\n            unlabel_batches(eternal_batches(data, n_unlabeled_per_batch, random))\n        )\n\n\ndef split_labeled(data):\n    is_labeled = (data[\'y\'] != -1)\n    return data[is_labeled], data[~is_labeled]\n\n\ndef combine_batches(*batch_generators):\n    return (np.concatenate(batches) for batches in zip(*batch_generators))\n\n\ndef eternal_batches(data, batch_size=100, random=np.random):\n    assert batch_size > 0 and len(data) > 0\n    for batch_idxs in eternal_random_index_batches(len(data), batch_size, random):\n        yield data[batch_idxs]\n\n\ndef unlabel_batches(batch_generator):\n    for batch in batch_generator:\n        batch[""y""] = -1\n        yield batch\n\n\ndef eternal_random_index_batches(max_index, batch_size, random=np.random):\n    def random_ranges():\n        while True:\n            indices = np.arange(max_index)\n            random.shuffle(indices)\n            yield indices\n\n    def batch_slices(iterable):\n        while True:\n            yield np.array(list(islice(iterable, batch_size)))\n\n    eternal_random_indices = chain.from_iterable(random_ranges())\n    return batch_slices(eternal_random_indices)\n'"
tensorflow/mean_teacher/model.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""Mean teacher model""\n\nimport logging\nimport os\nfrom collections import namedtuple\n\nimport tensorflow as tf\nfrom tensorflow.contrib import metrics, slim\nfrom tensorflow.contrib.metrics import streaming_mean\n\nfrom . import nn\nfrom . import weight_norm as wn\nfrom .framework import ema_variable_scope, name_variable_scope, assert_shape, HyperparamVariables\nfrom . import string_utils\n\n\nLOG = logging.getLogger(\'main\')\n\n\nclass Model:\n    DEFAULT_HYPERPARAMS = {\n        # Consistency hyperparameters\n        \'ema_consistency\': True,\n        \'apply_consistency_to_labeled\': True,\n        \'max_consistency_cost\': 100.0,\n        \'ema_decay_during_rampup\': 0.99,\n        \'ema_decay_after_rampup\': 0.999,\n        \'consistency_trust\': 0.0,\n        \'num_logits\': 1, # Either 1 or 2\n        \'logit_distance_cost\': 0.0, # Matters only with 2 outputs\n\n        # Optimizer hyperparameters\n        \'max_learning_rate\': 0.003,\n        \'adam_beta_1_before_rampdown\': 0.9,\n        \'adam_beta_1_after_rampdown\': 0.5,\n        \'adam_beta_2_during_rampup\': 0.99,\n        \'adam_beta_2_after_rampup\': 0.999,\n        \'adam_epsilon\': 1e-8,\n\n        # Architecture hyperparameters\n        \'input_noise\': 0.15,\n        \'student_dropout_probability\': 0.5,\n        \'teacher_dropout_probability\': 0.5,\n\n        # Training schedule\n        \'rampup_length\': 40000,\n        \'rampdown_length\': 25000,\n        \'training_length\': 150000,\n\n        # Input augmentation\n        \'flip_horizontally\': False,\n        \'translate\': True,\n\n        # Whether to scale each input image to mean=0 and std=1 per channel\n        # Use False if input is already normalized in some other way\n        \'normalize_input\': True,\n\n        # Output schedule\n        \'print_span\': 20,\n        \'evaluation_span\': 500,\n    }\n\n    #pylint: disable=too-many-instance-attributes\n    def __init__(self, run_context=None):\n        if run_context is not None:\n            self.training_log = run_context.create_train_log(\'training\')\n            self.validation_log = run_context.create_train_log(\'validation\')\n            self.checkpoint_path = os.path.join(run_context.transient_dir, \'checkpoint\')\n            self.tensorboard_path = os.path.join(run_context.result_dir, \'tensorboard\')\n\n        with tf.name_scope(""placeholders""):\n            self.images = tf.placeholder(dtype=tf.float32, shape=(None, 32, 32, 3), name=\'images\')\n            self.labels = tf.placeholder(dtype=tf.int32, shape=(None,), name=\'labels\')\n            self.is_training = tf.placeholder(dtype=tf.bool, shape=(), name=\'is_training\')\n\n        self.global_step = tf.Variable(0, trainable=False, name=\'global_step\')\n        tf.add_to_collection(""init_in_init"", self.global_step)\n        self.hyper = HyperparamVariables(self.DEFAULT_HYPERPARAMS)\n        for var in self.hyper.variables.values():\n            tf.add_to_collection(""init_in_init"", var)\n\n        with tf.name_scope(""ramps""):\n            sigmoid_rampup_value = sigmoid_rampup(self.global_step, self.hyper[\'rampup_length\'])\n            sigmoid_rampdown_value = sigmoid_rampdown(self.global_step,\n                                                      self.hyper[\'rampdown_length\'],\n                                                      self.hyper[\'training_length\'])\n            self.learning_rate = tf.multiply(sigmoid_rampup_value * sigmoid_rampdown_value,\n                                             self.hyper[\'max_learning_rate\'],\n                                             name=\'learning_rate\')\n            self.adam_beta_1 = tf.add(sigmoid_rampdown_value * self.hyper[\'adam_beta_1_before_rampdown\'],\n                                      (1 - sigmoid_rampdown_value) * self.hyper[\'adam_beta_1_after_rampdown\'],\n                                      name=\'adam_beta_1\')\n            self.cons_coefficient = tf.multiply(sigmoid_rampup_value,\n                                                self.hyper[\'max_consistency_cost\'],\n                                                name=\'consistency_coefficient\')\n\n            step_rampup_value = step_rampup(self.global_step, self.hyper[\'rampup_length\'])\n            self.adam_beta_2 = tf.add((1 - step_rampup_value) * self.hyper[\'adam_beta_2_during_rampup\'],\n                                      step_rampup_value * self.hyper[\'adam_beta_2_after_rampup\'],\n                                      name=\'adam_beta_2\')\n            self.ema_decay = tf.add((1 - step_rampup_value) * self.hyper[\'ema_decay_during_rampup\'],\n                                    step_rampup_value * self.hyper[\'ema_decay_after_rampup\'],\n                                    name=\'ema_decay\')\n\n        (\n            (self.class_logits_1, self.cons_logits_1),\n            (self.class_logits_2, self.cons_logits_2),\n            (self.class_logits_ema, self.cons_logits_ema)\n        ) = inference(\n            self.images,\n            is_training=self.is_training,\n            ema_decay=self.ema_decay,\n            input_noise=self.hyper[\'input_noise\'],\n            student_dropout_probability=self.hyper[\'student_dropout_probability\'],\n            teacher_dropout_probability=self.hyper[\'teacher_dropout_probability\'],\n            normalize_input=self.hyper[\'normalize_input\'],\n            flip_horizontally=self.hyper[\'flip_horizontally\'],\n            translate=self.hyper[\'translate\'],\n            num_logits=self.hyper[\'num_logits\'])\n\n        with tf.name_scope(""objectives""):\n            self.mean_error_1, self.errors_1 = errors(self.class_logits_1, self.labels)\n            self.mean_error_ema, self.errors_ema = errors(self.class_logits_ema, self.labels)\n\n            self.mean_class_cost_1, self.class_costs_1 = classification_costs(\n                self.class_logits_1, self.labels)\n            self.mean_class_cost_ema, self.class_costs_ema = classification_costs(\n                self.class_logits_ema, self.labels)\n\n            labeled_consistency = self.hyper[\'apply_consistency_to_labeled\']\n            consistency_mask = tf.logical_or(tf.equal(self.labels, -1), labeled_consistency)\n            self.mean_cons_cost_pi, self.cons_costs_pi = consistency_costs(\n                self.cons_logits_1, self.class_logits_2, self.cons_coefficient, consistency_mask, self.hyper[\'consistency_trust\'])\n            self.mean_cons_cost_mt, self.cons_costs_mt = consistency_costs(\n                self.cons_logits_1, self.class_logits_ema, self.cons_coefficient, consistency_mask, self.hyper[\'consistency_trust\'])\n\n\n            def l2_norms(matrix):\n                l2s = tf.reduce_sum(matrix ** 2, axis=1)\n                mean_l2 = tf.reduce_mean(l2s)\n                return mean_l2, l2s\n\n            self.mean_res_l2_1, self.res_l2s_1 = l2_norms(self.class_logits_1 - self.cons_logits_1)\n            self.mean_res_l2_ema, self.res_l2s_ema = l2_norms(self.class_logits_ema - self.cons_logits_ema)\n            self.res_costs_1 = self.hyper[\'logit_distance_cost\'] * self.res_l2s_1\n            self.mean_res_cost_1 = tf.reduce_mean(self.res_costs_1)\n            self.res_costs_ema = self.hyper[\'logit_distance_cost\'] * self.res_l2s_ema\n            self.mean_res_cost_ema = tf.reduce_mean(self.res_costs_ema)\n\n            self.mean_total_cost_pi, self.total_costs_pi = total_costs(\n                self.class_costs_1, self.cons_costs_pi, self.res_costs_1)\n            self.mean_total_cost_mt, self.total_costs_mt = total_costs(\n                self.class_costs_1, self.cons_costs_mt, self.res_costs_1)\n            assert_shape(self.total_costs_pi, [3])\n            assert_shape(self.total_costs_mt, [3])\n\n            self.cost_to_be_minimized = tf.cond(self.hyper[\'ema_consistency\'],\n                                                lambda: self.mean_total_cost_mt,\n                                                lambda: self.mean_total_cost_pi)\n\n        with tf.name_scope(""train_step""):\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            with tf.control_dependencies(update_ops):\n                self.train_step_op = nn.adam_optimizer(self.cost_to_be_minimized,\n                                                       self.global_step,\n                                                       learning_rate=self.learning_rate,\n                                                       beta1=self.adam_beta_1,\n                                                       beta2=self.adam_beta_2,\n                                                       epsilon=self.hyper[\'adam_epsilon\'])\n\n        self.training_control = training_control(self.global_step,\n                                                 self.hyper[\'print_span\'],\n                                                 self.hyper[\'evaluation_span\'],\n                                                 self.hyper[\'training_length\'])\n\n        self.training_metrics = {\n            ""learning_rate"": self.learning_rate,\n            ""adam_beta_1"": self.adam_beta_1,\n            ""adam_beta_2"": self.adam_beta_2,\n            ""ema_decay"": self.ema_decay,\n            ""cons_coefficient"": self.cons_coefficient,\n            ""train/error/1"": self.mean_error_1,\n            ""train/error/ema"": self.mean_error_ema,\n            ""train/class_cost/1"": self.mean_class_cost_1,\n            ""train/class_cost/ema"": self.mean_class_cost_ema,\n            ""train/cons_cost/pi"": self.mean_cons_cost_pi,\n            ""train/cons_cost/mt"": self.mean_cons_cost_mt,\n            ""train/res_cost/1"": self.mean_res_cost_1,\n            ""train/res_cost/ema"": self.mean_res_cost_ema,\n            ""train/total_cost/pi"": self.mean_total_cost_pi,\n            ""train/total_cost/mt"": self.mean_total_cost_mt,\n        }\n\n        with tf.variable_scope(""validation_metrics"") as metrics_scope:\n            self.metric_values, self.metric_update_ops = metrics.aggregate_metric_map({\n                ""eval/error/1"": streaming_mean(self.errors_1),\n                ""eval/error/ema"": streaming_mean(self.errors_ema),\n                ""eval/class_cost/1"": streaming_mean(self.class_costs_1),\n                ""eval/class_cost/ema"": streaming_mean(self.class_costs_ema),\n                ""eval/res_cost/1"": streaming_mean(self.res_costs_1),\n                ""eval/res_cost/ema"": streaming_mean(self.res_costs_ema),\n            })\n            metric_variables = slim.get_local_variables(scope=metrics_scope.name)\n            self.metric_init_op = tf.variables_initializer(metric_variables)\n\n        self.result_formatter = string_utils.DictFormatter(\n            order=[""eval/error/ema"", ""error/1"", ""class_cost/1"", ""cons_cost/mt""],\n            default_format=\'{name}: {value:>10.6f}\',\n            separator="",  "")\n        self.result_formatter.add_format(\'error\', \'{name}: {value:>6.1%}\')\n\n        with tf.name_scope(""initializers""):\n            init_init_variables = tf.get_collection(""init_in_init"")\n            train_init_variables = [\n                var for var in tf.global_variables() if var not in init_init_variables\n            ]\n            self.init_init_op = tf.variables_initializer(init_init_variables)\n            self.train_init_op = tf.variables_initializer(train_init_variables)\n\n        self.saver = tf.train.Saver()\n        self.session = tf.Session()\n        self.run(self.init_init_op)\n\n    def __setitem__(self, key, value):\n        self.hyper.assign(self.session, key, value)\n\n    def __getitem__(self, key):\n        return self.hyper.get(self.session, key)\n\n    def train(self, training_batches, evaluation_batches_fn):\n        self.run(self.train_init_op, self.feed_dict(next(training_batches)))\n        LOG.info(""Model variables initialized"")\n        self.evaluate(evaluation_batches_fn)\n        self.save_checkpoint()\n        for batch in training_batches:\n            results, _ = self.run([self.training_metrics, self.train_step_op],\n                                  self.feed_dict(batch))\n            step_control = self.get_training_control()\n            self.training_log.record(step_control[\'step\'], {**results, **step_control})\n            if step_control[\'time_to_print\']:\n                LOG.info(""step %5d:   %s"", step_control[\'step\'], self.result_formatter.format_dict(results))\n            if step_control[\'time_to_stop\']:\n                break\n            if step_control[\'time_to_evaluate\']:\n                self.evaluate(evaluation_batches_fn)\n                self.save_checkpoint()\n        self.evaluate(evaluation_batches_fn)\n        self.save_checkpoint()\n\n    def evaluate(self, evaluation_batches_fn):\n        self.run(self.metric_init_op)\n        for batch in evaluation_batches_fn():\n            self.run(self.metric_update_ops,\n                     feed_dict=self.feed_dict(batch, is_training=False))\n        step = self.run(self.global_step)\n        results = self.run(self.metric_values)\n        self.validation_log.record(step, results)\n        LOG.info(""step %5d:   %s"", step, self.result_formatter.format_dict(results))\n\n    def get_training_control(self):\n        return self.session.run(self.training_control)\n\n    def run(self, *args, **kwargs):\n        return self.session.run(*args, **kwargs)\n\n    def feed_dict(self, batch, is_training=True):\n        return {\n            self.images: batch[\'x\'],\n            self.labels: batch[\'y\'],\n            self.is_training: is_training\n        }\n\n    def save_checkpoint(self):\n        path = self.saver.save(self.session, self.checkpoint_path, global_step=self.global_step)\n        LOG.info(""Saved checkpoint: %r"", path)\n\n    def save_tensorboard_graph(self):\n        writer = tf.summary.FileWriter(self.tensorboard_path)\n        writer.add_graph(self.session.graph)\n        return writer.get_logdir()\n\n\nHyperparam = namedtuple(""Hyperparam"", [\'tensor\', \'getter\', \'setter\'])\n\n\ndef training_control(global_step, print_span, evaluation_span, max_step, name=None):\n    with tf.name_scope(name, ""training_control""):\n        return {\n            ""step"": global_step,\n            ""time_to_print"": tf.equal(tf.mod(global_step, print_span), 0),\n            ""time_to_evaluate"": tf.equal(tf.mod(global_step, evaluation_span), 0),\n            ""time_to_stop"": tf.greater_equal(global_step, max_step),\n        }\n\n\ndef step_rampup(global_step, rampup_length):\n    result = tf.cond(global_step < rampup_length,\n                     lambda: tf.constant(0.0),\n                     lambda: tf.constant(1.0))\n    return tf.identity(result, name=""step_rampup"")\n\n\ndef sigmoid_rampup(global_step, rampup_length):\n    global_step = tf.to_float(global_step)\n    rampup_length = tf.to_float(rampup_length)\n    def ramp():\n        phase = 1.0 - tf.maximum(0.0, global_step) / rampup_length\n        return tf.exp(-5.0 * phase * phase)\n\n    result = tf.cond(global_step < rampup_length, ramp, lambda: tf.constant(1.0))\n    return tf.identity(result, name=""sigmoid_rampup"")\n\n\ndef sigmoid_rampdown(global_step, rampdown_length, training_length):\n    global_step = tf.to_float(global_step)\n    rampdown_length = tf.to_float(rampdown_length)\n    training_length = tf.to_float(training_length)\n    def ramp():\n        phase = 1.0 - tf.maximum(0.0, training_length - global_step) / rampdown_length\n        return tf.exp(-12.5 * phase * phase)\n\n    result = tf.cond(global_step >= training_length - rampdown_length,\n                     ramp,\n                     lambda: tf.constant(1.0))\n    return tf.identity(result, name=""sigmoid_rampdown"")\n\n\ndef inference(inputs, is_training, ema_decay, input_noise, student_dropout_probability, teacher_dropout_probability,\n              normalize_input, flip_horizontally, translate, num_logits):\n    tower_args = dict(inputs=inputs,\n                      is_training=is_training,\n                      input_noise=input_noise,\n                      normalize_input=normalize_input,\n                      flip_horizontally=flip_horizontally,\n                      translate=translate,\n                      num_logits=num_logits)\n\n    with tf.variable_scope(""initialization"") as var_scope:\n        _ = tower(**tower_args, dropout_probability=student_dropout_probability, is_initialization=True)\n    with name_variable_scope(""primary"", var_scope, reuse=True) as (name_scope, _):\n        class_logits_1, cons_logits_1 = tower(**tower_args, dropout_probability=student_dropout_probability, name=name_scope)\n    with name_variable_scope(""secondary"", var_scope, reuse=True) as (name_scope, _):\n        class_logits_2, cons_logits_2 = tower(**tower_args, dropout_probability=teacher_dropout_probability, name=name_scope)\n    with ema_variable_scope(""ema"", var_scope, decay=ema_decay):\n        class_logits_ema, cons_logits_ema = tower(**tower_args, dropout_probability=teacher_dropout_probability, name=name_scope)\n        class_logits_ema, cons_logits_ema = tf.stop_gradient(class_logits_ema), tf.stop_gradient(cons_logits_ema)\n    return (class_logits_1, cons_logits_1), (class_logits_2, cons_logits_2), (class_logits_ema, cons_logits_ema)\n\n\ndef tower(inputs,\n          is_training,\n          dropout_probability,\n          input_noise,\n          normalize_input,\n          flip_horizontally,\n          translate,\n          num_logits,\n          is_initialization=False,\n          name=None):\n    with tf.name_scope(name, ""tower""):\n        default_conv_args = dict(\n            padding=\'SAME\',\n            kernel_size=[3, 3],\n            activation_fn=nn.lrelu,\n            init=is_initialization\n        )\n        training_mode_funcs = [\n            nn.random_translate, nn.flip_randomly, nn.gaussian_noise, slim.dropout,\n            wn.fully_connected, wn.conv2d\n        ]\n        training_args = dict(\n            is_training=is_training\n        )\n\n        with \\\n        slim.arg_scope([wn.conv2d], **default_conv_args), \\\n        slim.arg_scope(training_mode_funcs, **training_args):\n            #pylint: disable=no-value-for-parameter\n            net = inputs\n            assert_shape(net, [None, 32, 32, 3])\n\n            net = tf.cond(normalize_input,\n                          lambda: slim.layer_norm(net,\n                                                  scale=False,\n                                                  center=False,\n                                                  scope=\'normalize_inputs\'),\n                          lambda: net)\n            assert_shape(net, [None, 32, 32, 3])\n\n            net = nn.flip_randomly(net,\n                                   horizontally=flip_horizontally,\n                                   vertically=False,\n                                   name=\'random_flip\')\n            net = tf.cond(translate,\n                          lambda: nn.random_translate(net, scale=2, name=\'random_translate\'),\n                          lambda: net)\n            net = nn.gaussian_noise(net, scale=input_noise, name=\'gaussian_noise\')\n\n            net = wn.conv2d(net, 128, scope=""conv_1_1"")\n            net = wn.conv2d(net, 128, scope=""conv_1_2"")\n            net = wn.conv2d(net, 128, scope=""conv_1_3"")\n            net = slim.max_pool2d(net, [2, 2], scope=\'max_pool_1\')\n            net = slim.dropout(net, 1 - dropout_probability, scope=\'dropout_probability_1\')\n            assert_shape(net, [None, 16, 16, 128])\n\n            net = wn.conv2d(net, 256, scope=""conv_2_1"")\n            net = wn.conv2d(net, 256, scope=""conv_2_2"")\n            net = wn.conv2d(net, 256, scope=""conv_2_3"")\n            net = slim.max_pool2d(net, [2, 2], scope=\'max_pool_2\')\n            net = slim.dropout(net, 1 - dropout_probability, scope=\'dropout_probability_2\')\n            assert_shape(net, [None, 8, 8, 256])\n\n            net = wn.conv2d(net, 512, padding=\'VALID\', scope=""conv_3_1"")\n            assert_shape(net, [None, 6, 6, 512])\n            net = wn.conv2d(net, 256, kernel_size=[1, 1], scope=""conv_3_2"")\n            net = wn.conv2d(net, 128, kernel_size=[1, 1], scope=""conv_3_3"")\n            net = slim.avg_pool2d(net, [6, 6], scope=\'avg_pool\')\n            assert_shape(net, [None, 1, 1, 128])\n\n            net = slim.flatten(net)\n            assert_shape(net, [None, 128])\n\n            primary_logits = wn.fully_connected(net, 10, init=is_initialization)\n            secondary_logits = wn.fully_connected(net, 10, init=is_initialization)\n\n            with tf.control_dependencies([tf.assert_greater_equal(num_logits, 1),\n                                          tf.assert_less_equal(num_logits, 2)]):\n                secondary_logits = tf.case([\n                    (tf.equal(num_logits, 1), lambda: primary_logits),\n                    (tf.equal(num_logits, 2), lambda: secondary_logits),\n                ], exclusive=True, default=lambda: primary_logits)\n\n            assert_shape(primary_logits, [None, 10])\n            assert_shape(secondary_logits, [None, 10])\n            return primary_logits, secondary_logits\n\n\ndef errors(logits, labels, name=None):\n    """"""Compute error mean and whether each unlabeled example is erroneous\n\n    Assume unlabeled examples have label == -1.\n    Compute the mean error over unlabeled examples.\n    Mean error is NaN if there are no unlabeled examples.\n    Note that unlabeled examples are treated differently in cost calculation.\n    """"""\n    with tf.name_scope(name, ""errors"") as scope:\n        applicable = tf.not_equal(labels, -1)\n        labels = tf.boolean_mask(labels, applicable)\n        logits = tf.boolean_mask(logits, applicable)\n        predictions = tf.argmax(logits, -1)\n        labels = tf.cast(labels, tf.int64)\n        per_sample = tf.to_float(tf.not_equal(predictions, labels))\n        mean = tf.reduce_mean(per_sample, name=scope)\n        return mean, per_sample\n\n\ndef classification_costs(logits, labels, name=None):\n    """"""Compute classification cost mean and classification cost per sample\n\n    Assume unlabeled examples have label == -1. For unlabeled examples, cost == 0.\n    Compute the mean over all examples.\n    Note that unlabeled examples are treated differently in error calculation.\n    """"""\n    with tf.name_scope(name, ""classification_costs"") as scope:\n        applicable = tf.not_equal(labels, -1)\n\n        # Change -1s to zeros to make cross-entropy computable\n        labels = tf.where(applicable, labels, tf.zeros_like(labels))\n\n        # This will now have incorrect values for unlabeled examples\n        per_sample = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n\n        # Retain costs only for labeled\n        per_sample = tf.where(applicable, per_sample, tf.zeros_like(per_sample))\n\n        # Take mean over all examples, not just labeled examples.\n        labeled_sum = tf.reduce_sum(per_sample)\n        total_count = tf.to_float(tf.shape(per_sample)[0])\n        mean = tf.div(labeled_sum, total_count, name=scope)\n\n        return mean, per_sample\n\n\ndef consistency_costs(logits1, logits2, cons_coefficient, mask, consistency_trust, name=None):\n    """"""Takes a softmax of the logits and returns their distance as described below\n\n    Consistency_trust determines the distance metric to use\n    - trust=0: MSE\n    - 0 < trust < 1: a scaled KL-divergence but both sides mixtured with\n      a uniform distribution with given trust used as the mixture weight\n    - trust=1: scaled KL-divergence\n\n    When trust > 0, the cost is scaled to make the gradients\n    the same size as MSE when trust -> 0. The scaling factor used is\n    2 * (1 - 1/num_classes) / num_classes**2 / consistency_trust**2 .\n    To have consistency match the strength of classification, use\n    consistency coefficient = num_classes**2 / (1 - 1/num_classes) / 2\n    which is 55.5555... when num_classes=10.\n\n    Two potential stumbling blokcs:\n    - When trust=0, this gives gradients to both logits, but when trust > 0\n      this gives gradients only towards the first logit.\n      So do not use trust > 0 with the Pi model.\n    - Numerics may be unstable when 0 < trust < 1.\n    """"""\n\n    with tf.name_scope(name, ""consistency_costs"") as scope:\n        num_classes = 10\n        assert_shape(logits1, [None, num_classes])\n        assert_shape(logits2, [None, num_classes])\n        assert_shape(cons_coefficient, [])\n        softmax1 = tf.nn.softmax(logits1)\n        softmax2 = tf.nn.softmax(logits2)\n\n        kl_cost_multiplier = 2 * (1 - 1/num_classes) / num_classes**2 / consistency_trust**2\n\n        def pure_mse():\n            costs = tf.reduce_mean((softmax1 - softmax2) ** 2, -1)\n            return costs\n\n        def pure_kl():\n            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits1, labels=softmax2)\n            entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits2, labels=softmax2)\n            costs = cross_entropy - entropy\n            costs = costs * kl_cost_multiplier\n            return costs\n\n        def mixture_kl():\n            with tf.control_dependencies([tf.assert_greater(consistency_trust, 0.0),\n                                          tf.assert_less(consistency_trust, 1.0)]):\n                uniform = tf.constant(1 / num_classes, shape=[num_classes])\n                mixed_softmax1 = consistency_trust * softmax1 + (1 - consistency_trust) * uniform\n                mixed_softmax2 = consistency_trust * softmax2 + (1 - consistency_trust) * uniform\n                costs = tf.reduce_sum(mixed_softmax2 * tf.log(mixed_softmax2 / mixed_softmax1), axis=1)\n                costs = costs * kl_cost_multiplier\n                return costs\n\n        costs = tf.case([\n            (tf.equal(consistency_trust, 0.0), pure_mse),\n            (tf.equal(consistency_trust, 1.0), pure_kl)\n        ], default=mixture_kl)\n\n        costs = costs * tf.to_float(mask) * cons_coefficient\n        mean_cost = tf.reduce_mean(costs, name=scope)\n        assert_shape(costs, [None])\n        assert_shape(mean_cost, [])\n        return mean_cost, costs\n\n\ndef total_costs(*all_costs, name=None):\n    with tf.name_scope(name, ""total_costs"") as scope:\n        for cost in all_costs:\n            assert_shape(cost, [None])\n        costs = tf.reduce_sum(all_costs, axis=1)\n        mean_cost = tf.reduce_mean(costs, name=scope)\n        return mean_cost, costs\n'"
tensorflow/mean_teacher/nn.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""Functions for building neural networks with Tensorflow""\n\nimport logging\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nfrom .framework import assert_shape\n\n\nLOG = logging.getLogger(\'main\')\n\n\n@slim.add_arg_scope\ndef gaussian_noise(inputs, scale, is_training, name=None):\n    with tf.name_scope(name, \'gaussian_noise\', [inputs, scale, is_training]) as scope:\n        def do_add():\n            noise = tf.random_normal(tf.shape(inputs))\n            return inputs + noise * scale\n        return tf.cond(is_training, do_add, lambda: inputs, name=scope)\n\n\n@slim.add_arg_scope\ndef flip_randomly(inputs, horizontally, vertically, is_training, name=None):\n    """"""Flip images randomly. Make separate flipping decision for each image.\n\n    Args:\n        inputs (4-D tensor): Input images (batch size, height, width, channels).\n        horizontally (bool): If True, flip horizontally with 50% probability. Otherwise, don\'t.\n        vertically (bool): If True, flip vertically with 50% probability. Otherwise, don\'t.\n        is_training (bool): If False, no flip is performed.\n        scope: A name for the operation.\n    """"""\n    with tf.name_scope(name, ""flip_randomly"") as scope:\n        batch_size, height, width, _ = tf.unstack(tf.shape(inputs))\n        vertical_choices = (tf.random_uniform([batch_size], 0, 2, tf.int32) *\n                            tf.to_int32(vertically) *\n                            tf.to_int32(is_training))\n        horizontal_choices = (tf.random_uniform([batch_size], 0, 2, tf.int32) *\n                              tf.to_int32(horizontally) *\n                              tf.to_int32(is_training))\n        vertically_flipped = tf.reverse_sequence(inputs, vertical_choices * height, 1)\n        both_flipped = tf.reverse_sequence(vertically_flipped, horizontal_choices * width, 2)\n        return tf.identity(both_flipped, name=scope)\n\n\n\n@slim.add_arg_scope\ndef random_translate(inputs, scale, is_training,\n                     padding_mode=\'REFLECT\', name=\'random_translate\'):\n    """"""Translate images by a random number of pixels\n    The dimensions of the image tensor remain the same. Padding is added where necessary, and the\n    pixels outside image area are cropped off.\n    For performance reasons, the offset values need to be integers and not Tensors.\n    Args:\n        inputs (4-D tensor): Input images (batch size, height, width, channels).\n        scale (integer): Maximum translation in pixels. For each image on the batch, a random\n            2-D translation is picked uniformly from ([-scale, scale], [-scale, scale]).\n        is_training (bool): If False, no translation is performed.\n        padding_mode (string): Either \'CONSTANT\', \'SYMMETRIC\', or \'REFLECT\'. What values to use for\n            pixels that are translated from outside the original image. This parameter is passed\n            directly to tensorflow.pad fuction.\n        scope: A name for the operation.\n    """"""\n    assert isinstance(scale, int)\n\n    with tf.name_scope(name) as scope:\n        def random_offsets(batch_size, minval, inclusive_maxval, name=\'random_offsets\'):\n            with tf.name_scope(name) as scope:\n                return tf.random_uniform([batch_size],\n                                         minval=minval, maxval=inclusive_maxval + 1,\n                                         dtype=tf.int32, name=scope)\n\n        def do_translate(name=\'do_translate\'):\n            with tf.name_scope(name) as scope:\n                batch_size = tf.shape(inputs)[0]\n                offset_heights = random_offsets(batch_size, -scale, scale, \'offset_heights\')\n                offset_widths = random_offsets(batch_size, -scale, scale, \'offset_widths\')\n                return translate(inputs, offset_heights, offset_widths,\n                                 scale, padding_mode, name=scope)\n\n        return tf.cond(is_training, do_translate, lambda: inputs, name=scope)\n\n\ndef translate(inputs, vertical_offsets, horizontal_offsets, scale, padding_mode, name=\'translate\'):\n    """"""Translate images\n\n    The dimensions of the image remain the same. Padding is added where necessary, and the\n    pixels outside image area are cropped off.\n\n    Args:\n        inputs (4-D tensor): Input images (batch size, height, width, channels).\n        vertical_offsets (1-D tensor of integers): Vertical translation in pixels for each image.\n        horizontal offsets (1-D tensor of integers): Horizontal translation in pixels.\n        scale (integer): Maximum absolute offset (needed for performance reasons).\n        padding_mode (string): Either \'CONSTANT\', \'SYMMETRIC\', or \'REFLECT\'. What values to use for\n            pixels that are translated from outside the original image. This parameter is passed\n            directly to tensorflow.pad fuction.\n    """"""\n    assert isinstance(scale, int)\n    kernel_size = 1 + 2 * scale\n    batch_size, inp_height, inp_width, channels = inputs.get_shape().as_list()\n\n    def one_hots(offsets, name=\'one_hots\'):\n        with tf.name_scope(name) as scope:\n            with tf.control_dependencies([tf.assert_less_equal(tf.abs(offsets), scale)]):\n                result = tf.expand_dims(tf.one_hot(scale - offsets, kernel_size), 1, name=scope)\n                assert_shape(result, [batch_size, 1, kernel_size])\n                return result\n\n    def assert_equal_first_dim(tensor_a, tensor_b, name=\'assert_equal_first_dim\'):\n        with tf.name_scope(name) as scope:\n            first_dims = tf.shape(tensor_a)[0], tf.shape(tensor_b)[0]\n            return tf.Assert(tf.equal(*first_dims), first_dims, name=scope)\n\n    with tf.name_scope(name) as scope:\n        with tf.control_dependencies([\n            assert_equal_first_dim(inputs, vertical_offsets, ""assert_height""),\n            assert_equal_first_dim(inputs, horizontal_offsets, ""assert_width"")\n        ]):\n            filters = tf.matmul(one_hots(vertical_offsets),\n                                one_hots(horizontal_offsets),\n                                adjoint_a=True)\n            assert_shape(filters, [batch_size, kernel_size, kernel_size])\n\n            padding_sizes = [[0, 0], [scale, scale], [scale, scale], [0, 0]]\n            padded_inp = tf.pad(inputs, padding_sizes, mode=padding_mode)\n            assert_shape(padded_inp,\n                         [batch_size, inp_height + 2 * scale, inp_width + 2 * scale, channels])\n\n            depthwise_inp = tf.transpose(padded_inp, perm=[3, 1, 2, 0])\n            assert_shape(depthwise_inp,\n                         [channels, inp_height + 2 * scale, inp_width + 2 * scale, batch_size])\n\n            depthwise_filters = tf.expand_dims(tf.transpose(filters, [1, 2, 0]), -1)\n            assert_shape(depthwise_filters, [kernel_size, kernel_size, batch_size, 1])\n\n            convoluted = tf.nn.depthwise_conv2d_native(depthwise_inp, depthwise_filters,\n                                                       strides=[1, 1, 1, 1], padding=\'VALID\')\n            assert_shape(convoluted, [channels, inp_height, inp_width, batch_size])\n\n            result = tf.transpose(convoluted, (3, 1, 2, 0), name=scope)\n            assert_shape(result, [batch_size, inp_height, inp_width, channels])\n\n            return result\n\n\ndef lrelu(inputs, leak=0.1, name=None):\n    with tf.name_scope(name, \'lrelu\') as scope:\n        return tf.maximum(inputs, leak * inputs, name=scope)\n\n\ndef adam_optimizer(cost, global_step,\n                   learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8,\n                   name=None):\n    with tf.name_scope(name, ""adam_optimizer"") as scope:\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n                                           beta1=beta1,\n                                           beta2=beta2,\n                                           epsilon=epsilon)\n        return optimizer.minimize(cost, global_step=global_step, name=scope)\n'"
tensorflow/mean_teacher/string_utils.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nimport collections\nimport re\n\n\nclass DictFormatter:\n    """"""Format dictionaries into strings\n\n    The (key, value) pairs in dictionary are ordered and formatted\n    based on the regex patterns of the key. Use format_dict method\n    to actually return the string for a dict.\n\n    Args:\n        order (array of Regexes): The order of the keys in the resulting string.\n            All the pairs whose key match the first regex are output first\n            (in arbitrary order), then all the pairs whose key matches the second regex\n            (but not the first), and so on. If a key of a pair does not match any of the\n            regexes, the pair is not included in the output.\n        default_format (string): How each of the pairs will be formatted in the output.\n            Can be overriden based on key with add_format method.\n        separator (string): How the pairs are combined in the output.\n    """"""\n\n    def __init__(self, order=None, default_format=\'{name}: {value}\', separator="", ""):\n        self.default_format = default_format\n        self.formats = []\n        self.order = order or ["".+""]\n        self.separator = separator\n\n    def add_format(self, name_regex, string_format):\n        """"""Add format for all the keys that match name_regex.\n\n        When format_dict is called, if multiple formats match a key,\n        the first one is used. If no format matches a key,\n        the default format is used.\n        """"""\n        self.formats.append((name_regex, string_format))\n\n    def format_dict(self, dictionary):\n        """"""Return formatted string presentation of the dictionary""""""\n        name_order = [\n            name\n            for order_regex in self.order\n            for name in dictionary.keys()\n            if re.search(order_regex, name)\n            ]\n        name_order = uniq(name_order)\n        strings = [self._format_single(name, dictionary[name]) for name in name_order]\n        return self.separator.join(strings)\n\n    def _format_single(self, name, value):\n        for name_regex, string_format in self.formats:\n            if re.search(name_regex, name):\n                return string_format.format(name=name, value=value)\n        return self.default_format.format(name=name, value=value)\n\n\ndef uniq(lst):\n    return collections.OrderedDict(zip(lst, lst)).keys()\n'"
tensorflow/mean_teacher/weight_norm.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""Layers with weight normalization and mean-only batch normalization\n\nSee https://arxiv.org/abs/1602.07868 (Salimans & Kingma, 2016)\n\nThe code is adapted from\nhttps://github.com/openai/pixel-cnn/blob/fc86dbce1d508fa79f8e9a7d1942d229249a5366/pixel_cnn_pp/nn.py\n""""""\n\nimport tensorflow as tf\nfrom tensorflow.contrib.framework.python.ops import add_arg_scope\n\n\n@add_arg_scope\ndef fully_connected(inputs, num_outputs,\n                    activation_fn=None, init_scale=1., init=False,\n                    eval_mean_ema_decay=0.999, is_training=None, scope=None):\n    #pylint: disable=invalid-name\n    with tf.variable_scope(scope, ""fully_connected""):\n        if is_training is None:\n            is_training = tf.constant(True)\n        if init:\n            # data based initialization of parameters\n            V = tf.get_variable(\'V\',\n                                [int(inputs.get_shape()[1]), num_outputs],\n                                tf.float32,\n                                tf.random_normal_initializer(0, 0.05),\n                                trainable=True)\n            V_norm = tf.nn.l2_normalize(V.initialized_value(), [0])\n            x_init = tf.matmul(inputs, V_norm)\n            m_init, v_init = tf.nn.moments(x_init, [0])\n            scale_init = init_scale / tf.sqrt(v_init + 1e-10)\n            g = tf.get_variable(\'g\', dtype=tf.float32,\n                                initializer=scale_init, trainable=True)\n            b = tf.get_variable(\'b\', dtype=tf.float32,\n                                initializer=tf.zeros_like(m_init), trainable=True)\n            x_init = tf.reshape(\n                scale_init, [1, num_outputs]) * (x_init - tf.reshape(m_init, [1, num_outputs]))\n            if activation_fn is not None:\n                x_init = activation_fn(x_init)\n            return x_init\n        else:\n            V, g, b = [tf.get_variable(var_name) for var_name in [\'V\', \'g\', \'b\']]\n\n            # use weight normalization (Salimans & Kingma, 2016)\n            inputs = tf.matmul(inputs, V)\n            training_mean = tf.reduce_mean(inputs, [0])\n\n            with tf.name_scope(""eval_mean"") as var_name:\n                # Note that:\n                # - We do not want to reuse eval_mean, so we take its name from the\n                #   current name_scope and create it directly with tf.Variable\n                #   instead of using tf.get_variable.\n                # - We initialize with zero to avoid initialization order difficulties.\n                #   Initializing with training_mean would probably be better.\n                eval_mean = tf.Variable(tf.zeros(shape=training_mean.get_shape()),\n                                        name=var_name,\n                                        dtype=tf.float32,\n                                        trainable=False)\n\n            def _eval_mean_update():\n                difference = (1 - eval_mean_ema_decay) * (eval_mean - training_mean)\n                return tf.assign_sub(eval_mean, difference)\n\n            def _no_eval_mean_update():\n                ""Do nothing. Must return same type as _eval_mean_update.""\n                return eval_mean\n\n            eval_mean_update = tf.cond(is_training, _eval_mean_update, _no_eval_mean_update)\n            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, eval_mean_update)\n            mean = tf.cond(is_training, lambda: training_mean, lambda: eval_mean)\n            inputs = inputs - mean\n            scaler = g / tf.sqrt(tf.reduce_sum(tf.square(V), [0]))\n            inputs = tf.reshape(scaler, [1, num_outputs]) * \\\n                inputs + tf.reshape(b, [1, num_outputs])\n\n            # apply nonlinearity\n            if activation_fn is not None:\n                inputs = activation_fn(inputs)\n            return inputs\n\n\n@add_arg_scope\ndef conv2d(inputs, num_outputs,\n           kernel_size=[3, 3], stride=[1, 1], padding=\'SAME\',\n           activation_fn=None, init_scale=1., init=False,\n           eval_mean_ema_decay=0.999, is_training=None, scope=None):\n    #pylint: disable=invalid-name\n    with tf.variable_scope(scope, ""conv2d""):\n        if is_training is None:\n            is_training = tf.constant(True)\n        if init:\n            # data based initialization of parameters\n            V = tf.get_variable(\'V\', kernel_size + [int(inputs.get_shape()[-1]), num_outputs],\n                                tf.float32, tf.random_normal_initializer(0, 0.05), trainable=True)\n            V_norm = tf.nn.l2_normalize(V.initialized_value(), [0, 1, 2])\n            x_init = tf.nn.conv2d(inputs, V_norm, [1] + stride + [1], padding)\n            m_init, v_init = tf.nn.moments(x_init, [0, 1, 2])\n            scale_init = init_scale / tf.sqrt(v_init + 1e-8)\n            g = tf.get_variable(\'g\', dtype=tf.float32,\n                                initializer=scale_init, trainable=True)\n            b = tf.get_variable(\'b\', dtype=tf.float32,\n                                initializer=tf.zeros_like(m_init), trainable=True)\n            x_init = (tf.reshape(scale_init, [1, 1, 1, num_outputs]) *\n                      (x_init - tf.reshape(m_init, [1, 1, 1, num_outputs])))\n            if activation_fn is not None:\n                x_init = activation_fn(x_init)\n            return x_init\n\n        else:\n            V, g, b = [tf.get_variable(var_name) for var_name in [\'V\', \'g\', \'b\']]\n\n            # use weight normalization (Salimans & Kingma, 2016)\n            W = (tf.reshape(g, [1, 1, 1, num_outputs]) *\n                 tf.nn.l2_normalize(V, [0, 1, 2]))\n\n            # calculate convolutional layer output\n            inputs = tf.nn.conv2d(inputs, W, [1] + stride + [1], padding)\n            training_mean = tf.reduce_mean(inputs, [0, 1, 2])\n\n            with tf.name_scope(""eval_mean"") as var_name:\n                # Note that:\n                # - We do not want to reuse eval_mean, so we take its name from the\n                #   current name_scope and create it directly with tf.Variable\n                #   instead of using tf.get_variable.\n                # - We initialize with zero to avoid initialization order difficulties.\n                #   Initializing with training_mean would probably be better.\n                eval_mean = tf.Variable(tf.zeros(shape=training_mean.get_shape()),\n                                        name=var_name,\n                                        dtype=tf.float32,\n                                        trainable=False)\n\n            def _eval_mean_update():\n                difference = (1 - eval_mean_ema_decay) * (eval_mean - training_mean)\n                return tf.assign_sub(eval_mean, difference)\n\n            def _no_eval_mean_update():\n                ""Do nothing. Must return same type as _eval_mean_update.""\n                return eval_mean\n\n            eval_mean_update = tf.cond(is_training, _eval_mean_update, _no_eval_mean_update)\n            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, eval_mean_update)\n            mean = tf.cond(is_training, lambda: training_mean, lambda: eval_mean)\n            inputs = inputs - mean\n\n            inputs = tf.nn.bias_add(inputs, b)\n\n            # apply nonlinearity\n            if activation_fn is not None:\n                inputs = activation_fn(inputs)\n            return inputs\n'"
pytorch/data-local/bin/unpack_cifar10.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nimport re\nimport os\nimport pickle\nimport sys\n\nfrom tqdm import tqdm\nfrom torchvision.datasets import CIFAR10\nimport matplotlib.image\nimport numpy as np\n\n\nwork_dir = os.path.abspath(sys.argv[1])\ntest_dir = os.path.abspath(os.path.join(sys.argv[2], \'test\'))\ntrain_dir = os.path.abspath(os.path.join(sys.argv[2], \'train+val\'))\n\ncifar10 = CIFAR10(work_dir, download=True)\n\n\ndef load_file(file_name):\n    with open(os.path.join(work_dir, cifar10.base_folder, file_name), \'rb\') as meta_f:\n        return pickle.load(meta_f, encoding=""latin1"")\n\n\ndef unpack_data_file(source_file_name, target_dir, start_idx):\n    print(""Unpacking {} to {}"".format(source_file_name, target_dir))\n    data = load_file(source_file_name)\n    for idx, (image_data, label_idx) in tqdm(enumerate(zip(data[\'data\'], data[\'labels\'])), total=len(data[\'data\'])):\n        subdir = os.path.join(target_dir, label_names[label_idx])\n        name = ""{}_{}.png"".format(start_idx + idx, label_names[label_idx])\n        os.makedirs(subdir, exist_ok=True)\n        image = np.moveaxis(image_data.reshape(3, 32, 32), 0, 2)\n        matplotlib.image.imsave(os.path.join(subdir, name), image)\n    return len(data[\'data\'])\n\n\nlabel_names = load_file(\'batches.meta\')[\'label_names\']\nprint(""Found {} label names: {}"".format(len(label_names), "", "".join(label_names)))\n\nstart_idx = 0\nfor source_file_path, _ in cifar10.test_list:\n    start_idx += unpack_data_file(source_file_path, test_dir, start_idx)\n\nstart_idx = 0\nfor source_file_path, _ in cifar10.train_list:\n    start_idx += unpack_data_file(source_file_path, train_dir, start_idx)\n'"
pytorch/mean_teacher/tests/__init__.py,0,b''
pytorch/mean_teacher/tests/test_data.py,0,"b'# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nfrom itertools import islice, chain\n\nimport numpy as np\n\nfrom ..data import TwoStreamBatchSampler\n\ndef test_two_stream_batch_sampler():\n    import sys\n    print(sys.version)\n    sampler = TwoStreamBatchSampler(primary_indices=range(10),\n                                    secondary_indices=range(-2, 0),\n                                    batch_size=3,\n                                    secondary_batch_size=1)\n    batches = list(sampler)\n\n    # All batches have length 3\n    assert all(len(batch) == 3 for batch in batches)\n\n    # All batches include two items from the primary batch\n    assert all(len([i for i in batch if i >= 0]) == 2 for batch in batches)\n\n    # All batches include one item from the secondary batch\n    assert all(len([i for i in batch if i < 0]) == 1 for batch in batches)\n\n    # All primary items are included in the epoch\n    assert len(sampler.primary_indices) % sampler.secondary_batch_size == 0 # Pre-condition\n    assert sorted(i for i in chain(*batches) if i >= 0) == list(range(10)) # Post-condition\n\n    # Secondary items are iterated through before beginning again\n    assert sorted(i for i in chain(*batches[:2]) if i < 0) == list(range(-2, 0))\n\n\ndef test_two_stream_batch_sampler_uneven():\n    import sys\n    print(sys.version)\n    sampler = TwoStreamBatchSampler(primary_indices=range(11),\n                                    secondary_indices=range(-3, 0),\n                                    batch_size=5,\n                                    secondary_batch_size=2)\n    batches = list(sampler)\n\n    # All batches have length 5\n    assert all(len(batch) == 5 for batch in batches)\n\n    # All batches include 3 items from the primary batch\n    assert all(len([i for i in batch if i >= 0]) == 3 for batch in batches)\n\n    # All batches include 2 items from the secondary batch\n    assert all(len([i for i in batch if i < 0]) == 2 for batch in batches)\n\n    # Almost all primary items are included in the epoch\n    primary_items_met = [i for i in chain(*batches) if i >= 0]\n    left_out = set(range(11)) - set(primary_items_met)\n    assert len(left_out) == 11 % 3\n\n    # Secondary items are iterated through before beginning again\n    assert sorted(i for i in chain(*batches[:3]) if i < 0) == sorted(list(range(-3, 0)) * 2)\n'"
tensorflow/datasets/tests/__init__.py,0,b''
tensorflow/datasets/tests/test_cifar10.py,0,"b""# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nimport numpy as np\n\nfrom ..cifar10 import Cifar10ZCA\n\n\ndef test_supervised_testing():\n    cifar = Cifar10ZCA(test_phase=True)\n    assert_shapes(cifar.training, 50000)\n    assert_label_range(cifar.training, range(10))\n    assert_shapes(cifar.evaluation, 10000)\n    assert_label_range(cifar.evaluation, range(10))\n\n\ndef test_semisupervised_testing():\n    cifar = Cifar10ZCA(test_phase=True, n_labeled=100)\n    assert_shapes(cifar.training, 50000)\n    assert_label_range(cifar.training, range(-1, 10))\n    assert_label_distribution(cifar.training, [49900] + [10] * 10)\n    assert_shapes(cifar.evaluation, 10000)\n    assert_label_range(cifar.evaluation, range(10))\n\n\ndef test_supervised_validation():\n    cifar = Cifar10ZCA()\n    assert_shapes(cifar.training, 45000)\n    assert_label_range(cifar.training, range(10))\n    assert_shapes(cifar.evaluation, 5000)\n    assert_label_range(cifar.evaluation, range(10))\n\n\ndef test_semisupervised_validation():\n    cifar = Cifar10ZCA(n_labeled=100)\n    assert_shapes(cifar.training, 45000)\n    assert_label_range(cifar.training, range(-1, 10))\n    assert_label_distribution(cifar.training, [44900] + [10] * 10)\n    assert_shapes(cifar.evaluation, 5000)\n    assert_label_range(cifar.evaluation, range(10))\n\n\ndef assert_shapes(data, n_expected_examples):\n    assert data['x'].shape == (n_expected_examples, 32, 32, 3)\n    assert data['y'].shape == (n_expected_examples,)\n\n\ndef assert_label_range(data, expected_range):\n    assert np.min(data['y']) == min(expected_range)\n    assert np.max(data['y']) == max(expected_range)\n\n\ndef assert_label_distribution(data, expected_distribution):\n    label_distribution = np.bincount(data['y'] + 1)\n    assert label_distribution.tolist() == expected_distribution\n"""
tensorflow/datasets/tests/test_svhn.py,0,"b""# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nimport numpy as np\n\nfrom ..svhn import SVHN\n\n\ndef test_supervised_testing():\n    svhn = SVHN(test_phase=True)\n    assert_shapes(svhn.training, 73257)\n    assert_label_range(svhn.training, range(10))\n    assert_shapes(svhn.evaluation, 26032)\n    assert_label_range(svhn.evaluation, range(10))\n\n\ndef test_semisupervised_testing():\n    svhn = SVHN(test_phase=True, n_labeled=100)\n    assert_shapes(svhn.training, 73257)\n    assert_label_range(svhn.training, range(-1, 10))\n    assert_label_distribution(svhn.training, [73157] + [10] * 10)\n    assert_shapes(svhn.evaluation, 26032)\n    assert_label_range(svhn.evaluation, range(10))\n\n\ndef test_extra_unlabeled_testing():\n    svhn = SVHN(test_phase=True, n_labeled=100, n_extra_unlabeled=10000)\n    assert_shapes(svhn.training, 83257)\n    assert_label_range(svhn.training, range(-1, 10))\n    assert_label_distribution(svhn.training, [83157] + [10] * 10)\n    assert_shapes(svhn.evaluation, 26032)\n    assert_label_range(svhn.evaluation, range(10))\n\n\ndef test_supervised_validation():\n    svhn = SVHN()\n    assert_shapes(svhn.training, 65932)\n    assert_label_range(svhn.training, range(10))\n    assert_shapes(svhn.evaluation, 7325)\n    assert_label_range(svhn.evaluation, range(10))\n\n\ndef test_semisupervised_validation():\n    svhn = SVHN(n_labeled=100)\n    assert_shapes(svhn.training, 65932)\n    assert_label_range(svhn.training, range(-1, 10))\n    assert_label_distribution(svhn.training, [65832] + [10] * 10)\n    assert_shapes(svhn.evaluation, 7325)\n    assert_label_range(svhn.evaluation, range(10))\n\n\ndef test_extra_unlabeled_validation():\n    svhn = SVHN(n_labeled=100, n_extra_unlabeled=10000)\n    assert_shapes(svhn.training, 75932)\n    assert_label_range(svhn.training, range(-1, 10))\n    assert_label_distribution(svhn.training, [75832] + [10] * 10)\n    assert_shapes(svhn.evaluation, 7325)\n    assert_label_range(svhn.evaluation, range(10))\n\n\ndef assert_shapes(data, n_expected_examples):\n    assert data['x'].shape == (n_expected_examples, 32, 32, 3)\n    assert data['y'].shape == (n_expected_examples,)\n\n\ndef assert_label_range(data, expected_range):\n    assert np.min(data['y']) == min(expected_range)\n    assert np.max(data['y']) == max(expected_range)\n\n\ndef assert_label_distribution(data, expected_distribution):\n    label_distribution = np.bincount(data['y'] + 1)\n    assert label_distribution.tolist() == expected_distribution\n"""
tensorflow/datasets/tests/test_utils.py,0,"b""# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nimport numpy as np\n\nfrom ..utils import random_balanced_partitions\n\n\ndef test_random_balanced_partition():\n    results = [\n        random_balanced_partitions(np.array(['a', 'b', 'c']), 2, [3, 5, 5])\n        for _ in range(100)\n    ]\n    results = [(a.tolist(), b.tolist()) for (a, b) in results]\n    assert (['a', 'b'], ['c']) in results\n    assert (['a', 'c'], ['b']) in results\n    assert not (['b', 'c'], ['a']) in results\n"""
tensorflow/mean_teacher/tests/__init__.py,0,b''
tensorflow/mean_teacher/tests/test_minibatching.py,0,"b""# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nfrom itertools import islice\n\nimport numpy as np\n\nfrom ..minibatching import (combine_batches, eternal_batches,\n                            eternal_random_index_batches,\n                            training_batches)\n\n\ndef test_indexes():\n    batch_generator = eternal_random_index_batches(max_index=20, batch_size=5)\n\n    first_epoch = list(list(batch) for batch in islice(batch_generator, 4))\n    second_epoch = list(list(batch) for batch in islice(batch_generator, 4))\n    first_epoch_combined = [item for batch in first_epoch for item in batch]\n    second_epoch_combined = [item for batch in second_epoch for item in batch]\n\n    assert (len(batch) == 5 for batch in first_epoch)\n    assert (len(batch) == 5 for batch in second_epoch)\n    assert sorted(first_epoch_combined) == list(range(20))\n    assert sorted(second_epoch_combined) == list(range(20))\n    assert first_epoch_combined != second_epoch_combined\n\n\ndef test_indexes_uneven_epoch():\n    batch_generator = eternal_random_index_batches(max_index=5, batch_size=2)\n\n    first_two_epochs = list(list(batch) for batch in islice(batch_generator, 5))\n    first_two_epochs_combined = [item for batch in first_two_epochs for item in batch]\n    first_half = first_two_epochs_combined[:5]\n    second_half = first_two_epochs_combined[5:]\n\n    assert sorted(first_half) == list(range(5))\n    assert sorted(second_half) == list(range(5))\n\n\ndef test_eternal_training():\n    data = np.array(['a', 'b', 'c'])\n\n    batch_generator = eternal_batches(data, batch_size=2)\n\n    first_two_epochs = list(islice(batch_generator, 3))\n    assert [len(batch) for batch in first_two_epochs] == [2, 2, 2]\n\n    first_two_epochs_combined = [item for batch in first_two_epochs for item in batch]\n    first_half = first_two_epochs_combined[:3]\n    second_half = first_two_epochs_combined[3:]\n\n    assert sorted(first_half) == ['a', 'b', 'c']\n    assert sorted(second_half) == ['a', 'b', 'c']\n\n\ndef test_batches_from_two_sets():\n    data1 = np.array(['a', 'b'])\n    data2 = np.array(['c', 'd', 'e'])\n\n    batch_generator = combine_batches(\n        eternal_batches(data1, batch_size=1),\n        eternal_batches(data2, batch_size=2)\n    )\n\n    first_six_batches = list(islice(batch_generator, 6))\n    assert [len(batch) for batch in first_six_batches] == [3, 3, 3, 3, 3, 3]\n\n    batch_portions1 = [batch[:1] for batch in first_six_batches]\n    batch_portions2 = [batch[1:] for batch in first_six_batches]\n\n    returned1 = np.concatenate(batch_portions1)\n    returned2 = np.concatenate(batch_portions2)\n\n    epochs1 = np.split(returned1, 3)\n    epochs2 = np.split(returned2, 4)\n\n    assert all(sorted(items) == ['a', 'b'] for items in epochs1)\n    assert all(sorted(items) == ['c', 'd', 'e'] for items in epochs2)\n\n\ndef test_stratified_batches():\n    data = np.array([('a', -1), ('b', 0), ('c', 1), ('d', -1), ('e', -1)],\n                    dtype=[('x', np.str_, 8), ('y', np.int32)])\n\n    assert list(data['x']) == ['a', 'b', 'c', 'd', 'e']\n    assert list(data['y']) == [-1, 0, 1, -1, -1]\n\n    batch_generator = training_batches(data, batch_size=3, n_labeled_per_batch=1)\n\n    first_ten_batches = list(islice(batch_generator, 10))\n\n    labeled_batch_portions = [batch[:1] for batch in first_ten_batches]\n    unlabeled_batch_portions = [batch[1:] for batch in first_ten_batches]\n\n    labeled_epochs = np.split(np.concatenate(labeled_batch_portions), 5)\n    unlabeled_epochs = np.split(np.concatenate(unlabeled_batch_portions), 4)\n\n    assert ([sorted(items['x'].tolist()) for items in labeled_epochs] ==\n            [['b', 'c']] * 5)\n    assert ([sorted(items['y'].tolist()) for items in labeled_epochs] ==\n            [[0, 1]] * 5)\n    assert ([sorted(items['x'].tolist()) for items in unlabeled_epochs] ==\n            [['a', 'b', 'c', 'd', 'e']] * 4)\n    assert ([sorted(items['y'].tolist()) for items in unlabeled_epochs] ==\n            [[-1, -1, -1, -1, -1]] * 4)\n"""
