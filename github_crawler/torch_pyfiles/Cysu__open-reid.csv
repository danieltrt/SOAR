file_path,api_count,code
setup.py,0,"b""from setuptools import setup, find_packages\n\n\nsetup(name='open-reid',\n      version='0.2.0',\n      description='Deep Learning Library for Person Re-identification',\n      author='Tong Xiao',\n      author_email='st.cysu@gmail.com',\n      url='https://github.com/Cysu/open-reid',\n      license='MIT',\n      install_requires=[\n          'numpy', 'scipy', 'torch', 'torchvision',\n          'six', 'h5py', 'Pillow',\n          'scikit-learn', 'metric-learn'],\n      extras_require={\n          'docs': ['sphinx', 'sphinx_rtd_theme'],\n      },\n      packages=find_packages(),\n      keywords=[\n          'Person Re-identification',\n          'Computer Vision',\n          'Deep Learning',\n      ])\n"""
docs/conf.py,1,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# Open-ReID documentation build configuration file, created by\n# sphinx-quickstart on Sun Mar 19 15:49:51 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(\'.\'))\nsys.path.insert(0, os.path.abspath(\'..\'))\n\nimport sphinx_rtd_theme\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'Open-ReID\'\ncopyright = \'2017, Tong Xiao\'\nauthor = \'Tong Xiao\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n# Use a custom css file. See https://blog.deimos.fr/2014/10/02/sphinxdoc-and-readthedocs-theme-tricks-2/\n\nhtml_theme = \'sphinx_rtd_theme\'\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\nhtml_context = {\n    \'css_files\': [\n        \'https://fonts.googleapis.com/css?family=Lato\',\n        \'_static/css/openreid_theme.css\',\n    ]\n}\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    \'collapse_navigation\': False,\n    \'display_version\': False,\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'Open-ReIDdoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'Open-ReID.tex\', \'Open-ReID Documentation\',\n     \'Tong Xiao\', \'manual\'),\n]\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'open-reid\', \'Open-ReID Documentation\',\n     [author], 1)\n]\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'Open-ReID\', \'Open-ReID Documentation\',\n     author, \'Open-ReID\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/\', None),\n    \'numpy\': (\'http://docs.scipy.org/doc/numpy/\', None),\n    \'pytorch\': (\'http://pytorch.org/docs/master/\', None),\n}\n'"
examples/oim_loss.py,4,"b'from __future__ import print_function, absolute_import\nimport argparse\nimport os.path as osp\n\nimport numpy as np\nimport sys\nimport torch\nfrom torch import nn\nfrom torch.backends import cudnn\nfrom torch.utils.data import DataLoader\n\nfrom reid import datasets\nfrom reid import models\nfrom reid.dist_metric import DistanceMetric\nfrom reid.loss import OIMLoss\nfrom reid.trainers import Trainer\nfrom reid.evaluators import Evaluator\nfrom reid.utils.data import transforms as T\nfrom reid.utils.data.preprocessor import Preprocessor\nfrom reid.utils.logging import Logger\nfrom reid.utils.serialization import load_checkpoint, save_checkpoint\n\n\ndef get_data(name, split_id, data_dir, height, width, batch_size, workers,\n             combine_trainval):\n    root = osp.join(data_dir, name)\n\n    dataset = datasets.create(name, root, split_id=split_id)\n\n    normalizer = T.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n\n    train_set = dataset.trainval if combine_trainval else dataset.train\n    num_classes = (dataset.num_trainval_ids if combine_trainval\n                   else dataset.num_train_ids)\n\n    train_transformer = T.Compose([\n        T.RandomSizedRectCrop(height, width),\n        T.RandomHorizontalFlip(),\n        T.ToTensor(),\n        normalizer,\n    ])\n\n    test_transformer = T.Compose([\n        T.RectScale(height, width),\n        T.ToTensor(),\n        normalizer,\n    ])\n\n    train_loader = DataLoader(\n        Preprocessor(train_set, root=dataset.images_dir,\n                     transform=train_transformer),\n        batch_size=batch_size, num_workers=workers,\n        shuffle=True, pin_memory=True, drop_last=True)\n\n    val_loader = DataLoader(\n        Preprocessor(dataset.val, root=dataset.images_dir,\n                     transform=test_transformer),\n        batch_size=batch_size, num_workers=workers,\n        shuffle=False, pin_memory=True)\n\n    test_loader = DataLoader(\n        Preprocessor(list(set(dataset.query) | set(dataset.gallery)),\n                     root=dataset.images_dir, transform=test_transformer),\n        batch_size=batch_size, num_workers=workers,\n        shuffle=False, pin_memory=True)\n\n    return dataset, num_classes, train_loader, val_loader, test_loader\n\n\ndef main(args):\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    cudnn.benchmark = True\n\n    # Redirect print to both console and log file\n    if not args.evaluate:\n        sys.stdout = Logger(osp.join(args.logs_dir, \'log.txt\'))\n\n    # Create data loaders\n    if args.height is None or args.width is None:\n        args.height, args.width = (144, 56) if args.arch == \'inception\' else \\\n            (256, 128)\n    dataset, num_classes, train_loader, val_loader, test_loader = \\\n        get_data(args.dataset, args.split, args.data_dir, args.height,\n                 args.width, args.batch_size, args.workers,\n                 args.combine_trainval)\n\n    # Create model\n    model = models.create(args.arch, num_features=args.features, norm=True,\n                          dropout=args.dropout)\n\n    # Load from checkpoint\n    start_epoch = best_top1 = 0\n    if args.resume:\n        checkpoint = load_checkpoint(args.resume)\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        start_epoch = checkpoint[\'epoch\']\n        best_top1 = checkpoint[\'best_top1\']\n        print(""=> Start epoch {}  best top1 {:.1%}""\n              .format(start_epoch, best_top1))\n    model = nn.DataParallel(model).cuda()\n\n    # Distance metric\n    metric = DistanceMetric(algorithm=args.dist_metric)\n\n    # Evaluator\n    evaluator = Evaluator(model)\n    if args.evaluate:\n        metric.train(model, train_loader)\n        print(""Validation:"")\n        evaluator.evaluate(val_loader, dataset.val, dataset.val, metric)\n        print(""Test:"")\n        evaluator.evaluate(test_loader, dataset.query, dataset.gallery, metric)\n        return\n\n    # Criterion\n    criterion = OIMLoss(model.module.num_features, num_classes,\n                        scalar=args.oim_scalar,\n                        momentum=args.oim_momentum).cuda()\n\n    # Optimizer\n    if hasattr(model.module, \'base\'):\n        base_param_ids = set(map(id, model.module.base.parameters()))\n        new_params = [p for p in model.parameters() if\n                      id(p) not in base_param_ids]\n        param_groups = [\n            {\'params\': model.module.base.parameters(), \'lr_mult\': 0.1},\n            {\'params\': new_params, \'lr_mult\': 1.0}]\n    else:\n        param_groups = model.parameters()\n    optimizer = torch.optim.SGD(param_groups, lr=args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay,\n                                nesterov=True)\n\n    # Trainer\n    trainer = Trainer(model, criterion)\n\n    # Schedule learning rate\n    def adjust_lr(epoch):\n        step_size = 60 if args.arch == \'inception\' else 40\n        lr = args.lr * (0.1 ** (epoch // step_size))\n        for g in optimizer.param_groups:\n            g[\'lr\'] = lr * g.get(\'lr_mult\', 1)\n\n    # Start training\n    for epoch in range(start_epoch, args.epochs):\n        adjust_lr(epoch)\n        trainer.train(epoch, train_loader, optimizer)\n        if epoch < args.start_save:\n            continue\n        top1 = evaluator.evaluate(val_loader, dataset.val, dataset.val)\n\n        is_best = top1 > best_top1\n        best_top1 = max(top1, best_top1)\n        save_checkpoint({\n            \'state_dict\': model.module.state_dict(),\n            \'epoch\': epoch + 1,\n            \'best_top1\': best_top1,\n        }, is_best, fpath=osp.join(args.logs_dir, \'checkpoint.pth.tar\'))\n\n        print(\'\\n * Finished epoch {:3d}  top1: {:5.1%}  best: {:5.1%}{}\\n\'.\n              format(epoch, top1, best_top1, \' *\' if is_best else \'\'))\n\n    # Final test\n    print(\'Test with best model:\')\n    checkpoint = load_checkpoint(osp.join(args.logs_dir, \'model_best.pth.tar\'))\n    model.module.load_state_dict(checkpoint[\'state_dict\'])\n    metric.train(model, train_loader)\n    evaluator.evaluate(test_loader, dataset.query, dataset.gallery, metric)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=""OIM loss"")\n    # data\n    parser.add_argument(\'-d\', \'--dataset\', type=str, default=\'cuhk03\',\n                        choices=datasets.names())\n    parser.add_argument(\'-b\', \'--batch-size\', type=int, default=256)\n    parser.add_argument(\'-j\', \'--workers\', type=int, default=4)\n    parser.add_argument(\'--split\', type=int, default=0)\n    parser.add_argument(\'--height\', type=int,\n                        help=""input height, default: 256 for resnet*, ""\n                             ""144 for inception"")\n    parser.add_argument(\'--width\', type=int,\n                        help=""input width, default: 128 for resnet*, ""\n                             ""56 for inception"")\n    parser.add_argument(\'--combine-trainval\', action=\'store_true\',\n                        help=""train and val sets together for training, ""\n                             ""val set alone for validation"")\n    # model\n    parser.add_argument(\'-a\', \'--arch\', type=str, default=\'resnet50\',\n                        choices=models.names())\n    parser.add_argument(\'--features\', type=int, default=128)\n    parser.add_argument(\'--dropout\', type=float, default=0.5)\n    # loss\n    parser.add_argument(\'--oim-scalar\', type=float, default=30,\n                        help=\'reciprocal of the temperature in OIM loss\')\n    parser.add_argument(\'--oim-momentum\', type=float, default=0.5,\n                        help=\'momentum for updating the LUT in OIM loss\')\n    # optimizer\n    parser.add_argument(\'--lr\', type=float, default=0.1,\n                        help=""learning rate of new parameters, for pretrained ""\n                             ""parameters it is 10 times smaller than this"")\n    parser.add_argument(\'--momentum\', type=float, default=0.9)\n    parser.add_argument(\'--weight-decay\', type=float, default=5e-4)\n    # training configs\n    parser.add_argument(\'--resume\', type=str, default=\'\', metavar=\'PATH\')\n    parser.add_argument(\'--evaluate\', action=\'store_true\',\n                        help=""evaluation only"")\n    parser.add_argument(\'--epochs\', type=int, default=50)\n    parser.add_argument(\'--start_save\', type=int, default=0,\n                        help=""start saving checkpoints after specific epoch"")\n    parser.add_argument(\'--seed\', type=int, default=1)\n    parser.add_argument(\'--print-freq\', type=int, default=1)\n    # metric learning\n    parser.add_argument(\'--dist-metric\', type=str, default=\'euclidean\',\n                        choices=[\'euclidean\', \'kissme\'])\n    # misc\n    working_dir = osp.dirname(osp.abspath(__file__))\n    parser.add_argument(\'--data-dir\', type=str, metavar=\'PATH\',\n                        default=osp.join(working_dir, \'data\'))\n    parser.add_argument(\'--logs-dir\', type=str, metavar=\'PATH\',\n                        default=osp.join(working_dir, \'logs\'))\n    main(parser.parse_args())\n'"
examples/softmax_loss.py,4,"b'from __future__ import print_function, absolute_import\nimport argparse\nimport os.path as osp\n\nimport numpy as np\nimport sys\nimport torch\nfrom torch import nn\nfrom torch.backends import cudnn\nfrom torch.utils.data import DataLoader\n\nfrom reid import datasets\nfrom reid import models\nfrom reid.dist_metric import DistanceMetric\nfrom reid.trainers import Trainer\nfrom reid.evaluators import Evaluator\nfrom reid.utils.data import transforms as T\nfrom reid.utils.data.preprocessor import Preprocessor\nfrom reid.utils.logging import Logger\nfrom reid.utils.serialization import load_checkpoint, save_checkpoint\n\n\ndef get_data(name, split_id, data_dir, height, width, batch_size, workers,\n             combine_trainval):\n    root = osp.join(data_dir, name)\n\n    dataset = datasets.create(name, root, split_id=split_id)\n\n    normalizer = T.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n\n    train_set = dataset.trainval if combine_trainval else dataset.train\n    num_classes = (dataset.num_trainval_ids if combine_trainval\n                   else dataset.num_train_ids)\n\n    train_transformer = T.Compose([\n        T.RandomSizedRectCrop(height, width),\n        T.RandomHorizontalFlip(),\n        T.ToTensor(),\n        normalizer,\n    ])\n\n    test_transformer = T.Compose([\n        T.RectScale(height, width),\n        T.ToTensor(),\n        normalizer,\n    ])\n\n    train_loader = DataLoader(\n        Preprocessor(train_set, root=dataset.images_dir,\n                     transform=train_transformer),\n        batch_size=batch_size, num_workers=workers,\n        shuffle=True, pin_memory=True, drop_last=True)\n\n    val_loader = DataLoader(\n        Preprocessor(dataset.val, root=dataset.images_dir,\n                     transform=test_transformer),\n        batch_size=batch_size, num_workers=workers,\n        shuffle=False, pin_memory=True)\n\n    test_loader = DataLoader(\n        Preprocessor(list(set(dataset.query) | set(dataset.gallery)),\n                     root=dataset.images_dir, transform=test_transformer),\n        batch_size=batch_size, num_workers=workers,\n        shuffle=False, pin_memory=True)\n\n    return dataset, num_classes, train_loader, val_loader, test_loader\n\n\ndef main(args):\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    cudnn.benchmark = True\n\n    # Redirect print to both console and log file\n    if not args.evaluate:\n        sys.stdout = Logger(osp.join(args.logs_dir, \'log.txt\'))\n\n    # Create data loaders\n    if args.height is None or args.width is None:\n        args.height, args.width = (144, 56) if args.arch == \'inception\' else \\\n                                  (256, 128)\n    dataset, num_classes, train_loader, val_loader, test_loader = \\\n        get_data(args.dataset, args.split, args.data_dir, args.height,\n                 args.width, args.batch_size, args.workers,\n                 args.combine_trainval)\n\n    # Create model\n    model = models.create(args.arch, num_features=args.features,\n                          dropout=args.dropout, num_classes=num_classes)\n\n    # Load from checkpoint\n    start_epoch = best_top1 = 0\n    if args.resume:\n        checkpoint = load_checkpoint(args.resume)\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        start_epoch = checkpoint[\'epoch\']\n        best_top1 = checkpoint[\'best_top1\']\n        print(""=> Start epoch {}  best top1 {:.1%}""\n              .format(start_epoch, best_top1))\n    model = nn.DataParallel(model).cuda()\n\n    # Distance metric\n    metric = DistanceMetric(algorithm=args.dist_metric)\n\n    # Evaluator\n    evaluator = Evaluator(model)\n    if args.evaluate:\n        metric.train(model, train_loader)\n        print(""Validation:"")\n        evaluator.evaluate(val_loader, dataset.val, dataset.val, metric)\n        print(""Test:"")\n        evaluator.evaluate(test_loader, dataset.query, dataset.gallery, metric)\n        return\n\n    # Criterion\n    criterion = nn.CrossEntropyLoss().cuda()\n\n    # Optimizer\n    if hasattr(model.module, \'base\'):\n        base_param_ids = set(map(id, model.module.base.parameters()))\n        new_params = [p for p in model.parameters() if\n                      id(p) not in base_param_ids]\n        param_groups = [\n            {\'params\': model.module.base.parameters(), \'lr_mult\': 0.1},\n            {\'params\': new_params, \'lr_mult\': 1.0}]\n    else:\n        param_groups = model.parameters()\n    optimizer = torch.optim.SGD(param_groups, lr=args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay,\n                                nesterov=True)\n\n    # Trainer\n    trainer = Trainer(model, criterion)\n\n    # Schedule learning rate\n    def adjust_lr(epoch):\n        step_size = 60 if args.arch == \'inception\' else 40\n        lr = args.lr * (0.1 ** (epoch // step_size))\n        for g in optimizer.param_groups:\n            g[\'lr\'] = lr * g.get(\'lr_mult\', 1)\n\n    # Start training\n    for epoch in range(start_epoch, args.epochs):\n        adjust_lr(epoch)\n        trainer.train(epoch, train_loader, optimizer)\n        if epoch < args.start_save:\n            continue\n        top1 = evaluator.evaluate(val_loader, dataset.val, dataset.val)\n\n        is_best = top1 > best_top1\n        best_top1 = max(top1, best_top1)\n        save_checkpoint({\n            \'state_dict\': model.module.state_dict(),\n            \'epoch\': epoch + 1,\n            \'best_top1\': best_top1,\n        }, is_best, fpath=osp.join(args.logs_dir, \'checkpoint.pth.tar\'))\n\n        print(\'\\n * Finished epoch {:3d}  top1: {:5.1%}  best: {:5.1%}{}\\n\'.\n              format(epoch, top1, best_top1, \' *\' if is_best else \'\'))\n\n    # Final test\n    print(\'Test with best model:\')\n    checkpoint = load_checkpoint(osp.join(args.logs_dir, \'model_best.pth.tar\'))\n    model.module.load_state_dict(checkpoint[\'state_dict\'])\n    metric.train(model, train_loader)\n    evaluator.evaluate(test_loader, dataset.query, dataset.gallery, metric)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=""Softmax loss classification"")\n    # data\n    parser.add_argument(\'-d\', \'--dataset\', type=str, default=\'cuhk03\',\n                        choices=datasets.names())\n    parser.add_argument(\'-b\', \'--batch-size\', type=int, default=256)\n    parser.add_argument(\'-j\', \'--workers\', type=int, default=4)\n    parser.add_argument(\'--split\', type=int, default=0)\n    parser.add_argument(\'--height\', type=int,\n                        help=""input height, default: 256 for resnet*, ""\n                             ""144 for inception"")\n    parser.add_argument(\'--width\', type=int,\n                        help=""input width, default: 128 for resnet*, ""\n                             ""56 for inception"")\n    parser.add_argument(\'--combine-trainval\', action=\'store_true\',\n                        help=""train and val sets together for training, ""\n                             ""val set alone for validation"")\n    # model\n    parser.add_argument(\'-a\', \'--arch\', type=str, default=\'resnet50\',\n                        choices=models.names())\n    parser.add_argument(\'--features\', type=int, default=128)\n    parser.add_argument(\'--dropout\', type=float, default=0.5)\n    # optimizer\n    parser.add_argument(\'--lr\', type=float, default=0.1,\n                        help=""learning rate of new parameters, for pretrained ""\n                             ""parameters it is 10 times smaller than this"")\n    parser.add_argument(\'--momentum\', type=float, default=0.9)\n    parser.add_argument(\'--weight-decay\', type=float, default=5e-4)\n    # training configs\n    parser.add_argument(\'--resume\', type=str, default=\'\', metavar=\'PATH\')\n    parser.add_argument(\'--evaluate\', action=\'store_true\',\n                        help=""evaluation only"")\n    parser.add_argument(\'--epochs\', type=int, default=50)\n    parser.add_argument(\'--start_save\', type=int, default=0,\n                        help=""start saving checkpoints after specific epoch"")\n    parser.add_argument(\'--seed\', type=int, default=1)\n    parser.add_argument(\'--print-freq\', type=int, default=1)\n    # metric learning\n    parser.add_argument(\'--dist-metric\', type=str, default=\'euclidean\',\n                        choices=[\'euclidean\', \'kissme\'])\n    # misc\n    working_dir = osp.dirname(osp.abspath(__file__))\n    parser.add_argument(\'--data-dir\', type=str, metavar=\'PATH\',\n                        default=osp.join(working_dir, \'data\'))\n    parser.add_argument(\'--logs-dir\', type=str, metavar=\'PATH\',\n                        default=osp.join(working_dir, \'logs\'))\n    main(parser.parse_args())\n'"
examples/triplet_loss.py,4,"b'from __future__ import print_function, absolute_import\nimport argparse\nimport os.path as osp\n\nimport numpy as np\nimport sys\nimport torch\nfrom torch import nn\nfrom torch.backends import cudnn\nfrom torch.utils.data import DataLoader\n\nfrom reid import datasets\nfrom reid import models\nfrom reid.dist_metric import DistanceMetric\nfrom reid.loss import TripletLoss\nfrom reid.trainers import Trainer\nfrom reid.evaluators import Evaluator\nfrom reid.utils.data import transforms as T\nfrom reid.utils.data.preprocessor import Preprocessor\nfrom reid.utils.data.sampler import RandomIdentitySampler\nfrom reid.utils.logging import Logger\nfrom reid.utils.serialization import load_checkpoint, save_checkpoint\n\n\ndef get_data(name, split_id, data_dir, height, width, batch_size, num_instances,\n             workers, combine_trainval):\n    root = osp.join(data_dir, name)\n\n    dataset = datasets.create(name, root, split_id=split_id)\n\n    normalizer = T.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n\n    train_set = dataset.trainval if combine_trainval else dataset.train\n    num_classes = (dataset.num_trainval_ids if combine_trainval\n                   else dataset.num_train_ids)\n\n    train_transformer = T.Compose([\n        T.RandomSizedRectCrop(height, width),\n        T.RandomHorizontalFlip(),\n        T.ToTensor(),\n        normalizer,\n    ])\n\n    test_transformer = T.Compose([\n        T.RectScale(height, width),\n        T.ToTensor(),\n        normalizer,\n    ])\n\n    train_loader = DataLoader(\n        Preprocessor(train_set, root=dataset.images_dir,\n                     transform=train_transformer),\n        batch_size=batch_size, num_workers=workers,\n        sampler=RandomIdentitySampler(train_set, num_instances),\n        pin_memory=True, drop_last=True)\n\n    val_loader = DataLoader(\n        Preprocessor(dataset.val, root=dataset.images_dir,\n                     transform=test_transformer),\n        batch_size=batch_size, num_workers=workers,\n        shuffle=False, pin_memory=True)\n\n    test_loader = DataLoader(\n        Preprocessor(list(set(dataset.query) | set(dataset.gallery)),\n                     root=dataset.images_dir, transform=test_transformer),\n        batch_size=batch_size, num_workers=workers,\n        shuffle=False, pin_memory=True)\n\n    return dataset, num_classes, train_loader, val_loader, test_loader\n\n\ndef main(args):\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    cudnn.benchmark = True\n\n    # Redirect print to both console and log file\n    if not args.evaluate:\n        sys.stdout = Logger(osp.join(args.logs_dir, \'log.txt\'))\n\n    # Create data loaders\n    assert args.num_instances > 1, ""num_instances should be greater than 1""\n    assert args.batch_size % args.num_instances == 0, \\\n        \'num_instances should divide batch_size\'\n    if args.height is None or args.width is None:\n        args.height, args.width = (144, 56) if args.arch == \'inception\' else \\\n                                  (256, 128)\n    dataset, num_classes, train_loader, val_loader, test_loader = \\\n        get_data(args.dataset, args.split, args.data_dir, args.height,\n                 args.width, args.batch_size, args.num_instances, args.workers,\n                 args.combine_trainval)\n\n    # Create model\n    # Hacking here to let the classifier be the last feature embedding layer\n    # Net structure: avgpool -> FC(1024) -> FC(args.features)\n    model = models.create(args.arch, num_features=1024,\n                          dropout=args.dropout, num_classes=args.features)\n\n    # Load from checkpoint\n    start_epoch = best_top1 = 0\n    if args.resume:\n        checkpoint = load_checkpoint(args.resume)\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        start_epoch = checkpoint[\'epoch\']\n        best_top1 = checkpoint[\'best_top1\']\n        print(""=> Start epoch {}  best top1 {:.1%}""\n              .format(start_epoch, best_top1))\n    model = nn.DataParallel(model).cuda()\n\n    # Distance metric\n    metric = DistanceMetric(algorithm=args.dist_metric)\n\n    # Evaluator\n    evaluator = Evaluator(model)\n    if args.evaluate:\n        metric.train(model, train_loader)\n        print(""Validation:"")\n        evaluator.evaluate(val_loader, dataset.val, dataset.val, metric)\n        print(""Test:"")\n        evaluator.evaluate(test_loader, dataset.query, dataset.gallery, metric)\n        return\n\n    # Criterion\n    criterion = TripletLoss(margin=args.margin).cuda()\n\n    # Optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr,\n                                 weight_decay=args.weight_decay)\n\n    # Trainer\n    trainer = Trainer(model, criterion)\n\n    # Schedule learning rate\n    def adjust_lr(epoch):\n        lr = args.lr if epoch <= 100 else \\\n            args.lr * (0.001 ** ((epoch - 100) / 50.0))\n        for g in optimizer.param_groups:\n            g[\'lr\'] = lr * g.get(\'lr_mult\', 1)\n\n    # Start training\n    for epoch in range(start_epoch, args.epochs):\n        adjust_lr(epoch)\n        trainer.train(epoch, train_loader, optimizer)\n        if epoch < args.start_save:\n            continue\n        top1 = evaluator.evaluate(val_loader, dataset.val, dataset.val)\n\n        is_best = top1 > best_top1\n        best_top1 = max(top1, best_top1)\n        save_checkpoint({\n            \'state_dict\': model.module.state_dict(),\n            \'epoch\': epoch + 1,\n            \'best_top1\': best_top1,\n        }, is_best, fpath=osp.join(args.logs_dir, \'checkpoint.pth.tar\'))\n\n        print(\'\\n * Finished epoch {:3d}  top1: {:5.1%}  best: {:5.1%}{}\\n\'.\n              format(epoch, top1, best_top1, \' *\' if is_best else \'\'))\n\n    # Final test\n    print(\'Test with best model:\')\n    checkpoint = load_checkpoint(osp.join(args.logs_dir, \'model_best.pth.tar\'))\n    model.module.load_state_dict(checkpoint[\'state_dict\'])\n    metric.train(model, train_loader)\n    evaluator.evaluate(test_loader, dataset.query, dataset.gallery, metric)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=""Triplet loss classification"")\n    # data\n    parser.add_argument(\'-d\', \'--dataset\', type=str, default=\'cuhk03\',\n                        choices=datasets.names())\n    parser.add_argument(\'-b\', \'--batch-size\', type=int, default=256)\n    parser.add_argument(\'-j\', \'--workers\', type=int, default=4)\n    parser.add_argument(\'--split\', type=int, default=0)\n    parser.add_argument(\'--height\', type=int,\n                        help=""input height, default: 256 for resnet*, ""\n                             ""144 for inception"")\n    parser.add_argument(\'--width\', type=int,\n                        help=""input width, default: 128 for resnet*, ""\n                             ""56 for inception"")\n    parser.add_argument(\'--combine-trainval\', action=\'store_true\',\n                        help=""train and val sets together for training, ""\n                             ""val set alone for validation"")\n    parser.add_argument(\'--num-instances\', type=int, default=4,\n                        help=""each minibatch consist of ""\n                             ""(batch_size // num_instances) identities, and ""\n                             ""each identity has num_instances instances, ""\n                             ""default: 4"")\n    # model\n    parser.add_argument(\'-a\', \'--arch\', type=str, default=\'resnet50\',\n                        choices=models.names())\n    parser.add_argument(\'--features\', type=int, default=128)\n    parser.add_argument(\'--dropout\', type=float, default=0)\n    # loss\n    parser.add_argument(\'--margin\', type=float, default=0.5,\n                        help=""margin of the triplet loss, default: 0.5"")\n    # optimizer\n    parser.add_argument(\'--lr\', type=float, default=0.0002,\n                        help=""learning rate of all parameters"")\n    parser.add_argument(\'--weight-decay\', type=float, default=5e-4)\n    # training configs\n    parser.add_argument(\'--resume\', type=str, default=\'\', metavar=\'PATH\')\n    parser.add_argument(\'--evaluate\', action=\'store_true\',\n                        help=""evaluation only"")\n    parser.add_argument(\'--epochs\', type=int, default=150)\n    parser.add_argument(\'--start_save\', type=int, default=0,\n                        help=""start saving checkpoints after specific epoch"")\n    parser.add_argument(\'--seed\', type=int, default=1)\n    parser.add_argument(\'--print-freq\', type=int, default=1)\n    # metric learning\n    parser.add_argument(\'--dist-metric\', type=str, default=\'euclidean\',\n                        choices=[\'euclidean\', \'kissme\'])\n    # misc\n    working_dir = osp.dirname(osp.abspath(__file__))\n    parser.add_argument(\'--data-dir\', type=str, metavar=\'PATH\',\n                        default=osp.join(working_dir, \'data\'))\n    parser.add_argument(\'--logs-dir\', type=str, metavar=\'PATH\',\n                        default=osp.join(working_dir, \'logs\'))\n    main(parser.parse_args())\n'"
reid/__init__.py,0,"b""from __future__ import absolute_import\n\nfrom . import datasets\nfrom . import evaluation_metrics\nfrom . import feature_extraction\nfrom . import loss\nfrom . import metric_learning\nfrom . import models\nfrom . import utils\nfrom . import dist_metric\nfrom . import evaluators\nfrom . import trainers\n\n__version__ = '0.2.0'\n"""
reid/dist_metric.py,4,"b""from __future__ import absolute_import\n\nimport torch\n\nfrom .evaluators import extract_features\nfrom .metric_learning import get_metric\n\n\nclass DistanceMetric(object):\n    def __init__(self, algorithm='euclidean', *args, **kwargs):\n        super(DistanceMetric, self).__init__()\n        self.algorithm = algorithm\n        self.metric = get_metric(algorithm, *args, **kwargs)\n\n    def train(self, model, data_loader):\n        if self.algorithm == 'euclidean': return\n        features, labels = extract_features(model, data_loader)\n        features = torch.stack(features.values()).numpy()\n        labels = torch.Tensor(list(labels.values())).numpy()\n        self.metric.fit(features, labels)\n\n    def transform(self, X):\n        if torch.is_tensor(X):\n            X = X.numpy()\n            X = self.metric.transform(X)\n            X = torch.from_numpy(X)\n        else:\n            X = self.metric.transform(X)\n        return X\n\n"""
reid/evaluators.py,7,"b""from __future__ import print_function, absolute_import\nimport time\nfrom collections import OrderedDict\n\nimport torch\n\nfrom .evaluation_metrics import cmc, mean_ap\nfrom .feature_extraction import extract_cnn_feature\nfrom .utils.meters import AverageMeter\n\n\ndef extract_features(model, data_loader, print_freq=1, metric=None):\n    model.eval()\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n\n    features = OrderedDict()\n    labels = OrderedDict()\n\n    end = time.time()\n    for i, (imgs, fnames, pids, _) in enumerate(data_loader):\n        data_time.update(time.time() - end)\n\n        outputs = extract_cnn_feature(model, imgs)\n        for fname, output, pid in zip(fnames, outputs, pids):\n            features[fname] = output\n            labels[fname] = pid\n\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if (i + 1) % print_freq == 0:\n            print('Extract Features: [{}/{}]\\t'\n                  'Time {:.3f} ({:.3f})\\t'\n                  'Data {:.3f} ({:.3f})\\t'\n                  .format(i + 1, len(data_loader),\n                          batch_time.val, batch_time.avg,\n                          data_time.val, data_time.avg))\n\n    return features, labels\n\n\ndef pairwise_distance(features, query=None, gallery=None, metric=None):\n    if query is None and gallery is None:\n        n = len(features)\n        x = torch.cat(list(features.values()))\n        x = x.view(n, -1)\n        if metric is not None:\n            x = metric.transform(x)\n        dist = torch.pow(x, 2).sum(dim=1, keepdim=True) * 2\n        dist = dist.expand(n, n) - 2 * torch.mm(x, x.t())\n        return dist\n\n    x = torch.cat([features[f].unsqueeze(0) for f, _, _ in query], 0)\n    y = torch.cat([features[f].unsqueeze(0) for f, _, _ in gallery], 0)\n    m, n = x.size(0), y.size(0)\n    x = x.view(m, -1)\n    y = y.view(n, -1)\n    if metric is not None:\n        x = metric.transform(x)\n        y = metric.transform(y)\n    dist = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n           torch.pow(y, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n    dist.addmm_(1, -2, x, y.t())\n    return dist\n\n\ndef evaluate_all(distmat, query=None, gallery=None,\n                 query_ids=None, gallery_ids=None,\n                 query_cams=None, gallery_cams=None,\n                 cmc_topk=(1, 5, 10)):\n    if query is not None and gallery is not None:\n        query_ids = [pid for _, pid, _ in query]\n        gallery_ids = [pid for _, pid, _ in gallery]\n        query_cams = [cam for _, _, cam in query]\n        gallery_cams = [cam for _, _, cam in gallery]\n    else:\n        assert (query_ids is not None and gallery_ids is not None\n                and query_cams is not None and gallery_cams is not None)\n\n    # Compute mean AP\n    mAP = mean_ap(distmat, query_ids, gallery_ids, query_cams, gallery_cams)\n    print('Mean AP: {:4.1%}'.format(mAP))\n\n    # Compute all kinds of CMC scores\n    cmc_configs = {\n        'allshots': dict(separate_camera_set=False,\n                         single_gallery_shot=False,\n                         first_match_break=False),\n        'cuhk03': dict(separate_camera_set=True,\n                       single_gallery_shot=True,\n                       first_match_break=False),\n        'market1501': dict(separate_camera_set=False,\n                           single_gallery_shot=False,\n                           first_match_break=True)}\n    cmc_scores = {name: cmc(distmat, query_ids, gallery_ids,\n                            query_cams, gallery_cams, **params)\n                  for name, params in cmc_configs.items()}\n\n    print('CMC Scores{:>12}{:>12}{:>12}'\n          .format('allshots', 'cuhk03', 'market1501'))\n    for k in cmc_topk:\n        print('  top-{:<4}{:12.1%}{:12.1%}{:12.1%}'\n              .format(k, cmc_scores['allshots'][k - 1],\n                      cmc_scores['cuhk03'][k - 1],\n                      cmc_scores['market1501'][k - 1]))\n\n    # Use the allshots cmc top-1 score for validation criterion\n    return cmc_scores['allshots'][0]\n\n\nclass Evaluator(object):\n    def __init__(self, model):\n        super(Evaluator, self).__init__()\n        self.model = model\n\n    def evaluate(self, data_loader, query, gallery, metric=None):\n        features, _ = extract_features(self.model, data_loader)\n        distmat = pairwise_distance(features, query, gallery, metric=metric)\n        return evaluate_all(distmat, query=query, gallery=gallery)\n"""
reid/trainers.py,2,"b'from __future__ import print_function, absolute_import\nimport time\n\nimport torch\nfrom torch.autograd import Variable\n\nfrom .evaluation_metrics import accuracy\nfrom .loss import OIMLoss, TripletLoss\nfrom .utils.meters import AverageMeter\n\n\nclass BaseTrainer(object):\n    def __init__(self, model, criterion):\n        super(BaseTrainer, self).__init__()\n        self.model = model\n        self.criterion = criterion\n\n    def train(self, epoch, data_loader, optimizer, print_freq=1):\n        self.model.train()\n\n        batch_time = AverageMeter()\n        data_time = AverageMeter()\n        losses = AverageMeter()\n        precisions = AverageMeter()\n\n        end = time.time()\n        for i, inputs in enumerate(data_loader):\n            data_time.update(time.time() - end)\n\n            inputs, targets = self._parse_data(inputs)\n            loss, prec1 = self._forward(inputs, targets)\n\n            losses.update(loss.data[0], targets.size(0))\n            precisions.update(prec1, targets.size(0))\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if (i + 1) % print_freq == 0:\n                print(\'Epoch: [{}][{}/{}]\\t\'\n                      \'Time {:.3f} ({:.3f})\\t\'\n                      \'Data {:.3f} ({:.3f})\\t\'\n                      \'Loss {:.3f} ({:.3f})\\t\'\n                      \'Prec {:.2%} ({:.2%})\\t\'\n                      .format(epoch, i + 1, len(data_loader),\n                              batch_time.val, batch_time.avg,\n                              data_time.val, data_time.avg,\n                              losses.val, losses.avg,\n                              precisions.val, precisions.avg))\n\n    def _parse_data(self, inputs):\n        raise NotImplementedError\n\n    def _forward(self, inputs, targets):\n        raise NotImplementedError\n\n\nclass Trainer(BaseTrainer):\n    def _parse_data(self, inputs):\n        imgs, _, pids, _ = inputs\n        inputs = [Variable(imgs)]\n        targets = Variable(pids.cuda())\n        return inputs, targets\n\n    def _forward(self, inputs, targets):\n        outputs = self.model(*inputs)\n        if isinstance(self.criterion, torch.nn.CrossEntropyLoss):\n            loss = self.criterion(outputs, targets)\n            prec, = accuracy(outputs.data, targets.data)\n            prec = prec[0]\n        elif isinstance(self.criterion, OIMLoss):\n            loss, outputs = self.criterion(outputs, targets)\n            prec, = accuracy(outputs.data, targets.data)\n            prec = prec[0]\n        elif isinstance(self.criterion, TripletLoss):\n            loss, prec = self.criterion(outputs, targets)\n        else:\n            raise ValueError(""Unsupported loss:"", self.criterion)\n        return loss, prec\n'"
reid/datasets/__init__.py,0,"b'from __future__ import absolute_import\nimport warnings\n\nfrom .cuhk01 import CUHK01\nfrom .cuhk03 import CUHK03\nfrom .dukemtmc import DukeMTMC\nfrom .market1501 import Market1501\nfrom .viper import VIPeR\n\n\n__factory = {\n    \'viper\': VIPeR,\n    \'cuhk01\': CUHK01,\n    \'cuhk03\': CUHK03,\n    \'market1501\': Market1501,\n    \'dukemtmc\': DukeMTMC,\n}\n\n\ndef names():\n    return sorted(__factory.keys())\n\n\ndef create(name, root, *args, **kwargs):\n    """"""\n    Create a dataset instance.\n\n    Parameters\n    ----------\n    name : str\n        The dataset name. Can be one of \'viper\', \'cuhk01\', \'cuhk03\',\n        \'market1501\', and \'dukemtmc\'.\n    root : str\n        The path to the dataset directory.\n    split_id : int, optional\n        The index of data split. Default: 0\n    num_val : int or float, optional\n        When int, it means the number of validation identities. When float,\n        it means the proportion of validation to all the trainval. Default: 100\n    download : bool, optional\n        If True, will download the dataset. Default: False\n    """"""\n    if name not in __factory:\n        raise KeyError(""Unknown dataset:"", name)\n    return __factory[name](root, *args, **kwargs)\n\n\ndef get_dataset(name, root, *args, **kwargs):\n    warnings.warn(""get_dataset is deprecated. Use create instead."")\n    return create(name, root, *args, **kwargs)\n'"
reid/datasets/cuhk01.py,0,"b'from __future__ import print_function, absolute_import\nimport os.path as osp\n\nimport numpy as np\n\nfrom ..utils.data import Dataset\nfrom ..utils.osutils import mkdir_if_missing\nfrom ..utils.serialization import write_json\n\n\nclass CUHK01(Dataset):\n    url = \'https://docs.google.com/spreadsheet/viewform?formkey=dF9pZ1BFZkNiMG1oZUdtTjZPalR0MGc6MA\'\n    md5 = \'e6d55c0da26d80cda210a2edeb448e98\'\n\n    def __init__(self, root, split_id=0, num_val=100, download=True):\n        super(CUHK01, self).__init__(root, split_id=split_id)\n\n        if download:\n            self.download()\n\n        if not self._check_integrity():\n            raise RuntimeError(""Dataset not found or corrupted. "" +\n                               ""You can use download=True to download it."")\n\n        self.load(num_val)\n\n    def download(self):\n        if self._check_integrity():\n            print(""Files already downloaded and verified"")\n            return\n\n        import hashlib\n        import shutil\n        from glob import glob\n        from zipfile import ZipFile\n\n        raw_dir = osp.join(self.root, \'raw\')\n        mkdir_if_missing(raw_dir)\n\n        # Download the raw zip file\n        fpath = osp.join(raw_dir, \'CUHK01.zip\')\n        if osp.isfile(fpath) and \\\n          hashlib.md5(open(fpath, \'rb\').read()).hexdigest() == self.md5:\n            print(""Using downloaded file: "" + fpath)\n        else:\n            raise RuntimeError(""Please download the dataset manually from {} ""\n                               ""to {}"".format(self.url, fpath))\n\n        # Extract the file\n        exdir = osp.join(raw_dir, \'campus\')\n        if not osp.isdir(exdir):\n            print(""Extracting zip file"")\n            with ZipFile(fpath) as z:\n                z.extractall(path=raw_dir)\n\n        # Format\n        images_dir = osp.join(self.root, \'images\')\n        mkdir_if_missing(images_dir)\n\n        identities = [[[] for _ in range(2)] for _ in range(971)]\n\n        files = sorted(glob(osp.join(exdir, \'*.png\')))\n        for fpath in files:\n            fname = osp.basename(fpath)\n            pid, cam = int(fname[:4]), int(fname[4:7])\n            assert 1 <= pid <= 971\n            assert 1 <= cam <= 4\n            pid, cam = pid - 1, (cam - 1) // 2\n            fname = (\'{:08d}_{:02d}_{:04d}.png\'\n                     .format(pid, cam, len(identities[pid][cam])))\n            identities[pid][cam].append(fname)\n            shutil.copy(fpath, osp.join(images_dir, fname))\n\n        # Save meta information into a json file\n        meta = {\'name\': \'cuhk01\', \'shot\': \'multiple\', \'num_cameras\': 2,\n                \'identities\': identities}\n        write_json(meta, osp.join(self.root, \'meta.json\'))\n\n        # Randomly create ten training and test split\n        num = len(identities)\n        splits = []\n        for _ in range(10):\n            pids = np.random.permutation(num).tolist()\n            trainval_pids = sorted(pids[:num // 2])\n            test_pids = sorted(pids[num // 2:])\n            split = {\'trainval\': trainval_pids,\n                     \'query\': test_pids,\n                     \'gallery\': test_pids}\n            splits.append(split)\n        write_json(splits, osp.join(self.root, \'splits.json\'))\n'"
reid/datasets/cuhk03.py,0,"b'from __future__ import print_function, absolute_import\nimport os.path as osp\n\nimport numpy as np\n\nfrom ..utils.data import Dataset\nfrom ..utils.osutils import mkdir_if_missing\nfrom ..utils.serialization import write_json\n\n\nclass CUHK03(Dataset):\n    url = \'https://docs.google.com/spreadsheet/viewform?usp=drive_web&formkey=dHRkMkFVSUFvbTJIRkRDLWRwZWpONnc6MA#gid=0\'\n    md5 = \'728939e58ad9f0ff53e521857dd8fb43\'\n\n    def __init__(self, root, split_id=0, num_val=100, download=True):\n        super(CUHK03, self).__init__(root, split_id=split_id)\n\n        if download:\n            self.download()\n\n        if not self._check_integrity():\n            raise RuntimeError(""Dataset not found or corrupted. "" +\n                               ""You can use download=True to download it."")\n\n        self.load(num_val)\n\n    def download(self):\n        if self._check_integrity():\n            print(""Files already downloaded and verified"")\n            return\n\n        import h5py\n        import hashlib\n        from scipy.misc import imsave\n        from zipfile import ZipFile\n\n        raw_dir = osp.join(self.root, \'raw\')\n        mkdir_if_missing(raw_dir)\n\n        # Download the raw zip file\n        fpath = osp.join(raw_dir, \'cuhk03_release.zip\')\n        if osp.isfile(fpath) and \\\n          hashlib.md5(open(fpath, \'rb\').read()).hexdigest() == self.md5:\n            print(""Using downloaded file: "" + fpath)\n        else:\n            raise RuntimeError(""Please download the dataset manually from {} ""\n                               ""to {}"".format(self.url, fpath))\n\n        # Extract the file\n        exdir = osp.join(raw_dir, \'cuhk03_release\')\n        if not osp.isdir(exdir):\n            print(""Extracting zip file"")\n            with ZipFile(fpath) as z:\n                z.extractall(path=raw_dir)\n\n        # Format\n        images_dir = osp.join(self.root, \'images\')\n        mkdir_if_missing(images_dir)\n        matdata = h5py.File(osp.join(exdir, \'cuhk-03.mat\'), \'r\')\n\n        def deref(ref):\n            return matdata[ref][:].T\n\n        def dump_(refs, pid, cam, fnames):\n            for ref in refs:\n                img = deref(ref)\n                if img.size == 0 or img.ndim < 2: break\n                fname = \'{:08d}_{:02d}_{:04d}.jpg\'.format(pid, cam, len(fnames))\n                imsave(osp.join(images_dir, fname), img)\n                fnames.append(fname)\n\n        identities = []\n        for labeled, detected in zip(\n                matdata[\'labeled\'][0], matdata[\'detected\'][0]):\n            labeled, detected = deref(labeled), deref(detected)\n            assert labeled.shape == detected.shape\n            for i in range(labeled.shape[0]):\n                pid = len(identities)\n                images = [[], []]\n                dump_(labeled[i, :5], pid, 0, images[0])\n                dump_(detected[i, :5], pid, 0, images[0])\n                dump_(labeled[i, 5:], pid, 1, images[1])\n                dump_(detected[i, 5:], pid, 1, images[1])\n                identities.append(images)\n\n        # Save meta information into a json file\n        meta = {\'name\': \'cuhk03\', \'shot\': \'multiple\', \'num_cameras\': 2,\n                \'identities\': identities}\n        write_json(meta, osp.join(self.root, \'meta.json\'))\n\n        # Save training and test splits\n        splits = []\n        view_counts = [deref(ref).shape[0] for ref in matdata[\'labeled\'][0]]\n        vid_offsets = np.r_[0, np.cumsum(view_counts)]\n        for ref in matdata[\'testsets\'][0]:\n            test_info = deref(ref).astype(np.int32)\n            test_pids = sorted(\n                [int(vid_offsets[i-1] + j - 1) for i, j in test_info])\n            trainval_pids = list(set(range(vid_offsets[-1])) - set(test_pids))\n            split = {\'trainval\': trainval_pids,\n                     \'query\': test_pids,\n                     \'gallery\': test_pids}\n            splits.append(split)\n        write_json(splits, osp.join(self.root, \'splits.json\'))\n'"
reid/datasets/dukemtmc.py,0,"b'from __future__ import print_function, absolute_import\nimport os.path as osp\n\nfrom ..utils.data import Dataset\nfrom ..utils.osutils import mkdir_if_missing\nfrom ..utils.serialization import write_json\n\n\nclass DukeMTMC(Dataset):\n    url = \'https://drive.google.com/uc?id=0B0VOCNYh8HeRdnBPa2ZWaVBYSVk\'\n    md5 = \'2f93496f9b516d1ee5ef51c1d5e7d601\'\n\n    def __init__(self, root, split_id=0, num_val=100, download=True):\n        super(DukeMTMC, self).__init__(root, split_id=split_id)\n\n        if download:\n            self.download()\n\n        if not self._check_integrity():\n            raise RuntimeError(""Dataset not found or corrupted. "" +\n                               ""You can use download=True to download it."")\n\n        self.load(num_val)\n\n    def download(self):\n        if self._check_integrity():\n            print(""Files already downloaded and verified"")\n            return\n\n        import re\n        import hashlib\n        import shutil\n        from glob import glob\n        from zipfile import ZipFile\n\n        raw_dir = osp.join(self.root, \'raw\')\n        mkdir_if_missing(raw_dir)\n\n        # Download the raw zip file\n        fpath = osp.join(raw_dir, \'DukeMTMC-reID.zip\')\n        if osp.isfile(fpath) and \\\n          hashlib.md5(open(fpath, \'rb\').read()).hexdigest() == self.md5:\n            print(""Using downloaded file: "" + fpath)\n        else:\n            raise RuntimeError(""Please download the dataset manually from {} ""\n                               ""to {}"".format(self.url, fpath))\n\n        # Extract the file\n        exdir = osp.join(raw_dir, \'DukeMTMC-reID\')\n        if not osp.isdir(exdir):\n            print(""Extracting zip file"")\n            with ZipFile(fpath) as z:\n                z.extractall(path=raw_dir)\n\n        # Format\n        images_dir = osp.join(self.root, \'images\')\n        mkdir_if_missing(images_dir)\n\n        identities = []\n        all_pids = {}\n\n        def register(subdir, pattern=re.compile(r\'([-\\d]+)_c(\\d)\')):\n            fpaths = sorted(glob(osp.join(exdir, subdir, \'*.jpg\')))\n            pids = set()\n            for fpath in fpaths:\n                fname = osp.basename(fpath)\n                pid, cam = map(int, pattern.search(fname).groups())\n                assert 1 <= cam <= 8\n                cam -= 1\n                if pid not in all_pids:\n                    all_pids[pid] = len(all_pids)\n                pid = all_pids[pid]\n                pids.add(pid)\n                if pid >= len(identities):\n                    assert pid == len(identities)\n                    identities.append([[] for _ in range(8)])  # 8 camera views\n                fname = (\'{:08d}_{:02d}_{:04d}.jpg\'\n                         .format(pid, cam, len(identities[pid][cam])))\n                identities[pid][cam].append(fname)\n                shutil.copy(fpath, osp.join(images_dir, fname))\n            return pids\n\n        trainval_pids = register(\'bounding_box_train\')\n        gallery_pids = register(\'bounding_box_test\')\n        query_pids = register(\'query\')\n        assert query_pids <= gallery_pids\n        assert trainval_pids.isdisjoint(gallery_pids)\n\n        # Save meta information into a json file\n        meta = {\'name\': \'DukeMTMC\', \'shot\': \'multiple\', \'num_cameras\': 8,\n                \'identities\': identities}\n        write_json(meta, osp.join(self.root, \'meta.json\'))\n\n        # Save the only training / test split\n        splits = [{\n            \'trainval\': sorted(list(trainval_pids)),\n            \'query\': sorted(list(query_pids)),\n            \'gallery\': sorted(list(gallery_pids))}]\n        write_json(splits, osp.join(self.root, \'splits.json\'))\n'"
reid/datasets/market1501.py,0,"b'from __future__ import print_function, absolute_import\nimport os.path as osp\n\nfrom ..utils.data import Dataset\nfrom ..utils.osutils import mkdir_if_missing\nfrom ..utils.serialization import write_json\n\n\nclass Market1501(Dataset):\n    url = \'https://drive.google.com/file/d/0B8-rUzbwVRk0c054eEozWG9COHM/view\'\n    md5 = \'65005ab7d12ec1c44de4eeafe813e68a\'\n\n    def __init__(self, root, split_id=0, num_val=100, download=True):\n        super(Market1501, self).__init__(root, split_id=split_id)\n\n        if download:\n            self.download()\n\n        if not self._check_integrity():\n            raise RuntimeError(""Dataset not found or corrupted. "" +\n                               ""You can use download=True to download it."")\n\n        self.load(num_val)\n\n    def download(self):\n        if self._check_integrity():\n            print(""Files already downloaded and verified"")\n            return\n\n        import re\n        import hashlib\n        import shutil\n        from glob import glob\n        from zipfile import ZipFile\n\n        raw_dir = osp.join(self.root, \'raw\')\n        mkdir_if_missing(raw_dir)\n\n        # Download the raw zip file\n        fpath = osp.join(raw_dir, \'Market-1501-v15.09.15.zip\')\n        if osp.isfile(fpath) and \\\n          hashlib.md5(open(fpath, \'rb\').read()).hexdigest() == self.md5:\n            print(""Using downloaded file: "" + fpath)\n        else:\n            raise RuntimeError(""Please download the dataset manually from {} ""\n                               ""to {}"".format(self.url, fpath))\n\n        # Extract the file\n        exdir = osp.join(raw_dir, \'Market-1501-v15.09.15\')\n        if not osp.isdir(exdir):\n            print(""Extracting zip file"")\n            with ZipFile(fpath) as z:\n                z.extractall(path=raw_dir)\n\n        # Format\n        images_dir = osp.join(self.root, \'images\')\n        mkdir_if_missing(images_dir)\n\n        # 1501 identities (+1 for background) with 6 camera views each\n        identities = [[[] for _ in range(6)] for _ in range(1502)]\n\n        def register(subdir, pattern=re.compile(r\'([-\\d]+)_c(\\d)\')):\n            fpaths = sorted(glob(osp.join(exdir, subdir, \'*.jpg\')))\n            pids = set()\n            for fpath in fpaths:\n                fname = osp.basename(fpath)\n                pid, cam = map(int, pattern.search(fname).groups())\n                if pid == -1: continue  # junk images are just ignored\n                assert 0 <= pid <= 1501  # pid == 0 means background\n                assert 1 <= cam <= 6\n                cam -= 1\n                pids.add(pid)\n                fname = (\'{:08d}_{:02d}_{:04d}.jpg\'\n                         .format(pid, cam, len(identities[pid][cam])))\n                identities[pid][cam].append(fname)\n                shutil.copy(fpath, osp.join(images_dir, fname))\n            return pids\n\n        trainval_pids = register(\'bounding_box_train\')\n        gallery_pids = register(\'bounding_box_test\')\n        query_pids = register(\'query\')\n        assert query_pids <= gallery_pids\n        assert trainval_pids.isdisjoint(gallery_pids)\n\n        # Save meta information into a json file\n        meta = {\'name\': \'Market1501\', \'shot\': \'multiple\', \'num_cameras\': 6,\n                \'identities\': identities}\n        write_json(meta, osp.join(self.root, \'meta.json\'))\n\n        # Save the only training / test split\n        splits = [{\n            \'trainval\': sorted(list(trainval_pids)),\n            \'query\': sorted(list(query_pids)),\n            \'gallery\': sorted(list(gallery_pids))}]\n        write_json(splits, osp.join(self.root, \'splits.json\'))\n'"
reid/datasets/viper.py,0,"b'from __future__ import print_function, absolute_import\nimport os.path as osp\n\nimport numpy as np\n\nfrom ..utils.data import Dataset\nfrom ..utils.osutils import mkdir_if_missing\nfrom ..utils.serialization import write_json\n\n\nclass VIPeR(Dataset):\n    url = \'http://users.soe.ucsc.edu/~manduchi/VIPeR.v1.0.zip\'\n    md5 = \'1c2d9fc1cc800332567a0da25a1ce68c\'\n\n    def __init__(self, root, split_id=0, num_val=100, download=True):\n        super(VIPeR, self).__init__(root, split_id=split_id)\n\n        if download:\n            self.download()\n\n        if not self._check_integrity():\n            raise RuntimeError(""Dataset not found or corrupted. "" +\n                               ""You can use download=True to download it."")\n\n        self.load(num_val)\n\n    def download(self):\n        if self._check_integrity():\n            print(""Files already downloaded and verified"")\n            return\n\n        import hashlib\n        from glob import glob\n        from scipy.misc import imsave, imread\n        from six.moves import urllib\n        from zipfile import ZipFile\n\n        raw_dir = osp.join(self.root, \'raw\')\n        mkdir_if_missing(raw_dir)\n\n        # Download the raw zip file\n        fpath = osp.join(raw_dir, \'VIPeR.v1.0.zip\')\n        if osp.isfile(fpath) and \\\n           hashlib.md5(open(fpath, \'rb\').read()).hexdigest() == self.md5:\n            print(""Using downloaded file: "" + fpath)\n        else:\n            print(""Downloading {} to {}"".format(self.url, fpath))\n            urllib.request.urlretrieve(self.url, fpath)\n\n        # Extract the file\n        exdir = osp.join(raw_dir, \'VIPeR\')\n        if not osp.isdir(exdir):\n            print(""Extracting zip file"")\n            with ZipFile(fpath) as z:\n                z.extractall(path=raw_dir)\n\n        # Format\n        images_dir = osp.join(self.root, \'images\')\n        mkdir_if_missing(images_dir)\n        cameras = [sorted(glob(osp.join(exdir, \'cam_a\', \'*.bmp\'))),\n                   sorted(glob(osp.join(exdir, \'cam_b\', \'*.bmp\')))]\n        assert len(cameras[0]) == len(cameras[1])\n        identities = []\n        for pid, (cam1, cam2) in enumerate(zip(*cameras)):\n            images = []\n            # view-0\n            fname = \'{:08d}_{:02d}_{:04d}.jpg\'.format(pid, 0, 0)\n            imsave(osp.join(images_dir, fname), imread(cam1))\n            images.append([fname])\n            # view-1\n            fname = \'{:08d}_{:02d}_{:04d}.jpg\'.format(pid, 1, 0)\n            imsave(osp.join(images_dir, fname), imread(cam2))\n            images.append([fname])\n            identities.append(images)\n\n        # Save meta information into a json file\n        meta = {\'name\': \'VIPeR\', \'shot\': \'single\', \'num_cameras\': 2,\n                \'identities\': identities}\n        write_json(meta, osp.join(self.root, \'meta.json\'))\n\n        # Randomly create ten training and test split\n        num = len(identities)\n        splits = []\n        for _ in range(10):\n            pids = np.random.permutation(num).tolist()\n            trainval_pids = sorted(pids[:num // 2])\n            test_pids = sorted(pids[num // 2:])\n            split = {\'trainval\': trainval_pids,\n                     \'query\': test_pids,\n                     \'gallery\': test_pids}\n            splits.append(split)\n        write_json(splits, osp.join(self.root, \'splits.json\'))\n'"
reid/evaluation_metrics/__init__.py,0,"b""from __future__ import absolute_import\n\nfrom .classification import accuracy\nfrom .ranking import cmc, mean_ap\n\n__all__ = [\n    'accuracy',\n    'cmc',\n    'mean_ap',\n]\n"""
reid/evaluation_metrics/classification.py,0,"b'from __future__ import absolute_import\n\nfrom ..utils import to_torch\n\n\ndef accuracy(output, target, topk=(1,)):\n    output, target = to_torch(output), to_torch(target)\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    ret = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(dim=0, keepdim=True)\n        ret.append(correct_k.mul_(1. / batch_size))\n    return ret\n'"
reid/evaluation_metrics/ranking.py,0,"b'from __future__ import absolute_import\nfrom collections import defaultdict\n\nimport numpy as np\nfrom sklearn.metrics import average_precision_score\n\nfrom ..utils import to_numpy\n\n\ndef _unique_sample(ids_dict, num):\n    mask = np.zeros(num, dtype=np.bool)\n    for _, indices in ids_dict.items():\n        i = np.random.choice(indices)\n        mask[i] = True\n    return mask\n\n\ndef cmc(distmat, query_ids=None, gallery_ids=None,\n        query_cams=None, gallery_cams=None, topk=100,\n        separate_camera_set=False,\n        single_gallery_shot=False,\n        first_match_break=False):\n    distmat = to_numpy(distmat)\n    m, n = distmat.shape\n    # Fill up default values\n    if query_ids is None:\n        query_ids = np.arange(m)\n    if gallery_ids is None:\n        gallery_ids = np.arange(n)\n    if query_cams is None:\n        query_cams = np.zeros(m).astype(np.int32)\n    if gallery_cams is None:\n        gallery_cams = np.ones(n).astype(np.int32)\n    # Ensure numpy array\n    query_ids = np.asarray(query_ids)\n    gallery_ids = np.asarray(gallery_ids)\n    query_cams = np.asarray(query_cams)\n    gallery_cams = np.asarray(gallery_cams)\n    # Sort and find correct matches\n    indices = np.argsort(distmat, axis=1)\n    matches = (gallery_ids[indices] == query_ids[:, np.newaxis])\n    # Compute CMC for each query\n    ret = np.zeros(topk)\n    num_valid_queries = 0\n    for i in range(m):\n        # Filter out the same id and same camera\n        valid = ((gallery_ids[indices[i]] != query_ids[i]) |\n                 (gallery_cams[indices[i]] != query_cams[i]))\n        if separate_camera_set:\n            # Filter out samples from same camera\n            valid &= (gallery_cams[indices[i]] != query_cams[i])\n        if not np.any(matches[i, valid]): continue\n        if single_gallery_shot:\n            repeat = 10\n            gids = gallery_ids[indices[i][valid]]\n            inds = np.where(valid)[0]\n            ids_dict = defaultdict(list)\n            for j, x in zip(inds, gids):\n                ids_dict[x].append(j)\n        else:\n            repeat = 1\n        for _ in range(repeat):\n            if single_gallery_shot:\n                # Randomly choose one instance for each id\n                sampled = (valid & _unique_sample(ids_dict, len(valid)))\n                index = np.nonzero(matches[i, sampled])[0]\n            else:\n                index = np.nonzero(matches[i, valid])[0]\n            delta = 1. / (len(index) * repeat)\n            for j, k in enumerate(index):\n                if k - j >= topk: break\n                if first_match_break:\n                    ret[k - j] += 1\n                    break\n                ret[k - j] += delta\n        num_valid_queries += 1\n    if num_valid_queries == 0:\n        raise RuntimeError(""No valid query"")\n    return ret.cumsum() / num_valid_queries\n\n\ndef mean_ap(distmat, query_ids=None, gallery_ids=None,\n            query_cams=None, gallery_cams=None):\n    distmat = to_numpy(distmat)\n    m, n = distmat.shape\n    # Fill up default values\n    if query_ids is None:\n        query_ids = np.arange(m)\n    if gallery_ids is None:\n        gallery_ids = np.arange(n)\n    if query_cams is None:\n        query_cams = np.zeros(m).astype(np.int32)\n    if gallery_cams is None:\n        gallery_cams = np.ones(n).astype(np.int32)\n    # Ensure numpy array\n    query_ids = np.asarray(query_ids)\n    gallery_ids = np.asarray(gallery_ids)\n    query_cams = np.asarray(query_cams)\n    gallery_cams = np.asarray(gallery_cams)\n    # Sort and find correct matches\n    indices = np.argsort(distmat, axis=1)\n    matches = (gallery_ids[indices] == query_ids[:, np.newaxis])\n    # Compute AP for each query\n    aps = []\n    for i in range(m):\n        # Filter out the same id and same camera\n        valid = ((gallery_ids[indices[i]] != query_ids[i]) |\n                 (gallery_cams[indices[i]] != query_cams[i]))\n        y_true = matches[i, valid]\n        y_score = -distmat[i][indices[i]][valid]\n        if not np.any(y_true): continue\n        aps.append(average_precision_score(y_true, y_score))\n    if len(aps) == 0:\n        raise RuntimeError(""No valid query"")\n    return np.mean(aps)\n'"
reid/feature_extraction/__init__.py,0,"b""from __future__ import absolute_import\n\nfrom .cnn import extract_cnn_feature\nfrom .database import FeatureDatabase\n\n__all__ = [\n    'extract_cnn_feature',\n    'FeatureDatabase',\n]\n"""
reid/feature_extraction/cnn.py,1,"b'from __future__ import absolute_import\nfrom collections import OrderedDict\n\nfrom torch.autograd import Variable\n\nfrom ..utils import to_torch\n\n\ndef extract_cnn_feature(model, inputs, modules=None):\n    model.eval()\n    inputs = to_torch(inputs)\n    inputs = Variable(inputs, volatile=True)\n    if modules is None:\n        outputs = model(inputs)\n        outputs = outputs.data.cpu()\n        return outputs\n    # Register forward hook for each module\n    outputs = OrderedDict()\n    handles = []\n    for m in modules:\n        outputs[id(m)] = None\n        def func(m, i, o): outputs[id(m)] = o.data.cpu()\n        handles.append(m.register_forward_hook(func))\n    model(inputs)\n    for h in handles:\n        h.remove()\n    return list(outputs.values())\n'"
reid/feature_extraction/database.py,1,"b'from __future__ import absolute_import\n\nimport h5py\nimport numpy as np\nfrom torch.utils.data import Dataset\n\n\nclass FeatureDatabase(Dataset):\n    def __init__(self, *args, **kwargs):\n        super(FeatureDatabase, self).__init__()\n        self.fid = h5py.File(*args, **kwargs)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def __getitem__(self, keys):\n        if isinstance(keys, (tuple, list)):\n            return [self._get_single_item(k) for k in keys]\n        return self._get_single_item(keys)\n\n    def _get_single_item(self, key):\n        return np.asarray(self.fid[key])\n\n    def __setitem__(self, key, value):\n        if key in self.fid:\n            if self.fid[key].shape == value.shape and \\\n               self.fid[key].dtype == value.dtype:\n                self.fid[key][...] = value\n            else:\n                del self.fid[key]\n                self.fid.create_dataset(key, data=value)\n        else:\n            self.fid.create_dataset(key, data=value)\n\n    def __delitem__(self, key):\n        del self.fid[key]\n\n    def __len__(self):\n        return len(self.fid)\n\n    def __iter__(self):\n        return iter(self.fid)\n\n    def flush(self):\n        self.fid.flush()\n\n    def close(self):\n        self.fid.close()\n'"
reid/loss/__init__.py,0,"b""from __future__ import absolute_import\n\nfrom .oim import oim, OIM, OIMLoss\nfrom .triplet import TripletLoss\n\n__all__ = [\n    'oim',\n    'OIM',\n    'OIMLoss',\n    'TripletLoss',\n]\n"""
reid/loss/oim.py,2,"b""from __future__ import absolute_import\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, autograd\n\n\nclass OIM(autograd.Function):\n    def __init__(self, lut, momentum=0.5):\n        super(OIM, self).__init__()\n        self.lut = lut\n        self.momentum = momentum\n\n    def forward(self, inputs, targets):\n        self.save_for_backward(inputs, targets)\n        outputs = inputs.mm(self.lut.t())\n        return outputs\n\n    def backward(self, grad_outputs):\n        inputs, targets = self.saved_tensors\n        grad_inputs = None\n        if self.needs_input_grad[0]:\n            grad_inputs = grad_outputs.mm(self.lut)\n        for x, y in zip(inputs, targets):\n            self.lut[y] = self.momentum * self.lut[y] + (1. - self.momentum) * x\n            self.lut[y] /= self.lut[y].norm()\n        return grad_inputs, None\n\n\ndef oim(inputs, targets, lut, momentum=0.5):\n    return OIM(lut, momentum=momentum)(inputs, targets)\n\n\nclass OIMLoss(nn.Module):\n    def __init__(self, num_features, num_classes, scalar=1.0, momentum=0.5,\n                 weight=None, size_average=True):\n        super(OIMLoss, self).__init__()\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.momentum = momentum\n        self.scalar = scalar\n        self.weight = weight\n        self.size_average = size_average\n\n        self.register_buffer('lut', torch.zeros(num_classes, num_features))\n\n    def forward(self, inputs, targets):\n        inputs = oim(inputs, targets, self.lut, momentum=self.momentum)\n        inputs *= self.scalar\n        loss = F.cross_entropy(inputs, targets, weight=self.weight,\n                               size_average=self.size_average)\n        return loss, inputs\n"""
reid/loss/triplet.py,4,"b'from __future__ import absolute_import\n\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\n\nclass TripletLoss(nn.Module):\n    def __init__(self, margin=0):\n        super(TripletLoss, self).__init__()\n        self.margin = margin\n        self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n\n    def forward(self, inputs, targets):\n        n = inputs.size(0)\n        # Compute pairwise distance, replace by the official when merged\n        dist = torch.pow(inputs, 2).sum(dim=1, keepdim=True).expand(n, n)\n        dist = dist + dist.t()\n        dist.addmm_(1, -2, inputs, inputs.t())\n        dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n        # For each anchor, find the hardest positive and negative\n        mask = targets.expand(n, n).eq(targets.expand(n, n).t())\n        dist_ap, dist_an = [], []\n        for i in range(n):\n            dist_ap.append(dist[i][mask[i]].max())\n            dist_an.append(dist[i][mask[i] == 0].min())\n        dist_ap = torch.cat(dist_ap)\n        dist_an = torch.cat(dist_an)\n        # Compute ranking hinge loss\n        y = dist_an.data.new()\n        y.resize_as_(dist_an.data)\n        y.fill_(1)\n        y = Variable(y)\n        loss = self.ranking_loss(dist_an, dist_ap, y)\n        prec = (dist_an.data > dist_ap.data).sum() * 1. / y.size(0)\n        return loss, prec\n'"
reid/metric_learning/__init__.py,0,"b'from __future__ import absolute_import\n\nfrom metric_learn import (ITML_Supervised, LMNN, LSML_Supervised,\n                          SDML_Supervised, NCA, LFDA, RCA_Supervised)\n\nfrom .euclidean import Euclidean\nfrom .kissme import KISSME\n\n__factory = {\n    \'euclidean\': Euclidean,\n    \'kissme\': KISSME,\n    \'itml\': ITML_Supervised,\n    \'lmnn\': LMNN,\n    \'lsml\': LSML_Supervised,\n    \'sdml\': SDML_Supervised,\n    \'nca\': NCA,\n    \'lfda\': LFDA,\n    \'rca\': RCA_Supervised,\n}\n\n\ndef get_metric(algorithm, *args, **kwargs):\n    if algorithm not in __factory:\n        raise KeyError(""Unknown metric:"", algorithm)\n    return __factory[algorithm](*args, **kwargs)\n'"
reid/metric_learning/euclidean.py,0,"b'from __future__ import absolute_import\n\nimport numpy as np\nfrom metric_learn.base_metric import BaseMetricLearner\n\n\nclass Euclidean(BaseMetricLearner):\n    def __init__(self):\n        self.M_ = None\n\n    def metric(self):\n        return self.M_\n\n    def fit(self, X):\n        self.M_ = np.eye(X.shape[1])\n        self.X_ = X\n\n    def transform(self, X=None):\n        if X is None:\n            return self.X_\n        return X\n'"
reid/metric_learning/kissme.py,0,"b'from __future__ import absolute_import\n\nimport numpy as np\nfrom metric_learn.base_metric import BaseMetricLearner\n\n\ndef validate_cov_matrix(M):\n    M = (M + M.T) * 0.5\n    k = 0\n    I = np.eye(M.shape[0])\n    while True:\n        try:\n            _ = np.linalg.cholesky(M)\n            break\n        except np.linalg.LinAlgError:\n            # Find the nearest positive definite matrix for M. Modified from\n            # http://www.mathworks.com/matlabcentral/fileexchange/42885-nearestspd\n            # Might take several minutes\n            k += 1\n            w, v = np.linalg.eig(M)\n            min_eig = v.min()\n            M += (-min_eig * k * k + np.spacing(min_eig)) * I\n    return M\n\n\nclass KISSME(BaseMetricLearner):\n    def __init__(self):\n        self.M_ = None\n\n    def metric(self):\n        return self.M_\n\n    def fit(self, X, y=None):\n        n = X.shape[0]\n        if y is None:\n            y = np.arange(n)\n        X1, X2 = np.meshgrid(np.arange(n), np.arange(n))\n        X1, X2 = X1[X1 < X2], X2[X1 < X2]\n        matches = (y[X1] == y[X2])\n        num_matches = matches.sum()\n        num_non_matches = len(matches) - num_matches\n        idxa = X1[matches]\n        idxb = X2[matches]\n        S = X[idxa] - X[idxb]\n        C1 = S.transpose().dot(S) / num_matches\n        p = np.random.choice(num_non_matches, num_matches, replace=False)\n        idxa = X1[~matches]\n        idxb = X2[~matches]\n        idxa = idxa[p]\n        idxb = idxb[p]\n        S = X[idxa] - X[idxb]\n        C0 = S.transpose().dot(S) / num_matches\n        self.M_ = np.linalg.inv(C1) - np.linalg.inv(C0)\n        self.M_ = validate_cov_matrix(self.M_)\n        self.X_ = X\n'"
reid/models/__init__.py,0,"b'from __future__ import absolute_import\n\nfrom .inception import *\nfrom .resnet import *\n\n\n__factory = {\n    \'inception\': inception,\n    \'resnet18\': resnet18,\n    \'resnet34\': resnet34,\n    \'resnet50\': resnet50,\n    \'resnet101\': resnet101,\n    \'resnet152\': resnet152,\n}\n\n\ndef names():\n    return sorted(__factory.keys())\n\n\ndef create(name, *args, **kwargs):\n    """"""\n    Create a model instance.\n\n    Parameters\n    ----------\n    name : str\n        Model name. Can be one of \'inception\', \'resnet18\', \'resnet34\',\n        \'resnet50\', \'resnet101\', and \'resnet152\'.\n    pretrained : bool, optional\n        Only applied for \'resnet*\' models. If True, will use ImageNet pretrained\n        model. Default: True\n    cut_at_pooling : bool, optional\n        If True, will cut the model before the last global pooling layer and\n        ignore the remaining kwargs. Default: False\n    num_features : int, optional\n        If positive, will append a Linear layer after the global pooling layer,\n        with this number of output units, followed by a BatchNorm layer.\n        Otherwise these layers will not be appended. Default: 256 for\n        \'inception\', 0 for \'resnet*\'\n    norm : bool, optional\n        If True, will normalize the feature to be unit L2-norm for each sample.\n        Otherwise will append a ReLU layer after the above Linear layer if\n        num_features > 0. Default: False\n    dropout : float, optional\n        If positive, will append a Dropout layer with this dropout rate.\n        Default: 0\n    num_classes : int, optional\n        If positive, will append a Linear layer at the end as the classifier\n        with this number of output units. Default: 0\n    """"""\n    if name not in __factory:\n        raise KeyError(""Unknown model:"", name)\n    return __factory[name](*args, **kwargs)\n'"
reid/models/inception.py,3,"b""from __future__ import absolute_import\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.nn import init\n\n\n__all__ = ['InceptionNet', 'inception']\n\n\ndef _make_conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1,\n               bias=False):\n    conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size,\n                     stride=stride, padding=padding, bias=bias)\n    bn = nn.BatchNorm2d(out_planes)\n    relu = nn.ReLU(inplace=True)\n    return nn.Sequential(conv, bn, relu)\n\n\nclass Block(nn.Module):\n    def __init__(self, in_planes, out_planes, pool_method, stride):\n        super(Block, self).__init__()\n        self.branches = nn.ModuleList([\n            nn.Sequential(\n                _make_conv(in_planes, out_planes, kernel_size=1, padding=0),\n                _make_conv(out_planes, out_planes, stride=stride)\n            ),\n            nn.Sequential(\n                _make_conv(in_planes, out_planes, kernel_size=1, padding=0),\n                _make_conv(out_planes, out_planes),\n                _make_conv(out_planes, out_planes, stride=stride))\n        ])\n\n        if pool_method == 'Avg':\n            assert stride == 1\n            self.branches.append(\n                _make_conv(in_planes, out_planes, kernel_size=1, padding=0))\n            self.branches.append(nn.Sequential(\n                nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n                _make_conv(in_planes, out_planes, kernel_size=1, padding=0)))\n        else:\n            self.branches.append(\n                nn.MaxPool2d(kernel_size=3, stride=stride, padding=1))\n\n    def forward(self, x):\n        return torch.cat([b(x) for b in self.branches], 1)\n\n\nclass InceptionNet(nn.Module):\n    def __init__(self, cut_at_pooling=False, num_features=256, norm=False,\n                 dropout=0, num_classes=0):\n        super(InceptionNet, self).__init__()\n        self.cut_at_pooling = cut_at_pooling\n\n        self.conv1 = _make_conv(3, 32)\n        self.conv2 = _make_conv(32, 32)\n        self.conv3 = _make_conv(32, 32)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n        self.in_planes = 32\n        self.inception4a = self._make_inception(64, 'Avg', 1)\n        self.inception4b = self._make_inception(64, 'Max', 2)\n        self.inception5a = self._make_inception(128, 'Avg', 1)\n        self.inception5b = self._make_inception(128, 'Max', 2)\n        self.inception6a = self._make_inception(256, 'Avg', 1)\n        self.inception6b = self._make_inception(256, 'Max', 2)\n\n        if not self.cut_at_pooling:\n            self.num_features = num_features\n            self.norm = norm\n            self.dropout = dropout\n            self.has_embedding = num_features > 0\n            self.num_classes = num_classes\n\n            self.avgpool = nn.AdaptiveAvgPool2d(1)\n\n            if self.has_embedding:\n                self.feat = nn.Linear(self.in_planes, self.num_features)\n                self.feat_bn = nn.BatchNorm1d(self.num_features)\n            else:\n                # Change the num_features to CNN output channels\n                self.num_features = self.in_planes\n            if self.dropout > 0:\n                self.drop = nn.Dropout(self.dropout)\n            if self.num_classes > 0:\n                self.classifier = nn.Linear(self.num_features, self.num_classes)\n\n        self.reset_params()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.pool3(x)\n        x = self.inception4a(x)\n        x = self.inception4b(x)\n        x = self.inception5a(x)\n        x = self.inception5b(x)\n        x = self.inception6a(x)\n        x = self.inception6b(x)\n\n        if self.cut_at_pooling:\n            return x\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n\n        if self.has_embedding:\n            x = self.feat(x)\n            x = self.feat_bn(x)\n        if self.norm:\n            x = F.normalize(x)\n        elif self.has_embedding:\n            x = F.relu(x)\n        if self.dropout > 0:\n            x = self.drop(x)\n        if self.num_classes > 0:\n            x = self.classifier(x)\n        return x\n\n    def _make_inception(self, out_planes, pool_method, stride):\n        block = Block(self.in_planes, out_planes, pool_method, stride)\n        self.in_planes = (out_planes * 4 if pool_method == 'Avg' else\n                          out_planes * 2 + self.in_planes)\n        return block\n\n    def reset_params(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    init.constant(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant(m.weight, 1)\n                init.constant(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                init.normal(m.weight, std=0.001)\n                if m.bias is not None:\n                    init.constant(m.bias, 0)\n\n\ndef inception(**kwargs):\n    return InceptionNet(**kwargs)\n"""
reid/models/resnet.py,2,"b'from __future__ import absolute_import\n\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.nn import init\nimport torchvision\n\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\n\nclass ResNet(nn.Module):\n    __factory = {\n        18: torchvision.models.resnet18,\n        34: torchvision.models.resnet34,\n        50: torchvision.models.resnet50,\n        101: torchvision.models.resnet101,\n        152: torchvision.models.resnet152,\n    }\n\n    def __init__(self, depth, pretrained=True, cut_at_pooling=False,\n                 num_features=0, norm=False, dropout=0, num_classes=0):\n        super(ResNet, self).__init__()\n\n        self.depth = depth\n        self.pretrained = pretrained\n        self.cut_at_pooling = cut_at_pooling\n\n        # Construct base (pretrained) resnet\n        if depth not in ResNet.__factory:\n            raise KeyError(""Unsupported depth:"", depth)\n        self.base = ResNet.__factory[depth](pretrained=pretrained)\n\n        if not self.cut_at_pooling:\n            self.num_features = num_features\n            self.norm = norm\n            self.dropout = dropout\n            self.has_embedding = num_features > 0\n            self.num_classes = num_classes\n\n            out_planes = self.base.fc.in_features\n\n            # Append new layers\n            if self.has_embedding:\n                self.feat = nn.Linear(out_planes, self.num_features)\n                self.feat_bn = nn.BatchNorm1d(self.num_features)\n                init.kaiming_normal(self.feat.weight, mode=\'fan_out\')\n                init.constant(self.feat.bias, 0)\n                init.constant(self.feat_bn.weight, 1)\n                init.constant(self.feat_bn.bias, 0)\n            else:\n                # Change the num_features to CNN output channels\n                self.num_features = out_planes\n            if self.dropout > 0:\n                self.drop = nn.Dropout(self.dropout)\n            if self.num_classes > 0:\n                self.classifier = nn.Linear(self.num_features, self.num_classes)\n                init.normal(self.classifier.weight, std=0.001)\n                init.constant(self.classifier.bias, 0)\n\n        if not self.pretrained:\n            self.reset_params()\n\n    def forward(self, x):\n        for name, module in self.base._modules.items():\n            if name == \'avgpool\':\n                break\n            x = module(x)\n\n        if self.cut_at_pooling:\n            return x\n\n        x = F.avg_pool2d(x, x.size()[2:])\n        x = x.view(x.size(0), -1)\n\n        if self.has_embedding:\n            x = self.feat(x)\n            x = self.feat_bn(x)\n        if self.norm:\n            x = F.normalize(x)\n        elif self.has_embedding:\n            x = F.relu(x)\n        if self.dropout > 0:\n            x = self.drop(x)\n        if self.num_classes > 0:\n            x = self.classifier(x)\n        return x\n\n    def reset_params(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal(m.weight, mode=\'fan_out\')\n                if m.bias is not None:\n                    init.constant(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant(m.weight, 1)\n                init.constant(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                init.normal(m.weight, std=0.001)\n                if m.bias is not None:\n                    init.constant(m.bias, 0)\n\n\ndef resnet18(**kwargs):\n    return ResNet(18, **kwargs)\n\n\ndef resnet34(**kwargs):\n    return ResNet(34, **kwargs)\n\n\ndef resnet50(**kwargs):\n    return ResNet(50, **kwargs)\n\n\ndef resnet101(**kwargs):\n    return ResNet(101, **kwargs)\n\n\ndef resnet152(**kwargs):\n    return ResNet(152, **kwargs)\n'"
reid/utils/__init__.py,3,"b'from __future__ import absolute_import\n\nimport torch\n\n\ndef to_numpy(tensor):\n    if torch.is_tensor(tensor):\n        return tensor.cpu().numpy()\n    elif type(tensor).__module__ != \'numpy\':\n        raise ValueError(""Cannot convert {} to numpy array""\n                         .format(type(tensor)))\n    return tensor\n\n\ndef to_torch(ndarray):\n    if type(ndarray).__module__ == \'numpy\':\n        return torch.from_numpy(ndarray)\n    elif not torch.is_tensor(ndarray):\n        raise ValueError(""Cannot convert {} to torch tensor""\n                         .format(type(ndarray)))\n    return ndarray\n'"
reid/utils/logging.py,0,"b""from __future__ import absolute_import\nimport os\nimport sys\n\nfrom .osutils import mkdir_if_missing\n\n\nclass Logger(object):\n    def __init__(self, fpath=None):\n        self.console = sys.stdout\n        self.file = None\n        if fpath is not None:\n            mkdir_if_missing(os.path.dirname(fpath))\n            self.file = open(fpath, 'w')\n\n    def __del__(self):\n        self.close()\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, *args):\n        self.close()\n\n    def write(self, msg):\n        self.console.write(msg)\n        if self.file is not None:\n            self.file.write(msg)\n\n    def flush(self):\n        self.console.flush()\n        if self.file is not None:\n            self.file.flush()\n            os.fsync(self.file.fileno())\n\n    def close(self):\n        self.console.close()\n        if self.file is not None:\n            self.file.close()\n"""
reid/utils/meters.py,0,"b'from __future__ import absolute_import\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n'"
reid/utils/osutils.py,0,b'from __future__ import absolute_import\nimport os\nimport errno\n\n\ndef mkdir_if_missing(dir_path):\n    try:\n        os.makedirs(dir_path)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n'
reid/utils/serialization.py,3,"b'from __future__ import print_function, absolute_import\nimport json\nimport os.path as osp\nimport shutil\n\nimport torch\nfrom torch.nn import Parameter\n\nfrom .osutils import mkdir_if_missing\n\n\ndef read_json(fpath):\n    with open(fpath, \'r\') as f:\n        obj = json.load(f)\n    return obj\n\n\ndef write_json(obj, fpath):\n    mkdir_if_missing(osp.dirname(fpath))\n    with open(fpath, \'w\') as f:\n        json.dump(obj, f, indent=4, separators=(\',\', \': \'))\n\n\ndef save_checkpoint(state, is_best, fpath=\'checkpoint.pth.tar\'):\n    mkdir_if_missing(osp.dirname(fpath))\n    torch.save(state, fpath)\n    if is_best:\n        shutil.copy(fpath, osp.join(osp.dirname(fpath), \'model_best.pth.tar\'))\n\n\ndef load_checkpoint(fpath):\n    if osp.isfile(fpath):\n        checkpoint = torch.load(fpath)\n        print(""=> Loaded checkpoint \'{}\'"".format(fpath))\n        return checkpoint\n    else:\n        raise ValueError(""=> No checkpoint found at \'{}\'"".format(fpath))\n\n\ndef copy_state_dict(state_dict, model, strip=None):\n    tgt_state = model.state_dict()\n    copied_names = set()\n    for name, param in state_dict.items():\n        if strip is not None and name.startswith(strip):\n            name = name[len(strip):]\n        if name not in tgt_state:\n            continue\n        if isinstance(param, Parameter):\n            param = param.data\n        if param.size() != tgt_state[name].size():\n            print(\'mismatch:\', name, param.size(), tgt_state[name].size())\n            continue\n        tgt_state[name].copy_(param)\n        copied_names.add(name)\n\n    missing = set(tgt_state.keys()) - copied_names\n    if len(missing) > 0:\n        print(""missing keys in state_dict:"", missing)\n\n    return model\n'"
test/datasets/test_cuhk01.py,0,"b""from unittest import TestCase\n\n\nclass TestCUHK01(TestCase):\n    def test_init(self):\n        import os.path as osp\n        from reid.datasets import CUHK01\n        from reid.utils.serialization import read_json\n\n        root, split_id, num_val = '/tmp/open-reid/cuhk01', 0, 100\n        dataset = CUHK01(root, split_id=split_id, num_val=num_val, download=True)\n\n        self.assertTrue(osp.isfile(osp.join(root, 'meta.json')))\n        self.assertTrue(osp.isfile(osp.join(root, 'splits.json')))\n        meta = read_json(osp.join(root, 'meta.json'))\n        self.assertEquals(len(meta['identities']), 971)\n        splits = read_json(osp.join(root, 'splits.json'))\n        self.assertEquals(len(splits), 10)\n\n        self.assertDictEqual(meta, dataset.meta)\n        self.assertDictEqual(splits[split_id], dataset.split)\n"""
test/datasets/test_cuhk03.py,0,"b""from unittest import TestCase\n\n\nclass TestCUHK03(TestCase):\n    def test_init(self):\n        import os.path as osp\n        from reid.datasets.cuhk03 import CUHK03\n        from reid.utils.serialization import read_json\n\n        root, split_id, num_val = '/tmp/open-reid/cuhk03', 0, 100\n        dataset = CUHK03(root, split_id=split_id, num_val=num_val, download=True)\n\n        self.assertTrue(osp.isfile(osp.join(root, 'meta.json')))\n        self.assertTrue(osp.isfile(osp.join(root, 'splits.json')))\n        meta = read_json(osp.join(root, 'meta.json'))\n        self.assertEquals(len(meta['identities']), 1467)\n        splits = read_json(osp.join(root, 'splits.json'))\n        self.assertEquals(len(splits), 20)\n\n        self.assertDictEqual(meta, dataset.meta)\n        self.assertDictEqual(splits[split_id], dataset.split)\n"""
test/datasets/test_dukemtmc.py,0,"b""from unittest import TestCase\n\n\nclass TestDukeMTMC(TestCase):\n    def test_all(self):\n        import os.path as osp\n        from reid.datasets import DukeMTMC\n        from reid.utils.serialization import read_json\n\n        root, split_id, num_val = '/tmp/open-reid/dukemtmc', 0, 100\n        dataset = DukeMTMC(root, split_id=split_id, num_val=num_val,\n                           download=True)\n\n        self.assertTrue(osp.isfile(osp.join(root, 'meta.json')))\n        self.assertTrue(osp.isfile(osp.join(root, 'splits.json')))\n        meta = read_json(osp.join(root, 'meta.json'))\n        self.assertEquals(len(meta['identities']), 1812)\n        splits = read_json(osp.join(root, 'splits.json'))\n        self.assertEquals(len(splits), 1)\n\n        self.assertDictEqual(meta, dataset.meta)\n        self.assertDictEqual(splits[split_id], dataset.split)"""
test/datasets/test_market1501.py,0,"b""from unittest import TestCase\n\n\nclass TestMarket1501(TestCase):\n    def test_init(self):\n        import os.path as osp\n        from reid.datasets.market1501 import Market1501\n        from reid.utils.serialization import read_json\n\n        root, split_id, num_val = '/tmp/open-reid/market1501', 0, 100\n        dataset = Market1501(root, split_id=split_id, num_val=num_val,\n                             download=True)\n\n        self.assertTrue(osp.isfile(osp.join(root, 'meta.json')))\n        self.assertTrue(osp.isfile(osp.join(root, 'splits.json')))\n        meta = read_json(osp.join(root, 'meta.json'))\n        self.assertEquals(len(meta['identities']), 1502)\n        splits = read_json(osp.join(root, 'splits.json'))\n        self.assertEquals(len(splits), 1)\n\n        self.assertDictEqual(meta, dataset.meta)\n        self.assertDictEqual(splits[split_id], dataset.split)\n"""
test/datasets/test_viper.py,0,"b""from unittest import TestCase\n\n\nclass TestVIPeR(TestCase):\n    def test_init(self):\n        import os.path as osp\n        from reid.datasets.viper import VIPeR\n        from reid.utils.serialization import read_json\n\n        root, split_id, num_val = '/tmp/open-reid/viper', 0, 100\n        dataset = VIPeR(root, split_id=split_id, num_val=num_val, download=True)\n\n        self.assertTrue(osp.isfile(osp.join(root, 'meta.json')))\n        self.assertTrue(osp.isfile(osp.join(root, 'splits.json')))\n        meta = read_json(osp.join(root, 'meta.json'))\n        self.assertEquals(len(meta['identities']), 632)\n        splits = read_json(osp.join(root, 'splits.json'))\n        self.assertEquals(len(splits), 10)\n\n        self.assertDictEqual(meta, dataset.meta)\n        self.assertDictEqual(splits[split_id], dataset.split)\n"""
test/evaluation_metrics/test_cmc.py,0,"b'from unittest import TestCase\nimport numpy as np\n\nfrom reid.evaluation_metrics import cmc\n\n\nclass TestCMC(TestCase):\n    def test_only_distmat(self):\n        distmat = np.array([[0, 1, 2, 3, 4],\n                            [1, 0, 2, 3, 4],\n                            [0, 1, 2, 3, 4],\n                            [0, 1, 2, 3, 4],\n                            [1, 2, 3, 4, 0]])\n        ret = cmc(distmat)\n        self.assertTrue(np.all(ret[:5] == [0.6, 0.6, 0.8, 1.0, 1.0]))\n\n    def test_duplicate_ids(self):\n        distmat = np.tile(np.arange(4), (4, 1))\n        query_ids = [0, 0, 1, 1]\n        gallery_ids = [0, 0, 1, 1]\n        ret = cmc(distmat, query_ids=query_ids, gallery_ids=gallery_ids, topk=4,\n                  separate_camera_set=False, single_gallery_shot=False)\n        self.assertTrue(np.all(ret == [0.5, 0.5, 1, 1]))\n\n    def test_duplicate_cams(self):\n        distmat = np.tile(np.arange(5), (5, 1))\n        query_ids = [0,0,0,1,1]\n        gallery_ids = [0,0,0,1,1]\n        query_cams = [0,0,0,0,0]\n        gallery_cams = [0,1,1,1,1]\n        ret = cmc(distmat, query_ids=query_ids, gallery_ids=gallery_ids,\n                  query_cams=query_cams, gallery_cams=gallery_cams, topk=5,\n                  separate_camera_set=False, single_gallery_shot=False)\n        self.assertTrue(np.all(ret == [0.6, 0.6, 0.6, 1, 1]))\n'"
test/feature_extraction/test_database.py,0,"b""from unittest import TestCase\n\nimport numpy as np\n\nfrom reid.feature_extraction.database import FeatureDatabase\n\n\nclass TestFeatureDatabase(TestCase):\n    def test_all(self):\n        with FeatureDatabase('/tmp/open-reid/test.h5', 'w') as db:\n            db['img1'] = np.random.rand(3, 8, 8).astype(np.float32)\n            db['img2'] = np.arange(10)\n            db['img2'] = np.arange(10).reshape(2, 5).astype(np.float32)\n        with FeatureDatabase('/tmp/open-reid/test.h5', 'r') as db:\n            self.assertTrue('img1' in db)\n            self.assertTrue('img2' in db)\n            self.assertEquals(db['img1'].shape, (3, 8, 8))\n            x = db['img2']\n            self.assertEquals(x.shape, (2, 5))\n            self.assertTrue(np.all(x == np.arange(10).reshape(2, 5)))\n\n"""
test/loss/test_oim.py,9,"b'from unittest import TestCase\n\n\nclass TestOIMLoss(TestCase):\n    def test_forward_backward(self):\n        import torch\n        import torch.nn.functional as F\n        from torch.autograd import Variable\n        from reid.loss import OIMLoss\n        criterion = OIMLoss(3, 3, scalar=1.0, size_average=False)\n        criterion.lut = torch.eye(3)\n        x = Variable(torch.randn(3, 3), requires_grad=True)\n        y = Variable(torch.range(0, 2).long())\n        loss = criterion(x, y)\n        loss.backward()\n        probs = F.softmax(x)\n        grads = probs.data - torch.eye(3)\n        abs_diff = torch.abs(grads - x.grad.data)\n        self.assertEquals(torch.log(probs).diag().sum(), -loss)\n        self.assertTrue(torch.max(abs_diff) < 1e-6)\n'"
test/models/test_inception.py,3,"b'from unittest import TestCase\n\n\nclass TestInception(TestCase):\n    def test_forward(self):\n        import torch\n        from torch.autograd import Variable\n        from reid.models.inception import InceptionNet\n\n        # model = Inception(num_classes=5, num_features=256, dropout=0.5)\n        # x = Variable(torch.randn(10, 3, 144, 56), requires_grad=False)\n        # y = model(x)\n        # self.assertEquals(y.size(), (10, 5))\n\n        model = InceptionNet(num_features=8, norm=True, dropout=0)\n        x = Variable(torch.randn(10, 3, 144, 56), requires_grad=False)\n        y = model(x)\n        self.assertEquals(y.size(), (10, 8))\n        self.assertEquals(y.norm(2, 1).max(), 1)\n        self.assertEquals(y.norm(2, 1).min(), 1)\n'"
reid/utils/data/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .dataset import Dataset\nfrom .preprocessor import Preprocessor\n'
reid/utils/data/dataset.py,0,"b'from __future__ import print_function\nimport os.path as osp\n\nimport numpy as np\n\nfrom ..serialization import read_json\n\n\ndef _pluck(identities, indices, relabel=False):\n    ret = []\n    for index, pid in enumerate(indices):\n        pid_images = identities[pid]\n        for camid, cam_images in enumerate(pid_images):\n            for fname in cam_images:\n                name = osp.splitext(fname)[0]\n                x, y, _ = map(int, name.split(\'_\'))\n                assert pid == x and camid == y\n                if relabel:\n                    ret.append((fname, index, camid))\n                else:\n                    ret.append((fname, pid, camid))\n    return ret\n\n\nclass Dataset(object):\n    def __init__(self, root, split_id=0):\n        self.root = root\n        self.split_id = split_id\n        self.meta = None\n        self.split = None\n        self.train, self.val, self.trainval = [], [], []\n        self.query, self.gallery = [], []\n        self.num_train_ids, self.num_val_ids, self.num_trainval_ids = 0, 0, 0\n\n    @property\n    def images_dir(self):\n        return osp.join(self.root, \'images\')\n\n    def load(self, num_val=0.3, verbose=True):\n        splits = read_json(osp.join(self.root, \'splits.json\'))\n        if self.split_id >= len(splits):\n            raise ValueError(""split_id exceeds total splits {}""\n                             .format(len(splits)))\n        self.split = splits[self.split_id]\n\n        # Randomly split train / val\n        trainval_pids = np.asarray(self.split[\'trainval\'])\n        np.random.shuffle(trainval_pids)\n        num = len(trainval_pids)\n        if isinstance(num_val, float):\n            num_val = int(round(num * num_val))\n        if num_val >= num or num_val < 0:\n            raise ValueError(""num_val exceeds total identities {}""\n                             .format(num))\n        train_pids = sorted(trainval_pids[:-num_val])\n        val_pids = sorted(trainval_pids[-num_val:])\n\n        self.meta = read_json(osp.join(self.root, \'meta.json\'))\n        identities = self.meta[\'identities\']\n        self.train = _pluck(identities, train_pids, relabel=True)\n        self.val = _pluck(identities, val_pids, relabel=True)\n        self.trainval = _pluck(identities, trainval_pids, relabel=True)\n        self.query = _pluck(identities, self.split[\'query\'])\n        self.gallery = _pluck(identities, self.split[\'gallery\'])\n        self.num_train_ids = len(train_pids)\n        self.num_val_ids = len(val_pids)\n        self.num_trainval_ids = len(trainval_pids)\n\n        if verbose:\n            print(self.__class__.__name__, ""dataset loaded"")\n            print(""  subset   | # ids | # images"")\n            print(""  ---------------------------"")\n            print(""  train    | {:5d} | {:8d}""\n                  .format(self.num_train_ids, len(self.train)))\n            print(""  val      | {:5d} | {:8d}""\n                  .format(self.num_val_ids, len(self.val)))\n            print(""  trainval | {:5d} | {:8d}""\n                  .format(self.num_trainval_ids, len(self.trainval)))\n            print(""  query    | {:5d} | {:8d}""\n                  .format(len(self.split[\'query\']), len(self.query)))\n            print(""  gallery  | {:5d} | {:8d}""\n                  .format(len(self.split[\'gallery\']), len(self.gallery)))\n\n    def _check_integrity(self):\n        return osp.isdir(osp.join(self.root, \'images\')) and \\\n               osp.isfile(osp.join(self.root, \'meta.json\')) and \\\n               osp.isfile(osp.join(self.root, \'splits.json\'))\n'"
reid/utils/data/preprocessor.py,0,"b""from __future__ import absolute_import\nimport os.path as osp\n\nfrom PIL import Image\n\n\nclass Preprocessor(object):\n    def __init__(self, dataset, root=None, transform=None):\n        super(Preprocessor, self).__init__()\n        self.dataset = dataset\n        self.root = root\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, indices):\n        if isinstance(indices, (tuple, list)):\n            return [self._get_single_item(index) for index in indices]\n        return self._get_single_item(indices)\n\n    def _get_single_item(self, index):\n        fname, pid, camid = self.dataset[index]\n        fpath = fname\n        if self.root is not None:\n            fpath = osp.join(self.root, fname)\n        img = Image.open(fpath).convert('RGB')\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, fname, pid, camid\n"""
reid/utils/data/sampler.py,2,"b'from __future__ import absolute_import\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\nfrom torch.utils.data.sampler import (\n    Sampler, SequentialSampler, RandomSampler, SubsetRandomSampler,\n    WeightedRandomSampler)\n\n\nclass RandomIdentitySampler(Sampler):\n    def __init__(self, data_source, num_instances=1):\n        self.data_source = data_source\n        self.num_instances = num_instances\n        self.index_dic = defaultdict(list)\n        for index, (_, pid, _) in enumerate(data_source):\n            self.index_dic[pid].append(index)\n        self.pids = list(self.index_dic.keys())\n        self.num_samples = len(self.pids)\n\n    def __len__(self):\n        return self.num_samples * self.num_instances\n\n    def __iter__(self):\n        indices = torch.randperm(self.num_samples)\n        ret = []\n        for i in indices:\n            pid = self.pids[i]\n            t = self.index_dic[pid]\n            if len(t) >= self.num_instances:\n                t = np.random.choice(t, size=self.num_instances, replace=False)\n            else:\n                t = np.random.choice(t, size=self.num_instances, replace=True)\n            ret.extend(t)\n        return iter(ret)\n'"
reid/utils/data/transforms.py,0,"b'from __future__ import absolute_import\n\nfrom torchvision.transforms import *\nfrom PIL import Image\nimport random\nimport math\n\n\nclass RectScale(object):\n    def __init__(self, height, width, interpolation=Image.BILINEAR):\n        self.height = height\n        self.width = width\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        w, h = img.size\n        if h == self.height and w == self.width:\n            return img\n        return img.resize((self.width, self.height), self.interpolation)\n\n\nclass RandomSizedRectCrop(object):\n    def __init__(self, height, width, interpolation=Image.BILINEAR):\n        self.height = height\n        self.width = width\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        for attempt in range(10):\n            area = img.size[0] * img.size[1]\n            target_area = random.uniform(0.64, 1.0) * area\n            aspect_ratio = random.uniform(2, 3)\n\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w <= img.size[0] and h <= img.size[1]:\n                x1 = random.randint(0, img.size[0] - w)\n                y1 = random.randint(0, img.size[1] - h)\n\n                img = img.crop((x1, y1, x1 + w, y1 + h))\n                assert(img.size == (w, h))\n\n                return img.resize((self.width, self.height), self.interpolation)\n\n        # Fallback\n        scale = RectScale(self.height, self.width,\n                          interpolation=self.interpolation)\n        return scale(img)\n'"
test/utils/data/test_preprocessor.py,0,"b""from unittest import TestCase\n\n\nclass TestPreprocessor(TestCase):\n    def test_getitem(self):\n        import torchvision.transforms as t\n        from reid.datasets.viper import VIPeR\n        from reid.utils.data.preprocessor import Preprocessor\n\n        root, split_id, num_val = '/tmp/open-reid/viper', 0, 100\n        dataset = VIPeR(root, split_id=split_id, num_val=num_val, download=True)\n\n        preproc = Preprocessor(dataset.train, root=dataset.images_dir,\n                               transform=t.Compose([\n                                   t.Scale(256),\n                                   t.CenterCrop(224),\n                                   t.ToTensor(),\n                                   t.Normalize(mean=[0.485, 0.456, 0.406],\n                                               std=[0.229, 0.224, 0.225])\n                               ]))\n        self.assertEquals(len(preproc), len(dataset.train))\n        img, pid, camid = preproc[0]\n        self.assertEquals(img.size(), (3, 224, 224))\n"""
