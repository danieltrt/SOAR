file_path,api_count,code
setup.py,0,"b'#####################################################\n# Copyright (c) Xuanyi Dong [GitHub D-X-Y], 2019.08 #\n#####################################################\n# [2020.02.25] Initialize the API as v1.1\n# [2020.03.09] Upgrade the API to v1.2\n# [2020.03.16] Upgrade the API to v1.3\nimport os\nfrom setuptools import setup\n\n\ndef read(fname=\'README.md\'):\n  with open(os.path.join(os.path.dirname(__file__), fname), encoding=\'utf-8\') as cfile:\n    return cfile.read()\n\n\nsetup(\n    name = ""nas_bench_201"",\n    version = ""1.3"",\n    author = ""Xuanyi Dong"",\n    author_email = ""dongxuanyi888@gmail.com"",\n    description = ""API for NAS-Bench-201 (a benchmark for neural architecture search)."",\n    license = ""MIT"",\n    keywords = ""NAS Dataset API DeepLearning"",\n    url = ""https://github.com/D-X-Y/NAS-Bench-201"",\n    packages=[\'nas_201_api\'],\n    long_description=read(\'README.md\'),\n    long_description_content_type=\'text/markdown\',\n    classifiers=[\n        ""Programming Language :: Python"",\n        ""Topic :: Database"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n        ""License :: OSI Approved :: MIT License"",\n    ],\n)\n'"
nas_201_api/__init__.py,0,"b'#####################################################\n# Copyright (c) Xuanyi Dong [GitHub D-X-Y], 2019.08 #\n#####################################################\nfrom .api import NASBench201API\nfrom .api import ArchResults, ResultsCount\n\n# NAS_BENCH_201_API_VERSION=""v1.1""  # [2020.02.25]\n# NAS_BENCH_201_API_VERSION=""v1.2""  # [2020.03.09]\nNAS_BENCH_201_API_VERSION=""v1.3""  # [2020.03.16]\n'"
nas_201_api/api.py,3,"b'#####################################################\n# Copyright (c) Xuanyi Dong [GitHub D-X-Y], 2019.08 #\n############################################################################################\n# NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search, ICLR 2020 #\n############################################################################################\n# The history of benchmark files:\n# [2020.02.25] NAS-Bench-201-v1_0-e61699.pth : 6219 architectures are trained once, 1621 architectures are trained twice, 7785 architectures are trained three times. `LESS` only supports CIFAR10-VALID.\n# [2020.03.16] NAS-Bench-201-v1_1-096897.pth : 2225 architectures are trained once, 5439 archiitectures are trained twice, 7961 architectures are trained three times on all training sets. For the hyper-parameters with the total epochs of 12, each model is trained on CIFAR-10, CIFAR-100, ImageNet16-120 once, and is trained on CIFAR-10-VALID twice.\n#\n# I\'m still actively enhancing this benchmark. Please feel free to contact me if you have any question w.r.t. NAS-Bench-201.\n#\nimport os, copy, random, torch, numpy as np\nfrom pathlib import Path\nfrom typing import List, Text, Union, Dict\nfrom collections import OrderedDict, defaultdict\n\n\ndef print_information(information, extra_info=None, show=False):\n  dataset_names = information.get_dataset_names()\n  strings = [information.arch_str, \'datasets : {:}, extra-info : {:}\'.format(dataset_names, extra_info)]\n  def metric2str(loss, acc):\n    return \'loss = {:.3f}, top1 = {:.2f}%\'.format(loss, acc)\n\n  for ida, dataset in enumerate(dataset_names):\n    metric = information.get_compute_costs(dataset)\n    flop, param, latency = metric[\'flops\'], metric[\'params\'], metric[\'latency\']\n    str1 = \'{:14s} FLOP={:6.2f} M, Params={:.3f} MB, latency={:} ms.\'.format(dataset, flop, param, \'{:.2f}\'.format(latency*1000) if latency is not None and latency > 0 else None)\n    train_info = information.get_metrics(dataset, \'train\')\n    if dataset == \'cifar10-valid\':\n      valid_info = information.get_metrics(dataset, \'x-valid\')\n      str2 = \'{:14s} train : [{:}], valid : [{:}]\'.format(dataset, metric2str(train_info[\'loss\'], train_info[\'accuracy\']), metric2str(valid_info[\'loss\'], valid_info[\'accuracy\']))\n    elif dataset == \'cifar10\':\n      test__info = information.get_metrics(dataset, \'ori-test\')\n      str2 = \'{:14s} train : [{:}], test  : [{:}]\'.format(dataset, metric2str(train_info[\'loss\'], train_info[\'accuracy\']), metric2str(test__info[\'loss\'], test__info[\'accuracy\']))\n    else:\n      valid_info = information.get_metrics(dataset, \'x-valid\')\n      test__info = information.get_metrics(dataset, \'x-test\')\n      str2 = \'{:14s} train : [{:}], valid : [{:}], test : [{:}]\'.format(dataset, metric2str(train_info[\'loss\'], train_info[\'accuracy\']), metric2str(valid_info[\'loss\'], valid_info[\'accuracy\']), metric2str(test__info[\'loss\'], test__info[\'accuracy\']))\n    strings += [str1, str2]\n  if show: print(\'\\n\'.join(strings))\n  return strings\n\n""""""\nThis is the class for API of NAS-Bench-201.\n""""""\nclass NASBench201API(object):\n\n  """""" The initialization function that takes the dataset file path (or a dict loaded from that path) as input. """"""\n  def __init__(self, file_path_or_dict: Union[Text, Dict], verbose: bool=True):\n    self.filename = None\n    if isinstance(file_path_or_dict, str) or isinstance(file_path_or_dict, Path):\n      file_path_or_dict = str(file_path_or_dict)\n      if verbose: print(\'try to create the NAS-Bench-201 api from {:}\'.format(file_path_or_dict))\n      assert os.path.isfile(file_path_or_dict), \'invalid path : {:}\'.format(file_path_or_dict)\n      self.filename = Path(file_path_or_dict).name\n      file_path_or_dict = torch.load(file_path_or_dict, map_location=\'cpu\')\n    elif isinstance(file_path_or_dict, dict):\n      file_path_or_dict = copy.deepcopy( file_path_or_dict )\n    else: raise ValueError(\'invalid type : {:} not in [str, dict]\'.format(type(file_path_or_dict)))\n    assert isinstance(file_path_or_dict, dict), \'It should be a dict instead of {:}\'.format(type(file_path_or_dict))\n    self.verbose = verbose # [TODO] a flag indicating whether to print more logs\n    keys = (\'meta_archs\', \'arch2infos\', \'evaluated_indexes\')\n    for key in keys: assert key in file_path_or_dict, \'Can not find key[{:}] in the dict\'.format(key)\n    self.meta_archs = copy.deepcopy( file_path_or_dict[\'meta_archs\'] )\n    self.arch2infos_less = OrderedDict()\n    self.arch2infos_full = OrderedDict()\n    for xkey in sorted(list(file_path_or_dict[\'arch2infos\'].keys())):\n      all_info = file_path_or_dict[\'arch2infos\'][xkey]\n      self.arch2infos_less[xkey] = ArchResults.create_from_state_dict( all_info[\'less\'] )\n      self.arch2infos_full[xkey] = ArchResults.create_from_state_dict( all_info[\'full\'] )\n    self.evaluated_indexes = sorted(list(file_path_or_dict[\'evaluated_indexes\']))\n    self.archstr2index = {}\n    for idx, arch in enumerate(self.meta_archs):\n      #assert arch.tostr() not in self.archstr2index, \'This [{:}]-th arch {:} already in the dict ({:}).\'.format(idx, arch, self.archstr2index[arch.tostr()])\n      assert arch not in self.archstr2index, \'This [{:}]-th arch {:} already in the dict ({:}).\'.format(idx, arch, self.archstr2index[arch])\n      self.archstr2index[ arch ] = idx\n\n  def __getitem__(self, index: int):\n    return copy.deepcopy( self.meta_archs[index] )\n\n  def __len__(self):\n    return len(self.meta_archs)\n\n  def __repr__(self):\n    return (\'{name}({num}/{total} architectures, file={filename})\'.format(name=self.__class__.__name__, num=len(self.evaluated_indexes), total=len(self.meta_archs), filename=self.filename))\n\n  def random(self):\n    """"""Return a random index of all architectures.""""""\n    return random.randint(0, len(self.meta_archs)-1)\n\n  # This function is used to query the index of an architecture in the search space.\n  # The input arch can be an architecture string such as \'|nor_conv_3x3~0|+|nor_conv_3x3~0|avg_pool_3x3~1|+|skip_connect~0|nor_conv_3x3~1|skip_connect~2|\'\n  # or an instance that has the \'tostr\' function that can generate the architecture string.\n  # This function will return the index.\n  #   If return -1, it means this architecture is not in the search space.\n  #   Otherwise, it will return an int in [0, the-number-of-candidates-in-the-search-space).\n  def query_index_by_arch(self, arch):\n    if isinstance(arch, str):\n      if arch in self.archstr2index: arch_index = self.archstr2index[ arch ]\n      else                         : arch_index = -1\n    elif hasattr(arch, \'tostr\'):\n      if arch.tostr() in self.archstr2index: arch_index = self.archstr2index[ arch.tostr() ]\n      else                                 : arch_index = -1\n    else: arch_index = -1\n    return arch_index\n\n  def reload(self, archive_root: Text, index: int):\n    """"""Overwrite all information of the \'index\'-th architecture in the search space.\n         It will load its data from \'archive_root\'.\n    """"""\n    assert os.path.isdir(archive_root), \'invalid directory : {:}\'.format(archive_root)\n    xfile_path = os.path.join(archive_root, \'{:06d}-FULL.pth\'.format(index))\n    assert 0 <= index < len(self.meta_archs), \'invalid index of {:}\'.format(index)\n    assert os.path.isfile(xfile_path), \'invalid data path : {:}\'.format(xfile_path)\n    xdata = torch.load(xfile_path, map_location=\'cpu\')\n    assert isinstance(xdata, dict) and \'full\' in xdata and \'less\' in xdata, \'invalid format of data in {:}\'.format(xfile_path)\n    if index in self.arch2infos_less: del self.arch2infos_less[index]\n    if index in self.arch2infos_full: del self.arch2infos_full[index]\n    self.arch2infos_less[index] = ArchResults.create_from_state_dict( xdata[\'less\'] )\n    self.arch2infos_full[index] = ArchResults.create_from_state_dict( xdata[\'full\'] )\n\n  def clear_params(self, index: int, use_12epochs_result: Union[bool, None]):\n    """"""Remove the architecture\'s weights to save memory.\n    :arg\n      index: the index of the target architecture\n      use_12epochs_result: a flag to controll how to clear the parameters.\n        -- None: clear all the weights in both `less` and `full`, which indicates the training hyper-parameters.\n        -- True: clear all the weights in arch2infos_less, which by default is 12-epoch-training result.\n        -- False: clear all the weights in arch2infos_full, which by default is 200-epoch-training result.\n    """"""\n    if use_12epochs_result is None:\n      self.arch2infos_less[index].clear_params()\n      self.arch2infos_full[index].clear_params()\n    else:\n      if use_12epochs_result: arch2infos = self.arch2infos_less\n      else                  : arch2infos = self.arch2infos_full\n      arch2infos[index].clear_params()\n  \n  # This function is used to query the information of a specific archiitecture\n  # \'arch\' can be an architecture index or an architecture string\n  # When use_12epochs_result=True, the hyper-parameters used to train a model are in \'configs/nas-benchmark/CIFAR.config\'\n  # When use_12epochs_result=False, the hyper-parameters used to train a model are in \'configs/nas-benchmark/LESS.config\'\n  # The difference between these two configurations are the number of training epochs, which is 200 in CIFAR.config and 12 in LESS.config.\n  def query_by_arch(self, arch, use_12epochs_result=False):\n    if isinstance(arch, int):\n      arch_index = arch\n    else:\n      arch_index = self.query_index_by_arch(arch)\n    if arch_index == -1: return None # the following two lines are used to support few training epochs\n    if use_12epochs_result: arch2infos = self.arch2infos_less\n    else                  : arch2infos = self.arch2infos_full\n    if arch_index in arch2infos:\n      strings = print_information(arch2infos[ arch_index ], \'arch-index={:}\'.format(arch_index))\n      return \'\\n\'.join(strings)\n    else:\n      print (\'Find this arch-index : {:}, but this arch is not evaluated.\'.format(arch_index))\n      return None\n\n  # This \'query_by_index\' function is used to query information with the training of 12 epochs or 200 epochs.\n  # ------\n  # If use_12epochs_result=True, we train the model by 12 epochs (see config in configs/nas-benchmark/LESS.config)\n  # If use_12epochs_result=False, we train the model by 200 epochs (see config in configs/nas-benchmark/CIFAR.config)\n  # ------\n  # If dataname is None, return the ArchResults\n  # else, return a dict with all trials on that dataset (the key is the seed)\n  # Options are \'cifar10-valid\', \'cifar10\', \'cifar100\', \'ImageNet16-120\'.\n  #  -- cifar10-valid : training the model on the CIFAR-10 training set.\n  #  -- cifar10 : training the model on the CIFAR-10 training + validation set.\n  #  -- cifar100 : training the model on the CIFAR-100 training set.\n  #  -- ImageNet16-120 : training the model on the ImageNet16-120 training set.\n  def query_by_index(self, arch_index: int, dataname: Union[None, Text] = None,\n                     use_12epochs_result: bool = False):\n    if use_12epochs_result: basestr, arch2infos = \'12epochs\' , self.arch2infos_less\n    else                  : basestr, arch2infos = \'200epochs\', self.arch2infos_full\n    assert arch_index in arch2infos, \'arch_index [{:}] does not in arch2info with {:}\'.format(arch_index, basestr)\n    archInfo = copy.deepcopy( arch2infos[ arch_index ] )\n    if dataname is None: return archInfo\n    else:\n      assert dataname in archInfo.get_dataset_names(), \'invalid dataset-name : {:}\'.format(dataname)\n      info = archInfo.query(dataname)\n      return info\n\n  def query_meta_info_by_index(self, arch_index, use_12epochs_result=False):\n    if use_12epochs_result: basestr, arch2infos = \'12epochs\' , self.arch2infos_less\n    else                  : basestr, arch2infos = \'200epochs\', self.arch2infos_full\n    assert arch_index in arch2infos, \'arch_index [{:}] does not in arch2info with {:}\'.format(arch_index, basestr)\n    archInfo = copy.deepcopy( arch2infos[ arch_index ] )\n    return archInfo\n\n  def find_best(self, dataset, metric_on_set, FLOP_max=None, Param_max=None, use_12epochs_result=False):\n    """"""Find the architecture with the highest accuracy based on some constraints.""""""\n    if use_12epochs_result: basestr, arch2infos = \'12epochs\' , self.arch2infos_less\n    else                  : basestr, arch2infos = \'200epochs\', self.arch2infos_full\n    best_index, highest_accuracy = -1, None\n    for i, idx in enumerate(self.evaluated_indexes):\n      info = arch2infos[idx].get_compute_costs(dataset)\n      flop, param, latency = info[\'flops\'], info[\'params\'], info[\'latency\']\n      if FLOP_max  is not None and flop  > FLOP_max : continue\n      if Param_max is not None and param > Param_max: continue\n      xinfo = arch2infos[idx].get_metrics(dataset, metric_on_set)\n      loss, accuracy = xinfo[\'loss\'], xinfo[\'accuracy\']\n      if best_index == -1:\n        best_index, highest_accuracy = idx, accuracy\n      elif highest_accuracy < accuracy:\n        best_index, highest_accuracy = idx, accuracy\n    return best_index, highest_accuracy\n\n  def arch(self, index: int):\n    """"""Return the topology structure of the `index`-th architecture.""""""\n    assert 0 <= index < len(self.meta_archs), \'invalid index : {:} vs. {:}.\'.format(index, len(self.meta_archs))\n    return copy.deepcopy(self.meta_archs[index])\n\n  def get_net_param(self, index, dataset, seed, use_12epochs_result=False):\n    """"""\n      This function is used to obtain the trained weights of the `index`-th architecture on `dataset` with the seed of `seed`\n      Args [seed]:\n        -- None : return a dict containing the trained weights of all trials, where each key is a seed and its corresponding value is the weights.\n        -- a interger : return the weights of a specific trial, whose seed is this interger.\n      Args [use_12epochs_result]:\n        -- True : train the model by 12 epochs\n        -- False : train the model by 200 epochs\n    """"""\n    if use_12epochs_result: arch2infos = self.arch2infos_less\n    else: arch2infos = self.arch2infos_full\n    arch_result = arch2infos[index]\n    return arch_result.get_net_param(dataset, seed)\n\n  def get_net_config(self, index: int, dataset: Text):\n    """"""\n      This function is used to obtain the configuration for the `index`-th architecture on `dataset`.\n      Args [dataset] (4 possible options):\n        -- cifar10-valid : training the model on the CIFAR-10 training set.\n        -- cifar10 : training the model on the CIFAR-10 training + validation set.\n        -- cifar100 : training the model on the CIFAR-100 training set.\n        -- ImageNet16-120 : training the model on the ImageNet16-120 training set.\n      This function will return a dict.\n      ========= Some examlpes for using this function:\n      config = api.get_net_config(128, \'cifar10\')\n    """"""\n    archresult = self.arch2infos_full[index]\n    all_results = archresult.query(dataset, None)\n    if len(all_results) == 0: raise ValueError(\'can not find one valid trial for the {:}-th architecture on {:}\'.format(index, dataset))\n    for seed, result in all_results.items():\n      return result.get_config(None)\n      #print (\'SEED [{:}] : {:}\'.format(seed, result))\n    raise ValueError(\'Impossible to reach here!\')\n\n  def get_cost_info(self, index: int, dataset: Text, use_12epochs_result: bool = False) -> Dict[Text, float]:\n    """"""To obtain the cost metric for the `index`-th architecture on a dataset.""""""\n    if use_12epochs_result: arch2infos = self.arch2infos_less\n    else: arch2infos = self.arch2infos_full\n    arch_result = arch2infos[index]\n    return arch_result.get_compute_costs(dataset)\n\n  def get_latency(self, index: int, dataset: Text, use_12epochs_result: bool = False) -> float:\n    """"""\n    To obtain the latency of the network (by default it will return the latency with the batch size of 256).\n    :param index: the index of the target architecture\n    :param dataset: the dataset name (cifar10-valid, cifar10, cifar100, ImageNet16-120)\n    :return: return a float value in seconds\n    """"""\n    cost_dict = self.get_cost_info(index, dataset, use_12epochs_result)\n    return cost_dict[\'latency\']\n\n  # obtain the metric for the `index`-th architecture\n  # `dataset` indicates the dataset:\n  #   \'cifar10-valid\'  : using the proposed train set of CIFAR-10 as the training set\n  #   \'cifar10\'        : using the proposed train+valid set of CIFAR-10 as the training set\n  #   \'cifar100\'       : using the proposed train set of CIFAR-100 as the training set\n  #   \'ImageNet16-120\' : using the proposed train set of ImageNet-16-120 as the training set\n  # `iepoch` indicates the index of training epochs from 0 to 11/199.\n  #   When iepoch=None, it will return the metric for the last training epoch\n  #   When iepoch=11, it will return the metric for the 11-th training epoch (starting from 0)\n  # `use_12epochs_result` indicates different hyper-parameters for training\n  #   When use_12epochs_result=True, it trains the network with 12 epochs and the LR decayed from 0.1 to 0 within 12 epochs\n  #   When use_12epochs_result=False, it trains the network with 200 epochs and the LR decayed from 0.1 to 0 within 200 epochs\n  # `is_random`\n  #   When is_random=True, the performance of a random architecture will be returned\n  #   When is_random=False, the performanceo of all trials will be averaged.\n  def get_more_info(self, index: int, dataset, iepoch=None, use_12epochs_result=False, is_random=True):\n    if use_12epochs_result: basestr, arch2infos = \'12epochs\' , self.arch2infos_less\n    else                  : basestr, arch2infos = \'200epochs\', self.arch2infos_full\n    archresult = arch2infos[index]\n    # if randomly select one trial, select the seed at first\n    if isinstance(is_random, bool) and is_random:\n      seeds = archresult.get_dataset_seeds(dataset)\n      is_random = random.choice(seeds)\n    # collect the training information\n    train_info = archresult.get_metrics(dataset, \'train\', iepoch=iepoch, is_random=is_random)\n    total = train_info[\'iepoch\'] + 1\n    xinfo = {\'train-loss\'    : train_info[\'loss\'],\n             \'train-accuracy\': train_info[\'accuracy\'],\n             \'train-per-time\': train_info[\'all_time\'] / total,\n             \'train-all-time\': train_info[\'all_time\']}\n    # collect the evaluation information\n    if dataset == \'cifar10-valid\':\n      valid_info = archresult.get_metrics(dataset, \'x-valid\', iepoch=iepoch, is_random=is_random)\n      try:\n        test_info = archresult.get_metrics(dataset, \'ori-test\', iepoch=iepoch, is_random=is_random)\n      except:\n        test_info = None\n      valtest_info = None\n    else:\n      try: # collect results on the proposed test set\n        if dataset == \'cifar10\':\n          test_info = archresult.get_metrics(dataset, \'ori-test\', iepoch=iepoch, is_random=is_random)\n        else:\n          test_info = archresult.get_metrics(dataset, \'x-test\', iepoch=iepoch, is_random=is_random)\n      except:\n        test_info = None\n      try: # collect results on the proposed validation set\n        valid_info = archresult.get_metrics(dataset, \'x-valid\', iepoch=iepoch, is_random=is_random)\n      except:\n        valid_info = None\n      try:\n        if dataset != \'cifar10\':\n          valtest_info = archresult.get_metrics(dataset, \'ori-test\', iepoch=iepoch, is_random=is_random)\n        else:\n          valtest_info = None\n      except:\n        valtest_info = None\n    if valid_info is not None:\n      xinfo[\'valid-loss\'] = valid_info[\'loss\']\n      xinfo[\'valid-accuracy\'] = valid_info[\'accuracy\']\n      xinfo[\'valid-per-time\'] = valid_info[\'all_time\'] / total\n      xinfo[\'valid-all-time\'] = valid_info[\'all_time\']\n    if test_info is not None:\n      xinfo[\'test-loss\'] = test_info[\'loss\']\n      xinfo[\'test-accuracy\'] = test_info[\'accuracy\']\n      xinfo[\'test-per-time\'] = test_info[\'all_time\'] / total\n      xinfo[\'test-all-time\'] = test_info[\'all_time\']\n    if valtest_info is not None:\n      xinfo[\'valtest-loss\'] = valtest_info[\'loss\']\n      xinfo[\'valtest-accuracy\'] = valtest_info[\'accuracy\']\n      xinfo[\'valtest-per-time\'] = valtest_info[\'all_time\'] / total\n      xinfo[\'valtest-all-time\'] = valtest_info[\'all_time\']\n    return xinfo\n  """""" # The following logic is deprecated after March 15 2020, where the benchmark file upgrades from NAS-Bench-201-v1_0-e61699.pth to NAS-Bench-201-v1_1-096897.pth.\n  def get_more_info(self, index: int, dataset, iepoch=None, use_12epochs_result=False, is_random=True):\n    if use_12epochs_result: basestr, arch2infos = \'12epochs\' , self.arch2infos_less\n    else                  : basestr, arch2infos = \'200epochs\', self.arch2infos_full\n    archresult = arch2infos[index]\n    # if randomly select one trial, select the seed at first\n    if isinstance(is_random, bool) and is_random:\n      seeds = archresult.get_dataset_seeds(dataset)\n      is_random = random.choice(seeds)\n    if dataset == \'cifar10-valid\':\n      train_info = archresult.get_metrics(dataset, \'train\'   , iepoch=iepoch, is_random=is_random)\n      valid_info = archresult.get_metrics(dataset, \'x-valid\' , iepoch=iepoch, is_random=is_random)\n      try:\n        test__info = archresult.get_metrics(dataset, \'ori-test\', iepoch=iepoch, is_random=is_random)\n      except:\n        test__info = None\n      total      = train_info[\'iepoch\'] + 1\n      xifo = {\'train-loss\'    : train_info[\'loss\'],\n              \'train-accuracy\': train_info[\'accuracy\'],\n              \'train-per-time\': None if train_info[\'all_time\'] is None else train_info[\'all_time\'] / total,\n              \'train-all-time\': train_info[\'all_time\'],\n              \'valid-loss\'    : valid_info[\'loss\'],\n              \'valid-accuracy\': valid_info[\'accuracy\'],\n              \'valid-all-time\': valid_info[\'all_time\'],\n              \'valid-per-time\': None if valid_info[\'all_time\'] is None else valid_info[\'all_time\'] / total}\n      if test__info is not None:\n        xifo[\'test-loss\']     = test__info[\'loss\']\n        xifo[\'test-accuracy\'] = test__info[\'accuracy\']\n      return xifo\n    else:\n      train_info = archresult.get_metrics(dataset, \'train\'   , iepoch=iepoch, is_random=is_random)\n      try:\n        if dataset == \'cifar10\':\n          test__info = archresult.get_metrics(dataset, \'ori-test\', iepoch=iepoch, is_random=is_random)\n        else:\n          test__info = archresult.get_metrics(dataset, \'x-test\', iepoch=iepoch, is_random=is_random)\n      except:\n        test__info = None\n      try:\n        valid_info = archresult.get_metrics(dataset, \'x-valid\', iepoch=iepoch, is_random=is_random)\n      except:\n        valid_info = None\n      try:\n        est_valid_info = archresult.get_metrics(dataset, \'ori-test\', iepoch=iepoch, is_random=is_random)\n      except:\n        est_valid_info = None\n      xifo = {\'train-loss\'    : train_info[\'loss\'],\n              \'train-accuracy\': train_info[\'accuracy\']}\n      if test__info is not None:\n        xifo[\'test-loss\'] = test__info[\'loss\'],\n        xifo[\'test-accuracy\'] = test__info[\'accuracy\']\n      if valid_info is not None:\n        xifo[\'valid-loss\'] = valid_info[\'loss\']\n        xifo[\'valid-accuracy\'] = valid_info[\'accuracy\']\n      if est_valid_info is not None:\n        xifo[\'est-valid-loss\'] = est_valid_info[\'loss\']\n        xifo[\'est-valid-accuracy\'] = est_valid_info[\'accuracy\']\n      return xifo\n  """"""\n\n  def show(self, index: int = -1) -> None:\n    """"""\n    This function will print the information of a specific (or all) architecture(s).\n\n    :param index: If the index < 0: it will loop for all architectures and print their information one by one.\n                  else: it will print the information of the \'index\'-th archiitecture.\n    :return: nothing\n    """"""\n    if index < 0: # show all architectures\n      print(self)\n      for i, idx in enumerate(self.evaluated_indexes):\n        print(\'\\n\' + \'-\' * 10 + \' The ({:5d}/{:5d}) {:06d}-th architecture! \'.format(i, len(self.evaluated_indexes), idx) + \'-\'*10)\n        print(\'arch : {:}\'.format(self.meta_archs[idx]))\n        strings = print_information(self.arch2infos_full[idx])\n        print(\'>\' * 40 + \' {:03d} epochs \'.format(self.arch2infos_full[idx].get_total_epoch()) + \'>\' * 40)\n        print(\'\\n\'.join(strings))\n        strings = print_information(self.arch2infos_less[idx])\n        print(\'>\' * 40 + \' {:03d} epochs \'.format(self.arch2infos_less[idx].get_total_epoch()) + \'>\' * 40)\n        print(\'\\n\'.join(strings))\n        print(\'<\' * 40 + \'------------\' + \'<\' * 40)\n    else:\n      if 0 <= index < len(self.meta_archs):\n        if index not in self.evaluated_indexes: print(\'The {:}-th architecture has not been evaluated or not saved.\'.format(index))\n        else:\n          strings = print_information(self.arch2infos_full[index])\n          print(\'>\' * 40 + \' {:03d} epochs \'.format(self.arch2infos_full[index].get_total_epoch()) + \'>\' * 40)\n          print(\'\\n\'.join(strings))\n          strings = print_information(self.arch2infos_less[index])\n          print(\'>\' * 40 + \' {:03d} epochs \'.format(self.arch2infos_less[index].get_total_epoch()) + \'>\' * 40)\n          print(\'\\n\'.join(strings))\n          print(\'<\' * 40 + \'------------\' + \'<\' * 40)\n      else:\n        print(\'This index ({:}) is out of range (0~{:}).\'.format(index, len(self.meta_archs)))\n\n  def statistics(self, dataset: Text, use_12epochs_result: bool) -> Dict[int, int]:\n    """"""\n    This function will count the number of total trials.\n    """"""\n    valid_datasets = [\'cifar10-valid\', \'cifar10\', \'cifar100\', \'ImageNet16-120\']\n    if dataset not in valid_datasets:\n      raise ValueError(\'{:} not in {:}\'.format(dataset, valid_datasets))\n    if use_12epochs_result: arch2infos = self.arch2infos_less\n    else                  : arch2infos = self.arch2infos_full\n    nums = defaultdict(lambda: 0)\n    for index in range(len(self)):\n      archInfo = arch2infos[index]\n      dataset_seed = archInfo.dataset_seed\n      if dataset not in dataset_seed:\n        nums[0] += 1\n      else:\n        nums[len(dataset_seed[dataset])] += 1\n    return dict(nums)\n\n  @staticmethod\n  def str2lists(arch_str: Text) -> List[tuple]:\n    """"""\n    This function shows how to read the string-based architecture encoding.\n      It is the same as the `str2structure` func in `AutoDL-Projects/lib/models/cell_searchs/genotypes.py`\n\n    :param\n      arch_str: the input is a string indicates the architecture topology, such as\n                    |nor_conv_1x1~0|+|none~0|none~1|+|none~0|none~1|skip_connect~2|\n    :return: a list of tuple, contains multiple (op, input_node_index) pairs.\n\n    :usage\n      arch = api.str2lists( \'|nor_conv_1x1~0|+|none~0|none~1|+|none~0|none~1|skip_connect~2|\' )\n      print (\'there are {:} nodes in this arch\'.format(len(arch)+1)) # arch is a list\n      for i, node in enumerate(arch):\n        print(\'the {:}-th node is the sum of these {:} nodes with op: {:}\'.format(i+1, len(node), node))\n    """"""\n    node_strs = arch_str.split(\'+\')\n    genotypes = []\n    for i, node_str in enumerate(node_strs):\n      inputs = list(filter(lambda x: x != \'\', node_str.split(\'|\')))\n      for xinput in inputs: assert len(xinput.split(\'~\')) == 2, \'invalid input length : {:}\'.format(xinput)\n      inputs = ( xi.split(\'~\') for xi in inputs )\n      input_infos = tuple( (op, int(IDX)) for (op, IDX) in inputs)\n      genotypes.append( input_infos )\n    return genotypes\n\n  @staticmethod\n  def str2matrix(arch_str: Text,\n                 search_space: List[Text] = [\'none\', \'skip_connect\', \'nor_conv_1x1\', \'nor_conv_3x3\', \'avg_pool_3x3\']) -> np.ndarray:\n    """"""\n    This func shows how to convert the string-based architecture encoding to the encoding strategy in NAS-Bench-101.\n\n    :param\n      arch_str: the input is a string indicates the architecture topology, such as\n                    |nor_conv_1x1~0|+|none~0|none~1|+|none~0|none~1|skip_connect~2|\n      search_space: a list of operation string, the default list is the search space for NAS-Bench-201\n        the default value should be be consistent with this line https://github.com/D-X-Y/AutoDL-Projects/blob/master/lib/models/cell_operations.py#L24\n    :return\n      the numpy matrix (2-D np.ndarray) representing the DAG of this architecture topology\n    :usage\n      matrix = api.str2matrix( \'|nor_conv_1x1~0|+|none~0|none~1|+|none~0|none~1|skip_connect~2|\' )\n      This matrix is 4-by-4 matrix representing a cell with 4 nodes (only the lower left triangle is useful).\n         [ [0, 0, 0, 0],  # the first line represents the input (0-th) node\n           [2, 0, 0, 0],  # the second line represents the 1-st node, is calculated by 2-th-op( 0-th-node )\n           [0, 0, 0, 0],  # the third line represents the 2-nd node, is calculated by 0-th-op( 0-th-node ) + 0-th-op( 1-th-node )\n           [0, 0, 1, 0] ] # the fourth line represents the 3-rd node, is calculated by 0-th-op( 0-th-node ) + 0-th-op( 1-th-node ) + 1-th-op( 2-th-node )\n      In NAS-Bench-201 search space, 0-th-op is \'none\', 1-th-op is \'skip_connect\',\n         2-th-op is \'nor_conv_1x1\', 3-th-op is \'nor_conv_3x3\', 4-th-op is \'avg_pool_3x3\'.\n    :(NOTE)\n      If a node has two input-edges from the same node, this function does not work. One edge will be overlapped.\n    """"""\n    node_strs = arch_str.split(\'+\')\n    num_nodes = len(node_strs) + 1\n    matrix = np.zeros((num_nodes, num_nodes))\n    for i, node_str in enumerate(node_strs):\n      inputs = list(filter(lambda x: x != \'\', node_str.split(\'|\')))\n      for xinput in inputs: assert len(xinput.split(\'~\')) == 2, \'invalid input length : {:}\'.format(xinput)\n      for xi in inputs:\n        op, idx = xi.split(\'~\')\n        if op not in search_space: raise ValueError(\'this op ({:}) is not in {:}\'.format(op, search_space))\n        op_idx, node_idx = search_space.index(op), int(idx)\n        matrix[i+1, node_idx] = op_idx\n    return matrix\n\n\nclass ArchResults(object):\n\n  def __init__(self, arch_index, arch_str):\n    self.arch_index   = int(arch_index)\n    self.arch_str     = copy.deepcopy(arch_str)\n    self.all_results  = dict()\n    self.dataset_seed = dict()\n    self.clear_net_done = False\n\n  def get_compute_costs(self, dataset):\n    x_seeds = self.dataset_seed[dataset]\n    results = [self.all_results[ (dataset, seed) ] for seed in x_seeds]\n\n    flops     = [result.flop for result in results]\n    params    = [result.params for result in results]\n    latencies = [result.get_latency() for result in results]\n    latencies = [x for x in latencies if x > 0]\n    mean_latency = np.mean(latencies) if len(latencies) > 0 else None\n    time_infos = defaultdict(list)\n    for result in results:\n      time_info = result.get_times()\n      for key, value in time_info.items(): time_infos[key].append( value )\n     \n    info = {\'flops\'  : np.mean(flops),\n            \'params\' : np.mean(params),\n            \'latency\': mean_latency}\n    for key, value in time_infos.items():\n      if len(value) > 0 and value[0] is not None:\n        info[key] = np.mean(value)\n      else: info[key] = None\n    return info\n\n  def get_metrics(self, dataset, setname, iepoch=None, is_random=False):\n    """"""\n      This `get_metrics` function is used to obtain obtain the loss, accuracy, etc information on a specific dataset.\n      If not specify, each set refer to the proposed split in NAS-Bench-201 paper.\n      If some args return None or raise error, then it is not avaliable.\n      ========================================\n      Args [dataset] (4 possible options):\n        -- cifar10-valid : training the model on the CIFAR-10 training set.\n        -- cifar10 : training the model on the CIFAR-10 training + validation set.\n        -- cifar100 : training the model on the CIFAR-100 training set.\n        -- ImageNet16-120 : training the model on the ImageNet16-120 training set.\n      Args [setname] (each dataset has different setnames):\n        -- When dataset = cifar10-valid, you can use \'train\', \'x-valid\', \'ori-test\'\n        ------ \'train\' : the metric on the training set.\n        ------ \'x-valid\' : the metric on the validation set.\n        ------ \'ori-test\' : the metric on the test set.\n        -- When dataset = cifar10, you can use \'train\', \'ori-test\'.\n        ------ \'train\' : the metric on the training + validation set.\n        ------ \'ori-test\' : the metric on the test set.\n        -- When dataset = cifar100 or ImageNet16-120, you can use \'train\', \'ori-test\', \'x-valid\', \'x-test\'\n        ------ \'train\' : the metric on the training set.\n        ------ \'x-valid\' : the metric on the validation set.\n        ------ \'x-test\' : the metric on the test set.\n        ------ \'ori-test\' : the metric on the validation + test set.\n      Args [iepoch] (None or an integer in [0, the-number-of-total-training-epochs)\n        ------ None : return the metric after the last training epoch.\n        ------ an integer i : return the metric after the i-th training epoch.\n      Args [is_random]:\n        ------ True : return the metric of a randomly selected trial.\n        ------ False : return the averaged metric of all avaliable trials.\n        ------ an integer indicating the \'seed\' value : return the metric of a specific trial (whose random seed is \'is_random\').\n    """"""\n    x_seeds = self.dataset_seed[dataset]\n    results = [self.all_results[ (dataset, seed) ] for seed in x_seeds]\n    infos   = defaultdict(list)\n    for result in results:\n      if setname == \'train\':\n        info = result.get_train(iepoch)\n      else:\n        info = result.get_eval(setname, iepoch)\n      for key, value in info.items(): infos[key].append( value )\n    return_info = dict()\n    if isinstance(is_random, bool) and is_random: # randomly select one\n      index = random.randint(0, len(results)-1)\n      for key, value in infos.items(): return_info[key] = value[index]\n    elif isinstance(is_random, bool) and not is_random: # average\n      for key, value in infos.items():\n        if len(value) > 0 and value[0] is not None:\n          return_info[key] = np.mean(value)\n        else: return_info[key] = None\n    elif isinstance(is_random, int): # specify the seed\n      if is_random not in x_seeds: raise ValueError(\'can not find random seed ({:}) from {:}\'.format(is_random, x_seeds))\n      index = x_seeds.index(is_random)\n      for key, value in infos.items(): return_info[key] = value[index]\n    else:\n      raise ValueError(\'invalid value for is_random: {:}\'.format(is_random))\n    return return_info\n\n  def show(self, is_print=False):\n    return print_information(self, None, is_print)\n\n  def get_dataset_names(self):\n    return list(self.dataset_seed.keys())\n\n  def get_dataset_seeds(self, dataset):\n    return copy.deepcopy( self.dataset_seed[dataset] )\n\n  def get_net_param(self, dataset: Text, seed: Union[None, int] =None):\n    """"""\n    This function will return the trained network\'s weights on the \'dataset\'.\n    :arg\n      dataset: one of \'cifar10-valid\', \'cifar10\', \'cifar100\', and \'ImageNet16-120\'.\n      seed: an integer indicates the seed value or None that indicates returing all trials.\n    """"""\n    if seed is None:\n      x_seeds = self.dataset_seed[dataset]\n      return {seed: self.all_results[(dataset, seed)].get_net_param() for seed in x_seeds}\n    else:\n      return self.all_results[(dataset, seed)].get_net_param()\n\n  def reset_latency(self, dataset: Text, seed: Union[None, Text], latency: float) -> None:\n    """"""This function is used to reset the latency in all corresponding ResultsCount(s).""""""\n    if seed is None:\n      for seed in self.dataset_seed[dataset]:\n        self.all_results[(dataset, seed)].update_latency([latency])\n    else:\n      self.all_results[(dataset, seed)].update_latency([latency])\n\n  def reset_pseudo_train_times(self, dataset: Text, seed: Union[None, Text], estimated_per_epoch_time: float) -> None:\n    """"""This function is used to reset the train-times in all corresponding ResultsCount(s).""""""\n    if seed is None:\n      for seed in self.dataset_seed[dataset]:\n        self.all_results[(dataset, seed)].reset_pseudo_train_times(estimated_per_epoch_time)\n    else:\n      self.all_results[(dataset, seed)].reset_pseudo_train_times(estimated_per_epoch_time)\n\n  def reset_pseudo_eval_times(self, dataset: Text, seed: Union[None, Text], eval_name: Text, estimated_per_epoch_time: float) -> None:\n    """"""This function is used to reset the eval-times in all corresponding ResultsCount(s).""""""\n    if seed is None:\n      for seed in self.dataset_seed[dataset]:\n        self.all_results[(dataset, seed)].reset_pseudo_eval_times(eval_name, estimated_per_epoch_time)\n    else:\n      self.all_results[(dataset, seed)].reset_pseudo_eval_times(eval_name, estimated_per_epoch_time)\n\n  def get_latency(self, dataset: Text) -> float:\n    """"""Get the latency of a model on the target dataset. [Timestamp: 2020.03.09]""""""\n    latencies = []\n    for seed in self.dataset_seed[dataset]:\n      latency = self.all_results[(dataset, seed)].get_latency()\n      if not isinstance(latency, float) or latency <= 0:\n        raise ValueError(\'invalid latency of {:} for {:} with {:}\'.format(dataset))\n      latencies.append(latency)\n    return sum(latencies) / len(latencies)\n\n  def get_total_epoch(self, dataset=None):\n    """"""Return the total number of training epochs.""""""\n    if dataset is None:\n      epochss = []\n      for xdata, x_seeds in self.dataset_seed.items():\n        epochss += [self.all_results[(xdata, seed)].get_total_epoch() for seed in x_seeds]\n    elif isinstance(dataset, str):\n      x_seeds = self.dataset_seed[dataset]\n      epochss = [self.all_results[(dataset, seed)].get_total_epoch() for seed in x_seeds]\n    else:\n      raise ValueError(\'invalid dataset={:}\'.format(dataset))\n    if len(set(epochss)) > 1: raise ValueError(\'Each trial mush have the same number of training epochs : {:}\'.format(epochss))\n    return epochss[-1]\n\n  def query(self, dataset, seed=None):\n    """"""Return the ResultsCount object (containing all information of a single trial) for \'dataset\' and \'seed\'""""""\n    if seed is None:\n      x_seeds = self.dataset_seed[dataset]\n      return {seed: self.all_results[(dataset, seed)] for seed in x_seeds}\n    else:\n      return self.all_results[(dataset, seed)]\n\n  def arch_idx_str(self):\n    return \'{:06d}\'.format(self.arch_index)\n\n  def update(self, dataset_name, seed, result):\n    if dataset_name not in self.dataset_seed:\n      self.dataset_seed[dataset_name] = []\n    assert seed not in self.dataset_seed[dataset_name], \'{:}-th arch alreadly has this seed ({:}) on {:}\'.format(self.arch_index, seed, dataset_name)\n    self.dataset_seed[ dataset_name ].append( seed )\n    self.dataset_seed[ dataset_name ] = sorted( self.dataset_seed[ dataset_name ] )\n    assert (dataset_name, seed) not in self.all_results\n    self.all_results[ (dataset_name, seed) ] = result\n    self.clear_net_done = False\n\n  def state_dict(self):\n    state_dict = dict()\n    for key, value in self.__dict__.items():\n      if key == \'all_results\': # contain the class of ResultsCount\n        xvalue = dict()\n        assert isinstance(value, dict), \'invalid type of value for {:} : {:}\'.format(key, type(value))\n        for _k, _v in value.items():\n          assert isinstance(_v, ResultsCount), \'invalid type of value for {:}/{:} : {:}\'.format(key, _k, type(_v))\n          xvalue[_k] = _v.state_dict()\n      else:\n        xvalue = value\n      state_dict[key] = xvalue\n    return state_dict\n\n  def load_state_dict(self, state_dict):\n    new_state_dict = dict()\n    for key, value in state_dict.items():\n      if key == \'all_results\': # to convert to the class of ResultsCount\n        xvalue = dict()\n        assert isinstance(value, dict), \'invalid type of value for {:} : {:}\'.format(key, type(value))\n        for _k, _v in value.items():\n          xvalue[_k] = ResultsCount.create_from_state_dict(_v)\n      else: xvalue = value\n      new_state_dict[key] = xvalue\n    self.__dict__.update(new_state_dict)\n\n  @staticmethod\n  def create_from_state_dict(state_dict_or_file):\n    x = ArchResults(-1, -1)\n    if isinstance(state_dict_or_file, str): # a file path\n      state_dict = torch.load(state_dict_or_file, map_location=\'cpu\')\n    elif isinstance(state_dict_or_file, dict):\n      state_dict = state_dict_or_file\n    else:\n      raise ValueError(\'invalid type of state_dict_or_file : {:}\'.format(type(state_dict_or_file)))\n    x.load_state_dict(state_dict)\n    return x\n\n  # This function is used to clear the weights saved in each \'result\'\n  # This can help reduce the memory footprint.\n  def clear_params(self):\n    for key, result in self.all_results.items():\n      del result.net_state_dict\n      result.net_state_dict = None\n    self.clear_net_done = True\n\n  def debug_test(self):\n    """"""This function is used for me to debug and test, which will call most methods.""""""\n    all_dataset = [\'cifar10-valid\', \'cifar10\', \'cifar100\', \'ImageNet16-120\']\n    for dataset in all_dataset:\n      print(\'---->>>> {:}\'.format(dataset))\n      print(\'The latency on {:} is {:} s\'.format(dataset, self.get_latency(dataset)))\n      for seed in self.dataset_seed[dataset]:\n        result = self.all_results[(dataset, seed)]\n        print(\'  ==>> result = {:}\'.format(result))\n        print(\'  ==>> cost = {:}\'.format(result.get_times()))\n\n  def __repr__(self):\n    return (\'{name}(arch-index={index}, arch={arch}, {num} runs, clear={clear})\'.format(name=self.__class__.__name__, index=self.arch_index, arch=self.arch_str, num=len(self.all_results), clear=self.clear_net_done))\n\n\n""""""\nThis class (ResultsCount) is used to save the information of one trial for a single architecture.\nI did not write much comment for this class, because it is the lowest-level class in NAS-Bench-201 API, which will be rarely called.\nIf you have any question regarding this class, please open an issue or email me.\n""""""\nclass ResultsCount(object):\n\n  def __init__(self, name, state_dict, train_accs, train_losses, params, flop, arch_config, seed, epochs, latency):\n    self.name           = name\n    self.net_state_dict = state_dict\n    self.train_acc1es = copy.deepcopy(train_accs)\n    self.train_acc5es = None\n    self.train_losses = copy.deepcopy(train_losses)\n    self.train_times  = None\n    self.arch_config  = copy.deepcopy(arch_config)\n    self.params     = params\n    self.flop       = flop\n    self.seed       = seed\n    self.epochs     = epochs\n    self.latency    = latency\n    # evaluation results\n    self.reset_eval()\n\n  def update_train_info(self, train_acc1es, train_acc5es, train_losses, train_times) -> None:\n    self.train_acc1es = train_acc1es\n    self.train_acc5es = train_acc5es\n    self.train_losses = train_losses\n    self.train_times  = train_times\n\n  def reset_pseudo_train_times(self, estimated_per_epoch_time: float) -> None:\n    """"""Assign the training times.""""""\n    train_times = OrderedDict()\n    for i in range(self.epochs):\n      train_times[i] = estimated_per_epoch_time\n    self.train_times = train_times\n\n  def reset_pseudo_eval_times(self, eval_name: Text, estimated_per_epoch_time: float) -> None:\n    """"""Assign the evaluation times.""""""\n    if eval_name not in self.eval_names: raise ValueError(\'invalid eval name : {:}\'.format(eval_name))\n    for i in range(self.epochs):\n      self.eval_times[\'{:}@{:}\'.format(eval_name,i)] = estimated_per_epoch_time\n\n  def reset_eval(self):\n    self.eval_names  = []\n    self.eval_acc1es = {}\n    self.eval_times  = {}\n    self.eval_losses = {}\n\n  def update_latency(self, latency):\n    self.latency = copy.deepcopy( latency )\n\n  def get_latency(self) -> float:\n    """"""Return the latency value in seconds. -1 represents not avaliable ; otherwise it should be a float value""""""\n    if self.latency is None: return -1.0\n    else: return sum(self.latency) / len(self.latency)\n\n  def update_eval(self, accs, losses, times):  # new version\n    data_names = set([x.split(\'@\')[0] for x in accs.keys()])\n    for data_name in data_names:\n      assert data_name not in self.eval_names, \'{:} has already been added into eval-names\'.format(data_name)\n      self.eval_names.append( data_name )\n      for iepoch in range(self.epochs):\n        xkey = \'{:}@{:}\'.format(data_name, iepoch)\n        self.eval_acc1es[ xkey ] = accs[ xkey ]\n        self.eval_losses[ xkey ] = losses[ xkey ]\n        self.eval_times [ xkey ] = times[ xkey ]\n\n  def update_OLD_eval(self, name, accs, losses): # old version\n    assert name not in self.eval_names, \'{:} has already added\'.format(name)\n    self.eval_names.append( name )\n    for iepoch in range(self.epochs):\n      if iepoch in accs:\n        self.eval_acc1es[\'{:}@{:}\'.format(name,iepoch)] = accs[iepoch]\n        self.eval_losses[\'{:}@{:}\'.format(name,iepoch)] = losses[iepoch]\n\n  def __repr__(self):\n    num_eval = len(self.eval_names)\n    set_name = \'[\' + \', \'.join(self.eval_names) + \']\'\n    return (\'{name}({xname}, arch={arch}, FLOP={flop:.2f}M, Param={param:.3f}MB, seed={seed}, {num_eval} eval-sets: {set_name})\'.format(name=self.__class__.__name__, xname=self.name, arch=self.arch_config[\'arch_str\'], flop=self.flop, param=self.params, seed=self.seed, num_eval=num_eval, set_name=set_name))\n\n  def get_total_epoch(self):\n    return copy.deepcopy(self.epochs)\n\n  def get_times(self):\n    """"""Obtain the information regarding both training and evaluation time.""""""\n    if self.train_times is not None and isinstance(self.train_times, dict):\n      train_times = list( self.train_times.values() )\n      time_info = {\'T-train@epoch\': np.mean(train_times), \'T-train@total\': np.sum(train_times)}\n    else:\n      time_info = {\'T-train@epoch\':                 None, \'T-train@total\':               None }\n    for name in self.eval_names:\n      try:\n        xtimes = [self.eval_times[\'{:}@{:}\'.format(name,i)] for i in range(self.epochs)]\n        time_info[\'T-{:}@epoch\'.format(name)] = np.mean(xtimes)\n        time_info[\'T-{:}@total\'.format(name)] = np.sum(xtimes)\n      except:\n        time_info[\'T-{:}@epoch\'.format(name)] = None\n        time_info[\'T-{:}@total\'.format(name)] = None\n    return time_info\n\n  def get_eval_set(self):\n    return self.eval_names\n\n  # get the training information\n  def get_train(self, iepoch=None):\n    if iepoch is None: iepoch = self.epochs-1\n    assert 0 <= iepoch < self.epochs, \'invalid iepoch={:} < {:}\'.format(iepoch, self.epochs)\n    if self.train_times is not None:\n      xtime = self.train_times[iepoch]\n      atime = sum([self.train_times[i] for i in range(iepoch+1)])\n    else: xtime, atime = None, None\n    return {\'iepoch\'  : iepoch,\n            \'loss\'    : self.train_losses[iepoch],\n            \'accuracy\': self.train_acc1es[iepoch],\n            \'cur_time\': xtime,\n            \'all_time\': atime}\n\n  def get_eval(self, name, iepoch=None):\n    """"""Get the evaluation information ; there could be multiple evaluation sets (identified by the \'name\' argument).""""""\n    if iepoch is None: iepoch = self.epochs-1\n    assert 0 <= iepoch < self.epochs, \'invalid iepoch={:} < {:}\'.format(iepoch, self.epochs)\n    if isinstance(self.eval_times,dict) and len(self.eval_times) > 0:\n      xtime = self.eval_times[\'{:}@{:}\'.format(name,iepoch)]\n      atime = sum([self.eval_times[\'{:}@{:}\'.format(name,i)] for i in range(iepoch+1)])\n    else: xtime, atime = None, None\n    return {\'iepoch\'  : iepoch,\n            \'loss\'    : self.eval_losses[\'{:}@{:}\'.format(name,iepoch)],\n            \'accuracy\': self.eval_acc1es[\'{:}@{:}\'.format(name,iepoch)],\n            \'cur_time\': xtime,\n            \'all_time\': atime}\n\n  def get_net_param(self, clone=False):\n    if clone: return copy.deepcopy(self.net_state_dict)\n    else: return self.net_state_dict\n\n  def get_config(self, str2structure):\n    """"""This function is used to obtain the config dict for this architecture.""""""\n    if str2structure is None:\n      return {\'name\': \'infer.tiny\', \'C\': self.arch_config[\'channel\'],\n              \'N\'   : self.arch_config[\'num_cells\'],\n              \'arch_str\': self.arch_config[\'arch_str\'], \'num_classes\': self.arch_config[\'class_num\']}\n    else:\n      return {\'name\': \'infer.tiny\', \'C\': self.arch_config[\'channel\'],\n              \'N\'   : self.arch_config[\'num_cells\'],\n              \'genotype\': str2structure(self.arch_config[\'arch_str\']), \'num_classes\': self.arch_config[\'class_num\']}\n\n  def state_dict(self):\n    _state_dict = {key: value for key, value in self.__dict__.items()}\n    return _state_dict\n\n  def load_state_dict(self, state_dict):\n    self.__dict__.update(state_dict)\n\n  @staticmethod\n  def create_from_state_dict(state_dict):\n    x = ResultsCount(None, None, None, None, None, None, None, None, None, None)\n    x.load_state_dict(state_dict)\n    return x\n'"
