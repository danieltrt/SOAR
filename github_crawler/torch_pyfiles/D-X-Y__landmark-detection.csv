file_path,api_count,code
.backup/ProREG.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n#\nimport torch\nimport torch.nn as nn\n\ndef weights_init_reg(m):\n  if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n    m.weight.data.normal_(0, math.sqrt(2. / n))\n    if m.bias is not None:\n      m.bias.data.zero_()\n  elif isinstance(m, nn.BatchNorm2d):\n    m.weight.data.fill_(1)\n    m.bias.data.zero_()\n  elif isinstance(m, nn.Linear):\n    n = m.weight.size(1)\n    m.weight.data.normal_(0, 0.01)\n    m.bias.data.zero_()\n\n\nclass AddCoords(nn.Module):\n\n  def __init__(self, with_r=False):\n    super().__init__()\n    self.with_r = with_r\n\n  def forward(self, input_tensor):\n    """"""\n    Args:\n      input_tensor: shape(batch, channel, x_dim, y_dim)\n    """"""\n    batch_size, _, x_dim, y_dim = input_tensor.size()\n\n    xx_channel = torch.arange(x_dim).repeat(1, y_dim, 1)\n    yy_channel = torch.arange(y_dim).repeat(1, x_dim, 1).transpose(1, 2)\n\n    xx_channel = xx_channel.float() / (x_dim - 1)\n    yy_channel = yy_channel.float() / (y_dim - 1)\n\n    xx_channel = xx_channel * 2 - 1\n    yy_channel = yy_channel * 2 - 1\n\n    xx_channel = xx_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3)\n    yy_channel = yy_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3)\n\n    xx_channel, yy_channel = xx_channel.type_as(input_tensor), yy_channel.type_as(input_tensor)\n    ret = torch.cat([\n      input_tensor,\n      xx_channel,\n      yy_channel], dim=1)\n\n    if self.with_r:\n      rr = torch.sqrt(torch.pow(xx_channel - 0.5, 2) + torch.pow(yy_channel - 0.5, 2))\n      ret = torch.cat([ret, rr], dim=1)\n\n    return ret\n\n\nclass CoordConv(nn.Module):\n\n  def __init__(self, in_channels, out_channels, with_r=False, **kwargs):\n    super().__init__()\n    self.addcoords = AddCoords(with_r=with_r)\n    in_size = in_channels+2\n    if with_r:\n      in_size += 1\n    self.conv = nn.Conv2d(in_size, out_channels, **kwargs)\n\n  def forward(self, x):\n    ret = self.addcoords(x)\n    ret = self.conv(ret)\n    return ret\n\ndef conv_bn(inp, oup, kernels, stride, pad):\n  return nn.Sequential(\n    nn.Conv2d(inp, oup, kernels, stride, pad, bias=False),\n    nn.BatchNorm2d(oup),\n    nn.ReLU6(inplace=True)\n  )\n\n\ndef conv_1x1_bn(inp, oup):\n  return nn.Sequential(\n    nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n    nn.BatchNorm2d(oup),\n    nn.ReLU6(inplace=True)\n  )\n\n\nclass InvertedResidual(nn.Module):\n  def __init__(self, inp, oup, stride, expand_ratio):\n    super(InvertedResidual, self).__init__()\n    self.stride = stride\n    assert stride in [1, 2]\n\n    hidden_dim = round(inp * expand_ratio)\n    self.use_res_connect = self.stride == 1 and inp == oup\n\n    if expand_ratio == 1:\n      self.conv = nn.Sequential(\n        # dw\n        nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n        nn.BatchNorm2d(hidden_dim),\n        nn.ReLU6(inplace=True),\n        # pw-linear\n        nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n      )\n    else:\n      self.conv = nn.Sequential(\n        # pw\n        nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(hidden_dim),\n        nn.ReLU6(inplace=True),\n        # dw\n        nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n        nn.BatchNorm2d(hidden_dim),\n        nn.ReLU6(inplace=True),\n        # pw-linear\n        nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n      )\n\n  def forward(self, x):\n    if self.use_res_connect:\n      return x + self.conv(x)\n    else:\n      return self.conv(x)\n\n\nclass MobileNetV2REG(nn.Module):\n  def __init__(self, input_dim, input_channel, width_mult, pts_num):\n    super(MobileNetV2REG, self).__init__()\n    self.pts_num = pts_num\n    block = InvertedResidual\n    interverted_residual_setting = [\n      # t, c, n, s\n      [1, 48 , 1, 1],\n      [2, 48 , 5, 2],\n      [2, 96 , 1, 2],\n      [4, 96 , 6, 1],\n      [2, 16 , 1, 1],\n    ]\n\n    input_channel = int(input_channel * width_mult)\n    features      = [conv_bn(input_dim, input_channel, (3,3), 2, 1)]\n    # building inverted residual blocks\n    for t, c, n, s in interverted_residual_setting:\n      output_channel = int(c * width_mult)\n      for i in range(n):\n        if i == 0: stride = s\n        else     : stride = 1\n        features.append( block(input_channel, output_channel, stride, expand_ratio=t) )\n        input_channel = output_channel\n    features.append( nn.AdaptiveAvgPool2d( (14,14) ) )\n    self.features = nn.Sequential(*features)\n  \n    self.S1 = nn.Sequential(\n                  CoordConv(input_channel  , input_channel*2, True, kernel_size=3, padding=1),\n                  conv_bn(input_channel*2, input_channel*2, (3,3), 2, 1))\n    self.S2 = nn.Sequential(\n                  CoordConv(input_channel*2, input_channel*4, True, kernel_size=3, padding=1),\n                  conv_bn(input_channel*4, input_channel*8, (7,7), 1, 0))\n\n    output_neurons  = 14*14*input_channel + 7*7*input_channel*2 + input_channel*8\n    self.locator    = nn.Sequential(\n                         nn.Linear(output_neurons, pts_num*2))\n\n    #self.classifier = nn.Linear(output_neurons, pts_num)\n    #self.classifier = nn.Sequential(\n    #                     block(input_channel*1, input_channel*4, 1, 2),\n    #                     nn.AdaptiveAvgPool2d( (16,12) ),\n    #                    block(input_channel*4, input_channel*4, 1, 2),\n    #                     nn.AdaptiveAvgPool2d( (8,6) ),\n    #                     nn.Conv2d(input_channel*4, pts_num, (8,6)))\n\n    self.apply( weights_init_reg )\n\n  def forward(self, x):\n    batch, C, H, W = x.size()\n    features = self.features(x)\n    S1 = self.S1( features )\n    S2 = self.S2( S1 )\n    tensors = torch.cat((features.view(batch, -1), S1.view(batch, -1), S2.view(batch, -1)), dim=1)\n    batch_locs = self.locator(tensors).view(batch, self.pts_num, 2)\n    #batch_scos = self.classifier(tensors).view(batch, self.pts_num, 1)\n    return batch_locs\n\n\nif __name__ == \'__main__\':\n  model = MobileNetV2REG(3, 24, 1, 18) # REG on AFLW\n'"
SAN/base_cluster.py,8,"b'##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom __future__ import division\n\nimport os, sys, time, random, argparse, PIL\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True # please use Pillow 4.0.0 or it may fail for some images\nfrom os import path as osp\nimport numbers, numpy as np\nimport init_path\nimport torch\nimport datasets\nfrom shutil import copyfile\nfrom san_vision import transforms\nfrom utils import AverageMeter, print_log\nfrom utils import convert_size2str, convert_secs2time, time_string, time_for_file\nfrom visualization import draw_image_by_points, save_error_image\nimport debug, models, options\nfrom sklearn.cluster import KMeans\nfrom cluster import filter_cluster\n\nmodel_names = sorted(name for name in models.__dict__\n  if name.islower() and not name.startswith(""__"")\n  and callable(models.__dict__[name]))\n\nopt = options.Options(model_names)\nargs = opt.opt\n# Prepare options\nif args.manualSeed is None: args.manualSeed = random.randint(1, 10000)\nrandom.seed(args.manualSeed)\ntorch.manual_seed(args.manualSeed)\ntorch.cuda.manual_seed_all(args.manualSeed)\ntorch.backends.cudnn.enabled   = True\n#torch.backends.cudnn.benchmark = True\n\ndef main():\n  # Init logger\n  if not os.path.isdir(args.save_path): os.makedirs(args.save_path)\n  log = open(os.path.join(args.save_path, \'cluster_seed_{}_{}.txt\'.format(args.manualSeed, time_for_file())), \'w\')\n  print_log(\'save path : {}\'.format(args.save_path), log)\n  print_log(\'------------ Options -------------\', log)\n  for k, v in sorted(vars(args).items()):\n    print_log(\'Parameter : {:20} = {:}\'.format(k, v), log)\n  print_log(\'-------------- End ----------------\', log)\n  print_log(""Random Seed: {}"".format(args.manualSeed), log)\n  print_log(""python version : {}"".format(sys.version.replace(\'\\n\', \' \')), log)\n  print_log(""Pillow version : {}"".format(PIL.__version__), log)\n  print_log(""torch  version : {}"".format(torch.__version__), log)\n  print_log(""cudnn  version : {}"".format(torch.backends.cudnn.version()), log)\n\n  # General Data Argumentation\n  mean_fill   = tuple( [int(x*255) for x in [0.485, 0.456, 0.406] ] )\n  normalize   = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                      std=[0.229, 0.224, 0.225])\n  transform  = transforms.Compose([transforms.PreCrop(args.pre_crop_expand), transforms.TrainScale2WH((args.crop_width, args.crop_height)),  transforms.ToTensor(), normalize])\n\n  args.downsample = 8 # By default\n  args.sigma = args.sigma * args.scale_eval\n\n  data = datasets.GeneralDataset(transform, args.sigma, args.downsample, args.heatmap_type, args.dataset_name)\n  data.load_list(args.train_list, args.num_pts, True)\n  loader = torch.utils.data.DataLoader(data, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)\n\n  # Load all lists\n  all_lines = {}\n  for file_path in args.train_list:\n    listfile = open(file_path, \'r\')\n    listdata = listfile.read().splitlines()\n    listfile.close()\n    for line in listdata:\n      temp = line.split(\' \')\n      assert len(temp) == 6  or len(temp) == 7, \'This line has the wrong format : {}\'.format(line)\n      image_path = temp[0]\n      all_lines[ image_path ] = line\n\n  assert args.n_clusters >= 2, \'The cluster number must be greater than 2\'\n  resnet = models.resnet152(True).cuda()\n  all_features = []\n  for i, (inputs, target, mask, points, image_index, label_sign, ori_size) in enumerate(loader):\n    input_vars = torch.autograd.Variable(inputs.cuda(), volatile=True)\n    features, classifications = resnet(input_vars)\n    features = features.cpu().data.numpy()\n    all_features.append( features )\n    if i % args.print_freq == 0:\n      print_log(\'{} {}/{} extract features\'.format(time_string(), i, len(loader)), log)\n  all_features = np.concatenate(all_features, axis=0)\n  kmeans_result = KMeans(n_clusters=args.n_clusters, n_jobs=args.workers).fit( all_features )\n  print_log(\'kmeans [{}] calculate done\'.format(args.n_clusters), log)\n  labels = kmeans_result.labels_.copy()\n\n  cluster_idx = []\n  for iL in range(args.n_clusters):\n    indexes = np.where( labels == iL )[0]\n    cluster_idx.append( len(indexes) )\n  cluster_idx = np.argsort(cluster_idx)\n    \n  for iL in range(args.n_clusters):\n    ilabel = cluster_idx[iL]\n    indexes = np.where( labels == ilabel )\n    if isinstance(indexes, tuple) or isinstance(indexes, list): indexes = indexes[0]\n    cluster_features = all_features[indexes,:].copy()\n    filtered_index = filter_cluster(indexes.copy(), cluster_features, 0.8)\n\n    print_log(\'{:} [{:2d} / {:2d}] has {:4d} / {:4d} -> {:4d} = {:.2f} images \'.format(time_string(), iL, args.n_clusters, indexes.size, len(data), len(filtered_index), indexes.size*1./len(data)), log)\n    indexes = filtered_index.copy()\n    save_dir = osp.join(args.save_path, \'cluster-{:02d}-{:02d}\'.format(iL, args.n_clusters))\n    save_path = save_dir + \'.lst\'\n    #if not osp.isdir(save_path): os.makedirs( save_path )\n    print_log(\'save into {}\'.format(save_path), log)\n    txtfile = open( save_path , \'w\')\n    for idx in indexes:\n      image_path = data.datas[idx]\n      assert image_path in all_lines, \'Not find {}\'.format(image_path)\n      txtfile.write(\'{}\\n\'.format(all_lines[image_path]))\n      #basename = osp.basename( image_path )\n      #os.system( \'cp {} {}\'.format(image_path, save_dir) )\n    txtfile.close()\n\nif __name__ == \'__main__\':\n  main()\n'"
SAN/cluster.py,14,"b'##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom __future__ import division\n\nimport os, sys, time, random, argparse, PIL\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True # please use Pillow 4.0.0 or it may fail for some images\nfrom os import path as osp\nimport numbers, numpy as np\nimport init_path\nimport torch\nimport torchvision.datasets as visiondatasets\nimport torchvision.transforms as visiontransforms\nimport datasets\nfrom shutil import copyfile\nfrom san_vision import transforms\nfrom utils import AverageMeter, print_log\nfrom utils import convert_size2str, convert_secs2time, time_string, time_for_file\nfrom visualization import draw_image_by_points, save_error_image\nimport debug, models, options\nfrom sklearn.cluster import KMeans\nfrom cluster import filter_cluster\n\nmodel_names = sorted(name for name in models.__dict__\n  if name.islower() and not name.startswith(""__"")\n  and callable(models.__dict__[name]))\n\nopt = options.Options(model_names)\nargs = opt.opt\n# Prepare options\nif args.manualSeed is None: args.manualSeed = random.randint(1, 10000)\nrandom.seed(args.manualSeed)\ntorch.manual_seed(args.manualSeed)\ntorch.cuda.manual_seed_all(args.manualSeed)\ntorch.backends.cudnn.enabled   = True\n#torch.backends.cudnn.benchmark = True\n\ndef main():\n  # Init logger\n  if not os.path.isdir(args.save_path): os.makedirs(args.save_path)\n  log = open(os.path.join(args.save_path, \'cluster_seed_{}_{}.txt\'.format(args.manualSeed, time_for_file())), \'w\')\n  print_log(\'save path : {}\'.format(args.save_path), log)\n  print_log(\'------------ Options -------------\', log)\n  for k, v in sorted(vars(args).items()):\n    print_log(\'Parameter : {:20} = {:}\'.format(k, v), log)\n  print_log(\'-------------- End ----------------\', log)\n  print_log(""Random Seed: {}"".format(args.manualSeed), log)\n  print_log(""python version : {}"".format(sys.version.replace(\'\\n\', \' \')), log)\n  print_log(""Pillow version : {}"".format(PIL.__version__), log)\n  print_log(""torch  version : {}"".format(torch.__version__), log)\n  print_log(""cudnn  version : {}"".format(torch.backends.cudnn.version()), log)\n\n\n  # finetune resnet-152 to train style-discriminative features\n  resnet = models.resnet152(True, num_classes=4)\n  resnet = torch.nn.DataParallel(resnet).cuda()\n  # define loss function (criterion) and optimizer\n  criterion = torch.nn.CrossEntropyLoss().cuda()\n  optimizer = torch.optim.SGD(resnet.parameters(), args.learning_rate,\n                                momentum=args.momentum,\n                                weight_decay=args.decay)\n  cls_train_dir = args.style_train_root\n  cls_eval_dir = args.style_eval_root\n  assert osp.isdir(cls_train_dir), \'Does not know : {}\'.format(cls_train_dir)\n  # train data loader\n  vision_normalize = visiontransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n  print_log(\'Training dir : {:}\'.format(cls_train_dir), log)\n  print_log(\'Evaluate dir : {:}\'.format(cls_eval_dir), log)\n  cls_train_dataset = visiondatasets.ImageFolder(\n        cls_train_dir,\n        visiontransforms.Compose([\n            visiontransforms.RandomResizedCrop(224),\n            visiontransforms.RandomHorizontalFlip(),\n            visiontransforms.ToTensor(),\n            vision_normalize,\n        ]))\n  print_log(\'Training Dataset : {:}\'.format(cls_train_dataset), log)\n  assert len(cls_train_dataset.classes) == 4, \'There should have 4 kinds of classes instead of : {:}\'.format(cls_train_dataset.classes)\n\n  cls_train_loader = torch.utils.data.DataLoader(\n        cls_train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n\n  if cls_eval_dir is not None:\n    assert osp.isdir(cls_eval_dir), \'Does not know : {}\'.format(cls_eval_dir)\n    val_loader = torch.utils.data.DataLoader(\n        visiondatasets.ImageFolder(cls_eval_dir, visiontransforms.Compose([\n            visiontransforms.Resize(256),\n            visiontransforms.CenterCrop(224),\n            visiontransforms.ToTensor(),\n            vision_normalize,\n        ])),\n        batch_size=args.batch_size, shuffle=False,\n        num_workers=args.workers, pin_memory=True)\n  else: val_loader = None\n\n  for epoch in range(args.epochs):\n    learning_rate = adjust_learning_rate(optimizer, epoch, args)\n    print_log(\'epoch : [{}/{}] lr={}\'.format(epoch, args.epochs, learning_rate), log)\n    top1, losses = AverageMeter(), AverageMeter()\n    resnet.train()\n    for i, (inputs, target) in enumerate(cls_train_loader):\n      target = target.cuda(async=True)\n      # compute output\n      _, output = resnet(inputs)\n      loss = criterion(output, target)\n\n      # measure accuracy and record loss\n      prec1 = accuracy(output.data, target, topk=[1])\n      top1.update(prec1.item(), inputs.size(0))\n      losses.update(loss.item(), inputs.size(0))\n      # compute gradient and do SGD step\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n      if i % args.print_freq == 0 or i+1 == len(cls_train_loader):\n        print_log(\' [Train={:03d}] [{:}] [{:3d}/{:3d}] accuracy : {:.1f}, loss : {:.4f}, input:{:}, output:{:}\'.format(epoch, time_string(), i, len(cls_train_loader), top1.avg, losses.avg, inputs.size(), output.size()), log)\n\n    if val_loader is None: continue\n\n    # evaluate the model\n    resnet.eval()\n    top1, losses = AverageMeter(), AverageMeter()\n    for i, (inputs, target) in enumerate(val_loader):\n      target = target.cuda(async=True)\n      # compute output\n      with torch.no_grad():\n        _, output = resnet(inputs)\n        loss = criterion(output, target)\n        # measure accuracy and record loss\n        prec1 = accuracy(output.data, target, topk=[1])\n      top1.update(prec1.item(), inputs.size(0))\n      losses.update(loss.item(), inputs.size(0))\n      if i % args.print_freq_eval == 0 or i+1 == len(val_loader):\n        print_log(\' [Evalu={:03d}] [{:}] [{:3d}/{:3d}] accuracy : {:.1f}, loss : {:.4f}, input:{:}, output:{:}\'.format(epoch, time_string(), i, len(val_loader), top1.avg, losses.avg, inputs.size(), output.size()), log)\n    \n\n  # extract features\n  resnet.eval()\n  # General Data Argumentation\n  mean_fill   = tuple( [int(x*255) for x in [0.485, 0.456, 0.406] ] )\n  normalize   = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                      std=[0.229, 0.224, 0.225])\n  transform  = transforms.Compose([transforms.PreCrop(args.pre_crop_expand), transforms.TrainScale2WH((args.crop_width, args.crop_height)),  transforms.ToTensor(), normalize])\n\n  args.downsample = 8 # By default\n  args.sigma = args.sigma * args.scale_eval\n  data = datasets.GeneralDataset(transform, args.sigma, args.downsample, args.heatmap_type, args.dataset_name)\n  data.load_list(args.train_list, args.num_pts, True)\n  loader = torch.utils.data.DataLoader(data, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)\n\n  # Load all lists\n  all_lines = {}\n  for file_path in args.train_list:\n    listfile = open(file_path, \'r\')\n    listdata = listfile.read().splitlines()\n    listfile.close()\n    for line in listdata:\n      temp = line.split(\' \')\n      assert len(temp) == 6  or len(temp) == 7, \'This line has the wrong format : {}\'.format(line)\n      image_path = temp[0]\n      all_lines[ image_path ] = line\n\n  assert args.n_clusters >= 2, \'The cluster number must be greater than 2\'\n  all_features = []\n  for i, (inputs, target, mask, points, image_index, label_sign, ori_size) in enumerate(loader):\n    with torch.no_grad():\n      features, _ = resnet(inputs)\n      features = features.cpu().numpy()\n    all_features.append( features )\n    if i % args.print_freq == 0:\n      print_log(\'{} {}/{} extract features\'.format(time_string(), i, len(loader)), log)\n  all_features = np.concatenate(all_features, axis=0)\n  kmeans_result = KMeans(n_clusters=args.n_clusters, n_jobs=args.workers).fit( all_features )\n  print_log(\'kmeans [{}] calculate done\'.format(args.n_clusters), log)\n  labels = kmeans_result.labels_.copy()\n\n  cluster_idx = []\n  for iL in range(args.n_clusters):\n    indexes = np.where( labels == iL )[0]\n    cluster_idx.append( len(indexes) )\n  cluster_idx = np.argsort(cluster_idx)\n    \n  for iL in range(args.n_clusters):\n    ilabel = cluster_idx[iL]\n    indexes = np.where( labels == ilabel )\n    if isinstance(indexes, tuple) or isinstance(indexes, list): indexes = indexes[0]\n    #cluster_features = all_features[indexes,:].copy()\n    #filtered_index = filter_cluster(indexes.copy(), cluster_features, 0.8)\n    filtered_index = indexes.copy()\n\n    print_log(\'{:} [{:2d} / {:2d}] has {:4d} / {:4d} -> {:4d} = {:.2f} images\'.format(time_string(), iL, args.n_clusters, indexes.size, len(data), len(filtered_index), indexes.size*1./len(data)), log)\n    indexes = filtered_index.copy()\n    save_dir = osp.join(args.save_path, \'cluster-{:02d}-{:02d}\'.format(iL, args.n_clusters))\n    save_path = save_dir + \'.lst\'\n    #if not osp.isdir(save_path): os.makedirs( save_path )\n    print_log(\'save into {}\'.format(save_path), log)\n    txtfile = open( save_path , \'w\')\n    for idx in indexes:\n      image_path = data.datas[idx]\n      assert image_path in all_lines, \'Not find {}\'.format(image_path)\n      txtfile.write(\'{}\\n\'.format(all_lines[image_path]))\n      #basename = osp.basename( image_path )\n      #os.system( \'cp {} {}\'.format(image_path, save_dir) )\n    txtfile.close()\n\ndef adjust_learning_rate(optimizer, epoch, args):\n    lr = args.learning_rate * (0.1 ** (epoch // int(args.epochs/2)))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    return lr\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    if len(res) == 1: return res[0]\n    else: return res\n\nif __name__ == \'__main__\':\n  main()\n'"
SAN/crop_pic.py,1,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nimport os, random, time\nimport pdb\nimport torch\nfrom PIL import Image\nimport init_path\nimport numpy as np\nfrom os import path as osp\nimport datasets\nfrom san_vision import transforms\nfrom utils      import time_string\n\nPRINT_GAP = 1000\n\ndef crop_style(list_file, num_pts, save_dir):\n  #style = 'Original'\n  #save_dir = 'cache/{}'.format(style)\n  print ('Cropping face images into {:}'.format(save_dir))\n  if not osp.isdir(save_dir): os.makedirs(save_dir)\n  transform  = transforms.Compose([transforms.PreCrop(0.2), transforms.TrainScale2WH((256, 256))])\n  data = datasets.GeneralDataset(transform, 1, 8, 'gaussian', 'test')\n  data.load_list(list_file, num_pts, True)\n  #loader = torch.utils.data.DataLoader(data, batch_size=1, shuffle=False, num_workers=12, pin_memory=True)\n  for i, tempx in enumerate(data):\n    image = tempx[0]\n    #points = tempx[3]\n    basename = osp.basename(data.datas[i])\n    save_name = osp.join(save_dir, basename)\n    image.save(save_name)\n    if i % PRINT_GAP == 0:\n      print ('{:} --->>> process the {:4d}/{:4d}-th image into {:}'.format(time_string(), i, len(data), save_dir))\n\n\nif __name__ == '__main__':\n\n  this_dir = osp.dirname(osp.abspath(__file__))\n  print ('The root dir is : {}'.format(this_dir))\n\n  styles = ['Original', 'Gray', 'Light', 'Sketch']\n\n  for style in styles:\n    list_file = ['./cache_data/lists/300W/{:}/300w.train.GTB'.format(style),\n                 './cache_data/lists/300W/{:}/300w.test.full.GTB'.format(style)]\n    crop_style(list_file, 68, osp.join(this_dir, 'cache_data', 'cache', '300W', style))\n\n  for style in styles:\n    list_file = ['./cache_data/lists/AFLW/{:}/all.GTB'.format(style)]\n    crop_style(list_file, 19, osp.join(this_dir, 'cache_data', 'cache', 'AFLW', style))\n\n  #crop_style(['./snapshots/CLUSTER-300W_GTB-3/cluster-00-03.lst'], 68, osp.join('..', 'cache_data', 'cache', 'clusters', '300W-0'))\n  #crop_style(['./snapshots/CLUSTER-300W_GTB-3/cluster-01-03.lst'], 68, osp.join('..', 'cache_data', 'cache', 'clusters', '300W-1'))\n  #crop_style(['./snapshots/CLUSTER-300W_GTB-3/cluster-02-03.lst'], 68, osp.join('..', 'cache_data', 'cache', 'clusters', '300W-2'))\n"""
SAN/gen_mean_face.py,5,"b'import os, random, time\nimport pdb\nimport torch\nfrom PIL import Image\nimport init_path\nimport numpy as np\nfrom os import path as osp\nimport datasets\nfrom san_vision import transforms\nimport torch.nn.functional as F\n\ndef normalize(L, x):\n  return -1. + 2. * x / (L-1)\n\ndef np2variable(x, requires_grad=False, dtype=torch.FloatTensor):\n  v = torch.autograd.Variable(torch.from_numpy(x).type(dtype), requires_grad=requires_grad)\n  return v\n\ndef face_align(face, point, target):\n  spatial_size = np.array(face.size)\n  point, target = point.copy(), target.copy()\n  point[:,0] = normalize(spatial_size[0], point[:,0])\n  point[:,1] = normalize(spatial_size[1], point[:,1])\n  target[:,0] = normalize(spatial_size[0], target[:,0])\n  target[:,1] = normalize(spatial_size[1], target[:,1])\n  x, residual, rank, s = np.linalg.lstsq(target, point)\n  theta = x.T[:2,:]\n  theta = np2variable(theta).unsqueeze(0)\n  image = np.array(face.copy()).transpose(2, 0, 1)\n  image_var = np2variable(image, False).unsqueeze(0)\n  grid_size = torch.Size([1, 3, int(spatial_size[1]), int(spatial_size[0])])\n  grid = F.affine_grid(theta, grid_size)\n  aligned_image = F.grid_sample(image_var, grid)\n  aligned_image = aligned_image.data.numpy().squeeze()\n  aligned_image = aligned_image.transpose(1, 2, 0)\n  aligned_image = Image.fromarray(np.uint8(aligned_image))\n  return aligned_image\n\ndef calculate_mean(list_file, num_pts, save_path):\n  #style = \'Original\'\n  #save_dir = \'cache/{}\'.format(style)\n  save_dir = osp.dirname(save_path)\n  print (\'crop face images into {} <-> {}\'.format(save_dir, save_path))\n  if not osp.isdir(save_dir): os.makedirs(save_dir)\n  transform  = transforms.Compose([transforms.PreCrop(0.2), transforms.TrainScale2WH((256, 256))])\n  data = datasets.GeneralDataset(transform, 1, 8, \'gaussian\', \'test\')\n  data.load_list(list_file, num_pts, True)\n  ok_faces, ok_basenames, ok_points = [], [], []\n  for i, tempx in enumerate(data):\n    image, mask, points = tempx[0], tempx[2], tempx[3]\n    #points = points[[0, 8, 16, 36, 39, 42, 45, 33, 48, 54, 27, 57],:]\n    basename = osp.basename(data.datas[i])\n    if torch.sum(mask) == num_pts + 1:\n      ok_faces.append( image )\n      ok_basenames.append( basename )\n      ok_points.append( points.numpy() )\n  print (\'extract done {:} -> {:}\'.format(len(data), len(ok_faces)))\n  mean_landmark = np.array(ok_points).mean(axis=0)\n  all_faces = []\n  save_dir = save_dir + \'-all\'\n  if not osp.isdir(save_dir): os.makedirs(save_dir)\n\n  for face, point, basename in zip(ok_faces, ok_points, ok_basenames):\n    aligned_face = face_align(face, point, mean_landmark)\n    aligned_face.save(osp.join(save_dir, basename))\n    all_faces.append( np.array(aligned_face) )\n  all_faces = np.array(all_faces).mean(axis=0)\n  mean_face = Image.fromarray(np.uint8(all_faces))\n  mean_face.save(save_path)\n\ndef generate_300W(cluster_num):\n  for i in range(cluster_num):\n    save_path = osp.join(\'cache_data\', \'cache\', \'clusters\', \'300W-{:}\'.format(cluster_num), \'300W-{:}-{:}.png\'.format(i, cluster_num))\n    calculate_mean([\'./snapshots/CLUSTER-300W_GTB-{:d}/cluster-{:02d}-{:02d}.lst\'.format(cluster_num, i, cluster_num)], 68, save_path)\n\n    save_path = osp.join(\'cache_data\', \'cache\', \'clusters\', \'300W-BASE-{:}\'.format(cluster_num), \'300W-BASE-{:}-{:}.png\'.format(i, cluster_num))\n    calculate_mean([\'./snapshots/BASE-CLUSTER-300W_GTB-{:d}/cluster-{:02d}-{:02d}.lst\'.format(cluster_num, i, cluster_num)], 68, save_path)\n\ndef generate_AFLW(cluster_num):\n  for i in range(cluster_num):\n    save_path = osp.join(\'cache_data\', \'cache\', \'clusters\', \'AFLW-{:}\'.format(cluster_num), \'AFLW-{:}-{:}.png\'.format(i, cluster_num))\n    calculate_mean([\'./snapshots/CLUSTER-AFLW_GTB-{:d}/cluster-{:02d}-{:02d}.lst\'.format(cluster_num, i, cluster_num)], 19, save_path)\n    save_path = osp.join(\'cache_data\', \'cache\', \'clusters\', \'AFLW-BASE-{:}\'.format(cluster_num), \'AFLW-BASE-{:}-{:}.png\'.format(i, cluster_num))\n    calculate_mean([\'./snapshots/BASE-CLUSTER-AFLW_GTB-{:d}/cluster-{:02d}-{:02d}.lst\'.format(cluster_num, i, cluster_num)], 19, save_path)\n\nif __name__ == \'__main__\':\n  generate_300W(3)\n  """"""\n  for cluster_num in [3,4,5,6]:\n    generate_AFLW(cluster_num)\n    generate_300W(cluster_num)\n  """"""\n'"
SAN/init_path.py,0,"b'""""""Set up paths.""""""            \n  \nimport sys, os  \nfrom os import path as osp\n\ndef add_path(path):            \n  if path not in sys.path:   \n    sys.path.insert(0, path)\nthis_dir = osp.dirname(osp.abspath(__file__))\n\n# Add lib to PYTHONPATH  \nlib_path = osp.abspath(osp.join(this_dir, \'lib\'))\nadd_path(lib_path) \n'"
SAN/san_api.py,8,"b""import sys\nfrom os import path as osp\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nfrom PIL import ImageFile, Image\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n\n# this aims to avoid `sys.path` changing outside this module\nthis_dir = osp.dirname(osp.abspath(__file__))\nlib_path = osp.join(this_dir, 'lib')\nsys.path.insert(0, lib_path)\n\nimport models\nfrom datasets.dataset_utils import pil_loader\nfrom san_vision import transforms\nfrom datasets.point_meta import Point_Meta\n\nsys.path.pop(0)\n\n\nclass SanLandmarkDetector(object):\n    '''Wraps SAN Face Landmark Detector module to simple API for end user\n\n    Example:\n        ```python\n        image_path = './cache_data/cache/test_1.jpg'\n        model_path = './snapshots/checkpoint_49.pth.tar'\n        face = (819.27, 432.15, 971.70, 575.87)\n\n        from san_api import SanLandmarkDetector\n        det = SanLandmarkDetector(model_path, device)\n        locs, scores = det.detect(image_path, face)\n        ```\n    '''\n    def __init__(self, model_path, device=None, benchmark: bool=True):\n        '''\n        Args:\n            module_path: path to pre-trained model (available to download, see README)\n            device: CUDA device to use. str or torch.device instance\n                warning: this is restricted to 'cpu' or 'cuda' only\n                    ('cuda:1' won't work due to main package arcitecture)\n                default is choose 'cuda' if available\n            benchmark: to enable cudnn benchmark mode or not\n        '''\n        self.model_path = model_path\n        if device is None:\n            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.device = device\n\n        if benchmark:\n            torch.backends.cudnn.enabled = True\n            torch.backends.cudnn.benchmark = True\n\n        snapshot = torch.load(self.model_path, map_location=self.device)\n        self.param = snapshot['args']\n\n        self.transform  = transforms.Compose([\n            transforms.PreCrop(self.param.pre_crop_expand),\n            transforms.TrainScale2WH((self.param.crop_width, self.param.crop_height)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n        ])\n\n        self.net = models.__dict__[self.param.arch](self.param.modelconfig, None)\n        self.net.train(False).to(self.device)\n\n        weights = models.remove_module_dict(snapshot['state_dict'])\n        self.net.load_state_dict(weights)\n\n    def detect(self, image, face):\n        '''\n        Args:\n            image: either path to image or actual image: PIL, numpy of Tensor (HxWxC dims)\n            face: \n        Returns:\n            (\n                locations: 68x2 array of points detected,\n                scores: 68 confidence levels for each point,\n            )\n        '''\n        if isinstance(image, str) or isinstance(image, Path):\n            image = pil_loader(image)\n        elif isinstance(image, np.ndarray):\n            image = Image.fromarray(image)\n        elif isinstance(image, torch.Tensor):\n            image = Image.fromarray(image.numpy())\n        else:\n            raise ValueError(f'Unsupported input image type {type(image)}')\n\n        meta = Point_Meta(self.param.num_pts, None, np.array(face), '', 'custom')\n        image, meta = self.transform(image, meta)\n        temp_save_wh = meta.temp_save_wh\n        cropped_size = torch.IntTensor( [temp_save_wh[1], temp_save_wh[0], temp_save_wh[2], temp_save_wh[3]] )\n\n        # network forward\n        with torch.no_grad():\n            inputs = image.unsqueeze(0).to(self.device)\n            _, batch_locs, batch_scos, _ = self.net(inputs)\n\n        # obtain the locations on the image in the orignial size\n        np_batch_locs, np_batch_scos, cropped_size = batch_locs.cpu().numpy(), batch_scos.cpu().numpy(), cropped_size.numpy()\n        locations, scores = np_batch_locs[0,:-1,:], np.expand_dims(np_batch_scos[0,:-1], -1)\n\n        scale_h, scale_w = cropped_size[0] * 1. / inputs.size(-2) , cropped_size[1] * 1. / inputs.size(-1)\n\n        locations[:, 0], locations[:, 1] = locations[:, 0] * scale_w + cropped_size[2], locations[:, 1] * scale_h + cropped_size[3]\n        return locations.round().astype(np.int), scores\n"""
SAN/san_eval.py,7,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom __future__ import division\n\nimport os, sys, time, random, argparse, PIL\nfrom pathlib import Path\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True # please use Pillow 4.0.0 or it may fail for some images\nfrom os import path as osp\nimport numbers, numpy as np\nimport init_path\nimport torch\nimport models\nimport datasets\nfrom visualization import draw_image_by_points\nfrom san_vision import transforms\nfrom utils import time_string, time_for_file, get_model_infos\n\ndef evaluate(args):\n  if not args.cpu:\n    assert torch.cuda.is_available(), 'CUDA is not available.'\n    torch.backends.cudnn.enabled   = True\n    torch.backends.cudnn.benchmark = True\n\n  print ('The image is {:}'.format(args.image))\n  print ('The model is {:}'.format(args.model))\n  snapshot = Path(args.model)\n  assert snapshot.exists(), 'The model path {:} does not exist'\n  print ('The face bounding box is {:}'.format(args.face))\n  assert len(args.face) == 4, 'Invalid face input : {:}'.format(args.face)\n  if args.cpu: snapshot = torch.load(snapshot, map_location='cpu')\n  else       : snapshot = torch.load(snapshot)\n\n  mean_fill   = tuple( [int(x*255) for x in [0.5, 0.5, 0.5] ] )\n  normalize   = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                      std=[0.5, 0.5, 0.5])\n  param = snapshot['args']\n  eval_transform  = transforms.Compose([transforms.PreCrop(param.pre_crop_expand), transforms.TrainScale2WH((param.crop_width, param.crop_height)),  transforms.ToTensor(), normalize])\n\n  net = models.__dict__[param.arch](param.modelconfig, None)\n\n  if not args.cpu: net = net.cuda()\n  weights = models.remove_module_dict(snapshot['state_dict'])\n  net.load_state_dict(weights)\n\n  dataset = datasets.GeneralDataset(eval_transform, param.sigma, param.downsample, param.heatmap_type, param.dataset_name)\n  dataset.reset(param.num_pts)\n\n  print ('[{:}] prepare the input data'.format(time_string()))\n  [image, _, _, _, _, _, cropped_size], meta = dataset.prepare_input(args.image, args.face)\n  print ('[{:}] prepare the input data done'.format(time_string()))\n  print ('Net : \\n{:}'.format(net))\n  # network forward\n  with torch.no_grad():\n    if args.cpu: inputs = image.unsqueeze(0)\n    else       : inputs = image.unsqueeze(0).cuda()\n    batch_heatmaps, batch_locs, batch_scos, _ = net(inputs)\n    #print ('input-shape : {:}'.format(inputs.shape))\n    flops, params = get_model_infos(net, inputs.shape, None)\n    print ('\\nIN-shape : {:}, FLOPs : {:} MB, Params : {:}.'.format(list(inputs.shape), flops, params))\n    flops, params = get_model_infos(net, None, inputs)\n    print ('\\nIN-shape : {:}, FLOPs : {:} MB, Params : {:}.'.format(list(inputs.shape), flops, params))\n  print ('[{:}] the network forward done'.format(time_string()))\n\n  # obtain the locations on the image in the orignial size\n  cpu = torch.device('cpu')\n  np_batch_locs, np_batch_scos, cropped_size = batch_locs.to(cpu).numpy(), batch_scos.to(cpu).numpy(), cropped_size.numpy()\n  locations, scores = np_batch_locs[0,:-1,:], np.expand_dims(np_batch_scos[0,:-1], -1)\n\n  scale_h, scale_w = cropped_size[0] * 1. / inputs.size(-2) , cropped_size[1] * 1. / inputs.size(-1)\n\n  locations[:, 0], locations[:, 1] = locations[:, 0] * scale_w + cropped_size[2], locations[:, 1] * scale_h + cropped_size[3]\n  prediction = np.concatenate((locations, scores), axis=1).transpose(1,0)\n  for i in range(param.num_pts):\n    point = prediction[:, i]\n    print ('The coordinate of {:02d}/{:02d}-th points : ({:.1f}, {:.1f}), score = {:.3f}'.format(i, param.num_pts, float(point[0]), float(point[1]), float(point[2])))\n\n  if args.save_path:\n    image = draw_image_by_points(args.image, prediction, 1, (255,0,0), False, False)\n    image.save( args.save_path )\n    print ('save image with landmarks into {:}'.format(args.save_path))\n  print('finish san evaluation on a single image : {:}'.format(args.image))\n\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser(description='Evaluate a single image by the trained model', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument('--image',            type=str,   help='The evaluation image path.')\n  parser.add_argument('--model',            type=str,   help='The snapshot to the saved detector.')\n  parser.add_argument('--face',  nargs='+', type=float, help='The coordinate [x1,y1,x2,y2] of a face')\n  parser.add_argument('--save_path',        type=str,   help='The path to save the visualization results')\n  parser.add_argument('--cpu',     action='store_true', help='Use CPU or not.')\n  args = parser.parse_args()\n  evaluate(args)\n"""
SAN/san_main.py,8,"b'##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom __future__ import division\n\nimport os, sys, time, random, argparse, PIL\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True # please use Pillow 4.0.0 or it may fail for some images\nfrom os import path as osp\nimport numbers, numpy as np\nimport init_path\nimport torch\nimport datasets\nfrom san_vision import transforms\nfrom utils import print_log, count_parameters_in_MB\nfrom utils import convert_size2str, convert_secs2time, time_string, time_for_file\nfrom utils import get_model_infos\nimport debug, models, options, procedure\n\nopt = options.Options(None)\nargs = opt.opt\n# Prepare options\nif args.manualSeed is None: args.manualSeed = random.randint(1, 10000)\nrandom.seed(args.manualSeed)\ntorch.manual_seed(args.manualSeed)\ntorch.cuda.manual_seed_all(args.manualSeed)\ntorch.backends.cudnn.enabled   = True\n#torch.backends.cudnn.benchmark = True\n\ndef main():\n  # Init logger\n  if not os.path.isdir(args.save_path): os.makedirs(args.save_path)\n  log = open(os.path.join(args.save_path, \'seed-{}-{}.log\'.format(args.manualSeed, time_for_file())), \'w\')\n  print_log(\'The save path : {}\'.format(args.save_path), log)\n  print_log(\'------------ Options -------------\', log)\n  for k, v in sorted(vars(args).items()):\n    print_log(\'Parameter : {:20} = {:}\'.format(k, v), log)\n  print_log(\'-------------- End ----------------\', log)\n  print_log(""The random seed : {:}"".format(args.manualSeed), log)\n  print_log(""python version  : {:}"".format(sys.version.replace(\'\\n\', \' \')), log)\n  print_log(""Pillow version  : {:}"".format(PIL.__version__), log)\n  print_log(""torch  version  : {:}"".format(torch.__version__), log)\n  print_log(""cudnn  version  : {:}"".format(torch.backends.cudnn.version()), log)\n\n  # General Data Argumentation\n  mean_fill   = tuple( [int(x*255) for x in [0.5, 0.5, 0.5] ] )\n  normalize   = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                      std=[0.5, 0.5, 0.5])\n  assert args.arg_flip == False, \'The flip is : {}, rotate is {}\'.format(args.arg_flip, args.rotate_max)\n  train_transform = [transforms.PreCrop(args.pre_crop_expand)]\n  train_transform += [transforms.TrainScale2WH((args.crop_width, args.crop_height))]\n  train_transform += [transforms.AugScale(args.scale_prob, args.scale_min, args.scale_max)]\n  #if args.arg_flip:\n  #  train_transform += [transforms.AugHorizontalFlip()]\n  if args.rotate_max:\n    train_transform += [transforms.AugRotate(args.rotate_max)]\n  train_transform+= [transforms.AugCrop(args.crop_width, args.crop_height, args.crop_perturb_max, mean_fill)]\n  train_transform+= [transforms.ToTensor(), normalize]\n  train_transform = transforms.Compose( train_transform )\n\n  eval_transform  = transforms.Compose([transforms.PreCrop(args.pre_crop_expand), transforms.TrainScale2WH((args.crop_width, args.crop_height)),  transforms.ToTensor(), normalize])\n  assert (args.scale_min+args.scale_max) / 2 == args.scale_eval, \'The scale is not ok : {},{} vs {}\'.format(args.scale_min, args.scale_max, args.scale_eval)\n  \n  args.downsample = 8 # By default\n  args.sigma = args.sigma * args.scale_eval\n\n  train_data = datasets.GeneralDataset(train_transform, args.sigma, args.downsample, args.heatmap_type, args.dataset_name)\n  train_data.load_list(args.train_list, args.num_pts, True)\n  if args.convert68to49:\n    train_data.convert68to49()\n  elif args.convert68to51:\n    train_data.convert68to51()\n  train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n\n  eval_loaders = []\n  if args.eval_lists is not None:\n    for eval_list in args.eval_lists:\n      eval_data = datasets.GeneralDataset(eval_transform, args.sigma, args.downsample, args.heatmap_type, args.dataset_name)\n      eval_data.load_list(eval_list, args.num_pts, True)\n      if args.convert68to49:\n        eval_data.convert68to49()\n      elif args.convert68to51:\n        eval_data.convert68to51()\n      eval_loader = torch.utils.data.DataLoader(eval_data, batch_size=args.eval_batch, shuffle=False, num_workers=args.workers, pin_memory=True)\n      eval_loaders.append(eval_loader)\n\n  if args.convert68to49 or args.convert68to51:\n    assert args.num_pts == 68, \'The format of num-pts is not right : {}\'.format(args.num_pts)\n    assert args.convert68to49 + args.convert68to51 == 1, \'Only support one convert\'\n    if args.convert68to49: args.num_pts = 49\n    else:                  args.num_pts = 51\n\n  args.modelconfig = models.ModelConfig(train_data.NUM_PTS+1, args.cpm_stage, args.pretrain, args.argmax_size)\n\n  if args.cycle_model_path is None:\n    # define the network\n    itnetwork = models.itn_model(args.modelconfig, args, log)\n\n    print_log(\'itn-parameters : {:} MB\'.format(itnetwork.num_parameters()), log)\n    cycledata = datasets.CycleDataset(train_transform, args.dataset_name)\n    cycledata.set_a(args.cycle_a_lists)\n    cycledata.set_b(args.cycle_b_lists)\n    print_log(\'Cycle-data initialize done : {}\'.format(cycledata), log)\n\n    args.cycle_model_path = procedure.train_cycle_gan(cycledata, itnetwork, args, log)\n  assert osp.isdir(args.cycle_model_path), \'{:} does not exist or is not dir.\'.format(args.cycle_model_path)\n\n  # start train itn-cpm model\n  itn_cpm = models.__dict__[args.arch](args.modelconfig, args.cycle_model_path)\n  procedure.train_san_epoch(args, itn_cpm, train_loader, eval_loaders, log)\n\n  log.close()\n\nif __name__ == \'__main__\':\n  main()\n'"
TS3/test.py,0,"b""from models import cpm_vgg16, hourglass\nfrom models import count_network_param, get_model_infos\n\nstudent_cpm_config = {'stages': 3,\n                      'dilation': [1],\n                      'pooling' : [1, 1, 1,],\n                      'downsample': 8,\n                      'argmax'  : 4,\n                      'pretrained': False}\n\nstudent_cpm = cpm_vgg16(student_cpm_config, 68)\n#print('CPM:\\n{:}'.format(student_cpm))\nFLOPs, _    = get_model_infos(student_cpm, (1, 3, 64, 64))\nprint('CPM-Parameters : {:} MB, FLOP : {:} MB.'.format(count_network_param(student_cpm) / 1e6, FLOPs))\n\nstudent_hg_config  = {'nStack'  : 4,\n                      'nModules': 2,\n                      'nFeats'  : 256,\n                      'downsample' : 4}\n\nstudent_hg  = hourglass(student_hg_config, 68)\nFLOPs, _    = get_model_infos(student_hg, (1, 3, 64, 64))\n#print('CPM:\\n{:}'.format(student_cpm))\nprint('HG--Parameters : {:} MB, FLOP : {:} MB.'.format(count_network_param(student_hg ) / 1e6, FLOPs))\n"""
SAN/cache_data/aflw_from_mat.py,0,"b'##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nimport sqlite3\nimport os, math\nimport os.path as osp\nimport copy\nimport numpy as np\nimport init_path\nfrom scipy.io import loadmat\nimport datasets\n\n#Change this paths according to your directories\nthis_dir = osp.dirname(os.path.abspath(__file__))\nSAVE_DIR = osp.join(this_dir, \'lists\', \'AFLW\')\nHOME_STR = \'HOME\'\nassert HOME_STR in os.environ, \'Doest not find the HOME dir : {}\'.format(HOME_STR)\nprint (\'This dir : {}, HOME : [{}] : {}\'.format(this_dir, HOME_STR, os.environ[HOME_STR]))\nif not osp.isdir(SAVE_DIR): os.makedirs(SAVE_DIR)\nimage_dir = osp.join(os.environ[HOME_STR], \'datasets\', \'AFLW-Style\')\nannot_dir = osp.join(os.environ[HOME_STR], \'datasets\', \'AFLW-Style\', \'annotation\')\nprint (\'data dir : {}\'.format(image_dir))\nprint (\'annotation dir : {}\'.format(annot_dir))\n\n\nclass AFLWFace():\n  def __init__(self, index, name, mask, landmark, box):\n    self.image_path = name\n    self.face_id = index\n    self.face_box = [float(box[0]), float(box[2]), float(box[1]), float(box[3])]\n    mask = np.expand_dims(mask, axis=1)\n    landmark = landmark.copy()\n    self.landmarks = np.concatenate((landmark, mask), axis=1)\n\n  def get_face_size(self, use_box):\n    box = []\n    if use_box == \'GTL\':\n      box = datasets.dataset_utils.PTSconvert2box(self.landmarks.copy().T)\n    elif use_box == \'GTB\':\n      box = [self.face_box[0], self.face_box[1], self.face_box[2], self.face_box[3]]\n    else:\n      assert False, \'The box indicator not find : {}\'.format(use_box)\n    assert box[2] > box[0], \'The size of box is not right [{}] : {}\'.format(self.face_id, box)\n    assert box[3] > box[1], \'The size of box is not right [{}] : {}\'.format(self.face_id, box)\n    face_size = math.sqrt( float(box[3]-box[1]) * float(box[2]-box[0]) )\n    box_str = \'{:.2f} {:.2f} {:.2f} {:.2f}\'.format(box[0], box[1], box[2], box[3])\n    return box_str, face_size\n\n  def check_front(self):\n    oks = 0\n    box = self.face_box\n    for idx in range(self.landmarks.shape[0]):\n      if bool(self.landmarks[idx,2]):\n        x, y = self.landmarks[idx,0], self.landmarks[idx,1]\n        if x > self.face_box[0] and x < self.face_box[2]:\n          if y > self.face_box[1] and y < self.face_box[3]:\n            oks = oks + 1\n    return oks == 19\n    \n  def __repr__(self):\n    return (\'{name}(path={image_path}, face-id={face_id})\'.format(name=self.__class__.__name__, **self.__dict__))\n\n\ndef save_to_list_file(allfaces, lst_file, image_style_dir, annotation_dir, face_indexes, use_front, use_box):\n  save_faces = []\n  for index in face_indexes:\n    face = allfaces[index]\n    if use_front == False or face.check_front():\n      save_faces.append( face )\n  print (\'Prepare to save {} face images into {}\'.format(len(save_faces), lst_file))\n\n  lst_file = open(lst_file, \'w\')\n  all_face_sizes = []\n  for face in save_faces:\n    image_path = face.image_path\n    sub_dir, base_name = image_path.split(\'/\')\n    cannot_dir = osp.join(annotation_dir, sub_dir)\n    cannot_path = osp.join(cannot_dir, base_name.split(\'.\')[0] + \'-{}.pts\'.format(face.face_id))\n    if not osp.isdir(cannot_dir): os.makedirs(cannot_dir)\n    image_path = osp.join(image_style_dir, image_path)\n    assert osp.isfile(image_path), \'The image [{}/{}] {} does not exsit\'.format(index, len(save_faces), image_path)\n\n    pts_str = datasets.PTSconvert2str( face.landmarks.T )\n    pts_file = open(cannot_path, \'w\')\n    pts_file.write(\'{}\'.format(pts_str))\n    pts_file.close()\n\n    box_str, face_size = face.get_face_size(use_box)\n\n    lst_file.write(\'{} {} {} {}\\n\'.format(image_path, cannot_path, box_str, face_size))\n    all_face_sizes.append( face_size )\n  lst_file.close()\n\n  all_faces = np.array( all_face_sizes )\n  print (\'all faces : mean={}, std={}\'.format(all_faces.mean(), all_faces.std()))\n\nif __name__ == ""__main__"":\n  mat_path = osp.join(this_dir, \'AFLWinfo_release.mat\')\n  aflwinfo = dict()\n  mat = loadmat(mat_path)\n  total_image = 24386\n  # load train/test splits\n  ra = np.squeeze(mat[\'ra\']-1).tolist()\n  aflwinfo[\'train-index\'] = ra[:20000]\n  aflwinfo[\'test-index\'] = ra[20000:]\n  aflwinfo[\'name-list\'] = []\n  # load name-list\n  for i in range(total_image):\n    name = mat[\'nameList\'][i,0][0]\n    name = name[:-4] + \'.jpg\'\n    aflwinfo[\'name-list\'].append( name )\n  aflwinfo[\'mask\'] = mat[\'mask_new\'].copy()\n  aflwinfo[\'landmark\'] = mat[\'data\'].reshape((total_image, 2, 19))\n  aflwinfo[\'landmark\'] = np.transpose(aflwinfo[\'landmark\'], (0,2,1))\n  aflwinfo[\'box\'] = mat[\'bbox\'].copy()\n  allfaces = []\n  for i in range(total_image):\n    face = AFLWFace(i, aflwinfo[\'name-list\'][i], aflwinfo[\'mask\'][i], aflwinfo[\'landmark\'][i], aflwinfo[\'box\'][i])\n    allfaces.append( face )\n  \n  USE_BOXES = [\'GTL\', \'GTB\']\n  STYLES = [\'Original\', \'Gray\', \'Sketch\', \'Light\']\n  for STYLE in STYLES:\n    temp_dir = osp.join(SAVE_DIR, STYLE)\n    if not osp.isdir(temp_dir): os.makedirs(temp_dir)\n    for USE_BOX in USE_BOXES:\n      save_to_list_file(allfaces, osp.join(SAVE_DIR, STYLE, \'train.{}\'.format(USE_BOX)),      osp.join(image_dir, \'aflw-\'+STYLE), annot_dir, aflwinfo[\'train-index\'], False, USE_BOX)\n      save_to_list_file(allfaces, osp.join(SAVE_DIR, STYLE, \'test.{}\'.format(USE_BOX)),       osp.join(image_dir, \'aflw-\'+STYLE), annot_dir, aflwinfo[\'test-index\'], False, USE_BOX)\n      save_to_list_file(allfaces, osp.join(SAVE_DIR, STYLE, \'test.front.{}\'.format(USE_BOX)), osp.join(image_dir, \'aflw-\'+STYLE), annot_dir, aflwinfo[\'test-index\'], True, USE_BOX)\n      save_to_list_file(allfaces, osp.join(SAVE_DIR, STYLE, \'all.{}\'.format(USE_BOX)),        osp.join(image_dir, \'aflw-\'+STYLE), annot_dir, aflwinfo[\'train-index\'] + aflwinfo[\'test-index\'], False, USE_BOX)\n'"
SAN/cache_data/generate_300W.py,0,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nimport numpy as np\nimport math, pdb\nimport os, sys\nimport os.path as osp\nfrom pathlib import Path\nimport init_path\nimport datasets\nfrom scipy.io import loadmat\nfrom utils.file_utils import load_list_from_folders, load_txt_file\n\ndef load_box(mat_path, cdir):\n  mat = loadmat(mat_path)\n  mat = mat['bounding_boxes']\n  mat = mat[0]\n  assert len(mat) > 0, 'The length of this mat file should be greater than 0 vs {}'.format(len(mat))\n  all_object = []\n  for cobject in mat:\n    name = cobject[0][0][0][0]\n    bb_detector = cobject[0][0][1][0]\n    bb_ground_t = cobject[0][0][2][0]\n    image_path = osp.join(cdir, name)\n    image_path = image_path[:-4]\n    all_object.append( (image_path, bb_detector, bb_ground_t) )\n  return all_object\n\ndef load_mats(lists):\n  all_objects = []\n  for dataset in lists:\n    cobjects = load_box(dataset[0], dataset[1])\n    all_objects = all_objects + cobjects\n  return all_objects\n\ndef load_all_300w(root_dir, style):\n  mat_dir = osp.join(root_dir, 'Bounding_Boxes')\n  pairs = [(osp.join(mat_dir,  'bounding_boxes_lfpw_testset.mat'),   osp.join(root_dir, '300W-' + style, 'lfpw', 'testset')),\n           (osp.join(mat_dir,  'bounding_boxes_lfpw_trainset.mat'),  osp.join(root_dir, '300W-' + style, 'lfpw', 'trainset')),\n           (osp.join(mat_dir,  'bounding_boxes_ibug.mat'),           osp.join(root_dir, '300W-' + style, 'ibug')),\n           (osp.join(mat_dir,  'bounding_boxes_afw.mat'),            osp.join(root_dir, '300W-' + style, 'afw')),\n           (osp.join(mat_dir,  'bounding_boxes_helen_testset.mat'),  osp.join(root_dir, '300W-' + style, 'helen', 'testset')),\n           (osp.join(mat_dir,  'bounding_boxes_helen_trainset.mat'), osp.join(root_dir, '300W-' + style, 'helen', 'trainset')),]\n\n  all_datas = load_mats(pairs)\n  data_dict = {}\n  for i, cpair in enumerate(all_datas):\n    image_path = cpair[0].replace(' ', '')\n    data_dict[ image_path ] = (cpair[1], cpair[2])\n  return data_dict\n\ndef return_box(image_path, pts_path, all_dict, USE_BOX):\n  image_path = image_path[:-4]\n  assert image_path in all_dict, '{} not find'.format(image_path)\n  np_boxes = all_dict[ image_path ]\n  if USE_BOX == 'GTL':\n    box_str = datasets.dataset_utils.for_generate_box_str(pts_path, 68, 0)\n  elif USE_BOX == 'GTB':\n    box_str = '{:.3f} {:.3f} {:.3f} {:.3f}'.format(np_boxes[1][0], np_boxes[1][1], np_boxes[1][2], np_boxes[1][3])\n  elif USE_BOX == 'DET':\n    box_str = '{:.3f} {:.3f} {:.3f} {:.3f}'.format(np_boxes[0][0], np_boxes[0][1], np_boxes[0][2], np_boxes[0][3])\n  else:\n    assert False, 'The box indicator not find : {}'.format(USE_BOX)\n  return box_str\n\ndef generage_300w_list(root, save_dir, box_data, SUFFIX):\n  assert osp.isdir(root), '{} is not dir'.format(root)\n  if not osp.isdir(save_dir): os.makedirs(save_dir)\n  train_length, common_length, challeng_length = 3148, 554, 135\n  subsets = ['afw', 'helen', 'ibug', 'lfpw']\n  dir_lists = [osp.join(root, subset) for subset in subsets]\n  imagelist, num_image = load_list_from_folders(dir_lists, ext_filter=['png', 'jpg', 'jpeg'], depth=3)\n\n  indoor, indoor_num = load_list_from_folders([osp.join(root, '300W', '01_Indoor')], ext_filter=['png', 'jpg', 'jpeg'], depth=3)\n  otdoor, otdoor_num = load_list_from_folders([osp.join(root, '300W', '02_Outdoor')], ext_filter=['png', 'jpg', 'jpeg'], depth=3)\n  assert indoor_num == 300 and otdoor_num == 300, 'The number of images are not right for 300-W-IO: {} & {}'.format(indoor_num, otdoor_num)\n\n  train_set, common_set, challeng_set = [], [], []\n  for image_path in imagelist:\n    name, ext = osp.splitext(image_path)\n    anno_path = name + '.pts'\n    assert osp.isfile(anno_path), 'annotation {} for : {} does not exist'.format(image_path, anno_path)\n    if name.find('ibug') > 0:\n      challeng_set.append( (image_path, anno_path) )\n    elif name.find('afw') > 0:\n      train_set.append( (image_path, anno_path) )\n    elif name.find('helen') > 0 or name.find('lfpw') > 0:\n      if name.find('trainset') > 0:\n        train_set.append( (image_path, anno_path) )\n      elif name.find('testset') > 0:\n        common_set.append( (image_path, anno_path) )\n      else:\n        raise Exception('Unknow name : {}'.format(name))\n    else:\n      raise Exception('Unknow name : {}'.format(name))\n  assert len(train_set) == train_length, 'The length is not right for train : {} vs {}'.format(len(train_set), train_length)\n  assert len(common_set) == common_length, 'The length is not right for common : {} vs {}'.format(len(common_set), common_length)\n  assert len(challeng_set) == challeng_length, 'The length is not right for challeng : {} vs {}'.format(len(common_set), common_length)\n\n  print ('root={:}, save_dir={:}, {:}'.format(root, save_dir, SUFFIX))\n  all_lines = []\n  with open(osp.join(save_dir, '300w.train.' + SUFFIX), 'w') as txtfile:\n    for cpair in train_set:\n      box_str = return_box(cpair[0], cpair[1], box_data, SUFFIX)\n      txtfile.write('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n      all_lines.append('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n  txtfile.close()\n  print ('300W-Trarin : {:} lines'.format(len(all_lines)))\n\n  with open(osp.join(save_dir, '300w.test.common.' + SUFFIX), 'w') as txtfile:\n    for cpair in common_set:\n      box_str = return_box(cpair[0], cpair[1], box_data, SUFFIX)\n      txtfile.write('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n      all_lines.append('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n  txtfile.close()\n\n  with open(osp.join(save_dir, '300w.test.challenge.' + SUFFIX), 'w') as txtfile:\n    for cpair in challeng_set:\n      box_str = return_box(cpair[0], cpair[1], box_data, SUFFIX)\n      txtfile.write('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n      all_lines.append('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n  txtfile.close()\n\n  with open(osp.join(save_dir, '300w.test.full.' + SUFFIX), 'w') as txtfile:\n    for cpair in common_set:\n      box_str = return_box(cpair[0], cpair[1], box_data, SUFFIX)\n      txtfile.write('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n      all_lines.append('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n    for cpair in challeng_set:\n      box_str = return_box(cpair[0], cpair[1], box_data, SUFFIX)\n      txtfile.write('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n      all_lines.append('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n  txtfile.close()\n\n  with open(osp.join(save_dir, '300w.all.' + SUFFIX), 'w') as txtfile:\n    for line in all_lines:\n      txtfile.write('{}'.format(line))\n  txtfile.close()\n  print ('300W----ALL : {:} lines'.format(len(all_lines)))\n\nif __name__ == '__main__':\n  this_dir = osp.dirname(os.path.abspath(__file__))\n  print ('This dir : {:}, {:}'.format(this_dir, os.environ['HOME']))\n  path_300w = Path.home() / 'datasets' / '300W-Style'\n  print ('300W Dir : {:}'.format(path_300w))\n  assert path_300w.exists(), '{:} does not exists'.format(path_300w)\n  path_300w = str(path_300w)\n  styles = ['Original', 'Gray', 'Light', 'Sketch']\n  USE_BOXES = ['GTB', 'DET']\n  for USE_BOX in USE_BOXES:\n    for style in styles:\n      box_datas = load_all_300w(path_300w, style)\n      SAVE_DIR  = osp.join(this_dir, 'lists', '300W', style)\n      Data_DIR  = osp.join(path_300w, '300W-' + style)\n      generage_300w_list(Data_DIR, SAVE_DIR, box_datas, USE_BOX)\n"""
SAN/cache_data/init_path.py,0,"b'##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\n""""""Set up paths.""""""            \n  \nimport sys, os  \nfrom os import path as osp\n\ndef add_path(path):            \n  if path not in sys.path:   \n    sys.path.insert(0, path)\nthis_dir = osp.dirname(osp.abspath(__file__))\n\n# Add lib to PYTHONPATH  \nlib_path = osp.abspath(osp.join(this_dir, \'..\', \'lib\'))\nadd_path(lib_path)\n'"
SAN/cache_data/vis.py,0,"b""import PIL, numpy as np\nimport sys, os, torch\nfrom os import path as osp\nfrom pathlib import Path\nlib_dir = (Path(__file__).parent / '..' / 'lib').resolve()\nif str(lib_dir) not in sys.path: sys.path.insert(0, str(lib_dir))\nprint ('LIB : {:}'.format(lib_dir))\nimport models, datasets\nfrom san_vision import transforms\nfrom visualization import merge_images, generate_color_from_heatmaps, overlap_two_pil_image\nPRINT_GAP = 500\n\ndef visual(clist, cdir, num_pts):\n  if not cdir.exists(): os.makedirs(str(cdir))\n  shape = 256\n  transform  = transforms.Compose([transforms.PreCrop(0.2), transforms.TrainScale2WH((shape, shape))])\n  data = datasets.GeneralDataset(transform, 2, 1, 'gaussian', 'test')\n  data.load_list(clist, num_pts, True)\n  for i, tempx in enumerate(data):\n    image = tempx[0]\n    heats = models.variable2np(tempx[1]).transpose(1,2,0)\n    xheat = generate_color_from_heatmaps(heats, index=-1)\n    xheat = PIL.Image.fromarray(np.uint8(xheat*255))\n\n    cimage = overlap_two_pil_image(image, xheat)\n\n    basename = osp.basename(data.datas[i]).split('.')[0]\n    basename = str(cdir) + '/' + basename + '-{:}.jpg'\n\n    image.save(basename.format('ori'))\n    xheat.save(basename.format('heat'))\n    cimage.save(basename.format('over'))\n\n    if i % PRINT_GAP == 0:\n      print ('--->>> process the {:4d}/{:4d}-th image'.format(i, len(data)))\n\nif __name__ == '__main__':\n  clist = './lists/300W/Original/300w.train.GTB'\n  cdir = Path('./cache/temp-visualize')\n  visual(clist, cdir, 68)\n"""
SBR/cache_data/aflw_from_mat.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport sys, sqlite3\nimport os, math\nimport os.path as osp\nfrom pathlib import Path\nimport copy\nimport numpy as np\nlib_dir = (Path(__file__).parent / \'..\' / \'lib\').resolve()\nif str(lib_dir) not in sys.path: sys.path.insert(0, str(lib_dir))\nassert sys.version_info.major == 3, \'Please upgrade from {:} to Python 3.x\'.format(sys.version_info)\nfrom scipy.io import loadmat\nimport datasets\n\n#Change this paths according to your directories\nthis_dir = osp.dirname(os.path.abspath(__file__))\nSAVE_DIR = osp.join(this_dir, \'lists\', \'AFLW\')\nHOME_STR = \'DOME_HOME\'\nif HOME_STR not in os.environ: HOME_STR = \'HOME\'\nassert HOME_STR in os.environ, \'Doest not find the HOME dir : {}\'.format(HOME_STR)\nprint (\'This dir : {}, HOME : [{}] : {}\'.format(this_dir, HOME_STR, os.environ[HOME_STR]))\nif not osp.isdir(SAVE_DIR): os.makedirs(SAVE_DIR)\nimage_dir = osp.join(os.environ[HOME_STR], \'datasets\', \'landmark-datasets\', \'AFLW\', \'images\')\nannot_dir = osp.join(os.environ[HOME_STR], \'datasets\', \'landmark-datasets\', \'AFLW\', \'annotations\')\nprint (\'AFLW image dir : {}\'.format(image_dir))\nprint (\'AFLW annotation dir : {}\'.format(annot_dir))\nassert osp.isdir(image_dir), \'The image dir : {} does not exist\'.format(image_dir)\n#assert osp.isdir(image_dir), \'The image dir : {} does not exist\'.format(image_dir)\n\n\nclass AFLWFace():\n  def __init__(self, index, name, mask, landmark, box):\n    self.image_path = name\n    self.face_id = index\n    self.face_box = [float(box[0]), float(box[2]), float(box[1]), float(box[3])]\n    mask = np.expand_dims(mask, axis=1)\n    landmark = landmark.copy()\n    self.landmarks = np.concatenate((landmark, mask), axis=1)\n\n  def get_face_size(self, use_box):\n    box = []\n    if use_box == \'GTL\':\n      box = datasets.dataset_utils.PTSconvert2box(self.landmarks.copy().T)\n    elif use_box == \'GTB\':\n      box = [self.face_box[0], self.face_box[1], self.face_box[2], self.face_box[3]]\n    else:\n      assert False, \'The box indicator not find : {}\'.format(use_box)\n    assert box[2] > box[0], \'The size of box is not right [{}] : {}\'.format(self.face_id, box)\n    assert box[3] > box[1], \'The size of box is not right [{}] : {}\'.format(self.face_id, box)\n    face_size = math.sqrt( float(box[3]-box[1]) * float(box[2]-box[0]) )\n    box_str = \'{:.2f} {:.2f} {:.2f} {:.2f}\'.format(box[0], box[1], box[2], box[3])\n    return box_str, face_size\n\n  def check_front(self):\n    oks = 0\n    box = self.face_box\n    for idx in range(self.landmarks.shape[0]):\n      if bool(self.landmarks[idx,2]):\n        x, y = self.landmarks[idx,0], self.landmarks[idx,1]\n        if x > self.face_box[0] and x < self.face_box[2]:\n          if y > self.face_box[1] and y < self.face_box[3]:\n            oks = oks + 1\n    return oks == 19\n    \n  def __repr__(self):\n    return (\'{name}(path={image_path}, face-id={face_id})\'.format(name=self.__class__.__name__, **self.__dict__))\n\ndef save_to_list_file(allfaces, lst_file, image_style_dir, annotation_dir, face_indexes, use_front, use_box):\n  save_faces = []\n  for index in face_indexes:\n    face = allfaces[index]\n    if use_front == False or face.check_front():\n      save_faces.append( face )\n  print (\'Prepare to save {} face images into {}\'.format(len(save_faces), lst_file))\n\n  lst_file = open(lst_file, \'w\')\n  all_face_sizes = []\n  for face in save_faces:\n    image_path = face.image_path\n    sub_dir, base_name = image_path.split(\'/\')\n    cannot_dir = osp.join(annotation_dir, sub_dir)\n    cannot_path = osp.join(cannot_dir, base_name.split(\'.\')[0] + \'-{}.pts\'.format(face.face_id))\n    if not osp.isdir(cannot_dir): os.makedirs(cannot_dir)\n    image_path = osp.join(image_style_dir, image_path)\n    assert osp.isfile(image_path), \'The image [{}/{}] {} does not exsit\'.format(index, len(save_faces), image_path)\n\n    if not osp.isfile(cannot_path):\n      pts_str = datasets.PTSconvert2str( face.landmarks.T )\n      pts_file = open(cannot_path, \'w\')\n      pts_file.write(\'{}\'.format(pts_str))\n      pts_file.close()\n    else: pts_str = None\n\n    box_str, face_size = face.get_face_size(use_box)\n\n    lst_file.write(\'{} {} {} {}\\n\'.format(image_path, cannot_path, box_str, face_size))\n    all_face_sizes.append( face_size )\n  lst_file.close()\n\n  all_faces = np.array( all_face_sizes )\n  print (\'all faces : mean={}, std={}\'.format(all_faces.mean(), all_faces.std()))\n\nif __name__ == ""__main__"":\n  mat_path = osp.join(this_dir, \'AFLWinfo_release.mat\')\n  aflwinfo = dict()\n  mat = loadmat(mat_path)\n  total_image = 24386\n  # load train/test splits\n  ra = np.squeeze(mat[\'ra\']-1).tolist()\n  aflwinfo[\'train-index\'] = ra[:20000]\n  aflwinfo[\'test-index\'] = ra[20000:]\n  aflwinfo[\'name-list\'] = []\n  # load name-list\n  for i in range(total_image):\n    name = mat[\'nameList\'][i,0][0]\n    #name = name[:-4] + \'.jpg\'\n    aflwinfo[\'name-list\'].append( name )\n  aflwinfo[\'mask\'] = mat[\'mask_new\'].copy()\n  aflwinfo[\'landmark\'] = mat[\'data\'].reshape((total_image, 2, 19))\n  aflwinfo[\'landmark\'] = np.transpose(aflwinfo[\'landmark\'], (0,2,1))\n  aflwinfo[\'box\'] = mat[\'bbox\'].copy()\n  allfaces = []\n  for i in range(total_image):\n    face = AFLWFace(i, aflwinfo[\'name-list\'][i], aflwinfo[\'mask\'][i], aflwinfo[\'landmark\'][i], aflwinfo[\'box\'][i])\n    allfaces.append( face )\n  \n  USE_BOXES = [\'GTL\', \'GTB\']\n  for USE_BOX in USE_BOXES:\n    save_to_list_file(allfaces, osp.join(SAVE_DIR, \'train.{}\'.format(USE_BOX)),      image_dir, annot_dir, aflwinfo[\'train-index\'], False, USE_BOX)\n    save_to_list_file(allfaces, osp.join(SAVE_DIR, \'test.{}\'.format(USE_BOX)),       image_dir, annot_dir, aflwinfo[\'test-index\'],  False, USE_BOX)\n    save_to_list_file(allfaces, osp.join(SAVE_DIR, \'test.front.{}\'.format(USE_BOX)), image_dir, annot_dir, aflwinfo[\'test-index\'],  True,  USE_BOX)\n    save_to_list_file(allfaces, osp.join(SAVE_DIR, \'all.{}\'.format(USE_BOX)),        image_dir, annot_dir, aflwinfo[\'train-index\'] + aflwinfo[\'test-index\'], False, USE_BOX)\n'"
SBR/cache_data/demo_list.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport os, sys, glob, cv2\nfrom os import path as osp\nfrom pathlib import Path\nlib_dir = (Path(__file__).parent / \'..\' / \'lib\').resolve()\nif str(lib_dir) not in sys.path: sys.path.insert(0, str(lib_dir))\nassert sys.version_info.major == 3, \'Please upgrade from {:} to Python 3.x\'.format(sys.version_info)\nprint (\'lib-dir : {:}\'.format(lib_dir))\nfrom datasets import pil_loader\nfrom utils.file_utils import load_list_from_folders, load_txt_file\n# ffmpeg -i shooui.mp4 -filter:v ""crop=450:680:10:120"" -c:a copy ~/Desktop/demo.mp4\n\n\ndef generate(demo_dir, list_dir, savename, check):\n  imagelist, num_image = load_list_from_folders(demo_dir, ext_filter=[\'png\'], depth=1)\n  assert num_image == check, \'The number of images is not right vs. {:}\'.format(num_image)\n  if not osp.isdir(list_dir): os.makedirs(list_dir)\n  \n  gap, x1, y1, x2, y2 = 5, 5, 5, 450, 680\n\n  imagelist.sort()\n\n  txtfile = open(osp.join(list_dir, savename), \'w\')\n  for idx, image in enumerate(imagelist):\n    if idx < 2 or idx + 2 >= len(imagelist): continue\n    box_str = \'{:.1f} {:.1f} {:.1f} {:.1f}\'.format(gap, gap, x2-x1-gap, y2-y1-gap)\n    txtfile.write(\'{:} {:} {:}\\n\'.format(image, \'none\', box_str))\n    txtfile.flush()\n  txtfile.close()\n  print(\'there are {:} images for the demo video sequence\'.format(num_image))\n\nif __name__ == \'__main__\':\n  HOME_STR = \'DOME_HOME\'\n  if HOME_STR not in os.environ: HOME_STR = \'HOME\'\n  assert HOME_STR in os.environ, \'Doest not find the HOME dir : {}\'.format(HOME_STR)\n\n  this_dir = osp.dirname(os.path.abspath(__file__))\n  demo_dir = osp.join(this_dir, \'cache\', \'demo-sbrs\')\n  list_dir = osp.join(this_dir, \'lists\', \'demo\')\n  print (\'This dir : {}, HOME : [{}] : {}\'.format(this_dir, HOME_STR, os.environ[HOME_STR]))\n  generate(demo_dir, list_dir, \'demo-sbr.lst\', 275)\n\n  #demo_dir = osp.join(this_dir, \'cache\', \'demo-pams\')\n  #list_dir = osp.join(this_dir, \'lists\', \'demo\')\n  #print (\'This dir : {}, HOME : [{}] : {}\'.format(this_dir, HOME_STR, os.environ[HOME_STR]))\n  #generate(demo_dir, list_dir, \'demo-pam.lst\', 253)\n'"
SBR/cache_data/extrct_300VW.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport os, pdb, sys, glob\nfrom os import path as osp\n\ndef generate_extract_300vw(P300VW):\n  allfiles = glob.glob(osp.join(P300VW, '*'))\n  alldirs = []\n  for xfile in allfiles:\n    if osp.isdir( xfile ):\n      alldirs.append(xfile)\n  assert len(alldirs) == 114, 'The directories of 300VW should be 114 not {}'.format(len(alldirs))\n  cmds = []\n  for xdir in alldirs:\n    video = osp.join(xdir, 'vid.avi')\n    exdir = osp.join(xdir, 'extraction')\n    if not osp.isdir(exdir): os.makedirs(exdir)\n    cmd = 'ffmpeg -i {:} {:}/%06d.png'.format(video, exdir)\n    cmds.append( cmd )\n\n  if not osp.isdir('./cache'):\n    os.makedirs('./cache')\n\n  with open('./cache/Extract300VW.sh', 'w') as txtfile:\n    for cmd in cmds:\n      txtfile.write('{}\\n'.format(cmd))\n  txtfile.close()\n\nif __name__ == '__main__':\n  HOME = 'DOME_HOME' if 'DOME_HOME' in os.environ else 'HOME'\n  P300VW = osp.join(os.environ[HOME], 'datasets', 'landmark-datasets', '300VW_Dataset_2015_12_14')\n  generate_extract_300vw(P300VW)\n"""
SBR/cache_data/generate_300VW.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport numpy as np\nimport math, os, pdb, sys, glob\nfrom os import path as osp\nfrom pathlib import Path\nlib_dir = (Path(__file__).parent / '..' / 'lib').resolve()\nif str(lib_dir) not in sys.path: sys.path.insert(0, str(lib_dir))\nassert sys.version_info.major == 3, 'Please upgrade from {:} to Python 3.x'.format(sys.version_info)\nprint ('lib-dir : {:}'.format(lib_dir))\nimport datasets\n\nEXPAND_RATIO = 0.0\nafterfix='.10'\n\n\ndef str2size(box_str):\n  splits = box_str.split(' ')\n  x1, y1, x2, y2 = float(splits[0]), float(splits[1]), float(splits[2]), float(splits[3])\n  return math.sqrt( (x2-x1) * (y2-y1) )\n\n\ndef load_video_dir(root, dirs, save_dir, save_name):\n  videos, sparse_videos = [], []\n  first_videos = []\n  for idx, cdir in enumerate(dirs):\n    annot_path = osp.join(root, cdir, 'annot')\n    frame_path = osp.join(root, cdir, 'extraction')\n    all_frames = glob.glob( osp.join(frame_path, '*.png') )\n    all_annots = glob.glob( osp.join(annot_path, '*.pts') )\n    assert len(all_frames) == len(all_annots), 'The length is not right for {} : {} vs {}'.format(cdir, len(all_frames), len(all_annots))\n    all_frames = sorted(all_frames)\n    all_annots = sorted(all_annots)\n    current_video = []\n    txtfile = open(osp.join(save_dir, save_name + cdir), 'w')\n    nonefile = open(osp.join(save_dir, save_name + cdir + '.none'), 'w')\n\n    all_sizes = []\n    for frame, annot in zip(all_frames, all_annots):\n      basename_f = osp.basename(frame)\n      basename_a = osp.basename(annot)\n      assert basename_a[:6] == basename_f[:6], 'The name of {} is not right with {}'.format(frame, annot)\n      current_video.append( (frame, annot) )\n      box_str = datasets.dataset_utils.for_generate_box_str(annot, 68, EXPAND_RATIO)\n      txtfile.write('{} {} {}\\n'.format(frame, annot, box_str))\n      nonefile.write('{} None {}\\n'.format(frame, box_str))\n      all_sizes.append( str2size(box_str) )\n      if len(current_video) == 1:\n        first_videos.append( (frame, annot) )\n    txtfile.close()\n    nonefile.close()\n    videos.append( current_video )\n    all_sizes = np.array( all_sizes )\n    print ('--->>> {:} : [{:02d}/{:02d}] : {:} has {:} frames | face size : mean={:.2f}, std={:.2f}'.format(save_name, idx, len(dirs), cdir, len(all_frames), all_sizes.mean(), all_sizes.std()))\n\n    for jxj, video in enumerate(current_video):\n      if jxj <= 3 or jxj + 3 >= len(current_video): continue\n      if jxj % 10 == 3:\n        sparse_videos.append( video )\n\n  txtfile = open(osp.join(save_dir, save_name), 'w')\n  nonefile = open(osp.join(save_dir, save_name + '.none'), 'w')\n  for video in videos:\n    for cpair in video:\n      box_str = datasets.dataset_utils.for_generate_box_str(cpair[1], 68, EXPAND_RATIO)\n      txtfile.write('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n      nonefile.write('{} {} {}\\n'.format(cpair[0], 'None', box_str))\n      txtfile.flush()\n      nonefile.flush()\n  txtfile.close()\n  nonefile.close()\n\n  txtfile = open(osp.join(save_dir, save_name + '.sparse' + afterfix), 'w')\n  nonefile = open(osp.join(save_dir, save_name + '.sparse.none' + afterfix), 'w')\n  for cpair in sparse_videos:\n    box_str = datasets.dataset_utils.for_generate_box_str(cpair[1], 68, EXPAND_RATIO)\n    txtfile.write('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n    nonefile.write('{} {} {}\\n'.format(cpair[0], 'None', box_str))\n  txtfile.close()\n  nonefile.close()\n\n  txtfile = open(osp.join(save_dir, save_name + '.first'), 'w')\n  for cpair in first_videos:\n    box_str = datasets.dataset_utils.for_generate_box_str(cpair[1], 68, EXPAND_RATIO)\n    txtfile.write('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n  txtfile.close()\n\n  print ('{} finish save into {}'.format(save_name, save_dir))\n  return videos\n\ndef generate_300vw_list(root, save_dir):\n  assert osp.isdir(root), '{} is not dir'.format(root)\n  if not osp.isdir(save_dir): os.makedirs(save_dir)\n  test_1_dirs = [114, 124, 125, 126, 150, 158, 401, 402, 505, 506, 507, 508, 509, 510, 511, 514, 515, 518, 519, 520, 521, 522, 524, 525, 537, 538, 540, 541, 546, 547, 548]\n  test_2_dirs = [203, 208, 211, 212, 213, 214, 218, 224, 403, 404, 405, 406, 407, 408, 409, 412, 550, 551, 553]\n  test_3_dirs = [410, 411, 516, 517, 526, 528, 529, 530, 531, 533, 557, 558, 559, 562]\n  train_dirs  = ['009', '059', '002', '033', '020', '035', '018', '119', '120', '025', '205', '047', '007', '013', '004', '143',\n                 '034', '028', '053', '225', '041', '010', '031', '046', '049', '011', '027', '003', '016', '160', '113', '001', '029', '043',\n                 '112', '138', '144', '204', '057', '015', '044', '048', '017', '115', '223', '037', '123', '019', '039', '022']\n\n  test_1_dirs, test_2_dirs, test_3_dirs = [ '{}'.format(x) for x in test_1_dirs], [ '{}'.format(x) for x in test_2_dirs], [ '{}'.format(x) for x in test_3_dirs]\n  #all_dirs = os.listdir(root)\n  #train_dirs = set(all_dirs) - set(test_1_dirs) - set(test_2_dirs) - set(test_3_dirs) - set(['ReadMe.txt', 'extra.zip'])\n  #train_dirs = list( train_dirs )\n  assert len(train_dirs) == 50, 'The length of train_dirs is not right : {}'.format( len(train_dirs) )\n  assert len(test_3_dirs) == 14, 'The length of test_3_dirs is not right : {}'.format( len(test_3_dirs) )\n\n  load_video_dir(root,  train_dirs, save_dir, '300VW.train.lst')\n  load_video_dir(root, test_1_dirs, save_dir, '300VW.test-1.lst')\n  load_video_dir(root, test_2_dirs, save_dir, '300VW.test-2.lst')\n  load_video_dir(root, test_3_dirs, save_dir, '300VW.test-3.lst')\n\nif __name__ == '__main__':\n  HOME_STR = 'DOME_HOME'\n  if HOME_STR not in os.environ: HOME_STR = 'HOME'\n  assert HOME_STR in os.environ, 'Doest not find the HOME dir : {}'.format(HOME_STR)\n\n  this_dir = osp.dirname(os.path.abspath(__file__))\n  SAVE_DIR = osp.join(this_dir, 'lists', '300VW')\n  print ('This dir : {}, HOME : [{}] : {}'.format(this_dir, HOME_STR, os.environ[HOME_STR]))\n  path_300vw = osp.join(os.environ[HOME_STR], 'datasets', 'landmark-datasets', '300VW_Dataset_2015_12_14')\n  generate_300vw_list(path_300vw, SAVE_DIR)\n"""
SBR/cache_data/generate_300W.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport os, sys, glob\nfrom os import path as osp\nfrom pathlib import Path\nlib_dir = (Path(__file__).parent / '..' / 'lib').resolve()\nif str(lib_dir) not in sys.path: sys.path.insert(0, str(lib_dir))\nassert sys.version_info.major == 3, 'Please upgrade from {:} to Python 3.x'.format(sys.version_info)\nprint ('lib-dir : {:}'.format(lib_dir))\nimport datasets\nfrom scipy.io import loadmat\nfrom utils.file_utils import load_list_from_folders, load_txt_file\n\ndef load_box(mat_path, cdir):\n  mat = loadmat(mat_path)\n  mat = mat['bounding_boxes']\n  mat = mat[0]\n  assert len(mat) > 0, 'The length of this mat file should be greater than 0 vs {}'.format(len(mat))\n  all_object = []\n  for cobject in mat:\n    name = cobject[0][0][0][0]\n    bb_detector = cobject[0][0][1][0]\n    bb_ground_t = cobject[0][0][2][0]\n    image_path = osp.join(cdir, name)\n    image_path = image_path[:-4]\n    all_object.append( (image_path, bb_detector, bb_ground_t) )\n  return all_object\n\ndef load_mats(lists):\n  all_objects = []\n  for dataset in lists:\n    cobjects = load_box(dataset[0], dataset[1])\n    all_objects = all_objects + cobjects\n  return all_objects\n\ndef return_box(image_path, pts_path, all_dict, USE_BOX):\n  image_path = image_path[:-4]\n  assert image_path in all_dict, '{} not find'.format(image_path)\n  np_boxes = all_dict[ image_path ]\n  if USE_BOX == 'GTL':\n    box_str = datasets.dataset_utils.for_generate_box_str(pts_path, 68, 0)\n  elif USE_BOX == 'GTB':\n    box_str = '{:.4f} {:.4f} {:.4f} {:.4f}'.format(np_boxes[1][0], np_boxes[1][1], np_boxes[1][2], np_boxes[1][3])\n  elif USE_BOX == 'DET':\n    box_str = '{:.4f} {:.4f} {:.4f} {:.4f}'.format(np_boxes[0][0], np_boxes[0][1], np_boxes[0][2], np_boxes[0][3])\n  else:\n    assert False, 'The box indicator not find : {}'.format(USE_BOX)\n  return box_str\n\ndef load_all_300w(root_dir):\n  print ('300W Root Dir : {}'.format(root_dir))\n  mat_dir = osp.join(root_dir, 'Bounding_Boxes')\n  pairs = [(osp.join(mat_dir, 'bounding_boxes_lfpw_testset.mat'),   osp.join(root_dir, 'lfpw', 'testset')),\n           (osp.join(mat_dir, 'bounding_boxes_lfpw_trainset.mat'),  osp.join(root_dir, 'lfpw', 'trainset')),\n           (osp.join(mat_dir, 'bounding_boxes_ibug.mat'),           osp.join(root_dir, 'ibug')),\n           (osp.join(mat_dir, 'bounding_boxes_afw.mat'),            osp.join(root_dir, 'afw')),\n           (osp.join(mat_dir, 'bounding_boxes_helen_testset.mat'),  osp.join(root_dir, 'helen', 'testset')),\n           (osp.join(mat_dir, 'bounding_boxes_helen_trainset.mat'), osp.join(root_dir, 'helen', 'trainset')),]\n\n  all_datas = load_mats(pairs)\n  data_dict = {}\n  for i, cpair in enumerate(all_datas):\n    image_path = cpair[0].replace(' ', '')\n    data_dict[ image_path ] = (cpair[1], cpair[2])\n  return data_dict\n\ndef generate_300w_list(root, save_dir, box_data, SUFFIX):\n  assert osp.isdir(root), '{} is not dir'.format(root)\n  #assert osp.isdir(save_dir), '{} is not dir'.format(save_dir)\n  if not osp.isdir(save_dir): os.makedirs(save_dir)\n  train_length, common_length, challenge_length = 3148, 554, 135\n  subsets = ['afw', 'helen', 'ibug', 'lfpw']\n  dir_lists = [osp.join(root, subset) for subset in subsets]\n  imagelist, num_image = load_list_from_folders(dir_lists, ext_filter=['png', 'jpg', 'jpeg'], depth=3)\n\n  train_set, common_set, challenge_set = [], [], []\n  for image_path in imagelist:\n    name, ext = osp.splitext(image_path)\n    anno_path = name + '.pts'\n    assert osp.isfile(anno_path), 'annotation for : {} does not exist'.format(image_path)\n    if name.find('ibug') > 0:\n      challenge_set.append( (image_path, anno_path) )\n    elif name.find('afw') > 0:\n      train_set.append( (image_path, anno_path) )\n    elif name.find('helen') > 0 or name.find('lfpw') > 0:\n      if name.find('trainset') > 0:\n        train_set.append( (image_path, anno_path) )\n      elif name.find('testset') > 0:\n        common_set.append( (image_path, anno_path) )\n      else:\n        raise Exception('Unknow name : {}'.format(name))\n    else:\n      raise Exception('Unknow name : {}'.format(name))\n  assert len(train_set) == train_length, 'The length is not right for train : {} vs {}'.format(len(train_set), train_length)\n  assert len(common_set) == common_length, 'The length is not right for common : {} vs {}'.format(len(common_set), common_length)\n  assert len(challenge_set) == challenge_length, 'The length is not right for challeng : {} vs {}'.format(len(common_set), common_length)\n\n  with open(osp.join(save_dir, '300w.train.' + SUFFIX), 'w') as txtfile:\n    for cpair in train_set:\n      #box_str = datasets.dataset_utils.for_generate_box_str(cpair[1], 68, EXPAND_RATIO)\n      box_str = return_box(cpair[0], cpair[1], box_data, SUFFIX)\n      txtfile.write('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n  txtfile.close()\n\n  with open(osp.join(save_dir, '300w.test.common.' + SUFFIX), 'w') as txtfile:\n    for cpair in common_set:\n      #box_str = datasets.dataset_utils.for_generate_box_str(cpair[1], 68, EXPAND_RATIO)\n      box_str = return_box(cpair[0], cpair[1], box_data, SUFFIX)\n      txtfile.write('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n  txtfile.close()\n\n  with open(osp.join(save_dir, '300w.test.challenge.' + SUFFIX), 'w') as txtfile:\n    for cpair in challenge_set:\n      #box_str = datasets.dataset_utils.for_generate_box_str(cpair[1], 68, EXPAND_RATIO)\n      box_str = return_box(cpair[0], cpair[1], box_data, SUFFIX)\n      txtfile.write('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n  txtfile.close()\n\n  with open(osp.join(save_dir, '300w.test.full.' + SUFFIX), 'w') as txtfile:\n    fullset = common_set + challenge_set\n    for cpair in fullset:\n      #box_str = datasets.dataset_utils.for_generate_box_str(cpair[1], 68, EXPAND_RATIO)\n      box_str = return_box(cpair[0], cpair[1], box_data, SUFFIX)\n      txtfile.write('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n  txtfile.close()\n\nif __name__ == '__main__':\n  HOME_STR = 'DOME_HOME'\n  if HOME_STR not in os.environ: HOME_STR = 'HOME'\n  assert HOME_STR in os.environ, 'Doest not find the HOME dir : {}'.format(HOME_STR)\n  this_dir = osp.dirname(os.path.abspath(__file__))\n  SAVE_DIR = osp.join(this_dir, 'lists', '300W')\n  print ('This dir : {}, HOME : [{}] : {}'.format(this_dir, HOME_STR, os.environ[HOME_STR]))\n  path_300w = osp.join( os.environ[HOME_STR], 'datasets', 'landmark-datasets', '300W')\n  USE_BOXES = ['GTB', 'DET']\n  box_datas = load_all_300w(path_300w)\n\n  for USE_BOX in USE_BOXES:\n    generate_300w_list(path_300w, SAVE_DIR, box_datas, USE_BOX)\n"""
SBR/cache_data/init_path.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n""""""Set up paths.""""""            \n  \nimport sys, os  \nfrom os import path as osp\n\ndef add_path(path):            \n  if path not in sys.path:   \n    sys.path.insert(0, path)\nthis_dir = osp.dirname(osp.abspath(__file__))\n\n# Add lib to PYTHONPATH  \nlib_path = osp.abspath(osp.join(this_dir, \'..\', \'lib\'))\nadd_path(lib_path)\n'"
SBR/exps/basic_main.py,11,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom __future__ import division\n\nimport sys, time, torch, random, argparse, PIL\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom shutil import copyfile\nimport numbers, numpy as np\nlib_dir = (Path(__file__).parent / \'..\' / \'lib\').resolve()\nif str(lib_dir) not in sys.path: sys.path.insert(0, str(lib_dir))\nassert sys.version_info.major == 3, \'Please upgrade from {:} to Python 3.x\'.format(sys.version_info)\nfrom config_utils import obtain_basic_args\nfrom procedure import prepare_seed, save_checkpoint, basic_train as train, basic_eval_all as eval_all\nfrom datasets import GeneralDataset as Dataset\nfrom xvision import transforms\nfrom log_utils import Logger, AverageMeter, time_for_file, convert_secs2time, time_string\nfrom config_utils import load_configure\nfrom models import obtain_model\nfrom optimizer import obtain_optimizer\n\ndef main(args):\n  assert torch.cuda.is_available(), \'CUDA is not available.\'\n  torch.backends.cudnn.enabled   = True\n  torch.backends.cudnn.benchmark = True\n  prepare_seed(args.rand_seed)\n\n  logstr = \'seed-{:}-time-{:}\'.format(args.rand_seed, time_for_file())\n  logger = Logger(args.save_path, logstr)\n  logger.log(\'Main Function with logger : {:}\'.format(logger))\n  logger.log(\'Arguments : -------------------------------\')\n  for name, value in args._get_kwargs():\n    logger.log(\'{:16} : {:}\'.format(name, value))\n  logger.log(""Python  version : {}"".format(sys.version.replace(\'\\n\', \' \')))\n  logger.log(""Pillow  version : {}"".format(PIL.__version__))\n  logger.log(""PyTorch version : {}"".format(torch.__version__))\n  logger.log(""cuDNN   version : {}"".format(torch.backends.cudnn.version()))\n\n  # General Data Argumentation\n  mean_fill   = tuple( [int(x*255) for x in [0.485, 0.456, 0.406] ] )\n  normalize   = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                      std=[0.229, 0.224, 0.225])\n  assert args.arg_flip == False, \'The flip is : {}, rotate is {}\'.format(args.arg_flip, args.rotate_max)\n  train_transform  = [transforms.PreCrop(args.pre_crop_expand)]\n  train_transform += [transforms.TrainScale2WH((args.crop_width, args.crop_height))]\n  train_transform += [transforms.AugScale(args.scale_prob, args.scale_min, args.scale_max)]\n  #if args.arg_flip:\n  #  train_transform += [transforms.AugHorizontalFlip()]\n  if args.rotate_max:\n    train_transform += [transforms.AugRotate(args.rotate_max)]\n  train_transform += [transforms.AugCrop(args.crop_width, args.crop_height, args.crop_perturb_max, mean_fill)]\n  train_transform += [transforms.ToTensor(), normalize]\n  train_transform  = transforms.Compose( train_transform )\n\n  eval_transform  = transforms.Compose([transforms.PreCrop(args.pre_crop_expand), transforms.TrainScale2WH((args.crop_width, args.crop_height)),  transforms.ToTensor(), normalize])\n  assert (args.scale_min+args.scale_max) / 2 == args.scale_eval, \'The scale is not ok : {},{} vs {}\'.format(args.scale_min, args.scale_max, args.scale_eval)\n  \n  # Model Configure Load\n  model_config = load_configure(args.model_config, logger)\n  args.sigma   = args.sigma * args.scale_eval\n  logger.log(\'Real Sigma : {:}\'.format(args.sigma))\n\n  # Training Dataset\n  train_data   = Dataset(train_transform, args.sigma, model_config.downsample, args.heatmap_type, args.data_indicator)\n  train_data.load_list(args.train_lists, args.num_pts, True)\n  train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n\n\n  # Evaluation Dataloader\n  eval_loaders = []\n  if args.eval_vlists is not None:\n    for eval_vlist in args.eval_vlists:\n      eval_vdata = Dataset(eval_transform, args.sigma, model_config.downsample, args.heatmap_type, args.data_indicator)\n      eval_vdata.load_list(eval_vlist, args.num_pts, True)\n      eval_vloader = torch.utils.data.DataLoader(eval_vdata, batch_size=args.batch_size, shuffle=False,\n                                                 num_workers=args.workers, pin_memory=True)\n      eval_loaders.append((eval_vloader, True))\n\n  if args.eval_ilists is not None:\n    for eval_ilist in args.eval_ilists:\n      eval_idata = Dataset(eval_transform, args.sigma, model_config.downsample, args.heatmap_type, args.data_indicator)\n      eval_idata.load_list(eval_ilist, args.num_pts, True)\n      eval_iloader = torch.utils.data.DataLoader(eval_idata, batch_size=args.batch_size, shuffle=False,\n                                                 num_workers=args.workers, pin_memory=True)\n      eval_loaders.append((eval_iloader, False))\n\n  # Define network\n  logger.log(\'configure : {:}\'.format(model_config))\n  net = obtain_model(model_config, args.num_pts + 1)\n  assert model_config.downsample == net.downsample, \'downsample is not correct : {} vs {}\'.format(model_config.downsample, net.downsample)\n  logger.log(""=> network :\\n {}"".format(net))\n\n  logger.log(\'Training-data : {:}\'.format(train_data))\n  for i, eval_loader in enumerate(eval_loaders):\n    eval_loader, is_video = eval_loader\n    logger.log(\'The [{:2d}/{:2d}]-th testing-data [{:}] = {:}\'.format(i, len(eval_loaders), \'video\' if is_video else \'image\', eval_loader.dataset))\n    \n  logger.log(\'arguments : {:}\'.format(args))\n\n  opt_config = load_configure(args.opt_config, logger)\n\n  if hasattr(net, \'specify_parameter\'):\n    net_param_dict = net.specify_parameter(opt_config.LR, opt_config.Decay)\n  else:\n    net_param_dict = net.parameters()\n\n  optimizer, scheduler, criterion = obtain_optimizer(net_param_dict, opt_config, logger)\n  logger.log(\'criterion : {:}\'.format(criterion))\n  net, criterion = net.cuda(), criterion.cuda()\n  net = torch.nn.DataParallel(net)\n\n  last_info = logger.last_info()\n  if last_info.exists():\n    logger.log(""=> loading checkpoint of the last-info \'{:}\' start"".format(last_info))\n    last_info = torch.load(last_info)\n    start_epoch = last_info[\'epoch\'] + 1\n    checkpoint  = torch.load(last_info[\'last_checkpoint\'])\n    assert last_info[\'epoch\'] == checkpoint[\'epoch\'], \'Last-Info is not right {:} vs {:}\'.format(last_info, checkpoint[\'epoch\'])\n    net.load_state_dict(checkpoint[\'state_dict\'])\n    optimizer.load_state_dict(checkpoint[\'optimizer\'])\n    scheduler.load_state_dict(checkpoint[\'scheduler\'])\n    logger.log(""=> load-ok checkpoint \'{:}\' (epoch {:}) done"" .format(logger.last_info(), checkpoint[\'epoch\']))\n  else:\n    logger.log(""=> do not find the last-info file : {:}"".format(last_info))\n    start_epoch = 0\n\n\n  if args.eval_once:\n    logger.log(""=> only evaluate the model once"")\n    eval_results = eval_all(args, eval_loaders, net, criterion, \'eval-once\', logger, opt_config)\n    logger.close() ; return\n\n\n  # Main Training and Evaluation Loop\n  start_time = time.time()\n  epoch_time = AverageMeter()\n  for epoch in range(start_epoch, opt_config.epochs):\n\n    scheduler.step()\n    need_time = convert_secs2time(epoch_time.avg * (opt_config.epochs-epoch), True)\n    epoch_str = \'epoch-{:03d}-{:03d}\'.format(epoch, opt_config.epochs)\n    LRs       = scheduler.get_lr()\n    logger.log(\'\\n==>>{:s} [{:s}], [{:s}], LR : [{:.5f} ~ {:.5f}], Config : {:}\'.format(time_string(), epoch_str, need_time, min(LRs), max(LRs), opt_config))\n\n    # train for one epoch\n    train_loss, train_nme = train(args, train_loader, net, criterion, optimizer, epoch_str, logger, opt_config)\n    # log the results    \n    logger.log(\'==>>{:s} Train [{:}] Average Loss = {:.6f}, NME = {:.2f}\'.format(time_string(), epoch_str, train_loss, train_nme*100))\n\n    # remember best prec@1 and save checkpoint\n    save_path = save_checkpoint({\n          \'epoch\': epoch,\n          \'args\' : deepcopy(args),\n          \'arch\' : model_config.arch,\n          \'state_dict\': net.state_dict(),\n          \'detector\'  : net.state_dict(),\n          \'scheduler\' : scheduler.state_dict(),\n          \'optimizer\' : optimizer.state_dict(),\n          }, logger.path(\'model\') / \'{:}-{:}.pth\'.format(model_config.arch, epoch_str), logger)\n\n    last_info = save_checkpoint({\n          \'epoch\': epoch,\n          \'last_checkpoint\': save_path,\n          }, logger.last_info(), logger)\n\n    eval_results = eval_all(args, eval_loaders, net, criterion, epoch_str, logger, opt_config)\n    logger.log(\'NME Results : {:}\'.format( eval_results ))\n    \n    # measure elapsed time\n    epoch_time.update(time.time() - start_time)\n    start_time = time.time()\n\n  logger.close()\n\nif __name__ == \'__main__\':\n  args = obtain_basic_args()\n  main(args)\n'"
SBR/exps/eval.py,7,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom __future__ import division\n\nimport sys, time, torch, random, argparse, PIL\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nfrom copy import deepcopy\nfrom pathlib import Path\nimport numbers, numpy as np\nlib_dir = (Path(__file__).parent / '..' / 'lib').resolve()\nif str(lib_dir) not in sys.path: sys.path.insert(0, str(lib_dir))\nassert sys.version_info.major == 3, 'Please upgrade from {:} to Python 3.x'.format(sys.version_info)\nfrom datasets import GeneralDataset as Dataset\nfrom xvision  import transforms, draw_image_by_points\nfrom models   import obtain_model, remove_module_dict\nfrom utils    import get_model_infos\nfrom config_utils import load_configure\n\n\ndef evaluate(args):\n  if not args.cpu:\n    assert torch.cuda.is_available(), 'CUDA is not available.'\n    torch.backends.cudnn.enabled   = True\n    torch.backends.cudnn.benchmark = True\n\n  print ('The image is {:}'.format(args.image))\n  print ('The model is {:}'.format(args.model))\n  snapshot = Path(args.model)\n  assert snapshot.exists(), 'The model path {:} does not exist'\n  print ('The face bounding box is {:}'.format(args.face))\n  assert len(args.face) == 4, 'Invalid face input : {:}'.format(args.face)\n  if args.cpu: snapshot = torch.load(snapshot, map_location='cpu')\n  else       : snapshot = torch.load(snapshot)\n\n  # General Data Argumentation\n  mean_fill   = tuple( [int(x*255) for x in [0.485, 0.456, 0.406] ] )\n  normalize   = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                      std=[0.229, 0.224, 0.225])\n\n  param = snapshot['args']\n  eval_transform  = transforms.Compose([transforms.PreCrop(param.pre_crop_expand), transforms.TrainScale2WH((param.crop_width, param.crop_height)), transforms.ToTensor(), normalize])\n  model_config = load_configure(param.model_config, None)\n  dataset = Dataset(eval_transform, param.sigma, model_config.downsample, param.heatmap_type, param.data_indicator)\n  dataset.reset(param.num_pts)\n  \n  net = obtain_model(model_config, param.num_pts + 1)\n  if not args.cpu: net = net.cuda()\n  #import pdb; pdb.set_trace()\n  try:\n    weights = remove_module_dict(snapshot['detector'])\n  except:\n    weights = remove_module_dict(snapshot['state_dict'])\n  net.load_state_dict(weights)\n  print ('Prepare input data')\n  [image, _, _, _, _, _, cropped_size], meta = dataset.prepare_input(args.image, args.face)\n  # network forward\n  with torch.no_grad():\n    if args.cpu: inputs = image.unsqueeze(0)\n    else       : inputs = image.unsqueeze(0).cuda()\n    batch_heatmaps, batch_locs, batch_scos = net(inputs)\n    flops, params = get_model_infos(net, inputs.shape)\n    print ('IN-shape : {:}, FLOPs : {:} MB, Params : {:} MB'.format(list(inputs.shape), flops, params))\n  # obtain the locations on the image in the orignial size\n  cpu = torch.device('cpu')\n  np_batch_locs, np_batch_scos, cropped_size = batch_locs.to(cpu).numpy(), batch_scos.to(cpu).numpy(), cropped_size.numpy()\n  locations, scores = np_batch_locs[0,:-1,:], np.expand_dims(np_batch_scos[0,:-1], -1)\n\n  scale_h, scale_w = cropped_size[0] * 1. / inputs.size(-2) , cropped_size[1] * 1. / inputs.size(-1)\n\n  locations[:, 0], locations[:, 1] = locations[:, 0] * scale_w + cropped_size[2], locations[:, 1] * scale_h + cropped_size[3]\n  prediction = np.concatenate((locations, scores), axis=1).transpose(1,0)\n\n  print ('the coordinates for {:} facial landmarks:'.format(param.num_pts))\n  for i in range(param.num_pts):\n    point = prediction[:, i]\n    print ('the {:02d}/{:02d}-th point : ({:.1f}, {:.1f}), score = {:.2f}'.format(i, param.num_pts, float(point[0]), float(point[1]), float(point[2])))\n\n  if args.save:\n    resize = 512\n    image = draw_image_by_points(args.image, prediction, 2, (255, 0, 0), args.face, resize)\n    image.save(args.save)\n    print ('save the visualization results into {:}'.format(args.save))\n  else:\n    print ('ignore the visualization procedure')\n\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser(description='Evaluate a single image by the trained model', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument('--image',            type=str,   help='The evaluation image path.')\n  parser.add_argument('--model',            type=str,   help='The snapshot to the saved detector.')\n  parser.add_argument('--face',  nargs='+', type=float, help='The coordinate [x1,y1,x2,y2] of a face')\n  parser.add_argument('--save',             type=str,   help='The path to save the visualized results.')\n  parser.add_argument('--cpu',     action='store_true', help='Use CPU or not.')\n  args = parser.parse_args()\n  evaluate(args)\n"""
SBR/exps/lk_main.py,13,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom __future__ import division\n\nimport sys, time, torch, random, argparse, PIL\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom shutil import copyfile\nimport numbers, numpy as np\nlib_dir = (Path(__file__).parent / \'..\' / \'lib\').resolve()\nif str(lib_dir) not in sys.path: sys.path.insert(0, str(lib_dir))\nassert sys.version_info.major == 3, \'Please upgrade from {:} to Python 3.x\'.format(sys.version_info)\nfrom config_utils import obtain_lk_args as obtain_args\nfrom procedure import prepare_seed, save_checkpoint, lk_train as train, basic_eval_all as eval_all\nfrom datasets import VideoDataset as VDataset, GeneralDataset as IDataset\nfrom xvision import transforms\nfrom log_utils import Logger, AverageMeter, time_for_file, convert_secs2time, time_string\nfrom config_utils import load_configure\nfrom models import obtain_LK as obtain_model, remove_module_dict\nfrom optimizer import obtain_optimizer\n\ndef main(args):\n  assert torch.cuda.is_available(), \'CUDA is not available.\'\n  torch.backends.cudnn.enabled   = True\n  torch.backends.cudnn.benchmark = True\n  prepare_seed(args.rand_seed)\n\n  logstr = \'seed-{:}-time-{:}\'.format(args.rand_seed, time_for_file())\n  logger = Logger(args.save_path, logstr)\n  logger.log(\'Main Function with logger : {:}\'.format(logger))\n  logger.log(\'Arguments : -------------------------------\')\n  for name, value in args._get_kwargs():\n    logger.log(\'{:16} : {:}\'.format(name, value))\n  logger.log(""Python  version : {}"".format(sys.version.replace(\'\\n\', \' \')))\n  logger.log(""Pillow  version : {}"".format(PIL.__version__))\n  logger.log(""PyTorch version : {}"".format(torch.__version__))\n  logger.log(""cuDNN   version : {}"".format(torch.backends.cudnn.version()))\n\n  # General Data Argumentation\n  mean_fill   = tuple( [int(x*255) for x in [0.485, 0.456, 0.406] ] )\n  normalize   = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                      std=[0.229, 0.224, 0.225])\n  assert args.arg_flip == False, \'The flip is : {}, rotate is {}\'.format(args.arg_flip, args.rotate_max)\n  train_transform  = [transforms.PreCrop(args.pre_crop_expand)]\n  train_transform += [transforms.TrainScale2WH((args.crop_width, args.crop_height))]\n  train_transform += [transforms.AugScale(args.scale_prob, args.scale_min, args.scale_max)]\n  #if args.arg_flip:\n  #  train_transform += [transforms.AugHorizontalFlip()]\n  if args.rotate_max:\n    train_transform += [transforms.AugRotate(args.rotate_max)]\n  train_transform += [transforms.AugCrop(args.crop_width, args.crop_height, args.crop_perturb_max, mean_fill)]\n  train_transform += [transforms.ToTensor(), normalize]\n  train_transform  = transforms.Compose( train_transform )\n\n  eval_transform  = transforms.Compose([transforms.PreCrop(args.pre_crop_expand), transforms.TrainScale2WH((args.crop_width, args.crop_height)),  transforms.ToTensor(), normalize])\n  assert (args.scale_min+args.scale_max) / 2 == args.scale_eval, \'The scale is not ok : {},{} vs {}\'.format(args.scale_min, args.scale_max, args.scale_eval)\n  \n  # Model Configure Load\n  model_config = load_configure(args.model_config, logger)\n  args.sigma   = args.sigma * args.scale_eval\n  logger.log(\'Real Sigma : {:}\'.format(args.sigma))\n\n  # Training Dataset\n  train_data   = VDataset(train_transform, args.sigma, model_config.downsample, args.heatmap_type, args.data_indicator, args.video_parser)\n  train_data.load_list(args.train_lists, args.num_pts, True)\n  train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n  \n\n  # Evaluation Dataloader\n  eval_loaders = []\n  if args.eval_vlists is not None:\n    for eval_vlist in args.eval_vlists:\n      eval_vdata = IDataset(eval_transform, args.sigma, model_config.downsample, args.heatmap_type, args.data_indicator)\n      eval_vdata.load_list(eval_vlist, args.num_pts, True)\n      eval_vloader = torch.utils.data.DataLoader(eval_vdata, batch_size=args.batch_size, shuffle=False,\n                                                 num_workers=args.workers, pin_memory=True)\n      eval_loaders.append((eval_vloader, True))\n\n  if args.eval_ilists is not None:\n    for eval_ilist in args.eval_ilists:\n      eval_idata = IDataset(eval_transform, args.sigma, model_config.downsample, args.heatmap_type, args.data_indicator)\n      eval_idata.load_list(eval_ilist, args.num_pts, True)\n      eval_iloader = torch.utils.data.DataLoader(eval_idata, batch_size=args.batch_size, shuffle=False,\n                                                 num_workers=args.workers, pin_memory=True)\n      eval_loaders.append((eval_iloader, False))\n\n  # Define network\n  lk_config = load_configure(args.lk_config, logger)\n  logger.log(\'model configure : {:}\'.format(model_config))\n  logger.log(\'LK configure : {:}\'.format(lk_config))\n  net = obtain_model(model_config, lk_config, args.num_pts + 1)\n  assert model_config.downsample == net.downsample, \'downsample is not correct : {} vs {}\'.format(model_config.downsample, net.downsample)\n  logger.log(""=> network :\\n {}"".format(net))\n\n  logger.log(\'Training-data : {:}\'.format(train_data))\n  for i, eval_loader in enumerate(eval_loaders):\n    eval_loader, is_video = eval_loader\n    logger.log(\'The [{:2d}/{:2d}]-th testing-data [{:}] = {:}\'.format(i, len(eval_loaders), \'video\' if is_video else \'image\', eval_loader.dataset))\n    \n  logger.log(\'arguments : {:}\'.format(args))\n\n  opt_config = load_configure(args.opt_config, logger)\n\n  if hasattr(net, \'specify_parameter\'):\n    net_param_dict = net.specify_parameter(opt_config.LR, opt_config.Decay)\n  else:\n    net_param_dict = net.parameters()\n\n  optimizer, scheduler, criterion = obtain_optimizer(net_param_dict, opt_config, logger)\n  logger.log(\'criterion : {:}\'.format(criterion))\n  net, criterion = net.cuda(), criterion.cuda()\n  net = torch.nn.DataParallel(net)\n\n  last_info = logger.last_info()\n  if last_info.exists():\n    logger.log(""=> loading checkpoint of the last-info \'{:}\' start"".format(last_info))\n    last_info = torch.load(last_info)\n    start_epoch = last_info[\'epoch\'] + 1\n    checkpoint  = torch.load(last_info[\'last_checkpoint\'])\n    assert last_info[\'epoch\'] == checkpoint[\'epoch\'], \'Last-Info is not right {:} vs {:}\'.format(last_info, checkpoint[\'epoch\'])\n    net.load_state_dict(checkpoint[\'state_dict\'])\n    optimizer.load_state_dict(checkpoint[\'optimizer\'])\n    scheduler.load_state_dict(checkpoint[\'scheduler\'])\n    logger.log(""=> load-ok checkpoint \'{:}\' (epoch {:}) done"" .format(logger.last_info(), checkpoint[\'epoch\']))\n  elif args.init_model is not None:\n    init_model = Path(args.init_model)\n    assert init_model.exists(), \'init-model {:} does not exist\'.format(init_model)\n    checkpoint = torch.load(init_model)\n    checkpoint = remove_module_dict(checkpoint[\'state_dict\'], True)\n    net.module.detector.load_state_dict( checkpoint )\n    logger.log(""=> initialize the detector : {:}"".format(init_model))\n    start_epoch = 0\n  else:\n    logger.log(""=> do not find the last-info file : {:}"".format(last_info))\n    start_epoch = 0\n\n  detector = torch.nn.DataParallel(net.module.detector)\n\n  eval_results = eval_all(args, eval_loaders, detector, criterion, \'start-eval\', logger, opt_config)\n  if args.eval_once:\n    logger.log(""=> only evaluate the model once"")\n    logger.close() ; return\n\n  # Main Training and Evaluation Loop\n  start_time = time.time()\n  epoch_time = AverageMeter()\n  for epoch in range(start_epoch, opt_config.epochs):\n\n    scheduler.step()\n    need_time = convert_secs2time(epoch_time.avg * (opt_config.epochs-epoch), True)\n    epoch_str = \'epoch-{:03d}-{:03d}\'.format(epoch, opt_config.epochs)\n    LRs       = scheduler.get_lr()\n    logger.log(\'\\n==>>{:s} [{:s}], [{:s}], LR : [{:.5f} ~ {:.5f}], Config : {:}\'.format(time_string(), epoch_str, need_time, min(LRs), max(LRs), opt_config))\n\n    # train for one epoch\n    train_loss = train(args, train_loader, net, criterion, optimizer, epoch_str, logger, opt_config, lk_config, epoch>=lk_config.start)\n    # log the results    \n    logger.log(\'==>>{:s} Train [{:}] Average Loss = {:.6f}\'.format(time_string(), epoch_str, train_loss))\n\n    # remember best prec@1 and save checkpoint\n    save_path = save_checkpoint({\n          \'epoch\': epoch,\n          \'args\' : deepcopy(args),\n          \'arch\' : model_config.arch,\n          \'state_dict\': net.state_dict(),\n          \'detector\'  : detector.state_dict(),\n          \'scheduler\' : scheduler.state_dict(),\n          \'optimizer\' : optimizer.state_dict(),\n          }, logger.path(\'model\') / \'{:}-{:}.pth\'.format(model_config.arch, epoch_str), logger)\n\n    last_info = save_checkpoint({\n          \'epoch\': epoch,\n          \'last_checkpoint\': save_path,\n          }, logger.last_info(), logger)\n\n    eval_results = eval_all(args, eval_loaders, detector, criterion, epoch_str, logger, opt_config)\n\n    # measure elapsed time\n    epoch_time.update(time.time() - start_time)\n    start_time = time.time()\n\n  logger.close()\n\nif __name__ == \'__main__\':\n  args = obtain_args()\n  main(args)\n'"
SBR/exps/vis.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom __future__ import division\n\nimport os, sys, time, random, argparse, PIL\nfrom os import path as osp\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nfrom copy import deepcopy\nfrom pathlib import Path\nimport numbers, numpy as np\nlib_dir = (Path(__file__).parent / '..' / 'lib').resolve()\nif str(lib_dir) not in sys.path: sys.path.insert(0, str(lib_dir))\nassert sys.version_info.major == 3, 'Please upgrade from {:} to Python 3.x'.format(sys.version_info)\nfrom xvision import draw_image_by_points\nfrom xvision import Eval_Meta\n\ndef visualize(args):\n\n  print ('The result file is {:}'.format(args.meta))\n  print ('The save path is {:}'.format(args.save))\n  meta = Path(args.meta)\n  save = Path(args.save)\n  assert meta.exists(), 'The model path {:} does not exist'\n  xmeta = Eval_Meta()\n  xmeta.load(meta)\n  print ('this meta file has {:} predictions'.format(len(xmeta)))\n  if not save.exists(): os.makedirs( args.save )\n  for i in range(len(xmeta)):\n    image, prediction = xmeta.image_lists[i], xmeta.predictions[i]\n    name = osp.basename(image)\n    image = draw_image_by_points(image, prediction, 2, (255, 0, 0), False, False)\n    path = save / name\n    image.save(path)\n    print ('{:03d}-th image is saved into {:}'.format(i, path))\n\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser(description='visualize the results on a single ', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument('--meta',            type=str,   help='The evaluation image path.')\n  parser.add_argument('--save',            type=str,   help='The path to save the visualized results.')\n  args = parser.parse_args()\n  visualize(args)\n"""
TS3/models/__init__.py,0,b'from .student_cpm import cpm_vgg16\nfrom .student_hg  import hourglass\nfrom .teacher     import TeacherNet\nfrom .model_utils import count_network_param\nfrom .flop_benchmark import get_model_infos\n'
TS3/models/basic_batch.py,12,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numbers, math\nimport numpy as np\nimport models.model_utils as MU\n\ndef find_tensor_peak_batch(heatmap, radius, downsample, threshold = 0.000001):\n  assert heatmap.dim() == 3, 'The dimension of the heatmap is wrong : {}'.format(heatmap.size())\n  assert radius > 0 and isinstance(radius, numbers.Number), 'The radius is not ok : {}'.format(radius)\n  num_pts, H, W = heatmap.size(0), heatmap.size(1), heatmap.size(2)\n  assert W > 1 and H > 1, 'To avoid the normalization function divide zero'\n  # find the approximate location:\n  score, index = torch.max(heatmap.view(num_pts, -1), 1)\n  index_w = (index % W).float()\n  index_h = (index / W).float()\n  \n  def normalize(x, L):\n    return -1. + 2. * x.data / (L-1)\n  boxes = [index_w - radius, index_h - radius, index_w + radius, index_h + radius]\n  boxes[0] = normalize(boxes[0], W)\n  boxes[1] = normalize(boxes[1], H)\n  boxes[2] = normalize(boxes[2], W)\n  boxes[3] = normalize(boxes[3], H)\n  #affine_parameter = [(boxes[2]-boxes[0])/2, boxes[0]*0, (boxes[2]+boxes[0])/2,\n  #                   boxes[0]*0, (boxes[3]-boxes[1])/2, (boxes[3]+boxes[1])/2]\n  #theta = torch.stack(affine_parameter, 1).view(num_pts, 2, 3)\n\n  affine_parameter = torch.zeros((num_pts, 2, 3))\n  affine_parameter[:,0,0] = (boxes[2]-boxes[0])/2\n  affine_parameter[:,0,2] = (boxes[2]+boxes[0])/2\n  affine_parameter[:,1,1] = (boxes[3]-boxes[1])/2\n  affine_parameter[:,1,2] = (boxes[3]+boxes[1])/2\n  # extract the sub-region heatmap\n  theta = MU.np2variable(affine_parameter, heatmap.is_cuda, False)\n  grid_size = torch.Size([num_pts, 1, radius*2+1, radius*2+1])\n  grid = F.affine_grid(theta, grid_size)\n  sub_feature = F.grid_sample(heatmap.unsqueeze(1), grid).squeeze(1)\n  sub_feature = F.threshold(sub_feature, threshold, np.finfo(float).eps)\n\n  X = MU.np2variable(torch.arange(-radius, radius+1), heatmap.is_cuda, False).view(1, 1, radius*2+1)\n  Y = MU.np2variable(torch.arange(-radius, radius+1), heatmap.is_cuda, False).view(1, radius*2+1, 1)\n  \n  sum_region = torch.sum(sub_feature.view(num_pts,-1),1)\n  x = torch.sum((sub_feature*X).view(num_pts,-1),1) / sum_region + index_w\n  y = torch.sum((sub_feature*Y).view(num_pts,-1),1) / sum_region + index_h\n     \n  x = x * downsample + downsample / 2.0 - 0.5\n  y = y * downsample + downsample / 2.0 - 0.5\n  return torch.stack([x, y],1), score\n"""
TS3/models/flop_benchmark.py,15,"b'##################################################\n# Copyright (c) Xuanyi Dong [GitHub D-X-Y], 2019 #\n##################################################\n# modified from https://github.com/warmspringwinds/pytorch-segmentation-detection/blob/master/pytorch_segmentation_detection/utils/flops_benchmark.py\nimport copy, torch\nimport torch.nn as nn\nimport numpy as np\n\n\ndef count_parameters_in_MB(model):\n  if isinstance(model, nn.Module):\n    return np.sum(np.prod(v.size()) for v in model.parameters())/1e6\n  else:\n    return np.sum(np.prod(v.size()) for v in model)/1e6\n\n\ndef get_model_infos(model, shape):\n  #model = copy.deepcopy( model )\n\n  model = add_flops_counting_methods(model)\n  #model = model.cuda()\n  model.eval()\n\n  #cache_inputs = torch.zeros(*shape).cuda()\n  cache_inputs = torch.zeros(*shape)\n  if next(model.parameters()).is_cuda: cache_inputs = cache_inputs.cuda()\n  #print_log(\'In the calculating function : cache input size : {:}\'.format(cache_inputs.size()), log)\n  with torch.no_grad():\n    _____ = model(cache_inputs)\n  FLOPs = compute_average_flops_cost( model ) / 1e6\n  Param = count_parameters_in_MB(model)\n\n  if hasattr(model, \'auxiliary_param\'):\n    aux_params = count_parameters_in_MB(model.auxiliary_param()) \n    print (\'The auxiliary params of this model is : {:}\'.format(aux_params))\n    print (\'We remove the auxiliary params from the total params ({:}) when counting\'.format(Param))\n    Param = Param - aux_params\n  \n  #print_log(\'FLOPs : {:} MB\'.format(FLOPs), log)\n  torch.cuda.empty_cache()\n  model.apply( remove_hook_function )\n  return FLOPs, Param\n\n\n# ---- Public functions\ndef add_flops_counting_methods( model ):\n  model.__batch_counter__ = 0\n  add_batch_counter_hook_function( model )\n  model.apply( add_flops_counter_variable_or_reset )\n  model.apply( add_flops_counter_hook_function )\n  return model\n\n\n\ndef compute_average_flops_cost(model):\n  """"""\n  A method that will be available after add_flops_counting_methods() is called on a desired net object.\n  Returns current mean flops consumption per image.\n  """"""\n  batches_count = model.__batch_counter__\n  flops_sum = 0\n  #or isinstance(module, torch.nn.AvgPool2d) or isinstance(module, torch.nn.MaxPool2d) \\\n  for module in model.modules():\n    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear) \\\n      or isinstance(module, torch.nn.Conv1d) \\\n      or hasattr(module, \'calculate_flop_self\'):\n      flops_sum += module.__flops__\n  return flops_sum / batches_count\n\n\n# ---- Internal functions\ndef pool_flops_counter_hook(pool_module, inputs, output):\n  batch_size = inputs[0].size(0)\n  kernel_size = pool_module.kernel_size\n  out_C, output_height, output_width = output.shape[1:]\n  assert out_C == inputs[0].size(1), \'{:} vs. {:}\'.format(out_C, inputs[0].size())\n\n  overall_flops = batch_size * out_C * output_height * output_width * kernel_size * kernel_size\n  pool_module.__flops__ += overall_flops\n\n\ndef self_calculate_flops_counter_hook(self_module, inputs, output):\n  overall_flops = self_module.calculate_flop_self(inputs[0].shape, output.shape)\n  self_module.__flops__ += overall_flops\n\n\ndef fc_flops_counter_hook(fc_module, inputs, output):\n  batch_size = inputs[0].size(0)\n  xin, xout = fc_module.in_features, fc_module.out_features\n  assert xin == inputs[0].size(1) and xout == output.size(1), \'IO=({:}, {:})\'.format(xin, xout)\n  overall_flops = batch_size * xin * xout\n  if fc_module.bias is not None:\n    overall_flops += batch_size * xout\n  fc_module.__flops__ += overall_flops\n\n\ndef conv1d_flops_counter_hook(conv_module, inputs, outputs):\n  batch_size   = inputs[0].size(0)\n  outL         = outputs.shape[-1]\n  [kernel]     = conv_module.kernel_size\n  in_channels  = conv_module.in_channels\n  out_channels = conv_module.out_channels\n  groups       = conv_module.groups\n  conv_per_position_flops = kernel * in_channels * out_channels / groups\n  \n  active_elements_count = batch_size * outL \n  overall_flops = conv_per_position_flops * active_elements_count\n\n  if conv_module.bias is not None:\n    overall_flops += out_channels * active_elements_count\n  conv_module.__flops__ += overall_flops\n\n\ndef conv2d_flops_counter_hook(conv_module, inputs, output):\n  batch_size = inputs[0].size(0)\n  output_height, output_width = output.shape[2:]\n  \n  kernel_height, kernel_width = conv_module.kernel_size\n  in_channels  = conv_module.in_channels\n  out_channels = conv_module.out_channels\n  groups       = conv_module.groups\n  conv_per_position_flops = kernel_height * kernel_width * in_channels * out_channels / groups\n  \n  active_elements_count = batch_size * output_height * output_width\n  overall_flops = conv_per_position_flops * active_elements_count\n    \n  if conv_module.bias is not None:\n    overall_flops += out_channels * active_elements_count\n  conv_module.__flops__ += overall_flops\n\n  \ndef batch_counter_hook(module, inputs, output):\n  # Can have multiple inputs, getting the first one\n  inputs = inputs[0]\n  batch_size = inputs.shape[0]\n  module.__batch_counter__ += batch_size\n\n\ndef add_batch_counter_hook_function(module):\n  if not hasattr(module, \'__batch_counter_handle__\'):\n    handle = module.register_forward_hook(batch_counter_hook)\n    module.__batch_counter_handle__ = handle\n\n  \ndef add_flops_counter_variable_or_reset(module):\n  if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear) \\\n    or isinstance(module, torch.nn.Conv1d) \\\n    or isinstance(module, torch.nn.AvgPool2d) or isinstance(module, torch.nn.MaxPool2d) \\\n    or hasattr(module, \'calculate_flop_self\'):\n    module.__flops__ = 0\n\n\ndef add_flops_counter_hook_function(module):\n  if isinstance(module, torch.nn.Conv2d):\n    if not hasattr(module, \'__flops_handle__\'):\n      handle = module.register_forward_hook(conv2d_flops_counter_hook)\n      module.__flops_handle__ = handle\n  elif isinstance(module, torch.nn.Conv1d):\n    if not hasattr(module, \'__flops_handle__\'):\n      handle = module.register_forward_hook(conv1d_flops_counter_hook)\n      module.__flops_handle__ = handle\n  elif isinstance(module, torch.nn.Linear):\n    if not hasattr(module, \'__flops_handle__\'):\n      handle = module.register_forward_hook(fc_flops_counter_hook)\n      module.__flops_handle__ = handle\n  elif isinstance(module, torch.nn.AvgPool2d) or isinstance(module, torch.nn.MaxPool2d):\n    if not hasattr(module, \'__flops_handle__\'):\n      handle = module.register_forward_hook(pool_flops_counter_hook)\n      module.__flops_handle__ = handle\n  elif hasattr(module, \'calculate_flop_self\'): # self-defined module\n    if not hasattr(module, \'__flops_handle__\'):\n      handle = module.register_forward_hook(self_calculate_flops_counter_hook)\n      module.__flops_handle__ = handle\n\n\ndef remove_hook_function(module):\n  hookers = [\'__batch_counter_handle__\', \'__flops_handle__\']\n  for hooker in hookers:\n    if hasattr(module, hooker):\n      handle = getattr(module, hooker)\n      handle.remove()\n  keys = [\'__flops__\', \'__batch_counter__\', \'__flops__\'] + hookers\n  for ckey in keys:\n    if hasattr(module, ckey): delattr(module, ckey)\n'"
TS3/models/initialization.py,2,"b""import torch\nimport torch.nn as nn\nfrom torch.nn import init\n\ndef weights_init_cpm(m):\n  classname = m.__class__.__name__\n  # print(classname)\n  if classname.find('Conv') != -1:\n    m.weight.data.normal_(0, 0.01)\n    if m.bias is not None: m.bias.data.zero_()\n  elif classname.find('BatchNorm2d') != -1:\n    m.weight.data.fill_(1)\n    m.bias.data.zero_()\n\ndef weights_init_normal(m):\n  classname = m.__class__.__name__\n  # print(classname)\n  if classname.find('Conv') != -1:\n    init.uniform(m.weight.data, 0.0, 0.02)\n  elif classname.find('Linear') != -1:\n    init.uniform(m.weight.data, 0.0, 0.02)\n  elif classname.find('BatchNorm2d') != -1:\n    init.uniform(m.weight.data, 1.0, 0.02)\n    init.constant(m.bias.data, 0.0)\n\n\ndef weights_init_xavier(m):\n  classname = m.__class__.__name__\n  # print(classname)\n  if classname.find('Conv') != -1:\n    init.xavier_normal(m.weight.data, gain=1)\n  elif classname.find('Linear') != -1:\n    init.xavier_normal(m.weight.data, gain=1)\n  elif classname.find('BatchNorm2d') != -1:\n    init.uniform(m.weight.data, 1.0, 0.02)\n    init.constant(m.bias.data, 0.0)\n\n\ndef weights_init_kaiming(m):\n  classname = m.__class__.__name__\n  # print(classname)\n  if classname.find('Conv') != -1:\n    init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n  elif classname.find('Linear') != -1:\n    init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n  elif classname.find('BatchNorm2d') != -1:\n    init.uniform(m.weight.data, 1.0, 0.02)\n    init.constant(m.bias.data, 0.0)\n\n\ndef weights_init_orthogonal(m):\n  classname = m.__class__.__name__\n  print(classname)\n  if classname.find('Conv') != -1:\n    init.orthogonal(m.weight.data, gain=1)\n  elif classname.find('Linear') != -1:\n    init.orthogonal(m.weight.data, gain=1)\n  elif classname.find('BatchNorm2d') != -1:\n    init.uniform(m.weight.data, 1.0, 0.02)\n    init.constant(m.bias.data, 0.0)\n"""
TS3/models/layer_utils.py,2,"b""import time, math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Residual(nn.Module):\n  def __init__(self, numIn, numOut):\n    super(Residual, self).__init__()\n    self.numIn = numIn\n    self.numOut = numOut\n    middle = self.numOut // 2\n\n    self.conv_A = nn.Sequential(\n                    nn.BatchNorm2d(numIn), nn.ReLU(inplace=True),\n                    nn.Conv2d(numIn, middle, kernel_size=1, dilation=1, padding=0, bias=True))\n\n    self.conv_B = nn.Sequential(\n                    nn.BatchNorm2d(middle), nn.ReLU(inplace=True),\n                    nn.Conv2d(middle, middle, kernel_size=3, dilation=1, padding=1, bias=True))\n\n    self.conv_C = nn.Sequential(\n                    nn.BatchNorm2d(middle), nn.ReLU(inplace=True),\n                    nn.Conv2d(middle, numOut, kernel_size=1, dilation=1, padding=0, bias=True))\n\n    if self.numIn != self.numOut:\n      self.branch = nn.Sequential(\n                      nn.BatchNorm2d(self.numIn), nn.ReLU(inplace=True),\n                      nn.Conv2d(self.numIn, self.numOut, kernel_size=1, dilation=1, padding=0, bias=True))\n\n  def forward(self, x):\n    residual = x\n    \n    main = self.conv_A(x)\n    main = self.conv_B(main)\n    main = self.conv_C(main)\n    if hasattr(self, 'branch'):\n      residual = self.branch( residual )\n  \n    return main + residual\n"""
TS3/models/model_utils.py,7,"b""from scipy.ndimage.interpolation import zoom\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport copy, numbers, numpy as np\n\ndef np2variable(x, is_cuda=True, requires_grad=True, dtype=torch.FloatTensor):\n  if isinstance(x, np.ndarray):\n    v = torch.autograd.Variable(torch.from_numpy(x).type(dtype), requires_grad=requires_grad)\n  elif isinstance(x, torch.FloatTensor):\n    v = torch.autograd.Variable(x.type(dtype), requires_grad=requires_grad)\n  else:\n    raise Exception('Do not know this type : {}'.format( type(x) ))\n\n  if is_cuda: return v.cuda()\n  else:       return v\n\ndef variable2np(x):\n  if x.is_cuda:\n    x = x.cpu()\n  if isinstance(x, torch.autograd.Variable):\n    return x.data.numpy()\n  else:\n    return x.numpy()\n\ndef get_parameters(model, bias):\n  for m in model.modules():\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n      if bias:\n        yield m.bias\n      else:\n        yield m.weight\n    elif isinstance(m, nn.BatchNorm2d):\n      if bias:\n        yield m.bias\n      else:\n        yield m.weight\n\ndef load_weight_from_dict(model, weight_state_dict, param_pair=None, remove_prefix=True):\n  if remove_prefix: weight_state_dict = remove_module_dict(weight_state_dict)\n  all_parameter = model.state_dict()\n  all_weights   = []\n  finetuned_layer, random_initial_layer = [], []\n  for key, value in all_parameter.items():\n    if param_pair is not None and key in param_pair:\n      all_weights.append((key, weight_state_dict[ param_pair[key] ]))\n    elif key in weight_state_dict:\n      all_weights.append((key, weight_state_dict[key]))\n      finetuned_layer.append(key)\n    else:\n      all_weights.append((key, value))\n      random_initial_layer.append(key)\n  print ('==>[load_model] finetuned layers : {}'.format(finetuned_layer))\n  print ('==>[load_model] keeped layers : {}'.format(random_initial_layer))\n  all_weights = OrderedDict(all_weights)\n  model.load_state_dict(all_weights)\n\ndef remove_module_dict(state_dict):\n  new_state_dict = OrderedDict()\n  for k, v in state_dict.items():\n    name = k[7:] # remove `module.`\n    new_state_dict[name] = v\n  return new_state_dict\n\ndef count_network_param(net):\n  num_params = 0\n  for param in net.parameters():\n    num_params += param.numel()\n  return num_params\n"""
TS3/models/student_cpm.py,5,"b""from __future__ import division\nimport time, math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom collections import OrderedDict\nfrom copy import deepcopy\nfrom .model_utils import get_parameters, load_weight_from_dict\nfrom .basic_batch import find_tensor_peak_batch\nfrom .initialization import weights_init_cpm\n\nclass VGG16_base(nn.Module):\n  def __init__(self, model_config, pts_num):\n    super(VGG16_base, self).__init__()\n\n    self.config = deepcopy(model_config)\n    self.downsample = 8\n    self.pts_num = pts_num\n\n    self.features = nn.Sequential(\n          nn.Conv2d(  3,  64, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d( 64,  64, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.MaxPool2d(kernel_size=2, stride=2),\n          nn.Conv2d( 64, 128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.MaxPool2d(kernel_size=2, stride=2),\n          nn.Conv2d(128, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(256, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(256, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.MaxPool2d(kernel_size=2, stride=2),\n          nn.Conv2d(256, 512, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(512, 512, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(512, 512, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True))\n  \n\n    self.CPM_feature = nn.Sequential(\n          nn.Conv2d(512, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), #CPM_1\n          nn.Conv2d(256, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True)) #CPM_2\n\n    assert self.config['stages'] >= 1, 'stages of cpm must >= 1'\n    stage1 = nn.Sequential(\n          nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 512, kernel_size=1, padding=0), nn.ReLU(inplace=True),\n          nn.Conv2d(512, pts_num, kernel_size=1, padding=0))\n    stages = [stage1]\n    for i in range(1, self.config['stages']):\n      stagex = nn.Sequential(\n          nn.Conv2d(128+pts_num, 128, kernel_size=7, dilation=1, padding=3), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=7, dilation=1, padding=3), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=7, dilation=1, padding=3), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=1, padding=0), nn.ReLU(inplace=True),\n          nn.Conv2d(128,     pts_num, kernel_size=1, padding=0))\n      stages.append( stagex )\n    self.stages = nn.ModuleList(stages)\n  \n\n  def specify_parameter(self, base_lr, base_weight_decay):\n    params_dict = [ {'params': get_parameters(self.features,   bias=False), 'lr': base_lr  , 'weight_decay': base_weight_decay},\n                    {'params': get_parameters(self.features,   bias=True ), 'lr': base_lr*2, 'weight_decay': 0},\n                    {'params': get_parameters(self.CPM_feature, bias=False), 'lr': base_lr  , 'weight_decay': base_weight_decay},\n                    {'params': get_parameters(self.CPM_feature, bias=True ), 'lr': base_lr*2, 'weight_decay': 0},\n                  ]\n    for stage in self.stages:\n      params_dict.append( {'params': get_parameters(stage, bias=False), 'lr': base_lr*4, 'weight_decay': base_weight_decay} )\n      params_dict.append( {'params': get_parameters(stage, bias=True ), 'lr': base_lr*8, 'weight_decay': 0} )\n    return params_dict\n\n  # return : cpm-stages, locations\n  def forward(self, inputs):\n    assert inputs.dim() == 4, 'This model accepts 4 dimension input tensor: {}'.format(inputs.size())\n    batch_cpms = []\n\n    feature  = self.features(inputs)\n    xfeature = self.CPM_feature(feature)\n    for i in range(self.config['stages']):\n      if i == 0: cpm = self.stages[i]( xfeature )\n      else:      cpm = self.stages[i]( torch.cat([xfeature, batch_cpms[i-1]], 1) )\n      batch_cpms.append( cpm )\n\n    return batch_cpms\n\n# use vgg16 conv1_1 to conv4_4 as feature extracation        \nmodel_urls = 'https://download.pytorch.org/models/vgg16-397923af.pth'\n\ndef cpm_vgg16(model_config, pts):\n  \n  print ('Initialize cpm-vgg16 with configure : {}'.format(model_config))\n  model = VGG16_base(model_config, pts)\n  model.apply(weights_init_cpm)\n\n  if model_config['pretrained']:\n    print ('vgg16_base use pre-trained model')\n    weights = model_zoo.load_url(model_urls)\n    load_weight_from_dict(model, weights, None, False)\n  return model\n"""
TS3/models/student_hg.py,3,"b""from __future__ import division\nimport time, math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom collections import OrderedDict\nfrom .model_utils import get_parameters, load_weight_from_dict\nfrom .basic_batch import find_tensor_peak_batch\nfrom .initialization import weights_init_cpm\nfrom .layer_utils import Residual\n\nclass Hourglass(nn.Module):\n  def __init__(self, n, nModules, nFeats):\n    super(Hourglass, self).__init__()\n    self.n = n\n    self.nModules = nModules\n    self.nFeats = nFeats\n    \n    _up1_, _low1_, _low2_, _low3_ = [], [], [], []\n    for j in range(self.nModules):\n      _up1_.append(Residual(self.nFeats, self.nFeats))\n    self.low1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n    for j in range(self.nModules):\n      _low1_.append(Residual(self.nFeats, self.nFeats))\n    \n    if self.n > 1:\n      self.low2 = Hourglass(n - 1, self.nModules, self.nFeats)\n    else:\n      for j in range(self.nModules):\n        _low2_.append(Residual(self.nFeats, self.nFeats))\n      self.low2_ = nn.ModuleList(_low2_)\n    \n    for j in range(self.nModules):\n      _low3_.append(Residual(self.nFeats, self.nFeats))\n    \n    self.up1_ = nn.ModuleList(_up1_)\n    self.low1_ = nn.ModuleList(_low1_)\n    self.low3_ = nn.ModuleList(_low3_)\n    \n    self.up2 = nn.Upsample(scale_factor = 2)\n    \n  def forward(self, x):\n    up1 = x\n    for j in range(self.nModules):\n      up1 = self.up1_[j](up1)\n    \n    low1 = self.low1(x)\n    for j in range(self.nModules):\n      low1 = self.low1_[j](low1)\n    \n    if self.n > 1:\n      low2 = self.low2(low1)\n    else:\n      low2 = low1\n      for j in range(self.nModules):\n        low2 = self.low2_[j](low2)\n    \n    low3 = low2\n    for j in range(self.nModules):\n      low3 = self.low3_[j](low3)\n    up2 = self.up2(low3)\n    \n    return up1 + up2\n\n\nclass HourglassNet(nn.Module):\n  def __init__(self, nStack, nModules, nFeats, nJoints):\n    super(HourglassNet, self).__init__()\n    self.downsample = 4\n    self.pts_num = nJoints\n    self.nStack = nStack\n    self.nModules = nModules\n    self.nFeats = nFeats\n    self.conv = nn.Sequential(\n                  nn.Conv2d(3, 64, bias = True, kernel_size = 7, stride = 2, padding = 3), \n                  nn.BatchNorm2d(64), nn.ReLU(inplace = True))\n    self.ress = nn.Sequential(\n                  Residual(64, 128),\n                  nn.MaxPool2d(kernel_size = 2, stride = 2),\n                  Residual(128, 128), Residual(128, self.nFeats))\n    \n    _hourglass, _Residual, _lin_, _tmpOut, _ll_, _tmpOut_, _reg_ = [], [], [], [], [], [], []\n    for i in range(self.nStack):\n      _hourglass.append(Hourglass(4, self.nModules, self.nFeats))\n      for j in range(self.nModules):\n        _Residual.append(Residual(self.nFeats, self.nFeats))\n      lin = nn.Sequential(nn.Conv2d(self.nFeats, self.nFeats, bias = True, kernel_size = 1, stride = 1), \n                          nn.BatchNorm2d(self.nFeats), nn.ReLU(inplace = True))\n      _lin_.append(lin)\n      _tmpOut.append(nn.Conv2d(self.nFeats, nJoints, bias = True, kernel_size = 1, stride = 1))\n      if i < self.nStack - 1:\n        _ll_.append(nn.Conv2d(self.nFeats, self.nFeats, bias = True, kernel_size = 1, stride = 1))\n        _tmpOut_.append(nn.Conv2d(nJoints, self.nFeats, bias = True, kernel_size = 1, stride = 1))\n        \n    self.hourglass = nn.ModuleList(_hourglass)\n    self.Residual = nn.ModuleList(_Residual)\n    self.lin_ = nn.ModuleList(_lin_)\n    self.tmpOut = nn.ModuleList(_tmpOut)\n    self.ll_ = nn.ModuleList(_ll_)\n    self.tmpOut_ = nn.ModuleList(_tmpOut_)\n  \n  def forward(self, x):\n    x = self.conv(x)\n    x = self.ress(x)\n    \n    out = []\n    \n    for i in range(self.nStack):\n      hg = self.hourglass[i](x)\n      ll = hg\n      for j in range(self.nModules):\n        ll = self.Residual[i * self.nModules + j](ll)\n      ll = self.lin_[i](ll)\n      tmpOut = self.tmpOut[i](ll)\n      out.append(tmpOut)\n      if i < self.nStack - 1:\n        ll_ = self.ll_[i](ll)\n        tmpOut_ = self.tmpOut_[i](tmpOut)\n        x = x + ll_ + tmpOut_\n    \n    return out\n\ndef hourglass(model_config, pts):\n  print ('Initialize hourglass with configure : {}'.format(model_config))\n  model = HourglassNet(model_config['nStack'], model_config['nModules'], model_config['nFeats'], pts)\n  return model\n"""
TS3/models/teacher.py,2,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import init\n\nclass TeacherNet(nn.Module):\n  def __init__(self, input_dim, n_layers=3):\n    super(TeacherNet, self).__init__()\n    sequence = [\n      nn.Conv2d(input_dim, 64, kernel_size=4, stride=2, padding=1),\n      nn.LeakyReLU(0.2, True)\n    ]\n\n    nf_mult = 1\n    nf_mult_prev = 1\n    for n in range(1, n_layers):\n      nf_mult_prev = nf_mult\n      nf_mult = min(2**n, 8)\n      sequence += [\n        nn.Conv2d(64 * nf_mult_prev, 64 * nf_mult, kernel_size=4, stride=2, padding=1, bias=True),\n        nn.InstanceNorm2d(64 * nf_mult, affine=False),\n        nn.LeakyReLU(0.2, True)\n      ]\n\n    nf_mult_prev = nf_mult\n    nf_mult = min(2**n_layers, 8)\n    sequence += [\n      nn.Conv2d(64 * nf_mult_prev, 64 * nf_mult, kernel_size=4, stride=1, padding=1, bias=True),\n      nn.InstanceNorm2d(64 * nf_mult, affine=False),\n      nn.LeakyReLU(0.2, True)\n    ]\n\n    sequence += [nn.Conv2d(64 * nf_mult, 1, kernel_size=4, stride=1, padding=1)]\n\n    self.model = nn.Sequential(*sequence)\n\n  def forward(self, inputs):\n    return self.model(inputs)\n'"
SAN/lib/cluster/__init__.py,0,"b'##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom .cluster import filter_cluster\n'"
SAN/lib/cluster/cluster.py,0,"b'##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nimport numpy as np\nfrom sklearn.preprocessing import normalize\nimport pdb\n\ndef cos_dis(x, y):\n  x = normalize(x[:,np.newaxis], axis=0).ravel()\n  y = normalize(y[:,np.newaxis], axis=0).ravel()\n  return np.linalg.norm(x-y)\n\ndef filter_cluster(indexes, cluster_features, ratio):\n  num_feature = cluster_features.shape[0]\n  mean_feature = np.mean(cluster_features, axis=0)\n\n  all_L1, all_L2, all_LC = [], [], []\n  for i in range(num_feature):\n    x = cluster_features[i]\n    L1 = np.sum(np.abs((x-mean_feature)))\n    L2 = np.linalg.norm(x-mean_feature)\n    LC = cos_dis(x, mean_feature)\n    all_L1.append( L1 )\n    all_L2.append( L2 )\n    all_LC.append( LC )\n  all_L1 = np.array(all_L1)\n  all_L2 = np.array(all_L2)\n  all_LC = np.array(all_LC)\n  threshold = (all_L2.max()-all_L2.min())*ratio+all_L2.min()\n  selected = indexes[ all_L2 < threshold ]\n  return selected.copy()\n'"
SAN/lib/datasets/CycleDataset.py,1,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Modified from PyTorch Cycle-GAN                        ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom __future__ import print_function\nfrom PIL import Image\nimport os, random\nfrom os import path as osp\nimport numpy as np\nimport warnings\nimport math\n\nfrom utils import load_list_from_folders, load_txt_file\nfrom utils import generate_label_map_laplacian\nfrom utils import generate_label_map_gaussian\nfrom .dataset_utils import pil_loader\nfrom .dataset_utils import anno_parser\nfrom .point_meta import Point_Meta\nimport torch\nimport torch.utils.data as data\n\nclass CycleDataset(data.Dataset):\n\n  def __init__(self, transform, dataset_name):\n\n    self.transform = transform\n    self.dataset_name = dataset_name\n    self.reset()\n    #print ('The general dataset initialization done, dataset-name : {}, self is : {}'.format(dataset_name, self))\n\n  def __repr__(self):\n    return ('{name}(dataset={dataset_name}, A.size={A_size}, B.size={B_size})'.format(name=self.__class__.__name__, **self.__dict__))\n\n  def reset(self):\n    self.A_size = 0\n    self.A_datas = []\n    self.A_labels = []\n    self.B_size = 0\n    self.B_datas = []\n    self.B_labels = []\n    assert self.dataset_name is not None, 'The dataset name is None'\n\n\n  def __obtain(self, file_paths):\n    datas, boxes = [], []\n    for file_idx, file_path in enumerate(file_paths):\n      assert osp.isfile(file_path), 'The path : {} is not a file.'.format(file_path)\n      listfile = open(file_path, 'r')\n      listdata = listfile.read().splitlines()\n      listfile.close()\n      print ('Load [{:d}/{:d}]-th list : {:} with {:} images'.format(file_idx, len(file_paths), file_path, len(listdata)))\n      for idx, data in enumerate(listdata):\n        alls = data.split(' ')\n        if '' in alls: alls.remove('')\n        assert len(alls) == 6 or len(alls) == 7, 'The {:04d}-th line is wrong : {:}'.format(idx, data)\n        datas.append( alls[0] )\n        box = np.array( [ float(alls[2]), float(alls[3]), float(alls[4]), float(alls[5]) ] )\n        boxes.append( box )\n    labels = []\n    for idx, data in enumerate(datas):\n      assert isinstance(data, str), 'The type of data is not correct : {}'.format(data)\n      meta = Point_Meta(1, None, boxes[idx], data, self.dataset_name)\n      labels.append( meta )\n    return datas, labels\n\n  def set_a(self, file_paths):\n    self.A_datas, self.A_labels = self.__obtain(file_paths)\n    self.A_size = len(self.A_datas)\n    assert len(self.A_labels) == self.A_size and self.A_size > 0, 'The length is not right : {} vs {}'.format(len(self.A_datas), len(self.A_labels))\n    print ('Set the A-dataset from {} lists and obtain {} faces'.format(len(file_paths), self.A_size))\n\n  def set_b(self, file_paths):\n    self.B_datas, self.B_labels = self.__obtain(file_paths)\n    self.B_size = len(self.B_datas)\n    assert len(self.B_labels) == self.B_size and self.B_size > 0, 'The length is not right : {} vs {}'.format(len(self.B_datas), len(self.B_labels))\n    print ('Set the B-dataset from {} lists and obtain {} faces'.format(len(file_paths), self.B_size))\n\n  def append_a(self, dataset, indexes):\n    for index in indexes:\n      self.A_datas.append( dataset.datas[index] )\n      self.A_labels.append( dataset.labels[index].copy() )\n    self.A_size = len(self.A_datas)\n\n  def append_b(self, dataset, indexes):\n    for index in indexes:\n      self.B_datas.append( dataset.datas[index] )\n      self.B_labels.append( dataset.labels[index].copy() )\n    self.B_size = len(self.B_datas)\n\n  def __len__(self):\n    return max(self.A_size, self.B_size)\n\n  def __getitem__(self, index):\n    index_A = index % self.A_size\n    index_B = random.randint(0, self.B_size - 1)\n\n    A_img = pil_loader( self.A_datas[index_A] )\n    B_img = pil_loader( self.B_datas[index_B] )\n    A_target = self.A_labels[index_A].copy()\n    B_target = self.B_labels[index_B].copy()\n\n    # transform the image and points\n    if self.transform is not None:\n      A_image, A_target = self.transform(A_img, A_target)\n      B_image, B_target = self.transform(B_img, B_target)\n\n    return {'A': A_image, 'B': B_image, 'A_index': index_A, 'B_index': index_B}\n"""
SAN/lib/datasets/GeneralDataset.py,9,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom __future__ import print_function\nfrom PIL import Image\nimport os\nfrom os import path as osp\nimport numpy as np\nimport warnings\nimport math\n\nfrom utils import load_list_from_folders, load_txt_file\nfrom utils import generate_label_map_laplacian\nfrom utils import generate_label_map_gaussian\nfrom .dataset_utils import pil_loader\nfrom .dataset_utils import anno_parser\nfrom .point_meta import Point_Meta\nimport torch\nimport torch.utils.data as data\n\nclass GeneralDataset(data.Dataset):\n\n  def __init__(self, transform, sigma, downsample, heatmap_type, dataset_name):\n\n    self.transform = transform\n    self.sigma = sigma\n    self.downsample = downsample\n    self.heatmap_type = heatmap_type\n    self.dataset_name = dataset_name\n    self.reset()\n    print ('The general dataset initialization done, sigma is {}, downsample is {}, dataset-name : {}, self is : {}'.format(sigma, downsample, dataset_name, self))\n\n  def __repr__(self):\n    return ('{name}(number of point={NUM_PTS}, heatmap_type={heatmap_type})'.format(name=self.__class__.__name__, **self.__dict__))\n\n  def convert68to51(self):\n    # following 300-VW to remove the contour\n    assert self.NUM_PTS == 68, 'Can only support the initial points is 68 vs {}'.format(self.NUM_PTS)\n    print ('Start convert 68 points to 51 points for {} images'.format( len(self) ))\n    for label in self.labels:\n      label.convert68to51()\n    self.NUM_PTS = 51\n\n  def convert68to49(self):\n    # following 300-VW to remove the contour\n    assert self.NUM_PTS == 68, 'Can only support the initial points is 68 vs {}'.format(self.NUM_PTS)\n    print ('Start convert 68 points to 49 points for {} images'.format( len(self) ))\n    for label in self.labels:\n      label.convert68to49()\n    self.NUM_PTS = 49\n\n  def reset(self, num_pts=-1):\n    self.length = 0\n    self.NUM_PTS = num_pts\n    self.datas = []\n    self.labels = []\n    self.face_sizes = []\n    assert self.dataset_name is not None, 'The dataset name is None'\n\n  def append(self, data, label, box, face_size):\n    assert osp.isfile(data), 'The image path is not a file : {}'.format(data)\n    self.datas.append( data )\n    if label is not None:\n      if isinstance(label, str):\n        assert osp.isfile(label), 'The annotation path is not a file : {}'.format(label)\n        np_points, _ = anno_parser(label, self.NUM_PTS)\n        meta = Point_Meta(self.NUM_PTS, np_points, box, data, self.dataset_name)\n      elif isinstance(label, Point_Meta):\n        meta = label.copy()\n      else:\n        raise NameError('Do not know this label : {}'.format(label))\n    else:\n      meta = Point_Meta(self.NUM_PTS, None, box, data, self.dataset_name)\n    self.labels.append( meta )\n    self.face_sizes.append( face_size )\n    self.length = self.length + 1\n\n  def load_data(self, datas, labels, boxes, face_sizes, num_pts, reset):\n    # each data is a png file name\n    # each label is a Point_Meta class or the general pts format file (anno_parser_v1)\n    print ('Start load data for the general datas')\n    assert isinstance(datas, list), 'The type of the datas is not correct : {}'.format( type(datas) )\n    assert isinstance(labels, list) and len(datas) == len(labels), 'The type of the labels is not correct : {}'.format( type(labels) )\n    assert isinstance(boxes, list) and len(datas) == len(boxes), 'The type of the boxes is not correct : {}'.format( type(boxes) )\n    assert isinstance(face_sizes, list) and len(datas) == len(face_sizes), 'The type of the face_sizes is not correct : {}'.format( type(face_sizes) )\n    if reset: self.reset(num_pts)\n    else:     assert self.NUM_PTS == num_pts, 'The number of point is inconsistance : {} vs {}'.format(self.NUM_PTS, num_pts)\n\n    for idx, data in enumerate(datas):\n      assert isinstance(data, str), 'The type of data is not correct : {}'.format(data)\n      assert osp.isfile(datas[idx]), '{} is not a file'.format(datas[idx])\n      self.append(datas[idx], labels[idx], boxes[idx], face_sizes[idx])\n\n    assert len(self.datas) == self.length, 'The length and the data is not right {} vs {}'.format(self.length, len(self.datas))\n    assert len(self.labels) == self.length, 'The length and the labels is not right {} vs {}'.format(self.length, len(self.labels))\n    assert len(self.face_sizes) == self.length, 'The length and the face_sizes is not right {} vs {}'.format(self.length, len(self.face_sizes))\n    print ('Load data done for the general dataset, which has {} images.'.format(self.length))\n\n  def load_list(self, file_paths, num_pts, reset):\n    if file_paths is None:\n      print ('Input the None list file, skip load data.')\n      return\n    else:\n      print ('Load list from {}'.format(file_paths))\n    if isinstance(file_paths, str):\n      file_paths = [ file_paths ]\n\n    datas, labels, boxes, face_sizes = [], [], [], []\n    for file_idx, file_path in enumerate(file_paths):\n      assert osp.isfile(file_path), 'The path : {} is not a file.'.format(file_path)\n      listfile = open(file_path, 'r')\n      listdata = listfile.read().splitlines()\n      listfile.close()\n      print ('Load [{:d}/{:d}]-th list : {:} with {:} images'.format(file_idx, len(file_paths), file_path, len(listdata)))\n      for idx, data in enumerate(listdata):\n        alls = data.split(' ')\n        if '' in alls: alls.remove('')\n        assert len(alls) == 6 or len(alls) == 7, 'The {:04d}-th line is wrong : {:}'.format(idx, data)\n        datas.append( alls[0] )\n        if alls[1] == 'None':\n          labels.append( None )\n        else:\n          labels.append( alls[1] )\n        box = np.array( [ float(alls[2]), float(alls[3]), float(alls[4]), float(alls[5]) ] )\n        boxes.append( box )\n        if len(alls) == 6:\n          face_sizes.append( None )\n        else:\n          face_sizes.append( float(alls[6]) )\n    self.load_data(datas, labels, boxes, face_sizes, num_pts, reset)\n\n  def __len__(self):\n    assert len(self.datas) == self.length, 'The length is not correct : {}'.format(self.length)\n    return self.length\n\n  def prepare_input(self, image, box):\n    meta = Point_Meta(self.NUM_PTS, None, np.array(box), image, self.dataset_name)\n    image = pil_loader( image )\n    return self._process_(image, meta, -1), meta\n\n  def __getitem__(self, index):\n    image = pil_loader( self.datas[index] )\n    xtarget = self.labels[index].copy()\n    return self._process_(image, xtarget, index)\n\n  def _process_(self, image, xtarget, index):\n\n    # Get the label\n    if xtarget.is_none():\n      visiable = None\n    else:\n      visiable = xtarget.points[2, :].astype('bool')\n\n    # transform the image and points\n    if self.transform is not None:\n      image, xtarget = self.transform(image, xtarget)\n\n    # If for evaluation not load label, keeps the original data\n    temp_save_wh = xtarget.temp_save_wh\n    ori_size = torch.IntTensor( [temp_save_wh[1], temp_save_wh[0], temp_save_wh[2], temp_save_wh[3]] ) # H, W, Cropped_[x1,y1]\n        \n    if isinstance(image, Image.Image):\n      height, width = image.size[1], image.size[0]\n    elif isinstance(image, torch.FloatTensor):\n      height, width = image.size(1),  image.size(2)\n    else:\n      raise Exception('Unknown type of image : {}'.format( type(image) ))\n\n    if xtarget.is_none() == False:\n      xtarget.apply_bound(width, height)\n      points = xtarget.points.copy()\n      points = torch.from_numpy(points.transpose((1,0))).type(torch.FloatTensor)\n      Hpoint = xtarget.points.copy()\n    else:\n      points = torch.from_numpy(np.zeros((self.NUM_PTS,3))).type(torch.FloatTensor)\n      Hpoint = self.NUM_PTS\n\n    if self.heatmap_type == 'laplacian':\n      target, mask = generate_label_map_laplacian(Hpoint, height//self.downsample, width//self.downsample, self.sigma, self.downsample, visiable) # H*W*C\n    elif self.heatmap_type == 'gaussian':\n      target, mask = generate_label_map_gaussian(Hpoint, height//self.downsample, width//self.downsample, self.sigma, self.downsample, visiable) # H*W*C\n    else:\n      raise Exception('Unknown type of image : {}'.format( type(image) ))\n      \n\n    target = torch.from_numpy(target.transpose((2, 0, 1))).type(torch.FloatTensor)\n    mask   = torch.from_numpy(mask.transpose((2, 0, 1))).type(torch.ByteTensor)\n  \n    torch_index = torch.IntTensor([index])\n    torch_indicate = torch.ByteTensor( [ xtarget.is_none() == False ] )\n\n    return image, target, mask, points, torch_index, torch_indicate, ori_size\n"""
SAN/lib/datasets/__init__.py,0,"b'##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom .GeneralDataset import GeneralDataset\nfrom .dataset_utils import pil_loader\nfrom .point_meta import Point_Meta\nfrom .dataset_utils import PTSconvert2str\nfrom .dataset_utils import PTSconvert2box\nfrom .CycleDataset import CycleDataset\n'"
SAN/lib/datasets/dataset_utils.py,0,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom PIL import Image\nfrom scipy.ndimage.interpolation import zoom\nfrom utils.file_utils import load_txt_file\nimport numpy as np\nimport copy, math\n\ndef pil_loader(path):\n  # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n  with open(path, 'rb') as f:\n    with Image.open(f) as img:\n      return img.convert('RGB')\n\ndef remove_item_from_list(list_to_remove, item):\n  assert isinstance(list_to_remove, list), 'input list is not a list'\n    \n  try:\n    list_to_remove.remove(item)\n  except ValueError:\n    print('Warning!!!!!! Item to remove is not in the list. Remove operation is not done.')\n\n  return list_to_remove\n\ndef anno_parser(anno_path, num_pts):  \n  data, num_lines = load_txt_file(anno_path)                          \n  if data[0].find('version: ') == 0: # 300-W\n    return anno_parser_v0(anno_path, num_pts)\n  else:\n    return anno_parser_v1(anno_path, num_pts)\n\ndef anno_parser_v0(anno_path, num_pts):  \n  '''                        \n  parse the annotation for 300W dataset, which has a fixed format for .pts file                                \n  return:                    \n    pts: 3 x num_pts (x, y, oculusion)                                \n  '''                        \n  data, num_lines = load_txt_file(anno_path)                          \n  assert data[0].find('version: ') == 0, 'version is not correct'     \n  assert data[1].find('n_points: ') == 0, 'number of points in second line is not correct'                     \n  assert data[2] == '{' and data[-1] == '}', 'starting and end symbol is not correct'                          \n                             \n  assert data[0] == 'version: 1' or data[0] == 'version: 1.0', 'The version is wrong : {}'.format(data[0])\n  n_points = int(data[1][len('n_points: '):])                         \n                             \n  assert num_lines == n_points + 4, 'number of lines is not correct'    # 4 lines for general information: version, n_points, start and end symbol      \n  assert num_pts == n_points, 'number of points is not correct'\n                             \n  # read points coordinate   \n  pts = np.zeros((3, n_points), dtype='float32')                      \n  line_offset = 3    # first point starts at fourth line              \n  point_set = set()\n  for point_index in range(n_points):                                \n    try:                     \n      pts_list = data[point_index + line_offset].split(' ')       # x y format                                 \n      if len(pts_list) > 2:    # handle edge case where additional whitespace exists after point coordinates   \n        pts_list = remove_item_from_list(pts_list, '')              \n      pts[0, point_index] = float(pts_list[0])                        \n      pts[1, point_index] = float(pts_list[1])                        \n      pts[2, point_index] = float(1)      # oculusion flag, 0: oculuded, 1: visible. We use 1 for all points since no visibility is provided by 300-W   \n      point_set.add( point_index )\n    except ValueError:       \n      print('error in loading points in %s' % anno_path)              \n  return pts, point_set\n\ndef anno_parser_v1(anno_path, NUM_PTS, one_base=True):\n  '''\n  parse the annotation for MUGSY-Full-Face dataset, which has a fixed format for .pts file\n  return: pts: 3 x num_pts (x, y, oculusion)\n  '''\n  data, n_points = load_txt_file(anno_path)\n  assert n_points <= NUM_PTS, '{} has {} points'.format(anno_path, n_points)\n  # read points coordinate\n  pts = np.zeros((3, NUM_PTS), dtype='float32')\n  point_set = set()\n  for line in data:\n    try:\n      idx, point_x, point_y, oculusion = line.split(' ')\n      idx, point_x, point_y, oculusion = int(idx), float(point_x), float(point_y), oculusion == 'True'\n      if one_base==False: idx = idx+1\n      assert idx >= 1 and idx <= NUM_PTS, 'Wrong idx of points : {:02d}-th in {:s}'.format(idx, anno_path)\n      pts[0, idx-1] = point_x\n      pts[1, idx-1] = point_y\n      pts[2, idx-1] = float( oculusion )\n      point_set.add(idx)\n    except ValueError:\n      raise Exception('error in loading points in {}'.format(anno_path))\n  return pts, point_set\n\ndef PTSconvert2str(points):\n  assert isinstance(points, np.ndarray) and len(points.shape) == 2, 'The points is not right : {}'.format(points)\n  assert points.shape[0] == 2 or points.shape[0] == 3, 'The shape of points is not right : {}'.format(points.shape)\n  string = ''\n  num_pts = points.shape[1]\n  for i in range(num_pts):\n    ok = False\n    if points.shape[0] == 3 and bool(points[2, i]) == True: \n      ok = True\n    elif points.shape[0] == 2:\n      ok = True\n\n    if ok:\n      string = string + '{:02d} {:.2f} {:.2f} True\\n'.format(i+1, points[0, i], points[1, i])\n  string = string[:-1]\n  return string\n\ndef PTSconvert2box(points, expand_ratio=None):\n  assert isinstance(points, np.ndarray) and len(points.shape) == 2, 'The points is not right : {}'.format(points)\n  assert points.shape[0] == 2 or points.shape[0] == 3, 'The shape of points is not right : {}'.format(points.shape)\n  if points.shape[0] == 3:\n    points = points[:2, points[-1,:].astype('bool') ]\n  elif points.shape[0] == 2:\n    points = points[:2, :]\n  else:\n    raise Exception('The shape of points is not right : {}'.format(points.shape))\n  assert points.shape[1] >= 2, 'To get the box of points, there should be at least 2 vs {}'.format(points.shape)\n  box = np.array([ points[0,:].min(), points[1,:].min(), points[0,:].max(), points[1,:].max() ])\n  W = box[2] - box[0]\n  H = box[3] - box[1]\n  assert W > 0 and H > 0, 'The size of box should be greater than 0 vs {}'.format(box)\n  if expand_ratio is not None:\n    box[0] = int( math.floor(box[0] - W * expand_ratio) )\n    box[1] = int( math.floor(box[1] - H * expand_ratio) )\n    box[2] = int( math.ceil(box[2] + W * expand_ratio) )\n    box[3] = int( math.ceil(box[3] + H * expand_ratio) )\n  return box\n\ndef for_generate_box_str(anno_path, num_pts, extend):\n  if isinstance(anno_path, str):\n    points, _ = anno_parser(anno_path, num_pts)\n  else:\n    points = anno_path.copy()\n  box = PTSconvert2box(points, extend)\n  return '{:.2f} {:.2f} {:.2f} {:.2f}'.format(box[0], box[1], box[2], box[3])\n    \ndef resize_heatmap(maps, height, width, order=3):\n  # maps  = np.ndarray with shape [height, width, channels]\n  # order = 0 Nearest\n  # order = 1 Bilinear\n  # order = 2 Cubic\n  assert isinstance(maps, np.ndarray) and len(maps.shape) == 3, 'maps type : {}'.format(type(maps))\n  \n  scale = tuple(np.array([height,width], dtype=float) / np.array(maps.shape[:2]))\n  return zoom(maps, scale + (1,), order=order)\n\ndef analysis_dataset(dataset):\n  all_values = np.zeros((3,len(dataset.datas)), dtype=np.float64)\n  hs = np.zeros((len(dataset.datas),), dtype=np.float64)\n  ws = np.zeros((len(dataset.datas),), dtype=np.float64)\n\n  for index, image_path in enumerate(self.datas):\n    img = pil_loader(image_path)\n    ws[index] = image.size[0]\n    hs[index] = image.size[1]\n    img = np.array(img)\n    all_values[:, index] = np.mean(np.mean(img, axis=0), axis=0).astype('float64')\n  mean = np.mean(all_values, axis=1)\n  std  = np.std (all_values, axis=1)\n  return mean, std, ws, hs\n\ndef split_datasets(dataset, point_ids):\n  sub_dataset = copy.deepcopy(dataset)\n  assert len(point_ids) > 0\n  assert False, 'un finished'\n\ndef convert68to49(points):\n  points = points.copy()\n  assert len(points.shape) == 2 and (points.shape[0] == 3 or points.shape[0] == 2) and points.shape[1] == 68, 'The shape of points is not right : {}'.format(points.shape)\n  out = np.ones((68,)).astype('bool')\n  out[[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,60,64]] = False\n  cpoints = points[:, out]\n  assert len(cpoints.shape) == 2 and cpoints.shape[1] == 49\n  return cpoints\n\ndef convert68to51(points):\n  points = points.copy()\n  assert len(points.shape) == 2 and (points.shape[0] == 3 or points.shape[0] == 2) and points.shape[1] == 68, 'The shape of points is not right : {}'.format(points.shape)\n  out = np.ones((68,)).astype('bool')\n  out[[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]] = False\n  cpoints = points[:, out]\n  assert len(cpoints.shape) == 2 and cpoints.shape[1] == 51\n  return cpoints\n"""
SAN/lib/datasets/point_meta.py,0,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom PIL import Image\nfrom scipy.ndimage.interpolation import zoom\nfrom utils.file_utils import load_txt_file\nfrom .dataset_utils import convert68to49 as _convert68to49\nfrom .dataset_utils import convert68to51 as _convert68to51\nimport numpy as np\nimport copy, math\n\nclass Point_Meta():\n  # points: 3 x num_pts (x, y, oculusion)\n  # image_size: original [width, height]\n  def __init__(self, num_point, points, box, image_path, dataset_name):\n\n    self.num_point = num_point\n    assert len(box.shape) == 1 and box.shape[0] == 4, 'The shape of box is not right : {}'.format( box )\n    self.box = box.copy()\n    if points is None:\n      self.points = points\n    else:\n      assert len(points.shape) == 2 and points.shape[0] == 3 and points.shape[1] == self.num_point, 'The shape of point is not right : {}'.format( points )\n      self.points = points.copy()\n    self.update_center()\n    self.image_path = image_path\n    self.datasets = dataset_name\n    self.temp_save_wh = None\n\n  def __repr__(self):\n    return ('{name}(number of point={num_point})'.format(name=self.__class__.__name__, **self.__dict__))\n\n  def convert68to49(self):\n    if self.points is not None:\n      self.points = _convert68to49(self.points)\n\n  def convert68to51(self):\n    if self.points is not None:\n      self.points = _convert68to51(self.points)\n\n  def update_center(self):\n    if self.points is not None:\n      self.center = np.mean(self.points[:2, self.points[2,:]>0], axis=1)\n    else:\n      self.center = np.array([ (self.box[0]+self.box[2])/2, (self.box[1]+self.box[3])/2 ])\n\n  def apply_bound(self, width, height):\n    if self.points is not None:\n      oks = np.vstack((self.points[0, :] >= 0, self.points[1, :] >=0, self.points[0, :] <= width, self.points[1, :] <= height, self.points[2, :].astype('bool')))\n      oks = oks.transpose((1,0))\n      self.points[2, :] = np.sum(oks, axis=1) == 5\n    self.box[0], self.box[1] = np.max([self.box[0], 0]),     np.max([self.box[1], 0])\n    self.box[2], self.box[3] = np.min([self.box[2], width]), np.min([self.box[3], height])\n\n  def apply_scale(self, scale):\n    if len(scale) == 1:   # scale the same size for both x and y\n      if self.points is not None:\n        self.points[:2, self.points[2,:]>0] = self.points[:2, self.points[2,:]>0] * scale[0]\n      self.center                         = self.center   * scale[0]\n      self.box[0], self.box[1]            = self.box[0] * scale[0], self.box[1] * scale[0]\n      self.box[2], self.box[3]            = self.box[2] * scale[0], self.box[3] * scale[0]\n    elif len(scale) == 2: # scale the width and height\n      if self.points is not None:\n        self.points[0, self.points[2,:]>0] = self.points[0, self.points[2,:]>0] * scale[0]\n        self.points[1, self.points[2,:]>0] = self.points[1, self.points[2,:]>0] * scale[1]\n      self.center[0]                     = self.center[0] * scale[0]\n      self.center[1]                     = self.center[1] * scale[1]\n      self.box[0], self.box[1]            = self.box[0] * scale[0], self.box[1] * scale[1]\n      self.box[2], self.box[3]            = self.box[2] * scale[0], self.box[3] * scale[1]\n    else:\n      assert False, 'Does not support this scale : {}'.format(scale)\n\n  def apply_offset(self, ax=None, ay=None):\n    if ax is not None:\n      if self.points is not None:\n        self.points[0, self.points[2,:]>0] = self.points[0, self.points[2,:]>0] + ax\n      self.center[0]                     = self.center[0] + ax\n      self.box[0], self.box[2]           = self.box[0] + ax, self.box[2] + ax\n    if ay is not None:\n      if self.points is not None:\n        self.points[1, self.points[2,:]>0] = self.points[1, self.points[2,:]>0] + ay\n      self.center[1]                     = self.center[1] + ay\n      self.box[1], self.box[3]           = self.box[1] + ay, self.box[3] + ay\n\n  def apply_rotate(self, center, degree):\n    degree = math.radians(-degree)\n    if self.points is not None:\n      vis_xs = self.points[0, self.points[2,:]>0]\n      vis_ys = self.points[1, self.points[2,:]>0]\n      self.points[0, self.points[2,:]>0] = (vis_xs - center[0]) * np.cos(degree) - (vis_ys - center[1]) * np.sin(degree) + center[0]\n      self.points[1, self.points[2,:]>0] = (vis_xs - center[0]) * np.sin(degree) + (vis_ys - center[1]) * np.cos(degree) + center[1]\n    # rotate the box\n    corners = np.zeros((4,2))\n    corners[0,0], corners[0,1] = self.box[0], self.box[1]\n    corners[1,0], corners[1,1] = self.box[0], self.box[3]\n    corners[2,0], corners[2,1] = self.box[2], self.box[1]\n    corners[3,0], corners[3,1] = self.box[2], self.box[3]\n    corners[:, 0] = (corners[:, 0] - center[0]) * np.cos(degree) - (corners[:, 1] - center[1]) * np.sin(degree) + center[0]\n    corners[:, 1] = (corners[:, 0] - center[0]) * np.sin(degree) - (corners[:, 1] - center[1]) * np.cos(degree) + center[1]\n    self.box[0], self.box[1] = corners[0,0], corners[0,1]\n    self.box[2], self.box[3] = corners[3,0], corners[3,1]\n    \n  def apply_horizontal_flip(self, width):\n    self.points[0, :] = width - self.points[0, :] - 1\n    # Mugsy spefic or Synthetic\n    if self.datasets == 'Mugsy.full_face_v1':\n      ori = np.array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n      pos = np.array([ 3,  4,  1,  2,  9, 10, 11, 12,  5,  6,  7,  8, 14, 13, 15, 16, 17, 18, 19, 20])\n      self.points[:, pos-1] = self.points[:, ori-1]\n    elif self.datasets == 'Synthetic.v1':\n      ori = np.array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n      pos = np.array([ 3,  4,  1,  2,  9, 10, 11, 12,  5,  6,  7,  8, 14, 13, 15, 16, 17, 18, 19, 20])\n      self.points[:, pos-1] = self.points[:, ori-1]\n    else:\n      assert False, 'Does not support {}.{}'.format(self.datasets, self.subsets)\n    \n  # all points' range [0, w) [0, h)\n  def check_nan(self):\n    if math.isnan(self.center[0]) or math.isnan(self.center[1]):\n      return True\n    for i in range(self.num_point):\n      if self.points[2, i] > 0:\n        if math.isnan(self.points[0, i]) or math.isnan(self.points[1, i]):\n          return True\n    return False\n\n  def visiable_pts_num(self):\n    ans = self.points[2,:]>0\n    return np.sum(ans)\n\n  def set_precrop_wh(self, W, H, x1, y1, x2, y2):\n    self.temp_save_wh = [W, H, x1, y1, x2, y2]\n\n  def get_box(self):\n    return self.box.copy()\n\n  def get_points(self):\n    if self.points is not None:\n      return self.points.copy()\n    else:\n      return np.zeros((3, self.num_point), dtype='float32')\n\n  def is_none(self):\n    assert self.box is not None, 'The box should not be None'\n    return self.points is None\n\n  def copy(self):\n    return copy.deepcopy(self)\n"""
SAN/lib/debug/__init__.py,0,b'from .debug_main import main_debug_save\nfrom .check import register_nan_checks\nfrom .check import check_data\n'
SAN/lib/debug/check.py,0,"b""import torch\nimport numpy as np\n\ndef tocpudata(x):\n  if x.is_cuda: return x.cpu().data\n  else:         return x.data\n\ndef tonp(x):\n  if x.is_cuda: return x.cpu().data.numpy()\n  else:         return x.data.numpy()\n\ndef register_nan_checks(model):\n  def check_grad(module, grad_input, grad_output):\n    # print(module) you can add this to see that the hook is called\n    if any(np.all(np.isnan(tonp(gi))) for gi in grad_input if gi is not None):\n      print('NaN gradient in ' + type(module).__name__)\n  model.apply(lambda module: module.register_backward_hook(check_grad))\n\ndef check_data(dataset):\n  length = len(dataset)\n  for i, data in enumerate(dataset):\n    if i + 1 >= length:\n      nxt = 'none'\n    else:\n      nxt = dataset.datas[i+1]\n    print (' {:05d} / {:05d} : {:} -->> {:}'.format(i, length, dataset.datas[i], nxt))\n"""
SAN/lib/debug/debug_main.py,3,"b'import PIL\nimport torch\nimport numpy as np\nimport math, os\nimport os.path as osp\nimport models\nfrom san_vision import transforms\nfrom utils import print_log\nfrom visualization import save_error_image, draw_image_with_pts\nfrom visualization import merge_images, generate_color_from_heatmaps, overlap_two_pil_image\n\n\ndef main_debug_save(debug_save_dir, loader, image_index, input_vars, batch_locs, target, points, sign_list, batch_cpms, generations, log):\n  mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n  batch_size, num_pts = batch_locs.size(0), batch_locs.size(1)\n  assert batch_size == len(image_index)\n  print_log(\'Save-dir : {} : check {}\'.format(debug_save_dir, image_index), log)\n  for idx, imgid in enumerate(image_index):\n    basename = osp.basename( loader.dataset.datas[imgid] )\n    basename = basename.split(\'.\')[0]\n    print_log(\'Start visualization done for [{:03d}/{:03d}] : {:5} | {:}\'.format(idx, len(image_index), imgid, loader.dataset.datas[imgid]), log)\n    ## Save all images\n    images = [input_vars[idx], generations[0][idx], generations[1][idx]]\n    _pil_images = []\n    for iG, xinput in enumerate(images):\n      xinput = xinput.clone()\n      Xinput = []\n      for t, m, s in zip(xinput, mean, std):\n        t = torch.mul(t, s)\n        t = torch.add(t, m)\n        Xinput.append( t )\n      xinput = torch.stack(Xinput)\n      if xinput.is_cuda: xinput = xinput.cpu()\n      image = transforms.ToPILImage()(xinput.data)\n      debug_save_path = os.path.join(debug_save_dir, \'{}-G{}.png\'.format(basename, iG))\n      image.save(debug_save_path)\n      _pil_images.append( image )\n      """"""\n      debug_loc = []\n      for ipts in range(num_pts):\n        temploc = models.variable2np( batch_locs[idx][ipts] )\n        debug_loc.append( temploc )\n      debug_loc = np.array( debug_loc )\n      debug_save_path = os.path.join(debug_save_dir, \'{}-ans-points.png\'.format(basename))\n      pimage = draw_image_with_pts(image, debug_loc.transpose(1,0), radius=1, linewidth=1, fontScale=12, window=None)\n      pimage.save(debug_save_path)\n      """"""\n    image = _pil_images[0]\n    debug_save_path = os.path.join(debug_save_dir, \'{}-GG.png\'.format(basename))\n    overlap_two_pil_image(_pil_images[1], _pil_images[2]).save(debug_save_path)\n    # save the back ground heatmap\n    for icpm in range(len(batch_cpms)):\n      cpms = batch_cpms[icpm][idx]\n      xtarget = models.variable2np( cpms )\n      xtarget = xtarget.transpose(1,2,0)\n      xheatmaps = generate_color_from_heatmaps(xtarget, index=-1)\n      cimage = PIL.Image.fromarray(np.uint8(xheatmaps*255))\n      cimage = overlap_two_pil_image(image, cimage)\n      debug_save_path = os.path.join(debug_save_dir, \'{:}-BG-{}.png\'.format(basename, icpm))\n      cimage.save(debug_save_path)\n\n      xheatmaps = generate_color_from_heatmaps(xtarget, index=0)\n      cimage = PIL.Image.fromarray(np.uint8(xheatmaps*255))\n      cimage = overlap_two_pil_image(image, cimage)\n      debug_save_path = os.path.join(debug_save_dir, \'{:}-B0-{}.png\'.format(basename, icpm))\n      cimage.save(debug_save_path)\n\n    # save the ground truth heatmap\n    if sign_list[idx] and False:\n      xtarget = models.variable2np( target[idx] )\n      xtarget = xtarget.transpose(1,2,0)\n      xheatmaps = generate_color_from_heatmaps(xtarget)\n      all_images = []\n      for pid in range(len(xheatmaps)):\n        cimage = PIL.Image.fromarray(np.uint8(xheatmaps[pid]*255))\n        cimage = overlap_two_pil_image(center_img, cimage)\n        all_images.append( cimage )\n      debug_save_path = os.path.join(debug_save_dir, \'{}-gt-heatmap.png\'.format(basename))\n      all_images = merge_images(all_images, 10)\n      all_images.save( debug_save_path )\n  \n      # debug save for points\n      cpoints = models.variable2np( points[idx] )\n      debug_save_path = os.path.join(debug_save_dir, \'{}-gt-points.png\'.format(basename))\n      point_image = draw_image_with_pts(center_img, cpoints.transpose(1,0), radius=1, linewidth=1, fontScale=12, window=lk_window)\n      point_image.save(debug_save_path)\n      \n      print_log(\'Calculate gt-visualization done for [{:03d}/{:03d}] : {:5} | {:}\'.format(idx, len(image_index), imgid, loader.dataset.datas[imgid]), log)\n    else:\n      print_log(\'Skip the ground truth heatmap debug saving\', log)\n'"
SAN/lib/models/__init__.py,0,"b'##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom .vgg16_base import vgg16_base\nfrom .itn import itn_model\nfrom .itn_cpm import itn_cpm\n\nfrom .resnet import resnet50, resnet101, resnet152\n\nfrom .model_utils import ModelConfig\nfrom .model_utils import np2variable, variable2np\nfrom .model_utils import remove_module_dict, load_weight_from_dict\n'"
SAN/lib/models/basic_batch.py,11,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numbers, math\nimport numpy as np\nimport models.model_utils as MU\n\ndef find_tensor_peak_batch(heatmap, radius, downsample, threshold = 0.000001):\n  assert heatmap.dim() == 3, 'The dimension of the heatmap is wrong : {}'.format(heatmap.size())\n  assert radius > 0 and isinstance(radius, numbers.Number), 'The radius is not ok : {}'.format(radius)\n  num_pts, H, W = heatmap.size(0), heatmap.size(1), heatmap.size(2)\n  assert W > 1 and H > 1, 'To avoid the normalization function divide zero'\n  # find the approximate location:\n  score, index = torch.max(heatmap.view(num_pts, -1), 1)\n  index_w = (index % W).float()\n  index_h = (index / W).float()\n  \n  def normalize(x, L):\n    return -1. + 2. * x.data / (L-1)\n  boxes = [index_w - radius, index_h - radius, index_w + radius, index_h + radius]\n  boxes[0] = normalize(boxes[0], W)\n  boxes[1] = normalize(boxes[1], H)\n  boxes[2] = normalize(boxes[2], W)\n  boxes[3] = normalize(boxes[3], H)\n\n  affine_parameter = torch.zeros((num_pts, 2, 3))\n  affine_parameter[:,0,0] = (boxes[2]-boxes[0])/2\n  affine_parameter[:,0,2] = (boxes[2]+boxes[0])/2\n  affine_parameter[:,1,1] = (boxes[3]-boxes[1])/2\n  affine_parameter[:,1,2] = (boxes[3]+boxes[1])/2\n  # extract the sub-region heatmap\n  theta = MU.np2variable(affine_parameter, heatmap.is_cuda, False)\n  grid_size = torch.Size([num_pts, 1, radius*2+1, radius*2+1])\n  grid = F.affine_grid(theta, grid_size)\n  sub_feature = F.grid_sample(heatmap.unsqueeze(1), grid).squeeze(1)\n  sub_feature = F.threshold(sub_feature, threshold, np.finfo(float).eps)\n\n  X = MU.np2variable(torch.arange(-radius, radius+1), heatmap.is_cuda, False).view(1, 1, radius*2+1)\n  Y = MU.np2variable(torch.arange(-radius, radius+1), heatmap.is_cuda, False).view(1, radius*2+1, 1)\n  \n  sum_region = torch.sum(sub_feature.view(num_pts,-1),1)\n  x = torch.sum((sub_feature*X).view(num_pts,-1),1) / sum_region + index_w\n  y = torch.sum((sub_feature*Y).view(num_pts,-1),1) / sum_region + index_h\n     \n  x = x * downsample + downsample / 2.0 - 0.5\n  y = y * downsample + downsample / 2.0 - 0.5\n  return torch.stack([x, y],1), score\n"""
SAN/lib/models/cycle_util.py,7,"b""import os\nimport os.path as osp\nimport torch\nfrom torch.optim import lr_scheduler\nfrom san_vision import transforms\n\ndef get_scheduler(optimizer, opt):\n  if opt.lr_policy == 'lambda':\n    def lambda_rule(epoch):\n      lr_l = 1.0 - max(0, epoch + 1 + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\n      return lr_l\n    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n  elif opt.lr_policy == 'step':\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n  elif opt.lr_policy == 'plateau':\n    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n  else:\n    return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n  return scheduler\n\ndef save_network(save_dir, save_name, network, gpu_ids):\n  if not osp.isdir(save_dir): os.makedirs(save_dir)\n  save_path = osp.join(save_dir, '{}.pth'.format(save_name))\n  torch.save(network.cpu().state_dict(), save_path)\n  if len(gpu_ids) and torch.cuda.is_available():\n    network.cuda(gpu_ids[0])\n\ndef load_network(save_dir, save_name, network):\n  save_path = osp.join(save_dir, '{}.pth'.format(save_name))\n  assert osp.isfile(save_path), '{} does not exist'.format(save_path)\n  network.load_state_dict(torch.load(save_path))\n\ndef tensor2im(image_tensor):\n  mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n  # only extract the first image\n  image_tensor = image_tensor[0]\n  Xinput = []\n  for t, m, s in zip(image_tensor, mean, std):\n    t = torch.mul(t, s)\n    t = torch.add(t, m)\n    Xinput.append( t )\n  xinput = torch.stack(Xinput)\n  if xinput.is_cuda: xinput = xinput.cpu()\n  image = transforms.ToPILImage()(xinput)\n  return image\n"""
SAN/lib/models/discriminator_model.py,3,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import init\n\n# Defines the PatchGAN discriminator with the specified arguments.\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, n_layers=3, use_sigmoid=False, gpu_ids=[]):\n        super(NLayerDiscriminator, self).__init__()\n        self.gpu_ids = gpu_ids\n\n        kw = 4\n        padw = 1\n        sequence = [\n            nn.Conv2d(3, 64, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)\n            sequence += [\n                nn.Conv2d(64 * nf_mult_prev, 64 * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=True),\n                nn.InstanceNorm2d(64 * nf_mult, affine=False),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        sequence += [\n            nn.Conv2d(64 * nf_mult_prev, 64 * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=True),\n            nn.InstanceNorm2d(64 * nf_mult, affine=False),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(64 * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n\n        if use_sigmoid:\n            sequence += [nn.Sigmoid()]\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        if len(self.gpu_ids) and isinstance(input.data, torch.cuda.FloatTensor):\n            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n        else:\n            return self.model(input)\n'"
SAN/lib/models/gan_loss.py,4,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import init\nfrom torch.autograd import Variable\n\n# Defines the GAN loss which uses either LSGAN or the regular GAN.\n# When LSGAN is used, it is basically same as MSELoss,\n# but it abstracts away the need to create the target label tensor\n# that has the same size as the input\nclass GANLoss(nn.Module):\n  def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n         tensor=torch.FloatTensor):\n    super(GANLoss, self).__init__()\n    self.real_label = target_real_label\n    self.fake_label = target_fake_label\n    self.real_label_var = None\n    self.fake_label_var = None\n    self.Tensor = tensor\n    if use_lsgan:\n      self.loss = nn.MSELoss()\n    else:\n      self.loss = nn.BCELoss()\n\n  def get_target_tensor(self, input, target_is_real):\n    target_tensor = None\n    if target_is_real:\n      create_label = ((self.real_label_var is None) or\n              (self.real_label_var.numel() != input.numel()))\n      if create_label:\n        real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n        self.real_label_var = Variable(real_tensor, requires_grad=False)\n      target_tensor = self.real_label_var\n    else:\n      create_label = ((self.fake_label_var is None) or\n              (self.fake_label_var.numel() != input.numel()))\n      if create_label:\n        fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n        self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n      target_tensor = self.fake_label_var\n    return target_tensor\n\n  def __call__(self, input, target_is_real):\n    target_tensor = self.get_target_tensor(input, target_is_real)\n    return self.loss(input, target_tensor)\n'"
SAN/lib/models/generator_model.py,3,"b""import torch\nimport torch.nn as nn\nimport functools\nfrom torch.autograd import Variable\nimport numpy as np\n\n# Define a resnet block\nclass ResnetBlock(nn.Module):\n  def __init__(self, dim, padding_type, use_dropout, use_bias):\n    super(ResnetBlock, self).__init__()\n    self.conv_block = self.build_conv_block(dim, padding_type, use_dropout, use_bias)\n\n  def build_conv_block(self, dim, padding_type, use_dropout, use_bias):\n    conv_block = []\n    p = 0\n    if padding_type == 'reflect':\n      conv_block += [nn.ReflectionPad2d(1)]\n    elif padding_type == 'replicate':\n      conv_block += [nn.ReplicationPad2d(1)]\n    elif padding_type == 'zero':\n      p = 1\n    else:\n      raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\n    conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n             nn.InstanceNorm2d(dim, affine=False),\n             nn.ReLU(True)]\n    if use_dropout:\n      conv_block += [nn.Dropout(0.5)]\n\n    p = 0\n    if padding_type == 'reflect':\n      conv_block += [nn.ReflectionPad2d(1)]\n    elif padding_type == 'replicate':\n      conv_block += [nn.ReplicationPad2d(1)]\n    elif padding_type == 'zero':\n      p = 1\n    else:\n      raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n    conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n             nn.InstanceNorm2d(dim, affine=False)]\n\n    return nn.Sequential(*conv_block)\n\n  def forward(self, x):\n    out = x + self.conv_block(x)\n    return out\n\n\n# Defines the generator that consists of Resnet blocks between a few\n# downsampling/upsampling operations.\n# Code and idea originally from Justin Johnson's architecture.\n# https://github.com/jcjohnson/fast-neural-style/\nclass ResnetGenerator(nn.Module):\n  def __init__(self, n_blocks=6, gpu_ids=[], padding_type='reflect'):\n    assert(n_blocks >= 0)\n    super(ResnetGenerator, self).__init__()\n    self.gpu_ids = gpu_ids\n\n    model = [nn.ReflectionPad2d(3),\n         nn.Conv2d(3, 64, kernel_size=7, padding=0, bias=True),\n         nn.InstanceNorm2d(64, affine=False),\n         nn.ReLU(True)]\n\n    n_downsampling = 2\n    for i in range(n_downsampling):\n      mult = 2**i\n      model += [nn.Conv2d(64 * mult, 64 * mult * 2, kernel_size=3, stride=2, padding=1, bias=True),\n            nn.InstanceNorm2d(64 * mult * 2, affine=False),\n            nn.ReLU(True)]\n\n    mult = 2**n_downsampling\n    for i in range(n_blocks):\n      model += [ResnetBlock(64 * mult, padding_type=padding_type, use_dropout=False, use_bias=True)]\n\n    for i in range(n_downsampling):\n      mult = 2**(n_downsampling - i)\n      model += [nn.ConvTranspose2d(64 * mult, int(64 * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1, bias=True),\n            nn.InstanceNorm2d(int(64 * mult / 2), affine=False),\n            nn.ReLU(True)]\n    model += [nn.ReflectionPad2d(3)]\n    model += [nn.Conv2d(64, 3, kernel_size=7, padding=0)]\n    model += [nn.Tanh()]\n\n    self.model = nn.Sequential(*model)\n\n  def forward(self, input):\n    if self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor):\n      return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n    else:\n      return self.model(input)\n"""
SAN/lib/models/initialization.py,2,"b""import torch\nimport torch.nn as nn\nfrom torch.nn import init\n\ndef weights_init_cpm(m):\n  classname = m.__class__.__name__\n  # print(classname)\n  if classname.find('Conv') != -1:\n    m.weight.data.normal_(0, 0.01)\n    if m.bias is not None: m.bias.data.zero_()\n  elif classname.find('BatchNorm2d') != -1:\n    m.weight.data.fill_(1)\n    m.bias.data.zero_()\n\ndef weights_init_normal(m):\n  classname = m.__class__.__name__\n  # print(classname)\n  if classname.find('Conv') != -1:\n    init.uniform(m.weight.data, 0.0, 0.02)\n  elif classname.find('Linear') != -1:\n    init.uniform(m.weight.data, 0.0, 0.02)\n  elif classname.find('BatchNorm2d') != -1:\n    init.uniform(m.weight.data, 1.0, 0.02)\n    init.constant(m.bias.data, 0.0)\n\n\ndef weights_init_xavier(m):\n  classname = m.__class__.__name__\n  # print(classname)\n  if classname.find('Conv') != -1:\n    init.xavier_normal_(m.weight.data, gain=1)\n  elif classname.find('Linear') != -1:\n    init.xavier_normal_(m.weight.data, gain=1)\n  elif classname.find('BatchNorm2d') != -1:\n    init.uniform(m.weight.data, 1.0, 0.02)\n    init.constant(m.bias.data, 0.0)\n\n\ndef weights_init_kaiming(m):\n  classname = m.__class__.__name__\n  # print(classname)\n  if classname.find('Conv') != -1:\n    init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n  elif classname.find('Linear') != -1:\n    init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n  elif classname.find('BatchNorm2d') != -1:\n    init.uniform(m.weight.data, 1.0, 0.02)\n    init.constant(m.bias.data, 0.0)\n\n\ndef weights_init_orthogonal(m):\n  classname = m.__class__.__name__\n  print(classname)\n  if classname.find('Conv') != -1:\n    init.orthogonal(m.weight.data, gain=1)\n  elif classname.find('Linear') != -1:\n    init.orthogonal(m.weight.data, gain=1)\n  elif classname.find('BatchNorm2d') != -1:\n    init.uniform(m.weight.data, 1.0, 0.02)\n    init.constant(m.bias.data, 0.0)\n"""
SAN/lib/models/itn.py,10,"b""import os, itertools, torch, numpy as np\nfrom collections import OrderedDict\nfrom os import path as osp\nimport utils\nfrom utils.image_pool import ImagePool\nfrom utils import count_parameters_in_MB\nfrom utils import get_model_infos\nfrom .generator_model import ResnetGenerator\nfrom .discriminator_model import NLayerDiscriminator\nfrom .gan_loss       import GANLoss\nfrom .cycle_util     import get_scheduler, save_network, load_network, tensor2im\nfrom .model_utils    import print_network\nfrom .initialization import weights_init_xavier\n\n\ndef define_G(gpu_ids=[]):\n  netG = ResnetGenerator(gpu_ids=gpu_ids)\n  if len(gpu_ids) > 0:\n    netG.cuda(gpu_ids[0])\n  netG.apply(weights_init_xavier)\n  return netG\n\n\ndef define_D(gpu_ids=[]):\n  netD = NLayerDiscriminator(use_sigmoid=False, gpu_ids=gpu_ids)\n  if len(gpu_ids) > 0:\n    netD.cuda(gpu_ids[0])\n  netD.apply(weights_init_xavier)\n  return netD\n\n\nclass ITN():\n\n  def __repr__(self):\n    return ('{name})'.format(name=self.__class__.__name__, **self.__dict__))\n\n  def initialize(self, opt, log):\n    self.opt = opt\n    self.gpu_ids = opt.gpu_ids\n    self.Tensor = torch.cuda.FloatTensor if self.gpu_ids else torch.Tensor\n\n    nb = opt.cycle_batchSize\n    crop_height, crop_width = opt.crop_height, opt.crop_width\n    self.input_A = self.Tensor(nb, 3, crop_height, crop_width)\n    self.input_B = self.Tensor(nb, 3, crop_height, crop_width)\n\n    # load/define networks\n    # The naming conversion is different from those used in the paper\n    # Code (paper): G_A (G), G_B (F), D_A (D_Y), D_B (D_X)\n\n    self.netG_A = define_G(gpu_ids=self.gpu_ids)\n    self.netG_B = define_G(gpu_ids=self.gpu_ids)\n\n    self.netD_A = define_D(gpu_ids=self.gpu_ids)\n    self.netD_B = define_D(gpu_ids=self.gpu_ids)\n\n    # for training \n    self.fake_A_pool   = ImagePool(opt.pool_size)\n    self.fake_B_pool   = ImagePool(opt.pool_size)\n    # define loss functions\n    self.criterionGAN  = GANLoss(use_lsgan=True, tensor=self.Tensor)\n    self.criterionCycle = torch.nn.L1Loss()\n    self.criterionIdt  = torch.nn.L1Loss()\n    # initialize optimizers\n    self.optimizer_G   = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()),\n                             lr=opt.cycle_lr, betas=(opt.cycle_beta1, 0.999))\n    self.optimizer_D_A = torch.optim.Adam(self.netD_A.parameters(), lr=opt.cycle_lr, betas=(opt.cycle_beta1, 0.999))\n    self.optimizer_D_B = torch.optim.Adam(self.netD_B.parameters(), lr=opt.cycle_lr, betas=(opt.cycle_beta1, 0.999))\n    self.optimizers = []\n    self.schedulers = []\n    self.optimizers.append(self.optimizer_G)\n    self.optimizers.append(self.optimizer_D_A)\n    self.optimizers.append(self.optimizer_D_B)\n    for optimizer in self.optimizers:\n      self.schedulers.append(get_scheduler(optimizer, opt))\n\n    utils.print_log('------------ Networks initialized -------------', log)\n    print_network(self.netG_A, 'netG_A', log)\n    print_network(self.netG_B, 'netG_B', log)\n    print_network(self.netD_A, 'netD_A', log)\n    print_network(self.netD_B, 'netD_B', log)\n    utils.print_log('-----------------------------------------------', log)\n\n  def set_mode(self, mode):\n    if mode.lower() == 'train':\n      self.netG_A.train()\n      self.netG_B.train()\n      self.netD_A.train()\n      self.netD_B.train()\n      self.criterionGAN.train()\n      self.criterionCycle.train()\n      self.criterionIdt.train()\n    elif mode.lower() == 'eval':\n      self.netG_A.eval()\n      self.netG_B.eval()\n      self.netD_A.eval()\n      self.netD_B.eval()\n    else:\n      raise NameError('The wrong mode : {}'.format(mode))\n\n  def set_input(self, input):\n    input_A = input['A']\n    input_B = input['B']\n    self.input_A.resize_(input_A.size()).copy_(input_A)\n    self.input_B.resize_(input_B.size()).copy_(input_B)\n\n  def prepaer_input(self):\n    self.real_A = torch.autograd.Variable(self.input_A)\n    self.real_B = torch.autograd.Variable(self.input_B)\n\n  def num_parameters(self):\n    params = count_parameters_in_MB(self.netG_A)\n    params+= count_parameters_in_MB(self.netG_B)\n    params+= count_parameters_in_MB(self.netD_B)\n    params+= count_parameters_in_MB(self.netD_B)\n    return params\n\n  def num_flops(self):\n    self.prepaer_input()\n    flops1, params1 = get_model_infos(self.netG_A.model, None, self.real_A)\n    fake_B = self.netG_A( self.real_A )\n    flops2, params2 = get_model_infos(self.netD_A.model, None, fake_B)\n    return flops1 + flops2\n\n  def test(self):\n    self.real_A = torch.autograd.Variable(self.input_A, volatile=True)\n    self.fake_B = self.netG_A.forward(self.real_A)\n    self.rec_A = self.netG_B.forward(self.fake_B)\n\n    self.real_B = torch.autograd.Variable(self.input_B, volatile=True)\n    self.fake_A = self.netG_B.forward(self.real_B)\n    self.rec_B = self.netG_A.forward(self.fake_A)\n\n  def backward_D_basic(self, netD, real, fake):\n    # Real\n    pred_real = netD.forward(real)\n    loss_D_real = self.criterionGAN(pred_real, True)\n    # Fake\n    pred_fake = netD.forward(fake.detach())\n    loss_D_fake = self.criterionGAN(pred_fake, False)\n    # Combined loss\n    loss_D = (loss_D_real + loss_D_fake) * 0.5\n    # backward\n    loss_D.backward()\n    return loss_D\n\n  def backward_D_A(self):\n    fake_B = self.fake_B_pool.query(self.fake_B)\n    self.loss_D_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B)\n\n  def backward_D_B(self):\n    fake_A = self.fake_A_pool.query(self.fake_A)\n    self.loss_D_B = self.backward_D_basic(self.netD_B, self.real_A, fake_A)\n\n  def backward_G(self):\n    lambda_idt = self.opt.identity\n    lambda_A = self.opt.lambda_A\n    lambda_B = self.opt.lambda_B\n    # Identity loss\n    if lambda_idt > 0:\n      # G_A should be identity if real_B is fed.\n      self.idt_A = self.netG_A.forward(self.real_B)\n      self.loss_idt_A = self.criterionIdt(self.idt_A, self.real_B) * lambda_B * lambda_idt\n      # G_B should be identity if real_A is fed.\n      self.idt_B = self.netG_B.forward(self.real_A)\n      self.loss_idt_B = self.criterionIdt(self.idt_B, self.real_A) * lambda_A * lambda_idt\n    else:\n      self.loss_idt_A = 0\n      self.loss_idt_B = 0\n\n    # GAN loss\n    # D_A(G_A(A))\n    self.fake_B = self.netG_A.forward(self.real_A)\n    pred_fake = self.netD_A.forward(self.fake_B)\n    self.loss_G_A = self.criterionGAN(pred_fake, True)\n    # D_B(G_B(B))\n    self.fake_A = self.netG_B.forward(self.real_B)\n    pred_fake = self.netD_B.forward(self.fake_A)\n    self.loss_G_B = self.criterionGAN(pred_fake, True)\n    # Forward cycle loss\n    self.rec_A = self.netG_B.forward(self.fake_B)\n    self.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * lambda_A\n    # Backward cycle loss\n    self.rec_B = self.netG_A.forward(self.fake_A)\n    self.loss_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * lambda_B\n    # combined loss\n    self.loss_G = self.loss_G_A + self.loss_G_B + self.loss_cycle_A + self.loss_cycle_B + self.loss_idt_A + self.loss_idt_B\n    self.loss_G.backward()\n\n  def optimize_parameters(self):\n    # forward\n    self.prepaer_input()\n    # G_A and G_B\n    self.optimizer_G.zero_grad()\n    self.backward_G()\n    self.optimizer_G.step()\n    # D_A\n    self.optimizer_D_A.zero_grad()\n    self.backward_D_A()\n    self.optimizer_D_A.step()\n    # D_B\n    self.optimizer_D_B.zero_grad()\n    self.backward_D_B()\n    self.optimizer_D_B.step()\n\n  def get_current_errors(self):\n    D_A = self.loss_D_A.item()\n    G_A = self.loss_G_A.item()\n    Cyc_A = self.loss_cycle_A.item()\n    D_B = self.loss_D_B.item()\n    G_B = self.loss_G_B.item()\n    Cyc_B = self.loss_cycle_B.item()\n    if self.opt.identity > 0.0:\n      idt_A = self.loss_idt_A.item()\n      idt_B = self.loss_idt_B.item()\n      return OrderedDict([('D_A', D_A), ('G_A', G_A), ('Cyc_A', Cyc_A), ('idt_A', idt_A),\n                ('D_B', D_B), ('G_B', G_B), ('Cyc_B', Cyc_B), ('idt_B', idt_B)])\n    else:\n      return OrderedDict([('D_A', D_A), ('G_A', G_A), ('Cyc_A', Cyc_A),\n                ('D_B', D_B), ('G_B', G_B), ('Cyc_B', Cyc_B)])\n\n  def get_current_visuals(self, isTrain):\n    real_A = tensor2im(self.real_A.data)\n    rec_A = tensor2im(self.rec_A.data)\n    fake_A = tensor2im(self.fake_A.data)\n\n    real_B = tensor2im(self.real_B.data)\n    rec_B = tensor2im(self.rec_B.data)\n    fake_B = tensor2im(self.fake_B.data)\n\n    if isTrain and self.opt.identity > 0.0:\n      idt_A = tensor2im(self.idt_A.data)\n      idt_B = tensor2im(self.idt_B.data)\n      return OrderedDict([('real_A', real_A), ('fake_B', fake_B), ('rec_A', rec_A), ('idt_B', idt_B),\n                ('real_B', real_B), ('fake_A', fake_A), ('rec_B', rec_B), ('idt_A', idt_A)])\n    else:\n      return OrderedDict([('real_A', real_A), ('fake_B', fake_B), ('rec_A', rec_A),\n                ('real_B', real_B), ('fake_A', fake_A), ('rec_B', rec_B)])\n\n  def save(self, save_dir, log):\n    save_network(save_dir, 'G_A', self.netG_A, self.gpu_ids)\n    save_network(save_dir, 'D_A', self.netD_A, self.gpu_ids)\n    save_network(save_dir, 'G_B', self.netG_B, self.gpu_ids)\n    save_network(save_dir, 'D_B', self.netD_B, self.gpu_ids)\n    utils.print_log('save the model into {}'.format(save_dir), log)\n\n  def load(self, save_dir, log):\n    load_network(save_dir, 'G_A', self.netG_A)\n    load_network(save_dir, 'D_A', self.netD_A)\n    load_network(save_dir, 'G_B', self.netG_B)\n    load_network(save_dir, 'D_B', self.netD_B)\n    utils.print_log('load the model from {}'.format(save_dir), log)\n\n  # update learning rate (called once every epoch)\n  def update_learning_rate(self, log):\n    for scheduler in self.schedulers:\n      scheduler.step()\n    lr = self.optimizers[0].param_groups[0]['lr']\n    utils.print_log('learning rate = {:.7f}'.format(lr), log)\n\ndef itn_model(model_config, opt, log):\n  itnetwork = ITN()\n  itnetwork.initialize(opt, log)\n  return itnetwork\n"""
SAN/lib/models/itn_cpm.py,8,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom __future__ import division\nimport time, math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom collections import OrderedDict\nfrom .model_utils import get_parameters, load_weight_from_dict, count_network_param\nfrom .basic_batch import find_tensor_peak_batch\nfrom .initialization import weights_init_cpm\nfrom .cycle_util import load_network\nfrom .itn import define_G\n\nclass ITN_CPM(nn.Module):\n  def __init__(self, model_config):\n    super(ITN_CPM, self).__init__()\n\n    self.config = model_config.copy()\n    self.downsample = 1\n\n    self.netG_A = define_G()\n    self.netG_B = define_G()\n\n    self.features = nn.Sequential(\n          nn.Conv2d(  3,  64, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d( 64,  64, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.MaxPool2d(kernel_size=2, stride=2),\n          nn.Conv2d( 64, 128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.MaxPool2d(kernel_size=2, stride=2),\n          nn.Conv2d(128, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(256, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(256, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.MaxPool2d(kernel_size=2, stride=2),\n          nn.Conv2d(256, 512, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(512, 512, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(512, 512, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True))\n  \n    self.downsample = 8\n    pts_num = self.config.pts_num\n\n    self.CPM_feature = nn.Sequential(\n          nn.Conv2d(512, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), #CPM_1\n          nn.Conv2d(256, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True)) #CPM_2\n\n    self.stage1 = nn.Sequential(\n          nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 512, kernel_size=1, padding=0), nn.ReLU(inplace=True),\n          nn.Conv2d(512, pts_num, kernel_size=1, padding=0))\n\n    self.stage2 = nn.Sequential(\n          nn.Conv2d(128*2+pts_num*2, 128, kernel_size=7, dilation=1, padding=3), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=7, dilation=1, padding=3), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=7, dilation=1, padding=3), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=1, padding=0), nn.ReLU(inplace=True),\n          nn.Conv2d(128,     pts_num, kernel_size=1, padding=0))\n\n    self.stage3 = nn.Sequential(\n          nn.Conv2d(128*2+pts_num, 128, kernel_size=7, dilation=1, padding=3), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=7, dilation=1, padding=3), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=7, dilation=1, padding=3), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=1, padding=0), nn.ReLU(inplace=True),\n          nn.Conv2d(128,     pts_num, kernel_size=1, padding=0))\n  \n    assert self.config.num_stages >= 1, 'stages of cpm must >= 1'\n\n  def set_mode(self, mode):\n    if mode.lower() == 'train':\n      self.train()\n      for m in self.netG_A.modules():\n        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.InstanceNorm2d):\n          m.eval()\n      for m in self.netG_B.modules():\n        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.InstanceNorm2d):\n          m.eval()\n    elif mode.lower() == 'eval':\n      self.eval()\n    else:\n      raise NameError('The wrong mode : {}'.format(mode))\n\n  def specify_parameter(self, base_lr, base_weight_decay):\n    params_dict = [ {'params': self.netG_A.parameters()                   , 'lr': 0        , 'weight_decay': 0},\n                    {'params': self.netG_B.parameters()                   , 'lr': 0        , 'weight_decay': 0},\n                    {'params': get_parameters(self.features,   bias=False), 'lr': base_lr  , 'weight_decay': base_weight_decay},\n                    {'params': get_parameters(self.features,   bias=True ), 'lr': base_lr*2, 'weight_decay': 0},\n                    {'params': get_parameters(self.CPM_feature, bias=False), 'lr': base_lr  , 'weight_decay': base_weight_decay},\n                    {'params': get_parameters(self.CPM_feature, bias=True ), 'lr': base_lr*2, 'weight_decay': 0},\n                    {'params': get_parameters(self.stage1,      bias=False), 'lr': base_lr  , 'weight_decay': base_weight_decay},\n                    {'params': get_parameters(self.stage1,      bias=True ), 'lr': base_lr*2, 'weight_decay': 0},\n                    {'params': get_parameters(self.stage2,      bias=False), 'lr': base_lr*4, 'weight_decay': base_weight_decay},\n                    {'params': get_parameters(self.stage2,      bias=True ), 'lr': base_lr*8, 'weight_decay': 0},\n                    {'params': get_parameters(self.stage3,      bias=False), 'lr': base_lr*4, 'weight_decay': base_weight_decay},\n                    {'params': get_parameters(self.stage3,      bias=True ), 'lr': base_lr*8, 'weight_decay': 0}\n                  ]\n    return params_dict\n\n  # return : cpm-stages, locations\n  def forward(self, inputs):\n    assert inputs.dim() == 4, 'This model accepts 4 dimension input tensor: {}'.format(inputs.size())\n    batch_size = inputs.size(0)\n    num_stages, num_pts = self.config.num_stages, self.config.pts_num - 1\n\n    batch_cpms, batch_locs, batch_scos = [], [], []     # [Squence, Points]\n\n    features, stage1s = [], []\n    inputs = [inputs, (self.netG_A(inputs)+self.netG_B(inputs))/2]\n    for input in inputs:\n      feature  = self.features(input)\n      feature = self.CPM_feature(feature)\n      features.append(feature)\n      stage1s.append( self.stage1(feature) )\n\n    xfeature = torch.cat(features, 1)\n    cpm_stage2 = self.stage2(torch.cat([xfeature, stage1s[0], stage1s[1]], 1))\n    cpm_stage3 = self.stage3(torch.cat([xfeature, cpm_stage2], 1))\n    batch_cpms = [stage1s[0], stage1s[1]] + [cpm_stage2, cpm_stage3]\n\n    # The location of the current batch\n    for ibatch in range(batch_size):\n      batch_location, batch_score = find_tensor_peak_batch(cpm_stage3[ibatch], self.config.argmax, self.downsample)\n      batch_locs.append( batch_location )\n      batch_scos.append( batch_score )\n    batch_locs, batch_scos = torch.stack(batch_locs), torch.stack(batch_scos)\n\n    return batch_cpms, batch_locs, batch_scos, inputs[1:]\n\n\n# use vgg16 conv1_1 to conv4_4 as feature extracation        \nmodel_urls = 'https://download.pytorch.org/models/vgg16-397923af.pth'\n\ndef itn_cpm(model_config, cycle_model_path):\n  \n  print ('Initialize ITN-CPM with configure : {}'.format(model_config))\n  model = ITN_CPM(model_config)\n  model.apply(weights_init_cpm)\n\n  if model_config.pretrained:\n    print ('vgg16_base use pre-trained model')\n    weights = model_zoo.load_url(model_urls)\n    load_weight_from_dict(model, weights, None, False)\n\n  if cycle_model_path:\n    load_network(cycle_model_path, 'G_A', model.netG_A)\n    load_network(cycle_model_path, 'G_B', model.netG_B)\n\n  print ('initialize the generator network by {} with {} parameters'.format(cycle_model_path, count_network_param(model)))\n  return model\n"""
SAN/lib/models/model_utils.py,9,"b""from scipy.ndimage.interpolation import zoom\nfrom collections import OrderedDict\nimport utils\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport copy, numbers, numpy as np\n\ndef np2variable(x, is_cuda=True, requires_grad=True, dtype=torch.FloatTensor):\n  if isinstance(x, np.ndarray):\n    v = torch.autograd.Variable(torch.from_numpy(x).type(dtype), requires_grad=requires_grad)\n  elif isinstance(x, torch.FloatTensor) or isinstance(x, torch.Tensor):\n    v = torch.autograd.Variable(x.type(dtype), requires_grad=requires_grad)\n  else:\n    raise Exception('Do not know this type : {:}'.format( type(x) ))\n\n  if is_cuda: return v.cuda()\n  else:       return v\n\ndef variable2np(x):\n  if x.is_cuda:\n    x = x.cpu()\n  if isinstance(x, torch.autograd.Variable):\n    return x.data.numpy()\n  else:\n    return x.numpy()\n\ndef get_parameters(model, bias):\n  for m in model.modules():\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n      if bias:\n        yield m.bias\n      else:\n        yield m.weight\n    elif isinstance(m, nn.BatchNorm2d):\n      if bias:\n        yield m.bias\n      else:\n        yield m.weight\n\ndef load_weight_from_dict(model, weight_state_dict, param_pair=None, remove_prefix=True):\n  if remove_prefix: weight_state_dict = remove_module_dict(weight_state_dict)\n  all_parameter = model.state_dict()\n  all_weights   = []\n  finetuned_layer, random_initial_layer = [], []\n  for key, value in all_parameter.items():\n    if param_pair is not None and key in param_pair:\n      all_weights.append((key, weight_state_dict[ param_pair[key] ]))\n    elif key in weight_state_dict:\n      all_weights.append((key, weight_state_dict[key]))\n      finetuned_layer.append(key)\n    else:\n      all_weights.append((key, value))\n      random_initial_layer.append(key)\n  print ('==>[load_model] finetuned layers : {}'.format(finetuned_layer))\n  print ('==>[load_model] keeped layers : {}'.format(random_initial_layer))\n  all_weights = OrderedDict(all_weights)\n  model.load_state_dict(all_weights)\n\ndef remove_module_dict(state_dict):\n  new_state_dict = OrderedDict()\n  for k, v in state_dict.items():\n    name = k[7:] # remove `module.`\n    new_state_dict[name] = v\n  return new_state_dict\n\ndef roi_pooling(input, rois, size=(7,7)):\n  assert rois.dim() == 2 and rois.size(1) == 5, 'rois shape is wrong : {}'.format(rois.size())\n  output = []\n  num_rois = rois.size(0)\n  size = np.array(size)\n  spatial_size = np.array([input.size(3), input.size(2)])\n  for i in range(num_rois):\n    roi = variable2np(rois[i])\n    im_idx = int(roi[0])\n    theta = utils.crop2affine(spatial_size, roi[1:])\n    theta = np2variable(theta, input.is_cuda).unsqueeze(0)\n    grid_size = torch.Size([1, 3, int(size[1]), int(size[0])])\n    grid = F.affine_grid(theta, grid_size)\n    roi_feature = F.grid_sample(input.narrow(0, im_idx, 1), grid)\n    output.append( roi_feature )\n  return torch.cat(output, 0)\n\nclass ModelConfig():\n  def __init__(self, pts_num, num_stages, pretrained, softargmax_patch):\n    assert isinstance(pts_num, int), 'The pts-num is not right : {}'.format(pts_num)\n    assert isinstance(num_stages, int), 'The stage-num is not right : {}'.format(num_stages)\n    assert isinstance(pretrained, bool), 'The format of pretrained is not right : {}'.format(pretrained)\n    assert isinstance(softargmax_patch, numbers.Number), 'The format of softargmax_patch is not right : {}'.format(softargmax_patch)\n\n    self.pts_num = pts_num\n    self.num_stages = num_stages\n    self.pretrained = pretrained\n    self.argmax = softargmax_patch\n    \n  def __repr__(self):\n    return ('{name}(points={pts_num}, stage={num_stages}, PreTrain={pretrained}, ArgMax={argmax})'.format(name=self.__class__.__name__, **self.__dict__))\n\n  def copy(self):\n    return copy.deepcopy(self)\n\ndef print_network(net, net_str, log):\n  num_params = 0\n  for param in net.parameters():\n    num_params += param.numel()\n  utils.print_log(net, log)\n  utils.print_log('Total number of parameters for {} is {}'.format(net_str, num_params), log)\n\ndef count_network_param(net):\n  num_params = 0\n  for param in net.parameters():\n    num_params += param.numel()\n  return num_params\n"""
SAN/lib/models/resnet.py,7,"b'import torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nfrom .model_utils import load_weight_from_dict\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.cls = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        feature = x.view(x.size(0), -1)\n        cls = self.cls(feature)\n\n        return feature, cls\n\n\ndef resnet50(pretrained=False, **kwargs):\n  model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n  if pretrained:\n    model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n  return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n  model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n  if pretrained:\n    weights = model_zoo.load_url( model_urls[\'resnet101\'] )\n    load_weight_from_dict(model, weights, None, False)\n  return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n  model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n  if pretrained:\n    weights = model_zoo.load_url( model_urls[\'resnet152\'] )\n    load_weight_from_dict(model, weights, None, False)\n  return model\n'"
SAN/lib/models/vgg16_base.py,7,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom __future__ import division\nimport time, math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom collections import OrderedDict\nfrom .model_utils import get_parameters, load_weight_from_dict\nfrom .basic_batch import find_tensor_peak_batch\nfrom .initialization import weights_init_cpm\n\nclass VGG16_base(nn.Module):\n  def __init__(self, model_config):\n    super(VGG16_base, self).__init__()\n\n    self.config = model_config.copy()\n    self.downsample = 1\n\n    self.features = nn.Sequential(\n          nn.Conv2d(  3,  64, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d( 64,  64, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.MaxPool2d(kernel_size=2, stride=2),\n          nn.Conv2d( 64, 128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.MaxPool2d(kernel_size=2, stride=2),\n          nn.Conv2d(128, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(256, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(256, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.MaxPool2d(kernel_size=2, stride=2),\n          nn.Conv2d(256, 512, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(512, 512, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(512, 512, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True))\n  \n    self.downsample = 8\n    pts_num = self.config.pts_num\n\n    self.CPM_feature = nn.Sequential(\n          nn.Conv2d(512, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), #CPM_1\n          nn.Conv2d(256, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True)) #CPM_2\n\n    self.stage1 = nn.Sequential(\n          nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 512, kernel_size=1, padding=0), nn.ReLU(inplace=True),\n          nn.Conv2d(512, pts_num, kernel_size=1, padding=0))\n\n    self.stage2 = nn.Sequential(\n          nn.Conv2d(128+pts_num, 128, kernel_size=7, dilation=1, padding=3), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=7, dilation=1, padding=3), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=7, dilation=1, padding=3), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=1, padding=0), nn.ReLU(inplace=True),\n          nn.Conv2d(128,     pts_num, kernel_size=1, padding=0))\n\n    self.stage3 = nn.Sequential(\n          nn.Conv2d(128+pts_num, 128, kernel_size=7, dilation=1, padding=3), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=7, dilation=1, padding=3), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=7, dilation=1, padding=3), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=1, padding=0), nn.ReLU(inplace=True),\n          nn.Conv2d(128,     pts_num, kernel_size=1, padding=0))\n  \n    assert self.config.num_stages >= 1, 'stages of cpm must >= 1'\n\n  def specify_parameter(self, base_lr, base_weight_decay):\n    params_dict = [ {'params': get_parameters(self.features,   bias=False), 'lr': base_lr  , 'weight_decay': base_weight_decay},\n                    {'params': get_parameters(self.features,   bias=True ), 'lr': base_lr*2, 'weight_decay': 0},\n                    {'params': get_parameters(self.CPM_feature, bias=False), 'lr': base_lr  , 'weight_decay': base_weight_decay},\n                    {'params': get_parameters(self.CPM_feature, bias=True ), 'lr': base_lr*2, 'weight_decay': 0},\n                    {'params': get_parameters(self.stage1,      bias=False), 'lr': base_lr  , 'weight_decay': base_weight_decay},\n                    {'params': get_parameters(self.stage1,      bias=True ), 'lr': base_lr*2, 'weight_decay': 0},\n                    {'params': get_parameters(self.stage2,      bias=False), 'lr': base_lr*4, 'weight_decay': base_weight_decay},\n                    {'params': get_parameters(self.stage2,      bias=True ), 'lr': base_lr*8, 'weight_decay': 0},\n                    {'params': get_parameters(self.stage3,      bias=False), 'lr': base_lr*4, 'weight_decay': base_weight_decay},\n                    {'params': get_parameters(self.stage3,      bias=True ), 'lr': base_lr*8, 'weight_decay': 0}\n                  ]\n    return params_dict\n\n  # return : cpm-stages, locations\n  def forward(self, inputs):\n    assert inputs.dim() == 4, 'This model accepts 4 dimension input tensor: {}'.format(inputs.size())\n    batch_size = inputs.size(0)\n    num_stages, num_pts = self.config.num_stages, self.config.pts_num - 1\n\n    batch_cpms = []\n    batch_locs, batch_scos = [], []     # [Squence, Points]\n\n    feature  = self.features(inputs)\n    xfeature = self.CPM_feature(feature)\n  \n    stage1 = self.stage1( xfeature )\n    stage2 = self.stage2(torch.cat([xfeature, stage1], 1))\n    stage3 = self.stage3(torch.cat([xfeature, stage2], 1))\n    batch_cpms = [stage1, stage2, stage3]\n\n    # The location of the current batch\n    for ibatch in range(batch_size):\n      batch_location, batch_score = find_tensor_peak_batch(stage3[ibatch], self.config.argmax, self.downsample)\n      batch_locs.append( batch_location )\n      batch_scos.append( batch_score )\n    batch_locs, batch_scos = torch.stack(batch_locs), torch.stack(batch_scos)\n\n    return batch_cpms, batch_locs, batch_scos\n\n# use vgg16 conv1_1 to conv4_4 as feature extracation        \nmodel_urls = 'https://download.pytorch.org/models/vgg16-397923af.pth'\n\ndef vgg16_base(model_config):\n  \n  print ('Initialize vgg16_base with configure : {}'.format(model_config))\n  model = VGG16_base(model_config)\n  model.apply(weights_init_cpm)\n\n  if model_config.pretrained:\n    print ('vgg16_base use pre-trained model')\n    weights = model_zoo.load_url(model_urls)\n    load_weight_from_dict(model, weights, None, False)\n  return model\n"""
SAN/lib/options/__init__.py,0,"b'##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom .options import Options\n'"
SAN/lib/options/options.py,2,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nimport os, sys, time, pdb, random, argparse\nimport torch\nimport init_path\n\nclass Options():\n  def __init__(self, model_names):\n    parser = argparse.ArgumentParser(description='Train Style Aggregated Network', formatter_class=argparse.ArgumentDefaultsHelpFormatter)   \n    parser.add_argument('--train_list',       type=str,   nargs='+',      help='The list file path to the video training dataset.')\n    parser.add_argument('--eval_lists',       type=str,   nargs='+',      help='The list file path to the video testing dataset.')\n    parser.add_argument('--num_pts',          type=int,                   help='Number of point.')\n    parser.add_argument('--arch',             metavar='ARCH', default='itn_cpm', help='model architectures')\n    parser.add_argument('--cpm_stage',        type=int,                   help='Number of stages in CPM model.')\n    # Data Argumentation\n    parser.add_argument('--sigma',            type=float,                 help='sigma distance for CPM.')\n    parser.add_argument('--scale_prob',       type=float, default=1.1,    help='argument scale probability.')\n    parser.add_argument('--scale_min',        type=float,                 help='argument scale : minimum scale factor.')\n    parser.add_argument('--scale_max',        type=float,                 help='argument scale : maximum scale factor.')\n    parser.add_argument('--scale_eval',       type=float,                 help='argument scale : maximum scale factor.')\n    parser.add_argument('--rotate_max',       type=int,                   help='argument rotate : maximum rotate degree.')\n    parser.add_argument('--pre_crop_expand',  type=float,                 help='parameters for pre-crop expand ratio')\n    parser.add_argument('--crop_height',      type=int,                   help='argument crop : crop height.')\n    parser.add_argument('--crop_width',       type=int,                   help='argument crop : crop width.')\n    parser.add_argument('--crop_perturb_max', type=int,                   help='argument crop : center of maximum perturb distance.')\n    parser.add_argument('--arg_flip',         dest='arg_flip',            action='store_true', help='Using flip data argumentation or not ')\n    parser.add_argument('--dataset_name',     type=str,                   metavar='N', help='The specific name of the datasets.')\n    parser.add_argument('--heatmap_type',     type=str,   choices=['gaussian','laplacian'], metavar='N', help='The method for generating the heatmap.')\n    parser.add_argument('--argmax_size',      type=int,   default=8,      metavar='N', help='The patch size for the soft-argmax function')\n    parser.add_argument('--weight_of_idt',    type=float,                 metavar='N', help='The weight of identity loss in CPM')\n    # Cycle-GAN\n    parser.add_argument('--niter',            type=int, default=100,      help='# of iter at starting learning rate')\n    parser.add_argument('--niter_decay',      type=int, default=100,      help='# of iter to linearly decay learning rate to zero')\n    parser.add_argument('--epoch_count',      type=int, default=1,        help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n    parser.add_argument('--cycle_batchSize',  type=int, default=1,        help='input batch size')\n    parser.add_argument('--pool_size',        type=int, default=50,       help='the size of image buffer that stores previously generated images')\n    parser.add_argument('--visual_freq',      type=int, default=-1,       help='frequence of visualization')\n    parser.add_argument('--cycle_beta1',      type=float, default=0.5,    help='momentum term of adam')\n    parser.add_argument('--cycle_lr',         type=float, default=0.0002, help='initial learning rate for adam')\n    parser.add_argument('--lr_policy',        type=str, default='lambda', help='learning rate policy: lambda|step|plateau')\n    parser.add_argument('--identity',         type=float, default=0.0,    help='use identity mapping. Setting identity other than 1 has an effect of scaling the weight of the identity mapping loss. For example, if the weight of the identity loss should be 10 times smaller than the weight of the reconstruction loss, please set optidentity = 0.1')\n    parser.add_argument('--lambda_A',         type=float, default=10.0,   help='weight for cycle loss (A -> B -> A)')\n    parser.add_argument('--lambda_B',         type=float, default=10.0,   help='weight for cycle loss (B -> A -> B)')\n    parser.add_argument('--cycle_model_path', type=str,                   help='The dir to save the cycle model.')\n    parser.add_argument('--cycle_a_lists',    type=str,   nargs='+',      help='The list file path to cycle-gan a dataset.')\n    parser.add_argument('--cycle_b_lists',    type=str,   nargs='+',      help='The list file path to cycle-gan b dataset.')\n    # Optimization options\n    parser.add_argument('--evaluation',       dest='evaluation',          action='store_true', help='evaluation or not')\n    parser.add_argument('--pure_resume',      dest='pure_resume',         action='store_true', help='Only load the model not resume.')\n    parser.add_argument('--eval_once',        dest='eval_once',           action='store_true', help='evaluation only once for evaluation ')\n    parser.add_argument('--eval_init',        dest='eval_init',           action='store_true', help='evaluation only once for evaluation then start normal training')\n    parser.add_argument('--debug_save',       dest='debug_save',          action='store_true', help='debug to save the data')\n    parser.add_argument('--pretrain',         dest='pretrain',            action='store_true', help='pre-train model or not')\n    parser.add_argument('--convert68to49',    dest='convert68to49',       action='store_true', help='convert 68 to 49.')\n    parser.add_argument('--convert68to51',    dest='convert68to51',       action='store_true', help='convert 68 to 51.')\n    parser.add_argument('--error_bar',        type=float,                 help='For drawing the image with large distance error.')\n    parser.add_argument('--epochs',           type=int,   default=300,    help='Number of epochs to train.')\n    parser.add_argument('--batch_size',       type=int,   default=2,      help='Batch size for training.')\n    parser.add_argument('--eval_batch',       type=int,   default=4,      help='Batch size for testing.')\n    parser.add_argument('--learning_rate',    type=float, default=0.1,    help='The Learning Rate.')\n    parser.add_argument('--momentum',         type=float, default=0.9,    help='Momentum.')\n    parser.add_argument('--decay',            type=float, default=0.0005, help='Weight decay (L2 penalty).')\n    parser.add_argument('--schedule',         type=int,   nargs='+',      help='Decrease learning rate at these epochs.')\n    parser.add_argument('--gammas',           type=float, nargs='+',      help='LR is multiplied by gamma on schedule, number of gammas should be equal to schedule')\n    # Checkpoints\n    parser.add_argument('--print_freq',       type=int,   default=200,    metavar='N', help='print frequency (default: 200)')\n    parser.add_argument('--print_freq_eval',  type=int,   default=200,    metavar='N', help='print frequency for evaluation (default: 200)')\n    parser.add_argument('--save_path',        type=str,   default='./',                help='Folder to save checkpoints and log.')\n    parser.add_argument('--resume',           type=str,   default='',     metavar='PATH', help='path to latest checkpoint (default: none)')\n    parser.add_argument('--start_epoch',      type=int,   default=0,      metavar='N', help='manual epoch number (useful on restarts)')\n    # cluster kmeans\n    parser.add_argument('--style_train_root',       type=str,                   help='To train style-discriminative feature.')\n    parser.add_argument('--style_eval_root',       type=str,                   help='To train style-discriminative feature.')\n    parser.add_argument('--n_clusters',       type=int,                   help='number of n_clusters')\n    # Acceleration\n    parser.add_argument('--gpu_ids',          type=str,                   help='empty for CPU, other for GPU-IDs')\n    parser.add_argument('--workers',          type=int,   default=2,      help='number of data loading workers (default: 2)')\n    # Random seed\n    parser.add_argument('--manualSeed',       type=int,                   help='manual seed')\n    self.opt = parser.parse_args()\n    if self.opt.gpu_ids is None:\n      str_ids = []\n    else:\n      str_ids = self.opt.gpu_ids.split(',')\n    self.opt.gpu_ids = []\n    for str_id in str_ids:\n      if int(str_id) >= 0: self.opt.gpu_ids.append( int(str_id) )\n\n    if len(self.opt.gpu_ids) > 0:\n      self.opt.use_cuda = True\n      torch.cuda.set_device(self.opt.gpu_ids[0])\n      assert torch.cuda.is_available(), 'Use gpu [{}] but torch.cuda is not available'.format(self.opt.gpu_ids)\n    else:\n      self.opt.use_cuda = False\n\n  def show(self, log):\n    print_log('------------ Options ------------', log)\n    for k, v in sorted(args.items()):\n      print_log('{}: {}'.format(str(k), str(v)), log)\n    print_log('-------------- End --------------', log)\n"""
SAN/lib/procedure/__init__.py,0,b'from .train_cycle import train_cycle_gan\nfrom .train_detector import train_san_epoch\n'
SAN/lib/procedure/evaluate_detector.py,1,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nimport os, time, numpy as np\nimport numbers\nimport os.path as osp\nimport torch\nfrom utils import AverageMeter, print_log, convert_size2str, convert_secs2time, time_string, time_for_file\nfrom san_vision import Eval_Meta\nfrom models import variable2np\nfrom debug import main_debug_save\n\ndef evaluation(eval_loaders, net, log, save_path, opt):\n  print_log('Evaluation => {} datasets, save into {}'.format(len(eval_loaders), save_path), log)\n  if not osp.isdir(save_path): os.makedirs( save_path )\n  assert osp.isdir(save_path), 'The save path {} is not a dir'.format(save_path)\n  for i, eval_loader in enumerate(eval_loaders):\n    print_log('  Evaluate => [{:2d}/{:2d}]-th image dataset : {:}'.format(i, len(eval_loaders), opt.eval_lists[i]), log)\n    isave_path = osp.join(save_path, 'eval-set-{:02d}'.format(i))\n    with torch.no_grad():\n      meta = evaluation_image(eval_loader, net, log, isave_path, opt)\n    meta.compute_mse(log)\n    meta_path = osp.join(isave_path, 'evaluation.pth.tar')\n    meta.save(meta_path)\n  if len(eval_loaders) > 0: print_log('====>> Evaluate all image datasets done', log)\n\ndef evaluation_image(eval_loader, net, log, save_path, opt):\n  if not osp.isdir(save_path): os.makedirs(save_path)\n  if opt.debug_save:\n    debug_save_dir = osp.join(save_path, 'debug-eval')\n    if not os.path.isdir(debug_save_dir): os.makedirs(debug_save_dir)\n  else: debug_save_dir = None\n  print_log(' ==>start evaluation image dataset, save into {} with the error bar : {}, using the last stage, DEBUG SAVE DIR : {}'.format(save_path, opt.error_bar, debug_save_dir), log)\n\n  # switch to eval mode\n  net.eval()\n  # calculate the time\n  batch_time, end = AverageMeter(), time.time()\n  # save the evaluation results\n  eval_meta = Eval_Meta()\n  # save the number of points\n  num_pts, scale_eval = opt.num_pts, opt.scale_eval\n\n  for i, (inputs, target, mask, points, image_index, label_sign, ori_size) in enumerate(eval_loader):\n\n    # inputs : Batch, Squence, Channel, Height, Width\n    target = target.cuda(async=True)\n    mask = mask.cuda(async=True)\n\n    # forward, batch_locs [1 x points] [batch, 2]\n    batch_cpms, batch_locs, batch_scos, generated = net(inputs)\n    assert batch_locs.size(0) == batch_scos.size(0) and batch_locs.size(0) == inputs.size(0)\n    assert batch_locs.size(1) == num_pts + 1 and batch_scos.size(1) == num_pts + 1\n    assert batch_locs.size(2) == 2 and len(batch_scos.size()) == 2\n    np_batch_locs, np_batch_scos = variable2np(batch_locs), variable2np(batch_scos)\n    image_index = np.squeeze(variable2np( image_index )).tolist()\n    if isinstance(image_index, numbers.Number): image_index = [image_index]\n    sign_list = variable2np(label_sign).astype('bool').squeeze(1).tolist()\n    # recover the ground-truth label\n    real_sizess = variable2np( ori_size )\n\n    for ibatch, imgidx in enumerate(image_index):\n      locations, scores = np_batch_locs[ibatch,:-1,:], np_batch_scos[ibatch,:-1]\n      if len(scores.shape) == 1: scores = np.expand_dims(scores, axis=1)\n      xpoints = eval_loader.dataset.labels[imgidx].get_points()\n      assert real_sizess[ibatch,0] > 0 and real_sizess[ibatch,1] > 0, 'The ibatch={}, imgidx={} is not right.'.format(ibatch, imgidx)\n      scale_h, scale_w = real_sizess[ibatch,0] * 1. / inputs.size(-2) , real_sizess[ibatch,1] * 1. / inputs.size(-1)\n      locations[:, 0], locations[:, 1] = locations[:, 0] * scale_w + real_sizess[ibatch,2], locations[:, 1] * scale_h + real_sizess[ibatch,3]\n      assert xpoints.shape[1] == num_pts and locations.shape[0] == num_pts and scores.shape[0] == num_pts, 'The number of points is {} vs {} vs {} vs {}'.format(num_pts, xpoints.shape, locations.shape, scores.shape)\n      # recover the original resolution\n      locations = np.concatenate((locations, scores), axis=1)\n      image_path = eval_loader.dataset.datas[imgidx]\n      face_size  = eval_loader.dataset.face_sizes[imgidx]\n      image_name = osp.basename( image_path )\n      locations = locations.transpose(1,0)\n      eval_meta.append(locations, xpoints, image_path, face_size)\n\n      if opt.error_bar is not None:\n        errorpath = osp.join(save_path, image_name)\n        save_error_image(image_path, xpoints, locations, opt.error_bar, errorpath, radius=5, color=(30,255,30), rev_color=(255,30,30), fontScale=10, text_color=(255,255,255))\n    if opt.debug_save:\n      print_log('DEBUG --- > [{:03d}/{:03d}] '.format(i, len(eval_loader)), log)\n      main_debug_save(debug_save_dir, eval_loader, image_index, inputs, batch_locs, target, points, sign_list, batch_cpms, generated, log)\n\n    # measure elapsed time\n    batch_time.update(time.time() - end)\n    need_hour, need_mins, need_secs = convert_secs2time(batch_time.avg * (len(eval_loader)-i-1))\n    end = time.time()\n\n    if i % opt.print_freq_eval == 0 or i+1 == len(eval_loader):\n      print_log('  Evaluation: [{:03d}/{:03d}] Time {:6.3f} ({:6.3f}) Need [{:02d}:{:02d}:{:02d}]'.format(i, len(eval_loader), batch_time.val, batch_time.avg, need_hour, need_mins, need_secs) + ' locations.shape={}'.format(np_batch_locs.shape), log)\n      \n  return eval_meta\n"""
SAN/lib/procedure/san_util.py,1,"b'##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nimport os, torch\nimport utils\n\ndef adjust_learning_rate(optimizer, epoch, gammas, schedule):\n  assert len(gammas) == len(schedule), ""length of gammas and schedule should be equal""\n  multiple = 1\n  for (gamma, step) in zip(gammas, schedule):\n    if (epoch == step):\n      multiple = gamma\n      break\n  all_lrs = []\n  for param_group in optimizer.param_groups:\n    param_group[\'lr\'] = multiple * param_group[\'lr\']\n    all_lrs.append( param_group[\'lr\'] )\n  return set(all_lrs)\n\ndef save_checkpoint(state, save_path, filename, log):\n  filename = os.path.join(save_path, filename)\n  torch.save(state, filename)\n  utils.print_log (\'save checkpoint into {}\'.format(filename), log)\n  return filename\n'"
SAN/lib/procedure/train_cycle.py,2,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nimport os, sys, time\nimport os.path as osp\nimport torch\nimport torch.nn as nn\nfrom utils import AverageMeter, print_log, convert_size2str, convert_secs2time, time_string, time_for_file\n\n\ndef convert2string(errors):\n  string = 'D_A : {:.3f} G_A : {:.3f}'.format(errors['D_A'], errors['G_A'])\n  string = string + ' D_B : {:.3f} G_B : {:.3f}'.format(errors['D_B'], errors['G_B'])\n  if 'idt_A' in errors:\n    string = string + ' idt_A : {:.3f}'.format(errors['idt_A'])\n  if 'idt_B' in errors:\n    string = string + ' idt_B : {:.3f}'.format(errors['idt_B'])\n  return string\n\n\ndef save_visual(save_dir, visuals):\n  if not osp.isdir(save_dir): os.makedirs(save_dir)\n  visuals['real_A'].save( osp.join(save_dir, 'real_A.png') )\n  visuals['real_B'].save( osp.join(save_dir, 'real_B.png') )\n  visuals['fake_A'].save( osp.join(save_dir, 'fake_A.png') )\n  visuals['fake_B'].save( osp.join(save_dir, 'fake_B.png') )\n  visuals['rec_A'].save( osp.join(save_dir, 'rec_A.png') )\n  visuals['rec_B'].save( osp.join(save_dir, 'rec_B.png') )\n  if 'idt_A' in visuals: visuals['idt_A'].save( osp.join(save_dir, 'idt_A.png') )\n  if 'idt_B' in visuals: visuals['idt_B'].save( osp.join(save_dir, 'idt_B.png') )\n\n\ndef train_cycle_gan(dataset, model, opt, log):\n\n  save_dir = osp.join(opt.save_path, 'cycle-gan')\n  print_log('save dir into {:}'.format(save_dir), log)\n  model.set_mode('train')\n  dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.cycle_batchSize, shuffle=True, num_workers=opt.workers)\n  epoch_start_time = time.time()\n  epoch_time, total_steps = AverageMeter(), 0\n\n  final_epoch = opt.niter + opt.niter_decay\n  return_dir = osp.join(save_dir, 'itn-epoch-{}-{}'.format(final_epoch, final_epoch+1))\n  #model.set_input( next(iter(dataloader)) )\n  #flops = model.num_flops()\n  #import pdb; pdb.set_trace()\n  if osp.isdir(return_dir):\n    print_log('Exist cycle-gan model-save dir : {}, therefore skip train cycle-gan'.format(return_dir), log)\n    return return_dir\n  else:\n    print_log('Does not find cycle-gan model-save dir, start training', log)\n\n  for epoch in range(opt.epoch_count, final_epoch + 1):\n    need_hour, need_mins, need_secs = convert_secs2time(epoch_time.avg * (final_epoch+1-epoch))\n    print_log('\\n==>>{:s} Epoch[{:03d}/{:03d}], [Time Left: {:02d}:{:02d}:{:02d}]'.format(time_string(), epoch, opt.niter+opt.niter_decay+1, need_hour, need_mins, need_secs), log)\n\n    batch_time, data_time, epoch_iter = AverageMeter(), AverageMeter(), 0\n    iter_time = time.time()\n    for i, data in enumerate(dataloader):\n      # prepare input\n      total_steps += opt.cycle_batchSize\n      epoch_iter += opt.cycle_batchSize\n      model.set_input(data)\n      # count time\n      data_time.update(time.time() - iter_time)\n\n      # forward and backward time\n      model.optimize_parameters()\n      batch_time.update(time.time() - iter_time)\n\n      if i % opt.print_freq == 0:\n        errors = model.get_current_errors()\n        need_hour, need_mins, need_secs = convert_secs2time(batch_time.avg * (len(dataloader)-i))\n        print_log(' Epoch: [{:3d}/{:03d}][{:4d}/{:4d}]  '.format(epoch, opt.niter+opt.niter_decay+1, i, len(dataloader)) \\\n                + ' Time {batch_time.val:5.1f} ({batch_time.avg:5.1f}) | {data_time.val:5.1f} ({data_time.avg:5.1f}) '.format(batch_time=batch_time, data_time=data_time) \\\n                + ' iter : {:5d}, {:5d}. '.format(epoch_iter, total_steps) \\\n                + '[{:02d}:{:02d}:{:02d}]  '.format(need_hour, need_mins, need_secs) \\\n                + convert2string(errors), log)\n\n      if (opt.visual_freq > 0) and (i % opt.visual_freq == 0 or i + 1 == len(dataloader)):\n        visuals = model.get_current_visuals(True)\n        vis_save_dir = osp.join(save_dir, 'visual', '{:03d}-{:04d}'.format(epoch, i))\n        save_visual(vis_save_dir, visuals)\n      iter_time = time.time()\n\n    # save model\n    cur_save_dir = osp.join(save_dir, 'itn-epoch-{}-{}'.format(epoch, opt.niter+opt.niter_decay+1))\n    model.save(cur_save_dir, log)\n    epoch_time.update(time.time() - epoch_start_time)\n    epoch_start_time = time.time()\n    model.update_learning_rate(log)\n\n  return cur_save_dir\n"""
SAN/lib/procedure/train_detector.py,6,"b'##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nimport os, copy\nimport os.path as osp\nimport torch\nimport time, numpy as np\nfrom utils import AverageMeter, print_log, convert_size2str, convert_secs2time, time_string, time_for_file\nfrom .san_util import adjust_learning_rate, save_checkpoint\nfrom san_vision import Eval_Meta, show_stage_loss, compute_stage_loss\nfrom debug import main_debug_save\nfrom models import variable2np\nfrom .evaluate_detector import evaluation\n\ndef train_san_epoch(opt, net, train_loader, eval_loaders, log):\n\n  print_log(\'The ITN-CPM Network is : {}\'.format(net), log)\n  print_log(\'Train ITN-CPM using LR={:.6f}, Decay={:.6f}\'.format(opt.learning_rate, opt.decay), log)\n  # obtain the parameters\n  if hasattr(net, \'specify_parameter\'):\n    net_param_dict = net.specify_parameter(opt.learning_rate, opt.decay)\n  else:\n    net_param_dict = net.parameters()\n  # define loss function\n  criterion = torch.nn.MSELoss(False)\n  if opt.use_cuda:\n    net = torch.nn.DataParallel(net, device_ids=opt.gpu_ids)\n    net.cuda()\n    criterion.cuda()\n  # define optimizer\n  optimizer = torch.optim.SGD(net_param_dict, lr=opt.learning_rate, momentum=opt.momentum,\n                              weight_decay=opt.decay, nesterov=True)\n  if opt.resume:\n    assert osp.isfile(opt.resume), \'The resume file is not here : {}\'.format(opt.resume)\n    print_log(""=> loading checkpoint \'{}\' start"".format(opt.resume), log)\n    checkpoint       = torch.load(opt.resume)\n    net.load_state_dict(checkpoint[\'state_dict\'])\n    if opt.pure_resume == False:\n      opt.start_epoch = checkpoint[\'epoch\']\n      optimizer.load_state_dict(checkpoint[\'optimizer\'])\n    else:\n      print_log(\'pure resume, only load the parameters, skip optimizer and start-epoch\', log)\n    if opt.eval_once:\n      evaluation(eval_loaders, net, log, osp.join(opt.save_path, \'epoch-once\'), opt)\n      return\n      \n\n\n  # Main Training and Evaluation Loop\n  start_time, epoch_time = time.time(), AverageMeter()\n  \n  for epoch in range(opt.start_epoch, opt.epochs):\n    all_lrs = adjust_learning_rate(optimizer, epoch, opt.gammas, opt.schedule)\n\n    need_hour, need_mins, need_secs = convert_secs2time(epoch_time.avg * (opt.epochs-epoch))\n\n    print_log(\'\\n==>>{:s} Epoch[{:03d}/{:03d}], [Time Left: {:02d}:{:02d}:{:02d}], learning_rate : [{:.8f}, {:.8f}], Identity-Weight={:}\'.format(time_string(), epoch, opt.epochs, need_hour, need_mins, need_secs, min(all_lrs), max(all_lrs), opt.weight_of_idt), log)\n\n    # train for one epoch\n    train_loss = train(train_loader, net, criterion, optimizer, epoch, opt, log)\n    print_log(\'==>>{:s} Average Loss for Epoch[{:03d}] total=[{:.5f}] \'.format(time_string(), epoch, train_loss), log)\n\n    # remember best prec@1 and save checkpoint\n    save_name = save_checkpoint({\n          \'epoch\': epoch,\n          \'args\' : copy.deepcopy(opt),\n          \'state_dict\': net.state_dict(),\n          \'optimizer\' : optimizer.state_dict(),\n          }, opt.save_path, \'checkpoint_{}.pth.tar\'.format(epoch), log)\n\n    # evaluate\n    if opt.evaluation:\n      print_log(\'===>>{:s} Evaluate epoch [{:03d}] \'.format(time_string(), epoch), log)\n      evaluation(eval_loaders, net, log, osp.join(opt.save_path, \'epoch-{:03d}\'.format(epoch)), opt)\n\n    # measure elapsed time\n    epoch_time.update(time.time() - start_time)\n    start_time = time.time()\n\n# train function (forward, backward, update)\ndef train(train_loader, net, criterion, optimizer, epoch, opt, log):\n  batch_time = AverageMeter()\n  data_time = AverageMeter()\n  forward_time = AverageMeter()\n  visible_points = AverageMeter()\n  losses = AverageMeter()\n  # switch to train mode\n  net.module.set_mode(\'train\')\n  criterion.train()\n\n  if opt.debug_save:\n    debug_save_dir = os.path.join(opt.save_path, \'debug-train-{:02d}\'.format(epoch))\n    if not os.path.isdir(debug_save_dir): os.makedirs(debug_save_dir)\n\n  end = time.time()\n  for i, (inputs, target, mask, points, image_index, label_sign, _) in enumerate(train_loader):\n    # inputs : Batch, Squence, Channel, Height, Width\n    # data prepare\n    target = target.cuda(async=True)\n    # get the real mask\n    mask.masked_scatter_((1-label_sign).unsqueeze(-1).unsqueeze(-1), torch.ByteTensor(mask.size()).zero_())\n    mask_var   = mask.cuda(async=True)\n\n    batch_size, num_pts = inputs.size(0), mask.size(1)-1\n    image_index = variable2np(image_index).squeeze(1).tolist()\n    # check the label indicator, whether is has annotation or not\n    sign_list = variable2np(label_sign).astype(\'bool\').squeeze(1).tolist()\n\n    # measure data loading time\n    data_time.update(time.time() - end)\n    cvisible_points = torch.sum(mask[:,:-1,:,:]) * 1. / batch_size\n    visible_points.update(cvisible_points, batch_size)\n\n    # batch_cpms is a list for CPM stage-predictions, each element should be [Batch, Squence, C, H, W]\n    # batch_locs and batch_scos are two sequence-list of point-list, each element is [Batch, 2] / [Batch, 1]\n    # batch_next and batch_back are two sequence-list of point-list, each element is [Batch, 2] / [Batch, 2]\n    batch_cpms, batch_locs, batch_scos, generated = net(inputs)\n    \n    forward_time.update(time.time() - end)\n\n    total_labeled_cpm = int(np.sum(sign_list))\n\n    # collect all cpm stages for the middle frame\n    cpm_loss, each_stage_loss_values = compute_stage_loss(criterion, target, batch_cpms, mask_var, total_labeled_cpm, opt.weight_of_idt)\n  \n    # measure accuracy and record loss\n    losses.update(cpm_loss.item(), batch_size)\n    # compute gradient and do SGD step\n    optimizer.zero_grad()\n    cpm_loss.backward()\n    optimizer.step()\n\n    # measure elapsed time\n    batch_time.update(time.time() - end)\n    need_hour, need_mins, need_secs = convert_secs2time(batch_time.avg * (len(train_loader)-i-1))\n    last_time = \'[{:02d}:{:02d}:{:02d}]\'.format(need_hour, need_mins, need_secs)\n    end = time.time()\n\n    if i % opt.print_freq == 0 or i+1 == len(train_loader):\n      print_log(\'  Epoch: [{:03d}/{:03d}][{:03d}/{:03d}]  \'\n                \'Time {batch_time.val:5.2f} ({batch_time.avg:5.2f}) \'\n                \'Data {data_time.val:5.2f} ({data_time.avg:5.2f}) \'\n                \'Forward {forward_time.val:5.2f} ({forward_time.avg:5.2f}) \'\n                \'Loss {loss.val:6.3f} ({loss.avg:6.3f})  \'.format(\n                    epoch, opt.epochs, i, len(train_loader), batch_time=batch_time,\n                    data_time=data_time, forward_time=forward_time, loss=losses)\n                  + last_time + show_stage_loss(each_stage_loss_values) \\\n                  + \' In={} Tar={} Mask={}\'.format(convert_size2str(inputs.size()), convert_size2str(target.size()), convert_size2str(mask_var.size())) \\\n                  + \' Vis-PTS : {:2d} ({:.1f})\'.format(int(visible_points.val), visible_points.avg), log)\n\n    # Just for debug and show the intermediate results.\n    if opt.debug_save:\n      print_log(\'DEBUG --- > [{:03d}/{:03d}] \'.format(i, len(train_loader)), log)\n      main_debug_save(debug_save_dir, train_loader, image_index, inputs, batch_locs, target, points, sign_list, batch_cpms, generated, log)\n  \n  return losses.avg\n'"
SAN/lib/san_vision/__init__.py,0,"b'from .evaluation_util import Eval_Meta\nfrom .cpm_loss import compute_stage_loss, show_stage_loss, sum_stage_loss\n'"
SAN/lib/san_vision/common_eval.py,0,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nimport numpy as np\nimport pdb, os, time\nfrom utils.time_utils import print_log\nfrom datasets.dataset_utils import convert68to49, convert68to51\nfrom sklearn.metrics import auc\n\ndef evaluate_normalized_mean_error(predictions, groundtruth, log, extra_faces):\n  ## compute total average normlized mean error\n  assert len(predictions) == len(groundtruth), 'The lengths of predictions and ground-truth are not consistent : {} vs {}'.format( len(predictions), len(groundtruth) )\n  assert len(predictions) > 0, 'The length of predictions must be greater than 0 vs {}'.format( len(predictions) )\n  if extra_faces is not None: assert len(extra_faces) == len(predictions), 'The length of extra_faces is not right {} vs {}'.format( len(extra_faces), len(predictions) )\n  num_images = len(predictions)\n  for i in range(num_images):\n    c, g = predictions[i], groundtruth[i]\n    assert isinstance(c, np.ndarray) and isinstance(g, np.ndarray), 'The type of predictions is not right : [{:}] :: {} vs {} '.format(i, type(c), type(g))\n\n  num_points = predictions[0].shape[1]\n  error_per_image = np.zeros((num_images,1))\n  for i in range(num_images):\n    detected_points = predictions[i]\n    ground_truth_points = groundtruth[i]\n    if num_points == 68:\n      interocular_distance = np.linalg.norm(ground_truth_points[:2, 36] - ground_truth_points[:2, 45])\n      assert bool(ground_truth_points[2,36]) and bool(ground_truth_points[2,45])\n    elif num_points == 51 or num_points == 49:\n      interocular_distance = np.linalg.norm(ground_truth_points[:2, 19] - ground_truth_points[:2, 28])\n      assert bool(ground_truth_points[2,19]) and bool(ground_truth_points[2,28])\n    elif num_points == 19:\n      assert extra_faces is not None and extra_faces[i] is not None\n      interocular_distance = extra_faces[i]\n    else:\n      raise Exception('----> Unknown number of points : {}'.format(num_points))\n    dis_sum, pts_sum = 0, 0\n    for j in range(num_points):\n      if bool(ground_truth_points[2, j]):\n        dis_sum = dis_sum + np.linalg.norm(detected_points[:2, j] - ground_truth_points[:2, j])\n        pts_sum = pts_sum + 1\n    error_per_image[i] = dis_sum / (pts_sum*interocular_distance)\n\n  normalise_mean_error = error_per_image.mean()\n  # calculate the auc for 0.07\n  max_threshold = 0.07\n  threshold = np.linspace(0, max_threshold, num=2000)\n  accuracys = np.zeros(threshold.shape)\n  for i in range(threshold.size):\n    accuracys[i] = np.sum(error_per_image < threshold[i]) * 1.0 / error_per_image.size\n  area_under_curve07 = auc(threshold, accuracys) / max_threshold\n  # calculate the auc for 0.08\n  max_threshold = 0.08\n  threshold = np.linspace(0, max_threshold, num=2000)\n  accuracys = np.zeros(threshold.shape)\n  for i in range(threshold.size):\n    accuracys[i] = np.sum(error_per_image < threshold[i]) * 1.0 / error_per_image.size\n  area_under_curve08 = auc(threshold, accuracys) / max_threshold\n  \n  accuracy_under_007 = np.sum(error_per_image<0.07) * 100. / error_per_image.size\n  accuracy_under_008 = np.sum(error_per_image<0.08) * 100. / error_per_image.size\n\n  print_log('Compute NME and AUC for {:} images with {:} points :: [(nms): mean={:.2f}, std={:.2f}], auc@0.07={:.2f}, auc@0.08-{:.2f}, acc@0.07={:.2f}, acc@0.08={:.2f}'.format(num_images, num_points, normalise_mean_error*100, error_per_image.std()*100, area_under_curve07*100, area_under_curve08*100, accuracy_under_007, accuracy_under_008), log)\n\n  for_pck_curve = []\n  for x in range(0, 3501, 1):\n    error_bar = x * 0.0001\n    accuracy = np.sum(error_per_image < error_bar) * 1.0 / error_per_image.size\n    for_pck_curve.append((error_bar,accuracy))\n  \n  return normalise_mean_error, accuracy_under_008, for_pck_curve\n\ndef all_convert68to49(predictions, groundtruth, log):\n  assert len(predictions) == len(groundtruth), 'The lengths of predictions and ground-truth are not consistent : {} vs {}'.format( len(predictions), len(groundtruth) )\n  assert len(predictions) > 0, 'The length of predictions must be greater than 0 vs {}'.format( len(predictions) )\n  num_images = len(predictions)\n  new_predictions, new_groundtruth = [], []\n  for i in range(num_images):\n    c, g = convert68to49(predictions[i]), convert68to49(groundtruth[i])\n    assert isinstance(c, np.ndarray) and isinstance(g, np.ndarray), 'The type of predictions is not right : [{:}] :: {} vs {} '.format(i, type(c), type(g))\n    new_predictions.append( c )\n    new_groundtruth.append( g )\n  print_log('Convert {} images from 68 point -> 49 point'.format(num_images), log)\n  return new_predictions, new_groundtruth\n\ndef all_convert68to51(predictions, groundtruth, log):\n  assert len(predictions) == len(groundtruth), 'The lengths of predictions and ground-truth are not consistent : {} vs {}'.format( len(predictions), len(groundtruth) )\n  assert len(predictions) > 0, 'The length of predictions must be greater than 0 vs {}'.format( len(predictions) )\n  num_images = len(predictions)\n  new_predictions, new_groundtruth = [], []\n  for i in range(num_images):\n    c, g = convert68to51(predictions[i]), convert68to51(groundtruth[i])\n    assert isinstance(c, np.ndarray) and isinstance(g, np.ndarray), 'The type of predictions is not right : [{:}] :: {} vs {} '.format(i, type(c), type(g))\n    new_predictions.append( c )\n    new_groundtruth.append( g )\n  print_log('Convert {} images from 68 point -> 49 point'.format(num_images), log)\n  return new_predictions, new_groundtruth\n"""
SAN/lib/san_vision/cpm_loss.py,6,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nimport numpy as np\nimport numbers, torch\nimport torch.nn.functional as F\n\ndef compute_stage_loss(criterion, target_var, outputs, mask_var, total_labeled_cpm, weight_of_idt):\n\n  total_loss = 0\n  each_stage_loss = []\n  mask_outputs = []\n  for output_var in outputs:\n    stage_loss = 0\n    output = torch.masked_select(output_var, mask_var)\n    target = torch.masked_select(target_var, mask_var)\n    mask_outputs.append(output)\n\n    stage_loss = criterion(output, target) / (total_labeled_cpm*2)\n    total_loss = total_loss + stage_loss\n    each_stage_loss.append(stage_loss.item())\n  if weight_of_idt is not None and weight_of_idt > 0:\n    pair_loss_a = torch.sum( torch.abs(mask_outputs[0] - mask_outputs[1]) )\n    pair_loss_b = torch.sum( torch.abs(mask_outputs[0] - mask_outputs[2]) )\n    pair_loss_c = torch.sum( torch.abs(mask_outputs[1] - mask_outputs[2]) )\n    identity_loss = weight_of_idt * (pair_loss_a + pair_loss_b + pair_loss_c) / 3\n    each_stage_loss.append(identity_loss.item())\n    total_loss = total_loss + identity_loss\n  return total_loss, each_stage_loss\n\ndef show_stage_loss(each_stage_loss):\n  if each_stage_loss is None:            return 'None'\n  elif isinstance(each_stage_loss, str): return each_stage_loss\n  answer = ''\n  for index, loss in enumerate(each_stage_loss):\n    answer = answer + ' : L{:1d}={:6.3f}'.format(index+1, loss)\n  return answer\n\ndef sum_stage_loss(losses):\n  total_loss = None\n  each_stage_loss = []\n  for loss in losses:\n    if total_loss is None:\n      total_loss = loss\n    else:\n      total_loss = total_loss + loss\n    each_stage_loss.append(loss.item())\n  return total_loss, each_stage_loss\n"""
SAN/lib/san_vision/evaluation_util.py,2,"b""import os, time\nimport numpy as np\nfrom utils.time_utils import print_log\nimport torch\nimport json\nfrom collections import OrderedDict\nfrom scipy import interpolate\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom .common_eval import evaluate_normalized_mean_error\n\nclass Eval_Meta():\n\n  def __init__(self):\n    self.reset()\n\n  def __repr__(self):\n    return ('{name}'.format(name=self.__class__.__name__)+'(number of data = {})'.format(len(self)))\n\n  def reset(self):\n    self.predictions = []\n    self.groundtruth = []\n    self.image_lists = []\n    self.face_sizes  = []\n    self.mae_bars = [1, 2, 4, 8, 16, 32, 80]\n\n  def __len__(self):\n    return len(self.image_lists)\n\n  def append(self, _pred, _ground, image_path, face_size):\n    assert _pred.shape[0] == 3 and len(_pred.shape) == 2, 'Prediction\\'s shape is {} vs [should be (3,pts) or (2,pts)'.format(_pred.shape)\n    assert _pred.shape == _ground.shape, 'shapes must be the same : {} vs {}'.format(_pred.shape, _ground.shape)\n    if (not self.predictions) == False:\n      assert _pred.shape == self.predictions[-1].shape, 'shapes must be the same : {} vs {}'.format(_pred.shape, _ground.shape)\n    self.predictions.append(_pred)\n    self.groundtruth.append(_ground)\n    self.image_lists.append(image_path)\n    self.face_sizes.append(face_size)\n\n  def save(self, filename):\n    meta = { 'predictions': self.predictions, \n             'groundtruth': self.groundtruth,\n             'image_lists': self.image_lists,\n             'face_sizes': self.face_sizes,\n             'mae_bars':    self.mae_bars}\n    torch.save(meta, filename)\n    print ('save Eval_Meta into {}'.format(filename))\n\n  def load(self, filename):\n    assert os.path.isfile(filename), '{} is not a file'.format(filename)\n    checkpoint       = torch.load(filename)\n    self.predictions = checkpoint['predictions']\n    self.groundtruth = checkpoint['groundtruth']\n    self.image_lists = checkpoint['image_lists']\n    self.face_sizes  = checkpoint['face_sizes']\n    self.mae_bars    = checkpoint['mae_bars']\n\n  def compute_mse(self, log, return_curve=False):\n    nme, auc, for_pck_curve = evaluate_normalized_mean_error(self.predictions, self.groundtruth, log, self.face_sizes)\n    if return_curve: return nme, auc, for_pck_curve\n    else:            return nme, auc\n"""
SAN/lib/san_vision/transforms.py,10,"b'##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom __future__ import division\nimport torch\nimport sys, math, random, PIL\nfrom PIL import Image, ImageOps\nimport numpy as np\nimport numbers\nimport types\nimport collections\n\nif sys.version_info.major == 2:\n  import cPickle as pickle\nelse:\n  import pickle\n\n\nclass Compose(object):\n  def __init__(self, transforms):\n    self.transforms = transforms\n\n  def __call__(self, img, points):\n    for t in self.transforms:\n      img, points = t(img, points)\n    return img, points\n\n\nclass TrainScale2WH(object):\n  """"""Rescale the input PIL.Image to the given size.\n  Args:\n    size (sequence or int): Desired output size. If size is a sequence like\n      (w, h), output size will be matched to this. If size is an int,\n      smaller edge of the image will be matched to this number.\n      i.e, if height > width, then image will be rescaled to\n      (size * height / width, size)\n    interpolation (int, optional): Desired interpolation. Default is\n      ``PIL.Image.BILINEAR``\n  """"""\n\n  def __init__(self, target_size, interpolation=Image.BILINEAR):\n    assert isinstance(target_size, tuple) or isinstance(target_size, list), \'The type of target_size is not right : {}\'.format(target_size)\n    assert len(target_size) == 2, \'The length of target_size is not right : {}\'.format(target_size)\n    assert isinstance(target_size[0], int) and isinstance(target_size[1], int), \'The type of target_size is not right : {}\'.format(target_size)\n    self.target_size   = target_size\n    self.interpolation = interpolation\n\n  def __call__(self, imgs, point_meta):\n    """"""\n    Args:\n      img (PIL.Image): Image to be scaled.\n      points 3 * N numpy.ndarray [x, y, visiable]\n    Returns:\n      PIL.Image: Rescaled image.\n    """"""\n    point_meta = point_meta.copy()\n\n    if isinstance(imgs, list): is_list = True\n    else:                      is_list, imgs = False, [imgs]\n    \n    w, h = imgs[0].size\n    ow, oh = self.target_size[0], self.target_size[1]\n    point_meta.apply_scale( [ow*1./w, oh*1./h] )\n\n    imgs = [ img.resize((ow, oh), self.interpolation) for img in imgs ]\n    if is_list == False: imgs = imgs[0]\n\n    return imgs, point_meta\n\n\n\nclass ToPILImage(object):\n  """"""Convert a tensor to PIL Image.\n  Converts a torch.*Tensor of shape C x H x W or a numpy ndarray of shape\n  H x W x C to a PIL.Image while preserving the value range.\n  """"""\n\n  def __call__(self, pic):\n    """"""\n    Args:\n      pic (Tensor or numpy.ndarray): Image to be converted to PIL.Image.\n    Returns:\n      PIL.Image: Image converted to PIL.Image.\n    """"""\n    npimg = pic\n    mode = None\n    if isinstance(pic, torch.FloatTensor):\n      pic = pic.mul(255).byte()\n    if torch.is_tensor(pic):\n      npimg = np.transpose(pic.numpy(), (1, 2, 0))\n    assert isinstance(npimg, np.ndarray), \'pic should be Tensor or ndarray\'\n    if npimg.shape[2] == 1:\n      npimg = npimg[:, :, 0]\n\n      if npimg.dtype == np.uint8:\n        mode = \'L\'\n      if npimg.dtype == np.int16:\n        mode = \'I;16\'\n      if npimg.dtype == np.int32:\n        mode = \'I\'\n      elif npimg.dtype == np.float32:\n        mode = \'F\'\n    else:\n      if npimg.dtype == np.uint8:\n        mode = \'RGB\'\n    assert mode is not None, \'{} is not supported\'.format(npimg.dtype)\n    return Image.fromarray(npimg, mode=mode)\n\n\n\nclass ToTensor(object):\n  """"""Convert a ``PIL.Image`` or ``numpy.ndarray`` to tensor.\n  Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n  [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n  """"""\n\n  def __call__(self, pics, points):\n    """"""\n    Args:\n      pic (PIL.Image or numpy.ndarray): Image to be converted to tensor.\n      points 3 * N numpy.ndarray [x, y, visiable] or Point_Meta\n    Returns:\n      Tensor: Converted image.\n    """"""\n    ## add to support list\n    if isinstance(pics, list): is_list = True\n    else:                      is_list, pics = False, [pics]\n\n    returned = []\n    for pic in pics:\n      if isinstance(pic, np.ndarray):\n        # handle numpy array\n        img = torch.from_numpy(pic.transpose((2, 0, 1)))\n        # backward compatibility\n        returned.append( img.float().div(255) )\n        continue\n\n      # handle PIL Image\n      if pic.mode == \'I\':\n        img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n      elif pic.mode == \'I;16\':\n        img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n      else:\n        img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n      # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n      if pic.mode == \'YCbCr\':\n        nchannel = 3\n      elif pic.mode == \'I;16\':\n        nchannel = 1\n      else:\n        nchannel = len(pic.mode)\n      img = img.view(pic.size[1], pic.size[0], nchannel)\n      # put it from HWC to CHW format\n      # yikes, this transpose takes 80% of the loading time/CPU\n      img = img.transpose(0, 1).transpose(0, 2).contiguous()\n      if isinstance(img, torch.ByteTensor):\n        img = img.float().div(255)\n      returned.append(img)\n\n    if is_list == False:\n      assert len(returned) == 1, \'For non-list data, length of answer must be one not {}\'.format(len(returned))\n      returned = returned[0]\n\n    return returned, points\n\n\nclass Normalize(object):\n  """"""Normalize an tensor image with mean and standard deviation.\n  Given mean: (R, G, B) and std: (R, G, B),\n  will normalize each channel of the torch.*Tensor, i.e.\n  channel = (channel - mean) / std\n  Args:\n    mean (sequence): Sequence of means for R, G, B channels respecitvely.\n    std (sequence): Sequence of standard deviations for R, G, B channels\n      respecitvely.\n  """"""\n\n  def __init__(self, mean, std):\n    self.mean = mean\n    self.std = std\n\n  def __call__(self, tensors, points):\n    """"""\n    Args:\n      tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n    Returns:\n      Tensor: Normalized image.\n    """"""\n    # TODO: make efficient\n    if isinstance(tensors, list): is_list = True\n    else:                         is_list, tensors = False, [tensors]\n\n    for tensor in tensors:\n      for t, m, s in zip(tensor, self.mean, self.std):\n        t.sub_(m).div_(s)\n    \n    if is_list == False: tensors = tensors[0]\n\n    return tensors, points\n\n\nclass PreCrop(object):\n  """"""Crops the given PIL.Image at the center.\n\n  Args:\n    size (sequence or int): Desired output size of the crop. If size is an\n      int instead of sequence like (w, h), a square crop (size, size) is\n      made.\n  """"""\n\n  def __init__(self, expand_ratio):\n    assert expand_ratio is None or isinstance(expand_ratio, numbers.Number), \'The expand_ratio should not be {}\'.format(expand_ratio)\n    if expand_ratio is None:\n      self.expand_ratio = 0\n    else:\n      self.expand_ratio = expand_ratio\n    assert self.expand_ratio >= 0, \'The expand_ratio should not be {}\'.format(expand_ratio)\n\n  def __call__(self, imgs, point_meta):\n    ## AugCrop has something wrong... For unsupervised data\n\n    if isinstance(imgs, list): is_list = True\n    else:                      is_list, imgs = False, [imgs]\n\n    w, h = imgs[0].size\n    box = point_meta.get_box().tolist()\n    face_ex_w, face_ex_h = (box[2] - box[0]) * self.expand_ratio, (box[3] - box[1]) * self.expand_ratio\n    x1, y1 = int(max(math.floor(box[0]-face_ex_w), 0)), int(max(math.floor(box[1]-face_ex_h), 0))\n    x2, y2 = int(min(math.ceil(box[2]+face_ex_w), w)), int(min(math.ceil(box[3]+face_ex_h), h))\n    \n    imgs = [ img.crop((x1, y1, x2, y2)) for img in imgs ]\n    point_meta.set_precrop_wh( imgs[0].size[0], imgs[0].size[1], x1, y1, x2, y2)\n    point_meta.apply_offset(-x1, -y1)\n    point_meta.apply_bound(imgs[0].size[0], imgs[0].size[1])\n\n    if is_list == False: imgs = imgs[0]\n    return imgs, point_meta\n\n\nclass AugScale(object):\n  """"""Rescale the input PIL.Image to the given size.\n\n  Args:\n    size (sequence or int): Desired output size. If size is a sequence like\n      (w, h), output size will be matched to this. If size is an int,\n      smaller edge of the image will be matched to this number.\n      i.e, if height > width, then image will be rescaled to\n      (size * height / width, size)\n    interpolation (int, optional): Desired interpolation. Default is\n      ``PIL.Image.BILINEAR``\n  """"""\n\n  def __init__(self, scale_prob, scale_min, scale_max, interpolation=Image.BILINEAR):\n    assert isinstance(scale_prob, numbers.Number) and scale_prob >= 0\n    assert isinstance(scale_min,  numbers.Number) and isinstance(scale_max, numbers.Number)\n    self.scale_prob = scale_prob\n    self.scale_min  = scale_min\n    self.scale_max  = scale_max\n    self.interpolation = interpolation\n\n  def __call__(self, imgs, point_meta):\n    """"""\n    Args:\n      img (PIL.Image): Image to be scaled.\n      points 3 * N numpy.ndarray [x, y, visiable]\n    Returns:\n      PIL.Image: Rescaled image.\n    """"""\n    point_meta = point_meta.copy()\n\n    dice = random.random()\n    if dice > self.scale_prob:\n      return imgs, point_meta\n\n    if isinstance(imgs, list): is_list = True\n    else:                      is_list, imgs = False, [imgs]\n    \n    scale_multiplier = (self.scale_max - self.scale_min) * random.random() + self.scale_min\n    \n    w, h = imgs[0].size\n    ow, oh = int(w * scale_multiplier), int(h * scale_multiplier)\n\n    imgs = [ img.resize((ow, oh), self.interpolation) for img in imgs ]\n    point_meta.apply_scale( [scale_multiplier] )\n\n    if is_list == False: imgs = imgs[0]\n\n    return imgs, point_meta\n\n\nclass AugCrop(object):\n\n  def __init__(self, crop_x, crop_y, center_perterb_max, fill=0):\n    assert isinstance(crop_x, int) and isinstance(crop_y, int) and isinstance(center_perterb_max, numbers.Number)\n    self.crop_x = crop_x\n    self.crop_y = crop_y\n    self.center_perterb_max = center_perterb_max\n    assert isinstance(fill, numbers.Number) or isinstance(fill, str) or isinstance(fill, tuple)\n    self.fill   = fill\n\n  def __call__(self, imgs, point_meta=None):\n    ## AugCrop has something wrong... For unsupervised data\n  \n    point_meta = point_meta.copy()\n    if isinstance(imgs, list): is_list = True\n    else:                      is_list, imgs = False, [imgs]\n\n    dice_x, dice_y = random.random(), random.random()\n    x_offset = int( (dice_x-0.5) * 2 * self.center_perterb_max)\n    y_offset = int( (dice_y-0.5) * 2 * self.center_perterb_max)\n    \n    x1 = int(round( point_meta.center[0] + x_offset - self.crop_x / 2. ))\n    y1 = int(round( point_meta.center[1] + y_offset - self.crop_y / 2. ))\n    x2 = x1 + self.crop_x\n    y2 = y1 + self.crop_y\n\n    w, h = imgs[0].size\n    if x1 < 0 or y1 < 0 or x2 >= w or y2 >= h:\n      pad = max(0-x1, 0-y1, x2-w+1, y2-h+1)\n      assert pad > 0, \'padding operation in crop must be greater than 0\'\n      imgs = [ ImageOps.expand(img, border=pad, fill=self.fill) for img in imgs ]\n      x1, x2, y1, y2 = x1 + pad, x2 + pad, y1 + pad, y2 + pad\n      point_meta.apply_offset(pad, pad)\n      point_meta.apply_bound(imgs[0].size[0], imgs[0].size[1])\n\n    point_meta.apply_offset(-x1, -y1)\n    imgs = [ img.crop((x1, y1, x2, y2)) for img in imgs ]\n    point_meta.apply_bound(imgs[0].size[0], imgs[0].size[1])\n\n    if is_list == False: imgs = imgs[0]\n    return imgs, point_meta\n\n\nclass AugRotate(object):\n  """"""Rotate the given PIL.Image at the center.\n  Args:\n    size (sequence or int): Desired output size of the crop. If size is an\n      int instead of sequence like (w, h), a square crop (size, size) is\n      made.\n  """"""\n\n  def __init__(self, max_rotate_degree):\n    assert isinstance(max_rotate_degree, numbers.Number)\n    self.max_rotate_degree = max_rotate_degree\n\n  def __call__(self, imgs, point_meta):\n    """"""\n    Args:\n      img (PIL.Image): Image to be cropped.\n      point_meta : Point_Meta\n    Returns:\n      PIL.Image: Rotated image.\n    """"""\n    point_meta = point_meta.copy()\n    if isinstance(imgs, list): is_list = True\n    else:                      is_list, imgs = False, [imgs]\n\n    degree = (random.random() - 0.5) * 2 * self.max_rotate_degree\n    center = (imgs[0].size[0] / 2, imgs[0].size[1] / 2)\n    if PIL.__version__[0] == \'4\':\n      imgs = [ img.rotate(degree, center=center) for img in imgs ]\n    else:\n      imgs = [ img.rotate(degree) for img in imgs ]\n\n    point_meta.apply_rotate(center, degree)\n    point_meta.apply_bound(imgs[0].size[0], imgs[0].size[1])\n\n    if is_list == False: imgs = imgs[0]\n\n    return imgs, point_meta\n'"
SAN/lib/utils/__init__.py,0,"b'from .pts_utils import find_all_peaks, find_batch_peaks\nfrom .pts_utils import find_peaks_v1\nfrom .time_utils import time_for_file, time_string, time_string_short, time_print\nfrom .time_utils import AverageMeter, LossRecorderMeter, convert_secs2time, print_log\nfrom .file_utils     import load_list_from_folders, load_txt_file\nfrom .pts_utils      import generate_label_map_gaussian\nfrom .pts_utils      import generate_label_map_laplacian\nfrom .time_utils     import convert_size2str\nfrom .flop_benchmark import get_model_infos, count_parameters_in_MB\n\nfrom .stn_utils      import crop2affine\n'"
SAN/lib/utils/box_utils.py,0,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nimport numpy as np\n\ndef bboxcheck_TLBR(bbox):\n    '''\n    check the input bounding box to be TLBR format\n\n    parameter:\n        bbox:   N x 4 numpy array, TLBR format\n    \n    return:\n        True or False\n    '''\n    OK1 = isinstance(bbox, np.ndarray) and bbox.shape[1] == 4 and bbox.shape[0] > 0\n    OK2 = (bbox[:, 3] >= bbox[:, 1]).all() and (bbox[:, 2] >= bbox[:, 0]).all()\n    return OK1 and OK2\n\ndef bbox2center(bbox):\n    '''\n    convert a bounding box to a point, which is the center of this bounding box\n\n    parameter:\n        bbox:   N x 4 numpy array, TLBR format\n\n    return:\n        center: 2 x N numpy array, x and y correspond to first and second row respectively\n    '''\n    assert bboxcheck_TLBR(bbox), 'the input bounding box should be TLBR format'\n\n    num_bbox = bbox.shape[0]        \n    center = np.zeros((num_bbox, 2), dtype='float32')\n    center[:, 0] = (bbox[:, 0] + bbox[:, 2]) / 2.\n    center[:, 1] = (bbox[:, 1] + bbox[:, 3]) / 2.\n\n    return np.transpose(center)\n\ndef bbox_TLBR2TLWH(bbox):\n    '''\n    transform the input bounding box with TLBR format to TLWH format\n\n    parameter:\n        bbox: N X 4 numpy array, TLBR format\n\n    return \n        bbox: N X 4 numpy array, TLWH format\n    '''\n    assert bboxcheck_TLBR(bbox), 'the input bounding box should be TLBR format'\n\n    bbox_TLWH = np.zeros_like(bbox)\n    bbox_TLWH[:, 0] = bbox[:, 0]\n    bbox_TLWH[:, 1] = bbox[:, 1]\n    bbox_TLWH[:, 2] = bbox[:, 2] - bbox[:, 0]\n    bbox_TLWH[:, 3] = bbox[:, 3] - bbox[:, 1]\n    return bbox_TLWH\n"""
SAN/lib/utils/config.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Fast R-CNN config system.\n\nThis file specifies default config options for Fast R-CNN. You should not\nchange values in this file. Instead, you should write a config file (in yaml)\nand use cfg_from_file(yaml_file) to load it and override the default options.\n\nMost tools in $ROOT/tools take a --cfg option to specify an override file.\n    - See tools/{train,test}_net.py for example code that uses cfg_from_file()\n    - See experiments/cfgs/*.yml for example YAML config override files\n""""""\n\nimport os\nimport os.path as osp\nimport numpy as np\nfrom easydict import EasyDict as edict\n\n__C = edict()\n# Consumers can get config by:\n#   from fast_rcnn_config import cfg\ncfg = __C\n\n#\n# Training options\n#\n\n__C.TRAIN = edict()\n\n__C.DEBUG = False\n__C.VIS   = False\n\n#\n# Testing options\n#\n__C.num_levels_scale = 1\n__C.scale_starting = 0.8\n__C.scale_ending   = 1.2\n\n__C.TEST = edict()\n__C.TEST.rescale_height = 1280\n__C.TEST.rescale_width  = 960\n# percentage of max value in the heat-map for smoothing\n__C.TEST.heatmap_smooth_percentage = 0.4\n# visualize the detected landmark only when scores are larger than this threshold\n__C.TEST.visualization_threshold = 0.2\n__C.TEST.minimum_width = 512\n# use average heat-map from multi-scale\n__C.TEST.heatmap_merge = \'avg\'\n# choose best points location from multi-scale\n__C.TEST.pts_merge_scale = \'max\'\n# choose best points location from multiple peaks found (max or clustering)\n__C.TEST.pts_merge_peak = \'max\'\n# tolerance of distance during clustering peak points found\n__C.TEST.cluster_bandwidth = 10\n__C.TEST.padValue = 128\n__C.TEST.mean_value = 0.5\n\n# Root directory of project\n__C.ROOT_DIR = osp.abspath(osp.join(osp.dirname(__file__), \'..\', \'..\'))\n\n#\n# Path Settings\n__C.DATA_300VW = osp.abspath(osp.join(os.environ[\'HOME\'], \'datasets\', \'300VW\'))\n__C.DATA_300W = osp.abspath(osp.join(os.environ[\'HOME\'], \'datasets\', \'300VW\', \'300W-ALL\'))\n__C.DATA_LOCAL_300W = osp.abspath(osp.join(__C.ROOT_DIR, \'datasets\', \'300-W\'))\n\n__C.MUGSY = edict()\n__C.MUGSY.FullFace = osp.abspath(osp.join(os.environ[\'HOME\'], \'datasets\', \'MUGSY\', \'full_face\', \'face\'))\n__C.LOCAL_MUGSY = edict()\n__C.LOCAL_MUGSY.FullFace = osp.abspath(osp.join(__C.ROOT_DIR, \'datasets\', \'MUGSY\'))\n__C.caffe_path = osp.abspath(osp.join(__C.ROOT_DIR, \'caffe\'))\n\n#\n# Models\n__C.model_pretrained = osp.abspath(osp.join(os.environ[\'HOME\'], \'datasets\', \'model_pretrained\'))\n__C.TRAIN.model_save_dir   = osp.abspath(osp.join(os.environ[\'HOME\'], \'datasets\', \'model_saved\'))\n\n#\n# For temporal visual path\n__C.INT_MAX_CAFFE = 11184000\n\n#\n# MISC\n#\n\n# The mapping from image coordinates to feature map coordinates might cause\n# some boxes that are distinct in image space to become identical in feature\n# coordinates. If DEDUP_BOXES > 0, then DEDUP_BOXES is used as the scale factor\n# for identifying duplicate boxes.\n# 1/16 is correct for {Alex,Caffe}Net, VGG_CNN_M_1024, and VGG16\n__C.DEDUP_BOXES = 1./16.\n'"
SAN/lib/utils/convert_utils.py,0,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nimport numpy as np\nimport struct\n\ndef isscalar(scalar_test):\n    return isinstance(scalar_test, int) or isinstance(scalar_test, float)\n\ndef scalar_list2float_list(scalar_list):\n  '''\n  remove an empty string from a list\n  '''\n  assert isinstance(scalar_list, list) and all(isscalar(scalar_tmp) for scalar_tmp in scalar_list), 'input list is not a scalar list'\n  float_list = list()\n  for item in scalar_list:\n    float_list.append(float(item))\n  return float_list \n\ndef float_list2bytes(float_list):\n  '''\n  convert a float number to a set of bytes\n  '''\n  assert isinstance(float_list, float) or (isinstance(float_list, list) and all(isinstance(float_tmp, float) for float_tmp in float_list)), 'input is not a floating number or a list of floating number' \n  # convert a single floating number to a list with one item \n  if isinstance(float_list, float):\n    float_list = [float_list]\n  try:\n    binary = struct.pack('%sf' % len(float_list), *float_list)\n  except ValueError:\n    print('Warnings!!!! Failed to convert to bytes!!!!!') \n  return binary  \n"""
SAN/lib/utils/file_utils.py,0,"b""import os, sys, glob, numbers\nfrom os import path as osp\n\ndef mkdir_if_missing(path):\n  if not osp.isdir(path):\n    os.makedirs(path)\n\ndef is_path_exists(pathname):                                                                                                                                                                          \n  try:\n    return isinstance(pathname, str) and pathname and os.path.exists(pathname) \n  except OSError:\n    return False\n\ndef fileparts(pathname):\n  '''\n  this function return a tuple, which contains (directory, filename, extension)\n  if the file has multiple extension, only last one will be displayed\n  '''\n  pathname = osp.normpath(pathname)\n  if len(pathname) == 0:\n    return ('', '', '')\n  if pathname[-1] == '/':\n    if len(pathname) > 1:\n      return (pathname[:-1], '', '')  # ignore the final '/'\n    else:\n      return (pathname, '', '') # ignore the final '/'\n  directory = osp.dirname(osp.abspath(pathname))\n  filename  = osp.splitext(osp.basename(pathname))[0]\n  ext       = osp.splitext(pathname)[1]\n  return (directory, filename, ext)\n\ndef load_txt_file(file_path):\n  '''\n  load data or string from text file.\n  '''\n  file_path = osp.normpath(file_path)\n  assert is_path_exists(file_path), 'text file is not existing!'\n\n  with open(file_path, 'r') as file:\n    data = file.read().splitlines()\n  num_lines = len(data)\n  file.close()\n\n  return data, num_lines\n\n\ndef load_list_from_folder(folder_path, ext_filter=None, depth=1):\n  '''\n  load a list of files or folders from a system path\n\n  parameter:\n    folder_path: root to search \n    ext_filter: a string to represent the extension of files interested\n    depth: maximum depth of folder to search, when it's None, all levels of folders will be searched\n  '''\n  folder_path = osp.normpath(folder_path)\n  assert isinstance(depth, int) , 'input depth is not correct {}'.format(depth)\n  assert ext_filter is None or (isinstance(ext_filter, list) and all(isinstance(ext_tmp, str) for ext_tmp in ext_filter)) or isinstance(ext_filter, str), 'extension filter is not correct'\n  if isinstance(ext_filter, str):    # convert to a list\n    ext_filter = [ext_filter]\n\n  fulllist = list()\n  wildcard_prefix = '*'\n  for index in range(depth):\n    if ext_filter is not None:\n      for ext_tmp in ext_filter:\n        curpath = osp.join(folder_path, wildcard_prefix + '.' + ext_tmp)\n        fulllist += glob.glob(curpath)\n    else:\n      curpath = osp.join(folder_path, wildcard_prefix)\n      fulllist += glob.glob(curpath)\n    wildcard_prefix = osp.join(wildcard_prefix, '*')\n\n  fulllist = [osp.normpath(path_tmp) for path_tmp in fulllist]\n  num_elem = len(fulllist)\n\n  return fulllist, num_elem\n\ndef load_list_from_folders(folder_path_list, ext_filter=None, depth=1):\n  '''\n  load a list of files or folders from a list of system path\n  '''\n  assert isinstance(folder_path_list, list) or isinstance(folder_path_list, str), 'input path list is not correct'\n  if isinstance(folder_path_list, str):\n    folder_path_list = [folder_path_list]\n\n  fulllist = list()\n  num_elem = 0\n  for folder_path_tmp in folder_path_list:\n    fulllist_tmp, num_elem_tmp = load_list_from_folder(folder_path_tmp, ext_filter=ext_filter, depth=depth)\n    fulllist += fulllist_tmp\n    num_elem += num_elem_tmp\n\n  return fulllist, num_elem\n"""
SAN/lib/utils/flop_benchmark.py,15,"b'##################################################\n# Copyright (c) Xuanyi Dong [GitHub D-X-Y], 2019 #\n##################################################\n# modified from https://github.com/warmspringwinds/pytorch-segmentation-detection/blob/master/pytorch_segmentation_detection/utils/flops_benchmark.py\nimport copy, torch\nimport torch.nn as nn\nimport numpy as np\n\n\ndef count_parameters_in_MB(model):\n  if isinstance(model, nn.Module):\n    return np.sum(np.prod(v.size()) for v in model.parameters())/1e6\n  else:\n    return np.sum(np.prod(v.size()) for v in model)/1e6\n\n\ndef get_model_infos(model, shape, cache_inputs):\n  #model = copy.deepcopy( model )\n\n  model = add_flops_counting_methods(model)\n  #model = model.cuda()\n  model.eval()\n\n  #cache_inputs = torch.zeros(*shape).cuda()\n  if shape is None:\n    cache_inputs = cache_inputs\n  else:\n    cache_inputs = torch.zeros(*shape)\n  if next(model.parameters()).is_cuda: cache_inputs = cache_inputs.cuda()\n  #print_log(\'In the calculating function : cache input size : {:}\'.format(cache_inputs.size()), log)\n  with torch.no_grad():\n    _____ = model(cache_inputs)\n  FLOPs = compute_average_flops_cost( model ) / 1e6\n  Param = count_parameters_in_MB(model)\n\n  if hasattr(model, \'auxiliary_param\'):\n    aux_params = count_parameters_in_MB(model.auxiliary_param()) \n    print (\'The auxiliary params of this model is : {:}\'.format(aux_params))\n    print (\'We remove the auxiliary params from the total params ({:}) when counting\'.format(Param))\n    Param = Param - aux_params\n  \n  #print_log(\'FLOPs : {:} MB\'.format(FLOPs), log)\n  torch.cuda.empty_cache()\n  model.apply( remove_hook_function )\n  return FLOPs, Param\n\n\n# ---- Public functions\ndef add_flops_counting_methods( model ):\n  model.__batch_counter__ = 0\n  add_batch_counter_hook_function( model )\n  model.apply( add_flops_counter_variable_or_reset )\n  model.apply( add_flops_counter_hook_function )\n  return model\n\n\n\ndef compute_average_flops_cost(model):\n  """"""\n  A method that will be available after add_flops_counting_methods() is called on a desired net object.\n  Returns current mean flops consumption per image.\n  """"""\n  batches_count = model.__batch_counter__\n  flops_sum = 0\n  #or isinstance(module, torch.nn.AvgPool2d) or isinstance(module, torch.nn.MaxPool2d) \\\n  for module in model.modules():\n    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear) \\\n      or isinstance(module, torch.nn.Conv1d) \\\n      or hasattr(module, \'calculate_flop_self\'):\n      flops_sum += module.__flops__\n  return flops_sum / batches_count\n\n\n# ---- Internal functions\ndef pool_flops_counter_hook(pool_module, inputs, output):\n  batch_size = inputs[0].size(0)\n  kernel_size = pool_module.kernel_size\n  out_C, output_height, output_width = output.shape[1:]\n  assert out_C == inputs[0].size(1), \'{:} vs. {:}\'.format(out_C, inputs[0].size())\n\n  overall_flops = batch_size * out_C * output_height * output_width * kernel_size * kernel_size\n  pool_module.__flops__ += overall_flops\n\n\ndef self_calculate_flops_counter_hook(self_module, inputs, output):\n  overall_flops = self_module.calculate_flop_self(inputs[0].shape, output.shape)\n  self_module.__flops__ += overall_flops\n\n\ndef fc_flops_counter_hook(fc_module, inputs, output):\n  batch_size = inputs[0].size(0)\n  xin, xout = fc_module.in_features, fc_module.out_features\n  assert xin == inputs[0].size(1) and xout == output.size(1), \'IO=({:}, {:})\'.format(xin, xout)\n  overall_flops = batch_size * xin * xout\n  if fc_module.bias is not None:\n    overall_flops += batch_size * xout\n  fc_module.__flops__ += overall_flops\n\n\ndef conv1d_flops_counter_hook(conv_module, inputs, outputs):\n  batch_size   = inputs[0].size(0)\n  outL         = outputs.shape[-1]\n  [kernel]     = conv_module.kernel_size\n  in_channels  = conv_module.in_channels\n  out_channels = conv_module.out_channels\n  groups       = conv_module.groups\n  conv_per_position_flops = kernel * in_channels * out_channels / groups\n  \n  active_elements_count = batch_size * outL \n  overall_flops = conv_per_position_flops * active_elements_count\n\n  if conv_module.bias is not None:\n    overall_flops += out_channels * active_elements_count\n  conv_module.__flops__ += overall_flops\n\n\ndef conv2d_flops_counter_hook(conv_module, inputs, output):\n  batch_size = inputs[0].size(0)\n  output_height, output_width = output.shape[2:]\n  \n  kernel_height, kernel_width = conv_module.kernel_size\n  in_channels  = conv_module.in_channels\n  out_channels = conv_module.out_channels\n  groups       = conv_module.groups\n  conv_per_position_flops = kernel_height * kernel_width * in_channels * out_channels / groups\n  \n  active_elements_count = batch_size * output_height * output_width\n  overall_flops = conv_per_position_flops * active_elements_count\n    \n  if conv_module.bias is not None:\n    overall_flops += out_channels * active_elements_count\n  conv_module.__flops__ += overall_flops\n\n  \ndef batch_counter_hook(module, inputs, output):\n  # Can have multiple inputs, getting the first one\n  inputs = inputs[0]\n  batch_size = inputs.shape[0]\n  module.__batch_counter__ += batch_size\n\n\ndef add_batch_counter_hook_function(module):\n  if not hasattr(module, \'__batch_counter_handle__\'):\n    handle = module.register_forward_hook(batch_counter_hook)\n    module.__batch_counter_handle__ = handle\n\n  \ndef add_flops_counter_variable_or_reset(module):\n  if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear) \\\n    or isinstance(module, torch.nn.Conv1d) \\\n    or isinstance(module, torch.nn.AvgPool2d) or isinstance(module, torch.nn.MaxPool2d) \\\n    or hasattr(module, \'calculate_flop_self\'):\n    module.__flops__ = 0\n\n\ndef add_flops_counter_hook_function(module):\n  if isinstance(module, torch.nn.Conv2d):\n    if not hasattr(module, \'__flops_handle__\'):\n      handle = module.register_forward_hook(conv2d_flops_counter_hook)\n      module.__flops_handle__ = handle\n  elif isinstance(module, torch.nn.Conv1d):\n    if not hasattr(module, \'__flops_handle__\'):\n      handle = module.register_forward_hook(conv1d_flops_counter_hook)\n      module.__flops_handle__ = handle\n  elif isinstance(module, torch.nn.Linear):\n    if not hasattr(module, \'__flops_handle__\'):\n      handle = module.register_forward_hook(fc_flops_counter_hook)\n      module.__flops_handle__ = handle\n  elif isinstance(module, torch.nn.AvgPool2d) or isinstance(module, torch.nn.MaxPool2d):\n    if not hasattr(module, \'__flops_handle__\'):\n      handle = module.register_forward_hook(pool_flops_counter_hook)\n      module.__flops_handle__ = handle\n  elif hasattr(module, \'calculate_flop_self\'): # self-defined module\n    if not hasattr(module, \'__flops_handle__\'):\n      handle = module.register_forward_hook(self_calculate_flops_counter_hook)\n      module.__flops_handle__ = handle\n\n\ndef remove_hook_function(module):\n  hookers = [\'__batch_counter_handle__\', \'__flops_handle__\']\n  for hooker in hookers:\n    if hasattr(module, hooker):\n      handle = getattr(module, hooker)\n      handle.remove()\n  keys = [\'__flops__\', \'__batch_counter__\', \'__flops__\'] + hookers\n  for ckey in keys:\n    if hasattr(module, ckey): delattr(module, ckey)\n'"
SAN/lib/utils/image_pool.py,3,"b'import random\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nclass ImagePool():\n  def __init__(self, pool_size):\n    self.pool_size = pool_size\n    if self.pool_size > 0:\n      self.num_imgs = 0\n      self.images = []\n\n  def query(self, images):\n    if self.pool_size == 0:\n      return images\n    return_images = []\n    for image in images.data:\n      image = torch.unsqueeze(image, 0)\n      if self.num_imgs < self.pool_size:\n        self.num_imgs = self.num_imgs + 1\n        self.images.append(image)\n        return_images.append(image)\n      else:\n        p = random.uniform(0, 1)\n        if p > 0.5:\n          random_id = random.randint(0, self.pool_size-1)\n          tmp = self.images[random_id].clone()\n          self.images[random_id] = image\n          return_images.append(tmp)\n        else:\n          return_images.append(image)\n    return_images = Variable(torch.cat(return_images, 0))\n    return return_images\n\n'"
SAN/lib/utils/image_utils.py,0,"b""import numpy as np\nimport os\n\n## bbox is a vector has two values\ndef padHeight(image, padValue, bbox):\n  assert isinstance(image,np.ndarray), 'incorrect type : {}'.format(type(image))\n  assert len(image.shape) == 3 and image.shape[2] == 3, 'incorrect shape : {}'.format(image.shape)\n  height, width = image.shape[0], image.shape[1]\n  bbox[0] = np.max ( [ bbox[0], height] )\n  bbox[0] = np.ceil( bbox[0] / 8.0 ) * 8\n  #print ('----------- h/w : {} , bbox : {}'.format(image.shape, bbox))\n  bbox[1] = np.max ( [ bbox[1], width ] )\n  #print ('----------- h/w : {} , bbox : {}'.format(image.shape, bbox))\n  bbox[1] = np.ceil( bbox[1] / 8.0 ) * 8 \n  #print ('----------- h/w : {} , bbox : {}'.format(image.shape, bbox))\n\n  pad = [ np.floor((bbox[0]-height)/2), np.floor((bbox[1]-width)/2) ] # up, left\n  pad.append( bbox[0] - height - pad[0] ) # down\n  pad.append( bbox[1] - width  - pad[1] ) # right\n  pad = np.array( pad, dtype='int')\n\n  img_padded = image.copy()\n  pad_up     = np.zeros( (pad[0], img_padded.shape[1], image.shape[2]), dtype='float32') + padValue\n  img_padded = np.concatenate( (pad_up,   img_padded), axis=0).astype(image.dtype)\n  #print ('pad_up    shape : {}, img_padded shape : {}'.format(pad_up.shape,   img_padded.shape))\n  pad_left   = np.zeros( (img_padded.shape[0], pad[1], image.shape[2]), dtype='float32') + padValue\n  img_padded = np.concatenate( (pad_left, img_padded), axis=1).astype(image.dtype)\n  #print ('pad_left  shape : {}, img_padded shape : {}'.format(pad_left.shape, img_padded.shape))\n  pad_down   = np.zeros( (pad[2], img_padded.shape[1], image.shape[2]), dtype='float32') + padValue\n  img_padded = np.concatenate( (img_padded, pad_down), axis=0)\n  #print ('pad_down  shape : {}, img_padded shape : {}'.format(pad_down.shape, img_padded.shape))\n  pad_right  = np.zeros( (img_padded.shape[0], pad[1], image.shape[2]), dtype='float32') + padValue\n  img_padded = np.concatenate( (img_padded,pad_right), axis=1).astype(image.dtype)\n  #print ('pad_right shape : {}, img_padded shape : {}'.format(pad_right.shape, img_padded.shape))\n  #cv2.imwrite('{}/test.jpg'.format(os.environ['HOME']), img_padded)\n  return img_padded, pad\n\n\n## re-scale the score heat-map (H*W*C) to the size of original image\ndef resize2scaled_img(heatmap, pad):\n  assert isinstance(heatmap, np.ndarray), 'incorrect type : {}'.format(type(image))\n  assert len(heatmap.shape) == 3 and heatmap.shape[2] >= 1, 'incorrect shape : {}'.format(heatmap.shape)\n  assert len(pad) == 4, 'incorrect pad shape : {}'.format(pad)\n  score = heatmap.copy()\n  if pad[0] < 0:\n    pad_up    = np.zeros( (-pad[0], score.shape[1], score.shape[2]), dtype=heatmap.dtype)\n    score     = np.concatenate( (pad_up, score), axis=0).astype(heatmap.dtype)\n  elif pad[0] > 0:\n    score     = score[pad[0]:, :, :]\n\n  if pad[1] < 0:\n    padleft   = np.zeros( (score.shape[0], -pad[1], score.shape[2]), dtype=heatmap.dtype)\n    score     = np.concatenate( (padleft, score), axis=1).astype(heatmap.dtype)\n  elif pad[1] > 0:\n    score     = score[:, :-pad[1], :]\n\n  if pad[2] < 0:\n    paddown   = np.zeros( (-pad[2], score.shape[1], score.shape[2]), dtype=heatmap.dtype)\n    score     = np.concatenate( (score, paddown), axis=0).astype(heatmap.dtype)\n  elif pad[2] > 0:\n    score     = score[pad[2]:, :, :]\n\n  if pad[3] < 0:\n    padright  = np.zeros( (score.shape[0], -pad[3], score.shape[2]), dtype=heatmap.dtype)\n  elif pad[3] > 0:\n    score     = score[:, :-pad[3], :]\n\n  return score\n\ndef im2float(_image):\n  image = _image.copy()\n  if (image.dtype == 'uint8'):\n    image = image.astype(np.float32) / 255\n  elif (image.dtype == 'uint16'):\n    image = image.astype(np.float32) / 65535\n  else:\n    assert False, 'unsupport dtype : {}'.format(image.dtype)\n  return image\n"""
SAN/lib/utils/pts_utils.py,0,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom scipy.ndimage.interpolation import zoom\nimport numbers, math\nimport numpy as np\n\ndef pts2bbox(pts):\n  '''\n  convert a set of 2d points to a bounding box\n  parameter:\n    pts : 2 x N numpy array, N should >= 2\n    or  : 3 x N numpy array, N should >= 2, the last row means visiable or not\n  return:\n    bbox: 1 x 4 numpy array, TLBR format\n  '''\n\n  assert isinstance(pts, np.ndarray) and (pts.shape[0] == 2 or pts.shape[0] == 3), 'the input points should have shape 2 x num_pts'\n  if pts.shape[0] == 3:\n    pts = pts[0:1, pts[2,:] == 1]\n  assert pts.shape[1] >= 2 and len(pts.shape) == 2, 'number of points should be larger or equal than 2, and the dimension of points tensor can only be 2'\n  bbox = np.zeros((1, 4), dtype='float32')\n  bbox[0, 0] = np.min(pts[0, :])          # x coordinate of left top point\n  bbox[0, 1] = np.min(pts[1, :])          # y coordinate of left top point\n  bbox[0, 2] = np.max(pts[0, :])          # x coordinate of bottom right point\n  bbox[0, 3] = np.max(pts[1, :])          # y coordinate of bottom right point\n  return bbox\n\ndef find_peaks_v1(heatmap):\n  assert isinstance(heatmap, np.ndarray) and len(heatmap.shape) == 2\n  #heatmap = heatmap * ( heatmap > threshold )\n  index = np.unravel_index(heatmap.argmax(), heatmap.shape)\n  return index, heatmap[index]\n\ndef find_peaks_v2(heatmap, threshold=0.000001, eps=np.finfo(float).eps):\n  assert isinstance(heatmap, np.ndarray) and len(heatmap.shape) == 2  \n  #heatmap = heatmap * ( heatmap > threshold )    \n  if heatmap.max() < eps:\n    return find_peaks_v1(heatmap)\n\n  heatmap[ heatmap <= threshold ] = eps\n  w, h = heatmap.shape\n  x = np.sum(np.sum(heatmap, axis=1) * np.arange(0, w)) / heatmap.sum()    \n  y = np.sum(np.sum(heatmap, axis=0) * np.arange(0, h)) / heatmap.sum()    \n  x2, y2 = min(w-1, int(np.ceil(x))), min(h-1, int(np.ceil(y)))  \n  x1, y1 = max(0, x2-1), max(0, y2-1)   \n  ## Bilinear interpolation   \n  if x1 == x2: \n    R1, R2 = heatmap[x1, y1], heatmap[x1, y2]\n  else:   \n    R1 = (x2-x)/(x2-x1)*heatmap[x1, y1] + (x-x1)/(x2-x1)*heatmap[x2, y1]   \n    R2 = (x2-x)/(x2-x1)*heatmap[x1, y2] + (x-x1)/(x2-x1)*heatmap[x2, y2]   \n  #print ('{}, {}'.format(heatmap[x1, y1], heatmap[x2, y2]))\n  #print ('x1 : {}, y2 : {},  R1 : {}, R2 : {}'.format(x1, y1, R1, R2))\n  if y1 == y2: \n    score = R1 \n  else:   \n    score = (y2-y)/(y2-y1)*R1 + (y-y1)/(y2-y1)*R2 \n  return (x,y), score \n\ndef find_batch_peaks(heatmap, radius, downsample):\n  assert isinstance(heatmap, np.ndarray) and len(heatmap.shape) == 4, 'heatmap shape : {}'.format(heatmap.shape)\n  batch, num_pts, h, w = heatmap.shape\n  pts_locations = np.zeros( (batch, num_pts, 3), dtype='float32')\n  # batch x [x, y, score]\n\n  for bth in range(batch):\n    for pts_index in range(num_pts):\n      location, score = find_peaks_v1(heatmap[bth,pts_index,:,:])\n      sh, sw = location[0] - radius,     location[1] - radius\n      eh, ew = location[0] + radius + 1, location[1] + radius + 1\n      sw, sh = max(0, sw), max(0, sh)\n      ew, eh = min(w, ew), min(h, eh)\n      #temp = zoom(heatmap[bth, pts_index, sh:eh, sw:ew], downsample, order=3)\n      #loc, score = find_peaks_v2(temp)\n      loc, score = find_peaks_v2(heatmap[bth, pts_index, sh:eh, sw:ew])\n      pts_locations[bth, pts_index, 2] = score\n      pts_locations[bth, pts_index, 1] = sh * downsample + loc[0] * downsample + downsample / 2.0 - 0.5\n      pts_locations[bth, pts_index, 0] = sw * downsample + loc[1] * downsample + downsample / 2.0 - 0.5\n  return pts_locations\n\ndef find_all_peaks(heatmap, radius, downsample, threshold, image_resize):\n  assert isinstance(heatmap, np.ndarray), 'heatmap type : {}'.format(heatmap.shape)\n  assert len(heatmap.shape) == 3, 'heatmap shape : {}'.format(heatmap.shape)\n  assert threshold is None or isinstance(threshold, numbers.Number), 'threshold must greater than 0'\n  assert image_resize is None or (isinstance(image_resize, numbers.Number) and image_resize > 0)\n  w, h, num_pts = heatmap.shape\n  pts_locations = np.zeros( (3, num_pts), dtype='float32')\n\n  for pts_index in range(num_pts):\n    location, score = find_peaks_v1(heatmap[:,:,pts_index])\n    sw, sh = int(location[0] - radius),     int(location[1] - radius)\n    ew, eh = int(location[0] + radius + 1), int(location[1] + radius + 1)\n    sw, sh = max(0, sw), max(0, sh)\n    ew, eh = min(w, ew), min(h, eh)\n    #temp = zoom(heatmap[sw:ew, sh:eh, pts_index], downsample, order=3)\n    #loc, score = find_peaks(temp)\n    loc, score = find_peaks_v2(heatmap[sw:ew, sh:eh, pts_index])\n    #if pts_index == 0:\n    #  print ('location : {} , radius : {}'.format(location, radius))\n    #  print ('({}, {}) - ({}, {})'.format(sw,ew,sh,eh))\n    #  print ('loc : {}'.format(loc))\n    if threshold is not None and (score < threshold):\n      pts_locations[2, pts_index] = False\n    else:\n      pts_locations[2, pts_index] = score\n      pts_locations[0, pts_index] = sh * downsample + loc[1] * downsample + downsample / 2.0 - 0.5\n      pts_locations[1, pts_index] = sw * downsample + loc[0] * downsample + downsample / 2.0 - 0.5\n\n      if image_resize is not None:\n        pts_locations[0, pts_index] = pts_locations[0, pts_index] / image_resize\n        pts_locations[1, pts_index] = pts_locations[1, pts_index] / image_resize\n\n  return pts_locations\n\n## pts = 3 * N numpy array; points location is based on the image with size (height*downsample, width*downsample)\n## \n\ndef generate_label_map_laplacian(pts, height, width, sigma, downsample, visiable=None):\n  if isinstance(pts, numbers.Number):\n    # this image does not provide the annotation, pts is a int number representing the number of points\n    return np.zeros((height,width,pts+1), dtype='float32'), np.ones((1,1,1+pts), dtype='float32')\n\n  assert isinstance(pts, np.ndarray) and len(pts.shape) == 2 and pts.shape[0] == 3, 'The shape of points : {}'.format(pts.shape)\n  if isinstance(sigma, numbers.Number):\n    sigma = np.zeros((pts.shape[1])) + sigma\n  assert isinstance(sigma, np.ndarray) and len(sigma.shape) == 1 and sigma.shape[0] == pts.shape[1], 'The shape of sigma : {}'.format(sigma.shape)\n\n  offset = downsample / 2.0 - 0.5\n  num_points, threshold = pts.shape[1], 0.01\n\n  if visiable is None: visiable = pts[2, :].astype('bool')\n  assert visiable.shape[0] == num_points\n\n  transformed_label = np.fromfunction( lambda y, x, pid : ((offset + x*downsample - pts[0,pid])**2 \\\n                                                        + (offset + y*downsample - pts[1,pid])**2) \\\n                                                          / -2.0 / sigma[pid] / sigma[pid],\n                                                          (height, width, num_points), dtype=int)\n\n  mask_heatmap      = np.ones((1, 1, num_points+1), dtype='float32')\n  mask_heatmap[0, 0, :num_points] = visiable\n  mask_heatmap[0, 0, num_points]  = 1\n  \n  transformed_label = (1+transformed_label) * np.exp(transformed_label)\n  transformed_label[ transformed_label < threshold ] = 0\n  transformed_label[ transformed_label >         1 ] = 1\n\n  background_label  = 1 - np.amax(transformed_label, axis=2)\n  background_label[ background_label < 0 ] = 0\n  heatmap           = np.concatenate((transformed_label, np.expand_dims(background_label, axis=2)), axis=2).astype('float32')\n  \n  return heatmap*mask_heatmap, mask_heatmap\n\ndef generate_label_map_gaussian(pts, height, width, sigma, downsample, visiable=None):\n  if isinstance(pts, numbers.Number):\n    # this image does not provide the annotation, pts is a int number representing the number of points\n    return np.zeros((height,width,pts+1), dtype='float32'), np.ones((1,1,1+pts), dtype='float32')\n\n  assert isinstance(pts, np.ndarray) and len(pts.shape) == 2 and pts.shape[0] == 3, 'The shape of points : {}'.format(pts.shape)\n  if isinstance(sigma, numbers.Number):\n    sigma = np.zeros((pts.shape[1])) + sigma\n  assert isinstance(sigma, np.ndarray) and len(sigma.shape) == 1 and sigma.shape[0] == pts.shape[1], 'The shape of sigma : {}'.format(sigma.shape)\n\n  offset = downsample / 2.0 - 0.5\n  num_points, threshold = pts.shape[1], 0.01\n\n  if visiable is None: visiable = pts[2, :].astype('bool')\n  assert visiable.shape[0] == num_points\n\n  transformed_label = np.fromfunction( lambda y, x, pid : ((offset + x*downsample - pts[0,pid])**2 \\\n                                                        + (offset + y*downsample - pts[1,pid])**2) \\\n                                                          / -2.0 / sigma[pid] / sigma[pid],\n                                                          (height, width, num_points), dtype=int)\n\n  mask_heatmap      = np.ones((1, 1, num_points+1), dtype='float32')\n  mask_heatmap[0, 0, :num_points] = visiable\n  mask_heatmap[0, 0, num_points]  = 1\n  \n  transformed_label = np.exp(transformed_label)\n  transformed_label[ transformed_label < threshold ] = 0\n  transformed_label[ transformed_label >         1 ] = 1\n\n  background_label  = 1 - np.amax(transformed_label, axis=2)\n  background_label[ background_label < 0 ] = 0\n  heatmap           = np.concatenate((transformed_label, np.expand_dims(background_label, axis=2)), axis=2).astype('float32')\n  \n  return heatmap*mask_heatmap, mask_heatmap\n"""
SAN/lib/utils/stn_utils.py,0,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nimport numpy as np\n\ndef normalize_length(x, width):\n  return -1. + 2. * x / (width-1)\n\ndef get_abs_location(x, width):\n  return (x+1)/2.0 * (width-1)\n\ndef crop2affine(spatial_size, crop_box):\n  # retuen a 2x3 matrix containing the affine-transformation parameters.\n  assert isinstance(spatial_size, np.ndarray) and (spatial_size.dtype == 'int32' or spatial_size.dtype == 'int64') and spatial_size.shape == (2,), 'wrong type of spatial_size : {}'.format(spatial_size)\n  assert isinstance(crop_box, np.ndarray) and crop_box.shape == (4,), 'wrong type of crop_box : {}'.format(crop_box)\n  parameters = np.zeros((2,3), dtype='float32')\n  crop_box = crop_box.astype('float32')\n  ## normalize\n  crop_box[0] = normalize_length(crop_box[0], spatial_size[0])\n  crop_box[1] = normalize_length(crop_box[1], spatial_size[1])\n  crop_box[2] = normalize_length(crop_box[2], spatial_size[0])\n  crop_box[3] = normalize_length(crop_box[3], spatial_size[1])\n\n  parameters[0, 0] = (crop_box[2] - crop_box[0]) / 2\n  parameters[0, 2] = (crop_box[0] + crop_box[2]) / 2\n  parameters[1, 1] = (crop_box[3] - crop_box[1]) / 2\n  parameters[1, 2] = (crop_box[1] + crop_box[3]) / 2\n\n  return parameters\n\ndef identity2affine():\n  parameters = np.zeros((2,3), dtype='float32')\n  parameters[0, 0] = parameters[1, 1] = 1\n  return parameters\n  \n"""
SAN/lib/utils/time_utils.py,0,"b'##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nimport time, sys\nimport numpy as np\n\ndef time_for_file():\n  ISOTIMEFORMAT=\'%d-%h-at-%H-%M-%S\'\n  return \'{}\'.format(time.strftime( ISOTIMEFORMAT, time.gmtime(time.time()) ))\n\ndef time_string():\n  ISOTIMEFORMAT=\'%Y-%m-%d %X\'\n  string = \'[{}]\'.format(time.strftime( ISOTIMEFORMAT, time.gmtime(time.time()) ))\n  return string\n\ndef time_string_short():\n  ISOTIMEFORMAT=\'%Y%m%d\'\n  string = \'{}\'.format(time.strftime( ISOTIMEFORMAT, time.gmtime(time.time()) ))\n  return string\n\ndef time_print(string, is_print=True):\n  if (is_print):\n    print(\'{} : {}\'.format(time_string(), string))\n\nclass AverageMeter(object):     \n  """"""Computes and stores the average and current value""""""    \n  def __init__(self):   \n    self.reset()\n  \n  def reset(self):\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0    \n  \n  def update(self, val, n=1): \n    self.val = val    \n    self.sum += val * n     \n    self.count += n\n    self.avg = self.sum / self.count    \n\nclass LossRecorderMeter(object):\n  """"""Computes and stores the minimum loss value and its epoch index""""""\n  def __init__(self, total_epoch):\n    self.reset(total_epoch)\n\n  def reset(self, total_epoch):\n    assert total_epoch > 0\n    self.total_epoch   = total_epoch\n    self.current_epoch = 0\n    self.epoch_losses  = np.zeros((self.total_epoch, 2), dtype=np.float32) # [epoch, train/val]\n    self.epoch_losses  = self.epoch_losses + sys.float_info.max\n\n  def update(self, train_loss, idx, val_loss=None):\n    assert idx >= 0 and idx < self.total_epoch, \'total_epoch : {} , but update with the {} index\'.format(self.total_epoch, idx)\n    self.epoch_losses[idx, 0] = train_loss\n    if val_loss is not None:\n      self.epoch_losses[idx, 1] = val_loss\n    self.current_epoch = idx + 1\n  \n  def min_loss(self, Train=True):\n    if Train:\n      idx = np.argmin(self.epoch_losses[:self.current_epoch, 0])\n      return idx, self.epoch_losses[idx, 0]\n    else:\n      idx = np.argmin(self.epoch_losses[:self.current_epoch, 1])\n      if self.epoch_losses[idx, 1] >= sys.float_info.max / 10:\n        return idx, -1.\n      else:\n        return idx, self.epoch_losses[idx, 1]\n    \ndef convert_size2str(torch_size):\n  dims = len(torch_size)\n  string = \'[\'\n  for idim in range(dims):\n    string = string + \' {}\'.format(torch_size[idim])\n  return string + \']\'\n  \ndef convert_secs2time(epoch_time):    \n  need_hour = int(epoch_time / 3600)\n  need_mins = int((epoch_time - 3600*need_hour) / 60)  \n  need_secs = int(epoch_time - 3600*need_hour - 60*need_mins)\n  return need_hour, need_mins, need_secs\n\ndef print_log(print_string, log):\n  print(""{}"".format(print_string))\n  if log is not None:\n    log.write(\'{}\\n\'.format(print_string))\n    log.flush()\n'"
SAN/lib/visualization/__init__.py,0,"b'##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom .visualize import draw_image_with_pts\nfrom .visualize import generate_color_from_heatmap\nfrom .visualize import generate_color_from_heatmaps\nfrom .visualize import overlap_two_pil_image\nfrom .visualize import merge_images\nfrom .draw_image_by_points import draw_image_by_points\nfrom .save_error_image import save_error_image\n'"
SAN/lib/visualization/draw_image_by_points.py,0,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom PIL import Image\nfrom PIL import ImageDraw\nfrom PIL import ImageFont\nimport numpy as np\nfrom numpy import linspace\nfrom matplotlib import cm\nimport datasets\n\ndef draw_image_by_points(_image, pts, radius, color, crop, resize):\n  if isinstance(_image, str):\n    _image = datasets.pil_loader(_image)\n  assert isinstance(_image, Image.Image), 'image type is not PIL.Image.Image'\n  assert isinstance(pts, np.ndarray) and (pts.shape[0] == 2 or pts.shape[0] == 3), 'input points are not correct'\n  image, pts = _image.copy(), pts.copy()\n\n  num_points = pts.shape[1]\n  visiable_points = []\n  for idx in range(num_points):\n    if pts.shape[0] == 2 or bool(pts[2,idx]):\n      visiable_points.append( True )\n    else:\n      visiable_points.append( False )\n  visiable_points = np.array( visiable_points )\n  #print ('visiable points : {}'.format( np.sum(visiable_points) ))\n\n  if crop:\n    x1, x2 = pts[0, visiable_points].min(), pts[0, visiable_points].max()\n    y1, y2 = pts[1, visiable_points].min(), pts[1, visiable_points].max()\n    face_h, face_w = (y2-y1)*0.05, (x2-x1)*0.05\n    x1, x2 = int(x1 - face_w), int(x2 + face_w)\n    y1, y2 = int(y1 - face_h), int(y2 + face_h)\n    image = image.crop((x1, y1, x2, y2))\n    pts[0, visiable_points] = pts[0, visiable_points] - x1\n    pts[1, visiable_points] = pts[1, visiable_points] - y1\n\n  if resize:\n    width, height = image.size\n    image = image.resize((resize,resize), Image.BICUBIC)\n    pts[0, visiable_points] = pts[0, visiable_points] * 1.0 / width * resize\n    pts[1, visiable_points] = pts[1, visiable_points] * 1.0 / height * resize\n\n  draw  = ImageDraw.Draw(image)\n  for idx in range(num_points):\n    if visiable_points[ idx ]:\n      # draw hollow circle\n      point = (pts[0,idx]-radius, pts[1,idx]-radius, pts[0,idx]+radius, pts[1,idx]+radius)\n      if radius > 0:\n        draw.ellipse(point, fill=None, outline=color)\n\n  return image\n"""
SAN/lib/visualization/save_error_image.py,0,"b""##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom PIL import Image\nfrom PIL import ImageDraw\nfrom PIL import ImageFont\nimport os\nfrom os import path as osp\nimport numpy as np\nfrom numpy import linspace\nfrom matplotlib import cm\nimport datasets\n\ndef save_error_image(image, points, locations, error_bar, save_path, radius = 10, color = (193,255,193), rev_color = (193,255,193), fontScale = 16, text_color = (255,255,255)):\n  '''\n  visualize image and plot keypoints on top of it\n  parameter:\n    image:          PIL.Image.Image object\n    points:         (2 or 3) x num_pts numpy array : [x, y, visiable] Ground-Truth\n    locations:      predictions\n  '''\n  if isinstance(image, str):\n    image = datasets.pil_loader(image)\n  assert isinstance(image, Image.Image), 'image type is not PIL.Image.Image'\n  assert isinstance(points, np.ndarray) and points.shape[0] == 3, 'input points are not correct : {}'.format(points)\n  assert isinstance(locations, np.ndarray) and locations.shape[0] == 3 and locations.shape == points.shape, 'input locations are not correct : {}'.format(locations)\n  points, locations = points.copy(), locations.copy()\n  distance  = np.zeros((points.shape[1]))\n  for idx in range(points.shape[1]):\n    if bool(points[2,idx]):\n      dis = points[:2,idx] - locations[:2, idx]\n      distance[idx] = np.sqrt(np.sum(dis*dis))\n  if np.sum(distance > error_bar) == 0: return\n\n  x_min, y_min = int(points[0,:].min())-30, int(points[1,:].min())-50\n  x_max, y_max = int(points[0,:].max())+30, int(points[1,:].max())+10\n  image = image.crop((x_min, y_min, x_max, y_max))\n  locations[0,:] = locations[0,:] - x_min\n  locations[1,:] = locations[1,:] - y_min\n  points[0,:] = points[0,:] - x_min\n  points[1,:] = points[1,:] - y_min\n  locations = ( locations + points ) / 2\n \n  save_path = save_path[:-4]\n\n  zoom_in(image, locations, save_path+'.zoom.pdf', color)\n  # save zoom-in image\n  draw  = ImageDraw.Draw(image)\n  w, h = image.size\n\n  for idx in range(points.shape[1]):\n    if distance[idx] < error_bar: continue\n    #point = (int(points[0,idx])-radius, int(points[1,idx])-radius, int(points[0,idx])+radius, int(points[1,idx])+radius)\n    #if point[0] > 0 and point[1] > 0 and point[2] < w and point[3] < h:\n    #  draw.ellipse(point, fill=rev_color, outline=rev_color)\n    # draw hollow circle\n    point = (int(locations[0,idx])-radius, int(locations[1,idx])-radius, int(locations[0,idx])+radius, int(locations[1,idx])+radius)\n    draw.ellipse(point, fill=color, outline=color)\n\n  image.save(save_path+'.pdf')\n\ndef zoom_in(image, points, save_path, rev_color):\n  image = image.copy()\n  w, h = image.size\n  scale = 3\n  tw, th = w * scale, h * scale\n  image = image.resize((tw,th), Image.BICUBIC)\n  points = points * scale\n  draw  = ImageDraw.Draw(image)\n  radius = 1\n  for idx in range(points.shape[1]):\n    point = (int(points[0,idx])-radius, int(points[1,idx])-radius, int(points[0,idx])+radius, int(points[1,idx])+radius)\n    if point[0] > 0 and point[1] > 0 and point[2] < tw and point[3] < th:\n      draw.ellipse(point, fill=rev_color, outline=rev_color)\n  image = image.crop((240, 120, 420, 250))\n  image.save(save_path)\n"""
SAN/lib/visualization/visualize.py,0,"b'##############################################################\n### Copyright (c) 2018-present, Xuanyi Dong                ###\n### Style Aggregated Network for Facial Landmark Detection ###\n### Computer Vision and Pattern Recognition, 2018          ###\n##############################################################\nfrom PIL import Image\nfrom PIL import ImageDraw\nfrom PIL import ImageFont\nimport os\nfrom os import path as osp\nimport numpy as np\nfrom numpy import linspace\nfrom matplotlib import cm\nimport datasets\n\ndef merge_images(images, gap, direction=\'y\'):\n  assert len(images) > 0 and isinstance(gap, int), \'The gap must be interge : {}\'.format(gap)\n  assert direction == \'x\' or direction == \'y\', \'The direction must be x or y, not {}\'.format(direction)\n  for index, image in enumerate(images):\n    assert isinstance(image, Image.Image), \'The {}-th image is PIL.Image\'.format(index)\n  size = images[0].size\n  images = [np.array(image) for image in images]\n  imagelist = []\n  for index, image in enumerate(images):\n    if direction == \'y\': gap_shape = [gap, size[0], 3]\n    else:                gap_shape = [size[1], gap, 3]\n    if index > 0: imagelist.append(np.zeros(gap_shape).astype(\'uint8\'))\n    imagelist.append(image)\n  if direction == \'y\': stack = np.vstack( imagelist )\n  else:                stack = np.hstack( imagelist )\n  return Image.fromarray( stack )\n    \ndef merge_images_matrix(images, gap):\n  assert len(images) > 0 and isinstance(gap, int), \'The gap must be interge : {}\'.format(gap)\n  assert direction == \'x\' or direction == \'y\', \'The direction must be x or y, not {}\'.format(direction)\n  for index, image in enumerate(images):\n    assert isinstance(image, Image.Image), \'The {}-th image is PIL.Image\'.format(index)\n  images = [np.array(image) for image in images]\n  num_w = int( np.sqrt( len(images) ) )\n  num_h = int( np.ceil( len(images)/num_w ) )\n  assert False, \'Un finished\'\n  size = images[0].size\n  imagelist = []\n  for index, image in enumerate(images):\n    if direction == \'y\': gap_shape = [gap, size[0], 3]\n    else:                gap_shape = [size[1], gap, 3]\n    if index > 0: imagelist.append(np.zeros(gap_shape).astype(\'uint8\'))\n    imagelist.append(image)\n  if direction == \'y\': stack = np.vstack( imagelist )\n  else:                stack = np.hstack( imagelist )\n  return Image.fromarray( stack )\n\ndef overlap_two_pil_image(imageA, imageB):\n  assert isinstance(imageA, Image.Image), \'The 1-th image type is not PIL.Image.Image\'\n  assert isinstance(imageB, Image.Image), \'The 2-th image type is not PIL.Image.Image\'\n  width = max(imageA.size[0], imageB.size[0])\n  height = max(imageA.size[1], imageB.size[1])\n  imageA = imageA.resize((width, height), Image.BICUBIC)\n  imageB = imageB.resize((width, height), Image.BICUBIC)\n  image = (np.array(imageA) + np.array(imageB) * 1.0) / 2.0\n  return Image.fromarray(np.uint8(image))\n\ndef draw_image_with_pts(_image, pts, radius = 10, color = (193,255,193), fontScale = 16, text_color = (255,255,255), linewidth = 3, window = None):\n  \'\'\'\n  visualize image and plot keypoints on top of it\n  parameter:\n    image:          PIL.Image.Image object\n    pts:            (2 or 3) x num_pts numpy array : [x, y, visiable]\n  \'\'\'\n  if isinstance(_image, str):\n    _image = datasets.pil_loader(_image)\n  assert isinstance(_image, Image.Image), \'image type is not PIL.Image.Image\'\n  assert isinstance(pts, np.ndarray) and (pts.shape[0] == 2 or pts.shape[0] == 3), \'input points are not correct\'\n  image = _image.copy()\n  draw  = ImageDraw.Draw(image)\n\n  try:\n    font = ImageFont.truetype(\'Pillow/Tests/fonts/FreeMono.ttf\', fontScale)\n  except:\n    font_path = osp.join(os.environ[\'HOME\'], \'.fonts\', \'freefont\', \'FreeMono.ttf\')\n    font = ImageFont.truetype(font_path, fontScale)\n\n  line_radius = abs(radius)\n  for idx in range(pts.shape[1]):\n    #if (pts.shape[0] == 2 or pts[2, idx] >= 1): # visiable\n    if (pts.shape[0] == 2 or (pts[0, idx] > 0 and pts[1, idx] > 0)): # visiable\n      # draw hollow circle\n      point = (int(pts[0,idx])-radius, int(pts[1,idx])-radius, int(pts[0,idx])+radius, int(pts[1,idx])+radius)\n      if radius > 0:\n        draw.ellipse(point, fill=None, outline=color)\n\n      point = (int(pts[0,idx]-line_radius), int(pts[1,idx]-line_radius), int(pts[0,idx]+line_radius), int(pts[1,idx]+line_radius))\n      draw.line(point, fill=color, width = linewidth)\n\n      point = (int(pts[0,idx]-line_radius), int(pts[1,idx]+line_radius), int(pts[0,idx]+line_radius), int(pts[1,idx]-line_radius))\n      draw.line(point, fill=color, width = linewidth)\n      \n      point = (int(pts[0,idx]+line_radius), int(pts[1,idx]-line_radius))\n      draw.text(point, \'{}\'.format(idx+1), fill=text_color, font=font)\n  \n      if window is not None:\n        assert isinstance(window, int), \'The window is not ok : {}\'.format(window)\n        point = (int(pts[0,idx]-window), int(pts[1,idx]-window), int(pts[0,idx]-window), int(pts[1,idx]+window))\n        draw.line(point, fill=color, width = 1)\n        point = (int(pts[0,idx]+window), int(pts[1,idx]-window), int(pts[0,idx]+window), int(pts[1,idx]+window))\n        draw.line(point, fill=color, width = 1)\n        point = (int(pts[0,idx]-window), int(pts[1,idx]-window), int(pts[0,idx]+window), int(pts[1,idx]-window))\n        draw.line(point, fill=color, width = 1)\n        point = (int(pts[0,idx]-window), int(pts[1,idx]+window), int(pts[0,idx]+window), int(pts[1,idx]+window))\n        draw.line(point, fill=color, width = 1)\n        window = None\n    \n  #point = (int(image.size[0]/2.)-radius*3, int(image.size[1]/2.0)-radius*3, int(image.size[0]/2.)+radius*3, int(image.size[1]/2.0)+radius*3)\n  #draw.ellipse(point, fill=(0,0,0), outline=(0,0,0))\n\n  return image\n\n\ndef mat2im(mat, cmap, limits):\n  \'\'\'\n% PURPOSE\n% Uses vectorized code to convert matrix ""mat"" to an m-by-n-by-3\n% image matrix which can be handled by the Mathworks image-processing\n% functions. The the image is created using a specified color-map\n% and, optionally, a specified maximum value. Note that it discards\n% negative values!\n%\n% INPUTS\n% mat     - an m-by-n matrix  \n% cmap    - an m-by-3 color-map matrix. e.g. hot(100). If the colormap has \n%           few rows (e.g. less than 20 or so) then the image will appear \n%           contour-like.\n% limits  - by default the image is normalised to it\'s max and min values\n%           so as to use the full dynamic range of the\n%           colormap. Alternatively, it may be normalised to between\n%           limits(1) and limits(2). Nan values in limits are ignored. So\n%           to clip the max alone you would do, for example, [nan, 2]\n%          \n%\n% OUTPUTS\n% im - an m-by-n-by-3 image matrix  \n  \'\'\'\n\n  assert len(mat.shape) == 2\n  if len(limits) == 2:\n    minVal = limits[0]\n    tempss = np.zeros(mat.shape) + minVal\n    mat    = np.maximum(tempss, mat)\n    maxVal = limits[1]\n    tempss = np.zeros(mat.shape) + maxVal\n    mat    = np.minimum(tempss, mat)\n  else:\n    minVal = mat.min()\n    maxVal = mat.max()\n  L = len(cmap)\n  if maxVal <= minVal:\n    mat = mat-minVal\n  else:\n    mat = (mat-minVal) / (maxVal-minVal) * (L-1)\n  mat = mat.astype(np.int32)\n  \n  image = np.reshape(cmap[ np.reshape(mat, (mat.size)), : ], mat.shape + (3,))\n  return image\n\ndef jet(m):\n  cm_subsection = linspace(0, 1, m)\n  colors = [ cm.jet(x) for x in cm_subsection ]\n  J = np.array(colors)\n  J = J[:, :3]\n  return J\n\ndef generate_color_from_heatmap(maps, num_of_color=100, index=None):\n  assert isinstance(maps, np.ndarray)\n  if len(maps.shape) == 3:\n    return generate_color_from_heatmaps(maps, num_of_color, index)\n  elif len(maps.shape) == 2:\n    return mat2im( maps, jet(num_of_color), [maps.min(), maps.max()] )\n  else:\n    assert False, \'generate_color_from_heatmap wrong shape : {}\'.format(maps.shape)\n    \n\ndef generate_color_from_heatmaps(maps, num_of_color=100, index=None):\n  assert isinstance(maps, np.ndarray) and len(maps.shape) == 3, \'maps type : {}\'.format(type(maps))\n  __jet = jet(num_of_color)\n\n  if index is None:\n    answer = []\n    for i in range(maps.shape[2]):\n      temp = mat2im( maps[:,:,i], __jet, [maps[:,:,i].min(), maps[:,:,i].max()] )\n      answer.append( temp )\n    return answer\n  else:\n    return mat2im( maps[:,:,index], __jet, [maps[:,:,index].min(), maps[:,:,index].max()] )\n'"
SBR/lib/config_utils/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom .configure_utils import load_configure \nfrom .basic_args import obtain_args as obtain_basic_args\nfrom .lk_args import obtain_args as obtain_lk_args\n'"
SBR/lib/config_utils/basic_args.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport os, sys, time, random, argparse\n\ndef obtain_args():\n  parser = argparse.ArgumentParser(description='Train facial landmark detectors on 300-W or AFLW', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument('--train_lists',      type=str,   nargs='+',      help='The list file path to the video training dataset.')\n  parser.add_argument('--eval_vlists',      type=str,   nargs='+',      help='The list file path to the video testing dataset.')\n  parser.add_argument('--eval_ilists',      type=str,   nargs='+',      help='The list file path to the image testing dataset.')\n  parser.add_argument('--num_pts',          type=int,                   help='Number of point.')\n  parser.add_argument('--model_config',     type=str,                   help='The path to the model configuration')\n  parser.add_argument('--opt_config',       type=str,                   help='The path to the optimizer configuration')\n  # Data Generation\n  parser.add_argument('--heatmap_type',     type=str,   choices=['gaussian','laplacian'], help='The method for generating the heatmap.')\n  parser.add_argument('--data_indicator',   type=str, default='300W-68',help='The method for generating the heatmap.')\n  # Data Transform\n  parser.add_argument('--pre_crop_expand',  type=float,                 help='parameters for pre-crop expand ratio')\n  parser.add_argument('--sigma',            type=float,                 help='sigma distance for CPM.')\n  parser.add_argument('--scale_prob',       type=float,                 help='argument scale probability.')\n  parser.add_argument('--scale_min',        type=float,                 help='argument scale : minimum scale factor.')\n  parser.add_argument('--scale_max',        type=float,                 help='argument scale : maximum scale factor.')\n  parser.add_argument('--scale_eval',       type=float,                 help='argument scale : maximum scale factor.')\n  parser.add_argument('--rotate_max',       type=int,                   help='argument rotate : maximum rotate degree.')\n  parser.add_argument('--crop_height',      type=int,   default=256,    help='argument crop : crop height.')\n  parser.add_argument('--crop_width',       type=int,   default=256,    help='argument crop : crop width.')\n  parser.add_argument('--crop_perturb_max', type=int,                   help='argument crop : center of maximum perturb distance.')\n  parser.add_argument('--arg_flip',         action='store_true',        help='Using flip data argumentation or not ')\n  # Optimization options\n  parser.add_argument('--eval_once',        action='store_true',        help='evaluation only once for evaluation ')\n  parser.add_argument('--error_bar',        type=float,                 help='For drawing the image with large distance error.')\n  parser.add_argument('--batch_size',       type=int,   default=2,      help='Batch size for training.')\n  # Checkpoints\n  parser.add_argument('--print_freq',       type=int,   default=100,    help='print frequency (default: 200)')\n  parser.add_argument('--save_path',        type=str,                   help='Folder to save checkpoints and log.')\n  # Acceleration\n  parser.add_argument('--workers',          type=int,   default=8,      help='number of data loading workers (default: 2)')\n  # Random Seed\n  parser.add_argument('--rand_seed',        type=int,                   help='manual seed')\n  args = parser.parse_args()\n\n  if args.rand_seed is None:\n    args.rand_seed = random.randint(1, 100000)\n  assert args.save_path is not None, 'save-path argument can not be None'\n\n  #state = {k: v for k, v in args._get_kwargs()}\n  #Arguments = namedtuple('Arguments', ' '.join(state.keys()))\n  #arguments = Arguments(**state)\n  return args\n"""
SBR/lib/config_utils/configure_utils.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport os, sys, json\nfrom pathlib import Path\nfrom collections import namedtuple\n\nsupport_types = ('str', 'int', 'bool', 'float')\n\ndef convert_param(original_lists):\n  assert isinstance(original_lists, list), 'The type is not right : {:}'.format(original_lists)\n  ctype, value = original_lists[0], original_lists[1]\n  assert ctype in support_types, 'Ctype={:}, support={:}'.format(ctype, support_types)\n  is_list = isinstance(value, list)\n  if not is_list: value = [value]\n  outs = []\n  for x in value:\n    if ctype == 'int':\n      x = int(x)\n    elif ctype == 'str':\n      x = str(x)\n    elif ctype == 'bool':\n      x = bool(int(x))\n    elif ctype == 'float':\n      x = float(x)\n    else:\n      raise TypeError('Does not know this type : {:}'.format(ctype))\n    outs.append(x)\n  if not is_list: outs = outs[0]\n  return outs\n\ndef load_configure(path, logger):\n  path = str(path)\n  if logger is not None: logger.log(path)\n  assert os.path.exists(path), 'Can not find {:}'.format(path)\n  # Reading data back\n  with open(path, 'r') as f:\n    data = json.load(f)\n  f.close()\n  content = { k: convert_param(v) for k,v in data.items()}\n  Arguments = namedtuple('Configure', ' '.join(content.keys()))\n  content = Arguments(**content)\n  if logger is not None: logger.log('{:}'.format(content))\n  return content\n"""
SBR/lib/config_utils/lk_args.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport os, sys, time, random, argparse\n\ndef obtain_args():\n  parser = argparse.ArgumentParser(description='Train facial landmark detectors on 300-W, AFLW or Mugsy', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument('--train_lists',      type=str,   nargs='+',      help='The list file path to the video training dataset.')\n  parser.add_argument('--eval_vlists',      type=str,   nargs='+',      help='The list file path to the video testing dataset.')\n  parser.add_argument('--eval_ilists',      type=str,   nargs='+',      help='The list file path to the image testing dataset.')\n  parser.add_argument('--num_pts',          type=int,                   help='Number of point.')\n  parser.add_argument('--model_config',     type=str,                   help='The path to the model configuration')\n  parser.add_argument('--opt_config',       type=str,                   help='The path to the optimizer configuration')\n  parser.add_argument('--lk_config',        type=str,                   help='The path to the LK configuration')\n  # Data Generation\n  parser.add_argument('--heatmap_type',     type=str,   choices=['gaussian','laplacian'], help='The method for generating the heatmap.')\n  parser.add_argument('--data_indicator',   type=str, default='300W-68',help='The dataset indicator.')\n  parser.add_argument('--video_parser',     type=str,                   help='The video-parser indicator.')\n  # Data Transform\n  parser.add_argument('--pre_crop_expand',  type=float,                 help='parameters for pre-crop expand ratio')\n  parser.add_argument('--sigma',            type=float,                 help='sigma distance for CPM.')\n  parser.add_argument('--scale_prob',       type=float,                 help='argument scale probability.')\n  parser.add_argument('--scale_min',        type=float,                 help='argument scale : minimum scale factor.')\n  parser.add_argument('--scale_max',        type=float,                 help='argument scale : maximum scale factor.')\n  parser.add_argument('--scale_eval',       type=float,                 help='argument scale : maximum scale factor.')\n  parser.add_argument('--rotate_max',       type=int,                   help='argument rotate : maximum rotate degree.')\n  parser.add_argument('--crop_height',      type=int,   default=256,    help='argument crop : crop height.')\n  parser.add_argument('--crop_width',       type=int,   default=256,    help='argument crop : crop width.')\n  parser.add_argument('--crop_perturb_max', type=int,                   help='argument crop : center of maximum perturb distance.')\n  parser.add_argument('--arg_flip',         action='store_true',        help='Using flip data argumentation or not ')\n  # Optimization options\n  parser.add_argument('--eval_once',        action='store_true',        help='evaluation only once for evaluation ')\n  parser.add_argument('--error_bar',        type=float,                 help='For drawing the image with large distance error.')\n  parser.add_argument('--batch_size',       type=int,   default=2,      help='Batch size for training.')\n  # Checkpoints\n  parser.add_argument('--print_freq',       type=int,   default=100,    help='print frequency (default: 200)')\n  parser.add_argument('--init_model',       type=str,                   help='The detector model to be initalized.')\n  parser.add_argument('--save_path',        type=str,                   help='Folder to save checkpoints and log.')\n  # Acceleration\n  parser.add_argument('--workers',          type=int,   default=8,      help='number of data loading workers (default: 2)')\n  # Random Seed\n  parser.add_argument('--rand_seed',        type=int,                   help='manual seed')\n  args = parser.parse_args()\n\n  if args.rand_seed is None:\n    args.rand_seed = random.randint(1, 100000)\n  assert args.save_path is not None, 'save-path argument can not be None'\n\n  #state = {k: v for k, v in args._get_kwargs()}\n  #Arguments = namedtuple('Arguments', ' '.join(state.keys()))\n  #arguments = Arguments(**state)\n  return args\n"""
SBR/lib/datasets/GeneralDataset.py,9,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom __future__ import print_function\nfrom PIL import Image\nfrom os import path as osp\nimport numpy as np\nimport math\n\nfrom pts_utils import generate_label_map\nfrom .file_utils import load_file_lists\nfrom .dataset_utils import pil_loader\nfrom .dataset_utils import anno_parser\nfrom .point_meta import Point_Meta\nimport torch\nimport torch.utils.data as data\n\nclass GeneralDataset(data.Dataset):\n\n  def __init__(self, transform, sigma, downsample, heatmap_type, data_indicator):\n\n    self.transform = transform\n    self.sigma = sigma\n    self.downsample = downsample\n    self.heatmap_type = heatmap_type\n    self.dataset_name = data_indicator\n\n    self.reset()\n    print ('The general dataset initialization done : {:}'.format(self))\n\n  def __repr__(self):\n    return ('{name}(point-num={NUM_PTS}, sigma={sigma}, heatmap_type={heatmap_type}, length={length}, dataset={dataset_name})'.format(name=self.__class__.__name__, **self.__dict__))\n\n  def reset(self, num_pts=-1):\n    self.length = 0\n    self.NUM_PTS = num_pts\n    self.datas = []\n    self.labels = []\n    self.face_sizes = []\n    assert self.dataset_name is not None, 'The dataset name is None'\n\n  def __len__(self):\n    assert len(self.datas) == self.length, 'The length is not correct : {}'.format(self.length)\n    return self.length\n\n  def append(self, data, label, box, face_size):\n    assert osp.isfile(data), 'The image path is not a file : {}'.format(data)\n    self.datas.append( data )\n    if (label is not None) and (label.lower() != 'none'):\n      if isinstance(label, str):\n        assert osp.isfile(label), 'The annotation path is not a file : {}'.format(label)\n        np_points, _ = anno_parser(label, self.NUM_PTS)\n        meta = Point_Meta(self.NUM_PTS, np_points, box, data, self.dataset_name)\n      elif isinstance(label, Point_Meta):\n        meta = label.copy()\n      else:\n        raise NameError('Do not know this label : {}'.format(label))\n    else:\n      meta = Point_Meta(self.NUM_PTS, None, box, data, self.dataset_name)\n    self.labels.append( meta )\n    self.face_sizes.append( face_size )\n    self.length = self.length + 1\n\n  def prepare_input(self, image, box):\n    meta = Point_Meta(self.NUM_PTS, None, np.array(box), image, self.dataset_name)\n    image = pil_loader( image )\n    return self._process_(image, meta, -1), meta\n\n  def load_data(self, datas, labels, boxes, face_sizes, num_pts, reset):\n    # each data is a png file name\n    # each label is a Point_Meta class or the general pts format file (anno_parser_v1)\n    assert isinstance(datas, list), 'The type of the datas is not correct : {}'.format( type(datas) )\n    assert isinstance(labels, list) and len(datas) == len(labels), 'The type of the labels is not correct : {}'.format( type(labels) )\n    assert isinstance(boxes, list) and len(datas) == len(boxes), 'The type of the boxes is not correct : {}'.format( type(boxes) )\n    assert isinstance(face_sizes, list) and len(datas) == len(face_sizes), 'The type of the face_sizes is not correct : {}'.format( type(face_sizes) )\n    if reset: self.reset(num_pts)\n    else:     assert self.NUM_PTS == num_pts, 'The number of point is inconsistance : {} vs {}'.format(self.NUM_PTS, num_pts)\n\n    print ('[GeneralDataset] load-data {:} datas begin'.format(len(datas)))\n\n    for idx, data in enumerate(datas):\n      assert isinstance(data, str), 'The type of data is not correct : {}'.format(data)\n      assert osp.isfile(datas[idx]), '{} is not a file'.format(datas[idx])\n      self.append(datas[idx], labels[idx], boxes[idx], face_sizes[idx])\n\n    assert len(self.datas) == self.length, 'The length and the data is not right {} vs {}'.format(self.length, len(self.datas))\n    assert len(self.labels) == self.length, 'The length and the labels is not right {} vs {}'.format(self.length, len(self.labels))\n    assert len(self.face_sizes) == self.length, 'The length and the face_sizes is not right {} vs {}'.format(self.length, len(self.face_sizes))\n    print ('Load data done for the general dataset, which has {} images.'.format(self.length))\n\n  def load_list(self, file_lists, num_pts, reset):\n    lists = load_file_lists(file_lists)\n    print ('GeneralDataset : load-list : load {:} lines'.format(len(lists)))\n\n    datas, labels, boxes, face_sizes = [], [], [], []\n\n    for idx, data in enumerate(lists):\n      alls = [x for x in data.split(' ') if x != '']\n      \n      assert len(alls) == 6 or len(alls) == 7, 'The {:04d}-th line in {:} is wrong : {:}'.format(idx, data)\n      datas.append( alls[0] )\n      if alls[1] == 'None':\n        labels.append( None )\n      else:\n        labels.append( alls[1] )\n      box = np.array( [ float(alls[2]), float(alls[3]), float(alls[4]), float(alls[5]) ] )\n      boxes.append( box )\n      if len(alls) == 6:\n        face_sizes.append( None )\n      else:\n        face_sizes.append( float(alls[6]) )\n    self.load_data(datas, labels, boxes, face_sizes, num_pts, reset)\n\n  def __getitem__(self, index):\n    assert index >= 0 and index < self.length, 'Invalid index : {:}'.format(index)\n    image = pil_loader( self.datas[index] )\n    target = self.labels[index].copy()\n    return self._process_(image, target, index)\n\n  def _process_(self, image, target, index):\n\n    # transform the image and points\n    if self.transform is not None:\n      image, target = self.transform(image, target)\n\n    # obtain the visiable indicator vector\n    if target.is_none(): nopoints = True\n    else               : nopoints = False\n\n    # If for evaluation not load label, keeps the original data\n    temp_save_wh = target.temp_save_wh\n    ori_size = torch.IntTensor( [temp_save_wh[1], temp_save_wh[0], temp_save_wh[2], temp_save_wh[3]] ) # H, W, Cropped_[x1,y1]\n        \n    if isinstance(image, Image.Image):\n      height, width = image.size[1], image.size[0]\n    elif isinstance(image, torch.FloatTensor):\n      height, width = image.size(1),  image.size(2)\n    else:\n      raise Exception('Unknown type of image : {}'.format( type(image) ))\n\n    if target.is_none() == False:\n      target.apply_bound(width, height)\n      points = target.points.copy()\n      points = torch.from_numpy(points.transpose((1,0))).type(torch.FloatTensor)\n      Hpoint = target.points.copy()\n    else:\n      points = torch.from_numpy(np.zeros((self.NUM_PTS,3))).type(torch.FloatTensor)\n      Hpoint = np.zeros((3, self.NUM_PTS))\n\n    heatmaps, mask = generate_label_map(Hpoint, height//self.downsample, width//self.downsample, self.sigma, self.downsample, nopoints, self.heatmap_type) # H*W*C\n\n    heatmaps = torch.from_numpy(heatmaps.transpose((2, 0, 1))).type(torch.FloatTensor)\n    mask     = torch.from_numpy(mask.transpose((2, 0, 1))).type(torch.ByteTensor)\n  \n    torch_index = torch.IntTensor([index])\n    torch_nopoints = torch.ByteTensor( [ nopoints ] )\n\n    return image, heatmaps, mask, points, torch_index, torch_nopoints, ori_size\n"""
SBR/lib/datasets/VideoDataset.py,11,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom __future__ import print_function\nfrom PIL import Image\nfrom os import path as osp\nimport numpy as np\nimport math\n\nfrom pts_utils import generate_label_map\nfrom .file_utils import load_file_lists\nfrom .dataset_utils import pil_loader\nfrom .dataset_utils import anno_parser\nfrom .point_meta import Point_Meta\nfrom .parse_utils import parse_video_by_indicator\nimport torch\nimport torch.utils.data as data\n\nclass VideoDataset(data.Dataset):\n\n  def __init__(self, transform, sigma, downsample, heatmap_type, data_indicator, video_parser):\n\n    self.transform = transform\n    self.sigma = sigma\n    self.downsample = downsample\n    self.heatmap_type = heatmap_type\n    self.dataset_name = data_indicator\n    self.video_parser = video_parser\n    L, R = parse_video_by_indicator(None, self.video_parser, True)\n    self.video_length = L + R + 1\n    self.center_idx = L\n\n    self.reset()\n    print ('The general dataset initialization done : {:}'.format(self))\n\n  def __repr__(self):\n    return ('{name}(point-num={NUM_PTS}, sigma={sigma}, heatmap_type={heatmap_type}, length={length}, dataset={dataset_name}, parser={video_parser})'.format(name=self.__class__.__name__, **self.__dict__))\n\n  def reset(self, num_pts=-1):\n    self.length = 0\n    self.NUM_PTS = num_pts\n    self.datas = []\n    self.labels = []\n    self.face_sizes = []\n    assert self.dataset_name is not None, 'The dataset name is None'\n\n  def __len__(self):\n    assert len(self.datas) == self.length, 'The length is not correct : {}'.format(self.length)\n    return self.length\n\n  def append(self, data, label, box, face_size):\n    assert osp.isfile(data), 'The image path is not a file : {}'.format(data)\n    self.datas.append( data )\n    if (label is not None) and (label.lower() != 'none'):\n      if isinstance(label, str):\n        assert osp.isfile(label), 'The annotation path is not a file : {}'.format(label)\n        np_points, _ = anno_parser(label, self.NUM_PTS)\n        meta = Point_Meta(self.NUM_PTS, np_points, box, data, self.dataset_name)\n      elif isinstance(label, Point_Meta):\n        meta = label.copy()\n      else:\n        raise NameError('Do not know this label : {}'.format(label))\n    else:\n      meta = Point_Meta(self.NUM_PTS, None, box, data, self.dataset_name)\n    self.labels.append( meta )\n    self.face_sizes.append( face_size )\n    self.length = self.length + 1\n\n  def load_data(self, datas, labels, boxes, face_sizes, num_pts, reset):\n    # each data is a png file name\n    # each label is a Point_Meta class or the general pts format file (anno_parser_v1)\n    assert isinstance(datas, list), 'The type of the datas is not correct : {}'.format( type(datas) )\n    assert isinstance(labels, list) and len(datas) == len(labels), 'The type of the labels is not correct : {}'.format( type(labels) )\n    assert isinstance(boxes, list) and len(datas) == len(boxes), 'The type of the boxes is not correct : {}'.format( type(boxes) )\n    assert isinstance(face_sizes, list) and len(datas) == len(face_sizes), 'The type of the face_sizes is not correct : {}'.format( type(face_sizes) )\n    if reset: self.reset(num_pts)\n    else:     assert self.NUM_PTS == num_pts, 'The number of point is inconsistance : {} vs {}'.format(self.NUM_PTS, num_pts)\n\n    print ('[GeneralDataset] load-data {:} datas begin'.format(len(datas)))\n\n    for idx, data in enumerate(datas):\n      assert isinstance(data, str), 'The type of data is not correct : {}'.format(data)\n      assert osp.isfile(datas[idx]), '{} is not a file'.format(datas[idx])\n      self.append(datas[idx], labels[idx], boxes[idx], face_sizes[idx])\n\n    assert len(self.datas) == self.length, 'The length and the data is not right {} vs {}'.format(self.length, len(self.datas))\n    assert len(self.labels) == self.length, 'The length and the labels is not right {} vs {}'.format(self.length, len(self.labels))\n    assert len(self.face_sizes) == self.length, 'The length and the face_sizes is not right {} vs {}'.format(self.length, len(self.face_sizes))\n    print ('Load data done for the general dataset, which has {} images.'.format(self.length))\n\n  def load_list(self, file_lists, num_pts, reset):\n    lists = load_file_lists(file_lists)\n    print ('GeneralDataset : load-list : load {:} lines'.format(len(lists)))\n\n    datas, labels, boxes, face_sizes = [], [], [], []\n\n    for idx, data in enumerate(lists):\n      alls = [x for x in data.split(' ') if x != '']\n      \n      assert len(alls) == 6 or len(alls) == 7, 'The {:04d}-th line in {:} is wrong : {:}'.format(idx, data)\n      datas.append( alls[0] )\n      if alls[1] == 'None':\n        labels.append( None )\n      else:\n        labels.append( alls[1] )\n      box = np.array( [ float(alls[2]), float(alls[3]), float(alls[4]), float(alls[5]) ] )\n      boxes.append( box )\n      if len(alls) == 6:\n        face_sizes.append( None )\n      else:\n        face_sizes.append( float(alls[6]) )\n    self.load_data(datas, labels, boxes, face_sizes, num_pts, reset)\n\n  def __getitem__(self, index):\n    assert index >= 0 and index < self.length, 'Invalid index : {:}'.format(index)\n    images, is_video_or_not = parse_video_by_indicator(self.datas[index], self.video_parser, False)\n    images = [pil_loader(image) for image in images]\n\n    target = self.labels[index].copy()\n\n    # transform the image and points\n    if self.transform is not None:\n      images, target = self.transform(images, target)\n\n    # obtain the visiable indicator vector\n    if target.is_none(): nopoints = True\n    else               : nopoints = False\n\n    # If for evaluation not load label, keeps the original data\n    temp_save_wh = target.temp_save_wh\n    ori_size = torch.IntTensor( [temp_save_wh[1], temp_save_wh[0], temp_save_wh[2], temp_save_wh[3]] ) # H, W, Cropped_[x1,y1]\n        \n    if isinstance(images[0], Image.Image):\n      height, width = images[0].size[1], images[0].size[0]\n    elif isinstance(images[0], torch.FloatTensor):\n      height, width = images[0].size(1),  images[0].size(2)\n    else:\n      raise Exception('Unknown type of image : {}'.format( type(images[0]) ))\n\n    if target.is_none() == False:\n      target.apply_bound(width, height)\n      points = target.points.copy()\n      points = torch.from_numpy(points.transpose((1,0))).type(torch.FloatTensor)\n      Hpoint = target.points.copy()\n    else:\n      points = torch.from_numpy(np.zeros((self.NUM_PTS,3))).type(torch.FloatTensor)\n      Hpoint = np.zeros((3, self.NUM_PTS))\n\n    heatmaps, mask = generate_label_map(Hpoint, height//self.downsample, width//self.downsample, self.sigma, self.downsample, nopoints, self.heatmap_type) # H*W*C\n\n    heatmaps = torch.from_numpy(heatmaps.transpose((2, 0, 1))).type(torch.FloatTensor)\n    mask     = torch.from_numpy(mask.transpose((2, 0, 1))).type(torch.ByteTensor)\n  \n    torch_index = torch.IntTensor([index])\n    torch_nopoints = torch.ByteTensor( [ nopoints ] )\n    video_indicator = torch.ByteTensor( [is_video_or_not] )\n\n    return torch.stack(images), heatmaps, mask, points, torch_index, torch_nopoints, video_indicator, ori_size\n"""
SBR/lib/datasets/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom .GeneralDataset import GeneralDataset\nfrom .VideoDataset import VideoDataset\nfrom .dataset_utils import pil_loader\nfrom .point_meta import Point_Meta\nfrom .dataset_utils import PTSconvert2str\nfrom .dataset_utils import PTSconvert2box\nfrom .dataset_utils import merge_lists_from_file\n'"
SBR/lib/datasets/dataset_utils.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom os import path as osp\nfrom PIL import Image\nfrom scipy.ndimage.interpolation import zoom\nfrom utils.file_utils import load_txt_file\nimport numpy as np\nimport copy, math\n\ndef pil_loader(path):\n  # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n  with open(path, 'rb') as f:\n    with Image.open(f) as img:\n      return img.convert('RGB')\n\ndef remove_item_from_list(list_to_remove, item):\n  '''\n  remove a single item from a list\n  '''\n  assert isinstance(list_to_remove, list), 'input list is not a list'\n    \n  try:\n    list_to_remove.remove(item)\n  except ValueError:\n    print('Warning!!!!!! Item to remove is not in the list. Remove operation is not done.')\n\n  return list_to_remove\n\ndef anno_parser(anno_path, num_pts):  \n  data, num_lines = load_txt_file(anno_path)                          \n  if data[0].find('version: ') == 0: # 300-W\n    return anno_parser_v0(anno_path, num_pts)\n  else:\n    return anno_parser_v1(anno_path, num_pts)\n\ndef anno_parser_v0(anno_path, num_pts):  \n  '''                        \n  parse the annotation for 300W dataset, which has a fixed format for .pts file                                \n  return:                    \n    pts: 3 x num_pts (x, y, oculusion)                                \n  '''                        \n  data, num_lines = load_txt_file(anno_path)                          \n  assert data[0].find('version: ') == 0, 'version is not correct'     \n  assert data[1].find('n_points: ') == 0, 'number of points in second line is not correct'                     \n  assert data[2] == '{' and data[-1] == '}', 'starting and end symbol is not correct'                          \n                             \n  assert data[0] == 'version: 1' or data[0] == 'version: 1.0', 'The version is wrong : {}'.format(data[0])\n  n_points = int(data[1][len('n_points: '):])                         \n                             \n  assert num_lines == n_points + 4, 'number of lines is not correct'    # 4 lines for general information: version, n_points, start and end symbol      \n  assert num_pts == n_points, 'number of points is not correct'\n                             \n  # read points coordinate   \n  pts = np.zeros((3, n_points), dtype='float32')                      \n  line_offset = 3    # first point starts at fourth line              \n  point_set = set()\n  for point_index in range(n_points):                                \n    try:                     \n      pts_list = data[point_index + line_offset].split(' ')       # x y format                                 \n      if len(pts_list) > 2:    # handle edge case where additional whitespace exists after point coordinates   \n        pts_list = remove_item_from_list(pts_list, '')              \n      pts[0, point_index] = float(pts_list[0])                        \n      pts[1, point_index] = float(pts_list[1])                        \n      pts[2, point_index] = float(1)      # oculusion flag, 0: oculuded, 1: visible. We use 1 for all points since no visibility is provided by 300-W   \n      point_set.add( point_index )\n    except ValueError:       \n      print('error in loading points in %s' % anno_path)              \n  return pts, point_set\n\ndef anno_parser_v1(anno_path, NUM_PTS, one_base=True):\n  '''\n  parse the annotation for MUGSY-Full-Face dataset, which has a fixed format for .pts file\n  return: pts: 3 x num_pts (x, y, oculusion)\n  '''\n  data, n_points = load_txt_file(anno_path)\n  assert n_points <= NUM_PTS, '{} has {} points'.format(anno_path, n_points)\n  # read points coordinate\n  pts = np.zeros((3, NUM_PTS), dtype='float32')\n  point_set = set()\n  for line in data:\n    try:\n      idx, point_x, point_y, oculusion = line.split(' ')\n      idx, point_x, point_y, oculusion = int(idx), float(point_x), float(point_y), oculusion == 'True'\n      if one_base==False: idx = idx+1\n      assert idx >= 1 and idx <= NUM_PTS, 'Wrong idx of points : {:02d}-th in {:s}'.format(idx, anno_path)\n      pts[0, idx-1] = point_x\n      pts[1, idx-1] = point_y\n      pts[2, idx-1] = float( oculusion )\n      point_set.add(idx)\n    except ValueError:\n      raise Exception('error in loading points in {}'.format(anno_path))\n  return pts, point_set\n\ndef PTSconvert2str(points):\n  assert isinstance(points, np.ndarray) and len(points.shape) == 2, 'The points is not right : {}'.format(points)\n  assert points.shape[0] == 2 or points.shape[0] == 3, 'The shape of points is not right : {}'.format(points.shape)\n  string = ''\n  num_pts = points.shape[1]\n  for i in range(num_pts):\n    ok = False\n    if points.shape[0] == 3 and bool(points[2, i]) == True: \n      ok = True\n    elif points.shape[0] == 2:\n      ok = True\n\n    if ok:\n      string = string + '{:02d} {:.2f} {:.2f} True\\n'.format(i+1, points[0, i], points[1, i])\n  string = string[:-1]\n  return string\n\ndef PTSconvert2box(points, expand_ratio=None):\n  assert isinstance(points, np.ndarray) and len(points.shape) == 2, 'The points is not right : {}'.format(points)\n  assert points.shape[0] == 2 or points.shape[0] == 3, 'The shape of points is not right : {}'.format(points.shape)\n  if points.shape[0] == 3:\n    points = points[:2, points[-1,:].astype('bool') ]\n  elif points.shape[0] == 2:\n    points = points[:2, :]\n  else:\n    raise Exception('The shape of points is not right : {}'.format(points.shape))\n  assert points.shape[1] >= 2, 'To get the box of points, there should be at least 2 vs {}'.format(points.shape)\n  box = np.array([ points[0,:].min(), points[1,:].min(), points[0,:].max(), points[1,:].max() ])\n  W = box[2] - box[0]\n  H = box[3] - box[1]\n  assert W > 0 and H > 0, 'The size of box should be greater than 0 vs {}'.format(box)\n  if expand_ratio is not None:\n    box[0] = int( math.floor(box[0] - W * expand_ratio) )\n    box[1] = int( math.floor(box[1] - H * expand_ratio) )\n    box[2] = int( math.ceil(box[2] + W * expand_ratio) )\n    box[3] = int( math.ceil(box[3] + H * expand_ratio) )\n  return box\n\ndef for_generate_box_str(anno_path, num_pts, extend):\n  if isinstance(anno_path, str):\n    points, _ = anno_parser(anno_path, num_pts)\n  else:\n    points = anno_path.copy()\n  box = PTSconvert2box(points, extend)\n  return '{:.2f} {:.2f} {:.2f} {:.2f}'.format(box[0], box[1], box[2], box[3])\n    \ndef resize_heatmap(maps, height, width, order=3):\n  # maps  = np.ndarray with shape [height, width, channels]\n  # order = 0 Nearest\n  # order = 1 Bilinear\n  # order = 2 Cubic\n  assert isinstance(maps, np.ndarray) and len(maps.shape) == 3, 'maps type : {}'.format(type(maps))\n  \n  scale = tuple(np.array([height,width], dtype=float) / np.array(maps.shape[:2]))\n  return zoom(maps, scale + (1,), order=order)\n\ndef analysis_dataset(dataset):\n  all_values = np.zeros((3,len(dataset.datas)), dtype=np.float64)\n  hs = np.zeros((len(dataset.datas),), dtype=np.float64)\n  ws = np.zeros((len(dataset.datas),), dtype=np.float64)\n\n  for index, image_path in enumerate(dataset.datas):\n    img = pil_loader(image_path)\n    ws[index] = img.size[0]\n    hs[index] = img.size[1]\n    img = np.array(img)\n    all_values[:, index] = np.mean(np.mean(img, axis=0), axis=0).astype('float64')\n  mean = np.mean(all_values, axis=1)\n  std  = np.std (all_values, axis=1)\n  return mean, std, ws, hs\n\ndef split_datasets(dataset, point_ids):\n  sub_dataset = copy.deepcopy(dataset)\n  assert len(point_ids) > 0\n  assert False, 'un finished'\n\ndef convert68to49(points):\n  points = points.copy()\n  assert len(points.shape) == 2 and (points.shape[0] == 3 or points.shape[0] == 2) and points.shape[1] == 68, 'The shape of points is not right : {}'.format(points.shape)\n  out = np.ones((68,)).astype('bool')\n  out[[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,60,64]] = False\n  cpoints = points[:, out]\n  assert len(cpoints.shape) == 2 and cpoints.shape[1] == 49\n  return cpoints\n\ndef convert68to51(points):\n  points = points.copy()\n  assert len(points.shape) == 2 and (points.shape[0] == 3 or points.shape[0] == 2) and points.shape[1] == 68, 'The shape of points is not right : {}'.format(points.shape)\n  out = np.ones((68,)).astype('bool')\n  out[[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]] = False\n  cpoints = points[:, out]\n  assert len(cpoints.shape) == 2 and cpoints.shape[1] == 51\n  return cpoints\n\ndef merge_lists_from_file(file_paths, seed=None):\n  assert file_paths is not None, 'The input can not be None'\n  if isinstance(file_paths, str):\n    file_paths = [ file_paths ]\n  print ('merge lists from {} files with seed={} for random shuffle'.format(len(file_paths), seed))\n  # load the data\n  all_data = []\n  for file_path in file_paths:\n    assert osp.isfile(file_path), '{} does not exist'.format(file_path)\n    listfile = open(file_path, 'r')\n    listdata = listfile.read().splitlines()\n    listfile.close()\n    all_data = all_data + listdata\n  total = len(all_data)\n  print ('merge all the lists done, total : {}'.format(total))\n  # random shuffle\n  if seed is not None:\n    np.random.seed(seed)\n    order = np.random.permutation(total).tolist()\n    new_data = [ all_data[idx] for idx in order ]\n    all_data = new_data\n  return all_data\n"""
SBR/lib/datasets/file_utils.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom os import path as osp\n\ndef load_file_lists(file_paths):\n  if isinstance(file_paths, str):\n    file_paths = [ file_paths ]\n  print ('Function [load_lists] input {:} files'.format(len(file_paths)))\n  all_strings = []\n  for file_idx, file_path in enumerate(file_paths):\n    assert osp.isfile(file_path), 'The {:}-th path : {:} is not a file.'.format(file_idx, file_path)\n    listfile = open(file_path, 'r')\n    listdata = listfile.read().splitlines()\n    listfile.close()\n    print ('Load [{:d}/{:d}]-th list : {:} with {:} images'.format(file_idx, len(file_paths), file_path, len(listdata)))\n    all_strings += listdata\n  return all_strings\n"""
SBR/lib/datasets/parse_utils.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport os\nimport warnings\nfrom os import path as osp\n\ndef parse_basic(ori_filename, length_l, length_r):\n  folder = osp.dirname(ori_filename)\n  filename = osp.basename(ori_filename)\n  # 300-VW\n  if folder[-10:] == 'extraction':\n    assert filename[-4:] == '.png', 'The filename is not right : {}'.format(filename)\n    idx = int(filename[: filename.find('.png')])\n    assert idx >= 0, 'The index must be greater than 0'\n    images = []\n    for i in range(idx-length_l, idx+length_r+1):\n      path = osp.join(folder, '{:06d}.png'.format(i))\n      if not osp.isfile(path):\n        xpath = osp.join(folder, '{:06d}.png'.format(idx))\n        warnings.warn('Path [{}] does not exist, maybe it reaches the start or end of the video, use {} instead.'.format(path, xpath), UserWarning)\n        path = xpath\n      assert osp.isfile(path), '!!WRONG file path : {}, the original frame is {}'.format(path, filename)\n      images.append(path)\n    return images, True\n  # YouTube Cele..\n  elif folder.find('YouTube_Celebrities_Annotation') > 0:\n    assert filename[-4:] == '.png', 'The filename is not right : {}'.format(filename)\n    idx = int(filename[filename.find('_')+1: filename.find('.png')])\n    assert idx >= 0, 'The index must be greater than 0'\n    images = []\n    for i in range(idx-length_l, idx+length_r+1):\n      path = osp.join(folder, 'frame_{:05d}.png'.format(i))\n      if not osp.isfile(path):\n        xpath = osp.join(folder, 'frame_{:05d}.png'.format(idx))\n        warnings.warn('Path [{}] does not exist, maybe it reaches the start or end of the video, use {} instead.'.format(path, xpath), UserWarning)\n        path = xpath\n      assert osp.isfile(path), '!!WRONG file path : {}, the original frame is {}'.format(path, filename)\n      images.append(path)\n    return images, True\n  # Talking Face..\n  elif folder.find('talking_face') > 0:\n    assert filename[-4:] == '.jpg', 'The filename is not right : {}'.format(filename)\n    idx = int(filename[filename.find('_')+1: filename.find('.jpg')])\n    assert idx >= 0, 'The index must be greater than 0'\n    images = []\n    for i in range(idx-length_l, idx+length_r+1):\n      path = osp.join(folder, 'franck_{:05d}.png'.format(i))\n      if not osp.isfile(path):\n        xpath = osp.join(folder, 'franck_{:05d}.png'.format(idx))\n        warnings.warn('Path [{}] does not exist, maybe it reaches the start or end of the video, use {} instead.'.format(path, xpath), UserWarning)\n        path = xpath\n      assert osp.isfile(path), '!!WRONG file path : {}, the original frame is {}'.format(path, filename)\n      images.append(path)\n    return images, True\n  # YouTube Face..\n  elif folder.find('YouTube-Face') > 0:\n    assert filename[-4:] == '.jpg', 'The filename is not right : {}'.format(filename)\n    splits = filename.split('.')\n    assert len(splits) == 3, 'The format is not right : {}'.format(filename)\n    idx = int(splits[1])\n    images = []\n    for i in range(idx-length_l, idx+length_r+1):\n      path = osp.join(folder, '{}.{}.{}'.format(splits[0], i, splits[2]))\n      if not osp.isfile(path):\n        xpath = osp.join(folder, '{}.{}.{}'.format(splits[0], idx, splits[2]))\n        warnings.warn('Path [{}] does not exist, maybe it reaches the start or end of the video, use {} instead.'.format(path, xpath), UserWarning)\n        path = xpath\n      assert osp.isfile(path), '!!WRONG file path : {}, the original frame is {}'.format(path, filename)\n      images.append(path)\n    return images, True\n  elif folder.find('demo-pams') > 0 or folder.find('demo-sbrs') > 0:\n    assert filename[-4:] == '.png', 'The filename is not right : {}'.format(filename)\n    assert filename[:5] == 'image', 'The filename is not right : {}'.format(filename)\n    splits = filename.split('.')\n    assert len(splits) == 2, 'The format is not right : {}'.format(filename)\n    idx = int(splits[0][5:])\n    images = []\n    for i in range(idx-length_l, idx+length_r+1):\n      path = osp.join(folder, 'image{:04d}.{:}'.format(i, splits[1]))\n      if not osp.isfile(path):\n        xpath = osp.join(folder, 'image{:04d}.{:}'.format(idx, splits[1]))\n        warnings.warn('Path [{}] does not exist, maybe it reaches the start or end of the video, use {} instead.'.format(path, xpath), UserWarning)\n        path = xpath\n      assert osp.isfile(path), '!!WRONG file path : {}, the original frame is {}'.format(path, filename)\n      images.append(path)\n    return images, True\n  else:\n    return [ori_filename] * (length_l+length_r+1), False\n\ndef parse_video_by_indicator(image_path, parser, return_info=False):\n  if parser is None or parser.lower() == 'none':\n    method, offset_l, offset_r = 'None', 0, 0\n  else:\n    parser = parser.split('-')\n    assert len(parser) == 3, 'The video parser must be 3 elements : {:}'.format(parser)\n    method, offset_l, offset_r = parser[0], int(parser[1]), int(parser[2])\n  if return_info:\n    return offset_l, offset_r\n  else:\n    images, is_video_or_not = parse_basic(image_path, offset_l, offset_r)\n  return images, is_video_or_not\n"""
SBR/lib/datasets/point_meta.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom PIL import Image\nfrom scipy.ndimage.interpolation import zoom\nfrom utils.file_utils import load_txt_file\nfrom .dataset_utils import convert68to49 as _convert68to49\nfrom .dataset_utils import convert68to51 as _convert68to51\nimport numpy as np\nimport copy, math\n\nclass Point_Meta():\n  # points: 3 x num_pts (x, y, oculusion)\n  # image_size: original [width, height]\n  def __init__(self, num_point, points, box, image_path, dataset_name):\n\n    self.num_point = num_point\n    assert len(box.shape) == 1 and box.shape[0] == 4, 'The shape of box is not right : {}'.format( box )\n    self.box = box.copy()\n    if points is None:\n      self.points = points\n    else:\n      assert len(points.shape) == 2 and points.shape[0] == 3 and points.shape[1] == self.num_point, 'The shape of point is not right : {}'.format( points )\n      self.points = points.copy()\n    self.update_center()\n    self.image_path = image_path\n    self.datasets = dataset_name\n    self.temp_save_wh = None\n\n  def __repr__(self):\n    return ('{name}(number of point={num_point})'.format(name=self.__class__.__name__, **self.__dict__))\n\n  def convert68to49(self):\n    if self.points is not None:\n      self.points = _convert68to49(self.points)\n\n  def convert68to51(self):\n    if self.points is not None:\n      self.points = _convert68to51(self.points)\n\n  def update_center(self):\n    if self.points is not None:\n      self.center = np.mean(self.points[:2, self.points[2,:]>0], axis=1)\n    else:\n      self.center = np.array([ (self.box[0]+self.box[2])/2, (self.box[1]+self.box[3])/2 ])\n\n  def apply_bound(self, width, height):\n    if self.points is not None:\n      oks = np.vstack((self.points[0, :] >= 0, self.points[1, :] >=0, self.points[0, :] <= width, self.points[1, :] <= height, self.points[2, :].astype('bool')))\n      oks = oks.transpose((1,0))\n      self.points[2, :] = np.sum(oks, axis=1) == 5\n    self.box[0], self.box[1] = np.max([self.box[0], 0]),     np.max([self.box[1], 0])\n    self.box[2], self.box[3] = np.min([self.box[2], width]), np.min([self.box[3], height])\n\n  def apply_scale(self, scale):\n    if len(scale) == 1:   # scale the same size for both x and y\n      if self.points is not None:\n        self.points[:2, self.points[2,:]>0] = self.points[:2, self.points[2,:]>0] * scale[0]\n      self.center                         = self.center   * scale[0]\n      self.box[0], self.box[1]            = self.box[0] * scale[0], self.box[1] * scale[0]\n      self.box[2], self.box[3]            = self.box[2] * scale[0], self.box[3] * scale[0]\n    elif len(scale) == 2: # scale the width and height\n      if self.points is not None:\n        self.points[0, self.points[2,:]>0] = self.points[0, self.points[2,:]>0] * scale[0]\n        self.points[1, self.points[2,:]>0] = self.points[1, self.points[2,:]>0] * scale[1]\n      self.center[0]                     = self.center[0] * scale[0]\n      self.center[1]                     = self.center[1] * scale[1]\n      self.box[0], self.box[1]            = self.box[0] * scale[0], self.box[1] * scale[1]\n      self.box[2], self.box[3]            = self.box[2] * scale[0], self.box[3] * scale[1]\n    else:\n      assert False, 'Does not support this scale : {}'.format(scale)\n\n  def apply_offset(self, ax=None, ay=None):\n    if ax is not None:\n      if self.points is not None:\n        self.points[0, self.points[2,:]>0] = self.points[0, self.points[2,:]>0] + ax\n      self.center[0]                     = self.center[0] + ax\n      self.box[0], self.box[2]           = self.box[0] + ax, self.box[2] + ax\n    if ay is not None:\n      if self.points is not None:\n        self.points[1, self.points[2,:]>0] = self.points[1, self.points[2,:]>0] + ay\n      self.center[1]                     = self.center[1] + ay\n      self.box[1], self.box[3]           = self.box[1] + ay, self.box[3] + ay\n\n  def apply_rotate(self, center, degree):\n    degree = math.radians(-degree)\n    if self.points is not None:\n      vis_xs = self.points[0, self.points[2,:]>0]\n      vis_ys = self.points[1, self.points[2,:]>0]\n      self.points[0, self.points[2,:]>0] = (vis_xs - center[0]) * np.cos(degree) - (vis_ys - center[1]) * np.sin(degree) + center[0]\n      self.points[1, self.points[2,:]>0] = (vis_xs - center[0]) * np.sin(degree) + (vis_ys - center[1]) * np.cos(degree) + center[1]\n    # rotate the box\n    corners = np.zeros((4,2))\n    corners[0,0], corners[0,1] = self.box[0], self.box[1]\n    corners[1,0], corners[1,1] = self.box[0], self.box[3]\n    corners[2,0], corners[2,1] = self.box[2], self.box[1]\n    corners[3,0], corners[3,1] = self.box[2], self.box[3]\n    corners[:, 0] = (corners[:, 0] - center[0]) * np.cos(degree) - (corners[:, 1] - center[1]) * np.sin(degree) + center[0]\n    corners[:, 1] = (corners[:, 0] - center[0]) * np.sin(degree) - (corners[:, 1] - center[1]) * np.cos(degree) + center[1]\n    self.box[0], self.box[1] = corners[0,0], corners[0,1]\n    self.box[2], self.box[3] = corners[3,0], corners[3,1]\n    \n  def apply_horizontal_flip(self, width):\n    self.points[0, :] = width - self.points[0, :] - 1\n    # Mugsy spefic or Synthetic\n    if self.datasets == 'Mugsy.full_face_v1':\n      ori = np.array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n      pos = np.array([ 3,  4,  1,  2,  9, 10, 11, 12,  5,  6,  7,  8, 14, 13, 15, 16, 17, 18, 19, 20])\n      self.points[:, pos-1] = self.points[:, ori-1]\n    elif self.datasets == 'Synthetic.v1':\n      ori = np.array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n      pos = np.array([ 3,  4,  1,  2,  9, 10, 11, 12,  5,  6,  7,  8, 14, 13, 15, 16, 17, 18, 19, 20])\n      self.points[:, pos-1] = self.points[:, ori-1]\n    else:\n      assert False, 'Does not support {}.{}'.format(self.datasets, self.subsets)\n    \n  # all points' range [0, w) [0, h)\n  def check_nan(self):\n    if math.isnan(self.center[0]) or math.isnan(self.center[1]):\n      return True\n    for i in range(self.num_point):\n      if self.points[2, i] > 0:\n        if math.isnan(self.points[0, i]) or math.isnan(self.points[1, i]):\n          return True\n    return False\n\n  def visiable_pts_num(self):\n    ans = self.points[2,:]>0\n    return np.sum(ans)\n\n  def set_precrop_wh(self, W, H, x1, y1, x2, y2):\n    self.temp_save_wh = [W, H, x1, y1, x2, y2]\n\n  def get_box(self):\n    return self.box.copy()\n\n  def get_points(self):\n    if self.points is not None:\n      return self.points.copy()\n    else:\n      return np.zeros((3, self.num_point), dtype='float32')\n\n  def is_none(self):\n    assert self.box is not None, 'The box should not be None'\n    return self.points is None\n\n  def copy(self):\n    return copy.deepcopy(self)\n"""
SBR/lib/lk/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom .basic_lk import lk_tensor_track\nfrom .basic_lk_batch import lk_tensor_track_batch\nfrom .basic_lk_batch import lk_forward_backward_batch\n'"
SBR/lib/lk/basic_lk.py,7,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numbers, math\nimport numpy as np\nimport models.model_utils as MU\nfrom .basic_utils import SobelConv\nfrom .basic_utils import ComputeGradient, Generate_Weight, warp_feature\nfrom .basic_utils import torch_inverse\n\ndef lk_tensor_track(feature_old, feature_new, pts_locations, patch_size, max_step, threshold=0.0001):\n  # feature[old,new] : 3-D tensor [C, H, W]\n  # pts_locations is a 2-D point [ Y, X ]\n  assert feature_old.dim() == 3 and feature_new.dim() == 3, 'The dimension is not right : {} and {}'.format(feature_old.dim(), feature_new.dim())\n  C, H, W = feature_old.size(0), feature_old.size(1), feature_old.size(2)\n  assert C == feature_new.size(0) and H == feature_new.size(1) and W == feature_new.size(2), 'The size is not right : {}'.format(feature_new.size())\n  assert pts_locations.dim() == 1 and pts_locations.size(0) == 2, 'The location is not right : {}'.format(pts_locations)\n  if isinstance(patch_size, int): patch_size = (patch_size, patch_size)\n  assert isinstance(patch_size, tuple) and len(patch_size) == 2 and isinstance(max_step, int), 'The format of lk-parameters are not right : {}, {}'.format(patch_size, max_step)\n  assert isinstance(patch_size[0], int) and isinstance(patch_size[1], int), 'The format of lk-parameters are not right : {}'.format(patch_size)\n\n  def abserror(deltap):\n    deltap = MU.variable2np(deltap)\n    return float(np.sqrt(np.sum(deltap*deltap)))\n  \n  weight_map = Generate_Weight( [patch_size[0]*2+1, patch_size[1]*2+1] ) # [H, W]\n  with torch.cuda.device_of(feature_old):\n    weight_map = MU.np2variable(weight_map, feature_old.is_cuda, False).unsqueeze(0)\n\n    feature_T = warp_feature(feature_old, pts_locations, patch_size)\n    gradiant_x = ComputeGradient(feature_T, 'x')\n    gradiant_y = ComputeGradient(feature_T, 'y')\n    J = torch.stack([gradiant_x, gradiant_y])\n    weightedJ = J*weight_map\n    H = torch.mm( weightedJ.view(2,-1), J.view(2, -1).transpose(1,0) )\n    inverseH = torch_inverse(H)\n\n    for step in range(max_step):\n      # Step-1 Warp I with W(x,p) to compute I(W(x;p))\n      feature_I = warp_feature(feature_new, pts_locations, patch_size)\n      # Step-2 Compute the error feature\n      r = feature_I - feature_T\n      # Step-7 Compute sigma\n      sigma = torch.mm(weightedJ.view(2,-1), r.view(-1, 1))\n      # Step-8 Compute delta-p\n      deltap = torch.mm(inverseH, sigma).squeeze(1)\n      pts_locations = pts_locations - deltap\n      if abserror(deltap) < threshold: break\n\n  return pts_locations\n"""
SBR/lib/lk/basic_lk_batch.py,14,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numbers, math\nimport numpy as np\nimport models.model_utils as MU\nfrom .basic_utils import SobelConv, Generate_Weight\nfrom .basic_utils_batch import torch_inverse_batch, warp_feature_batch\n\n""""""\npeak_config = {}\ndef obtain_config(heatmap, radius):\n  identity_str = \'{}-{}\'.format(radius, heatmap.get_device() if heatmap.is_cuda else -1 )\n  if identity_str not in peak_config:\n    if heatmap.is_cuda:\n      with torch.cuda.device_of(heatmap):\n        X = MU.np2variable(torch.arange(-radius, radius+1), heatmap.is_cuda, False).view(1, 1, radius*2+1)\n        Y = MU.np2variable(torch.arange(-radius, radius+1), heatmap.is_cuda, False).view(1, radius*2+1, 1)\n    else:\n      X = MU.np2variable(torch.arange(-radius, radius+1), heatmap.is_cuda, False).view(1, 1, radius*2+1)\n      Y = MU.np2variable(torch.arange(-radius, radius+1), heatmap.is_cuda, False).view(1, radius*2+1, 1)\n    peak_config[ identity_str ] = [X, Y]\n  return peak_config[ identity_str ]\n""""""\n\n\ndef lk_tensor_track_batch(feature_old, feature_new, pts_locations, patch_size, max_step, feature_template=None):\n  # feature[old,new] : 4-D tensor [1, C, H, W]\n  # pts_locations is a 2-D tensor [Num-Pts, (Y,X)]\n  if feature_new.dim() == 3:\n    feature_new = feature_new.unsqueeze(0)\n  if feature_old is not None and feature_old.dim() == 3:\n    feature_old = feature_old.unsqueeze(0)\n  assert feature_new.dim() == 4, \'The dimension of feature-new is not right : {}.\'.format(feature_new.dim())\n  BB, C, H, W = list(feature_new.size())\n  if feature_old is not None:\n    assert 1 == feature_old.size(0) and 1 == BB, \'The first dimension of feature should be one not {}\'.format(feature_old.size())\n    assert C == feature_old.size(1) and H == feature_old.size(2) and W == feature_old.size(3), \'The size is not right : {}\'.format(feature_old.size())\n  assert isinstance(patch_size, int), \'The format of lk-parameters are not right : {}\'.format(patch_size)\n  num_pts = pts_locations.size(0)\n  device = feature_new.device\n\n  weight_map = Generate_Weight( [patch_size*2+1, patch_size*2+1] ) # [H, W]\n  with torch.no_grad():\n    weight_map = torch.tensor(weight_map).view(1, 1, 1, patch_size*2+1, patch_size*2+1).to(device)\n\n    sobelconvx = SobelConv(\'x\', feature_new.dtype).to(device)\n    sobelconvy = SobelConv(\'y\', feature_new.dtype).to(device)\n  \n  # feature_T should be a [num_pts, C, patch, patch] tensor\n  if feature_template is None:\n    feature_T = warp_feature_batch(feature_old, pts_locations, patch_size)\n  else:\n    assert feature_old is None, \'When feature_template is not None. feature_old must be None\'\n    feature_T = feature_template\n  assert feature_T.size(2) == patch_size * 2 + 1 and feature_T.size(3) == patch_size * 2 + 1, \'The size of feature-template is not ok : {}\'.format(feature_T.size())\n  gradiant_x = sobelconvx(feature_T)\n  gradiant_y = sobelconvy(feature_T)\n  J = torch.stack([gradiant_x, gradiant_y], dim=1)\n  weightedJ = J * weight_map\n  H = torch.bmm( weightedJ.view(num_pts,2,-1), J.view(num_pts, 2, -1).transpose(2,1) )\n  inverseH = torch_inverse_batch(H)\n\n  #print (\'PTS : {}\'.format(pts_locations))\n  for step in range(max_step):\n    # Step-1 Warp I with W(x,p) to compute I(W(x;p))\n    feature_I = warp_feature_batch(feature_new, pts_locations, patch_size)\n    # Step-2 Compute the error feature\n    r = feature_I - feature_T\n    # Step-7 Compute sigma\n    sigma = torch.bmm(weightedJ.view(num_pts,2,-1), r.view(num_pts,-1, 1))\n    # Step-8 Compute delta-p\n    deltap = torch.bmm(inverseH, sigma).squeeze(-1)\n    pts_locations = pts_locations - deltap\n\n  return pts_locations\n\n\ndef lk_forward_backward_batch(features, locations, window, steps):\n  sequence, C, H, W = list(features.size())\n  seq, num_pts, _ = list(locations.size())\n  assert seq == sequence, \'{:} vs {:}\'.format(features.size(), locations.size())\n\n  previous_pts = [ locations[0] ]\n  for iseq in range(1, sequence):\n    feature_old = features.narrow(0, iseq-1, 1)\n    feature_new = features.narrow(0, iseq  , 1)\n    nextPts = lk_tensor_track_batch(feature_old, feature_new, previous_pts[iseq-1], window, steps, None)\n    previous_pts.append(nextPts)\n\n  fback_pts = [None] * (sequence-1) + [ previous_pts[-1] ]\n  for iseq in range(sequence-2, -1, -1):\n    feature_old = features.narrow(0, iseq+1, 1)\n    feature_new = features.narrow(0, iseq  , 1)\n    backPts = lk_tensor_track_batch(feature_old, feature_new, fback_pts[iseq+1]   , window, steps, None)\n    fback_pts[iseq] = backPts\n\n  back_pts = [None] * (sequence-1) + [ locations[-1] ]\n  for iseq in range(sequence-2, -1, -1):\n    feature_old = features.narrow(0, iseq+1, 1)\n    feature_new = features.narrow(0, iseq  , 1)\n    backPts = lk_tensor_track_batch(feature_old, feature_new, back_pts[iseq+1]    , window, steps, None)\n    back_pts[iseq] = backPts\n\n  return torch.stack(previous_pts), torch.stack(fback_pts), torch.stack(back_pts)\n'"
SBR/lib/lk/basic_utils.py,8,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numbers, math\nimport numpy as np\nimport models.model_utils as MU\n\n#### The utils for LK\n\ndef torch_inverse(deltp):\n  assert deltp.dim() == 2 and deltp.size(0) == 2 and deltp.size(1) == 2, 'The deltp format is not right : {}'.format( deltp.size() )\n  a, b, c, d = deltp[0,0], deltp[0,1], deltp[1,0], deltp[1,1]\n  a = a + np.finfo(float).eps\n  d = d + np.finfo(float).eps\n  divide = a*d-b*c\n  inverse = torch.cat([d, -b, -c, a]).view(2,2)\n  return inverse / divide\n\nclass SobelConv(nn.Module):\n  def __init__(self, tag, dtype):\n    super(SobelConv, self).__init__()\n    if tag == 'x':\n      Sobel = np.array([ [-1./8, 0, 1./8], [-2./8, 0, 2./8], [ -1./8, 0, 1./8] ])\n      #Sobel = np.array([ [ 0, 0, 0], [-0.5,0,0.5], [ 0, 0, 0] ])\n    elif tag == 'y':\n      Sobel = np.array([ [ -1./8, -2./8, -1./8], [ 0, 0, 0], [ 1./8, 2./8, 1./8] ])\n      #Sobel = np.array([ [ 0,-0.5, 0], [ 0, 0, 0], [ 0, 0.5, 0] ])\n    else:\n      raise NameError('Do not know this tag for Sobel Kernel : {}'.format(tag))\n    Sobel = torch.from_numpy(Sobel).type(dtype)\n    Sobel = Sobel.view(1, 1, 3, 3) \n    self.register_buffer('weight', Sobel)\n    self.tag = tag\n\n  def forward(self, input):\n    weight = self.weight.expand(input.size(1), 1, 3, 3).contiguous()\n    return F.conv2d(input, weight, groups=input.size(1), padding=1)\n\n  def __repr__(self):\n    return ('{name}(tag={tag})'.format(name=self.__class__.__name__, **self.__dict__))\n\ndef ComputeGradient(feature, tag):\n  if feature.dim() == 3:\n    feature = feature.unsqueeze(0)\n    squeeze = True\n  else:\n    squeeze = False\n  assert feature.dim() == 4, 'feature must be [batch x C x H x W] not {}'.format(feature.size())\n  sobel = SobelConv(tag)\n  if feature.is_cuda: sobel.cuda()\n  if squeeze: return sobel(feature).squeeze(0)\n  else:       return sobel(feature)\n\ndef Generate_Weight(patch_size, sigma=None):\n  assert isinstance(patch_size, list) or isinstance(patch_size, tuple)\n  assert patch_size[0] > 0 and patch_size[1] > 0, 'the patch size must > 0 rather :{}'.format(patch_size)\n  center = [(patch_size[0]-1.)/2, (patch_size[1]-1.)/2]\n  maps = np.fromfunction( lambda x, y: (x-center[0])**2 + (y-center[1])**2, (patch_size[0], patch_size[1]), dtype=int)\n  if sigma is None: sigma = min(patch_size[0], patch_size[1])/2.\n  maps = np.exp(maps / -2.0 / sigma / sigma)\n  maps[0, :] = maps[-1, :] = maps[:, 0] = maps[:, -1] = 0\n  return maps.astype(np.float32)\n\ndef warp_feature(feature, pts_location, patch_size):\n  # pts_location is [X,Y], patch_size is [H,W]\n  C, H, W = feature.size(0), feature.size(1), feature.size(2)\n  def normalize(x, L):\n    return -1. + 2. * x / (L-1)\n\n  crop_box = [pts_location[0]-patch_size[1], pts_location[1]-patch_size[0], pts_location[0]+patch_size[1], pts_location[1]+patch_size[0]]\n  crop_box[0] = normalize(crop_box[0], W)\n  crop_box[1] = normalize(crop_box[1], H)\n  crop_box[2] = normalize(crop_box[2], W)\n  crop_box[3] = normalize(crop_box[3], H)\n  affine_parameter = [(crop_box[2]-crop_box[0])/2, MU.np2variable(torch.zeros(1),feature.is_cuda,False), (crop_box[0]+crop_box[2])/2,\n                      MU.np2variable(torch.zeros(1),feature.is_cuda,False), (crop_box[3]-crop_box[1])/2, (crop_box[1]+crop_box[3])/2]\n\n  affine_parameter = torch.cat(affine_parameter).view(2, 3)\n \n  theta = affine_parameter.unsqueeze(0)\n  feature = feature.unsqueeze(0)\n  grid_size = torch.Size([1, 1, 2*patch_size[0]+1, 2*patch_size[1]+1])\n  grid = F.affine_grid(theta, grid_size)\n  sub_feature = F.grid_sample(feature, grid).squeeze(0)\n  return sub_feature\n"""
SBR/lib/lk/basic_utils_batch.py,8,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numbers, math\nimport numpy as np\nimport models.model_utils as MU\n\n#### The utils for LK\ndef torch_inverse_batch(deltp):\n  # deltp must be [K,2]\n  assert deltp.dim() == 3 and deltp.size(1) == 2 and deltp.size(2) == 2, 'The deltp format is not right : {}'.format( deltp.size() )\n  a, b, c, d = deltp[:,0,0], deltp[:,0,1], deltp[:,1,0], deltp[:,1,1]\n  a = a + np.finfo(float).eps\n  d = d + np.finfo(float).eps\n  divide = a*d-b*c+np.finfo(float).eps\n  inverse = torch.stack([d, -b, -c, a], dim=1) / divide.unsqueeze(1)\n  return inverse.view(-1,2,2)\n\n\ndef warp_feature_batch(feature, pts_location, patch_size):\n  # feature must be [1,C,H,W] and pts_location must be [Num-Pts, (x,y)]\n  _, C, H, W = list(feature.size())\n  num_pts = pts_location.size(0)\n  assert isinstance(patch_size, int) and feature.size(0) == 1 and pts_location.size(1) == 2, 'The shapes of feature or points are not right : {} vs {}'.format(feature.size(), pts_location.size())\n  assert W > 1 and H > 1, 'To guarantee normalization {}, {}'.format(W, H)\n\n  def normalize(x, L):\n    return -1. + 2. * x / (L-1)\n\n  crop_box = torch.cat([pts_location-patch_size, pts_location+patch_size], 1)\n  crop_box[:, [0,2]] = normalize(crop_box[:, [0,2]], W)\n  crop_box[:, [1,3]] = normalize(crop_box[:, [1,3]], H)\n \n  affine_parameter = [(crop_box[:,2]-crop_box[:,0])/2, crop_box[:,0]*0, (crop_box[:,2]+crop_box[:,0])/2,\n                      crop_box[:,0]*0, (crop_box[:,3]-crop_box[:,1])/2, (crop_box[:,3]+crop_box[:,1])/2]\n  #affine_parameter = [(crop_box[:,2]-crop_box[:,0])/2, MU.np2variable(torch.zeros(num_pts),feature.is_cuda,False), (crop_box[:,2]+crop_box[:,0])/2,\n  #                    MU.np2variable(torch.zeros(num_pts),feature.is_cuda,False), (crop_box[:,3]-crop_box[:,1])/2, (crop_box[:,3]+crop_box[:,1])/2]\n  theta = torch.stack(affine_parameter, 1).view(num_pts, 2, 3)\n  feature = feature.expand(num_pts,C, H, W)\n  grid_size = torch.Size([num_pts, 1, 2*patch_size+1, 2*patch_size+1])\n  grid = F.affine_grid(theta, grid_size)\n  sub_feature = F.grid_sample(feature, grid)\n  return sub_feature\n"""
SBR/lib/log_utils/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom .logger import Logger\nfrom .meter import AverageMeter\nfrom .time_utils import time_for_file, time_string, time_string_short, time_print, convert_size2str, convert_secs2time, print_log\n'"
SBR/lib/log_utils/logger.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom pathlib import Path\nimport importlib, warnings\nimport os, sys, time, numpy as np\nimport scipy.misc \nif sys.version_info.major == 2: # Python 2.x\n  from StringIO import StringIO as BIO\nelse:                           # Python 3.x\n  from io import BytesIO as BIO\n\nclass Logger(object):\n  \n  def __init__(self, log_dir, logstr):\n    """"""Create a summary writer logging to log_dir.""""""\n    self.log_dir = Path(log_dir)\n    self.model_dir = Path(log_dir) / \'checkpoint\'\n    self.meta_dir = Path(log_dir) / \'metas\'\n    self.log_dir.mkdir(mode=0o775, parents=True, exist_ok=True)\n    self.model_dir.mkdir(mode=0o775, parents=True, exist_ok=True)\n    self.meta_dir.mkdir(mode=0o775, parents=True, exist_ok=True)\n\n    self.logger_path = self.log_dir / \'{:}.log\'.format(logstr)\n    self.logger_file = open(self.logger_path, \'w\')\n\n\n  def __repr__(self):\n    return (\'{name}(dir={log_dir})\'.format(name=self.__class__.__name__, **self.__dict__))\n\n  def path(self, mode):\n    if mode == \'meta\'   : return self.meta_dir\n    elif mode == \'model\': return self.model_dir\n    elif mode == \'log\'  : return self.log_dir\n    else: raise TypeError(\'Unknow mode = {:}\'.format(mode))\n\n  def last_info(self):\n    return self.log_dir / \'last-info.pth\'\n\n  def extract_log(self):\n    return self.logger_file\n\n  def close(self):\n    self.logger_file.close()\n\n  def log(self, string, save=True):\n    print (string)\n    if save:\n      self.logger_file.write(\'{:}\\n\'.format(string))\n      self.logger_file.flush()\n'"
SBR/lib/log_utils/meter.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport time, sys\nimport numpy as np\n\nclass AverageMeter(object):     \n  """"""Computes and stores the average and current value""""""    \n  def __init__(self):   \n    self.reset()\n  \n  def reset(self):\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0    \n  \n  def update(self, val, n=1): \n    self.val = val    \n    self.sum += val * n     \n    self.count += n\n    self.avg = self.sum / self.count    \n'"
SBR/lib/log_utils/time_utils.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport time, sys\nimport numpy as np\nfrom .logger import Logger\n\ndef time_for_file():\n  ISOTIMEFORMAT=\'%d-%h-at-%H-%M-%S\'\n  return \'{}\'.format(time.strftime( ISOTIMEFORMAT, time.gmtime(time.time()) ))\n\ndef time_string():\n  ISOTIMEFORMAT=\'%Y-%m-%d %X\'\n  string = \'[{}]\'.format(time.strftime( ISOTIMEFORMAT, time.gmtime(time.time()) ))\n  return string\n\ndef time_string_short():\n  ISOTIMEFORMAT=\'%Y%m%d\'\n  string = \'{}\'.format(time.strftime( ISOTIMEFORMAT, time.gmtime(time.time()) ))\n  return string\n\ndef time_print(string, is_print=True):\n  if (is_print):\n    print(\'{} : {}\'.format(time_string(), string))\n\ndef convert_size2str(torch_size):\n  dims = len(torch_size)\n  string = \'[\'\n  for idim in range(dims):\n    string = string + \' {}\'.format(torch_size[idim])\n  return string + \']\'\n  \ndef convert_secs2time(epoch_time, return_str=False):    \n  need_hour = int(epoch_time / 3600)\n  need_mins = int((epoch_time - 3600*need_hour) / 60)  \n  need_secs = int(epoch_time - 3600*need_hour - 60*need_mins)\n  if return_str:\n    str = \'[Time Left: {:02d}:{:02d}:{:02d}]\'.format(need_hour, need_mins, need_secs)\n    return str\n  else:\n    return need_hour, need_mins, need_secs\n\ndef print_log(print_string, log):\n  if isinstance(log, Logger): log.log(\'{:}\'.format(print_string))\n  else:\n    print(""{:}"".format(print_string))\n    if log is not None:\n      log.write(\'{:}\\n\'.format(print_string))\n      log.flush()\n'"
SBR/lib/models/LK.py,2,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport torch, copy\nimport torch.nn as nn\nimport lk\n\nclass LK(nn.Module):\n  def __init__(self, model, lkconfig, points):\n    super(LK, self).__init__()\n    self.detector = model\n    self.downsample = self.detector.downsample\n    self.config = copy.deepcopy(lkconfig)\n    self.points = points\n\n  def forward(self, inputs):\n    assert inputs.dim() == 5, 'This model accepts 5 dimension input tensor: {}'.format(inputs.size())\n    batch_size, sequence, C, H, W = list( inputs.size() )\n    gathered_inputs = inputs.view(batch_size * sequence, C, H, W)\n    heatmaps, batch_locs, batch_scos = self.detector(gathered_inputs)\n    heatmaps = [x.view(batch_size, sequence, self.points, H//self.downsample, W//self.downsample) for x in heatmaps]\n    batch_locs, batch_scos = batch_locs.view(batch_size, sequence, self.points, 2), batch_scos.view(batch_size, sequence, self.points)\n    batch_next, batch_fback, batch_back = [], [], []\n\n    for ibatch in range(batch_size):\n      feature_old = inputs[ibatch]\n      nextPts, fbackPts, backPts = lk.lk_forward_backward_batch(inputs[ibatch], batch_locs[ibatch], self.config.window, self.config.steps)\n\n      batch_next.append(nextPts)\n      batch_fback.append(fbackPts)\n      batch_back.append(backPts)\n    batch_next, batch_fback, batch_back = torch.stack(batch_next), torch.stack(batch_fback), torch.stack(batch_back)\n    return heatmaps, batch_locs, batch_scos, batch_next, batch_fback, batch_back\n"""
SBR/lib/models/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom .basic import obtain_model\nfrom .basic import obtain_LK\nfrom .model_utils import remove_module_dict\n'"
SBR/lib/models/basic.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom .cpm_vgg16 import cpm_vgg16\nfrom .LK import LK\n\ndef obtain_model(configure, points):\n  if configure.arch == 'cpm_vgg16':\n    net = cpm_vgg16(configure, points)\n  else:\n    raise TypeError('Unkonw type : {:}'.format(configure.arch))\n  return net\n\ndef obtain_LK(configure, lkconfig, points):\n  model = obtain_model(configure, points)\n  lk_model = LK(model, lkconfig, points)\n  return lk_model\n"""
SBR/lib/models/basic_batch.py,12,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numbers, math\nimport numpy as np\n\ndef find_tensor_peak_batch(heatmap, radius, downsample, threshold = 0.000001):\n  assert heatmap.dim() == 3, 'The dimension of the heatmap is wrong : {}'.format(heatmap.size())\n  assert radius > 0 and isinstance(radius, numbers.Number), 'The radius is not ok : {}'.format(radius)\n  num_pts, H, W = heatmap.size(0), heatmap.size(1), heatmap.size(2)\n  assert W > 1 and H > 1, 'To avoid the normalization function divide zero'\n  # find the approximate location:\n  score, index = torch.max(heatmap.view(num_pts, -1), 1)\n  index_w = (index % W).float()\n  index_h = (index / W).float()\n  \n  def normalize(x, L):\n    return -1. + 2. * x.data / (L-1)\n  boxes = [index_w - radius, index_h - radius, index_w + radius, index_h + radius]\n  boxes[0] = normalize(boxes[0], W)\n  boxes[1] = normalize(boxes[1], H)\n  boxes[2] = normalize(boxes[2], W)\n  boxes[3] = normalize(boxes[3], H)\n  #affine_parameter = [(boxes[2]-boxes[0])/2, boxes[0]*0, (boxes[2]+boxes[0])/2,\n  #                   boxes[0]*0, (boxes[3]-boxes[1])/2, (boxes[3]+boxes[1])/2]\n  #theta = torch.stack(affine_parameter, 1).view(num_pts, 2, 3)\n\n  affine_parameter = torch.zeros((num_pts, 2, 3))\n  affine_parameter[:,0,0] = (boxes[2]-boxes[0])/2\n  affine_parameter[:,0,2] = (boxes[2]+boxes[0])/2\n  affine_parameter[:,1,1] = (boxes[3]-boxes[1])/2\n  affine_parameter[:,1,2] = (boxes[3]+boxes[1])/2\n  # extract the sub-region heatmap\n  theta = affine_parameter.to(heatmap.device)\n  grid_size = torch.Size([num_pts, 1, radius*2+1, radius*2+1])\n  grid = F.affine_grid(theta, grid_size)\n  sub_feature = F.grid_sample(heatmap.unsqueeze(1), grid).squeeze(1)\n  sub_feature = F.threshold(sub_feature, threshold, np.finfo(float).eps)\n\n  X = torch.arange(-radius, radius+1).to(heatmap).view(1, 1, radius*2+1)\n  Y = torch.arange(-radius, radius+1).to(heatmap).view(1, radius*2+1, 1)\n  \n  sum_region = torch.sum(sub_feature.view(num_pts,-1),1)\n  x = torch.sum((sub_feature*X).view(num_pts,-1),1) / sum_region + index_w\n  y = torch.sum((sub_feature*Y).view(num_pts,-1),1) / sum_region + index_h\n     \n  x = x * downsample + downsample / 2.0 - 0.5\n  y = y * downsample + downsample / 2.0 - 0.5\n  return torch.stack([x, y],1), score\n"""
SBR/lib/models/cpm_vgg16.py,6,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom __future__ import division\nimport time, math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom collections import OrderedDict\nfrom copy import deepcopy\nfrom .model_utils import get_parameters\nfrom .basic_batch import find_tensor_peak_batch\nfrom .initialization import weights_init_cpm\n\nclass VGG16_base(nn.Module):\n  def __init__(self, config, pts_num):\n    super(VGG16_base, self).__init__()\n\n    self.config = deepcopy(config)\n    self.downsample = 8\n    self.pts_num = pts_num\n\n    self.features = nn.Sequential(\n          nn.Conv2d(  3,  64, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d( 64,  64, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.MaxPool2d(kernel_size=2, stride=2),\n          nn.Conv2d( 64, 128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.MaxPool2d(kernel_size=2, stride=2),\n          nn.Conv2d(128, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(256, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(256, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.MaxPool2d(kernel_size=2, stride=2),\n          nn.Conv2d(256, 512, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(512, 512, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(512, 512, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True))\n  \n\n    self.CPM_feature = nn.Sequential(\n          nn.Conv2d(512, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), #CPM_1\n          nn.Conv2d(256, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True)) #CPM_2\n\n    assert self.config.stages >= 1, 'stages of cpm must >= 1 not : {:}'.format(self.config.stages)\n    stage1 = nn.Sequential(\n          nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128, 512, kernel_size=1, padding=0), nn.ReLU(inplace=True),\n          nn.Conv2d(512, pts_num, kernel_size=1, padding=0))\n    stages = [stage1]\n    for i in range(1, self.config.stages):\n      stagex = nn.Sequential(\n          nn.Conv2d(128+pts_num, 128, kernel_size=7, dilation=1, padding=3), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=7, dilation=1, padding=3), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=7, dilation=1, padding=3), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n          nn.Conv2d(128,         128, kernel_size=1, padding=0), nn.ReLU(inplace=True),\n          nn.Conv2d(128,     pts_num, kernel_size=1, padding=0))\n      stages.append( stagex )\n    self.stages = nn.ModuleList(stages)\n  \n  def specify_parameter(self, base_lr, base_weight_decay):\n    params_dict = [ {'params': get_parameters(self.features,   bias=False), 'lr': base_lr  , 'weight_decay': base_weight_decay},\n                    {'params': get_parameters(self.features,   bias=True ), 'lr': base_lr*2, 'weight_decay': 0},\n                    {'params': get_parameters(self.CPM_feature, bias=False), 'lr': base_lr  , 'weight_decay': base_weight_decay},\n                    {'params': get_parameters(self.CPM_feature, bias=True ), 'lr': base_lr*2, 'weight_decay': 0},\n                  ]\n    for stage in self.stages:\n      params_dict.append( {'params': get_parameters(stage, bias=False), 'lr': base_lr*4, 'weight_decay': base_weight_decay} )\n      params_dict.append( {'params': get_parameters(stage, bias=True ), 'lr': base_lr*8, 'weight_decay': 0} )\n    return params_dict\n\n  # return : cpm-stages, locations\n  def forward(self, inputs):\n    assert inputs.dim() == 4, 'This model accepts 4 dimension input tensor: {}'.format(inputs.size())\n    batch_size, feature_dim = inputs.size(0), inputs.size(1)\n    batch_cpms, batch_locs, batch_scos = [], [], []\n\n    feature  = self.features(inputs)\n    xfeature = self.CPM_feature(feature)\n    for i in range(self.config.stages):\n      if i == 0: cpm = self.stages[i]( xfeature )\n      else:      cpm = self.stages[i]( torch.cat([xfeature, batch_cpms[i-1]], 1) )\n      batch_cpms.append( cpm )\n\n    # The location of the current batch\n    for ibatch in range(batch_size):\n      batch_location, batch_score = find_tensor_peak_batch(batch_cpms[-1][ibatch], self.config.argmax, self.downsample)\n      batch_locs.append( batch_location )\n      batch_scos.append( batch_score )\n    batch_locs, batch_scos = torch.stack(batch_locs), torch.stack(batch_scos)\n\n    return batch_cpms, batch_locs, batch_scos\n\n# use vgg16 conv1_1 to conv4_4 as feature extracation        \nmodel_urls = 'https://download.pytorch.org/models/vgg16-397923af.pth'\n\ndef cpm_vgg16(config, pts):\n  \n  print ('Initialize cpm-vgg16 with configure : {}'.format(config))\n  model = VGG16_base(config, pts)\n  model.apply(weights_init_cpm)\n\n  if config.pretrained:\n    print ('vgg16_base use pre-trained model')\n    weights = model_zoo.load_url(model_urls)\n    model.load_state_dict(weights, strict=False)\n  return model\n"""
SBR/lib/models/initialization.py,2,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport torch\nimport torch.nn as nn\nfrom torch.nn import init\n\ndef weights_init_cpm(m):\n  classname = m.__class__.__name__\n  # print(classname)\n  if classname.find('Conv') != -1:\n    m.weight.data.normal_(0, 0.01)\n    if m.bias is not None: m.bias.data.zero_()\n  elif classname.find('BatchNorm2d') != -1:\n    m.weight.data.fill_(1)\n    m.bias.data.zero_()\n"""
SBR/lib/models/model_utils.py,2,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom scipy.ndimage.interpolation import zoom\nfrom collections import OrderedDict\nimport utils\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport copy, numbers, numpy as np\n\ndef get_parameters(model, bias):\n  for m in model.modules():\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n      if bias:\n        yield m.bias\n      else:\n        yield m.weight\n    elif isinstance(m, nn.BatchNorm2d):\n      if bias:\n        yield m.bias\n      else:\n        yield m.weight\n\ndef remove_module_dict(state_dict, is_print=False):\n  new_state_dict = OrderedDict()\n  for k, v in state_dict.items():\n    if k[:7] == 'module.':\n      name = k[7:] # remove `module.`\n    else:\n      name = k\n    new_state_dict[name] = v\n  if is_print: print(new_state_dict.keys())\n  return new_state_dict\n"""
SBR/lib/optimizer/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom .opt_utils import obtain_optimizer\n'"
SBR/lib/optimizer/opt_utils.py,5,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport torch\n\ndef obtain_optimizer(params, config, logger):\n  assert hasattr(config, 'optimizer'), 'Must have the optimizer attribute'\n  optimizer = config.optimizer.lower()\n  if optimizer == 'sgd':\n    opt = torch.optim.SGD(params, lr=config.LR, momentum=config.momentum,\n                          weight_decay=config.Decay, nesterov=config.nesterov)\n  elif optimizer == 'rmsprop':\n    opt = torch.optim.RMSprop(params, lr=config.LR, momentum=config.momentum,\n                          alpha = config.alpha, eps=config.epsilon,\n                          weight_decay = config.weight_decay)\n  elif optimizer == 'adam':\n    opt = torch.optim.Adam(params, lr=config.LR, amsgrad=config.amsgrad)\n  else:\n    raise TypeError('Does not know this optimizer : {:}'.format(config))\n\n  scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=config.schedule, gamma=config.gamma)\n\n  strs = config.criterion.split('-')\n  assert len(strs) == 2, 'illegal criterion : {:}'.format(config.criterion)\n  if strs[0].lower() == 'mse':\n    size_average = strs[1].lower() == 'avg'\n    criterion = torch.nn.MSELoss(size_average)\n    message = 'Optimizer : {:}, MSE Loss with size-average={:}'.format(opt, size_average)\n    if logger is not None: logger.log(message)\n    else                 : print(message)\n  else:\n    raise TypeError('Does not know this optimizer : {:}'.format(config.criterion))\n\n  return opt, scheduler, criterion\n"""
SBR/lib/procedure/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom .starts import prepare_seed\nfrom .basic_train import basic_train\nfrom .saver import save_checkpoint\nfrom .basic_eval import basic_eval_all\n# LK\nfrom .lk_train import lk_train\n'"
SBR/lib/procedure/basic_eval.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport time, os, sys, numpy as np\nimport torch\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom xvision import Eval_Meta\nfrom log_utils import AverageMeter, time_for_file, time_string, convert_secs2time\nfrom .losses import compute_stage_loss, show_stage_loss\n\ndef basic_eval_all(args, loaders, net, criterion, epoch_str, logger, opt_config):\n  args = deepcopy(args)\n  logger.log(\'Basic-Eval-All evaluates {:} dataset\'.format(len(loaders)))\n  nmes = []\n  for i, (loader, is_video) in enumerate(loaders):\n    logger.log(\'==>>{:}, [{:}], evaluate the {:}/{:}-th dataset [{:}] : {:}\'.format(time_string(), epoch_str, i, len(loaders), \'video\' if is_video else \'image\', loader.dataset))\n    with torch.no_grad():\n      eval_loss, eval_meta = basic_eval(args, loader, net, criterion, epoch_str+""::{:}/{:}"".format(i,len(loaders)), logger, opt_config)\n    nme, _, _ = eval_meta.compute_mse(logger)\n    meta_path = logger.path(\'meta\') / \'eval-{:}-{:02d}-{:02d}.pth\'.format(epoch_str, i, len(loaders))\n    eval_meta.save(meta_path)\n    nmes.append(nme*100)\n  return \', \'.join([\'{:.2f}\'.format(x) for x in nmes])\n  \n\ndef basic_eval(args, loader, net, criterion, epoch_str, logger, opt_config):\n  batch_time, data_time, forward_time, eval_time = AverageMeter(), AverageMeter(), AverageMeter(), AverageMeter()\n  visible_points, losses = AverageMeter(), AverageMeter()\n  eval_meta = Eval_Meta()\n  cpu = torch.device(\'cpu\')\n\n  # switch to train mode\n  net.eval()\n  criterion.eval()\n\n  end = time.time()\n  for i, (inputs, target, mask, points, image_index, nopoints, cropped_size) in enumerate(loader):\n    # inputs : Batch, Channel, Height, Width\n\n    target = target.cuda(non_blocking=True)\n\n    image_index = image_index.numpy().squeeze(1).tolist()\n    batch_size, num_pts = inputs.size(0), args.num_pts\n    visible_point_num   = float(np.sum(mask.numpy()[:,:-1,:,:])) / batch_size\n    visible_points.update(visible_point_num, batch_size)\n    nopoints    = nopoints.numpy().squeeze(1).tolist()\n    annotated_num = batch_size - sum(nopoints)\n\n    # measure data loading time\n    mask = mask.cuda(non_blocking=True)\n    data_time.update(time.time() - end)\n\n    # batch_heatmaps is a list for stage-predictions, each element should be [Batch, C, H, W]\n    batch_heatmaps, batch_locs, batch_scos = net(inputs)\n    forward_time.update(time.time() - end)\n\n    if annotated_num > 0:\n      loss, each_stage_loss_value = compute_stage_loss(criterion, target, batch_heatmaps, mask)\n      if opt_config.lossnorm:\n        loss, each_stage_loss_value = loss / annotated_num, [x/annotated_num for x in each_stage_loss_value]\n      each_stage_loss_value = show_stage_loss(each_stage_loss_value)\n      # measure accuracy and record loss\n      losses.update(loss.item(), batch_size)\n    else:\n      loss, each_stage_loss_value = 0, \'no-det-loss\'\n\n    eval_time.update(time.time() - end)\n\n    np_batch_locs, np_batch_scos = batch_locs.to(cpu).numpy(), batch_scos.to(cpu).numpy()\n    cropped_size = cropped_size.numpy()\n    # evaluate the training data\n    for ibatch, (imgidx, nopoint) in enumerate(zip(image_index, nopoints)):\n      #if nopoint == 1: continue\n      locations, scores = np_batch_locs[ibatch,:-1,:], np.expand_dims(np_batch_scos[ibatch,:-1], -1)\n      xpoints = loader.dataset.labels[imgidx].get_points()\n      assert cropped_size[ibatch,0] > 0 and cropped_size[ibatch,1] > 0, \'The ibatch={:}, imgidx={:} is not right.\'.format(ibatch, imgidx, cropped_size[ibatch])\n      scale_h, scale_w = cropped_size[ibatch,0] * 1. / inputs.size(-2) , cropped_size[ibatch,1] * 1. / inputs.size(-1)\n      locations[:, 0], locations[:, 1] = locations[:, 0] * scale_w + cropped_size[ibatch,2], locations[:, 1] * scale_h + cropped_size[ibatch,3]\n      assert xpoints.shape[1] == num_pts and locations.shape[0] == num_pts and scores.shape[0] == num_pts, \'The number of points is {} vs {} vs {} vs {}\'.format(num_pts, xpoints.shape, locations.shape, scores.shape)\n      # recover the original resolution\n      prediction = np.concatenate((locations, scores), axis=1).transpose(1,0)\n      image_path = loader.dataset.datas[imgidx]\n      face_size  = loader.dataset.face_sizes[imgidx]\n      if nopoint == 1:\n        eval_meta.append(prediction, None, image_path, face_size)\n      else:\n        eval_meta.append(prediction, xpoints, image_path, face_size)\n\n    # measure elapsed time\n    batch_time.update(time.time() - end)\n    last_time = convert_secs2time(batch_time.avg * (len(loader)-i-1), True)\n    end = time.time()\n\n    if i % (args.print_freq) == 0 or i+1 == len(loader):\n      logger.log(\' -->>[Eval]: [{:}][{:03d}/{:03d}] \'\n                \'Time {batch_time.val:4.2f} ({batch_time.avg:4.2f}) \'\n                \'Data {data_time.val:4.2f} ({data_time.avg:4.2f}) \'\n                \'Forward {forward_time.val:4.2f} ({forward_time.avg:4.2f}) \'\n                \'Loss {loss.val:7.4f} ({loss.avg:7.4f})  \'.format(\n                    epoch_str, i, len(loader), batch_time=batch_time,\n                    data_time=data_time, forward_time=forward_time, loss=losses)\n                  + last_time + each_stage_loss_value \\\n                  + \' In={:} Tar={:}\'.format(list(inputs.size()), list(target.size())) \\\n                  + \' Vis-PTS : {:2d} ({:.1f})\'.format(int(visible_points.val), visible_points.avg))\n  return losses.avg, eval_meta\n'"
SBR/lib/procedure/basic_train.py,1,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport time, os\nimport numpy as np\nimport torch\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom xvision import Eval_Meta\nfrom log_utils import AverageMeter, time_for_file, convert_secs2time\nfrom .losses import compute_stage_loss, show_stage_loss\n\n# train function (forward, backward, update)\ndef basic_train(args, loader, net, criterion, optimizer, epoch_str, logger, opt_config):\n  args = deepcopy(args)\n  batch_time, data_time, forward_time, eval_time = AverageMeter(), AverageMeter(), AverageMeter(), AverageMeter()\n  visible_points, losses = AverageMeter(), AverageMeter()\n  eval_meta = Eval_Meta()\n  cpu = torch.device('cpu')\n\n  # switch to train mode\n  net.train()\n  criterion.train()\n\n  end = time.time()\n  for i, (inputs, target, mask, points, image_index, nopoints, cropped_size) in enumerate(loader):\n    # inputs : Batch, Channel, Height, Width\n\n    target = target.cuda(non_blocking=True)\n\n    image_index = image_index.numpy().squeeze(1).tolist()\n    batch_size, num_pts = inputs.size(0), args.num_pts\n    visible_point_num   = float(np.sum(mask.numpy()[:,:-1,:,:])) / batch_size\n    visible_points.update(visible_point_num, batch_size)\n    nopoints    = nopoints.numpy().squeeze(1).tolist()\n    annotated_num = batch_size - sum(nopoints)\n\n    # measure data loading time\n    mask = mask.cuda(non_blocking=True)\n    data_time.update(time.time() - end)\n\n    # batch_heatmaps is a list for stage-predictions, each element should be [Batch, C, H, W]\n    batch_heatmaps, batch_locs, batch_scos = net(inputs)\n    forward_time.update(time.time() - end)\n\n    loss, each_stage_loss_value = compute_stage_loss(criterion, target, batch_heatmaps, mask)\n\n    if opt_config.lossnorm:\n      loss, each_stage_loss_value = loss / annotated_num / 2, [x/annotated_num/2 for x in each_stage_loss_value]\n\n    # measure accuracy and record loss\n    losses.update(loss.item(), batch_size)\n\n    # compute gradient and do SGD step\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    eval_time.update(time.time() - end)\n\n    np_batch_locs, np_batch_scos = batch_locs.detach().to(cpu).numpy(), batch_scos.detach().to(cpu).numpy()\n    cropped_size = cropped_size.numpy()\n    # evaluate the training data\n    for ibatch, (imgidx, nopoint) in enumerate(zip(image_index, nopoints)):\n      if nopoint == 1: continue\n      locations, scores = np_batch_locs[ibatch,:-1,:], np.expand_dims(np_batch_scos[ibatch,:-1], -1)\n      xpoints = loader.dataset.labels[imgidx].get_points()\n      assert cropped_size[ibatch,0] > 0 and cropped_size[ibatch,1] > 0, 'The ibatch={:}, imgidx={:} is not right.'.format(ibatch, imgidx, cropped_size[ibatch])\n      scale_h, scale_w = cropped_size[ibatch,0] * 1. / inputs.size(-2) , cropped_size[ibatch,1] * 1. / inputs.size(-1)\n      locations[:, 0], locations[:, 1] = locations[:, 0] * scale_w + cropped_size[ibatch,2], locations[:, 1] * scale_h + cropped_size[ibatch,3]\n      assert xpoints.shape[1] == num_pts and locations.shape[0] == num_pts and scores.shape[0] == num_pts, 'The number of points is {} vs {} vs {} vs {}'.format(num_pts, xpoints.shape, locations.shape, scores.shape)\n      # recover the original resolution\n      prediction = np.concatenate((locations, scores), axis=1).transpose(1,0)\n      image_path = loader.dataset.datas[imgidx]\n      face_size  = loader.dataset.face_sizes[imgidx]\n      eval_meta.append(prediction, xpoints, image_path, face_size)\n\n    # measure elapsed time\n    batch_time.update(time.time() - end)\n    last_time = convert_secs2time(batch_time.avg * (len(loader)-i-1), True)\n    end = time.time()\n\n    if i % args.print_freq == 0 or i+1 == len(loader):\n      logger.log(' -->>[Train]: [{:}][{:03d}/{:03d}] '\n                'Time {batch_time.val:4.2f} ({batch_time.avg:4.2f}) '\n                'Data {data_time.val:4.2f} ({data_time.avg:4.2f}) '\n                'Forward {forward_time.val:4.2f} ({forward_time.avg:4.2f}) '\n                'Loss {loss.val:7.4f} ({loss.avg:7.4f})  '.format(\n                    epoch_str, i, len(loader), batch_time=batch_time,\n                    data_time=data_time, forward_time=forward_time, loss=losses)\n                  + last_time + show_stage_loss(each_stage_loss_value) \\\n                  + ' In={:} Tar={:}'.format(list(inputs.size()), list(target.size())) \\\n                  + ' Vis-PTS : {:2d} ({:.1f})'.format(int(visible_points.val), visible_points.avg))\n  nme, _, _ = eval_meta.compute_mse(logger)\n  return losses.avg, nme\n"""
SBR/lib/procedure/lk_loss.py,9,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport models\nimport torch\nimport numpy as np\nimport math, numbers\n\ndef lk_input_check(batch_locs, batch_scos, batch_next, batch_fback, batch_back):\n  batch, sequence, num_pts, _ = list(batch_locs.size())\n  assert batch_locs.size() == batch_next.size() == batch_fback.size() == batch_back.size(), '{:} vs {:} vs {:} vs {:}'.format(batch_locs.size(), batch_next.size(), batch_fback.size(), batch_back.size())\n  assert _ == 2, '{:}'.format(batch_locs.size())\n  assert batch_scos.size(0) == batch and batch_scos.size(1) == sequence and batch_scos.size(2) == num_pts, '{:} vs {:}'.format(batch_locs.size(), batch_scos.size())\n  return batch, sequence, num_pts\n\ndef p2string(point):\n  if isinstance(point, numbers.Number):\n    return '{:.1f}'.format(point*1.0)\n  elif point.size == 2:\n    return '{:.1f},{:.1f}'.format(point[0], point[1])\n  else:\n    return '{}'.format(point)\n\ndef lk_target_loss(batch_locs, batch_scos, batch_next, batch_fbak, batch_back, lk_config, video_or_not, mask, nopoints):\n  # return the calculate target from the first frame to the whole sequence.\n  batch, sequence, num_pts = lk_input_check(batch_locs, batch_scos, batch_next, batch_fbak, batch_back)\n\n  # remove the background\n  num_pts = num_pts - 1\n  sequence_checks = np.ones((batch, num_pts), dtype='bool')\n\n  # Check the confidence score for each point\n  for ibatch in range(batch):\n    if video_or_not[ibatch] == False:\n      sequence_checks[ibatch, :] = False\n    else:\n      for iseq in range(sequence):\n        for ipts in range(num_pts):\n          score = batch_scos[ibatch, iseq, ipts]\n          if mask[ibatch, ipts] == False and nopoints[ibatch] == 0:\n            sequence_checks[ibatch, ipts] = False\n          if score.item() < lk_config.conf_thresh:\n            sequence_checks[ibatch, ipts] = False\n\n  losses = []\n  for ibatch in range(batch):\n    for ipts in range(num_pts):\n      if not sequence_checks[ibatch, ipts]: continue\n      loss = 0\n      for iseq in range(sequence):\n      \n        targets = batch_locs[ibatch, iseq, ipts]\n        nextPts = batch_next[ibatch, iseq, ipts]\n        fbakPts = batch_fbak[ibatch, iseq, ipts]\n        backPts = batch_back[ibatch, iseq, ipts]\n\n        with torch.no_grad():\n          fbak_distance = torch.dist(nextPts, fbakPts)\n          back_distance = torch.dist(targets, backPts)\n          forw_distance = torch.dist(targets, nextPts)\n\n        #print ('[{:02d},{:02d},{:02d}] : {:.2f}, {:.2f}, {:.2f}'.format(ibatch, ipts, iseq, fbak_distance.item(), back_distance.item(), forw_distance.item()))\n        #loss += back_distance + forw_distance\n\n        if fbak_distance.item() > lk_config.fb_thresh or fbak_distance.item() < lk_config.eps: # forward-backward-check\n          if iseq+1 < sequence: sequence_checks[ibatch, ipts] = False\n        if forw_distance.item() > lk_config.forward_max or forw_distance.item() < lk_config.eps: # to avoid the tracker point is too far\n          if iseq   > 0       : sequence_checks[ibatch, ipts] = False\n        if back_distance.item() > lk_config.forward_max or back_distance.item() < lk_config.eps: # to avoid the tracker point is too far\n          if iseq+1 < sequence: sequence_checks[ibatch, ipts] = False\n\n        if iseq > 0:\n          if lk_config.stable: loss += torch.dist(targets, backPts.detach())\n          else               : loss += torch.dist(targets, backPts)\n        if iseq + 1 < sequence:\n          if lk_config.stable: loss += torch.dist(targets, nextPts.detach())\n          else               : loss += torch.dist(targets, nextPts)\n\n      if sequence_checks[ibatch, ipts]:\n        losses.append(loss)\n\n  avaliable = int(np.sum(sequence_checks))\n  if avaliable == 0: return None, avaliable\n  else             : return torch.mean(torch.stack(losses)), avaliable\n"""
SBR/lib/procedure/lk_train.py,1,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport time, os, numpy as np\nimport torch\nimport numbers, warnings\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom log_utils import AverageMeter, time_for_file, convert_secs2time\nfrom .losses import compute_stage_loss, show_stage_loss\nfrom .lk_loss import lk_target_loss\n\n# train function (forward, backward, update)\ndef lk_train(args, loader, net, criterion, optimizer, epoch_str, logger, opt_config, lk_config, use_lk):\n  args = deepcopy(args)\n  batch_time, data_time, forward_time, eval_time = AverageMeter(), AverageMeter(), AverageMeter(), AverageMeter()\n  visible_points, detlosses, lklosses = AverageMeter(), AverageMeter(), AverageMeter()\n  alk_points, losses = AverageMeter(), AverageMeter()\n  cpu = torch.device('cpu')\n  \n  annotate_index = loader.dataset.center_idx\n\n  # switch to train mode\n  net.train()\n  criterion.train()\n\n  end = time.time()\n  for i, (inputs, target, mask, points, image_index, nopoints, video_or_not, cropped_size) in enumerate(loader):\n    # inputs : Batch, Sequence Channel, Height, Width\n\n    target = target.cuda(non_blocking=True)\n\n    image_index = image_index.numpy().squeeze(1).tolist()\n    batch_size, sequence, num_pts = inputs.size(0), inputs.size(1), args.num_pts\n    mask_np = mask.numpy().squeeze(-1).squeeze(-1)\n    visible_point_num   = float(np.sum(mask.numpy()[:,:-1,:,:])) / batch_size\n    visible_points.update(visible_point_num, batch_size)\n    nopoints    = nopoints.numpy().squeeze(1).tolist()\n    video_or_not= video_or_not.numpy().squeeze(1).tolist()\n    annotated_num = batch_size - sum(nopoints)\n\n    # measure data loading time\n    mask = mask.cuda(non_blocking=True)\n    data_time.update(time.time() - end)\n\n    # batch_heatmaps is a list for stage-predictions, each element should be [Batch, Sequence, PTS, H/Down, W/Down]\n    batch_heatmaps, batch_locs, batch_scos, batch_next, batch_fback, batch_back = net(inputs)\n    annot_heatmaps = [x[:, annotate_index] for x in batch_heatmaps]\n    forward_time.update(time.time() - end)\n\n    if annotated_num > 0:\n      # have the detection loss\n      detloss, each_stage_loss_value = compute_stage_loss(criterion, target, annot_heatmaps, mask)\n      if opt_config.lossnorm:\n        detloss, each_stage_loss_value = detloss / annotated_num / 2, [x/annotated_num/2 for x in each_stage_loss_value]\n      # measure accuracy and record loss\n      detlosses.update(detloss.item(), batch_size)\n      each_stage_loss_value = show_stage_loss(each_stage_loss_value)\n    else:\n      detloss, each_stage_loss_value = 0, 'no-det-loss'\n\n    if use_lk:\n      lkloss, avaliable = lk_target_loss(batch_locs, batch_scos, batch_next, batch_fback, batch_back, lk_config, video_or_not, mask_np, nopoints)\n      if lkloss is not None:\n        lklosses.update(lkloss.item(), avaliable)\n      else: lkloss = 0\n      alk_points.update(float(avaliable)/batch_size, batch_size)\n    else  : lkloss = 0\n     \n    loss = detloss + lkloss * lk_config.weight\n\n    if isinstance(loss, numbers.Number):\n      warnings.warn('The {:}-th iteration has no detection loss and no lk loss'.format(i))\n    else:\n      losses.update(loss.item(), batch_size)\n      # compute gradient and do SGD step\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n\n    eval_time.update(time.time() - end)\n\n    # measure elapsed time\n    batch_time.update(time.time() - end)\n    last_time = convert_secs2time(batch_time.avg * (len(loader)-i-1), True)\n    end = time.time()\n\n    if i % args.print_freq == 0 or i+1 == len(loader):\n      logger.log(' -->>[Train]: [{:}][{:03d}/{:03d}] '\n                'Time {batch_time.val:4.2f} ({batch_time.avg:4.2f}) '\n                'Data {data_time.val:4.2f} ({data_time.avg:4.2f}) '\n                'Forward {forward_time.val:4.2f} ({forward_time.avg:4.2f}) '\n                'Loss {loss.val:7.4f} ({loss.avg:7.4f}) [LK={lk.val:7.4f} ({lk.avg:7.4f})] '.format(\n                    epoch_str, i, len(loader), batch_time=batch_time,\n                    data_time=data_time, forward_time=forward_time, loss=losses, lk=lklosses)\n                  + each_stage_loss_value + ' ' + last_time \\\n                  + ' Vis-PTS : {:2d} ({:.1f})'.format(int(visible_points.val), visible_points.avg) \\\n                  + ' Ava-PTS : {:.1f} ({:.1f})'.format(alk_points.val, alk_points.avg))\n\n  return losses.avg\n"""
SBR/lib/procedure/losses.py,3,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport numpy as np\nimport numbers, torch\nimport torch.nn.functional as F\n\ndef compute_stage_loss(criterion, targets, outputs, masks):\n  assert isinstance(outputs, list), 'The ouputs type is wrong : {:}'.format(type(outputs))\n  total_loss = 0\n  each_stage_loss = []\n  \n  for output in outputs:\n    stage_loss = 0\n    output = torch.masked_select(output , masks)\n    target = torch.masked_select(targets, masks)\n\n    stage_loss = criterion(output, target)\n    total_loss = total_loss + stage_loss\n    each_stage_loss.append(stage_loss.item())\n  return total_loss, each_stage_loss\n\n\ndef show_stage_loss(each_stage_loss):\n  if each_stage_loss is None:            return 'None'\n  elif isinstance(each_stage_loss, str): return each_stage_loss\n  answer = ''\n  for index, loss in enumerate(each_stage_loss):\n    answer = answer + ' : L{:1d}={:7.4f}'.format(index+1, loss)\n  return answer\n\n\ndef sum_stage_loss(losses):\n  total_loss = None\n  each_stage_loss = []\n  for loss in losses:\n    if total_loss is None:\n      total_loss = loss\n    else:\n      total_loss = total_loss + loss\n    each_stage_loss.append(loss.data[0])\n  return total_loss, each_stage_loss\n"""
SBR/lib/procedure/saver.py,1,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport torch\n\ndef save_checkpoint(state, filename, logger):\n  torch.save(state, filename)\n  logger.log('save checkpoint into {}'.format(filename))\n  return filename\n"""
SBR/lib/procedure/starts.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport os, sys, time\nimport numpy as np\nimport torch\nimport random\n\ndef prepare_seed(rand_seed):\n  np.random.seed(rand_seed)\n  random.seed(rand_seed)\n  torch.manual_seed(rand_seed)\n  torch.cuda.manual_seed_all(rand_seed)\n'"
SBR/lib/pts_utils/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom .generation import generate_label_map\n'"
SBR/lib/pts_utils/generation.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom scipy.ndimage.interpolation import zoom\nimport numbers, math\nimport numpy as np\n\n## pts = 3 * N numpy array; points location is based on the image with size (height*downsample, width*downsample)\n\ndef generate_label_map(pts, height, width, sigma, downsample, nopoints, ctype):\n  #if isinstance(pts, numbers.Number):\n  # this image does not provide the annotation, pts is a int number representing the number of points\n  #return np.zeros((height,width,pts+1), dtype='float32'), np.ones((1,1,1+pts), dtype='float32')\n  # nopoints == True means this image does not provide the annotation, pts is a int number representing the number of points\n  \n  assert isinstance(pts, np.ndarray) and len(pts.shape) == 2 and pts.shape[0] == 3, 'The shape of points : {}'.format(pts.shape)\n  if isinstance(sigma, numbers.Number):\n    sigma = np.zeros((pts.shape[1])) + sigma\n  assert isinstance(sigma, np.ndarray) and len(sigma.shape) == 1 and sigma.shape[0] == pts.shape[1], 'The shape of sigma : {}'.format(sigma.shape)\n\n  offset = downsample / 2.0 - 0.5\n  num_points, threshold = pts.shape[1], 0.01\n\n  if nopoints == False: visiable = pts[2, :].astype('bool')\n  else                : visiable = (pts[2, :]*0).astype('bool')\n  #assert visiable.shape[0] == num_points\n\n  transformed_label = np.fromfunction( lambda y, x, pid : ((offset + x*downsample - pts[0,pid])**2 \\\n                                                        + (offset + y*downsample - pts[1,pid])**2) \\\n                                                          / -2.0 / sigma[pid] / sigma[pid],\n                                                          (height, width, num_points), dtype=int)\n\n  mask_heatmap      = np.ones((1, 1, num_points+1), dtype='float32')\n  mask_heatmap[0, 0, :num_points] = visiable\n  mask_heatmap[0, 0, num_points]  = (nopoints==False)\n  \n  if ctype == 'laplacian':\n    transformed_label = (1+transformed_label) * np.exp(transformed_label)\n  elif ctype == 'gaussian':\n    transformed_label = np.exp(transformed_label)\n  else:\n    raise TypeError('Does not know this type [{:}] for label generation'.format(ctype))\n  transformed_label[ transformed_label < threshold ] = 0\n  transformed_label[ transformed_label >         1 ] = 1\n  transformed_label = transformed_label * mask_heatmap[:, :, :num_points]\n\n  background_label  = 1 - np.amax(transformed_label, axis=2)\n  background_label[ background_label < 0 ] = 0\n  heatmap           = np.concatenate((transformed_label, np.expand_dims(background_label, axis=2)), axis=2).astype('float32')\n  \n  return heatmap*mask_heatmap, mask_heatmap\n"""
SBR/lib/utils/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom .file_utils     import load_list_from_folders, load_txt_file\nfrom .flop_benchmark import get_model_infos\n'"
SBR/lib/utils/file_utils.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport os, sys, glob, numbers\nfrom os import path as osp\n\ndef mkdir_if_missing(path):\n  if not osp.isdir(path):\n    os.makedirs(path)\n\ndef is_path_exists(pathname):                                                                                                                                                                          \n  try:\n    return isinstance(pathname, str) and pathname and os.path.exists(pathname) \n  except OSError:\n    return False\n\ndef fileparts(pathname):\n  '''\n  this function return a tuple, which contains (directory, filename, extension)\n  if the file has multiple extension, only last one will be displayed\n  '''\n  pathname = osp.normpath(pathname)\n  if len(pathname) == 0:\n    return ('', '', '')\n  if pathname[-1] == '/':\n    if len(pathname) > 1:\n      return (pathname[:-1], '', '')  # ignore the final '/'\n    else:\n      return (pathname, '', '') # ignore the final '/'\n  directory = osp.dirname(osp.abspath(pathname))\n  filename  = osp.splitext(osp.basename(pathname))[0]\n  ext       = osp.splitext(pathname)[1]\n  return (directory, filename, ext)\n\ndef load_txt_file(file_path):\n  '''\n  load data or string from text file.\n  '''\n  with open(file_path, 'r') as cfile:\n    content = cfile.readlines()\n  cfile.close()\n  content = [x.strip() for x in content]\n  num_lines = len(content)\n  return content, num_lines\n\ndef load_list_from_folder(folder_path, ext_filter=None, depth=1):\n  '''\n  load a list of files or folders from a system path\n\n  parameter:\n    folder_path: root to search \n    ext_filter: a string to represent the extension of files interested\n    depth: maximum depth of folder to search, when it's None, all levels of folders will be searched\n  '''\n  folder_path = osp.normpath(folder_path)\n  assert isinstance(depth, int) , 'input depth is not correct {}'.format(depth)\n  assert ext_filter is None or (isinstance(ext_filter, list) and all(isinstance(ext_tmp, str) for ext_tmp in ext_filter)) or isinstance(ext_filter, str), 'extension filter is not correct'\n  if isinstance(ext_filter, str):    # convert to a list\n    ext_filter = [ext_filter]\n\n  fulllist = list()\n  wildcard_prefix = '*'\n  for index in range(depth):\n    if ext_filter is not None:\n      for ext_tmp in ext_filter:\n        curpath = osp.join(folder_path, wildcard_prefix + '.' + ext_tmp)\n        fulllist += glob.glob(curpath)\n    else:\n      curpath = osp.join(folder_path, wildcard_prefix)\n      fulllist += glob.glob(curpath)\n    wildcard_prefix = osp.join(wildcard_prefix, '*')\n\n  fulllist = [osp.normpath(path_tmp) for path_tmp in fulllist]\n  num_elem = len(fulllist)\n\n  return fulllist, num_elem\n\ndef load_list_from_folders(folder_path_list, ext_filter=None, depth=1):\n  '''\n  load a list of files or folders from a list of system path\n  '''\n  assert isinstance(folder_path_list, list) or isinstance(folder_path_list, str), 'input path list is not correct'\n  if isinstance(folder_path_list, str):\n    folder_path_list = [folder_path_list]\n\n  fulllist = list()\n  num_elem = 0\n  for folder_path_tmp in folder_path_list:\n    fulllist_tmp, num_elem_tmp = load_list_from_folder(folder_path_tmp, ext_filter=ext_filter, depth=depth)\n    fulllist += fulllist_tmp\n    num_elem += num_elem_tmp\n\n  return fulllist, num_elem\n"""
SBR/lib/utils/flop_benchmark.py,15,"b'##################################################\n# Copyright (c) Xuanyi Dong [GitHub D-X-Y], 2019 #\n##################################################\n# modified from https://github.com/warmspringwinds/pytorch-segmentation-detection/blob/master/pytorch_segmentation_detection/utils/flops_benchmark.py\nimport copy, torch\nimport torch.nn as nn\nimport numpy as np\n\n\ndef count_parameters_in_MB(model):\n  if isinstance(model, nn.Module):\n    return np.sum(np.prod(v.size()) for v in model.parameters())/1e6\n  else:\n    return np.sum(np.prod(v.size()) for v in model)/1e6\n\n\ndef get_model_infos(model, shape):\n  #model = copy.deepcopy( model )\n\n  model = add_flops_counting_methods(model)\n  #model = model.cuda()\n  model.eval()\n\n  #cache_inputs = torch.zeros(*shape).cuda()\n  cache_inputs = torch.zeros(*shape)\n  if next(model.parameters()).is_cuda: cache_inputs = cache_inputs.cuda()\n  #print_log(\'In the calculating function : cache input size : {:}\'.format(cache_inputs.size()), log)\n  with torch.no_grad():\n    _____ = model(cache_inputs)\n  FLOPs = compute_average_flops_cost( model ) / 1e6\n  Param = count_parameters_in_MB(model)\n\n  if hasattr(model, \'auxiliary_param\'):\n    aux_params = count_parameters_in_MB(model.auxiliary_param()) \n    print (\'The auxiliary params of this model is : {:}\'.format(aux_params))\n    print (\'We remove the auxiliary params from the total params ({:}) when counting\'.format(Param))\n    Param = Param - aux_params\n  \n  #print_log(\'FLOPs : {:} MB\'.format(FLOPs), log)\n  torch.cuda.empty_cache()\n  model.apply( remove_hook_function )\n  return FLOPs, Param\n\n\n# ---- Public functions\ndef add_flops_counting_methods( model ):\n  model.__batch_counter__ = 0\n  add_batch_counter_hook_function( model )\n  model.apply( add_flops_counter_variable_or_reset )\n  model.apply( add_flops_counter_hook_function )\n  return model\n\n\n\ndef compute_average_flops_cost(model):\n  """"""\n  A method that will be available after add_flops_counting_methods() is called on a desired net object.\n  Returns current mean flops consumption per image.\n  """"""\n  batches_count = model.__batch_counter__\n  flops_sum = 0\n  #or isinstance(module, torch.nn.AvgPool2d) or isinstance(module, torch.nn.MaxPool2d) \\\n  for module in model.modules():\n    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear) \\\n      or isinstance(module, torch.nn.Conv1d) \\\n      or hasattr(module, \'calculate_flop_self\'):\n      flops_sum += module.__flops__\n  return flops_sum / batches_count\n\n\n# ---- Internal functions\ndef pool_flops_counter_hook(pool_module, inputs, output):\n  batch_size = inputs[0].size(0)\n  kernel_size = pool_module.kernel_size\n  out_C, output_height, output_width = output.shape[1:]\n  assert out_C == inputs[0].size(1), \'{:} vs. {:}\'.format(out_C, inputs[0].size())\n\n  overall_flops = batch_size * out_C * output_height * output_width * kernel_size * kernel_size\n  pool_module.__flops__ += overall_flops\n\n\ndef self_calculate_flops_counter_hook(self_module, inputs, output):\n  overall_flops = self_module.calculate_flop_self(inputs[0].shape, output.shape)\n  self_module.__flops__ += overall_flops\n\n\ndef fc_flops_counter_hook(fc_module, inputs, output):\n  batch_size = inputs[0].size(0)\n  xin, xout = fc_module.in_features, fc_module.out_features\n  assert xin == inputs[0].size(1) and xout == output.size(1), \'IO=({:}, {:})\'.format(xin, xout)\n  overall_flops = batch_size * xin * xout\n  if fc_module.bias is not None:\n    overall_flops += batch_size * xout\n  fc_module.__flops__ += overall_flops\n\n\ndef conv1d_flops_counter_hook(conv_module, inputs, outputs):\n  batch_size   = inputs[0].size(0)\n  outL         = outputs.shape[-1]\n  [kernel]     = conv_module.kernel_size\n  in_channels  = conv_module.in_channels\n  out_channels = conv_module.out_channels\n  groups       = conv_module.groups\n  conv_per_position_flops = kernel * in_channels * out_channels / groups\n  \n  active_elements_count = batch_size * outL \n  overall_flops = conv_per_position_flops * active_elements_count\n\n  if conv_module.bias is not None:\n    overall_flops += out_channels * active_elements_count\n  conv_module.__flops__ += overall_flops\n\n\ndef conv2d_flops_counter_hook(conv_module, inputs, output):\n  batch_size = inputs[0].size(0)\n  output_height, output_width = output.shape[2:]\n  \n  kernel_height, kernel_width = conv_module.kernel_size\n  in_channels  = conv_module.in_channels\n  out_channels = conv_module.out_channels\n  groups       = conv_module.groups\n  conv_per_position_flops = kernel_height * kernel_width * in_channels * out_channels / groups\n  \n  active_elements_count = batch_size * output_height * output_width\n  overall_flops = conv_per_position_flops * active_elements_count\n    \n  if conv_module.bias is not None:\n    overall_flops += out_channels * active_elements_count\n  conv_module.__flops__ += overall_flops\n\n  \ndef batch_counter_hook(module, inputs, output):\n  # Can have multiple inputs, getting the first one\n  inputs = inputs[0]\n  batch_size = inputs.shape[0]\n  module.__batch_counter__ += batch_size\n\n\ndef add_batch_counter_hook_function(module):\n  if not hasattr(module, \'__batch_counter_handle__\'):\n    handle = module.register_forward_hook(batch_counter_hook)\n    module.__batch_counter_handle__ = handle\n\n  \ndef add_flops_counter_variable_or_reset(module):\n  if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear) \\\n    or isinstance(module, torch.nn.Conv1d) \\\n    or isinstance(module, torch.nn.AvgPool2d) or isinstance(module, torch.nn.MaxPool2d) \\\n    or hasattr(module, \'calculate_flop_self\'):\n    module.__flops__ = 0\n\n\ndef add_flops_counter_hook_function(module):\n  if isinstance(module, torch.nn.Conv2d):\n    if not hasattr(module, \'__flops_handle__\'):\n      handle = module.register_forward_hook(conv2d_flops_counter_hook)\n      module.__flops_handle__ = handle\n  elif isinstance(module, torch.nn.Conv1d):\n    if not hasattr(module, \'__flops_handle__\'):\n      handle = module.register_forward_hook(conv1d_flops_counter_hook)\n      module.__flops_handle__ = handle\n  elif isinstance(module, torch.nn.Linear):\n    if not hasattr(module, \'__flops_handle__\'):\n      handle = module.register_forward_hook(fc_flops_counter_hook)\n      module.__flops_handle__ = handle\n  elif isinstance(module, torch.nn.AvgPool2d) or isinstance(module, torch.nn.MaxPool2d):\n    if not hasattr(module, \'__flops_handle__\'):\n      handle = module.register_forward_hook(pool_flops_counter_hook)\n      module.__flops_handle__ = handle\n  elif hasattr(module, \'calculate_flop_self\'): # self-defined module\n    if not hasattr(module, \'__flops_handle__\'):\n      handle = module.register_forward_hook(self_calculate_flops_counter_hook)\n      module.__flops_handle__ = handle\n\n\ndef remove_hook_function(module):\n  hookers = [\'__batch_counter_handle__\', \'__flops_handle__\']\n  for hooker in hookers:\n    if hasattr(module, hooker):\n      handle = getattr(module, hooker)\n      handle.remove()\n  keys = [\'__flops__\', \'__batch_counter__\', \'__flops__\'] + hookers\n  for ckey in keys:\n    if hasattr(module, ckey): delattr(module, ckey)\n'"
SBR/lib/xvision/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom .evaluation_util import Eval_Meta\nfrom .visualization import draw_image_by_points\n'"
SBR/lib/xvision/common_eval.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport numpy as np\nimport pdb, os, time\nfrom log_utils import print_log\nfrom datasets.dataset_utils import convert68to49, convert68to51\nfrom sklearn.metrics import auc\n\ndef evaluate_normalized_mean_error(predictions, groundtruth, log, extra_faces):\n  ## compute total average normlized mean error\n  assert len(predictions) == len(groundtruth), 'The lengths of predictions and ground-truth are not consistent : {} vs {}'.format( len(predictions), len(groundtruth) )\n  assert len(predictions) > 0, 'The length of predictions must be greater than 0 vs {}'.format( len(predictions) )\n  if extra_faces is not None: assert len(extra_faces) == len(predictions), 'The length of extra_faces is not right {} vs {}'.format( len(extra_faces), len(predictions) )\n  num_images = len(predictions)\n  for i in range(num_images):\n    c, g = predictions[i], groundtruth[i]\n    assert isinstance(c, np.ndarray) and isinstance(g, np.ndarray), 'The type of predictions is not right : [{:}] :: {} vs {} '.format(i, type(c), type(g))\n\n  num_points = predictions[0].shape[1]\n  error_per_image = np.zeros((num_images,1))\n  for i in range(num_images):\n    detected_points = predictions[i]\n    ground_truth_points = groundtruth[i]\n    if num_points == 68:\n      interocular_distance = np.linalg.norm(ground_truth_points[:2, 36] - ground_truth_points[:2, 45])\n      assert bool(ground_truth_points[2,36]) and bool(ground_truth_points[2,45])\n    elif num_points == 51 or num_points == 49:\n      interocular_distance = np.linalg.norm(ground_truth_points[:2, 19] - ground_truth_points[:2, 28])\n      assert bool(ground_truth_points[2,19]) and bool(ground_truth_points[2,28])\n    elif num_points == 19:\n      assert extra_faces is not None and extra_faces[i] is not None\n      interocular_distance = extra_faces[i]\n    else:\n      raise Exception('----> Unknown number of points : {}'.format(num_points))\n    dis_sum, pts_sum = 0, 0\n    for j in range(num_points):\n      if bool(ground_truth_points[2, j]):\n        dis_sum = dis_sum + np.linalg.norm(detected_points[:2, j] - ground_truth_points[:2, j])\n        pts_sum = pts_sum + 1\n    error_per_image[i] = dis_sum / (pts_sum*interocular_distance)\n\n  normalise_mean_error = error_per_image.mean()\n  # calculate the auc for 0.07\n  max_threshold = 0.07\n  threshold = np.linspace(0, max_threshold, num=2000)\n  accuracys = np.zeros(threshold.shape)\n  for i in range(threshold.size):\n    accuracys[i] = np.sum(error_per_image < threshold[i]) * 1.0 / error_per_image.size\n  area_under_curve07 = auc(threshold, accuracys) / max_threshold\n  # calculate the auc for 0.08\n  max_threshold = 0.08\n  threshold = np.linspace(0, max_threshold, num=2000)\n  accuracys = np.zeros(threshold.shape)\n  for i in range(threshold.size):\n    accuracys[i] = np.sum(error_per_image < threshold[i]) * 1.0 / error_per_image.size\n  area_under_curve08 = auc(threshold, accuracys) / max_threshold\n  \n  accuracy_under_007 = np.sum(error_per_image<0.07) * 100. / error_per_image.size\n  accuracy_under_008 = np.sum(error_per_image<0.08) * 100. / error_per_image.size\n\n  print_log('Compute NME and AUC for {:} images with {:} points :: [(NME): mean={:.3f}, std={:.3f}], auc@0.07={:.3f}, auc@0.08-{:.3f}, acc@0.07={:.3f}, acc@0.08={:.3f}'.format(num_images, num_points, normalise_mean_error*100, error_per_image.std()*100, area_under_curve07*100, area_under_curve08*100, accuracy_under_007, accuracy_under_008), log)\n\n  for_pck_curve = []\n  for x in range(0, 3501, 1):\n    error_bar = x * 0.0001\n    accuracy = np.sum(error_per_image < error_bar) * 1.0 / error_per_image.size\n    for_pck_curve.append((error_bar, accuracy))\n  \n  return normalise_mean_error, accuracy_under_008, for_pck_curve\n"""
SBR/lib/xvision/evaluation_util.py,2,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nimport os, time\nimport numpy as np\nimport torch\nimport json\nfrom log_utils import print_log\nfrom collections import OrderedDict\nfrom scipy import interpolate\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom .common_eval import evaluate_normalized_mean_error\n\nclass Eval_Meta():\n\n  def __init__(self):\n    self.reset()\n\n  def __repr__(self):\n    return ('{name}'.format(name=self.__class__.__name__)+'(number of data = {:})'.format(len(self)))\n\n  def reset(self):\n    self.predictions = []\n    self.groundtruth = []\n    self.image_lists = []\n    self.face_sizes  = []\n\n  def __len__(self):\n    return len(self.image_lists)\n\n  def append(self, _pred, _ground, image_path, face_size):\n    assert _pred.shape[0] == 3 and len(_pred.shape) == 2, 'Prediction\\'s shape is {:} vs [should be (3,pts) or (2,pts)]'.format(_pred.shape)\n    if _ground is not None:\n      assert _pred.shape == _ground.shape, 'shapes must be the same : {} vs {}'.format(_pred.shape, _ground.shape)\n    if (not self.predictions) == False:\n      assert _pred.shape == self.predictions[-1].shape, 'shapes must be the same : {} vs {}'.format(_pred.shape, self.predictions[-1].shape)\n    self.predictions.append(_pred)\n    self.groundtruth.append(_ground)\n    self.image_lists.append(image_path)\n    self.face_sizes.append(face_size)\n\n  def save(self, filename):\n    meta = {'predictions': self.predictions, \n            'groundtruth': self.groundtruth,\n            'image_lists': self.image_lists,\n            'face_sizes' : self.face_sizes}\n    torch.save(meta, filename)\n    print ('save eval-meta into {}'.format(filename))\n\n  def load(self, filename):\n    assert os.path.isfile(filename), '{} is not a file'.format(filename)\n    checkpoint       = torch.load(filename, map_location='cpu')\n    self.predictions = checkpoint['predictions']\n    self.groundtruth = checkpoint['groundtruth']\n    self.image_lists = checkpoint['image_lists']\n    self.face_sizes  = checkpoint['face_sizes']\n\n  def compute_mse(self, log):\n    predictions, groundtruth, face_sizes, num = [], [], [], 0\n    for x, gt, face in zip(self.predictions, self.groundtruth, self.face_sizes):\n      if gt is None: continue\n      predictions.append(x)\n      groundtruth.append(gt)\n      face_sizes.append(face)\n      num += 1\n    print_log('Filter the unlabeled data from {:} into {:} data'.format(len(self), num), log)\n    if num == 0:\n      nme, auc, pck_curves = -1, None, None\n    else:\n      nme, auc, pck_curves = evaluate_normalized_mean_error(self.predictions, self.groundtruth, log, self.face_sizes)\n    return nme, auc, pck_curves\n"""
SBR/lib/xvision/transforms.py,10,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom __future__ import division\nimport torch\nimport sys, math, random, PIL\nfrom PIL import Image, ImageOps\nimport numpy as np\nimport numbers\nimport types\nimport collections\n\nif sys.version_info.major == 2:\n  import cPickle as pickle\nelse:\n  import pickle\n\nclass Compose(object):\n  def __init__(self, transforms):\n    self.transforms = transforms\n\n  def __call__(self, img, points):\n    for t in self.transforms:\n      img, points = t(img, points)\n    return img, points\n\nclass TrainScale2WH(object):\n  """"""Rescale the input PIL.Image to the given size.\n  Args:\n    size (sequence or int): Desired output size. If size is a sequence like\n      (w, h), output size will be matched to this. If size is an int,\n      smaller edge of the image will be matched to this number.\n      i.e, if height > width, then image will be rescaled to\n      (size * height / width, size)\n    interpolation (int, optional): Desired interpolation. Default is\n      ``PIL.Image.BILINEAR``\n  """"""\n\n  def __init__(self, target_size, interpolation=Image.BILINEAR):\n    assert isinstance(target_size, tuple) or isinstance(target_size, list), \'The type of target_size is not right : {}\'.format(target_size)\n    assert len(target_size) == 2, \'The length of target_size is not right : {}\'.format(target_size)\n    assert isinstance(target_size[0], int) and isinstance(target_size[1], int), \'The type of target_size is not right : {}\'.format(target_size)\n    self.target_size   = target_size\n    self.interpolation = interpolation\n\n  def __call__(self, imgs, point_meta):\n    """"""\n    Args:\n      img (PIL.Image): Image to be scaled.\n      points 3 * N numpy.ndarray [x, y, visiable]\n    Returns:\n      PIL.Image: Rescaled image.\n    """"""\n    point_meta = point_meta.copy()\n\n    if isinstance(imgs, list): is_list = True\n    else:                      is_list, imgs = False, [imgs]\n    \n    w, h = imgs[0].size\n    ow, oh = self.target_size[0], self.target_size[1]\n    point_meta.apply_scale( [ow*1./w, oh*1./h] )\n\n    imgs = [ img.resize((ow, oh), self.interpolation) for img in imgs ]\n    if is_list == False: imgs = imgs[0]\n\n    return imgs, point_meta\n\n\n\nclass ToPILImage(object):\n  """"""Convert a tensor to PIL Image.\n  Converts a torch.*Tensor of shape C x H x W or a numpy ndarray of shape\n  H x W x C to a PIL.Image while preserving the value range.\n  """"""\n\n  def __call__(self, pic):\n    """"""\n    Args:\n      pic (Tensor or numpy.ndarray): Image to be converted to PIL.Image.\n    Returns:\n      PIL.Image: Image converted to PIL.Image.\n    """"""\n    npimg = pic\n    mode = None\n    if isinstance(pic, torch.FloatTensor):\n      pic = pic.mul(255).byte()\n    if torch.is_tensor(pic):\n      npimg = np.transpose(pic.numpy(), (1, 2, 0))\n    assert isinstance(npimg, np.ndarray), \'pic should be Tensor or ndarray\'\n    if npimg.shape[2] == 1:\n      npimg = npimg[:, :, 0]\n\n      if npimg.dtype == np.uint8:\n        mode = \'L\'\n      if npimg.dtype == np.int16:\n        mode = \'I;16\'\n      if npimg.dtype == np.int32:\n        mode = \'I\'\n      elif npimg.dtype == np.float32:\n        mode = \'F\'\n    else:\n      if npimg.dtype == np.uint8:\n        mode = \'RGB\'\n    assert mode is not None, \'{} is not supported\'.format(npimg.dtype)\n    return Image.fromarray(npimg, mode=mode)\n\n\n\nclass ToTensor(object):\n  """"""Convert a ``PIL.Image`` or ``numpy.ndarray`` to tensor.\n  Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n  [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n  """"""\n\n  def __call__(self, pics, points):\n    """"""\n    Args:\n      pic (PIL.Image or numpy.ndarray): Image to be converted to tensor.\n      points 3 * N numpy.ndarray [x, y, visiable] or Point_Meta\n    Returns:\n      Tensor: Converted image.\n    """"""\n    ## add to support list\n    if isinstance(pics, list): is_list = True\n    else:                      is_list, pics = False, [pics]\n\n    returned = []\n    for pic in pics:\n      if isinstance(pic, np.ndarray):\n        # handle numpy array\n        img = torch.from_numpy(pic.transpose((2, 0, 1)))\n        # backward compatibility\n        returned.append( img.float().div(255) )\n        continue\n\n      # handle PIL Image\n      if pic.mode == \'I\':\n        img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n      elif pic.mode == \'I;16\':\n        img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n      else:\n        img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n      # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n      if pic.mode == \'YCbCr\':\n        nchannel = 3\n      elif pic.mode == \'I;16\':\n        nchannel = 1\n      else:\n        nchannel = len(pic.mode)\n      img = img.view(pic.size[1], pic.size[0], nchannel)\n      # put it from HWC to CHW format\n      # yikes, this transpose takes 80% of the loading time/CPU\n      img = img.transpose(0, 1).transpose(0, 2).contiguous()\n      if isinstance(img, torch.ByteTensor):\n        img = img.float().div(255)\n      returned.append(img)\n\n    if is_list == False:\n      assert len(returned) == 1, \'For non-list data, length of answer must be one not {}\'.format(len(returned))\n      returned = returned[0]\n\n    return returned, points\n\n\nclass Normalize(object):\n  """"""Normalize an tensor image with mean and standard deviation.\n  Given mean: (R, G, B) and std: (R, G, B),\n  will normalize each channel of the torch.*Tensor, i.e.\n  channel = (channel - mean) / std\n  Args:\n    mean (sequence): Sequence of means for R, G, B channels respecitvely.\n    std (sequence): Sequence of standard deviations for R, G, B channels\n      respecitvely.\n  """"""\n\n  def __init__(self, mean, std):\n    self.mean = mean\n    self.std = std\n\n  def __call__(self, tensors, points):\n    """"""\n    Args:\n      tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n    Returns:\n      Tensor: Normalized image.\n    """"""\n    # TODO: make efficient\n    if isinstance(tensors, list): is_list = True\n    else:                         is_list, tensors = False, [tensors]\n\n    for tensor in tensors:\n      for t, m, s in zip(tensor, self.mean, self.std):\n        t.sub_(m).div_(s)\n    \n    if is_list == False: tensors = tensors[0]\n\n    return tensors, points\n\n\nclass PreCrop(object):\n  """"""Crops the given PIL.Image at the center.\n\n  Args:\n    size (sequence or int): Desired output size of the crop. If size is an\n      int instead of sequence like (w, h), a square crop (size, size) is\n      made.\n  """"""\n\n  def __init__(self, expand_ratio):\n    assert expand_ratio is None or isinstance(expand_ratio, numbers.Number), \'The expand_ratio should not be {}\'.format(expand_ratio)\n    if expand_ratio is None:\n      self.expand_ratio = 0\n    else:\n      self.expand_ratio = expand_ratio\n    assert self.expand_ratio >= 0, \'The expand_ratio should not be {}\'.format(expand_ratio)\n\n  def __call__(self, imgs, point_meta):\n    ## AugCrop has something wrong... For unsupervised data\n\n    if isinstance(imgs, list): is_list = True\n    else:                      is_list, imgs = False, [imgs]\n\n    w, h = imgs[0].size\n    box = point_meta.get_box().tolist()\n    face_ex_w, face_ex_h = (box[2] - box[0]) * self.expand_ratio, (box[3] - box[1]) * self.expand_ratio\n    x1, y1 = int(max(math.floor(box[0]-face_ex_w), 0)), int(max(math.floor(box[1]-face_ex_h), 0))\n    x2, y2 = int(min(math.ceil(box[2]+face_ex_w), w)), int(min(math.ceil(box[3]+face_ex_h), h))\n    \n    imgs = [ img.crop((x1, y1, x2, y2)) for img in imgs ]\n    point_meta.set_precrop_wh( imgs[0].size[0], imgs[0].size[1], x1, y1, x2, y2)\n    point_meta.apply_offset(-x1, -y1)\n    point_meta.apply_bound(imgs[0].size[0], imgs[0].size[1])\n\n    if is_list == False: imgs = imgs[0]\n    return imgs, point_meta\n\n\nclass AugScale(object):\n  """"""Rescale the input PIL.Image to the given size.\n\n  Args:\n    size (sequence or int): Desired output size. If size is a sequence like\n      (w, h), output size will be matched to this. If size is an int,\n      smaller edge of the image will be matched to this number.\n      i.e, if height > width, then image will be rescaled to\n      (size * height / width, size)\n    interpolation (int, optional): Desired interpolation. Default is\n      ``PIL.Image.BILINEAR``\n  """"""\n\n  def __init__(self, scale_prob, scale_min, scale_max, interpolation=Image.BILINEAR):\n    assert isinstance(scale_prob, numbers.Number) and scale_prob >= 0, \'scale_prob : {:}\'.format(scale_prob)\n    assert isinstance(scale_min,  numbers.Number) and isinstance(scale_max, numbers.Number), \'scales : {:}, {:}\'.format(scale_min, scale_max)\n    self.scale_prob = scale_prob\n    self.scale_min  = scale_min\n    self.scale_max  = scale_max\n    self.interpolation = interpolation\n\n  def __call__(self, imgs, point_meta):\n    """"""\n    Args:\n      img (PIL.Image): Image to be scaled.\n      points 3 * N numpy.ndarray [x, y, visiable]\n    Returns:\n      PIL.Image: Rescaled image.\n    """"""\n    point_meta = point_meta.copy()\n\n    dice = random.random()\n    if dice > self.scale_prob:\n      return imgs, point_meta\n\n    if isinstance(imgs, list): is_list = True\n    else:                      is_list, imgs = False, [imgs]\n    \n    scale_multiplier = (self.scale_max - self.scale_min) * random.random() + self.scale_min\n    \n    w, h = imgs[0].size\n    ow, oh = int(w * scale_multiplier), int(h * scale_multiplier)\n\n    imgs = [ img.resize((ow, oh), self.interpolation) for img in imgs ]\n    point_meta.apply_scale( [scale_multiplier] )\n\n    if is_list == False: imgs = imgs[0]\n\n    return imgs, point_meta\n\n\nclass AugCrop(object):\n\n  def __init__(self, crop_x, crop_y, center_perterb_max, fill=0):\n    assert isinstance(crop_x, int) and isinstance(crop_y, int) and isinstance(center_perterb_max, numbers.Number)\n    self.crop_x = crop_x\n    self.crop_y = crop_y\n    self.center_perterb_max = center_perterb_max\n    assert isinstance(fill, numbers.Number) or isinstance(fill, str) or isinstance(fill, tuple)\n    self.fill   = fill\n\n  def __call__(self, imgs, point_meta=None):\n    ## AugCrop has something wrong... For unsupervised data\n  \n    point_meta = point_meta.copy()\n    if isinstance(imgs, list): is_list = True\n    else:                      is_list, imgs = False, [imgs]\n\n    dice_x, dice_y = random.random(), random.random()\n    x_offset = int( (dice_x-0.5) * 2 * self.center_perterb_max)\n    y_offset = int( (dice_y-0.5) * 2 * self.center_perterb_max)\n    \n    x1 = int(round( point_meta.center[0] + x_offset - self.crop_x / 2. ))\n    y1 = int(round( point_meta.center[1] + y_offset - self.crop_y / 2. ))\n    x2 = x1 + self.crop_x\n    y2 = y1 + self.crop_y\n\n    w, h = imgs[0].size\n    if x1 < 0 or y1 < 0 or x2 >= w or y2 >= h:\n      pad = max(0-x1, 0-y1, x2-w+1, y2-h+1)\n      assert pad > 0, \'padding operation in crop must be greater than 0\'\n      imgs = [ ImageOps.expand(img, border=pad, fill=self.fill) for img in imgs ]\n      x1, x2, y1, y2 = x1 + pad, x2 + pad, y1 + pad, y2 + pad\n      point_meta.apply_offset(pad, pad)\n      point_meta.apply_bound(imgs[0].size[0], imgs[0].size[1])\n\n    point_meta.apply_offset(-x1, -y1)\n    imgs = [ img.crop((x1, y1, x2, y2)) for img in imgs ]\n    point_meta.apply_bound(imgs[0].size[0], imgs[0].size[1])\n\n    if is_list == False: imgs = imgs[0]\n    return imgs, point_meta\n\n\nclass AugRotate(object):\n  """"""Rotate the given PIL.Image at the center.\n  Args:\n    size (sequence or int): Desired output size of the crop. If size is an\n      int instead of sequence like (w, h), a square crop (size, size) is\n      made.\n  """"""\n\n  def __init__(self, max_rotate_degree):\n    assert isinstance(max_rotate_degree, numbers.Number)\n    self.max_rotate_degree = max_rotate_degree\n\n  def __call__(self, imgs, point_meta):\n    """"""\n    Args:\n      img (PIL.Image): Image to be cropped.\n      point_meta : Point_Meta\n    Returns:\n      PIL.Image: Rotated image.\n    """"""\n    point_meta = point_meta.copy()\n    if isinstance(imgs, list): is_list = True\n    else:                      is_list, imgs = False, [imgs]\n\n    degree = (random.random() - 0.5) * 2 * self.max_rotate_degree\n    center = (imgs[0].size[0] / 2, imgs[0].size[1] / 2)\n    if PIL.__version__[0] == \'4\':\n      imgs = [ img.rotate(degree, center=center) for img in imgs ]\n    else:\n      imgs = [ img.rotate(degree) for img in imgs ]\n\n    point_meta.apply_rotate(center, degree)\n    point_meta.apply_bound(imgs[0].size[0], imgs[0].size[1])\n\n    if is_list == False: imgs = imgs[0]\n\n    return imgs, point_meta\n'"
SBR/lib/xvision/visualization.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom PIL import Image\nfrom PIL import ImageDraw\nfrom PIL import ImageFont\nimport numpy as np\nimport datasets\n\ndef draw_image_by_points(_image, pts, radius, color, crop, resize):\n  if isinstance(_image, str):\n    _image = datasets.pil_loader(_image)\n  assert isinstance(_image, Image.Image), 'image type is not PIL.Image.Image'\n  assert isinstance(pts, np.ndarray) and (pts.shape[0] == 2 or pts.shape[0] == 3), 'input points are not correct'\n  image, pts = _image.copy(), pts.copy()\n\n  num_points = pts.shape[1]\n  visiable_points = []\n  for idx in range(num_points):\n    if pts.shape[0] == 2 or bool(pts[2,idx]):\n      visiable_points.append( True )\n    else:\n      visiable_points.append( False )\n  visiable_points = np.array( visiable_points )\n  #print ('visiable points : {}'.format( np.sum(visiable_points) ))\n\n  if crop:\n    if isinstance(crop, list):\n      x1, y1, x2, y2 = int(crop[0]), int(crop[1]), int(crop[2]), int(crop[3])\n    else:\n      x1, x2 = pts[0, visiable_points].min(), pts[0, visiable_points].max()\n      y1, y2 = pts[1, visiable_points].min(), pts[1, visiable_points].max()\n      face_h, face_w = (y2-y1)*0.1, (x2-x1)*0.1\n      x1, x2 = int(x1 - face_w), int(x2 + face_w)\n      y1, y2 = int(y1 - face_h), int(y2 + face_h)\n    image = image.crop((x1, y1, x2, y2))\n    pts[0, visiable_points] = pts[0, visiable_points] - x1\n    pts[1, visiable_points] = pts[1, visiable_points] - y1\n\n  if resize:\n    width, height = image.size\n    image = image.resize((resize,resize), Image.BICUBIC)\n    pts[0, visiable_points] = pts[0, visiable_points] * 1.0 / width * resize\n    pts[1, visiable_points] = pts[1, visiable_points] * 1.0 / height * resize\n\n  finegrain = True\n  if finegrain:\n    owidth, oheight = image.size\n    image = image.resize((owidth*8,oheight*8), Image.BICUBIC)\n    pts[0, visiable_points] = pts[0, visiable_points] * 8.0\n    pts[1, visiable_points] = pts[1, visiable_points] * 8.0\n    radius = radius * 8\n\n  draw  = ImageDraw.Draw(image)\n  for idx in range(num_points):\n    if visiable_points[ idx ]:\n      # draw hollow circle\n      point = (pts[0,idx]-radius, pts[1,idx]-radius, pts[0,idx]+radius, pts[1,idx]+radius)\n      if radius > 0:\n        draw.ellipse(point, fill=color, outline=color)\n\n  if finegrain:\n    image = image.resize((owidth,oheight), Image.BICUBIC)\n\n  return image\n"""
