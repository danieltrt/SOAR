file_path,api_count,code
setup.py,0,"b'import os\nfrom setuptools import setup, find_packages\n\nwith open(os.path.join(\'stable_baselines3\', \'version.txt\'), \'r\') as file_handler:\n    __version__ = file_handler.read().strip()\n\n\nlong_description = """"""\n\n# Stable Baselines3\n\nStable Baselines3 is a set of improved implementations of reinforcement learning algorithms in PyTorch. It is the next major version of [Stable Baselines](https://github.com/hill-a/stable-baselines).\n\nThese algorithms will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details.\n\n\n## Links\n\nRepository:\nhttps://github.com/DLR-RM/stable-baselines3\n\nMedium article:\nhttps://medium.com/@araffin/df87c4b2fc82\n\nDocumentation:\nhttps://stable-baselines3.readthedocs.io/en/master/\n\nRL Baselines3 Zoo:\nhttps://github.com/DLR-RM/rl-baselines3-zoo\n\n## Quick example\n\nMost of the library tries to follow a sklearn-like syntax for the Reinforcement Learning algorithms using Gym.\n\nHere is a quick example of how to train and run PPO on a cartpole environment:\n\n```python\nimport gym\n\nfrom stable_baselines3 import PPO\n\nenv = gym.make(\'CartPole-v1\')\n\nmodel = PPO(\'MlpPolicy\', env, verbose=1)\nmodel.learn(total_timesteps=10000)\n\nobs = env.reset()\nfor i in range(1000):\n    action, _states = model.predict(obs, deterministic=True)\n    obs, reward, done, info = env.step(action)\n    env.render()\n    if done:\n        obs = env.reset()\n```\n\nOr just train a model with a one liner if [the environment is registered in Gym](https://github.com/openai/gym/wiki/Environments) and if [the policy is registered](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html):\n\n```python\nfrom stable_baselines3 import PPO\n\nmodel = PPO(\'MlpPolicy\', \'CartPole-v1\').learn(10000)\n```\n\n""""""  # noqa:E501\n\n\nsetup(name=\'stable_baselines3\',\n      packages=[package for package in find_packages()\n                if package.startswith(\'stable_baselines3\')],\n      package_data={\n          \'stable_baselines3\': [\'py.typed\', \'version.txt\']\n      },\n      install_requires=[\n          \'gym>=0.17\',\n          \'numpy\',\n          \'torch>=1.4.0\',\n          # For saving models\n          \'cloudpickle\',\n          # For reading logs\n          \'pandas\',\n          # Plotting learning curves\n          \'matplotlib\'\n      ],\n      extras_require={\n          \'tests\': [\n              # Run tests and coverage\n              \'pytest\',\n              \'pytest-cov\',\n              \'pytest-env\',\n              \'pytest-xdist\',\n              # Type check\n              \'pytype\',\n              # Lint code\n              \'flake8>=3.8\'\n          ],\n          \'docs\': [\n              \'sphinx\',\n              \'sphinx-autobuild\',\n              \'sphinx-rtd-theme\',\n              # For spelling\n              \'sphinxcontrib.spelling\',\n              # Type hints support\n              # \'sphinx-autodoc-typehints\'\n          ],\n          \'extra\': [\n              # For render\n              \'opencv-python\',\n              # For atari games,\n              \'atari_py~=0.2.0\', \'pillow\',\n              # Tensorboard support\n              \'tensorboard\'\n          ]\n      },\n      description=\'Pytorch version of Stable Baselines, implementations of reinforcement learning algorithms.\',\n      author=\'Antonin Raffin\',\n      url=\'https://github.com/DLR-RM/stable-baselines3\',\n      author_email=\'antonin.raffin@dlr.de\',\n      keywords=""reinforcement-learning-algorithms reinforcement-learning machine-learning ""\n               ""gym openai stable baselines toolbox python data-science"",\n      license=""MIT"",\n      long_description=long_description,\n      long_description_content_type=\'text/markdown\',\n      version=__version__,\n      )\n\n# python setup.py sdist\n# python setup.py bdist_wheel\n# twine upload --repository-url https://test.pypi.org/legacy/ dist/*\n# twine upload dist/*\n'"
docs/conf.py,2,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nfrom unittest.mock import MagicMock\n\n# We CANNOT enable \'sphinxcontrib.spelling\' because ReadTheDocs.org does not support\n# PyEnchant.\ntry:\n    import sphinxcontrib.spelling\n    enable_spell_check = True\nexcept ImportError:\n    enable_spell_check = False\n\n# source code directory, relative to this file, for sphinx-autobuild\nsys.path.insert(0, os.path.abspath(\'..\'))\n\n\nclass Mock(MagicMock):\n    __subclasses__ = []\n\n    @classmethod\n    def __getattr__(cls, name):\n        return MagicMock()\n\n\n# Mock modules that requires C modules\n# Note: because of that we cannot test examples using CI\n# \'torch\', \'torch.nn\', \'torch.nn.functional\',\n# DO not mock modules for now, we will need to do that for read the docs later\nMOCK_MODULES = []\nsys.modules.update((mod_name, Mock()) for mod_name in MOCK_MODULES)\n\n# Read version from file\nversion_file = os.path.join(os.path.dirname(__file__), \'../stable_baselines3\', \'version.txt\')\nwith open(version_file, \'r\') as file_handler:\n    __version__ = file_handler.read().strip()\n\n# -- Project information -----------------------------------------------------\n\nproject = \'Stable Baselines3\'\ncopyright = \'2020, Stable Baselines3\'\nauthor = \'Stable Baselines3 Contributors\'\n\n# The short X.Y version\nversion = \'master (\' + __version__ + \' )\'\n# The full version, including alpha/beta/rc tags\nrelease = __version__\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    # \'sphinx_autodoc_typehints\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.ifconfig\',\n    \'sphinx.ext.viewcode\',\n    # \'sphinx.ext.intersphinx\',\n    # \'sphinx.ext.doctest\'\n]\n\nif enable_spell_check:\n    extensions.append(\'sphinxcontrib.spelling\')\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n\n# Fix for read the docs\non_rtd = os.environ.get(\'READTHEDOCS\') == \'True\'\nif on_rtd:\n    html_theme = \'default\'\nelse:\n    html_theme = \'sphinx_rtd_theme\'\n\nhtml_logo = \'_static/img/logo.png\'\n\n\ndef setup(app):\n    app.add_stylesheet(""css/baselines_theme.css"")\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'StableBaselines3doc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'StableBaselines3.tex\', \'Stable Baselines3 Documentation\',\n     \'Stable Baselines3 Contributors\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'stablebaselines3\', \'Stable Baselines3 Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'StableBaselines3\', \'Stable Baselines3 Documentation\',\n     author, \'StableBaselines3\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Extension configuration -------------------------------------------------\n\n# Example configuration for intersphinx: refer to the Python standard library.\n# intersphinx_mapping = {\n#     \'python\': (\'https://docs.python.org/3/\', None),\n#     \'numpy\': (\'http://docs.scipy.org/doc/numpy/\', None),\n#     \'torch\': (\'http://pytorch.org/docs/master/\', None),\n# }\n'"
stable_baselines3/__init__.py,0,"b""import os\n\nfrom stable_baselines3.a2c import A2C\nfrom stable_baselines3.ppo import PPO\nfrom stable_baselines3.sac import SAC\nfrom stable_baselines3.td3 import TD3\n\n# Read version from file\nversion_file = os.path.join(os.path.dirname(__file__), 'version.txt')\nwith open(version_file, 'r') as file_handler:\n    __version__ = file_handler.read().strip()\n"""
tests/__init__.py,0,b''
tests/test_callbacks.py,0,"b'import os\nimport shutil\n\nimport pytest\nimport gym\n\nfrom stable_baselines3 import A2C, PPO, SAC, TD3\nfrom stable_baselines3.common.callbacks import (CallbackList, CheckpointCallback, EvalCallback,\n                                                EveryNTimesteps, StopTrainingOnRewardThreshold)\n\n\n@pytest.mark.parametrize(""model_class"", [A2C, PPO, SAC, TD3])\ndef test_callbacks(model_class):\n    log_folder = \'./logs/callbacks/\'\n    # Create RL model\n    # Small network for fast test\n    model = model_class(\'MlpPolicy\', \'Pendulum-v0\', policy_kwargs=dict(net_arch=[32]))\n\n    checkpoint_callback = CheckpointCallback(save_freq=1000, save_path=log_folder)\n\n    eval_env = gym.make(\'Pendulum-v0\')\n    # Stop training if the performance is good enough\n    callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-1200, verbose=1)\n\n    eval_callback = EvalCallback(eval_env, callback_on_new_best=callback_on_best,\n                                 best_model_save_path=log_folder,\n                                 log_path=log_folder, eval_freq=100)\n\n    # Equivalent to the `checkpoint_callback`\n    # but here in an event-driven manner\n    checkpoint_on_event = CheckpointCallback(save_freq=1, save_path=log_folder,\n                                             name_prefix=\'event\')\n    event_callback = EveryNTimesteps(n_steps=500, callback=checkpoint_on_event)\n\n    callback = CallbackList([checkpoint_callback, eval_callback, event_callback])\n\n    model.learn(500, callback=callback)\n    model.learn(500, callback=None)\n    # Transform callback into a callback list automatically\n    model.learn(500, callback=[checkpoint_callback, eval_callback])\n    # Automatic wrapping, old way of doing callbacks\n    model.learn(500, callback=lambda _locals, _globals: True)\n    if os.path.exists(log_folder):\n        shutil.rmtree(log_folder)\n'"
tests/test_cnn.py,0,"b""import os\n\nimport numpy as np\nimport pytest\n\nfrom stable_baselines3 import A2C, PPO, SAC, TD3\nfrom stable_baselines3.common.identity_env import FakeImageEnv\n\nSAVE_PATH = './cnn_model.zip'\n\n\n@pytest.mark.parametrize('model_class', [A2C, PPO, SAC, TD3])\ndef test_cnn(model_class):\n    # Fake grayscale with frameskip\n    # Atari after preprocessing: 84x84x1, here we are using lower resolution\n    # to check that the network handle it automatically\n    env = FakeImageEnv(screen_height=40, screen_width=40, n_channels=1,\n                       discrete=model_class not in {SAC, TD3})\n    if model_class in {A2C, PPO}:\n        kwargs = dict(n_steps=100)\n    else:\n        # Avoid memory error when using replay buffer\n        # Reduce the size of the features\n        kwargs = dict(buffer_size=250,\n                      policy_kwargs=dict(features_extractor_kwargs=dict(features_dim=32)))\n    model = model_class('CnnPolicy', env, **kwargs).learn(250)\n\n    obs = env.reset()\n\n    action, _ = model.predict(obs, deterministic=True)\n\n    model.save(SAVE_PATH)\n    del model\n\n    model = model_class.load(SAVE_PATH)\n\n    # Check that the prediction is the same\n    assert np.allclose(action, model.predict(obs, deterministic=True)[0])\n\n    os.remove(SAVE_PATH)\n"""
tests/test_custom_policy.py,0,"b""import pytest\nimport torch as th\n\nfrom stable_baselines3 import A2C, PPO, SAC, TD3\n\n\n@pytest.mark.parametrize('net_arch', [\n    [12, dict(vf=[16], pi=[8])],\n    [4],\n    [],\n    [4, 4],\n    [12, dict(vf=[8, 4], pi=[8])],\n    [12, dict(vf=[8], pi=[8, 4])],\n    [12, dict(pi=[8])],\n])\n@pytest.mark.parametrize('model_class', [A2C, PPO])\ndef test_flexible_mlp(model_class, net_arch):\n    _ = model_class('MlpPolicy', 'CartPole-v1', policy_kwargs=dict(net_arch=net_arch), n_steps=100).learn(1000)\n\n\n@pytest.mark.parametrize('net_arch', [\n    [4],\n    [4, 4],\n])\n@pytest.mark.parametrize('model_class', [SAC, TD3])\ndef test_custom_offpolicy(model_class, net_arch):\n    _ = model_class('MlpPolicy', 'Pendulum-v0', policy_kwargs=dict(net_arch=net_arch)).learn(1000)\n\n\n@pytest.mark.parametrize('model_class', [A2C, PPO, SAC, TD3])\n@pytest.mark.parametrize('optimizer_kwargs', [None, dict(weight_decay=0.0)])\ndef test_custom_optimizer(model_class, optimizer_kwargs):\n    policy_kwargs = dict(optimizer_class=th.optim.AdamW, optimizer_kwargs=optimizer_kwargs, net_arch=[32])\n    _ = model_class('MlpPolicy', 'Pendulum-v0', policy_kwargs=policy_kwargs).learn(1000)\n"""
tests/test_deterministic.py,0,"b'import pytest\n\nfrom stable_baselines3 import A2C, PPO, SAC, TD3\nfrom stable_baselines3.common.noise import NormalActionNoise\n\nN_STEPS_TRAINING = 3000\nSEED = 0\n\n\n@pytest.mark.parametrize(""algo"", [A2C, PPO, SAC, TD3])\ndef test_deterministic_training_common(algo):\n    results = [[], []]\n    rewards = [[], []]\n    # Smaller network\n    kwargs = {\'policy_kwargs\': dict(net_arch=[64])}\n    if algo in [TD3, SAC]:\n        env_id = \'Pendulum-v0\'\n        kwargs.update({\'action_noise\': NormalActionNoise(0.0, 0.1),\n                       \'learning_starts\': 100})\n    else:\n        env_id = \'CartPole-v1\'\n        # if algo == DQN:\n        #     kwargs.update({\'learning_starts\': 100})\n\n    for i in range(2):\n        model = algo(\'MlpPolicy\', env_id, seed=SEED, **kwargs)\n        model.learn(N_STEPS_TRAINING)\n        env = model.get_env()\n        obs = env.reset()\n        for _ in range(100):\n            action, _ = model.predict(obs, deterministic=False)\n            obs, reward, _, _ = env.step(action)\n            results[i].append(action)\n            rewards[i].append(reward)\n    assert sum(results[0]) == sum(results[1]), results\n    assert sum(rewards[0]) == sum(rewards[1]), rewards\n'"
tests/test_distributions.py,0,"b'import pytest\nimport torch as th\n\nfrom stable_baselines3 import A2C, PPO\nfrom stable_baselines3.common.distributions import (DiagGaussianDistribution, TanhBijector,\n                                                    StateDependentNoiseDistribution,\n                                                    CategoricalDistribution, SquashedDiagGaussianDistribution,\n                                                    MultiCategoricalDistribution, BernoulliDistribution)\nfrom stable_baselines3.common.utils import set_random_seed\n\n\nN_ACTIONS = 2\nN_FEATURES = 3\nN_SAMPLES = int(5e6)\n\n\ndef test_bijector():\n    """"""\n    Test TanhBijector\n    """"""\n    actions = th.ones(5) * 2.0\n    bijector = TanhBijector()\n\n    squashed_actions = bijector.forward(actions)\n    # Check that the boundaries are not violated\n    assert th.max(th.abs(squashed_actions)) <= 1.0\n    # Check the inverse method\n    assert th.isclose(TanhBijector.inverse(squashed_actions), actions).all()\n\n\n@pytest.mark.parametrize(""model_class"", [A2C, PPO])\ndef test_squashed_gaussian(model_class):\n    """"""\n    Test run with squashed Gaussian (notably entropy computation)\n    """"""\n    model = model_class(\'MlpPolicy\', \'Pendulum-v0\', use_sde=True, n_steps=100, policy_kwargs=dict(squash_output=True))\n    model.learn(500)\n\n    gaussian_mean = th.rand(N_SAMPLES, N_ACTIONS)\n    dist = SquashedDiagGaussianDistribution(N_ACTIONS)\n    _, log_std = dist.proba_distribution_net(N_FEATURES)\n    dist = dist.proba_distribution(gaussian_mean, log_std)\n    actions = dist.get_actions()\n    assert th.max(th.abs(actions)) <= 1.0\n\n\ndef test_sde_distribution():\n    n_actions = 1\n    deterministic_actions = th.ones(N_SAMPLES, n_actions) * 0.1\n    state = th.ones(N_SAMPLES, N_FEATURES) * 0.3\n    dist = StateDependentNoiseDistribution(n_actions, full_std=True, squash_output=False)\n\n    set_random_seed(1)\n    _, log_std = dist.proba_distribution_net(N_FEATURES)\n    dist.sample_weights(log_std, batch_size=N_SAMPLES)\n\n    dist = dist.proba_distribution(deterministic_actions, log_std, state)\n    actions = dist.get_actions()\n\n    assert th.allclose(actions.mean(), dist.distribution.mean.mean(), rtol=2e-3)\n    assert th.allclose(actions.std(), dist.distribution.scale.mean(), rtol=2e-3)\n\n\n# TODO: analytical form for squashed Gaussian?\n@pytest.mark.parametrize(""dist"", [\n    DiagGaussianDistribution(N_ACTIONS),\n    StateDependentNoiseDistribution(N_ACTIONS, squash_output=False),\n])\ndef test_entropy(dist):\n    # The entropy can be approximated by averaging the negative log likelihood\n    # mean negative log likelihood == differential entropy\n    set_random_seed(1)\n    state = th.rand(N_SAMPLES, N_FEATURES)\n    deterministic_actions = th.rand(N_SAMPLES, N_ACTIONS)\n    _, log_std = dist.proba_distribution_net(N_FEATURES, log_std_init=th.log(th.tensor(0.2)))\n\n    if isinstance(dist, DiagGaussianDistribution):\n        dist = dist.proba_distribution(deterministic_actions, log_std)\n    else:\n        dist.sample_weights(log_std, batch_size=N_SAMPLES)\n        dist = dist.proba_distribution(deterministic_actions, log_std, state)\n\n    actions = dist.get_actions()\n    entropy = dist.entropy()\n    log_prob = dist.log_prob(actions)\n    assert th.allclose(entropy.mean(), -log_prob.mean(), rtol=5e-3)\n\n\ncategorical_params = [\n    (CategoricalDistribution(N_ACTIONS), N_ACTIONS),\n    (MultiCategoricalDistribution([2, 3]), sum([2, 3])),\n    (BernoulliDistribution(N_ACTIONS), N_ACTIONS)\n]\n\n\n@pytest.mark.parametrize(""dist, CAT_ACTIONS"", categorical_params)\ndef test_categorical(dist, CAT_ACTIONS):\n    # The entropy can be approximated by averaging the negative log likelihood\n    # mean negative log likelihood == entropy\n    set_random_seed(1)\n    action_logits = th.rand(N_SAMPLES, CAT_ACTIONS)\n    dist = dist.proba_distribution(action_logits)\n    actions = dist.get_actions()\n    entropy = dist.entropy()\n    log_prob = dist.log_prob(actions)\n    assert th.allclose(entropy.mean(), -log_prob.mean(), rtol=5e-3)\n'"
tests/test_envs.py,0,"b'import pytest\nimport gym\nfrom gym import spaces\nimport numpy as np\n\nfrom stable_baselines3.common.env_checker import check_env\nfrom stable_baselines3.common.bit_flipping_env import BitFlippingEnv\nfrom stable_baselines3.common.identity_env import (IdentityEnv, IdentityEnvBox, FakeImageEnv,\n                                                   IdentityEnvMultiBinary, IdentityEnvMultiDiscrete)\n\nENV_CLASSES = [BitFlippingEnv, IdentityEnv, IdentityEnvBox, IdentityEnvMultiBinary,\n               IdentityEnvMultiDiscrete, FakeImageEnv]\n\n\n@pytest.mark.parametrize(""env_id"", [\'CartPole-v0\', \'Pendulum-v0\'])\ndef test_env(env_id):\n    """"""\n    Check that environmnent integrated in Gym pass the test.\n\n    :param env_id: (str)\n    """"""\n    env = gym.make(env_id)\n    with pytest.warns(None) as record:\n        check_env(env)\n\n    # Pendulum-v0 will produce a warning because the action space is\n    # in [-2, 2] and not [-1, 1]\n    if env_id == \'Pendulum-v0\':\n        assert len(record) == 1\n    else:\n        # The other environments must pass without warning\n        assert len(record) == 0\n\n\n@pytest.mark.parametrize(""env_class"", ENV_CLASSES)\ndef test_custom_envs(env_class):\n    env = env_class()\n    check_env(env)\n\n\ndef test_high_dimension_action_space():\n    """"""\n    Test for continuous action space\n    with more than one action.\n    """"""\n    env = FakeImageEnv()\n    # Patch the action space\n    env.action_space = spaces.Box(low=-1, high=1, shape=(20,), dtype=np.float32)\n\n    # Patch to avoid error\n    def patched_step(_action):\n        return env.observation_space.sample(), 0.0, False, {}\n    env.step = patched_step\n    check_env(env)\n\n\n@pytest.mark.parametrize(""new_obs_space"", [\n    # Small image\n    spaces.Box(low=0, high=255, shape=(32, 32, 3), dtype=np.uint8),\n    # Range not in [0, 255]\n    spaces.Box(low=0, high=1, shape=(64, 64, 3), dtype=np.uint8),\n    # Wrong dtype\n    spaces.Box(low=0, high=255, shape=(64, 64, 3), dtype=np.float32),\n    # Not an image, it should be a 1D vector\n    spaces.Box(low=-1, high=1, shape=(64, 3), dtype=np.float32),\n    # Tuple space is not supported by SB\n    spaces.Tuple([spaces.Discrete(5), spaces.Discrete(10)]),\n    # Dict space is not supported by SB when env is not a GoalEnv\n    spaces.Dict({""position"": spaces.Discrete(5)}),\n])\ndef test_non_default_spaces(new_obs_space):\n    env = FakeImageEnv()\n    env.observation_space = new_obs_space\n    # Patch methods to avoid errors\n    env.reset = new_obs_space.sample\n\n    def patched_step(_action):\n        return new_obs_space.sample(), 0.0, False, {}\n\n    env.step = patched_step\n    with pytest.warns(UserWarning):\n        check_env(env)\n\n\ndef check_reset_assert_error(env, new_reset_return):\n    """"""\n    Helper to check that the error is caught.\n    :param env: (gym.Env)\n    :param new_reset_return: (Any)\n    """"""\n\n    def wrong_reset():\n        return new_reset_return\n\n    # Patch the reset method with a wrong one\n    env.reset = wrong_reset\n    with pytest.raises(AssertionError):\n        check_env(env)\n\n\ndef test_common_failures_reset():\n    """"""\n    Test that common failure cases of the `reset_method` are caught\n    """"""\n    env = IdentityEnvBox()\n    # Return an observation that does not match the observation_space\n    check_reset_assert_error(env, np.ones((3,)))\n    # The observation is not a numpy array\n    check_reset_assert_error(env, 1)\n\n    # Return not only the observation\n    check_reset_assert_error(env, (env.observation_space.sample(), False))\n\n\ndef check_step_assert_error(env, new_step_return=()):\n    """"""\n    Helper to check that the error is caught.\n    :param env: (gym.Env)\n    :param new_step_return: (tuple)\n    """"""\n\n    def wrong_step(_action):\n        return new_step_return\n\n    # Patch the step method with a wrong one\n    env.step = wrong_step\n    with pytest.raises(AssertionError):\n        check_env(env)\n\n\ndef test_common_failures_step():\n    """"""\n    Test that common failure cases of the `step` method are caught\n    """"""\n    env = IdentityEnvBox()\n\n    # Wrong shape for the observation\n    check_step_assert_error(env, (np.ones((4,)), 1.0, False, {}))\n    # Obs is not a numpy array\n    check_step_assert_error(env, (1, 1.0, False, {}))\n\n    # Return a wrong reward\n    check_step_assert_error(env, (env.observation_space.sample(), np.ones(1), False, {}))\n\n    # Info dict is not returned\n    check_step_assert_error(env, (env.observation_space.sample(), 0.0, False))\n\n    # Done is not a boolean\n    check_step_assert_error(env, (env.observation_space.sample(), 0.0, 3.0, {}))\n    check_step_assert_error(env, (env.observation_space.sample(), 0.0, 1, {}))\n'"
tests/test_identity.py,0,"b'import numpy as np\nimport pytest\n\nfrom stable_baselines3 import A2C, PPO, SAC, TD3\nfrom stable_baselines3.common.identity_env import (IdentityEnvBox, IdentityEnv,\n                                                   IdentityEnvMultiBinary, IdentityEnvMultiDiscrete)\n\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.noise import NormalActionNoise\n\n\nDIM = 4\n\n\n@pytest.mark.parametrize(""model_class"", [A2C, PPO])\n@pytest.mark.parametrize(""env"", [IdentityEnv(DIM), IdentityEnvMultiDiscrete(DIM), IdentityEnvMultiBinary(DIM)])\ndef test_discrete(model_class, env):\n    env = DummyVecEnv([lambda: env])\n    model = model_class(\'MlpPolicy\', env, gamma=0.5, seed=1).learn(3000)\n\n    evaluate_policy(model, env, n_eval_episodes=20, reward_threshold=90)\n    obs = env.reset()\n\n    assert np.shape(model.predict(obs)[0]) == np.shape(obs)\n\n\n@pytest.mark.parametrize(""model_class"", [A2C, PPO, SAC, TD3])\ndef test_continuous(model_class):\n    env = IdentityEnvBox(eps=0.5)\n\n    n_steps = {\n        A2C: 3500,\n        PPO: 3000,\n        SAC: 700,\n        TD3: 500\n    }[model_class]\n\n    kwargs = dict(\n        policy_kwargs=dict(net_arch=[64, 64]),\n        seed=0,\n        gamma=0.95\n    )\n    if model_class in [TD3]:\n        n_actions = 1\n        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n        kwargs[\'action_noise\'] = action_noise\n\n    model = model_class(\'MlpPolicy\', env, **kwargs).learn(n_steps)\n\n    evaluate_policy(model, env, n_eval_episodes=20, reward_threshold=90)\n'"
tests/test_logger.py,0,"b'import pytest\nimport numpy as np\n\nfrom stable_baselines3.common.logger import (make_output_format, read_csv, read_json, DEBUG, ScopedConfigure,\n                                             info, debug, set_level, configure, record, record_dict,\n                                             dump, record_mean, warn, error, reset)\n\nKEY_VALUES = {\n    ""test"": 1,\n    ""b"": -3.14,\n    ""8"": 9.9,\n    ""l"": [1, 2],\n    ""a"": np.array([1, 2, 3]),\n    ""f"": np.array(1),\n    ""g"": np.array([[[1]]]),\n}\n\nKEY_EXCLUDED = {}\nfor key in KEY_VALUES.keys():\n    KEY_EXCLUDED[key] = None\n\n\ndef test_main(tmp_path):\n    """"""\n    tests for the logger module\n    """"""\n    info(""hi"")\n    debug(""shouldn\'t appear"")\n    set_level(DEBUG)\n    debug(""should appear"")\n    configure(folder=str(tmp_path))\n    record(""a"", 3)\n    record(""b"", 2.5)\n    dump()\n    record(""b"", -2.5)\n    record(""a"", 5.5)\n    dump()\n    info(""^^^ should see a = 5.5"")\n    record_mean(""b"", -22.5)\n    record_mean(""b"", -44.4)\n    record(""a"", 5.5)\n    dump()\n    with ScopedConfigure(None, None):\n        info(""^^^ should see b = 33.3"")\n\n    with ScopedConfigure(str(tmp_path / ""test-logger""), [""json""]):\n        record(""b"", -2.5)\n        dump()\n\n    reset()\n    record(""a"", ""longasslongasslongasslongasslongasslongassvalue"")\n    dump()\n    warn(""hey"")\n    error(""oh"")\n    record_dict({""test"": 1})\n\n\n@pytest.mark.parametrize(\'_format\', [\'stdout\', \'log\', \'json\', \'csv\', \'tensorboard\'])\ndef test_make_output(tmp_path, _format):\n    """"""\n    test make output\n\n    :param _format: (str) output format\n    """"""\n    if _format == \'tensorboard\':\n        # Skip if no tensorboard installed\n        pytest.importorskip(""tensorboard"")\n\n    writer = make_output_format(_format, tmp_path)\n    writer.write(KEY_VALUES, KEY_EXCLUDED)\n    if _format == ""csv"":\n        read_csv(tmp_path / \'progress.csv\')\n    elif _format == \'json\':\n        read_json(tmp_path / \'progress.json\')\n    writer.close()\n\n\ndef test_make_output_fail(tmp_path):\n    """"""\n    test value error on logger\n    """"""\n    with pytest.raises(ValueError):\n        make_output_format(\'dummy_format\', tmp_path)\n'"
tests/test_monitor.py,0,"b'import uuid\nimport json\nimport os\n\nimport pandas\nimport gym\n\nfrom stable_baselines3.common.monitor import Monitor, get_monitor_files, load_results\n\n\ndef test_monitor(tmp_path):\n    """"""\n    Test the monitor wrapper\n    """"""\n    env = gym.make(""CartPole-v1"")\n    env.seed(0)\n    monitor_file = os.path.join(str(tmp_path), ""stable_baselines-test-{}.monitor.csv"".format(uuid.uuid4()))\n    monitor_env = Monitor(env, monitor_file)\n    monitor_env.reset()\n    total_steps = 1000\n    ep_rewards = []\n    ep_lengths = []\n    ep_len, ep_reward = 0, 0\n    for _ in range(total_steps):\n        _, reward, done, _ = monitor_env.step(monitor_env.action_space.sample())\n        ep_len += 1\n        ep_reward += reward\n        if done:\n            ep_rewards.append(ep_reward)\n            ep_lengths.append(ep_len)\n            monitor_env.reset()\n            ep_len, ep_reward = 0, 0\n\n    monitor_env.close()\n    assert monitor_env.get_total_steps() == total_steps\n    assert sum(ep_lengths) == sum(monitor_env.get_episode_lengths())\n    assert sum(monitor_env.get_episode_rewards()) == sum(ep_rewards)\n    _ = monitor_env.get_episode_times()\n\n    with open(monitor_file, \'rt\') as file_handler:\n        first_line = file_handler.readline()\n        assert first_line.startswith(\'#\')\n        metadata = json.loads(first_line[1:])\n        assert metadata[\'env_id\'] == ""CartPole-v1""\n        assert set(metadata.keys()) == {\'env_id\', \'t_start\'}, ""Incorrect keys in monitor metadata""\n\n        last_logline = pandas.read_csv(file_handler, index_col=None)\n        assert set(last_logline.keys()) == {\'l\', \'t\', \'r\'}, ""Incorrect keys in monitor logline""\n    os.remove(monitor_file)\n\n\ndef test_monitor_load_results(tmp_path):\n    """"""\n    test load_results on log files produced by the monitor wrapper\n    """"""\n    tmp_path = str(tmp_path)\n    env1 = gym.make(""CartPole-v1"")\n    env1.seed(0)\n    monitor_file1 = os.path.join(tmp_path, ""stable_baselines-test-{}.monitor.csv"".format(uuid.uuid4()))\n    monitor_env1 = Monitor(env1, monitor_file1)\n\n    monitor_files = get_monitor_files(tmp_path)\n    assert len(monitor_files) == 1\n    assert monitor_file1 in monitor_files\n\n    monitor_env1.reset()\n    episode_count1 = 0\n    for _ in range(1000):\n        _, _, done, _ = monitor_env1.step(monitor_env1.action_space.sample())\n        if done:\n            episode_count1 += 1\n            monitor_env1.reset()\n\n    results_size1 = len(load_results(os.path.join(tmp_path)).index)\n    assert results_size1 == episode_count1\n\n    env2 = gym.make(""CartPole-v1"")\n    env2.seed(0)\n    monitor_file2 = os.path.join(tmp_path, ""stable_baselines-test-{}.monitor.csv"".format(uuid.uuid4()))\n    monitor_env2 = Monitor(env2, monitor_file2)\n    monitor_files = get_monitor_files(tmp_path)\n    assert len(monitor_files) == 2\n    assert monitor_file1 in monitor_files\n    assert monitor_file2 in monitor_files\n\n    monitor_env2.reset()\n    episode_count2 = 0\n    for _ in range(1000):\n        _, _, done, _ = monitor_env2.step(monitor_env2.action_space.sample())\n        if done:\n            episode_count2 += 1\n            monitor_env2.reset()\n\n    results_size2 = len(load_results(os.path.join(tmp_path)).index)\n\n    assert results_size2 == (results_size1 + episode_count2)\n\n    os.remove(monitor_file1)\n    os.remove(monitor_file2)\n'"
tests/test_predict.py,0,"b'import gym\nimport pytest\n\nfrom stable_baselines3 import A2C, PPO, SAC, TD3\nfrom stable_baselines3.common.vec_env import DummyVecEnv\n\nMODEL_LIST = [\n    PPO,\n    A2C,\n    TD3,\n    SAC,\n]\n\n\n@pytest.mark.parametrize(""model_class"", MODEL_LIST)\ndef test_auto_wrap(model_class):\n    # test auto wrapping of env into a VecEnv\n    env = gym.make(\'Pendulum-v0\')\n    eval_env = gym.make(\'Pendulum-v0\')\n    model = model_class(\'MlpPolicy\', env)\n    model.learn(100, eval_env=eval_env)\n\n\n@pytest.mark.parametrize(""model_class"", MODEL_LIST)\n@pytest.mark.parametrize(""env_id"", [\'Pendulum-v0\', \'CartPole-v1\'])\ndef test_predict(model_class, env_id):\n    if env_id == \'CartPole-v1\' and model_class not in [PPO, A2C]:\n        return\n\n    # test detection of different shapes by the predict method\n    model = model_class(\'MlpPolicy\', env_id)\n    env = gym.make(env_id)\n    vec_env = DummyVecEnv([lambda: gym.make(env_id), lambda: gym.make(env_id)])\n\n    obs = env.reset()\n    action, _ = model.predict(obs)\n    assert action.shape == env.action_space.shape\n    assert env.action_space.contains(action)\n\n    vec_env_obs = vec_env.reset()\n    action, _ = model.predict(vec_env_obs)\n    assert action.shape[0] == vec_env_obs.shape[0]\n'"
tests/test_run.py,0,"b'import numpy as np\nimport pytest\n\nfrom stable_baselines3 import A2C, PPO, SAC, TD3\nfrom stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n\nnormal_action_noise = NormalActionNoise(np.zeros(1), 0.1 * np.ones(1))\n\n\n@pytest.mark.parametrize(\'action_noise\', [normal_action_noise, OrnsteinUhlenbeckActionNoise(np.zeros(1), 0.1 * np.ones(1))])\ndef test_td3(action_noise):\n    model = TD3(\'MlpPolicy\', \'Pendulum-v0\', policy_kwargs=dict(net_arch=[64, 64]),\n                learning_starts=100, verbose=1, create_eval_env=True, action_noise=action_noise)\n    model.learn(total_timesteps=1000, eval_freq=500)\n\n\n@pytest.mark.parametrize(""model_class"", [A2C, PPO])\n@pytest.mark.parametrize(""env_id"", [\'CartPole-v1\', \'Pendulum-v0\'])\ndef test_onpolicy(model_class, env_id):\n    model = model_class(\'MlpPolicy\', env_id, seed=0, policy_kwargs=dict(net_arch=[16]), verbose=1, create_eval_env=True)\n    model.learn(total_timesteps=1000, eval_freq=500)\n\n\n@pytest.mark.parametrize(""ent_coef"", [\'auto\', 0.01])\ndef test_sac(ent_coef):\n    model = SAC(\'MlpPolicy\', \'Pendulum-v0\', policy_kwargs=dict(net_arch=[64, 64]),\n                learning_starts=100, verbose=1, create_eval_env=True, ent_coef=ent_coef,\n                action_noise=NormalActionNoise(np.zeros(1), np.zeros(1)))\n    model.learn(total_timesteps=1000, eval_freq=500)\n'"
tests/test_save_load.py,0,"b'import os\nfrom copy import deepcopy\n\nimport pytest\nimport numpy as np\nimport torch as th\n\nfrom stable_baselines3 import A2C, PPO, SAC, TD3\nfrom stable_baselines3.common.identity_env import IdentityEnvBox\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.identity_env import FakeImageEnv\n\n\nMODEL_LIST = [\n    PPO,\n    A2C,\n    TD3,\n    SAC,\n]\n\n\n@pytest.mark.parametrize(""model_class"", MODEL_LIST)\ndef test_save_load(model_class):\n    """"""\n    Test if \'save\' and \'load\' saves and loads model correctly\n    and if \'load_parameters\' and \'get_policy_parameters\' work correctly\n\n    \'\'warning does not test function of optimizer parameter load\n\n    :param model_class: (BaseAlgorithm) A RL model\n    """"""\n    env = DummyVecEnv([lambda: IdentityEnvBox(10)])\n\n    # create model\n    model = model_class(\'MlpPolicy\', env, policy_kwargs=dict(net_arch=[16]), verbose=1)\n    model.learn(total_timesteps=500, eval_freq=250)\n\n    env.reset()\n    observations = np.concatenate([env.step(env.action_space.sample())[0] for _ in range(10)], axis=0)\n\n    # Get dictionary of current parameters\n    params = deepcopy(model.policy.state_dict())\n\n    # Modify all parameters to be random values\n    random_params = dict((param_name, th.rand_like(param)) for param_name, param in params.items())\n\n    # Update model parameters with the new random values\n    model.policy.load_state_dict(random_params)\n\n    new_params = model.policy.state_dict()\n    # Check that all params are different now\n    for k in params:\n        assert not th.allclose(params[k], new_params[k]), ""Parameters did not change as expected.""\n\n    params = new_params\n\n    # get selected actions\n    selected_actions, _ = model.predict(observations, deterministic=True)\n\n    # Check\n    model.save(""test_save.zip"")\n    del model\n    model = model_class.load(""test_save"", env=env)\n\n    # check if params are still the same after load\n    new_params = model.policy.state_dict()\n\n    # Check that all params are the same as before save load procedure now\n    for key in params:\n        assert th.allclose(params[key], new_params[key]), ""Model parameters not the same after save and load.""\n\n    # check if model still selects the same actions\n    new_selected_actions, _ = model.predict(observations, deterministic=True)\n    assert np.allclose(selected_actions, new_selected_actions, 1e-4)\n\n    # check if learn still works\n    model.learn(total_timesteps=1000, eval_freq=500)\n\n    # clear file from os\n    os.remove(""test_save.zip"")\n\n\n@pytest.mark.parametrize(""model_class"", MODEL_LIST)\ndef test_set_env(model_class):\n    """"""\n    Test if set_env function does work correct\n    :param model_class: (BaseAlgorithm) A RL model\n    """"""\n    env = DummyVecEnv([lambda: IdentityEnvBox(10)])\n    env2 = DummyVecEnv([lambda: IdentityEnvBox(10)])\n    env3 = IdentityEnvBox(10)\n\n    # create model\n    model = model_class(\'MlpPolicy\', env, policy_kwargs=dict(net_arch=[16]))\n    # learn\n    model.learn(total_timesteps=1000, eval_freq=500)\n\n    # change env\n    model.set_env(env2)\n    # learn again\n    model.learn(total_timesteps=1000, eval_freq=500)\n\n    # change env test wrapping\n    model.set_env(env3)\n    # learn again\n    model.learn(total_timesteps=1000, eval_freq=500)\n\n\n@pytest.mark.parametrize(""model_class"", MODEL_LIST)\ndef test_exclude_include_saved_params(model_class):\n    """"""\n    Test if exclude and include parameters of save() work\n\n    :param model_class: (BaseAlgorithm) A RL model\n    """"""\n    env = DummyVecEnv([lambda: IdentityEnvBox(10)])\n\n    # create model, set verbose as 2, which is not standard\n    model = model_class(\'MlpPolicy\', env, policy_kwargs=dict(net_arch=[16]), verbose=2)\n\n    # Check if exclude works\n    model.save(""test_save.zip"", exclude=[""verbose""])\n    del model\n    model = model_class.load(""test_save"")\n    # check if verbose was not saved\n    assert model.verbose != 2\n\n    # set verbose as something different then standard settings\n    model.verbose = 2\n    # Check if include works\n    model.save(""test_save.zip"", exclude=[""verbose""], include=[""verbose""])\n    del model\n    model = model_class.load(""test_save"")\n    assert model.verbose == 2\n\n    # clear file from os\n    os.remove(""test_save.zip"")\n\n\n@pytest.mark.parametrize(""model_class"", [SAC, TD3])\ndef test_save_load_replay_buffer(model_class):\n    log_folder = \'logs\'\n    replay_path = os.path.join(log_folder, \'replay_buffer.pkl\')\n    os.makedirs(log_folder, exist_ok=True)\n    model = model_class(\'MlpPolicy\', \'Pendulum-v0\', buffer_size=1000)\n    model.learn(500)\n    old_replay_buffer = deepcopy(model.replay_buffer)\n    model.save_replay_buffer(log_folder)\n    model.replay_buffer = None\n    model.load_replay_buffer(replay_path)\n\n    assert np.allclose(old_replay_buffer.observations, model.replay_buffer.observations)\n    assert np.allclose(old_replay_buffer.actions, model.replay_buffer.actions)\n    assert np.allclose(old_replay_buffer.next_observations, model.replay_buffer.next_observations)\n    assert np.allclose(old_replay_buffer.rewards, model.replay_buffer.rewards)\n    assert np.allclose(old_replay_buffer.dones, model.replay_buffer.dones)\n\n    # test extending replay buffer\n    model.replay_buffer.extend(old_replay_buffer.observations, old_replay_buffer.next_observations,\n                               old_replay_buffer.actions, old_replay_buffer.rewards, old_replay_buffer.dones)\n\n    # clear file from os\n    os.remove(replay_path)\n\n\n@pytest.mark.parametrize(""model_class"", MODEL_LIST)\n@pytest.mark.parametrize(""policy_str"", [\'MlpPolicy\', \'CnnPolicy\'])\ndef test_save_load_policy(model_class, policy_str):\n    """"""\n    Test saving and loading policy only.\n\n    :param model_class: (BaseAlgorithm) A RL model\n    :param policy_str: (str) Name of the policy.\n    """"""\n    kwargs = {}\n    if policy_str == \'MlpPolicy\':\n        env = IdentityEnvBox(10)\n    else:\n        if model_class in [SAC, TD3]:\n            # Avoid memory error when using replay buffer\n            # Reduce the size of the features\n            kwargs = dict(buffer_size=250)\n        env = FakeImageEnv(screen_height=40, screen_width=40, n_channels=2,\n                           discrete=False)\n\n    env = DummyVecEnv([lambda: env])\n\n    # create model\n    model = model_class(policy_str, env, policy_kwargs=dict(net_arch=[16]),\n                        verbose=1, **kwargs)\n    model.learn(total_timesteps=500, eval_freq=250)\n\n    env.reset()\n    observations = np.concatenate([env.step(env.action_space.sample())[0] for _ in range(10)], axis=0)\n\n    policy = model.policy\n    policy_class = policy.__class__\n    actor, actor_class = None, None\n    if model_class in [SAC, TD3]:\n        actor = policy.actor\n        actor_class = actor.__class__\n\n    # Get dictionary of current parameters\n    params = deepcopy(policy.state_dict())\n\n    # Modify all parameters to be random values\n    random_params = dict((param_name, th.rand_like(param)) for param_name, param in params.items())\n\n    # Update model parameters with the new random values\n    policy.load_state_dict(random_params)\n\n    new_params = policy.state_dict()\n    # Check that all params are different now\n    for k in params:\n        assert not th.allclose(params[k], new_params[k]), ""Parameters did not change as expected.""\n\n    params = new_params\n\n    # get selected actions\n    selected_actions, _ = policy.predict(observations, deterministic=True)\n    # Should also work with the actor only\n    if actor is not None:\n        selected_actions_actor, _ = actor.predict(observations, deterministic=True)\n\n    # Save and load policy\n    policy.save(""./logs/policy.pkl"")\n    # Save and load actor\n    if actor is not None:\n        actor.save(""./logs/actor.pkl"")\n\n    del policy, actor\n\n    policy = policy_class.load(""./logs/policy.pkl"")\n    if actor_class is not None:\n        actor = actor_class.load(""./logs/actor.pkl"")\n\n    # check if params are still the same after load\n    new_params = policy.state_dict()\n\n    # Check that all params are the same as before save load procedure now\n    for key in params:\n        assert th.allclose(params[key], new_params[key]), ""Policy parameters not the same after save and load.""\n\n    # check if model still selects the same actions\n    new_selected_actions, _ = policy.predict(observations, deterministic=True)\n    assert np.allclose(selected_actions, new_selected_actions, 1e-4)\n\n    if actor_class is not None:\n        new_selected_actions_actor, _ = actor.predict(observations, deterministic=True)\n        assert np.allclose(selected_actions_actor, new_selected_actions_actor, 1e-4)\n        assert np.allclose(selected_actions_actor, new_selected_actions, 1e-4)\n\n    # clear file from os\n    os.remove(""./logs/policy.pkl"")\n    if actor_class is not None:\n        os.remove(""./logs/actor.pkl"")\n'"
tests/test_sde.py,1,"b'import pytest\nimport torch as th\nfrom torch.distributions import Normal\n\nfrom stable_baselines3 import A2C, SAC, PPO\n\n\ndef test_state_dependent_exploration_grad():\n    """"""\n    Check that the gradient correspond to the expected one\n    """"""\n    n_states = 2\n    state_dim = 3\n    action_dim = 10\n    sigma_hat = th.ones(state_dim, action_dim, requires_grad=True)\n    # Reduce the number of parameters\n    # sigma_ = th.ones(state_dim, action_dim) * sigma_\n    # weights_dist = Normal(th.zeros_like(log_sigma), th.exp(log_sigma))\n    th.manual_seed(2)\n    weights_dist = Normal(th.zeros_like(sigma_hat), sigma_hat)\n    weights = weights_dist.rsample()\n\n    state = th.rand(n_states, state_dim)\n    mu = th.ones(action_dim)\n    noise = th.mm(state, weights)\n\n    action = mu + noise\n\n    variance = th.mm(state ** 2, sigma_hat ** 2)\n    action_dist = Normal(mu, th.sqrt(variance))\n\n    # Sum over the action dimension because we assume they are independent\n    loss = action_dist.log_prob(action.detach()).sum(dim=-1).mean()\n    loss.backward()\n\n    # From Rueckstiess paper: check that the computed gradient\n    # correspond to the analytical form\n    grad = th.zeros_like(sigma_hat)\n    for j in range(action_dim):\n        # sigma_hat is the std of the gaussian distribution of the noise matrix weights\n        # sigma_j = sum_j(state_i **2 * sigma_hat_ij ** 2)\n        # sigma_j is the standard deviation of the policy gaussian distribution\n        sigma_j = th.sqrt(variance[:, j])\n        for i in range(state_dim):\n            # Derivative of the log probability of the jth component of the action\n            # w.r.t. the standard deviation sigma_j\n            d_log_policy_j = (noise[:, j] ** 2 - sigma_j ** 2) / sigma_j ** 3\n            # Derivative of sigma_j w.r.t. sigma_hat_ij\n            d_log_sigma_j = (state[:, i] ** 2 * sigma_hat[i, j]) / sigma_j\n            # Chain rule, average over the minibatch\n            grad[i, j] = (d_log_policy_j * d_log_sigma_j).mean()\n\n    # sigma.grad should be equal to grad\n    assert sigma_hat.grad.allclose(grad)\n\n\n@pytest.mark.parametrize(""model_class"", [SAC, A2C, PPO])\n@pytest.mark.parametrize(""sde_net_arch"", [None, [32, 16], []])\n@pytest.mark.parametrize(""use_expln"", [False, True])\ndef test_state_dependent_offpolicy_noise(model_class, sde_net_arch, use_expln):\n    model = model_class(\'MlpPolicy\', \'Pendulum-v0\', use_sde=True, seed=None, create_eval_env=True,\n                        verbose=1, policy_kwargs=dict(log_std_init=-2, sde_net_arch=sde_net_arch, use_expln=use_expln))\n    model.learn(total_timesteps=int(500), eval_freq=250)\n'"
tests/test_spaces.py,0,"b'import numpy as np\nimport pytest\nimport gym\n\nfrom stable_baselines3 import SAC, TD3\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\n\nclass DummyMultiDiscreteSpace(gym.Env):\n    def __init__(self, nvec):\n        super(DummyMultiDiscreteSpace, self).__init__()\n        self.observation_space = gym.spaces.MultiDiscrete(nvec)\n        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n\n    def reset(self):\n        return self.observation_space.sample()\n\n    def step(self, action):\n        return self.observation_space.sample(), 0.0, False, {}\n\n\nclass DummyMultiBinary(gym.Env):\n    def __init__(self, n):\n        super(DummyMultiBinary, self).__init__()\n        self.observation_space = gym.spaces.MultiBinary(n)\n        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n\n    def reset(self):\n        return self.observation_space.sample()\n\n    def step(self, action):\n        return self.observation_space.sample(), 0.0, False, {}\n\n\n@pytest.mark.parametrize(""model_class"", [SAC, TD3])\n@pytest.mark.parametrize(""env"", [DummyMultiDiscreteSpace([4, 3]), DummyMultiBinary(8)])\ndef test_identity_spaces(model_class, env):\n    """"""\n    Additional tests for SAC/TD3 to check observation space support\n    for MultiDiscrete and MultiBinary.\n    """"""\n    env = gym.wrappers.TimeLimit(env, max_episode_steps=100)\n\n    model = model_class(""MlpPolicy"", env, gamma=0.5, seed=1, policy_kwargs=dict(net_arch=[64]))\n    model.learn(total_timesteps=500)\n\n    evaluate_policy(model, env, n_eval_episodes=5)\n'"
tests/test_tensorboard.py,0,"b'import os\nimport pytest\n\nfrom stable_baselines3 import A2C, PPO, SAC, TD3\n\nMODEL_DICT = {\n    \'a2c\': (A2C, \'CartPole-v1\'),\n    \'ppo\': (PPO, \'CartPole-v1\'),\n    \'sac\': (SAC, \'Pendulum-v0\'),\n    \'td3\': (TD3, \'Pendulum-v0\'),\n}\n\nN_STEPS = 100\n\n\n@pytest.mark.parametrize(""model_name"", MODEL_DICT.keys())\ndef test_tensorboard(tmp_path, model_name):\n    # Skip if no tensorboard installed\n    pytest.importorskip(""tensorboard"")\n\n    logname = model_name.upper()\n    algo, env_id = MODEL_DICT[model_name]\n    model = algo(\'MlpPolicy\', env_id, verbose=1, tensorboard_log=tmp_path)\n    model.learn(N_STEPS)\n    model.learn(N_STEPS, reset_num_timesteps=False)\n\n    assert os.path.isdir(tmp_path / str(logname + ""_1""))\n    assert not os.path.isdir(tmp_path / str(logname + ""_2""))\n\n    logname = ""tb_multiple_runs_"" + model_name\n    model.learn(N_STEPS, tb_log_name=logname)\n    model.learn(N_STEPS, tb_log_name=logname)\n\n    assert os.path.isdir(tmp_path / str(logname + ""_1""))\n    # Check that the log dir name increments correctly\n    assert os.path.isdir(tmp_path / str(logname + ""_2""))\n'"
tests/test_utils.py,0,"b'import os\nimport shutil\n\nimport pytest\nimport gym\nimport numpy as np\n\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.cmd_util import make_vec_env, make_atari_env\nfrom stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\nfrom stable_baselines3.common.noise import (\n    VectorizedActionNoise, OrnsteinUhlenbeckActionNoise, ActionNoise)\n\n\n@pytest.mark.parametrize(""env_id"", [\'CartPole-v1\', lambda: gym.make(\'CartPole-v1\')])\n@pytest.mark.parametrize(""n_envs"", [1, 2])\n@pytest.mark.parametrize(""vec_env_cls"", [None, SubprocVecEnv])\n@pytest.mark.parametrize(""wrapper_class"", [None, gym.wrappers.TimeLimit])\ndef test_make_vec_env(env_id, n_envs, vec_env_cls, wrapper_class):\n    env = make_vec_env(env_id, n_envs, vec_env_cls=vec_env_cls,\n                       wrapper_class=wrapper_class, monitor_dir=None, seed=0)\n\n    assert env.num_envs == n_envs\n\n    if vec_env_cls is None:\n        assert isinstance(env, DummyVecEnv)\n        if wrapper_class is not None:\n            assert isinstance(env.envs[0], wrapper_class)\n        else:\n            assert isinstance(env.envs[0], Monitor)\n    else:\n        assert isinstance(env, SubprocVecEnv)\n    # Kill subprocesses\n    env.close()\n\n\n@pytest.mark.parametrize(""env_id"", [\'BreakoutNoFrameskip-v4\'])\n@pytest.mark.parametrize(""n_envs"", [1, 2])\n@pytest.mark.parametrize(""wrapper_kwargs"", [None, dict(clip_reward=False, screen_size=60)])\ndef test_make_atari_env(env_id, n_envs, wrapper_kwargs):\n    env_id = \'BreakoutNoFrameskip-v4\'\n    env = make_atari_env(env_id, n_envs,\n                         wrapper_kwargs=wrapper_kwargs, monitor_dir=None, seed=0)\n\n    assert env.num_envs == n_envs\n\n    obs = env.reset()\n\n    new_obs, reward, _, _ = env.step([env.action_space.sample() for _ in range(n_envs)])\n\n    assert obs.shape == new_obs.shape\n\n    # Wrapped into DummyVecEnv\n    wrapped_atari_env = env.envs[0]\n    if wrapper_kwargs is not None:\n        assert obs.shape == (n_envs, 60, 60, 1)\n        assert wrapped_atari_env.observation_space.shape == (60, 60, 1)\n        assert wrapped_atari_env.clip_reward is False\n    else:\n        assert obs.shape == (n_envs, 84, 84, 1)\n        assert wrapped_atari_env.observation_space.shape == (84, 84, 1)\n        assert wrapped_atari_env.clip_reward is True\n        assert np.max(np.abs(reward)) < 1.0\n\n\ndef test_custom_vec_env(tmp_path):\n    """"""\n    Stand alone test for a special case (passing a custom VecEnv class) to avoid doubling the number of tests.\n    """"""\n    monitor_dir = tmp_path / \'test_make_vec_env/\'\n    env = make_vec_env(\'CartPole-v1\', n_envs=1,\n                       monitor_dir=monitor_dir, seed=0,\n                       vec_env_cls=SubprocVecEnv, vec_env_kwargs={\'start_method\': None})\n\n    assert env.num_envs == 1\n    assert isinstance(env, SubprocVecEnv)\n    assert os.path.isdir(monitor_dir)\n    # Kill subprocess\n    env.close()\n    # Cleanup folder\n    shutil.rmtree(monitor_dir)\n\n    # This should fail because DummyVecEnv does not have any keyword argument\n    with pytest.raises(TypeError):\n        make_vec_env(\'CartPole-v1\', n_envs=1, vec_env_kwargs={\'dummy\': False})\n\n\ndef test_evaluate_policy():\n    model = A2C(\'MlpPolicy\', \'Pendulum-v0\', seed=0)\n    n_steps_per_episode, n_eval_episodes = 200, 2\n    model.n_callback_calls = 0\n\n    def dummy_callback(locals_, _globals):\n        locals_[\'model\'].n_callback_calls += 1\n\n    _, episode_lengths = evaluate_policy(model, model.get_env(), n_eval_episodes, deterministic=True,\n                                         render=False, callback=dummy_callback, reward_threshold=None,\n                                         return_episode_rewards=True)\n\n    n_steps = sum(episode_lengths)\n    assert n_steps == n_steps_per_episode * n_eval_episodes\n    assert n_steps == model.n_callback_calls\n\n    # Reaching a mean reward of zero is impossible with the Pendulum env\n    with pytest.raises(AssertionError):\n        evaluate_policy(model, model.get_env(), n_eval_episodes, reward_threshold=0.0)\n\n    episode_rewards, _ = evaluate_policy(model, model.get_env(), n_eval_episodes, return_episode_rewards=True)\n    assert len(episode_rewards) == n_eval_episodes\n\n\ndef test_vec_noise():\n    num_envs = 4\n    num_actions = 10\n    mu = np.zeros(num_actions)\n    sigma = np.ones(num_actions) * 0.4\n    base: ActionNoise = OrnsteinUhlenbeckActionNoise(mu, sigma)\n    with pytest.raises(ValueError):\n        vec = VectorizedActionNoise(base, -1)\n    with pytest.raises(ValueError):\n        vec = VectorizedActionNoise(base, None)\n    with pytest.raises(ValueError):\n        vec = VectorizedActionNoise(base, ""whatever"")\n\n    vec = VectorizedActionNoise(base, num_envs)\n    assert vec.n_envs == num_envs\n    assert vec().shape == (num_envs, num_actions)\n    assert not (vec() == base()).all()\n    with pytest.raises(ValueError):\n        vec = VectorizedActionNoise(None, num_envs)\n    with pytest.raises(TypeError):\n        vec = VectorizedActionNoise(12, num_envs)\n    with pytest.raises(AssertionError):\n        vec.noises = []\n    with pytest.raises(TypeError):\n        vec.noises = None\n    with pytest.raises(ValueError):\n        vec.noises = [None] * vec.n_envs\n    with pytest.raises(AssertionError):\n        vec.noises = [base] * (num_envs - 1)\n    assert all(isinstance(noise, type(base)) for noise in vec.noises)\n    assert len(vec.noises) == num_envs\n'"
tests/test_vec_check_nan.py,0,"b'import gym\nfrom gym import spaces\nimport numpy as np\n\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecCheckNan\n\n\nclass NanAndInfEnv(gym.Env):\n    """"""Custom Environment that raised NaNs and Infs""""""\n    metadata = {\'render.modes\': [\'human\']}\n\n    def __init__(self):\n        super(NanAndInfEnv, self).__init__()\n        self.action_space = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float64)\n        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float64)\n\n    @staticmethod\n    def step(action):\n        if np.all(np.array(action) > 0):\n            obs = float(\'NaN\')\n        elif np.all(np.array(action) < 0):\n            obs = float(\'inf\')\n        else:\n            obs = 0\n        return [obs], 0.0, False, {}\n\n    @staticmethod\n    def reset():\n        return [0.0]\n\n    def render(self, mode=\'human\', close=False):\n        pass\n\n\ndef test_check_nan():\n    """"""Test VecCheckNan Object""""""\n\n    env = DummyVecEnv([NanAndInfEnv])\n    env = VecCheckNan(env, raise_exception=True)\n\n    env.step([[0]])\n\n    try:\n        env.step([[float(\'NaN\')]])\n    except ValueError:\n        pass\n    else:\n        assert False\n\n    try:\n        env.step([[float(\'inf\')]])\n    except ValueError:\n        pass\n    else:\n        assert False\n\n    try:\n        env.step([[-1]])\n    except ValueError:\n        pass\n    else:\n        assert False\n\n    try:\n        env.step([[1]])\n    except ValueError:\n        pass\n    else:\n        assert False\n\n    env.step(np.array([[0, 1], [0, 1]]))\n'"
tests/test_vec_envs.py,0,"b'import collections\nimport functools\nimport itertools\nimport multiprocessing\n\nimport pytest\nimport gym\nimport numpy as np\n\nfrom stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecNormalize, VecFrameStack\n\nN_ENVS = 3\nVEC_ENV_CLASSES = [DummyVecEnv, SubprocVecEnv]\nVEC_ENV_WRAPPERS = [None, VecNormalize, VecFrameStack]\n\n\nclass CustomGymEnv(gym.Env):\n    def __init__(self, space):\n        """"""\n        Custom gym environment for testing purposes\n        """"""\n        self.action_space = space\n        self.observation_space = space\n        self.current_step = 0\n        self.ep_length = 4\n\n    def reset(self):\n        self.current_step = 0\n        self._choose_next_state()\n        return self.state\n\n    def step(self, action):\n        reward = 1\n        self._choose_next_state()\n        self.current_step += 1\n        done = self.current_step >= self.ep_length\n        return self.state, reward, done, {}\n\n    def _choose_next_state(self):\n        self.state = self.observation_space.sample()\n\n    def render(self, mode=\'human\'):\n        if mode == \'rgb_array\':\n            return np.zeros((4, 4, 3))\n\n    def seed(self, seed=None):\n        pass\n\n    @staticmethod\n    def custom_method(dim_0=1, dim_1=1):\n        """"""\n        Dummy method to test call to custom method\n        from VecEnv\n\n        :param dim_0: (int)\n        :param dim_1: (int)\n        :return: (np.ndarray)\n        """"""\n        return np.ones((dim_0, dim_1))\n\n\n@pytest.mark.parametrize(\'vec_env_class\', VEC_ENV_CLASSES)\n@pytest.mark.parametrize(\'vec_env_wrapper\', VEC_ENV_WRAPPERS)\ndef test_vecenv_custom_calls(vec_env_class, vec_env_wrapper):\n    """"""Test access to methods/attributes of vectorized environments""""""\n\n    def make_env():\n        return CustomGymEnv(gym.spaces.Box(low=np.zeros(2), high=np.ones(2)))\n\n    vec_env = vec_env_class([make_env for _ in range(N_ENVS)])\n\n    if vec_env_wrapper is not None:\n        if vec_env_wrapper == VecFrameStack:\n            vec_env = vec_env_wrapper(vec_env, n_stack=2)\n        else:\n            vec_env = vec_env_wrapper(vec_env)\n\n    # Test seed method\n    vec_env.seed(0)\n    # Test render method call\n    # vec_env.render()  # we need a X server  to test the ""human"" mode\n    vec_env.render(mode=\'rgb_array\')\n    env_method_results = vec_env.env_method(\'custom_method\', 1, indices=None, dim_1=2)\n    setattr_results = []\n    # Set current_step to an arbitrary value\n    for env_idx in range(N_ENVS):\n        setattr_results.append(vec_env.set_attr(\'current_step\', env_idx, indices=env_idx))\n    # Retrieve the value for each environment\n    getattr_results = vec_env.get_attr(\'current_step\')\n\n    assert len(env_method_results) == N_ENVS\n    assert len(setattr_results) == N_ENVS\n    assert len(getattr_results) == N_ENVS\n\n    for env_idx in range(N_ENVS):\n        assert (env_method_results[env_idx] == np.ones((1, 2))).all()\n        assert setattr_results[env_idx] is None\n        assert getattr_results[env_idx] == env_idx\n\n    # Call env_method on a subset of the VecEnv\n    env_method_subset = vec_env.env_method(\'custom_method\', 1, indices=[0, 2], dim_1=3)\n    assert (env_method_subset[0] == np.ones((1, 3))).all()\n    assert (env_method_subset[1] == np.ones((1, 3))).all()\n    assert len(env_method_subset) == 2\n\n    # Test to change value for all the environments\n    setattr_result = vec_env.set_attr(\'current_step\', 42, indices=None)\n    getattr_result = vec_env.get_attr(\'current_step\')\n    assert setattr_result is None\n    assert getattr_result == [42 for _ in range(N_ENVS)]\n\n    # Additional tests for setattr that does not affect all the environments\n    vec_env.reset()\n    setattr_result = vec_env.set_attr(\'current_step\', 12, indices=[0, 1])\n    getattr_result = vec_env.get_attr(\'current_step\')\n    getattr_result_subset = vec_env.get_attr(\'current_step\', indices=[0, 1])\n    assert setattr_result is None\n    assert getattr_result == [12 for _ in range(2)] + [0 for _ in range(N_ENVS - 2)]\n    assert getattr_result_subset == [12, 12]\n    assert vec_env.get_attr(\'current_step\', indices=[0, 2]) == [12, 0]\n\n    vec_env.reset()\n    # Change value only for first and last environment\n    setattr_result = vec_env.set_attr(\'current_step\', 12, indices=[0, -1])\n    getattr_result = vec_env.get_attr(\'current_step\')\n    assert setattr_result is None\n    assert getattr_result == [12] + [0 for _ in range(N_ENVS - 2)] + [12]\n    assert vec_env.get_attr(\'current_step\', indices=[-1]) == [12]\n\n    vec_env.close()\n\n\nclass StepEnv(gym.Env):\n    def __init__(self, max_steps):\n        """"""Gym environment for testing that terminal observation is inserted\n        correctly.""""""\n        self.action_space = gym.spaces.Discrete(2)\n        self.observation_space = gym.spaces.Box(np.array([0]), np.array([999]),\n                                                dtype=\'int\')\n        self.max_steps = max_steps\n        self.current_step = 0\n\n    def reset(self):\n        self.current_step = 0\n        return np.array([self.current_step], dtype=\'int\')\n\n    def step(self, action):\n        prev_step = self.current_step\n        self.current_step += 1\n        done = self.current_step >= self.max_steps\n        return np.array([prev_step], dtype=\'int\'), 0.0, done, {}\n\n\n@pytest.mark.parametrize(\'vec_env_class\', VEC_ENV_CLASSES)\n@pytest.mark.parametrize(\'vec_env_wrapper\', VEC_ENV_WRAPPERS)\ndef test_vecenv_terminal_obs(vec_env_class, vec_env_wrapper):\n    """"""Test that \'terminal_observation\' gets added to info dict upon\n    termination.""""""\n    step_nums = [i + 5 for i in range(N_ENVS)]\n    vec_env = vec_env_class([functools.partial(StepEnv, n) for n in step_nums])\n\n    if vec_env_wrapper is not None:\n        if vec_env_wrapper == VecFrameStack:\n            vec_env = vec_env_wrapper(vec_env, n_stack=2)\n        else:\n            vec_env = vec_env_wrapper(vec_env)\n\n    zero_acts = np.zeros((N_ENVS,), dtype=\'int\')\n    prev_obs_b = vec_env.reset()\n    for step_num in range(1, max(step_nums) + 1):\n        obs_b, _, done_b, info_b = vec_env.step(zero_acts)\n        assert len(obs_b) == N_ENVS\n        assert len(done_b) == N_ENVS\n        assert len(info_b) == N_ENVS\n        env_iter = zip(prev_obs_b, obs_b, done_b, info_b, step_nums)\n        for prev_obs, obs, done, info, final_step_num in env_iter:\n            assert done == (step_num == final_step_num)\n            if not done:\n                assert \'terminal_observation\' not in info\n            else:\n                terminal_obs = info[\'terminal_observation\']\n\n                # do some rough ordering checks that should work for all\n                # wrappers, including VecNormalize\n                assert np.all(prev_obs < terminal_obs)\n                assert np.all(obs < prev_obs)\n\n                if not isinstance(vec_env, VecNormalize):\n                    # more precise tests that we can\'t do with VecNormalize\n                    # (which changes observation values)\n                    assert np.all(prev_obs + 1 == terminal_obs)\n                    assert np.all(obs == 0)\n\n        prev_obs_b = obs_b\n\n    vec_env.close()\n\n\nSPACES = collections.OrderedDict([\n    (\'discrete\', gym.spaces.Discrete(2)),\n    (\'multidiscrete\', gym.spaces.MultiDiscrete([2, 3])),\n    (\'multibinary\', gym.spaces.MultiBinary(3)),\n    (\'continuous\', gym.spaces.Box(low=np.zeros(2), high=np.ones(2))),\n])\n\n\ndef check_vecenv_spaces(vec_env_class, space, obs_assert):\n    """"""Helper method to check observation spaces in vectorized environments.""""""\n\n    def make_env():\n        return CustomGymEnv(space)\n\n    vec_env = vec_env_class([make_env for _ in range(N_ENVS)])\n    obs = vec_env.reset()\n    obs_assert(obs)\n\n    dones = [False] * N_ENVS\n    while not any(dones):\n        actions = [vec_env.action_space.sample() for _ in range(N_ENVS)]\n        obs, _rews, dones, _infos = vec_env.step(actions)\n        obs_assert(obs)\n    vec_env.close()\n\n\ndef check_vecenv_obs(obs, space):\n    """"""Helper method to check observations from multiple environments each belong to\n       the appropriate observation space.""""""\n    assert obs.shape[0] == N_ENVS\n    for value in obs:\n        assert space.contains(value)\n\n\n@pytest.mark.parametrize(\'vec_env_class,space\', itertools.product(VEC_ENV_CLASSES, SPACES.values()))\ndef test_vecenv_single_space(vec_env_class, space):\n    def obs_assert(obs):\n        return check_vecenv_obs(obs, space)\n\n    check_vecenv_spaces(vec_env_class, space, obs_assert)\n\n\nclass _UnorderedDictSpace(gym.spaces.Dict):\n    """"""Like DictSpace, but returns an unordered dict when sampling.""""""\n\n    def sample(self):\n        return dict(super().sample())\n\n\n@pytest.mark.parametrize(\'vec_env_class\', VEC_ENV_CLASSES)\ndef test_vecenv_dict_spaces(vec_env_class):\n    """"""Test dictionary observation spaces with vectorized environments.""""""\n    space = gym.spaces.Dict(SPACES)\n\n    def obs_assert(obs):\n        assert isinstance(obs, collections.OrderedDict)\n        assert obs.keys() == space.spaces.keys()\n        for key, values in obs.items():\n            check_vecenv_obs(values, space.spaces[key])\n\n    check_vecenv_spaces(vec_env_class, space, obs_assert)\n\n    unordered_space = _UnorderedDictSpace(SPACES)\n    # Check that vec_env_class can accept unordered dict observations (and convert to OrderedDict)\n    check_vecenv_spaces(vec_env_class, unordered_space, obs_assert)\n\n\n@pytest.mark.parametrize(\'vec_env_class\', VEC_ENV_CLASSES)\ndef test_vecenv_tuple_spaces(vec_env_class):\n    """"""Test tuple observation spaces with vectorized environments.""""""\n    space = gym.spaces.Tuple(tuple(SPACES.values()))\n\n    def obs_assert(obs):\n        assert isinstance(obs, tuple)\n        assert len(obs) == len(space.spaces)\n        for values, inner_space in zip(obs, space.spaces):\n            check_vecenv_obs(values, inner_space)\n\n    return check_vecenv_spaces(vec_env_class, space, obs_assert)\n\n\ndef test_subproc_start_method():\n    start_methods = [None]\n    # Only test thread-safe methods. Others may deadlock tests! (gh/428)\n    # Note: adding unsafe `fork` method as we are now using PyTorch\n    all_methods = {\'forkserver\', \'spawn\', \'fork\'}\n    available_methods = multiprocessing.get_all_start_methods()\n    start_methods += list(all_methods.intersection(available_methods))\n    space = gym.spaces.Discrete(2)\n\n    def obs_assert(obs):\n        return check_vecenv_obs(obs, space)\n\n    for start_method in start_methods:\n        vec_env_class = functools.partial(SubprocVecEnv, start_method=start_method)\n        check_vecenv_spaces(vec_env_class, space, obs_assert)\n\n    with pytest.raises(ValueError, match=""cannot find context for \'illegal_method\'""):\n        vec_env_class = functools.partial(SubprocVecEnv, start_method=\'illegal_method\')\n        check_vecenv_spaces(vec_env_class, space, obs_assert)\n\n\nclass CustomWrapperA(VecNormalize):\n    def __init__(self, venv):\n        VecNormalize.__init__(self, venv)\n        self.var_a = \'a\'\n\n\nclass CustomWrapperB(VecNormalize):\n    def __init__(self, venv):\n        VecNormalize.__init__(self, venv)\n        self.var_b = \'b\'\n\n    def func_b(self):\n        return self.var_b\n\n    def name_test(self):\n        return self.__class__\n\n\nclass CustomWrapperBB(CustomWrapperB):\n    def __init__(self, venv):\n        CustomWrapperB.__init__(self, venv)\n        self.var_bb = \'bb\'\n\n\ndef test_vecenv_wrapper_getattr():\n    def make_env():\n        return CustomGymEnv(gym.spaces.Box(low=np.zeros(2), high=np.ones(2)))\n\n    vec_env = DummyVecEnv([make_env for _ in range(N_ENVS)])\n    wrapped = CustomWrapperA(CustomWrapperBB(vec_env))\n    assert wrapped.var_a == \'a\'\n    assert wrapped.var_b == \'b\'\n    assert wrapped.var_bb == \'bb\'\n    assert wrapped.func_b() == \'b\'\n    assert wrapped.name_test() == CustomWrapperBB\n\n    double_wrapped = CustomWrapperA(CustomWrapperB(wrapped))\n    _ = double_wrapped.var_a  # should not raise as it is directly defined here\n    with pytest.raises(AttributeError):  # should raise due to ambiguity\n        _ = double_wrapped.var_b\n    with pytest.raises(AttributeError):  # should raise as does not exist\n        _ = double_wrapped.nonexistent_attribute\n'"
tests/test_vec_normalize.py,0,"b'import gym\nimport pytest\nimport numpy as np\n\nfrom stable_baselines3.common.running_mean_std import RunningMeanStd\nfrom stable_baselines3.common.vec_env import (DummyVecEnv, VecNormalize, VecFrameStack, sync_envs_normalization,\n                                              unwrap_vec_normalize)\nfrom stable_baselines3 import SAC, TD3\n\nENV_ID = \'Pendulum-v0\'\n\n\ndef make_env():\n    return gym.make(ENV_ID)\n\n\ndef check_rms_equal(rmsa, rmsb):\n    assert np.all(rmsa.mean == rmsb.mean)\n    assert np.all(rmsa.var == rmsb.var)\n    assert np.all(rmsa.count == rmsb.count)\n\n\ndef check_vec_norm_equal(norma, normb):\n    assert norma.observation_space == normb.observation_space\n    assert norma.action_space == normb.action_space\n    assert norma.num_envs == normb.num_envs\n\n    check_rms_equal(norma.obs_rms, normb.obs_rms)\n    check_rms_equal(norma.ret_rms, normb.ret_rms)\n    assert norma.clip_obs == normb.clip_obs\n    assert norma.clip_reward == normb.clip_reward\n    assert norma.norm_obs == normb.norm_obs\n    assert norma.norm_reward == normb.norm_reward\n\n    assert np.all(norma.ret == normb.ret)\n    assert norma.gamma == normb.gamma\n    assert norma.epsilon == normb.epsilon\n    assert norma.training == normb.training\n\n\ndef _make_warmstart_cartpole():\n    """"""Warm-start VecNormalize by stepping through CartPole""""""\n    venv = DummyVecEnv([lambda: gym.make(""CartPole-v1"")])\n    venv = VecNormalize(venv)\n    venv.reset()\n    venv.get_original_obs()\n\n    for _ in range(100):\n        actions = [venv.action_space.sample()]\n        venv.step(actions)\n    return venv\n\n\ndef test_runningmeanstd():\n    """"""Test RunningMeanStd object""""""\n    for (x_1, x_2, x_3) in [\n            (np.random.randn(3), np.random.randn(4), np.random.randn(5)),\n            (np.random.randn(3, 2), np.random.randn(4, 2), np.random.randn(5, 2))]:\n        rms = RunningMeanStd(epsilon=0.0, shape=x_1.shape[1:])\n\n        x_cat = np.concatenate([x_1, x_2, x_3], axis=0)\n        moments_1 = [x_cat.mean(axis=0), x_cat.var(axis=0)]\n        rms.update(x_1)\n        rms.update(x_2)\n        rms.update(x_3)\n        moments_2 = [rms.mean, rms.var]\n\n        assert np.allclose(moments_1, moments_2)\n\n\ndef test_vec_env(tmpdir):\n    """"""Test VecNormalize Object""""""\n    clip_obs = 0.5\n    clip_reward = 5.0\n\n    orig_venv = DummyVecEnv([make_env])\n    norm_venv = VecNormalize(orig_venv, norm_obs=True, norm_reward=True, clip_obs=clip_obs, clip_reward=clip_reward)\n    _, done = norm_venv.reset(), [False]\n    while not done[0]:\n        actions = [norm_venv.action_space.sample()]\n        obs, rew, done, _ = norm_venv.step(actions)\n        assert np.max(np.abs(obs)) <= clip_obs\n        assert np.max(np.abs(rew)) <= clip_reward\n\n    path = str(tmpdir.join(""vec_normalize""))\n    norm_venv.save(path)\n    deserialized = VecNormalize.load(path, venv=orig_venv)\n    check_vec_norm_equal(norm_venv, deserialized)\n\n\ndef test_get_original():\n    venv = _make_warmstart_cartpole()\n    for _ in range(3):\n        actions = [venv.action_space.sample()]\n        obs, rewards, _, _ = venv.step(actions)\n        obs = obs[0]\n        orig_obs = venv.get_original_obs()[0]\n        rewards = rewards[0]\n        orig_rewards = venv.get_original_reward()[0]\n\n        assert np.all(orig_rewards == 1)\n        assert orig_obs.shape == obs.shape\n        assert orig_rewards.dtype == rewards.dtype\n        assert not np.array_equal(orig_obs, obs)\n        assert not np.array_equal(orig_rewards, rewards)\n        np.testing.assert_allclose(venv.normalize_obs(orig_obs), obs)\n        np.testing.assert_allclose(venv.normalize_reward(orig_rewards), rewards)\n\n\ndef test_normalize_external():\n    venv = _make_warmstart_cartpole()\n\n    rewards = np.array([1, 1])\n    norm_rewards = venv.normalize_reward(rewards)\n    assert norm_rewards.shape == rewards.shape\n    # Episode return is almost always >= 1 in CartPole. So reward should shrink.\n    assert np.all(norm_rewards < 1)\n\n\n@pytest.mark.parametrize(""model_class"", [SAC, TD3])\ndef test_offpolicy_normalization(model_class):\n    env = DummyVecEnv([make_env])\n    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10., clip_reward=10.)\n\n    eval_env = DummyVecEnv([make_env])\n    eval_env = VecNormalize(eval_env, training=False, norm_obs=True, norm_reward=False, clip_obs=10., clip_reward=10.)\n\n    model = model_class(\'MlpPolicy\', env, verbose=1)\n    model.learn(total_timesteps=1000, eval_env=eval_env, eval_freq=500)\n    # Check getter\n    assert isinstance(model.get_vec_normalize_env(), VecNormalize)\n\n\ndef test_sync_vec_normalize():\n    env = DummyVecEnv([make_env])\n\n    assert unwrap_vec_normalize(env) is None\n\n    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10., clip_reward=10.)\n\n    assert isinstance(unwrap_vec_normalize(env), VecNormalize)\n\n    env = VecFrameStack(env, 1)\n\n    assert isinstance(unwrap_vec_normalize(env), VecNormalize)\n\n    eval_env = DummyVecEnv([make_env])\n    eval_env = VecNormalize(eval_env, training=False, norm_obs=True, norm_reward=True, clip_obs=10., clip_reward=10.)\n    eval_env = VecFrameStack(eval_env, 1)\n\n    env.reset()\n    # Initialize running mean\n    for _ in range(100):\n        env.step([env.action_space.sample()])\n\n    obs = env.reset()\n    original_obs = env.get_original_obs()\n    dummy_rewards = np.random.rand(10)\n    # Normalization must be different\n    assert not np.allclose(obs, eval_env.normalize_obs(original_obs))\n\n    sync_envs_normalization(env, eval_env)\n\n    # Now they must be synced\n    assert np.allclose(obs, eval_env.normalize_obs(original_obs))\n    assert np.allclose(env.normalize_reward(dummy_rewards), eval_env.normalize_reward(dummy_rewards))\n'"
stable_baselines3/a2c/__init__.py,0,"b'from stable_baselines3.a2c.a2c import A2C\nfrom stable_baselines3.a2c.policies import MlpPolicy, CnnPolicy\n'"
stable_baselines3/a2c/a2c.py,1,"b'import torch as th\nimport torch.nn.functional as F\nfrom gym import spaces\nfrom typing import Type, Union, Callable, Optional, Dict, Any\n\nfrom stable_baselines3.common import logger\nfrom stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\nfrom stable_baselines3.common.type_aliases import GymEnv, MaybeCallback\nfrom stable_baselines3.common.utils import explained_variance\nfrom stable_baselines3.common.policies import ActorCriticPolicy\n\n\nclass A2C(OnPolicyAlgorithm):\n    """"""\n    Advantage Actor Critic (A2C)\n\n    Paper: https://arxiv.org/abs/1602.01783\n    Code: This implementation borrows code from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\n    and Stable Baselines (https://github.com/hill-a/stable-baselines)\n\n    Introduction to A2C: https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752\n\n    :param policy: (ActorCriticPolicy or str) The policy model to use (MlpPolicy, CnnPolicy, ...)\n    :param env: (Gym environment or str) The environment to learn from (if registered in Gym, can be str)\n    :param learning_rate: (float or callable) The learning rate, it can be a function\n    :param n_steps: (int) The number of steps to run for each environment per update\n        (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n    :param gamma: (float) Discount factor\n    :param gae_lambda: (float) Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n        Equivalent to classic advantage when set to 1.\n    :param ent_coef: (float) Entropy coefficient for the loss calculation\n    :param vf_coef: (float) Value function coefficient for the loss calculation\n    :param max_grad_norm: (float) The maximum value for the gradient clipping\n    :param rms_prop_eps: (float) RMSProp epsilon. It stabilizes square root computation in denominator\n        of RMSProp update\n    :param use_rms_prop: (bool) Whether to use RMSprop (default) or Adam as optimizer\n    :param use_sde: (bool) Whether to use generalized State Dependent Exploration (gSDE)\n        instead of action noise exploration (default: False)\n    :param sde_sample_freq: (int) Sample a new noise matrix every n steps when using gSDE\n        Default: -1 (only sample at the beginning of the rollout)\n    :param normalize_advantage: (bool) Whether to normalize or not the advantage\n    :param tensorboard_log: (str) the log location for tensorboard (if None, no logging)\n    :param create_eval_env: (bool) Whether to create a second environment that will be\n        used for evaluating the agent periodically. (Only available when passing string for the environment)\n    :param policy_kwargs: (dict) additional arguments to be passed to the policy on creation\n    :param verbose: (int) the verbosity level: 0 no output, 1 info, 2 debug\n    :param seed: (int) Seed for the pseudo random generators\n    :param device: (str or th.device) Device (cpu, cuda, ...) on which the code should be run.\n        Setting it to auto, the code will be run on the GPU if possible.\n    :param _init_setup_model: (bool) Whether or not to build the network at the creation of the instance\n    """"""\n\n    def __init__(self, policy: Union[str, Type[ActorCriticPolicy]],\n                 env: Union[GymEnv, str],\n                 learning_rate: Union[float, Callable] = 7e-4,\n                 n_steps: int = 5,\n                 gamma: float = 0.99,\n                 gae_lambda: float = 1.0,\n                 ent_coef: float = 0.0,\n                 vf_coef: float = 0.5,\n                 max_grad_norm: float = 0.5,\n                 rms_prop_eps: float = 1e-5,\n                 use_rms_prop: bool = True,\n                 use_sde: bool = False,\n                 sde_sample_freq: int = -1,\n                 normalize_advantage: bool = False,\n                 tensorboard_log: Optional[str] = None,\n                 create_eval_env: bool = False,\n                 policy_kwargs: Optional[Dict[str, Any]] = None,\n                 verbose: int = 0,\n                 seed: Optional[int] = None,\n                 device: Union[th.device, str] = \'auto\',\n                 _init_setup_model: bool = True):\n\n        super(A2C, self).__init__(policy, env, learning_rate=learning_rate,\n                                  n_steps=n_steps, gamma=gamma, gae_lambda=gae_lambda,\n                                  ent_coef=ent_coef, vf_coef=vf_coef, max_grad_norm=max_grad_norm,\n                                  use_sde=use_sde, sde_sample_freq=sde_sample_freq,\n                                  tensorboard_log=tensorboard_log, policy_kwargs=policy_kwargs,\n                                  verbose=verbose, device=device, create_eval_env=create_eval_env,\n                                  seed=seed, _init_setup_model=False)\n\n        self.normalize_advantage = normalize_advantage\n\n        # Update optimizer inside the policy if we want to use RMSProp\n        # (original implementation) rather than Adam\n        if use_rms_prop and \'optimizer_class\' not in self.policy_kwargs:\n            self.policy_kwargs[\'optimizer_class\'] = th.optim.RMSprop\n            self.policy_kwargs[\'optimizer_kwargs\'] = dict(alpha=0.99, eps=rms_prop_eps,\n                                                          weight_decay=0)\n\n        if _init_setup_model:\n            self._setup_model()\n\n    def train(self) -> None:\n        """"""\n        Update policy using the currently gathered\n        rollout buffer (one gradient step over whole data).\n        """"""\n        # Update optimizer learning rate\n        self._update_learning_rate(self.policy.optimizer)\n\n        for rollout_data in self.rollout_buffer.get(batch_size=None):\n\n            actions = rollout_data.actions\n            if isinstance(self.action_space, spaces.Discrete):\n                # Convert discrete action from float to long\n                actions = actions.long().flatten()\n\n            # TODO: avoid second computation of everything because of the gradient\n            values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)\n            values = values.flatten()\n\n            # Normalize advantage (not present in the original implementation)\n            advantages = rollout_data.advantages\n            if self.normalize_advantage:\n                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n            # Policy gradient loss\n            policy_loss = -(advantages * log_prob).mean()\n\n            # Value loss using the TD(gae_lambda) target\n            value_loss = F.mse_loss(rollout_data.returns, values)\n\n            # Entropy loss favor exploration\n            if entropy is None:\n                # Approximate entropy when no analytical form\n                entropy_loss = -log_prob.mean()\n            else:\n                entropy_loss = -th.mean(entropy)\n\n            loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n\n            # Optimization step\n            self.policy.optimizer.zero_grad()\n            loss.backward()\n\n            # Clip grad norm\n            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n            self.policy.optimizer.step()\n\n        explained_var = explained_variance(self.rollout_buffer.returns.flatten(),\n                                           self.rollout_buffer.values.flatten())\n\n        self._n_updates += 1\n        logger.record(""train/n_updates"", self._n_updates, exclude=""tensorboard"")\n        logger.record(""train/explained_variance"", explained_var)\n        logger.record(""train/entropy_loss"", entropy_loss.item())\n        logger.record(""train/policy_loss"", policy_loss.item())\n        logger.record(""train/value_loss"", value_loss.item())\n        if hasattr(self.policy, \'log_std\'):\n            logger.record(""train/std"", th.exp(self.policy.log_std).mean().item())\n\n    def learn(self,\n              total_timesteps: int,\n              callback: MaybeCallback = None,\n              log_interval: int = 100,\n              eval_env: Optional[GymEnv] = None,\n              eval_freq: int = -1,\n              n_eval_episodes: int = 5,\n              tb_log_name: str = ""A2C"",\n              eval_log_path: Optional[str] = None,\n              reset_num_timesteps: bool = True) -> \'A2C\':\n\n        return super(A2C, self).learn(total_timesteps=total_timesteps, callback=callback,\n                                      log_interval=log_interval, eval_env=eval_env, eval_freq=eval_freq,\n                                      n_eval_episodes=n_eval_episodes, tb_log_name=tb_log_name,\n                                      eval_log_path=eval_log_path, reset_num_timesteps=reset_num_timesteps)\n'"
stable_baselines3/a2c/policies.py,0,"b'# This file is here just to define MlpPolicy/CnnPolicy\n# that work for A2C\nfrom stable_baselines3.common.policies import ActorCriticPolicy, ActorCriticCnnPolicy, register_policy\n\nMlpPolicy = ActorCriticPolicy\nCnnPolicy = ActorCriticCnnPolicy\n\nregister_policy(""MlpPolicy"", ActorCriticPolicy)\nregister_policy(""CnnPolicy"", ActorCriticCnnPolicy)\n'"
stable_baselines3/common/__init__.py,0,"b'from stable_baselines3.common.cmd_util import make_vec_env, make_atari_env\nfrom stable_baselines3.common.utils import set_random_seed\n'"
stable_baselines3/common/atari_wrappers.py,0,"b'import gym\nfrom gym.wrappers import AtariPreprocessing\nimport numpy as np\n\nfrom stable_baselines3.common.type_aliases import GymStepReturn\n\n\nclass AtariWrapper(gym.Wrapper):\n    """"""\n    Atari 2600 preprocessings\n\n    It is a wrapper around the one found in gym.\n    It reshapes the observation to have an additional dimension and clip the reward.\n    See https://github.com/openai/gym/blob/master/gym/wrappers/atari_preprocessing.py\n    .\n    This class follows the guidelines in\n    Machado et al. (2018), ""Revisiting the Arcade Learning Environment:\n    Evaluation Protocols and Open Problems for General Agents"".\n\n    Specifically:\n\n    * NoopReset: obtain initial state by taking random number of no-ops on reset.\n    * Frame skipping: 4 by default\n    * Max-pooling: most recent two observations\n    * Termination signal when a life is lost: turned off by default. Not recommended by Machado et al. (2018).\n    * Resize to a square image: 84x84 by default\n    * Grayscale observation: by default\n    * Scale observation: optional\n\n    :param env: (gym.Env) gym environment\n    :param noop_max: (int): max number of no-ops\n    :param frame_skip: (int): the frequency at which the agent experiences the game.\n    :param screen_size: (int): resize Atari frame\n    :param terminal_on_life_loss: (bool): if True, then step() returns done=True whenever a\n            life is lost.\n    :param grayscale_obs: (bool): if True (default), then gray scale observation is returned, otherwise, RGB observation\n            is returned.\n    :param scale_obs: (bool): if True, then observation normalized in range [0,1] is returned. It also limits memory\n            optimization benefits of FrameStack Wrapper.\n    :param scale_obs: (bool) If True (default), the reward is clip to {-1, 0, 1} depending on its sign.\n    """"""\n    def __init__(self, env: gym.Env,\n                 noop_max: int = 30,\n                 frame_skip: int = 4,\n                 screen_size: int = 84,\n                 terminal_on_life_loss: bool = False,\n                 grayscale_obs: bool = True,\n                 scale_obs: bool = False,\n                 clip_reward: bool = True):\n        env = AtariPreprocessing(env, noop_max=noop_max, frame_skip=frame_skip, screen_size=screen_size,\n                                 terminal_on_life_loss=terminal_on_life_loss, grayscale_obs=grayscale_obs,\n                                 scale_obs=scale_obs)\n        # Add channel dimension\n        if grayscale_obs:\n            obs_space = env.observation_space\n            _low, _high, _obs_dtype = (0, 255, np.uint8) if not scale_obs else (0, 1, np.float32)\n            env.observation_space = gym.spaces.Box(low=_low, high=_high, shape=obs_space.shape + (1,),\n                                                   dtype=_obs_dtype)\n\n        super(AtariWrapper, self).__init__(env)\n        self.clip_reward = clip_reward\n\n    def _add_axis(self, obs: np.ndarray) -> np.ndarray:\n        if self.env.grayscale_obs:\n            return obs[..., np.newaxis]\n        return obs\n\n    def reset(self) -> np.ndarray:\n        return self._add_axis(self.env.reset())\n\n    def step(self, action: int) -> GymStepReturn:\n        obs, reward, done, info = self.env.step(action)\n        # Bin reward to {+1, 0, -1} by its sign.\n        if self.clip_reward:\n            reward = np.sign(reward)\n        return self._add_axis(obs), reward, done, info\n'"
stable_baselines3/common/base_class.py,0,"b'import time\nfrom typing import Union, Type, Optional, Dict, Any, List, Tuple, Callable\nfrom abc import ABC, abstractmethod\nfrom collections import deque\n\nimport gym\nimport torch as th\nimport numpy as np\n\nfrom stable_baselines3.common import logger, utils\nfrom stable_baselines3.common.policies import BasePolicy, get_policy_from_name\nfrom stable_baselines3.common.utils import (set_random_seed, get_schedule_fn, update_learning_rate, get_device,\n                                            check_for_correct_spaces)\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecEnv, unwrap_vec_normalize, VecNormalize, VecTransposeImage\nfrom stable_baselines3.common.preprocessing import is_image_space\nfrom stable_baselines3.common.save_util import (recursive_getattr, recursive_setattr, save_to_zip_file,\n                                                load_from_zip_file)\nfrom stable_baselines3.common.type_aliases import GymEnv, MaybeCallback\nfrom stable_baselines3.common.callbacks import BaseCallback, CallbackList, ConvertCallback, EvalCallback\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.noise import ActionNoise\n\n\nclass BaseAlgorithm(ABC):\n    """"""\n    The base of RL algorithms\n\n    :param policy: (Type[BasePolicy]) Policy object\n    :param env: (Union[GymEnv, str]) The environment to learn from\n                (if registered in Gym, can be str. Can be None for loading trained models)\n    :param policy_base: (Type[BasePolicy]) The base policy used by this method\n    :param learning_rate: (float or callable) learning rate for the optimizer,\n        it can be a function of the current progress remaining (from 1 to 0)\n    :param policy_kwargs: (Dict[str, Any]) Additional arguments to be passed to the policy on creation\n    :param tensorboard_log: (str) the log location for tensorboard (if None, no logging)\n    :param verbose: (int) The verbosity level: 0 none, 1 training information, 2 debug\n    :param device: (Union[th.device, str]) Device on which the code should run.\n        By default, it will try to use a Cuda compatible device and fallback to cpu\n        if it is not possible.\n    :param support_multi_env: (bool) Whether the algorithm supports training\n        with multiple environments (as in A2C)\n    :param create_eval_env: (bool) Whether to create a second environment that will be\n        used for evaluating the agent periodically. (Only available when passing string for the environment)\n    :param monitor_wrapper: (bool) When creating an environment, whether to wrap it\n        or not in a Monitor wrapper.\n    :param seed: (Optional[int]) Seed for the pseudo random generators\n    :param use_sde: (bool) Whether to use generalized State Dependent Exploration (gSDE)\n        instead of action noise exploration (default: False)\n    :param sde_sample_freq: (int) Sample a new noise matrix every n steps when using gSDE\n        Default: -1 (only sample at the beginning of the rollout)\n    """"""\n\n    def __init__(self,\n                 policy: Type[BasePolicy],\n                 env: Union[GymEnv, str],\n                 policy_base: Type[BasePolicy],\n                 learning_rate: Union[float, Callable],\n                 policy_kwargs: Dict[str, Any] = None,\n                 tensorboard_log: Optional[str] = None,\n                 verbose: int = 0,\n                 device: Union[th.device, str] = \'auto\',\n                 support_multi_env: bool = False,\n                 create_eval_env: bool = False,\n                 monitor_wrapper: bool = True,\n                 seed: Optional[int] = None,\n                 use_sde: bool = False,\n                 sde_sample_freq: int = -1):\n\n        if isinstance(policy, str) and policy_base is not None:\n            self.policy_class = get_policy_from_name(policy_base, policy)\n        else:\n            self.policy_class = policy\n\n        self.device = get_device(device)\n        if verbose > 0:\n            print(f""Using {self.device} device"")\n\n        self.env = None  # type: Optional[GymEnv]\n        # get VecNormalize object if needed\n        self._vec_normalize_env = unwrap_vec_normalize(env)\n        self.verbose = verbose\n        self.policy_kwargs = {} if policy_kwargs is None else policy_kwargs\n        self.observation_space = None  # type: Optional[gym.spaces.Space]\n        self.action_space = None  # type: Optional[gym.spaces.Space]\n        self.n_envs = None\n        self.num_timesteps = 0\n        self.eval_env = None\n        self.seed = seed\n        self.action_noise = None  # type: Optional[ActionNoise]\n        self.start_time = None\n        self.policy = None\n        self.learning_rate = learning_rate\n        self.tensorboard_log = tensorboard_log\n        self.lr_schedule = None  # type: Optional[Callable]\n        self._last_obs = None  # type: Optional[np.ndarray]\n        # When using VecNormalize:\n        self._last_original_obs = None  # type: Optional[np.ndarray]\n        self._episode_num = 0\n        # Used for gSDE only\n        self.use_sde = use_sde\n        self.sde_sample_freq = sde_sample_freq\n        # Track the training progress remaining (from 1 to 0)\n        # this is used to update the learning rate\n        self._current_progress_remaining = 1\n        # Buffers for logging\n        self.ep_info_buffer = None  # type: Optional[deque]\n        self.ep_success_buffer = None  # type: Optional[deque]\n        # For logging\n        self._n_updates = 0  # type: int\n\n        # Create and wrap the env if needed\n        if env is not None:\n            if isinstance(env, str):\n                if create_eval_env:\n                    eval_env = gym.make(env)\n                    if monitor_wrapper:\n                        eval_env = Monitor(eval_env, filename=None)\n                    self.eval_env = DummyVecEnv([lambda: eval_env])\n                if self.verbose >= 1:\n                    print(""Creating environment from the given name, wrapped in a DummyVecEnv."")\n\n                env = gym.make(env)\n                if monitor_wrapper:\n                    env = Monitor(env, filename=None)\n                env = DummyVecEnv([lambda: env])\n\n            env = self._wrap_env(env)\n\n            self.observation_space = env.observation_space\n            self.action_space = env.action_space\n            self.n_envs = env.num_envs\n            self.env = env\n\n            if not support_multi_env and self.n_envs > 1:\n                raise ValueError(""Error: the model does not support multiple envs requires a single vectorized""\n                                 "" environment."")\n\n    def _wrap_env(self, env: GymEnv) -> VecEnv:\n        if not isinstance(env, VecEnv):\n            if self.verbose >= 1:\n                print(""Wrapping the env in a DummyVecEnv."")\n            env = DummyVecEnv([lambda: env])\n\n        if is_image_space(env.observation_space) and not isinstance(env, VecTransposeImage):\n            if self.verbose >= 1:\n                print(""Wrapping the env in a VecTransposeImage."")\n            env = VecTransposeImage(env)\n        return env\n\n    @abstractmethod\n    def _setup_model(self) -> None:\n        """"""\n        Create networks, buffer and optimizers\n        """"""\n        raise NotImplementedError()\n\n    def _get_eval_env(self, eval_env: Optional[GymEnv]) -> Optional[GymEnv]:\n        """"""\n        Return the environment that will be used for evaluation.\n\n        :param eval_env: (Optional[GymEnv]))\n        :return: (Optional[GymEnv])\n        """"""\n        if eval_env is None:\n            eval_env = self.eval_env\n\n        if eval_env is not None:\n            eval_env = self._wrap_env(eval_env)\n            assert eval_env.num_envs == 1\n        return eval_env\n\n    def _setup_lr_schedule(self) -> None:\n        """"""Transform to callable if needed.""""""\n        self.lr_schedule = get_schedule_fn(self.learning_rate)\n\n    def _update_current_progress_remaining(self, num_timesteps: int, total_timesteps: int) -> None:\n        """"""\n        Compute current progress remaining (starts from 1 and ends to 0)\n\n        :param num_timesteps: current number of timesteps\n        :param total_timesteps:\n        """"""\n        self._current_progress_remaining = 1.0 - float(num_timesteps) / float(total_timesteps)\n\n    def _update_learning_rate(self, optimizers: Union[List[th.optim.Optimizer], th.optim.Optimizer]) -> None:\n        """"""\n        Update the optimizers learning rate using the current learning rate schedule\n        and the current progress remaining (from 1 to 0).\n\n        :param optimizers: (Union[List[th.optim.Optimizer], th.optim.Optimizer])\n            An optimizer or a list of optimizers.\n        """"""\n        # Log the current learning rate\n        logger.record(""train/learning_rate"", self.lr_schedule(self._current_progress_remaining))\n\n        if not isinstance(optimizers, list):\n            optimizers = [optimizers]\n        for optimizer in optimizers:\n            update_learning_rate(optimizer, self.lr_schedule(self._current_progress_remaining))\n\n    def get_env(self) -> Optional[VecEnv]:\n        """"""\n        Returns the current environment (can be None if not defined).\n\n        :return: (Optional[VecEnv]) The current environment\n        """"""\n        return self.env\n\n    def get_vec_normalize_env(self) -> Optional[VecNormalize]:\n        """"""\n        Return the ``VecNormalize`` wrapper of the training env\n        if it exists.\n        :return: Optional[VecNormalize] The ``VecNormalize`` env.\n        """"""\n        return self._vec_normalize_env\n\n    def set_env(self, env: GymEnv) -> None:\n        """"""\n        Checks the validity of the environment, and if it is coherent, set it as the current environment.\n        Furthermore wrap any non vectorized env into a vectorized\n        checked parameters:\n        - observation_space\n        - action_space\n\n        :param env: The environment for learning a policy\n        """"""\n        check_for_correct_spaces(env, self.observation_space, self.action_space)\n        # it must be coherent now\n        # if it is not a VecEnv, make it a VecEnv\n        env = self._wrap_env(env)\n\n        self.n_envs = env.num_envs\n        self.env = env\n\n    def get_torch_variables(self) -> Tuple[List[str], List[str]]:\n        """"""\n        Get the name of the torch variable that will be saved.\n        ``th.save`` and ``th.load`` will be used with the right device\n        instead of the default pickling strategy.\n\n        :return: (Tuple[List[str], List[str]])\n            name of the variables with state dicts to save, name of additional torch tensors,\n        """"""\n        state_dicts = [""policy""]\n\n        return state_dicts, []\n\n    @abstractmethod\n    def learn(self, total_timesteps: int,\n              callback: MaybeCallback = None,\n              log_interval: int = 100,\n              tb_log_name: str = ""run"",\n              eval_env: Optional[GymEnv] = None,\n              eval_freq: int = -1,\n              n_eval_episodes: int = 5,\n              eval_log_path: Optional[str] = None,\n              reset_num_timesteps: bool = True) -> \'BaseAlgorithm\':\n        """"""\n        Return a trained model.\n\n        :param total_timesteps: (int) The total number of samples (env steps) to train on\n        :param callback: (function (dict, dict)) -> boolean function called at every steps with state of the algorithm.\n            It takes the local and global variables. If it returns False, training is aborted.\n        :param log_interval: (int) The number of timesteps before logging.\n        :param tb_log_name: (str) the name of the run for tensorboard log\n        :param eval_env: (gym.Env) Environment that will be used to evaluate the agent\n        :param eval_freq: (int) Evaluate the agent every ``eval_freq`` timesteps (this may vary a little)\n        :param n_eval_episodes: (int) Number of episode to evaluate the agent\n        :param eval_log_path: (Optional[str]) Path to a folder where the evaluations will be saved\n        :param reset_num_timesteps: (bool) whether or not to reset the current timestep number (used in logging)\n        :return: (BaseAlgorithm) the trained model\n        """"""\n        raise NotImplementedError()\n\n    def predict(self, observation: np.ndarray,\n                state: Optional[np.ndarray] = None,\n                mask: Optional[np.ndarray] = None,\n                deterministic: bool = False) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n        """"""\n        Get the model\'s action(s) from an observation\n\n        :param observation: (np.ndarray) the input observation\n        :param state: (Optional[np.ndarray]) The last states (can be None, used in recurrent policies)\n        :param mask: (Optional[np.ndarray]) The last masks (can be None, used in recurrent policies)\n        :param deterministic: (bool) Whether or not to return deterministic actions.\n        :return: (Tuple[np.ndarray, Optional[np.ndarray]]) the model\'s action and the next state\n            (used in recurrent policies)\n        """"""\n        return self.policy.predict(observation, state, mask, deterministic)\n\n    @classmethod\n    def load(cls, load_path: str, env: Optional[GymEnv] = None, **kwargs):\n        """"""\n        Load the model from a zip-file\n\n        :param load_path: the location of the saved data\n        :param env: the new environment to run the loaded model on\n            (can be None if you only need prediction from a trained model) has priority over any saved environment\n        :param kwargs: extra arguments to change the model when loading\n        """"""\n        data, params, tensors = load_from_zip_file(load_path)\n\n        if \'policy_kwargs\' in data:\n            for arg_to_remove in [\'device\']:\n                if arg_to_remove in data[\'policy_kwargs\']:\n                    del data[\'policy_kwargs\'][arg_to_remove]\n\n        if \'policy_kwargs\' in kwargs and kwargs[\'policy_kwargs\'] != data[\'policy_kwargs\']:\n            raise ValueError(f""The specified policy kwargs do not equal the stored policy kwargs.""\n                             f""Stored kwargs: {data[\'policy_kwargs\']}, specified kwargs: {kwargs[\'policy_kwargs\']}"")\n\n        # check if observation space and action space are part of the saved parameters\n        if (""observation_space"" not in data or ""action_space"" not in data) and ""env"" not in data:\n            raise ValueError(""The observation_space and action_space was not given, can\'t verify new environments"")\n        # check if given env is valid\n        if env is not None:\n            check_for_correct_spaces(env, data[""observation_space""], data[""action_space""])\n        # if no new env was given use stored env if possible\n        if env is None and ""env"" in data:\n            env = data[""env""]\n\n        # noinspection PyArgumentList\n        model = cls(policy=data[""policy_class""], env=env, device=\'auto\', _init_setup_model=False)\n\n        # load parameters\n        model.__dict__.update(data)\n        model.__dict__.update(kwargs)\n        if not hasattr(model, ""_setup_model"") and len(params) > 0:\n            raise NotImplementedError(f""{cls} has no ``_setup_model()`` method"")\n        model._setup_model()\n\n        # put state_dicts back in place\n        for name in params:\n            attr = recursive_getattr(model, name)\n            attr.load_state_dict(params[name])\n\n        # put tensors back in place\n        if tensors is not None:\n            for name in tensors:\n                recursive_setattr(model, name, tensors[name])\n\n        # Sample gSDE exploration matrix, so it uses the right device\n        # see issue #44\n        if model.use_sde:\n            model.policy.reset_noise()\n        return model\n\n    def set_random_seed(self, seed: Optional[int] = None) -> None:\n        """"""\n        Set the seed of the pseudo-random generators\n        (python, numpy, pytorch, gym, action_space)\n\n        :param seed: (int)\n        """"""\n        if seed is None:\n            return\n        set_random_seed(seed, using_cuda=self.device == th.device(\'cuda\'))\n        self.action_space.seed(seed)\n        if self.env is not None:\n            self.env.seed(seed)\n        if self.eval_env is not None:\n            self.eval_env.seed(seed)\n\n    def _init_callback(self,\n                       callback: Union[None, Callable, List[BaseCallback], BaseCallback],\n                       eval_env: Optional[VecEnv] = None,\n                       eval_freq: int = 10000,\n                       n_eval_episodes: int = 5,\n                       log_path: Optional[str] = None) -> BaseCallback:\n        """"""\n        :param callback: (Union[callable, [BaseCallback], BaseCallback, None])\n        :return: (BaseCallback)\n        """"""\n        # Convert a list of callbacks into a callback\n        if isinstance(callback, list):\n            callback = CallbackList(callback)\n\n        # Convert functional callback to object\n        if not isinstance(callback, BaseCallback):\n            callback = ConvertCallback(callback)\n\n        # Create eval callback in charge of the evaluation\n        if eval_env is not None:\n            eval_callback = EvalCallback(eval_env,\n                                         best_model_save_path=log_path,\n                                         log_path=log_path, eval_freq=eval_freq, n_eval_episodes=n_eval_episodes)\n            callback = CallbackList([callback, eval_callback])\n\n        callback.init_callback(self)\n        return callback\n\n    def _setup_learn(self,\n                     total_timesteps: int,\n                     eval_env: Optional[GymEnv],\n                     callback: Union[None, Callable, List[BaseCallback], BaseCallback] = None,\n                     eval_freq: int = 10000,\n                     n_eval_episodes: int = 5,\n                     log_path: Optional[str] = None,\n                     reset_num_timesteps: bool = True,\n                     tb_log_name: str = \'run\',\n                     ) -> Tuple[int, \'BaseCallback\']:\n        """"""\n        Initialize different variables needed for training.\n\n        :param total_timesteps: (int) The total number of samples (env steps) to train on\n        :param eval_env: (Optional[GymEnv])\n        :param callback: (Union[None, BaseCallback, List[BaseCallback, Callable]])\n        :param eval_freq: (int) How many steps between evaluations\n        :param n_eval_episodes: (int) How many episodes to play per evaluation\n        :param log_path (Optional[str]): Path to a log folder\n        :param reset_num_timesteps: (bool) Whether to reset or not the ``num_timesteps`` attribute\n        :param tb_log_name: (str) the name of the run for tensorboard log\n        :return: (int, Tuple[BaseCallback])\n        """"""\n        self.start_time = time.time()\n        self.ep_info_buffer = deque(maxlen=100)\n        self.ep_success_buffer = deque(maxlen=100)\n\n        if self.action_noise is not None:\n            self.action_noise.reset()\n\n        if reset_num_timesteps:\n            self.num_timesteps = 0\n            self._episode_num = 0\n        else:\n            # Make sure training timesteps are ahead of the internal counter\n            total_timesteps += self.num_timesteps\n\n        # Avoid resetting the environment when calling ``.learn()`` consecutive times\n        if reset_num_timesteps or self._last_obs is None:\n            self._last_obs = self.env.reset()\n            # Retrieve unnormalized observation for saving into the buffer\n            if self._vec_normalize_env is not None:\n                self._last_original_obs = self._vec_normalize_env.get_original_obs()\n\n        if eval_env is not None and self.seed is not None:\n            eval_env.seed(self.seed)\n\n        eval_env = self._get_eval_env(eval_env)\n\n        # Configure logger\'s outputs\n        utils.configure_logger(self.verbose, self.tensorboard_log, tb_log_name, reset_num_timesteps)\n\n        # Create eval callback if needed\n        callback = self._init_callback(callback, eval_env, eval_freq, n_eval_episodes, log_path)\n\n        return total_timesteps, callback\n\n    def _update_info_buffer(self, infos: List[Dict[str, Any]], dones: Optional[np.ndarray] = None) -> None:\n        """"""\n        Retrieve reward and episode length and update the buffer\n        if using Monitor wrapper.\n\n        :param infos: ([dict])\n        """"""\n        if dones is None:\n            dones = np.array([False] * len(infos))\n        for idx, info in enumerate(infos):\n            maybe_ep_info = info.get(\'episode\')\n            maybe_is_success = info.get(\'is_success\')\n            if maybe_ep_info is not None:\n                self.ep_info_buffer.extend([maybe_ep_info])\n            if maybe_is_success is not None and dones[idx]:\n                self.ep_success_buffer.append(maybe_is_success)\n\n    def excluded_save_params(self) -> List[str]:\n        """"""\n        Returns the names of the parameters that should be excluded by default\n        when saving the model.\n\n        :return: ([str]) List of parameters that should be excluded from save\n        """"""\n        return [""policy"", ""device"", ""env"", ""eval_env"", ""replay_buffer"", ""rollout_buffer"", ""_vec_normalize_env""]\n\n    def save(self, path: str, exclude: Optional[List[str]] = None, include: Optional[List[str]] = None) -> None:\n        """"""\n        Save all the attributes of the object and the model parameters in a zip-file.\n\n        :param path: path to the file where the rl agent should be saved\n        :param exclude: name of parameters that should be excluded in addition to the default one\n        :param include: name of parameters that might be excluded but should be included anyway\n        """"""\n        # copy parameter list so we don\'t mutate the original dict\n        data = self.__dict__.copy()\n        # use standard list of excluded parameters if none given\n        if exclude is None:\n            exclude = self.excluded_save_params()\n        else:\n            # append standard exclude params to the given params\n            exclude.extend([param for param in self.excluded_save_params() if param not in exclude])\n\n        # do not exclude params if they are specifically included\n        if include is not None:\n            exclude = [param_name for param_name in exclude if param_name not in include]\n\n        state_dicts_names, tensors_names = self.get_torch_variables()\n        # any params that are in the save vars must not be saved by data\n        torch_variables = state_dicts_names + tensors_names\n        for torch_var in torch_variables:\n            # we need to get only the name of the top most module as we\'ll remove that\n            var_name = torch_var.split(\'.\')[0]\n            exclude.append(var_name)\n\n        # Remove parameter entries of parameters which are to be excluded\n        for param_name in exclude:\n            if param_name in data:\n                data.pop(param_name, None)\n\n        # Build dict of tensor variables\n        tensors = None\n        if tensors_names is not None:\n            tensors = {}\n            for name in tensors_names:\n                attr = recursive_getattr(self, name)\n                tensors[name] = attr\n\n        # Build dict of state_dicts\n        params_to_save = {}\n        for name in state_dicts_names:\n            attr = recursive_getattr(self, name)\n            # Retrieve state dict\n            params_to_save[name] = attr.state_dict()\n\n        save_to_zip_file(path, data=data, params=params_to_save, tensors=tensors)\n'"
stable_baselines3/common/bit_flipping_env.py,0,"b'from collections import OrderedDict\nfrom typing import Optional, Union\n\nimport numpy as np\nfrom gym import GoalEnv, spaces\n\nfrom stable_baselines3.common.type_aliases import GymStepReturn\n\n\nclass BitFlippingEnv(GoalEnv):\n    """"""\n    Simple bit flipping env, useful to test HER.\n    The goal is to flip all the bits to get a vector of ones.\n    In the continuous variant, if the ith action component has a value > 0,\n    then the ith bit will be flipped.\n\n    :param n_bits: (int) Number of bits to flip\n    :param continuous: (bool) Whether to use the continuous actions version or not,\n        by default, it uses the discrete one\n    :param max_steps: (Optional[int]) Max number of steps, by default, equal to n_bits\n    :param discrete_obs_space: (bool) Whether to use the discrete observation\n        version or not, by default, it uses the MultiBinary one\n    """"""\n    def __init__(self, n_bits: int = 10,\n                 continuous: bool = False,\n                 max_steps: Optional[int] = None,\n                 discrete_obs_space: bool = False):\n        super(BitFlippingEnv, self).__init__()\n        # The achieved goal is determined by the current state\n        # here, it is a special where they are equal\n        if discrete_obs_space:\n            # In the discrete case, the agent act on the binary\n            # representation of the observation\n            self.observation_space = spaces.Dict({\n                \'observation\': spaces.Discrete(2 ** n_bits - 1),\n                \'achieved_goal\': spaces.Discrete(2 ** n_bits - 1),\n                \'desired_goal\': spaces.Discrete(2 ** n_bits - 1)\n            })\n        else:\n            self.observation_space = spaces.Dict({\n                \'observation\': spaces.MultiBinary(n_bits),\n                \'achieved_goal\': spaces.MultiBinary(n_bits),\n                \'desired_goal\': spaces.MultiBinary(n_bits)\n            })\n\n        self.obs_space = spaces.MultiBinary(n_bits)\n\n        if continuous:\n            self.action_space = spaces.Box(-1, 1, shape=(n_bits,), dtype=np.float32)\n        else:\n            self.action_space = spaces.Discrete(n_bits)\n        self.continuous = continuous\n        self.discrete_obs_space = discrete_obs_space\n        self.state = None\n        self.desired_goal = np.ones((n_bits,))\n        if max_steps is None:\n            max_steps = n_bits\n        self.max_steps = max_steps\n        self.current_step = 0\n        self.reset()\n\n    def convert_if_needed(self, state: np.ndarray) -> Union[int, np.ndarray]:\n        """"""\n        Convert to discrete space if needed.\n\n        :param state: (np.ndarray)\n        :return: (np.ndarray or int)\n        """"""\n        if self.discrete_obs_space:\n            # The internal state is the binary representation of the\n            # observed one\n            return int(sum([state[i] * 2**i for i in range(len(state))]))\n        return state\n\n    def _get_obs(self) -> OrderedDict:\n        """"""\n        Helper to create the observation.\n\n        :return: (OrderedDict<int or ndarray>)\n        """"""\n        return OrderedDict([\n            (\'observation\', self.convert_if_needed(self.state.copy())),\n            (\'achieved_goal\', self.convert_if_needed(self.state.copy())),\n            (\'desired_goal\', self.convert_if_needed(self.desired_goal.copy()))\n        ])\n\n    def reset(self) -> OrderedDict:\n        self.current_step = 0\n        self.state = self.obs_space.sample()\n        return self._get_obs()\n\n    def step(self, action: Union[np.ndarray, int]) -> GymStepReturn:\n        if self.continuous:\n            self.state[action > 0] = 1 - self.state[action > 0]\n        else:\n            self.state[action] = 1 - self.state[action]\n        obs = self._get_obs()\n        reward = self.compute_reward(obs[\'achieved_goal\'], obs[\'desired_goal\'], None)\n        done = reward == 0\n        self.current_step += 1\n        # Episode terminate when we reached the goal or the max number of steps\n        info = {\'is_success\': done}\n        done = done or self.current_step >= self.max_steps\n        return obs, reward, done, info\n\n    def compute_reward(self,\n                       achieved_goal: np.ndarray,\n                       desired_goal: np.ndarray,\n                       _info) -> float:\n        # Deceptive reward: it is positive only when the goal is achieved\n        if self.discrete_obs_space:\n            return 0.0 if achieved_goal == desired_goal else -1.0\n        return 0.0 if (achieved_goal == desired_goal).all() else -1.0\n\n    def render(self, mode: str = \'human\') -> Optional[np.ndarray]:\n        if mode == \'rgb_array\':\n            return self.state.copy()\n        print(self.state)\n\n    def close(self) -> None:\n        pass\n'"
stable_baselines3/common/buffers.py,0,"b'from typing import Union, Optional, Generator\n\nimport numpy as np\nimport torch as th\nfrom gym import spaces\n\nfrom stable_baselines3.common.vec_env import VecNormalize\nfrom stable_baselines3.common.type_aliases import RolloutBufferSamples, ReplayBufferSamples\nfrom stable_baselines3.common.preprocessing import get_action_dim, get_obs_shape\n\n\nclass BaseBuffer(object):\n    """"""\n    Base class that represent a buffer (rollout or replay)\n\n    :param buffer_size: (int) Max number of element in the buffer\n    :param observation_space: (spaces.Space) Observation space\n    :param action_space: (spaces.Space) Action space\n    :param device: (Union[th.device, str]) PyTorch device\n        to which the values will be converted\n    :param n_envs: (int) Number of parallel environments\n    """"""\n\n    def __init__(self,\n                 buffer_size: int,\n                 observation_space: spaces.Space,\n                 action_space: spaces.Space,\n                 device: Union[th.device, str] = \'cpu\',\n                 n_envs: int = 1):\n        super(BaseBuffer, self).__init__()\n        self.buffer_size = buffer_size\n        self.observation_space = observation_space\n        self.action_space = action_space\n        self.obs_shape = get_obs_shape(observation_space)\n        self.action_dim = get_action_dim(action_space)\n        self.pos = 0\n        self.full = False\n        self.device = device\n        self.n_envs = n_envs\n\n    @staticmethod\n    def swap_and_flatten(arr: np.ndarray) -> np.ndarray:\n        """"""\n        Swap and then flatten axes 0 (buffer_size) and 1 (n_envs)\n        to convert shape from [n_steps, n_envs, ...] (when ... is the shape of the features)\n        to [n_steps * n_envs, ...] (which maintain the order)\n\n        :param arr: (np.ndarray)\n        :return: (np.ndarray)\n        """"""\n        shape = arr.shape\n        if len(shape) < 3:\n            shape = shape + (1,)\n        return arr.swapaxes(0, 1).reshape(shape[0] * shape[1], *shape[2:])\n\n    def size(self) -> int:\n        """"""\n        :return: (int) The current size of the buffer\n        """"""\n        if self.full:\n            return self.buffer_size\n        return self.pos\n\n    def add(self, *args, **kwargs) -> None:\n        """"""\n        Add elements to the buffer.\n        """"""\n        raise NotImplementedError()\n\n    def extend(self, *args, **kwargs) -> None:\n        """"""\n        Add a new batch of transitions to the buffer\n        """"""\n        # Do a for loop along the batch axis\n        for data in zip(*args):\n            self.add(*data)\n\n    def reset(self) -> None:\n        """"""\n        Reset the buffer.\n        """"""\n        self.pos = 0\n        self.full = False\n\n    def sample(self,\n               batch_size: int,\n               env: Optional[VecNormalize] = None\n               ):\n        """"""\n        :param batch_size: (int) Number of element to sample\n        :param env: (Optional[VecNormalize]) associated gym VecEnv\n            to normalize the observations/rewards when sampling\n        :return: (Union[RolloutBufferSamples, ReplayBufferSamples])\n        """"""\n        upper_bound = self.buffer_size if self.full else self.pos\n        batch_inds = np.random.randint(0, upper_bound, size=batch_size)\n        return self._get_samples(batch_inds, env=env)\n\n    def _get_samples(self,\n                     batch_inds: np.ndarray,\n                     env: Optional[VecNormalize] = None\n                     ):\n        """"""\n        :param batch_inds: (th.Tensor)\n        :param env: (Optional[VecNormalize])\n        :return: (Union[RolloutBufferSamples, ReplayBufferSamples])\n        """"""\n        raise NotImplementedError()\n\n    def to_torch(self, array: np.ndarray, copy: bool = True) -> th.Tensor:\n        """"""\n        Convert a numpy array to a PyTorch tensor.\n        Note: it copies the data by default\n\n        :param array: (np.ndarray)\n        :param copy: (bool) Whether to copy or not the data\n            (may be useful to avoid changing things be reference)\n        :return: (th.Tensor)\n        """"""\n        if copy:\n            return th.tensor(array).to(self.device)\n        return th.as_tensor(array).to(self.device)\n\n    @staticmethod\n    def _normalize_obs(obs: np.ndarray,\n                       env: Optional[VecNormalize] = None) -> np.ndarray:\n        if env is not None:\n            return env.normalize_obs(obs).astype(np.float32)\n        return obs\n\n    @staticmethod\n    def _normalize_reward(reward: np.ndarray,\n                          env: Optional[VecNormalize] = None) -> np.ndarray:\n        if env is not None:\n            return env.normalize_reward(reward).astype(np.float32)\n        return reward\n\n\nclass ReplayBuffer(BaseBuffer):\n    """"""\n    Replay buffer used in off-policy algorithms like SAC/TD3.\n\n    :param buffer_size: (int) Max number of element in the buffer\n    :param observation_space: (spaces.Space) Observation space\n    :param action_space: (spaces.Space) Action space\n    :param device: (th.device)\n    :param n_envs: (int) Number of parallel environments\n    """"""\n\n    def __init__(self,\n                 buffer_size: int,\n                 observation_space: spaces.Space,\n                 action_space: spaces.Space,\n                 device: Union[th.device, str] = \'cpu\',\n                 n_envs: int = 1):\n        super(ReplayBuffer, self).__init__(buffer_size, observation_space,\n                                           action_space, device, n_envs=n_envs)\n\n        assert n_envs == 1, ""Replay buffer only support single environment for now""\n\n        self.observations = np.zeros((self.buffer_size, self.n_envs,) + self.obs_shape, dtype=np.float32)\n        self.actions = np.zeros((self.buffer_size, self.n_envs, self.action_dim), dtype=np.float32)\n        self.next_observations = np.zeros((self.buffer_size, self.n_envs,) + self.obs_shape, dtype=np.float32)\n        self.rewards = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n        self.dones = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n\n    def add(self,\n            obs: np.ndarray,\n            next_obs: np.ndarray,\n            action: np.ndarray,\n            reward: np.ndarray,\n            done: np.ndarray) -> None:\n        # Copy to avoid modification by reference\n        self.observations[self.pos] = np.array(obs).copy()\n        self.next_observations[self.pos] = np.array(next_obs).copy()\n        self.actions[self.pos] = np.array(action).copy()\n        self.rewards[self.pos] = np.array(reward).copy()\n        self.dones[self.pos] = np.array(done).copy()\n\n        self.pos += 1\n        if self.pos == self.buffer_size:\n            self.full = True\n            self.pos = 0\n\n    def _get_samples(self,\n                     batch_inds: np.ndarray,\n                     env: Optional[VecNormalize] = None\n                     ) -> ReplayBufferSamples:\n        data = (self._normalize_obs(self.observations[batch_inds, 0, :], env),\n                self.actions[batch_inds, 0, :],\n                self._normalize_obs(self.next_observations[batch_inds, 0, :], env),\n                self.dones[batch_inds],\n                self._normalize_reward(self.rewards[batch_inds], env))\n        return ReplayBufferSamples(*tuple(map(self.to_torch, data)))\n\n\nclass RolloutBuffer(BaseBuffer):\n    """"""\n    Rollout buffer used in on-policy algorithms like A2C/PPO.\n\n    :param buffer_size: (int) Max number of element in the buffer\n    :param observation_space: (spaces.Space) Observation space\n    :param action_space: (spaces.Space) Action space\n    :param device: (th.device)\n    :param gae_lambda: (float) Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n        Equivalent to classic advantage when set to 1.\n    :param gamma: (float) Discount factor\n    :param n_envs: (int) Number of parallel environments\n    """"""\n\n    def __init__(self,\n                 buffer_size: int,\n                 observation_space: spaces.Space,\n                 action_space: spaces.Space,\n                 device: Union[th.device, str] = \'cpu\',\n                 gae_lambda: float = 1,\n                 gamma: float = 0.99,\n                 n_envs: int = 1):\n\n        super(RolloutBuffer, self).__init__(buffer_size, observation_space,\n                                            action_space, device, n_envs=n_envs)\n        self.gae_lambda = gae_lambda\n        self.gamma = gamma\n        self.observations, self.actions, self.rewards, self.advantages = None, None, None, None\n        self.returns, self.dones, self.values, self.log_probs = None, None, None, None\n        self.generator_ready = False\n        self.reset()\n\n    def reset(self) -> None:\n        self.observations = np.zeros((self.buffer_size, self.n_envs,) + self.obs_shape, dtype=np.float32)\n        self.actions = np.zeros((self.buffer_size, self.n_envs, self.action_dim), dtype=np.float32)\n        self.rewards = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n        self.returns = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n        self.dones = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n        self.values = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n        self.log_probs = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n        self.advantages = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n        self.generator_ready = False\n        super(RolloutBuffer, self).reset()\n\n    def compute_returns_and_advantage(self,\n                                      last_value: th.Tensor,\n                                      dones: np.ndarray,\n                                      use_gae: bool = True) -> None:\n        """"""\n        Post-processing step: compute the returns (sum of discounted rewards)\n        and advantage (A(s) = R - V(S)).\n        Adapted from Stable-Baselines PPO2.\n\n        :param last_value: (th.Tensor)\n        :param dones: (np.ndarray)\n        :param use_gae: (bool) Whether to use Generalized Advantage Estimation\n            or normal advantage for advantage computation.\n        """"""\n        # convert to numpy\n        last_value = last_value.clone().cpu().numpy().flatten()\n\n        if use_gae:\n            last_gae_lam = 0\n            for step in reversed(range(self.buffer_size)):\n                if step == self.buffer_size - 1:\n                    next_non_terminal = 1.0 - dones\n                    next_value = last_value\n                else:\n                    next_non_terminal = 1.0 - self.dones[step + 1]\n                    next_value = self.values[step + 1]\n                delta = self.rewards[step] + self.gamma * next_value * next_non_terminal - self.values[step]\n                last_gae_lam = delta + self.gamma * self.gae_lambda * next_non_terminal * last_gae_lam\n                self.advantages[step] = last_gae_lam\n            self.returns = self.advantages + self.values\n        else:\n            # Discounted return with value bootstrap\n            # Note: this is equivalent to GAE computation\n            # with gae_lambda = 1.0\n            last_return = 0.0\n            for step in reversed(range(self.buffer_size)):\n                if step == self.buffer_size - 1:\n                    next_non_terminal = 1.0 - dones\n                    next_value = last_value\n                    last_return = self.rewards[step] + next_non_terminal * next_value\n                else:\n                    next_non_terminal = 1.0 - self.dones[step + 1]\n                    last_return = self.rewards[step] + self.gamma * last_return * next_non_terminal\n                self.returns[step] = last_return\n            self.advantages = self.returns - self.values\n\n    def add(self,\n            obs: np.ndarray,\n            action: np.ndarray,\n            reward: np.ndarray,\n            done: np.ndarray,\n            value: th.Tensor,\n            log_prob: th.Tensor) -> None:\n        """"""\n        :param obs: (np.ndarray) Observation\n        :param action: (np.ndarray) Action\n        :param reward: (np.ndarray)\n        :param done: (np.ndarray) End of episode signal.\n        :param value: (th.Tensor) estimated value of the current state\n            following the current policy.\n        :param log_prob: (th.Tensor) log probability of the action\n            following the current policy.\n        """"""\n        if len(log_prob.shape) == 0:\n            # Reshape 0-d tensor to avoid error\n            log_prob = log_prob.reshape(-1, 1)\n\n        self.observations[self.pos] = np.array(obs).copy()\n        self.actions[self.pos] = np.array(action).copy()\n        self.rewards[self.pos] = np.array(reward).copy()\n        self.dones[self.pos] = np.array(done).copy()\n        self.values[self.pos] = value.clone().cpu().numpy().flatten()\n        self.log_probs[self.pos] = log_prob.clone().cpu().numpy()\n        self.pos += 1\n        if self.pos == self.buffer_size:\n            self.full = True\n\n    def get(self, batch_size: Optional[int] = None) -> Generator[RolloutBufferSamples, None, None]:\n        assert self.full, \'\'\n        indices = np.random.permutation(self.buffer_size * self.n_envs)\n        # Prepare the data\n        if not self.generator_ready:\n            for tensor in [\'observations\', \'actions\', \'values\',\n                           \'log_probs\', \'advantages\', \'returns\']:\n                self.__dict__[tensor] = self.swap_and_flatten(self.__dict__[tensor])\n            self.generator_ready = True\n\n        # Return everything, don\'t create minibatches\n        if batch_size is None:\n            batch_size = self.buffer_size * self.n_envs\n\n        start_idx = 0\n        while start_idx < self.buffer_size * self.n_envs:\n            yield self._get_samples(indices[start_idx:start_idx + batch_size])\n            start_idx += batch_size\n\n    def _get_samples(self, batch_inds: np.ndarray,\n                     env: Optional[VecNormalize] = None) -> RolloutBufferSamples:\n        data = (self.observations[batch_inds],\n                self.actions[batch_inds],\n                self.values[batch_inds].flatten(),\n                self.log_probs[batch_inds].flatten(),\n                self.advantages[batch_inds].flatten(),\n                self.returns[batch_inds].flatten())\n        return RolloutBufferSamples(*tuple(map(self.to_torch, data)))\n'"
stable_baselines3/common/callbacks.py,0,"b'import os\nfrom abc import ABC, abstractmethod\nimport warnings\nimport typing\nfrom typing import Union, List, Dict, Any, Optional\n\nimport gym\nimport numpy as np\n\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecEnv, sync_envs_normalization\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common import logger\n\nif typing.TYPE_CHECKING:\n    from stable_baselines3.common.base_class import BaseAlgorithm  # pytype: disable=pyi-error\n\n\nclass BaseCallback(ABC):\n    """"""\n    Base class for callback.\n\n    :param verbose: (int)\n    """"""\n    def __init__(self, verbose: int = 0):\n        super(BaseCallback, self).__init__()\n        # The RL model\n        self.model = None  # type: Optional[BaseAlgorithm]\n        # An alias for self.model.get_env(), the environment used for training\n        self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n        # Number of time the callback was called\n        self.n_calls = 0  # type: int\n        # n_envs * n times env.step() was called\n        self.num_timesteps = 0  # type: int\n        self.verbose = verbose\n        self.locals = None  # type: Optional[Dict[str, Any]]\n        self.globals = None  # type: Optional[Dict[str, Any]]\n        self.logger = None\n        # Sometimes, for event callback, it is useful\n        # to have access to the parent object\n        self.parent = None  # type: Optional[BaseCallback]\n\n    # Type hint as string to avoid circular import\n    def init_callback(self, model: \'BaseAlgorithm\') -> None:\n        """"""\n        Initialize the callback by saving references to the\n        RL model and the training environment for convenience.\n        """"""\n        self.model = model\n        self.training_env = model.get_env()\n        self.logger = logger\n        self._init_callback()\n\n    def _init_callback(self) -> None:\n        pass\n\n    def on_training_start(self, locals_: Dict[str, Any], globals_: Dict[str, Any]) -> None:\n        # Those are reference and will be updated automatically\n        self.locals = locals_\n        self.globals = globals_\n        self._on_training_start()\n\n    def _on_training_start(self) -> None:\n        pass\n\n    def on_rollout_start(self) -> None:\n        self._on_rollout_start()\n\n    def _on_rollout_start(self) -> None:\n        pass\n\n    @abstractmethod\n    def _on_step(self) -> bool:\n        """"""\n        :return: (bool) If the callback returns False, training is aborted early.\n        """"""\n        return True\n\n    def on_step(self) -> bool:\n        """"""\n        This method will be called by the model after each call to ``env.step()``.\n\n        For child callback (of an ``EventCallback``), this will be called\n        when the event is triggered.\n\n        :return: (bool) If the callback returns False, training is aborted early.\n        """"""\n        self.n_calls += 1\n        # timesteps start at zero\n        self.num_timesteps = self.model.num_timesteps + 1\n\n        return self._on_step()\n\n    def on_training_end(self) -> None:\n        self._on_training_end()\n\n    def _on_training_end(self) -> None:\n        pass\n\n    def on_rollout_end(self) -> None:\n        self._on_rollout_end()\n\n    def _on_rollout_end(self) -> None:\n        pass\n\n\nclass EventCallback(BaseCallback):\n    """"""\n    Base class for triggering callback on event.\n\n    :param callback: (Optional[BaseCallback]) Callback that will be called\n        when an event is triggered.\n    :param verbose: (int)\n    """"""\n    def __init__(self, callback: Optional[BaseCallback] = None, verbose: int = 0):\n        super(EventCallback, self).__init__(verbose=verbose)\n        self.callback = callback\n        # Give access to the parent\n        if callback is not None:\n            self.callback.parent = self\n\n    def init_callback(self, model: \'BaseAlgorithm\') -> None:\n        super(EventCallback, self).init_callback(model)\n        if self.callback is not None:\n            self.callback.init_callback(self.model)\n\n    def _on_training_start(self) -> None:\n        if self.callback is not None:\n            self.callback.on_training_start(self.locals, self.globals)\n\n    def _on_event(self) -> bool:\n        if self.callback is not None:\n            return self.callback.on_step()\n        return True\n\n    def _on_step(self) -> bool:\n        return True\n\n\nclass CallbackList(BaseCallback):\n    """"""\n    Class for chaining callbacks.\n\n    :param callbacks: (List[BaseCallback]) A list of callbacks that will be called\n        sequentially.\n    """"""\n    def __init__(self, callbacks: List[BaseCallback]):\n        super(CallbackList, self).__init__()\n        assert isinstance(callbacks, list)\n        self.callbacks = callbacks\n\n    def _init_callback(self) -> None:\n        for callback in self.callbacks:\n            callback.init_callback(self.model)\n\n    def _on_training_start(self) -> None:\n        for callback in self.callbacks:\n            callback.on_training_start(self.locals, self.globals)\n\n    def _on_rollout_start(self) -> None:\n        for callback in self.callbacks:\n            callback.on_rollout_start()\n\n    def _on_step(self) -> bool:\n        continue_training = True\n        for callback in self.callbacks:\n            # Return False (stop training) if at least one callback returns False\n            continue_training = callback.on_step() and continue_training\n        return continue_training\n\n    def _on_rollout_end(self) -> None:\n        for callback in self.callbacks:\n            callback.on_rollout_end()\n\n    def _on_training_end(self) -> None:\n        for callback in self.callbacks:\n            callback.on_training_end()\n\n\nclass CheckpointCallback(BaseCallback):\n    """"""\n    Callback for saving a model every ``save_freq`` steps\n\n    :param save_freq: (int)\n    :param save_path: (str) Path to the folder where the model will be saved.\n    :param name_prefix: (str) Common prefix to the saved models\n    """"""\n    def __init__(self, save_freq: int, save_path: str, name_prefix=\'rl_model\', verbose=0):\n        super(CheckpointCallback, self).__init__(verbose)\n        self.save_freq = save_freq\n        self.save_path = save_path\n        self.name_prefix = name_prefix\n\n    def _init_callback(self) -> None:\n        # Create folder if needed\n        if self.save_path is not None:\n            os.makedirs(self.save_path, exist_ok=True)\n\n    def _on_step(self) -> bool:\n        if self.n_calls % self.save_freq == 0:\n            path = os.path.join(self.save_path, f\'{self.name_prefix}_{self.num_timesteps}_steps\')\n            self.model.save(path)\n            if self.verbose > 1:\n                print(f""Saving model checkpoint to {path}"")\n        return True\n\n\nclass ConvertCallback(BaseCallback):\n    """"""\n    Convert functional callback (old-style) to object.\n\n    :param callback: (callable)\n    :param verbose: (int)\n    """"""\n    def __init__(self, callback, verbose=0):\n        super(ConvertCallback, self).__init__(verbose)\n        self.callback = callback\n\n    def _on_step(self) -> bool:\n        if self.callback is not None:\n            return self.callback(self.locals, self.globals)\n        return True\n\n\nclass EvalCallback(EventCallback):\n    """"""\n    Callback for evaluating an agent.\n\n    :param eval_env: (Union[gym.Env, VecEnv]) The environment used for initialization\n    :param callback_on_new_best: (Optional[BaseCallback]) Callback to trigger\n        when there is a new best model according to the ``mean_reward``\n    :param n_eval_episodes: (int) The number of episodes to test the agent\n    :param eval_freq: (int) Evaluate the agent every eval_freq call of the callback.\n    :param log_path: (str) Path to a folder where the evaluations (``evaluations.npz``)\n        will be saved. It will be updated at each evaluation.\n    :param best_model_save_path: (str) Path to a folder where the best model\n        according to performance on the eval env will be saved.\n    :param deterministic: (bool) Whether the evaluation should\n        use a stochastic or deterministic actions.\n    :param deterministic: (bool) Whether to render or not the environment during evaluation\n    :param render: (bool) Whether to render or not the environment during evaluation\n    :param verbose: (int)\n    """"""\n    def __init__(self, eval_env: Union[gym.Env, VecEnv],\n                 callback_on_new_best: Optional[BaseCallback] = None,\n                 n_eval_episodes: int = 5,\n                 eval_freq: int = 10000,\n                 log_path: str = None,\n                 best_model_save_path: str = None,\n                 deterministic: bool = True,\n                 render: bool = False,\n                 verbose: int = 1):\n        super(EvalCallback, self).__init__(callback_on_new_best, verbose=verbose)\n        self.n_eval_episodes = n_eval_episodes\n        self.eval_freq = eval_freq\n        self.best_mean_reward = -np.inf\n        self.last_mean_reward = -np.inf\n        self.deterministic = deterministic\n        self.render = render\n\n        # Convert to VecEnv for consistency\n        if not isinstance(eval_env, VecEnv):\n            eval_env = DummyVecEnv([lambda: eval_env])\n\n        if isinstance(eval_env, VecEnv):\n            assert eval_env.num_envs == 1, ""You must pass only one environment for evaluation""\n\n        self.eval_env = eval_env\n        self.best_model_save_path = best_model_save_path\n        # Logs will be written in ``evaluations.npz``\n        if log_path is not None:\n            log_path = os.path.join(log_path, \'evaluations\')\n        self.log_path = log_path\n        self.evaluations_results = []\n        self.evaluations_timesteps = []\n        self.evaluations_length = []\n\n    def _init_callback(self):\n        # Does not work in some corner cases, where the wrapper is not the same\n        if not isinstance(self.training_env, type(self.eval_env)):\n            warnings.warn(""Training and eval env are not of the same type""\n                          f""{self.training_env} != {self.eval_env}"")\n\n        # Create folders if needed\n        if self.best_model_save_path is not None:\n            os.makedirs(self.best_model_save_path, exist_ok=True)\n        if self.log_path is not None:\n            os.makedirs(os.path.dirname(self.log_path), exist_ok=True)\n\n    def _on_step(self) -> bool:\n\n        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n            # Sync training and eval env if there is VecNormalize\n            sync_envs_normalization(self.training_env, self.eval_env)\n\n            episode_rewards, episode_lengths = evaluate_policy(self.model, self.eval_env,\n                                                               n_eval_episodes=self.n_eval_episodes,\n                                                               render=self.render,\n                                                               deterministic=self.deterministic,\n                                                               return_episode_rewards=True)\n\n            if self.log_path is not None:\n                self.evaluations_timesteps.append(self.num_timesteps)\n                self.evaluations_results.append(episode_rewards)\n                self.evaluations_length.append(episode_lengths)\n                np.savez(self.log_path, timesteps=self.evaluations_timesteps,\n                         results=self.evaluations_results, ep_lengths=self.evaluations_length)\n\n            mean_reward, std_reward = np.mean(episode_rewards), np.std(episode_rewards)\n            mean_ep_length, std_ep_length = np.mean(episode_lengths), np.std(episode_lengths)\n            self.last_mean_reward = mean_reward\n\n            if self.verbose > 0:\n                print(f""Eval num_timesteps={self.num_timesteps}, ""\n                      f""episode_reward={mean_reward:.2f} +/- {std_reward:.2f}"")\n                print(f""Episode length: {mean_ep_length:.2f} +/- {std_ep_length:.2f}"")\n            # Add to current Logger\n            self.logger.record(\'eval/mean_reward\', float(mean_reward))\n            self.logger.record(\'eval/mean_ep_length\', mean_ep_length)\n\n            if mean_reward > self.best_mean_reward:\n                if self.verbose > 0:\n                    print(""New best mean reward!"")\n                if self.best_model_save_path is not None:\n                    self.model.save(os.path.join(self.best_model_save_path, \'best_model\'))\n                self.best_mean_reward = mean_reward\n                # Trigger callback if needed\n                if self.callback is not None:\n                    return self._on_event()\n\n        return True\n\n\nclass StopTrainingOnRewardThreshold(BaseCallback):\n    """"""\n    Stop the training once a threshold in episodic reward\n    has been reached (i.e. when the model is good enough).\n\n    It must be used with the ``EvalCallback``.\n\n    :param reward_threshold: (float)  Minimum expected reward per episode\n        to stop training.\n    :param verbose: (int)\n    """"""\n    def __init__(self, reward_threshold: float, verbose: int = 0):\n        super(StopTrainingOnRewardThreshold, self).__init__(verbose=verbose)\n        self.reward_threshold = reward_threshold\n\n    def _on_step(self) -> bool:\n        assert self.parent is not None, (""``StopTrainingOnMinimumReward`` callback must be used ""\n                                         ""with an ``EvalCallback``"")\n        # Convert np.bool to bool, otherwise callback() is False won\'t work\n        continue_training = bool(self.parent.best_mean_reward < self.reward_threshold)\n        if self.verbose > 0 and not continue_training:\n            print(f""Stopping training because the mean reward {self.parent.best_mean_reward:.2f} ""\n                  f"" is above the threshold {self.reward_threshold}"")\n        return continue_training\n\n\nclass EveryNTimesteps(EventCallback):\n    """"""\n    Trigger a callback every ``n_steps``\xc2\xa0timesteps\n\n    :param n_steps: (int) Number of timesteps between two trigger.\n    :param callback: (BaseCallback) Callback that will be called\n        when the event is triggered.\n    """"""\n    def __init__(self, n_steps: int, callback: BaseCallback):\n        super(EveryNTimesteps, self).__init__(callback)\n        self.n_steps = n_steps\n        self.last_time_trigger = 0\n\n    def _on_step(self) -> bool:\n        if (self.num_timesteps - self.last_time_trigger) >= self.n_steps:\n            self.last_time_trigger = self.num_timesteps\n            return self._on_event()\n        return True\n'"
stable_baselines3/common/cmd_util.py,0,"b'import os\nimport warnings\nfrom typing import Dict, Any, Optional, Callable, Type, Union\n\nimport gym\n\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.atari_wrappers import AtariWrapper\nfrom stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n\n\ndef make_vec_env(env_id: Union[str, Type[gym.Env]],\n                 n_envs: int = 1,\n                 seed: Optional[int] = None,\n                 start_index: int = 0,\n                 monitor_dir: Optional[str] = None,\n                 wrapper_class: Optional[Callable] = None,\n                 env_kwargs: Optional[Dict[str, Any]] = None,\n                 vec_env_cls: Optional[Union[DummyVecEnv, SubprocVecEnv]] = None,\n                 vec_env_kwargs: Optional[Dict[str, Any]] = None):\n    """"""\n    Create a wrapped, monitored ``VecEnv``.\n    By default it uses a ``DummyVecEnv`` which is usually faster\n    than a ``SubprocVecEnv``.\n\n    :param env_id: (str or Type[gym.Env]) the environment ID or the environment class\n    :param n_envs: (int) the number of environments you wish to have in parallel\n    :param seed: (int) the initial seed for the random number generator\n    :param start_index: (int) start rank index\n    :param monitor_dir: (str) Path to a folder where the monitor files will be saved.\n        If None, no file will be written, however, the env will still be wrapped\n        in a Monitor wrapper to provide additional information about training.\n    :param wrapper_class: (gym.Wrapper or callable) Additional wrapper to use on the environment.\n        This can also be a function with single argument that wraps the environment in many things.\n    :param env_kwargs: (dict) Optional keyword argument to pass to the env constructor\n    :param vec_env_cls: (Type[VecEnv]) A custom ``VecEnv`` class constructor. Default: None.\n    :param vec_env_kwargs: (dict) Keyword arguments to pass to the ``VecEnv`` class constructor.\n    :return: (VecEnv) The wrapped environment\n    """"""\n    env_kwargs = {} if env_kwargs is None else env_kwargs\n    vec_env_kwargs = {} if vec_env_kwargs is None else vec_env_kwargs\n\n    def make_env(rank):\n        def _init():\n            if isinstance(env_id, str):\n                env = gym.make(env_id)\n                if len(env_kwargs) > 0:\n                    warnings.warn(""No environment class was passed (only an env ID) so ``env_kwargs`` will be ignored"")\n            else:\n                env = env_id(**env_kwargs)\n            if seed is not None:\n                env.seed(seed + rank)\n                env.action_space.seed(seed + rank)\n            # Wrap the env in a Monitor wrapper\n            # to have additional training information\n            monitor_path = os.path.join(monitor_dir, str(rank)) if monitor_dir is not None else None\n            # Create the monitor folder if needed\n            if monitor_path is not None:\n                os.makedirs(monitor_dir, exist_ok=True)\n            env = Monitor(env, filename=monitor_path)\n            # Optionally, wrap the environment with the provided wrapper\n            if wrapper_class is not None:\n                env = wrapper_class(env)\n            return env\n        return _init\n\n    # No custom VecEnv is passed\n    if vec_env_cls is None:\n        # Default: use a DummyVecEnv\n        vec_env_cls = DummyVecEnv\n\n    return vec_env_cls([make_env(i + start_index) for i in range(n_envs)], **vec_env_kwargs)\n\n\ndef make_atari_env(env_id: Union[str, Type[gym.Env]],\n                   n_envs: int = 1,\n                   seed: Optional[int] = None,\n                   start_index: int = 0,\n                   monitor_dir: Optional[str] = None,\n                   wrapper_kwargs: Optional[Dict[str, Any]] = None,\n                   env_kwargs: Optional[Dict[str, Any]] = None,\n                   vec_env_cls: Optional[Union[DummyVecEnv, SubprocVecEnv]] = None,\n                   vec_env_kwargs: Optional[Dict[str, Any]] = None):\n    """"""\n    Create a wrapped, monitored VecEnv for Atari.\n    It is a wrapper around ``make_vec_env`` that includes common preprocessing for Atari games.\n\n    :param env_id: (str or Type[gym.Env]) the environment ID or the environment class\n    :param n_envs: (int) the number of environments you wish to have in parallel\n    :param seed: (int) the initial seed for the random number generator\n    :param start_index: (int) start rank index\n    :param monitor_dir: (str) Path to a folder where the monitor files will be saved.\n        If None, no file will be written, however, the env will still be wrapped\n        in a Monitor wrapper to provide additional information about training.\n    :param wrapper_kwargs: (Dict[str, Any]) Optional keyword argument to pass to the ``AtariWrapper``\n    :param env_kwargs: (Dict[str, Any]) Optional keyword argument to pass to the env constructor\n    :param vec_env_cls: (Type[VecEnv]) A custom `VecEnv` class constructor. Default: None.\n    :param vec_env_kwargs: (Dict[str, Any]) Keyword arguments to pass to the `VecEnv` class constructor.\n    :return: (VecEnv) The wrapped environment\n    """"""\n    if wrapper_kwargs is None:\n        wrapper_kwargs = {}\n\n    def atari_wrapper(env: gym.Env) -> gym.Env:\n        env = AtariWrapper(env, **wrapper_kwargs)\n        return env\n\n    return make_vec_env(env_id, n_envs=n_envs, seed=seed, start_index=start_index,\n                        monitor_dir=monitor_dir, wrapper_class=atari_wrapper,\n                        env_kwargs=env_kwargs, vec_env_cls=vec_env_cls, vec_env_kwargs=vec_env_kwargs)\n'"
stable_baselines3/common/distributions.py,4,"b'from typing import Optional, Tuple, Dict, Any, List\nimport gym\nimport torch as th\nimport torch.nn as nn\nfrom torch.distributions import Normal, Categorical, Bernoulli\nfrom gym import spaces\n\nfrom stable_baselines3.common.preprocessing import get_action_dim\n\n\nclass Distribution(object):\n    def __init__(self):\n        super(Distribution, self).__init__()\n\n    def log_prob(self, x: th.Tensor) -> th.Tensor:\n        """"""\n        returns the log likelihood\n\n        :param x: (th.Tensor) the taken action\n        :return: (th.Tensor) The log likelihood of the distribution\n        """"""\n        raise NotImplementedError\n\n    def entropy(self) -> Optional[th.Tensor]:\n        """"""\n        Returns Shannon\'s entropy of the probability\n\n        :return: (Optional[th.Tensor]) the entropy,\n            return None if no analytical form is known\n        """"""\n        raise NotImplementedError\n\n    def sample(self) -> th.Tensor:\n        """"""\n        Returns a sample from the probability distribution\n\n        :return: (th.Tensor) the stochastic action\n        """"""\n        raise NotImplementedError\n\n    def mode(self) -> th.Tensor:\n        """"""\n        Returns the most likely action (deterministic output)\n        from the probability distribution\n\n        :return: (th.Tensor) the stochastic action\n        """"""\n        raise NotImplementedError\n\n    def get_actions(self, deterministic: bool = False) -> th.Tensor:\n        """"""\n        Return actions according to the probability distribution.\n\n        :param deterministic: (bool)\n        :return: (th.Tensor)\n        """"""\n        if deterministic:\n            return self.mode()\n        return self.sample()\n\n    def actions_from_params(self, *args, **kwargs) -> th.Tensor:\n        """"""\n        Returns samples from the probability distribution\n        given its parameters.\n\n        :return: (th.Tensor) actions\n        """"""\n        raise NotImplementedError\n\n    def log_prob_from_params(self, *args, **kwargs) -> Tuple[th.Tensor, th.Tensor]:\n        """"""\n        Returns samples and the associated log probabilities\n        from the probability distribution given its parameters.\n\n        :return: (th.Tuple[th.Tensor, th.Tensor]) actions and log prob\n        """"""\n        raise NotImplementedError\n\n\ndef sum_independent_dims(tensor: th.Tensor) -> th.Tensor:\n    """"""\n    Continuous actions are usually considered to be independent,\n    so we can sum the components for the ``log_prob``\n    or the entropy.\n\n    :param tensor: (th.Tensor) shape: (n_batch, n_actions) or (n_batch,)\n    :return: (th.Tensor) shape: (n_batch,)\n    """"""\n    if len(tensor.shape) > 1:\n        tensor = tensor.sum(dim=1)\n    else:\n        tensor = tensor.sum()\n    return tensor\n\n\nclass DiagGaussianDistribution(Distribution):\n    """"""\n    Gaussian distribution with diagonal covariance matrix,\n    for continuous actions.\n\n    :param action_dim: (int)  Dimension of the action space.\n    """"""\n\n    def __init__(self, action_dim: int):\n        super(DiagGaussianDistribution, self).__init__()\n        self.distribution = None\n        self.action_dim = action_dim\n        self.mean_actions = None\n        self.log_std = None\n\n    def proba_distribution_net(self, latent_dim: int,\n                               log_std_init: float = 0.0) -> Tuple[nn.Module, nn.Parameter]:\n        """"""\n        Create the layers and parameter that represent the distribution:\n        one output will be the mean of the Gaussian, the other parameter will be the\n        standard deviation (log std in fact to allow negative values)\n\n        :param latent_dim: (int) Dimension og the last layer of the policy (before the action layer)\n        :param log_std_init: (float) Initial value for the log standard deviation\n        :return: (nn.Linear, nn.Parameter)\n        """"""\n        mean_actions = nn.Linear(latent_dim, self.action_dim)\n        # TODO: allow action dependent std\n        log_std = nn.Parameter(th.ones(self.action_dim) * log_std_init, requires_grad=True)\n        return mean_actions, log_std\n\n    def proba_distribution(self, mean_actions: th.Tensor,\n                           log_std: th.Tensor) -> \'DiagGaussianDistribution\':\n        """"""\n        Create the distribution given its parameters (mean, std)\n\n        :param mean_actions: (th.Tensor)\n        :param log_std: (th.Tensor)\n        :return: (DiagGaussianDistribution)\n        """"""\n        action_std = th.ones_like(mean_actions) * log_std.exp()\n        self.distribution = Normal(mean_actions, action_std)\n        return self\n\n    def mode(self) -> th.Tensor:\n        return self.distribution.mean\n\n    def sample(self) -> th.Tensor:\n        # Reparametrization trick to pass gradients\n        return self.distribution.rsample()\n\n    def entropy(self) -> th.Tensor:\n        return sum_independent_dims(self.distribution.entropy())\n\n    def actions_from_params(self, mean_actions: th.Tensor,\n                            log_std: th.Tensor,\n                            deterministic: bool = False) -> th.Tensor:\n        # Update the proba distribution\n        self.proba_distribution(mean_actions, log_std)\n        return self.get_actions(deterministic=deterministic)\n\n    def log_prob_from_params(self, mean_actions: th.Tensor,\n                             log_std: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n        """"""\n        Compute the log probability of taking an action\n        given the distribution parameters.\n\n        :param mean_actions: (th.Tensor)\n        :param log_std: (th.Tensor)\n        :return: (Tuple[th.Tensor, th.Tensor])\n        """"""\n        actions = self.actions_from_params(mean_actions, log_std)\n        log_prob = self.log_prob(actions)\n        return actions, log_prob\n\n    def log_prob(self, actions: th.Tensor) -> th.Tensor:\n        """"""\n        Get the log probabilities of actions according to the distribution.\n        Note that you must call ``proba_distribution()`` method before.\n\n        :param actions: (th.Tensor)\n        :return: (th.Tensor)\n        """"""\n        log_prob = self.distribution.log_prob(actions)\n        return sum_independent_dims(log_prob)\n\n\nclass SquashedDiagGaussianDistribution(DiagGaussianDistribution):\n    """"""\n    Gaussian distribution with diagonal covariance matrix,\n    followed by a squashing function (tanh) to ensure bounds.\n\n    :param action_dim: (int) Dimension of the action space.\n    :param epsilon: (float) small value to avoid NaN due to numerical imprecision.\n    """"""\n\n    def __init__(self, action_dim: int, epsilon: float = 1e-6):\n        super(SquashedDiagGaussianDistribution, self).__init__(action_dim)\n        # Avoid NaN (prevents division by zero or log of zero)\n        self.epsilon = epsilon\n        self.gaussian_actions = None\n\n    def proba_distribution(self, mean_actions: th.Tensor,\n                           log_std: th.Tensor) -> \'SquashedDiagGaussianDistribution\':\n        super(SquashedDiagGaussianDistribution, self).proba_distribution(mean_actions, log_std)\n        return self\n\n    def mode(self) -> th.Tensor:\n        self.gaussian_actions = self.distribution.mean\n        # Squash the output\n        return th.tanh(self.gaussian_actions)\n\n    def entropy(self) -> Optional[th.Tensor]:\n        # No analytical form,\n        # entropy needs to be estimated using -log_prob.mean()\n        return None\n\n    def sample(self) -> th.Tensor:\n        # Reparametrization trick to pass gradients\n        self.gaussian_actions = self.distribution.rsample()\n        return th.tanh(self.gaussian_actions)\n\n    def log_prob_from_params(self, mean_actions: th.Tensor,\n                             log_std: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n        action = self.actions_from_params(mean_actions, log_std)\n        log_prob = self.log_prob(action, self.gaussian_actions)\n        return action, log_prob\n\n    def log_prob(self, actions: th.Tensor,\n                 gaussian_actions: Optional[th.Tensor] = None) -> th.Tensor:\n        # Inverse tanh\n        # Naive implementation (not stable): 0.5 * torch.log((1 + x) / (1 - x))\n        # We use numpy to avoid numerical instability\n        if gaussian_actions is None:\n            # It will be clipped to avoid NaN when inversing tanh\n            gaussian_actions = TanhBijector.inverse(actions)\n\n        # Log likelihood for a Gaussian distribution\n        log_prob = super(SquashedDiagGaussianDistribution, self).log_prob(gaussian_actions)\n        # Squash correction (from original SAC implementation)\n        # this comes from the fact that tanh is bijective and differentiable\n        log_prob -= th.sum(th.log(1 - actions ** 2 + self.epsilon), dim=1)\n        return log_prob\n\n\nclass CategoricalDistribution(Distribution):\n    """"""\n    Categorical distribution for discrete actions.\n\n    :param action_dim: (int) Number of discrete actions\n    """"""\n\n    def __init__(self, action_dim: int):\n        super(CategoricalDistribution, self).__init__()\n        self.distribution = None\n        self.action_dim = action_dim\n\n    def proba_distribution_net(self, latent_dim: int) -> nn.Module:\n        """"""\n        Create the layer that represents the distribution:\n        it will be the logits of the Categorical distribution.\n        You can then get probabilities using a softmax.\n\n        :param latent_dim: (int) Dimension of the last layer\n            of the policy network (before the action layer)\n        :return: (nn.Linear)\n        """"""\n        action_logits = nn.Linear(latent_dim, self.action_dim)\n        return action_logits\n\n    def proba_distribution(self, action_logits: th.Tensor) -> \'CategoricalDistribution\':\n        self.distribution = Categorical(logits=action_logits)\n        return self\n\n    def mode(self) -> th.Tensor:\n        return th.argmax(self.distribution.probs, dim=1)\n\n    def sample(self) -> th.Tensor:\n        return self.distribution.sample()\n\n    def entropy(self) -> th.Tensor:\n        return self.distribution.entropy()\n\n    def actions_from_params(self, action_logits: th.Tensor,\n                            deterministic: bool = False) -> th.Tensor:\n        # Update the proba distribution\n        self.proba_distribution(action_logits)\n        return self.get_actions(deterministic=deterministic)\n\n    def log_prob_from_params(self, action_logits: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n        actions = self.actions_from_params(action_logits)\n        log_prob = self.log_prob(actions)\n        return actions, log_prob\n\n    def log_prob(self, actions: th.Tensor) -> th.Tensor:\n        return self.distribution.log_prob(actions)\n\n\nclass MultiCategoricalDistribution(Distribution):\n    """"""\n    MultiCategorical distribution for multi discrete actions.\n\n    :param action_dims: (List[int]) List of sizes of discrete action spaces\n    """"""\n\n    def __init__(self, action_dims: List[int]):\n        super(MultiCategoricalDistribution, self).__init__()\n        self.action_dims = action_dims\n        self.distributions = None\n\n    def proba_distribution_net(self, latent_dim: int) -> nn.Module:\n        """"""\n        Create the layer that represents the distribution:\n        it will be the logits (flattened) of the MultiCategorical distribution.\n        You can then get probabilities using a softmax on each sub-space.\n\n        :param latent_dim: (int) Dimension of the last layer\n            of the policy network (before the action layer)\n        :return: (nn.Linear)\n        """"""\n\n        action_logits = nn.Linear(latent_dim, sum(self.action_dims))\n        return action_logits\n\n    def proba_distribution(self, action_logits: th.Tensor) -> \'MultiCategoricalDistribution\':\n        self.distributions = [Categorical(logits=split) for split in th.split(action_logits, tuple(self.action_dims), dim=1)]\n        return self\n\n    def mode(self) -> th.Tensor:\n        return th.stack([th.argmax(dist.probs, dim=1) for dist in self.distributions], dim=1)\n\n    def sample(self) -> th.Tensor:\n        return th.stack([dist.sample() for dist in self.distributions], dim=1)\n\n    def entropy(self) -> th.Tensor:\n        return th.stack([dist.entropy() for dist in self.distributions], dim=1).sum(dim=1)\n\n    def actions_from_params(self, action_logits: th.Tensor,\n                            deterministic: bool = False) -> th.Tensor:\n        # Update the proba distribution\n        self.proba_distribution(action_logits)\n        return self.get_actions(deterministic=deterministic)\n\n    def log_prob_from_params(self, action_logits: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n        actions = self.actions_from_params(action_logits)\n        log_prob = self.log_prob(actions)\n        return actions, log_prob\n\n    def log_prob(self, actions: th.Tensor) -> th.Tensor:\n        # Extract each discrete action and compute log prob for their respective distributions\n        return th.stack([dist.log_prob(action) for dist, action in zip(self.distributions,\n                                                                       th.unbind(actions, dim=1))], dim=1).sum(dim=1)\n\n\nclass BernoulliDistribution(Distribution):\n    """"""\n    Bernoulli distribution for MultiBinary action spaces.\n\n    :param action_dim: (int) Number of binary actions\n    """"""\n\n    def __init__(self, action_dims: int):\n        super(BernoulliDistribution, self).__init__()\n        self.distribution = None\n        self.action_dims = action_dims\n\n    def proba_distribution_net(self, latent_dim: int) -> nn.Module:\n        """"""\n        Create the layer that represents the distribution:\n        it will be the logits of the Bernoulli distribution.\n\n        :param latent_dim: (int) Dimension of the last layer\n            of the policy network (before the action layer)\n        :return: (nn.Linear)\n        """"""\n        action_logits = nn.Linear(latent_dim, self.action_dims)\n        return action_logits\n\n    def proba_distribution(self, action_logits: th.Tensor) -> \'BernoulliDistribution\':\n        self.distribution = Bernoulli(logits=action_logits)\n        return self\n\n    def mode(self) -> th.Tensor:\n        return th.round(self.distribution.probs)\n\n    def sample(self) -> th.Tensor:\n        return self.distribution.sample()\n\n    def entropy(self) -> th.Tensor:\n        return self.distribution.entropy().sum(dim=1)\n\n    def actions_from_params(self, action_logits: th.Tensor,\n                            deterministic: bool = False) -> th.Tensor:\n        # Update the proba distribution\n        self.proba_distribution(action_logits)\n        return self.get_actions(deterministic=deterministic)\n\n    def log_prob_from_params(self, action_logits: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n        actions = self.actions_from_params(action_logits)\n        log_prob = self.log_prob(actions)\n        return actions, log_prob\n\n    def log_prob(self, actions: th.Tensor) -> th.Tensor:\n        return self.distribution.log_prob(actions).sum(dim=1)\n\n\nclass StateDependentNoiseDistribution(Distribution):\n    """"""\n    Distribution class for using generalized State Dependent Exploration (gSDE).\n    Paper: https://arxiv.org/abs/2005.05719\n\n    It is used to create the noise exploration matrix and\n    compute the log probability of an action with that noise.\n\n    :param action_dim: (int) Dimension of the action space.\n    :param full_std: (bool) Whether to use (n_features x n_actions) parameters\n        for the std instead of only (n_features,)\n    :param use_expln: (bool) Use ``expln()`` function instead of ``exp()`` to ensure\n        a positive standard deviation (cf paper). It allows to keep variance\n        above zero and prevent it from growing too fast. In practice, ``exp()`` is usually enough.\n    :param squash_output: (bool) Whether to squash the output using a tanh function,\n        this allows to ensure boundaries.\n    :param learn_features: (bool) Whether to learn features for gSDE or not.\n        This will enable gradients to be backpropagated through the features\n        ``latent_sde`` in the code.\n    :param epsilon: (float) small value to avoid NaN due to numerical imprecision.\n    """"""\n\n    def __init__(self, action_dim: int,\n                 full_std: bool = True,\n                 use_expln: bool = False,\n                 squash_output: bool = False,\n                 learn_features: bool = False,\n                 epsilon: float = 1e-6):\n        super(StateDependentNoiseDistribution, self).__init__()\n        self.distribution = None\n        self.action_dim = action_dim\n        self.latent_sde_dim = None\n        self.mean_actions = None\n        self.log_std = None\n        self.weights_dist = None\n        self.exploration_mat = None\n        self.exploration_matrices = None\n        self._latent_sde = None\n        self.use_expln = use_expln\n        self.full_std = full_std\n        self.epsilon = epsilon\n        self.learn_features = learn_features\n        if squash_output:\n            self.bijector = TanhBijector(epsilon)\n        else:\n            self.bijector = None\n\n    def get_std(self, log_std: th.Tensor) -> th.Tensor:\n        """"""\n        Get the standard deviation from the learned parameter\n        (log of it by default). This ensures that the std is positive.\n\n        :param log_std: (th.Tensor)\n        :return: (th.Tensor)\n        """"""\n        if self.use_expln:\n            # From gSDE paper, it allows to keep variance\n            # above zero and prevent it from growing too fast\n            below_threshold = th.exp(log_std) * (log_std <= 0)\n            # Avoid NaN: zeros values that are below zero\n            safe_log_std = log_std * (log_std > 0) + self.epsilon\n            above_threshold = (th.log1p(safe_log_std) + 1.0) * (log_std > 0)\n            std = below_threshold + above_threshold\n        else:\n            # Use normal exponential\n            std = th.exp(log_std)\n\n        if self.full_std:\n            return std\n        # Reduce the number of parameters:\n        return th.ones(self.latent_sde_dim, self.action_dim).to(log_std.device) * std\n\n    def sample_weights(self, log_std: th.Tensor, batch_size: int = 1) -> None:\n        """"""\n        Sample weights for the noise exploration matrix,\n        using a centered Gaussian distribution.\n\n        :param log_std: (th.Tensor)\n        :param batch_size: (int)\n        """"""\n        std = self.get_std(log_std)\n        self.weights_dist = Normal(th.zeros_like(std), std)\n        # Reparametrization trick to pass gradients\n        self.exploration_mat = self.weights_dist.rsample()\n        # Pre-compute matrices in case of parallel exploration\n        self.exploration_matrices = self.weights_dist.rsample((batch_size,))\n\n    def proba_distribution_net(self, latent_dim: int, log_std_init: float = -2.0,\n                               latent_sde_dim: Optional[int] = None) -> Tuple[nn.Module, nn.Parameter]:\n        """"""\n        Create the layers and parameter that represent the distribution:\n        one output will be the deterministic action, the other parameter will be the\n        standard deviation of the distribution that control the weights of the noise matrix.\n\n        :param latent_dim: (int) Dimension of the last layer of the policy (before the action layer)\n        :param log_std_init: (float) Initial value for the log standard deviation\n        :param latent_sde_dim: (Optional[int]) Dimension of the last layer of the feature extractor\n            for gSDE. By default, it is shared with the policy network.\n        :return: (nn.Linear, nn.Parameter)\n        """"""\n        # Network for the deterministic action, it represents the mean of the distribution\n        mean_actions_net = nn.Linear(latent_dim, self.action_dim)\n        # When we learn features for the noise, the feature dimension\n        # can be different between the policy and the noise network\n        self.latent_sde_dim = latent_dim if latent_sde_dim is None else latent_sde_dim\n        # Reduce the number of parameters if needed\n        log_std = th.ones(self.latent_sde_dim, self.action_dim) if self.full_std else th.ones(self.latent_sde_dim, 1)\n        # Transform it to a parameter so it can be optimized\n        log_std = nn.Parameter(log_std * log_std_init, requires_grad=True)\n        # Sample an exploration matrix\n        self.sample_weights(log_std)\n        return mean_actions_net, log_std\n\n    def proba_distribution(self, mean_actions: th.Tensor,\n                           log_std: th.Tensor,\n                           latent_sde: th.Tensor) -> \'StateDependentNoiseDistribution\':\n        """"""\n        Create the distribution given its parameters (mean, std)\n\n        :param mean_actions: (th.Tensor)\n        :param log_std: (th.Tensor)\n        :param latent_sde: (th.Tensor)\n        :return: (StateDependentNoiseDistribution)\n        """"""\n        # Stop gradient if we don\'t want to influence the features\n        self._latent_sde = latent_sde if self.learn_features else latent_sde.detach()\n        variance = th.mm(latent_sde ** 2, self.get_std(log_std) ** 2)\n        self.distribution = Normal(mean_actions, th.sqrt(variance + self.epsilon))\n        return self\n\n    def mode(self) -> th.Tensor:\n        actions = self.distribution.mean\n        if self.bijector is not None:\n            return self.bijector.forward(actions)\n        return actions\n\n    def get_noise(self, latent_sde: th.Tensor) -> th.Tensor:\n        latent_sde = latent_sde if self.learn_features else latent_sde.detach()\n        # Default case: only one exploration matrix\n        if len(latent_sde) == 1 or len(latent_sde) != len(self.exploration_matrices):\n            return th.mm(latent_sde, self.exploration_mat)\n        # Use batch matrix multiplication for efficient computation\n        # (batch_size, n_features) -> (batch_size, 1, n_features)\n        latent_sde = latent_sde.unsqueeze(1)\n        # (batch_size, 1, n_actions)\n        noise = th.bmm(latent_sde, self.exploration_matrices)\n        return noise.squeeze(1)\n\n    def sample(self) -> th.Tensor:\n        noise = self.get_noise(self._latent_sde)\n        actions = self.distribution.mean + noise\n        if self.bijector is not None:\n            return self.bijector.forward(actions)\n        return actions\n\n    def entropy(self) -> Optional[th.Tensor]:\n        # No analytical form,\n        # entropy needs to be estimated using -log_prob.mean()\n        if self.bijector is not None:\n            return None\n        return sum_independent_dims(self.distribution.entropy())\n\n    def actions_from_params(self, mean_actions: th.Tensor,\n                            log_std: th.Tensor,\n                            latent_sde: th.Tensor,\n                            deterministic: bool = False) -> th.Tensor:\n        # Update the proba distribution\n        self.proba_distribution(mean_actions, log_std, latent_sde)\n        return self.get_actions(deterministic=deterministic)\n\n    def log_prob_from_params(self, mean_actions: th.Tensor,\n                             log_std: th.Tensor,\n                             latent_sde: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n        actions = self.actions_from_params(mean_actions, log_std, latent_sde)\n        log_prob = self.log_prob(actions)\n        return actions, log_prob\n\n    def log_prob(self, actions: th.Tensor) -> th.Tensor:\n        if self.bijector is not None:\n            gaussian_actions = self.bijector.inverse(actions)\n        else:\n            gaussian_actions = actions\n        # log likelihood for a gaussian\n        log_prob = self.distribution.log_prob(gaussian_actions)\n        # Sum along action dim\n        log_prob = sum_independent_dims(log_prob)\n\n        if self.bijector is not None:\n            # Squash correction (from original SAC implementation)\n            log_prob -= th.sum(self.bijector.log_prob_correction(gaussian_actions), dim=1)\n        return log_prob\n\n\nclass TanhBijector(object):\n    """"""\n    Bijective transformation of a probability distribution\n    using a squashing function (tanh)\n    TODO: use Pyro instead (https://pyro.ai/)\n\n    :param epsilon: (float) small value to avoid NaN due to numerical imprecision.\n    """"""\n\n    def __init__(self, epsilon: float = 1e-6):\n        super(TanhBijector, self).__init__()\n        self.epsilon = epsilon\n\n    @staticmethod\n    def forward(x: th.Tensor) -> th.Tensor:\n        return th.tanh(x)\n\n    @staticmethod\n    def atanh(x: th.Tensor) -> th.Tensor:\n        """"""\n        Inverse of Tanh\n\n        Taken from pyro: https://github.com/pyro-ppl/pyro\n        0.5 * torch.log((1 + x ) / (1 - x))\n        """"""\n        return 0.5 * (x.log1p() - (-x).log1p())\n\n    @staticmethod\n    def inverse(y: th.Tensor) -> th.Tensor:\n        """"""\n        Inverse tanh.\n\n        :param y: (th.Tensor)\n        :return: (th.Tensor)\n        """"""\n        eps = th.finfo(y.dtype).eps\n        # Clip the action to avoid NaN\n        return TanhBijector.atanh(y.clamp(min=-1. + eps, max=1. - eps))\n\n    def log_prob_correction(self, x: th.Tensor) -> th.Tensor:\n        # Squash correction (from original SAC implementation)\n        return th.log(1.0 - th.tanh(x) ** 2 + self.epsilon)\n\n\ndef make_proba_distribution(action_space: gym.spaces.Space,\n                            use_sde: bool = False,\n                            dist_kwargs: Optional[Dict[str, Any]] = None) -> Distribution:\n    """"""\n    Return an instance of Distribution for the correct type of action space\n\n    :param action_space: (gym.spaces.Space) the input action space\n    :param use_sde: (bool) Force the use of StateDependentNoiseDistribution\n        instead of DiagGaussianDistribution\n    :param dist_kwargs: (Optional[Dict[str, Any]]) Keyword arguments to pass to the probability distribution\n    :return: (Distribution) the appropriate Distribution object\n    """"""\n    if dist_kwargs is None:\n        dist_kwargs = {}\n\n    if isinstance(action_space, spaces.Box):\n        assert len(action_space.shape) == 1, ""Error: the action space must be a vector""\n        if use_sde:\n            return StateDependentNoiseDistribution(get_action_dim(action_space), **dist_kwargs)\n        return DiagGaussianDistribution(get_action_dim(action_space), **dist_kwargs)\n    elif isinstance(action_space, spaces.Discrete):\n        return CategoricalDistribution(action_space.n, **dist_kwargs)\n    elif isinstance(action_space, spaces.MultiDiscrete):\n        return MultiCategoricalDistribution(action_space.nvec, **dist_kwargs)\n    elif isinstance(action_space, spaces.MultiBinary):\n        return BernoulliDistribution(action_space.n, **dist_kwargs)\n    else:\n        raise NotImplementedError(""Error: probability distribution, not implemented for action space""\n                                  f""of type {type(action_space)}.""\n                                  "" Must be of type Gym Spaces: Box, Discrete, MultiDiscrete or MultiBinary."")\n'"
stable_baselines3/common/env_checker.py,0,"b'import warnings\nfrom typing import Union\n\nimport gym\nfrom gym import spaces\nimport numpy as np\n\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecCheckNan\n\n\ndef _enforce_array_obs(observation_space: spaces.Space) -> bool:\n    """"""\n    Whether to check that the returned observation is a numpy array\n    it is not mandatory for `Dict` and `Tuple` spaces.\n    """"""\n    return not isinstance(observation_space, (spaces.Dict, spaces.Tuple))\n\n\ndef _check_image_input(observation_space: spaces.Box) -> None:\n    """"""\n    Check that the input will be compatible with Stable-Baselines\n    when the observation is apparently an image.\n    """"""\n    if observation_space.dtype != np.uint8:\n        warnings.warn(""It seems that your observation is an image but the `dtype` ""\n                      ""of your observation_space is not `np.uint8`. ""\n                      ""If your observation is not an image, we recommend you to flatten the observation ""\n                      ""to have only a 1D vector"")\n\n    if np.any(observation_space.low != 0) or np.any(observation_space.high != 255):\n        warnings.warn(""It seems that your observation space is an image but the ""\n                      ""upper and lower bounds are not in [0, 255]. ""\n                      ""Because the CNN policy normalize automatically the observation ""\n                      ""you may encounter issue if the values are not in that range.""\n                      )\n\n    if observation_space.shape[0] < 36 or observation_space.shape[1] < 36:\n        warnings.warn(""The minimal resolution for an image is 36x36 for the default CnnPolicy. ""\n                      ""You might need to use a custom `cnn_extractor` ""\n                      ""cf https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html"")\n\n\ndef _check_unsupported_obs_spaces(env: gym.Env, observation_space: spaces.Space) -> None:\n    """"""Emit warnings when the observation space used is not supported by Stable-Baselines.""""""\n\n    if isinstance(observation_space, spaces.Dict) and not isinstance(env, gym.GoalEnv):\n        warnings.warn(""The observation space is a Dict but the environment is not a gym.GoalEnv ""\n                      ""(cf https://github.com/openai/gym/blob/master/gym/core.py), ""\n                      ""this is currently not supported by Stable Baselines ""\n                      ""(cf https://github.com/hill-a/stable-baselines/issues/133), ""\n                      ""you will need to use a custom policy. ""\n                      )\n\n    if isinstance(observation_space, spaces.Tuple):\n        warnings.warn(""The observation space is a Tuple,""\n                      ""this is currently not supported by Stable Baselines ""\n                      ""(cf https://github.com/hill-a/stable-baselines/issues/133), ""\n                      ""you will need to flatten the observation and maybe use a custom policy. ""\n                      )\n\n\ndef _check_nan(env: gym.Env) -> None:\n    """"""Check for Inf and NaN using the VecWrapper.""""""\n    vec_env = VecCheckNan(DummyVecEnv([lambda: env]))\n    for _ in range(10):\n        action = [env.action_space.sample()]\n        _, _, _, _ = vec_env.step(action)\n\n\ndef _check_obs(obs: Union[tuple, dict, np.ndarray, int],\n               observation_space: spaces.Space,\n               method_name: str) -> None:\n    """"""\n    Check that the observation returned by the environment\n    correspond to the declared one.\n    """"""\n    if not isinstance(observation_space, spaces.Tuple):\n        assert not isinstance(obs, tuple), (""The observation returned by the `{}()` ""\n                                            ""method should be a single value, not a tuple"".format(method_name))\n\n    # The check for a GoalEnv is done by the base class\n    if isinstance(observation_space, spaces.Discrete):\n        assert isinstance(obs, int), ""The observation returned by `{}()` method must be an int"".format(method_name)\n    elif _enforce_array_obs(observation_space):\n        assert isinstance(obs, np.ndarray), (""The observation returned by `{}()` ""\n                                             ""method must be a numpy array"".format(method_name))\n\n    assert observation_space.contains(obs), (""The observation returned by the `{}()` ""\n                                             ""method does not match the given observation space"".format(method_name))\n\n\ndef _check_returned_values(env: gym.Env, observation_space: spaces.Space, action_space: spaces.Space) -> None:\n    """"""\n    Check the returned values by the env when calling `.reset()` or `.step()` methods.\n    """"""\n    # because env inherits from gym.Env, we assume that `reset()` and `step()` methods exists\n    obs = env.reset()\n\n    _check_obs(obs, observation_space, \'reset\')\n\n    # Sample a random action\n    action = action_space.sample()\n    data = env.step(action)\n\n    assert len(data) == 4, ""The `step()` method must return four values: obs, reward, done, info""\n\n    # Unpack\n    obs, reward, done, info = data\n\n    _check_obs(obs, observation_space, \'step\')\n\n    # We also allow int because the reward will be cast to float\n    assert isinstance(reward, (float, int)), ""The reward returned by `step()` must be a float""\n    assert isinstance(done, bool), ""The `done` signal must be a boolean""\n    assert isinstance(info, dict), ""The `info` returned by `step()` must be a python dictionary""\n\n    if isinstance(env, gym.GoalEnv):\n        # For a GoalEnv, the keys are checked at reset\n        assert reward == env.compute_reward(obs[\'achieved_goal\'], obs[\'desired_goal\'], info)\n\n\ndef _check_spaces(env: gym.Env) -> None:\n    """"""\n    Check that the observation and action spaces are defined\n    and inherit from gym.spaces.Space.\n    """"""\n    # Helper to link to the code, because gym has no proper documentation\n    gym_spaces = "" cf https://github.com/openai/gym/blob/master/gym/spaces/""\n\n    assert hasattr(env, \'observation_space\'), ""You must specify an observation space (cf gym.spaces)"" + gym_spaces\n    assert hasattr(env, \'action_space\'), ""You must specify an action space (cf gym.spaces)"" + gym_spaces\n\n    assert isinstance(env.observation_space,\n                      spaces.Space), ""The observation space must inherit from gym.spaces"" + gym_spaces\n    assert isinstance(env.action_space, spaces.Space), ""The action space must inherit from gym.spaces"" + gym_spaces\n\n\ndef _check_render(env: gym.Env, warn: bool = True, headless: bool = False) -> None:\n    """"""\n    Check the declared render modes and the `render()`/`close()`\n    method of the environment.\n\n    :param env: (gym.Env) The environment to check\n    :param warn: (bool) Whether to output additional warnings\n    :param headless: (bool) Whether to disable render modes\n        that require a graphical interface. False by default.\n    """"""\n    render_modes = env.metadata.get(\'render.modes\')\n    if render_modes is None:\n        if warn:\n            warnings.warn(""No render modes was declared in the environment ""\n                          "" (env.metadata[\'render.modes\'] is None or not defined), ""\n                          ""you may have trouble when calling `.render()`"")\n\n    else:\n        # Don\'t check render mode that require a\n        # graphical interface (useful for CI)\n        if headless and \'human\' in render_modes:\n            render_modes.remove(\'human\')\n        # Check all declared render modes\n        for render_mode in render_modes:\n            env.render(mode=render_mode)\n        env.close()\n\n\ndef check_env(env: gym.Env, warn: bool = True, skip_render_check: bool = True) -> None:\n    """"""\n    Check that an environment follows Gym API.\n    This is particularly useful when using a custom environment.\n    Please take a look at https://github.com/openai/gym/blob/master/gym/core.py\n    for more information about the API.\n\n    It also optionally check that the environment is compatible with Stable-Baselines.\n\n    :param env: (gym.Env) The Gym environment that will be checked\n    :param warn: (bool) Whether to output additional warnings\n        mainly related to the interaction with Stable Baselines\n    :param skip_render_check: (bool) Whether to skip the checks for the render method.\n        True by default (useful for the CI)\n    """"""\n    assert isinstance(env, gym.Env), (""You environment must inherit from gym.Env class ""\n                                      "" cf https://github.com/openai/gym/blob/master/gym/core.py"")\n\n    # ============= Check the spaces (observation and action) ================\n    _check_spaces(env)\n\n    # Define aliases for convenience\n    observation_space = env.observation_space\n    action_space = env.action_space\n\n    # Warn the user if needed.\n    # A warning means that the environment may run but not work properly with Stable Baselines algorithms\n    if warn:\n        _check_unsupported_obs_spaces(env, observation_space)\n\n        # If image, check the low and high values, the type and the number of channels\n        # and the shape (minimal value)\n        if isinstance(observation_space, spaces.Box) and len(observation_space.shape) == 3:\n            _check_image_input(observation_space)\n\n        if isinstance(observation_space, spaces.Box) and len(observation_space.shape) not in [1, 3]:\n            warnings.warn(""Your observation has an unconventional shape (neither an image, nor a 1D vector). ""\n                          ""We recommend you to flatten the observation ""\n                          ""to have only a 1D vector"")\n\n        # Check for the action space, it may lead to hard-to-debug issues\n        if (isinstance(action_space, spaces.Box) and\n                (np.any(np.abs(action_space.low) != np.abs(action_space.high))\n                 or np.any(np.abs(action_space.low) > 1) or np.any(np.abs(action_space.high) > 1))):\n            warnings.warn(""We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) ""\n                          ""cf https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html"")\n\n    # ============ Check the returned values ===============\n    _check_returned_values(env, observation_space, action_space)\n\n    # ==== Check the render method and the declared render modes ====\n    if not skip_render_check:\n        _check_render(env, warn=warn)\n\n    # The check only works with numpy arrays\n    if _enforce_array_obs(observation_space):\n        _check_nan(env)\n'"
stable_baselines3/common/evaluation.py,0,"b'# Copied from stable_baselines\nimport numpy as np\n\nfrom stable_baselines3.common.vec_env import VecEnv\n\n\ndef evaluate_policy(model, env, n_eval_episodes=10, deterministic=True,\n                    render=False, callback=None, reward_threshold=None,\n                    return_episode_rewards=False):\n    """"""\n    Runs policy for ``n_eval_episodes`` episodes and returns average reward.\n    This is made to work only with one env.\n\n    :param model: (BaseAlgorithm) The RL agent you want to evaluate.\n    :param env: (gym.Env or VecEnv) The gym environment. In the case of a ``VecEnv``\n        this must contain only one environment.\n    :param n_eval_episodes: (int) Number of episode to evaluate the agent\n    :param deterministic: (bool) Whether to use deterministic or stochastic actions\n    :param render: (bool) Whether to render the environment or not\n    :param callback: (callable) callback function to do additional checks,\n        called after each step.\n    :param reward_threshold: (float) Minimum expected reward per episode,\n        this will raise an error if the performance is not met\n    :param return_episode_rewards: (bool) If True, a list of reward per episode\n        will be returned instead of the mean.\n    :return: (float, float) Mean reward per episode, std of reward per episode\n        returns ([float], [int]) when ``return_episode_rewards`` is True\n    """"""\n    if isinstance(env, VecEnv):\n        assert env.num_envs == 1, ""You must pass only one environment when using this function""\n\n    episode_rewards, episode_lengths = [], []\n    for _ in range(n_eval_episodes):\n        obs = env.reset()\n        done, state = False, None\n        episode_reward = 0.0\n        episode_length = 0\n        while not done:\n            action, state = model.predict(obs, state=state, deterministic=deterministic)\n            obs, reward, done, _info = env.step(action)\n            episode_reward += reward\n            if callback is not None:\n                callback(locals(), globals())\n            episode_length += 1\n            if render:\n                env.render()\n        episode_rewards.append(episode_reward)\n        episode_lengths.append(episode_length)\n    mean_reward = np.mean(episode_rewards)\n    std_reward = np.std(episode_rewards)\n    if reward_threshold is not None:\n        assert mean_reward > reward_threshold, (\'Mean reward below threshold: \'\n                                                f\'{mean_reward:.2f} < {reward_threshold:.2f}\')\n    if return_episode_rewards:\n        return episode_rewards, episode_lengths\n    return mean_reward, std_reward\n'"
stable_baselines3/common/identity_env.py,0,"b'from typing import Union, Optional\n\nimport numpy as np\nfrom gym import Env, Space\nfrom gym.spaces import Discrete, MultiDiscrete, MultiBinary, Box\n\n\nfrom stable_baselines3.common.type_aliases import GymStepReturn, GymObs\n\n\nclass IdentityEnv(Env):\n    def __init__(self,\n                 dim: Optional[int] = None,\n                 space: Optional[Space] = None,\n                 ep_length: int = 100):\n        """"""\n        Identity environment for testing purposes\n\n        :param dim: the size of the action and observation dimension you want\n            to learn. Provide at most one of ``dim`` and ``space``. If both are\n            None, then initialization proceeds with ``dim=1`` and ``space=None``.\n        :param space: the action and observation space. Provide at most one of\n            ``dim`` and ``space``.\n        :param ep_length: the length of each episode in timesteps\n        """"""\n        if space is None:\n            if dim is None:\n                dim = 1\n            space = Discrete(dim)\n        else:\n            assert dim is None, ""arguments for both \'dim\' and \'space\' provided: at most one allowed""\n\n        self.action_space = self.observation_space = space\n        self.ep_length = ep_length\n        self.current_step = 0\n        self.num_resets = -1  # Becomes 0 after __init__ exits.\n        self.reset()\n\n    def reset(self) -> GymObs:\n        self.current_step = 0\n        self.num_resets += 1\n        self._choose_next_state()\n        return self.state\n\n    def step(self, action: Union[int, np.ndarray]) -> GymStepReturn:\n        reward = self._get_reward(action)\n        self._choose_next_state()\n        self.current_step += 1\n        done = self.current_step >= self.ep_length\n        return self.state, reward, done, {}\n\n    def _choose_next_state(self) -> None:\n        self.state = self.action_space.sample()\n\n    def _get_reward(self, action: Union[int, np.ndarray]) -> float:\n        return 1.0 if np.all(self.state == action) else 0.0\n\n    def render(self, mode: str = \'human\') -> None:\n        pass\n\n\nclass IdentityEnvBox(IdentityEnv):\n    def __init__(self, low: float = -1.0,\n                 high: float = 1.0, eps: float = 0.05,\n                 ep_length: int = 100):\n        """"""\n        Identity environment for testing purposes\n\n        :param low: (float) the lower bound of the box dim\n        :param high: (float) the upper bound of the box dim\n        :param eps: (float) the epsilon bound for correct value\n        :param ep_length: (int) the length of each episode in timesteps\n        """"""\n        space = Box(low=low, high=high, shape=(1,), dtype=np.float32)\n        super().__init__(ep_length=ep_length, space=space)\n        self.eps = eps\n\n    def step(self, action: np.ndarray) -> GymStepReturn:\n        reward = self._get_reward(action)\n        self._choose_next_state()\n        self.current_step += 1\n        done = self.current_step >= self.ep_length\n        return self.state, reward, done, {}\n\n    def _get_reward(self, action: np.ndarray) -> float:\n        return 1.0 if (self.state - self.eps) <= action <= (self.state + self.eps) else 0.0\n\n\nclass IdentityEnvMultiDiscrete(IdentityEnv):\n    def __init__(self, dim: int = 1, ep_length: int = 100):\n        """"""\n        Identity environment for testing purposes\n\n        :param dim: (int) the size of the dimensions you want to learn\n        :param ep_length: (int) the length of each episode in timesteps\n        """"""\n        space = MultiDiscrete([dim, dim])\n        super().__init__(ep_length=ep_length, space=space)\n\n\nclass IdentityEnvMultiBinary(IdentityEnv):\n    def __init__(self, dim: int = 1, ep_length: int = 100):\n        """"""\n        Identity environment for testing purposes\n\n        :param dim: (int) the size of the dimensions you want to learn\n        :param ep_length: (int) the length of each episode in timesteps\n        """"""\n        space = MultiBinary(dim)\n        super().__init__(ep_length=ep_length, space=space)\n\n\nclass FakeImageEnv(Env):\n    """"""\n    Fake image environment for testing purposes, it mimics Atari games.\n\n    :param action_dim: (int) Number of discrete actions\n    :param screen_height: (int) Height of the image\n    :param screen_width: (int) Width of the image\n    :param n_channels: (int) Number of color channels\n    :param discrete: (bool)\n    """"""\n    def __init__(self, action_dim: int = 6,\n                 screen_height: int = 84,\n                 screen_width: int = 84,\n                 n_channels: int = 1,\n                 discrete: bool = True):\n\n        self.observation_space = Box(low=0, high=255, shape=(screen_height, screen_width,\n                                                             n_channels), dtype=np.uint8)\n        if discrete:\n            self.action_space = Discrete(action_dim)\n        else:\n            self.action_space = Box(low=-1, high=1, shape=(5,), dtype=np.float32)\n        self.ep_length = 10\n        self.current_step = 0\n\n    def reset(self) -> np.ndarray:\n        self.current_step = 0\n        return self.observation_space.sample()\n\n    def step(self, action: Union[np.ndarray, int]) -> GymStepReturn:\n        reward = 0.0\n        self.current_step += 1\n        done = self.current_step >= self.ep_length\n        return self.observation_space.sample(), reward, done, {}\n\n    def render(self, mode: str = \'human\') -> None:\n        pass\n'"
stable_baselines3/common/logger.py,1,"b'import sys\nimport datetime\nimport json\nimport os\nimport tempfile\nimport warnings\nfrom collections import defaultdict\nfrom typing import Dict, List, TextIO, Union, Any, Optional, Tuple\n\nimport pandas\nimport numpy as np\nimport torch as th\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept ImportError:\n    SummaryWriter = None\n\nDEBUG = 10\nINFO = 20\nWARN = 30\nERROR = 40\nDISABLED = 50\n\n\nclass KVWriter(object):\n    """"""\n    Key Value writer\n    """"""\n\n    def write(self, key_values: Dict[str, Any],\n              key_excluded: Dict[str, Union[str, Tuple[str, ...]]], step: int = 0) -> None:\n        """"""\n        Write a dictionary to file\n\n        :param key_values: (dict)\n        :param key_excluded: (dict)\n        :param step: (int)\n        """"""\n        raise NotImplementedError\n\n    def close(self) -> None:\n        """"""\n        Close owned resources\n        """"""\n        raise NotImplementedError\n\n\nclass SeqWriter(object):\n    """"""\n    sequence writer\n    """"""\n\n    def write_sequence(self, sequence: List):\n        """"""\n        write_sequence an array to file\n\n        :param sequence: (list)\n        """"""\n        raise NotImplementedError\n\n\nclass HumanOutputFormat(KVWriter, SeqWriter):\n    def __init__(self, filename_or_file: Union[str, TextIO]):\n        """"""\n        log to a file, in a human readable format\n\n        :param filename_or_file: (str or File) the file to write the log to\n        """"""\n        if isinstance(filename_or_file, str):\n            self.file = open(filename_or_file, \'wt\')\n            self.own_file = True\n        else:\n            assert hasattr(filename_or_file, \'write\'), f\'Expected file or str, got {filename_or_file}\'\n            self.file = filename_or_file\n            self.own_file = False\n\n    def write(self, key_values: Dict, key_excluded: Dict, step: int = 0) -> None:\n        # Create strings for printing\n        key2str = {}\n        tag = None\n        for (key, value), (_, excluded) in zip(sorted(key_values.items()), sorted(key_excluded.items())):\n\n            if excluded is not None and \'stdout\' in excluded:\n                continue\n\n            if isinstance(value, float):\n                # Align left\n                value_str = f\'{value:<8.3g}\'\n            else:\n                value_str = str(value)\n\n            if key.find(\'/\') > 0:  # Find tag and add it to the dict\n                tag = key[:key.find(\'/\') + 1]\n                key2str[self._truncate(tag)] = \'\'\n            # Remove tag from key\n            if tag is not None and tag in key:\n                key = str(\'   \' + key[len(tag):])\n\n            key2str[self._truncate(key)] = self._truncate(value_str)\n\n        # Find max widths\n        if len(key2str) == 0:\n            warnings.warn(\'Tried to write empty key-value dict\')\n            return\n        else:\n            key_width = max(map(len, key2str.keys()))\n            val_width = max(map(len, key2str.values()))\n\n        # Write out the data\n        dashes = \'-\' * (key_width + val_width + 7)\n        lines = [dashes]\n        for key, value in key2str.items():\n            key_space = \' \' * (key_width - len(key))\n            val_space = \' \' * (val_width - len(value))\n            lines.append(f""| {key}{key_space} | {value}{val_space} |"")\n        lines.append(dashes)\n        self.file.write(\'\\n\'.join(lines) + \'\\n\')\n\n        # Flush the output to the file\n        self.file.flush()\n\n    @classmethod\n    def _truncate(cls, string: str, max_length: int = 23) -> str:\n        return string[:max_length - 3] + \'...\' if len(string) > max_length else string\n\n    def write_sequence(self, sequence: List) -> None:\n        sequence = list(sequence)\n        for i, elem in enumerate(sequence):\n            self.file.write(elem)\n            if i < len(sequence) - 1:  # add space unless this is the last one\n                self.file.write(\' \')\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n    def close(self) -> None:\n        """"""\n        closes the file\n        """"""\n        if self.own_file:\n            self.file.close()\n\n\nclass JSONOutputFormat(KVWriter):\n    def __init__(self, filename: str):\n        """"""\n        log to a file, in the JSON format\n\n        :param filename: (str) the file to write the log to\n        """"""\n        self.file = open(filename, \'wt\')\n\n    def write(self, key_values: Dict[str, Any],\n              key_excluded: Dict[str, Union[str, Tuple[str, ...]]], step: int = 0) -> None:\n        for (key, value), (_, excluded) in zip(sorted(key_values.items()), sorted(key_excluded.items())):\n\n            if excluded is not None and \'json\' in excluded:\n                continue\n\n            if hasattr(value, \'dtype\'):\n                if value.shape == () or len(value) == 1:\n                    # if value is a dimensionless numpy array or of length 1, serialize as a float\n                    key_values[key] = float(value)\n                else:\n                    # otherwise, a value is a numpy array, serialize as a list or nested lists\n                    key_values[key] = value.tolist()\n        self.file.write(json.dumps(key_values) + \'\\n\')\n        self.file.flush()\n\n    def close(self) -> None:\n        """"""\n        closes the file\n        """"""\n\n        self.file.close()\n\n\nclass CSVOutputFormat(KVWriter):\n    def __init__(self, filename: str):\n        """"""\n        log to a file, in a CSV format\n\n        :param filename: (str) the file to write the log to\n        """"""\n\n        self.file = open(filename, \'w+t\')\n        self.keys = []\n        self.separator = \',\'\n\n    def write(self, key_values: Dict[str, Any],\n              key_excluded: Dict[str, Union[str, Tuple[str, ...]]], step: int = 0) -> None:\n        # Add our current row to the history\n        extra_keys = key_values.keys() - self.keys\n        if extra_keys:\n            self.keys.extend(extra_keys)\n            self.file.seek(0)\n            lines = self.file.readlines()\n            self.file.seek(0)\n            for (i, key) in enumerate(self.keys):\n                if i > 0:\n                    self.file.write(\',\')\n                self.file.write(key)\n            self.file.write(\'\\n\')\n            for line in lines[1:]:\n                self.file.write(line[:-1])\n                self.file.write(self.separator * len(extra_keys))\n                self.file.write(\'\\n\')\n        for i, key in enumerate(self.keys):\n            if i > 0:\n                self.file.write(\',\')\n            value = key_values.get(key)\n            if value is not None:\n                self.file.write(str(value))\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n    def close(self) -> None:\n        """"""\n        closes the file\n        """"""\n        self.file.close()\n\n\nclass TensorBoardOutputFormat(KVWriter):\n    def __init__(self, folder: str):\n        """"""\n        Dumps key/value pairs into TensorBoard\'s numeric format.\n\n        :param folder: (str) the folder to write the log to\n        """"""\n        assert SummaryWriter is not None, (""tensorboard is not installed, you can use ""\n                                           ""pip install tensorboard to do so"")\n        self.writer = SummaryWriter(log_dir=folder)\n\n    def write(self, key_values: Dict[str, Any],\n              key_excluded: Dict[str, Union[str, Tuple[str, ...]]], step: int = 0) -> None:\n\n        for (key, value), (_, excluded) in zip(sorted(key_values.items()),\n                                               sorted(key_excluded.items())):\n\n            if excluded is not None and \'tensorboard\' in excluded:\n                continue\n\n            if isinstance(value, np.ScalarType):\n                self.writer.add_scalar(key, value, step)\n\n            if isinstance(value, th.Tensor):\n                self.writer.add_histogram(key, value, step)\n\n        # Flush the output to the file\n        self.writer.flush()\n\n    def close(self) -> None:\n        """"""\n        closes the file\n        """"""\n        if self.writer:\n            self.writer.close()\n            self.writer = None\n\n\ndef make_output_format(_format: str, log_dir: str, log_suffix: str = \'\') -> KVWriter:\n    """"""\n    return a logger for the requested format\n\n    :param _format: (str) the requested format to log to (\'stdout\', \'log\', \'json\' or \'csv\' or \'tensorboard\')\n    :param log_dir: (str) the logging directory\n    :param log_suffix: (str) the suffix for the log file\n    :return: (KVWriter) the logger\n    """"""\n    os.makedirs(log_dir, exist_ok=True)\n    if _format == \'stdout\':\n        return HumanOutputFormat(sys.stdout)\n    elif _format == \'log\':\n        return HumanOutputFormat(os.path.join(log_dir, f\'log{log_suffix}.txt\'))\n    elif _format == \'json\':\n        return JSONOutputFormat(os.path.join(log_dir, f\'progress{log_suffix}.json\'))\n    elif _format == \'csv\':\n        return CSVOutputFormat(os.path.join(log_dir, f\'progress{log_suffix}.csv\'))\n    elif _format == \'tensorboard\':\n        return TensorBoardOutputFormat(log_dir)\n    else:\n        raise ValueError(f\'Unknown format specified: {_format}\')\n\n\n# ================================================================\n# API\n# ================================================================\n\ndef record(key: str, value: Any,\n           exclude: Optional[Union[str, Tuple[str, ...]]] = None) -> None:\n    """"""\n    Log a value of some diagnostic\n    Call this once for each diagnostic quantity, each iteration\n    If called many times, last value will be used.\n\n    :param key: (Any) save to log this key\n    :param value: (Any) save to log this value\n    :param exclude: (str or tuple) outputs to be excluded\n    """"""\n    Logger.CURRENT.record(key, value, exclude)\n\n\ndef record_mean(key: str, value: Union[int, float],\n                exclude: Optional[Union[str, Tuple[str, ...]]] = None) -> None:\n    """"""\n    The same as record(), but if called many times, values averaged.\n\n    :param key: (Any) save to log this key\n    :param value: (Number) save to log this value\n    :param exclude: (str or tuple) outputs to be excluded\n    """"""\n    Logger.CURRENT.record_mean(key, value, exclude)\n\n\ndef record_dict(key_values: Dict[str, Any]) -> None:\n    """"""\n    Log a dictionary of key-value pairs.\n\n    :param key_values: (dict) the list of keys and values to save to log\n    """"""\n    for key, value in key_values.items():\n        record(key, value)\n\n\ndef dump(step: int = 0) -> None:\n    """"""\n    Write all of the diagnostics from the current iteration\n    """"""\n    Logger.CURRENT.dump(step)\n\n\ndef get_log_dict() -> Dict:\n    """"""\n    get the key values logs\n\n    :return: (dict) the logged values\n    """"""\n    return Logger.CURRENT.name_to_value\n\n\ndef log(*args, level: int = INFO) -> None:\n    """"""\n    Write the sequence of args, with no separators,\n    to the console and output files (if you\'ve configured an output file).\n\n    level: int. (see logger.py docs) If the global logger level is higher than\n                the level argument here, don\'t print to stdout.\n\n    :param args: (list) log the arguments\n    :param level: (int) the logging level (can be DEBUG=10, INFO=20, WARN=30, ERROR=40, DISABLED=50)\n    """"""\n    Logger.CURRENT.log(*args, level=level)\n\n\ndef debug(*args) -> None:\n    """"""\n    Write the sequence of args, with no separators,\n    to the console and output files (if you\'ve configured an output file).\n    Using the DEBUG level.\n\n    :param args: (list) log the arguments\n    """"""\n    log(*args, level=DEBUG)\n\n\ndef info(*args) -> None:\n    """"""\n    Write the sequence of args, with no separators,\n    to the console and output files (if you\'ve configured an output file).\n    Using the INFO level.\n\n    :param args: (list) log the arguments\n    """"""\n    log(*args, level=INFO)\n\n\ndef warn(*args) -> None:\n    """"""\n    Write the sequence of args, with no separators,\n    to the console and output files (if you\'ve configured an output file).\n    Using the WARN level.\n\n    :param args: (list) log the arguments\n    """"""\n    log(*args, level=WARN)\n\n\ndef error(*args) -> None:\n    """"""\n    Write the sequence of args, with no separators,\n    to the console and output files (if you\'ve configured an output file).\n    Using the ERROR level.\n\n    :param args: (list) log the arguments\n    """"""\n    log(*args, level=ERROR)\n\n\ndef set_level(level: int) -> None:\n    """"""\n    Set logging threshold on current logger.\n\n    :param level: (int) the logging level (can be DEBUG=10, INFO=20, WARN=30, ERROR=40, DISABLED=50)\n    """"""\n    Logger.CURRENT.set_level(level)\n\n\ndef get_level() -> int:\n    """"""\n    Get logging threshold on current logger.\n    :return: (int) the logging level (can be DEBUG=10, INFO=20, WARN=30, ERROR=40, DISABLED=50)\n    """"""\n    return Logger.CURRENT.level\n\n\ndef get_dir() -> str:\n    """"""\n    Get directory that log files are being written to.\n    will be None if there is no output directory (i.e., if you didn\'t call start)\n\n    :return: (str) the logging directory\n    """"""\n    return Logger.CURRENT.get_dir()\n\n\nrecord_tabular = record\ndump_tabular = dump\n\n\n# ================================================================\n# Backend\n# ================================================================\n\nclass Logger(object):\n    # A logger with no output files. (See right below class definition)\n    #  So that you can still log to the terminal without setting up any output files\n    DEFAULT = None\n    CURRENT = None  # Current logger being used by the free functions above\n\n    def __init__(self, folder: Optional[str], output_formats: List[KVWriter]):\n        """"""\n        the logger class\n\n        :param folder: (str) the logging location\n        :param output_formats: ([str]) the list of output format\n        """"""\n        self.name_to_value = defaultdict(float)  # values this iteration\n        self.name_to_count = defaultdict(int)\n        self.name_to_excluded = defaultdict(str)\n        self.level = INFO\n        self.dir = folder\n        self.output_formats = output_formats\n\n    # Logging API, forwarded\n    # ----------------------------------------\n    def record(self, key: str, value: Any,\n               exclude: Optional[Union[str, Tuple[str, ...]]] = None) -> None:\n        """"""\n        Log a value of some diagnostic\n        Call this once for each diagnostic quantity, each iteration\n        If called many times, last value will be used.\n\n        :param key: (Any) save to log this key\n        :param value: (Any) save to log this value\n        :param exclude: (str or tuple) outputs to be excluded\n        """"""\n        self.name_to_value[key] = value\n        self.name_to_excluded[key] = exclude\n\n    def record_mean(self, key: str, value: Any,\n                    exclude: Optional[Union[str, Tuple[str, ...]]] = None) -> None:\n        """"""\n        The same as record(), but if called many times, values averaged.\n\n        :param key: (Any) save to log this key\n        :param value: (Number) save to log this value\n        :param exclude: (str or tuple) outputs to be excluded\n        """"""\n        if value is None:\n            self.name_to_value[key] = None\n            return\n        old_val, count = self.name_to_value[key], self.name_to_count[key]\n        self.name_to_value[key] = old_val * count / (count + 1) + value / (count + 1)\n        self.name_to_count[key] = count + 1\n        self.name_to_excluded[key] = exclude\n\n    def dump(self, step: int = 0) -> None:\n        """"""\n        Write all of the diagnostics from the current iteration\n        """"""\n        if self.level == DISABLED:\n            return\n        for _format in self.output_formats:\n            if isinstance(_format, KVWriter):\n                _format.write(self.name_to_value, self.name_to_excluded, step)\n\n        self.name_to_value.clear()\n        self.name_to_count.clear()\n        self.name_to_excluded.clear()\n\n    def log(self, *args, level: int = INFO) -> None:\n        """"""\n        Write the sequence of args, with no separators,\n        to the console and output files (if you\'ve configured an output file).\n\n        level: int. (see logger.py docs) If the global logger level is higher than\n                    the level argument here, don\'t print to stdout.\n\n        :param args: (list) log the arguments\n        :param level: (int) the logging level (can be DEBUG=10, INFO=20, WARN=30, ERROR=40, DISABLED=50)\n        """"""\n        if self.level <= level:\n            self._do_log(args)\n\n    # Configuration\n    # ----------------------------------------\n    def set_level(self, level: int) -> None:\n        """"""\n        Set logging threshold on current logger.\n\n        :param level: (int) the logging level (can be DEBUG=10, INFO=20, WARN=30, ERROR=40, DISABLED=50)\n        """"""\n        self.level = level\n\n    def get_dir(self) -> str:\n        """"""\n        Get directory that log files are being written to.\n        will be None if there is no output directory (i.e., if you didn\'t call start)\n\n        :return: (str) the logging directory\n        """"""\n        return self.dir\n\n    def close(self) -> None:\n        """"""\n        closes the file\n        """"""\n        for _format in self.output_formats:\n            _format.close()\n\n    # Misc\n    # ----------------------------------------\n    def _do_log(self, args) -> None:\n        """"""\n        log to the requested format outputs\n\n        :param args: (list) the arguments to log\n        """"""\n        for _format in self.output_formats:\n            if isinstance(_format, SeqWriter):\n                _format.write_sequence(map(str, args))\n\n\n# Initialize logger\nLogger.DEFAULT = Logger.CURRENT = Logger(folder=None, output_formats=[HumanOutputFormat(sys.stdout)])\n\n\ndef configure(folder: Optional[str] = None, format_strings: Optional[List[str]] = None) -> None:\n    """"""\n    configure the current logger\n\n    :param folder: (Optional[str]) the save location\n        (if None, $SB3_LOGDIR, if still None, tempdir/baselines-[date & time])\n    :param format_strings: (Optional[List[str]]) the output logging format\n        (if None, $SB3_LOG_FORMAT, if still None, [\'stdout\', \'log\', \'csv\'])\n    """"""\n    if folder is None:\n        folder = os.getenv(\'SB3_LOGDIR\')\n    if folder is None:\n        folder = os.path.join(tempfile.gettempdir(), datetime.datetime.now().strftime(""SB3-%Y-%m-%d-%H-%M-%S-%f""))\n    assert isinstance(folder, str)\n    os.makedirs(folder, exist_ok=True)\n\n    log_suffix = \'\'\n    if format_strings is None:\n        format_strings = os.getenv(\'SB3_LOG_FORMAT\', \'stdout,log,csv\').split(\',\')\n\n    format_strings = filter(None, format_strings)\n    output_formats = [make_output_format(f, folder, log_suffix) for f in format_strings]\n\n    Logger.CURRENT = Logger(folder=folder, output_formats=output_formats)\n    log(f\'Logging to {folder}\')\n\n\ndef reset() -> None:\n    """"""\n    reset the current logger\n    """"""\n    if Logger.CURRENT is not Logger.DEFAULT:\n        Logger.CURRENT.close()\n        Logger.CURRENT = Logger.DEFAULT\n        log(\'Reset logger\')\n\n\nclass ScopedConfigure(object):\n    def __init__(self, folder: Optional[str] = None, format_strings: Optional[List[str]] = None):\n        """"""\n        Class for using context manager while logging\n\n        usage:\n        with ScopedConfigure(folder=None, format_strings=None):\n            {code}\n\n        :param folder: (str) the logging folder\n        :param format_strings: ([str]) the list of output logging format\n        """"""\n        self.dir = folder\n        self.format_strings = format_strings\n        self.prev_logger = None\n\n    def __enter__(self) -> None:\n        self.prev_logger = Logger.CURRENT\n        configure(folder=self.dir, format_strings=self.format_strings)\n\n    def __exit__(self, *args) -> None:\n        Logger.CURRENT.close()\n        Logger.CURRENT = self.prev_logger\n\n\n# ================================================================\n# Readers\n# ================================================================\n\ndef read_json(filename: str) -> pandas.DataFrame:\n    """"""\n    read a json file using pandas\n\n    :param filename: (str) the file path to read\n    :return: (pandas.DataFrame) the data in the json\n    """"""\n    data = []\n    with open(filename, \'rt\') as file_handler:\n        for line in file_handler:\n            data.append(json.loads(line))\n    return pandas.DataFrame(data)\n\n\ndef read_csv(filename: str) -> pandas.DataFrame:\n    """"""\n    read a csv file using pandas\n\n    :param filename: (str) the file path to read\n    :return: (pandas.DataFrame) the data in the csv\n    """"""\n    return pandas.read_csv(filename, index_col=None, comment=\'#\')\n'"
stable_baselines3/common/monitor.py,0,"b'__all__ = [\'Monitor\', \'get_monitor_files\', \'load_results\']\n\nimport csv\nimport json\nimport os\nimport time\nfrom glob import glob\nfrom typing import Tuple, Dict, Any, List, Optional\n\nimport gym\nimport pandas\nimport numpy as np\n\n\nclass Monitor(gym.Wrapper):\n    """"""\n    A monitor wrapper for Gym environments, it is used to know the episode reward, length, time and other data.\n\n    :param env: (gym.Env) The environment\n    :param filename: (Optional[str]) the location to save a log file, can be None for no log\n    :param allow_early_resets: (bool) allows the reset of the environment before it is done\n    :param reset_keywords: (Tuple[str, ...]) extra keywords for the reset call,\n        if extra parameters are needed at reset\n    :param info_keywords: (Tuple[str, ...]) extra information to log, from the information return of env.step()\n    """"""\n    EXT = ""monitor.csv""\n\n    def __init__(self,\n                 env: gym.Env,\n                 filename: Optional[str] = None,\n                 allow_early_resets: bool = True,\n                 reset_keywords: Tuple[str, ...] = (),\n                 info_keywords: Tuple[str, ...] = ()):\n        super(Monitor, self).__init__(env=env)\n        self.t_start = time.time()\n        if filename is None:\n            self.file_handler = None\n            self.logger = None\n        else:\n            if not filename.endswith(Monitor.EXT):\n                if os.path.isdir(filename):\n                    filename = os.path.join(filename, Monitor.EXT)\n                else:\n                    filename = filename + ""."" + Monitor.EXT\n            self.file_handler = open(filename, ""wt"")\n            self.file_handler.write(\'#%s\\n\' % json.dumps({""t_start"": self.t_start, \'env_id\': env.spec and env.spec.id}))\n            self.logger = csv.DictWriter(self.file_handler,\n                                         fieldnames=(\'r\', \'l\', \'t\') + reset_keywords + info_keywords)\n            self.logger.writeheader()\n            self.file_handler.flush()\n\n        self.reset_keywords = reset_keywords\n        self.info_keywords = info_keywords\n        self.allow_early_resets = allow_early_resets\n        self.rewards = None\n        self.needs_reset = True\n        self.episode_rewards = []\n        self.episode_lengths = []\n        self.episode_times = []\n        self.total_steps = 0\n        self.current_reset_info = {}  # extra info about the current episode, that was passed in during reset()\n\n    def reset(self, **kwargs) -> np.ndarray:\n        """"""\n        Calls the Gym environment reset. Can only be called if the environment is over, or if allow_early_resets is True\n\n        :param kwargs: Extra keywords saved for the next episode. only if defined by reset_keywords\n        :return: (np.ndarray) the first observation of the environment\n        """"""\n        if not self.allow_early_resets and not self.needs_reset:\n            raise RuntimeError(""Tried to reset an environment before done. If you want to allow early resets, ""\n                               ""wrap your env with Monitor(env, path, allow_early_resets=True)"")\n        self.rewards = []\n        self.needs_reset = False\n        for key in self.reset_keywords:\n            value = kwargs.get(key)\n            if value is None:\n                raise ValueError(\'Expected you to pass kwarg {} into reset\'.format(key))\n            self.current_reset_info[key] = value\n        return self.env.reset(**kwargs)\n\n    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict[Any, Any]]:\n        """"""\n        Step the environment with the given action\n\n        :param action: (np.ndarray) the action\n        :return: (Tuple[np.ndarray, float, bool, Dict[Any, Any]]) observation, reward, done, information\n        """"""\n        if self.needs_reset:\n            raise RuntimeError(""Tried to step environment that needs reset"")\n        observation, reward, done, info = self.env.step(action)\n        self.rewards.append(reward)\n        if done:\n            self.needs_reset = True\n            ep_rew = sum(self.rewards)\n            ep_len = len(self.rewards)\n            ep_info = {""r"": round(ep_rew, 6), ""l"": ep_len, ""t"": round(time.time() - self.t_start, 6)}\n            for key in self.info_keywords:\n                ep_info[key] = info[key]\n            self.episode_rewards.append(ep_rew)\n            self.episode_lengths.append(ep_len)\n            self.episode_times.append(time.time() - self.t_start)\n            ep_info.update(self.current_reset_info)\n            if self.logger:\n                self.logger.writerow(ep_info)\n                self.file_handler.flush()\n            info[\'episode\'] = ep_info\n        self.total_steps += 1\n        return observation, reward, done, info\n\n    def close(self):\n        """"""\n        Closes the environment\n        """"""\n        super(Monitor, self).close()\n        if self.file_handler is not None:\n            self.file_handler.close()\n\n    def get_total_steps(self) -> int:\n        """"""\n        Returns the total number of timesteps\n\n        :return: (int)\n        """"""\n        return self.total_steps\n\n    def get_episode_rewards(self) -> List[float]:\n        """"""\n        Returns the rewards of all the episodes\n\n        :return: ([float])\n        """"""\n        return self.episode_rewards\n\n    def get_episode_lengths(self) -> List[int]:\n        """"""\n        Returns the number of timesteps of all the episodes\n\n        :return: ([int])\n        """"""\n        return self.episode_lengths\n\n    def get_episode_times(self) -> List[float]:\n        """"""\n        Returns the runtime in seconds of all the episodes\n\n        :return: ([float])\n        """"""\n        return self.episode_times\n\n\nclass LoadMonitorResultsError(Exception):\n    """"""\n    Raised when loading the monitor log fails.\n    """"""\n    pass\n\n\ndef get_monitor_files(path: str) -> List[str]:\n    """"""\n    get all the monitor files in the given path\n\n    :param path: (str) the logging folder\n    :return: ([str]) the log files\n    """"""\n    return glob(os.path.join(path, ""*"" + Monitor.EXT))\n\n\ndef load_results(path: str) -> pandas.DataFrame:\n    """"""\n    Load all Monitor logs from a given directory path matching ``*monitor.csv``\n\n    :param path: (str) the directory path containing the log file(s)\n    :return: (pandas.DataFrame) the logged data\n    """"""\n    monitor_files = get_monitor_files(path)\n    if len(monitor_files) == 0:\n        raise LoadMonitorResultsError(""no monitor files of the form *%s found in %s"" % (Monitor.EXT, path))\n    data_frames, headers = [], []\n    for file_name in monitor_files:\n        with open(file_name, \'rt\') as file_handler:\n            first_line = file_handler.readline()\n            assert first_line[0] == \'#\'\n            header = json.loads(first_line[1:])\n            data_frame = pandas.read_csv(file_handler, index_col=None)\n            headers.append(header)\n            data_frame[\'t\'] += header[\'t_start\']\n        data_frames.append(data_frame)\n    data_frame = pandas.concat(data_frames)\n    data_frame.sort_values(\'t\', inplace=True)\n    data_frame.reset_index(inplace=True)\n    data_frame[\'t\'] -= min(header[\'t_start\'] for header in headers)\n    return data_frame\n'"
stable_baselines3/common/noise.py,0,"b'from typing import Optional, List, Iterable\nfrom abc import ABC, abstractmethod\nimport copy\n\nimport numpy as np\n\n\nclass ActionNoise(ABC):\n    """"""\n    The action noise base class\n    """"""\n\n    def __init__(self):\n        super(ActionNoise, self).__init__()\n\n    def reset(self) -> None:\n        """"""\n        call end of episode reset for the noise\n        """"""\n        pass\n\n    @abstractmethod\n    def __call__(self) -> np.ndarray:\n        raise NotImplementedError()\n\n\nclass NormalActionNoise(ActionNoise):\n    """"""\n    A Gaussian action noise\n\n    :param mean: (np.ndarray) the mean value of the noise\n    :param sigma: (np.ndarray) the scale of the noise (std here)\n    """"""\n\n    def __init__(self, mean: np.ndarray, sigma: np.ndarray):\n        self._mu = mean\n        self._sigma = sigma\n        super(NormalActionNoise, self).__init__()\n\n    def __call__(self) -> np.ndarray:\n        return np.random.normal(self._mu, self._sigma)\n\n    def __repr__(self) -> str:\n        return f\'NormalActionNoise(mu={self._mu}, sigma={self._sigma})\'\n\n\nclass OrnsteinUhlenbeckActionNoise(ActionNoise):\n    """"""\n    An Ornstein Uhlenbeck action noise, this is designed to aproximate brownian motion with friction.\n\n    Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n\n    :param mean: (np.ndarray) the mean of the noise\n    :param sigma: (np.ndarray) the scale of the noise\n    :param theta: (float) the rate of mean reversion\n    :param dt: (float) the timestep for the noise\n    :param initial_noise: (Optional[np.ndarray]) the initial value for the noise output, (if None: 0)\n    """"""\n\n    def __init__(self, mean: np.ndarray,\n                 sigma: np.ndarray,\n                 theta: float = .15,\n                 dt: float = 1e-2,\n                 initial_noise: Optional[np.ndarray] = None):\n        self._theta = theta\n        self._mu = mean\n        self._sigma = sigma\n        self._dt = dt\n        self.initial_noise = initial_noise\n        self.noise_prev = np.zeros_like(self._mu)\n        self.reset()\n        super(OrnsteinUhlenbeckActionNoise, self).__init__()\n\n    def __call__(self) -> np.ndarray:\n        noise = (self.noise_prev + self._theta * (self._mu - self.noise_prev) * self._dt\n                 + self._sigma * np.sqrt(self._dt) * np.random.normal(size=self._mu.shape))\n        self.noise_prev = noise\n        return noise\n\n    def reset(self) -> None:\n        """"""\n        reset the Ornstein Uhlenbeck noise, to the initial position\n        """"""\n        self.noise_prev = self.initial_noise if self.initial_noise is not None else np.zeros_like(self._mu)\n\n    def __repr__(self) -> str:\n        return f\'OrnsteinUhlenbeckActionNoise(mu={self._mu}, sigma={self._sigma})\'\n\n\nclass VectorizedActionNoise(ActionNoise):\n    """"""\n    A Vectorized action noise for parallel environments.\n\n    :param base_noise: ActionNoise The noise generator to use\n    :param n_envs: (int) The number of parallel environments\n    """"""\n\n    def __init__(self, base_noise: ActionNoise, n_envs: int):\n        try:\n            self.n_envs = int(n_envs)\n            assert self.n_envs > 0\n        except (TypeError, AssertionError):\n            raise ValueError(f""Expected n_envs={n_envs} to be positive integer greater than 0"")\n\n        self.base_noise = base_noise\n        self.noises = [copy.deepcopy(self.base_noise) for _ in range(n_envs)]\n\n    def reset(self, indices: Optional[Iterable[int]] = None) -> None:\n        """"""\n        Reset all the noise processes, or those listed in indices\n\n        :param indices: Optional[Iterable[int]] The indices to reset. Default: None.\n            If the parameter is None, then all processes are reset to their initial position.\n        """"""\n        if indices is None:\n            indices = range(len(self.noises))\n\n        for index in indices:\n            self.noises[index].reset()\n\n    def __repr__(self) -> str:\n        return f""VecNoise(BaseNoise={repr(self.base_noise)}), n_envs={len(self.noises)})""\n\n    def __call__(self) -> np.ndarray:\n        """"""\n        Generate and stack the action noise from each noise object\n        """"""\n        noise = np.stack([noise() for noise in self.noises])\n        return noise\n\n    @property\n    def base_noise(self) -> ActionNoise:\n        return self._base_noise\n\n    @base_noise.setter\n    def base_noise(self, base_noise: ActionNoise):\n        if base_noise is None:\n            raise ValueError(""Expected base_noise to be an instance of ActionNoise, not None"", ActionNoise)\n        if not isinstance(base_noise, ActionNoise):\n            raise TypeError(""Expected base_noise to be an instance of type ActionNoise"", ActionNoise)\n        self._base_noise = base_noise\n\n    @property\n    def noises(self) -> List[ActionNoise]:\n        return self._noises\n\n    @noises.setter\n    def noises(self, noises: List[ActionNoise]) -> None:\n        noises = list(noises)  # raises TypeError if not iterable\n        assert len(noises) == self.n_envs, f""Expected a list of {self.n_envs} ActionNoises, found {len(noises)}.""\n\n        different_types = [\n            i for i, noise in enumerate(noises)\n            if not isinstance(noise, type(self.base_noise))\n        ]\n\n        if len(different_types):\n            raise ValueError(\n                f""Noise instances at indices {different_types} don\'t match the type of base_noise"",\n                type(self.base_noise)\n            )\n\n        self._noises = noises\n        for noise in noises:\n            noise.reset()\n'"
stable_baselines3/common/off_policy_algorithm.py,0,"b'import time\nimport os\nimport pickle\nimport warnings\nfrom typing import Union, Type, Optional, Dict, Any, Callable\n\nimport gym\nimport torch as th\nimport numpy as np\n\nfrom stable_baselines3.common import logger\nfrom stable_baselines3.common.base_class import BaseAlgorithm\nfrom stable_baselines3.common.policies import BasePolicy\nfrom stable_baselines3.common.utils import safe_mean\nfrom stable_baselines3.common.vec_env import VecEnv\nfrom stable_baselines3.common.type_aliases import GymEnv, RolloutReturn\nfrom stable_baselines3.common.callbacks import BaseCallback\nfrom stable_baselines3.common.noise import ActionNoise\nfrom stable_baselines3.common.buffers import ReplayBuffer\n\n\nclass OffPolicyAlgorithm(BaseAlgorithm):\n    """"""\n    The base for Off-Policy algorithms (ex: SAC/TD3)\n\n    :param policy: Policy object\n    :param env: The environment to learn from\n                (if registered in Gym, can be str. Can be None for loading trained models)\n    :param policy_base: The base policy used by this method\n    :param learning_rate: (float or callable) learning rate for the optimizer,\n        it can be a function of the current progress remaining (from 1 to 0)\n    :param buffer_size: (int) size of the replay buffer\n    :param learning_starts: (int) how many steps of the model to collect transitions for before learning starts\n    :param batch_size: (int) Minibatch size for each gradient update\n    :param policy_kwargs: Additional arguments to be passed to the policy on creation\n    :param tensorboard_log: (str) the log location for tensorboard (if None, no logging)\n    :param verbose: The verbosity level: 0 none, 1 training information, 2 debug\n    :param device: Device on which the code should run.\n        By default, it will try to use a Cuda compatible device and fallback to cpu\n        if it is not possible.\n    :param support_multi_env: Whether the algorithm supports training\n        with multiple environments (as in A2C)\n    :param create_eval_env: Whether to create a second environment that will be\n        used for evaluating the agent periodically. (Only available when passing string for the environment)\n    :param monitor_wrapper: When creating an environment, whether to wrap it\n        or not in a Monitor wrapper.\n    :param seed: Seed for the pseudo random generators\n    :param use_sde: Whether to use State Dependent Exploration (SDE)\n        instead of action noise exploration (default: False)\n    :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n        Default: -1 (only sample at the beginning of the rollout)\n    :param use_sde_at_warmup: (bool) Whether to use gSDE instead of uniform sampling\n        during the warm up phase (before learning starts)\n    :param sde_support: (bool) Whether the model support gSDE or not\n    """"""\n\n    def __init__(self,\n                 policy: Type[BasePolicy],\n                 env: Union[GymEnv, str],\n                 policy_base: Type[BasePolicy],\n                 learning_rate: Union[float, Callable],\n                 buffer_size: int = int(1e6),\n                 learning_starts: int = 100,\n                 batch_size: int = 256,\n                 policy_kwargs: Dict[str, Any] = None,\n                 tensorboard_log: Optional[str] = None,\n                 verbose: int = 0,\n                 device: Union[th.device, str] = \'auto\',\n                 support_multi_env: bool = False,\n                 create_eval_env: bool = False,\n                 monitor_wrapper: bool = True,\n                 seed: Optional[int] = None,\n                 use_sde: bool = False,\n                 sde_sample_freq: int = -1,\n                 use_sde_at_warmup: bool = False,\n                 sde_support: bool = True):\n\n        super(OffPolicyAlgorithm, self).__init__(policy=policy, env=env, policy_base=policy_base,\n                                                 learning_rate=learning_rate, policy_kwargs=policy_kwargs,\n                                                 tensorboard_log=tensorboard_log, verbose=verbose,\n                                                 device=device, support_multi_env=support_multi_env,\n                                                 create_eval_env=create_eval_env, monitor_wrapper=monitor_wrapper,\n                                                 seed=seed, use_sde=use_sde, sde_sample_freq=sde_sample_freq)\n        self.buffer_size = buffer_size\n        self.batch_size = batch_size\n        self.learning_starts = learning_starts\n        self.actor = None  # type: Optional[th.nn.Module]\n        self.replay_buffer = None  # type: Optional[ReplayBuffer]\n        # Update policy keyword arguments\n        if sde_support:\n            self.policy_kwargs[\'use_sde\'] = self.use_sde\n        self.policy_kwargs[\'device\'] = self.device\n        # For gSDE only\n        self.use_sde_at_warmup = use_sde_at_warmup\n\n    def _setup_model(self):\n        self._setup_lr_schedule()\n        self.set_random_seed(self.seed)\n        self.replay_buffer = ReplayBuffer(self.buffer_size, self.observation_space,\n                                          self.action_space, self.device)\n        self.policy = self.policy_class(self.observation_space, self.action_space,\n                                        self.lr_schedule, **self.policy_kwargs)\n        self.policy = self.policy.to(self.device)\n\n    def save_replay_buffer(self, path: str):\n        """"""\n        Save the replay buffer as a pickle file.\n\n        :param path: (str) Path to a log folder\n        """"""\n        assert self.replay_buffer is not None, ""The replay buffer is not defined""\n        with open(os.path.join(path, \'replay_buffer.pkl\'), \'wb\') as file_handler:\n            pickle.dump(self.replay_buffer, file_handler)\n\n    def load_replay_buffer(self, path: str):\n        """"""\n        Load a replay buffer from a pickle file.\n\n        :param path: (str) Path to the pickled replay buffer.\n        """"""\n        with open(path, \'rb\') as file_handler:\n            self.replay_buffer = pickle.load(file_handler)\n        assert isinstance(self.replay_buffer, ReplayBuffer), \'The replay buffer must inherit from ReplayBuffer class\'\n\n    def collect_rollouts(self,  # noqa: C901\n                         env: VecEnv,\n                         # Type hint as string to avoid circular import\n                         callback: \'BaseCallback\',\n                         n_episodes: int = 1,\n                         n_steps: int = -1,\n                         action_noise: Optional[ActionNoise] = None,\n                         learning_starts: int = 0,\n                         replay_buffer: Optional[ReplayBuffer] = None,\n                         log_interval: Optional[int] = None) -> RolloutReturn:\n        """"""\n        Collect experiences and store them into a ReplayBuffer.\n\n        :param env: (VecEnv) The training environment\n        :param callback: (BaseCallback) Callback that will be called at each step\n            (and at the beginning and end of the rollout)\n        :param n_episodes: (int) Number of episodes to use to collect rollout data\n            You can also specify a ``n_steps`` instead\n        :param n_steps: (int) Number of steps to use to collect rollout data\n            You can also specify a ``n_episodes`` instead.\n        :param action_noise: (Optional[ActionNoise]) Action noise that will be used for exploration\n            Required for deterministic policy (e.g. TD3). This can also be used\n            in addition to the stochastic policy for SAC.\n        :param learning_starts: (int) Number of steps before learning for the warm-up phase.\n        :param replay_buffer: (ReplayBuffer)\n        :param log_interval: (int) Log data every ``log_interval`` episodes\n        :return: (RolloutReturn)\n        """"""\n        episode_rewards, total_timesteps = [], []\n        total_steps, total_episodes = 0, 0\n\n        assert isinstance(env, VecEnv), ""You must pass a VecEnv""\n        assert env.num_envs == 1, ""OffPolicyAlgorithm only support single environment""\n\n        if n_episodes > 0 and n_steps > 0:\n            # Note we are refering to the constructor arguments\n            # that are named `train_freq` and `n_episodes_rollout`\n            # but correspond to `n_steps` and `n_episodes` here\n            warnings.warn(""You passed a positive value for `train_freq` and `n_episodes_rollout`.""\n                          ""Please make sure this is intended. ""\n                          ""The agent will collect data by stepping in the environment ""\n                          ""until both conditions are true: ""\n                          ""`number of steps in the env` >= `train_freq` and ""\n                          ""`number of episodes` > `n_episodes_rollout`"")\n\n        if self.use_sde:\n            self.actor.reset_noise()\n\n        callback.on_rollout_start()\n        continue_training = True\n\n        while total_steps < n_steps or total_episodes < n_episodes:\n            done = False\n            episode_reward, episode_timesteps = 0.0, 0\n\n            while not done:\n\n                if self.use_sde and self.sde_sample_freq > 0 and total_steps % self.sde_sample_freq == 0:\n                    # Sample a new noise matrix\n                    self.actor.reset_noise()\n\n                # Select action randomly or according to policy\n                if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n                    # Warmup phase\n                    unscaled_action = np.array([self.action_space.sample()])\n                else:\n                    # Note: we assume that the policy uses tanh to scale the action\n                    # We use non-deterministic action in the case of SAC, for TD3, it does not matter\n                    unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n\n                # Rescale the action from [low, high] to [-1, 1]\n                if isinstance(self.action_space, gym.spaces.Box):\n                    scaled_action = self.policy.scale_action(unscaled_action)\n\n                    # Add noise to the action (improve exploration)\n                    if action_noise is not None:\n                        # NOTE: in the original implementation of TD3, the noise was applied to the unscaled action\n                        # Update(October 2019): Not anymore\n                        scaled_action = np.clip(scaled_action + action_noise(), -1, 1)\n\n                    # We store the scaled action in the buffer\n                    buffer_action = scaled_action\n                    action = self.policy.unscale_action(scaled_action)\n                else:\n                    # Discrete case, no need to normalize or clip\n                    buffer_action = unscaled_action\n                    action = buffer_action\n\n                # Rescale and perform action\n                new_obs, reward, done, infos = env.step(action)\n\n                # Only stop training if return value is False, not when it is None.\n                if callback.on_step() is False:\n                    return RolloutReturn(0.0, total_steps, total_episodes, continue_training=False)\n\n                episode_reward += reward\n\n                # Retrieve reward and episode length if using Monitor wrapper\n                self._update_info_buffer(infos, done)\n\n                # Store data in replay buffer\n                if replay_buffer is not None:\n                    # Store only the unnormalized version\n                    if self._vec_normalize_env is not None:\n                        new_obs_ = self._vec_normalize_env.get_original_obs()\n                        reward_ = self._vec_normalize_env.get_original_reward()\n                    else:\n                        # Avoid changing the original ones\n                        self._last_original_obs, new_obs_, reward_ = self._last_obs, new_obs, reward\n\n                    replay_buffer.add(self._last_original_obs, new_obs_, buffer_action, reward_, done)\n\n                self._last_obs = new_obs\n                # Save the unnormalized observation\n                if self._vec_normalize_env is not None:\n                    self._last_original_obs = new_obs_\n\n                self.num_timesteps += 1\n                episode_timesteps += 1\n                total_steps += 1\n                if 0 < n_steps <= total_steps:\n                    break\n\n            if done:\n                total_episodes += 1\n                self._episode_num += 1\n                episode_rewards.append(episode_reward)\n                total_timesteps.append(episode_timesteps)\n\n                if action_noise is not None:\n                    action_noise.reset()\n\n                # Log training infos\n                if log_interval is not None and self._episode_num % log_interval == 0:\n                    fps = int(self.num_timesteps / (time.time() - self.start_time))\n                    logger.record(""time/episodes"", self._episode_num, exclude=""tensorboard"")\n                    if len(self.ep_info_buffer) > 0 and len(self.ep_info_buffer[0]) > 0:\n                        logger.record(\'rollout/ep_rew_mean\', safe_mean([ep_info[\'r\'] for ep_info in self.ep_info_buffer]))\n                        logger.record(\'rollout/ep_len_mean\', safe_mean([ep_info[\'l\'] for ep_info in self.ep_info_buffer]))\n                    logger.record(""time/fps"", fps)\n                    logger.record(\'time/time_elapsed\', int(time.time() - self.start_time), exclude=""tensorboard"")\n                    logger.record(""time/total timesteps"", self.num_timesteps, exclude=""tensorboard"")\n                    if self.use_sde:\n                        logger.record(""train/std"", (self.actor.get_std()).mean().item())\n\n                    if len(self.ep_success_buffer) > 0:\n                        logger.record(\'rollout/success rate\', safe_mean(self.ep_success_buffer))\n                    # Pass the number of timesteps for tensorboard\n                    logger.dump(step=self.num_timesteps)\n\n        mean_reward = np.mean(episode_rewards) if total_episodes > 0 else 0.0\n\n        callback.on_rollout_end()\n\n        return RolloutReturn(mean_reward, total_steps, total_episodes, continue_training)\n'"
stable_baselines3/common/on_policy_algorithm.py,0,"b'import time\nfrom typing import Union, Type, Optional, Dict, Any, List, Tuple, Callable\n\nimport gym\nimport torch as th\nimport numpy as np\n\nfrom stable_baselines3.common import logger\nfrom stable_baselines3.common.utils import safe_mean\nfrom stable_baselines3.common.base_class import BaseAlgorithm\nfrom stable_baselines3.common.policies import ActorCriticPolicy\nfrom stable_baselines3.common.vec_env import VecEnv\nfrom stable_baselines3.common.type_aliases import GymEnv, MaybeCallback\nfrom stable_baselines3.common.callbacks import BaseCallback\nfrom stable_baselines3.common.buffers import RolloutBuffer\n\n\nclass OnPolicyAlgorithm(BaseAlgorithm):\n    """"""\n    The base for On-Policy algorithms (ex: A2C/PPO).\n\n    :param policy: (ActorCriticPolicy or str) The policy model to use (MlpPolicy, CnnPolicy, ...)\n    :param env: (Gym environment or str) The environment to learn from (if registered in Gym, can be str)\n    :param learning_rate: (float or callable) The learning rate, it can be a function\n        of the current progress remaining (from 1 to 0)\n    :param n_steps: (int) The number of steps to run for each environment per update\n        (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n    :param gamma: (float) Discount factor\n    :param gae_lambda: (float) Factor for trade-off of bias vs variance for Generalized Advantage Estimator.\n        Equivalent to classic advantage when set to 1.\n    :param ent_coef: (float) Entropy coefficient for the loss calculation\n    :param vf_coef: (float) Value function coefficient for the loss calculation\n    :param max_grad_norm: (float) The maximum value for the gradient clipping\n    :param use_sde: (bool) Whether to use generalized State Dependent Exploration (gSDE)\n        instead of action noise exploration (default: False)\n    :param sde_sample_freq: (int) Sample a new noise matrix every n steps when using gSDE\n        Default: -1 (only sample at the beginning of the rollout)\n    :param tensorboard_log: (str) the log location for tensorboard (if None, no logging)\n    :param create_eval_env: (bool) Whether to create a second environment that will be\n        used for evaluating the agent periodically. (Only available when passing string for the environment)\n    :param monitor_wrapper: When creating an environment, whether to wrap it\n        or not in a Monitor wrapper.\n    :param policy_kwargs: (dict) additional arguments to be passed to the policy on creation\n    :param verbose: (int) the verbosity level: 0 no output, 1 info, 2 debug\n    :param seed: (int) Seed for the pseudo random generators\n    :param device: (str or th.device) Device (cpu, cuda, ...) on which the code should be run.\n        Setting it to auto, the code will be run on the GPU if possible.\n    :param _init_setup_model: (bool) Whether or not to build the network at the creation of the instance\n    """"""\n\n    def __init__(self,\n                 policy: Union[str, Type[ActorCriticPolicy]],\n                 env: Union[GymEnv, str],\n                 learning_rate: Union[float, Callable],\n                 n_steps: int,\n                 gamma: float,\n                 gae_lambda: float,\n                 ent_coef: float,\n                 vf_coef: float,\n                 max_grad_norm: float,\n                 use_sde: bool,\n                 sde_sample_freq: int,\n                 tensorboard_log: Optional[str] = None,\n                 create_eval_env: bool = False,\n                 monitor_wrapper: bool = True,\n                 policy_kwargs: Optional[Dict[str, Any]] = None,\n                 verbose: int = 0,\n                 seed: Optional[int] = None,\n                 device: Union[th.device, str] = \'auto\',\n                 _init_setup_model: bool = True):\n\n        super(OnPolicyAlgorithm, self).__init__(policy=policy, env=env, policy_base=ActorCriticPolicy,\n                                                learning_rate=learning_rate, policy_kwargs=policy_kwargs,\n                                                verbose=verbose, device=device, use_sde=use_sde,\n                                                sde_sample_freq=sde_sample_freq, create_eval_env=create_eval_env,\n                                                support_multi_env=True, seed=seed, tensorboard_log=tensorboard_log)\n\n        self.n_steps = n_steps\n        self.gamma = gamma\n        self.gae_lambda = gae_lambda\n        self.ent_coef = ent_coef\n        self.vf_coef = vf_coef\n        self.max_grad_norm = max_grad_norm\n        self.rollout_buffer = None\n\n        if _init_setup_model:\n            self._setup_model()\n\n    def _setup_model(self) -> None:\n        self._setup_lr_schedule()\n        self.set_random_seed(self.seed)\n\n        self.rollout_buffer = RolloutBuffer(self.n_steps, self.observation_space,\n                                            self.action_space, self.device,\n                                            gamma=self.gamma, gae_lambda=self.gae_lambda,\n                                            n_envs=self.n_envs)\n        self.policy = self.policy_class(self.observation_space, self.action_space,\n                                        self.lr_schedule, use_sde=self.use_sde, device=self.device,\n                                        **self.policy_kwargs)\n        self.policy = self.policy.to(self.device)\n\n    def collect_rollouts(self,\n                         env: VecEnv,\n                         callback: BaseCallback,\n                         rollout_buffer: RolloutBuffer,\n                         n_rollout_steps: int) -> bool:\n        """"""\n        Collect rollouts using the current policy and fill a `RolloutBuffer`.\n\n        :param env: (VecEnv) The training environment\n        :param callback: (BaseCallback) Callback that will be called at each step\n            (and at the beginning and end of the rollout)\n        :param rollout_buffer: (RolloutBuffer) Buffer to fill with rollouts\n        :param n_steps: (int) Number of experiences to collect per environment\n        :return: (bool) True if function returned with at least `n_rollout_steps`\n            collected, False if callback terminated rollout prematurely.\n        """"""\n        assert self._last_obs is not None, ""No previous observation was provided""\n        n_steps = 0\n        rollout_buffer.reset()\n        # Sample new weights for the state dependent exploration\n        if self.use_sde:\n            self.policy.reset_noise(env.num_envs)\n\n        callback.on_rollout_start()\n\n        while n_steps < n_rollout_steps:\n            if self.use_sde and self.sde_sample_freq > 0 and n_steps % self.sde_sample_freq == 0:\n                # Sample a new noise matrix\n                self.policy.reset_noise(env.num_envs)\n\n            with th.no_grad():\n                # Convert to pytorch tensor\n                obs_tensor = th.as_tensor(self._last_obs).to(self.device)\n                actions, values, log_probs = self.policy.forward(obs_tensor)\n            actions = actions.cpu().numpy()\n\n            # Rescale and perform action\n            clipped_actions = actions\n            # Clip the actions to avoid out of bound error\n            if isinstance(self.action_space, gym.spaces.Box):\n                clipped_actions = np.clip(actions, self.action_space.low, self.action_space.high)\n\n            new_obs, rewards, dones, infos = env.step(clipped_actions)\n\n            if callback.on_step() is False:\n                return False\n\n            self._update_info_buffer(infos)\n            n_steps += 1\n            self.num_timesteps += env.num_envs\n\n            if isinstance(self.action_space, gym.spaces.Discrete):\n                # Reshape in case of discrete action\n                actions = actions.reshape(-1, 1)\n            rollout_buffer.add(self._last_obs, actions, rewards, dones, values, log_probs)\n            self._last_obs = new_obs\n\n        rollout_buffer.compute_returns_and_advantage(values, dones=dones)\n\n        callback.on_rollout_end()\n\n        return True\n\n    def train(self) -> None:\n        """"""\n        Consume current rollout data and update policy parameters.\n        Implemented by individual algorithms.\n        """"""\n        raise NotImplementedError\n\n    def learn(self,\n              total_timesteps: int,\n              callback: MaybeCallback = None,\n              log_interval: int = 1,\n              eval_env: Optional[GymEnv] = None,\n              eval_freq: int = -1,\n              n_eval_episodes: int = 5,\n              tb_log_name: str = ""OnPolicyAlgorithm"",\n              eval_log_path: Optional[str] = None,\n              reset_num_timesteps: bool = True) -> \'OnPolicyAlgorithm\':\n        iteration = 0\n\n        total_timesteps, callback = self._setup_learn(total_timesteps, eval_env, callback, eval_freq,\n                                                      n_eval_episodes, eval_log_path, reset_num_timesteps,\n                                                      tb_log_name)\n\n        callback.on_training_start(locals(), globals())\n\n        while self.num_timesteps < total_timesteps:\n\n            continue_training = self.collect_rollouts(self.env, callback,\n                                                      self.rollout_buffer,\n                                                      n_rollout_steps=self.n_steps)\n\n            if continue_training is False:\n                break\n\n            iteration += 1\n            self._update_current_progress_remaining(self.num_timesteps, total_timesteps)\n\n            # Display training infos\n            if log_interval is not None and iteration % log_interval == 0:\n                fps = int(self.num_timesteps / (time.time() - self.start_time))\n                logger.record(""time/iterations"", iteration, exclude=""tensorboard"")\n                if len(self.ep_info_buffer) > 0 and len(self.ep_info_buffer[0]) > 0:\n                    logger.record(""rollout/ep_rew_mean"",\n                                  safe_mean([ep_info[""r""] for ep_info in self.ep_info_buffer]))\n                    logger.record(""rollout/ep_len_mean"",\n                                  safe_mean([ep_info[""l""] for ep_info in self.ep_info_buffer]))\n                logger.record(""time/fps"", fps)\n                logger.record(""time/time_elapsed"", int(time.time() - self.start_time), exclude=""tensorboard"")\n                logger.record(""time/total_timesteps"", self.num_timesteps, exclude=""tensorboard"")\n                logger.dump(step=self.num_timesteps)\n\n            self.train()\n\n        callback.on_training_end()\n\n        return self\n\n    def get_torch_variables(self) -> Tuple[List[str], List[str]]:\n        """"""\n        cf base class\n        """"""\n        state_dicts = [""policy"", ""policy.optimizer""]\n\n        return state_dicts, []\n'"
stable_baselines3/common/policies.py,1,"b'from typing import Union, Type, Dict, List, Tuple, Optional, Any, Callable\nfrom functools import partial\n\nimport gym\nimport torch as th\nimport torch.nn as nn\nimport numpy as np\n\nfrom stable_baselines3.common.preprocessing import preprocess_obs, is_image_space\nfrom stable_baselines3.common.torch_layers import (FlattenExtractor, BaseFeaturesExtractor, create_mlp,\n                                                   NatureCNN, MlpExtractor)\nfrom stable_baselines3.common.utils import get_device, is_vectorized_observation\nfrom stable_baselines3.common.vec_env import VecTransposeImage\nfrom stable_baselines3.common.distributions import (make_proba_distribution, Distribution,\n                                                    DiagGaussianDistribution, CategoricalDistribution,\n                                                    MultiCategoricalDistribution, BernoulliDistribution,\n                                                    StateDependentNoiseDistribution)\n\n\nclass BasePolicy(nn.Module):\n    """"""\n    The base policy object\n\n    :param observation_space: (gym.spaces.Space) The observation space of the environment\n    :param action_space: (gym.spaces.Space) The action space of the environment\n    :param device: (Union[th.device, str]) Device on which the code should run.\n    :param features_extractor_class: (Type[BaseFeaturesExtractor]) Features extractor to use.\n    :param features_extractor_kwargs: (Optional[Dict[str, Any]]) Keyword arguments\n        to pass to the feature extractor.\n    :param features_extractor: (nn.Module) Network to extract features\n        (a CNN when using images, a nn.Flatten() layer otherwise)\n    :param normalize_images: (bool) Whether to normalize images or not,\n         dividing by 255.0 (True by default)\n    :param optimizer_class: (Type[th.optim.Optimizer]) The optimizer to use,\n        ``th.optim.Adam`` by default\n    :param optimizer_kwargs: (Optional[Dict[str, Any]]) Additional keyword arguments,\n        excluding the learning rate, to pass to the optimizer\n    :param squash_output: (bool) For continuous actions, whether the output is squashed\n        or not using a ``tanh()`` function.\n    """"""\n\n    def __init__(self,\n                 observation_space: gym.spaces.Space,\n                 action_space: gym.spaces.Space,\n                 device: Union[th.device, str] = \'auto\',\n                 features_extractor_class: Type[BaseFeaturesExtractor] = FlattenExtractor,\n                 features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n                 features_extractor: Optional[nn.Module] = None,\n                 normalize_images: bool = True,\n                 optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n                 optimizer_kwargs: Optional[Dict[str, Any]] = None,\n                 squash_output: bool = False):\n        super(BasePolicy, self).__init__()\n\n        if optimizer_kwargs is None:\n            optimizer_kwargs = {}\n\n        if features_extractor_kwargs is None:\n            features_extractor_kwargs = {}\n\n        self.observation_space = observation_space\n        self.action_space = action_space\n        self.device = get_device(device)\n        self.features_extractor = features_extractor\n        self.normalize_images = normalize_images\n        self._squash_output = squash_output\n\n        self.optimizer_class = optimizer_class\n        self.optimizer_kwargs = optimizer_kwargs\n        self.optimizer = None  # type: Optional[th.optim.Optimizer]\n\n        self.features_extractor_class = features_extractor_class\n        self.features_extractor_kwargs = features_extractor_kwargs\n\n    def extract_features(self, obs: th.Tensor) -> th.Tensor:\n        """"""\n        Preprocess the observation if needed and extract features.\n\n        :param obs: (th.Tensor)\n        :return: (th.Tensor)\n        """"""\n        assert self.features_extractor is not None, \'No feature extractor was set\'\n        preprocessed_obs = preprocess_obs(obs, self.observation_space, normalize_images=self.normalize_images)\n        return self.features_extractor(preprocessed_obs)\n\n    @property\n    def squash_output(self) -> bool:\n        """""" (bool) Getter for squash_output.""""""\n        return self._squash_output\n\n    @staticmethod\n    def init_weights(module: nn.Module, gain: float = 1) -> None:\n        """"""\n        Orthogonal initialization (used in PPO and A2C)\n        """"""\n        if isinstance(module, (nn.Linear, nn.Conv2d)):\n            nn.init.orthogonal_(module.weight, gain=gain)\n            module.bias.data.fill_(0.0)\n\n    @staticmethod\n    def _dummy_schedule(_progress_remaining: float) -> float:\n        """""" (float) Useful for pickling policy.""""""\n        return 0.0\n\n    def forward(self, *_args, **kwargs):\n        raise NotImplementedError()\n\n    def _predict(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:\n        """"""\n        Get the action according to the policy for a given observation.\n\n        :param observation: (th.Tensor)\n        :param deterministic: (bool) Whether to use stochastic or deterministic actions\n        :return: (th.Tensor) Taken action according to the policy\n        """"""\n        raise NotImplementedError()\n\n    def predict(self,\n                observation: np.ndarray,\n                state: Optional[np.ndarray] = None,\n                mask: Optional[np.ndarray] = None,\n                deterministic: bool = False) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n        """"""\n        Get the policy action and state from an observation (and optional state).\n        Includes sugar-coating to handle different observations (e.g. normalizing images).\n\n        :param observation: (np.ndarray) the input observation\n        :param state: (Optional[np.ndarray]) The last states (can be None, used in recurrent policies)\n        :param mask: (Optional[np.ndarray]) The last masks (can be None, used in recurrent policies)\n        :param deterministic: (bool) Whether or not to return deterministic actions.\n        :return: (Tuple[np.ndarray, Optional[np.ndarray]]) the model\'s action and the next state\n            (used in recurrent policies)\n        """"""\n        # if state is None:\n        #     state = self.initial_state\n        # if mask is None:\n        #     mask = [False for _ in range(self.n_envs)]\n        observation = np.array(observation)\n\n        # Handle the different cases for images\n        # as PyTorch use channel first format\n        if is_image_space(self.observation_space):\n            if (observation.shape == self.observation_space.shape\n                    or observation.shape[1:] == self.observation_space.shape):\n                pass\n            else:\n                # Try to re-order the channels\n                transpose_obs = VecTransposeImage.transpose_image(observation)\n                if (transpose_obs.shape == self.observation_space.shape\n                        or transpose_obs.shape[1:] == self.observation_space.shape):\n                    observation = transpose_obs\n\n        vectorized_env = is_vectorized_observation(observation, self.observation_space)\n\n        observation = observation.reshape((-1,) + self.observation_space.shape)\n\n        observation = th.as_tensor(observation).to(self.device)\n        with th.no_grad():\n            actions = self._predict(observation, deterministic=deterministic)\n        # Convert to numpy\n        actions = actions.cpu().numpy()\n\n        # Rescale to proper domain when using squashing\n        if isinstance(self.action_space, gym.spaces.Box) and self.squash_output:\n            actions = self.unscale_action(actions)\n\n        clipped_actions = actions\n        # Clip the actions to avoid out of bound error when using gaussian distribution\n        if isinstance(self.action_space, gym.spaces.Box) and not self.squash_output:\n            clipped_actions = np.clip(actions, self.action_space.low, self.action_space.high)\n\n        if not vectorized_env:\n            if state is not None:\n                raise ValueError(""Error: The environment must be vectorized when using recurrent policies."")\n            clipped_actions = clipped_actions[0]\n\n        return clipped_actions, state\n\n    def scale_action(self, action: np.ndarray) -> np.ndarray:\n        """"""\n        Rescale the action from [low, high] to [-1, 1]\n        (no need for symmetric action space)\n\n        :param action: (np.ndarray) Action to scale\n        :return: (np.ndarray) Scaled action\n        """"""\n        low, high = self.action_space.low, self.action_space.high\n        return 2.0 * ((action - low) / (high - low)) - 1.0\n\n    def unscale_action(self, scaled_action: np.ndarray) -> np.ndarray:\n        """"""\n        Rescale the action from [-1, 1] to [low, high]\n        (no need for symmetric action space)\n\n        :param scaled_action: Action to un-scale\n        """"""\n        low, high = self.action_space.low, self.action_space.high\n        return low + (0.5 * (scaled_action + 1.0) * (high - low))\n\n    def _get_data(self) -> Dict[str, Any]:\n        """"""\n        Get data that need to be saved in order to re-create the policy.\n        This corresponds to the arguments of the constructor.\n\n        :return: (Dict[str, Any])\n        """"""\n        return dict(\n            observation_space=self.observation_space,\n            action_space=self.action_space,\n            # Passed to the constructor by child class\n            # squash_output=self.squash_output,\n            # features_extractor=self.features_extractor\n            normalize_images=self.normalize_images,\n        )\n\n    def save(self, path: str) -> None:\n        """"""\n        Save policy to a given location.\n\n        :param path: (str)\n        """"""\n        th.save({\'state_dict\': self.state_dict(), \'data\': self._get_data()}, path)\n\n    @classmethod\n    def load(cls, path: str, device: Union[th.device, str] = \'auto\') -> \'BasePolicy\':\n        """"""\n        Load policy from path.\n\n        :param path: (str)\n        :param device: ( Union[th.device, str]) Device on which the policy should be loaded.\n        :return: (BasePolicy)\n        """"""\n        device = get_device(device)\n        saved_variables = th.load(path, map_location=device)\n        # Create policy object\n        model = cls(**saved_variables[\'data\'])\n        # Load weights\n        model.load_state_dict(saved_variables[\'state_dict\'])\n        model.to(device)\n        return model\n\n    def load_from_vector(self, vector: np.ndarray):\n        """"""\n        Load parameters from a 1D vector.\n\n        :param vector: (np.ndarray)\n        """"""\n        th.nn.utils.vector_to_parameters(th.FloatTensor(vector).to(self.device), self.parameters())\n\n    def parameters_to_vector(self) -> np.ndarray:\n        """"""\n        Convert the parameters to a 1D vector.\n\n        :return: (np.ndarray)\n        """"""\n        return th.nn.utils.parameters_to_vector(self.parameters()).detach().cpu().numpy()\n\n\nclass ActorCriticPolicy(BasePolicy):\n    """"""\n    Policy class for actor-critic algorithms (has both policy and value prediction).\n    Used by A2C, PPO and the likes.\n\n    :param observation_space: (gym.spaces.Space) Observation space\n    :param action_space: (gym.spaces.Space) Action space\n    :param lr_schedule: (Callable) Learning rate schedule (could be constant)\n    :param net_arch: ([int or dict]) The specification of the policy and value networks.\n    :param device: (str or th.device) Device on which the code should run.\n    :param activation_fn: (Type[nn.Module]) Activation function\n    :param ortho_init: (bool) Whether to use or not orthogonal initialization\n    :param use_sde: (bool) Whether to use State Dependent Exploration or not\n    :param log_std_init: (float) Initial value for the log standard deviation\n    :param full_std: (bool) Whether to use (n_features x n_actions) parameters\n        for the std instead of only (n_features,) when using gSDE\n    :param sde_net_arch: ([int]) Network architecture for extracting features\n        when using gSDE. If None, the latent features from the policy will be used.\n        Pass an empty list to use the states as features.\n    :param use_expln: (bool) Use ``expln()`` function instead of ``exp()`` to ensure\n        a positive standard deviation (cf paper). It allows to keep variance\n        above zero and prevent it from growing too fast. In practice, ``exp()`` is usually enough.\n    :param squash_output: (bool) Whether to squash the output using a tanh function,\n        this allows to ensure boundaries when using gSDE.\n    :param features_extractor_class: (Type[BaseFeaturesExtractor]) Features extractor to use.\n    :param features_extractor_kwargs: (Optional[Dict[str, Any]]) Keyword arguments\n        to pass to the feature extractor.\n    :param normalize_images: (bool) Whether to normalize images or not,\n         dividing by 255.0 (True by default)\n    :param optimizer_class: (Type[th.optim.Optimizer]) The optimizer to use,\n        ``th.optim.Adam`` by default\n    :param optimizer_kwargs: (Optional[Dict[str, Any]]) Additional keyword arguments,\n        excluding the learning rate, to pass to the optimizer\n    """"""\n\n    def __init__(self,\n                 observation_space: gym.spaces.Space,\n                 action_space: gym.spaces.Space,\n                 lr_schedule: Callable,\n                 net_arch: Optional[List[Union[int, Dict[str, List[int]]]]] = None,\n                 device: Union[th.device, str] = \'auto\',\n                 activation_fn: Type[nn.Module] = nn.Tanh,\n                 ortho_init: bool = True,\n                 use_sde: bool = False,\n                 log_std_init: float = 0.0,\n                 full_std: bool = True,\n                 sde_net_arch: Optional[List[int]] = None,\n                 use_expln: bool = False,\n                 squash_output: bool = False,\n                 features_extractor_class: Type[BaseFeaturesExtractor] = FlattenExtractor,\n                 features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n                 normalize_images: bool = True,\n                 optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n                 optimizer_kwargs: Optional[Dict[str, Any]] = None):\n\n        if optimizer_kwargs is None:\n            optimizer_kwargs = {}\n            # Small values to avoid NaN in ADAM optimizer\n            if optimizer_class == th.optim.Adam:\n                optimizer_kwargs[\'eps\'] = 1e-5\n\n        super(ActorCriticPolicy, self).__init__(observation_space,\n                                                action_space,\n                                                device,\n                                                features_extractor_class,\n                                                features_extractor_kwargs,\n                                                optimizer_class=optimizer_class,\n                                                optimizer_kwargs=optimizer_kwargs,\n                                                squash_output=squash_output)\n\n        # Default network architecture, from stable-baselines\n        if net_arch is None:\n            if features_extractor_class == FlattenExtractor:\n                net_arch = [dict(pi=[64, 64], vf=[64, 64])]\n            else:\n                net_arch = []\n\n        self.net_arch = net_arch\n        self.activation_fn = activation_fn\n        self.ortho_init = ortho_init\n\n        self.features_extractor = features_extractor_class(self.observation_space,\n                                                           **self.features_extractor_kwargs)\n        self.features_dim = self.features_extractor.features_dim\n\n        self.normalize_images = normalize_images\n        self.log_std_init = log_std_init\n        dist_kwargs = None\n        # Keyword arguments for gSDE distribution\n        if use_sde:\n            dist_kwargs = {\n                \'full_std\': full_std,\n                \'squash_output\': squash_output,\n                \'use_expln\': use_expln,\n                \'learn_features\': sde_net_arch is not None\n            }\n\n        self.sde_features_extractor = None\n        self.sde_net_arch = sde_net_arch\n        self.use_sde = use_sde\n        self.dist_kwargs = dist_kwargs\n\n        # Action distribution\n        self.action_dist = make_proba_distribution(action_space, use_sde=use_sde, dist_kwargs=dist_kwargs)\n\n        self._build(lr_schedule)\n\n    def _get_data(self) -> Dict[str, Any]:\n        data = super()._get_data()\n\n        data.update(dict(\n            net_arch=self.net_arch,\n            activation_fn=self.activation_fn,\n            use_sde=self.use_sde,\n            log_std_init=self.log_std_init,\n            squash_output=self.dist_kwargs[\'squash_output\'] if self.dist_kwargs else None,\n            full_std=self.dist_kwargs[\'full_std\'] if self.dist_kwargs else None,\n            sde_net_arch=self.dist_kwargs[\'sde_net_arch\'] if self.dist_kwargs else None,\n            use_expln=self.dist_kwargs[\'use_expln\'] if self.dist_kwargs else None,\n            lr_schedule=self._dummy_schedule,  # dummy lr schedule, not needed for loading policy alone\n            ortho_init=self.ortho_init,\n            optimizer_class=self.optimizer_class,\n            optimizer_kwargs=self.optimizer_kwargs,\n            features_extractor_class=self.features_extractor_class,\n            features_extractor_kwargs=self.features_extractor_kwargs\n        ))\n        return data\n\n    def reset_noise(self, n_envs: int = 1) -> None:\n        """"""\n        Sample new weights for the exploration matrix.\n\n        :param n_envs: (int)\n        """"""\n        assert isinstance(self.action_dist,\n                          StateDependentNoiseDistribution), \'reset_noise() is only available when using gSDE\'\n        self.action_dist.sample_weights(self.log_std, batch_size=n_envs)\n\n    def _build(self, lr_schedule: Callable) -> None:\n        """"""\n        Create the networks and the optimizer.\n\n        :param lr_schedule: (Callable) Learning rate schedule\n            lr_schedule(1) is the initial learning rate\n        """"""\n        # Note: If net_arch is None and some features extractor is used,\n        #       net_arch here is an empty list and mlp_extractor does not\n        #       really contain any layers (acts like an identity module).\n        self.mlp_extractor = MlpExtractor(self.features_dim, net_arch=self.net_arch,\n                                          activation_fn=self.activation_fn, device=self.device)\n\n        latent_dim_pi = self.mlp_extractor.latent_dim_pi\n\n        # Separate feature extractor for gSDE\n        if self.sde_net_arch is not None:\n            self.sde_features_extractor, latent_sde_dim = create_sde_features_extractor(self.features_dim,\n                                                                                        self.sde_net_arch,\n                                                                                        self.activation_fn)\n\n        if isinstance(self.action_dist, DiagGaussianDistribution):\n            self.action_net, self.log_std = self.action_dist.proba_distribution_net(latent_dim=latent_dim_pi,\n                                                                                    log_std_init=self.log_std_init)\n        elif isinstance(self.action_dist, StateDependentNoiseDistribution):\n            latent_sde_dim = latent_dim_pi if self.sde_net_arch is None else latent_sde_dim\n            self.action_net, self.log_std = self.action_dist.proba_distribution_net(latent_dim=latent_dim_pi,\n                                                                                    latent_sde_dim=latent_sde_dim,\n                                                                                    log_std_init=self.log_std_init)\n        elif isinstance(self.action_dist, CategoricalDistribution):\n            self.action_net = self.action_dist.proba_distribution_net(latent_dim=latent_dim_pi)\n        elif isinstance(self.action_dist, MultiCategoricalDistribution):\n            self.action_net = self.action_dist.proba_distribution_net(latent_dim=latent_dim_pi)\n        elif isinstance(self.action_dist, BernoulliDistribution):\n            self.action_net = self.action_dist.proba_distribution_net(latent_dim=latent_dim_pi)\n\n        self.value_net = nn.Linear(self.mlp_extractor.latent_dim_vf, 1)\n        # Init weights: use orthogonal initialization\n        # with small initial weight for the output\n        if self.ortho_init:\n            # TODO: check for features_extractor\n            # Values from stable-baselines.\n            # feature_extractor/mlp values are\n            # originally from openai/baselines (default gains/init_scales).\n            module_gains = {\n                self.features_extractor: np.sqrt(2),\n                self.mlp_extractor: np.sqrt(2),\n                self.action_net: 0.01,\n                self.value_net: 1\n            }\n            for module, gain in module_gains.items():\n                module.apply(partial(self.init_weights, gain=gain))\n\n        # Setup optimizer with initial learning rate\n        self.optimizer = self.optimizer_class(self.parameters(), lr=lr_schedule(1), **self.optimizer_kwargs)\n\n    def forward(self, obs: th.Tensor,\n                deterministic: bool = False) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n        """"""\n        Forward pass in all the networks (actor and critic)\n\n        :param obs: (th.Tensor) Observation\n        :param deterministic: (bool) Whether to sample or use deterministic actions\n        :return: (Tuple[th.Tensor, th.Tensor, th.Tensor]) action, value and log probability of the action\n        """"""\n        latent_pi, latent_vf, latent_sde = self._get_latent(obs)\n        # Evaluate the values for the given observations\n        values = self.value_net(latent_vf)\n        distribution = self._get_action_dist_from_latent(latent_pi, latent_sde=latent_sde)\n        actions = distribution.get_actions(deterministic=deterministic)\n        log_prob = distribution.log_prob(actions)\n        return actions, values, log_prob\n\n    def _get_latent(self, obs: th.Tensor) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n        """"""\n        Get the latent code (i.e., activations of the last layer of each network)\n        for the different networks.\n\n        :param obs: (th.Tensor) Observation\n        :return: (Tuple[th.Tensor, th.Tensor, th.Tensor]) Latent codes\n            for the actor, the value function and for gSDE function\n        """"""\n        # Preprocess the observation if needed\n        features = self.extract_features(obs)\n        latent_pi, latent_vf = self.mlp_extractor(features)\n\n        # Features for sde\n        latent_sde = latent_pi\n        if self.sde_features_extractor is not None:\n            latent_sde = self.sde_features_extractor(features)\n        return latent_pi, latent_vf, latent_sde\n\n    def _get_action_dist_from_latent(self, latent_pi: th.Tensor,\n                                     latent_sde: Optional[th.Tensor] = None) -> Distribution:\n        """"""\n        Retrieve action distribution given the latent codes.\n\n        :param latent_pi: (th.Tensor) Latent code for the actor\n        :param latent_sde: (Optional[th.Tensor]) Latent code for the gSDE exploration function\n        :return: (Distribution) Action distribution\n        """"""\n        mean_actions = self.action_net(latent_pi)\n\n        if isinstance(self.action_dist, DiagGaussianDistribution):\n            return self.action_dist.proba_distribution(mean_actions, self.log_std)\n        elif isinstance(self.action_dist, CategoricalDistribution):\n            # Here mean_actions are the logits before the softmax\n            return self.action_dist.proba_distribution(action_logits=mean_actions)\n        elif isinstance(self.action_dist, MultiCategoricalDistribution):\n            # Here mean_actions are the flattened logits\n            return self.action_dist.proba_distribution(action_logits=mean_actions)\n        elif isinstance(self.action_dist, BernoulliDistribution):\n            # Here mean_actions are the logits (before rounding to get the binary actions)\n            return self.action_dist.proba_distribution(action_logits=mean_actions)\n        elif isinstance(self.action_dist, StateDependentNoiseDistribution):\n            return self.action_dist.proba_distribution(mean_actions, self.log_std, latent_sde)\n        else:\n            raise ValueError(\'Invalid action distribution\')\n\n    def _predict(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:\n        """"""\n        Get the action according to the policy for a given observation.\n\n        :param observation: (th.Tensor)\n        :param deterministic: (bool) Whether to use stochastic or deterministic actions\n        :return: (th.Tensor) Taken action according to the policy\n        """"""\n        latent_pi, _, latent_sde = self._get_latent(observation)\n        distribution = self._get_action_dist_from_latent(latent_pi, latent_sde)\n        return distribution.get_actions(deterministic=deterministic)\n\n    def evaluate_actions(self, obs: th.Tensor,\n                         actions: th.Tensor) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n        """"""\n        Evaluate actions according to the current policy,\n        given the observations.\n\n        :param obs: (th.Tensor)\n        :param actions: (th.Tensor)\n        :return: (th.Tensor, th.Tensor, th.Tensor) estimated value, log likelihood of taking those actions\n            and entropy of the action distribution.\n        """"""\n        latent_pi, latent_vf, latent_sde = self._get_latent(obs)\n        distribution = self._get_action_dist_from_latent(latent_pi, latent_sde)\n        log_prob = distribution.log_prob(actions)\n        values = self.value_net(latent_vf)\n        return values, log_prob, distribution.entropy()\n\n\nclass ActorCriticCnnPolicy(ActorCriticPolicy):\n    """"""\n    CNN policy class for actor-critic algorithms (has both policy and value prediction).\n    Used by A2C, PPO and the likes.\n\n    :param observation_space: (gym.spaces.Space) Observation space\n    :param action_space: (gym.spaces.Space) Action space\n    :param lr_schedule: (Callable) Learning rate schedule (could be constant)\n    :param net_arch: ([int or dict]) The specification of the policy and value networks.\n    :param device: (str or th.device) Device on which the code should run.\n    :param activation_fn: (Type[nn.Module]) Activation function\n    :param ortho_init: (bool) Whether to use or not orthogonal initialization\n    :param use_sde: (bool) Whether to use State Dependent Exploration or not\n    :param log_std_init: (float) Initial value for the log standard deviation\n    :param full_std: (bool) Whether to use (n_features x n_actions) parameters\n        for the std instead of only (n_features,) when using gSDE\n    :param sde_net_arch: ([int]) Network architecture for extracting features\n        when using gSDE. If None, the latent features from the policy will be used.\n        Pass an empty list to use the states as features.\n    :param use_expln: (bool) Use ``expln()`` function instead of ``exp()`` to ensure\n        a positive standard deviation (cf paper). It allows to keep variance\n        above zero and prevent it from growing too fast. In practice, ``exp()`` is usually enough.\n    :param squash_output: (bool) Whether to squash the output using a tanh function,\n        this allows to ensure boundaries when using gSDE.\n    :param features_extractor_class: (Type[BaseFeaturesExtractor]) Features extractor to use.\n    :param features_extractor_kwargs: (Optional[Dict[str, Any]]) Keyword arguments\n        to pass to the feature extractor.\n    :param normalize_images: (bool) Whether to normalize images or not,\n         dividing by 255.0 (True by default)\n    :param optimizer_class: (Type[th.optim.Optimizer]) The optimizer to use,\n        ``th.optim.Adam`` by default\n    :param optimizer_kwargs: (Optional[Dict[str, Any]]) Additional keyword arguments,\n        excluding the learning rate, to pass to the optimizer\n    """"""\n\n    def __init__(self,\n                 observation_space: gym.spaces.Space,\n                 action_space: gym.spaces.Space,\n                 lr_schedule: Callable,\n                 net_arch: Optional[List[Union[int, Dict[str, List[int]]]]] = None,\n                 device: Union[th.device, str] = \'auto\',\n                 activation_fn: Type[nn.Module] = nn.Tanh,\n                 ortho_init: bool = True,\n                 use_sde: bool = False,\n                 log_std_init: float = 0.0,\n                 full_std: bool = True,\n                 sde_net_arch: Optional[List[int]] = None,\n                 use_expln: bool = False,\n                 squash_output: bool = False,\n                 features_extractor_class: Type[BaseFeaturesExtractor] = NatureCNN,\n                 features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n                 normalize_images: bool = True,\n                 optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n                 optimizer_kwargs: Optional[Dict[str, Any]] = None):\n        super(ActorCriticCnnPolicy, self).__init__(observation_space,\n                                                   action_space,\n                                                   lr_schedule,\n                                                   net_arch,\n                                                   device,\n                                                   activation_fn,\n                                                   ortho_init,\n                                                   use_sde,\n                                                   log_std_init,\n                                                   full_std,\n                                                   sde_net_arch,\n                                                   use_expln,\n                                                   squash_output,\n                                                   features_extractor_class,\n                                                   features_extractor_kwargs,\n                                                   normalize_images,\n                                                   optimizer_class,\n                                                   optimizer_kwargs)\n\n\ndef create_sde_features_extractor(features_dim: int,\n                                  sde_net_arch: List[int],\n                                  activation_fn: Type[nn.Module]) -> Tuple[nn.Sequential, int]:\n    """"""\n    Create the neural network that will be used to extract features\n    for the gSDE exploration function.\n\n    :param features_dim: (int)\n    :param sde_net_arch: ([int])\n    :param activation_fn: (Type[nn.Module])\n    :return: (nn.Sequential, int)\n    """"""\n    # Special case: when using states as features (i.e. sde_net_arch is an empty list)\n    # don\'t use any activation function\n    sde_activation = activation_fn if len(sde_net_arch) > 0 else None\n    latent_sde_net = create_mlp(features_dim, -1, sde_net_arch, activation_fn=sde_activation, squash_output=False)\n    latent_sde_dim = sde_net_arch[-1] if len(sde_net_arch) > 0 else features_dim\n    sde_features_extractor = nn.Sequential(*latent_sde_net)\n    return sde_features_extractor, latent_sde_dim\n\n\n_policy_registry = dict()  # type: Dict[Type[BasePolicy], Dict[str, Type[BasePolicy]]]\n\n\ndef get_policy_from_name(base_policy_type: Type[BasePolicy], name: str) -> Type[BasePolicy]:\n    """"""\n    Returns the registered policy from the base type and name.\n    See `register_policy` for registering policies and explanation.\n\n    :param base_policy_type: (Type[BasePolicy]) the base policy class\n    :param name: (str) the policy name\n    :return: (Type[BasePolicy]) the policy\n    """"""\n    if base_policy_type not in _policy_registry:\n        raise ValueError(f""Error: the policy type {base_policy_type} is not registered!"")\n    if name not in _policy_registry[base_policy_type]:\n        raise ValueError(f""Error: unknown policy type {name},""\n                         f""the only registed policy type are: {list(_policy_registry[base_policy_type].keys())}!"")\n    return _policy_registry[base_policy_type][name]\n\n\ndef register_policy(name: str, policy: Type[BasePolicy]) -> None:\n    """"""\n    Register a policy, so it can be called using its name.\n    e.g. SAC(\'MlpPolicy\', ...) instead of SAC(MlpPolicy, ...).\n\n    The goal here is to standardize policy naming, e.g.\n    all algorithms can call upon ""MlpPolicy"" or ""CnnPolicy"",\n    and they receive respective policies that work for them.\n    Consider following:\n\n    OnlinePolicy\n    -- OnlineMlpPolicy (""MlpPolicy"")\n    -- OnlineCnnPolicy (""CnnPolicy"")\n    OfflinePolicy\n    -- OfflineMlpPolicy (""MlpPolicy"")\n    -- OfflineCnnPolicy (""CnnPolicy"")\n\n    Two policies have name ""MlpPolicy"" and two have ""CnnPolicy"".\n    In `get_policy_from_name`, the parent class (e.g. OnlinePolicy)\n    is given and used to select and return the correct policy.\n\n    :param name: (str) the policy name\n    :param policy: (Type[BasePolicy]) the policy class\n    """"""\n    sub_class = None\n    for cls in BasePolicy.__subclasses__():\n        if issubclass(policy, cls):\n            sub_class = cls\n            break\n    if sub_class is None:\n        raise ValueError(f""Error: the policy {policy} is not of any known subclasses of BasePolicy!"")\n\n    if sub_class not in _policy_registry:\n        _policy_registry[sub_class] = {}\n    if name in _policy_registry[sub_class]:\n        # Check if the registered policy is same\n        # we try to register. If not so,\n        # do not override and complain.\n        if _policy_registry[sub_class][name] != policy:\n            raise ValueError(f""Error: the name {name} is already registered for a different policy, will not override."")\n    _policy_registry[sub_class][name] = policy\n'"
stable_baselines3/common/preprocessing.py,1,"b'from typing import Tuple\n\nimport torch as th\nimport torch.nn.functional as F\nfrom gym import spaces\nimport numpy as np\n\n\ndef is_image_space(observation_space: spaces.Space,\n                   channels_last: bool = True,\n                   check_channels: bool = False) -> bool:\n    """"""\n    Check if a observation space has the shape, limits and dtype\n    of a valid image.\n    The check is conservative, so that it returns False\n    if there is a doubt.\n\n    Valid images: RGB, RGBD, GrayScale with values in [0, 255]\n\n    :param observation_space: (spaces.Space)\n    :param channels_last: (bool)\n    :param check_channels: (bool) Whether to do or not the check for the number of channels.\n        e.g., with frame-stacking, the observation space may have more channels than expected.\n    :return: (bool)\n    """"""\n    if isinstance(observation_space, spaces.Box) and len(observation_space.shape) == 3:\n        # Check the type\n        if observation_space.dtype != np.uint8:\n            return False\n\n        # Check the value range\n        if np.any(observation_space.low != 0) or np.any(observation_space.high != 255):\n            return False\n\n        # Skip channels check\n        if not check_channels:\n            return True\n        # Check the number of channels\n        if channels_last:\n            n_channels = observation_space.shape[-1]\n        else:\n            n_channels = observation_space.shape[0]\n        # RGB, RGBD, GrayScale\n        return n_channels in [1, 3, 4]\n    return False\n\n\ndef preprocess_obs(obs: th.Tensor, observation_space: spaces.Space,\n                   normalize_images: bool = True) -> th.Tensor:\n    """"""\n    Preprocess observation to be to a neural network.\n    For images, it normalizes the values by dividing them by 255 (to have values in [0, 1])\n    For discrete observations, it create a one hot vector.\n\n    :param obs: (th.Tensor) Observation\n    :param observation_space: (spaces.Space)\n    :param normalize_images: (bool) Whether to normalize images or not\n        (True by default)\n    :return: (th.Tensor)\n    """"""\n    if isinstance(observation_space, spaces.Box):\n        if is_image_space(observation_space) and normalize_images:\n            return obs.float() / 255.0\n        return obs.float()\n\n    elif isinstance(observation_space, spaces.Discrete):\n        # One hot encoding and convert to float to avoid errors\n        return F.one_hot(obs.long(), num_classes=observation_space.n).float()\n\n    elif isinstance(observation_space, spaces.MultiDiscrete):\n        # Tensor concatenation of one hot encodings of each Categorical sub-space\n        return th.cat([F.one_hot(obs_.long(), num_classes=int(observation_space.nvec[idx])).float()\n                       for idx, obs_ in enumerate(th.split(obs.long(), 1, dim=1))],\n                      dim=-1).view(obs.shape[0], sum(observation_space.nvec))\n\n    elif isinstance(observation_space, spaces.MultiBinary):\n        return obs.float()\n\n    else:\n        raise NotImplementedError()\n\n\ndef get_obs_shape(observation_space: spaces.Space) -> Tuple[int, ...]:\n    """"""\n    Get the shape of the observation (useful for the buffers).\n\n    :param observation_space: (spaces.Space)\n    :return: (Tuple[int, ...])\n    """"""\n    if isinstance(observation_space, spaces.Box):\n        return observation_space.shape\n    elif isinstance(observation_space, spaces.Discrete):\n        # Observation is an int\n        return 1,\n    elif isinstance(observation_space, spaces.MultiDiscrete):\n        # Number of discrete features\n        return int(len(observation_space.nvec)),\n    elif isinstance(observation_space, spaces.MultiBinary):\n        # Number of binary features\n        return int(observation_space.n),\n    else:\n        raise NotImplementedError()\n\n\ndef get_flattened_obs_dim(observation_space: spaces.Space) -> int:\n    """"""\n    Get the dimension of the observation space when flattened.\n    It does not apply to image observation space.\n\n    :param observation_space: (spaces.Space)\n    :return: (int)\n    """"""\n    # See issue https://github.com/openai/gym/issues/1915\n    # it may be a problem for Dict/Tuple spaces too...\n    if isinstance(observation_space, spaces.MultiDiscrete):\n        return sum(observation_space.nvec)\n    else:\n        # Use Gym internal method\n        return spaces.utils.flatdim(observation_space)\n\n\ndef get_action_dim(action_space: spaces.Space) -> int:\n    """"""\n    Get the dimension of the action space.\n\n    :param action_space: (spaces.Space)\n    :return: (int)\n    """"""\n    if isinstance(action_space, spaces.Box):\n        return int(np.prod(action_space.shape))\n    elif isinstance(action_space, spaces.Discrete):\n        # Action is an int\n        return 1\n    elif isinstance(action_space, spaces.MultiDiscrete):\n        # Number of discrete actions\n        return int(len(action_space.nvec))\n    elif isinstance(action_space, spaces.MultiBinary):\n        # Number of binary actions\n        return int(action_space.n)\n    else:\n        raise NotImplementedError()\n'"
stable_baselines3/common/results_plotter.py,0,"b'from typing import Tuple, Callable, List, Optional\n\nimport numpy as np\nimport pandas as pd\n# import matplotlib\n# matplotlib.use(\'TkAgg\')  # Can change to \'Agg\' for non-interactive mode\nimport matplotlib.pyplot as plt\n\nfrom stable_baselines3.common.monitor import load_results\n\n\nX_TIMESTEPS = \'timesteps\'\nX_EPISODES = \'episodes\'\nX_WALLTIME = \'walltime_hrs\'\nPOSSIBLE_X_AXES = [X_TIMESTEPS, X_EPISODES, X_WALLTIME]\nEPISODES_WINDOW = 100\n\n\ndef rolling_window(array: np.ndarray, window: int) -> np.ndarray:\n    """"""\n    Apply a rolling window to a np.ndarray\n\n    :param array: (np.ndarray) the input Array\n    :param window: (int) length of the rolling window\n    :return: (np.ndarray) rolling window on the input array\n    """"""\n    shape = array.shape[:-1] + (array.shape[-1] - window + 1, window)\n    strides = array.strides + (array.strides[-1],)\n    return np.lib.stride_tricks.as_strided(array, shape=shape, strides=strides)\n\n\ndef window_func(var_1: np.ndarray, var_2: np.ndarray,\n                window: int, func: Callable) -> Tuple[np.ndarray, np.ndarray]:\n    """"""\n    Apply a function to the rolling window of 2 arrays\n\n    :param var_1: (np.ndarray) variable 1\n    :param var_2: (np.ndarray) variable 2\n    :param window: (int) length of the rolling window\n    :param func: (numpy function) function to apply on the rolling window on variable 2 (such as np.mean)\n    :return: (Tuple[np.ndarray, np.ndarray])  the rolling output with applied function\n    """"""\n    var_2_window = rolling_window(var_2, window)\n    function_on_var2 = func(var_2_window, axis=-1)\n    return var_1[window - 1:], function_on_var2\n\n\ndef ts2xy(data_frame: pd.DataFrame, x_axis: str) -> Tuple[np.ndarray, np.ndarray]:\n    """"""\n    Decompose a data frame variable to x ans ys\n\n    :param data_frame: (pd.DataFrame) the input data\n    :param x_axis: (str) the axis for the x and y output\n        (can be X_TIMESTEPS=\'timesteps\', X_EPISODES=\'episodes\' or X_WALLTIME=\'walltime_hrs\')\n    :return: (Tuple[np.ndarray, np.ndarray]) the x and y output\n    """"""\n    if x_axis == X_TIMESTEPS:\n        x_var = np.cumsum(data_frame.l.values)\n        y_var = data_frame.r.values\n    elif x_axis == X_EPISODES:\n        x_var = np.arange(len(data_frame))\n        y_var = data_frame.r.values\n    elif x_axis == X_WALLTIME:\n        # Convert to hours\n        x_var = data_frame.t.values / 3600.\n        y_var = data_frame.r.values\n    else:\n        raise NotImplementedError\n    return x_var, y_var\n\n\ndef plot_curves(xy_list: List[Tuple[np.ndarray, np.ndarray]],\n                x_axis: str, title: str, figsize: Tuple[int, int] = (8, 2)) -> None:\n    """"""\n    plot the curves\n\n    :param xy_list: (List[Tuple[np.ndarray, np.ndarray]]) the x and y coordinates to plot\n    :param x_axis: (str) the axis for the x and y output\n        (can be X_TIMESTEPS=\'timesteps\', X_EPISODES=\'episodes\' or X_WALLTIME=\'walltime_hrs\')\n    :param title: (str) the title of the plot\n    :param figsize: (Tuple[int, int]) Size of the figure (width, height)\n    """"""\n\n    plt.figure(title, figsize=figsize)\n    max_x = max(xy[0][-1] for xy in xy_list)\n    min_x = 0\n    for (i, (x, y)) in enumerate(xy_list):\n        plt.scatter(x, y, s=2)\n        # Do not plot the smoothed curve at all if the timeseries is shorter than window size.\n        if x.shape[0] >= EPISODES_WINDOW:\n            # Compute and plot rolling mean with window of size EPISODE_WINDOW\n            x, y_mean = window_func(x, y, EPISODES_WINDOW, np.mean)\n            plt.plot(x, y_mean)\n    plt.xlim(min_x, max_x)\n    plt.title(title)\n    plt.xlabel(x_axis)\n    plt.ylabel(""Episode Rewards"")\n    plt.tight_layout()\n\n\ndef plot_results(dirs: List[str], num_timesteps: Optional[int],\n                 x_axis: str, task_name: str, figsize: Tuple[int, int] = (8, 2)) -> None:\n    """"""\n    Plot the results using csv files from ``Monitor`` wrapper.\n\n    :param dirs: ([str]) the save location of the results to plot\n    :param num_timesteps: (int or None) only plot the points below this value\n    :param x_axis: (str) the axis for the x and y output\n        (can be X_TIMESTEPS=\'timesteps\', X_EPISODES=\'episodes\' or X_WALLTIME=\'walltime_hrs\')\n    :param task_name: (str) the title of the task to plot\n    :param figsize: (Tuple[int, int]) Size of the figure (width, height)\n    """"""\n\n    data_frames = []\n    for folder in dirs:\n        data_frame = load_results(folder)\n        if num_timesteps is not None:\n            data_frame = data_frame[data_frame.l.cumsum() <= num_timesteps]\n        data_frames.append(data_frame)\n    xy_list = [ts2xy(data_frame, x_axis) for data_frame in data_frames]\n    plot_curves(xy_list, x_axis, task_name, figsize)\n'"
stable_baselines3/common/running_mean_std.py,0,"b'from typing import Tuple\n\nimport numpy as np\n\n\nclass RunningMeanStd(object):\n    def __init__(self, epsilon: float = 1e-4, shape: Tuple[int, ...] = ()):\n        """"""\n        Calulates the running mean and std of a data stream\n        https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n\n        :param epsilon: (float) helps with arithmetic issues\n        :param shape: (tuple) the shape of the data stream\'s output\n        """"""\n        self.mean = np.zeros(shape, np.float64)\n        self.var = np.ones(shape, np.float64)\n        self.count = epsilon\n\n    def update(self, arr: np.ndarray) -> None:\n        batch_mean = np.mean(arr, axis=0)\n        batch_var = np.var(arr, axis=0)\n        batch_count = arr.shape[0]\n        self.update_from_moments(batch_mean, batch_var, batch_count)\n\n    def update_from_moments(self, batch_mean: np.ndarray,\n                            batch_var: np.ndarray,\n                            batch_count: int) -> None:\n        delta = batch_mean - self.mean\n        tot_count = self.count + batch_count\n\n        new_mean = self.mean + delta * batch_count / tot_count\n        m_a = self.var * self.count\n        m_b = batch_var * batch_count\n        m_2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)\n        new_var = m_2 / (self.count + batch_count)\n\n        new_count = batch_count + self.count\n\n        self.mean = new_mean\n        self.var = new_var\n        self.count = new_count\n'"
stable_baselines3/common/save_util.py,0,"b'""""""\nSave util taken from stable_baselines\nused to serialize data (class parameters) of model classes\n""""""\nimport os\nimport io\nimport json\nimport base64\nimport functools\nfrom typing import Dict, Any, Tuple, Optional\nimport warnings\nimport zipfile\n\nimport torch as th\nimport cloudpickle\n\nfrom stable_baselines3.common.type_aliases import TensorDict\nfrom stable_baselines3.common.utils import get_device\n\n\ndef recursive_getattr(obj: Any, attr: str, *args) -> Any:\n    """"""\n    Recursive version of getattr\n    taken from https://stackoverflow.com/questions/31174295\n\n    Ex:\n    > MyObject.sub_object = SubObject(name=\'test\')\n    > recursive_getattr(MyObject, \'sub_object.name\')  # return test\n    :param obj: (Any)\n    :param attr: (str) Attribute to retrieve\n    :return: (Any) The attribute\n    """"""\n    def _getattr(obj: Any, attr: str) -> Any:\n        return getattr(obj, attr, *args)\n\n    return functools.reduce(_getattr, [obj] + attr.split(\'.\'))\n\n\ndef recursive_setattr(obj: Any, attr: str, val: Any) -> None:\n    """"""\n    Recursive version of setattr\n    taken from https://stackoverflow.com/questions/31174295\n\n    Ex:\n    > MyObject.sub_object = SubObject(name=\'test\')\n    > recursive_setattr(MyObject, \'sub_object.name\', \'hello\')\n    :param obj: (Any)\n    :param attr: (str) Attribute to set\n    :param val: (Any) New value of the attribute\n    """"""\n    pre, _, post = attr.rpartition(\'.\')\n    return setattr(recursive_getattr(obj, pre) if pre else obj, post, val)\n\n\ndef is_json_serializable(item: Any) -> bool:\n    """"""\n    Test if an object is serializable into JSON\n\n    :param item: (object) The object to be tested for JSON serialization.\n    :return: (bool) True if object is JSON serializable, false otherwise.\n    """"""\n    # Try with try-except struct.\n    json_serializable = True\n    try:\n        _ = json.dumps(item)\n    except TypeError:\n        json_serializable = False\n    return json_serializable\n\n\ndef data_to_json(data: Dict[str, Any]) -> str:\n    """"""\n    Turn data (class parameters) into a JSON string for storing\n\n    :param data: (Dict[str, Any]) Dictionary of class parameters to be\n        stored. Items that are not JSON serializable will be\n        pickled with Cloudpickle and stored as bytearray in\n        the JSON file\n    :return: (str) JSON string of the data serialized.\n    """"""\n    # First, check what elements can not be JSONfied,\n    # and turn them into byte-strings\n    serializable_data = {}\n    for data_key, data_item in data.items():\n        # See if object is JSON serializable\n        if is_json_serializable(data_item):\n            # All good, store as it is\n            serializable_data[data_key] = data_item\n        else:\n            # Not serializable, cloudpickle it into\n            # bytes and convert to base64 string for storing.\n            # Also store type of the class for consumption\n            # from other languages/humans, so we have an\n            # idea what was being stored.\n            base64_encoded = base64.b64encode(\n                cloudpickle.dumps(data_item)\n            ).decode()\n\n            # Use "":"" to make sure we do\n            # not override these keys\n            # when we include variables of the object later\n            cloudpickle_serialization = {\n                "":type:"": str(type(data_item)),\n                "":serialized:"": base64_encoded\n            }\n\n            # Add first-level JSON-serializable items of the\n            # object for further details (but not deeper than this to\n            # avoid deep nesting).\n            # First we check that object has attributes (not all do,\n            # e.g. numpy scalars)\n            if hasattr(data_item, ""__dict__"") or isinstance(data_item, dict):\n                # Take elements from __dict__ for custom classes\n                item_generator = (\n                    data_item.items if isinstance(data_item, dict) else data_item.__dict__.items\n                )\n                for variable_name, variable_item in item_generator():\n                    # Check if serializable. If not, just include the\n                    # string-representation of the object.\n                    if is_json_serializable(variable_item):\n                        cloudpickle_serialization[variable_name] = variable_item\n                    else:\n                        cloudpickle_serialization[variable_name] = str(variable_item)\n\n            serializable_data[data_key] = cloudpickle_serialization\n    json_string = json.dumps(serializable_data, indent=4)\n    return json_string\n\n\ndef json_to_data(json_string: str,\n                 custom_objects: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    """"""\n    Turn JSON serialization of class-parameters back into dictionary.\n\n    :param json_string: (str) JSON serialization of the class-parameters\n        that should be loaded.\n    :param custom_objects: (dict) Dictionary of objects to replace\n        upon loading. If a variable is present in this dictionary as a\n        key, it will not be deserialized and the corresponding item\n        will be used instead. Similar to custom_objects in\n        `keras.models.load_model`. Useful when you have an object in\n        file that can not be deserialized.\n    :return: (dict) Loaded class parameters.\n    """"""\n    if custom_objects is not None and not isinstance(custom_objects, dict):\n        raise ValueError(""custom_objects argument must be a dict or None"")\n\n    json_dict = json.loads(json_string)\n    # This will be filled with deserialized data\n    return_data = {}\n    for data_key, data_item in json_dict.items():\n        if custom_objects is not None and data_key in custom_objects.keys():\n            # If item is provided in custom_objects, replace\n            # the one from JSON with the one in custom_objects\n            return_data[data_key] = custom_objects[data_key]\n        elif isinstance(data_item, dict) and "":serialized:"" in data_item.keys():\n            # If item is dictionary with "":serialized:""\n            # key, this means it is serialized with cloudpickle.\n            serialization = data_item["":serialized:""]\n            # Try-except deserialization in case we run into\n            # errors. If so, we can tell bit more information to\n            # user.\n            try:\n                base64_object = base64.b64decode(serialization.encode())\n                deserialized_object = cloudpickle.loads(base64_object)\n            except RuntimeError:\n                warnings.warn(f""Could not deserialize object {data_key}. "" +\n                              ""Consider using `custom_objects` argument to replace "" +\n                              ""this object."")\n            return_data[data_key] = deserialized_object\n        else:\n            # Read as it is\n            return_data[data_key] = data_item\n    return return_data\n\n\ndef save_to_zip_file(save_path: str, data: Dict[str, Any] = None,\n                     params: Dict[str, Any] = None, tensors: Dict[str, Any] = None) -> None:\n    """"""\n    Save a model to a zip archive.\n\n    :param save_path: Where to store the model.\n    :param data: Class parameters being stored.\n    :param params: Model parameters being stored expected to contain an entry for every\n                   state_dict with its name and the state_dict.\n    :param tensors: Extra tensor variables expected to contain name and value of tensors\n    """"""\n\n    # data/params can be None, so do not\n    # try to serialize them blindly\n    if data is not None:\n        serialized_data = data_to_json(data)\n\n    # Check postfix if save_path is a string\n    if isinstance(save_path, str):\n        _, ext = os.path.splitext(save_path)\n        if ext == """":\n            save_path += "".zip""\n\n    # Create a zip-archive and write our objects\n    # there. This works when save_path is either\n    # str or a file-like\n    with zipfile.ZipFile(save_path, ""w"") as archive:\n        # Do not try to save ""None"" elements\n        if data is not None:\n            archive.writestr(""data"", serialized_data)\n        if tensors is not None:\n            with archive.open(\'tensors.pth\', mode=""w"") as tensors_file:\n                th.save(tensors, tensors_file)\n        if params is not None:\n            for file_name, dict_ in params.items():\n                with archive.open(file_name + \'.pth\', mode=""w"") as param_file:\n                    th.save(dict_, param_file)\n\n\ndef load_from_zip_file(load_path: str, load_data: bool = True) -> (Tuple[Optional[Dict[str, Any]],\n                                                                   Optional[TensorDict],\n                                                                   Optional[TensorDict]]):\n    """"""\n    Load model data from a .zip archive\n\n    :param load_path: Where to load the model from\n    :param load_data: Whether we should load and return data\n        (class parameters). Mainly used by \'load_parameters\' to only load model parameters (weights)\n    :return: (dict),(dict),(dict) Class parameters, model state_dicts (dict of state_dict)\n        and dict of extra tensors\n    """"""\n    # Check if file exists if load_path is a string\n    if isinstance(load_path, str):\n        if not os.path.exists(load_path):\n            if os.path.exists(load_path + "".zip""):\n                load_path += "".zip""\n            else:\n                raise ValueError(f""Error: the file {load_path} could not be found"")\n\n    # set device to cpu if cuda is not available\n    device = get_device()\n\n    # Open the zip archive and load data\n    try:\n        with zipfile.ZipFile(load_path, ""r"") as archive:\n            namelist = archive.namelist()\n            # If data or parameters is not in the\n            # zip archive, assume they were stored\n            # as None (_save_to_file_zip allows this).\n            data = None\n            tensors = None\n            params = {}\n\n            if ""data"" in namelist and load_data:\n                # Load class parameters and convert to string\n                json_data = archive.read(""data"").decode()\n                data = json_to_data(json_data)\n\n            if ""tensors.pth"" in namelist and load_data:\n                # Load extra tensors\n                with archive.open(\'tensors.pth\', mode=""r"") as tensor_file:\n                    # File has to be seekable, but opt_param_file is not, so load in BytesIO first\n                    # fixed in python >= 3.7\n                    file_content = io.BytesIO()\n                    file_content.write(tensor_file.read())\n                    # go to start of file\n                    file_content.seek(0)\n                    # load the parameters with the right ``map_location``\n                    tensors = th.load(file_content, map_location=device)\n\n            # check for all other .pth files\n            other_files = [file_name for file_name in namelist if\n                           os.path.splitext(file_name)[1] == "".pth"" and file_name != ""tensors.pth""]\n            # if there are any other files which end with .pth and aren\'t ""params.pth""\n            # assume that they each are optimizer parameters\n            if len(other_files) > 0:\n                for file_path in other_files:\n                    with archive.open(file_path, mode=""r"") as opt_param_file:\n                        # File has to be seekable, but opt_param_file is not, so load in BytesIO first\n                        # fixed in python >= 3.7\n                        file_content = io.BytesIO()\n                        file_content.write(opt_param_file.read())\n                        # go to start of file\n                        file_content.seek(0)\n                        # load the parameters with the right ``map_location``\n                        params[os.path.splitext(file_path)[0]] = th.load(file_content, map_location=device)\n    except zipfile.BadZipFile:\n        # load_path wasn\'t a zip file\n        raise ValueError(f""Error: the file {load_path} wasn\'t a zip-file"")\n    return data, params, tensors\n'"
stable_baselines3/common/torch_layers.py,1,"b'from typing import Union, Type, Dict, List, Tuple\n\nfrom itertools import zip_longest\n\nimport gym\nimport torch as th\nimport torch.nn as nn\n\nfrom stable_baselines3.common.preprocessing import get_flattened_obs_dim, is_image_space\nfrom stable_baselines3.common.utils import get_device\n\n\nclass BaseFeaturesExtractor(nn.Module):\n    """"""\n    Base class that represents a features extractor.\n\n    :param observation_space: (gym.Space)\n    :param features_dim: (int) Number of features extracted.\n    """"""\n\n    def __init__(self, observation_space: gym.Space, features_dim: int = 0):\n        super(BaseFeaturesExtractor, self).__init__()\n        assert features_dim > 0\n        self._observation_space = observation_space\n        self._features_dim = features_dim\n\n    @property\n    def features_dim(self) -> int:\n        return self._features_dim\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        raise NotImplementedError()\n\n\nclass FlattenExtractor(BaseFeaturesExtractor):\n    """"""\n    Feature extract that flatten the input.\n    Used as a placeholder when feature extraction is not needed.\n\n    :param observation_space: (gym.Space)\n    """"""\n\n    def __init__(self, observation_space: gym.Space):\n        super(FlattenExtractor, self).__init__(observation_space, get_flattened_obs_dim(observation_space))\n        self.flatten = nn.Flatten()\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.flatten(observations)\n\n\nclass NatureCNN(BaseFeaturesExtractor):\n    """"""\n    CNN from DQN nature paper:\n        Mnih, Volodymyr, et al.\n        ""Human-level control through deep reinforcement learning.""\n        Nature 518.7540 (2015): 529-533.\n\n    :param observation_space: (gym.Space)\n    :param features_dim: (int) Number of features extracted.\n        This corresponds to the number of unit for the last layer.\n    """"""\n\n    def __init__(self, observation_space: gym.spaces.Box,\n                 features_dim: int = 512):\n        super(NatureCNN, self).__init__(observation_space, features_dim)\n        # We assume CxHxW images (channels first)\n        # Re-ordering will be done by pre-preprocessing or wrapper\n        assert is_image_space(observation_space), (\'You should use NatureCNN \'\n                                                   f\'only with images not with {observation_space} \'\n                                                   \'(you are probably using `CnnPolicy` instead of `MlpPolicy`)\')\n        n_input_channels = observation_space.shape[0]\n        self.cnn = nn.Sequential(nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n                                 nn.ReLU(),\n                                 nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n                                 nn.ReLU(),\n                                 nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=0),\n                                 nn.ReLU(),\n                                 nn.Flatten())\n\n        # Compute shape by doing one forward pass\n        with th.no_grad():\n            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n\n        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.linear(self.cnn(observations))\n\n\ndef create_mlp(input_dim: int,\n               output_dim: int,\n               net_arch: List[int],\n               activation_fn: Type[nn.Module] = nn.ReLU,\n               squash_output: bool = False) -> List[nn.Module]:\n    """"""\n    Create a multi layer perceptron (MLP), which is\n    a collection of fully-connected layers each followed by an activation function.\n\n    :param input_dim: (int) Dimension of the input vector\n    :param output_dim: (int)\n    :param net_arch: (List[int]) Architecture of the neural net\n        It represents the number of units per layer.\n        The length of this list is the number of layers.\n    :param activation_fn: (Type[nn.Module]) The activation function\n        to use after each layer.\n    :param squash_output: (bool) Whether to squash the output using a Tanh\n        activation function\n    :return: (List[nn.Module])\n    """"""\n\n    if len(net_arch) > 0:\n        modules = [nn.Linear(input_dim, net_arch[0]), activation_fn()]\n    else:\n        modules = []\n\n    for idx in range(len(net_arch) - 1):\n        modules.append(nn.Linear(net_arch[idx], net_arch[idx + 1]))\n        modules.append(activation_fn())\n\n    if output_dim > 0:\n        last_layer_dim = net_arch[-1] if len(net_arch) > 0 else input_dim\n        modules.append(nn.Linear(last_layer_dim, output_dim))\n    if squash_output:\n        modules.append(nn.Tanh())\n    return modules\n\n\nclass MlpExtractor(nn.Module):\n    """"""\n    Constructs an MLP that receives observations as an input and outputs a latent representation for the policy and\n    a value network. The ``net_arch`` parameter allows to specify the amount and size of the hidden layers and how many\n    of them are shared between the policy network and the value network. It is assumed to be a list with the following\n    structure:\n\n    1. An arbitrary length (zero allowed) number of integers each specifying the number of units in a shared layer.\n       If the number of ints is zero, there will be no shared layers.\n    2. An optional dict, to specify the following non-shared layers for the value network and the policy network.\n       It is formatted like ``dict(vf=[<value layer sizes>], pi=[<policy layer sizes>])``.\n       If it is missing any of the keys (pi or vf), no non-shared layers (empty list) is assumed.\n\n    For example to construct a network with one shared layer of size 55 followed by two non-shared layers for the value\n    network of size 255 and a single non-shared layer of size 128 for the policy network, the following layers_spec\n    would be used: ``[55, dict(vf=[255, 255], pi=[128])]``. A simple shared network topology with two layers of size 128\n    would be specified as [128, 128].\n\n    Adapted from Stable Baselines.\n\n    :param feature_dim: (int) Dimension of the feature vector (can be the output of a CNN)\n    :param net_arch: ([int or dict]) The specification of the policy and value networks.\n        See above for details on its formatting.\n    :param activation_fn: (Type[nn.Module]) The activation function to use for the networks.\n    :param device: (th.device)\n    """"""\n\n    def __init__(self, feature_dim: int,\n                 net_arch: List[Union[int, Dict[str, List[int]]]],\n                 activation_fn: Type[nn.Module],\n                 device: Union[th.device, str] = \'auto\'):\n        super(MlpExtractor, self).__init__()\n        device = get_device(device)\n        shared_net, policy_net, value_net = [], [], []\n        policy_only_layers = []  # Layer sizes of the network that only belongs to the policy network\n        value_only_layers = []  # Layer sizes of the network that only belongs to the value network\n        last_layer_dim_shared = feature_dim\n\n        # Iterate through the shared layers and build the shared parts of the network\n        for idx, layer in enumerate(net_arch):\n            if isinstance(layer, int):  # Check that this is a shared layer\n                layer_size = layer\n                # TODO: give layer a meaningful name\n                shared_net.append(nn.Linear(last_layer_dim_shared, layer_size))\n                shared_net.append(activation_fn())\n                last_layer_dim_shared = layer_size\n            else:\n                assert isinstance(layer, dict), ""Error: the net_arch list can only contain ints and dicts""\n                if \'pi\' in layer:\n                    assert isinstance(layer[\'pi\'], list), ""Error: net_arch[-1][\'pi\'] must contain a list of integers.""\n                    policy_only_layers = layer[\'pi\']\n\n                if \'vf\' in layer:\n                    assert isinstance(layer[\'vf\'], list), ""Error: net_arch[-1][\'vf\'] must contain a list of integers.""\n                    value_only_layers = layer[\'vf\']\n                break  # From here on the network splits up in policy and value network\n\n        last_layer_dim_pi = last_layer_dim_shared\n        last_layer_dim_vf = last_layer_dim_shared\n\n        # Build the non-shared part of the network\n        for idx, (pi_layer_size, vf_layer_size) in enumerate(zip_longest(policy_only_layers, value_only_layers)):\n            if pi_layer_size is not None:\n                assert isinstance(pi_layer_size, int), ""Error: net_arch[-1][\'pi\'] must only contain integers.""\n                policy_net.append(nn.Linear(last_layer_dim_pi, pi_layer_size))\n                policy_net.append(activation_fn())\n                last_layer_dim_pi = pi_layer_size\n\n            if vf_layer_size is not None:\n                assert isinstance(vf_layer_size, int), ""Error: net_arch[-1][\'vf\'] must only contain integers.""\n                value_net.append(nn.Linear(last_layer_dim_vf, vf_layer_size))\n                value_net.append(activation_fn())\n                last_layer_dim_vf = vf_layer_size\n\n        # Save dim, used to create the distributions\n        self.latent_dim_pi = last_layer_dim_pi\n        self.latent_dim_vf = last_layer_dim_vf\n\n        # Create networks\n        # If the list of layers is empty, the network will just act as an Identity module\n        self.shared_net = nn.Sequential(*shared_net).to(device)\n        self.policy_net = nn.Sequential(*policy_net).to(device)\n        self.value_net = nn.Sequential(*value_net).to(device)\n\n    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n        """"""\n        :return: (th.Tensor, th.Tensor) latent_policy, latent_value of the specified network.\n            If all layers are shared, then ``latent_policy == latent_value``\n        """"""\n        shared_latent = self.shared_net(features)\n        return self.policy_net(shared_latent), self.value_net(shared_latent)\n'"
stable_baselines3/common/type_aliases.py,0,"b'""""""\nCommon aliases for type hint\n""""""\nfrom typing import Union, Dict, Any, NamedTuple, List, Callable, Tuple\n\nimport numpy as np\nimport torch as th\nimport gym\n\nfrom stable_baselines3.common.vec_env import VecEnv\nfrom stable_baselines3.common.callbacks import BaseCallback\n\n\nGymEnv = Union[gym.Env, VecEnv]\nGymObs = Union[Tuple, Dict[str, Any], np.ndarray, int]\nGymStepReturn = Tuple[GymObs, float, bool, Dict]\nTensorDict = Dict[str, th.Tensor]\nOptimizerStateDict = Dict[str, Any]\nMaybeCallback = Union[None, Callable, List[BaseCallback], BaseCallback]\n\n\nclass RolloutBufferSamples(NamedTuple):\n    observations: th.Tensor\n    actions: th.Tensor\n    old_values: th.Tensor\n    old_log_prob: th.Tensor\n    advantages: th.Tensor\n    returns: th.Tensor\n\n\nclass ReplayBufferSamples(NamedTuple):\n    observations: th.Tensor\n    actions: th.Tensor\n    next_observations: th.Tensor\n    dones: th.Tensor\n    rewards: th.Tensor\n\n\nclass RolloutReturn(NamedTuple):\n    episode_reward: float\n    episode_timesteps: int\n    n_episodes: int\n    continue_training: bool\n'"
stable_baselines3/common/utils.py,1,"b'from collections import deque\nfrom typing import Callable, Union, Optional\nimport random\nimport os\nimport glob\n\n\nimport gym\nimport numpy as np\nimport torch as th\n# Check if tensorboard is available for pytorch\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept ImportError:\n    SummaryWriter = None\n\nfrom stable_baselines3.common import logger\nfrom stable_baselines3.common.type_aliases import GymEnv\nfrom stable_baselines3.common.preprocessing import is_image_space\nfrom stable_baselines3.common.vec_env import VecTransposeImage\n\n\ndef set_random_seed(seed: int, using_cuda: bool = False) -> None:\n    """"""\n    Seed the different random generators\n    :param seed: (int)\n    :param using_cuda: (bool)\n    """"""\n    # Seed python RNG\n    random.seed(seed)\n    # Seed numpy RNG\n    np.random.seed(seed)\n    # seed the RNG for all devices (both CPU and CUDA)\n    th.manual_seed(seed)\n\n    if using_cuda:\n        # Deterministic operations for CuDNN, it may impact performances\n        th.backends.cudnn.deterministic = True\n        th.backends.cudnn.benchmark = False\n\n\n# From stable baselines\ndef explained_variance(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n    """"""\n    Computes fraction of variance that ypred explains about y.\n    Returns 1 - Var[y-ypred] / Var[y]\n\n    interpretation:\n        ev=0  =>  might as well have predicted zero\n        ev=1  =>  perfect prediction\n        ev<0  =>  worse than just predicting zero\n\n    :param y_pred: (np.ndarray) the prediction\n    :param y_true: (np.ndarray) the expected value\n    :return: (float) explained variance of ypred and y\n    """"""\n    assert y_true.ndim == 1 and y_pred.ndim == 1\n    var_y = np.var(y_true)\n    return np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n\n\ndef update_learning_rate(optimizer: th.optim.Optimizer, learning_rate: float) -> None:\n    """"""\n    Update the learning rate for a given optimizer.\n    Useful when doing linear schedule.\n\n    :param optimizer: (th.optim.Optimizer)\n    :param learning_rate: (float)\n    """"""\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = learning_rate\n\n\ndef get_schedule_fn(value_schedule: Union[Callable, float]) -> Callable:\n    """"""\n    Transform (if needed) learning rate and clip range (for PPO)\n    to callable.\n\n    :param value_schedule: (callable or float)\n    :return: (function)\n    """"""\n    # If the passed schedule is a float\n    # create a constant function\n    if isinstance(value_schedule, (float, int)):\n        # Cast to float to avoid errors\n        value_schedule = constant_fn(float(value_schedule))\n    else:\n        assert callable(value_schedule)\n    return value_schedule\n\n\ndef constant_fn(val: float) -> Callable:\n    """"""\n    Create a function that returns a constant\n    It is useful for learning rate schedule (to avoid code duplication)\n\n    :param val: (float)\n    :return: (Callable)\n    """"""\n\n    def func(_):\n        return val\n\n    return func\n\n\ndef get_device(device: Union[th.device, str] = \'auto\') -> th.device:\n    """"""\n    Retrieve PyTorch device.\n    It checks that the requested device is available first.\n    For now, it supports only cpu and cuda.\n    By default, it tries to use the gpu.\n\n    :param device: (Union[str, th.device]) One for \'auto\', \'cuda\', \'cpu\'\n    :return: (th.device)\n    """"""\n    # Cuda by default\n    if device == \'auto\':\n        device = \'cuda\'\n    # Force conversion to th.device\n    device = th.device(device)\n\n    # Cuda not available\n    if device == th.device(\'cuda\') and not th.cuda.is_available():\n        return th.device(\'cpu\')\n\n    return device\n\n\ndef get_latest_run_id(log_path: Optional[str] = None, log_name: str = \'\') -> int:\n    """"""\n    Returns the latest run number for the given log name and log path,\n    by finding the greatest number in the directories.\n\n    :return: (int) latest run number\n    """"""\n    max_run_id = 0\n    for path in glob.glob(f""{log_path}/{log_name}_[0-9]*""):\n        file_name = path.split(os.sep)[-1]\n        ext = file_name.split(""_"")[-1]\n        if log_name == ""_"".join(file_name.split(""_"")[:-1]) and ext.isdigit() and int(ext) > max_run_id:\n            max_run_id = int(ext)\n    return max_run_id\n\n\ndef configure_logger(verbose: int = 0, tensorboard_log: Optional[str] = None,\n                     tb_log_name: str = \'\', reset_num_timesteps: bool = True) -> None:\n    """"""\n    Configure the logger\'s outputs.\n\n    :param verbose: (int) the verbosity level: 0 no output, 1 info, 2 debug\n    :param tensorboard_log: (str) the log location for tensorboard (if None, no logging)\n    :param tb_log_name: (str) tensorboard log\n    """"""\n    if tensorboard_log is not None and SummaryWriter is not None:\n        latest_run_id = get_latest_run_id(tensorboard_log, tb_log_name)\n        if not reset_num_timesteps:\n            # Continue training in the same directory\n            latest_run_id -= 1\n        save_path = os.path.join(tensorboard_log, f""{tb_log_name}_{latest_run_id + 1}"")\n        if verbose >= 1:\n            logger.configure(save_path, [""stdout"", ""tensorboard""])\n        else:\n            logger.configure(save_path, [""tensorboard""])\n    elif verbose == 0:\n        logger.configure(format_strings=[""""])\n\n\ndef check_for_correct_spaces(env: GymEnv, observation_space: gym.spaces.Space, action_space: gym.spaces.Space):\n    """"""\n    Checks that the environment has same spaces as provided ones. Used by BaseAlgorithm to check if\n    spaces match after loading the model with given env.\n    Checked parameters:\n    - observation_space\n    - action_space\n\n    :param env: (GymEnv) Environment to check for valid spaces\n    :param observation_space: (gym.spaces.Space) Observation space to check against\n    :param action_space: (gym.spaces.Space) Action space to check against\n    """"""\n    if (observation_space != env.observation_space\n        # Special cases for images that need to be transposed\n        and not (is_image_space(env.observation_space)\n                 and observation_space == VecTransposeImage.transpose_space(env.observation_space))):\n        raise ValueError(f\'Observation spaces do not match: {observation_space} != {env.observation_space}\')\n    if action_space != env.action_space:\n        raise ValueError(f\'Action spaces do not match: {action_space} != {env.action_space}\')\n\n\ndef is_vectorized_observation(observation: np.ndarray, observation_space: gym.spaces.Space) -> bool:\n    """"""\n    For every observation type, detects and validates the shape,\n    then returns whether or not the observation is vectorized.\n\n    :param observation: (np.ndarray) the input observation to validate\n    :param observation_space: (gym.spaces) the observation space\n    :return: (bool) whether the given observation is vectorized or not\n    """"""\n    if isinstance(observation_space, gym.spaces.Box):\n        if observation.shape == observation_space.shape:\n            return False\n        elif observation.shape[1:] == observation_space.shape:\n            return True\n        else:\n            raise ValueError(f""Error: Unexpected observation shape {observation.shape} for ""\n                             + f""Box environment, please use {observation_space.shape} ""\n                             + ""or (n_env, {}) for the observation shape.""\n                             .format("", "".join(map(str, observation_space.shape))))\n    elif isinstance(observation_space, gym.spaces.Discrete):\n        if observation.shape == ():  # A numpy array of a number, has shape empty tuple \'()\'\n            return False\n        elif len(observation.shape) == 1:\n            return True\n        else:\n            raise ValueError(f""Error: Unexpected observation shape {observation.shape} for ""\n                             + ""Discrete environment, please use (1,) or (n_env, 1) for the observation shape."")\n\n    elif isinstance(observation_space, gym.spaces.MultiDiscrete):\n        if observation.shape == (len(observation_space.nvec),):\n            return False\n        elif len(observation.shape) == 2 and observation.shape[1] == len(observation_space.nvec):\n            return True\n        else:\n            raise ValueError(f""Error: Unexpected observation shape {observation.shape} for MultiDiscrete ""\n                             + f""environment, please use ({len(observation_space.nvec)},) or ""\n                             + f""(n_env, {len(observation_space.nvec)}) for the observation shape."")\n    elif isinstance(observation_space, gym.spaces.MultiBinary):\n        if observation.shape == (observation_space.n,):\n            return False\n        elif len(observation.shape) == 2 and observation.shape[1] == observation_space.n:\n            return True\n        else:\n            raise ValueError(f""Error: Unexpected observation shape {observation.shape} for MultiBinary ""\n                             + f""environment, please use ({observation_space.n},) or ""\n                             + f""(n_env, {observation_space.n}) for the observation shape."")\n    else:\n        raise ValueError(""Error: Cannot determine if the observation is vectorized ""\n                         + f"" with the space type {observation_space}."")\n\n\ndef safe_mean(arr: Union[np.ndarray, list, deque]) -> np.ndarray:\n    """"""\n    Compute the mean of an array if there is at least one element.\n    For empty array, return NaN. It is used for logging only.\n\n    :param arr:\n    :return:\n    """"""\n    return np.nan if len(arr) == 0 else np.mean(arr)\n'"
stable_baselines3/ppo/__init__.py,0,"b'from stable_baselines3.ppo.ppo import PPO\nfrom stable_baselines3.ppo.policies import MlpPolicy, CnnPolicy\n'"
stable_baselines3/ppo/policies.py,0,"b'# This file is here just to define MlpPolicy/CnnPolicy\n# that work for PPO\nfrom stable_baselines3.common.policies import ActorCriticPolicy, ActorCriticCnnPolicy, register_policy\n\nMlpPolicy = ActorCriticPolicy\nCnnPolicy = ActorCriticCnnPolicy\n\nregister_policy(""MlpPolicy"", ActorCriticPolicy)\nregister_policy(""CnnPolicy"", ActorCriticCnnPolicy)\n'"
stable_baselines3/ppo/ppo.py,1,"b'from typing import Type, Union, Callable, Optional, Dict, Any\n\nfrom gym import spaces\nimport torch as th\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom stable_baselines3.common import logger\nfrom stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\nfrom stable_baselines3.common.type_aliases import GymEnv, MaybeCallback\nfrom stable_baselines3.common.utils import explained_variance, get_schedule_fn\nfrom stable_baselines3.common.policies import ActorCriticPolicy\n\n\nclass PPO(OnPolicyAlgorithm):\n    """"""\n    Proximal Policy Optimization algorithm (PPO) (clip version)\n\n    Paper: https://arxiv.org/abs/1707.06347\n    Code: This implementation borrows code from OpenAI Spinning Up (https://github.com/openai/spinningup/)\n    https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\n    and Stable Baselines (PPO2 from https://github.com/hill-a/stable-baselines)\n\n    Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n\n    :param policy: (ActorCriticPolicy or str) The policy model to use (MlpPolicy, CnnPolicy, ...)\n    :param env: (Gym environment or str) The environment to learn from (if registered in Gym, can be str)\n    :param learning_rate: (float or callable) The learning rate, it can be a function\n        of the current progress remaining (from 1 to 0)\n    :param n_steps: (int) The number of steps to run for each environment per update\n        (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n    :param batch_size: (int) Minibatch size\n    :param n_epochs: (int) Number of epoch when optimizing the surrogate loss\n    :param gamma: (float) Discount factor\n    :param gae_lambda: (float) Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n    :param clip_range: (float or callable) Clipping parameter, it can be a function of the current progress\n        remaining (from 1 to 0).\n    :param clip_range_vf: (float or callable) Clipping parameter for the value function,\n        it can be a function of the current progress remaining (from 1 to 0).\n        This is a parameter specific to the OpenAI implementation. If None is passed (default),\n        no clipping will be done on the value function.\n        IMPORTANT: this clipping depends on the reward scaling.\n    :param ent_coef: (float) Entropy coefficient for the loss calculation\n    :param vf_coef: (float) Value function coefficient for the loss calculation\n    :param max_grad_norm: (float) The maximum value for the gradient clipping\n    :param use_sde: (bool) Whether to use generalized State Dependent Exploration (gSDE)\n        instead of action noise exploration (default: False)\n    :param sde_sample_freq: (int) Sample a new noise matrix every n steps when using gSDE\n        Default: -1 (only sample at the beginning of the rollout)\n    :param target_kl: (float) Limit the KL divergence between updates,\n        because the clipping is not enough to prevent large update\n        see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\n        By default, there is no limit on the kl div.\n    :param tensorboard_log: (str) the log location for tensorboard (if None, no logging)\n    :param create_eval_env: (bool) Whether to create a second environment that will be\n        used for evaluating the agent periodically. (Only available when passing string for the environment)\n    :param policy_kwargs: (dict) additional arguments to be passed to the policy on creation\n    :param verbose: (int) the verbosity level: 0 no output, 1 info, 2 debug\n    :param seed: (int) Seed for the pseudo random generators\n    :param device: (str or th.device) Device (cpu, cuda, ...) on which the code should be run.\n        Setting it to auto, the code will be run on the GPU if possible.\n    :param _init_setup_model: (bool) Whether or not to build the network at the creation of the instance\n    """"""\n\n    def __init__(self, policy: Union[str, Type[ActorCriticPolicy]],\n                 env: Union[GymEnv, str],\n                 learning_rate: Union[float, Callable] = 3e-4,\n                 n_steps: int = 2048,\n                 batch_size: Optional[int] = 64,\n                 n_epochs: int = 10,\n                 gamma: float = 0.99,\n                 gae_lambda: float = 0.95,\n                 clip_range: float = 0.2,\n                 clip_range_vf: Optional[float] = None,\n                 ent_coef: float = 0.0,\n                 vf_coef: float = 0.5,\n                 max_grad_norm: float = 0.5,\n                 use_sde: bool = False,\n                 sde_sample_freq: int = -1,\n                 target_kl: Optional[float] = None,\n                 tensorboard_log: Optional[str] = None,\n                 create_eval_env: bool = False,\n                 policy_kwargs: Optional[Dict[str, Any]] = None,\n                 verbose: int = 0,\n                 seed: Optional[int] = None,\n                 device: Union[th.device, str] = ""auto"",\n                 _init_setup_model: bool = True):\n\n        super(PPO, self).__init__(policy, env, learning_rate=learning_rate,\n                                  n_steps=n_steps, gamma=gamma, gae_lambda=gae_lambda,\n                                  ent_coef=ent_coef, vf_coef=vf_coef, max_grad_norm=max_grad_norm,\n                                  use_sde=use_sde, sde_sample_freq=sde_sample_freq,\n                                  tensorboard_log=tensorboard_log, policy_kwargs=policy_kwargs,\n                                  verbose=verbose, device=device, create_eval_env=create_eval_env,\n                                  seed=seed, _init_setup_model=False)\n\n        self.batch_size = batch_size\n        self.n_epochs = n_epochs\n        self.clip_range = clip_range\n        self.clip_range_vf = clip_range_vf\n        self.target_kl = target_kl\n\n        if _init_setup_model:\n            self._setup_model()\n\n    def _setup_model(self) -> None:\n        super(PPO, self)._setup_model()\n\n        # Initialize schedules for policy/value clipping\n        self.clip_range = get_schedule_fn(self.clip_range)\n        if self.clip_range_vf is not None:\n            if isinstance(self.clip_range_vf, (float, int)):\n                assert self.clip_range_vf > 0, (""`clip_range_vf` must be positive, ""\n                                                ""pass `None` to deactivate vf clipping"")\n\n            self.clip_range_vf = get_schedule_fn(self.clip_range_vf)\n\n    def train(self) -> None:\n        """"""\n        Update policy using the currently gathered\n        rollout buffer.\n        """"""\n        # Update optimizer learning rate\n        self._update_learning_rate(self.policy.optimizer)\n        # Compute current clip range\n        clip_range = self.clip_range(self._current_progress_remaining)\n        # Optional: clip range for the value function\n        if self.clip_range_vf is not None:\n            clip_range_vf = self.clip_range_vf(self._current_progress_remaining)\n\n        entropy_losses, all_kl_divs = [], []\n        pg_losses, value_losses = [], []\n        clip_fractions = []\n\n        # train for gradient_steps epochs\n        for epoch in range(self.n_epochs):\n            approx_kl_divs = []\n            # Do a complete pass on the rollout buffer\n            for rollout_data in self.rollout_buffer.get(self.batch_size):\n                actions = rollout_data.actions\n                if isinstance(self.action_space, spaces.Discrete):\n                    # Convert discrete action from float to long\n                    actions = rollout_data.actions.long().flatten()\n\n                # Re-sample the noise matrix because the log_std has changed\n                # TODO: investigate why there is no issue with the gradient\n                # if that line is commented (as in SAC)\n                if self.use_sde:\n                    self.policy.reset_noise(self.batch_size)\n\n                values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)\n                values = values.flatten()\n                # Normalize advantage\n                advantages = rollout_data.advantages\n                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n                # ratio between old and new policy, should be one at the first iteration\n                ratio = th.exp(log_prob - rollout_data.old_log_prob)\n\n                # clipped surrogate loss\n                policy_loss_1 = advantages * ratio\n                policy_loss_2 = advantages * th.clamp(ratio, 1 - clip_range, 1 + clip_range)\n                policy_loss = -th.min(policy_loss_1, policy_loss_2).mean()\n\n                # Logging\n                pg_losses.append(policy_loss.item())\n                clip_fraction = th.mean((th.abs(ratio - 1) > clip_range).float()).item()\n                clip_fractions.append(clip_fraction)\n\n                if self.clip_range_vf is None:\n                    # No clipping\n                    values_pred = values\n                else:\n                    # Clip the different between old and new value\n                    # NOTE: this depends on the reward scaling\n                    values_pred = rollout_data.old_values + th.clamp(values - rollout_data.old_values, -clip_range_vf,\n                                                                     clip_range_vf)\n                # Value loss using the TD(gae_lambda) target\n                value_loss = F.mse_loss(rollout_data.returns, values_pred)\n                value_losses.append(value_loss.item())\n\n                # Entropy loss favor exploration\n                if entropy is None:\n                    # Approximate entropy when no analytical form\n                    entropy_loss = -log_prob.mean()\n                else:\n                    entropy_loss = -th.mean(entropy)\n\n                entropy_losses.append(entropy_loss.item())\n\n                loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n\n                # Optimization step\n                self.policy.optimizer.zero_grad()\n                loss.backward()\n                # Clip grad norm\n                th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n                self.policy.optimizer.step()\n                approx_kl_divs.append(th.mean(rollout_data.old_log_prob - log_prob).detach().cpu().numpy())\n\n            all_kl_divs.append(np.mean(approx_kl_divs))\n\n            if self.target_kl is not None and np.mean(approx_kl_divs) > 1.5 * self.target_kl:\n                print(f""Early stopping at step {epoch} due to reaching max kl: {np.mean(approx_kl_divs):.2f}"")\n                break\n\n        self._n_updates += self.n_epochs\n        explained_var = explained_variance(self.rollout_buffer.returns.flatten(),\n                                           self.rollout_buffer.values.flatten())\n\n        # Logs\n        logger.record(""train/entropy_loss"", np.mean(entropy_losses))\n        logger.record(""train/policy_gradient_loss"", np.mean(pg_losses))\n        logger.record(""train/value_loss"", np.mean(value_losses))\n        logger.record(""train/approx_kl"", np.mean(approx_kl_divs))\n        logger.record(""train/clip_fraction"", np.mean(clip_fraction))\n        logger.record(""train/loss"", loss.item())\n        logger.record(""train/explained_variance"", explained_var)\n        if hasattr(self.policy, ""log_std""):\n            logger.record(""train/std"", th.exp(self.policy.log_std).mean().item())\n\n        logger.record(""train/n_updates"", self._n_updates, exclude=""tensorboard"")\n        logger.record(""train/clip_range"", clip_range)\n        if self.clip_range_vf is not None:\n            logger.record(""train/clip_range_vf"", clip_range_vf)\n\n    def learn(self,\n              total_timesteps: int,\n              callback: MaybeCallback = None,\n              log_interval: int = 1,\n              eval_env: Optional[GymEnv] = None,\n              eval_freq: int = -1,\n              n_eval_episodes: int = 5,\n              tb_log_name: str = ""PPO"",\n              eval_log_path: Optional[str] = None,\n              reset_num_timesteps: bool = True) -> ""PPO"":\n\n        return super(PPO, self).learn(total_timesteps=total_timesteps, callback=callback,\n                                      log_interval=log_interval, eval_env=eval_env, eval_freq=eval_freq,\n                                      n_eval_episodes=n_eval_episodes, tb_log_name=tb_log_name,\n                                      eval_log_path=eval_log_path, reset_num_timesteps=reset_num_timesteps)\n'"
stable_baselines3/sac/__init__.py,0,"b'from stable_baselines3.sac.sac import SAC\nfrom stable_baselines3.sac.policies import MlpPolicy, CnnPolicy\n'"
stable_baselines3/sac/policies.py,1,"b'from typing import Optional, List, Tuple, Callable, Union, Type, Dict, Any\n\nimport gym\nimport torch as th\nimport torch.nn as nn\n\nfrom stable_baselines3.common.preprocessing import get_action_dim\nfrom stable_baselines3.common.policies import BasePolicy, register_policy, create_sde_features_extractor\nfrom stable_baselines3.common.torch_layers import create_mlp, NatureCNN, BaseFeaturesExtractor, FlattenExtractor\nfrom stable_baselines3.common.distributions import SquashedDiagGaussianDistribution, StateDependentNoiseDistribution\n\n# CAP the standard deviation of the actor\nLOG_STD_MAX = 2\nLOG_STD_MIN = -20\n\n\nclass Actor(BasePolicy):\n    """"""\n    Actor network (policy) for SAC.\n\n    :param observation_space: (gym.spaces.Space) Obervation space\n    :param action_space: (gym.spaces.Space) Action space\n    :param net_arch: ([int]) Network architecture\n    :param features_extractor: (nn.Module) Network to extract features\n        (a CNN when using images, a nn.Flatten() layer otherwise)\n    :param features_dim: (int) Number of features\n    :param activation_fn: (Type[nn.Module]) Activation function\n    :param use_sde: (bool) Whether to use State Dependent Exploration or not\n    :param log_std_init: (float) Initial value for the log standard deviation\n    :param full_std: (bool) Whether to use (n_features x n_actions) parameters\n        for the std instead of only (n_features,) when using gSDE.\n    :param sde_net_arch: ([int]) Network architecture for extracting features\n        when using gSDE. If None, the latent features from the policy will be used.\n        Pass an empty list to use the states as features.\n    :param use_expln: (bool) Use ``expln()`` function instead of ``exp()`` when using gSDE to ensure\n        a positive standard deviation (cf paper). It allows to keep variance\n        above zero and prevent it from growing too fast. In practice, ``exp()`` is usually enough.\n    :param clip_mean: (float) Clip the mean output when using gSDE to avoid numerical instability.\n    :param normalize_images: (bool) Whether to normalize images or not,\n         dividing by 255.0 (True by default)\n    :param device: (Union[th.device, str]) Device on which the code should run.\n    """"""\n\n    def __init__(self, observation_space: gym.spaces.Space,\n                 action_space: gym.spaces.Space,\n                 net_arch: List[int],\n                 features_extractor: nn.Module,\n                 features_dim: int,\n                 activation_fn: Type[nn.Module] = nn.ReLU,\n                 use_sde: bool = False,\n                 log_std_init: float = -3,\n                 full_std: bool = True,\n                 sde_net_arch: Optional[List[int]] = None,\n                 use_expln: bool = False,\n                 clip_mean: float = 2.0,\n                 normalize_images: bool = True,\n                 device: Union[th.device, str] = \'auto\'):\n        super(Actor, self).__init__(observation_space, action_space,\n                                    features_extractor=features_extractor,\n                                    normalize_images=normalize_images,\n                                    device=device,\n                                    squash_output=True)\n\n        # Save arguments to re-create object at loading\n        self.use_sde = use_sde\n        self.sde_features_extractor = None\n        self.sde_net_arch = sde_net_arch\n        self.net_arch = net_arch\n        self.features_dim = features_dim\n        self.activation_fn = activation_fn\n        self.log_std_init = log_std_init\n        self.sde_net_arch = sde_net_arch\n        self.use_expln = use_expln\n        self.full_std = full_std\n        self.clip_mean = clip_mean\n\n        action_dim = get_action_dim(self.action_space)\n        latent_pi_net = create_mlp(features_dim, -1, net_arch, activation_fn)\n        self.latent_pi = nn.Sequential(*latent_pi_net)\n        last_layer_dim = net_arch[-1] if len(net_arch) > 0 else features_dim\n\n        if self.use_sde:\n            latent_sde_dim = last_layer_dim\n            # Separate feature extractor for gSDE\n            if sde_net_arch is not None:\n                self.sde_features_extractor, latent_sde_dim = create_sde_features_extractor(features_dim, sde_net_arch,\n                                                                                            activation_fn)\n\n            self.action_dist = StateDependentNoiseDistribution(action_dim, full_std=full_std, use_expln=use_expln,\n                                                               learn_features=True, squash_output=True)\n            self.mu, self.log_std = self.action_dist.proba_distribution_net(latent_dim=last_layer_dim,\n                                                                            latent_sde_dim=latent_sde_dim,\n                                                                            log_std_init=log_std_init)\n            # Avoid numerical issues by limiting the mean of the Gaussian\n            # to be in [-clip_mean, clip_mean]\n            if clip_mean > 0.0:\n                self.mu = nn.Sequential(self.mu, nn.Hardtanh(min_val=-clip_mean, max_val=clip_mean))\n        else:\n            self.action_dist = SquashedDiagGaussianDistribution(action_dim)\n            self.mu = nn.Linear(last_layer_dim, action_dim)\n            self.log_std = nn.Linear(last_layer_dim, action_dim)\n\n    def _get_data(self) -> Dict[str, Any]:\n        data = super()._get_data()\n\n        data.update(dict(\n            net_arch=self.net_arch,\n            features_dim=self.features_dim,\n            activation_fn=self.activation_fn,\n            use_sde=self.use_sde,\n            log_std_init=self.log_std_init,\n            full_std=self.full_std,\n            sde_net_arch=self.sde_net_arch,\n            use_expln=self.use_expln,\n            features_extractor=self.features_extractor,\n            clip_mean=self.clip_mean\n        ))\n        return data\n\n    def get_std(self) -> th.Tensor:\n        """"""\n        Retrieve the standard deviation of the action distribution.\n        Only useful when using gSDE.\n        It corresponds to ``th.exp(log_std)`` in the normal case,\n        but is slightly different when using ``expln`` function\n        (cf StateDependentNoiseDistribution doc).\n\n        :return: (th.Tensor)\n        """"""\n        msg = \'get_std() is only available when using gSDE\'\n        assert isinstance(self.action_dist, StateDependentNoiseDistribution), msg\n        return self.action_dist.get_std(self.log_std)\n\n    def reset_noise(self, batch_size: int = 1) -> None:\n        """"""\n        Sample new weights for the exploration matrix, when using gSDE.\n\n        :param batch_size: (int)\n        """"""\n        msg = \'reset_noise() is only available when using gSDE\'\n        assert isinstance(self.action_dist, StateDependentNoiseDistribution), msg\n        self.action_dist.sample_weights(self.log_std, batch_size=batch_size)\n\n    def get_action_dist_params(self, obs: th.Tensor) -> Tuple[th.Tensor, th.Tensor, Dict[str, th.Tensor]]:\n        """"""\n        Get the parameters for the action distribution.\n\n        :param obs: (th.Tensor)\n        :return: (Tuple[th.Tensor, th.Tensor, Dict[str, th.Tensor]])\n            Mean, standard deviation and optional keyword arguments.\n        """"""\n        features = self.extract_features(obs)\n        latent_pi = self.latent_pi(features)\n        mean_actions = self.mu(latent_pi)\n\n        if self.use_sde:\n            latent_sde = latent_pi\n            if self.sde_features_extractor is not None:\n                latent_sde = self.sde_features_extractor(features)\n            return mean_actions, self.log_std, dict(latent_sde=latent_sde)\n        # Unstructured exploration (Original implementation)\n        log_std = self.log_std(latent_pi)\n        # Original Implementation to cap the standard deviation\n        log_std = th.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)\n        return mean_actions, log_std, {}\n\n    def forward(self, obs: th.Tensor, deterministic: bool = False) -> th.Tensor:\n        mean_actions, log_std, kwargs = self.get_action_dist_params(obs)\n        # Note: the action is squashed\n        return self.action_dist.actions_from_params(mean_actions, log_std,\n                                                    deterministic=deterministic, **kwargs)\n\n    def action_log_prob(self, obs: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n        mean_actions, log_std, kwargs = self.get_action_dist_params(obs)\n        # return action and associated log prob\n        return self.action_dist.log_prob_from_params(mean_actions, log_std, **kwargs)\n\n    def _predict(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:\n        return self.forward(observation, deterministic)\n\n\nclass Critic(BasePolicy):\n    """"""\n    Critic network (q-value function) for SAC.\n\n    :param observation_space: (gym.spaces.Space) Obervation space\n    :param action_space: (gym.spaces.Space) Action space\n    :param net_arch: ([int]) Network architecture\n    :param features_extractor: (nn.Module) Network to extract features\n        (a CNN when using images, a nn.Flatten() layer otherwise)\n    :param features_dim: (int) Number of features\n    :param activation_fn: (Type[nn.Module]) Activation function\n    :param normalize_images: (bool) Whether to normalize images or not,\n         dividing by 255.0 (True by default)\n    :param device: (Union[th.device, str]) Device on which the code should run.\n    """"""\n\n    def __init__(self, observation_space: gym.spaces.Space,\n                 action_space: gym.spaces.Space,\n                 net_arch: List[int],\n                 features_extractor: nn.Module,\n                 features_dim: int,\n                 activation_fn: Type[nn.Module] = nn.ReLU,\n                 normalize_images: bool = True,\n                 device: Union[th.device, str] = \'auto\'):\n        super(Critic, self).__init__(observation_space, action_space,\n                                     features_extractor=features_extractor,\n                                     normalize_images=normalize_images,\n                                     device=device)\n\n        action_dim = get_action_dim(self.action_space)\n\n        q1_net = create_mlp(features_dim + action_dim, 1, net_arch, activation_fn)\n        self.q1_net = nn.Sequential(*q1_net)\n\n        q2_net = create_mlp(features_dim + action_dim, 1, net_arch, activation_fn)\n        self.q2_net = nn.Sequential(*q2_net)\n\n        self.q_networks = [self.q1_net, self.q2_net]\n\n    def forward(self, obs: th.Tensor, action: th.Tensor) -> List[th.Tensor]:\n        # Learn the features extractor using the policy loss only\n        # this is much faster\n        with th.no_grad():\n            features = self.extract_features(obs)\n        qvalue_input = th.cat([features, action], dim=1)\n        return [q_net(qvalue_input) for q_net in self.q_networks]\n\n\nclass SACPolicy(BasePolicy):\n    """"""\n    Policy class (with both actor and critic) for SAC.\n\n    :param observation_space: (gym.spaces.Space) Observation space\n    :param action_space: (gym.spaces.Space) Action space\n    :param lr_schedule: (callable) Learning rate schedule (could be constant)\n    :param net_arch: (Optional[List[int]]) The specification of the policy and value networks.\n    :param device: (str or th.device) Device on which the code should run.\n    :param activation_fn: (Type[nn.Module]) Activation function\n    :param use_sde: (bool) Whether to use State Dependent Exploration or not\n    :param log_std_init: (float) Initial value for the log standard deviation\n    :param sde_net_arch: ([int]) Network architecture for extracting features\n        when using gSDE. If None, the latent features from the policy will be used.\n        Pass an empty list to use the states as features.\n    :param use_expln: (bool) Use ``expln()`` function instead of ``exp()`` when using gSDE to ensure\n        a positive standard deviation (cf paper). It allows to keep variance\n        above zero and prevent it from growing too fast. In practice, ``exp()`` is usually enough.\n    :param clip_mean: (float) Clip the mean output when using gSDE to avoid numerical instability.\n    :param features_extractor_class: (Type[BaseFeaturesExtractor]) Features extractor to use.\n    :param features_extractor_kwargs: (Optional[Dict[str, Any]]) Keyword arguments\n        to pass to the feature extractor.\n    :param normalize_images: (bool) Whether to normalize images or not,\n         dividing by 255.0 (True by default)\n    :param optimizer_class: (Type[th.optim.Optimizer]) The optimizer to use,\n        ``th.optim.Adam`` by default\n    :param optimizer_kwargs: (Optional[Dict[str, Any]]) Additional keyword arguments,\n        excluding the learning rate, to pass to the optimizer\n    """"""\n\n    def __init__(self, observation_space: gym.spaces.Space,\n                 action_space: gym.spaces.Space,\n                 lr_schedule: Callable,\n                 net_arch: Optional[List[int]] = None,\n                 device: Union[th.device, str] = \'auto\',\n                 activation_fn: Type[nn.Module] = nn.ReLU,\n                 use_sde: bool = False,\n                 log_std_init: float = -3,\n                 sde_net_arch: Optional[List[int]] = None,\n                 use_expln: bool = False,\n                 clip_mean: float = 2.0,\n                 features_extractor_class: Type[BaseFeaturesExtractor] = FlattenExtractor,\n                 features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n                 normalize_images: bool = True,\n                 optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n                 optimizer_kwargs: Optional[Dict[str, Any]] = None):\n        super(SACPolicy, self).__init__(observation_space, action_space,\n                                        device,\n                                        features_extractor_class,\n                                        features_extractor_kwargs,\n                                        optimizer_class=optimizer_class,\n                                        optimizer_kwargs=optimizer_kwargs,\n                                        squash_output=True)\n\n        if net_arch is None:\n            if features_extractor_class == FlattenExtractor:\n                net_arch = [256, 256]\n            else:\n                net_arch = []\n\n        # Create shared features extractor\n        self.features_extractor = features_extractor_class(self.observation_space,\n                                                           **self.features_extractor_kwargs)\n        self.features_dim = self.features_extractor.features_dim\n\n        self.net_arch = net_arch\n        self.activation_fn = activation_fn\n        self.net_args = {\n            \'observation_space\': self.observation_space,\n            \'action_space\': self.action_space,\n            \'features_extractor\': self.features_extractor,\n            \'features_dim\': self.features_dim,\n            \'net_arch\': self.net_arch,\n            \'activation_fn\': self.activation_fn,\n            \'normalize_images\': normalize_images,\n            \'device\': device\n        }\n        self.actor_kwargs = self.net_args.copy()\n        sde_kwargs = {\n            \'use_sde\': use_sde,\n            \'log_std_init\': log_std_init,\n            \'sde_net_arch\': sde_net_arch,\n            \'use_expln\': use_expln,\n            \'clip_mean\': clip_mean\n        }\n        self.actor_kwargs.update(sde_kwargs)\n        self.actor, self.actor_target = None, None\n        self.critic, self.critic_target = None, None\n\n        self._build(lr_schedule)\n\n    def _build(self, lr_schedule: Callable) -> None:\n        self.actor = self.make_actor()\n        self.actor.optimizer = self.optimizer_class(self.actor.parameters(), lr=lr_schedule(1),\n                                                    **self.optimizer_kwargs)\n\n        self.critic = self.make_critic()\n        self.critic_target = self.make_critic()\n        self.critic_target.load_state_dict(self.critic.state_dict())\n        # Do not optimize the shared feature extractor with the critic loss\n        # otherwise, there are gradient computation issues\n        # Another solution: having duplicated features extractor but requires more memory and computation\n        critic_parameters = [param for name, param in self.critic.named_parameters() if\n                             \'features_extractor\' not in name]\n        self.critic.optimizer = self.optimizer_class(critic_parameters, lr=lr_schedule(1),\n                                                     **self.optimizer_kwargs)\n\n    def _get_data(self) -> Dict[str, Any]:\n        data = super()._get_data()\n\n        data.update(dict(\n            net_arch=self.net_args[\'net_arch\'],\n            activation_fn=self.net_args[\'activation_fn\'],\n            use_sde=self.actor_kwargs[\'use_sde\'],\n            log_std_init=self.actor_kwargs[\'log_std_init\'],\n            sde_net_arch=self.actor_kwargs[\'sde_net_arch\'],\n            use_expln=self.actor_kwargs[\'use_expln\'],\n            clip_mean=self.actor_kwargs[\'clip_mean\'],\n            lr_schedule=self._dummy_schedule,  # dummy lr schedule, not needed for loading policy alone\n            optimizer_class=self.optimizer_class,\n            optimizer_kwargs=self.optimizer_kwargs,\n            features_extractor_class=self.features_extractor_class,\n            features_extractor_kwargs=self.features_extractor_kwargs\n        ))\n        return data\n\n    def reset_noise(self, batch_size: int = 1) -> None:\n        """"""\n        Sample new weights for the exploration matrix, when using gSDE.\n\n        :param batch_size: (int)\n        """"""\n        self.actor.reset_noise(batch_size=batch_size)\n\n    def make_actor(self) -> Actor:\n        return Actor(**self.actor_kwargs).to(self.device)\n\n    def make_critic(self) -> Critic:\n        return Critic(**self.net_args).to(self.device)\n\n    def forward(self, obs: th.Tensor, deterministic: bool = False) -> th.Tensor:\n        return self._predict(obs, deterministic=deterministic)\n\n    def _predict(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:\n        return self.actor(observation, deterministic)\n\n\nMlpPolicy = SACPolicy\n\n\nclass CnnPolicy(SACPolicy):\n    """"""\n    Policy class (with both actor and critic) for SAC.\n\n    :param observation_space: (gym.spaces.Space) Observation space\n    :param action_space: (gym.spaces.Space) Action space\n    :param lr_schedule: (callable) Learning rate schedule (could be constant)\n    :param net_arch: (Optional[List[int]]) The specification of the policy and value networks.\n    :param device: (str or th.device) Device on which the code should run.\n    :param activation_fn: (Type[nn.Module]) Activation function\n    :param use_sde: (bool) Whether to use State Dependent Exploration or not\n    :param log_std_init: (float) Initial value for the log standard deviation\n    :param sde_net_arch: ([int]) Network architecture for extracting features\n        when using gSDE. If None, the latent features from the policy will be used.\n        Pass an empty list to use the states as features.\n    :param use_expln: (bool) Use ``expln()`` function instead of ``exp()`` when using gSDE to ensure\n        a positive standard deviation (cf paper). It allows to keep variance\n        above zero and prevent it from growing too fast. In practice, ``exp()`` is usually enough.\n    :param clip_mean: (float) Clip the mean output when using gSDE to avoid numerical instability.\n    :param features_extractor_class: (Type[BaseFeaturesExtractor]) Features extractor to use.\n    :param normalize_images: (bool) Whether to normalize images or not,\n         dividing by 255.0 (True by default)\n    :param optimizer_class: (Type[th.optim.Optimizer]) The optimizer to use,\n        ``th.optim.Adam`` by default\n    :param optimizer_kwargs: (Optional[Dict[str, Any]]) Additional keyword arguments,\n        excluding the learning rate, to pass to the optimizer\n    """"""\n\n    def __init__(self, observation_space: gym.spaces.Space,\n                 action_space: gym.spaces.Space,\n                 lr_schedule: Callable,\n                 net_arch: Optional[List[int]] = None,\n                 device: Union[th.device, str] = \'auto\',\n                 activation_fn: Type[nn.Module] = nn.ReLU,\n                 use_sde: bool = False,\n                 log_std_init: float = -3,\n                 sde_net_arch: Optional[List[int]] = None,\n                 use_expln: bool = False,\n                 clip_mean: float = 2.0,\n                 features_extractor_class: Type[BaseFeaturesExtractor] = NatureCNN,\n                 features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n                 normalize_images: bool = True,\n                 optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n                 optimizer_kwargs: Optional[Dict[str, Any]] = None):\n        super(CnnPolicy, self).__init__(observation_space,\n                                        action_space,\n                                        lr_schedule,\n                                        net_arch,\n                                        device,\n                                        activation_fn,\n                                        use_sde,\n                                        log_std_init,\n                                        sde_net_arch,\n                                        use_expln,\n                                        clip_mean,\n                                        features_extractor_class,\n                                        features_extractor_kwargs,\n                                        normalize_images,\n                                        optimizer_class,\n                                        optimizer_kwargs)\n\n\nregister_policy(""MlpPolicy"", MlpPolicy)\nregister_policy(""CnnPolicy"", CnnPolicy)\n'"
stable_baselines3/sac/sac.py,1,"b'from typing import List, Tuple, Type, Union, Callable, Optional, Dict, Any\nimport torch as th\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom stable_baselines3.common import logger\nfrom stable_baselines3.common.off_policy_algorithm import OffPolicyAlgorithm\nfrom stable_baselines3.common.type_aliases import GymEnv, MaybeCallback\nfrom stable_baselines3.common.noise import ActionNoise\nfrom stable_baselines3.sac.policies import SACPolicy\n\n\nclass SAC(OffPolicyAlgorithm):\n    """"""\n    Soft Actor-Critic (SAC)\n    Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,\n    This implementation borrows code from original implementation (https://github.com/haarnoja/sac)\n    from OpenAI Spinning Up (https://github.com/openai/spinningup), from the softlearning repo\n    (https://github.com/rail-berkeley/softlearning/)\n    and from Stable Baselines (https://github.com/hill-a/stable-baselines)\n    Paper: https://arxiv.org/abs/1801.01290\n    Introduction to SAC: https://spinningup.openai.com/en/latest/algorithms/sac.html\n\n    Note: we use double q target and not value target as discussed\n    in https://github.com/hill-a/stable-baselines/issues/270\n\n    :param policy: (SACPolicy or str) The policy model to use (MlpPolicy, CnnPolicy, ...)\n    :param env: (GymEnv or str) The environment to learn from (if registered in Gym, can be str)\n    :param learning_rate: (float or callable) learning rate for adam optimizer,\n        the same learning rate will be used for all networks (Q-Values, Actor and Value function)\n        it can be a function of the current progress remaining (from 1 to 0)\n    :param buffer_size: (int) size of the replay buffer\n    :param learning_starts: (int) how many steps of the model to collect transitions for before learning starts\n    :param batch_size: (int) Minibatch size for each gradient update\n    :param tau: (float) the soft update coefficient (""Polyak update"", between 0 and 1)\n    :param gamma: (float) the discount factor\n    :param train_freq: (int) Update the model every ``train_freq`` steps.\n    :param gradient_steps: (int) How many gradient update after each step\n    :param n_episodes_rollout: (int) Update the model every ``n_episodes_rollout`` episodes.\n        Note that this cannot be used at the same time as ``train_freq``\n    :param action_noise: (ActionNoise) the action noise type (None by default), this can help\n        for hard exploration problem. Cf common.noise for the different action noise type.\n    :param ent_coef: (str or float) Entropy regularization coefficient. (Equivalent to\n        inverse of reward scale in the original SAC paper.)  Controlling exploration/exploitation trade-off.\n        Set it to \'auto\' to learn it automatically (and \'auto_0.1\' for using 0.1 as initial value)\n    :param target_update_interval: (int) update the target network every ``target_network_update_freq`` steps.\n    :param target_entropy: (str or float) target entropy when learning ``ent_coef`` (``ent_coef = \'auto\'``)\n    :param use_sde: (bool) Whether to use generalized State Dependent Exploration (gSDE)\n        instead of action noise exploration (default: False)\n    :param sde_sample_freq: (int) Sample a new noise matrix every n steps when using gSDE\n        Default: -1 (only sample at the beginning of the rollout)\n    :param use_sde_at_warmup: (bool) Whether to use gSDE instead of uniform sampling\n        during the warm up phase (before learning starts)\n    :param create_eval_env: (bool) Whether to create a second environment that will be\n        used for evaluating the agent periodically. (Only available when passing string for the environment)\n    :param policy_kwargs: (dict) additional arguments to be passed to the policy on creation\n    :param verbose: (int) the verbosity level: 0 no output, 1 info, 2 debug\n    :param seed: (int) Seed for the pseudo random generators\n    :param device: (str or th.device) Device (cpu, cuda, ...) on which the code should be run.\n        Setting it to auto, the code will be run on the GPU if possible.\n    :param _init_setup_model: (bool) Whether or not to build the network at the creation of the instance\n    """"""\n\n    def __init__(self, policy: Union[str, Type[SACPolicy]],\n                 env: Union[GymEnv, str],\n                 learning_rate: Union[float, Callable] = 3e-4,\n                 buffer_size: int = int(1e6),\n                 learning_starts: int = 100,\n                 batch_size: int = 256,\n                 tau: float = 0.005,\n                 gamma: float = 0.99,\n                 train_freq: int = 1,\n                 gradient_steps: int = 1,\n                 n_episodes_rollout: int = -1,\n                 action_noise: Optional[ActionNoise] = None,\n                 ent_coef: Union[str, float] = \'auto\',\n                 target_update_interval: int = 1,\n                 target_entropy: Union[str, float] = \'auto\',\n                 use_sde: bool = False,\n                 sde_sample_freq: int = -1,\n                 use_sde_at_warmup: bool = False,\n                 tensorboard_log: Optional[str] = None,\n                 create_eval_env: bool = False,\n                 policy_kwargs: Dict[str, Any] = None,\n                 verbose: int = 0,\n                 seed: Optional[int] = None,\n                 device: Union[th.device, str] = \'auto\',\n                 _init_setup_model: bool = True):\n\n        super(SAC, self).__init__(policy, env, SACPolicy, learning_rate,\n                                  buffer_size, learning_starts, batch_size,\n                                  policy_kwargs, tensorboard_log, verbose, device,\n                                  create_eval_env=create_eval_env, seed=seed,\n                                  use_sde=use_sde, sde_sample_freq=sde_sample_freq,\n                                  use_sde_at_warmup=use_sde_at_warmup)\n\n        self.target_entropy = target_entropy\n        self.log_ent_coef = None  # type: Optional[th.Tensor]\n        self.target_update_interval = target_update_interval\n        self.tau = tau\n        # Entropy coefficient / Entropy temperature\n        # Inverse of the reward scale\n        self.ent_coef = ent_coef\n        self.target_update_interval = target_update_interval\n        self.train_freq = train_freq\n        self.gradient_steps = gradient_steps\n        self.n_episodes_rollout = n_episodes_rollout\n        self.action_noise = action_noise\n        self.gamma = gamma\n        self.ent_coef_optimizer = None\n\n        if _init_setup_model:\n            self._setup_model()\n\n    def _setup_model(self) -> None:\n        super(SAC, self)._setup_model()\n        self._create_aliases()\n\n        # Target entropy is used when learning the entropy coefficient\n        if self.target_entropy == \'auto\':\n            # automatically set target entropy if needed\n            self.target_entropy = -np.prod(self.env.action_space.shape).astype(np.float32)\n        else:\n            # Force conversion\n            # this will also throw an error for unexpected string\n            self.target_entropy = float(self.target_entropy)\n\n        # The entropy coefficient or entropy can be learned automatically\n        # see Automating Entropy Adjustment for Maximum Entropy RL section\n        # of https://arxiv.org/abs/1812.05905\n        if isinstance(self.ent_coef, str) and self.ent_coef.startswith(\'auto\'):\n            # Default initial value of ent_coef when learned\n            init_value = 1.0\n            if \'_\' in self.ent_coef:\n                init_value = float(self.ent_coef.split(\'_\')[1])\n                assert init_value > 0., ""The initial value of ent_coef must be greater than 0""\n\n            # Note: we optimize the log of the entropy coeff which is slightly different from the paper\n            # as discussed in https://github.com/rail-berkeley/softlearning/issues/37\n            self.log_ent_coef = th.log(th.ones(1, device=self.device) * init_value).requires_grad_(True)\n            self.ent_coef_optimizer = th.optim.Adam([self.log_ent_coef], lr=self.lr_schedule(1))\n        else:\n            # Force conversion to float\n            # this will throw an error if a malformed string (different from \'auto\')\n            # is passed\n            self.ent_coef_tensor = th.tensor(float(self.ent_coef)).to(self.device)\n\n    def _create_aliases(self) -> None:\n        self.actor = self.policy.actor\n        self.critic = self.policy.critic\n        self.critic_target = self.policy.critic_target\n\n    def train(self, gradient_steps: int, batch_size: int = 64) -> None:\n        # Update optimizers learning rate\n        optimizers = [self.actor.optimizer, self.critic.optimizer]\n        if self.ent_coef_optimizer is not None:\n            optimizers += [self.ent_coef_optimizer]\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate(optimizers)\n\n        ent_coef_losses, ent_coefs = [], []\n        actor_losses, critic_losses = [], []\n\n        for gradient_step in range(gradient_steps):\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            # We need to sample because `log_std` may have changed between two gradient steps\n            if self.use_sde:\n                self.actor.reset_noise()\n\n            # Action by the current actor for the sampled state\n            actions_pi, log_prob = self.actor.action_log_prob(replay_data.observations)\n            log_prob = log_prob.reshape(-1, 1)\n\n            ent_coef_loss = None\n            if self.ent_coef_optimizer is not None:\n                # Important: detach the variable from the graph\n                # so we don\'t change it with other losses\n                # see https://github.com/rail-berkeley/softlearning/issues/60\n                ent_coef = th.exp(self.log_ent_coef.detach())\n                ent_coef_loss = -(self.log_ent_coef * (log_prob + self.target_entropy).detach()).mean()\n                ent_coef_losses.append(ent_coef_loss.item())\n            else:\n                ent_coef = self.ent_coef_tensor\n\n            ent_coefs.append(ent_coef.item())\n\n            # Optimize entropy coefficient, also called\n            # entropy temperature or alpha in the paper\n            if ent_coef_loss is not None:\n                self.ent_coef_optimizer.zero_grad()\n                ent_coef_loss.backward()\n                self.ent_coef_optimizer.step()\n\n            with th.no_grad():\n                # Select action according to policy\n                next_actions, next_log_prob = self.actor.action_log_prob(replay_data.next_observations)\n                # Compute the target Q value\n                target_q1, target_q2 = self.critic_target(replay_data.next_observations, next_actions)\n                target_q = th.min(target_q1, target_q2)\n                target_q = replay_data.rewards + (1 - replay_data.dones) * self.gamma * target_q\n                # td error + entropy term\n                q_backup = target_q - ent_coef * next_log_prob.reshape(-1, 1)\n\n            # Get current Q estimates\n            # using action from the replay buffer\n            current_q1, current_q2 = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = 0.5 * (F.mse_loss(current_q1, q_backup) + F.mse_loss(current_q2, q_backup))\n            critic_losses.append(critic_loss.item())\n\n            # Optimize the critic\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Compute actor loss\n            # Alternative: actor_loss = th.mean(log_prob - qf1_pi)\n            qf1_pi, qf2_pi = self.critic.forward(replay_data.observations, actions_pi)\n            min_qf_pi = th.min(qf1_pi, qf2_pi)\n            actor_loss = (ent_coef * log_prob - min_qf_pi).mean()\n            actor_losses.append(actor_loss.item())\n\n            # Optimize the actor\n            self.actor.optimizer.zero_grad()\n            actor_loss.backward()\n            self.actor.optimizer.step()\n\n            # Update target networks\n            if gradient_step % self.target_update_interval == 0:\n                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n\n        self._n_updates += gradient_steps\n\n        logger.record(""train/n_updates"", self._n_updates, exclude=\'tensorboard\')\n        logger.record(""train/ent_coef"", np.mean(ent_coefs))\n        logger.record(""train/actor_loss"", np.mean(actor_losses))\n        logger.record(""train/critic_loss"", np.mean(critic_losses))\n        if len(ent_coef_losses) > 0:\n            logger.record(""train/ent_coef_loss"", np.mean(ent_coef_losses))\n\n    def learn(self,\n              total_timesteps: int,\n              callback: MaybeCallback = None,\n              log_interval: int = 4,\n              eval_env: Optional[GymEnv] = None,\n              eval_freq: int = -1,\n              n_eval_episodes: int = 5,\n              tb_log_name: str = ""SAC"",\n              eval_log_path: Optional[str] = None,\n              reset_num_timesteps: bool = True) -> OffPolicyAlgorithm:\n\n        total_timesteps, callback = self._setup_learn(total_timesteps, eval_env, callback, eval_freq,\n                                                      n_eval_episodes, eval_log_path, reset_num_timesteps,\n                                                      tb_log_name)\n        callback.on_training_start(locals(), globals())\n\n        while self.num_timesteps < total_timesteps:\n            rollout = self.collect_rollouts(self.env, n_episodes=self.n_episodes_rollout,\n                                            n_steps=self.train_freq, action_noise=self.action_noise,\n                                            callback=callback,\n                                            learning_starts=self.learning_starts,\n                                            replay_buffer=self.replay_buffer,\n                                            log_interval=log_interval)\n\n            if rollout.continue_training is False:\n                break\n\n            self._update_current_progress_remaining(self.num_timesteps, total_timesteps)\n\n            if self.num_timesteps > 0 and self.num_timesteps > self.learning_starts:\n                gradient_steps = self.gradient_steps if self.gradient_steps > 0 else rollout.episode_timesteps\n                self.train(gradient_steps, batch_size=self.batch_size)\n\n        callback.on_training_end()\n        return self\n\n    def excluded_save_params(self) -> List[str]:\n        """"""\n        Returns the names of the parameters that should be excluded by default\n        when saving the model.\n\n        :return: (List[str]) List of parameters that should be excluded from save\n        """"""\n        # Exclude aliases\n        return super(SAC, self).excluded_save_params() + [""actor"", ""critic"", ""critic_target""]\n\n    def get_torch_variables(self) -> Tuple[List[str], List[str]]:\n        """"""\n        cf base class\n        """"""\n        state_dicts = [""policy"", ""actor.optimizer"", ""critic.optimizer""]\n        saved_tensors = [\'log_ent_coef\']\n        if self.ent_coef_optimizer is not None:\n            state_dicts.append(\'ent_coef_optimizer\')\n        else:\n            saved_tensors.append(\'ent_coef_tensor\')\n        return state_dicts, saved_tensors\n'"
stable_baselines3/td3/__init__.py,0,"b'from stable_baselines3.td3.td3 import TD3\nfrom stable_baselines3.td3.policies import MlpPolicy, CnnPolicy\n'"
stable_baselines3/td3/policies.py,1,"b'from typing import Optional, List, Tuple, Callable, Union, Type, Any, Dict\n\nimport gym\nimport torch as th\nimport torch.nn as nn\n\nfrom stable_baselines3.common.preprocessing import get_action_dim\nfrom stable_baselines3.common.policies import BasePolicy, register_policy\nfrom stable_baselines3.common.torch_layers import create_mlp, NatureCNN, BaseFeaturesExtractor, FlattenExtractor\n\n\nclass Actor(BasePolicy):\n    """"""\n    Actor network (policy) for TD3.\n\n    :param observation_space: (gym.spaces.Space) Obervation space\n    :param action_space: (gym.spaces.Space) Action space\n    :param net_arch: ([int]) Network architecture\n    :param features_extractor: (nn.Module) Network to extract features\n        (a CNN when using images, a nn.Flatten() layer otherwise)\n    :param features_dim: (int) Number of features\n    :param activation_fn: (Type[nn.Module]) Activation function\n    :param normalize_images: (bool) Whether to normalize images or not,\n         dividing by 255.0 (True by default)\n    :param device: (Union[th.device, str]) Device on which the code should run.\n    """"""\n\n    def __init__(self,\n                 observation_space: gym.spaces.Space,\n                 action_space: gym.spaces.Space,\n                 net_arch: List[int],\n                 features_extractor: nn.Module,\n                 features_dim: int,\n                 activation_fn: Type[nn.Module] = nn.ReLU,\n                 normalize_images: bool = True,\n                 device: Union[th.device, str] = \'auto\'):\n        super(Actor, self).__init__(observation_space, action_space,\n                                    features_extractor=features_extractor,\n                                    normalize_images=normalize_images,\n                                    device=device,\n                                    squash_output=True)\n\n        self.features_extractor = features_extractor\n        self.normalize_images = normalize_images\n        self.net_arch = net_arch\n        self.features_dim = features_dim\n        self.activation_fn = activation_fn\n\n        action_dim = get_action_dim(self.action_space)\n        actor_net = create_mlp(features_dim, action_dim, net_arch, activation_fn, squash_output=True)\n        # Deterministic action\n        self.mu = nn.Sequential(*actor_net)\n\n    def _get_data(self) -> Dict[str, Any]:\n        data = super()._get_data()\n\n        data.update(dict(\n            net_arch=self.net_arch,\n            features_dim=self.features_dim,\n            activation_fn=self.activation_fn,\n            features_extractor=self.features_extractor\n        ))\n        return data\n\n    def forward(self, obs: th.Tensor, deterministic: bool = True) -> th.Tensor:\n        # assert deterministic, \'The TD3 actor only outputs deterministic actions\'\n        features = self.extract_features(obs)\n        return self.mu(features)\n\n    def _predict(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:\n        return self.forward(observation, deterministic=deterministic)\n\n\nclass Critic(BasePolicy):\n    """"""\n    Critic network for TD3,\n    in fact it represents the action-state value function (Q-value function)\n\n    :param observation_space: (gym.spaces.Space) Obervation space\n    :param action_space: (gym.spaces.Space) Action space\n    :param net_arch: ([int]) Network architecture\n    :param features_extractor: (nn.Module) Network to extract features\n        (a CNN when using images, a nn.Flatten() layer otherwise)\n    :param features_dim: (int) Number of features\n    :param activation_fn: (Type[nn.Module]) Activation function\n    :param normalize_images: (bool) Whether to normalize images or not,\n         dividing by 255.0 (True by default)\n    :param device: (Union[th.device, str]) Device on which the code should run.\n    """"""\n\n    def __init__(self, observation_space: gym.spaces.Space,\n                 action_space: gym.spaces.Space,\n                 net_arch: List[int],\n                 features_extractor: nn.Module,\n                 features_dim: int,\n                 activation_fn: Type[nn.Module] = nn.ReLU,\n                 normalize_images: bool = True,\n                 device: Union[th.device, str] = \'auto\'):\n        super(Critic, self).__init__(observation_space, action_space,\n                                     features_extractor=features_extractor,\n                                     normalize_images=normalize_images,\n                                     device=device)\n\n        action_dim = get_action_dim(self.action_space)\n\n        q1_net = create_mlp(features_dim + action_dim, 1, net_arch, activation_fn)\n        self.q1_net = nn.Sequential(*q1_net)\n\n        q2_net = create_mlp(features_dim + action_dim, 1, net_arch, activation_fn)\n        self.q2_net = nn.Sequential(*q2_net)\n\n    def forward(self, obs: th.Tensor, actions: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n        # Learn the features extractor using the policy loss only\n        with th.no_grad():\n            features = self.extract_features(obs)\n        qvalue_input = th.cat([features, actions], dim=1)\n        return self.q1_net(qvalue_input), self.q2_net(qvalue_input)\n\n    def q1_forward(self, obs: th.Tensor, actions: th.Tensor) -> th.Tensor:\n        with th.no_grad():\n            features = self.extract_features(obs)\n        return self.q1_net(th.cat([features, actions], dim=1))\n\n\nclass TD3Policy(BasePolicy):\n    """"""\n    Policy class (with both actor and critic) for TD3.\n\n    :param observation_space: (gym.spaces.Space) Observation space\n    :param action_space: (gym.spaces.Space) Action space\n    :param lr_schedule: (Callable) Learning rate schedule (could be constant)\n    :param net_arch: (Optional[List[int]]) The specification of the policy and value networks.\n    :param device: (Union[th.device, str]) Device on which the code should run.\n    :param activation_fn: (Type[nn.Module]) Activation function\n    :param features_extractor_class: (Type[BaseFeaturesExtractor]) Features extractor to use.\n    :param features_extractor_kwargs: (Optional[Dict[str, Any]]) Keyword arguments\n        to pass to the feature extractor.\n    :param normalize_images: (bool) Whether to normalize images or not,\n         dividing by 255.0 (True by default)\n    :param optimizer_class: (Type[th.optim.Optimizer]) The optimizer to use,\n        ``th.optim.Adam`` by default\n    :param optimizer_kwargs: (Optional[Dict[str, Any]]) Additional keyword arguments,\n        excluding the learning rate, to pass to the optimizer\n    """"""\n\n    def __init__(self, observation_space: gym.spaces.Space,\n                 action_space: gym.spaces.Space,\n                 lr_schedule: Callable,\n                 net_arch: Optional[List[int]] = None,\n                 device: Union[th.device, str] = \'auto\',\n                 activation_fn: Type[nn.Module] = nn.ReLU,\n                 features_extractor_class: Type[BaseFeaturesExtractor] = FlattenExtractor,\n                 features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n                 normalize_images: bool = True,\n                 optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n                 optimizer_kwargs: Optional[Dict[str, Any]] = None):\n        super(TD3Policy, self).__init__(observation_space, action_space,\n                                        device,\n                                        features_extractor_class,\n                                        features_extractor_kwargs,\n                                        optimizer_class=optimizer_class,\n                                        optimizer_kwargs=optimizer_kwargs,\n                                        squash_output=True)\n\n        # Default network architecture, from the original paper\n        if net_arch is None:\n            if features_extractor_class == FlattenExtractor:\n                net_arch = [400, 300]\n            else:\n                net_arch = []\n\n        self.features_extractor = features_extractor_class(self.observation_space,\n                                                           **self.features_extractor_kwargs)\n        self.features_dim = self.features_extractor.features_dim\n\n        self.net_arch = net_arch\n        self.activation_fn = activation_fn\n        self.net_args = {\n            \'observation_space\': self.observation_space,\n            \'action_space\': self.action_space,\n            \'features_extractor\': self.features_extractor,\n            \'features_dim\': self.features_dim,\n            \'net_arch\': self.net_arch,\n            \'activation_fn\': self.activation_fn,\n            \'normalize_images\': normalize_images,\n            \'device\': device\n        }\n        self.actor, self.actor_target = None, None\n        self.critic, self.critic_target = None, None\n\n        self._build(lr_schedule)\n\n    def _build(self, lr_schedule: Callable) -> None:\n        self.actor = self.make_actor()\n        self.actor_target = self.make_actor()\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.actor.optimizer = self.optimizer_class(self.actor.parameters(), lr=lr_schedule(1),\n                                                    **self.optimizer_kwargs)\n        self.critic = self.make_critic()\n        self.critic_target = self.make_critic()\n        self.critic_target.load_state_dict(self.critic.state_dict())\n        self.critic.optimizer = self.optimizer_class(self.critic.parameters(), lr=lr_schedule(1),\n                                                     **self.optimizer_kwargs)\n\n    def _get_data(self) -> Dict[str, Any]:\n        data = super()._get_data()\n\n        data.update(dict(\n            net_arch=self.net_args[\'net_arch\'],\n            activation_fn=self.net_args[\'activation_fn\'],\n            lr_schedule=self._dummy_schedule,  # dummy lr schedule, not needed for loading policy alone\n            optimizer_class=self.optimizer_class,\n            optimizer_kwargs=self.optimizer_kwargs,\n            features_extractor_class=self.features_extractor_class,\n            features_extractor_kwargs=self.features_extractor_kwargs\n        ))\n        return data\n\n    def make_actor(self) -> Actor:\n        return Actor(**self.net_args).to(self.device)\n\n    def make_critic(self) -> Critic:\n        return Critic(**self.net_args).to(self.device)\n\n    def forward(self, observation: th.Tensor, deterministic: bool = False):\n        return self._predict(observation, deterministic=deterministic)\n\n    def _predict(self, observation: th.Tensor, deterministic: bool = False) -> th.Tensor:\n        return self.actor(observation, deterministic=deterministic)\n\n\nMlpPolicy = TD3Policy\n\n\nclass CnnPolicy(TD3Policy):\n    """"""\n    Policy class (with both actor and critic) for TD3.\n\n    :param observation_space: (gym.spaces.Space) Observation space\n    :param action_space: (gym.spaces.Space) Action space\n    :param lr_schedule: (Callable) Learning rate schedule (could be constant)\n    :param net_arch: (Optional[List[int]]) The specification of the policy and value networks.\n    :param device: (Union[th.device, str]) Device on which the code should run.\n    :param activation_fn: (Type[nn.Module]) Activation function\n    :param features_extractor_class: (Type[BaseFeaturesExtractor]) Features extractor to use.\n    :param features_extractor_kwargs: (Optional[Dict[str, Any]]) Keyword arguments\n        to pass to the feature extractor.\n    :param normalize_images: (bool) Whether to normalize images or not,\n         dividing by 255.0 (True by default)\n    :param optimizer_class: (Type[th.optim.Optimizer]) The optimizer to use,\n        ``th.optim.Adam`` by default\n    :param optimizer_kwargs: (Optional[Dict[str, Any]]) Additional keyword arguments,\n        excluding the learning rate, to pass to the optimizer\n    """"""\n\n    def __init__(self, observation_space: gym.spaces.Space,\n                 action_space: gym.spaces.Space,\n                 lr_schedule: Callable,\n                 net_arch: Optional[List[int]] = None,\n                 device: Union[th.device, str] = \'auto\',\n                 activation_fn: Type[nn.Module] = nn.ReLU,\n                 features_extractor_class: Type[BaseFeaturesExtractor] = NatureCNN,\n                 features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n                 normalize_images: bool = True,\n                 optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n                 optimizer_kwargs: Optional[Dict[str, Any]] = None):\n        super(CnnPolicy, self).__init__(observation_space,\n                                        action_space,\n                                        lr_schedule,\n                                        net_arch,\n                                        device,\n                                        activation_fn,\n                                        features_extractor_class,\n                                        features_extractor_kwargs,\n                                        normalize_images,\n                                        optimizer_class,\n                                        optimizer_kwargs)\n\n\nregister_policy(""MlpPolicy"", MlpPolicy)\nregister_policy(""CnnPolicy"", CnnPolicy)\n'"
stable_baselines3/td3/td3.py,1,"b'import torch as th\nimport torch.nn.functional as F\nfrom typing import List, Tuple, Type, Union, Callable, Optional, Dict, Any\n\nfrom stable_baselines3.common import logger\nfrom stable_baselines3.common.off_policy_algorithm import OffPolicyAlgorithm\nfrom stable_baselines3.common.noise import ActionNoise\nfrom stable_baselines3.common.type_aliases import GymEnv, MaybeCallback\nfrom stable_baselines3.td3.policies import TD3Policy\n\n\nclass TD3(OffPolicyAlgorithm):\n    """"""\n    Twin Delayed DDPG (TD3)\n    Addressing Function Approximation Error in Actor-Critic Methods.\n\n    Original implementation: https://github.com/sfujim/TD3\n    Paper: https://arxiv.org/abs/1802.09477\n    Introduction to TD3: https://spinningup.openai.com/en/latest/algorithms/td3.html\n\n    :param policy: (TD3Policy or str) The policy model to use (MlpPolicy, CnnPolicy, ...)\n    :param env: (GymEnv or str) The environment to learn from (if registered in Gym, can be str)\n    :param learning_rate: (float or callable) learning rate for adam optimizer,\n        the same learning rate will be used for all networks (Q-Values, Actor and Value function)\n        it can be a function of the current progress remaining (from 1 to 0)\n    :param buffer_size: (int) size of the replay buffer\n    :param learning_starts: (int) how many steps of the model to collect transitions for before learning starts\n    :param batch_size: (int) Minibatch size for each gradient update\n    :param tau: (float) the soft update coefficient (""Polyak update"", between 0 and 1)\n    :param gamma: (float) the discount factor\n    :param train_freq: (int) Update the model every ``train_freq`` steps.\n    :param gradient_steps: (int) How many gradient update after each step\n    :param n_episodes_rollout: (int) Update the model every ``n_episodes_rollout`` episodes.\n        Note that this cannot be used at the same time as ``train_freq``\n    :param action_noise: (ActionNoise) the action noise type (None by default), this can help\n        for hard exploration problem. Cf common.noise for the different action noise type.\n    :param policy_delay: (int) Policy and target networks will only be updated once every policy_delay steps\n        per training steps. The Q values will be updated policy_delay more often (update every training step).\n    :param target_policy_noise: (float) Standard deviation of Gaussian noise added to target policy\n        (smoothing noise)\n    :param target_noise_clip: (float) Limit for absolute value of target policy smoothing noise.\n    :param create_eval_env: (bool) Whether to create a second environment that will be\n        used for evaluating the agent periodically. (Only available when passing string for the environment)\n    :param policy_kwargs: (dict) additional arguments to be passed to the policy on creation\n    :param verbose: (int) the verbosity level: 0 no output, 1 info, 2 debug\n    :param seed: (int) Seed for the pseudo random generators\n    :param device: (str or th.device) Device (cpu, cuda, ...) on which the code should be run.\n        Setting it to auto, the code will be run on the GPU if possible.\n    :param _init_setup_model: (bool) Whether or not to build the network at the creation of the instance\n    """"""\n\n    def __init__(self, policy: Union[str, Type[TD3Policy]],\n                 env: Union[GymEnv, str],\n                 learning_rate: Union[float, Callable] = 1e-3,\n                 buffer_size: int = int(1e6),\n                 learning_starts: int = 100,\n                 batch_size: int = 100,\n                 tau: float = 0.005,\n                 gamma: float = 0.99,\n                 train_freq: int = -1,\n                 gradient_steps: int = -1,\n                 n_episodes_rollout: int = 1,\n                 action_noise: Optional[ActionNoise] = None,\n                 policy_delay: int = 2,\n                 target_policy_noise: float = 0.2,\n                 target_noise_clip: float = 0.5,\n                 tensorboard_log: Optional[str] = None,\n                 create_eval_env: bool = False,\n                 policy_kwargs: Dict[str, Any] = None,\n                 verbose: int = 0,\n                 seed: Optional[int] = None,\n                 device: Union[th.device, str] = \'auto\',\n                 _init_setup_model: bool = True):\n\n        super(TD3, self).__init__(policy, env, TD3Policy, learning_rate,\n                                  buffer_size, learning_starts, batch_size,\n                                  policy_kwargs, tensorboard_log, verbose, device,\n                                  create_eval_env=create_eval_env, seed=seed,\n                                  sde_support=False)\n\n        self.train_freq = train_freq\n        self.gradient_steps = gradient_steps\n        self.n_episodes_rollout = n_episodes_rollout\n        self.tau = tau\n        self.gamma = gamma\n        self.action_noise = action_noise\n        self.policy_delay = policy_delay\n        self.target_noise_clip = target_noise_clip\n        self.target_policy_noise = target_policy_noise\n\n        if _init_setup_model:\n            self._setup_model()\n\n    def _setup_model(self) -> None:\n        super(TD3, self)._setup_model()\n        self._create_aliases()\n\n    def _create_aliases(self) -> None:\n        self.actor = self.policy.actor\n        self.actor_target = self.policy.actor_target\n        self.critic = self.policy.critic\n        self.critic_target = self.policy.critic_target\n\n    def train(self, gradient_steps: int, batch_size: int = 100, policy_delay: int = 2) -> None:\n\n        # Update learning rate according to lr schedule\n        self._update_learning_rate([self.actor.optimizer, self.critic.optimizer])\n\n        for gradient_step in range(gradient_steps):\n\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n\n            with th.no_grad():\n                # Select action according to policy and add clipped noise\n                noise = replay_data.actions.clone().data.normal_(0, self.target_policy_noise)\n                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n                next_actions = (self.actor_target(replay_data.next_observations) + noise).clamp(-1, 1)\n\n                # Compute the target Q value\n                target_q1, target_q2 = self.critic_target(replay_data.next_observations, next_actions)\n                target_q = th.min(target_q1, target_q2)\n                target_q = replay_data.rewards + (1 - replay_data.dones) * self.gamma * target_q\n\n            # Get current Q estimates\n            current_q1, current_q2 = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n\n            # Optimize the critic\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Delayed policy updates\n            if gradient_step % policy_delay == 0:\n                # Compute actor loss\n                actor_loss = -self.critic.q1_forward(replay_data.observations,\n                                                     self.actor(replay_data.observations)).mean()\n\n                # Optimize the actor\n                self.actor.optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor.optimizer.step()\n\n                # Update the frozen target networks\n                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n\n                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n\n        self._n_updates += gradient_steps\n        logger.record(""train/n_updates"", self._n_updates, exclude=\'tensorboard\')\n\n    def learn(self,\n              total_timesteps: int,\n              callback: MaybeCallback = None,\n              log_interval: int = 4,\n              eval_env: Optional[GymEnv] = None,\n              eval_freq: int = -1,\n              n_eval_episodes: int = 5,\n              tb_log_name: str = ""TD3"",\n              eval_log_path: Optional[str] = None,\n              reset_num_timesteps: bool = True) -> OffPolicyAlgorithm:\n\n        total_timesteps, callback = self._setup_learn(total_timesteps, eval_env, callback, eval_freq,\n                                                      n_eval_episodes, eval_log_path, reset_num_timesteps,\n                                                      tb_log_name)\n        callback.on_training_start(locals(), globals())\n\n        while self.num_timesteps < total_timesteps:\n\n            rollout = self.collect_rollouts(self.env, n_episodes=self.n_episodes_rollout,\n                                            n_steps=self.train_freq, action_noise=self.action_noise,\n                                            callback=callback,\n                                            learning_starts=self.learning_starts,\n                                            replay_buffer=self.replay_buffer,\n                                            log_interval=log_interval)\n\n            if rollout.continue_training is False:\n                break\n\n            self._update_current_progress_remaining(self.num_timesteps, total_timesteps)\n\n            if self.num_timesteps > 0 and self.num_timesteps > self.learning_starts:\n                gradient_steps = self.gradient_steps if self.gradient_steps > 0 else rollout.episode_timesteps\n                self.train(gradient_steps, batch_size=self.batch_size, policy_delay=self.policy_delay)\n\n        callback.on_training_end()\n\n        return self\n\n    def excluded_save_params(self) -> List[str]:\n        """"""\n        Returns the names of the parameters that should be excluded by default\n        when saving the model.\n\n        :return: (List[str]) List of parameters that should be excluded from save\n        """"""\n        # Exclude aliases\n        return super(TD3, self).excluded_save_params() + [""actor"", ""critic"", ""actor_target"", ""critic_target""]\n\n    def get_torch_variables(self) -> Tuple[List[str], List[str]]:\n        """"""\n        cf base class\n        """"""\n        state_dicts = [""policy"", ""actor.optimizer"", ""critic.optimizer""]\n        return state_dicts, []\n'"
stable_baselines3/common/vec_env/__init__.py,0,"b'# flake8: noqa F401\nimport typing\nfrom typing import Optional, Union\nfrom copy import deepcopy\n\nfrom stable_baselines3.common.vec_env.base_vec_env import (AlreadySteppingError, NotSteppingError,\n                                                           VecEnv, VecEnvWrapper, CloudpickleWrapper)\nfrom stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\nfrom stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\nfrom stable_baselines3.common.vec_env.vec_frame_stack import VecFrameStack\nfrom stable_baselines3.common.vec_env.vec_normalize import VecNormalize\nfrom stable_baselines3.common.vec_env.vec_transpose import VecTransposeImage\nfrom stable_baselines3.common.vec_env.vec_video_recorder import VecVideoRecorder\nfrom stable_baselines3.common.vec_env.vec_check_nan import VecCheckNan\n\n# Avoid circular import\nif typing.TYPE_CHECKING:\n    from stable_baselines3.common.type_aliases import GymEnv\n\n\ndef unwrap_vec_normalize(env: Union[\'GymEnv\', VecEnv]) -> Optional[VecNormalize]:\n    """"""\n    :param env: (gym.Env)\n    :return: (VecNormalize)\n    """"""\n    env_tmp = env\n    while isinstance(env_tmp, VecEnvWrapper):\n        if isinstance(env_tmp, VecNormalize):\n            return env_tmp\n        env_tmp = env_tmp.venv\n    return None\n\n\n# Define here to avoid circular import\ndef sync_envs_normalization(env: \'GymEnv\', eval_env: \'GymEnv\') -> None:\n    """"""\n    Sync eval env and train env when using VecNormalize\n\n    :param env: (GymEnv)\n    :param eval_env: (GymEnv)\n    """"""\n    env_tmp, eval_env_tmp = env, eval_env\n    while isinstance(env_tmp, VecEnvWrapper):\n        if isinstance(env_tmp, VecNormalize):\n            eval_env_tmp.obs_rms = deepcopy(env_tmp.obs_rms)\n            eval_env_tmp.ret_rms = deepcopy(env_tmp.ret_rms)\n        env_tmp = env_tmp.venv\n        eval_env_tmp = eval_env_tmp.venv\n'"
stable_baselines3/common/vec_env/base_vec_env.py,0,"b'import inspect\nimport pickle\nfrom abc import ABC, abstractmethod\nfrom typing import Sequence, Optional, List, Union\n\nimport numpy as np\nimport cloudpickle\n\nfrom stable_baselines3.common import logger\n\n\ndef tile_images(img_nhwc: Sequence[np.ndarray]) -> np.ndarray:  # pragma: no cover\n    """"""\n    Tile N images into one big PxQ image\n    (P,Q) are chosen to be as close as possible, and if N\n    is square, then P=Q.\n\n    :param img_nhwc: (Sequence[np.ndarray]) list or array of images, ndim=4 once turned into array. img nhwc\n        n = batch index, h = height, w = width, c = channel\n    :return: (np.ndarray) img_HWc, ndim=3\n    """"""\n    img_nhwc = np.asarray(img_nhwc)\n    n_images, height, width, n_channels = img_nhwc.shape\n    # new_height was named H before\n    new_height = int(np.ceil(np.sqrt(n_images)))\n    # new_width was named W before\n    new_width = int(np.ceil(float(n_images) / new_height))\n    img_nhwc = np.array(list(img_nhwc) + [img_nhwc[0] * 0 for _ in range(n_images, new_height * new_width)])\n    # img_HWhwc\n    out_image = img_nhwc.reshape((new_height, new_width, height, width, n_channels))\n    # img_HhWwc\n    out_image = out_image.transpose(0, 2, 1, 3, 4)\n    # img_Hh_Ww_c\n    out_image = out_image.reshape((new_height * height, new_width * width, n_channels))\n    return out_image\n\n\nclass AlreadySteppingError(Exception):\n    """"""\n    Raised when an asynchronous step is running while\n    step_async() is called again.\n    """"""\n\n    def __init__(self):\n        msg = \'already running an async step\'\n        Exception.__init__(self, msg)\n\n\nclass NotSteppingError(Exception):\n    """"""\n    Raised when an asynchronous step is not running but\n    step_wait() is called.\n    """"""\n\n    def __init__(self):\n        msg = \'not running an async step\'\n        Exception.__init__(self, msg)\n\n\nclass VecEnv(ABC):\n    """"""\n    An abstract asynchronous, vectorized environment.\n\n    :param num_envs: (int) the number of environments\n    :param observation_space: (Gym Space) the observation space\n    :param action_space: (Gym Space) the action space\n    """"""\n    metadata = {\n        \'render.modes\': [\'human\', \'rgb_array\']\n    }\n\n    def __init__(self, num_envs, observation_space, action_space):\n        self.num_envs = num_envs\n        self.observation_space = observation_space\n        self.action_space = action_space\n\n    @abstractmethod\n    def reset(self):\n        """"""\n        Reset all the environments and return an array of\n        observations, or a tuple of observation arrays.\n\n        If step_async is still doing work, that work will\n        be cancelled and step_wait() should not be called\n        until step_async() is invoked again.\n\n        :return: ([int] or [float]) observation\n        """"""\n        raise NotImplementedError()\n\n    @abstractmethod\n    def step_async(self, actions):\n        """"""\n        Tell all the environments to start taking a step\n        with the given actions.\n        Call step_wait() to get the results of the step.\n\n        You should not call this if a step_async run is\n        already pending.\n        """"""\n        raise NotImplementedError()\n\n    @abstractmethod\n    def step_wait(self):\n        """"""\n        Wait for the step taken with step_async().\n\n        :return: ([int] or [float], [float], [bool], dict) observation, reward, done, information\n        """"""\n        raise NotImplementedError()\n\n    @abstractmethod\n    def close(self):\n        """"""\n        Clean up the environment\'s resources.\n        """"""\n        raise NotImplementedError()\n\n    @abstractmethod\n    def get_attr(self, attr_name, indices=None):\n        """"""\n        Return attribute from vectorized environment.\n\n        :param attr_name: (str) The name of the attribute whose value to return\n        :param indices: (list,int) Indices of envs to get attribute from\n        :return: (list) List of values of \'attr_name\' in all environments\n        """"""\n        raise NotImplementedError()\n\n    @abstractmethod\n    def set_attr(self, attr_name, value, indices=None):\n        """"""\n        Set attribute inside vectorized environments.\n\n        :param attr_name: (str) The name of attribute to assign new value\n        :param value: (obj) Value to assign to `attr_name`\n        :param indices: (list,int) Indices of envs to assign value\n        :return: (NoneType)\n        """"""\n        raise NotImplementedError()\n\n    @abstractmethod\n    def env_method(self, method_name, *method_args, indices=None, **method_kwargs):\n        """"""\n        Call instance methods of vectorized environments.\n\n        :param method_name: (str) The name of the environment method to invoke.\n        :param indices: (list,int) Indices of envs whose method to call\n        :param method_args: (tuple) Any positional arguments to provide in the call\n        :param method_kwargs: (dict) Any keyword arguments to provide in the call\n        :return: (list) List of items returned by the environment\'s method call\n        """"""\n        raise NotImplementedError()\n\n    def step(self, actions):\n        """"""\n        Step the environments with the given action\n\n        :param actions: ([int] or [float]) the action\n        :return: ([int] or [float], [float], [bool], dict) observation, reward, done, information\n        """"""\n        self.step_async(actions)\n        return self.step_wait()\n\n    def get_images(self) -> Sequence[np.ndarray]:\n        """"""\n        Return RGB images from each environment\n        """"""\n        raise NotImplementedError\n\n    def render(self, mode: str = \'human\'):\n        """"""\n        Gym environment rendering\n\n        :param mode: the rendering type\n        """"""\n        try:\n            imgs = self.get_images()\n        except NotImplementedError:\n            logger.warn(f\'Render not defined for {self}\')\n            return\n\n        # Create a big image by tiling images from subprocesses\n        bigimg = tile_images(imgs)\n        if mode == \'human\':\n            import cv2  # pytype:disable=import-error\n            cv2.imshow(\'vecenv\', bigimg[:, :, ::-1])\n            cv2.waitKey(1)\n        elif mode == \'rgb_array\':\n            return bigimg\n        else:\n            raise NotImplementedError(f\'Render mode {mode} is not supported by VecEnvs\')\n\n    @abstractmethod\n    def seed(self, seed: Optional[int] = None) -> List[Union[None, int]]:\n        """"""\n        Sets the random seeds for all environments, based on a given seed.\n        Each individual environment will still get its own seed, by incrementing the given seed.\n\n        :param seed: (Optional[int]) The random seed. May be None for completely random seeding.\n        :return: (List[Union[None, int]]) Returns a list containing the seeds for each individual env.\n            Note that all list elements may be None, if the env does not return anything when being seeded.\n        """"""\n        pass\n\n    @property\n    def unwrapped(self):\n        if isinstance(self, VecEnvWrapper):\n            return self.venv.unwrapped\n        else:\n            return self\n\n    def getattr_depth_check(self, name, already_found):\n        """"""Check if an attribute reference is being hidden in a recursive call to __getattr__\n\n        :param name: (str) name of attribute to check for\n        :param already_found: (bool) whether this attribute has already been found in a wrapper\n        :return: (str or None) name of module whose attribute is being shadowed, if any.\n        """"""\n        if hasattr(self, name) and already_found:\n            return f""{type(self).__module__}.{type(self).__name__}""\n        else:\n            return None\n\n    def _get_indices(self, indices):\n        """"""\n        Convert a flexibly-typed reference to environment indices to an implied list of indices.\n\n        :param indices: (None,int,Iterable) refers to indices of envs.\n        :return: (list) the implied list of indices.\n        """"""\n        if indices is None:\n            indices = range(self.num_envs)\n        elif isinstance(indices, int):\n            indices = [indices]\n        return indices\n\n\nclass VecEnvWrapper(VecEnv):\n    """"""\n    Vectorized environment base class\n\n    :param venv: (VecEnv) the vectorized environment to wrap\n    :param observation_space: (Gym Space) the observation space (can be None to load from venv)\n    :param action_space: (Gym Space) the action space (can be None to load from venv)\n    """"""\n\n    def __init__(self, venv, observation_space=None, action_space=None):\n        self.venv = venv\n        VecEnv.__init__(self, num_envs=venv.num_envs, observation_space=observation_space or venv.observation_space,\n                        action_space=action_space or venv.action_space)\n        self.class_attributes = dict(inspect.getmembers(self.__class__))\n\n    def step_async(self, actions):\n        self.venv.step_async(actions)\n\n    @abstractmethod\n    def reset(self):\n        pass\n\n    @abstractmethod\n    def step_wait(self):\n        pass\n\n    def seed(self, seed=None):\n        return self.venv.seed(seed)\n\n    def close(self):\n        return self.venv.close()\n\n    def render(self, mode: str = \'human\'):\n        return self.venv.render(mode=mode)\n\n    def get_images(self):\n        return self.venv.get_images()\n\n    def get_attr(self, attr_name, indices=None):\n        return self.venv.get_attr(attr_name, indices)\n\n    def set_attr(self, attr_name, value, indices=None):\n        return self.venv.set_attr(attr_name, value, indices)\n\n    def env_method(self, method_name, *method_args, indices=None, **method_kwargs):\n        return self.venv.env_method(method_name, *method_args, indices=indices, **method_kwargs)\n\n    def __getattr__(self, name):\n        """"""Find attribute from wrapped venv(s) if this wrapper does not have it.\n        Useful for accessing attributes from venvs which are wrapped with multiple wrappers\n        which have unique attributes of interest.\n        """"""\n        blocked_class = self.getattr_depth_check(name, already_found=False)\n        if blocked_class is not None:\n            own_class = f""{type(self).__module__}.{type(self).__name__}""\n            error_str = (f""Error: Recursive attribute lookup for {name} from {own_class} is ""\n                         ""ambiguous and hides attribute from {blocked_class}"")\n            raise AttributeError(error_str)\n\n        return self.getattr_recursive(name)\n\n    def _get_all_attributes(self):\n        """"""Get all (inherited) instance and class attributes\n\n        :return: (dict<str, object>) all_attributes\n        """"""\n        all_attributes = self.__dict__.copy()\n        all_attributes.update(self.class_attributes)\n        return all_attributes\n\n    def getattr_recursive(self, name):\n        """"""Recursively check wrappers to find attribute.\n\n        :param name (str) name of attribute to look for\n        :return: (object) attribute\n        """"""\n        all_attributes = self._get_all_attributes()\n        if name in all_attributes:  # attribute is present in this wrapper\n            attr = getattr(self, name)\n        elif hasattr(self.venv, \'getattr_recursive\'):\n            # Attribute not present, child is wrapper. Call getattr_recursive rather than getattr\n            # to avoid a duplicate call to getattr_depth_check.\n            attr = self.venv.getattr_recursive(name)\n        else:  # attribute not present, child is an unwrapped VecEnv\n            attr = getattr(self.venv, name)\n\n        return attr\n\n    def getattr_depth_check(self, name, already_found):\n        """"""See base class.\n\n        :return: (str or None) name of module whose attribute is being shadowed, if any.\n        """"""\n        all_attributes = self._get_all_attributes()\n        if name in all_attributes and already_found:\n            # this venv\'s attribute is being hidden because of a higher venv.\n            shadowed_wrapper_class = f""{type(self).__module__}.{type(self).__name__}""\n        elif name in all_attributes and not already_found:\n            # we have found the first reference to the attribute. Now check for duplicates.\n            shadowed_wrapper_class = self.venv.getattr_depth_check(name, True)\n        else:\n            # this wrapper does not have the attribute. Keep searching.\n            shadowed_wrapper_class = self.venv.getattr_depth_check(name, already_found)\n\n        return shadowed_wrapper_class\n\n\nclass CloudpickleWrapper(object):\n    def __init__(self, var):\n        """"""\n        Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n\n        :param var: (Any) the variable you wish to wrap for pickling with cloudpickle\n        """"""\n        self.var = var\n\n    def __getstate__(self):\n        return cloudpickle.dumps(self.var)\n\n    def __setstate__(self, obs):\n        self.var = pickle.loads(obs)\n'"
stable_baselines3/common/vec_env/dummy_vec_env.py,0,"b'from collections import OrderedDict\nfrom copy import deepcopy\nfrom typing import Sequence\n\nimport numpy as np\n\nfrom stable_baselines3.common.vec_env.base_vec_env import VecEnv\nfrom stable_baselines3.common.vec_env.util import copy_obs_dict, dict_to_obs, obs_space_info\n\n\nclass DummyVecEnv(VecEnv):\n    """"""\n    Creates a simple vectorized wrapper for multiple environments, calling each environment in sequence on the current\n    Python process. This is useful for computationally simple environment such as ``cartpole-v1``,\n    as the overhead of multiprocess or multithread outweighs the environment computation time.\n    This can also be used for RL methods that\n    require a vectorized environment, but that you want a single environments to train with.\n\n    :param env_fns: ([Gym Environment]) the list of environments to vectorize\n    """"""\n\n    def __init__(self, env_fns):\n        self.envs = [fn() for fn in env_fns]\n        env = self.envs[0]\n        VecEnv.__init__(self, len(env_fns), env.observation_space, env.action_space)\n        obs_space = env.observation_space\n        self.keys, shapes, dtypes = obs_space_info(obs_space)\n\n        self.buf_obs = OrderedDict([\n            (k, np.zeros((self.num_envs,) + tuple(shapes[k]), dtype=dtypes[k]))\n            for k in self.keys])\n        self.buf_dones = np.zeros((self.num_envs,), dtype=np.bool)\n        self.buf_rews = np.zeros((self.num_envs,), dtype=np.float32)\n        self.buf_infos = [{} for _ in range(self.num_envs)]\n        self.actions = None\n        self.metadata = env.metadata\n\n    def step_async(self, actions):\n        self.actions = actions\n\n    def step_wait(self):\n        for env_idx in range(self.num_envs):\n            obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] =\\\n                self.envs[env_idx].step(self.actions[env_idx])\n            if self.buf_dones[env_idx]:\n                # save final observation where user can get it, then reset\n                self.buf_infos[env_idx][\'terminal_observation\'] = obs\n                obs = self.envs[env_idx].reset()\n            self._save_obs(env_idx, obs)\n        return (self._obs_from_buf(), np.copy(self.buf_rews), np.copy(self.buf_dones),\n                deepcopy(self.buf_infos))\n\n    def seed(self, seed=None):\n        seeds = list()\n        for idx, env in enumerate(self.envs):\n            seeds.append(env.seed(seed + idx))\n        return seeds\n\n    def reset(self):\n        for env_idx in range(self.num_envs):\n            obs = self.envs[env_idx].reset()\n            self._save_obs(env_idx, obs)\n        return self._obs_from_buf()\n\n    def close(self):\n        for env in self.envs:\n            env.close()\n\n    def get_images(self) -> Sequence[np.ndarray]:\n        return [env.render(mode=\'rgb_array\') for env in self.envs]\n\n    def render(self, mode: str = \'human\'):\n        """"""\n        Gym environment rendering. If there are multiple environments then\n        they are tiled together in one image via ``BaseVecEnv.render()``.\n        Otherwise (if ``self.num_envs == 1``), we pass the render call directly to the\n        underlying environment.\n\n        Therefore, some arguments such as ``mode`` will have values that are valid\n        only when ``num_envs == 1``.\n\n        :param mode: The rendering type.\n        """"""\n        if self.num_envs == 1:\n            return self.envs[0].render(mode=mode)\n        else:\n            return super().render(mode=mode)\n\n    def _save_obs(self, env_idx, obs):\n        for key in self.keys:\n            if key is None:\n                self.buf_obs[key][env_idx] = obs\n            else:\n                self.buf_obs[key][env_idx] = obs[key]\n\n    def _obs_from_buf(self):\n        return dict_to_obs(self.observation_space, copy_obs_dict(self.buf_obs))\n\n    def get_attr(self, attr_name, indices=None):\n        """"""Return attribute from vectorized environment (see base class).""""""\n        target_envs = self._get_target_envs(indices)\n        return [getattr(env_i, attr_name) for env_i in target_envs]\n\n    def set_attr(self, attr_name, value, indices=None):\n        """"""Set attribute inside vectorized environments (see base class).""""""\n        target_envs = self._get_target_envs(indices)\n        for env_i in target_envs:\n            setattr(env_i, attr_name, value)\n\n    def env_method(self, method_name, *method_args, indices=None, **method_kwargs):\n        """"""Call instance methods of vectorized environments.""""""\n        target_envs = self._get_target_envs(indices)\n        return [getattr(env_i, method_name)(*method_args, **method_kwargs) for env_i in target_envs]\n\n    def _get_target_envs(self, indices):\n        indices = self._get_indices(indices)\n        return [self.envs[i] for i in indices]\n'"
stable_baselines3/common/vec_env/subproc_vec_env.py,0,"b'import multiprocessing\nfrom collections import OrderedDict\nfrom typing import Sequence\n\nimport gym\nimport numpy as np\n\nfrom stable_baselines3.common.vec_env.base_vec_env import VecEnv, CloudpickleWrapper\n\n\ndef _worker(remote, parent_remote, env_fn_wrapper):\n    parent_remote.close()\n    env = env_fn_wrapper.var()\n    while True:\n        try:\n            cmd, data = remote.recv()\n            if cmd == \'step\':\n                observation, reward, done, info = env.step(data)\n                if done:\n                    # save final observation where user can get it, then reset\n                    info[\'terminal_observation\'] = observation\n                    observation = env.reset()\n                remote.send((observation, reward, done, info))\n            elif cmd == \'seed\':\n                remote.send(env.seed(data))\n            elif cmd == \'reset\':\n                observation = env.reset()\n                remote.send(observation)\n            elif cmd == \'render\':\n                remote.send(env.render(data))\n            elif cmd == \'close\':\n                remote.close()\n                break\n            elif cmd == \'get_spaces\':\n                remote.send((env.observation_space, env.action_space))\n            elif cmd == \'env_method\':\n                method = getattr(env, data[0])\n                remote.send(method(*data[1], **data[2]))\n            elif cmd == \'get_attr\':\n                remote.send(getattr(env, data))\n            elif cmd == \'set_attr\':\n                remote.send(setattr(env, data[0], data[1]))\n            else:\n                raise NotImplementedError(f""`{cmd}` is not implemented in the worker"")\n        except EOFError:\n            break\n\n\nclass SubprocVecEnv(VecEnv):\n    """"""\n    Creates a multiprocess vectorized wrapper for multiple environments, distributing each environment to its own\n    process, allowing significant speed up when the environment is computationally complex.\n\n    For performance reasons, if your environment is not IO bound, the number of environments should not exceed the\n    number of logical cores on your CPU.\n\n    .. warning::\n\n        Only \'forkserver\' and \'spawn\' start methods are thread-safe,\n        which is important when TensorFlow sessions or other non thread-safe\n        libraries are used in the parent (see issue #217). However, compared to\n        \'fork\' they incur a small start-up cost and have restrictions on\n        global variables. With those methods, users must wrap the code in an\n        ``if __name__ == ""__main__"":`` block.\n        For more information, see the multiprocessing documentation.\n\n    :param env_fns: ([Gym Environment]) Environments to run in subprocesses\n    :param start_method: (str) method used to start the subprocesses.\n           Must be one of the methods returned by multiprocessing.get_all_start_methods().\n           Defaults to \'forkserver\' on available platforms, and \'spawn\' otherwise.\n    """"""\n\n    def __init__(self, env_fns, start_method=None):\n        self.waiting = False\n        self.closed = False\n        n_envs = len(env_fns)\n\n        if start_method is None:\n            # Fork is not a thread safe method (see issue #217)\n            # but is more user friendly (does not require to wrap the code in\n            # a `if __name__ == ""__main__"":`)\n            forkserver_available = \'forkserver\' in multiprocessing.get_all_start_methods()\n            start_method = \'forkserver\' if forkserver_available else \'spawn\'\n        ctx = multiprocessing.get_context(start_method)\n\n        self.remotes, self.work_remotes = zip(*[ctx.Pipe() for _ in range(n_envs)])\n        self.processes = []\n        for work_remote, remote, env_fn in zip(self.work_remotes, self.remotes, env_fns):\n            args = (work_remote, remote, CloudpickleWrapper(env_fn))\n            # daemon=True: if the main process crashes, we should not cause things to hang\n            process = ctx.Process(target=_worker, args=args, daemon=True)  # pytype:disable=attribute-error\n            process.start()\n            self.processes.append(process)\n            work_remote.close()\n\n        self.remotes[0].send((\'get_spaces\', None))\n        observation_space, action_space = self.remotes[0].recv()\n        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n\n    def step_async(self, actions):\n        for remote, action in zip(self.remotes, actions):\n            remote.send((\'step\', action))\n        self.waiting = True\n\n    def step_wait(self):\n        results = [remote.recv() for remote in self.remotes]\n        self.waiting = False\n        obs, rews, dones, infos = zip(*results)\n        return _flatten_obs(obs, self.observation_space), np.stack(rews), np.stack(dones), infos\n\n    def seed(self, seed=None):\n        for idx, remote in enumerate(self.remotes):\n            remote.send((\'seed\', seed + idx))\n        return [remote.recv() for remote in self.remotes]\n\n    def reset(self):\n        for remote in self.remotes:\n            remote.send((\'reset\', None))\n        obs = [remote.recv() for remote in self.remotes]\n        return _flatten_obs(obs, self.observation_space)\n\n    def close(self):\n        if self.closed:\n            return\n        if self.waiting:\n            for remote in self.remotes:\n                remote.recv()\n        for remote in self.remotes:\n            remote.send((\'close\', None))\n        for process in self.processes:\n            process.join()\n        self.closed = True\n\n    def get_images(self) -> Sequence[np.ndarray]:\n        for pipe in self.remotes:\n            # gather images from subprocesses\n            # `mode` will be taken into account later\n            pipe.send((\'render\', \'rgb_array\'))\n        imgs = [pipe.recv() for pipe in self.remotes]\n        return imgs\n\n    def get_attr(self, attr_name, indices=None):\n        """"""Return attribute from vectorized environment (see base class).""""""\n        target_remotes = self._get_target_remotes(indices)\n        for remote in target_remotes:\n            remote.send((\'get_attr\', attr_name))\n        return [remote.recv() for remote in target_remotes]\n\n    def set_attr(self, attr_name, value, indices=None):\n        """"""Set attribute inside vectorized environments (see base class).""""""\n        target_remotes = self._get_target_remotes(indices)\n        for remote in target_remotes:\n            remote.send((\'set_attr\', (attr_name, value)))\n        for remote in target_remotes:\n            remote.recv()\n\n    def env_method(self, method_name, *method_args, indices=None, **method_kwargs):\n        """"""Call instance methods of vectorized environments.""""""\n        target_remotes = self._get_target_remotes(indices)\n        for remote in target_remotes:\n            remote.send((\'env_method\', (method_name, method_args, method_kwargs)))\n        return [remote.recv() for remote in target_remotes]\n\n    def _get_target_remotes(self, indices):\n        """"""\n        Get the connection object needed to communicate with the wanted\n        envs that are in subprocesses.\n\n        :param indices: (None,int,Iterable) refers to indices of envs.\n        :return: ([multiprocessing.Connection]) Connection object to communicate between processes.\n        """"""\n        indices = self._get_indices(indices)\n        return [self.remotes[i] for i in indices]\n\n\ndef _flatten_obs(obs, space):\n    """"""\n    Flatten observations, depending on the observation space.\n\n    :param obs: (list<X> or tuple<X> where X is dict<ndarray>, tuple<ndarray> or ndarray) observations.\n                A list or tuple of observations, one per environment.\n                Each environment observation may be a NumPy array, or a dict or tuple of NumPy arrays.\n    :return (OrderedDict<ndarray>, tuple<ndarray> or ndarray) flattened observations.\n            A flattened NumPy array or an OrderedDict or tuple of flattened numpy arrays.\n            Each NumPy array has the environment index as its first axis.\n    """"""\n    assert isinstance(obs, (list, tuple)), ""expected list or tuple of observations per environment""\n    assert len(obs) > 0, ""need observations from at least one environment""\n\n    if isinstance(space, gym.spaces.Dict):\n        assert isinstance(space.spaces, OrderedDict), ""Dict space must have ordered subspaces""\n        assert isinstance(obs[0], dict), ""non-dict observation for environment with Dict observation space""\n        return OrderedDict([(k, np.stack([o[k] for o in obs])) for k in space.spaces.keys()])\n    elif isinstance(space, gym.spaces.Tuple):\n        assert isinstance(obs[0], tuple), ""non-tuple observation for environment with Tuple observation space""\n        obs_len = len(space.spaces)\n        return tuple((np.stack([o[i] for o in obs]) for i in range(obs_len)))\n    else:\n        return np.stack(obs)\n'"
stable_baselines3/common/vec_env/util.py,0,"b'""""""\nHelpers for dealing with vectorized environments.\n""""""\n\nfrom collections import OrderedDict\n\nimport gym\nimport numpy as np\n\n\ndef copy_obs_dict(obs):\n    """"""\n    Deep-copy a dict of numpy arrays.\n\n    :param obs: (OrderedDict<ndarray>): a dict of numpy arrays.\n    :return (OrderedDict<ndarray>) a dict of copied numpy arrays.\n    """"""\n    assert isinstance(obs, OrderedDict), f""unexpected type for observations \'{type(obs)}\'""\n    return OrderedDict([(k, np.copy(v)) for k, v in obs.items()])\n\n\ndef dict_to_obs(space, obs_dict):\n    """"""\n    Convert an internal representation raw_obs into the appropriate type\n    specified by space.\n\n    :param space: (gym.spaces.Space) an observation space.\n    :param obs_dict: (OrderedDict<ndarray>) a dict of numpy arrays.\n    :return (ndarray, tuple<ndarray> or dict<ndarray>): returns an observation\n            of the same type as space. If space is Dict, function is identity;\n            if space is Tuple, converts dict to Tuple; otherwise, space is\n            unstructured and returns the value raw_obs[None].\n    """"""\n    if isinstance(space, gym.spaces.Dict):\n        return obs_dict\n    elif isinstance(space, gym.spaces.Tuple):\n        assert len(obs_dict) == len(space.spaces), ""size of observation does not match size of observation space""\n        return tuple((obs_dict[i] for i in range(len(space.spaces))))\n    else:\n        assert set(obs_dict.keys()) == {None}, ""multiple observation keys for unstructured observation space""\n        return obs_dict[None]\n\n\ndef obs_space_info(obs_space):\n    """"""\n    Get dict-structured information about a gym.Space.\n\n    Dict spaces are represented directly by their dict of subspaces.\n    Tuple spaces are converted into a dict with keys indexing into the tuple.\n    Unstructured spaces are represented by {None: obs_space}.\n\n    :param obs_space: (gym.spaces.Space) an observation space\n    :return (tuple) A tuple (keys, shapes, dtypes):\n        keys: a list of dict keys.\n        shapes: a dict mapping keys to shapes.\n        dtypes: a dict mapping keys to dtypes.\n    """"""\n    if isinstance(obs_space, gym.spaces.Dict):\n        assert isinstance(obs_space.spaces, OrderedDict), ""Dict space must have ordered subspaces""\n        subspaces = obs_space.spaces\n    elif isinstance(obs_space, gym.spaces.Tuple):\n        subspaces = {i: space for i, space in enumerate(obs_space.spaces)}\n    else:\n        assert not hasattr(obs_space, \'spaces\'), f""Unsupported structured space \'{type(obs_space)}\'""\n        subspaces = {None: obs_space}\n    keys = []\n    shapes = {}\n    dtypes = {}\n    for key, box in subspaces.items():\n        keys.append(key)\n        shapes[key] = box.shape\n        dtypes[key] = box.dtype\n    return keys, shapes, dtypes\n'"
stable_baselines3/common/vec_env/vec_check_nan.py,0,"b'import warnings\n\nimport numpy as np\n\nfrom stable_baselines3.common.vec_env.base_vec_env import VecEnvWrapper\n\n\nclass VecCheckNan(VecEnvWrapper):\n    """"""\n    NaN and inf checking wrapper for vectorized environment, will raise a warning by default,\n    allowing you to know from what the NaN of inf originated from.\n\n    :param venv: (VecEnv) the vectorized environment to wrap\n    :param raise_exception: (bool) Whether or not to raise a ValueError, instead of a UserWarning\n    :param warn_once: (bool) Whether or not to only warn once.\n    :param check_inf: (bool) Whether or not to check for +inf or -inf as well\n    """"""\n\n    def __init__(self, venv, raise_exception=False, warn_once=True, check_inf=True):\n        VecEnvWrapper.__init__(self, venv)\n        self.raise_exception = raise_exception\n        self.warn_once = warn_once\n        self.check_inf = check_inf\n        self._actions = None\n        self._observations = None\n        self._user_warned = False\n\n    def step_async(self, actions):\n        self._check_val(async_step=True, actions=actions)\n\n        self._actions = actions\n        self.venv.step_async(actions)\n\n    def step_wait(self):\n        observations, rewards, news, infos = self.venv.step_wait()\n\n        self._check_val(async_step=False, observations=observations, rewards=rewards, news=news)\n\n        self._observations = observations\n        return observations, rewards, news, infos\n\n    def reset(self):\n        observations = self.venv.reset()\n        self._actions = None\n\n        self._check_val(async_step=False, observations=observations)\n\n        self._observations = observations\n        return observations\n\n    def _check_val(self, *, async_step, **kwargs):\n        # if warn and warn once and have warned once: then stop checking\n        if not self.raise_exception and self.warn_once and self._user_warned:\n            return\n\n        found = []\n        for name, val in kwargs.items():\n            has_nan = np.any(np.isnan(val))\n            has_inf = self.check_inf and np.any(np.isinf(val))\n            if has_inf:\n                found.append((name, ""inf""))\n            if has_nan:\n                found.append((name, ""nan""))\n\n        if found:\n            self._user_warned = True\n            msg = """"\n            for i, (name, type_val) in enumerate(found):\n                msg += ""found {} in {}"".format(type_val, name)\n                if i != len(found) - 1:\n                    msg += "", ""\n\n            msg += "".\\r\\nOriginated from the ""\n\n            if not async_step:\n                if self._actions is None:\n                    msg += ""environment observation (at reset)""\n                else:\n                    msg += ""environment, Last given value was: \\r\\n\\taction={}"".format(self._actions)\n            else:\n                msg += ""RL model, Last given value was: \\r\\n\\tobservations={}"".format(self._observations)\n\n            if self.raise_exception:\n                raise ValueError(msg)\n            else:\n                warnings.warn(msg, UserWarning)\n'"
stable_baselines3/common/vec_env/vec_frame_stack.py,0,"b'import warnings\n\nimport numpy as np\nfrom gym import spaces\n\nfrom stable_baselines3.common.vec_env.base_vec_env import VecEnv, VecEnvWrapper\n\n\nclass VecFrameStack(VecEnvWrapper):\n    """"""\n    Frame stacking wrapper for vectorized environment\n\n    :param venv: the vectorized environment to wrap\n    :param n_stack: Number of frames to stack\n    """"""\n\n    def __init__(self, venv: VecEnv, n_stack: int):\n        self.venv = venv\n        self.n_stack = n_stack\n        wrapped_obs_space = venv.observation_space\n        low = np.repeat(wrapped_obs_space.low, self.n_stack, axis=-1)\n        high = np.repeat(wrapped_obs_space.high, self.n_stack, axis=-1)\n        self.stackedobs = np.zeros((venv.num_envs,) + low.shape, low.dtype)\n        observation_space = spaces.Box(low=low, high=high, dtype=venv.observation_space.dtype)\n        VecEnvWrapper.__init__(self, venv, observation_space=observation_space)\n\n    def step_wait(self):\n        observations, rewards, dones, infos = self.venv.step_wait()\n        last_ax_size = observations.shape[-1]\n        self.stackedobs = np.roll(self.stackedobs, shift=-last_ax_size, axis=-1)\n        for i, done in enumerate(dones):\n            if done:\n                if \'terminal_observation\' in infos[i]:\n                    old_terminal = infos[i][\'terminal_observation\']\n                    new_terminal = np.concatenate(\n                        (self.stackedobs[i, ..., :-last_ax_size], old_terminal), axis=-1)\n                    infos[i][\'terminal_observation\'] = new_terminal\n                else:\n                    warnings.warn(\n                        ""VecFrameStack wrapping a VecEnv without terminal_observation info"")\n                self.stackedobs[i] = 0\n        self.stackedobs[..., -observations.shape[-1]:] = observations\n        return self.stackedobs, rewards, dones, infos\n\n    def reset(self):\n        """"""\n        Reset all environments\n        """"""\n        obs = self.venv.reset()\n        self.stackedobs[...] = 0\n        self.stackedobs[..., -obs.shape[-1]:] = obs\n        return self.stackedobs\n\n    def close(self):\n        self.venv.close()\n'"
stable_baselines3/common/vec_env/vec_normalize.py,0,"b'import pickle\n\nimport numpy as np\n\nfrom stable_baselines3.common.vec_env.base_vec_env import VecEnvWrapper\nfrom stable_baselines3.common.running_mean_std import RunningMeanStd\n\n\nclass VecNormalize(VecEnvWrapper):\n    """"""\n    A moving average, normalizing wrapper for vectorized environment.\n    has support for saving/loading moving average,\n\n    :param venv: (VecEnv) the vectorized environment to wrap\n    :param training: (bool) Whether to update or not the moving average\n    :param norm_obs: (bool) Whether to normalize observation or not (default: True)\n    :param norm_reward: (bool) Whether to normalize rewards or not (default: True)\n    :param clip_obs: (float) Max absolute value for observation\n    :param clip_reward: (float) Max value absolute for discounted reward\n    :param gamma: (float) discount factor\n    :param epsilon: (float) To avoid division by zero\n    """"""\n\n    def __init__(self, venv, training=True, norm_obs=True, norm_reward=True,\n                 clip_obs=10., clip_reward=10., gamma=0.99, epsilon=1e-8):\n        VecEnvWrapper.__init__(self, venv)\n        self.obs_rms = RunningMeanStd(shape=self.observation_space.shape)\n        self.ret_rms = RunningMeanStd(shape=())\n        self.clip_obs = clip_obs\n        self.clip_reward = clip_reward\n        # Returns: discounted rewards\n        self.ret = np.zeros(self.num_envs)\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.training = training\n        self.norm_obs = norm_obs\n        self.norm_reward = norm_reward\n        self.old_obs = np.array([])\n        self.old_reward = np.array([])\n\n    def __getstate__(self):\n        """"""\n        Gets state for pickling.\n\n        Excludes self.venv, as in general VecEnv\'s may not be pickleable.""""""\n        state = self.__dict__.copy()\n        # these attributes are not pickleable\n        del state[\'venv\']\n        del state[\'class_attributes\']\n        # these attributes depend on the above and so we would prefer not to pickle\n        del state[\'ret\']\n        return state\n\n    def __setstate__(self, state):\n        """"""\n        Restores pickled state.\n\n        User must call set_venv() after unpickling before using.\n\n        :param state: (dict)""""""\n        self.__dict__.update(state)\n        assert \'venv\' not in state\n        self.venv = None\n\n    def set_venv(self, venv):\n        """"""\n        Sets the vector environment to wrap to venv.\n\n        Also sets attributes derived from this such as `num_env`.\n\n        :param venv: (VecEnv)\n        """"""\n        if self.venv is not None:\n            raise ValueError(""Trying to set venv of already initialized VecNormalize wrapper."")\n        VecEnvWrapper.__init__(self, venv)\n        if self.obs_rms.mean.shape != self.observation_space.shape:\n            raise ValueError(""venv is incompatible with current statistics."")\n        self.ret = np.zeros(self.num_envs)\n\n    def step_wait(self):\n        """"""\n        Apply sequence of actions to sequence of environments\n        actions -> (observations, rewards, news)\n\n        where \'news\' is a boolean vector indicating whether each element is new.\n        """"""\n        obs, rews, news, infos = self.venv.step_wait()\n        self.old_obs = obs\n        self.old_reward = rews\n\n        if self.training:\n            self.obs_rms.update(obs)\n        obs = self.normalize_obs(obs)\n\n        if self.training:\n            self._update_reward(rews)\n        rews = self.normalize_reward(rews)\n\n        self.ret[news] = 0\n        return obs, rews, news, infos\n\n    def _update_reward(self, reward):\n        """"""Update reward normalization statistics.""""""\n        self.ret = self.ret * self.gamma + reward\n        self.ret_rms.update(self.ret)\n\n    def normalize_obs(self, obs):\n        """"""\n        Normalize observations using this VecNormalize\'s observations statistics.\n        Calling this method does not update statistics.\n        """"""\n        if self.norm_obs:\n            obs = np.clip((obs - self.obs_rms.mean) / np.sqrt(self.obs_rms.var + self.epsilon),\n                          -self.clip_obs,\n                          self.clip_obs)\n        return obs\n\n    def normalize_reward(self, reward):\n        """"""\n        Normalize rewards using this VecNormalize\'s rewards statistics.\n        Calling this method does not update statistics.\n        """"""\n        if self.norm_reward:\n            reward = np.clip(reward / np.sqrt(self.ret_rms.var + self.epsilon),\n                             -self.clip_reward, self.clip_reward)\n        return reward\n\n    def unnormalize_obs(self, obs):\n        if self.norm_obs:\n            return (obs * np.sqrt(self.obs_rms.var + self.epsilon)) + self.obs_rms.mean\n        return obs\n\n    def unnormalize_reward(self, reward):\n        if self.norm_reward:\n            return reward * np.sqrt(self.ret_rms.var + self.epsilon)\n        return reward\n\n    def get_original_obs(self):\n        """"""\n        Returns an unnormalized version of the observations from the most recent\n        step or reset.\n        """"""\n        return self.old_obs.copy()\n\n    def get_original_reward(self):\n        """"""\n        Returns an unnormalized version of the rewards from the most recent step.\n        """"""\n        return self.old_reward.copy()\n\n    def reset(self):\n        """"""\n        Reset all environments\n        """"""\n        obs = self.venv.reset()\n        self.old_obs = obs\n        self.ret = np.zeros(self.num_envs)\n        if self.training:\n            self._update_reward(self.ret)\n        return self.normalize_obs(obs)\n\n    @staticmethod\n    def load(load_path, venv):\n        """"""\n        Loads a saved VecNormalize object.\n\n        :param load_path: the path to load from.\n        :param venv: the VecEnv to wrap.\n        :return: (VecNormalize)\n        """"""\n        with open(load_path, ""rb"") as file_handler:\n            vec_normalize = pickle.load(file_handler)\n        vec_normalize.set_venv(venv)\n        return vec_normalize\n\n    def save(self, save_path):\n        with open(save_path, ""wb"") as file_handler:\n            pickle.dump(self, file_handler)\n\n    def save_running_average(self, path):\n        """"""\n        :param path: (str) path to log dir\n        """"""\n        for rms, name in zip([self.obs_rms, self.ret_rms], [\'obs_rms\', \'ret_rms\']):\n            with open(f""{path}/{name}.pkl"", \'wb\') as file_handler:\n                pickle.dump(rms, file_handler)\n\n    def load_running_average(self, path):\n        """"""\n        :param path: (str) path to log dir\n        """"""\n        for name in [\'obs_rms\', \'ret_rms\']:\n            with open(f""{path}/{name}.pkl"", \'rb\') as file_handler:\n                setattr(self, name, pickle.load(file_handler))\n'"
stable_baselines3/common/vec_env/vec_transpose.py,0,"b'import typing\nimport numpy as np\nfrom gym import spaces\n\nfrom stable_baselines3.common.vec_env.base_vec_env import VecEnv, VecEnvWrapper\nfrom stable_baselines3.common.preprocessing import is_image_space\n\nif typing.TYPE_CHECKING:\n    from stable_baselines3.common.type_aliases import GymStepReturn  # noqa: F401\n\n\nclass VecTransposeImage(VecEnvWrapper):\n    """"""\n    Re-order channels, from HxWxC to CxHxW.\n    It is required for PyTorch convolution layers.\n\n    :param venv: (VecEnv)\n    """"""\n\n    def __init__(self, venv: VecEnv):\n        assert is_image_space(venv.observation_space), \'The observation space must be an image\'\n\n        observation_space = self.transpose_space(venv.observation_space)\n        super(VecTransposeImage, self).__init__(venv, observation_space=observation_space)\n\n    @staticmethod\n    def transpose_space(observation_space: spaces.Box) -> spaces.Box:\n        """"""\n        Transpose an observation space (re-order channels).\n\n        :param observation_space: (spaces.Box)\n        :return: (spaces.Box)\n        """"""\n        assert is_image_space(observation_space), \'The observation space must be an image\'\n        width, height, channels = observation_space.shape\n        new_shape = (channels, width, height)\n        return spaces.Box(low=0, high=255, shape=new_shape, dtype=observation_space.dtype)\n\n    @staticmethod\n    def transpose_image(image: np.ndarray) -> np.ndarray:\n        """"""\n        Transpose an image or batch of images (re-order channels).\n\n        :param image: (np.ndarray)\n        :return: (np.ndarray)\n        """"""\n        if len(image.shape) == 3:\n            return np.transpose(image, (2, 0, 1))\n        return np.transpose(image, (0, 3, 1, 2))\n\n    def step_wait(self) -> \'GymStepReturn\':\n        observations, rewards, dones, infos = self.venv.step_wait()\n        return self.transpose_image(observations), rewards, dones, infos\n\n    def reset(self) -> np.ndarray:\n        """"""\n        Reset all environments\n        """"""\n        return self.transpose_image(self.venv.reset())\n\n    def close(self) -> None:\n        self.venv.close()\n'"
stable_baselines3/common/vec_env/vec_video_recorder.py,0,"b'import os\n\nfrom gym.wrappers.monitoring import video_recorder\n\nfrom stable_baselines3.common import logger\nfrom stable_baselines3.common.vec_env.base_vec_env import VecEnvWrapper\nfrom stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\nfrom stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\nfrom stable_baselines3.common.vec_env.vec_frame_stack import VecFrameStack\nfrom stable_baselines3.common.vec_env.vec_normalize import VecNormalize\n\n\nclass VecVideoRecorder(VecEnvWrapper):\n    """"""\n    Wraps a VecEnv or VecEnvWrapper object to record rendered image as mp4 video.\n    It requires ffmpeg or avconv to be installed on the machine.\n\n    :param venv: (VecEnv or VecEnvWrapper)\n    :param video_folder: (str) Where to save videos\n    :param record_video_trigger: (func) Function that defines when to start recording.\n                                        The function takes the current number of step,\n                                        and returns whether we should start recording or not.\n    :param video_length: (int)  Length of recorded videos\n    :param name_prefix: (str) Prefix to the video name\n    """"""\n\n    def __init__(self, venv, video_folder, record_video_trigger,\n                 video_length=200, name_prefix=\'rl-video\'):\n\n        VecEnvWrapper.__init__(self, venv)\n\n        self.env = venv\n        # Temp variable to retrieve metadata\n        temp_env = venv\n\n        # Unwrap to retrieve metadata dict\n        # that will be used by gym recorder\n        while isinstance(temp_env, VecNormalize) or isinstance(temp_env, VecFrameStack):\n            temp_env = temp_env.venv\n\n        if isinstance(temp_env, DummyVecEnv) or isinstance(temp_env, SubprocVecEnv):\n            metadata = temp_env.get_attr(\'metadata\')[0]\n        else:\n            metadata = temp_env.metadata\n\n        self.env.metadata = metadata\n\n        self.record_video_trigger = record_video_trigger\n        self.video_recorder = None\n\n        self.video_folder = os.path.abspath(video_folder)\n        # Create output folder if needed\n        os.makedirs(self.video_folder, exist_ok=True)\n\n        self.name_prefix = name_prefix\n        self.step_id = 0\n        self.video_length = video_length\n\n        self.recording = False\n        self.recorded_frames = 0\n\n    def reset(self):\n        obs = self.venv.reset()\n        self.start_video_recorder()\n        return obs\n\n    def start_video_recorder(self):\n        self.close_video_recorder()\n\n        video_name = f\'{self.name_prefix}-step-{self.step_id}-to-step-{self.step_id + self.video_length}\'\n        base_path = os.path.join(self.video_folder, video_name)\n        self.video_recorder = video_recorder.VideoRecorder(env=self.env,\n                                                           base_path=base_path,\n                                                           metadata={\'step_id\': self.step_id}\n                                                           )\n\n        self.video_recorder.capture_frame()\n        self.recorded_frames = 1\n        self.recording = True\n\n    def _video_enabled(self):\n        return self.record_video_trigger(self.step_id)\n\n    def step_wait(self):\n        obs, rews, dones, infos = self.venv.step_wait()\n\n        self.step_id += 1\n        if self.recording:\n            self.video_recorder.capture_frame()\n            self.recorded_frames += 1\n            if self.recorded_frames > self.video_length:\n                logger.info(""Saving video to "", self.video_recorder.path)\n                self.close_video_recorder()\n        elif self._video_enabled():\n            self.start_video_recorder()\n\n        return obs, rews, dones, infos\n\n    def close_video_recorder(self):\n        if self.recording:\n            self.video_recorder.close()\n        self.recording = False\n        self.recorded_frames = 1\n\n    def close(self):\n        VecEnvWrapper.close(self)\n        self.close_video_recorder()\n\n    def __del__(self):\n        self.close()\n'"
