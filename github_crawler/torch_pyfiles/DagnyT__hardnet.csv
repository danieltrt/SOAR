file_path,api_count,code
benchmarks/hpatches_extract_HardNet.py,11,"b'import sys\nimport argparse\nimport torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport time\nimport os\nimport sys\nsys.path.insert(0, \'/home/ubuntu/dev/opencv-3.1/build/lib\')\nimport cv2\nimport math\nimport numpy as np\nfrom tqdm import tqdm\nfrom copy import deepcopy\nimport random\nimport time\nimport numpy as np\nimport glob\nimport os\ndescr_name = \'HardNet\'\nUSE_CUDA = True\n  \n# all types of patches \ntps = [\'ref\',\'e1\',\'e2\',\'e3\',\'e4\',\'e5\',\'h1\',\'h2\',\'h3\',\'h4\',\'h5\',\\\n       \'t1\',\'t2\',\'t3\',\'t4\',\'t5\']\n\nclass hpatches_sequence:\n    """"""Class for loading an HPatches sequence from a sequence folder""""""\n    itr = tps\n    def __init__(self,base):\n        name = base.split(\'/\')\n        self.name = name[-1]\n        self.base = base\n        for t in self.itr:\n            im_path = os.path.join(base, t+\'.png\')\n            im = cv2.imread(im_path,0)\n            self.N = im.shape[0]/65\n            setattr(self, t, np.split(im, self.N))\n\nclass L2Norm(nn.Module):\n    def __init__(self):\n        super(L2Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sqrt(torch.sum(x * x, dim = 1) + self.eps)\n        x= x / norm.unsqueeze(-1).expand_as(x)\n        return x\n\nclass L1Norm(nn.Module):\n    def __init__(self):\n        super(L1Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sum(torch.abs(x), dim = 1) + self.eps\n        x= x / norm.expand_as(x)\n        return x\n\nclass HardNet(nn.Module):\n    """"""HardNet model definition\n    """"""\n    def __init__(self):\n        super(HardNet, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2,padding=1, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Conv2d(128, 128, kernel_size=8, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n\n        )\n        #self.features.apply(weights_init)\n\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n\n    def forward(self, input):\n        x_features = self.features(self.input_norm(input))\n        x = x_features.view(x_features.size(0), -1)\n        return L2Norm()(x)\n\nww = [""../pretrained/train_yosemite/checkpoint_yosemite_no_aug.pth"",\n""../pretrained/train_liberty/checkpoint_liberty_no_aug.pth"",\n""../pretrained/train_notredame/checkpoint_notredame_no_aug.pth"",\n""../pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth"",\n""../pretrained/train_notredame_with_aug/checkpoint_notredame_with_aug.pth"",\n""../pretrained/train_yosemite_with_aug/checkpoint_yosemite_with_aug.pth"",\n""../pretrained/pretrained_all_datasets/HardNet++.pth""\n]\ntry:\n    input_dir = sys.argv[1]\n    output_dir = sys.argv[2]\n    seqs = glob.glob(sys.argv[1]+\'/*\')\n    seqs = [os.path.abspath(p) for p in seqs]   \nexcept:\n    print(\'Wrong input format. Try python hpatches_extract_HardNet.py /home/ubuntu/dev/hpatches/hpatches-benchmark/data/hpatches-release /home/old-ufo/dev/hpatches/hpatches-benchmark/data/descriptors\')\n    sys.exit(1)\n    \nw = 65\n\nmodel = HardNet()\nif USE_CUDA:\n    model.cuda()\n\nfor model_weights in ww:\n    desc_suffix = model_weights.split(\'/\')[-1].replace(\'.pth\', \'\').replace(\'checkpoint_\', \'\')\n    curr_desc_name = descr_name + \'_\' + desc_suffix\n    checkpoint = torch.load(model_weights)\n    model.load_state_dict(checkpoint[\'state_dict\'])\n    for seq_path in seqs:\n        seq = hpatches_sequence(seq_path)\n        path = os.path.join(output_dir, os.path.join(curr_desc_name,seq.name))\n        if not os.path.exists(path):\n            os.makedirs(path)\n        descr = np.zeros((seq.N,128)) # trivial (mi,sigma) descriptor\n        for tp in tps:\n            print(seq.name+\'/\'+tp)\n            if os.path.isfile(os.path.join(path,tp+\'.csv\')):\n                continue\n            n_patches = 0\n            for i,patch in enumerate(getattr(seq, tp)):\n                n_patches+=1\n            t = time.time()\n            patches_for_net = np.zeros((n_patches, 1, 32, 32))\n            uuu = 0\n            for i,patch in enumerate(getattr(seq, tp)):\n                patches_for_net[i,0,:,:] = cv2.resize(patch[0:w,0:w],(32,32))\n            ###\n            model.eval()\n            outs = []\n            bs = 128;\n            n_batches = n_patches / bs + 1\n            for batch_idx in range(n_batches):\n                st = batch_idx * bs\n                if batch_idx == n_batches - 1:\n                    if (batch_idx + 1) * bs > n_patches:\n                        end = n_patches\n                    else:\n                        end = (batch_idx + 1) * bs\n                else:\n                    end = (batch_idx + 1) * bs\n                if st >= end:\n                    continue\n                data_a = patches_for_net[st: end, :, :, :].astype(np.float32)\n                data_a = torch.from_numpy(data_a)\n                if USE_CUDA:\n                    data_a = data_a.cuda()\n                data_a = Variable(data_a, volatile=True)\n                # compute output\n                out_a = model(data_a)\n                outs.append(out_a.data.cpu().numpy().reshape(-1, 128))\n            res_desc = np.concatenate(outs)\n            print(res_desc.shape, n_patches)\n            res_desc = np.reshape(res_desc, (n_patches, -1))\n            out = np.reshape(res_desc, (n_patches,-1))\n            np.savetxt(os.path.join(path,tp+\'.csv\'), out, delimiter=\',\', fmt=\'%10.5f\')   # X is an array'"
code/EvalMetrics.py,0,"b'""""""Utility methods for computing evaluating metrics. All methods assumes greater\nscores for better matches, and assumes label == 1 means match.\n\n""""""\nimport numpy as np\ndef ErrorRateAt95Recall(labels, scores):\n    distances = 1.0 / (scores + 1e-8)\n    recall_point = 0.95\n    labels = labels[np.argsort(distances)]\n    # Sliding threshold: get first index where recall >= recall_point. \n    # This is the index where the number of elements with label==1 below the threshold reaches a fraction of \n    # \'recall_point\' of the total number of elements with label==1. \n    # (np.argmax returns the first occurrence of a \'1\' in a bool array). \n    threshold_index = np.argmax(np.cumsum(labels) >= recall_point * np.sum(labels)) \n\n    FP = np.sum(labels[:threshold_index] == 0) # Below threshold (i.e., labelled positive), but should be negative\n    TN = np.sum(labels[threshold_index:] == 0) # Above threshold (i.e., labelled negative), and should be negative\n    return float(FP) / float(FP + TN)\n\'\'\'import operator\n\n\ndef ErrorRateAt95Recall(labels, scores):\n    recall_point = 0.95\n    # Sort label-score tuples by the score in descending order.\n    sorted_scores = zip(labels, scores)\n    sorted_scores.sort(key=operator.itemgetter(1), reverse=False)\n\n    # Compute error rate\n    n_match = sum(1 for x in sorted_scores if x[0] == 1)\n    n_thresh = recall_point * n_match\n    tp = 0\n    count = 0\n    for label, score in sorted_scores:\n        count += 1\n        if label == 1:\n            tp += 1\n        if tp >= n_thresh:\n            break\n\n    return float(count - tp) / count\n\'\'\'\n'"
code/HardNet.py,27,"b'#!/usr/bin/python2 -utt\n#-*- coding: utf-8 -*-\n""""""\nThis is HardNet local patch descriptor. The training code is based on PyTorch TFeat implementation\nhttps://github.com/edgarriba/examples/tree/master/triplet\nby Edgar Riba.\n\nIf you use this code, please cite \n@article{HardNet2017,\n author = {Anastasiya Mishchuk, Dmytro Mishkin, Filip Radenovic, Jiri Matas},\n    title = ""{Working hard to know your neighbor\'s margins:Local descriptor learning loss}"",\n     year = 2017}\n(c) 2017 by Anastasiia Mishchuk, Dmytro Mishkin \n""""""\n\nfrom __future__ import division, print_function\nimport sys\nfrom copy import deepcopy\nimport math\nimport argparse\nimport torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport os\nfrom tqdm import tqdm\nimport numpy as np\nimport random\nimport cv2\nimport copy\nimport PIL\nfrom EvalMetrics import ErrorRateAt95Recall\nfrom Losses import loss_HardNet, loss_random_sampling, loss_L2Net, global_orthogonal_regularization\nfrom W1BS import w1bs_extract_descs_and_save\nfrom Utils import L2Norm, cv2_scale, np_reshape\nfrom Utils import str2bool\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CorrelationPenaltyLoss(nn.Module):\n    def __init__(self):\n        super(CorrelationPenaltyLoss, self).__init__()\n\n    def forward(self, input):\n        mean1 = torch.mean(input, dim=0)\n        zeroed = input - mean1.expand_as(input)\n        cor_mat = torch.bmm(torch.t(zeroed).unsqueeze(0), zeroed.unsqueeze(0)).squeeze(0)\n        d = torch.diag(torch.diag(cor_mat))\n        no_diag = cor_mat - d\n        d_sq = no_diag * no_diag\n        return torch.sqrt(d_sq.sum())/input.size(0)\n\n# Training settings\nparser = argparse.ArgumentParser(description=\'PyTorch HardNet\')\n# Model options\n\nparser.add_argument(\'--w1bsroot\', type=str,\n                    default=\'data/sets/wxbs-descriptors-benchmark/code/\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--dataroot\', type=str,\n                    default=\'data/sets/\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--enable-logging\',type=str2bool, default=False,\n                    help=\'output to tensorlogger\')\nparser.add_argument(\'--log-dir\', default=\'data/logs/\',\n                    help=\'folder to output log\')\nparser.add_argument(\'--model-dir\', default=\'data/models/\',\n                    help=\'folder to output model checkpoints\')\nparser.add_argument(\'--experiment-name\', default= \'liberty_train/\',\n                    help=\'experiment path\')\nparser.add_argument(\'--training-set\', default= \'liberty\',\n                    help=\'Other options: notredame, yosemite\')\nparser.add_argument(\'--loss\', default= \'triplet_margin\',\n                    help=\'Other options: softmax, contrastive\')\nparser.add_argument(\'--batch-reduce\', default= \'min\',\n                    help=\'Other options: average, random, random_global, L2Net\')\nparser.add_argument(\'--num-workers\', default= 0, type=int,\n                    help=\'Number of workers to be created\')\nparser.add_argument(\'--pin-memory\',type=bool, default= True,\n                    help=\'\')\nparser.add_argument(\'--decor\',type=str2bool, default = False,\n                    help=\'L2Net decorrelation penalty\')\nparser.add_argument(\'--anchorave\', type=str2bool, default=False,\n                    help=\'anchorave\')\nparser.add_argument(\'--imageSize\', type=int, default=32,\n                    help=\'the height / width of the input image to network\')\nparser.add_argument(\'--mean-image\', type=float, default=0.443728476019,\n                    help=\'mean of train dataset for normalization\')\nparser.add_argument(\'--std-image\', type=float, default=0.20197947209,\n                    help=\'std of train dataset for normalization\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--epochs\', type=int, default=10, metavar=\'E\',\n                    help=\'number of epochs to train (default: 10)\')\nparser.add_argument(\'--anchorswap\', type=str2bool, default=True,\n                    help=\'turns on anchor swap\')\nparser.add_argument(\'--batch-size\', type=int, default=1024, metavar=\'BS\',\n                    help=\'input batch size for training (default: 1024)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=1024, metavar=\'BST\',\n                    help=\'input batch size for testing (default: 1024)\')\nparser.add_argument(\'--n-triplets\', type=int, default=5000000, metavar=\'N\',\n                    help=\'how many triplets will generate from the dataset\')\nparser.add_argument(\'--margin\', type=float, default=1.0, metavar=\'MARGIN\',\n                    help=\'the margin value for the triplet loss function (default: 1.0\')\nparser.add_argument(\'--gor\',type=str2bool, default=False,\n                    help=\'use gor\')\nparser.add_argument(\'--freq\', type=float, default=10.0,\n                    help=\'frequency for cyclic learning rate\')\nparser.add_argument(\'--alpha\', type=float, default=1.0, metavar=\'ALPHA\',\n                    help=\'gor parameter\')\nparser.add_argument(\'--lr\', type=float, default=10.0, metavar=\'LR\',\n                    help=\'learning rate (default: 10.0. Yes, ten is not typo)\')\nparser.add_argument(\'--fliprot\', type=str2bool, default=True,\n                    help=\'turns on flip and 90deg rotation augmentation\')\nparser.add_argument(\'--augmentation\', type=str2bool, default=False,\n                    help=\'turns on shift and small scale rotation augmentation\')\nparser.add_argument(\'--lr-decay\', default=1e-6, type=float, metavar=\'LRD\',\n                    help=\'learning rate decay ratio (default: 1e-6\')\nparser.add_argument(\'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--optimizer\', default=\'sgd\', type=str,\n                    metavar=\'OPT\', help=\'The optimizer to use (default: SGD)\')\n# Device options\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'enables CUDA training\')\nparser.add_argument(\'--gpu-id\', default=\'0\', type=str,\n                    help=\'id(s) for CUDA_VISIBLE_DEVICES\')\nparser.add_argument(\'--seed\', type=int, default=0, metavar=\'S\',\n                    help=\'random seed (default: 0)\')\nparser.add_argument(\'--log-interval\', type=int, default=10, metavar=\'LI\',\n                    help=\'how many batches to wait before logging training status\')\n\nargs = parser.parse_args()\n\nsuffix = \'{}_{}_{}\'.format(args.experiment_name, args.training_set, args.batch_reduce)\n\nif args.gor:\n    suffix = suffix+\'_gor_alpha{:1.1f}\'.format(args.alpha)\nif args.anchorswap:\n    suffix = suffix + \'_as\'\nif args.anchorave:\n    suffix = suffix + \'_av\'\nif args.fliprot:\n        suffix = suffix + \'_fliprot\'\n\ntriplet_flag = (args.batch_reduce == \'random_global\') or args.gor\n\ndataset_names = [\'liberty\', \'notredame\', \'yosemite\']\n\nTEST_ON_W1BS = False\n# check if path to w1bs dataset testing module exists\nif os.path.isdir(args.w1bsroot):\n    sys.path.insert(0, args.w1bsroot)\n    import utils.w1bs as w1bs\n    TEST_ON_W1BS = True\n\n# set the device to use by setting CUDA_VISIBLE_DEVICES env variable in\n# order to prevent any memory allocation on unused GPUs\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu_id\n\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nprint ((""NOT "" if not args.cuda else """") + ""Using cuda"")\n\nif args.cuda:\n    cudnn.benchmark = True\n    torch.cuda.manual_seed_all(args.seed)\ntorch.backends.cudnn.deterministic = True\n\n# create loggin directory\nif not os.path.exists(args.log_dir):\n    os.makedirs(args.log_dir)\n\n# set random seeds\nrandom.seed(args.seed)\ntorch.manual_seed(args.seed)\nnp.random.seed(args.seed)\n\nclass TripletPhotoTour(dset.PhotoTour):\n    """"""\n    From the PhotoTour Dataset it generates triplet samples\n    note: a triplet is composed by a pair of matching images and one of\n    different class.\n    """"""\n    def __init__(self, train=True, transform=None, batch_size = None,load_random_triplets = False,  *arg, **kw):\n        super(TripletPhotoTour, self).__init__(*arg, **kw)\n        self.transform = transform\n        self.out_triplets = load_random_triplets\n        self.train = train\n        self.n_triplets = args.n_triplets\n        self.batch_size = batch_size\n\n        if self.train:\n            print(\'Generating {} triplets\'.format(self.n_triplets))\n            self.triplets = self.generate_triplets(self.labels, self.n_triplets)\n\n    @staticmethod\n    def generate_triplets(labels, num_triplets):\n        def create_indices(_labels):\n            inds = dict()\n            for idx, ind in enumerate(_labels):\n                if ind not in inds:\n                    inds[ind] = []\n                inds[ind].append(idx)\n            return inds\n\n        triplets = []\n        indices = create_indices(labels.numpy())\n        unique_labels = np.unique(labels.numpy())\n        n_classes = unique_labels.shape[0]\n        # add only unique indices in batch\n        already_idxs = set()\n\n        for x in tqdm(range(num_triplets)):\n            if len(already_idxs) >= args.batch_size:\n                already_idxs = set()\n            c1 = np.random.randint(0, n_classes)\n            while c1 in already_idxs:\n                c1 = np.random.randint(0, n_classes)\n            already_idxs.add(c1)\n            c2 = np.random.randint(0, n_classes)\n            while c1 == c2:\n                c2 = np.random.randint(0, n_classes)\n            if len(indices[c1]) == 2:  # hack to speed up process\n                n1, n2 = 0, 1\n            else:\n                n1 = np.random.randint(0, len(indices[c1]))\n                n2 = np.random.randint(0, len(indices[c1]))\n                while n1 == n2:\n                    n2 = np.random.randint(0, len(indices[c1]))\n            n3 = np.random.randint(0, len(indices[c2]))\n            triplets.append([indices[c1][n1], indices[c1][n2], indices[c2][n3]])\n        return torch.LongTensor(np.array(triplets))\n\n    def __getitem__(self, index):\n        def transform_img(img):\n            if self.transform is not None:\n                img = self.transform(img.numpy())\n            return img\n\n        if not self.train:\n            m = self.matches[index]\n            img1 = transform_img(self.data[m[0]])\n            img2 = transform_img(self.data[m[1]])\n            return img1, img2, m[2]\n\n        t = self.triplets[index]\n        a, p, n = self.data[t[0]], self.data[t[1]], self.data[t[2]]\n\n        img_a = transform_img(a)\n        img_p = transform_img(p)\n        img_n = None\n        if self.out_triplets:\n            img_n = transform_img(n)\n        # transform images if required\n        if args.fliprot:\n            do_flip = random.random() > 0.5\n            do_rot = random.random() > 0.5\n            if do_rot:\n                img_a = img_a.permute(0,2,1)\n                img_p = img_p.permute(0,2,1)\n                if self.out_triplets:\n                    img_n = img_n.permute(0,2,1)\n            if do_flip:\n                img_a = torch.from_numpy(deepcopy(img_a.numpy()[:,:,::-1]))\n                img_p = torch.from_numpy(deepcopy(img_p.numpy()[:,:,::-1]))\n                if self.out_triplets:\n                    img_n = torch.from_numpy(deepcopy(img_n.numpy()[:,:,::-1]))\n        if self.out_triplets:\n            return (img_a, img_p, img_n)\n        else:\n            return (img_a, img_p)\n\n    def __len__(self):\n        if self.train:\n            return self.triplets.size(0)\n        else:\n            return self.matches.size(0)\n\nclass HardNet(nn.Module):\n    """"""HardNet model definition\n    """"""\n    def __init__(self):\n        super(HardNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2,padding=1, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Conv2d(128, 128, kernel_size=8, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n        )\n        self.features.apply(weights_init)\n        return\n    \n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    \n    def forward(self, input):\n        x_features = self.features(self.input_norm(input))\n        x = x_features.view(x_features.size(0), -1)\n        return L2Norm()(x)\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.orthogonal(m.weight.data, gain=0.6)\n        try:\n            nn.init.constant(m.bias.data, 0.01)\n        except:\n            pass\n    return\n\ndef create_loaders(load_random_triplets = False):\n\n    test_dataset_names = copy.copy(dataset_names)\n    test_dataset_names.remove(args.training_set)\n\n    kwargs = {\'num_workers\': args.num_workers, \'pin_memory\': args.pin_memory} if args.cuda else {}\n\n    np_reshape64 = lambda x: np.reshape(x, (64, 64, 1))\n    transform_test = transforms.Compose([\n            transforms.Lambda(np_reshape64),\n            transforms.ToPILImage(),\n            transforms.Resize(32),\n            transforms.ToTensor()])\n    transform_train = transforms.Compose([\n            transforms.Lambda(np_reshape64),\n            transforms.ToPILImage(),\n            transforms.RandomRotation(5,PIL.Image.BILINEAR),\n            transforms.RandomResizedCrop(32, scale = (0.9,1.0),ratio = (0.9,1.1)),\n            transforms.Resize(32),\n            transforms.ToTensor()])\n    transform = transforms.Compose([\n            transforms.Lambda(cv2_scale),\n            transforms.Lambda(np_reshape),\n            transforms.ToTensor(),\n            transforms.Normalize((args.mean_image,), (args.std_image,))])\n    if not args.augmentation:\n        transform_train = transform\n        transform_test = transform\n    train_loader = torch.utils.data.DataLoader(\n            TripletPhotoTour(train=True,\n                             load_random_triplets = load_random_triplets,\n                             batch_size=args.batch_size,\n                             root=args.dataroot,\n                             name=args.training_set,\n                             download=True,\n                             transform=transform_train),\n                             batch_size=args.batch_size,\n                             shuffle=False, **kwargs)\n\n    test_loaders = [{\'name\': name,\n                     \'dataloader\': torch.utils.data.DataLoader(\n             TripletPhotoTour(train=False,\n                     batch_size=args.test_batch_size,\n                     root=args.dataroot,\n                     name=name,\n                     download=True,\n                     transform=transform_test),\n                        batch_size=args.test_batch_size,\n                        shuffle=False, **kwargs)}\n                    for name in test_dataset_names]\n\n    return train_loader, test_loaders\n\ndef train(train_loader, model, optimizer, epoch, logger, load_triplets  = False):\n    # switch to train mode\n    model.train()\n    pbar = tqdm(enumerate(train_loader))\n    for batch_idx, data in pbar:\n        if load_triplets:\n            data_a, data_p, data_n = data\n        else:\n            data_a, data_p = data\n\n        if args.cuda:\n            data_a, data_p  = data_a.cuda(), data_p.cuda()\n            data_a, data_p = Variable(data_a), Variable(data_p)\n            out_a = model(data_a)\n            out_p = model(data_p)\n        if load_triplets:\n            data_n  = data_n.cuda()\n            data_n = Variable(data_n)\n            out_n = model(data_n)\n\n        if args.batch_reduce == \'L2Net\':\n            loss = loss_L2Net(out_a, out_p, anchor_swap = args.anchorswap,\n                    margin = args.margin, loss_type = args.loss)\n        elif args.batch_reduce == \'random_global\':\n            loss = loss_random_sampling(out_a, out_p, out_n,\n                margin=args.margin,\n                anchor_swap=args.anchorswap,\n                loss_type = args.loss)\n        else:\n            loss = loss_HardNet(out_a, out_p,\n                            margin=args.margin,\n                            anchor_swap=args.anchorswap,\n                            anchor_ave=args.anchorave,\n                            batch_reduce = args.batch_reduce,\n                            loss_type = args.loss)\n\n        if args.decor:\n            loss += CorrelationPenaltyLoss()(out_a)\n            \n        if args.gor:\n            loss += args.alpha*global_orthogonal_regularization(out_a, out_n)\n            \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        adjust_learning_rate(optimizer)\n        if batch_idx % args.log_interval == 0:\n            pbar.set_description(\n                \'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                    epoch, batch_idx * len(data_a), len(train_loader.dataset),\n                           100. * batch_idx / len(train_loader),\n                    loss.item()))\n\n    if (args.enable_logging):\n        logger.log_value(\'loss\', loss.item()).step()\n\n    try:\n        os.stat(\'{}{}\'.format(args.model_dir,suffix))\n    except:\n        os.makedirs(\'{}{}\'.format(args.model_dir,suffix))\n\n    torch.save({\'epoch\': epoch + 1, \'state_dict\': model.state_dict()},\n               \'{}{}/checkpoint_{}.pth\'.format(args.model_dir,suffix,epoch))\n\ndef test(test_loader, model, epoch, logger, logger_test_name):\n    # switch to evaluate mode\n    model.eval()\n\n    labels, distances = [], []\n\n    pbar = tqdm(enumerate(test_loader))\n    for batch_idx, (data_a, data_p, label) in pbar:\n\n        if args.cuda:\n            data_a, data_p = data_a.cuda(), data_p.cuda()\n\n        data_a, data_p, label = Variable(data_a, volatile=True), \\\n                                Variable(data_p, volatile=True), Variable(label)\n        out_a = model(data_a)\n        out_p = model(data_p)\n        dists = torch.sqrt(torch.sum((out_a - out_p) ** 2, 1))  # euclidean distance\n        distances.append(dists.data.cpu().numpy().reshape(-1,1))\n        ll = label.data.cpu().numpy().reshape(-1, 1)\n        labels.append(ll)\n\n        if batch_idx % args.log_interval == 0:\n            pbar.set_description(logger_test_name+\' Test Epoch: {} [{}/{} ({:.0f}%)]\'.format(\n                epoch, batch_idx * len(data_a), len(test_loader.dataset),\n                       100. * batch_idx / len(test_loader)))\n\n    num_tests = test_loader.dataset.matches.size(0)\n    labels = np.vstack(labels).reshape(num_tests)\n    distances = np.vstack(distances).reshape(num_tests)\n\n    fpr95 = ErrorRateAt95Recall(labels, 1.0 / (distances + 1e-8))\n    print(\'\\33[91mTest set: Accuracy(FPR95): {:.8f}\\n\\33[0m\'.format(fpr95))\n\n    if (args.enable_logging):\n        logger.log_value(logger_test_name+\' fpr95\', fpr95)\n    return\n\ndef adjust_learning_rate(optimizer):\n    """"""Updates the learning rate given the learning rate decay.\n    The routine has been implemented according to the original Lua SGD optimizer\n    """"""\n    for group in optimizer.param_groups:\n        if \'step\' not in group:\n            group[\'step\'] = 0.\n        else:\n            group[\'step\'] += 1.\n        group[\'lr\'] = args.lr * (\n        1.0 - float(group[\'step\']) * float(args.batch_size) / (args.n_triplets * float(args.epochs)))\n    return\n\ndef create_optimizer(model, new_lr):\n    # setup optimizer\n    if args.optimizer == \'sgd\':\n        optimizer = optim.SGD(model.parameters(), lr=new_lr,\n                              momentum=0.9, dampening=0.9,\n                              weight_decay=args.wd)\n    elif args.optimizer == \'adam\':\n        optimizer = optim.Adam(model.parameters(), lr=new_lr,\n                               weight_decay=args.wd)\n    else:\n        raise Exception(\'Not supported optimizer: {0}\'.format(args.optimizer))\n    return optimizer\n\n\ndef main(train_loader, test_loaders, model, logger, file_logger):\n    # print the experiment configuration\n    print(\'\\nparsed options:\\n{}\\n\'.format(vars(args)))\n\n    #if (args.enable_logging):\n    #    file_logger.log_string(\'logs.txt\', \'\\nparsed options:\\n{}\\n\'.format(vars(args)))\n\n    if args.cuda:\n        model.cuda()\n\n    optimizer1 = create_optimizer(model.features, args.lr)\n\n    # optionally resume from a checkpoint\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(\'=> loading checkpoint {}\'.format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            checkpoint = torch.load(args.resume)\n            model.load_state_dict(checkpoint[\'state_dict\'])\n        else:\n            print(\'=> no checkpoint found at {}\'.format(args.resume))\n            \n    \n    start = args.start_epoch\n    end = start + args.epochs\n    for epoch in range(start, end):\n\n        # iterate over test loaders and test results\n        train(train_loader, model, optimizer1, epoch, logger, triplet_flag)\n        for test_loader in test_loaders:\n            test(test_loader[\'dataloader\'], model, epoch, logger, test_loader[\'name\'])\n        \n        if TEST_ON_W1BS :\n            # print(weights_path)\n            patch_images = w1bs.get_list_of_patch_images(\n                DATASET_DIR=args.w1bsroot.replace(\'/code\', \'/data/W1BS\'))\n            desc_name = \'curr_desc\'# + str(random.randint(0,100))\n            \n            DESCS_DIR = LOG_DIR + \'/temp_descs/\' #args.w1bsroot.replace(\'/code\', ""/data/out_descriptors"")\n            OUT_DIR = DESCS_DIR.replace(\'/temp_descs/\', ""/out_graphs/"")\n\n            for img_fname in patch_images:\n                w1bs_extract_descs_and_save(img_fname, model, desc_name, cuda = args.cuda,\n                                            mean_img=args.mean_image,\n                                            std_img=args.std_image, out_dir = DESCS_DIR)\n\n\n            force_rewrite_list = [desc_name]\n            w1bs.match_descriptors_and_save_results(DESC_DIR=DESCS_DIR, do_rewrite=True,\n                                                    dist_dict={},\n                                                    force_rewrite_list=force_rewrite_list)\n            if(args.enable_logging):\n                w1bs.draw_and_save_plots_with_loggers(DESC_DIR=DESCS_DIR, OUT_DIR=OUT_DIR,\n                                         methods=[""SNN_ratio""],\n                                         descs_to_draw=[desc_name],\n                                         logger=file_logger,\n                                         tensor_logger = logger)\n            else:\n                w1bs.draw_and_save_plots(DESC_DIR=DESCS_DIR, OUT_DIR=OUT_DIR,\n                                         methods=[""SNN_ratio""],\n                                         descs_to_draw=[desc_name])\n        #randomize train loader batches\n        train_loader, test_loaders2 = create_loaders(load_random_triplets=triplet_flag)\n\n\nif __name__ == \'__main__\':\n    LOG_DIR = args.log_dir\n    if not os.path.isdir(LOG_DIR):\n        os.makedirs(LOG_DIR)\n    LOG_DIR = os.path.join(args.log_dir, suffix)\n    DESCS_DIR = os.path.join(LOG_DIR, \'temp_descs\')\n    if TEST_ON_W1BS:\n        if not os.path.isdir(DESCS_DIR):\n            os.makedirs(DESCS_DIR)\n    logger, file_logger = None, None\n    model = HardNet()\n    if(args.enable_logging):\n        from Loggers import Logger, FileLogger\n        logger = Logger(LOG_DIR)\n        #file_logger = FileLogger(./log/+suffix)\n    train_loader, test_loaders = create_loaders(load_random_triplets = triplet_flag)\n    main(train_loader, test_loaders, model, logger, file_logger)\n'"
code/HardNetClassicalHardNegMining.py,23,"b'\xef\xbb\xbf#!/usr/bin/python2 -utt\n#-*- coding: utf-8 -*-\n\n""""""\nThis is HardNet local patch descriptor. The training code is based on PyTorch TFeat implementation\nhttps://github.com/edgarriba/examples/tree/master/triplet\nby Edgar Riba.\n\nIf you use this code, please cite \n@article{HardNet2017,\n author = {Anastasiya Mishchuk, Dmytro Mishkin, Filip Radenovic, Jiri Matas},\n    title = ""{Working hard to know your neighbor\'s margins:Local descriptor learning loss}"",\n     year = 2017}\n(c) 2017 by Anastasiia Mishchuk, Dmytro Mishkin \n""""""\n\nfrom __future__ import division, print_function\nimport sys\nfrom copy import deepcopy\nimport argparse\nimport torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport os\nfrom tqdm import tqdm\nimport numpy as np\nimport random\nimport cv2\nimport copy\nfrom EvalMetrics import ErrorRateAt95Recall\nfrom Loggers import Logger, FileLogger\nfrom W1BS import w1bs_extract_descs_and_save\nfrom Utils import L2Norm, cv2_scale, np_reshape\nfrom Utils import str2bool\nimport torch.utils.data as data\nimport torch.utils.data as data_utils\nimport torch.nn.functional as F\n\nimport faiss\n\n# Training settings\nparser = argparse.ArgumentParser(description=\'PyTorch HardNet\')\n# Model options\n\nparser.add_argument(\'--w1bsroot\', type=str,\n                    default=\'data/sets/wxbs-descriptors-benchmark/code\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--dataroot\', type=str,\n                    default=\'data/sets/\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--enable-logging\',type=bool, default=True,\n                    help=\'folder to output model checkpoints\')\nparser.add_argument(\'--log-dir\', default=\'data/logs/\',\n                    help=\'folder to output model checkpoints\')\nparser.add_argument(\'--experiment-name\', default= \'liberty_train_hard_mining/\',\n                    help=\'experiment path\')\nparser.add_argument(\'--training-set\', default= \'liberty\',\n                    help=\'Other options: notredame, yosemite\')\nparser.add_argument(\'--num-workers\', default= 8,\n                    help=\'Number of workers to be created\')\nparser.add_argument(\'--pin-memory\',type=bool, default= True,\n                    help=\'\')\nparser.add_argument(\'--anchorave\', type=bool, default=False,\n                    help=\'anchorave\')\nparser.add_argument(\'--imageSize\', type=int, default=32,\n                    help=\'the height / width of the input image to network\')\nparser.add_argument(\'--mean-image\', type=float, default=0.443728476019,\n                    help=\'mean of train dataset for normalization\')\nparser.add_argument(\'--std-image\', type=float, default=0.20197947209,\n                    help=\'std of train dataset for normalization\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--epochs\', type=int, default=10, metavar=\'E\',\n                    help=\'number of epochs to train (default: 10)\')\nparser.add_argument(\'--anchorswap\', type=bool, default=True,\n                    help=\'turns on anchor swap\')\nparser.add_argument(\'--batch-size\', type=int, default=1024, metavar=\'BS\',\n                    help=\'input batch size for training (default: 128)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=1024, metavar=\'BST\',\n                    help=\'input batch size for testing (default: 1000)\')\nparser.add_argument(\'--n-triplets\', type=int, default=5000000, metavar=\'N\',\n                    help=\'how many triplets will generate from the dataset\')\nparser.add_argument(\'--margin\', type=float, default=1.0, metavar=\'MARGIN\',\n                    help=\'the margin value for the triplet loss function (default: 1.0\')\nparser.add_argument(\'--act-decay\', type=float, default=0,\n                    help=\'activity L2 decay, default 0\')\nparser.add_argument(\'--lr\', type=float, default=0.1, metavar=\'LR\',\n                    help=\'learning rate (default: 0.1)\')\nparser.add_argument(\'--fliprot\', type=str2bool, default=False,\n                    help=\'turns on flip and 90deg rotation augmentation\')\nparser.add_argument(\'--lr-decay\', default=1e-6, type=float, metavar=\'LRD\',\n                    help=\'learning rate decay ratio (default: 1e-6\')\nparser.add_argument(\'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--optimizer\', default=\'sgd\', type=str,\n                    metavar=\'OPT\', help=\'The optimizer to use (default: SGD)\')\n# Device options\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'enables CUDA training\')\nparser.add_argument(\'--gpu-id\', default=\'0\', type=str,\n                    help=\'id(s) for CUDA_VISIBLE_DEVICES\')\nparser.add_argument(\'--seed\', type=int, default=0, metavar=\'S\',\n                    help=\'random seed (default: 0)\')\nparser.add_argument(\'--log-interval\', type=int, default=10, metavar=\'LI\',\n                    help=\'how many batches to wait before logging training status\')\n\nargs = parser.parse_args()\n\ndataset_names = [\'liberty\', \'notredame\', \'yosemite\']\n\n# check if path to w1bs dataset testing module exists\nif os.path.isdir(args.w1bsroot):\n    sys.path.insert(0, args.w1bsroot)\n    import utils.w1bs as w1bs\n    TEST_ON_W1BS = True\n\n# set the device to use by setting CUDA_VISIBLE_DEVICES env variable in\n# order to prevent any memory allocation on unused GPUs\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu_id\n\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nif args.cuda:\n    cudnn.benchmark = True\n    torch.cuda.manual_seed_all(args.seed)\n\nLOG_DIR = args.log_dir + args.experiment_name\n# create loggin directory\nif not os.path.exists(LOG_DIR):\n    os.makedirs(LOG_DIR)\n# set random seeds\ntorch.manual_seed(args.seed)\nnp.random.seed(args.seed)\n\nclass TripletPhotoTour(dset.PhotoTour):\n    """"""From the PhotoTour Dataset it generates triplet samples\n    note: a triplet is composed by a pair of matching images and one of\n    different class.\n    """"""\n    def __init__(self, train=True, transform=None, batch_size = None, *arg, **kw):\n        super(TripletPhotoTour, self).__init__(*arg, **kw)\n        self.transform = transform\n\n        self.train = train\n        self.n_triplets = args.n_triplets\n        self.batch_size = batch_size\n\n        if self.train:\n            print(\'Generating {} triplets\'.format(self.n_triplets))\n            self.triplets = self.generate_triplets(self.labels, self.n_triplets)\n\n    @staticmethod\n    def generate_triplets(labels, num_triplets):\n        def create_indices(_labels):\n            inds = dict()\n            for idx, ind in enumerate(_labels):\n                if ind not in inds:\n                    inds[ind] = []\n                inds[ind].append(idx)\n            return inds\n\n        triplets = []\n        indices = create_indices(labels)\n        unique_labels = np.unique(labels.numpy())\n        n_classes = unique_labels.shape[0]\n        # add only unique indices in batch\n        already_idxs = set()\n\n        for x in tqdm(range(num_triplets)):\n            if len(already_idxs) >= args.batch_size:\n                already_idxs = set()\n            c1 = np.random.randint(0, n_classes - 1)\n            while c1 in already_idxs:\n                c1 = np.random.randint(0, n_classes - 1)\n            already_idxs.add(c1)\n            c2 = np.random.randint(0, n_classes - 1)\n            while c1 == c2:\n                c2 = np.random.randint(0, n_classes - 1)\n            if len(indices[c1]) == 2:  # hack to speed up process\n                n1, n2 = 0, 1\n            else:\n                n1 = np.random.randint(0, len(indices[c1]) - 1)\n                n2 = np.random.randint(0, len(indices[c1]) - 1)\n                while n1 == n2:\n                    n2 = np.random.randint(0, len(indices[c1]) - 1)\n            n3 = np.random.randint(0, len(indices[c2]) - 1)\n            triplets.append([indices[c1][n1], indices[c1][n2], indices[c2][n3]])\n        return torch.LongTensor(np.array(triplets))\n\n    def __getitem__(self, index):\n        def transform_img(img):\n            if self.transform is not None:\n                img = self.transform(img.numpy())\n            return img\n\n        if not self.train:\n            m = self.matches[index]\n            img1 = transform_img(self.data[m[0]])\n            img2 = transform_img(self.data[m[1]])\n            return img1, img2, m[2]\n\n        t = self.triplets[index]\n        a, p, n = self.data[t[0]], self.data[t[1]], self.data[t[2]]\n\n        img_a = transform_img(a)\n        img_p = transform_img(p)\n        img_n = transform_img(n)\n\n        # transform images if required\n        # if args.fliprot:\n        #     do_flip = random.random() > 0.5\n        #     do_rot = random.random() > 0.5\n        #\n        #     if do_rot:\n        #         img_a = img_a.permute(0,2,1)\n        #         img_p = img_p.permute(0,2,1)\n        #\n        #     if do_flip:\n        #         img_a = torch.from_numpy(deepcopy(img_a.numpy()[:,:,::-1]))\n        #         img_p = torch.from_numpy(deepcopy(img_p.numpy()[:,:,::-1]))\n\n        return img_a, img_p, img_n\n\n    def __len__(self):\n        if self.train:\n            return self.triplets.size(0)\n        else:\n            return self.matches.size(0)\n\nclass TripletPhotoTourHardNegatives(dset.PhotoTour):\n    """"""From the PhotoTour Dataset it generates triplet samples\n    note: a triplet is composed by a pair of matching images and one of\n    different class.\n    """"""\n    def __init__(self, negative_indices, train=True, transform=None, batch_size = None, *arg, **kw):\n        super(TripletPhotoTourHardNegatives, self).__init__(*arg, **kw)\n        self.transform = transform\n\n        self.train = train\n        self.n_triplets = args.n_triplets\n        self.negative_indices = negative_indices\n        self.batch_size = batch_size\n\n        if self.train:\n            print(\'Generating {} triplets\'.format(self.n_triplets))\n            self.triplets = self.generate_triplets(self.labels, self.n_triplets, self.negative_indices)\n\n\n    @staticmethod\n    def generate_triplets(labels, num_triplets, negative_indices):\n        def create_indices(_labels):\n            inds = dict()\n            for idx, ind in enumerate(_labels):\n                if ind not in inds:\n                    inds[ind] = []\n                inds[ind].append(idx)\n            return inds\n\n        triplets = []\n        indices = create_indices(labels)\n        unique_labels = np.unique(labels.numpy())\n        n_classes = unique_labels.shape[0]\n\n        # add only unique indices in batch\n        already_idxs = set()\n        count  = 0\n        for x in tqdm(range(num_triplets)):\n            if len(already_idxs) >= args.batch_size:\n                already_idxs = set()\n            c1 = np.random.randint(0, n_classes - 1)\n            while c1 in already_idxs:\n                c1 = np.random.randint(0, n_classes - 1)\n            already_idxs.add(c1)\n            if len(indices[c1]) == 2:  # hack to speed up process\n                n1, n2 = 0, 1\n            else:\n                n1 = np.random.randint(0, len(indices[c1]) - 1)\n                n2 = np.random.randint(0, len(indices[c1]) - 1)\n                while n1 == n2:\n                    n2 = np.random.randint(0, len(indices[c1]) - 1)\n            indx = indices[c1][n1]\n            if(len(negative_indices[indx])>0):\n                negative_indx = random.choice(negative_indices[indx])\n            else:\n                count+=1\n                c2 = np.random.randint(0, n_classes - 1)\n                while c1 == c2:\n                    c2 = np.random.randint(0, n_classes - 1)\n                n3 = np.random.randint(0, len(indices[c2]) - 1)\n                negative_indx = indices[c2][n3]\n\n            already_idxs.add(c1)\n\n            triplets.append([indices[c1][n1], indices[c1][n2], negative_indx])\n\n        print(count)\n        print(\'triplets are generated. amount of triplets: {}\'.format(len(triplets)))\n        return torch.LongTensor(np.array(triplets))\n\n\n\n    def __getitem__(self, index):\n        def transform_img(img):\n            if self.transform is not None:\n                img = self.transform(img.numpy())\n            return img\n\n        if not self.train:\n            m = self.matches[index]\n            img1 = transform_img(self.data[m[0]])\n            img2 = transform_img(self.data[m[1]])\n            return img1, img2, m[2]\n\n        t = self.triplets[index]\n        a, p, n = self.data[t[0]], self.data[t[1]], self.data[t[2]]\n\n        img_a = transform_img(a)\n        img_p = transform_img(p)\n        img_n = transform_img(n)\n\n        return img_a, img_p, img_n\n\n    def __len__(self):\n        if self.train:\n            return self.triplets.size(0)\n        else:\n            return self.matches.size(0)\n\nclass TNet(nn.Module):\n    """"""TFeat model definition\n    """"""\n    def __init__(self):\n        super(TNet, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2,padding=1),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Conv2d(128, 128, kernel_size=8),\n            nn.BatchNorm2d(128, affine=False),\n\n        )\n        self.features.apply(weights_init)\n\n    def forward(self, input):\n        flat = input.view(input.size(0), -1)\n        mp = torch.sum(flat, dim=1) / (32. * 32.)\n        sp = torch.std(flat, dim=1) + 1e-7\n        x_features = self.features(\n            (input - mp.unsqueeze(-1).unsqueeze(-1).expand_as(input)) / sp.unsqueeze(-1).unsqueeze(1).expand_as(input))\n        x = x_features.view(x_features.size(0), -1)\n        return L2Norm()(x)\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.orthogonal(m.weight.data, gain=0.7)\n        nn.init.constant(m.bias.data, 0.01)\n    if isinstance(m, nn.Linear):\n        nn.init.orthogonal(m.weight.data, gain=0.01)\n        nn.init.constant(m.bias.data, 0.)\n\ndef create_loaders():\n\n    test_dataset_names = copy.copy(dataset_names)\n    test_dataset_names.remove(args.training_set)\n\n    kwargs = {\'num_workers\': args.num_workers, \'pin_memory\': args.pin_memory} if args.cuda else {}\n\n    transform = transforms.Compose([\n            transforms.Lambda(cv2_scale),\n            transforms.Lambda(np_reshape),\n            transforms.ToTensor(),\n            transforms.Normalize((args.mean_image,), (args.std_image,))])\n\n    trainPhotoTourDataset =  TripletPhotoTour(train=True,\n                             batch_size=args.batch_size,\n                             root=args.dataroot,\n                             name=args.training_set,\n                             download=True,\n                             transform=transform)\n\n    test_loaders = [{\'name\': name,\n                     \'dataloader\': torch.utils.data.DataLoader(\n             TripletPhotoTour(train=False,\n                     batch_size=args.test_batch_size,\n                     root=args.dataroot,\n                     name=name,\n                     download=True,\n                     transform=transform),\n                        batch_size=args.test_batch_size,\n                        shuffle=False, **kwargs)}\n                    for name in test_dataset_names]\n\n    return trainPhotoTourDataset, test_loaders\n\ndef train(train_loader, model, optimizer, epoch, logger):\n    # switch to train mode\n    model.train()\n    pbar = tqdm(enumerate(train_loader))\n    for batch_idx, (data_a, data_p, data_n) in pbar:\n\n        if args.cuda:\n            data_a, data_p, data_n = data_a.cuda(), data_p.cuda(), data_n.cuda()\n\n        data_a, data_p, data_n = Variable(data_a), Variable(data_p), Variable(data_n)\n\n        out_a, out_p, out_n = model(data_a), model(data_p), model(data_n)\n\n        #hardnet loss\n        loss = F.triplet_margin_loss(out_p, out_a, out_n, margin=args.margin, swap=args.anchorswap)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        adjust_learning_rate(optimizer)\n        if(logger!=None):\n         logger.log_value(\'loss\', loss.data[0]).step()\n\n        if batch_idx % args.log_interval == 0:\n            pbar.set_description(\n                \'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                    epoch, batch_idx * len(data_a), len(train_loader.dataset),\n                           100. * batch_idx / len(train_loader),\n                    loss.data[0]))\n\n    torch.save({\'epoch\': epoch + 1, \'state_dict\': model.state_dict()},\n               \'{}/checkpoint_{}.pth\'.format(LOG_DIR, epoch))\n\ndef test(test_loader, model, epoch, logger, logger_test_name):\n    # switch to evaluate mode\n    model.eval()\n\n    labels, distances = [], []\n\n    pbar = tqdm(enumerate(test_loader))\n    for batch_idx, (data_a, data_p, label) in pbar:\n\n        if args.cuda:\n            data_a, data_p = data_a.cuda(), data_p.cuda()\n\n        data_a, data_p, label = Variable(data_a, volatile=True), \\\n                                Variable(data_p, volatile=True), Variable(label)\n\n        out_a, out_p = model(data_a), model(data_p)\n        dists = torch.sqrt(torch.sum((out_a - out_p) ** 2, 1))  # euclidean distance\n        distances.append(dists.data.cpu().numpy())\n        ll = label.data.cpu().numpy().reshape(-1, 1)\n        labels.append(ll)\n\n        if batch_idx % args.log_interval == 0:\n            pbar.set_description(logger_test_name+\' Test Epoch: {} [{}/{} ({:.0f}%)]\'.format(\n                epoch, batch_idx * len(data_a), len(test_loader.dataset),\n                       100. * batch_idx / len(test_loader)))\n\n    num_tests = test_loader.dataset.matches.size(0)\n    labels = np.vstack(labels).reshape(num_tests)\n    distances = np.vstack(distances).reshape(num_tests)\n\n    fpr95 = ErrorRateAt95Recall(labels, 1.0 / (distances + 1e-8))\n    print(\'\\33[91mTest set: Accuracy(FPR95): {:.8f}\\n\\33[0m\'.format(fpr95))\n\n    if (args.enable_logging):\n        if(logger!=None):\n            logger.log_value(logger_test_name+\' fpr95\', fpr95)\n    return\n\ndef adjust_learning_rate(optimizer):\n    """"""Updates the learning rate given the learning rate decay.\n    The routine has been implemented according to the original Lua SGD optimizer\n    """"""\n    for group in optimizer.param_groups:\n        if \'step\' not in group:\n            group[\'step\'] = 0.\n        else:\n            group[\'step\'] += 1.\n        group[\'lr\'] = args.lr * (\n        1.0 - float(group[\'step\']) * float(args.batch_size) / (args.n_triplets * float(args.epochs)))\n    return\n\ndef create_optimizer(model, new_lr):\n    # setup optimizer\n    if args.optimizer == \'sgd\':\n        optimizer = optim.SGD(model.parameters(), lr=new_lr,\n                              momentum=0.9, dampening=0.9,\n                              weight_decay=args.wd)\n    elif args.optimizer == \'adam\':\n        optimizer = optim.Adam(model.parameters(), lr=new_lr,\n                               weight_decay=args.wd)\n    else:\n        raise Exception(\'Not supported optimizer: {0}\'.format(args.optimizer))\n    return optimizer\n\n\ndef main(trainPhotoTourDataset, test_loaders, model, logger, file_logger):\n    # print the experiment configuration\n    print(\'\\nparsed options:\\n{}\\n\'.format(vars(args)))\n\n    if (args.enable_logging):\n        file_logger.log_string(\'logs.txt\', \'\\nparsed options:\\n{}\\n\'.format(vars(args)))\n\n    if args.cuda:\n        model.cuda()\n\n    optimizer1 = create_optimizer(model.features, args.lr)\n\n    # optionally resume from a checkpoint\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(\'=> loading checkpoint {}\'.format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            checkpoint = torch.load(args.resume)\n            model.load_state_dict(checkpoint[\'state_dict\'])\n        else:\n            print(\'=> no checkpoint found at {}\'.format(args.resume))\n\n    start = args.start_epoch\n    end = start + args.epochs\n\n    kwargs = {\'num_workers\': args.num_workers, \'pin_memory\': args.pin_memory} if args.cuda else {}\n\n    transform = transforms.Compose([\n            transforms.Lambda(cv2_scale),\n            transforms.Lambda(np_reshape),\n            transforms.ToTensor(),\n            transforms.Normalize((args.mean_image,), (args.std_image,))])\n\n    for epoch in range(start, end):\n\n        model.eval()\n        # #\n        descriptors = get_descriptors_for_dataset(model, trainPhotoTourDataset)\n        # #\n        np.save(\'descriptors.npy\', descriptors)\n        descriptors = np.load(\'descriptors.npy\')\n        #\n        hard_negatives = get_hard_negatives(trainPhotoTourDataset, descriptors)\n        np.save(\'descriptors_min_dist.npy\', hard_negatives)\n        hard_negatives = np.load(\'descriptors_min_dist.npy\')\n        print(hard_negatives[0])\n\n        trainPhotoTourDatasetWithHardNegatives = TripletPhotoTourHardNegatives(train=True,\n                                                              negative_indices=hard_negatives,\n                                                              batch_size=args.batch_size,\n                                                              root=args.dataroot,\n                                                              name=args.training_set,\n                                                              download=True,\n                                                              transform=transform)\n\n        train_loader = torch.utils.data.DataLoader(trainPhotoTourDatasetWithHardNegatives,\n                                                   batch_size=args.batch_size,\n                                                   shuffle=False, **kwargs)\n\n        train(train_loader, model, optimizer1, epoch, logger)\n\n        # iterate over test loaders and test results\n        for test_loader in test_loaders:\n            test(test_loader[\'dataloader\'], model, epoch, logger, test_loader[\'name\'])\n\n        if TEST_ON_W1BS :\n            # print(weights_path)\n            patch_images = w1bs.get_list_of_patch_images(\n                DATASET_DIR=args.w1bsroot.replace(\'/code\', \'/data/W1BS\'))\n            desc_name = \'curr_desc\'\n\n            for img_fname in patch_images:\n                w1bs_extract_descs_and_save(img_fname, model, desc_name, cuda = args.cuda,\n                                            mean_img=args.mean_image,\n                                            std_img=args.std_image)\n\n            DESCS_DIR = args.w1bsroot.replace(\'/code\', ""/data/out_descriptors"")\n            OUT_DIR = args.w1bsroot.replace(\'/code\', ""/data/out_graphs"")\n\n            force_rewrite_list = [desc_name]\n            w1bs.match_descriptors_and_save_results(DESC_DIR=DESCS_DIR, do_rewrite=True,\n                                                    dist_dict={},\n                                                    force_rewrite_list=force_rewrite_list)\n            if(args.enable_logging):\n                w1bs.draw_and_save_plots_with_loggers(DESC_DIR=DESCS_DIR, OUT_DIR=OUT_DIR,\n                                         methods=[""SNN_ratio""],\n                                         descs_to_draw=[desc_name],\n                                         logger=file_logger,\n                                         tensor_logger = None)\n            else:\n                w1bs.draw_and_save_plots(DESC_DIR=DESCS_DIR, OUT_DIR=OUT_DIR,\n                                         methods=[""SNN_ratio""],\n                                         descs_to_draw=[desc_name],\n                                         really_draw = False)\n\n\nclass PhototourTrainingData(data.Dataset):\n\n    def __init__(self, data):\n        self.data_files = data\n\n    def __getitem__(self, item):\n        res = self.data_files[item]\n        return res\n\n    def __len__(self):\n        return len(self.data_files)\n\ndef BuildKNNGraphByFAISS_GPU(db,k):\n    dbsize, dim = db.shape\n    flat_config = faiss.GpuIndexFlatConfig()\n    flat_config.device = 0\n    res = faiss.StandardGpuResources()\n    nn = faiss.GpuIndexFlatL2(res, dim, flat_config)\n    nn.add(db)\n    dists,idx = nn.search(db, k+1)\n    return idx[:,1:],dists[:,1:]\n\ndef get_descriptors_for_dataset(model, trainPhotoTourDataset):\n\n    transformed = []\n\n    for img in trainPhotoTourDataset.data:\n        transformed.append(trainPhotoTourDataset.transform(img.numpy()))\n    print(len(transformed))\n    phototour_loader = data_utils.DataLoader(PhototourTrainingData(transformed), batch_size=128, shuffle=False)\n    descriptors = []\n    pbar = tqdm(enumerate(phototour_loader))\n    for batch_idx, data_a in pbar:\n\n        if args.cuda:\n            model.cuda()\n            data_a = data_a.cuda()\n\n        data_a = Variable(data_a, volatile=True),\n        out_a = model(data_a[0])\n        descriptors.extend(out_a.data.cpu().numpy())\n\n    return descriptors\n\n\ndef remove_descriptors_with_same_index(min_dist_indices, indices, labels, descriptors):\n\n    res_min_dist_indices = []\n\n    for current_index in range(0, len(min_dist_indices)):\n        # get indices of the same 3d points\n        point3d_indices = labels[indices[current_index]]\n        indices_to_remove = []\n        for indx in min_dist_indices[current_index]:\n            # add to removal list indices of the same 3d point and same images in other 3d point\n            if(indx in point3d_indices or (descriptors[indx] == descriptors[current_index]).all()):\n                indices_to_remove.append(indx)\n\n        curr_desc = [x for x in min_dist_indices[current_index] if x not in indices_to_remove]\n        res_min_dist_indices.append(curr_desc)\n\n\n    return res_min_dist_indices\n\ndef get_hard_negatives(trainPhotoTourDataset, descriptors):\n\n    def create_indices(_labels):\n        inds = dict()\n        for idx, ind in enumerate(_labels):\n            if ind not in inds:\n                inds[ind] = []\n            inds[ind].append(idx)\n        return inds\n\n    labels = create_indices(trainPhotoTourDataset.labels)\n    indices = {}\n    for key, value in labels.iteritems():\n        for ind in value:\n            indices[ind] = key\n\n    print(\'getting closest indices .... \')\n    descriptors_min_dist, inidices = BuildKNNGraphByFAISS_GPU(descriptors, 12)\n\n    print(\'removing descriptors with same indices .... \')\n    descriptors_min_dist = remove_descriptors_with_same_index(descriptors_min_dist, indices, labels, descriptors)\n\n    return descriptors_min_dist\n\nif __name__ == \'__main__\':\n\n            LOG_DIR = args.log_dir + args.experiment_name\n            logger, file_logger = None, None\n            model = TNet()\n\n            if(args.enable_logging):\n                #logger = Logger(LOG_DIR)\n                file_logger = FileLogger(LOG_DIR)\n\n            test_dataset_names = copy.copy(dataset_names)\n            test_dataset_names.remove(args.training_set)\n\n            trainPhotoTourDataset, test_loaders = create_loaders()\n            main(trainPhotoTourDataset, test_loaders, model, logger, file_logger)\n'"
code/HardNetClassicalHardNegMiningSiftInit.py,29,"b'#!/usr/bin/python2 -utt\n#-*- coding: utf-8 -*-\n\n""""""\nThis is HardNet local patch descriptor. The training code is based on PyTorch TFeat implementation\nhttps://github.com/edgarriba/examples/tree/master/triplet\nby Edgar Riba.\n\nIf you use this code, please cite\n@article{HardNet2017,\n author = {Anastasiya Mishchuk, Dmytro Mishkin, Filip Radenovic, Jiri Matas},\n    title = ""{Working hard to know your neighbor\'s margins:Local descriptor learning loss}"",\n     year = 2017}\n(c) 2017 by Anastasiia Mishchuk, Dmytro Mishkin\n""""""\n\nfrom __future__ import division, print_function\nimport sys\nfrom copy import deepcopy\nimport argparse\nimport torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport os\nfrom tqdm import tqdm\nimport numpy as np\nimport random\nimport cv2\nimport copy\nfrom EvalMetrics import ErrorRateAt95Recall\nfrom Loggers import Logger, FileLogger\nfrom W1BS import w1bs_extract_descs_and_save\nfrom Utils import L2Norm, cv2_scale, np_reshape\nfrom Utils import str2bool\nimport torch.utils.data as data\nimport torch.utils.data as data_utils\nimport torch.nn.functional as F\nfrom Losses import loss_HardNet, loss_random_sampling, loss_L2Net\n\nimport faiss\n\n# Training settings\nparser = argparse.ArgumentParser(description=\'PyTorch HardNet\')\n# Model options\n\nparser.add_argument(\'--w1bsroot\', type=str,\n                    default=\'data/sets/wxbs-descriptors-benchmark/code/\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--dataroot\', type=str,\n                    default=\'data/sets/\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--enable-logging\',type=bool, default=True,\n                    help=\'folder to output model checkpoints\')\nparser.add_argument(\'--log-dir\', default=\'data/logs/\',\n                    help=\'folder to output model checkpoints\')\nparser.add_argument(\'--experiment-name\', default= \'liberty_train_hard_mining/\',\n                    help=\'experiment path\')\nparser.add_argument(\'--decor\',type=str2bool, default = False,\n                    help=\'L2Net decorrelation penalty\')\nparser.add_argument(\'--training-set\', default= \'liberty\',\n                    help=\'Other options: notredame, yosemite\')\nparser.add_argument(\'--num-workers\', default= 8,\n                    help=\'Number of workers to be created\')\nparser.add_argument(\'--pin-memory\',type=bool, default= True,\n                    help=\'\')\nparser.add_argument(\'--anchorave\', type=bool, default=False,\n                    help=\'anchorave\')\nparser.add_argument(\'--hardnegatives\', type=int, default=7,\n                    help=\'the height / width of the input image to network\')\nparser.add_argument(\'--imageSize\', type=int, default=32,\n                    help=\'the height / width of the input image to network\')\nparser.add_argument(\'--mean-image\', type=float, default=0.443728476019,\n                    help=\'mean of train dataset for normalization\')\nparser.add_argument(\'--std-image\', type=float, default=0.20197947209,\n                    help=\'std of train dataset for normalization\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--epochs\', type=int, default=10, metavar=\'E\',\n                    help=\'number of epochs to train (default: 10)\')\nparser.add_argument(\'--anchorswap\', type=bool, default=True,\n                    help=\'turns on anchor swap\')\nparser.add_argument(\'--batch-size\', type=int, default=1024, metavar=\'BS\',\n                    help=\'input batch size for training (default: 128)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=1024, metavar=\'BST\',\n                    help=\'input batch size for testing (default: 1000)\')\nparser.add_argument(\'--n-triplets\', type=int, default=5000000, metavar=\'N\',\n                    help=\'how many triplets will generate from the dataset\')\nparser.add_argument(\'--margin\', type=float, default=1.0, metavar=\'MARGIN\',\n                    help=\'the margin value for the triplet loss function (default: 1.0\')\nparser.add_argument(\'--act-decay\', type=float, default=0,\n                    help=\'activity L2 decay, default 0\')\nparser.add_argument(\'--lr\', type=float, default=0.1, metavar=\'LR\',\n                    help=\'learning rate (default: 0.1)\')\nparser.add_argument(\'--fliprot\', type=str2bool, default=False,\n                    help=\'turns on flip and 90deg rotation augmentation\')\nparser.add_argument(\'--lr-decay\', default=1e-6, type=float, metavar=\'LRD\',\n                    help=\'learning rate decay ratio (default: 1e-6\')\nparser.add_argument(\'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--optimizer\', default=\'sgd\', type=str,\n                    metavar=\'OPT\', help=\'The optimizer to use (default: SGD)\')\n# Device options\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'enables CUDA training\')\nparser.add_argument(\'--gpu-id\', default=\'0\', type=str,\n                    help=\'id(s) for CUDA_VISIBLE_DEVICES\')\nparser.add_argument(\'--seed\', type=int, default=0, metavar=\'S\',\n                    help=\'random seed (default: 0)\')\nparser.add_argument(\'--log-interval\', type=int, default=10, metavar=\'LI\',\n                    help=\'how many batches to wait before logging training status\')\n\nargs = parser.parse_args()\n\ndataset_names = [\'liberty\', \'notredame\', \'yosemite\']\n\n# check if path to w1bs dataset testing module exists\nif os.path.isdir(args.w1bsroot):\n    sys.path.insert(0, args.w1bsroot)\n    import utils.w1bs as w1bs\n    TEST_ON_W1BS = True\n\n# set the device to use by setting CUDA_VISIBLE_DEVICES env variable in\n# order to prevent any memory allocation on unused GPUs\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu_id\n\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nif args.cuda:\n    cudnn.benchmark = True\n    torch.cuda.manual_seed_all(args.seed)\n\nLOG_DIR = args.log_dir + args.experiment_name\n# create loggin directory\nif not os.path.exists(LOG_DIR):\n    os.makedirs(LOG_DIR)\n# set random seeds\ntorch.manual_seed(args.seed)\nnp.random.seed(args.seed)\n\n\nclass CorrelationPenaltyLoss(nn.Module):\n    def __init__(self):\n        super(CorrelationPenaltyLoss, self).__init__()\n\n    def forward(self, input):\n        mean1 = torch.mean(input, dim=0)\n        zeroed = input - mean1.expand_as(input)\n        cor_mat = torch.bmm(torch.t(zeroed).unsqueeze(0), zeroed.unsqueeze(0)).squeeze(0)\n        d = torch.diag(torch.diag(cor_mat))\n        no_diag = cor_mat - d\n        d_sq = no_diag * no_diag\n        return torch.sqrt(d_sq.sum()) / input.size(0)\n\n\nfrom matplotlib.pyplot import figure, imshow, axis\n\ndef showImagesHorizontally(images):\n    fig = figure()\n    for i in range(len(images)):\n        a=fig.add_subplot(1,len(images),i+1)\n        image = images[i]\n        imshow(image,cmap=\'Greys_r\')\n        axis(\'off\')\n\nclass TripletPhotoTour(dset.PhotoTour):\n    """"""From the PhotoTour Dataset it generates triplet samples\n    note: a triplet is composed by a pair of matching images and one of\n    different class.\n    """"""\n    def __init__(self, train=True, transform=None, batch_size = None, *arg, **kw):\n        super(TripletPhotoTour, self).__init__(*arg, **kw)\n        self.transform = transform\n\n        self.train = train\n        self.n_triplets = args.n_triplets\n        self.batch_size = batch_size\n\n        if self.train:\n            print(\'Generating {} triplets\'.format(self.n_triplets))\n            self.triplets = self.generate_triplets(self.labels, self.n_triplets)\n\n    @staticmethod\n    def generate_triplets(labels, num_triplets):\n        def create_indices(_labels):\n            inds = dict()\n            for idx, ind in enumerate(_labels):\n                if ind not in inds:\n                    inds[ind] = []\n                inds[ind].append(idx)\n            return inds\n\n        triplets = []\n        indices = create_indices(labels)\n        unique_labels = np.unique(labels.numpy())\n        n_classes = unique_labels.shape[0]\n        # add only unique indices in batch\n        already_idxs = set()\n\n        for x in tqdm(range(num_triplets)):\n            if len(already_idxs) >= args.batch_size:\n                already_idxs = set()\n            c1 = np.random.randint(0, n_classes - 1)\n            while c1 in already_idxs:\n                c1 = np.random.randint(0, n_classes - 1)\n            already_idxs.add(c1)\n            c2 = np.random.randint(0, n_classes - 1)\n            while c1 == c2:\n                c2 = np.random.randint(0, n_classes - 1)\n            if len(indices[c1]) == 2:  # hack to speed up process\n                n1, n2 = 0, 1\n            else:\n                n1 = np.random.randint(0, len(indices[c1]) - 1)\n                n2 = np.random.randint(0, len(indices[c1]) - 1)\n                while n1 == n2:\n                    n2 = np.random.randint(0, len(indices[c1]) - 1)\n            n3 = np.random.randint(0, len(indices[c2]) - 1)\n            triplets.append([indices[c1][n1], indices[c1][n2], indices[c2][n3]])\n        return torch.LongTensor(np.array(triplets))\n\n    def __getitem__(self, index):\n        def transform_img(img):\n            if self.transform is not None:\n                img = self.transform(img.numpy())\n            return img\n\n        if not self.train:\n            m = self.matches[index]\n            img1 = transform_img(self.data[m[0]])\n            img2 = transform_img(self.data[m[1]])\n            return img1, img2, m[2]\n\n        t = self.triplets[index]\n        a, p, n = self.data[t[0]], self.data[t[1]], self.data[t[2]]\n\n        img_a = transform_img(a)\n        img_p = transform_img(p)\n        img_n = transform_img(n)\n\n        # transform images if required\n        # if args.fliprot:\n        #     do_flip = random.random() > 0.5\n        #     do_rot = random.random() > 0.5\n        #\n        #     if do_rot:\n        #         img_a = img_a.permute(0,2,1)\n        #         img_p = img_p.permute(0,2,1)\n        #\n        #     if do_flip:\n        #         img_a = torch.from_numpy(deepcopy(img_a.numpy()[:,:,::-1]))\n        #         img_p = torch.from_numpy(deepcopy(img_p.numpy()[:,:,::-1]))\n\n        return img_a, img_p, img_n\n\n    def __len__(self):\n        if self.train:\n            return self.triplets.size(0)\n        else:\n            return self.matches.size(0)\n\nclass TripletPhotoTourHardNegatives(dset.PhotoTour):\n    """"""From the PhotoTour Dataset it generates triplet samples\n    note: a triplet is composed by a pair of matching images and one of\n    different class.\n    """"""\n    def __init__(self, negative_indices, train=True, transform=None, batch_size = None, *arg, **kw):\n        super(TripletPhotoTourHardNegatives, self).__init__(*arg, **kw)\n        self.transform = transform\n\n        self.train = train\n        self.n_triplets = args.n_triplets\n        self.negative_indices = negative_indices\n        self.batch_size = batch_size\n\n        if self.train:\n            print(\'Generating {} triplets\'.format(self.n_triplets))\n            self.triplets = self.generate_triplets(self.labels, self.n_triplets, self.negative_indices)\n\n\n    @staticmethod\n    def generate_triplets(labels, num_triplets, negative_indices):\n        def create_indices(_labels):\n            inds = dict()\n            for idx, ind in enumerate(_labels):\n                if ind not in inds:\n                    inds[ind] = []\n                inds[ind].append(idx)\n            return inds\n\n        triplets = []\n        indices = create_indices(labels)\n        unique_labels = np.unique(labels.numpy())\n        n_classes = unique_labels.shape[0]\n\n        # add only unique indices in batch\n        already_idxs = set()\n        count  = 0\n        for x in tqdm(range(num_triplets)):\n            if len(already_idxs) >= args.batch_size:\n                already_idxs = set()\n            c1 = np.random.randint(0, n_classes - 1)\n            while c1 in already_idxs:\n                c1 = np.random.randint(0, n_classes - 1)\n            already_idxs.add(c1)\n            if len(indices[c1]) == 2:  # hack to speed up process\n                n1, n2 = 0, 1\n            else:\n                n1 = np.random.randint(0, len(indices[c1]) - 1)\n                n2 = np.random.randint(0, len(indices[c1]) - 1)\n                while n1 == n2:\n                    n2 = np.random.randint(0, len(indices[c1]) - 1)\n            indx = indices[c1][n1]\n            if(len(negative_indices[indx])>0):\n\n                negative_indx = random.choice(negative_indices[indx])\n                negative_indices[indx].remove(negative_indx)\n\n                if(indx in negative_indices[negative_indx]):\n                    negative_indices[negative_indx].remove(indx)\n\n            else:\n                count+=1\n                c2 = np.random.randint(0, n_classes - 1)\n                while c1 == c2:\n                    c2 = np.random.randint(0, n_classes - 1)\n                n3 = np.random.randint(0, len(indices[c2]) - 1)\n                negative_indx = indices[c2][n3]\n\n            triplets.append([indices[c1][n1], indices[c1][n2], negative_indx])\n\n        print(count)\n        print(\'triplets are generated. amount of triplets: {}\'.format(len(triplets)))\n        return torch.LongTensor(np.array(triplets))\n\n\n    def __getitem__(self, index):\n        def transform_img(img):\n            if self.transform is not None:\n                img = self.transform(img.numpy())\n            return img\n\n        if not self.train:\n            m = self.matches[index]\n            img1 = transform_img(self.data[m[0]])\n            img2 = transform_img(self.data[m[1]])\n            return img1, img2, m[2]\n\n        t = self.triplets[index]\n        a, p, n = self.data[t[0]], self.data[t[1]], self.data[t[2]]\n\n        img_a = transform_img(a)\n        img_p = transform_img(p)\n        img_n = transform_img(n)\n\n        return img_a, img_p, img_n\n\n    def __len__(self):\n        if self.train:\n            return self.triplets.size(0)\n        else:\n            return self.matches.size(0)\n\nclass TNet(nn.Module):\n    """"""TFeat model definition\n    """"""\n    def __init__(self):\n        super(TNet, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2,padding=1),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Conv2d(128, 128, kernel_size=8),\n            nn.BatchNorm2d(128, affine=False),\n\n        )\n        self.features.apply(weights_init)\n\n    def forward(self, input):\n        flat = input.view(input.size(0), -1)\n        mp = torch.sum(flat, dim=1) / (32. * 32.)\n        sp = torch.std(flat, dim=1) + 1e-7\n        x_features = self.features(\n            (input - mp.unsqueeze(-1).unsqueeze(-1).expand_as(input)) / sp.unsqueeze(-1).unsqueeze(1).expand_as(input))\n        x = x_features.view(x_features.size(0), -1)\n        return L2Norm()(x)\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.orthogonal(m.weight.data, gain=0.7)\n        nn.init.constant(m.bias.data, 0.01)\n    if isinstance(m, nn.Linear):\n        nn.init.orthogonal(m.weight.data, gain=0.01)\n        nn.init.constant(m.bias.data, 0.)\n\ndef create_loaders():\n\n    test_dataset_names = copy.copy(dataset_names)\n    test_dataset_names.remove(args.training_set)\n\n    kwargs = {\'num_workers\': args.num_workers, \'pin_memory\': args.pin_memory} if args.cuda else {}\n\n    transform = transforms.Compose([\n            transforms.Lambda(cv2_scale),\n            transforms.Lambda(np_reshape),\n            transforms.ToTensor(),\n            transforms.Normalize((args.mean_image,), (args.std_image,))])\n\n    trainPhotoTourDataset =  TripletPhotoTour(train=True,\n                             batch_size=args.batch_size,\n                             root=args.dataroot,\n                             name=args.training_set,\n                             download=True,\n                             transform=transform)\n\n    test_loaders = [{\'name\': name,\n                     \'dataloader\': torch.utils.data.DataLoader(\n             TripletPhotoTour(train=False,\n                     batch_size=args.test_batch_size,\n                     root=args.dataroot,\n                     name=name,\n                     download=True,\n                     transform=transform),\n                        batch_size=args.test_batch_size,\n                        shuffle=False, **kwargs)}\n                    for name in test_dataset_names]\n\n    return trainPhotoTourDataset, test_loaders\n\ndef train(train_loader, model, optimizer, epoch, logger):\n    # switch to train mode\n    model.train()\n    pbar = tqdm(enumerate(train_loader))\n    for batch_idx, (data_a, data_p, data_n) in pbar:\n\n        if args.cuda:\n            data_a, data_p, data_n = data_a.cuda(), data_p.cuda(), data_n.cuda()\n\n        data_a, data_p, data_n = Variable(data_a), Variable(data_p), Variable(data_n)\n\n        out_a, out_p, out_n = model(data_a), model(data_p), model(data_n)\n\n        #hardnet loss\n        loss = loss_random_sampling(out_a, out_p, out_n, margin=args.margin)\n\n        if args.decor:\n            loss += CorrelationPenaltyLoss()(out_a)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        adjust_learning_rate(optimizer)\n        if(logger!=None):\n         logger.log_value(\'loss\', loss.data[0]).step()\n\n        if batch_idx % args.log_interval == 0:\n            pbar.set_description(\n                \'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                    epoch, batch_idx * len(data_a), len(train_loader.dataset),\n                           100. * batch_idx / len(train_loader),\n                    loss.data[0]))\n\n    torch.save({\'epoch\': epoch + 1, \'state_dict\': model.state_dict()},\n               \'{}/checkpoint_{}.pth\'.format(LOG_DIR, epoch))\n\ndef test(test_loader, model, epoch, logger, logger_test_name):\n    # switch to evaluate mode\n    model.eval()\n\n    labels, distances = [], []\n\n    pbar = tqdm(enumerate(test_loader))\n    for batch_idx, (data_a, data_p, label) in pbar:\n\n        if args.cuda:\n            data_a, data_p = data_a.cuda(), data_p.cuda()\n\n        data_a, data_p, label = Variable(data_a, volatile=True), \\\n                                Variable(data_p, volatile=True), Variable(label)\n\n        out_a, out_p = model(data_a), model(data_p)\n        dists = torch.sqrt(torch.sum((out_a - out_p) ** 2, 1))  # euclidean distance\n        distances.append(dists.data.cpu().numpy())\n        ll = label.data.cpu().numpy().reshape(-1, 1)\n        labels.append(ll)\n\n        if batch_idx % args.log_interval == 0:\n            pbar.set_description(logger_test_name+\' Test Epoch: {} [{}/{} ({:.0f}%)]\'.format(\n                epoch, batch_idx * len(data_a), len(test_loader.dataset),\n                       100. * batch_idx / len(test_loader)))\n\n    num_tests = test_loader.dataset.matches.size(0)\n    labels = np.vstack(labels).reshape(num_tests)\n    distances = np.vstack(distances).reshape(num_tests)\n\n    fpr95 = ErrorRateAt95Recall(labels, 1.0 / (distances + 1e-8))\n    print(\'\\33[91mTest set: Accuracy(FPR95): {:.8f}\\n\\33[0m\'.format(fpr95))\n\n    if (args.enable_logging):\n        if(logger!=None):\n            logger.log_value(logger_test_name+\' fpr95\', fpr95)\n    return\n\ndef adjust_learning_rate(optimizer):\n    """"""Updates the learning rate given the learning rate decay.\n    The routine has been implemented according to the original Lua SGD optimizer\n    """"""\n    for group in optimizer.param_groups:\n        if \'step\' not in group:\n            group[\'step\'] = 0.\n        else:\n            group[\'step\'] += 1.\n        group[\'lr\'] = args.lr * (\n        1.0 - float(group[\'step\']) * float(args.batch_size) / (args.n_triplets * float(args.epochs)))\n    return\n\ndef create_optimizer(model, new_lr):\n    # setup optimizer\n    if args.optimizer == \'sgd\':\n        optimizer = optim.SGD(model.parameters(), lr=new_lr,\n                              momentum=0.9, dampening=0.9,\n                              weight_decay=args.wd)\n    elif args.optimizer == \'adam\':\n        optimizer = optim.Adam(model.parameters(), lr=new_lr,\n                               weight_decay=args.wd)\n    else:\n        raise Exception(\'Not supported optimizer: {0}\'.format(args.optimizer))\n    return optimizer\n\n\ndef main(trainPhotoTourDataset, test_loaders, model, logger, file_logger):\n    # print the experiment configuration\n    print(\'\\nparsed options:\\n{}\\n\'.format(vars(args)))\n\n    if (args.enable_logging):\n        file_logger.log_string(\'logs.txt\', \'\\nparsed options:\\n{}\\n\'.format(vars(args)))\n\n    if args.cuda:\n        model.cuda()\n\n    optimizer1 = create_optimizer(model.features, args.lr)\n\n    # optionally resume from a checkpoint\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(\'=> loading checkpoint {}\'.format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            checkpoint = torch.load(args.resume)\n            model.load_state_dict(checkpoint[\'state_dict\'])\n        else:\n            print(\'=> no checkpoint found at {}\'.format(args.resume))\n\n    start = args.start_epoch\n    end = start + args.epochs\n\n    kwargs = {\'num_workers\': args.num_workers, \'pin_memory\': args.pin_memory} if args.cuda else {}\n\n    transform = transforms.Compose([\n            transforms.Lambda(cv2_scale),\n            transforms.Lambda(np_reshape),\n            transforms.ToTensor(),\n            transforms.Normalize((args.mean_image,), (args.std_image,))])\n\n    first_init = False\n\n    for epoch in range(start, end):\n\n        model.eval()\n\n        if(not first_init):\n\n            descriptors = pre_init_with_sift(trainPhotoTourDataset)\n            np.save(\'descriptors_sift.npy\', descriptors)\n            descriptors = np.load(\'descriptors_sift.npy\')\n\n            hard_negatives = get_hard_negatives(trainPhotoTourDataset, descriptors)\n            np.save(\'descriptors_min_dist_sift.npy\', hard_negatives)\n            hard_negatives = np.load(\'descriptors_min_dist_sift.npy\')\n\n            first_init = True\n\n        else:\n            # #\n            descriptors = get_descriptors_for_dataset(model, trainPhotoTourDataset)\n                # #\n            np.save(\'descriptors.npy\', descriptors)\n            descriptors = np.load(\'descriptors.npy\')\n                #\n            hard_negatives = get_hard_negatives(trainPhotoTourDataset, descriptors)\n            np.save(\'descriptors_min_dist.npy\', hard_negatives)\n            hard_negatives = np.load(\'descriptors_min_dist.npy\')\n\n        trainPhotoTourDatasetWithHardNegatives = TripletPhotoTourHardNegatives(train=True,\n                                                              negative_indices=hard_negatives,\n                                                              batch_size=args.batch_size,\n                                                              root=args.dataroot,\n                                                              name=args.training_set,\n                                                              download=True,\n                                                              transform=transform)\n\n        train_loader = torch.utils.data.DataLoader(trainPhotoTourDatasetWithHardNegatives,\n                                                   batch_size=args.batch_size,\n                                                   shuffle=True, **kwargs)\n\n        train(train_loader, model, optimizer1, epoch, logger)\n\n        # iterate over test loaders and test results\n        for test_loader in test_loaders:\n            test(test_loader[\'dataloader\'], model, epoch, logger, test_loader[\'name\'])\n\n        if TEST_ON_W1BS :\n            # print(weights_path)\n            patch_images = w1bs.get_list_of_patch_images(\n                DATASET_DIR=args.w1bsroot.replace(\'/code\', \'/data/W1BS\'))\n            desc_name = \'curr_desc\'\n\n            for img_fname in patch_images:\n                w1bs_extract_descs_and_save(img_fname, model, desc_name, cuda = args.cuda,\n                                            mean_img=args.mean_image,\n                                            std_img=args.std_image)\n\n            DESCS_DIR = args.w1bsroot.replace(\'/code\', ""/data/out_descriptors"")\n            OUT_DIR = args.w1bsroot.replace(\'/code\', ""/data/out_graphs"")\n\n            force_rewrite_list = [desc_name]\n            w1bs.match_descriptors_and_save_results(DESC_DIR=DESCS_DIR, do_rewrite=True,\n                                                    dist_dict={},\n                                                    force_rewrite_list=force_rewrite_list)\n            if(args.enable_logging):\n                w1bs.draw_and_save_plots_with_loggers(DESC_DIR=DESCS_DIR, OUT_DIR=OUT_DIR,\n                                         methods=[""SNN_ratio""],\n                                         descs_to_draw=[desc_name],\n                                         logger=file_logger,\n                                         tensor_logger = None)\n            else:\n                w1bs.draw_and_save_plots(DESC_DIR=DESCS_DIR, OUT_DIR=OUT_DIR,\n                                         methods=[""SNN_ratio""],\n                                         descs_to_draw=[desc_name],\n                                         really_draw = False)\n\n\nclass PhototourTrainingData(data.Dataset):\n\n    def __init__(self, data):\n        self.data_files = data\n\n    def __getitem__(self, item):\n        res = self.data_files[item]\n        return res\n\n    def __len__(self):\n        return len(self.data_files)\n\ndef BuildKNNGraphByFAISS_GPU(db,k):\n    dbsize, dim = db.shape\n    flat_config = faiss.GpuIndexFlatConfig()\n    flat_config.device = 0\n    res = faiss.StandardGpuResources()\n    nn = faiss.GpuIndexFlatL2(res, dim, flat_config)\n    nn.add(db)\n    dists,idx = nn.search(db, k+1)\n    return idx[:,1:],dists[:,1:]\n\nfrom pytorch_sift import SIFTNet\n\ndef pre_init_with_sift(trainPhotoTourDataset):\n\n    patch_size = 65\n    ON_GPU = True\n    SIFT = SIFTNet(patch_size=patch_size, do_cuda=ON_GPU)\n    SIFT.eval()\n\n    if ON_GPU:\n        SIFT.cuda()\n\n    transformed = []\n    for img in trainPhotoTourDataset.data:\n        transformed.append(np.expand_dims(cv2.resize(img.cpu().numpy(), (65,65)), axis=0))\n\n    phototour_loader = data_utils.DataLoader(PhototourTrainingData(transformed), batch_size=256, shuffle=False)\n    descriptors = []\n\n    pbar = tqdm(enumerate(phototour_loader))\n    for batch_idx, data_a in pbar:\n\n        if ON_GPU:\n            torch_patches = Variable(data_a.type(torch.FloatTensor).cuda(), volatile=True)\n        else:\n            torch_patches = Variable(data_a.type(torch.FloatTensor), volatile=True)\n\n        res = SIFT(torch_patches)\n        sift = np.round(512. * res.data.cpu().numpy()).astype(np.float32)\n        descriptors.extend(sift)\n\n    return np.array(descriptors)\n\n\ndef get_descriptors_for_dataset(model, trainPhotoTourDataset):\n    model.eval()\n    transformed = []\n\n    for img in trainPhotoTourDataset.data:\n        transformed.append(trainPhotoTourDataset.transform(img.numpy()))\n    print(len(transformed))\n    phototour_loader = data_utils.DataLoader(PhototourTrainingData(transformed), batch_size=128, shuffle=False)\n    descriptors = []\n    pbar = tqdm(enumerate(phototour_loader))\n    for batch_idx, data_a in pbar:\n\n        if args.cuda:\n            model.cuda()\n            data_a = data_a.cuda()\n\n        data_a = Variable(data_a, volatile=True),\n        out_a = model(data_a[0])\n        descriptors.extend(out_a.data.cpu().numpy())\n\n    return descriptors\n\n\ndef remove_descriptors_with_same_index(min_dist_indices, indices, labels, descriptors):\n\n    res_min_dist_indices = []\n\n    for current_index in range(0, len(min_dist_indices)):\n        # get indices of the same 3d points\n        point3d_indices = labels[indices[current_index]]\n        indices_to_remove = []\n        for indx in min_dist_indices[current_index]:\n            # add to removal list indices of the same 3d point and same images in other 3d point\n            if(indx in point3d_indices or (descriptors[indx] == descriptors[current_index]).all()):\n                indices_to_remove.append(indx)\n\n            # check if 3dpoint indices equals to any of min dist indices\n            for point3d_indx in point3d_indices:\n                if ((descriptors[point3d_indx]==descriptors[indx]).all()):\n                    indices_to_remove.append(indx)\n\n        curr_desc = [x for x in min_dist_indices[current_index] if x not in indices_to_remove]\n        res_min_dist_indices.append(curr_desc)\n\n\n    return res_min_dist_indices\n\n\ndef get_hard_negatives(trainPhotoTourDataset, descriptors):\n\n    def create_indices(_labels):\n        inds = dict()\n        for idx, ind in enumerate(_labels):\n            if ind not in inds:\n                inds[ind] = []\n            inds[ind].append(idx)\n        return inds\n\n    labels = create_indices(trainPhotoTourDataset.labels)\n    indices = {}\n    for key, value in labels.iteritems():\n        for ind in value:\n            indices[ind] = key\n\n    print(\'getting closest indices .... \')\n    descriptors_min_dist, inidices = BuildKNNGraphByFAISS_GPU(descriptors, args.hardnegatives)\n    print(descriptors_min_dist[0])\n\n    print(\'removing descriptors with same indices .... \')\n    descriptors_min_dist = remove_descriptors_with_same_index(descriptors_min_dist, indices, labels, descriptors)\n    print(descriptors_min_dist[0])\n\n    return descriptors_min_dist\n\nif __name__ == \'__main__\':\n\n            LOG_DIR = args.log_dir + args.experiment_name\n            logger, file_logger = None, None\n            model = TNet()\n\n            if(args.enable_logging):\n                #logger = Logger(LOG_DIR)\n                file_logger = FileLogger(LOG_DIR)\n\n            test_dataset_names = copy.copy(dataset_names)\n            test_dataset_names.remove(args.training_set)\n\n            trainPhotoTourDataset, test_loaders = create_loaders()\n            main(trainPhotoTourDataset, test_loaders, model, logger, file_logger)\n'"
code/HardNetHPatchesSplits.py,32,"b'#!/usr/bin/python2 utt\n# -*- coding: utf-8 -*-\n""""""\nThis is HardNet local patch descriptor. The training code is based on PyTorch TFeat implementation\nhttps://github.com/edgarriba/examples/tree/master/triplet\nby Edgar Riba.\n\nIf you use this code, please cite\n@article{HardNet2017,\n author = {Anastasiya Mishchuk, Dmytro Mishkin, Filip Radenovic, Jiri Matas},\n    title = ""{Working hard to know your neighbor\'s margins:Local descriptor learning loss}"",\n     year = 2017}\n(c) 2017 by Anastasiia Mishchuk, Dmytro Mishkin\n""""""\n\nfrom __future__ import division, print_function\nimport sys\nfrom copy import deepcopy\nimport argparse\nimport torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport os\nfrom tqdm import tqdm\nimport numpy as np\nimport random\nimport cv2\nimport PIL\nimport math\nimport copy\nfrom EvalMetrics import ErrorRateAt95Recall#, ErrorRateFDRAt95Recall, convertFDR2FPR, convertFPR2FDR\nfrom Losses import loss_HardNet, loss_random_sampling, loss_L2Net, global_orthogonal_regularization\nfrom W1BS import w1bs_extract_descs_and_save\nfrom Utils import L2Norm, cv2_scale, np_reshape\nfrom Utils import str2bool\nimport torch.nn as nn\nimport torch.utils.data as data\n\n\nclass CorrelationPenaltyLoss(nn.Module):\n    def __init__(self):\n        super(CorrelationPenaltyLoss, self).__init__()\n\n    def forward(self, input):\n        mean1 = torch.mean(input, dim=0)\n        zeroed = input - mean1.expand_as(input)\n        cor_mat = torch.bmm(torch.t(zeroed).unsqueeze(0), zeroed.unsqueeze(0)).squeeze(0)\n        d = torch.diag(torch.diag(cor_mat))\n        no_diag = cor_mat - d\n        d_sq = no_diag * no_diag\n        return torch.sqrt(d_sq.sum()) / input.size(0)\n\n\n# Training settings\nparser = argparse.ArgumentParser(description=\'PyTorch HardNet\')\n# Model options\n\nparser.add_argument(\'--w1bsroot\', type=str,\n                    default=\'data/sets/wxbs-descriptors-benchmark/code/\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--hpatches-split\', type=str,\n                    default=\'data/sets/\',\n                    help=\'path to HPatches split generated by HPatchesDatasetCreator\')\nparser.add_argument(\'--dataroot\', type=str,\n                    default=\'data/sets/\',\n                    help=\'path to Brown datasets for testing\')\nparser.add_argument(\'--enable-logging\', type=str2bool, default=False,\n                    help=\'output to tensorlogger\')\nparser.add_argument(\'--log-dir\', default=\'data/logs/\',\n                    help=\'folder to output log\')\nparser.add_argument(\'--model-dir\', default=\'data/models/\',\n                    help=\'folder to output model checkpoints\')\nparser.add_argument(\'--experiment-name\', default=\'/multiple_datasets_all/\',\n                    help=\'experiment path\')\nparser.add_argument(\'--training-set\', default=\'all\',\n                    help=\'Other options: notredame, yosemite\')\nparser.add_argument(\'--loss\', default=\'triplet_margin\',\n                    help=\'Other options: softmax, contrastive\')\nparser.add_argument(\'--batch-reduce\', default=\'min\',\n                    help=\'Other options: average, random, random_global, L2Net\')\nparser.add_argument(\'--num-workers\', default=1, type=int,\n                    help=\'Number of workers to be created\')\nparser.add_argument(\'--pin-memory\', type=bool, default=True,\n                    help=\'\')\nparser.add_argument(\'--decor\', type=str2bool, default=False,\n                    help=\'L2Net decorrelation penalty\')\nparser.add_argument(\'--anchorave\', type=str2bool, default=False,\n                    help=\'anchorave\')\nparser.add_argument(\'--imageSize\', type=int, default=32,\n                    help=\'the height / width of the input image to network\')\nparser.add_argument(\'--mean-image\', type=float, default=0.443728476019,\n                    help=\'mean of train dataset for normalization\')\nparser.add_argument(\'--std-image\', type=float, default=0.20197947209,\n                    help=\'std of train dataset for normalization\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--epochs\', type=int, default=10, metavar=\'E\',\n                    help=\'number of epochs to train (default: 10)\')\nparser.add_argument(\'--anchorswap\', type=bool, default=True,\n                    help=\'turns on anchor swap\')\nparser.add_argument(\'--batch-size\', type=int, default=1024, metavar=\'BS\',\n                    help=\'input batch size for training (default: 1024)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=1024, metavar=\'BST\',\n                    help=\'input batch size for testing (default: 1024)\')\nparser.add_argument(\'--n-triplets\', type=int, default=15000000, metavar=\'N\',\n                    help=\'how many triplets will generate from the dataset\')\nparser.add_argument(\'--margin\', type=float, default=1.0, metavar=\'MARGIN\',\n                    help=\'the margin value for the triplet loss function (default: 1.0\')\nparser.add_argument(\'--gor\', type=str2bool, default=False,\n                    help=\'use gor\')\nparser.add_argument(\'--alpha\', type=float, default=1.0, metavar=\'ALPHA\',\n                    help=\'gor parameter\')\nparser.add_argument(\'--act-decay\', type=float, default=0,\n                    help=\'activity L2 decay, default 0\')\nparser.add_argument(\'--lr\', type=float, default=10.0, metavar=\'LR\',\n                    help=\'learning rate (default: 10.0)\')\nparser.add_argument(\'--fliprot\', type=str2bool, default=True,\n                    help=\'turns on flip and 90deg rotation augmentation\')\nparser.add_argument(\'--augmentation\', type=str2bool, default=False,\n                    help=\'turns on shift and small scale rotation augmentation\')\nparser.add_argument(\'--lr-decay\', default=1e-6, type=float, metavar=\'LRD\',\n                    help=\'learning rate decay ratio (default: 1e-6\')\nparser.add_argument(\'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--optimizer\', default=\'sgd\', type=str,\n                    metavar=\'OPT\', help=\'The optimizer to use (default: SGD)\')\n# Device options\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'enables CUDA training\')\nparser.add_argument(\'--gpu-id\', default=\'0\', type=str,\n                    help=\'id(s) for CUDA_VISIBLE_DEVICES\')\nparser.add_argument(\'--seed\', type=int, default=0, metavar=\'S\',\n                    help=\'random seed (default: 0)\')\nparser.add_argument(\'--log-interval\', type=int, default=10, metavar=\'LI\',\n                    help=\'how many batches to wait before logging training status\')\n\nargs = parser.parse_args()\n\nsuffix = \'{}_{}\'.format(args.training_set, args.batch_reduce)\n\nif args.gor:\n    suffix = suffix + \'_gor_alpha{:1.1f}\'.format(args.alpha)\nif args.anchorswap:\n    suffix = suffix + \'_as\'\nif args.anchorave:\n    suffix = suffix + \'_av\'\n\ntriplet_flag = (args.batch_reduce == \'random_global\') or args.gor\n\ndataset_names = [\'liberty\', \'notredame\', \'yosemite\']\n\nTEST_ON_W1BS = False\n# check if path to w1bs dataset testing module exists\nif os.path.isdir(args.w1bsroot):\n    sys.path.insert(0, args.w1bsroot)\n    import utils.w1bs as w1bs\n\n    TEST_ON_W1BS = True\n\n# set the device to use by setting CUDA_VISIBLE_DEVICES env variable in\n# order to prevent any memory allocation on unused GPUs\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu_id\n\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nif args.cuda:\n    cudnn.benchmark = True\n    torch.cuda.manual_seed_all(args.seed)\n\n# create loggin directory\nif not os.path.exists(args.log_dir):\n    os.makedirs(args.log_dir)\n\n# set random seeds\ntorch.manual_seed(args.seed)\nnp.random.seed(args.seed)\n\nclass TotalDatasetsLoader(data.Dataset):\n\n    def __init__(self, datasets_path, train = True, transform = None, batch_size = None, n_triplets = 5000000, fliprot = False, *arg, **kw):\n        super(TotalDatasetsLoader, self).__init__()\n        #datasets_path = [os.path.join(datasets_path, dataset) for dataset in os.listdir(datasets_path) if \'.pt\' in dataset]\n        datasets_path = [datasets_path]\n        datasets = [torch.load(dataset) for dataset in datasets_path]\n        print (datasets_path)\n        data, labels = datasets[0][0], datasets[0][1]\n\n        for i in range(1,len(datasets)):\n            data = torch.cat([data,datasets[i][0]])\n            labels = torch.cat([labels, datasets[i][1]+torch.max(labels)+1])\n\n        del datasets\n\n        self.data, self.labels = data, labels\n        self.transform = transform\n        self.train = train\n        self.n_triplets = n_triplets\n        self.batch_size = batch_size\n        self.fliprot = fliprot\n        if self.train:\n                print(\'Generating {} triplets\'.format(self.n_triplets))\n                self.triplets = self.generate_triplets(self.labels, self.n_triplets, self.batch_size)\n\n    @staticmethod\n    def generate_triplets(labels, num_triplets, batch_size):\n            def create_indices(_labels):\n                inds = dict()\n                for idx, ind in enumerate(_labels):\n                    if ind not in inds:\n                        inds[ind] = []\n                    inds[ind].append(idx)\n                return inds\n\n            triplets = []\n            indices = create_indices(labels.numpy())\n            unique_labels = np.unique(labels.numpy())\n            n_classes = unique_labels.shape[0]\n            # add only unique indices in batch\n            already_idxs = set()\n\n            for x in tqdm(range(num_triplets)):\n                if len(already_idxs) >= batch_size:\n                    already_idxs = set()\n                c1 = np.random.randint(0, n_classes)\n                while c1 in already_idxs:\n                    c1 = np.random.randint(0, n_classes)\n                already_idxs.add(c1)\n                c2 = np.random.randint(0, n_classes)\n                while c1 == c2:\n                    c2 = np.random.randint(0, n_classes)\n                if len(indices[c1]) == 2:  # hack to speed up process\n                    n1, n2 = 0, 1\n                else:\n                    n1 = np.random.randint(0, len(indices[c1]))\n                    n2 = np.random.randint(0, len(indices[c1]))\n                    while n1 == n2:\n                        n2 = np.random.randint(0, len(indices[c1]))\n                n3 = np.random.randint(0, len(indices[c2]))\n                triplets.append([indices[c1][n1], indices[c1][n2], indices[c2][n3]])\n            return torch.LongTensor(np.array(triplets))\n\n    def __getitem__(self, index):\n            def transform_img(img):\n                if self.transform is not None:\n                    img = self.transform(img.numpy())\n                return img\n\n            t = self.triplets[index]\n            a, p, n = self.data[t[0]], self.data[t[1]], self.data[t[2]]\n\n            img_a = transform_img(a)\n            img_p = transform_img(p)\n\n            # transform images if required\n            if self.fliprot:\n                do_flip = random.random() > 0.5\n                do_rot = random.random() > 0.5\n\n                if do_rot:\n                    img_a = img_a.permute(0,2,1)\n                    img_p = img_p.permute(0,2,1)\n\n                if do_flip:\n                    img_a = torch.from_numpy(deepcopy(img_a.numpy()[:,:,::-1]))\n                    img_p = torch.from_numpy(deepcopy(img_p.numpy()[:,:,::-1]))\n            return img_a, img_p\n\n    def __len__(self):\n            if self.train:\n                return self.triplets.size(0)\n\nclass TripletPhotoTour(dset.PhotoTour):\n    """"""\n    From the PhotoTour Dataset it generates triplet samples\n    note: a triplet is composed by a pair of matching images and one of\n    different class.\n    """"""\n    def __init__(self, train=True, transform=None, n_triplets = 1000, batch_size=None, load_random_triplets=False, *arg, **kw):\n        super(TripletPhotoTour, self).__init__(*arg, **kw)\n        self.transform = transform\n        self.out_triplets = load_random_triplets\n        self.train = train\n        self.n_triplets = n_triplets\n        self.batch_size = batch_size\n        if self.train:\n            print(\'Generating {} triplets\'.format(self.n_triplets))\n            self.triplets = self.generate_triplets(self.labels, self.n_triplets)\n\n    @staticmethod\n    def generate_triplets(labels, num_triplets):\n        def create_indices(_labels):\n            inds = dict()\n            for idx, ind in enumerate(_labels):\n                if ind not in inds:\n                    inds[ind] = []\n                inds[ind].append(idx)\n            return inds\n\n        triplets = []\n        indices = create_indices(labels.numpy())\n        unique_labels = np.unique(labels.numpy())\n        n_classes = unique_labels.shape[0]\n        # add only unique indices in batch\n        already_idxs = set()\n\n        for x in tqdm(range(num_triplets)):\n            if len(already_idxs) >= args.batch_size:\n                already_idxs = set()\n            c1 = np.random.randint(0, n_classes - 1)\n            while c1 in already_idxs:\n                c1 = np.random.randint(0, n_classes - 1)\n            already_idxs.add(c1)\n            c2 = np.random.randint(0, n_classes - 1)\n            while c1 == c2:\n                c2 = np.random.randint(0, n_classes - 1)\n            if len(indices[c1]) == 2:  # hack to speed up process\n                n1, n2 = 0, 1\n            else:\n                n1 = np.random.randint(0, len(indices[c1]) - 1)\n                n2 = np.random.randint(0, len(indices[c1]) - 1)\n                while n1 == n2:\n                    n2 = np.random.randint(0, len(indices[c1]) - 1)\n            n3 = np.random.randint(0, len(indices[c2]) - 1)\n            triplets.append([indices[c1][n1], indices[c1][n2], indices[c2][n3]])\n        return torch.LongTensor(np.array(triplets))\n\n    def __getitem__(self, index):\n        def transform_img(img):\n            if self.transform is not None:\n                img = self.transform(img.numpy())\n            return img\n\n        if not self.train:\n            m = self.matches[index]\n            img1 = transform_img(self.data[m[0]])\n            img2 = transform_img(self.data[m[1]])\n            return img1, img2, m[2]\n\n        t = self.triplets[index]\n        a, p, n = self.data[t[0]], self.data[t[1]], self.data[t[2]]\n\n        img_a = transform_img(a)\n        img_p = transform_img(p)\n        img_n = None\n        if self.out_triplets:\n            img_n = transform_img(n)\n        # transform images if required\n        if args.fliprot:\n            do_flip = random.random() > 0.5\n            do_rot = random.random() > 0.5\n            if do_rot:\n                img_a = img_a.permute(0, 2, 1)\n                img_p = img_p.permute(0, 2, 1)\n                if self.out_triplets:\n                    img_n = img_n.permute(0, 2, 1)\n            if do_flip:\n                img_a = torch.from_numpy(deepcopy(img_a.numpy()[:, :, ::-1]))\n                img_p = torch.from_numpy(deepcopy(img_p.numpy()[:, :, ::-1]))\n                if self.out_triplets:\n                    img_n = torch.from_numpy(deepcopy(img_n.numpy()[:, :, ::-1]))\n        if self.out_triplets:\n            return (img_a, img_p, img_n)\n        else:\n            return (img_a, img_p)\n\n    def __len__(self):\n        if self.train:\n            return self.triplets.size(0)\n        else:\n            return self.matches.size(0)\n\n\nclass HardNet(nn.Module):\n    """"""HardNet model definition\n    """"""\n\n    def __init__(self):\n        super(HardNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Conv2d(128, 128, kernel_size=8, bias=False),\n            nn.BatchNorm2d(128, affine=False),\n        )\n        self.features.apply(weights_init)\n        return\n\n    def input_norm(self, x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(\n            -1).unsqueeze(1).expand_as(x)\n\n    def forward(self, input):\n        x_features = self.features(self.input_norm(input))\n        x = x_features.view(x_features.size(0), -1)\n        return L2Norm()(x)\n\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.orthogonal(m.weight.data, gain=0.6)\n        try:\n            nn.init.constant(m.bias.data, 0.01)\n        except:\n            pass\n    return\n\n\ndef create_loaders(load_random_triplets=False):\n    test_dataset_names = copy.copy(dataset_names)\n    #test_dataset_names.remove(args.training_set)\n    kwargs = {\'num_workers\': args.num_workers, \'pin_memory\': args.pin_memory} if args.cuda else {}\n    np_reshape64 = lambda x: np.reshape(x, (64, 64, 1))\n    transform_test = transforms.Compose([\n            transforms.Lambda(np_reshape64),\n            transforms.ToPILImage(),\n            transforms.Resize(32),\n            transforms.ToTensor()])\n    transform_train = transforms.Compose([\n            transforms.Lambda(np_reshape64),\n            transforms.ToPILImage(),\n            transforms.RandomRotation(5,PIL.Image.BILINEAR),\n            transforms.RandomResizedCrop(32, scale = (0.9,1.0),ratio = (0.9,1.1)),\n            transforms.Resize(32),\n            transforms.ToTensor()])\n    transform = transforms.Compose([\n            transforms.Lambda(cv2_scale),\n            transforms.Lambda(np_reshape),\n            transforms.ToTensor(),\n            transforms.Normalize((args.mean_image,), (args.std_image,))])\n    if not args.augmentation:\n        transform_train = transform\n        transform_test = transform\n\n    train_loader = torch.utils.data.DataLoader(\n        TotalDatasetsLoader(train=True,\n                         load_random_triplets=load_random_triplets,\n                         batch_size=args.batch_size,\n                         datasets_path=args.hpatches_split,\n                         fliprot=args.fliprot,\n                         n_triplets=args.n_triplets,\n                         name=args.training_set,\n                         download=True,\n                         transform=transform_train),\n        batch_size=args.batch_size,\n        shuffle=False, **kwargs)\n\n    test_loaders = [{\'name\': name,\n                     \'dataloader\': torch.utils.data.DataLoader(\n                         TripletPhotoTour(train=False,\n                                          batch_size=args.test_batch_size,\n                                          n_triplets = 1000,\n                                          root=args.dataroot,\n                                          name=name,\n                                          download=True,\n                                          transform=transform_test),\n                         batch_size=args.test_batch_size,\n                         shuffle=False, **kwargs)}\n                    for name in test_dataset_names]\n\n    return train_loader, test_loaders\n\n\ndef train(train_loader, model, optimizer, epoch, logger, load_triplets=False):\n    # switch to train mode\n    model.train()\n    pbar = tqdm(enumerate(train_loader))\n    for batch_idx, data in pbar:\n        if load_triplets:\n            data_a, data_p, data_n = data\n        else:\n            data_a, data_p = data\n\n        if args.cuda:\n            data_a, data_p = data_a.cuda(), data_p.cuda()\n            data_a, data_p = Variable(data_a), Variable(data_p)\n            out_a = model(data_a)\n            out_p = model(data_p)\n\n        if load_triplets:\n            data_n = data_n.cuda()\n            data_n = Variable(data_n)\n            out_n = model(data_n)\n\n        if args.batch_reduce == \'L2Net\':\n            loss = loss_L2Net(out_a, out_p, anchor_swap=args.anchorswap,\n                              margin=args.margin, loss_type=args.loss)\n        elif args.batch_reduce == \'random_global\':\n            loss = loss_random_sampling(out_a, out_p, out_n,\n                                        margin=args.margin,\n                                        anchor_swap=args.anchorswap,\n                                        loss_type=args.loss)\n        else:\n            loss = loss_HardNet(out_a, out_p,\n                                margin=args.margin,\n                                anchor_swap=args.anchorswap,\n                                anchor_ave=args.anchorave,\n                                batch_reduce=args.batch_reduce,\n                                loss_type=args.loss)\n\n        if args.decor:\n            loss += CorrelationPenaltyLoss()(out_a)\n\n        if args.gor:\n            loss += args.alpha * global_orthogonal_regularization(out_a, out_n)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        adjust_learning_rate(optimizer)\n        if batch_idx % args.log_interval == 0:\n            pbar.set_description(\n                \'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                    epoch, batch_idx * len(data_a), len(train_loader.dataset),\n                           100. * batch_idx / len(train_loader),\n                    loss.data[0]))\n    if (args.enable_logging):\n        logger.log_value(\'loss\', loss.data[0]).step()\n    try:\n        os.stat(\'{}{}\'.format(args.model_dir, suffix))\n    except:\n        os.makedirs(\'{}{}\'.format(args.model_dir, suffix))\n\n    torch.save({\'epoch\': epoch + 1, \'state_dict\': model.state_dict()},\n               \'{}{}/checkpoint_{}.pth\'.format(args.model_dir, suffix, epoch))\n\n\ndef test(test_loader, model, epoch, logger, logger_test_name):\n    # switch to evaluate mode\n    model.eval()\n\n    labels, distances = [], []\n\n    pbar = tqdm(enumerate(test_loader))\n    for batch_idx, (data_a, data_p, label) in pbar:\n\n        if args.cuda:\n            data_a, data_p = data_a.cuda(), data_p.cuda()\n\n        data_a, data_p, label = Variable(data_a, volatile=True), \\\n                                Variable(data_p, volatile=True), Variable(label)\n\n        out_a = model(data_a)\n        out_p = model(data_p)\n        dists = torch.sqrt(torch.sum((out_a - out_p) ** 2, 1))  # euclidean distance\n        distances.append(dists.data.cpu().numpy().reshape(-1, 1))\n        ll = label.data.cpu().numpy().reshape(-1, 1)\n        labels.append(ll)\n\n        if batch_idx % args.log_interval == 0:\n            pbar.set_description(logger_test_name + \' Test Epoch: {} [{}/{} ({:.0f}%)]\'.format(\n                epoch, batch_idx * len(data_a), len(test_loader.dataset),\n                       100. * batch_idx / len(test_loader)))\n\n    num_tests = test_loader.dataset.matches.size(0)\n    labels = np.vstack(labels).reshape(num_tests)\n    distances = np.vstack(distances).reshape(num_tests)\n\n    fpr95 = ErrorRateAt95Recall(labels, 1.0 / (distances + 1e-8))\n    #fdr95 = ErrorRateFDRAt95Recall(labels, 1.0 / (distances + 1e-8))\n\n    #fpr2 = convertFDR2FPR(fdr95, 0.95, 50000, 50000)\n    #fpr2fdr = convertFPR2FDR(fpr2, 0.95, 50000, 50000)\n\n    #print(\'\\33[91mTest set: Accuracy(FDR95): {:.8f}\\n\\33[0m\'.format(fdr95))\n    print(\'\\33[91mTest set: Accuracy(FPR95): {:.8f}\\n\\33[0m\'.format(fpr95))\n    #print(\'\\33[91mTest set: Accuracy(FDR2FPR): {:.8f}\\n\\33[0m\'.format(fpr2))\n    #print(\'\\33[91mTest set: Accuracy(FPR2FDR): {:.8f}\\n\\33[0m\'.format(fpr2fdr))\n\n    #fpr2 = convertFDR2FPR(round(fdr95,2), 0.95, 50000, 50000)\n    #fpr2fdr = convertFPR2FDR(round(fpr2,2), 0.95, 50000, 50000)\n\n    #print(\'\\33[91mTest set: Accuracy(FDR2FPR): {:.8f}\\n\\33[0m\'.format(fpr2))\n    #print(\'\\33[91mTest set: Accuracy(FPR2FDR): {:.8f}\\n\\33[0m\'.format(fpr2fdr))\n\n    if (args.enable_logging):\n        logger.log_value(logger_test_name + \' fpr95\', fpr95)\n    return\n\n\ndef adjust_learning_rate(optimizer):\n    """"""Updates the learning rate given the learning rate decay.\n    The routine has been implemented according to the original Lua SGD optimizer\n    """"""\n    for group in optimizer.param_groups:\n        if \'step\' not in group:\n            group[\'step\'] = 0.\n        else:\n            group[\'step\'] += 1.\n        group[\'lr\'] = args.lr * (\n            1.0 - float(group[\'step\']) * float(args.batch_size) / (args.n_triplets * float(args.epochs)))\n    return\n\n\ndef create_optimizer(model, new_lr):\n    # setup optimizer\n    if args.optimizer == \'sgd\':\n        optimizer = optim.SGD(model.parameters(), lr=new_lr,\n                              momentum=0.9, dampening=0.9,\n                              weight_decay=args.wd)\n    elif args.optimizer == \'adam\':\n        optimizer = optim.Adam(model.parameters(), lr=new_lr,\n                               weight_decay=args.wd)\n    else:\n        raise Exception(\'Not supported optimizer: {0}\'.format(args.optimizer))\n    return optimizer\n\n\ndef main(train_loader, test_loaders, model, logger, file_logger):\n    # print the experiment configuration\n    print(\'\\nparsed options:\\n{}\\n\'.format(vars(args)))\n\n    # if (args.enable_logging):\n    #    file_logger.log_string(\'logs.txt\', \'\\nparsed options:\\n{}\\n\'.format(vars(args)))\n\n    if args.cuda:\n        model.cuda()\n\n    optimizer1 = create_optimizer(model.features, args.lr)\n\n    # optionally resume from a checkpoint\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(\'=> loading checkpoint {}\'.format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            checkpoint = torch.load(args.resume)\n            model.load_state_dict(checkpoint[\'state_dict\'])\n        else:\n            print(\'=> no checkpoint found at {}\'.format(args.resume))\n\n    start = args.start_epoch\n    end = start + args.epochs\n    for epoch in range(start, end):\n        # iterate over test loaders and test results\n        #train_loader, test_loaders2 = create_loaders(load_random_triplets=triplet_flag)\n        train(train_loader, model, optimizer1, epoch, logger, triplet_flag)\n        for test_loader in test_loaders:\n            test(test_loader[\'dataloader\'], model, epoch, logger, test_loader[\'name\'])\n        if TEST_ON_W1BS:\n            # print(weights_path)\n            patch_images = w1bs.get_list_of_patch_images(\n                DATASET_DIR=args.w1bsroot.replace(\'/code\', \'/data/W1BS\'))\n            desc_name = \'curr_desc\'  # + str(random.randint(0,100))\n\n            DESCS_DIR = LOG_DIR + \'/temp_descs/\'  # args.w1bsroot.replace(\'/code\', ""/data/out_descriptors"")\n            OUT_DIR = DESCS_DIR.replace(\'/temp_descs/\', ""/out_graphs/"")\n\n            for img_fname in patch_images:\n                w1bs_extract_descs_and_save(img_fname, model, desc_name, cuda=args.cuda,\n                                            mean_img=args.mean_image,\n                                            std_img=args.std_image, out_dir=DESCS_DIR)\n\n            force_rewrite_list = [desc_name]\n            w1bs.match_descriptors_and_save_results(DESC_DIR=DESCS_DIR, do_rewrite=True,\n                                                    dist_dict={},\n                                                    force_rewrite_list=force_rewrite_list)\n            if (args.enable_logging):\n                w1bs.draw_and_save_plots_with_loggers(DESC_DIR=DESCS_DIR, OUT_DIR=OUT_DIR,\n                                                      methods=[""SNN_ratio""],\n                                                      descs_to_draw=[desc_name],\n                                                      logger=file_logger,\n                                                      tensor_logger=logger)\n            else:\n                w1bs.draw_and_save_plots(DESC_DIR=DESCS_DIR, OUT_DIR=OUT_DIR,\n                                         methods=[""SNN_ratio""],\n                                         descs_to_draw=[desc_name])\n\nif __name__ == \'__main__\':\n    LOG_DIR = args.log_dir\n    if not os.path.isdir(LOG_DIR):\n        os.makedirs(LOG_DIR)\n    LOG_DIR = os.path.join(args.log_dir, suffix)\n    DESCS_DIR = os.path.join(LOG_DIR, \'temp_descs\')\n    if TEST_ON_W1BS:\n        if not os.path.isdir(DESCS_DIR):\n            os.makedirs(DESCS_DIR)\n    logger, file_logger = None, None\n    model = HardNet()\n    if (args.enable_logging):\n        from Loggers import Logger, FileLogger\n        logger = Logger(LOG_DIR)\n        # file_logger = FileLogger(./log/+suffix)\n    train_loader, test_loaders = create_loaders(load_random_triplets=triplet_flag)\n    main(train_loader, test_loaders, model, logger, file_logger)\n'"
code/HardNetMultipleDatasets.py,32,"b'#!/usr/bin/python2 -utt\n# -*- coding: utf-8 -*-\n""""""\nThis is HardNet local patch descriptor. The training code is based on PyTorch TFeat implementation\nhttps://github.com/edgarriba/examples/tree/master/triplet\nby Edgar Riba.\n\nIf you use this code, please cite\n@article{HardNet2017,\n author = {Anastasiya Mishchuk, Dmytro Mishkin, Filip Radenovic, Jiri Matas},\n    title = ""{Working hard to know your neighbor\'s margins:Local descriptor learning loss}"",\n     year = 2017}\n(c) 2017 by Anastasiia Mishchuk, Dmytro Mishkin\n""""""\n\nfrom __future__ import division, print_function\nimport sys\nimport math\nimport PIL\nfrom copy import deepcopy\nimport argparse\nimport torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport os\nfrom tqdm import tqdm\nimport numpy as np\nimport random\nimport cv2\nimport copy\nfrom EvalMetrics import ErrorRateAt95Recall#, ErrorRateFDRAt95Recall, convertFDR2FPR, convertFPR2FDR\nfrom Losses import loss_HardNet, loss_random_sampling, loss_L2Net, global_orthogonal_regularization\nfrom W1BS import w1bs_extract_descs_and_save\nfrom Utils import L2Norm, cv2_scale, np_reshape\nfrom Utils import str2bool\nimport torch.nn as nn\nimport torch.utils.data as data\n\n\nclass CorrelationPenaltyLoss(nn.Module):\n    def __init__(self):\n        super(CorrelationPenaltyLoss, self).__init__()\n\n    def forward(self, input):\n        mean1 = torch.mean(input, dim=0)\n        zeroed = input - mean1.expand_as(input)\n        cor_mat = torch.bmm(torch.t(zeroed).unsqueeze(0), zeroed.unsqueeze(0)).squeeze(0)\n        d = torch.diag(torch.diag(cor_mat))\n        no_diag = cor_mat - d\n        d_sq = no_diag * no_diag\n        return torch.sqrt(d_sq.sum()) / input.size(0)\n\n\n# Training settings\nparser = argparse.ArgumentParser(description=\'PyTorch HardNet\')\n# Model options\n\nparser.add_argument(\'--w1bsroot\', type=str,\n                    default=\'data/sets/wxbs-descriptors-benchmark/code\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--dataroot\', type=str,\n                    default=\'data/sets/\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--enable-logging\', type=str2bool, default=False,\n                    help=\'output to tensorlogger\')\nparser.add_argument(\'--log-dir\', default=\'data/logs/\',\n                    help=\'folder to output log\')\nparser.add_argument(\'--model-dir\', default=\'data/models/\',\n                    help=\'folder to output model checkpoints\')\nparser.add_argument(\'--experiment-name\', default=\'multiple_datasets_all/\',\n                    help=\'experiment path\')\nparser.add_argument(\'--training-set\', default=\'all\',\n                    help=\'Other options: notredame, yosemite\')\nparser.add_argument(\'--loss\', default=\'triplet_margin\',\n                    help=\'Other options: softmax, contrastive\')\nparser.add_argument(\'--batch-reduce\', default=\'min\',\n                    help=\'Other options: average, random, random_global, L2Net\')\nparser.add_argument(\'--num-workers\', default=4, type=int,\n                    help=\'Number of workers to be created\')\nparser.add_argument(\'--pin-memory\', type=bool, default=True,\n                    help=\'\')\nparser.add_argument(\'--decor\', type=str2bool, default=False,\n                    help=\'L2Net decorrelation penalty\')\nparser.add_argument(\'--anchorave\', type=str2bool, default=False,\n                    help=\'anchorave\')\nparser.add_argument(\'--imageSize\', type=int, default=32,\n                    help=\'the height / width of the input image to network\')\nparser.add_argument(\'--mean-image\', type=float, default=0.443728476019,\n                    help=\'mean of train dataset for normalization\')\nparser.add_argument(\'--std-image\', type=float, default=0.20197947209,\n                    help=\'std of train dataset for normalization\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--epochs\', type=int, default=10, metavar=\'E\',\n                    help=\'number of epochs to train (default: 10)\')\nparser.add_argument(\'--anchorswap\', type=bool, default=True,\n                    help=\'turns on anchor swap\')\nparser.add_argument(\'--batch-size\', type=int, default=1024, metavar=\'BS\',\n                    help=\'input batch size for training (default: 1024)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=1024, metavar=\'BST\',\n                    help=\'input batch size for testing (default: 1024)\')\nparser.add_argument(\'--n-triplets\', type=int, default=1000, metavar=\'N\',\n                    help=\'how many triplets will generate from the dataset\')\nparser.add_argument(\'--margin\', type=float, default=1.0, metavar=\'MARGIN\',\n                    help=\'the margin value for the triplet loss function (default: 1.0\')\nparser.add_argument(\'--gor\', type=str2bool, default=False,\n                    help=\'use gor\')\nparser.add_argument(\'--alpha\', type=float, default=1.0, metavar=\'ALPHA\',\n                    help=\'gor parameter\')\nparser.add_argument(\'--act-decay\', type=float, default=0,\n                    help=\'activity L2 decay, default 0\')\nparser.add_argument(\'--lr\', type=float, default=10.0, metavar=\'LR\',\n                    help=\'learning rate (default: 10.0)\')\nparser.add_argument(\'--fliprot\', type=str2bool, default=True,\n                    help=\'turns on flip and 90deg rotation augmentation\')\nparser.add_argument(\'--augmentation\', type=str2bool, default=False,\n                    help=\'turns on shift and small scale rotation augmentation\')\nparser.add_argument(\'--lr-decay\', default=1e-6, type=float, metavar=\'LRD\',\n                    help=\'learning rate decay ratio (default: 1e-6\')\nparser.add_argument(\'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--optimizer\', default=\'sgd\', type=str,\n                    metavar=\'OPT\', help=\'The optimizer to use (default: SGD)\')\n# Device options\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'enables CUDA training\')\nparser.add_argument(\'--gpu-id\', default=\'0\', type=str,\n                    help=\'id(s) for CUDA_VISIBLE_DEVICES\')\nparser.add_argument(\'--seed\', type=int, default=0, metavar=\'S\',\n                    help=\'random seed (default: 0)\')\nparser.add_argument(\'--log-interval\', type=int, default=10, metavar=\'LI\',\n                    help=\'how many batches to wait before logging training status\')\n\nargs = parser.parse_args()\n\nsuffix = \'{}_{}\'.format(args.training_set, args.batch_reduce)\n\nif args.gor:\n    suffix = suffix + \'_gor_alpha{:1.1f}\'.format(args.alpha)\nif args.anchorswap:\n    suffix = suffix + \'_as\'\nif args.anchorave:\n    suffix = suffix + \'_av\'\n\ntriplet_flag = (args.batch_reduce == \'random_global\') or args.gor\n\ndataset_names = [\'liberty\', \'notredame\', \'yosemite\']\n\nTEST_ON_W1BS = False\n# check if path to w1bs dataset testing module exists\nif os.path.isdir(args.w1bsroot):\n    sys.path.insert(0, args.w1bsroot)\n    import utils.w1bs as w1bs\n\n    TEST_ON_W1BS = True\n\n# set the device to use by setting CUDA_VISIBLE_DEVICES env variable in\n# order to prevent any memory allocation on unused GPUs\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu_id\n\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nif args.cuda:\n    cudnn.benchmark = True\n    torch.cuda.manual_seed_all(args.seed)\n\n# create loggin directory\nif not os.path.exists(args.log_dir):\n    os.makedirs(args.log_dir)\n\n# set random seeds\ntorch.manual_seed(args.seed)\nnp.random.seed(args.seed)\n\nclass TotalDatasetsLoader(data.Dataset):\n\n    def __init__(self, datasets_path, train = True, transform = None, batch_size = None, n_triplets = 5000000, fliprot = False, *arg, **kw):\n        super(TotalDatasetsLoader, self).__init__()\n        datasets_path = [os.path.join(datasets_path, dataset) for dataset in os.listdir(datasets_path) if \'.pt\' in dataset]\n        datasets = [torch.load(dataset) for dataset in datasets_path]\n        print (datasets_path)\n        data, labels = datasets[0][0], datasets[0][1]\n\n        for i in range(1,len(datasets)):\n            data = torch.cat([data,datasets[i][0]])\n            labels = torch.cat([labels, datasets[i][1]+torch.max(labels)+1])\n\n        del datasets\n\n        self.data, self.labels = data, labels\n        self.transform = transform\n        self.train = train\n        self.n_triplets = n_triplets\n        self.batch_size = batch_size\n        self.fliprot = fliprot\n        if self.train:\n                print(\'Generating {} triplets\'.format(self.n_triplets))\n                self.triplets = self.generate_triplets(self.labels, self.n_triplets, self.batch_size)\n\n    @staticmethod\n    def generate_triplets(labels, num_triplets, batch_size):\n            def create_indices(_labels):\n                inds = dict()\n                for idx, ind in enumerate(_labels):\n                    if ind not in inds:\n                        inds[ind] = []\n                    inds[ind].append(idx)\n                return inds\n\n            triplets = []\n            indices = create_indices(labels.numpy())\n            unique_labels = np.unique(labels.numpy())\n            n_classes = unique_labels.shape[0]\n            # add only unique indices in batch\n            already_idxs = set()\n\n            for x in tqdm(range(num_triplets)):\n                if len(already_idxs) >= batch_size:\n                    already_idxs = set()\n                c1 = np.random.randint(0, n_classes)\n                while c1 in already_idxs:\n                    c1 = np.random.randint(0, n_classes)\n                already_idxs.add(c1)\n                c2 = np.random.randint(0, n_classes)\n                while c1 == c2:\n                    c2 = np.random.randint(0, n_classes)\n                if len(indices[c1]) == 2:  # hack to speed up process\n                    n1, n2 = 0, 1\n                else:\n                    n1 = np.random.randint(0, len(indices[c1]))\n                    n2 = np.random.randint(0, len(indices[c1]))\n                    while n1 == n2:\n                        n2 = np.random.randint(0, len(indices[c1]))\n                n3 = np.random.randint(0, len(indices[c2]))\n                triplets.append([indices[c1][n1], indices[c1][n2], indices[c2][n3]])\n            return torch.LongTensor(np.array(triplets))\n\n    def __getitem__(self, index):\n            def transform_img(img):\n                if self.transform is not None:\n                    img = img.numpy()\n                    img = self.transform(img)\n                return img\n\n            t = self.triplets[index]\n            a, p, n = self.data[t[0]], self.data[t[1]], self.data[t[2]]\n\n            img_a = transform_img(a)\n            img_p = transform_img(p)\n\n            # transform images if required\n            if self.fliprot:\n                do_flip = random.random() > 0.5\n                do_rot = random.random() > 0.5\n\n                if do_rot:\n                    img_a = img_a.permute(0,2,1)\n                    img_p = img_p.permute(0,2,1)\n\n                if do_flip:\n                    img_a = torch.from_numpy(deepcopy(img_a.numpy()[:,:,::-1]))\n                    img_p = torch.from_numpy(deepcopy(img_p.numpy()[:,:,::-1]))\n            return img_a, img_p\n\n    def __len__(self):\n            if self.train:\n                return self.triplets.size(0)\n\nclass TripletPhotoTour(dset.PhotoTour):\n    """"""\n    From the PhotoTour Dataset it generates triplet samples\n    note: a triplet is composed by a pair of matching images and one of\n    different class.\n    """"""\n    def __init__(self, train=True, transform=None, n_triplets = 1000, batch_size=None, load_random_triplets=False, *arg, **kw):\n        super(TripletPhotoTour, self).__init__(*arg, **kw)\n        self.transform = transform\n        self.out_triplets = load_random_triplets\n        self.train = train\n        self.n_triplets = n_triplets\n        self.batch_size = batch_size\n        if self.train:\n            print(\'Generating {} triplets\'.format(self.n_triplets))\n            self.triplets = self.generate_triplets(self.labels, self.n_triplets)\n\n    @staticmethod\n    def generate_triplets(labels, num_triplets):\n        def create_indices(_labels):\n            inds = dict()\n            for idx, ind in enumerate(_labels):\n                if ind not in inds:\n                    inds[ind] = []\n                inds[ind].append(idx)\n            return inds\n\n        triplets = []\n        indices = create_indices(labels.numpy())\n        unique_labels = np.unique(labels.numpy())\n        n_classes = unique_labels.shape[0]\n        # add only unique indices in batch\n        already_idxs = set()\n\n        for x in tqdm(range(num_triplets)):\n            if len(already_idxs) >= args.batch_size:\n                already_idxs = set()\n            c1 = np.random.randint(0, n_classes - 1)\n            while c1 in already_idxs:\n                c1 = np.random.randint(0, n_classes - 1)\n            already_idxs.add(c1)\n            c2 = np.random.randint(0, n_classes - 1)\n            while c1 == c2:\n                c2 = np.random.randint(0, n_classes - 1)\n            if len(indices[c1]) == 2:  # hack to speed up process\n                n1, n2 = 0, 1\n            else:\n                n1 = np.random.randint(0, len(indices[c1]) - 1)\n                n2 = np.random.randint(0, len(indices[c1]) - 1)\n                while n1 == n2:\n                    n2 = np.random.randint(0, len(indices[c1]) - 1)\n            n3 = np.random.randint(0, len(indices[c2]) - 1)\n            triplets.append([indices[c1][n1], indices[c1][n2], indices[c2][n3]])\n        return torch.LongTensor(np.array(triplets))\n\n    def __getitem__(self, index):\n        def transform_img(img):\n            if self.transform is not None:\n                img = self.transform(img.numpy())\n            return img\n\n        if not self.train:\n            m = self.matches[index]\n            img1 = transform_img(self.data[m[0]])\n            img2 = transform_img(self.data[m[1]])\n            return img1, img2, m[2]\n\n        t = self.triplets[index]\n        a, p, n = self.data[t[0]], self.data[t[1]], self.data[t[2]]\n\n        img_a = transform_img(a)\n        img_p = transform_img(p)\n        img_n = None\n        if self.out_triplets:\n            img_n = transform_img(n)\n        # transform images if required\n        if args.fliprot:\n            do_flip = random.random() > 0.5\n            do_rot = random.random() > 0.5\n            if do_rot:\n                img_a = img_a.permute(0, 2, 1)\n                img_p = img_p.permute(0, 2, 1)\n                if self.out_triplets:\n                    img_n = img_n.permute(0, 2, 1)\n            if do_flip:\n                img_a = torch.from_numpy(deepcopy(img_a.numpy()[:, :, ::-1]))\n                img_p = torch.from_numpy(deepcopy(img_p.numpy()[:, :, ::-1]))\n                if self.out_triplets:\n                    img_n = torch.from_numpy(deepcopy(img_n.numpy()[:, :, ::-1]))\n        if self.out_triplets:\n            return (img_a, img_p, img_n)\n        else:\n            return (img_a, img_p)\n\n    def __len__(self):\n        if self.train:\n            return self.triplets.size(0)\n        else:\n            return self.matches.size(0)\n\n\nclass HardNet(nn.Module):\n    """"""HardNet model definition\n    """"""\n\n    def __init__(self):\n        super(HardNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Conv2d(128, 128, kernel_size=8, bias=False),\n            nn.BatchNorm2d(128, affine=False),\n        )\n        self.features.apply(weights_init)\n        return\n\n    def input_norm(self, x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(\n            -1).unsqueeze(1).expand_as(x)\n\n    def forward(self, input):\n        x_features = self.features(self.input_norm(input))\n        x = x_features.view(x_features.size(0), -1)\n        return L2Norm()(x)\n\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.orthogonal(m.weight.data, gain=0.6)\n        try:\n            nn.init.constant(m.bias.data, 0.01)\n        except:\n            pass\n    return\n\n\ndef create_loaders(load_random_triplets=False):\n    test_dataset_names = copy.copy(dataset_names)\n    #test_dataset_names.remove(args.training_set)\n    kwargs = {\'num_workers\': int(args.num_workers), \'pin_memory\': args.pin_memory} if args.cuda else {}\n    np_reshape64 = lambda x: np.reshape(x, (64, 64, 1))\n    transform_test = transforms.Compose([\n            transforms.Lambda(np_reshape64),\n            transforms.ToPILImage(),\n            transforms.Resize(32),\n            transforms.ToTensor()])\n    transform_train = transforms.Compose([\n            transforms.Lambda(np_reshape64),\n            transforms.ToPILImage(),\n            transforms.RandomRotation(5,PIL.Image.BILINEAR),\n            transforms.RandomResizedCrop(32, scale = (0.9,1.0),ratio = (0.9,1.1)),\n            transforms.Resize(32),\n            transforms.ToTensor()])\n    transform = transforms.Compose([\n            transforms.Lambda(cv2_scale),\n            transforms.Lambda(np_reshape),\n            transforms.ToTensor(),\n            transforms.Normalize((args.mean_image,), (args.std_image,))])\n    if not args.augmentation:\n        transform_train = transform\n        transform_test = transform\n\n\n    train_loader = torch.utils.data.DataLoader(\n        TotalDatasetsLoader(train=True,\n                         load_random_triplets=load_random_triplets,\n                         batch_size=args.batch_size,\n                         datasets_path=args.dataroot,\n                         fliprot=args.fliprot,\n                         n_triplets=args.n_triplets,\n                         name=args.training_set,\n                         download=True,\n                         transform=transform_train),\n        batch_size=args.batch_size,\n        shuffle=False, **kwargs)\n\n    test_loaders = [{\'name\': name,\n                     \'dataloader\': torch.utils.data.DataLoader(\n                         TripletPhotoTour(train=False,\n                                          batch_size=args.test_batch_size,\n                                          n_triplets = 1000,\n                                          root=args.dataroot,\n                                          name=name,\n                                          download=True,\n                                          transform=transform_test),\n                         batch_size=args.test_batch_size,\n                         shuffle=False, **kwargs)}\n                    for name in test_dataset_names]\n\n    return train_loader, test_loaders\n\n\ndef train(train_loader, model, optimizer, epoch, logger, load_triplets=False):\n    # switch to train mode\n    model.train()\n    pbar = tqdm(enumerate(train_loader))\n    for batch_idx, data in pbar:\n        if load_triplets:\n            data_a, data_p, data_n = data\n        else:\n            data_a, data_p = data\n\n        if args.cuda:\n            data_a, data_p = data_a.cuda(), data_p.cuda()\n            data_a, data_p = Variable(data_a), Variable(data_p)\n            out_a = model(data_a)\n            out_p = model(data_p)\n        if load_triplets:\n            data_n = data_n.cuda()\n            data_n = Variable(data_n)\n            out_n = model(data_n)\n\n        if args.batch_reduce == \'L2Net\':\n            loss = loss_L2Net(out_a, out_p, anchor_swap=args.anchorswap,\n                              margin=args.margin, loss_type=args.loss)\n        elif args.batch_reduce == \'random_global\':\n            loss = loss_random_sampling(out_a, out_p, out_n,\n                                        margin=args.margin,\n                                        anchor_swap=args.anchorswap,\n                                        loss_type=args.loss)\n        else:\n            loss = loss_HardNet(out_a, out_p,\n                                margin=args.margin,\n                                anchor_swap=args.anchorswap,\n                                anchor_ave=args.anchorave,\n                                batch_reduce=args.batch_reduce,\n                                loss_type=args.loss)\n\n        if args.decor:\n            loss += CorrelationPenaltyLoss()(out_a)\n\n        if args.gor:\n            loss += args.alpha * global_orthogonal_regularization(out_a, out_n)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        adjust_learning_rate(optimizer)\n        if batch_idx % args.log_interval == 0:\n            pbar.set_description(\n                \'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                    epoch, batch_idx * len(data_a), len(train_loader.dataset),\n                           100. * batch_idx / len(train_loader),\n                    loss.data[0]))\n    if (args.enable_logging):\n        logger.log_value(\'loss\', loss.data[0]).step()\n    try:\n        os.stat(\'{}{}\'.format(args.model_dir, suffix))\n    except:\n        os.makedirs(\'{}{}\'.format(args.model_dir, suffix))\n\n    torch.save({\'epoch\': epoch + 1, \'state_dict\': model.state_dict()},\n               \'{}{}/checkpoint_{}.pth\'.format(args.model_dir, suffix, epoch))\n\n\ndef test(test_loader, model, epoch, logger, logger_test_name):\n    # switch to evaluate mode\n    model.eval()\n\n    labels, distances = [], []\n\n    pbar = tqdm(enumerate(test_loader))\n    for batch_idx, (data_a, data_p, label) in pbar:\n\n        if args.cuda:\n            data_a, data_p = data_a.cuda(), data_p.cuda()\n\n        data_a, data_p, label = Variable(data_a, volatile=True), \\\n                                Variable(data_p, volatile=True), Variable(label)\n\n        out_a = model(data_a)\n        out_p = model(data_p)\n        dists = torch.sqrt(torch.sum((out_a - out_p) ** 2, 1))  # euclidean distance\n        distances.append(dists.data.cpu().numpy().reshape(-1, 1))\n        ll = label.data.cpu().numpy().reshape(-1, 1)\n        labels.append(ll)\n\n        if batch_idx % args.log_interval == 0:\n            pbar.set_description(logger_test_name + \' Test Epoch: {} [{}/{} ({:.0f}%)]\'.format(\n                epoch, batch_idx * len(data_a), len(test_loader.dataset),\n                       100. * batch_idx / len(test_loader)))\n\n    num_tests = test_loader.dataset.matches.size(0)\n    labels = np.vstack(labels).reshape(num_tests)\n    distances = np.vstack(distances).reshape(num_tests)\n\n    fpr95 = ErrorRateAt95Recall(labels, 1.0 / (distances + 1e-8))\n    #fdr95 = ErrorRateFDRAt95Recall(labels, 1.0 / (distances + 1e-8))\n\n    #fpr2 = convertFDR2FPR(fdr95, 0.95, 50000, 50000)\n    #fpr2fdr = convertFPR2FDR(fpr2, 0.95, 50000, 50000)\n\n    #print(\'\\33[91mTest set: Accuracy(FDR95): {:.8f}\\n\\33[0m\'.format(fdr95))\n    print(\'\\33[91mTest set: Accuracy(FPR95): {:.8f}\\n\\33[0m\'.format(fpr95))\n    #print(\'\\33[91mTest set: Accuracy(FDR2FPR): {:.8f}\\n\\33[0m\'.format(fpr2))\n    #print(\'\\33[91mTest set: Accuracy(FPR2FDR): {:.8f}\\n\\33[0m\'.format(fpr2fdr))\n\n    #fpr2 = convertFDR2FPR(round(fdr95,2), 0.95, 50000, 50000)\n    #fpr2fdr = convertFPR2FDR(round(fpr2,2), 0.95, 50000, 50000)\n\n    #print(\'\\33[91mTest set: Accuracy(FDR2FPR): {:.8f}\\n\\33[0m\'.format(fpr2))\n    #print(\'\\33[91mTest set: Accuracy(FPR2FDR): {:.8f}\\n\\33[0m\'.format(fpr2fdr))\n\n    if (args.enable_logging):\n        logger.log_value(logger_test_name + \' fpr95\', fpr95)\n    return\n\n\ndef adjust_learning_rate(optimizer):\n    """"""Updates the learning rate given the learning rate decay.\n    The routine has been implemented according to the original Lua SGD optimizer\n    """"""\n    for group in optimizer.param_groups:\n        if \'step\' not in group:\n            group[\'step\'] = 0.\n        else:\n            group[\'step\'] += 1.\n        group[\'lr\'] = args.lr * (\n            1.0 - float(group[\'step\']) * float(args.batch_size) / (args.n_triplets * float(args.epochs)))\n    return\n\n\ndef create_optimizer(model, new_lr):\n    # setup optimizer\n    if args.optimizer == \'sgd\':\n        optimizer = optim.SGD(model.parameters(), lr=new_lr,\n                              momentum=0.9, dampening=0.9,\n                              weight_decay=args.wd)\n    elif args.optimizer == \'adam\':\n        optimizer = optim.Adam(model.parameters(), lr=new_lr,\n                               weight_decay=args.wd)\n    else:\n        raise Exception(\'Not supported optimizer: {0}\'.format(args.optimizer))\n    return optimizer\n\n\ndef main(train_loader, test_loaders, model, logger, file_logger):\n    # print the experiment configuration\n    print(\'\\nparsed options:\\n{}\\n\'.format(vars(args)))\n\n    # if (args.enable_logging):\n    #    file_logger.log_string(\'logs.txt\', \'\\nparsed options:\\n{}\\n\'.format(vars(args)))\n\n    if args.cuda:\n        model.cuda()\n\n    optimizer1 = create_optimizer(model.features, args.lr)\n\n    # optionally resume from a checkpoint\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(\'=> loading checkpoint {}\'.format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            checkpoint = torch.load(args.resume)\n            model.load_state_dict(checkpoint[\'state_dict\'])\n        else:\n            print(\'=> no checkpoint found at {}\'.format(args.resume))\n\n    start = args.start_epoch\n    end = start + args.epochs\n    for epoch in range(start, end):\n        # iterate over test loaders and test results\n        #train_loader, test_loaders2 = create_loaders(load_random_triplets=triplet_flag)\n        train(train_loader, model, optimizer1, epoch, logger, triplet_flag)\n        for test_loader in test_loaders:\n            test(test_loader[\'dataloader\'], model, epoch, logger, test_loader[\'name\'])\n        if TEST_ON_W1BS:\n            # print(weights_path)\n            patch_images = w1bs.get_list_of_patch_images(\n                DATASET_DIR=args.w1bsroot.replace(\'/code\', \'/data/W1BS\'))\n            desc_name = \'curr_desc\'  # + str(random.randint(0,100))\n\n            DESCS_DIR = LOG_DIR + \'/temp_descs/\'  # args.w1bsroot.replace(\'/code\', ""/data/out_descriptors"")\n            OUT_DIR = DESCS_DIR.replace(\'/temp_descs/\', ""/out_graphs/"")\n\n            for img_fname in patch_images:\n                w1bs_extract_descs_and_save(img_fname, model, desc_name, cuda=args.cuda,\n                                            mean_img=args.mean_image,\n                                            std_img=args.std_image, out_dir=DESCS_DIR)\n\n            force_rewrite_list = [desc_name]\n            w1bs.match_descriptors_and_save_results(DESC_DIR=DESCS_DIR, do_rewrite=True,\n                                                    dist_dict={},\n                                                    force_rewrite_list=force_rewrite_list)\n            if (args.enable_logging):\n                w1bs.draw_and_save_plots_with_loggers(DESC_DIR=DESCS_DIR, OUT_DIR=OUT_DIR,\n                                                      methods=[""SNN_ratio""],\n                                                      descs_to_draw=[desc_name],\n                                                      logger=file_logger,\n                                                      tensor_logger=logger)\n            else:\n                w1bs.draw_and_save_plots(DESC_DIR=DESCS_DIR, OUT_DIR=OUT_DIR,\n                                         methods=[""SNN_ratio""],\n                                         descs_to_draw=[desc_name])\n\nif __name__ == \'__main__\':\n    LOG_DIR = args.log_dir\n    if not os.path.isdir(LOG_DIR):\n        os.makedirs(LOG_DIR)\n    LOG_DIR = os.path.join(args.log_dir, suffix)\n    DESCS_DIR = os.path.join(LOG_DIR, \'temp_descs\')\n    if TEST_ON_W1BS:\n        if not os.path.isdir(DESCS_DIR):\n            os.makedirs(DESCS_DIR)\n    logger, file_logger = None, None\n    model = HardNet()\n    if (args.enable_logging):\n        from Loggers import Logger, FileLogger\n        logger = Logger(LOG_DIR)\n        # file_logger = FileLogger(./log/+suffix)\n    train_loader, test_loaders = create_loaders(load_random_triplets=triplet_flag)\n    main(train_loader, test_loaders, model, logger, file_logger)\n'"
code/Loggers.py,0,"b'from tensorboard_logger import configure, log_value\nimport os\n\nclass FileLogger:\n    ""Log text in file.""\n    def __init__(self, path):\n        self.path = path\n\n    def log_string(self, file_name, string):\n        """"""Stores log string in specified file.""""""\n        text_file = open(self.path+file_name+"".log"", ""a"")\n        text_file.write(string+\'\'+str(string)+\'\\n\')\n        text_file.close()\n\n    def log_stats(self, file_name, text_to_save, value):\n        """"""Stores log in specified file.""""""\n        text_file = open(self.path+file_name+"".log"", ""a"")\n        text_file.write(text_to_save+\' \'+str(value)+\'\\n\')\n        text_file.close()\n\n\nclass Logger(object):\n    ""Tensorboard Logger""\n    def __init__(self, log_dir):\n        # clean previous logged data under the same directory name\n        self._remove(log_dir)\n\n        # configure the project\n        configure(log_dir)\n\n        self.global_step = 0\n\n    def log_value(self, name, value):\n        log_value(name, value, self.global_step)\n        return self\n\n    def step(self):\n        self.global_step += 1\n\n    @staticmethod\n    def _remove(path):\n        """""" param <path> could either be relative or absolute. """"""\n        if os.path.isfile(path):\n            os.remove(path)  # remove the file\n        elif os.path.isdir(path):\n            import shutil\n            shutil.rmtree(path)  # remove dir and all contains\n'"
code/Losses.py,49,"b'import torch\nimport torch.nn as nn\nimport sys\n\ndef distance_matrix_vector(anchor, positive):\n    """"""Given batch of anchor descriptors and positive descriptors calculate distance matrix""""""\n\n    d1_sq = torch.sum(anchor * anchor, dim=1).unsqueeze(-1)\n    d2_sq = torch.sum(positive * positive, dim=1).unsqueeze(-1)\n\n    eps = 1e-6\n    return torch.sqrt((d1_sq.repeat(1, positive.size(0)) + torch.t(d2_sq.repeat(1, anchor.size(0)))\n                      - 2.0 * torch.bmm(anchor.unsqueeze(0), torch.t(positive).unsqueeze(0)).squeeze(0))+eps)\n\ndef distance_vectors_pairwise(anchor, positive, negative = None):\n    """"""Given batch of anchor descriptors and positive descriptors calculate distance matrix""""""\n\n    a_sq = torch.sum(anchor * anchor, dim=1)\n    p_sq = torch.sum(positive * positive, dim=1)\n\n    eps = 1e-8\n    d_a_p = torch.sqrt(a_sq + p_sq - 2*torch.sum(anchor * positive, dim = 1) + eps)\n    if negative is not None:\n        n_sq = torch.sum(negative * negative, dim=1)\n        d_a_n = torch.sqrt(a_sq + n_sq - 2*torch.sum(anchor * negative, dim = 1) + eps)\n        d_p_n = torch.sqrt(p_sq + n_sq - 2*torch.sum(positive * negative, dim = 1) + eps)\n        return d_a_p, d_a_n, d_p_n\n    return d_a_p\ndef loss_random_sampling(anchor, positive, negative, anchor_swap = False, margin = 1.0, loss_type = ""triplet_margin""):\n    """"""Loss with random sampling (no hard in batch).\n    """"""\n\n    assert anchor.size() == positive.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.size() == negative.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.dim() == 2, ""Inputd must be a 2D matrix.""\n    eps = 1e-8\n    (pos, d_a_n, d_p_n) = distance_vectors_pairwise(anchor, positive, negative)\n    if anchor_swap:\n       min_neg = torch.min(d_a_n, d_p_n)\n    else:\n       min_neg = d_a_n\n\n    if loss_type == ""triplet_margin"":\n        loss = torch.clamp(margin + pos - min_neg, min=0.0)\n    elif loss_type == \'softmax\':\n        exp_pos = torch.exp(2.0 - pos);\n        exp_den = exp_pos + torch.exp(2.0 - min_neg) + eps;\n        loss = - torch.log( exp_pos / exp_den )\n    elif loss_type == \'contrastive\':\n        loss = torch.clamp(margin - min_neg, min=0.0) + pos;\n    else: \n        print (\'Unknown loss type. Try triplet_margin, softmax or contrastive\')\n        sys.exit(1)\n    loss = torch.mean(loss)\n    return loss\n\ndef loss_L2Net(anchor, positive, anchor_swap = False,  margin = 1.0, loss_type = ""triplet_margin""):\n    """"""L2Net losses: using whole batch as negatives, not only hardest.\n    """"""\n\n    assert anchor.size() == positive.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.dim() == 2, ""Inputd must be a 2D matrix.""\n    eps = 1e-8\n    dist_matrix = distance_matrix_vector(anchor, positive)\n    eye = torch.autograd.Variable(torch.eye(dist_matrix.size(1))).cuda()\n\n    # steps to filter out same patches that occur in distance matrix as negatives\n    pos1 = torch.diag(dist_matrix)\n    dist_without_min_on_diag = dist_matrix+eye*10\n    mask = (dist_without_min_on_diag.ge(0.008)-1)*-1\n    mask = mask.type_as(dist_without_min_on_diag)*10\n    dist_without_min_on_diag = dist_without_min_on_diag+mask\n    \n    if loss_type == \'softmax\':\n        exp_pos = torch.exp(2.0 - pos1);\n        exp_den = torch.sum(torch.exp(2.0 - dist_matrix),1) + eps;\n        loss = -torch.log( exp_pos / exp_den )\n        if anchor_swap:\n            exp_den1 = torch.sum(torch.exp(2.0 - dist_matrix),0) + eps;\n            loss += -torch.log( exp_pos / exp_den1 )\n    else: \n        print (\'Only softmax loss works with L2Net sampling\')\n        sys.exit(1)\n    loss = torch.mean(loss)\n    return loss\n\ndef loss_HardNet(anchor, positive, anchor_swap = False, anchor_ave = False,\\\n        margin = 1.0, batch_reduce = \'min\', loss_type = ""triplet_margin""):\n    """"""HardNet margin loss - calculates loss based on distance matrix based on positive distance and closest negative distance.\n    """"""\n\n    assert anchor.size() == positive.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.dim() == 2, ""Inputd must be a 2D matrix.""\n    eps = 1e-8\n    dist_matrix = distance_matrix_vector(anchor, positive) +eps\n    eye = torch.autograd.Variable(torch.eye(dist_matrix.size(1))).cuda()\n\n    # steps to filter out same patches that occur in distance matrix as negatives\n    pos1 = torch.diag(dist_matrix)\n    dist_without_min_on_diag = dist_matrix+eye*10\n    mask = (dist_without_min_on_diag.ge(0.008).float()-1.0)*(-1)\n    mask = mask.type_as(dist_without_min_on_diag)*10\n    dist_without_min_on_diag = dist_without_min_on_diag+mask\n    if batch_reduce == \'min\':\n        min_neg = torch.min(dist_without_min_on_diag,1)[0]\n        if anchor_swap:\n            min_neg2 = torch.min(dist_without_min_on_diag,0)[0]\n            min_neg = torch.min(min_neg,min_neg2)\n        if False:\n            dist_matrix_a = distance_matrix_vector(anchor, anchor)+ eps\n            dist_matrix_p = distance_matrix_vector(positive,positive)+eps\n            dist_without_min_on_diag_a = dist_matrix_a+eye*10\n            dist_without_min_on_diag_p = dist_matrix_p+eye*10\n            min_neg_a = torch.min(dist_without_min_on_diag_a,1)[0]\n            min_neg_p = torch.t(torch.min(dist_without_min_on_diag_p,0)[0])\n            min_neg_3 = torch.min(min_neg_p,min_neg_a)\n            min_neg = torch.min(min_neg,min_neg_3)\n            print (min_neg_a)\n            print (min_neg_p)\n            print (min_neg_3)\n            print (min_neg)\n        min_neg = min_neg\n        pos = pos1\n    elif batch_reduce == \'average\':\n        pos = pos1.repeat(anchor.size(0)).view(-1,1).squeeze(0)\n        min_neg = dist_without_min_on_diag.view(-1,1)\n        if anchor_swap:\n            min_neg2 = torch.t(dist_without_min_on_diag).contiguous().view(-1,1)\n            min_neg = torch.min(min_neg,min_neg2)\n        min_neg = min_neg.squeeze(0)\n    elif batch_reduce == \'random\':\n        idxs = torch.autograd.Variable(torch.randperm(anchor.size()[0]).long()).cuda()\n        min_neg = dist_without_min_on_diag.gather(1,idxs.view(-1,1))\n        if anchor_swap:\n            min_neg2 = torch.t(dist_without_min_on_diag).gather(1,idxs.view(-1,1)) \n            min_neg = torch.min(min_neg,min_neg2)\n        min_neg = torch.t(min_neg).squeeze(0)\n        pos = pos1\n    else: \n        print (\'Unknown batch reduce mode. Try min, average or random\')\n        sys.exit(1)\n    if loss_type == ""triplet_margin"":\n        loss = torch.clamp(margin + pos - min_neg, min=0.0)\n    elif loss_type == \'softmax\':\n        exp_pos = torch.exp(2.0 - pos);\n        exp_den = exp_pos + torch.exp(2.0 - min_neg) + eps;\n        loss = - torch.log( exp_pos / exp_den )\n    elif loss_type == \'contrastive\':\n        loss = torch.clamp(margin - min_neg, min=0.0) + pos;\n    else: \n        print (\'Unknown loss type. Try triplet_margin, softmax or contrastive\')\n        sys.exit(1)\n    loss = torch.mean(loss)\n    return loss\n\ndef global_orthogonal_regularization(anchor, negative):\n\n    neg_dis = torch.sum(torch.mul(anchor,negative),1)\n    dim = anchor.size(1)\n    gor = torch.pow(torch.mean(neg_dis),2) + torch.clamp(torch.mean(torch.pow(neg_dis,2))-1.0/dim, min=0.0)\n    \n    return gor\n\n'"
code/Utils.py,4,"b""import torch\nimport torch.nn.init\nimport torch.nn as nn\nimport cv2\nimport numpy as np\n\n# resize image to size 32x32\ncv2_scale36 = lambda x: cv2.resize(x, dsize=(36, 36),\n                                 interpolation=cv2.INTER_LINEAR)\ncv2_scale = lambda x: cv2.resize(x, dsize=(32, 32),\n                                 interpolation=cv2.INTER_LINEAR)\n# reshape image\nnp_reshape = lambda x: np.reshape(x, (32, 32, 1))\n\nclass L2Norm(nn.Module):\n    def __init__(self):\n        super(L2Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sqrt(torch.sum(x * x, dim = 1) + self.eps)\n        x= x / norm.unsqueeze(-1).expand_as(x)\n        return x\n\nclass L1Norm(nn.Module):\n    def __init__(self):\n        super(L1Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sum(torch.abs(x), dim = 1) + self.eps\n        x= x / norm.expand_as(x)\n        return x\n\n\ndef str2bool(v):\n    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n"""
code/W1BS.py,2,"b'import os\nimport numpy as np\nimport cv2\nimport time\nfrom tqdm import tqdm\nimport torch\nfrom torch.autograd import Variable\nfrom skimage.io import imread\n\ndef w1bs_extract_descs_and_save(input_img_fname, model, desc_name, mean_img=0.443728476019, std_img=0.20197947209, cuda = False, out_dir = None):\n    if out_dir is None:\n        out_fname = input_img_fname.replace(""data/W1BS"", ""data/out_descriptors"").replace("".bmp"", ""."" + desc_name)\n        out_dir = os.path.dirname(out_fname)\n    else:\n        out_fname = out_dir + input_img_fname[input_img_fname.find(\'data/W1BS\'):].replace(""data/W1BS"", """").replace("".bmp"", ""."" + desc_name)\n        out_fname = out_fname.replace(\'//\', \'/\')\n        out_dir = os.path.dirname(out_fname)\n    if len(out_dir) > 0:\n        if not os.path.isdir(out_dir):\n            os.makedirs(out_dir)\n    image = imread(input_img_fname, 0)\n    h, w = image.shape\n    # print(h,w)\n    n_patches = int(h / w)\n    patches_for_net = np.zeros((n_patches, 1, 32, 32))\n    for i in range(n_patches):\n        patch = cv2.resize(image[i * (w): (i + 1) * (w), 0:w], (32, 32))\n        patches_for_net[i, 0, :, :] = patch[0:w, 0:w]\n    patches_for_net = patches_for_net/255\n    patches_for_net -= mean_img  # np.mean(patches_for_net)\n    patches_for_net /= std_img  # np.std(patches_for_net)\n    t = time.time()\n    ###\n    model.eval()\n    outs = []\n    labels, distances = [], []\n    pbar = tqdm(enumerate(patches_for_net))\n    bs = 128\n    n_batches = int(n_patches / bs) + 1\n    for batch_idx in range(n_batches):\n        if batch_idx == n_batches - 1:\n            if (batch_idx + 1) * bs > n_patches:\n                end = n_patches\n            else:\n                end = (batch_idx + 1) * bs\n        else:\n            end = (batch_idx + 1) * bs\n        data_a = patches_for_net[batch_idx * bs: end, :, :, :].astype(np.float32)\n        data_a = torch.from_numpy(data_a)\n        if cuda:\n            data_a = data_a.cuda()\n        data_a = Variable(data_a, volatile=True)\n        out_a = model(data_a)\n        outs.append(out_a.data.cpu().numpy().reshape(-1, 128))\n    ###\n    res_desc = np.concatenate(outs)\n    print(res_desc.shape, n_patches)\n    res_desc = np.reshape(res_desc, (n_patches, -1))\n    np.savetxt(out_fname, res_desc, delimiter=\' \', fmt=\'%10.7f\')\n    return\n'"
code/check_gor_HardNet.py,0,"b'""""""\nCheck the correctness of gor on HardNet loss using multiple GPUs\nUsage: check_gor_HardNet.py\n\nAuthor: Xu Zhang\nEmail: xu.zhang@columbia.edu.cn\n""""""\n\n#! /usr/bin/env python2\n\nimport numpy as np\nimport scipy.io as sio\nimport time\nimport os\nimport sys\nimport pandas as pd\nimport subprocess\nimport shlex\nimport argparse\n####################################################################\n# Parse command line\n####################################################################\ndef usage():\n    print >> sys.stderr \n    sys.exit(1)\n\nclass cd:\n    """"""Context manager for changing the current working directory""""""\n    def __init__(self, newPath):\n        self.newPath = os.path.expanduser(newPath)\n\n    def __enter__(self):\n        self.savedPath = os.getcwd()\n        os.chdir(self.newPath)\n\n    def __exit__(self, etype, value, traceback):\n        os.chdir(self.savedPath)\n\ngpu_set = [\'0\',\'1\']\nparameter_set = [\'False\',\'True\']\nnumber_gpu = len(gpu_set)\n\n#datasets = [\'notredame\', \'yosemite\', \'liberty\']\ndatasets = [\'notredame\']\nprocess_set = []\n\n\nfor dataset in datasets:\n    for idx, parameter in enumerate(parameter_set):\n        print(\'Test Parameter: {}\'.format(parameter))\n        command = \'python HardNet.py --training-set {} --fliprot=False --n-triplets=1000000 --batch-size=128 --epochs 10 --gor={} --w1bsroot=None --gpu-id {} --log-dir ../ubc_log/ --enable-logging=True --batch-reduce=min --model-dir ../ubc_model/ \'\\\n                .format(dataset, parameter, gpu_set[idx%number_gpu])\n    \n        print(command)\n        p = subprocess.Popen(shlex.split(command))\n        process_set.append(p)\n        \n        if (idx+1)%number_gpu == 0:\n            print(\'Wait for process end\')\n            for sub_process in process_set:\n                sub_process.wait()\n        \n            process_set = []\n    \n        time.sleep(60)\n    \n    for sub_process in process_set:\n        sub_process.wait()\n\n'"
code/check_gor_triplet.py,0,"b'""""""\nCheck the correctness of gor on triple loss using multiple GPUs\nUsage: check_gor_triplet.py\n\nAuthor: Xu Zhang\nEmail: xu.zhang@columbia.edu.cn\n""""""\n\n#! /usr/bin/env python2\n\nimport numpy as np\nimport scipy.io as sio\nimport time\nimport os\nimport sys\nimport pandas as pd\nimport subprocess\nimport shlex\nimport argparse\n\n\ndef usage():\n    print >> sys.stderr \n    sys.exit(1)\n\nclass cd:\n    """"""Context manager for changing the current working directory""""""\n    def __init__(self, newPath):\n        self.newPath = os.path.expanduser(newPath)\n\n    def __enter__(self):\n        self.savedPath = os.getcwd()\n        os.chdir(self.newPath)\n\n    def __exit__(self, etype, value, traceback):\n        os.chdir(self.savedPath)\n\ngpu_set = [\'0\',\'1\']\nparameter_set = [\'False\',\'True\']\nnumber_gpu = len(gpu_set)\n\ndatasets = [\'notredame\', \'yosemite\', \'liberty\']\nprocess_set = []\n\n\nfor dataset in datasets:\n    for idx, parameter in enumerate(parameter_set):\n        print(\'Test Parameter: {}\'.format(parameter))\n        command = \'python HardNet.py --training-set {} --fliprot=False --n-triplets=1000000 --batch-size=128 --epochs 10 --gor={} --w1bsroot=None --gpu-id {} --log-dir ../ubc_triplet_log/  --enable-logging=True --batch-reduce=random_global --model-dir ../ubc_model/ --margin=0.5\'\\\n                .format(dataset, parameter, gpu_set[idx%number_gpu])\n    \n        print(command)\n        p = subprocess.Popen(shlex.split(command))\n        process_set.append(p)\n        \n        if (idx+1)%number_gpu == 0:\n            print(\'Wait for process end\')\n            for sub_process in process_set:\n                sub_process.wait()\n        \n            process_set = []\n    \n        time.sleep(60)\n    \n    for sub_process in process_set:\n        sub_process.wait()\n\n'"
code/dataset.py,25,"b'# Training settings\nimport os\nimport errno\nimport numpy as np\nfrom PIL import Image\nimport torchvision.datasets as dset\n\nimport sys\nfrom copy import deepcopy\nimport argparse\nimport math\nimport torch.utils.data as data\nimport torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nfrom tqdm import tqdm\nimport random\nimport cv2\nimport copy\nfrom Utils import str2bool\n\ndef find_files(_data_dir, _image_ext):\n    """"""Return a list with the file names of the images containing the patches\n    """"""\n    files = []\n    # find those files with the specified extension\n    for file_dir in os.listdir(_data_dir):\n        if file_dir.endswith(_image_ext):\n            files.append(os.path.join(_data_dir, file_dir))\n    return sorted(files)  # sort files in ascend order to keep relations\ndef np2torch(npr):\n    if len(npr.shape) == 4:\n        return torch.from_numpy(np.rollaxis(npr, 3, 1))\n    elif len(npr.shape) == 3:\n        torch.from_numpy(np.rollaxis(npr, 2, 0))\n    else:\n        return torch.from_numpy(npr)\ndef read_patch_file(fname, patch_w = 65, patch_h = 65, start_patch_idx = 0):\n    img = Image.open(fname).convert(\'RGB\')\n    width, height = img.size\n    #print (img.size, patch_w, patch_h)\n    assert ((height % patch_h == 0) and (width % patch_w == 0))\n    patch_idxs = []\n    patches = []\n    current_patch_idx = start_patch_idx\n    for y in range(0, height, patch_h):\n        patch_idxs.append([])\n        curr_patches = []\n        for x in range(0, width, patch_w):\n            patch = np.array(img.crop((x, y, x + patch_w, y + patch_h))).mean(axis = 2, keepdims = True)\n            #print(patch.astype(np.float32).std(), patch.mean())\n            if (patch.mean() != 0) and (patch.astype(np.float32).std() > 1e-2):\n                curr_patches.append(patch.astype(np.uint8))\n                patch_idxs[-1].append(current_patch_idx)\n                current_patch_idx+=1\n        if len(curr_patches) > 1:\n            patches = patches + curr_patches\n        else:\n            for i in range(len(curr_patches)):\n                current_patch_idx -=1\n            patch_idxs = patch_idxs[:-1] \n    return np2torch(np.array(patches)), patch_idxs, patch_idxs[-1][-1]\n\ndef read_image_dir(dir_name, ext, patch_w, patch_h, good_fnames):\n    fnames = find_files(dir_name, ext)\n    patches = []\n    idxs = []\n    current_max_idx = 0\n    for f in fnames:\n        if f.split(\'/\')[-1].replace(\'.png\', \'\') not in good_fnames:\n            continue\n        try:\n            torch_patches, p_idxs_list, max_idx = read_patch_file(f, patch_w, patch_h, current_max_idx)\n        except:\n            continue\n        current_max_idx = max_idx + 1\n        #if patches is None:\n        #    patches = torch_patches\n        #    idxs = p_idxs_list\n        #else:\n        patches.append(torch_patches)\n        idxs = idxs + p_idxs_list\n        print (f, len(idxs))\n    print( \'torch.cat\')\n    patches = torch.cat(patches, dim = 0)\n    print (\'done\')\n    return patches, idxs\n\n\nclass HPatchesDM(data.Dataset):\n    image_ext = \'png\'\n    def __init__(self, root, name, train=True, transform=None,\n                 download=True, pw = 65, ph = 65,\n                 n_pairs = 1000, batch_size = 128, split_name = \'b\'):\n        self.root = os.path.expanduser(root)\n        self.name = name\n        self.n_pairs = n_pairs\n        self.split_name = split_name\n        self.batch_size = batch_size\n        self.train = train\n        self.data_dir = os.path.join(self.root, name)\n        if self.train:\n            self.data_file = os.path.join(self.root, \'{}.pt\'.format(self.name  + \'_train\' ))\n        else:\n            self.data_file = os.path.join(self.root, \'{}.pt\'.format(self.name  + \'_test\' ))            \n        self.transform = transform\n        self.patch_h = ph\n        self.patch_w = pw\n        self.batch_size = batch_size\n        if download:\n            self.download()\n\n        if not self._check_datafile_exists():\n            raise RuntimeError(\'Dataset not found.\' +\n                               \' You can use download=True to download it\')\n\n        # load the serialized data\n        self.patches, self.idxs = torch.load(self.data_file)\n        print(\'Generating {} triplets\'.format(self.n_pairs))\n        self.pairs = self.generate_pairs(self.idxs, self.n_pairs)\n        return\n    def generate_pairs(self, labels, n_pairs):\n        pairs = []\n        n_classes = len(labels)\n        # add only unique indices in batch\n        already_idxs = set()\n        for x in tqdm(range(n_pairs)):\n            if len(already_idxs) >= self.batch_size:\n                already_idxs = set()\n            c1 = np.random.randint(0, n_classes)\n            while c1 in already_idxs:\n                c1 = np.random.randint(0, n_classes)\n            while len(labels[c1]) < 3:\n                c1 = np.random.randint(0, n_classes)\n            already_idxs.add(c1)\n            if len(labels[c1]) == 2:  # hack to speed up process\n                n1, n2 = 0, 1\n            else:\n                n1 = np.random.randint(0, len(labels[c1]))\n                while (self.patches[labels[c1][n1],:,:,:].float().std() < 1e-2):\n                    n1 = np.random.randint(0, len(labels[c1]))\n                n2 = np.random.randint(0, len(labels[c1]))\n                while (self.patches[labels[c1][n2],:,:,:].float().std() < 1e-2):\n                    n2 = np.random.randint(0, len(labels[c1]))\n            pairs.append([labels[c1][n1], labels[c1][n2]])\n        return torch.LongTensor(np.array(pairs))\n    def __getitem__(self, index):\n        def transform_pair(i1,i2):\n            if self.transform is not None:\n                return self.transform(i1.cpu().numpy()), self.transform(i2.cpu().numpy())\n            else:\n                return i1,i2\n        t = self.pairs[index]\n        a, p = self.patches[t[0],:,:,:], self.patches[t[1],:,:,:]\n        a1,p1 = transform_pair(a,p)\n        return (a1,p1)\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def _check_datafile_exists(self):\n        return os.path.exists(self.data_file)\n\n    def _check_downloaded(self):\n        return os.path.exists(self.data_dir)\n\n    def download(self):\n        if self._check_datafile_exists():\n            print(\'# Found cached data {}\'.format(self.data_file))\n            return\n        # process and save as torch files\n        print(\'# Caching data {}\'.format(self.data_file))\n        import json\n        from pprint import pprint\n        #print self.urls[\'splits\']\n        with open(os.path.join(self.root, \'splits.json\')) as splits_file:    \n            data = json.load(splits_file)\n        if self.train:\n            self.img_fnames = data[self.split_name][\'train\']\n        else:\n            self.img_fnames = data[self.split_name][\'test\']\n        dataset = read_image_dir(self.data_dir, self.image_ext, self.patch_w, self.patch_h, self.img_fnames)\n        print(\'saving...\')\n        with open(self.data_file, \'wb\') as f:\n            torch.save(dataset, f)\n        return\nclass TotalDatasetsLoader(data.Dataset):\n\n    def __init__(self, datasets_path, train = True, transform = None, batch_size = None, n_triplets = 5000000, fliprot = False, *arg, **kw):\n        super(TotalDatasetsLoader, self).__init__()\n\n        datasets_path = [os.path.join(datasets_path, dataset) for dataset in os.listdir(datasets_path)]\n\n        datasets = [torch.load(dataset) for dataset in datasets_path]\n\n        data, labels = datasets[0][0], datasets[0][1]\n\n        for i in range(1,len(datasets)):\n            data = torch.cat([data,datasets[i][0]])\n            labels = torch.cat([labels, datasets[i][1]+torch.max(labels)+1])\n\n        del datasets\n\n        self.data, self.labels = data, labels\n        self.transform = transform\n        self.train = train\n        self.n_triplets = n_triplets\n        self.batch_size = batch_size\n        self.fliprot = fliprot\n        if self.train:\n                print(\'Generating {} triplets\'.format(self.n_triplets))\n                self.triplets = self.generate_triplets(self.labels, self.n_triplets, self.batch_size)\n\n\n    def generate_triplets(self, labels, num_triplets, batch_size):\n            def create_indices(_labels):\n                inds = dict()\n                for idx, ind in enumerate(_labels):\n                    if ind not in inds:\n                        inds[ind] = []\n                    inds[ind].append(idx)\n                return inds\n\n            triplets = []\n            indices = create_indices(labels)\n            unique_labels = np.unique(labels.numpy())\n            n_classes = unique_labels.shape[0]\n            # add only unique indices in batch\n            already_idxs = set()\n\n            for x in tqdm(range(num_triplets)):\n                if len(already_idxs) >= batch_size:\n                    already_idxs = set()\n                c1 = np.random.randint(0, n_classes)\n                while c1 in already_idxs:\n                    c1 = np.random.randint(0, n_classes)\n                already_idxs.add(c1)\n                c2 = np.random.randint(0, n_classes)\n                while c1 == c2:\n                    c2 = np.random.randint(0, n_classes)\n                if len(indices[c1]) == 2:  # hack to speed up process\n                    n1, n2 = 0, 1\n                else:\n                    n1 = np.random.randint(0, len(indices[c1]))\n                    n2 = np.random.randint(0, len(indices[c1]))\n                    while n1 == n2:\n                        n2 = np.random.randint(0, len(indices[c1]))\n                n3 = np.random.randint(0, len(indices[c2]))\n                triplets.append([indices[c1][n1], indices[c1][n2], indices[c2][n3]])\n            return torch.LongTensor(np.array(triplets))\n\n    def __getitem__(self, index):\n            def transform_img(img):\n                if self.transform is not None:\n                    img = (img.numpy())/255.0\n                    img = self.transform(img)\n                return img\n\n            t = self.triplets[index]\n            a, p, n = self.data[t[0]], self.data[t[1]], self.data[t[2]]\n\n            img_a = transform_img(a)\n            img_p = transform_img(p)\n\n            # transform images if required\n            if self.fliprot:\n                do_flip = random.random() > 0.5\n                do_rot = random.random() > 0.5\n\n                if do_rot:\n                    img_a = img_a.permute(0,2,1)\n                    img_p = img_p.permute(0,2,1)\n\n                if do_flip:\n                    img_a = torch.from_numpy(deepcopy(img_a.numpy()[:,:,::-1]))\n                    img_p = torch.from_numpy(deepcopy(img_p.numpy()[:,:,::-1]))\n            return img_a, img_p\n\n    def __len__(self):\n            if self.train:\n                return self.triplets.size(0)\n\nclass TripletPhotoTour(dset.PhotoTour):\n    """"""From the PhotoTour Dataset it generates triplet samples\n    note: a triplet is composed by a pair of matching images and one of\n    different class.\n    """"""\n    urls = {\n        \'notredame_harris\': [\n            \'http://matthewalunbrown.com/patchdata/notredame_harris.zip\',\n            \'notredame_harris.zip\',\n            \'69f8c90f78e171349abdf0307afefe4d\'\n        ],\n        \'yosemite_harris\': [\n            \'http://matthewalunbrown.com/patchdata/yosemite_harris.zip\',\n            \'yosemite_harris.zip\',\n            \'a73253d1c6fbd3ba2613c45065c00d46\'\n        ],\n        \'liberty_harris\': [\n            \'http://matthewalunbrown.com/patchdata/liberty_harris.zip\',\n            \'liberty_harris.zip\',\n            \'c731fcfb3abb4091110d0ae8c7ba182c\'\n        ],\n        \'notredame\': [\n            \'http://icvl.ee.ic.ac.uk/vbalnt/notredame.zip\',\n            \'notredame.zip\',\n            \'509eda8535847b8c0a90bbb210c83484\'\n        ],\n        \'yosemite\': [\n            \'http://icvl.ee.ic.ac.uk/vbalnt/yosemite.zip\',\n            \'yosemite.zip\',\n            \'533b2e8eb7ede31be40abc317b2fd4f0\'\n        ],\n        \'liberty\': [\n            \'http://icvl.ee.ic.ac.uk/vbalnt/liberty.zip\',\n            \'liberty.zip\',\n            \'fdd9152f138ea5ef2091746689176414\'\n        ],\n    }\n    mean = {\'notredame\': 0.4854, \'yosemite\': 0.4844, \'liberty\': 0.4437, \'notredame_harris\': 0.4854, \'yosemite_harris\': 0.4844, \'liberty_harris\': 0.4437}\n    std = {\'notredame\': 0.1864, \'yosemite\': 0.1818, \'liberty\': 0.2019, \'notredame_harris\': 0.1864, \'yosemite_harris\': 0.1818, \'liberty_harris\': 0.2019}\n    lens = {\'notredame\': 468159, \'yosemite\': 633587, \'liberty\': 450092, \'liberty_harris\': 379587, \'yosemite_harris\': 450912 , \'notredame_harris\': 325295}\n    def __init__(self, train=True, transform=None, batch_size = None, n_triplets = 5000, load_random_triplets = False,  *arg, **kw):\n        super(TripletPhotoTour, self).__init__(*arg, **kw)\n        self.transform = transform\n        self.out_triplets = load_random_triplets\n        self.train = train\n        self.n_triplets = 1000\n        self.batch_size = batch_size\n\n        if self.train:\n            print(\'Generating {} triplets\'.format(self.n_triplets))\n            self.triplets = self.generate_triplets(self.labels, self.n_triplets)\n    def generate_triplets(self,labels, num_triplets):\n        def create_indices(_labels):\n            inds = dict()\n            for idx, ind in enumerate(_labels):\n                if ind not in inds:\n                    inds[ind] = []\n                inds[ind].append(idx)\n            return inds\n        triplets = []\n        indices = create_indices(labels)\n        unique_labels = np.unique(labels.numpy())\n        n_classes = unique_labels.shape[0]\n        # add only unique indices in batch\n        already_idxs = set()\n        for x in tqdm(range(num_triplets)):\n            if len(already_idxs) >= self.batch_size:\n                already_idxs = set()\n            c1 = np.random.randint(0, n_classes - 1)\n            while c1 in already_idxs:\n                c1 = np.random.randint(0, n_classes - 1)\n            already_idxs.add(c1)\n            c2 = np.random.randint(0, n_classes - 1)\n            while c1 == c2:\n                c2 = np.random.randint(0, n_classes - 1)\n            if len(indices[c1]) == 2:  # hack to speed up process\n                n1, n2 = 0, 1\n            else:\n                n1 = np.random.randint(0, len(indices[c1]) - 1)\n                n2 = np.random.randint(0, len(indices[c1]) - 1)\n                while n1 == n2:\n                    n2 = np.random.randint(0, len(indices[c1]) - 1)\n            n3 = np.random.randint(0, len(indices[c2]) - 1)\n            triplets.append([indices[c1][n1], indices[c1][n2], indices[c2][n3]])\n        return torch.LongTensor(np.array(triplets))\n    def __getitem__(self, index):\n        def transform_img(img):\n            if self.transform is not None:\n                img = self.transform(img.numpy())\n            return img\n\n        if not self.train:\n            m = self.matches[index]\n            img1 = transform_img(self.data[m[0]])\n            img2 = transform_img(self.data[m[1]])\n            return img1, img2, m[2]\n\n        t = self.triplets[index]\n        a, p, n = self.data[t[0]], self.data[t[1]], self.data[t[2]]\n        img_a = transform_img(a)\n        img_p = transform_img(p)\n        img_n = None\n        if self.out_triplets:\n            img_n = transform_img(n)\n        # transform images if required\n        if True:#args.fliprot:\n            do_flip = random.random() > 0.5\n            do_rot = random.random() > 0.5\n            if do_rot:\n                img_a = img_a.permute(0,2,1)\n                img_p = img_p.permute(0,2,1)\n                if self.out_triplets:\n                    img_n = img_n.permute(0,2,1)\n            if do_flip:\n                img_a = torch.from_numpy(deepcopy(img_a.numpy()[:,:,::-1]))\n                img_p = torch.from_numpy(deepcopy(img_p.numpy()[:,:,::-1]))\n                if self.out_triplets:\n                    img_n = torch.from_numpy(deepcopy(img_n.numpy()[:,:,::-1]))\n        if self.out_triplets:\n            return (img_a, img_p, img_n)\n        else:\n            return (img_a, img_p)\n\n    def __len__(self):\n        if self.train:\n            return self.triplets.size(0)\n        else:\n            return self.matches.size(0)\n\n'"
code/download_all_datasets.py,13,"b""\nimport os\nimport errno\nimport numpy as np\nfrom PIL import Image\nimport torchvision.datasets as dset\n\nimport sys\nfrom copy import deepcopy\nimport argparse\nimport math\nimport torch.utils.data as data\nimport torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nfrom tqdm import tqdm\nimport random\nimport cv2\nimport copy\nfrom Utils import str2bool\n\nfrom dataset import TripletPhotoTour\nroot='../data/sets'\ntrain_loader = torch.utils.data.DataLoader(\n        TripletPhotoTour(train=True,\n                         batch_size=128,\n                         root=root,\n                         name='notredame',\n                         download=True,\n                         transform=None),\n                         batch_size=128,\n                         shuffle=False)\ntrain_loader = torch.utils.data.DataLoader(\n        TripletPhotoTour(train=True,\n                         batch_size=128,\n                         root=root,\n                         name='yosemite',\n                         download=True,\n                         transform=None),\n                         batch_size=128,\n                         shuffle=False)\ntrain_loader = torch.utils.data.DataLoader(\n        TripletPhotoTour(train=True,\n                         batch_size=128,\n                         root=root,\n                         name='liberty',\n                         download=True,\n                         transform=None),\n                         batch_size=128,\n                         shuffle=False)\ntrain_loader = torch.utils.data.DataLoader(\n        TripletPhotoTour(train=True,\n                         batch_size=128,\n                         root=root,\n                         name='notredame_harris',\n                         download=True,\n                         transform=None),\n                         batch_size=128,\n                         shuffle=False)\ntrain_loader = torch.utils.data.DataLoader(\n        TripletPhotoTour(train=True,\n                         batch_size=128,\n                         root=root,\n                         name='yosemite_harris',\n                         download=True,\n                         transform=None),\n                         batch_size=128,\n                         shuffle=False)\ntrain_loader = torch.utils.data.DataLoader(\n        TripletPhotoTour(train=True,\n                         batch_size=128,\n                         root=root,\n                         name='liberty_harris',\n                         download=True,\n                         transform=None),\n                         batch_size=128,\n                         shuffle=False)\n"""
examples/extract_DenseHardNet.py,9,"b'#!/usr/bin/python2 -utt\n# -*- coding: utf-8 -*-\nimport sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport time\nimport os\nsys.path.insert(0, \'/home/ubuntu/dev/opencv-3.1/build/lib\')\nimport cv2\nimport math\nimport numpy as np\nfrom PIL import Image\n\nclass L2Norm(nn.Module):\n    def __init__(self):\n        super(L2Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sqrt(torch.sum(x * x, dim = 1, keepdim = True) + self.eps)\n        x= x / norm.expand_as(x)\n        return x\n\nclass LocalNorm2d(nn.Module):\n    def __init__(self, kernel_size = 32):\n        super(LocalNorm2d, self).__init__()\n        self.ks = kernel_size\n        self.pool = nn.AvgPool2d(kernel_size = self.ks, stride = 1,  padding = 0)\n        self.eps = 1e-10\n        return\n    def forward(self,x):\n        pd = int(self.ks/2)\n        mean = self.pool(F.pad(x, (pd,pd,pd,pd), \'reflect\'))\n        return torch.clamp((x - mean) / (torch.sqrt(torch.abs(self.pool(F.pad(x*x,  (pd,pd,pd,pd), \'reflect\')) - mean*mean )) + self.eps), min = -6.0, max = 6.0)\n    \nclass DenseHardNet(nn.Module):\n    """"""HardNet model definition\n    """"""\n    def __init__(self, _stride = 2):\n        super(DenseHardNet, self).__init__()\n        self.input_norm = LocalNorm2d(17)\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=_stride, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=_stride,padding=1, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Conv2d(128, 128, kernel_size=8, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n            L2Norm()\n        )\n        return\n\n    def forward(self, input, upscale = False):\n        if input.size(1) > 1:\n            feats = self.features(self.input_norm(input.mean(dim = 1, keepdim = True)))\n        else:\n            feats = self.features(self.input_norm(input))\n        if upscale:\n            return F.upsample(feats, (input.size(2), input.size(3)),mode=\'bilinear\')\n        return feats\n    \ndef load_grayscale_var(fname):\n    img = Image.open(fname).convert(\'RGB\')\n    img = np.mean(np.array(img), axis = 2)\n    var_image = torch.autograd.Variable(torch.from_numpy(img.astype(np.float32)), volatile = True)\n    var_image_reshape = var_image.view(1, 1, var_image.size(0),var_image.size(1))\n    return var_image_reshape\n\nif __name__ == \'__main__\':\n    DO_CUDA = True\n    UPSCALE = False\n    stride = 2;\n    try:\n          input_img_fname = sys.argv[1]\n          output_fname = sys.argv[2]\n          if len(sys.argv) > 3:\n              DO_CUDA = sys.argv[3] != \'cpu\'\n          if len(sys.argv) > 4:\n              UPSCALE = sys.argv[4] == \'UPSCALE\'\n              if  sys.argv[4] == \'NOSTRIDE\':\n                  stride = 1\n              \n    except:\n          print(""Wrong input format. Try ./extract_DenseHardNet.py imgs/ref.png out.txt gpu"")\n          sys.exit(1)\n    model_weights = \'../pretrained/pretrained_all_datasets/HardNet++.pth\'\n    model = DenseHardNet(stride)\n    checkpoint = torch.load(model_weights)\n    model.load_state_dict(checkpoint[\'state_dict\'])\n    model.eval()\n    img = load_grayscale_var(input_img_fname)\n    if DO_CUDA:\n        model.cuda()\n        img = img.cuda()\n        print(\'Extracting on GPU\')\n    else:\n        print(\'Extracting on CPU\')\n        model = model.cpu()\n    t = time.time()\n    with torch.no_grad():\n        desc = model(img, UPSCALE)\n    et  = time.time() - t\n    print(\'processing\', et)\n    desc_numpy = desc.cpu().detach().float().squeeze().numpy();\n    desc_numpy = np.clip(((desc_numpy + 0.45) * 210.0).astype(np.int32), 0, 255).astype(np.uint8)\n    print(desc_numpy.shape)\n    np.save(output_fname, desc_numpy)\n'"
examples/extract_hardnet_desc_from_hpatches_file.py,11,"b'#!/usr/bin/python2 -utt\n# -*- coding: utf-8 -*-\nimport sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport time\nimport os\nimport cv2\nimport math\nimport numpy as np\n\nclass L2Norm(nn.Module):\n    def __init__(self):\n        super(L2Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sqrt(torch.sum(x * x, dim = 1) + self.eps)\n        x= x / norm.unsqueeze(-1).expand_as(x)\n        return x\n\nclass L1Norm(nn.Module):\n    def __init__(self):\n        super(L1Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sum(torch.abs(x), dim = 1) + self.eps\n        x= x / norm.expand_as(x)\n        return x\n\nclass HardNet(nn.Module):\n    """"""HardNet model definition\n    """"""\n    def __init__(self):\n        super(HardNet, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2,padding=1, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Conv2d(128, 128, kernel_size=8, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n\n        )\n        #self.features.apply(weights_init)\n\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n\n    def forward(self, input):\n        x_features = self.features(self.input_norm(input))\n        x = x_features.view(x_features.size(0), -1)\n        return L2Norm()(x)\n    \n\nif __name__ == \'__main__\':\n    DO_CUDA = True\n    try:\n          input_img_fname = sys.argv[1]\n          output_fname = sys.argv[2]\n          if len(sys.argv) > 3:\n              DO_CUDA = sys.argv[3] != \'cpu\'\n    except:\n          print(""Wrong input format. Try ./extract_hardnet_desc_from_hpatches_file.py imgs/ref.png out.txt gpu"")\n          sys.exit(1)\n    model_weights = \'../pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth\'\n    model = HardNet()\n    checkpoint = torch.load(model_weights)\n    model.load_state_dict(checkpoint[\'state_dict\'])\n    model.eval()\n    if DO_CUDA:\n        model.cuda()\n        print(\'Extracting on GPU\')\n    else:\n        print(\'Extracting on CPU\')\n        model = model.cpu()\n    image = cv2.imread(input_img_fname,0)\n    h,w = image.shape\n    print(h,w)\n\n    n_patches =  int(h/w)\n\n    print(\'Amount of patches: {}\'.format(n_patches))\n\n    t = time.time()\n    patches = np.ndarray((n_patches, 1, 32, 32), dtype=np.float32)\n    for i in range(n_patches):\n        patch =  image[i*(w): (i+1)*(w), 0:w]\n        patches[i,0,:,:] = cv2.resize(patch,(32,32)) / 255.\n    patches -= 0.443728476019\n    patches /= 0.20197947209\n    bs = 128\n    outs = []\n    n_batches = int(n_patches / bs) + 1\n    t = time.time()\n    descriptors_for_net = np.zeros((len(patches), 128))\n    for i in range(0, len(patches), bs):\n        data_a = patches[i: i + bs, :, :, :].astype(np.float32)\n        data_a = torch.from_numpy(data_a)\n        if DO_CUDA:\n            data_a = data_a.cuda()\n        data_a = Variable(data_a)\n        # compute output\n        with torch.no_grad():\n            out_a = model(data_a)\n        descriptors_for_net[i: i + bs,:] = out_a.data.cpu().numpy().reshape(-1, 128)\n    print(descriptors_for_net.shape)\n    assert n_patches == descriptors_for_net.shape[0]\n    et  = time.time() - t\n    print(\'processing\', et, et/float(n_patches), \' per patch\')\n    np.savetxt(output_fname, descriptors_for_net, delimiter=\' \', fmt=\'%10.5f\')\n'"
code/dataloaders/HPatchesDatasetCreator.py,3,"b'import os\nimport numpy as np\nimport torch\nimport torch.utils.data as data\nimport cv2\nimport sys\nimport json\n\ntypes = [\'e1\',\'e2\',\'e3\',\'e4\',\'e5\',\'ref\',\'h1\',\'h2\',\'h3\',\'h4\',\'h5\', \'t1\', \'t2\', \'t3\', \'t4\', \'t5\']\nsplits = [\'a\', \'b\', \'c\', \'view\', \'illum\']\n\n#images_to_exclude = [\'v_adam\', \'v_boat\', \'v_graffiti\', \'v_there\',\'i_dome\']\n\ndef mean_image(patches):\n    mean = np.mean(patches)\n    return mean\n\ndef std_image(patches):\n    std = np.std(patches)\n    return std\n\nclass HPatches(data.Dataset):\n\n    def __init__(self, train=True, transform=None, download=False, good_fnames = []):\n        self.train = train\n        self.transform = transform\n\n    def read_image_file(self, data_dir):\n        """"""Return a Tensor containing the patches\n        """"""\n        patches = []\n        labels = []\n        counter = 0\n        hpatches_sequences = [x[1] for x in os.walk(data_dir)][0]\n        for directory in hpatches_sequences:\n           if (directory in good_fnames):\n            print(directory)\n            for type in types:\n                sequence_path = os.path.join(data_dir, directory,type)+\'.png\'\n                image = cv2.imread(sequence_path, 0)\n                h, w = image.shape\n                n_patches = int(h / w)\n                for i in range(n_patches):\n                    patch = image[i * (w): (i + 1) * (w), 0:w]\n                    patch = cv2.resize(patch, (64, 64))\n                    patch = np.array(patch, dtype=np.uint8)\n                    patches.append(patch)\n                    labels.append(i+counter)\n            counter += n_patches\n        print counter\n        return torch.ByteTensor(np.array(patches, dtype=np.uint8)), torch.LongTensor(labels)\n\nif __name__ == \'__main__\':\n    # need to be specified\n    try:\n        path_to_hpatches_dir = sys.argv[1]\n        path_to_splits_json = sys.argv[2]\n        output_dir  = sys.argv[3]\n    except:\n        print ""Wrong input format. Try python HPatchesDatasetCreator.py path_to_hpatches path_to_splits_json output_dir""\n        sys.exit(1)\n    splits_json = json.load(open(path_to_splits_json, \'rb\'))\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    for split in splits:\n        if len(split) == 1:\n            t = \'train\'\n        else:\n            t = \'test\'# view and illum are kind of train/test for each other\n        good_fnames = splits_json[split][t]\n        hPatches = HPatches(good_fnames = good_fnames)\n        images, labels = hPatches.read_image_file(path_to_hpatches_dir)\n        with open(os.path.join(output_dir, \'hpatches_split_\' + split +  \'_\' + t + \'.pt\'), \'wb\') as f:\n            torch.save((images, labels), f)\n        print(split, t, \'Saved\')\n'"
code/dataloaders/TotalDataLoader.py,7,"b'import os\nimport numpy as np\nimport torch\nimport torch.utils.data as data\nimport tqdm\nimport random\nfrom copy import deepcopy\n\nclass TotalDatasetsLoader(data.Dataset):\n\n    def __init__(self, datasets_path, train = True, transform = None, batch_size = None, n_triplets = 100000, fliprot = False, *arg, **kw):\n        super(TotalDatasetsLoader, self).__init__(*arg, **kw)\n\n        datasets = [torch.load(dataset) for dataset in datasets_path]\n        data, labels = datasets[0][0], datasets[0][1]\n\n        for i in range(1,len(datasets)):\n            data = torch.cat([data,datasets[i][0]])\n            labels = torch.cat([labels, datasets[i][1]+torch.max(labels)+1])\n\n        del datasets\n        self.transform = transform\n        self.train = train\n        self.n_triplets = n_triplets\n        self.batch_size = batch_size\n\n        if self.train:\n                print(\'Generating {} triplets\'.format(self.n_triplets))\n                self.triplets = self.generate_triplets(self.labels, self.n_triplets)\n\n        @staticmethod\n        def generate_triplets(labels, num_triplets):\n            def create_indices(_labels):\n                inds = dict()\n                for idx, ind in enumerate(_labels):\n                    if ind not in inds:\n                        inds[ind] = []\n                    inds[ind].append(idx)\n                return inds\n\n            triplets = []\n            indices = create_indices(labels)\n            unique_labels = np.unique(labels.numpy())\n            n_classes = unique_labels.shape[0]\n            # add only unique indices in batch\n            already_idxs = set()\n\n            for x in tqdm(range(num_triplets)):\n                if len(already_idxs) >= batch_size:\n                    already_idxs = set()\n                c1 = np.random.randint(0, n_classes)\n                while c1 in already_idxs:\n                    c1 = np.random.randint(0, n_classes)\n                already_idxs.add(c1)\n                c2 = np.random.randint(0, n_classes)\n                while c1 == c2:\n                    c2 = np.random.randint(0, n_classes)\n                if len(indices[c1]) == 2:  # hack to speed up process\n                    n1, n2 = 0, 1\n                else:\n                    n1 = np.random.randint(0, len(indices[c1]))\n                    n2 = np.random.randint(0, len(indices[c1]))\n                    while n1 == n2:\n                        n2 = np.random.randint(0, len(indices[c1]))\n                n3 = np.random.randint(0, len(indices[c2]))\n                triplets.append([indices[c1][n1], indices[c1][n2], indices[c2][n3]])\n            return torch.LongTensor(np.array(triplets))\n\n        def __getitem__(self, index):\n            def transform_img(img):\n                if self.transform is not None:\n                    img = (img.numpy())/255.0\n                    img = self.transform(img)\n                return img\n\n            t = self.triplets[index]\n            a, p, n = self.data[t[0]], self.data[t[1]], self.data[t[2]]\n\n            img_a = transform_img(a)\n            img_p = transform_img(p)\n\n            # transform images if required\n            if fliprot:\n                do_flip = random.random() > 0.5\n                do_rot = random.random() > 0.5\n\n                if do_rot:\n                    img_a = img_a.permute(0,2,1)\n                    img_p = img_p.permute(0,2,1)\n\n                if do_flip:\n                    img_a = torch.from_numpy(deepcopy(img_a.numpy()[:,:,::-1]))\n                    img_p = torch.from_numpy(deepcopy(img_p.numpy()[:,:,::-1]))\n            return img_a, img_p\n\n        def __len__(self):\n            if self.train:\n                return self.triplets.size(0)\n\n\nif __name__ == \'__main__\':\n    datasets_path = os.path.join(os.path.abspath(__file__ + ""/../../../""),\'datasets\')\n    datasets = [os.path.join(datasets_path, dataset) for dataset in os.listdir(datasets_path)]\n    TotalDatasetsLoader(datasets)'"
code/dataloaders/__init__.py,0,b''
examples/caffe/convert_weights_to_caffe.py,10,"b'import numpy as np\n# Make sure that caffe is on the python path:\ncaffe_root = \'/home/ubuntu/dev/caffe/\'\nimport sys\nsys.path.insert(0, caffe_root + \'python\')\nimport caffe\n\n\nimport sys\nimport argparse\nimport torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport time\nimport os\nimport sys\nsys.path.insert(0, \'/home/ubuntu/dev/opencv-3.1/build/lib\')\nimport cv2\nimport math\nimport numpy as np\nfrom copy import deepcopy\nimport random\nimport time\nimport numpy as np\nimport glob\nimport os\n\nclass L2Norm(nn.Module):\n    def __init__(self):\n        super(L2Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sqrt(torch.sum(x * x, dim = 1) + self.eps)\n        x= x / norm.unsqueeze(-1).expand_as(x)\n        return x\n\nclass L1Norm(nn.Module):\n    def __init__(self):\n        super(L1Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sum(torch.abs(x), dim = 1) + self.eps\n        x= x / norm.expand_as(x)\n        return x\n\nclass HardNet(nn.Module):\n    """"""HardNet model definition\n    """"""\n    def __init__(self):\n        super(HardNet, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2,padding=1, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Conv2d(128, 128, kernel_size=8, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n\n        )\n        #self.features.apply(weights_init)\n\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n\n    def forward(self, input):\n        x_features = self.features(self.input_norm(input))\n        x = x_features.view(x_features.size(0), -1)\n        return L2Norm()(x)\ngg = {}\ngg[\'counter\'] = 1\ndef copy_weights(m,):\n    if isinstance(m, nn.Conv2d):\n        counter = gg[\'counter\']\n        l_name = \'conv\' + str(counter)\n        print l_name,  m.weight.data.cpu().numpy().shape;\n        net.params[l_name][0].data[:] = m.weight.data.cpu().numpy();\n        #try:\n        #    net.params[l_name][1].data[:] = m.bias.data.cpu().numpy();\n        #except:\n        #    pass\n    if isinstance(m, nn.BatchNorm2d):\n        counter = gg[\'counter\']\n        l_name = \'conv\' + str(counter) + \'_BN\'\n        print l_name\n        net.params[l_name][0].data[:] = m.running_mean.cpu().numpy();\n        net.params[l_name][1].data[:] = m.running_var.cpu().numpy();\n        net.params[l_name][2].data[:] = 1.\n        gg[\'counter\'] += 1\n\nmodel = HardNet()\nmodel.cuda()\n    \nmws = [\n""../../pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth"",\n#""../../pretrained/train_yosemite/checkpoint_yosemite_no_aug.pth"",\n""../../pretrained/train_liberty/checkpoint_liberty_no_aug.pth"",\n#""../../pretrained/train_notredame_with_aug/checkpoint_notredame_with_aug.pth"",\n#""../../pretrained/train_notredame/checkpoint_notredame_no_aug.pth"",\n#""../../pretrained/train_yosemite_with_aug/checkpoint_yosemite_with_aug.pth"",\n#""../../pretrained/pretrained_all_datasets/HardNet++.pth""\n""../../pretrained/6Brown/hardnetBr6.pth""\n    ]\n\nfor model_weights in mws:\n    gg[\'counter\'] = 1\n    print(model_weights)\n    checkpoint = torch.load(model_weights)\n    model.load_state_dict(checkpoint[\'state_dict\'])\n    model.eval()\n    caffe.set_mode_cpu()\n    net = None\n    net = caffe.Net(\'HardNet.prototxt\', caffe.TEST)\n    model.features.apply(copy_weights)\n    caffe_weights_fname = model_weights.split(\'/\')[-1].replace(\'.pth\', \'.caffemodel\')\n    net.save(caffe_weights_fname)\n\n    \n'"
examples/caffe/extract_hardnetCaffe_desc_from_hpatches_file.py,0,"b'#!/usr/bin/python2 -utt\n# -*- coding: utf-8 -*-\nimport sys\nimport numpy as np\ntry:\n    caffe_root = \'/home/ubuntu/dev/caffe/\' \n    sys.path.insert(0, caffe_root + \'python\')\n    sys.path.insert(0, \'/home/ubuntu/dev/opencv-3.1/build/lib\')\n    import caffe\n    caffe_imported = True\nexcept ImportError:\n    caffe_imported = None\nif caffe_imported is None:\n    raise ImportError(""Please, install caffe."")\nimport os\nimport sys\nimport time\nimport cv2\nPATCH_SIZE = 32\nDESC_SIZE = 128\nBATCH_SIZE = 256\nPROTOTXT_FNAME = \'HardNet.prototxt\'\nWEIGHT_FNAME = \'HardNet++.caffemodel\'\n\n\ndef preprocess_patch(patch):\n    out = cv2.resize(patch, (PATCH_SIZE, PATCH_SIZE)).astype(np.float32) \n    return out.reshape(1, PATCH_SIZE, PATCH_SIZE)\n\ndef extract_tfeats(net,patches):\n    n_desc = len(patches)\n    n_batches = int(np.ceil(float(n_desc) / BATCH_SIZE))\n    descriptors = np.zeros((n_desc,DESC_SIZE))\n    for i in range(n_batches):\n        start = i * BATCH_SIZE\n        if i < n_batches - 1:\n            end = (i + 1) * BATCH_SIZE\n        else:\n            end = n_desc\n        current_batch_size = end - start;\n        currect_patches = patches[start:end, :, :, :]\n        net.blobs[\'data\'].reshape(current_batch_size, 1, PATCH_SIZE, PATCH_SIZE)\n        net.blobs[\'data\'].data[...] = currect_patches\n        descriptors[start:end,:] = net.forward()[\'final\'].reshape(current_batch_size, DESC_SIZE)\n    return descriptors\n\nif __name__ == \'__main__\':\n    if len(sys.argv) < 2:\n        print(\'Wrong input format. Try python extract_hardnetCaffe_desc_from_hpatches_file.py ../../imgs/ref.png ref_caffe.TFEAT\')\n        sys.exit(1)\n\n    input_img_fname = sys.argv[1]\n    output_fname = sys.argv[2]\n\n    t = time.time()\n    image = cv2.imread(input_img_fname,0) #hpatch image is patch column 65*n x 65\n    h,w = image.shape\n    n_patches = h/w\n    print(\'{0} patches to describe in {1}\'.format(n_patches, input_img_fname))\n    patches = np.zeros((n_patches,1,PATCH_SIZE, PATCH_SIZE))\n    for i in range(n_patches):\n        patches[i,:,:,:] = preprocess_patch(image[i*(w): (i+1)*(w), 0:w]) \n    caffe.set_mode_gpu()\n    net = None\n    net = caffe.Net(PROTOTXT_FNAME, caffe.TEST, weights = WEIGHT_FNAME)\n    print \'Initialization and preprocessing time\', time.time() - t\n    t = time.time()\n    out_descs = extract_tfeats(net,patches)\n    print \'extraction time\', time.time() - t \n    np.savetxt(output_fname, out_descs, delimiter=\' \', fmt=\'%10.5f\')    \n'"
