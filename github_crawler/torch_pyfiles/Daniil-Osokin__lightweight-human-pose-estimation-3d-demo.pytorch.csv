file_path,api_count,code
demo.py,0,"b'from argparse import ArgumentParser\nimport json\nimport os\n\nimport cv2\nimport numpy as np\n\nfrom modules.input_reader import VideoReader, ImageReader\nfrom modules.draw import Plotter3d, draw_poses\nfrom modules.parse_poses import parse_poses\n\n\ndef rotate_poses(poses_3d, R, t):\n    R_inv = np.linalg.inv(R)\n    for pose_id in range(len(poses_3d)):\n        pose_3d = poses_3d[pose_id].reshape((-1, 4)).transpose()\n        pose_3d[0:3, :] = np.dot(R_inv, pose_3d[0:3, :] - t)\n        poses_3d[pose_id] = pose_3d.transpose().reshape(-1)\n\n    return poses_3d\n\n\nif __name__ == \'__main__\':\n    parser = ArgumentParser(description=\'Lightweight 3D human pose estimation demo. \'\n                                        \'Press esc to exit, ""p"" to (un)pause video or process next image.\')\n    parser.add_argument(\'-m\', \'--model\',\n                        help=\'Required. Path to checkpoint with a trained model \'\n                             \'(or an .xml file in case of OpenVINO inference).\',\n                        type=str, required=True)\n    parser.add_argument(\'--video\', help=\'Optional. Path to video file or camera id.\', type=str, default=\'\')\n    parser.add_argument(\'-d\', \'--device\',\n                        help=\'Optional. Specify the target device to infer on: CPU or GPU. \'\n                             \'The demo will look for a suitable plugin for device specified \'\n                             \'(by default, it is GPU).\',\n                        type=str, default=\'GPU\')\n    parser.add_argument(\'--use-openvino\',\n                        help=\'Optional. Run network with OpenVINO as inference engine. \'\n                             \'CPU, GPU, FPGA, HDDL or MYRIAD devices are supported.\',\n                        action=\'store_true\')\n    parser.add_argument(\'--images\', help=\'Optional. Path to input image(s).\', nargs=\'+\', default=\'\')\n    parser.add_argument(\'--height-size\', help=\'Optional. Network input layer height size.\', type=int, default=256)\n    parser.add_argument(\'--extrinsics-path\',\n                        help=\'Optional. Path to file with camera extrinsics.\',\n                        type=str, default=None)\n    parser.add_argument(\'--fx\', type=np.float32, default=-1, help=\'Optional. Camera focal length.\')\n    args = parser.parse_args()\n\n    if args.video == \'\' and args.images == \'\':\n        raise ValueError(\'Either --video or --image has to be provided\')\n\n    stride = 8\n    if args.use_openvino:\n        from modules.inference_engine_openvino import InferenceEngineOpenVINO\n        net = InferenceEngineOpenVINO(args.model, args.device)\n    else:\n        from modules.inference_engine_pytorch import InferenceEnginePyTorch\n        net = InferenceEnginePyTorch(args.model, args.device)\n\n    canvas_3d = np.zeros((720, 1280, 3), dtype=np.uint8)\n    plotter = Plotter3d(canvas_3d.shape[:2])\n    canvas_3d_window_name = \'Canvas 3D\'\n    cv2.namedWindow(canvas_3d_window_name)\n    cv2.setMouseCallback(canvas_3d_window_name, Plotter3d.mouse_callback)\n\n    file_path = args.extrinsics_path\n    if file_path is None:\n        file_path = os.path.join(\'data\', \'extrinsics.json\')\n    with open(file_path, \'r\') as f:\n        extrinsics = json.load(f)\n    R = np.array(extrinsics[\'R\'], dtype=np.float32)\n    t = np.array(extrinsics[\'t\'], dtype=np.float32)\n\n    frame_provider = ImageReader(args.images)\n    is_video = False\n    if args.video != \'\':\n        frame_provider = VideoReader(args.video)\n        is_video = True\n    base_height = args.height_size\n    fx = args.fx\n\n    delay = 1\n    esc_code = 27\n    p_code = 112\n    space_code = 32\n    mean_time = 0\n    for frame in frame_provider:\n        current_time = cv2.getTickCount()\n        if frame is None:\n            break\n        input_scale = base_height / frame.shape[0]\n        scaled_img = cv2.resize(frame, dsize=None, fx=input_scale, fy=input_scale)\n        scaled_img = scaled_img[:, 0:scaled_img.shape[1] - (scaled_img.shape[1] % stride)]  # better to pad, but cut out for demo\n        if fx < 0:  # Focal length is unknown\n            fx = np.float32(0.8 * frame.shape[1])\n\n        inference_result = net.infer(scaled_img)\n        poses_3d, poses_2d = parse_poses(inference_result, input_scale, stride, fx, is_video)\n        edges = []\n        if len(poses_3d):\n            poses_3d = rotate_poses(poses_3d, R, t)\n            poses_3d_copy = poses_3d.copy()\n            x = poses_3d_copy[:, 0::4]\n            y = poses_3d_copy[:, 1::4]\n            z = poses_3d_copy[:, 2::4]\n            poses_3d[:, 0::4], poses_3d[:, 1::4], poses_3d[:, 2::4] = -z, x, -y\n\n            poses_3d = poses_3d.reshape(poses_3d.shape[0], 19, -1)[:, :, 0:3]\n            edges = (Plotter3d.SKELETON_EDGES + 19 * np.arange(poses_3d.shape[0]).reshape((-1, 1, 1))).reshape((-1, 2))\n        plotter.plot(canvas_3d, poses_3d, edges)\n        cv2.imshow(canvas_3d_window_name, canvas_3d)\n\n        draw_poses(frame, poses_2d)\n        current_time = (cv2.getTickCount() - current_time) / cv2.getTickFrequency()\n        if mean_time == 0:\n            mean_time = current_time\n        else:\n            mean_time = mean_time * 0.95 + current_time * 0.05\n        cv2.putText(frame, \'FPS: {}\'.format(int(1 / mean_time * 10) / 10),\n                    (40, 80), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255))\n        cv2.imshow(\'ICV 3D Human Pose Estimation\', frame)\n\n        key = cv2.waitKey(delay)\n        if key == esc_code:\n            break\n        if key == p_code:\n            if delay == 1:\n                delay = 0\n            else:\n                delay = 1\n        if delay == 0 or not is_video:  # allow to rotate 3D canvas while on pause\n            key = 0\n            while (key != p_code\n                   and key != esc_code\n                   and key != space_code):\n                plotter.plot(canvas_3d, poses_3d, edges)\n                cv2.imshow(canvas_3d_window_name, canvas_3d)\n                key = cv2.waitKey(33)\n            if key == esc_code:\n                break\n            else:\n                delay = 1\n'"
setup.py,0,"b""import os\nimport platform\nfrom setuptools import setup, Extension\nfrom setuptools.command.build_ext import build_ext\nimport subprocess\nimport sys\n\n\nPACKAGE_NAME = 'pose_extractor'\n\n\noptions = {'--debug': 'OFF'}\nif '--debug' in sys.argv:\n    options['--debug'] = 'ON'\n\n\nclass CMakeExtension(Extension):\n    def __init__(self, name, cmake_lists_dir=PACKAGE_NAME, **kwargs):\n        Extension.__init__(self, name, sources=[], **kwargs)\n        self.cmake_lists_dir = os.path.abspath(cmake_lists_dir)\n\n\nclass CMakeBuild(build_ext):\n    def build_extensions(self):\n        try:\n            subprocess.check_output(['cmake', '--version'])\n        except OSError:\n            raise RuntimeError('Cannot find CMake executable')\n\n        ext = self.extensions[0]\n        build_dir = os.path.abspath(os.path.join(PACKAGE_NAME, 'build'))\n        if not os.path.exists(build_dir):\n            os.mkdir(build_dir)\n        tmp_dir = os.path.join(build_dir, 'tmp')\n        if not os.path.exists(tmp_dir):\n            os.mkdir(tmp_dir)\n\n        cfg = 'Debug' if options['--debug'] == 'ON' else 'Release'\n\n        cmake_args = [\n            '-DCMAKE_BUILD_TYPE={}'.format(cfg),\n            '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY_{}={}'.format(cfg.upper(), build_dir),\n            '-DCMAKE_ARCHIVE_OUTPUT_DIRECTORY_{}={}'.format(cfg.upper(), tmp_dir),\n            '-DPYTHON_EXECUTABLE={}'.format(sys.executable)\n        ]\n\n        if platform.system() == 'Windows':\n            platform_type = ('x64' if platform.architecture()[0] == '64bit' else 'Win32')\n            cmake_args += [\n                '-DCMAKE_WINDOWS_EXPORT_ALL_SYMBOLS=TRUE',\n                '-DCMAKE_RUNTIME_OUTPUT_DIRECTORY_{}={}'.format(cfg.upper(), build_dir),\n            ]\n\n            if self.compiler.compiler_type == 'msvc':\n                cmake_args += [\n                    '-DCMAKE_GENERATOR_PLATFORM={}'.format(platform_type),\n                ]\n            else:\n                cmake_args += [\n                    '-G', 'MinGW Makefiles',\n                ]\n\n        subprocess.check_call(['cmake', ext.cmake_lists_dir] + cmake_args, cwd=tmp_dir)\n        subprocess.check_call(['cmake', '--build', '.', '--config', cfg], cwd=tmp_dir)\n\n\nsetup(name=PACKAGE_NAME,\n      packages=[PACKAGE_NAME],\n      version='1.0',\n      description='Auxiliary C++ module for fast 2d pose extraction from network output',\n      ext_modules=[CMakeExtension(PACKAGE_NAME)],\n      cmdclass={'build_ext': CMakeBuild})\n"""
models/__init__.py,0,b''
models/with_mobilenet.py,5,"b'import torch\nfrom torch import nn\n\nfrom modules.conv import conv, conv_dw, conv_dw_no_bn\n\n\nclass Cpm(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.align = conv(in_channels, out_channels, kernel_size=1, padding=0, bn=False)\n        self.trunk = nn.Sequential(\n            conv_dw_no_bn(out_channels, out_channels),\n            conv_dw_no_bn(out_channels, out_channels),\n            conv_dw_no_bn(out_channels, out_channels)\n        )\n        self.conv = conv(out_channels, out_channels, bn=False)\n\n    def forward(self, x):\n        x = self.align(x)\n        x = self.conv(x + self.trunk(x))\n        return x\n\n\nclass InitialStage(nn.Module):\n    def __init__(self, num_channels, num_heatmaps, num_pafs):\n        super().__init__()\n        self.trunk = nn.Sequential(\n            conv(num_channels, num_channels, bn=False),\n            conv(num_channels, num_channels, bn=False),\n            conv(num_channels, num_channels, bn=False)\n        )\n        self.heatmaps = nn.Sequential(\n            conv(num_channels, 512, kernel_size=1, padding=0, bn=False),\n            conv(512, num_heatmaps, kernel_size=1, padding=0, bn=False, relu=False)\n        )\n        self.pafs = nn.Sequential(\n            conv(num_channels, 512, kernel_size=1, padding=0, bn=False),\n            conv(512, num_pafs, kernel_size=1, padding=0, bn=False, relu=False)\n        )\n\n    def forward(self, x):\n        trunk_features = self.trunk(x)\n        heatmaps = self.heatmaps(trunk_features)\n        pafs = self.pafs(trunk_features)\n        return [heatmaps, pafs]\n\n\nclass RefinementStageBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.initial = conv(in_channels, out_channels, kernel_size=1, padding=0, bn=False)\n        self.trunk = nn.Sequential(\n            conv(out_channels, out_channels),\n            conv(out_channels, out_channels, dilation=2, padding=2)\n        )\n\n    def forward(self, x):\n        initial_features = self.initial(x)\n        trunk_features = self.trunk(initial_features)\n        return initial_features + trunk_features\n\n\nclass RefinementStage(nn.Module):\n    def __init__(self, in_channels, out_channels, num_heatmaps, num_pafs):\n        super().__init__()\n        self.trunk = nn.Sequential(\n            RefinementStageBlock(in_channels, out_channels),\n            RefinementStageBlock(out_channels, out_channels),\n            RefinementStageBlock(out_channels, out_channels),\n            RefinementStageBlock(out_channels, out_channels),\n            RefinementStageBlock(out_channels, out_channels)\n        )\n        self.heatmaps = nn.Sequential(\n            conv(out_channels, out_channels, kernel_size=1, padding=0, bn=False),\n            conv(out_channels, num_heatmaps, kernel_size=1, padding=0, bn=False, relu=False)\n        )\n        self.pafs = nn.Sequential(\n            conv(out_channels, out_channels, kernel_size=1, padding=0, bn=False),\n            conv(out_channels, num_pafs, kernel_size=1, padding=0, bn=False, relu=False)\n        )\n\n    def forward(self, x):\n        trunk_features = self.trunk(x)\n        heatmaps = self.heatmaps(trunk_features)\n        pafs = self.pafs(trunk_features)\n        return [heatmaps, pafs]\n\n\nclass RefinementStageLight(nn.Module):\n    def __init__(self, in_channels, mid_channels, out_channels):\n        super().__init__()\n        self.trunk = nn.Sequential(\n            RefinementStageBlock(in_channels, mid_channels),\n            RefinementStageBlock(mid_channels, mid_channels)\n        )\n        self.feature_maps = nn.Sequential(\n            conv(mid_channels, mid_channels, kernel_size=1, padding=0, bn=False),\n            conv(mid_channels, out_channels, kernel_size=1, padding=0, bn=False, relu=False)\n        )\n\n    def forward(self, x):\n        trunk_features = self.trunk(x)\n        feature_maps = self.feature_maps(trunk_features)\n        return [feature_maps]\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, ratio, should_align=False):\n        super().__init__()\n        self.should_align = should_align\n        self.bottleneck = nn.Sequential(\n            conv(in_channels, in_channels // ratio, kernel_size=1, padding=0),\n            conv(in_channels // ratio, in_channels // ratio),\n            conv(in_channels // ratio, out_channels, kernel_size=1, padding=0)\n        )\n        if self.should_align:\n            self.align = conv(in_channels, out_channels, kernel_size=1, padding=0)\n\n    def forward(self, x):\n        res = self.bottleneck(x)\n        if self.should_align:\n            x = self.align(x)\n        return x + res\n\n\nclass Pose3D(nn.Module):\n    def __init__(self, in_channels, num_2d_heatmaps, ratio=2, out_channels=57):\n        super().__init__()\n        self.stem = nn.Sequential(\n            ResBlock(in_channels + num_2d_heatmaps, in_channels, ratio, should_align=True),\n            ResBlock(in_channels, in_channels, ratio),\n            ResBlock(in_channels, in_channels, ratio),\n            ResBlock(in_channels, in_channels, ratio),\n            ResBlock(in_channels, in_channels, ratio),\n        )\n\n        self.prediction = RefinementStageLight(in_channels, in_channels, out_channels)\n\n    def forward(self, x, feature_maps_2d):\n        stem = self.stem(torch.cat([x, feature_maps_2d], 1))\n        feature_maps = self.prediction(stem)\n        return feature_maps\n\n\nclass PoseEstimationWithMobileNet(nn.Module):\n    def __init__(self, num_refinement_stages=1, num_channels=128, num_heatmaps=19, num_pafs=38,\n                 is_convertible_by_mo=False):\n        super().__init__()\n        self.is_convertible_by_mo = is_convertible_by_mo\n        self.model = nn.Sequential(\n            conv(     3,  32, stride=2, bias=False),\n            conv_dw( 32,  64),\n            conv_dw( 64, 128, stride=2),\n            conv_dw(128, 128),\n            conv_dw(128, 256, stride=2),\n            conv_dw(256, 256),\n            conv_dw(256, 512),  # conv4_2\n            conv_dw(512, 512, dilation=2, padding=2),\n            conv_dw(512, 512),\n            conv_dw(512, 512),\n            conv_dw(512, 512),\n            conv_dw(512, 512)   # conv5_5\n        )\n        self.cpm = Cpm(512, num_channels)\n\n        self.initial_stage = InitialStage(num_channels, num_heatmaps, num_pafs)\n        self.refinement_stages = nn.ModuleList()\n\n        for idx in range(num_refinement_stages):\n            self.refinement_stages.append(RefinementStage(num_channels + num_heatmaps + num_pafs, num_channels,\n                                                          num_heatmaps, num_pafs))\n        self.Pose3D = Pose3D(128, num_2d_heatmaps=57)\n        if self.is_convertible_by_mo:\n            self.fake_conv_heatmaps = nn.Conv2d(num_heatmaps, num_heatmaps, kernel_size=1, bias=False)\n            self.fake_conv_heatmaps.weight = nn.Parameter(torch.zeros(num_heatmaps, num_heatmaps, 1, 1))\n            self.fake_conv_pafs = nn.Conv2d(num_pafs, num_pafs, kernel_size=1, bias=False)\n            self.fake_conv_pafs.weight = nn.Parameter(torch.zeros(num_pafs, num_pafs, 1, 1))\n\n    def forward(self, x):\n        model_features = self.model(x)\n        backbone_features = self.cpm(model_features)\n\n        stages_output = self.initial_stage(backbone_features)\n        for refinement_stage in self.refinement_stages:\n            stages_output.extend(\n                refinement_stage(torch.cat([backbone_features, stages_output[-2], stages_output[-1]], dim=1)))\n        keypoints2d_maps = stages_output[-2]\n        paf_maps = stages_output[-1]\n        if self.is_convertible_by_mo:  # Model Optimizer R3 2019 cuts out these two network outputs, add fake op to fix it\n            keypoints2d_maps = stages_output[-2] + self.fake_conv_heatmaps(stages_output[-2])\n            paf_maps = stages_output[-1] + self.fake_conv_pafs(stages_output[-1])\n        out = self.Pose3D(backbone_features, torch.cat([stages_output[-2], stages_output[-1]], dim=1))\n\n        return out, keypoints2d_maps, paf_maps\n\n'"
modules/__init__.py,0,b''
modules/conv.py,0,"b'from torch import nn\n\n\ndef conv(in_channels, out_channels, kernel_size=3, padding=1, bn=True, dilation=1, stride=1, relu=True, bias=True):\n    modules = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=bias)]\n    if bn:\n        modules.append(nn.BatchNorm2d(out_channels))\n    if relu:\n        modules.append(nn.ReLU(inplace=True))\n    return nn.Sequential(*modules)\n\n\ndef conv_dw(in_channels, out_channels, kernel_size=3, padding=1, stride=1, dilation=1):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation=dilation, groups=in_channels, bias=False),\n        nn.BatchNorm2d(in_channels),\n        nn.ReLU(inplace=True),\n\n        nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True),\n    )\n\n\ndef conv_dw_no_bn(in_channels, out_channels, kernel_size=3, padding=1, stride=1, dilation=1):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation=dilation, groups=in_channels, bias=False),\n        nn.ELU(inplace=True),\n\n        nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),\n        nn.ELU(inplace=True),\n    )\n'"
modules/draw.py,0,"b'import math\n\nimport cv2\nimport numpy as np\n\n\nprevious_position = []\ntheta, phi = 3.1415/4, -3.1415/6\nshould_rotate = False\nscale_dx = 800\nscale_dy = 800\n\n\nclass Plotter3d:\n    SKELETON_EDGES = np.array([[11, 10], [10, 9], [9, 0], [0, 3], [3, 4], [4, 5], [0, 6], [6, 7], [7, 8], [0, 12],\n                               [12, 13], [13, 14], [0, 1], [1, 15], [15, 16], [1, 17], [17, 18]])\n\n    def __init__(self, canvas_size, origin=(0.5, 0.5), scale=1):\n        self.origin = np.array([origin[1] * canvas_size[1], origin[0] * canvas_size[0]], dtype=np.float32)  # x, y\n        self.scale = np.float32(scale)\n        self.theta = 0\n        self.phi = 0\n        axis_length = 200\n        axes = [\n            np.array([[-axis_length/2, -axis_length/2, 0], [axis_length/2, -axis_length/2, 0]], dtype=np.float32),\n            np.array([[-axis_length/2, -axis_length/2, 0], [-axis_length/2, axis_length/2, 0]], dtype=np.float32),\n            np.array([[-axis_length/2, -axis_length/2, 0], [-axis_length/2, -axis_length/2, axis_length]], dtype=np.float32)]\n        step = 20\n        for step_id in range(axis_length // step + 1):  # add grid\n            axes.append(np.array([[-axis_length / 2, -axis_length / 2 + step_id * step, 0],\n                                  [axis_length / 2, -axis_length / 2 + step_id * step, 0]], dtype=np.float32))\n            axes.append(np.array([[-axis_length / 2 + step_id * step, -axis_length / 2, 0],\n                                  [-axis_length / 2 + step_id * step, axis_length / 2, 0]], dtype=np.float32))\n        self.axes = np.array(axes)\n\n    def plot(self, img, vertices, edges):\n        global theta, phi\n        img.fill(0)\n        R = self._get_rotation(theta, phi)\n        self._draw_axes(img, R)\n        if len(edges) != 0:\n            self._plot_edges(img, vertices, edges, R)\n\n    def _draw_axes(self, img, R):\n        axes_2d = np.dot(self.axes, R)\n        axes_2d = axes_2d * self.scale + self.origin\n        for axe in axes_2d:\n            axe = axe.astype(int)\n            cv2.line(img, tuple(axe[0]), tuple(axe[1]), (128, 128, 128), 1, cv2.LINE_AA)\n\n    def _plot_edges(self, img, vertices, edges, R):\n        vertices_2d = np.dot(vertices, R)\n        edges_vertices = vertices_2d.reshape((-1, 2))[edges] * self.scale + self.origin\n        for edge_vertices in edges_vertices:\n            edge_vertices = edge_vertices.astype(int)\n            cv2.line(img, tuple(edge_vertices[0]), tuple(edge_vertices[1]), (255, 255, 255), 1, cv2.LINE_AA)\n\n    def _get_rotation(self, theta, phi):\n        sin, cos = math.sin, math.cos\n        return np.array([\n            [ cos(theta),  sin(theta) * sin(phi)],\n            [-sin(theta),  cos(theta) * sin(phi)],\n            [ 0,                       -cos(phi)]\n        ], dtype=np.float32)  # transposed\n\n    @staticmethod\n    def mouse_callback(event, x, y, flags, params):\n        global previous_position, theta, phi, should_rotate, scale_dx, scale_dy\n        if event == cv2.EVENT_LBUTTONDOWN:\n            previous_position = [x, y]\n            should_rotate = True\n        if event == cv2.EVENT_MOUSEMOVE and should_rotate:\n            theta += (x - previous_position[0]) / scale_dx * 6.2831  # 360 deg\n            phi -= (y - previous_position[1]) / scale_dy * 6.2831 * 2  # 360 deg\n            phi = max(min(3.1415 / 2, phi), -3.1415 / 2)\n            previous_position = [x, y]\n        if event == cv2.EVENT_LBUTTONUP:\n            should_rotate = False\n\n\nbody_edges = np.array(\n    [[0, 1],  # neck - nose\n     [1, 16], [16, 18],  # nose - l_eye - l_ear\n     [1, 15], [15, 17],  # nose - r_eye - r_ear\n     [0, 3], [3, 4], [4, 5],     # neck - l_shoulder - l_elbow - l_wrist\n     [0, 9], [9, 10], [10, 11],  # neck - r_shoulder - r_elbow - r_wrist\n     [0, 6], [6, 7], [7, 8],        # neck - l_hip - l_knee - l_ankle\n     [0, 12], [12, 13], [13, 14]])  # neck - r_hip - r_knee - r_ankle\n\n\ndef draw_poses(img, poses_2d):\n    for pose_id in range(len(poses_2d)):\n        pose = np.array(poses_2d[pose_id][0:-1]).reshape((-1, 3)).transpose()\n        was_found = pose[2, :] > 0\n        for edge in body_edges:\n            if was_found[edge[0]] and was_found[edge[1]]:\n                cv2.line(img, tuple(pose[0:2, edge[0]].astype(int)), tuple(pose[0:2, edge[1]].astype(int)),\n                         (255, 255, 0), 4, cv2.LINE_AA)\n        for kpt_id in range(pose.shape[1]):\n            if pose[2, kpt_id] != -1:\n                cv2.circle(img, tuple(pose[0:2, kpt_id].astype(int)), 3, (0, 255, 255), -1, cv2.LINE_AA)\n'"
modules/inference_engine_openvino.py,0,"b""import os\n\nimport numpy as np\nfrom openvino.inference_engine import IENetwork, IECore\n\n\nclass InferenceEngineOpenVINO:\n    def __init__(self, net_model_xml_path, device):\n        self.device = device\n\n        net_model_bin_path = os.path.splitext(net_model_xml_path)[0] + '.bin'\n        self.net = IENetwork(model=net_model_xml_path, weights=net_model_bin_path)\n        required_input_key = {'data'}\n        assert required_input_key == set(self.net.inputs.keys()), \\\n            'Demo supports only topologies with the following input key: {}'.format(', '.join(required_input_key))\n        required_output_keys = {'features', 'heatmaps', 'pafs'}\n        assert required_output_keys.issubset(self.net.outputs.keys()), \\\n            'Demo supports only topologies with the following output keys: {}'.format(', '.join(required_output_keys))\n\n        self.ie = IECore()\n        self.exec_net = self.ie.load_network(network=self.net, num_requests=1, device_name=device)\n\n    def infer(self, img):\n        input_layer = next(iter(self.net.inputs))\n        n, c, h, w = self.net.inputs[input_layer].shape\n        if h != img.shape[0] or w != img.shape[1]:\n            self.net.reshape({input_layer: (n, c, img.shape[0], img.shape[1])})\n            self.exec_net = self.ie.load_network(network=self.net, num_requests=1, device_name=self.device)\n        img = np.transpose(img, (2, 0, 1))[None, ]\n\n        inference_result = self.exec_net.infer(inputs={'data': img})\n\n        inference_result = (inference_result['features'][0],\n                            inference_result['heatmaps'][0], inference_result['pafs'][0])\n        return inference_result\n\n"""
modules/inference_engine_pytorch.py,4,"b""import numpy as np\nimport torch\n\n\nclass InferenceEnginePyTorch:\n    def __init__(self, checkpoint_path, device,\n                 img_mean=np.array([128, 128, 128], dtype=np.float32),\n                 img_scale=np.float32(1/255)):\n        from models.with_mobilenet import PoseEstimationWithMobileNet\n        from modules.load_state import load_state\n        self.img_mean = img_mean\n        self.img_scale = img_scale\n        self.device = 'cpu'\n        if device != 'CPU':\n            if torch.cuda.is_available():\n                self.device = torch.device('cuda:0')\n            else:\n                print('No CUDA device found, inferring on CPU')\n\n        net = PoseEstimationWithMobileNet()\n        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n        load_state(net, checkpoint)\n        net = net.to(self.device)\n        net.eval()\n        self.net = net\n\n    def infer(self, img):\n        normalized_img = InferenceEnginePyTorch._normalize(img, self.img_mean, self.img_scale)\n        data = torch.from_numpy(normalized_img).permute(2, 0, 1).unsqueeze(0).to(self.device)\n\n        features, heatmaps, pafs = self.net(data)\n\n        return (features[-1].squeeze().data.cpu().numpy(),\n                heatmaps[-1].squeeze().data.cpu().numpy(), pafs[-1].squeeze().data.cpu().numpy())\n\n    @staticmethod\n    def _normalize(img, img_mean, img_scale):\n        normalized_img = (img.astype(np.float32) - img_mean) * img_scale\n        return normalized_img\n\n"""
modules/input_reader.py,0,"b""import cv2\n\n\nclass ImageReader:\n    def __init__(self, file_names):\n        self.file_names = file_names\n        self.max_idx = len(file_names)\n\n    def __iter__(self):\n        self.idx = 0\n        return self\n\n    def __next__(self):\n        if self.idx == self.max_idx:\n            raise StopIteration\n        img = cv2.imread(self.file_names[self.idx], cv2.IMREAD_COLOR)\n        if img.size == 0:\n            raise IOError('Image {} cannot be read'.format(self.file_names[self.idx]))\n        self.idx = self.idx + 1\n        return img\n\n\nclass VideoReader:\n    def __init__(self, file_name):\n        self.file_name = file_name\n        try:  # OpenCV needs int to read from webcam\n            self.file_name = int(file_name)\n        except ValueError:\n            pass\n\n    def __iter__(self):\n        self.cap = cv2.VideoCapture(self.file_name)\n        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n        if not self.cap.isOpened():\n            raise IOError('Video {} cannot be opened'.format(self.file_name))\n        return self\n\n    def __next__(self):\n        was_read, img = self.cap.read()\n        if not was_read:\n            raise StopIteration\n        return img\n"""
modules/legacy_pose_extractor.py,0,"b""from operator import itemgetter\n\nimport cv2\nimport math\nimport numpy as np\n\nBODY_PARTS_KPT_IDS = [[1, 2], [1, 5], [2, 3], [3, 4], [5, 6], [6, 7], [1, 8], [8, 9], [9, 10], [1, 11],\n                      [11, 12], [12, 13], [1, 0], [0, 14], [14, 16], [0, 15], [15, 17], [2, 16], [5, 17]]\nBODY_PARTS_PAF_IDS = ([12, 13], [20, 21], [14, 15], [16, 17], [22, 23], [24, 25], [0, 1], [2, 3], [4, 5],\n                      [6, 7], [8, 9], [10, 11], [28, 29], [30, 31], [34, 35], [32, 33], [36, 37], [18, 19], [26, 27])\n\n\ndef linspace2d(start, stop, n=10):\n    points = 1 / (n - 1) * (stop - start)\n    return points[:, None] * np.arange(n) + start[:, None]\n\n\ndef extract_keypoints(heatmap, all_keypoints, total_keypoint_num):\n    heatmap[heatmap < 0.1] = 0\n    heatmap_with_borders = np.pad(heatmap, [(2, 2), (2, 2)], mode='constant')\n    heatmap_center = heatmap_with_borders[1:heatmap_with_borders.shape[0]-1, 1:heatmap_with_borders.shape[1]-1]\n    heatmap_left = heatmap_with_borders[1:heatmap_with_borders.shape[0]-1, 2:heatmap_with_borders.shape[1]]\n    heatmap_right = heatmap_with_borders[1:heatmap_with_borders.shape[0]-1, 0:heatmap_with_borders.shape[1]-2]\n    heatmap_up = heatmap_with_borders[2:heatmap_with_borders.shape[0], 1:heatmap_with_borders.shape[1]-1]\n    heatmap_down = heatmap_with_borders[0:heatmap_with_borders.shape[0]-2, 1:heatmap_with_borders.shape[1]-1]\n\n    heatmap_peaks = (heatmap_center > heatmap_left) &\\\n                    (heatmap_center > heatmap_right) &\\\n                    (heatmap_center > heatmap_up) &\\\n                    (heatmap_center > heatmap_down)\n    heatmap_peaks = heatmap_peaks[1:heatmap_center.shape[0]-1, 1:heatmap_center.shape[1]-1]\n    keypoints = list(zip(np.nonzero(heatmap_peaks)[1], np.nonzero(heatmap_peaks)[0]))  # (w, h)\n    keypoints = sorted(keypoints, key=itemgetter(0))\n\n    suppressed = np.zeros(len(keypoints), np.uint8)\n    keypoints_with_score_and_id = []\n    keypoint_num = 0\n    for i in range(len(keypoints)):\n        if suppressed[i]:\n            continue\n        for j in range(i+1, len(keypoints)):\n            if math.sqrt((keypoints[i][0] - keypoints[j][0]) ** 2 +\n                         (keypoints[i][1] - keypoints[j][1]) ** 2) < 6:\n                suppressed[j] = 1\n        keypoint_with_score_and_id = (keypoints[i][0], keypoints[i][1], heatmap[keypoints[i][1], keypoints[i][0]],\n                                      total_keypoint_num + keypoint_num)\n        keypoints_with_score_and_id.append(keypoint_with_score_and_id)\n        keypoint_num += 1\n    all_keypoints.append(keypoints_with_score_and_id)\n    return keypoint_num\n\n\ndef group_keypoints(all_keypoints_by_type, pafs, pose_entry_size=20, min_paf_score=0.05):\n    pose_entries = []\n    all_keypoints = np.array([item for sublist in all_keypoints_by_type for item in sublist])\n    for part_id in range(len(BODY_PARTS_PAF_IDS)):\n        part_pafs = pafs[BODY_PARTS_PAF_IDS[part_id]]\n        kpts_a = all_keypoints_by_type[BODY_PARTS_KPT_IDS[part_id][0]]\n        kpts_b = all_keypoints_by_type[BODY_PARTS_KPT_IDS[part_id][1]]\n        num_kpts_a = len(kpts_a)\n        num_kpts_b = len(kpts_b)\n        kpt_a_id = BODY_PARTS_KPT_IDS[part_id][0]\n        kpt_b_id = BODY_PARTS_KPT_IDS[part_id][1]\n\n        if num_kpts_a == 0 and num_kpts_b == 0:  # no keypoints for such body part\n            continue\n        elif num_kpts_a == 0:  # body part has just 'b' keypoints\n            for i in range(num_kpts_b):\n                num = 0\n                for j in range(len(pose_entries)):  # check if already in some pose, was added by another body part\n                    if pose_entries[j][kpt_b_id] == kpts_b[i][3]:\n                        num += 1\n                        continue\n                if num == 0:\n                    pose_entry = np.ones(pose_entry_size) * -1\n                    pose_entry[kpt_b_id] = kpts_b[i][3]  # keypoint idx\n                    pose_entry[-1] = 1                   # num keypoints in pose\n                    pose_entry[-2] = kpts_b[i][2]        # pose score\n                    pose_entries.append(pose_entry)\n            continue\n        elif num_kpts_b == 0:  # body part has just 'a' keypoints\n            for i in range(num_kpts_a):\n                num = 0\n                for j in range(len(pose_entries)):\n                    if pose_entries[j][kpt_a_id] == kpts_a[i][3]:\n                        num += 1\n                        continue\n                if num == 0:\n                    pose_entry = np.ones(pose_entry_size) * -1\n                    pose_entry[kpt_a_id] = kpts_a[i][3]\n                    pose_entry[-1] = 1\n                    pose_entry[-2] = kpts_a[i][2]\n                    pose_entries.append(pose_entry)\n            continue\n\n        connections = []\n        for i in range(num_kpts_a):\n            kpt_a = np.array(kpts_a[i][0:2])\n            for j in range(num_kpts_b):\n                kpt_b = np.array(kpts_b[j][0:2])\n                mid_point = [(), ()]\n                mid_point[0] = (int(round((kpt_a[0] + kpt_b[0]) * 0.5)),\n                                int(round((kpt_a[1] + kpt_b[1]) * 0.5)))\n                mid_point[1] = mid_point[0]\n\n                vec = [kpt_b[0] - kpt_a[0], kpt_b[1] - kpt_a[1]]\n                vec_norm = math.sqrt(vec[0] ** 2 + vec[1] ** 2)\n                if vec_norm == 0:\n                    continue\n                vec[0] /= vec_norm\n                vec[1] /= vec_norm\n                cur_point_score = (vec[0] * part_pafs[0, mid_point[0][1], mid_point[0][0]] +\n                                   vec[1] * part_pafs[1, mid_point[1][1], mid_point[1][0]])\n\n                height_n = pafs.shape[1] // 2\n                success_ratio = 0\n                point_num = 10  # number of points to integration over paf\n                ratio = 0\n                if cur_point_score > -100:\n                    passed_point_score = 0\n                    passed_point_num = 0\n                    x, y = linspace2d(kpt_a, kpt_b)\n                    for point_idx in range(point_num):\n                        px = int(x[point_idx])\n                        py = int(y[point_idx])\n                        paf = part_pafs[:, py, px]\n                        cur_point_score = vec[0] * paf[0] + vec[1] * paf[1]\n                        if cur_point_score > min_paf_score:\n                            passed_point_score += cur_point_score\n                            passed_point_num += 1\n                    success_ratio = passed_point_num / point_num\n                    if passed_point_num > 0:\n                        ratio = passed_point_score / passed_point_num\n                    ratio += min(height_n / vec_norm - 1, 0)\n                if ratio > 0 and success_ratio > 0.8:\n                    score_all = ratio + kpts_a[i][2] + kpts_b[j][2]\n                    connections.append([i, j, ratio, score_all])\n        if len(connections) > 0:\n            connections = sorted(connections, key=itemgetter(2), reverse=True)\n\n        num_connections = min(num_kpts_a, num_kpts_b)\n        has_kpt_a = np.zeros(num_kpts_a, dtype=np.int32)\n        has_kpt_b = np.zeros(num_kpts_b, dtype=np.int32)\n        filtered_connections = []\n        for row in range(len(connections)):\n            if len(filtered_connections) == num_connections:\n                break\n            i, j, cur_point_score = connections[row][0:3]\n            if not has_kpt_a[i] and not has_kpt_b[j]:\n                filtered_connections.append([kpts_a[i][3], kpts_b[j][3], cur_point_score])\n                has_kpt_a[i] = 1\n                has_kpt_b[j] = 1\n        connections = filtered_connections\n        if len(connections) == 0:\n            continue\n\n        if part_id == 0:\n            pose_entries = [np.ones(pose_entry_size) * -1 for _ in range(len(connections))]\n            for i in range(len(connections)):\n                pose_entries[i][BODY_PARTS_KPT_IDS[0][0]] = connections[i][0]\n                pose_entries[i][BODY_PARTS_KPT_IDS[0][1]] = connections[i][1]\n                pose_entries[i][-1] = 2\n                pose_entries[i][-2] = np.sum(all_keypoints[connections[i][0:2], 2]) + connections[i][2]\n        elif part_id == 17 or part_id == 18:\n            kpt_a_id = BODY_PARTS_KPT_IDS[part_id][0]\n            kpt_b_id = BODY_PARTS_KPT_IDS[part_id][1]\n            for i in range(len(connections)):\n                for j in range(len(pose_entries)):\n                    if pose_entries[j][kpt_a_id] == connections[i][0] and pose_entries[j][kpt_b_id] == -1:\n                        pose_entries[j][kpt_b_id] = connections[i][1]\n                    elif pose_entries[j][kpt_b_id] == connections[i][1] and pose_entries[j][kpt_a_id] == -1:\n                        pose_entries[j][kpt_a_id] = connections[i][0]\n            continue\n        else:\n            kpt_a_id = BODY_PARTS_KPT_IDS[part_id][0]\n            kpt_b_id = BODY_PARTS_KPT_IDS[part_id][1]\n            for i in range(len(connections)):\n                num = 0\n                for j in range(len(pose_entries)):\n                    if pose_entries[j][kpt_a_id] == connections[i][0]:\n                        pose_entries[j][kpt_b_id] = connections[i][1]\n                        num += 1\n                        pose_entries[j][-1] += 1\n                        pose_entries[j][-2] += all_keypoints[connections[i][1], 2] + connections[i][2]\n                if num == 0:\n                    pose_entry = np.ones(pose_entry_size) * -1\n                    pose_entry[kpt_a_id] = connections[i][0]\n                    pose_entry[kpt_b_id] = connections[i][1]\n                    pose_entry[-1] = 2\n                    pose_entry[-2] = np.sum(all_keypoints[connections[i][0:2], 2]) + connections[i][2]\n                    pose_entries.append(pose_entry)\n\n    filtered_entries = []\n    for i in range(len(pose_entries)):\n        if pose_entries[i][-1] < 3 or (pose_entries[i][-2] / pose_entries[i][-1] < 0.2):\n            continue\n        filtered_entries.append(pose_entries[i])\n    pose_entries = np.asarray(filtered_entries)\n    return pose_entries, all_keypoints\n\n\ndef extract_poses(heatmaps, pafs, upsample_ratio):\n    heatmaps = np.transpose(heatmaps, (1, 2, 0))\n    pafs = np.transpose(pafs, (1, 2, 0))\n    heatmaps = cv2.resize(heatmaps, dsize=None, fx=upsample_ratio, fy=upsample_ratio)\n    pafs = cv2.resize(pafs, dsize=None, fx=upsample_ratio, fy=upsample_ratio)\n    heatmaps = np.transpose(heatmaps, (2, 0, 1))\n    pafs = np.transpose(pafs, (2, 0, 1))\n\n    num_keypoints = heatmaps.shape[0]\n    total_keypoints_num = 0\n    all_keypoints_by_type = []\n    for kpt_idx in range(num_keypoints):\n        total_keypoints_num += extract_keypoints(heatmaps[kpt_idx], all_keypoints_by_type, total_keypoints_num)\n    \n    pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs)\n\n    found_poses = []\n    for pose_entry in pose_entries:\n        if len(pose_entry) == 0:\n            continue\n        pose_keypoints = np.ones((num_keypoints * 3 + 1), dtype=np.float32) * -1\n        for kpt_id in range(num_keypoints):\n            if pose_entry[kpt_id] != -1.0:\n                pose_keypoints[kpt_id * 3 + 0] = all_keypoints[int(pose_entry[kpt_id]), 0]\n                pose_keypoints[kpt_id * 3 + 1] = all_keypoints[int(pose_entry[kpt_id]), 1]\n                pose_keypoints[kpt_id * 3 + 2] = all_keypoints[int(pose_entry[kpt_id]), 2]\n        pose_keypoints[-1] = pose_entry[18]\n        found_poses.append(pose_keypoints)\n\n    if not found_poses:\n        return np.array(found_poses, dtype=np.float32).reshape((0,0)), None\n\n    return np.array(found_poses, dtype=np.float32), None\n"""
modules/load_state.py,0,"b""import collections\n\n\ndef load_state(net, checkpoint):\n    source_state = checkpoint['state_dict']\n    target_state = net.state_dict()\n    new_target_state = collections.OrderedDict()\n    for target_key, target_value in target_state.items():\n        if target_key in source_state and source_state[target_key].size() == target_state[target_key].size():\n            new_target_state[target_key] = source_state[target_key]\n        else:\n            new_target_state[target_key] = target_state[target_key]\n            print('[WARNING] Not found pre-trained parameters for {}'.format(target_key))\n\n    net.load_state_dict(new_target_state)\n"""
modules/one_euro_filter.py,0,"b""import math\n\n\ndef get_alpha(rate=30, cutoff=1):\n    tau = 1 / (2 * math.pi * cutoff)\n    te = 1 / rate\n    return 1 / (1 + tau / te)\n\n\nclass LowPassFilter:\n    def __init__(self):\n        self.x_previous = None\n\n    def __call__(self, x, alpha=0.5):\n        if self.x_previous is None:\n            self.x_previous = x\n            return x\n        x_filtered = alpha * x + (1 - alpha) * self.x_previous\n        self.x_previous = x_filtered\n        return x_filtered\n\n\nclass OneEuroFilter:\n    def __init__(self, freq=15, mincutoff=1, beta=1, dcutoff=1):\n        self.freq = freq\n        self.mincutoff = mincutoff\n        self.beta = beta\n        self.dcutoff = dcutoff\n        self.filter_x = LowPassFilter()\n        self.filter_dx = LowPassFilter()\n        self.x_previous = None\n        self.dx = None\n\n    def __call__(self, x):\n        if self.dx is None:\n            self.dx = 0\n        else:\n            self.dx = (x - self.x_previous) * self.freq\n        dx_smoothed = self.filter_dx(self.dx, get_alpha(self.freq, self.dcutoff))\n        cutoff = self.mincutoff + self.beta * abs(dx_smoothed)\n        x_filtered = self.filter_x(x, get_alpha(self.freq, cutoff))\n        self.x_previous = x\n        return x_filtered\n\n\nif __name__ == '__main__':\n    filter = OneEuroFilter(freq=15, beta=0.1)\n    for val in range(10):\n        x = val + (-1)**(val % 2)\n        x_filtered = filter(x)\n        print(x_filtered, x)"""
modules/parse_poses.py,0,"b""import numpy as np\n\nfrom modules.pose import Pose, propagate_ids\ntry:\n    from pose_extractor import extract_poses\nexcept:\n    print('#### Cannot load fast pose extraction, switched to legacy slow implementation. ####')\n    from modules.legacy_pose_extractor import extract_poses\n\nAVG_PERSON_HEIGHT = 180\n\n# pelvis (body center) is missing, id == 2\nmap_id_to_panoptic = [1, 0, 9, 10, 11, 3, 4, 5, 12, 13, 14, 6, 7, 8, 15, 16, 17, 18]\n\nlimbs = [[18, 17, 1],\n         [16, 15, 1],\n         [5, 4, 3],\n         [8, 7, 6],\n         [11, 10, 9],\n         [14, 13, 12]]\n\n\ndef get_root_relative_poses(inference_results):\n    features, heatmap, paf_map = inference_results\n\n    upsample_ratio = 4\n    found_poses = extract_poses(heatmap[0:-1], paf_map, upsample_ratio)[0]\n    # scale coordinates to features space\n    found_poses[:, 0:-1:3] /= upsample_ratio\n    found_poses[:, 1:-1:3] /= upsample_ratio\n\n    poses_2d = []\n    num_kpt_panoptic = 19\n    num_kpt = 18\n    for pose_id in range(found_poses.shape[0]):\n        if found_poses[pose_id, 3] == -1:  # skip pose if does not found neck\n            continue\n        pose_2d = np.ones(num_kpt_panoptic * 3 + 1, dtype=np.float32) * -1  # +1 for pose confidence\n        for kpt_id in range(num_kpt):\n            if found_poses[pose_id, kpt_id * 3] != -1:\n                x_2d, y_2d = found_poses[pose_id, kpt_id * 3:kpt_id * 3 + 2]\n                conf = found_poses[pose_id, kpt_id * 3 + 2]\n                pose_2d[map_id_to_panoptic[kpt_id] * 3] = x_2d  # just repacking\n                pose_2d[map_id_to_panoptic[kpt_id] * 3 + 1] = y_2d\n                pose_2d[map_id_to_panoptic[kpt_id] * 3 + 2] = conf\n        pose_2d[-1] = found_poses[pose_id, -1]\n        poses_2d.append(pose_2d)\n\n    keypoint_treshold = 0.1\n    poses_3d = np.ones((len(poses_2d), num_kpt_panoptic * 4), dtype=np.float32) * -1\n    for pose_id in range(len(poses_3d)):\n        if poses_2d[pose_id][2] > keypoint_treshold:\n            neck_2d = poses_2d[pose_id][:2].astype(int)\n            # read all pose coordinates at neck location\n            for kpt_id in range(num_kpt_panoptic):\n                map_3d = features[kpt_id * 3:(kpt_id + 1) * 3]\n                poses_3d[pose_id][kpt_id * 4] = map_3d[0, neck_2d[1], neck_2d[0]] * AVG_PERSON_HEIGHT\n                poses_3d[pose_id][kpt_id * 4 + 1] = map_3d[1, neck_2d[1], neck_2d[0]] * AVG_PERSON_HEIGHT\n                poses_3d[pose_id][kpt_id * 4 + 2] = map_3d[2, neck_2d[1], neck_2d[0]] * AVG_PERSON_HEIGHT\n                poses_3d[pose_id][kpt_id * 4 + 3] = poses_2d[pose_id][kpt_id * 3 + 2]\n\n            # refine keypoints coordinates at corresponding limbs locations\n            for limb in limbs:\n                for kpt_id_from in limb:\n                    if poses_2d[pose_id][kpt_id_from * 3 + 2] > keypoint_treshold:\n                        for kpt_id_where in limb:\n                            kpt_from_2d = poses_2d[pose_id][kpt_id_from*3: kpt_id_from*3 + 2].astype(int)\n                            map_3d = features[kpt_id_where * 3:(kpt_id_where + 1) * 3]\n                            poses_3d[pose_id][kpt_id_where * 4] = map_3d[0, kpt_from_2d[1], kpt_from_2d[0]] * AVG_PERSON_HEIGHT\n                            poses_3d[pose_id][kpt_id_where * 4 + 1] = map_3d[1, kpt_from_2d[1], kpt_from_2d[0]] * AVG_PERSON_HEIGHT\n                            poses_3d[pose_id][kpt_id_where * 4 + 2] = map_3d[2, kpt_from_2d[1], kpt_from_2d[0]] * AVG_PERSON_HEIGHT\n                        break\n\n    return poses_3d, np.array(poses_2d), features.shape\n\n\nprevious_poses_2d = []\n\n\ndef parse_poses(inference_results, input_scale, stride, fx, is_video=False):\n    global previous_poses_2d\n    poses_3d, poses_2d, features_shape = get_root_relative_poses(inference_results)\n    poses_2d_scaled = []\n    for pose_2d in poses_2d:\n        num_kpt = (pose_2d.shape[0] - 1) // 3\n        pose_2d_scaled = np.ones(pose_2d.shape[0], dtype=np.float32) * -1  # +1 for pose confidence\n        for kpt_id in range(num_kpt):\n            if pose_2d[kpt_id * 3] != -1:\n                pose_2d_scaled[kpt_id * 3] = int(pose_2d[kpt_id * 3] * stride / input_scale)\n                pose_2d_scaled[kpt_id * 3 + 1] = int(pose_2d[kpt_id * 3 + 1] * stride / input_scale)\n                pose_2d_scaled[kpt_id * 3 + 2] = pose_2d[kpt_id * 3 + 2]\n        pose_2d_scaled[-1] = pose_2d[-1]\n        poses_2d_scaled.append(pose_2d_scaled)\n\n    if is_video:  # track poses ids\n        current_poses_2d = []\n        for pose_id in range(len(poses_2d_scaled)):\n            pose_keypoints = np.ones((Pose.num_kpts, 2), dtype=np.int32) * -1\n            for kpt_id in range(Pose.num_kpts):\n                if poses_2d_scaled[pose_id][kpt_id * 3] != -1.0:  # keypoint was found\n                    pose_keypoints[kpt_id, 0] = int(poses_2d_scaled[pose_id][kpt_id * 3 + 0])\n                    pose_keypoints[kpt_id, 1] = int(poses_2d_scaled[pose_id][kpt_id * 3 + 1])\n            pose = Pose(pose_keypoints, poses_2d_scaled[pose_id][-1])\n            current_poses_2d.append(pose)\n        propagate_ids(previous_poses_2d, current_poses_2d)\n        previous_poses_2d = current_poses_2d\n\n    translated_poses_3d = []\n    # translate poses\n    for pose_id in range(len(poses_3d)):\n        pose_3d = poses_3d[pose_id].reshape((-1, 4)).transpose()\n        pose_2d = poses_2d[pose_id][:-1].reshape((-1, 3)).transpose()\n        num_valid = np.count_nonzero(pose_2d[2] != -1)\n        pose_3d_valid = np.zeros((3, num_valid), dtype=np.float32)\n        pose_2d_valid = np.zeros((2, num_valid), dtype=np.float32)\n        valid_id = 0\n        for kpt_id in range(pose_3d.shape[1]):\n            if pose_2d[2, kpt_id] == -1:\n                continue\n            pose_3d_valid[:, valid_id] = pose_3d[0:3, kpt_id]\n            pose_2d_valid[:, valid_id] = pose_2d[0:2, kpt_id]\n            valid_id += 1\n\n        pose_2d_valid[0] = pose_2d_valid[0] - features_shape[2]/2\n        pose_2d_valid[1] = pose_2d_valid[1] - features_shape[1]/2\n        mean_3d = np.expand_dims(pose_3d_valid.mean(axis=1), axis=1)\n        mean_2d = np.expand_dims(pose_2d_valid.mean(axis=1), axis=1)\n        numerator = np.trace(np.dot((pose_3d_valid[:2, :] - mean_3d[:2, :]).transpose(),\n                                    pose_3d_valid[:2, :] - mean_3d[:2, :])).sum()\n        numerator = np.sqrt(numerator)\n        denominator = np.sqrt(np.trace(np.dot((pose_2d_valid[:2, :] - mean_2d[:2, :]).transpose(),\n                                              pose_2d_valid[:2, :] - mean_2d[:2, :])).sum())\n        mean_2d = np.array([mean_2d[0, 0], mean_2d[1, 0], fx * input_scale / stride])\n        mean_3d = np.array([mean_3d[0, 0], mean_3d[1, 0], 0])\n        translation = numerator / denominator * mean_2d - mean_3d\n\n        if is_video:\n            translation = current_poses_2d[pose_id].filter(translation)\n        for kpt_id in range(19):\n            pose_3d[0, kpt_id] = pose_3d[0, kpt_id] + translation[0]\n            pose_3d[1, kpt_id] = pose_3d[1, kpt_id] + translation[1]\n            pose_3d[2, kpt_id] = pose_3d[2, kpt_id] + translation[2]\n        translated_poses_3d.append(pose_3d.transpose().reshape(-1))\n\n    return np.array(translated_poses_3d), np.array(poses_2d_scaled)\n"""
modules/pose.py,0,"b'import cv2\nimport numpy as np\n\nfrom modules.one_euro_filter import OneEuroFilter\n\n\nclass Pose:\n    num_kpts = 18\n    kpt_names = [\'neck\', \'nose\',\n                 \'l_sho\', \'l_elb\', \'l_wri\', \'l_hip\', \'l_knee\', \'l_ank\',\n                 \'r_sho\', \'r_elb\', \'r_wri\', \'r_hip\', \'r_knee\', \'r_ank\',\n                 \'r_eye\', \'l_eye\',\n                 \'r_ear\', \'l_ear\']\n    sigmas = np.array([.79, .26, .79, .72, .62, 1.07, .87, .89, .79, .72, .62, 1.07, .87, .89, .25, .25, .35, .35],\n                      dtype=np.float32) / 10.0\n    vars = (sigmas * 2) ** 2\n    last_id = -1\n    color = [0, 224, 255]\n\n    def __init__(self, keypoints, confidence):\n        super().__init__()\n        self.keypoints = keypoints\n        self.confidence = confidence\n        found_keypoints = np.zeros((np.count_nonzero(keypoints[:, 0] != -1), 2), dtype=np.int32)\n        found_kpt_id = 0\n        for kpt_id in range(keypoints.shape[0]):\n            if keypoints[kpt_id, 0] == -1:\n                continue\n            found_keypoints[found_kpt_id] = keypoints[kpt_id]\n            found_kpt_id += 1\n        self.bbox = cv2.boundingRect(found_keypoints)\n        self.id = None\n        self.translation_filter = [OneEuroFilter(freq=80, beta=0.01),\n                                   OneEuroFilter(freq=80, beta=0.01),\n                                   OneEuroFilter(freq=80, beta=0.01)]\n\n    def update_id(self, id=None):\n        self.id = id\n        if self.id is None:\n            self.id = Pose.last_id + 1\n            Pose.last_id += 1\n\n    def filter(self, translation):\n        filtered_translation = []\n        for coordinate_id in range(3):\n            filtered_translation.append(self.translation_filter[coordinate_id](translation[coordinate_id]))\n        return filtered_translation\n\n\ndef get_similarity(a, b, threshold=0.5):\n    num_similar_kpt = 0\n    for kpt_id in range(Pose.num_kpts):\n        if a.keypoints[kpt_id, 0] != -1 and b.keypoints[kpt_id, 0] != -1:\n            distance = np.sum((a.keypoints[kpt_id] - b.keypoints[kpt_id]) ** 2)\n            area = max(a.bbox[2] * a.bbox[3], b.bbox[2] * b.bbox[3])\n            similarity = np.exp(-distance / (2 * (area + np.spacing(1)) * Pose.vars[kpt_id]))\n            if similarity > threshold:\n                num_similar_kpt += 1\n    return num_similar_kpt\n\n\ndef propagate_ids(previous_poses, current_poses, threshold=3):\n    """"""Propagate poses ids from previous frame results. Id is propagated,\n    if there are at least `threshold` similar keypoints between pose from previous frame and current.\n\n    :param previous_poses: poses from previous frame with ids\n    :param current_poses: poses from current frame to assign ids\n    :param threshold: minimal number of similar keypoints between poses\n    :return: None\n    """"""\n    current_poses_sorted_ids = list(range(len(current_poses)))\n    current_poses_sorted_ids = sorted(\n        current_poses_sorted_ids, key=lambda pose_id: current_poses[pose_id].confidence, reverse=True)  # match confident poses first\n    mask = np.ones(len(previous_poses), dtype=np.int32)\n    for current_pose_id in current_poses_sorted_ids:\n        best_matched_id = None\n        best_matched_pose_id = None\n        best_matched_iou = 0\n        for previous_pose_id in range(len(previous_poses)):\n            if not mask[previous_pose_id]:\n                continue\n            iou = get_similarity(current_poses[current_pose_id], previous_poses[previous_pose_id])\n            if iou > best_matched_iou:\n                best_matched_iou = iou\n                best_matched_pose_id = previous_poses[previous_pose_id].id\n                best_matched_id = previous_pose_id\n        if best_matched_iou >= threshold:\n            mask[best_matched_id] = 0\n        else:  # pose not similar to any previous\n            best_matched_pose_id = None\n        current_poses[current_pose_id].update_id(best_matched_pose_id)\n        if best_matched_pose_id is not None:\n            current_poses[current_pose_id].translation_filter = previous_poses[best_matched_id].translation_filter\n'"
scripts/convert_to_onnx.py,3,"b""import argparse\n\nimport torch\n\nfrom models.with_mobilenet import PoseEstimationWithMobileNet\nfrom modules.load_state import load_state\n\n\ndef convert_to_onnx(net, output_name):\n    input = torch.randn(1, 3, 256, 448)\n    input_names = ['data']\n    output_names = ['features', 'heatmaps', 'pafs']\n\n    torch.onnx.export(net, input, output_name, verbose=True, input_names=input_names, output_names=output_names)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--checkpoint-path', type=str, required=True, help='path to the checkpoint')\n    parser.add_argument('--output-name', type=str, default='human-pose-estimation-3d.onnx',\n                        help='name of output model in ONNX format')\n    args = parser.parse_args()\n\n    net = PoseEstimationWithMobileNet(is_convertible_by_mo=True)\n    checkpoint = torch.load(args.checkpoint_path)\n    load_state(net, checkpoint)\n\n    convert_to_onnx(net, args.output_name)\n"""
