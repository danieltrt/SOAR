file_path,api_count,code
demo.py,2,"b""import argparse\n\nimport cv2\nimport numpy as np\nimport torch\n\nfrom models.with_mobilenet import PoseEstimationWithMobileNet\nfrom modules.keypoints import extract_keypoints, group_keypoints\nfrom modules.load_state import load_state\nfrom modules.pose import Pose, track_poses\nfrom val import normalize, pad_width\n\n\nclass ImageReader(object):\n    def __init__(self, file_names):\n        self.file_names = file_names\n        self.max_idx = len(file_names)\n\n    def __iter__(self):\n        self.idx = 0\n        return self\n\n    def __next__(self):\n        if self.idx == self.max_idx:\n            raise StopIteration\n        img = cv2.imread(self.file_names[self.idx], cv2.IMREAD_COLOR)\n        if img.size == 0:\n            raise IOError('Image {} cannot be read'.format(self.file_names[self.idx]))\n        self.idx = self.idx + 1\n        return img\n\n\nclass VideoReader(object):\n    def __init__(self, file_name):\n        self.file_name = file_name\n        try:  # OpenCV needs int to read from webcam\n            self.file_name = int(file_name)\n        except ValueError:\n            pass\n\n    def __iter__(self):\n        self.cap = cv2.VideoCapture(self.file_name)\n        if not self.cap.isOpened():\n            raise IOError('Video {} cannot be opened'.format(self.file_name))\n        return self\n\n    def __next__(self):\n        was_read, img = self.cap.read()\n        if not was_read:\n            raise StopIteration\n        return img\n\n\ndef infer_fast(net, img, net_input_height_size, stride, upsample_ratio, cpu,\n               pad_value=(0, 0, 0), img_mean=(128, 128, 128), img_scale=1/256):\n    height, width, _ = img.shape\n    scale = net_input_height_size / height\n\n    scaled_img = cv2.resize(img, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n    scaled_img = normalize(scaled_img, img_mean, img_scale)\n    min_dims = [net_input_height_size, max(scaled_img.shape[1], net_input_height_size)]\n    padded_img, pad = pad_width(scaled_img, stride, pad_value, min_dims)\n\n    tensor_img = torch.from_numpy(padded_img).permute(2, 0, 1).unsqueeze(0).float()\n    if not cpu:\n        tensor_img = tensor_img.cuda()\n\n    stages_output = net(tensor_img)\n\n    stage2_heatmaps = stages_output[-2]\n    heatmaps = np.transpose(stage2_heatmaps.squeeze().cpu().data.numpy(), (1, 2, 0))\n    heatmaps = cv2.resize(heatmaps, (0, 0), fx=upsample_ratio, fy=upsample_ratio, interpolation=cv2.INTER_CUBIC)\n\n    stage2_pafs = stages_output[-1]\n    pafs = np.transpose(stage2_pafs.squeeze().cpu().data.numpy(), (1, 2, 0))\n    pafs = cv2.resize(pafs, (0, 0), fx=upsample_ratio, fy=upsample_ratio, interpolation=cv2.INTER_CUBIC)\n\n    return heatmaps, pafs, scale, pad\n\n\ndef run_demo(net, image_provider, height_size, cpu, track, smooth):\n    net = net.eval()\n    if not cpu:\n        net = net.cuda()\n\n    stride = 8\n    upsample_ratio = 4\n    num_keypoints = Pose.num_kpts\n    previous_poses = []\n    delay = 33\n    for img in image_provider:\n        orig_img = img.copy()\n        heatmaps, pafs, scale, pad = infer_fast(net, img, height_size, stride, upsample_ratio, cpu)\n\n        total_keypoints_num = 0\n        all_keypoints_by_type = []\n        for kpt_idx in range(num_keypoints):  # 19th for bg\n            total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n\n        pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs, demo=True)\n        for kpt_id in range(all_keypoints.shape[0]):\n            all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n            all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n        current_poses = []\n        for n in range(len(pose_entries)):\n            if len(pose_entries[n]) == 0:\n                continue\n            pose_keypoints = np.ones((num_keypoints, 2), dtype=np.int32) * -1\n            for kpt_id in range(num_keypoints):\n                if pose_entries[n][kpt_id] != -1.0:  # keypoint was found\n                    pose_keypoints[kpt_id, 0] = int(all_keypoints[int(pose_entries[n][kpt_id]), 0])\n                    pose_keypoints[kpt_id, 1] = int(all_keypoints[int(pose_entries[n][kpt_id]), 1])\n            pose = Pose(pose_keypoints, pose_entries[n][18])\n            current_poses.append(pose)\n\n        if track:\n            track_poses(previous_poses, current_poses, smooth=smooth)\n            previous_poses = current_poses\n        for pose in current_poses:\n            pose.draw(img)\n        img = cv2.addWeighted(orig_img, 0.6, img, 0.4, 0)\n        for pose in current_poses:\n            cv2.rectangle(img, (pose.bbox[0], pose.bbox[1]),\n                          (pose.bbox[0] + pose.bbox[2], pose.bbox[1] + pose.bbox[3]), (0, 255, 0))\n            if track:\n                cv2.putText(img, 'id: {}'.format(pose.id), (pose.bbox[0], pose.bbox[1] - 16),\n                            cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255))\n        cv2.imshow('Lightweight Human Pose Estimation Python Demo', img)\n        key = cv2.waitKey(delay)\n        if key == 27:  # esc\n            return\n        elif key == 112:  # 'p'\n            if delay == 33:\n                delay = 0\n            else:\n                delay = 33\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description='''Lightweight human pose estimation python demo.\n                       This is just for quick results preview.\n                       Please, consider c++ demo for the best performance.''')\n    parser.add_argument('--checkpoint-path', type=str, required=True, help='path to the checkpoint')\n    parser.add_argument('--height-size', type=int, default=256, help='network input layer height size')\n    parser.add_argument('--video', type=str, default='', help='path to video file or camera id')\n    parser.add_argument('--images', nargs='+', default='', help='path to input image(s)')\n    parser.add_argument('--cpu', action='store_true', help='run network inference on cpu')\n    parser.add_argument('--track', type=int, default=1, help='track pose id in video')\n    parser.add_argument('--smooth', type=int, default=1, help='smooth pose keypoints')\n    args = parser.parse_args()\n\n    if args.video == '' and args.images == '':\n        raise ValueError('Either --video or --image has to be provided')\n\n    net = PoseEstimationWithMobileNet()\n    checkpoint = torch.load(args.checkpoint_path, map_location='cpu')\n    load_state(net, checkpoint)\n\n    frame_provider = ImageReader(args.images)\n    if args.video != '':\n        frame_provider = VideoReader(args.video)\n    else:\n        args.track = 0\n\n    run_demo(net, frame_provider, args.height_size, args.cpu, args.track, args.smooth)\n"""
train.py,5,"b""import argparse\nimport cv2\nimport os\n\nimport torch\nfrom torch.nn import DataParallel\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\nfrom datasets.coco import CocoTrainDataset\nfrom datasets.transformations import ConvertKeypoints, Scale, Rotate, CropPad, Flip\nfrom modules.get_parameters import get_parameters_conv, get_parameters_bn, get_parameters_conv_depthwise\nfrom models.with_mobilenet import PoseEstimationWithMobileNet\nfrom modules.loss import l2_loss\nfrom modules.load_state import load_state, load_from_mobilenet\nfrom val import evaluate\n\ncv2.setNumThreads(0)\ncv2.ocl.setUseOpenCL(False)  # To prevent freeze of DataLoader\n\n\ndef train(prepared_train_labels, train_images_folder, num_refinement_stages, base_lr, batch_size, batches_per_iter,\n          num_workers, checkpoint_path, weights_only, from_mobilenet, checkpoints_folder, log_after,\n          val_labels, val_images_folder, val_output_name, checkpoint_after, val_after):\n    net = PoseEstimationWithMobileNet(num_refinement_stages)\n\n    stride = 8\n    sigma = 7\n    path_thickness = 1\n    dataset = CocoTrainDataset(prepared_train_labels, train_images_folder,\n                               stride, sigma, path_thickness,\n                               transform=transforms.Compose([\n                                   ConvertKeypoints(),\n                                   Scale(),\n                                   Rotate(pad=(128, 128, 128)),\n                                   CropPad(pad=(128, 128, 128)),\n                                   Flip()]))\n    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n\n    optimizer = optim.Adam([\n        {'params': get_parameters_conv(net.model, 'weight')},\n        {'params': get_parameters_conv_depthwise(net.model, 'weight'), 'weight_decay': 0},\n        {'params': get_parameters_bn(net.model, 'weight'), 'weight_decay': 0},\n        {'params': get_parameters_bn(net.model, 'bias'), 'lr': base_lr * 2, 'weight_decay': 0},\n        {'params': get_parameters_conv(net.cpm, 'weight'), 'lr': base_lr},\n        {'params': get_parameters_conv(net.cpm, 'bias'), 'lr': base_lr * 2, 'weight_decay': 0},\n        {'params': get_parameters_conv_depthwise(net.cpm, 'weight'), 'weight_decay': 0},\n        {'params': get_parameters_conv(net.initial_stage, 'weight'), 'lr': base_lr},\n        {'params': get_parameters_conv(net.initial_stage, 'bias'), 'lr': base_lr * 2, 'weight_decay': 0},\n        {'params': get_parameters_conv(net.refinement_stages, 'weight'), 'lr': base_lr * 4},\n        {'params': get_parameters_conv(net.refinement_stages, 'bias'), 'lr': base_lr * 8, 'weight_decay': 0},\n        {'params': get_parameters_bn(net.refinement_stages, 'weight'), 'weight_decay': 0},\n        {'params': get_parameters_bn(net.refinement_stages, 'bias'), 'lr': base_lr * 2, 'weight_decay': 0},\n    ], lr=base_lr, weight_decay=5e-4)\n\n    num_iter = 0\n    current_epoch = 0\n    drop_after_epoch = [100, 200, 260]\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=drop_after_epoch, gamma=0.333)\n    if checkpoint_path:\n        checkpoint = torch.load(checkpoint_path)\n\n        if from_mobilenet:\n            load_from_mobilenet(net, checkpoint)\n        else:\n            load_state(net, checkpoint)\n            if not weights_only:\n                optimizer.load_state_dict(checkpoint['optimizer'])\n                scheduler.load_state_dict(checkpoint['scheduler'])\n                num_iter = checkpoint['iter']\n                current_epoch = checkpoint['current_epoch']\n\n    net = DataParallel(net).cuda()\n    net.train()\n    for epochId in range(current_epoch, 280):\n        scheduler.step()\n        total_losses = [0, 0] * (num_refinement_stages + 1)  # heatmaps loss, paf loss per stage\n        batch_per_iter_idx = 0\n        for batch_data in train_loader:\n            if batch_per_iter_idx == 0:\n                optimizer.zero_grad()\n\n            images = batch_data['image'].cuda()\n            keypoint_masks = batch_data['keypoint_mask'].cuda()\n            paf_masks = batch_data['paf_mask'].cuda()\n            keypoint_maps = batch_data['keypoint_maps'].cuda()\n            paf_maps = batch_data['paf_maps'].cuda()\n\n            stages_output = net(images)\n\n            losses = []\n            for loss_idx in range(len(total_losses) // 2):\n                losses.append(l2_loss(stages_output[loss_idx * 2], keypoint_maps, keypoint_masks, images.shape[0]))\n                losses.append(l2_loss(stages_output[loss_idx * 2 + 1], paf_maps, paf_masks, images.shape[0]))\n                total_losses[loss_idx * 2] += losses[-2].item() / batches_per_iter\n                total_losses[loss_idx * 2 + 1] += losses[-1].item() / batches_per_iter\n\n            loss = losses[0]\n            for loss_idx in range(1, len(losses)):\n                loss += losses[loss_idx]\n            loss /= batches_per_iter\n            loss.backward()\n            batch_per_iter_idx += 1\n            if batch_per_iter_idx == batches_per_iter:\n                optimizer.step()\n                batch_per_iter_idx = 0\n                num_iter += 1\n            else:\n                continue\n\n            if num_iter % log_after == 0:\n                print('Iter: {}'.format(num_iter))\n                for loss_idx in range(len(total_losses) // 2):\n                    print('\\n'.join(['stage{}_pafs_loss:     {}', 'stage{}_heatmaps_loss: {}']).format(\n                        loss_idx + 1, total_losses[loss_idx * 2 + 1] / log_after,\n                        loss_idx + 1, total_losses[loss_idx * 2] / log_after))\n                for loss_idx in range(len(total_losses)):\n                    total_losses[loss_idx] = 0\n            if num_iter % checkpoint_after == 0:\n                snapshot_name = '{}/checkpoint_iter_{}.pth'.format(checkpoints_folder, num_iter)\n                torch.save({'state_dict': net.module.state_dict(),\n                            'optimizer': optimizer.state_dict(),\n                            'scheduler': scheduler.state_dict(),\n                            'iter': num_iter,\n                            'current_epoch': epochId},\n                           snapshot_name)\n            if num_iter % val_after == 0:\n                print('Validation...')\n                evaluate(val_labels, val_output_name, val_images_folder, net)\n                net.train()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--prepared-train-labels', type=str, required=True,\n                        help='path to the file with prepared annotations')\n    parser.add_argument('--train-images-folder', type=str, required=True, help='path to COCO train images folder')\n    parser.add_argument('--num-refinement-stages', type=int, default=1, help='number of refinement stages')\n    parser.add_argument('--base-lr', type=float, default=4e-5, help='initial learning rate')\n    parser.add_argument('--batch-size', type=int, default=80, help='batch size')\n    parser.add_argument('--batches-per-iter', type=int, default=1, help='number of batches to accumulate gradient from')\n    parser.add_argument('--num-workers', type=int, default=8, help='number of workers')\n    parser.add_argument('--checkpoint-path', type=str, required=True, help='path to the checkpoint to continue training from')\n    parser.add_argument('--from-mobilenet', action='store_true',\n                        help='load weights from mobilenet feature extractor')\n    parser.add_argument('--weights-only', action='store_true',\n                        help='just initialize layers with pre-trained weights and start training from the beginning')\n    parser.add_argument('--experiment-name', type=str, default='default',\n                        help='experiment name to create folder for checkpoints')\n    parser.add_argument('--log-after', type=int, default=100, help='number of iterations to print train loss')\n\n    parser.add_argument('--val-labels', type=str, required=True, help='path to json with keypoints val labels')\n    parser.add_argument('--val-images-folder', type=str, required=True, help='path to COCO val images folder')\n    parser.add_argument('--val-output-name', type=str, default='detections.json',\n                        help='name of output json file with detected keypoints')\n    parser.add_argument('--checkpoint-after', type=int, default=5000,\n                        help='number of iterations to save checkpoint')\n    parser.add_argument('--val-after', type=int, default=5000,\n                        help='number of iterations to run validation')\n    args = parser.parse_args()\n\n    checkpoints_folder = '{}_checkpoints'.format(args.experiment_name)\n    if not os.path.exists(checkpoints_folder):\n        os.makedirs(checkpoints_folder)\n\n    train(args.prepared_train_labels, args.train_images_folder, args.num_refinement_stages, args.base_lr, args.batch_size,\n          args.batches_per_iter, args.num_workers, args.checkpoint_path, args.weights_only, args.from_mobilenet,\n          checkpoints_folder, args.log_after, args.val_labels, args.val_images_folder, args.val_output_name,\n          args.checkpoint_after, args.val_after)\n"""
val.py,2,"b""import argparse\nimport cv2\nimport json\nimport math\nimport numpy as np\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nimport torch\n\nfrom datasets.coco import CocoValDataset\nfrom models.with_mobilenet import PoseEstimationWithMobileNet\nfrom modules.keypoints import extract_keypoints, group_keypoints\nfrom modules.load_state import load_state\n\n\ndef run_coco_eval(gt_file_path, dt_file_path):\n    annotation_type = 'keypoints'\n    print('Running test for {} results.'.format(annotation_type))\n\n    coco_gt = COCO(gt_file_path)\n    coco_dt = coco_gt.loadRes(dt_file_path)\n\n    result = COCOeval(coco_gt, coco_dt, annotation_type)\n    result.evaluate()\n    result.accumulate()\n    result.summarize()\n\n\ndef normalize(img, img_mean, img_scale):\n    img = np.array(img, dtype=np.float32)\n    img = (img - img_mean) * img_scale\n    return img\n\n\ndef pad_width(img, stride, pad_value, min_dims):\n    h, w, _ = img.shape\n    h = min(min_dims[0], h)\n    min_dims[0] = math.ceil(min_dims[0] / float(stride)) * stride\n    min_dims[1] = max(min_dims[1], w)\n    min_dims[1] = math.ceil(min_dims[1] / float(stride)) * stride\n    pad = []\n    pad.append(int(math.floor((min_dims[0] - h) / 2.0)))\n    pad.append(int(math.floor((min_dims[1] - w) / 2.0)))\n    pad.append(int(min_dims[0] - h - pad[0]))\n    pad.append(int(min_dims[1] - w - pad[1]))\n    padded_img = cv2.copyMakeBorder(img, pad[0], pad[2], pad[1], pad[3],\n                                    cv2.BORDER_CONSTANT, value=pad_value)\n    return padded_img, pad\n\n\ndef convert_to_coco_format(pose_entries, all_keypoints):\n    coco_keypoints = []\n    scores = []\n    for n in range(len(pose_entries)):\n        if len(pose_entries[n]) == 0:\n            continue\n        keypoints = [0] * 17 * 3\n        to_coco_map = [0, -1, 6, 8, 10, 5, 7, 9, 12, 14, 16, 11, 13, 15, 2, 1, 4, 3]\n        person_score = pose_entries[n][-2]\n        position_id = -1\n        for keypoint_id in pose_entries[n][:-2]:\n            position_id += 1\n            if position_id == 1:  # no 'neck' in COCO\n                continue\n\n            cx, cy, score, visibility = 0, 0, 0, 0  # keypoint not found\n            if keypoint_id != -1:\n                cx, cy, score = all_keypoints[int(keypoint_id), 0:3]\n                cx = cx + 0.5\n                cy = cy + 0.5\n                visibility = 1\n            keypoints[to_coco_map[position_id] * 3 + 0] = cx\n            keypoints[to_coco_map[position_id] * 3 + 1] = cy\n            keypoints[to_coco_map[position_id] * 3 + 2] = visibility\n        coco_keypoints.append(keypoints)\n        scores.append(person_score * max(0, (pose_entries[n][-1] - 1)))  # -1 for 'neck'\n    return coco_keypoints, scores\n\n\ndef infer(net, img, scales, base_height, stride, pad_value=(0, 0, 0), img_mean=(128, 128, 128), img_scale=1/256):\n    normed_img = normalize(img, img_mean, img_scale)\n    height, width, _ = normed_img.shape\n    scales_ratios = [scale * base_height / float(height) for scale in scales]\n    avg_heatmaps = np.zeros((height, width, 19), dtype=np.float32)\n    avg_pafs = np.zeros((height, width, 38), dtype=np.float32)\n\n    for ratio in scales_ratios:\n        scaled_img = cv2.resize(normed_img, (0, 0), fx=ratio, fy=ratio, interpolation=cv2.INTER_CUBIC)\n        min_dims = [base_height, max(scaled_img.shape[1], base_height)]\n        padded_img, pad = pad_width(scaled_img, stride, pad_value, min_dims)\n\n        tensor_img = torch.from_numpy(padded_img).permute(2, 0, 1).unsqueeze(0).float().cuda()\n        stages_output = net(tensor_img)\n\n        stage2_heatmaps = stages_output[-2]\n        heatmaps = np.transpose(stage2_heatmaps.squeeze().cpu().data.numpy(), (1, 2, 0))\n        heatmaps = cv2.resize(heatmaps, (0, 0), fx=stride, fy=stride, interpolation=cv2.INTER_CUBIC)\n        heatmaps = heatmaps[pad[0]:heatmaps.shape[0] - pad[2], pad[1]:heatmaps.shape[1] - pad[3]:, :]\n        heatmaps = cv2.resize(heatmaps, (width, height), interpolation=cv2.INTER_CUBIC)\n        avg_heatmaps = avg_heatmaps + heatmaps / len(scales_ratios)\n\n        stage2_pafs = stages_output[-1]\n        pafs = np.transpose(stage2_pafs.squeeze().cpu().data.numpy(), (1, 2, 0))\n        pafs = cv2.resize(pafs, (0, 0), fx=stride, fy=stride, interpolation=cv2.INTER_CUBIC)\n        pafs = pafs[pad[0]:pafs.shape[0] - pad[2], pad[1]:pafs.shape[1] - pad[3], :]\n        pafs = cv2.resize(pafs, (width, height), interpolation=cv2.INTER_CUBIC)\n        avg_pafs = avg_pafs + pafs / len(scales_ratios)\n\n    return avg_heatmaps, avg_pafs\n\n\ndef evaluate(labels, output_name, images_folder, net, multiscale=False, visualize=False):\n    net = net.cuda().eval()\n    base_height = 368\n    scales = [1]\n    if multiscale:\n        scales = [0.5, 1.0, 1.5, 2.0]\n    stride = 8\n\n    dataset = CocoValDataset(labels, images_folder)\n    coco_result = []\n    for sample in dataset:\n        file_name = sample['file_name']\n        img = sample['img']\n\n        avg_heatmaps, avg_pafs = infer(net, img, scales, base_height, stride)\n\n        total_keypoints_num = 0\n        all_keypoints_by_type = []\n        for kpt_idx in range(18):  # 19th for bg\n            total_keypoints_num += extract_keypoints(avg_heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n\n        pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, avg_pafs)\n\n        coco_keypoints, scores = convert_to_coco_format(pose_entries, all_keypoints)\n\n        image_id = int(file_name[0:file_name.rfind('.')])\n        for idx in range(len(coco_keypoints)):\n            coco_result.append({\n                'image_id': image_id,\n                'category_id': 1,  # person\n                'keypoints': coco_keypoints[idx],\n                'score': scores[idx]\n            })\n\n        if visualize:\n            for keypoints in coco_keypoints:\n                for idx in range(len(keypoints) // 3):\n                    cv2.circle(img, (int(keypoints[idx * 3]), int(keypoints[idx * 3 + 1])),\n                               3, (255, 0, 255), -1)\n            cv2.imshow('keypoints', img)\n            key = cv2.waitKey()\n            if key == 27:  # esc\n                return\n\n    with open(output_name, 'w') as f:\n        json.dump(coco_result, f, indent=4)\n\n    run_coco_eval(labels, output_name)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--labels', type=str, required=True, help='path to json with keypoints val labels')\n    parser.add_argument('--output-name', type=str, default='detections.json',\n                        help='name of output json file with detected keypoints')\n    parser.add_argument('--images-folder', type=str, required=True, help='path to COCO val images folder')\n    parser.add_argument('--checkpoint-path', type=str, required=True, help='path to the checkpoint')\n    parser.add_argument('--multiscale', action='store_true', help='average inference results over multiple scales')\n    parser.add_argument('--visualize', action='store_true', help='show keypoints')\n    args = parser.parse_args()\n\n    net = PoseEstimationWithMobileNet()\n    checkpoint = torch.load(args.checkpoint_path)\n    load_state(net, checkpoint)\n\n    evaluate(args.labels, args.output_name, args.images_folder, net, args.multiscale, args.visualize)\n"""
datasets/__init__.py,0,b''
datasets/coco.py,1,"b""import copy\nimport json\nimport math\nimport os\nimport pickle\n\nimport cv2\nimport numpy as np\nimport pycocotools\n\nfrom torch.utils.data.dataset import Dataset\n\nBODY_PARTS_KPT_IDS = [[1, 8], [8, 9], [9, 10], [1, 11], [11, 12], [12, 13], [1, 2], [2, 3], [3, 4], [2, 16],\n                      [1, 5], [5, 6], [6, 7], [5, 17], [1, 0], [0, 14], [0, 15], [14, 16], [15, 17]]\n\n\ndef get_mask(segmentations, mask):\n    for segmentation in segmentations:\n        rle = pycocotools.mask.frPyObjects(segmentation, mask.shape[0], mask.shape[1])\n        mask[pycocotools.mask.decode(rle) > 0.5] = 0\n    return mask\n\n\nclass CocoTrainDataset(Dataset):\n    def __init__(self, labels, images_folder, stride, sigma, paf_thickness, transform=None):\n        super().__init__()\n        self._images_folder = images_folder\n        self._stride = stride\n        self._sigma = sigma\n        self._paf_thickness = paf_thickness\n        self._transform = transform\n        with open(labels, 'rb') as f:\n            self._labels = pickle.load(f)\n\n    def __getitem__(self, idx):\n        label = copy.deepcopy(self._labels[idx])  # label modified in transform\n        image = cv2.imread(os.path.join(self._images_folder, label['img_paths']), cv2.IMREAD_COLOR)\n        mask = np.ones(shape=(label['img_height'], label['img_width']), dtype=np.float32)\n        mask = get_mask(label['segmentations'], mask)\n        sample = {\n            'label': label,\n            'image': image,\n            'mask': mask\n        }\n        if self._transform:\n            sample = self._transform(sample)\n\n        mask = cv2.resize(sample['mask'], dsize=None, fx=1/self._stride, fy=1/self._stride, interpolation=cv2.INTER_AREA)\n        keypoint_maps = self._generate_keypoint_maps(sample)\n        sample['keypoint_maps'] = keypoint_maps\n        keypoint_mask = np.zeros(shape=keypoint_maps.shape, dtype=np.float32)\n        for idx in range(keypoint_mask.shape[0]):\n            keypoint_mask[idx] = mask\n        sample['keypoint_mask'] = keypoint_mask\n\n        paf_maps = self._generate_paf_maps(sample)\n        sample['paf_maps'] = paf_maps\n        paf_mask = np.zeros(shape=paf_maps.shape, dtype=np.float32)\n        for idx in range(paf_mask.shape[0]):\n            paf_mask[idx] = mask\n        sample['paf_mask'] = paf_mask\n\n        image = sample['image'].astype(np.float32)\n        image = (image - 128) / 256\n        sample['image'] = image.transpose((2, 0, 1))\n        return sample\n\n    def __len__(self):\n        return len(self._labels)\n\n    def _generate_keypoint_maps(self, sample):\n        n_keypoints = 18\n        n_rows, n_cols, _ = sample['image'].shape\n        keypoint_maps = np.zeros(shape=(n_keypoints + 1,\n                                        n_rows // self._stride, n_cols // self._stride), dtype=np.float32)  # +1 for bg\n\n        label = sample['label']\n        for keypoint_idx in range(n_keypoints):\n            keypoint = label['keypoints'][keypoint_idx]\n            if keypoint[2] <= 1:\n                self._add_gaussian(keypoint_maps[keypoint_idx], keypoint[0], keypoint[1], self._stride, self._sigma)\n            for another_annotation in label['processed_other_annotations']:\n                keypoint = another_annotation['keypoints'][keypoint_idx]\n                if keypoint[2] <= 1:\n                    self._add_gaussian(keypoint_maps[keypoint_idx], keypoint[0], keypoint[1], self._stride, self._sigma)\n        keypoint_maps[-1] = 1 - keypoint_maps.max(axis=0)\n        return keypoint_maps\n\n    def _add_gaussian(self, keypoint_map, x, y, stride, sigma):\n        n_sigma = 4\n        tl = [int(x - n_sigma * sigma), int(y - n_sigma * sigma)]\n        tl[0] = max(tl[0], 0)\n        tl[1] = max(tl[1], 0)\n\n        br = [int(x + n_sigma * sigma), int(y + n_sigma * sigma)]\n        map_h, map_w = keypoint_map.shape\n        br[0] = min(br[0], map_w * stride)\n        br[1] = min(br[1], map_h * stride)\n\n        shift = stride / 2 - 0.5\n        for map_y in range(tl[1] // stride, br[1] // stride):\n            for map_x in range(tl[0] // stride, br[0] // stride):\n                d2 = (map_x * stride + shift - x) * (map_x * stride + shift - x) + \\\n                    (map_y * stride + shift - y) * (map_y * stride + shift - y)\n                exponent = d2 / 2 / sigma / sigma\n                if exponent > 4.6052:  # threshold, ln(100), ~0.01\n                    continue\n                keypoint_map[map_y, map_x] += math.exp(-exponent)\n                if keypoint_map[map_y, map_x] > 1:\n                    keypoint_map[map_y, map_x] = 1\n\n    def _generate_paf_maps(self, sample):\n        n_pafs = len(BODY_PARTS_KPT_IDS)\n        n_rows, n_cols, _ = sample['image'].shape\n        paf_maps = np.zeros(shape=(n_pafs * 2, n_rows // self._stride, n_cols // self._stride), dtype=np.float32)\n\n        label = sample['label']\n        for paf_idx in range(n_pafs):\n            keypoint_a = label['keypoints'][BODY_PARTS_KPT_IDS[paf_idx][0]]\n            keypoint_b = label['keypoints'][BODY_PARTS_KPT_IDS[paf_idx][1]]\n            if keypoint_a[2] <= 1 and keypoint_b[2] <= 1:\n                self._set_paf(paf_maps[paf_idx * 2:paf_idx * 2 + 2],\n                              keypoint_a[0], keypoint_a[1], keypoint_b[0], keypoint_b[1],\n                              self._stride, self._paf_thickness)\n            for another_annotation in label['processed_other_annotations']:\n                keypoint_a = another_annotation['keypoints'][BODY_PARTS_KPT_IDS[paf_idx][0]]\n                keypoint_b = another_annotation['keypoints'][BODY_PARTS_KPT_IDS[paf_idx][1]]\n                if keypoint_a[2] <= 1 and keypoint_b[2] <= 1:\n                    self._set_paf(paf_maps[paf_idx * 2:paf_idx * 2 + 2],\n                                  keypoint_a[0], keypoint_a[1], keypoint_b[0], keypoint_b[1],\n                                  self._stride, self._paf_thickness)\n        return paf_maps\n\n    def _set_paf(self, paf_map, x_a, y_a, x_b, y_b, stride, thickness):\n        x_a /= stride\n        y_a /= stride\n        x_b /= stride\n        y_b /= stride\n        x_ba = x_b - x_a\n        y_ba = y_b - y_a\n        _, h_map, w_map = paf_map.shape\n        x_min = int(max(min(x_a, x_b) - thickness, 0))\n        x_max = int(min(max(x_a, x_b) + thickness, w_map))\n        y_min = int(max(min(y_a, y_b) - thickness, 0))\n        y_max = int(min(max(y_a, y_b) + thickness, h_map))\n        norm_ba = (x_ba * x_ba + y_ba * y_ba) ** 0.5\n        if norm_ba < 1e-7:  # Same points, no paf\n            return\n        x_ba /= norm_ba\n        y_ba /= norm_ba\n\n        for y in range(y_min, y_max):\n            for x in range(x_min, x_max):\n                x_ca = x - x_a\n                y_ca = y - y_a\n                d = math.fabs(x_ca * y_ba - y_ca * x_ba)\n                if d <= thickness:\n                    paf_map[0, y, x] = x_ba\n                    paf_map[1, y, x] = y_ba\n\n\nclass CocoValDataset(Dataset):\n    def __init__(self, labels, images_folder):\n        super().__init__()\n        with open(labels, 'r') as f:\n            self._labels = json.load(f)\n        self._images_folder = images_folder\n\n    def __getitem__(self, idx):\n        file_name = self._labels['images'][idx]['file_name']\n        img = cv2.imread(os.path.join(self._images_folder, file_name), cv2.IMREAD_COLOR)\n        return {\n            'img': img,\n            'file_name': file_name\n        }\n\n    def __len__(self):\n        return len(self._labels['images'])\n"""
datasets/transformations.py,0,"b""import random\n\nimport cv2\nimport numpy as np\n\n\nclass ConvertKeypoints:\n    def __call__(self, sample):\n        label = sample['label']\n        h, w, _ = sample['image'].shape\n        keypoints = label['keypoints']\n        for keypoint in keypoints:  # keypoint[2] == 0: occluded, == 1: visible, == 2: not in image\n            if keypoint[0] == keypoint[1] == 0:\n                keypoint[2] = 2\n            if (keypoint[0] < 0\n                    or keypoint[0] >= w\n                    or keypoint[1] < 0\n                    or keypoint[1] >= h):\n                keypoint[2] = 2\n        for other_label in label['processed_other_annotations']:\n            keypoints = other_label['keypoints']\n            for keypoint in keypoints:\n                if keypoint[0] == keypoint[1] == 0:\n                    keypoint[2] = 2\n                if (keypoint[0] < 0\n                        or keypoint[0] >= w\n                        or keypoint[1] < 0\n                        or keypoint[1] >= h):\n                    keypoint[2] = 2\n        label['keypoints'] = self._convert(label['keypoints'], w, h)\n\n        for other_label in label['processed_other_annotations']:\n            other_label['keypoints'] = self._convert(other_label['keypoints'], w, h)\n        return sample\n\n    def _convert(self, keypoints, w, h):\n        # Nose, Neck, R hand, L hand, R leg, L leg, Eyes, Ears\n        reorder_map = [1, 7, 9, 11, 6, 8, 10, 13, 15, 17, 12, 14, 16, 3, 2, 5, 4]\n        converted_keypoints = list(keypoints[i - 1] for i in reorder_map)\n        converted_keypoints.insert(1, [(keypoints[5][0] + keypoints[6][0]) / 2,\n                                       (keypoints[5][1] + keypoints[6][1]) / 2, 0])  # Add neck as a mean of shoulders\n        if keypoints[5][2] == 2 or keypoints[6][2] == 2:\n            converted_keypoints[1][2] = 2\n        elif keypoints[5][2] == 1 and keypoints[6][2] == 1:\n            converted_keypoints[1][2] = 1\n        if (converted_keypoints[1][0] < 0\n                or converted_keypoints[1][0] >= w\n                or converted_keypoints[1][1] < 0\n                or converted_keypoints[1][1] >= h):\n            converted_keypoints[1][2] = 2\n        return converted_keypoints\n\n\nclass Scale:\n    def __init__(self, prob=1, min_scale=0.5, max_scale=1.1, target_dist=0.6):\n        self._prob = prob\n        self._min_scale = min_scale\n        self._max_scale = max_scale\n        self._target_dist = target_dist\n\n    def __call__(self, sample):\n        prob = random.random()\n        scale_multiplier = 1\n        if prob <= self._prob:\n            prob = random.random()\n            scale_multiplier = (self._max_scale - self._min_scale) * prob + self._min_scale\n        label = sample['label']\n        scale_abs = self._target_dist / label['scale_provided']\n        scale = scale_abs * scale_multiplier\n        sample['image'] = cv2.resize(sample['image'], dsize=(0, 0), fx=scale, fy=scale)\n        label['img_height'], label['img_width'], _ = sample['image'].shape\n        sample['mask'] = cv2.resize(sample['mask'], dsize=(0, 0), fx=scale, fy=scale)\n\n        label['objpos'][0] *= scale\n        label['objpos'][1] *= scale\n        for keypoint in sample['label']['keypoints']:\n            keypoint[0] *= scale\n            keypoint[1] *= scale\n        for other_annotation in sample['label']['processed_other_annotations']:\n            other_annotation['objpos'][0] *= scale\n            other_annotation['objpos'][1] *= scale\n            for keypoint in other_annotation['keypoints']:\n                keypoint[0] *= scale\n                keypoint[1] *= scale\n        return sample\n\n\nclass Rotate:\n    def __init__(self, pad, max_rotate_degree=40):\n        self._pad = pad\n        self._max_rotate_degree = max_rotate_degree\n\n    def __call__(self, sample):\n        prob = random.random()\n        degree = (prob - 0.5) * 2 * self._max_rotate_degree\n        h, w, _ = sample['image'].shape\n        img_center = (w / 2, h / 2)\n        R = cv2.getRotationMatrix2D(img_center, degree, 1)\n\n        abs_cos = abs(R[0, 0])\n        abs_sin = abs(R[0, 1])\n\n        bound_w = int(h * abs_sin + w * abs_cos)\n        bound_h = int(h * abs_cos + w * abs_sin)\n        dsize = (bound_w, bound_h)\n\n        R[0, 2] += dsize[0] / 2 - img_center[0]\n        R[1, 2] += dsize[1] / 2 - img_center[1]\n        sample['image'] = cv2.warpAffine(sample['image'], R, dsize=dsize,\n                                         borderMode=cv2.BORDER_CONSTANT, borderValue=self._pad)\n        sample['label']['img_height'], sample['label']['img_width'], _ = sample['image'].shape\n        sample['mask'] = cv2.warpAffine(sample['mask'], R, dsize=dsize,\n                                        borderMode=cv2.BORDER_CONSTANT, borderValue=(1, 1, 1))  # border is ok\n        label = sample['label']\n        label['objpos'] = self._rotate(label['objpos'], R)\n        for keypoint in label['keypoints']:\n            point = [keypoint[0], keypoint[1]]\n            point = self._rotate(point, R)\n            keypoint[0], keypoint[1] = point[0], point[1]\n        for other_annotation in label['processed_other_annotations']:\n            for keypoint in other_annotation['keypoints']:\n                point = [keypoint[0], keypoint[1]]\n                point = self._rotate(point, R)\n                keypoint[0], keypoint[1] = point[0], point[1]\n        return sample\n\n    def _rotate(self, point, R):\n        return [R[0, 0] * point[0] + R[0, 1] * point[1] + R[0, 2],\n                R[1, 0] * point[0] + R[1, 1] * point[1] + R[1, 2]]\n\n\nclass CropPad:\n    def __init__(self, pad, center_perterb_max=40, crop_x=368, crop_y=368):\n        self._pad = pad\n        self._center_perterb_max = center_perterb_max\n        self._crop_x = crop_x\n        self._crop_y = crop_y\n\n    def __call__(self, sample):\n        prob_x = random.random()\n        prob_y = random.random()\n\n        offset_x = int((prob_x - 0.5) * 2 * self._center_perterb_max)\n        offset_y = int((prob_y - 0.5) * 2 * self._center_perterb_max)\n        label = sample['label']\n        shifted_center = (label['objpos'][0] + offset_x, label['objpos'][1] + offset_y)\n        offset_left = -int(shifted_center[0] - self._crop_x / 2)\n        offset_up = -int(shifted_center[1] - self._crop_y / 2)\n\n        cropped_image = np.empty(shape=(self._crop_y, self._crop_x, 3), dtype=np.uint8)\n        for i in range(3):\n            cropped_image[:, :, i].fill(self._pad[i])\n        cropped_mask = np.empty(shape=(self._crop_y, self._crop_x), dtype=np.uint8)\n        cropped_mask.fill(1)\n\n        image_x_start = int(shifted_center[0] - self._crop_x / 2)\n        image_y_start = int(shifted_center[1] - self._crop_y / 2)\n        image_x_finish = image_x_start + self._crop_x\n        image_y_finish = image_y_start + self._crop_y\n        crop_x_start = 0\n        crop_y_start = 0\n        crop_x_finish = self._crop_x\n        crop_y_finish = self._crop_y\n\n        w, h = label['img_width'], label['img_height']\n        should_crop = True\n        if image_x_start < 0:  # Adjust crop area\n            crop_x_start -= image_x_start\n            image_x_start = 0\n        if image_x_start >= w:\n            should_crop = False\n\n        if image_y_start < 0:\n            crop_y_start -= image_y_start\n            image_y_start = 0\n        if image_y_start >= w:\n            should_crop = False\n\n        if image_x_finish > w:\n            diff = image_x_finish - w\n            image_x_finish -= diff\n            crop_x_finish -= diff\n        if image_x_finish < 0:\n            should_crop = False\n\n        if image_y_finish > h:\n            diff = image_y_finish - h\n            image_y_finish -= diff\n            crop_y_finish -= diff\n        if image_y_finish < 0:\n            should_crop = False\n\n        if should_crop:\n            cropped_image[crop_y_start:crop_y_finish, crop_x_start:crop_x_finish, :] =\\\n                sample['image'][image_y_start:image_y_finish, image_x_start:image_x_finish, :]\n            cropped_mask[crop_y_start:crop_y_finish, crop_x_start:crop_x_finish] =\\\n                sample['mask'][image_y_start:image_y_finish, image_x_start:image_x_finish]\n\n        sample['image'] = cropped_image\n        sample['mask'] = cropped_mask\n        label['img_width'] = self._crop_x\n        label['img_height'] = self._crop_y\n\n        label['objpos'][0] += offset_left\n        label['objpos'][1] += offset_up\n        for keypoint in label['keypoints']:\n            keypoint[0] += offset_left\n            keypoint[1] += offset_up\n        for other_annotation in label['processed_other_annotations']:\n            for keypoint in other_annotation['keypoints']:\n                keypoint[0] += offset_left\n                keypoint[1] += offset_up\n\n        return sample\n\n    def _inside(self, point, width, height):\n        if point[0] < 0 or point[1] < 0:\n            return False\n        if point[0] >= width or point[1] >= height:\n            return False\n        return True\n\n\nclass Flip:\n    def __init__(self, prob=0.5):\n        self._prob = prob\n\n    def __call__(self, sample):\n        prob = random.random()\n        do_flip = prob <= self._prob\n        if not do_flip:\n            return sample\n\n        sample['image'] = cv2.flip(sample['image'], 1)\n        sample['mask'] = cv2.flip(sample['mask'], 1)\n\n        label = sample['label']\n        w, h = label['img_width'], label['img_height']\n        label['objpos'][0] = w - 1 - label['objpos'][0]\n        for keypoint in label['keypoints']:\n            keypoint[0] = w - 1 - keypoint[0]\n        label['keypoints'] = self._swap_left_right(label['keypoints'])\n\n        for other_annotation in label['processed_other_annotations']:\n            other_annotation['objpos'][0] = w - 1 - other_annotation['objpos'][0]\n            for keypoint in other_annotation['keypoints']:\n                keypoint[0] = w - 1 - keypoint[0]\n            other_annotation['keypoints'] = self._swap_left_right(other_annotation['keypoints'])\n\n        return sample\n\n    def _swap_left_right(self, keypoints):\n        right = [2, 3, 4, 8, 9, 10, 14, 16]\n        left = [5, 6, 7, 11, 12, 13, 15, 17]\n        for r, l in zip(right, left):\n            keypoints[r], keypoints[l] = keypoints[l], keypoints[r]\n        return keypoints\n"""
models/__init__.py,0,b''
models/with_mobilenet.py,1,"b'import torch\nfrom torch import nn\n\nfrom modules.conv import conv, conv_dw, conv_dw_no_bn\n\n\nclass Cpm(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.align = conv(in_channels, out_channels, kernel_size=1, padding=0, bn=False)\n        self.trunk = nn.Sequential(\n            conv_dw_no_bn(out_channels, out_channels),\n            conv_dw_no_bn(out_channels, out_channels),\n            conv_dw_no_bn(out_channels, out_channels)\n        )\n        self.conv = conv(out_channels, out_channels, bn=False)\n\n    def forward(self, x):\n        x = self.align(x)\n        x = self.conv(x + self.trunk(x))\n        return x\n\n\nclass InitialStage(nn.Module):\n    def __init__(self, num_channels, num_heatmaps, num_pafs):\n        super().__init__()\n        self.trunk = nn.Sequential(\n            conv(num_channels, num_channels, bn=False),\n            conv(num_channels, num_channels, bn=False),\n            conv(num_channels, num_channels, bn=False)\n        )\n        self.heatmaps = nn.Sequential(\n            conv(num_channels, 512, kernel_size=1, padding=0, bn=False),\n            conv(512, num_heatmaps, kernel_size=1, padding=0, bn=False, relu=False)\n        )\n        self.pafs = nn.Sequential(\n            conv(num_channels, 512, kernel_size=1, padding=0, bn=False),\n            conv(512, num_pafs, kernel_size=1, padding=0, bn=False, relu=False)\n        )\n\n    def forward(self, x):\n        trunk_features = self.trunk(x)\n        heatmaps = self.heatmaps(trunk_features)\n        pafs = self.pafs(trunk_features)\n        return [heatmaps, pafs]\n\n\nclass RefinementStageBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.initial = conv(in_channels, out_channels, kernel_size=1, padding=0, bn=False)\n        self.trunk = nn.Sequential(\n            conv(out_channels, out_channels),\n            conv(out_channels, out_channels, dilation=2, padding=2)\n        )\n\n    def forward(self, x):\n        initial_features = self.initial(x)\n        trunk_features = self.trunk(initial_features)\n        return initial_features + trunk_features\n\n\nclass RefinementStage(nn.Module):\n    def __init__(self, in_channels, out_channels, num_heatmaps, num_pafs):\n        super().__init__()\n        self.trunk = nn.Sequential(\n            RefinementStageBlock(in_channels, out_channels),\n            RefinementStageBlock(out_channels, out_channels),\n            RefinementStageBlock(out_channels, out_channels),\n            RefinementStageBlock(out_channels, out_channels),\n            RefinementStageBlock(out_channels, out_channels)\n        )\n        self.heatmaps = nn.Sequential(\n            conv(out_channels, out_channels, kernel_size=1, padding=0, bn=False),\n            conv(out_channels, num_heatmaps, kernel_size=1, padding=0, bn=False, relu=False)\n        )\n        self.pafs = nn.Sequential(\n            conv(out_channels, out_channels, kernel_size=1, padding=0, bn=False),\n            conv(out_channels, num_pafs, kernel_size=1, padding=0, bn=False, relu=False)\n        )\n\n    def forward(self, x):\n        trunk_features = self.trunk(x)\n        heatmaps = self.heatmaps(trunk_features)\n        pafs = self.pafs(trunk_features)\n        return [heatmaps, pafs]\n\n\nclass PoseEstimationWithMobileNet(nn.Module):\n    def __init__(self, num_refinement_stages=1, num_channels=128, num_heatmaps=19, num_pafs=38):\n        super().__init__()\n        self.model = nn.Sequential(\n            conv(     3,  32, stride=2, bias=False),\n            conv_dw( 32,  64),\n            conv_dw( 64, 128, stride=2),\n            conv_dw(128, 128),\n            conv_dw(128, 256, stride=2),\n            conv_dw(256, 256),\n            conv_dw(256, 512),  # conv4_2\n            conv_dw(512, 512, dilation=2, padding=2),\n            conv_dw(512, 512),\n            conv_dw(512, 512),\n            conv_dw(512, 512),\n            conv_dw(512, 512)   # conv5_5\n        )\n        self.cpm = Cpm(512, num_channels)\n\n        self.initial_stage = InitialStage(num_channels, num_heatmaps, num_pafs)\n        self.refinement_stages = nn.ModuleList()\n        for idx in range(num_refinement_stages):\n            self.refinement_stages.append(RefinementStage(num_channels + num_heatmaps + num_pafs, num_channels,\n                                                          num_heatmaps, num_pafs))\n\n    def forward(self, x):\n        backbone_features = self.model(x)\n        backbone_features = self.cpm(backbone_features)\n\n        stages_output = self.initial_stage(backbone_features)\n        for refinement_stage in self.refinement_stages:\n            stages_output.extend(\n                refinement_stage(torch.cat([backbone_features, stages_output[-2], stages_output[-1]], dim=1)))\n\n        return stages_output\n'"
modules/__init__.py,0,b''
modules/conv.py,0,"b'from torch import nn\n\n\ndef conv(in_channels, out_channels, kernel_size=3, padding=1, bn=True, dilation=1, stride=1, relu=True, bias=True):\n    modules = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=bias)]\n    if bn:\n        modules.append(nn.BatchNorm2d(out_channels))\n    if relu:\n        modules.append(nn.ReLU(inplace=True))\n    return nn.Sequential(*modules)\n\n\ndef conv_dw(in_channels, out_channels, kernel_size=3, padding=1, stride=1, dilation=1):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation=dilation, groups=in_channels, bias=False),\n        nn.BatchNorm2d(in_channels),\n        nn.ReLU(inplace=True),\n\n        nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True),\n    )\n\n\ndef conv_dw_no_bn(in_channels, out_channels, kernel_size=3, padding=1, stride=1, dilation=1):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation=dilation, groups=in_channels, bias=False),\n        nn.ELU(inplace=True),\n\n        nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),\n        nn.ELU(inplace=True),\n    )\n'"
modules/get_parameters.py,0,"b'from torch import nn\n\n\ndef get_parameters(model, predicate):\n    for module in model.modules():\n        for param_name, param in module.named_parameters():\n            if predicate(module, param_name):\n                yield param\n\n\ndef get_parameters_conv(model, name):\n    return get_parameters(model, lambda m, p: isinstance(m, nn.Conv2d) and m.groups == 1 and p == name)\n\n\ndef get_parameters_conv_depthwise(model, name):\n    return get_parameters(model, lambda m, p: isinstance(m, nn.Conv2d)\n                                              and m.groups == m.in_channels\n                                              and m.in_channels == m.out_channels\n                                              and p == name)\n\n\ndef get_parameters_bn(model, name):\n    return get_parameters(model, lambda m, p: isinstance(m, nn.BatchNorm2d) and p == name)\n'"
modules/keypoints.py,0,"b""import math\nimport numpy as np\nfrom operator import itemgetter\n\nBODY_PARTS_KPT_IDS = [[1, 2], [1, 5], [2, 3], [3, 4], [5, 6], [6, 7], [1, 8], [8, 9], [9, 10], [1, 11],\n                      [11, 12], [12, 13], [1, 0], [0, 14], [14, 16], [0, 15], [15, 17], [2, 16], [5, 17]]\nBODY_PARTS_PAF_IDS = ([12, 13], [20, 21], [14, 15], [16, 17], [22, 23], [24, 25], [0, 1], [2, 3], [4, 5],\n                      [6, 7], [8, 9], [10, 11], [28, 29], [30, 31], [34, 35], [32, 33], [36, 37], [18, 19], [26, 27])\n\n\ndef linspace2d(start, stop, n=10):\n    points = 1 / (n - 1) * (stop - start)\n    return points[:, None] * np.arange(n) + start[:, None]\n\n\ndef extract_keypoints(heatmap, all_keypoints, total_keypoint_num):\n    heatmap[heatmap < 0.1] = 0\n    heatmap_with_borders = np.pad(heatmap, [(2, 2), (2, 2)], mode='constant')\n    heatmap_center = heatmap_with_borders[1:heatmap_with_borders.shape[0]-1, 1:heatmap_with_borders.shape[1]-1]\n    heatmap_left = heatmap_with_borders[1:heatmap_with_borders.shape[0]-1, 2:heatmap_with_borders.shape[1]]\n    heatmap_right = heatmap_with_borders[1:heatmap_with_borders.shape[0]-1, 0:heatmap_with_borders.shape[1]-2]\n    heatmap_up = heatmap_with_borders[2:heatmap_with_borders.shape[0], 1:heatmap_with_borders.shape[1]-1]\n    heatmap_down = heatmap_with_borders[0:heatmap_with_borders.shape[0]-2, 1:heatmap_with_borders.shape[1]-1]\n\n    heatmap_peaks = (heatmap_center > heatmap_left) &\\\n                    (heatmap_center > heatmap_right) &\\\n                    (heatmap_center > heatmap_up) &\\\n                    (heatmap_center > heatmap_down)\n    heatmap_peaks = heatmap_peaks[1:heatmap_center.shape[0]-1, 1:heatmap_center.shape[1]-1]\n    keypoints = list(zip(np.nonzero(heatmap_peaks)[1], np.nonzero(heatmap_peaks)[0]))  # (w, h)\n    keypoints = sorted(keypoints, key=itemgetter(0))\n\n    suppressed = np.zeros(len(keypoints), np.uint8)\n    keypoints_with_score_and_id = []\n    keypoint_num = 0\n    for i in range(len(keypoints)):\n        if suppressed[i]:\n            continue\n        for j in range(i+1, len(keypoints)):\n            if math.sqrt((keypoints[i][0] - keypoints[j][0]) ** 2 +\n                         (keypoints[i][1] - keypoints[j][1]) ** 2) < 6:\n                suppressed[j] = 1\n        keypoint_with_score_and_id = (keypoints[i][0], keypoints[i][1], heatmap[keypoints[i][1], keypoints[i][0]],\n                                      total_keypoint_num + keypoint_num)\n        keypoints_with_score_and_id.append(keypoint_with_score_and_id)\n        keypoint_num += 1\n    all_keypoints.append(keypoints_with_score_and_id)\n    return keypoint_num\n\n\ndef group_keypoints(all_keypoints_by_type, pafs, pose_entry_size=20, min_paf_score=0.05, demo=False):\n    pose_entries = []\n    all_keypoints = np.array([item for sublist in all_keypoints_by_type for item in sublist])\n    for part_id in range(len(BODY_PARTS_PAF_IDS)):\n        part_pafs = pafs[:, :, BODY_PARTS_PAF_IDS[part_id]]\n        kpts_a = all_keypoints_by_type[BODY_PARTS_KPT_IDS[part_id][0]]\n        kpts_b = all_keypoints_by_type[BODY_PARTS_KPT_IDS[part_id][1]]\n        num_kpts_a = len(kpts_a)\n        num_kpts_b = len(kpts_b)\n        kpt_a_id = BODY_PARTS_KPT_IDS[part_id][0]\n        kpt_b_id = BODY_PARTS_KPT_IDS[part_id][1]\n\n        if num_kpts_a == 0 and num_kpts_b == 0:  # no keypoints for such body part\n            continue\n        elif num_kpts_a == 0:  # body part has just 'b' keypoints\n            for i in range(num_kpts_b):\n                num = 0\n                for j in range(len(pose_entries)):  # check if already in some pose, was added by another body part\n                    if pose_entries[j][kpt_b_id] == kpts_b[i][3]:\n                        num += 1\n                        continue\n                if num == 0:\n                    pose_entry = np.ones(pose_entry_size) * -1\n                    pose_entry[kpt_b_id] = kpts_b[i][3]  # keypoint idx\n                    pose_entry[-1] = 1                   # num keypoints in pose\n                    pose_entry[-2] = kpts_b[i][2]        # pose score\n                    pose_entries.append(pose_entry)\n            continue\n        elif num_kpts_b == 0:  # body part has just 'a' keypoints\n            for i in range(num_kpts_a):\n                num = 0\n                for j in range(len(pose_entries)):\n                    if pose_entries[j][kpt_a_id] == kpts_a[i][3]:\n                        num += 1\n                        continue\n                if num == 0:\n                    pose_entry = np.ones(pose_entry_size) * -1\n                    pose_entry[kpt_a_id] = kpts_a[i][3]\n                    pose_entry[-1] = 1\n                    pose_entry[-2] = kpts_a[i][2]\n                    pose_entries.append(pose_entry)\n            continue\n\n        connections = []\n        for i in range(num_kpts_a):\n            kpt_a = np.array(kpts_a[i][0:2])\n            for j in range(num_kpts_b):\n                kpt_b = np.array(kpts_b[j][0:2])\n                mid_point = [(), ()]\n                mid_point[0] = (int(round((kpt_a[0] + kpt_b[0]) * 0.5)),\n                                int(round((kpt_a[1] + kpt_b[1]) * 0.5)))\n                mid_point[1] = mid_point[0]\n\n                vec = [kpt_b[0] - kpt_a[0], kpt_b[1] - kpt_a[1]]\n                vec_norm = math.sqrt(vec[0] ** 2 + vec[1] ** 2)\n                if vec_norm == 0:\n                    continue\n                vec[0] /= vec_norm\n                vec[1] /= vec_norm\n                cur_point_score = (vec[0] * part_pafs[mid_point[0][1], mid_point[0][0], 0] +\n                                   vec[1] * part_pafs[mid_point[1][1], mid_point[1][0], 1])\n\n                height_n = pafs.shape[0] // 2\n                success_ratio = 0\n                point_num = 10  # number of points to integration over paf\n                if cur_point_score > -100:\n                    passed_point_score = 0\n                    passed_point_num = 0\n                    x, y = linspace2d(kpt_a, kpt_b)\n                    for point_idx in range(point_num):\n                        if not demo:\n                            px = int(round(x[point_idx]))\n                            py = int(round(y[point_idx]))\n                        else:\n                            px = int(x[point_idx])\n                            py = int(y[point_idx])\n                        paf = part_pafs[py, px, 0:2]\n                        cur_point_score = vec[0] * paf[0] + vec[1] * paf[1]\n                        if cur_point_score > min_paf_score:\n                            passed_point_score += cur_point_score\n                            passed_point_num += 1\n                    success_ratio = passed_point_num / point_num\n                    ratio = 0\n                    if passed_point_num > 0:\n                        ratio = passed_point_score / passed_point_num\n                    ratio += min(height_n / vec_norm - 1, 0)\n                if ratio > 0 and success_ratio > 0.8:\n                    score_all = ratio + kpts_a[i][2] + kpts_b[j][2]\n                    connections.append([i, j, ratio, score_all])\n        if len(connections) > 0:\n            connections = sorted(connections, key=itemgetter(2), reverse=True)\n\n        num_connections = min(num_kpts_a, num_kpts_b)\n        has_kpt_a = np.zeros(num_kpts_a, dtype=np.int32)\n        has_kpt_b = np.zeros(num_kpts_b, dtype=np.int32)\n        filtered_connections = []\n        for row in range(len(connections)):\n            if len(filtered_connections) == num_connections:\n                break\n            i, j, cur_point_score = connections[row][0:3]\n            if not has_kpt_a[i] and not has_kpt_b[j]:\n                filtered_connections.append([kpts_a[i][3], kpts_b[j][3], cur_point_score])\n                has_kpt_a[i] = 1\n                has_kpt_b[j] = 1\n        connections = filtered_connections\n        if len(connections) == 0:\n            continue\n\n        if part_id == 0:\n            pose_entries = [np.ones(pose_entry_size) * -1 for _ in range(len(connections))]\n            for i in range(len(connections)):\n                pose_entries[i][BODY_PARTS_KPT_IDS[0][0]] = connections[i][0]\n                pose_entries[i][BODY_PARTS_KPT_IDS[0][1]] = connections[i][1]\n                pose_entries[i][-1] = 2\n                pose_entries[i][-2] = np.sum(all_keypoints[connections[i][0:2], 2]) + connections[i][2]\n        elif part_id == 17 or part_id == 18:\n            kpt_a_id = BODY_PARTS_KPT_IDS[part_id][0]\n            kpt_b_id = BODY_PARTS_KPT_IDS[part_id][1]\n            for i in range(len(connections)):\n                for j in range(len(pose_entries)):\n                    if pose_entries[j][kpt_a_id] == connections[i][0] and pose_entries[j][kpt_b_id] == -1:\n                        pose_entries[j][kpt_b_id] = connections[i][1]\n                    elif pose_entries[j][kpt_b_id] == connections[i][1] and pose_entries[j][kpt_a_id] == -1:\n                        pose_entries[j][kpt_a_id] = connections[i][0]\n            continue\n        else:\n            kpt_a_id = BODY_PARTS_KPT_IDS[part_id][0]\n            kpt_b_id = BODY_PARTS_KPT_IDS[part_id][1]\n            for i in range(len(connections)):\n                num = 0\n                for j in range(len(pose_entries)):\n                    if pose_entries[j][kpt_a_id] == connections[i][0]:\n                        pose_entries[j][kpt_b_id] = connections[i][1]\n                        num += 1\n                        pose_entries[j][-1] += 1\n                        pose_entries[j][-2] += all_keypoints[connections[i][1], 2] + connections[i][2]\n                if num == 0:\n                    pose_entry = np.ones(pose_entry_size) * -1\n                    pose_entry[kpt_a_id] = connections[i][0]\n                    pose_entry[kpt_b_id] = connections[i][1]\n                    pose_entry[-1] = 2\n                    pose_entry[-2] = np.sum(all_keypoints[connections[i][0:2], 2]) + connections[i][2]\n                    pose_entries.append(pose_entry)\n\n    filtered_entries = []\n    for i in range(len(pose_entries)):\n        if pose_entries[i][-1] < 3 or (pose_entries[i][-2] / pose_entries[i][-1] < 0.2):\n            continue\n        filtered_entries.append(pose_entries[i])\n    pose_entries = np.asarray(filtered_entries)\n    return pose_entries, all_keypoints\n"""
modules/load_state.py,0,"b""import collections\n\n\ndef load_state(net, checkpoint):\n    source_state = checkpoint['state_dict']\n    target_state = net.state_dict()\n    new_target_state = collections.OrderedDict()\n    for target_key, target_value in target_state.items():\n        if target_key in source_state and source_state[target_key].size() == target_state[target_key].size():\n            new_target_state[target_key] = source_state[target_key]\n        else:\n            new_target_state[target_key] = target_state[target_key]\n            print('[WARNING] Not found pre-trained parameters for {}'.format(target_key))\n\n    net.load_state_dict(new_target_state)\n\n\ndef load_from_mobilenet(net, checkpoint):\n    source_state = checkpoint['state_dict']\n    target_state = net.state_dict()\n    new_target_state = collections.OrderedDict()\n    for target_key, target_value in target_state.items():\n        k = target_key\n        if k.find('model') != -1:\n            k = k.replace('model', 'module.model')\n        if k in source_state and source_state[k].size() == target_state[target_key].size():\n            new_target_state[target_key] = source_state[k]\n        else:\n            new_target_state[target_key] = target_state[target_key]\n            print('[WARNING] Not found pre-trained parameters for {}'.format(target_key))\n\n    net.load_state_dict(new_target_state)\n"""
modules/loss.py,0,"b'def l2_loss(input, target, mask, batch_size):\n    loss = (input - target) * mask\n    loss = (loss * loss) / 2 / batch_size\n\n    return loss.sum()\n'"
modules/one_euro_filter.py,0,"b""import math\n\n\ndef get_alpha(rate=30, cutoff=1):\n    tau = 1 / (2 * math.pi * cutoff)\n    te = 1 / rate\n    return 1 / (1 + tau / te)\n\n\nclass LowPassFilter:\n    def __init__(self):\n        self.x_previous = None\n\n    def __call__(self, x, alpha=0.5):\n        if self.x_previous is None:\n            self.x_previous = x\n            return x\n        x_filtered = alpha * x + (1 - alpha) * self.x_previous\n        self.x_previous = x_filtered\n        return x_filtered\n\n\nclass OneEuroFilter:\n    def __init__(self, freq=15, mincutoff=1, beta=0.05, dcutoff=1):\n        self.freq = freq\n        self.mincutoff = mincutoff\n        self.beta = beta\n        self.dcutoff = dcutoff\n        self.filter_x = LowPassFilter()\n        self.filter_dx = LowPassFilter()\n        self.x_previous = None\n        self.dx = None\n\n    def __call__(self, x):\n        if self.dx is None:\n            self.dx = 0\n        else:\n            self.dx = (x - self.x_previous) * self.freq\n        dx_smoothed = self.filter_dx(self.dx, get_alpha(self.freq, self.dcutoff))\n        cutoff = self.mincutoff + self.beta * abs(dx_smoothed)\n        x_filtered = self.filter_x(x, get_alpha(self.freq, cutoff))\n        self.x_previous = x\n        return x_filtered\n\n\nif __name__ == '__main__':\n    filter = OneEuroFilter(freq=15, beta=0.1)\n    for val in range(10):\n        x = val + (-1)**(val % 2)\n        x_filtered = filter(x)\n        print(x_filtered, x)\n"""
modules/pose.py,0,"b'import cv2\nimport numpy as np\n\nfrom modules.keypoints import BODY_PARTS_KPT_IDS, BODY_PARTS_PAF_IDS\nfrom modules.one_euro_filter import OneEuroFilter\n\n\nclass Pose:\n    num_kpts = 18\n    kpt_names = [\'nose\', \'neck\',\n                 \'r_sho\', \'r_elb\', \'r_wri\', \'l_sho\', \'l_elb\', \'l_wri\',\n                 \'r_hip\', \'r_knee\', \'r_ank\', \'l_hip\', \'l_knee\', \'l_ank\',\n                 \'r_eye\', \'l_eye\',\n                 \'r_ear\', \'l_ear\']\n    sigmas = np.array([.26, .79, .79, .72, .62, .79, .72, .62, 1.07, .87, .89, 1.07, .87, .89, .25, .25, .35, .35],\n                      dtype=np.float32) / 10.0\n    vars = (sigmas * 2) ** 2\n    last_id = -1\n    color = [0, 224, 255]\n\n    def __init__(self, keypoints, confidence):\n        super().__init__()\n        self.keypoints = keypoints\n        self.confidence = confidence\n        self.bbox = Pose.get_bbox(self.keypoints)\n        self.id = None\n        self.filters = [[OneEuroFilter(), OneEuroFilter()] for _ in range(Pose.num_kpts)]\n\n    @staticmethod\n    def get_bbox(keypoints):\n        found_keypoints = np.zeros((np.count_nonzero(keypoints[:, 0] != -1), 2), dtype=np.int32)\n        found_kpt_id = 0\n        for kpt_id in range(Pose.num_kpts):\n            if keypoints[kpt_id, 0] == -1:\n                continue\n            found_keypoints[found_kpt_id] = keypoints[kpt_id]\n            found_kpt_id += 1\n        bbox = cv2.boundingRect(found_keypoints)\n        return bbox\n\n    def update_id(self, id=None):\n        self.id = id\n        if self.id is None:\n            self.id = Pose.last_id + 1\n            Pose.last_id += 1\n\n    def draw(self, img):\n        assert self.keypoints.shape == (Pose.num_kpts, 2)\n\n        for part_id in range(len(BODY_PARTS_PAF_IDS) - 2):\n            kpt_a_id = BODY_PARTS_KPT_IDS[part_id][0]\n            global_kpt_a_id = self.keypoints[kpt_a_id, 0]\n            if global_kpt_a_id != -1:\n                x_a, y_a = self.keypoints[kpt_a_id]\n                cv2.circle(img, (int(x_a), int(y_a)), 3, Pose.color, -1)\n            kpt_b_id = BODY_PARTS_KPT_IDS[part_id][1]\n            global_kpt_b_id = self.keypoints[kpt_b_id, 0]\n            if global_kpt_b_id != -1:\n                x_b, y_b = self.keypoints[kpt_b_id]\n                cv2.circle(img, (int(x_b), int(y_b)), 3, Pose.color, -1)\n            if global_kpt_a_id != -1 and global_kpt_b_id != -1:\n                cv2.line(img, (int(x_a), int(y_a)), (int(x_b), int(y_b)), Pose.color, 2)\n\n\ndef get_similarity(a, b, threshold=0.5):\n    num_similar_kpt = 0\n    for kpt_id in range(Pose.num_kpts):\n        if a.keypoints[kpt_id, 0] != -1 and b.keypoints[kpt_id, 0] != -1:\n            distance = np.sum((a.keypoints[kpt_id] - b.keypoints[kpt_id]) ** 2)\n            area = max(a.bbox[2] * a.bbox[3], b.bbox[2] * b.bbox[3])\n            similarity = np.exp(-distance / (2 * (area + np.spacing(1)) * Pose.vars[kpt_id]))\n            if similarity > threshold:\n                num_similar_kpt += 1\n    return num_similar_kpt\n\n\ndef track_poses(previous_poses, current_poses, threshold=3, smooth=False):\n    """"""Propagate poses ids from previous frame results. Id is propagated,\n    if there are at least `threshold` similar keypoints between pose from previous frame and current.\n    If correspondence between pose on previous and current frame was established, pose keypoints are smoothed.\n\n    :param previous_poses: poses from previous frame with ids\n    :param current_poses: poses from current frame to assign ids\n    :param threshold: minimal number of similar keypoints between poses\n    :param smooth: smooth pose keypoints between frames\n    :return: None\n    """"""\n    current_poses = sorted(current_poses, key=lambda pose: pose.confidence, reverse=True)  # match confident poses first\n    mask = np.ones(len(previous_poses), dtype=np.int32)\n    for current_pose in current_poses:\n        best_matched_id = None\n        best_matched_pose_id = None\n        best_matched_iou = 0\n        for id, previous_pose in enumerate(previous_poses):\n            if not mask[id]:\n                continue\n            iou = get_similarity(current_pose, previous_pose)\n            if iou > best_matched_iou:\n                best_matched_iou = iou\n                best_matched_pose_id = previous_pose.id\n                best_matched_id = id\n        if best_matched_iou >= threshold:\n            mask[best_matched_id] = 0\n        else:  # pose not similar to any previous\n            best_matched_pose_id = None\n        current_pose.update_id(best_matched_pose_id)\n\n        if smooth:\n            for kpt_id in range(Pose.num_kpts):\n                if current_pose.keypoints[kpt_id, 0] == -1:\n                    continue\n                # reuse filter if previous pose has valid filter\n                if (best_matched_pose_id is not None\n                        and previous_poses[best_matched_id].keypoints[kpt_id, 0] != -1):\n                    current_pose.filters[kpt_id] = previous_poses[best_matched_id].filters[kpt_id]\n                current_pose.keypoints[kpt_id, 0] = current_pose.filters[kpt_id][0](current_pose.keypoints[kpt_id, 0])\n                current_pose.keypoints[kpt_id, 1] = current_pose.filters[kpt_id][1](current_pose.keypoints[kpt_id, 1])\n            current_pose.bbox = Pose.get_bbox(current_pose.keypoints)\n'"
scripts/convert_to_onnx.py,3,"b""import argparse\n\nimport torch\n\nfrom models.with_mobilenet import PoseEstimationWithMobileNet\nfrom modules.load_state import load_state\n\n\ndef convert_to_onnx(net, output_name):\n    input = torch.randn(1, 3, 256, 456)\n    input_names = ['data']\n    output_names = ['stage_0_output_1_heatmaps', 'stage_0_output_0_pafs',\n                    'stage_1_output_1_heatmaps', 'stage_1_output_0_pafs']\n\n    torch.onnx.export(net, input, output_name, verbose=True, input_names=input_names, output_names=output_names)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--checkpoint-path', type=str, required=True, help='path to the checkpoint')\n    parser.add_argument('--output-name', type=str, default='human-pose-estimation.onnx',\n                        help='name of output model in ONNX format')\n    args = parser.parse_args()\n\n    net = PoseEstimationWithMobileNet()\n    checkpoint = torch.load(args.checkpoint_path)\n    load_state(net, checkpoint)\n\n    convert_to_onnx(net, args.output_name)\n"""
scripts/make_val_subset.py,0,"b""import argparse\nimport json\nimport random\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--labels', type=str, required=True, help='path to json with keypoints val labels')\n    parser.add_argument('--output-name', type=str, default='val_subset.json',\n                        help='name of output file with subset of val labels')\n    parser.add_argument('--num-images', type=int, default=250, help='number of images in subset')\n    args = parser.parse_args()\n\n    with open(args.labels, 'r') as f:\n        data = json.load(f)\n\n    random.seed(0)\n    total_val_images = 5000\n    idxs = list(range(total_val_images))\n    random.shuffle(idxs)\n\n    images_by_id = {}\n    for idx in idxs[:args.num_images]:\n        images_by_id[data['images'][idx]['id']] = data['images'][idx]\n\n    annotations_by_image_id = {}\n    for annotation in data['annotations']:\n        if annotation['image_id'] in images_by_id:\n            if not annotation['image_id'] in annotations_by_image_id:\n                annotations_by_image_id[annotation['image_id']] = []\n            annotations_by_image_id[annotation['image_id']].append(annotation)\n\n    subset = {\n        'info': data['info'],\n        'licenses': data['licenses'],\n        'images': [],\n        'annotations': [],\n        'categories': data['categories']\n    }\n    for image_id, image in images_by_id.items():\n        subset['images'].append(image)\n        if image_id in annotations_by_image_id:  # image has at least 1 annotation\n            subset['annotations'].extend(annotations_by_image_id[image_id])\n\n    with open(args.output_name, 'w') as f:\n        json.dump(subset, f, indent=4)\n\n"""
scripts/prepare_train_labels.py,0,"b'import argparse\nimport json\nimport pickle\n\n\ndef prepare_annotations(annotations_per_image, images_info, net_input_size):\n    """"""Prepare labels for training. For each annotated person calculates center\n    to perform crop around it during the training. Also converts data to the internal format.\n\n    :param annotations_per_image: all annotations for specified image id\n    :param images_info: auxiliary information about all images\n    :param net_input_size: network input size during training\n    :return: list of prepared annotations\n    """"""\n    prepared_annotations = []\n    for _, annotations in annotations_per_image.items():\n        previous_centers = []\n        for annotation in annotations[0]:\n            if (annotation[\'num_keypoints\'] < 5\n                    or annotation[\'area\'] < 32 * 32):\n                continue\n            person_center = [annotation[\'bbox\'][0] + annotation[\'bbox\'][2] / 2,\n                             annotation[\'bbox\'][1] + annotation[\'bbox\'][3] / 2]\n            is_close = False\n            for previous_center in previous_centers:\n                distance_to_previous = ((person_center[0] - previous_center[0]) ** 2\n                                        + (person_center[1] - previous_center[1]) ** 2) ** 0.5\n                if distance_to_previous < previous_center[2] * 0.3:\n                    is_close = True\n                    break\n            if is_close:\n                continue\n\n            prepared_annotation = {\n                \'img_paths\': images_info[annotation[\'image_id\']][\'file_name\'],\n                \'img_width\': images_info[annotation[\'image_id\']][\'width\'],\n                \'img_height\': images_info[annotation[\'image_id\']][\'height\'],\n                \'objpos\': person_center,\n                \'image_id\': annotation[\'image_id\'],\n                \'bbox\': annotation[\'bbox\'],\n                \'segment_area\': annotation[\'area\'],\n                \'scale_provided\': annotation[\'bbox\'][3] / net_input_size,\n                \'num_keypoints\': annotation[\'num_keypoints\'],\n                \'segmentations\': annotations[1]\n            }\n\n            keypoints = []\n            for i in range(len(annotation[\'keypoints\']) // 3):\n                keypoint = [annotation[\'keypoints\'][i * 3], annotation[\'keypoints\'][i * 3 + 1], 2]\n                if annotation[\'keypoints\'][i * 3 + 2] == 1:\n                    keypoint[2] = 0\n                elif annotation[\'keypoints\'][i * 3 + 2] == 2:\n                    keypoint[2] = 1\n                keypoints.append(keypoint)\n            prepared_annotation[\'keypoints\'] = keypoints\n\n            prepared_other_annotations = []\n            for other_annotation in annotations[0]:\n                if other_annotation == annotation:\n                    continue\n\n                prepared_other_annotation = {\n                    \'objpos\': [other_annotation[\'bbox\'][0] + other_annotation[\'bbox\'][2] / 2,\n                               other_annotation[\'bbox\'][1] + other_annotation[\'bbox\'][3] / 2],\n                    \'bbox\': other_annotation[\'bbox\'],\n                    \'segment_area\': other_annotation[\'area\'],\n                    \'scale_provided\': other_annotation[\'bbox\'][3] / net_input_size,\n                    \'num_keypoints\': other_annotation[\'num_keypoints\']\n                }\n\n                keypoints = []\n                for i in range(len(other_annotation[\'keypoints\']) // 3):\n                    keypoint = [other_annotation[\'keypoints\'][i * 3], other_annotation[\'keypoints\'][i * 3 + 1], 2]\n                    if other_annotation[\'keypoints\'][i * 3 + 2] == 1:\n                        keypoint[2] = 0\n                    elif other_annotation[\'keypoints\'][i * 3 + 2] == 2:\n                        keypoint[2] = 1\n                    keypoints.append(keypoint)\n                prepared_other_annotation[\'keypoints\'] = keypoints\n                prepared_other_annotations.append(prepared_other_annotation)\n\n            prepared_annotation[\'processed_other_annotations\'] = prepared_other_annotations\n            prepared_annotations.append(prepared_annotation)\n\n            previous_centers.append((person_center[0], person_center[1], annotation[\'bbox\'][2], annotation[\'bbox\'][3]))\n    return prepared_annotations\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--labels\', type=str, required=True, help=\'path to json with keypoints train labels\')\n    parser.add_argument(\'--output-name\', type=str, default=\'prepared_train_annotation.pkl\',\n                        help=\'name of output file with prepared keypoints annotation\')\n    parser.add_argument(\'--net-input-size\', type=int, default=368, help=\'network input size\')\n    args = parser.parse_args()\n    with open(args.labels, \'r\') as f:\n        data = json.load(f)\n\n    annotations_per_image_mapping = {}\n    for annotation in data[\'annotations\']:\n        if annotation[\'num_keypoints\'] != 0 and not annotation[\'iscrowd\']:\n            if annotation[\'image_id\'] not in annotations_per_image_mapping:\n                annotations_per_image_mapping[annotation[\'image_id\']] = [[], []]\n            annotations_per_image_mapping[annotation[\'image_id\']][0].append(annotation)\n\n    crowd_segmentations_per_image_mapping = {}\n    for annotation in data[\'annotations\']:\n        if annotation[\'iscrowd\']:\n            if annotation[\'image_id\'] not in crowd_segmentations_per_image_mapping:\n                crowd_segmentations_per_image_mapping[annotation[\'image_id\']] = []\n            crowd_segmentations_per_image_mapping[annotation[\'image_id\']].append(annotation[\'segmentation\'])\n\n    for image_id, crowd_segmentations in crowd_segmentations_per_image_mapping.items():\n        if image_id in annotations_per_image_mapping:\n            annotations_per_image_mapping[image_id][1] = crowd_segmentations\n\n    images_info = {}\n    for image_info in data[\'images\']:\n        images_info[image_info[\'id\']] = image_info\n\n    prepared_annotations = prepare_annotations(annotations_per_image_mapping, images_info, args.net_input_size)\n\n    with open(args.output_name, \'wb\') as f:\n        pickle.dump(prepared_annotations, f)\n\n'"
