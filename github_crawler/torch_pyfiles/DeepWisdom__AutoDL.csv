file_path,api_count,code
download_public_datasets.py,0,"b'#!/usr/bin/python\n# Author: Liu Zhengying\n# Date: 10 Apr 2019\n# Description: This script downloads the public datasets used in AutoDL challenges and\n#   put them under the folder AutoDL_public_data/. This script supports\n#   breakpoint resume, which means that you can recover downloading from where\n#   your network broke down.\n# Usage:\n#   python download_public_datasets.py\n\nimport os\nimport sys\n\ndef main(*argv):\n  dataset_names = [\'Munster\', \'City\', \'Chucky\',\'Pedro\', \'Decal\', \'Hammer\', # images\n                   \'Kreatur\', \'Kreatur3\', \'Katze\', \'Kraut\',                # videos\n                   \'data01\', \'data02\', \'data03\', \'data04\', \'data05\',       # speech\n                   \'O1\', \'O2\', \'O3\', \'O4\', \'O5\',                           # text\n                   \'Adult\', \'Dilbert\', \'Digits\', \'Madeline\']               # tabular\n  dataset_names = [ # images\n                                  # videos\n                   \'data04\', \'data05\',       # speech\n                   #\'O1\', \'O2\', \'O3\', \'O4\', \'O5\',                           # text\n                   #\'Adult\', \'Dilbert\', \'Digits\', \'Madeline\']               # tabular\n                   ]\n\n  dataset_names = [\'Munster\', \'City\', \'Chucky\',\'Pedro\', \'Decal\', \'Hammer\', # images\n                   \'Kreatur\', \'Kreatur3\', \'Katze\', \'Kraut\',]                # videos\n\n  data_urls = {\n      \'Munster\':\'https://autodl.lri.fr/my/datasets/download/d29000a6-b5b8-4ccf-9050-2686af874a71\',\n      \'City\':\'https://autodl.lri.fr/my/datasets/download/cf0f810e-4818-4c8a-bf48-cbf9b6599928\',\n      \'Chucky\':\'https://autodl.lri.fr/my/datasets/download/cf2176c2-5454-4d07-9c4e-758e3c5bcb31\',\n      \'Pedro\':\'https://autodl.lri.fr/my/datasets/download/d556ca67-01c7-4a8d-9a74-a2bd9c06414d\',\n      \'Decal\':\'https://autodl.lri.fr/my/datasets/download/31a34c03-a75c-4e0f-b72d-3723ba303dac\',\n      \'Hammer\':\'https://autodl.lri.fr/my/datasets/download/3507841e-59fe-4598-a27e-a9e170b26e44\',\n      \'Kreatur\':\'https://autodl.lri.fr/my/datasets/download/c240df57-b144-41df-a059-05bc859d2621\',\n      \'Kreatur3\':\'https://autodl.lri.fr/my/datasets/download/08c2afcd-74b1-4c5e-8b93-9f6c9a96add2\',\n      \'Kraut\':\'https://autodl.lri.fr/my/datasets/download/a1d9f706-cf8d-4a63-a544-552d6b85d6c4\',\n      \'Katze\':\'https://autodl.lri.fr/my/datasets/download/611a42fa-da42-4a30-8c7a-69230d9eeb92\',\n      \'data01\':\'https://autodl.lri.fr/my/datasets/download/c15f1b70-4f07-4e9e-9817-d785b1674966\',\n      \'data02\':\'https://autodl.lri.fr/my/datasets/download/3961f962-88db-47ee-a756-2152753ba900\',\n      \'data03\':\'https://autodl.lri.fr/my/datasets/download/a97ddb39-1470-4a80-81b0-1c26dfa29335\',\n      \'data04\':\'https://autodl.lri.fr/my/datasets/download/c8d15be0-e1fa-4899-940e-0d7e1794a835\',\n      \'data05\':\'https://autodl.lri.fr/my/datasets/download/cccc9147-7d1f-4119-888b-1b87f142b721\',\n      \'O1\':\'https://autodl.lri.fr/my/datasets/download/4b98c65f-1922-4ff4-8e2a-ab7a022ef1da\',\n      \'O2\':\'https://autodl.lri.fr/my/datasets/download/f831b0d6-0a53-4c93-b9cf-8cf1f2128d24\',\n      \'O3\':\'https://autodl.lri.fr/my/datasets/download/4545d366-12f4-442c-87e4-f908fcd79698\',\n      \'O4\':\'https://autodl.lri.fr/my/datasets/download/2bdf5e4e-8d02-4c85-98b2-0b28a6176db9\',\n      \'O5\':\'https://autodl.lri.fr/my/datasets/download/09ec4daf-fba2-41e1-80d0-429772d59d58\',\n      \'Adult\':\'https://autodl.lri.fr/my/datasets/download/4ad27a85-4932-409b-a33d-a3b1c4ec1893\',\n      \'Dilbert\':\'https://autodl.lri.fr/my/datasets/download/71f517b0-85c2-4a7d-8df3-d2a5998a9d78\',\n      \'Digits\':\'https://autodl.lri.fr/my/datasets/download/03e69995-2b8b-4f60-b43b-4458aa51e9c0\',\n      \'Madeline\':\'https://autodl.lri.fr/my/datasets/download/1d7910ca-ee43-41fc-aca9-0dfcd800d93b\'\n  }\n  solution_urls = {\n      \'Munster\':\'https://autodl.lri.fr/my/datasets/download/5a24d8f3-dfb6-4935-b798-14baccda695f\',\n      \'City\':\'https://autodl.lri.fr/my/datasets/download/c64e3ebb-664f-45f1-8666-1054d262a85c\',\n      \'Chucky\':\'https://autodl.lri.fr/my/datasets/download/ba4837bf-275d-43a6-a481-d03dce7ba127\',\n      \'Pedro\':\'https://autodl.lri.fr/my/datasets/download/9993ea27-955e-4faa-9d28-4a7dfe1fcc55\',\n      \'Decal\':\'https://autodl.lri.fr/my/datasets/download/cc93c74c-2732-4e7d-ae7f-a2c3bc555360\',\n      \'Hammer\':\'https://autodl.lri.fr/my/datasets/download/e5b6188f-a377-4a5d-bbe1-a586716af487\',\n      \'Kraut\':\'https://autodl.lri.fr/my/datasets/download/47ff016d-cc66-47a9-945d-bc01fd9096c9\',\n      \'Katze\':\'https://autodl.lri.fr/my/datasets/download/a04de92e-b04b-49a6-96c2-5910c64f9b3c\',\n      \'Kreatur\':\'https://autodl.lri.fr/my/datasets/download/31ecdb19-c25a-420f-9764-8d1783705deb\',\n      \'Kreatur3\':\'https://autodl.lri.fr/my/datasets/download/10e04890-f05a-4004-a499-1cc167769edd\',\n      \'data01\':\'https://autodl.lri.fr/my/datasets/download/358a227e-986d-48ad-a994-70b12a9bfcc3\',\n      \'data02\':\'https://autodl.lri.fr/my/datasets/download/9be871dc-1356-4600-962e-9a43154a1e38\',\n      \'data03\':\'https://autodl.lri.fr/my/datasets/download/a6baf245-03a6-42b5-b870-df57e3a27723\',\n      \'data04\':\'https://autodl.lri.fr/my/datasets/download/32fd9ca3-865c-4135-a70c-1543160cf6ab\',\n      \'data05\':\'https://autodl.lri.fr/my/datasets/download/4f802113-a899-40a8-b293-e119dd7c54f5\',\n      \'O1\':\'https://autodl.lri.fr/my/datasets/download/888e1c1c-a39c-40b9-b6f7-cc2eff7a299d\',\n      \'O2\':\'https://autodl.lri.fr/my/datasets/download/b6513cb2-e4f2-46a8-a7cf-1d8441a00a56\',\n      \'O3\':\'https://autodl.lri.fr/my/datasets/download/4bb6ebd3-f991-4a6b-8cd1-864a0f3a1abd\',\n      \'O4\':\'https://autodl.lri.fr/my/datasets/download/e09837eb-8144-4850-ad38-c1ba81426c0b\',\n      \'O5\':\'https://autodl.lri.fr/my/datasets/download/18c8e3eb-2341-41ed-9a44-8d2c94042c30\',\n      \'Adult\':\'https://autodl.lri.fr/my/datasets/download/c125d32c-3e89-456a-a82d-760fc4b60e4c\',\n      \'Dilbert\':\'https://autodl.lri.fr/my/datasets/download/7734cc00-1583-44c8-80f5-156a11b12952\',\n      \'Digits\':\'https://autodl.lri.fr/my/datasets/download/e29c6cb2-8748-4e26-9c91-66a2b0dd41c2\',\n      \'Madeline\':\'https://autodl.lri.fr/my/datasets/download/a86e0e7f-9b07-44f1-92ba-0a5f72cddb6b\'\n  }\n\n  def _HERE(*args):\n      h = os.path.dirname(os.path.realpath(__file__))\n      return os.path.abspath(os.path.join(h, *args))\n  starting_kit_dir = _HERE()\n  public_date_dir = os.path.join(starting_kit_dir, \'AutoDL_public_data\')\n\n  for dataset_name in dataset_names:\n    msg = ""Downloading data files and solution file for the dataset {}...""\\\n          .format(dataset_name)\n    le = len(msg)\n    print(\'\\n\' + \'#\'*(le+10))\n    print(\'#\'*4+\' \' + msg + \' \'+\'#\'*4)\n    print(\'#\'*(le+10) + \'\\n\')\n    data_url = data_urls[dataset_name]\n    solution_url = solution_urls[dataset_name]\n    dataset_dir = os.path.join(public_date_dir, dataset_name)\n    os.system(\'mkdir -p {}\'.format(dataset_dir))\n    data_zip_file = os.path.join(dataset_dir, dataset_name + \'.data.zip\')\n    solution_zip_file = os.path.join(dataset_dir,\n                                     dataset_name + \'.solution.zip\')\n    os.system(\'wget -q --show-progress -c -N {} -O {}\'\\\n              .format(data_url, data_zip_file))\n    os.system(\'wget -q --show-progress -c -N {} -O {}\'\\\n              .format(solution_url, solution_zip_file))\n    os.system(\'unzip -n -d {} {}\'\\\n              .format(dataset_dir, data_zip_file))\n    os.system(\'unzip -n -d {} {}\'\\\n              .format(dataset_dir, solution_zip_file))\n  print(""\\nFinished downloading {} public datasets: {}""\\\n        .format(len(dataset_names),dataset_names))\n  print(""Now you should find them under the directory: {}""\\\n        .format(public_date_dir))\n\nif __name__ == \'__main__\':\n  main(sys.argv)\n'"
run_local_test.py,0,"b'################################################################################\n# Name:         Run Local Test Tool\n# Author:       Zhengying Liu\n# Created on:   20 Sep 2018\n# Update time:  5 May 2019\n# Usage: \t\t    python run_local_test.py -dataset_dir=<dataset_dir> -code_dir=<code_dir>\n\nVERISION = ""v20190505""\nDESCRIPTION =\\\n""""""This script allows participants to run local test of their method within the\ndownloaded starting kit folder (and avoid using submission quota on CodaLab). To\ndo this, run:\n```\npython run_local_test.py -dataset_dir=./AutoDL_sample_data/miniciao -code_dir=./AutoDL_sample_code_submission/\n```\nin the starting kit directory. If you want to test the performance of a\ndifferent algorithm on a different dataset, please specify them using respective\narguments.\n\nIf you want to use default folders (i.e. those in above command line), simply\nrun\n```\npython run_local_test.py\n```\n""""""\n\n# ALL INFORMATION, SOFTWARE, DOCUMENTATION, AND DATA ARE PROVIDED ""AS-IS"".\n# ISABELLE GUYON, CHALEARN, AND/OR OTHER ORGANIZERS OR CODE AUTHORS DISCLAIM\n# ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR ANY PARTICULAR PURPOSE, AND THE\n# WARRANTY OF NON-INFRINGEMENT OF ANY THIRD PARTY\'S INTELLECTUAL PROPERTY RIGHTS.\n# IN NO EVENT SHALL ISABELLE GUYON AND/OR OTHER ORGANIZERS BE LIABLE FOR ANY SPECIAL,\n# INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER ARISING OUT OF OR IN\n# CONNECTION WITH THE USE OR PERFORMANCE OF SOFTWARE, DOCUMENTS, MATERIALS,\n# PUBLICATIONS, OR INFORMATION MADE AVAILABLE FOR THE CHALLENGE.\n################################################################################\n\n# Verbosity level of logging.\n# Can be: NOTSET, DEBUG, INFO, WARNING, ERROR, CRITICAL\nverbosity_level = \'INFO\'\n\nimport logging\nimport os\nimport tensorflow as tf\nimport time\nimport shutil # for deleting a whole directory\nimport webbrowser\nfrom multiprocessing import Process\n\nlogging.basicConfig(\n    level=getattr(logging, verbosity_level),\n    format=\'%(asctime)s %(levelname)s %(filename)s: %(message)s\',\n    datefmt=\'%Y-%m-%d %H:%M:%S\'\n)\n\ndef _HERE(*args):\n    h = os.path.dirname(os.path.realpath(__file__))\n    return os.path.join(h, *args)\n\ndef get_path_to_ingestion_program(starting_kit_dir):\n  return os.path.join(starting_kit_dir,\n                      \'AutoDL_ingestion_program\', \'ingestion.py\')\n\ndef get_path_to_scoring_program(starting_kit_dir):\n  return os.path.join(starting_kit_dir,\n                      \'AutoDL_scoring_program\', \'score.py\')\n\ndef remove_dir(output_dir):\n  """"""Remove the directory `output_dir`.\n\n  This aims to clean existing output of last run of local test.\n  """"""\n  if os.path.isdir(output_dir):\n    logging.info(""Cleaning existing output directory of last run: {}""\\\n                .format(output_dir))\n    shutil.rmtree(output_dir)\n\ndef get_basename(path):\n  if len(path) == 0:\n    return """"\n  if path[-1] == os.sep:\n    path = path[:-1]\n  return path.split(os.sep)[-1]\n\ndef run_baseline(dataset_dir, code_dir, time_budget=1200):\n  logging.info(""#""*50)\n  logging.info(""Begin running local test using"")\n  logging.info(""code_dir = {}"".format(get_basename(code_dir)))\n  logging.info(""dataset_dir = {}"".format(get_basename(dataset_dir)))\n  logging.info(""#""*50)\n\n  # Current directory containing this script\n  starting_kit_dir = os.path.dirname(os.path.realpath(__file__))\n  path_ingestion = get_path_to_ingestion_program(starting_kit_dir)\n  path_scoring = get_path_to_scoring_program(starting_kit_dir)\n\n  # Run ingestion and scoring at the same time\n  command_ingestion =\\\n    ""python {} --dataset_dir={} --code_dir={} --time_budget={}""\\\n    .format(path_ingestion, dataset_dir, code_dir, time_budget)\n  command_scoring =\\\n    \'python {} --solution_dir={}\'\\\n    .format(path_scoring, dataset_dir)\n  def run_ingestion():\n    print(command_ingestion)\n    exit_code = os.system(command_ingestion)\n    assert exit_code == 0\n  def run_scoring():\n    print(command_scoring)\n    exit_code = os.system(command_scoring)\n    assert exit_code == 0\n  ingestion_process = Process(name=\'ingestion\', target=run_ingestion)\n  scoring_process = Process(name=\'scoring\', target=run_scoring)\n  ingestion_output_dir = os.path.join(starting_kit_dir,\n                                      \'AutoDL_sample_result_submission\')\n  score_dir = os.path.join(starting_kit_dir,\n                                      \'AutoDL_scoring_output\')\n  remove_dir(ingestion_output_dir)\n  remove_dir(score_dir)\n  ingestion_process.start()\n  scoring_process.start()\n  detailed_results_page = os.path.join(starting_kit_dir,\n                                       \'AutoDL_scoring_output\',\n                                       \'detailed_results.html\')\n  detailed_results_page = os.path.abspath(detailed_results_page)\n\n  # Open detailed results page in a browser\n  time.sleep(2)\n  for i in range(30):\n    if os.path.isfile(detailed_results_page):\n      webbrowser.open(\'file://\'+detailed_results_page, new=2)\n      break\n      time.sleep(1)\n\n  ingestion_process.join()\n  scoring_process.join()\n  if not ingestion_process.exitcode == 0:\n    logging.info(""Some error occurred in ingestion program."")\n  if not scoring_process.exitcode == 0:\n    raise Exception(""Some error occurred in scoring program."")\n\nif __name__ == \'__main__\':\n  default_starting_kit_dir = _HERE()\n  # The default dataset is \'miniciao\' under the folder AutoDL_sample_data/\n  default_dataset_dir = os.path.join(default_starting_kit_dir,\n                                     \'AutoDL_sample_data\', \'miniciao\')\n  default_code_dir = os.path.join(default_starting_kit_dir,\n                                     \'AutoDL_sample_code_submission\')\n  default_time_budget = 1200\n\n  tf.flags.DEFINE_string(\'dataset_dir\', default_dataset_dir,\n                        ""Directory containing the content (e.g. adult.data/ + ""\n                        ""adult.solution) of an AutoDL dataset. Specify this ""\n                        ""argument if you want to test on a different dataset."")\n\n  tf.flags.DEFINE_string(\'code_dir\', default_code_dir,\n                        ""Directory containing a `model.py` file. Specify this ""\n                        ""argument if you want to test on a different algorithm.""\n                        )\n\n  tf.flags.DEFINE_float(\'time_budget\', default_time_budget,\n                        ""Time budget for running ingestion "" +\n                        ""(training + prediction).""\n                        )\n\n  FLAGS = tf.flags.FLAGS\n  dataset_dir = FLAGS.dataset_dir\n  code_dir = FLAGS.code_dir\n  time_budget = FLAGS.time_budget\n\n  run_baseline(dataset_dir, code_dir, time_budget)\n'"
AutoDL_ingestion_program/__init__.py,0,b''
AutoDL_ingestion_program/algorithm.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS-IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Class for supervised machine learning algorithms for the autodl project.\n\nThis is the API; see model.py, algorithm_scikit.py for implementations.\n""""""\n\nclass Algorithm(object):\n  """"""Algorithm class: API (abstract class).""""""\n\n  def __init__(self, metadata):\n    self.done_training = False # Indicate ingestion to stop further training\n    self.metadata_ = metadata # An AutoDLMetadata object\n\n  def train(self, dataset, remaining_time_budget=None):\n    """"""Train this algorithm on the tensorflow |dataset|.\n\n    This method will be called REPEATEDLY during the whole training/predicting\n    process. So your `train` method should be able to handle repeated calls and\n    hopefully improve your model performance after each call.\n\n    Args:\n      dataset: a `tf.data.Dataset` object. Each of its examples is of the form\n            (example, labels)\n          where `example` is a dense 4-D Tensor of shape\n            (sequence_size, row_count, col_count, num_channels)\n          and `labels` is a 1-D Tensor of shape\n            (output_dim,).\n          Here `output_dim` represents number of classes of this\n          multilabel classification task.\n\n          IMPORTANT: some of the dimensions of `example` might be `None`,\n          which means the shape on this dimension might be variable. In this\n          case, some preprocessing technique should be applied in order to\n          feed the training of a neural network. For example, if an image\n          dataset has `example` of shape\n            (1, None, None, 3)\n          then the images in this datasets may have different sizes. On could\n          apply resizing, cropping or padding in order to have a fixed size\n          input tensor.\n\n      remaining_time_budget: time remaining to execute train(). The method\n          should keep track of its execution time to avoid exceeding its time\n          budget. If remaining_time_budget is None, no time budget is imposed.\n    """"""\n    raise NotImplementedError(""Algorithm class does not have any training."")\n\n  def test(self, dataset, remaining_time_budget=None):\n    """"""Make predictions on the test set `dataset` (which is different from that\n    of the method `train`).\n\n    Args:\n      Same as that of `train` method, except that the `labels` will be empty\n          since this time `dataset` is a test set.\n    Returns:\n      predictions: A `numpy.ndarray` matrix of shape (sample_count, output_dim).\n          here `sample_count` is the number of examples in this dataset as test\n          set and `output_dim` is the number of labels to be predicted. The\n          values should be binary or in the interval [0,1].\n\n          IMPORTANT: if returns `None`, this means that the algorithm\n          chooses to stop training, and the whole train/test will stop. The\n          performance of the last prediction will be used to compute area under\n          learning curve.\n    """"""\n    raise NotImplementedError(""Algorithm class does not have any testing."")\n'"
AutoDL_ingestion_program/data_converter.py,0,"b'# Functions performing various data conversions for the ChaLearn AutoML challenge\n\n# Main contributors: Arthur Pesah and Isabelle Guyon, August-October 2014\n\n# ALL INFORMATION, SOFTWARE, DOCUMENTATION, AND DATA ARE PROVIDED ""AS-IS"". \n# ISABELLE GUYON, CHALEARN, AND/OR OTHER ORGANIZERS OR CODE AUTHORS DISCLAIM\n# ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR ANY PARTICULAR PURPOSE, AND THE\n# WARRANTY OF NON-INFRIGEMENT OF ANY THIRD PARTY\'S INTELLECTUAL PROPERTY RIGHTS. \n# IN NO EVENT SHALL ISABELLE GUYON AND/OR OTHER ORGANIZERS BE LIABLE FOR ANY SPECIAL, \n# INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER ARISING OUT OF OR IN\n# CONNECTION WITH THE USE OR PERFORMANCE OF SOFTWARE, DOCUMENTS, MATERIALS, \n# PUBLICATIONS, OR INFORMATION MADE AVAILABLE FOR THE CHALLENGE. \n\nimport numpy as np\nfrom scipy.sparse import *\nfrom sklearn.datasets import load_svmlight_file\nimport os \n# Note: to check for nan values np.any(map(np.isnan,X_train))\ndef file_to_array (filename, verbose=False):\n    \'\'\' Converts a file to a list of list of STRING\n    It differs from np.genfromtxt in that the number of columns doesn\'t need to be constant\'\'\'\n    data =[]\n    with open(filename, ""r"") as data_file:\n        if verbose: print (""Reading {}..."".format(filename))\n        lines = data_file.readlines()\n        if verbose: print (""Converting {} to correct array..."".format(filename))\n        data = [lines[i].strip().split() for i in range (len(lines))]\n    return data\n\ndef file_to_libsvm (filename, data_binary  , n_features):\n    \'\'\' Converts a file to svmlib format and return csr matrix \n    filname = path of file \n    data_binary = True if is sparse binary data False else \n    n_features = number of features\n    \'\'\'\n    data =[]\n    with open(filename, ""r"") as data_file:\n        lines = data_file.readlines()\n        with open(\'tmp.txt\', \'w\') as f:\n            for l in lines  :\n                tmp = l.strip().split()\n                f.write(""0 "")\n                for i in range (len(tmp) ):\n                    if(data_binary):\n                        f.write(tmp[i]+"":1 "")\n                    else:\n                        f.write(tmp[i]+"" "")\n                f.write(""\\n"")\n    print (""-------------------- file_to_libsvm  ---------------------"")\n    l = load_svmlight_file(\'tmp.txt\', zero_based= False ,  n_features = n_features )\n    os.remove(""tmp.txt"")\n    return l[0]\n\ndef read_first_line (filename):\n\t\'\'\' Read fist line of file\'\'\'\n\tdata =[]\n\twith open(filename, ""r"") as data_file:\n\t\tline = data_file.readline()\n\t\tdata = line.strip().split()\n\treturn data  \n \ndef num_lines (filename):\n\t\'\'\' Count the number of lines of file\'\'\'\n\treturn sum(1 for line in open(filename))\n\ndef binarization (array):\n\t\'\'\' Takes a binary-class datafile and turn the max value (positive class) into 1 and the min into 0\'\'\'\n\tarray = np.array(array, dtype=float) # conversion needed to use np.inf after\n\tif len(np.unique(array)) > 2:\n\t\traise ValueError (""The argument must be a binary-class datafile. {} classes detected"".format(len(np.unique(array))))\n\t\n\t# manipulation which aims at avoid error in data with for example classes \'1\' and \'2\'.\n\tarray[array == np.amax(array)] = np.inf\n\tarray[array == np.amin(array)] = 0\n\tarray[array == np.inf] = 1\n\treturn np.array(array, dtype=int)\n\n\ndef multilabel_to_multiclass (array):\n\tarray = binarization (array)\n\treturn np.array([np.nonzero(array[i,:])[0][0] for i in range (len(array))])\n\t\ndef convert_to_num(Ybin, verbose=True):\n\t\'\'\' Convert binary targets to numeric vector (typically classification target values)\'\'\'\n\tif verbose: print(""\\tConverting to numeric vector"")\n\tYbin = np.array(Ybin)\n\tif len(Ybin.shape) ==1:\n         return Ybin\n\tclassid=range(Ybin.shape[1])\n\tYcont = np.dot(Ybin, classid)\n\tif verbose: print(Ycont)\n\treturn Ycont\n \ndef convert_to_bin(Ycont, nval, verbose=True):\n    \'\'\' Convert numeric vector to binary (typically classification target values)\'\'\'\n    if verbose: print (""\\t_______ Converting to binary representation"")\n    Ybin=[[0]*nval for x in xrange(len(Ycont))]\n    for i in range(len(Ybin)):\n        line = Ybin[i]\n        line[np.int(Ycont[i])]=1\n        Ybin[i] = line\n    return Ybin\n\n\ndef tp_filter(X, Y, feat_num=1000, verbose=True):\n    \'\'\' TP feature selection in the spirit of the winners of the KDD cup 2001\n    Only for binary classification and sparse matrices\'\'\'\n        \n    if issparse(X) and len(Y.shape)==1 and len(set(Y))==2 and (sum(Y)/Y.shape[0])<0.1: \n        if verbose: print(""========= Filtering features..."")\n        Posidx=Y>0\n        nz=X.nonzero()\n        mx=X[nz].max()\n        if X[nz].min()==mx: # sparse binary\n            if mx!=1: X[nz]=1\n            tp=csr_matrix.sum(X[Posidx,:], axis=0)\n     \n        else:\n            tp=np.sum(X[Posidx,:]>0, axis=0)\n  \n\n        tp=np.ravel(tp)\n        idx=sorted(range(len(tp)), key=tp.__getitem__, reverse=True)   \n        return idx[0:feat_num]\n    else:\n        feat_num = X.shape[1]\n        return range(feat_num)\n    \ndef replace_missing(X):\n    # This is ugly, but\n    try:\n        if X.getformat()==\'csr\':\n            return X\n    except:\n        XX = np.nan_to_num(X)\n        \n    return XX\n'"
AutoDL_ingestion_program/data_io.py,0,"b'# Functions performing various input/output operations for the ChaLearn AutoML challenge\n\n# Main contributors: Arthur Pesah and Isabelle Guyon, August-October 2014\n\n# ALL INFORMATION, SOFTWARE, DOCUMENTATION, AND DATA ARE PROVIDED ""AS-IS"".\n# ISABELLE GUYON, CHALEARN, AND/OR OTHER ORGANIZERS OR CODE AUTHORS DISCLAIM\n# ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR ANY PARTICULAR PURPOSE, AND THE\n# WARRANTY OF NON-INFRIGEMENT OF ANY THIRD PARTY\'S INTELLECTUAL PROPERTY RIGHTS.\n# IN NO EVENT SHALL ISABELLE GUYON AND/OR OTHER ORGANIZERS BE LIABLE FOR ANY SPECIAL,\n# INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER ARISING OUT OF OR IN\n# CONNECTION WITH THE USE OR PERFORMANCE OF SOFTWARE, DOCUMENTS, MATERIALS,\n# PUBLICATIONS, OR INFORMATION MADE AVAILABLE FOR THE CHALLENGE.\n\nfrom __future__ import print_function\nfrom sys import getsizeof, stderr\nfrom itertools import chain\nfrom collections import deque\ntry:\n    from reprlib import repr\nexcept ImportError:\n    pass\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport shutil\nfrom scipy.sparse import * # used in data_binary_sparse\nfrom zipfile import ZipFile, ZIP_DEFLATED\nfrom contextlib import closing\nimport data_converter\nfrom sys import stderr\nfrom sys import version\nfrom glob import glob as ls\nfrom os import getcwd as pwd\nfrom os.path import isfile\n\n# get_installed_distributions has gone from pip v10\ntry:\n    from pip._internal.utils.misc import get_installed_distributions as lib\nexcept ImportError:  # pip < 10\n    from pip import get_installed_distributions as lib\n\nimport yaml\nfrom shutil import copy2\nimport csv\nimport psutil\nimport platform\n\n# ================ Small auxiliary functions =================\n\ndef read_as_df(basename, type=""train""):\n    \'\'\' Function to read the AutoML format and return a Panda Data Frame \'\'\'\n    csvfile = basename + \'_\' + type + \'.csv\'\n    if isfile(csvfile):\n        print(\'Reading \'+ basename + \'_\' + type + \' from CSV\')\n        XY = pd.read_csv(csvfile)\n        return XY\n\n    print(\'Reading \'+ basename + \'_\' + type+ \' from AutoML format\')\n    feat_name = pd.read_csv(basename + \'_feat.name\', header=None)\n    label_name = pd.read_csv(basename + \'_label.name\', header=None)\n    X = pd.read_csv(basename + \'_\' + type + \'.data\', sep=\' \', names = np.ravel(feat_name))\n    [patnum, featnum] = X.shape\n    print(\'Number of examples = %d\' % patnum)\n    print(\'Number of features = %d\' % featnum)\n\n    XY=X\n    Y=[]\n    solution_file = basename + \'_\' + type + \'.solution\'\n    if isfile(solution_file):\n        Y = pd.read_csv(solution_file, sep=\' \', names = np.ravel(label_name))\n        [patnum2, classnum] = Y.shape\n        assert(patnum==patnum2)\n        print(\'Number of classes = %d\' % classnum)\n        # Here we add the target values as a last column, this is convenient to use seaborn\n        # Look at http://seaborn.pydata.org/tutorial/axis_grids.html for other ideas\n        label_range = np.arange(classnum).transpose()         # This is just a column vector [[0], [1], [2]]\n        numerical_target = Y.dot(label_range)                 # This is a column vector of dim patnum with numerical categories\n        nominal_target = pd.Series(np.array(label_name)[numerical_target].ravel()) # Same with nominal categories\n        XY = X.assign(target=nominal_target.values)          # Add the last column\n\n    return XY\n\n# ================ Small auxiliary functions =================\n\nswrite = stderr.write\n\nif (os.name == ""nt""):\n       filesep = \'\\\\\'\nelse:\n       filesep = \'/\'\n\ndef write_list(lst):\n    \'\'\' Write a list of items to stderr (for debug purposes)\'\'\'\n    for item in lst:\n        swrite(item + ""\\n"")\n\ndef print_dict(verbose, dct):\n    \'\'\' Write a dict to stderr (for debug purposes)\'\'\'\n    if verbose:\n        for item in dct:\n            print(item + "" = "" + str(dct[item]))\n\ndef mkdir(d):\n    \'\'\' Create a new directory\'\'\'\n    if not os.path.exists(d):\n        os.makedirs(d)\n\ndef mvdir(source, dest):\n    \'\'\' Move a directory\'\'\'\n    if os.path.exists(source):\n        os.rename(source, dest)\n\ndef rmdir(d):\n    \'\'\' Remove an existingdirectory\'\'\'\n    if os.path.exists(d):\n        shutil.rmtree(d)\n\ndef vprint(mode, t):\n    \'\'\' Print to stdout, only if in verbose mode\'\'\'\n    if(mode):\n            print(t)\n\n# ================ Output prediction results and prepare code submission =================\n\ndef write(filename, predictions):\n    \'\'\' Write prediction scores in prescribed format\'\'\'\n    filename_temp = \'temp_prediction_file_\' + str(np.random.randint(10000))\n    filename_temp = os.path.join(os.path.dirname(filename), filename_temp)\n    with open(filename_temp, ""w"") as output_file:\n        for row in predictions:\n            if type(row) is not np.ndarray and type(row) is not list:\n                row = [row]\n            output_file.write(\' \'.join([\'{0:g}\'.format(float(val)) for val in row]))\n            output_file.write(\'\\n\')\n    os.rename(filename_temp, filename)\n\ndef zipdir(archivename, basedir):\n    \'\'\'Zip directory, from J.F. Sebastian http://stackoverflow.com/\'\'\'\n    assert os.path.isdir(basedir)\n    with closing(ZipFile(archivename, ""w"", ZIP_DEFLATED)) as z:\n        for root, dirs, files in os.walk(basedir):\n            #NOTE: ignore empty directories\n            for fn in files:\n                if not fn.endswith(\'.zip\'):\n                    absfn = os.path.join(root, fn)\n                    zfn = absfn[len(basedir):] #XXX: relative path\n                    assert absfn[:len(basedir)] == basedir\n                    if zfn[0] == os.sep:\n                        zfn = zfn[1:]\n                    z.write(absfn, zfn)\n\n# ================ Inventory input data and create data structure =================\n\ndef inventory_data(input_dir):\n    \'\'\' Inventory the datasets in the input directory and return them in alphabetical order\'\'\'\n    # Assume first that there is a hierarchy dataname/dataname_train.data\n    training_names = ls(os.path.join(input_dir, \'*.data\'))\n    training_names = [ name.split(\'/\')[-1] for name in training_names ]\n\n    ntr=len(training_names)\n    if ntr==0:\n        print(\'WARNING: Inventory data - No data file found\')\n        training_names = []\n    training_names.sort()\n    # check_dataset\n    return training_names\n\n\ndef check_dataset(dirname, name):\n    \'\'\' Check the test and valid files are in the directory, as well as the solution\'\'\'\n    valid_file = os.path.join(dirname, name + \'_valid.data\')\n    if not os.path.isfile(valid_file):\n        print(\'No validation file for \' + name)\n        exit(1)\n    test_file = os.path.join(dirname, name + \'_test.data\')\n    if not os.path.isfile(test_file):\n        print(\'No test file for \' + name)\n        exit(1)\n    # Check the training labels are there\n    training_solution = os.path.join(dirname, name + \'_train.solution\')\n    if not os.path.isfile(training_solution):\n        print(\'No training labels for \' + name)\n        exit(1)\n    return True\n\n\ndef data(filename, nbr_features=None, verbose = False):\n    \'\'\' The 2nd parameter makes possible a using of the 3 functions of data reading (data, data_sparse, data_binary_sparse) without changing parameters\'\'\'\n    if verbose: print (np.array(data_converter.file_to_array(filename)))\n    return np.array(data_converter.file_to_array(filename), dtype=float)\n\ndef data_sparse (filename, nbr_features):\n    \'\'\' This function takes as argument a file representing a sparse matrix\n    sparse_matrix[i][j] = ""a:b"" means matrix[i][a] = basename and load it with the loadsvm load_svmlight_file\n    \'\'\'\n    return data_converter.file_to_libsvm (filename = filename, data_binary = False  , n_features = nbr_features)\n\n\n\ndef data_binary_sparse (filename , nbr_features):\n    \'\'\' This fuction takes as argument a file representing a sparse binary matrix\n    sparse_binary_matrix[i][j] = ""a""and transforms it temporarily into file svmlibs format( <index2>:<value2>)\n    to load it with the loadsvm load_svmlight_file\n    \'\'\'\n    return data_converter.file_to_libsvm (filename = filename, data_binary = True  , n_features = nbr_features)\n\n\n\n# ================ Copy results from input to output ==========================\n\ndef copy_results(datanames, result_dir, output_dir, verbose):\n    \'\'\' This function copies all the [dataname.predict] results from result_dir to output_dir\'\'\'\n    missing_files = []\n    for basename in datanames:\n        try:\n            missing = False\n            test_files = ls(result_dir + ""/"" + basename + ""*_test*.predict"")\n            if len(test_files)==0:\n                vprint(verbose, ""[-] Missing \'test\' result files for "" + basename)\n                missing = True\n            valid_files = ls(result_dir + ""/"" + basename + ""*_valid*.predict"")\n            if len(valid_files)==0:\n                vprint(verbose, ""[-] Missing \'valid\' result files for "" + basename)\n                missing = True\n            if missing == False:\n                for f in test_files: copy2(f, output_dir)\n                for f in valid_files: copy2(f, output_dir)\n                vprint( verbose,  ""[+] "" + basename.capitalize() + "" copied"")\n            else:\n                missing_files.append(basename)\n        except:\n            vprint(verbose, ""[-] Missing result files"")\n            return datanames\n    return missing_files\n\n# ================ Display directory structure and code version (for debug purposes) =================\n\ndef show_dir(run_dir):\n    print(\'\\n=== Listing run dir ===\')\n    write_list(ls(run_dir))\n    write_list(ls(run_dir + \'/*\'))\n    write_list(ls(run_dir + \'/*/*\'))\n    write_list(ls(run_dir + \'/*/*/*\'))\n    write_list(ls(run_dir + \'/*/*/*/*\'))\n\ndef show_io(input_dir, output_dir):\n    swrite(\'\\n=== DIRECTORIES ===\\n\\n\')\n    # Show this directory\n    swrite(""-- Current directory "" + pwd() + "":\\n"")\n    write_list(ls(\'.\'))\n    write_list(ls(\'./*\'))\n    write_list(ls(\'./*/*\'))\n    swrite(""\\n"")\n\n    # List input and output directories\n    swrite(""-- Input directory "" + input_dir + "":\\n"")\n    write_list(ls(input_dir))\n    write_list(ls(input_dir + \'/*\'))\n    write_list(ls(input_dir + \'/*/*\'))\n    write_list(ls(input_dir + \'/*/*/*\'))\n    swrite(""\\n"")\n    swrite(""-- Output directory  "" + output_dir + "":\\n"")\n    write_list(ls(output_dir))\n    write_list(ls(output_dir + \'/*\'))\n    swrite(""\\n"")\n\n    # write meta data to sdterr\n    swrite(\'\\n=== METADATA ===\\n\\n\')\n    swrite(""-- Current directory "" + pwd() + "":\\n"")\n    try:\n        metadata = yaml.load(open(\'metadata\', \'r\'))\n        for key,value in metadata.items():\n            swrite(key + \': \')\n            swrite(str(value) + \'\\n\')\n    except:\n        swrite(""none\\n"");\n    swrite(""-- Input directory "" + input_dir + "":\\n"")\n    try:\n        metadata = yaml.load(open(os.path.join(input_dir, \'metadata\'), \'r\'))\n        for key,value in metadata.items():\n            swrite(key + \': \')\n            swrite(str(value) + \'\\n\')\n        swrite(""\\n"")\n    except:\n        swrite(""none\\n"");\n\ndef show_version():\n    # Python version and library versions\n    swrite(\'\\n=== VERSIONS ===\\n\\n\')\n    # Python version\n    swrite(""Python version: "" + version + ""\\n\\n"")\n    # Give information on the version installed\n    swrite(""Versions of libraries installed:\\n"")\n    map(swrite, sorted([""%s==%s\\n"" % (i.key, i.version) for i in lib()]))\n\n # Compute the total memory size of an object in bytes\n\ndef total_size(o, handlers={}, verbose=False):\n    """""" Returns the approximate memory footprint an object and all of its contents.\n\n    Automatically finds the contents of the following builtin containers and\n    their subclasses:  tuple, list, deque, dict, set and frozenset.\n    To search other containers, add handlers to iterate over their contents:\n\n        handlers = {SomeContainerClass: iter,\n                    OtherContainerClass: OtherContainerClass.get_elements}\n\n    """"""\n    dict_handler = lambda d: chain.from_iterable(d.items())\n    all_handlers = {tuple: iter,\n                    list: iter,\n                    deque: iter,\n                    dict: dict_handler,\n                    set: iter,\n                    frozenset: iter,\n                   }\n    all_handlers.update(handlers)     # user handlers take precedence\n    seen = set()                      # track which object id\'s have already been seen\n    default_size = getsizeof(0)       # estimate sizeof object without __sizeof__\n\n    def sizeof(o):\n        if id(o) in seen:       # do not double count the same object\n            return 0\n        seen.add(id(o))\n        s = getsizeof(o, default_size)\n\n        if verbose:\n            print(s, type(o), repr(o), file=stderr)\n\n        for typ, handler in all_handlers.items():\n            if isinstance(o, typ):\n                s += sum(map(sizeof, handler(o)))\n                break\n        return s\n\n    return sizeof(o)\n\n    # write the results in a csv file\ndef platform_score ( basename , mem_used ,n_estimators , time_spent , time_budget ):\n# write the results and platform information in a csv file (performance.csv)\n    with open(\'performance.csv\', \'a\') as fp:\n        a = csv.writer(fp, delimiter=\',\')\n        #[\'Data name\',\'Nb estimators\',\'System\', \'Machine\' , \'Platform\' ,\'memory used (Mb)\' , \'number of CPU\' ,\' time spent (sec)\' , \'time budget (sec)\'],\n        data = [\n        [basename,n_estimators,platform.system(), platform.machine(),platform.platform() , float(""{0:.2f}"".format(mem_used/1048576.0)) , str(psutil.cpu_count()) , float(""{0:.2f}"".format(time_spent)) ,    time_budget ]\n        ]\n        a.writerows(data)\n'"
AutoDL_ingestion_program/data_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: data.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'data.proto\',\n  package=\'autodl\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n\\ndata.proto\\x12\\x06\\x61utodl\\""\\x1f\\n\\nDenseValue\\x12\\x11\\n\\x05value\\x18\\x01 \\x03(\\x02\\x42\\x02\\x10\\x01\\""6\\n\\x0bSparseEntry\\x12\\x0b\\n\\x03row\\x18\\x01 \\x01(\\x05\\x12\\x0b\\n\\x03\\x63ol\\x18\\x02 \\x01(\\x05\\x12\\r\\n\\x05value\\x18\\x03 \\x01(\\x02\\""#\\n\\nCompressed\\x12\\x15\\n\\rencoded_image\\x18\\x01 \\x01(\\x0c\\""1\\n\\x0bSparseValue\\x12\\""\\n\\x05\\x65ntry\\x18\\x01 \\x03(\\x0b\\x32\\x13.autodl.SparseEntry\\""\\xac\\x02\\n\\nMatrixSpec\\x12\\x11\\n\\tcol_count\\x18\\x01 \\x01(\\x05\\x12\\x11\\n\\trow_count\\x18\\x02 \\x01(\\x05\\x12\\x17\\n\\x0fis_sequence_col\\x18\\x03 \\x01(\\x08\\x12\\x17\\n\\x0fis_sequence_row\\x18\\x04 \\x01(\\x08\\x12\\x18\\n\\x10has_locality_col\\x18\\x05 \\x01(\\x08\\x12\\x18\\n\\x10has_locality_row\\x18\\x06 \\x01(\\x08\\x12\\x30\\n\\x06\\x66ormat\\x18\\x08 \\x01(\\x0e\\x32\\x19.autodl.MatrixSpec.Format:\\x05\\x44\\x45NSE\\x12\\x15\\n\\tis_sparse\\x18\\x07 \\x01(\\x08\\x42\\x02\\x18\\x01\\x12\\x18\\n\\x0cnum_channels\\x18\\t \\x01(\\x05:\\x02-1\\""/\\n\\x06\\x46ormat\\x12\\t\\n\\x05\\x44\\x45NSE\\x10\\x00\\x12\\n\\n\\x06SPARSE\\x10\\x01\\x12\\x0e\\n\\nCOMPRESSED\\x10\\x02\\""\\xc0\\x01\\n\\x06Matrix\\x12%\\n\\x06sparse\\x18\\x01 \\x01(\\x0b\\x32\\x13.autodl.SparseValueH\\x00\\x12#\\n\\x05\\x64\\x65nse\\x18\\x02 \\x01(\\x0b\\x32\\x12.autodl.DenseValueH\\x00\\x12(\\n\\ncompressed\\x18\\x05 \\x01(\\x0b\\x32\\x12.autodl.CompressedH\\x00\\x12 \\n\\x04spec\\x18\\x03 \\x01(\\x0b\\x32\\x12.autodl.MatrixSpec\\x12\\x14\\n\\x0c\\x62undle_index\\x18\\x04 \\x01(\\x05\\x42\\x08\\n\\x06values\\""F\\n\\x0cMatrixBundle\\x12\\x1e\\n\\x06matrix\\x18\\x01 \\x03(\\x0b\\x32\\x0e.autodl.Matrix\\x12\\x16\\n\\x0esequence_index\\x18\\x02 \\x01(\\x05\\""B\\n\\x05Input\\x12$\\n\\x06\\x62undle\\x18\\x01 \\x03(\\x0b\\x32\\x14.autodl.MatrixBundle\\x12\\x13\\n\\x0bis_sequence\\x18\\x02 \\x01(\\x08\\""%\\n\\x05Label\\x12\\r\\n\\x05index\\x18\\x01 \\x01(\\x05\\x12\\r\\n\\x05score\\x18\\x02 \\x01(\\x02\\""&\\n\\x06Output\\x12\\x1c\\n\\x05label\\x18\\x01 \\x03(\\x0b\\x32\\r.autodl.Label\\""R\\n\\x06Sample\\x12\\n\\n\\x02id\\x18\\x01 \\x01(\\x05\\x12\\x1c\\n\\x05input\\x18\\x02 \\x01(\\x0b\\x32\\r.autodl.Input\\x12\\x1e\\n\\x06output\\x18\\x03 \\x01(\\x0b\\x32\\x0e.autodl.Output\\""\\xad\\x04\\n\\x11\\x44\\x61taSpecification\\x12\\\'\\n\\x0bmatrix_spec\\x18\\x01 \\x03(\\x0b\\x32\\x12.autodl.MatrixSpec\\x12\\x13\\n\\x0bis_sequence\\x18\\x02 \\x01(\\x08\\x12\\x12\\n\\noutput_dim\\x18\\x03 \\x01(\\x05\\x12J\\n\\x12label_to_index_map\\x18\\x04 \\x03(\\x0b\\x32..autodl.DataSpecification.LabelToIndexMapEntry\\x12N\\n\\x14\\x66\\x65\\x61ture_to_index_map\\x18\\x05 \\x03(\\x0b\\x32\\x30.autodl.DataSpecification.FeatureToIndexMapEntry\\x12N\\n\\x14\\x63hannel_to_index_map\\x18\\x06 \\x03(\\x0b\\x32\\x30.autodl.DataSpecification.ChannelToIndexMapEntry\\x12\\x18\\n\\rsequence_size\\x18\\x07 \\x01(\\x05:\\x01\\x31\\x12\\x14\\n\\x0csample_count\\x18\\x08 \\x01(\\x05\\x1a\\x36\\n\\x14LabelToIndexMapEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x05:\\x02\\x38\\x01\\x1a\\x38\\n\\x16\\x46\\x65\\x61tureToIndexMapEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x05:\\x02\\x38\\x01\\x1a\\x38\\n\\x16\\x43hannelToIndexMapEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x05:\\x02\\x38\\x01\')\n)\n\n\n\n_MATRIXSPEC_FORMAT = _descriptor.EnumDescriptor(\n  name=\'Format\',\n  full_name=\'autodl.MatrixSpec.Format\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DENSE\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SPARSE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'COMPRESSED\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=453,\n  serialized_end=500,\n)\n_sym_db.RegisterEnumDescriptor(_MATRIXSPEC_FORMAT)\n\n\n_DENSEVALUE = _descriptor.Descriptor(\n  name=\'DenseValue\',\n  full_name=\'autodl.DenseValue\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'autodl.DenseValue.value\', index=0,\n      number=1, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\')), file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=22,\n  serialized_end=53,\n)\n\n\n_SPARSEENTRY = _descriptor.Descriptor(\n  name=\'SparseEntry\',\n  full_name=\'autodl.SparseEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'row\', full_name=\'autodl.SparseEntry.row\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'col\', full_name=\'autodl.SparseEntry.col\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'autodl.SparseEntry.value\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=55,\n  serialized_end=109,\n)\n\n\n_COMPRESSED = _descriptor.Descriptor(\n  name=\'Compressed\',\n  full_name=\'autodl.Compressed\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'encoded_image\', full_name=\'autodl.Compressed.encoded_image\', index=0,\n      number=1, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=111,\n  serialized_end=146,\n)\n\n\n_SPARSEVALUE = _descriptor.Descriptor(\n  name=\'SparseValue\',\n  full_name=\'autodl.SparseValue\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'entry\', full_name=\'autodl.SparseValue.entry\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=148,\n  serialized_end=197,\n)\n\n\n_MATRIXSPEC = _descriptor.Descriptor(\n  name=\'MatrixSpec\',\n  full_name=\'autodl.MatrixSpec\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'col_count\', full_name=\'autodl.MatrixSpec.col_count\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'row_count\', full_name=\'autodl.MatrixSpec.row_count\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'is_sequence_col\', full_name=\'autodl.MatrixSpec.is_sequence_col\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'is_sequence_row\', full_name=\'autodl.MatrixSpec.is_sequence_row\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'has_locality_col\', full_name=\'autodl.MatrixSpec.has_locality_col\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'has_locality_row\', full_name=\'autodl.MatrixSpec.has_locality_row\', index=5,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'format\', full_name=\'autodl.MatrixSpec.format\', index=6,\n      number=8, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'is_sparse\', full_name=\'autodl.MatrixSpec.is_sparse\', index=7,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\')), file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'num_channels\', full_name=\'autodl.MatrixSpec.num_channels\', index=8,\n      number=9, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=-1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _MATRIXSPEC_FORMAT,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=200,\n  serialized_end=500,\n)\n\n\n_MATRIX = _descriptor.Descriptor(\n  name=\'Matrix\',\n  full_name=\'autodl.Matrix\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'sparse\', full_name=\'autodl.Matrix.sparse\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'dense\', full_name=\'autodl.Matrix.dense\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'compressed\', full_name=\'autodl.Matrix.compressed\', index=2,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'spec\', full_name=\'autodl.Matrix.spec\', index=3,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'bundle_index\', full_name=\'autodl.Matrix.bundle_index\', index=4,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'values\', full_name=\'autodl.Matrix.values\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=503,\n  serialized_end=695,\n)\n\n\n_MATRIXBUNDLE = _descriptor.Descriptor(\n  name=\'MatrixBundle\',\n  full_name=\'autodl.MatrixBundle\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'matrix\', full_name=\'autodl.MatrixBundle.matrix\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sequence_index\', full_name=\'autodl.MatrixBundle.sequence_index\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=697,\n  serialized_end=767,\n)\n\n\n_INPUT = _descriptor.Descriptor(\n  name=\'Input\',\n  full_name=\'autodl.Input\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'bundle\', full_name=\'autodl.Input.bundle\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'is_sequence\', full_name=\'autodl.Input.is_sequence\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=769,\n  serialized_end=835,\n)\n\n\n_LABEL = _descriptor.Descriptor(\n  name=\'Label\',\n  full_name=\'autodl.Label\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'index\', full_name=\'autodl.Label.index\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'score\', full_name=\'autodl.Label.score\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=837,\n  serialized_end=874,\n)\n\n\n_OUTPUT = _descriptor.Descriptor(\n  name=\'Output\',\n  full_name=\'autodl.Output\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'label\', full_name=\'autodl.Output.label\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=876,\n  serialized_end=914,\n)\n\n\n_SAMPLE = _descriptor.Descriptor(\n  name=\'Sample\',\n  full_name=\'autodl.Sample\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'autodl.Sample.id\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'input\', full_name=\'autodl.Sample.input\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'output\', full_name=\'autodl.Sample.output\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=916,\n  serialized_end=998,\n)\n\n\n_DATASPECIFICATION_LABELTOINDEXMAPENTRY = _descriptor.Descriptor(\n  name=\'LabelToIndexMapEntry\',\n  full_name=\'autodl.DataSpecification.LabelToIndexMapEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'autodl.DataSpecification.LabelToIndexMapEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'autodl.DataSpecification.LabelToIndexMapEntry.value\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1388,\n  serialized_end=1442,\n)\n\n_DATASPECIFICATION_FEATURETOINDEXMAPENTRY = _descriptor.Descriptor(\n  name=\'FeatureToIndexMapEntry\',\n  full_name=\'autodl.DataSpecification.FeatureToIndexMapEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'autodl.DataSpecification.FeatureToIndexMapEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'autodl.DataSpecification.FeatureToIndexMapEntry.value\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1444,\n  serialized_end=1500,\n)\n\n_DATASPECIFICATION_CHANNELTOINDEXMAPENTRY = _descriptor.Descriptor(\n  name=\'ChannelToIndexMapEntry\',\n  full_name=\'autodl.DataSpecification.ChannelToIndexMapEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'autodl.DataSpecification.ChannelToIndexMapEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'autodl.DataSpecification.ChannelToIndexMapEntry.value\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1502,\n  serialized_end=1558,\n)\n\n_DATASPECIFICATION = _descriptor.Descriptor(\n  name=\'DataSpecification\',\n  full_name=\'autodl.DataSpecification\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'matrix_spec\', full_name=\'autodl.DataSpecification.matrix_spec\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'is_sequence\', full_name=\'autodl.DataSpecification.is_sequence\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'output_dim\', full_name=\'autodl.DataSpecification.output_dim\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'label_to_index_map\', full_name=\'autodl.DataSpecification.label_to_index_map\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'feature_to_index_map\', full_name=\'autodl.DataSpecification.feature_to_index_map\', index=4,\n      number=5, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'channel_to_index_map\', full_name=\'autodl.DataSpecification.channel_to_index_map\', index=5,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sequence_size\', full_name=\'autodl.DataSpecification.sequence_size\', index=6,\n      number=7, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sample_count\', full_name=\'autodl.DataSpecification.sample_count\', index=7,\n      number=8, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_DATASPECIFICATION_LABELTOINDEXMAPENTRY, _DATASPECIFICATION_FEATURETOINDEXMAPENTRY, _DATASPECIFICATION_CHANNELTOINDEXMAPENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1001,\n  serialized_end=1558,\n)\n\n_SPARSEVALUE.fields_by_name[\'entry\'].message_type = _SPARSEENTRY\n_MATRIXSPEC.fields_by_name[\'format\'].enum_type = _MATRIXSPEC_FORMAT\n_MATRIXSPEC_FORMAT.containing_type = _MATRIXSPEC\n_MATRIX.fields_by_name[\'sparse\'].message_type = _SPARSEVALUE\n_MATRIX.fields_by_name[\'dense\'].message_type = _DENSEVALUE\n_MATRIX.fields_by_name[\'compressed\'].message_type = _COMPRESSED\n_MATRIX.fields_by_name[\'spec\'].message_type = _MATRIXSPEC\n_MATRIX.oneofs_by_name[\'values\'].fields.append(\n  _MATRIX.fields_by_name[\'sparse\'])\n_MATRIX.fields_by_name[\'sparse\'].containing_oneof = _MATRIX.oneofs_by_name[\'values\']\n_MATRIX.oneofs_by_name[\'values\'].fields.append(\n  _MATRIX.fields_by_name[\'dense\'])\n_MATRIX.fields_by_name[\'dense\'].containing_oneof = _MATRIX.oneofs_by_name[\'values\']\n_MATRIX.oneofs_by_name[\'values\'].fields.append(\n  _MATRIX.fields_by_name[\'compressed\'])\n_MATRIX.fields_by_name[\'compressed\'].containing_oneof = _MATRIX.oneofs_by_name[\'values\']\n_MATRIXBUNDLE.fields_by_name[\'matrix\'].message_type = _MATRIX\n_INPUT.fields_by_name[\'bundle\'].message_type = _MATRIXBUNDLE\n_OUTPUT.fields_by_name[\'label\'].message_type = _LABEL\n_SAMPLE.fields_by_name[\'input\'].message_type = _INPUT\n_SAMPLE.fields_by_name[\'output\'].message_type = _OUTPUT\n_DATASPECIFICATION_LABELTOINDEXMAPENTRY.containing_type = _DATASPECIFICATION\n_DATASPECIFICATION_FEATURETOINDEXMAPENTRY.containing_type = _DATASPECIFICATION\n_DATASPECIFICATION_CHANNELTOINDEXMAPENTRY.containing_type = _DATASPECIFICATION\n_DATASPECIFICATION.fields_by_name[\'matrix_spec\'].message_type = _MATRIXSPEC\n_DATASPECIFICATION.fields_by_name[\'label_to_index_map\'].message_type = _DATASPECIFICATION_LABELTOINDEXMAPENTRY\n_DATASPECIFICATION.fields_by_name[\'feature_to_index_map\'].message_type = _DATASPECIFICATION_FEATURETOINDEXMAPENTRY\n_DATASPECIFICATION.fields_by_name[\'channel_to_index_map\'].message_type = _DATASPECIFICATION_CHANNELTOINDEXMAPENTRY\nDESCRIPTOR.message_types_by_name[\'DenseValue\'] = _DENSEVALUE\nDESCRIPTOR.message_types_by_name[\'SparseEntry\'] = _SPARSEENTRY\nDESCRIPTOR.message_types_by_name[\'Compressed\'] = _COMPRESSED\nDESCRIPTOR.message_types_by_name[\'SparseValue\'] = _SPARSEVALUE\nDESCRIPTOR.message_types_by_name[\'MatrixSpec\'] = _MATRIXSPEC\nDESCRIPTOR.message_types_by_name[\'Matrix\'] = _MATRIX\nDESCRIPTOR.message_types_by_name[\'MatrixBundle\'] = _MATRIXBUNDLE\nDESCRIPTOR.message_types_by_name[\'Input\'] = _INPUT\nDESCRIPTOR.message_types_by_name[\'Label\'] = _LABEL\nDESCRIPTOR.message_types_by_name[\'Output\'] = _OUTPUT\nDESCRIPTOR.message_types_by_name[\'Sample\'] = _SAMPLE\nDESCRIPTOR.message_types_by_name[\'DataSpecification\'] = _DATASPECIFICATION\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nDenseValue = _reflection.GeneratedProtocolMessageType(\'DenseValue\', (_message.Message,), dict(\n  DESCRIPTOR = _DENSEVALUE,\n  __module__ = \'data_pb2\'\n  # @@protoc_insertion_point(class_scope:autodl.DenseValue)\n  ))\n_sym_db.RegisterMessage(DenseValue)\n\nSparseEntry = _reflection.GeneratedProtocolMessageType(\'SparseEntry\', (_message.Message,), dict(\n  DESCRIPTOR = _SPARSEENTRY,\n  __module__ = \'data_pb2\'\n  # @@protoc_insertion_point(class_scope:autodl.SparseEntry)\n  ))\n_sym_db.RegisterMessage(SparseEntry)\n\nCompressed = _reflection.GeneratedProtocolMessageType(\'Compressed\', (_message.Message,), dict(\n  DESCRIPTOR = _COMPRESSED,\n  __module__ = \'data_pb2\'\n  # @@protoc_insertion_point(class_scope:autodl.Compressed)\n  ))\n_sym_db.RegisterMessage(Compressed)\n\nSparseValue = _reflection.GeneratedProtocolMessageType(\'SparseValue\', (_message.Message,), dict(\n  DESCRIPTOR = _SPARSEVALUE,\n  __module__ = \'data_pb2\'\n  # @@protoc_insertion_point(class_scope:autodl.SparseValue)\n  ))\n_sym_db.RegisterMessage(SparseValue)\n\nMatrixSpec = _reflection.GeneratedProtocolMessageType(\'MatrixSpec\', (_message.Message,), dict(\n  DESCRIPTOR = _MATRIXSPEC,\n  __module__ = \'data_pb2\'\n  # @@protoc_insertion_point(class_scope:autodl.MatrixSpec)\n  ))\n_sym_db.RegisterMessage(MatrixSpec)\n\nMatrix = _reflection.GeneratedProtocolMessageType(\'Matrix\', (_message.Message,), dict(\n  DESCRIPTOR = _MATRIX,\n  __module__ = \'data_pb2\'\n  # @@protoc_insertion_point(class_scope:autodl.Matrix)\n  ))\n_sym_db.RegisterMessage(Matrix)\n\nMatrixBundle = _reflection.GeneratedProtocolMessageType(\'MatrixBundle\', (_message.Message,), dict(\n  DESCRIPTOR = _MATRIXBUNDLE,\n  __module__ = \'data_pb2\'\n  # @@protoc_insertion_point(class_scope:autodl.MatrixBundle)\n  ))\n_sym_db.RegisterMessage(MatrixBundle)\n\nInput = _reflection.GeneratedProtocolMessageType(\'Input\', (_message.Message,), dict(\n  DESCRIPTOR = _INPUT,\n  __module__ = \'data_pb2\'\n  # @@protoc_insertion_point(class_scope:autodl.Input)\n  ))\n_sym_db.RegisterMessage(Input)\n\nLabel = _reflection.GeneratedProtocolMessageType(\'Label\', (_message.Message,), dict(\n  DESCRIPTOR = _LABEL,\n  __module__ = \'data_pb2\'\n  # @@protoc_insertion_point(class_scope:autodl.Label)\n  ))\n_sym_db.RegisterMessage(Label)\n\nOutput = _reflection.GeneratedProtocolMessageType(\'Output\', (_message.Message,), dict(\n  DESCRIPTOR = _OUTPUT,\n  __module__ = \'data_pb2\'\n  # @@protoc_insertion_point(class_scope:autodl.Output)\n  ))\n_sym_db.RegisterMessage(Output)\n\nSample = _reflection.GeneratedProtocolMessageType(\'Sample\', (_message.Message,), dict(\n  DESCRIPTOR = _SAMPLE,\n  __module__ = \'data_pb2\'\n  # @@protoc_insertion_point(class_scope:autodl.Sample)\n  ))\n_sym_db.RegisterMessage(Sample)\n\nDataSpecification = _reflection.GeneratedProtocolMessageType(\'DataSpecification\', (_message.Message,), dict(\n\n  LabelToIndexMapEntry = _reflection.GeneratedProtocolMessageType(\'LabelToIndexMapEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _DATASPECIFICATION_LABELTOINDEXMAPENTRY,\n    __module__ = \'data_pb2\'\n    # @@protoc_insertion_point(class_scope:autodl.DataSpecification.LabelToIndexMapEntry)\n    ))\n  ,\n\n  FeatureToIndexMapEntry = _reflection.GeneratedProtocolMessageType(\'FeatureToIndexMapEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _DATASPECIFICATION_FEATURETOINDEXMAPENTRY,\n    __module__ = \'data_pb2\'\n    # @@protoc_insertion_point(class_scope:autodl.DataSpecification.FeatureToIndexMapEntry)\n    ))\n  ,\n\n  ChannelToIndexMapEntry = _reflection.GeneratedProtocolMessageType(\'ChannelToIndexMapEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _DATASPECIFICATION_CHANNELTOINDEXMAPENTRY,\n    __module__ = \'data_pb2\'\n    # @@protoc_insertion_point(class_scope:autodl.DataSpecification.ChannelToIndexMapEntry)\n    ))\n  ,\n  DESCRIPTOR = _DATASPECIFICATION,\n  __module__ = \'data_pb2\'\n  # @@protoc_insertion_point(class_scope:autodl.DataSpecification)\n  ))\n_sym_db.RegisterMessage(DataSpecification)\n_sym_db.RegisterMessage(DataSpecification.LabelToIndexMapEntry)\n_sym_db.RegisterMessage(DataSpecification.FeatureToIndexMapEntry)\n_sym_db.RegisterMessage(DataSpecification.ChannelToIndexMapEntry)\n\n\n_DENSEVALUE.fields_by_name[\'value\'].has_options = True\n_DENSEVALUE.fields_by_name[\'value\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_MATRIXSPEC.fields_by_name[\'is_sparse\'].has_options = True\n_MATRIXSPEC.fields_by_name[\'is_sparse\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))\n_DATASPECIFICATION_LABELTOINDEXMAPENTRY.has_options = True\n_DATASPECIFICATION_LABELTOINDEXMAPENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n_DATASPECIFICATION_FEATURETOINDEXMAPENTRY.has_options = True\n_DATASPECIFICATION_FEATURETOINDEXMAPENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n_DATASPECIFICATION_CHANNELTOINDEXMAPENTRY.has_options = True\n_DATASPECIFICATION_CHANNELTOINDEXMAPENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n# @@protoc_insertion_point(module_scope)\n'"
AutoDL_ingestion_program/dataset.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS-IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""AutoDL datasets.\n\nReads data in the Tensorflow AutoDL standard format.\n""""""\nimport os\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import app\nfrom tensorflow import flags\nfrom tensorflow import gfile\nfrom tensorflow import logging\nfrom google.protobuf import text_format\nimport dataset_utils\nfrom data_pb2 import DataSpecification\nfrom data_pb2 import MatrixSpec\n\n\ndef metadata_filename(dataset_name):\n  return os.path.join("""", dataset_name, ""metadata.textproto"")\n\n\ndef dataset_file_pattern(dataset_name):\n  return os.path.join("""", dataset_name, ""sample*"")\n\n\nclass AutoDLMetadata(object):\n  """"""AutoDL data specification.""""""\n\n  def __init__(self, dataset_name):\n    self.dataset_name_ = dataset_name\n    self.metadata_ = DataSpecification()\n    with gfile.GFile(metadata_filename(dataset_name), ""r"") as f:\n      text_format.Merge(f.read(), self.metadata_)\n\n  def get_dataset_name(self):\n    return self.dataset_name_\n\n  def is_compressed(self, bundle_index=0):\n    return self.metadata_.matrix_spec[\n        bundle_index].format == MatrixSpec.COMPRESSED\n\n  def is_sparse(self, bundle_index=0):\n    return self.metadata_.matrix_spec[bundle_index].format == MatrixSpec.SPARSE\n\n  def get_bundle_size(self):\n    return len(self.metadata_.matrix_spec)\n\n  def get_matrix_size(self, bundle_index=0):\n    return (self.metadata_.matrix_spec[bundle_index].row_count,\n            self.metadata_.matrix_spec[bundle_index].col_count)\n\n  def get_num_channels(self, bundle_index=0):\n    num_channels = self.metadata_.matrix_spec[bundle_index].num_channels\n    if num_channels == -1: # Unknown or undefined num_channels\n      if self.is_compressed(bundle_index): # If is compressed image, set to 3\n        return 3\n      else:\n        return 1\n    else:\n      return num_channels\n\n  def get_tensor_size(self, bundle_index=0):\n    """"""For a dataset with examples of shape (T,H,W,C), return the shape (H,W,C).\n    """"""\n    matrix_size = self.get_matrix_size(bundle_index)\n    num_channels = self.get_num_channels(bundle_index)\n    return matrix_size[0], matrix_size[1], num_channels\n\n  def get_tensor_shape(self, bundle_index=0):\n    """""" get_tensor_size updated with sequence size """"""\n    sequence_size = self.get_sequence_size()\n    row_count, col_count = self.get_matrix_size(bundle_index)\n    num_channels = self.get_num_channels(bundle_index)\n    return (sequence_size, row_count, col_count, num_channels)\n\n  def get_sequence_size(self):\n    return self.metadata_.sequence_size\n\n  def get_output_size(self):\n    return self.metadata_.output_dim\n\n  def size(self):\n    return self.metadata_.sample_count\n\n  def get_label_to_index_map(self):\n    return self.metadata_.label_to_index_map\n\n  def get_channel_to_index_map(self):\n    return self.metadata_.channel_to_index_map\n\n  def get_feature_to_index_map(self):\n    return self.metadata_.feature_to_index_map\n\n\nclass AutoDLDataset(object):\n  """"""AutoDL Datasets out of TFRecords of SequenceExamples.\n\n     See cs///experimental/autodl/export/tensorflow/README.md for more details\n     on the features and labels.\n  """"""\n\n  def __init__(self, dataset_name, num_parallel_readers=3):\n    """"""Construct an AutoDL Dataset.\n\n    Args:\n      dataset_name: name of the dataset under the \'dataset_dir\' flag.\n    """"""\n    self.dataset_name_ = dataset_name\n    self.num_parallel_readers = num_parallel_readers\n    self.metadata_ = AutoDLMetadata(dataset_name)\n    self._create_dataset()\n    self.dataset_ = self.dataset_.map(self._parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n  def get_dataset(self):\n    """"""Returns a tf.data.dataset object.""""""\n    return self.dataset_\n\n  def get_metadata(self):\n    """"""Returns an AutoDLMetadata object.""""""\n    return self.metadata_\n\n  def _feature_key(self, index, feature_name):\n    return str(index) + ""_"" + feature_name\n\n  def _parse_function(self, sequence_example_proto):\n    """"""Parse a SequenceExample in the AutoDL/TensorFlow format.\n\n    Args:\n      sequence_example_proto: a SequenceExample with ""x_dense_input"" or sparse\n          input representation.\n    Returns:\n      An array of tensors. For first edition of AutoDl challenge, returns a\n          pair `(features, labels)` where `features` is a Tensor of shape\n            [sequence_size, row_count, col_count, num_channels]\n          and `labels` a Tensor of shape\n            [output_dim, ]\n    """"""\n    sequence_features = {}\n    for i in range(self.metadata_.get_bundle_size()):\n      if self.metadata_.is_sparse(i):\n        sequence_features[self._feature_key(\n            i, ""sparse_col_index"")] = tf.VarLenFeature(tf.int64)\n        sequence_features[self._feature_key(\n            i, ""sparse_row_index"")] = tf.VarLenFeature(tf.int64)\n        sequence_features[self._feature_key(\n            i, ""sparse_channel_index"")] = tf.VarLenFeature(tf.int64)\n        sequence_features[self._feature_key(\n            i, ""sparse_value"")] = tf.VarLenFeature(tf.float32)\n      elif self.metadata_.is_compressed(i):\n        sequence_features[self._feature_key(\n            i, ""compressed"")] = tf.VarLenFeature(tf.string)\n      else:\n        sequence_features[self._feature_key(\n            i, ""dense_input"")] = tf.FixedLenSequenceFeature(\n                self.metadata_.get_tensor_size(i), dtype=tf.float32)\n    # read TFRecord\n    contexts, features = tf.parse_single_sequence_example(\n        sequence_example_proto,\n        context_features={\n            ""label_index"": tf.VarLenFeature(tf.int64),\n            ""label_score"": tf.VarLenFeature(tf.float32)\n        },\n        sequence_features=sequence_features)\n\n    sample = [] # will contain [features, labels]\n    for i in range(self.metadata_.get_bundle_size()):\n      key_dense = self._feature_key(i, ""dense_input"")\n      row_count, col_count = self.metadata_.get_matrix_size(i)\n      num_channels = self.metadata_.get_num_channels(i)\n      sequence_size = self.metadata_.get_sequence_size()\n      fixed_matrix_size = row_count > 0 and col_count > 0\n      row_count = row_count if row_count > 0 else None\n      col_count = col_count if col_count > 0 else None\n      if key_dense in features:\n        f = features[key_dense]\n        if not fixed_matrix_size:\n          raise ValueError(""To parse dense data, the tensor shape should "" +\n                           ""be known but got {} instead...""\\\n                           .format((sequence_size, row_count, col_count)))\n        f = tf.reshape(f, [sequence_size, row_count, col_count, num_channels])\n        sample.append(f)\n\n      sequence_size = sequence_size if sequence_size > 0 else None\n      key_compressed = self._feature_key(i, ""compressed"")\n      if key_compressed in features:\n        compressed_images = features[key_compressed].values\n        decompress_image_func =\\\n          lambda x: dataset_utils.decompress_image(x, num_channels=num_channels)\n        # `images` here is a 4D-tensor of shape [T, H, W, C], some of which might be unknown\n        images = tf.map_fn(\n            decompress_image_func,\n            compressed_images, dtype=tf.float32)\n        images.set_shape([sequence_size, row_count, col_count, num_channels])\n        sample.append(images)\n\n      key_sparse_val = self._feature_key(i, ""sparse_value"")\n      if key_sparse_val in features:\n        key_sparse_col = self._feature_key(i, ""sparse_col_index"")\n        key_sparse_row = self._feature_key(i, ""sparse_row_index"")\n        key_sparse_channel = self._feature_key(i, ""sparse_channel_index"")\n        sparse_col = features[key_sparse_col].values\n        sparse_row = features[key_sparse_row].values\n        try: # For back-compatibility. Before, there was no channel dimension.\n          sparse_channel = features[key_sparse_channel].values\n        except:\n          # I think this won\'t work, Tensor object has no \'len\'\n          sparse_channel = [0] * len(sparse_col)\n        sparse_val = features[key_sparse_val].values\n\n        if col_count > num_channels:\n            print(\'Sparse tabular data\')\n            # TABULAR: [120, 1]\n            #          [1000, 2]\n            #          [1504, 1]\n            # each row is (index, value)\n            sparse_col = tf.cast(sparse_col, tf.float32)\n            sparse_channel = tf.cast(sparse_channel, tf.float32)\n            tensor = tf.concat([\n                tf.reshape(sparse_col, [-1, 1]),\n                tf.reshape(sparse_val, [-1, 1])\n                ], 1)\n            tensor = tf.reshape(tensor, [1, -1, 2, 1])\n            tensor = tf.cast(tensor, tf.float32)\n            # Could use SparseTensor (to dense) because the shape of the dense tensor is known:\n            # (1, col_count, 1, 1)\n        else:\n            print(\'Sparse text data\')\n            # TEXT: [232, 2, 41]\n            # each example is a \'time series\' of indexes\n            tensor = tf.reshape(sparse_channel, [-1, 1, 1, 1])\n            tensor = tf.cast(tensor, tf.float32)\n\n        sample.append(tensor)\n        # TODO: see how we can keep sparse tensors instead of\n        # returning dense ones.\n\n    label_indices = (contexts[""label_index""].values,)\n    label_indices = tf.reshape(label_indices, [-1, 1])\n    sparse_tensor = tf.sparse.SparseTensor(indices=label_indices,\n                                           values=contexts[""label_score""].values,\n                                           dense_shape=(self.metadata_.get_output_size(),))\n    labels = tf.sparse.to_dense(sparse_tensor, validate_indices=False)\n    sample.append(labels)\n    return sample\n\n  def _create_dataset(self):\n    if not hasattr(self, ""dataset_""):\n      files = gfile.Glob(dataset_file_pattern(self.dataset_name_))\n      if not files:\n        raise IOError(""Unable to find training files. data_pattern=\'"" +\n                      dataset_file_pattern(self.dataset_name_) + ""\'."")\n      # logging.info(""Number of training files: %s."", str(len(files)))\n      if len(files) > 1:\n        # Read in multiple tfrecord files and interleave them in parallel\n        files = tf.data.Dataset.from_tensor_slices(files)\n        dataset = files.interleave(\n          tf.data.TFRecordDataset, cycle_length=self.num_parallel_readers,\n          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n      else:\n        # Only a single tfrecord was given\n        dataset = tf.data.TFRecordDataset(files, num_parallel_reads=self.num_parallel_readers)\n      self.dataset_ = dataset\n\n  def get_class_labels(self):\n    """"""Get all class labels""""""\n    # -- IG: inefficient, but... not needed very often\n    metadata=self.get_metadata()\n    label_to_index_map = metadata.get_label_to_index_map()\n    classes_list = [None] * len(label_to_index_map)\n    for label in label_to_index_map:\n      index = label_to_index_map[label]\n      classes_list[index] = label\n    return classes_list\n\n  def get_nth_element(self, num):\n    """"""Get n-th element in `autodl_dataset` using iterator.""""""\n    # -- IG: replaced previous 3d version\n    dataset = self.get_dataset()\n    iterator = dataset.make_one_shot_iterator()\n    next_element = iterator.get_next()\n    with tf.Session() as sess:\n      for _ in range(num+1):\n        tensor_4d, labels = sess.run(next_element)\n    return tensor_4d, labels\n\n  def show_image(self, num):\n    """"""Visualize a image represented by `tensor_4d` in RGB or grayscale.""""""\n    # -- IG: replaced previous 3d version\n    tensor_4d, label_confidence_pairs = self.get_nth_element(num)\n    num_channels = tensor_4d.shape[-1]\n    image = np.squeeze(tensor_4d[0])\n    # If the entries are float but in [0,255]\n    if not np.issubdtype(image.dtype, np.integer) and np.max(image) > 100:\n      image = image / 256\n    if num_channels == 1:\n      plt.imshow(image, cmap=\'gray\')\n    else:\n      # if not num_channels == 3:\n      #   raise ValueError(""Expected num_channels = 3 but got {} instead.""\\\n      #                    .format(num_channels))\n      plt.imshow(image)\n    labels = self.get_class_labels()\n    if labels:\n      lbl_list = [lbl for lbl, pos in zip(labels, label_confidence_pairs) if pos]\n      plt.title(\'Label(s): \' + \' \'.join(lbl_list))\n    else:\n      plt.title(\'Labels: \' + str(label_confidence_pairs))\n    plt.show()\n    return plt\n\ndef main(argv):\n  del argv  # Unused.\n  dataset = AutoDLDataset(""mnist"")\n  # dataset.init()\n  iterator = dataset.get_dataset().make_one_shot_iterator()\n  next_element = iterator.get_next()\n\n  sess = tf.Session()\n  for idx in range(10):\n    print(""Example "" + str(idx))\n    print(sess.run(next_element))\n\n\nif __name__ == ""__main__"":\n  app.run(main)\n'"
AutoDL_ingestion_program/dataset_utils.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS-IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Util functions to help parsing a Tensorflow dataset.""""""\n\nimport tensorflow as tf\n\n\ndef enforce_sequence_size(sample, sequence_size):\n  """"""Takes a Sample as 4-D tensor and enfore the sequence size.\n\n  The first dimension of the tensor represents the sequence length. Bundles\n  will be added or removed at the end.\n\n  Args:\n    sample: 4-D tensor representing a Sample.\n    sequence_size: int representing the maximum sequence length.\n  Returns:\n    The input 4-D tensor with added padds or removed bundles if it didn\'t\n    respect the sequence_size.\n  """"""\n  pad_size = tf.maximum(sequence_size - tf.shape(sample)[0], 0)\n\n  padded_sample = tf.pad(sample, ((0, pad_size), (0, 0), (0, 0), (0, 0)))\n\n  sample = tf.slice(padded_sample, [0, 0, 0, 0], [sequence_size, -1, -1, -1])\n  return sample\n\n\ndef decompress_image(compressed_image, num_channels=3):\n  """"""Decode a JPEG compressed image into a 3-D float Tensor.\n\n  TODO(andreamichi): Test this function.\n\n  Args:\n    compressed_image: string representing an image compressed as JPEG.\n  Returns:\n    3-D float Tensor with values ranging from [0, 1).\n  """"""\n  # Note that the resulting image contains an unknown height and width\n  # that is set dynamically by decode_jpeg. The returned image\n  # is a 3-D Tensor of uint8 [0, 255]. The third dimension is the channel.\n  image = tf.image.decode_image(compressed_image, channels=num_channels)\n\n  # Use float32 rather than uint8.\n  image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n  image.set_shape([None, None, num_channels])\n\n  return image\n'"
AutoDL_ingestion_program/ingestion.py,0,"b'################################################################################\r\n# Name:         Ingestion Program\r\n# Author:       Zhengying Liu, Isabelle Guyon, Adrien Pavao, Zhen Xu\r\n# Update time:  13 Aug 2019\r\n# Usage: python ingestion.py --dataset_dir=<dataset_dir> --output_dir=<prediction_dir> --ingestion_program_dir=<ingestion_program_dir> --code_dir=<code_dir> --score_dir=<score_dir>\r\n\r\n# AS A PARTICIPANT, DO NOT MODIFY THIS CODE.\r\n\r\nVERSION = \'v20191204\'\r\nDESCRIPTION =\\\r\n""""""This is the ""ingestion program"" written by the organizers. It takes the\r\ncode written by participants (with `model.py`) and one dataset as input,\r\nrun the code on the dataset and produce predictions on test set. For more\r\ninformation on the code/directory structure, please see comments in this\r\ncode (ingestion.py) and the README file of the starting kit.\r\nPrevious updates:\r\n20191204: [ZY] Add timer and separate model initialization from train/predict\r\n               process, : now model initilization doesn\'t consume time budget\r\n               quota (but can only use 20min)\r\n20190820: [ZY] Mark the beginning of ingestion right before model.py to reduce\r\n               variance\r\n20190708: [ZY] Integrate Julien\'s parallel data loader\r\n20190516: [ZY] Change time budget to 20 minutes.\r\n20190508: [ZY] Add time_budget to \'start.txt\'\r\n20190507: [ZY] Write timestamps to \'start.txt\'\r\n20190505: [ZY] Use argparse to parse directories AND time budget;\r\n               Rename input_dir to dataset_dir;\r\n               Rename submission_dir to code_dir;\r\n20190504: [ZY] Check if model.py has attribute done_training and use it to\r\n               determinate whether ingestion has ended;\r\n               Use module-specific logger instead of logging (with root logger);\r\n               At beginning, write start.txt with ingestion_pid and start_time;\r\n               In the end, write end.txt with end_time and ingestion_success;\r\n20190429: [ZY] Remove useless code block; better code layout.\r\n20190425: [ZY] Check prediction shape.\r\n20190424: [ZY] Use logging instead of logger; remove start.txt checking;\r\n20190419: [ZY] Try-except clause for training process;\r\n          always terminates successfully.\r\n""""""\r\n# The dataset directory dataset_dir (e.g. AutoDL_sample_data/) contains one dataset\r\n# folder (e.g. adult.data/) with the training set (train/)  and test set (test/),\r\n# each containing an some tfrecords data with a `metadata.textproto` file of\r\n# metadata on the dataset. So one AutoDL dataset will look like\r\n#\r\n#   adult.data\r\n#   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 test\r\n#   \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 metadata.textproto\r\n#   \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 sample-adult-test.tfrecord\r\n#   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 train\r\n#       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 metadata.textproto\r\n#       \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 sample-adult-train.tfrecord\r\n#\r\n# The output directory output_dir (e.g. AutoDL_sample_result_submission/)\r\n# will first have a start.txt file written by ingestion then receive\r\n# all predictions made during the whole train/predict process\r\n# (thus this directory is updated when a new prediction is made):\r\n# \tadult.predict_0\r\n# \tadult.predict_1\r\n# \tadult.predict_2\r\n#        ...\r\n# after ingestion has finished, a file end.txt will be written, containing\r\n# info on the duration ingestion used. This file is also used as a signal\r\n# for scoring program showing that ingestion has terminated.\r\n#\r\n# The code directory submission_program_dir (e.g. AutoDL_sample_code_submission/)\r\n# should contain your code submission model.py (and possibly other functions\r\n# it depends upon).\r\n#\r\n# We implemented several classes:\r\n# 1) DATA LOADING:\r\n#    ------------\r\n# dataset.py\r\n# dataset.AutoDLMetadata: Read metadata in metadata.textproto\r\n# dataset.AutoDLDataset: Read data and give tf.data.Dataset\r\n# 2) LEARNING MACHINE:\r\n#    ----------------\r\n# model.py\r\n# model.Model.train\r\n# model.Model.test\r\n#\r\n# ALL INFORMATION, SOFTWARE, DOCUMENTATION, AND DATA ARE PROVIDED ""AS-IS"".\r\n# UNIVERSITE PARIS SUD, CHALEARN, AND/OR OTHER ORGANIZERS OR CODE AUTHORS DISCLAIM\r\n# ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\r\n# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR ANY PARTICULAR PURPOSE, AND THE\r\n# WARRANTY OF NON-INFRIGEMENT OF ANY THIRD PARTY\'S INTELLECTUAL PROPERTY RIGHTS.\r\n# IN NO EVENT SHALL UNIVERSITE PARIS SUD AND/OR OTHER ORGANIZERS BE LIABLE FOR ANY SPECIAL,\r\n# INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER ARISING OUT OF OR IN\r\n# CONNECTION WITH THE USE OR PERFORMANCE OF SOFTWARE, DOCUMENTS, MATERIALS,\r\n# PUBLICATIONS, OR INFORMATION MADE AVAILABLE FOR THE CHALLENGE.\r\n#\r\n# Main contributors: Isabelle Guyon and Zhengying Liu\r\n\r\n# =========================== BEGIN OPTIONS ==============================\r\n\r\n# Verbosity level of logging:\r\n##############\r\n# Can be: NOTSET, DEBUG, INFO, WARNING, ERROR, CRITICAL\r\nverbosity_level = \'INFO\'\r\n\r\n# Some common useful packages\r\nfrom contextlib import contextmanager\r\nfrom os import getcwd as pwd\r\nfrom os.path import join\r\nfrom sys import argv, path\r\nimport argparse\r\nimport datetime\r\nimport glob\r\nimport logging\r\nimport math\r\nimport numpy as np\r\nimport os\r\nimport sys\r\nimport signal\r\nimport time\r\n\r\ndef get_logger(verbosity_level, use_error_log=False):\r\n  """"""Set logging format to something like:\r\n       2019-04-25 12:52:51,924 INFO score.py: <message>\r\n  """"""\r\n  logger = logging.getLogger(__file__)\r\n  logging_level = getattr(logging, verbosity_level)\r\n  logger.setLevel(logging_level)\r\n  formatter = logging.Formatter(\r\n    fmt=\'%(asctime)s %(levelname)s %(filename)s: %(message)s\')\r\n  stdout_handler = logging.StreamHandler(sys.stdout)\r\n  stdout_handler.setLevel(logging_level)\r\n  stdout_handler.setFormatter(formatter)\r\n  logger.addHandler(stdout_handler)\r\n  if use_error_log:\r\n    stderr_handler = logging.StreamHandler(sys.stderr)\r\n    stderr_handler.setLevel(logging.WARNING)\r\n    stderr_handler.setFormatter(formatter)\r\n    logger.addHandler(stderr_handler)\r\n  logger.propagate = False\r\n  return logger\r\n\r\nlogger = get_logger(verbosity_level)\r\n\r\ndef _HERE(*args):\r\n  """"""Helper function for getting the current directory of this script.""""""\r\n  h = os.path.dirname(os.path.realpath(__file__))\r\n  return os.path.abspath(os.path.join(h, *args))\r\n\r\ndef write_start_file(output_dir, start_time=None, time_budget=None,\r\n                     task_name=None):\r\n  """"""Create start file \'start.txt\' in `output_dir` with ingestion\'s pid and\r\n  start time.\r\n\r\n  The content of this file will be similar to:\r\n      ingestion_pid: 1\r\n      task_name: beatriz\r\n      time_budget: 7200\r\n      start_time: 1557923830.3012087\r\n      0: 1557923854.504741\r\n      1: 1557923860.091236\r\n      2: 1557923865.9630117\r\n      3: 1557923872.3627956\r\n      <more timestamps of predictions>\r\n  """"""\r\n  ingestion_pid = os.getpid()\r\n  start_filename =  \'start.txt\'\r\n  start_filepath = os.path.join(output_dir, start_filename)\r\n  with open(start_filepath, \'w\') as f:\r\n    f.write(\'ingestion_pid: {}\\n\'.format(ingestion_pid))\r\n    f.write(\'task_name: {}\\n\'.format(task_name))\r\n    f.write(\'time_budget: {}\\n\'.format(time_budget))\r\n    f.write(\'start_time: {}\\n\'.format(start_time))\r\n  logger.debug(""Finished writing \'start.txt\' file."")\r\n\r\ndef write_timestamp(output_dir, predict_idx, timestamp):\r\n  start_filename = \'start.txt\'\r\n  start_filepath = os.path.join(output_dir, start_filename)\r\n  with open(start_filepath, \'a\') as f:\r\n    f.write(\'{}: {}\\n\'.format(predict_idx, timestamp))\r\n  logger.debug(""Wrote timestamp {} to \'start.txt\' for predition {}.""\\\r\n               .format(timestamp, predict_idx))\r\n\r\nclass ModelApiError(Exception):\r\n  pass\r\n\r\nclass BadPredictionShapeError(Exception):\r\n  pass\r\n\r\nclass TimeoutException(Exception):\r\n  pass\r\n\r\nclass Timer:\r\n  def __init__(self):\r\n    self.duration = 0\r\n    self.total = None\r\n    self.remain = None\r\n    self.exec = None\r\n\r\n  def set(self, time_budget):\r\n    self.total = time_budget\r\n    self.remain = time_budget\r\n    self.exec = 0\r\n\r\n  @contextmanager\r\n  def time_limit(self, pname):\r\n    def signal_handler(signum, frame):\r\n      raise TimeoutException(""Timed out!"")\r\n    signal.signal(signal.SIGALRM, signal_handler)\r\n    signal.alarm(int(math.ceil(self.remain)))\r\n    start_time = time.time()\r\n\r\n    try:\r\n      yield\r\n    finally:\r\n      exec_time = time.time() - start_time\r\n      signal.alarm(0)\r\n      self.exec += exec_time\r\n      self.duration += exec_time\r\n      self.remain = self.total - self.exec\r\n\r\n      logger.info(""{} success, time spent so far {} sec""\\\r\n                  .format(pname, self.exec))\r\n\r\n      if self.remain <= 0:\r\n        raise TimeoutException(""Timed out for the process: {}!"".format(pname))\r\n\r\n# =========================== BEGIN PROGRAM ================================\r\n\r\nif __name__==""__main__"":\r\n\r\n    #### Check whether everything went well\r\n    ingestion_success = True\r\n\r\n    # Parse directories from input arguments\r\n    root_dir = _HERE(os.pardir)\r\n    default_dataset_dir = join(root_dir, ""AutoDL_sample_data"")\r\n    default_output_dir = join(root_dir, ""AutoDL_sample_result_submission"")\r\n    default_ingestion_program_dir = join(root_dir, ""AutoDL_ingestion_program"")\r\n    default_code_dir = join(root_dir, ""AutoDL_sample_code_submission"")\r\n    default_score_dir = join(root_dir, ""AutoDL_scoring_output"")\r\n    default_time_budget = 1200\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\'--dataset_dir\', type=str,\r\n                        default=default_dataset_dir,\r\n                        help=""Directory storing the dataset (containing "" +\r\n                             ""e.g. adult.data/)"")\r\n    parser.add_argument(\'--output_dir\', type=str,\r\n                        default=default_output_dir,\r\n                        help=""Directory storing the predictions. It will "" +\r\n                             ""contain e.g. [start.txt, adult.predict_0, "" +\r\n                             ""adult.predict_1, ..., end.txt] when ingestion "" +\r\n                             ""terminates."")\r\n    parser.add_argument(\'--ingestion_program_dir\', type=str,\r\n                        default=default_ingestion_program_dir,\r\n                        help=""Directory storing the ingestion program "" +\r\n                             ""`ingestion.py` and other necessary packages."")\r\n    parser.add_argument(\'--code_dir\', type=str,\r\n                        default=default_code_dir,\r\n                        help=""Directory storing the submission code "" +\r\n                             ""`model.py` and other necessary packages."")\r\n    parser.add_argument(\'--score_dir\', type=str,\r\n                        default=default_score_dir,\r\n                        help=""Directory storing the scoring output "" +\r\n                             ""e.g. `scores.txt` and `detailed_results.html`."")\r\n    parser.add_argument(\'--time_budget\', type=float,\r\n                        default=default_time_budget,\r\n                        help=""Time budget for running ingestion program."")\r\n    args = parser.parse_args()\r\n    logger.debug(""Parsed args are: "" + str(args))\r\n    logger.debug(""-"" * 50)\r\n    dataset_dir = args.dataset_dir\r\n    output_dir = args.output_dir\r\n    ingestion_program_dir= args.ingestion_program_dir\r\n    code_dir= args.code_dir\r\n    score_dir = args.score_dir\r\n    time_budget = args.time_budget\r\n    if dataset_dir.endswith(\'run/input\') and\\\r\n       code_dir.endswith(\'run/program\'):\r\n      logger.debug(""Since dataset_dir ends with \'run/input\' and code_dir ""\r\n                  ""ends with \'run/program\', suppose running on "" +\r\n                  ""CodaLab platform. Modify dataset_dir to \'run/input_data\' ""\r\n                  ""and code_dir to \'run/submission\'. "" +\r\n                  ""Directory parsing should be more flexible in the code of "" +\r\n                  ""compute worker: we need explicit directories for "" +\r\n                  ""dataset_dir and code_dir."")\r\n      dataset_dir = dataset_dir.replace(\'run/input\', \'run/input_data\')\r\n      code_dir = code_dir.replace(\'run/program\', \'run/submission\')\r\n\r\n    # Show directories for debugging\r\n    logger.debug(""sys.argv = "" + str(sys.argv))\r\n    logger.debug(""Using dataset_dir: "" + dataset_dir)\r\n    logger.debug(""Using output_dir: "" + output_dir)\r\n    logger.debug(""Using ingestion_program_dir: "" + ingestion_program_dir)\r\n    logger.debug(""Using code_dir: "" + code_dir)\r\n\r\n\t  # Our libraries\r\n    path.append(ingestion_program_dir)\r\n    path.append(code_dir)\r\n    #IG: to allow submitting the starting kit as sample submission\r\n    path.append(code_dir + \'/AutoDL_sample_code_submission\')\r\n    import data_io\r\n    from dataset import AutoDLDataset # THE class of AutoDL datasets\r\n\r\n    data_io.mkdir(output_dir)\r\n\r\n    #### INVENTORY DATA (and sort dataset names alphabetically)\r\n    datanames = data_io.inventory_data(dataset_dir)\r\n    #### Delete zip files and metadata file\r\n    datanames = [x for x in datanames if x.endswith(\'.data\')]\r\n\r\n    if len(datanames) != 1:\r\n      raise ValueError(""{} datasets found in dataset_dir={}!\\n""\\\r\n                       .format(len(datanames), dataset_dir) +\r\n                       ""Please put only ONE dataset under dataset_dir."")\r\n\r\n    basename = datanames[0]\r\n\r\n    logger.info(""************************************************"")\r\n    logger.info(""******** Processing dataset "" + basename[:-5].capitalize() +\r\n                 "" ********"")\r\n    logger.info(""************************************************"")\r\n    logger.debug(""Version: {}. Description: {}"".format(VERSION, DESCRIPTION))\r\n\r\n    ##### Begin creating training set and test set #####\r\n    logger.info(""Reading training set and test set..."")\r\n    D_train = AutoDLDataset(os.path.join(dataset_dir, basename, ""train""))\r\n    D_test = AutoDLDataset(os.path.join(dataset_dir, basename, ""test""))\r\n    ##### End creating training set and test set #####\r\n\r\n    ## Get correct prediction shape\r\n    num_examples_test = D_test.get_metadata().size()\r\n    output_dim = D_test.get_metadata().get_output_size()\r\n    correct_prediction_shape = (num_examples_test, output_dim)\r\n\r\n    # 20 min for participants to initializing and install other packages\r\n    try:\r\n      init_time_budget = 20 * 60 # time budget for initilization.\r\n      timer = Timer()\r\n      timer.set(init_time_budget)\r\n      with timer.time_limit(""Initialization""):\r\n        ##### Begin creating model #####\r\n        logger.info(""Creating model...this process should not exceed 20min."")\r\n        from model import Model # in participants\' model.py\r\n        M = Model(D_train.get_metadata()) # The metadata of D_train and D_test only differ in sample_count\r\n        ###### End creating model ######\r\n    except TimeoutException as e:\r\n      logger.info(""[-] Initialization phase exceeded time budget. Move to train/predict phase"")\r\n    except Exception as e:\r\n      logger.error(""Failed to initializing model."")\r\n      logger.error(""Encountered exception:\\n"" + str(e), exc_info=True)\r\n\r\n    # Mark starting time of ingestion\r\n    start = time.time()\r\n    logger.info(""=""*5 + "" Start core part of ingestion program. "" +\r\n                ""Version: {} "".format(VERSION) + ""=""*5)\r\n\r\n    write_start_file(output_dir, start_time=start, time_budget=time_budget,\r\n                     task_name=basename.split(\'.\')[0])\r\n\r\n    try:\r\n      # Check if the model has methods `train` and `test`.\r\n      for attr in [\'train\', \'test\']:\r\n        if not hasattr(M, attr):\r\n          raise ModelApiError(""Your model object doesn\'t have the method "" +\r\n                              ""`{}`. Please implement it in model.py."")\r\n\r\n      # Check if model.py uses new done_training API instead of marking\r\n      # stopping by returning None\r\n      use_done_training_api = hasattr(M, \'done_training\')\r\n      if not use_done_training_api:\r\n        logger.warning(""Your model object doesn\'t have an attribute "" +\r\n                       ""`done_training`. But this is necessary for ingestion "" +\r\n                       ""program to know whether the model has done training "" +\r\n                       ""and to decide whether to proceed more training. "" +\r\n                       ""Please add this attribute to your model."")\r\n\r\n      # Keeping track of how many predictions are made\r\n      prediction_order_number = 0\r\n\r\n      # Start the CORE PART: train/predict process\r\n      while(not (use_done_training_api and M.done_training)):\r\n        remaining_time_budget = start + time_budget - time.time()\r\n        # Train the model\r\n        logger.info(""Begin training the model..."")\r\n        M.train(D_train.get_dataset(),\r\n                remaining_time_budget=remaining_time_budget)\r\n        logger.info(""Finished training the model."")\r\n        remaining_time_budget = start + time_budget - time.time()\r\n        # Make predictions using the trained model\r\n        logger.info(""Begin testing the model by making predictions "" +\r\n                     ""on test set..."")\r\n        Y_pred = M.test(D_test.get_dataset(),\r\n                        remaining_time_budget=remaining_time_budget)\r\n        logger.info(""Finished making predictions."")\r\n        if Y_pred is None: # Stop train/predict process if Y_pred is None\r\n          logger.info(""The method model.test returned `None`. "" +\r\n                      ""Stop train/predict process."")\r\n          break\r\n        else: # Check if the prediction has good shape\r\n          prediction_shape = tuple(Y_pred.shape)\r\n          if prediction_shape != correct_prediction_shape:\r\n            raise BadPredictionShapeError(\r\n              ""Bad prediction shape! Expected {} but got {}.""\\\r\n              .format(correct_prediction_shape, prediction_shape)\r\n            )\r\n        # Write timestamp to \'start.txt\'\r\n        write_timestamp(output_dir, predict_idx=prediction_order_number,\r\n                        timestamp=time.time())\r\n        # Prediction files: adult.predict_0, adult.predict_1, ...\r\n        filename_test = basename[:-5] + \'.predict_\' +\\\r\n          str(prediction_order_number)\r\n        # Write predictions to output_dir\r\n        data_io.write(os.path.join(output_dir,filename_test), Y_pred)\r\n        prediction_order_number += 1\r\n        logger.info(""[+] {0:d} predictions made, time spent so far {1:.2f} sec""\\\r\n                     .format(prediction_order_number, time.time() - start))\r\n        remaining_time_budget = start + time_budget - time.time()\r\n        logger.info( ""[+] Time left {0:.2f} sec"".format(remaining_time_budget))\r\n        if remaining_time_budget<=0:\r\n          break\r\n    except Exception as e:\r\n      ingestion_success = False\r\n      logger.info(""Failed to run ingestion."")\r\n      logger.error(""Encountered exception:\\n"" + str(e), exc_info=True)\r\n\r\n    # Finishing ingestion program\r\n    end_time = time.time()\r\n    overall_time_spent = end_time - start\r\n\r\n    # Write overall_time_spent to a end.txt file\r\n    end_filename =  \'end.txt\'\r\n    with open(os.path.join(output_dir, end_filename), \'w\') as f:\r\n      f.write(\'ingestion_duration: \' + str(overall_time_spent) + \'\\n\')\r\n      f.write(\'ingestion_success: \' + str(int(ingestion_success)) + \'\\n\')\r\n      f.write(\'end_time: \' + str(end_time) + \'\\n\')\r\n      logger.info(""Wrote the file {} marking the end of ingestion.""\\\r\n                  .format(end_filename))\r\n      if ingestion_success:\r\n          logger.info(""[+] Done. Ingestion program successfully terminated."")\r\n          logger.info(""[+] Overall time spent %5.2f sec "" % overall_time_spent)\r\n      else:\r\n          logger.info(""[-] Done, but encountered some errors during ingestion."")\r\n          logger.info(""[-] Overall time spent %5.2f sec "" % overall_time_spent)\r\n\r\n    # Copy all files in output_dir to score_dir\r\n    os.system(""cp -R {} {}"".format(os.path.join(output_dir, \'*\'), score_dir))\r\n    logger.debug(""Copied all ingestion output to scoring output directory."")\r\n\r\n    logger.info(""[Ingestion terminated]"")\r\n'"
AutoDL_sample_code_submission/log_utils.py,0,"b'import logging\nimport os\nimport sys\nimport json\nimport time\nfrom typing import Any\nimport multiprocessing\nfrom collections import OrderedDict\nimport psutil\n\nimport functools\nnesting_level = 0\nis_start = None\nNCPU = multiprocessing.cpu_count()\n\n\ndef log(entry: Any):\n    global nesting_level\n    space = ""-"" * (4 * nesting_level)\n    logger.info(""{}{}"".format(space, entry))\n\n\ndef get_logger(verbosity_level, use_error_log=False, log_path=None):\n\n    logger = logging.getLogger(__file__)\n    logging_level = getattr(logging, verbosity_level)\n    logger.setLevel(logging_level)\n\n    if log_path is None:\n        log_dir = os.path.join("".."", ""log"")\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n        log_path = os.path.join(log_dir, ""log.txt"")\n    else:\n        log_path = os.path.join(log_path, ""log.txt"")\n\n    formatter = logging.Formatter(\n        fmt=\'%(asctime)s %(levelname)s %(filename)s: %(funcName)s: %(lineno)d: %(message)s\')\n\n    stdout_handler = logging.StreamHandler(sys.stdout)\n    stdout_handler.setLevel(logging_level)\n    stdout_handler.setFormatter(formatter)\n\n    logger.addHandler(stdout_handler)\n\n    fh = logging.FileHandler(log_path)\n    fh.setLevel(logging_level)\n    fh.setFormatter(formatter)\n\n    logger.addHandler(fh)\n\n    if use_error_log:\n        stderr_handler = logging.StreamHandler(sys.stderr)\n        stderr_handler.setLevel(logging.WARNING)\n        stderr_handler.setFormatter(formatter)\n        logger.addHandler(stderr_handler)\n    logger.propagate = False\n    return logger\n\n\nlogger = get_logger(\'INFO\')\n\ndebug = logger.debug\ninfo = logger.info\nwarning = logger.warning\nerror = logger.error\n\n\ndef timeit(method, start_log=None):\n    @functools.wraps(method)\n    def timed(*args, **kw):\n        global is_start\n        global nesting_level\n\n        if not is_start:\n            print()\n\n        is_start = True\n        # log(f""Start [{method.__name__}]:"" + (start_log if start_log else """"))\n        log(""Start [{}]:"" + (start_log if start_log else """").format(method.__name__))\n        nesting_level += 1\n\n        start_time = time.time()\n        result = method(*args, **kw)\n        end_time = time.time()\n\n        nesting_level -= 1\n        log(""End   [{}]. Time elapsed: {} sec."".format(method.__name__, end_time - start_time))\n        is_start = False\n\n        return result\n\n    return timed\n\n\n\nclass ASTimer():\n    def __init__(self):\n        self.times = [time.time()]\n        # self.accumulation = OrderedDict({})\n        self.accumulation = list()\n        self.total_time = 0.0\n        self.step_time = 0.0\n        self.counter = 0\n        self.repr_update_cnt = 0\n        self.train_start_t = time.time()\n        self.test_start_t = time.time()\n\n    def __call__(self, time_name):\n        if time_name == ""train_start"":\n            self.train_start_t = time.time()\n            self.times.append(self.train_start_t)\n            delta = self.times[-1] - self.times[-2]\n        elif time_name == ""train_end"":\n            self.times.append((time.time()))\n            delta = self.times[-1] - self.train_start_t\n        elif time_name == ""test_start"":\n            self.test_start_t = time.time()\n            self.times.append(self.test_start_t)\n            delta = self.times[-1] - self.times[-2]\n        elif time_name == ""test_end"":\n            self.times.append((time.time()))\n            delta = self.times[-1] - self.test_start_t\n        else:\n            self.times.append((time.time()))\n            delta = self.times[-1] - self.times[-2]\n\n\n        self.accumulation.append([""{}_{}"".format(self.counter, time_name), delta])\n        self.counter += 1\n\n    def __repr__(self):\n\n        timer_res = [""{}:        {}s"".format(t[0], round(t[1], 3)) for t in self.accumulation[self.repr_update_cnt: self.counter]]\n        self.repr_update_cnt = self.counter\n        return json.dumps(timer_res, indent=4)\n\n    def print_all(self):\n        timer_res = [""{}:       {}s"".format(t[0], t[1]) for t in self.accumulation]\n        return json.dumps(timer_res, indent=4)\n\n\nas_timer = ASTimer()\n\nos.system(""apt install wget"")\nhere = os.path.dirname(os.path.abspath(__file__))\n\ndef get_env_monitor():\n    # CPU usage\n    print(""=======Env Monitor: CPU Usage======"")\n    for i, percentage in enumerate(psutil.cpu_percent(percpu=True)):\n        print(""Core {}: {}%"".format(i, percentage))\n    print(""Total CPU Usage: {}%"".format(psutil.cpu_percent()))\n\n\n'"
AutoDL_sample_code_submission/model.py,0,"b'""""""Combine all winner solutions in previous challenges (AutoCV, AutoCV2,\nAutoNLP and AutoSpeech).\n""""""\n\n\nimport os\nimport sys\n\nhere = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.join(here, """"))\n\nmodel_dirs = [\n    """",  # current directory\n    ""Auto_Tabular"",\n]\nfor model_dir in model_dirs:\n    sys.path.append(os.path.join(here, model_dir))\n\n\ndef meta_domain_2_model(domain):\n    if domain in [""image""]:\n        sys.path.append(os.path.join(here, ""Auto_Image""))\n        from Auto_Image.model import Model as AutoImageModel\n        return AutoImageModel\n    elif domain in [""video""]:\n        sys.path.append(os.path.join(here, ""Auto_Video""))\n        from Auto_Video.model import Model as AutoVideoModel\n        return AutoVideoModel\n    elif domain in [""text""]:\n        from model_nlp import Model as AutoNlpModel\n        return AutoNlpModel\n    elif domain in [""speech""]:\n        from at_speech.model import Model as AutoSpeechModel\n        return AutoSpeechModel\n    else:\n        from Auto_Tabular.model import Model as TabularModel\n        return TabularModel\n\n\nclass Model:\n    """"""A model that combine all winner solutions. Using domain inferring and\n  apply winner solution in the corresponding domain.""""""\n\n    def __init__(self, metadata):\n        """"""\n        Args:\n          metadata: an AutoDLMetadata object. Its definition can be found in\n              AutoDL_ingestion_program/dataset.py\n        """"""\n        self.done_training = False\n        self.metadata = metadata\n        self.domain = infer_domain(metadata)\n        DomainModel = meta_domain_2_model(self.domain)\n        self.domain_model = DomainModel(self.metadata)\n        self.has_exception = False\n        self.y_pred_last = None\n\n    def train(self, dataset, remaining_time_budget=None):\n        """"""Train method of domain-specific model.""""""\n        # Convert training dataset to necessary format and\n        # store as self.domain_dataset_train\n\n        try:\n            self.domain_model.train(dataset, remaining_time_budget)\n            self.done_training = self.domain_model.done_training\n\n        except Exception as exp:\n            self.has_exception = True\n            self.done_training = True\n\n    def test(self, dataset, remaining_time_budget=None):\n        """"""Test method of domain-specific model.""""""\n        # Convert test dataset to necessary format and\n        # store as self.domain_dataset_test\n        # Make predictions\n\n        if self.done_training is True or self.has_exception is True:\n            return self.y_pred_last\n\n        try:\n            Y_pred = self.domain_model.test(dataset, remaining_time_budget=remaining_time_budget)\n\n            self.y_pred_last = Y_pred\n            self.done_training = self.domain_model.done_training\n\n        except MemoryError as mem_error:\n            self.has_exception = True\n            self.done_training = True\n        except Exception as exp:\n            self.has_exception = True\n            self.done_training = True\n\n        return self.y_pred_last\n\n\ndef infer_domain(metadata):\n    """"""Infer the domain from the shape of the 4-D tensor.\n\n  Args:\n    metadata: an AutoDLMetadata object.\n  """"""\n    row_count, col_count = metadata.get_matrix_size(0)\n    sequence_size = metadata.get_sequence_size()\n    channel_to_index_map = metadata.get_channel_to_index_map()\n    domain = None\n    if sequence_size == 1:\n        if row_count == 1 or col_count == 1:\n            domain = ""tabular""\n        else:\n            domain = ""image""\n    else:\n        if row_count == 1 and col_count == 1:\n            if len(channel_to_index_map) > 0:\n                domain = ""text""\n            else:\n                domain = ""speech""\n        else:\n            domain = ""video""\n    return domain\n'"
AutoDL_sample_code_submission/model_nlp.py,0,"b'""""""Combine all winner solutions in previous challenges (AutoCV, AutoCV2,\nAutoNLP and AutoSpeech).\n""""""\n\n\nimport numpy as np\nimport os\nimport sys\nimport tensorflow as tf\nfrom scipy import stats\n\nhere = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.join(here, """"))\n\nfrom nlp_autodl_config import autodl_g_conf_repr, META_SOLUS, DM_DS_PARAS\nimport multiprocessing\n\nNCPU = multiprocessing.cpu_count() - 1\n\nfeature_dict = {\'avg_upper_cnt\': 0.13243329407894736, \'check_len\': 224.19571428571427, \'imbalance_level\': 0,\n                \'avg_punct_cnt\': 0.009241672368421052, \'is_shuffle\': False, \'train_num\': 40000, \'language\': \'EN\',\n                \'kurtosis\': -2.0, \'avg_digit_cnt\': 0.0056662657894736845, \'seq_len_std\': 170,\n                \'first_detect_normal_std\': 0.00025, \'test_num\': 10000, \'max_length\': 1830, \'avg_length\': 230,\n                \'class_num\': 2, \'min_length\': 1}\n\n\nmodel_dirs = [\'\',  # current directory\n              \'AutoCV/{}\'.format(META_SOLUS.cv_solution),  # AutoCV/AutoCV2 winner model\n              \'AutoNLP/{}\'.format(META_SOLUS.nlp_solution),  # AutoNLP 2nd place winner\n              # \'AutoSpeech/PASA_NJU\',    # AutoSpeech winner\n              \'AutoSpeech/{}\'.format(META_SOLUS.speech_solution),  # AutoSpeech winner\n              \'tabular_Meysam\']  # simple NN model\nfor model_dir in model_dirs:\n    sys.path.append(os.path.join(here, model_dir))\n\nseq_len = []\nfrom nlp_dataset_convertor import TfDatasetsConvertor as TfDatasetTransformer\nfrom log_utils import logger\n\n\ndef meta_domain_2_model(domain):\n    from at_nlp.run_model import RunModel as AutoNLPModel\n    return AutoNLPModel\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True  #\nconfig.log_device_placement = False  #\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.9\n\nFIRST_SNOOP_DATA_NUM = 700\n\nINDEX_TO_TOKENS = []\nNLP_SEP = "" ""\n\n\ndef linear_sampling_func(call_num):\n    return min(max(call_num * 0.08, 0), 1)\n\n\nclass Model():\n    """"""A model that combine all winner solutions. Using domain inferring and\n    apply winner solution in the corresponding domain.""""""\n\n    def __init__(self, metadata):\n        """"""\n        Args:\n          metadata: an AutoDLMetadata object. Its definition can be found in\n              AutoDL_ingestion_program/dataset.py\n        """"""\n        self.done_training = False\n        self.metadata = metadata\n        self.first_round_sample_maxnum = 200\n        self.call_num = -1  # 0\n        self.domain_dataset_train_dict = {""x"": [],\n                                          ""y"": np.array([])}\n        self.domain = ""text""\n\n        DomainModel = meta_domain_2_model(self.domain)\n\n        self.domain_metadata = get_domain_metadata(metadata, self.domain)\n        self.class_num = self.domain_metadata[""class_num""]\n        self.train_num = self.domain_metadata[""train_num""]\n\n        self.domain_model = DomainModel(self.domain_metadata)\n        self.nlp_index_to_token = None\n        self.nlp_sep = None\n        self.init_nlp()\n        self.domain_model.vocab = self.vocabulary\n        self.shuffle = False\n        self.check_len = 0\n        self.imbalance_level = -1\n\n        self.tf_dataset_trainsformer = TfDatasetTransformer(if_train_shuffle=False, config=config)\n        self.tf_dataset_trainsformer.init_nlp_data(self.nlp_index_to_token, self.nlp_sep)\n        self.time_record = {}\n        self.seq_len = []\n        self.first_round_X = []\n        self.first_round_Y = np.array([])\n        self.X_test_raw = None\n\n    def init_nlp(self):\n\n        self.vocabulary = self.metadata.get_channel_to_index_map()\n        self.index_to_token = [None] * len(self.vocabulary)\n\n        for token in self.vocabulary:\n            index = self.vocabulary[token]\n            self.index_to_token[index] = token\n\n\n        if self.domain_metadata[""language""] == ""ZH"":\n            self.nlp_sep = """"\n\n        else:\n            self.nlp_sep = "" ""\n\n    def train(self, dataset, remaining_time_budget=None):\n        """"""Train method of domain-specific model.""""""\n        # Convert training dataset to necessary format and\n        # store as self.domain_dataset_train\n\n        self.tf_dataset_trainsformer.init_train_tfds(dataset, self.train_num)\n\n        if ""train_num"" not in self.domain_model.feature_dict:\n            self.domain_model.feature_dict[""train_num""] = self.train_num\n            self.domain_model.feature_dict[""class_num""] = self.class_num\n            self.domain_model.feature_dict[""language""] = self.domain_metadata[\'language\']\n\n        self.set_domain_dataset(dataset, is_training=True)\n\n\n        if self.call_num == -1:\n            self.domain_model.train(self.domain_dataset_train_dict[""x""], self.domain_dataset_train_dict[""y""],\n                                    remaining_time_budget=remaining_time_budget)\n        else:\n\n            self.domain_model.train(self.domain_dataset_train_dict[""x""], self.domain_dataset_train_dict[""y""],\n                                    remaining_time_budget=remaining_time_budget)\n            self.call_num += 1\n\n\n        self.done_training = self.domain_model.done_training\n\n    def test(self, dataset, remaining_time_budget=None):\n        """"""Test method of domain-specific model.""""""\n        # Convert test dataset to necessary format and\n\n        self.tf_dataset_trainsformer.init_test_tfds(dataset)\n\n        self.set_domain_dataset(dataset, is_training=False)\n\n        if self.domain in [\'text\', \'speech\'] and \\\n                (not self.domain_metadata[\'test_num\'] >= 0):\n            self.domain_metadata[\'test_num\'] = len(self.X_test)\n\n        if self.call_num == -1:\n            Y_pred = self.domain_model.test(self.domain_dataset_test,\n                                            remaining_time_budget=remaining_time_budget)\n            self.call_num += 1\n        else:\n            Y_pred = self.domain_model.test(self.domain_dataset_test,\n                                            remaining_time_budget=remaining_time_budget)\n        if ""test_num"" not in self.domain_model.feature_dict:\n            self.domain_model.feature_dict[""test_num""] = self.domain_metadata[\'test_num\']\n\n\n        self.done_training = self.domain_model.done_training\n\n        return Y_pred\n\n    def check_label_coverage(self, input_y):\n        _label_distribution = np.sum(np.array(input_y), 0)\n        empty_class_ = [i for i in range(_label_distribution.shape[0]) if _label_distribution[i] == 0]\n        normal_std = np.std(_label_distribution) / np.sum(_label_distribution)\n        if empty_class_:\n            label_coverage = 1-float(len(empty_class_)) / float(self.class_num)\n        else:\n            label_coverage = 1.0\n        return label_coverage, normal_std\n\n    def check_label_distribution(self, input_y):\n        _label_distribution = np.sum(np.array(input_y), 0)\n        empty_class_ = [i for i in range(_label_distribution.shape[0]) if _label_distribution[i] == 0]  # \xe5\x8c\x85\xe5\x90\xab\xe6\xa0\xb7\xe6\x9c\xac\xe9\x87\x8f\xe4\xb8\xba0\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\n        self.kurtosis = stats.kurtosis(_label_distribution)\n        self.normal_std = np.std(_label_distribution) / np.sum(_label_distribution)\n        if len(empty_class_) == 0:  #\n            self.shuffle = False\n        else:\n            self.shuffle = True\n        if self.normal_std > 0.3:\n            self.imbalance_level = 2  #\n        elif self.normal_std > 0.07:\n            self.imbalance_level = 1\n        else:\n            self.imbalance_level = 0\n\n\n    def check_input_length(self, input_x):\n        check_seq_len = []\n        for x in input_x:\n            x = x[x != -1]\n            check_seq_len.append(x.shape[0])\n        self.check_len = np.average(check_seq_len)\n        self.check_len_std = np.std(check_seq_len)\n\n\n    def decide_first_num(self):\n        snoop_data_num = min(0.01 * self.train_num, FIRST_SNOOP_DATA_NUM)  #\n        snoop_X, snoop_Y = self.tf_dataset_trainsformer.get_nlp_train_dataset(snoop_data_num)\n        label_coverage, normal_std = self.check_label_coverage(snoop_Y)\n        self.check_input_length(snoop_X[:FIRST_SNOOP_DATA_NUM])\n        if normal_std>0.3:\n            dataset_read_num = min(5000, int(0.1 * self.train_num))\n        else:\n            if self.class_num == 2 and self.train_num <= 50000:\n                if label_coverage == 1.0:  #\n                    dataset_read_num = max(int(0.01 * self.train_num), 500)\n\n                    if self.train_num <= 10000:\n                        dataset_read_num = min(5000, self.domain_metadata[""class_num""] * 3000)\n                else:\n                    dataset_read_num = min(5000, int(0.1 * self.train_num))\n\n            elif self.class_num == 2 and self.train_num > 50000:\n                if label_coverage == 1.0:  #\n\n                    dataset_read_num = min(int(0.01 * self.train_num), 1000)\n                else:  #\n                    dataset_read_num = min(5000, int(0.1 * self.train_num))\n\n\n            elif self.class_num > 2 and self.train_num <= 50000:\n                if label_coverage == 1.0:  #\n                    dataset_read_num = min(int((2 / self.class_num) * self.train_num), 1000)\n                    #\n                    if self.train_num <= 10000:\n                        dataset_read_num = min(5000, self.domain_metadata[""class_num""] * 3000)\n                else:\n                    dataset_read_num = min(5000, int(0.1 * self.train_num))\n            elif self.class_num > 2 and self.train_num > 50000:\n                if label_coverage == 1.0:  #\n                    #\n                    dataset_read_num = min(int((2 / self.class_num) * self.train_num), 1500)\n                else:  #\n                    dataset_read_num = min(5000, int(0.1 * self.train_num))\n\n                if self.domain_metadata[""language""] == ""ZH"" and self.check_len<=40:\n                    dataset_read_num += min(2000, 0.1*self.train_num)\n\n        X, Y = self.tf_dataset_trainsformer.get_nlp_train_dataset(dataset_read_num)\n        X = X + snoop_X\n        Y = np.concatenate([Y, snoop_Y], axis=0)\n        return dataset_read_num, X, Y\n\n    def set_domain_dataset(self, dataset, is_training=True):\n        """"""Recover the dataset in corresponding competition format (esp. AutoNLP\n        and AutoSpeech) and set corresponding attributes:\n          self.domain_dataset_train\n          self.domain_dataset_test\n        according to `is_training`.\n        """"""\n        # self.dataset = None\n        if is_training:\n            subset = \'train\'\n        else:\n            subset = \'test\'\n        attr_dataset = \'domain_dataset_{}\'.format(subset)\n\n        if not hasattr(self, attr_dataset):\n            if self.domain == \'text\':\n                if DM_DS_PARAS.text.if_sample and is_training:\n\n                    dataset_read_num, X, Y = self.decide_first_num()\n\n\n                    self.check_label_distribution(np.array(Y))\n                    self.domain_model.imbalance_level = self.imbalance_level\n\n                    feature_dict[""check_len""] = float(self.check_len)\n                    feature_dict[""kurtosis""] = float(self.kurtosis)\n                    feature_dict[""first_detect_normal_std""] = float(self.normal_std)\n                    feature_dict[""imbalance_level""] = self.imbalance_level\n                    feature_dict[""is_shuffle""] = self.shuffle\n\n                    if self.shuffle and self.domain_metadata[""language""] == ""ZH"":\n                        self.shuffle = False\n                        dataset_read_num = int(0.4 * self.train_num)\n\n                        _X, _Y = self.tf_dataset_trainsformer.get_nlp_train_dataset(dataset_read_num)\n                        X = X + _X\n                        Y = np.concatenate([Y, _Y], axis=0)\n\n\n                        _label_distribution = np.sum(Y, 0)\n                        occu_class_ = [i for i in range(_label_distribution.shape[0]) if _label_distribution[i] != 0] #\n\n                        if len(occu_class_)>=2:\n                            pass\n                        else:\n                            #\n                            dataset_read_num = int(0.2 * self.train_num)\n                            _X, _Y = self.tf_dataset_trainsformer.get_nlp_train_dataset(dataset_read_num)\n                            X = X + _X\n                            Y = np.concatenate([Y, _Y], axis=0)\n                            _label_distribution = np.sum(Y, 0)\n                            occu_class_ = [i for i in range(_label_distribution.shape[0]) if\n                                           _label_distribution[i] != 0]\n                            if len(occu_class_)<2:\n                                dataset_read_num = int(self.train_num)\n                                _X, _Y = self.tf_dataset_trainsformer.get_nlp_train_dataset(dataset_read_num)\n                                X = X + _X\n                                Y = np.concatenate([Y, _Y], axis=0)\n\n\n                    if self.shuffle:\n\n                        del self.tf_dataset_trainsformer\n                        self.tf_dataset_trainsformer = TfDatasetTransformer(if_train_shuffle=True, config=config)\n\n                        shuffle_size = max(int(0.5 * (self.train_num)), 10000)\n\n                        shuffle_dataset = dataset.shuffle(shuffle_size)\n\n\n                        self.tf_dataset_trainsformer.init_train_tfds(shuffle_dataset, self.train_num, pad_num=20)\n\n                        X, Y = self.tf_dataset_trainsformer.get_nlp_train_dataset(dataset_read_num)\n\n                        _label_distribution = np.sum(Y, 0)\n                        occu_class_ = [i for i in range(_label_distribution.shape[0]) if\n                                       _label_distribution[i] != 0]  #\n                        if len(occu_class_) >= 2:\n                            pass\n                        else:\n                            dataset_read_num = int(1 * (self.train_num))\n                            _X, _Y = self.tf_dataset_trainsformer.get_nlp_train_dataset(dataset_read_num)\n                            X = X + _X\n                            Y = np.concatenate([Y, _Y], axis=0)\n\n\n                    self.domain_model.avg_word_per_sample = float(\n                        len(self.vocabulary) / self.domain_metadata[""train_num""])\n                    if ""avg_word_per_sample"" not in feature_dict:\n                        feature_dict[""avg_word_per_sample""] = self.domain_model.avg_word_per_sample\n                    self.domain_model.feature_dict = feature_dict\n\n                elif not is_training:\n                    pad_num = 20\n                    X, Y = self.tf_dataset_trainsformer.get_nlp_test_dataset(pad_num=pad_num)\n\n                if is_training:\n                    self.first_round_X = X\n                    self.first_round_Y = Y\n                #\n\n                if self.call_num == 0:\n                    corpus = []\n                    seq_len = []\n\n                    for _x in X:\n                        _x = _x[_x != -1]\n                        num_words = max(int(_x.shape[0] * 0.1), 301)\n                        _x = _x[:num_words]\n                        _x = _x.astype(str)\n                        tokens = _x.tolist()\n                        document = self.nlp_sep.join(tokens)\n                        corpus.append(document)\n\n                else:\n                    corpus, seq_len = to_corpus(X, self.index_to_token, self.nlp_sep)\n\n\n                self.seq_len = seq_len\n\n\n                if is_training:\n                    labels = np.array(Y)\n                    cnt = np.sum(np.count_nonzero(labels, axis=1), axis=0)\n                    if cnt > labels.shape[0]:\n                        self.domain_model.multi_label = True\n\n                        self.domain_model.second_stage_model = None\n                        self.domain_model.ft_model = None\n                    domain_dataset = corpus, labels\n\n                    self.domain_dataset_train_dict[""x""] = corpus\n                    self.domain_dataset_train_dict[""y""] = labels\n                else:\n                    domain_dataset = corpus\n\n                    self.domain_dataset_train_dict[""x""] = corpus\n                    self.X_test = corpus\n\n                setattr(self, attr_dataset, domain_dataset)\n\n            elif self.domain == \'speech\':\n\n                setattr(self, attr_dataset, dataset)\n\n            elif self.domain in [\'image\', \'video\', \'tabular\']:\n                setattr(self, attr_dataset, dataset)\n            else:\n                raise ValueError(""The domain {} doesn\'t exist."".format(self.domain))\n\n        else:\n            if subset == \'test\':\n                if self.X_test_raw:\n                    self.domain_dataset_test, test_seq_len = to_corpus(self.X_test_raw, self.index_to_token,\n                                                                       self.nlp_sep)\n                self.X_test_raw = None\n                return\n\n            if self.domain == \'text\':\n                if DM_DS_PARAS.text.if_sample and is_training:\n                    if self.domain_model.multi_label:\n                        self.domain_model.use_multi_svm = True\n                        self.domain_model.start_cnn_call_num = 2\n                        dataset_read_num = self.train_num\n                        if dataset_read_num>50000:\n                            dataset_read_num = 50000\n                    else:\n                        if self.imbalance_level >= 1:\n                            dataset_read_num = self.train_num\n                            self.domain_model.use_multi_svm = False\n                            self.domain_model.start_cnn_call_num = 1\n                            if dataset_read_num > 50000:\n                                dataset_read_num = 50000\n                        else:\n                            self.domain_model.use_multi_svm = True\n                            if self.call_num <= self.domain_model.start_first_stage_call_num - 1:\n                                dataset_read_num = 3000\n                                if self.check_len <= 40 or self.normal_std >= 0.2:\n                                    dataset_read_num += min(int(0.2 * self.train_num), 12000)\n                            else:\n                                if self.call_num == self.domain_model.start_first_stage_call_num:\n                                    dataset_read_num = int(0.9 * self.domain_metadata[""train_num""])\n                                    if dataset_read_num > 50000:\n                                        dataset_read_num = 50000\n                                else:\n                                    if self.train_num <= 55555:\n                                        dataset_read_num = 4000\n                                    else:\n                                        dataset_read_num = 5500\n\n\n                    X, Y = self.tf_dataset_trainsformer.get_nlp_train_dataset(dataset_read_num)\n\n\n                    if self.call_num == 1:\n                        pass\n\n\n                corpus, seq_len = to_corpus(X, self.index_to_token, self.nlp_sep)\n                self.seq_len.extend(seq_len)\n\n                if ""avg_length"" not in self.domain_model.feature_dict:\n                    self.domain_model.feature_dict[""avg_length""] = int(np.average(self.seq_len))\n                    self.domain_model.feature_dict[""max_length""] = int(np.max(self.seq_len))\n                    self.domain_model.feature_dict[""min_length""] = int(np.min(self.seq_len))\n                    self.domain_model.feature_dict[""seq_len_std""] = int(np.std(self.seq_len))\n\n                if self.domain_model.max_length == 0:\n                    if int(np.max(self.seq_len)) <= 301:\n                        self.domain_model.max_length = int(np.max(self.seq_len))\n                        self.domain_model.ft_model_check_length = int(np.max(self.seq_len))\n\n                    else:\n                        self.domain_model.max_length = int(np.average(self.seq_len))\n                        self.domain_model.ft_model_check_length = int(np.average(self.seq_len))\n\n                    self.domain_model.seq_len_std = int(np.std(self.seq_len))\n\n\n                if is_training:\n                    labels = np.array(Y)\n                    domain_dataset = corpus, labels\n                    self.domain_dataset_train_dict[""x""] = corpus\n                    self.domain_dataset_train_dict[""y""] = labels\n                else:\n                    domain_dataset = corpus\n\n\ndef to_corpus(x, index_to_token=INDEX_TO_TOKENS, nlp_sep=NLP_SEP):\n    corpus = []\n    seq_len = []\n    for _x in x:\n        _x = _x[_x != -1]\n        tokens = [index_to_token[i] for i in _x]\n        seq_len.append(len(tokens))\n        document = nlp_sep.join(tokens)\n        corpus.append(document)\n    return corpus, seq_len\n\n\ndef infer_domain(metadata):\n    """"""Infer the domain from the shape of the 4-D tensor.\n\n    Args:\n      metadata: an AutoDLMetadata object.\n    """"""\n    row_count, col_count = metadata.get_matrix_size(0)\n    sequence_size = metadata.get_sequence_size()\n    channel_to_index_map = metadata.get_channel_to_index_map()\n    domain = None\n    if sequence_size == 1:\n        if row_count == 1 or col_count == 1:\n            domain = ""tabular""\n        else:\n            domain = ""image""\n    else:\n        if row_count == 1 and col_count == 1:\n            if len(channel_to_index_map) > 0:\n                domain = ""text""\n            else:\n                domain = ""speech""\n        else:\n            domain = ""video""\n    return domain\n\n\ndef is_chinese(tokens):\n    """"""Judge if the tokens are in Chinese. The current criterion is if each token\n    contains one single character, because when the documents are in Chinese,\n    we tokenize each character when formatting the dataset.\n    """"""\n    is_of_len_1 = all([len(t) == 1 for t in tokens[:100]])\n    num = [1 for t in tokens[:100] if len(t) == 1]\n    ratio = np.sum(num) / 100\n    if ratio > 0.95:\n        return True\n    else:\n        return False\n\n\n\ndef get_domain_metadata(metadata, domain, is_training=True):\n    """"""Recover the metadata in corresponding competitions, esp. AutoNLP\n    and AutoSpeech.\n\n    Args:\n      metadata: an AutoDLMetadata object.\n      domain: str, can be one of \'image\', \'video\', \'text\', \'speech\' or \'tabular\'.\n    """"""\n    if domain == \'text\':\n        # Fetch metadata info from `metadata`\n        class_num = metadata.get_output_size()\n        num_examples = metadata.size()\n        channel_to_index_map = metadata.get_channel_to_index_map()\n        revers_map = {v: k for k, v in channel_to_index_map.items()}\n        tokens = [revers_map[int(id)] for id in range(100)]\n        language = \'ZH\' if is_chinese(tokens) else \'EN\'\n        time_budget = 1200  # WARNING: Hard-coded\n\n        # Create domain metadata\n        domain_metadata = {}\n        domain_metadata[\'class_num\'] = class_num\n        if is_training:\n            domain_metadata[\'train_num\'] = num_examples\n            domain_metadata[\'test_num\'] = -1\n        else:\n            domain_metadata[\'train_num\'] = -1\n            domain_metadata[\'test_num\'] = num_examples\n        domain_metadata[\'language\'] = language\n        domain_metadata[\'time_budget\'] = time_budget\n\n        return domain_metadata\n\n    else:\n        return metadata\n'"
AutoDL_sample_code_submission/nlp_autodl_config.py,0,"b'import os\nimport json\nfrom collections import namedtuple\nimport copy\n\n#\nIF_RESET_TFGRAPH_SESS_RUN = False\nTF_DATASET_TO_NUMPY_MODE = ""graph"" # eager/graph\n#\n\n#\nautodl_global_config = {\n    ""meta_solution"": {\n        ""cv_solution"": ""DeepWisdom"",\n        ""nlp_solution"": ""upwind_flys"",\n        ""speech_solution"": ""PASA_NJU"",\n    },\n    ""data_space"": {\n        ""domain_dataset"": {\n            ""text"": {""if_sample"": True, ""sample_ratio"": 0.5},\n            ""speech"": {""if_sample"": True, ""sample_ratio"": 0.5},\n        }\n    },\n}\n\n\nclass MetaSoluConf(object):\n    def __init__(self):\n        self.cv_solution = None\n        self.nlp_solution = None\n        self.speech_solution = None\n\n\nclass DsDomainDatasetConf(object):\n    def __init__(self):\n        self.if_sample = None\n        self.sample_ratio = None\n\n\nclass DsDomainDatasetSets(object):\n    def __init__(self):\n        self.text = DsDomainDatasetConf()\n        self.speech = DsDomainDatasetConf()\n\n\nclass DsConf(object):\n    def __init__(self):\n        self.domain_dataset = DsDomainDatasetSets()\n\n\nclass AutoDlConf(object):\n    def __init__(self):\n        self.meta_solution = MetaSoluConf()\n        self.data_space = DsConf()\n\n\nclass ConfigParserA(object):\n    def _json_object_hook(self, d):\n        return namedtuple(""X"", d.keys())(*d.values())\n\n    def json2obj(self, data):\n        return json.loads(data, object_hook=self._json_object_hook)\n\n    def from_type_autodlconf(self, conf_data) -> AutoDlConf:\n        return copy.deepcopy(self.json2obj(json.dumps(conf_data)))\n\n\nautodl_g_conf_repr = json.dumps(autodl_global_config, indent=4)\n\nconfig_parser_a = ConfigParserA()\nAUTODL_G_CONF = config_parser_a.from_type_autodlconf(autodl_global_config)\nMETA_SOLUS = AUTODL_G_CONF.meta_solution\nDM_DS_PARAS = AUTODL_G_CONF.data_space.domain_dataset\n\n\n'"
AutoDL_sample_code_submission/nlp_dataset_convertor.py,0,"b'import tensorflow as tf\nimport numpy as np\nimport time\n\n# from Auto_NLP.utils.time_utils import info, error\n\n\nclass TfDatasetsConvertor(object):\n    def __init__(self, if_train_shuffle=False, config=None):\n        self.train_tfds = None\n        self.test_tfds = None\n        self.train_num = 0\n        self.test_num = 0\n        self.accum_train_x = list()\n        self.accum_train_y = None\n        self.accm_train_cnt = 0\n        self.accum_test_x = list()\n        self.accum_test_y = list()\n        self.accm_test_cnt = 0\n\n        self.tfds_train_os_iterator = None\n        self.tfds_train_iter_next = None\n\n        self.speech_train_dataset = {""x"": None, ""y"": None}\n        self.speech_test_dataset = None\n        self.speech_x_test = None\n        self.if_train_shuffle = if_train_shuffle\n\n        self.tfds_convertor_sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n        self.nlp_index_to_token_map = None\n        self.nlp_sep = None\n\n    def init_nlp_data(self, nlp_index_to_token_map, nlp_sep):\n        if self.nlp_index_to_token_map is None or self.nlp_sep is None:\n            self.nlp_index_to_token_map = nlp_index_to_token_map\n            self.nlp_sep = nlp_sep\n\n    def init_train_tfds(self, train_tfds, train_num, pad_num=30):\n\n        if self.train_tfds is None or self.train_num == 0:\n\n            #\n            self.train_tfds = train_tfds.padded_batch(pad_num, padded_shapes=([None, 1, 1, 1], [None]),\n                                                      padding_values=(\n                                                          tf.constant(-1, dtype=tf.float32)\n                                                          , tf.constant(-1, dtype=tf.float32)))\n\n            self.train_num = train_num\n\n    def init_test_tfds(self, test_tfds, pad_num=20):\n        if self.test_tfds is None:\n\n            self.test_tfds = test_tfds\n    def get_train_numpy(self, update_train_num):\n        if self.train_tfds is None:\n\n            return self.accum_train_x, self.accum_train_y\n\n        if self.tfds_train_os_iterator is None:\n            self.tfds_train_os_iterator = self.train_tfds.make_one_shot_iterator()\n            self.tfds_train_iter_next =self.tfds_train_os_iterator.get_next()\n\n        X, Y = [], []\n        cur_get_cnt = 0\n        cur_data_y = list()\n\n        if self.accm_train_cnt < self.train_num:\n\n            time_train_np_start = time.time()\n            while True:\n                try:\n                    example, labels = self.tfds_convertor_sess.run(self.tfds_train_iter_next)\n                    example = np.squeeze(example, (2, 3))\n                    example = np.squeeze(example, axis=-1)\n\n                    example = example.astype(np.int)\n                    self.accum_train_x.extend(example)\n\n                    X.extend(example)\n                    Y.extend(labels)\n\n                    cur_get_cnt += example.shape[0]\n                    self.accm_train_cnt += example.shape[0]\n\n                    if cur_get_cnt >= update_train_num or self.accm_train_cnt >= self.train_num:\n                        time_train_np_end = time.time()\n\n                        break\n\n                except tf.errors.OutOfRangeError:\n                    break\n\n            if self.accum_train_y is None:\n                self.accum_train_y = np.array(cur_data_y)\n            else:\n                pass\n\n        else:\n            self.tfds_convertor_sess.close()\n\n        return X, Y\n\n    def get_test_numpy(self):\n        if self.test_tfds is None:\n\n            return self.accum_test_x, self.accum_test_y\n        X, Y = [], []\n\n        if len(self.accum_test_x) == 0:\n            time_test_np_start = time.time()\n            tfds_test_os_iterator = self.test_tfds.make_one_shot_iterator()\n            tfds_test_iter_next = tfds_test_os_iterator.get_next()\n\n            with tf.Session(config=tf.ConfigProto(log_device_placement=False)) as sess:\n\n                while True:\n                    try:\n                        example, labels = sess.run(tfds_test_iter_next)\n                        example = np.squeeze(example, (2, 3))\n                        example = np.squeeze(example, axis=-1)\n                        example = example.astype(np.int)\n                        X.extend(example)\n                        Y.extend(labels)\n                        self.accum_test_x.extend(example)\n                        self.accum_test_y.extend(labels)\n                        self.accm_test_cnt += example.shape[0]\n\n                    except tf.errors.OutOfRangeError:\n                        break\n\n            time_test_np_end = time.time()\n\n\n        return X, Y\n\n    def get_nlp_train_dataset(self, take_size=100):\n        X, Y = self.get_train_numpy(update_train_num=take_size)\n        return X, Y\n\n    def get_nlp_test_dataset(self, pad_num=20):\n        self.test_tfds = self.test_tfds.padded_batch(pad_num, padded_shapes=([None, 1, 1, 1], [None]),\n                                                     padding_values=(\n                                                         tf.constant(-1, dtype=tf.float32)\n                                                         , tf.constant(-1, dtype=tf.float32)))\n\n        X, Y = self.get_test_numpy()\n        return X, Y\n\n'"
AutoDL_scoring_program/__init__.py,0,b''
AutoDL_scoring_program/libscores.py,0,"b'# Score library for NUMPY arrays\n# ChaLearn AutoML challenge\n\n# For regression:\n# solution and prediction are vectors of numerical values of the same dimension\n\n# For classification:\n# solution = array(p,n) of 0,1 truth values, samples in lines, classes in columns\n# prediction = array(p,n) of numerical scores between 0 and 1 (analogous to probabilities)\n\n# Isabelle Guyon and Arthur Pesah, ChaLearn, August-November 2014\n\n# ALL INFORMATION, SOFTWARE, DOCUMENTATION, AND DATA ARE PROVIDED ""AS-IS"".\n# ISABELLE GUYON, CHALEARN, AND/OR OTHER ORGANIZERS OR CODE AUTHORS DISCLAIM\n# ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR ANY PARTICULAR PURPOSE, AND THE\n# WARRANTY OF NON-INFRINGEMENT OF ANY THIRD PARTY\'S INTELLECTUAL PROPERTY RIGHTS.\n# IN NO EVENT SHALL ISABELLE GUYON AND/OR OTHER ORGANIZERS BE LIABLE FOR ANY SPECIAL,\n# INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER ARISING OUT OF OR IN\n# CONNECTION WITH THE USE OR PERFORMANCE OF SOFTWARE, DOCUMENTS, MATERIALS,\n# PUBLICATIONS, OR INFORMATION MADE AVAILABLE FOR THE CHALLENGE.\n\nimport os\nimport sys\nfrom sys import stderr\nfrom sys import version\n\nimport logging\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nfrom sklearn import metrics\nfrom sklearn.preprocessing import *\n\nswrite = stderr.write\nfrom os import getcwd as pwd\n\n# get_installed_distributions has gone from pip v10\ntry:\n    from pip._internal.utils.misc import get_installed_distributions as lib\nexcept ImportError:  # pip < 10\n    from pip import get_installed_distributions as lib\n\nfrom glob import glob\nimport platform\nimport psutil\nfrom functools import reduce\n\nif (os.name == ""nt""):\n    filesep = \'\\\\\'\nelse:\n    filesep = \'/\'\n\n\n# ========= Useful functions ==============\n\ndef _HERE(*args):\n    """"""Helper function for getting the current directory of the script.""""""\n    h = os.path.dirname(os.path.realpath(__file__))\n    return os.path.abspath(os.path.join(h, *args))\n\ndef get_logger(verbosity_level, use_error_log=False):\n  """"""Set logging format to something like:\n       2019-04-25 12:52:51,924 INFO score.py: <message>\n  """"""\n  logger = logging.getLogger(__file__)\n  logging_level = getattr(logging, verbosity_level)\n  logger.setLevel(logging_level)\n  formatter = logging.Formatter(\n    fmt=\'%(asctime)s %(levelname)s %(filename)s: %(message)s\')\n  stdout_handler = logging.StreamHandler(sys.stdout)\n  stdout_handler.setLevel(logging_level)\n  stdout_handler.setFormatter(formatter)\n  logger.addHandler(stdout_handler)\n  if use_error_log:\n    stderr_handler = logging.StreamHandler(sys.stderr)\n    stderr_handler.setLevel(logging.WARNING)\n    stderr_handler.setFormatter(formatter)\n    logger.addHandler(stderr_handler)\n  logger.propagate = False\n  return logger\n\ndef read_array(filename):\n    \'\'\' Read array and convert to 2d np arrays \'\'\'\n    array = np.loadtxt(filename)\n    if len(array.shape) == 1:\n        array = array.reshape(-1, 1)\n    return array\n\ndef list_files(startpath):\n    """"""List a tree structure of directories and files from startpath""""""\n    for root, dirs, files in os.walk(startpath):\n        level = root.replace(startpath, \'\').count(os.sep)\n        indent = \' \' * 4 * (level)\n        logger.debug(\'{}{}/\'.format(indent, os.path.basename(root)))\n        subindent = \' \' * 4 * (level + 1)\n        for f in files:\n            logger.debug(\'{}{}\'.format(subindent, f))\n\ndef sanitize_array(array):\n    \'\'\' Replace NaN and Inf (there should not be any!)\'\'\'\n    a = np.ravel(array)\n    maxi = np.nanmax((list(map(lambda x: x != float(\'inf\'), a))))  # Max except NaN and Inf\n    mini = np.nanmin((list(map(lambda x: x != float(\'-inf\'), a))))  # Mini except NaN and Inf\n    array[array == float(\'inf\')] = maxi\n    array[array == float(\'-inf\')] = mini\n    mid = (maxi + mini) / 2\n    array[np.isnan(array)] = mid\n    return array\n\n\ndef normalize_array(solution, prediction):\n    \'\'\' Use min and max of solution as scaling factors to normalize prediction,\n    then threshold it to [0, 1]. Binarize solution to {0, 1}.\n    This allows applying classification scores to all cases.\n    In principle, this should not do anything to properly formatted\n    classification inputs and outputs.\'\'\'\n    # Binarize solution\n    sol = np.ravel(solution)  # convert to 1-d array\n    maxi = np.nanmax((list(map(lambda x: x != float(\'inf\'), sol))))  # Max except NaN and Inf\n    mini = np.nanmin((list(map(lambda x: x != float(\'-inf\'), sol))))  # Mini except NaN and Inf\n    if maxi == mini:\n        print(\'Warning, cannot normalize\')\n        return [solution, prediction]\n    diff = maxi - mini\n    mid = (maxi + mini) / 2.\n    new_solution = np.copy(solution)\n    new_solution[solution >= mid] = 1\n    new_solution[solution < mid] = 0\n    # Normalize and threshold predictions (takes effect only if solution not in {0, 1})\n    new_prediction = (np.copy(prediction) - float(mini)) / float(diff)\n    new_prediction[new_prediction > 1] = 1  # and if predictions exceed the bounds [0, 1]\n    new_prediction[new_prediction < 0] = 0\n    # Make probabilities smoother\n    # new_prediction = np.power(new_prediction, (1./10))\n    return [new_solution, new_prediction]\n\n\ndef binarize_predictions(array, task=\'binary.classification\'):\n    \'\'\' Turn predictions into decisions {0,1} by selecting the class with largest\n    score for multiclass problems and thresholding at 0.5 for other cases.\'\'\'\n    # add a very small random value as tie breaker (a bit bad because this changes the score every time)\n    # so to make sure we get the same result every time, we seed it\n    # eps = 1e-15\n    # np.random.seed(sum(array.shape))\n    # array = array + eps*np.random.rand(array.shape[0],array.shape[1])\n    bin_array = np.zeros(array.shape)\n    if (task != \'multiclass.classification\') or (array.shape[1] == 1):\n        bin_array[array >= 0.5] = 1\n    else:\n        sample_num = array.shape[0]\n        for i in range(sample_num):\n            j = np.argmax(array[i, :])\n            bin_array[i, j] = 1\n    return bin_array\n\n\ndef acc_stat(solution, prediction):\n    \'\'\' Return accuracy statistics TN, FP, TP, FN\n     Assumes that solution and prediction are binary 0/1 vectors.\'\'\'\n    # This uses floats so the results are floats\n    TN = sum(np.multiply((1 - solution), (1 - prediction)))\n    FN = sum(np.multiply(solution, (1 - prediction)))\n    TP = sum(np.multiply(solution, prediction))\n    FP = sum(np.multiply((1 - solution), prediction))\n    # print ""TN ="",TN\n    # print ""FP ="",FP\n    # print ""TP ="",TP\n    # print ""FN ="",FN\n    return (TN, FP, TP, FN)\n\n\ndef tiedrank(a):\n    \'\'\' Return the ranks (with base 1) of a list resolving ties by averaging.\n     This works for numpy arrays.\'\'\'\n    m = len(a)\n    # Sort a in ascending order (sa=sorted vals, i=indices)\n    i = a.argsort()\n    sa = a[i]\n    # Find unique values\n    uval = np.unique(a)\n    # Test whether there are ties\n    R = np.arange(m, dtype=float) + 1  # Ranks with base 1\n    if len(uval) != m:\n        # Average the ranks for the ties\n        oldval = sa[0]\n        newval = sa[0]\n        k0 = 0\n        for k in range(1, m):\n            newval = sa[k]\n            if newval == oldval:\n                # moving average\n                R[k0:k + 1] = R[k - 1] * (k - k0) / (k - k0 + 1) + R[k] / (k - k0 + 1)\n            else:\n                k0 = k;\n                oldval = newval\n    # Invert the index\n    S = np.empty(m)\n    S[i] = R\n    return S\n\n\ndef mvmean(R, axis=0):\n    \'\'\' Moving average to avoid rounding errors. A bit slow, but...\n    Computes the mean along the given axis, except if this is a vector, in which case the mean is returned.\n    Does NOT flatten.\'\'\'\n    if len(R.shape) == 0: return R\n    average = lambda x: reduce(lambda i, j: (0, (j[0] / (j[0] + 1.)) * i[1] + (1. / (j[0] + 1)) * j[1]), enumerate(x))[\n        1]\n    R = np.array(R)\n    if len(R.shape) == 1: return average(R)\n    if axis == 1:\n        return np.array(map(average, R))\n    else:\n        return np.array(map(average, R.transpose()))\n\n\n# ======= Default metrics ========\n\ndef bac_binary(solution, prediction):\n    return bac_metric(solution, prediction, task=\'binary.classification\')\n\ndef bac_multiclass(solution, prediction):\n    return bac_metric(solution, prediction, task=\'multiclass.classification\')\n\ndef bac_multilabel(solution, prediction):\n    return bac_metric(solution, prediction, task=\'multilabel.classification\')\n\ndef auc_binary(solution, prediction):\n    return auc_metric(solution, prediction, task=\'binary.classification\')\n\ndef auc_multilabel(solution, prediction):\n    return auc_metric(solution, prediction, task=\'multilabel.classification\')\n\ndef pac_binary(solution, prediction):\n    return pac_metric(solution, prediction, task=\'binary.classification\')\n\ndef pac_multiclass(solution, prediction):\n    return pac_metric(solution, prediction, task=\'multiclass.classification\')\n\ndef pac_multilabel(solution, prediction):\n    return pac_metric(solution, prediction, task=\'multilabel.classification\')\n\ndef f1_binary(solution, prediction):\n    return f1_metric(solution, prediction, task=\'binary.classification\')\n\ndef f1_multilabel(solution, prediction):\n    return f1_metric(solution, prediction, task=\'multilabel.classification\')\n\ndef abs_regression(solution, prediction):\n    return a_metric(solution, prediction, task=\'regression\')\n\ndef r2_regression(solution, prediction):\n    return r2_metric(solution, prediction, task=\'regression\')\n\n\n# ======= Pre-made metrics ========\n\n### REGRESSION METRICS (work on raw solution and prediction)\n# These can be computed on all solutions and predictions (classification included)\n\ndef r2_metric(solution, prediction, task=\'regression\'):\n    \'\'\' 1 - Mean squared error divided by variance \'\'\'\n    mse = mvmean((solution - prediction) ** 2)\n    var = mvmean((solution - mvmean(solution)) ** 2)\n    score = 1 - mse / var\n    return mvmean(score)\n\n\ndef a_metric(solution, prediction, task=\'regression\'):\n    \'\'\' 1 - Mean absolute error divided by mean absolute deviation \'\'\'\n    mae = mvmean(np.abs(solution - prediction))  # mean absolute error\n    mad = mvmean(np.abs(solution - mvmean(solution)))  # mean absolute deviation\n    score = 1 - mae / mad\n    return mvmean(score)\n\n\n### END REGRESSION METRICS\n\n### CLASSIFICATION METRICS (work on solutions in {0, 1} and predictions in [0, 1])\n# These can be computed for regression scores only after running normalize_array\n\ndef bac_metric(solution, prediction, task=\'binary.classification\'):\n    \'\'\' Compute the normalized balanced accuracy. The binarization and\n    the normalization differ for the multi-label and multi-class case. \'\'\'\n    label_num = solution.shape[1]\n    score = np.zeros(label_num)\n    bin_prediction = binarize_predictions(prediction, task)\n    [tn, fp, tp, fn] = acc_stat(solution, bin_prediction)\n    # Bounding to avoid division by 0\n    eps = 1e-15\n    tp = sp.maximum(eps, tp)\n    pos_num = sp.maximum(eps, tp + fn)\n    tpr = tp / pos_num  # true positive rate (sensitivity)\n    if (task != \'multiclass.classification\') or (label_num == 1):\n        tn = sp.maximum(eps, tn)\n        neg_num = sp.maximum(eps, tn + fp)\n        tnr = tn / neg_num  # true negative rate (specificity)\n        bac = 0.5 * (tpr + tnr)\n        base_bac = 0.5  # random predictions for binary case\n    else:\n        bac = tpr\n        base_bac = 1. / label_num  # random predictions for multiclass case\n    bac = mvmean(bac)  # average over all classes\n    # Normalize: 0 for random, 1 for perfect\n    score = (bac - base_bac) / sp.maximum(eps, (1 - base_bac))\n    return score\n\n\ndef pac_metric(solution, prediction, task=\'binary.classification\'):\n    \'\'\' Probabilistic Accuracy based on log_loss metric.\n    We assume the solution is in {0, 1} and prediction in [0, 1].\n    Otherwise, run normalize_array.\'\'\'\n    debug_flag = False\n    [sample_num, label_num] = solution.shape\n    if label_num == 1: task = \'binary.classification\'\n    eps = 1e-15\n    the_log_loss = log_loss(solution, prediction, task)\n    # Compute the base log loss (using the prior probabilities)\n    pos_num = 1. * sum(solution)  # float conversion!\n    frac_pos = pos_num / sample_num  # prior proba of positive class\n    the_base_log_loss = prior_log_loss(frac_pos, task)\n    # Alternative computation of the same thing (slower)\n    # Should always return the same thing except in the multi-label case\n    # For which the analytic solution makes more sense\n    if debug_flag:\n        base_prediction = np.empty(prediction.shape)\n        for k in range(sample_num): base_prediction[k, :] = frac_pos\n        base_log_loss = log_loss(solution, base_prediction, task)\n        diff = np.array(abs(the_base_log_loss - base_log_loss))\n        if len(diff.shape) > 0: diff = max(diff)\n        if (diff) > 1e-10:\n            print(\'Arrggh {} != {}\'.format(the_base_log_loss, base_log_loss))\n    # Exponentiate to turn into an accuracy-like score.\n    # In the multi-label case, we need to average AFTER taking the exp\n    # because it is an NL operation\n    pac = mvmean(np.exp(-the_log_loss))\n    base_pac = mvmean(np.exp(-the_base_log_loss))\n    # Normalize: 0 for random, 1 for perfect\n    score = (pac - base_pac) / sp.maximum(eps, (1 - base_pac))\n    return score\n\n\ndef f1_metric(solution, prediction, task=\'binary.classification\'):\n    \'\'\' Compute the normalized f1 measure. The binarization differs\n        for the multi-label and multi-class case.\n        A non-weighted average over classes is taken.\n        The score is normalized.\'\'\'\n    label_num = solution.shape[1]\n    score = np.zeros(label_num)\n    bin_prediction = binarize_predictions(prediction, task)\n    [tn, fp, tp, fn] = acc_stat(solution, bin_prediction)\n    # Bounding to avoid division by 0\n    eps = 1e-15\n    true_pos_num = sp.maximum(eps, tp + fn)\n    found_pos_num = sp.maximum(eps, tp + fp)\n    tp = sp.maximum(eps, tp)\n    tpr = tp / true_pos_num  # true positive rate (recall)\n    ppv = tp / found_pos_num  # positive predictive value (precision)\n    arithmetic_mean = 0.5 * sp.maximum(eps, tpr + ppv)\n    # Harmonic mean:\n    f1 = tpr * ppv / arithmetic_mean\n    # Average over all classes\n    f1 = mvmean(f1)\n    # Normalize: 0 for random, 1 for perfect\n    if (task != \'multiclass.classification\') or (label_num == 1):\n        # How to choose the ""base_f1""?\n        # For the binary/multilabel classification case, one may want to predict all 1.\n        # In that case tpr = 1 and ppv = frac_pos. f1 = 2 * frac_pos / (1+frac_pos)\n        #     frac_pos = mvmean(solution.ravel())\n        #     base_f1 = 2 * frac_pos / (1+frac_pos)\n        # or predict random values with probability 0.5, in which case\n        #     base_f1 = 0.5\n        # the first solution is better only if frac_pos > 1/3.\n        # The solution in which we predict according to the class prior frac_pos gives\n        # f1 = tpr = ppv = frac_pos, which is worse than 0.5 if frac_pos<0.5\n        # So, because the f1 score is used if frac_pos is small (typically <0.1)\n        # the best is to assume that base_f1=0.5\n        base_f1 = 0.5\n    # For the multiclass case, this is not possible (though it does not make much sense to\n    # use f1 for multiclass problems), so the best would be to assign values at random to get\n    # tpr=ppv=frac_pos, where frac_pos=1/label_num\n    else:\n        base_f1 = 1. / label_num\n    score = (f1 - base_f1) / sp.maximum(eps, (1 - base_f1))\n    return score\n\n\ndef auc_metric(solution, prediction, task=\'binary.classification\'):\n    \'\'\' Normarlized Area under ROC curve (AUC).\n    Return Gini index = 2*AUC-1 for  binary classification problems.\n    Should work for a vector of binary 0/1 (or -1/1)""solution"" and any discriminant values\n    for the predictions. If solution and prediction are not vectors, the AUC\n    of the columns of the matrices are computed and averaged (with no weight).\n    The same for all classification problems (in fact it treats well only the\n    binary and multilabel classification problems).\'\'\'\n    # auc = metrics.roc_auc_score(solution, prediction, average=None)\n    # There is a bug in metrics.roc_auc_score: auc([1,0,0],[1e-10,0,0]) incorrect\n    label_num = solution.shape[1]\n    auc = np.empty(label_num)\n    for k in range(label_num):\n        r_ = tiedrank(prediction[:, k])\n        s_ = solution[:, k]\n        if sum(s_) == 0: print(\'WARNING: no positive class example in class {}\'.format(k + 1))\n        npos = sum(s_ == 1)\n        nneg = sum(s_ < 1)\n        auc[k] = (sum(r_[s_ == 1]) - npos * (npos + 1) / 2) / (nneg * npos)\n        # print(\'AUC[%d]=\' % k + \'%5.2f\' % auc[k])\n    return 2 * mvmean(auc) - 1\n\n\n### END CLASSIFICATION METRICS\n\n# ======= Specialized scores ========\n# We run all of them for all tasks even though they don\'t make sense for some tasks\n\ndef nbac_binary_score(solution, prediction):\n    \'\'\' Normalized balanced accuracy for binary and multilabel classification \'\'\'\n    return bac_metric(solution, prediction, task=\'binary.classification\')\n\n\ndef nbac_multiclass_score(solution, prediction):\n    \'\'\' Multiclass accuracy for binary and multilabel classification \'\'\'\n    return bac_metric(solution, prediction, task=\'multiclass.classification\')\n\n\ndef npac_binary_score(solution, prediction):\n    \'\'\' Normalized balanced accuracy for binary and multilabel classification \'\'\'\n    return pac_metric(solution, prediction, task=\'binary.classification\')\n\n\ndef npac_multiclass_score(solution, prediction):\n    \'\'\' Multiclass accuracy for binary and multilabel classification \'\'\'\n    return pac_metric(solution, prediction, task=\'multiclass.classification\')\n\n\ndef f1_binary_score(solution, prediction):\n    \'\'\' Normalized balanced accuracy for binary and multilabel classification \'\'\'\n    return f1_metric(solution, prediction, task=\'binary.classification\')\n\n\ndef f1_multiclass_score(solution, prediction):\n    \'\'\' Multiclass accuracy for binary and multilabel classification \'\'\'\n    return f1_metric(solution, prediction, task=\'multiclass.classification\')\n\n\ndef log_loss(solution, prediction, task=\'binary.classification\'):\n    \'\'\' Log loss for binary and multiclass. \'\'\'\n    [sample_num, label_num] = solution.shape\n    eps = 1e-15\n\n    pred = np.copy(prediction)  # beware: changes in prediction occur through this\n    sol = np.copy(solution)\n    if (task == \'multiclass.classification\') and (label_num > 1):\n        # Make sure the lines add up to one for multi-class classification\n        norma = np.sum(prediction, axis=1)\n        for k in range(sample_num):\n            pred[k, :] /= sp.maximum(norma[k], eps)\n            # Make sure there is a single label active per line for multi-class classification\n        sol = binarize_predictions(solution, task=\'multiclass.classification\')\n        # For the base prediction, this solution is ridiculous in the multi-label case\n\n    # Bounding of predictions to avoid log(0),1/0,...\n    pred = sp.minimum(1 - eps, sp.maximum(eps, pred))\n    # Compute the log loss\n    pos_class_log_loss = - mvmean(sol * np.log(pred), axis=0)\n    if (task != \'multiclass.classification\') or (label_num == 1):\n        # The multi-label case is a bunch of binary problems.\n        # The second class is the negative class for each column.\n        neg_class_log_loss = - mvmean((1 - sol) * np.log(1 - pred), axis=0)\n        log_loss = pos_class_log_loss + neg_class_log_loss\n        # Each column is an independent problem, so we average.\n        # The probabilities in one line do not add up to one.\n        # log_loss = mvmean(log_loss)\n        # print(\'binary {}\'.format(log_loss))\n        # In the multilabel case, the right thing i to AVERAGE not sum\n        # We return all the scores so we can normalize correctly later on\n    else:\n        # For the multiclass case the probabilities in one line add up one.\n        log_loss = pos_class_log_loss\n        # We sum the contributions of the columns.\n        log_loss = np.sum(log_loss)\n        # print(\'multiclass {}\'.format(log_loss))\n    return log_loss\n\n\ndef prior_log_loss(frac_pos, task=\'binary.classification\'):\n    \'\'\' Baseline log loss. For multiplr classes ot labels return the volues for each column\'\'\'\n    eps = 1e-15\n    frac_pos_ = sp.maximum(eps, frac_pos)\n    if (task != \'multiclass.classification\'):  # binary case\n        frac_neg = 1 - frac_pos\n        frac_neg_ = sp.maximum(eps, frac_neg)\n        pos_class_log_loss_ = - frac_pos * np.log(frac_pos_)\n        neg_class_log_loss_ = - frac_neg * np.log(frac_neg_)\n        base_log_loss = pos_class_log_loss_ + neg_class_log_loss_\n        # base_log_loss = mvmean(base_log_loss)\n        # print(\'binary {}\'.format(base_log_loss))\n        # In the multilabel case, the right thing i to AVERAGE not sum\n        # We return all the scores so we can normalize correctly later on\n    else:  # multiclass case\n        fp = frac_pos_ / sum(frac_pos_)  # Need to renormalize the lines in multiclass case\n        # Only ONE label is 1 in the multiclass case active for each line\n        pos_class_log_loss_ = - frac_pos * np.log(fp)\n        base_log_loss = np.sum(pos_class_log_loss_)\n    return base_log_loss\n\n\n# sklearn implementations for comparison\ndef log_loss_(solution, prediction):\n    return metrics.log_loss(solution, prediction)\n\n\ndef r2_score_(solution, prediction):\n    return metrics.r2_score(solution, prediction)\n\n\ndef a_score_(solution, prediction):\n    mad = float(mvmean(abs(solution - mvmean(solution))))\n    return 1 - metrics.mean_absolute_error(solution, prediction) / mad\n\n\ndef auc_score_(solution, prediction):\n    auc = metrics.roc_auc_score(solution, prediction, average=None)\n    return mvmean(auc)\n\n\n### SOME I/O functions\n\ndef ls(filename):\n    return sorted(glob(filename))\n\n\ndef write_list(lst):\n    for item in lst:\n        swrite(item + ""\\n"")\n\n\ndef mkdir(d):\n    if not os.path.exists(d):\n        os.makedirs(d)\n\n\ndef get_info(filename):\n    \'\'\' Get all information {attribute = value} pairs from the public.info file\'\'\'\n    info = {}\n    with open(filename, ""r"") as info_file:\n        lines = info_file.readlines()\n        features_list = list(map(lambda x: tuple(x.strip(""\\\'"").split("" = "")), lines))\n        for (key, value) in features_list:\n            info[key] = value.rstrip().strip(""\'"").strip(\' \')\n            if info[key].isdigit():  # if we have a number, we want it to be an integer\n                info[key] = int(info[key])\n    return info\n\n\ndef show_io(input_dir, output_dir):\n    \'\'\' show directory structure and inputs and autputs to scoring program\'\'\'\n    swrite(\'\\n=== DIRECTORIES ===\\n\\n\')\n    # Show this directory\n    swrite(""-- Current directory "" + pwd() + "":\\n"")\n    write_list(ls(\'.\'))\n    write_list(ls(\'./*\'))\n    write_list(ls(\'./*/*\'))\n    swrite(""\\n"")\n\n    # List input and output directories\n    swrite(""-- Input directory "" + input_dir + "":\\n"")\n    write_list(ls(input_dir))\n    write_list(ls(input_dir + \'/*\'))\n    write_list(ls(input_dir + \'/*/*\'))\n    write_list(ls(input_dir + \'/*/*/*\'))\n    swrite(""\\n"")\n    swrite(""-- Output directory  "" + output_dir + "":\\n"")\n    write_list(ls(output_dir))\n    write_list(ls(output_dir + \'/*\'))\n    swrite(""\\n"")\n\n    # write meta data to sdterr\n    swrite(\'\\n=== METADATA ===\\n\\n\')\n    swrite(""-- Current directory "" + pwd() + "":\\n"")\n    try:\n        metadata = yaml.load(open(\'metadata\', \'r\'))\n        for key, value in metadata.items():\n            swrite(key + \': \')\n            swrite(str(value) + \'\\n\')\n    except:\n        swrite(""none\\n"");\n    swrite(""-- Input directory "" + input_dir + "":\\n"")\n    try:\n        metadata = yaml.load(open(os.path.join(input_dir, \'metadata\'), \'r\'))\n        for key, value in metadata.items():\n            swrite(key + \': \')\n            swrite(str(value) + \'\\n\')\n        swrite(""\\n"")\n    except:\n        swrite(""none\\n"");\n\n\ndef show_version(scoring_version):\n    \'\'\' Python version and library versions \'\'\'\n    swrite(\'\\n=== VERSIONS ===\\n\\n\')\n    # Scoring program version\n    swrite(""Scoring program version: "" + str(scoring_version) + ""\\n\\n"")\n    # Python version\n    swrite(""Python version: "" + version + ""\\n\\n"")\n    # Give information on the version installed\n    swrite(""Versions of libraries installed:\\n"")\n    map(swrite, sorted([""%s==%s\\n"" % (i.key, i.version) for i in lib()]))\n\n\ndef show_platform():\n    \'\'\' Show information on platform\'\'\'\n    swrite(\'\\n=== SYSTEM ===\\n\\n\')\n    try:\n        linux_distribution = platform.linux_distribution()\n    except:\n        linux_distribution = ""N/A""\n    swrite(""""""\n    dist: %s\n    linux_distribution: %s\n    system: %s\n    machine: %s\n    platform: %s\n    uname: %s\n    version: %s\n    mac_ver: %s\n    memory: %s\n    number of CPU: %s\n    """""" % (\n        str(platform.dist()),\n        linux_distribution,\n        platform.system(),\n        platform.machine(),\n        platform.platform(),\n        platform.uname(),\n        platform.version(),\n        platform.mac_ver(),\n        psutil.virtual_memory(),\n        str(psutil.cpu_count())\n    ))\n\n\ndef compute_all_scores(solution, prediction):\n    \'\'\' Compute all the scores and return them as a dist\'\'\'\n    missing_score = -0.999999\n    scoring = {\'BAC (multilabel)\': nbac_binary_score,\n               \'BAC (multiclass)\': nbac_multiclass_score,\n               \'F1  (multilabel)\': f1_binary_score,\n               \'F1  (multiclass)\': f1_multiclass_score,\n               \'Regression ABS  \': a_metric,\n               \'Regression R2   \': r2_metric,\n               \'AUC (multilabel)\': auc_metric,\n               \'PAC (multilabel)\': npac_binary_score,\n               \'PAC (multiclass)\': npac_multiclass_score}\n    # Normalize/sanitize inputs\n    [csolution, cprediction] = normalize_array(solution, prediction)\n    solution = sanitize_array(solution);\n    prediction = sanitize_array(prediction)\n    # Compute all scores\n    score_names = sorted(scoring.keys())\n    scores = {}\n    for key in score_names:\n        scoring_func = scoring[key]\n        try:\n            if key == \'Regression R2   \' or key == \'Regression ABS  \':\n                scores[key] = scoring_func(solution, prediction)\n            else:\n                scores[key] = scoring_func(csolution, cprediction)\n        except:\n            scores[key] = missing_score\n    return scores\n\n\ndef write_scores(fp, scores):\n    \'\'\' Write scores to file opened under file pointer fp\'\'\'\n    for key in scores.keys():\n        str_temp = ""%s --> %s\\n"" % (key, scores[key])\n        fp.write(str_temp.encode(\'utf-8\'))\n        print(key + "" --> "" + str(scores[key]))\n\n\ndef show_all_scores(solution, prediction):\n    \'\'\' Compute and display all the scores for debug purposes\'\'\'\n    scores = compute_all_scores(solution, prediction)\n    for key in scores.keys():\n        print(key + "" --> "" + str(scores[key]))\n\n\n############################### TEST PROGRAM ##########################################\nif __name__ == ""__main__"":\n    # This shows a bug in metrics.roc_auc_score\n    #    print(\'\\n\\nBug in sklearn.metrics.roc_auc_score:\')\n    #    print(\'auc([1,0,0],[1e-10,0,0])=1\')\n    #    print(\'Correct (ours): \' +str(auc_metric(np.array([[1,0,0]]).transpose(),np.array([[1e-10,0,0]]).transpose())))\n    #    print(\'Incorrect (sklearn): \' +str(metrics.roc_auc_score(np.array([1,0,0]),np.array([1e-10,0,0]))))\n\n    # This checks the binary and multi-class cases are well implemented\n    # In the 2-class case, all results should be identical, except for f1 because\n    # this is a score that is not symmetric in the 2 classes.\n    eps = 1e-15\n    print(\'\\n\\nBinary score verification:\')\n    print(\'\\n\\n==========================\')\n\n    sol0 = np.array([[1, 0], [1, 0], [0, 1], [0, 1]])\n\n    comment = [\'PERFECT\']\n    Pred = [sol0]\n    Sol = [sol0]\n\n    comment.append(\'ANTI-PERFECT, very bad for r2_score\')\n    Pred.append(1 - sol0)\n    Sol.append(sol0)\n\n    comment.append(\'UNEVEN PROBA, BUT BINARIZED VERSION BALANCED (bac and auc=0.5)\')\n    Pred.append(\n        np.array([[0.7, 0.3], [0.4, 0.6], [0.49, 0.51], [0.2, 0.8]]))  # here is we have only 2, pac not 0 in uni-col\n    Sol.append(sol0)\n\n    comment.append(\'PROBA=0.5, TIES BROKEN WITH SMALL VALUE TO EVEN THE BINARIZED VERSION\')\n    Pred.append(\n        np.array([[0.5 + eps, 0.5 - eps], [0.5 - eps, 0.5 + eps], [0.5 + eps, 0.5 - eps], [0.5 - eps, 0.5 + eps]]))\n    Sol.append(sol0)\n\n    comment.append(\'PROBA=0.5, TIES NOT BROKEN (bad for f1 score)\')\n    Pred.append(np.array([[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]))\n    Sol.append(sol0)\n\n    sol1 = np.array([[1, 0], [0, 1], [0, 1]])\n\n    comment.append(\'EVEN PROBA, but wrong PAC prior because uneven number of samples\')\n    Pred.append(np.array([[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]))\n    Sol.append(sol1)\n\n    comment.append(\n        \'Correct PAC prior; score generally 0. But 100% error on positive class because of binarization so f1 (1 col) is at its worst.\')\n    p = len(sol1)\n    Pred.append(np.array([sum(sol1) * 1. / p] * p))\n    Sol.append(sol1)\n\n    comment.append(\'All positive\')\n    Pred.append(np.array([[1, 1], [1, 1], [1, 1]]))\n    Sol.append(sol1)\n\n    comment.append(\'All negative\')\n    Pred.append(np.array([[0, 0], [0, 0], [0, 0]]))\n    Sol.append(sol1)\n\n    for k in range(len(Sol)):\n        sol = Sol[k]\n        pred = Pred[k]\n        print(\'****** ({}) {} ******\'.format(k, comment[k]))\n        print(\'------ 2 columns ------\')\n        show_all_scores(sol, pred)\n        print(\'------ 1 column  ------\')\n        sol = np.array([sol[:, 0]]).transpose()\n        pred = np.array([pred[:, 0]]).transpose()\n        show_all_scores(sol, pred)\n\n    print(\'\\n\\nMulticlass score verification:\')\n    print(\'\\n\\n==========================\')\n    sol2 = np.array([[1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]])\n\n    comment = [\'Three classes perfect\']\n    Pred = [sol2]\n    Sol = [sol2]\n\n    comment.append(\'Three classes all wrong\')\n    Pred.append(np.array([[0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1]]))\n    Sol.append(sol2)\n\n    comment.append(\'Three classes equi proba\')\n    Pred.append(np.array([[1 / 3, 1 / 3, 1 / 3], [1 / 3, 1 / 3, 1 / 3], [1 / 3, 1 / 3, 1 / 3], [1 / 3, 1 / 3, 1 / 3]]))\n    Sol.append(sol2)\n\n    comment.append(\'Three classes some proba that do not add up\')\n    Pred.append(np.array([[0.2, 0, 0.5], [0.8, 0.4, 0.1], [0.9, 0.1, 0.2], [0.7, 0.3, 0.3]]))\n    Sol.append(sol2)\n\n    comment.append(\'Three classes predict prior\')\n    Pred.append(np.array([[0.75, 0.25, 0.], [0.75, 0.25, 0.], [0.75, 0.25, 0.], [0.75, 0.25, 0.]]))\n    Sol.append(sol2)\n\n    for k in range(len(Sol)):\n        sol = Sol[k]\n        pred = Pred[k]\n        print(\'****** ({}) {} ******\'.format(k, comment[k]))\n        show_all_scores(sol, pred)\n\n    print(\'\\n\\nMulti-label score verification: 1) all identical labels\')\n    print(\'\\n\\n=======================================================\')\n    print(\'\\nIt is normal that for more then 2 labels the results are different for the multiclass scores.\')\n    print(\'\\nBut they should be indetical for the multilabel scores.\')\n    num = 2\n\n    sol = np.array([[1, 1, 1], [0, 0, 0], [0, 0, 0], [0, 0, 0]])\n    sol3 = sol[:, 0:num]\n    if num == 1:\n        sol3 = np.array([sol3[:, 0]]).transpose()\n\n    comment = [\'{} labels perfect\'.format(num)]\n    Pred = [sol3]\n    Sol = [sol3]\n\n    comment.append(\'All wrong, in the multi-label sense\')\n    Pred.append(1 - sol3)\n    Sol.append(sol3)\n\n    comment.append(\'All equi proba: 0.5\')\n    sol = np.array([[0.5, 0.5, 0.5], [0.5, 0.5, 0.5], [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]])\n    if num == 1:\n        Pred.append(np.array([sol[:, 0]]).transpose())\n    else:\n        Pred.append(sol[:, 0:num])\n    Sol.append(sol3)\n\n    comment.append(\'All equi proba, prior: 0.25\')\n    sol = np.array([[0.25, 0.25, 0.25], [0.25, 0.25, 0.25], [0.25, 0.25, 0.25], [0.25, 0.25, 0.25]])\n    if num == 1:\n        Pred.append(np.array([sol[:, 0]]).transpose())\n    else:\n        Pred.append(sol[:, 0:num])\n    Sol.append(sol3)\n\n    comment.append(\'Some proba\')\n    sol = np.array([[0.2, 0.2, 0.2], [0.8, 0.8, 0.8], [0.9, 0.9, 0.9], [0.7, 0.7, 0.7]])\n    if num == 1:\n        Pred.append(np.array([sol[:, 0]]).transpose())\n    else:\n        Pred.append(sol[:, 0:num])\n    Sol.append(sol3)\n\n    comment.append(\'Invert both solution and prediction\')\n    if num == 1:\n        Pred.append(np.array([sol[:, 0]]).transpose())\n    else:\n        Pred.append(sol[:, 0:num])\n    Sol.append(1 - sol3)\n\n    for k in range(len(Sol)):\n        sol = Sol[k]\n        pred = Pred[k]\n        print(\'****** ({}) {} ******\'.format(k, comment[k]))\n        show_all_scores(sol, pred)\n\n    print(\'\\n\\nMulti-label score verification:\')\n    print(\'\\n\\n==========================\')\n\n    sol4 = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1]])\n\n    comment = [\'Three labels perfect\']\n    Pred = [sol4]\n    Sol = [sol4]\n\n    comment.append(\'Three classes all wrong, in the multi-label sense\')\n    Pred.append(1 - sol4)\n    Sol.append(sol4)\n\n    comment.append(\'Three classes equi proba\')\n    Pred.append(np.array([[1 / 3, 1 / 3, 1 / 3], [1 / 3, 1 / 3, 1 / 3], [1 / 3, 1 / 3, 1 / 3], [1 / 3, 1 / 3, 1 / 3]]))\n    Sol.append(sol4)\n\n    comment.append(\'Three classes some proba that do not add up\')\n    Pred.append(np.array([[0.2, 0, 0.5], [0.8, 0.4, 0.1], [0.9, 0.1, 0.2], [0.7, 0.3, 0.3]]))\n    Sol.append(sol4)\n\n    comment.append(\'Three classes predict prior\')\n    Pred.append(np.array([[0.25, 0.25, 0.5], [0.25, 0.25, 0.5], [0.25, 0.25, 0.5], [0.25, 0.25, 0.5]]))\n    Sol.append(sol4)\n\n    for k in range(len(Sol)):\n        sol = Sol[k]\n        pred = Pred[k]\n        print(\'****** ({}) {} ******\'.format(k, comment[k]))\n        show_all_scores(sol, pred)\n'"
AutoDL_scoring_program/score.py,0,"b'################################################################################\r\n# Name:         Scoring Program\r\n# Author:       Zhengying Liu, Isabelle Guyon, Adrien Pavao, Zhen Xu\r\n# Update time:  13 Aug 2019\r\n# Usage: \t\tpython score.py --solution_dir=<solution_dir> --prediction_dir=<prediction_dir> --score_dir=<score_dir>\r\n#           solution_dir contains  e.g. adult.solution\r\n#           prediction_dir should contain e.g. start.txt, adult.predict_0, adult.predict_1,..., end.txt.\r\n#           score_dir should contain scores.txt, detailed_results.html\r\n\r\nVERSION = \'v20191204\'\r\nDESCRIPTION =\\\r\n""""""This is the scoring program for AutoDL challenge. It takes the predictions\r\nmade by ingestion program as input and compare to the solution file and produce\r\na learning curve.\r\nPrevious updates:\r\n20191204: [ZY] Set scoring waiting time to 30min (1800s)\r\n20190908: [ZY] Add algebraic operations of learning curves\r\n20190823: [ZY] Fix the ALC in learning curve: use auc_step instead of auc\r\n20190820: [ZY] Minor fix: change wait time (for ingestion) from 30s to 90s\r\n20190709: [ZY] Resolve all issues; rearrange some logging messages;\r\n               simplify main function; fix exit_code of run_local_test.py;\r\n20190708: [ZY] Write the class Evaluator for object-oriented scoring program\r\n20190519: [ZY] Use the correct function for computing AUC of step functions\r\n20190516: [ZY] Change time budget to 20 minutes.\r\n20190508: [ZY] Decompose drawing learning curve functions;\r\n               Remove redirect output feature;\r\n               Use AUC instead of BAC;\r\n               Ignore columns with only one class when computing AUC;\r\n               Use step function instead of trapezoidal rule;\r\n               Change t0=300 from t0=1 in time transformation:\r\n                 log(1 + t/t0) / log(1 + T/t0)\r\n               Add second x-axis for the real time in seconds\r\n20190505: [ZY] Use argparse to parse directories AND time budget;\r\n               Fix num_preds not updated error.\r\n20190504: [ZY] Don\'t raise Exception anymore (for most cases) in order to\r\n               always have \'Finished\' for each submission;\r\n               Kill ingestion when time limit is exceeded;\r\n               Use the last modified time of the file \'start.txt\' written by\r\n               ingestion as the start time (`ingestion_start`);\r\n               Use module-specific logger instead of logging (with root logger);\r\n               Use the function update_score_and_learning_curve;\r\n20190429: [ZY] Remove useless code block such as the function is_started;\r\n               Better code layout.\r\n20190426.4: [ZY] Fix yaml format in scores.txt (add missing spaces)\r\n20190426.3: [ZY] Use f.write instead of yaml.dump to write scores.txt\r\n20190426.2: [ZY] Add logging info when writing scores and learning curves.\r\n20190426: [ZY] Now write to scores.txt whenever a new prediction is made. This\r\n               way, participants can still get a score when they exceed time\r\n               limit (but the submission\'s status will be marked as \'Failed\').\r\n20190425: [ZY] Add ScoringError and IngestionError: throw error in these cases.\r\n               Participants will get \'Failed\' for their error. But a score will\r\n               still by computed if applicable.\r\n               Improve importing order.\r\n               Log CPU usage.\r\n20190424: [ZY] Use logging instead of logger; remove start.txt checking.\r\n20190424: [ZY] Add version and description.\r\n20190419: [ZY] Judge if ingestion is alive by duration.txt; use logger.""""""\r\n\r\n# Scoring program for the AutoDL challenge\r\n# Isabelle Guyon and Zhengying Liu, ChaLearn, April 2018-\r\n\r\n# ALL INFORMATION, SOFTWARE, DOCUMENTATION, AND DATA ARE PROVIDED ""AS-IS"".\r\n# ISABELLE GUYON, CHALEARN, AND/OR OTHER ORGANIZERS OR CODE AUTHORS DISCLAIM\r\n# ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\r\n# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR ANY PARTICULAR PURPOSE, AND THE\r\n# WARRANTY OF NON-INFRINGEMENT OF ANY THIRD PARTY\'S INTELLECTUAL PROPERTY RIGHTS.\r\n# IN NO EVENT SHALL ISABELLE GUYON AND/OR OTHER ORGANIZERS BE LIABLE FOR ANY SPECIAL,\r\n# INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER ARISING OUT OF OR IN\r\n# CONNECTION WITH THE USE OR PERFORMANCE OF SOFTWARE, DOCUMENTS, MATERIALS,\r\n# PUBLICATIONS, OR INFORMATION MADE AVAILABLE FOR THE CHALLENGE.\r\n################################################################################\r\n\r\n\r\n################################################################################\r\n# User defined constants\r\n################################################################################\r\n\r\n# Verbosity level of logging.\r\n# Can be: NOTSET, DEBUG, INFO, WARNING, ERROR, CRITICAL\r\nverbosity_level = \'INFO\'\r\n\r\nfrom libscores import read_array, sp, ls, mvmean, tiedrank, _HERE, get_logger\r\nfrom os.path import join\r\nfrom sys import argv\r\nfrom sklearn.metrics import auc\r\nimport argparse\r\nimport base64\r\nimport datetime\r\nimport matplotlib; matplotlib.use(\'Agg\') # Solve the Tkinter display issue\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport os\r\nimport psutil\r\nimport sys\r\nimport time\r\nimport yaml\r\nfrom random import randrange\r\n\r\nlogger = get_logger(verbosity_level)\r\n\r\n################################################################################\r\n# Functions\r\n################################################################################\r\n\r\n# Metric used to compute the score of a point on the learning curve\r\ndef autodl_auc(solution, prediction, valid_columns_only=True):\r\n  """"""Compute normarlized Area under ROC curve (AUC).\r\n  Return Gini index = 2*AUC-1 for  binary classification problems.\r\n  Should work for a vector of binary 0/1 (or -1/1)""solution"" and any discriminant values\r\n  for the predictions. If solution and prediction are not vectors, the AUC\r\n  of the columns of the matrices are computed and averaged (with no weight).\r\n  The same for all classification problems (in fact it treats well only the\r\n  binary and multilabel classification problems). When `valid_columns` is not\r\n  `None`, only use a subset of columns for computing the score.\r\n  """"""\r\n  if valid_columns_only:\r\n    valid_columns = get_valid_columns(solution)\r\n    if len(valid_columns) < solution.shape[-1]:\r\n      logger.info(""Some columns in solution have only one class, "" +\r\n                     ""ignoring these columns for evaluation."")\r\n    solution = solution[:, valid_columns].copy()\r\n    prediction = prediction[:, valid_columns].copy()\r\n  label_num = solution.shape[1]\r\n  auc = np.empty(label_num)\r\n  for k in range(label_num):\r\n    r_ = tiedrank(prediction[:, k])\r\n    s_ = solution[:, k]\r\n    if sum(s_) == 0: print(""WARNING: no positive class example in class {}""\\\r\n                           .format(k + 1))\r\n    npos = sum(s_ == 1)\r\n    nneg = sum(s_ < 1)\r\n    auc[k] = (sum(r_[s_ == 1]) - npos * (npos + 1) / 2) / (nneg * npos)\r\n  return 2 * mvmean(auc) - 1\r\n\r\ndef accuracy(solution, prediction):\r\n  """"""Get accuracy of \'prediction\' w.r.t true labels \'solution\'.""""""\r\n  epsilon = 1e-15\r\n  # normalize prediction\r\n  prediction_normalized =\\\r\n    prediction / (np.sum(np.abs(prediction), axis=1, keepdims=True) + epsilon)\r\n  return np.sum(solution * prediction_normalized) / solution.shape[0]\r\n\r\nscoring_functions = {\'nauc\': autodl_auc,\r\n                     \'accuracy\': accuracy\r\n                     }\r\n\r\ndef get_valid_columns(solution):\r\n  """"""Get a list of column indices for which the column has more than one class.\r\n  This is necessary when computing BAC or AUC which involves true positive and\r\n  true negative in the denominator. When some class is missing, these scores\r\n  don\'t make sense (or you have to add an epsilon to remedy the situation).\r\n\r\n  Args:\r\n    solution: array, a matrix of binary entries, of shape\r\n      (num_examples, num_features)\r\n  Returns:\r\n    valid_columns: a list of indices for which the column has more than one\r\n      class.\r\n  """"""\r\n  num_examples = solution.shape[0]\r\n  col_sum = np.sum(solution, axis=0)\r\n  valid_columns = np.where(1 - np.isclose(col_sum, 0) -\r\n                               np.isclose(col_sum, num_examples))[0]\r\n  return valid_columns\r\n\r\ndef is_one_hot_vector(x, axis=None, keepdims=False):\r\n  """"""Check if a vector \'x\' is one-hot (i.e. one entry is 1 and others 0).""""""\r\n  norm_1 = np.linalg.norm(x, ord=1, axis=axis, keepdims=keepdims)\r\n  norm_inf = np.linalg.norm(x, ord=np.inf, axis=axis, keepdims=keepdims)\r\n  return np.logical_and(norm_1 == 1, norm_inf == 1)\r\n\r\ndef is_multiclass(solution):\r\n  """"""Return if a task is a multi-class classification task, i.e.  each example\r\n  only has one label and thus each binary vector in `solution` only has\r\n  one \'1\' and all the rest components are \'0\'.\r\n\r\n  This function is useful when we want to compute metrics (e.g. accuracy) that\r\n  are only applicable for multi-class task (and not for multi-label task).\r\n\r\n  Args:\r\n    solution: a numpy.ndarray object of shape [num_examples, num_classes].\r\n  """"""\r\n  return all(is_one_hot_vector(solution, axis=1))\r\n\r\ndef get_fig_name(task_name):\r\n  """"""Helper function for getting learning curve figure name.""""""\r\n  fig_name = ""learning-curve-"" + task_name + "".png""\r\n  return fig_name\r\n\r\ndef get_solution(solution_dir):\r\n  """"""Get the solution array from solution directory.""""""\r\n  solution_names = sorted(ls(os.path.join(solution_dir, \'*.solution\')))\r\n  if len(solution_names) != 1: # Assert only one file is found\r\n    logger.warning(""{} solution files found: {}! ""\\\r\n                   .format(len(solution_names), solution_names) +\r\n                   ""Return `None` as solution."")\r\n    return None\r\n  solution_file = solution_names[0]\r\n  solution = read_array(solution_file)\r\n  return solution\r\n\r\ndef get_task_name(solution_dir):\r\n  """"""Get the task name from solution directory.""""""\r\n  solution_names = sorted(ls(os.path.join(solution_dir, \'*.solution\')))\r\n  if len(solution_names) != 1: # Assert only one file is found\r\n    logger.warning(""{} solution files found: {}! ""\\\r\n                   .format(len(solution_names), solution_names) +\r\n                   ""Return `None` as task name."")\r\n    return None\r\n  solution_file = solution_names[0]\r\n  task_name = solution_file.split(os.sep)[-1].split(\'.\')[0]\r\n  return task_name\r\n\r\ndef transform_time(t, T, t0=None):\r\n  if t0 is None:\r\n    t0 = T\r\n  return np.log(1 + t / t0) / np.log(1 + T / t0)\r\n\r\ndef auc_step(X, Y):\r\n  """"""Compute area under curve using step function (in \'post\' mode).""""""\r\n  if len(X) != len(Y):\r\n    raise ValueError(""The length of X and Y should be equal but got "" +\r\n                     ""{} and {} !"".format(len(X), len(Y)))\r\n  area = 0\r\n  for i in range(len(X) - 1):\r\n    delta_X = X[i + 1] - X[i]\r\n    area += delta_X * Y[i]\r\n  return area\r\n\r\ndef plot_learning_curve(timestamps, scores,\r\n                        start_time=0, time_budget=7200, method=\'step\',\r\n                        transform=None, task_name=None,\r\n                        area_color=\'cyan\', fill_area=True, model_name=None,\r\n                        clear_figure=True, fig=None, show_final_score=True,\r\n                        show_title=True,\r\n                        **kwargs):\r\n  """"""Plot learning curve using scores and corresponding timestamps.\r\n\r\n  Args:\r\n    timestamps: iterable of float, each element is the timestamp of\r\n      corresponding performance. These timestamps should be INCREASING.\r\n    scores: iterable of float, scores at each timestamp\r\n    start_time: float, the start time, should be smaller than any timestamp\r\n    time_budget: float, the time budget, should be larger than any timestamp\r\n    method: string, can be one of [\'step\', \'trapez\']\r\n    transform: callable that transform [0, time_budget] into [0, 1]. If `None`,\r\n      use the default transformation\r\n          lambda t: np.log2(1 + t / time_budget)\r\n    task_name: string, name of the task\r\n    area_color: matplotlib color, color of the area under learning curve\r\n    fill_area: boolean, fill the area under the curve or not\r\n    model_name: string, name of the model (learning algorithm).\r\n    clear_figure: boolean, clear previous figures or not\r\n    fig: the figure to plot on\r\n    show_final_score: boolean, show the last score or not\r\n    show_title: boolean, show the plot title or not\r\n    kwargs: Line2D properties, optional\r\n  Returns:\r\n    alc: float, the area under learning curve.\r\n    fig: the figure with learning curve\r\n  Raises:\r\n    ValueError: if the length of `timestamps` and `scores` are not equal,\r\n      or if `timestamps` is not increasing, or if certain timestamp is not in\r\n      the interval [start_time, start_time + time_budget], or if `method` has\r\n      bad values.\r\n  """"""\r\n  le = len(timestamps)\r\n  if not le == len(scores):\r\n    raise ValueError(""The number of timestamps {} "".format(le) +\r\n                     ""should be equal to the number of "" +\r\n                     ""scores {}!"".format(len(scores)))\r\n  for i in range(le):\r\n    if i < le - 1 and not timestamps[i] <= timestamps[i + 1]:\r\n      raise ValueError(""The timestamps should be increasing! But got "" +\r\n                       ""[{}, {}] "".format(timestamps[i], timestamps[i + 1]) +\r\n                       ""at index [{}, {}]."".format(i, i + 1))\r\n    if timestamps[i] < start_time:\r\n      raise ValueError(""The timestamp {} at index {}"".format(timestamps[i], i) +\r\n                       "" is earlier than start time {}!"".format(start_time))\r\n  timestamps = [t for t in timestamps if t <= time_budget + start_time]\r\n  if len(timestamps) < le:\r\n    logger.warning(""Some predictions are made after the time budget! "" +\r\n                   ""Ignoring all predictions from the index {}.""\\\r\n                   .format(len(timestamps)))\r\n    scores = scores[:len(timestamps)]\r\n  if transform is None:\r\n    t0 = 60\r\n    # default transformation\r\n    transform = lambda t: transform_time(t, time_budget, t0=t0)\r\n    xlabel = ""Transformed time: "" +\\\r\n             r\'$\\tilde{t} = \\frac{\\log (1 + t / t_0)}{ \\log (1 + T / t_0)}$ \' +\\\r\n             \' ($T = \' + str(int(time_budget)) + \'$, \' +\\\r\n             \' $t_0 = \' + str(int(t0)) + \'$)\'\r\n  else:\r\n    xlabel = ""Transformed time: "" + r\'$\\tilde{t}$\'\r\n  relative_timestamps = [t - start_time for t in timestamps]\r\n  # Transform X\r\n  X = [transform(t) for t in relative_timestamps]\r\n  Y = list(scores.copy())\r\n  # Add origin as the first point of the curve\r\n  X.insert(0, 0)\r\n  Y.insert(0, 0)\r\n  # Draw learning curve\r\n  if clear_figure:\r\n    plt.clf()\r\n  if fig is None or len(fig.axes) == 0:\r\n    fig = plt.figure(figsize=(7, 7.07))\r\n    ax = fig.add_subplot(111)\r\n    if show_title:\r\n      plt.title(""Learning curve for task: {}"".format(task_name), y=1.06)\r\n    ax.set_xlabel(xlabel)\r\n    ax.set_xlim(left=0, right=1)\r\n    ax.set_ylabel(\'score (2 * AUC - 1)\')\r\n    ax.set_ylim(bottom=-0.01, top=1)\r\n    ax.grid(True, zorder=5)\r\n    # Show real time in seconds in a second x-axis\r\n    ax2 = ax.twiny()\r\n    ticks = [10, 60, 300, 600, 1200] +\\\r\n            list(range(1800, int(time_budget) + 1, 1800))\r\n    ax2.set_xticks([transform(t) for t in ticks])\r\n    ax2.set_xticklabels(ticks)\r\n  ax = fig.axes[0]\r\n  if method == \'step\':\r\n    drawstyle = \'steps-post\'\r\n    step = \'post\'\r\n    auc_func = auc_step\r\n  elif method == \'trapez\':\r\n    drawstyle = \'default\'\r\n    step = None\r\n    auc_func = auc\r\n  else:\r\n    raise ValueError(""The `method` variable should be one of "" +\r\n                     ""[\'step\', \'trapez\']!"")\r\n  # Add a point on the final line using last prediction\r\n  X.append(1)\r\n  Y.append(Y[-1])\r\n  # Compute AUC using step function rule or trapezoidal rule\r\n  alc = auc_func(X, Y)\r\n  if model_name:\r\n    label = ""{}: ALC={:.4f}"".format(model_name, alc)\r\n  else:\r\n    label = ""ALC={:.4f}"".format(alc)\r\n  # Plot the major part of the figure: the curve\r\n  if \'marker\' not in kwargs:\r\n    kwargs[\'marker\'] = \'o\'\r\n  if \'markersize\' not in kwargs:\r\n    kwargs[\'markersize\'] = 3\r\n  if \'label\' not in kwargs:\r\n    kwargs[\'label\'] = label\r\n  ax.plot(X[:-1], Y[:-1], drawstyle=drawstyle, **kwargs)\r\n  # Fill area under the curve\r\n  if fill_area:\r\n    ax.fill_between(X, Y, color=\'cyan\', step=step)\r\n  # Show the latest/final score\r\n  if show_final_score:\r\n    ax.text(X[-1], Y[-1], ""{:.4f}"".format(Y[-1]))\r\n  # Draw a dotted line from last prediction\r\n  kwargs[\'linestyle\'] = \'--\'\r\n  kwargs[\'linewidth\'] = 1\r\n  kwargs[\'marker\'] = None\r\n  kwargs.pop(\'label\', None)\r\n  ax.plot(X[-2:], Y[-2:], **kwargs)\r\n  ax.legend()\r\n  return alc, fig\r\n\r\ndef get_ingestion_info(prediction_dir):\r\n  """"""Get info on ingestion program: PID, start time, etc. from \'start.txt\'.\r\n\r\n  Args:\r\n    prediction_dir: a string, directory containing predictions (output of\r\n      ingestion)\r\n  Returns:\r\n    A dictionary with keys \'ingestion_pid\' and \'start_time\' if the file\r\n      \'start.txt\' exists. Otherwise return `None`.\r\n  """"""\r\n  start_filepath = os.path.join(prediction_dir, \'start.txt\')\r\n  if os.path.exists(start_filepath):\r\n    with open(start_filepath, \'r\') as f:\r\n      ingestion_info = yaml.safe_load(f)\r\n    return ingestion_info\r\n  else:\r\n    return None\r\n\r\ndef get_timestamps(prediction_dir):\r\n  """"""Read predictions\' timestamps stored in \'start.txt\'.\r\n\r\n  The \'start.txt\' file should be similar to\r\n    ingestion_pid: 31315\r\n    start_time: 1557269921.7939095\r\n    0: 1557269953.5586617\r\n    1: 1557269956.012751\r\n    2: 1557269958.3\r\n  We see there are 3 predictions. Then this function will return\r\n    start_time, timestamps =\r\n      1557269921.7939095, [1557269953.5586617, 1557269956.012751, 1557269958.3]\r\n  """"""\r\n  start_filepath = os.path.join(prediction_dir, \'start.txt\')\r\n  if os.path.exists(start_filepath):\r\n    with open(start_filepath, \'r\') as f:\r\n      ingestion_info = yaml.safe_load(f)\r\n    start_time = ingestion_info[\'start_time\']\r\n    timestamps = []\r\n    idx = 0\r\n    while idx in ingestion_info:\r\n      timestamps.append(ingestion_info[idx])\r\n      idx += 1\r\n    return start_time, timestamps\r\n  else:\r\n    logger.warning(""No \'start.txt\' file found in the prediction directory "" +\r\n                   ""{}. Return `None` as timestamps."")\r\n    return None\r\n\r\ndef get_scores(scoring_function, solution, predictions):\r\n  """"""Compute a list of scores for a list of predictions.\r\n\r\n  Args:\r\n    scoring_function: callable with signature\r\n      scoring_function(solution, predictions)\r\n    solution: Numpy array, the solution (true labels).\r\n    predictions: list of array, predictions.\r\n  Returns:\r\n    a list of float, scores\r\n  """"""\r\n  scores = [scoring_function(solution, pred) for pred in predictions]\r\n  return scores\r\n\r\ndef compute_scores_bootstrap(scoring_function, solution, prediction, n=10):\r\n    """"""Compute a list of scores using bootstrap.\r\n\r\n       Args:\r\n         scoring function: scoring metric taking y_true and y_pred\r\n         solution: ground truth vector\r\n         prediction: proposed solution\r\n         n: number of scores to compute\r\n    """"""\r\n    scores = []\r\n    l = len(solution)\r\n    for _ in range(n): # number of scoring\r\n      size = solution.shape[0]\r\n      idx = np.random.randint(0, size, size) # bootstrap index\r\n      scores.append(scoring_function(solution[idx], prediction[idx]))\r\n    return scores\r\n\r\ndef end_file_generated(prediction_dir):\r\n  """"""Check if ingestion is still alive by checking if the file \'end.txt\'\r\n  is generated in the folder of predictions.\r\n  """"""\r\n  end_filepath =  os.path.join(prediction_dir, \'end.txt\')\r\n  logger.debug(""CPU usage: {}%"".format(psutil.cpu_percent()))\r\n  logger.debug(""Virtual memory: {}"".format(psutil.virtual_memory()))\r\n  return os.path.isfile(end_filepath)\r\n\r\ndef is_process_alive(pid):\r\n  """"""Check if a process is alive according to its PID.""""""\r\n  try:\r\n    os.kill(pid, 0)\r\n  except OSError:\r\n    return False\r\n  else:\r\n    return True\r\n\r\ndef terminate_process(pid):\r\n  """"""Kill a process according to its PID.""""""\r\n  process = psutil.Process(pid)\r\n  process.terminate()\r\n  logger.debug(""Terminated process with pid={} in scoring."".format(pid))\r\n\r\nclass IngestionError(Exception):\r\n  pass\r\n\r\nclass ScoringError(Exception):\r\n  pass\r\n\r\nclass LearningCurve(object):\r\n  """"""Learning curve object for AutoDL challenges. Contains at least an\r\n  increasing list of float as timestamps and another list of the same length\r\n  of the corresponding score at each timestamp.\r\n  """"""\r\n\r\n  def __init__(self, timestamps=None, scores=None, time_budget=1200,\r\n               score_name=None, task_name=None,\r\n               participant_name=None, algorithm_name=None, subset=\'test\'):\r\n    """"""\r\n    Args:\r\n      timestamps: list of float, should be increasing\r\n      scores: list of float, should have the same length as `timestamps`\r\n      time_budget: float, the time budget (for ingestion) of the task\r\n      score_name: string, can be \'nauc\' or \'accuracy\' (if is multiclass task)\r\n      task_name: string, name of the task, optional\r\n      participant_name: string, name of the participant, optional\r\n      algorithm_name: string, name of the algorithm, optional\r\n    """"""\r\n    self.timestamps = timestamps or [] # relative timestamps\r\n    self.scores = scores or []\r\n    if len(self.timestamps) != len(self.scores):\r\n      raise ValueError(""The number of timestamps should be equal to "" +\r\n                       ""the number of scores, but got "" +\r\n                       ""{} and {}"".format(len(self.timestamps),\r\n                                          len(self.scores)))\r\n    self.time_budget = time_budget\r\n    self.score_name = score_name or \'nauc\'\r\n    self.task_name = task_name\r\n    self.participant_name = participant_name\r\n    self.algorithm_name = algorithm_name\r\n\r\n  def __repr__(self):\r\n    return ""Learning curve for: participant={}, task={}""\\\r\n           .format(self.participant_name, self.task_name)\r\n\r\n  def __add__(self, lc):\r\n    if not isinstance(lc, LearningCurve):\r\n      raise ValueError(""Can only add two learning curves but got {}.""\\\r\n                       .format(type(lc)))\r\n    if self.time_budget != lc.time_budget:\r\n      raise ValueError(""Cannot add two learning curves of different "" +\r\n                       ""time budget: {} and {}!""\\\r\n                       .format(self.time_budget, lc.time_budget))\r\n    else:\r\n      time_budget = self.time_budget\r\n    if self.score_name != lc.score_name:\r\n      raise ValueError(""Cannot add two learning curves of different "" +\r\n                       ""score names: {} and {}!""\\\r\n                       .format(self.score_name, lc.score_name))\r\n    else:\r\n      score_name = self.score_name\r\n    task_name = self.task_name if self.task_name == lc.task_name else None\r\n    participant_name = self.participant_name \\\r\n                       if self.participant_name == lc.participant_name else None\r\n    algorithm_name = self.algorithm_name \\\r\n                       if self.algorithm_name == lc.algorithm_name else None\r\n    # Begin merging scores and timestamps\r\n    new_timestamps = []\r\n    new_scores = []\r\n    # Indices of next point to add\r\n    i = 0\r\n    j = 0\r\n    while i < len(self.timestamps) or j < len(lc.timestamps):\r\n      # When two timestamps are close, append only one timestamp\r\n      if i < len(self.timestamps) and j < len(lc.timestamps) and \\\r\n        np.isclose(self.timestamps[i], lc.timestamps[j]):\r\n        new_timestamps.append(self.timestamps[i])\r\n        new_scores.append(self.scores[i] + lc.scores[j])\r\n        i += 1\r\n        j += 1\r\n        continue\r\n      # In some cases, use the timestamp/score of this learning curve\r\n      if j == len(lc.timestamps) or \\\r\n        (i < len(self.timestamps) and self.timestamps[i] < lc.timestamps[j]):\r\n        new_timestamps.append(self.timestamps[i])\r\n        other_score = 0 if j == 0 else lc.scores[j - 1]\r\n        new_scores.append(self.scores[i] + other_score)\r\n        i += 1\r\n      # In other cases, use the timestamp/score of the other learning curve\r\n      else:\r\n        new_timestamps.append(lc.timestamps[j])\r\n        this_score = 0 if i == 0 else self.scores[i - 1]\r\n        new_scores.append(this_score + lc.scores[j])\r\n        j += 1\r\n    new_lc = LearningCurve(timestamps=new_timestamps,\r\n                           scores=new_scores,\r\n                           time_budget=time_budget,\r\n                           score_name=score_name,\r\n                           task_name=task_name,\r\n                           participant_name=participant_name,\r\n                           algorithm_name=algorithm_name)\r\n    return new_lc\r\n\r\n  def __mul__(self, real_number):\r\n    if isinstance(real_number, int):\r\n      real_number = float(real_number)\r\n    if not isinstance(real_number, float):\r\n      raise ValueError(""Can only multiply a learning curve by a float but got"" +\r\n                       "" {}."".format(type(real_number)))\r\n    new_scores = [real_number * s for s in self.scores]\r\n    new_lc = LearningCurve(timestamps=self.timestamps,\r\n                           scores=new_scores,\r\n                           time_budget=self.time_budget,\r\n                           score_name=self.score_name,\r\n                           task_name=self.task_name,\r\n                           participant_name=self.participant_name,\r\n                           algorithm_name=self.algorithm_name)\r\n    return new_lc\r\n\r\n  def __neg__(self):\r\n    return self * (-1)\r\n\r\n  def __sub__(self, other):\r\n    return self + (-other)\r\n\r\n  def __truediv__(self, real_number):\r\n    return self * (1 / real_number)\r\n\r\n  def plot(self, method=\'step\', transform=None,\r\n           area_color=\'cyan\', fill_area=True, model_name=None,\r\n           fig=None, show_final_score=True, **kwargs):\r\n    """"""Plot the learning curve using `matplotlib.pyplot`.\r\n\r\n    method: string, can be \'step\' or \'trapez\'. Decides which drawstyle to use.\r\n        Also effects ALC (Area under Learning Curve)\r\n    transform: callable, for transforming time axis to [0,1] interval, mostly\r\n        optional\r\n    area_color: string or color code, decides the color of the area under curve,\r\n        optional\r\n    fill_area: boolean, whether fill the area under curve with color or not\r\n    model_name: string, if not `None`, will be shown on the legend\r\n    fig: matplotlib.figure.Figure, the figure to plot on. If `None` create a new\r\n        one\r\n    show_final_score: boolean, whether show final score on the figure. Useful\r\n        when overlapping curves\r\n    kwargs: Line2D properties, will be passed for plotting the curve\r\n        see https://matplotlib.org/api/_as_gen/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D\r\n    """"""\r\n    timestamps = self.timestamps\r\n    scores = self.scores\r\n    time_budget = self.time_budget\r\n    task_name = self.task_name\r\n    alc, fig = plot_learning_curve(timestamps, scores,\r\n                  start_time=0, time_budget=time_budget, method=method,\r\n                  transform=transform, task_name=task_name,\r\n                  area_color=area_color,\r\n                  fill_area=fill_area, model_name=model_name,\r\n                  clear_figure=False, fig=fig,\r\n                  show_final_score=show_final_score, **kwargs)\r\n    return alc, fig\r\n\r\n  def get_alc(self, t0=60, method=\'step\'):\r\n    X = [transform_time(t, T=self.time_budget, t0=t0)\r\n         for t in self.timestamps]\r\n    Y = list(self.scores.copy())\r\n    X.insert(0, 0)\r\n    Y.insert(0, 0)\r\n    X.append(1)\r\n    Y.append(Y[-1])\r\n    if method == \'step\':\r\n      auc_func = auc_step\r\n    elif method == \'trapez\':\r\n      auc_func = auc\r\n    alc = auc_func(X, Y)\r\n    return alc\r\n\r\n  def get_time_used(self):\r\n    if len(self.timestamps) > 0:\r\n      return self.timestamps[-1]\r\n    else:\r\n      return 0\r\n\r\n  def get_final_score(self):\r\n    if len(self.scores) > 0:\r\n      return self.scores[-1]\r\n    else:\r\n      return 0\r\n\r\n  def save_figure(self, output_dir):\r\n    alc, ax = self.plot()\r\n    fig_name = get_fig_name(self.task_name)\r\n    path_to_fig = os.path.join(output_dir, fig_name)\r\n    plt.savefig(path_to_fig)\r\n    plt.close()\r\n\r\nclass Evaluator(object):\r\n\r\n  def __init__(self, solution_dir=None, prediction_dir=None, score_dir=None,\r\n               scoring_functions=None, task_name=None, participant_name=None,\r\n               algorithm_name=None, submission_id=None):\r\n    """"""\r\n    Args:\r\n      scoring_functions: a dict containing (string, scoring_function) pairs\r\n    """"""\r\n    self.start_time = time.time()\r\n\r\n    self.solution_dir = solution_dir\r\n    self.prediction_dir = prediction_dir\r\n    self.score_dir = score_dir\r\n    self.scoring_functions = scoring_functions\r\n\r\n    self.task_name = task_name or get_task_name(solution_dir)\r\n    self.participant_name = participant_name\r\n    self.algorithm_name = algorithm_name\r\n    self.submission_id = submission_id\r\n\r\n    # State variables\r\n    self.scoring_success = None\r\n    self.time_limit_exceeded = None\r\n    self.prediction_files_so_far = []\r\n    self.new_prediction_files = []\r\n    self.scores_so_far = {\'nauc\':[]}\r\n    self.relative_timestamps = []\r\n\r\n    # Resolve info from directories\r\n    self.solution = self.get_solution()\r\n    # Check if the task is multilabel (i.e. with one hot label)\r\n    self.is_multiclass_task = is_multiclass(self.solution)\r\n\r\n    self.initialize_learning_curve_page()\r\n    self.fetch_ingestion_info()\r\n    self.learning_curve = self.get_learning_curve()\r\n\r\n  def get_solution(self):\r\n    """"""Get solution as NumPy array from `self.solution_dir`.""""""\r\n    solution = get_solution(self.solution_dir)\r\n    logger.debug(""Successfully loaded solution from solution_dir={}""\\\r\n                 .format(self.solution_dir))\r\n    return solution\r\n\r\n  def initialize_learning_curve_page(self):\r\n    """"""Initialize learning curve page with a message for waiting.""""""\r\n    # Create the output directory, if it does not already exist\r\n    if not os.path.isdir(self.score_dir):\r\n      os.mkdir(self.score_dir)\r\n    # Initialize detailed_results.html (learning curve page)\r\n    detailed_results_filepath = os.path.join(self.score_dir,\r\n                                             \'detailed_results.html\')\r\n    html_head = \'<html><head> <meta http-equiv=""refresh"" content=""5""> \' +\\\r\n                \'</head><body><pre>\'\r\n    html_end = \'</pre></body></html>\'\r\n    with open(detailed_results_filepath, \'a\') as html_file:\r\n      html_file.write(html_head)\r\n      html_file.write(""Starting training process... <br> Please be patient. "" +\r\n                      ""Learning curves will be generated when first "" +\r\n                      ""predictions are made."")\r\n      html_file.write(html_end)\r\n\r\n  def fetch_ingestion_info(self):\r\n    """"""Resolve some information from output of ingestion program. This includes\r\n    especially: `ingestion_start`, `ingestion_pid`, `time_budget`.\r\n\r\n    Raises:\r\n      IngestionError if no sign of ingestion starting detected after 1800\r\n      seconds.\r\n    """"""\r\n    logger.debug(""Fetching ingestion info..."")\r\n    prediction_dir = self.prediction_dir\r\n    # Wait 1800 seconds for ingestion to start and write \'start.txt\',\r\n    # Otherwise, raise an exception.\r\n    wait_time = 1800\r\n    ingestion_info = None\r\n    for i in range(wait_time):\r\n      ingestion_info = get_ingestion_info(prediction_dir)\r\n      if not ingestion_info is None:\r\n        logger.info(""Detected the start of ingestion after {} "".format(i) +\r\n                    ""seconds. Start scoring."")\r\n        break\r\n      time.sleep(1)\r\n    else:\r\n      raise IngestionError(""[-] Failed: scoring didn\'t detected the start of "" +\r\n                           ""ingestion after {} seconds."".format(wait_time))\r\n    # Get ingestion start time\r\n    ingestion_start = ingestion_info[\'start_time\']\r\n    # Get ingestion PID\r\n    ingestion_pid = ingestion_info[\'ingestion_pid\']\r\n    # Get time_budget for ingestion\r\n    assert \'time_budget\' in ingestion_info\r\n    time_budget = ingestion_info[\'time_budget\']\r\n    # Set attributes\r\n    self.ingestion_info = ingestion_info\r\n    self.ingestion_start = ingestion_start\r\n    self.ingestion_pid = ingestion_pid\r\n    self.time_budget = time_budget\r\n    logger.debug(""Ingestion start time: {}"".format(ingestion_start))\r\n    logger.debug(""Scoring start time: {}"".format(self.start_time))\r\n    logger.debug(""Ingestion info successfully fetched."")\r\n\r\n  def end_file_generated(self):\r\n    return end_file_generated(self.prediction_dir)\r\n\r\n  def ingestion_is_alive(self):\r\n    return is_process_alive(self.ingestion_pid)\r\n\r\n  def kill_ingestion(self):\r\n    terminate_process(self.ingestion_pid)\r\n    assert not self.ingestion_is_alive()\r\n\r\n  def prediction_filename_pattern(self):\r\n    return ""{}.predict_*"".format(self.task_name)\r\n\r\n  def get_new_prediction_files(self):\r\n    """"""Fetch new prediction file(name)s found in prediction directory and update\r\n    corresponding attributes.\r\n\r\n    Examples of prediction file name: mini.predict_0, mini.predict_1\r\n\r\n    Returns:\r\n      List of new prediction files found.\r\n    """"""\r\n    prediction_files = ls(os.path.join(self.prediction_dir,\r\n                                       self.prediction_filename_pattern()))\r\n    logger.debug(""Prediction files: {}"".format(prediction_files))\r\n    new_prediction_files = [p for p in prediction_files\r\n                            if p not in self.prediction_files_so_far]\r\n    order_key = lambda filename: int(filename.split(\'_\')[-1])\r\n    self.new_prediction_files = sorted(new_prediction_files, key=order_key)\r\n    return self.new_prediction_files\r\n\r\n  def compute_score_per_prediction(self):\r\n    """"""For new predictions found, compute their score using `self.solution`\r\n    and scoring functions in `self.scoring_functions`. Then concatenate\r\n    the list of new predictions to the list of resolved predictions so far.\r\n    """"""\r\n    for score_name in self.scoring_functions:\r\n      scoring_function = self.scoring_functions[score_name]\r\n      if score_name != \'accuracy\' or self.is_multiclass_task:\r\n        new_scores = [scoring_function(self.solution, read_array(pred))\r\n                      for pred in self.new_prediction_files]\r\n        if score_name in self.scores_so_far:\r\n          self.scores_so_far[score_name] += new_scores\r\n        else:\r\n          self.scores_so_far[score_name] = new_scores\r\n    # If new predictions are found, update state variables\r\n    if self.new_prediction_files:\r\n      self.prediction_files_so_far += self.new_prediction_files\r\n      num_preds = len(self.prediction_files_so_far)\r\n      self.relative_timestamps = self.get_relative_timestamps()[:num_preds]\r\n      self.learning_curve = self.get_learning_curve()\r\n      self.new_prediction_files = []\r\n\r\n  def get_relative_timestamps(self):\r\n    """"""Get a list of relative timestamps. The beginning has relative timestamp\r\n    zero.\r\n    """"""\r\n    ingestion_start, timestamps = get_timestamps(self.prediction_dir)\r\n    relative_timestamps = [t - ingestion_start for t in timestamps]\r\n    return relative_timestamps\r\n\r\n  def write_score(self):\r\n    """"""Write score and duration to score_dir/scores.txt""""""\r\n    score_dir = self.score_dir\r\n    score = self.learning_curve.get_alc()\r\n    duration = self.learning_curve.get_time_used()\r\n    score_filename = os.path.join(score_dir, \'scores.txt\')\r\n    score_info_dict = {\'score\': score, # ALC\r\n                       \'Duration\': duration,\r\n                       \'task_name\': self.task_name,\r\n                       \'timestamps\': self.relative_timestamps,\r\n                       \'nauc_scores\': self.scores_so_far[\'nauc\']\r\n                      }\r\n    if self.is_multiclass_task:\r\n      score_info_dict[\'accuracy\'] = self.scores_so_far[\'accuracy\']\r\n    with open(score_filename, \'w\') as f:\r\n      f.write(\'score: \' + str(score) + \'\\n\')\r\n      f.write(\'Duration: \' + str(duration) + \'\\n\')\r\n      f.write(\'timestamps: {}\\n\'.format(self.relative_timestamps))\r\n      f.write(\'nauc_scores: {}\\n\'.format(self.scores_so_far[\'nauc\']))\r\n      if self.is_multiclass_task:\r\n        f.write(\'accuracy: {}\\n\'.format(self.scores_so_far[\'accuracy\']))\r\n    logger.debug(""Wrote to score_filename={} with score={}, duration={}""\\\r\n                  .format(score_filename, score, duration))\r\n    return score_info_dict\r\n\r\n  def write_scores_html(self, auto_refresh=True, append=False):\r\n    score_dir = self.score_dir\r\n    filename = \'detailed_results.html\'\r\n    image_paths = sorted(ls(os.path.join(score_dir, \'*.png\')))\r\n    if auto_refresh:\r\n      html_head = \'<html><head> <meta http-equiv=""refresh"" content=""5""> \' +\\\r\n                  \'</head><body><pre>\'\r\n    else:\r\n      html_head = """"""<html><body><pre>""""""\r\n    html_end = \'</pre></body></html>\'\r\n    if append:\r\n      mode = \'a\'\r\n    else:\r\n      mode = \'w\'\r\n    filepath = os.path.join(score_dir, filename)\r\n    with open(filepath, mode) as html_file:\r\n        html_file.write(html_head)\r\n        for image_path in image_paths:\r\n          with open(image_path, ""rb"") as image_file:\r\n            encoded_string = base64.b64encode(image_file.read())\r\n            encoded_string = encoded_string.decode(\'utf-8\')\r\n            s = \'<img src=""data:image/png;charset=utf-8;base64,{}""/>\'\\\r\n                .format(encoded_string)\r\n            html_file.write(s + \'<br>\')\r\n        html_file.write(html_end)\r\n    logger.debug(""Wrote learning curve page to {}"".format(filepath))\r\n\r\n  def get_learning_curve(self, score_name=\'nauc\'):\r\n    timestamps = self.relative_timestamps\r\n    scores = self.scores_so_far[score_name]\r\n    return LearningCurve(timestamps=timestamps, scores=scores,\r\n                         time_budget=self.time_budget,\r\n                         score_name=score_name, task_name=self.task_name,\r\n                         participant_name=self.participant_name,\r\n                         algorithm_name=self.algorithm_name)\r\n\r\n  def draw_learning_curve(self, **kwargs):\r\n    """"""Draw learning curve for one task and save to `score_dir`.""""""\r\n    self.compute_score_per_prediction()\r\n    scores = self.scores_so_far[\'nauc\']\r\n    is_multiclass_task = self.is_multiclass_task\r\n    timestamps = self.get_relative_timestamps()\r\n    sorted_pairs = sorted(zip(timestamps, scores))\r\n    start = 0\r\n    time_used = -1\r\n    if len(timestamps) > 0:\r\n      time_used = sorted_pairs[-1][0] - start\r\n      latest_score = sorted_pairs[-1][1]\r\n      if is_multiclass_task:\r\n        accuracy_scores = self.scores_so_far[\'accuracy\']\r\n        sorted_pairs_acc = sorted(zip(timestamps, accuracy_scores))\r\n        latest_acc = sorted_pairs_acc[-1][1]\r\n    X = [t for t, _ in sorted_pairs]\r\n    Y = [s for _, s in sorted_pairs]\r\n    alc, fig = plot_learning_curve(X, Y, time_budget=self.time_budget,\r\n                            task_name=self.task_name, **kwargs)\r\n    fig_name = get_fig_name(self.task_name)\r\n    path_to_fig = os.path.join(self.score_dir, fig_name)\r\n    plt.savefig(path_to_fig)\r\n    plt.close()\r\n    return alc, time_used\r\n\r\n  def update_score_and_learning_curve(self):\r\n    self.draw_learning_curve()\r\n    # Update learning curve page (detailed_results.html)\r\n    self.write_scores_html()\r\n    # Write score\r\n    score = self.write_score()[\'score\']\r\n    return score\r\n\r\n  def compute_error_bars(self, n=10):\r\n    """"""Compute error bars on evaluation with bootstrap.\r\n\r\n    Args:\r\n        n: number of times to compute the score (more means more precision)\r\n    Returns:\r\n        (mean, std, var)\r\n    """"""\r\n    try:\r\n        scoring_function = self.scoring_functions[\'nauc\']\r\n        solution = self.solution\r\n        last_prediction = read_array(self.prediction_files_so_far[-1])\r\n        scores = compute_scores_bootstrap(scoring_function, solution, last_prediction, n=n)\r\n        return np.mean(scores), np.std(scores), np.var(scores)\r\n    except: # not able to compute error bars\r\n        return -1, -1, -1\r\n\r\n  def compute_alc_error_bars(self, n=10):\r\n      """""" Return mean, std and variance of ALC score with n runs.\r\n          n curves are created:\r\n              For each timestamp, the value of AUC is computed from boostraps of y_true and y_pred.\r\n              During one curve building, we keep the same boostrap index for each prediction timestamp.\r\n\r\n          Args:\r\n              n: number of times to compute the score (more means more precision)\r\n          Returns:\r\n              (mean, std, var)\r\n      """"""\r\n      try:\r\n          scoring_function = self.scoring_functions[\'nauc\']\r\n          solution = self.solution\r\n          alc_scores = []\r\n          for _ in range(n): # n learning curves to compute\r\n              scores = []\r\n              size = solution.shape[0]\r\n              idx = np.random.randint(0, size, size) # bootstrap index\r\n              for prediction_file in self.prediction_files_so_far:\r\n                  prediction = read_array(prediction_file)\r\n                  scores.append(scoring_function(solution[idx], prediction[idx]))\r\n              # create new learning curve\r\n              learning_curve = LearningCurve(timestamps=self.relative_timestamps, # self.learning_curve.timestamps,\r\n                                             scores=scores, # list of AUC scores\r\n                                             time_budget=self.time_budget)\r\n              alc_scores.append(learning_curve.get_alc())\r\n          return np.mean(alc_scores), np.std(alc_scores), np.var(alc_scores)\r\n      except: # not able to compute error bars\r\n          return -1, -1, -1\r\n\r\n  def score_new_predictions(self):\r\n    new_prediction_files = evaluator.get_new_prediction_files()\r\n    if len(new_prediction_files) > 0:\r\n      score = evaluator.update_score_and_learning_curve()\r\n      logger.info(""[+] New prediction found. Now number of predictions "" +\r\n                   ""made = {}""\\\r\n                   .format(len(evaluator.prediction_files_so_far)))\r\n      logger.info(""Current area under learning curve for {}: {:.4f}""\\\r\n                .format(evaluator.task_name, score))\r\n      logger.info(""(2 * AUC - 1) of the latest prediction is {:.4f}.""\\\r\n                .format(evaluator.scores_so_far[\'nauc\'][-1]))\r\n      if evaluator.is_multiclass_task:\r\n        logger.info(""Accuracy of the latest prediction is {:.4f}.""\\\r\n                  .format(evaluator.scores_so_far[\'accuracy\'][-1]))\r\n\r\n# =============================== MAIN ========================================\r\n\r\nif __name__ == ""__main__"":\r\n    logger.info(""=""*5 + "" Start scoring program. "" +\r\n                ""Version: {} "".format(VERSION) + ""=""*5)\r\n\r\n    # Default I/O directories:\r\n    root_dir = _HERE(os.pardir)\r\n    default_solution_dir = join(root_dir, ""AutoDL_sample_data"")\r\n    default_prediction_dir = join(root_dir, ""AutoDL_sample_result_submission"")\r\n    default_score_dir = join(root_dir, ""AutoDL_scoring_output"")\r\n\r\n    # Parse directories from input arguments\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\'--solution_dir\', type=str,\r\n                        default=default_solution_dir,\r\n                        help=""Directory storing the solution with true "" +\r\n                             ""labels, e.g. adult.solution."")\r\n    parser.add_argument(\'--prediction_dir\', type=str,\r\n                        default=default_prediction_dir,\r\n                        help=""Directory storing the predictions. It should"" +\r\n                             ""contain e.g. [start.txt, adult.predict_0, "" +\r\n                             ""adult.predict_1, ..., end.txt]."")\r\n    parser.add_argument(\'--score_dir\', type=str,\r\n                        default=default_score_dir,\r\n                        help=""Directory storing the scoring output "" +\r\n                             ""e.g. `scores.txt` and `detailed_results.html`."")\r\n    args = parser.parse_args()\r\n    logger.debug(""Parsed args are: "" + str(args))\r\n    logger.debug(""-"" * 50)\r\n    solution_dir = args.solution_dir\r\n    prediction_dir = args.prediction_dir\r\n    score_dir = args.score_dir\r\n\r\n    logger.debug(""Version: {}. Description: {}"".format(VERSION, DESCRIPTION))\r\n    logger.debug(""Using solution_dir: "" + str(solution_dir))\r\n    logger.debug(""Using prediction_dir: "" + str(prediction_dir))\r\n    logger.debug(""Using score_dir: "" + str(score_dir))\r\n\r\n    #################################################################\r\n    # Initialize an evaluator (scoring program) object\r\n    evaluator = Evaluator(solution_dir, prediction_dir, score_dir,\r\n                          scoring_functions=scoring_functions)\r\n    #################################################################\r\n\r\n    ingestion_start = evaluator.ingestion_start\r\n    time_budget = evaluator.time_budget\r\n\r\n    try:\r\n      while(time.time() < ingestion_start + time_budget):\r\n        if evaluator.end_file_generated():\r\n          logger.info(""Detected ingestion program had stopped running "" +\r\n                      ""because an \'end.txt\' file is written by ingestion. "" +\r\n                      ""Stop scoring now."")\r\n          evaluator.scoring_success = True\r\n          break\r\n        time.sleep(1)\r\n\r\n        ### Fetch new predictions, compute their scores and update variables ###\r\n        evaluator.score_new_predictions()\r\n        ########################################################################\r\n\r\n        logger.debug(""Prediction files so far: {}""\\\r\n                     .format(evaluator.prediction_files_so_far))\r\n      else: # When time budget is used up, kill ingestion\r\n        if evaluator.ingestion_is_alive():\r\n          evaluator.time_limit_exceeded = True\r\n          evaluator.kill_ingestion()\r\n          logger.info(""Detected time budget is used up. Killed ingestion and "" +\r\n                      ""terminating scoring..."")\r\n    except Exception as e:\r\n      evaluator.scoring_success = False\r\n      logger.error(""[-] Error occurred in scoring:\\n"" + str(e),\r\n                    exc_info=True)\r\n\r\n    evaluator.score_new_predictions()\r\n\r\n    logger.info(""Final area under learning curve for {}: {:.4f}""\\\r\n              .format(evaluator.task_name, evaluator.learning_curve.get_alc()))\r\n\r\n    # Write one last time the detailed results page without auto-refreshing\r\n    evaluator.write_scores_html(auto_refresh=False)\r\n\r\n    # Compute scoring error bars of last prediction\r\n    n = 10\r\n    logger.info(""Computing error bars with {} scorings..."".format(n))\r\n    mean, std, var = evaluator.compute_error_bars(n=n)\r\n    logger.info(""\\nLatest prediction NAUC:\\n* Mean: {}\\n* Standard deviation: {}\\n* Variance: {}"".format(mean, std, var))\r\n\r\n    # Compute ALC error bars\r\n    n = 5\r\n    logger.info(""Computing ALC error bars with {} curves..."".format(n))\r\n    mean, std, var = evaluator.compute_alc_error_bars(n=n)\r\n    logger.info(""\\nArea under Learning Curve:\\n* Mean: {}\\n* Standard deviation: {}\\n* Variance: {}"".format(mean, std, var))\r\n\r\n    scoring_start = evaluator.start_time\r\n    # Use \'end.txt\' file to detect if ingestion program ends\r\n    end_filepath =  os.path.join(prediction_dir, \'end.txt\')\r\n    if not evaluator.scoring_success is None and not evaluator.scoring_success:\r\n      logger.error(""[-] Some error occurred in scoring program. "" +\r\n                  ""Please see output/error log of Scoring Step."")\r\n    elif not os.path.isfile(end_filepath):\r\n      if evaluator.time_limit_exceeded:\r\n        logger.error(""[-] Ingestion program exceeded time budget. "" +\r\n                     ""Predictions made so far will be used for evaluation."")\r\n      else: # Less probable to fall in this case\r\n        if evaluator.ingestion_is_alive():\r\n          evaluator.kill_ingestion()\r\n        logger.error(""[-] No \'end.txt\' file is produced by ingestion. "" +\r\n                     ""Ingestion or scoring may have not terminated normally."")\r\n    else:\r\n      with open(end_filepath, \'r\') as f:\r\n        end_info_dict = yaml.safe_load(f)\r\n      ingestion_duration = end_info_dict[\'ingestion_duration\']\r\n\r\n      if end_info_dict[\'ingestion_success\'] == 0:\r\n        logger.error(""[-] Some error occurred in ingestion program. "" +\r\n                    ""Please see output/error log of Ingestion Step."")\r\n      else:\r\n        logger.info(""[+] Successfully finished scoring! "" +\r\n                  ""Scoring duration: {:.2f} sec. ""\\\r\n                  .format(time.time() - scoring_start) +\r\n                  ""Ingestion duration: {:.2f} sec. ""\\\r\n                  .format(ingestion_duration) +\r\n                  ""The score of your algorithm on the task \'{}\' is: {:.6f}.""\\\r\n                  .format(evaluator.task_name,\r\n                          evaluator.learning_curve.get_alc()))\r\n\r\n    logger.info(""[Scoring terminated]"")\r\n'"
AutoDL_sample_code_submission/Auto_Image/model.py,24,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport os\nimport threading\nimport random\n\nimport tensorflow as tf\nimport torch\nimport torchvision as tv\nimport numpy as np\n\nimport skeleton\nfrom architectures.resnet import ResNet9, ResNet18\nfrom skeleton.projects import LogicModel, get_logger\nfrom skeleton.projects.others import AUC, five_crop\n\ntorch.backends.cudnn.benchmark = True\nthreads = [\n    threading.Thread(target=lambda: torch.cuda.synchronize()),\n    threading.Thread(target=lambda: tf.Session())\n]\n[t.start() for t in threads]\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n# LOGGER = get_logger(__name__)\n\n\ndef set_random_seed_all(seed, deterministic=False):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    tf.random.set_random_seed(seed)\n    if deterministic:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n\nclass Model(LogicModel):\n    def __init__(self, metadata):\n        super(Model, self).__init__(metadata)\n        self.use_test_time_augmentation = False\n        self.update_transforms = False\n\n    def build(self):\n        base_dir = os.path.dirname(os.path.abspath(__file__))\n        in_channels = self.info['dataset']['shape'][-1]\n        num_class = self.info['dataset']['num_class']\n\n        [t.join() for t in threads]\n\n        self.device = torch.device('cuda', 0)\n        self.session = tf.Session()\n\n        Network = ResNet18\n        self.model = Network(in_channels, num_class)\n        self.model_pred = Network(in_channels, num_class).eval()\n        self.model_9 = ResNet9(in_channels, num_class)\n        self.model_9_pred = ResNet9(in_channels, num_class).eval()\n\n        if Network in [ResNet9, ResNet18]:\n            model_path = os.path.join(base_dir, 'models')\n\n            self.model.init(model_dir=model_path, gain=1.0)\n            self.model_9.init(model_dir=model_path, gain=1.0)\n        else:\n            self.model.init(gain=1.0)\n            self.model_9.init(gain=1.0)\n\n        self.model = self.model.to(device=self.device, non_blocking=True)  # .half()\n        self.model_pred = self.model_pred.to(device=self.device, non_blocking=True)  # .half()\n        self.model_9 = self.model_9.to(device=self.device, non_blocking=True)  # .half()\n        self.model_9_pred = self.model_9_pred.to(device=self.device, non_blocking=True)  # .half()\n        self.is_half = self.model._half\n\n\n    def update_model(self):\n        num_class = self.info['dataset']['num_class']\n\n        epsilon = min(0.1, max(0.001, 0.001 * pow(num_class / 10, 2)))\n        if self.is_multiclass():\n            self.model.loss_fn = torch.nn.BCEWithLogitsLoss(reduction='none')\n            self.tau = 8.0\n        else:\n            self.model.loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n            self.tau = 8.0\n        self.model_pred.loss_fn = self.model.loss_fn\n        self.init_opt()\n\n    def init_opt(self):\n        steps_per_epoch = self.hyper_params['dataset']['steps_per_epoch']\n        batch_size = self.hyper_params['dataset']['batch_size']\n\n        params = [p for p in self.model.parameters() if p.requires_grad]\n        params_fc = [p for n, p in self.model.named_parameters() if\n                     p.requires_grad and 'fc' == n[:2] or 'conv1d' == n[:6]]\n        params_not_fc = [p for n, p in self.model.named_parameters() if\n                         p.requires_grad and not ('fc' == n[:2] or 'conv1d' == n[:6])]\n        init_lr = self.hyper_params['optimizer']['lr']\n        warmup_multiplier = 2.0\n        lr_multiplier = max(0.5, batch_size / 32)\n        scheduler_lr = skeleton.optim.get_change_scale(\n            skeleton.optim.gradual_warm_up(\n                skeleton.optim.get_reduce_on_plateau_scheduler(\n                    init_lr * lr_multiplier / warmup_multiplier,\n                    patience=10, factor=.5, metric_name='train_loss'\n                ),\n                warm_up_epoch=5,\n                multiplier=warmup_multiplier\n            ),\n            init_scale=1.0\n        )\n        self.optimizer_fc = skeleton.optim.ScheduledOptimizer(\n            params_fc,\n            torch.optim.SGD,\n            steps_per_epoch=steps_per_epoch,\n            clip_grad_max_norm=None,\n            lr=scheduler_lr,\n            momentum=0.9,\n            weight_decay=0.00025,\n            nesterov=True\n        )\n        self.optimizer = skeleton.optim.ScheduledOptimizer(\n            params,\n            torch.optim.SGD,\n            steps_per_epoch=steps_per_epoch,\n            clip_grad_max_norm=None,\n            lr=scheduler_lr,\n            momentum=0.9,\n            weight_decay=0.00025,\n            nesterov=True\n        )\n\n    def adapt(self, remaining_time_budget=None):\n        epoch = self.info['loop']['epoch']\n        input_shape = self.hyper_params['dataset']['input']\n        height, width = input_shape[:2]\n        batch_size = self.hyper_params['dataset']['batch_size']\n\n        train_score = np.average([c['train']['score'] for c in self.checkpoints[-5:]])\n        valid_score = np.average([c['valid']['score'] for c in self.checkpoints[-5:]])\n\n        self.use_test_time_augmentation = self.info['loop']['test'] > 3\n\n        if self.hyper_params['conditions']['use_fast_auto_aug']:\n            self.hyper_params['conditions']['use_fast_auto_aug'] = valid_score < 0.995\n\n        if self.hyper_params['conditions']['use_fast_auto_aug'] and \\\n                (train_score > 0.995 or self.info['terminate']) and \\\n                remaining_time_budget > 120 and \\\n                valid_score > 0.01 and \\\n                self.dataloaders['valid'] is not None and \\\n                not self.update_transforms:\n            self.update_transforms = True\n            self.last_predict = None\n            self.info['terminate'] = True\n\n            original_valid_policy = self.dataloaders['valid'].dataset.transform.transforms\n            policy = skeleton.data.augmentations.autoaug_policy()\n\n            num_policy_search = 100\n            num_sub_policy = 3\n            num_select_policy = 5\n            searched_policy = []\n            for policy_search in range(num_policy_search):\n                selected_idx = np.random.choice(list(range(len(policy))), num_sub_policy)\n                selected_policy = [policy[i] for i in selected_idx]\n\n                self.dataloaders['valid'].dataset.transform.transforms = original_valid_policy + [\n                    lambda t: t.cpu().float() if isinstance(t, torch.Tensor) else torch.Tensor(t),\n                    tv.transforms.ToPILImage(),\n                    skeleton.data.augmentations.Augmentation(\n                        selected_policy\n                    ),\n                    tv.transforms.ToTensor(),\n                    lambda t: t.to(device=self.device)\n                ]\n\n                metrics = []\n                for policy_eval in range(num_sub_policy * 2):\n                    valid_dataloader = self.build_or_get_dataloader('valid', self.datasets['valid'],\n                                                                    self.datasets['num_valids'])\n                    valid_metrics = self.epoch_valid(self.info['loop']['epoch'], valid_dataloader, reduction='max')\n                    metrics.append(valid_metrics)\n                loss = np.max([m['loss'] for m in metrics])\n                score = np.max([m['score'] for m in metrics])\n\n                searched_policy.append({\n                    'loss': loss,\n                    'score': score,\n                    'policy': selected_policy\n                })\n\n            flatten = lambda l: [item for sublist in l for item in sublist]\n\n            searched_policy = [p for p in searched_policy if p['score'] > valid_score]\n\n            if len(searched_policy) > 0:\n                policy_sorted_index = np.argsort([p['score'] for p in searched_policy])[::-1][:num_select_policy]\n                policy = flatten([searched_policy[idx]['policy'] for idx in policy_sorted_index])\n                policy = skeleton.data.augmentations.remove_duplicates(policy)\n\n                original_train_policy = self.dataloaders['train'].dataset.transform.transforms\n                self.dataloaders['train'].dataset.transform.transforms += [\n                    lambda t: t.cpu().float() if isinstance(t, torch.Tensor) else torch.Tensor(t),\n                    tv.transforms.ToPILImage(),\n                    skeleton.data.augmentations.Augmentation(\n                        policy\n                    ),\n                    tv.transforms.ToTensor(),\n                    lambda t: t.to(device=self.device)\n                ]\n\n            self.dataloaders['valid'].dataset.transform.transforms = original_valid_policy\n            self.hyper_params['optimizer']['lr'] /= 2.0\n            self.init_opt()\n            self.hyper_params['conditions']['max_inner_loop_ratio'] *= 3\n            self.hyper_params['conditions']['threshold_valid_score_diff'] = 0.00001\n            self.hyper_params['conditions']['min_lr'] = 1e-8\n\n    def activation(self, logits):\n        if self.is_multiclass():\n            logits = torch.sigmoid(logits)\n            prediction = (logits > 0.5).to(logits.dtype)\n        else:\n            logits = torch.softmax(logits, dim=-1)\n            _, k = logits.max(-1)\n            prediction = torch.zeros(logits.shape, dtype=logits.dtype, device=logits.device).scatter_(-1, k.view(-1, 1),\n                                                                                                      1.0)\n        return logits, prediction\n\n    def epoch_train(self, epoch, train, model=None, optimizer=None):\n        model = model if model is not None else self.model\n        if epoch < 0:\n            optimizer = optimizer if optimizer is not None else self.optimizer_fc\n        else:\n            optimizer = optimizer if optimizer is not None else self.optimizer\n        model.train()\n        model.zero_grad()\n\n        num_steps = len(train)\n        metrics = []\n        if self.switch == True:\n            self.checkpoints = []\n            step = 0\n            for (examples, labels, original_labels) in (self.pre_data * 2):\n                logits, loss = model(examples, labels, tau=self.tau, reduction='avg')\n                loss = loss.sum()\n                loss.backward()\n\n                max_epoch = self.hyper_params['dataset']['max_epoch']\n                optimizer.update(maximum_epoch=max_epoch)\n                optimizer.step()\n                model.zero_grad()\n\n                logits, prediction = self.activation(logits.float())\n                auc = AUC(logits, original_labels.float())\n                score = auc\n                metrics.append({\n                    'loss': loss.detach().float().cpu(),\n                    'score': score,\n                })\n\n                step += 1\n            self.switch = False\n            del self.pre_data\n        else:\n            for step, (examples, labels) in enumerate(train):\n                if examples.shape[0] == 1:\n                    examples = examples[0]\n                    labels = labels[0]\n                original_labels = labels\n                if not self.is_multiclass():\n                    labels = labels.argmax(dim=-1)\n\n                skeleton.nn.MoveToHook.to((examples, labels), self.device, self.is_half)\n                logits, loss = model(examples, labels, tau=self.tau, reduction='avg')\n                loss = loss.sum()\n                loss.backward()\n\n                max_epoch = self.hyper_params['dataset']['max_epoch']\n                optimizer.update(maximum_epoch=max_epoch)\n                optimizer.step()\n                model.zero_grad()\n                if self.info['loop']['epoch'] < 2:\n                    self.pre_data.append((examples, labels, original_labels))\n\n                logits, prediction = self.activation(logits.float())\n                auc = AUC(logits, original_labels.float())\n                score = auc\n                metrics.append({\n                    'loss': loss.detach().float().cpu(),\n                    'score': score,\n                })\n\n\n        train_loss = np.average([m['loss'] for m in metrics])\n        train_score = np.average([m['score'] for m in metrics])\n        optimizer.update(train_loss=train_loss)\n\n        return {\n            'loss': train_loss,\n            'score': train_score,\n        }\n\n    def epoch_valid(self, epoch, valid, reduction='avg'):\n        test_time_augmentation = False\n        self.model.eval()\n        num_steps = len(valid)\n        metrics = []\n        tau = self.tau\n\n        with torch.no_grad():\n            for step, (examples, labels) in enumerate(valid):\n                original_labels = labels\n                if not self.is_multiclass():\n                    labels = labels.argmax(dim=-1)\n\n                batch_size = examples.size(0)\n\n                if self.use_test_time_augmentation and test_time_augmentation:\n                    examples = torch.cat([examples, torch.flip(examples, dims=[-1])], dim=0)\n                    labels = torch.cat([labels, labels], dim=0)\n\n                logits, loss = self.model(examples, labels, tau=tau, reduction=reduction)\n\n                if self.use_test_time_augmentation and test_time_augmentation:\n                    logits1, logits2 = torch.split(logits, batch_size, dim=0)\n                    logits = (logits1 + logits2) / 2.0\n\n                logits, prediction = self.activation(logits.float())\n                if reduction == 'avg':\n                    auc = AUC(logits, original_labels.float())\n                else:\n                    auc = max([AUC(logits[i:i + 16], original_labels[i:i + 16].float()) for i in\n                               range(int(len(logits)) // 16)])\n\n                score = auc\n                metrics.append({\n                    'loss': loss.detach().float().cpu(),\n                    'score': score,\n                })\n\n            if reduction == 'avg':\n                valid_loss = np.average([m['loss'] for m in metrics])\n                valid_score = np.average([m['score'] for m in metrics])\n            elif reduction in ['min', 'max']:\n                valid_loss = np.min([m['loss'] for m in metrics])\n                valid_score = np.max([m['score'] for m in metrics])\n            else:\n                raise Exception('not support reduction method: %s' % reduction)\n        self.optimizer.update(valid_loss=np.average(valid_loss))\n\n        return {\n            'loss': valid_loss,\n            'score': valid_score,\n        }\n\n    def skip_valid(self, epoch):\n        return {\n            'loss': 99.9,\n            'score': epoch * 1e-4,\n        }\n\n    def prediction(self, dataloader, model=None, test_time_augmentation=True, detach=True, num_step=None):\n        tau = self.tau\n        if model is None:\n            model = self.model_pred\n\n            best_idx = np.argmax(np.array([c['valid']['score'] for c in self.checkpoints]))\n            best_loss = self.checkpoints[best_idx]['valid']['loss']\n            best_score = self.checkpoints[best_idx]['valid']['score']\n\n            states = self.checkpoints[best_idx]['model']\n            model.load_state_dict(states)\n\n        num_step = len(dataloader) if num_step is None else num_step\n\n        model.eval()\n        with torch.no_grad():\n            predictions = []\n            for step, (examples, labels) in zip(range(num_step), dataloader):\n                batch_size = examples.size(0)\n                height = int(examples.size(2) * 3 / 4)\n                width = int(examples.size(3) * 3 / 4)\n                if self.use_test_time_augmentation and test_time_augmentation:\n                    examples1 = torch.cat([examples, torch.flip(examples, dims=[-1])], dim=0)\n                    logits_1 = model(examples1, tau=tau)\n                    logits1, logits2 = torch.split(logits_1, batch_size, dim=0)\n                    logits = (logits1 + logits2) / 2.0\n                else:\n                    logits = model(examples, tau=tau)\n                logits, prediction = self.activation(logits)\n\n                if detach:\n                    predictions.append(logits.detach().float().cpu().numpy())\n                else:\n                    predictions.append(logits)\n\n            if detach:\n                predictions = np.concatenate(predictions, axis=0).astype(np.float)\n            else:\n                predictions = torch.cat(predictions, dim=0)\n        return predictions\n"""
AutoDL_sample_code_submission/Auto_NLP/__init__.py,0,b''
AutoDL_sample_code_submission/Auto_Tabular/CONSTANT.py,0,"b'NUMERICAL_TYPE = ""num""\nNUMERICAL_PREFIX = ""n_""\n\nCATEGORY_TYPE = ""cat""\nCATEGORY_PREFIX = ""c_""\n\nTIME_TYPE = ""time""\nTIME_PREFIX = ""t_""\n\nMULTI_CAT_TYPE = ""multi-cat""\nMULTI_CAT_PREFIX = ""m_""\nMULTI_CAT_DELIMITER = "",""\n\nVECTOR_TYPE = \'vec\'\nVECTOR_PREFIX = \'v_\'\n\n\nBINARY_TYPE = ""binary""\nBINARY_PREFIX = \'b_\'\n\nMAIN_TABLE_NAME = ""main""\nMAIN_TABLE_TEST_NAME = ""main_test""\nTABLE_PREFIX = ""table_""\n\nLABEL = ""label""\n\ntype2prefix = {\n    NUMERICAL_TYPE:NUMERICAL_PREFIX,\n    CATEGORY_TYPE:CATEGORY_PREFIX,\n    TIME_TYPE:TIME_PREFIX,\n    MULTI_CAT_TYPE:MULTI_CAT_PREFIX,\n    BINARY_TYPE: BINARY_PREFIX\n}\n\nJOBS = 4\nSEED = 2020\n\nMISSING = -1\n\n'"
AutoDL_sample_code_submission/Auto_Tabular/__init__.py,0,b''
AutoDL_sample_code_submission/Auto_Tabular/data_space.py,0,"b""import numpy as np\nimport tensorflow as tf\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nimport gc\n\nfrom Auto_Tabular.utils.data_utils import ohe2cat\nfrom Auto_Tabular.utils.log_utils import log, timeit\nfrom Auto_Tabular.utils.data_utils import fill_na\nfrom Auto_Tabular.utils.sample import AutoSample\nfrom Auto_Tabular import CONSTANT\n\n\n\nclass TabularDataSpace:\n    def __init__(self, metadata, eda_info, main_df, y, lgb_info):\n        self.metadata = metadata\n\n        self.lgb_info = lgb_info\n\n        self.data = main_df\n        self.all_idxs = main_df.index\n        self.all_train_idxs = main_df.index[main_df.index >= 0]\n        self.test_idxs = main_df.index[main_df.index < 0]\n\n        self.y = y\n\n        self.cat_cols = eda_info['cat_cols']\n        self.num_cols = eda_info['num_cols']\n\n        self.col2type = {}\n\n        self.post_drop_set = set()\n\n        self.init_col2type()\n\n        self.splits = {}\n\n        self.train_valid_split_idxs()\n\n        self.update = False\n\n    @timeit\n    def train_valid_split_idxs(self, ratio=0.2):\n        sss = StratifiedShuffleSplit(n_splits=5, test_size=ratio, random_state=0)\n        idxs = np.arange(len(self.y))\n        i = 0\n        for train, val in sss.split(idxs, ohe2cat(self.y)):\n            self.splits[i] = (train, val)\n            i += 1\n\n        self.train_idxs, self.val_idxs = self.splits[0]\n\n        self.m = len(self.train_idxs)\n        self.auto_sample = AutoSample(self.y[self.train_idxs])\n\n    @timeit\n    def get_dataloader(self, train_loop_num, round_num, run_num, use_all_data, model_type):\n\n        self.train_idxs, self.val_idxs = self.splits[round_num-1]\n        print('round {}'.format(round_num))\n\n        data_loader = {}\n        do_sample_col = False\n\n        if use_all_data:\n            sample_idxs = self.all_train_idxs\n        else:\n            sample_idxs = self.train_idxs\n\n\n        cat_cols = self.get_categories(self.data)\n\n        if model_type == 'nn_keras':\n            feats = self.nn_process(self.data, cat_cols)\n            feats = pd.DataFrame(feats, index=self.all_idxs)\n            data_loader['X'], data_loader['y'] = feats, self.y\n            data_loader['shape'] = feats.shape[1]\n\n        elif model_type == 'emb_nn':\n            feats = pd.DataFrame(self.data, index=self.all_idxs)\n            self.label_encode(feats, cat_cols)\n            data_loader['X'], data_loader['y'] = feats, self.y\n            data_loader['shape'] = feats.shape[1]\n        elif model_type == 'tree':\n            feats = self.data\n            if do_sample_col:\n                data_loader['X'] = feats\n            else:\n                data_loader['X'] = feats\n            data_loader['y'] = self.y\n            data_loader['cat_cols'] = cat_cols\n        elif model_type == 'lr':\n            feats = self.nn_process(self.data, cat_cols)\n            feats = pd.DataFrame(feats, index=self.all_idxs)\n            data_loader['X'], data_loader['y'] = feats, self.y\n            data_loader['cat_cols'] = cat_cols\n\n        data_loader['all_train_idxs'], data_loader['train_idxs'], \\\n        data_loader['val_idxs'], data_loader['test_idxs'], \\\n        data_loader['splits'], data_loader['cat_cols']\\\n            = self.all_train_idxs, sample_idxs, self.val_idxs, self.test_idxs, self.splits, cat_cols\n\n        return data_loader\n\n    def to_tfdataset(self, feats, y=None, mode=None):\n        if mode == 'train':\n            return tf.data.Dataset.from_tensor_slices((feats, y))\n        elif mode == 'val':\n            return tf.data.Dataset.from_tensor_slices(feats), y\n        elif mode == 'test':\n            return tf.data.Dataset.from_tensor_slices(feats)\n\n    def drop_post_drop_column(self, df):\n        if len(self.post_drop_set) != 0:\n            drop_cols = list(self.post_drop_set)\n            df.drop(drop_cols, axis=1, inplace=True)\n            gc.collect()\n\n    def get_categories(self, df):\n        categories = []\n        col_set = set(df.columns)\n        for col in self.cat_cols:\n            if col in col_set:\n                if df[col].nunique() <= 10:\n                    categories.append(col)\n        return categories\n\n    def update_data(self, df, col2type):\n        self.data = df\n        self.update_col2type(col2type)\n\n    def update_col2type(self, col2type):\n        self.col2type.update(col2type)\n        self.type_reset()\n\n    def type_reset(self):\n\n        cat_cols = []\n        num_cols = []\n\n        for cname, ctype in self.col2type.items():\n            if ctype == CONSTANT.CATEGORY_TYPE:\n                cat_cols.append(cname)\n            elif ctype == CONSTANT.NUMERICAL_TYPE:\n                num_cols.append(cname)\n\n        self.cat_cols = sorted(cat_cols)\n        self.num_cols = sorted(num_cols)\n\n    def init_col2type(self):\n        for col in self.cat_cols:\n            self.col2type[col] = CONSTANT.CATEGORY_TYPE\n        for col in self.num_cols:\n            self.col2type[col] = CONSTANT.NUMERICAL_TYPE\n\n    def num_fit_transform(self, df, num_cols):\n        if not num_cols:\n            return []\n        scaler = StandardScaler()\n        df = df[list(num_cols)]\n        arr = df.values\n        norm_arr = scaler.fit_transform(arr)\n        norm_arr[np.isnan(norm_arr)] = 0\n        return norm_arr\n\n    def cat_fit_transform(self, df, cat_cols):\n        if len(cat_cols) == 0:\n            return []\n        res = []\n        for cat in cat_cols:\n            enc = OneHotEncoder(handle_unknown='ignore')\n            arr = enc.fit_transform(df[cat].values.reshape(-1, 1)).toarray()\n            res.append(arr)\n        cat_feats = np.concatenate(res, axis=1)\n        return cat_feats\n\n    def nn_process(self, X, cat_cols):\n        num_cols = [col for col in X.columns if col not in cat_cols]\n\n\n        X = fill_na(X)\n\n        cat_feats = self.cat_fit_transform(X, cat_cols)\n        num_feats = self.num_fit_transform(X, num_cols)\n\n        if len(cat_feats) > 0 and len(num_feats) > 0:\n            feats = np.concatenate([cat_feats, num_feats], axis=1)\n        elif len(cat_feats) > 0:\n            feats = cat_feats\n        elif len(num_feats) > 0:\n            feats = num_feats\n        return feats\n\n\n    def label_encode(self, X, cat_cols):\n        if cat_cols == []:\n            return\n\n        for col in cat_cols:\n            X[col] = pd.Categorical(X[col]).codes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"""
AutoDL_sample_code_submission/Auto_Tabular/explore.py,0,"b""import numpy as np\nimport gc\nimport collections\nfrom tensorflow.python.keras import backend as K\n\nfrom Auto_Tabular.feature.feat_engine import FeatEngine\nfrom Auto_Tabular.utils.log_utils import log, timeit\n\n\n\nclass Explore:\n    def __init__(self, metadata, info, model_space, data_space):\n        self.metadata = metadata\n        self.info = info\n        self.info['mode'] = 'first_round'\n        self.model_space = model_space\n        self.data_space = data_space\n\n        self.model = None\n\n        self.model_prior = model_space.model_prior\n        self.model_idx = 0\n\n        self.input_shape = None\n\n        self.patience = 3\n        self.auc_gain_threshold = 1e-4\n        self.ensemble_std_threshold = 1e-2\n\n        self.round_num = 1\n\n        self.hist_info = {}\n\n        self.dataloader = None\n\n        self.feat_engine = FeatEngine()\n\n        self.update_predcit = True\n\n        self.use_all_data = False\n\n    def explore_space(self, train_loop_num, time_remain=None):\n        self.explore_model_space(train_loop_num)\n        self.explore_data_space(train_loop_num)\n        self.create_model(self.metadata.output_dim)\n\n        # train and evaluate\n        self.model.epoch_train(self.dataloader, run_num=self.model.run_num,\n                               is_multi_label=self.info['is_multi_label'], info=self.info, time_remain=time_remain)\n\n\n        if not self.use_all_data:\n            val_auc = self.model.epoch_valid(self.dataloader)\n\n        else:\n            val_auc = self.model.best_auc+0.0001\n            self.use_all_data = False\n\n        self.update_model_hist(val_auc)\n\n    def explore_model_space(self, train_loop_num):\n        if train_loop_num == 1:\n            self.model = self.model_space.get_model(self.model_prior[self.model_idx], self.round_num)\n            self.last_model_type = self.model.type\n        else:\n            if self.model.not_rise_num == self.model.patience \\\n                    or (self.model.not_gain_num > self.model.not_gain_threhlod) \\\n                    or self.model.run_num >= self.model.max_run or self.info['mode'] =='bagging':\n                self.model_idx += 1\n                self.reset_model_cache()\n                if self.model_idx == len(self.model_prior):\n                    self.sort_model_prior()\n                    self.info['mode'] = 'bagging'\n                    self.data_space.update = True\n                self.model = self.model_space.get_model(self.model_prior[self.model_idx], self.round_num)\n                self.use_all_data = False\n                if self.model.type != self.last_model_type:\n                    self.dataloader = None\n                    gc.collect()\n\n    def explore_data_space(self, train_loop_num):\n        self.feat_engine.fit_transform(self.data_space, train_loop_num, info=self.info)\n\n        if self.data_space.update or self.dataloader is None:\n            self.dataloader = self.data_space.get_dataloader(train_loop_num=train_loop_num,\n                                                             round_num=self.round_num,\n                                                             run_num=self.model.run_num,\n                                                             use_all_data=self.use_all_data,\n                                                             model_type=self.model.type)\n            self.data_space.update = False\n\n    def update_model_hist(self, val_auc):\n        self.model.run_num += 1\n        self.model.auc_gain = val_auc - self.model.hist_auc[-1]\n        if self.model.auc_gain < self.auc_gain_threshold:\n            self.model.not_gain_num += 1\n        else:\n            self.model.not_gain_num = 0\n        self.model.hist_auc.append(val_auc)\n        if val_auc > self.model.best_auc:\n            self.model.best_auc = val_auc\n            self.update_predcit = True\n        else:\n            self.update_predcit = False\n            self.model.not_rise_num += 1\n\n        if self.model.run_num >= self.model.all_data_round or self.model.not_gain_num > 3:\n            self.use_all_data = True\n        else:\n            self.use_all_data = False\n\n        if hasattr(self.model, 'all_data_round_pre'):\n            if self.model.run_num == self.model.all_data_round_pre:\n                self.use_all_data = True\n\n    def reset_model_cache(self):\n        del self.model\n        self.model = None\n        gc.collect()\n        K.clear_session()\n\n    def create_model(self, class_num):\n        if not self.model.is_init:\n            if self.model.type == 'nn_keras':\n                self.model.init_model(class_num, shape=self.dataloader['shape'], is_multi_label=self.info['is_multi_label'])\n            else:\n                self.model.init_model(class_num)\n\n    def sort_model_prior(self):\n        model_perform = collections.defaultdict(list)\n        for name, info in self.hist_info.items():\n            first_name = name.split('_')[0]\n            auc = info[0]\n            if first_name in model_perform:\n                model_perform[first_name].append(auc)\n        self.model_prior = sorted(self.model_prior, key=lambda x: np.mean(model_perform[x]), reverse=True)\n        self.model_idx = 0\n        self.round_num += 1\n\n    def get_top_preds(self):\n        models_name = self.hist_info.keys()\n        models_auc = [self.hist_info[name][0] for name in models_name]\n        models_name_sorted, models_auc_sored = (list(i) for i in\n                                                zip(*sorted(zip(models_name, models_auc), key=lambda x: x[1], reverse=True)))\n\n        for i in range(len(models_auc_sored), 0, -1):\n            std = np.std(models_auc_sored[:i])\n            top_num = i\n            if std < self.ensemble_std_threshold:\n                break\n\n        top_auc = np.array(models_auc_sored[:top_num])\n        # weights = top_auc / top_auc.sum()\n        # print(weights)\n\n        top_auc = top_auc + 15*(top_auc - top_auc.mean())\n        top_auc = np.array([max(0.01, i) for i in top_auc])\n        weights = top_auc / top_auc.sum()\n        print(weights)\n\n        top_preds = []\n        for i in range(top_num):\n            name = models_name_sorted[i]\n            rank = i + 1\n            auc = models_auc_sored[i]\n            weight = weights[i]\n            preds = self.hist_info[name][1]\n            top_preds.append((name, rank, auc, weight, preds))\n        return top_preds\n\n    def predict(self):\n        if self.update_predcit:\n            preds = self.model.predict(self.dataloader)\n            if self.model.hist_auc[-1] == self.model.best_auc:\n                self.model.best_preds = preds\n                self.hist_info[self.model.name] = (self.model.best_auc, self.model.best_preds)\n\n        preds = self.blending_predict()\n        return preds\n\n    #@timeit\n    def blending_predict(self):\n        top_preds = self.get_top_preds()\n        ensemble_preds = 0\n        for name, rank, auc, weight, preds in top_preds:\n            m = np.mean(preds)\n            ensemble_preds += weight * preds/m\n        return ensemble_preds\n\n    def stacking_predict(self):\n        pass\n\n    def softmax(self, x):\n        x = x - x.max()\n        e = np.exp(x)\n        return e / e.sum()\n"""
AutoDL_sample_code_submission/Auto_Tabular/model.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS-IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# Modified by: Zhengying Liu, Isabelle Guyon\n\n""""""An example of code submission for the AutoDL challenge.\n\nIt implements 3 compulsory methods (\'__init__\', \'train\' and \'test\') and\nan attribute \'done_training\' for indicating if the model will not proceed more\ntraining due to convergence or limited time budget.\n\nTo create a valid submission, zip model.py together with other necessary files\nsuch as Python modules/packages, pre-trained weights. The final zip file should\nnot exceed 300MB.\n""""""\n\nfrom tensorflow.python.client import device_lib\nimport logging\nimport numpy as np\nimport os\n\nos.system(""pip install -i https://pypi.tuna.tsinghua.edu.cn/simple hyperopt==0.2.3"")\nos.system(""pip install -i https://pypi.tuna.tsinghua.edu.cn/simple xgboost==0.90"")\nos.system(""pip install -i https://pypi.tuna.tsinghua.edu.cn/simple catboost==0.21"")\n\n\nimport sys\nimport pandas as pd\nimport gc\nimport lightgbm as lgb\n\nfrom explore import Explore\nfrom data_space import TabularDataSpace\nfrom model_space import TabularModelSpace\nfrom .utils.eda import AutoEDA\nfrom .utils.log_utils import timeit, log\nfrom .utils.data_utils import ohe2cat\nfrom Auto_Tabular import CONSTANT\nimport random\nimport time\nfrom catboost import CatBoostClassifier\n\nimport tensorflow as tf\nfrom tensorflow.python.keras.backend import set_session\n\nos.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""\nconfig = tf.ConfigProto()\n# dynamically grow the memory used on the GPU\nconfig.gpu_options.allow_growth = True\n# to log device placement (on which device the operation ran)\nconfig.log_device_placement = False\n# (nothing gets printed in Jupyter, only if you run it standalone)\nsess = tf.Session(config=config)\n# set this TensorFlow session as the default session for Keras\nset_session(sess)\n\nclass Model(object):\n  """"""Trivial example of valid model. Returns all-zero predictions.""""""\n\n  def __init__(self, metadata):\n    """"""\n    Args:\n      metadata: an AutoDLMetadata object. Its definition can be found in\n          AutoDL_ingestion_program/dataset.py\n    """"""\n    self.done_training = False\n    self.metadata = metadata\n\n    self.metadata_info = metadata.metadata_\n    print(self.metadata_info)\n    self.train_loop_num = 0\n\n    self.auto_eda = AutoEDA()\n\n    self.X = []\n    self.Y = []\n\n    self.pre_increament_preds = True\n\n    self.X_test = None\n\n    self.next_element = None\n\n    self.lgb_info = {}\n\n    self.imp_cols = None\n\n    self.is_multi_label = None\n\n    self.models = {}\n\n    self.sample_cols = None\n\n    self.unknow_cols = None\n\n    self.first_preds = False\n\n    self.model = None\n\n    self.keep_training_booster = False\n\n  #@timeit\n  def train(self, dataset, remaining_time_budget=None):\n    """"""Train this algorithm on the tensorflow |dataset|.\n\n    This method will be called REPEATEDLY during the whole training/predicting\n    process. So your `train` method should be able to handle repeated calls and\n    hopefully improve your model performance after each call.\n\n    ****************************************************************************\n    ****************************************************************************\n    IMPORTANT: the loop of calling `train` and `test` will only run if\n        self.done_training = False\n      (the corresponding code can be found in ingestion.py, search\n      \'M.done_training\')\n      Otherwise, the loop will go on until the time budget is used up. Please\n      pay attention to set self.done_training = True when you think the model is\n      converged or when there is not enough time for next round of training.\n    ****************************************************************************\n    ****************************************************************************\n\n    Args:\n      dataset: a `tf.data.Dataset` object. Each of its examples is of the form\n            (example, labels)\n          where `example` is a dense 4-D Tensor of shape\n            (sequence_size, row_count, col_count, num_channels)\n          and `labels` is a 1-D Tensor of shape\n            (output_dim,).\n          Here `output_dim` represents number of classes of this\n          multilabel classification task.\n\n          IMPORTANT: some of the dimensions of `example` might be `None`,\n          which means the shape on this dimension might be variable. In this\n          case, some preprocessing technique should be applied in order to\n          feed the training of a neural network. For example, if an image\n          dataset has `example` of shape\n            (1, None, None, 3)\n          then the images in this datasets may have different sizes. On could\n          apply resizing, cropping or padding in order to have a fixed size\n          input tensor.\n\n      remaining_time_budget: time remaining to execute train(). The method\n          should keep track of its execution time to avoid exceeding its time\n          budget. If remaining_time_budget is None, no time budget is imposed.\n    """"""\n    self.train_loop_num += 1\n    if self.pre_increament_preds:\n      self.X_train, self.Y_train = self.to_numpy_train(dataset)\n      self.X_train = pd.DataFrame(self.X_train)\n\n    if not self.pre_increament_preds and self.train_loop_num > 50:\n      self.done_training = True\n\n  #@timeit\n  def test(self, dataset, remaining_time_budget=None):\n    """"""Make predictions on the test set `dataset` (which is different from that\n    of the method `train`).\n\n    Args:\n      Same as that of `train` method, except that the `labels` will be empty\n          since this time `dataset` is a test set.\n    Returns:\n      predictions: A `numpy.ndarray` matrix of shape (sample_count, output_dim).\n          here `sample_count` is the number of examples in this dataset as test\n          set and `output_dim` is the number of labels to be predicted. The\n          values should be binary or in the interval [0,1].\n    """"""\n    if self.pre_increament_preds or self.first_preds:\n      if self.X_test is None:\n        self.X_test, _ = self.to_numpy_test(dataset)\n        self.X_test = pd.DataFrame(self.X_test)\n\n      preds = self.simple_lgb(self.X_train, self.Y_train, self.X_test)\n      if self.first_preds:\n        self.first_preds = False\n        self.train_loop_num = 0\n    else:\n      if self.train_loop_num == 1:\n        self.X_test.index = -self.X_test.index - 1\n        main_df = pd.concat([self.X_train, self.X_test], axis=0)\n\n        self.X_test.drop(self.X_test.columns, axis=1, inplace=True)\n        self.X_train.drop(self.X_train.columns, axis=1, inplace=True)\n        del self.X_train, self.X_test, self.X, self.Y\n        gc.collect()\n\n        eda_info = self.auto_eda.get_info(main_df)\n        eda_info[\'is_multi_label\'] = self.is_multi_label\n        self.data_space = TabularDataSpace(self.metadata_info, eda_info, main_df, self.Y_train, self.lgb_info)\n        self.model_space = TabularModelSpace(self.metadata_info, eda_info)\n        self.explore = Explore(self.metadata_info, eda_info, self.model_space, self.data_space)\n      print(\'time\', remaining_time_budget)\n      self.explore.explore_space(train_loop_num=self.train_loop_num, time_remain=remaining_time_budget)\n      preds = self.explore.predict()\n    return preds\n\n  #@timeit\n  def simple_lgb(self, X, y, test_x):\n    self.params = {\n      ""boosting_type"": ""gbdt"",\n      ""objective"": ""multiclass"",\n      \'num_class\': y.shape[1],\n      ""metric"": ""multi_logloss"",\n      ""verbosity"": -1,\n      ""seed"": CONSTANT.SEED,\n      ""num_threads"": CONSTANT.JOBS,\n    }\n\n    self.hyperparams = {\n      \'num_leaves\': 31,\n      \'max_depth\': -1,\n      \'min_child_samples\': 20,\n      \'max_bin\': 110,\n      \'subsample\': 1,\n      \'subsample_freq\': 1,\n      \'colsample_bytree\': 0.8,\n      \'min_child_weight\': 0.001,\n      \'min_split_gain\': 0.02,\n      \'reg_alpha\': 0.1,\n      \'reg_lambda\': 0.1,\n      ""learning_rate"": 0.1,\n      \'num_boost_round\': 10,\n    }\n\n    print(\'sample lgb predict num:\', self.train_loop_num)\n    if self.train_loop_num == 1:\n      if X.shape[1] > 500:\n        self.sample_cols = list(set(X.columns))[::2]\n        self.unknow_cols = [col for col in X.columns if col not in self.sample_cols]\n        X = X[self.sample_cols]\n        test_x = test_x[self.sample_cols]\n      if self.is_multi_label:\n        self.params[\'num_class\'] = 2\n        all_preds = []\n        for cls in range(y.shape[1]):\n          cls_y = y[:, cls]\n          data = lgb.Dataset(X, cls_y)\n          self.models[cls] = lgb.train({**self.params, **self.hyperparams}, data)\n          preds = self.models[cls].predict(test_x)\n          all_preds.append(preds[:,1])\n        preds = np.stack(all_preds, axis=1)\n      else:\n        lgb_train = lgb.Dataset(X, ohe2cat(y))\n        self.model = lgb.train({**self.params, **self.hyperparams},\n                               train_set=lgb_train)\n        preds = self.model.predict(test_x)\n      self.log_feat_importances()\n    else:\n      self.hyperparams[\'num_boost_round\'] += self.train_loop_num * 5\n      self.hyperparams[\'num_boost_round\'] = min(40, self.hyperparams[\'num_boost_round\'])\n      print(self.hyperparams[\'num_boost_round\'])\n\n      if self.is_multi_label:\n        models = {}\n        all_preds = []\n        for cls in range(y.shape[1]):\n          cls_y = y[:, cls]\n          data = lgb.Dataset(X[self.imp_cols], cls_y)\n          models[cls] = lgb.train({**self.params, **self.hyperparams}, data)\n          preds = models[cls].predict(test_x[self.imp_cols])\n          all_preds.append(preds[:, 1])\n        preds = np.stack(all_preds, axis=1)\n      else:\n        lgb_train = lgb.Dataset(X[self.imp_cols], ohe2cat(y))\n        model = lgb.train({**self.params, **self.hyperparams}, train_set=lgb_train)\n        preds = model.predict(test_x[self.imp_cols])\n    return preds\n\n\n  #@timeit\n  def to_numpy_train(self, dataset):\n    if self.next_element is None:\n      dataset = dataset.batch(100)\n      iterator = dataset.make_one_shot_iterator()\n      self.next_element = iterator.get_next()\n    if self.train_loop_num == 1 or self.train_loop_num == 2:\n      size = 500\n      #1000\n    elif self.train_loop_num == 3 or self.train_loop_num == 4:\n      size = 1000\n    else:\n      size = 500*2**(self.train_loop_num-3)\n    for i in range(int(size/100)):\n      try:\n        example, labels = sess.run(self.next_element)\n        self.X.extend(example)\n        self.Y.extend(labels)\n      except tf.errors.OutOfRangeError:\n        self.pre_increament_preds = False\n        if self.train_loop_num == 1:\n          self.first_preds = True\n        self.train_loop_num = 1\n        break\n\n    X, y = np.asarray(self.X), np.asarray(self.Y)\n    print(self.train_loop_num)\n    print(X.shape)\n\n    if self.train_loop_num == 1:\n      if any(y.sum(axis=1) > 1):\n        print(\'is multi label\')\n        self.is_multi_label = True\n\n    return X[:,0,0,:,0], y\n\n  def to_numpy_test(self, dataset):\n    dataset = dataset.batch(100)\n    iterator = dataset.make_one_shot_iterator()\n    next_element = iterator.get_next()\n    X = []\n    Y = []\n    while True:\n      try:\n        example, labels = sess.run(next_element)\n        X.extend(example)\n        Y.extend(labels)\n      except tf.errors.OutOfRangeError:\n        break\n    X, y = np.asarray(X), np.asarray(Y)\n    return X[:,0,0,:,0], y\n\n  def log_feat_importances(self):\n    if not self.is_multi_label:\n      importances = pd.DataFrame({\'features\': [i for i in self.model.feature_name()],\n                                  \'importances\': self.model.feature_importance(""gain"")})\n    else:\n      importances = pd.DataFrame({\'features\': [i for i in self.models[0].feature_name()],\n                                  \'importances\': self.models[0].feature_importance(""gain"")})\n\n    importances.sort_values(\'importances\', ascending=False, inplace=True)\n\n    importances = importances[importances[\'importances\'] > 0]\n    size = int(len(importances)*0.8)\n    if self.imp_cols is None:\n      if self.unknow_cols is not None:\n        self.imp_cols = self.unknow_cols + [int(col) for col in importances[\'features\'].values]\n      else:\n        self.imp_cols = [int(col) for col in importances[\'features\'].values]\n    else:\n      self.imp_cols = [int(col) for col in importances[\'features\'].values]\n    self.lgb_info[\'imp_cols\'] = self.imp_cols\n\n\n\n\n\n  def infer_domain(self):\n    """"""Infer the domain from the shape of the 4-D tensor.""""""\n    row_count, col_count = self.metadata.get_matrix_size(0)\n    sequence_size = self.metadata.get_sequence_size()\n    channel_to_index_map = dict(self.metadata.get_channel_to_index_map())\n    domain = None\n    if sequence_size == 1:\n      if row_count == 1 or col_count == 1:\n        domain = ""tabular""\n      else:\n        domain = ""image""\n    else:\n      if row_count == 1 and col_count == 1:\n        if channel_to_index_map:\n          domain = ""text""\n        else:\n          domain = ""speech""\n      else:\n        domain = ""video""\n    self.domain = domain\n    tf.logging.info(""The inferred domain of the dataset is: {}."".format(domain))\n    return domain\n\ndef has_regular_shape(dataset):\n  """"""Check if the examples of a TF dataset has regular shape.""""""\n  with tf.Graph().as_default():\n    iterator = dataset.make_one_shot_iterator()\n    example, labels = iterator.get_next()\n    return all([x > 0 for x in example.shape])\n\ndef get_logger(verbosity_level):\n  """"""Set logging format to something like:\n       2019-04-25 12:52:51,924 INFO score.py: <message>\n  """"""\n  logger = logging.getLogger(__file__)\n  logging_level = getattr(logging, verbosity_level)\n  logger.setLevel(logging_level)\n  formatter = logging.Formatter(\n    fmt=\'%(asctime)s %(levelname)s %(filename)s: %(message)s\')\n  stdout_handler = logging.StreamHandler(sys.stdout)\n  stdout_handler.setLevel(logging_level)\n  stdout_handler.setFormatter(formatter)\n  stderr_handler = logging.StreamHandler(sys.stderr)\n  stderr_handler.setLevel(logging.WARNING)\n  stderr_handler.setFormatter(formatter)\n  logger.addHandler(stdout_handler)\n  logger.addHandler(stderr_handler)\n  logger.propagate = False\n  return logger\n\nlogger = get_logger(\'INFO\')\n'"
AutoDL_sample_code_submission/Auto_Tabular/model_space.py,0,"b""from Auto_Tabular.utils.log_utils import info\nfrom model_lib import *\n\nclass TabularModelSpace:\n    def __init__(self, metadata, info):\n        self.metadata = metadata\n        self.info = info\n\n        self.model_init_prior = ['lgb', 'cb', 'xgb', 'dnn']\n        self.model_prior = self.sort_prior_by_meta()\n\n        self.model_lib = {\n            'lr': LogisticRegression,\n            'lgb': LGBModel,\n            'xgb': XGBModel,\n            'cb': CBModel,\n            'dnn': DnnModel,\n            'enn': ENNModel\n        }\n\n    def sort_prior_by_meta(self):\n        prior_copy = self.model_init_prior.copy()\n        return prior_copy\n\n    def get_model(self, model_name, round_num):\n        if model_name in self.model_lib:\n            model = self.model_lib[model_name]()\n            model.name = '{}_{}'.format(model.name, round_num)\n            return model\n\n    def destroy_model(self, model_name):\n        if model_name in self.model_lib:\n            self.model_prior.remove(model_name)\n"""
AutoDL_sample_code_submission/Auto_Video/model.py,21,"b'# -*- coding: utf-8 -*-\r\nfrom __future__ import absolute_import\r\nimport os\r\n\r\nos.system(\r\n    ""pip install --extra-index-url https://developer.download.nvidia.com/compute/redist/cuda/10.0 nvidia-dali && pip3 install torch torchvision"")\r\n\r\nimport threading\r\nimport random\r\n\r\nimport tensorflow as tf\r\nimport torch\r\nimport torchvision as tv\r\nimport numpy as np\r\n\r\nimport skeleton\r\nfrom architectures.mc3 import ResNet as MC3\r\nfrom skeleton.projects import LogicModel, get_logger\r\nfrom skeleton.projects.others import NBAC, AUC\r\nimport time\r\nfrom collections import OrderedDict\r\nfrom skeleton.utils.log_utils import timeit\r\n\r\nimport pdb\r\n\r\ntorch.backends.cudnn.benchmark = True\r\nthreads = [\r\n    threading.Thread(target=lambda: torch.cuda.synchronize()),\r\n    threading.Thread(target=lambda: tf.Session())\r\n]\r\n[t.start() for t in threads]\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\r\n\r\nLOGGER = get_logger(__name__)\r\n\r\ndef set_random_seed_all(seed, deterministic=False):\r\n    random.seed(seed)\r\n    os.environ[\'PYTHONHASHSEED\'] = str(seed)\r\n    np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed_all(seed)\r\n    tf.random.set_random_seed(seed)\r\n    if deterministic:\r\n        torch.backends.cudnn.deterministic = True\r\n        torch.backends.cudnn.benchmark = False\r\n\r\ndef get_top_players(data, sort_keys, reverse=True, n=2, order=True):\r\n    top = sorted(data.items(), key=lambda x: x[1][sort_keys], reverse=reverse)[:n]\r\n    if order:\r\n        return OrderedDict(top)\r\n    return dict(top)\r\n\r\nclass Model(LogicModel):\r\n    def __init__(self, metadata):\r\n        set_random_seed_all(0xC0FFEE,True)\r\n        super(Model, self).__init__(metadata)\r\n\r\n    def build(self):\r\n        base_dir = os.path.dirname(os.path.abspath(__file__))\r\n        in_channels = self.base_info[\'dataset\'][\'shape\'][-1]\r\n        num_class = self.base_info[\'dataset\'][\'num_class\']\r\n        # torch.cuda.synchronize()\r\n\r\n        LOGGER.info(\'[init] session\')\r\n        [t.join() for t in threads]\r\n\r\n        self.device = torch.device(\'cuda\', 0)\r\n        self.session = tf.Session()\r\n\r\n        LOGGER.info(\'[init] Model\')\r\n\r\n        model_space = [MC3,MC3,MC3,MC3]\r\n        self.model_space =[]\r\n        self.ensembleconfig.MODEL_LENGTH = len(self.model_space)\r\n        for key,model in enumerate(model_space):\r\n            if model in [MC3]:\r\n                Network = model(in_channels, num_class)\r\n                model_path = os.path.join(base_dir, \'models\')\r\n                LOGGER.info(\'model path: %s\', model_path)\r\n                Network.init(model_dir=model_path, gain=1.0)\r\n            else:\r\n                Network = model(in_channels, num_class)\r\n                Network.init(model_dir=model_path, gain=1.0)\r\n            self.model_space.append(Network)\r\n\r\n        self.model = self.model_space[self.ensembleconfig.MODEL_INDEX]\r\n        self.model_pred = self.model_space[self.ensembleconfig.MODEL_INDEX].eval()\r\n\r\n        LOGGER.info(\'[init] copy to device\')\r\n        self.model = self.model.to(device=self.device, non_blocking=True)  # .half()\r\n        self.model_pred = self.model_pred.to(device=self.device, non_blocking=True)  # .half()self\xe3\x80\x82\r\n        LOGGER.info(\'[init] done.\')\r\n\r\n    def update_model(self):\r\n        self.is_half = self.model._half\r\n        num_class = self.base_info[\'dataset\'][\'num_class\']\r\n\r\n        epsilon = min(0.1, max(0.001, 0.001 * pow(num_class / 10, 2)))\r\n        if self.is_multiclass():\r\n            self.model.loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\'none\')\r\n            self.tau = 8.0\r\n            LOGGER.info(\'[update_model] %s (tau:%f, epsilon:%f)\', self.model.loss_fn.__class__.__name__, self.tau,\r\n                        epsilon)\r\n        else:\r\n            self.model.loss_fn = torch.nn.CrossEntropyLoss(reduction=\'none\')\r\n            self.tau = 8.0\r\n            LOGGER.info(\'[update_model] %s (tau:%f, epsilon:%f)\', self.model.loss_fn.__class__.__name__, self.tau,\r\n                        epsilon)\r\n        self.model_pred.loss_fn = self.model.loss_fn\r\n\r\n        self.model.hyper_params[\'conditions\'][\'use_fast_auto_aug\'] = False\r\n        times = self.model.hyper_params[\'dataset\'][\'input\'][0]\r\n        self.model.set_video(times=times)\r\n        self.model_pred.set_video(times=times)\r\n\r\n        self.init_opt()\r\n        LOGGER.info(\'[update] done.\')\r\n\r\n    def init_opt(self):\r\n        steps_per_epoch = self.model.hyper_params[\'dataset\'][\'steps_per_epoch\']\r\n        batch_size = self.model.hyper_params[\'dataset\'][\'batch_size\']\r\n        init_lr = self.model.hyper_params[\'optimizer\'][\'lr\']\r\n        warmup_multiplier = self.model.hyper_params[\'optimizer\'][\'warmup_multiplier\']\r\n        warmup_epoch = self.model.hyper_params[\'optimizer\'][\'warmup_epoch\']\r\n        self.model.init_opt(steps_per_epoch,batch_size,init_lr,warmup_multiplier,warmup_epoch)\r\n        LOGGER.info(\'[optimizer] %s (batch_size:%d)\', self.model.optimizer._optimizer.__class__.__name__, batch_size)\r\n\r\n    @timeit\r\n    def adapt(self, remaining_time_budget=None):\r\n        epoch = self.model.info[\'loop\'][\'epoch\']\r\n        input_shape = self.model.hyper_params[\'dataset\'][\'input\']\r\n        height, width = input_shape[:2]\r\n        batch_size = self.model.hyper_params[\'dataset\'][\'batch_size\']\r\n\r\n        train_score = np.average([c[\'train\'][\'score\'] for c in self.model.checkpoints[-5:]])\r\n        valid_score = np.average([c[\'valid\'][\'score\'] for c in self.model.checkpoints[-5:]])\r\n        LOGGER.info(\'[adapt] [%04d/%04d] train:%.3f valid:%.3f\',\r\n                    epoch, self.model.hyper_params[\'dataset\'][\'max_epoch\'],\r\n                    train_score, valid_score)\r\n\r\n        self.model.use_test_time_augmentation = self.model.info[\'loop\'][\'test\'] > 1\r\n\r\n        if self.model.hyper_params[\'conditions\'][\'use_fast_auto_aug\']:\r\n            self.model.hyper_params[\'conditions\'][\'use_fast_auto_aug\'] = valid_score < 0.995\r\n\r\n        # Adapt Apply Fast auto aug\r\n        if self.model.hyper_params[\'conditions\'][\'use_fast_auto_aug\'] and \\\r\n                (train_score > 0.995 or self.model.info[\'terminate\']) and \\\r\n                remaining_time_budget > 120 and \\\r\n                valid_score > 0.01 and \\\r\n                self.dataloaders[\'valid\'] is not None and \\\r\n                not self.model.update_transforms:\r\n            LOGGER.info(\'[adapt] search fast auto aug policy\')\r\n            self.update_transforms = True\r\n\r\n    def activation(self, logits):\r\n        if self.is_multiclass():\r\n            logits = torch.sigmoid(logits)\r\n            prediction = (logits > 0.5).to(logits.dtype)\r\n        else:\r\n            logits = torch.softmax(logits, dim=-1)\r\n            _, k = logits.max(-1)\r\n            prediction = torch.zeros(logits.shape, dtype=logits.dtype, device=logits.device).scatter_(-1, k.view(-1, 1),\r\n                                                                                                      1.0)\r\n        return logits, prediction\r\n    \r\n    def decision_if_single_ensemble(self,model):\r\n        pre_ens_round_start = 3\r\n        if model.round_idx > pre_ens_round_start:\r\n            model.single_ensemble = True\r\n            return model.single_ensemble\r\n        else:\r\n            model.single_ensemble = False\r\n            return model.single_ensemble\r\n\r\n    @timeit\r\n    def epoch_train(self, epoch, train, model=None):\r\n        model = model if model is not None else self.model\r\n        model.round_idx += 1\r\n        if epoch < 0:\r\n            optimizer = model.optimizer_fc\r\n        else:\r\n            optimizer = model.optimizer\r\n\r\n        model.train()\r\n        model.zero_grad()\r\n        if model.info[\'condition\'][\'first\'][\'train\']:\r\n            num_steps = 10000\r\n        else:\r\n            num_steps = model.hyper_params[\'dataset\'][\'steps_per_epoch\']\r\n        # train\r\n        metrics = []\r\n        scores = []\r\n        score = 0\r\n        step = 0\r\n\r\n        for step, (examples, labels) in zip(range(num_steps), train):\r\n            if examples.shape[0] == 1:\r\n                examples = examples[0]\r\n                labels = labels[0]\r\n            original_labels = labels\r\n            if not self.is_multiclass():\r\n                labels = labels.argmax(dim=-1)\r\n            # LOGGER.info(\'*\'*30+str(self.is_multiclass()) + \'*\'*30)\r\n            skeleton.nn.MoveToHook.to((examples, labels), self.device, self.is_half)\r\n            logits, loss = model(examples, labels, tau=self.tau, reduction=\'avg\')\r\n            loss = loss.sum()\r\n            loss.backward()\r\n\r\n            max_epoch =model.hyper_params[\'dataset\'][\'max_epoch\']\r\n            optimizer.update(maximum_epoch=max_epoch)\r\n            optimizer.step()\r\n            model.zero_grad()\r\n            if model.info[\'condition\'][\'first\'][\'train\']:\r\n                logits, prediction = self.activation(logits.float())\r\n                score = AUC(logits, original_labels.float())\r\n                scores.append(score)\r\n                if step > 10 and sum(scores[-10:]) > 2.:\r\n                    break\r\n        \r\n            if step == num_steps - 1:\r\n                logits, prediction = self.activation(logits.float())\r\n                score = AUC(logits, original_labels.float())\r\n\r\n            metrics.append({\r\n                \'loss\': loss.detach().float().cpu(),\r\n                \'score\': 0,\r\n            })\r\n\r\n        train_loss = np.average([m[\'loss\'] for m in metrics])\r\n        train_score = score\r\n        optimizer.update(train_loss=train_loss)\r\n\r\n        self.train_loss = train_loss\r\n        self.train_score = train_score\r\n\r\n        return {\r\n            \'loss\': train_loss,\r\n            \'score\': train_score,\r\n            \'run_steps\': step\r\n        }\r\n\r\n    @timeit\r\n    def epoch_valid(self, epoch, valid, model=None,reduction=\'avg\'):\r\n        test_time_augmentation = False\r\n        model = model if model is not None else self.model\r\n        model.train()\r\n        num_steps = len(valid)\r\n        metrics = []\r\n        tau = self.tau\r\n\r\n        with torch.no_grad():\r\n            for step, (examples, labels) in zip(range(num_steps), valid):\r\n                original_labels = labels\r\n                if not self.is_multiclass():\r\n                    labels = labels.argmax(dim=-1)\r\n                batch_size = examples.size(0)\r\n\r\n                if model.use_test_time_augmentation and test_time_augmentation:\r\n                    examples = torch.cat([examples, torch.flip(examples, dims=[-1])], dim=0)\r\n                    labels = torch.cat([labels, labels], dim=0)\r\n\r\n                skeleton.nn.MoveToHook.to((examples, labels), self.device, self.is_half)\r\n                logits, loss = model(examples, labels, tau=tau, reduction=reduction)\r\n\r\n                if model.use_test_time_augmentation and test_time_augmentation:\r\n                    logits1, logits2 = torch.split(logits, batch_size, dim=0)\r\n                    logits = (logits1 + logits2) / 2.0\r\n\r\n                logits, prediction = self.activation(logits.float())\r\n                tpr, tnr, nbac = NBAC(prediction, original_labels.float())\r\n                if reduction == \'avg\':\r\n                    auc = AUC(logits, original_labels.float())\r\n                else:\r\n                    auc = max([AUC(logits[i:i + 16], original_labels[i:i + 16].float()) for i in\r\n                               range(int(len(logits)) // 16)])\r\n\r\n                score = auc if model.hyper_params[\'conditions\'][\'score_type\'] == \'auc\' else float(nbac.detach().float())\r\n                metrics.append({\r\n                    \'loss\': loss.detach().float().cpu(),\r\n                    \'score\': score,\r\n                })\r\n\r\n                LOGGER.debug(\r\n                    \'[valid] [%02d] [%03d/%03d] loss:%.6f AUC:%.3f NBAC:%.3f tpr:%.3f tnr:%.3f, lr:%.8f\',\r\n                    epoch, step, num_steps, loss, auc, nbac, tpr, tnr,\r\n                    model.optimizer.get_learning_rate()\r\n                )\r\n            \r\n            if reduction == \'avg\':\r\n                valid_loss = np.average([m[\'loss\'] for m in metrics])\r\n                valid_score = np.average([m[\'score\'] for m in metrics])\r\n            elif reduction in [\'min\', \'max\']:\r\n                valid_loss = np.min([m[\'loss\'] for m in metrics])\r\n                valid_score = np.max([m[\'score\'] for m in metrics])\r\n            else:\r\n                raise Exception(\'not support reduction method: %s\' % reduction)\r\n            \r\n        model.optimizer.update(valid_loss=np.average(valid_loss))\r\n\r\n        self.valid_loss = valid_loss\r\n        self.valid_score = valid_score\r\n        \r\n        model.g_his_eval_dict[model.round_idx] = {\r\n            ""t_loss"": self.train_loss,\r\n            ""t_acc"": self.train_score,\r\n            ""v_loss"": self.valid_loss,\r\n            ""v_acc"": self.valid_score\r\n        }\r\n\r\n        return {\r\n            \'loss\': valid_loss,\r\n            \'score\': valid_score,\r\n        }\r\n\r\n    def skip_valid(self, model):\r\n\r\n        model.g_his_eval_dict[model.round_idx] = {\r\n            ""t_loss"": self.train_loss,\r\n            ""t_acc"": self.train_score,\r\n            ""v_loss"": 99.9,\r\n            ""v_acc"": model.info[\'loop\'][\'epoch\'] * 1e-4\r\n        }\r\n\r\n        return {\r\n            \'loss\': 99.9,\r\n            \'score\': model.info[\'loop\'][\'epoch\'] * 1e-4,\r\n        }\r\n\r\n    @timeit\r\n    def prediction(self, dataloader, model, checkpoints,test_time_augmentation=True, detach=True, num_step=None):\r\n        tau = self.tau\r\n        best_idx = np.argmax(np.array([c[\'valid\'][\'score\'] for c in checkpoints]))\r\n        best_loss = checkpoints[best_idx][\'valid\'][\'loss\']\r\n        best_score = checkpoints[best_idx][\'valid\'][\'score\']\r\n\r\n        states = checkpoints[best_idx][\'model\']\r\n        model.load_state_dict(states)\r\n        LOGGER.info(\'best checkpoints at %d/%d (valid loss:%f score:%f) tau:%f\',\r\n                    best_idx + 1, len(checkpoints), best_loss, best_score, tau)\r\n\r\n        num_step = len(dataloader) if num_step is None else num_step\r\n        model.train()\r\n        \r\n        with torch.no_grad():\r\n            predictions = []\r\n            for step, (examples, labels) in enumerate(dataloader):\r\n\r\n                batch_size = examples.size(0)\r\n                if model.use_test_time_augmentation and test_time_augmentation:\r\n                    examples = torch.cat([examples, torch.flip(examples, dims=[-1])], dim=0)\r\n\r\n                # skeleton.nn.MoveToHook.to((examples, labels), self.device, self.is_half)\r\n                logits = model(examples, tau=tau)\r\n\r\n                if model.use_test_time_augmentation and test_time_augmentation:\r\n                    logits1, logits2 = torch.split(logits, batch_size, dim=0)\r\n                    logits = (logits1 + logits2) / 2.0\r\n\r\n                logits, prediction = self.activation(logits)\r\n\r\n                if detach:\r\n                    predictions.append(logits.detach().float().cpu().numpy())\r\n                else:\r\n                    predictions.append(logits)\r\n\r\n            if detach:\r\n                predictions = np.concatenate(predictions, axis=0).astype(np.float)\r\n            else:\r\n                predictions = torch.cat(predictions, dim=0)\r\n        return predictions'"
AutoDL_sample_code_submission/at_nlp/evaluator.py,0,"b'# -*- coding: utf-8 -*-\n# @Date    : 2020/3/4 16:11\nimport numpy as np\nfrom scipy import stats\n\nfrom log_utils import info\nfrom at_nlp.utils import color_msg, ohe2cat\nfrom at_toolkit.at_evalator import ATEvaluator\nfrom at_nlp.generators.data_generator import DataGenerator as BatchDataGenerator\n\n\nclass Evaluator():\n    def __init__(self, x, y):\n        self.x = x\n        self.label = y\n        self.pred = None\n        self.tokenizer = None\n        self.eval_generator = None\n        self.batch_size = 32\n        self.language = ""EN""\n        self.max_length = 0.0\n        self.num_features = 20000\n        self.cur_val_auc = 0.0\n        self.best_auc = 0.0\n        self.is_best = False\n        self.model_weights_list = []\n        self.val_auc_list = []\n        self.k = 0\n        self.patience = 3\n        self.max_epoch = 5\n        self.last_val_auc = 0.0\n        self.stop_criteria = False\n        self.best_call_num = 0\n\n    def _reset(self):\n        self.model_weights_list = []\n        self.val_auc_list = []\n        self.best_auc = 0.0\n        self.last_val_auc = 0.0\n        self.is_best = False\n        self.best_call_num = 0\n        self.k = 0\n        self.stop_criteria = False\n\n\n    def update_val_data(self, new_x, new_y):\n        self.x = new_x\n        self.label = new_y\n\n    def update_setting(self, language, max_length, num_features, tokenizer):\n        self.language = language\n        self.max_length = max_length\n        self.num_features = num_features\n        self.tokenizer = tokenizer\n        self.eval_generator = BatchDataGenerator(self.x, self.label,\n                                                 batch_size=self.batch_size,\n                                                 language=self.language,\n                                                 max_length=self.max_length\n                                                 if self.max_length else 100,\n                                                 vocab=None,\n                                                 tokenizer=self.tokenizer,\n                                                 num_features=self.num_features,\n                                                 shuffle=False)\n\n    def valid_auc(self, is_svm=False, model=None, use_autodl_auc=True):\n        if is_svm:\n            x_valid = self.tokenizer.transform(self.x)\n\n            result = model.predict_proba(x_valid)\n            result = self.rebuild_predict_prob(result)\n\n        else:\n\n            result = model.predict_generator(self.eval_generator)\n\n        if use_autodl_auc:\n            self.cur_val_auc = ATEvaluator.autodl_auc(solution=self.label, prediction=result)\n        else:\n            self.cur_val_auc = ATEvaluator.auc_metric(solution=self.label, prediction=result)\n\n        info(color_msg(""Note: cur_val_auc is {}"".format(self.cur_val_auc), color=\'blue\'))\n\n    def _reset_pred(self, prediction):\n        new_prob_arary = prediction\n        for sample_i in range(prediction.shape[0]):\n            np_median_value = np.median(prediction[sample_i])\n            for empty_cls in self.empty_class_:\n                new_prob_arary[sample_i][empty_cls] = np_median_value\n        return new_prob_arary\n\n    def rebuild_predict_prob(self, prediction):\n        new_prob_arary = prediction\n        val_label_distribution = np.sum(np.array(self.label), 0)\n\n        self.empty_class_ = [i for i in range(val_label_distribution.shape[0]) if val_label_distribution[i] == 0]\n        self.kurtosis = stats.kurtosis(val_label_distribution)\n        self.nomalized_std = np.std(val_label_distribution) / np.sum(val_label_distribution)\n\n        if self.empty_class_:\n            new_prob_arary = self._reset_pred(prediction)\n\n        return new_prob_arary\n\n    def check_early_stop_criteria(self, train_epoch):\n        # \xe6\x97\xa9\xe5\x81\x9c\xe6\x9d\xa1\xe4\xbb\xb6: \xe5\x87\xba\xe7\x8e\xb0k\xe6\xac\xa1\xe4\xbd\x8e\xe4\xba\x8e\xe6\x9c\x80\xe4\xbd\xb3auc\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\n        early_stop_criteria_1 = self.k >= self.patience or train_epoch > self.max_epoch\n        # \xe6\x97\xa9\xe5\x81\x9c\xe6\x9d\xa1\xe4\xbb\xb61: \xe5\xbd\x93\xe5\x89\x8d\xe8\xaf\x84\xe4\xbc\xb0auc\xe8\xb6\xb3\xe5\xa4\x9f\xe9\xab\x98\xe4\xb8\x94\xe8\xae\xad\xe7\xbb\x83\xe6\xac\xa1\xe6\x95\xb0\xe8\xb6\xb3\xe5\xa4\x9f\xe5\xa4\xa7\xef\xbc\x8c\xe5\x87\xba\xe7\x8e\xb0\xe4\xb8\x80\xe6\xac\xa1\xe4\xb8\x8b\xe9\x99\x8d\xe5\x8d\xb3\xe5\x81\x9c\n        early_stop_criteria_2 = self.cur_val_auc < self.last_val_auc and self.cur_val_auc > 0.96 and train_epoch > self.max_epoch\n        # \xe6\x97\xa9\xe5\x81\x9c\xe6\x9d\xa1\xe4\xbb\xb62: \xe5\xbd\x93\xe5\x89\x8d\xe8\xae\xad\xe7\xbb\x83\xe6\xac\xa1\xe6\x95\xb0\xe8\xbe\xbe\xe5\x88\xb0\xe9\x98\x88\xe5\x80\xbc\xef\xbc\x8c\xe4\xb8\x94\xe8\xbf\x9e\xe7\xbb\xad\xe4\xb8\x8b\xe9\x99\x8d\xe6\xac\xa1\xe6\x95\xb0\xe8\xbe\xbe\xe5\x88\xb0\xe9\x98\x88\xe5\x80\xbc\xe5\x8d\xb3\xe5\x81\x9c\n        early_stop_criteria_3 = train_epoch>= 5 and self.k >= 2\n        return (early_stop_criteria_1 or early_stop_criteria_2 or early_stop_criteria_3)\n\n    def update_early_stop_params(self):\n        if self.val_auc_list:\n            max_auc = np.max(self.val_auc_list)\n            self.last_val_auc = self.val_auc_list[-1]\n        else:\n            max_auc = 0.0\n            self.last_val_auc = 0.0\n\n        if self.cur_val_auc > max_auc and max_auc!=0.0:\n            self.k = 0\n        else:\n            self.k += 1\n\n    def update_model_weights(self, model, train_epoch, is_svm=False):\n        info(color_msg(""train_epoch is {}\xef\xbc\x8c cur model: model_weight_list is {}\\n"".format(train_epoch,\n                                                                                        len(self.model_weights_list))))\n        if self.model_weights_list:\n            if self.cur_val_auc > self.best_auc:\n                info(color_msg(""Update best result!""))\n                self.best_auc = self.cur_val_auc\n                self.is_best = True\n                self.best_call_num = train_epoch\n\n            else:\n                self.is_best = False\n                model.set_weights(self.model_weights_list[self.best_call_num])\n\n        else: # \xe6\x96\xb0\xe5\xa2\x9e\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x9d\x83\xe5\x80\xbc\n            self.is_best = True\n            self.best_auc = self.cur_val_auc\n            self.best_call_num = train_epoch\n        if is_svm:\n            pass\n        else:\n            model_weights = model.get_weights()\n            self.model_weights_list.append(model_weights)\n\n    def decide_stop(self, train_epoch):\n        self.update_early_stop_params()\n        self.val_auc_list.append(self.cur_val_auc)\n        self.stop_criteria = self.check_early_stop_criteria(train_epoch)\n        info(color_msg(""Note: stop condition is {}"".format(self.stop_criteria), color=\'blue\'))\n        return self.stop_criteria'"
AutoDL_sample_code_submission/at_nlp/run_model.py,0,"b'import numpy as np\nimport math\nimport gc\n\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.preprocessing import sequence\nfrom keras.callbacks import EarlyStopping, LearningRateScheduler\n\nfrom at_toolkit.at_utils import autodl_nlp_install_download\n\nautodl_nlp_install_download()\n\nfrom log_utils import info\nfrom at_nlp.data_manager.data_sampler import DataGenerator\nfrom at_nlp.data_manager.sample_utils import downsampling_input_data\nfrom at_nlp.utils import color_msg, ohe2cat\nfrom at_nlp.model_manager.emb_utils import _load_emb\nfrom at_nlp.data_manager.sample_config import sample_strategy\nfrom at_nlp.generators.model_generator import ModelGenerator\nfrom at_nlp.generators.feature_generator import FeatureGenerator\nfrom at_nlp.generators.data_generator import DataGenerator as BatchDataGenerator\nfrom at_nlp.data_manager.preprocess_utils import _tokenize_chinese_words\nfrom at_nlp.evaluator import Evaluator\nfrom at_toolkit.at_evalator import ATEvaluator\n\nfrom Auto_NLP.second_stage_models.model_iter_second_stage import Model as Second_Stage_Model\n\nINIT_BATCH_SIZE = 32\nMAX_SVM_FIT_NUM = 20000\nMAX_BERT_FIT_NUM = 3000\nMAX_EPOCH_NUM = 120\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\nconfig.log_device_placement = False  # to log device placement (on which device the operation ran)\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.9\nsess = tf.Session(config=config)\nK.set_session(sess)\n\n\nclass RunModel(object):\n    def __init__(self, metadata, train_output_path=""./"", test_input_path=""./""):\n        self.done_training = False\n        self.metadata = metadata\n        self.train_output_path = train_output_path\n        self.test_input_path = test_input_path\n\n        self.multi_label = False\n        self.multi_label_cnt_thred = 10\n        self.fasttext_embeddings_index = None\n\n        self.load_pretrain_emb = True\n        if self.load_pretrain_emb:\n            self.fasttext_embeddings_index = _load_emb(self.metadata[""language""])\n\n        self.model = None\n        self.accu_nn_tokenizer_x = True\n\n        self.second_stage_model = Second_Stage_Model(self.metadata, fasttext_emb=self.fasttext_embeddings_index)\n\n        ################# second stage model \xe7\x9b\xb8\xe5\x85\xb3 #########################\n        self.use_second_stage_model = False\n        self.start_second_stage_model = False\n        self.second_stage_patience = 0\n        self.second_stage_patience_max_num = 8\n        ################# second stage model \xe7\x9b\xb8\xe5\x85\xb3 #########################\n\n        self.call_num = 0\n        ## run model \xe9\x85\x8d\xe7\xbd\xae\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe9\x87\x87\xe6\xa0\xb7\xe5\x99\xa8\xef\xbc\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9e\x8b\xe9\x80\x89\xe6\x8b\xa9\xe5\x99\xa8\n        self.data_sampler = None\n        self.model_selector = None\n        self.imbalance_level = -1\n        self.start_cnn_call_num = 3\n\n        self.split_val_x = False\n        self.imbalance_flow_control = 1\n        self.feature_dict = {}\n        self.vocab = {}\n        self.time_record = {}\n        self.imbalance_level = -1\n        self.avg_word_per_sample = 0\n        self.use_multi_svm = True\n        self.max_length = 0\n        self.ft_model_check_length = 0\n        self.seq_len_std = 0\n\n        self.val_x = []\n        self.clean_val_x = []\n        self.val_y = np.array([])\n        self.build_tokenizer_x = []\n        self.build_tokenizer_y = np.array([])\n\n        self.first_stage_done = False\n        self.second_stage_done = False\n        self.third_stage_done = False\n        self.start_first_stage_call_num = 3\n        self.tokenize_test = False\n\n        self.switch_new_model = True\n        self.switch_new_feature_mode = False\n        self.cur_model_train_start = False\n        self.train_epoch = 0\n        self.max_train_epoch = 3\n        self.model_id = 0\n        self.train_model_id = -1\n        self.feature_id = -1\n\n        self.best_auc = 0.0\n\n        self.model_lib = [\'text_cnn\']\n\n        self.feature_lib = [\'char-level + 300dim-embedding\',\n                            \'word-level + pretrained embedding300dim\']\n\n        self.use_ft_model = False\n\n        self.callbacks = []\n        normal_lr = LearningRateScheduler(self.lr_decay)\n        step_lr = LearningRateScheduler(self.step_decay)\n        self.callbacks_ = [normal_lr, step_lr]\n        self.cur_lr = None\n\n        self.test_result_list = [0] * 30\n        self.cur_model_test_res = []\n        self.svm_test_result = []\n        self.model_weights_list = [[]] * 20\n        self.hist_test = [[]] * 20\n\n        self.is_best = False\n        self.best_call_num = 0\n        self.best_svm = 0.0\n        self.best_cnt = []\n        self.best_svm_scores = []\n\n        self.batch_size = INIT_BATCH_SIZE\n\n    def _show_runtime_info(self):\n        info(color_msg(""********************************************************""))\n        info(color_msg(""current model_id is {}, model_name is {}"".format(self.model_id, self.model_lib[self.model_id])))\n        info(color_msg(\n            ""current feature_id is {}, feature_name is {}"".format(self.feature_id, self.feature_lib[self.feature_id])))\n        info(color_msg(""train_model_id is {}"".format(self.train_model_id)))\n        info(color_msg(""********************************************************\\n""))\n\n    def _set_sampling_strategy(self, y_train):\n        strategy = sample_strategy[\'sample_iter_incremental_no_train_split\']\n\n        if y_train.shape[0] > 0:  # \xe5\xa6\x82\xe6\x9e\x9c\xe5\xbd\x93\xe5\x89\x8d\xe6\x9c\x89\xe5\xa2\x9e\xe9\x87\x8f\xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\x9b\xe6\xa0\xb7\n            if self.call_num == 0 or self.call_num >= self.start_cnn_call_num:\n                strategy = sample_strategy[\'sample_iter_incremental_no_train_split\']\n\n\n            elif self.call_num < self.start_cnn_call_num:\n                strategy = sample_strategy[""sample_iter_incremental_with_train_split""]\n\n\n            if self.start_cnn_call_num == self.imbalance_flow_control and not self.split_val_x:\n                strategy = sample_strategy[""sample_from_full_data""]\n\n\n        else:  # \xe5\xbd\x93\xe5\x89\x8d\xe5\xb7\xb2\xe6\x97\xa0\xe5\xa2\x9e\xe9\x87\x8f\xe6\x95\xb0\xe6\x8d\xae\n            if self.val_y.shape[0] > 0:  # \xe5\xa6\x82\xe6\x9e\x9c\xe5\xb7\xb2\xe7\xbb\x8f\xe6\x9c\x89val\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n                strategy = sample_strategy[""sample_from_full_train_data""]\n\n            else:\n                strategy = sample_strategy[""sample_from_full_data""]\n\n\n        return strategy\n\n    def do_data_sampling(self, y_train):\n        strategy = self._set_sampling_strategy(y_train)\n        return self.data_manager.sample_dataset_iter(add_val_to_train=strategy[""add_val_to_train""],\n                                                     update_train=strategy[""update_train""],\n                                                     use_full=strategy[""use_full""])\n\n    def run_svm(self, model_name, train_x, train_y):\n        self.feature_generator.tokenizer = None\n        if self.metadata[""language""] == ""ZH"" and self.call_num <= 1:\n            analyzer = ""char""\n        else:\n            analyzer = ""word""\n        self.feature_generator.build_tokenizer(train_x, \'svm\', analyzer)\n        # \xe5\x90\x8e\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe5\xb0\x86\xe6\x96\x87\xe6\x9c\xac\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba tfidf feature\n        if len(train_x) > MAX_SVM_FIT_NUM:\n            train_x = train_x[:MAX_SVM_FIT_NUM]\n            train_y = train_y[:MAX_SVM_FIT_NUM, :]\n\n        train_data = self.feature_generator.postprocess_data(train_x)\n        classifier = self.model_manager.select_classifier(model_name=model_name, feature_mode=None,\n                                                          data_feature=self.feature_generator.data_feature)\n\n        self.svm_token = self.feature_generator.tokenizer\n        if self.multi_label:\n            classifier.fit(train_data, train_y)\n        else:\n            classifier.fit(train_data, ohe2cat(train_y))\n        return classifier\n\n    def prepare_nn_tokenizer(self, train_x):\n        self.feature_generator.build_tokenizer(train_x, model_name=\'nn\')\n        self.feature_generator.set_data_feature()\n        self.tokenizer = self.feature_generator.tokenizer\n\n    def model_fit(self, train_x, train_y, model):\n        num_epochs = 1\n        bs_training_generator = BatchDataGenerator(train_x, train_y, batch_size=self.batch_size,\n                                                   language=self.metadata[""language""],\n                                                   max_length=self.feature_generator.max_length\n                                                   if self.feature_generator.max_length else 100,\n                                                   vocab=None,\n                                                   tokenizer=self.tokenizer,\n                                                   num_features=self.feature_generator.num_features)\n\n        model.fit_generator(generator=bs_training_generator, verbose=1,\n                            epochs=num_epochs,\n                            callbacks=self.callbacks,\n                            shuffle=True)\n        return model\n\n    def init_generators(self, x_train, y_train):\n        self.data_manager = DataGenerator(x_train, y_train, self.metadata, self.imbalance_level, self.multi_label)\n        self.model_manager = ModelGenerator(load_pretrain_emb=self.load_pretrain_emb,\n                                            fasttext_embeddings_index=self.fasttext_embeddings_index,\n                                            multi_label=self.multi_label)\n        # ZH \xe5\x89\x8d\xe6\x9c\x9f\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\x8d\xe5\x88\x87\xe5\x88\x86\xe8\xaf\x8d\xef\xbc\x8c\xe5\x90\x8e\xe6\x9c\x9f\xe9\x9c\x80\xe5\x88\x87\xe8\xaf\x8d\n        self.feature_generator = FeatureGenerator(self.metadata[""language""], do_seg=False,\n                                                  num_class=self.metadata[""class_num""])\n\n        self.evaluator = Evaluator(self.clean_val_x, self.val_y)\n\n    def process_val_data(self, val_diff_x, val_diff_y):\n        # \xe5\xa4\x84\xe7\x90\x86\xe5\xa2\x9e\xe9\x87\x8fval\xe6\x95\xb0\xe6\x8d\xae\n        if val_diff_x:\n            clean_val_x = self.feature_generator.preprocess_data(val_diff_x)\n            if self.val_y.shape[0] > 0:\n                self.val_x = np.concatenate([self.val_x, val_diff_x], axis=0)\n                self.val_y = np.concatenate([self.val_y, val_diff_y], axis=0)\n                self.clean_val_x = np.concatenate([self.clean_val_x, clean_val_x], axis=0)\n\n            else:\n                self.val_x = val_diff_x\n                self.val_y = val_diff_y\n                self.clean_val_x = clean_val_x\n            return clean_val_x\n        else:\n            return None\n\n    def _reset_train_status(self):\n        self.cur_model_train_start = False\n        self.switch_new_model = True\n        self.cur_lr = None\n        self.train_epoch = 0\n        self.best_cnt = []\n        self.model_weights_list[self.train_model_id] = self.evaluator.model_weights_list\n        self.cur_model_test_res = []\n        self.evaluator.max_epoch = 5\n        self.evaluator._reset()\n\n    def _clear_train_space(self):\n        del self.model\n        gc.collect()\n        K.clear_session()\n\n    def _init_nn_train_process(self):\n        self.feature_id += 1\n        self.train_model_id = self.model_id + self.feature_id * len(self.model_lib)\n        self._show_runtime_info()\n\n        self.model_weights_list[self.train_model_id] = []\n        self.cur_model_train_start = True\n        self.switch_new_model = False\n        self.train_epoch = 0\n        self.callbacks = []\n\n    def do_evaluation(self, eval_svm=True, update_val=True, update_setting=True):\n        if eval_svm:\n            tokenizer = self.svm_token\n        else:\n            tokenizer = self.tokenizer\n\n        if update_val:\n            self.evaluator.update_val_data(self.clean_val_x, self.val_y)\n        if update_setting:\n            self.evaluator.update_setting(self.metadata[""language""],\n                                          self.feature_generator.max_length,\n                                          self.feature_generator.num_features,\n                                          tokenizer)\n\n        self.evaluator.valid_auc(is_svm=eval_svm, model=self.model, use_autodl_auc=True)\n\n    def _train_nn_process(self, train_preprocessed_data, train_diff_y):\n        self.model = self.model_fit(train_preprocessed_data, train_diff_y, self.model)\n        if self.cur_model_train_start:  # \xe5\xa6\x82\xe6\x9e\x9c\xe5\xbd\x93\xe5\x89\x8d\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xad\xa3\xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe8\xaf\x84\xe4\xbc\xb0\xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x9c\n            if self.call_num == self.start_first_stage_call_num:\n                self.do_evaluation(eval_svm=False, update_val=True, update_setting=True)\n            else:\n                self.do_evaluation(eval_svm=False, update_val=False, update_setting=False)\n            self.evaluator.update_model_weights(self.model, self.train_epoch)\n\n            self.train_epoch += 1\n\n    def _ensemble_multi_models(self):\n        if self.call_num <= self.start_first_stage_call_num:\n            ensemble_condition = self.evaluator.best_auc > self.best_auc\n        else:\n            # \xe5\xaf\xb9\xe4\xba\x8e\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaaNN\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe8\xa6\x81\xe6\xb1\x82\xe8\xbe\xbe\xe5\x88\xb0\xe5\x89\x8d\xe4\xb8\x80\xe6\xac\xa1NN\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x95\x88\xe6\x9e\x9c\xe7\x9a\x8497%\n            ensemble_condition = self.evaluator.best_auc > 0.97 * self.best_auc\n\n        if ensemble_condition:\n            self.is_best = True  # \xe5\x85\x81\xe8\xae\xb8\xe5\xa4\x9a\xe6\xa8\xa1\xe5\x9e\x8b\xe8\x9e\x8d\xe5\x90\x88\n            self.best_auc = max(self.evaluator.best_auc, self.best_auc)\n\n        else:\n            self.is_best = False\n\n        self.best_cnt.append(self.is_best)\n\n        if self.call_num < self.start_first_stage_call_num:\n            self.best_svm = self.best_auc\n            self.best_svm_scores.append(self.best_svm)\n\n    def is_stage_done(self):\n        if self.model_id == len(self.model_lib) - 1:\n            if self.feature_id == len(self.feature_lib) - 1:\n                self.first_stage_done = True\n\n    def meta_strategy(self):\n\n        if self.max_length <= 156 and self.avg_word_per_sample <= 12:\n            self.use_ft_model = False\n            self.evaluator.max_epoch = 8\n            self.feature_lib = [\'char-level + 300dim-embedding\',\n                                \'word-level + pretrained embedding300dim\']\n\n        elif self.max_length > 156:\n            self.use_ft_model = False\n            self.second_stage_patience_max_num = 8\n            self.feature_lib = [\'char-level + 300dim-embedding\',\n                                \'word-level + pretrained embedding300dim\']\n\n\n        if self.imbalance_level == 2 and self.metadata[""language""] == ""EN"":\n            self.feature_lib = [\'char-level + 300dim-embedding\']\n\n        if self.metadata[""language""] == ""ZH"":\n            self.feature_lib = [\'word-level + pretrained embedding300dim\',\n                                \'char-level + 300dim-embedding\']\n            self.second_stage_patience_max_num = 8\n\n        if self.multi_label:\n            self.feature_lib = [\'char-level + 300dim-embedding\']\n\n    def _update_build_tokenizer_data(self):\n        pass\n\n    def prepare_clean_data(self, train_diff_x, val_diff_x, val_diff_y):\n        # \xe5\x89\x8d\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x9a\xe6\xa0\xb9\xe6\x8d\xae\xe7\xbb\x99\xe5\xae\x9a\xe7\x9a\x84\xe5\x89\x8d\xe5\xa4\x84\xe7\x90\x86\xe6\x96\xb9\xe5\xbc\x8f \xe6\xb8\x85\xe6\xb4\x97\xe6\x96\x87\xe6\x9c\xac\xe6\x95\xb0\xe6\x8d\xae, \xe9\xbb\x98\xe8\xae\xa4default\n        train_preprocessed_data = self.feature_generator.preprocess_data(train_diff_x)\n        self.process_val_data(val_diff_x, val_diff_y)\n        if self.call_num == 2 and self.metadata[""language""] == ""ZH"":\n            self.clean_val_x = list(map(_tokenize_chinese_words, self.clean_val_x))\n\n        if self.accu_nn_tokenizer_x:\n            if self.call_num == 2 and self.metadata[""language""] == ""ZH"":\n                self.build_tokenizer_x = train_preprocessed_data\n            else:\n\n                self.build_tokenizer_x = self.build_tokenizer_x + train_preprocessed_data\n\n        return train_preprocessed_data\n\n\n    def run_second_stage(self, remaining_time_budget):\n        if not self.start_second_stage_model:\n            self._clear_train_space()\n            self.start_second_stage_model = True\n            if self.imbalance_level == 2:\n                self.second_stage_model.split_val = False\n\n        if self.second_stage_model.model_id == len(\n                self.second_stage_model.cand_models) and self.second_stage_model.data_id == self.second_stage_model.max_data:\n            self.second_stage_done = True\n            info(""finish second stage!"")\n            return\n        if self.second_stage_patience >= self.second_stage_patience_max_num:\n            self.second_stage_model.epoch = 1\n            self.second_stage_patience = 0\n            do_clean = True\n        else:\n            do_clean = False\n\n        if self.second_stage_model.split_val:\n            self.second_stage_model.train_iter((self.data_manager.meta_data_x, self.data_manager.meta_data_y),\n                                               eval_dataset=(self.val_x, self.val_y),\n                                               remaining_time_budget=remaining_time_budget,\n                                               do_clean=do_clean)\n        else:\n            self.second_stage_model.train_iter((self.data_manager.meta_train_x, self.data_manager.meta_train_y),\n                                               eval_dataset=(self.val_x, self.val_y),\n                                               remaining_time_budget=remaining_time_budget,\n                                               do_clean=do_clean)\n\n        second_auc = self.second_stage_model.best_sco\n        self.evaluator.best_auc = second_auc\n        if second_auc == -1 or second_auc == 0.02:\n            second_auc = 0.0\n        if second_auc >= self.best_auc * 0.97 and second_auc > 0.0:\n\n            self.use_second_stage_model = True\n            if self.second_stage_model.Xtest is None and self.second_stage_model.FIRSTROUND:\n                self.second_stage_model.START = True\n            elif self.second_stage_model.Xtest is None and self.second_stage_model.new_data:\n                self.second_stage_model.START = False\n            return\n        else:\n\n            if self.second_stage_model.START == False and self.second_stage_model.FIRSTROUND == False and self.second_stage_model.LASTROUND:\n                self.second_stage_model.is_best = False\n                self.second_stage_model.LASTROUND = False\n            elif self.second_stage_model.START == True:  # \xe5\xa6\x82\xe6\x9e\x9cSTART\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xb2\xa1\xe6\x9c\x89\xe8\xb6\x85\xe8\xbf\x87\xe5\xbd\x93\xe5\x89\x8d\n                self.second_stage_model.START = False\n\n            self.use_second_stage_model = False\n            return\n\n    def run_first_stage_model(self, train_preprocessed_data, train_diff_y):\n        if self.switch_new_model and not self.cur_model_train_start:  # \xe5\xa6\x82\xe6\x9e\x9c\xe5\x88\x87\xe6\x8d\xa2\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe4\xb8\x94\xe5\xbd\x93\xe5\x89\x8d\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xb2\xa1\xe6\x9c\x89\xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\n            self._clear_train_space()\n            self._init_nn_train_process()\n            self.model = self.model_manager.select_classifier(model_name=self.model_lib[self.model_id],\n                                                              feature_mode=self.feature_lib[self.feature_id],\n                                                              data_feature=self.feature_generator.data_feature)\n            info(color_msg(""start new nn model training!""))\n\n            if self.model_lib[self.model_id] == ""text_cnn"":\n                if self.imbalance_level == 2 or self.metadata[""class_num""] >= 5:\n                    self.callbacks = []\n                else:\n                    self.callbacks = [self.callbacks_[0]]\n            else:\n                self.callbacks = [self.callbacks_[1]]\n\n        self._train_nn_process(train_preprocessed_data, train_diff_y)\n\n        if self.train_model_id >= 1:  # \xe8\xae\xad\xe7\xbb\x83\xe4\xba\x86\xe8\x87\xb3\xe5\xb0\x912\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9e\x8b\n            self._ensemble_multi_models()\n        else:\n            self._ensemble_multi_models()\n\n        #  \xe8\xbe\xbe\xe5\x88\xb0\xe7\xbb\x93\xe6\x9d\x9f\xe7\x9a\x84\xe6\x9d\xa1\xe4\xbb\xb6\n        if self.evaluator.decide_stop(train_epoch=self.train_epoch):\n            self._reset_train_status()\n\n    def train(self, x_train, y_train, remaining_time_budget=None):\n        if self.done_training:\n            return\n        if not self.use_multi_svm and self.metadata[""language""] == ""EN"":\n            self.start_first_stage_call_num = 1\n        if not self.use_multi_svm and self.metadata[""language""] == ""ZH"":\n            self.start_first_stage_call_num = 2\n        if self.use_multi_svm and self.multi_label:\n            self.start_first_stage_call_num = 2\n\n\n        if self.call_num == 0:\n            self.init_generators(x_train, y_train)\n            if self.use_multi_svm:\n                self.evaluator.max_epoch = 15\n            if self.multi_label:\n                self.evaluator.max_epoch = 18\n\n        else:\n            if self.call_num == 2 and self.metadata[""language""] == ""ZH"":\n                self.feature_generator.do_seg = True\n\n            if y_train.shape[0] > 0:\n                self.data_manager.update_meta_data(x_train, y_train)\n\n        # \xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x87\xe6\xa0\xb7\n        train_diff_x, train_diff_y, val_diff_x, val_diff_y = self.do_data_sampling(y_train)\n\n        ########################## \xe8\xae\xbe\xe5\xae\x9a\xe9\x87\x87\xe7\x94\xa8\xe7\x9a\x84\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe6\x96\xb9\xe5\xbc\x8f###########################################\n\n        ########################## \xe6\x95\xb0\xe6\x8d\xae\xe5\x89\x8d\xe5\xa4\x84\xe7\x90\x86 ####################################################\n        train_preprocessed_data = self.prepare_clean_data(train_diff_x, val_diff_x, val_diff_y)\n\n        ############################ SVM \xe9\x98\xb6\xe6\xae\xb5\xe6\xa8\xa1\xe5\x9e\x8b ############################\n        if self.call_num < self.start_first_stage_call_num:\n            self.model = self.run_svm(\'svm\', train_preprocessed_data, train_diff_y)\n\n            if self.val_y.shape[0] > 0:\n                self.do_evaluation(eval_svm=True)\n                self.evaluator.update_model_weights(self.model, self.train_epoch, is_svm=True)\n            self._ensemble_multi_models()\n\n        else:\n\n            if self.call_num == self.start_first_stage_call_num:\n                # \xe8\xbf\x9b\xe8\xa1\x8c\xe6\xa8\xa1\xe5\x9e\x8b\xe9\xa2\x84\xe9\x80\x89\xe6\x8b\xa9\n                self.meta_strategy()\n                self.feature_generator.reset_tokenizer()\n                # \xe8\xae\xbe\xe5\xae\x9a\xe6\x96\x87\xe6\x9c\xac\xe9\x95\xbf\xe5\xba\xa6\xe5\x8f\x8a\xe6\x96\x87\xe6\x9c\xac\xe9\x95\xbf\xe5\xba\xa6std\xef\xbc\x8c\xe5\xbd\xb1\xe5\x93\x8d\xe5\x90\x8e\xe5\xa4\x84\xe7\x90\x86pad\xe9\x95\xbf\xe5\xba\xa6\xe8\xae\xbe\xe7\xbd\xae\n                self.feature_generator.max_length = self.max_length\n                self.feature_generator.seq_len_std = self.seq_len_std\n                self.prepare_nn_tokenizer(train_x=self.build_tokenizer_x)\n                self.accu_nn_tokenizer_x = False\n\n            ############################ \xe8\xbf\x9b\xe5\x85\xa5\xe7\xac\xac\xe4\xba\x8c\xe9\x98\xb6\xe6\xae\xb5\xe6\xa8\xa1\xe5\x9e\x8b ############################\n            if self.first_stage_done:\n                if not self.multi_label:\n                    self.run_second_stage(remaining_time_budget)\n                else:\n                    info(color_msg(""do not run second stage model when multi_label is {}"".format(self.multi_label)))\n                    self.second_stage_done = False\n            ############################ \xe8\xbf\x9b\xe5\x85\xa5\xe7\xac\xac\xe4\xb8\x80\xe9\x98\xb6\xe6\xae\xb5\xe6\xa8\xa1\xe5\x9e\x8b ############################\n            else:\n                if self.switch_new_model and not self.cur_model_train_start:\n                    self.is_stage_done()\n                if not self.first_stage_done:\n                    self.run_first_stage_model(train_preprocessed_data, train_diff_y)\n\n        return\n\n    def _update_multi_model_result(self, pred):\n        if self.is_best:\n            self.test_result_list[self.train_model_id] = pred\n            result = np.mean(self.test_result_list[:self.train_model_id + 1], axis=0)\n        else:\n            if isinstance(self.test_result_list[self.train_model_id], int):\n                result = self.test_result_list[0]\n            else:\n                result = np.mean(self.test_result_list[:self.train_model_id + 1], axis=0)\n        return result\n\n    def transform_test(self):\n        if self.call_num < self.start_first_stage_call_num:\n            x_test_feature = self.svm_token.transform(self.x_test_clean)\n            return x_test_feature\n\n        else:\n            if not self.tokenize_test:\n                x_test_feature = self.tokenizer.texts_to_sequences(self.x_test_clean)\n                x_test = sequence.pad_sequences(x_test_feature,\n                                                maxlen=self.feature_generator.max_length,\n                                                padding=\'post\')\n                self.tokenize_test = True\n            else:\n                x_test = self.x_test\n            return x_test\n\n    def output_svm_result(self):\n        result = self.model.predict_proba(self.x_test)\n        if self.is_best:\n            self.svm_test_result.append(result)\n        elif self.call_num > 0:\n            result = self.svm_test_result[-1]\n        return result\n\n    def output_second_stage_result(self):\n        info(""Output in second stage!"")\n        # \xe7\xac\xac\xe4\xba\x8c\xe9\x98\xb6\xe6\xae\xb5\xe6\xb2\xa1\xe6\x9c\x89\xe7\xbb\x93\xe6\x9d\x9f\xef\xbc\x9a\xe5\x8f\xaa\xe6\x9c\x89\xe4\xb8\xa4\xe4\xb8\xaa\xe9\x80\x89\xe6\x8b\xa9\xef\xbc\x9asecond_stage \xe6\xa8\xa1\xe5\x9e\x8b or \xe7\xac\xac\xe4\xb8\x80\xe9\x98\xb6\xe6\xae\xb5\xe6\x9c\x80\xe4\xbc\x98\xe6\xa8\xa1\xe5\x9e\x8b\n        if self.use_second_stage_model:\n            self.second_stage_patience = 0\n            info(color_msg(""Use second_stage Model!!""))\n            second_stage_result = self.second_stage_model.test(self.x_test_raw)\n\n            # info(color_msg(""second_stage result is {}"".format(type(second_stage_result))))\n            # if isinstance(second_stage_result, list):\n            #     info(color_msg(""second_stage result is {}"".format(len(second_stage_result))))\n            # if isinstance(second_stage_result, np.ndarray):\n            #     info(color_msg(""second_stage result is {}"".format(second_stage_result.shape[0])))\n            # if isinstance(second_stage_result, np.float):\n            #     info(color_msg(""second_stage result is {}"".format(second_stage_result)))\n\n            # \xe5\xa6\x82\xe6\x9e\x9csecond_stage\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\xba\xe7\xa9\xba\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe9\x98\xb6\xe6\xae\xb5\xe7\xbb\x93\xe6\x9e\x9c\n            if second_stage_result.shape[0] == 0:\n                if isinstance(self.test_result_list[2], int):\n                    result = np.mean(self.test_result_list[:2], axis=0)\n                else:\n                    result = np.mean(self.test_result_list[:3], axis=0)\n                return result\n            else:\n                self.test_result_list[2] = second_stage_result\n                result = np.mean(self.test_result_list[:3], axis=0)\n                return result\n        else:\n            info(color_msg(""Do Not Use second_stage Model!! second_stage_patience is {}"".format(self.second_stage_patience)))\n            self.second_stage_patience += 1\n            if self.start_second_stage_model:\n                if isinstance(self.test_result_list[2], int):\n                    result = np.mean(self.test_result_list[:2], axis=0)\n                else:\n                    result = np.mean(self.test_result_list[:3], axis=0)\n            else:\n                if self.train_model_id == 0:\n                    result = self.test_result_list[0]\n                else:\n                    result = np.mean(self.test_result_list[:2], axis=0)\n            return result\n\n    def output_first_stage_result_with_svm(self, result):\n        if self.is_best:\n            if np.std([np.max(self.best_svm_scores), self.best_auc]) < 0.005:\n                self.svm_test_result.append(result)\n                result = np.mean(self.svm_test_result, axis=0)\n            else:\n                self.multi_label_cnt_thred-=1\n                result = result\n        else:\n            if np.std([np.max(self.best_svm_scores), self.best_auc]) < 0.02:\n                self.svm_test_result.append(result)\n                result = np.mean(self.svm_test_result, axis=0)\n                self.svm_test_result.pop(-1)\n            else:\n                result = self.svm_test_result[-1]\n        return result\n\n    def output_first_stage_result(self):\n        result = self.model.predict(self.x_test,\n                                    batch_size=self.batch_size * 16)\n        self.cur_model_test_res.append(result)\n        if self.train_model_id == 0 and not self.cur_model_train_start:\n            self.test_result_list[0] = self.cur_model_test_res[-1]\n\n        if self.train_model_id==1 and not self.cur_model_train_start:\n            if isinstance(self.test_result_list[self.train_model_id], int):\n\n                self.test_result_list[1] = self.test_result_list[0]\n\n        if self.train_model_id >= 1:\n            result = self._update_multi_model_result(result)\n\n        if self.call_num == self.start_first_stage_call_num:\n            if self.is_best:\n                if np.std([np.max(self.best_svm_scores), self.best_auc]) < 0.008:\n                    self.svm_test_result.append(result)\n                    result = np.mean(self.svm_test_result, axis=0)\n                else:\n                    result = result\n            else:\n                if np.std([np.max(self.best_svm_scores), self.best_auc]) < 0.02:\n                    self.svm_test_result.append(result)\n                    result = np.mean(self.svm_test_result, axis=0)\n                    self.svm_test_result.pop(-1)\n                else:\n                    result = self.svm_test_result[-1]\n\n        if self.multi_label:\n            if self.train_model_id >= 1:\n                result = self._update_multi_model_result(result)\n            else:\n                result = self.output_first_stage_result_with_svm(result)\n\n        return result\n\n    def test(self, x_test, remaining_time_budget):\n        if self.call_num == 0:\n            self.x_test_raw = x_test\n            self.x_test_clean = self.feature_generator.preprocess_data(self.x_test_raw)\n\n        if self.metadata[""language""] == ""ZH"" and self.call_num == 2:\n            # feature.do_seg \xe5\xb7\xb2\xe7\xbb\x8f\xe6\x9b\xb4\xe6\x96\xb0\n            self.x_test_clean = self.feature_generator.preprocess_data(self.x_test_raw)\n\n        self.x_test = self.transform_test()\n\n        # \xe8\xbe\x93\xe5\x87\xbasvm \xe7\xbb\x93\xe6\x9e\x9c\n        if self.call_num < self.start_first_stage_call_num:\n            result = self.output_svm_result()\n\n        elif self.second_stage_done:\n            result = np.mean(self.test_result_list[:3], axis=0)\n\n        elif self.first_stage_done:\n            if self.multi_label:\n                if self.multi_label_cnt_thred<0:\n                    result = self.cur_model_test_res[-1]\n                    return result\n                else:\n                    result = self.svm_test_result[-1]\n            else:\n                result = self.output_second_stage_result()\n\n        else:\n            ## \xe5\xbd\x93\xe5\x89\x8d\xe4\xb8\xbaNN\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\x93\xe6\x9e\x9c\n            result = self.output_first_stage_result()\n\n        self.done_training = False\n        self.call_num += 1\n        if self.call_num == MAX_EPOCH_NUM:\n            self.done_training = True\n            self.ft_model = None\n        return result\n\n    def lr_decay(self, epoch):\n        if self.call_num == 1 or self.cur_lr is None:\n            self.cur_lr = self.model_manager.lr\n        if self.train_epoch % 3 == 0 and self.train_epoch > 0:\n            self.cur_lr = 3 * self.cur_lr / 5\n        self.cur_lr = max(self.cur_lr, 0.0001)\n        lr = self.cur_lr\n        return lr\n\n    def step_decay(self, epoch):\n        epoch = self.train_epoch // 3\n        initial_lrate = self.model_manager.lr  # 0.016 #0.0035 #\n        drop = 0.65  # 0.65\n        epochs_drop = 1.0  # 2.0\n        if (self.train_epoch) <= 2:\n            lrate = initial_lrate\n        else:\n            lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n        lrate = max(lrate, 0.0001)\n        return lrate\n'"
AutoDL_sample_code_submission/at_nlp/utils.py,0,"b'import numpy as np\n\nfrom sklearn.multiclass import OneVsRestClassifier\n\ndef color_msg(msg, color=""red""):\n    if color == ""red"":\n        return \'\\033[31m{}\\033[0m\'.format(msg)\n\n    elif color == ""blue"":\n        return \'\\033[34m{}\\033[0m\'.format(msg)\n\n    elif color == ""yellow"":\n        return \'\\033[33m{}\\033[0m\'.format(msg)\n\n    elif color == ""green"":\n        return \'\\033[36m{}\\033[0m\'.format(msg)\n\n\n# onhot encode to category\ndef ohe2cat(label):\n    return np.argmax(label, axis=1)\n\n\n\n'"
AutoDL_sample_code_submission/at_speech/__init__.py,0,"b'import os\n\nfrom at_speech.data_space import DNpAugPreprocessor, MixupGenerator, TTAGenerator\nfrom at_speech.backbones.thinresnet34 import build_tr34_model\nfrom at_speech.data_space.examples_gen_maker import DataGenerator as Tr34DataGenerator\nfrom at_speech.classifier import SLLRLiblinear, SLLRSag, ThinResnet34Classifier\n\n'"
AutoDL_sample_code_submission/at_speech/at_speech_config.py,0,"b'from at_speech.at_speech_cons import CLS_LR_LIBLINEAER, CLS_LR_SAG, CLS_TR34\n\n\n\n# for data01\nMODEL_SELECT_DEF = {\n    0: CLS_LR_LIBLINEAER,\n    1: CLS_LR_LIBLINEAER,\n    2: CLS_TR34,\n}\n\n\nTR34_TRAINPIP_WARMUP = 2\nIF_VAL_ON = False\n\n\nTFDS2NP_TAKESIZE_RATION_LIST = [0.1, 0.2, 0.3, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n\nIF_TRAIN_BREAK_CONDITION = False\n\n\nclass Tr34SamplerHpParams:\n    SAMPL_PA_F_PERC_NUM = 10\n    SAMPL_PA_F_MAX_NUM = 200\n    SAMPL_PA_F_MIN_NUM = 200\n\n\nclass ThinRes34Config(object):\n    ENABLE_CB_LRS = True\n    ENABLE_CB_ES = True\n\n    Epoch = 1\n    VERBOSE = 2\n    MAX_SEQ_NUM_CUT = 140000\n\n    MAX_ROUND_NUM = 3000\n    MAX_LEFT_TIME_BUD = 10\n    TR34_INIT_WD = 1e-3\n    INIT_BRZ_L_NUM = 124\n    INIT_BRZ_L_NUM_WILD = 100\n    PRED_SIZE = 8\n    CLASS_NUM_THS = 37\n\n    ENABLE_PRE_ENSE = True\n    ENS_TOP_VLOSS_NUM = 8\n    ENS_TOP_VACC_NUM = 0\n\n    MAX_BATCHSIZE = 32\n    FIRST_ROUND_EPOCH = 8\n    LEFT_ROUND_EPOCH = 1\n\n\n    TR34_INIT_LR = 0.00175\n    STEP_DE_LR = 0.002\n    MAX_LR = 1e-3 * 1.5\n    MIN_LR = 1e-4 * 5\n\n    FULL_VAL_R_START = 2500\n    G_VAL_CL_NUM = 3\n    G_VAL_T_MAX_MUM = 0\n    G_VAL_T_MIN_NUM = 0\n\n    HIS_METRIC_SHOW_NUM = 10\n\n    FE_RS_SPEC_LEN_CONFIG = {\n        1: (125, 125, 0.002),\n        5: (250, 250, 0.003),\n        20: (1500, 1500, 0.004),\n        50: (2250, 2250, 0.004),\n    }\n\n    FE_RS_SPEC_LEN_CONFIG_AGGR = {\n        1: (150, 150, 0.002),\n        10: (500, 500, 0.003),\n        21: (1500, 1500, 0.004),\n        50: (2250, 2250, 0.004),\n    }\n\n    FE_RS_SPEC_LEN_CONFIG_MILD = {\n        1: (350, 350, 0.002),\n        10: (500, 500, 0.004),\n        20: (1000, 1000, 0.002),\n        50: (1500, 1500, 0.004),\n    }\n'"
AutoDL_sample_code_submission/at_speech/at_speech_cons.py,0,"b'import os\n\nAT_SPEECH_DIR = os.path.abspath(os.path.dirname(__file__))\nTR34_PRETRAIN_PATH = os.path.join(AT_SPEECH_DIR, ""pretrained_models/thin_resnet34.h5"")\n\nFEAT_KAPRE_MELSPECGRAM = ""FEAT_KAPRE_MELSPECGRAM""\nFEAT_LBS_MELSPECGRAM = ""FEAT_LBS_MELSPECGRAM""\nFEAT_LBS_TR34 = ""FEAT_LBS_TR34""\n\nCLS_LR_LIBLINEAER = ""LR_LIBLINEAR_CLS""\nCLS_LR_SAG = ""LR_SAG_CLS""\nCLS_TR34 = ""TR34_CLS""\n\n'"
AutoDL_sample_code_submission/at_speech/model.py,0,"b'""""""Combine all winner solutions in previous challenges (AutoCV, AutoCV2,\nAutoNLP and AutoSpeech).\n""""""\n\nimport logging\nimport numpy as np\nimport os\nimport sys\nimport tensorflow as tf\n\nconfig = tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)\nconfig.gpu_options.allow_growth = True\n\nhere = os.path.dirname(os.path.abspath(__file__))\nimport_dir = os.path.abspath(os.path.join(here, ""..""))\nsys.path.append(import_dir)\n\nsys.path.append(os.path.join(here, """"))\nfrom at_toolkit.at_utils import autodl_install_download\n\nautodl_install_download(""speech"")\n\nfrom at_speech.policy_space.model_executor import ModelExecutor\n# from at_toolkit import logger, info, error, as_timer\nfrom at_speech.at_speech_config import IF_TRAIN_BREAK_CONDITION\nfrom at_speech.at_speech_cons import CLS_TR34\n\nEVAL_TLOSS_TAIL_SIZE = 8\nEVAL_TLOSS_GODOWN_RATE_THRES = 0.7\n\n\nclass Model:\n    """"""A model that combine all winner solutions. Using domain inferring and\n    apply winner solution in the corresponding domain.""""""\n\n    def __init__(self, metadata):\n        """"""\n        Args:\n            metadata: an AutoDLMetadata object. Its definition can be found in\n            AutoDL_ingestion_program/dataset.py\n        """"""\n        self.done_training = False\n        self.metadata = metadata\n        self.domain = ""speech""\n        test_metadata_filename = self.metadata.get_dataset_name().replace(\'train\', \'test\') + \'/metadata.textproto\'\n        self.test_num = [int(line.split(\':\')[1]) for line in open(test_metadata_filename, \'r\').readlines()[:3] if \'sample_count\' in line][0]\n\n        self.domain_metadata = get_domain_metadata(metadata, self.domain)\n        self.domain_metadata[""test_num""] = self.test_num\n        self.class_num = self.domain_metadata[""class_num""]\n        self.train_num = self.domain_metadata[""train_num""]\n\n        self.domain_model = ModelExecutor(self.domain_metadata)\n        self.ensemble_val_record_list = list()\n        self.ensemble_val_nauc_list = list()\n        self.cur_cls_name = None\n        self.cur_train_his_report = dict()\n        self.g_predevel_space = list()\n        self.g_train_loss_list = list()\n\n    def get_accept_nauc(self):\n        if len(self.ensemble_val_nauc_list) == 0:\n            return 0\n        if max(self.ensemble_val_nauc_list) <= 0.9:\n            return max(self.ensemble_val_nauc_list)*0.98\n        elif 0.9 < max(self.ensemble_val_nauc_list) <= 0.95:\n            return max(self.ensemble_val_nauc_list) * 0.99\n        elif 0.95 < max(self.ensemble_val_nauc_list) <= 1:\n            return max(self.ensemble_val_nauc_list) * 0.996\n        else:\n            return 0\n\n    def train(self, dataset, remaining_time_budget=None):\n        """"""Train method of domain-specific model.""""""\n        if IF_TRAIN_BREAK_CONDITION:\n            while True:\n                self.cur_train_his_report = self.domain_model.train_pipeline(dataset)\n                self.cur_cls_name = self.cur_train_his_report.get(""cls_name"")\n\n                cur_val_nauc = self.cur_train_his_report[""val_nauc""]\n                self.ensemble_val_record_list.append([self.cur_cls_name, cur_val_nauc])\n                self.ensemble_val_nauc_list.append(cur_val_nauc)\n                if cur_val_nauc == -1 or cur_val_nauc > self.get_accept_nauc():\n                    break\n                else:\n                    pass\n\n        else:\n            self.cur_train_his_report = self.domain_model.train_pipeline(dataset)\n            self.cur_cls_name = self.cur_train_his_report.get(""cls_name"")\n\n            cur_t_loss = self.cur_train_his_report.get(""t_loss"")\n            if cur_t_loss is None:\n                self.g_train_loss_list.append(100000)\n            else:\n                self.g_train_loss_list.append(cur_t_loss)\n\n            cur_val_nauc = self.cur_train_his_report[""val_nauc""]\n\n            self.ensemble_val_record_list.append([self.cur_cls_name, cur_val_nauc])\n            self.ensemble_val_nauc_list.append(cur_val_nauc)\n\n\n    def test(self, dataset, remaining_time_budget=None):\n        """"""Test method of domain-specific model.""""""\n        cur_y_pred = self.domain_model.test_pipeline(dataset)\n\n        self.cur_train_his_report[""pred_probas""] = cur_y_pred\n\n        if self.cur_cls_name == CLS_TR34 and self.domain_model.tr34_cls_train_pip_run >= 8:\n            loss_godown_rate = self.domain_model.decision_maker.ensemble_learner.get_loss_godown_rate(self.g_train_loss_list, EVAL_TLOSS_TAIL_SIZE)\n            if loss_godown_rate >= EVAL_TLOSS_GODOWN_RATE_THRES:\n                self.domain_model.decision_maker.ensemble_learner.add_eval_pred_item(self.cur_train_his_report)\n\n        if self.domain_model.tr34_cls_train_pip_run >= 15:\n            pred_rule = {\n                ""t_loss"": 5,\n                # ""t_acc"": 1\n            }\n            pred_ensemble = self.domain_model.decision_maker.ensemble_learner.softvoting_ensemble_preds(pred_rule)\n        else:\n            pred_ensemble = cur_y_pred\n\n        self.done_training = False\n        return pred_ensemble\n\n\ndef get_domain_metadata(metadata, domain, is_training=True):\n    # Specific for Speech.\n    """"""Recover the metadata in corresponding competitions, esp. AutoNLP\n  and AutoSpeech.\n\n  Args:\n    metadata: an AutoDLMetadata object.\n    domain: str, can be one of \'image\', \'video\', \'text\', \'speech\' or \'tabular\'.\n  """"""\n\n    # Fetch metadata info from `metadata`\n    class_num = metadata.get_output_size()\n    num_examples = metadata.size()\n\n    # WARNING: hard-coded properties\n    file_format = ""wav""\n    # sample_rate = 16000\n    sample_rate = 8000\n\n    # Create domain metadata\n    domain_metadata = {}\n    domain_metadata[""class_num""] = class_num\n    if is_training:\n        domain_metadata[""train_num""] = num_examples\n        domain_metadata[""test_num""] = -1\n    else:\n        domain_metadata[""train_num""] = -1\n        domain_metadata[""test_num""] = num_examples\n    domain_metadata[""file_format""] = file_format\n    domain_metadata[""sample_rate""] = sample_rate\n\n    return domain_metadata\n\n'"
AutoDL_sample_code_submission/at_toolkit/__init__.py,0,"b'from at_toolkit.at_utils import logger, debug, info, warning, error\nfrom at_toolkit.interface.adl_classifier import AdlClassifier, AdlOfflineClassifier, AdlOnlineClassifier\nfrom at_toolkit.interface.adl_metadata import AdlSpeechDMetadata\nfrom at_toolkit.at_evalator import ATEvaluator\n\nfrom at_toolkit.at_cons import SPEECH_TR34_PT_MODEL_PATH'"
AutoDL_sample_code_submission/at_toolkit/at_cons.py,0,"b'import os\n\nADL_ROOT_PATH = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))\nprint(ADL_ROOT_PATH)\n\nTR34_FN = ""thin_resnet34.h5""\n\nSPEECH_TR34_PT_MODEL_DIR = os.path.join(ADL_ROOT_PATH, ""at_speech"", ""pretrained_models"")\nSPEECH_TR34_PT_MODEL_PATH = os.path.join(SPEECH_TR34_PT_MODEL_DIR, TR34_FN)\n\n'"
AutoDL_sample_code_submission/at_toolkit/at_evalator.py,0,"b'import numpy as np\nfrom sklearn.metrics import roc_auc_score\nfrom functools import reduce\n# from tools import log_warning\nfrom at_toolkit import warning as log_warning\n\ndef get_valid_columns(solution):\n    """"""Get a list of column indices for which the column has more than one class.\n    This is necessary when computing BAC or AUC which involves true positive and\n    true negative in the denominator. When some class is missing, these scores\n    don\'t make sense (or you have to add an epsilon to remedy the situation).\n\n    Args:\n        solution: array, a matrix of binary entries, of shape\n        (num_examples, num_features)\n    Returns:\n        valid_columns: a list of indices for which the column has more than one\n        class.\n    """"""\n    num_examples = solution.shape[0]\n    col_sum = np.sum(solution, axis=0)\n    valid_columns = np.where(1 - np.isclose(col_sum, 0) -\n                             np.isclose(col_sum, num_examples))[0]\n    return valid_columns\n\n\ndef tiedrank(a):\n    \'\'\' Return the ranks (with base 1) of a list resolving ties by averaging.\n     This works for numpy arrays.\'\'\'\n    m = len(a)\n    # Sort a in ascending order (sa=sorted vals, i=indices)\n    i = a.argsort()\n    sa = a[i]\n    # Find unique values\n    uval = np.unique(a)\n    # Test whether there are ties\n    R = np.arange(m, dtype=float) + 1  # Ranks with base 1\n    if len(uval) != m:\n        # Average the ranks for the ties\n        oldval = sa[0]\n        k0 = 0\n        for k in range(1, m):\n            if sa[k] != oldval:\n                R[k0: k] = sum(R[k0: k]) / (k - k0)\n                k0 = k\n                oldval = sa[k]\n        R[k0: m] = sum(R[k0: m]) / (m - k0)\n    # Invert the index\n    S = np.empty(m)\n    S[i] = R\n    return S\n\n\ndef mvmean(R, axis=0):\n    \'\'\' Moving average to avoid rounding errors. A bit slow, but...\n    Computes the mean along the given axis, except if this is a vector, in which case the mean is returned.\n    Does NOT flatten.\'\'\'\n    if len(R.shape) == 0: return R\n    average = lambda x: reduce(lambda i, j: (0, (j[0] / (j[0] + 1.)) * i[1] + (1. / (j[0] + 1)) * j[1]), enumerate(x))[\n        1]\n    R = np.array(R)\n    if len(R.shape) == 1: return average(R)\n    if axis == 1:\n        return np.array(map(average, R))\n    else:\n        return np.array(map(average, R.transpose()))\n\n\nclass ATEvaluator(object):\n    # Metric used to compute the score of a point on the learning curve\n    @staticmethod\n    def autodl_auc(solution, prediction, valid_columns_only=True):\n        """"""Compute normarlized Area under ROC curve (AUC).\n        Return Gini index = 2*AUC-1 for  binary classification problems.\n        Should work for a vector of binary 0/1 (or -1/1)""solution"" and any discriminant values\n        for the predictions. If solution and prediction are not vectors, the AUC\n        of the columns of the matrices are computed and averaged (with no weight).\n        The same for all classification problems (in fact it treats well only the\n        binary and multilabel classification problems). When `valid_columns` is not\n        `None`, only use a subset of columns for computing the score.\n        """"""\n        if valid_columns_only:\n            valid_columns = get_valid_columns(solution)\n            if len(valid_columns) < solution.shape[-1]:\n                log_warning(""Some columns in solution have only one class, "" +\n                            ""ignoring these columns for evaluation."")\n            solution = solution[:, valid_columns].copy()\n            prediction = prediction[:, valid_columns].copy()\n        label_num = solution.shape[1]\n        auc = np.empty(label_num)\n        for k in range(label_num):\n            r_ = tiedrank(prediction[:, k])\n            s_ = solution[:, k]\n            if sum(s_) == 0: print(""WARNING: no positive class example in class {}"" \\\n                                   .format(k + 1))\n            npos = sum(s_ == 1)\n            nneg = sum(s_ < 1)\n            auc[k] = (sum(r_[s_ == 1]) - npos * (npos + 1) / 2) / (nneg * npos)\n        return 2 * mvmean(auc) - 1\n\n    @staticmethod\n    def auc_metric(solution, prediction, task=\'binary.classification\'):\n        \'\'\'roc_auc_score() in sklearn is fast than code provided by sponsor\n        \'\'\'\n        if solution.sum(axis=0).min() == 0:\n            return np.nan\n        auc = roc_auc_score(solution, prediction, average=\'macro\')\n        return np.mean(auc * 2 - 1)\n\n    @staticmethod\n    def skl_auc_macro(solution, prediction):\n        valid_score = roc_auc_score(solution, prediction, average=\'macro\')\n        return valid_score\n'"
AutoDL_sample_code_submission/at_toolkit/at_sampler.py,0,"b'import numpy as np\nimport random\nfrom sklearn.model_selection import train_test_split\n\n# from at_toolkit import info, error\n\ndef ohe2cat(label):\n    return np.argmax(label, axis=1)\n\ndef ohe2cat_new(y_labels):\n    assert isinstance(y_labels, np.ndarray)\n    one_index = np.argwhere(y_labels == 1)\n    sample_num = y_labels.shape[0]\n    cat_list = [-1] * sample_num\n    for index_item in one_index:\n        cat_list[index_item[0]] = index_item[1]\n\n    return cat_list\n\n\nclass AutoSamplerBasic:\n    def __init__(self, class_num):\n        self._num_classes = class_num\n        self._max_class_num = 0\n        self._min_class_num = 0\n        self.train_y = None\n        self._train_y_num = None\n\n    def init_train_y(self, train_y):\n        self.train_y = train_y\n        self._train_y_num = len(self.train_y)\n\n    def init_each_class_index_by_y(self, cur_train_y):\n        cur_train_y = np.stack(cur_train_y)\n        each_class_count = np.sum(np.array(cur_train_y), axis=0)\n        self._max_class_num, self._min_class_num = int(np.max(each_class_count)), int(np.min(each_class_count))\n\n        each_class_index = []\n        for i in range(self._num_classes):\n            each_class_index.append(\n                list(np.where(cur_train_y[:, i] == 1)[0]))\n\n        return each_class_index\n\n    def init_even_class_index_by_each(self, each_class_index_list):\n        even_class_index = []\n        sample_per_class = max(int(self._train_y_num / self._num_classes), 1)\n        for i in range(self._num_classes):\n            class_cnt = len(each_class_index_list[i])\n            tmp = []\n            if class_cnt == 0:\n                pass\n            elif class_cnt < sample_per_class:\n                tmp = each_class_index_list[i] * \\\n                      int(sample_per_class / class_cnt)\n                tmp += random.sample(\n                    each_class_index_list[i],\n                    sample_per_class - len(tmp))\n            else:\n                tmp += random.sample(\n                    each_class_index_list[i], sample_per_class)\n            random.shuffle(tmp)\n            even_class_index.append(tmp)\n\n        return even_class_index\n\n\nclass AutoSpeechEDA(object):\n    def __init__(self, data_expe_flag=False, x_train=None, y_train=None):\n        self.train_data_sinffer_num_per_class = 50\n        self.expe_x_data_num = 3\n        self.expe_y_labels_num = 3\n        self.data_expe_flag = data_expe_flag\n\n    def get_y_label_distribution_by_bincount(self, y_onehot_labels):\n        y_sample_num, y_label_num = y_onehot_labels.shape\n        y_as_label = ohe2cat(y_onehot_labels)\n        y_as_label_bincount = np.bincount(y_as_label)\n        y_label_distribution_array = y_as_label_bincount / y_sample_num\n        y_label_distribution_array = list(y_label_distribution_array)\n        return y_label_distribution_array\n\n    def get_y_label_eda_report(self, y_onehot_labels):\n        y_train_len_num = len(y_onehot_labels)\n        if self.data_expe_flag:\n            expe_x_data_list = [a_x_data.tolist() for a_x_data in y_onehot_labels[:self.expe_x_data_num]]\n        y_sample_num, y_label_num = y_onehot_labels.shape\n        y_label_distribution_array = self.get_y_label_distribution_by_bincount(y_onehot_labels=y_onehot_labels)\n        eda_y_report = dict()\n        eda_y_report[""y_sample_num""] = int(y_sample_num)\n        eda_y_report[""y_class_num""] = int(y_label_num)\n        eda_y_report[""y_label_distribution_array""] = y_label_distribution_array\n        return eda_y_report\n\n    def get_x_data_report(self, x_data):\n        x_sample_num = len(x_data)\n        if self.data_expe_flag:\n            expe_x_data_list = [a_x_data.tolist() for a_x_data in x_data[:self.expe_x_data_num]]\n        x_train_word_len_list = list()\n        for x_train_sample in x_data:\n            len_a_x_sample = x_train_sample.shape[0]\n            x_train_word_len_list.append(len_a_x_sample)\n        x_train_word_len_array = np.array(x_train_word_len_list)\n        x_train_sample_mean = x_train_word_len_array.mean()\n        x_train_sample_std = x_train_word_len_array.std()\n\n        eda_x_data_report = dict()\n        eda_x_data_report[""x_total_seq_num""] = int(x_train_word_len_array.sum())\n        eda_x_data_report[""x_seq_len_mean""] = int(x_train_sample_mean)\n        eda_x_data_report[""x_seq_len_std""] = x_train_sample_std\n        eda_x_data_report[""x_seq_len_max""] = int(x_train_word_len_array.max())\n        eda_x_data_report[""x_seq_len_min""] = int(x_train_word_len_array.min())\n        eda_x_data_report[""x_seq_len_median""] = int(np.median(x_train_word_len_array))\n        eda_x_data_report[""x_sample_num""] = int(x_sample_num)\n        return eda_x_data_report\n\n\nclass AutoSpSamplerNew(object):\n    def __init__(self, y_train_labels):\n\n        self.autosp_eda = AutoSpeechEDA()\n        self.y_train_labels = y_train_labels\n        self.y_train_cat_list = None\n        self.y_class_num = None\n        self.g_label_sample_id_list = list()\n\n    def set_up(self):\n\n        self.y_train_cat_list = ohe2cat(self.y_train_labels)\n        y_labels_eda_report = self.autosp_eda.get_y_label_eda_report(y_onehot_labels=self.y_train_labels)\n        self.y_class_num = y_labels_eda_report.get(""y_class_num"")\n        for y_label_id in range(self.y_class_num):\n            label_sample_id_list = list(np.where(self.y_train_cat_list == y_label_id)[0])\n            self.g_label_sample_id_list.append(label_sample_id_list)\n\n    def get_downsample_index_list_by_class(self, per_class_num, max_sample_num, min_sample_num):\n        train_data_sample_id_list = list()\n        min_sample_perclass = int(min_sample_num / self.y_class_num)\n        for y_label_id in range(self.y_class_num):\n            random_sample_k = per_class_num\n            random_sample_k = max(min_sample_perclass, random_sample_k)\n            label_sample_id_list = self.g_label_sample_id_list[y_label_id]\n            if len(label_sample_id_list) > random_sample_k:\n                downsampling_label_sample_id_list = random.sample(population=label_sample_id_list, k=random_sample_k)\n                train_data_sample_id_list.extend(downsampling_label_sample_id_list)\n            else:\n                train_data_sample_id_list.extend(label_sample_id_list)\n\n        if len(train_data_sample_id_list) > max_sample_num:\n            train_data_sample_id_list = random.sample(population=train_data_sample_id_list, k=max_sample_num)\n        return train_data_sample_id_list\n\n\n    def get_downsample_index_list_by_random(self, max_sample_num, min_sample_num):\n\n        assert min_sample_num <= max_sample_num, ""Error: min_sample_num={}, max_sample_num={}"".format(min_sample_num, max_sample_num)\n\n        sample_num, class_num = self.y_train_labels.shape[0], self.y_train_labels.shape[1]\n        if sample_num <= min_sample_num:\n            cur_sample_idxs = list(range(sample_num))\n            random.shuffle(cur_sample_idxs)\n        elif min_sample_num < sample_num <= max_sample_num:\n            cur_sample_idxs = list(range(sample_num))\n            random.shuffle(cur_sample_idxs)\n        elif sample_num > max_sample_num:\n            cur_sample_idxs = random.sample(population=range(sample_num), k=max_sample_num)\n        else:\n            cur_sample_idxs = list(range(sample_num))\n            random.shuffle(cur_sample_idxs)\n\n        return cur_sample_idxs\n\n\nclass AutoValidSplitor:\n    def __init__(self, class_num):\n        self.class_num = class_num\n        self.ohe_y_labels = None\n\n    def get_valid_sample_idxs(self, ohe_y_labels, val_num, mode=""random""):\n        self.ohe_y_labels = ohe_y_labels\n        assert self.class_num == ohe_y_labels.shape[1]\n        y_sample_num = ohe_y_labels.shape[0]\n        if mode == ""random"":\n            split_res = train_test_split(range(y_sample_num), test_size=val_num, shuffle=True)\n            return split_res[1]\n        else:\n            y_cats_list = ohe2cat(ohe_y_labels)\n            y_label_inverted_index_array = list()\n            for y_label_id in range(self.class_num):\n                label_sample_id_list = list(np.where(y_cats_list == y_label_id)[0])\n                y_label_inverted_index_array.append(label_sample_id_list)\n\n            val_sample_idxs = list()\n            avg_num_perclass = int(np.ceil(val_num/self.class_num))\n            for y_label_id in range(self.class_num):\n                a_label_sample_idxs = y_label_inverted_index_array[y_label_id]\n                if len(a_label_sample_idxs) >= avg_num_perclass:\n                    sampled_sampled_idxs = random.sample(population=a_label_sample_idxs,\n                                                                      k=avg_num_perclass)\n                else:\n                    sampled_sampled_idxs = a_label_sample_idxs\n\n                val_sample_idxs.extend(sampled_sampled_idxs)\n            return val_sample_idxs\n\n    def get_val_label_array(self, sampld_idxs):\n        val_label_array = list()\n        for sampld_idx in sampld_idxs:\n            i, = np.where(self.ohe_y_labels[sampld_idx] == 1)\n\n            val_label_array.append(i)\n\n        return val_label_array\n\n\ndef minisamples_edaer(mini_xs:list, mini_y:np.ndarray):\n\n    x_seq_len_list = list()\n    for x_train_sample in mini_xs:\n        len_a_x_sample = x_train_sample.shape[0]\n        x_seq_len_list.append(len_a_x_sample)\n    x_word_len_array = np.array(x_seq_len_list)\n    x_seq_len_mean = x_word_len_array.mean()\n    x_seq_len_std = x_word_len_array.std()\n\n    mini_num, class_num = mini_y.shape[0], mini_y.shape[1]\n    each_class_index = []\n    class_val_count = 0\n    for i in range(class_num):\n        where_i = np.where(mini_y[:, i] == 1)\n        class_i_ids = list(where_i[0])\n        if len(class_i_ids) > 0:\n            class_val_count += 1\n\n        each_class_index.append(class_i_ids)\n\n    class_cover_rate = round(class_val_count/class_num, 4)\n    class_dis_array = [round(len(i)/mini_num, 4) for i in each_class_index]\n\n    onehot_y_sum = np.sum(mini_y)\n    is_multilabel = False\n    if onehot_y_sum > mini_num:\n        is_multilabel = True\n\n    mini_eda_report = {\n        ""minis_num"": len(mini_xs),\n        ""x_seqlen_mean"": round(x_seq_len_mean, 4),\n        ""x_seqlen_std"": round(x_seq_len_std, 4),\n        ""y_cover_rate"": class_cover_rate,\n        ""y_dis_array"": class_dis_array,\n        ""is_multilabel"": is_multilabel\n    }\n    return mini_eda_report\n\n\ndef sample_y_edaer(samples_y: np.ndarray):\n    mini_num, class_num = samples_y.shape[0], samples_y.shape[1]\n    each_class_index = []\n    class_val_count = 0\n    for i in range(class_num):\n        where_i = np.where(samples_y[:, i] == 1)\n        class_i_ids = list(where_i[0])\n        if len(class_i_ids) > 0:\n            class_val_count += 1\n\n        each_class_index.append(class_i_ids)\n\n    class_cover_rate = round(class_val_count / class_num, 4)\n    class_dis_array = [round(len(i) / mini_num, 4) for i in each_class_index]\n    samples_y_eda_report = {\n        ""y_cover_rate"": class_cover_rate,\n        ""y_dis_array"": class_dis_array\n    }\n    return samples_y_eda_report\n\n\n'"
AutoDL_sample_code_submission/at_toolkit/at_tfds_convertor.py,0,"b'import tensorflow as tf\nimport numpy as np\nimport time\n\n# from at_toolkit.at_utils import info, error, as_timer\nfrom at_toolkit.interface.adl_tfds_convertor import AbsTfdsConvertor\n\n\nclass TfdsConvertor(AbsTfdsConvertor):\n    def __init__(self, if_train_shuffle=False, train_shuffle_size=100, if_pad_batch=False, padded_batch_size=20, domain=None):\n        self.train_tfds = None\n        self.test_tfds = None\n        self.train_num = 0\n        self.test_num = 0\n        self.accum_train_x = list()\n        self.accum_train_y = None\n        self.accm_train_cnt = 0\n        self.accum_test_x = list()\n        self.accum_test_y = list()\n        self.accm_test_cnt = 0\n\n        self.tfds_train_os_iterator = None\n        self.tfds_train_iter_next = None\n\n        self.speech_train_dataset = {""x"": None, ""y"": None}\n        self.speech_test_dataset = None\n        self.speech_x_test = None\n        self.if_train_shuffle = if_train_shuffle\n        self.train_shuffle_size = train_shuffle_size\n        self.train_max_shuffle_size = 1000\n        self.if_padded_batch = if_pad_batch\n        self.padded_batch_size = padded_batch_size\n\n        self.tfds_convertor_sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n\n        self.domain = domain\n\n    def init_train_tfds(self, train_tfds, train_num, force_shuffle=False):\n        if self.train_tfds is None or self.train_num == 0 or force_shuffle is True:\n            self.train_num = train_num\n            self.train_tfds = train_tfds\n\n            if self.if_train_shuffle or force_shuffle is True:\n                self.train_tfds = self.train_tfds.shuffle(buffer_size=min(self.train_max_shuffle_size, int(self.train_num * 0.6)))\n\n            if self.if_padded_batch:\n                self.train_tfds = self.train_tfds.padded_batch(\n                    self.padded_batch_size,\n                    padded_shapes=([None, 1, 1, 1], [None]),\n                    padding_values=(tf.constant(-1, dtype=tf.float32), tf.constant(-1, dtype=tf.float32)),\n                )\n            self.tfds_train_os_iterator = None\n\n    def init_test_tfds(self, test_tfds):\n        if self.test_tfds is None:\n            self.test_tfds = test_tfds\n\n            if self.if_padded_batch:\n                self.test_tfds = test_tfds.padded_batch(\n                    self.padded_batch_size,\n                    padded_shapes=([None, 1, 1, 1], [None]),\n                    padding_values=(tf.constant(-1, dtype=tf.float32), tf.constant(-1, dtype=tf.float32)),\n                )\n\n    def get_train_np(self, take_size):\n        if self.train_tfds is None:\n            return self.accum_train_x, self.accum_train_y\n\n        if self.tfds_train_os_iterator is None:\n            self.tfds_train_os_iterator = self.train_tfds.make_one_shot_iterator()\n            self.tfds_train_iter_next = self.tfds_train_os_iterator.get_next()\n\n        cur_get_cnt = 0\n        cur_data_y = list()\n        cur_incre_train_x = list()\n\n        if self.accm_train_cnt < self.train_num:\n            time_train_np_start = time.time()\n            if self.if_padded_batch:\n                while True:\n                    example_batch_num = 0\n                    try:\n                        example, labels = self.tfds_convertor_sess.run(self.tfds_train_iter_next)\n                        example = np.squeeze(example, (2, 3))\n                        example = np.squeeze(example, axis=-1)\n                        example = example.astype(np.int)\n                        cur_incre_train_x.extend(example)\n                        cur_data_y.extend(labels)\n                        cur_get_cnt += example.shape[0]\n                        self.accm_train_cnt += example.shape[0]\n                        example_batch_num += 1\n\n                        if cur_get_cnt >= take_size or self.accm_train_cnt >= self.train_num:\n                            break\n\n                    except tf.errors.OutOfRangeError:\n                        break\n\n            else:\n                while True:\n                    try:\n                        example, labels = self.tfds_convertor_sess.run(self.tfds_train_iter_next)\n\n                        cur_incre_train_x.append(example)\n                        cur_data_y.append(labels)\n                        cur_get_cnt += 1\n                        self.accm_train_cnt += 1\n                        if cur_get_cnt >= take_size or self.accm_train_cnt >= self.train_num:\n                            break\n\n                    except tf.errors.OutOfRangeError:\n                        break\n\n            self.accum_train_x.extend(cur_incre_train_x)\n\n            if self.accum_train_y is None:\n                self.accum_train_y = np.array(cur_data_y)\n            else:\n                self.accum_train_y = np.concatenate((self.accum_train_y, np.array(cur_data_y)))\n\n        else:\n            self.tfds_convertor_sess.close()\n\n        return {""x"": [np.squeeze(x) for x in cur_incre_train_x], ""y"": np.array(cur_data_y)}\n\n    def get_train_np_accm(self, take_size) -> dict:\n        self.get_train_np(take_size)\n        return {""x"": [np.squeeze(x) for x in self.accum_train_x], ""y"": np.array(self.accum_train_y)}\n\n    def get_train_np_full(self):\n        left_train_num = self.train_num - self.accm_train_cnt\n        self.get_train_np(take_size=left_train_num)\n        return {""x"": [np.squeeze(x) for x in self.accum_train_x], ""y"": np.array(self.accum_train_y)}\n\n    def get_test_np(self):\n        if self.test_tfds is None:\n            return self.accum_test_x, self.accum_test_y\n\n        if len(self.accum_test_x) == 0:\n            time_test_np_start = time.time()\n            tfds_test_os_iterator = self.test_tfds.make_one_shot_iterator()\n            tfds_test_iter_next = tfds_test_os_iterator.get_next()\n\n            if self.if_padded_batch:\n                while True:\n                    try:\n                        example, labels = self.tfds_convertor_sess.run(tfds_test_iter_next)\n                        example = np.squeeze(example, (2, 3))\n                        example = np.squeeze(example, axis=-1)\n                        example = example.astype(np.int)\n\n                        self.accum_test_x.extend(example)\n                        self.accum_test_y.extend(labels)\n                        self.accm_test_cnt += example.shape[0]\n                    except tf.errors.OutOfRangeError:\n                        break\n            else:\n                while True:\n                    try:\n                        example, labels = self.tfds_convertor_sess.run(tfds_test_iter_next)\n                        self.accum_test_x.append(example)\n                        self.accum_test_y.append(labels)\n                        self.accm_test_cnt += 1\n\n                    except tf.errors.OutOfRangeError:\n                        break\n\n            time_test_np_end = time.time()\n\n            self.accum_test_y = np.array(self.accum_test_y)\n\n        return [np.squeeze(x) for x in self.accum_test_x]\n\n\n'"
AutoDL_sample_code_submission/at_toolkit/at_utils.py,0,"b'import logging\nimport os\nimport sys\nimport json\nimport time\nfrom typing import Any\nimport multiprocessing\nfrom collections import OrderedDict\nimport psutil\nimport functools\n\nfrom at_toolkit.at_cons import SPEECH_TR34_PT_MODEL_PATH, SPEECH_TR34_PT_MODEL_DIR\n\n\nnesting_level = 0\nis_start = None\nNCPU = multiprocessing.cpu_count()\n\n\ndef log(entry: Any):\n    global nesting_level\n    space = ""-"" * (4 * nesting_level)\n    # logger.info(f""{space}{entry}"")\n    logger.info(""{}{}"".format(space, entry))\n\n\ndef get_logger(verbosity_level, use_error_log=False, log_path=None):\n    """"""Set logging format to something like:\n         2019-04-25 12:52:51,924 INFO score.py: <message>\n    """"""\n    logger = logging.getLogger(__file__)\n    logging_level = getattr(logging, verbosity_level)\n    logger.setLevel(logging_level)\n\n    if log_path is None:\n        log_dir = os.path.join("".."", ""log"")\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n        log_path = os.path.join(log_dir, ""log.txt"")\n    else:\n        log_path = os.path.join(log_path, ""log.txt"")\n\n    formatter = logging.Formatter(\n        fmt=\'%(asctime)s %(levelname)s %(filename)s: %(funcName)s: %(lineno)d: %(message)s\')\n\n    stdout_handler = logging.StreamHandler(sys.stdout)\n    stdout_handler.setLevel(logging_level)\n    stdout_handler.setFormatter(formatter)\n\n    logger.addHandler(stdout_handler)\n\n    fh = logging.FileHandler(log_path)\n    fh.setLevel(logging_level)\n    fh.setFormatter(formatter)\n\n    logger.addHandler(fh)\n\n    if use_error_log:\n        stderr_handler = logging.StreamHandler(sys.stderr)\n        stderr_handler.setLevel(logging.WARNING)\n        stderr_handler.setFormatter(formatter)\n        logger.addHandler(stderr_handler)\n    logger.propagate = False\n    return logger\n\n\nlogger = get_logger(\'INFO\')\n\ndebug = logger.debug\ninfo = logger.info\nwarning = logger.warning\nerror = logger.error\n\n\ndef timeit(method, start_log=None):\n    @functools.wraps(method)\n    def timed(*args, **kw):\n        global is_start\n        global nesting_level\n\n        if not is_start:\n            print()\n\n        is_start = True\n        # log(f""Start [{method.__name__}]:"" + (start_log if start_log else """"))\n        log(""Start [{}]:"" + (start_log if start_log else """").format(method.__name__))\n        nesting_level += 1\n\n        start_time = time.time()\n        result = method(*args, **kw)\n        end_time = time.time()\n\n        nesting_level -= 1\n        # log(f""End   [{method.__name__}]. Time elapsed: {end_time - start_time:0.2f} sec."")\n        log(""End   [{}]. Time elapsed: {} sec."".format(method.__name__, end_time - start_time))\n        is_start = False\n\n        return result\n\n    return timed\n\n\n\nclass ASTimer():\n    def __init__(self):\n        self.times = [time.time()]\n        # self.accumulation = OrderedDict({})\n        self.accumulation = list()\n        self.total_time = 0.0\n        self.step_time = 0.0\n        self.counter = 0\n        self.repr_update_cnt = 0\n        self.train_start_t = time.time()\n        self.test_start_t = time.time()\n\n    def __call__(self, time_name):\n        if time_name == ""train_start"":\n            self.train_start_t = time.time()\n            self.times.append(self.train_start_t)\n            delta = self.times[-1] - self.times[-2]\n        elif time_name == ""train_end"":\n            self.times.append((time.time()))\n            delta = self.times[-1] - self.train_start_t\n        elif time_name == ""test_start"":\n            self.test_start_t = time.time()\n            self.times.append(self.test_start_t)\n            delta = self.times[-1] - self.times[-2]\n        elif time_name == ""test_end"":\n            self.times.append((time.time()))\n            delta = self.times[-1] - self.test_start_t\n        else:\n            self.times.append((time.time()))\n            delta = self.times[-1] - self.times[-2]\n\n        # self.accumulation[""{}_{}"".format(self.counter, time_name)] = delta\n\n        self.accumulation.append([""{}_{}"".format(self.counter, time_name), delta])\n        self.counter += 1\n\n    def __repr__(self):\n        # for list\n        # timer_res = [""{}:{}s"".format(t[0], t[1]) for t in self.accumulation]\n        # for ordered dict.\n        # for n, t in self.accumulation.items():\n        #     timer_res.append(""{}:{}s"".format(n, round(t, 3)))\n        # timer_res = [""{}:        {}s"".format(t[0], round(t[1], 3)) for t in self.accumulation[self.repr_update_cnt: self.counter]]\n        timer_res = [[t[0], round(t[1], 3)] for t in self.accumulation[self.repr_update_cnt: self.counter]]\n        self.repr_update_cnt = self.counter\n        # return json.dumps(timer_res, indent=4)\n        return json.dumps(timer_res)\n\n    def print_all(self):\n        timer_res = [""{}:       {}s"".format(t[0], t[1]) for t in self.accumulation]\n        return json.dumps(timer_res, indent=4)\n\n\n\ndef autodl_image_install_download():\n\n    pass\n\n\ndef autodl_video_install_download():\n\n    pass\n\n\ndef autodl_speech_install_download():\n\n    os.system(""apt install wget"")\n\n    if not os.path.isfile(SPEECH_TR34_PT_MODEL_PATH):\n        print(""Error: {} not file"".format(SPEECH_TR34_PT_MODEL_PATH))\n\n    os.system(\'pip3 install kapre==0.1.4 -i https://pypi.tuna.tsinghua.edu.cn/simple\')\n\n\ndef autodl_nlp_install_download():\n\n    os.system(""pip install jieba_fast -i https://pypi.tuna.tsinghua.edu.cn/simple"")\n    os.system(""pip install jieba -i https://pypi.tuna.tsinghua.edu.cn/simple"")\n    os.system(""pip install pathos -i https://pypi.tuna.tsinghua.edu.cn/simple"")\n    os.system(""pip install bpemb -i https://pypi.tuna.tsinghua.edu.cn/simple"")\n    os.system(""pip install keras-radam -i https://pypi.tuna.tsinghua.edu.cn/simple"")\n    os.system(""apt-get install wget"")\n\n\ndef autodl_tabular_install_download():\n\n    pass\n\n\ndef autodl_install_download(domain):\n    if domain == ""image"":\n        autodl_image_install_download()\n    elif domain == ""video"":\n        autodl_video_install_download()\n    elif domain == ""speech"":\n        autodl_speech_install_download()\n    elif domain == ""nlp"":\n        autodl_nlp_install_download()\n    elif domain == ""tabular"":\n        autodl_tabular_install_download()\n    else:\n        error(""Error: domain is {}, can not install_download"".format(domain))\n\n\n'"
AutoDL_sample_code_submission/Auto_Image/architectures/__init__.py,0,b''
AutoDL_sample_code_submission/Auto_Image/architectures/resnet.py,25,"b'import logging\nimport sys\nfrom collections import OrderedDict\nimport copy\nimport torch\nimport torchvision.models as models\nfrom torch.utils import model_zoo\nfrom torchvision.models.resnet import model_urls\n\nimport skeleton\nimport torch.nn as nn\n\nformatter = logging.Formatter(fmt=\'[%(asctime)s %(levelname)s %(filename)s] %(message)s\')\n\nhandler = logging.StreamHandler(sys.stdout)\nhandler.setFormatter(formatter)\n\nLOGGER = logging.getLogger(__name__)\nLOGGER.setLevel(logging.INFO)\nLOGGER.addHandler(handler)\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n    __constants__ = [\'downsample\']\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError(\'BasicBlock only supports groups=1 and base_width=64\')\n        if dilation > 1:\n            raise NotImplementedError(""Dilation > 1 not supported in BasicBlock"")\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.activate = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.activate(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.activate(out)\n\n        return out\n\n\nclass ResNet18(models.ResNet):\n    Block = BasicBlock\n\n    def __init__(self, in_channels, num_classes=10, **kwargs):\n        Block = BasicBlock\n        self.in_channels = in_channels\n        super(ResNet18, self).__init__(Block, [2, 2, 2, 2], num_classes=num_classes, **kwargs)\n\n        if in_channels == 3:\n            self.stem = torch.nn.Sequential(\n                skeleton.nn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225], inplace=False),\n            )\n        elif in_channels == 1:\n            self.stem = torch.nn.Sequential(\n                skeleton.nn.Normalize(0.5, 0.25, inplace=False),\n                skeleton.nn.CopyChannels(3),\n            )\n        else:\n            self.stem = torch.nn.Sequential(\n                skeleton.nn.Normalize(0.5, 0.25, inplace=False),\n                torch.nn.Conv2d(in_channels, 3, kernel_size=3, stride=1, padding=1, bias=False),\n                torch.nn.BatchNorm2d(3),\n            )\n        self.last_channels = 512 * Block.expansion\n        self.pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = torch.nn.Linear(self.last_channels, num_classes, bias=False)\n        self._half = False\n        self._class_normalize = True\n\n    def init(self, model_dir=None, gain=1.):\n        self.model_dir = model_dir if model_dir is not None else self.model_dir\n        sd = model_zoo.load_url(model_urls[\'resnet18\'], model_dir=self.model_dir)\n        del sd[\'fc.weight\']\n        del sd[\'fc.bias\']\n        self.load_state_dict(sd, strict=False)\n        torch.nn.init.xavier_uniform_(self.fc.weight, gain=gain)\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward_origin(self, x, targets):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n    def forward(self, inputs, targets=None, tau=8.0, reduction=\'avg\'):\n        inputs = self.stem(inputs)\n        logits = self.forward_origin(inputs, targets)\n        logits /= tau\n\n        if targets is None:\n            return logits\n        if targets.device != logits.device:\n            targets = targets.to(device=logits.device)\n\n        loss = self.loss_fn(input=logits, target=targets)\n\n        if self._class_normalize and isinstance(self.loss_fn, (\n                torch.nn.BCEWithLogitsLoss, skeleton.nn.BinaryCrossEntropyLabelSmooth)):\n            pos = (targets == 1).to(logits.dtype)\n            neg = (targets < 1).to(logits.dtype)\n            npos = pos.sum(dim=0)\n            nneg = neg.sum(dim=0)\n\n            positive_ratio = torch.clamp((npos) / (npos + nneg), min=0.03, max=0.97).view(1, loss.shape[1])\n            negative_ratio = torch.clamp((nneg) / (npos + nneg), min=0.03, max=0.97).view(1, loss.shape[1])\n\n            normalized_loss = (loss * pos) / positive_ratio\n            normalized_loss += (loss * neg) / negative_ratio\n\n            loss = normalized_loss\n        if reduction == \'avg\':\n            loss = loss.mean()\n        elif reduction == \'max\':\n            loss = loss.max()\n        elif reduction == \'min\':\n            loss = loss.min()\n        return logits, loss\n\n    def half(self):\n        for module in self.modules():\n            if len([c for c in module.children()]) > 0:\n                continue\n\n            if not isinstance(module, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d)):\n                module.half()\n            else:\n                module.float()\n        self._half = True\n        return self\n\nclass ResLayer(nn.Module):\n    def __init__(self, in_c, out_c, groups=1):\n        super(ResLayer, self).__init__()\n        self.act = nn.CELU(0.075, inplace=False)\n        conv = nn.Conv2d(in_channels=in_c, out_channels=out_c, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),\n                         bias=False, groups=groups)\n        norm = nn.BatchNorm2d(num_features=out_c)\n        pool = nn.MaxPool2d(2)\n        self.pre_conv = nn.Sequential(\n            OrderedDict([(\'conv\', conv), (\'pool\', pool), (\'norm\', norm), (\'act\', nn.CELU(0.075, inplace=False))]))\n        self.res1 = nn.Sequential(OrderedDict([\n            (\'conv\', nn.Conv2d(in_channels=out_c, out_channels=out_c, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),\n                               bias=False, groups=groups)), (\'bn\', nn.BatchNorm2d(out_c)),\n            (\'act\', nn.CELU(0.075, inplace=False))]))\n        self.res2 = nn.Sequential(OrderedDict([\n            (\'conv\', nn.Conv2d(in_channels=out_c, out_channels=out_c, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),\n                               bias=False, groups=groups)), (\'bn\', nn.BatchNorm2d(out_c)),\n            (\'act\', nn.CELU(0.075, inplace=False))]))\n\n    def forward(self, x):\n        x = self.pre_conv(x)\n        out = self.res1(x)\n        out = self.res2(out)\n        out = out + x\n        return out\n\n\nclass ResNet9(nn.Module):\n\n    def __init__(self, in_channels, num_classes=10, **kwargs):\n        super(ResNet9, self).__init__()  # resnet18\n        channels = [64, 128, 256, 512]\n        group = 1\n        self.in_channels = in_channels\n        if in_channels == 3:\n            self.stem = torch.nn.Sequential(\n                skeleton.nn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225], inplace=False),\n            )\n        elif in_channels == 1:\n            self.stem = torch.nn.Sequential(\n                skeleton.nn.Normalize(0.5, 0.25, inplace=False),\n                skeleton.nn.CopyChannels(3),\n            )\n        else:\n            self.stem = torch.nn.Sequential(\n                skeleton.nn.Normalize(0.5, 0.25, inplace=False),\n                torch.nn.Conv2d(in_channels, 3, kernel_size=3, stride=1, padding=1, bias=False),\n                torch.nn.BatchNorm2d(3),\n            )\n        conv1 = nn.Conv2d(in_channels=3, out_channels=channels[0], kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),\n                          bias=False)\n        norm1 = nn.BatchNorm2d(num_features=channels[0])\n        act = nn.CELU(0.075, inplace=False)\n        pool = nn.MaxPool2d(2)\n        self.prep = nn.Sequential(OrderedDict([(\'conv\', conv1), (\'bn\', norm1), (\'act\', act)]))\n        self.layer1 = ResLayer(channels[0], channels[1], groups=group)\n        conv2 = nn.Conv2d(in_channels=channels[1], out_channels=channels[2], kernel_size=(3, 3), stride=(1, 1),\n                          padding=(1, 1),\n                          bias=False, groups=group)\n        norm2 = nn.BatchNorm2d(num_features=channels[2])\n        self.layer2 = nn.Sequential(OrderedDict([(\'conv\', conv2), (\'pool\', pool), (\'bn\', norm2), (\'act\', act)]))\n        self.layer3 = ResLayer(channels[2], channels[3], groups=group)\n        self.pool4 = nn.AdaptiveMaxPool2d(1)\n        self.fc = torch.nn.Linear(channels[3], num_classes, bias=False)\n        self._half = False\n        self._class_normalize = True\n\n    def init(self, model_dir=None, gain=1.):\n        self.model_dir = model_dir if model_dir is not None else self.model_dir\n        sd = model_zoo.load_url(\n            \'https://github.com/DeepWisdom/AutoDL/releases/download/opensource/r9-70e4b5c2.pth.tar\',\n            model_dir=self.model_dir)\n        new_sd = copy.deepcopy(sd[\'state_dict\'])\n        for key, value in sd[\'state_dict\'].items():\n            new_sd[key[7:]] = sd[\'state_dict\'][key]\n        self.load_state_dict(new_sd, strict=False)\n\n    def forward_origin(self, x, targets):\n        x = self.prep(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.pool4(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n    def forward(self, inputs, targets=None, tau=8.0, reduction=\'avg\'):  # pylint: disable=arguments-differ\n        inputs = self.stem(inputs)\n        logits = self.forward_origin(inputs, targets)\n        logits /= tau\n\n        if targets is None:\n            return logits\n        if targets.device != logits.device:\n            targets = targets.to(device=logits.device)\n\n        loss = self.loss_fn(input=logits, target=targets)\n\n        if self._class_normalize and isinstance(self.loss_fn, (\n                torch.nn.BCEWithLogitsLoss, skeleton.nn.BinaryCrossEntropyLabelSmooth)):\n            pos = (targets == 1).to(logits.dtype)\n            neg = (targets < 1).to(logits.dtype)\n            npos = pos.sum(dim=0)\n            nneg = neg.sum(dim=0)\n\n            positive_ratio = torch.clamp((npos) / (npos + nneg), min=0.03, max=0.97).view(1, loss.shape[1])\n            negative_ratio = torch.clamp((nneg) / (npos + nneg), min=0.03, max=0.97).view(1, loss.shape[1])\n\n            normalized_loss = (loss * pos) / positive_ratio\n            normalized_loss += (loss * neg) / negative_ratio\n\n            loss = normalized_loss\n        if reduction == \'avg\':\n            loss = loss.mean()\n        elif reduction == \'max\':\n            loss = loss.max()\n        elif reduction == \'min\':\n            loss = loss.min()\n        return logits, loss\n\n    def half(self):\n        for module in self.modules():\n            if len([c for c in module.children()]) > 0:\n                continue\n\n            if not isinstance(module, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d)):\n                module.half()\n            else:\n                module.float()\n        self._half = True\n        return self\n'"
AutoDL_sample_code_submission/Auto_Image/skeleton/__init__.py,0,b'# -*- coding: utf-8 -*-\n# pylint: disable=wildcard-import\nfrom __future__ import absolute_import\n\nfrom . import nn\nfrom . import optim\nfrom . import utils\n\nfrom . import data\n'
AutoDL_sample_code_submission/Auto_NLP/second_stage_models/__init__.py,0,b''
AutoDL_sample_code_submission/Auto_NLP/second_stage_models/ac.py,0,"b'""""""\nCreated on Mon Aug 26 11:04:11 2019\n@author: Milk\n@Concat: milk@pku.edu.cn\n""""""\n\nimport numpy as np\nimport time \n# from cython cimport boundscheck,wraparound\nimport re\nimport jieba\n\n\n\'\'\'\n************************EN SEG1************************\n\'\'\'\ndef clean_text_en_seg1(dat,sentence_len):\n    n = len(dat)\n    i = 0\n    REPLACE_BY_SPACE_RE = re.compile(\'[""/(){}\\[\\]\\|@,;]\')\n    BAD_SYMBOLS_RE = re.compile(\'[^0-9a-zA-Z #+_]\')\n    if sentence_len is None:\n        ret = []\n        for i in range(n):\n            line = dat[i]\n            line = line.lower()\n            line = REPLACE_BY_SPACE_RE.sub(\' \', line)\n            line = BAD_SYMBOLS_RE.sub(\'\', line)\n            line = line.strip()\n            line = line.split(\' \')\n            ret.append(line)\n        return ret\n        \n    else:\n        ret = []\n        for i in range(n):\n            line = dat[i]\n            line = line[0:sentence_len]\n            line = line.lower()\n            line = REPLACE_BY_SPACE_RE.sub(\' \', line)\n            line = BAD_SYMBOLS_RE.sub(\'\', line)\n            line = line.strip()\n            line = line.split(\' \')\n            ret.append(line)\n        return ret\n\n\n\n\n\'\'\'\n************************EN SEG2************************\n\'\'\'\ndef clean_text_en_seg2(dat, sentence_len):\n    n = len(dat)\n    i = 0\n    REPLACE_BY_SPACE_RE = re.compile(\'[""/(){}\\[\\]\\|@,;]\')\n    BAD_SYMBOLS_RE = re.compile(\'[^0-9a-zA-Z #+_]\')\n    \n    ret = []\n    for i in range(n):\n        line = dat[i]\n        line = REPLACE_BY_SPACE_RE.sub(\' \', line)\n        line = BAD_SYMBOLS_RE.sub(\'\', line)\n        line = line.strip()\n        line = line.split(\' \')\n        ret.append(line)\n    return ret\n\'\'\'\n************************ZH SEG1************************\n\'\'\'\n\ndef clean_text_zh_seg1(dat,sentence_len):\n    n = len(dat)\n    i = 0\n    REPLACE_BY_SPACE_RE = re.compile(\'[\xe2\x80\x9c\xe2\x80\x9d\xe3\x80\x90\xe3\x80\x91/\xef\xbc\x88\xef\xbc\x89\xef\xbc\x9a\xef\xbc\x81\xef\xbd\x9e\xe3\x80\x8c\xe3\x80\x8d\xe3\x80\x81|\xef\xbc\x8c\xef\xbc\x9b\xe3\x80\x82""/(){}\\[\\]\\|@,\\.;]\')\n    if sentence_len is None:\n        ret = []\n        for i in range(n):\n            line = dat[i]\n            line = REPLACE_BY_SPACE_RE.sub(\'\', line)\n            ret.append(line)\n        return ret\n        \n    else:\n        ret = []\n        for i in range(n):\n            line = dat[i]\n            line = line[0:sentence_len]\n            line = REPLACE_BY_SPACE_RE.sub(\'\', line)\n            ret.append(line)\n        return ret\n\n\n\n\n\n\'\'\'\n************************ZH SEG2************************\n\'\'\'\n\n\ndef _tokenize_chinese_words(text):\n    gen = jieba.cut(text, cut_all=False, HMM = False)\n    ans = []\n    for i in gen:\n        ans.append(i)\n    return ans\n\ndef clean_text_zh_seg2(dat, sentence_len):\n    n = len(dat)\n    i = 0\n    REPLACE_BY_SPACE_RE = re.compile(\'[\xe2\x80\x9c\xe2\x80\x9d\xe3\x80\x90\xe3\x80\x91/\xef\xbc\x88\xef\xbc\x89\xef\xbc\x9a\xef\xbc\x81\xef\xbd\x9e\xe3\x80\x8c\xe3\x80\x8d\xe3\x80\x81|\xef\xbc\x8c\xef\xbc\x9b\xe3\x80\x82""/(){}\\[\\]\\|@,\\.;]\')\n    ret = []\n    for i in range(n):\n        line = dat[i]\n        line = REPLACE_BY_SPACE_RE.sub(\' \', line)\n        line = line.strip()\n        ret.append(_tokenize_chinese_words(line))\n    return ret\n\n\n\'\'\'\n************************Sequentical************************\n\'\'\'\n\n\n\ndef bulid_index(data, num_sentence):\n#    s1 = time.time()\n    \n    min_df = 3\n    text_lens = np.zeros( num_sentence,dtype=np.int32 )\n    word2cnt = {}\n    word2index = {}\n    i = 0\n    ind = 1\n        \n    for i in range(num_sentence):\n        line = data[i]\n        \n        for w in line:\n            if w in word2cnt:\n                word2cnt[w] += 1\n            else:\n                word2cnt[w] = 1\n        \n        text_lens[i] = len(line)\n    \n    for k,v in word2cnt.items():\n        if v >= min_df:\n            word2index[k] = ind\n            ind += 1\n    MAX_VOCAB_SIZE = ind\n\n    MAX_SEQ_LENGTH = np.sort(text_lens)[int(num_sentence*0.95)]\n\n\n    return MAX_VOCAB_SIZE, MAX_SEQ_LENGTH, word2index,text_lens\n\n\ndef texts_to_sequences_and_pad(data, num_sentence, word2index, max_length, text_lens, data_type):\n    ans = np.zeros((num_sentence, max_length), dtype=np.int32)\n    k = len(word2index) + 1\n    x_train = ans\n\n    if data_type == 0:\n        for i in range(num_sentence):\n            line = data[i]\n            n = min(max_length, text_lens[i])\n            for j in range(n):\n                w = line[j]\n                if w in word2index:\n                    x_train[i][j] = word2index[w]\n                else:\n                    x_train[i][j] = k\n    else:\n        for i in range(num_sentence):\n            line = data[i]\n            n = min(max_length, len(line))\n            for j in range(n):\n                w = line[j]\n                if w in word2index:\n                    x_train[i][j] = word2index[w]\n                else:\n                    x_train[i][j] = k\n        \n    return ans\n\n\n\t'"
AutoDL_sample_code_submission/Auto_NLP/second_stage_models/ac_new.py,0,"b'""""""\nCreated on Mon Aug 26 11:04:11 2019\n@author: Milk\n@Concat: milk@pku.edu.cn\n""""""\n\nimport numpy as np\nimport time \n# from cython cimport boundscheck,wraparound\nimport re\nimport jieba\n\n\n\'\'\'\n************************EN SEG1************************\n\'\'\'\ndef clean_text_en_seg1(dat,sentence_len):\n    n = len(dat)\n    i = 0\n    REPLACE_BY_SPACE_RE = re.compile(\'[""/(){}\\[\\]\\|@,;]\')\n    BAD_SYMBOLS_RE = re.compile(\'[^0-9a-zA-Z #+_]\')\n    if sentence_len is None:\n        ret = []\n        for i in range(n):\n            line = dat[i]\n            line = line.lower()\n            line = REPLACE_BY_SPACE_RE.sub(\' \', line)\n            line = BAD_SYMBOLS_RE.sub(\'\', line)\n            line = line.strip()\n            line = line.split(\' \')\n            ret.append(line)\n        return ret\n        \n    else:\n        ret = []\n        for i in range(n):\n            line = dat[i]\n            line = line[0:sentence_len]\n            line = line.lower()\n            line = REPLACE_BY_SPACE_RE.sub(\' \', line)\n            line = BAD_SYMBOLS_RE.sub(\'\', line)\n            line = line.strip()\n            line = line.split(\' \')\n            ret.append(line)\n        return ret\n\n\n\n\n\'\'\'\n************************EN SEG2************************\n\'\'\'\ndef clean_text_en_seg2(dat, sentence_len):\n    n = len(dat)\n    i = 0\n    REPLACE_BY_SPACE_RE = re.compile(\'[""/(){}\\[\\]\\|@,;]\')\n    BAD_SYMBOLS_RE = re.compile(\'[^0-9a-zA-Z #+_]\')\n    \n    ret = []\n    for i in range(n):\n        line = dat[i]\n        line = REPLACE_BY_SPACE_RE.sub(\' \', line)\n        line = BAD_SYMBOLS_RE.sub(\'\', line)\n        line = line.strip()\n        line = line.split(\' \')\n        ret.append(line)\n    return ret\n\'\'\'\n************************ZH SEG1************************\n\'\'\'\n\ndef clean_text_zh_seg1(dat,sentence_len):\n    n = len(dat)\n    i = 0\n    REPLACE_BY_SPACE_RE = re.compile(\'[\xe2\x80\x9c\xe2\x80\x9d\xe3\x80\x90\xe3\x80\x91/\xef\xbc\x88\xef\xbc\x89\xef\xbc\x9a\xef\xbc\x81\xef\xbd\x9e\xe3\x80\x8c\xe3\x80\x8d\xe3\x80\x81|\xef\xbc\x8c\xef\xbc\x9b\xe3\x80\x82""/(){}\\[\\]\\|@,\\.;]\')\n    if sentence_len is None:\n        ret = []\n        for i in range(n):\n            line = dat[i]\n            line = REPLACE_BY_SPACE_RE.sub(\'\', line)\n            ret.append(line)\n        return ret\n        \n    else:\n        ret = []\n        for i in range(n):\n            line = dat[i]\n            line = line[0:sentence_len]\n            line = REPLACE_BY_SPACE_RE.sub(\'\', line)\n            ret.append(line)\n        return ret\n\n\n\n\n\n\'\'\'\n************************ZH SEG2************************\n\'\'\'\n\n\ndef _tokenize_chinese_words(text):\n    gen = jieba.cut(text, cut_all=False, HMM = False)\n    ans = []\n    for i in gen:\n        ans.append(i)\n    return ans\n\ndef clean_text_zh_seg2(dat, sentence_len):\n    n = len(dat)\n    i = 0\n    REPLACE_BY_SPACE_RE = re.compile(\'[\xe2\x80\x9c\xe2\x80\x9d\xe3\x80\x90\xe3\x80\x91/\xef\xbc\x88\xef\xbc\x89\xef\xbc\x9a\xef\xbc\x81\xef\xbd\x9e\xe3\x80\x8c\xe3\x80\x8d\xe3\x80\x81|\xef\xbc\x8c\xef\xbc\x9b\xe3\x80\x82""/(){}\\[\\]\\|@,\\.;]\')\n    ret = []\n    for i in range(n):\n        line = dat[i]\n        line = REPLACE_BY_SPACE_RE.sub(\' \', line)\n        line = line.strip()\n        ret.append(_tokenize_chinese_words(line))\n    return ret\n\n\n\'\'\'\n************************Sequentical************************\n\'\'\'\n\n\n\ndef bulid_index(data, num_sentence):\n#    s1 = time.time()\n    \n    min_df = 3\n    text_lens = np.zeros( num_sentence,dtype=np.int32 )\n    word2cnt = {}\n    word2index = {}\n    i = 0\n    ind = 1\n        \n    for i in range(num_sentence):\n        line = data[i]\n        \n        for w in line:\n            if w in word2cnt:\n                word2cnt[w] += 1\n            else:\n                word2cnt[w] = 1\n        \n        text_lens[i] = len(line)\n    \n    for k,v in word2cnt.items():\n        if v >= min_df:\n            word2index[k] = ind\n            ind += 1\n    MAX_VOCAB_SIZE = ind\n\n    MAX_SEQ_LENGTH = np.sort(text_lens)[int(num_sentence*0.95)]\n\n\n    return MAX_VOCAB_SIZE, MAX_SEQ_LENGTH, word2index,text_lens\n\n\ndef texts_to_sequences_and_pad(data, num_sentence, word2index, max_length, text_lens, data_type):\n    ans = np.zeros((num_sentence, max_length), dtype=np.int32)\n    k = len(word2index) + 1\n    x_train = ans\n\n    if data_type == 0:\n        for i in range(num_sentence):\n            line = data[i]\n            n = min(max_length, text_lens[i])\n            for j in range(n):\n                w = line[j]\n                if w in word2index:\n                    x_train[i][j] = word2index[w]\n                else:\n                    x_train[i][j] = k\n    else:\n        for i in range(num_sentence):\n            line = data[i]\n            n = min(max_length, len(line))\n            for j in range(n):\n                w = line[j]\n                if w in word2index:\n                    x_train[i][j] = word2index[w]\n                else:\n                    x_train[i][j] = k\n        \n    return ans\n\n\n\t'"
AutoDL_sample_code_submission/Auto_NLP/second_stage_models/get_embedding.py,0,"b""# -*- coding: utf-8 -*-\nimport gzip\nimport os\nimport time\nimport numpy as np\nimport gc\n\nclass GET_EMBEDDING:\n    stime = time.time()\n    embedding_path = '/app/embedding'\n    fasttext_embeddings_index = {}\n\n    fasttext_embeddings_index_zh = {}\n    fasttext_embeddings_index_en = {}\n\n    f_zh = gzip.open(os.path.join(embedding_path, 'cc.zh.300.vec.gz'),'rb')\n    f_en = gzip.open(os.path.join(embedding_path, 'cc.en.300.vec.gz'),'rb')\n\n    for line in f_zh.readlines():\n        values = line.strip().split()\n        word = values[0].decode('utf8')\n        coefs = np.asarray(values[1:], dtype='float32')\n        fasttext_embeddings_index_zh[word] = coefs\n    embedding_dict_zh = fasttext_embeddings_index_zh\n    del f_zh, values, word, coefs\n    gc.collect()\n    print('read zh embedding time: {}s.'.format(time.time()-stime))\n\n    for line in f_en.readlines():\n        values = line.strip().split()\n        word = values[0].decode('utf8')\n        coefs = np.asarray(values[1:], dtype='float32')\n        fasttext_embeddings_index_en[word] = coefs\n    embedding_dict_en = fasttext_embeddings_index_en\n    print('read en embedding time: {}s.'.format(time.time()-stime))\n"""
AutoDL_sample_code_submission/Auto_NLP/second_stage_models/model_iter_second_stage.py,0,"b'# -*- coding: utf-8 -*-\nimport pandas as pd\nimport os\nimport re\nimport time\nimport jieba\nimport numpy as np\nimport sys, getopt\nimport math\nimport gc\n\nimport keras\nfrom keras.layers import Dense, Input, LSTM, Bidirectional, Conv1D, GRU, Activation\nfrom keras.layers import Dropout, Embedding, Dot, Concatenate, PReLU\nfrom keras.preprocessing import text, sequence\nfrom keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D, TimeDistributed\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import LearningRateScheduler  # , EarlyStopping\nimport keras.backend as K\nfrom sklearn.model_selection import train_test_split\nfrom functools import reduce\nfrom keras.layers import CuDNNGRU\n\nimport tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\n\nfrom Auto_NLP.second_stage_models.tf_model import *\nfrom Auto_NLP.second_stage_models import ac\n\nimport warnings\n\nwarnings.filterwarnings(\'ignore\')\n\ntry:\n    import gzip\nexcept:\n    os.system(\'pip3 install gzip\')\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True  #\nconfig.log_device_placement = False  # )\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.5\n\nMAX_SEQ_LENGTH = 50\nMAX_VOCAB_SIZE = 200000  #\n\nfrom sklearn.metrics import roc_auc_score\n\n\ndef auc_metric(solution, prediction, task=\'binary.classification\'):\n\n    if solution.sum(axis=0).min() == 0:\n        return np.nan\n    auc = roc_auc_score(solution, prediction, average=\'macro\')\n    return np.mean(auc * 2 - 1)\n\n\ndef _get_last_layer_units_and_activation(num_classes):\n\n    if num_classes == 2:\n        activation = \'sigmoid\'\n        units = 1\n    else:\n        activation = \'softmax\'\n        units = num_classes\n    return units, activation\n\n\ndef CNN_Model(seq_len, num_classes, num_features, embedding_matrix=None):\n    in_text = Input(shape=(seq_len,))\n    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n\n    trainable = True\n    if embedding_matrix is None:\n        x = Embedding(num_features, 64, trainable=trainable)(in_text)\n    else:\n        x = Embedding(num_features, 300, trainable=trainable, weights=[embedding_matrix])(in_text)\n\n    x = Conv1D(128, kernel_size=5, padding=\'valid\', kernel_initializer=\'glorot_uniform\')(x)\n    x = GlobalMaxPooling1D()(x)\n\n    x = Dense(128)(x)  #\n    x = PReLU()(x)\n    x = Dropout(0.35)(x)  # 0\n    x = BatchNormalization()(x)\n\n    y = Dense(op_units, activation=op_activation)(x)\n\n    md = keras.models.Model(inputs=[in_text], outputs=y)\n\n    return md\n\n\ndef RNN_Model(seq_len, num_classes, num_features, embedding_matrix=None):\n    in_text = Input(shape=(seq_len,))\n    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n\n    trainable = True\n    if embedding_matrix is None:\n        x = Embedding(num_features, 64, trainable=trainable)(in_text)\n    else:\n        x = Embedding(num_features, 300, trainable=trainable, weights=[embedding_matrix])(in_text)\n\n    x = CuDNNGRU(128, return_sequences=True)(x)\n    x = GlobalMaxPooling1D()(x)\n\n    x = Dense(128)(x)  #\n    x = PReLU()(x)\n    x = Dropout(0.35)(x)  # 0\n    x = BatchNormalization()(x)\n\n    y = Dense(op_units, activation=op_activation)(x)\n\n    md = keras.models.Model(inputs=[in_text], outputs=y)\n\n    return md\n\n\ndef vectorize_data(x_train, x_val=None):\n    vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n    if x_val:\n        full_text = x_train + x_val\n    else:\n        full_text = x_train\n    vectorizer.fit(full_text)\n    train_vectorized = vectorizer.transform(x_train)\n    if x_val:\n        val_vectorized = vectorizer.transform(x_val)\n        return train_vectorized, val_vectorized, vectorizer\n    return train_vectorized, vectorizer\n\n\ndef ohe2cat(label):\n    return np.argmax(label, axis=1)\n\n\nclass Model(object):\n\n    def __init__(self, metadata, train_output_path=""./"", test_input_path=""./"", fasttext_emb=None):\n        self.done_training = False\n        self.metadata = metadata\n        self.train_output_path = train_output_path\n        self.test_input_path = test_input_path\n\n        self.epoch = 1\n        self.max_epoch = 8\n\n        self.model = None\n\n        self.X_train = None\n        self.X_val = None\n        self.y_train = None\n        self.y_val = None\n        self.te_y = None\n        self.split_val = True\n\n        self.word_index = None\n        self.max_length = None\n        self.seq_len = None\n        self.num_features = None\n\n        self.scos = [-1]\n        self.his_scos = []\n\n        self.patience = 3\n        self.k = 0\n\n        self.Xtest = None\n\n        self.best_sco = -1\n        self.best_res = []\n\n        self.best_val_res = [0] * 30\n        self.best_test_res = [0] * 30\n\n        self.model_num = -1\n\n        self.model_id = 0\n        self.cand_models = [\'CNN\', \'GRU\']\n        self.lrs = [0.0035, 0.016]\n\n        self.data_id = 0\n        self.max_data = 3\n\n        self.max_seq_len = 1600\n\n        self.is_best = False\n        self.new_data = False\n\n        self.test_id = 0\n\n        self.embedding_matrix = None\n        self.fasttext_emb = fasttext_emb\n\n        self.START = True  #\n        self.FIRSTROUND = True  #\n        self.LASTROUND = False\n        self.FIRSTEPOCH = 6\n\n        self.FIRST_CUT = 1200\n        self.SENTENCE_LEN = 6000\n        self.SAMPLENUM = 100000\n\n        self.emb_size = 64\n        self.out_size = 128\n        self.sess_config = config\n        self.TRAIN_STATUS = False  #\n        self.Switch_to_New_Model = True\n        self.use_ft_tf_model = False\n\n    def step_decay(self, epoch):\n        epoch = self.epoch\n        initial_lrate = self.lrs[self.model_id]  #\n        drop = 0.65\n        epochs_drop = 1.0  #\n        lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n        return lrate\n\n    def is_done(self):\n        if self.model_id == len(self.cand_models):\n            if self.data_id == self.max_data:\n                self.done_training = True\n            else:\n                self.model_id = 0\n\n    def get_batch_size(self, data_size, batch_size):\n\n        N = 7633305600 / (self.seq_len * self.emb_size * self.out_size)\n\n        batch_size = min(batch_size, N)\n        batch_size = max(batch_size, 4)\n\n        return int(batch_size)\n\n    def train_single_model(self, model_name):\n        pass\n\n    def get_embedding_new(self, num_features, word_index, fasttext_embeddings_index=None):\n        EMBEDDING_DIM = 300\n        embedding_matrix = np.zeros((num_features, EMBEDDING_DIM))\n        cnt = 0\n\n        for word, i in word_index.items():\n            if i >= num_features:  #\n                continue\n            embedding_vector = fasttext_embeddings_index.get(word)\n            if embedding_vector is not None:\n\n                embedding_matrix[i] = embedding_vector\n            else:\n\n                embedding_matrix[i] = np.zeros(300)\n                cnt += 1\n        print(\'fastText oov words: %s\' % cnt)\n\n        return embedding_matrix\n\n    def _process_feature(self, x_train, y_train, x_eval, y_eval, data_lan, data_type, deal_seg, sentence_len,\n                         sample_num):\n\n        x_train = np.array(x_train, dtype=\'object\')\n        y_train = np.array(y_train, dtype=\'object\')\n        x_eval = np.array(x_eval, dtype=\'object\')\n        y_eval = np.array(y_eval, dtype=\'object\')\n        len_train = len(x_train)\n        index = [i for i in range(len_train)]\n        np.random.shuffle(index)\n        index = index[0:sample_num]\n        x_train = x_train[index]\n        y_train = y_train[index]\n\n        x_train, word_index, num_features, max_length = self.deal_data(x_train, data_lan, data_type, deal_seg,\n                                                                       sentence_len)\n\n        num_classes = self.metadata[\'class_num\']\n        self.word_index = word_index\n        self.max_length = max_length\n\n        if self.split_val:\n\n            self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(x_train, ohe2cat(y_train),\n                                                                                  test_size=len(x_eval),\n                                                                                  random_state=666)\n        else:\n\n            self.X_train = x_train\n            self.y_train = ohe2cat(y_train)\n\n            x_eval = self.deal_data(x_eval, data_lan, data_type=1, deal_seg=deal_seg, sentence_len=max_length)\n            self.X_val = x_eval\n            self.y_val = ohe2cat(y_eval)\n\n\n        te_y = np.eye(num_classes)[self.y_val]\n        self.te_y = te_y\n\n        self.seq_len = len(x_train[0])\n        self.num_features = num_features\n\n    def preprocess_features(self, train_dataset, eval_dataset):\n        self.embedding_matrix = None\n\n        x_train, y_train = train_dataset\n        x_eval, y_eval = eval_dataset\n\n        start = time.time()\n        data_type = 0\n        if self.metadata[\'language\'] == \'ZH\':\n            data_lan = 0\n        else:\n            data_lan = 1\n\n        if self.START:\n\n            deal_seg = 1\n            sentence_len = self.FIRST_CUT\n            self._process_feature(x_train, y_train, x_eval, y_eval, data_lan=data_lan, data_type=data_type,\n                                  deal_seg=deal_seg, sentence_len=sentence_len, sample_num=self.SAMPLENUM)\n        else:\n            sentence_len = self.SENTENCE_LEN\n            if self.data_id < 2:\n                deal_seg = self.data_id + 1\n                self._process_feature(x_train, y_train, x_eval, y_eval, data_lan=data_lan, data_type=data_type,\n                                      deal_seg=deal_seg, sentence_len=sentence_len, sample_num=len(x_train))\n\n            if self.data_id == 1:\n                self.emb_size = 300\n                self.embedding_matrix = self.get_embedding_new(num_features=self.num_features,\n                                                               word_index=self.word_index,\n                                                               fasttext_embeddings_index=self.fasttext_emb)\n            elif self.data_id == 0 or self.data_id == 2:\n                self.emb_size = 64\n                self.embedding_matrix = None\n\n            self.data_id += 1\n            self.new_data = True\n\n\n    def deal_data(self, data, data_lan, data_type, deal_seg, sentence_len):\n\n        if data_type == 0:\n            s1 = time.time()\n            t1 = time.time()\n            if deal_seg == 1:\n                if data_lan == 0:\n                    data = ac.clean_text_zh_seg1(data, sentence_len)\n                else:\n                    data = ac.clean_text_en_seg1(data, sentence_len)\n            elif deal_seg == 2:\n                if data_lan == 0:\n                    data = ac.clean_text_zh_seg2(data, sentence_len)\n                else:\n                    data = ac.clean_text_en_seg2(data, sentence_len)\n\n            t2 = time.time()\n\n            num_sentence = len(data)\n            t = np.array(data, dtype=\'object\')\n            MAX_VOCAB_SIZE, MAX_SEQ_LENGTH, word2index, text_lens = ac.bulid_index(t, num_sentence)\n\n            t3 = time.time()\n            max_length = MAX_SEQ_LENGTH\n            res = ac.texts_to_sequences_and_pad(t, num_sentence, word2index, max_length, text_lens, data_type)\n            num_features = min(len(word2index) + 1, MAX_VOCAB_SIZE)\n\n            t4 = time.time()\n            s2 = time.time()\n\n            return res, word2index, num_features, max_length\n        else:\n            s1 = time.time()\n            if deal_seg == 1:\n                if data_lan == 0:\n                    data = ac.clean_text_zh_seg1(data, sentence_len)\n                else:\n                    data = ac.clean_text_en_seg1(data, sentence_len)\n            elif deal_seg == 2:\n                if data_lan == 0:\n                    data = ac.clean_text_zh_seg2(data, sentence_len)\n                else:\n                    data = ac.clean_text_en_seg2(data, sentence_len)\n            num_sentence = len(data)\n\n            t = np.array(data, dtype=\'object\')\n            word2index = self.word_index\n            max_length = self.max_length\n            res = ac.texts_to_sequences_and_pad(t, num_sentence, word2index, max_length, None, data_type)\n            return res\n\n    def train_start_stage(self):\n        pass\n\n    def train_firstround_stage(self):\n        pass\n\n    def build_and_compile_model(self, train_dataset):\n        is_balance = 0\n\n        if (self.START) and (self.metadata[\'class_num\'] == 2) and (is_balance) and self.use_ft_tf_model:\n\n            config = {\n                \'sequence_length\': self.seq_len,\n                \'embedding_size\': self.emb_size,\n                \'vocabulary_size\': self.num_features,\n                \'num_classes\': self.metadata[\'class_num\']\n            }\n            model = FT_tf_model(config)\n            self.use_ft_tf_model = True\n\n\n        else:\n\n            start3 = time.time()\n\n            if self.cand_models[self.model_id] == \'CNN\':\n                model = CNN_Model(self.seq_len, num_classes=self.metadata[\'class_num\'],\n                                  num_features=self.num_features, embedding_matrix=self.embedding_matrix)\n            elif self.cand_models[self.model_id] == \'GRU\':\n                if self.seq_len > self.max_seq_len:\n                    self.model_id += 1\n                    self.is_done()\n                    return\n\n                model = RNN_Model(self.seq_len, num_classes=self.metadata[\'class_num\'],\n                                  num_features=self.num_features, embedding_matrix=self.embedding_matrix)\n            elif self.cand_models[self.model_id] == \'Att\':\n                if self.seq_len > self.max_seq_len:\n                    self.model_id += 1\n                    self.is_done()\n                    return\n\n                model = GRU_Attention_Model(self.seq_len, num_classes=self.metadata[\'class_num\'],\n                                            num_features=self.num_features, embedding_matrix=self.embedding_matrix)\n\n\n            if self.metadata[\'class_num\'] == 2:\n                loss = \'binary_crossentropy\'\n            else:\n                loss = \'sparse_categorical_crossentropy\'\n\n            opt = keras.optimizers.Adam(lr=0.001)\n            model.compile(optimizer=opt, loss=loss, metrics=[\'acc\'])\n            self.use_ft_tf_model = False\n\n        self.model_num += 1\n        return model\n\n    def update_train_parameters(self):\n        lrate = LearningRateScheduler(self.step_decay)\n        callbacks = [lrate]\n\n        X_train = self.X_train\n        y_train = self.y_train\n        batch_size = 64\n\n        if len(y_train) > 10000:\n            if self.epoch < 4:\n                batch_size = (6 - self.epoch) * 32 * int(len(self.X_train) / 10000)  #\n            else:\n                batch_size = 16 * int(len(self.X_train) / 6000)  #\n\n            batch_size = min(batch_size, 2048)\n            batch_size = max(batch_size, 32)\n\n        if self.epoch == 1 and len(y_train) > 5000:\n            batch_size = max(batch_size, 128)\n\n        batch_size = self.get_batch_size(len(self.y_train), batch_size)\n        print(\'###train batch size:\', batch_size)\n        return callbacks, batch_size\n\n    def trasf_res(self, result, test_num, class_num):\n        y_test = np.zeros([test_num, class_num])\n\n        if self.metadata[\'class_num\'] == 2:\n            result = result.flatten()\n            y_test[:, 0] = 1 - result\n            y_test[:, 1] = result\n        else:\n            y_test = result\n        return y_test\n\n    def evaluate_model(self, model):\n        pred = None\n        max_auc = np.max(self.scos)\n\n        if self.epoch == 0:\n            val_auc = 0.001 * self.epoch\n        else:\n            batch_size = self.get_batch_size(len(self.y_val), 1024)\n            result = model.predict(self.X_val, batch_size=batch_size)\n\n            pred = self.trasf_res(result, len(self.y_val), self.metadata[\'class_num\'])\n            val_auc = auc_metric(self.te_y, pred)\n\n        if val_auc > max_auc:\n            self.k = 0\n            self.best_val_res[self.model_num] = pred\n        else:\n            self.k += 1\n\n        self.scos.append(val_auc)\n        print(\'val aucs:\', self.scos)\n        return max_auc, val_auc\n\n    def control_logic(self):\n        pass\n\n    def ensemble(self):\n\n        feat_size = len(self.his_scos) + 1\n\n        best_val = [element for element in self.best_test_res if not isinstance(element, int)]\n        if best_val:\n            return np.mean(best_val, axis=0)\n        else:\n            return np.array([])\n\n\n    def train_iter(self, train_dataset, eval_dataset, remaining_time_budget=None, do_clean=False):\n\n        if remaining_time_budget <= self.metadata[\'time_budget\'] * 0.125:\n            self.done_training = True\n            self.model = None\n            return\n\n        if do_clean:\n            model = self.model\n            del model\n            gc.collect()\n            K.clear_session()\n\n            self.model_id += 1\n\n            self.is_done()\n\n            self.epoch = 1\n            self.k = 0\n\n            if not self.LASTROUND:\n                self.his_scos.append(self.scos)\n            self.scos = [-1]\n        t1 = time.time()\n\n        if self.START:\n            data = train_dataset[0]\n            len_sum = 0\n            shape = min(len(data), 10000)\n            for i in data[:shape]:\n                len_sum += len(i)\n\n            len_mean = len_sum // shape\n            cut = 0\n            if len_mean > self.FIRST_CUT:\n                cut = 1\n            len_mean = min(self.FIRST_CUT, len_mean)\n\n            len_mean_for_compute = max(100, len_mean)\n            sample_row = int(-90.8 * len_mean_for_compute + 128960)\n\n            MAX_SAMPLE_ROW = 100000\n            MIN_SAMPLE_ROW = 16666\n\n            sample_row = min(sample_row, MAX_SAMPLE_ROW)\n            sample_row = max(sample_row, MIN_SAMPLE_ROW)\n\n            sample = 1\n            if sample_row >= len(data):\n                sample = 0\n\n            cut = 1\n            sample = 1\n            if cut == 0 and sample == 0:\n                self.START = False\n                self.FIRSTROUND = False\n                self.LASTROUND = False\n            else:\n                self.FIRST_CUT = len_mean  #\n                self.SAMPLENUM = sample_row\n\n            self.preprocess_features(train_dataset, eval_dataset)\n\n        if self.done_training:\n            return\n\n        if self.START or self.FIRSTROUND or self.LASTROUND:\n            self.max_epoch = self.FIRSTEPOCH\n        else:\n            self.max_epoch = 8\n\n            models = [\'CNN\', \'GRU\', \'\', \'\', \'\']\n            methods = [\'\', \'char-level\', \'word-level + pretrained embedding300dim\', \'word-level + 64dim-embedding\', \'\',\n                       \'\', \'\']\n\n\n        if self.START:\n            self.Switch_to_New_Model = True\n\n        elif self.FIRSTROUND:\n\n            pass\n\n        else:\n\n            if self.epoch == 1 and self.model_id == 0:\n                self.preprocess_features(train_dataset, eval_dataset)\n\n        if self.epoch == 1:\n            model = self.build_and_compile_model(train_dataset)\n            if model is None:\n                return\n        else:\n            model = self.model\n\n        callbacks, batch_size = self.update_train_parameters()\n        start7 = time.time()\n        history = model.fit([self.X_train], self.y_train,\n                            epochs=1,\n                            callbacks=callbacks,\n                            verbose=1,\n                            batch_size=batch_size,\n                            shuffle=True)\n\n        max_auc, val_auc = self.evaluate_model(model)\n        self.epoch += 1\n\n        if self.k >= self.patience or self.epoch >= self.max_epoch:\n            del model\n            gc.collect()\n            K.clear_session()\n            model = None\n\n            if self.FIRSTROUND:\n                self.FIRSTROUND = False\n                self.LASTROUND = True\n            else:\n                self.model_id += 1\n\n            self.is_done()\n\n            self.epoch = 1\n            self.k = 0\n\n            if not self.LASTROUND:\n                self.his_scos.append(self.scos)\n            else:\n                self.his_scos.append(self.scos)\n            self.scos = [-1]\n\n        if self.model_num == 0:\n            if self.his_scos:\n                max_auc = np.max(self.his_scos[0])\n\n            else:\n                max_auc = np.max(self.scos)\n            self.best_sco = max_auc\n        else:\n            if val_auc > self.best_sco:\n                self.is_best = True\n                self.best_sco = val_auc\n\n        self.model = model\n\n        if self.LASTROUND:\n\n            pass\n        print(\'AFTER TRAIN best_sco:\', self.best_sco, \' his_scos :\', self.his_scos)\n\n    def test(self, x_test, remaining_time_budget=None):\n        if self.START or self.FIRSTROUND or self.LASTROUND:\n\n            pass\n\n        else:\n            pass\n\n        self.test_id += 1\n\n        if self.k != 0 or self.model == None or self.model == -1:\n            print(""check k {} or self.model {}"".format(self.k, self.model))\n\n            if self.k == 0 and self.model == None and len(self.his_scos) > 1:\n                self.model = -1\n                self.best_res = self.ensemble()\n\n            self.is_best = False\n            self.LASTROUND = False\n\n            return self.best_res\n\n        model = self.model\n        word_index = self.word_index\n        max_length = self.max_length\n\n        train_num, test_num = self.metadata[\'train_num\'], self.metadata[\'test_num\']\n        class_num = self.metadata[\'class_num\']\n\n        start = time.time()\n        if self.START:\n\n            start = time.time()\n\n            data_type = 1\n            if self.metadata[\'language\'] == \'ZH\':\n                data_lan = 0\n            else:\n                data_lan = 1\n            deal_seg = 1\n            sentence_len = self.FIRST_CUT\n            x_test = np.array(x_test, dtype=\'object\')\n            x_test = self.deal_data(x_test, data_lan, data_type, deal_seg, sentence_len)\n\n            self.Xtest = x_test\n            self.START = False\n\n        elif self.FIRSTROUND:\n            pass\n        elif self.LASTROUND:\n            self.LASTROUND = False\n        else:\n            if self.new_data:\n                self.new_data = False\n                if self.data_id == 3:\n                    if self.Xtest is None:\n\n                        start = time.time()\n\n                        data_type = 1\n                        if self.metadata[\'language\'] == \'ZH\':\n                            data_lan = 0\n                        else:\n                            data_lan = 1\n                        deal_seg = 2\n                        sentence_len = self.SENTENCE_LEN\n                        x_test = np.array(x_test, dtype=\'object\')\n                        x_test = self.deal_data(x_test, data_lan, data_type, deal_seg, sentence_len)\n\n                        self.Xtest = x_test\n                    else:\n                        x_test = self.Xtest\n\n                else:\n                    print(\'###Start Init TestData \')\n                    start = time.time()\n\n                    data_type = 1\n                    if self.metadata[\'language\'] == \'ZH\':\n                        data_lan = 0\n                    else:\n                        data_lan = 1\n                    deal_seg = self.data_id\n                    sentence_len = self.SENTENCE_LEN\n                    x_test = np.array(x_test, dtype=\'object\')\n                    x_test = self.deal_data(x_test, data_lan, data_type, deal_seg, sentence_len)\n\n                    self.Xtest = x_test\n\n        x_test = self.Xtest\n\n        batch_size = 32 * int(len(x_test) / 2000)\n        batch_size = min(batch_size, 2048)\n        batch_size = max(batch_size, 32)\n\n        batch_size = self.get_batch_size(len(x_test), batch_size)\n        result = model.predict(x_test, batch_size=batch_size)  #\n\n        y_test = self.trasf_res(result, test_num, class_num)\n\n        self.best_test_res[self.model_num] = y_test\n\n        if self.model_num == 0:\n            self.best_res = y_test\n        else:\n            if self.is_best:\n                self.is_best = False\n                self.best_res = y_test\n            else:\n                y_test = self.best_res\n\n        return y_test\n'"
AutoDL_sample_code_submission/Auto_NLP/second_stage_models/set_up.py,0,"b'from distutils.core import setup\nfrom Cython.Build import cythonize\n \nsetup(\n    ext_modules = cythonize(""ac.pyx"")\n)\n'"
AutoDL_sample_code_submission/Auto_NLP/second_stage_models/tf_model.py,0,"b'import tensorflow as tf \nimport numpy as np\n\nclass FT_tf_model(object):\n    def __init__(self, config):\n        self.sequence_length = config[\'sequence_length\']\n        self.embedding_size = config[\'embedding_size\']\n        self.vocabulary_size = config[\'vocabulary_size\']\n        self.num_classes = config[\'num_classes\']\n        \n        self.rr = 52\n        self.add_rr = 20\n        self.l2_reg_lambda = 0\n        self.hidden_size = 128\n        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=""input_x"")\n        self.input_y = tf.placeholder(tf.float32, [None, self.num_classes], name=""input_y"")\n\n        self.ft_w_embed = self.get_token_embeddings(self.vocabulary_size, self.embedding_size, zero_pad = False, name = \'w_embed\')\n        \n        \n        with tf.variable_scope(\'FT\', reuse=tf.AUTO_REUSE):\n            embed1_word = tf.nn.embedding_lookup(self.ft_w_embed, self.input_x) \n            self.embed1_word = tf.expand_dims(embed1_word, -1)\n            \n            pooled_outputs = []\n            filter_sizes = [2,3]\n            for i, filter_size in enumerate(filter_sizes):\n                with tf.name_scope(""conv-maxpool-%s"" % filter_size):\n                    filter_shape = [filter_size, self.embedding_size, 1, 64]\n                    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W"")\n                    b = tf.Variable(tf.constant(0.1, shape=[64]), name=""b"")\n                    conv = tf.nn.conv2d(\n                        self.embed1_word,\n                        W,\n                        strides=[1, 1, 1, 1],\n                        padding=""VALID"",\n                        name=""conv"")\n                    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=""relu"")\n                    pooled = tf.nn.max_pool(\n                        h,\n                        ksize=[1, self.sequence_length - filter_size + 1, 1, 1],\n                        strides=[1, 1, 1, 1],\n                        padding=\'VALID\',\n                        name=""pool"")\n                    pooled_outputs.append(pooled)\n            \n            num_filters_total = 64 * len(filter_sizes)\n            self.h_pool = tf.concat(pooled_outputs, 3)\n            self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n            W1 = tf.get_variable(shape=[num_filters_total, 128], dtype=tf.float32, name=\'des_w1\', initializer=tf.contrib.layers.xavier_initializer())\n            b1 = tf.get_variable(shape=[128], dtype=tf.float32, name=\'des_b1\', initializer=tf.contrib.layers.xavier_initializer())\n            x1 = tf.nn.xw_plus_b(self.h_pool_flat, W1, b1, name=""dense1"")\n            x1 = tf.nn.relu(x1)\n            \n            W = tf.get_variable(shape=[128, self.num_classes], dtype=tf.float32, name=\'des_w\', initializer=tf.contrib.layers.xavier_initializer())\n            b = tf.get_variable(shape=[self.num_classes], dtype=tf.float32, name=\'des_b\', initializer=tf.contrib.layers.xavier_initializer())\n            x = tf.nn.xw_plus_b(x1, W, b, name=""dense"")\n            \n            if self.num_classes == 2:\n                self.probs = tf.nn.sigmoid(x)\n                self.losses = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=x, labels=self.input_y))\n            else:\n                self.probs = tf.nn.softmax(x)\n                self.losses = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=x, labels=self.input_y))\n        \n        \n    def get_token_embeddings(self, vocab_size, num_units, zero_pad=True, name = \'shared_weight_matrix\'):\n\n        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n            embeddings = tf.get_variable(name+\'_tag_weight_mat\', dtype=tf.float32, shape=(vocab_size, num_units), initializer=tf.contrib.layers.xavier_initializer())\n            if zero_pad:\n                embeddings = tf.concat((tf.zeros(shape=[1, num_units]), embeddings[1:, :]), 0)\n            \n            return embeddings\n\n    \n    \n    \n    def fit(self, X_list, Y, epochs = 1, callbacks = None, verbose = 1, batch_size = 64, shuffle = True):\n        \n        index = [ i for i in range(len(Y))]\n        np.random.shuffle(index)\n        X = X_list[0]\n        Y_b = np.eye(self.num_classes)[Y]\n        X = X[index]\n        Y_b = Y_b[index]\n        \n        if batch_size >= len(X):\n            batch_size = int(len(X)/2)\n        \n        train_step = tf.train.AdamOptimizer(0.0025).minimize(self.losses)\n        init_global = tf.global_variables_initializer()\n        init_local = tf.local_variables_initializer()\n        saver = tf.train.Saver() \n        with tf.Session() as sess:\n            sess.run([init_global, init_local])\n            rounds = min(int(len(X)/batch_size), self.rr)\n\n            for i in range(rounds):\n                start = i*batch_size\n                end = (i+1)*batch_size\n                _ = sess.run(train_step, feed_dict = {self.input_x:X[start:end], self.input_y:Y_b[start:end]})\n                \n            self.rr += self.add_rr\n            saver.save(sess, \'ft.ckpt\')\n            \n        return 0\n            \n\n    def predict(self, X, batch_size = 64, training = False):\n        if batch_size >= len(X):\n            batch_size = int(len(X)/2)\n            \n        rounds = int(len(X)/batch_size)\n        saver = tf.train.Saver() \n        with tf.Session() as sess:\n            saver.restore(sess, \'ft.ckpt\')\n            for i in range(rounds):\n                start = i*batch_size\n                end = (i+1)*batch_size \n                if i == 0:\n                    probs = sess.run(self.probs, feed_dict = {self.input_x:X[start:end]})\n                else:\n                    p = sess.run(self.probs, feed_dict = {self.input_x:X[start:end]})\n                    probs = np.concatenate((probs, p), axis = 0)\n            if rounds*batch_size < len(X):\n                start = rounds*batch_size\n                p = sess.run(self.probs, feed_dict = {self.input_x:X[start:]})\n                probs = np.concatenate((probs, p), axis = 0)\n        if self.num_classes == 2:\n            return probs[:,1]\n        else:\n            return probs\n'"
AutoDL_sample_code_submission/Auto_NLP/utils/__init__.py,0,b'# -*- coding: utf-8 -*-\n# @Date    : 2020/4/1 14:27\n# @Author  : stellahong (stellahong@fuzhi.ai)\n# @Desc    :'
AutoDL_sample_code_submission/Auto_NLP/utils/log_utils.py,0,"b'# -*- coding: utf-8 -*-\n# @Date    : 2020/1/8 16:16\n# @Author  : stellahong (stellahong@fuzhi.ai)\n# @Desc    :\nimport logging\nimport sys\nimport os\nverbosity_level = \'INFO\'\n\ndef get_logger(verbosity_level, use_error_log=False, log_path=None):\n    """"""Set logging format to something like:\n       2019-04-25 12:52:51,924 INFO score.py: <message>\n  """"""\n    logger = logging.getLogger(__file__)\n    logging_level = getattr(logging, verbosity_level)\n    logger.setLevel(logging_level)\n\n    if log_path is None:\n        log_dir = os.path.join(""./"", ""log"")\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n        log_path = os.path.join(log_dir, ""log.txt"")\n    else:\n        log_path = os.path.join(log_path, ""log.txt"")\n\n    formatter = logging.Formatter(\n        fmt=\'%(asctime)s %(levelname)s %(filename)s: %(funcName)s: %(lineno)d: %(message)s\')\n    stdout_handler = logging.StreamHandler(sys.stdout)\n    stdout_handler.setLevel(logging_level)\n    stdout_handler.setFormatter(formatter)\n    logger.addHandler(stdout_handler)\n    fh = logging.FileHandler(log_path)\n    fh.setFormatter(formatter)\n    fh.setLevel(logging_level)\n    logger.addHandler(fh)\n\n    if use_error_log:\n        stderr_handler = logging.StreamHandler(sys.stderr)\n        stderr_handler.setLevel(logging.WARNING)\n        stderr_handler.setFormatter(formatter)\n        logger.addHandler(stderr_handler)\n    logger.propagate = False\n    return logger\n\n\nlogger = get_logger(verbosity_level)'"
AutoDL_sample_code_submission/Auto_NLP/utils/time_utils.py,0,"b'# -*- coding: utf-8 -*-\n# @Date    : 2019/8/4 17:01\n# @Author  : stellahong (stellahong@fuzhi.ai)\n# @Desc    :\nimport os\nimport sys\nimport time\nimport logging\nfrom typing import Any\nimport multiprocessing\nfrom multiprocessing import Pool\nfrom joblib import Parallel, delayed\n\nnesting_level = 0\nis_start = None\n\nNCPU = multiprocessing.cpu_count() - 1\n\ncolor_list = [\'black\', \'red\', \'green\', \'yellow\', \'blue\', \'purple\', \'qinglan\', \'white\']\ncolor_range = range(30, 38)\ncolor_dict = dict(zip(color_list, color_range))\n\nfrom collections import defaultdict, OrderedDict, Callable\n\n\nlog_dir = os.path.dirname(__file__)\n\ndef get_logger(verbosity_level, use_error_log=False, log_path=None):\n    """"""Set logging format to something like:\n         2019-04-25 12:52:51,924 INFO score.py: <message>\n    """"""\n    logger = logging.getLogger(__file__)\n    logging_level = getattr(logging, verbosity_level)\n    logger.setLevel(logging_level)\n\n    if log_path is None:\n        log_dir = os.path.join(""./"", ""log"")\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n        log_path = os.path.join(log_dir, ""log.txt"")\n    else:\n        log_path = os.path.join(log_path, ""log.txt"")\n\n    formatter = logging.Formatter(\n        fmt=\'%(asctime)s %(levelname)s %(filename)s: %(message)s\')\n    stdout_handler = logging.StreamHandler(sys.stdout)\n    stdout_handler.setLevel(logging_level)\n    stdout_handler.setFormatter(formatter)\n    logger.addHandler(stdout_handler)\n    fh = logging.FileHandler(log_path)\n    fh.setFormatter(formatter)\n    logger.addHandler(fh)\n\n    if use_error_log:\n        stderr_handler = logging.StreamHandler(sys.stderr)\n        stderr_handler.setLevel(logging.WARNING)\n        stderr_handler.setFormatter(formatter)\n        logger.addHandler(stderr_handler)\n    logger.propagate = False\n    return logger\n\n\nlogger = get_logger(\'INFO\', log_path=log_dir)\n\ndebug = logger.debug\ninfo = logger.info\nwarning = logger.warning\nerror = logger.error\n\n\nclass DefaultOrderedDict(OrderedDict):\n    # Source: http://stackoverflow.com/a/6190500/562769\n    def __init__(self, default_factory=None, *a, **kw):\n        if (default_factory is not None and\n                not isinstance(default_factory, Callable)):\n            raise TypeError(\'first argument must be callable\')\n        OrderedDict.__init__(self, *a, **kw)\n        self.default_factory = default_factory\n\n    def __getitem__(self, key):\n        try:\n            return OrderedDict.__getitem__(self, key)\n        except KeyError:\n            return self.__missing__(key)\n\n    def __missing__(self, key):\n        if self.default_factory is None:\n            raise KeyError(key)\n        self[key] = value = self.default_factory()\n        return value\n\n    def __reduce__(self):\n        if self.default_factory is None:\n            args = tuple()\n        else:\n            args = self.default_factory,\n        return type(self), args, None, None, self.items()\n\n    def copy(self):\n        return self.__copy__()\n\n    def __copy__(self):\n        return type(self)(self.default_factory, self)\n\n    def __deepcopy__(self, memo):\n        import copy\n        return type(self)(self.default_factory,\n                          copy.deepcopy(self.items()))\n\n    def __repr__(self):\n        return \'OrderedDefaultDict(%s, %s)\' % (self.default_factory,\n                                               OrderedDict.__repr__(self))\n\n\ntime_records = DefaultOrderedDict(list)\n\n\nclass TimerD:\n    def __init__(self):\n        self.start = time.time()\n        self.history = [self.start]\n\n    def check(self, info):\n        current = time.time()\n        duration = current - self.history[-1]\n        if duration < 0.05:\n            pass\n        else:\n            # log(f""[{info}] spend {duration}sec"")\n            pass\n        self.history.append(current)\n\n\ndef colored(s, color=\'red\'):\n    return \'\\033[1;{0}m{1}\\033[0m\'.format(color_dict[color], s)\n\n\ndef log(entry: Any):\n    global nesting_level\n    space = ""-"" * (4 * nesting_level)\n    logger.info(""{0}{1}"".format(space, entry))\n\n\ndef timeit_factory(oneline=False, end_log=None):\n    def timeit(method, start_log=None):\n        def timed(*args, **kw):\n            global is_start\n            global nesting_level\n\n            if not is_start and not oneline:\n                print()\n\n            is_start = True\n            if not oneline:\n                log(""Start [{0}]:"" + (start_log if start_log else """").format(method.__name__))\n            nesting_level += 1\n\n            start_time = time.time()\n            result = method(*args, **kw)\n            end_time = time.time()\n\n            nesting_level -= 1\n            duration = end_time - start_time\n            duration_str = \'{}\'.format(duration)\n            # if duration > 10:\n            duration_str = colored(duration_str, color=\'purple\')\n            time_records[nesting_level].append((method.__name__, duration))\n            log(""End   [{0}]. Time elapsed: {1} sec.{2}"".format(method.__name__,\n                                                                duration_str,\n                                                                end_log))\n            is_start = False\n\n            return result\n\n        return timed\n\n    return timeit\n\n\ndef NCPUP(*args, **kwargs):\n    return Parallel(n_jobs=NCPU, verbose=1)(*args, **kwargs)\n\n\ndef NCPUDICT(fn, *args, **kwargs):\n    dict = {}\n    res = []\n    p = Pool(NCPU)\n    for i in range(NCPU):\n        res.append(p.apply_async(fn, args=(i, kwargs)))\n    p.close()\n    p.join()\n    for i in res:\n        dict.update(i.get())\n    for k, v in dict.items():\n        print(k, v)\n    return dict\n\n\ntimeit_endl = timeit_factory(oneline=False, end_log=\'\\n\')\ntimeit = timeit_factory(oneline=False)\ntimeit_ol = timeit_factory(oneline=True)\n'"
AutoDL_sample_code_submission/Auto_NLP/utils/utils.py,0,"b'# -*- coding: utf-8 -*-\n# @Date    : 2020/1/17 14:32\n# @Author  : stellahong (stellahong@fuzhi.ai)\n# @Desc    :\nimport os\nimport re\nimport string\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\n# print(stopwords)\nfrom keras.preprocessing import text\nfrom keras.preprocessing import sequence\nfrom nltk.stem.snowball import EnglishStemmer, SnowballStemmer\nstemmer = SnowballStemmer(\'english\')\n\nMAX_SEQ_LENGTH = 601\nMAX_CHAR_LENGTH = 96\nMAX_EN_CHAR_LENGTH = 35\nimport multiprocessing\nfrom multiprocessing import Pool\nwith open(os.path.join(os.path.dirname(__file__), ""en_stop_words_nltk.txt""), ""r+"",encoding=\'utf-8\') as fp:\n    nltk_english_stopwords = fp.readlines()\n    nltk_english_stopwords = [word.strip() for word in nltk_english_stopwords]\n\nfull_stop_words = list(stopwords)+nltk_english_stopwords\nNCPU = multiprocessing.cpu_count() - 1\n\ndef set_mp(processes=4):\n    import multiprocessing as mp\n\n    def init_worker():\n        import signal\n        signal.signal(signal.SIGINT, signal.SIG_IGN)\n\n    global pool\n    try:\n        pool.terminate()\n    except BaseException:\n        pass\n\n    if processes:\n        pool = mp.Pool(processes=processes, initializer=init_worker)\n    else:\n        pool = None\n    return pool\n\n\ndef clean_zh_text(dat, ratio=0.1, is_ratio=False):\n    REPLACE_BY_SPACE_RE = re.compile(\'[\xe2\x80\x9c\xe2\x80\x9d\xe3\x80\x90\xe3\x80\x91/\xef\xbc\x88\xef\xbc\x89\xef\xbc\x9a\xef\xbc\x81\xef\xbd\x9e\xe3\x80\x8c\xe3\x80\x8d\xe3\x80\x81|\xef\xbc\x8c\xef\xbc\x9b\xe3\x80\x82""/(){}\\[\\]\\|@,\\.;]\')\n\n    ret = []\n    for line in dat:\n        line = REPLACE_BY_SPACE_RE.sub(\' \', line)\n        line = line.strip()\n\n        if is_ratio:\n            NUM_CHAR = max(int(len(line) * ratio), MAX_CHAR_LENGTH)\n        else:\n            NUM_CHAR = MAX_CHAR_LENGTH\n\n        if len(line) > NUM_CHAR:\n            line = line[0:NUM_CHAR]\n        ret.append(line)\n    return ret\n\ndef clean_en_text(dat, ratio=0.1, is_ratio=True, vocab=None, rmv_stop_words=True):\n\n    trantab = str.maketrans(dict.fromkeys(string.punctuation+""@!#$%^&*()-<>[]<=>;:?.\\/+[\\\\]^_`{|}~\\t\\n""+\'0123456789\'))\n    ret = []\n    for line in dat:\n        line = line.strip()\n        line = line.translate(trantab)\n        line_split = line.split()\n        line_split = [word.lower() for word in line_split if (len(word)<MAX_EN_CHAR_LENGTH and len(word)>1)]\n        if vocab is not None:\n            # print(""use tfidf vocab!"")\n            _line_split = list(set(line_split).intersection(vocab))\n            _line_split.sort(key=line_split.index)\n            line_split = _line_split\n\n\n        if rmv_stop_words:\n\n            new_line_split = list(set(line_split).difference(set(full_stop_words)))\n            new_line_split.sort(key=line_split.index)\n\n        else:\n            new_line_split = line_split\n\n        if is_ratio:\n            NUM_WORD = max(int(len(new_line_split) * ratio), MAX_SEQ_LENGTH)\n        else:\n            NUM_WORD = MAX_SEQ_LENGTH\n        # new_line_split = [stemmer.stem(word) for word in new_line_split]\n        if len(new_line_split) > NUM_WORD:\n            line = "" "".join(new_line_split[0:NUM_WORD])\n        else:\n            line = "" "".join(new_line_split)\n        ret.append(line)\n\n    return ret\n\ndef chunkIt(seq, num):\n\n    avg = len(seq) / float(num)\n    out = []\n    last = 0.0\n\n    while last < len(seq):\n        out.append(seq[int(last):int(last + avg)])\n        last += avg\n\n    return out\n\ndef clean_en_text_parallel(dat, worker_num=NCPU, partition_num=10, vocab=None):\n    sub_data_list = chunkIt(dat, num=partition_num)\n    p = Pool(processes=worker_num)\n\n    data = p.map(clean_en_text, sub_data_list)\n    p.close()\n\n    flat_data = [item for sublist in data for item in sublist]\n\n    return flat_data\n\ndef clean_data(data, language, max_length, vocab, rmv_stop_words=True):\n    if language==""EN"":\n        data = clean_en_text(data, vocab=vocab, rmv_stop_words=rmv_stop_words)\n    else:\n        data = clean_zh_text(data)\n    return data\n\ndef clean_en_text_single(line, vocab, ratio=0.1, is_ratio=True, rmv_stop_words=True):\n    trantab = str.maketrans(\n        dict.fromkeys(string.punctuation + ""@!#$%^&*()-<>[]<=>;:?.\\/+[\\\\]^_`{|}~\\t\\n"" + \'0123456789\'))\n    line = line.strip()\n    line = line.translate(trantab)\n    line_split = line.split()\n    line_split = [word.lower() for word in line_split if (len(word) < MAX_EN_CHAR_LENGTH and len(word) > 1)]\n    if vocab is not None:\n        _line_split = list(set(line_split).intersection(vocab))\n        _line_split.sort(key=line_split.index)\n        line_split = _line_split\n\n\n    if rmv_stop_words:\n        new_line_split = list(set(line_split).difference(set(full_stop_words)))\n        new_line_split.sort(key=line_split.index)\n    else:\n        new_line_split = line_split\n\n    if is_ratio:\n        NUM_WORD = max(int(len(new_line_split) * ratio), MAX_SEQ_LENGTH)\n    else:\n        NUM_WORD = MAX_SEQ_LENGTH\n    if len(new_line_split) > NUM_WORD:\n        _line = "" "".join(new_line_split[0:NUM_WORD])\n    else:\n        _line = "" "".join(new_line_split)\n    return _line\n\ndef pad_sequence(data_ids, padding_val, max_length):\n\n    x_ids = sequence.pad_sequences(data_ids, maxlen=max_length, padding=\'post\', value=padding_val)\n    return x_ids\n\n'"
AutoDL_sample_code_submission/Auto_Tabular/feature/feat_engine.py,0,"b""from .feat_gen import *\nfrom sklearn.utils import shuffle\nfrom Auto_Tabular.utils.log_utils import log ,timeit\n\nclass FeatEngine:\n    def __init__(self):\n\n        self.order2s = []\n\n    def fit(self, data_space, order):\n        if order != 2:\n            return\n        order_name = 'order{}s'.format(order)\n        pipline = getattr(self, order_name)\n        self.feats = []\n        for feat_cls in pipline:\n            feat = feat_cls()\n            feat.fit(data_space)\n            self.feats.append(feat)\n\n    def transform(self, data_space, order):\n        for feat in self.feats:\n            feat.transform(data_space)\n\n    @timeit\n    def fit_transform(self, data_space, order, info=None):\n        if order != 2:\n            return\n        order_name = 'order{}s'.format(order)\n        pipline = getattr(self, order_name)\n        X, y = data_space.data, data_space.y\n        cats = data_space.cat_cols\n        for feat_cls in pipline:\n            feat = feat_cls()\n            feat.fit_transform(X, y, cat_cols=cats, num_cols=info['imp_nums'])\n        data_space.data = X\n        data_space.update = True\n\n\n"""
AutoDL_sample_code_submission/Auto_Tabular/feature/feat_gen.py,0,"b""import numpy as np\nimport pandas as pd\nfrom .feat_namer import FeatNamer\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom Auto_Tabular import CONSTANT\nfrom joblib import Parallel, delayed\nfrom Auto_Tabular.utils.log_utils import log ,timeit\n\n\nclass TargetEncoder:\n    def __init__(self, cols):\n        self.cats = cols\n        self.map = {}\n        self.means = {}\n        self.a = 1\n        self.num_class = None\n        self.y_df = None\n\n    def fit(self, X, y):\n        self.num_class = y.shape[1]\n        self.y_df = pd.DataFrame(y, columns=range(self.num_class), index=X.index).astype(float)\n\n        for i in range(self.num_class):\n            y_ss = y[i]\n            self.means[i] = y_ss.mean()\n\n        for col in self.cats:\n            for i in range(self.num_class):\n                y_ss = self.y_df[i]\n                result = y_ss.groupby(X[col]).agg(['sum', 'count'])\n                self.map[(col, i)] = result\n\n    def transform(self, X, y=None):\n\n        for (col, i), colmap in self.map.items():\n            if y is None:\n                level_notunique = colmap['count'] > 1\n                level_means = ((colmap['sum'] + self.means[i]) / (colmap['count'] + self.a)).where(level_notunique, self.means[i])\n                ss = X[col].map(level_means)\n                col_name = 'n_target_encode_{}_{}'.format(col, i)\n                X[col_name] = ss\n            else:\n                y_ss = self.y_df[i]\n                temp = y_ss.groupby(X[col].astype(str)).agg(['cumsum', 'cumcount'])\n                ss = (temp['cumsum'] - y_ss + self.means[i]) / (temp['cumcount'] + self.a)\n                col_name = 'n_target_encode_{}_{}'.format(col, i)\n                X[col_name] = ss\n\n    def fit_transform(self, X, y):\n        self.fit(X, y)\n        self.transform(X, y)\n\n\nclass MyTargetEncoder:\n    def __init__(self, cols):\n        self.cats = cols\n        self.map = {}\n        self.means = {}\n        self.a = 1\n        self.num_class = None\n        self.y_df = None\n        self.folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n\n    def fit(self, X, y):\n        self.num_class = y.shape[1]\n        self.y_df = pd.DataFrame(y, columns=range(self.num_class), index=X.index).astype(float)\n\n        for i in range(self.num_class):\n            y_ss = y[i]\n            self.means[i] = y_ss.mean()\n\n        for col in self.cats:\n            for i in range(self.num_class):\n                y_ss = self.y_df[i]\n                result = y_ss.groupby(X[col]).agg(['sum', 'count'])\n                self.map[(col, i)] = result\n\n    def transform(self, X, y=None):\n\n        for (col, i), colmap in self.map.items():\n            if y is None:\n                level_notunique = colmap['count'] > 1\n                level_means = ((colmap['sum'] + self.means[i]) / (colmap['count'] + self.a)).where(level_notunique, self.means[i])\n                ss = X[col].map(level_means)\n                col_name = 'n_target_encode_{}_{}'.format(col, i)\n                X[col_name] = ss\n            else:\n                y_ss = self.y_df[i]\n                temp = y_ss.groupby(X[col].astype(str)).agg(['cumsum', 'cumcount'])\n                ss = (temp['cumsum'] - y_ss + self.means[i]) / (temp['cumcount'] + self.a)\n                col_name = 'n_target_encode_{}_{}'.format(col, i)\n                X[col_name] = ss\n\n    def fit_transform(self, X, y):\n        self.fit(X, y)\n        self.transform(X, y)\n\n\n\nclass GroupbyMeanMinusSelf:\n\n\n    def fit(self, X, y, cat_cols, num_cols):\n        pass\n\n    def transform(self, X, y, cat_cols, num_cols):\n        num_cols = [i for i in num_cols if i.startswith('n_')]\n\n        if cat_cols == [] or num_cols == []:\n            return\n\n\n        def groupby_mean(df):\n            cat_col, num_col = df.columns[0], df.columns[1]\n\n            means = df.groupby(cat_col, sort=False)[num_col].mean()\n            ss1 = df[cat_col].map(means)\n\n            param = 'mean'\n            obj= '({})({})'.format(cat_col, num_col)\n            ss1.name = FeatNamer.gen_feat_name(self.__class__.__name__, obj, param, CONSTANT.NUMERICAL_TYPE)\n\n            ss2 = ss1 - df[num_col]\n            param = 'minus'\n            ss2.name = FeatNamer.gen_feat_name(self.__class__.__name__, obj, param, CONSTANT.NUMERICAL_TYPE)\n\n            return ss1, ss2\n\n        exec_cols = []\n        for col1 in cat_cols:\n            for col2 in num_cols:\n                exec_cols.append((col1, col2))\n\n        opt = groupby_mean\n        res = Parallel(n_jobs=CONSTANT.JOBS, require='sharedmem')(\n            delayed(opt)(X[[col1, col2]]) for col1, col2 in exec_cols)\n        if res:\n            for tp in res:\n                for ss in tp:\n                    X[ss.name] = ss\n\n    @timeit\n    def fit_transform(self, X, y, cat_cols, num_cols):\n        self.fit(X, y, cat_cols, num_cols)\n        self.transform(X, y, cat_cols, num_cols)\n\n\n\n\n\n\n\n\n\n\n\n"""
AutoDL_sample_code_submission/Auto_Tabular/feature/feat_namer.py,0,"b'from Auto_Tabular import CONSTANT\r\n\r\nclass FeatNamer:\r\n    @staticmethod\r\n    def gen_feat_name(cls_name, feat_name, param, feat_type):\r\n        prefix = CONSTANT.type2prefix[feat_type]\r\n        if param == None:\r\n            return ""{}{}:{}"".format(prefix, cls_name, feat_name)\r\n        else:\r\n            return ""{}{}:{}:{}"".format(prefix, cls_name, feat_name, param)\r\n\r\n\r\n'"
AutoDL_sample_code_submission/Auto_Tabular/feature/feat_opt.py,0,b''
AutoDL_sample_code_submission/Auto_Tabular/model_lib/__init__.py,0,b'\nfrom .logistic_regression import LogisticRegression\nfrom .dnn import DnnModel\nfrom .my_emb_nn import ENNModel\nfrom .lgb import LGBModel\nfrom .xgb import XGBModel\nfrom .cb import CBModel\n\n\n'
AutoDL_sample_code_submission/Auto_Tabular/model_lib/cb.py,0,"b'from catboost import CatBoostClassifier\nfrom sklearn.metrics import roc_auc_score\nimport hyperopt\nfrom hyperopt import STATUS_OK, Trials, hp, space_eval, tpe\nimport lightgbm as lgb\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\n\nfrom Auto_Tabular.utils.log_utils import log, timeit\nfrom Auto_Tabular import CONSTANT\nfrom Auto_Tabular.utils.data_utils import ohe2cat\nfrom .meta_model import MetaModel\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n\nclass CBModel(MetaModel):\n\n    def __init__(self):\n        super(CBModel, self).__init__()\n        self.max_run = 2\n        self.all_data_round = 1\n        self.explore_params_round = 0\n\n        self.not_gain_threhlod = 3\n\n        self.patience = 3\n\n        self.is_init = False\n\n        self.name = \'cb\'\n        self.type = \'tree\'\n\n        self._model = None\n\n        self.params = {\n            \'task_type\': \'GPU\',\n            ""loss_function"": ""MultiClass"",\n            ""random_seed"": CONSTANT.SEED,\n            \'verbose\': False\n        }\n\n        self.hyperparams = {\n            ""learning_rate"": 0.02,\n            \'iterations\': 1200,\n        }\n\n        self.is_multi_label = None\n\n        self.num_class = None\n\n        self.models = {}\n\n    def init_model(self, num_class, **kwargs):\n        self.is_init = True\n        self.num_class = num_class\n\n    #@timeit\n    def epoch_train(self, dataloader, run_num, is_multi_label=None, info=None, time_remain=None):\n        self.is_multi_label = is_multi_label\n        X, y, train_idxs, cat = dataloader[\'X\'], dataloader[\'y\'], dataloader[\'train_idxs\'], dataloader[\'cat_cols\']\n        train_x, train_y = X.loc[train_idxs], y[train_idxs]\n\n        if info[\'mode\'] == \'bagging\':\n            self.hyperparams = info[\'cb\'].copy()\n            self.hyperparams[\'random_seed\'] = np.random.randint(0, 2020)\n            run_num = self.explore_params_round\n\n\n        if run_num == self.explore_params_round:\n            train_x, train_y, val_x, val_y, = self.split_data(train_x, train_y)\n\n            self.import_cols = info[\'imp_cols\']\n\n            if train_x.shape[1] > 300 and train_x.shape[0] > 20000:\n                train_x = train_x[self.import_cols[:300]]\n                val_x = val_x[self.import_cols[:300]]\n                train_x.reset_index(drop=True, inplace=True)\n                train_x = train_x.sample(n=20000)\n                train_y = train_y[list(train_x.index)]\n\n            elif train_x.shape[0] > 20000:\n                train_x.reset_index(drop=True, inplace=True)\n                train_x = train_x.sample(n=20000)\n                train_y = train_y[list(train_x.index)]\n\n            elif train_x.shape[1] > 300:\n                train_x = train_x[self.import_cols[:300]]\n                val_x = val_x[self.import_cols[:300]]\n\n            self.bayes_opt(train_x, val_x, train_y, val_y, cat)\n            self.early_stop_opt(train_x, val_x, train_y, val_y, cat)\n\n            info[\'cb\'] = self.hyperparams.copy()\n\n        train_x, train_y = X.loc[train_idxs], y[train_idxs]\n        if run_num == self.all_data_round:\n            all_train_idxs = dataloader[\'all_train_idxs\']\n            train_x = X.loc[all_train_idxs]\n            train_y = y[all_train_idxs]\n\n        if self.is_multi_label:\n            for cls in range(self.num_class):\n                cls_y = train_y[:, cls]\n                self.models[cls] = CatBoostClassifier(**{**self.params, **self.hyperparams})\n                self.models[cls].fit(train_x, cls_y)\n        else:\n            self._model = CatBoostClassifier(**{**self.params, **self.hyperparams})\n            self._model.fit(train_x, ohe2cat(train_y))\n\n    #@timeit\n    def epoch_valid(self, dataloader):\n        X, y, val_idxs= dataloader[\'X\'], dataloader[\'y\'], dataloader[\'val_idxs\']\n        val_x, val_y = X.loc[val_idxs], y[val_idxs]\n        if not self.is_multi_label:\n            preds = self._model.predict_proba(val_x)\n        else:\n            all_preds = []\n            for cls in range(y.shape[1]):\n                preds = self.models[cls].predict_proba(val_x)\n                all_preds.append(preds[:,1])\n            preds = np.stack(all_preds, axis=1)\n        valid_auc = roc_auc_score(val_y, preds)\n        return valid_auc\n\n    #@timeit\n    def predict(self, dataloader):\n        X, test_idxs = dataloader[\'X\'], dataloader[\'test_idxs\']\n        test_x = X.loc[test_idxs]\n        if not self.is_multi_label:\n            return self._model.predict_proba(test_x)\n        else:\n            all_preds = []\n            for cls in range(self.num_class):\n                preds = self.models[cls].predict_proba(test_x)\n                all_preds.append(preds[:, 1])\n            return np.stack(all_preds, axis=1)\n\n    def bayes_opt(self, X_train, X_eval, y_train, y_eval, categories):\n        if self.is_multi_label:\n            y_train = y_train[:, 1]\n            y_eval = y_eval[:, 1]\n        else:\n            y_train = ohe2cat(y_train)\n\n        space = {\n            ""learning_rate"": hp.loguniform(""learning_rate"", np.log(0.01), np.log(0.1)),\n            ""depth"": hp.choice(""depth"", [4, 6, 8, 10, 12]),\n            ""l2_leaf_reg"": hp.uniform(\'l2_leaf_reg\', 0.1, 2),\n\n        }\n\n        def objective(hyperparams):\n            hyperparams = self.hyperparams.copy()\n            hyperparams[\'iterations\'] = 300\n            model = CatBoostClassifier(**{**self.params, **hyperparams})\n            model.fit(X_train, y_train)\n            pred = model.predict_proba(X_eval)\n\n            if self.is_multi_label:\n                score = roc_auc_score(y_eval, pred[:, 1])\n            else:\n                score = roc_auc_score(y_eval, pred)\n\n            return {\'loss\': -score, \'status\': STATUS_OK}\n\n        trials = Trials()\n        best = hyperopt.fmin(fn=objective, space=space, trials=trials,\n                             algo=tpe.suggest, max_evals=15, verbose=1,\n                             rstate=np.random.RandomState(1))\n\n        self.hyperparams.update(space_eval(space, best))\n\n    def early_stop_opt(self, X_train, X_eval, y_train, y_eval, categories):\n        if self.is_multi_label:\n            y_train = y_train[:, 1]\n            y_eval = y_eval[:, 1]\n        else:\n            y_train = ohe2cat(y_train)\n            y_eval = ohe2cat(y_eval)\n\n        model = CatBoostClassifier(**{**self.params, **self.hyperparams})\n        model.fit(X_train, y_train, eval_set=[(X_eval, y_eval)],\n                  use_best_model=True, verbose=10, early_stopping_rounds=20)\n\n        self.params[\'iterations\'] = model.best_iteration_\n\n    def split_data(self, x, y):\n        new_x = x.copy()\n        new_x.reset_index(drop=True, inplace=True)\n        sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n        self.splits = {}\n        i = 0\n        for train_idxs, val_idxs in sss.split(new_x, y):\n            self.splits[i] = [train_idxs, val_idxs]\n            i += 1\n        new_train_x = new_x.loc[self.splits[0][0]]\n        new_train_y = y[self.splits[0][0]]\n\n        new_val_x = new_x.loc[self.splits[0][1]]\n        new_val_y = y[self.splits[0][1]]\n\n        return new_train_x, new_train_y, new_val_x, new_val_y'"
AutoDL_sample_code_submission/Auto_Tabular/model_lib/dnn.py,0,"b'import librosa\nimport numpy as np\nimport tensorflow as tf\nimport keras\nfrom tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout, Activation, Add\n\nfrom Auto_Tabular.CONSTANT import *\nfrom Auto_Tabular.utils.data_utils import ohe2cat\nfrom .meta_model import MetaModel\nfrom sklearn.metrics import roc_auc_score\n\n\nclass DnnModel(MetaModel):\n    def __init__(self):\n        super(DnnModel, self).__init__()\n        self.max_length = None\n        self.mean = None\n        self.std = None\n\n        self._model = None\n        self.is_init = False\n\n        self.name = \'dnn\'\n        self.type = \'nn_keras\'\n        self.patience = 500\n\n        self.max_run = 1000\n\n        self.all_data_round = 950\n\n        self.not_gain_threhlod = 500\n\n        self.is_multi_label = None\n\n    def init_model(self,\n                   num_classes,\n                   shape=None,\n                   is_multi_label=False,\n                   **kwargs):\n        self.is_multi_label = is_multi_label\n        weight_decay = 1e-3\n        inputs = Input(shape=(shape,))\n        x = BatchNormalization(axis=1)(inputs)\n        x = Dropout(0.4)(x)\n        x_skip = self.fc(x, 500, weight_decay)\n        x = self.fc(x_skip, 500, weight_decay)\n        x = Dropout(0.4)(x)\n        x = Add()([x, x_skip])\n        x_mid = self.fc(x, shape, weight_decay)\n        x = self.fc(x_mid, 500, weight_decay)\n        for i in range(3):\n            x = self.fc(x, 500, weight_decay)\n\n        if self.is_multi_label:\n            logits = Dense(num_classes,\n                           #activation=""softmax"",\n                           #kernel_initializer=""orthogonal"",\n                           use_bias=True,\n                           trainable=True,\n                           kernel_regularizer=keras.regularizers.l2(weight_decay),\n                           bias_regularizer=keras.regularizers.l2(weight_decay),\n                           name=""prediction"")(x)\n            self._model = tf.keras.Model(inputs=inputs, outputs=logits)\n            opt = tf.keras.optimizers.Adam()\n            loss = sigmoid_cross_entropy_with_logits\n        else:\n            preds = Dense(num_classes,\n                           activation=""softmax"",\n                           #kernel_initializer=""orthogonal"",\n                           use_bias=True,\n                           trainable=True,\n                           kernel_regularizer=keras.regularizers.l2(weight_decay),\n                           bias_regularizer=keras.regularizers.l2(weight_decay),\n                           name=""prediction"")(x)\n            self._model = tf.keras.Model(inputs=inputs, outputs=preds)\n            opt = tf.keras.optimizers.Adam()\n            loss = tf.keras.losses.CategoricalCrossentropy()\n\n        self._model.compile(optimizer=opt, loss=loss, metrics=[""acc""])\n        self.is_init = True\n\n    def fc(self, x, out_dim, weight_decay):\n        x = Dropout(0.2)(x)\n        x = Dense(out_dim,\n                  kernel_regularizer=keras.regularizers.l2(weight_decay),\n                  use_bias=False,\n                  )(x)\n        x = BatchNormalization(axis=1)(x)\n        x = Activation(\'relu\')(x)\n        return x\n\n    def epoch_train(self, dataloader, run_num, **kwargs):\n        X, y, train_idxs = dataloader[\'X\'], dataloader[\'y\'], dataloader[\'train_idxs\']\n        train_x, train_y = X.loc[train_idxs].values, y[train_idxs]\n        print(\'epoch train shape\')\n        print(train_x.shape)\n\n        epochs = 5\n\n        if not self.is_multi_label:\n            train_y = ohe2cat(train_y)\n        self._model.fit(train_x, train_y,\n                        epochs=epochs,\n                        #callbacks=callbacks,\n                        #validation_data=(val_x, ohe2cat(val_y)),\n                        # validation_split=0.2,\n                        verbose=1,  # Logs once per epoch.\n                        batch_size=128,\n                        shuffle=True,\n                        # initial_epoch=self.epoch_cnt,\n                        # use_multiprocessing=True\n                        )\n\n    def epoch_valid(self, dataloader):\n        X, y, val_idxs= dataloader[\'X\'], dataloader[\'y\'], dataloader[\'val_idxs\']\n        val_x, val_y = X.loc[val_idxs].values, y[val_idxs]\n        preds = self._model.predict(val_x)\n        if self.is_multi_label:\n            preds = sigmoid(preds)\n        valid_auc = roc_auc_score(val_y, preds)\n        return valid_auc\n\n    def predict(self, dataloader, batch_size=32):\n        X, test_idxs = dataloader[\'X\'], dataloader[\'test_idxs\']\n        test_x = X.loc[test_idxs].values\n        preds = self._model.predict(test_x)\n        if self.is_multi_label:\n            preds = sigmoid(preds)\n        return preds\n\n\ndef sigmoid_cross_entropy_with_logits(y_true, y_pred):\n    #labels = tf.cast(labels, dtype=tf.float32)\n    relu_preds = tf.nn.relu(y_pred)\n    exp_logits = tf.exp(- tf.abs(y_pred))\n    sigmoid_logits = tf.log(1 + exp_logits)\n    element_wise_xent = relu_preds - y_true * y_pred + sigmoid_logits\n    return tf.reduce_sum(element_wise_xent)\n\n\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\n\ndef softmax(x):\n    x = x - x.max(axis=1).reshape(-1,1)\n    e = np.exp(x)\n    return e/e.sum(axis=1).reshape(-1,1)'"
AutoDL_sample_code_submission/Auto_Tabular/model_lib/dnn_n.py,0,"b'import numpy as np\nimport tensorflow as tf\nfrom CONSTANT import *\nfrom utils.data_utils import ohe2cat\nfrom .meta_model import MetaModel\nfrom sklearn.metrics import roc_auc_score\n\n\nclass DnnModel(MetaModel):\n    def __init__(self):\n        super(DnnModel, self).__init__()\n        self.max_length = None\n        self.mean = None\n        self.std = None\n\n        self._model = None\n        self.is_init = False\n\n        self.name = \'dnn\'\n        self.type = \'nn\'\n        self.patience = 10\n\n        self.not_gain_threhlod = 3\n\n    def init_model(self,\n                   num_classes,\n                   **kwargs):\n        self.num_classes = num_classes\n        model_fn = self.model_fn\n        self._model = tf.estimator.Estimator(model_fn=model_fn)\n        self.is_init = True\n\n    def epoch_train(self, dataloader, epoch):\n        dataset_train = dataloader[\'train\']\n        train_input_fn = lambda: self.input_function(dataset_train, \'train\')\n        self._model.train(input_fn=train_input_fn, steps=100)\n\n    def epoch_valid(self, dataloader):\n        dataset_valid, label = dataloader[\'valid\']\n        valid_input_fn = lambda: self.input_function(dataset_valid, \'valid\')\n        valid_results = self._model.predict(input_fn=valid_input_fn)\n        preds = [x[\'probabilities\'] for x in valid_results]\n        preds = np.array(preds)\n        valid_auc = roc_auc_score(label, preds)\n        return valid_auc\n\n    def predict(self, dataloader, batch_size=32):\n        dataset_test = dataloader[\'test\']\n        valid_input_fn = lambda: self.input_function(dataset_test, \'test\')\n        test_results = self._model.predict(input_fn=valid_input_fn)\n        preds = [x[\'probabilities\'] for x in test_results]\n        preds = np.array(preds)\n        return preds\n\n    def model_fn(self, features, labels, mode):\n        is_training = False\n        keep_prob = 1\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            is_training = True\n            keep_prob = 0.8\n\n        input_layer = features\n\n        # Replace missing values by 0\n        mask = tf.is_nan(input_layer)\n        input_layer = tf.where(mask, tf.zeros_like(input_layer), input_layer)\n\n        # Sum over time axis\n        input_layer = tf.reduce_sum(input_layer, axis=1)\n        mask = tf.reduce_sum(1 - tf.cast(mask, tf.float32), axis=1)\n\n        # Flatten\n        input_layer = tf.layers.flatten(input_layer)\n        mask = tf.layers.flatten(mask)\n        f = input_layer.get_shape().as_list()[1]  # tf.shape(input_layer)[1]\n\n        # Build network\n        x = tf.layers.batch_normalization(input_layer, training=is_training)\n        x = tf.nn.dropout(x, keep_prob)\n        x_skip = self.fc(x, 256, is_training)\n        x = self.fc(x_skip, 256, is_training)\n        x = tf.nn.dropout(x, keep_prob)\n        x = self.fc(x, 256, is_training) + x_skip\n        x_mid = self.fc(x, f, is_training)\n\n        x = self.fc(x_mid, 256, is_training)\n        for i in range(3):\n            x = self.fc(x, 256, is_training)\n\n        logits = tf.layers.dense(inputs=x, units=self.num_classes)\n        sigmoid_tensor = tf.nn.sigmoid(logits, name=""sigmoid_tensor"")\n\n        predictions = {\n\n            ""classes"": sigmoid_tensor > 0.5,\n\n            ""probabilities"": sigmoid_tensor\n        }\n\n        if mode == tf.estimator.ModeKeys.PREDICT:\n            return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n\n        loss_labels = tf.reduce_sum(sigmoid_cross_entropy_with_logits(labels=labels, logits=logits))\n        loss_reconst = tf.reduce_sum(mask * tf.abs(tf.subtract(input_layer, x_mid)))\n        loss = loss_labels + loss_reconst\n\n        # Configure the Training Op (for TRAIN mode)\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            optimizer = tf.train.AdamOptimizer()\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            with tf.control_dependencies(update_ops):\n                train_op = optimizer.minimize(\n                    loss=loss,\n                    global_step=tf.train.get_global_step())\n            return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n\n        assert mode == tf.estimator.ModeKeys.EVAL\n        eval_metric_ops = {\n            ""accuracy"": tf.metrics.accuracy(\n                labels=labels, predictions=predictions[""classes""])}\n        return tf.estimator.EstimatorSpec(\n            mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n\n    def fc(self, x, out_dim, is_training):\n        x = tf.layers.dense(inputs=x, units=out_dim)\n        x = tf.layers.batch_normalization(x, training=is_training)\n        x = tf.nn.relu(x)\n        return x\n\n    def input_function(self, dataset, mode):\n        if mode == \'train\':\n            dataset = dataset.shuffle(buffer_size=100)\n            dataset = dataset.repeat()\n\n        dataset = dataset.batch(batch_size=128)\n\n        iterator_name = \'iterator_\' + mode\n\n        if not hasattr(self, iterator_name):\n            self.iterator = dataset.make_one_shot_iterator()\n        iterator = dataset.make_one_shot_iterator()\n        if mode == \'train\':\n            example, labels = iterator.get_next()\n            return example, labels\n        if mode == \'valid\' or mode == \'test\':\n            example = iterator.get_next()\n            return example\n\n\ndef sigmoid_cross_entropy_with_logits(labels, logits):\n    labels = tf.cast(labels, dtype=tf.float32)\n    relu_logits = tf.nn.relu(logits)\n    exp_logits = tf.exp(- tf.abs(logits))\n    sigmoid_logits = tf.log(1 + exp_logits)\n    element_wise_xent = relu_logits - labels * logits + sigmoid_logits\n    return tf.reduce_sum(element_wise_xent)'"
AutoDL_sample_code_submission/Auto_Tabular/model_lib/emb_nn.py,2,"b""import librosa\nimport numpy as np\nimport tensorflow as tf\nimport keras\nfrom tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout, Activation, Add\n\nfrom Auto_Tabular.CONSTANT import *\nfrom Auto_Tabular.utils.data_utils import ohe2cat\nfrom .meta_model import MetaModel\nfrom sklearn.metrics import roc_auc_score\n\nfrom fastai.tabular import *\n\nimport pytorch as torch\n\nimport fastai\nfrom fastai.text import *\n\n\nclass ENNModel(MetaModel):\n    def __init__(self):\n        super(ENNModel, self).__init__()\n        #clear_session()\n        self.max_length = None\n        self.mean = None\n        self.std = None\n\n        self._model = None\n        self.is_init = False\n\n        self.name = 'enn'\n        self.type = 'emb_nn'\n        self.patience = 50\n\n        self.max_run = 100\n\n        self.all_data_round = 80\n\n        self.not_gain_threhlod = 50\n\n        self.data_gen = None\n\n    def init_model(self,\n                   num_classes,\n                   shape=None,\n                   **kwargs):\n\n        self.is_init = True\n\n    def epoch_train(self, dataloader, run_num):\n        if self.data_gen == None:\n            X, y, cats = dataloader['X'], dataloader['y'], dataloader['cat_cols']\n            all_train_idxs, val_idxs, test_idxs = dataloader['all_train_idxs'],  dataloader['val_idxs'], dataloader['test_idxs']\n            all_x, all_y, test_x = X.loc[all_train_idxs], y[all_train_idxs], X.loc[test_idxs]\n            all_x['target'] = all_y\n            procs = []\n            path = './'\n            num_cols = [col for col in X.columns if col not in cats]\n\n            self.data_gen = TabularDataBunch.from_df(\n                path, all_x, 'target', valid_idx=val_idxs, procs=procs, cat_names=cats, cont_names=num_cols)\n            emb_szs = {col: 5 for col in cats}\n            self._model = tabular_learner(self.data_gen , layers=[200, 100], emb_szs=emb_szs, metrics=accuracy)\n\n        self._model.fit(epochs=5)\n\n\n    def epoch_valid(self, dataloader):\n        X, y, val_idxs= dataloader['X'], dataloader['y'], dataloader['val_idxs']\n        val_x, val_y = X.loc[val_idxs].values, y[val_idxs]\n        preds = self._model.predict(val_x)\n        preds = preds.cpu().numpy()[:, 1]\n        valid_auc = roc_auc_score(val_y, preds)\n        return valid_auc\n\n    def predict(self, dataloader, batch_size=32):\n        X, test_idxs = dataloader['X'], dataloader['test_idxs']\n        test_x = X.loc[test_idxs].values\n        preds = self._model.predict(test_x)\n        preds = preds.cpu().numpy()[:, 1]\n        return preds\n\n\n\ndef auroc_score(input, target):\n    input, target = input.cpu().numpy()[:, 1], target.cpu().numpy()\n    return roc_auc_score(target, input)\n\n\n# Callback to calculate AUC at the end of each epoch\n# class AUROC(Callback):\n#     _order = -20  # Needs to run before the recorder\n#\n#     def __init__(self, learn, **kwargs):\n#         self.learn = learn\n#\n#     def on_train_begin(self, **kwargs):\n#         self.learn.recorder.add_metric_names(['AUROC'])\n#\n#     def on_epoch_begin(self, **kwargs):\n#         self.output, self.target = [], []\n#\n#     def on_batch_end(self, last_target, last_output, train, **kwargs):\n#         if not train:\n#             self.output.append(last_output)\n#             self.target.append(last_target)\n#\n#     def on_epoch_end(self, last_metrics, **kwargs):\n#         if len(self.output) > 0:\n#             output = torch.cat(self.output)\n#             target = torch.cat(self.target)\n#             preds = F.softmax(output, dim=1)\n#             metric = auroc_score(preds, target)\n#             return add_metrics(last_metrics, [metric])\n\n\n"""
AutoDL_sample_code_submission/Auto_Tabular/model_lib/lgb.py,0,"b'from sklearn.metrics import roc_auc_score\nimport hyperopt\nfrom hyperopt import STATUS_OK, Trials, hp, space_eval, tpe\nimport lightgbm as lgb\nimport pandas as pd\nimport numpy as np\n\nfrom Auto_Tabular.utils.data_utils import ohe2cat\nfrom Auto_Tabular.utils.log_utils import log ,timeit\nfrom .meta_model import MetaModel\n\nfrom Auto_Tabular import CONSTANT\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom collections import defaultdict\n\npd.set_option(\'display.max_rows\', 100)\n\nclass LGBModel(MetaModel):\n    def __init__(self):\n        super(LGBModel, self).__init__()\n        self.run_num = 0\n        self.max_run = 4\n        self.rise_num = 0\n        self.not_rise_num = 0\n        self.not_gain_num = 0\n\n        self.all_data_round_pre = 1\n        self.all_data_round = 3\n        self.explore_params_round = 2\n\n        self.ensemble_num = 10\n\n        self.not_gain_threhlod = 1\n\n        self.max_length = None\n        self.mean = None\n        self.std = None\n\n        self.patience = 10\n\n        self.is_init = False\n\n        self.name = \'lgb\'\n        self.type = \'tree\'\n\n        self._model = None\n\n        self.models = {}\n        self.ensemble_pred = False\n\n        self.last_preds = None\n\n        self.params = {\n            ""boosting_type"": ""gbdt"",\n            ""objective"": ""multiclass"",\n            \'num_class\': 3,\n            ""metric"": ""multi_logloss"",\n            ""verbosity"": -1,\n            ""seed"": CONSTANT.SEED,\n            ""num_threads"": CONSTANT.JOBS,\n        }\n\n        self.hyperparams = {\n            \'num_leaves\': 31,\n            \'max_depth\': -1,\n            \'min_child_samples\': 20,\n            \'max_bin\': 200,\n            \'subsample\': 0.9,\n            \'subsample_freq\': 1,\n            \'colsample_bytree\': 0.8,\n            \'min_child_weight\': 0.001,\n            \'min_split_gain\': 0.02,\n            \'reg_alpha\': 0.1,\n            \'reg_lambda\': 0.1,\n            ""learning_rate"": 0.08,\n        }\n        self.learning_rates = None\n\n        self.is_multi_label = None\n\n        self.num_class = None\n\n        self.en_models = {}\n\n        self.imp_nums = None\n\n        self.import_cols = None\n\n    def init_model(self, num_class, **kwargs):\n        self.is_init = True\n        self.params.update({\'num_class\': num_class})\n        self.num_class = num_class\n\n    #@timeit\n    def epoch_train(self, dataloader, run_num, is_multi_label=False, info=None, time_remain=None):\n        self.is_multi_label = is_multi_label\n        X, y, train_idxs, cat = dataloader[\'X\'], dataloader[\'y\'], dataloader[\'train_idxs\'], dataloader[\'cat_cols\']\n        train_x, train_y = X.loc[train_idxs], y[train_idxs]\n\n        if info[\'mode\'] == \'bagging\':\n            self.hyperparams = info[\'lgb\'].copy()\n            self.hyperparams[\'seed\'] = np.random.randint(0, 2020)\n            num_leaves = self.hyperparams[\'num_leaves\']\n            self.hyperparams[\'num_leaves\'] += np.random.randint(-int(num_leaves/10), int(num_leaves/10))\n            run_num = 0\n\n        if run_num == self.explore_params_round:\n            print(\'lgb explore_params_round\')\n            train_x, train_y, val_x, val_y,  = self.split_data(train_x, train_y)\n\n            self.log_feat_importances()\n\n            if train_x.shape[1] > 300 and train_x.shape[0] > 20000:\n                train_x = train_x[self.import_cols[:300]]\n                val_x = val_x[self.import_cols[:300]]\n                log(\'explore_params_round sample 300 cols\')\n                train_x.reset_index(drop=True, inplace=True)\n                train_x = train_x.sample(n=20000)\n                train_y = train_y[list(train_x.index)]\n                log(\'explore_params_round sample 2w samples\')\n\n            elif train_x.shape[0] > 20000:\n                train_x.reset_index(drop=True, inplace=True)\n                train_x = train_x.sample(n=20000)\n                train_y = train_y[list(train_x.index)]\n                log(\'explore_params_round sample 2w samples\')\n\n            elif train_x.shape[1] > 300:\n                train_x = train_x[self.import_cols[:300]]\n                val_x = val_x[self.import_cols[:300]]\n                log(\'explore_params_round sample 300 cols\')\n\n            print(\'shape: \', train_x.shape)\n\n            self.bayes_opt(train_x, val_x, train_y, val_y, cat, phase=1)\n            self.early_stop_opt(train_x, val_x, train_y, val_y, cat)\n            info[\'lgb\'] = self.hyperparams.copy()\n            info[\'imp_cols\'] = self.import_cols\n\n        if run_num == self.ensemble_num:\n            print(\'lgb ensemble_num\')\n            splits = dataloader[\'splits\']\n            for i in range(len(splits)):\n                train_idxs, val_idxs = splits[i]\n                train_x, train_y = X.loc[train_idxs], y[train_idxs]\n                hyperparams = self.hyperparams.copy()\n                # num_leaves = hyperparams[\'num_leaves\']\n                # num_leaves += np.random.randint(-int(num_leaves/10), int(num_leaves/10))\n                # hyperparams[\'num_leaves\'] = num_leaves\n                # log(\'model {} leaves {}\'.format(i, num_leaves))\n                if self.is_multi_label:\n                    self.en_models = defaultdict(list)\n                    for cls in range(self.num_class):\n                        cls_y = train_y[:, cls]\n                        lgb_train = lgb.Dataset(train_x, cls_y)\n                        if not self.learning_rates:\n                            self.en_models[i].append(lgb.train({**self.params, **hyperparams}, train_set=lgb_train))\n                        else:\n                            self.en_models[i].append(\n                                lgb.train({**self.params, **hyperparams},\n                                          train_set=lgb_train, learning_rates=self.learning_rates))\n                else:\n                    lgb_train = lgb.Dataset(train_x, ohe2cat(train_y))\n                    if not self.learning_rates:\n                        self.en_models[i] = lgb.train({**self.params, **hyperparams}, train_set=lgb_train)\n                    else:\n                        self.en_models[i] = lgb.train({**self.params, **hyperparams},\n                                                   train_set=lgb_train, learning_rates=self.learning_rates)\n                self.ensemble_pred = True\n\n        else:\n            print(\'lgb norm train\')\n            train_x, train_y = X.loc[train_idxs], y[train_idxs]\n            hyperparams = self.hyperparams.copy()\n            log(\'hyperparams {}\'.format(hyperparams))\n            if run_num == self.all_data_round_pre or run_num == self.all_data_round:\n                print(\'lgb all data round\')\n                all_train_idxs = dataloader[\'all_train_idxs\']\n                train_x = X.loc[all_train_idxs]\n                train_y = y[all_train_idxs]\n            print(\'shape: \', train_x.shape)\n            if not is_multi_label:\n                lgb_train = lgb.Dataset(train_x, ohe2cat(train_y))\n                if not self.learning_rates:\n                    self._model = lgb.train({**self.params, **hyperparams}, train_set=lgb_train)\n                else:\n                    self._model = lgb.train({**self.params, **hyperparams},\n                                            train_set=lgb_train, learning_rates=self.learning_rates)\n            else:\n                self.params[\'num_class\'] = 2\n                for cls in range(self.num_class):\n                    cls_y = train_y[:, cls]\n                    lgb_train = lgb.Dataset(train_x, cls_y)\n                    if not self.learning_rates:\n                        self.models[cls] = lgb.train({**self.params, **self.hyperparams}, train_set=lgb_train)\n                    else:\n                        self.models[cls] = lgb.train({**self.params, **self.hyperparams},\n                                                     train_set=lgb_train, learning_rates=self.learning_rates)\n            self.log_feat_importances()\n            if self.imp_nums is not None:\n                info[\'imp_nums\'] = self.imp_nums\n\n\n    #@timeit\n    def epoch_valid(self, dataloader):\n        X, y, val_idxs= dataloader[\'X\'], dataloader[\'y\'], dataloader[\'val_idxs\']\n        val_x, val_y = X.loc[val_idxs], y[val_idxs]\n        if not self.is_multi_label:\n            preds = self._model.predict(val_x)\n        else:\n            all_preds = []\n            for cls in range(y.shape[1]):\n                preds = self.models[cls].predict(val_x)\n                all_preds.append(preds[:,1])\n            preds = np.stack(all_preds, axis=1)\n        valid_auc = roc_auc_score(val_y, preds)\n        return valid_auc\n\n    #@timeit\n    def predict(self, dataloader):\n        X, test_idxs = dataloader[\'X\'], dataloader[\'test_idxs\']\n        test_x = X.loc[test_idxs]\n        if not self.ensemble_pred:\n            if not self.is_multi_label:\n                preds = self._model.predict(test_x)\n            else:\n                all_preds = []\n                for cls in range(self.num_class):\n                    preds = self.models[cls].predict(test_x)\n                    all_preds.append(preds[:,1])\n                preds = np.stack(all_preds, axis=1)\n\n            self.last_preds = preds\n            return preds\n        else:\n            ensemble_preds = 0\n            preds = 0\n            for i, model in self.en_models.items():\n                if not self.is_multi_label:\n                    preds = model.predict(test_x)\n                else:\n                    all_preds = []\n                    for cls in range(self.num_class):\n                        preds = model[cls].predict(test_x)\n                        all_preds.append(preds[:,1])\n                    preds = np.stack(all_preds, axis=1)\n                m = np.mean(preds)\n                preds = preds/m/5\n                ensemble_preds += preds\n            last_mean = np.mean(self.last_preds)\n            last_preds = self.last_preds/last_mean\n            return 0.7*last_preds + 0.3*preds\n\n    def log_feat_importances(self, return_info=False):\n        if not self.is_multi_label:\n            importances = pd.DataFrame({\'features\': [i for i in self._model.feature_name()],\n                                        \'importances\': self._model.feature_importance(""gain"")})\n        else:\n            importances = pd.DataFrame({\'features\': [i for i in self.models[0].feature_name()],\n                                        \'importances\': self.models[0].feature_importance(""gain"")})\n\n        importances.sort_values(\'importances\', ascending=False, inplace=True)\n\n\n        importances = importances[importances[\'importances\'] > 0]\n\n        size = int(len(importances)*0.8)\n\n\n        self.import_cols = importances[\'features\'][:size].values\n\n        self.imp_nums = list(importances[\'features\'][:30].values)\n\n    #@timeit\n    def bayes_opt(self, X_train, X_eval, y_train, y_eval, categories, phase=1):\n        if self.is_multi_label:\n            train_data = lgb.Dataset(X_train, label=y_train[:,0])\n            valid_data = lgb.Dataset(X_eval, label=y_eval[:,0])\n        else:\n            y_train = ohe2cat(y_train)\n            y_eval = ohe2cat(y_eval)\n            train_data = lgb.Dataset(X_train, label=y_train)\n            valid_data = lgb.Dataset(X_eval, label=y_eval)\n\n        params = self.params\n\n        if phase == 1:\n            space = {\n                \'max_depth\': hp.choice(""max_depth"", [-1, 5, 7, 9]),\n                ""num_leaves"": hp.choice(""num_leaves"", np.linspace(20, 61, 10, dtype=int)),\n                ""reg_alpha"": hp.uniform(""reg_alpha"", 0, 1),\n                ""reg_lambda"": hp.uniform(""reg_lambda"", 0, 1),\n                ""min_child_samples"": hp.choice(""min_data_in_leaf"", np.linspace(10, 120, 10, dtype=int)),\n                ""min_child_weight"": hp.uniform(\'min_child_weight\', 0.01, 1),\n                ""min_split_gain"": hp.uniform(\'min_split_gain\', 0.001, 0.1),\n                \'colsample_bytree\': hp.choice(""colsample_bytree"", [0.7, 0.9]),\n                ""learning_rate"": hp.loguniform(""learning_rate"", np.log(0.01), np.log(0.1)),\n            }\n            tmp_hyperparams = {}\n            tmp_hyperparams[\'num_boost_round\'] = 100\n            max_evals = 20\n\n        else:\n            space = {\n                ""learning_rate"": hp.loguniform(""learning_rate"", np.log(0.01), np.log(0.1)),\n            }\n            tmp_hyperparams = {}\n            update = [\'max_depth\', \'num_leaves\', \'reg_alpha\', \'reg_lambda\', \'min_data_in_leaf\', \'min_child_weight\', \'min_split_gain\']\n\n            for p in update:\n                tmp_hyperparams[p] = self.hyperparams[p]\n\n            tmp_hyperparams[\'num_boost_round\'] = 500\n            max_evals = 5\n\n        def objective(hyperparams):\n            tmp_hyperparams.update(hyperparams)\n            model = lgb.train({**params, **tmp_hyperparams}, train_set=train_data, valid_sets=valid_data,\n                              #categorical_feature=categories,\n                              early_stopping_rounds=18, verbose_eval=0)\n\n            score = model.best_score[""valid_0""][params[""metric""]]\n\n            # in classification, less is better\n            return {\'loss\': score, \'status\': STATUS_OK}\n\n        trials = Trials()\n        best = hyperopt.fmin(fn=objective, space=space, trials=trials,\n                             algo=tpe.suggest, max_evals=max_evals, verbose=1,\n                             rstate=np.random.RandomState(1))\n        self.hyperparams.update(space_eval(space, best))\n\n    #@timeit\n    def early_stop_opt(self, X_train, X_eval, y_train, y_eval, categories):\n        if self.is_multi_label:\n            lgb_train = lgb.Dataset(X_train, y_train[:, 0])\n            lgb_eval = lgb.Dataset(X_eval, y_eval[:, 0], reference=lgb_train)\n        else:\n            y_train = ohe2cat(y_train)\n            y_eval = ohe2cat(y_eval)\n            lgb_train = lgb.Dataset(X_train, y_train)\n            lgb_eval = lgb.Dataset(X_eval, y_eval, reference=lgb_train)\n\n        self.hyperparams[\'num_boost_round\'] = 1000\n        tmp_lr = self.hyperparams.pop(\'learning_rate\')\n        self.learning_rates = self.get_log_lr(1000, tmp_lr, tmp_lr*0.6)\n\n        self._model = lgb.train({**self.params, **self.hyperparams}, verbose_eval=20,\n                          train_set=lgb_train, valid_sets=lgb_eval, valid_names=\'eval\',\n                          early_stopping_rounds=20, learning_rates=self.learning_rates) #categorical_feature=categories)\n\n        self.hyperparams[\'num_boost_round\'] = self._model.best_iteration\n        self.learning_rates = self.learning_rates[:self._model.best_iteration]\n\n\n    def get_log_lr(self,num_boost_round,max_lr,min_lr):\n        learning_rates = [max_lr+(min_lr-max_lr)/np.log(num_boost_round)*np.log(i) for i in range(1,num_boost_round+1)]\n        return learning_rates\n\n\n    def split_data(self, x, y):\n        new_x = x.copy()\n        new_x.reset_index(drop=True, inplace=True)\n        sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n        self.splits = {}\n        i = 0\n        for train_idxs, val_idxs in sss.split(new_x, y):\n            self.splits[i] = [train_idxs, val_idxs]\n            i += 1\n        new_train_x = new_x.loc[self.splits[0][0]]\n        new_train_y = y[self.splits[0][0]]\n\n        new_val_x = new_x.loc[self.splits[0][1]]\n        new_val_y = y[self.splits[0][1]]\n        return new_train_x, new_train_y, new_val_x, new_val_y\n'"
AutoDL_sample_code_submission/Auto_Tabular/model_lib/logistic_regression.py,0,"b""import numpy as np\nfrom sklearn.linear_model import logistic\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import roc_auc_score\n\nfrom CONSTANT import *\nfrom .meta_model import MetaModel\nfrom Auto_Tabular.utils.log_utils import log\nfrom Auto_Tabular.utils.data_utils import ohe2cat\n\n\nclass LogisticRegression(MetaModel):\n    def __init__(self):\n        super(LogisticRegression, self).__init__()\n        self._model = None\n        self.is_init = False\n\n        self.name = 'lr'\n        self.type = 'lr'\n        self.patience = 1\n\n        self.max_run = 1\n\n        self.all_data_round = 1\n\n    def init_model(self, class_num):\n        self._model = logistic.LogisticRegression(\n            C=1.0, max_iter=200, solver='liblinear', multi_class='auto')\n        self.is_init = True\n\n    def epoch_train(self, dataloader, run_num):\n        X, y, train_idxs = dataloader['X'], dataloader['y'], dataloader['train_idxs']\n        train_x, train_y = X.loc[train_idxs], y[train_idxs]\n        self._model.fit(train_x, ohe2cat(train_y))\n\n    def epoch_valid(self, dataloader):\n        X, y, val_idxs= dataloader['X'], dataloader['y'], dataloader['val_idxs']\n        val_x, val_y = X.loc[val_idxs], y[val_idxs]\n        preds = self._model.predict_proba(val_x)\n        valid_auc = roc_auc_score(val_y, preds)\n        return valid_auc\n\n    def predict(self, dataloader, batch_size=32):\n        X, test_idxs = dataloader['X'], dataloader['test_idxs']\n        test_x = X.loc[test_idxs]\n        return self._model.predict_proba(test_x)\n\n\n\n\n"""
AutoDL_sample_code_submission/Auto_Tabular/model_lib/meta_model.py,0,"b'import abc\n\nclass MetaModel:\n    def __init__(self):\n        self.run_num = 0\n        self.max_run = None\n        self.rise_num = 0\n        self.not_rise_num = 0\n        self.not_gain_num = 0\n\n        self.all_data_round = None\n\n        self.not_gain_threhlod = 1\n\n        self.auc_gain = 0\n        self.best_auc = 0\n        self.hist_auc = [0]\n\n        self.best_preds = None\n\n        self.is_init = False\n\n        self.name = None\n        self.type = None\n\n    @abc.abstractmethod\n    def init_model(self, num_class, **kwargs):\n        pass\n\n    @abc.abstractmethod\n    def preprocess_data(self, x):\n        pass\n\n    @abc.abstractmethod\n    def epch_train(self, dataloader, **kwargs):\n        pass\n\n    @abc.abstractmethod\n    def epoch_valid(self, dataloader):\n        pass\n\n    @abc.abstractmethod\n    def predict(self, dataloader):\n        pass'"
AutoDL_sample_code_submission/Auto_Tabular/model_lib/my_emb_nn.py,12,"b'import librosa\nimport numpy as np\n\n\nfrom Auto_Tabular.CONSTANT import *\nfrom Auto_Tabular.utils.data_utils import ohe2cat\nfrom .meta_model import MetaModel\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\n\n\nclass ENNModel(MetaModel):\n    def __init__(self):\n        super(ENNModel, self).__init__()\n        #clear_session()\n        self.max_length = None\n        self.mean = None\n        self.std = None\n\n        self._model = None\n        self.is_init = False\n\n        self.name = \'enn\'\n        self.type = \'emb_nn\'\n        self.patience = 50\n\n        self.max_run = 100\n\n        self.all_data_round = 80\n\n        self.not_gain_threhlod = 50\n\n        self.train_gen = None\n        self.val_gen = None\n        self.test_gen = None\n\n        self.model=None\n\n        self.num_classes = None\n\n        self.device = torch.device(\'cuda\', 0)\n\n\n    def init_model(self,\n                   num_classes,\n                   shape=None,\n                   **kwargs):\n\n        self.num_classes = num_classes\n        self.is_init = True\n\n    def epoch_train(self, dataloader, run_num, **kwargs):\n        if self.train_gen is None:\n            X, y, cats = dataloader[\'X\'], dataloader[\'y\'], dataloader[\'cat_cols\']\n            train_idxs, val_idxs, test_idxs = dataloader[\'train_idxs\'],  dataloader[\'val_idxs\'], dataloader[\'test_idxs\']\n\n            train_x, train_y = X.loc[train_idxs], ohe2cat(y[train_idxs]).reshape(len(train_idxs), 1)\n            val_x, valy = X.loc[val_idxs], ohe2cat(y[val_idxs]).reshape(len(val_idxs), 1)\n            test_x = X.loc[test_idxs]\n\n            train_x.reset_index(drop=True, inplace=True)\n            val_x.reset_index(drop=True, inplace=True)\n            test_x.reset_index(drop=True, inplace=True)\n\n            self.train_gen = DataLoader(DataGen(train_x, train_y, cats, mode=\'train\'), batch_size=32,\n                                       shuffle=True, num_workers=4)\n\n            self.val_gen = DataLoader(DataGen(val_x, None, cats, mode=\'val\'), batch_size=100,\n                                       shuffle=False, num_workers=4)\n\n            self.test_gen = DataLoader(DataGen(test_x, None, cats, mode=\'test\'), batch_size=100,\n                                       shuffle=False, num_workers=4)\n\n            emb_szs = [[X[col].nunique(), 4] for col in cats]\n            n_cont = X.shape[1] - len(cats)\n            print(\'input len\', 4*len(emb_szs)+n_cont)\n            out_sz = self.num_classes\n            layers = [500, 500]\n            self.model = TabularModel(emb_szs, n_cont, out_sz, layers).to(self.device)\n\n\n            self.criterion = nn.CrossEntropyLoss()\n            self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n\n        epochs = 10\n        for epoch in range(epochs):\n            running_loss = 0.0\n            for i, data in enumerate(self.train_gen , 0):\n                cat_feats, num_feats, labels = data[0].to(self.device), data[1].to(self.device),data[2].to(self.device)\n                self.optimizer.zero_grad()\n                preds = self.model(cat_feats, num_feats)\n\n                loss = self.criterion(preds, labels.squeeze())\n                loss.backward()\n                self.optimizer.step()\n\n                running_loss += loss.item()\n\n                if i % 100 == 99:  # print every 2000 mini-batches\n                    print(\'[%d, %5d] loss: %.3f\' %\n                          (epoch + 1, i + 1, running_loss / 100))\n                    running_loss = 0.0\n        import pdb;pdb.set_trace()\n\n\n    def epoch_valid(self, dataloader):\n        y, val_idxs = dataloader[\'y\'], dataloader[\'val_idxs\']\n        val_y = y[val_idxs]\n        with torch.no_grad():\n            predictions = []\n            for i, data in enumerate(self.val_gen, 0):\n                cat_feats, num_feats = data[0].to(self.device), data[1].to(self.device)\n                preds = self.model(cat_feats, num_feats)\n                preds = preds.cpu().numpy()\n                predictions.append(preds)\n        preds = np.concatenate(predictions, axis=0)\n        valid_auc = roc_auc_score(val_y, preds)\n        return valid_auc\n\n    def predict(self, dataloader, batch_size=32):\n        with torch.no_grad():\n            predictions = []\n            for i, data in enumerate(self.test_gen, 0):\n                cat_feats, num_feats = data[0].to(self.device), data[1].to(self.device)\n                preds = self.model(cat_feats, num_feats)\n                preds = preds.cpu().numpy()\n                predictions.append(preds)\n        preds = np.concatenate(predictions, axis=0)\n        return preds\n\n\ndef auroc_score(input, target):\n    input, target = input.cpu().numpy()[:, 1], target.cpu().numpy()\n    return roc_auc_score(target, input)\n\n\nclass DataGen(Dataset):\n\n    def __init__(self, X, y, cats, mode=\'train\'):\n        self.X = X\n        self.y = y\n\n        self.mode = mode\n\n        self.cats = cats\n        self.nums = [col for col in self.X.columns if col not in self.cats]\n\n    def __len__(self):\n        return self.X.shape[0]\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        cat_feats = self.X.loc[idx, self.cats].values\n        num_feats = self.X.loc[idx, self.nums].values\n\n        if self.mode == \'train\':\n            label = self.y[idx]\n            return torch.from_numpy(cat_feats).long(),  torch.from_numpy(num_feats), torch.from_numpy(label)\n        else:\n            return torch.from_numpy(cat_feats).long(),  torch.from_numpy(num_feats)\n\n\nclass TabularModel(nn.Module):\n    ""Basic model for tabular data.""\n    def __init__(self, emb_szs, n_cont, out_sz, layers,\n                 emb_drop=0.2, use_bn=True, bn_final=False):\n        super(TabularModel, self).__init__()\n\n        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in emb_szs])\n        self.emb_drop = nn.Dropout(emb_drop)\n        self.bn_cont = nn.BatchNorm1d(n_cont)\n        n_emb = sum(e.embedding_dim for e in self.embeds)\n        self.n_emb, self.n_cont, = n_emb, n_cont\n\n        ps = [0.2]*len(layers)\n\n        sizes = self.get_sizes(layers, out_sz)\n        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]\n        layers = []\n        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\n            layers += self.bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n        if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n        self.layers = nn.Sequential(*layers)\n\n    def get_sizes(self, layers, out_sz):\n        return [self.n_emb + self.n_cont] + layers + [out_sz]\n\n    def forward(self, x_cat, x_cont):\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n            x = torch.cat(x, 1)\n            x = self.emb_drop(x)\n        if self.n_cont != 0:\n            x_cont = self.bn_cont(x_cont)\n            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n\n        x = self.layers(x)\n\n        x = torch.sigmoid(x)\n\n        return x\n\n    def bn_drop_lin(self, n_in: int, n_out: int, bn: bool = True, p: float = 0., actn=None):\n        ""Sequence of batchnorm (if `bn`), dropout (with `p`) and linear (`n_in`,`n_out`) layers followed by `actn`.""\n        layers = [nn.BatchNorm1d(n_in)] if bn else []\n        if p != 0: layers.append(nn.Dropout(p))\n        layers.append(nn.Linear(n_in, n_out))\n        if actn is not None: layers.append(actn)\n        return layers\n\n\n'"
AutoDL_sample_code_submission/Auto_Tabular/model_lib/xgb.py,0,"b'import xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\nimport hyperopt\nfrom hyperopt import STATUS_OK, Trials, hp, space_eval, tpe\nimport lightgbm as lgb\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\n\nfrom Auto_Tabular.utils.log_utils import log, timeit\nfrom Auto_Tabular import CONSTANT\nfrom Auto_Tabular.utils.data_utils import ohe2cat\nfrom .meta_model import MetaModel\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n\nclass XGBModel(MetaModel):\n\n    def __init__(self):\n        super(XGBModel, self).__init__()\n        self.max_run = 2\n        self.all_data_round = 1\n        self.explore_params_round = 0\n\n        self.not_gain_threhlod = 3\n\n        self.patience = 3\n\n        self.is_init = False\n\n        self.name = \'xgb\'\n        self.type = \'tree\'\n\n        self._model = None\n\n        self.params = {\n            ""boosting_type"": ""gbdt"",\n            ""objective"": ""multi:softprob"",\n            ""nthread"": CONSTANT.JOBS,\n            ""tree_method"": ""hist"",\n            ""eval_metric"": ""mlogloss"",\n            ""seed"": CONSTANT.SEED,\n        }\n\n        self.hyperparams = {\n            ""learning_rate"": 0.02,\n            ""max_depth"": 6,\n            ""min_child_weight"": 0.01,\n            ""min_data_in_leaf"": 100,\n            ""gamma"": 0.1,\n            ""lambda"": 0.1,\n            ""alpha"": 0.1}\n\n        self.is_multi_label = None\n\n        self.num_class = None\n\n        self.models = {}\n\n        self.import_cols = None\n\n    def init_model(self, num_class, **kwargs):\n        self.is_init = True\n        self.params.update({\'num_class\': num_class})\n        self.num_class = num_class\n\n    #@timeit\n    def epoch_train(self, dataloader, run_num, is_multi_label=None, info=None, time_remain=None):\n        self.is_multi_label = is_multi_label\n        X, y, train_idxs, cat = dataloader[\'X\'], dataloader[\'y\'], dataloader[\'train_idxs\'], dataloader[\'cat_cols\']\n        train_x, train_y = X.loc[train_idxs], y[train_idxs]\n\n        if info[\'mode\'] == \'bagging\':\n            self.hyperparams = info[\'xgb\'].copy()\n            self.hyperparams[\'random_seed\'] = np.random.randint(0, 2020)\n            run_num = self.explore_params_round\n\n        if run_num == self.explore_params_round:\n            print(\'xgb explore_params_round\')\n            train_x, train_y, val_x, val_y, = self.split_data(train_x, train_y)\n\n            self.import_cols = info[\'imp_cols\']\n\n            if train_x.shape[1] > 300 and train_x.shape[0] > 10000:\n                train_x = train_x[self.import_cols[:300]]\n                val_x = val_x[self.import_cols[:300]]\n                train_x.reset_index(drop=True, inplace=True)\n                train_x = train_x.sample(n=10000)\n                train_y = train_y[list(train_x.index)]\n\n            elif train_x.shape[0] > 10000:\n                train_x.reset_index(drop=True, inplace=True)\n                train_x = train_x.sample(n=10000)\n                train_y = train_y[list(train_x.index)]\n\n            elif train_x.shape[1] > 300:\n                train_x = train_x[self.import_cols[:300]]\n                val_x = val_x[self.import_cols[:300]]\n\n            self.bayes_opt(train_x, val_x, train_y, val_y, cat)\n            self.early_stop_opt(train_x, val_x, train_y, val_y, cat)\n            info[\'xgb\'] = self.hyperparams.copy()\n\n        train_x, train_y = X.loc[train_idxs], y[train_idxs]\n        if run_num == self.all_data_round:\n            all_train_idxs = dataloader[\'all_train_idxs\']\n            train_x = X.loc[all_train_idxs]\n            train_y = y[all_train_idxs]\n        if not self.is_multi_label:\n            xgb_train = xgb.DMatrix(train_x, ohe2cat(train_y))\n            self._model = xgb.train({**self.params, **self.hyperparams}, xgb_train)\n        else:\n            for cls in range(self.num_class):\n                cls_y = train_y[:, cls]\n                xgb_train = xgb.DMatrix(train_x, cls_y)\n                self.models[cls] = self._model = xgb.train({**self.params, **self.hyperparams}, xgb_train)\n\n\n    #@timeit\n    def epoch_valid(self, dataloader):\n        X, y, val_idxs= dataloader[\'X\'], dataloader[\'y\'], dataloader[\'val_idxs\']\n        val_x, val_y = X.loc[val_idxs], y[val_idxs]\n        val_x = xgb.DMatrix(val_x)\n        if not self.is_multi_label:\n            preds = self._model.predict(val_x)\n        else:\n            all_preds = []\n            for cls in range(y.shape[1]):\n                preds = self.models[cls].predict(val_x)\n                all_preds.append(preds[:,1])\n            preds = np.stack(all_preds, axis=1)\n        valid_auc = roc_auc_score(val_y, preds)\n        return valid_auc\n\n    #@timeit\n    def predict(self, dataloader):\n        X, test_idxs = dataloader[\'X\'], dataloader[\'test_idxs\']\n        test_x = X.loc[test_idxs]\n        test_x = xgb.DMatrix(test_x)\n        if not self.is_multi_label:\n            return self._model.predict(test_x)\n        else:\n            all_preds = []\n            for cls in range(self.num_class):\n                preds = self.models[cls].predict(test_x)\n                all_preds.append(preds[:, 1])\n            return np.stack(all_preds, axis=1)\n\n    #@timeit\n    def bayes_opt(self, X_train, X_eval, y_train, y_eval, categories):\n        if self.is_multi_label:\n            dtrain = xgb.DMatrix(X_train, y_train[:, 1])\n            dvalid = xgb.DMatrix(X_eval, y_eval[:, 1])\n        else:\n            dtrain = xgb.DMatrix(X_train, ohe2cat(y_train))\n            dvalid = xgb.DMatrix(X_eval, ohe2cat(y_eval))\n        space = {\n            ""learning_rate"": hp.loguniform(""learning_rate"", np.log(0.01), np.log(0.1)),\n            ""max_depth"": hp.choice(""max_depth"", [4, 6, 8, 10, 12]),\n            ""min_child_weight"": hp.uniform(\'min_child_weight\', 0.01, 1),\n            ""min_data_in_leaf"": hp.choice(""min_data_in_leaf"", np.linspace(10, 100, 20, dtype=int)),\n            ""gamma"": hp.uniform(""gamma"", 0.001, 0.1),\n            ""lambda"": hp.uniform(""lambda"", 0, 1),\n            ""alpha"": hp.uniform(""alpha"", 0, 1),\n            ""colsample_bytree"": hp.choice(""colsample_bytree"", [0.7, 0.9]),\n            ""colsample_bylevel"": hp.choice(""colsample_bylevel"", [0.7, 0.9]),\n            ""colsample_bynode"": hp.choice(""colsample_bynode"", [0.7, 0.9]),\n\n        }\n\n        def objective(hyperparams):\n            model = xgb.train({**self.params, **hyperparams}, dtrain,  num_boost_round=50)\n\n            pred = model.predict(dvalid)\n            if self.is_multi_label:\n                score = roc_auc_score(y_eval[:, 1], pred[:, 1])\n            else:\n                score = roc_auc_score(y_eval, pred)\n\n            return {\'loss\': -score, \'status\': STATUS_OK}\n\n        trials = Trials()\n        best = hyperopt.fmin(fn=objective, space=space, trials=trials,\n                             algo=tpe.suggest, max_evals=10, verbose=1,\n                             rstate=np.random.RandomState(1))\n\n        self.hyperparams.update(space_eval(space, best))\n\n    def early_stop_opt(self, X_train, X_eval, y_train, y_eval, categories):\n        if self.is_multi_label:\n            dtrain = xgb.DMatrix(X_train, y_train[:, 1])\n            dvalid = xgb.DMatrix(X_eval, y_eval[:, 1])\n        else:\n            dtrain = xgb.DMatrix(X_train, ohe2cat(y_train))\n            dvalid = xgb.DMatrix(X_eval, ohe2cat(y_eval))\n\n        model = xgb.train({**self.params, **self.hyperparams}, dtrain, evals=[(dvalid, \'eval\')], num_boost_round=1200,\n                          early_stopping_rounds=10) #categorical_feature=categories)\n\n\n        self.params[\'num_boost_round\'] = model.best_iteration\n\n    def split_data(self, x, y):\n        new_x = x.copy()\n        new_x.reset_index(drop=True, inplace=True)\n        sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n        self.splits = {}\n        i = 0\n        for train_idxs, val_idxs in sss.split(new_x, y):\n            self.splits[i] = [train_idxs, val_idxs]\n            i += 1\n        new_train_x = new_x.loc[self.splits[0][0]]\n        new_train_y = y[self.splits[0][0]]\n\n        new_val_x = new_x.loc[self.splits[0][1]]\n        new_val_y = y[self.splits[0][1]]\n\n        return new_train_x, new_train_y, new_val_x, new_val_y\n\n\n'"
AutoDL_sample_code_submission/Auto_Tabular/utils/__init__.py,0,b''
AutoDL_sample_code_submission/Auto_Tabular/utils/data_utils.py,0,"b""import numpy as np\nimport pandas as pd\nfrom Auto_Tabular import CONSTANT\n\n\ndef ohe2cat(label):\n    return np.argmax(label, axis=1)\n\n\ndef downcast(series, accuracy_loss=True, min_float_type='float16'):\n    if series.dtype == np.int64:\n        ii8 = np.iinfo(np.int8)\n        ii16 = np.iinfo(np.int16)\n        ii32 = np.iinfo(np.int32)\n        max_value = series.max()\n        min_value = series.min()\n        \n        if max_value <= ii8.max and min_value >= ii8.min:\n            return series.astype(np.int8)\n        elif max_value <= ii16.max and min_value >= ii16.min:\n            return series.astype(np.int16)\n        elif max_value <= ii32.max and min_value >= ii32.min:\n            return series.astype(np.int32)\n        else:\n            return series\n        \n    elif series.dtype == np.float64:\n        fi16 = np.finfo(np.float16)\n        fi32 = np.finfo(np.float32)\n        \n        if accuracy_loss:\n            max_value = series.max()\n            min_value = series.min()\n            if np.isnan(max_value):\n                max_value = 0\n            \n            if np.isnan(min_value):\n                min_value = 0\n                \n            if min_float_type=='float16' and max_value <= fi16.max and min_value >= fi16.min:\n                return series.astype(np.float16)\n            elif max_value <= fi32.max and min_value >= fi32.min:\n                return series.astype(np.float32)\n            else:\n                return series\n        else:\n            tmp = series[~pd.isna(series)]\n            if(len(tmp)==0):\n                return series.astype(np.float16)\n            \n            if (tmp == tmp.astype(np.float16)).sum() == len(tmp):\n                return series.astype(np.float16)\n            elif (tmp == tmp.astype(np.float32)).sum() == len(tmp):\n                return series.astype(np.float32)\n           \n            else:\n                return series\n            \n    else:\n        return series\n\n\ndef check_density(sparse_matrix, num_feat=100):\n    density = np.array(sparse_matrix.sum(axis=0)/sparse_matrix.shape[0])[0]\n    sort_index = sorted(range(len(density)), key=lambda k: density[k], reverse=True)\n    return sort_index[:min(num_feat,len(density))]\n\n\ndef fill_na(df):\n    columns = df.columns\n    for col in columns:\n        if col.startswith(CONSTANT.CATEGORY_PREFIX):\n            df[col].fillna(df[col].max()+1, inplace=True)\n        elif col.startswith(CONSTANT.NUMERICAL_PREFIX):\n            df[col].fillna(df[col].astype(float).mean(), inplace=True)\n    return df\n\n\n\n"""
AutoDL_sample_code_submission/Auto_Tabular/utils/eda.py,0,"b'# coding:utf-8\n\n\n""""""\n    \xe6\x95\xb0\xe6\x8d\xaeEDA\xef\xbc\x8c\xe6\x9a\x82\xe6\x97\xb6\xe6\x9c\x8d\xe5\x8a\xa1\xe4\xba\x8enlp\xe5\x92\x8cspeech,\xe4\xb8\x8d\xe6\x9c\x8d\xe5\x8a\xa1cv\xe7\x9a\x84tf.Dataset.\n\n""""""\n\nimport os\nimport numpy as np\nimport sys\n\n\nfrom .log_utils import info, debug, timeit\nfrom .data_utils import ohe2cat\nfrom CONSTANT import *\n\n\nclass AutoEDA(object):\n\n\n    def get_info(self, df):\n        eda_info = {}\n        eda_info[\'cat_cols\'], eda_info[\'num_cols\'] = self.recognize_col2type(df)\n        return eda_info\n\n    @timeit\n    def recognize_col2type(self, df):\n        m, n = df.shape\n        cat_cols = []\n        num_cols = []\n        if n > 1000:\n            num_cols = [\'n_{}\'.format(col) for col in df.columns]\n            df.columns = num_cols\n        else:\n            for col in df.columns:\n                nunique = df[col].nunique()\n                min_v = df[col].min()\n                if nunique == 1:\n                    df.drop(col, axis=1, inplace=True)\n                else:\n                    if nunique < 30 and (min_v == 0 or min_v == 1):\n                        col_name = \'c_{}\'.format(col)\n                        cat_cols.append(col_name)\n                        df.rename(columns={col: col_name}, inplace=True)\n                    else:\n                        col_name = \'n_{}\'.format(col)\n                        num_cols.append(col_name)\n                        df.rename(columns={col: col_name}, inplace=True)\n        info(\'cat_cols: {} num_cols: {}\'.format(cat_cols, num_cols))\n        return cat_cols, num_cols\n\n\n    def get_label_distribution(self, y_onehot, verbose=True):\n        """"""\n        \xe8\x8e\xb7\xe5\x8f\x96\xe5\xb9\xb6\xe6\x89\x93\xe5\x8d\xb0y\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\xe5\x88\x86\xe5\xb8\x83\n        :param y_onehot: \xe7\xb1\xbb\xe5\x9e\x8b\xe4\xb8\xba ndarray, shape = (y_sample_num, y_label_num)\n        :param verbose: \xe6\x98\xaf\xe5\x90\xa6\xe6\x89\x93\xe5\x8d\xb0\xe4\xbf\xa1\xe6\x81\xaf\n        :return: ndarray\n        """"""\n\n        y_sample_num, y_label_num = y_onehot.shape\n\n        y_distribution_array = y_onehot.sum(axis=0)/y_sample_num\n\n        return y_distribution_array\n\n\n\n\n'"
AutoDL_sample_code_submission/Auto_Tabular/utils/log_utils.py,0,"b'import logging\nimport os\nimport sys\n\nimport time\nfrom typing import Any\nimport multiprocessing\n\nimport functools\nnesting_level = 0\nis_start = None\nNCPU = multiprocessing.cpu_count()\n\n\ndef log(entry: Any):\n    global nesting_level\n    space = ""-"" * (4 * nesting_level)\n    logger.info(""{}{}"".format(space, entry))\n\n\ndef get_logger(verbosity_level, use_error_log=False, log_path=None):\n    """"""Set logging format to something like:\n         2019-04-25 12:52:51,924 INFO score.py: <message>\n    """"""\n    logger = logging.getLogger(__file__)\n    logging_level = getattr(logging, verbosity_level)\n    logger.setLevel(logging_level)\n\n    if log_path is None:\n        log_dir = os.path.join("".."", ""log"")\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n        log_path = os.path.join(log_dir, ""log.txt"")\n    else:\n        log_path = os.path.join(log_path, ""log.txt"")\n\n    formatter = logging.Formatter(\n        fmt=\'%(asctime)s %(levelname)s %(filename)s: %(funcName)s: %(lineno)d: %(message)s\')\n\n    stdout_handler = logging.StreamHandler(sys.stdout)\n    stdout_handler.setLevel(logging_level)\n    stdout_handler.setFormatter(formatter)\n\n    logger.addHandler(stdout_handler)\n\n    fh = logging.FileHandler(log_path)\n    fh.setLevel(logging_level)\n    fh.setFormatter(formatter)\n\n    logger.addHandler(fh)\n\n    if use_error_log:\n        stderr_handler = logging.StreamHandler(sys.stderr)\n        stderr_handler.setLevel(logging.WARNING)\n        stderr_handler.setFormatter(formatter)\n        logger.addHandler(stderr_handler)\n    logger.propagate = False\n    return logger\n\n\nlogger = get_logger(\'INFO\')\n\ndebug = logger.debug\ninfo = logger.info\nwarning = logger.warning\nerror = logger.error\n\n\ndef timeit(method, start_log=None):\n    @functools.wraps(method)\n    def timed(*args, **kw):\n        global is_start\n        global nesting_level\n\n        if not is_start:\n            print()\n\n        is_start = True\n        log(""Start [{}]:"".format(method.__name__)+ (start_log if start_log else """"))\n        nesting_level += 1\n\n        start_time = time.time()\n        result = method(*args, **kw)\n        end_time = time.time()\n\n        nesting_level -= 1\n        log(""End   [{}]. Time elapsed: {} sec."".format(method.__name__, end_time - start_time))\n        is_start = False\n\n        return result\n\n    return timed'"
AutoDL_sample_code_submission/Auto_Tabular/utils/sample.py,0,"b'import numpy as np\nimport random\nimport os\nfrom .eda import AutoEDA\nfrom .data_utils import ohe2cat\nfrom .log_utils import info, debug\n\nclass AutoSample:\n    def __init__(self, y_onehot):\n        self.auto_eda = AutoEDA()\n        self.y_onehot = y_onehot\n        self.sample_num, self.class_num = y_onehot.shape\n        self.y_distribution = self.auto_eda.get_label_distribution(y_onehot)\n        self.class_idx = self.get_idx_by_class()\n\n    def get_idx_by_class(self):\n        """"""\xe8\x8e\xb7\xe5\x8f\x96\xe6\xaf\x8f\xe4\xb8\xaa\xe7\xb1\xbb\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95""""""\n        class_idx = {}\n        idx = np.arange(self.sample_num)\n        for i in range(self.class_num):\n            idx_list = idx[self.y_onehot[:,i] == 1]\n            class_idx[i] = list(idx_list)\n        return class_idx\n\n    def sample_fix_size_by_class(self, per_class_num, max_sample_num=1000, min_sample_num=100):\n        """"""\n        \xe5\xaf\xb9\xe6\xaf\x8f\xe4\xb8\xaa\xe7\xb1\xbb\xe9\x87\x87\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xe9\x87\x8f\n        :param per_class_num: \xe6\xaf\x8f\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe9\x87\x87\xe6\xa0\xb7\xe6\x95\xb0\n        :param max_sample_num: \xe6\xaf\x8f\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe9\x87\x87\xe6\xa0\xb7\xe6\x95\xb0\n        :param min_sample_num: \xe6\xaf\x8f\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe6\x9c\x80\xe5\xb0\x8f\xe9\x87\x87\xe6\xa0\xb7\xe6\x95\xb0\n        :return:\n        """"""\n\n        # \xe8\xa6\x81\xe9\x87\x87\xe6\xa0\xb7\xe7\x9a\x84\xe6\x80\xbb\xe4\xbd\x93 sample_index_id_list.\n        final_sample_idx = list()\n        min_sample_perclass = int(min_sample_num / self.class_num) + 1\n\n        for label_id in range(self.class_num):\n            random_sample_k = per_class_num\n            # \xe5\xa6\x82\xe6\x9e\x9c k \xe5\xbe\x88\xe5\xb0\x8f\xef\xbc\x8c\xe5\xb0\xb1\xe7\x94\xa8 min_sample_perclass.\n            random_sample_k = max(min_sample_perclass, random_sample_k)\n            label_idx_list = self.class_idx[label_id]\n            label_idx_len = len(label_idx_list)\n\n            # \xe5\xa6\x82\xe6\x9e\x9c\xe5\x8d\x95\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0\xe9\x87\x8f\xe8\xb6\xb3\xe5\xa4\x9f\xef\xbc\x8c\xe4\xb8\x8d\xe8\xb6\xb3\xe4\xb8\xa4\xe7\xa7\x8d\xe6\x83\x85\xe5\x86\xb5, \xe9\x81\xbf\xe5\x85\x8d\xe6\xba\xa2\xe5\x87\xba.\n            if label_idx_len > random_sample_k:\n                downsample_class_idx = random.sample(population=label_idx_list, k=random_sample_k)\n                final_sample_idx.extend(downsample_class_idx)\n            else:\n                div, mod = divmod(random_sample_k, label_idx_len)\n                new_label_idx_list = label_idx_list*div + label_idx_list[:int(mod*label_idx_len)]\n                final_sample_idx.extend(new_label_idx_list)\n\n        # \xe8\xa6\x81\xe6\xbb\xa1\xe8\xb6\xb3 max_sample_num/min_sample_num. \xe5\xa6\x82\xe6\x9e\x9c\xe5\xa4\xaa\xe5\xa4\x9a\xef\xbc\x8c\xe5\x88\x99\xe8\xa6\x81\xe9\x87\x8d\xe6\x96\xb0\xe9\x99\x8d\xe4\xbd\x8e \xe5\x88\xb0\xe4\xb8\x8a\xe9\x99\x90.\n        if len(final_sample_idx) > max_sample_num:\n            final_sample_idx = random.sample(population=final_sample_idx, k=max_sample_num)\n        # \xe8\xbf\x94\xe5\x9b\x9e\xe8\xaf\x95\xe9\xaa\x8cSample\xe7\x9a\x84id_list.\n        info(\'final sample length: {}\'.format(len(final_sample_idx)))\n        random.shuffle(final_sample_idx)\n        return final_sample_idx\n\n\n\n\n'"
AutoDL_sample_code_submission/Auto_Video/architectures/__init__.py,0,b''
AutoDL_sample_code_submission/Auto_Video/architectures/mc3.py,15,"b""## \nimport logging\nimport sys\nfrom collections import OrderedDict\n\nimport torch\nimport torchvision.models.video.resnet as models\nfrom torch.utils import model_zoo\nfrom torchvision.models.video.resnet import model_urls\nfrom itertools import chain\n\nimport skeleton\nimport torch.nn as nn\nimport math\n\nformatter = logging.Formatter(fmt='[%(asctime)s %(levelname)s %(filename)s] %(message)s')\n\nhandler = logging.StreamHandler(sys.stdout)\nhandler.setFormatter(formatter)\n\nLOGGER = logging.getLogger(__name__)\nLOGGER.setLevel(logging.INFO)\nLOGGER.addHandler(handler)\n\n\nclass Conv3DSimple(nn.Conv3d):\n    def __init__(self,\n                 in_planes,\n                 out_planes,\n                 midplanes=None,\n                 stride=1,\n                 padding=1):\n\n        super(Conv3DSimple, self).__init__(\n            in_channels=in_planes,\n            out_channels=out_planes,\n            kernel_size=(3, 3, 3),\n            stride=stride,\n            padding=padding,\n            bias=False)\n\n    @staticmethod\n    def get_downsample_stride(stride):\n        return (stride, stride, stride)\n\n\nclass Conv2Plus1D(nn.Sequential):\n\n    def __init__(self,\n                 in_planes,\n                 out_planes,\n                 midplanes,\n                 stride=1,\n                 padding=1):\n        super(Conv2Plus1D, self).__init__(\n            nn.Conv3d(in_planes, midplanes, kernel_size=(1, 3, 3),\n                      stride=(1, stride, stride), padding=(0, padding, padding),\n                      bias=False),\n            nn.BatchNorm3d(midplanes),\n            nn.ReLU(inplace=False),\n            nn.Conv3d(midplanes, out_planes, kernel_size=(3, 1, 1),\n                      stride=(stride, 1, 1), padding=(padding, 0, 0),\n                      bias=False))\n\n    @staticmethod\n    def get_downsample_stride(stride):\n        return (stride, stride, stride)\n\n\nclass Conv3DNoTemporal(nn.Conv3d):\n\n    def __init__(self,\n                 in_planes,\n                 out_planes,\n                 midplanes=None,\n                 stride=1,\n                 padding=1):\n\n        super(Conv3DNoTemporal, self).__init__(\n            in_channels=in_planes,\n            out_channels=out_planes,\n            kernel_size=(1, 3, 3),\n            stride=(1, stride, stride),\n            padding=(0, padding, padding),\n            bias=False)\n\n    @staticmethod\n    def get_downsample_stride(stride):\n        return (1, stride, stride)\n\n\nclass BasicBlock(nn.Module):\n\n    expansion = 1\n\n    def __init__(self, inplanes, planes, conv_builder, stride=1, downsample=None):\n        midplanes = (inplanes * planes * 3 * 3 * 3) // (inplanes * 3 * 3 + 3 * planes)\n\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Sequential(\n            conv_builder(inplanes, planes, midplanes, stride),\n            nn.BatchNorm3d(planes),\n            nn.ReLU(inplace=False)\n        )\n        self.conv2 = nn.Sequential(\n            conv_builder(planes, planes, midplanes),\n            nn.BatchNorm3d(planes)\n        )\n        self.relu = nn.ReLU(inplace=False)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.conv2(out)\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, conv_builder, stride=1, downsample=None):\n\n        super(Bottleneck, self).__init__()\n        midplanes = (inplanes * planes * 3 * 3 * 3) // (inplanes * 3 * 3 + 3 * planes)\n\n        self.conv1 = nn.Sequential(\n            nn.Conv3d(inplanes, planes, kernel_size=1, bias=False),\n            nn.BatchNorm3d(planes),\n            nn.ReLU(inplace=False)\n        )\n        self.conv2 = nn.Sequential(\n            conv_builder(planes, planes, midplanes, stride),\n            nn.BatchNorm3d(planes),\n            nn.ReLU(inplace=False)\n        )\n\n        self.conv3 = nn.Sequential(\n            nn.Conv3d(planes, planes * self.expansion, kernel_size=1, bias=False),\n            nn.BatchNorm3d(planes * self.expansion)\n        )\n        self.relu = nn.ReLU(inplace=False)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.conv2(out)\n        out = self.conv3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass BasicStem(nn.Sequential):\n    def __init__(self):\n        super(BasicStem, self).__init__(\n            nn.Conv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2),\n                      padding=(1, 3, 3), bias=False),\n            nn.BatchNorm3d(64),\n            nn.ReLU(inplace=False))\n\n\nclass R2Plus1dStem(nn.Sequential):\n\n    def __init__(self):\n        super(R2Plus1dStem, self).__init__(\n            nn.Conv3d(3, 45, kernel_size=(1, 7, 7),\n                      stride=(1, 2, 2), padding=(0, 3, 3),\n                      bias=False),\n            nn.BatchNorm3d(45),\n            nn.ReLU(inplace=False),\n            nn.Conv3d(45, 64, kernel_size=(3, 1, 1),\n                      stride=(1, 1, 1), padding=(1, 0, 0),\n                      bias=False),\n            nn.BatchNorm3d(64),\n            nn.ReLU(inplace=False))\n\n\nclass VideoResNet(nn.Module):\n\n    def __init__(self, block, conv_makers, layers,\n                 stem, num_classes=400,\n                 zero_init_residual=False):\n        super(VideoResNet, self).__init__()\n        self.inplanes = 64\n\n        self.stem = stem()\n\n        self.layer1 = self._make_layer(block, conv_makers[0], 64, layers[0], stride=1)\n        self.layer2 = self._make_layer(block, conv_makers[1], 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, conv_makers[2], 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, conv_makers[3], 512, layers[3], stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        self._initialize_weights()\n\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n\n    def forward(self, x):\n        x = self.stem(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.flatten(1)\n        x = self.fc(x)\n\n        return x\n\n    def _make_layer(self, block, conv_builder, planes, blocks, stride=1):\n        downsample = None\n\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            ds_stride = conv_builder.get_downsample_stride(stride)\n            downsample = nn.Sequential(\n                nn.Conv3d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=ds_stride, bias=False),\n                nn.BatchNorm3d(planes * block.expansion)\n            )\n        layers = []\n        layers.append(block(self.inplanes, planes, conv_builder, stride, downsample))\n\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, conv_builder))\n\n        return nn.Sequential(*layers)\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out',\n                                        nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm3d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\nclass ResNet(VideoResNet):\n    def __init__(self, in_channels, num_classes=10, **kwargs):\n        Block = BasicBlock\n        super(ResNet, self).__init__(Block, num_classes=num_classes,\n                                     conv_makers=[Conv3DSimple] + [Conv3DNoTemporal] * 3, layers=[2, 2, 2, 2],\n                                     stem=BasicStem, **kwargs)\n\n        self._class_normalize = True\n        self._is_video = True\n        self._half = False\n        self.init_hyper_params()\n        self.checkpoints = []\n        self.predict_prob_list =dict()\n        self.round_idx = 0\n        self.single_ensemble = False\n        self.use_test_time_augmentation = False\n        self.update_transforms = False\n        self.history_predictions = dict()\n        self.g_his_eval_dict = dict()\n        self.last_y_pred_round = 0\n        self.ensemble_scores =dict()\n        self.ensemble_predictions = dict()\n        self.ensemble_test_index = 0\n\n        if in_channels == 3:\n            self.preprocess = torch.nn.Sequential(\n                skeleton.nn.Normalize([0.43216, 0.394666, 0.37645], [0.22803, 0.22145, 0.216989], mode='conv3d',inplace=False),\n            )\n        elif in_channels == 1:\n            self.preprocess = torch.nn.Sequential(\n                skeleton.nn.Normalize(0.5, 0.25, mode='conv3d',inplace=False),\n                skeleton.nn.CopyChannels(3),\n            )\n        else:\n            self.preprocess = torch.nn.Sequential(\n                skeleton.nn.Normalize(0.5, 0.25,mode='conv3d', inplace=False),\n                torch.nn.Conv2d(in_channels, 3, kernel_size=3, stride=1, padding=1, bias=False),\n                torch.nn.BatchNorm2d(3),\n            )\n\n        self.last_channels = 512 * Block.expansion\n        self.fc = torch.nn.Linear(self.last_channels, num_classes, bias=False)\n\n    def init_hyper_params(self):\n        self.info = {\n            'loop': {\n                'epoch': 0,\n                'test': 0,\n                'best_score': 0.0\n            },\n            'condition': {\n                'first': {\n                    'train': True,\n                    'valid': True,\n                    'test': True\n                }\n            },\n            'terminate': False\n        }\n\n        self.hyper_params = {\n            'optimizer': {\n                'lr': 0.15,\n                'warmup_multiplier':2.0,\n                'warmup_epoch':3\n            },\n            'dataset': {\n                'train_info_sample': 256,\n                'cv_valid_ratio': 0.1,\n                'max_valid_count': 256,\n\n                'max_size': 64,\n                'base': 16,  #\n                'max_times': 3,\n\n                'enough_count': {\n                    'image': 10000,\n                    'video': 1000\n                },\n\n                'batch_size': 32,\n                'steps_per_epoch': 30,\n                'max_epoch': 1000,  #\n                'batch_size_test': 256,\n            },\n            'checkpoints': {\n                'keep': 30\n            },\n            'conditions': {\n                'score_type': 'auc',\n                'early_epoch': 1,\n                'skip_valid_score_threshold': 0.90,  #\n                'test_after_at_least_seconds': 1,\n                'test_after_at_least_seconds_max': 90,\n                'test_after_at_least_seconds_step': 2,\n\n                'threshold_valid_score_diff': 0.001,\n                'threshold_valid_best_score': 0.997,\n                'decide_threshold_valid_best_score': 0.9300,\n                'max_inner_loop_ratio': 0.1,\n                'min_lr': 1e-6,\n                'use_fast_auto_aug': True\n            }\n        }\n\n    def init(self, model_dir=None, gain=1.):\n        self.model_dir = model_dir if model_dir is not None else self.model_dir\n        sd = model_zoo.load_url(model_urls['mc3_18'], model_dir=self.model_dir)\n        del sd['fc.weight']\n        del sd['fc.bias']\n        for m in self.layer1.modules():\n            for p in m.parameters():\n                p.requires_grad_(False)\n        for m in self.stem.modules():\n            for p in m.parameters():\n                p.requires_grad_(False)\n        self.load_state_dict(sd, strict=False)\n        torch.nn.init.xavier_uniform_(self.fc.weight, gain=gain)\n\n    def init_opt(self,steps_per_epoch,batch_size,init_lr,warmup_multiplier,warm_up_epoch):\n        lr_multiplier = max(0.5, batch_size / 32)\n        \n        params = [p for p in self.parameters() if p.requires_grad]\n        params_fc = [p for n, p in self.named_parameters() if\n                     p.requires_grad and 'fc' == n[:2] or 'conv1d' == n[:6]]\n        \n        scheduler_lr = skeleton.optim.get_change_scale(\n            skeleton.optim.gradual_warm_up(\n                skeleton.optim.get_reduce_on_plateau_scheduler(\n                    init_lr * lr_multiplier / warmup_multiplier,\n                    patience=10, factor=.5, metric_name='train_loss'\n                ),\n                warm_up_epoch=warm_up_epoch,\n                multiplier=warmup_multiplier\n            ),\n            init_scale=1.0\n        )\n\n        self.optimizer_fc = skeleton.optim.ScheduledOptimizer(\n            params_fc,\n            torch.optim.SGD,\n            steps_per_epoch=steps_per_epoch,\n            clip_grad_max_norm=None,\n            lr=scheduler_lr,\n            momentum=0.9,\n            weight_decay=0.00025,\n            nesterov=True\n        )\n        self.optimizer = skeleton.optim.ScheduledOptimizer(\n            params,\n            torch.optim.SGD,\n            steps_per_epoch=steps_per_epoch,\n            clip_grad_max_norm=None,\n            lr=scheduler_lr,\n            momentum=0.9,\n            weight_decay=0.00025,\n            nesterov=True\n        )\n\n    def set_video(self, is_video=True, times=False):\n        self._is_video = is_video\n        if is_video:\n            self.conv1d_prev = torch.nn.Sequential(\n                skeleton.nn.SplitTime(times),\n                skeleton.nn.Permute(0, 2, 1, 3, 4),  #\n            )\n\n            self.conv1d_post = torch.nn.Sequential(\n            )\n\n    def forward_origin(self, x):\n        x = self.preprocess(x)\n\n        x = self.stem(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = x.flatten(1)\n        x = self.fc(x)\n\n        return x\n\n    def forward(self, inputs, targets=None, tau=8.0, reduction='avg'):\n        dims = len(inputs.shape)\n        logits = self.forward_origin(inputs)\n        logits /= tau\n\n        if targets is None:\n            return logits\n        if targets.device != logits.device:\n            targets = targets.to(device=logits.device)\n\n        loss = self.loss_fn(input=logits, target=targets)\n\n        if self._class_normalize and isinstance(self.loss_fn, (\n                torch.nn.BCEWithLogitsLoss, skeleton.nn.BinaryCrossEntropyLabelSmooth)):\n            pos = (targets == 1).to(logits.dtype)\n            neg = (targets < 1).to(logits.dtype)\n            npos = pos.sum()\n            nneg = neg.sum()\n\n            positive_ratio = max(0.1, min(0.9, (npos) / (npos + nneg)))\n            negative_ratio = max(0.1, min(0.9, (nneg) / (npos + nneg)))\n\n            normalized_loss = (loss * pos) / positive_ratio\n            normalized_loss += (loss * neg) / negative_ratio\n\n            loss = normalized_loss\n\n        if reduction == 'avg':\n            loss = loss.mean()\n        elif reduction == 'max':\n            loss = loss.max()\n        elif reduction == 'min':\n            loss = loss.min()\n        return logits, loss\n\n        def half(self):\n            for module in self.modules():\n                if len([c for c in module.children()]) > 0:\n                    continue\n\n                if not isinstance(module, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d)):\n                    module.half()\n                else:\n                    module.float()\n            self._half = True\n            return self\n"""
AutoDL_sample_code_submission/Auto_Video/skeleton/__init__.py,0,b'# -*- coding: utf-8 -*-\n# pylint: disable=wildcard-import\nfrom __future__ import absolute_import\n\nfrom . import nn\nfrom . import optim\nfrom . import utils\n\nfrom . import data'
AutoDL_sample_code_submission/at_nlp/data_manager/__init__.py,0,b''
AutoDL_sample_code_submission/at_nlp/data_manager/data_sampler.py,0,"b'""""""\nMIT License\n\nCopyright (c) 2019 Lenovo Inc.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the ""Software""), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n""""""\n\nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom keras.preprocessing import text\n\nCHI_WORD_LENGTH = 2\nMAX_CHAR_LENGTH = 96\nMAX_VOCAB_SIZE = 20000\nMAX_SEQ_LENGTH = 601\n\nMAX_VALID_PERCLASS_SAMPLE = 400\nMAX_SAMPLE_TRIAN = 18000\n\nMAX_TRAIN_PERCLASS_SAMPLE = 1000\nmin_token_length = 2\n\nMAX_EN_CHAR_LENGTH = 35\n\nfrom Auto_NLP.utils.time_utils import info\nfrom at_nlp.data_manager.sample_utils import *\nfrom at_nlp.utils import color_msg\n\npunctuations = string.punctuation\n\n\ndef sample_input_data(input_x, input_y, num_classes, max_num=500):\n    all_index = []\n    meta_train_index = []\n\n    for i in range(num_classes):\n        all_index.append(\n            list(np.where((input_y[:, i] == 1) == True)[0]))\n\n    for i in range(num_classes):  # \xe6\x8c\x89label\xe7\xb1\xbb\xe5\x88\xab\xe6\x8a\xbd\xe5\x8f\x96\n        if len(all_index[i]) < max_num and len(all_index[i]) > 0:\n            tmp = all_index[i] * int(\n                max_num / len(all_index[i]))\n            tmp += random.sample(all_index[i],\n                                 max_num - len(tmp))\n            meta_train_index += tmp\n        else:\n            meta_train_index += random.sample(\n                all_index[i], max_num)\n\n    random.shuffle(meta_train_index)\n\n    train_sample_x = [input_x[i] for i in meta_train_index]\n    train_sample_y = input_y[meta_train_index, :]\n    return train_sample_x, train_sample_y\n\n\nclass DataGenerator(object):\n    def __init__(self,\n                 x_train, y_train,\n                 metadata,\n                 imbalance_level=-1,\n                 multi_label=False):\n\n        self.meta_data_x, self.meta_data_y = x_train, y_train\n        # \xe7\xac\xac\xe4\xb8\x80\xe6\xac\xa1\xe5\xa2\x9e\xe9\x87\x8f\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\xb0\xb1\xe6\x98\xaf\xe5\x88\x9d\xe5\xa7\x8b\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n        self.update_x, self.update_y = x_train, y_train\n\n        self.metadata = metadata\n\n        self.num_classes = self.metadata[\'class_num\']\n        self.num_samples_train = self.metadata[\'train_num\']\n        self.language = metadata[\'language\']\n        self.multi_label = multi_label\n\n        self.val_index = None\n        self.tokenizer = None\n        self.max_length = None\n        self.sample_num_per_class = None\n        self.data_feature = {}\n        self.eda_feature = {}\n        self.pseudo_x_train_size = 0\n        self.full_x = []\n        self.full_y = np.array([])\n\n        self.x_dict = {i: [] for i in range(self.num_classes)}\n        self.imbalance_flg = False\n        self.do_generate_sample = False\n        self.empty_class_ = []\n        self.meta_train_x = []\n        self.meta_train_y = np.array([])\n\n        self.full_index = None\n        self.imbalance_level = imbalance_level\n        self.MAX_TRAIN_PERCLASS_SAMPLE = MAX_TRAIN_PERCLASS_SAMPLE\n\n        if self.num_classes <= 5 and self.imbalance_level <= 1 and self.num_classes > 2:\n            self.MAX_TRAIN_PERCLASS_SAMPLE = 3000\n        elif self.num_classes == 2 and self.imbalance_level <= 1:\n            self.MAX_TRAIN_PERCLASS_SAMPLE = 3500\n\n        if self.multi_label:\n            if self.num_classes<50: #\xe7\xb1\xbb\xe5\x88\xab\xe5\xb0\x91\xef\xbc\x8c\xe6\xaf\x8f\xe7\xb1\xbb\xe5\x8f\x96100\xe6\x9d\xa1\n                self.MAX_TRAIN_PERCLASS_SAMPLE = 100\n            elif self.num_classes<100:\n                self.MAX_TRAIN_PERCLASS_SAMPLE = 50\n            else:\n                self.MAX_TRAIN_PERCLASS_SAMPLE = 20\n\n    def update_meta_data(self, x_train, y_train):\n        self.update_x = x_train\n        self.update_y = y_train\n        # \xe6\xb7\xbb\xe5\x8a\xa0\xe6\x96\xb0\xe5\xa2\x9e\xe6\x95\xb0\xe6\x8d\xae\n        self.meta_data_x = self.meta_data_x + x_train\n        self.meta_data_y = np.concatenate([self.meta_data_y, y_train], axis=0)\n\n    def set_sample_num_per_class(self, sample_num_per_class):\n        self.sample_num_per_class = sample_num_per_class\n\n    def _get_val_index(self, train_label_ratio, train_num, index):\n        # \xe8\xae\xbe\xe7\xbd\xae\xe9\x87\x87\xe6\xa0\xb7\xe6\xaf\x94\xe4\xbe\x8b\n        val_ratio = set_val_ratio(train_label_ratio, train_num)\n        # \xe9\x9a\x8f\xe6\x9c\xba\xe9\x87\x87\xe6\xa0\xb7\n        tmp = random.sample(index, int(len(index) * val_ratio))\n        # \xe8\xae\xbe\xe7\xbd\xae\xe9\x87\x87\xe6\xa0\xb7\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\n        if len(tmp) > MAX_VALID_PERCLASS_SAMPLE:\n            tmp = tmp[:MAX_VALID_PERCLASS_SAMPLE]\n        return tmp\n\n    def sample_val_index(self, input_y):\n        train_label_distribution = np.sum(np.array(input_y), 0)\n        train_label_ratio = train_label_distribution / np.sum(train_label_distribution)\n        all_index = get_sample_index(input_y, self.num_classes)\n        val_index = []\n\n        for i in range(self.num_classes):\n            if train_label_distribution[i] == 0:\n                continue\n            tmp = self._get_val_index(train_label_ratio=train_label_ratio[i],\n                                      train_num=train_label_distribution[i],\n                                      index=all_index[i])\n            val_index += tmp\n            # \xe5\x8e\xbb\xe9\x99\xa4train/val \xe4\xba\xa4\xe5\x8f\x89\xe6\xa0\xb7\xe6\x9c\xac\n            all_index[i] = list(set(all_index[i]).difference(set(tmp)))\n\n        return all_index, val_index\n\n    def _set_max_train_sample_num(self, train_label_distribution):\n        self.max_sample_num_per_class = int(\n            np.max(train_label_distribution) * 4 / 5)\n\n        if self.sample_num_per_class is None:\n            if self.num_samples_train < MAX_SAMPLE_TRIAN:\n                self.sample_num_per_class = self.max_sample_num_per_class\n            else:\n                self.sample_num_per_class = min(self.max_sample_num_per_class, self.MAX_TRAIN_PERCLASS_SAMPLE)\n        else:\n            # \xe9\x81\xbf\xe5\x85\x8d\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0\xe5\xa4\x9a\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\xe4\xb8\x8b\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x80\xe6\xac\xa1\xe8\xbf\x9b\xe6\xa0\xb7\xe5\xb0\x91\xef\xbc\x8c\xe5\xaf\xbc\xe8\x87\xb4\xe5\x90\x8e\xe9\x9d\xa2\xe8\xbf\x9e\xe7\xbb\xad\xe9\x87\x87\xe6\xa0\xb7\xe8\xbf\x87\xe4\xbd\x8e\n            self.sample_num_per_class = max(self.max_sample_num_per_class, int(np.mean(train_label_distribution)))\n\n        if self.imbalance_flg:\n            max_sample_num = min(self.sample_num_per_class, int(np.mean(train_label_distribution)))\n            max_sample_num = min(max_sample_num, self.MAX_TRAIN_PERCLASS_SAMPLE)\n        else:\n            max_sample_num = min(self.sample_num_per_class, self.MAX_TRAIN_PERCLASS_SAMPLE)\n\n        return max_sample_num\n\n    def balance_sampling_index(self, all_index, train_label_distribution):\n        max_sample_num = self._set_max_train_sample_num(train_label_distribution)\n        meta_train_index = balance_sampling(all_index, max_sample_num, num_classes=self.num_classes)\n        random.shuffle(meta_train_index)\n        self.meta_train_index = meta_train_index\n        return meta_train_index\n\n    def check_imbalance_level(self, train_label_distribution):\n        if self.normal_std >= 0.1 or 0.0 in train_label_distribution:\n            self.imbalance_flg = True\n\n    def generate_presudo_samples(self, all_index):\n        new_generate_index = []\n        for i in range(self.num_classes):\n            new_generate_index.append([])\n\n        for i in range(self.num_classes):  # \xe6\x8c\x89label\xe7\xb1\xbb\xe5\x88\xab\xe6\x8a\xbd\xe5\x8f\x96\n            if len(all_index[i]) == 0:\n                self.do_generate_sample = True\n                new_samples = do_random_generate_sample(data_x=self.meta_data_x, language=self.language, num=1)\n                new_generate_index[i] = new_samples\n\n        return new_generate_index\n\n    def generate_pseudo_samples(self, train_x, train_y):\n        info(""Do Radam Create Samples!"")\n        for i in range(self.num_classes):\n            new_samples = self.new_generate_samples_idx[i]\n            if len(new_samples) == 0:\n                continue\n            train_x.extend(new_samples)\n            new_label = np.zeros((len(new_samples), self.num_classes))\n            new_label[:, i] = 1\n            train_y = np.concatenate([train_y, new_label], axis=0)\n\n        return train_x, train_y\n\n    def _update_train_meta(self, train_diff_x, train_diff_y):\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe5\x85\xa8\xe9\x87\x8f\xe7\x9a\x84train \xe6\xa0\xb7\xe6\x9c\xac\n        if self.meta_train_y.shape[0] == 0:\n            self.meta_train_x = train_diff_x\n            self.meta_train_y = train_diff_y\n        else:\n            self.meta_train_x = self.meta_train_x + train_diff_x\n            self.meta_train_y = np.concatenate([self.meta_train_y, train_diff_y], axis=0)\n\n    def extend_train_data(self, x, y):\n        train_x, train_y = map_x_y(self.meta_train_index, x, y)\n        if self.do_generate_sample and not self.multi_label:\n            train_x, train_y = self.generate_pseudo_samples(train_x, train_y)\n            self.do_generate_sample = False\n        return train_x, train_y\n\n    def get_sampling_data_frm_full_train(self):\n        """"""\n        \xe4\xbb\x8e\xe5\x85\xa8\xe5\xb1\x80\xe7\x9a\x84train data\xe4\xb8\xad\xe9\x87\x87\xe6\xa0\xb7\xef\xbc\x8c\xe5\x8f\xaa\xe7\x9c\x8b\xe5\xbd\x93\xe5\x89\x8d\xe7\x9a\x84 meta_train_x, meta_train_y\n        :return:\n        """"""\n        sample_index = get_sample_index(self.meta_train_y, self.num_classes)\n        train_label_distribution = np.sum(np.array(self.meta_train_y), 0)\n        self.balance_sampling_index(sample_index, train_label_distribution)\n        # \xe6\xaf\x8f\xe6\xac\xa1\xe5\x8f\xaa\xe7\x9c\x8b\xe5\xbd\x93\xe5\x89\x8d\xe9\x9c\x80\xe8\xa6\x81\xe9\x87\x87\xe6\xa0\xb7\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe6\x98\xaf\xe5\x90\xa6\xe5\x9d\x87\xe8\xa1\xa1\xef\xbc\x8c\xe6\x98\xaf\xe5\x90\xa6\xe9\x9c\x80\xe8\xa6\x81\xe7\x94\x9f\xe6\x88\x90\xe4\xbc\xaa\xe6\xa0\xb7\xe6\x9c\xac\n        self.normal_std, self.empty_class_ = get_imbalance_statistic(train_label_distribution)\n        self.check_imbalance_level(train_label_distribution)\n        self.new_generate_samples_idx = self.generate_presudo_samples(sample_index)\n        self.imbalance_flg = False\n        train_x, train_y = self.extend_train_data(x=self.meta_train_x, y=self.meta_train_y)\n        train_label_distribution = np.sum(np.array(train_y), 0)\n        return train_x, train_y\n\n    def sample_dataset_pipeline(self, use_val=False, update_train=True, data_x=None, data_y=None):\n        """"""\n        \xe5\x85\xa8\xe5\xb1\x80\xe9\x87\x87\xe6\xa0\xb7pipeline\n        :param use_val: \xe6\x98\xaf\xe5\x90\xa6\xe9\x87\x87\xe7\x94\xa8val\xe6\x95\xb0\xe6\x8d\xae\n        :param update_train: \xe6\x98\xaf\xe5\x90\xa6\xe6\x9b\xb4\xe6\x96\xb0\xe5\x85\xa8\xe9\x87\x8ftrain\n        :param data_x: \xe9\x87\x87\xe6\xa0\xb7\xe6\x95\xb0\xe6\x8d\xae\xe6\x9d\xa5\xe6\xba\x90x\xef\xbc\x9a\xe5\xa2\x9e\xe9\x87\x8f\xe6\x95\xb0\xe6\x8d\xae\xe6\x88\x96\xe8\x80\x85\xe5\x85\xa8\xe9\x87\x8f\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\n        :param data_y: \xe9\x87\x87\xe6\xa0\xb7\xe6\x95\xb0\xe6\x8d\xae\xe6\x9d\xa5\xe6\xba\x90y\xef\xbc\x9a\xe5\xa2\x9e\xe9\x87\x8f\xe6\x95\xb0\xe6\x8d\xae\xe6\x88\x96\xe8\x80\x85\xe5\x85\xa8\xe9\x87\x8f\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\n        :return: \xe5\x9d\x87\xe8\xa1\xa1\xe9\x87\x87\xe6\xa0\xb7\xe5\x90\x8e\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86/\xe8\xaf\x84\xe4\xbc\xb0\xe9\x9b\x86\xef\xbc\x8cuse_val\xe4\xb8\xbaTrue\xe6\x97\xb6\xef\xbc\x8c\xe8\xaf\x84\xe4\xbc\xb0\xe9\x9b\x86\xe4\xb8\xba\xe7\xa9\xba\n        """"""\n        val_diff_x, val_diff_y = None, None\n        ############################ \xe9\x87\x87\xe6\xa0\xb7\xe5\x87\x86\xe5\xa4\x87\xe9\x98\xb6\xe6\xae\xb5 ###################################\n        if update_train:\n            # \xe5\xa2\x9e\xe9\x87\x8f\xe6\x9b\xb4\xe6\x96\xb0\xef\xbc\x88\xe7\xac\xac\xe4\xb8\x80\xe6\xac\xa1\xe6\xa0\xb7\xe6\x9c\xac\xe5\x8d\xb3\xe5\xa2\x9e\xe9\x87\x8f\xef\xbc\x89\n            self.add_index, self.add_val_index = self.sample_val_index(data_y)\n\n            val_diff_x, val_diff_y = map_x_y(self.add_val_index, data_x, data_y)\n            # \xe6\xad\xa4\xe6\x97\xb6\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\xb2\xa1\xe6\x9c\x89\xe8\xbf\x9b\xe8\xa1\x8c\xe9\x87\x87\xe6\xa0\xb7\n            train_diff_x, train_diff_y = flat_map_x_y(index=self.add_index, x=data_x, y=data_y)\n\n            if use_val:  # \xe5\xa6\x82\xe6\x9e\x9c\xe9\x87\x87\xe7\x94\xa8val\xef\xbc\x8c\xe5\x8d\xb3\xe5\xbd\x93\xe5\x89\x8d\xe4\xb8\x8d\xe5\x88\x86train valid\xef\xbc\x8c\xe5\x85\xa8\xe9\x83\xa8\xe6\x95\xb0\xe6\x8d\xae\xe6\x9b\xb4\xe6\x96\xb0meta_train\n                train_diff_x = train_diff_x + val_diff_x\n                train_diff_y = np.concatenate([train_diff_y, val_diff_y], axis=0)\n                val_diff_x = None\n                val_diff_y = None\n\n            self._update_train_meta(train_diff_x, train_diff_y)\n\n        if val_diff_x:\n            val_label_distribution = np.sum(np.array(val_diff_y), 0)\n\n        ############################ \xe8\xbf\x9b\xe5\x85\xa5\xe9\x87\x87\xe6\xa0\xb7\xe9\x98\xb6\xe6\xae\xb5 ###################################\n        train_x, train_y = self.get_sampling_data_frm_full_train()\n        return train_x, train_y, val_diff_x, val_diff_y\n\n    def sample_dataset_iter(self, add_val_to_train=False, update_train=True, use_full=False):\n        """"""\n\n        :param add_val_to_train: \xe6\x98\xaf\xe5\x90\xa6\xe9\x87\x87\xe7\x94\xa8val\xe6\x95\xb0\xe6\x8d\xae\n        :param update_train: \xe6\x98\xaf\xe5\x90\xa6\xe6\x9b\xb4\xe6\x96\xb0\xe5\x85\xa8\xe9\x87\x8ftrain\xef\xbc\x9a\xe5\xbd\x93\xe6\xb2\xa1\xe6\x9c\x89\xe6\x96\xb0\xe5\xa2\x9e\xe6\xa0\xb7\xe6\x9c\xac\xe6\x97\xb6\xef\xbc\x8c\xe4\xb9\x9f\xe4\xb8\x8d\xe5\x86\x8d\xe6\x9b\xb4\xe6\x96\xb0\xe5\x85\xa8\xe9\x87\x8ftrain\n        :param use_full:\xe4\xbd\xbf\xe7\x94\xa8\xe5\xa2\x9e\xe9\x87\x8f\xe6\x95\xb0\xe6\x8d\xae or \xe4\xbd\xbf\xe7\x94\xa8\xe5\x85\xa8\xe9\x87\x8f\xe6\x95\xb0\xe6\x8d\xae\n        :return:\n        """"""\n        if not use_full:\n            # \xe5\x9c\xa8\xe6\x96\xb0\xe6\xa0\xb7\xe6\x9c\xac\xe4\xb8\x8a\xe9\x87\x8d\xe6\x96\xb0\xe5\x88\x92\xe5\x88\x86\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\x92\x8c\xe8\xaf\x84\xe4\xbc\xb0\xe9\x9b\x86\n            return self.sample_dataset_pipeline(add_val_to_train, update_train,\n                                                data_x=self.update_x,\n                                                data_y=self.update_y)\n        else:\n            # \xe6\xb8\x85\xe7\xa9\xba\xe5\x8e\x86\xe5\x8f\xb2\xe8\xae\xb0\xe5\xbd\x95\xe6\x95\xb0\xe6\x8d\xae\n            self.meta_train_y = np.array([])\n            self.meta_train_x = []\n\n            return self.sample_dataset_pipeline(use_val=False, update_train=True,\n                                                data_x=self.meta_data_x, data_y=self.meta_data_y)\n\n    def vectorize_data(self, x_train, x_val=None, analyzer=\'word\'):\n        vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=20000, analyzer=analyzer)\n\n        if x_val:\n            full_text = x_train + x_val\n        else:\n            full_text = x_train\n        vectorizer.fit(full_text)\n        train_vectorized = vectorizer.transform(x_train)\n        if x_val:\n            val_vectorized = vectorizer.transform(x_val)\n            return train_vectorized, val_vectorized, vectorizer\n        return train_vectorized, vectorizer\n\n    def sequentialize_data_no_padding(self, train_contents, feature_mode, val_contents=[], tokenizer=None,\n                                      max_length=None,\n                                      Max_Vocab_Size=None):\n        if Max_Vocab_Size is None:\n            Vocab_Size = MAX_VOCAB_SIZE\n        else:\n            Vocab_Size = Max_Vocab_Size\n        if tokenizer is None:\n            if feature_mode == 0:\n                tokenizer = text.Tokenizer(num_words=Vocab_Size,\n                                           char_level=True,\n                                           oov_token=""UNK"")\n            elif feature_mode == 1:\n                tokenizer = text.Tokenizer(num_words=Vocab_Size)\n\n            tokenizer.fit_on_texts(train_contents)\n\n        _max_length = max_length\n        word_index = tokenizer.word_index\n        num_features = min(len(word_index) + 1, Vocab_Size)\n\n        if val_contents:\n            return word_index, num_features, tokenizer, _max_length\n        else:\n            return word_index, num_features, tokenizer, _max_length\n'"
AutoDL_sample_code_submission/at_nlp/data_manager/feature_config.py,0,"b""# -*- coding: utf-8 -*-\nMAX_SEQ_LENGTH = 301\nMAX_SEQ_EN_LENGTH_PRE = 128\nMAX_SEQ_EN_LENGTH_POST = 256\nMAX_SEQ_ZH_LENGTH_PRE = 128\nMAX_SEQ_ZH_LENGTH_POST = 128\n\nDEFAULT_EN_CONF = {\n    'is_ratio':\n        {'ratio': 0.1},\n}\n\nCUSTOM_EN_CONF = {\n    'is_ratio':\n        {'ratio': 0.1},\n    'filter_word_len':\n        {'word_len_min': 1,\n         'word_len_max': 300},\n\n    'cut_style':\n        {'cut_style': 1,\n         'cut_length_pre': MAX_SEQ_EN_LENGTH_PRE,\n         'cut_length_post': MAX_SEQ_EN_LENGTH_POST}}\n\nDEFAULT_ZH_CONF = {'cut_style': {'cut_style': 0,\n                                   'cut_length_pre': MAX_SEQ_LENGTH,\n                                   'cut_length_post': 0}}\n\nCUSTOM_ZH_CONF = {'cut_style': {'cut_style': 1,\n                                  'cut_length_pre': MAX_SEQ_ZH_LENGTH_PRE,\n                                  'cut_length_post': MAX_SEQ_ZH_LENGTH_POST}}\n"""
AutoDL_sample_code_submission/at_nlp/data_manager/feature_utils.py,0,"b'# -*- coding: utf-8 -*-\n# @Date    : 2020/3/2 11:59\n# @Author  : stella\n# @Desc    :\nfrom sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\nfrom keras.preprocessing import text\nfrom keras.preprocessing import sequence\n\nfrom at_nlp.data_manager.preprocess_utils import _tokenize_chinese_words, clean_en_text_custom, clean_zh_text_custom\n\n\ndef preprocess_data(x, preprocess_conf, language=\'EN\', do_seg=False):\n    """"""\n    \xe6\x96\x87\xe6\x9c\xac\xe6\xb8\x85\xe6\xb4\x97\n    :param x:list<string>\xef\xbc\x9a\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x96\x87\xe6\x9c\xac\n    :param preprocess_conf: :\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\xb8\x85\xe6\xb4\x97\xe9\x80\x89\xe9\xa1\xb9\xe5\x8f\x8a\xe6\x93\x8d\xe4\xbd\x9c\n    :param language: \xe6\x96\x87\xe6\x9c\xac\xe5\xaf\xb9\xe5\xba\x94\xe8\xaf\xad\xe8\xa8\x80\n    :param do_seg: \xe6\x98\xaf\xe5\x90\xa6\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe8\xaf\x8d(\xe9\x92\x88\xe5\xaf\xb9ZH)\n    :return: \xe6\xb8\x85\xe6\xb4\x97\xe5\x90\x8e\xe7\x9a\x84\xe6\x96\x87\xe6\x9c\xac\xe5\x88\x97\xe8\xa1\xa8\n    """"""\n    if language == \'ZH\':\n        if not do_seg:\n            clean_dat = clean_zh_text_custom(x, preprocess_conf)\n        else:\n\n            clean_dat = clean_zh_text_custom(x, preprocess_conf)\n            clean_dat = list(map(_tokenize_chinese_words, clean_dat))\n    else:\n        clean_dat = clean_en_text_custom(x, preprocess_conf)\n\n    return clean_dat\n\n\ndef build_tokenizer(dat, tokenizer_type=""svm"", **kwargs):\n    if tokenizer_type == ""svm"":\n        if kwargs[""tfidf""]:\n            tokenizer = TfidfVectorizer(ngram_range=(1, 1),\n                                        max_features=kwargs[""max_features""],\n                                        analyzer=kwargs[""analyzer""])\n            tokenizer.fit(dat)\n            return tokenizer\n        elif kwargs[""hashing""]:\n            tokenizer = HashingVectorizer(ngram_range=(1, 1),\n                                          n_features=kwargs[""max_features""],\n                                          analyzer=kwargs[""analyzer""])\n            return tokenizer\n\n    elif tokenizer_type == ""nn"":\n        tokenizer = text.Tokenizer(num_words=kwargs[""num_words""])\n        tokenizer.fit_on_texts(dat)\n        return tokenizer\n\n\ndef postprocess_data(clean_dat, tokenizer, tokenizer_type=""svm"", **kwargs):\n    if tokenizer_type == ""svm"":\n        vectorized_dat = tokenizer.transform(clean_dat)\n        return vectorized_dat\n    elif tokenizer_type == ""nn"":\n        id_dat = tokenizer.texts_to_sequences(clean_dat)\n        max_length = kwargs[""pad_max_length""]\n        padding_method = kwargs[""padding""]\n        sequentialize_dat = sequence.pad_sequences(id_dat, maxlen=max_length, padding=padding_method)\n        return sequentialize_dat\n\n\n'"
AutoDL_sample_code_submission/at_nlp/data_manager/preprocess_utils.py,0,"b'import re\nimport string\nimport numpy as np\n\nimport jieba_fast as jieba\n\nfrom Auto_NLP.second_stage_models import ac\nfrom at_nlp.data_manager.feature_config import MAX_SEQ_LENGTH\n\n\ndef detect_digits(input_str):\n    trantab = str.maketrans(dict.fromkeys(string.punctuation + ""@!#$%^&*()-<>[]?.\\/+_~:""))\n    input_str = input_str.strip()\n    clean_line = input_str.translate(trantab)\n    cnt = 0\n    words = clean_line.strip().split()\n    for word in words:\n        if word.isdigit():\n            cnt += 1\n    return round(float(cnt) / float(len(words)), 4)\n\n\ndef detect_supper_and_digits(input_str_list):\n    trantab = str.maketrans(dict.fromkeys(string.punctuation + ""@!#$%^&*()-<>[]?.\\/+_~:""))\n    upper_cnt, digits_cnt = [], []\n    for input_str in input_str_list:\n        input_str = input_str.strip()\n        clean_line = input_str.translate(trantab)\n        cnt = 0\n        digit_cnt = 0\n        words = clean_line.strip().split()\n        for word in words:\n            if word.istitle() or word.isupper():\n                cnt += 1\n            if word.isdigit():\n                digit_cnt += 1\n        if len(words) > 0:\n            upper_cnt.append(round(float(cnt) / float(len(words)), 5))\n            digits_cnt.append(round(float(digit_cnt) / float(len(words)), 5))\n    return np.average(upper_cnt), np.average(digits_cnt)\n\n\ndef detect_punctuation(input_str_lst):\n    trantab = str.maketrans(dict.fromkeys(string.punctuation + ""@!#$%^&*()-<>[]?.\\/+_~:"" + \'0123456789\'))\n    cnt = []\n    for input_str in input_str_lst:\n        input_str = input_str.strip()\n        clean_line = input_str.translate(trantab)\n        cnt_original = len(input_str.split())\n        cnt_clean = len(clean_line.split())\n        if cnt_original == 0:\n            cnt.append(0.0)\n        else:\n            cnt.append(round(float(cnt_original - cnt_clean) / float(cnt_original), 5))\n    return np.average(cnt)\n\n\ndef _tokenize_chinese_words(text):\n    return \' \'.join(jieba.cut(text, cut_all=False))\n\n\ndef clean_en_text(dat, ratio=0.1, is_ratio=True):\n    REPLACE_BY_SPACE_RE = re.compile(\'[""/(){}\\[\\]\\|@,;-]\')\n    BAD_SYMBOLS_RE = re.compile(\'[^0-9a-zA-Z #+_]\')\n\n    ret = []\n    for line in dat:\n        line = REPLACE_BY_SPACE_RE.sub(\' \', line)\n        # line = BAD_SYMBOLS_RE.sub(\'\', line)\n        line = line.strip()\n        line_split = line.split()\n\n        if is_ratio:\n            NUM_WORD = max(int(len(line_split) * ratio), MAX_SEQ_LENGTH)\n        else:\n            NUM_WORD = MAX_SEQ_LENGTH\n\n        if len(line_split) > NUM_WORD:\n            line = "" "".join(line_split[0:NUM_WORD])\n\n        ret.append(line)\n    return ret\n\n\ndef clean_zh_text(dat, seq_len):\n    dat_array = np.array(dat, dtype=\'object\')\n    return ac.clean_text_zh_seg1(dat_array, seq_len)\n\n\ndef apply_ratio(sentence, ratio=0.1, max_seq_length=MAX_SEQ_LENGTH):\n    num_words = max(int(len(sentence) * ratio), max_seq_length)\n    return sentence[:num_words]\n\n\ndef apply_filter_word_len(sentence, filter_word_len_min=1, filter_word_len_max=100):\n    words = [w for w in sentence if (len(w) > filter_word_len_min and len(w) < filter_word_len_max)]\n    return words\n\n\ndef apply_cut_sentence(sentence, cut_style=0, cut_length_pre=301, cut_length_post=0):\n    if cut_style == 0:\n        return sentence[:cut_length_pre]\n    elif cut_style == 1:\n        if len(sentence) <= cut_length_pre + cut_length_post:\n            return sentence\n        return sentence[:cut_length_pre] + sentence[-cut_length_post:]\n\n\ndef clean_en_text_custom(dat, custom_config):\n    ret = []\n    for line in dat:\n        REPLACE_BY_SPACE_RE = re.compile(\'[""/(){}\\[\\]\\|@,;-]\')\n        line = REPLACE_BY_SPACE_RE.sub(\' \', line)\n        line = line.strip()\n        line_split = line.split()\n        # \xe5\x85\x88\xe6\x88\xaa\xe6\x96\xad\xe5\x86\x8d\xe6\xb8\x85\xe6\xb4\x97\n        if ""cut_style"" in custom_config:\n            line_split = apply_cut_sentence(line_split,\n                                            custom_config[""cut_style""][""cut_style""],\n                                            custom_config[""cut_style""][""cut_length_pre""],\n                                            custom_config[""cut_style""][""cut_length_post""])\n\n        if ""filter_word_len"" in custom_config:\n            line_split = apply_filter_word_len(line_split, custom_config[""filter_word_len""][""word_len_min""],\n                                               custom_config[""filter_word_len""][""word_len_max""])\n\n        if ""is_ratio"" in custom_config:\n            line_split = apply_ratio(line_split, custom_config[""is_ratio""][""ratio""])\n\n        line = "" "".join(line_split)\n        ret.append(line)\n    return ret\n\n\ndef clean_zh_text_custom(dat, custom_config):\n    ret = []\n    for line in dat:\n        line = line.strip()\n        if ""cut_style"" in custom_config:\n            line = apply_cut_sentence(line,\n                                      custom_config[""cut_style""][""cut_style""],\n                                      custom_config[""cut_style""][""cut_length_pre""],\n                                      custom_config[""cut_style""][""cut_length_post""])\n        ret.append(line)\n    ret = clean_zh_text(ret, seq_len=MAX_SEQ_LENGTH)\n    return ret\n'"
AutoDL_sample_code_submission/at_nlp/data_manager/sample_config.py,0,"b'sample_strategy = {\n\n    ""sample_iter_incremental_with_train_split"": {\'add_val_to_train\': False,\n                                                 \'update_train\': True,\n                                                 \'use_full\': False\n                                                 },\n\n    ""sample_iter_incremental_no_train_split"": {\'add_val_to_train\': True,\n                                               \'update_train\': True,\n                                               \'use_full\': False},\n\n\n    ""sample_from_full_data"": {\'add_val_to_train\': False,\n                              \'update_train\': False,\n                              \'use_full\': True},\n\n\n    ""sample_from_full_train_data"": {\'add_val_to_train\': False,\n                                    \'update_train\': False,\n                                    \'use_full\': False}\n}\n'"
AutoDL_sample_code_submission/at_nlp/data_manager/sample_utils.py,0,"b'import numpy as np\nimport random\n\n\ndef set_val_ratio(train_label_ratio, train_num):\n    if train_label_ratio < 0.01:\n        ratio = 0.075\n    else:\n        ratio = 0.2\n\n    return ratio\n\n\ndef get_sample_index(input_y, num_class):\n    # \xe8\x8e\xb7\xe5\x8f\x96\xe5\xaf\xb9\xe5\xba\x94label\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84index\n    sample_index = []\n    for i in range(num_class):\n        if list(np.where((input_y[:, i] == 1) == True)[0]) == 0:\n            continue\n        sample_index.append(\n            list(np.where((input_y[:, i] == 1) == True)[0]))\n    return sample_index\n\ndef downsampling_input_data(input_x, input_y, num_class, max_sample_num):\n    sample_index = get_sample_index(input_y, num_class)\n    meta_train_index = balance_sampling(sample_index, max_sample_num=max_sample_num, num_classes=num_class)\n    random.shuffle(meta_train_index)\n    train_sample_x, train_sample_y = map_x_y(meta_train_index, input_x, input_y)\n    return train_sample_x, train_sample_y\n\ndef balance_sampling(all_index, max_sample_num, num_classes):\n    meta_train_index = []\n    for i in range(num_classes):  # \xe6\x8c\x89label\xe7\xb1\xbb\xe5\x88\xab\xe6\x8a\xbd\xe5\x8f\x96\n        if len(all_index[i]) == 0:\n            continue\n        elif len(all_index[i]) < max_sample_num and len(all_index[i]) > 0:\n            tmp = all_index[i] * int(\n                max_sample_num / len(all_index[i]))\n            tmp += random.sample(all_index[i],\n                                 max_sample_num - len(tmp))\n            meta_train_index += tmp\n        else:\n            meta_train_index += random.sample(\n                all_index[i], max_sample_num)\n    return meta_train_index\n\n\ndef map_x_y(index, x, y):\n    _x = [x[i] for i in index]\n    _y = y[index, :]\n    return _x, _y\n\n\ndef flat_map_x_y(index, x, y):\n    flat_index = []\n    for i in range(len(index)):\n        flat_index.extend(index[i])\n    _x, _y = map_x_y(flat_index, x, y)\n    return _x, _y\n\n\ndef get_imbalance_statistic(train_label_distribution):\n    normal_std = np.std(train_label_distribution) / np.sum(train_label_distribution)\n    empty_class_ = [i for i in range(train_label_distribution.shape[0]) if train_label_distribution[i] == 0]\n    return normal_std, empty_class_\n\n\ndef do_random_generate_sample(data_x, language, num=1):\n    # \xe9\xbb\x98\xe8\xae\xa4\xe5\x8f\xaa\xe9\x9a\x8f\xe6\x9c\xba\xe4\xba\xa7\xe7\x94\x9f\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xef\xbc\x8c\xe5\x87\x8f\xe5\xb0\x91\xe5\xb9\xb2\xe6\x89\xb0\n    sentence_list = []\n    for i in range(num):\n        rand_int = random.randint(0, len(data_x) - 1)\n        sentence_list.append(data_x[rand_int])\n    generate_samples = []\n    for sentence in sentence_list:\n        if language == ""EN"":\n            words = sentence.split("" "")\n        else:\n            words = sentence\n        if len(words) <= 1:\n            new_words = "",""\n        else:\n            w_i = random.randint(0, len(words) - 1)\n            new_words = words[w_i]\n        generate_samples.append(new_words)\n    return generate_samples\n\ndef check_x_avg_length(input_x):\n    return np.mean([len(x.split("" "")) for x in input_x])'"
AutoDL_sample_code_submission/at_nlp/generators/__init__.py,0,b''
AutoDL_sample_code_submission/at_nlp/generators/data_generator.py,0,"b""# -*- coding: utf-8 -*-\n# @Date    : 2020/1/17 14:19\n# @Author  :\n# @Desc    :\nimport numpy as np\nimport keras\nfrom keras.preprocessing import sequence\n\nclass DataGenerator(keras.utils.Sequence):\n    def __init__(self, x_train, labels, batch_size,  language, max_length, vocab, num_features,\n                 tokenizer=None,\n                 shuffle=True):\n        self.indices_ = None\n        self.batch_size = batch_size\n        self.X = x_train\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.language = language\n        self.shuffle = shuffle\n        self.max_length = max_length\n        self.vocab = vocab\n        self.num_features = num_features\n        self.on_epoch_end()\n\n    def __len__(self):\n        if self.shuffle:\n            if len(self.X)>self.batch_size: # \xe4\xbf\x9d\xe8\xaf\x81\xe8\x87\xb3\xe5\xb0\x91\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaabatch\n                return int(np.floor(len(self.X) / self.batch_size))\n            else:\n                return 1\n        else:\n            if len(self.X) % self.batch_size==0:\n                return int((len(self.X) / self.batch_size))\n            else:\n                return int((len(self.X) / self.batch_size)+1)\n\n    def __getitem__(self, index):\n        indexes = self.indices_[index * self.batch_size:min((index + 1) * self.batch_size, len(self.X))]\n        X_temp = [self.X[k] for k in indexes]\n        batch_x, batch_y = self._process(X_temp, indexes)\n        return batch_x, batch_y\n\n    def on_epoch_end(self):\n        self.indices_ = np.arange(len(self.X))\n        if self.shuffle:\n            np.random.shuffle(self.indices_)\n\n    def _process(self, X_temp, indexes):\n        data_ids = self.tokenizer.texts_to_sequences(X_temp)\n        max_length = self.max_length\n        batch_x = sequence.pad_sequences(data_ids, maxlen=max_length, padding='post')\n        batch_y = self.labels[indexes]\n        return batch_x, batch_y\n"""
AutoDL_sample_code_submission/at_nlp/generators/ensemble_manager.py,0,b'# -*- coding: utf-8 -*-\n# @Date    : 2020/3/4 17:10\n\nclass EnsembleManager(object):\n    def __init__(self):\n        pass\n\n    def single_model_ensemble(self):\n        pass\n\n    def multi_model_ensemble(self):\n        pass\n\n\n'
AutoDL_sample_code_submission/at_nlp/generators/feature_generator.py,0,"b'# -*- coding: utf-8 -*-\n# @Date    : 2020/3/3 16:29\nfrom log_utils import info\nfrom at_nlp.utils import color_msg\nfrom at_nlp.data_manager.feature_utils import *\nfrom at_nlp.data_manager.preprocess_utils import *\nfrom at_nlp.data_manager.feature_config import DEFAULT_EN_CONF, DEFAULT_ZH_CONF, CUSTOM_ZH_CONF\n\nMAX_VOCAB_SIZE = 20000\nMAX_SEQ_LENGTH = 301\nMAX_TOLERANT_STD = 150\n\nclass FeatureGenerator():\n    def __init__(self, language, do_seg, num_class):\n        self.language = language\n        self.num_classes = num_class\n        if self.language == ""EN"":\n            self.default_preprocess_conf = DEFAULT_EN_CONF\n        else:\n            self.default_preprocess_conf = CUSTOM_ZH_CONF\n        self.do_seg = do_seg\n        self.tokenizer = None\n        self.tokenizer_type = """"\n        self.tokenizer_conf = {}\n        self.data_feature = {}\n\n        self.max_length = MAX_SEQ_LENGTH\n        self.seq_len_std = 0.0\n        self.vocab_size = MAX_VOCAB_SIZE\n        self.num_features = 0\n        self.word_index = None\n\n    def reset_tokenizer(self):\n        self.tokenizer = None\n        self.tokenizer_type = """"\n        self.tokenizer_conf = {}\n        self.data_feature = {}\n\n\n    def _set_tokenizer_conf(self, model_name=\'svm\'):\n        if model_name == ""svm"":\n            tokenizer_conf = {\n                \'tfidf\': True,\n                \'max_features\': 20000,\n                \'analyzer\': \'word\'}\n            tokenizer_type = \'svm\'\n\n        else:\n            tokenizer_conf = {\n                \'num_words\': 20000,\n                \'use_char\': False,\n                \'pad_max_length\': 301,\n                \'padding\': \'post\'}\n            tokenizer_type = \'nn\'\n\n        return tokenizer_type, tokenizer_conf\n\n    def update_preprocess_conf(self):\n        pass\n\n    def preprocess_data(self, x):\n        return preprocess_data(x, self.default_preprocess_conf, self.language, do_seg=self.do_seg)\n\n    def set_tokenizer(self, dat, tokenizer_type):\n        if tokenizer_type == ""svm"":\n            self.tokenizer = build_tokenizer(dat, tokenizer_type, **self.tokenizer_conf)\n\n        elif tokenizer_type == \'nn\':\n            self.set_max_seq_len()\n            self.set_max_vocab_size(dat)\n            self.tokenizer_conf[\'num_words\'] = self.vocab_size\n            self.tokenizer_conf[\'pad_max_length\'] = self.max_length\n            self.tokenizer = build_tokenizer(dat, tokenizer_type, **self.tokenizer_conf)\n            self.word_index = self.tokenizer.word_index\n            self.num_features = min(len(self.word_index) + 1, self.vocab_size)\n\n\n    def build_tokenizer(self, clean_dat, model_name, analyzer=\'word\'):\n        if self.tokenizer is None:\n            self.tokenizer_type, self.tokenizer_conf = self._set_tokenizer_conf(model_name=model_name)\n            if model_name == ""svm"":\n                self.tokenizer_conf[""analyzer""] = analyzer\n            self.set_tokenizer(clean_dat, self.tokenizer_type)\n\n    def postprocess_data(self, clean_dat):\n        return postprocess_data(clean_dat, self.tokenizer, tokenizer_type=self.tokenizer_type,  **self.tokenizer_conf)\n\n    def set_data_feature(self):\n        self.data_feature[""num_features""] = self.num_features\n        self.data_feature[""num_class""] = self.num_classes\n        self.data_feature[\'max_length\'] = self.max_length\n        self.data_feature[\'input_shape\'] = self.max_length\n        self.data_feature[""rnn_units""] = 128\n        self.data_feature[""filter_num""] = 64\n        self.data_feature[""word_index""] = self.word_index\n\n\n    def set_max_seq_len(self):\n        if self.max_length > MAX_SEQ_LENGTH:\n            self.max_length = MAX_SEQ_LENGTH\n        if self.seq_len_std > MAX_TOLERANT_STD:\n            self.max_length = MAX_SEQ_LENGTH\n\n    def set_max_vocab_size(self, input_x):\n        avg_punct_cnt = detect_punctuation(input_x)\n        avg_upper_cnt, avg_digit_cnt = detect_supper_and_digits(input_x)\n        info(""avg_punct_cnt is {} and avg_upper_cnt is {} and avg_digit_cnt is {}"".format(avg_punct_cnt,\n                                                                                          avg_upper_cnt,\n                                                                                          avg_digit_cnt))\n        if avg_punct_cnt <= 0.02:\n            Max_Vocab_Size = 30000\n        else:\n            Max_Vocab_Size = 20000\n        self.vocab_size = Max_Vocab_Size\n'"
AutoDL_sample_code_submission/at_nlp/generators/model_generator.py,0,"b'# -*- coding: utf-8 -*-\n# @Date    : 2020/3/3 10:46\nfrom sklearn.svm import LinearSVC\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.multiclass import OneVsRestClassifier\n\nfrom keras_radam import RAdam\nfrom keras.optimizers import SGD, RMSprop, Adamax, Adadelta, Adam\n\nfrom log_utils import info\nfrom at_nlp.model_manager.emb_utils import generate_emb_matrix\nfrom at_nlp.model_lib.cnn_models import TextCNN_Model, CNN_Model\nfrom at_nlp.model_lib.rnn_models import TextRCNN_Model, RNN_Model\n\n\nclass ModelGenerator(object):\n    def __init__(self,\n                 load_pretrain_emb=False,\n                 data_feature=None,\n                 meta_data_feature=None,\n                 fasttext_embeddings_index=None,\n                 multi_label=False):\n\n        self.data_feature = data_feature\n        self.load_pretrain_emb = load_pretrain_emb\n        self.meta_data_feature = meta_data_feature\n        self.oov_cnt = 0\n        self.embedding_matrix = None\n        self.use_bpe = False\n        self.multi_label = multi_label\n        self.lr = 0.001\n        self.cur_lr = 0.0\n        self.emb_size = 300\n        self.fasttext_embeddings_index = fasttext_embeddings_index\n\n        self.model_lib = {\'text_cnn\': TextCNN_Model,\n                          \'text_cnn_2d\': CNN_Model,\n                          \'text_rcnn\': TextRCNN_Model,\n                          \'text_rnn\': RNN_Model}\n\n        self.feature_lib = {\'char-level + 64dim-embedding\', \'char-level + 300dim-embedding\',\n                            \'word-level + pretrained embedding300dim\', \'word-level + 64dim-embedding\'}\n\n    def select_classifier(self, model_name, feature_mode, data_feature):\n        _feature = {}\n        if feature_mode == \'char-level + 64dim-embedding\':\n            _feature = {\'use_fasttext_emb\': False,\n                        \'emb_size\': 64}\n        elif feature_mode == \'char-level + 300dim-embedding\':\n            _feature = {\'use_fasttext_emb\': False,\n                        \'emb_size\': 300}\n        elif feature_mode == \'word-level + pretrained embedding300dim\':\n            _feature = {\'use_fasttext_emb\': True,\n                        \'emb_size\': 300}\n        elif feature_mode == \'word-level + 64dim-embedding\':\n            _feature = {\'use_fasttext_emb\': False,\n                        \'emb_size\': 64}\n\n        data_feature.update(_feature)\n        model = self.build_model(model_name, data_feature=data_feature)\n        return model\n\n    def _set_model_compile_params(self, optimizer_name, lr, metrics=[]):\n        optimizer = self._set_optimizer(optimizer_name=optimizer_name, lr=lr)\n        loss_fn = self._set_loss_fn()\n        print(loss_fn)\n        if metrics:\n            metrics = metrics\n        else:\n            metrics = [\'accuracy\']\n\n        return optimizer, loss_fn, metrics\n\n    def _set_model_train_params(self):\n        pass\n\n    def build_model(self, model_name, data_feature):\n        if model_name == \'svm\':\n            model = LinearSVC(random_state=0, tol=1e-5, max_iter=500)\n            self.model = CalibratedClassifierCV(model)\n            if self.multi_label:\n                info(""use OneVsRestClassifier"")\n                self.model = OneVsRestClassifier(self.model, n_jobs=-1)\n\n        else:\n            if data_feature[""use_fasttext_emb""]:\n                self.oov_cnt, self.embedding_matrix = self.generate_emb_matrix(\n                    num_features=data_feature[""num_features""],\n                    word_index=data_feature[""word_index""])\n            else:\n                self.embedding_matrix = None\n\n            self.emb_size = data_feature[""emb_size""]\n\n            kwargs = {\'embedding_matrix\': self.embedding_matrix,\n                      \'input_shape\': data_feature[\'input_shape\'],\n                      \'max_length\': data_feature[\'max_length\'],\n                      \'num_features\': data_feature[\'num_features\'],\n                      \'num_classes\': data_feature[\'num_class\'],\n                      ""filter_num"": data_feature[""filter_num""],\n                      ""trainable"": False,\n                      ""emb_size"": self.emb_size}\n            if self.multi_label:\n                kwargs[""use_multi_label""] = True\n\n            self.model = self.model_lib[model_name](**kwargs)\n            self._set_init_lr(model_name)\n            optimizer, loss_fn, metrics = self._set_model_compile_params(optimizer_name=\'RMSprop\',\n                                                                         lr=self.lr)\n            if self.multi_label:\n                loss_fn = \'binary_crossentropy\'\n\n            self.model.compile(loss=loss_fn, optimizer=optimizer, metrics=metrics)\n\n        return self.model\n\n    def _set_loss_fn(self):\n        loss_fn = \'categorical_crossentropy\'\n        return loss_fn\n\n    def _set_optimizer(self, optimizer_name, lr=0.001):\n        if optimizer_name == \'RAdam\':\n            opt = RAdam(learning_rate=lr)\n        elif optimizer_name == \'RMSprop\':\n            opt = RMSprop(lr=lr)\n        elif optimizer_name == ""Adam"":\n            opt = Adam(lr=lr)\n        return opt\n\n    def _set_init_lr(self, model_name):\n        if model_name == ""text_cnn"":\n            self.lr = 0.001\n        elif model_name == ""text_cnn_2d"":\n            self.lr = 0.016\n        elif model_name == ""text_rcnn"":\n            self.lr = 0.025\n        elif model_name == ""text_rnn"":\n            self.lr = 0.0035\n\n\n\n    def model_pre_select(self, model_name=""svm""):\n        self.model_name = model_name\n\n    def generate_emb_matrix(self, num_features, word_index):\n        return generate_emb_matrix(num_features=num_features, word_index=word_index,\n                                   fasttext_embeddings_index=self.fasttext_embeddings_index)\n'"
AutoDL_sample_code_submission/at_nlp/model_lib/__init__.py,0,b''
AutoDL_sample_code_submission/at_nlp/model_lib/cnn_models.py,0,"b""# -*- coding: utf-8 -*-\n# @Date    : 2020/3/3 11:23\nimport keras\nfrom keras.layers import Input, Dense\nfrom keras.layers import Embedding, Flatten, Conv1D, concatenate\nfrom keras.layers import MaxPooling1D, GlobalMaxPooling1D\nfrom keras.layers import Dropout, BatchNormalization\nfrom keras.layers import PReLU\n\nfrom at_nlp.model_lib.model_utils import _get_last_layer_units_and_activation\n\n\ndef TextCNN_Model(input_shape,\n                  embedding_matrix,\n                  max_length,\n                  num_features,\n                  num_classes,\n                  input_tensor=None,\n                  filter_num=64,\n                  emb_size=300,\n                  trainable=False,\n                  use_multi_label = False\n                  ):\n    op_units, op_activation = _get_last_layer_units_and_activation(num_classes, use_softmax=True)\n\n    in_text = Input(name='inputs', shape=[max_length], tensor=input_tensor)\n\n    if embedding_matrix is None:\n        layer = Embedding(input_dim=num_features,\n                          output_dim=emb_size,\n                          input_length=input_shape)(in_text)\n    else:\n        layer = Embedding(input_dim=num_features,\n                          output_dim=emb_size,\n                          input_length=input_shape,\n                          weights=[embedding_matrix],\n                          trainable=trainable)(in_text)\n\n    cnns = []\n    filter_sizes = [2, 3, 4, 5]\n    for size in filter_sizes:\n        cnn_l = Conv1D(filter_num,\n                       size,\n                       padding='same',\n                       strides=1,\n                       activation='relu')(layer)\n\n        pooling_0 = MaxPooling1D(max_length - size + 1)(cnn_l)\n        pooling_0 = Flatten()(pooling_0)\n        cnns.append(pooling_0)\n\n    cnn_merge = concatenate(cnns, axis=-1)\n    out = Dropout(0.2)(cnn_merge)\n    if use_multi_label:\n        main_output = Dense(op_units, activation='sigmoid')(out)\n    else:\n        main_output = Dense(op_units, activation=op_activation)(out)\n    md = keras.models.Model(inputs=in_text, outputs=main_output)\n    return md\n\n\ndef CNN_Model(max_length, num_classes, num_features, embedding_matrix=None,\n              trainable=False, input_shape=None,\n              input_tensor=None,\n              filter_num=64,\n              emb_size=300):\n    in_text = Input(shape=(max_length,))\n    op_units, op_activation = _get_last_layer_units_and_activation(num_classes, use_softmax=True)\n\n    trainable = True\n    if embedding_matrix is None:\n        x = Embedding(num_features, 64, trainable=trainable)(in_text)\n    else:\n        x = Embedding(num_features, 300, trainable=trainable, weights=[embedding_matrix])(in_text)\n\n    x = Conv1D(128, kernel_size=5, padding='valid', kernel_initializer='glorot_uniform')(x)\n    x = GlobalMaxPooling1D()(x)\n\n    x = Dense(128)(x)\n    x = PReLU()(x)\n    x = Dropout(0.35)(x)\n    x = BatchNormalization()(x)\n\n    y = Dense(op_units, activation=op_activation)(x)\n\n    md = keras.models.Model(inputs=[in_text], outputs=y)\n\n    return md\n"""
AutoDL_sample_code_submission/at_nlp/model_lib/model_utils.py,0,"b'# -*- coding: utf-8 -*-\n# @Date    : 2020/3/3 11:22\n\ndef _get_last_layer_units_and_activation(num_classes, use_softmax=True):\n    """"""Gets the # units and activation function for the last network layer.\n\n    Args:\n        num_classes: Number of classes.\n\n    Returns:\n        units, activation values.\n    """"""\n    if num_classes == 2 and not use_softmax:\n        activation = \'sigmoid\'\n        units = 1\n    else:\n        activation = \'softmax\'\n        units = num_classes\n    return units, activation\n'"
AutoDL_sample_code_submission/at_nlp/model_lib/rnn_models.py,0,"b'# -*- coding: utf-8 -*-\n# @Date    : 2020/3/3 11:32\nimport keras\nfrom keras.layers import Input, Dense\nfrom keras.layers import Embedding, Flatten,  Conv2D\nfrom keras.layers import GlobalMaxPooling1D, MaxPooling2D\nfrom keras.layers import Dropout, BatchNormalization, Concatenate, Reshape\nfrom keras.layers import PReLU\nfrom keras.layers import CuDNNGRU, Bidirectional\nfrom keras import backend as K\n\nfrom at_nlp.model_lib.model_utils import _get_last_layer_units_and_activation\n\n\ndef TextRCNN_Model(input_shape,\n                   embedding_matrix,\n                   max_length,\n                   num_features,\n                   num_classes,\n                   input_tensor=None,\n                   emb_size=300,\n                   filter_num=64,\n                   rnn_units=128,\n                   trainable=False):\n    inputs = Input(name=\'inputs\', shape=[max_length], tensor=input_tensor)\n    op_units, op_activation = _get_last_layer_units_and_activation(num_classes, use_softmax=True)\n\n    if embedding_matrix is None:\n        layer = Embedding(input_dim=num_features,\n                          output_dim=emb_size,\n                          input_length=input_shape)(inputs)\n    else:\n\n        layer = Embedding(input_dim=num_features,\n                          output_dim=emb_size,\n                          weights=[embedding_matrix],\n                          trainable=trainable)(inputs)\n\n    layer_cell = CuDNNGRU\n    embedding_output = layer\n    # \xe6\x8b\xbc\xe6\x8e\xa5\n    x_feb = Bidirectional(layer_cell(units=rnn_units,\n                                     return_sequences=True))(embedding_output)\n\n    x_feb = Concatenate(axis=2)([x_feb, embedding_output])\n\n    ####\xe4\xbd\xbf\xe7\x94\xa8\xe5\xa4\x9a\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8##################################################\n    x_feb = Dropout(rate=0.5)(x_feb)\n\n    dim_2 = K.int_shape(x_feb)[2]\n\n    len_max = max_length\n    x_feb_reshape = Reshape((len_max, dim_2, 1))(x_feb)\n    # \xe6\x8f\x90\xe5\x8f\x96n-gram\xe7\x89\xb9\xe5\xbe\x81\xe5\x92\x8c\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xef\xbc\x8c \xe4\xb8\x80\xe8\x88\xac\xe4\xb8\x8d\xe7\x94\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe6\xb1\xa0\xe5\x8c\x96\n    conv_pools = []\n    filters = [2, 3, 4, 5]\n\n    for filter_size in filters:\n        conv = Conv2D(filters=filter_num,\n                      kernel_size=(filter_size, dim_2),\n                      padding=\'valid\',\n                      kernel_initializer=\'normal\',\n                      activation=\'relu\',\n                      )(x_feb_reshape)\n\n        print(""check conv"", conv.get_shape())\n        pooled = MaxPooling2D(pool_size=(len_max - filter_size + 1, 1),\n                              strides=(1, 1),\n                              padding=\'valid\',\n                              )(conv)\n        print(""check pooled"", pooled.get_shape())\n        conv_pools.append(pooled)\n\n    # \xe6\x8b\xbc\xe6\x8e\xa5\n    x = Concatenate()(conv_pools)\n    x = Flatten()(x)\n    #########################################################################\n    output = Dense(op_units, activation=op_activation)(x)\n    md = keras.models.Model(inputs=inputs, outputs=output)\n    return md\n\n\ndef RNN_Model(max_length, num_classes, num_features, embedding_matrix=None,\n              trainable=False, input_shape=None,\n              input_tensor=None,\n              filter_num=64,\n              emb_size=300\n              ):\n    in_text = Input(shape=(max_length,))\n    op_units, op_activation = _get_last_layer_units_and_activation(num_classes, use_softmax=True)\n\n    trainable = True\n    if embedding_matrix is None:\n        x = Embedding(num_features, 64, trainable=trainable)(in_text)\n    else:\n        x = Embedding(num_features, 300, trainable=trainable, weights=[embedding_matrix])(in_text)\n\n    x = CuDNNGRU(128, return_sequences=True)(x)\n    x = GlobalMaxPooling1D()(x)\n\n    x = Dense(128)(x)  #\n    x = PReLU()(x)\n    x = Dropout(0.35)(x)  # 0\n    x = BatchNormalization()(x)\n\n    y = Dense(op_units, activation=op_activation)(x)\n\n    md = keras.models.Model(inputs=[in_text], outputs=y)\n\n    return md\n'"
AutoDL_sample_code_submission/at_nlp/model_manager/__init__.py,0,b''
AutoDL_sample_code_submission/at_nlp/model_manager/emb_utils.py,0,"b'# -*- coding: utf-8 -*-\n# @Date    : 2020/3/3 11:13\nimport os\nimport gzip\nimport numpy as np\n\nfrom log_utils import info\n\nEMBEDDING_DIM = 300\n\n\ndef _load_emb(language):\n    # loading pretrained embedding\n\n    FT_DIR = \'/app/embedding\'\n    fasttext_embeddings_index = {}\n    if language == \'ZH\':\n        f = gzip.open(os.path.join(FT_DIR, \'cc.zh.300.vec.gz\'), \'rb\')\n    elif language== \'EN\':\n        f = gzip.open(os.path.join(FT_DIR, \'cc.en.300.vec.gz\'), \'rb\')\n    else:\n        raise ValueError(\'Unexpected embedding path:\'\n                         \' {unexpected_embedding}. \'.format(\n            unexpected_embedding=FT_DIR))\n\n    for line in f.readlines():\n        values = line.strip().split()\n        if language== \'ZH\':\n            word = values[0].decode(\'utf8\')\n        else:\n            word = values[0].decode(\'utf8\')\n        coefs = np.asarray(values[1:], dtype=\'float32\')\n        fasttext_embeddings_index[word] = coefs\n\n    info(\'Found %s fastText word vectors.\' %\n         len(fasttext_embeddings_index))\n    return fasttext_embeddings_index\n\ndef generate_emb_matrix(num_features, word_index, fasttext_embeddings_index):\n    cnt = 0\n    embedding_matrix = np.zeros((num_features, EMBEDDING_DIM))\n    for word, i in word_index.items():\n        if i >= num_features:\n            continue\n        embedding_vector = fasttext_embeddings_index.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n        else:\n            embedding_matrix[i] = np.random.uniform(\n                -0.02, 0.02, size=EMBEDDING_DIM)\n            cnt += 1\n    print(""check self embedding_vector "", embedding_matrix.shape)\n    oov_cnt = cnt\n\n    print(\'fastText oov words: %s\' % cnt)\n    return oov_cnt, embedding_matrix'"
AutoDL_sample_code_submission/at_speech/backbones/__init__.py,0,b''
AutoDL_sample_code_submission/at_speech/backbones/thinresnet34.py,0,"b'from __future__ import print_function\nfrom __future__ import absolute_import\nimport keras\nimport tensorflow as tf\nimport keras.backend as K\n\nimport at_speech.backbones.tr34_bb as backbone\nweight_decay = 1e-3\n\n\nclass ModelMGPU(keras.Model):\n    def __init__(self, ser_model, gpus):\n        pmodel = keras.utils.multi_gpu_model(ser_model, gpus)\n        self.__dict__.update(pmodel.__dict__)\n        self._smodel = ser_model\n\n    def __getattribute__(self, attrname):\n        \'\'\'Override load and save methods to be used from the serial-model. The\n        serial-model holds references to the weights in the multi-gpu model.\n        \'\'\'\n        # return Model.__getattribute__(self, attrname)\n        if \'load\' in attrname or \'save\' in attrname:\n            return getattr(self._smodel, attrname)\n\n        return super(ModelMGPU, self).__getattribute__(attrname)\n\n\nclass VladPooling(keras.engine.Layer):\n    def __init__(self, mode, k_centers, g_centers=0, **kwargs):\n        self.k_centers = k_centers\n        self.g_centers = g_centers\n        self.mode = mode\n        super(VladPooling, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.cluster = self.add_weight(shape=[self.k_centers+self.g_centers, input_shape[0][-1]],\n                                       name=\'centers\',\n                                       initializer=\'orthogonal\')\n        self.built = True\n\n    def compute_output_shape(self, input_shape):\n        assert input_shape\n        return (input_shape[0][0], self.k_centers*input_shape[0][-1])\n\n    def call(self, x):\n        feat, cluster_score = x\n        num_features = feat.shape[-1]\n        max_cluster_score = K.max(cluster_score, -1, keepdims=True)\n        exp_cluster_score = K.exp(cluster_score - max_cluster_score)\n        A = exp_cluster_score / K.sum(exp_cluster_score, axis=-1, keepdims = True)\n        A = K.expand_dims(A, -1)\n        feat_broadcast = K.expand_dims(feat, -2)\n        feat_res = feat_broadcast - self.cluster\n        weighted_res = tf.multiply(A, feat_res)\n        cluster_res = K.sum(weighted_res, [1, 2])\n\n        if self.mode == \'gvlad\':\n            cluster_res = cluster_res[:, :self.k_centers, :]\n\n        cluster_l2 = K.l2_normalize(cluster_res, -1)\n        outputs = K.reshape(cluster_l2, [-1, int(self.k_centers) * int(num_features)])\n        return outputs\n\n\ndef amsoftmax_loss(y_true, y_pred, scale=30, margin=0.35):\n    y_pred = y_true * (y_pred - margin) + (1 - y_true) * y_pred\n    y_pred *= scale\n    return K.categorical_crossentropy(y_true, y_pred, from_logits=True)\n\n\ndef vggvox_resnet2d_icassp(input_dim=(257, 250, 1), num_class=8631, mode=\'train\', config=None, net=None):\n    if not net:\n        net=config[\'net\']\n    loss=config[\'loss\']\n    vlad_clusters=config[\'vlad_cluster\']\n    ghost_clusters=config[\'ghost_cluster\']\n    bottleneck_dim=config[\'bottleneck_dim\']\n    aggregation = config[\'aggregation_mode\']\n    backbone_net = backbone.choose_net(net)\n    inputs, x = backbone_net(input_dim=input_dim, mode=mode)\n    x_fc = keras.layers.Conv2D(bottleneck_dim, (7, 1),\n                               strides=(1, 1),\n                               activation=\'relu\',\n                               kernel_initializer=\'orthogonal\',\n                               use_bias=True, trainable=True,\n                               kernel_regularizer=keras.regularizers.l2(weight_decay),\n                               bias_regularizer=keras.regularizers.l2(weight_decay),\n                               name=\'x_fc\')(x)\n\n    if aggregation == \'avg\':\n        if mode == \'train\':\n            x = keras.layers.AveragePooling2D((1, 5), strides=(1, 1), name=\'avg_pool\')(x)\n            x = keras.layers.Reshape((-1, bottleneck_dim))(x)\n        else:\n            x = keras.layers.GlobalAveragePooling2D(name=\'avg_pool\')(x)\n            x = keras.layers.Reshape((1, bottleneck_dim))(x)\n\n    elif aggregation == \'vlad\':\n        x_k_center = keras.layers.Conv2D(vlad_clusters, (7, 1),\n                                         strides=(1, 1),\n                                         kernel_initializer=\'orthogonal\',\n                                         use_bias=True, trainable=True,\n                                         kernel_regularizer=keras.regularizers.l2(weight_decay),\n                                         bias_regularizer=keras.regularizers.l2(weight_decay),\n                                         name=\'vlad_center_assignment\')(x)\n        x = VladPooling(k_centers=vlad_clusters, mode=\'vlad\', name=\'vlad_pool\')([x_fc, x_k_center])\n\n    elif aggregation == \'gvlad\':\n        x_k_center = keras.layers.Conv2D(vlad_clusters+ghost_clusters, (7, 1),\n                                         strides=(1, 1),\n                                         kernel_initializer=\'orthogonal\',\n                                         use_bias=True, trainable=True,\n                                         kernel_regularizer=keras.regularizers.l2(weight_decay),\n                                         bias_regularizer=keras.regularizers.l2(weight_decay),\n                                         name=\'gvlad_center_assignment\')(x)\n        x = VladPooling(k_centers=vlad_clusters, g_centers=ghost_clusters, mode=\'gvlad\', name=\'gvlad_pool\')([x_fc, x_k_center])\n\n    else:\n        raise IOError(\'==> unknown aggregation mode\')\n\n    x = keras.layers.Dense(bottleneck_dim, activation=\'relu\',\n                           kernel_initializer=\'orthogonal\',\n                           use_bias=True, trainable=True,\n                           kernel_regularizer=keras.regularizers.l2(weight_decay),\n                           bias_regularizer=keras.regularizers.l2(weight_decay),\n                           name=\'fc6\')(x)\n\n    if loss == \'softmax\':\n        y = keras.layers.Dense(num_class, activation=\'softmax\',\n                               kernel_initializer=\'orthogonal\',\n                               use_bias=False, trainable=True,\n                               kernel_regularizer=keras.regularizers.l2(weight_decay),\n                               bias_regularizer=keras.regularizers.l2(weight_decay),\n                               name=\'prediction\')(x)\n        trnloss = \'categorical_crossentropy\'\n\n    elif loss == \'amsoftmax\':\n        x_l2 = keras.layers.Lambda(lambda x: K.l2_normalize(x, 1))(x)\n        y = keras.layers.Dense(num_class,\n                               kernel_initializer=\'orthogonal\',\n                               use_bias=False, trainable=True,\n                               kernel_constraint=keras.constraints.unit_norm(),\n                               kernel_regularizer=keras.regularizers.l2(weight_decay),\n                               bias_regularizer=keras.regularizers.l2(weight_decay),\n                               name=\'prediction\')(x_l2)\n        trnloss = amsoftmax_loss\n\n    else:\n        raise IOError(\'==> unknown loss.\')\n\n    if mode == \'pretrain\':\n        model = keras.models.Model(inputs, x, name=\'vggvox_resnet2D_{}_{}\'.format(loss, aggregation))\n    if mode == \'pred\':\n        model = keras.models.Model(inputs, y, name=\'vggvox_resnet2D_{}_{}\'.format(loss, aggregation))\n    return model\n\n\ndef build_tr34_model(net_name, input_dim, num_class, tr34_bb_config):\n    return vggvox_resnet2d_icassp(\n        input_dim=input_dim,\n        num_class=num_class,\n        mode=""pretrain"",\n        config=tr34_bb_config,\n        net=net_name,\n    )\n'"
AutoDL_sample_code_submission/at_speech/backbones/tr34_bb.py,0,"b""from __future__ import print_function\nfrom __future__ import absolute_import\n\nfrom keras import layers\nfrom keras.regularizers import l2\nfrom keras.layers import Activation, Conv1D, Conv2D, Input, Lambda\nfrom keras.layers import BatchNormalization, Flatten, Dense, Reshape\nfrom keras.layers import MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n\nweight_decay = 1e-3\n\n\ndef identity_block_2D(input_tensor, kernel_size, filters, stage, block, trainable=True):\n    filters1, filters2, filters3 = filters\n    bn_axis = 3\n    conv_name_1 = 'conv' + str(stage) + '_' + str(block) + '_1x1_reduce'\n    bn_name_1 = 'conv' + str(stage) + '_' + str(block) + '_1x1_reduce/bn'\n    x = Conv2D(filters1, (1, 1),\n               kernel_initializer='orthogonal',\n               use_bias=False,\n               trainable=trainable,\n               kernel_regularizer=l2(weight_decay),\n               name=conv_name_1)(input_tensor)\n    x = BatchNormalization(axis=bn_axis, trainable=trainable, name=bn_name_1)(x)\n    x = Activation('relu')(x)\n    conv_name_2 = 'conv' + str(stage) + '_' + str(block) + '_3x3'\n    bn_name_2 = 'conv' + str(stage) + '_' + str(block) + '_3x3/bn'\n    x = Conv2D(filters2, kernel_size,\n               padding='same',\n               kernel_initializer='orthogonal',\n               use_bias=False,\n               trainable=trainable,\n               kernel_regularizer=l2(weight_decay),\n               name=conv_name_2)(x)\n    x = BatchNormalization(axis=bn_axis, trainable=trainable, name=bn_name_2)(x)\n    x = Activation('relu')(x)\n    conv_name_3 = 'conv' + str(stage) + '_' + str(block) + '_1x1_increase'\n    bn_name_3 = 'conv' + str(stage) + '_' + str(block) + '_1x1_increase/bn'\n    x = Conv2D(filters3, (1, 1),\n               kernel_initializer='orthogonal',\n               use_bias=False,\n               trainable=trainable,\n               kernel_regularizer=l2(weight_decay),\n               name=conv_name_3)(x)\n    x = BatchNormalization(axis=bn_axis, trainable=trainable, name=bn_name_3)(x)\n    x = layers.add([x, input_tensor])\n    x = Activation('relu')(x)\n    return x\n\n\ndef conv_block_2D(input_tensor, kernel_size, filters, stage, block, strides=(2, 2), trainable=True):\n    filters1, filters2, filters3 = filters\n    bn_axis = 3\n    conv_name_1 = 'conv' + str(stage) + '_' + str(block) + '_1x1_reduce'\n    bn_name_1 = 'conv' + str(stage) + '_' + str(block) + '_1x1_reduce/bn'\n    x = Conv2D(filters1, (1, 1),\n               strides=strides,\n               kernel_initializer='orthogonal',\n               use_bias=False,\n               trainable=trainable,\n               kernel_regularizer=l2(weight_decay),\n               name=conv_name_1)(input_tensor)\n    x = BatchNormalization(axis=bn_axis, trainable=trainable, name=bn_name_1)(x)\n    x = Activation('relu')(x)\n    conv_name_2 = 'conv' + str(stage) + '_' + str(block) + '_3x3'\n    bn_name_2 = 'conv' + str(stage) + '_' + str(block) + '_3x3/bn'\n    x = Conv2D(filters2, kernel_size, padding='same',\n               kernel_initializer='orthogonal',\n               use_bias=False,\n               trainable=trainable,\n               kernel_regularizer=l2(weight_decay),\n               name=conv_name_2)(x)\n    x = BatchNormalization(axis=bn_axis, trainable=trainable, name=bn_name_2)(x)\n    x = Activation('relu')(x)\n    conv_name_3 = 'conv' + str(stage) + '_' + str(block) + '_1x1_increase'\n    bn_name_3 = 'conv' + str(stage) + '_' + str(block) + '_1x1_increase/bn'\n    x = Conv2D(filters3, (1, 1),\n               kernel_initializer='orthogonal',\n               use_bias=False,\n               trainable=trainable,\n               kernel_regularizer=l2(weight_decay),\n               name=conv_name_3)(x)\n    x = BatchNormalization(axis=bn_axis, trainable=trainable, name=bn_name_3)(x)\n    conv_name_4 = 'conv' + str(stage) + '_' + str(block) + '_1x1_proj'\n    bn_name_4 = 'conv' + str(stage) + '_' + str(block) + '_1x1_proj/bn'\n    shortcut = Conv2D(filters3, (1, 1), strides=strides,\n                      kernel_initializer='orthogonal',\n                      use_bias=False,\n                      trainable=trainable,\n                      kernel_regularizer=l2(weight_decay),\n                      name=conv_name_4)(input_tensor)\n    shortcut = BatchNormalization(axis=bn_axis, trainable=trainable, name=bn_name_4)(shortcut)\n    x = layers.add([x, shortcut])\n    x = Activation('relu')(x)\n    return x\n\n\ndef resnet_2D_v1(input_dim, mode='train'):\n    bn_axis = 3\n    if mode == 'train':\n        inputs = Input(shape=input_dim, name='input')\n    else:\n        inputs = Input(shape=(input_dim[0], None, input_dim[-1]), name='input')\n    x1 = Conv2D(64, (7, 7),\n                kernel_initializer='orthogonal',\n                use_bias=False, trainable=True,\n                kernel_regularizer=l2(weight_decay),\n                padding='same',\n                name='conv1_1/3x3_s1')(inputs)\n    x1 = BatchNormalization(axis=bn_axis, name='conv1_1/3x3_s1/bn', trainable=True)(x1)\n    x1 = Activation('relu')(x1)\n    x1 = MaxPooling2D((2, 2), strides=(2, 2))(x1)\n    x2 = conv_block_2D(x1, 3, [48, 48, 96], stage=2, block='a', strides=(1, 1), trainable=True)\n    x2 = identity_block_2D(x2, 3, [48, 48, 96], stage=2, block='b', trainable=True)\n    x3 = conv_block_2D(x2, 3, [96, 96, 128], stage=3, block='a', trainable=True)\n    x3 = identity_block_2D(x3, 3, [96, 96, 128], stage=3, block='b', trainable=True)\n    x3 = identity_block_2D(x3, 3, [96, 96, 128], stage=3, block='c', trainable=True)\n    x4 = conv_block_2D(x3, 3, [128, 128, 256], stage=4, block='a', trainable=True)\n    x4 = identity_block_2D(x4, 3, [128, 128, 256], stage=4, block='b', trainable=True)\n    x4 = identity_block_2D(x4, 3, [128, 128, 256], stage=4, block='c', trainable=True)\n    x5 = conv_block_2D(x4, 3, [256, 256, 512], stage=5, block='a', trainable=True)\n    x5 = identity_block_2D(x5, 3, [256, 256, 512], stage=5, block='b', trainable=True)\n    x5 = identity_block_2D(x5, 3, [256, 256, 512], stage=5, block='c', trainable=True)\n    y = MaxPooling2D((3, 1), strides=(2, 1), name='mpool2')(x5)\n    return inputs, y\n\n\ndef resnet_2D_v2(input_dim, mode='train'):\n    bn_axis = 3\n    if mode == 'train':\n        inputs = Input(shape=input_dim, name='input')\n    else:\n        inputs = Input(shape=(input_dim[0], None, input_dim[-1]), name='input')\n    x1 = Conv2D(64, (7, 7), strides=(2, 2),\n                kernel_initializer='orthogonal',\n                use_bias=False, trainable=True,\n                kernel_regularizer=l2(weight_decay),\n                padding='same',\n                name='conv1_1/3x3_s1')(inputs)\n    x1 = BatchNormalization(axis=bn_axis, name='conv1_1/3x3_s1/bn', trainable=True)(x1)\n    x1 = Activation('relu')(x1)\n    x1 = MaxPooling2D((2, 2), strides=(2, 2))(x1)\n    x2 = conv_block_2D(x1, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), trainable=True)\n    x2 = identity_block_2D(x2, 3, [64, 64, 256], stage=2, block='b', trainable=True)\n    x2 = identity_block_2D(x2, 3, [64, 64, 256], stage=2, block='c', trainable=True)\n    x3 = conv_block_2D(x2, 3, [128, 128, 512], stage=3, block='a', trainable=True)\n    x3 = identity_block_2D(x3, 3, [128, 128, 512], stage=3, block='b', trainable=True)\n    x3 = identity_block_2D(x3, 3, [128, 128, 512], stage=3, block='c', trainable=True)\n    x4 = conv_block_2D(x3, 3, [256, 256, 1024], stage=4, block='a', strides=(1, 1), trainable=True)\n    x4 = identity_block_2D(x4, 3, [256, 256, 1024], stage=4, block='b', trainable=True)\n    x4 = identity_block_2D(x4, 3, [256, 256, 1024], stage=4, block='c', trainable=True)\n    x5 = conv_block_2D(x4, 3, [512, 512, 2048], stage=5, block='a', trainable=True)\n    x5 = identity_block_2D(x5, 3, [512, 512, 2048], stage=5, block='b', trainable=True)\n    x5 = identity_block_2D(x5, 3, [512, 512, 2048], stage=5, block='c', trainable=True)\n    y = MaxPooling2D((3, 1), strides=(2, 1), name='mpool2')(x5)\n    return inputs, y\n\n\ndef choose_net(token='resnet34s'):\n    token_backbone = {\n        'resnet34s': resnet_2D_v1,\n        'default': resnet_2D_v2\n    }\n    if token in token_backbone:\n        return token_backbone[token]\n    else:\n        return token_backbone['default']"""
AutoDL_sample_code_submission/at_speech/classifier/__init__.py,0,"b'from at_speech.classifier.sklearn_lr import SLLRLiblinear, SLLRSag\nfrom at_speech.classifier.thinresnet34_cls import ThinResnet34Classifier'"
AutoDL_sample_code_submission/at_speech/classifier/sklearn_lr.py,0,"b'from __future__ import absolute_import\nimport numpy as np\n\nfrom sklearn.linear_model import logistic, SGDClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom at_toolkit.interface.adl_classifier import AdlOfflineClassifier, AdlOnlineClassifier\n# from at_toolkit import info, error, as_timer\n\n\ndef ohe2cat(label):\n    return np.argmax(label, axis=1)\n\n\nclass SLLRLiblinear(AdlOfflineClassifier):\n\n    def init(self, class_num: int, init_params: dict = None):\n        self.clf_name = ""sl_lr_liblinear""\n        self.class_num = class_num\n        self.model = logistic.LogisticRegression(solver=""liblinear"")\n        self.ml_mode = 2\n        self.ml_models = [OneVsRestClassifier(logistic.LogisticRegression(solver=""liblinear"")) for i in range(class_num)]\n        self.ml_model = OneVsRestClassifier(logistic.LogisticRegression(solver=""liblinear""))\n        self.logReg_pipeline = Pipeline([(\'clf\', OneVsRestClassifier(logistic.LogisticRegression(solver=\'liblinear\'), n_jobs=-1)),])\n\n    def offline_fit(self, train_examples_x: np.ndarray, train_examples_y: np.ndarray, fit_params: dict = None):\n        if fit_params.get(""if_multilabel"") is False:\n            train_examples_y = ohe2cat(train_examples_y)\n            self.model.fit(train_examples_x, train_examples_y)\n            self.label_map = self.model.classes_\n        else:\n            if self.ml_mode == 1:\n                for cls in range(self.class_num):\n                    cls_y = train_examples_y[:, cls]\n                    self.ml_models[cls].fit(train_examples_x, cls_y)\n\n            elif self.ml_mode == 2:\n               self.ml_model.fit(train_examples_x, train_examples_y)\n\n            elif self.ml_mode == 3:\n                for cls in range(self.class_num):\n                    cls_y = train_examples_y[:, cls]\n                    self.logReg_pipeline.fit(train_examples_x, cls_y)\n\n            else:\n                pass\n\n    def predict_proba(self, test_examples: np.ndarray, predict_prob_params: dict = None) -> np.ndarray:\n        if predict_prob_params.get(""if_multilabel"") is True:\n            return self.predict_proba_multilabel(test_examples)\n\n        else:\n            raw_pred_probas = self.model.predict_proba(test_examples)\n            if len(self.label_map) < self.class_num:\n                rebuilt_pred_proba = self.rebuild_prob_res(self.label_map, raw_pred_probas)\n                return rebuilt_pred_proba\n            else:\n                return raw_pred_probas\n\n    def predict_proba_multilabel(self, test_examples: np.ndarray):\n        if self.ml_mode == 1:\n            all_preds = []\n            for cls in range(self.class_num):\n                preds = self.ml_models[cls].predict_proba(test_examples)\n                all_preds.append(preds[:, 1])\n\n            preds = np.stack(all_preds, axis=1)\n\n        elif self.ml_mode == 2:\n            preds = self.ml_model.predict_proba(test_examples)\n\n        elif self.ml_mode == 3:\n            all_preds = []\n            for cls in range(self.class_num):\n                preds = self.logReg_pipeline.predict_proba(test_examples)\n                all_preds.append(preds[:, 1])\n\n            preds = np.stack(all_preds, axis=1)\n\n        else:\n            preds = self.ml_model.predict_proba(test_examples)\n\n        return preds\n\n\nclass SLLRSag(AdlOfflineClassifier):\n\n    def init(self, class_num, init_params: dict):\n        self.clf_name = ""sl_lr_sag""\n        self.class_num = class_num\n        self.max_iter = init_params.get(""max_iter"")\n        self.model = logistic.LogisticRegression(C=1.0, max_iter=self.max_iter, solver=""sag"", multi_class=""auto"")\n\n        self.ml_model = OneVsRestClassifier(logistic.LogisticRegression(solver=""liblinear""))\n\n    def offline_fit(self, train_examples_x: np.ndarray, train_examples_y: np.ndarray, fit_params: dict = None):\n        if fit_params.get(""if_multilabel"") is False:\n            train_examples_y = ohe2cat(train_examples_y)\n            self.model.fit(train_examples_x, train_examples_y)\n            self.label_map = self.model.classes_\n\n        else:\n            self.ml_model.fit(train_examples_x, train_examples_y)\n\n    def predict_proba(self, test_examples: np.ndarray, predict_prob_params: dict = None) -> np.ndarray:\n        if predict_prob_params.get(""if_multilabel"") is True:\n            return self.predict_proba_multilabel(test_examples)\n\n        else:\n            raw_pred_probas = self.model.predict_proba(test_examples)\n            if len(self.label_map) < self.class_num:\n                rebuilt_pred_proba = self.rebuild_prob_res(self.label_map, raw_pred_probas)\n                return rebuilt_pred_proba\n            else:\n                return raw_pred_probas\n\n\n    def predict_proba_multilabel(self, test_examples: np.ndarray, predict_prob_params: dict = None) -> np.ndarray:\n        preds = self.ml_model.predict_proba(test_examples)\n        return preds\n\n\nclass MLLRLiblinear(AdlOfflineClassifier):\n    def init(self, class_num: int, init_params: dict):\n        self.clf_name = ""ml_sl_lr_liblinear""\n        self.class_num = class_num\n        self.model = logistic.LogisticRegression(solver=""liblinear"")\n\n    def offline_fit(self, train_examples_x: np.ndarray, train_examples_y: np.ndarray, fit_params:dict):\n        pass\n\n    def predict_proba(self, test_examples: np.ndarray, predict_prob_params: dict) -> np.ndarray:\n        pass\n\n\ndef main():\n    class_num = 100\n    lr_libl_cls_init_params = {}\n    lr_sag_cls_init_params = {""max_iter"": 30}  # 50/100\n    lr_libl_cls = SLLRLiblinear()\n    lr_libl_cls.init(class_num)\n\n    lr_sag_cls = SLLRSag()\n    lr_sag_cls.init(class_num, lr_sag_cls_init_params)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
AutoDL_sample_code_submission/at_speech/classifier/thinresnet34_cls.py,0,"b'from __future__ import absolute_import\nimport os\nimport numpy as np\nimport keras\nfrom keras import backend as K\nfrom keras.callbacks import Callback, EarlyStopping, LearningRateScheduler\n\nfrom at_toolkit.interface.adl_classifier import AdlOfflineClassifier, AdlOnlineClassifier\n# from at_toolkit import info, error, as_timer, SPEECH_TR34_PT_MODEL_PATH\nfrom at_speech.backbones.thinresnet34 import build_tr34_model\nfrom at_speech.at_speech_cons import TR34_PRETRAIN_PATH\nfrom at_speech.at_speech_config import ThinRes34Config\nfrom at_speech.data_space.examples_gen_maker import DataGenerator as Tr34DataGenerator\n\n\nIF_TR34_MODELSUMMARY = True\n\n\nTR34_BB_CONFIG = {\n            ""gpu"": 1,\n            ""multiprocess"": 4,\n            ""net"": ""resnet34s"",\n            ""ghost_cluster"": 2,\n            ""vlad_cluster"": 8,\n            ""bottleneck_dim"": 512,\n            ""aggregation_mode"": ""gvlad"",\n            ""warmup_ratio"": 0.1,\n            ""loss"": ""softmax"",\n            ""optimizer"": ""adam"",\n            ""ohem_level"": 0,\n        }\n\n\ndef set_mp(processes=4):\n    import multiprocessing as mp\n\n    def init_worker():\n        import signal\n        signal.signal(signal.SIGINT, signal.SIG_IGN)\n\n    global pool\n    try:\n        pool.terminate()\n    except BaseException:\n        pass\n\n    if processes:\n        pool = mp.Pool(processes=processes, initializer=init_worker)\n    else:\n        pool = None\n    return pool\n\n\nclass TerminateOnBaseline(Callback):\n    def __init__(self, monitor=""acc"", baseline=0.9):\n        super(TerminateOnBaseline, self).__init__()\n        self.monitor = monitor\n        self.baseline = baseline\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        acc = logs.get(self.monitor)\n        if acc is not None:\n            if acc >= self.baseline:\n                print(""Epoch %d: Reached baseline, terminating training"" % (epoch))\n                self.model.stop_training = True\n\n\nclass ThinResnet34Classifier(AdlOnlineClassifier):\n    def __init__(self):\n        self.tr34_mconfig = ThinRes34Config()\n        self.batch_size = self.set_batch_size()\n\n        self.tr34_cls_params = {\n            ""dim"": (257, 250, 1),\n            ""mp_pooler"": set_mp(processes=4),\n            ""nfft"": 512,\n            ""spec_len"": 250,\n            ""win_length"": 400,\n            ""hop_length"": 160,\n            ""n_classes"": None,\n            ""batch_size"": self.batch_size,\n            ""shuffle"": True,\n            ""normalize"": True,\n        }\n\n        self.spec_len_status = 0\n        self.round_spec_len = self.tr34_mconfig.FE_RS_SPEC_LEN_CONFIG\n\n        self.round_idx = 1\n        self.test_idx = 1\n        self.g_accept_cur_list = list()\n        self.model, self.callbacks = None, None\n\n        self.last_y_pred = None\n        self.last_y_pred_round = 0\n\n        self.ml_model = None\n        self.is_multilabel = False\n\n\n    def set_batch_size(self):\n        bs = 32\n        bs = min(bs, self.tr34_mconfig.MAX_BATCHSIZE)\n        return bs\n\n    def step_decay(self, epoch):\n        epoch = self.round_idx - 1\n        stage1, stage2, stage3 = 10, 20, 40\n\n        if epoch < self.tr34_mconfig.FULL_VAL_R_START:\n            lr = self.tr34_mconfig.TR34_INIT_LR\n        if epoch == self.tr34_mconfig.FULL_VAL_R_START:\n            self.cur_lr = self.tr34_mconfig.STEP_DE_LR\n            lr = self.cur_lr\n        if epoch > self.tr34_mconfig.FULL_VAL_R_START:\n            if self.g_accept_cur_list[-10:].count(False) == 10:\n                self.cur_lr = self.tr34_mconfig.MAX_LR\n            if self.g_accept_cur_list[-10:].count(True) >= 2:\n                self.cur_lr = self.cur_lr * 1.05\n            self.cur_lr = max(1e-4 * 3, self.cur_lr)\n            self.cur_lr = min(1e-3 * 1.5, self.cur_lr)\n            lr = self.cur_lr\n        return np.float(lr)\n\n    def tr34_model_init(self, class_num):\n        self.tr34_cls_params[""n_classes""] = class_num\n        model_34 = build_tr34_model(\n            net_name=\'resnet34s\',\n            input_dim=self.tr34_cls_params[""dim""],\n            num_class=self.tr34_cls_params[""n_classes""],\n            tr34_bb_config=TR34_BB_CONFIG\n        )\n\n        model = model_34\n        if TR34_PRETRAIN_PATH:\n            if os.path.isfile(TR34_PRETRAIN_PATH):\n                model.load_weights(TR34_PRETRAIN_PATH, by_name=True, skip_mismatch=True)\n                if self.tr34_cls_params[""n_classes""] >= self.tr34_mconfig.CLASS_NUM_THS:\n                    frz_layer_num = self.tr34_mconfig.INIT_BRZ_L_NUM\n                else:\n                    frz_layer_num = self.tr34_mconfig.INIT_BRZ_L_NUM_WILD\n                for layer in model.layers[: frz_layer_num]:\n                    layer.trainable = False\n\n            else:\n                pass\n\n            pretrain_output = model.output\n            weight_decay = self.tr34_mconfig.TR34_INIT_WD\n\n            y = keras.layers.Dense(\n                self.tr34_cls_params[""n_classes""],\n                activation=""softmax"",\n                kernel_initializer=""orthogonal"",\n                use_bias=False,\n                trainable=True,\n                kernel_regularizer=keras.regularizers.l2(weight_decay),\n                bias_regularizer=keras.regularizers.l2(weight_decay),\n                name=""prediction"",\n            )(pretrain_output)\n            model = keras.models.Model(model.input, y, name=""vggvox_resnet2D_{}_{}_new"".format(""softmax"", ""gvlad""))\n            opt = keras.optimizers.Adam(lr=1e-3)\n            model.compile(optimizer=opt, loss=""categorical_crossentropy"", metrics=[""acc""])\n\n            ml_y = keras.layers.Dense(\n                self.tr34_cls_params[""n_classes""],\n                activation=""sigmoid"",\n                kernel_initializer=""orthogonal"",\n                use_bias=False,\n                trainable=True,\n                kernel_regularizer=keras.regularizers.l2(weight_decay),\n                bias_regularizer=keras.regularizers.l2(weight_decay),\n                name=""prediction"",\n            )(pretrain_output)\n            self.ml_model = keras.models.Model(model.input, ml_y, name=""vggvox_resnet2D_{}_{}_new"".format(""sigmoid"", ""gvlad""))\n            ml_opt = keras.optimizers.Adam(lr=1e-3)\n            self.ml_model.compile(optimizer=ml_opt, loss=""binary_crossentropy"", metrics=[""acc""])\n\n\n        if IF_TR34_MODELSUMMARY:\n            model.summary()\n            self.ml_model.summary()\n\n        callbacks = list()\n        if self.tr34_mconfig.ENABLE_CB_ES:\n            early_stopping = EarlyStopping(monitor=""val_loss"", patience=15)\n            callbacks.append(early_stopping)\n        if self.tr34_mconfig.ENABLE_CB_LRS:\n            normal_lr = LearningRateScheduler(self.step_decay)\n            callbacks.append(normal_lr)\n        return model, callbacks\n\n\n    def init(self, class_num: int, init_params: dict=None):\n        self.class_num = class_num\n        self.model, self.callbacks = self.tr34_model_init(class_num)\n        self.choose_round_spec_len()\n\n    def try_to_update_spec_len(self):\n        if self.round_idx in self.round_spec_len:\n            train_spec_len, test_spec_len, suggest_lr = self.round_spec_len[self.round_idx]\n            self.update_spec_len(train_spec_len, test_spec_len)\n\n    def choose_round_spec_len(self):\n        if self.class_num >= 37:\n            self.round_spec_len = self.tr34_mconfig.FE_RS_SPEC_LEN_CONFIG_AGGR\n        else:\n            self.round_spec_len = self.tr34_mconfig.FE_RS_SPEC_LEN_CONFIG_MILD\n        return\n\n    def update_spec_len(self, train_spec_len, test_spec_len):\n        self.imp_feat_args = {\n            ""train_spec_len"": train_spec_len,\n            ""test_spec_len"": test_spec_len,\n            ""train_wav_len"": train_spec_len * 160,\n            ""test_wav_len"": test_spec_len * 160,\n            ""mode"": ""train"",\n        }\n        self.spec_len_status = 1\n        return True\n\n    def decide_if_renew_trainfeats(self):\n        # must setup mode.\n        self.try_to_update_spec_len()\n        self.imp_feat_args[""mode""] = ""train""\n        if self.spec_len_status == 1:\n            self.spec_len_status = 2\n            return True\n\n    def decide_if_renew_valfeats(self):\n        if self.test_idx == 1 or self.spec_len_status == 2:\n            self.spec_len_status = 0\n            self.imp_feat_args[""mode""] = ""test""\n            return True\n        else:\n            return False\n\n    def decide_if_renew_testfeats(self):\n        if self.test_idx == 1 or self.spec_len_status == 2:\n            self.spec_len_status = 0\n            self.imp_feat_args[""mode""] = ""test""\n            return True\n        else:\n            return False\n\n    def decide_epoch_curround(self, first_epoch=14, left_epoch=1):\n        if self.round_idx == 1:\n            cur_epoch_num = first_epoch\n        else:\n            cur_epoch_num = left_epoch\n        return cur_epoch_num\n\n    def decide_stepperepoch_curround(self, cur_train_len):\n        if self.round_idx == 1:\n            return max(1, int(cur_train_len // self.tr34_cls_params[""batch_size""] // 2))\n        else:\n            return max(1, int(cur_train_len // self.tr34_cls_params[""batch_size""]))\n\n    def renew_if_multilabel(self, is_multilabel=False):\n        self.is_multilabel = is_multilabel\n        if self.is_multilabel is False:\n            pass\n        else:\n            self.model = self.ml_model\n\n    def online_fit(self, train_examples_x: np.ndarray, train_examples_y: np.ndarray, fit_params:dict):\n\n        self.trn_gen = Tr34DataGenerator(train_examples_x, train_examples_y, **self.tr34_cls_params)\n        cur_train_len = len(train_examples_x)\n        self.first_r_data_generator = self.trn_gen\n        cur_epoch = self.decide_epoch_curround(fit_params.get(""first_epoch"", 14), fit_params.get(""left_epoch"", 1))\n        early_stopping = TerminateOnBaseline(monitor=""acc"", baseline=0.999)\n        cur_fit_history = self.model.fit_generator(\n            self.first_r_data_generator,\n            steps_per_epoch=self.decide_stepperepoch_curround(cur_train_len),\n            validation_data=fit_params.get(""valid_data""), #todo: put in.\n            epochs=cur_epoch,\n            max_queue_size=10,\n            callbacks=self.callbacks + [early_stopping],\n            use_multiprocessing=False,\n            workers=1,\n            verbose=ThinRes34Config.VERBOSE,\n        )\n        self.round_idx += 1\n        cur_train_loss = round(cur_fit_history.history.get(""loss"")[-1], 6)\n        cur_train_acc = round(cur_fit_history.history.get(""acc"")[-1], 6)\n        cur_lr = cur_fit_history.history.get(""lr"")[-1]\n        cur_fit_history_report = {\n            ""t_loss"": cur_train_loss,\n            ""t_acc"": cur_train_acc\n        }\n        return cur_fit_history_report\n\n    def good_to_predict(self):\n        flag = (\n            (self.round_idx in self.round_spec_len)\n            or (self.round_idx < 10)\n            or (self.round_idx < 21 and self.round_idx % 2 == 1)\n            or (self.round_idx - self.last_y_pred_round > 3)\n        )\n        return flag\n\n    def predict_val_proba(self, test_examples: np.ndarray, predict_prob_params: dict = None) -> np.ndarray:\n        K.set_learning_phase(0)\n        y_pred = self.model.predict(test_examples, batch_size=self.tr34_mconfig.PRED_SIZE)\n        return y_pred\n\n    def predict_proba(self, test_examples: np.ndarray, predict_prob_params: dict=None) -> np.ndarray:\n        K.set_learning_phase(0)\n        if self.good_to_predict():\n            y_pred = self.model.predict(test_examples, batch_size=self.tr34_mconfig.PRED_SIZE)\n            self.test_idx += 1\n\n            self.last_y_pred = y_pred\n            self.last_y_pred_round = self.round_idx\n            return y_pred\n        else:\n            return self.last_y_pred\n\n    def predict_proba_2(self, test_examples: np.ndarray, predict_prob_params: dict=None) -> np.ndarray:\n        K.set_learning_phase(0)\n        y_pred = self.model.predict(test_examples, batch_size=self.tr34_mconfig.PRED_SIZE)\n        self.test_idx += 1\n        return y_pred\n\n'"
AutoDL_sample_code_submission/at_speech/data_space/__init__.py,0,"b'from at_speech.data_space.data_augment import DNpAugPreprocessor, MixupGenerator, TTAGenerator'"
AutoDL_sample_code_submission/at_speech/data_space/data_augment.py,0,"b'import numpy as np\n\nfrom sklearn.utils import safe_indexing\n\n\nclass DNpAugPreprocessor(object):\n    @staticmethod\n    def frequency_masking(image, p=0.5, F=0.2):\n        _, w, _ = image.shape\n        p_1 = np.random.rand()\n\n        if p_1 > p:\n            return image\n\n        f = np.random.randint(0, int(w * F))\n        f0 = np.random.randint(0, w - f)\n\n        image[:, f0 : f0 + f, :] = 0.0\n\n        return image\n\n    @staticmethod\n    def crop_image(image):\n        h, w, _ = image.shape\n        h0 = np.random.randint(0, h - w)\n        image = image[h0 : h0 + w]\n\n        return image\n\n\nclass MixupGenerator(object):\n    def __init__(self, X, y, alpha=0.2, batch_size=32, datagen=None, shuffle=True):\n        self.X = X\n        self.y = y\n        self.alpha = alpha\n        self.batch_size = batch_size\n        self.datagen = datagen\n        self.shuffle = shuffle\n\n    def __call__(self):\n        while True:\n            indices = self.__get_exploration_order()\n            n_samples, _, _, _ = self.X.shape\n            itr_num = int(n_samples // (2 * self.batch_size))\n\n            for i in range(itr_num):\n                indices_head = indices[2 * i * self.batch_size : (2 * i + 1) * self.batch_size]\n                indices_tail = indices[(2 * i + 1) * self.batch_size : (2 * i + 2) * self.batch_size]\n\n                yield self.__data_generation(indices_head, indices_tail)\n\n    def __get_exploration_order(self):\n        n_samples = len(self.X)\n        indices = np.arange(n_samples)\n\n        if self.shuffle:\n            np.random.shuffle(indices)\n\n        return indices\n\n    def __data_generation(self, indices_head, indices_tail):\n        l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n        X_l = l.reshape(self.batch_size, 1, 1, 1)\n        y_l = l.reshape(self.batch_size, 1)\n\n        X1_tmp = safe_indexing(self.X, indices_head)\n        X2_tmp = safe_indexing(self.X, indices_tail)\n        n, _, w, _ = X1_tmp.shape\n        X1 = np.zeros((n, w, w, 1))\n        X2 = np.zeros((n, w, w, 1))\n\n        for i in range(self.batch_size):\n            X1[i] = DNpAugPreprocessor.crop_image(X1_tmp[i])\n            X2[i] = DNpAugPreprocessor.crop_image(X2_tmp[i])\n\n        X = X1 * X_l + X2 * (1.0 - X_l)\n\n        y1 = safe_indexing(self.y, indices_head)\n        y2 = safe_indexing(self.y, indices_tail)\n        y = y1 * y_l + y2 * (1.0 - y_l)\n\n        if self.datagen is not None:\n            for i in range(self.batch_size):\n                X[i] = self.datagen.random_transform(X[i])\n                X[i] = self.datagen.standardize(X[i])\n\n        return X, y\n\n\nclass TTAGenerator(object):\n    def __init__(self, X, batch_size):\n        self.X = X\n        self.batch_size = batch_size\n\n        self.n_samples, _, _, _ = X.shape\n\n    def __call__(self):\n        while True:\n            for start in range(0, self.n_samples, self.batch_size):\n                end = min(start + self.batch_size, self.n_samples)\n                X_batch = self.X[start:end]\n\n                yield self.__data_generation(X_batch)\n\n    def __data_generation(self, X_batch):\n        n, _, w, _ = X_batch.shape\n        X = np.zeros((n, w, w, 1))\n\n        for i in range(n):\n            X[i] = DNpAugPreprocessor.crop_image(X_batch[i])\n\n        return X, None\n'"
AutoDL_sample_code_submission/at_speech/data_space/examples_gen_maker.py,0,"b'# System\nimport keras\nimport numpy as np\n\n\ndef load_data(mag, train_spec_len=250, test_spec_len=250, mode=""train""):\n    freq, time = mag.shape\n    if mode == ""train"":\n        if time - train_spec_len > 0:\n            randtime = np.random.randint(0, time - train_spec_len)\n            spec_mag = mag[:, randtime : randtime + train_spec_len]\n        else:\n            spec_mag = mag[:, :train_spec_len]\n    else:\n        spec_mag = mag[:, :test_spec_len]\n    mu = np.mean(spec_mag, 0, keepdims=True)\n    std = np.std(spec_mag, 0, keepdims=True)\n    return (spec_mag - mu) / (std + 1e-5)\n\n\nclass DataGenerator(keras.utils.Sequence):\n\n    def __init__(\n        self,\n        X,\n        labels,\n        dim,\n        mp_pooler,\n        augmentation=True,\n        batch_size=32,\n        nfft=512,\n        spec_len=250,\n        win_length=400,\n        hop_length=160,\n        n_classes=5994,\n        shuffle=True,\n        normalize=True,\n    ):\n        self.dim = dim\n        self.nfft = nfft\n        self.spec_len = spec_len\n        self.normalize = normalize\n        self.mp_pooler = mp_pooler\n        self.win_length = win_length\n        self.hop_length = hop_length\n\n        self.labels = labels\n        self.shuffle = shuffle\n        self.X = X\n        self.n_classes = n_classes\n        self.batch_size = batch_size\n        self.augmentation = augmentation\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(np.floor(len(self.X) / self.batch_size))\n\n    def __getitem__(self, index):\n        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n        X_temp = [self.X[k] for k in indexes]\n        X, y = self.__data_generation_mp(X_temp, indexes)\n        return X, y\n\n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.X))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation_mp(self, X_temp, indexes):\n        X = [self.mp_pooler.apply_async(load_data, args=(mag, self.spec_len)) for mag in X_temp]\n        X = np.expand_dims(np.array([p.get() for p in X]), -1)\n        y = self.labels[indexes]\n        return X, y\n'"
AutoDL_sample_code_submission/at_speech/data_space/feats_data_space.py,0,"b'# coding:utf-8\nimport numpy as np\nfrom at_speech.data_space.raw_data_space import RawDataNpDb\nfrom at_speech.data_space.feats_engine import (\n    AbsFeatsMaker,\n    KapreMelSpectroGramFeatsMaker,\n    LibrosaMelSpectroGramFeatsMaker,\n    LbrsTr34FeatsMaker,\n)\nfrom at_speech.at_speech_cons import *\n\n\nclass FeatsDataDb:\n    def __init__(self, raw_train_num, raw_test_num):\n        self.raw_train_num = raw_train_num\n        self.raw_test_num = raw_test_num\n        self.raw_train_feats_data_tables = dict()\n        self.raw_test_feats_data_tables = dict()\n        self.split_val_feats_data_tables = dict()\n        self.raw_feat_makers_table = dict()\n        self.split_val_num = None\n        self.raw_data_db = RawDataNpDb(self.raw_train_num, self.raw_test_num)\n\n        self.kapre_melspecgram_featmaker = KapreMelSpectroGramFeatsMaker(""KAPRE"", FEAT_KAPRE_MELSPECGRAM)\n        self.lbs_melspecgram_featmaker = LibrosaMelSpectroGramFeatsMaker(""LIBROSA"", FEAT_LBS_MELSPECGRAM)\n        self.lbs_tr34_featmaker = LbrsTr34FeatsMaker(""LIBROSA"", FEAT_LBS_TR34)\n\n        self.add_feats_data_table(FEAT_KAPRE_MELSPECGRAM, self.kapre_melspecgram_featmaker)\n        self.add_feats_data_table(FEAT_LBS_MELSPECGRAM, self.lbs_melspecgram_featmaker)\n        self.add_feats_data_table(FEAT_LBS_TR34, self.lbs_tr34_featmaker)\n\n        self.raw_test_feats_status_table = {\n            FEAT_KAPRE_MELSPECGRAM: False,\n            FEAT_LBS_MELSPECGRAM: False,\n            FEAT_LBS_TR34: False,\n        }\n        self.split_val_feats_status_table = {\n            FEAT_KAPRE_MELSPECGRAM: False,\n            FEAT_LBS_MELSPECGRAM: False,\n            FEAT_LBS_TR34: False,\n        }\n\n    def add_feats_data_table(self, feat_name, feats_maker: AbsFeatsMaker):\n        if feat_name not in self.raw_train_feats_data_tables.keys():\n            self.raw_train_feats_data_tables[feat_name] = np.array([None] * self.raw_train_num)\n            self.raw_test_feats_data_tables[feat_name] = np.array([None] * self.raw_test_num)\n            self.split_val_feats_data_tables[feat_name] = np.array([None] * self.raw_train_num)\n            self.raw_feat_makers_table[feat_name] = feats_maker\n\n    def put_raw_test_feats(self, feat_name, raw_test_feats_np):\n        assert feat_name in self.raw_test_feats_data_tables.keys(), ""feat_name {} not exists in db"".format(feat_name)\n        self.raw_test_feats_data_tables[feat_name] = raw_test_feats_np\n\n    def get_raw_test_feats(self, feat_name, feats_maker_params: dict = None, forced=False):\n        if self.raw_test_feats_status_table.get(feat_name) is False or forced is True:\n            raw_test_feats_np = self.raw_feat_makers_table.get(feat_name).make_features(\n                self.raw_data_db.raw_test_x_np_table, feats_maker_params\n            )\n            self.put_raw_test_feats(feat_name, raw_test_feats_np)\n            self.raw_test_feats_status_table[feat_name] = True\n\n        return self.raw_test_feats_data_tables.get(feat_name)\n\n    def get_raw_train_feats(self, feat_name, raw_train_idxs, feats_maker_params: dict = None, forced=False):\n        need_make_feats_idxs = list()\n        if forced:\n            self.raw_train_feats_data_tables[feat_name] = np.array([None] * self.raw_train_num)\n            need_make_feats_idxs = raw_train_idxs\n        else:\n            for raw_train_idx in raw_train_idxs:\n                if self.raw_train_feats_data_tables.get(feat_name)[raw_train_idx] is None:\n                    need_make_feats_idxs.append(raw_train_idx)\n\n        if len(need_make_feats_idxs) > 0:\n            need_make_feats_rawdata = self.raw_data_db.raw_train_x_np_table[need_make_feats_idxs]\n            make_feats_done = self.raw_feat_makers_table.get(feat_name).make_features(\n                need_make_feats_rawdata, feats_maker_params\n            )\n            make_feats_done = np.array(make_feats_done)\n            for i in range(len(need_make_feats_idxs)):\n                self.raw_train_feats_data_tables.get(feat_name)[need_make_feats_idxs[i]] = make_feats_done[i]\n\n        cur_train_feats = [self.raw_train_feats_data_tables.get(feat_name)[i].shape for i in raw_train_idxs]\n        return np.stack(self.raw_train_feats_data_tables.get(feat_name)[raw_train_idxs])\n\n    def get_raw_train_y(self, raw_train_idxs):\n        return np.stack(self.raw_data_db.raw_train_y_np_table[raw_train_idxs])\n\n    def get_split_val_feats(self, feat_name:str, split_val_idxs:list, feats_maker_params: dict = None, forced=False):\n        if self.split_val_num is None:\n            self.split_val_num = self.raw_data_db.split_val_sample_num\n\n        if self.split_val_feats_status_table.get(feat_name) is False or forced is True:\n            self.split_val_feats_data_tables[feat_name] = np.array([None] * self.split_val_num)\n            need_make_feats_rawdata = self.raw_data_db.raw_train_x_np_table[split_val_idxs]\n            make_feats_done = self.raw_feat_makers_table.get(feat_name).make_features(\n                need_make_feats_rawdata, feats_maker_params\n            )\n            make_feats_done = np.array(make_feats_done)\n            self.split_val_feats_data_tables[feat_name] = make_feats_done\n            self.split_val_feats_status_table[feat_name] = True\n\n        return self.split_val_feats_data_tables.get(feat_name)\n\n'"
AutoDL_sample_code_submission/at_speech/data_space/feats_engine.py,0,"b'import os\nfrom functools import partial\nfrom itertools import repeat\nimport json\nimport multiprocessing\nfrom multiprocessing.pool import ThreadPool, Pool\nimport keras\nimport numpy as np\nimport librosa\nfrom sklearn.preprocessing import StandardScaler\nfrom kapre.time_frequency import Melspectrogram, Spectrogram\n\nfrom at_toolkit.interface.adl_feats_maker import AbsFeatsMaker\n# from at_toolkit import info, as_timer\n\nNCPU = multiprocessing.cpu_count()\nSAMPLING_RATE = 16000\nMAX_AUDIO_DURATION = 5\n\nAUDIO_SAMPLE_RATE = 16000\nKAPRE_FMAKER_WARMUP = True\n\n\ndef wav_to_mag_old(wav, params, win_length=400, hop_length=160, n_fft=512):\n    mode = params[""mode""]\n    wav = extend_wav(wav, params[""train_wav_len""], params[""test_wav_len""], mode=mode)\n    linear_spect = lin_spectogram_from_wav(wav, hop_length, win_length, n_fft)\n    mag, _ = librosa.magphase(np.asfortranarray(linear_spect))\n    mag_T = mag.T\n    if mode == ""test"":\n        mag_T = load_data(mag_T, params[""train_spec_len""], params[""test_spec_len""], mode)\n    return mag_T\n\n\ndef make_kapre_mag_maker(n_fft=1024, hop_length=128, audio_data_len=80000):\n    stft_model = keras.models.Sequential()\n    stft_model.add(Spectrogram(n_dft=n_fft, n_hop=hop_length, input_shape=(1, audio_data_len),\n                               power_spectrogram=2.0, return_decibel_spectrogram=False,\n                               trainable_kernel=False, name=\'stft\'))\n    return stft_model\n\n\ndef wav_to_mag(wav, params, win_length=400, hop_length=160, n_fft=512):\n    mode = params[""mode""]\n    wav = extend_wav(wav, params[""train_wav_len""], params[""test_wav_len""], mode=mode)\n    wav2feat_mode = 1\n\n    if wav2feat_mode == 0:\n        pass\n    elif wav2feat_mode == 1:\n        linear_sft = librosa.stft(np.asfortranarray(wav), n_fft=n_fft, win_length=win_length, hop_length=hop_length)  # linear spectrogram\n        mag_T = np.abs(linear_sft)\n        pass\n\n    elif wav2feat_mode == 2:\n        linear_sft = librosa.stft(np.asfortranarray(wav), n_fft=n_fft, win_length=win_length,\n                                  hop_length=hop_length)  # linear spectrogram\n        mag_T = linear_sft\n\n    if mode == ""test"":\n        mag_T = load_data(mag_T, params[""train_spec_len""], params[""test_spec_len""], mode)\n    return mag_T\n\n\ndef get_fixed_array(X_list, len_sample=5, sr=SAMPLING_RATE):\n    for i in range(len(X_list)):\n        if len(X_list[i]) < len_sample * sr:\n            n_repeat = np.ceil(sr * len_sample / X_list[i].shape[0]).astype(np.int32)\n            X_list[i] = np.tile(X_list[i], n_repeat)\n\n        X_list[i] = X_list[i][: len_sample * sr]\n\n    X = np.asarray(X_list)\n    X = np.stack(X)\n    X = X[:, :, np.newaxis]\n    X = X.transpose(0, 2, 1)\n    return X\n\ndef mel_feats_transform(x_mel):\n    x_feas = []\n    for i in range(len(x_mel)):\n        mel = np.mean(x_mel[i], axis=0).reshape(-1)\n        mel_std = np.std(x_mel[i], axis=0).reshape(-1)\n        fea_item = np.concatenate([mel, mel_std], axis=-1)\n        x_feas.append(fea_item)\n\n    x_feas = np.asarray(x_feas)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(x_feas[:, :])\n    return X\n\ndef extract_parallel(data, extract):\n    data_with_index = list(zip(data, range(len(data))))\n    results_with_index = list(pool.map(extract, data_with_index))\n    results_with_index.sort(key=lambda x: x[1])\n    results = []\n    for res, idx in results_with_index:\n        results.append(res)\n\n    return np.asarray(results)\n\ndef extract_for_one_sample(tuple, extract, use_power_db=False, **kwargs):\n    data, idx = tuple\n    r = extract(data, **kwargs)\n    if use_power_db:\n        r = librosa.power_to_db(r)\n\n    r = r.transpose()\n    return r, idx\n\ndef extend_wav(wav, train_wav_len=40000, test_wav_len=40000, mode=""train""):\n    if mode == ""train"":\n        div, mod = divmod(train_wav_len, wav.shape[0])\n        extended_wav = np.concatenate([wav] * div + [wav[:mod]])\n        if np.random.random() < 0.3:\n            extended_wav = extended_wav[::-1]\n        return extended_wav\n    else:\n        div, mod = divmod(test_wav_len, wav.shape[0])\n        extended_wav = np.concatenate([wav] * div + [wav[:mod]])\n        return extended_wav\n\ndef lin_spectogram_from_wav(wav, hop_length, win_length, n_fft=1024):\n    linear = librosa.stft(\n        np.asfortranarray(wav), n_fft=n_fft, win_length=win_length, hop_length=hop_length\n    )\n    return linear.T\n\n\ndef load_data(mag, train_spec_len=250, test_spec_len=250, mode=""train""):\n    freq, time = mag.shape\n    if mode == ""train"":\n        if time - train_spec_len > 0:\n            randtime = np.random.randint(0, time - train_spec_len)\n            spec_mag = mag[:, randtime : randtime + train_spec_len]\n        else:\n            spec_mag = mag[:, :train_spec_len]\n    else:\n        spec_mag = mag[:, :test_spec_len]\n    mu = np.mean(spec_mag, 0, keepdims=True)\n    std = np.std(spec_mag, 0, keepdims=True)\n    return (spec_mag - mu) / (std + 1e-5)\n\n\nclass KapreMelSpectroGramFeatsMaker(AbsFeatsMaker):\n    SAMPLING_RATE = 16000\n    N_MELS = 30\n    HOP_LENGTH = int(SAMPLING_RATE * 0.04)\n    N_FFT = 1024\n    FMIN = 20\n    FMAX = SAMPLING_RATE // 2\n\n    CROP_SEC = 5\n\n    def __init__(self, feat_name, feat_tool=""Kapre""):\n        super().__init__(feat_tool, feat_name)\n        self.kapre_melspectrogram_extractor = None\n        self.kape_params = {\n            ""SAMPLING_RATE"": self.SAMPLING_RATE,\n            ""N_MELS"": self.N_MELS,\n            ""HOP_LENGTH"": int(self.SAMPLING_RATE * 0.04),\n            ""N_FFT"": self.N_FFT,\n            ""FMIN"": self.FMIN,\n            ""FMAX"": self.SAMPLING_RATE // 2,\n            ""CROP_SEC"": self.CROP_SEC,\n        }\n        self.init_kapre_melspectrogram_extractor()\n\n    def make_melspectrogram_extractor(self, input_shape, sr=SAMPLING_RATE):\n        model = keras.models.Sequential()\n        model.add(\n            Melspectrogram(\n                fmax=self.kape_params.get(""FMAX""),\n                fmin=self.kape_params.get(""FMIN""),\n                n_dft=self.kape_params.get(""N_FFT""),\n                n_hop=self.kape_params.get(""HOP_LENGTH""),\n                n_mels=self.kape_params.get(""N_MELS""),\n                name=""melgram"",\n                image_data_format=""channels_last"",\n                input_shape=input_shape,\n                return_decibel_melgram=True,\n                power_melgram=2.0,\n                sr=sr,\n                trainable_kernel=False,\n            )\n        )\n        return model\n\n    def init_kapre_melspectrogram_extractor(self):\n        self.kapre_melspectrogram_extractor = self.make_melspectrogram_extractor(\n            (1, self.kape_params.get(""CROP_SEC"") * self.kape_params.get(""SAMPLING_RATE""))\n        )\n        if KAPRE_FMAKER_WARMUP:\n            warmup_size = 10\n            warmup_x = [\n                np.array([np.random.uniform() for i in range(48000)], dtype=np.float32) for j in range(warmup_size)\n            ]\n            warmup_x_mel = self.make_features(warmup_x, feats_maker_params={""len_sample"": 5, ""sr"": 16000})\n\n    def make_features(self, raw_data, feats_maker_params: dict):\n        raw_data = [sample[0 : MAX_AUDIO_DURATION * AUDIO_SAMPLE_RATE] for sample in raw_data]\n\n        X = get_fixed_array(raw_data, len_sample=feats_maker_params.get(""len_sample""), sr=feats_maker_params.get(""sr""))\n        X = self.kapre_melspectrogram_extractor.predict(X)\n\n        X = np.squeeze(X)\n        X = X.transpose(0, 2, 1)\n\n        X = mel_feats_transform(X)\n        return X\n\n\nclass LibrosaMelSpectroGramFeatsMaker(AbsFeatsMaker):\n    FFT_DURATION = 0.1\n    HOP_DURATION = 0.04\n\n    def __init__(self, feat_name, feat_tool=""Librosa""):\n        super().__init__(feat_tool, feat_name)\n\n    def extract_melspectrogram_parallel(\n        self, data, sr=16000, n_fft=None, hop_length=None, n_mels=30, use_power_db=False\n    ):\n        if n_fft is None:\n            n_fft = int(sr * self.FFT_DURATION)\n        if hop_length is None:\n            hop_length = int(sr * self.HOP_DURATION)\n        extract = partial(\n            extract_for_one_sample,\n            extract=librosa.feature.melspectrogram,\n            sr=sr,\n            n_fft=n_fft,\n            hop_length=hop_length,\n            n_mels=n_mels,\n            use_power_db=use_power_db,\n        )\n        results = extract_parallel(data, extract)\n\n        return results\n\n    def make_features(self, raw_data, feats_maker_params: dict):\n        x_mel = self.extract_melspectrogram_parallel(raw_data, n_mels=30, use_power_db=True)\n        # tranform melspectrogram features.\n        x_mel_transformed = mel_feats_transform(x_mel)\n        return x_mel_transformed\n\n\npool = Pool(os.cpu_count())\n\n\nclass LbrsTr34FeatsMaker(AbsFeatsMaker):\n    def __init__(self, feat_tool, feat_name):\n        super().__init__(feat_tool, feat_name)\n        self.feat_des = ""for_TR34""\n\n    def pre_trans_wav_update(self, wav_list, params):\n        if len(wav_list) == 0:\n            return []\n        elif len(wav_list) > NCPU * 2:\n            mag_arr = pool.starmap(wav_to_mag, zip(wav_list, repeat(params)))\n            return mag_arr\n        else:\n            mag_arr = [wav_to_mag(wav, params) for wav in wav_list]\n            return mag_arr\n\n    def make_features(self, raw_data, feats_maker_params: dict):\n        tr34_data_features = self.pre_trans_wav_update(raw_data, feats_maker_params)\n        return tr34_data_features\n'"
AutoDL_sample_code_submission/at_speech/data_space/raw_data_space.py,0,"b'# coding:utf-8\nimport numpy as np\nfrom at_toolkit import info\n\n\nclass RawDataNpDb:\n    def __init__(self, train_mum, test_num):\n        self.train_num = train_mum\n        self.test_num = test_num\n        self.raw_train_x_np_table = np.array([None] * self.train_num)\n        self.raw_train_y_np_table = np.array([None] * self.train_num)\n        self.raw_train_x_np_table_filled = None\n        self.raw_train_y_np_table_filled = None\n        self.raw_test_x_np_table = np.array([None] * self.test_num)\n        self.raw_train_np_filled_num = 0\n        self.if_raw_test_2_np_done = False\n        self.split_val_x_np_table = None\n        self.split_val_y_np_table = None\n        self.split_val_sample_idxs = list()\n        self.split_val_sample_num = None\n\n    def put_raw_train_np(self, raw_train_x_array, raw_train_y_array):\n        put_len = len(raw_train_x_array)\n        for i in range(put_len):\n            self.raw_train_x_np_table[i] = np.array(raw_train_x_array[i])\n            self.raw_train_y_np_table[i] = np.array(raw_train_y_array[i])\n\n        self.raw_train_np_filled_num = put_len\n        self.raw_train_x_np_table_filled = self.raw_train_x_np_table[: self.raw_train_np_filled_num]\n        self.raw_train_y_np_table_filled = self.raw_train_y_np_table[: self.raw_train_np_filled_num]\n\n    def put_raw_test_np(self, raw_test_x_array):\n        self.raw_test_x_np_table = raw_test_x_array.copy()\n        self.if_raw_test_2_np_done = True\n\n    def put_split_valid_np(self, val_sample_idxs:list):\n        self.split_val_sample_idxs = val_sample_idxs\n        self.split_val_x_np_table = self.raw_train_x_np_table[self.split_val_sample_idxs]\n        self.split_val_y_np_table = self.raw_train_y_np_table[self.split_val_sample_idxs]\n        self.split_val_sample_num = len(val_sample_idxs)\n'"
AutoDL_sample_code_submission/at_speech/policy_space/__init__.py,0,b''
AutoDL_sample_code_submission/at_speech/policy_space/decision_making.py,0,"b'from at_speech.policy_space.meta_learning import ModelSelectLearner\nfrom at_speech.policy_space.ensemble_learning import EnsembleLearner\nfrom at_toolkit import AdlSpeechDMetadata\nfrom at_speech.at_speech_cons import CLS_LR_LIBLINEAER, CLS_LR_SAG, CLS_TR34\n\n\nclass DecisionMaker(object):\n    def __init__(self, aspeech_metadata: AdlSpeechDMetadata):\n        self.aspeech_metadata = aspeech_metadata\n        self.meta_model_select_learner = ModelSelectLearner()\n        self.ensemble_learner = EnsembleLearner(self.aspeech_metadata)\n        self.aspeech_metadata_minix_report_flag = False\n\n    def learn_train_minisamples_report(self, train_minis_report:dict):\n        self.aspeech_metadata.init_train_minisamples_report(train_minis_report)\n        self.aspeech_metadata_minix_report_flag = True\n\n    def decide_if_start_val(self):\n        self.IF_START_VAL = False\n\n    def decide_if_ensemble_pred(self):\n        self.IF_ENSEMBLE_PRED = False\n\n    def decide_model_select(self, train_pip_id):\n        return self.meta_model_select_learner.predict_train_cls_select(train_pip_id)\n\n    def decide_g_valid_num(self) -> int:\n        return self.ensemble_learner.predict_g_valid_num()\n\n    def decide_if_split_val(self, token_size):\n        return self.ensemble_learner.predict_if_split_val(token_size)\n\n    def decide_tfds2np_array(self):\n        assert self.aspeech_metadata_minix_report_flag is True, ""Error:Meta mini_samples_report flag is False""\n        if self.aspeech_metadata.train_minisamples_report.get(""x_seqlen_mean"") > 200000:\n            return [0.1, 0.2, 0.3, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n        elif self.aspeech_metadata.class_num >= 40:\n            return [0.1, 0.2, 0.4, 0.3]\n        else:\n            return [0.1, 0.2, 0.4, 0.3]\n\n\n    def infer_model_select_def(self):\n        model_select_def = None\n        if self.aspeech_metadata.class_num < 10:\n            model_select_def = {\n                0: CLS_LR_LIBLINEAER,\n                1: CLS_LR_LIBLINEAER,\n                2: CLS_TR34,\n            }\n        else:\n            model_select_def = {\n                0: CLS_LR_LIBLINEAER,\n                1: CLS_LR_LIBLINEAER,\n                2: CLS_LR_SAG,\n                3: CLS_TR34,\n            }\n        self.meta_model_select_learner.model_select_def = model_select_def\n\n    def infer_tr34_trainpip_warmup(self):\n        if self.aspeech_metadata.class_num <= 10:\n            return 2\n        elif 10 < self.aspeech_metadata.class_num <= 37:\n            return 8\n        else:\n            return 11\n\n    def infer_tr34_hps_epoch(self):\n        if self.aspeech_metadata.class_num <= 10:\n            first_epoch = 8\n        else:\n            first_epoch = 14\n        left_epoch = 1\n        return {""first_epoch"": first_epoch, ""left_epoch"":left_epoch}\n\n    def infer_tr34_hps_samplenum(self):\n        tr34_hps_sample_info = dict()\n        if self.aspeech_metadata.class_num > 37 or self.aspeech_metadata.train_num > 1000:\n            tr34_hps_sample_info[""SAMP_MAX_NUM""] = 300\n            tr34_hps_sample_info[""SAMP_MIN_NUM""] = 300\n        else:\n            tr34_hps_sample_info[""SAMP_MAX_NUM""] = 200\n            tr34_hps_sample_info[""SAMP_MIN_NUM""] = 200\n        return tr34_hps_sample_info\n\n'"
AutoDL_sample_code_submission/at_speech/policy_space/ensemble_learning.py,0,"b'# coding:utf-8\nimport numpy as np\nfrom at_toolkit.interface.adl_metadata import AdlSpeechDMetadata as ASMD\n# from at_toolkit import info\n\n\n# Config\nG_VAL_CL_NUM = 3\nG_VAL_T_MAX_MUM = 300\nG_VAL_T_MIN_NUM = 0\n\n\ndef listofdict_topn_sorter(raw_listofdict, attr_key, reverse=True):\n    sorted_res = sorted(enumerate(raw_listofdict), key=lambda x: x[1][attr_key], reverse=reverse)\n    sorted_ids = [x[0] for x in sorted_res]\n    sorted_attrs = [x[1][attr_key] for x in sorted_res]\n    return sorted_ids, sorted_attrs\n\nclass EvalPredSpace:\n    def __init__(self):\n        self.eval_pred_pool = list()\n        self.g_sort_idx_pool = {""val_nauc"": None, ""t_acc"": None, ""t_loss"": None}\n\n    def put_epoch_eval_preds(self, epoch_item):\n        self.eval_pred_pool.append(epoch_item)\n        self.g_sort_val_nauc_idxs, self.g_sort_val_naucs = listofdict_topn_sorter(\n            raw_listofdict=self.eval_pred_pool, attr_key=""val_nauc""\n        )\n\n        self.g_sort_train_loss_idxs, self.g_sort_train_losss = listofdict_topn_sorter(\n            raw_listofdict=self.eval_pred_pool, attr_key=""t_loss"", reverse=False\n        )\n        self.g_sort_train_acc_idxs, self.g_sort_train_accs = listofdict_topn_sorter(\n            raw_listofdict=self.eval_pred_pool, attr_key=""t_acc""\n        )\n\n        self.g_sort_idx_pool[""val_nauc""] = self.g_sort_val_nauc_idxs\n        self.g_sort_idx_pool[""t_loss""] = self.g_sort_train_loss_idxs\n        self.g_sort_idx_pool[""t_acc""] = self.g_sort_train_acc_idxs\n\n\nclass EnsembleLearner:\n    COMM_KEY_LIST = [""t_loss"", ""t_acc"", ""val_nauc"", ""val_auc"", ""val_acc"", ""val_loss""]\n\n    def __init__(self, d_metadata: ASMD):\n        self.d_metadata = d_metadata\n        self.eval_pred_space = EvalPredSpace()\n        self.commitee_id_pool = list()\n        self.has_split_val = False\n\n    def add_eval_pred_item(self, eval_pred_item: dict):\n        self.eval_pred_space.put_epoch_eval_preds(eval_pred_item)\n\n    def gen_committee(self, voting_conditions: dict):\n\n        self.commitee_id_pool = list()\n        for k, v in voting_conditions.items():\n            assert k in self.COMM_KEY_LIST\n            condition_comit_ids = self.eval_pred_space.g_sort_idx_pool.get(k)[:v]\n            condition_comit_values =  [self.eval_pred_space.eval_pred_pool[i].get(k) for i in condition_comit_ids]\n            if k == ""t_acc"" and self.eval_pred_space.g_sort_train_accs[:10].count(1) == 10:\n                pass\n            else:\n                self.commitee_id_pool.extend(condition_comit_ids)\n\n    def softvoting_ensemble_preds(self, voting_conditions: dict):\n        self.gen_committee(voting_conditions)\n\n        commitee_pool_preds = [\n            self.eval_pred_space.eval_pred_pool[epkey].get(""pred_probas"") for epkey in self.commitee_id_pool\n        ]\n        return np.mean(commitee_pool_preds, axis=0)\n\n    def predict_g_valid_num(self):\n        def_ratio = 0.1\n        g_valid_num = self.d_metadata.class_num * G_VAL_CL_NUM\n        g_valid_num = max(g_valid_num, int(self.d_metadata.train_num * def_ratio))\n        g_valid_num = min(g_valid_num, G_VAL_T_MAX_MUM)\n        g_valid_num = max(g_valid_num, G_VAL_T_MIN_NUM)\n        return g_valid_num\n\n    def predict_if_split_val(self, token_size):\n        if token_size >= self.d_metadata.train_num * 0.3:\n            self.has_split_val = True\n            return True\n        else:\n            return False\n\n\n    def get_loss_godown_rate(self, g_loss_list, tail_window_size):\n        tail_loss_list = g_loss_list[-tail_window_size:]\n        loss_num = len(tail_loss_list)\n        loss_godown_count_list = list()\n        for i in range(1, loss_num):\n            if tail_loss_list[i] - tail_loss_list[i - 1] < 0:\n                loss_godown_count_list.append(1)\n            else:\n                loss_godown_count_list.append(-1)\n\n        loss_godown_count_num = loss_godown_count_list.count(1)\n        loss_godown_count_rate = round(loss_godown_count_num / (loss_num - 1), 4)\n\n        return loss_godown_count_rate\n\n\ndef main():\n    ensemble_learner = EnsembleLearner()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
AutoDL_sample_code_submission/at_speech/policy_space/meta_learning.py,0,"b'# coding:utf-8\nfrom at_speech.at_speech_cons import CLS_LR_LIBLINEAER, CLS_LR_SAG, CLS_TR34\nfrom at_speech.at_speech_config import MODEL_SELECT_DEF\n\n\nclass SpeechMetaLearner(object):\n    def __init__(self):\n        self.d_metadata = None\n        self.d_eda_report = None\n        self.class_num = None\n        self.train_num = None\n        self.mini_samples = None\n\n        self.speech_domin_classifier = SpeechDomainClassifier()\n        self.model_select_learner = ModelSelectLearner()\n        self.tr34_hypparams_learner = Tr34HParamsLearner()\n\n    def init_metadata(self, d_metadata):\n        self.d_metadata = d_metadata\n        self.class_num = self.d_metadata.get(""class_num"")\n        self.train_num = self.d_metadata.get(""train_num"")\n\n    def init_eda_report(self, d_eda_report):\n        self.d_eda_report = d_eda_report\n\n    def init_minisamples(self, minisamples):\n        self.mini_samples = minisamples\n\n    def predict_tr34_hypparams(self):\n        tr34_hypparams = dict()\n        tr34_hypparams[""freeze_layer_num""] = self.tr34_hypparams_learner.predict_freeze_layernum(self.d_metadata)\n        tr34_hypparams[""spec_len_config""] = self.tr34_hypparams_learner.predict_freeze_layernum(self.d_metadata)\n        return tr34_hypparams\n\n    def predict_model_select(self, train_loop_num):\n        return self.model_select_learner.predict_train_cls_select(train_loop_num)\n\n\nclass ModelSelectLearner:\n\n    OFFLINE_CLS_ZOO = [CLS_LR_LIBLINEAER, CLS_LR_SAG]\n    ONLINE_CLS_ZOO = [CLS_TR34]\n\n    def __init__(self):\n        self.cur_train_cls_name = None\n        self.model_select_def = MODEL_SELECT_DEF\n\n    def predict_train_cls_select(self, train_loop_num):\n        if train_loop_num in self.model_select_def:\n            self.cur_train_cls_name = self.model_select_def.get(train_loop_num)\n\n        return self.cur_train_cls_name\n\n\nclass SpeechDomainClassifier(object):\n    DOMAIN_SPEAKER = ""SPEAKER""\n    DOMAIN_EMOTION = ""EMOTION""\n    DOMAIN_ACCENT = ""ACCENT""\n    DOMAIN_MUSIC = ""MUSIC""\n    DOMAIN_LANGUAGE = ""LANGU""\n\n    DOMAIN_LIST = [DOMAIN_SPEAKER, DOMAIN_EMOTION, DOMAIN_ACCENT, DOMAIN_MUSIC, DOMAIN_LANGUAGE]\n\n    def predict_speech_domain(self) -> str:\n        i = 0\n        return self.DOMAIN_LIST[i]\n\n\nclass Tr34HParamsLearner:\n    INIT_FRZ_L_NUM = 124\n    INIT_FRZ_L_NUM_WILD = 100\n    CLASS_NUM_THS = 37\n\n    FE_RS_SPEC_LEN_CONFIG_AGGR = {\n        1: (100, 100, 0.002),\n        10: (500, 500, 0.003),\n        21: (1500, 1500, 0.004),\n        50: (2250, 2250, 0.004),\n    }\n\n    FE_RS_SPEC_LEN_CONFIG_MILD = {\n        1: (250, 250, 0.002),\n        10: (500, 500, 0.004),\n        20: (1000, 1000, 0.002),\n        50: (1500, 1500, 0.004),\n    }\n\n    def predict_freeze_layernum(self, d_metadata: dict) -> int:\n        if d_metadata.get(""class_num"") >= self.CLASS_NUM_THS:\n            frz_layer_num = self.INIT_FRZ_L_NUM\n        else:\n            frz_layer_num = self.INIT_FRZ_L_NUM_WILD\n        return frz_layer_num\n\n    def predict_round_spec_len(self, d_metadata: dict) -> dict:\n        if d_metadata.get(""class_num"") >= self.CLASS_NUM_THS:\n            round_spec_len = self.FE_RS_SPEC_LEN_CONFIG_AGGR\n        else:\n            round_spec_len = self.FE_RS_SPEC_LEN_CONFIG_MILD\n        return round_spec_len\n\n'"
AutoDL_sample_code_submission/at_speech/policy_space/model_executor.py,0,"b'import os, sys\nimport numpy as np\n\nhere = os.path.abspath(os.path.dirname(__file__))\nCODE_SUB_DIR = os.path.abspath(os.path.join(here, "".."", ""..""))\nprint(CODE_SUB_DIR)\nsys.path.append(CODE_SUB_DIR)\n\n\nfrom at_speech import SLLRLiblinear, SLLRSag, ThinResnet34Classifier\nfrom at_speech.data_space.raw_data_space import RawDataNpDb\nfrom at_toolkit.at_sampler import AutoSamplerBasic, AutoSpSamplerNew, AutoValidSplitor, minisamples_edaer, sample_y_edaer\nfrom at_speech.data_space.feats_data_space import FeatsDataDb\nfrom at_speech.policy_space.decision_making import DecisionMaker\nfrom at_toolkit.at_tfds_convertor import TfdsConvertor\nfrom at_toolkit import AdlClassifier, AdlSpeechDMetadata\nfrom at_toolkit.at_evalator import  ATEvaluator\nfrom at_speech.at_speech_cons import *\nfrom at_speech.at_speech_config import TFDS2NP_TAKESIZE_RATION_LIST, TR34_TRAINPIP_WARMUP, IF_VAL_ON, Tr34SamplerHpParams\n\n\nCLS_REG_TABLE = {\n    CLS_LR_LIBLINEAER: SLLRLiblinear,\n    CLS_LR_SAG: SLLRSag,\n    CLS_TR34: ThinResnet34Classifier,\n}\n\nCLS_2_FEATNAME_REG_TABLE = {\n    CLS_LR_LIBLINEAER: FEAT_KAPRE_MELSPECGRAM,\n    CLS_LR_SAG: FEAT_KAPRE_MELSPECGRAM,\n    CLS_TR34: FEAT_LBS_TR34,\n}\n\n\nclass MetaClsHPParams:\n    lr_sag_cls_init_params = {""max_iter"": 50}\n\n\nclass ModelExecutor:\n    def __init__(self, ds_metadata):\n\n        self.class_num = ds_metadata.get(""class_num"")\n        self.train_num = ds_metadata.get(""train_num"")\n        self.test_num = ds_metadata.get(""test_num"")\n        self.aspeech_metadata = AdlSpeechDMetadata(ds_metadata)\n\n        self.cls_tpye_libs = [CLS_LR_LIBLINEAER, CLS_LR_SAG, CLS_TR34]\n        self.lr_libl_cls = None\n        self.lr_sag_cls = None\n        self.tr34_cls = None\n        self.tr34_cls_train_pip_run = 0\n        self.tfds_convertor = TfdsConvertor()\n        self.feats_data_db = FeatsDataDb(self.train_num, self.test_num)\n\n        self.init_pipeline()\n\n        self.train_pip_id = 0\n        self.test_pip_id = 0\n\n        self.token_train_size = 0\n\n        self.cur_cls_ins_table = {\n            CLS_LR_LIBLINEAER: self.lr_libl_cls,\n            CLS_LR_SAG: self.lr_sag_cls,\n            CLS_TR34: self.tr34_cls,\n        }\n\n        self.decision_maker = DecisionMaker(self.aspeech_metadata)\n        self.cur_cls = None\n        self.cur_sampler = None\n        self.val_sample_idxs = list()\n        self.cur_val_examples_y = None\n        self.cur_val_nauc = None\n        self.cur_train_his_report = dict()\n\n        self.minis_eda_report = None\n        self.is_multilabel = False\n\n        self.lr_sampler = AutoSamplerBasic(self.class_num)\n        self.tr34_sampler = AutoSpSamplerNew(None)\n        self.val_splitor = AutoValidSplitor(self.class_num)\n\n        self.cur_sampler_table = {\n            CLS_LR_LIBLINEAER: self.lr_sampler,\n            CLS_LR_SAG: self.lr_sampler,\n            CLS_TR34: self.tr34_sampler,\n        }\n\n        self.tfds2np_take_size_array = TFDS2NP_TAKESIZE_RATION_LIST\n        self.tfds2np_takesize_flag = False\n        self.decision_maker.infer_model_select_def()\n        self.tr34_trainpip_warmup = self.decision_maker.infer_tr34_trainpip_warmup()\n        self.tr34_hps_epochs_dict = self.decision_maker.infer_tr34_hps_epoch()\n        self.tr34_hps_sample_dict = self.decision_maker.infer_tr34_hps_samplenum()\n\n\n    def init_pipeline(self):\n        self.lr_libl_cls = SLLRLiblinear()\n        self.lr_libl_cls.init(self.class_num)\n\n        self.lr_sag_cls = SLLRSag()\n        self.lr_sag_cls.init(self.class_num, MetaClsHPParams.lr_sag_cls_init_params)\n\n        self.tr34_cls = ThinResnet34Classifier()\n        self.tr34_cls.init(self.class_num)\n\n\n    def train_pipeline(self, train_tfds, update_train_data=True):\n        if self.train_pip_id < len(self.tfds2np_take_size_array):\n            if self.train_pip_id == 1:\n                take_train_size = max(200, int(self.tfds2np_take_size_array[self.train_pip_id] * self.train_num))\n            else:\n                take_train_size = int(self.tfds2np_take_size_array[self.train_pip_id] * self.train_num)\n        else:\n            take_train_size = 200\n        self.token_train_size += take_train_size\n        self.cur_train_his_report = dict()\n\n        self.tfds_convertor.init_train_tfds(train_tfds, self.train_num)\n        if update_train_data is True and self.feats_data_db.raw_data_db.raw_train_np_filled_num < self.train_num:\n            accm_raw_train_np_dict = self.tfds_convertor.get_train_np_accm(take_train_size)\n            self.minis_eda_report = minisamples_edaer(accm_raw_train_np_dict[""x""], accm_raw_train_np_dict[""y""])\n            if self.minis_eda_report.get(""y_cover_rate"") <= 0.5:\n                self.tfds_convertor.init_train_tfds(train_tfds, self.train_num, force_shuffle=True)\n                accm_raw_train_np_dict = self.tfds_convertor.get_train_np_accm(take_train_size)\n                self.minis_eda_report = minisamples_edaer(accm_raw_train_np_dict[""x""], accm_raw_train_np_dict[""y""])\n\n            self.is_multilabel = self.minis_eda_report.get(""is_multilabel"")\n            self.tr34_cls.renew_if_multilabel(self.is_multilabel)\n\n            if self.tfds2np_takesize_flag is False:\n                self.decision_maker.learn_train_minisamples_report(self.minis_eda_report)\n                self.tfds2np_take_size_array = self.decision_maker.decide_tfds2np_array()\n                self.tfds2np_takesize_flag = True\n\n            self.feats_data_db.raw_data_db.put_raw_train_np(accm_raw_train_np_dict[""x""], accm_raw_train_np_dict[""y""])\n\n        if_split_val = self.decision_maker.decide_if_split_val(self.token_train_size)\n        if IF_VAL_ON and if_split_val and len(self.val_sample_idxs) == 0:\n            val_mode = ""bal""\n            val_num = self.decision_maker.decide_g_valid_num()\n            self.val_sample_idxs = self.val_splitor.get_valid_sample_idxs(\n                np.stack(self.feats_data_db.raw_data_db.raw_train_y_np_table_filled), val_num=val_num, mode=val_mode\n            )\n            self.feats_data_db.raw_data_db.put_split_valid_np(self.val_sample_idxs)\n            self.cur_val_examples_y = self.feats_data_db.get_raw_train_y(self.val_sample_idxs)\n\n        self.cur_cls_name = self.decision_maker.decide_model_select(self.train_pip_id)\n        self.cur_cls = self.cur_cls_ins_table.get(self.cur_cls_name)\n        self.cur_sampler = self.cur_sampler_table.get(self.cur_cls_name)\n\n        if self.cur_cls_name in [CLS_LR_LIBLINEAER, CLS_LR_SAG]:\n            if self.is_multilabel is False:\n                self.lr_sampler.init_train_y(self.feats_data_db.raw_data_db.raw_train_y_np_table_filled)\n                class_inverted_index_array = self.lr_sampler.init_each_class_index_by_y(self.lr_sampler.train_y)\n\n                cur_train_sample_idxs = self.lr_sampler.init_even_class_index_by_each(class_inverted_index_array)\n                cur_train_sample_idxs = [item for sublist in cur_train_sample_idxs for item in sublist]\n                cur_train_sample_idxs = [i for i in cur_train_sample_idxs if i not in self.val_sample_idxs]\n\n            else:\n                cur_train_sample_idxs = range(len(self.feats_data_db.raw_data_db.raw_train_y_np_table_filled))\n\n            self.cur_feat_name = CLS_2_FEATNAME_REG_TABLE.get(self.cur_cls_name)\n            self.use_feat_params = {""len_sample"": 5, ""sr"": 16000}\n            cur_train_examples_x = self.feats_data_db.get_raw_train_feats(\n                self.cur_feat_name, cur_train_sample_idxs, self.use_feat_params\n            )\n            cur_train_examples_y = self.feats_data_db.get_raw_train_y(cur_train_sample_idxs)\n\n            train_eda_report = sample_y_edaer(cur_train_examples_y)\n            if self.cur_cls_name == CLS_LR_LIBLINEAER:\n                assert isinstance(self.cur_cls, SLLRLiblinear), ""Error cur_cls is not {}"".format(SLLRLiblinear.__name__)\n                self.cur_cls.offline_fit(cur_train_examples_x, cur_train_examples_y, fit_params={""if_multilabel"": self.is_multilabel})\n            elif self.cur_cls_name == CLS_LR_SAG:\n                assert isinstance(self.cur_cls, SLLRSag), ""Error cur_cls is not {}"".format(SLLRSag.__name__)\n                self.cur_cls.offline_fit(cur_train_examples_x, cur_train_examples_y, fit_params={""if_multilabel"": self.is_multilabel})\n\n\n        elif self.cur_cls_name in [CLS_TR34]:\n            assert isinstance(self.cur_cls, ThinResnet34Classifier), ""Error, cls select is {}"".format(\n                type(self.cur_cls)\n            )\n            train_use_y_labels = np.stack(self.feats_data_db.raw_data_db.raw_train_y_np_table_filled)\n\n            self.tr34_sampler = AutoSpSamplerNew(y_train_labels=train_use_y_labels)\n\n            if self.is_multilabel is False:\n                self.tr34_sampler.set_up()\n                cur_train_sample_idxs = self.tr34_sampler.get_downsample_index_list_by_class(\n                    per_class_num=Tr34SamplerHpParams.SAMPL_PA_F_PERC_NUM,\n                    max_sample_num=self.tr34_hps_sample_dict.get(""SAMP_MAX_NUM""),\n                    min_sample_num=self.tr34_hps_sample_dict.get(""SAMP_MIN_NUM""),\n                )\n            else:\n                cur_train_sample_idxs = self.tr34_sampler.get_downsample_index_list_by_random(\n                    max_sample_num=self.tr34_hps_sample_dict.get(""SAMP_MAX_NUM""),\n                    min_sample_num=self.tr34_hps_sample_dict.get(""SAMP_MIN_NUM""))\n\n            cur_train_sample_idxs = [i for i in cur_train_sample_idxs if i not in self.val_sample_idxs]\n\n            self.cur_feat_name = CLS_2_FEATNAME_REG_TABLE.get(self.cur_cls_name)\n            if_train_feats_force = self.cur_cls.decide_if_renew_trainfeats()\n            self.use_feat_params = self.cur_cls.imp_feat_args\n            cur_train_examples_x = self.feats_data_db.get_raw_train_feats(\n                self.cur_feat_name, cur_train_sample_idxs, self.use_feat_params, if_train_feats_force\n            )\n            cur_train_examples_y = self.feats_data_db.get_raw_train_y(cur_train_sample_idxs)\n            train_eda_report = sample_y_edaer(cur_train_examples_y)\n            self.tr34_cls_train_pip_run += 1\n            self.cur_train_his_report = self.cur_cls.online_fit(cur_train_examples_x, cur_train_examples_y, fit_params=self.tr34_hps_epochs_dict)\n\n        if len(self.val_sample_idxs) > 0:\n            if self.cur_cls_name == CLS_TR34:\n                assert isinstance(self.cur_cls, ThinResnet34Classifier)\n                if_force_val_feats = self.cur_cls.decide_if_renew_valfeats()\n                use_feat_params = self.cur_cls.imp_feat_args\n                cur_val_examples_x = self.feats_data_db.get_split_val_feats(\n                    self.cur_feat_name, self.val_sample_idxs, use_feat_params, if_force_val_feats\n                )\n                cur_val_examples_x = np.array(cur_val_examples_x)\n                cur_val_examples_x = cur_val_examples_x[:, :, :, np.newaxis]\n                cur_val_preds = self.cur_cls.predict_val_proba(cur_val_examples_x)\n            else:\n                cur_val_examples_x = self.feats_data_db.get_split_val_feats(self.cur_feat_name, self.val_sample_idxs, self.use_feat_params)\n                cur_val_preds = self.cur_cls.predict_proba(cur_val_examples_x, predict_prob_params={""if_multilabel"": self.is_multilabel})\n\n            self.cur_val_nauc = ATEvaluator.autodl_auc(solution=self.cur_val_examples_y, prediction=cur_val_preds)\n        else:\n            self.cur_val_nauc = -1\n\n        self.train_pip_id += 1\n        self.cur_train_his_report[""val_nauc""] = self.cur_val_nauc\n        self.cur_train_his_report[""cls_name""] = self.cur_cls_name\n        return self.cur_train_his_report.copy()\n\n\n    def test_pipeline(self, test_tfds):\n        self.tfds_convertor.init_test_tfds(test_tfds)\n        if not self.feats_data_db.raw_data_db.if_raw_test_2_np_done:\n            raw_test_np = self.tfds_convertor.get_test_np()\n            assert isinstance(raw_test_np, list), ""raw_test_np is not list""\n            self.feats_data_db.raw_data_db.put_raw_test_np(raw_test_np)\n\n        if self.cur_cls_name in [CLS_LR_LIBLINEAER, CLS_LR_SAG]:\n            use_feat_params = {""len_sample"": 5, ""sr"": 16000}\n            cur_test_examples_x = self.feats_data_db.get_raw_test_feats(self.cur_feat_name, use_feat_params)\n\n            assert isinstance(self.cur_cls, AdlClassifier)\n            cur_test_preds = self.cur_cls.predict_proba(cur_test_examples_x, predict_prob_params={""if_multilabel"": self.is_multilabel})\n            self.test_pip_id += 1\n            return np.array(cur_test_preds)\n\n        if self.cur_cls_name in [CLS_TR34]:\n            while self.tr34_cls_train_pip_run < self.tr34_trainpip_warmup:\n                self.train_pipeline(train_tfds=None, update_train_data=False)\n\n            assert isinstance(self.cur_cls, ThinResnet34Classifier), ""Error, cur_cls type error.""\n            if_force_test_feats = self.cur_cls.decide_if_renew_testfeats()\n            use_feat_params = self.cur_cls.imp_feat_args\n            cur_test_examples_x = self.feats_data_db.get_raw_test_feats(\n                self.cur_feat_name, use_feat_params, if_force_test_feats\n            )\n            cur_test_examples_x = np.asarray(cur_test_examples_x)\n\n            cur_test_examples_x = cur_test_examples_x[:, :, :, np.newaxis]\n\n            assert isinstance(self.cur_cls, AdlClassifier)\n\n            cur_test_preds = self.cur_cls.predict_proba(cur_test_examples_x)\n\n            del cur_test_examples_x\n\n            self.test_pip_id += 1\n            return cur_test_preds\n\n\n'"
AutoDL_sample_code_submission/at_toolkit/interface/__init__.py,0,b''
AutoDL_sample_code_submission/at_toolkit/interface/adl_classifier.py,0,"b'import numpy as np\n\n\nclass AdlClassifier(object):\n    def init(self, class_num: int, init_params: dict):\n        self.class_num = class_num\n        self.label_map = list()\n        self.clf_name = None\n        raise NotImplementedError\n\n    def fit(self, train_examples_x: np.ndarray, train_examples_y: np.ndarray, fit_params:dict):\n        raise NotImplementedError\n\n    def predict_proba(self, test_examples: np.ndarray, predict_prob_params: dict) -> np.ndarray:\n        raise NotImplementedError\n\n    def rebuild_prob_res(self, input_label_list, orig_prob_array):\n        new_prob_arary = np.zeros((orig_prob_array.shape[0], self.class_num))\n        for i, cls in enumerate(input_label_list):\n            new_prob_arary[:, cls] = orig_prob_array[:, i]\n\n        empty_cls_list = list()\n        for i in range(self.class_num):\n            if i not in input_label_list:\n                empty_cls_list.append(i)\n\n        for sample_i in range(orig_prob_array.shape[0]):\n            np_median_value = np.median(new_prob_arary[sample_i])\n            for empty_cls in empty_cls_list:\n                new_prob_arary[sample_i][empty_cls] = np_median_value\n\n        return new_prob_arary\n\n\nclass AdlOfflineClassifier(AdlClassifier):\n    def offline_fit(self, train_examples_x: np.ndarray, train_examples_y: np.ndarray, fit_params:dict):\n        raise NotImplementedError\n\n\nclass AdlOnlineClassifier(AdlClassifier):\n    def online_fit(self, train_examples_x: np.ndarray, train_examples_y: np.ndarray, fit_params:dict):\n        raise NotImplementedError\n\n\n'"
AutoDL_sample_code_submission/at_toolkit/interface/adl_feats_maker.py,0,"b'#coding:utf-8\n\nclass AbsFeatsMaker:\n    def __init__(self, feat_tool, feat_name):\n        self.feat_tool = feat_tool\n        self.feat_name = feat_name\n        self.feat_des = None\n        self.corr_cls_list = list()\n\n    def make_features(self, raw_data, feats_maker_params:dict):\n        raise NotImplementedError\n'"
AutoDL_sample_code_submission/at_toolkit/interface/adl_metadata.py,0,"b'# coding:utf-8\n\n\nclass AdlSpeechDMetadata:\n    def __init__(self, raw_metadata:dict):\n        self.train_num = raw_metadata.get(""train_num"")\n        self.test_num = raw_metadata.get(""test_num"")\n        self.class_num = raw_metadata.get(""class_num"")\n        self.train_minisamples_report = None\n\n    def init_train_minisamples_report(self, minispls_report:dict):\n        self.train_minisamples_report = minispls_report\n'"
AutoDL_sample_code_submission/at_toolkit/interface/adl_tfds_convertor.py,0,"b'#coding:utf-8\n\nclass AbsTfdsConvertor:\n    def init_train_tfds(self, train_tfds, train_num):\n        raise NotImplementedError\n\n    def init_test_tfds(self, test_tfds):\n        raise NotImplementedError\n\n    def get_train_np(self, take_size)-> dict:\n        raise NotImplementedError\n\n    def get_train_np_accm(self, take_size) -> dict:\n        raise NotImplementedError\n\n    def get_train_np_full(self) -> dict:\n        raise NotImplementedError\n\n    def get_test_np(self) -> dict:\n        raise NotImplementedError\n\n\n\n'"
AutoDL_sample_code_submission/Auto_Image/skeleton/data/__init__.py,0,"b'# -*- coding: utf-8 -*-\n# pylint: disable=wildcard-import\nfrom __future__ import absolute_import\n\nfrom .dataset import TFDataset, TransformDataset, prefetch_dataset\nfrom .dataloader import FixedSizeDataLoader, InfiniteSampler, PrefetchDataLoader\nfrom .transforms import *\nfrom . import augmentations\n'"
AutoDL_sample_code_submission/Auto_Image/skeleton/data/augmentations.py,0,"b""# code in this file is adpated from rpmcruz/autoaugment\n# https://github.com/rpmcruz/autoaugment/blob/master/transformations.py\nimport random\n\nimport PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\nimport numpy as np\n\nrandom_mirror = True\n\n\ndef ShearX(img, v):  # [-0.3, 0.3]\n    assert -0.3 <= v <= 0.3\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0))\n\n\ndef ShearY(img, v):  # [-0.3, 0.3]\n    assert -0.3 <= v <= 0.3\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0))\n\n\ndef TranslateX(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert -0.45 <= v <= 0.45\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    v = v * img.size[0]\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n\n\ndef TranslateY(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert -0.45 <= v <= 0.45\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    v = v * img.size[1]\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n\n\ndef TranslateXAbs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert 0 <= v <= 10\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n\n\ndef TranslateYAbs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert 0 <= v <= 10\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n\n\ndef Rotate(img, v):  # [-30, 30]\n    assert -30 <= v <= 30\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    return img.rotate(v)\n\n\ndef AutoContrast(img, _):\n    return PIL.ImageOps.autocontrast(img)\n\n\ndef Invert(img, _):\n    return PIL.ImageOps.invert(img)\n\n\ndef Equalize(img, _):\n    return PIL.ImageOps.equalize(img)\n\n\ndef Flip(img, _):  # not from the paper\n    return PIL.ImageOps.mirror(img)\n\n\ndef Solarize(img, v):  # [0, 256]\n    assert 0 <= v <= 256\n    return PIL.ImageOps.solarize(img, v)\n\n\ndef Posterize(img, v):  # [4, 8]\n    assert 4 <= v <= 8\n    v = int(v)\n    return PIL.ImageOps.posterize(img, v)\n\n\ndef Posterize2(img, v):  # [0, 4]\n    assert 0 <= v <= 4\n    v = int(v)\n    return PIL.ImageOps.posterize(img, v)\n\n\ndef Contrast(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Contrast(img).enhance(v)\n\n\ndef Color(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Color(img).enhance(v)\n\n\ndef Brightness(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Brightness(img).enhance(v)\n\n\ndef Sharpness(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n\n\ndef Cutout(img, v):  # [0, 60] => percentage: [0, 0.2]\n    assert 0.0 <= v <= 0.2\n    if v <= 0.:\n        return img\n\n    v = v * img.size[0]\n\n    return CutoutAbs(img, v)\n\n\n\ndef CutoutAbs(img, v):  # [0, 60] => percentage: [0, 0.2]\n    # assert 0 <= v <= 20\n    if v < 0:\n        return img\n    w, h = img.size\n    x0 = np.random.uniform(w)\n    y0 = np.random.uniform(h)\n\n    x0 = int(max(0, x0 - v / 2.))\n    y0 = int(max(0, y0 - v / 2.))\n    x1 = min(w, x0 + v)\n    y1 = min(h, y0 + v)\n\n    xy = (x0, y0, x1, y1)\n    color = (125, 123, 114)\n    # color = (0, 0, 0)\n    img = img.copy()\n    PIL.ImageDraw.Draw(img).rectangle(xy, color)\n    return img\n\n\ndef SamplePairing(imgs):  # [0, 0.4]\n    def f(img1, v):\n        i = np.random.choice(len(imgs))\n        img2 = PIL.Image.fromarray(imgs[i])\n        return PIL.Image.blend(img1, img2, v)\n\n    return f\n\n\ndef augment_list(for_autoaug=True):  #\n    l = [\n        (ShearX, -0.3, 0.3),  # 0\n        (ShearY, -0.3, 0.3),  # 1\n        (TranslateX, -0.45, 0.45),  # 2\n        (TranslateY, -0.45, 0.45),  # 3\n        (Rotate, -30, 30),  # 4\n        (AutoContrast, 0, 1),  # 5\n        (Invert, 0, 1),  # 6\n        (Equalize, 0, 1),  # 7\n        (Solarize, 0, 256),  # 8\n        (Posterize, 4, 8),  # 9\n        (Contrast, 0.1, 1.9),  # 10\n        (Color, 0.1, 1.9),  # 11\n        (Brightness, 0.1, 1.9),  # 12\n        (Sharpness, 0.1, 1.9),  # 13\n        (Cutout, 0, 0.2),  # 14\n        # (SamplePairing(imgs), 0, 0.4),  # 15\n    ]\n    if for_autoaug:\n        l += [\n            (CutoutAbs, 0, 20),  #\n            (Posterize2, 0, 4),  # 9\n            (TranslateXAbs, 0, 10),  # 9\n            (TranslateYAbs, 0, 10),  # 9\n        ]\n    return l\n\n\naugment_dict = {fn.__name__: (fn, v1, v2) for fn, v1, v2 in augment_list()}\n\n\ndef get_augment(name):\n    return augment_dict[name]\n\n\ndef apply_augment(img, name, level):\n    augment_fn, low, high = get_augment(name)\n    return augment_fn(img.copy(), level * (high - low) + low)\n\n\nclass Augmentation(object):\n    def __init__(self, policies):\n        self.policies = policies\n\n    def __call__(self, img):\n        for _ in range(1):\n            policy = random.choice(self.policies)\n            for name, pr, level in policy:\n                if random.random() > pr:\n                    continue\n                img = apply_augment(img, name, level)\n        return img\n\n\nfrom collections import defaultdict\n\nPARAMETER_MAX = 10\n\n\ndef remove_duplicates(policies):\n    s = set()\n    new_policies = []\n    for ops in policies:\n        key = []\n        for op in ops:\n            key.append(op[0])\n        key = '_'.join(key)\n        if key in s:\n            continue\n        else:\n            s.add(key)\n            new_policies.append(ops)\n\n    return new_policies\n\n\ndef float_parameter(level, maxval):\n    return float(level) * maxval / PARAMETER_MAX\n\n\ndef int_parameter(level, maxval):\n    return int(float_parameter(level, maxval))\n\n\ndef autoaug2arsaug(f):\n    def autoaug():\n        mapper = defaultdict(lambda: lambda x: x)\n        mapper.update({\n            'ShearX': lambda x: float_parameter(x, 0.3),\n            'ShearY': lambda x: float_parameter(x, 0.3),\n            'TranslateX': lambda x: int_parameter(x, 10),\n            'TranslateY': lambda x: int_parameter(x, 10),\n            'Rotate': lambda x: int_parameter(x, 30),\n            'Solarize': lambda x: 256 - int_parameter(x, 256),\n            'Posterize2': lambda x: 4 - int_parameter(x, 4),\n            'Contrast': lambda x: float_parameter(x, 1.8) + .1,\n            'Color': lambda x: float_parameter(x, 1.8) + .1,\n            'Brightness': lambda x: float_parameter(x, 1.8) + .1,\n            'Sharpness': lambda x: float_parameter(x, 1.8) + .1,\n            'CutoutAbs': lambda x: int_parameter(x, 20)\n        })\n\n        def low_high(name, prev_value):\n            _, low, high = get_augment(name)\n            return float(prev_value - low) / (high - low)\n\n        policies = f()\n        new_policies = []\n        for policy in policies:\n            new_policies.append([(name, pr, low_high(name, mapper[name](level))) for name, pr, level in policy])\n        return new_policies\n\n    return autoaug\n\n\n@autoaug2arsaug\ndef autoaug_policy():\n    exp0_0 = [\n        [('Invert', 0.1, 7), ('Contrast', 0.2, 6)],\n        [('Rotate', 0.7, 2), ('TranslateXAbs', 0.3, 9)],\n        [('Sharpness', 0.8, 1), ('Sharpness', 0.9, 3)],\n        [('ShearY', 0.5, 8), ('TranslateYAbs', 0.7, 9)],\n        [('AutoContrast', 0.5, 8), ('Equalize', 0.9, 2)]]\n    exp0_1 = [\n        [('Solarize', 0.4, 5), ('AutoContrast', 0.9, 3)],\n        [('TranslateYAbs', 0.9, 9), ('TranslateYAbs', 0.7, 9)],\n        [('AutoContrast', 0.9, 2), ('Solarize', 0.8, 3)],\n        [('Equalize', 0.8, 8), ('Invert', 0.1, 3)],\n        [('TranslateYAbs', 0.7, 9), ('AutoContrast', 0.9, 1)]]\n    exp0_2 = [\n        [('Solarize', 0.4, 5), ('AutoContrast', 0.0, 2)],\n        [('TranslateYAbs', 0.7, 9), ('TranslateYAbs', 0.7, 9)],\n        [('AutoContrast', 0.9, 0), ('Solarize', 0.4, 3)],\n        [('Equalize', 0.7, 5), ('Invert', 0.1, 3)],\n        [('TranslateYAbs', 0.7, 9), ('TranslateYAbs', 0.7, 9)]]\n    exp0_3 = [\n        [('Solarize', 0.4, 5), ('AutoContrast', 0.9, 1)],\n        [('TranslateYAbs', 0.8, 9), ('TranslateYAbs', 0.9, 9)],\n        [('AutoContrast', 0.8, 0), ('TranslateYAbs', 0.7, 9)],\n        [('TranslateYAbs', 0.2, 7), ('Color', 0.9, 6)],\n        [('Equalize', 0.7, 6), ('Color', 0.4, 9)]]\n    exp1_0 = [\n        [('ShearY', 0.2, 7), ('Posterize2', 0.3, 7)],\n        [('Color', 0.4, 3), ('Brightness', 0.6, 7)],\n        [('Sharpness', 0.3, 9), ('Brightness', 0.7, 9)],\n        [('Equalize', 0.6, 5), ('Equalize', 0.5, 1)],\n        [('Contrast', 0.6, 7), ('Sharpness', 0.6, 5)]]\n    exp1_1 = [\n        [('Brightness', 0.3, 7), ('AutoContrast', 0.5, 8)],\n        [('AutoContrast', 0.9, 4), ('AutoContrast', 0.5, 6)],\n        [('Solarize', 0.3, 5), ('Equalize', 0.6, 5)],\n        [('TranslateYAbs', 0.2, 4), ('Sharpness', 0.3, 3)],\n        [('Brightness', 0.0, 8), ('Color', 0.8, 8)]]\n    exp1_2 = [\n        [('Solarize', 0.2, 6), ('Color', 0.8, 6)],\n        [('Solarize', 0.2, 6), ('AutoContrast', 0.8, 1)],\n        [('Solarize', 0.4, 1), ('Equalize', 0.6, 5)],\n        [('Brightness', 0.0, 0), ('Solarize', 0.5, 2)],\n        [('AutoContrast', 0.9, 5), ('Brightness', 0.5, 3)]]\n    exp1_3 = [\n        [('Contrast', 0.7, 5), ('Brightness', 0.0, 2)],\n        [('Solarize', 0.2, 8), ('Solarize', 0.1, 5)],\n        [('Contrast', 0.5, 1), ('TranslateYAbs', 0.2, 9)],\n        [('AutoContrast', 0.6, 5), ('TranslateYAbs', 0.0, 9)],\n        [('AutoContrast', 0.9, 4), ('Equalize', 0.8, 4)]]\n    exp1_4 = [\n        [('Brightness', 0.0, 7), ('Equalize', 0.4, 7)],\n        [('Solarize', 0.2, 5), ('Equalize', 0.7, 5)],\n        [('Equalize', 0.6, 8), ('Color', 0.6, 2)],\n        [('Color', 0.3, 7), ('Color', 0.2, 4)],\n        [('AutoContrast', 0.5, 2), ('Solarize', 0.7, 2)]]\n    exp1_5 = [\n        [('AutoContrast', 0.2, 0), ('Equalize', 0.1, 0)],\n        [('ShearY', 0.6, 5), ('Equalize', 0.6, 5)],\n        [('Brightness', 0.9, 3), ('AutoContrast', 0.4, 1)],\n        [('Equalize', 0.8, 8), ('Equalize', 0.7, 7)],\n        [('Equalize', 0.7, 7), ('Solarize', 0.5, 0)]]\n    exp1_6 = [\n        [('Equalize', 0.8, 4), ('TranslateYAbs', 0.8, 9)],\n        [('TranslateYAbs', 0.8, 9), ('TranslateYAbs', 0.6, 9)],\n        [('TranslateYAbs', 0.9, 0), ('TranslateYAbs', 0.5, 9)],\n        [('AutoContrast', 0.5, 3), ('Solarize', 0.3, 4)],\n        [('Solarize', 0.5, 3), ('Equalize', 0.4, 4)]]\n    exp2_0 = [\n        [('Color', 0.7, 7), ('TranslateXAbs', 0.5, 8)],\n        [('Equalize', 0.3, 7), ('AutoContrast', 0.4, 8)],\n        [('TranslateYAbs', 0.4, 3), ('Sharpness', 0.2, 6)],\n        [('Brightness', 0.9, 6), ('Color', 0.2, 8)],\n        [('Solarize', 0.5, 2), ('Invert', 0.0, 3)]]\n    exp2_1 = [\n        [('AutoContrast', 0.1, 5), ('Brightness', 0.0, 0)],\n        # [('CutoutAbs', 0.2, 4), ('Equalize', 0.1, 1)],\n        [('Equalize', 0.7, 7), ('AutoContrast', 0.6, 4)],\n        [('Color', 0.1, 8), ('ShearY', 0.2, 3)],\n        [('ShearY', 0.4, 2), ('Rotate', 0.7, 0)]]\n    exp2_2 = [\n        [('ShearY', 0.1, 3), ('AutoContrast', 0.9, 5)],\n        # [('TranslateYAbs', 0.3, 6), ('CutoutAbs', 0.3, 3)],\n        [('Equalize', 0.5, 0), ('Solarize', 0.6, 6)],\n        [('AutoContrast', 0.3, 5), ('Rotate', 0.2, 7)],\n        [('Equalize', 0.8, 2), ('Invert', 0.4, 0)]]\n    exp2_3 = [\n        [('Equalize', 0.9, 5), ('Color', 0.7, 0)],\n        [('Equalize', 0.1, 1), ('ShearY', 0.1, 3)],\n        [('AutoContrast', 0.7, 3), ('Equalize', 0.7, 0)],\n        [('Brightness', 0.5, 1), ('Contrast', 0.1, 7)],\n        [('Contrast', 0.1, 4), ('Solarize', 0.6, 5)]]\n    exp2_4 = [\n        [('Solarize', 0.2, 3), ('ShearX', 0.0, 0)],\n        [('TranslateXAbs', 0.3, 0), ('TranslateXAbs', 0.6, 0)],\n        [('Equalize', 0.5, 9), ('TranslateYAbs', 0.6, 7)],\n        [('ShearX', 0.1, 0), ('Sharpness', 0.5, 1)],\n        [('Equalize', 0.8, 6), ('Invert', 0.3, 6)]]\n    exp2_5 = [\n        # [('AutoContrast', 0.3, 9), ('CutoutAbs', 0.5, 3)],\n        [('ShearX', 0.4, 4), ('AutoContrast', 0.9, 2)],\n        [('ShearX', 0.0, 3), ('Posterize2', 0.0, 3)],\n        [('Solarize', 0.4, 3), ('Color', 0.2, 4)],\n        [('Equalize', 0.1, 4), ('Equalize', 0.7, 6)]]\n    exp2_6 = [\n        [('Equalize', 0.3, 8), ('AutoContrast', 0.4, 3)],\n        [('Solarize', 0.6, 4), ('AutoContrast', 0.7, 6)],\n        [('AutoContrast', 0.2, 9), ('Brightness', 0.4, 8)],\n        [('Equalize', 0.1, 0), ('Equalize', 0.0, 6)],\n        [('Equalize', 0.8, 4), ('Equalize', 0.0, 4)]]\n    exp2_7 = [\n        [('Equalize', 0.5, 5), ('AutoContrast', 0.1, 2)],\n        [('Solarize', 0.5, 5), ('AutoContrast', 0.9, 5)],\n        [('AutoContrast', 0.6, 1), ('AutoContrast', 0.7, 8)],\n        [('Equalize', 0.2, 0), ('AutoContrast', 0.1, 2)],\n        [('Equalize', 0.6, 9), ('Equalize', 0.4, 4)]]\n    exp0s = exp0_0 + exp0_1 + exp0_2 + exp0_3\n    exp1s = exp1_0 + exp1_1 + exp1_2 + exp1_3 + exp1_4 + exp1_5 + exp1_6\n    exp2s = exp2_0 + exp2_1 + exp2_2 + exp2_3 + exp2_4 + exp2_5 + exp2_6 + exp2_7\n\n    return exp0s + exp1s + exp2s\n"""
AutoDL_sample_code_submission/Auto_Image/skeleton/data/dataloader.py,8,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport logging\n\nimport torch\n\n\n# LOGGER = logging.getLogger(__name__)\n\n\nclass FixedSizeDataLoader:\n    def __init__(self, dataset, steps, batch_size=1, shuffle=False, num_workers=0, pin_memory=False, drop_last=False,\n                 sampler=None):\n        sampler = InfiniteSampler(dataset, shuffle) if sampler is None else sampler\n        self.batch_size = batch_size\n        batch_size = 1 if batch_size is None else batch_size\n\n        self.steps = steps\n        self.dataset = dataset\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=batch_size,\n            sampler=sampler,\n            num_workers=num_workers,\n            pin_memory=pin_memory,\n            drop_last=drop_last\n        )\n\n    def __len__(self):\n        return self.steps\n\n    def __iter__(self):\n        if self.steps is not None:\n            for _, data in zip(range(self.steps), self.dataloader):\n                yield ([t[0] for t in data] if self.batch_size is None else data)\n        else:\n            for data in self.dataloader:\n                yield ([t[0] for t in data] if self.batch_size is None else data)\n\n\nclass InfiniteSampler(torch.utils.data.sampler.Sampler):\n    def __init__(self, data_source, shuffle=False):\n        self.data_source = data_source\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        n = len(self.data_source)\n        while True:\n            index_list = torch.randperm(n).tolist() if self.shuffle else list(range(n))\n            for idx in index_list:\n                yield idx\n\n    def __len__(self):\n        return len(self.data_source)\n\n\nclass PrefetchDataLoader:\n    def __init__(self, dataloader, device, half=False):\n        self.loader = dataloader\n        self.iter = None\n        self.device = device\n        self.dtype = torch.float16 if half else torch.float32\n        self.stream = torch.cuda.Stream()\n        self.next_data = None\n\n    def __len__(self):\n        return len(self.loader)\n\n    def async_prefech(self):\n        try:\n            self.next_data = next(self.iter)\n        except StopIteration:\n            self.next_data = None\n            return\n\n        with torch.cuda.stream(self.stream):\n            if isinstance(self.next_data, torch.Tensor):\n                self.next_data = self.next_data.to(dtype=self.dtype, device=self.device, non_blocking=True)\n            elif isinstance(self.next_data, (list, tuple)):\n                self.next_data = [\n                    t.to(dtype=self.dtype, device=self.device, non_blocking=True) if t.is_floating_point() else t.to(device=self.device, non_blocking=True) for t in self.next_data\n                ]\n\n    def __iter__(self):\n        self.iter = iter(self.loader)\n        self.async_prefech()\n        while self.next_data is not None:\n            torch.cuda.current_stream().wait_stream(self.stream)\n            data = self.next_data\n            self.async_prefech()\n            yield data\n'"
AutoDL_sample_code_submission/Auto_Image/skeleton/data/dataset.py,7,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport logging\n\nimport torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport tensorflow as tf\nfrom ..nn.modules.hooks import MoveToHook\n\n# LOGGER = logging.getLogger(__name__)\n\n\nclass TFDataset(Dataset):\n    def __init__(self, session, dataset, num_samples):\n        super(TFDataset, self).__init__()\n        self.session = session\n        self.dataset = dataset\n        self.num_samples = num_samples\n        self.next_element = None\n\n        self.reset()\n\n    def reset(self):\n        dataset = self.dataset\n        iterator = dataset.make_one_shot_iterator()\n        self.next_element = iterator.get_next()\n        return self\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, index):\n        session = self.session if self.session is not None else tf.Session()\n        try:\n            example, label = session.run(self.next_element)\n        except tf.errors.OutOfRangeError:\n            self.reset()\n            raise StopIteration\n\n        return example, label\n\n    def scan(self, samples=1000000, with_tensors=False, is_batch=False, device=None, half=False):\n        shapes, counts, tensors = [], [], []\n        labels = []\n        min_list, max_list = [], []\n        is_255 = False\n        for i in range(min(self.num_samples, samples)):\n            try:\n                example, label = self.__getitem__(i)\n                if i == 0 and np.mean(example) > 1:\n                    is_255 = True\n            except tf.errors.OutOfRangeError:\n                break\n            except StopIteration:\n                break\n\n            shape = example.shape\n            count = np.sum(label, axis=None if not is_batch else -1)\n            labels.append(label)\n\n            shapes.append(shape)\n            counts.append(count)\n            min_list.append(np.min(example))\n            max_list.append(np.max(example))\n\n            if with_tensors:\n                example = torch.Tensor(example)\n                label = torch.Tensor(label)\n\n                example.data = example.data.to(device=device)\n                if half and example.is_floating_point():\n                    example.data = example.data.half()\n\n                label.data = label.data.to(device=device)\n                if half and label.is_floating_point():\n                    label.data = label.data.half()\n\n                tensors.append([example, label])\n\n        shapes = np.array(shapes)\n        counts = np.array(counts) if not is_batch else np.concatenate(counts)\n\n        labels = np.array(labels) if not is_batch else np.concatenate(labels)\n        num_samples = labels.shape[0]\n        labels = np.sum(labels, axis=0)\n        zero_count = sum(labels == 0)\n\n        pos_weights = (num_samples - labels + 10) / (labels + 10)\n        info = {\n            'count': len(counts),\n            'is_multiclass': counts.max() > 1.0,\n            'is_video': int(np.median(shapes, axis=0)[0]) > 1,\n            'example': {\n                'shape': [int(v) for v in np.median(shapes, axis=0)],\n                'shape_avg': [int(v) for v in np.average(shapes, axis=0)],\n                'value': {'min': min(min_list), 'max': max(max_list)},\n                'is_255': is_255\n            },\n            'label': {\n                'min': counts.min(),\n                'max': counts.max(),\n                'average': counts.mean(),\n                'median': np.median(counts),\n                'zero_count': zero_count,\n                'pos_weights': pos_weights\n            },\n\n        }\n\n        if with_tensors:\n            return info, tensors\n        return info\n\n\nclass TransformDataset(Dataset):\n    def __init__(self, dataset, transform=None, index=None):\n        self.dataset = dataset\n        self.transform = transform\n        self.index = index\n\n    def __getitem__(self, index):\n        tensors = self.dataset[index]\n        tensors = list(tensors)\n\n        if self.transform is not None:\n            if self.index is None:\n                tensors = self.transform(*tensors)\n            else:\n                tensors[self.index] = self.transform(tensors[self.index])\n\n        return tuple(tensors)\n\n    def __len__(self):\n        return len(self.dataset)\n\n\ndef prefetch_dataset(dataset, num_workers=4, batch_size=32, device=None, half=False):\n    if isinstance(dataset, list) and isinstance(dataset[0], torch.Tensor):\n        tensors = dataset\n    else:\n        dataloader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False, drop_last=False,\n            num_workers=num_workers, pin_memory=False\n        )\n        tensors = [t for t in dataloader]\n        tensors = [torch.cat(t, dim=0) for t in zip(*tensors)]\n\n    if device is not None:\n        tensors = [t.to(device=device) for t in tensors]\n    if half:\n        tensors = [t.half() if t.is_floating_point() else t for t in tensors]\n\n    return torch.utils.data.TensorDataset(*tensors)\n"""
AutoDL_sample_code_submission/Auto_Image/skeleton/data/transforms.py,3,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport os\nimport logging\nimport hashlib\nimport random\n\nimport numpy as np\nimport torch\n\n\n# LOGGER = logging.getLogger(__name__)\n\n\nclass Identity:\n    def __call__(self, image):\n        return image\n\n\nclass Normalize:\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, image):\n        return (image - self.mean) / self.std\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\n\nclass RandomFlip:\n    def __init__(self, p=0.5, dims=[-1]):\n        self.p = p\n        self.dims = dims\n\n    def __call__(self, tensor):\n        if random.random() < self.p:\n            tensor = torch.flip(tensor, dims=self.dims)\n        return tensor\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(p={})'.format(self.p)\n\n\nclass Crop:\n    def __init__(self, height, width):\n        self.height = height\n        self.width = width\n\n    def __call__(self, image):\n        h, w = image.shape[-2:]\n\n        y = np.random.randint(h - self.height)\n        x = np.random.randint(w - self.width)\n\n        return image[:, y:y+self.height, x:x+self.width]\n\n\nclass Cutout:\n    def __init__(self, height, width):\n        self.height = height\n        self.width = width\n\n    def __call__(self, image):\n        if self.height > 0 or self.width > 0:\n            if isinstance(image, torch.Tensor):\n                mask = torch.ones_like(image)\n            elif isinstance(image, np.ndarray):\n                mask = np.ones_like(image)\n            else:\n                raise NotImplementedError('support only tensor or numpy array')\n\n            h, w = image.shape[-2:]\n\n            y = np.random.randint(h)\n            x = np.random.randint(w)\n\n            y1 = np.clip(y - self.height // 2, 0, h)\n            y2 = np.clip(y + self.height // 2, 0, h)\n            x1 = np.clip(x - self.width // 2, 0, w)\n            x2 = np.clip(x + self.width // 2, 0, w)\n\n            if len(mask.shape) == 3:\n                mask[:, y1: y2, x1: x2] = 0.\n            else:\n                mask[:, :, y1: y2, x1: x2] = 0.\n            image *= mask\n        return image\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(height={0}, width={1})'.format(self.height, self.width)\n\n\nclass RandomHorizontalFlip:\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, img):\n        if random.random() < self.p:\n            img = np.flip(img, axis=-1).copy()\n        return img\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(p={})'.format(self.p)\n\n\nclass Writer:\n    def __init__(self, path, format='jpg'):\n        self.path = path\n        self.format = format\n        os.makedirs(self.path, exist_ok=True)\n\n    def __call__(self, image):\n        filename = hashlib.md5(image.tobytes()).hexdigest()\n        path = self.path + '/' + filename + '.' + self.format\n        image.save(path)\n        return image\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(path={0}, format={1})'.format(self.path, self.format)\n"""
AutoDL_sample_code_submission/Auto_Image/skeleton/nn/__init__.py,0,b'# -*- coding: utf-8 -*-\n# pylint: disable=wildcard-import\nfrom __future__ import absolute_import\n\nfrom .modules import *\n'
AutoDL_sample_code_submission/Auto_Image/skeleton/optim/__init__.py,0,b'# -*- coding: utf-8 -*-\n# pylint: disable=wildcard-import\nfrom __future__ import absolute_import\n\nfrom .optimizers import *\nfrom .scheduler import *\nfrom .sgdw import SGDW\n'
AutoDL_sample_code_submission/Auto_Image/skeleton/optim/optimizers.py,1,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport logging\n\nimport torch\n\n\n# LOGGER = logging.getLogger(__name__)\n\n\nclass ScheduledOptimizer:\n    def __init__(self, parameters, optimizer, steps_per_epoch=1, clip_grad_max_norm=None, tag=None, **opt_params):\n        self.epoch = 0.0\n        self.tag = tag\n        self._parameters = parameters\n        self.steps_per_epoch = steps_per_epoch\n        self.clip_grad_max_norm = clip_grad_max_norm\n        self._opt_params = opt_params\n\n        self._optimizer = optimizer(parameters, **self.update_params(0))\n\n    def update_params(self, epoch=None, **kwargs):\n        return {\n            k: v(self.epoch if epoch is None else epoch, **kwargs) if callable(v) else v\n            for k, v in self._opt_params.items()\n        }\n\n    def update(self, epoch=None, **kwargs):\n        opt_pararms = self.update_params(epoch, **kwargs)\n        self._optimizer.param_groups[0].update(**opt_pararms)\n\n        for key, value in opt_pararms.items():\n            tag = self.tag if self.tag is not None else 'train'\n            if not isinstance(value, (float, int)):\n                continue\n        return self\n\n    def step(self, epoch=None):\n        self.epoch = self.epoch + (1.0 / self.steps_per_epoch) if epoch is None else epoch\n        if self.clip_grad_max_norm is not None and self.clip_grad_max_norm > 0.0:\n            torch.nn.utils.clip_grad_norm_(self._parameters, self.clip_grad_max_norm, norm_type=1)\n        self._optimizer.step()\n\n    def state_dict(self):\n        state_dict = self._optimizer.state_dict()\n        state_dict.update({'epoch': self.epoch})\n        return state_dict\n\n    def load_state_dict(self, state_dict):\n        self.epoch = state_dict.pop('epoch')\n        return self._optimizer.load_state_dict(state_dict)\n\n    def zero_grad(self):\n        return self._optimizer.zero_grad()\n\n    def get_learning_rate(self):\n        return self._optimizer.param_groups[0]['lr']\n\n    def __getattr__(self, item):\n        return getattr(self._optimizer, item)\n"""
AutoDL_sample_code_submission/Auto_Image/skeleton/optim/scheduler.py,0,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport math\nimport logging\n\n# LOGGER = logging.getLogger(__name__)\n\n\ndef gradual_warm_up(scheduler, warm_up_epoch, multiplier):\n    def schedule(e, **kwargs):\n        lr = scheduler(e, **kwargs)\n        lr = lr * ((multiplier - 1.0) * min(e, warm_up_epoch) / warm_up_epoch + 1)\n        return lr\n\n    return schedule\n\n\ndef get_discrete_epoch(scheduler):\n    def schedule(e, **kwargs):\n        return scheduler(int(e), **kwargs)\n\n    return schedule\n\n\ndef get_change_scale(scheduler, init_scale=1.0):\n    def schedule(e, scale=None, **kwargs):\n        lr = scheduler(e, **kwargs)\n        return lr * (scale if scale is not None else init_scale)\n\n    return schedule\n\n\ndef get_step_scheduler(init_lr, step_size, gamma=0.1):\n    def schedule(e, **kwargs):\n        lr = init_lr * gamma ** (e // step_size)\n        return lr\n\n    return schedule\n\n\ndef get_cosine_scheduler(init_lr, maximum_epoch, eta_min=0):\n    def schedule(e, **kwargs):\n        maximum = kwargs['maximum_epoch'] if 'maximum_epoch' in kwargs else maximum_epoch\n        lr = eta_min + (init_lr - eta_min) * (1 + math.cos(math.pi * e / maximum)) / 2\n        return lr\n\n    return schedule\n\n\nclass PlateauScheduler:\n    def __init__(self, init_lr, factor=0.1, patience=10, threshold=1e-4):\n        self.init_lr = init_lr\n        self.factor = factor\n        self.patience = patience\n        self.threshold = threshold\n\n        self.curr_lr = init_lr\n        self.best_loss = 10000\n        self.prev_epoch = 0\n        self.num_bad_epochs = 0\n\n    def __call__(self, epoch, loss=None, **kwargs):\n        if loss is None:\n            loss = self.best_loss\n\n        if self.best_loss - self.threshold > loss:\n            self.num_bad_epochs = 0\n            self.best_loss = loss\n        else:\n            self.num_bad_epochs += epoch - self.prev_epoch\n\n        if self.num_bad_epochs >= self.patience:\n            self.num_bad_epochs = 0\n            self.curr_lr *= self.factor\n\n        self.prev_epoch = epoch\n        return self.curr_lr\n\n\ndef get_reduce_on_plateau_scheduler(init_lr, factor=0.1, patience=10, threshold=1e-4, min_lr=0, metric_name='metric'):\n    class Schedule:\n        def __init__(self):\n            self.num_bad_epochs = 0\n            self.lr = init_lr\n            self.best = None\n            self.metric_name = metric_name\n\n        def __call__(self, e, **kwargs):\n            if 'diverge_scale' in kwargs:  # reduce learning rate while divergence\n                self.lr *= kwargs['diverge_scale']\n            if self.metric_name not in kwargs:\n                return self.lr\n            metric = kwargs[self.metric_name]\n\n\n            if self.best is None or self.best > metric:\n                self.best = metric - threshold\n                self.num_bad_epochs = 0\n            else:\n                self.num_bad_epochs += 1\n\n            if self.num_bad_epochs > patience:\n                self.num_bad_epochs = 0\n                lr = max(min_lr, self.lr * factor)\n                self.lr = lr\n            return self.lr\n\n    return Schedule()\n"""
AutoDL_sample_code_submission/Auto_Image/skeleton/optim/sgdw.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\n\nimport torch\n\n\nclass SGDW(torch.optim.SGD):\n    """"""\n    Decoupled Weight Decay Regularization\n    reference: https://arxiv.org/abs/1711.05101\n    """"""\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            weight_decay = group[\'weight_decay\']\n            momentum = group[\'momentum\']\n            dampening = group[\'dampening\']\n            nesterov = group[\'nesterov\']\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                d_p = p.grad.data\n                if momentum != 0:\n                    param_state = self.state[p]\n                    if \'momentum_buffer\' not in param_state:\n                        buf = param_state[\'momentum_buffer\'] = torch.zeros_like(p.data)\n                        buf.mul_(momentum).add_(d_p)\n                    else:\n                        buf = param_state[\'momentum_buffer\']\n                        buf.mul_(momentum).add_(1 - dampening, d_p)\n                    if nesterov:\n                        d_p = d_p.add(momentum, buf)\n                    else:\n                        d_p = buf\n\n                # decoupled weight decay\n                if weight_decay != 0:\n                    d_p.add_(weight_decay, p.data)\n\n                p.data.add_(-group[\'lr\'], d_p)\n\n        return loss\n'"
AutoDL_sample_code_submission/Auto_Image/skeleton/projects/__init__.py,0,b'# -*- coding: utf-8 -*-\n# pylint: disable=wildcard-import\nfrom __future__ import absolute_import\n\nfrom .api import *\nfrom .others import *\nfrom .logic import *'
AutoDL_sample_code_submission/Auto_Image/skeleton/projects/logic.py,2,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport tensorflow as tf\nfrom .api import Model\nfrom .others import *\nimport skeleton\n# LOGGER = get_logger(__name__)\n\n\nclass LogicModel(Model):\n    def __init__(self, metadata, session=None):\n        super(LogicModel, self).__init__(metadata)\n\n\n        test_metadata_filename = self.metadata.get_dataset_name().replace('train', 'test') + '/metadata.textproto'\n        self.num_test = [int(line.split(':')[1]) for line in open(test_metadata_filename, 'r').readlines()[:3] if\n                         'sample_count' in line][0]\n\n        self.timers = {\n            'train': skeleton.utils.Timer(),\n            'test': skeleton.utils.Timer()\n        }\n        self.info = {\n            'dataset': {\n                'path': self.metadata.get_dataset_name(),\n                'shape': self.metadata.get_tensor_size(0),\n                'size': self.metadata.size(),\n                'num_class': self.metadata.get_output_size()\n            },\n            'loop': {\n                'epoch': 0,\n                'test': 0,\n                'best_score': 0.0\n            },\n            'condition': {\n                'first': {\n                    'train': True,\n                    'valid': True,\n                    'test': True\n                }\n            },\n            'terminate': False\n        }\n\n        self.hyper_params = {\n            'optimizer': {\n                'lr': 0.05,\n            },\n            'dataset': {\n                'train_info_sample': 256,\n                'cv_valid_ratio': 0.1,\n                'max_valid_count': 256,\n\n                'max_size': 64,\n                'base': 16,  #\n                'max_times': 8,\n\n                'enough_count': {\n                    'image': 10000,\n                },\n\n                'batch_size': 32,\n                'steps_per_epoch': 30,\n                'max_epoch': 1000,  #\n                'batch_size_test': 64,\n            },\n            'checkpoints': {\n                'keep': 50\n            },\n            'conditions': {\n                'score_type': 'auc',\n                'early_epoch': 1,\n                'skip_valid_score_threshold': 0.90,  #\n                'skip_valid_after_test': min(10, max(3, int(self.info['dataset']['size'] // 1000))),\n                'test_after_at_least_seconds': 1,\n                'test_after_at_least_seconds_max': 90,\n                'test_after_at_least_seconds_step': 2,\n\n                'threshold_valid_score_diff': 0.001,\n                'threshold_valid_best_score': 0.997,\n                'max_inner_loop_ratio': 0.2,\n                'min_lr': 1e-6,\n                'use_fast_auto_aug': True\n            }\n        }\n        self.checkpoints = []\n\n        self.build()\n\n        self.dataloaders = {\n            'train': None,\n            'valid': None,\n            'test': None\n        }\n        self.is_skip_valid = True\n        self.last_predict = None\n        self.switch = False\n        self.need_switch = True\n        self.pre_data = []\n\n    def __repr__(self):\n        return '\\n---------[{0}]---------\\ninfo:{1}\\nparams:{2}\\n---------- ---------'.format(\n            self.__class__.__name__,\n            self.info, self.hyper_params\n        )\n\n    def build(self):\n        raise NotImplementedError\n\n    def update_model(self):\n        pass\n\n    def epoch_train(self, epoch, train):\n        raise NotImplementedError\n\n    def epoch_valid(self, epoch, valid):\n        raise NotImplementedError\n\n    def skip_valid(self, epoch):\n        raise NotImplementedError\n\n    def prediction(self, dataloader):\n        raise NotImplementedError\n\n    def adapt(self):\n        raise NotImplementedError\n\n    def is_multiclass(self):\n        return self.info['dataset']['sample']['is_multiclass']\n\n    def build_or_get_train_dataloader(self, dataset):\n        if not self.info['condition']['first']['train']:\n            return self.build_or_get_dataloader('train')\n\n        num_images = self.info['dataset']['size']\n\n        num_valids = int(min(num_images * self.hyper_params['dataset']['cv_valid_ratio'],\n                             self.hyper_params['dataset']['max_valid_count']))\n        num_trains = num_images - num_valids\n\n        num_samples = self.hyper_params['dataset']['train_info_sample']\n        sample = dataset.take(num_samples).prefetch(buffer_size=num_samples)\n        train = skeleton.data.TFDataset(self.session, sample, num_samples)\n        self.info['dataset']['sample'] = train.scan(samples=num_samples)\n        del train\n        del sample\n\n        times, height, width, channels = self.info['dataset']['sample']['example']['shape']\n        values = self.info['dataset']['sample']['example']['value']\n        aspect_ratio = width / height\n\n        if aspect_ratio > 2 or 1. / aspect_ratio > 2:\n            self.hyper_params['dataset']['max_size'] *= 2\n            self.need_switch = False\n        size = [min(s, self.hyper_params['dataset']['max_size']) for s in [height, width]]\n\n        if aspect_ratio > 1:\n            size[0] = size[1] / aspect_ratio\n        else:\n            size[1] = size[0] * aspect_ratio\n\n        if width <= 32 and height <= 32:\n            input_shape = [times, height, width, channels]\n        else:\n            fit_size_fn = lambda x: int(x / self.hyper_params['dataset']['base'] + 0.8) * self.hyper_params['dataset'][\n                'base']\n            size = list(map(fit_size_fn, size))\n            min_times = min(times, self.hyper_params['dataset']['max_times'])\n            input_shape = [fit_size_fn(min_times) if min_times > self.hyper_params['dataset'][\n                'base'] else min_times] + size + [channels]\n        self.input_shape = input_shape\n\n\n        self.hyper_params['dataset']['input'] = input_shape\n\n        num_class = self.info['dataset']['num_class']\n        batch_size = self.hyper_params['dataset']['batch_size']\n        step_num = self.hyper_params['dataset']['steps_per_epoch']\n        if num_class > batch_size / 2:\n            self.hyper_params['dataset']['batch_size'] = batch_size * 2\n        preprocessor1 = get_tf_resize(input_shape[1], input_shape[2], times=input_shape[0], min_value=values['min'],\n                                      max_value=values['max'])\n\n        dataset = dataset.map(\n            lambda *x: (preprocessor1(x[0]), x[1]),\n            num_parallel_calls=tf.data.experimental.AUTOTUNE\n        )\n        must_shuffle = self.info['dataset']['sample']['label']['zero_count'] / self.info['dataset']['num_class'] >= 0.5\n        enough_count = self.hyper_params['dataset']['enough_count']['image']\n        if must_shuffle or num_images < enough_count:\n            dataset = dataset.shuffle(buffer_size=4 * num_valids, reshuffle_each_iteration=False)\n\n        train = dataset.skip(num_valids)\n        valid = dataset.take(num_valids)\n        self.datasets = {\n            'train': train,\n            'valid': valid,\n            'num_trains': num_trains,\n            'num_valids': num_valids\n        }\n        return self.build_or_get_dataloader('train', self.datasets['train'], num_trains)\n\n    def build_or_get_dataloader(self, mode, dataset=None, num_items=0):\n        if mode in self.dataloaders and self.dataloaders[mode] is not None:\n            return self.dataloaders[mode]\n\n        enough_count = self.hyper_params['dataset']['enough_count']['image']\n\n        values = self.info['dataset']['sample']['example']['value']\n        if mode == 'train':\n            batch_size = self.hyper_params['dataset']['batch_size']\n            preprocessor = get_tf_to_tensor(is_random_flip=True)\n\n            if num_items < enough_count:\n                dataset = dataset.cache()\n\n            dataset = dataset.repeat()\n            dataset = dataset.map(\n                lambda *x: (preprocessor(x[0]), x[1]),\n                num_parallel_calls=tf.data.experimental.AUTOTUNE\n            )\n            dataset = dataset.prefetch(buffer_size=batch_size * 8)\n\n            dataset = skeleton.data.TFDataset(self.session, dataset, num_items)\n            transform = tv.transforms.Compose([\n            ])\n            dataset = skeleton.data.TransformDataset(dataset, transform, index=0)\n\n            self.dataloaders['train'] = skeleton.data.FixedSizeDataLoader(\n                dataset,\n                steps=self.hyper_params['dataset']['steps_per_epoch'],\n                batch_size=batch_size,\n                shuffle=False, drop_last=True, num_workers=0, pin_memory=False\n            )\n        elif mode in ['valid', 'test']:\n            batch_size = self.hyper_params['dataset']['batch_size_test']\n            input_shape = self.hyper_params['dataset']['input']\n\n            preprocessor2 = get_tf_to_tensor(is_random_flip=False)\n            if mode == 'valid':\n                preprocessor = preprocessor2\n            else:\n                preprocessor1 = get_tf_resize(input_shape[1], input_shape[2], times=input_shape[0],\n                                              min_value=values['min'], max_value=values['max'])\n                preprocessor = lambda *tensor: preprocessor2(preprocessor1(*tensor))\n\n            tf_dataset = dataset.apply(\n                tf.data.experimental.map_and_batch(\n                    map_func=lambda *x: (preprocessor(x[0]), x[1]),\n                    batch_size=batch_size,\n                    drop_remainder=False,\n                    num_parallel_calls=tf.data.experimental.AUTOTUNE\n                )\n            ).prefetch(buffer_size=8)\n\n            dataset = skeleton.data.TFDataset(self.session, tf_dataset, num_items)\n\n            self.info['dataset'][mode], tensors = dataset.scan(\n                with_tensors=True, is_batch=True,\n                device=self.device, half=self.is_half\n            )\n            tensors = [torch.cat(t, dim=0) for t in zip(*tensors)]\n\n            del tf_dataset\n            del dataset\n            dataset = skeleton.data.prefetch_dataset(tensors)\n\n            if 'valid' == mode:\n                transform = tv.transforms.Compose([\n                ])\n                dataset = skeleton.data.TransformDataset(dataset, transform, index=0)\n\n            self.dataloaders[mode] = torch.utils.data.DataLoader(\n                dataset,\n                batch_size=self.hyper_params['dataset']['batch_size_test'],\n                shuffle=False, drop_last=False, num_workers=0, pin_memory=False\n            )\n\n            self.info['condition']['first'][mode] = False\n\n        return self.dataloaders[mode]\n\n    def update_condition(self, metrics=None):\n        self.info['condition']['first']['train'] = False\n        self.info['loop']['epoch'] += 1\n        self.last_checkpoint = metrics\n        metrics.update({'epoch': self.info['loop']['epoch']})\n        self.checkpoints.append(metrics)\n\n        indices = np.argsort(\n            np.array([v['valid']['score'] for v in self.checkpoints] if len(self.checkpoints) > 0 else [0]))\n        indices = sorted(indices[::-1][:self.hyper_params['checkpoints']['keep']])\n        self.checkpoints = [self.checkpoints[i] for i in indices]\n\n    def handle_divergence(self):\n        self.model.load_state_dict(self.last_checkpoint['model'])\n        self.optimizer.update(diverge_scale=0.25)\n\n    def break_train_loop_condition(self, remaining_time_budget=None, inner_epoch=1):\n        consume = inner_epoch * self.timers['train'].step_time\n\n        best_idx = np.argmax(np.array([c['valid']['score'] for c in self.checkpoints]))\n        best_epoch = self.checkpoints[best_idx]['epoch']\n        best_loss = self.checkpoints[best_idx]['valid']['loss']\n        best_score = self.checkpoints[best_idx]['valid']['score']\n        lr = self.optimizer.get_learning_rate()\n\n\n        if self.info['loop']['epoch'] <= self.hyper_params['conditions']['early_epoch']:\n            return True\n\n        if best_score > self.hyper_params['conditions']['threshold_valid_best_score']:\n            return True\n\n        if consume > self.hyper_params['conditions']['test_after_at_least_seconds'] and \\\n                self.checkpoints[best_idx]['epoch'] > self.info['loop']['epoch'] - inner_epoch and \\\n                best_score > self.info['loop']['best_score'] * 1.001:\n            self.hyper_params['conditions']['test_after_at_least_seconds'] = min(\n                self.hyper_params['conditions']['test_after_at_least_seconds_max'],\n                self.hyper_params['conditions']['test_after_at_least_seconds'] + self.hyper_params['conditions'][\n                    'test_after_at_least_seconds_step']\n            )\n\n            self.info['loop']['best_score'] = best_score\n            return True\n\n        if lr < self.hyper_params['conditions']['min_lr']:\n            return True\n\n        early_term_budget = 3 * 60\n        expected_more_time = (self.timers['test'].step_time + (self.timers['train'].step_time * 2)) * 1.5\n        if remaining_time_budget is not None and \\\n                remaining_time_budget - early_term_budget < expected_more_time:\n\n            return True\n\n        if self.info['loop']['epoch'] >= 20 and \\\n                inner_epoch > self.hyper_params['dataset']['max_epoch'] * self.hyper_params['conditions'][\n            'max_inner_loop_ratio']:\n            return True\n\n        return False\n\n    def terminate_train_loop_condition(self, remaining_time_budget=None, inner_epoch=0):\n        early_term_budget = 3 * 60\n        expected_more_time = (self.timers['test'].step_time + (self.timers['train'].step_time * 2)) * 1.5\n        if remaining_time_budget is not None and \\\n                remaining_time_budget - early_term_budget < expected_more_time:\n\n            self.info['terminate'] = True\n            self.done_training = True\n            return True\n\n        best_idx = np.argmax(np.array([c['valid']['score'] for c in self.checkpoints]))\n        best_score = self.checkpoints[best_idx]['valid']['score']\n        if best_score > self.hyper_params['conditions']['threshold_valid_best_score']:\n            done = True if self.info['terminate'] else False\n            self.info['terminate'] = True\n            self.done_training = True\n            return True\n\n        scores = [c['valid']['score'] for c in self.checkpoints]\n        diff = (max(scores) - min(scores)) * (1 - max(scores))\n        threshold = self.hyper_params['conditions']['threshold_valid_score_diff']\n        if 1e-8 < diff and diff < threshold and \\\n                self.info['loop']['epoch'] >= 20:\n            done = True if self.info['terminate'] else False\n            self.info['terminate'] = True\n            self.done_training = done\n            return True\n\n        if self.optimizer.get_learning_rate() < self.hyper_params['conditions']['min_lr']:\n            done = True if self.info['terminate'] else False\n            self.info['terminate'] = True\n            self.done_training = done\n            return True\n\n        if self.info['loop']['epoch'] >= 20 and \\\n                inner_epoch > self.hyper_params['dataset']['max_epoch'] * self.hyper_params['conditions'][\n            'max_inner_loop_ratio']:\n            done = True if self.info['terminate'] else False\n            self.info['terminate'] = True\n            self.done_training = done\n            return True\n\n        return False\n\n    def get_total_time(self):\n        return sum([self.timers[key].total_time for key in self.timers.keys()])\n\n    def train(self, dataset, remaining_time_budget=None):\n        self.timers['train']('outer_start', exclude_total=True, reset_step=True)\n\n        train_dataloader = self.build_or_get_train_dataloader(dataset)\n        if self.info['condition']['first']['train']:\n            self.update_model()\n        self.timers['train']('build_dataset')\n\n        inner_epoch = 0\n        last_metrics = {\n            'loss': 0,\n            'score': 0,\n        }\n        last_skip_valid = False  # do not perform continuous validation\n        while True:\n            if self.info['loop']['epoch'] == 2 and self.need_switch:\n                self.model = self.model_9\n                self.model_pred = self.model_9_pred\n                self.switch = True\n                self.update_model()\n            inner_epoch += 1\n            remaining_time_budget -= self.timers['train'].step_time\n\n            self.timers['train']('start', reset_step=True)\n            train_metrics = self.epoch_train(self.info['loop']['epoch'], train_dataloader)\n            self.timers['train']('train')\n            if last_metrics['score'] - train_metrics['score'] > 0.1 and train_metrics['score'] < 0.95:\n                self.handle_divergence()\n            last_metrics = dict(train_metrics)\n            train_score = np.min([c['train']['score'] for c in self.checkpoints[-20:] + [{'train': train_metrics}]])\n            if last_skip_valid and (train_score > self.hyper_params['conditions']['skip_valid_score_threshold'] or \\\n                                    self.info['loop']['test'] >= self.hyper_params['conditions'][\n                                        'skip_valid_after_test']):\n                is_first = self.info['condition']['first']['valid']\n                valid_dataloader = self.build_or_get_dataloader('valid', self.datasets['valid'],\n                                                                self.datasets['num_valids'])\n                self.timers['train']('valid_dataset', exclude_step=is_first)\n\n                valid_metrics = self.epoch_valid(self.info['loop']['epoch'], valid_dataloader)\n                self.is_skip_valid = False\n                last_skip_valid = False\n            else:\n                valid_metrics = self.skip_valid(self.info['loop']['epoch'])\n                self.is_skip_valid = True\n                last_skip_valid = True\n            self.timers['train']('valid')\n\n            metrics = {\n                'epoch': self.info['loop']['epoch'],\n                'model': self.model.state_dict().copy(),\n                'train': train_metrics,\n                'valid': valid_metrics,\n            }\n\n            self.update_condition(metrics)\n            self.timers['train']('adapt', exclude_step=True)\n\n\n            self.hyper_params['dataset']['max_epoch'] = self.info['loop']['epoch'] + remaining_time_budget // \\\n                                                        self.timers['train'].step_time\n\n            if self.break_train_loop_condition(remaining_time_budget, inner_epoch):\n                break\n\n            self.timers['train']('end')\n\n        remaining_time_budget -= self.timers['train'].step_time\n        self.terminate_train_loop_condition(remaining_time_budget, inner_epoch)\n\n        if not self.done_training:\n            self.adapt(remaining_time_budget)\n\n        self.timers['train']('outer_end')\n\n\n    def test(self, dataset, remaining_time_budget=None):\n        self.timers['test']('start', exclude_total=True, reset_step=True)\n        is_first = self.info['condition']['first']['test']\n        self.info['loop']['test'] += 1\n\n        dataloader = self.build_or_get_dataloader('test', dataset, self.num_test)\n        self.timers['test']('build_dataset', reset_step=is_first)\n\n        rv = self.prediction(dataloader)\n        self.timers['test']('end')\n\n        return rv\n"""
AutoDL_sample_code_submission/Auto_Image/skeleton/projects/others.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport sys\nimport os\nimport logging\nfrom functools import reduce\nimport random\nimport tensorflow as tf\nimport torchvision as tv\nimport numpy as np\nimport torch\n\n\ndef get_logger(name, stream=sys.stderr):\n    formatter = logging.Formatter(fmt=\'[%(asctime)s %(levelname)s %(filename)s] %(message)s\')\n\n    handler = logging.StreamHandler(stream)\n    handler.setFormatter(formatter)\n\n    logger = logging.getLogger(name)\n    level = logging.INFO if os.environ.get(\'LOG_LEVEL\', \'INFO\') == \'INFO\' else logging.DEBUG\n    logger.setLevel(level)\n    logger.addHandler(handler)\n    return logger\n\n\n# LOGGER = get_logger(__name__)\n\n\ndef get_tf_resize(height, width, times=1, min_value=0.0, max_value=1.0):\n    def preprocessor(tensor):\n        in_times, in_height, in_width, in_channels = tensor.get_shape()\n\n        if width == in_width and height == in_height:\n            pass\n        else:\n            tensor = tf.image.resize_images(tensor, (height, width), method=tf.image.ResizeMethod.BICUBIC)\n\n        if times != in_times or times > 1:\n            tensor = tf.reshape(tensor, [-1, height * width, in_channels])\n            tensor = tf.image.resize_images(tensor, (times, height * width),\n                                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n            tensor = tf.reshape(tensor, [times, height, width, in_channels])\n\n        if times == 1:\n            tensor = tensor[int(times // 2)]\n\n        delta = max_value - min_value\n        if delta < 0.9 or delta > 1.1 or min_value < -0.1 or min_value > 0.1:\n            tensor = (tensor - min_value) / delta\n\n        return tensor\n\n    return preprocessor\n\n\ndef get_tf_to_tensor(is_random_flip=True):\n    def preprocessor(tensor):\n        if is_random_flip:\n            tensor = tf.image.random_flip_left_right(tensor)\n\n        dims = len(tensor.shape)\n        if dims == 3:\n            tensor = tf.transpose(tensor, perm=[2, 0, 1])\n        elif dims == 4:\n            tensor = tf.transpose(tensor, perm=[0, 3, 1, 2])\n        return tensor\n\n    return preprocessor\n\n\ndef tiedrank(a):\n    \'\'\' Return the ranks (with base 1) of a list resolving ties by averaging.\n     This works for numpy arrays.\'\'\'\n    m = len(a)\n    # Sort a in ascending order (sa=sorted vals, i=indices)\n    i = a.argsort()\n    sa = a[i]\n    # Find unique values\n    uval = np.unique(a)\n    # Test whether there are ties\n    R = np.arange(m, dtype=float) + 1  # Ranks with base 1\n    if len(uval) != m:\n        # Average the ranks for the ties\n        oldval = sa[0]\n        newval = sa[0]\n        k0 = 0\n        for k in range(1, m):\n            newval = sa[k]\n            if newval == oldval:\n                # moving average\n                R[k0:k + 1] = R[k - 1] * (k - k0) / (k - k0 + 1) + R[k] / (k - k0 + 1)\n            else:\n                k0 = k;\n                oldval = newval\n    # Invert the index\n    S = np.empty(m)\n    S[i] = R\n    return S\n\n\ndef mvmean(R, axis=0):\n    \'\'\' Moving average to avoid rounding errors. A bit slow, but...\n    Computes the mean along the given axis, except if this is a vector, in which case the mean is returned.\n    Does NOT flatten.\'\'\'\n    if len(R.shape) == 0: return R\n    average = lambda x: reduce(lambda i, j: (0, (j[0] / (j[0] + 1.)) * i[1] + (1. / (j[0] + 1)) * j[1]), enumerate(x))[\n        1]\n    R = np.array(R)\n    if len(R.shape) == 1: return average(R)\n    if axis == 1:\n        return np.array(map(average, R))\n    else:\n        return np.array(map(average, R.transpose()))\n\n\ndef get_valid_columns(solution):\n    """"""Get a list of column indices for which the column has more than one class.\n    This is necessary when computing BAC or AUC which involves true positive and\n    true negative in the denominator. When some class is missing, these scores\n    don\'t make sense (or you have to add an epsilon to remedy the situation).\n\n    Args:\n    solution: array, a matrix of binary entries, of shape\n      (num_examples, num_features)\n    Returns:\n    valid_columns: a list of indices for which the column has more than one\n      class.\n    """"""\n    num_examples = solution.shape[0]\n    col_sum = np.sum(solution, axis=0)\n    valid_columns = np.where(1 - np.isclose(col_sum, 0) -\n                             np.isclose(col_sum, num_examples))[0]\n    return valid_columns\n\n\ndef AUC(logits, labels):\n    logits = logits.detach().float().cpu().numpy()\n    labels = labels.detach().float().cpu().numpy()\n\n    valid_columns = get_valid_columns(labels)\n\n    logits = logits[:, valid_columns].copy()\n    labels = labels[:, valid_columns].copy()\n\n    label_num = labels.shape[1]\n    if label_num == 0:\n        return 0.0\n\n    auc = np.empty(label_num)\n    for k in range(label_num):\n        r_ = tiedrank(logits[:, k])\n        s_ = labels[:, k]\n\n        npos = sum(s_ == 1)\n        nneg = sum(s_ < 1)\n        auc[k] = (sum(r_[s_ == 1]) - npos * (npos + 1) / 2) / (nneg * npos)\n\n    return 2 * mvmean(auc) - 1\n\n\ndef crop(img, top, left, height, width):\n    bs, c = img.shape[0], img.shape[1]\n    new_img = torch.Tensor(bs, c, height, width).cuda()\n    for i in range(bs):\n        new_img[i] = img[i][..., top:top + height, left:left + width]\n    return new_img\n\n\ndef center_crop(img, output_size):\n    _, _, image_width, image_height = img.size()\n    crop_height, crop_width = output_size\n    crop_top = int(round((image_height - crop_height) / 2.))\n    crop_left = int(round((image_width - crop_width) / 2.))\n\n    return crop(img, crop_top, crop_left, crop_height, crop_width)\n\n\ndef five_crop(img, size):\n    assert len(size) == 2, ""Please provide only two dimensions (h, w) for size.""\n\n    _, _, image_height, image_width = img.size()\n    crop_height, crop_width = size\n    if crop_width > image_width or crop_height > image_height:\n        msg = ""Requested crop size {} is bigger than input size {}""\n        raise ValueError(msg.format(size, (image_height, image_width)))\n\n    tl = crop(img, 0, 0, crop_height, crop_width)\n    tr = crop(img, 0, image_width - crop_width, crop_height, crop_width)\n    bl = crop(img, image_height - crop_height, 0, crop_height, crop_width)\n    br = crop(img, image_height - crop_height, image_width - crop_width, crop_height, crop_width)\n    center = center_crop(img, (crop_height, crop_width))\n\n    return torch.cat([tl, tr, bl, br, center])\n'"
AutoDL_sample_code_submission/Auto_Image/skeleton/utils/__init__.py,0,b'# -*- coding: utf-8 -*-\n# pylint: disable=wildcard-import\nfrom __future__ import absolute_import\n\nfrom .timer import Timer'
AutoDL_sample_code_submission/Auto_Image/skeleton/utils/timer.py,0,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport time\nfrom collections import OrderedDict\nimport logging\n\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass Timer:\n    def __init__(self):\n        self.times = [time.time()]\n        self.accumulation = OrderedDict({})\n        self.total_time = 0.0\n        self.step_time = 0.0\n\n    def __call__(self, name, exclude_total=False, exclude_step=False, reset_step=False):\n        self.times.append(time.time())\n        delta = self.times[-1] - self.times[-2]\n\n        if name not in self.accumulation:\n            self.accumulation[name] = 0.0\n        self.accumulation[name] += delta\n\n        if not exclude_total:\n            self.total_time += delta\n\n        if reset_step:\n            self.step_time = 0.0\n        elif not exclude_step:\n            self.step_time += delta\n\n        return delta\n\n    def __repr__(self):\n        results = []\n        for key, value in self.accumulation.items():\n            results.append('{0}={1:.3f}'.format(key, value))\n        return self.__class__.__name__ + '(total={0}, step={1}, {2})'.format(\n            self.total_time, self.step_time, ', '.join(results)\n        )\n"""
AutoDL_sample_code_submission/Auto_Video/skeleton/data/__init__.py,0,"b'# -*- coding: utf-8 -*-\n# pylint: disable=wildcard-import\nfrom __future__ import absolute_import\n\nfrom .dataset import TFDataset, TransformDataset, prefetch_dataset\nfrom .dataloader import FixedSizeDataLoader, InfiniteSampler, PrefetchDataLoader\nfrom .transforms import *\nfrom .stratified_sampler import StratifiedSampler\nfrom . import augmentations\n# from .dali_data import *\n'"
AutoDL_sample_code_submission/Auto_Video/skeleton/data/augmentations.py,0,"b""# code in this file is adpated from rpmcruz/autoaugment\n# https://github.com/rpmcruz/autoaugment/blob/master/transformations.py\nimport random\n\nimport PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\nimport numpy as np\n\nrandom_mirror = True\n\n\ndef ShearX(img, v):  # [-0.3, 0.3]\n    assert -0.3 <= v <= 0.3\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0))\n\n\ndef ShearY(img, v):  # [-0.3, 0.3]\n    assert -0.3 <= v <= 0.3\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0))\n\n\ndef TranslateX(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert -0.45 <= v <= 0.45\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    v = v * img.size[0]\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n\n\ndef TranslateY(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert -0.45 <= v <= 0.45\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    v = v * img.size[1]\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n\n\ndef TranslateXAbs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert 0 <= v <= 10\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n\n\ndef TranslateYAbs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert 0 <= v <= 10\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n\n\ndef Rotate(img, v):  # [-30, 30]\n    assert -30 <= v <= 30\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    return img.rotate(v)\n\n\ndef AutoContrast(img, _):\n    return PIL.ImageOps.autocontrast(img)\n\n\ndef Invert(img, _):\n    return PIL.ImageOps.invert(img)\n\n\ndef Equalize(img, _):\n    return PIL.ImageOps.equalize(img)\n\n\ndef Flip(img, _):  # not from the paper\n    return PIL.ImageOps.mirror(img)\n\n\ndef Solarize(img, v):  # [0, 256]\n    assert 0 <= v <= 256\n    return PIL.ImageOps.solarize(img, v)\n\n\ndef Posterize(img, v):  # [4, 8]\n    assert 4 <= v <= 8\n    v = int(v)\n    return PIL.ImageOps.posterize(img, v)\n\n\ndef Posterize2(img, v):  # [0, 4]\n    assert 0 <= v <= 4\n    v = int(v)\n    return PIL.ImageOps.posterize(img, v)\n\n\ndef Contrast(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Contrast(img).enhance(v)\n\n\ndef Color(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Color(img).enhance(v)\n\n\ndef Brightness(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Brightness(img).enhance(v)\n\n\ndef Sharpness(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n\n\ndef Cutout(img, v):  # [0, 60] => percentage: [0, 0.2]\n    assert 0.0 <= v <= 0.2\n    if v <= 0.:\n        return img\n\n    v = v * img.size[0]\n\n    return CutoutAbs(img, v)\n\n\n\n\ndef CutoutAbs(img, v):  # [0, 60] => percentage: [0, 0.2]\n    # assert 0 <= v <= 20\n    if v < 0:\n        return img\n    w, h = img.size\n    x0 = np.random.uniform(w)\n    y0 = np.random.uniform(h)\n\n    x0 = int(max(0, x0 - v / 2.))\n    y0 = int(max(0, y0 - v / 2.))\n    x1 = min(w, x0 + v)\n    y1 = min(h, y0 + v)\n\n    xy = (x0, y0, x1, y1)\n    color = (125, 123, 114)\n    # color = (0, 0, 0)\n    img = img.copy()\n    PIL.ImageDraw.Draw(img).rectangle(xy, color)\n    return img\n\n\ndef SamplePairing(imgs):  # [0, 0.4]\n    def f(img1, v):\n        i = np.random.choice(len(imgs))\n        img2 = PIL.Image.fromarray(imgs[i])\n        return PIL.Image.blend(img1, img2, v)\n\n    return f\n\n\ndef augment_list(for_autoaug=True):  # 16 operations and their ranges\n    l = [\n        (ShearX, -0.3, 0.3),  # 0\n        (ShearY, -0.3, 0.3),  # 1\n        (TranslateX, -0.45, 0.45),  # 2\n        (TranslateY, -0.45, 0.45),  # 3\n        (Rotate, -30, 30),  # 4\n        (AutoContrast, 0, 1),  # 5\n        (Invert, 0, 1),  # 6\n        (Equalize, 0, 1),  # 7\n        (Solarize, 0, 256),  # 8\n        (Posterize, 4, 8),  # 9\n        (Contrast, 0.1, 1.9),  # 10\n        (Color, 0.1, 1.9),  # 11\n        (Brightness, 0.1, 1.9),  # 12\n        (Sharpness, 0.1, 1.9),  # 13\n        (Cutout, 0, 0.2),  # 14\n        # (SamplePairing(imgs), 0, 0.4),  # 15\n    ]\n    if for_autoaug:\n        l += [\n            (CutoutAbs, 0, 20),  # compatible with auto-augment\n            (Posterize2, 0, 4),  # 9\n            (TranslateXAbs, 0, 10),  # 9\n            (TranslateYAbs, 0, 10),  # 9\n        ]\n    return l\n\n\naugment_dict = {fn.__name__: (fn, v1, v2) for fn, v1, v2 in augment_list()}\n\n\ndef get_augment(name):\n    return augment_dict[name]\n\n\ndef apply_augment(img, name, level):\n    augment_fn, low, high = get_augment(name)\n    return augment_fn(img.copy(), level * (high - low) + low)\n\n\nclass Augmentation(object):\n    def __init__(self, policies):\n        self.policies = policies\n\n    def __call__(self, img):\n        for _ in range(1):\n            policy = random.choice(self.policies)\n            for name, pr, level in policy:\n                if random.random() > pr:\n                    continue\n                img = apply_augment(img, name, level)\n        return img\n\n\nfrom collections import defaultdict\n\nPARAMETER_MAX = 10\n\n\ndef remove_duplicates(policies):\n    s = set()\n    new_policies = []\n    for ops in policies:\n        key = []\n        for op in ops:\n            key.append(op[0])\n        key = '_'.join(key)\n        if key in s:\n            continue\n        else:\n            s.add(key)\n            new_policies.append(ops)\n\n    return new_policies\n\n\ndef float_parameter(level, maxval):\n    return float(level) * maxval / PARAMETER_MAX\n\n\ndef int_parameter(level, maxval):\n    return int(float_parameter(level, maxval))\n\n\ndef autoaug2arsaug(f):\n    def autoaug():\n        mapper = defaultdict(lambda: lambda x: x)\n        mapper.update({\n            'ShearX': lambda x: float_parameter(x, 0.3),\n            'ShearY': lambda x: float_parameter(x, 0.3),\n            'TranslateX': lambda x: int_parameter(x, 10),\n            'TranslateY': lambda x: int_parameter(x, 10),\n            'Rotate': lambda x: int_parameter(x, 30),\n            'Solarize': lambda x: 256 - int_parameter(x, 256),\n            'Posterize2': lambda x: 4 - int_parameter(x, 4),\n            'Contrast': lambda x: float_parameter(x, 1.8) + .1,\n            'Color': lambda x: float_parameter(x, 1.8) + .1,\n            'Brightness': lambda x: float_parameter(x, 1.8) + .1,\n            'Sharpness': lambda x: float_parameter(x, 1.8) + .1,\n            'CutoutAbs': lambda x: int_parameter(x, 20)\n        })\n\n        def low_high(name, prev_value):\n            _, low, high = get_augment(name)\n            return float(prev_value - low) / (high - low)\n\n        policies = f()\n        new_policies = []\n        for policy in policies:\n            new_policies.append([(name, pr, low_high(name, mapper[name](level))) for name, pr, level in policy])\n        return new_policies\n\n    return autoaug\n\n\n@autoaug2arsaug\ndef autoaug_policy():\n    exp0_0 = [\n        [('Invert', 0.1, 7), ('Contrast', 0.2, 6)],\n        [('Rotate', 0.7, 2), ('TranslateXAbs', 0.3, 9)],\n        [('Sharpness', 0.8, 1), ('Sharpness', 0.9, 3)],\n        [('ShearY', 0.5, 8), ('TranslateYAbs', 0.7, 9)],\n        [('AutoContrast', 0.5, 8), ('Equalize', 0.9, 2)]]\n    exp0_1 = [\n        [('Solarize', 0.4, 5), ('AutoContrast', 0.9, 3)],\n        [('TranslateYAbs', 0.9, 9), ('TranslateYAbs', 0.7, 9)],\n        [('AutoContrast', 0.9, 2), ('Solarize', 0.8, 3)],\n        [('Equalize', 0.8, 8), ('Invert', 0.1, 3)],\n        [('TranslateYAbs', 0.7, 9), ('AutoContrast', 0.9, 1)]]\n    exp0_2 = [\n        [('Solarize', 0.4, 5), ('AutoContrast', 0.0, 2)],\n        [('TranslateYAbs', 0.7, 9), ('TranslateYAbs', 0.7, 9)],\n        [('AutoContrast', 0.9, 0), ('Solarize', 0.4, 3)],\n        [('Equalize', 0.7, 5), ('Invert', 0.1, 3)],\n        [('TranslateYAbs', 0.7, 9), ('TranslateYAbs', 0.7, 9)]]\n    exp0_3 = [\n        [('Solarize', 0.4, 5), ('AutoContrast', 0.9, 1)],\n        [('TranslateYAbs', 0.8, 9), ('TranslateYAbs', 0.9, 9)],\n        [('AutoContrast', 0.8, 0), ('TranslateYAbs', 0.7, 9)],\n        [('TranslateYAbs', 0.2, 7), ('Color', 0.9, 6)],\n        [('Equalize', 0.7, 6), ('Color', 0.4, 9)]]\n    exp1_0 = [\n        [('ShearY', 0.2, 7), ('Posterize2', 0.3, 7)],\n        [('Color', 0.4, 3), ('Brightness', 0.6, 7)],\n        [('Sharpness', 0.3, 9), ('Brightness', 0.7, 9)],\n        [('Equalize', 0.6, 5), ('Equalize', 0.5, 1)],\n        [('Contrast', 0.6, 7), ('Sharpness', 0.6, 5)]]\n    exp1_1 = [\n        [('Brightness', 0.3, 7), ('AutoContrast', 0.5, 8)],\n        [('AutoContrast', 0.9, 4), ('AutoContrast', 0.5, 6)],\n        [('Solarize', 0.3, 5), ('Equalize', 0.6, 5)],\n        [('TranslateYAbs', 0.2, 4), ('Sharpness', 0.3, 3)],\n        [('Brightness', 0.0, 8), ('Color', 0.8, 8)]]\n    exp1_2 = [\n        [('Solarize', 0.2, 6), ('Color', 0.8, 6)],\n        [('Solarize', 0.2, 6), ('AutoContrast', 0.8, 1)],\n        [('Solarize', 0.4, 1), ('Equalize', 0.6, 5)],\n        [('Brightness', 0.0, 0), ('Solarize', 0.5, 2)],\n        [('AutoContrast', 0.9, 5), ('Brightness', 0.5, 3)]]\n    exp1_3 = [\n        [('Contrast', 0.7, 5), ('Brightness', 0.0, 2)],\n        [('Solarize', 0.2, 8), ('Solarize', 0.1, 5)],\n        [('Contrast', 0.5, 1), ('TranslateYAbs', 0.2, 9)],\n        [('AutoContrast', 0.6, 5), ('TranslateYAbs', 0.0, 9)],\n        [('AutoContrast', 0.9, 4), ('Equalize', 0.8, 4)]]\n    exp1_4 = [\n        [('Brightness', 0.0, 7), ('Equalize', 0.4, 7)],\n        [('Solarize', 0.2, 5), ('Equalize', 0.7, 5)],\n        [('Equalize', 0.6, 8), ('Color', 0.6, 2)],\n        [('Color', 0.3, 7), ('Color', 0.2, 4)],\n        [('AutoContrast', 0.5, 2), ('Solarize', 0.7, 2)]]\n    exp1_5 = [\n        [('AutoContrast', 0.2, 0), ('Equalize', 0.1, 0)],\n        [('ShearY', 0.6, 5), ('Equalize', 0.6, 5)],\n        [('Brightness', 0.9, 3), ('AutoContrast', 0.4, 1)],\n        [('Equalize', 0.8, 8), ('Equalize', 0.7, 7)],\n        [('Equalize', 0.7, 7), ('Solarize', 0.5, 0)]]\n    exp1_6 = [\n        [('Equalize', 0.8, 4), ('TranslateYAbs', 0.8, 9)],\n        [('TranslateYAbs', 0.8, 9), ('TranslateYAbs', 0.6, 9)],\n        [('TranslateYAbs', 0.9, 0), ('TranslateYAbs', 0.5, 9)],\n        [('AutoContrast', 0.5, 3), ('Solarize', 0.3, 4)],\n        [('Solarize', 0.5, 3), ('Equalize', 0.4, 4)]]\n    exp2_0 = [\n        [('Color', 0.7, 7), ('TranslateXAbs', 0.5, 8)],\n        [('Equalize', 0.3, 7), ('AutoContrast', 0.4, 8)],\n        [('TranslateYAbs', 0.4, 3), ('Sharpness', 0.2, 6)],\n        [('Brightness', 0.9, 6), ('Color', 0.2, 8)],\n        [('Solarize', 0.5, 2), ('Invert', 0.0, 3)]]\n    exp2_1 = [\n        [('AutoContrast', 0.1, 5), ('Brightness', 0.0, 0)],\n        # [('CutoutAbs', 0.2, 4), ('Equalize', 0.1, 1)],\n        [('Equalize', 0.7, 7), ('AutoContrast', 0.6, 4)],\n        [('Color', 0.1, 8), ('ShearY', 0.2, 3)],\n        [('ShearY', 0.4, 2), ('Rotate', 0.7, 0)]]\n    exp2_2 = [\n        [('ShearY', 0.1, 3), ('AutoContrast', 0.9, 5)],\n        # [('TranslateYAbs', 0.3, 6), ('CutoutAbs', 0.3, 3)],\n        [('Equalize', 0.5, 0), ('Solarize', 0.6, 6)],\n        [('AutoContrast', 0.3, 5), ('Rotate', 0.2, 7)],\n        [('Equalize', 0.8, 2), ('Invert', 0.4, 0)]]\n    exp2_3 = [\n        [('Equalize', 0.9, 5), ('Color', 0.7, 0)],\n        [('Equalize', 0.1, 1), ('ShearY', 0.1, 3)],\n        [('AutoContrast', 0.7, 3), ('Equalize', 0.7, 0)],\n        [('Brightness', 0.5, 1), ('Contrast', 0.1, 7)],\n        [('Contrast', 0.1, 4), ('Solarize', 0.6, 5)]]\n    exp2_4 = [\n        [('Solarize', 0.2, 3), ('ShearX', 0.0, 0)],\n        [('TranslateXAbs', 0.3, 0), ('TranslateXAbs', 0.6, 0)],\n        [('Equalize', 0.5, 9), ('TranslateYAbs', 0.6, 7)],\n        [('ShearX', 0.1, 0), ('Sharpness', 0.5, 1)],\n        [('Equalize', 0.8, 6), ('Invert', 0.3, 6)]]\n    exp2_5 = [\n        # [('AutoContrast', 0.3, 9), ('CutoutAbs', 0.5, 3)],\n        [('ShearX', 0.4, 4), ('AutoContrast', 0.9, 2)],\n        [('ShearX', 0.0, 3), ('Posterize2', 0.0, 3)],\n        [('Solarize', 0.4, 3), ('Color', 0.2, 4)],\n        [('Equalize', 0.1, 4), ('Equalize', 0.7, 6)]]\n    exp2_6 = [\n        [('Equalize', 0.3, 8), ('AutoContrast', 0.4, 3)],\n        [('Solarize', 0.6, 4), ('AutoContrast', 0.7, 6)],\n        [('AutoContrast', 0.2, 9), ('Brightness', 0.4, 8)],\n        [('Equalize', 0.1, 0), ('Equalize', 0.0, 6)],\n        [('Equalize', 0.8, 4), ('Equalize', 0.0, 4)]]\n    exp2_7 = [\n        [('Equalize', 0.5, 5), ('AutoContrast', 0.1, 2)],\n        [('Solarize', 0.5, 5), ('AutoContrast', 0.9, 5)],\n        [('AutoContrast', 0.6, 1), ('AutoContrast', 0.7, 8)],\n        [('Equalize', 0.2, 0), ('AutoContrast', 0.1, 2)],\n        [('Equalize', 0.6, 9), ('Equalize', 0.4, 4)]]\n    exp0s = exp0_0 + exp0_1 + exp0_2 + exp0_3\n    exp1s = exp1_0 + exp1_1 + exp1_2 + exp1_3 + exp1_4 + exp1_5 + exp1_6\n    exp2s = exp2_0 + exp2_1 + exp2_2 + exp2_3 + exp2_4 + exp2_5 + exp2_6 + exp2_7\n\n    return exp0s + exp1s + exp2s\n"""
AutoDL_sample_code_submission/Auto_Video/skeleton/data/dali_data.py,0,"b'import tensorflow as tf\nimport torch\nimport pdb\nimport inspect\nimport numpy as np\n\nfrom nvidia.dali.pipeline import Pipeline\nimport nvidia.dali.ops as ops\nimport nvidia.dali.types as types\nfrom nvidia.dali.plugin.pytorch import DALIClassificationIterator, DALIGenericIterator\n\nclass ExternalInputIterator(object):\n    def __init__(self, data, batch_size):\n        self.data = data\n        self.batch_size = batch_size\n\n    def __iter__(self):\n        self.i = 0\n        self.n = len(self.data)\n        return self\n\n    def __next__(self):\n        batch = []\n        labels = []\n\n        if self.i >= self.n:\n            raise StopIteration\n        \n        for _ in range(self.batch_size):\n            batch.append(self.data[self.i][0])\n            labels.append(self.data[self.i][1])\n            self.i = (self.i + 1) % self.n\n        return (batch, labels)\n\n    @property\n    def size(self,):\n        return len(self.data)\n\n    next = __next__\n\nclass ExternalInputIterator2(object):\n    def __init__(self, session, element, num_samples, batch_size):\n        self.session = session\n        self.element = element\n        self.batch_size = batch_size\n        self.num_samples = num_samples\n\n    def __iter__(self):\n        self.i = 0\n        self.n = self.num_samples\n        return self\n\n    def __next__(self):\n        batch = []\n        labels = []\n\n        if self.i >= self.n:\n            raise StopIteration\n        \n        for _ in range(self.batch_size):\n            example, label = self.session.run(self.element)\n            batch.append(example)\n            labels.append(label)\n            self.i = (self.i + 1) % self.n\n        return (batch, labels)\n\n\n    @property\n    def size(self,):\n        return self.num_samples\n\n    next = __next__\n\nclass ExternalInputIterator3(object):\n    def __init__(self, session, dataset, num_samples, batch_size, fill_last_batch=True):\n        self.session = session\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.num_samples = num_samples\n        self.fill_last_batch = fill_last_batch\n        self.steps = self.num_samples // self.batch_size\n        if self.num_samples % self.batch_size != 0:\n            self.steps += 1\n        self.total = self.batch_size * self.steps\n\n    def __iter__(self):\n        self.i = 0\n        self.n = self.num_samples\n        return self\n\n    def __next__(self):\n        batch = []\n        labels = []\n    \n        for batch_idx in range(self.batch_size):\n            if not self.fill_last_batch and self.i >= self.num_samples:\n                example, label = np.zeros(self.example_size, dtype=np.float32), np.zeros(self.label_size, dtype=np.float32)\n            else:\n                try:\n                    example, label = self.session.run(self.next_element)\n\n                    if self.i == 0:\n                        self.example_size = example.shape\n                        self.label_size = label.shape\n\n\n                except tf.errors.OutOfRangeError:\n                    self.reset()\n                    raise StopIteration\n\n            self.i  = (self.i + 1) % self.total\n\n            batch.append(example)\n            labels.append(label)\n\n        return (batch, labels)\n\n    def reset(self):\n        dataset = self.dataset\n        iterator = dataset.make_one_shot_iterator()\n        self.next_element = iterator.get_next()\n        return self\n\n\n    @property\n    def size(self,):\n        return self.num_samples\n\n    next = __next__\n\nclass ExternalSourcePipeline(Pipeline):\n    def __init__(self, session, dataset, batch_size, num_threads, is_random_flip=True, num_samples=1000000, device_id=0, preprocess=None, fill_last_batch=True):\n        super(ExternalSourcePipeline, self).__init__(batch_size,\n                                      num_threads,\n                                      device_id,\n                                      seed=12)\n        self.session = session\n        self.num_samples = num_samples\n        self.dataset = dataset\n        self.is_random_flip = is_random_flip\n        self.preprocess = preprocess\n\n        if self.preprocess is not None:\n            crop = (preprocess[\'width\'], preprocess[\'height\'])\n\n            self.res = ops.Resize(resize_x=preprocess[\'width\'], resize_y=preprocess[\'height\'])\n\n        self.flip = ops.Flip()\n\n        self.coin = ops.CoinFlip(probability=0.5)\n        self.coin2 = ops.CoinFlip(probability=0.5)\n        \n        self.iterator = iter(ExternalInputIterator3(self.session, self.dataset, self.num_samples, batch_size, fill_last_batch))\n        self.iterator.reset()\n        \n        self.input = ops.ExternalSource()\n        self.input_label = ops.ExternalSource()\n\n    def reset(self):\n        dataset = self.dataset\n        iterator = dataset.make_one_shot_iterator()\n        self.next_element = iterator.get_next()\n        return self\n\n    def define_graph(self):\n        rng = self.coin()\n        rng2 = self.coin2()\n\n        self.x = self.input()\n        output = self.x\n        \n        self.labels = self.input_label()\n        return [output, self.labels]\n\n    def iter_setup(self):\n        (x, labels) = self.iterator.next()\n        self.feed_input(self.x, x)\n        self.feed_input(self.labels, labels)\n        \n\nclass DALILoader(object):\n    def gen_loader(loader, steps):\n        for i, data in enumerate(loader):\n            input = data[0][\'data\'].cuda()\n            target = data[0][""label""].cuda()\n            yield input, target\n\n    def __init__(self, session, dataset, num_samples, batch_size, steps=None, num_threads=0, fill_last_batch=True, is_random_flip=True, preprocess=None):\n        self.steps = steps\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.num_samples = num_samples\n\n        pipe = ExternalSourcePipeline(session, self.dataset, batch_size=batch_size, num_threads=num_threads, num_samples=self.num_samples, device_id=0, is_random_flip=is_random_flip, preprocess=preprocess, fill_last_batch=fill_last_batch)\n        pipe.build()\n        self.dataloader = DALIClassificationIterator(pipe, self.num_samples, auto_reset=True, fill_last_batch=fill_last_batch, last_batch_padded=True)\n\n    def __len__(self):\n        if self.steps is None:\n            steps = self.num_samples // self.batch_size\n            if self.num_samples % self.batch_size != 0:\n                steps += 1\n            return steps\n            \n        return self.steps\n\n    def __iter__(self):\n        return DALILoader.gen_loader(self.dataloader, self.steps)\n\n    def reset(self):\n        self.dataloader.reset()\n\n'"
AutoDL_sample_code_submission/Auto_Video/skeleton/data/dataloader.py,8,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport logging\n\nimport torch\n\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass FixedSizeDataLoader:\n    def __init__(self, dataset, steps, batch_size=1, shuffle=False, num_workers=0, pin_memory=False, drop_last=False,\n                 sampler=None):\n        sampler = InfiniteSampler(dataset, shuffle) if sampler is None else sampler\n        self.batch_size = batch_size\n        batch_size = 1 if batch_size is None else batch_size\n\n        self.steps = steps\n        self.dataset = dataset\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=batch_size,\n            sampler=sampler,\n            num_workers=num_workers,\n            pin_memory=pin_memory,\n            drop_last=drop_last\n        )\n\n    def __len__(self):\n        return self.steps\n\n    def __iter__(self):\n        if self.steps is not None:\n            for i, data in zip(range(self.steps), self.dataloader):\n                yield ([t[0] for t in data] if self.batch_size is None else data)\n        else:\n            for data in self.dataloader:\n                yield ([t[0] for t in data] if self.batch_size is None else data)\n\n\nclass InfiniteSampler(torch.utils.data.sampler.Sampler):\n    def __init__(self, data_source, shuffle=False):\n        self.data_source = data_source\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        n = len(self.data_source)\n        while True:\n            index_list = torch.randperm(n).tolist() if self.shuffle else list(range(n))\n            for idx in index_list:\n                yield idx\n\n    def __len__(self):\n        return len(self.data_source)\n\n\nclass PrefetchDataLoader:\n    def __init__(self, dataloader, device, half=False):\n        self.loader = dataloader\n        self.iter = None\n        self.device = device\n        self.dtype = torch.float16 if half else torch.float32\n        self.stream = torch.cuda.Stream()\n        self.next_data = None\n\n    def __len__(self):\n        return len(self.loader)\n\n    def async_prefech(self):\n        try:\n            self.next_data = next(self.iter)\n        except StopIteration:\n            self.next_data = None\n            return\n\n        with torch.cuda.stream(self.stream):\n            if isinstance(self.next_data, torch.Tensor):\n                self.next_data = self.next_data.to(dtype=self.dtype, device=self.device, non_blocking=True)\n            elif isinstance(self.next_data, (list, tuple)):\n                self.next_data = [\n                    t.to(dtype=self.dtype, device=self.device, non_blocking=True) if t.is_floating_point() else t.to(device=self.device, non_blocking=True) for t in self.next_data\n                ]\n\n    def __iter__(self):\n        self.iter = iter(self.loader)\n        self.async_prefech()\n        while self.next_data is not None:\n            torch.cuda.current_stream().wait_stream(self.stream)\n            data = self.next_data\n            self.async_prefech()\n            yield data\n'"
AutoDL_sample_code_submission/Auto_Video/skeleton/data/dataset.py,9,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport logging\n\nimport torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport tensorflow as tf\nfrom ..nn.modules.hooks import MoveToHook\n\n\nimport types\nimport collections\nimport numpy as np\nfrom random import shuffle\nimport pdb\n\nfrom skeleton.utils.log_utils import timeit\n\n# LOGGER = logging.getLogger(__name__)\n\n\nclass TFDataset(Dataset):\n    def __init__(self, session, dataset, num_samples):\n        super(TFDataset, self).__init__()\n        self.session = session\n        self.dataset = dataset\n        self.num_samples = num_samples\n        self.next_element = None\n\n        self.reset()\n\n    def reset(self):\n        dataset = self.dataset\n        iterator = dataset.make_one_shot_iterator()\n        self.next_element = iterator.get_next()\n        return self\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, index):\n        session = self.session if self.session is not None else tf.Session()\n        try:\n            example, label = session.run(self.next_element)\n        except tf.errors.OutOfRangeError:\n            self.reset()\n            raise StopIteration\n\n        return example, label\n\n    def scan(self, samples=1000000, with_tensors=False, is_batch=False, device=None, half=False):\n        shapes, counts, tensors = [], [], []\n        labels = []\n        min_list, max_list = [], []\n        for i in range(min(self.num_samples, samples)):\n            try:\n                example, label = self.__getitem__(i)\n            except tf.errors.OutOfRangeError:\n                break\n            except StopIteration:\n                break\n\n            shape = example.shape\n            count = np.sum(label, axis=None if not is_batch else -1)\n            labels.append(label)\n\n            shapes.append(shape)\n            counts.append(count)\n            min_list.append(np.min(example))\n            max_list.append(np.max(example))\n\n            if with_tensors:\n                example = torch.Tensor(example)\n                label = torch.Tensor(label)\n\n                example.data = example.data.to(device=device)\n                if half and example.is_floating_point():\n                    example.data = example.data.half()\n\n                label.data = label.data.to(device=device)\n                if half and label.is_floating_point():\n                    label.data = label.data.half()\n\n                tensors.append([example, label])\n\n        shapes = np.array(shapes)\n        counts = np.array(counts) if not is_batch else np.concatenate(counts)\n\n        labels = np.array(labels) if not is_batch else np.concatenate(labels)\n        labels = np.sum(labels, axis=0)\n        zero_count = sum(labels == 0)\n\n\n        info = {\n            'count': len(counts),\n            'is_multiclass': counts.max() > 1.0,\n            'is_video': int(np.median(shapes, axis=0)[0]) > 1,\n            'example': {\n                'shape': [int(v) for v in np.median(shapes, axis=0)],  #\n                'shape_avg': [int(v) for v in np.average(shapes, axis=0)],  #\n                'value': {'min': min(min_list), 'max': max(max_list)}\n            },\n            'label': {\n                'min': counts.min(),\n                'max': counts.max(),\n                'average': counts.mean(),\n                'median': np.median(counts),\n                'zero_count': zero_count,\n            },\n\n        }\n\n        if with_tensors:\n            return info, tensors\n        return info\n\n\n    @timeit\n    def to_numpy(self, samples=1000000, with_tensors=False, is_batch=False, device=None, half=False):\n        tensors = []\n        while True:\n            try:\n                example, label = self.__getitem__(1)\n\n            except tf.errors.OutOfRangeError:\n                break\n            except StopIteration:\n                break\n\n            if with_tensors:\n                example = torch.Tensor(example)\n                label = torch.Tensor(label)\n\n                example.data = example.data.to(device=device)\n                if half and example.is_floating_point():\n                    example.data = example.data.half()\n\n                label.data = label.data.to(device=device)\n                if half and label.is_floating_point():\n                    label.data = label.data.half()\n\n                tensors.append([example, label])\n\n        if with_tensors:\n            return {}, tensors\n\n\nclass TransformDataset(Dataset):\n    def __init__(self, dataset, transform=None, index=None):\n        self.dataset = dataset\n        self.transform = transform\n        self.index = index\n\n    def __getitem__(self, index):\n        tensors = self.dataset[index]\n        tensors = list(tensors)\n\n        if self.transform is not None:\n            if self.index is None:\n                tensors = self.transform(*tensors)\n            else:\n                tensors[self.index] = self.transform(tensors[self.index])\n\n        return tuple(tensors)\n\n    def __len__(self):\n        return len(self.dataset)\n\n\ndef prefetch_dataset(dataset, num_workers=4, batch_size=32, device=None, half=False):\n    if isinstance(dataset, list) and isinstance(dataset[0], torch.Tensor):\n        tensors = dataset\n    else:\n        dataloader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False, drop_last=False,\n            num_workers=num_workers, pin_memory=False\n        )\n        tensors = [t for t in dataloader]\n        tensors = [torch.cat(t, dim=0) for t in zip(*tensors)]\n\n    if device is not None:\n        tensors = [t.to(device=device) for t in tensors]\n    if half:\n        tensors = [t.half() if  t.is_floating_point() else t for t in tensors]\n\n\n    return torch.utils.data.TensorDataset(*tensors)\n"""
AutoDL_sample_code_submission/Auto_Video/skeleton/data/stratified_sampler.py,1,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport logging\n\nimport os\nimport random\nfrom abc import ABC\nfrom collections import defaultdict\n\nfrom torch.utils.data import Sampler\n\n\nclass StratifiedSampler(Sampler, ABC):\n    def __init__(self, labels):\n        self.idx_by_lb = defaultdict(list)\n        for idx, lb in enumerate(labels):\n            self.idx_by_lb[lb].append(idx)\n\n        self.size = len(labels)\n\n    def __len__(self):\n        return self.size\n\n    def __iter__(self):\n        while True:\n            songs_list = []\n            artists_list = []\n            for lb, v in self.idx_by_lb.items():\n                for idx in v:\n                    songs_list.append(idx)\n                    artists_list.append(lb)\n\n            shuffled = spotifyShuffle(songs_list, artists_list)\n            for idx in shuffled:\n                yield idx\n\n\ndef fisherYatesShuffle(arr):\n\n    for i in range(len(arr)-1, 0, -1):\n        j = random.randint(0, i)\n        arr[i], arr[j] = arr[j], arr[i]\n    return arr\n\n\ndef spotifyShuffle(songs_list, artists_list):\n    artist2songs = defaultdict(list)\n    for artist, song in zip(artists_list, songs_list):\n        artist2songs[artist].append(song)\n    songList = []\n    songsLocs = []\n    for artist, songs in artist2songs.items():\n        songs = fisherYatesShuffle(songs)\n        songList += songs\n        songsLocs += get_locs(len(songs))\n    return [songList[idx] for idx in argsort(songsLocs)]\n\n\ndef argsort(seq):\n    return [i for i, j in sorted(enumerate(seq), key=lambda x:x[1])]\n\n\ndef get_locs(n):\n    percent = 1. / n\n    locs = [percent * random.random()]\n    last = locs[0]\n    for i in range(n - 1):\n        value = last + percent * random.uniform(0.8, 1.2)  #\n        locs.append(value)\n        last = value\n    return locs\n'"
AutoDL_sample_code_submission/Auto_Video/skeleton/data/transforms.py,3,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport os\nimport logging\nimport hashlib\nimport random\n\nimport numpy as np\nimport torch\n\n\n# LOGGER = logging.getLogger(__name__)\n\n\nclass Identity:\n    def __call__(self, image):\n        return image\n\n\nclass Normalize:\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, image):\n        return (image - self.mean) / self.std\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\n\nclass RandomFlip:\n    def __init__(self, p=0.5, dims=[-1]):\n        self.p = p\n        self.dims = dims\n\n    def __call__(self, tensor):\n        if random.random() < self.p:\n            tensor = torch.flip(tensor, dims=self.dims)\n        return tensor\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(p={})'.format(self.p)\n\n\nclass Crop:\n    def __init__(self, height, width):\n        self.height = height\n        self.width = width\n\n    def __call__(self, image):\n        h, w = image.shape[-2:]\n\n        y = np.random.randint(h - self.height)\n        x = np.random.randint(w - self.width)\n\n        return image[:, y:y+self.height, x:x+self.width]\n\n\nclass Cutout:\n    def __init__(self, height, width):\n        self.height = height\n        self.width = width\n\n    def __call__(self, image):\n        if self.height > 0 or self.width > 0:\n            if isinstance(image, torch.Tensor):\n                mask = torch.ones_like(image)\n            elif isinstance(image, np.ndarray):\n                mask = np.ones_like(image)\n            else:\n                raise NotImplementedError('support only tensor or numpy array')\n\n            h, w = image.shape[-2:]\n\n            y = np.random.randint(h)\n            x = np.random.randint(w)\n\n            y1 = np.clip(y - self.height // 2, 0, h)\n            y2 = np.clip(y + self.height // 2, 0, h)\n            x1 = np.clip(x - self.width // 2, 0, w)\n            x2 = np.clip(x + self.width // 2, 0, w)\n\n            if len(mask.shape) == 3:\n                mask[:, y1: y2, x1: x2] = 0.\n            else:\n                mask[:, :, y1: y2, x1: x2] = 0.\n            image *= mask\n        return image\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(height={0}, width={1})'.format(self.height, self.width)\n\n\nclass RandomHorizontalFlip:\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, img):\n        if random.random() < self.p:\n            img = np.flip(img, axis=-1).copy()\n        return img\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(p={})'.format(self.p)\n\n\nclass Writer:\n    def __init__(self, path, format='jpg'):\n        self.path = path\n        self.format = format\n        os.makedirs(self.path, exist_ok=True)\n\n    def __call__(self, image):\n        filename = hashlib.md5(image.tobytes()).hexdigest()\n        path = self.path + '/' + filename + '.' + self.format\n        image.save(path)\n        return image\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(path={0}, format={1})'.format(self.path, self.format)\n"""
AutoDL_sample_code_submission/Auto_Video/skeleton/nn/__init__.py,0,b'# -*- coding: utf-8 -*-\n# pylint: disable=wildcard-import\nfrom __future__ import absolute_import\n\nfrom .modules import *\n'
AutoDL_sample_code_submission/Auto_Video/skeleton/optim/__init__.py,0,b'# -*- coding: utf-8 -*-\n# pylint: disable=wildcard-import\nfrom __future__ import absolute_import\n\nfrom .optimizers import *\nfrom .scheduler import *\nfrom .sgdw import SGDW\n'
AutoDL_sample_code_submission/Auto_Video/skeleton/optim/optimizers.py,1,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport logging\n\nimport torch\n\n\n# LOGGER = logging.getLogger(__name__)\n\n\nclass ScheduledOptimizer:\n    def __init__(self, parameters, optimizer, steps_per_epoch=1, clip_grad_max_norm=None, tag=None, **opt_params):\n        self.epoch = 0.0\n        self.tag = tag\n        self._parameters = parameters\n        self.steps_per_epoch = steps_per_epoch\n        self.clip_grad_max_norm = clip_grad_max_norm\n        self._opt_params = opt_params\n\n        self._optimizer = optimizer(parameters, **self.update_params(0))\n\n    def update_params(self, epoch=None, **kwargs):\n        return {\n            k: v(self.epoch if epoch is None else epoch, **kwargs) if callable(v) else v\n            for k, v in self._opt_params.items()\n        }\n\n    def update(self, epoch=None, **kwargs):\n        opt_pararms = self.update_params(epoch, **kwargs)\n        self._optimizer.param_groups[0].update(**opt_pararms)\n\n        for key, value in opt_pararms.items():\n            tag = self.tag if self.tag is not None else 'train'\n            if not isinstance(value, (float, int)):\n                continue\n        return self\n\n    def step(self, epoch=None):\n        self.epoch = self.epoch + (1.0 / self.steps_per_epoch) if epoch is None else epoch\n        if self.clip_grad_max_norm is not None and self.clip_grad_max_norm > 0.0:\n            torch.nn.utils.clip_grad_norm_(self._parameters, self.clip_grad_max_norm, norm_type=1)\n        self._optimizer.step()\n\n    def state_dict(self):\n        state_dict = self._optimizer.state_dict()\n        state_dict.update({'epoch': self.epoch})\n        return state_dict\n\n    def load_state_dict(self, state_dict):\n        self.epoch = state_dict.pop('epoch')\n        return self._optimizer.load_state_dict(state_dict)\n\n    def zero_grad(self):\n        return self._optimizer.zero_grad()\n\n    def get_learning_rate(self):\n        return self._optimizer.param_groups[0]['lr']\n\n    def __getattr__(self, item):\n        return getattr(self._optimizer, item)\n"""
AutoDL_sample_code_submission/Auto_Video/skeleton/optim/scheduler.py,0,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport math\nimport logging\n\n\n# LOGGER = logging.getLogger(__name__)\n\n\ndef gradual_warm_up(scheduler, warm_up_epoch, multiplier):\n    def schedule(e, **kwargs):\n        lr = scheduler(e, **kwargs)\n        lr = lr * ((multiplier - 1.0) * min(e, warm_up_epoch) / warm_up_epoch + 1)\n        return lr\n    return schedule\n\n\ndef get_discrete_epoch(scheduler):\n    def schedule(e, **kwargs):\n        return scheduler(int(e), **kwargs)\n    return schedule\n\n\ndef get_change_scale(scheduler, init_scale=1.0):\n    def schedule(e, scale=None, **kwargs):\n        lr = scheduler(e, **kwargs)\n        return lr * (scale if scale is not None else init_scale)\n    return schedule\n\n\ndef get_step_scheduler(init_lr, step_size, gamma=0.1):\n    def schedule(e, **kwargs):\n        lr = init_lr * gamma ** (e // step_size)\n        return lr\n    return schedule\n\n\ndef get_cosine_scheduler(init_lr, maximum_epoch, eta_min=0):\n    def schedule(e, **kwargs):\n        maximum = kwargs['maximum_epoch'] if 'maximum_epoch' in kwargs else maximum_epoch\n        lr = eta_min + (init_lr - eta_min) * (1 + math.cos(math.pi * e / maximum)) / 2\n        return lr\n    return schedule\n\n\nclass PlateauScheduler:\n    def __init__(self, init_lr, factor=0.1, patience=10, threshold=1e-4):\n        self.init_lr = init_lr\n        self.factor = factor\n        self.patience = patience\n        self.threshold = threshold\n\n        self.curr_lr = init_lr\n        self.best_loss = 10000\n        self.prev_epoch = 0\n        self.num_bad_epochs = 0\n\n    def __call__(self, epoch, loss=None, **kwargs):\n        if loss is None:\n            loss = self.best_loss\n\n        if self.best_loss - self.threshold > loss:\n            self.num_bad_epochs = 0\n            self.best_loss = loss\n        else:\n            self.num_bad_epochs += epoch - self.prev_epoch\n\n        if self.num_bad_epochs >= self.patience:\n            self.num_bad_epochs = 0\n            self.curr_lr *= self.factor\n\n        self.prev_epoch = epoch\n        return self.curr_lr\n\n\ndef get_reduce_on_plateau_scheduler(init_lr, factor=0.1, patience=10, threshold=1e-4, min_lr=0, metric_name='metric'):\n    class Schedule:\n        def __init__(self):\n            self.num_bad_epochs = 0\n            self.lr = init_lr\n            self.best = None\n            self.metric_name = metric_name\n\n        def __call__(self, e, **kwargs):\n            if self.metric_name not in kwargs:\n                return self.lr\n            metric = kwargs[self.metric_name]\n\n\n            if self.best is None or self.best > metric:\n                self.best = metric - threshold\n                self.num_bad_epochs = 0\n            else:\n                self.num_bad_epochs += 1\n\n            if self.num_bad_epochs > patience:\n                self.num_bad_epochs = 0\n                lr = max(min_lr, self.lr * factor)\n                self.lr = lr\n            return self.lr\n    return Schedule()\n"""
AutoDL_sample_code_submission/Auto_Video/skeleton/optim/sgdw.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\n\nimport torch\n\n\nclass SGDW(torch.optim.SGD):\n    """"""\n    Decoupled Weight Decay Regularization\n    reference: https://arxiv.org/abs/1711.05101\n    """"""\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            weight_decay = group[\'weight_decay\']\n            momentum = group[\'momentum\']\n            dampening = group[\'dampening\']\n            nesterov = group[\'nesterov\']\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                d_p = p.grad.data\n                if momentum != 0:\n                    param_state = self.state[p]\n                    if \'momentum_buffer\' not in param_state:\n                        buf = param_state[\'momentum_buffer\'] = torch.zeros_like(p.data)\n                        buf.mul_(momentum).add_(d_p)\n                    else:\n                        buf = param_state[\'momentum_buffer\']\n                        buf.mul_(momentum).add_(1 - dampening, d_p)\n                    if nesterov:\n                        d_p = d_p.add(momentum, buf)\n                    else:\n                        d_p = buf\n\n                # decoupled weight decay\n                if weight_decay != 0:\n                    d_p.add_(weight_decay, p.data)\n\n                p.data.add_(-group[\'lr\'], d_p)\n\n        return loss\n'"
AutoDL_sample_code_submission/Auto_Video/skeleton/projects/__init__.py,0,b'# -*- coding: utf-8 -*-\n# pylint: disable=wildcard-import\nfrom __future__ import absolute_import\n\nfrom .api import *\nfrom .others import *\nfrom .logic import *'
AutoDL_sample_code_submission/Auto_Video/skeleton/projects/logic.py,2,"b'# -*- coding: utf-8 -*-\r\nfrom __future__ import absolute_import\r\nimport random\r\n\r\nimport tensorflow as tf\r\nimport torchvision as tv\r\nimport torch\r\nimport numpy as np\r\n\r\nfrom .api import Model\r\nfrom .others import *\r\nimport skeleton\r\nfrom collections import OrderedDict\r\nfrom sklearn.preprocessing import LabelEncoder\r\nfrom sklearn.model_selection import StratifiedShuffleSplit\r\n\r\n# LOGGER = get_logger(__name__)\r\n\r\nclass EnsembleConfig(object):\r\n    ENABLE_PRE_ENSE = True\r\n    ENS_TOP_VLOSS_NUM = 2\r\n    ENS_TOP_VACC_NUM = 2\r\n    ENS_TOP_ONE_VACC_NUM=1\r\n    ENS_TOP_MODEL_NUM=3\r\n    MODEL_INDEX =0\r\n    MAX_TIMES_LIST= [3,10,12,12]\r\n    THRESHOLD_DIFF_LIST = [0.001 for i in range(len(MAX_TIMES_LIST))]\r\n\r\nclass LogicModel(Model):\r\n    def __init__(self, metadata, session=None):\r\n        super(LogicModel, self).__init__(metadata)\r\n\r\n        test_metadata_filename = self.metadata.get_dataset_name().replace(\'train\', \'test\') + \'/metadata.textproto\'\r\n        self.num_test = [int(line.split(\':\')[1]) for line in open(test_metadata_filename, \'r\').readlines()[:3] if \'sample_count\' in line][0]\r\n\r\n        self.release_freeze = False\r\n        self.epoch_metrics = dict()\r\n        self.ensembleconfig = EnsembleConfig()\r\n        self.best_score = 0.95\r\n        self.best_loss = 100.00\r\n        self.history_preds = None\r\n\r\n        self.base_info = {\r\n            \'dataset\': {\r\n                \'path\': self.metadata.get_dataset_name(),\r\n                \'shape\': self.metadata.get_tensor_size(0),\r\n                \'size\': self.metadata.size(),\r\n                \'num_class\': self.metadata.get_output_size()\r\n            },\r\n            \'terminate\':False\r\n        }\r\n\r\n        self.base_hyper_params = {\r\n            \'dataset\': {\r\n                \'max_epoch\': 1000,  # initial value\r\n            },\r\n            \'conditions\': {\r\n                \'score_type\': \'auc\',\r\n                \'early_epoch\': 1,\r\n                \'skip_valid_score_threshold\': 0.90,  # if bigger then 1.0 is not use\r\n                \'skip_valid_after_test\': min(10, max(3, int(self.base_info[\'dataset\'][\'size\'] // 1000))),\r\n                \'test_after_at_least_seconds\': 1,\r\n                \'test_after_at_least_seconds_max\': 90,\r\n                \'test_after_at_least_seconds_step\': 2,\r\n                \'threshold_valid_score_diff\': 0.01,\r\n                \'threshold_valid_best_score\': 0.997,\r\n                \'decide_threshold_valid_best_score\': 0.9300,\r\n                \'max_inner_loop_ratio\': 0.1,\r\n                \'min_lr\': 1e-6,\r\n                \'use_fast_auto_aug\': True\r\n            }\r\n        }\r\n\r\n        self.ensemble_test_index = 0\r\n        self.change_next_model = False\r\n        self.ensemble_predictions = dict()\r\n        self.ensemble_predict_prob_list = dict()\r\n        self.ensemble_checkpoints = dict()\r\n        self.ensemble_g_his_eval_dict = dict()\r\n        self.start_ensemble = False\r\n        self.ensemble_lr = [1.0]\r\n        self.ensemble_epoch = [-1]\r\n\r\n        self.timers = {\r\n            \'train\': skeleton.utils.Timer(),\r\n            \'test\': skeleton.utils.Timer()\r\n        }\r\n\r\n        self.build()\r\n\r\n        self.dataloaders = {\r\n            \'train\': None,\r\n            \'valid\': None,\r\n            \'test\': None\r\n        }\r\n        self.is_skip_valid = True\r\n\r\n    def __repr__(self):\r\n        return \'\\n---------[{0}]---------\\nbase_info:{1}\\ninfo:{2}\\nparams:{3}\\n---------- ---------\'.format(\r\n            self.__class__.__name__,\r\n            self.base_info,self.model.info,self.model.hyper_params\r\n        )\r\n\r\n    def build(self):\r\n        raise NotImplementedError\r\n\r\n    def update_model(self):\r\n        # call after to scan train sample\r\n        pass\r\n\r\n    def epoch_train(self, epoch, train):\r\n        raise NotImplementedError\r\n\r\n    def epoch_valid(self, epoch, valid):\r\n        raise NotImplementedError\r\n\r\n    def skip_valid(self, epoch):\r\n        raise NotImplementedError\r\n\r\n    def prediction(self, dataloader, model, checkpoints):\r\n        raise NotImplementedError\r\n\r\n    def adapt(self):\r\n        raise NotImplementedError\r\n\r\n    def is_multiclass(self):\r\n        return self.base_info[\'dataset\'][\'sample\'][\'is_multiclass\']\r\n\r\n    def is_video(self):\r\n        return self.base_info[\'dataset\'][\'sample\'][\'is_video\']\r\n\r\n    def release_model(self,remain_times):\r\n        if remain_times < 1120 and self.ensembleconfig.MODEL_INDEX ==0:\r\n            for m in self.model.layer1.modules():\r\n                for p in m.parameters():\r\n                    p.requires_grad_(True)\r\n            for m in self.model.stem.modules():\r\n                for p in m.parameters():\r\n                    p.requires_grad_(True)\r\n        self.release_freeze = True\r\n\r\n    def build_or_get_train_dataloader(self, dataset):\r\n        if not self.model.info[\'condition\'][\'first\'][\'train\']:\r\n            return self.build_or_get_dataloader(\'train\')\r\n\r\n        num_images = self.base_info[\'dataset\'][\'size\']\r\n\r\n        num_valids = int(min(num_images * self.model.hyper_params[\'dataset\'][\'cv_valid_ratio\'], self.model.hyper_params[\'dataset\'][\'max_valid_count\']))\r\n        num_trains = num_images - num_valids\r\n\r\n        num_samples = self.model.hyper_params[\'dataset\'][\'train_info_sample\']\r\n        sample = dataset.take(num_samples).prefetch(buffer_size=num_samples)\r\n        train = skeleton.data.TFDataset(self.session, sample, num_samples)\r\n        self.base_info[\'dataset\'][\'sample\'] = train.scan(samples=num_samples)\r\n        del train\r\n        del sample\r\n\r\n        # input_shape = [min(s, self.model.hyper_params[\'dataset\'][\'max_size\']) for s in self.info[\'dataset\'][\'shape\']]\r\n        times, height, width, channels = self.base_info[\'dataset\'][\'sample\'][\'example\'][\'shape\']\r\n        values = self.base_info[\'dataset\'][\'sample\'][\'example\'][\'value\']\r\n        aspect_ratio = width / height\r\n\r\n        if aspect_ratio > 2 or 1. / aspect_ratio > 2:\r\n            self.model.hyper_params[\'dataset\'][\'max_size\'] *= 2\r\n        size = [min(s, self.model.hyper_params[\'dataset\'][\'max_size\']) for s in [height, width]]\r\n\r\n        if aspect_ratio > 1:\r\n            size[0] = size[1] / aspect_ratio\r\n        else:\r\n            size[1] = size[0] * aspect_ratio\r\n\r\n        if width <= 32 and height <= 32:\r\n            input_shape = [times, height, width, channels]\r\n        else:\r\n            fit_size_fn = lambda x: int(x / self.model.hyper_params[\'dataset\'][\'base\'] + 0.8) * self.model.hyper_params[\'dataset\'][\'base\']\r\n            size = list(map(fit_size_fn, size))\r\n            min_times = min(times, self.model.hyper_params[\'dataset\'][\'max_times\'])\r\n            input_shape = [fit_size_fn(min_times) if min_times > self.model.hyper_params[\'dataset\'][\'base\'] else min_times] + size + [channels]\r\n\r\n        if self.is_video():\r\n            self.model.hyper_params[\'dataset\'][\'batch_size\'] = int(self.model.hyper_params[\'dataset\'][\'batch_size\'] // 2)\r\n\r\n        self.model.hyper_params[\'dataset\'][\'input\'] = input_shape\r\n\r\n        num_class = self.base_info[\'dataset\'][\'num_class\']\r\n        batch_size = self.model.hyper_params[\'dataset\'][\'batch_size\']\r\n        if num_class > batch_size / 2 and not self.is_video():\r\n            self.model.hyper_params[\'dataset\'][\'batch_size\'] = batch_size * 2\r\n        batch_size = self.model.hyper_params[\'dataset\'][\'batch_size\']\r\n\r\n        preprocessor1 = get_tf_resize(input_shape[1], input_shape[2], times=input_shape[0], min_value=values[\'min\'], max_value=values[\'max\'])\r\n        \r\n        dataset = dataset.map(\r\n            lambda *x: (preprocessor1(x[0]), x[1]),\r\n            num_parallel_calls=4\r\n        )\r\n\r\n        must_shuffle = self.base_info[\'dataset\'][\'sample\'][\'label\'][\'zero_count\'] / self.base_info[\'dataset\'][\'num_class\'] >= 0.5\r\n        enough_count = self.model.hyper_params[\'dataset\'][\'enough_count\'][\'video\'] if self.is_video() else self.model.hyper_params[\'dataset\'][\'enough_count\'][\'image\']\r\n        if must_shuffle or num_images < enough_count:\r\n            dataset = dataset.shuffle(buffer_size=min(enough_count, num_images), reshuffle_each_iteration=False)\r\n\r\n        train = dataset.skip(num_valids)\r\n        valid = dataset.take(num_valids)\r\n        self.datasets = {\r\n            \'train\': train,\r\n            \'valid\': valid,\r\n            \'num_trains\': num_trains,\r\n            \'num_valids\': num_valids\r\n        }\r\n        return self.build_or_get_dataloader(\'train\', self.datasets[\'train\'], num_trains)\r\n\r\n    def build_or_get_dataloader(self, mode, dataset=None, num_items=0):\r\n        if mode in self.dataloaders and self.dataloaders[mode] is not None:\r\n            return self.dataloaders[mode]\r\n\r\n        enough_count = self.model.hyper_params[\'dataset\'][\'enough_count\'][\'video\'] if self.is_video() else self.model.hyper_params[\'dataset\'][\'enough_count\'][\'image\']\r\n\r\n        values = self.base_info[\'dataset\'][\'sample\'][\'example\'][\'value\']\r\n        if mode == \'train\':\r\n            batch_size = self.model.hyper_params[\'dataset\'][\'batch_size\']\r\n            preprocessor = get_tf_to_tensor(is_random_flip=True)\r\n\r\n            if num_items < enough_count:\r\n                dataset = dataset.cache()\r\n\r\n            dataset = dataset.repeat()\r\n            dataset = dataset.map(\r\n                lambda *x: (preprocessor(x[0]), x[1]),\r\n                num_parallel_calls=4\r\n            )\r\n            dataset = dataset.prefetch(buffer_size=batch_size * 8)\r\n\r\n            dataset = skeleton.data.TFDataset(self.session, dataset, num_items)\r\n\r\n            transform = tv.transforms.Compose([\r\n                # skeleton.data.Cutout(int(input_shape[1] // 4), int(input_shape[2] // 4))\r\n            ])\r\n            dataset = skeleton.data.TransformDataset(dataset, transform, index=0)\r\n\r\n            self.dataloaders[\'train\'] = skeleton.data.FixedSizeDataLoader(\r\n                dataset,\r\n                steps=self.model.hyper_params[\'dataset\'][\'steps_per_epoch\'],\r\n                batch_size=batch_size,\r\n                shuffle=False, drop_last=True, num_workers=0, pin_memory=False\r\n            )\r\n        \r\n        elif mode in [\'valid\', \'test\']:\r\n            batch_size = self.model.hyper_params[\'dataset\'][\'batch_size_test\']\r\n            input_shape = self.model.hyper_params[\'dataset\'][\'input\']\r\n\r\n            preprocessor2 = get_tf_to_tensor(is_random_flip=False)\r\n            if mode == \'valid\':\r\n                preprocessor = preprocessor2\r\n            else:\r\n                preprocessor1 = get_tf_resize(input_shape[1], input_shape[2], times=input_shape[0], min_value=values[\'min\'], max_value=values[\'max\'])\r\n                preprocessor = lambda *tensor: preprocessor2(preprocessor1(*tensor))\r\n\r\n            tf_dataset = dataset.apply(\r\n                tf.data.experimental.map_and_batch(\r\n                    map_func=lambda *x: (preprocessor(x[0]), x[1]),\r\n                    batch_size=batch_size,\r\n                    drop_remainder=False,\r\n                    num_parallel_calls=4\r\n                )\r\n            ).prefetch(buffer_size=8)\r\n\r\n            dataset = skeleton.data.TFDataset(self.session, tf_dataset, num_items)\r\n\r\n            self.base_info[\'dataset\'][mode], tensors = dataset.scan(\r\n                with_tensors=True, is_batch=True,\r\n                device=self.device, half=self.is_half\r\n            )\r\n            tensors = [torch.cat(t, dim=0) for t in zip(*tensors)]\r\n\r\n            del tf_dataset\r\n            del dataset\r\n            dataset = skeleton.data.prefetch_dataset(tensors)\r\n            if \'valid\' == mode:\r\n                transform = tv.transforms.Compose([\r\n                ])\r\n                dataset = skeleton.data.TransformDataset(dataset, transform, index=0)\r\n\r\n            self.dataloaders[mode] = torch.utils.data.DataLoader(\r\n                dataset,\r\n                batch_size=self.model.hyper_params[\'dataset\'][\'batch_size_test\'],\r\n                shuffle=False, drop_last=False, num_workers=0, pin_memory=False\r\n            )\r\n\r\n            self.model.info[\'condition\'][\'first\'][mode] = False\r\n\r\n        return self.dataloaders[mode]\r\n\r\n    def update_condition(self, metrics=None):\r\n        self.model.info[\'condition\'][\'first\'][\'train\'] = False\r\n        self.model.info[\'loop\'][\'epoch\'] += 1\r\n\r\n        metrics.update({\'epoch\': self.model.info[\'loop\'][\'epoch\']})\r\n        self.model.checkpoints.append(metrics)\r\n\r\n        indices = np.argsort(np.array([v[\'valid\'][\'score\'] for v in self.model.checkpoints] if len(self.model.checkpoints) > 0 else [0]))\r\n        indices = sorted(indices[::-1][:self.model.hyper_params[\'checkpoints\'][\'keep\']])\r\n        self.model.checkpoints = [self.model.checkpoints[i] for i in indices]\r\n    \r\n    def break_train_loop_condition(self, remaining_time_budget=None, inner_epoch=1):\r\n        consume = inner_epoch * self.timers[\'train\'].step_time\r\n\r\n        best_idx = np.argmax(np.array([c[\'valid\'][\'score\'] for c in self.model.checkpoints]))\r\n        best_epoch = self.model.checkpoints[best_idx][\'epoch\']\r\n        best_loss = self.model.checkpoints[best_idx][\'valid\'][\'loss\']\r\n        best_score = self.model.checkpoints[best_idx][\'valid\'][\'score\']\r\n        lr = self.model.optimizer.get_learning_rate()\r\n\r\n        if self.model.info[\'loop\'][\'epoch\'] <= self.model.hyper_params[\'conditions\'][\'early_epoch\']:\r\n            return True\r\n\r\n        if best_score > self.model.hyper_params[\'conditions\'][\'threshold_valid_best_score\']:\r\n            return True\r\n\r\n        if consume > self.model.hyper_params[\'conditions\'][\'test_after_at_least_seconds\'] and \\\r\n            self.model.checkpoints[best_idx][\'epoch\'] > self.model.info[\'loop\'][\'epoch\'] - inner_epoch and \\\r\n            best_score > self.model.info[\'loop\'][\'best_score\'] * 1.001:\r\n\r\n            self.model.hyper_params[\'conditions\'][\'test_after_at_least_seconds\'] = min(\r\n                self.model.hyper_params[\'conditions\'][\'test_after_at_least_seconds_max\'],\r\n                self.model.hyper_params[\'conditions\'][\'test_after_at_least_seconds\'] + self.model.hyper_params[\'conditions\'][\'test_after_at_least_seconds_step\']\r\n            )\r\n\r\n            self.model.info[\'loop\'][\'best_score\'] = best_score\r\n            return True\r\n\r\n        if lr < self.model.hyper_params[\'conditions\'][\'min_lr\']:\r\n            return True\r\n\r\n        early_term_budget = 3 * 60\r\n        expected_more_time = (self.timers[\'test\'].step_time + (self.timers[\'train\'].step_time * 2)) * 1.5\r\n        \r\n        if remaining_time_budget is not None and \\\r\n            remaining_time_budget - early_term_budget < expected_more_time:\r\n            return True\r\n        \r\n        if self.model.info[\'loop\'][\'epoch\'] >= 20 and \\\r\n            inner_epoch > self.model.hyper_params[\'dataset\'][\'max_epoch\'] * self.model.hyper_params[\'conditions\'][\'max_inner_loop_ratio\']:\r\n            return True\r\n\r\n        return False\r\n\r\n    def decide_change_next_model(self,remaining_time_budget=None, inner_epoch=0):\r\n        \r\n        if self.ensembleconfig.MODEL_INDEX ==0:\r\n            valid_abs_losses = 10\r\n            valid_abs_scores = False\r\n        else:\r\n            valid_losses = [(c[\'valid\'][\'loss\']-self.best_loss) for c in self.model.checkpoints][-10:]\r\n            valid_abs_losses = sum([num > 0.0 for num in valid_losses])\r\n\r\n        valid_scores = [c[\'valid\'][\'score\'] for c in self.model.checkpoints]\r\n        diff = (max(valid_scores) - min(valid_scores)) * (1 - max(valid_scores))\r\n        train_scores = [c[\'train\'][\'score\'] for c in self.model.checkpoints]\r\n        \r\n        threshold = self.model.hyper_params[\'conditions\'][\'threshold_valid_score_diff\']\r\n        \r\n        early_stop_scores = np.mean(np.array(train_scores[-3:])) ==1.00\r\n\r\n        if self.ensembleconfig.MODEL_INDEX == 0 and remaining_time_budget <= 900:\r\n            self.ensembleconfig.MODEL_INDEX == 0\r\n            self.change_next_model = False\r\n\r\n        elif self.ensembleconfig.MODEL_INDEX ==0 and max(valid_scores)>=0.9 and remaining_time_budget>=1000 and early_stop_scores or \\\r\n            (diff < threshold and self.model.info[\'loop\'][\'epoch\'] >= 20):\r\n            self.ensemble_checkpoints[self.ensembleconfig.MODEL_INDEX] = self.model.checkpoints\r\n            self.ensemble_predict_prob_list[self.ensembleconfig.MODEL_INDEX]= self.model.predict_prob_list\r\n            self.ensemble_g_his_eval_dict[self.ensembleconfig.MODEL_INDEX] = self.model.g_his_eval_dict\r\n            self.ensemble_epoch.append(self.model.info[\'loop\'][\'epoch\'])\r\n            self.ensemble_lr.append(self.model.optimizer.get_learning_rate())\r\n\r\n            self.ensembleconfig.MODEL_INDEX += 1\r\n            if self.ensembleconfig.MODEL_INDEX == len(self.model_space):\r\n                self.ensembleconfig.MODEL_INDEX = len(self.model_space) - 1\r\n                self.change_next_model = False\r\n            else:\r\n                self.ensembleconfig.MODEL_INDEX = self.ensembleconfig.MODEL_INDEX\r\n                self.change_next_model = True\r\n\r\n            return self.change_next_model\r\n\r\n\r\n        elif (valid_abs_losses == 0 and max(valid_scores)>=0.9) or (diff < threshold and self.model.info[\'loop\'][\'epoch\'] >= 20):\r\n            self.ensemble_checkpoints[self.ensembleconfig.MODEL_INDEX] = self.model.checkpoints\r\n            self.ensemble_predict_prob_list[self.ensembleconfig.MODEL_INDEX]= self.model.predict_prob_list\r\n            self.ensemble_g_his_eval_dict[self.ensembleconfig.MODEL_INDEX] = self.model.g_his_eval_dict\r\n            self.ensemble_epoch.append(self.model.info[\'loop\'][\'epoch\'])\r\n            self.ensemble_lr.append(self.model.optimizer.get_learning_rate())\r\n\r\n            self.ensembleconfig.MODEL_INDEX += 1\r\n            if self.ensembleconfig.MODEL_INDEX == len(self.model_space):\r\n                self.ensembleconfig.MODEL_INDEX = len(self.model_space) -1\r\n                self.change_next_model = False\r\n            else:\r\n                self.ensembleconfig.MODEL_INDEX = self.ensembleconfig.MODEL_INDEX\r\n                self.change_next_model = True\r\n\r\n            return self.change_next_model\r\n        else:\r\n            self.change_next_model = False\r\n        \r\n            return self.change_next_model\r\n\r\n    def terminate_train_loop_condition(self, remaining_time_budget=None, inner_epoch=0):\r\n        early_term_budget = 3 * 60\r\n        expected_more_time = (self.timers[\'test\'].step_time + (self.timers[\'train\'].step_time * 2)) * 1.5\r\n        \r\n        if remaining_time_budget is not None and \\\r\n            remaining_time_budget - early_term_budget < expected_more_time:\r\n            self.base_info[\'terminate\'] = True\r\n            self.model.info[\'terminate\'] = True\r\n            self.done_training = True\r\n            return True\r\n\r\n        if self.ensembleconfig.MODEL_INDEX >= len(self.ensembleconfig.MAX_TIMES_LIST):\r\n            return True\r\n\r\n        if min(self.ensemble_lr) < self.model.hyper_params[\'conditions\'][\'min_lr\']:\r\n            done = True if self.base_info[\'terminate\'] else False\r\n            self.base_info[\'terminate\'] = True\r\n            self.model.info[\'terminate\'] = True\r\n            self.done_training = done\r\n            return True\r\n\r\n        if max(self.ensemble_epoch) >= 20 and \\\r\n            inner_epoch > self.base_hyper_params[\'dataset\'][\'max_epoch\'] * self.base_hyper_params[\'conditions\'][\'max_inner_loop_ratio\']:\r\n            done = True if self.base_info[\'terminate\'] else False\r\n            self.base_info[\'terminate\'] = True\r\n            self.model.info[\'terminate\'] = True\r\n            self.done_training = done\r\n            return True\r\n\r\n        return False\r\n\r\n    def get_total_time(self):\r\n        return sum([self.timers[key].total_time for key in self.timers.keys()])\r\n\r\n    def train(self, dataset, remaining_time_budget=None):\r\n        if self.change_next_model == True:\r\n            self.release_freeze = False\r\n            self.model = self.model_space[self.ensembleconfig.MODEL_INDEX].to(device=self.device, non_blocking=True)\r\n            self.model_pred = self.model_space[self.ensembleconfig.MODEL_INDEX].to(device=self.device, non_blocking=True).eval()\r\n            self.datasets.clear()\r\n            self.dataloaders.clear()\r\n            self.change_next_model = False\r\n        \r\n        self.timers[\'train\'](\'outer_start\', exclude_total=True, reset_step=True)\r\n        self.model.hyper_params[\'dataset\'][\'max_times\'] = self.ensembleconfig.MAX_TIMES_LIST[self.ensembleconfig.MODEL_INDEX]\r\n        self.model.hyper_params[\'conditions\'][\'threshold_valid_score_diff\'] = self.ensembleconfig.THRESHOLD_DIFF_LIST[self.ensembleconfig.MODEL_INDEX]\r\n\r\n        train_dataloader = self.build_or_get_train_dataloader(dataset)\r\n        if self.model.info[\'condition\'][\'first\'][\'train\']:\r\n            self.update_model()\r\n        self.timers[\'train\'](\'build_dataset\')\r\n\r\n        inner_epoch = 0\r\n        while True:\r\n            inner_epoch += 1\r\n            remaining_time_budget -= self.timers[\'train\'].step_time\r\n\r\n            self.timers[\'train\'](\'start\', reset_step=True)\r\n            train_metrics = self.epoch_train(self.model.info[\'loop\'][\'epoch\'], train_dataloader)\r\n            self.timers[\'train\'](\'train\')\r\n\r\n            train_score = np.min([c[\'train\'][\'score\'] for c in self.model.checkpoints[-20:] + [{\'train\': train_metrics}]])\r\n            if train_score > self.model.hyper_params[\'conditions\'][\'skip_valid_score_threshold\'] or \\\r\n                self.model.info[\'loop\'][\'test\'] >= self.base_hyper_params[\'conditions\'][\'skip_valid_after_test\']:\r\n                self.is_skip_valid = False\r\n                is_first = self.model.info[\'condition\'][\'first\'][\'valid\']\r\n                valid_dataloader = self.build_or_get_dataloader(\'valid\', self.datasets[\'valid\'], self.datasets[\'num_valids\'])\r\n                self.timers[\'train\'](\'valid_dataset\', exclude_step=is_first)\r\n                valid_metrics = self.epoch_valid(self.model.info[\'loop\'][\'epoch\'], valid_dataloader)\r\n                \r\n            else:\r\n                self.is_skip_valid = True\r\n                valid_metrics = self.skip_valid(self.model)\r\n            self.timers[\'train\'](\'valid\')\r\n\r\n            if self.best_loss >= valid_metrics[\'loss\']:\r\n                self.best_loss = valid_metrics[\'loss\']\r\n\r\n            metrics = {\r\n                \'epoch\': self.model.info[\'loop\'][\'epoch\'],\r\n                \'model\': self.model.state_dict().copy(),\r\n                \'train\': train_metrics,\r\n                \'valid\': valid_metrics,\r\n            }\r\n\r\n            self.epoch_metrics = metrics\r\n            self.update_condition(metrics)\r\n            self.timers[\'train\'](\'adapt\', exclude_step=True)\r\n\r\n\r\n            self.model.hyper_params[\'dataset\'][\'max_epoch\'] = self.model.info[\'loop\'][\'epoch\'] + remaining_time_budget // self.timers[\'train\'].step_time\r\n\r\n            if self.break_train_loop_condition(remaining_time_budget, inner_epoch):\r\n                break\r\n\r\n            self.timers[\'train\'](\'end\')\r\n\r\n        remaining_time_budget -= self.timers[\'train\'].step_time\r\n        \r\n        if self.release_freeze== False:\r\n            self.release_model(remaining_time_budget)\r\n\r\n        self.decide_change_next_model(remaining_time_budget, inner_epoch)\r\n        self.terminate_train_loop_condition(remaining_time_budget, inner_epoch)\r\n\r\n        if not self.done_training:\r\n            self.adapt(remaining_time_budget)\r\n\r\n        self.timers[\'train\'](\'outer_end\')\r\n\r\n\r\n    def test(self, dataset, remaining_time_budget=None):\r\n        if (self.ensembleconfig.MODEL_INDEX == 0) and (self.model == self.model_space[self.ensembleconfig.MODEL_INDEX]):\r\n            rv=self.base_test(dataset,self.model,self.model.checkpoints,remaining_time_budget=None)\r\n        else:\r\n            self.base_test(dataset,self.model,self.model.checkpoints,remaining_time_budget=None)\r\n            rv=self.ensemble_prediction()\r\n        self.history_preds = rv\r\n        return rv\r\n\r\n    def base_test(self, dataset, model, checkpoints, remaining_time_budget=None):\r\n        self.model.ensemble_test_index += 1\r\n\r\n        self.timers[\'test\'](\'start\', exclude_total=True, reset_step=True)\r\n        is_first = self.model.info[\'condition\'][\'first\'][\'test\']\r\n        self.model.info[\'loop\'][\'test\'] += 1\r\n\r\n        dataloader = self.build_or_get_dataloader(\'test\', dataset, self.num_test)\r\n        self.timers[\'test\'](\'build_dataset\', reset_step=is_first)\r\n        rv = self.prediction(dataloader, model=model,checkpoints=model.checkpoints)\r\n        self.timers[\'test\'](\'end\')\r\n\r\n        scores = [c[\'valid\'][\'score\'] for c in self.model.checkpoints]\r\n        diff = (max(scores) - min(scores)) * (1 - max(scores))\r\n        if self.best_score <= max(scores):\r\n            self.best_score = max(scores)\r\n        \r\n        if (self.ensembleconfig.MODEL_INDEX == 0 and max(scores)>=0.9):\r\n            self.model.ensemble_scores[self.model.ensemble_test_index] = {\r\n                \'loss\':self.epoch_metrics[\'valid\'][\'loss\'],\r\n                \'score\':self.epoch_metrics[\'valid\'][\'score\']\r\n            }\r\n            self.model.ensemble_predictions[self.model.ensemble_test_index]={\r\n                \'predictions\':rv\r\n            }\r\n        elif self.epoch_metrics[\'valid\'][\'score\'] >= 0.99 * self.best_score and self.epoch_metrics[\'valid\'][\'loss\'] <= self.best_loss:\r\n            self.model.ensemble_scores[self.model.ensemble_test_index] = {\r\n                \'loss\':self.epoch_metrics[\'valid\'][\'loss\'],\r\n                \'score\':self.epoch_metrics[\'valid\'][\'score\']\r\n            }\r\n            self.model.ensemble_predictions[self.model.ensemble_test_index]={\r\n                \'predictions\':rv\r\n            }\r\n        return rv\r\n\r\n    def get_top_players(self, data, sort_keys, reverse=True, n=2, order=True):\r\n        top = sorted(data.items(), key=lambda x: x[1][sort_keys], reverse=reverse)[:n]\r\n        if order:\r\n            return OrderedDict(top)\r\n        return dict(top)\r\n\r\n    def ensemble_prediction(self):\r\n        preds_dict = list()\r\n        for _,item_model in enumerate(self.model_space[:self.ensembleconfig.MODEL_INDEX+1]):\r\n            if len(item_model.ensemble_predictions)>=1:\r\n                preds_dict.append(self.ensemble_single_prediction(item_model.ensemble_predictions,item_model.ensemble_scores))\r\n\r\n        if len(preds_dict) == 0:\r\n            return self.history_preds\r\n        elif len(preds_dict) == 1:\r\n            return preds_dict[0]\r\n        else:\r\n            for key,logits in enumerate(preds_dict):\r\n                logits = (logits - np.min(logits))/(np.max(logits)-np.min(logits))\r\n                if key ==0:\r\n                    logits = logits\r\n                else:\r\n                    logits += logits\r\n            return logits/(key+1)\r\n\r\n    def ensemble_single_prediction(self,predict_prob_list,g_his_eval_dict):\r\n        key_t_loss = ""loss""\r\n        key_t_acc = ""score""\r\n        key_loss = key_t_loss\r\n        key_acc = key_t_acc\r\n\r\n        topn_vloss = self.ensembleconfig.ENS_TOP_VLOSS_NUM\r\n        topn_vacc = self.ensembleconfig.ENS_TOP_VACC_NUM\r\n\r\n        pre_en_eval_rounds = list(predict_prob_list.keys())\r\n        \r\n        cur_eval_dict = {k: g_his_eval_dict.get(k) for k in pre_en_eval_rounds}\r\n        top_n_val_acc_evals = self.get_top_players(data=cur_eval_dict, sort_keys=key_acc, n=topn_vacc, reverse=True)\r\n        top_n_val_acc_evals = list(top_n_val_acc_evals.items())\r\n        topn_valacc_roundidx = [a[0] for a in top_n_val_acc_evals]\r\n\r\n        top_n_val_loss_evals = self.get_top_players(data=cur_eval_dict, sort_keys=key_loss, n=topn_vloss,reverse=False)\r\n        top_n_val_loss_evals = list(top_n_val_loss_evals.items())\r\n        topn_valloss_roundidx = [a[0] for a in top_n_val_loss_evals]\r\n        \r\n        \r\n        merge_roundids = list()\r\n        merge_roundids.extend(topn_valloss_roundidx)\r\n        merge_roundids.extend(topn_valacc_roundidx)\r\n\r\n        merge_preds_res = [predict_prob_list[roundid] for roundid in merge_roundids]\r\n        if len(merge_roundids) == 1:\r\n            return merge_preds_res[0][\'predictions\']\r\n        else:\r\n            for key,items in enumerate(merge_preds_res):\r\n                logits = items[\'predictions\']\r\n                if key ==0:\r\n                    logits = logits\r\n                else:\r\n                    logits += logits\r\n            return logits/(key+1)'"
AutoDL_sample_code_submission/Auto_Video/skeleton/projects/others.py,0,"b'# -*- coding: utf-8 -*-\r\nfrom __future__ import absolute_import\r\nimport sys\r\nimport os\r\nimport logging\r\nfrom functools import reduce\r\n\r\nimport tensorflow as tf\r\nimport torchvision as tv\r\nimport numpy as np\r\n\r\n\r\ndef get_logger(name, stream=sys.stderr, file_path=\'debug.log\'):\r\n    formatter = logging.Formatter(fmt=\'[%(asctime)s %(levelname)s %(filename)s] %(message)s\')\r\n\r\n    handler = logging.StreamHandler(stream)\r\n    # handler = logging.FileHandler(file_path)\r\n    handler.setFormatter(formatter)\r\n\r\n    logger = logging.getLogger(name)\r\n    level = logging.INFO if os.environ.get(\'LOG_LEVEL\', \'INFO\') == \'INFO\' else logging.DEBUG\r\n    logger.setLevel(level)\r\n    logger.addHandler(handler)\r\n    return logger\r\n\r\n\r\nLOGGER = get_logger(__name__)\r\n\r\ndef get_tf_resize(height, width, times=1, min_value=0.0, max_value=1.0):\r\n    def preprocessor(tensor):\r\n        in_times, in_height, in_width, in_channels = tensor.get_shape()\r\n\r\n        if width == in_width and height == in_height:\r\n            pass\r\n        else:\r\n            tensor = tf.image.resize_images(tensor, (height, width), method=tf.image.ResizeMethod.BICUBIC)\r\n\r\n        if times != in_times or times > 1:\r\n            tensor = tf.reshape(tensor, [-1, height * width, in_channels])\r\n            tensor = tf.image.resize_images(tensor, (times, height * width),\r\n                                            method=tf.image.ResizeMethod.BICUBIC)\r\n            tensor = tf.reshape(tensor, [times, height, width, in_channels])\r\n        if times == 1:\r\n            tensor = tensor[int(times // 2)]\r\n\r\n        delta = max_value - min_value\r\n        if delta < 0.9 or delta > 1.1 or min_value < -0.1 or min_value > 0.1:\r\n            tensor = (tensor - min_value) / delta\r\n        return tensor\r\n\r\n    return preprocessor\r\n\r\n\r\ndef get_tf_to_tensor(is_random_flip=True):\r\n    def preprocessor(tensor):\r\n        if is_random_flip:\r\n            tensor = tf.image.random_flip_left_right(tensor)\r\n        dims = len(tensor.shape)\r\n        if dims == 3:\r\n            tensor = tf.transpose(tensor, perm=[2, 0, 1])\r\n        elif dims == 4:\r\n            tensor = tf.transpose(tensor, perm=[3, 0, 1, 2])\r\n        return tensor\r\n\r\n    return preprocessor\r\n\r\n\r\ndef NBAC(logits, labels):\r\n    if logits.device != labels.device:\r\n        labels = labels.to(device=logits.device)\r\n\r\n    positive_mask = labels > 0\r\n    negative_mask = labels < 1\r\n\r\n    tpr = (logits * labels).sum() / positive_mask.sum()\r\n    tnr = ((1 - logits) * (1 - labels)).sum() / negative_mask.sum()\r\n    return tpr, tnr, (tpr + tnr - 1)\r\n\r\n\r\ndef tiedrank(a):\r\n    \'\'\' Return the ranks (with base 1) of a list resolving ties by averaging.\r\n     This works for numpy arrays.\'\'\'\r\n    m = len(a)\r\n    # Sort a in ascending order (sa=sorted vals, i=indices)\r\n    i = a.argsort()\r\n    sa = a[i]\r\n    # Find unique values\r\n    uval = np.unique(a)\r\n    # Test whether there are ties\r\n    R = np.arange(m, dtype=float) + 1  # Ranks with base 1\r\n    if len(uval) != m:\r\n        # Average the ranks for the ties\r\n        oldval = sa[0]\r\n        newval = sa[0]\r\n        k0 = 0\r\n        for k in range(1, m):\r\n            newval = sa[k]\r\n            if newval == oldval:\r\n                # moving average\r\n                R[k0:k + 1] = R[k - 1] * (k - k0) / (k - k0 + 1) + R[k] / (k - k0 + 1)\r\n            else:\r\n                k0 = k;\r\n                oldval = newval\r\n    # Invert the index\r\n    S = np.empty(m)\r\n    S[i] = R\r\n    return S\r\n\r\n\r\ndef mvmean(R, axis=0):\r\n    \'\'\' Moving average to avoid rounding errors. A bit slow, but...\r\n    Computes the mean along the given axis, except if this is a vector, in which case the mean is returned.\r\n    Does NOT flatten.\'\'\'\r\n    if len(R.shape) == 0: return R\r\n    average = lambda x: reduce(lambda i, j: (0, (j[0] / (j[0] + 1.)) * i[1] + (1. / (j[0] + 1)) * j[1]), enumerate(x))[\r\n        1]\r\n    R = np.array(R)\r\n    if len(R.shape) == 1: return average(R)\r\n    if axis == 1:\r\n        return np.array(map(average, R))\r\n    else:\r\n        return np.array(map(average, R.transpose()))\r\n\r\n\r\ndef get_valid_columns(solution):\r\n    """"""Get a list of column indices for which the column has more than one class.\r\n    This is necessary when computing BAC or AUC which involves true positive and\r\n    true negative in the denominator. When some class is missing, these scores\r\n    don\'t make sense (or you have to add an epsilon to remedy the situation).\r\n\r\n    Args:\r\n    solution: array, a matrix of binary entries, of shape\r\n      (num_examples, num_features)\r\n    Returns:\r\n    valid_columns: a list of indices for which the column has more than one\r\n      class.\r\n    """"""\r\n    num_examples = solution.shape[0]\r\n    col_sum = np.sum(solution, axis=0)\r\n    valid_columns = np.where(1 - np.isclose(col_sum, 0) -\r\n                             np.isclose(col_sum, num_examples))[0]\r\n    return valid_columns\r\n\r\n\r\ndef AUC(logits, labels):\r\n    logits = logits.detach().float().cpu().numpy()\r\n    labels = labels.detach().float().cpu().numpy()\r\n\r\n    valid_columns = get_valid_columns(labels)\r\n\r\n    logits = logits[:, valid_columns].copy()\r\n    labels = labels[:, valid_columns].copy()\r\n\r\n    label_num = labels.shape[1]\r\n    if label_num == 0:\r\n        return 0.0\r\n\r\n    auc = np.empty(label_num)\r\n    for k in range(label_num):\r\n        r_ = tiedrank(logits[:, k])\r\n        s_ = labels[:, k]\r\n\r\n        npos = sum(s_ == 1)\r\n        nneg = sum(s_ < 1)\r\n        auc[k] = (sum(r_[s_ == 1]) - npos * (npos + 1) / 2) / (nneg * npos)\r\n\r\n    return 2 * mvmean(auc) - 1\r\n'"
AutoDL_sample_code_submission/Auto_Video/skeleton/utils/__init__.py,0,b'# -*- coding: utf-8 -*-\n# pylint: disable=wildcard-import\nfrom __future__ import absolute_import\n\nfrom .timer import Timer'
AutoDL_sample_code_submission/Auto_Video/skeleton/utils/log_utils.py,0,"b'import logging\nimport os\nimport sys\n\nimport time\nfrom typing import Any\nimport multiprocessing\n\nimport functools\nnesting_level = 0\nis_start = None\nNCPU = multiprocessing.cpu_count()\n\n\ndef log(entry: Any):\n    global nesting_level\n    space = ""-"" * (4 * nesting_level)\n    logger.info(""{}{}"".format(space, entry))\n\n\ndef get_logger(verbosity_level, use_error_log=False, log_path=None):\n    """"""Set logging format to something like:\n         2019-04-25 12:52:51,924 INFO score.py: <message>\n    """"""\n    logger = logging.getLogger(__file__)\n    logging_level = getattr(logging, verbosity_level)\n    logger.setLevel(logging_level)\n\n    if log_path is None:\n        log_dir = os.path.join("".."", ""log"")\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n        log_path = os.path.join(log_dir, ""log.txt"")\n    else:\n        log_path = os.path.join(log_path, ""log.txt"")\n\n    formatter = logging.Formatter(\n        fmt=\'%(asctime)s %(levelname)s %(filename)s: %(funcName)s: %(lineno)d: %(message)s\')\n\n    stdout_handler = logging.StreamHandler(sys.stdout)\n    stdout_handler.setLevel(logging_level)\n    stdout_handler.setFormatter(formatter)\n\n    logger.addHandler(stdout_handler)\n\n    fh = logging.FileHandler(log_path)\n    fh.setLevel(logging_level)\n    fh.setFormatter(formatter)\n\n    logger.addHandler(fh)\n\n    if use_error_log:\n        stderr_handler = logging.StreamHandler(sys.stderr)\n        stderr_handler.setLevel(logging.WARNING)\n        stderr_handler.setFormatter(formatter)\n        logger.addHandler(stderr_handler)\n    logger.propagate = False\n    return logger\n\n\nlogger = get_logger(\'INFO\')\n\ndebug = logger.debug\ninfo = logger.info\nwarning = logger.warning\nerror = logger.error\n\n\ndef timeit(method, start_log=None):\n    @functools.wraps(method)\n    def timed(*args, **kw):\n        global is_start\n        global nesting_level\n\n        if not is_start:\n            print()\n\n        is_start = True\n        log(""Start [{}]:"".format(method.__name__)+ (start_log if start_log else """"))\n        nesting_level += 1\n\n        start_time = time.time()\n        result = method(*args, **kw)\n        end_time = time.time()\n\n        nesting_level -= 1\n        log(""End   [{}]. Time elapsed: {} sec."".format(method.__name__, end_time - start_time))\n        is_start = False\n\n        return result\n\n    return timed'"
AutoDL_sample_code_submission/Auto_Video/skeleton/utils/timer.py,0,"b""# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nimport time\nfrom collections import OrderedDict\nimport logging\n\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass Timer:\n    def __init__(self):\n        self.times = [time.time()]\n        self.accumulation = OrderedDict({})\n        self.total_time = 0.0\n        self.step_time = 0.0\n\n    def __call__(self, name, exclude_total=False, exclude_step=False, reset_step=False):\n        self.times.append(time.time())\n        delta = self.times[-1] - self.times[-2]\n\n        if name not in self.accumulation:\n            self.accumulation[name] = 0.0\n        self.accumulation[name] += delta\n\n        if not exclude_total:\n            self.total_time += delta\n\n        if reset_step:\n            self.step_time = 0.0\n        elif not exclude_step:\n            self.step_time += delta\n\n        return delta\n\n    def __repr__(self):\n        results = []\n        for key, value in self.accumulation.items():\n            results.append('{0}={1:.3f}'.format(key, value))\n        return self.__class__.__name__ + '(total={0}, step={1}, {2})'.format(\n            self.total_time, self.step_time, ', '.join(results)\n        )\n"""
AutoDL_sample_code_submission/Auto_Image/skeleton/nn/modules/__init__.py,0,b'# -*- coding: utf-8 -*-\n# pylint: disable=wildcard-import\nfrom __future__ import absolute_import\n\nfrom .hooks import MoveToHook\nfrom .wrappers import *\nfrom .loss import *\n'
AutoDL_sample_code_submission/Auto_Image/skeleton/nn/modules/hooks.py,1,"b'# -*- coding: utf-8 -*-\n# pylint: disable=arguments-differ, abstract-method\nfrom __future__ import absolute_import\nimport logging\n\nimport torch\nfrom torch import nn\n\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass MoveToHook(nn.Module):\n    @staticmethod\n    def to(tensors, device, half=False):\n        for t in tensors:\n            if isinstance(t, (tuple, list)):\n                MoveToHook.to(t, device, half)\n            if not isinstance(t, torch.Tensor):\n                continue\n            t.data = t.data.to(device=device)\n            if half:\n                if t.is_floating_point():\n                    t.data = t.data.half()\n\n    @staticmethod\n    def get_forward_pre_hook(device, half=False):\n        def hook(module, inputs):\n            _ = module\n            MoveToHook.to(inputs, device, half)\n        return hook\n\n'"
AutoDL_sample_code_submission/Auto_Image/skeleton/nn/modules/loss.py,4,"b""# -*- coding: utf-8 -*-\n# pylint: disable=arguments-differ, abstract-method\nfrom __future__ import absolute_import\nimport logging\n\nimport torch\n\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass CrossEntropyLabelSmooth(torch.nn.Module):\n    def __init__(self, num_classes, epsilon=0.1, sparse_target=True, reduction='avg'):\n        super(CrossEntropyLabelSmooth, self).__init__()\n        self.num_classes = num_classes\n        self.epsilon = epsilon\n        self.sparse_target = sparse_target\n        self.logsoftmax = torch.nn.LogSoftmax(dim=1)\n        self.reduction = reduction\n\n    def forward(self, input, target):  # pylint: disable=redefined-builtin\n        log_probs = self.logsoftmax(input)\n        if self.sparse_target:\n            targets = torch.zeros_like(log_probs).scatter_(1, target.unsqueeze(1), 1)\n        else:\n            targets = target\n        targets = (1 - self.epsilon) * targets + (self.epsilon / self.num_classes)\n        loss = (-targets * log_probs)\n        if self.reduction == 'avg':\n            loss = loss.mean(0).sum()\n        elif self.reduction == 'sum':\n            loss = loss.sum()\n\n        return loss\n\n\nclass BinaryCrossEntropyLabelSmooth(torch.nn.BCEWithLogitsLoss):\n    def __init__(self, num_classes, epsilon=0.1, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None):\n        super(BinaryCrossEntropyLabelSmooth, self).__init__(weight, size_average, reduce, reduction, pos_weight)\n        self.num_classes = num_classes\n        self.epsilon = epsilon\n\n    def forward(self, input, target):  # pylint: disable=redefined-builtin\n        target = (1 - self.epsilon) * target + self.epsilon\n        return super(BinaryCrossEntropyLabelSmooth, self).forward(input, target)\n"""
AutoDL_sample_code_submission/Auto_Image/skeleton/nn/modules/wrappers.py,30,"b""# -*- coding: utf-8 -*-\n# pylint: disable=arguments-differ\nfrom __future__ import absolute_import\nimport logging\nfrom functools import wraps\nfrom collections import OrderedDict\n\nimport torch\nimport numpy as np\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass ToDevice(torch.nn.Module):\n    def __init__(self):\n        super(ToDevice, self).__init__()\n        self.register_buffer('buf', torch.zeros(1, dtype=torch.float32))\n\n    def forward(self, *xs):\n        if len(xs) == 1 and isinstance(xs[0], (tuple, list)):\n            xs = xs[0]\n\n        device = self.buf.device\n        out = []\n        for x in xs:\n            if x is not None and x.device != device:\n                out.append(x.to(device=device))\n            else:\n                out.append(x)\n        return out[0] if len(xs) == 1 else tuple(out)\n\n\nclass CopyChannels(torch.nn.Module):\n    def __init__(self, multiple=3, dim=1):\n        super(CopyChannels, self).__init__()\n        self.multiple = multiple\n        self.dim = dim\n\n    def forward(self, x):\n        return torch.cat([x for _ in range(self.multiple)], dim=self.dim)\n\n\nclass Normalize(torch.nn.Module):\n    def __init__(self, mean, std, inplace=False):\n        super(Normalize, self).__init__()\n        if isinstance(mean, list):\n            self.register_buffer('mean', torch.tensor(mean, dtype=torch.float32)[None, :, None, None])\n            self.register_buffer('std', torch.tensor(std, dtype=torch.float32)[None, :, None, None])\n        else:\n            self.register_buffer('mean', torch.tensor([mean], dtype=torch.float32)[None, :, None, None])\n            self.register_buffer('std', torch.tensor([std], dtype=torch.float32)[None, :, None, None])\n        self.inplace = inplace\n\n    def forward(self, x):\n        if not self.inplace:\n            x = x.clone()\n\n        x.sub_(self.mean).div_(self.std)\n        return x\n\n\nclass Reshape(torch.nn.Module):\n    def __init__(self, *shape):\n        super(Reshape, self).__init__()\n        self.shape = shape\n\n    def forward(self, x):\n        return x.view(*self.shape)\n\n\nclass Flatten(torch.nn.Module):\n    def __init__(self):\n        super(Flatten, self).__init__()\n\n    def forward(self, x):\n        batch = x.shape[0]\n        return x.view([batch, -1])\n\n\nclass SplitTime(torch.nn.Module):\n    def __init__(self, times):\n        super(SplitTime, self).__init__()\n        self.times = times\n\n    def forward(self, x):\n        batch, channels, height, width = x.shape\n        return x.view(-1, self.times, channels, height, width)\n\n\nclass Permute(torch.nn.Module):\n    def __init__(self, *dims):\n        super(Permute, self).__init__()\n        self.dims = dims\n\n    def forward(self, x):\n        return x.permute(*self.dims)\n\n\nclass Cutout(torch.nn.Module):\n    def __init__(self, ratio=0.0):\n        super(Cutout, self).__init__()\n        self.ratio = ratio\n\n    def forward(self, input):\n        batch, channel, height, width = input.shape\n        w = int(width * self.ratio)\n        h = int(height * self.ratio)\n\n        if self.training and w > 0 and h > 0:\n            x = np.random.randint(width, size=(batch,))\n            y = np.random.randint(height, size=(batch,))\n\n            x1s = np.clip(x - w // 2, 0, width)\n            x2s = np.clip(x + w // 2, 0, width)\n            y1s = np.clip(y - h // 2, 0, height)\n            y2s = np.clip(y + h // 2, 0, height)\n\n            mask = torch.ones_like(input)\n            for idx, (x1, x2, y1, y2) in enumerate(zip(x1s, x2s, y1s, y2s)):\n                mask[idx, :, y1:y2, x1:x2] = 0.\n\n            input = input * mask\n        return input\n\n\nclass Mul(torch.nn.Module):\n    def __init__(self, weight):\n        super(Mul, self).__init__()\n        self.weight = weight\n\n    def forward(self, x):\n        return x * self.weight\n\n\nclass Flatten(torch.nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\n\ndef decorator_tuple_to_args(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        args = list(args)\n        if len(args) == 2 and isinstance(args[1], (tuple, list)):\n            args[1:] = list(args[1])\n        return func(*args, **kwargs)\n\n    return wrapper\n\n\nclass Concat(torch.nn.Module):\n    def __init__(self, dim=1):\n        super(Concat, self).__init__()\n        self.dim = dim\n\n    @decorator_tuple_to_args\n    def forward(self, *xs):\n        return torch.cat(xs, dim=self.dim)\n\n\nclass MergeSum(torch.nn.Module):\n    @decorator_tuple_to_args\n    def forward(self, *xs):\n        return torch.sum(torch.stack(xs), dim=0)\n\n\nclass MergeProd(torch.nn.Module):\n    @decorator_tuple_to_args\n    def forward(self, *xs):\n        return xs[0] * xs[1]\n\n\nclass Choice(torch.nn.Module):\n    def __init__(self, idx=0):\n        super(Choice, self).__init__()\n        self.idx = idx\n\n    @decorator_tuple_to_args\n    def forward(self, *xs):\n        return xs[self.idx]\n\n\nclass Toggle(torch.nn.Module):\n    def __init__(self, module):\n        super(Toggle, self).__init__()\n        self.module = module\n        self.on = True\n\n    def forward(self, x):\n        return self.module(x) if self.on else x\n\n\nclass Split(torch.nn.Module):\n    def __init__(self, *modules):\n        super(Split, self).__init__()\n        if len(modules) == 1 and isinstance(modules[0], OrderedDict):\n            for key, module in modules[0].items():\n                self.add_module(key, module)\n        else:\n            for idx, module in enumerate(modules):\n                self.add_module(str(idx), module)\n\n    def forward(self, x):\n        return tuple([m(x) for m in self._modules.values()])\n\n\nclass DropPath(torch.nn.Module):\n    def __init__(self, drop_prob=0.0):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        self._half = False\n\n    def forward(self, x):\n        if self.training and self.drop_prob > 0.:\n            shape = list(x.shape[:1]) + [1 for _ in x.shape[1:]]\n            keep_prob = 1. - self.drop_prob\n            mask = torch.cuda.FloatTensor(*shape).bernoulli_(keep_prob)\n            if self._half:\n                mask = mask.half()\n            x.div_(keep_prob)\n            x.mul_(mask)\n        return x\n\n    def half(self):\n        self._half = True\n\n    def float(self):\n        self._half = False\n\n\nclass DelayedPass(torch.nn.Module):\n    def __init__(self):\n        super(DelayedPass, self).__init__()\n        self.register_buffer('keep', None)\n\n    def forward(self, x):\n        rv = self.keep  # pylint: disable=access-member-before-definition\n        self.keep = x\n        return rv\n\n\nclass Reader(torch.nn.Module):\n    def __init__(self, x=None):\n        super(Reader, self).__init__()\n        self.x = x\n\n    def forward(self, x):  # pylint: disable=unused-argument\n        return self.x\n\n\nclass KeepByPass(torch.nn.Module):\n    def __init__(self):\n        super(KeepByPass, self).__init__()\n        self._reader = Reader()\n        self.info = {}\n\n    @property\n    def x(self):\n        return self._reader.x\n\n    def forward(self, x):\n        self._reader.x = x\n        return x\n\n    def reader(self):\n        return self._reader\n"""
AutoDL_sample_code_submission/Auto_Image/skeleton/projects/api/__init__.py,0,b'# -*- coding: utf-8 -*-\n# pylint: disable=wildcard-import\nfrom __future__ import absolute_import\n\nfrom .model import Model\n'
AutoDL_sample_code_submission/Auto_Image/skeleton/projects/api/model.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\n\n\nclass Model:\n    def __init__(self, metadata):\n        """"""\n        :param metadata: an AutoDLMetadata object. Its definition can be found in\n            https://github.com/zhengying-liu/autodl_starting_kit_stable/blob/master/AutoDL_ingestion_program/dataset.py#L41\n        """"""\n        self.metadata = metadata\n\n        self.done_training = False\n        # the loop of calling \'train\' and \'test\' will only run if self.done_training = False\n        # otherwinse, the looop will go until the time budge in used up set self.done_training = True\n        # when you think the model is converged or when is not enough time for next round of traning\n\n    def train(self, dataset, remaining_time_budget=None):\n        """"""\n        :param dataset: a `tf.data.Dataset` object. Each of its examples is of the form (example, labels)\n            where `example` is a dense 4-D Tensor of shape  (sequence_size, row_count, col_count, num_channels)\n            and `labels` is a 1-D Tensor of shape (output_dim,)\n            Here `output_dim` represents number of classes of this multilabel classification task.\n        :param remaining_time_budget: a float, time remaining to execute train()\n        :return: None\n        """"""\n        raise NotImplementedError\n\n    def test(self, dataset, remaining_time_budget=None):\n        """"""\n        :param: Same as that of `train` method, except that the labes will be empty (all zeros)\n        :return: predictions: A `numpy.ndarray` matrix of shape (sample_count, output_dim)\n            The values should be binary or in the interval [0,1].\n        """"""\n        raise NotImplementedError\n'"
AutoDL_sample_code_submission/Auto_Video/skeleton/nn/modules/__init__.py,0,b'# -*- coding: utf-8 -*-\n# pylint: disable=wildcard-import\nfrom __future__ import absolute_import\n\nfrom .profile import Profile\nfrom .hooks import MoveToHook\nfrom .wrappers import *\nfrom .loss import *\n'
AutoDL_sample_code_submission/Auto_Video/skeleton/nn/modules/hooks.py,1,"b'# -*- coding: utf-8 -*-\n# pylint: disable=arguments-differ, abstract-method\nfrom __future__ import absolute_import\nimport logging\n\nimport torch\nfrom torch import nn\n\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass MoveToHook(nn.Module):\n    @staticmethod\n    def to(tensors, device, half=False):\n        for t in tensors:\n            if isinstance(t, (tuple, list)):\n                MoveToHook.to(t, device, half)\n            if not isinstance(t, torch.Tensor):\n                continue\n            t.data = t.data.to(device=device)\n            if half:\n                if t.is_floating_point():\n                    t.data = t.data.half()\n\n    @staticmethod\n    def get_forward_pre_hook(device, half=False):\n        def hook(module, inputs):\n            _ = module\n            MoveToHook.to(inputs, device, half)\n        return hook\n\n'"
AutoDL_sample_code_submission/Auto_Video/skeleton/nn/modules/loss.py,4,"b""# -*- coding: utf-8 -*-\n# pylint: disable=arguments-differ, abstract-method\nfrom __future__ import absolute_import\nimport logging\n\nimport torch\n\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass CrossEntropyLabelSmooth(torch.nn.Module):\n    def __init__(self, num_classes, epsilon=0.1, sparse_target=True, reduction='avg'):\n        super(CrossEntropyLabelSmooth, self).__init__()\n        self.num_classes = num_classes\n        self.epsilon = epsilon\n        self.sparse_target = sparse_target\n        self.logsoftmax = torch.nn.LogSoftmax(dim=1)\n        self.reduction = reduction\n\n    def forward(self, input, target):  # pylint: disable=redefined-builtin\n        log_probs = self.logsoftmax(input)\n        if self.sparse_target:\n            targets = torch.zeros_like(log_probs).scatter_(1, target.unsqueeze(1), 1)\n        else:\n            targets = target\n        targets = (1 - self.epsilon) * targets + (self.epsilon / self.num_classes)\n        loss = (-targets * log_probs)\n        if self.reduction == 'avg':\n            loss = loss.mean(0).sum()\n        elif self.reduction == 'sum':\n            loss = loss.sum()\n\n        return loss\n\n\nclass BinaryCrossEntropyLabelSmooth(torch.nn.BCEWithLogitsLoss):\n    def __init__(self, num_classes, epsilon=0.1, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None):\n        super(BinaryCrossEntropyLabelSmooth, self).__init__(weight, size_average, reduce, reduction, pos_weight)\n        self.num_classes = num_classes\n        self.epsilon = epsilon\n\n    def forward(self, input, target):  # pylint: disable=redefined-builtin\n        target = (1 - self.epsilon) * target + self.epsilon\n        return super(BinaryCrossEntropyLabelSmooth, self).forward(input, target)\n"""
AutoDL_sample_code_submission/Auto_Video/skeleton/nn/modules/profile.py,19,"b""# -*- coding: utf-8 -*-\n# pylint: disable=arguments-differ, abstract-method\nfrom __future__ import absolute_import\nimport logging\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\n\n# LOGGER = logging.getLogger(__name__)\n\n\nclass Profile:\n    def __init__(self, module):\n        self.module = module\n\n    def params(self, name_filter=lambda name: True):\n        return np.sum(params.numel() for name, params in self.module.named_parameters() if name_filter(name))\n\n    def flops(self, *inputs, name_filter=lambda name: 'skeleton' not in name and 'loss' not in name):\n        operation_flops = []\n\n        def get_hook(name):\n            def counting(module, inp, outp):\n                class_name = module.__class__.__name__\n\n                flops = 0\n                module_type = type(module)\n                if not name_filter(str(module_type)):\n                    pass\n                elif module_type in COUNT_FN_MAP:\n                    fn = COUNT_FN_MAP[module_type]\n                    flops = fn(module, inp, outp) if fn is not None else 0\n                else:\n                    pass\n\n                data = {\n                    'name': name,\n                    'class_name': class_name,\n                    'flops': flops,\n                }\n                operation_flops.append(data)\n            return counting\n\n        handles = []\n        for name, module in self.module.named_modules():\n            if len(list(module.children())) > 0:  # pylint: disable=len-as-condition\n                continue\n            handle = module.register_forward_hook(get_hook(name))\n            handles.append(handle)\n\n        _ = self.module(*inputs)\n\n        # remove hook\n        _ = [h.remove() for h in handles]\n\n        return np.sum([data['flops'] for data in operation_flops if name_filter(data['name'])])\n\n\nCOUNT_OP_MULTIPLY_ADD = 1\n\n\ndef count_conv2d(m, x, y):\n    # TODO: add support for pad and dilation\n    x = x[0]\n\n    cin = m.in_channels\n    cout = m.out_channels\n    kh, kw = m.kernel_size\n    batch_size = x.size()[0]\n\n    out_w = y.size(2) // m.stride[0]\n    out_h = y.size(3) // m.stride[1]\n\n\n    kernel_ops = COUNT_OP_MULTIPLY_ADD * kh * kw * cin // m.groups\n    bias_ops = 1 if m.bias is not None else 0\n    ops_per_element = kernel_ops + bias_ops\n\n\n    output_elements = batch_size * out_w * out_h * cout\n    total_ops = output_elements * ops_per_element\n\n    return int(total_ops)\n\n\ndef count_bn2d(m, x, y):\n    x = x[0]\n\n    nelements = x.numel()\n    total_sub = nelements\n    total_div = nelements\n    total_ops = total_sub + total_div\n\n    return int(total_ops)\n\n\ndef count_relu(m, x, y):\n    x = x[0]\n\n    nelements = x.numel()\n    total_ops = nelements\n\n    return int(total_ops)\n\n\ndef count_softmax(m, x, y):\n    x = x[0]\n\n    batch_size, nfeatures = x.size()\n\n    total_exp = nfeatures\n    total_add = nfeatures - 1\n    total_div = nfeatures\n    total_ops = batch_size * (total_exp + total_add + total_div)\n\n    return int(total_ops)\n\n\ndef count_maxpool(m, x, y):\n    kernel_ops = torch.prod(torch.Tensor([m.kernel_size])) - 1\n    num_elements = y.numel()\n    total_ops = kernel_ops * num_elements\n\n    return int(total_ops)\n\n\ndef count_avgpool(m, x, y):\n    total_add = torch.prod(torch.Tensor([m.kernel_size])) - 1\n    total_div = 1\n    kernel_ops = total_add + total_div\n    num_elements = y.numel()\n    total_ops = kernel_ops * num_elements\n\n    return int(total_ops)\n\n\ndef count_global_avgpool(m, x, y):\n    x = x[0]\n\n    w, h = x.size(2), x.size(1)\n    total_add = w * h - 1\n    total_div = 1\n    kernel_ops = total_add + total_div\n    num_elements = y.numel()\n    total_ops = kernel_ops * num_elements\n\n    return int(total_ops)\n\n\ndef count_linear(m, x, y):\n    total_mul = m.in_features\n    total_add = m.in_features - 1\n    num_elements = y.numel()\n    total_ops = (total_mul + total_add) * num_elements\n\n    return int(total_ops)\n\n\nCOUNT_FN_MAP = {\n    torch.nn.Conv2d: count_conv2d,\n    torch.nn.BatchNorm2d: count_bn2d,\n    torch.nn.ReLU: count_relu,\n    torch.nn.ReLU6: count_relu,\n    torch.nn.LeakyReLU: count_relu,\n    torch.nn.MaxPool1d: count_maxpool,\n    torch.nn.MaxPool2d: count_maxpool,\n    torch.nn.MaxPool3d: count_maxpool,\n    torch.nn.AvgPool1d: count_avgpool,\n    torch.nn.AvgPool2d: count_avgpool,\n    torch.nn.AvgPool3d: count_avgpool,\n    torch.nn.AdaptiveAvgPool1d: count_global_avgpool,\n    torch.nn.AdaptiveAvgPool2d: count_global_avgpool,\n    torch.nn.AdaptiveAvgPool3d: count_global_avgpool,\n    torch.nn.Linear: count_linear,\n    torch.nn.Dropout: None,\n    # torch.nn.Identity: None,\n}\n"""
AutoDL_sample_code_submission/Auto_Video/skeleton/nn/modules/wrappers.py,32,"b""# -*- coding: utf-8 -*-\n# pylint: disable=arguments-differ\nfrom __future__ import absolute_import\nimport logging\nfrom functools import wraps\nfrom collections import OrderedDict\n\nimport torch\nimport numpy as np\n\n# LOGGER = logging.getLogger(__name__)\n\n\nclass ToDevice(torch.nn.Module):\n    def __init__(self):\n        super(ToDevice, self).__init__()\n        self.register_buffer('buf', torch.zeros(1, dtype=torch.float32))\n\n    def forward(self, *xs):\n        if len(xs) == 1 and isinstance(xs[0], (tuple, list)):\n            xs = xs[0]\n\n        device = self.buf.device\n        out = []\n        for x in xs:\n            if x is not None and x.device != device:\n                out.append(x.to(device=device))\n            else:\n                out.append(x)\n        return out[0] if len(xs) == 1 else tuple(out)\n\n\nclass CopyChannels(torch.nn.Module):\n    def __init__(self, multiple=3, dim=1):\n        super(CopyChannels, self).__init__()\n        self.multiple = multiple\n        self.dim = dim\n\n    def forward(self, x):\n        return torch.cat([x for _ in range(self.multiple)], dim=self.dim)\n\nclass Normalize(torch.nn.Module):\n    def __init__(self, mean, std, mode,inplace=False):\n        super(Normalize, self).__init__()\n        if mode =='conv3d':\n            if isinstance(mean, list):\n                self.register_buffer('mean', torch.tensor(mean, dtype=torch.float32)[None, :, None, None, None])\n                self.register_buffer('std', torch.tensor(std, dtype=torch.float32)[None, :, None, None, None])\n            else:\n                self.register_buffer('mean', torch.tensor([mean], dtype=torch.float32)[None, :, None, None, None])\n                self.register_buffer('std', torch.tensor([std], dtype=torch.float32)[None, :, None, None, None])\n        else:\n            self.register_buffer('mean', torch.tensor([mean], dtype=torch.float32)[None, :, None, None])\n            self.register_buffer('std', torch.tensor([std], dtype=torch.float32)[None, :, None, None])\n        \n        self.inplace = inplace\n\n    def forward(self, x):\n        if not self.inplace:\n            x = x.clone()\n\n        x.sub_(self.mean).div_(self.std)\n        return x\n\n\nclass Reshape(torch.nn.Module):\n    def __init__(self, *shape):\n        super(Reshape, self).__init__()\n        self.shape = shape\n\n    def forward(self, x):\n        return x.view(*self.shape)\n\n\nclass Flatten(torch.nn.Module):\n    def __init__(self):\n        super(Flatten, self).__init__()\n\n    def forward(self, x):\n        batch = x.shape[0]\n        return x.view([batch, -1])\n\n\nclass SplitTime(torch.nn.Module):\n    def __init__(self, times):\n        super(SplitTime, self).__init__()\n        self.times = times\n\n    def forward(self, x):\n        batch, channels, height, width = x.shape\n        return x.view(-1, self.times, channels, height, width)\n\n\nclass Permute(torch.nn.Module):\n    def __init__(self, *dims):\n        super(Permute, self).__init__()\n        self.dims = dims\n\n    def forward(self, x):\n        return x.permute(*self.dims)\n\n\nclass Cutout(torch.nn.Module):\n    def __init__(self, ratio=0.0):\n        super(Cutout, self).__init__()\n        self.ratio = ratio\n\n    def forward(self, input):\n        batch, channel, height, width = input.shape\n        w = int(width * self.ratio)\n        h = int(height * self.ratio)\n\n        if self.training and w > 0 and h > 0:\n            x = np.random.randint(width, size=(batch,))\n            y = np.random.randint(height, size=(batch,))\n\n            x1s = np.clip(x - w // 2, 0, width)\n            x2s = np.clip(x + w // 2, 0, width)\n            y1s = np.clip(y - h // 2, 0, height)\n            y2s = np.clip(y + h // 2, 0, height)\n\n            mask = torch.ones_like(input)\n            for idx, (x1, x2, y1, y2) in enumerate(zip(x1s, x2s, y1s, y2s)):\n                mask[idx, :, y1:y2, x1:x2] = 0.\n\n            input = input * mask\n        return input\n\n\nclass Mul(torch.nn.Module):\n    def __init__(self, weight):\n        super(Mul, self).__init__()\n        self.weight = weight\n\n    def forward(self, x):\n        return x * self.weight\n\n\nclass Flatten(torch.nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\n\ndef decorator_tuple_to_args(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        args = list(args)\n        if len(args) == 2 and isinstance(args[1], (tuple, list)):\n            args[1:] = list(args[1])\n        return func(*args, **kwargs)\n\n    return wrapper\n\n\nclass Concat(torch.nn.Module):\n    def __init__(self, dim=1):\n        super(Concat, self).__init__()\n        self.dim = dim\n\n    @decorator_tuple_to_args\n    def forward(self, *xs):\n        return torch.cat(xs, dim=self.dim)\n\n\nclass MergeSum(torch.nn.Module):\n    @decorator_tuple_to_args\n    def forward(self, *xs):\n        return torch.sum(torch.stack(xs), dim=0)\n\n\nclass MergeProd(torch.nn.Module):\n    @decorator_tuple_to_args\n    def forward(self, *xs):\n\n        return xs[0] * xs[1]\n\n\nclass Choice(torch.nn.Module):\n    def __init__(self, idx=0):\n        super(Choice, self).__init__()\n        self.idx = idx\n\n    @decorator_tuple_to_args\n    def forward(self, *xs):\n        return xs[self.idx]\n\n\nclass Toggle(torch.nn.Module):\n    def __init__(self, module):\n        super(Toggle, self).__init__()\n        self.module = module\n        self.on = True\n\n    def forward(self, x):\n        return self.module(x) if self.on else x\n\n\nclass Split(torch.nn.Module):\n    def __init__(self, *modules):\n        super(Split, self).__init__()\n        if len(modules) == 1 and isinstance(modules[0], OrderedDict):\n            for key, module in modules[0].items():\n                self.add_module(key, module)\n        else:\n            for idx, module in enumerate(modules):\n                self.add_module(str(idx), module)\n\n    def forward(self, x):\n        return tuple([m(x) for m in self._modules.values()])\n\n\nclass DropPath(torch.nn.Module):\n    def __init__(self, drop_prob=0.0):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        self._half = False\n\n    def forward(self, x):\n        if self.training and self.drop_prob > 0.:\n            shape = list(x.shape[:1]) + [1 for _ in x.shape[1:]]\n            keep_prob = 1. - self.drop_prob\n            mask = torch.cuda.FloatTensor(*shape).bernoulli_(keep_prob)\n            if self._half:\n                mask = mask.half()\n            x.div_(keep_prob)\n            x.mul_(mask)\n        return x\n\n    def half(self):\n        self._half = True\n\n    def float(self):\n        self._half = False\n\n\nclass DelayedPass(torch.nn.Module):\n    def __init__(self):\n        super(DelayedPass, self).__init__()\n        self.register_buffer('keep', None)\n\n    def forward(self, x):\n        rv = self.keep  # pylint: disable=access-member-before-definition\n        self.keep = x\n        return rv\n\n\nclass Reader(torch.nn.Module):\n    def __init__(self, x=None):\n        super(Reader, self).__init__()\n        self.x = x\n\n    def forward(self, x):  # pylint: disable=unused-argument\n        return self.x\n\n\nclass KeepByPass(torch.nn.Module):\n    def __init__(self):\n        super(KeepByPass, self).__init__()\n        self._reader = Reader()\n        self.info = {}\n\n    @property\n    def x(self):\n        return self._reader.x\n\n    def forward(self, x):\n        self._reader.x = x\n        return x\n\n    def reader(self):\n        return self._reader\n"""
AutoDL_sample_code_submission/Auto_Video/skeleton/projects/api/__init__.py,0,b'# -*- coding: utf-8 -*-\n# pylint: disable=wildcard-import\nfrom __future__ import absolute_import\n\nfrom .model import Model\n'
AutoDL_sample_code_submission/Auto_Video/skeleton/projects/api/model.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\n\n\nclass Model:\n    def __init__(self, metadata):\n        """"""\n        :param metadata: an AutoDLMetadata object. Its definition can be found in\n            https://github.com/zhengying-liu/autodl_starting_kit_stable/blob/master/AutoDL_ingestion_program/dataset.py#L41\n        """"""\n        self.metadata = metadata\n\n        self.done_training = False\n        # the loop of calling \'train\' and \'test\' will only run if self.done_training = False\n        # otherwinse, the looop will go until the time budge in used up set self.done_training = True\n        # when you think the model is converged or when is not enough time for next round of traning\n\n    def train(self, dataset, remaining_time_budget=None):\n        """"""\n        :param dataset: a `tf.data.Dataset` object. Each of its examples is of the form (example, labels)\n            where `example` is a dense 4-D Tensor of shape  (sequence_size, row_count, col_count, num_channels)\n            and `labels` is a 1-D Tensor of shape (output_dim,)\n            Here `output_dim` represents number of classes of this multilabel classification task.\n        :param remaining_time_budget: a float, time remaining to execute train()\n        :return: None\n        """"""\n        raise NotImplementedError\n\n    def test(self, dataset, remaining_time_budget=None):\n        """"""\n        :param: Same as that of `train` method, except that the labes will be empty (all zeros)\n        :return: predictions: A `numpy.ndarray` matrix of shape (sample_count, output_dim)\n            The values should be binary or in the interval [0,1].\n        """"""\n        raise NotImplementedError\n'"
