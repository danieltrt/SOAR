file_path,api_count,code
layers.py,24,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass GraphAttentionLayer(nn.Module):\n    """"""\n    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n    """"""\n\n    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n        super(GraphAttentionLayer, self).__init__()\n        self.dropout = dropout\n        self.in_features = in_features\n        self.out_features = out_features\n        self.alpha = alpha\n        self.concat = concat\n\n        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n\n        self.leakyrelu = nn.LeakyReLU(self.alpha)\n\n    def forward(self, input, adj):\n        h = torch.mm(input, self.W)\n        N = h.size()[0]\n\n        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n\n        zero_vec = -9e15*torch.ones_like(e)\n        attention = torch.where(adj > 0, e, zero_vec)\n        attention = F.softmax(attention, dim=1)\n        attention = F.dropout(attention, self.dropout, training=self.training)\n        h_prime = torch.matmul(attention, h)\n\n        if self.concat:\n            return F.elu(h_prime)\n        else:\n            return h_prime\n\n    def __repr__(self):\n        return self.__class__.__name__ + \' (\' + str(self.in_features) + \' -> \' + str(self.out_features) + \')\'\n\n\nclass SpecialSpmmFunction(torch.autograd.Function):\n    """"""Special function for only sparse region backpropataion layer.""""""\n    @staticmethod\n    def forward(ctx, indices, values, shape, b):\n        assert indices.requires_grad == False\n        a = torch.sparse_coo_tensor(indices, values, shape)\n        ctx.save_for_backward(a, b)\n        ctx.N = shape[0]\n        return torch.matmul(a, b)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        a, b = ctx.saved_tensors\n        grad_values = grad_b = None\n        if ctx.needs_input_grad[1]:\n            grad_a_dense = grad_output.matmul(b.t())\n            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n            grad_values = grad_a_dense.view(-1)[edge_idx]\n        if ctx.needs_input_grad[3]:\n            grad_b = a.t().matmul(grad_output)\n        return None, grad_values, None, grad_b\n\n\nclass SpecialSpmm(nn.Module):\n    def forward(self, indices, values, shape, b):\n        return SpecialSpmmFunction.apply(indices, values, shape, b)\n\n    \nclass SpGraphAttentionLayer(nn.Module):\n    """"""\n    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n    """"""\n\n    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n        super(SpGraphAttentionLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.alpha = alpha\n        self.concat = concat\n\n        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n        nn.init.xavier_normal_(self.W.data, gain=1.414)\n                \n        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n        nn.init.xavier_normal_(self.a.data, gain=1.414)\n\n        self.dropout = nn.Dropout(dropout)\n        self.leakyrelu = nn.LeakyReLU(self.alpha)\n        self.special_spmm = SpecialSpmm()\n\n    def forward(self, input, adj):\n        dv = \'cuda\' if input.is_cuda else \'cpu\'\n\n        N = input.size()[0]\n        edge = adj.nonzero().t()\n\n        h = torch.mm(input, self.W)\n        # h: N x out\n        assert not torch.isnan(h).any()\n\n        # Self-attention on the nodes - Shared attention mechanism\n        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n        # edge: 2*D x E\n\n        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n        assert not torch.isnan(edge_e).any()\n        # edge_e: E\n\n        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv))\n        # e_rowsum: N x 1\n\n        edge_e = self.dropout(edge_e)\n        # edge_e: E\n\n        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n        assert not torch.isnan(h_prime).any()\n        # h_prime: N x out\n        \n        h_prime = h_prime.div(e_rowsum)\n        # h_prime: N x out\n        assert not torch.isnan(h_prime).any()\n\n        if self.concat:\n            # if this layer is not last layer,\n            return F.elu(h_prime)\n        else:\n            # if this layer is last layer,\n            return h_prime\n\n    def __repr__(self):\n        return self.__class__.__name__ + \' (\' + str(self.in_features) + \' -> \' + str(self.out_features) + \')\'\n'"
models.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom layers import GraphAttentionLayer, SpGraphAttentionLayer\n\n\nclass GAT(nn.Module):\n    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n        """"""Dense version of GAT.""""""\n        super(GAT, self).__init__()\n        self.dropout = dropout\n\n        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n        for i, attention in enumerate(self.attentions):\n            self.add_module(\'attention_{}\'.format(i), attention)\n\n        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n\n    def forward(self, x, adj):\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = F.elu(self.out_att(x, adj))\n        return F.log_softmax(x, dim=1)\n\n\nclass SpGAT(nn.Module):\n    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n        """"""Sparse version of GAT.""""""\n        super(SpGAT, self).__init__()\n        self.dropout = dropout\n\n        self.attentions = [SpGraphAttentionLayer(nfeat, \n                                                 nhid, \n                                                 dropout=dropout, \n                                                 alpha=alpha, \n                                                 concat=True) for _ in range(nheads)]\n        for i, attention in enumerate(self.attentions):\n            self.add_module(\'attention_{}\'.format(i), attention)\n\n        self.out_att = SpGraphAttentionLayer(nhid * nheads, \n                                             nclass, \n                                             dropout=dropout, \n                                             alpha=alpha, \n                                             concat=False)\n\n    def forward(self, x, adj):\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = F.elu(self.out_att(x, adj))\n        return F.log_softmax(x, dim=1)\n\n'"
train.py,9,"b'from __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport glob\nimport time\nimport random\nimport argparse\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nfrom utils import load_data, accuracy\nfrom models import GAT, SpGAT\n\n# Training settings\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False, help=\'Disables CUDA training.\')\nparser.add_argument(\'--fastmode\', action=\'store_true\', default=False, help=\'Validate during training pass.\')\nparser.add_argument(\'--sparse\', action=\'store_true\', default=False, help=\'GAT with sparse version or not.\')\nparser.add_argument(\'--seed\', type=int, default=72, help=\'Random seed.\')\nparser.add_argument(\'--epochs\', type=int, default=10000, help=\'Number of epochs to train.\')\nparser.add_argument(\'--lr\', type=float, default=0.005, help=\'Initial learning rate.\')\nparser.add_argument(\'--weight_decay\', type=float, default=5e-4, help=\'Weight decay (L2 loss on parameters).\')\nparser.add_argument(\'--hidden\', type=int, default=8, help=\'Number of hidden units.\')\nparser.add_argument(\'--nb_heads\', type=int, default=8, help=\'Number of head attentions.\')\nparser.add_argument(\'--dropout\', type=float, default=0.6, help=\'Dropout rate (1 - keep probability).\')\nparser.add_argument(\'--alpha\', type=float, default=0.2, help=\'Alpha for the leaky_relu.\')\nparser.add_argument(\'--patience\', type=int, default=100, help=\'Patience\')\n\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nrandom.seed(args.seed)\nnp.random.seed(args.seed)\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\n# Load data\nadj, features, labels, idx_train, idx_val, idx_test = load_data()\n\n# Model and optimizer\nif args.sparse:\n    model = SpGAT(nfeat=features.shape[1], \n                nhid=args.hidden, \n                nclass=int(labels.max()) + 1, \n                dropout=args.dropout, \n                nheads=args.nb_heads, \n                alpha=args.alpha)\nelse:\n    model = GAT(nfeat=features.shape[1], \n                nhid=args.hidden, \n                nclass=int(labels.max()) + 1, \n                dropout=args.dropout, \n                nheads=args.nb_heads, \n                alpha=args.alpha)\noptimizer = optim.Adam(model.parameters(), \n                       lr=args.lr, \n                       weight_decay=args.weight_decay)\n\nif args.cuda:\n    model.cuda()\n    features = features.cuda()\n    adj = adj.cuda()\n    labels = labels.cuda()\n    idx_train = idx_train.cuda()\n    idx_val = idx_val.cuda()\n    idx_test = idx_test.cuda()\n\nfeatures, adj, labels = Variable(features), Variable(adj), Variable(labels)\n\n\ndef train(epoch):\n    t = time.time()\n    model.train()\n    optimizer.zero_grad()\n    output = model(features, adj)\n    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n    acc_train = accuracy(output[idx_train], labels[idx_train])\n    loss_train.backward()\n    optimizer.step()\n\n    if not args.fastmode:\n        # Evaluate validation set performance separately,\n        # deactivates dropout during validation run.\n        model.eval()\n        output = model(features, adj)\n\n    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n    acc_val = accuracy(output[idx_val], labels[idx_val])\n    print(\'Epoch: {:04d}\'.format(epoch+1),\n          \'loss_train: {:.4f}\'.format(loss_train.data.item()),\n          \'acc_train: {:.4f}\'.format(acc_train.data.item()),\n          \'loss_val: {:.4f}\'.format(loss_val.data.item()),\n          \'acc_val: {:.4f}\'.format(acc_val.data.item()),\n          \'time: {:.4f}s\'.format(time.time() - t))\n\n    return loss_val.data.item()\n\n\ndef compute_test():\n    model.eval()\n    output = model(features, adj)\n    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n    acc_test = accuracy(output[idx_test], labels[idx_test])\n    print(""Test set results:"",\n          ""loss= {:.4f}"".format(loss_test.data[0]),\n          ""accuracy= {:.4f}"".format(acc_test.data[0]))\n\n# Train model\nt_total = time.time()\nloss_values = []\nbad_counter = 0\nbest = args.epochs + 1\nbest_epoch = 0\nfor epoch in range(args.epochs):\n    loss_values.append(train(epoch))\n\n    torch.save(model.state_dict(), \'{}.pkl\'.format(epoch))\n    if loss_values[-1] < best:\n        best = loss_values[-1]\n        best_epoch = epoch\n        bad_counter = 0\n    else:\n        bad_counter += 1\n\n    if bad_counter == args.patience:\n        break\n\n    files = glob.glob(\'*.pkl\')\n    for file in files:\n        epoch_nb = int(file.split(\'.\')[0])\n        if epoch_nb < best_epoch:\n            os.remove(file)\n\nfiles = glob.glob(\'*.pkl\')\nfor file in files:\n    epoch_nb = int(file.split(\'.\')[0])\n    if epoch_nb > best_epoch:\n        os.remove(file)\n\nprint(""Optimization Finished!"")\nprint(""Total time elapsed: {:.4f}s"".format(time.time() - t_total))\n\n# Restore best model\nprint(\'Loading {}th epoch\'.format(best_epoch))\nmodel.load_state_dict(torch.load(\'{}.pkl\'.format(best_epoch)))\n\n# Testing\ncompute_test()\n'"
utils.py,6,"b'import numpy as np\nimport scipy.sparse as sp\nimport torch\n\n\ndef encode_onehot(labels):\n    classes = set(labels)\n    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n    return labels_onehot\n\n\ndef load_data(path=""./data/cora/"", dataset=""cora""):\n    """"""Load citation network dataset (cora only for now)""""""\n    print(\'Loading {} dataset...\'.format(dataset))\n\n    idx_features_labels = np.genfromtxt(""{}{}.content"".format(path, dataset), dtype=np.dtype(str))\n    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n    labels = encode_onehot(idx_features_labels[:, -1])\n\n    # build graph\n    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n    idx_map = {j: i for i, j in enumerate(idx)}\n    edges_unordered = np.genfromtxt(""{}{}.cites"".format(path, dataset), dtype=np.int32)\n    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n\n    # build symmetric adjacency matrix\n    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n\n    features = normalize_features(features)\n    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n\n    idx_train = range(140)\n    idx_val = range(200, 500)\n    idx_test = range(500, 1500)\n\n    adj = torch.FloatTensor(np.array(adj.todense()))\n    features = torch.FloatTensor(np.array(features.todense()))\n    labels = torch.LongTensor(np.where(labels)[1])\n\n    idx_train = torch.LongTensor(idx_train)\n    idx_val = torch.LongTensor(idx_val)\n    idx_test = torch.LongTensor(idx_test)\n\n    return adj, features, labels, idx_train, idx_val, idx_test\n\n\ndef normalize_adj(mx):\n    """"""Row-normalize sparse matrix""""""\n    rowsum = np.array(mx.sum(1))\n    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n\n\ndef normalize_features(mx):\n    """"""Row-normalize sparse matrix""""""\n    rowsum = np.array(mx.sum(1))\n    r_inv = np.power(rowsum, -1).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = sp.diags(r_inv)\n    mx = r_mat_inv.dot(mx)\n    return mx\n\n\ndef accuracy(output, labels):\n    preds = output.max(1)[1].type_as(labels)\n    correct = preds.eq(labels).double()\n    correct = correct.sum()\n    return correct / len(labels)\n\n'"
visualize_graph.py,4,"b'from graphviz import Digraph\n\nimport torch\nimport models\n\ndef make_dot(var, params):\n    """""" Produces Graphviz representation of PyTorch autograd graph\n    \n    Blue nodes are the Variables that require grad, orange are Tensors\n    saved for backward in torch.autograd.Function\n    \n    Args:\n        var: output Variable\n        params: dict of (name, Variable) to add names to node that\n            require grad (TODO: make optional)\n    """"""\n    param_map = {id(v): k for k, v in params.items()}\n    print(param_map)\n    \n    node_attr = dict(style=\'filled\',\n                     shape=\'box\',\n                     align=\'left\',\n                     fontsize=\'12\',\n                     ranksep=\'0.1\',\n                     height=\'0.2\')\n    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=""12,12""))\n    seen = set()\n    \n    def size_to_str(size):\n        return \'(\'+(\', \').join([\'%d\'% v for v in size])+\')\'\n\n    def add_nodes(var):\n        if var not in seen:\n            if torch.is_tensor(var):\n                dot.node(str(id(var)), size_to_str(var.size()), fillcolor=\'orange\')\n            elif hasattr(var, \'variable\'):\n                u = var.variable\n                node_name = \'%s\\n %s\' % (param_map.get(id(u)), size_to_str(u.size()))\n                dot.node(str(id(var)), node_name, fillcolor=\'lightblue\')\n            else:\n                dot.node(str(id(var)), str(type(var).__name__))\n            seen.add(var)\n            if hasattr(var, \'next_functions\'):\n                for u in var.next_functions:\n                    if u[0] is not None:\n                        dot.edge(str(id(u[0])), str(id(var)))\n                        add_nodes(u[0])\n            if hasattr(var, \'saved_tensors\'):\n                for t in var.saved_tensors:\n                    dot.edge(str(id(t)), str(id(var)))\n                    add_nodes(t)\n    add_nodes(var.grad_fn)\n    return dot\n\ninputs = torch.randn(100, 50).cuda()\nadj = torch.randn(100, 100).cuda()\nmodel = models.SpGAT(50, 8, 7, 0.5, 0.01, 3)\nmodel = model.cuda()\ny = model(inputs, adj)\n\ng = make_dot(y, model.state_dict())\ng.view()\n'"
