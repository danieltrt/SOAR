file_path,api_count,code
amsgrad/cnn.py,6,"b""import torch \nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom optim import AMSGrad \n\n\n# Hyper Parameters\nnum_epochs = 5\nbatch_size = 100\n\nlr = 1e-3\nweight_decay = 1e-4\n\n# BN\neps = 1e-6\nmomentum = 0.99\n\n# MNIST Dataset\ntrain_dataset = dsets.MNIST(root='../data/',\n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data/',\n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# CNN Model (2 conv layer)\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n            nn.BatchNorm2d(16, momentum=momentum, eps=eps),\n            nn.MaxPool2d(2),\n            nn.ReLU())\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n            nn.BatchNorm2d(32, momentum=momentum, eps=eps),\n            nn.MaxPool2d(2),\n            nn.ReLU())\n        self.fc = nn.Linear(7*7*32, 10)\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        return out\n        \ncnn = CNN()\n\n\n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = AMSGrad(cnn.parameters(), lr=lr, weight_decay=weight_decay)  \n\n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images)\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs = cnn(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n\n# Test the Model\ncnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images)\n    outputs = cnn(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))\n\n# Save the Trained Model\ntorch.save(cnn.state_dict(), 'cnn.pkl')\n"""
amsgrad/mlp.py,5,"b""import torch\nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom optim import AMSGrad\n\n# Hyper Parameters \ninput_size = 784\nhidden_size = 500\nnum_classes = 10\nnum_epochs = 5\nbatch_size = 100\nlearning_rate = 0.001\n\n# MNIST Dataset \ntrain_dataset = dsets.MNIST(root='../data', \n                            train=True, \n                            transform=transforms.ToTensor(),  \n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# Neural Network Model (1 hidden layer)\nclass Net(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size) \n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)  \n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n    \nnet = Net(input_size, hidden_size, num_classes)\n\n    \n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()  \noptimizer = AMSGrad(net.parameters(), lr=learning_rate, weight_decay=0.01)  \n\n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        # Convert torch tensor to Variable\n        images = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()  # zero the gradient buffer\n        outputs = net(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n\n# Test the Model\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, 28*28))\n    outputs = net(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Accuracy of the network on the 10000 test images: %.2f %%' % (100. * correct / total))\n"""
amsgrad/optim.py,1,"b'import math\nfrom torch.optim.optimizer import Optimizer\n\n\nclass AMSGrad(Optimizer):\n    """"""Implements AMSGrad algorithm.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n\n    """"""\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay)\n        super(AMSGrad, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = grad.new().resize_as_(grad).zero_()\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = grad.new().resize_as_(grad).zero_()\n                    # Exponential moving average of squared gradient values hat\n                    state[\'exp_avg_sq_hat\'] = grad.new().resize_as_(grad).zero_()\n\n                exp_avg, exp_avg_sq, exp_avg_sq_hat = state[\'exp_avg\'], state[\'exp_avg_sq\'], state[\'exp_avg_sq_hat\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                tmp = exp_avg_sq_hat.max(exp_avg_sq)\n                exp_avg_sq_hat.copy_(tmp)\n\n                denom = exp_avg_sq_hat.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1 ** state[\'step\']\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n                step_size = group[\'lr\'] * math.sqrt(bias_correction2) / bias_correction1\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n                if group[\'weight_decay\'] != 0:\n                    p.data.add_(-group[\'weight_decay\'] * group[\'lr\'], p.data)\n\n        return loss\n'"
basic/custom_function.py,6,"b""from __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom functions import relu\n\n\n# Hyper Parameters \ninput_size = 784\nhidden_size = 500\nnum_classes = 10\nnum_epochs = 5\nbatch_size = 100\nlearning_rate = 0.001\n\n# MNIST Dataset \ntrain_dataset = dsets.MNIST(root='../data', \n                            train=True, \n                            transform=transforms.ToTensor(),  \n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# Neural Network Model (1 hidden layer)\nclass Net(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size) \n        self.fc2 = nn.Linear(hidden_size, num_classes)  \n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = relu(out)\n        out = self.fc2(out)\n        return out\n    \nnet = Net(input_size, hidden_size, num_classes)\n\n    \n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()  \noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n\n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        # Convert torch tensor to Variable\n        images = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()  # zero the gradient buffer\n        outputs = net(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n\n# Test the Model\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, 28*28))\n    outputs = net(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"""
basic/custom_module.py,6,"b""import torch\nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom modules import *\n\n\n# Hyper Parameters \ninput_size = 784\nhidden_size = 500\nnum_classes = 10\nnum_epochs = 5\nbatch_size = 100\nlearning_rate = 0.001\n\n# MNIST Dataset \ntrain_dataset = dsets.MNIST(root='../data', \n                            train=True, \n                            transform=transforms.ToTensor(),  \n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# Neural Network Model (1 hidden layer)\nclass Net(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(Net, self).__init__()\n        self.fc1 = Linear(input_size, hidden_size) \n        self.relu = ReLU()\n        self.fc2 = Linear(hidden_size, num_classes)  \n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n    \nnet = Net(input_size, hidden_size, num_classes)\n\n    \n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()  \noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n\n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        # Convert torch tensor to Variable\n        images = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()  # zero the gradient buffer\n        outputs = net(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n\n# Test the Model\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, 28*28))\n    outputs = net(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"""
basic/functions.py,6,"b'from __future__ import print_function\n\nimport math\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Function\n\n\nclass ReLUF(Function):\n\n    @staticmethod\n    def forward(cxt, input):\n        cxt.save_for_backward(input)\n\n        output = input.clamp(min=0)\n\n        return output\n\n    @staticmethod\n    def backward(cxt, grad_output):\n        input, = cxt.saved_tensors\n\n        grad_input = grad_output.clone()\n        grad_input[input < 0] = 0\n\n        return grad_input\n\n\nclass LinearF(Function):\n\n    @staticmethod\n    def forward(cxt, input, weight, bias=None):\n        cxt.save_for_backward(input, weight, bias)\n\n        output = input.mm(weight.t())\n        if bias is not None:\n            output += bias\n\n        return output\n\n    @staticmethod\n    def backward(cxt, grad_output):\n        input, weight, bias = cxt.saved_variables\n\n        grad_input = grad_weight = grad_bias = None\n        if cxt.needs_input_grad[0]:\n            grad_input = grad_output.mm(weight)\n        if cxt.needs_input_grad[1]:\n            grad_weight = grad_output.t().mm(input)\n        if bias is not None and cxt.needs_input_grad[2]:\n            grad_bias = grad_output.sum(0).squeeze(0)\n\n        if bias is not None:\n            return grad_input, grad_weight, grad_bias\n        else:\n            return grad_input, grad_weight\n\n\n# aliases\nrelu = ReLUF.apply\nlinear = LinearF.apply\n\n\n# simple test\nif __name__ == ""__main__"":\n    from torch.autograd import Variable\n\n    torch.manual_seed(1111)\n    a = torch.randn(2, 3)\n\n    va = Variable(a, requires_grad=True)\n    vb = relu(va)\n    print(va.data, vb.data)\n\n    vb.backward(torch.ones(va.size()))\n    print(va.grad.data)\n'"
basic/modules.py,4,"b'import math\n\nimport torch\nimport torch.nn as nn\n\nfrom functions import *\n\n\nclass ReLU(nn.Module):\n    def forward(self, input):\n        return relu(input)\n\n\nclass Linear(nn.Module):\n    r""""""Applies a linear transformation to the incoming data: :math:`y = Ax + b`\n\n    Args:\n        in_features: size of each input sample\n        out_features: size of each output sample\n        bias: If set to False, the layer will not learn an additive bias. Default: True\n\n    Shape:\n        - Input: :math:`(N, in\\_features)`\n        - Output: :math:`(N, out\\_features)`\n\n    Attributes:\n        weight: the learnable weights of the module of shape (out_features x in_features)\n        bias:   the learnable bias of the module of shape (out_features)\n\n    Examples::\n\n        >>> m = Linear(20, 30)\n        >>> input = autograd.Variable(torch.randn(128, 20))\n        >>> output = m(input)\n        >>> print(output.size())\n    """"""\n\n    def __init__(self, in_features, out_features, bias=True):\n        super(Linear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(out_features))\n        else:\n            self.register_parameter(\'bias\', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        """"""Glorot Initialization\n        """"""\n        stdv = math.sqrt(2. / (sum(self.weight.size())))\n        self.weight.data.normal_(0, stdv)\n        if self.bias is not None:\n            self.bias.data.zero_()\n\n    def forward(self, input):\n        return linear(input, self.weight, self.bias)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \' (\' \\\n            + str(self.in_features) + \' -> \' \\\n            + str(self.out_features) + \')\'\n'"
basic/reference.py,6,"b""import torch\nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\n\n# Hyper Parameters \ninput_size = 784\nhidden_size = 500\nnum_classes = 10\nnum_epochs = 5\nbatch_size = 100\nlearning_rate = 0.001\n\n# MNIST Dataset \ntrain_dataset = dsets.MNIST(root='../data', \n                            train=True, \n                            transform=transforms.ToTensor(),  \n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# Neural Network Model (1 hidden layer)\nclass Net(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size) \n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)  \n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n    \nnet = Net(input_size, hidden_size, num_classes)\n\n    \n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()  \noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n\n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        # Convert torch tensor to Variable\n        images = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()  # zero the gradient buffer\n        outputs = net(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n\n# Test the Model\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, 28*28))\n    outputs = net(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"""
binary/adam.py,1,"b'import math\nfrom torch.optim import Optimizer\n\n\nclass Adam(Optimizer):\n    """"""Implements Adam algorithm.\n\n    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n\n    .. _Adam\\: A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    """"""\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay)\n        super(Adam, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = grad.new().resize_as_(grad).zero_()\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = grad.new().resize_as_(grad).zero_()\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1 ** state[\'step\']\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n                lr = group[\'lr\'] * getattr(p, \'lr_scale\', 1)\n                step_size = lr * math.sqrt(bias_correction2) / bias_correction1\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n        return loss\n'"
binary/cnn.py,7,"b""import torch \nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom modules import *\n\n\n# Hyper Parameters\nnum_epochs = 50\nbatch_size = 100\n\nlr_start = 1e-3\nlr_end = 1e-4\nlr_decay = (lr_end / lr_start)**(1. / num_epochs)\n\nmomentum = 0.9\neps = 1e-6\n\n# MNIST Dataset\ntrain_dataset = dsets.MNIST(root='../data/',\n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data/',\n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# CNN Model (2 conv layer)\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            BinaryConv2d(1, 16, kernel_size=5, padding=2),\n            nn.BatchNorm2d(16, momentum=momentum, eps=eps),\n            nn.MaxPool2d(2),\n            BinaryTanh())\n        self.layer2 = nn.Sequential(\n            BinaryConv2d(16, 32, kernel_size=5, padding=2),\n            nn.BatchNorm2d(32, momentum=momentum, eps=eps),\n            nn.MaxPool2d(2),\n            BinaryTanh())\n        self.fc = BinaryLinear(7*7*32, 10)\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        return out\n        \ncnn = CNN()\n\n\n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()\n\n# Optimizer\ndef scale_lr(parameters, lr):\n    param_groups = [] \n    other_params = []\n    for param in parameters:\n        if hasattr(param, 'lr_scale'):\n            g = {'params': [param], 'lr': lr * param.lr_scale}\n            param_groups.append(g)\n        else:\n            other_params.append(param)\n    param_groups.append({'params': other_params})\n    return param_groups\n\noptimizer = torch.optim.Adam(scale_lr(cnn.parameters(), lr_start), lr=lr_start)  \n\n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images)\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs = cnn(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n\n    # Test the Model\n    cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = Variable(images)\n        outputs = cnn(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum()\n\n    print('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))\n\n    cnn.train()\n\n# Save the Trained Model\ntorch.save(cnn.state_dict(), 'cnn.pkl')\n"""
binary/functions.py,2,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Function\n\n\nclass BinarizeF(Function):\n\n    @staticmethod\n    def forward(cxt, input):\n        output = input.new(input.size())\n        output[input >= 0] = 1\n        output[input < 0] = -1\n        return output\n\n    @staticmethod\n    def backward(cxt, grad_output):\n        grad_input = grad_output.clone()\n        return grad_input\n\n# aliases\nbinarize = BinarizeF.apply\n'"
binary/mlp.py,7,"b""import torch\nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom modules import BinaryLinear, BinaryTanh\nfrom adam import Adam\n\n\ntorch.manual_seed(1111)\n\n\n# Hyper Parameters \ninput_size = 784\nhidden_size = 1024 \nnum_layers = 1\nnum_classes = 10\nnum_epochs = 50\nbatch_size = 100\n\nlr_start = 1e-3\nlr_end = 1e-4\nlr_decay = (lr_end / lr_start)**(1. / num_epochs)\n\ndrop_in = 0.2\ndrop_hid = 0.5\n\nmomentum = 0.9\neps = 1e-6\n\n# MNIST Dataset \ntrain_dataset = dsets.MNIST(root='../data', \n                            train=True, \n                            transform=transforms.ToTensor(),  \n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# Neural Network Model\nclass MLP(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes, num_layers=1):\n        super(MLP, self).__init__()\n        self.num_layers = num_layers\n\n        self.p_in = nn.Dropout(p=drop_in)\n        for i in range(1, self.num_layers+1):\n            in_features = input_size if i == 1 else hidden_size\n            out_features = hidden_size\n            layer = nn.Sequential(\n                BinaryLinear(in_features, out_features, bias=False),\n                nn.BatchNorm1d(out_features, momentum=momentum, eps=eps),\n                BinaryTanh(),\n                nn.Dropout(p=drop_hid))\n            setattr(self, 'layer{}'.format(i), layer)\n        self.fc = BinaryLinear(hidden_size, num_classes, bias=False)  \n    \n    def forward(self, x):\n        out = self.p_in(x)\n        for i in range(1, self.num_layers+1):\n            out = getattr(self, 'layer{}'.format(i))(out)\n        out = self.fc(out)\n        return out\n    \nmlp = MLP(input_size, hidden_size, num_classes, num_layers=num_layers)\n\n    \n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()  \noptimizer = Adam(mlp.parameters(), lr=lr_start)  \n\n\ndef clip_weight(parameters):\n    for p in parameters:\n        p = p.data\n        p.clamp_(-1., 1.)\n\n\n# learning rate schedule\ndef adjust_learning_rate(optimizer):\n    for param_group in optimizer.param_groups:\n        lr = param_group['lr']\n        lr = lr * lr_decay\n        param_group['lr'] = lr\n\n\n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        # Convert torch tensor to Variable\n        images = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()  # zero the gradient buffer\n        outputs = mlp(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        clip_weight(mlp.parameters())\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n\n    adjust_learning_rate(optimizer)\n\n    # Test the Model\n    mlp.eval()\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = Variable(images.view(-1, 28*28))\n        outputs = mlp(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum()\n\n    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n\n    mlp.train()\n\n# Save the Trained Model\ntorch.save(mlp.state_dict(), 'mlp.pkl')\n"""
binary/modules.py,2,"b'import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom functions import *\n\n\nclass BinaryTanh(nn.Module):\n    def __init__(self):\n        super(BinaryTanh, self).__init__()\n        self.hardtanh = nn.Hardtanh()\n\n    def forward(self, input):\n        output = self.hardtanh(input)\n        output = binarize(output)\n        return output\n        \n\nclass BinaryLinear(nn.Linear):\n\n    def forward(self, input):\n        binary_weight = binarize(self.weight)\n        if self.bias is None:\n            return F.linear(input, binary_weight)\n        else:\n            return F.linear(input, binary_weight, self.bias)\n\n    def reset_parameters(self):\n        # Glorot initialization\n        in_features, out_features = self.weight.size()\n        stdv = math.sqrt(1.5 / (in_features + out_features))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.zero_()\n\n        self.weight.lr_scale = 1. / stdv\n\n\n\nclass BinaryConv2d(nn.Conv2d):\n\n    def forward(self, input):\n        bw = binarize(self.weight)\n        return F.conv2d(input, bw, self.bias, self.stride,\n                               self.padding, self.dilation, self.groups)\n\n    def reset_parameters(self):\n        # Glorot initialization\n        in_features = self.in_channels\n        out_features = self.out_channels\n        for k in self.kernel_size:\n            in_features *= k\n            out_features *= k\n        stdv = math.sqrt(1.5 / (in_features + out_features))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.zero_()\n\n        self.weight.lr_scale = 1. / stdv\n'"
cffi/build.py,2,"b""import os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n\nsources = ['src/ext_lib.c']\nheaders = ['src/ext_lib.h']\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/ext_lib_cuda.c']\n    headers += ['src/ext_lib_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nffi = create_extension(\n    '_ext.ext_lib',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
cffi/test.py,6,"b""import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom modules.relu import ReLUM\n\n\ntorch.manual_seed(1111)\n\nclass MyNetwork(nn.Module):\n    def __init__(self):\n        super(MyNetwork, self).__init__()\n        self.relu = ReLUM()\n\n    def forward(self, input):\n        return self.relu(input)\n\nmodel = MyNetwork()\nx = torch.randn(1, 25).view(5, 5)\ninput = Variable(x, requires_grad=True)\noutput = model(input)\nprint(output)\nprint(input.clamp(min=0))\n\noutput.backward(torch.ones(input.size()))\nprint(input.grad.data)\n\nif torch.cuda.is_available():\n    print('test cuda')\n    input = input.cuda()\n    output = model(input)\n    print(output)\n    print(input.clamp(min=0))\n"""
dgc/dgc.py,6,"b'import torch\nfrom torch.optim.optimizer import Optimizer, required\n\nimport numpy as np\n\n\ndef kth(arr, topk, sample_rate=1):\n    # to numpy array\n    arr = arr.numpy().ravel()\n\n    if sample_rate < 1:\n        arr = np.random.choice(arr, int(arr.size * sample_rate), replace=False)\n\n    arr = np.abs(arr)\n    num = arr.size\n\n    k = max(1, topk * num // 100)\n    ids = np.argpartition(arr, -k)[-k:]\n    thr = float(np.min(arr[ids]))\n\n    return thr\n\n\nclass DGC(Optimizer):\n    r""""""Implement Deep Gradient Compression for momentum SGD.\n\n    Args:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float): learning rate\n        momentum (float, optional): momentum factor (default: 0)\n        topk: keep topk percent gradients\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        nesterov (bool, optional): enables Nesterov momentum (default: False)\n        max_val (float, optinal): clip graidient if abs is greater than max_val\n\n    Example:\n        >>> optimizer = DGC(model.parameters(), lr=0.1, momentum=0.9)\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> optimizer.step()\n    """"""\n\n    def __init__(self, params, lr=required, momentum=0, topk=1,\n                 weight_decay=0, nesterov=False, \n                 max_val=None, sample_rate=0.1):\n        defaults = dict(lr=lr, momentum=momentum, topk=topk,\n                        weight_decay=weight_decay, nesterov=nesterov,\n                        max_val=max_val, sample_rate=sample_rate)\n        if nesterov and momentum <= 0:\n            raise ValueError(""Nesterov momentum requires a momentum"")\n        super(DGC, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(DGC, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'nesterov\', False)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            weight_decay = group[\'weight_decay\']\n            momentum = group[\'momentum\']\n            nesterov = group[\'nesterov\']\n            topk = group[\'topk\']\n            max_val = group[\'max_val\']\n            sample_rate = group[\'sample_rate\']\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n\n                d_p = p.grad.data\n\n                if weight_decay != 0:\n                    d_p.add_(weight_decay, p.data)\n\n                # clip gradient\n                if max_val is not None and max_val > 0:\n                    d_p.clamp_(-max_val, max_val)\n\n                if momentum != 0:\n                    param_state = self.state[p]\n\n                    if \'u_buffer\' not in param_state:\n                        param_state[\'u_buffer\'] = d_p.clone()\n                    u = param_state[\'u_buffer\']\n\n                    if \'v_buffer\' not in param_state:\n                        param_state[\'v_buffer\'] = d_p.clone()\n                    v = param_state[\'v_buffer\']\n\n                    if nesterov:\n                        u.add_(d_p).mul_(momentum)\n                        v.add_(u + d_p)\n                    else:\n                        u.mul_(momentum).add_(d_p)\n                        v.add_(u)\n\n                    # threshold\n                    thr = kth(v, topk, sample_rate=sample_rate)\n\n                    mask = (v.abs() >= thr).type(d_p.type())\n                    nmask = (v.abs() < thr).type(d_p.type())\n\n                    torch.mul(v, mask, out=d_p)\n\n                    torch.mul(v, nmask, out=v)\n                    torch.mul(u, nmask, out=u)\n                else:  # SGD\n                    param_state = self.state[p]\n                    if \'g_buffer\' not in param_state:\n                        param_state[\'g_buffer\'] = d_p.clone()\n                    g = param_state[\'g_buffer\']\n                    g.add_(d_p)\n\n                    # threshold\n                    thr = kth(g, topk, sample_rate=sample_rate)\n\n                    mask = (g.abs() >= thr).type(d_p.type())\n                    nmask = (g.abs() < thr).type(d_p.type())\n\n                    torch.mul(g, mask, out=d_p)\n                    torch.mul(g, nmask, out=g)\n\n                p.data.add_(-group[\'lr\'], d_p)\n\n        return loss\n'"
dgc/mlp.py,6,"b""import torch\nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom dgc import DGC\n\n\n# Hyper Parameters \ninput_size = 784\nhidden_size = 500\nnum_classes = 10\nnum_epochs = 5\nbatch_size = 100\nlearning_rate = 0.1\nmomentum = 0.9\nnesterov = True\ntopk = 1  # percent\nmax_val = 10\nsample_rate = 0.1\n\n# MNIST Dataset \ntrain_dataset = dsets.MNIST(root='../data', \n                            train=True, \n                            transform=transforms.ToTensor(),  \n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# Neural Network Model (1 hidden layer)\nclass Net(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size) \n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)  \n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n    \nnet = Net(input_size, hidden_size, num_classes)\n\n    \n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()  \noptimizer = DGC(net.parameters(), lr=learning_rate,\n                momentum=momentum, nesterov=nesterov,\n                topk=topk, max_val=max_val, \n                sample_rate=sample_rate) \n#optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate,\n#                            momentum=momentum, nesterov=nesterov)  \n\n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        # Convert torch tensor to Variable\n        images = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()  # zero the gradient buffer\n        outputs = net(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n\n# Test the Model\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, 28*28))\n    outputs = net(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"""
dni/mlp.py,9,"b""import torch\nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\n\n# Hyper Parameters\ninput_size = 784\nhidden_size = 256\ndni_size = 1024\nnum_classes = 10\nnum_epochs = 50\nbatch_size = 500\nlearning_rate = 1e-3\n\nuse_cuda = torch.cuda.is_available()\n\n# MNIST Dataset\ntrain_dataset = dsets.MNIST(root='../data',\n                            train=True,\n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data',\n                           train=False,\n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size,\n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=False)\n\n\nclass DNI(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(DNI, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.bn1 = nn.BatchNorm1d(hidden_size)\n        self.act1 = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.bn1(out)\n        out = self.act1(out)\n        out = self.fc2(out)\n        return out\n\n    def reset_parameters(self):\n        super(DNI, self).reset_parameters()\n        for param in self.fc2.parameters():\n            param.data.zero_()\n\n\ndni = DNI(hidden_size, dni_size)\n\n\nclass Net1(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(Net1, self).__init__()\n        self.mlp = nn.Sequential(nn.Linear(input_size, hidden_size),\n                                 nn.BatchNorm1d(hidden_size),\n                                 nn.ReLU())\n\n    def forward(self, x):\n        return self.mlp.forward(x)\n\n\nnet1 = Net1(input_size, hidden_size)\n\n\nclass Net2(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(Net2, self).__init__()\n        self.mlp = nn.Sequential()\n        self.mlp.add_module('fc1', nn.Linear(input_size, hidden_size))\n        self.mlp.add_module('bn1', nn.BatchNorm1d(hidden_size))\n        self.mlp.add_module('act1', nn.ReLU())\n\n        self.mlp.add_module('fc', nn.Linear(hidden_size, num_classes))\n\n    def forward(self, x):\n        return self.mlp.forward(x)\n\n\nnet2 = Net2(hidden_size, hidden_size, num_classes)\n\n\n# Loss\nxent = nn.CrossEntropyLoss()\nmse = nn.MSELoss()\n\n# Optimizers\nopt_net1 = torch.optim.Adam(net1.parameters(), lr=learning_rate)\nopt_net2 = torch.optim.Adam(net2.parameters(), lr=learning_rate)\nopt_dni = torch.optim.Adam(dni.parameters(), lr=learning_rate)\n\nif use_cuda:\n    net1.cuda()\n    net2.cuda()\n    dni.cuda()\n\n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Convert torch tensor to Variable\n        if use_cuda:\n            images = images.cuda()\n            labels = labels.cuda()\n        images = Variable(images.view(-1, 28 * 28))\n        labels = Variable(labels)\n\n        # Forward + Backward + Optimize\n        opt_net1.zero_grad()  # zero the gradient buffer\n        opt_net2.zero_grad()  # zero the gradient buffer\n        opt_dni.zero_grad()  # zero the gradient buffer\n\n        # Forward, Stage1\n        h = net1(images)\n        h1 = Variable(h.data, requires_grad=True)\n        h2 = Variable(h.data, requires_grad=False)\n\n        # Forward, Stage2\n        outputs = net2(h1)\n\n        # Backward\n        loss = xent(outputs, labels)\n        loss.backward()\n\n        # Synthetic gradient and backward\n        grad = dni(h2)\n        h.backward(grad)\n\n        # regress\n        regress_loss = mse(grad, Variable(h1.grad.data))\n        regress_loss.backward()\n\n        # optimize\n        opt_net1.step()\n        opt_net2.step()\n        opt_dni.step()\n\n        if (i + 1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n                   % (epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.data[0]))\n\n# Test the Model\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    if use_cuda:\n        images = images.cuda()\n        labels = labels.cuda()\n    images = Variable(images.view(-1, 28 * 28))\n    outputs = net2(net1(images))\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' %\n      (100 * correct / total))\n"""
focalloss/loss.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\ndef one_hot(index, classes):\n    size = index.size() + (classes,)\n    view = index.size() + (1,)\n\n    mask = torch.Tensor(*size).fill_(0)\n    index = index.view(*view)\n    ones = 1.\n\n    if isinstance(index, Variable):\n        ones = Variable(torch.Tensor(index.size()).fill_(1))\n        mask = Variable(mask, volatile=index.volatile)\n\n    return mask.scatter_(1, index, ones)\n\n\nclass FocalLoss(nn.Module):\n\n    def __init__(self, gamma=0, eps=1e-7):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.eps = eps\n\n    def forward(self, input, target):\n        y = one_hot(target, input.size(-1))\n        logit = F.softmax(input, dim=-1)\n        logit = logit.clamp(self.eps, 1. - self.eps)\n\n        loss = -1 * y * torch.log(logit) # cross entropy\n        loss = loss * (1 - logit) ** self.gamma # focal loss\n\n        return loss.sum()\n'"
focalloss/mnist_mlp.py,7,"b""import torch\nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom loss import FocalLoss\n\n\ntorch.manual_seed(1111)\n\n# Hyper Parameters \ninput_size = 784\nhidden_size = 500\nnum_classes = 10\nnum_epochs = 5\nbatch_size = 100\nlearning_rate = 0.001\n\ngamma = 2\n\n# MNIST Dataset \ntrain_dataset = dsets.MNIST(root='../data', \n                            train=True, \n                            transform=transforms.ToTensor(),  \n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# Neural Network Model (1 hidden layer)\nclass Net(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size) \n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)  \n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n    \nnet = Net(input_size, hidden_size, num_classes)\n\n    \n# Loss and Optimizer\ncriterion = FocalLoss(gamma) #nn.CrossEntropyLoss()  \noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n\n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        # Convert torch tensor to Variable\n        images = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()  # zero the gradient buffer\n        outputs = net(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n\n# Test the Model\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, 28*28))\n    outputs = net(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"""
meprop/cnn.py,8,"b""import torch \nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom modules import *\n\ntorch.manual_seed(1111)\n\n# Hyper Parameters\nnum_epochs = 50\nbatch_size = 100\n\nk = 20\nsimplified = False\n\nlr_start = 1e-3\nlr_end = 1e-4\nlr_decay = (lr_end / lr_start)**(1. / num_epochs)\n\nmomentum = 0.9\neps = 1e-6\n\n# MNIST Dataset\ntrain_dataset = dsets.MNIST(root='../data/',\n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data/',\n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# CNN Model (2 conv layer)\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            meConv2d(1, 16, kernel_size=5, padding=2, k=k, simplified=simplified),\n            nn.BatchNorm2d(16, momentum=momentum, eps=eps),\n            nn.MaxPool2d(2),\n            nn.ReLU())\n        self.layer2 = nn.Sequential(\n            meConv2d(16, 32, kernel_size=5, padding=2, k=k, simplified=simplified),\n            nn.BatchNorm2d(32, momentum=momentum, eps=eps),\n            nn.MaxPool2d(2),\n            nn.ReLU())\n        self.fc = nn.Linear(7*7*32, 10)\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        return out\n        \ncnn = CNN()\n\n\n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()\n\n# Optimizer\ndef scale_lr(parameters, lr):\n    param_groups = [] \n    other_params = []\n    for param in parameters:\n        if hasattr(param, 'lr_scale'):\n            g = {'params': [param], 'lr': lr * param.lr_scale}\n            param_groups.append(g)\n        else:\n            other_params.append(param)\n    param_groups.append({'params': other_params})\n    return param_groups\n\noptimizer = torch.optim.Adam(scale_lr(cnn.parameters(), lr_start), lr=lr_start)  \n\n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images)\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs = cnn(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n\n    # Test the Model\n    cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = Variable(images)\n        outputs = cnn(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum()\n\n    print('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))\n\n    cnn.train()\n\n# Save the Trained Model\ntorch.save(cnn.state_dict(), 'cnn.pkl')\n"""
meprop/lstm.py,9,"b""import torch \nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom modules import meLSTM\n\ntorch.manual_seed(1111)\n\n# Hyper Parameters\nsequence_length = 28\ninput_size = 28\nhidden_size = 128\nnum_layers = 2\nnum_classes = 10\nbatch_size = 100\nnum_epochs = 2\nlearning_rate = 0.01\n\nk = 100 \nsimplified = False\n\n# MNIST Dataset\ntrain_dataset = dsets.MNIST(root='../data/',\n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data/',\n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# RNN Model (Many-to-One)\nclass RNNModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes, \n                 bias=True, grad_clip=None, k=1, simplified=False):\n        super(RNNModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.rnn = meLSTM(input_size, hidden_size, num_layers=num_layers, \n                        bias=bias, return_sequences=False, grad_clip=None,\n                        k=k, simplified=simplified)\n        self.fc = nn.Linear(hidden_size, num_classes, bias=bias)\n    \n    def forward(self, x):\n        # Set initial states \n        zeros = Variable(torch.zeros(x.size(0), self.hidden_size))\n        initial_states = [(zeros, zeros)] * self.num_layers\n        \n        # Forward propagate RNN\n        out, _ = self.rnn(x, initial_states)  \n        \n        # Decode hidden state of last time step\n        out = self.fc(out)  \n        return out\n\nrnn = RNNModel(input_size, hidden_size, num_layers, num_classes, \n               bias=True, grad_clip=1, k=k, simplified=simplified)\n\n\n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n    \n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images.view(-1, sequence_length, input_size))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs = rnn(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n\n# Test the Model\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, sequence_length, input_size))\n    outputs = rnn(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total)) \n\n# Save the Model\ntorch.save(rnn.state_dict(), 'rnn.pkl')\n"""
meprop/mlp.py,8,"b""import torch\nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom modules import meLinear\n\ntorch.manual_seed(1111)\n\n# Hyper Parameters \ninput_size = 784\nhidden_size = 512 \nnum_layers = 2\nnum_classes = 10\nnum_epochs = 50\nbatch_size = 100\n\nk = 100\nsimplified = False\nlr = 1e-3\n\ndrop_in = 0.\ndrop_hid = 0.\n\nmomentum = 0.9\neps = 1e-6\n\n# MNIST Dataset \ntrain_dataset = dsets.MNIST(root='../data', \n                            train=True, \n                            transform=transforms.ToTensor(),  \n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# Neural Network Model\nclass MLP(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes, num_layers=1):\n        super(MLP, self).__init__()\n        self.num_layers = num_layers\n\n        self.p_in = nn.Dropout(p=drop_in)\n        for i in range(1, self.num_layers+1):\n            in_features = input_size if i == 1 else hidden_size\n            out_features = hidden_size\n            layer = nn.Sequential(\n                meLinear(in_features, out_features, bias=False, k=k, simplified=simplified),\n                nn.BatchNorm1d(out_features, momentum=momentum, eps=eps),\n                nn.ReLU(),\n                nn.Dropout(p=drop_hid))\n            setattr(self, 'layer{}'.format(i), layer)\n        self.fc = nn.Linear(hidden_size, num_classes, bias=False)  \n    \n    def forward(self, x):\n        out = self.p_in(x)\n        for i in range(1, self.num_layers+1):\n            out = getattr(self, 'layer{}'.format(i))(out)\n        out = self.fc(out)\n        return out\n    \nmlp = MLP(input_size, hidden_size, num_classes, num_layers=num_layers)\n\n    \n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()  \noptimizer = torch.optim.Adam(mlp.parameters(), lr=lr)  \n\n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        # Convert torch tensor to Variable\n        images = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()  # zero the gradient buffer\n        outputs = mlp(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n\n    # Test the Model\n    mlp.eval()\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = Variable(images.view(-1, 28*28))\n        outputs = mlp(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum()\n\n    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n\n    mlp.train()\n\n# Save the Trained Model\ntorch.save(mlp.state_dict(), 'mlp.pkl')\n"""
meprop/modules.py,9,"b""import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef sparsify_grad(v, k, simplified=True):\n    if simplified:\n        v.register_hook(lambda g: simplified_topk(g, k))\n    else:\n        v.register_hook(lambda g: topk(g, k))\n    return v\n\n\ndef simplified_topk(x, k):\n    ''' Proof-of-concept implementation of simplified topk\n    Note all we neend the k-th largest vaule, thus an algorithm of log(n) complexity exists.\n    '''\n    original_size = None\n    if x.dim() > 2:\n        original_size = x.size()\n        x = x.view(x.size(0), -1)\n    ax = x.data.abs().sum(0).view(-1)\n    topk, ids = ax.topk(x.size(-1)-k, dim=0, largest=False)\n    y = x.clone()\n    # zero out small values\n    for id in ids:\n        y[:, id] = 0\n\n    if original_size:\n        y = y.view(original_size)\n    return y\n\n\ndef topk(x, k):\n    ''' Proof-of-concept implementation of topk.\n    '''\n    original_size = None\n    if x.dim() > 2:\n        original_size = x.size()\n        x = x.view(x.size(0), -1)\n    ax = torch.abs(x.data)\n    topk, _ = ax.topk(k)\n    topk = topk[:, -1]\n    y = x.clone()\n    # zero out small values\n    y[ax < topk.repeat(x.size(-1), 1).transpose(0, 1)] = 0\n\n    if original_size:\n        y = y.view(original_size)\n    return y\n\n\nclass meLinear(nn.Linear):\n\n    def __init__(self, in_features, out_features, bias=False, k=1, simplified=True):\n        super(meLinear, self).__init__(in_features, out_features, bias)\n        self.k = k\n        self.simplified = simplified\n\n    def forward(self, input):\n        output = F.linear(input, self.weight, self.bias)\n        return sparsify_grad(output, self.k, self.simplified)\n\n    def reset_parameters(self):\n        # Glorot initialization\n        in_features, out_features = self.weight.size()\n        stdv = math.sqrt(1.5 / (in_features + out_features))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.zero_()\n\n\nclass meConv2d(nn.Conv2d):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, \n                 dilation=1, groups=1, bias=True, k=1, simplified=True):\n        super(meConv2d, self).__init__(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, \n                           stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        self.k = k\n        self.simplified = simplified\n\n    def forward(self, input):\n        output = F.conv2d(input, self.weight, self.bias, self.stride,\n                               self.padding, self.dilation, self.groups)\n        return sparsify_grad(output, self.k, self.simplified)\n\n    def reset_parameters(self):\n        # Glorot initialization\n        in_features = self.in_channels\n        out_features = self.out_channels\n        for k in self.kernel_size:\n            in_features *= k\n            out_features *= k\n        stdv = math.sqrt(1.5 / (in_features + out_features))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.zero_()\n\n        self.weight.lr_scale = 1. / stdv\n\n\ndef clip_grad(v, min, max):\n    v.register_hook(lambda g: g.clamp(min, max))\n    return v\n\n\nclass meLSTMCell(nn.Module):\n\n    def __init__(self, input_size, hidden_size, bias=True, grad_clip=None, k=1, simplified=False):\n        super(meLSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.grad_clip = grad_clip\n\n        self.k = k\n        self.simplified = simplified\n\n        self.weight_ih = nn.Parameter(torch.Tensor(4 * hidden_size, input_size))\n        self.weight_hh = nn.Parameter(torch.Tensor(4 * hidden_size, hidden_size))\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(4 * hidden_size))\n        else:\n            self.register_parameter('bias', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.hidden_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, hx):\n        h, c = hx\n\n        pre = F.linear(input, self.weight_ih, self.bias) \\\n                    + F.linear(h, self.weight_hh)\n\n        pre = sparsify_grad(pre, self.k, self.simplified)\n\n        if self.grad_clip:\n            pre = clip_grad(pre, -self.grad_clip, self.grad_clip)\n\n        i = F.sigmoid(pre[:, :self.hidden_size])\n        f = F.sigmoid(pre[:, self.hidden_size: self.hidden_size * 2])\n        g = F.tanh(pre[:, self.hidden_size * 2: self.hidden_size * 3])\n        o = F.sigmoid(pre[:, self.hidden_size * 3:])\n\n        c = f * c + i * g\n        h = o * F.tanh(c)\n        return h, c\n\n\nclass meLSTM(nn.Module):\n\n    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, \n                 return_sequences=True, grad_clip=None, k=1, simplified=False):\n        super(meLSTM, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.return_sequences = return_sequences\n        self.grad_clip = grad_clip\n\n        self.k = k\n        self.simplified = simplified\n\n        kwargs = {'input_size': input_size,\n                  'hidden_size': hidden_size,\n                  'bias': bias,\n                  'grad_clip': grad_clip,\n                  'k': k,\n                  'simplified': simplified}\n\n        self.cell0= meLSTMCell(**kwargs)\n        for i in range(1, num_layers):\n            kwargs['input_size'] = hidden_size\n            cell = meLSTMCell(**kwargs)\n            setattr(self, 'cell{}'.format(i), cell)\n\n    def forward(self, input, initial_states=None):\n        if initial_states is None:\n            zeros = Variable(torch.zeros(input.size(0), self.hidden_size))\n            initial_states = [(zeros, zeros), ] * self.num_layers\n        assert len(initial_states) == self.num_layers\n\n        states = initial_states\n        outputs = []\n\n        time_steps = input.size(1)\n        for t in range(time_steps):\n            x = input[:, t, :]\n            for l in range(self.num_layers):\n                hx = getattr(self, 'cell{}'.format(l))(x, states[l])\n                states[l] = hx\n                x = hx[0]\n            outputs.append(hx)\n\n        if self.return_sequences:\n            hs, cs = zip(*outputs)\n            h = torch.stack(hs).transpose(0, 1)\n            c = torch.stack(cs).transpose(0, 1)\n            output = (h, c)\n        else:\n            output = outputs[-1]\n        return output\n"""
nvrtc/mlp.py,15,"b""import torch\nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom relu import ReLU\n\n\n# Hyper Parameters \ninput_size = 784\nhidden_size = 500\nnum_classes = 10\nnum_epochs = 5\nbatch_size = 100\nlearning_rate = 0.001\n\n# MNIST Dataset \ntrain_dataset = dsets.MNIST(root='../data', \n                            train=True, \n                            transform=transforms.ToTensor(),  \n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# Neural Network Model (1 hidden layer)\nclass Net(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size) \n        self.relu = ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)  \n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out, use_cuda=torch.cuda.is_available())\n        out = self.fc2(out)\n        return out\n    \nnet = Net(input_size, hidden_size, num_classes)\nif torch.cuda.is_available():\n    net.cuda()\n\n    \n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()  \noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n\n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        if torch.cuda.is_available():\n            images = images.type(torch.cuda.FloatTensor)\n            labels = labels.type(torch.cuda.LongTensor)\n\n        # Convert torch tensor to Variable\n        images = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()  # zero the gradient buffer\n        outputs = net(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.type(torch.FloatTensor).data[0]))\n\n# Test the Model\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    if torch.cuda.is_available():\n        images = images.type(torch.cuda.FloatTensor)\n        labels = labels.type(torch.cuda.LongTensor)\n\n    images = Variable(images.view(-1, 28*28))\n    outputs = net(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Accuracy of the network on the 10000 test images: %.2f %%' % (100. * correct / total))\n"""
nvrtc/relu.py,11,"b'from __future__ import print_function\n\nimport math\nimport torch\nfrom torch.autograd import Variable\nfrom collections import namedtuple\n\nfrom cupy.cuda import function\nfrom pynvrtc.compiler import Program\n\n###\n\nkernel = \'\'\'\nextern ""C""\n__global__ void relu_forward(float *output, const float *input, int num)\n{\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for (; tid < num; tid += stride) {\n     output[tid] = input[tid] >= 0 ? input[tid] : 0;\n  }\n}\n\nextern ""C""\n__global__ void relu_backward(float *input_grad, const float *output_grad, const float *input, int num)\n{\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for (; tid < num; tid += stride) {\n     input_grad[tid] = input[tid] >= 0 ? output_grad[tid] : 0;\n  }\n}\n\'\'\'\n\n###\n\nclass GPUReLUF(torch.autograd.Function):\n    configured_gpus = {}\n    ptx = None\n\n    def compile(self):\n        if self.ptx is None:\n            program = Program(kernel, \'relu.cu\')\n            GPUReLUF.ptx = program.compile()\n\n        if torch.cuda.current_device() not in GPUReLUF.configured_gpus:\n            m = function.Module()\n            m.load(bytes(self.ptx))\n\n            self.relu_forward = m.get_function(\'relu_forward\')\n            self.relu_backward = m.get_function(\'relu_backward\')\n\n            Stream = namedtuple(\'Stream\', [\'ptr\'])\n            self.stream = Stream(ptr=torch.cuda.current_stream().cuda_stream)\n\n            GPUReLUF.configured_gpus[torch.cuda.current_device()] = (self.relu_forward, self.relu_backward, self.stream)\n\n        self.relu_forward, self.relu_backward, self.stream = GPUReLUF.configured_gpus[torch.cuda.current_device()]\n\n    def forward(self, x):\n        self.compile()\n\n        self.save_for_backward(x)\n\n        y = x.new(*x.size())\n\n        ###\n        batch_size, hidden_size = x.size()\n\tnum = batch_size * hidden_size\n        grid_hidden_size = min(num, 512)\n        grid = (int(math.ceil(num / grid_hidden_size)), 1)\n        self.relu_forward(grid=grid, block=(grid_hidden_size, 1), args=[y.data_ptr(), x.data_ptr(), num], stream=self.stream)\n\n        return y \n\n    def backward(self, grad_y):\n        self.compile()\n\n        x, = self.saved_tensors\n\n        grad_x = x.new(*x.size())\n\n        ###\n        batch_size, hidden_size = x.size()\n\tnum = batch_size * hidden_size\n        grid_hidden_size = min(num, 512)\n        grid = (int(math.ceil(num/ grid_hidden_size)), 1)\n        self.relu_backward(grid=grid, block=(grid_hidden_size, 1), args=[grad_x.data_ptr(), grad_y.data_ptr(), x.data_ptr(), num], stream=self.stream)\n\n        return grad_x\n\n\nclass ReLU(torch.nn.Module):\n\n    def __init__(self):\n        super(ReLU, self).__init__()\n\n    def forward(self, x, use_cuda=True):\n        # Use CUDA by default unless it\'s available\n        use_cuda = use_cuda and torch.cuda.is_available()\n        # Ensure the user is aware when ForgetMult is not GPU version as it\'s far faster\n        if use_cuda: assert x.is_cuda, \'GPU ReLU with fast element-wise CUDA kernel requested but tensors not on GPU\'\n        ###\n        return GPUReLUF()(x) if use_cuda else torch.nn.functional.relu(x)\n\n\nif __name__ == \'__main__\':\n    batch, hidden = 20, 650\n    x = Variable(torch.rand(batch, hidden).cuda(), requires_grad=True)\n    y = Variable(torch.rand(batch, hidden).cuda(), requires_grad=True)\n\n    print(\'CUDA ReLU\')\n    print(\'=-=-\' * 5)\n\n    ya = ReLU()(x, use_cuda=True)\n    loss = ya.mean()\n    loss.backward()\n\n    print(\'Result =\', loss.data[0])\n    print(\'X grad =\', x.grad.mean().data[0])\n\n    x_grad_copy = x.grad.clone()\n\n    print(\'CPU ReLU\')\n    print(\'=-=-\' * 5)\n\n    x.cpu()\n\n    x.grad.data *= 0\n\n    yb = ReLU()(x, use_cuda=False)\n    print(yb.size())\n    loss = yb.mean()\n    loss.backward()\n\n    print(\'Result =\', loss.data[0])\n    print(\'X grad =\', x.grad.mean().data[0])\n\n    ###\n\n    print()\n    print(\'=-=-\' * 5)\n    print(\'(Xgrad - Xgrad).sum() =\', (x_grad_copy - x.grad).sum().data[0])\n    print(\'Residual error for result\')\n    print(\'=-=-\' * 5)\n    residual = (ya- yb)\n    print(residual.abs().sum().data[0])\n'"
rnn/modules.py,32,"b'import math\n\nimport torch\nfrom torch.nn import Module, Parameter\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\ndef clip_grad(v, min, max):\n    v_tmp = v.expand_as(v)\n    v_tmp.register_hook(lambda g: g.clamp(min, max))\n    return v_tmp\n\n\nclass RNNCellBase(Module):\n\n    def __repr__(self):\n        s = \'{name}({input_size}, {hidden_size}\'\n        if \'bias\' in self.__dict__ and self.bias is not True:\n            s += \', bias={bias}\'\n        if \'nonlinearity\' in self.__dict__ and self.nonlinearity != ""tanh"":\n            s += \', nonlinearity={nonlinearity}\'\n        s += \')\'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass RNNCell(RNNCellBase):\n    def __init__(self, input_size, hidden_size, bias=True, grad_clip=None):\n        super(RNNCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.grad_clip = grad_clip\n\n        self.weight_ih = Parameter(torch.Tensor(hidden_size, input_size))\n        self.weight_hh = Parameter(torch.Tensor(hidden_size, hidden_size))\n        if bias:\n            self.bias = Parameter(torch.Tensor(hidden_size))\n        else:\n            self.register_parameter(\'bias\', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.hidden_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, h):\n        output = F.linear(input, self.weight_ih, self.bias) + F.linear(h, self.weight_hh)\n        if self.grad_clip:\n            output = clip_grad(output, -self.grad_clip, self.grad_clip) # avoid explosive gradient\n        output = F.relu(output)\n\n        return output\n\n\nclass GRUCell(RNNCellBase):\n\n    def __init__(self, input_size, hidden_size, bias=True, grad_clip=None):\n        super(GRUCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.grad_clip = grad_clip\n\n        self.weight_ih = Parameter(torch.Tensor(3 * hidden_size, input_size))\n        self.weight_hh_rz = Parameter(torch.Tensor(2 * hidden_size, hidden_size))\n        self.weight_hh = Parameter(torch.Tensor(hidden_size, hidden_size))\n        if bias:\n            self.bias = Parameter(torch.Tensor(3 * hidden_size))\n        else:\n            self.register_parameter(\'bias\', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.hidden_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, h):\n        ih = F.linear(input, self.weight_ih, self.bias)\n        hh_rz = F.linear(h, self.weight_hh_rz)\n\n        if self.grad_clip:\n            ih = clip_grad(ih, -self.grad_clip, self.grad_clip)\n            hh_rz = clip_grad(hh_rz, -self.grad_clip, self.grad_clip)\n\n        r = F.sigmoid(ih[:, :self.hidden_size] + hh_rz[:, :self.hidden_size])\n        i = F.sigmoid(ih[:, self.hidden_size: self.hidden_size * 2] + hh_rz[:, self.hidden_size:])\n\n        hhr = F.linear(h * r, self.weight_hh)\n        if self.grad_clip:\n            hhr = clip_grad(hhr, -self.grad_clip, self.grad_clip)\n            \n        n = F.relu(ih[:, self.hidden_size * 2:] + hhr)\n        h = (1 - i) * n + i * h\n\n        return h\n\n\nclass LSTMCell(RNNCellBase):\n\n    def __init__(self, input_size, hidden_size, bias=True, grad_clip=None):\n        super(LSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.grad_clip = grad_clip\n\n        self.weight_ih = Parameter(torch.Tensor(4 * hidden_size, input_size))\n        self.weight_hh = Parameter(torch.Tensor(4 * hidden_size, hidden_size))\n        if bias:\n            self.bias = Parameter(torch.Tensor(4 * hidden_size))\n        else:\n            self.register_parameter(\'bias\', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.hidden_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, hx):\n        h, c = hx\n\n        pre = F.linear(input, self.weight_ih, self.bias) \\\n                    + F.linear(h, self.weight_hh)\n\n        if self.grad_clip:\n            pre = clip_grad(pre, -self.grad_clip, self.grad_clip)\n\n        i = F.sigmoid(pre[:, :self.hidden_size])\n        f = F.sigmoid(pre[:, self.hidden_size: self.hidden_size * 2])\n        g = F.tanh(pre[:, self.hidden_size * 2: self.hidden_size * 3])\n        o = F.sigmoid(pre[:, self.hidden_size * 3:])\n\n        c = f * c + i * g\n        h = o * F.tanh(c)\n        return h, c\n\n\ndef cumax(logits, dim=-1):\n    return torch.cumsum(F.softmax(logits, dim), dim=dim)\n\n\nclass LSTMONCell(RNNCellBase):\n    \'\'\'\n    Shen & Tan et al. ORDERED NEURONS: INTEGRATING TREE STRUCTURES INTO RECURRENT NEURAL NETWORKS\n    \'\'\'\n\n    def __init__(self, input_size, hidden_size, bias=True, grad_clip=None):\n        super(LSTMONCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.grad_clip = grad_clip\n\n        self.weight_ih = Parameter(torch.Tensor(6 * hidden_size, input_size))\n        self.weight_hh = Parameter(torch.Tensor(6 * hidden_size, hidden_size))\n        if bias:\n            self.bias = Parameter(torch.Tensor(6 * hidden_size))\n        else:\n            self.register_parameter(\'bias\', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.hidden_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, hx):\n        h, c = hx\n\n        pre = F.linear(input, self.weight_ih, self.bias) \\\n                    + F.linear(h, self.weight_hh)\n\n        if self.grad_clip:\n            pre = clip_grad(pre, -self.grad_clip, self.grad_clip)\n\n        i = F.sigmoid(pre[:, :self.hidden_size])\n        f = F.sigmoid(pre[:, self.hidden_size: self.hidden_size * 2])\n        g = F.tanh(pre[:, self.hidden_size * 2: self.hidden_size * 3])\n        o = F.sigmoid(pre[:, self.hidden_size * 3: self.hidden_size * 4])\n        ff = cumax(pre[:, self.hidden_size * 4: self.hidden_size * 5])\n        ii = 1 - cumax(pre[:, self.hidden_size * 5: self.hidden_size * 6])\n\n        w = ff * ii\n        f = f * w + (ff - w)\n        i = i * w + (ii - w)\n\n        c = f * c + i * g\n        h = o * F.tanh(c)\n        return h, c\n\n\nclass LSTMPCell(RNNCellBase):\n\n    def __init__(self, input_size, hidden_size, recurrent_size, bias=True, grad_clip=None):\n        super(LSTMPCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.recurrent_size = recurrent_size\n        self.grad_clip = grad_clip\n\n        self.weight_ih = Parameter(torch.Tensor(4 * hidden_size, input_size))\n        self.weight_hh = Parameter(torch.Tensor(4 * hidden_size, recurrent_size))\n        self.weight_rec = Parameter(torch.Tensor(recurrent_size, hidden_size))\n        if bias:\n            self.bias = Parameter(torch.Tensor(4 * hidden_size))\n        else:\n            self.register_parameter(\'bias\', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.hidden_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, hx):\n        h, c = hx\n\n        pre = F.linear(input, self.weight_ih, self.bias) \\\n                    + F.linear(h, self.weight_hh)\n\n        if self.grad_clip:\n            pre = clip_grad(pre, -self.grad_clip, self.grad_clip)\n\n        i = F.sigmoid(pre[:, :self.hidden_size])\n        f = F.sigmoid(pre[:, self.hidden_size: self.hidden_size * 2])\n        g = F.tanh(pre[:, self.hidden_size * 2: self.hidden_size * 3])\n        o = F.sigmoid(pre[:, self.hidden_size * 3:])\n\n        c = f * c + i * g\n        h = o * F.tanh(c)\n        h = F.linear(h, self.weight_rec)\n        return h, c\n\n\nclass MGRUCell(RNNCellBase):\n    \'\'\'Minimal GRU\n    Reference:\n    Ravanelli et al. [Improving speech recognition by revising gated recurrent units](https://arxiv.org/abs/1710.00641).\n    \'\'\'\n\n    def __init__(self, input_size, hidden_size, bias=True, grad_clip=None):\n        super(MGRUCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.grad_clip = grad_clip\n\n        self.weight_ih = Parameter(torch.Tensor(2 * hidden_size, input_size))\n        self.weight_hh = Parameter(torch.Tensor(2 * hidden_size, hidden_size))\n        if bias:\n            self.bias = Parameter(torch.Tensor(2 * hidden_size))\n        else:\n            self.register_parameter(\'bias\', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.hidden_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, h):\n        ih = F.linear(input, self.weight_ih, self.bias)\n        hh = F.linear(h, self.weight_hh)\n\n        if self.grad_clip:\n            ih = clip_grad(ih, -self.grad_clip, self.grad_clip)\n            hh = clip_grad(hh, -self.grad_clip, self.grad_clip)\n\n        z = F.sigmoid(ih[:, :self.hidden_size] + hh[:, :self.hidden_size])\n        n = F.relu(ih[:, self.hidden_size:] + hh[:, self.hidden_size:])\n        h = (1 - z) * n + z * h\n\n        return h\n\n\nclass IndRNNCell(RNNCellBase):\n    \'\'\'\n    References:\n    Li et al. [Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN](https://arxiv.org/abs/1803.04831).\n    \'\'\'\n\n    def __init__(self, input_size, hidden_size, bias=True, grad_clip=None):\n        super(IndRNNCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.grad_clip = grad_clip\n\n        self.weight_ih = Parameter(torch.Tensor(hidden_size, input_size))\n        self.weight_hh = Parameter(torch.Tensor(hidden_size))\n        if bias:\n            self.bias = Parameter(torch.Tensor(hidden_size))\n        else:\n            self.register_parameter(\'bias\', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.hidden_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, h):\n        output = F.linear(input, self.weight_ih, self.bias) + h * self.weight_hh\n        if self.grad_clip:\n            output = clip_grad(output, -self.grad_clip, self.grad_clip) # avoid explosive gradient\n        output = F.relu(output)\n\n        return output\n\n\nclass RNNBase(Module):\n\n    def __init__(self, mode, input_size, hidden_size, recurrent_size=None, num_layers=1, bias=True, \n                 return_sequences=True, grad_clip=None):\n        super(RNNBase, self).__init__()\n        self.mode = mode\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.recurrent_size = recurrent_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.return_sequences = return_sequences\n        self.grad_clip = grad_clip\n\n        mode2cell = {\'RNN\': RNNCell,\n                     \'IndRNN\': IndRNNCell,\n                     \'GRU\': GRUCell,\n                     \'MGRU\': GRUCell,\n                     \'LSTM\': LSTMCell,\n                     \'LSTMON\': LSTMONCell,\n                     \'LSTMP\': LSTMPCell}\n        Cell = mode2cell[mode]\n\n        kwargs = {\'input_size\': input_size,\n                  \'hidden_size\': hidden_size,\n                  \'bias\': bias,\n                  \'grad_clip\': grad_clip}\n        if self.mode == \'LSTMP\':\n            kwargs[\'recurrent_size\'] = recurrent_size\n\n        self.cell0= Cell(**kwargs)\n        for i in range(1, num_layers):\n            kwargs[\'input_size\'] = recurrent_size if self.mode == \'LSTMP\' else hidden_size\n            cell = Cell(**kwargs)\n            setattr(self, \'cell{}\'.format(i), cell)\n\n    def forward(self, input, initial_states=None):\n        if initial_states is None:\n            zeros = Variable(torch.zeros(input.size(0), self.hidden_size))\n            if self.mode == \'LSTM\' or self.mode == \'LSTMON\':\n                initial_states = [(zeros, zeros), ] * self.num_layers\n            elif self.mode == \'LSTMP\':\n                zeros_h = Variable(torch.zeros(input.size(0), self.recurrent_size))\n                initial_states = [(zeros_h, zeros), ] * self.num_layers\n            else:\n                initial_states = [zeros] * self.num_layers\n        assert len(initial_states) == self.num_layers\n\n        states = initial_states\n        outputs = []\n\n        time_steps = input.size(1)\n        for t in range(time_steps):\n            x = input[:, t, :]\n            for l in range(self.num_layers):\n                hx = getattr(self, \'cell{}\'.format(l))(x, states[l])\n                states[l] = hx\n                if self.mode.startswith(\'LSTM\'):\n                    x = hx[0]\n                else:\n                    x = hx\n            outputs.append(hx)\n\n        if self.return_sequences:\n            if self.mode.startswith(\'LSTM\'):\n                hs, cs = zip(*outputs)\n                h = torch.stack(hs).transpose(0, 1)\n                c = torch.stack(cs).transpose(0, 1)\n                output = (h, c)\n            else:\n                output = torch.stack(outputs).transpose(0, 1)\n        else:\n            output = outputs[-1]\n        return output\n\n\nclass RNN(RNNBase):\n\n    def __init__(self, *args, **kwargs):\n        super(RNN, self).__init__(\'RNN\', *args, **kwargs)\n\n\nclass GRU(RNNBase):\n\n    def __init__(self, *args, **kwargs):\n        super(GRU, self).__init__(\'GRU\', *args, **kwargs)\n\n\nclass MGRU(RNNBase):\n\n    def __init__(self, *args, **kwargs):\n        super(MGRU, self).__init__(\'MGRU\', *args, **kwargs)\n\n\nclass LSTM(RNNBase):\n\n    def __init__(self, *args, **kwargs):\n        super(LSTM, self).__init__(\'LSTM\', *args, **kwargs)\n\n\nclass LSTMON(RNNBase):\n\n    def __init__(self, *args, **kwargs):\n        super(LSTMON, self).__init__(\'LSTMON\', *args, **kwargs)\n\n\nclass LSTMP(RNNBase):\n\n    def __init__(self, *args, **kwargs):\n        super(LSTMP, self).__init__(\'LSTMP\', *args, **kwargs)\n\n\nclass IndRNN(RNNBase):\n    \'\'\'\n    References:\n    Li et al. [Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN](https://arxiv.org/abs/1803.04831).\n    \'\'\'\n\n    def __init__(self, *args, **kwargs):\n        super(IndRNN, self).__init__(\'IndRNN\', *args, **kwargs)\n'"
rnn/test_gru.py,9,"b""import torch \nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom modules import GRU\n\n\ntorch.manual_seed(1111)\n\n# Hyper Parameters\nsequence_length = 28\ninput_size = 28\nhidden_size = 128\nnum_layers = 2\nnum_classes = 10\nbatch_size = 100\nnum_epochs = 2\nlearning_rate = 0.01\n\n# MNIST Dataset\ntrain_dataset = dsets.MNIST(root='../data/',\n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data/',\n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# RNN Model (Many-to-One)\nclass RNNModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes, bias=True, grad_clip=None):\n        super(RNNModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.rnn = GRU(input_size, hidden_size, num_layers=num_layers, \n                       bias=bias, return_sequences=False, grad_clip=grad_clip)\n        self.fc = nn.Linear(hidden_size, num_classes, bias=bias)\n    \n    def forward(self, x):\n        # Set initial states \n        initial_states = [Variable(torch.zeros(x.size(0), self.hidden_size)) for _ in range(self.num_layers)]\n        \n        # Forward propagate RNN\n        out = self.rnn(x, initial_states)  \n        \n        # Decode hidden state of last time step\n        out = self.fc(out)  \n        return out\n\nrnn = RNNModel(input_size, hidden_size, num_layers, num_classes, bias=True, grad_clip=10)\n\n\n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n    \n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images.view(-1, sequence_length, input_size))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs = rnn(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.item()))\n\n# Test the Model\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, sequence_length, input_size))\n    outputs = rnn(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total)) \n\n# Save the Model\ntorch.save(rnn.state_dict(), 'gru.pkl')\n"""
rnn/test_indrnn.py,9,"b""import torch \nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom modules import IndRNN\n\n\ntorch.manual_seed(1111)\n\n# Hyper Parameters\nsequence_length = 28\ninput_size = 28\nhidden_size = 128\nnum_layers = 2\nnum_classes = 10\nbatch_size = 100\nnum_epochs = 2\nlearning_rate = 0.01\n\n# MNIST Dataset\ntrain_dataset = dsets.MNIST(root='../data/',\n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data/',\n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# RNN Model (Many-to-One)\nclass RNNModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes, \n                 bias=True, grad_clip=None):\n        super(RNNModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.rnn = IndRNN(input_size, hidden_size, num_layers=num_layers, \n                          bias=bias, return_sequences=False, grad_clip=grad_clip)\n        self.fc = nn.Linear(hidden_size, num_classes, bias=bias)\n    \n    def forward(self, x):\n        # Set initial states \n        initial_states = [Variable(torch.zeros(x.size(0), self.hidden_size)) for _ in range(self.num_layers)]\n        \n        # Forward propagate RNN\n        out = self.rnn(x, initial_states)  \n        \n        # Decode hidden state of last time step\n        out = self.fc(out)  \n        return out\n\nrnn = RNNModel(input_size, hidden_size, num_layers, num_classes, bias=True, grad_clip=10)\n\n\n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n    \n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images.view(-1, sequence_length, input_size))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs = rnn(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.item()))\n\n# Test the Model\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, sequence_length, input_size))\n    outputs = rnn(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total)) \n\n# Save the Model\ntorch.save(rnn.state_dict(), 'rnn.pkl')\n"""
rnn/test_lstm.py,9,"b""import torch \nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom modules import LSTM\n\n\ntorch.manual_seed(1111)\n\n# Hyper Parameters\nsequence_length = 28\ninput_size = 28\nhidden_size = 128\nnum_layers = 2\nnum_classes = 10\nbatch_size = 100\nnum_epochs = 2\nlearning_rate = 0.01\n\n# MNIST Dataset\ntrain_dataset = dsets.MNIST(root='../data/',\n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data/',\n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# RNN Model (Many-to-One)\nclass RNNModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes, bias=True, grad_clip=None):\n        super(RNNModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.rnn = LSTM(input_size, hidden_size, num_layers=num_layers, \n                        bias=bias, return_sequences=False, grad_clip=grad_clip)\n        self.fc = nn.Linear(hidden_size, num_classes, bias=bias)\n    \n    def forward(self, x):\n        # Set initial states \n        zeros = Variable(torch.zeros(x.size(0), self.hidden_size))\n        initial_states = [(zeros, zeros)] * self.num_layers\n        \n        # Forward propagate RNN\n        out, _ = self.rnn(x, initial_states)  \n        \n        # Decode hidden state of last time step\n        out = self.fc(out)  \n        return out\n\nrnn = RNNModel(input_size, hidden_size, num_layers, num_classes, bias=True, grad_clip=10)\n\n\n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n    \n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images.view(-1, sequence_length, input_size))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs = rnn(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.item()))\n\n# Test the Model\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, sequence_length, input_size))\n    outputs = rnn(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total)) \n\n# Save the Model\ntorch.save(rnn.state_dict(), 'lstm.pkl')\n"""
rnn/test_lstmon.py,9,"b""import torch \nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom modules import LSTMON\n\n\ntorch.manual_seed(1111)\n\n# Hyper Parameters\nsequence_length = 28\ninput_size = 28\nhidden_size = 128\nnum_layers = 2\nnum_classes = 10\nbatch_size = 100\nnum_epochs = 2\nlearning_rate = 0.01\n\n# MNIST Dataset\ntrain_dataset = dsets.MNIST(root='../data/',\n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data/',\n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# RNN Model (Many-to-One)\nclass RNNModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes, bias=True, grad_clip=None):\n        super(RNNModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.rnn = LSTMON(input_size, hidden_size, num_layers=num_layers, \n                        bias=bias, return_sequences=False, grad_clip=grad_clip)\n        self.fc = nn.Linear(hidden_size, num_classes, bias=bias)\n    \n    def forward(self, x):\n        # Set initial states \n        zeros = Variable(torch.zeros(x.size(0), self.hidden_size))\n        initial_states = [(zeros, zeros)] * self.num_layers\n        \n        # Forward propagate RNN\n        out, _ = self.rnn(x, initial_states)  \n        \n        # Decode hidden state of last time step\n        out = self.fc(out)  \n        return out\n\nrnn = RNNModel(input_size, hidden_size, num_layers, num_classes, bias=True, grad_clip=10)\n\n\n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n    \n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images.view(-1, sequence_length, input_size))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs = rnn(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.item()))\n\n# Test the Model\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, sequence_length, input_size))\n    outputs = rnn(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total)) \n\n# Save the Model\ntorch.save(rnn.state_dict(), 'lstm.pkl')\n"""
rnn/test_lstmp.py,10,"b""import torch \nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom modules import LSTMP\n\n\ntorch.manual_seed(1111)\n\n# Hyper Parameters\nsequence_length = 28\ninput_size = 28\nhidden_size = 128\nrecurrent_size = 64\nnum_layers = 2\nnum_classes = 10\nbatch_size = 100\nnum_epochs = 2\nlearning_rate = 0.01\n\n# MNIST Dataset\ntrain_dataset = dsets.MNIST(root='../data/',\n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data/',\n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# RNN Model (Many-to-One)\nclass RNNModel(nn.Module):\n    def __init__(self, input_size, hidden_size, recurrent_size, num_layers, num_classes, bias=True, grad_clip=None):\n        super(RNNModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.recurrent_size = recurrent_size\n        self.num_layers = num_layers\n\n        self.rnn = LSTMP(input_size, hidden_size, recurrent_size, num_layers=num_layers, \n                         bias=bias, return_sequences=False, grad_clip=grad_clip)\n        self.fc = nn.Linear(recurrent_size, num_classes, bias=bias)\n    \n    def forward(self, x):\n        # Set initial states \n        zeros_h = Variable(torch.zeros(x.size(0), self.recurrent_size))\n        zeros_c = Variable(torch.zeros(x.size(0), self.hidden_size))\n        initial_states = [(zeros_h, zeros_c)] * self.num_layers\n        \n        # Forward propagate RNN\n        #out = self.rnn(x, initial_states)  \n        out, _ = self.rnn(x, initial_states=None)  \n        \n        # Decode hidden state of last time step\n        out = self.fc(out)  \n        return out\n\nrnn = RNNModel(input_size, hidden_size, recurrent_size, num_layers, num_classes, bias=True, grad_clip=10)\n\n\n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n    \n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images.view(-1, sequence_length, input_size))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs = rnn(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.item()))\n\n# Test the Model\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, sequence_length, input_size))\n    outputs = rnn(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total)) \n\n# Save the Model\ntorch.save(rnn.state_dict(), 'lstmp.pkl')\n"""
rnn/test_mgru.py,9,"b""import torch \nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom modules import MGRU\n\n\ntorch.manual_seed(1111)\n\n# Hyper Parameters\nsequence_length = 28\ninput_size = 28\nhidden_size = 128\nnum_layers = 2\nnum_classes = 10\nbatch_size = 100\nnum_epochs = 2\nlearning_rate = 0.01\n\n# MNIST Dataset\ntrain_dataset = dsets.MNIST(root='../data/',\n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data/',\n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# RNN Model (Many-to-One)\nclass RNNModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes, bias=True, grad_clip=None):\n        super(RNNModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.rnn = MGRU(input_size, hidden_size, num_layers=num_layers, \n                       bias=bias, return_sequences=False, grad_clip=grad_clip)\n        self.fc = nn.Linear(hidden_size, num_classes, bias=bias)\n    \n    def forward(self, x):\n        # Set initial states \n        initial_states = [Variable(torch.zeros(x.size(0), self.hidden_size)) for _ in range(self.num_layers)]\n        \n        # Forward propagate RNN\n        out = self.rnn(x, initial_states)  \n        \n        # Decode hidden state of last time step\n        out = self.fc(out)  \n        return out\n\nrnn = RNNModel(input_size, hidden_size, num_layers, num_classes, bias=True, grad_clip=10)\n\n\n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n    \n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images.view(-1, sequence_length, input_size))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs = rnn(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.item()))\n\n# Test the Model\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, sequence_length, input_size))\n    outputs = rnn(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total)) \n\n# Save the Model\ntorch.save(rnn.state_dict(), 'mgru.pkl')\n"""
rnn/test_rnn.py,9,"b""import torch \nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom modules import RNN\n\n\ntorch.manual_seed(1111)\n\n# Hyper Parameters\nsequence_length = 28\ninput_size = 28\nhidden_size = 128\nnum_layers = 2\nnum_classes = 10\nbatch_size = 100\nnum_epochs = 2\nlearning_rate = 0.01\n\n# MNIST Dataset\ntrain_dataset = dsets.MNIST(root='../data/',\n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data/',\n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# RNN Model (Many-to-One)\nclass RNNModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes, \n                 bias=True, grad_clip=None):\n        super(RNNModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.rnn = RNN(input_size, hidden_size, num_layers=num_layers, \n                       bias=bias, return_sequences=False, grad_clip=grad_clip)\n        self.fc = nn.Linear(hidden_size, num_classes, bias=bias)\n    \n    def forward(self, x):\n        # Set initial states \n        initial_states = [Variable(torch.zeros(x.size(0), self.hidden_size)) for _ in range(self.num_layers)]\n        \n        # Forward propagate RNN\n        out = self.rnn(x, initial_states)  \n        \n        # Decode hidden state of last time step\n        out = self.fc(out)  \n        return out\n\nrnn = RNNModel(input_size, hidden_size, num_layers, num_classes, bias=True, grad_clip=10)\n\n\n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n    \n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images.view(-1, sequence_length, input_size))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs = rnn(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.item()))\n\n# Test the Model\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, sequence_length, input_size))\n    outputs = rnn(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total)) \n\n# Save the Model\ntorch.save(rnn.state_dict(), 'rnn.pkl')\n"""
senet/cnn.py,8,"b""import torch \nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom modules import SEWrapper\n\ntorch.manual_seed(1111)\n\n\n# Hyper Parameters\nnum_epochs = 50\nbatch_size = 100\nlr = 1e-3\n\nratio = 3 # reduction ratio for SE \n\n# MNIST Dataset\ntrain_dataset = dsets.MNIST(root='../data/',\n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data/',\n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# CNN Model (2 conv layer)\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n            SEWrapper(16, ratio),\n            nn.BatchNorm2d(16),\n            nn.MaxPool2d(2),\n            nn.ReLU())\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n            SEWrapper(32, ratio),\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(2),\n            nn.ReLU())\n        self.fc = nn.Linear(7*7*32, 10)\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        return out\n        \ncnn = CNN()\n\n\n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = torch.optim.Adam(cnn.parameters(), lr=lr)  \n\n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images)\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs = cnn(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n\n    # Test the Model\n    cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = Variable(images)\n        outputs = cnn(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum()\n\n    print('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))\n\n    cnn.train()\n\n# Save the Trained Model\ntorch.save(cnn.state_dict(), 'cnn.pkl')\n"""
senet/modules.py,1,"b'import math\n\nimport torch\nimport torch.nn as nn\n\nclass SEWrapper(nn.Module):\n\n    def __init__(self, channels, ratio=4):\n        super(SEWrapper, self).__init__()\n\n        self.linear = nn.Sequential(nn.Linear(channels, channels // ratio), \n                                    nn.ReLU(),\n                                    nn.Linear(channels // ratio, channels), \n                                    nn.Sigmoid())\n\n    def forward(self, input):\n        sq = input.mean(-1).mean(-1)\n        ex = self.linear(sq)\n\n        return input * ex.unsqueeze(-1).unsqueeze(-1)\n'"
swish/activation.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SiLU(nn.Module):\n\n    def __init__(self):\n        super(SiLU, self).__init__()\n\n    def forward(self, input):\n        return 1.67653251702 * input * F.sigmoid(input)\n'"
swish/mnist_mlp.py,7,"b""import torch\nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom activation import SiLU\n\n\ntorch.manual_seed(1111)\n\n# Hyper Parameters \ninput_size = 784\nhidden_size = 500\nnum_classes = 10\nnum_epochs = 5\nbatch_size = 100\nlearning_rate = 0.001\n\ngamma = 2\n\n# MNIST Dataset \ntrain_dataset = dsets.MNIST(root='../data', \n                            train=True, \n                            transform=transforms.ToTensor(),  \n                            download=True)\n\ntest_dataset = dsets.MNIST(root='../data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# Neural Network Model (1 hidden layer)\nclass Net(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size) \n        self.act = SiLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)  \n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.act(out)\n        out = self.fc2(out)\n        return out\n    \nnet = Net(input_size, hidden_size, num_classes)\n\n    \n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()  \noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n\n# Train the Model\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        # Convert torch tensor to Variable\n        images = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()  # zero the gradient buffer\n        outputs = net(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        if (i+1) % 100 == 0:\n            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n\n# Test the Model\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    images = Variable(images.view(-1, 28*28))\n    outputs = net(images)\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"""
cffi/functions/__init__.py,0,b''
cffi/functions/relu.py,1,"b'# functions/relu.py\nimport torch\nfrom torch.autograd import Function\nfrom _ext import ext_lib\n\n\nclass ReLUF(Function):\n    def forward(self, input):\n        self.save_for_backward(input)\n\n        output = input.new()\n        if not input.is_cuda:\n            ext_lib.relu_forward(input, output)\n        else:\n            raise Exception(""No CUDA Implementation"")\n        return output\n\n    def backward(self, grad_output):\n        input, = self.saved_tensors\n\n        grad_input = grad_output.new()\n        if not grad_output.is_cuda:\n            ext_lib.relu_backward(grad_output, input, grad_input)\n        else:\n            raise Exception(""No CUDA Implementation"")\n        return grad_input\n'"
cffi/modules/__init__.py,0,b''
cffi/modules/relu.py,1,"b'from torch.nn.modules.module import Module\nfrom functions.relu import ReLUF\n\nclass ReLUM(Module):\n    def forward(self, input):\n        return ReLUF()(input)\n'"
