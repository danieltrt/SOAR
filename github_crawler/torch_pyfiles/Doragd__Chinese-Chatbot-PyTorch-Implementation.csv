file_path,api_count,code
config.py,2,"b'# -*- coding: utf-8 -*- \r\n\r\nimport torch\r\n\r\nclass Config:\r\n    \'\'\'\r\n    Chatbot\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\r\n    \'\'\'\r\n    corpus_data_path = \'corpus.pth\' #\xe5\xb7\xb2\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xaf\xb9\xe8\xaf\x9d\xe6\x95\xb0\xe6\x8d\xae\r\n    use_QA_first = True #\xe6\x98\xaf\xe5\x90\xa6\xe8\xbd\xbd\xe5\x85\xa5\xe7\x9f\xa5\xe8\xaf\x86\xe5\xba\x93\r\n    max_input_length = 50 #\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe5\x8f\xa5\xe5\xad\x90\xe9\x95\xbf\xe5\xba\xa6\r\n    max_generate_length = 20 #\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe5\x8f\xa5\xe5\xad\x90\xe9\x95\xbf\xe5\xba\xa6\r\n    prefix = \'checkpoints/chatbot\'  #\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x96\xad\xe7\x82\xb9\xe8\xb7\xaf\xe5\xbe\x84\xe5\x89\x8d\xe7\xbc\x80\r\n    model_ckpt  = \'checkpoints/chatbot_0509_1437\' #\xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xb7\xaf\xe5\xbe\x84\r\n    \'\'\'\r\n    \xe8\xae\xad\xe7\xbb\x83\xe8\xb6\x85\xe5\x8f\x82\xe6\x95\xb0\r\n    \'\'\'\r\n    batch_size = 2048\r\n    shuffle = True #dataloader\xe6\x98\xaf\xe5\x90\xa6\xe6\x89\x93\xe4\xb9\xb1\xe6\x95\xb0\xe6\x8d\xae\r\n    num_workers = 0 #dataloader\xe5\xa4\x9a\xe8\xbf\x9b\xe7\xa8\x8b\xe6\x8f\x90\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\r\n    bidirectional = True #Encoder-RNN\xe6\x98\xaf\xe5\x90\xa6\xe5\x8f\x8c\xe5\x90\x91\r\n    hidden_size = 256\r\n    embedding_dim = 256\r\n    method = \'dot\' #attention method\r\n    dropout = 0 #\xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8dropout\r\n    clip = 50.0 #\xe6\xa2\xaf\xe5\xba\xa6\xe8\xa3\x81\xe5\x89\xaa\xe9\x98\x88\xe5\x80\xbc\r\n    num_layers = 2 #Encoder-RNN\xe5\xb1\x82\xe6\x95\xb0\r\n    learning_rate = 1e-3\r\n    teacher_forcing_ratio = 1.0 #teacher_forcing\xe6\xaf\x94\xe4\xbe\x8b\r\n    decoder_learning_ratio = 5.0\r\n    \'\'\'\r\n    \xe8\xae\xad\xe7\xbb\x83\xe5\x91\xa8\xe6\x9c\x9f\xe4\xbf\xa1\xe6\x81\xaf\r\n    \'\'\'\r\n    epoch = 6000\r\n    print_every = 1 #\xe6\xaf\x8f\xe9\x9a\x94print_every\xe4\xb8\xaaIteration\xe6\x89\x93\xe5\x8d\xb0\xe4\xb8\x80\xe6\xac\xa1\r\n    save_every = 50 #\xe6\xaf\x8f\xe9\x9a\x94save_every\xe4\xb8\xaaEpoch\xe6\x89\x93\xe5\x8d\xb0\xe4\xb8\x80\xe6\xac\xa1 \r\n    \'\'\'\r\n    GPU\r\n    \'\'\'\r\n    use_gpu = torch.cuda.is_available() #\xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8gpu\r\n    device = torch.device(""cuda"" if use_gpu else ""cpu"") #device\r\n\r\n    \r\n    '"
dataload.py,6,"b""# -*- coding: utf-8 -*- \r\n\r\nimport torch\r\nimport itertools\r\nfrom torch.utils import data as dataimport\r\n\r\ndef zeroPadding(l, fillvalue):\r\n    '''\r\n    l\xe6\x98\xaf\xe5\xa4\x9a\xe4\xb8\xaa\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90(list)\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8zip_longest padding\xe6\x88\x90\xe5\xae\x9a\xe9\x95\xbf\xef\xbc\x8c\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba\xe6\x9c\x80\xe9\x95\xbf\xe5\x8f\xa5\xe5\xad\x90\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\r\n    \xe5\x9c\xa8zeroPadding\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xad\xe9\x9a\x90\xe5\xbc\x8f\xe8\xbd\xac\xe7\xbd\xae\r\n    [batch_size, max_seq_len] ==> [max_seq_len, batch_size]\r\n    '''\r\n    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\r\n\r\ndef binaryMatrix(l, value):\r\n    '''\r\n    \xe7\x94\x9f\xe6\x88\x90mask\xe7\x9f\xa9\xe9\x98\xb5, 0\xe8\xa1\xa8\xe7\xa4\xbapadding,1\xe8\xa1\xa8\xe7\xa4\xba\xe6\x9c\xaapadding\r\n    shape\xe5\x90\x8cl,\xe5\x8d\xb3[max_seq_len, batch_size]\r\n    '''\r\n    m = []\r\n    for i, seq in enumerate(l):\r\n        m.append([])\r\n        for token in seq:\r\n            if token == value:\r\n                m[i].append(0)\r\n            else:\r\n                m[i].append(1)\r\n    return m\r\n\r\ndef create_collate_fn(padding, eos):\r\n    '''\r\n    \xe8\xaf\xb4\xe6\x98\x8edataloader\xe5\xa6\x82\xe4\xbd\x95\xe5\x8c\x85\xe8\xa3\x85\xe4\xb8\x80\xe4\xb8\xaabatch,\xe4\xbc\xa0\xe5\x85\xa5\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe4\xb8\xba</PAD>\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95padding,</EOS>\xe5\xad\x97\xe7\xac\xa6\xe7\xb4\xa2\xe5\xbc\x95eos\r\n    collate_fn\xe4\xbc\xa0\xe5\x85\xa5\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe6\x98\xaf\xe7\x94\xb1\xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84__getitem__\xe6\x96\xb9\xe6\xb3\x95\xe7\x9a\x84\xe8\xbf\x94\xe5\x9b\x9e\xe5\x80\xbc\xe7\xbb\x84\xe6\x88\x90\xe7\x9a\x84corpus_item\r\n\r\n    corpus_item: \r\n        lsit, \xe5\xbd\xa2\xe5\xa6\x82[(inputVar1, targetVar1, index1),(inputVar2, targetVar2, index2),...]\r\n        inputVar1: [word_ix, word_ix,word_ix,...]\r\n        targetVar1: [word_ix, word_ix,word_ix,...]\r\n    inputs: \r\n        \xe5\x8f\x96\xe5\x87\xba\xe6\x89\x80\xe6\x9c\x89inputVar\xe7\xbb\x84\xe6\x88\x90\xe7\x9a\x84list,\xe5\xbd\xa2\xe5\xa6\x82[inputVar1,inputVar2,inputVar3,...], \r\n        padding\xe5\x90\x8e(\xe8\xbf\x99\xe9\x87\x8c\xe6\x9c\x89\xe9\x9a\x90\xe5\xbc\x8f\xe8\xbd\xac\xe7\xbd\xae)\xe8\xbd\xac\xe4\xb8\xbatensor\xe5\x90\x8e\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\xba:[max_seq_len, batch_size]\r\n    targets:\r\n        \xe5\x8f\x96\xe5\x87\xba\xe6\x89\x80\xe6\x9c\x89targetVar\xe7\xbb\x84\xe6\x88\x90\xe7\x9a\x84list,\xe5\xbd\xa2\xe5\xa6\x82[targetVar1,targetVar2,targetVar3,...]\r\n        padding\xe5\x90\x8e(\xe8\xbf\x99\xe9\x87\x8c\xe6\x9c\x89\xe9\x9a\x90\xe5\xbc\x8f\xe8\xbd\xac\xe7\xbd\xae)\xe8\xbd\xac\xe4\xb8\xbatensor\xe5\x90\x8e\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\xba:[max_seq_len, batch_size]\r\n    input_lengths: \r\n        \xe5\x9c\xa8padding\xe5\x89\x8d\xe8\xa6\x81\xe8\xae\xb0\xe5\xbd\x95\xe5\x8e\x9f\xe6\x9d\xa5\xe7\x9a\x84inputVar\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6, \xe7\x94\xa8\xe4\xba\x8epad_packed_sequence\r\n        \xe5\xbd\xa2\xe5\xa6\x82: [length_inputVar1, length_inputVar2, length_inputVar3, ...]\r\n    max_targets_length:\r\n        \xe8\xaf\xa5\xe6\x89\xb9\xe6\xac\xa1\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89target\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\r\n    mask:\r\n        \xe5\xbd\xa2\xe7\x8a\xb6: [max_seq_len, batch_size]\r\n    indexes:\r\n        \xe8\xae\xb0\xe5\xbd\x95\xe4\xb8\x80\xe4\xb8\xaabatch\xe4\xb8\xad\xe6\xaf\x8f\xe4\xb8\xaa \xe5\x8f\xa5\xe5\xad\x90\xe5\xaf\xb9 \xe5\x9c\xa8corpus\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xad\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\r\n        \xe5\xbd\xa2\xe5\xa6\x82: [index1, index2, ...]\r\n\r\n    '''\r\n    def collate_fn(corpus_item):\r\n        #\xe6\x8c\x89\xe7\x85\xa7inputVar\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x8e\x92\xe5\xba\x8f,\xe6\x98\xaf\xe8\xb0\x83\xe7\x94\xa8pad_packed_sequence\xe6\x96\xb9\xe6\xb3\x95\xe7\x9a\x84\xe8\xa6\x81\xe6\xb1\x82\r\n        corpus_item.sort(key=lambda p: len(p[0]), reverse=True) \r\n        inputs, targets, indexes = zip(*corpus_item)\r\n        input_lengths = torch.tensor([len(inputVar) for inputVar in inputs])\r\n        inputs = zeroPadding(inputs, padding)\r\n        inputs = torch.LongTensor(inputs) #\xe6\xb3\xa8\xe6\x84\x8f\xe8\xbf\x99\xe9\x87\x8c\xe8\xa6\x81LongTensor\r\n        \r\n        max_target_length = max([len(targetVar) for targetVar in targets])\r\n        targets = zeroPadding(targets, padding)\r\n        mask = binaryMatrix(targets, padding)\r\n        mask = torch.ByteTensor(mask)\r\n        targets = torch.LongTensor(targets)\r\n        \r\n        \r\n        return inputs, targets, mask, input_lengths, max_target_length, indexes\r\n\r\n    return collate_fn\r\n\r\n\r\n\r\n\r\nclass CorpusDataset(dataimport.Dataset):\r\n\r\n    def __init__(self, opt):\r\n        self.opt = opt\r\n        self._data = torch.load(opt.corpus_data_path)\r\n        self.word2ix = self._data['word2ix']\r\n        self.corpus = self._data['corpus']\r\n        self.padding = self.word2ix.get(self._data.get('padding'))\r\n        self.eos = self.word2ix.get(self._data.get('eos'))\r\n        self.sos = self.word2ix.get(self._data.get('sos'))\r\n        \r\n    def __getitem__(self, index):\r\n        inputVar = self.corpus[index][0]\r\n        targetVar = self.corpus[index][1]\r\n        return inputVar,targetVar, index\r\n\r\n    def __len__(self):\r\n        return len(self.corpus)\r\n\r\n\r\ndef get_dataloader(opt):\r\n    dataset = CorpusDataset(opt)\r\n    dataloader = dataimport.DataLoader(dataset,\r\n                                 batch_size=opt.batch_size,\r\n                                 shuffle=opt.shuffle, #\xe6\x98\xaf\xe5\x90\xa6\xe6\x89\x93\xe4\xb9\xb1\xe6\x95\xb0\xe6\x8d\xae\r\n                                 num_workers=opt.num_workers, #\xe5\xa4\x9a\xe8\xbf\x9b\xe7\xa8\x8b\xe6\x8f\x90\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\r\n                                 drop_last=True, #\xe4\xb8\xa2\xe6\x8e\x89\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x8d\xe8\xb6\xb3\xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\r\n                                 collate_fn=create_collate_fn(dataset.padding, dataset.eos))\r\n    return dataloader"""
datapreprocess.py,1,"b'# -*- coding: utf-8 -*- \r\n\r\nimport jieba\r\nimport torch\r\nimport re\r\nimport logging\r\njieba.setLogLevel(logging.INFO) #\xe5\x85\xb3\xe9\x97\xadjieba\xe8\xbe\x93\xe5\x87\xba\xe4\xbf\xa1\xe6\x81\xaf\r\n\r\ncorpus_file = \'clean_chat_corpus/qingyun.tsv\' #\xe6\x9c\xaa\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xaf\xb9\xe8\xaf\x9d\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\r\ncop = re.compile(""[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]"") #\xe5\x88\x86\xe8\xaf\x8d\xe5\xa4\x84\xe7\x90\x86\xe6\xad\xa3\xe5\x88\x99\r\nunknown = \'</UNK>\' #unknown\xe5\xad\x97\xe7\xac\xa6\r\neos = \'</EOS>\' #\xe5\x8f\xa5\xe5\xad\x90\xe7\xbb\x93\xe6\x9d\x9f\xe7\xac\xa6\r\nsos = \'</SOS>\' #\xe5\x8f\xa5\xe5\xad\x90\xe5\xbc\x80\xe5\xa7\x8b\xe7\xac\xa6\r\npadding = \'</PAD>\' #\xe5\x8f\xa5\xe5\xad\x90\xe5\xa1\xab\xe5\x85\x85\xe8\xb4\x9f\r\nmax_voc_length = 10000 #\xe5\xad\x97\xe5\x85\xb8\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\r\nmin_word_appear = 10 #\xe5\x8a\xa0\xe5\x85\xa5\xe5\xad\x97\xe5\x85\xb8\xe7\x9a\x84\xe8\xaf\x8d\xe7\x9a\x84\xe8\xaf\x8d\xe9\xa2\x91\xe6\x9c\x80\xe5\xb0\x8f\xe5\x80\xbc\r\nmax_sentence_length = 50 #\xe6\x9c\x80\xe5\xa4\xa7\xe5\x8f\xa5\xe5\xad\x90\xe9\x95\xbf\xe5\xba\xa6\r\nsave_path = \'corpus.pth\' #\xe5\xb7\xb2\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xaf\xb9\xe8\xaf\x9d\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xbf\x9d\xe5\xad\x98\xe8\xb7\xaf\xe5\xbe\x84\r\n\r\ndef preprocess():\r\n    print(""preprocessing..."")\r\n    \'\'\'\xe5\xa4\x84\xe7\x90\x86\xe5\xaf\xb9\xe8\xaf\x9d\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\'\'\'\r\n    data = []\r\n    with open(corpus_file, encoding=\'utf-8\') as f:\r\n        lines = f.readlines()\r\n        for line in lines:\r\n            values = line.strip(\'\\n\').split(\'\\t\')\r\n            sentences = []\r\n            for value in values:\r\n                sentence = jieba.lcut(cop.sub("""",value))\r\n                sentence = sentence[:max_sentence_length] + [eos]\r\n                sentences.append(sentence)\r\n            data.append(sentences)\r\n\r\n    \'\'\'\xe7\x94\x9f\xe6\x88\x90\xe5\xad\x97\xe5\x85\xb8\xe5\x92\x8c\xe5\x8f\xa5\xe5\xad\x90\xe7\xb4\xa2\xe5\xbc\x95\'\'\'\r\n    word_nums = {} #\xe7\xbb\x9f\xe8\xae\xa1\xe5\x8d\x95\xe8\xaf\x8d\xe7\x9a\x84\xe8\xaf\x8d\xe9\xa2\x91\r\n    def update(word_nums):\r\n        def fun(word):\r\n            word_nums[word] = word_nums.get(word, 0) + 1\r\n            return None\r\n        return fun\r\n    lambda_ = update(word_nums)\r\n    _ = {lambda_(word) for sentences in data for sentence in sentences for word in sentence}\r\n    #\xe6\x8c\x89\xe8\xaf\x8d\xe9\xa2\x91\xe4\xbb\x8e\xe9\xab\x98\xe5\x88\xb0\xe4\xbd\x8e\xe6\x8e\x92\xe5\xba\x8f\r\n    word_nums_list = sorted([(num, word) for word, num in word_nums.items()], reverse=True)\r\n    #\xe8\xaf\x8d\xe5\x85\xb8\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6: max_voc_length \xe6\x9c\x80\xe5\xb0\x8f\xe5\x8d\x95\xe8\xaf\x8d\xe8\xaf\x8d\xe9\xa2\x91: min_word_appear\r\n    words = [word[1] for word in word_nums_list[:max_voc_length] if word[0] >= min_word_appear]\r\n    #\xe6\xb3\xa8\xe6\x84\x8f: \xe8\xbf\x99\xe9\x87\x8ceos\xe5\xb7\xb2\xe7\xbb\x8f\xe5\x9c\xa8\xe5\x89\x8d\xe9\x9d\xa2\xe5\x8a\xa0\xe5\x85\xa5\xe4\xba\x86words,\xe6\x95\x85\xe8\xbf\x99\xe9\x87\x8c\xe4\xb8\x8d\xe7\x94\xa8\xe9\x87\x8d\xe5\xa4\x8d\xe5\x8a\xa0\xe5\x85\xa5\r\n    words = [unknown, padding, sos] + words\r\n    word2ix = {word: ix for ix, word in enumerate(words)}\r\n    ix2word = {ix: word for word, ix in word2ix.items()}\r\n    ix_corpus = [[[word2ix.get(word, word2ix.get(unknown)) for word in sentence]\r\n                        for sentence in item]\r\n                        for item in data]\r\n\r\n    \'\'\'\r\n    \xe4\xbf\x9d\xe5\xad\x98\xe5\xa4\x84\xe7\x90\x86\xe5\xa5\xbd\xe7\x9a\x84\xe5\xaf\xb9\xe8\xaf\x9d\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\r\n\r\n    ix_corpus: list, \xe5\x85\xb6\xe4\xb8\xad\xe5\x85\x83\xe7\xb4\xa0\xe4\xb8\xba[Question_sequence_list, Answer_seqence_list]\r\n    Question_sequence_list: e.g. [word_ix, word_ix, word_ix, ...]\r\n\r\n    word2ix: dict, \xe5\x8d\x95\xe8\xaf\x8d:\xe7\xb4\xa2\xe5\xbc\x95\r\n\r\n    ix2word: dict, \xe7\xb4\xa2\xe5\xbc\x95:\xe5\x8d\x95\xe8\xaf\x8d\r\n\r\n    \'\'\'\r\n    clean_data = {\r\n        \'corpus\': ix_corpus, \r\n        \'word2ix\': word2ix,\r\n        \'ix2word\': ix2word,\r\n        \'unknown\' : \'</UNK>\',\r\n        \'eos\' : \'</EOS>\',\r\n        \'sos\' : \'</SOS>\',\r\n        \'padding\': \'</PAD>\',\r\n    }\r\n    torch.save(clean_data, save_path)\r\n    print(\'save clean data in %s\' % save_path)\r\n\r\nif __name__ == ""__main__"":\r\n    preprocess()'"
main.py,0,"b'# -*- coding: utf-8 -*- \r\nimport os\r\nfrom datapreprocess import preprocess\r\nimport train_eval\r\nimport fire\r\nfrom QA_data import QA_test\r\nfrom config import Config\r\n\r\n\r\n\r\ndef chat(**kwargs):\r\n    \r\n    opt = Config()\r\n    for k, v in kwargs.items(): #\xe8\xae\xbe\xe7\xbd\xae\xe5\x8f\x82\xe6\x95\xb0\r\n        setattr(opt, k, v)   \r\n\r\n    searcher, sos, eos, unknown, word2ix, ix2word = train_eval.test(opt)\r\n\r\n    if os.path.isfile(opt.corpus_data_path) == False:\r\n        preprocess()\r\n\r\n    while(1):\r\n        input_sentence = input(\'Doragd > \')\r\n        if input_sentence == \'q\' or input_sentence == \'quit\' or input_sentence == \'exit\': break\r\n        if opt.use_QA_first:\r\n            query_res = QA_test.match(input_sentence)\r\n            if(query_res == tuple()):\r\n                output_words = train_eval.output_answer(input_sentence, searcher, sos, eos, unknown, opt, word2ix, ix2word)\r\n            else:\r\n                output_words = ""\xe6\x82\xa8\xe6\x98\xaf\xe4\xb8\x8d\xe6\x98\xaf\xe8\xa6\x81\xe6\x89\xbe\xe4\xbb\xa5\xe4\xb8\x8b\xe9\x97\xae\xe9\xa2\x98: "" + query_res[1] + \'\xef\xbc\x8c\xe6\x82\xa8\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xb0\x9d\xe8\xaf\x95\xe8\xbf\x99\xe6\xa0\xb7: \' + query_res[2]\r\n        else:\r\n            output_words = train_eval.output_answer(input_sentence, searcher, sos, eos, unknown, opt, word2ix, ix2word)\r\n        print(\'BOT > \',output_words)\r\n\r\n    QA_test.conn.close()\r\n    \r\nif __name__ == ""__main__"":\r\n    fire.Fire()\r\n'"
model.py,15,"b'# -*- coding: utf-8 -*- \r\nimport torch\r\nimport logging\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom utils.greedysearch import GreedySearchDecoder\r\n\r\n\r\nclass EncoderRNN(nn.Module):\r\n    def __init__(self, opt, voc_length):\r\n        \'\'\'\r\n        voc_length: \xe5\xad\x97\xe5\x85\xb8\xe9\x95\xbf\xe5\xba\xa6,\xe5\x8d\xb3\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\x8d\x95\xe8\xaf\x8d\xe7\x9a\x84one-hot\xe7\xbc\x96\xe7\xa0\x81\xe9\x95\xbf\xe5\xba\xa6\r\n        \'\'\'\r\n        super(EncoderRNN, self).__init__()\r\n        self.num_layers = opt.num_layers\r\n        self.hidden_size = opt.hidden_size\r\n        #nn.Embedding\xe8\xbe\x93\xe5\x85\xa5\xe5\x90\x91\xe9\x87\x8f\xe7\xbb\xb4\xe5\xba\xa6\xe6\x98\xaf\xe5\xad\x97\xe5\x85\xb8\xe9\x95\xbf\xe5\xba\xa6,\xe8\xbe\x93\xe5\x87\xba\xe5\x90\x91\xe9\x87\x8f\xe7\xbb\xb4\xe5\xba\xa6\xe6\x98\xaf\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\xbb\xb4\xe5\xba\xa6\r\n        self.embedding = nn.Embedding(voc_length, opt.embedding_dim)\r\n        #\xe5\x8f\x8c\xe5\x90\x91GRU\xe4\xbd\x9c\xe4\xb8\xbaEncoder\r\n        self.gru = nn.GRU(opt.embedding_dim, self.hidden_size, self.num_layers,\r\n                          dropout=(0 if opt.num_layers == 1 else opt.dropout), bidirectional=opt.bidirectional)\r\n\r\n    def forward(self, input_seq, input_lengths, hidden=None):\r\n        \'\'\'\r\n        input_seq: \r\n            shape: [max_seq_len, batch_size]\r\n        input_lengths: \r\n            \xe4\xb8\x80\xe6\x89\xb9\xe6\xac\xa1\xe4\xb8\xad\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xe9\x95\xbf\xe5\xba\xa6\xe5\x88\x97\xe8\xa1\xa8\r\n            shape:[batch_size]\r\n        hidden:\r\n            Encoder\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8bhidden\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xbaNone\r\n            shape: [num_layers*num_directions, batch_size, hidden_size]\r\n            \xe5\xae\x9e\xe9\x99\x85\xe6\x8e\x92\xe5\x88\x97\xe9\xa1\xba\xe5\xba\x8f\xe6\x98\xafnum_directions\xe5\x9c\xa8\xe5\x89\x8d\xe9\x9d\xa2, \r\n            \xe5\x8d\xb3\xe5\xaf\xb9\xe4\xba\x8e4\xe5\xb1\x82\xe5\x8f\x8c\xe5\x90\x91\xe7\x9a\x84GRU, num_layers*num_directions = 8\r\n            \xe5\x89\x8d4\xe5\xb1\x82\xe6\x98\xaf\xe6\xad\xa3\xe5\x90\x91: [:4, batch_size, hidden_size]\r\n            \xe5\x90\x8e4\xe5\xb1\x82\xe6\x98\xaf\xe5\x8f\x8d\xe5\x90\x91: [4:, batch_size, hidden_size]\r\n        embedded:\r\n            \xe7\xbb\x8f\xe8\xbf\x87\xe8\xaf\x8d\xe5\xb5\x8c\xe5\x85\xa5\xe5\x90\x8e\xe7\x9a\x84\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\r\n            shape: [max_seq_len, batch_size, embedding_dim]\r\n        outputs:\r\n            \xe6\x89\x80\xe6\x9c\x89\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84hidden\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\r\n            \xe4\xb8\x80\xe5\xbc\x80\xe5\xa7\x8b\xe7\x9a\x84shape: [max_seq_len, batch_size, hidden_size*num_directions]\r\n            \xe6\xb3\xa8\xe6\x84\x8f: num_directions\xe5\x9c\xa8\xe5\x89\x8d\xe9\x9d\xa2, \xe5\x8d\xb3\xe5\x89\x8d\xe9\x9d\xa2hidden_size\xe4\xb8\xaa\xe6\x98\xaf\xe6\xad\xa3\xe5\x90\x91\xe7\x9a\x84,\xe5\x90\x8e\xe9\x9d\xa2hidden_size\xe4\xb8\xaa\xe6\x98\xaf\xe5\x8f\x8d\xe5\x90\x91\xe7\x9a\x84\r\n            \xe6\xad\xa3\xe5\x90\x91: [:, :, :hidden_size] \xe5\x8f\x8d\xe5\x90\x91: [:, :, hidden_size:]\r\n            \xe6\x9c\x80\xe5\x90\x8e\xe5\xaf\xb9\xe5\x8f\x8c\xe5\x90\x91GRU\xe6\xb1\x82\xe5\x92\x8c,\xe5\xbe\x97\xe5\x88\xb0\xe6\x9c\x80\xe7\xbb\x88\xe7\x9a\x84outputs: shape\xe4\xb8\xba[max_seq_len, batch_size, hidden_size]\r\n        \xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84hidden:\r\n            [num_layers*num_directions, batch_size, hidden_size]\r\n        \'\'\'\r\n        \r\n        embedded = self.embedding(input_seq) \r\n        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\r\n        outputs, hidden = self.gru(packed, hidden)\r\n        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\r\n        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\r\n        return outputs, hidden\r\n\r\n\r\n\r\nclass Attn(torch.nn.Module):\r\n    def __init__(self, attn_method, hidden_size):\r\n        super(Attn, self).__init__()\r\n        self.method = attn_method #attention\xe6\x96\xb9\xe6\xb3\x95\r\n        self.hidden_size = hidden_size\r\n        if self.method not in [\'dot\', \'general\', \'concat\']:\r\n            raise ValueError(self.method, ""is not an appropriate attention method."")\r\n        if self.method == \'general\':\r\n            self.attn = torch.nn.Linear(self.hidden_size, self.hidden_size)\r\n        elif self.method == \'concat\':\r\n            self.attn = torch.nn.Linear(self.hidden_size * 2, self.hidden_size)\r\n            self.v = torch.nn.Parameter(torch.FloatTensor(self.hidden_size))\r\n\r\n    def dot_score(self, hidden, encoder_outputs):\r\n        \'\'\'\r\n        encoder_outputs:\r\n            encoder(\xe5\x8f\x8c\xe5\x90\x91GRU)\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84hidden\xe8\xbe\x93\xe5\x87\xba\r\n            shape: [max_seq_len, batch_size, hidden_size]\r\n            \xe6\x95\xb0\xe5\xad\xa6\xe7\xac\xa6\xe5\x8f\xb7\xe8\xa1\xa8\xe7\xa4\xba: h_s\r\n        hidden:\r\n            decoder(\xe5\x8d\x95\xe5\x90\x91GRU)\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84hidden\xe8\xbe\x93\xe5\x87\xba,\xe5\x8d\xb3decoder_ouputs\r\n            shape: [max_seq_len, batch_size, hidden_size]\r\n            \xe6\x95\xb0\xe5\xad\xa6\xe7\xac\xa6\xe5\x8f\xb7\xe8\xa1\xa8\xe7\xa4\xba: h_t\r\n        \xe6\xb3\xa8\xe6\x84\x8f: attention method: \'dot\', Hadamard\xe4\xb9\x98\xe6\xb3\x95,\xe5\xaf\xb9\xe5\xba\x94\xe5\x85\x83\xe7\xb4\xa0\xe7\x9b\xb8\xe4\xb9\x98\xef\xbc\x8c\xe7\x94\xa8*\xe5\xb0\xb1\xe5\xa5\xbd\xe4\xba\x86\r\n            torch.matmul\xe6\x98\xaf\xe7\x9f\xa9\xe9\x98\xb5\xe4\xb9\x98\xe6\xb3\x95, \xe6\x89\x80\xe4\xbb\xa5\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe6\x98\xafh_s * h_t\r\n            h_s\xe7\x9a\x84\xe5\x85\x83\xe7\xb4\xa0\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaahidden_size\xe5\x90\x91\xe9\x87\x8f, \xe8\xa6\x81\xe5\xbe\x97\xe5\x88\xb0score\xe5\x80\xbc,\xe9\x9c\x80\xe8\xa6\x81\xe5\x9c\xa8dim=2\xe4\xb8\x8a\xe6\xb1\x82\xe5\x92\x8c\r\n            \xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe5\x85\x88\xe4\xb8\x8d\xe7\x9c\x8bbatch_size,h_s * h_t \xe8\xa6\x81\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe6\x98\xaf [max_seq_len]\r\n            \xe5\x8d\xb3\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x97\xb6\xe5\x88\xbb\xe9\x83\xbd\xe8\xa6\x81\xe5\xbe\x97\xe5\x88\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe5\x88\x86\xe6\x95\xb0\xe5\x80\xbc, \xe6\x9c\x80\xe5\x90\x8e\xe6\x8a\x8abatch_size\xe5\x8a\xa0\xe8\xbf\x9b\xe6\x9d\xa5,\r\n            \xe6\x9c\x80\xe7\xbb\x88shape\xe4\xb8\xba: [max_seq_len, batch_size]   \r\n        \'\'\'\r\n\r\n        return torch.sum(hidden * encoder_outputs, dim=2)\r\n\r\n    def general_score(self, hidden, encoder_outputs):\r\n        #\xe5\x85\x88\xe5\xad\xa6\xe4\xb9\xa0\xe5\x88\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe7\xba\xbf\xe6\x80\xa7\xe5\x8f\x98\xe6\x8d\xa2Wh_s,\xe5\x8d\xb3energy\r\n        #\xe7\x84\xb6\xe5\x90\x8e\xe8\xbf\x9b\xe8\xa1\x8c\xe7\x82\xb9\xe4\xb9\x98,\xe6\x89\x80\xe4\xbb\xa5\xe6\x9c\x80\xe5\x90\x8e\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\xba h_t*Wh_s\r\n        energy = self.attn(encoder_outputs)\r\n        return torch.sum(hidden * energy, dim=2)\r\n\r\n    def concat_score(self, hidden, encoder_outputs):\r\n        \'\'\'\r\n        hidden:\r\n            h_t, shape: [max_seq_len, batch_size, hidden_size]\r\n            expand(max_seq_len, -1,-1) ==> [max_seq_len, batch_size, hidden_size]\r\n        \xe4\xb8\x8eencoder_outputs\xe5\x9c\xa8\xe7\xac\xac2\xe7\xbb\xb4\xe4\xb8\x8a\xe8\xbf\x9b\xe8\xa1\x8ccat, \xe6\x9c\x80\xe5\x90\x8eshape: [max_seq_len, batch_size, hidden_size*2]\r\n        \xe7\xbb\x8f\xe8\xbf\x87attn\xe5\x90\x8e\xe5\xbe\x97\xe5\x88\xb0[max_seq_len, batch_size, hidden_size],\xe5\x86\x8d\xe8\xbf\x9b\xe8\xa1\x8ctanh,shape\xe4\xb8\x8d\xe5\x8f\x98\r\n        \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x8ev\xe4\xb9\x98\r\n        \'\'\'\r\n        energy = self.attn(torch.cat((hidden.expand(encoder_outputs.size(0), -1, -1), \r\n\t\t\t\t      encoder_outputs), 2)).tanh()\r\n        return torch.sum(self.v * energy, dim=2)\r\n    \r\n    def forward(self, hidden, encoder_outputs):\r\n        if self.method == \'general\':\r\n            attn_energies = self.general_score(hidden, encoder_outputs)\r\n        elif self.method == \'concat\':\r\n            attn_energies = self.concat_score(hidden, encoder_outputs)\r\n        elif self.method == \'dot\':\r\n            attn_energies = self.dot_score(hidden, encoder_outputs)\r\n        #\xe5\xbe\x97\xe5\x88\xb0score,shape\xe4\xb8\xba[max_seq_len, batch_size],\xe7\x84\xb6\xe5\x90\x8e\xe8\xbd\xac\xe7\xbd\xae\xe4\xb8\xba[batch_size, max_seq_len]\r\n        attn_energies = attn_energies.t()\r\n        #\xe5\xaf\xb9dim=1\xe8\xbf\x9b\xe8\xa1\x8csoftmax,\xe7\x84\xb6\xe5\x90\x8e\xe6\x8f\x92\xe5\x85\xa5\xe7\xbb\xb4\xe5\xba\xa6[batch_size, 1, max_seq_len]\r\n        return F.softmax(attn_energies, dim=1).unsqueeze(1)\r\n\r\nclass LuongAttnDecoderRNN(nn.Module):\r\n    def __init__(self, opt, voc_length):\r\n        super(LuongAttnDecoderRNN, self).__init__()\r\n\r\n        self.attn_method = opt.method\r\n        self.hidden_size = opt.hidden_size\r\n        self.output_size = voc_length\r\n        self.num_layers = opt.num_layers\r\n        self.dropout = opt.dropout\r\n        self.embedding = nn.Embedding(voc_length, opt.embedding_dim)\r\n        self.embedding_dropout = nn.Dropout(self.dropout)\r\n        self.gru = nn.GRU(opt.embedding_dim, self.hidden_size, self.num_layers, dropout=(0 if self.num_layers == 1 else self.dropout))\r\n        self.concat = nn.Linear(self.hidden_size * 2, self.hidden_size)\r\n        self.out = nn.Linear(self.hidden_size, self.output_size)\r\n        self.attn = Attn(self.attn_method, self.hidden_size)\r\n\r\n    def forward(self, input_step, last_hidden, encoder_outputs):\r\n        \'\'\'\r\n        input_step: \r\n            decoder\xe6\x98\xaf\xe9\x80\x90\xe5\xad\x97\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84,\xe5\x8d\xb3\xe6\xaf\x8f\xe4\xb8\xaatimestep\xe4\xba\xa7\xe7\x94\x9f\xe4\xb8\x80\xe4\xb8\xaa\xe5\xad\x97,\r\n            decoder\xe6\x8e\xa5\xe6\x94\xb6\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5: input_step=\'/SOS\'\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95 \xe5\x92\x8c encoder\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82hidden\xe8\xbe\x93\xe5\x87\xba\r\n            \xe6\x95\x85shape:[1, batch_size]\r\n        last_hidden:\r\n            \xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaaGRUCell\xe7\x9a\x84hidden\xe8\xbe\x93\xe5\x87\xba\r\n            \xe5\x88\x9d\xe5\xa7\x8b\xe5\x80\xbc\xe4\xb8\xbaencoder\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82hidden\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe4\xbc\xa0\xe5\x85\xa5\xe7\x9a\x84\xe6\x98\xafencoder_hidden\xe7\x9a\x84\xe6\xad\xa3\xe5\x90\x91\xe9\x83\xa8\xe5\x88\x86,\r\n            \xe5\x8d\xb3encoder_hidden[:decoder.num_layers], \xe4\xb8\xba\xe4\xba\x86\xe5\x92\x8cdecoder\xe5\xaf\xb9\xe5\xba\x94,\xe6\x89\x80\xe4\xbb\xa5\xe5\x8f\x96\xe7\x9a\x84\xe6\x98\xafdecoder\xe7\x9a\x84num_layers\r\n            shape\xe4\xb8\xba[num_layers, batch_size, hidden_size]\r\n        encoder_outputs:\r\n            \xe8\xbf\x99\xe9\x87\x8c\xe8\xbf\x98\xe6\x8e\xa5\xe6\x94\xb6\xe4\xba\x86encoder_outputs\xe8\xbe\x93\xe5\x85\xa5,\xe7\x94\xa8\xe4\xba\x8e\xe8\xae\xa1\xe7\xae\x97attention\r\n        \'\'\'\r\n        #\xe8\xbd\xac\xe4\xb8\xba\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f[1, batch_size, embedding_dim]\r\n        embedded = self.embedding(input_step) \r\n        embedded = self.embedding_dropout(embedded)\r\n        #rnn_output: [1, batch_size, hidden_size]\r\n        #hidden: [num_layers, batch_size, hidden_size]\r\n        rnn_output, hidden = self.gru(embedded, last_hidden)\r\n        #attn_weights: [batch_size, 1, max_seq_len]\r\n        attn_weights = self.attn(rnn_output, encoder_outputs)\r\n        #bmm\xe6\x89\xb9\xe9\x87\x8f\xe7\x9f\xa9\xe9\x98\xb5\xe7\x9b\xb8\xe4\xb9\x98, \r\n        #attn_weights\xe6\x98\xafbatch_size\xe4\xb8\xaa\xe7\x9f\xa9\xe9\x98\xb5[1, max_seq_len]\r\n        #encoder_outputs.transpose(0, 1)\xe6\x98\xafbatch_size\xe4\xb8\xaa[max_seq_len, hidden_size]\r\n        #\xe7\x9b\xb8\xe4\xb9\x98\xe7\xbb\x93\xe6\x9e\x9ccontext\xe4\xb8\xba: [batch_size, 1, hidden_size]\r\n        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\r\n        rnn_output = rnn_output.squeeze(0)\r\n        context = context.squeeze(1)\r\n        #\xe5\x8e\x8b\xe7\xbc\xa9\xe7\xbb\xb4\xe5\xba\xa6\xe5\x90\x8e,rnn_output[batch_size, hidden_size],context[batch_size, hidden_size]\r\n        #\xe5\x9c\xa8dim=1\xe4\xb8\x8a\xe8\xbf\x9e\xe6\x8e\xa5: [batch_size, hidden_size * 2], contact\xe5\x90\x8e[batch_size, hidden_size]\r\n        #tanh\xe5\x90\x8eshape\xe4\xb8\x8d\xe5\x8f\x98: [batch_size, hidden_size]\r\n        #\xe6\x9c\x80\xe5\x90\x8e\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5,\xe6\x98\xa0\xe5\xb0\x84\xe4\xb8\xba[batch_size, voc_length]\r\n        #\xe6\x9c\x80\xe5\x90\x8e\xe8\xbf\x9b\xe8\xa1\x8csoftmax: [batch_size, voc_length]\r\n        concat_input = torch.cat((rnn_output, context), 1)\r\n        concat_output = torch.tanh(self.concat(concat_input))  \r\n        output = self.out(concat_output)\r\n        output = F.softmax(output, dim=1)\r\n\r\n        return output, hidden'"
train_eval.py,16,"b'# -*- coding: utf-8 -*- \r\nimport re\r\nimport time\r\nimport random\r\nimport jieba\r\nimport torch\r\nimport logging\r\nimport torch.nn as nn\r\nfrom torchnet import meter\r\nfrom model import EncoderRNN, LuongAttnDecoderRNN\r\nfrom utils.greedysearch import GreedySearchDecoder\r\nfrom dataload import get_dataloader\r\nfrom config import Config\r\njieba.setLogLevel(logging.INFO) #\xe5\x85\xb3\xe9\x97\xadjieba\xe8\xbe\x93\xe5\x87\xba\xe4\xbf\xa1\xe6\x81\xaf\r\n\r\ndef maskNLLLoss(inp, target, mask):\r\n    \'\'\'\r\n    inp: shape [batch_size,voc_length]\r\n    target: shape [batch_size] \xe7\xbb\x8f\xe8\xbf\x87view ==> [batch_size, 1] \xe8\xbf\x99\xe6\xa0\xb7\xe5\xb0\xb1\xe5\x92\x8cinp\xe7\xbb\xb4\xe6\x95\xb0\xe7\x9b\xb8\xe5\x90\x8c\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x94\xa8gather\r\n        target\xe4\xbd\x9c\xe4\xb8\xba\xe7\xb4\xa2\xe5\xbc\x95,\xe5\x9c\xa8dim=1\xe4\xb8\x8a\xe7\xb4\xa2\xe5\xbc\x95inp\xe7\x9a\x84\xe5\x80\xbc,\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe5\x90\x8ctarget [batch_size, 1]\r\n        \xe7\x84\xb6\xe5\x90\x8e\xe5\x8e\x8b\xe7\xbc\xa9\xe7\xbb\xb4\xe5\xba\xa6,\xe5\xbe\x97\xe5\x88\xb0[batch_size], \xe5\x8f\x96\xe8\xb4\x9f\xe5\xaf\xb9\xe6\x95\xb0\xe5\x90\x8e\r\n        \xe9\x80\x89\xe6\x8b\xa9\xe9\x82\xa3\xe4\xba\x9b\xe5\x80\xbc\xe4\xb8\xba1\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97loss, \xe5\xb9\xb6\xe6\xb1\x82\xe5\xb9\xb3\xe5\x9d\x87\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0loss\r\n        \xe6\x95\x85loss\xe5\xae\x9e\xe9\x99\x85\xe6\x98\xafbatch_size\xe9\x82\xa3\xe5\x88\x97\xe7\x9a\x84\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x8c\xe8\xa1\xa8\xe7\xa4\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe5\x9c\xa8\xe6\x9f\x90\xe4\xb8\xaa\xe4\xbd\x8d\xe7\xbd\xae(t)\xe4\xb8\x8a\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\r\n        \xe6\x95\x85nTotal\xe8\xa1\xa8\xe7\xa4\xbanTotal\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe5\x9c\xa8\xe6\x9f\x90\xe4\xb8\xaa\xe4\xbd\x8d\xe7\xbd\xae\xe4\xb8\x8a\xe6\x9c\x89\xe5\x80\xbc\r\n    mask: shape [batch_size]\r\n    loss: \xe5\xb9\xb3\xe5\x9d\x87\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe5\x9c\xa8t\xe4\xbd\x8d\xe7\xbd\xae\xe4\xb8\x8a\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\r\n    \'\'\'\r\n    nTotal = mask.sum() #padding\xe6\x98\xaf0\xef\xbc\x8c\xe9\x9d\x9epadding\xe6\x98\xaf1\xef\xbc\x8c\xe5\x9b\xa0\xe6\xad\xa4sum\xe5\xb0\xb1\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xbe\x97\xe5\x88\xb0\xe8\xaf\x8d\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\r\n    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\r\n    loss = crossEntropy.masked_select(mask).mean()\r\n    return loss, nTotal.item()\r\n\r\ndef train_by_batch(sos, opt, data, encoder_optimizer, decoder_optimizer, encoder, decoder):\r\n    #\xe6\xb8\x85\xe7\xa9\xba\xe6\xa2\xaf\xe5\xba\xa6\r\n\r\n    encoder_optimizer.zero_grad()\r\n    decoder_optimizer.zero_grad()\r\n\r\n    #\xe5\xa4\x84\xe7\x90\x86\xe4\xb8\x80\xe4\xb8\xaabatch\xe6\x95\xb0\xe6\x8d\xae\r\n    inputs, targets, mask, input_lengths, max_target_length, indexes = data\r\n    inputs = inputs.to(opt.device)\r\n    targets = targets.to(opt.device)\r\n    mask = mask.to(opt.device)\r\n    input_lengths =  input_lengths.to(opt.device)\r\n\r\n\r\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x8f\x98\xe9\x87\x8f\r\n    loss = 0\r\n    print_losses = []\r\n    n_totals = 0\r\n\r\n    #forward\xe8\xae\xa1\xe7\xae\x97\r\n    \'\'\'\r\n    inputs: shape [max_seq_len, batch_size]\r\n    input_lengths: shape [batch_size]\r\n    encoder_outputs: shape [max_seq_len, batch_size, hidden_size]\r\n    encoder_hidden: shape [num_layers*num_directions, batch_size, hidden_size]\r\n    decoder_input: shape [1, batch_size]\r\n    decoder_hidden: decoder\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8bhidden\xe8\xbe\x93\xe5\x85\xa5,\xe6\x98\xafencoder_hidden\xe5\x8f\x96\xe6\xad\xa3\xe6\x96\xb9\xe5\x90\x91\r\n    \'\'\'\r\n    encoder_outputs, encoder_hidden = encoder(inputs, input_lengths)\r\n    decoder_input = torch.LongTensor([[sos for _ in range(opt.batch_size)]])\r\n    decoder_input = decoder_input.to(opt.device)\r\n    decoder_hidden = encoder_hidden[:decoder.num_layers]\r\n\r\n    # \xe7\xa1\xae\xe5\xae\x9a\xe6\x98\xaf\xe5\x90\xa6teacher forcing\r\n    use_teacher_forcing = True if random.random() < opt.teacher_forcing_ratio else False\r\n\r\n    \'\'\'\r\n    \xe4\xb8\x80\xe6\xac\xa1\xe5\xa4\x84\xe7\x90\x86\xe4\xb8\x80\xe4\xb8\xaa\xe6\x97\xb6\xe5\x88\xbb\xef\xbc\x8c\xe5\x8d\xb3\xe4\xb8\x80\xe4\xb8\xaa\xe5\xad\x97\xe7\xac\xa6\r\n    decoder_output: [batch_size, voc_length]\r\n    decoder_hidden: [decoder_num_layers, batch_size, hidden_size]\r\n    \xe5\xa6\x82\xe6\x9e\x9c\xe4\xbd\xbf\xe7\x94\xa8teacher_forcing,\xe4\xb8\x8b\xe4\xb8\x80\xe4\xb8\xaa\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xaf\xe5\xbd\x93\xe5\x89\x8d\xe6\xad\xa3\xe7\xa1\xae\xe7\xad\x94\xe6\xa1\x88\xef\xbc\x8c\xe5\x8d\xb3\r\n    targets[t] shape: [batch_size] ==> view\xe5\x90\x8e [1, batch_size] \xe4\xbd\x9c\xe4\xb8\xbadecoder_input\r\n    \'\'\'\r\n    if use_teacher_forcing:\r\n        for t in range(max_target_length):\r\n            decoder_output, decoder_hidden = decoder(\r\n                decoder_input, decoder_hidden, encoder_outputs\r\n            )\r\n            decoder_input = targets[t].view(1, -1)\r\n            \r\n            \r\n            # \xe8\xae\xa1\xe7\xae\x97\xe7\xb4\xaf\xe8\xae\xa1\xe7\x9a\x84loss\r\n            \'\'\'\r\n            \xe6\xaf\x8f\xe6\xac\xa1\xe8\xbf\xad\xe4\xbb\xa3,\r\n            targets[t]: \xe4\xb8\x80\xe4\xb8\xaabatch\xe6\x89\x80\xe6\x9c\x89\xe6\xa0\xb7\xe6\x9c\xac\xe6\x8c\x87\xe5\xae\x9a\xe4\xbd\x8d\xe7\xbd\xae(t\xe4\xbd\x8d\xe7\xbd\xae)\xe4\xb8\x8a\xe7\x9a\x84\xe5\x80\xbc\xe7\xbb\x84\xe6\x88\x90\xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8cshape [batch_size]\r\n            mask[t]: \xe4\xb8\x80\xe4\xb8\xaabatch\xe6\x89\x80\xe6\x9c\x89\xe6\xa0\xb7\xe6\x9c\xac\xe6\x8c\x87\xe5\xae\x9a\xe4\xbd\x8d\xe7\xbd\xae(t\xe4\xbd\x8d\xe7\xbd\xae)\xe4\xb8\x8a\xe7\x9a\x84\xe5\x80\xbc\xe7\xbb\x84\xe6\x88\x90\xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f, \xe5\x80\xbc\xe4\xb8\xba1\xe8\xa1\xa8\xe7\xa4\xba\xe6\xad\xa4\xe5\xa4\x84\xe6\x9c\xaapadding\r\n            decoder_output: [batch_size, voc_length]\r\n            \'\'\'\r\n            mask_loss, nTotal = maskNLLLoss(decoder_output, targets[t], mask[t])\r\n            mask_loss = mask_loss.to(opt.device)\r\n            loss += mask_loss\r\n            \'\'\'\r\n            \xe8\xbf\x99\xe9\x87\x8closs\xe5\x9c\xa8seq_len\xe6\x96\xb9\xe5\x90\x91\xe8\xbf\xad\xe4\xbb\xa3\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xb4\xaf\xe5\x8a\xa0, \xe6\x9c\x80\xe7\xbb\x88\xe5\xbe\x97\xe5\x88\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe5\x9c\xa8\xe6\xaf\x8f\xe4\xb8\xaa\xe4\xbd\x8d\xe7\xbd\xae\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x9d\x87\xe5\x80\xbc\xe4\xb9\x8b\xe5\x92\x8c\r\n            \xe6\x80\xbb\xe7\xbb\x93:  mask_loss\xe5\x9c\xa8batch_size\xe6\x96\xb9\xe5\x90\x91\xe7\xb4\xaf\xe5\x8a\xa0,\xe7\x84\xb6\xe5\x90\x8e\xe6\xb1\x82\xe5\x9d\x87\xe5\x80\xbc,loss\xe5\x9c\xa8seq_len\xe6\x96\xb9\xe5\x90\x91\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xb4\xaf\xe5\x8a\xa0\r\n            \xe5\x8d\xb3: \xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0: \xe5\x85\x88\xe8\xae\xa1\xe7\xae\x97\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\xa5\xe5\xad\x90\xe5\x9c\xa8\xe6\xaf\x8f\xe4\xb8\xaa\xe4\xbd\x8d\xe7\xbd\xae\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe6\x80\xbb\xe5\x92\x8c,\xe5\x86\x8d\xe9\x99\xa4batch_size\r\n            \xe8\xbf\x99\xe9\x87\x8c\xe7\x9a\x84loss\xe5\x8f\x98\xe9\x87\x8f\xe7\x94\xa8\xe4\xba\x8e\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe6\x8d\x9f\xe5\xa4\xb1\r\n            \'\'\'\r\n            print_losses.append(mask_loss.item() * nTotal)\r\n            n_totals += nTotal\r\n    else:\r\n        for t in range(max_target_length):\r\n            decoder_output, decoder_hidden = decoder(\r\n                decoder_input, decoder_hidden, encoder_outputs\r\n            )\r\n            # \xe4\xb8\x8d\xe6\x98\xafteacher forcing: \xe4\xb8\x8b\xe4\xb8\x80\xe4\xb8\xaa\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xaf\xe5\xbd\x93\xe5\x89\x8d\xe6\xa8\xa1\xe5\x9e\x8b\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa6\x82\xe7\x8e\x87\xe6\x9c\x80\xe9\xab\x98\xe7\x9a\x84\xe5\x80\xbc\r\n            _, topi = decoder_output.topk(1)\r\n            decoder_input = torch.LongTensor([[topi[i][0] for i in range(opt.batch_size)]])\r\n            decoder_input = decoder_input.to(opt.device)\r\n            # \xe8\xae\xa1\xe7\xae\x97\xe7\xb4\xaf\xe8\xae\xa1\xe7\x9a\x84loss\r\n            mask_loss, nTotal = maskNLLLoss(decoder_output, targets[t], mask[t])\r\n            loss += mask_loss\r\n            print_losses.append(mask_loss.item() * nTotal)\r\n            n_totals += nTotal\r\n\r\n    #\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\r\n    loss.backward()\r\n\r\n    # \xe5\xaf\xb9encoder\xe5\x92\x8cdecoder\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe8\xa3\x81\xe5\x89\xaa\r\n    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), opt.clip)\r\n    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), opt.clip)\r\n\r\n    #\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\r\n    encoder_optimizer.step()\r\n    decoder_optimizer.step()\r\n    #\xe8\xbf\x99\xe9\x87\x8c\xe6\x98\xafbatch\xe4\xb8\xad\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbd\x8d\xe7\xbd\xae(\xe8\xa7\x86\xe4\xbd\x9c\xe4\xba\x8c\xe7\xbb\xb4\xe7\x9f\xa9\xe9\x98\xb5\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa0\xbc\xe5\xad\x90)\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe6\x8d\x9f\xe5\xa4\xb1\r\n    return sum(print_losses) / n_totals \r\n\r\ndef train(**kwargs):\r\n\r\n    opt = Config()\r\n    for k, v in kwargs.items(): #\xe8\xae\xbe\xe7\xbd\xae\xe5\x8f\x82\xe6\x95\xb0\r\n        setattr(opt, k, v)   \r\n\r\n    # \xe6\x95\xb0\xe6\x8d\xae\r\n    dataloader = get_dataloader(opt) \r\n    _data = dataloader.dataset._data\r\n    word2ix = _data[\'word2ix\']\r\n    sos = word2ix.get(_data.get(\'sos\'))\r\n    voc_length = len(word2ix)\r\n        \r\n    #\xe5\xae\x9a\xe4\xb9\x89\xe6\xa8\xa1\xe5\x9e\x8b\r\n    encoder = EncoderRNN(opt, voc_length)\r\n    decoder = LuongAttnDecoderRNN(opt, voc_length)\r\n\r\n    #\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x96\xad\xe7\x82\xb9,\xe4\xbb\x8e\xe4\xb8\x8a\xe6\xac\xa1\xe7\xbb\x93\xe6\x9d\x9f\xe5\x9c\xb0\xe6\x96\xb9\xe5\xbc\x80\xe5\xa7\x8b\r\n    if opt.model_ckpt:\r\n        checkpoint = torch.load(opt.model_ckpt)\r\n        encoder.load_state_dict(checkpoint[\'en\'])\r\n        decoder.load_state_dict(checkpoint[\'de\'])\r\n        \r\n    \r\n    #\xe5\x88\x87\xe6\x8d\xa2\xe6\xa8\xa1\xe5\xbc\x8f\r\n    encoder = encoder.to(opt.device)\r\n    decoder = decoder.to(opt.device)\r\n    encoder.train()\r\n    decoder.train()\r\n\r\n\r\n    #\xe5\xae\x9a\xe4\xb9\x89\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8(\xe6\xb3\xa8\xe6\x84\x8f\xe4\xb8\x8eencoder.to(device)\xe5\x89\x8d\xe5\x90\x8e\xe4\xb8\x8d\xe8\xa6\x81\xe5\x8f\x8d)\r\n    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=opt.learning_rate)\r\n    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=opt.learning_rate * opt.decoder_learning_ratio)\r\n    if opt.model_ckpt:\r\n        encoder_optimizer.load_state_dict(checkpoint[\'en_opt\'])\r\n        decoder_optimizer.load_state_dict(checkpoint[\'de_opt\']) \r\n\r\n    #\xe5\xae\x9a\xe4\xb9\x89\xe6\x89\x93\xe5\x8d\xb0loss\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\r\n    print_loss = 0\r\n    \r\n    for epoch in range(opt.epoch):\r\n        for ii, data in enumerate(dataloader):\r\n            #\xe5\x8f\x96\xe4\xb8\x80\xe4\xb8\xaabatch\xe8\xae\xad\xe7\xbb\x83\r\n            loss = train_by_batch(sos, opt, data, encoder_optimizer, decoder_optimizer, encoder, decoder)\r\n            print_loss += loss\r\n            #\xe6\x89\x93\xe5\x8d\xb0\xe6\x8d\x9f\xe5\xa4\xb1   \r\n            if ii % opt.print_every == 0:\r\n                print_loss_avg = print_loss / opt.print_every\r\n                print(""Epoch: {}; Epoch Percent complete: {:.1f}%; Average loss: {:.4f}""\r\n                .format(epoch, epoch / opt.epoch * 100, print_loss_avg))\r\n                print_loss = 0\r\n                \r\n        # \xe4\xbf\x9d\xe5\xad\x98checkpoint\r\n        if epoch % opt.save_every == 0:\r\n            checkpoint_path = \'{prefix}_{time}\'.format(prefix=opt.prefix,\r\n                                        time=time.strftime(\'%m%d_%H%M\'))\r\n            torch.save({\r\n                \'en\': encoder.state_dict(),\r\n                \'de\': decoder.state_dict(),\r\n                \'en_opt\': encoder_optimizer.state_dict(),\r\n                \'de_opt\': decoder_optimizer.state_dict(),\r\n            }, checkpoint_path)\r\n\r\ndef generate(input_seq, searcher, sos, eos, opt):\r\n    #input_seq: \xe5\xb7\xb2\xe5\x88\x86\xe8\xaf\x8d\xe4\xb8\x94\xe8\xbd\xac\xe4\xb8\xba\xe7\xb4\xa2\xe5\xbc\x95\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97\r\n    #input_batch: shape: [1, seq_len] ==> [seq_len,1] (\xe5\x8d\xb3batch_size=1)\r\n    input_batch = [input_seq]\r\n    input_lengths = torch.tensor([len(seq) for seq in input_batch])\r\n    input_batch = torch.LongTensor([input_seq]).transpose(0,1)\r\n    input_batch = input_batch.to(opt.device)\r\n    input_lengths = input_lengths.to(opt.device)\r\n    tokens, scores = searcher(sos, eos, input_batch, input_lengths, opt.max_generate_length, opt.device)\r\n    return tokens\r\n\r\ndef eval(**kwargs):\r\n\r\n    opt = Config()\r\n    for k, v in kwargs.items(): #\xe8\xae\xbe\xe7\xbd\xae\xe5\x8f\x82\xe6\x95\xb0\r\n        setattr(opt, k, v)   \r\n\r\n\r\n    # \xe6\x95\xb0\xe6\x8d\xae\r\n    dataloader = get_dataloader(opt) \r\n    _data = dataloader.dataset._data\r\n    word2ix,ix2word = _data[\'word2ix\'], _data[\'ix2word\']\r\n    sos = word2ix.get(_data.get(\'sos\'))\r\n    eos = word2ix.get(_data.get(\'eos\'))\r\n    unknown = word2ix.get(_data.get(\'unknown\'))\r\n    voc_length = len(word2ix)\r\n\r\n    #\xe5\xae\x9a\xe4\xb9\x89\xe6\xa8\xa1\xe5\x9e\x8b\r\n    encoder = EncoderRNN(opt, voc_length)\r\n    decoder = LuongAttnDecoderRNN(opt, voc_length)\r\n\r\n    #\xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\r\n    if opt.model_ckpt == None:\r\n        raise ValueError(\'model_ckpt is None.\')\r\n        return False\r\n    checkpoint = torch.load(opt.model_ckpt, map_location=lambda s, l: s)\r\n    encoder.load_state_dict(checkpoint[\'en\'])\r\n    decoder.load_state_dict(checkpoint[\'de\'])\r\n\r\n    with torch.no_grad():\r\n        #\xe5\x88\x87\xe6\x8d\xa2\xe6\xa8\xa1\xe5\xbc\x8f\r\n        encoder = encoder.to(opt.device)\r\n        decoder = decoder.to(opt.device)\r\n        encoder.eval()\r\n        decoder.eval()\r\n        #\xe5\xae\x9a\xe4\xb9\x89seracher\r\n        searcher = GreedySearchDecoder(encoder, decoder)\r\n\r\n        while(1):\r\n            input_sentence = input(\'> \')\r\n            if input_sentence == \'q\' or input_sentence == \'quit\': break\r\n            cop = re.compile(""[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]"") #\xe5\x88\x86\xe8\xaf\x8d\xe5\xa4\x84\xe7\x90\x86\xe6\xad\xa3\xe5\x88\x99\r\n            input_seq = jieba.lcut(cop.sub("""",input_sentence)) #\xe5\x88\x86\xe8\xaf\x8d\xe5\xba\x8f\xe5\x88\x97\r\n            input_seq = input_seq[:opt.max_input_length] + [\'</EOS>\']\r\n            input_seq = [word2ix.get(word, unknown) for word in input_seq]\r\n            tokens = generate(input_seq, searcher, sos, eos, opt)\r\n            output_words = \'\'.join([ix2word[token.item()] for token in tokens])\r\n            print(\'BOT: \', output_words)\r\n\r\ndef test(opt):\r\n\r\n    # \xe6\x95\xb0\xe6\x8d\xae\r\n    dataloader = get_dataloader(opt) \r\n    _data = dataloader.dataset._data\r\n    word2ix,ix2word = _data[\'word2ix\'], _data[\'ix2word\']\r\n    sos = word2ix.get(_data.get(\'sos\'))\r\n    eos = word2ix.get(_data.get(\'eos\'))\r\n    unknown = word2ix.get(_data.get(\'unknown\'))\r\n    voc_length = len(word2ix)\r\n\r\n    #\xe5\xae\x9a\xe4\xb9\x89\xe6\xa8\xa1\xe5\x9e\x8b\r\n    encoder = EncoderRNN(opt, voc_length)\r\n    decoder = LuongAttnDecoderRNN(opt, voc_length)\r\n\r\n    #\xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\r\n    if opt.model_ckpt == None:\r\n        raise ValueError(\'model_ckpt is None.\')\r\n        return False\r\n    checkpoint = torch.load(opt.model_ckpt, map_location=lambda s, l: s)\r\n    encoder.load_state_dict(checkpoint[\'en\'])\r\n    decoder.load_state_dict(checkpoint[\'de\'])\r\n\r\n    with torch.no_grad():\r\n        #\xe5\x88\x87\xe6\x8d\xa2\xe6\xa8\xa1\xe5\xbc\x8f\r\n        encoder = encoder.to(opt.device)\r\n        decoder = decoder.to(opt.device)\r\n        encoder.eval()\r\n        decoder.eval()\r\n        #\xe5\xae\x9a\xe4\xb9\x89seracher\r\n        searcher = GreedySearchDecoder(encoder, decoder)\r\n        return searcher, sos, eos, unknown, word2ix, ix2word\r\n\r\ndef output_answer(input_sentence, searcher, sos, eos, unknown, opt, word2ix, ix2word):\r\n    cop = re.compile(""[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]"") #\xe5\x88\x86\xe8\xaf\x8d\xe5\xa4\x84\xe7\x90\x86\xe6\xad\xa3\xe5\x88\x99\r\n    input_seq = jieba.lcut(cop.sub("""",input_sentence)) #\xe5\x88\x86\xe8\xaf\x8d\xe5\xba\x8f\xe5\x88\x97\r\n    input_seq = input_seq[:opt.max_input_length] + [\'</EOS>\']\r\n    input_seq = [word2ix.get(word, unknown) for word in input_seq]\r\n    tokens = generate(input_seq, searcher, sos, eos, opt)\r\n    output_words = \'\'.join([ix2word[token.item()] for token in tokens if token.item() != eos])\r\n    return output_words\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    import fire\r\n    fire.Fire()\r\n        '"
QA_data/QA_test.py,0,"b'# -*- coding: UTF-8 -*-\r\n\r\nimport sqlite3\r\nimport jieba\r\nimport logging\r\njieba.setLogLevel(logging.INFO) #\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\x8d\xe8\xbe\x93\xe5\x87\xba\xe4\xbf\xa1\xe6\x81\xaf\r\n\r\nconn = sqlite3.connect(\'./QA_data/QA.db\')\r\n\r\ncursor = conn.cursor()\r\nstop_words = []\r\nwith open(\'./QA_data/stop_words.txt\', encoding=\'gbk\') as f:\r\n    for line in f.readlines():\r\n        stop_words.append(line.strip(\'\\n\'))\r\n\r\ndef match(input_question):\r\n    res = []\r\n    cnt = {}\r\n    question = list(jieba.cut(input_question, cut_all=False)) #\xe5\xaf\xb9\xe6\x9f\xa5\xe8\xaf\xa2\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe8\xaf\x8d\r\n    for word in reversed(question):  #\xe5\x8e\xbb\xe9\x99\xa4\xe5\x81\x9c\xe7\x94\xa8\xe8\xaf\x8d\r\n        if word in stop_words:\r\n            question.remove(word)\r\n    for tag in question: #\xe6\x8c\x89\xe7\x85\xa7\xe6\xaf\x8f\xe4\xb8\xaatag\xef\xbc\x8c\xe5\xbe\xaa\xe7\x8e\xaf\xe6\x9e\x84\xe9\x80\xa0\xe6\x9f\xa5\xe8\xaf\xa2\xe8\xaf\xad\xe5\x8f\xa5\r\n        keyword = ""\'%"" + tag + ""%\'""\r\n        result = cursor.execute(""select * from QA where tag like "" + keyword)\r\n        for row in result:\r\n            if row[0] not in cnt.keys():\r\n                cnt[row[0]]  = 0\r\n            cnt[row[0]] += 1 #\xe7\xbb\x9f\xe8\xae\xa1\xe8\xae\xb0\xe5\xbd\x95\xe5\x87\xba\xe7\x8e\xb0\xe7\x9a\x84\xe6\xac\xa1\xe6\x95\xb0\r\n    try:\r\n        res_id = sorted(cnt.items(), key=lambda d:d[1],reverse=True)[0][0] #\xe8\xbf\x94\xe5\x9b\x9e\xe5\x87\xba\xe7\x8e\xb0\xe6\xac\xa1\xe6\x95\xb0\xe6\x9c\x80\xe9\xab\x98\xe7\x9a\x84\xe8\xae\xb0\xe5\xbd\x95\xe7\x9a\x84id\r\n    except:\r\n        return tuple() #\xe8\x8b\xa5\xe6\x9f\xa5\xe8\xaf\xa2\xe4\xb8\x8d\xe5\x87\xba\xe5\x88\x99\xe8\xbf\x94\xe5\x9b\x9e\xe7\xa9\xba\r\n    cursor.execute(""select * from QA where id= "" + str(res_id))\r\n    res = cursor.fetchone()\r\n    if type(res) == type(tuple()):\r\n        return res #\xe8\xbf\x94\xe5\x9b\x9e\xe5\x85\x83\xe7\xbb\x84\xe7\xb1\xbb\xe5\x9e\x8b(id, question, answer, tag)\r\n    else:\r\n        return tuple() #\xe8\x8b\xa5\xe6\x9f\xa5\xe8\xaf\xa2\xe4\xb8\x8d\xe5\x87\xba\xe5\x88\x99\xe8\xbf\x94\xe5\x9b\x9e\xe7\xa9\xba\r\n\r\n'"
QA_data/__init__.py,0,b''
utils/__init__.py,0,b''
utils/beamsearch.py,0,b''
utils/greedysearch.py,8,"b'# -*- coding: utf-8 -*- \r\n\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\n\r\nclass GreedySearchDecoder(nn.Module):\r\n    def __init__(self, encoder, decoder):\r\n        super(GreedySearchDecoder, self).__init__()\r\n        self.encoder = encoder\r\n        self.decoder = decoder\r\n\r\n    def forward(self, sos, eos, input_seq, input_length, max_length, device):\r\n\r\n        # Encoder\xe7\x9a\x84Forward\xe8\xae\xa1\xe7\xae\x97 \r\n        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\r\n        # \xe6\x8a\x8aEncoder\xe6\x9c\x80\xe5\x90\x8e\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84\xe9\x9a\x90\xe7\x8a\xb6\xe6\x80\x81\xe4\xbd\x9c\xe4\xb8\xbaDecoder\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe5\x80\xbc\r\n        decoder_hidden = encoder_hidden[:self.decoder.num_layers]\r\n        # \xe5\x9b\xa0\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe9\x83\xbd\xe6\x98\xaf\xe8\xa6\x81\xe6\xb1\x82(time,batch)\xef\xbc\x8c\xe5\x9b\xa0\xe6\xad\xa4\xe5\x8d\xb3\xe4\xbd\xbf\xe5\x8f\xaa\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe4\xb9\x9f\xe8\xa6\x81\xe5\x81\x9a\xe5\x87\xba\xe4\xba\x8c\xe7\xbb\xb4\xe7\x9a\x84\xe3\x80\x82\r\n        # Decoder\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xafSOS\r\n        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * sos\r\n        # \xe7\x94\xa8\xe4\xba\x8e\xe4\xbf\x9d\xe5\xad\x98\xe8\xa7\xa3\xe7\xa0\x81\xe7\xbb\x93\xe6\x9e\x9c\xe7\x9a\x84tensor\r\n        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\r\n        all_scores = torch.zeros([0], device=device)\r\n        # \xe5\xbe\xaa\xe7\x8e\xaf\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\xaa\xe4\xbd\xbf\xe7\x94\xa8\xe9\x95\xbf\xe5\xba\xa6\xe9\x99\x90\xe5\x88\xb6\xef\xbc\x8c\xe5\x90\x8e\xe9\x9d\xa2\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe6\x8a\x8aEOS\xe5\x8e\xbb\xe6\x8e\x89\xe4\xba\x86\xe3\x80\x82\r\n        for _ in range(max_length):\r\n            # Decoder forward\xe4\xb8\x80\xe6\xad\xa5\r\n            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, \r\n\t\t\t\t\t\t\t\tencoder_outputs)\r\n            # decoder_outputs\xe6\x98\xaf(batch=1, vob_size)\r\n            # \xe4\xbd\xbf\xe7\x94\xa8max\xe8\xbf\x94\xe5\x9b\x9e\xe6\xa6\x82\xe7\x8e\x87\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe8\xaf\x8d\xe5\x92\x8c\xe5\xbe\x97\xe5\x88\x86\r\n            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\r\n            # \xe6\x8a\x8a\xe8\xa7\xa3\xe7\xa0\x81\xe7\xbb\x93\xe6\x9e\x9c\xe4\xbf\x9d\xe5\xad\x98\xe5\x88\xb0all_tokens\xe5\x92\x8call_scores\xe9\x87\x8c\r\n            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\r\n            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\r\n            # decoder_input\xe6\x98\xaf\xe5\xbd\x93\xe5\x89\x8d\xe6\x97\xb6\xe5\x88\xbb\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe8\xaf\x8d\xe7\x9a\x84ID\xef\xbc\x8c\xe8\xbf\x99\xe6\x98\xaf\xe4\xb8\xaa\xe4\xb8\x80\xe7\xbb\xb4\xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xbamax\xe4\xbc\x9a\xe5\x87\x8f\xe5\xb0\x91\xe4\xb8\x80\xe7\xbb\xb4\xe3\x80\x82\r\n            # \xe4\xbd\x86\xe6\x98\xafdecoder\xe8\xa6\x81\xe6\xb1\x82\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaabatch\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x8c\xe5\x9b\xa0\xe6\xad\xa4\xe7\x94\xa8unsqueeze\xe5\xa2\x9e\xe5\x8a\xa0batch\xe7\xbb\xb4\xe5\xba\xa6\xe3\x80\x82\r\n            if decoder_input.item() == eos:\r\n                break\r\n            decoder_input = torch.unsqueeze(decoder_input, 0)\r\n            \r\n        # \xe8\xbf\x94\xe5\x9b\x9e\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe8\xaf\x8d\xe5\x92\x8c\xe5\xbe\x97\xe5\x88\x86\xe3\x80\x82\r\n        return all_tokens, all_scores'"
