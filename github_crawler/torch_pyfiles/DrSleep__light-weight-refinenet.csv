file_path,api_count,code
models/__init__.py,0,b''
models/mobilenet.py,1,"b'""""""RefineNet-LightWeight\n\nRefineNet-LigthWeight PyTorch for non-commercial purposes\n\nCopyright (c) 2018, Vladimir Nekrasov (vladimir.nekrasov@adelaide.edu.au)\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom utils.helpers import maybe_download\nfrom utils.layer_factory import batchnorm, conv1x1, conv3x3, convbnrelu, CRPBlock\n\n\ndata_info = {21: ""VOC""}\n\nmodels_urls = {\n    ""mbv2_voc"": ""https://cloudstor.aarnet.edu.au/plus/s/PsEL9uEuxOtIxJV/download""\n}\n\n\nclass InvertedResidualBlock(nn.Module):\n    """"""Inverted Residual Block from https://arxiv.org/abs/1801.04381""""""\n\n    def __init__(self, in_planes, out_planes, expansion_factor, stride=1):\n        super(InvertedResidualBlock, self).__init__()\n        intermed_planes = in_planes * expansion_factor\n        self.residual = (in_planes == out_planes) and (stride == 1)\n        self.output = nn.Sequential(\n            convbnrelu(in_planes, intermed_planes, 1),\n            convbnrelu(\n                intermed_planes,\n                intermed_planes,\n                3,\n                stride=stride,\n                groups=intermed_planes,\n            ),\n            convbnrelu(intermed_planes, out_planes, 1, act=False),\n        )\n\n    def forward(self, x):\n        residual = x\n        out = self.output(x)\n        if self.residual:\n            return out + residual\n        else:\n            return out\n\n\nclass MBv2(nn.Module):\n    """"""Net Definition""""""\n\n    mobilenet_config = [\n        [1, 16, 1, 1],  # expansion rate, output channels, number of repeats, stride\n        [6, 24, 2, 2],\n        [6, 32, 3, 2],\n        [6, 64, 4, 2],\n        [6, 96, 3, 1],\n        [6, 160, 3, 2],\n        [6, 320, 1, 1],\n    ]\n    in_planes = 32  # number of input channels\n    num_layers = len(mobilenet_config)\n\n    def __init__(self, num_classes):\n        super(MBv2, self).__init__()\n\n        self.layer1 = convbnrelu(3, self.in_planes, kernel_size=3, stride=2)\n        c_layer = 2\n        for t, c, n, s in self.mobilenet_config:\n            layers = []\n            for idx in range(n):\n                layers.append(\n                    InvertedResidualBlock(\n                        self.in_planes,\n                        c,\n                        expansion_factor=t,\n                        stride=s if idx == 0 else 1,\n                    )\n                )\n                self.in_planes = c\n            setattr(self, ""layer{}"".format(c_layer), nn.Sequential(*layers))\n            c_layer += 1\n\n        ## Light-Weight RefineNet ##\n        self.conv8 = conv1x1(320, 256, bias=False)\n        self.conv7 = conv1x1(160, 256, bias=False)\n        self.conv6 = conv1x1(96, 256, bias=False)\n        self.conv5 = conv1x1(64, 256, bias=False)\n        self.conv4 = conv1x1(32, 256, bias=False)\n        self.conv3 = conv1x1(24, 256, bias=False)\n        self.crp4 = self._make_crp(256, 256, 4)\n        self.crp3 = self._make_crp(256, 256, 4)\n        self.crp2 = self._make_crp(256, 256, 4)\n        self.crp1 = self._make_crp(256, 256, 4)\n\n        self.conv_adapt4 = conv1x1(256, 256, bias=False)\n        self.conv_adapt3 = conv1x1(256, 256, bias=False)\n        self.conv_adapt2 = conv1x1(256, 256, bias=False)\n\n        self.segm = conv3x3(256, num_classes, bias=True)\n        self.relu = nn.ReLU6(inplace=True)\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)  # x / 2\n        l3 = self.layer3(x)  # 24, x / 4\n        l4 = self.layer4(l3)  # 32, x / 8\n        l5 = self.layer5(l4)  # 64, x / 16\n        l6 = self.layer6(l5)  # 96, x / 16\n        l7 = self.layer7(l6)  # 160, x / 32\n        l8 = self.layer8(l7)  # 320, x / 32\n        l8 = self.conv8(l8)\n        l7 = self.conv7(l7)\n        l7 = self.relu(l8 + l7)\n        l7 = self.crp4(l7)\n        l7 = self.conv_adapt4(l7)\n        l7 = nn.Upsample(size=l6.size()[2:], mode=""bilinear"", align_corners=True)(l7)\n\n        l6 = self.conv6(l6)\n        l5 = self.conv5(l5)\n        l5 = self.relu(l5 + l6 + l7)\n        l5 = self.crp3(l5)\n        l5 = self.conv_adapt3(l5)\n        l5 = nn.Upsample(size=l4.size()[2:], mode=""bilinear"", align_corners=True)(l5)\n\n        l4 = self.conv4(l4)\n        l4 = self.relu(l5 + l4)\n        l4 = self.crp2(l4)\n        l4 = self.conv_adapt2(l4)\n        l4 = nn.Upsample(size=l3.size()[2:], mode=""bilinear"", align_corners=True)(l4)\n\n        l3 = self.conv3(l3)\n        l3 = self.relu(l3 + l4)\n        l3 = self.crp1(l3)\n\n        out_segm = self.segm(l3)\n\n        return out_segm\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight.data.normal_(0, 0.01)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_crp(self, in_planes, out_planes, stages):\n        layers = [CRPBlock(in_planes, out_planes, stages)]\n        return nn.Sequential(*layers)\n\n\ndef mbv2(num_classes, pretrained=True, **kwargs):\n    """"""Constructs the network.\n\n    Args:\n        num_classes (int): the number of classes for the segmentation head to output.\n\n    """"""\n    model = MBv2(num_classes, **kwargs)\n    if pretrained:\n        dataset = data_info.get(num_classes, None)\n        if dataset:\n            bname = ""mbv2_"" + dataset.lower()\n            key = ""rf_lw"" + bname\n            url = models_urls[bname]\n            model.load_state_dict(maybe_download(key, url), strict=False)\n    return model\n'"
models/resnet.py,5,"b'""""""RefineNet-LightWeight\n\nRefineNet-LigthWeight PyTorch for non-commercial purposes\n\nCopyright (c) 2018, Vladimir Nekrasov (vladimir.nekrasov@adelaide.edu.au)\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n""""""\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\nimport numpy as np\n\nfrom utils.helpers import maybe_download\nfrom utils.layer_factory import conv1x1, conv3x3, CRPBlock\n\ndata_info = {7: ""Person"", 21: ""VOC"", 40: ""NYU"", 60: ""Context""}\n\nmodels_urls = {\n    ""50_person"": ""https://cloudstor.aarnet.edu.au/plus/s/mLA7NxVSPjNL7Oo/download"",\n    ""101_person"": ""https://cloudstor.aarnet.edu.au/plus/s/f1tGGpwdCnYS3xu/download"",\n    ""152_person"": ""https://cloudstor.aarnet.edu.au/plus/s/Ql64rWqiTvWGAA0/download"",\n    ""50_voc"": ""https://cloudstor.aarnet.edu.au/plus/s/2E1KrdF2Rfc5khB/download"",\n    ""101_voc"": ""https://cloudstor.aarnet.edu.au/plus/s/CPRKWiaCIDRdOwF/download"",\n    ""152_voc"": ""https://cloudstor.aarnet.edu.au/plus/s/2w8bFOd45JtPqbD/download"",\n    ""50_nyu"": ""https://cloudstor.aarnet.edu.au/plus/s/gE8dnQmHr9svpfu/download"",\n    ""101_nyu"": ""https://cloudstor.aarnet.edu.au/plus/s/VnsaSUHNZkuIqeB/download"",\n    ""152_nyu"": ""https://cloudstor.aarnet.edu.au/plus/s/EkPQzB2KtrrDnKf/download"",\n    ""101_context"": ""https://cloudstor.aarnet.edu.au/plus/s/hqmplxWOBbOYYjN/download"",\n    ""152_context"": ""https://cloudstor.aarnet.edu.au/plus/s/O84NszlYlsu00fW/download"",\n    ""50_imagenet"": ""https://download.pytorch.org/models/resnet50-19c8e357.pth"",\n    ""101_imagenet"": ""https://download.pytorch.org/models/resnet101-5d3b4d8f.pth"",\n    ""152_imagenet"": ""https://download.pytorch.org/models/resnet152-b121ed2d.pth"",\n}\n\nstages_suffixes = {0: ""_conv"", 1: ""_conv_relu_varout_dimred""}\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(\n            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n        )\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNetLW(nn.Module):\n    def __init__(self, block, layers, num_classes=21):\n        self.inplanes = 64\n        super(ResNetLW, self).__init__()\n        self.do = nn.Dropout(p=0.5)\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.p_ims1d2_outl1_dimred = conv1x1(2048, 512, bias=False)\n        self.mflow_conv_g1_pool = self._make_crp(512, 512, 4)\n        self.mflow_conv_g1_b3_joint_varout_dimred = conv1x1(512, 256, bias=False)\n        self.p_ims1d2_outl2_dimred = conv1x1(1024, 256, bias=False)\n        self.adapt_stage2_b2_joint_varout_dimred = conv1x1(256, 256, bias=False)\n        self.mflow_conv_g2_pool = self._make_crp(256, 256, 4)\n        self.mflow_conv_g2_b3_joint_varout_dimred = conv1x1(256, 256, bias=False)\n\n        self.p_ims1d2_outl3_dimred = conv1x1(512, 256, bias=False)\n        self.adapt_stage3_b2_joint_varout_dimred = conv1x1(256, 256, bias=False)\n        self.mflow_conv_g3_pool = self._make_crp(256, 256, 4)\n        self.mflow_conv_g3_b3_joint_varout_dimred = conv1x1(256, 256, bias=False)\n\n        self.p_ims1d2_outl4_dimred = conv1x1(256, 256, bias=False)\n        self.adapt_stage4_b2_joint_varout_dimred = conv1x1(256, 256, bias=False)\n        self.mflow_conv_g4_pool = self._make_crp(256, 256, 4)\n\n        self.clf_conv = nn.Conv2d(\n            256, num_classes, kernel_size=3, stride=1, padding=1, bias=True\n        )\n\n    def _make_crp(self, in_planes, out_planes, stages):\n        layers = [CRPBlock(in_planes, out_planes, stages)]\n        return nn.Sequential(*layers)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        l1 = self.layer1(x)\n        l2 = self.layer2(l1)\n        l3 = self.layer3(l2)\n        l4 = self.layer4(l3)\n\n        l4 = self.do(l4)\n        l3 = self.do(l3)\n\n        x4 = self.p_ims1d2_outl1_dimred(l4)\n        x4 = self.relu(x4)\n        x4 = self.mflow_conv_g1_pool(x4)\n        x4 = self.mflow_conv_g1_b3_joint_varout_dimred(x4)\n        x4 = nn.Upsample(size=l3.size()[2:], mode=""bilinear"", align_corners=True)(x4)\n\n        x3 = self.p_ims1d2_outl2_dimred(l3)\n        x3 = self.adapt_stage2_b2_joint_varout_dimred(x3)\n        x3 = x3 + x4\n        x3 = F.relu(x3)\n        x3 = self.mflow_conv_g2_pool(x3)\n        x3 = self.mflow_conv_g2_b3_joint_varout_dimred(x3)\n        x3 = nn.Upsample(size=l2.size()[2:], mode=""bilinear"", align_corners=True)(x3)\n\n        x2 = self.p_ims1d2_outl3_dimred(l2)\n        x2 = self.adapt_stage3_b2_joint_varout_dimred(x2)\n        x2 = x2 + x3\n        x2 = F.relu(x2)\n        x2 = self.mflow_conv_g3_pool(x2)\n        x2 = self.mflow_conv_g3_b3_joint_varout_dimred(x2)\n        x2 = nn.Upsample(size=l1.size()[2:], mode=""bilinear"", align_corners=True)(x2)\n\n        x1 = self.p_ims1d2_outl4_dimred(l1)\n        x1 = self.adapt_stage4_b2_joint_varout_dimred(x1)\n        x1 = x1 + x2\n        x1 = F.relu(x1)\n        x1 = self.mflow_conv_g4_pool(x1)\n\n        out = self.clf_conv(x1)\n        return out\n\n\ndef rf_lw50(num_classes, imagenet=False, pretrained=True, **kwargs):\n    model = ResNetLW(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, **kwargs)\n    if imagenet:\n        key = ""50_imagenet""\n        url = models_urls[key]\n        model.load_state_dict(maybe_download(key, url), strict=False)\n    elif pretrained:\n        dataset = data_info.get(num_classes, None)\n        if dataset:\n            bname = ""50_"" + dataset.lower()\n            key = ""rf_lw"" + bname\n            url = models_urls[bname]\n            model.load_state_dict(maybe_download(key, url), strict=False)\n    return model\n\n\ndef rf_lw101(num_classes, imagenet=False, pretrained=True, **kwargs):\n    model = ResNetLW(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, **kwargs)\n    if imagenet:\n        key = ""101_imagenet""\n        url = models_urls[key]\n        model.load_state_dict(maybe_download(key, url), strict=False)\n    elif pretrained:\n        dataset = data_info.get(num_classes, None)\n        if dataset:\n            bname = ""101_"" + dataset.lower()\n            key = ""rf_lw"" + bname\n            url = models_urls[bname]\n            model.load_state_dict(maybe_download(key, url), strict=False)\n    return model\n\n\ndef rf_lw152(num_classes, imagenet=False, pretrained=True, **kwargs):\n    model = ResNetLW(Bottleneck, [3, 8, 36, 3], num_classes=num_classes, **kwargs)\n    if imagenet:\n        key = ""152_imagenet""\n        url = models_urls[key]\n        model.load_state_dict(maybe_download(key, url), strict=False)\n    elif pretrained:\n        dataset = data_info.get(num_classes, None)\n        if dataset:\n            bname = ""152_"" + dataset.lower()\n            key = ""rf_lw"" + bname\n            url = models_urls[bname]\n            model.load_state_dict(maybe_download(key, url), strict=False)\n    return model\n'"
src/config.py,0,"b'import numpy as np\n\n# DATASET PARAMETERS\nTRAIN_DIR = ""/datasets/nyud/""\nVAL_DIR = TRAIN_DIR\nTRAIN_LIST = [""./data/train.nyu""] * 3\nVAL_LIST = [""./data/val.nyu""] * 3\nSHORTER_SIDE = [350] * 3\nCROP_SIZE = [500] * 3\nNORMALISE_PARAMS = [\n    1.0 / 255,  # SCALE\n    np.array([0.485, 0.456, 0.406]).reshape((1, 1, 3)),  # MEAN\n    np.array([0.229, 0.224, 0.225]).reshape((1, 1, 3)),\n]  # STD\nBATCH_SIZE = [6] * 3\nNUM_WORKERS = 16\nNUM_CLASSES = [40] * 3\nLOW_SCALE = [0.5] * 3\nHIGH_SCALE = [2.0] * 3\nIGNORE_LABEL = 255\n\n# ENCODER PARAMETERS\nENC = ""50""\nENC_PRETRAINED = True  # pre-trained on ImageNet or randomly initialised\n\n# GENERAL\nEVALUATE = False\nFREEZE_BN = [True] * 3\nNUM_SEGM_EPOCHS = [100] * 3\nPRINT_EVERY = 10\nRANDOM_SEED = 42\nSNAPSHOT_DIR = ""./ckpt/""\nCKPT_PATH = ""./ckpt/checkpoint.pth.tar""\nVAL_EVERY = [5] * 3  # how often to record validation scores\n\n# OPTIMISERS\' PARAMETERS\nLR_ENC = [5e-4, 2.5e-4, 1e-4]  # TO FREEZE, PUT 0\nLR_DEC = [5e-3, 2.5e-3, 1e-3]\nMOM_ENC = [0.9] * 3  # TO FREEZE, PUT 0\nMOM_DEC = [0.9] * 3\nWD_ENC = [1e-5] * 3  # TO FREEZE, PUT 0\nWD_DEC = [1e-5] * 3\nOPTIM_DEC = ""sgd""\n'"
src/datasets.py,3,"b'""""""RefineNet-LightWeight\n\nRefineNet-LigthWeight PyTorch for non-commercial purposes\n\nCopyright (c) 2018, Vladimir Nekrasov (vladimir.nekrasov@adelaide.edu.au)\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n""""""\n\nfrom __future__ import print_function, division\n\nimport collections\nimport glob\nimport os\nimport random\nimport warnings\n\nimport cv2\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom skimage import io, transform\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms, utils\n\nwarnings.filterwarnings(""ignore"")\n\n\nclass Pad(object):\n    """"""Pad image and mask to the desired size\n\n    Args:\n      size (int) : minimum length/width\n      img_val (array) : image padding value\n      msk_val (int) : mask padding value\n\n    """"""\n\n    def __init__(self, size, img_val, msk_val):\n        self.size = size\n        self.img_val = img_val\n        self.msk_val = msk_val\n\n    def __call__(self, sample):\n        image, mask = sample[""image""], sample[""mask""]\n        h, w = image.shape[:2]\n        h_pad = int(np.clip(((self.size - h) + 1) // 2, 0, 1e6))\n        w_pad = int(np.clip(((self.size - w) + 1) // 2, 0, 1e6))\n        pad = ((h_pad, h_pad), (w_pad, w_pad))\n        image = np.stack(\n            [\n                np.pad(\n                    image[:, :, c],\n                    pad,\n                    mode=""constant"",\n                    constant_values=self.img_val[c],\n                )\n                for c in range(3)\n            ],\n            axis=2,\n        )\n        mask = np.pad(mask, pad, mode=""constant"", constant_values=self.msk_val)\n        return {""image"": image, ""mask"": mask}\n\n\nclass RandomCrop(object):\n    """"""Crop randomly the image in a sample.\n\n    Args:\n        output_size (tuple or int): Desired output size. If int, square crop\n            is made.\n    """"""\n\n    def __init__(self, crop_size):\n        assert isinstance(crop_size, int)\n        self.crop_size = crop_size\n        if self.crop_size % 2 != 0:\n            self.crop_size -= 1\n\n    def __call__(self, sample):\n        image, mask = sample[""image""], sample[""mask""]\n        h, w = image.shape[:2]\n        new_h = min(h, self.crop_size)\n        new_w = min(w, self.crop_size)\n        top = np.random.randint(0, h - new_h + 1)\n        left = np.random.randint(0, w - new_w + 1)\n        image = image[top : top + new_h, left : left + new_w]\n        mask = mask[top : top + new_h, left : left + new_w]\n        return {""image"": image, ""mask"": mask}\n\n\nclass ResizeShorterScale(object):\n    """"""Resize shorter side to a given value and randomly scale.""""""\n\n    def __init__(self, shorter_side, low_scale, high_scale):\n        assert isinstance(shorter_side, int)\n        self.shorter_side = shorter_side\n        self.low_scale = low_scale\n        self.high_scale = high_scale\n\n    def __call__(self, sample):\n        image, mask = sample[""image""], sample[""mask""]\n        min_side = min(image.shape[:2])\n        scale = np.random.uniform(self.low_scale, self.high_scale)\n        if min_side * scale < self.shorter_side:\n            scale = self.shorter_side * 1.0 / min_side\n        image = cv2.resize(\n            image, None, fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC\n        )\n        mask = cv2.resize(\n            mask, None, fx=scale, fy=scale, interpolation=cv2.INTER_NEAREST\n        )\n        return {""image"": image, ""mask"": mask}\n\n\nclass RandomMirror(object):\n    """"""Randomly flip the image and the mask""""""\n\n    def __init__(self):\n        pass\n\n    def __call__(self, sample):\n        image, mask = sample[""image""], sample[""mask""]\n        do_mirror = np.random.randint(2)\n        if do_mirror:\n            image = cv2.flip(image, 1)\n            mask = cv2.flip(mask, 1)\n        return {""image"": image, ""mask"": mask}\n\n\nclass Normalise(object):\n    """"""Normalise a tensor image with mean and standard deviation.\n    Given mean: (R, G, B) and std: (R, G, B),\n    will normalise each channel of the torch.*Tensor, i.e.\n    channel = (channel - mean) / std\n\n    Args:\n        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n        std (sequence): Sequence of standard deviations for R, G, B channels\n            respecitvely.\n    """"""\n\n    def __init__(self, scale, mean, std):\n        self.scale = scale\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, sample):\n        image = sample[""image""]\n        return {\n            ""image"": (self.scale * image - self.mean) / self.std,\n            ""mask"": sample[""mask""],\n        }\n\n\nclass ToTensor(object):\n    """"""Convert ndarrays in sample to Tensors.""""""\n\n    def __call__(self, sample):\n        image, mask = sample[""image""], sample[""mask""]\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        image = image.transpose((2, 0, 1))\n        return {""image"": torch.from_numpy(image), ""mask"": torch.from_numpy(mask)}\n\n\nclass NYUDataset(Dataset):\n    """"""NYUv2-40""""""\n\n    def __init__(self, data_file, data_dir, transform_trn=None, transform_val=None):\n        """"""\n        Args:\n            data_file (string): Path to the data file with annotations.\n            data_dir (string): Directory with all the images.\n            transform_{trn, val} (callable, optional): Optional transform to be applied\n                on a sample.\n        """"""\n        with open(data_file, ""rb"") as f:\n            datalist = f.readlines()\n        self.datalist = [\n            (k, v)\n            for k, v in map(\n                lambda x: x.decode(""utf-8"").strip(""\\n"").split(""\\t""), datalist\n            )\n        ]\n        self.root_dir = data_dir\n        self.transform_trn = transform_trn\n        self.transform_val = transform_val\n        self.stage = ""train""\n\n    def set_stage(self, stage):\n        self.stage = stage\n\n    def __len__(self):\n        return len(self.datalist)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, self.datalist[idx][0])\n        msk_name = os.path.join(self.root_dir, self.datalist[idx][1])\n\n        def read_image(x):\n            img_arr = np.array(Image.open(x))\n            if len(img_arr.shape) == 2:  # grayscale\n                img_arr = np.tile(img_arr, [3, 1, 1]).transpose(1, 2, 0)\n            return img_arr\n\n        image = read_image(img_name)\n        mask = np.array(Image.open(msk_name))\n        if img_name != msk_name:\n            assert len(mask.shape) == 2, ""Masks must be encoded without colourmap""\n        sample = {""image"": image, ""mask"": mask}\n        if self.stage == ""train"":\n            if self.transform_trn:\n                sample = self.transform_trn(sample)\n        elif self.stage == ""val"":\n            if self.transform_val:\n                sample = self.transform_val(sample)\n        return sample\n'"
src/setup.py,0,"b'from distutils.core import setup\nfrom Cython.Build import cythonize\nimport numpy\n\nsetup(\n    ext_modules=cythonize(""./src/*.pyx""), include_dirs=[numpy.get_include()],\n)\n'"
src/train.py,15,"b'""""""RefineNet-LightWeight\n\nRefineNet-LigthWeight PyTorch for non-commercial purposes\n\nCopyright (c) 2018, Vladimir Nekrasov (vladimir.nekrasov@adelaide.edu.au)\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n""""""\n\n# general libs\nimport argparse\nimport logging\nimport os\nimport random\nimport re\nimport sys\nimport time\n\n# misc\nimport cv2\nimport numpy as np\n\n# pytorch libs\nimport torch\nimport torch.nn as nn\n\n# custom libs\nfrom config import *\nfrom miou_utils import compute_iu, fast_cm\nfrom util import *\n\n\ndef get_arguments():\n    """"""Parse all the arguments provided from the CLI.\n\n    Returns:\n      A list of parsed arguments.\n    """"""\n    parser = argparse.ArgumentParser(description=""Full Pipeline Training"")\n\n    # Dataset\n    parser.add_argument(\n        ""--train-dir"",\n        type=str,\n        default=TRAIN_DIR,\n        help=""Path to the training set directory."",\n    )\n    parser.add_argument(\n        ""--val-dir"",\n        type=str,\n        default=VAL_DIR,\n        help=""Path to the validation set directory."",\n    )\n    parser.add_argument(\n        ""--train-list"",\n        type=str,\n        nargs=""+"",\n        default=TRAIN_LIST,\n        help=""Path to the training set list."",\n    )\n    parser.add_argument(\n        ""--val-list"",\n        type=str,\n        nargs=""+"",\n        default=VAL_LIST,\n        help=""Path to the validation set list."",\n    )\n    parser.add_argument(\n        ""--shorter-side"",\n        type=int,\n        nargs=""+"",\n        default=SHORTER_SIDE,\n        help=""Shorter side transformation."",\n    )\n    parser.add_argument(\n        ""--crop-size"",\n        type=int,\n        nargs=""+"",\n        default=CROP_SIZE,\n        help=""Crop size for training,"",\n    )\n    parser.add_argument(\n        ""--normalise-params"",\n        type=list,\n        default=NORMALISE_PARAMS,\n        help=""Normalisation parameters [scale, mean, std],"",\n    )\n    parser.add_argument(\n        ""--batch-size"",\n        type=int,\n        nargs=""+"",\n        default=BATCH_SIZE,\n        help=""Batch size to train the segmenter model."",\n    )\n    parser.add_argument(\n        ""--num-workers"",\n        type=int,\n        default=NUM_WORKERS,\n        help=""Number of workers for pytorch\'s dataloader."",\n    )\n    parser.add_argument(\n        ""--num-classes"",\n        type=int,\n        nargs=""+"",\n        default=NUM_CLASSES,\n        help=""Number of output classes for each task."",\n    )\n    parser.add_argument(\n        ""--low-scale"",\n        type=float,\n        nargs=""+"",\n        default=LOW_SCALE,\n        help=""Lower bound for random scale"",\n    )\n    parser.add_argument(\n        ""--high-scale"",\n        type=float,\n        nargs=""+"",\n        default=HIGH_SCALE,\n        help=""Upper bound for random scale"",\n    )\n    parser.add_argument(\n        ""--ignore-label"",\n        type=int,\n        default=IGNORE_LABEL,\n        help=""Label to ignore during training"",\n    )\n\n    # Encoder\n    parser.add_argument(""--enc"", type=str, default=ENC, help=""Encoder net type."")\n    parser.add_argument(\n        ""--enc-pretrained"",\n        type=bool,\n        default=ENC_PRETRAINED,\n        help=""Whether to init with imagenet weights."",\n    )\n    # General\n    parser.add_argument(\n        ""--evaluate"",\n        type=bool,\n        default=EVALUATE,\n        help=""If true, only validate segmentation."",\n    )\n    parser.add_argument(\n        ""--freeze-bn"",\n        type=bool,\n        nargs=""+"",\n        default=FREEZE_BN,\n        help=""Whether to keep batch norm statistics intact."",\n    )\n    parser.add_argument(\n        ""--num-segm-epochs"",\n        type=int,\n        nargs=""+"",\n        default=NUM_SEGM_EPOCHS,\n        help=""Number of epochs to train for segmentation network."",\n    )\n    parser.add_argument(\n        ""--print-every"",\n        type=int,\n        default=PRINT_EVERY,\n        help=""Print information every often."",\n    )\n    parser.add_argument(\n        ""--random-seed"",\n        type=int,\n        default=RANDOM_SEED,\n        help=""Seed to provide (near-)reproducibility."",\n    )\n    parser.add_argument(\n        ""--snapshot-dir"",\n        type=str,\n        default=SNAPSHOT_DIR,\n        help=""Path to directory for storing checkpoints."",\n    )\n    parser.add_argument(\n        ""--ckpt-path"", type=str, default=CKPT_PATH, help=""Path to the checkpoint file.""\n    )\n    parser.add_argument(\n        ""--val-every"",\n        nargs=""+"",\n        type=int,\n        default=VAL_EVERY,\n        help=""How often to validate current architecture."",\n    )\n\n    # Optimisers\n    parser.add_argument(\n        ""--lr-enc"",\n        type=float,\n        nargs=""+"",\n        default=LR_ENC,\n        help=""Learning rate for encoder."",\n    )\n    parser.add_argument(\n        ""--lr-dec"",\n        type=float,\n        nargs=""+"",\n        default=LR_DEC,\n        help=""Learning rate for decoder."",\n    )\n    parser.add_argument(\n        ""--mom-enc"",\n        type=float,\n        nargs=""+"",\n        default=MOM_ENC,\n        help=""Momentum for encoder."",\n    )\n    parser.add_argument(\n        ""--mom-dec"",\n        type=float,\n        nargs=""+"",\n        default=MOM_DEC,\n        help=""Momentum for decoder."",\n    )\n    parser.add_argument(\n        ""--wd-enc"",\n        type=float,\n        nargs=""+"",\n        default=WD_ENC,\n        help=""Weight decay for encoder."",\n    )\n    parser.add_argument(\n        ""--wd-dec"",\n        type=float,\n        nargs=""+"",\n        default=WD_DEC,\n        help=""Weight decay for decoder."",\n    )\n    parser.add_argument(\n        ""--optim-dec"",\n        type=str,\n        default=OPTIM_DEC,\n        help=""Optimiser algorithm for decoder."",\n    )\n    return parser.parse_args()\n\n\ndef create_segmenter(net, pretrained, num_classes):\n    """"""Create Encoder; for now only ResNet [50,101,152]""""""\n    from models.resnet import rf_lw50, rf_lw101, rf_lw152\n\n    if str(net) == ""50"":\n        return rf_lw50(num_classes, imagenet=pretrained)\n    elif str(net) == ""101"":\n        return rf_lw101(num_classes, imagenet=pretrained)\n    elif str(net) == ""152"":\n        return rf_lw152(num_classes, imagenet=pretrained)\n    else:\n        raise ValueError(""{} is not supported"".format(str(net)))\n\n\ndef create_loaders(\n    train_dir,\n    val_dir,\n    train_list,\n    val_list,\n    shorter_side,\n    crop_size,\n    low_scale,\n    high_scale,\n    normalise_params,\n    batch_size,\n    num_workers,\n    ignore_label,\n):\n    """"""\n    Args:\n      train_dir (str) : path to the root directory of the training set.\n      val_dir (str) : path to the root directory of the validation set.\n      train_list (str) : path to the training list.\n      val_list (str) : path to the validation list.\n      shorter_side (int) : parameter of the shorter_side resize transformation.\n      crop_size (int) : square crop to apply during the training.\n      low_scale (float) : lowest scale ratio for augmentations.\n      high_scale (float) : highest scale ratio for augmentations.\n      normalise_params (list / tuple) : img_scale, img_mean, img_std.\n      batch_size (int) : training batch size.\n      num_workers (int) : number of workers to parallelise data loading operations.\n      ignore_label (int) : label to pad segmentation masks with\n\n    Returns:\n      train_loader, val loader\n\n    """"""\n    # Torch libraries\n    from torchvision import transforms\n    from torch.utils.data import DataLoader, random_split\n\n    # Custom libraries\n    from datasets import NYUDataset as Dataset\n    from datasets import (\n        Pad,\n        RandomCrop,\n        RandomMirror,\n        ResizeShorterScale,\n        ToTensor,\n        Normalise,\n    )\n\n    ## Transformations during training ##\n    composed_trn = transforms.Compose(\n        [\n            ResizeShorterScale(shorter_side, low_scale, high_scale),\n            Pad(crop_size, [123.675, 116.28, 103.53], ignore_label),\n            RandomMirror(),\n            RandomCrop(crop_size),\n            Normalise(*normalise_params),\n            ToTensor(),\n        ]\n    )\n    composed_val = transforms.Compose([Normalise(*normalise_params), ToTensor()])\n    ## Training and validation sets ##\n    trainset = Dataset(\n        data_file=train_list,\n        data_dir=train_dir,\n        transform_trn=composed_trn,\n        transform_val=composed_val,\n    )\n\n    valset = Dataset(\n        data_file=val_list,\n        data_dir=val_dir,\n        transform_trn=None,\n        transform_val=composed_val,\n    )\n    logger.info(\n        "" Created train set = {} examples, val set = {} examples"".format(\n            len(trainset), len(valset)\n        )\n    )\n    ## Training and validation loaders ##\n    train_loader = DataLoader(\n        trainset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n        drop_last=True,\n    )\n    val_loader = DataLoader(\n        valset, batch_size=1, shuffle=False, num_workers=num_workers, pin_memory=True\n    )\n    return train_loader, val_loader\n\n\ndef create_optimisers(\n    lr_enc, lr_dec, mom_enc, mom_dec, wd_enc, wd_dec, param_enc, param_dec, optim_dec\n):\n    """"""Create optimisers for encoder, decoder and controller""""""\n    optim_enc = torch.optim.SGD(\n        param_enc, lr=lr_enc, momentum=mom_enc, weight_decay=wd_enc\n    )\n    if optim_dec == ""sgd"":\n        optim_dec = torch.optim.SGD(\n            param_dec, lr=lr_dec, momentum=mom_dec, weight_decay=wd_dec\n        )\n    elif optim_dec == ""adam"":\n        optim_dec = torch.optim.Adam(\n            param_dec, lr=lr_dec, weight_decay=wd_dec, eps=1e-3\n        )\n    return optim_enc, optim_dec\n\n\ndef load_ckpt(ckpt_path, ckpt_dict):\n    best_val = epoch_start = 0\n    if os.path.exists(args.ckpt_path):\n        ckpt = torch.load(ckpt_path)\n        for (k, v) in ckpt_dict.items():\n            if k in ckpt:\n                v.load_state_dict(ckpt[k])\n        best_val = ckpt.get(""best_val"", 0)\n        epoch_start = ckpt.get(""epoch_start"", 0)\n        logger.info(\n            "" Found checkpoint at {} with best_val {:.4f} at epoch {}"".format(\n                ckpt_path, best_val, epoch_start\n            )\n        )\n    return best_val, epoch_start\n\n\ndef train_segmenter(\n    segmenter, train_loader, optim_enc, optim_dec, epoch, segm_crit, freeze_bn\n):\n    """"""Training segmenter\n\n    Args:\n      segmenter (nn.Module) : segmentation network\n      train_loader (DataLoader) : training data iterator\n      optim_enc (optim) : optimiser for encoder\n      optim_dec (optim) : optimiser for decoder\n      epoch (int) : current epoch\n      segm_crit (nn.Loss) : segmentation criterion\n      freeze_bn (bool) : whether to keep BN params intact\n\n    """"""\n    train_loader.dataset.set_stage(""train"")\n    segmenter.train()\n    if freeze_bn:\n        for m in segmenter.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    for i, sample in enumerate(train_loader):\n        start = time.time()\n        input = sample[""image""].cuda()\n        target = sample[""mask""].cuda()\n        input_var = torch.autograd.Variable(input).float()\n        target_var = torch.autograd.Variable(target).long()\n        # Compute output\n        output = segmenter(input_var)\n        output = nn.functional.interpolate(\n            output, size=target_var.size()[1:], mode=""bilinear"", align_corners=False\n        )\n        soft_output = nn.LogSoftmax()(output)\n        # Compute loss and backpropagate\n        loss = segm_crit(soft_output, target_var)\n        optim_enc.zero_grad()\n        optim_dec.zero_grad()\n        loss.backward()\n        optim_enc.step()\n        optim_dec.step()\n        losses.update(loss.item())\n        batch_time.update(time.time() - start)\n        if i % args.print_every == 0:\n            logger.info(\n                "" Train epoch: {} [{}/{}]\\t""\n                ""Avg. Loss: {:.3f}\\t""\n                ""Avg. Time: {:.3f}"".format(\n                    epoch, i, len(train_loader), losses.avg, batch_time.avg\n                )\n            )\n\n\ndef validate(segmenter, val_loader, epoch, num_classes=-1):\n    """"""Validate segmenter\n\n    Args:\n      segmenter (nn.Module) : segmentation network\n      val_loader (DataLoader) : training data iterator\n      epoch (int) : current epoch\n      num_classes (int) : number of classes to consider\n\n    Returns:\n      Mean IoU (float)\n    """"""\n    val_loader.dataset.set_stage(""val"")\n    segmenter.eval()\n    cm = np.zeros((num_classes, num_classes), dtype=int)\n    with torch.no_grad():\n        for i, sample in enumerate(val_loader):\n            input = sample[""image""]\n            target = sample[""mask""]\n            input_var = torch.autograd.Variable(input).float().cuda()\n            # Compute output\n            output = segmenter(input_var)\n            output = (\n                cv2.resize(\n                    output[0, :num_classes].data.cpu().numpy().transpose(1, 2, 0),\n                    target.size()[1:][::-1],\n                    interpolation=cv2.INTER_CUBIC,\n                )\n                .argmax(axis=2)\n                .astype(np.uint8)\n            )\n            # Compute IoU\n            gt = target[0].data.cpu().numpy().astype(np.uint8)\n            gt_idx = (\n                gt < num_classes\n            )  # Ignore every class index larger than the number of classes\n            cm += fast_cm(output[gt_idx], gt[gt_idx], num_classes)\n\n            if i % args.print_every == 0:\n                logger.info(\n                    "" Val epoch: {} [{}/{}]\\t""\n                    ""Mean IoU: {:.3f}"".format(\n                        epoch, i, len(val_loader), compute_iu(cm).mean()\n                    )\n                )\n\n    ious = compute_iu(cm)\n    logger.info("" IoUs: {}"".format(ious))\n    miou = np.mean(ious)\n    logger.info("" Val epoch: {}\\tMean IoU: {:.3f}"".format(epoch, miou))\n    return miou\n\n\ndef main():\n    global args, logger\n    args = get_arguments()\n    logger = logging.getLogger(__name__)\n    ## Add args ##\n    args.num_stages = len(args.num_classes)\n    ## Set random seeds ##\n    torch.backends.cudnn.deterministic = True\n    torch.manual_seed(args.random_seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(args.random_seed)\n    np.random.seed(args.random_seed)\n    random.seed(args.random_seed)\n    ## Generate Segmenter ##\n    segmenter = nn.DataParallel(\n        create_segmenter(args.enc, args.enc_pretrained, args.num_classes[0])\n    ).cuda()\n    logger.info(\n        "" Loaded Segmenter {}, ImageNet-Pre-Trained={}, #PARAMS={:3.2f}M"".format(\n            args.enc, args.enc_pretrained, compute_params(segmenter) / 1e6\n        )\n    )\n    ## Restore if any ##\n    best_val, epoch_start = load_ckpt(args.ckpt_path, {""segmenter"": segmenter})\n    ## Criterion ##\n    segm_crit = nn.NLLLoss2d(ignore_index=args.ignore_label).cuda()\n\n    ## Saver ##\n    saver = Saver(\n        args=vars(args),\n        ckpt_dir=args.snapshot_dir,\n        best_val=best_val,\n        condition=lambda x, y: x > y,\n    )  # keep checkpoint with the best validation score\n\n    logger.info("" Training Process Starts"")\n    for task_idx in range(args.num_stages):\n        start = time.time()\n        torch.cuda.empty_cache()\n        ## Create dataloaders ##\n        train_loader, val_loader = create_loaders(\n            args.train_dir,\n            args.val_dir,\n            args.train_list[task_idx],\n            args.val_list[task_idx],\n            args.shorter_side[task_idx],\n            args.crop_size[task_idx],\n            args.low_scale[task_idx],\n            args.high_scale[task_idx],\n            args.normalise_params,\n            args.batch_size[task_idx],\n            args.num_workers,\n            args.ignore_label,\n        )\n        if args.evaluate:\n            return validate(\n                segmenter, val_loader, 0, num_classes=args.num_classes[task_idx]\n            )\n\n        logger.info("" Training Stage {}"".format(str(task_idx)))\n        ## Optimisers ##\n        enc_params = []\n        dec_params = []\n        for k, v in segmenter.named_parameters():\n            if bool(re.match("".*conv1.*|.*bn1.*|.*layer.*"", k)):\n                enc_params.append(v)\n                logger.info("" Enc. parameter: {}"".format(k))\n            else:\n                dec_params.append(v)\n                logger.info("" Dec. parameter: {}"".format(k))\n        optim_enc, optim_dec = create_optimisers(\n            args.lr_enc[task_idx],\n            args.lr_dec[task_idx],\n            args.mom_enc[task_idx],\n            args.mom_dec[task_idx],\n            args.wd_enc[task_idx],\n            args.wd_dec[task_idx],\n            enc_params,\n            dec_params,\n            args.optim_dec,\n        )\n        for epoch in range(args.num_segm_epochs[task_idx]):\n            train_segmenter(\n                segmenter,\n                train_loader,\n                optim_enc,\n                optim_dec,\n                epoch_start,\n                segm_crit,\n                args.freeze_bn[task_idx],\n            )\n            if (epoch + 1) % (args.val_every[task_idx]) == 0:\n                miou = validate(\n                    segmenter, val_loader, epoch_start, args.num_classes[task_idx]\n                )\n                saver.save(\n                    miou,\n                    {""segmenter"": segmenter.state_dict(), ""epoch_start"": epoch_start},\n                    logger,\n                )\n            epoch_start += 1\n        logger.info(\n            ""Stage {} finished, time spent {:.3f}min"".format(\n                task_idx, (time.time() - start) / 60.0\n            )\n        )\n    logger.info(\n        ""All stages are now finished. Best Val is {:.3f}"".format(saver.best_val)\n    )\n\n\nif __name__ == ""__main__"":\n    logging.basicConfig(level=logging.INFO)\n    main()\n'"
src/util.py,1,"b'""""""Helper definitions""""""\n\nimport json\nimport os\n\nimport torch\n\n\ndef compute_params(model):\n    """"""Compute number of parameters""""""\n    n_total_params = 0\n    for name, m in model.named_parameters():\n        n_elem = m.numel()\n        n_total_params += n_elem\n    return n_total_params\n\n\n# Adopted from https://raw.githubusercontent.com/pytorch/examples/master/imagenet/main.py\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass Saver:\n    """"""Saver class for managing parameters""""""\n\n    def __init__(self, args, ckpt_dir, best_val=0, condition=lambda x, y: x > y):\n        """"""\n        Args:\n            args (dict): dictionary with arguments.\n            ckpt_dir (str): path to directory in which to store the checkpoint.\n            best_val (float): initial best value.\n            condition (function): how to decide whether to save the new checkpoint\n                                    by comparing best value and new value (x,y).\n\n        """"""\n        if not os.path.exists(ckpt_dir):\n            os.makedirs(ckpt_dir)\n        with open(""{}/args.json"".format(ckpt_dir), ""w"") as f:\n            json.dump(\n                {k: v for k, v in args.items() if isinstance(v, (int, float, str))},\n                f,\n                sort_keys=True,\n                indent=4,\n                ensure_ascii=False,\n            )\n        self.ckpt_dir = ckpt_dir\n        self.best_val = best_val\n        self.condition = condition\n        self._counter = 0\n\n    def _do_save(self, new_val):\n        """"""Check whether need to save""""""\n        return self.condition(new_val, self.best_val)\n\n    def save(self, new_val, dict_to_save, logger):\n        """"""Save new checkpoint""""""\n        self._counter += 1\n        if self._do_save(new_val):\n            logger.info(\n                "" New best value {:.4f}, was {:.4f}"".format(new_val, self.best_val)\n            )\n            self.best_val = new_val\n            dict_to_save[""best_val""] = new_val\n            torch.save(dict_to_save, ""{}/checkpoint.pth.tar"".format(self.ckpt_dir))\n            return True\n        return False\n'"
src_v2/arguments.py,0,"b'import argparse\n\n\ndef get_arguments():\n    """"""Parse all the arguments provided from the CLI.""""""\n    parser = argparse.ArgumentParser(\n        description=""Arguments for Light-Weight-RefineNet Training Pipelien""\n    )\n\n    # Common transformations\n    parser.add_argument(""--img-scale"", type=float, default=1.0 / 255)\n    parser.add_argument(\n        ""--img-mean"", type=float, nargs=""+"", default=(0.485, 0.456, 0.406)\n    )\n    parser.add_argument(\n        ""--img-std"", type=float, nargs=""+"", default=(0.229, 0.224, 0.225)\n    )\n\n    # Training augmentations\n    parser.add_argument(""--crop-size"", type=int, nargs=""+"", default=(500, 500, 500,))\n    parser.add_argument(""--shorter-side"", type=int, nargs=""+"", default=(350, 350, 350,))\n    parser.add_argument(""--low-scale"", type=float, nargs=""+"", default=(0.5, 0.5, 0.5,))\n    parser.add_argument(""--high-scale"", type=float, nargs=""+"", default=(2.0, 2.0, 2.0,))\n    parser.add_argument(\n        ""--augmentations-type"",\n        type=str,\n        choices=[""densetorch"", ""albumentations""],\n        default=""densetorch"",\n    )\n\n    # Dataset\n    parser.add_argument(\n        ""--train-list-path"", type=str, nargs=""+"", default=(""./data/train.nyu"",)\n    )\n    parser.add_argument(\n        ""--val-list-path"", type=str, default=""./data/val.nyu"",\n    )\n    parser.add_argument(\n        ""--train-dir"", type=str, nargs=""+"", default=(""./datasets/nyud/"",)\n    )\n    parser.add_argument(\n        ""--val-dir"", type=str, default=""./datasets/nyud/"",\n    )\n    parser.add_argument(""--train-batch-size"", type=int, nargs=""+"", default=(6, 6, 6,))\n    parser.add_argument(""--val-batch-size"", type=int, default=1)\n\n    # Optimisation\n    parser.add_argument(\n        ""--enc-optim-type"", type=str, default=""sgd"",\n    )\n    parser.add_argument(\n        ""--dec-optim-type"", type=str, default=""sgd"",\n    )\n    parser.add_argument(\n        ""--enc-lr"", type=float, default=5e-4,\n    )\n    parser.add_argument(\n        ""--dec-lr"", type=float, default=5e-3,\n    )\n    parser.add_argument(\n        ""--enc-weight-decay"", type=float, default=1e-5,\n    )\n    parser.add_argument(\n        ""--dec-weight-decay"", type=float, default=1e-5,\n    )\n    parser.add_argument(\n        ""--enc-momentum"", type=float, default=0.9,\n    )\n    parser.add_argument(\n        ""--dec-momentum"", type=float, default=0.9,\n    )\n    parser.add_argument(\n        ""--enc-lr-gamma"",\n        type=float,\n        default=0.5,\n        help=""Multilpy lr_enc by this value after each stage."",\n    )\n    parser.add_argument(\n        ""--dec-lr-gamma"",\n        type=float,\n        default=0.5,\n        help=""Multilpy lr_dec by this value after each stage."",\n    )\n    parser.add_argument(\n        ""--ignore-label"",\n        type=int,\n        default=255,\n        help=""Ignore this label in the training loss."",\n    )\n    parser.add_argument(""--random-seed"", type=int, default=42)\n    parser.add_argument(\n        ""--freeze-bn"", type=int, choices=[0, 1], nargs=""+"", default=(1, 1, 1,)\n    )\n\n    # Training / validation setup\n    parser.add_argument(\n        ""--enc-backbone"", type=str, choices=[""50"", ""101"", ""152""], default=""50""\n    )\n    parser.add_argument(""--enc-pretrained"", type=int, choices=[0, 1], default=1)\n    parser.add_argument(\n        ""--num-stages"",\n        type=int,\n        default=3,\n        help=""Number of training stages. All other arguments with nargs=\'+\' must ""\n        ""have the number of arguments equal to this value. Otherwise, the given ""\n        ""arguments will be broadcasted to have the required length."",\n    )\n    parser.add_argument(\n        ""--epochs-per-stage"", type=int, nargs=""+"", default=(100, 100, 100),\n    )\n    parser.add_argument(""--val-every"", type=int, nargs=""+"", default=(5, 5, 5,))\n    parser.add_argument(""--num-classes"", type=int, default=40)\n    parser.add_argument(\n        ""--dataset-type"",\n        type=str,\n        default=""densetorch"",\n        choices=[""densetorch"", ""torchvision""],\n    )\n    parser.add_argument(\n        ""--stage-names"",\n        type=str,\n        nargs=""+"",\n        choices=[""SBD"", ""VOC""],\n        default=(""SBD"", ""VOC"",),\n        help=""Only used if dataset_type == torchvision."",\n    )\n    parser.add_argument(\n        ""--train-download"",\n        type=int,\n        nargs=""+"",\n        choices=[0, 1],\n        default=(0, 0,),\n        help=""Only used if dataset_type == torchvision."",\n    )\n    parser.add_argument(\n        ""--val-download"",\n        type=int,\n        choices=[0, 1],\n        default=0,\n        help=""Only used if dataset_type == torchvision."",\n    )\n\n    # Checkpointing configuration\n    parser.add_argument(""--ckpt-dir"", type=str, default=""./checkpoints/"")\n    parser.add_argument(\n        ""--ckpt-path"",\n        type=str,\n        default=""./checkpoints/checkpoint.pth.tar"",\n        help=""Path to the checkpoint file."",\n    )\n\n    return parser.parse_args()\n'"
src_v2/data.py,5,"b'import cv2\nimport numpy as np\nimport torch\n\nfrom densetorch.misc import broadcast\n\n\ndef albumentations2torchvision(transforms):\n    """"""Wrap albumentations transformation so that they can be used in torchvision dataset""""""\n    from albumentations import Compose\n\n    def wrapper_func(image, target):\n        keys = [""image"", ""mask""]\n        np_dtypes = [np.float32, np.uint8]\n        torch_dtypes = [torch.float32, torch.long]\n        sample_dict = {\n            key: np.array(value, dtype=dtype)\n            for key, value, dtype in zip(keys, [image, target], np_dtypes)\n        }\n        output = Compose(transforms)(**sample_dict)\n        return [output[key].to(dtype) for key, dtype in zip(keys, torch_dtypes)]\n\n    return wrapper_func\n\n\ndef albumentations_transforms(\n    crop_size,\n    shorter_side,\n    low_scale,\n    high_scale,\n    img_mean,\n    img_std,\n    img_scale,\n    ignore_label,\n    num_stages,\n    dataset_type,\n):\n    from albumentations import (\n        Normalize,\n        VerticalFlip,\n        HorizontalFlip,\n        RandomCrop,\n        PadIfNeeded,\n        RandomScale,\n        LongestMaxSize,\n        SmallestMaxSize,\n        OneOf,\n    )\n    from albumentations.pytorch import ToTensorV2 as ToTensor\n    from densetorch.data import albumentations2densetorch\n\n    if dataset_type == ""densetorch"":\n        wrapper = albumentations2densetorch\n    elif dataset_type == ""torchvision"":\n        wrapper = albumentations2torchvision\n    else:\n        raise ValueError(f""Unknown dataset type: {dataset_type}"")\n\n    common_transformations = [\n        Normalize(max_pixel_value=1.0 / img_scale, mean=img_mean, std=img_std),\n        ToTensor(),\n    ]\n    train_transforms = []\n    for stage in range(num_stages):\n        train_transforms.append(\n            wrapper(\n                [\n                    OneOf(\n                        [\n                            RandomScale(\n                                scale_limit=(low_scale[stage], high_scale[stage])\n                            ),\n                            LongestMaxSize(max_size=shorter_side[stage]),\n                            SmallestMaxSize(max_size=shorter_side[stage]),\n                        ]\n                    ),\n                    PadIfNeeded(\n                        min_height=crop_size[stage],\n                        min_width=crop_size[stage],\n                        border_mode=cv2.BORDER_CONSTANT,\n                        value=np.array(img_mean) / img_scale,\n                        mask_value=ignore_label,\n                    ),\n                    HorizontalFlip(p=0.5,),\n                    RandomCrop(height=crop_size[stage], width=crop_size[stage],),\n                ]\n                + common_transformations\n            )\n        )\n    val_transforms = wrapper(common_transformations)\n    return train_transforms, val_transforms\n\n\ndef densetorch_transforms(\n    crop_size,\n    shorter_side,\n    low_scale,\n    high_scale,\n    img_mean,\n    img_std,\n    img_scale,\n    ignore_label,\n    num_stages,\n    dataset_type,\n):\n    from torchvision.transforms import Compose\n    from densetorch.data import (\n        Pad,\n        RandomCrop,\n        RandomMirror,\n        ResizeAndScale,\n        ToTensor,\n        Normalise,\n        densetorch2torchvision,\n    )\n\n    if dataset_type == ""densetorch"":\n        wrapper = Compose\n    elif dataset_type == ""torchvision"":\n        wrapper = densetorch2torchvision\n    else:\n        raise ValueError(f""Unknown dataset type: {dataset_type}"")\n\n    common_transformations = [\n        Normalise(scale=img_scale, mean=img_mean, std=img_std),\n        ToTensor(),\n    ]\n    train_transforms = []\n    for stage in range(num_stages):\n        train_transforms.append(\n            wrapper(\n                [\n                    ResizeAndScale(\n                        shorter_side[stage], low_scale[stage], high_scale[stage]\n                    ),\n                    Pad(crop_size[stage], img_mean, ignore_label),\n                    RandomMirror(),\n                    RandomCrop(crop_size[stage]),\n                ]\n                + common_transformations\n            )\n        )\n    val_transforms = wrapper(common_transformations)\n    return train_transforms, val_transforms\n\n\ndef get_transforms(\n    crop_size,\n    shorter_side,\n    low_scale,\n    high_scale,\n    img_mean,\n    img_std,\n    img_scale,\n    ignore_label,\n    num_stages,\n    augmentations_type,\n    dataset_type,\n):\n    """"""\n    Args:\n\n      crop_size (int) : square crop to apply during the training.\n      shorter_side (int) : parameter of the shorter_side resize transformation.\n      low_scale (float) : lowest scale ratio for augmentations.\n      high_scale (float) : highest scale ratio for augmentations.\n      img_mean (list of float) : image mean.\n      img_std (list of float) : image standard deviation\n      img_scale (list of float) : image scale.\n      ignore_label (int) : label to pad segmentation masks with.\n      num_stages (int): broadcast training parameters to have this length.\n      augmentations_type (str): whether to use densetorch augmentations or albumentations.\n      dataset_type (str): whether to use densetorch or torchvision dataset, needed to correctly wrap transformations.\n\n    Returns:\n      train_transforms, val_transforms\n\n    """"""\n    crop_size, shorter_side, low_scale, high_scale = [\n        broadcast(param, num_stages)\n        for param in (crop_size, shorter_side, low_scale, high_scale)\n    ]\n\n    if augmentations_type == ""densetorch"":\n        func = densetorch_transforms\n    elif augmentations_type == ""albumentations"":\n        func = albumentations_transforms\n    else:\n        raise ValueError(f""Unknown augmentations type {augmentations_type}"")\n    return func(\n        crop_size=crop_size,\n        shorter_side=shorter_side,\n        low_scale=low_scale,\n        high_scale=high_scale,\n        img_mean=img_mean,\n        img_std=img_std,\n        img_scale=img_scale,\n        ignore_label=ignore_label,\n        num_stages=num_stages,\n        dataset_type=dataset_type,\n    )\n\n\ndef densetorch_dataset(\n    train_dir,\n    val_dir,\n    train_list_path,\n    val_list_path,\n    train_transforms,\n    val_transforms,\n    masks_names,\n    stage_names,\n    train_download,\n    val_download,\n):\n    from densetorch.data import MMDataset as Dataset\n\n    def line_to_paths_fn(x):\n        rgb, segm = x.decode(""utf-8"").strip(""\\n"").split(""\\t"")\n        return [rgb, segm]\n\n    train_sets = [\n        Dataset(\n            data_file=train_list_path[i],\n            data_dir=train_dir[i],\n            line_to_paths_fn=line_to_paths_fn,\n            masks_names=masks_names,\n            transform=train_transforms[i],\n        )\n        for i in range(len(train_transforms))\n    ]\n    val_set = Dataset(\n        data_file=val_list_path,\n        data_dir=val_dir,\n        line_to_paths_fn=line_to_paths_fn,\n        masks_names=masks_names,\n        transform=val_transforms,\n    )\n    return train_sets, val_set\n\n\ndef torchvision_dataset(\n    train_dir,\n    val_dir,\n    train_list_path,\n    val_list_path,\n    train_transforms,\n    val_transforms,\n    masks_names,\n    stage_names,\n    train_download,\n    val_download,\n):\n    from torchvision.datasets.voc import VOCSegmentation\n    from torchvision.datasets import SBDataset\n    from functools import partial\n\n    train_sets = []\n    for i, stage in enumerate(stage_names):\n        if stage.lower() == ""voc"":\n            Dataset = partial(VOCSegmentation, image_set=""train"", year=""2012"",)\n        elif stage.lower() == ""sbd"":\n            Dataset = partial(SBDataset, mode=""segmentation"", image_set=""train_noval"")\n        train_sets.append(\n            Dataset(\n                root=train_dir[i],\n                transforms=train_transforms[i],\n                download=train_download[i],\n            )\n        )\n\n    val_set = VOCSegmentation(\n        root=val_dir,\n        image_set=""val"",\n        year=""2012"",\n        download=val_download,\n        transforms=val_transforms,\n    )\n\n    return train_sets, val_set\n\n\ndef get_datasets(\n    train_dir,\n    val_dir,\n    train_list_path,\n    val_list_path,\n    train_transforms,\n    val_transforms,\n    masks_names,\n    dataset_type,\n    stage_names,\n    train_download,\n    val_download,\n):\n    # Broadcast train dir to have the same length as train_transforms\n    train_dir = broadcast(train_dir, len(train_transforms))\n    train_list_path = broadcast(train_list_path, len(train_transforms))\n    train_download = broadcast(train_download, len(train_transforms))\n    stage_names = broadcast(stage_names, len(train_transforms))\n    if dataset_type == ""densetorch"":\n        func = densetorch_dataset\n    elif dataset_type == ""torchvision"":\n        func = torchvision_dataset\n    else:\n        raise ValueError(f""Unknown dataset type {dataset_type}"")\n    return func(\n        train_dir,\n        val_dir,\n        train_list_path,\n        val_list_path,\n        train_transforms,\n        val_transforms,\n        masks_names,\n        stage_names,\n        train_download,\n        val_download,\n    )\n'"
src_v2/network.py,0,"b'from models.resnet import rf_lw50, rf_lw101, rf_lw152\n\n\ndef get_segmenter(\n    enc_backbone, enc_pretrained, num_classes,\n):\n    """"""Create Encoder; for now only ResNet [50,101,152]""""""\n    if enc_backbone == ""50"":\n        return rf_lw50(num_classes, imagenet=enc_pretrained)\n    elif enc_backbone == ""101"":\n        return rf_lw101(num_classes, imagenet=enc_pretrained)\n    elif enc_backbone == ""152"":\n        return rf_lw152(num_classes, imagenet=enc_pretrained)\n    else:\n        raise ValueError(""{} is not supported"".format(str(enc_backbone)))\n'"
src_v2/optimisers.py,1,"b'import logging\nimport numpy as np\nimport re\nfrom torch.optim.lr_scheduler import MultiStepLR\n\nimport densetorch as dt\n\n\ndef get_lr_schedulers(\n    enc_optim, dec_optim, enc_lr_gamma, dec_lr_gamma, epochs_per_stage,\n):\n    milestones = np.cumsum(epochs_per_stage)\n    schedulers = [\n        MultiStepLR(enc_optim, milestones=milestones, gamma=enc_lr_gamma),\n        MultiStepLR(dec_optim, milestones=milestones, gamma=dec_lr_gamma),\n    ]\n    return schedulers\n\n\ndef get_optimisers(\n    model,\n    enc_optim_type,\n    enc_lr,\n    enc_weight_decay,\n    enc_momentum,\n    dec_optim_type,\n    dec_lr,\n    dec_weight_decay,\n    dec_momentum,\n):\n    logger = logging.getLogger(__name__)\n    # Filter parameters of encoder / decoder\n    enc_params = []\n    dec_params = []\n    for k, v in model.named_parameters():\n        if bool(re.match("".*conv1.*|.*bn1.*|.*layer.*"", k)):\n            enc_params.append(v)\n            logger.info("" Enc. parameter: {}"".format(k))\n        else:\n            dec_params.append(v)\n            logger.info("" Dec. parameter: {}"".format(k))\n    # Create optimisers\n    optimisers = [\n        dt.misc.create_optim(\n            optim_type=enc_optim_type,\n            parameters=enc_params,\n            lr=enc_lr,\n            weight_decay=enc_weight_decay,\n            momentum=enc_momentum,\n        ),\n        dt.misc.create_optim(\n            optim_type=dec_optim_type,\n            parameters=dec_params,\n            lr=dec_lr,\n            weight_decay=dec_weight_decay,\n            momentum=dec_momentum,\n        ),\n    ]\n    return optimisers\n'"
src_v2/train.py,3,"b'# general libs\nimport logging\n\n# pytorch libs\nimport torch\nimport torch.nn as nn\n\n# densetorch wrapper\nimport densetorch as dt\n\n# configuration for light-weight refinenet\nfrom arguments import get_arguments\nfrom data import get_datasets, get_transforms\nfrom network import get_segmenter\nfrom optimisers import get_optimisers, get_lr_schedulers\n\n\ndef setup_network(args, device):\n    logger = logging.getLogger(__name__)\n    segmenter = get_segmenter(\n        enc_backbone=args.enc_backbone,\n        enc_pretrained=args.enc_pretrained,\n        num_classes=args.num_classes,\n    ).to(device)\n    if device == ""cuda"":\n        segmenter = nn.DataParallel(segmenter)\n    logger.info(\n        "" Loaded Segmenter {}, ImageNet-Pre-Trained={}, #PARAMS={:3.2f}M"".format(\n            args.enc_backbone,\n            args.enc_pretrained,\n            dt.misc.compute_params(segmenter) / 1e6,\n        )\n    )\n    training_loss = nn.CrossEntropyLoss(ignore_index=args.ignore_label).to(device)\n    validation_loss = dt.engine.MeanIoU(num_classes=args.num_classes)\n    return segmenter, training_loss, validation_loss\n\n\ndef setup_checkpoint_and_maybe_restore(args, model):\n    saver = dt.misc.Saver(\n        args=vars(args),\n        ckpt_dir=args.ckpt_dir,\n        best_val=0,\n        condition=lambda x, y: x > y,\n    )  # keep checkpoint with the best validation score\n    epoch_start, _, state_dict = saver.load(\n        ckpt_path=args.ckpt_path, keys_to_load=[""epoch"", ""best_val"", ""state_dict""],\n    )\n    if epoch_start is None:\n        epoch_start = 0\n    dt.misc.load_state_dict(model, state_dict)\n    return saver, epoch_start\n\n\ndef setup_data_loaders(args):\n    train_transforms, val_transforms = get_transforms(\n        crop_size=args.crop_size,\n        shorter_side=args.shorter_side,\n        low_scale=args.low_scale,\n        high_scale=args.high_scale,\n        img_mean=args.img_mean,\n        img_std=args.img_std,\n        img_scale=args.img_scale,\n        ignore_label=args.ignore_label,\n        num_stages=args.num_stages,\n        augmentations_type=args.augmentations_type,\n        dataset_type=args.dataset_type,\n    )\n    train_sets, val_set = get_datasets(\n        train_dir=args.train_dir,\n        val_dir=args.val_dir,\n        train_list_path=args.train_list_path,\n        val_list_path=args.val_list_path,\n        train_transforms=train_transforms,\n        val_transforms=val_transforms,\n        masks_names=(""segm"",),\n        dataset_type=args.dataset_type,\n        stage_names=args.stage_names,\n        train_download=args.train_download,\n        val_download=args.val_download,\n    )\n    train_loaders, val_loader = dt.data.get_loaders(\n        train_batch_size=args.train_batch_size,\n        val_batch_size=args.val_batch_size,\n        train_set=train_sets,\n        val_set=val_set,\n        num_stages=args.num_stages,\n    )\n    return train_loaders, val_loader\n\n\ndef setup_optimisers_and_schedulers(args, model):\n    enc_optim, dec_optim = get_optimisers(\n        model=model,\n        enc_optim_type=args.enc_optim_type,\n        enc_lr=args.enc_lr,\n        enc_weight_decay=args.enc_weight_decay,\n        enc_momentum=args.enc_momentum,\n        dec_optim_type=args.dec_optim_type,\n        dec_lr=args.dec_lr,\n        dec_weight_decay=args.dec_weight_decay,\n        dec_momentum=args.dec_momentum,\n    )\n    schedulers = get_lr_schedulers(\n        enc_optim=enc_optim,\n        dec_optim=dec_optim,\n        enc_lr_gamma=args.enc_lr_gamma,\n        dec_lr_gamma=args.dec_lr_gamma,\n        epochs_per_stage=args.epochs_per_stage,\n    )\n    return [enc_optim, dec_optim], schedulers\n\n\ndef main():\n    args = get_arguments()\n    logger = logging.getLogger(__name__)\n    torch.backends.cudnn.deterministic = True\n    dt.misc.set_seed(args.random_seed)\n    device = ""cuda"" if torch.cuda.is_available() else ""cpu""\n    # Network\n    segmenter, training_loss, validation_loss = setup_network(args, device=device)\n    # Checkpoint\n    saver, epoch_start = setup_checkpoint_and_maybe_restore(args, model=segmenter)\n    # Data\n    train_loaders, val_loader = setup_data_loaders(args)\n    # Optimisers\n    optimisers, schedulers = setup_optimisers_and_schedulers(args, model=segmenter)\n\n    total_epoch = epoch_start\n    for stage, num_epochs in enumerate(args.epochs_per_stage):\n        if stage > 0:\n            epoch_start = 0\n        for epoch in range(epoch_start, num_epochs):\n            logger.info(f""Training: stage {stage} epoch {epoch}"")\n            dt.engine.train(\n                model=segmenter,\n                opts=optimisers,\n                crits=training_loss,\n                dataloader=train_loaders[stage],\n                freeze_bn=args.freeze_bn[stage],\n            )\n            total_epoch += 1\n            for scheduler in schedulers:\n                scheduler.step(total_epoch)\n            if (epoch + 1) % args.val_every[stage] == 0:\n                logger.info(f""Validation: stage {stage} epoch {epoch}"")\n                vals = dt.engine.validate(\n                    model=segmenter, metrics=validation_loss, dataloader=val_loader,\n                )\n                saver.maybe_save(\n                    new_val=vals,\n                    dict_to_save={\n                        ""state_dict"": segmenter.state_dict(),\n                        ""epoch"": total_epoch,\n                    },\n                )\n\n\nif __name__ == ""__main__"":\n    logging.basicConfig(\n        format=""%(asctime)s :: %(levelname)s :: %(name)s :: %(message)s"",\n        level=logging.INFO,\n    )\n    main()\n'"
utils/__init__.py,0,b''
utils/helpers.py,1,"b'import numpy as np\nimport torch\n\nIMG_SCALE = 1.0 / 255\nIMG_MEAN = np.array([0.485, 0.456, 0.406]).reshape((1, 1, 3))\nIMG_STD = np.array([0.229, 0.224, 0.225]).reshape((1, 1, 3))\n\n\ndef maybe_download(model_name, model_url, model_dir=None, map_location=None):\n    import os\n    import sys\n    from six.moves import urllib\n\n    if model_dir is None:\n        torch_home = os.path.expanduser(os.getenv(""TORCH_HOME"", ""~/.torch""))\n        model_dir = os.getenv(""TORCH_MODEL_ZOO"", os.path.join(torch_home, ""models""))\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    filename = ""{}.pth.tar"".format(model_name)\n    cached_file = os.path.join(model_dir, filename)\n    if not os.path.exists(cached_file):\n        url = model_url\n        sys.stderr.write(\'Downloading: ""{}"" to {}\\n\'.format(url, cached_file))\n        urllib.request.urlretrieve(url, cached_file)\n    return torch.load(cached_file, map_location=map_location)\n\n\ndef prepare_img(img):\n    return (img * IMG_SCALE - IMG_MEAN) / IMG_STD\n'"
utils/layer_factory.py,1,"b'""""""RefineNet-LightWeight-CRP Block\n\nRefineNet-LigthWeight PyTorch for non-commercial purposes\n\nCopyright (c) 2018, Vladimir Nekrasov (vladimir.nekrasov@adelaide.edu.au)\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n""""""\n\nimport torch.nn as nn\n\n\ndef batchnorm(in_planes):\n    ""batch norm 2d""\n    return nn.BatchNorm2d(in_planes, affine=True, eps=1e-5, momentum=0.1)\n\n\ndef conv3x3(in_planes, out_planes, stride=1, bias=False):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(\n        in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=bias\n    )\n\n\ndef conv1x1(in_planes, out_planes, stride=1, bias=False):\n    ""1x1 convolution""\n    return nn.Conv2d(\n        in_planes, out_planes, kernel_size=1, stride=stride, padding=0, bias=bias\n    )\n\n\ndef convbnrelu(in_planes, out_planes, kernel_size, stride=1, groups=1, act=True):\n    ""conv-batchnorm-relu""\n    if act:\n        return nn.Sequential(\n            nn.Conv2d(\n                in_planes,\n                out_planes,\n                kernel_size,\n                stride=stride,\n                padding=int(kernel_size / 2.0),\n                groups=groups,\n                bias=False,\n            ),\n            batchnorm(out_planes),\n            nn.ReLU6(inplace=True),\n        )\n    else:\n        return nn.Sequential(\n            nn.Conv2d(\n                in_planes,\n                out_planes,\n                kernel_size,\n                stride=stride,\n                padding=int(kernel_size / 2.0),\n                groups=groups,\n                bias=False,\n            ),\n            batchnorm(out_planes),\n        )\n\n\nclass CRPBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, n_stages):\n        super(CRPBlock, self).__init__()\n        for i in range(n_stages):\n            setattr(\n                self,\n                ""{}_{}"".format(i + 1, ""outvar_dimred""),\n                conv1x1(\n                    in_planes if (i == 0) else out_planes,\n                    out_planes,\n                    stride=1,\n                    bias=False,\n                ),\n            )\n        self.stride = 1\n        self.n_stages = n_stages\n        self.maxpool = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)\n\n    def forward(self, x):\n        top = x\n        for i in range(self.n_stages):\n            top = self.maxpool(top)\n            top = getattr(self, ""{}_{}"".format(i + 1, ""outvar_dimred""))(top)\n            x = top + x\n        return x\n'"
