file_path,api_count,code
Existing paper reading/test.py,17,"b'from __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport time\r\nimport argparse\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nfrom torch.utils import data\r\nimport os, sys, math, pickle, gc\r\n\r\nfrom utils.utils import convert_index, get_subgraph, adjust_sent_order, load_dict, mean_rank, convert_idx2name, write_triples\r\nfrom utils.data_loader import LinkPredictionDataset, LinkTestTotal, LinkTestDataset\r\nfrom model.GATA import GATA\r\nfrom collections import OrderedDict\r\n\r\n\r\nparser = argparse.ArgumentParser()\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\r\n    \'--data_dir\', type=str, default=\'paper_reading/\'\r\n)\r\nparser.add_argument(\r\n    \'--gpu\', default=\'1\', type=int, help=\'default is 1. set 0 to disable use gpu.\'\r\n)\r\nparser.add_argument(\r\n    \'--batch_size\', type=int, default=400, help=\'Size of a single batch\'\r\n)\r\nparser.add_argument(\r\n    ""--model"", default=""models/GATA/best_dev_model.pth.tar"",\r\n    help=""Model location""\r\n)\r\nargs = parser.parse_args()\r\ndevice = torch.device(\'cuda:0\' if torch.cuda.is_available()  and args.gpu == 1 else \'cpu\')\r\n\r\n# Initialize\r\n# load previously saved data\r\nstate = pickle.load(open(\'dataset.pth\', \'rb\'))\r\nparameters = state[\'parameters\']\r\ngraph = state[\'graph\']\r\ntext_f = state[\'text_f\']\r\nmappings = state[\'mappings\']\r\nnum_ent = state[\'num_ent\']\r\nnum_rel = state[\'num_rel\']\r\nid2ent = state[\'id2ent\']\r\nnum_ent = state[\'num_ent\']\r\nprint(""finish load"")\r\n\r\n\r\nrel_f = os.path.join(args.data_dir, \'relation2id.txt\')\r\nid2rel, _ = load_dict(rel_f)\r\nname_f = os.path.join(args.data_dir, \'term.pth\')\r\nent2name = pickle.load(open(name_f,""rb""))\r\n\r\n# Load Positive and Negative Examples\r\nparams = {\'batch_size\': args.batch_size, \'shuffle\': True, \'collate_fn\': adjust_sent_order}\r\ntrain_set = LinkPredictionDataset(os.path.join(args.data_dir, \'train2id.txt\'), text_f, id2ent, num_ent)\r\ntrain_triple_dict = train_set.get_triple_dict()\r\ntrain_generator = data.DataLoader(train_set, **params)\r\nprint(\'Finish loading train\')\r\n\r\nparams_test = {\'batch_size\': args.batch_size, \'shuffle\': False, \'collate_fn\': adjust_sent_order}\r\ntest_set = LinkTestTotal(os.path.join(args.data_dir, \'test2id.txt\'), num_ent)\r\nprint(\'Finish loading test\')\r\n\r\ny = torch.FloatTensor([-1])\r\ny = y.to(device)\r\n\r\n# Initialize Model\r\nmodel = GATA(**parameters)\r\nmodel.to(device)\r\n\r\n\r\ndef test():\r\n    print(\'Testing...\')\r\n    # model.cpu()\r\n    model.eval()\r\n\r\n    output_file = open(\'test_top10.txt\', \'w\')\r\n    hitk_all = 0\r\n    mean_rank_head = []\r\n    mean_rank_tail = []\r\n    all_named_triples = set()\r\n    for batch_idx, (new_head_triple, new_tail_triple, correct) in enumerate(test_set):\r\n        t_current = time.time()\r\n        print(""Current: "", batch_idx, ""Total: "", len(test_set))\r\n        test = LinkTestDataset(new_head_triple, new_tail_triple, text_f, id2ent)\r\n        test_generator = data.DataLoader(test, **params_test)\r\n        scores_heads = []\r\n        scores_tails = []\r\n\r\n        for current_idx, instance in enumerate(test_generator):\r\n            head, tail, hht_bef, htt_bef, tht_bef, ttt_bef = instance\r\n            head = head.to(device)\r\n            tail = tail.to(device)\r\n            # text information\r\n            hht = list(map(lambda x:x.to(device),hht_bef[0:3]))\r\n            htt = list(map(lambda x:x.to(device),htt_bef[0:3]))\r\n            tht = list(map(lambda x:x.to(device),tht_bef[0:3]))\r\n            ttt = list(map(lambda x:x.to(device),ttt_bef[0:3]))\r\n            batch_nodes, batch_adj = get_subgraph(head, train_triple_dict, graph)\r\n            # get relative location according to the batch_nodes\r\n            shifted_head = convert_index([head], batch_nodes)\r\n            batch_nodes = torch.LongTensor(batch_nodes.tolist()).to(device)\r\n            batch_adj = torch.from_numpy(batch_adj).to(device)\r\n            shifted_head = torch.LongTensor(shifted_head[0]).to(device)\r\n            score_head = model(batch_nodes, batch_adj, head, shifted_head, hht[0], hht[1], hht[2],\r\n                        htt[0], htt[1], htt[2])\r\n            scores_heads.append(score_head.detach())\r\n            del batch_nodes, batch_adj\r\n            batch_nodes, batch_adj = get_subgraph(tail, train_triple_dict, graph)\r\n            # get relative location according to the batch_nodes\r\n            shifted_tail = convert_index([tail], batch_nodes)\r\n            shifted_tail = torch.LongTensor(shifted_tail[0]).to(device)\r\n            batch_nodes = torch.LongTensor(batch_nodes.tolist()).to(device)\r\n            batch_adj = torch.from_numpy(batch_adj).to(device)\r\n            score_tail = model(batch_nodes, batch_adj, tail, shifted_tail, tht[0], tht[1], tht[2],\r\n                        ttt[0], ttt[1], ttt[2])\r\n            scores_tails.append(score_tail.detach())\r\n            del batch_nodes, batch_adj, head, shifted_head, hht, htt, tail, shifted_tail, tht, ttt\r\n            sys.stdout.write(\r\n                \'%d batches processed.\\r\' %\r\n                (current_idx)\r\n            )\r\n\r\n        # get head scores\r\n        scores_head = torch.cat(scores_heads, 0)\r\n        scores_head = torch.sum(scores_head, 1).squeeze()\r\n        assert scores_head.size(0) == num_ent\r\n        sorted_head_idx = np.argsort(scores_head.tolist())\r\n        topk_head = new_head_triple[sorted_head_idx][:10]\r\n\r\n        #get tail socres\r\n        scores_tail = torch.cat(scores_tails, 0)\r\n        scores_tail = torch.sum(scores_tail, 1).squeeze()\r\n        sorted_tail_idx = np.argsort(scores_tail.tolist())\r\n        topk_tail = new_tail_triple[sorted_tail_idx][:10]\r\n\r\n        # predict and output top 10 triples\r\n        named_triples_head = convert_idx2name(topk_head, id2ent, ent2name, id2rel)\r\n        named_triples_tail = convert_idx2name(topk_tail, id2ent, ent2name, id2rel)\r\n        write_triples(named_triples_head, output_file)\r\n        write_triples(named_triples_tail, output_file)\r\n\r\n        mean_rank_result_head = mean_rank(new_head_triple, sorted_head_idx, correct, 0)\r\n        mean_rank_result_tail = mean_rank(new_tail_triple, sorted_tail_idx, correct, 1)\r\n        if mean_rank_result_head <= 10:\r\n            hitk_all += 1\r\n        if mean_rank_result_tail <= 10:\r\n            hitk_all += 1\r\n        mean_rank_head.append(mean_rank_result_head)\r\n        mean_rank_tail.append(mean_rank_result_tail)\r\n        del test\r\n    gc.collect()\r\n    output_file.close()\r\n\r\n    print(\'Final mean rank for head is %f\'%(np.mean(mean_rank_head)))\r\n    print(\'Final median rank for head is %f\' % np.median(mean_rank_head))\r\n    print(\'Final mean rank for tail is %f\' % (np.mean(mean_rank_tail)))\r\n    print(\'Final median rank for tail is %f\' % np.median(mean_rank_tail))\r\n    print(\'Final hit10 is %f\'%(hitk_all/(len(mean_rank_tail)+1)/2))\r\n    return hitk_all, mean_rank_head, mean_rank_tail\r\n\r\n\r\n\r\nt_total = time.time()\r\nprint(\'loading model from:\', args.model)\r\nif args.gpu:\r\n    state = torch.load(args.model)\r\nelse:\r\n    state = torch.load(args.model, map_location=lambda storage, loc: storage)\r\nstate_dict = state[\'state_dict\']\r\nmodel.load_state_dict(state_dict)\r\nstart_epoch = state[\'epoch\']\r\nbest_dev = state[\'best_prec1\']\r\ntest()\r\nprint(\'Test Finished!\')\r\nprint(\'Total time elapsed: {:.4f}s\'.format(time.time() - t_total))\r\n'"
Existing paper reading/train.py,20,"b'from __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport time\r\nimport argparse\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nfrom torch.utils import data\r\nimport os, sys, math, pickle, gc\r\n\r\nfrom utils.utils import convert_index, get_subgraph, adjust_sent_order, load_dict, load_graph, load_text, mean_rank\r\nfrom utils.data_loader import LinkPredictionDataset, LinkTestTotal, LinkTestDataset\r\nfrom model.GATA import GATA\r\nfrom collections import OrderedDict\r\n\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\r\n    \'--data_dir\', type=str, default=\'paper_reading/\'\r\n)\r\nparser.add_argument(\r\n    \'--epochs\', type=int, default=100, help=\'Number of epochs to train.\'\r\n)\r\nparser.add_argument(\r\n    \'--lr\', type=float, default=0.001, help=\'Initial learning rate.\'\r\n)\r\nparser.add_argument(\r\n    \'--gpu\', default=\'1\', type=int, help=\'default is 1. set 0 to disable use gpu.\'\r\n)\r\nparser.add_argument(\r\n    \'--weight_decay\', type=float, default=5e-4, help=\'Weight decay (L2 loss on parameters).\'\r\n)\r\nparser.add_argument(\r\n    \'--batch_size\', type=int, default=50, help=\'Size of a single batch\'\r\n)\r\nparser.add_argument(\r\n    \'--hidden\', type=int, default=8, help=\'Number of hidden units.\'\r\n)\r\nparser.add_argument(\r\n    \'--nb_heads\', type=int, default=8, help=\'Number of head attentions.\'\r\n)\r\nparser.add_argument(\r\n    \'--dropout\', type=float, default=0.6, help=\'Dropout rate (1 - keep probability).\'\r\n)\r\nparser.add_argument(\r\n    \'--alpha\', type=float, default=0.2, help=\'Alpha for the leaky_relu.\'\r\n)\r\nparser.add_argument(\r\n    \'--freq\', default=\'3\',  type=int, help=\'Min freq.\'\r\n)\r\nparser.add_argument(\r\n    \'--max_len\', default=\'100\', type=int, help=\'Max length of context text\'\r\n)\r\nparser.add_argument(\r\n    \'--margin\', type=int, default=1, help=\'Margin Value\'\r\n)\r\nparser.add_argument(\r\n    \'--patience\', type=int, default=30, help=\'Patience\'\r\n)\r\nparser.add_argument(\r\n    \'--load\', action=\'store_true\', help=\'Load dataset.\'\r\n)\r\nparser.add_argument(\r\n    \'--cont\', action=\'store_true\', help=\'Continue training.\'\r\n)\r\nparser.add_argument(\r\n    ""--model"", default=""models/GATA/best_dev_model.pth.tar"",\r\n    help=""Model location""\r\n)\r\nparser.add_argument(\r\n    ""--model_dp"", default=""models/"",\r\n    help=""model directory path""\r\n)\r\nargs = parser.parse_args()\r\ndevice = torch.device(\'cuda:0\' if torch.cuda.is_available()  and args.gpu == 1 else \'cpu\')\r\n\r\n# Initialize\r\n# load previously saved data\r\nif args.load:\r\n    state = pickle.load(open(\'dataset.pth\', \'rb\'))\r\n    parameters = state[\'parameters\']\r\n    graph = state[\'graph\']\r\n    text_f = state[\'text_f\']\r\n    mappings = state[\'mappings\']\r\n    num_ent = state[\'num_ent\']\r\n    num_rel = state[\'num_rel\']\r\n    id2ent = state[\'id2ent\']\r\n    print(""finish load"")\r\nelse:\r\n    embedding_dim = args.nb_heads * args.hidden\r\n    num_rel = int(open(os.path.join(args.data_dir, \'relation2id.txt\')).readline())\r\n    text_file = os.path.join(args.data_dir, \'entity_text_title_tokenized.json\')\r\n\r\n    mappings, text_f = load_text(text_file, args.freq, args.max_len)\r\n    ent_f = os.path.join(args.data_dir, \'entity2id.txt\')\r\n    id2ent, num_ent = load_dict(ent_f)\r\n    # Load Graph Data\r\n    graph, _ = load_graph(os.path.join(args.data_dir, \'train2id.txt\'), num_ent)\r\n\r\n    # Parse parameters\r\n    parameters = OrderedDict()\r\n    parameters[\'emb_dim\'] = embedding_dim\r\n    parameters[\'hid_dim\'] = args.hidden\r\n    parameters[\'out_dim\'] = args.hidden*args.nb_heads\r\n    parameters[\'num_voc\'] = len(mappings[\'idx2word\'])\r\n    parameters[\'num_heads\'] = args.nb_heads\r\n    parameters[\'num_ent\'] = num_ent\r\n    parameters[\'num_rel\'] = num_rel\r\n    parameters[\'dropout\'] = args.dropout\r\n    parameters[\'alpha\'] = args.alpha\r\n    parameters[\'margin\'] = args.margin\r\n    state = {\r\n        \'parameters\': parameters,\r\n        \'graph\': graph,\r\n        \'text_f\': text_f,\r\n        \'id2ent\': id2ent,\r\n        \'mappings\': mappings,\r\n        \'num_ent\': num_ent,\r\n        \'num_rel\': num_rel\r\n    }\r\n    pickle.dump(state, open(\'dataset.pth\', ""wb""))\r\n    print(""finish_dump"")\r\n\r\nmodel_dir = args.model_dp\r\nmodel_name = [\'GATA\']\r\nfor k, v in parameters.items():\r\n    if v == """":\r\n        continue\r\n    model_name.append(\'=\'.join((k, str(v))))\r\nmodel_dir = os.path.join(model_dir, \',\'.join(model_name[:-1]))\r\nos.makedirs(model_dir, exist_ok=True)\r\n\r\n# Load Positive and Negative Examples\r\nparams = {\'batch_size\': args.batch_size, \'shuffle\': True, \'collate_fn\': adjust_sent_order}\r\ntrain_set = LinkPredictionDataset(os.path.join(args.data_dir, \'train2id.txt\'), text_f, id2ent, num_ent)\r\ntrain_triple_dict = train_set.get_triple_dict()\r\ntrain_generator = data.DataLoader(train_set, **params)\r\nprint(\'Finish loading train\')\r\n\r\nvalid_set = LinkPredictionDataset(os.path.join(args.data_dir, \'valid2id.txt\'), text_f, id2ent, num_ent)\r\nvalid_generator = data.DataLoader(valid_set, **params)\r\nprint(\'Finish loading valid\')\r\n\r\nparams_test = {\'batch_size\': args.batch_size, \'shuffle\': False, \'collate_fn\': adjust_sent_order}\r\ntest_set = LinkTestTotal(os.path.join(args.data_dir, \'test2id.txt\'), num_ent)\r\nprint(\'Finish loading test\')\r\n\r\ny = torch.FloatTensor([-1])\r\ny = y.to(device)\r\n\r\n# Initialize Model\r\nmodel = GATA(**parameters)\r\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, weight_decay=args.weight_decay)\r\nmodel.to(device)\r\n\r\n# Train\r\ndef train(epoch):\r\n    print(""Epoch"", epoch)\r\n    t = time.time()\r\n    model.train(True)\r\n    torch.set_grad_enabled(True)\r\n    eloss = 0\r\n    for batch_idx, instance in enumerate(train_generator):\r\n        pos, neg, pht_bef, ptt_bef, nht_bef, ntt_bef = instance\r\n        pos = pos.to(device)\r\n        neg = neg.to(device)\r\n        # text information\r\n        pht = list(map(lambda x:x.to(device),pht_bef[0:3]))\r\n        ptt = list(map(lambda x:x.to(device),ptt_bef[0:3]))\r\n        nht = list(map(lambda x:x.to(device),nht_bef[0:3]))\r\n        ntt = list(map(lambda x:x.to(device),ntt_bef[0:3]))\r\n        batch_nodes, batch_adj = get_subgraph(pos, train_triple_dict, graph)\r\n        # get relative location according to the batch_nodes\r\n        shifted_pos, shifted_neg = convert_index([pos, neg], batch_nodes)\r\n        batch_nodes = torch.LongTensor(batch_nodes.tolist()).to(device)\r\n        batch_adj = torch.from_numpy(batch_adj).to(device)\r\n        shifted_pos = torch.LongTensor(shifted_pos).to(device)\r\n        shifted_neg = torch.LongTensor(shifted_neg).to(device)\r\n        score_pos = model(batch_nodes, batch_adj, pos, shifted_pos, pht[0], pht[1], pht[2],\r\n                    ptt[0], ptt[1], ptt[2])\r\n        score_neg = model(batch_nodes, batch_adj, neg, shifted_neg, nht[0], nht[1], nht[2],\r\n                    ntt[0], ntt[1], ntt[2])\r\n        loss_train = F.margin_ranking_loss(score_pos, score_neg, y, margin=args.margin)\r\n        sys.stdout.write(\r\n            \'%d batches processed. current train batch loss: %f\\r\' %\r\n            (batch_idx, loss_train.item())\r\n        )\r\n        eloss += loss_train.item()\r\n        loss_train.backward()\r\n        del batch_nodes, batch_adj, shifted_pos, shifted_neg, pos, neg, pht_bef, ptt_bef, nht_bef, ntt_bef\r\n        optimizer.step()\r\n        if batch_idx%500==0:\r\n            gc.collect()\r\n    print(\'\\n\')\r\n    print(\'Epoch: {:04d}\'.format(epoch+1),\r\n          \'loss_train: {:.4f}\'.format(eloss/(batch_idx+1)),\r\n          \'time: {:.4f}s\'.format(time.time() - t))\r\n\r\n    return eloss\r\n\r\n\r\n# Valid\r\ndef validate(epoch):\r\n    t = time.time()\r\n    model.eval()\r\n    torch.set_grad_enabled(False)\r\n    eloss = 0\r\n    for batch_idx, instance in enumerate(valid_generator):\r\n        pos, neg, pht_bef, ptt_bef, nht_bef, ntt_bef = instance\r\n        pos = pos.to(device)\r\n        neg = neg.to(device)\r\n        # text information\r\n        pht = list(map(lambda x:x.to(device),pht_bef[0:3]))\r\n        ptt = list(map(lambda x:x.to(device),ptt_bef[0:3]))\r\n        nht = list(map(lambda x:x.to(device),nht_bef[0:3]))\r\n        ntt = list(map(lambda x:x.to(device),ntt_bef[0:3]))\r\n        batch_nodes, batch_adj = get_subgraph(pos, train_triple_dict, graph)\r\n        # get relative location according to the batch_nodes\r\n        shifted_pos, shifted_neg = convert_index([pos, neg], batch_nodes)\r\n        batch_nodes = torch.LongTensor(batch_nodes.tolist()).to(device)\r\n        batch_adj = torch.from_numpy(batch_adj).to(device)\r\n        shifted_pos = torch.LongTensor(shifted_pos).to(device)\r\n        shifted_neg = torch.LongTensor(shifted_neg).to(device)\r\n        score_pos = model(batch_nodes, batch_adj, pos, shifted_pos, pht[0], pht[1], pht[2],\r\n                    ptt[0], ptt[1], ptt[2])\r\n        score_neg = model(batch_nodes, batch_adj, neg, shifted_neg, nht[0], nht[1], nht[2],\r\n                    ntt[0], ntt[1], ntt[2])\r\n        loss_train = F.margin_ranking_loss(score_pos, score_neg, y, margin=args.margin)\r\n        sys.stdout.write(\r\n            \'%d batches processed. current valid batch loss: %f\\r\' %\r\n            (batch_idx, loss_train.item())\r\n        )\r\n        eloss += loss_train.item()\r\n        del batch_nodes, batch_adj, shifted_pos, shifted_neg, pos, neg, pht_bef, ptt_bef, nht_bef, ntt_bef\r\n        if batch_idx%500==0:\r\n            gc.collect()\r\n    print(\'Epoch: {:04d}\'.format(epoch+1),\r\n          \'loss_valid: {:.4f}\'.format(eloss/(batch_idx+1)),\r\n          \'time: {:.4f}s\'.format(time.time() - t))\r\n\r\n    return eloss\r\n\r\n\r\nt_total = time.time()\r\nbest_dev = math.inf\r\nbad_counter = 0\r\nstart_epoch = 0\r\nif args.cont:\r\n    print(\'loading model from:\', args.model)\r\n    if args.gpu:\r\n        state = torch.load(args.model)\r\n    else:\r\n        state = torch.load(args.model, map_location=lambda storage, loc: storage)\r\n    state_dict = state[\'state_dict\']\r\n    model.load_state_dict(state_dict)\r\n    state_dict = state[\'optimizer\']\r\n    optimizer.load_state_dict(state_dict)\r\n    start_epoch = state[\'epoch\']\r\n    best_dev = state[\'best_prec1\']\r\nfor epoch in range(args.epochs):\r\n    train(start_epoch+epoch)\r\n    torch.cuda.empty_cache()\r\n    current_valid = validate(start_epoch+epoch)\r\n    torch.cuda.empty_cache()\r\n    if current_valid < best_dev:\r\n        best_dev=current_valid\r\n        print(\'new best score on dev: %.4f\' % best_dev)\r\n        print(\'saving the current model to disk...\')\r\n\r\n        state = {\r\n            \'epoch\': start_epoch+epoch + 1,\r\n            \'state_dict\': model.state_dict(),\r\n            \'best_prec1\': best_dev,\r\n            \'optimizer\': optimizer.state_dict(),\r\n            \'parameters\': parameters,\r\n            \'optimizer\': optimizer.state_dict(),\r\n            \'mappings\': mappings\r\n        }\r\n        torch.save(state, os.path.join(model_dir, \'best_dev_model.pth.tar\'))\r\n        bad_counter = 0\r\n    else:\r\n        bad_counter += 1\r\n\r\n    if bad_counter == args.patience:\r\n        break\r\n\r\nprint(\'Optimization Finished!\')\r\nprint(\'Total time elapsed: {:.4f}s\'.format(time.time() - t_total))\r\n'"
New paper writing/eval.py,0,"b'import pickle\nimport collections\nimport sys\n\nsys.path.append(\'pycocoevalcap\')\nfrom pycocoevalcap.bleu.bleu import Bleu\nfrom pycocoevalcap.rouge.rouge import Rouge\nfrom pycocoevalcap.meteor.meteor import Meteor\n#from pycocoevalcap.cider.cider import Cider\n\nclass Evaluate(object):\n    def __init__(self):\n        self.scorers = [\n            (Bleu(4),  [""Bleu_1"", ""Bleu_2"", ""Bleu_3"", ""Bleu_4""]),\n            # (Meteor(), ""METEOR""),\n            (Rouge(), ""ROUGE_L"")\n        ]#,        (Cider(), ""CIDEr"")\n\n    def convert(self, data):\n        if isinstance(data, basestring):\n            return data.encode(\'utf-8\')\n        elif isinstance(data, collections.Mapping):\n            return dict(map(convert, data.items()))\n        elif isinstance(data, collections.Iterable):\n            return type(data)(map(convert, data))\n        else:\n            return data\n\n    def score(self, ref, hypo):\n        final_scores = {}\n        for scorer, method in self.scorers:\n            score, scores = scorer.compute_score(ref, hypo)\n            if type(score) == list:\n                for m, s in zip(method, score):\n                    final_scores[m] = s\n            else:\n                final_scores[method] = score\n\n        return final_scores\n\n    def evaluate(self, get_scores=True, live=False, **kwargs):\n        if live:\n            temp_ref = kwargs.pop(\'ref\', {})\n            cand = kwargs.pop(\'cand\', {})\n        else:\n            reference_path = kwargs.pop(\'ref\', \'\')\n            candidate_path = kwargs.pop(\'cand\', \'\')\n\n            # load caption data\n            with open(reference_path, \'rb\') as f:\n                temp_ref = pickle.load(f)\n            with open(candidate_path, \'rb\') as f:\n                cand = pickle.load(f)\n\n        # make dictionary\n        hypo = {}\n        ref = {}\n        i = 0\n        for vid, caption in cand.items():\n            hypo[i] = [caption]\n            ref[i] = temp_ref[vid]\n            i += 1\n\n        # compute scores\n        final_scores = self.score(ref, hypo)\n        # """"""\n        # print out scores\n        # print (\'Bleu_1:\\t\', final_scores[\'Bleu_1\'])\n        # print (\'Bleu_2:\\t\', final_scores[\'Bleu_2\'])\n        # print (\'Bleu_3:\\t\', final_scores[\'Bleu_3\'])\n        print (\'Bleu_4:\\t\', final_scores[\'Bleu_4\'])\n        # print (\'METEOR:\\t\', final_scores[\'METEOR\'])\n        print (\'ROUGE_L:\', final_scores[\'ROUGE_L\'])\n        # print (\'CIDEr:\\t\', final_scores[\'CIDEr\'])\n        # """"""\n\n        if get_scores:\n            return final_scores\n\n\nif __name__ == \'__main__\':\n    cand = {\'generated_description1\': \'how are you\', \'generated_description2\': \'Hello how are you\'}\n    ref = {\'generated_description1\': [\'what are you\', \'where are you\'],\n           \'generated_description2\': [\'Hello how are you\', \'Hello how is your day\']}\n    x = Evaluate()\n    x.evaluate(live=True, cand=cand, ref=ref)\n'"
New paper writing/eval_final.py,0,"b'import pickle\nimport os\nimport collections\nimport sys\n\nsys.path.append(\'pycocoevalcap\')\nfrom pycocoevalcap.bleu.bleu import Bleu\nfrom pycocoevalcap.rouge.rouge import Rouge\nfrom pycocoevalcap.meteor.meteor import Meteor\n#from pycocoevalcap.cider.cider import Cider\n\nclass Evaluate(object):\n    def __init__(self):\n        self.scorers = [\n            (Bleu(4), [""Bleu_1"", ""Bleu_2"", ""Bleu_3"", ""Bleu_4""]),\n            (Meteor(), ""METEOR""),\n            (Rouge(), ""ROUGE_L"")\n        ]#,        (Cider(), ""CIDEr"")\n\n    def convert(self, data):\n        if isinstance(data, basestring):\n            return data.encode(\'utf-8\')\n        elif isinstance(data, collections.Mapping):\n            return dict(map(convert, data.items()))\n        elif isinstance(data, collections.Iterable):\n            return type(data)(map(convert, data))\n        else:\n            return data\n\n    def score(self, ref, hypo):\n        final_scores = {}\n        for scorer, method in self.scorers:\n            score, scores = scorer.compute_score(ref, hypo)\n            if type(score) == list:\n                for m, s in zip(method, score):\n                    final_scores[m] = s\n            else:\n                final_scores[method] = score\n\n        return final_scores\n\n    def evaluate(self, get_scores=True, live=False, **kwargs):\n        if live:\n            temp_ref = kwargs.pop(\'ref\', {})\n            cand = kwargs.pop(\'cand\', {})\n        else:\n            reference_path = kwargs.pop(\'ref\', \'\')\n            candidate_path = kwargs.pop(\'cand\', \'\')\n\n            # load caption data\n            with open(reference_path, \'rb\') as f:\n                temp_ref = pickle.load(f)\n            with open(candidate_path, \'rb\') as f:\n                cand = pickle.load(f)\n\n        # make dictionary\n        hypo = {}\n        ref = {}\n        i = 0\n        for vid, caption in cand.items():\n            hypo[i] = [caption]\n            ref[i] = temp_ref[vid]\n            i += 1\n\n        # compute scores\n        final_scores = self.score(ref, hypo)\n        #""""""\n        # print out scores\n        print (\'Bleu_1:\\t\', final_scores[\'Bleu_1\'])\n        print (\'Bleu_2:\\t\', final_scores[\'Bleu_2\'])\n        print (\'Bleu_3:\\t\', final_scores[\'Bleu_3\'])\n        print (\'Bleu_4:\\t\', final_scores[\'Bleu_4\'])\n        print (\'METEOR:\\t\', final_scores[\'METEOR\'])\n        print (\'ROUGE_L:\', final_scores[\'ROUGE_L\'])\n        #print (\'CIDEr:\\t\', final_scores[\'CIDEr\'])\n        #""""""\n\n        if get_scores:\n            return final_scores\n\n\nif __name__ == \'__main__\':\n    cand = {\'generated_description1\': \'how are you\', \'generated_description2\': \'Hello how are you\'}\n    ref = {\'generated_description1\': [\'what are you\', \'where are you\'],\n           \'generated_description2\': [\'Hello how are you\', \'Hello how is your day\']}\n    x = Evaluate()\n    x.evaluate(live=True, cand=cand, ref=ref)\n'"
New paper writing/input.py,9,"b'import torch\nimport pickle\nimport argparse\nimport torch.nn as nn\nfrom loader.preprocessing import prepare_mapping, filter_stopwords\n\nfrom memory_generator.seq2seq import Seq2seq\nfrom memory_generator.Encoder import EncoderRNN\nfrom memory_generator.Encoder import TermEncoder\nfrom memory_generator.predictor import Predictor\nfrom memory_generator.Decoder import DecoderRNN\n\n# Read parameters from command line\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    ""--model"", default=""models/memory/best_dev_model.pth.tar"",\n    help=""Model location""\n)\nparser.add_argument(\n    ""--gpu"", default=""1"",\n    type=int, help=""default is 1. set 0 to disable use gpu.""\n)\nparser.add_argument(\n    ""--batch_size"", default=""100"",\n    type=int, help=""Batch size.""\n)\nparser.add_argument(\n    ""--max_len"", default=""150"",\n    type=int, help=""Max length.""\n)\nparser.add_argument(\n    ""--stop_words"", default=""1"",\n    type=int, help=""default is 1. set 0 to disable use stopwords.""\n)\nparser.add_argument(\n    ""--data_path"", default=""data"",\n    help=""data directory path""\n)\nargs = parser.parse_args()\n\n# input()\nprint(\'loading model from:\', args.model)\nif args.gpu:\n    state = torch.load(args.model)\nelse:\n    state = torch.load(args.model, map_location=lambda storage, loc: storage)\n\nparameters = state[\'parameters\']\n# Data parameters\nlower = parameters[\'lower\']\nparameters[\'gpu\'] = args.gpu == 1\n\ndata = pickle.load(open(args.data_path + \'/dataset.pth\', \'rb\'))\nwords = data[\'words\']\ntry:\n    mappings = state[\'mappings\']\nexcept:\n    mappings, words_freq = prepare_mapping(words, lower, parameters[\'freq\'])\nstate_dict = state[\'state_dict\']\n\n# print model parameters\nprint(\'Model parameters:\')\nfor k, v in parameters.items():\n    print(\'%s=%s\' % (k, v))\n\n# Index data\nword2id = mappings[\'word2id\']\nid2word = mappings[\'id2word\']\nvocab_size = len(mappings[\'id2word\'])\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() and parameters[\'gpu\'] else ""cpu"")\nembedding = nn.Embedding(vocab_size, parameters[\'word_dim\'], padding_idx=0)\n\n\nref_encoder = EncoderRNN(vocab_size, embedding, parameters[\'word_dim\'], parameters[\'input_dropout_p\'])\nterm_encoder = TermEncoder(embedding, parameters[\'input_dropout_p\'])\ndecoder = DecoderRNN(vocab_size, embedding, **parameters)\nmodel = Seq2seq(ref_encoder, term_encoder, decoder)\nmodel.load_state_dict(state_dict)\nmodel = model.to(device)\nstopwords = filter_stopwords(word2id)\n# print(stopwords)\n#\n# training starts\n#\nwhile (1):\n    batch_s, batch_o_s, source_len, batch_term, batch_o_term, term_len = [], [], [], [], [], []\n    seq_str = input(""Source:\\n"")\n    seq = seq_str.strip().split(\' \')\n    _source, source_oov,_o_source, list_oovs = [],[],[],[]\n    for _s in seq:\n        try:\n            _source.append(word2id[_s])\n        except KeyError:\n            _source.append(word2id[\'<unk>\'])\n            if _s not in source_oov:\n                _o_source.append(len(source_oov) + vocab_size)\n                source_oov.append(_s)\n            else:\n                _o_source.append(source_oov.index(_s) + vocab_size)\n        else:\n            _o_source.append(word2id[_s])\n    terms = input(""terms:\\n"")\n    terms = terms.strip().split(\' \')\n    _term, _o_term = [], []\n    for _t in terms:\n        try:\n            _term.append(word2id[_t])\n        except KeyError:\n            _term.append(word2id[\'<unk>\'])\n            if _t not in source_oov:\n                _o_term.append(len(source_oov) + vocab_size)\n                source_oov.append(_t)\n            else:\n                _o_term.append(source_oov.index(_t) + vocab_size)\n        else:\n            _o_term.append(word2id[_t])\n    max_source_oov = len(source_oov)\n    source_len.append(len(_o_source))\n    oovidx2word = {idx: word for idx, word in enumerate(source_oov)}\n    list_oovs.append(oovidx2word)\n    batch_s = torch.LongTensor([_source]).cuda()\n    batch_o_s = torch.LongTensor([_o_source]).cuda()\n\n    batch_term = torch.LongTensor([_term]).cuda()\n    batch_o_term = torch.LongTensor([_o_term]).cuda()\n\n    source_len = torch.LongTensor(source_len).cuda()\n\n    predictor = Predictor(model, id2word, vocab_size)\n    print(""Output:"")\n    predictor.predict_beam(batch_s, batch_o_s, source_len, max_source_oov, batch_term, batch_o_term, list_oovs,\n                           stopwords, args.stop_words)\n    print()\n'"
New paper writing/test.py,4,"b'import gc\nimport os\nimport time\nimport torch\nimport pickle\nimport argparse\nimport torch.nn as nn\n\nfrom eval_final import Evaluate\nfrom loader.preprocessing import prepare_mapping, AssembleMem, printcand, filter_stopwords\nfrom loader.loader import load_file_with_terms\n\nfrom memory_generator.seq2seq import Seq2seq\nfrom memory_generator.Encoder import EncoderRNN\nfrom memory_generator.Encoder import TermEncoder\nfrom memory_generator.predictor import Predictor\nfrom memory_generator.Decoder import DecoderRNN\n\n# Read parameters from command line\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    ""--model"", default=""models/memory/best_dev_model.pth.tar"",\n    help=""Model location""\n)\nparser.add_argument(\n    ""--gpu"", default=""1"",\n    type=int, help=""default is 1. set 0 to disable use gpu.""\n)\nparser.add_argument(\n    ""--max_len"", default=""150"",\n    type=int, help=""Max length.""\n)\nparser.add_argument(\n    ""--data_path"", default=""data"",\n    help=""data directory path""\n)\nparser.add_argument(\n    ""--output"", default=""data.txt"",\n    help=""data directory path""\n)\nparser.add_argument(\n    ""--stop_words"", default=""1"",\n    type=int, help=""default is 1. set 0 to disable use stopwords.""\n)\nargs = parser.parse_args()\n\nprint(\'loading model from:\', args.model)\nif args.gpu:\n    state = torch.load(args.model)\nelse:\n    state = torch.load(args.model, map_location=lambda storage, loc: storage)\n\nparameters = state[\'parameters\']\n# Data parameters\nlower = parameters[\'lower\']\nparameters[\'gpu\'] = args.gpu == 1\n\ndata = pickle.load(open(args.data_path + \'/dataset.pth\', \'rb\'))\nwords = data[\'words\']\n_, t_dataset = load_file_with_terms(args.data_path + \'/test.txt\')\ntry:\n    mappings = state[\'mappings\']\nexcept:\n    mappings, words_freq = prepare_mapping(words, lower, parameters[\'freq\'])\nstate_dict = state[\'state_dict\']\n\n# print model parameters\nprint(\'Model parameters:\')\nfor k, v in parameters.items():\n    print(\'%s=%s\' % (k, v))\n\n# Index data\nt_dataset = AssembleMem(t_dataset, mappings[\'word2id\'], lower, 1, args.max_len, parameters[\'gpu\'])\nprint(""Vocabulary size"", t_dataset.vocab_size)\nprint(""%i sentences in test."" % (t_dataset.len))\n\nword2id = mappings[\'word2id\']\nid2word = mappings[\'id2word\']\nvocab_size = len(mappings[\'id2word\'])\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() and parameters[\'gpu\'] else ""cpu"")\nembedding = nn.Embedding(t_dataset.vocab_size, parameters[\'word_dim\'], padding_idx=0)\n\n\nref_encoder = EncoderRNN(vocab_size, embedding, parameters[\'word_dim\'], parameters[\'input_dropout_p\'])\nterm_encoder = TermEncoder(embedding, parameters[\'input_dropout_p\'])\ndecoder = DecoderRNN(vocab_size, embedding, **parameters)\nmodel = Seq2seq(ref_encoder, term_encoder, decoder)\nmodel.load_state_dict(state_dict)\nmodel = model.to(device)\nstopwords = filter_stopwords(word2id)\n#\n# training starts\n#\nsince = time.time()\nbest_dev = 0.0\nepoch_examples_total = t_dataset.len\ntrain_loader = t_dataset.corpus\nlen_batch_t = len(train_loader)\n\n\npredictor = Predictor(model, id2word, vocab_size)\neval_f = Evaluate()\nprint(""Start computing"")\ncands, refs, titles, terms = predictor.preeval_batch_beam(t_dataset, False, stopwords, args.stop_words)\nprint(""Start Evaluating"")\nfinal_scores = eval_f.evaluate(live=True, cand=cands, ref=refs)\n\n\nprintcand(args.output, cands)\n'"
New paper writing/train.py,9,"b'import gc\r\nimport os\r\nimport sys\r\nimport time\r\nimport torch\r\nimport pickle\r\nimport argparse\r\nimport torch.nn as nn\r\nfrom collections import OrderedDict\r\n\r\nfrom eval import Evaluate\r\nfrom loader.logger import Tee\r\nfrom loader.loader import load_file_with_terms\r\nfrom loader.preprocessing import prepare_mapping, AssembleMem\r\n\r\nfrom utils.optim import get_optimizer\r\nfrom memory_generator.seq2seq import Seq2seq\r\nfrom memory_generator.Encoder import EncoderRNN\r\nfrom memory_generator.Encoder import TermEncoder\r\nfrom memory_generator.predictor import Predictor\r\nfrom memory_generator.Decoder import DecoderRNN\r\n\r\n# Read parameters from command line\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\r\n    ""--lower"", default=\'0\',\r\n    type=int, help=""Lowercase words (this will not affect character inputs)""\r\n)\r\nparser.add_argument(\r\n    ""--word_dim"", default=""128"",\r\n    type=int, help=""Token embedding dimension""\r\n)\r\nparser.add_argument(\r\n    ""--h"", default=""8"",\r\n    type=int, help=""No of attention heads""\r\n)\r\nparser.add_argument(\r\n    ""--hop"", default=""3"",\r\n    type=int, help=""No of Memory layers""\r\n)\r\nparser.add_argument(\r\n    ""--dropout"", default=""0.2"",\r\n    type=float, help=""Dropout on the embeddings (0 = no dropout)""\r\n)\r\nparser.add_argument(\r\n    ""--layer_dropout"", default=""0.2"",\r\n    type=float, help=""Dropout on the layer (0 = no dropout)""\r\n)\r\nparser.add_argument(\r\n    ""--lr_method"", default=""adam"",\r\n    help=""Learning method (SGD, Adadelta, Adam..)""\r\n)\r\nparser.add_argument(\r\n    ""--lr_rate"", default=""0.001"",\r\n    type=float, help=""Learning method (SGD, Adadelta, Adam..)""\r\n)\r\nparser.add_argument(\r\n    ""--model_dp"", default=""models/"",\r\n    help=""model directory path""\r\n)\r\nparser.add_argument(\r\n    ""--pre_emb"", default="""",\r\n    help=""Location of pretrained embeddings""\r\n)\r\nparser.add_argument(\r\n    ""--gpu"", default=""1"",\r\n    type=int, help=""default is 1. set 0 to disable use gpu.""\r\n)\r\nparser.add_argument(\r\n    ""--num_epochs"", default=""100"",\r\n    type=int, help=""Number of training epochs""\r\n)\r\nparser.add_argument(\r\n    ""--batch_size"", default=""2"",\r\n    type=int, help=""Batch size.""\r\n)\r\nparser.add_argument(\r\n    ""--max_len"", default=""150"",\r\n    type=int, help=""Max length.""\r\n)\r\nparser.add_argument(\r\n    ""--freq"", default=""5"",\r\n    type=int, help=""Min freq.""\r\n)\r\nparser.add_argument(\r\n    ""--cont"", action=\'store_true\', help=""Continue training.""\r\n)\r\nparser.add_argument(\r\n    ""--model"", default=""models/memory/best_dev_model.pth.tar"",\r\n    help=""Model location""\r\n)\r\nparser.add_argument(\r\n    ""--load"", action=\'store_true\', help=""Load dataset.""\r\n)\r\nparser.add_argument(\r\n    ""--data_path"", default=""data"",\r\n    help=""data directory path""\r\n)\r\nargs = parser.parse_args()\r\n\r\n# Parse parameters\r\nparameters = OrderedDict()\r\nparameters[\'lower\'] = args.lower == 1\r\nparameters[\'freq\'] = args.freq\r\nparameters[\'word_dim\'] = args.word_dim\r\nparameters[\'h\'] = args.h\r\nparameters[\'hop\'] = args.hop\r\nparameters[\'pre_emb\'] = args.pre_emb\r\nparameters[\'input_dropout_p\'] = args.dropout\r\nparameters[\'layer_dropout\'] = args.layer_dropout\r\nparameters[\'gpu\'] = args.gpu == 1\r\nparameters[\'batch_size\'] = args.batch_size\r\nparameters[\'max_len\'] = args.max_len\r\nparameters[\'gpu\'] = args.gpu == 1\r\nparameters[\'lr_method\'] = args.lr_method\r\nparameters[\'lr_rate\'] = args.lr_rate\r\nparameters[\'data_path\'] = args.data_path\r\n# Check parameters validity\r\nassert os.path.isdir(args.data_path)\r\nassert 0. <= parameters[\'input_dropout_p\'] < 1.0\r\nassert 0. <= parameters[\'layer_dropout\'] < 1.0\r\nassert not parameters[\'pre_emb\'] or parameters[\'word_dim\'] > 0\r\nassert not parameters[\'pre_emb\'] or os.path.isfile(parameters[\'pre_emb\'])\r\nmodel_dir = args.model_dp\r\n\r\nmodel_name = [\'memory\']\r\nfor k, v in parameters.items():\r\n    if v == """":\r\n        continue\r\n    if k == \'pre_emb\':\r\n        v = os.path.basename(v)\r\n    model_name.append(\'=\'.join((k, str(v))))\r\nmodel_dir = os.path.join(model_dir, \',\'.join(model_name[:-1]))\r\nos.makedirs(model_dir, exist_ok=True)\r\n\r\n# register logger to save print(messages to both stdout and disk)\r\ntraining_log_path = os.path.join(model_dir, \'training_log.txt\')\r\nif os.path.exists(training_log_path):\r\n    os.remove(training_log_path)\r\nf = open(training_log_path, \'w\')\r\nsys.stdout = Tee(sys.stdout, f)\r\n\r\n# print model parameters\r\nprint(""Model location: %s"" % model_dir)\r\nprint(\'Model parameters:\')\r\nfor k, v in parameters.items():\r\n    print(\'%s=%s\' % (k, v))\r\n\r\n# Data parameters\r\nlower = parameters[\'lower\']\r\n\r\n# load previously saved data\r\nif args.load:\r\n    state = pickle.load(open(args.data_path + \'/dataset.pth\', \'rb\'))\r\n    words = state[\'words\']\r\n    r_dataset = state[\'r_dataset\']\r\n    v_dataset = state[\'v_dataset\']\r\n    t_dataset = state[\'t_dataset\']\r\nelse:\r\n    words = []\r\n    r_words, r_dataset = load_file_with_terms(args.data_path + \'/train.txt\')\r\n    words.extend(r_words)\r\n    v_words, v_dataset = load_file_with_terms(args.data_path + \'/valid.txt\')\r\n    t_words, t_dataset = load_file_with_terms(args.data_path + \'/test.txt\')\r\n    state = {\r\n        \'words\': words,\r\n        \'r_dataset\': r_dataset,\r\n        \'v_dataset\': v_dataset,\r\n        \'t_dataset\': t_dataset\r\n    }\r\n    pickle.dump(state, open(args.data_path + \'/dataset.pth\', ""wb""))\r\n\r\nmappings, words_freq = prepare_mapping(words, lower, args.freq)\r\nparameters[\'unk_id\'] = mappings[\'word2id\'][\'<unk>\']\r\nparameters[\'sos_id\'] = mappings[\'word2id\'][\'<sos>\']\r\nparameters[\'eos_id\'] = mappings[\'word2id\'][\'<eos>\']\r\n\r\n# Index data\r\nr_dataset = AssembleMem(r_dataset, mappings[\'word2id\'], lower, args.batch_size, args.max_len, parameters[\'gpu\'])\r\nv_dataset = AssembleMem(v_dataset, mappings[\'word2id\'], lower, args.batch_size, args.max_len, parameters[\'gpu\'])\r\nprint(""%i / %i pairs in train / dev."" % (r_dataset.len, v_dataset.len))\r\n\r\nword2id = mappings[\'word2id\']\r\nid2word = mappings[\'id2word\']\r\nvocab_size = len(mappings[\'id2word\'])\r\n\r\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() and parameters[\'gpu\'] else ""cpu"")\r\n# model initialization\r\nembedding = nn.Embedding(r_dataset.vocab_size, args.word_dim, padding_idx=0)\r\nref_encoder = EncoderRNN(vocab_size, embedding, parameters[\'word_dim\'], parameters[\'input_dropout_p\'])\r\nterm_encoder = TermEncoder(embedding, parameters[\'input_dropout_p\'])\r\ndecoder = DecoderRNN(vocab_size, embedding, **parameters)\r\nmodel = Seq2seq(ref_encoder, term_encoder, decoder)\r\nmodel = model.to(device)\r\n\r\noptimizer = get_optimizer(model, parameters[\'lr_method\'], parameters[\'lr_rate\'])\r\n#\r\n# training starts\r\n#\r\nsince = time.time()\r\nbest_dev = 0.0\r\nnum_epochs = args.num_epochs\r\nepoch_examples_total = r_dataset.len\r\ntrain_loader = r_dataset.corpus\r\nlen_batch_t = len(train_loader)\r\nprint(\'train batches\', len_batch_t)\r\nstart_epoch = 0\r\n# continue training\r\nif args.cont:\r\n    print(\'loading model from:\', args.model)\r\n    if args.gpu:\r\n        state = torch.load(args.model)\r\n    else:\r\n        state = torch.load(args.model, map_location=lambda storage, loc: storage)\r\n    state_dict = state[\'state_dict\']\r\n    model.load_state_dict(state_dict)\r\n    state_dict = state[\'optimizer\']\r\n    optimizer.load_state_dict(state_dict)\r\n    start_epoch = state[\'epoch\']\r\n    best_dev = state[\'best_prec1\']\r\n\r\nfor epoch in range(num_epochs):\r\n    print(\'-\' * 10)\r\n    print(\'Epoch {}/{}\'.format(epoch + start_epoch, num_epochs - 1))\r\n    # epoch start time\r\n    time_epoch_start = time.time()\r\n\r\n    # train\r\n    model.train(True)\r\n    torch.set_grad_enabled(True)\r\n    epoch_loss = 0\r\n    for batch_idx in range(len_batch_t):\r\n        batch_s, batch_o_s, source_len, max_source_oov, batch_term, batch_o_term, batch_t, \\\r\n         batch_o_t = r_dataset.get_batch(batch_idx)\r\n        losses = model(batch_s, batch_o_s, source_len, max_source_oov, batch_term, batch_o_term, batch_t,\r\n                       batch_o_t, teacher_forcing_ratio=1)\r\n        batch_loss = losses.mean()\r\n        # print(losses)\r\n\r\n        model.zero_grad()\r\n        batch_loss.backward()\r\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\r\n        optimizer.step()\r\n        num_examples = batch_s.size(0)\r\n        loss = batch_loss.item()\r\n        epoch_loss += num_examples * loss\r\n        sys.stdout.write(\r\n            \'%d batches processed. current batch loss: %f\\r\' %\r\n            (batch_idx, loss)\r\n        )\r\n        sys.stdout.flush()\r\n        del batch_s, batch_o_s, batch_t, batch_o_t, source_len, batch_term, batch_o_term\r\n    gc.collect()\r\n    # torch.cuda.empty_cache()\r\n    epoch_loss_avg = epoch_loss / float(epoch_examples_total)\r\n    log_msg = ""Finished epoch %d: Train %s: %.4f"" % (epoch + start_epoch, ""Avg NLLLoss"", epoch_loss_avg)\r\n    print()\r\n    print(log_msg)\r\n\r\n    predictor = Predictor(model, id2word, vocab_size)\r\n    eval_f = Evaluate()\r\n    print(""Start Evaluating"")\r\n    cand, ref, titles, terms = predictor.preeval_batch(v_dataset)\r\n    final_scores = eval_f.evaluate(live=True, cand=cand, ref=ref)\r\n    final_scores[\'Bleu_4\'] *= 10.0\r\n    epoch_score = 2*final_scores[\'ROUGE_L\']*final_scores[\'Bleu_4\']/(final_scores[\'Bleu_4\']+ final_scores[\'ROUGE_L\'])\r\n    if epoch_score > best_dev:\r\n            best_dev = epoch_score\r\n            print(\'new best score on dev: %.4f\' % best_dev)\r\n            print(\'saving the current model to disk...\')\r\n\r\n            state = {\r\n                \'epoch\': epoch + 1,\r\n                \'parameters\': parameters,\r\n                \'state_dict\': model.state_dict(),\r\n                \'best_prec1\': best_dev,\r\n                \'optimizer\': optimizer.state_dict(),\r\n                \'mappings\': mappings\r\n            }\r\n            torch.save(state, os.path.join(model_dir, \'best_dev_model.pth.tar\'))\r\n\r\n            print(""Examples"")\r\n            print(""Output:"", cand[1])\r\n            print(""Refer:"", ref[1])\r\n    # epoch end time\r\n    time_epoch_end = time.time()\r\n    # torch.cuda.empty_cache()\r\n    print(\'epoch training time: %f seconds\' % round(\r\n        (time_epoch_end - time_epoch_start), 2))\r\n    print(\'best dev: \', best_dev)\r\n'"
Existing paper reading/model/GAT.py,3,"b""import torch.nn as nn\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom .graph_attention import GraphAttentionLayer\r\n\r\n\r\nclass GAT(nn.Module):\r\n    def __init__(self, nfeat, nhid, dropout, alpha, nheads):\r\n        super(GAT, self).__init__()\r\n        self.dropout = dropout\r\n        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\r\n        for i, attention in enumerate(self.attentions):\r\n            self.add_module('attention_{}'.format(i), attention)\r\n\r\n\r\n    def forward(self, x, adj):\r\n        x = F.dropout(x, self.dropout, training=self.training)\r\n        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\r\n        x = F.dropout(x, self.dropout, training=self.training)\r\n        return x\r\n"""
Existing paper reading/model/GATA.py,3,"b'# --------- Link Prediction Model with both TAT and GAT contained -----------\r\nimport torch.nn as nn\r\nimport torch\r\nfrom .GAT import GAT\r\nfrom .TAT import TAT\r\n\r\n\r\nclass GATA(nn.Module):\r\n    def __init__(self, emb_dim, hid_dim, out_dim, num_voc, num_heads, num_ent, num_rel, dropout, alpha, **kwargs):\r\n        super(GATA, self).__init__()\r\n        self.ent_embedding = nn.Embedding(num_ent, emb_dim)\r\n        self.rel_embedding = nn.Embedding(num_rel, emb_dim)\r\n        self.graph = GAT(nfeat=emb_dim, nhid=hid_dim, dropout=dropout, nheads=num_heads, alpha=alpha)\r\n        self.text = TAT(emb_dim, num_voc)\r\n        self.gate = nn.Embedding(num_ent, out_dim)\r\n\r\n    def forward(self, nodes, adj, pos, shifted_pos, h_sents, h_order, h_lengths, t_sents, t_order, t_lengths):\r\n        node_features = self.ent_embedding(nodes)\r\n        graph = self.graph(node_features, adj)\r\n        head_graph = graph[[shifted_pos[:, 0].squeeze()]]\r\n        tail_graph = graph[[shifted_pos[:, 1].squeeze()]]\r\n\r\n        head_text = self.text(h_sents, h_order, h_lengths, node_features[[shifted_pos[:, 0].squeeze()]])\r\n        tail_text = self.text(t_sents, t_order, t_lengths, node_features[[shifted_pos[:, 1].squeeze()]])\r\n\r\n        r_pos = self.rel_embedding(pos[:, 2].squeeze())\r\n\r\n        gate_head = self.gate(pos[:, 0].squeeze())\r\n        gate_tail = self.gate(pos[:, 1].squeeze())\r\n\r\n        score_pos = self._score(head_graph, head_text, tail_graph, tail_text, r_pos, gate_head, gate_tail)\r\n        return score_pos\r\n\r\n    def _score(self, hg, ht, tg, tt, r, gh, gt):\r\n        gate_h = torch.sigmoid(gh)\r\n        gate_t = torch.sigmoid(gt)\r\n        head = gate_h * hg + (1-gate_h) * ht\r\n        tail = gate_t * tg + (1-gate_t) * tt\r\n        s = (head + r - tail)** 2\r\n        return s\r\n'"
Existing paper reading/model/TAT.py,7,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\n\nclass TAT(nn.Module):\n    """"""\n    A Bi-LSTM layer with attention\n    """"""\n    def __init__(self, embedding_dim, voc_size):\n        super(TAT, self).__init__()\n        self.hidden_dim = embedding_dim\n        self.word_embeddings = nn.Embedding(voc_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim//2, bidirectional=True)\n        self.attF = nn.Linear(self.hidden_dim, self.hidden_dim)\n\n    def forward(self, sentence, orders, lengths, ent_emb):\n        embedded = self.word_embeddings(sentence)\n        padded_sent = pack_padded_sequence(embedded, lengths, batch_first=True)\n        output = padded_sent\n        output, hidden = self.lstm(output)\n        output, _ = pad_packed_sequence(output, batch_first=True)\n        output = output[orders]\n        att = torch.unsqueeze(self.attF(ent_emb), 2)\n        att_score = F.softmax(torch.bmm(output, att), dim=1)\n        o = torch.squeeze(torch.bmm(output.transpose(1,2), att_score))\n        return o\n'"
Existing paper reading/model/__init__.py,0,b''
Existing paper reading/model/graph_attention.py,9,"b'import numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass GraphAttentionLayer(nn.Module):\r\n    """"""\r\n    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\r\n    """"""\r\n\r\n    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\r\n        super(GraphAttentionLayer, self).__init__()\r\n        self.dropout = dropout\r\n        self.in_features = in_features\r\n        self.out_features = out_features\r\n        self.alpha = alpha\r\n        self.concat = concat\r\n\r\n        self.W = nn.Parameter(nn.init.xavier_uniform_(torch.FloatTensor(in_features, out_features).type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor), gain=np.sqrt(2.0)), requires_grad=True)\r\n        self.a1 = nn.Parameter(nn.init.xavier_uniform_(torch.FloatTensor(out_features, 1).type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor), gain=np.sqrt(2.0)), requires_grad=True)\r\n        self.a2 = nn.Parameter(nn.init.xavier_uniform_(torch.FloatTensor(out_features, 1).type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor), gain=np.sqrt(2.0)),requires_grad=True)\r\n\r\n        self.leakyrelu = nn.LeakyReLU(self.alpha)\r\n\r\n    def forward(self, input, adj):\r\n        h = torch.mm(input, self.W)\r\n        N = h.size()[0]\r\n\r\n        f_1 = h @ self.a1\r\n        f_2 = h @ self.a2\r\n        e = self.leakyrelu(f_1 + f_2.transpose(0, 1)) #node_num * node_num\r\n\r\n        zero_vec = -9e15*torch.ones_like(e)\r\n        attention = torch.where(adj > 0, e, zero_vec)\r\n        attention = F.softmax(attention, dim=1)\r\n        attention = F.dropout(attention, self.dropout, training=self.training)\r\n        h_prime = torch.matmul(attention, h)\r\n\r\n        if self.concat:\r\n            return F.sigmoid(h_prime)\r\n        else:\r\n            return h_prime\r\n\r\n    def __repr__(self):\r\n        return self.__class__.__name__ + \' (\' + str(self.in_features) + \' -> \' + str(self.out_features) + \')\'\r\n'"
Existing paper reading/utils/__init__.py,0,b''
Existing paper reading/utils/data_loader.py,2,"b'import torch\r\nfrom torch.utils.data import Dataset, DataLoader\r\nimport numpy as np\r\nfrom .utils import generate_corrupt_triples, load_triple_dict\r\nfrom torch.utils import data\r\n\r\n\r\nclass LinkPredictionDataset(Dataset):\r\n    def __init__(self, kg_file, txt_file, id2ent, num_ent):\r\n        self.triples, self.triple_dict, self.triple_dict_rev = load_triple_dict(kg_file)\r\n        self.texts = txt_file\r\n        self.id2ent = id2ent\r\n        self.num_ent = num_ent\r\n        self.negative = generate_corrupt_triples(self.triples, self.num_ent, self.triple_dict, self.triple_dict_rev)\r\n        self.num_neg = len(self.negative)\r\n        self.triples = np.array(self.triples)\r\n        self.negative = np.array(self.negative)\r\n\r\n    def __len__(self):\r\n        return len(self.triples)\r\n\r\n    def __getitem__(self, idx):\r\n        positive = self.triples[idx,:]\r\n        negative = self.negative[idx,:]\r\n        pos_h_text = self.texts[self.id2ent[positive[0].item()]]\r\n        pos_t_text = self.texts[self.id2ent[positive[1].item()]]\r\n        neg_h_text = self.texts[self.id2ent[negative[0].item()]]\r\n        neg_t_text = self.texts[self.id2ent[negative[1].item()]]\r\n\r\n        return positive, negative, pos_h_text, pos_t_text, neg_h_text, neg_t_text\r\n\r\n    def get_triples(self):\r\n        return self.triples\r\n\r\n    def get_num_ent(self):\r\n        return self.num_ent\r\n\r\n    def get_triple_dict(self):\r\n        return self.triple_dict\r\n\r\n    def get_triple_dict_rev(self):\r\n        return self.triple_dict_rev\r\n\r\nclass LinkTestTotal:\r\n    def __init__(self, kg_file, num_ent):\r\n        self.triples, _, _ = load_triple_dict(kg_file)\r\n        self.num_ent = num_ent\r\n        self.triples = np.array(self.triples)\r\n        self.new_head = np.expand_dims(np.array(list(range(self.num_ent))), axis=1)\r\n\r\n    def __len__(self):\r\n        return len(self.triples)\r\n\r\n    def __getitem__(self, idx):\r\n        t = self.triples[idx,:]\r\n        np_tri = np.repeat([t], self.num_ent, axis=0)\r\n        old_tail = np_tri[:, 1:]\r\n        old_head = np_tri[:, 0]\r\n        new_head_triple = np.concatenate([self.new_head, old_tail], axis=1)\r\n        new_tail_triple = np_tri\r\n        new_tail_triple[:, 1] = self.new_head.squeeze()\r\n        return new_head_triple, new_tail_triple, np.array(t)\r\n\r\n\r\nclass LinkTestDataset(Dataset):\r\n    def __init__(self, head_triple, tail_triple, txt_file, id2ent):\r\n        self.head_triple = head_triple\r\n        self.tail_triple = tail_triple\r\n        self.texts = txt_file\r\n        self.id2ent = id2ent\r\n\r\n    def __len__(self):\r\n        return len(self.head_triple)\r\n\r\n    def __getitem__(self, idx):\r\n        head = self.head_triple[idx,:]\r\n        tail = self.tail_triple[idx,:]\r\n        head_h_text = self.texts[self.id2ent[head[0].item()]]\r\n        head_t_text = self.texts[self.id2ent[head[1].item()]]\r\n        tail_h_text = self.texts[self.id2ent[tail[0].item()]]\r\n        tail_t_text = self.texts[self.id2ent[tail[1].item()]]\r\n\r\n        return head, tail, head_h_text, head_t_text, tail_h_text, tail_t_text\r\n'"
Existing paper reading/utils/utils.py,6,"b'import numpy as np\r\nfrom sys import getsizeof\r\nimport torch\r\nimport math\r\nimport networkx as nx\r\nimport json\r\nimport pickle\r\nimport codecs\r\nfrom collections import defaultdict, Counter\r\n\r\n\r\nclass KnowledgeGraph:\r\n    def __init__(self):\r\n        self.G = nx.DiGraph()\r\n        self.triples = []\r\n\r\n    def load_file(self, fn, delimiter, threshold):\r\n        fo = open(fn)\r\n        for line in fo:\r\n            line = line.strip()\r\n            if line:\r\n                ent1, ent2, weight = line.split(delimiter)\r\n                if int(weight) >= threshold:\r\n                    self.triples.append((ent1, ent2, weight))\r\n\r\n    def load_triple_noweight(self, triples):\r\n        for t in triples:\r\n            ent1, ent2, weight = t\r\n            self.triples.append((ent1, ent2, {\'label\':weight}))\r\n\r\n    def load_file_noweight(self, fn, delimiter, threshold):\r\n        fo = open(fn)\r\n        _ = fo.readline()\r\n        for line in fo:\r\n            line = line.strip()\r\n            if line:\r\n                ent1, ent2, weight = line.split(delimiter)\r\n                self.triples.append((int(ent1), int(ent2), {\'label\': weight}))\r\n\r\n    def filter_small_nodes(self, threshold):\r\n        # this will filter out all the nodes that have less outgoing edges\r\n        to_delete = []\r\n        for node in self.G.nodes():\r\n            if self.G.degree()[node] < threshold:\r\n                to_delete.append(node)\r\n        for n in to_delete:\r\n            self.G.remove_node(n)\r\n\r\n    def node_info(self, s):\r\n        seg1 = s.find(\'<\')\r\n        label1 = s[:seg1]\r\n        type1 = s[seg1 + 1:-1]\r\n        return label1, type1\r\n\r\n    def triple2graph(self):\r\n        self.G.add_weighted_edges_from(self.triples)\r\n\r\n    def triple2graph_noweight(self):\r\n        self.G.add_edges_from(self.triples)\r\n\r\n\r\ndef new_KG(fn):\r\n    KG = KnowledgeGraph()\r\n    KG.load_file_noweight(fn, \'\\t\', 0)\r\n    KG.triple2graph_noweight()\r\n    return KG\r\n\r\n\r\ndef load_triples(kg_f):\r\n    fo = open(kg_f)\r\n    triples = []\r\n    for line in fo:\r\n        line = line.strip()\r\n        ele = line.split(\'\\t\')\r\n        if len(ele)==3:\r\n            ele = list(map(int, ele))\r\n            triples.append(ele)\r\n\r\n\r\ndef load_dict(f):\r\n    fo = open(f)\r\n    d = {}\r\n    num = int(fo.readline().strip())\r\n    for line in fo:\r\n        line = line.strip()\r\n        name, idd = line.split(\'\\t\')\r\n        d[int(idd)] = name\r\n    return d, num\r\n\r\n\r\ndef load_graph(kbf, num_ent):\r\n    graph = new_KG(kbf)\r\n    adj = torch.FloatTensor(nx.adjacency_matrix(graph.G, nodelist=range(num_ent)).todense())\r\n    return graph, adj\r\n\r\n\r\ndef load_kg_embeddings(f):\r\n    fo = open(f)\r\n    embeddings = json.loads(fo.read())\r\n    return embeddings\r\n\r\n\r\ndef load_triple_dict(f):\r\n    fo = open(f)\r\n    triples = []\r\n    triple_dict = defaultdict(set)\r\n    triple_dict_rev = defaultdict(set)\r\n    for line in fo:\r\n        line = line.strip()\r\n        ele = line.split(\'\\t\')\r\n        if len(ele) == 3:\r\n            ele = list(map(int, ele))\r\n            triples.append(ele)\r\n            triple_dict[ele[0]].add((ele[1], ele[2]))\r\n            triple_dict_rev[ele[1]].add((ele[0], ele[2]))\r\n    return triples, triple_dict, triple_dict_rev\r\n\r\n\r\ndef create_mapping(freq, min_freq=0, max_vocab=50000):\r\n    freq = freq.most_common(max_vocab)\r\n    item2id = {\r\n        \'<pad>\': 0,\r\n        \'<unk>\': 1\r\n    }\r\n    offset = len(item2id)\r\n    for i, v in enumerate(freq):\r\n        if v[1] > min_freq:\r\n            item2id[v[0]] = i + offset\r\n    id2item = {i: v for v, i in item2id.items()}\r\n    return item2id, id2item\r\n\r\n\r\ndef create_dict(item_list):\r\n    assert type(item_list) is list\r\n    freq = Counter(item_list)\r\n    return freq\r\n\r\n\r\ndef prepare_mapping(words, min_freq):\r\n    words = [w.lower() for w in words]\r\n    words_freq = create_dict(words)\r\n    word2id, id2word = create_mapping(words_freq, min_freq)\r\n    print(""Found %i unique words (%i in total)"" % (\r\n        len(word2id), sum(len(x) for x in words)\r\n    ))\r\n\r\n    mappings = {\r\n        \'word2idx\': word2id,\r\n        \'idx2word\': id2word\r\n    }\r\n\r\n    return mappings\r\n\r\n\r\ndef load_text(f, min_freq, max_len):\r\n    with open(f) as jsf:\r\n        txt = json.load(jsf)\r\n    words = []\r\n    new_txt = {}\r\n    for key in txt:\r\n        tmp = []\r\n        for sent in txt[key]:\r\n            tmp.extend(sent)\r\n            tmp.append(\'<eos>\')\r\n        tmp = tmp[:max_len]\r\n        new_txt[key] = tmp\r\n        words.extend(tmp)\r\n    mappings = prepare_mapping(words, min_freq)\r\n    word2idx = mappings[""word2idx""]\r\n\r\n    vectorize_txt = defaultdict(list)\r\n    for key in new_txt:\r\n        for w in new_txt[key]:\r\n            try:\r\n                vectorize_txt[key].append(word2idx[w])\r\n            except:\r\n                vectorize_txt[key].append(word2idx[\'<unk>\'])\r\n    return mappings, vectorize_txt\r\n\r\n\r\ndef bern(triple_dict, triple_dict_rev, tri):\r\n    h = tri[0]\r\n    t = tri[1]\r\n    tph = len(triple_dict[h])\r\n    hpt = len(triple_dict_rev[t])\r\n    deno = tph+hpt\r\n    return tph/float(deno), hpt/float(deno)\r\n\r\n\r\ndef adjust_single_sent_order(t):\r\n    batch_size = len(t)\r\n    list_t = t\r\n    sorted_r = sorted([(len(r), r_n, r) for r_n,r in enumerate(list_t)], reverse=True)\r\n    lr, r_n, ordered_list_rev = zip(*sorted_r)\r\n    max_sents = lr[0]\r\n    lr = torch.LongTensor(list(lr))\r\n    r_n = torch.LongTensor(list(r_n))\r\n    batch_t = torch.zeros(batch_size, max_sents).long()                         # (sents ordered by len)\r\n    for i, s in enumerate(ordered_list_rev):\r\n        batch_t[i,0:len(s)] = torch.LongTensor(s)\r\n    return batch_t, r_n, lr # mainly to record batchified text, sentence order, length of sentence,\r\n\r\n\r\ndef adjust_sent_order(l):\r\n    pos,neg,pos_h_text, pos_t_text, neg_h_text, neg_t_text = zip(*l)\r\n    ph = adjust_single_sent_order(pos_h_text)\r\n    pt = adjust_single_sent_order(pos_t_text)\r\n    nh = adjust_single_sent_order(neg_h_text)\r\n    nt = adjust_single_sent_order(neg_t_text)\r\n    return torch.LongTensor(pos),torch.LongTensor(neg),ph,pt,nh,nt\r\n\r\ndef convert_index(blist, nodeslist):\r\n    new_blist = []\r\n    for b in blist:\r\n        b = b.cpu().numpy()\r\n        index = np.argsort(nodeslist)\r\n        sorted_nodeslist = nodeslist[index]\r\n        found_index = np.searchsorted(sorted_nodeslist, b[:,0:2])\r\n        bindex = np.take(index, found_index, mode=\'clip\')\r\n        bindex_exp = np.hstack((bindex, b[:, 2][:, np.newaxis]))\r\n        new_blist.append(np.array(bindex_exp))\r\n    return new_blist\r\n\r\n\r\ndef generate_corrupt_triples(pos, num_ent, triple_dict, triple_dict_rev):\r\n    neg = []\r\n    for p in pos:\r\n        sub = np.random.randint(num_ent)\r\n        tph, hpt = bern(triple_dict, triple_dict_rev, p)\r\n        n = [sub, p[1], p[2]]\r\n        chose = np.random.choice(2,1,p=[tph, hpt])\r\n        if chose[0] == 1:\r\n            n = [p[0], sub, p[2]]\r\n        neg.append(n)\r\n    return neg\r\n\r\ndef get_subgraph(triples, triple_dict, whole_graph):\r\n    # Only handle 1-hop for now\r\n    # Data Types for Nodes are INT\r\n    in_graph = set()\r\n    for triple in triples:\r\n        head = triple[0]\r\n        tail = triple[1]\r\n        in_graph.add(tuple(triple))\r\n        for tri in triple_dict[head.item()]:\r\n            single1 = (head, tri[0], tri[1])\r\n            in_graph.add(single1)\r\n        for tri in triple_dict[tail.item()]:\r\n            single2 = (tail, tri[0], tri[1])\r\n            in_graph.add(single2)\r\n    in_kg = KnowledgeGraph()\r\n    in_kg.load_triple_noweight(in_graph)\r\n    in_kg.triple2graph_noweight()\r\n    included_nodes = list(in_kg.G)\r\n    adj_ingraph = nx.adjacency_matrix(whole_graph.G, nodelist=included_nodes).todense()\r\n    return np.array(included_nodes), adj_ingraph\r\n\r\n\r\ndef mean_rank(triples, sorted_idx, correct, loc):\r\n    reordered_triples = triples[sorted_idx]\r\n    rank = np.argwhere(reordered_triples[:, loc] == correct[loc])[0][0]\r\n    # print(""rank is"",rank)\r\n    return rank\r\n\r\ndef convert_idx2name(triples, id2ent, ent2name, id2rel):\r\n    result = []\r\n    for t in triples:\r\n        e1 = ent2name[id2ent[t[0].item()]]\r\n        e2 = ent2name[id2ent[t[1].item()]]\r\n        rel = id2rel[t[2].item()]\r\n        result.append([e1,e2,rel])\r\n    return result\r\n\r\ndef write_triples(triple_list, wf):\r\n    for t in triple_list:\r\n        wf.write(\'\\t\'.join(list(map(str, t)))+\'\\n\')\r\n'"
New paper writing/loader/__init__.py,0,b''
New paper writing/loader/loader.py,2,"b'import os\nimport json\nimport gzip\nimport lzma\nimport torch\nimport torch.nn as nn\n\nfrom loader.preprocessing import create_mapping\n\n\ndef load_files(path):\n    sources = []\n    targets = []\n    words = []\n    for line in open(path, \'r\'):\n        line = line.strip()\n        file = json.loads(line)\n        sources.append(file[\'title\'])\n        targets.append(file[\'abs\'])\n        words.extend(file[\'words\'])\n    return words, [sources, targets]\n\n\ndef load_file_with_terms(path):\n    sources = []\n    targets = []\n    terms = []\n    words = []\n    for line in open(path, \'r\'):\n        line = line.strip()\n        file = json.loads(line)\n        sources.append(file[\'title\'])\n        targets.append(file[\'abs\'])\n        terms.append(file[\'terms\'])\n        words.extend(file[\'words\'])\n    return words, [sources, terms, targets]\n\n\ndef load_file_with_pmid(path):\n    heart = [\'heart\', \'cardio\', \'cardiac\', \'annulus\', \'arrhyth\', \'atrium\', \'cardi\', \'coronary\', \'pulmonary\', \'valve\']\n    # heart = [\'DNA\', \'Atomic\', \'Genome\', \'Monolayer\', \'Molecular\', \'Polymer\', \'Self-assembly\', \'Quantum\', \'Ontological\',\n    #          \'Autogeny\', \'MEMS\', \'NEMS\', \'Plasmonics\', \'Biomimetic\', \'nano\', \'Molecular\', \'Electrospinning\']\n    pmids = []\n    sources = []\n    targets = []\n    terms = []\n    words = []\n    for line in open(path, \'r\'):\n        flag = False\n        line = line.strip()\n        file = json.loads(line)\n        for h in heart:\n            if h.lower() in "" "".join(file[\'title\']).lower():\n                flag = True\n                break\n            if h.lower() in  "" "".join(file[\'terms\']).lower():\n                flag = True\n                break\n            if h.lower() in "" "".join(file[\'abs\']).lower():\n                flag = True\n                break\n        if flag:\n            sources.append(file[\'title\'])\n            targets.append(file[\'abs\'])\n            terms.append(file[\'terms\'])\n            words.extend(file[\'words\'])\n            pmids.append(file[\'pmid\'])\n    return words, [sources, terms, targets, pmids]\n\n\ndef load_file_with_pmid_no_filter(path):\n    pmids = []\n    sources = []\n    targets = []\n    terms = []\n    words = []\n    for line in open(path, \'r\'):\n        line = line.strip()\n        file = json.loads(line)\n        sources.append(file[\'title\'])\n        targets.append(file[\'abs\'])\n        terms.append(file[\'terms\'])\n        words.extend(file[\'words\'])\n        pmids.append(file[\'pmid\'])\n    return words, [sources, terms, targets, pmids]\n\n\ndef load_test_data(path, t_dataset):\n    osources, oterms, otargets, opmids = t_dataset\n    data = list(zip(osources, oterms, otargets, opmids))\n    gturth = {}\n    pmids = []\n    sources = []\n    targets = []\n    terms = []\n\n    for d in data:\n        gturth[d[3]] = [d[0],d[1],d[2]]\n\n    for line in open(path, \'r\'):\n        line = line.strip()\n        file = json.loads(line)\n        if file[\'pmid\'] in gturth:\n            sources.append(file[\'Output\'])\n            targets.append(gturth[file[\'pmid\']][2])\n            terms.append(file[\'Terms\'])\n            pmids.append(file[\'pmid\'])\n\n    return [sources, terms, targets, pmids]\n\n\ndef load_test_new_data(path):\n    gturth = {}\n    pmids = []\n    sources = []\n    targets = []\n    terms = []\n\n    for line in open(path, \'r\'):\n        line = line.strip()\n        file = json.loads(line)\n        sources.append(file[\'Output\'])\n        targets.append(file[\'Ref\'])\n        terms.append(file[\'Terms\'])\n        pmids.append(file[\'pmid\'])\n\n    return [sources, terms, targets, pmids]\n\n\ndef load_pretrained(word_emb, id2word, pre_emb):\n    if not pre_emb:\n        return\n\n    word_dim = word_emb.weight.size(1)\n\n    # Initialize with pretrained embeddings\n    new_weights = word_emb.weight.data\n    print(\'Loading pretrained embeddings from %s...\' % pre_emb)\n    pretrained = {}\n    emb_invalid = 0\n    for i, line in enumerate(load_embedding(pre_emb)):\n        if type(line) == bytes:\n            try:\n                line = str(line, \'utf-8\')\n            except UnicodeDecodeError:\n                continue\n        line = line.rstrip().split()\n        if len(line) == word_dim + 1:\n            pretrained[line[0]] = torch.Tensor(\n                [float(x) for x in line[1:]])\n        else:\n            emb_invalid += 1\n    if emb_invalid > 0:\n        print(\'WARNING: %i invalid lines\' % emb_invalid)\n    c_found = 0\n    c_lower = 0\n    # Lookup table initialization\n    for i in range(len(id2word)):\n        word = id2word[i]\n        if word in pretrained:\n            new_weights[i] = pretrained[word]\n            c_found += 1\n        elif word.lower() in pretrained:\n            new_weights[i] = pretrained[word.lower()]\n            c_lower += 1\n    word_emb.weight = nn.Parameter(new_weights)\n\n    print(\'Loaded %i pretrained embeddings.\' % len(pretrained))\n    print(\'%i / %i (%.4f%%) words have been initialized with \'\n          \'pretrained embeddings.\' % (\n           c_found + c_lower , len(id2word),\n           100. * (c_found + c_lower) / len(id2word)\n          ))\n    print(\'%i found directly, %i after lowercasing, \' % (\n           c_found, c_lower))\n    return word_emb\n\n\ndef load_embedding(pre_emb):\n    if os.path.basename(pre_emb).endswith(\'.xz\'):\n        return lzma.open(pre_emb)\n    if os.path.basename(pre_emb).endswith(\'.gz\'):\n        return gzip.open(pre_emb, \'rb\')\n    else:\n        return open(pre_emb, \'r\', errors=\'replace\')\n\n\ndef augment_with_pretrained(words_freq, ext_emb_path):\n    """"""\n    Augment the dictionary with words that have a pretrained embedding.\n    """"""\n    print(\n        \'Augmenting words by pretrained embeddings from %s...\' % ext_emb_path\n    )\n    assert os.path.isfile(ext_emb_path)\n\n    # Load pretrained embeddings from file\n    pretrained = []\n    if len(ext_emb_path) > 0:\n        for line in load_embedding(ext_emb_path):\n            if not line.strip():\n                continue\n            if type(line) == bytes:\n                try:\n                    pretrained.append(str(line, \'utf-8\').rstrip().split()[0].strip())\n                except UnicodeDecodeError:\n                    continue\n            else:\n                pretrained.append(line.rstrip().split()[0].strip())\n\n    pretrained = set(pretrained)\n\n    # We add every word in the pretrained file\n    for word in pretrained:\n        words_freq[word] += 10\n\n    word2id, id2word = create_mapping(words_freq)\n\n    mappings = {\n        \'word2id\': word2id,\n        \'id2word\': id2word\n    }\n\n    return mappings\n\n# old\n\n\ndef load_file_with_pred(path_title):\n    sources = []\n    targets = []\n    preds = []\n    words = []\n    path = \'/data/m1/wangq16/End-end_title_generation/data/\' + path_title\n    for line in open(os.path.abspath(path), \'r\'):\n        line = line.strip()\n        file = json.loads(line)\n        sources.append(file[\'source\'])\n        targets.append(file[\'target\'])\n        preds.append(file[\'preds\'])\n        words.extend(file[\'words\'])\n    return words, [sources, preds, targets]\n'"
New paper writing/loader/logger.py,0,"b""import logging\nimport sys\n\n\ndef get_logger(name, level=logging.INFO, handler=sys.stdout, filename=None,\n               formatter='%(asctime)s %(name)s %(levelname)s %(message)s'\n               ):\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.INFO)\n    formatter = logging.Formatter(formatter)\n    stream_handler = logging.StreamHandler(handler)\n    stream_handler.setLevel(level)\n    stream_handler.setFormatter(formatter)\n    logger.addHandler(stream_handler)\n\n    if filename:\n        file_handler = logging.FileHandler(filename, 'w')\n        file_handler.setLevel(level)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n    return logger\n\n\nclass Tee(object):\n    def __init__(self, *files):\n        self.files = files\n\n    def write(self, obj):\n        for f in self.files:\n            f.write(obj)\n            f.flush()  # If you want the output to be visible immediately\n\n    def flush(self) :\n        for f in self.files:\n            f.flush()\n\n"""
New paper writing/loader/preprocessing.py,24,"b'from collections import Counter\nimport torch\nimport json\nimport string\n\n\n# Mask variable\ndef _mask(prev_generated_seq, device, eos_id):\n    prev_mask = torch.eq(prev_generated_seq, eos_id)\n    lengths = torch.argmax(prev_mask, dim=1)\n    max_len = prev_generated_seq.size(1)\n    mask = []\n    for i in range(prev_generated_seq.size(0)):\n        if lengths[i] == 0:\n            mask_line = [0] * max_len\n        else:\n            mask_line = [0] * lengths[i].item()\n            mask_line.extend([1] * (max_len - lengths[i].item()))\n        mask.append(mask_line)\n    mask = torch.ByteTensor(mask)\n    mask = mask.to(device)\n    return prev_generated_seq.data.masked_fill_(mask, 0)\n\n\ndef create_mapping(freq, min_freq=0, max_vocab=50000):\n    """"""\n    Create a mapping (item to ID / ID to item) from a dictionary.\n    Items are ordered by decreasing frequency.\n    char is an indicator whether the mapping is for char/word or the\n    """"""\n    freq = freq.most_common(max_vocab)\n    item2id = {\n        \'<pad>\': 0,\n        \'<unk>\': 1,\n        \'<sos>\': 2,\n        \'<eos>\': 3\n    }\n    offset = len(item2id)\n    for i, v in enumerate(freq):\n        if v[1] > min_freq:\n            item2id[v[0]] = i + offset\n    id2item = {i: v for v, i in item2id.items()}\n    return item2id, id2item\n\n\ndef create_dict(item_list):\n    """"""\n    Create a dictionary of items from a list of list of items.\n    """"""\n    assert type(item_list) is list\n    freq = Counter(item_list)\n    return freq\n\n\ndef prepare_mapping(words, lower, min_freq):\n    """"""\n    prepare word2id\n    :param words: words corpus\n    :param lower: whether lower char\n    :return mappings\n    """"""\n    if lower:\n        words = [w.lower() for w in words]\n    words_freq = create_dict(words)\n    word2id, id2word = create_mapping(words_freq, min_freq)\n    print(""Found %i unique words (%i in total)"" % (\n        len(word2id), sum(len(x) for x in words)\n    ))\n\n    mappings = {\n        \'word2id\': word2id,\n        \'id2word\': id2word\n    }\n\n    return mappings, words_freq\n\n\nclass AssembleMem:\n    def __init__(self, dataset, word2id, lower=True, batch_size=64, max_len=30,\n                 cuda=torch.cuda.is_available(), pmid=False):\n        if pmid:\n            sources, terms, targets, pmids = dataset\n            data = list(zip(sources, terms, targets, pmids))\n        else:\n            sources, terms, targets = dataset\n            data = list(zip(sources, terms, targets))\n        self.pmid = pmid\n        self.batch_size = batch_size\n        self.len = len(sources)\n        self.word2id = word2id\n        self.lower = lower\n        self.max_len = max_len\n        self.device = torch.device(""cuda:0"" if cuda and torch.cuda.is_available() else ""cpu"")\n        self.vocab_size = len(word2id)\n        self.corpus = self.batchfy(data)\n\n    def batchfy(self, data):\n        self.len = len(data)\n        data = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n        corpus = [self.vectorize(sample) for sample in data]\n        return corpus\n\n    def add_start_end(self, vector):\n        vector.append(self.word2id[\'<eos>\'])\n        return [self.word2id[\'<sos>\']] + vector\n\n    def pad_vector(self, vector, max_len):\n        padding = max_len - len(vector)\n        vector.extend([0] * padding)\n        return vector\n\n    def vectorize(self, sample):\n        sample.sort(key=lambda x: len(x[0]), reverse = True)\n        list_oovs, targets, sources, terms = [], [], [], []\n        batch_s, batch_o_s, source_len, batch_term, batch_o_term, term_len = [], [], [], [], [], []\n        batch_t, batch_o_t, target_len = [], [], []\n        max_source_oov = 0\n        if self.pmid:\n            pmids = []\n        for data in sample:\n            # title\n            source_oov = []\n            _o_source, _source = [], []\n            sources.append(data[0])\n            if self.pmid:\n                pmids.append(data[3])\n            for _s in data[0]:\n                if self.lower:\n                    _s = _s.lower()\n                try:\n                    _source.append(self.word2id[_s])\n                except KeyError:\n                    _source.append(self.word2id[\'<unk>\'])\n                    if _s not in source_oov:\n                        _o_source.append(len(source_oov) + self.vocab_size)\n                        source_oov.append(_s)\n                    else:\n                        _o_source.append(source_oov.index(_s) + self.vocab_size)\n                else:\n                    _o_source.append(self.word2id[_s])\n            # terms\n            _o_term, _term = [], []\n            terms.append(data[1])\n            for _t in data[1]:\n                if self.lower:\n                    _t = _t.lower()\n                try:\n                    _term.append(self.word2id[_t])\n                except KeyError:\n                    _term.append(self.word2id[\'<unk>\'])\n                    if _t not in source_oov:\n                        _o_term.append(len(source_oov) + self.vocab_size)\n                        source_oov.append(_t)\n                    else:\n                        _o_term.append(source_oov.index(_t) + self.vocab_size)\n                else:\n                    _o_term.append(self.word2id[_t])\n\n            if max_source_oov < len(source_oov):\n                max_source_oov = len(source_oov)\n            batch_s.append(_source)\n            batch_o_s.append(_o_source)\n            source_len.append(len(_o_source))\n            oovidx2word = {idx: word for idx, word in enumerate(source_oov)}\n            list_oovs.append(oovidx2word)\n\n            batch_term.append(_term)\n            batch_o_term.append(_o_term)\n            term_len.append(len(_o_term))\n\n            _o_target, _target = [], []\n            targets.append(data[2])\n            for _t in data[2]:\n                if self.lower:\n                    _t = _t.lower()\n                try:\n                    _target.append(self.word2id[_t])\n                except KeyError:\n                    _target.append(self.word2id[\'<unk>\'])\n                    if _t in source_oov:\n                        _o_target.append(source_oov.index(_t) + self.vocab_size)\n                    else:\n                        _o_target.append(self.word2id[\'<unk>\'])\n                else:\n                    _o_target.append(self.word2id[_t])\n\n            _target = self.add_start_end(_target)\n            batch_t.append(_target)\n            batch_o_t.append(self.add_start_end(_o_target))\n            target_len.append(len(_target))\n\n        if len(source_len) == 1:\n            batch_s = torch.LongTensor(batch_s)\n            batch_o_s = torch.LongTensor(batch_o_s)\n            batch_term = torch.LongTensor(batch_term)\n            batch_o_term = torch.LongTensor(batch_o_term)\n            batch_t = torch.LongTensor(batch_t)\n            batch_o_t = torch.LongTensor(batch_o_t)\n        else:\n            batch_s = [torch.LongTensor(self.pad_vector(i, max(source_len))) for i in batch_s]\n            batch_o_s = [torch.LongTensor(self.pad_vector(i, max(source_len))) for i in batch_o_s]\n            batch_s = torch.stack(batch_s, dim=0)\n            batch_o_s = torch.stack(batch_o_s, dim=0)\n\n            batch_term = [torch.LongTensor(self.pad_vector(i, max(term_len))) for i in batch_term]\n            batch_o_term = [torch.LongTensor(self.pad_vector(i, max(term_len))) for i in batch_o_term]\n            batch_term = torch.stack(batch_term, dim=0)\n            batch_o_term = torch.stack(batch_o_term, dim=0)\n\n            batch_t = [torch.LongTensor(self.pad_vector(i, max(target_len))) for i in batch_t]\n            batch_o_t = [torch.LongTensor(self.pad_vector(i, max(target_len))) for i in batch_o_t]\n            batch_t = torch.stack(batch_t, dim=0)\n            batch_o_t = torch.stack(batch_o_t, dim=0)\n        source_len = torch.LongTensor(source_len)\n        if self.pmid:\n            return batch_s, batch_o_s, source_len, max_source_oov, batch_term, batch_o_term, batch_t, batch_o_t, list_oovs, \\\n            targets, sources, terms, pmids\n        else:\n            return batch_s, batch_o_s, source_len, max_source_oov, batch_term, batch_o_term, batch_t, batch_o_t, list_oovs, \\\n            targets, sources, terms\n\n    def get_batch(self, i, train=True):\n        if self.pmid:\n            batch_s, batch_o_s, source_len, max_source_oov, batch_term, batch_o_term, batch_t, batch_o_t, list_oovs, \\\n             targets, sources, terms, pmids = self.corpus[i]\n        else:\n            batch_s, batch_o_s, source_len, max_source_oov, batch_term, batch_o_term, batch_t, batch_o_t, list_oovs, \\\n             targets, sources, terms = self.corpus[i]\n        if train:\n            return batch_s.to(self.device), batch_o_s.to(self.device), source_len.to(self.device),\\\n                   max_source_oov, batch_term.to(self.device), batch_o_term.to(self.device), batch_t.to(self.device),\\\n                   batch_o_t.to(self.device)\n        else:\n            if self.pmid:\n                return batch_s.to(self.device), batch_o_s.to(self.device), source_len.to(self.device),\\\n                       max_source_oov, batch_term.to(self.device), batch_o_term.to(self.device), list_oovs, targets,\\\n                       sources, terms, pmids\n            else:\n                return batch_s.to(self.device), batch_o_s.to(self.device), source_len.to(self.device),\\\n                       max_source_oov, batch_term.to(self.device), batch_o_term.to(self.device), list_oovs, targets,\\\n                       sources, terms\n\n\ndef filter_stopwords(word2id):\n    nltk_stopwords = [\'i\', \'me\', \'my\', \'myself\', \'we\', \'our\', \'ours\', \'ourselves\', \'you\', ""you\'re"", ""you\'ve"", ""you\'ll"", ""you\'d"", \'your\', \'yours\', \'yourself\', \'yourselves\', \'he\', \'him\', \'his\', \'himself\', \'she\', ""she\'s"", \'her\', \'hers\', \'herself\', \'it\', ""it\'s"", \'its\', \'itself\', \'they\', \'them\', \'their\', \'theirs\', \'themselves\', \'what\', \'which\', \'who\', \'whom\', \'this\', \'that\', ""that\'ll"", \'these\', \'those\', \'am\', \'is\', \'are\', \'was\', \'were\', \'be\', \'been\', \'being\', \'have\', \'has\', \'had\', \'having\', \'do\', \'does\', \'did\', \'doing\', \'a\', \'an\', \'the\', \'and\', \'but\', \'if\', \'or\', \'because\', \'as\', \'until\', \'while\', \'of\', \'at\', \'by\', \'for\', \'with\', \'about\', \'against\', \'between\', \'into\', \'through\', \'during\', \'before\', \'after\', \'above\', \'below\', \'to\', \'from\', \'up\', \'down\', \'in\', \'out\', \'on\', \'off\', \'over\', \'under\', \'again\', \'further\', \'then\', \'once\', \'here\', \'there\', \'when\', \'where\', \'why\', \'how\', \'all\', \'any\', \'both\', \'each\', \'few\', \'more\', \'most\', \'other\', \'some\', \'such\', \'no\', \'nor\', \'not\', \'only\', \'own\', \'same\', \'so\', \'than\', \'too\', \'very\', \'s\', \'t\', \'can\', \'will\', \'just\', \'don\', ""don\'t"", \'should\', ""should\'ve"", \'now\', \'d\', \'ll\', \'m\', \'o\', \'re\', \'ve\', \'y\', \'ain\', \'aren\', ""aren\'t"", \'couldn\', ""couldn\'t"", \'didn\', ""didn\'t"", \'doesn\', ""doesn\'t"", \'hadn\', ""hadn\'t"", \'hasn\', ""hasn\'t"", \'haven\', ""haven\'t"", \'isn\', ""isn\'t"", \'ma\', \'mightn\', ""mightn\'t"", \'mustn\', ""mustn\'t"", \'needn\', ""needn\'t"", \'shan\', ""shan\'t"", \'shouldn\', ""shouldn\'t"", \'wasn\', ""wasn\'t"", \'weren\', ""weren\'t"", \'won\', ""won\'t"", \'wouldn\', ""wouldn\'t""]\n    stop_words = set()\n    for w in set(nltk_stopwords) | set(string.punctuation):\n        if w in word2id:\n            stop_words.add(word2id[w])\n        if w.title() in word2id:\n            stop_words.add(word2id[w.title()])\n    return stop_words\n\n\ndef printcand(path, cand):\n    f_output = open(path, \'w\')\n    for i in cand:\n        f_output.write(cand[i] + ""\\n"")\n    f_output.close()\n'"
New paper writing/memory_generator/Decoder.py,25,"b'import sys\r\nimport torch\r\nimport numpy as np\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom .baseRNN import BaseRNN\r\nfrom .utils import MemoryComponent\r\n\r\n\r\nclass DecoderRNN(BaseRNN):\r\n\r\n    def __init__(self, vocab_size, embedding, word_dim, sos_id, eos_id, unk_id,\r\n                max_len=150, input_dropout_p=0, layer_dropout=0, n_layers=1, lmbda=1,\r\n                gpu=torch.cuda.is_available(), rnn_type=\'gru\', h=8, hop=3,\r\n                beam_size=4, **kwargs):\r\n        hidden_size = word_dim\r\n        super(DecoderRNN, self).__init__(vocab_size, hidden_size, input_dropout_p, n_layers, rnn_type)\r\n\r\n        self.rnn = self.rnn_cell(word_dim, hidden_size, n_layers, batch_first=True)\r\n        self.output_size = vocab_size\r\n        self.hidden_size = hidden_size\r\n        self.max_length = max_len\r\n        self.eos_id = eos_id\r\n        self.sos_id = sos_id\r\n        self.unk_id = unk_id\r\n        self.lmbda = lmbda\r\n        self.embedding = embedding\r\n        self.device = torch.device(""cuda:0"" if gpu and torch.cuda.is_available() else ""cpu"")\r\n        self.beam_size = beam_size\r\n\r\n        # initialization\r\n        self.memory_init = MemoryComponent(hop, h, hidden_size, layer_dropout)\r\n        self.Wh = nn.Linear(hidden_size * 2, hidden_size)\r\n        # params for ref attention\r\n        self.Wih = nn.Linear(hidden_size * 2, hidden_size)  # for obtaining projection from encoder hidden states\r\n        self.Ws = nn.Linear(hidden_size, hidden_size)  # for obtaining e from current state\r\n        self.Wc = nn.Linear(1, hidden_size)  # for obtaining e from context vector\r\n        self.Wr = nn.Linear(hidden_size * 2, hidden_size)\r\n        self.vt = nn.Linear(hidden_size, 1)\r\n        # params for memory\r\n        self.memory = MemoryComponent(hop, h, hidden_size, layer_dropout)\r\n        # output\r\n        self.V = nn.Linear(hidden_size * 3, self.output_size)\r\n        # parameters for p_gen\r\n        # for changing refcontext vector into a scalar\r\n        self.w_p = nn.Linear(hidden_size * 2, 1)\r\n        # for changing context vector into a scalar\r\n        self.w_h = nn.Linear(hidden_size * 2, 1)\r\n\r\n    def decode_step(self, sources_ids, _h, enc_proj, batch_size, cov_ref, cov_mem, max_enc_len, enc_mask,\r\n                    encoder_outputs, embed_target, max_source_oov, term_output, term_id, term_mask): #\r\n\r\n        # reference attention\r\n        dec_proj = self.Ws(_h).unsqueeze(1).expand_as(enc_proj)\r\n        # print(\'decoder proj\', dec_proj.size())\r\n        cov_proj = self.Wc(cov_ref.view(-1, 1)).view(batch_size, max_enc_len, -1)\r\n        # print(\'cov proj\', cov_proj.size())\r\n        e_t = self.vt(torch.tanh(enc_proj + dec_proj + cov_proj).view(batch_size*max_enc_len, -1))\r\n        # mask to -INF before applying softmax\r\n        ref_attn = e_t.view(batch_size, max_enc_len)\r\n        del e_t\r\n        ref_attn.data.masked_fill_(enc_mask.data.byte(), -float(\'inf\')) # float(\'-inf\')\r\n        ref_attn = F.softmax(ref_attn, dim=1)\r\n\r\n        ref_context = self.Wr(ref_attn.unsqueeze(1).bmm(encoder_outputs).squeeze(1))\r\n\r\n        # terms attention\r\n        term_context, term_attn = self.memory(_h.unsqueeze(0), term_output, term_mask, cov_mem)\r\n        term_context = term_context.squeeze(0)\r\n\r\n        # output proj calculation\r\n        p_vocab = F.softmax(self.V(torch.cat((_h, ref_context, term_context), 1)), dim=1)\r\n\r\n        # pgen\r\n        # print(embed_target.size())\r\n        p_gen = torch.sigmoid(self.w_p(torch.cat((_h, embed_target), 1)))\r\n        p_ref = torch.sigmoid(self.w_h(torch.cat((ref_context, term_context), 1)))\r\n\r\n        weighted_Pvocab = p_vocab * p_gen\r\n        weighted_ref_attn = (1 - p_gen) * p_ref * ref_attn\r\n        weighted_term_attn = (1 - p_gen) * (1 - p_ref) * term_attn\r\n\r\n        if max_source_oov > 0:\r\n            # create OOV (but in-article) zero vectors\r\n            ext_vocab = torch.zeros(batch_size, max_source_oov)\r\n            ext_vocab=ext_vocab.to(self.device)\r\n            combined_vocab = torch.cat((weighted_Pvocab, ext_vocab), 1)\r\n            del ext_vocab\r\n        else:\r\n            combined_vocab = weighted_Pvocab\r\n        del weighted_Pvocab\r\n        # scatter article word probs to combined vocab prob.\r\n        combined_vocab = combined_vocab.scatter_add(1, sources_ids, weighted_ref_attn)\r\n        combined_vocab = combined_vocab.scatter_add(1, term_id, weighted_term_attn)\r\n\r\n        return combined_vocab, ref_attn, term_attn\r\n\r\n    def forward(self, max_source_oov=0, targets=None, targets_id=None,\r\n                sources_ids=None, enc_mask=None, encoder_hidden=None,\r\n                encoder_outputs=None, term_id=None, term_mask=None,\r\n                term_output=None, teacher_forcing_ratio=None, beam=False,\r\n                stopwords=None, sflag=False):\r\n        if beam:\r\n            return self.decode(max_source_oov, sources_ids, enc_mask, encoder_hidden, encoder_outputs,\r\n               term_id, term_mask, term_output, stopwords, sflag)\r\n        # initialization\r\n        targets, batch_size, max_length, max_enc_len = self._validate_args(targets, encoder_hidden, encoder_outputs,\r\n                            teacher_forcing_ratio)\r\n        decoder_hidden = self._init_state(encoder_hidden)\r\n        cov_ref = torch.zeros(batch_size, max_enc_len)\r\n        cov_ref = cov_ref.to(self.device)\r\n        cov_mem = torch.zeros_like(term_mask, dtype=torch.float)\r\n        cov_mem = cov_mem.to(self.device)\r\n        # memory initialization\r\n        decoder_hidden, _ = self.memory_init(decoder_hidden, term_output, term_mask)\r\n        # print(encoder_outputs.size())\r\n        enc_proj = self.Wih(encoder_outputs.contiguous().view(batch_size * max_enc_len, -1)).view(batch_size,\r\n                                                                                                  max_enc_len, -1)\r\n        if teacher_forcing_ratio:\r\n            embedded = self.embedding(targets)\r\n            embed_targets = self.input_dropout(embedded)\r\n            dec_lens = (targets > 0).float().sum(1)\r\n            lm_loss, cov_loss = [], []  # , cov_loss_mem , []\r\n            hidden, _ = self.rnn(embed_targets, decoder_hidden)\r\n\r\n            # step through decoder hidden states\r\n            for _step in range(max_length):\r\n                _h = hidden[:, _step, :]\r\n                target_id = targets_id[:, _step+1].unsqueeze(1)\r\n                embed_target = embed_targets[:, _step, :]\r\n                combined_vocab, ref_attn, term_attn = self.decode_step(sources_ids, _h, enc_proj, batch_size,\r\n                                cov_ref, cov_mem, max_enc_len, enc_mask,\r\n                                encoder_outputs, embed_target,\r\n                                max_source_oov, term_output, term_id, term_mask)\r\n                # mask the output to account for PAD , cov_ref\r\n                target_mask_0 = target_id.ne(0).detach()\r\n                output = combined_vocab.gather(1, target_id).add_(sys.float_info.epsilon)\r\n                lm_loss.append(output.log().mul(-1) * target_mask_0.float())\r\n\r\n                cov_ref = cov_ref + ref_attn\r\n                cov_mem = cov_mem + term_attn\r\n\r\n                # Coverage Loss\r\n                # take minimum across both attn_scores and coverage\r\n                _cov_loss_ref, _ = torch.stack((cov_ref, ref_attn), 2).min(2)\r\n                _cov_loss_mem, _ = torch.stack((cov_mem, term_attn), 2).min(2)\r\n                cov_loss.append(_cov_loss_ref.sum(1) + _cov_loss_mem.sum(1))\r\n                # print(cov_loss_ref[-1].size())\r\n                # cov_loss_mem.append(_)\r\n            total_masked_loss = torch.cat(lm_loss, 1).sum(1).div(dec_lens) + self.lmbda * torch.stack(cov_loss, 1)\\\r\n                .sum(1).div(dec_lens) / 2.0\r\n            return total_masked_loss\r\n        else:\r\n            return self.evaluate(targets, batch_size, max_length, max_source_oov, encoder_outputs, decoder_hidden,\r\n                                 enc_mask, sources_ids, enc_proj, max_enc_len, term_output, term_id, term_mask,\r\n                                 cov_ref, cov_mem)\r\n\r\n    def evaluate(self, targets, batch_size, max_length, max_source_oov, encoder_outputs, decoder_hidden, enc_mask,\r\n                 sources_ids, enc_proj, max_enc_len, term_output, term_id, term_mask, cov_ref, cov_mem):\r\n\r\n        lengths = np.array([max_length] * batch_size)\r\n        decoded_outputs = []\r\n        embed_target = self.embedding(targets)\r\n        # step through decoder hidden states\r\n        for _step in range(max_length):\r\n            _h, decoder_hidden = self.rnn(embed_target, decoder_hidden)\r\n            combined_vocab, ref_attn, term_attn = self.decode_step(sources_ids, _h.squeeze(1), enc_proj, batch_size,\r\n            cov_ref, cov_mem, max_enc_len, enc_mask,\r\n            encoder_outputs, embed_target.squeeze(1),\r\n            max_source_oov, term_output, term_id, term_mask)\r\n            # not allow decoder to output UNK\r\n            combined_vocab[:, self.unk_id] = 0\r\n            symbols = combined_vocab.topk(1)[1]\r\n            decoded_outputs.append(symbols.clone())\r\n            eos_batches = symbols.data.eq(self.eos_id)\r\n            if eos_batches.dim() > 0:\r\n                eos_batches = eos_batches.cpu().view(-1).numpy()\r\n                update_idx = ((lengths > _step) & eos_batches) != 0\r\n                lengths[update_idx] = len(decoded_outputs)\r\n            symbols.masked_fill_((symbols > self.vocab_size - 1), self.unk_id)\r\n            embed_target = self.embedding(symbols)\r\n            cov_ref = cov_ref + ref_attn\r\n            cov_mem = cov_mem + term_attn\r\n        return torch.stack(decoded_outputs, 1).squeeze(2), lengths.tolist()\r\n\r\n    def _init_state(self, encoder_hidden):\r\n        """""" Initialize the encoder hidden state. """"""\r\n        if encoder_hidden is None:\r\n            return None\r\n        else:\r\n            if isinstance(encoder_hidden, tuple):\r\n                encoder_hidden = tuple([self._cat_directions(h) for h in encoder_hidden])\r\n            else:\r\n                encoder_hidden = self._cat_directions(encoder_hidden)\r\n            encoder_hidden = self.Wh(encoder_hidden)\r\n        return encoder_hidden\r\n\r\n    def _cat_directions(self, h):\r\n        h = torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2)\r\n        return h\r\n\r\n    def _validate_args(self, targets, encoder_hidden, encoder_outputs, teacher_forcing_ratio):\r\n        if encoder_outputs is None:\r\n            raise ValueError(""Argument encoder_outputs cannot be None when attention is used."")\r\n        else:\r\n            max_enc_len = encoder_outputs.size(1)\r\n        # inference batch size\r\n        if targets is None and encoder_hidden is None:\r\n            batch_size = 1\r\n        else:\r\n            if targets is not None:\r\n                batch_size = targets.size(0)\r\n            else:\r\n                batch_size = encoder_hidden.size(1)\r\n\r\n        # set default targets and max decoding length\r\n        if targets is None:\r\n            if teacher_forcing_ratio > 0:\r\n                raise ValueError(""Teacher forcing has to be disabled (set 0) when no targets is provided."")\r\n            # torch.set_grad_enabled(False)\r\n            targets = torch.LongTensor([self.sos_id] * batch_size).view(batch_size, 1)\r\n            targets = targets.to(self.device)\r\n            max_length = self.max_length\r\n        else:\r\n            max_length = targets.size(1) - 1     # minus the start of sequence symbol\r\n\r\n        return targets, batch_size, max_length, max_enc_len\r\n\r\n    def getOverallTopk(self, vocab_probs, ref_attn, term_attn, cov_ref, cov_mem,\r\n                        all_hyps, results, decoder_hidden, stopwords):\r\n        new_decoder_hidden, new_cov_ref, new_cov_mem = [], [], []\r\n        new_vocab_probs = []\r\n        for i, hypo in enumerate(all_hyps):\r\n            curr_vocab_probs = vocab_probs[i]\r\n            curr_vocab_probs[hypo.used_words] = 0\r\n            # print(hypo.used_words)\r\n            new_vocab_probs.append(curr_vocab_probs.unsqueeze(0))\r\n        vocab_probs = torch.cat(new_vocab_probs, 0)\r\n        cov_ref += ref_attn\r\n        cov_mem += term_attn\r\n        # return top-k values i.e. top-k over all beams i.e. next step input ids\r\n        # return hidden, cell states corresponding to topk\r\n        probs, inds = vocab_probs.topk(k=self.beam_size, dim=1)\r\n        probs = probs.log()\r\n        candidates = []\r\n        assert len(all_hyps) == probs.size(0), \'# Hypothesis and log-prob size dont match\'\r\n        # cycle through all hypothesis in full beam\r\n        for i, hypo in enumerate(probs.tolist()):\r\n            for j, _ in enumerate(hypo):\r\n                new_cand = all_hyps[i].extend(token_id=inds[i,j].item(),\r\n                                              hidden_state=decoder_hidden[i].unsqueeze(0),\r\n                                              cov_ref=cov_ref[i].unsqueeze(0),\r\n                                              cov_mem=cov_mem[i].unsqueeze(0),\r\n                                              log_prob= probs[i,j],\r\n                                              stopwords=stopwords)\r\n                candidates.append(new_cand)\r\n        # sort in descending order\r\n        candidates = sorted(candidates, key=lambda x:x.survivability, reverse=True)\r\n        new_beam, next_inp = [], []\r\n        # prune hypotheses and generate new beam\r\n        for h in candidates:\r\n            if h.full_prediction[-1] == self.eos_id:\r\n                # weed out small sentences that likely have no meaning\r\n                if len(h.full_prediction) >= 5:\r\n                    results.append(h)\r\n            else:\r\n                new_beam.append(h)\r\n                next_inp.append(h.full_prediction[-1])\r\n                new_decoder_hidden.append(h.hidden_state)\r\n                new_cov_ref.append(h.cov_ref)\r\n                new_cov_mem.append(h.cov_mem)\r\n            if len(new_beam) >= self.beam_size:\r\n                break\r\n        assert len(new_beam) >= 1, \'Non-existent beam\'\r\n        # print(next_inp)\r\n        return new_beam, torch.LongTensor(next_inp).to(self.device).view(-1, 1), results, \\\r\n               torch.cat(new_decoder_hidden, 0).unsqueeze(0), torch.cat(new_cov_ref, 0), torch.cat(new_cov_mem, 0)\r\n\r\n    # Beam Search Decoding\r\n    def decode(self, max_source_oov=0, sources_ids=None, enc_mask=None, encoder_hidden=None, encoder_outputs=None,\r\n               term_id=None, term_mask=None, term_output=None, stopwords=None, sflag=False):\r\n        # print(encoder_outputs.size())\r\n        # print(term_output.size())\r\n        max_length = self.max_length\r\n        if not sflag:\r\n            stopwords = set(range(max_source_oov + self.vocab_size))\r\n        targets = torch.LongTensor([[self.sos_id]]).to(self.device)\r\n        decoder_hidden = self._init_state(encoder_hidden)\r\n        max_enc_len = encoder_outputs.size(1)\r\n        max_term_len = term_id.size(1)\r\n        cov_ref = torch.zeros(1, max_enc_len)\r\n        cov_ref = cov_ref.to(self.device)\r\n        cov_mem = torch.zeros(1, max_term_len)\r\n        cov_mem = cov_mem.to(self.device)\r\n        # memory initialization\r\n        decoder_hidden, _ = self.memory_init(decoder_hidden, term_output, term_mask)\r\n\r\n        decoded_outputs = []\r\n        # all_hyps --> list of current beam hypothesis. start with base initial hypothesis\r\n        all_hyps = [Hypothesis([self.sos_id], decoder_hidden, cov_ref, cov_mem, 0, stopwords)]\r\n        # start decoding\r\n        enc_proj = self.Wih(encoder_outputs.contiguous().view(max_enc_len, -1)).view(1, max_enc_len, -1)\r\n        # print(enc_proj.size())\r\n        embed_target = self.embedding(targets)\r\n        # print(embed_target.size())\r\n        for _step in range(max_length):\r\n            # print(_step)\r\n            # after first step, input is of batch_size=curr_beam_size\r\n            # curr_beam_size <= self.beam_size due to pruning of beams that have terminated\r\n            # adjust enc_states and init_state accordingly\r\n            _h, decoder_hidden = self.rnn(embed_target, decoder_hidden)\r\n            # print(decoder_hidden.size())\r\n            curr_beam_size = embed_target.size(0)\r\n            # print(\'curr_beam_size\', curr_beam_size)\r\n\r\n            combined_vocab, ref_attn, term_attn = self.decode_step(sources_ids, _h.squeeze(1), enc_proj, curr_beam_size,\r\n                                                                   cov_ref, cov_mem, max_enc_len, enc_mask,\r\n                                                                   encoder_outputs, embed_target.squeeze(1),\r\n                                                                   max_source_oov, term_output, term_id, term_mask)\r\n            combined_vocab[:, self.unk_id] = 0\r\n\r\n            # does bulk of the beam search\r\n            # decoded_outputs --> list of all ouputs terminated with stop tokens and of minimal length\r\n            all_hyps, symbols, decoded_outputs, decoder_hidden, cov_ref, cov_mem = self.getOverallTopk(combined_vocab,\r\n                                ref_attn, term_attn, cov_ref, cov_mem,\r\n                                all_hyps, decoded_outputs,\r\n                                decoder_hidden.squeeze(0), stopwords)\r\n\r\n            symbols.masked_fill_((symbols > self.vocab_size - 1), self.unk_id)\r\n            embed_target = self.embedding(symbols)\r\n            # print(\'embed_target\', embed_target.size())\r\n            curr_beam_size = embed_target.size(0)\r\n            # print(\'curr_beam_size\', curr_beam_size)\r\n            if embed_target.size(0) > encoder_outputs.size(0):\r\n                encoder_outputs = encoder_outputs.expand(curr_beam_size, -1, -1).contiguous()\r\n                enc_mask = enc_mask.expand(curr_beam_size, -1).contiguous()\r\n                sources_ids = sources_ids.expand(curr_beam_size, -1).contiguous()\r\n                term_id = term_id.expand(curr_beam_size, -1).contiguous()\r\n                term_mask = term_mask.expand(curr_beam_size, -1).contiguous()\r\n                term_output = term_output.expand(curr_beam_size, -1, -1).contiguous()\r\n                enc_proj = self.Wih(encoder_outputs.contiguous().view(curr_beam_size * max_enc_len, -1))\\\r\n                    .view(curr_beam_size, max_enc_len, -1)\r\n                # print(\'encoder proj\', enc_proj.size())\r\n        if len(decoded_outputs) > 0:\r\n            candidates = decoded_outputs\r\n        else:\r\n            candidates = all_hyps\r\n        all_outputs = sorted(candidates, key=lambda x:x.survivability, reverse=True)\r\n        return all_outputs[0].full_prediction #\r\n\r\n\r\nclass Hypothesis(object):\r\n    def __init__(self, token_id, hidden_state, cov_ref, cov_mem, log_prob, stopwords):\r\n        self._h = hidden_state\r\n        self.log_prob = log_prob\r\n        self.hidden_state = hidden_state\r\n        self.cov_ref = cov_ref.detach()\r\n        self.cov_mem = cov_mem.detach()\r\n        self.full_prediction = token_id # list\r\n        self.used_words = list(set(token_id) - stopwords)\r\n        self.survivability = self.log_prob/ float(len(self.full_prediction))\r\n\r\n    def extend(self, token_id, hidden_state, cov_ref, cov_mem, log_prob, stopwords):\r\n        return Hypothesis(token_id= self.full_prediction + [token_id],\r\n                          hidden_state=hidden_state,\r\n                          cov_ref=cov_ref.detach(),\r\n                          cov_mem=cov_mem.detach(),\r\n                          log_prob= self.log_prob + log_prob,\r\n                          stopwords=stopwords)\r\n'"
New paper writing/memory_generator/Encoder.py,1,"b""import torch.nn as nn\r\nfrom .baseRNN import BaseRNN\r\n\r\n\r\nclass EncoderRNN(BaseRNN):\r\n\r\n    def __init__(self, vocab_size, embedding, hidden_size, input_dropout_p,\r\n                 n_layers=1, bidirectional=True, rnn_type='gru'):\r\n        super(EncoderRNN, self).__init__(vocab_size, hidden_size, input_dropout_p, n_layers, rnn_type)\r\n\r\n        self.embedding = embedding\r\n        self.rnn = self.rnn_cell(hidden_size, hidden_size, n_layers,\r\n                                 batch_first=True, bidirectional=bidirectional)\r\n\r\n    def forward(self, source, input_lengths=None):\r\n\r\n        # get mask for location of PAD\r\n        mask = source.eq(0).detach()\r\n\r\n        embedded = self.embedding(source)\r\n        embedded = self.input_dropout(embedded)\r\n        embedded = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, batch_first=True)\r\n\r\n        output, hidden = self.rnn(embedded)\r\n\r\n        output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\r\n        # print('o', output.size())\r\n        return output, hidden, mask\r\n\r\n\r\nclass TermEncoder(nn.Module):\r\n\r\n    def __init__(self, embedding, input_dropout_p):\r\n        super(TermEncoder, self).__init__()\r\n\r\n        self.embedding = embedding\r\n        self.input_dropout = nn.Dropout(input_dropout_p)\r\n\r\n    def forward(self, term):\r\n        mask = term.eq(0).detach()\r\n        embedded = self.embedding(term)\r\n        embedded = self.input_dropout(embedded)\r\n        return embedded, mask\r\n"""
New paper writing/memory_generator/baseRNN.py,1,"b'"""""" A base class for RNN. """"""\r\nimport torch.nn as nn\r\n\r\n\r\nclass BaseRNN(nn.Module):\r\n    def __init__(self, vocab_size, hidden_size, input_dropout_p, n_layers, rnn_cell):\r\n        super(BaseRNN, self).__init__()\r\n        self.vocab_size = vocab_size\r\n        self.hidden_size = hidden_size\r\n        self.n_layers = n_layers\r\n        self.input_dropout_p = input_dropout_p\r\n        self.input_dropout = nn.Dropout(input_dropout_p)\r\n        if rnn_cell.lower() == \'lstm\':\r\n            self.rnn_cell = nn.LSTM\r\n        elif rnn_cell.lower() == \'gru\':\r\n            self.rnn_cell = nn.GRU\r\n        else:\r\n            raise ValueError(""Unsupported RNN Cell: {0}"".format(rnn_cell))\r\n\r\n    def forward(self, *args, **kwargs):\r\n        raise NotImplementedError()\r\n'"
New paper writing/memory_generator/predictor.py,6,"b'import gc\nfrom itertools import groupby\nimport torch\nimport statistics\n\n\ndef filter_duplicate(sents):\n    sents = sents.split(\'.\')\n    used = []\n    used_s = []\n    tmp = """"\n    for ss in sents:\n        tttmp = \'\'\n        for s in ss.split(\',\'):\n            if s not in used:\n                if len(s) < 2:\n                    continue\n                used.append(s)\n                no_dupes = ([k for k, v in groupby(s.split())])\n                ns = \' \'.join(no_dupes)\n                if ns not in used_s:\n                    used_s.append(ns)\n                    if s[-1] == \',\':\n                        tttmp += ns + \' \'\n                    else:\n                        tttmp += ns + \' , \'\n        if len(tttmp) == 0:\n            continue\n        tttmp = ""%s%s"" % (tttmp[0].upper(), tttmp[1:])\n        if tttmp[-1] == \'.\':\n            tmp += tttmp + \' \'\n        else:\n            if tttmp[-2:] == \', \':\n                tmp += tttmp[:-2]\n            else:\n                tmp += tttmp\n            tmp += \' . \'\n    return tmp\n\n\nclass Predictor(object):\n    def __init__(self, model, id2word, vocab_size):\n        self.model = model\n        self.model.eval()\n        self.id2word = id2word\n        self.vocab_size = vocab_size\n\n    def predict(self, batch_s, batch_o_s, source_len, max_source_oov, batch_term, batch_o_term, list_oovs):\n        torch.set_grad_enabled(False)\n        decoded_outputs, lengths = self.model(batch_s, batch_o_s, source_len, max_source_oov, batch_term,\n                                            batch_o_term)\n        length = lengths[0]\n        output = []\n        # print(decoded_outputs)\n        for i in range(length):\n            symbol = decoded_outputs[0][i].item()\n            if symbol < self.vocab_size:\n                output.append(self.id2word[symbol])\n            else:\n                output.append(list_oovs[0][symbol-self.vocab_size])\n        return self.prepare_for_bleu(output, True)[0]\n\n    def predict_beam(self, batch_s, batch_o_s, source_len, max_source_oov, batch_term, batch_o_term, list_oovs,\n                     stopwords, sflag=False):\n        torch.set_grad_enabled(False)\n        decoded_outputs = self.model(batch_s, batch_o_s, source_len, max_source_oov, batch_term, batch_o_term,\n                                     beam=True, stopwords=stopwords, sflag=sflag)\n        outputs = []\n        for symbol in decoded_outputs:\n            if symbol < self.vocab_size:\n                outputs.append(self.id2word[symbol])\n            else:\n                outputs.append(list_oovs[0][symbol - self.vocab_size])\n        outputs = self.prepare_for_bleu(outputs, True)[0]\n        print(outputs)\n        return outputs\n\n    def preeval_batch(self, dataset, pmid=False):\n        torch.set_grad_enabled(False)\n        refs = {}\n        cands = {}\n        titles = {}\n        new_terms = {}\n        new_pmids = {}\n        avg_len_ref = []\n        avg_len_out = []\n        i = 0\n        for batch_idx in range(len(dataset.corpus)):\n            if pmid:\n                batch_s, batch_o_s, source_len, max_source_oov, batch_term, batch_o_term, list_oovs, targets, \\\n                 sources, terms, pmids = dataset.get_batch(batch_idx, False)\n            else:\n                batch_s, batch_o_s, source_len, max_source_oov, batch_term, batch_o_term, list_oovs, targets, \\\n                 sources, terms = dataset.get_batch(batch_idx, False)\n            decoded_outputs, lengths = self.model(batch_s, batch_o_s, source_len, max_source_oov, batch_term,\n                                                  batch_o_term)\n            for j in range(len(lengths)):\n                i += 1\n                ref, lref = self.prepare_for_bleu(targets[j])\n                if pmid:\n                    refs[i] = ref.split()\n                    titles[i] = sources[j]\n                    new_terms[i] = terms[j]\n                else:\n                    avg_len_ref.append(lref)\n                    refs[i] = [ref]\n                    titles[i] = "" "".join(sources[j])\n                    new_terms[i] = "" "".join(terms[j])\n                out_seq = []\n                for k in range(lengths[j]):\n                    symbol = decoded_outputs[j][k].item()\n                    if symbol < self.vocab_size:\n                        out_seq.append(self.id2word[symbol])\n                    else:\n                        out_seq.append(list_oovs[j][symbol-self.vocab_size])\n                out, lout = self.prepare_for_bleu(out_seq, True)\n                if pmid:\n                    new_pmids[i] = pmids[j]\n                    cands[i] = out.split()\n                else:\n                    avg_len_out.append(lout)\n                    cands[i] = out\n\n                if i % 500 == 0:\n                    print(""Percentages:  %.4f"" % (i/float(dataset.len)))\n\n            # del batch_s, batch_o_s, source_len, batch_term, batch_o_term\n            # gc.collect()\n            # torch.cuda.empty_cache()\n\n        if pmid:\n            return cands, refs, titles, new_terms, new_pmids\n        else:\n            print(""Reference length "", statistics.mean(avg_len_ref))\n            print(""Output length "", statistics.mean(avg_len_out))\n            return cands, refs, titles, new_terms\n\n    def preeval_batch_beam(self, dataset, pmid=False, stopwords=None, sflag=True):\n        torch.set_grad_enabled(False)\n        refs = {}\n        cands = {}\n        titles = {}\n        new_terms = {}\n        new_pmids = {}\n        avg_len_ref = []\n        avg_len_out = []\n        i = 0\n        for batch_idx in range(len(dataset.corpus)): #\n            if pmid:\n                batch_s, batch_o_s, source_len, max_source_oov, batch_term, batch_o_term, list_oovs, targets, \\\n                 sources, terms, pmids = dataset.get_batch(batch_idx, False)\n            else:\n                batch_s, batch_o_s, source_len, max_source_oov, batch_term, batch_o_term, list_oovs, targets, \\\n                 sources, terms = dataset.get_batch(batch_idx, False)\n            decoded_outputs = self.model(batch_s, batch_o_s, source_len, max_source_oov, batch_term,\n                                                  batch_o_term, beam=True, stopwords=stopwords, sflag=sflag)\n            i += 1\n            ref, lref = self.prepare_for_bleu(targets[0])\n            if pmid:\n                refs[i] = ref.split()\n                titles[i] = sources[0]\n                new_terms[i] = terms[0]\n            else:\n                avg_len_ref.append(lref)\n                refs[i] = [ref]\n                titles[i] = "" "".join(sources[0])\n                new_terms[i] = "" "".join(terms[0])\n            out_seq = []\n            for symbol in decoded_outputs:\n                if symbol < self.vocab_size:\n                    out_seq.append(self.id2word[symbol])\n                else:\n                    out_seq.append(list_oovs[0][symbol-self.vocab_size])\n            out, lout = self.prepare_for_bleu(out_seq, True)\n            if pmid:\n                new_pmids[i] = pmids[0]\n                cands[i] = out.split()\n            else:\n                avg_len_out.append(lout)\n                cands[i] = out\n\n            if i % 10 == 0:\n                print(""Percentages:  %.4f"" % (i/float(dataset.len)))\n\n            # del batch_s, batch_o_s, source_len, batch_term, batch_o_term\n            # gc.collect()\n            # torch.cuda.empty_cache()\n\n        if pmid:\n            return cands, refs, titles, new_terms, new_pmids\n        else:\n            print(""Reference length "", statistics.mean(avg_len_ref))\n            print(""Output length "", statistics.mean(avg_len_out))\n            return cands, refs, titles, new_terms\n\n    def prepare_for_bleu(self, sentence, train=False):\n        sent = [x for x in sentence if x != \'<pad>\' and x != \'<eos>\' and x != \'<sos>\']\n        l = len(sent)\n        sent = \' \'.join(sent)\n        if train:\n            sent = filter_duplicate(sent)\n        return sent, l\n'"
New paper writing/memory_generator/seq2seq.py,1,"b'import torch.nn as nn\n\n\nclass Seq2seq(nn.Module):\n\n    def __init__(self, ref_encoder, term_encoder, decoder):\n        super(Seq2seq, self).__init__()\n        self.ref_encoder = ref_encoder\n        self.term_encoder = term_encoder\n        self.decoder = decoder\n\n    def forward(self, batch_s, batch_o_s, source_len, max_source_oov, batch_term, batch_o_term, batch_t=None,\n                batch_o_t=None, teacher_forcing_ratio=0, beam=False, stopwords=None, sflag=False): # w2fs=None\n        encoder_outputs, encoder_hidden, enc_mask = self.ref_encoder(batch_s, source_len)\n        term_output, term_mask = self.term_encoder(batch_term)\n        result = self.decoder(max_source_oov, batch_t, batch_o_t, batch_o_s, enc_mask, encoder_hidden,\n                              encoder_outputs, batch_o_term, term_mask, term_output, teacher_forcing_ratio, beam,\n                              stopwords, sflag)\n        return result\n'"
New paper writing/memory_generator/utils.py,4,"b'import torch\nimport copy\nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef clones(module, N):\n    ""Produce N identical layers.""\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n\n\nclass MemoryComponent(nn.Module):\n\n    def __init__(self, hop, h, d_model, dropout_p):\n        super(MemoryComponent, self).__init__()\n        self.max_hops = hop\n        self.h = h\n        vt = nn.Linear(d_model, 1)\n        self.vt_layers = clones(vt, hop)\n        Wih = nn.Linear(d_model, d_model)\n        self.Wih_layers = clones(Wih, hop)\n        Ws = nn.Linear(d_model, d_model)\n        self.Ws_layers = clones(Ws, hop)\n        self.Wc = nn.Linear(1, d_model)\n\n    def forward(self, query, src, src_mask, cov_mem=None):\n        u = query.transpose(0, 1)\n        batch_size, max_enc_len = src_mask.size()\n        for i in range(self.max_hops ):\n            enc_proj = self.Wih_layers[i](src.view(batch_size * max_enc_len, -1)).view(batch_size, max_enc_len, -1)\n            dec_proj = self.Ws_layers[i](u).expand_as(enc_proj)\n            if cov_mem is not None:\n                cov_proj = self.Wc(cov_mem.view(-1, 1)).view(batch_size, max_enc_len, -1)\n                e_t = self.vt_layers[i](torch.tanh(enc_proj + dec_proj + cov_proj).view(batch_size * max_enc_len, -1))\n            else:\n                e_t = self.vt_layers[i](torch.tanh(enc_proj + dec_proj).view(batch_size * max_enc_len, -1))\n            term_attn = e_t.view(batch_size, max_enc_len)\n            del e_t\n            term_attn.data.masked_fill_(src_mask.data.byte(), -float(\'inf\'))\n            term_attn = F.softmax(term_attn, dim=1)\n            term_context = term_attn.unsqueeze(1).bmm(src)\n            u = u + term_context\n        return term_context.transpose(0, 1), term_attn\n'"
New paper writing/pycocoevalcap/__init__.py,0,"b""__author__ = 'tylin'\n"""
New paper writing/utils/__init__.py,0,b''
New paper writing/utils/optim.py,1,"b'import torch.optim as optim\n\n\ndef get_optimizer(model, lr_method, lr_rate):\n    """"""\n    parse optimization method parameters, and initialize optimizer function\n    """"""\n    lr_method_name = lr_method\n\n    # initialize optimizer function\n    if lr_method_name == \'sgd\':\n        optimizer = optim.SGD(model.parameters(), lr=lr_rate, momentum=0.9)\n        # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n    elif lr_method_name == \'adagrad\':\n        optimizer = optim.Adagrad(model.parameters(), lr=lr_rate)\n        # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n    elif lr_method_name == \'adam\':\n        optimizer = optim.Adam(model.parameters(), lr=lr_rate)\n        # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.001)\n    else:\n        raise Exception(\'unknown optimization method.\')\n\n    return optimizer # , scheduler\n'"
New paper writing/pycocoevalcap/bleu/__init__.py,0,"b""__author__ = 'tylin'\n"""
New paper writing/pycocoevalcap/bleu/bleu.py,0,"b'# -*- coding: utf-8 -*-\n# File Name : bleu.py\n#\n# Description : Wrapper for BLEU scorer.\n#\n# Creation Date : 06-01-2015\n# Last Modified : Thu 19 Mar 2015 09:13:28 PM PDT\n# Authors : Hao Fang <hfang@uw.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n\nfrom .bleu_scorer import BleuScorer\n\nclass Bleu:\n    def __init__(self, n=4):\n        # default compute Blue score up to 4\n        self._n = n\n        self._hypo_for_image = {}\n        self.ref_for_image = {}\n\n    def compute_score(self, gts, res):\n\n        bleu_scorer = BleuScorer(n=self._n)\n        for id in sorted(gts.keys()):\n            hypo = res[id]\n            ref = gts[id]\n\n            # Sanity check.\n            assert(type(hypo) is list)\n            assert(len(hypo) == 1)\n            assert(type(ref) is list)\n            assert(len(ref) >= 1)\n\n            bleu_scorer += (hypo[0], ref)\n\n        #score, scores = bleu_scorer.compute_score(option=\'shortest\')\n        #score, scores = bleu_scorer.compute_score(option=\'average\', verbose=1)\n        score, scores = bleu_scorer.compute_score(option=\'closest\', verbose=0)\n\n        # return (bleu, bleu_info)\n        return score, scores\n\n    def method(self):\n        return ""Bleu""\n'"
New paper writing/pycocoevalcap/bleu/bleu_scorer.py,0,"b'# -*- coding: utf-8 -*-\n# bleu_scorer.py\n# David Chiang <chiang@isi.edu>\n\n# Copyright (c) 2004-2006 University of Maryland. All rights\n# reserved. Do not redistribute without permission from the\n# author. Not for commercial use.\n\n# Modified by:\n# Hao Fang <hfang@uw.edu>\n# Tsung-Yi Lin <tl483@cornell.edu>\n\n\'\'\'Provides:\ncook_refs(refs, n=4): Transform a list of reference sentences as strings into a form usable by cook_test().\ncook_test(test, refs, n=4): Transform a test sentence as a string (together with the cooked reference sentences) into a form usable by score_cooked().\n\'\'\'\n\nimport copy\nimport math\nfrom collections import defaultdict\n\ndef precook(s, n=4, out=False):\n    """"""Takes a string as input and returns an object that can be given to\n    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n    can take string arguments as well.""""""\n    words = s.split()\n    counts = defaultdict(int)\n    for k in range(1,n+1):\n        for i in range(len(words)-k+1):\n            ngram = tuple(words[i:i+k])\n            counts[ngram] += 1\n    return (len(words), counts)\n\ndef cook_refs(refs, eff=None, n=4): ## lhuang: oracle will call with ""average""\n    \'\'\'Takes a list of reference sentences for a single segment\n    and returns an object that encapsulates everything that BLEU\n    needs to know about them.\'\'\'\n\n    reflen = []\n    maxcounts = {}\n    for ref in refs:\n        rl, counts = precook(ref, n)\n        reflen.append(rl)\n        for (ngram,count) in counts.items():\n            maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n\n    # Calculate effective reference sentence length.\n    if eff == ""shortest"":\n        reflen = min(reflen)\n    elif eff == ""average"":\n        reflen = float(sum(reflen))/len(reflen)\n\n    ## lhuang: N.B.: leave reflen computaiton to the very end!!\n\n    ## lhuang: N.B.: in case of ""closest"", keep a list of reflens!! (bad design)\n\n    return (reflen, maxcounts)\n\ndef cook_test(test, ref_len_counts, eff=None, n=4):\n    \'\'\'Takes a test sentence and returns an object that\n    encapsulates everything that BLEU needs to know about it.\'\'\'\n    (reflen, refmaxcounts) = ref_len_counts\n    testlen, counts = precook(test, n, True)\n\n    result = {}\n\n    # Calculate effective reference sentence length.\n\n    if eff == ""closest"":\n        result[""reflen""] = min((abs(l-testlen), l) for l in reflen)[1]\n    else: ## i.e., ""average"" or ""shortest"" or None\n        result[""reflen""] = reflen\n\n    result[""testlen""] = testlen\n\n    result[""guess""] = [max(0,testlen-k+1) for k in range(1,n+1)]\n\n    result[\'correct\'] = [0]*n\n    for (ngram, count) in counts.items():\n        result[""correct""][len(ngram)-1] += min(refmaxcounts.get(ngram,0), count)\n\n    return result\n\nclass BleuScorer(object):\n    """"""Bleu scorer.\n    """"""\n\n    __slots__ = ""n"", ""crefs"", ""ctest"", ""_score"", ""_ratio"", ""_testlen"", ""_reflen"", ""special_reflen""\n    # special_reflen is used in oracle (proportional effective ref len for a node).\n\n    def copy(self):\n        \'\'\' copy the refs.\'\'\'\n        new = BleuScorer(n=self.n)\n        new.ctest = copy.copy(self.ctest)\n        new.crefs = copy.copy(self.crefs)\n        new._score = None\n        return new\n\n    def __init__(self, test=None, refs=None, n=4, special_reflen=None):\n        \'\'\' singular instance \'\'\'\n\n        self.n = n\n        self.crefs = []\n        self.ctest = []\n        self.cook_append(test, refs)\n        self.special_reflen = special_reflen\n\n    def cook_append(self, test, refs):\n        \'\'\'called by constructor and __iadd__ to avoid creating new instances.\'\'\'\n\n        if refs is not None:\n            self.crefs.append(cook_refs(refs))\n            if test is not None:\n                cooked_test = cook_test(test, self.crefs[-1])\n                self.ctest.append(cooked_test) ## N.B.: -1\n            else:\n                self.ctest.append(None) # lens of crefs and ctest have to match\n\n        self._score = None ## need to recompute\n\n    def ratio(self, option=None):\n        self.compute_score(option=option)\n        return self._ratio\n\n    def score_ratio(self, option=None):\n        \'\'\'return (bleu, len_ratio) pair\'\'\'\n        return (self.fscore(option=option), self.ratio(option=option))\n\n    def score_ratio_str(self, option=None):\n        return ""%.4f (%.2f)"" % self.score_ratio(option)\n\n    def reflen(self, option=None):\n        self.compute_score(option=option)\n        return self._reflen\n\n    def testlen(self, option=None):\n        self.compute_score(option=option)\n        return self._testlen\n\n    def retest(self, new_test):\n        if type(new_test) is str:\n            new_test = [new_test]\n        assert len(new_test) == len(self.crefs), new_test\n        self.ctest = []\n        for t, rs in zip(new_test, self.crefs):\n            self.ctest.append(cook_test(t, rs))\n        self._score = None\n\n        return self\n\n    def rescore(self, new_test):\n        \'\'\' replace test(s) with new test(s), and returns the new score.\'\'\'\n\n        return self.retest(new_test).compute_score()\n\n    def size(self):\n        assert len(self.crefs) == len(self.ctest), ""refs/test mismatch! %d<>%d"" % (len(self.crefs), len(self.ctest))\n        return len(self.crefs)\n\n    def __iadd__(self, other):\n        \'\'\'add an instance (e.g., from another sentence).\'\'\'\n\n        if type(other) is tuple:\n            ## avoid creating new BleuScorer instances\n            self.cook_append(other[0], other[1])\n        else:\n            assert self.compatible(other), ""incompatible BLEUs.""\n            self.ctest.extend(other.ctest)\n            self.crefs.extend(other.crefs)\n            self._score = None ## need to recompute\n\n        return self\n\n    def compatible(self, other):\n        return isinstance(other, BleuScorer) and self.n == other.n\n\n    def single_reflen(self, option=""average""):\n        return self._single_reflen(self.crefs[0][0], option)\n\n    def _single_reflen(self, reflens, option=None, testlen=None):\n\n        if option == ""shortest"":\n            reflen = min(reflens)\n        elif option == ""average"":\n            reflen = float(sum(reflens))/len(reflens)\n        elif option == ""closest"":\n            reflen = min((abs(l-testlen), l) for l in reflens)[1]\n        else:\n            assert False, ""unsupported reflen option %s"" % option\n\n        return reflen\n\n    def recompute_score(self, option=None, verbose=0):\n        self._score = None\n        return self.compute_score(option, verbose)\n\n    def compute_score(self, option=None, verbose=0):\n        n = self.n\n        small = 1e-9\n        tiny = 1e-15 ## so that if guess is 0 still return 0\n        bleu_list = [[] for _ in range(n)]\n\n        if self._score is not None:\n            return self._score\n\n        if option is None:\n            option = ""average"" if len(self.crefs) == 1 else ""closest""\n\n        self._testlen = 0\n        self._reflen = 0\n        totalcomps = {\'testlen\':0, \'reflen\':0, \'guess\':[0]*n, \'correct\':[0]*n}\n\n        # for each sentence\n        for comps in self.ctest:\n            testlen = comps[\'testlen\']\n            self._testlen += testlen\n\n            if self.special_reflen is None: ## need computation\n                reflen = self._single_reflen(comps[\'reflen\'], option, testlen)\n            else:\n                reflen = self.special_reflen\n\n            self._reflen += reflen\n\n            for key in [\'guess\',\'correct\']:\n                for k in range(n):\n                    totalcomps[key][k] += comps[key][k]\n\n            # append per image bleu score\n            bleu = 1.\n            for k in range(n):\n                bleu *= (float(comps[\'correct\'][k]) + tiny) \\\n                        /(float(comps[\'guess\'][k]) + small)\n                bleu_list[k].append(bleu ** (1./(k+1)))\n            ratio = (testlen + tiny) / (reflen + small) ## N.B.: avoid zero division\n            if ratio < 1:\n                for k in range(n):\n                    bleu_list[k][-1] *= math.exp(1 - 1/ratio)\n\n            if verbose > 1:\n                print(comps, reflen)\n\n        totalcomps[\'reflen\'] = self._reflen\n        totalcomps[\'testlen\'] = self._testlen\n\n        bleus = []\n        bleu = 1.\n        for k in range(n):\n            bleu *= float(totalcomps[\'correct\'][k] + tiny) \\\n                    / (totalcomps[\'guess\'][k] + small)\n            bleus.append(bleu ** (1./(k+1)))\n        ratio = (self._testlen + tiny) / (self._reflen + small) ## N.B.: avoid zero division\n        if ratio < 1:\n            for k in range(n):\n                bleus[k] *= math.exp(1 - 1/ratio)\n\n        if verbose > 0:\n            print(totalcomps)\n            print(""ratio:"", ratio)\n\n        # Normalize to percentage\n        bleus = [100*b for b in bleus]\n        self._score = bleus\n        return self._score, bleu_list\n'"
New paper writing/pycocoevalcap/cider/__init__.py,0,"b""__author__ = 'tylin'\n"""
New paper writing/pycocoevalcap/cider/cider.py,0,"b'# -*- coding: utf-8 -*-\n# Filename: cider.py\n#\n# Description: Describes the class to compute the CIDEr (Consensus-Based Image Description Evaluation) Metric\n#               by Vedantam, Zitnick, and Parikh (http://arxiv.org/abs/1411.5726)\n#\n# Creation Date: Sun Feb  8 14:16:54 2015\n#\n# Authors: Ramakrishna Vedantam <vrama91@vt.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n\nfrom .cider_scorer import CiderScorer\n\nclass Cider(object):\n    """"""\n    Main Class to compute the CIDEr metric\n    """"""\n    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n        # set cider to sum over 1 to 4-grams\n        self._n = n\n        # set the standard deviation parameter for gaussian penalty\n        self._sigma = sigma\n\n    def compute_score(self, gts, res):\n        """"""\n        Main function to compute CIDEr score\n        :param  hypo_for_image (dict) : dictionary with key <image> and value <tokenized hypothesis / candidate sentence>\n                ref_for_image (dict)  : dictionary with key <image> and value <tokenized reference sentence>\n        :return: cider (float) : computed CIDEr score for the corpus\n        """"""\n\n        cider_scorer = CiderScorer(n=self._n, sigma=self._sigma)\n\n        for id in sorted(gts.keys()):\n            hypo = res[id]\n            ref = gts[id]\n\n            # Sanity check.\n            assert(type(hypo) is list)\n            assert(len(hypo) == 1)\n            assert(type(ref) is list)\n            assert(len(ref) > 0)\n\n            cider_scorer += (hypo[0], ref)\n\n        (score, scores) = cider_scorer.compute_score()\n\n        return score, scores\n\n    def method(self):\n        return ""CIDEr""\n'"
New paper writing/pycocoevalcap/cider/cider_scorer.py,0,"b'# -*- coding: utf-8 -*-\n# Tsung-Yi Lin <tl483@cornell.edu>\n# Ramakrishna Vedantam <vrama91@vt.edu>\n\nimport copy\nfrom collections import defaultdict\nimport numpy as np\nimport math\n\ndef precook(s, n=4, out=False):\n    """"""\n    Takes a string as input and returns an object that can be given to\n    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n    can take string arguments as well.\n    :param s: string : sentence to be converted into ngrams\n    :param n: int    : number of ngrams for which representation is calculated\n    :return: term frequency vector for occuring ngrams\n    """"""\n    words = s.split()\n    counts = defaultdict(int)\n    for k in range(1,n+1):\n        for i in range(len(words)-k+1):\n            ngram = tuple(words[i:i+k])\n            counts[ngram] += 1\n    return counts\n\ndef cook_refs(refs, n=4): ## lhuang: oracle will call with ""average""\n    \'\'\'Takes a list of reference sentences for a single segment\n    and returns an object that encapsulates everything that BLEU\n    needs to know about them.\n    :param refs: list of string : reference sentences for some image\n    :param n: int : number of ngrams for which (ngram) representation is calculated\n    :return: result (list of dict)\n    \'\'\'\n    return [precook(ref, n) for ref in refs]\n\ndef cook_test(test, n=4):\n    \'\'\'Takes a test sentence and returns an object that\n    encapsulates everything that BLEU needs to know about it.\n    :param test: list of string : hypothesis sentence for some image\n    :param n: int : number of ngrams for which (ngram) representation is calculated\n    :return: result (dict)\n    \'\'\'\n    return precook(test, n, True)\n\nclass CiderScorer(object):\n    """"""CIDEr scorer.\n    """"""\n\n    def copy(self):\n        \'\'\' copy the refs.\'\'\'\n        new = CiderScorer(n=self.n)\n        new.ctest = copy.copy(self.ctest)\n        new.crefs = copy.copy(self.crefs)\n        return new\n\n    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n        \'\'\' singular instance \'\'\'\n        self.n = n\n        self.sigma = sigma\n        self.crefs = []\n        self.ctest = []\n        self.document_frequency = defaultdict(float)\n        self.cook_append(test, refs)\n        self.ref_len = None\n\n    def cook_append(self, test, refs):\n        \'\'\'called by constructor and __iadd__ to avoid creating new instances.\'\'\'\n\n        if refs is not None:\n            self.crefs.append(cook_refs(refs))\n            if test is not None:\n                self.ctest.append(cook_test(test)) ## N.B.: -1\n            else:\n                self.ctest.append(None) # lens of crefs and ctest have to match\n\n    def size(self):\n        assert len(self.crefs) == len(self.ctest), ""refs/test mismatch! %d<>%d"" % (len(self.crefs), len(self.ctest))\n        return len(self.crefs)\n\n    def __iadd__(self, other):\n        \'\'\'add an instance (e.g., from another sentence).\'\'\'\n\n        if type(other) is tuple:\n            ## avoid creating new CiderScorer instances\n            self.cook_append(other[0], other[1])\n        else:\n            self.ctest.extend(other.ctest)\n            self.crefs.extend(other.crefs)\n\n        return self\n    def compute_doc_freq(self):\n        \'\'\'\n        Compute term frequency for reference data.\n        This will be used to compute idf (inverse document frequency later)\n        The term frequency is stored in the object\n        :return: None\n        \'\'\'\n        for refs in self.crefs:\n            # refs, k ref captions of one image\n            for ngram in set([ngram for ref in refs for (ngram,count) in ref.items()]):\n                self.document_frequency[ngram] += 1\n            # maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n\n    def compute_cider(self):\n        def counts2vec(cnts):\n            """"""\n            Function maps counts of ngram to vector of tfidf weights.\n            The function returns vec, an array of dictionary that store mapping of n-gram and tf-idf weights.\n            The n-th entry of array denotes length of n-grams.\n            :param cnts:\n            :return: vec (array of dict), norm (array of float), length (int)\n            """"""\n            vec = [defaultdict(float) for _ in range(self.n)]\n            length = 0\n            norm = [0.0 for _ in range(self.n)]\n            for (ngram,term_freq) in cnts.items():\n                # give word count 1 if it doesn\'t appear in reference corpus\n                df = np.log(max(1.0, self.document_frequency[ngram]))\n                # ngram index\n                n = len(ngram)-1\n                # tf (term_freq) * idf (precomputed idf) for n-grams\n                vec[n][ngram] = float(term_freq)*(self.ref_len - df)\n                # compute norm for the vector.  the norm will be used for computing similarity\n                norm[n] += pow(vec[n][ngram], 2)\n\n                if n == 1:\n                    length += term_freq\n            norm = [np.sqrt(n) for n in norm]\n            return vec, norm, length\n\n        def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):\n            \'\'\'\n            Compute the cosine similarity of two vectors.\n            :param vec_hyp: array of dictionary for vector corresponding to hypothesis\n            :param vec_ref: array of dictionary for vector corresponding to reference\n            :param norm_hyp: array of float for vector corresponding to hypothesis\n            :param norm_ref: array of float for vector corresponding to reference\n            :param length_hyp: int containing length of hypothesis\n            :param length_ref: int containing length of reference\n            :return: array of score for each n-grams cosine similarity\n            \'\'\'\n            delta = float(length_hyp - length_ref)\n            # measure consine similarity\n            val = np.array([0.0 for _ in range(self.n)])\n            for n in range(self.n):\n                # ngram\n                for (ngram,count) in vec_hyp[n].items():\n                    # vrama91 : added clipping\n                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]\n\n                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):\n                    val[n] /= (norm_hyp[n]*norm_ref[n])\n\n                assert(not math.isnan(val[n]))\n                # vrama91: added a length based gaussian penalty\n                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))\n            return val\n\n        # compute log reference length\n        self.ref_len = np.log(float(len(self.crefs)))\n\n        scores = []\n        for test, refs in zip(self.ctest, self.crefs):\n            # compute vector for test captions\n            vec, norm, length = counts2vec(test)\n            # compute vector for ref captions\n            score = np.array([0.0 for _ in range(self.n)])\n            for ref in refs:\n                vec_ref, norm_ref, length_ref = counts2vec(ref)\n                score += sim(vec, vec_ref, norm, norm_ref, length, length_ref)\n            # change by vrama91 - mean of ngram scores, instead of sum\n            score_avg = np.mean(score)\n            # divide by number of references\n            score_avg /= len(refs)\n            # multiply score by 10\n            score_avg *= 10.0\n            # append score of an image to the score list\n            scores.append(score_avg)\n        return scores\n\n    def compute_score(self, option=None, verbose=0):\n        # compute idf\n        self.compute_doc_freq()\n        # assert to check document frequency\n        assert(len(self.ctest) >= max(self.document_frequency.values()))\n        # compute cider score\n        score = self.compute_cider()\n        return np.mean(np.array(score)), np.array(score)\n'"
New paper writing/pycocoevalcap/meteor/__init__.py,0,"b""__author__ = 'tylin'\n"""
New paper writing/pycocoevalcap/meteor/meteor.py,0,"b'# -*- coding: utf-8 -*-\n# Python wrapper for METEOR implementation, by Xinlei Chen\n# Acknowledge Michael Denkowski for the generous discussion and help\n\nimport os\nimport threading\nimport subprocess\nimport pkg_resources\n\nMETEOR_JAR = \'meteor-1.5.jar\'\nclass Meteor(object):\n    def __init__(self, language=\'en\', norm=True):\n        self.meteor_cmd = [\'java\', \'-jar\', \'-Xmx2G\', METEOR_JAR, \'-\', \'-\', \'-stdio\', \'-l\', language]\n\n        if norm:\n            self.meteor_cmd.append(\'-norm\')\n\n        self.meteor_p = subprocess.Popen(self.meteor_cmd, stdin=subprocess.PIPE,\n            cwd=os.path.dirname(os.path.abspath(__file__)), \n                                        stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True, bufsize=1)\n        # Used to guarantee thread safety\n        self.lock = threading.Lock()\n\n    def method(self):\n        return ""METEOR""\n\n    def compute_score(self, gts, res):\n        imgIds = sorted(list(gts.keys()))\n        scores = []\n\n        eval_line = \'EVAL\'\n        self.lock.acquire()\n        for i in imgIds:\n            assert(len(res[i]) == 1)\n\n            hypothesis_str = res[i][0].replace(\'|||\', \'\').replace(\'  \', \' \')\n            score_line = \' ||| \'.join((\'SCORE\', \' ||| \'.join(gts[i]), hypothesis_str))\n\n            # We obtained --> SCORE ||| reference 1 words ||| reference n words ||| hypothesis words\n            self.meteor_p.stdin.write(score_line + \'\\n\')\n            stat = self.meteor_p.stdout.readline().strip()\n            eval_line += \' ||| {}\'.format(stat)\n\n        # Send to METEOR\n        self.meteor_p.stdin.write(eval_line + \'\\n\')\n\n        # Collect segment scores\n        for i in range(len(imgIds)):\n            score = float(self.meteor_p.stdout.readline().strip())\n            scores.append(score)\n\n        # Final score\n        final_score = 100*float(self.meteor_p.stdout.readline().strip())\n        self.lock.release()\n\n        return final_score, scores\n\n    def __del__(self):\n        self.lock.acquire()\n        self.meteor_p.stdin.close()\n        self.meteor_p.wait()\n        self.lock.release()\n'"
New paper writing/pycocoevalcap/rouge/__init__.py,0,"b""__author__ = 'vrama91'\n"""
New paper writing/pycocoevalcap/rouge/rouge.py,0,"b'\r\n# -*- coding: utf-8 -*-\r\n# File Name : rouge.py\r\n#\r\n# Description : Computes ROUGE-L metric as described by Lin and Hovey (2004)\r\n#\r\n# Creation Date : 2015-01-07 06:03\r\n# Author : Ramakrishna Vedantam <vrama91@vt.edu>\r\n\r\nimport numpy as np\r\n\r\ndef my_lcs(string, sub):\r\n    """"""\r\n    Calculates longest common subsequence for a pair of tokenized strings\r\n    :param string : list of str : tokens from a string split using whitespace\r\n    :param sub : list of str : shorter string, also split using whitespace\r\n    :returns: length (list of int): length of the longest common subsequence between the two strings\r\n    Note: my_lcs only gives length of the longest common subsequence, not the actual LCS\r\n    """"""\r\n    if(len(string)< len(sub)):\r\n        sub, string = string, sub\r\n\r\n    lengths = [[0 for i in range(0,len(sub)+1)] for j in range(0,len(string)+1)]\r\n\r\n    for j in range(1,len(sub)+1):\r\n        for i in range(1,len(string)+1):\r\n            if(string[i-1] == sub[j-1]):\r\n                lengths[i][j] = lengths[i-1][j-1] + 1\r\n            else:\r\n                lengths[i][j] = max(lengths[i-1][j] , lengths[i][j-1])\r\n\r\n    return lengths[len(string)][len(sub)]\r\n\r\nclass Rouge():\r\n    \'\'\'\r\n    Class for computing ROUGE-L score for a set of candidate sentences for the MS COCO test set\r\n    \'\'\'\r\n    def __init__(self):\r\n        # vrama91: updated the value below based on discussion with Hovey\r\n        self.beta = 1.2\r\n\r\n    def calc_score(self, candidate, refs):\r\n        """"""\r\n        Compute ROUGE-L score given one candidate and references for an image\r\n        :param candidate: str : candidate sentence to be evaluated\r\n        :param refs: list of str : COCO reference sentences for the particular image to be evaluated\r\n        :returns score: int (ROUGE-L score for the candidate evaluated against references)\r\n        """"""\r\n        assert(len(candidate)==1)\r\n        assert(len(refs)>0)\r\n        prec = []\r\n        rec = []\r\n\r\n        # split into tokens\r\n        token_c = candidate[0].split("" "")\r\n\r\n        for reference in refs:\r\n            # split into tokens\r\n            token_r = reference.split("" "")\r\n            # compute the longest common subsequence\r\n            lcs = my_lcs(token_r, token_c)\r\n            prec.append(lcs/float(len(token_c)))\r\n            rec.append(lcs/float(len(token_r)))\r\n\r\n        prec_max = max(prec)\r\n        rec_max = max(rec)\r\n\r\n        if(prec_max!=0 and rec_max !=0):\r\n            score = ((1 + self.beta**2)*prec_max*rec_max)/float(rec_max + self.beta**2*prec_max)\r\n        else:\r\n            score = 0.0\r\n        return score\r\n\r\n    def compute_score(self, gts, res):\r\n        """"""\r\n        Computes Rouge-L score given a set of reference and candidate sentences for the dataset\r\n        Invoked by evaluate_captions.py\r\n        :param hypo_for_image: dict : candidate / test sentences with ""image name"" key and ""tokenized sentences"" as values\r\n        :param ref_for_image: dict : reference MS-COCO sentences with ""image name"" key and ""tokenized sentences"" as values\r\n        :returns: average_score: float (mean ROUGE-L score computed by averaging scores for all the images)\r\n        """"""\r\n        score = []\r\n        for id in sorted(gts.keys()):\r\n            hypo = res[id]\r\n            ref  = gts[id]\r\n\r\n            score.append(self.calc_score(hypo, ref))\r\n\r\n            # Sanity check.\r\n            assert(type(hypo) is list)\r\n            assert(len(hypo) == 1)\r\n            assert(type(ref) is list)\r\n            assert(len(ref) > 0)\r\n\r\n        average_score = np.mean(np.array(score))\r\n        #cconvert to percentage\r\n        return 100*average_score, np.array(score)\r\n\r\n    def method(self):\r\n        return ""Rouge""\r\n'"
