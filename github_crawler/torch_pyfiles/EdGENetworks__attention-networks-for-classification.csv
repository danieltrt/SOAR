file_path,api_count,code
model.py,27,"b""import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\n\n# ## Functions to accomplish attention\n\ndef batch_matmul_bias(seq, weight, bias, nonlinearity=''):\n    s = None\n    bias_dim = bias.size()\n    for i in range(seq.size(0)):\n        _s = torch.mm(seq[i], weight) \n        _s_bias = _s + bias.expand(bias_dim[0], _s.size()[0]).transpose(0,1)\n        if(nonlinearity=='tanh'):\n            _s_bias = torch.tanh(_s_bias)\n        _s_bias = _s_bias.unsqueeze(0)\n        if(s is None):\n            s = _s_bias\n        else:\n            s = torch.cat((s,_s_bias),0)\n    return s.squeeze()\n\n\n\ndef batch_matmul(seq, weight, nonlinearity=''):\n    s = None\n    for i in range(seq.size(0)):\n        _s = torch.mm(seq[i], weight)\n        if(nonlinearity=='tanh'):\n            _s = torch.tanh(_s)\n        _s = _s.unsqueeze(0)\n        if(s is None):\n            s = _s\n        else:\n            s = torch.cat((s,_s),0)\n    return s.squeeze()\n\n\n\ndef attention_mul(rnn_outputs, att_weights):\n    attn_vectors = None\n    for i in range(rnn_outputs.size(0)):\n        h_i = rnn_outputs[i]\n        a_i = att_weights[i].unsqueeze(1).expand_as(h_i)\n        h_i = a_i * h_i\n        h_i = h_i.unsqueeze(0)\n        if(attn_vectors is None):\n            attn_vectors = h_i\n        else:\n            attn_vectors = torch.cat((attn_vectors,h_i),0)\n    return torch.sum(attn_vectors, 0)\n\n\n# ## Word attention model with bias\n\nclass AttentionWordRNN(nn.Module):\n    \n    \n    def __init__(self, batch_size, num_tokens, embed_size, word_gru_hidden, bidirectional= True):        \n        \n        super(AttentionWordRNN, self).__init__()\n        \n        self.batch_size = batch_size\n        self.num_tokens = num_tokens\n        self.embed_size = embed_size\n        self.word_gru_hidden = word_gru_hidden\n        self.bidirectional = bidirectional\n        \n        self.lookup = nn.Embedding(num_tokens, embed_size)\n        if bidirectional == True:\n            self.word_gru = nn.GRU(embed_size, word_gru_hidden, bidirectional= True)\n            self.weight_W_word = nn.Parameter(torch.Tensor(2* word_gru_hidden,2*word_gru_hidden))\n            self.bias_word = nn.Parameter(torch.Tensor(2* word_gru_hidden,1))\n            self.weight_proj_word = nn.Parameter(torch.Tensor(2*word_gru_hidden, 1))\n        else:\n            self.word_gru = nn.GRU(embed_size, word_gru_hidden, bidirectional= False)\n            self.weight_W_word = nn.Parameter(torch.Tensor(word_gru_hidden, word_gru_hidden))\n            self.bias_word = nn.Parameter(torch.Tensor(word_gru_hidden,1))\n            self.weight_proj_word = nn.Parameter(torch.Tensor(word_gru_hidden, 1))\n            \n        self.softmax_word = nn.Softmax()\n        self.weight_W_word.data.uniform_(-0.1, 0.1)\n        self.weight_proj_word.data.uniform_(-0.1,0.1)\n\n        \n        \n    def forward(self, embed, state_word):\n        # embeddings\n        embedded = self.lookup(embed)\n        # word level gru\n        output_word, state_word = self.word_gru(embedded, state_word)\n#         print output_word.size()\n        word_squish = batch_matmul_bias(output_word, self.weight_W_word,self.bias_word, nonlinearity='tanh')\n        word_attn = batch_matmul(word_squish, self.weight_proj_word)\n        word_attn_norm = self.softmax_word(word_attn.transpose(1,0))\n        word_attn_vectors = attention_mul(output_word, word_attn_norm.transpose(1,0))        \n        return word_attn_vectors, state_word, word_attn_norm\n    \n    def init_hidden(self):\n        if self.bidirectional == True:\n            return Variable(torch.zeros(2, self.batch_size, self.word_gru_hidden))\n        else:\n            return Variable(torch.zeros(1, self.batch_size, self.word_gru_hidden))        \n\n\n# ## Sentence Attention model with bias\n\n\nclass AttentionSentRNN(nn.Module):\n    \n    \n    def __init__(self, batch_size, sent_gru_hidden, word_gru_hidden, n_classes, bidirectional= True):        \n        \n        super(AttentionSentRNN, self).__init__()\n        \n        self.batch_size = batch_size\n        self.sent_gru_hidden = sent_gru_hidden\n        self.n_classes = n_classes\n        self.word_gru_hidden = word_gru_hidden\n        self.bidirectional = bidirectional\n        \n        \n        if bidirectional == True:\n            self.sent_gru = nn.GRU(2 * word_gru_hidden, sent_gru_hidden, bidirectional= True)        \n            self.weight_W_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden ,2* sent_gru_hidden))\n            self.bias_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden,1))\n            self.weight_proj_sent = nn.Parameter(torch.Tensor(2* sent_gru_hidden, 1))\n            self.final_linear = nn.Linear(2* sent_gru_hidden, n_classes)\n        else:\n            self.sent_gru = nn.GRU(word_gru_hidden, sent_gru_hidden, bidirectional= False)        \n            self.weight_W_sent = nn.Parameter(torch.Tensor(sent_gru_hidden ,sent_gru_hidden))\n            self.bias_sent = nn.Parameter(torch.Tensor(sent_gru_hidden,1))\n            self.weight_proj_sent = nn.Parameter(torch.Tensor(sent_gru_hidden, 1))\n            self.final_linear = nn.Linear(sent_gru_hidden, n_classes)\n        self.softmax_sent = nn.Softmax()\n        self.final_softmax = nn.Softmax()\n        self.weight_W_sent.data.uniform_(-0.1, 0.1)\n        self.weight_proj_sent.data.uniform_(-0.1,0.1)\n        \n        \n    def forward(self, word_attention_vectors, state_sent):\n        output_sent, state_sent = self.sent_gru(word_attention_vectors, state_sent)        \n        sent_squish = batch_matmul_bias(output_sent, self.weight_W_sent,self.bias_sent, nonlinearity='tanh')\n        sent_attn = batch_matmul(sent_squish, self.weight_proj_sent)\n        sent_attn_norm = self.softmax_sent(sent_attn.transpose(1,0))\n        sent_attn_vectors = attention_mul(output_sent, sent_attn_norm.transpose(1,0))        \n        # final classifier\n        final_map = self.final_linear(sent_attn_vectors.squeeze(0))\n        return F.log_softmax(final_map), state_sent, sent_attn_norm\n    \n    def init_hidden(self):\n        if self.bidirectional == True:\n            return Variable(torch.zeros(2, self.batch_size, self.sent_gru_hidden))\n        else:\n            return Variable(torch.zeros(1, self.batch_size, self.sent_gru_hidden))\n"""
