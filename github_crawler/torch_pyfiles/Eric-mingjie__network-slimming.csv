file_path,api_count,code
denseprune.py,13,"b'import os\nimport argparse\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\nfrom models import *\n\n\n# Prune settings\nparser = argparse.ArgumentParser(description=\'PyTorch Slimming CIFAR prune\')\nparser.add_argument(\'--dataset\', type=str, default=\'cifar100\',\n                    help=\'training dataset (default: cifar10)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=256, metavar=\'N\',\n                    help=\'input batch size for testing (default: 256)\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'disables CUDA training\')\nparser.add_argument(\'--depth\', type=int, default=40,\n                    help=\'depth of the resnet\')\nparser.add_argument(\'--percent\', type=float, default=0.5,\n                    help=\'scale sparse rate (default: 0.5)\')\nparser.add_argument(\'--model\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to the model (default: none)\')\nparser.add_argument(\'--save\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to save pruned model (default: none)\')\n\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nif not os.path.exists(args.save):\n    os.makedirs(args.save)\n\nmodel = densenet(depth=args.depth, dataset=args.dataset)\n\nif args.cuda:\n    model.cuda()\nif args.model:\n    if os.path.isfile(args.model):\n        print(""=> loading checkpoint \'{}\'"".format(args.model))\n        checkpoint = torch.load(args.model)\n        args.start_epoch = checkpoint[\'epoch\']\n        best_prec1 = checkpoint[\'best_prec1\']\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        print(""=> loaded checkpoint \'{}\' (epoch {}) Prec1: {:f}""\n              .format(args.model, checkpoint[\'epoch\'], best_prec1))\n    else:\n        print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n\ntotal = 0\nfor m in model.modules():\n    if isinstance(m, nn.BatchNorm2d):\n        total += m.weight.data.shape[0]\n\nbn = torch.zeros(total)\nindex = 0\nfor m in model.modules():\n    if isinstance(m, nn.BatchNorm2d):\n        size = m.weight.data.shape[0]\n        bn[index:(index+size)] = m.weight.data.abs().clone()\n        index += size\n\ny, i = torch.sort(bn)\nthre_index = int(total * args.percent)\nthre = y[thre_index]\n\npruned = 0\ncfg = []\ncfg_mask = []\nfor k, m in enumerate(model.modules()):\n    if isinstance(m, nn.BatchNorm2d):\n        weight_copy = m.weight.data.abs().clone()\n        mask = weight_copy.gt(thre).float().cuda()\n        pruned = pruned + mask.shape[0] - torch.sum(mask)\n        m.weight.data.mul_(mask)\n        m.bias.data.mul_(mask)\n        cfg.append(int(torch.sum(mask)))\n        cfg_mask.append(mask.clone())\n        print(\'layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}\'.\n            format(k, mask.shape[0], int(torch.sum(mask))))\n    elif isinstance(m, nn.MaxPool2d):\n        cfg.append(\'M\')\n\npruned_ratio = pruned/total\n\nprint(\'Pre-processing Successful!\')\n\n# simple test model after Pre-processing prune (simple set BN scales to zeros)\ndef test(model):\n    kwargs = {\'num_workers\': 1, \'pin_memory\': True} if args.cuda else {}\n    if args.dataset == \'cifar10\':\n        test_loader = torch.utils.data.DataLoader(\n            datasets.CIFAR10(\'./data.cifar10\', train=False, transform=transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),\n            batch_size=args.test_batch_size, shuffle=False, **kwargs)\n    elif args.dataset == \'cifar100\':\n        test_loader = torch.utils.data.DataLoader(\n            datasets.CIFAR100(\'./data.cifar100\', train=False, transform=transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),\n            batch_size=args.test_batch_size, shuffle=False, **kwargs)\n    else:\n        raise ValueError(""No valid dataset is given."")\n    model.eval()\n    correct = 0\n    for data, target in test_loader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    print(\'\\nTest set: Accuracy: {}/{} ({:.1f}%)\\n\'.format(\n        correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n    return correct / float(len(test_loader.dataset))\n\nacc = test(model)\n\nprint(""Cfg:"")\nprint(cfg)\n\nnewmodel = densenet(depth=args.depth, dataset=args.dataset, cfg=cfg)\n\nif args.cuda:\n    newmodel.cuda()\n\nnum_parameters = sum([param.nelement() for param in newmodel.parameters()])\nsavepath = os.path.join(args.save, ""prune.txt"")\nwith open(savepath, ""w"") as fp:\n    fp.write(""Configuration: \\n""+str(cfg)+""\\n"")\n    fp.write(""Number of parameters: \\n""+str(num_parameters)+""\\n"")\n    fp.write(""Test accuracy: \\n""+str(acc))\n\nold_modules = list(model.modules())\nnew_modules = list(newmodel.modules())\n\nlayer_id_in_cfg = 0\nstart_mask = torch.ones(3)\nend_mask = cfg_mask[layer_id_in_cfg]\nfirst_conv = True\n\nfor layer_id in range(len(old_modules)):\n    m0 = old_modules[layer_id]\n    m1 = new_modules[layer_id]\n    if isinstance(m0, nn.BatchNorm2d):\n        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n        if idx1.size == 1:\n            idx1 = np.resize(idx1,(1,))\n\n        if isinstance(old_modules[layer_id + 1], channel_selection):\n            # If the next layer is the channel selection layer, then the current batch normalization layer won\'t be pruned.\n            m1.weight.data = m0.weight.data.clone()\n            m1.bias.data = m0.bias.data.clone()\n            m1.running_mean = m0.running_mean.clone()\n            m1.running_var = m0.running_var.clone()\n\n            # We need to set the mask parameter `indexes` for the channel selection layer.\n            m2 = new_modules[layer_id + 1]\n            m2.indexes.data.zero_()\n            m2.indexes.data[idx1.tolist()] = 1.0\n\n            layer_id_in_cfg += 1\n            start_mask = end_mask.clone()\n            if layer_id_in_cfg < len(cfg_mask):\n                end_mask = cfg_mask[layer_id_in_cfg]\n            continue\n\n    elif isinstance(m0, nn.Conv2d):\n        if first_conv:\n            # We don\'t change the first convolution layer.\n            m1.weight.data = m0.weight.data.clone()\n            first_conv = False\n            continue\n        if isinstance(old_modules[layer_id - 1], channel_selection):\n            idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n            idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n            print(\'In shape: {:d}, Out shape {:d}.\'.format(idx0.size, idx1.size))\n            if idx0.size == 1:\n                idx0 = np.resize(idx0, (1,))\n            if idx1.size == 1:\n                idx1 = np.resize(idx1, (1,))\n\n            # If the last layer is channel selection layer, then we don\'t change the number of output channels of the current\n            # convolutional layer.\n            w1 = m0.weight.data[:, idx0.tolist(), :, :].clone()\n            m1.weight.data = w1.clone()\n            continue\n\n    elif isinstance(m0, nn.Linear):\n        idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n        if idx0.size == 1:\n            idx0 = np.resize(idx0, (1,))\n\n        m1.weight.data = m0.weight.data[:, idx0].clone()\n        m1.bias.data = m0.bias.data.clone()\n\ntorch.save({\'cfg\': cfg, \'state_dict\': newmodel.state_dict()}, os.path.join(args.save, \'pruned.pth.tar\'))\n\nprint(newmodel)\nmodel = newmodel\ntest(model)'"
main.py,15,"b'from __future__ import print_function\nimport os\nimport argparse\nimport shutil\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\nimport models\n\n\n# Training settings\nparser = argparse.ArgumentParser(description=\'PyTorch Slimming CIFAR training\')\nparser.add_argument(\'--dataset\', type=str, default=\'cifar100\',\n                    help=\'training dataset (default: cifar100)\')\nparser.add_argument(\'--sparsity-regularization\', \'-sr\', dest=\'sr\', action=\'store_true\',\n                    help=\'train with channel sparsity regularization\')\nparser.add_argument(\'--s\', type=float, default=0.0001,\n                    help=\'scale sparse rate (default: 0.0001)\')\nparser.add_argument(\'--refine\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to the pruned model to be fine tuned\')\nparser.add_argument(\'--batch-size\', type=int, default=64, metavar=\'N\',\n                    help=\'input batch size for training (default: 64)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=256, metavar=\'N\',\n                    help=\'input batch size for testing (default: 256)\')\nparser.add_argument(\'--epochs\', type=int, default=160, metavar=\'N\',\n                    help=\'number of epochs to train (default: 160)\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--lr\', type=float, default=0.1, metavar=\'LR\',\n                    help=\'learning rate (default: 0.1)\')\nparser.add_argument(\'--momentum\', type=float, default=0.9, metavar=\'M\',\n                    help=\'SGD momentum (default: 0.9)\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'disables CUDA training\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'--log-interval\', type=int, default=100, metavar=\'N\',\n                    help=\'how many batches to wait before logging training status\')\nparser.add_argument(\'--save\', default=\'./logs\', type=str, metavar=\'PATH\',\n                    help=\'path to save prune model (default: current directory)\')\nparser.add_argument(\'--arch\', default=\'vgg\', type=str, \n                    help=\'architecture to use\')\nparser.add_argument(\'--depth\', default=19, type=int,\n                    help=\'depth of the neural network\')\n\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\nif not os.path.exists(args.save):\n    os.makedirs(args.save)\n\nkwargs = {\'num_workers\': 1, \'pin_memory\': True} if args.cuda else {}\nif args.dataset == \'cifar10\':\n    train_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(\'./data.cifar10\', train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.Pad(4),\n                           transforms.RandomCrop(32),\n                           transforms.RandomHorizontalFlip(),\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n                       ])),\n        batch_size=args.batch_size, shuffle=True, **kwargs)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(\'./data.cifar10\', train=False, transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n                       ])),\n        batch_size=args.test_batch_size, shuffle=True, **kwargs)\nelse:\n    train_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR100(\'./data.cifar100\', train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.Pad(4),\n                           transforms.RandomCrop(32),\n                           transforms.RandomHorizontalFlip(),\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n                       ])),\n        batch_size=args.batch_size, shuffle=True, **kwargs)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR100(\'./data.cifar100\', train=False, transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n                       ])),\n        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n\nif args.refine:\n    checkpoint = torch.load(args.refine)\n    model = models.__dict__[args.arch](dataset=args.dataset, depth=args.depth, cfg=checkpoint[\'cfg\'])\n    model.load_state_dict(checkpoint[\'state_dict\'])\nelse:\n    model = models.__dict__[args.arch](dataset=args.dataset, depth=args.depth)\n\nif args.cuda:\n    model.cuda()\n\noptimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n\nif args.resume:\n    if os.path.isfile(args.resume):\n        print(""=> loading checkpoint \'{}\'"".format(args.resume))\n        checkpoint = torch.load(args.resume)\n        args.start_epoch = checkpoint[\'epoch\']\n        best_prec1 = checkpoint[\'best_prec1\']\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        print(""=> loaded checkpoint \'{}\' (epoch {}) Prec1: {:f}""\n              .format(args.resume, checkpoint[\'epoch\'], best_prec1))\n    else:\n        print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n\n# additional subgradient descent on the sparsity-induced penalty term\ndef updateBN():\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            m.weight.grad.data.add_(args.s*torch.sign(m.weight.data))  # L1\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.cross_entropy(output, target)\n        pred = output.data.max(1, keepdim=True)[1]\n        loss.backward()\n        if args.sr:\n            updateBN()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print(\'Train Epoch: {} [{}/{} ({:.1f}%)]\\tLoss: {:.6f}\'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.data[0]))\n\ndef test():\n    model.eval()\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        test_loss += F.cross_entropy(output, target, size_average=False).data[0] # sum up batch loss\n        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    test_loss /= len(test_loader.dataset)\n    print(\'\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n\'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n    return correct / float(len(test_loader.dataset))\n\ndef save_checkpoint(state, is_best, filepath):\n    torch.save(state, os.path.join(filepath, \'checkpoint.pth.tar\'))\n    if is_best:\n        shutil.copyfile(os.path.join(filepath, \'checkpoint.pth.tar\'), os.path.join(filepath, \'model_best.pth.tar\'))\n\nbest_prec1 = 0.\nfor epoch in range(args.start_epoch, args.epochs):\n    if epoch in [args.epochs*0.5, args.epochs*0.75]:\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] *= 0.1\n    train(epoch)\n    prec1 = test()\n    is_best = prec1 > best_prec1\n    best_prec1 = max(prec1, best_prec1)\n    save_checkpoint({\n        \'epoch\': epoch + 1,\n        \'state_dict\': model.state_dict(),\n        \'best_prec1\': best_prec1,\n        \'optimizer\': optimizer.state_dict(),\n    }, is_best, filepath=args.save)\n\nprint(""Best accuracy: ""+str(best_prec1))'"
resprune.py,13,"b'import os\nimport argparse\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\nfrom models import *\n\n\n# Prune settings\nparser = argparse.ArgumentParser(description=\'PyTorch Slimming CIFAR prune\')\nparser.add_argument(\'--dataset\', type=str, default=\'cifar100\',\n                    help=\'training dataset (default: cifar10)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=256, metavar=\'N\',\n                    help=\'input batch size for testing (default: 256)\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'disables CUDA training\')\nparser.add_argument(\'--depth\', type=int, default=164,\n                    help=\'depth of the resnet\')\nparser.add_argument(\'--percent\', type=float, default=0.5,\n                    help=\'scale sparse rate (default: 0.5)\')\nparser.add_argument(\'--model\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to the model (default: none)\')\nparser.add_argument(\'--save\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to save pruned model (default: none)\')\n\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nif not os.path.exists(args.save):\n    os.makedirs(args.save)\n\nmodel = resnet(depth=args.depth, dataset=args.dataset)\n\nif args.cuda:\n    model.cuda()\nif args.model:\n    if os.path.isfile(args.model):\n        print(""=> loading checkpoint \'{}\'"".format(args.model))\n        checkpoint = torch.load(args.model)\n        args.start_epoch = checkpoint[\'epoch\']\n        best_prec1 = checkpoint[\'best_prec1\']\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        print(""=> loaded checkpoint \'{}\' (epoch {}) Prec1: {:f}""\n              .format(args.model, checkpoint[\'epoch\'], best_prec1))\n    else:\n        print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n\ntotal = 0\n\nfor m in model.modules():\n    if isinstance(m, nn.BatchNorm2d):\n        total += m.weight.data.shape[0]\n\nbn = torch.zeros(total)\nindex = 0\nfor m in model.modules():\n    if isinstance(m, nn.BatchNorm2d):\n        size = m.weight.data.shape[0]\n        bn[index:(index+size)] = m.weight.data.abs().clone()\n        index += size\n\ny, i = torch.sort(bn)\nthre_index = int(total * args.percent)\nthre = y[thre_index]\n\n\npruned = 0\ncfg = []\ncfg_mask = []\nfor k, m in enumerate(model.modules()):\n    if isinstance(m, nn.BatchNorm2d):\n        weight_copy = m.weight.data.abs().clone()\n        mask = weight_copy.gt(thre).float().cuda()\n        pruned = pruned + mask.shape[0] - torch.sum(mask)\n        m.weight.data.mul_(mask)\n        m.bias.data.mul_(mask)\n        cfg.append(int(torch.sum(mask)))\n        cfg_mask.append(mask.clone())\n        print(\'layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}\'.\n            format(k, mask.shape[0], int(torch.sum(mask))))\n    elif isinstance(m, nn.MaxPool2d):\n        cfg.append(\'M\')\n\npruned_ratio = pruned/total\n\nprint(\'Pre-processing Successful!\')\n\n# simple test model after Pre-processing prune (simple set BN scales to zeros)\ndef test(model):\n    kwargs = {\'num_workers\': 1, \'pin_memory\': True} if args.cuda else {}\n    if args.dataset == \'cifar10\':\n        test_loader = torch.utils.data.DataLoader(\n            datasets.CIFAR10(\'./data.cifar10\', train=False, transform=transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),\n            batch_size=args.test_batch_size, shuffle=False, **kwargs)\n    elif args.dataset == \'cifar100\':\n        test_loader = torch.utils.data.DataLoader(\n            datasets.CIFAR100(\'./data.cifar100\', train=False, transform=transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),\n            batch_size=args.test_batch_size, shuffle=False, **kwargs)\n    else:\n        raise ValueError(""No valid dataset is given."")\n    model.eval()\n    correct = 0\n    for data, target in test_loader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    print(\'\\nTest set: Accuracy: {}/{} ({:.1f}%)\\n\'.format(\n        correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n    return correct / float(len(test_loader.dataset))\n\nacc = test(model)\n\nprint(""Cfg:"")\nprint(cfg)\n\nnewmodel = resnet(depth=args.depth, dataset=args.dataset, cfg=cfg)\nif args.cuda:\n    newmodel.cuda()\n\nnum_parameters = sum([param.nelement() for param in newmodel.parameters()])\nsavepath = os.path.join(args.save, ""prune.txt"")\nwith open(savepath, ""w"") as fp:\n    fp.write(""Configuration: \\n""+str(cfg)+""\\n"")\n    fp.write(""Number of parameters: \\n""+str(num_parameters)+""\\n"")\n    fp.write(""Test accuracy: \\n""+str(acc))\n\nold_modules = list(model.modules())\nnew_modules = list(newmodel.modules())\nlayer_id_in_cfg = 0\nstart_mask = torch.ones(3)\nend_mask = cfg_mask[layer_id_in_cfg]\nconv_count = 0\n\nfor layer_id in range(len(old_modules)):\n    m0 = old_modules[layer_id]\n    m1 = new_modules[layer_id]\n    if isinstance(m0, nn.BatchNorm2d):\n        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n        if idx1.size == 1:\n            idx1 = np.resize(idx1,(1,))\n\n        if isinstance(old_modules[layer_id + 1], channel_selection):\n            # If the next layer is the channel selection layer, then the current batchnorm 2d layer won\'t be pruned.\n            m1.weight.data = m0.weight.data.clone()\n            m1.bias.data = m0.bias.data.clone()\n            m1.running_mean = m0.running_mean.clone()\n            m1.running_var = m0.running_var.clone()\n\n            # We need to set the channel selection layer.\n            m2 = new_modules[layer_id + 1]\n            m2.indexes.data.zero_()\n            m2.indexes.data[idx1.tolist()] = 1.0\n\n            layer_id_in_cfg += 1\n            start_mask = end_mask.clone()\n            if layer_id_in_cfg < len(cfg_mask):\n                end_mask = cfg_mask[layer_id_in_cfg]\n        else:\n            m1.weight.data = m0.weight.data[idx1.tolist()].clone()\n            m1.bias.data = m0.bias.data[idx1.tolist()].clone()\n            m1.running_mean = m0.running_mean[idx1.tolist()].clone()\n            m1.running_var = m0.running_var[idx1.tolist()].clone()\n            layer_id_in_cfg += 1\n            start_mask = end_mask.clone()\n            if layer_id_in_cfg < len(cfg_mask):  # do not change in Final FC\n                end_mask = cfg_mask[layer_id_in_cfg]\n    elif isinstance(m0, nn.Conv2d):\n        if conv_count == 0:\n            m1.weight.data = m0.weight.data.clone()\n            conv_count += 1\n            continue\n        if isinstance(old_modules[layer_id-1], channel_selection) or isinstance(old_modules[layer_id-1], nn.BatchNorm2d):\n            # This convers the convolutions in the residual block.\n            # The convolutions are either after the channel selection layer or after the batch normalization layer.\n            conv_count += 1\n            idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n            idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n            print(\'In shape: {:d}, Out shape {:d}.\'.format(idx0.size, idx1.size))\n            if idx0.size == 1:\n                idx0 = np.resize(idx0, (1,))\n            if idx1.size == 1:\n                idx1 = np.resize(idx1, (1,))\n            w1 = m0.weight.data[:, idx0.tolist(), :, :].clone()\n\n            # If the current convolution is not the last convolution in the residual block, then we can change the \n            # number of output channels. Currently we use `conv_count` to detect whether it is such convolution.\n            if conv_count % 3 != 1:\n                w1 = w1[idx1.tolist(), :, :, :].clone()\n            m1.weight.data = w1.clone()\n            continue\n\n        # We need to consider the case where there are downsampling convolutions. \n        # For these convolutions, we just copy the weights.\n        m1.weight.data = m0.weight.data.clone()\n    elif isinstance(m0, nn.Linear):\n        idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n        if idx0.size == 1:\n            idx0 = np.resize(idx0, (1,))\n\n        m1.weight.data = m0.weight.data[:, idx0].clone()\n        m1.bias.data = m0.bias.data.clone()\n\ntorch.save({\'cfg\': cfg, \'state_dict\': newmodel.state_dict()}, os.path.join(args.save, \'pruned.pth.tar\'))\n\nprint(newmodel)\nmodel = newmodel\ntest(model)'"
vggprune.py,13,"b'import os\nimport argparse\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\nfrom models import *\n\n\n# Prune settings\nparser = argparse.ArgumentParser(description=\'PyTorch Slimming CIFAR prune\')\nparser.add_argument(\'--dataset\', type=str, default=\'cifar100\',\n                    help=\'training dataset (default: cifar10)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=256, metavar=\'N\',\n                    help=\'input batch size for testing (default: 256)\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'disables CUDA training\')\nparser.add_argument(\'--depth\', type=int, default=19,\n                    help=\'depth of the vgg\')\nparser.add_argument(\'--percent\', type=float, default=0.5,\n                    help=\'scale sparse rate (default: 0.5)\')\nparser.add_argument(\'--model\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to the model (default: none)\')\nparser.add_argument(\'--save\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to save pruned model (default: none)\')\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nif not os.path.exists(args.save):\n    os.makedirs(args.save)\n\nmodel = vgg(dataset=args.dataset, depth=args.depth)\nif args.cuda:\n    model.cuda()\n\nif args.model:\n    if os.path.isfile(args.model):\n        print(""=> loading checkpoint \'{}\'"".format(args.model))\n        checkpoint = torch.load(args.model)\n        args.start_epoch = checkpoint[\'epoch\']\n        best_prec1 = checkpoint[\'best_prec1\']\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        print(""=> loaded checkpoint \'{}\' (epoch {}) Prec1: {:f}""\n              .format(args.model, checkpoint[\'epoch\'], best_prec1))\n    else:\n        print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n\nprint(model)\ntotal = 0\nfor m in model.modules():\n    if isinstance(m, nn.BatchNorm2d):\n        total += m.weight.data.shape[0]\n\nbn = torch.zeros(total)\nindex = 0\nfor m in model.modules():\n    if isinstance(m, nn.BatchNorm2d):\n        size = m.weight.data.shape[0]\n        bn[index:(index+size)] = m.weight.data.abs().clone()\n        index += size\n\ny, i = torch.sort(bn)\nthre_index = int(total * args.percent)\nthre = y[thre_index]\n\npruned = 0\ncfg = []\ncfg_mask = []\nfor k, m in enumerate(model.modules()):\n    if isinstance(m, nn.BatchNorm2d):\n        weight_copy = m.weight.data.abs().clone()\n        mask = weight_copy.gt(thre).float().cuda()\n        pruned = pruned + mask.shape[0] - torch.sum(mask)\n        m.weight.data.mul_(mask)\n        m.bias.data.mul_(mask)\n        cfg.append(int(torch.sum(mask)))\n        cfg_mask.append(mask.clone())\n        print(\'layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}\'.\n            format(k, mask.shape[0], int(torch.sum(mask))))\n    elif isinstance(m, nn.MaxPool2d):\n        cfg.append(\'M\')\n\npruned_ratio = pruned/total\n\nprint(\'Pre-processing Successful!\')\n\n# simple test model after Pre-processing prune (simple set BN scales to zeros)\ndef test(model):\n    kwargs = {\'num_workers\': 1, \'pin_memory\': True} if args.cuda else {}\n    if args.dataset == \'cifar10\':\n        test_loader = torch.utils.data.DataLoader(\n            datasets.CIFAR10(\'./data.cifar10\', train=False, transform=transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),\n            batch_size=args.test_batch_size, shuffle=True, **kwargs)\n    elif args.dataset == \'cifar100\':\n        test_loader = torch.utils.data.DataLoader(\n            datasets.CIFAR100(\'./data.cifar100\', train=False, transform=transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),\n            batch_size=args.test_batch_size, shuffle=True, **kwargs)\n    else:\n        raise ValueError(""No valid dataset is given."")\n    model.eval()\n    correct = 0\n    for data, target in test_loader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    print(\'\\nTest set: Accuracy: {}/{} ({:.1f}%)\\n\'.format(\n        correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n    return correct / float(len(test_loader.dataset))\n\nacc = test(model)\n\n# Make real prune\nprint(cfg)\nnewmodel = vgg(dataset=args.dataset, cfg=cfg)\nif args.cuda:\n    newmodel.cuda()\n\nnum_parameters = sum([param.nelement() for param in newmodel.parameters()])\nsavepath = os.path.join(args.save, ""prune.txt"")\nwith open(savepath, ""w"") as fp:\n    fp.write(""Configuration: \\n""+str(cfg)+""\\n"")\n    fp.write(""Number of parameters: \\n""+str(num_parameters)+""\\n"")\n    fp.write(""Test accuracy: \\n""+str(acc))\n\nlayer_id_in_cfg = 0\nstart_mask = torch.ones(3)\nend_mask = cfg_mask[layer_id_in_cfg]\nfor [m0, m1] in zip(model.modules(), newmodel.modules()):\n    if isinstance(m0, nn.BatchNorm2d):\n        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n        if idx1.size == 1:\n            idx1 = np.resize(idx1,(1,))\n        m1.weight.data = m0.weight.data[idx1.tolist()].clone()\n        m1.bias.data = m0.bias.data[idx1.tolist()].clone()\n        m1.running_mean = m0.running_mean[idx1.tolist()].clone()\n        m1.running_var = m0.running_var[idx1.tolist()].clone()\n        layer_id_in_cfg += 1\n        start_mask = end_mask.clone()\n        if layer_id_in_cfg < len(cfg_mask):  # do not change in Final FC\n            end_mask = cfg_mask[layer_id_in_cfg]\n    elif isinstance(m0, nn.Conv2d):\n        idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n        print(\'In shape: {:d}, Out shape {:d}.\'.format(idx0.size, idx1.size))\n        if idx0.size == 1:\n            idx0 = np.resize(idx0, (1,))\n        if idx1.size == 1:\n            idx1 = np.resize(idx1, (1,))\n        w1 = m0.weight.data[:, idx0.tolist(), :, :].clone()\n        w1 = w1[idx1.tolist(), :, :, :].clone()\n        m1.weight.data = w1.clone()\n    elif isinstance(m0, nn.Linear):\n        idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n        if idx0.size == 1:\n            idx0 = np.resize(idx0, (1,))\n        m1.weight.data = m0.weight.data[:, idx0].clone()\n        m1.bias.data = m0.bias.data.clone()\n\ntorch.save({\'cfg\': cfg, \'state_dict\': newmodel.state_dict()}, os.path.join(args.save, \'pruned.pth.tar\'))\n\nprint(newmodel)\nmodel = newmodel\ntest(model)'"
mask-impl/main_mask.py,15,"b'from __future__ import print_function\nimport argparse\nimport math\nimport os\nimport shutil\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\nimport models\n\n\n# Training settings\nparser = argparse.ArgumentParser(description=\'PyTorch Slimming CIFAR training\')\nparser.add_argument(\'--dataset\', type=str, default=\'cifar100\',\n                    help=\'training dataset (default: cifar100)\')\nparser.add_argument(\'--sparsity-regularization\', \'-sr\', dest=\'sr\', action=\'store_true\',\n                    help=\'train with channel sparsity regularization\')\nparser.add_argument(\'--s\', type=float, default=0.0001,\n                    help=\'scale sparse rate (default: 0.0001)\')\nparser.add_argument(\'--refine\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'refine from prune model\')\nparser.add_argument(\'--batch-size\', type=int, default=64, metavar=\'N\',\n                    help=\'input batch size for training (default: 100)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=100, metavar=\'N\',\n                    help=\'input batch size for testing (default: 1000)\')\nparser.add_argument(\'--epochs\', type=int, default=160, metavar=\'N\',\n                    help=\'number of epochs to train (default: 160)\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--lr\', type=float, default=0.1, metavar=\'LR\',\n                    help=\'learning rate (default: 0.1)\')\nparser.add_argument(\'--momentum\', type=float, default=0.9, metavar=\'M\',\n                    help=\'SGD momentum (default: 0.9)\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'disables CUDA training\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'--log-interval\', type=int, default=100, metavar=\'N\',\n                    help=\'how many batches to wait before logging training status\')\nparser.add_argument(\'--save\', default=\'./logs\', type=str, metavar=\'PATH\',\n                    help=\'path to save prune model (default: current directory)\')\nparser.add_argument(\'--arch\', default=\'vgg\', type=str,\n                    help=\'architecture to use\')\nparser.add_argument(\'--depth\', default=19, type=int,\n                    help=\'depth of the neural network\')\n\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\nif not os.path.exists(args.save):\n    os.makedirs(args.save)\n\nkwargs = {\'num_workers\': 1, \'pin_memory\': True} if args.cuda else {}\nif args.dataset == \'cifar10\':\n    train_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(\'./data.cifar10\', train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.Pad(4),\n                           transforms.RandomCrop(32),\n                           transforms.RandomHorizontalFlip(),\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n                       ])),\n        batch_size=args.batch_size, shuffle=True, **kwargs)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(\'./data.cifar10\', train=False, transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n                       ])),\n        batch_size=args.test_batch_size, shuffle=True, **kwargs)\nelse:\n    train_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR100(\'./data.cifar100\', train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.Pad(4),\n                           transforms.RandomCrop(32),\n                           transforms.RandomHorizontalFlip(),\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n                       ])),\n        batch_size=args.batch_size, shuffle=True, **kwargs)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR100(\'./data.cifar100\', train=False, transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n                       ])),\n        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n\nmodel = models.__dict__[args.arch](dataset=args.dataset, depth=args.depth)\n\nif args.refine:\n    checkpoint = torch.load(args.refine)\n    model.load_state_dict(checkpoint[\'state_dict\'])\n\nif args.cuda:\n    model.cuda()\n\noptimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n\nif args.resume:\n    if os.path.isfile(args.resume):\n        print(""=> loading checkpoint \'{}\'"".format(args.resume))\n        checkpoint = torch.load(args.resume)\n        args.start_epoch = checkpoint[\'epoch\']\n        best_prec1 = checkpoint[\'best_prec1\']\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        print(""=> loaded checkpoint \'{}\' (epoch {}) Prec1: {:f}""\n              .format(args.resume, checkpoint[\'epoch\'], best_prec1))\n    else:\n        print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n\n# additional subgradient descent on the sparsity-induced penalty term\ndef updateBN():\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            m.weight.grad.data.add_(args.s*torch.sign(m.weight.data))  # L1\n\ndef BN_grad_zero():\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            mask = (m.weight.data != 0)\n            mask = mask.float().cuda()\n            m.weight.grad.data.mul_(mask)\n            m.bias.grad.data.mul_(mask)\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.cross_entropy(output, target)\n        pred = output.data.max(1, keepdim=True)[1]\n        loss.backward()\n        if args.sr:\n            updateBN()\n        BN_grad_zero()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print(\'Train Epoch: {} [{}/{} ({:.1f}%)]\\tLoss: {:.6f}\'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.data[0]))\n\ndef test():\n    model.eval()\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        test_loss += F.cross_entropy(output, target, size_average=False).data[0] # sum up batch loss\n        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    test_loss /= len(test_loader.dataset)\n    print(\'\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n\'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n    return correct / float(len(test_loader.dataset))\n\n\ndef save_checkpoint(state, is_best, filepath):\n    torch.save(state, os.path.join(filepath, \'checkpoint.pth.tar\'))\n    if is_best:\n        shutil.copyfile(os.path.join(filepath, \'checkpoint.pth.tar\'), os.path.join(filepath, \'model_best.pth.tar\'))\n\nbest_prec1 = 0.\nfor epoch in range(args.start_epoch, args.epochs):\n    if epoch in [args.epochs*0.5, args.epochs*0.75]:\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] *= 0.1\n    train(epoch)\n    prec1 = test()\n    is_best = prec1 > best_prec1\n    best_prec1 = max(prec1, best_prec1)\n    save_checkpoint({\n        \'epoch\': epoch + 1,\n        \'state_dict\': model.state_dict(),\n        \'best_prec1\': best_prec1,\n        \'optimizer\': optimizer.state_dict(),\n    }, is_best, filepath=args.save)\n\nprint(""Best accuracy: ""+str(best_prec1))'"
mask-impl/prune_mask.py,12,"b'import os\nimport argparse\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\nimport models\n\n\n# Prune settings\nparser = argparse.ArgumentParser(description=\'PyTorch Slimming CIFAR prune\')\nparser.add_argument(\'--dataset\', type=str, default=\'cifar100\',\n                    help=\'training dataset (default: cifar100)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=100, metavar=\'N\',\n                    help=\'input batch size for testing (default: 100)\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'disables CUDA training\')\nparser.add_argument(\'--percent\', type=float, default=0.5,\n                    help=\'scale sparse rate (default: 0.5)\')\nparser.add_argument(\'--model\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to raw trained model (default: none)\')\nparser.add_argument(\'--save\', default=\'.\', type=str, metavar=\'PATH\',\n                    help=\'path to save prune model (default: none)\')\nparser.add_argument(\'--depth\', default=19, type=int,\n                    help=\'depth of resnet and densenet\')\nparser.add_argument(\'--arch\', default=\'vgg\', type=str,\n                    help=\'architecture to use\')\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nif not os.path.exists(args.save):\n    os.makedirs(args.save)\n\nmodel = models.__dict__[args.arch](dataset=args.dataset, depth=args.depth)\n\nif args.cuda:\n    model.cuda()\nif args.model:\n    if os.path.isfile(args.model):\n        print(""=> loading checkpoint \'{}\'"".format(args.model))\n        checkpoint = torch.load(args.model)\n        args.start_epoch = checkpoint[\'epoch\']\n        best_prec1 = checkpoint[\'best_prec1\']\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        print(""=> loaded checkpoint \'{}\' (epoch {}) Prec1: {:f}""\n              .format(args.model, checkpoint[\'epoch\'], best_prec1))\n\nprint(model)\ntotal = 0\nfor m in model.modules():\n    if isinstance(m, nn.BatchNorm2d):\n        total += m.weight.data.shape[0]\n\nbn = torch.zeros(total)\nindex = 0\nfor m in model.modules():\n    if isinstance(m, nn.BatchNorm2d):\n        size = m.weight.data.shape[0]\n        bn[index:(index+size)] = m.weight.data.abs().clone()\n        index += size\n\ny, i = torch.sort(bn)\nthre_index = int(total * args.percent)\nthre = y[thre_index]\n\npruned = 0\ncfg = []\ncfg_mask = []\nfor k, m in enumerate(model.modules()):\n    if isinstance(m, nn.BatchNorm2d):\n        weight_copy = m.weight.data.abs().clone()\n        mask = weight_copy.gt(thre).float().cuda()\n        pruned = pruned + mask.shape[0] - torch.sum(mask)\n        m.weight.data.mul_(mask)\n        m.bias.data.mul_(mask)\n        cfg.append(int(torch.sum(mask)))\n        cfg_mask.append(mask.clone())\n        print(\'layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}\'.\n            format(k, mask.shape[0], int(torch.sum(mask))))\n    elif isinstance(m, nn.MaxPool2d):\n        cfg.append(\'M\')\n\ntorch.save({\'cfg\': cfg, \'state_dict\': model.state_dict()}, os.path.join(args.save, \'pruned.pth.tar\'))\n\npruned_ratio = pruned/total\n\nprint(\'Pre-processing Successful!\')\n\n\n# simple test model after Pre-processing prune (simple set BN scales to zeros)\ndef test():\n    kwargs = {\'num_workers\': 1, \'pin_memory\': True} if args.cuda else {}\n    if args.dataset == \'cifar10\':\n        test_loader = torch.utils.data.DataLoader(\n            datasets.CIFAR10(\'./data.cifar10\', train=False, transform=transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),\n            batch_size=args.test_batch_size, shuffle=True, **kwargs)\n    elif args.dataset == \'cifar100\':\n        test_loader = torch.utils.data.DataLoader(\n            datasets.CIFAR100(\'./data.cifar100\', train=False, transform=transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),\n            batch_size=args.test_batch_size, shuffle=True, **kwargs)\n    else:\n        raise ValueError(""No valid dataset is given."")\n    model.eval()\n    correct = 0\n    for data, target in test_loader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    print(\'\\nTest set: Accuracy: {}/{} ({:.1f}%)\\n\'.format(\n        correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n    return correct / float(len(test_loader.dataset))\n\nacc = test()\nprint(cfg)\n\nsavepath = os.path.join(args.save, ""prune.txt"")\nwith open(savepath, ""w"") as fp:\n    fp.write(""Configuration: \\n"")\n    fp.write(str(cfg)+""\\n"")\n    fp.write(""Test accuracy: \\n"")\n    fp.write(str(acc))\n'"
models/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .vgg import *\nfrom .preresnet import *\nfrom .densenet import *\nfrom .channel_selection import *'
models/channel_selection.py,2,"b'import numpy as np\nimport torch\nimport torch.nn as nn\n\n\nclass channel_selection(nn.Module):\n    """"""\n    Select channels from the output of BatchNorm2d layer. It should be put directly after BatchNorm2d layer.\n    The output shape of this layer is determined by the number of 1 in `self.indexes`.\n    """"""\n    def __init__(self, num_channels):\n        """"""\n        Initialize the `indexes` with all one vector with the length same as the number of channels.\n        During pruning, the places in `indexes` which correpond to the channels to be pruned will be set to 0.\n        """"""\n        super(channel_selection, self).__init__()\n        self.indexes = nn.Parameter(torch.ones(num_channels))\n\n    def forward(self, input_tensor):\n        """"""\n        Parameter\n        ---------\n        input_tensor: (N,C,H,W). It should be the output of BatchNorm2d layer.\n        """"""\n        selected_index = np.squeeze(np.argwhere(self.indexes.data.cpu().numpy()))\n        if selected_index.size == 1:\n            selected_index = np.resize(selected_index, (1,)) \n        output = input_tensor[:, selected_index, :, :]\n        return output'"
models/densenet.py,4,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom .channel_selection import channel_selection\n\n\n__all__ = [\'densenet\']\n\n""""""\ndensenet with basic block.\n""""""\n\nclass BasicBlock(nn.Module):\n    def __init__(self, inplanes, cfg, expansion=1, growthRate=12, dropRate=0):\n        super(BasicBlock, self).__init__()\n        planes = expansion * growthRate\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.select = channel_selection(inplanes)\n        self.conv1 = nn.Conv2d(cfg, growthRate, kernel_size=3, \n                               padding=1, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropRate = dropRate\n\n    def forward(self, x):\n        out = self.bn1(x)\n        out = self.select(out)\n        out = self.relu(out)\n        out = self.conv1(out)\n        if self.dropRate > 0:\n            out = F.dropout(out, p=self.dropRate, training=self.training)\n\n        out = torch.cat((x, out), 1)\n\n        return out\n\nclass Transition(nn.Module):\n    def __init__(self, inplanes, outplanes, cfg):\n        super(Transition, self).__init__()\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.select = channel_selection(inplanes)\n        self.conv1 = nn.Conv2d(cfg, outplanes, kernel_size=1,\n                               bias=False)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        out = self.bn1(x)\n        out = self.select(out)\n        out = self.relu(out)\n        out = self.conv1(out)\n        out = F.avg_pool2d(out, 2)\n        return out\n\nclass densenet(nn.Module):\n\n    def __init__(self, depth=40, \n        dropRate=0, dataset=\'cifar10\', growthRate=12, compressionRate=1, cfg = None):\n        super(densenet, self).__init__()\n\n        assert (depth - 4) % 3 == 0, \'depth should be 3n+4\'\n        n = (depth - 4) // 3\n        block = BasicBlock\n\n        self.growthRate = growthRate\n        self.dropRate = dropRate\n\n        if cfg == None:\n            cfg = []\n            start = growthRate*2\n            for _ in range(3):\n                cfg.append([start + growthRate*i for i in range(n+1)])\n                start += growthRate*n\n            cfg = [item for sub_list in cfg for item in sub_list]\n\n        assert len(cfg) == 3*n+3, \'length of config variable cfg should be 3n+3\'\n\n        # self.inplanes is a global variable used across multiple\n        # helper functions\n        self.inplanes = growthRate * 2\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, padding=1,\n                               bias=False)\n        self.dense1 = self._make_denseblock(block, n, cfg[0:n])\n        self.trans1 = self._make_transition(compressionRate, cfg[n])\n        self.dense2 = self._make_denseblock(block, n, cfg[n+1:2*n+1])\n        self.trans2 = self._make_transition(compressionRate, cfg[2*n+1])\n        self.dense3 = self._make_denseblock(block, n, cfg[2*n+2:3*n+2])\n        self.bn = nn.BatchNorm2d(self.inplanes)\n        self.select = channel_selection(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(8)\n\n        if dataset == \'cifar10\':\n            self.fc = nn.Linear(cfg[-1], 10)\n        elif dataset == \'cifar100\':\n            self.fc = nn.Linear(cfg[-1], 100)\n\n        # Weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(0.5)\n                m.bias.data.zero_()\n\n    def _make_denseblock(self, block, blocks, cfg):\n        layers = []\n        assert blocks == len(cfg), \'Length of the cfg parameter is not right.\'\n        for i in range(blocks):\n            # Currently we fix the expansion ratio as the default value\n            layers.append(block(self.inplanes, cfg = cfg[i], growthRate=self.growthRate, dropRate=self.dropRate))\n            self.inplanes += self.growthRate\n\n        return nn.Sequential(*layers)\n\n    def _make_transition(self, compressionRate, cfg):\n        # cfg is a number in this case.\n        inplanes = self.inplanes\n        outplanes = int(math.floor(self.inplanes // compressionRate))\n        self.inplanes = outplanes\n        return Transition(inplanes, outplanes, cfg)\n\n    def forward(self, x):\n        x = self.conv1(x)\n\n        x = self.trans1(self.dense1(x))\n        x = self.trans2(self.dense2(x))\n        x = self.dense3(x)\n        x = self.bn(x)\n        x = self.select(x)\n        x = self.relu(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x'"
models/preresnet.py,1,"b'from __future__ import absolute_import\nimport math\nimport torch.nn as nn\nfrom .channel_selection import channel_selection\n\n\n__all__ = [\'resnet\']\n\n""""""\npreactivation resnet with bottleneck design.\n""""""\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, cfg, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.select = channel_selection(inplanes)\n        self.conv1 = nn.Conv2d(cfg[0], cfg[1], kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(cfg[1])\n        self.conv2 = nn.Conv2d(cfg[1], cfg[2], kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(cfg[2])\n        self.conv3 = nn.Conv2d(cfg[2], planes * 4, kernel_size=1, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.bn1(x)\n        out = self.select(out)\n        out = self.relu(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        out = self.bn3(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n\n        return out\n\nclass resnet(nn.Module):\n    def __init__(self, depth=164, dataset=\'cifar10\', cfg=None):\n        super(resnet, self).__init__()\n        assert (depth - 2) % 9 == 0, \'depth should be 9n+2\'\n\n        n = (depth - 2) // 9\n        block = Bottleneck\n\n        if cfg is None:\n            # Construct config variable.\n            cfg = [[16, 16, 16], [64, 16, 16]*(n-1), [64, 32, 32], [128, 32, 32]*(n-1), [128, 64, 64], [256, 64, 64]*(n-1), [256]]\n            cfg = [item for sub_list in cfg for item in sub_list]\n\n        self.inplanes = 16\n\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1,\n                               bias=False)\n        self.layer1 = self._make_layer(block, 16, n, cfg = cfg[0:3*n])\n        self.layer2 = self._make_layer(block, 32, n, cfg = cfg[3*n:6*n], stride=2)\n        self.layer3 = self._make_layer(block, 64, n, cfg = cfg[6*n:9*n], stride=2)\n        self.bn = nn.BatchNorm2d(64 * block.expansion)\n        self.select = channel_selection(64 * block.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(8)\n\n        if dataset == \'cifar10\':\n            self.fc = nn.Linear(cfg[-1], 10)\n        elif dataset == \'cifar100\':\n            self.fc = nn.Linear(cfg[-1], 100)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(0.5)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, cfg, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, cfg[0:3], stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, cfg[3*i: 3*(i+1)]))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n\n        x = self.layer1(x)  # 32x32\n        x = self.layer2(x)  # 16x16\n        x = self.layer3(x)  # 8x8\n        x = self.bn(x)\n        x = self.select(x)\n        x = self.relu(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x'"
models/vgg.py,3,"b""import math\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n\n__all__ = ['vgg']\n\ndefaultcfg = {\n    11 : [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n    13 : [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n    16 : [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512],\n    19 : [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512],\n}\n\nclass vgg(nn.Module):\n    def __init__(self, dataset='cifar10', depth=19, init_weights=True, cfg=None):\n        super(vgg, self).__init__()\n        if cfg is None:\n            cfg = defaultcfg[depth]\n\n        self.feature = self.make_layers(cfg, True)\n\n        if dataset == 'cifar10':\n            num_classes = 10\n        elif dataset == 'cifar100':\n            num_classes = 100\n        self.classifier = nn.Linear(cfg[-1], num_classes)\n        if init_weights:\n            self._initialize_weights()\n\n    def make_layers(self, cfg, batch_norm=False):\n        layers = []\n        in_channels = 3\n        for v in cfg:\n            if v == 'M':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1, bias=False)\n                if batch_norm:\n                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n                else:\n                    layers += [conv2d, nn.ReLU(inplace=True)]\n                in_channels = v\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.feature(x)\n        x = nn.AvgPool2d(2)(x)\n        x = x.view(x.size(0), -1)\n        y = self.classifier(x)\n        return y\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(0.5)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\nif __name__ == '__main__':\n    net = vgg()\n    x = Variable(torch.FloatTensor(16, 3, 40, 40))\n    y = net(x)\n    print(y.data.shape)"""
mask-impl/models/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .vgg import *\nfrom .preresnet import *\nfrom .densenet import *'
mask-impl/models/densenet.py,4,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\n__all__ = [\'densenet\']\n\n""""""\ndensenet with basic block.\n""""""\n\nclass BasicBlock(nn.Module):\n    def __init__(self, inplanes, cfg, expansion=1, growthRate=12, dropRate=0):\n        super(BasicBlock, self).__init__()\n        planes = expansion * growthRate\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.conv1 = nn.Conv2d(cfg, growthRate, kernel_size=3, \n                               padding=1, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropRate = dropRate\n\n    def forward(self, x):\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n        if self.dropRate > 0:\n            out = F.dropout(out, p=self.dropRate, training=self.training)\n\n        out = torch.cat((x, out), 1)\n\n        return out\n\nclass Transition(nn.Module):\n    def __init__(self, inplanes, outplanes, cfg):\n        super(Transition, self).__init__()\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.conv1 = nn.Conv2d(cfg, outplanes, kernel_size=1,\n                               bias=False)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n        out = F.avg_pool2d(out, 2)\n        return out\n\nclass densenet(nn.Module):\n\n    def __init__(self, depth=40, \n        dropRate=0, dataset=\'cifar10\', growthRate=12, compressionRate=1, cfg = None):\n        super(densenet, self).__init__()\n\n        assert (depth - 4) % 3 == 0, \'depth should be 3n+4\'\n        n = (depth - 4) // 3\n        block = BasicBlock\n\n        self.growthRate = growthRate\n        self.dropRate = dropRate\n\n        if cfg == None:\n            cfg = []\n            start = growthRate*2\n            for i in range(3):\n                cfg.append([start+ growthRate*i for i in range(n+1)])\n                start += growthRate*n\n            cfg = [item for sub_list in cfg for item in sub_list]\n\n        assert len(cfg) == 3*n+3, \'length of config variable cfg should be 3n+3\'\n\n        # self.inplanes is a global variable used across multiple\n        # helper functions\n        self.inplanes = growthRate * 2\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, padding=1,\n                               bias=False)\n        self.dense1 = self._make_denseblock(block, n, cfg[0:n])\n        self.trans1 = self._make_transition(compressionRate, cfg[n])\n        self.dense2 = self._make_denseblock(block, n, cfg[n+1:2*n+1])\n        self.trans2 = self._make_transition(compressionRate, cfg[2*n+1])\n        self.dense3 = self._make_denseblock(block, n, cfg[2*n+2:3*n+2])\n        self.bn = nn.BatchNorm2d(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(8)\n\n        if dataset == \'cifar10\':\n            self.fc = nn.Linear(cfg[-1], 10)\n        elif dataset == \'cifar100\':\n            self.fc = nn.Linear(cfg[-1], 100)\n\n        # Weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(0.5)\n                m.bias.data.zero_()\n\n    def _make_denseblock(self, block, blocks, cfg):\n        layers = []\n        assert blocks == len(cfg), \'Length of the cfg parameter is not right.\'\n        for i in range(blocks):\n            # Currently we fix the expansion ratio as the default value\n            layers.append(block(self.inplanes, cfg = cfg[i], growthRate=self.growthRate, dropRate=self.dropRate))\n            self.inplanes += self.growthRate\n\n        return nn.Sequential(*layers)\n\n    def _make_transition(self, compressionRate, cfg):\n        # cfg is a number in this case.\n        inplanes = self.inplanes\n        outplanes = int(math.floor(self.inplanes // compressionRate))\n        self.inplanes = outplanes\n        return Transition(inplanes, outplanes, cfg)\n\n    def forward(self, x):\n        x = self.conv1(x)\n\n        x = self.trans1(self.dense1(x))\n        x = self.trans2(self.dense2(x))\n        x = self.dense3(x)\n        x = self.bn(x)\n        x = self.relu(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x'"
mask-impl/models/preresnet.py,1,"b'from __future__ import absolute_import\nimport math\nimport torch.nn as nn\n\n\n__all__ = [\'resnet\']\n\n""""""\npreactivation resnet with bottleneck design.\n""""""\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, cfg, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.conv1 = nn.Conv2d(cfg[0], cfg[1], kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(cfg[1])\n        self.conv2 = nn.Conv2d(cfg[1], cfg[2], kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(cfg[2])\n        self.conv3 = nn.Conv2d(cfg[2], planes * 4, kernel_size=1, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        out = self.bn3(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n\n        return out\n\nclass resnet(nn.Module):\n    def __init__(self, depth=164, dataset=\'cifar10\', cfg=None):\n        super(resnet, self).__init__()\n        assert (depth - 2) % 9 == 0, \'depth should be 9n+2\'\n\n        n = (depth - 2) // 9\n        block = Bottleneck\n\n        if cfg is None:\n            # Construct config variable.\n            cfg = [[16, 16, 16], [64, 16, 16]*(n-1), [64, 32, 32], [128, 32, 32]*(n-1), [128, 64, 64], [256, 64, 64]*(n-1), [256]]\n            cfg = [item for sub_list in cfg for item in sub_list]\n\n        self.inplanes = 16\n\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1,\n                               bias=False)\n        self.layer1 = self._make_layer(block, 16, n, cfg = cfg[0:3*n])\n        self.layer2 = self._make_layer(block, 32, n, cfg = cfg[3*n:6*n], stride=2)\n        self.layer3 = self._make_layer(block, 64, n, cfg = cfg[6*n:9*n], stride=2)\n        self.bn = nn.BatchNorm2d(64 * block.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(8)\n\n        if dataset == \'cifar10\':\n            self.fc = nn.Linear(cfg[-1], 10)\n        elif dataset == \'cifar100\':\n            self.fc = nn.Linear(cfg[-1], 100)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(0.5)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, cfg, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, cfg[0:3], stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, cfg[3*i: 3*(i+1)]))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n\n        x = self.layer1(x)  # 32x32\n        x = self.layer2(x)  # 16x16\n        x = self.layer3(x)  # 8x8\n        x = self.bn(x)\n        x = self.relu(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x'"
mask-impl/models/vgg.py,3,"b""import math\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n\n__all__ = ['vgg']\n\ndefaultcfg = {\n    11 : [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n    13 : [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n    16 : [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512],\n    19 : [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512],\n}\n\nclass vgg(nn.Module):\n    def __init__(self, dataset='cifar10', depth=19, init_weights=True, cfg=None):\n        super(vgg, self).__init__()\n        if cfg is None:\n            cfg = defaultcfg[depth]\n\n        self.feature = self.make_layers(cfg, True)\n\n        if dataset == 'cifar10':\n            num_classes = 10\n        elif dataset == 'cifar100':\n            num_classes = 100\n        self.classifier = nn.Linear(cfg[-1], num_classes)\n        if init_weights:\n            self._initialize_weights()\n\n    def make_layers(self, cfg, batch_norm=False):\n        layers = []\n        in_channels = 3\n        for v in cfg:\n            if v == 'M':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1, bias=False)\n                if batch_norm:\n                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n                else:\n                    layers += [conv2d, nn.ReLU(inplace=True)]\n                in_channels = v\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.feature(x)\n        x = nn.AvgPool2d(2)(x)\n        x = x.view(x.size(0), -1)\n        y = self.classifier(x)\n        return y\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(0.5)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\nif __name__ == '__main__':\n    net = vgg()\n    x = Variable(torch.FloatTensor(16, 3, 40, 40))\n    y = net(x)\n    print(y.data.shape)"""
