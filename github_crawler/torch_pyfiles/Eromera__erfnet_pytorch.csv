file_path,api_count,code
eval/dataset.py,1,"b'# Code with dataset loader for VOC12 and Cityscapes (adapted from bodokaiser/piwise code)\n# Sept 2017\n# Eduardo Romera\n#######################\n\nimport numpy as np\nimport os\n\nfrom PIL import Image\n\nfrom torch.utils.data import Dataset\n\nEXTENSIONS = [\'.jpg\', \'.png\']\n\ndef load_image(file):\n    return Image.open(file)\n\ndef is_image(filename):\n    return any(filename.endswith(ext) for ext in EXTENSIONS)\n\ndef is_label(filename):\n    return filename.endswith(""_labelTrainIds.png"")\n\ndef image_path(root, basename, extension):\n    return os.path.join(root, f\'{basename}{extension}\')\n\ndef image_path_city(root, name):\n    return os.path.join(root, f\'{name}\')\n\ndef image_basename(filename):\n    return os.path.basename(os.path.splitext(filename)[0])\n\nclass VOC12(Dataset):\n\n    def __init__(self, root, input_transform=None, target_transform=None):\n        self.images_root = os.path.join(root, \'images\')\n        self.labels_root = os.path.join(root, \'labels\')\n\n        self.filenames = [image_basename(f)\n            for f in os.listdir(self.labels_root) if is_image(f)]\n        self.filenames.sort()\n\n        self.input_transform = input_transform\n        self.target_transform = target_transform\n\n    def __getitem__(self, index):\n        filename = self.filenames[index]\n\n        with open(image_path(self.images_root, filename, \'.jpg\'), \'rb\') as f:\n            image = load_image(f).convert(\'RGB\')\n        with open(image_path(self.labels_root, filename, \'.png\'), \'rb\') as f:\n            label = load_image(f).convert(\'P\')\n\n        if self.input_transform is not None:\n            image = self.input_transform(image)\n        if self.target_transform is not None:\n            label = self.target_transform(label)\n\n        return image, label\n\n    def __len__(self):\n        return len(self.filenames)\n\n\nclass cityscapes(Dataset):\n\n    def __init__(self, root, input_transform=None, target_transform=None, subset=\'val\'):\n        self.images_root = os.path.join(root, \'leftImg8bit/\' + subset)\n        self.labels_root = os.path.join(root, \'gtFine/\' + subset)\n\n        self.filenames = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(self.images_root)) for f in fn if is_image(f)]\n        self.filenames.sort()\n\n        self.filenamesGt = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(self.labels_root)) for f in fn if is_label(f)]\n        self.filenamesGt.sort()\n\n        self.input_transform = input_transform\n        self.target_transform = target_transform\n\n    def __getitem__(self, index):\n        filename = self.filenames[index]\n        filenameGt = self.filenamesGt[index]\n\n        #print(filename)\n\n        with open(image_path_city(self.images_root, filename), \'rb\') as f:\n            image = load_image(f).convert(\'RGB\')\n        with open(image_path_city(self.labels_root, filenameGt), \'rb\') as f:\n            label = load_image(f).convert(\'P\')\n\n        if self.input_transform is not None:\n            image = self.input_transform(image)\n        if self.target_transform is not None:\n            label = self.target_transform(label)\n\n        return image, label, filename, filenameGt\n\n    def __len__(self):\n        return len(self.filenames)\n\n'"
eval/erfnet.py,4,"b'# ERFNET full network definition for Pytorch\n# Sept 2017\n# Eduardo Romera\n#######################\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\n\nclass DownsamplerBlock (nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n\n        self.conv = nn.Conv2d(ninput, noutput-ninput, (3, 3), stride=2, padding=1, bias=True)\n        self.pool = nn.MaxPool2d(2, stride=2)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n        output = torch.cat([self.conv(input), self.pool(input)], 1)\n        output = self.bn(output)\n        return F.relu(output)\n    \n\nclass non_bottleneck_1d (nn.Module):\n    def __init__(self, chann, dropprob, dilated):        \n        super().__init__()\n\n        self.conv3x1_1 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1,0), bias=True)\n\n        self.conv1x3_1 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1), bias=True)\n\n        self.bn1 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.conv3x1_2 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1*dilated,0), bias=True, dilation = (dilated,1))\n\n        self.conv1x3_2 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1*dilated), bias=True, dilation = (1, dilated))\n\n        self.bn2 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.dropout = nn.Dropout2d(dropprob)\n        \n\n    def forward(self, input):\n\n        output = self.conv3x1_1(input)\n        output = F.relu(output)\n        output = self.conv1x3_1(output)\n        output = self.bn1(output)\n        output = F.relu(output)\n\n        output = self.conv3x1_2(output)\n        output = F.relu(output)\n        output = self.conv1x3_2(output)\n        output = self.bn2(output)\n\n        if (self.dropout.p != 0):\n            output = self.dropout(output)\n        \n        return F.relu(output+input)    #+input = identity (residual connection)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.initial_block = DownsamplerBlock(3,16)\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(DownsamplerBlock(16,64))\n\n        for x in range(0, 5):    #5 times\n           self.layers.append(non_bottleneck_1d(64, 0.1, 1))  \n\n        self.layers.append(DownsamplerBlock(64,128))\n\n        for x in range(0, 2):    #2 times\n            self.layers.append(non_bottleneck_1d(128, 0.1, 2))\n            self.layers.append(non_bottleneck_1d(128, 0.1, 4))\n            self.layers.append(non_bottleneck_1d(128, 0.1, 8))\n            self.layers.append(non_bottleneck_1d(128, 0.1, 16))\n\n        #only for encoder mode:\n        self.output_conv = nn.Conv2d(128, num_classes, 1, stride=1, padding=0, bias=True)\n\n    def forward(self, input, predict=False):\n        output = self.initial_block(input)\n\n        for layer in self.layers:\n            output = layer(output)\n\n        if predict:\n            output = self.output_conv(output)\n\n        return output\n\n\nclass UpsamplerBlock (nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n        self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n        output = self.conv(input)\n        output = self.bn(output)\n        return F.relu(output)\n\nclass Decoder (nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(UpsamplerBlock(128,64))\n        self.layers.append(non_bottleneck_1d(64, 0, 1))\n        self.layers.append(non_bottleneck_1d(64, 0, 1))\n\n        self.layers.append(UpsamplerBlock(64,16))\n        self.layers.append(non_bottleneck_1d(16, 0, 1))\n        self.layers.append(non_bottleneck_1d(16, 0, 1))\n\n        self.output_conv = nn.ConvTranspose2d( 16, num_classes, 2, stride=2, padding=0, output_padding=0, bias=True)\n\n    def forward(self, input):\n        output = input\n\n        for layer in self.layers:\n            output = layer(output)\n\n        output = self.output_conv(output)\n\n        return output\n\n\nclass ERFNet(nn.Module):\n    def __init__(self, num_classes, encoder=None):  #use encoder to pass pretrained encoder\n        super().__init__()\n\n        if (encoder == None):\n            self.encoder = Encoder(num_classes)\n        else:\n            self.encoder = encoder\n        self.decoder = Decoder(num_classes)\n\n    def forward(self, input, only_encode=False):\n        if only_encode:\n            return self.encoder.forward(input, predict=True)\n        else:\n            output = self.encoder(input)    #predict=False by default\n            return self.decoder.forward(output)\n\n'"
eval/erfnet_nobn.py,4,"b'# ERFNET full network definition for Pytorch - without batch normalization layers nor dropout\n# Sept 2017\n# Eduardo Romera\n#######################\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\n#ERFNET definition\n\nclass DownsamplerBlock (nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n\n        self.conv = nn.Conv2d(ninput, noutput-ninput, (3, 3), stride=2, padding=1, bias=True)\n        self.pool = nn.MaxPool2d(2, stride=2)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n        output = torch.cat([self.conv(input), self.pool(input)], 1)\n        #output = self.bn(output)\n        return F.relu(output, inplace=True)\n    \n\nclass non_bottleneck_1d (nn.Module):\n    def __init__(self, chann, dropprob, dilated):        \n        super().__init__()\n\n        self.conv3x1_1 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1,0), bias=True)\n\n        self.conv1x3_1 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1), bias=True)\n\n        self.bn1 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.conv3x1_2 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1*dilated,0), bias=True, dilation = (dilated,1))\n\n        self.conv1x3_2 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1*dilated), bias=True, dilation = (1, dilated))\n\n        self.bn2 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.dropout = nn.Dropout2d(dropprob)\n        \n\n    def forward(self, input):\n\n        output = self.conv3x1_1(input)\n        output = F.relu(output, inplace=True)\n        output = self.conv1x3_1(output)\n        #output = self.bn1(output)\n        output = F.relu(output, inplace=True)\n\n        output = self.conv3x1_2(output)\n        output = F.relu(output)\n        output = self.conv1x3_2(output)\n        #output = self.bn2(output)\n        #output = F.relu(output)    #ESTO ESTABA MAL\n\n        #if (self.dropout.p != 0):\n        #    output = self.dropout(output)\n        \n        return F.relu(output+input, inplace=True)    #+input = identity (residual connection)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.initial_block = DownsamplerBlock(3,16)\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(DownsamplerBlock(16,64))\n\n        for x in range(0, 5):    #5 times\n           self.layers.append(non_bottleneck_1d(64, 0.03, 1))   #Dropout here was wrong in prev trainings\n\n        self.layers.append(DownsamplerBlock(64,128))\n\n        for x in range(0, 2):    #2 times\n            self.layers.append(non_bottleneck_1d(128, 0.3, 2))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 4))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 8))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 16))\n\n        #only for encoder mode:\n        self.output_conv = nn.Conv2d(128, num_classes, 1, stride=1, padding=0, bias=True)\n\n    def forward(self, input, predict=False):\n        output = self.initial_block(input)\n\n        for layer in self.layers:\n            output = layer(output)\n\n        if predict:\n            output = self.output_conv(output)\n\n        return output\n\n\nclass UpsamplerBlock (nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n        self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n        output = self.conv(input)\n        #output = self.bn(output)\n        return F.relu(output, inplace=True)\n\nclass Decoder (nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(UpsamplerBlock(128,64))\n        self.layers.append(non_bottleneck_1d(64, 0, 1))\n        self.layers.append(non_bottleneck_1d(64, 0, 1))\n\n        self.layers.append(UpsamplerBlock(64,16))\n        self.layers.append(non_bottleneck_1d(16, 0, 1))\n        self.layers.append(non_bottleneck_1d(16, 0, 1))\n\n        self.output_conv = nn.ConvTranspose2d( 16, num_classes, 2, stride=2, padding=0, output_padding=0, bias=True)\n\n    def forward(self, input):\n        output = input\n\n        for layer in self.layers:\n            output = layer(output)\n\n        output = self.output_conv(output)\n\n        return output\n\n\nclass ERFNet(nn.Module):\n    def __init__(self, num_classes, encoder=None):  #use encoder to pass pretrained encoder\n        super().__init__()\n\n        if (encoder == None):\n            self.encoder = Encoder(num_classes)\n        else:\n            self.encoder = encoder\n        self.decoder = Decoder(num_classes)\n\n    def forward(self, input, only_encode=False):\n        if only_encode:\n            return self.encoder.forward(input, predict=True)\n        else:\n            output = self.encoder(input)    #predict=False by default\n            return self.decoder.forward(output)\n\n'"
eval/eval_cityscapes_color.py,7,"b'# Code to produce colored segmentation output in Pytorch for all cityscapes subsets  \n# Sept 2017\n# Eduardo Romera\n#######################\n\nimport numpy as np\nimport torch\nimport os\nimport importlib\n\nfrom PIL import Image\nfrom argparse import ArgumentParser\n\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import Compose, CenterCrop, Normalize, Resize\nfrom torchvision.transforms import ToTensor, ToPILImage\n\nfrom dataset import cityscapes\nfrom erfnet import ERFNet\nfrom transform import Relabel, ToLabel, Colorize\n\nimport visdom\n\n\nNUM_CHANNELS = 3\nNUM_CLASSES = 20\n\nimage_transform = ToPILImage()\ninput_transform_cityscapes = Compose([\n    Resize((512,1024),Image.BILINEAR),\n    ToTensor(),\n    #Normalize([.485, .456, .406], [.229, .224, .225]),\n])\ntarget_transform_cityscapes = Compose([\n    Resize((512,1024),Image.NEAREST),\n    ToLabel(),\n    Relabel(255, 19),   #ignore label to 19\n])\n\ncityscapes_trainIds2labelIds = Compose([\n    Relabel(19, 255),  \n    Relabel(18, 33),\n    Relabel(17, 32),\n    Relabel(16, 31),\n    Relabel(15, 28),\n    Relabel(14, 27),\n    Relabel(13, 26),\n    Relabel(12, 25),\n    Relabel(11, 24),\n    Relabel(10, 23),\n    Relabel(9, 22),\n    Relabel(8, 21),\n    Relabel(7, 20),\n    Relabel(6, 19),\n    Relabel(5, 17),\n    Relabel(4, 13),\n    Relabel(3, 12),\n    Relabel(2, 11),\n    Relabel(1, 8),\n    Relabel(0, 7),\n    Relabel(255, 0),\n    ToPILImage(),\n])\n\ndef main(args):\n\n    modelpath = args.loadDir + args.loadModel\n    weightspath = args.loadDir + args.loadWeights\n\n    print (""Loading model: "" + modelpath)\n    print (""Loading weights: "" + weightspath)\n\n    #Import ERFNet model from the folder\n    #Net = importlib.import_module(modelpath.replace(""/"", "".""), ""ERFNet"")\n    model = ERFNet(NUM_CLASSES)\n  \n    model = torch.nn.DataParallel(model)\n    if (not args.cpu):\n        model = model.cuda()\n\n    #model.load_state_dict(torch.load(args.state))\n    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n\n    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n        own_state = model.state_dict()\n        for name, param in state_dict.items():\n            if name not in own_state:\n                 continue\n            own_state[name].copy_(param)\n        return model\n\n    model = load_my_state_dict(model, torch.load(weightspath))\n    print (""Model and weights LOADED successfully"")\n\n    model.eval()\n\n    if(not os.path.exists(args.datadir)):\n        print (""Error: datadir could not be loaded"")\n\n\n    loader = DataLoader(cityscapes(args.datadir, input_transform_cityscapes, target_transform_cityscapes, subset=args.subset),\n        num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n\n    # For visualizer:\n    # must launch in other window ""python3.6 -m visdom.server -port 8097""\n    # and access localhost:8097 to see it\n    if (args.visualize):\n        vis = visdom.Visdom()\n\n    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n        if (not args.cpu):\n            images = images.cuda()\n            #labels = labels.cuda()\n\n        inputs = Variable(images)\n        #targets = Variable(labels)\n        with torch.no_grad():\n            outputs = model(inputs)\n\n        label = outputs[0].max(0)[1].byte().cpu().data\n        #label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n        label_color = Colorize()(label.unsqueeze(0))\n\n        filenameSave = ""./save_color/"" + filename[0].split(""leftImg8bit/"")[1]\n        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n        #image_transform(label.byte()).save(filenameSave)      \n        label_save = ToPILImage()(label_color)           \n        label_save.save(filenameSave) \n\n        if (args.visualize):\n            vis.image(label_color.numpy())\n        print (step, filenameSave)\n\n    \n\nif __name__ == \'__main__\':\n    parser = ArgumentParser()\n\n    parser.add_argument(\'--state\')\n\n    parser.add_argument(\'--loadDir\',default=""../trained_models/"")\n    parser.add_argument(\'--loadWeights\', default=""erfnet_pretrained.pth"")\n    parser.add_argument(\'--loadModel\', default=""erfnet.py"")\n    parser.add_argument(\'--subset\', default=""val"")  #can be val, test, train, demoSequence\n\n    parser.add_argument(\'--datadir\', default=os.getenv(""HOME"") + ""/datasets/cityscapes/"")\n    parser.add_argument(\'--num-workers\', type=int, default=4)\n    parser.add_argument(\'--batch-size\', type=int, default=1)\n    parser.add_argument(\'--cpu\', action=\'store_true\')\n\n    parser.add_argument(\'--visualize\', action=\'store_true\')\n    main(parser.parse_args())\n'"
eval/eval_cityscapes_server.py,7,"b'# Code to produce segmentation output in Pytorch for all cityscapes subset \n# Sept 2017\n# Eduardo Romera\n#######################\n\nimport numpy as np\nimport torch\nimport os\nimport importlib\n\nfrom PIL import Image\nfrom argparse import ArgumentParser\n\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import Compose, CenterCrop, Normalize, Resize\nfrom torchvision.transforms import ToTensor, ToPILImage\n\nfrom dataset import cityscapes\nfrom erfnet import ERFNet\nfrom transform import Relabel, ToLabel, Colorize\n\n\nNUM_CHANNELS = 3\nNUM_CLASSES = 20\n\nimage_transform = ToPILImage()\ninput_transform_cityscapes = Compose([\n    Resize(512),\n    ToTensor(),\n    #Normalize([.485, .456, .406], [.229, .224, .225]),\n])\ntarget_transform_cityscapes = Compose([\n    Resize(512),\n    ToLabel(),\n    Relabel(255, 19),   #ignore label to 19\n])\n\ncityscapes_trainIds2labelIds = Compose([\n    Relabel(19, 255),  \n    Relabel(18, 33),\n    Relabel(17, 32),\n    Relabel(16, 31),\n    Relabel(15, 28),\n    Relabel(14, 27),\n    Relabel(13, 26),\n    Relabel(12, 25),\n    Relabel(11, 24),\n    Relabel(10, 23),\n    Relabel(9, 22),\n    Relabel(8, 21),\n    Relabel(7, 20),\n    Relabel(6, 19),\n    Relabel(5, 17),\n    Relabel(4, 13),\n    Relabel(3, 12),\n    Relabel(2, 11),\n    Relabel(1, 8),\n    Relabel(0, 7),\n    Relabel(255, 0),\n    ToPILImage(),\n    Resize(1024, Image.NEAREST),\n])\n\ndef main(args):\n\n    modelpath = args.loadDir + args.loadModel\n    weightspath = args.loadDir + args.loadWeights\n\n    print (""Loading model: "" + modelpath)\n    print (""Loading weights: "" + weightspath)\n\n    #Import ERFNet model from the folder\n    #Net = importlib.import_module(modelpath.replace(""/"", "".""), ""ERFNet"")\n    model = ERFNet(NUM_CLASSES)\n\n    model = torch.nn.DataParallel(model)\n    if (not args.cpu):\n        model = model.cuda()\n\n    #model.load_state_dict(torch.load(args.state))\n    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n\n    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n        own_state = model.state_dict()\n        for name, param in state_dict.items():\n            if name not in own_state:\n                 continue\n            own_state[name].copy_(param)\n        return model\n\n    model = load_my_state_dict(model, torch.load(weightspath))\n    print (""Model and weights LOADED successfully"")\n\n    model.eval()\n\n    if(not os.path.exists(args.datadir)):\n        print (""Error: datadir could not be loaded"")\n\n\n    loader = DataLoader(cityscapes(args.datadir, input_transform_cityscapes, target_transform_cityscapes, subset=args.subset),\n        num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n\n    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n        if (not args.cpu):\n            images = images.cuda()\n            #labels = labels.cuda()\n\n        inputs = Variable(images)\n        #targets = Variable(labels)\n        with torch.no_grad():\n            outputs = model(inputs)\n\n        label = outputs[0].max(0)[1].byte().cpu().data\n        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n        #print (numpy.unique(label.numpy()))  #debug\n\n        filenameSave = ""./save_results/"" + filename[0].split(""leftImg8bit/"")[1]\n        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n        #image_transform(label.byte()).save(filenameSave)\n        label_cityscapes.save(filenameSave)\n\n        print (step, filenameSave)\n\n    \n\nif __name__ == \'__main__\':\n    parser = ArgumentParser()\n\n    parser.add_argument(\'--state\')\n\n    parser.add_argument(\'--loadDir\',default=""../trained_models/"")\n    parser.add_argument(\'--loadWeights\', default=""erfnet_pretrained.pth"")\n    parser.add_argument(\'--loadModel\', default=""erfnet.py"")\n    parser.add_argument(\'--subset\', default=""val"")  #can be val, test, train, demoSequence\n    parser.add_argument(\'--datadir\', default=os.getenv(""HOME"") + ""/datasets/cityscapes/"")\n    parser.add_argument(\'--num-workers\', type=int, default=4)\n    parser.add_argument(\'--batch-size\', type=int, default=1)\n    parser.add_argument(\'--cpu\', action=\'store_true\')\n\n    main(parser.parse_args())\n'"
eval/eval_forwardTime.py,6,"b'# Code to evaluate forward pass time in Pytorch\n# Sept 2017\n# Eduardo Romera\n#######################\n\nimport os\nimport numpy as np\nimport torch\nimport time\n\nfrom PIL import Image\nfrom argparse import ArgumentParser\n\nfrom torch.autograd import Variable\n\nfrom erfnet_nobn import ERFNet\nfrom transform import Relabel, ToLabel, Colorize\n\nimport torch.backends.cudnn as cudnn\ncudnn.benchmark = True\n\ndef main(args):\n    model = ERFNet(19)\n    if (not args.cpu):\n        model = model.cuda()#.half()\t#HALF seems to be doing slower for some reason\n    #model = torch.nn.DataParallel(model).cuda()\n\n    model.eval()\n\n\n    images = torch.randn(args.batch_size, args.num_channels, args.height, args.width)\n\n    if (not args.cpu):\n        images = images.cuda()#.half()\n\n    time_train = []\n\n    i=0\n\n    while(True):\n    #for step, (images, labels, filename, filenameGt) in enumerate(loader):\n\n        start_time = time.time()\n\n        inputs = Variable(images)\n        with torch.no_grad():\n            outputs = model(inputs)\n\n        #preds = outputs.cpu()\n        if (not args.cpu):\n            torch.cuda.synchronize()    #wait for cuda to finish (cuda is asynchronous!)\n\n        if i!=0:    #first run always takes some time for setup\n            fwt = time.time() - start_time\n            time_train.append(fwt)\n            print (""Forward time per img (b=%d): %.3f (Mean: %.3f)"" % (args.batch_size, fwt/args.batch_size, sum(time_train) / len(time_train) / args.batch_size))\n        \n        time.sleep(1)   #to avoid overheating the GPU too much\n        i+=1\n\nif __name__ == \'__main__\':\n    parser = ArgumentParser()\n\n    parser.add_argument(\'--width\', type=int, default=1024)\n    parser.add_argument(\'--height\', type=int, default=512)\n    parser.add_argument(\'--num-channels\', type=int, default=3)\n    parser.add_argument(\'--batch-size\', type=int, default=1)\n    parser.add_argument(\'--cpu\', action=\'store_true\')\n\n    main(parser.parse_args())\n'"
eval/eval_iou.py,7,"b'# Code to calculate IoU (mean and per-class) in a dataset\n# Nov 2017\n# Eduardo Romera\n#######################\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport os\nimport importlib\nimport time\n\nfrom PIL import Image\nfrom argparse import ArgumentParser\n\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import Compose, CenterCrop, Normalize, Resize\nfrom torchvision.transforms import ToTensor, ToPILImage\n\nfrom dataset import cityscapes\nfrom erfnet import ERFNet\nfrom transform import Relabel, ToLabel, Colorize\nfrom iouEval import iouEval, getColorEntry\n\nNUM_CHANNELS = 3\nNUM_CLASSES = 20\n\nimage_transform = ToPILImage()\ninput_transform_cityscapes = Compose([\n    Resize(512, Image.BILINEAR),\n    ToTensor(),\n])\ntarget_transform_cityscapes = Compose([\n    Resize(512, Image.NEAREST),\n    ToLabel(),\n    Relabel(255, 19),   #ignore label to 19\n])\n\ndef main(args):\n\n    modelpath = args.loadDir + args.loadModel\n    weightspath = args.loadDir + args.loadWeights\n\n    print (""Loading model: "" + modelpath)\n    print (""Loading weights: "" + weightspath)\n\n    model = ERFNet(NUM_CLASSES)\n\n    #model = torch.nn.DataParallel(model)\n    if (not args.cpu):\n        model = torch.nn.DataParallel(model).cuda()\n\n    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n        own_state = model.state_dict()\n        for name, param in state_dict.items():\n            if name not in own_state:\n                if name.startswith(""module.""):\n                    own_state[name.split(""module."")[-1]].copy_(param)\n                else:\n                    print(name, "" not loaded"")\n                    continue\n            else:\n                own_state[name].copy_(param)\n        return model\n\n    model = load_my_state_dict(model, torch.load(weightspath, map_location=lambda storage, loc: storage))\n    print (""Model and weights LOADED successfully"")\n\n\n    model.eval()\n\n    if(not os.path.exists(args.datadir)):\n        print (""Error: datadir could not be loaded"")\n\n\n    loader = DataLoader(cityscapes(args.datadir, input_transform_cityscapes, target_transform_cityscapes, subset=args.subset), num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n\n\n    iouEvalVal = iouEval(NUM_CLASSES)\n\n    start = time.time()\n\n    for step, (images, labels, filename, filenameGt) in enumerate(loader):\n        if (not args.cpu):\n            images = images.cuda()\n            labels = labels.cuda()\n\n        inputs = Variable(images)\n        with torch.no_grad():\n            outputs = model(inputs)\n\n        iouEvalVal.addBatch(outputs.max(1)[1].unsqueeze(1).data, labels)\n\n        filenameSave = filename[0].split(""leftImg8bit/"")[1] \n\n        print (step, filenameSave)\n\n\n    iouVal, iou_classes = iouEvalVal.getIoU()\n\n    iou_classes_str = []\n    for i in range(iou_classes.size(0)):\n        iouStr = getColorEntry(iou_classes[i])+\'{:0.2f}\'.format(iou_classes[i]*100) + \'\\033[0m\'\n        iou_classes_str.append(iouStr)\n\n    print(""---------------------------------------"")\n    print(""Took "", time.time()-start, ""seconds"")\n    print(""======================================="")\n    #print(""TOTAL IOU: "", iou * 100, ""%"")\n    print(""Per-Class IoU:"")\n    print(iou_classes_str[0], ""Road"")\n    print(iou_classes_str[1], ""sidewalk"")\n    print(iou_classes_str[2], ""building"")\n    print(iou_classes_str[3], ""wall"")\n    print(iou_classes_str[4], ""fence"")\n    print(iou_classes_str[5], ""pole"")\n    print(iou_classes_str[6], ""traffic light"")\n    print(iou_classes_str[7], ""traffic sign"")\n    print(iou_classes_str[8], ""vegetation"")\n    print(iou_classes_str[9], ""terrain"")\n    print(iou_classes_str[10], ""sky"")\n    print(iou_classes_str[11], ""person"")\n    print(iou_classes_str[12], ""rider"")\n    print(iou_classes_str[13], ""car"")\n    print(iou_classes_str[14], ""truck"")\n    print(iou_classes_str[15], ""bus"")\n    print(iou_classes_str[16], ""train"")\n    print(iou_classes_str[17], ""motorcycle"")\n    print(iou_classes_str[18], ""bicycle"")\n    print(""======================================="")\n    iouStr = getColorEntry(iouVal)+\'{:0.2f}\'.format(iouVal*100) + \'\\033[0m\'\n    print (""MEAN IoU: "", iouStr, ""%"")\n\nif __name__ == \'__main__\':\n    parser = ArgumentParser()\n\n    parser.add_argument(\'--state\')\n\n    parser.add_argument(\'--loadDir\',default=""../trained_models/"")\n    parser.add_argument(\'--loadWeights\', default=""erfnet_pretrained.pth"")\n    parser.add_argument(\'--loadModel\', default=""erfnet.py"")\n    parser.add_argument(\'--subset\', default=""val"")  #can be val or train (must have labels)\n    parser.add_argument(\'--datadir\', default=os.getenv(""HOME"") + ""/datasets/cityscapes/"")\n    parser.add_argument(\'--num-workers\', type=int, default=4)\n    parser.add_argument(\'--batch-size\', type=int, default=1)\n    parser.add_argument(\'--cpu\', action=\'store_true\')\n\n    main(parser.parse_args())\n'"
eval/iouEval.py,9,"b'# Code for evaluating IoU \n# Nov 2017\n# Eduardo Romera\n#######################\n\nimport torch\n\nclass iouEval:\n\n    def __init__(self, nClasses, ignoreIndex=19):\n        self.nClasses = nClasses\n        self.ignoreIndex = ignoreIndex if nClasses>ignoreIndex else -1 #if ignoreIndex is larger than nClasses, consider no ignoreIndex\n        self.reset()\n\n    def reset (self):\n        classes = self.nClasses if self.ignoreIndex==-1 else self.nClasses-1\n        self.tp = torch.zeros(classes).double()\n        self.fp = torch.zeros(classes).double()\n        self.fn = torch.zeros(classes).double()        \n\n    def addBatch(self, x, y):   #x=preds, y=targets\n        #sizes should be ""batch_size x nClasses x H x W""\n        \n        #print (""X is cuda: "", x.is_cuda)\n        #print (""Y is cuda: "", y.is_cuda)\n\n        if (x.is_cuda or y.is_cuda):\n            x = x.cuda()\n            y = y.cuda()\n\n        #if size is ""batch_size x 1 x H x W"" scatter to onehot\n        if (x.size(1) == 1):\n            x_onehot = torch.zeros(x.size(0), self.nClasses, x.size(2), x.size(3))  \n            if x.is_cuda:\n                x_onehot = x_onehot.cuda()\n            x_onehot.scatter_(1, x, 1).float()\n        else:\n            x_onehot = x.float()\n\n        if (y.size(1) == 1):\n            y_onehot = torch.zeros(y.size(0), self.nClasses, y.size(2), y.size(3))\n            if y.is_cuda:\n                y_onehot = y_onehot.cuda()\n            y_onehot.scatter_(1, y, 1).float()\n        else:\n            y_onehot = y.float()\n\n        if (self.ignoreIndex != -1): \n            ignores = y_onehot[:,self.ignoreIndex].unsqueeze(1)\n            x_onehot = x_onehot[:, :self.ignoreIndex]\n            y_onehot = y_onehot[:, :self.ignoreIndex]\n        else:\n            ignores=0\n\n        #print(type(x_onehot))\n        #print(type(y_onehot))\n        #print(x_onehot.size())\n        #print(y_onehot.size())\n\n        tpmult = x_onehot * y_onehot    #times prediction and gt coincide is 1\n        tp = torch.sum(torch.sum(torch.sum(tpmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze()\n        fpmult = x_onehot * (1-y_onehot-ignores) #times prediction says its that class and gt says its not (subtracting cases when its ignore label!)\n        fp = torch.sum(torch.sum(torch.sum(fpmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze()\n        fnmult = (1-x_onehot) * (y_onehot) #times prediction says its not that class and gt says it is\n        fn = torch.sum(torch.sum(torch.sum(fnmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze() \n\n        self.tp += tp.double().cpu()\n        self.fp += fp.double().cpu()\n        self.fn += fn.double().cpu()\n\n    def getIoU(self):\n        num = self.tp\n        den = self.tp + self.fp + self.fn + 1e-15\n        iou = num / den\n        return torch.mean(iou), iou     #returns ""iou mean"", ""iou per class""\n\n# Class for colors\nclass colors:\n    RED       = \'\\033[31;1m\'\n    GREEN     = \'\\033[32;1m\'\n    YELLOW    = \'\\033[33;1m\'\n    BLUE      = \'\\033[34;1m\'\n    MAGENTA   = \'\\033[35;1m\'\n    CYAN      = \'\\033[36;1m\'\n    BOLD      = \'\\033[1m\'\n    UNDERLINE = \'\\033[4m\'\n    ENDC      = \'\\033[0m\'\n\n# Colored value output if colorized flag is activated.\ndef getColorEntry(val):\n    if not isinstance(val, float):\n        return colors.ENDC\n    if (val < .20):\n        return colors.RED\n    elif (val < .40):\n        return colors.YELLOW\n    elif (val < .60):\n        return colors.BLUE\n    elif (val < .80):\n        return colors.CYAN\n    else:\n        return colors.GREEN\n\n'"
eval/transform.py,4,"b""# Code with transformations for Cityscapes (adapted from bodokaiser/piwise code)\n# Sept 2017\n# Eduardo Romera\n#######################\n\nimport numpy as np\nimport torch\n\nfrom PIL import Image\n\ndef colormap_cityscapes(n):\n    cmap=np.zeros([n, 3]).astype(np.uint8)\n    cmap[0,:] = np.array([128, 64,128])\n    cmap[1,:] = np.array([244, 35,232])\n    cmap[2,:] = np.array([ 70, 70, 70])\n    cmap[3,:] = np.array([ 102,102,156])\n    cmap[4,:] = np.array([ 190,153,153])\n    cmap[5,:] = np.array([ 153,153,153])\n\n    cmap[6,:] = np.array([ 250,170, 30])\n    cmap[7,:] = np.array([ 220,220,  0])\n    cmap[8,:] = np.array([ 107,142, 35])\n    cmap[9,:] = np.array([ 152,251,152])\n    cmap[10,:] = np.array([ 70,130,180])\n\n    cmap[11,:] = np.array([ 220, 20, 60])\n    cmap[12,:] = np.array([ 255,  0,  0])\n    cmap[13,:] = np.array([ 0,  0,142])\n    cmap[14,:] = np.array([  0,  0, 70])\n    cmap[15,:] = np.array([  0, 60,100])\n\n    cmap[16,:] = np.array([  0, 80,100])\n    cmap[17,:] = np.array([  0,  0,230])\n    cmap[18,:] = np.array([ 119, 11, 32])\n    cmap[19,:] = np.array([ 0,  0,  0])\n    \n    return cmap\n\n\ndef colormap(n):\n    cmap=np.zeros([n, 3]).astype(np.uint8)\n\n    for i in np.arange(n):\n        r, g, b = np.zeros(3)\n\n        for j in np.arange(8):\n            r = r + (1<<(7-j))*((i&(1<<(3*j))) >> (3*j))\n            g = g + (1<<(7-j))*((i&(1<<(3*j+1))) >> (3*j+1))\n            b = b + (1<<(7-j))*((i&(1<<(3*j+2))) >> (3*j+2))\n\n        cmap[i,:] = np.array([r, g, b])\n\n    return cmap\n\nclass Relabel:\n\n    def __init__(self, olabel, nlabel):\n        self.olabel = olabel\n        self.nlabel = nlabel\n\n    def __call__(self, tensor):\n        assert isinstance(tensor, torch.LongTensor) or isinstance(tensor, torch.ByteTensor) , 'tensor needs to be LongTensor'\n        tensor[tensor == self.olabel] = self.nlabel\n        return tensor\n\n\nclass ToLabel:\n\n    def __call__(self, image):\n        return torch.from_numpy(np.array(image)).long().unsqueeze(0)\n\n\nclass Colorize:\n\n    def __init__(self, n=22):\n        #self.cmap = colormap(256)\n        self.cmap = colormap_cityscapes(256)\n        self.cmap[n] = self.cmap[-1]\n        self.cmap = torch.from_numpy(self.cmap[:n])\n\n    def __call__(self, gray_image):\n        size = gray_image.size()\n        color_image = torch.ByteTensor(3, size[1], size[2]).fill_(0)\n\n        #for label in range(1, len(self.cmap)):\n        for label in range(0, len(self.cmap)):\n            mask = gray_image[0] == label\n\n            color_image[0][mask] = self.cmap[label][0]\n            color_image[1][mask] = self.cmap[label][1]\n            color_image[2][mask] = self.cmap[label][2]\n\n        return color_image\n"""
imagenet/erfnet_imagenet.py,4,"b'# ERFNet encoder model definition used for pretraining in ImageNet\n# Sept 2017\n# Eduardo Romera\n#######################\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\nclass DownsamplerBlock (nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n\n        self.conv = nn.Conv2d(ninput, noutput-ninput, (3, 3), stride=2, padding=1, bias=True)\n        self.pool = nn.MaxPool2d(2, stride=2)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n        output = torch.cat([self.conv(input), self.pool(input)], 1)\n        output = self.bn(output)\n        return F.relu(output)\n\nclass non_bottleneck_1d (nn.Module):\n    def __init__(self, chann, dropprob, dilated):        \n        super().__init__()\n\n        \n        self.conv3x1_1 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1,0), bias=True)\n        self.conv1x3_1 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1), bias=True)\n\n        self.conv3x1_2 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1*dilated,0), bias=True, dilation = (dilated,1))\n        self.conv1x3_2 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1*dilated), bias=True, dilation = (1, dilated))\n\n        self.bn1 = nn.BatchNorm2d(chann, eps=1e-03)\n        self.bn2 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.dropout = nn.Dropout2d(dropprob)\n        \n\n    def forward(self, input):\n\n        output = self.conv3x1_1(input)\n        output = F.relu(output)\n        output = self.conv1x3_1(output)\n        output = self.bn1(output)\n        output = F.relu(output)\n\n        output = self.conv3x1_2(output)\n        output = F.relu(output)\n        output = self.conv1x3_2(output)\n        output = self.bn2(output)\n\n        if (self.dropout.p != 0):\n            output = self.dropout(output)\n        \n        return F.relu(output+input)  \n\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.initial_block = DownsamplerBlock(3,16)\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(DownsamplerBlock(16,64))\n\n        for x in range(0, 5):    #5 times\n            self.layers.append(non_bottleneck_1d(64, 0.1, 1))  \n        self.layers.append(DownsamplerBlock(64,128))\n\n        for x in range(0, 2):    #2 times\n            self.layers.append(non_bottleneck_1d(128, 0.1, 2))\n            self.layers.append(non_bottleneck_1d(128, 0.1, 4))\n            self.layers.append(non_bottleneck_1d(128, 0.1, 8))\n            self.layers.append(non_bottleneck_1d(128, 0.1, 16))\n\n\n    def forward(self, input):\n        output = self.initial_block(input)\n\n        for layer in self.layers:\n            output = layer(output)\n\n        return output\n\n\nclass Features(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = Encoder()\n        self.extralayer1 = nn.MaxPool2d(2, stride=2)\n        self.extralayer2 = nn.AvgPool2d(14,1,0)\n\n    def forward(self, input):\n        output = self.encoder(input)\n        output = self.extralayer1(output)\n        output = self.extralayer2(output)\n        return output\n\nclass Classifier(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.linear = nn.Linear(128, num_classes)\n\n    def forward(self, input):\n        output = input.view(input.size(0), 128) #first is batch_size\n        output = self.linear(output)\n        return output\n\nclass ERFNet(nn.Module):\n    def __init__(self, num_classes):  #use encoder to pass pretrained encoder\n        super().__init__()\n\n        self.features = Features()\n        self.classifier = Classifier(num_classes)\n\n    def forward(self, input):\n        output = self.features(input)\n        output = self.classifier(output)\n        return output\n\n'"
imagenet/main.py,16,"b'# Script for training ERFNet encoder in ImageNet\n# Sept 2017\n# Eduardo Romera\n#######################\n\nimport argparse\nimport os\nimport shutil\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\n\nfrom torch.optim import lr_scheduler\n\nfrom erfnet_imagenet import ERFNet\n\nmodel_names = sorted(name for name in models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(models.__dict__[name]))\n\n\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\nparser.add_argument(\'data\', metavar=\'DIR\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'erfnet\',\n                    #choices=model_names,\n                    help=\'model architecture: \' +\n                        \' | \'.join(model_names) +\n                        \' (default: resnet18)\')\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\nparser.add_argument(\'--epochs\', default=90, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=256, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 256)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.1, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--print-freq\', \'-p\', default=10, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 10)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                    help=\'evaluate model on validation set\')\nparser.add_argument(\'--pretrained\', dest=\'pretrained\', action=\'store_true\',\n                    help=\'use pre-trained model\')\n\nbest_prec1 = 0\n\ndef main():\n    global args, best_prec1\n    args = parser.parse_args()\n\n    # create model\n    if (args.arch == \'erfnet\'):\n        model = ERFNet(1000)\n    else:\n        if args.pretrained:\n            print(""=> using pre-trained model \'{}\'"".format(args.arch))\n            model = models.__dict__[args.arch](pretrained=True)\n        else:\n            print(""=> creating model \'{}\'"".format(args.arch))\n            model = models.__dict__[args.arch]()\n\n    model = torch.nn.DataParallel(model).cuda()\n\n    # define loss function (criterion) and optimizer\n    criterion = nn.CrossEntropyLoss().cuda()\n\n    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n\n   # optionally resume from a checkpoint\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_prec1 = checkpoint[\'best_prec1\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.resume, checkpoint[\'epoch\']))\n        else:\n            print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n\n    cudnn.benchmark = True\n\n    # Data loading code\n    traindir = os.path.join(args.data, \'train\')\n    valdir = os.path.join(args.data, \'val\')\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n\n    train_loader = torch.utils.data.DataLoader(\n        datasets.ImageFolder(traindir, transforms.Compose([\n            #RemoveExif(),\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ])),\n        batch_size=args.batch_size, shuffle=True,\n        num_workers=args.workers, pin_memory=True)\n\n    val_loader = torch.utils.data.DataLoader(\n        datasets.ImageFolder(valdir, transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            normalize,\n        ])),\n        batch_size=args.batch_size, shuffle=False,\n        num_workers=args.workers, pin_memory=True)\n\n    if args.evaluate:\n        validate(val_loader, model, criterion)\n        return\n\n    for epoch in range(args.start_epoch, args.epochs):\n        adjust_learning_rate(optimizer, epoch)\n\n        # train for one epoch\n        train(train_loader, model, criterion, optimizer, epoch)\n\n        # evaluate on validation set\n        prec1 = validate(val_loader, model, criterion)\n\n        # remember best prec@1 and save checkpoint\n        is_best = prec1 > best_prec1\n        best_prec1 = max(prec1, best_prec1)\n        save_checkpoint({\n            \'epoch\': epoch + 1,\n            \'arch\': args.arch,\n            \'state_dict\': model.state_dict(),\n            \'best_prec1\': best_prec1,\n            \'optimizer\' : optimizer.state_dict(),\n        }, is_best)\n\n        #scheduler.step(prec1, epoch)    #decreases learning rate if prec1 plateaus\n\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (input, target) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        target = target.cuda(async=True)\n        input_var = torch.autograd.Variable(input)\n        target_var = torch.autograd.Variable(target)\n\n        # compute output\n        output = model(input_var)\n        loss = criterion(output, target_var)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n        losses.update(loss.data[0], input.size(0))\n        top1.update(prec1[0], input.size(0))\n        top5.update(prec5[0], input.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            for param_group in optimizer.param_groups:\n                lr = param_group[\'lr\']\n            print(\'Epoch: [{0}][{1}/{2}][lr:{lr:.6g}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.2f}) / \'\n                  \'Data {data_time.val:.3f} ({data_time.avg:.2f})\\t\'\n                  \'Loss {loss.val:.3f} ({loss.avg:.3f})\\t\'\n                  \'Prec@1 {top1.val:.2f} ({top1.avg:.2f})\\t\'\n                  \'Prec@5 {top5.val:.2f} ({top5.avg:.2f})\'.format(\n                   epoch, i, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses, top1=top1, top5=top5, lr=lr))\n\n\ndef validate(val_loader, model, criterion):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    for i, (input, target) in enumerate(val_loader):\n        target = target.cuda(async=True)\n        input_var = torch.autograd.Variable(input, volatile=True)\n        target_var = torch.autograd.Variable(target, volatile=True)\n\n        # compute output\n        output = model(input_var)\n        loss = criterion(output, target_var)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n        losses.update(loss.data[0], input.size(0))\n        top1.update(prec1[0], input.size(0))\n        top5.update(prec5[0], input.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print(\'Test: [{0}/{1}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                  \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                   i, len(val_loader), batch_time=batch_time, loss=losses,\n                   top1=top1, top5=top5))\n\n    print(\' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}\'\n          .format(top1=top1, top5=top5))\n\n    return top1.avg\n\n\ndef save_checkpoint(state, is_best, filename=\'checkpoint.pth.tar\'):\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, \'model_best.pth.tar\')\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef adjust_learning_rate(optimizer, epoch):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""    \n    lr = args.lr\n    wd = 1e-4\n    milestone = 15  #after epoch milestone, lr is reduced exponentially\n    if epoch > milestone:\n        lr = args.lr * (0.95 ** (epoch-milestone))\n        wd = 0\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n        param_group[\'weight_decay\'] = wd\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\n\nif __name__ == \'__main__\':\n    main()\n'"
train/dataset.py,1,"b'import numpy as np\nimport os\n\nfrom PIL import Image\n\nfrom torch.utils.data import Dataset\n\nEXTENSIONS = [\'.jpg\', \'.png\']\n\ndef load_image(file):\n    return Image.open(file)\n\ndef is_image(filename):\n    return any(filename.endswith(ext) for ext in EXTENSIONS)\n\ndef is_label(filename):\n    return filename.endswith(""_labelTrainIds.png"")\n\ndef image_path(root, basename, extension):\n    return os.path.join(root, f\'{basename}{extension}\')\n\ndef image_path_city(root, name):\n    return os.path.join(root, f\'{name}\')\n\ndef image_basename(filename):\n    return os.path.basename(os.path.splitext(filename)[0])\n\nclass VOC12(Dataset):\n\n    def __init__(self, root, input_transform=None, target_transform=None):\n        self.images_root = os.path.join(root, \'images\')\n        self.labels_root = os.path.join(root, \'labels\')\n\n        self.filenames = [image_basename(f)\n            for f in os.listdir(self.labels_root) if is_image(f)]\n        self.filenames.sort()\n\n        self.input_transform = input_transform\n        self.target_transform = target_transform\n\n    def __getitem__(self, index):\n        filename = self.filenames[index]\n\n        with open(image_path(self.images_root, filename, \'.jpg\'), \'rb\') as f:\n            image = load_image(f).convert(\'RGB\')\n        with open(image_path(self.labels_root, filename, \'.png\'), \'rb\') as f:\n            label = load_image(f).convert(\'P\')\n\n        if self.input_transform is not None:\n            image = self.input_transform(image)\n        if self.target_transform is not None:\n            label = self.target_transform(label)\n\n        return image, label\n\n    def __len__(self):\n        return len(self.filenames)\n\n\n\n\nclass cityscapes(Dataset):\n\n    def __init__(self, root, co_transform=None, subset=\'train\'):\n        self.images_root = os.path.join(root, \'leftImg8bit/\')\n        self.labels_root = os.path.join(root, \'gtFine/\')\n        \n        self.images_root += subset\n        self.labels_root += subset\n\n        print (self.images_root)\n        #self.filenames = [image_basename(f) for f in os.listdir(self.images_root) if is_image(f)]\n        self.filenames = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(self.images_root)) for f in fn if is_image(f)]\n        self.filenames.sort()\n\n        #[os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(""."")) for f in fn]\n        #self.filenamesGt = [image_basename(f) for f in os.listdir(self.labels_root) if is_image(f)]\n        self.filenamesGt = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(self.labels_root)) for f in fn if is_label(f)]\n        self.filenamesGt.sort()\n\n        self.co_transform = co_transform # ADDED THIS\n\n\n    def __getitem__(self, index):\n        filename = self.filenames[index]\n        filenameGt = self.filenamesGt[index]\n\n        with open(image_path_city(self.images_root, filename), \'rb\') as f:\n            image = load_image(f).convert(\'RGB\')\n        with open(image_path_city(self.labels_root, filenameGt), \'rb\') as f:\n            label = load_image(f).convert(\'P\')\n\n        if self.co_transform is not None:\n            image, label = self.co_transform(image, label)\n\n        return image, label\n\n    def __len__(self):\n        return len(self.filenames)\n\n'"
train/erfnet.py,4,"b'# ERFNet full model definition for Pytorch\n# Sept 2017\n# Eduardo Romera\n#######################\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\nclass DownsamplerBlock (nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n\n        self.conv = nn.Conv2d(ninput, noutput-ninput, (3, 3), stride=2, padding=1, bias=True)\n        self.pool = nn.MaxPool2d(2, stride=2)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n        output = torch.cat([self.conv(input), self.pool(input)], 1)\n        output = self.bn(output)\n        return F.relu(output)\n    \n\nclass non_bottleneck_1d (nn.Module):\n    def __init__(self, chann, dropprob, dilated):        \n        super().__init__()\n\n        self.conv3x1_1 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1,0), bias=True)\n\n        self.conv1x3_1 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1), bias=True)\n\n        self.bn1 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.conv3x1_2 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1*dilated,0), bias=True, dilation = (dilated,1))\n\n        self.conv1x3_2 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1*dilated), bias=True, dilation = (1, dilated))\n\n        self.bn2 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.dropout = nn.Dropout2d(dropprob)\n        \n\n    def forward(self, input):\n\n        output = self.conv3x1_1(input)\n        output = F.relu(output)\n        output = self.conv1x3_1(output)\n        output = self.bn1(output)\n        output = F.relu(output)\n\n        output = self.conv3x1_2(output)\n        output = F.relu(output)\n        output = self.conv1x3_2(output)\n        output = self.bn2(output)\n\n        if (self.dropout.p != 0):\n            output = self.dropout(output)\n        \n        return F.relu(output+input)    #+input = identity (residual connection)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.initial_block = DownsamplerBlock(3,16)\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(DownsamplerBlock(16,64))\n\n        for x in range(0, 5):    #5 times\n           self.layers.append(non_bottleneck_1d(64, 0.03, 1)) \n\n        self.layers.append(DownsamplerBlock(64,128))\n\n        for x in range(0, 2):    #2 times\n            self.layers.append(non_bottleneck_1d(128, 0.3, 2))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 4))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 8))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 16))\n\n        #Only in encoder mode:\n        self.output_conv = nn.Conv2d(128, num_classes, 1, stride=1, padding=0, bias=True)\n\n    def forward(self, input, predict=False):\n        output = self.initial_block(input)\n\n        for layer in self.layers:\n            output = layer(output)\n\n        if predict:\n            output = self.output_conv(output)\n\n        return output\n\n\nclass UpsamplerBlock (nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n        self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n        output = self.conv(input)\n        output = self.bn(output)\n        return F.relu(output)\n\nclass Decoder (nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(UpsamplerBlock(128,64))\n        self.layers.append(non_bottleneck_1d(64, 0, 1))\n        self.layers.append(non_bottleneck_1d(64, 0, 1))\n\n        self.layers.append(UpsamplerBlock(64,16))\n        self.layers.append(non_bottleneck_1d(16, 0, 1))\n        self.layers.append(non_bottleneck_1d(16, 0, 1))\n\n        self.output_conv = nn.ConvTranspose2d( 16, num_classes, 2, stride=2, padding=0, output_padding=0, bias=True)\n\n    def forward(self, input):\n        output = input\n\n        for layer in self.layers:\n            output = layer(output)\n\n        output = self.output_conv(output)\n\n        return output\n\n#ERFNet\nclass Net(nn.Module):\n    def __init__(self, num_classes, encoder=None):  #use encoder to pass pretrained encoder\n        super().__init__()\n\n        if (encoder == None):\n            self.encoder = Encoder(num_classes)\n        else:\n            self.encoder = encoder\n        self.decoder = Decoder(num_classes)\n\n    def forward(self, input, only_encode=False):\n        if only_encode:\n            return self.encoder.forward(input, predict=True)\n        else:\n            output = self.encoder(input)    #predict=False by default\n            return self.decoder.forward(output)\n'"
train/erfnet_imagenet.py,4,"b'# ERFNet encoder model definition used for pretraining in ImageNet\n# Sept 2017\n# Eduardo Romera\n#######################\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\nclass DownsamplerBlock (nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n\n        self.conv = nn.Conv2d(ninput, noutput-ninput, (3, 3), stride=2, padding=1, bias=True)\n        self.pool = nn.MaxPool2d(2, stride=2)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n        output = torch.cat([self.conv(input), self.pool(input)], 1)\n        output = self.bn(output)\n        return F.relu(output)\n    \n\nclass non_bottleneck_1d (nn.Module):\n    def __init__(self, chann, dropprob, dilated):        \n        super().__init__()\n\n        \n        self.conv3x1_1 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1,0), bias=True)\n\n        self.conv1x3_1 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1), bias=True)\n\n\n        self.conv3x1_2 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1*dilated,0), bias=True, dilation = (dilated,1))\n\n        self.conv1x3_2 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1*dilated), bias=True, dilation = (1, dilated))\n        \n        self.bn1 = nn.BatchNorm2d(chann, eps=1e-03)\n        self.bn2 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.dropout = nn.Dropout2d(dropprob)\n        \n\n    def forward(self, input):\n\n        output = self.conv3x1_1(input)\n        output = F.relu(output)\n        output = self.conv1x3_1(output)\n        output = self.bn1(output)\n        output = F.relu(output)\n\n        output = self.conv3x1_2(output)\n        output = F.relu(output)\n        output = self.conv1x3_2(output)\n        output = self.bn2(output)\n\n        if (self.dropout.p != 0):\n            output = self.dropout(output)\n        \n        return F.relu(output+input)    #+input = identity (residual connection)\n\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.initial_block = DownsamplerBlock(3,16)\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(DownsamplerBlock(16,64))\n\n        for x in range(0, 5):    #5 times\n           self.layers.append(non_bottleneck_1d(64, 0.1, 1))  \n\n        self.layers.append(DownsamplerBlock(64,128))\n\n        for x in range(0, 2):    #2 times\n            self.layers.append(non_bottleneck_1d(128, 0.1, 2))\n            self.layers.append(non_bottleneck_1d(128, 0.1, 4))\n            self.layers.append(non_bottleneck_1d(128, 0.1, 8))\n            self.layers.append(non_bottleneck_1d(128, 0.1, 16))\n\n\n    def forward(self, input):\n        output = self.initial_block(input)\n\n        for layer in self.layers:\n            output = layer(output)\n\n        return output\n\n\nclass Features(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = Encoder()\n        self.extralayer1 = nn.MaxPool2d(2, stride=2)\n        self.extralayer2 = nn.AvgPool2d(14,1,0)\n\n    def forward(self, input):\n        #print(""Feat input: "", input.size())\n        output = self.encoder(input)\n        output = self.extralayer1(output)\n        output = self.extralayer2(output)\n        #print(""Feat output: "", output.size())\n        return output\n\nclass Classifier(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.linear = nn.Linear(128, num_classes)\n\n    def forward(self, input):\n        output = input.view(input.size(0), 128) #first is batch_size\n        output = self.linear(output)\n        return output\n\nclass ERFNet(nn.Module):\n    def __init__(self, num_classes):  #use encoder to pass pretrained encoder\n        super().__init__()\n\n        self.features = Features()\n        self.classifier = Classifier(num_classes)\n\n    def forward(self, input):\n        output = self.features(input)\n        output = self.classifier(output)\n        return output\n\n\n'"
train/iouEval.py,9,"b'# Code for evaluating IoU \n# Nov 2017\n# Eduardo Romera\n#######################\n\nimport torch\n\nclass iouEval:\n\n    def __init__(self, nClasses, ignoreIndex=19):\n        self.nClasses = nClasses\n        self.ignoreIndex = ignoreIndex if nClasses>ignoreIndex else -1 #if ignoreIndex is larger than nClasses, consider no ignoreIndex\n        self.reset()\n\n    def reset (self):\n        classes = self.nClasses if self.ignoreIndex==-1 else self.nClasses-1\n        self.tp = torch.zeros(classes).double()\n        self.fp = torch.zeros(classes).double()\n        self.fn = torch.zeros(classes).double()        \n\n    def addBatch(self, x, y):   #x=preds, y=targets\n        #sizes should be ""batch_size x nClasses x H x W""\n        \n        #print (""X is cuda: "", x.is_cuda)\n        #print (""Y is cuda: "", y.is_cuda)\n\n        if (x.is_cuda or y.is_cuda):\n            x = x.cuda()\n            y = y.cuda()\n\n        #if size is ""batch_size x 1 x H x W"" scatter to onehot\n        if (x.size(1) == 1):\n            x_onehot = torch.zeros(x.size(0), self.nClasses, x.size(2), x.size(3))  \n            if x.is_cuda:\n                x_onehot = x_onehot.cuda()\n            x_onehot.scatter_(1, x, 1).float()\n        else:\n            x_onehot = x.float()\n\n        if (y.size(1) == 1):\n            y_onehot = torch.zeros(y.size(0), self.nClasses, y.size(2), y.size(3))\n            if y.is_cuda:\n                y_onehot = y_onehot.cuda()\n            y_onehot.scatter_(1, y, 1).float()\n        else:\n            y_onehot = y.float()\n\n        if (self.ignoreIndex != -1): \n            ignores = y_onehot[:,self.ignoreIndex].unsqueeze(1)\n            x_onehot = x_onehot[:, :self.ignoreIndex]\n            y_onehot = y_onehot[:, :self.ignoreIndex]\n        else:\n            ignores=0\n\n        #print(type(x_onehot))\n        #print(type(y_onehot))\n        #print(x_onehot.size())\n        #print(y_onehot.size())\n\n        tpmult = x_onehot * y_onehot    #times prediction and gt coincide is 1\n        tp = torch.sum(torch.sum(torch.sum(tpmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze()\n        fpmult = x_onehot * (1-y_onehot-ignores) #times prediction says its that class and gt says its not (subtracting cases when its ignore label!)\n        fp = torch.sum(torch.sum(torch.sum(fpmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze()\n        fnmult = (1-x_onehot) * (y_onehot) #times prediction says its not that class and gt says it is\n        fn = torch.sum(torch.sum(torch.sum(fnmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze() \n\n        self.tp += tp.double().cpu()\n        self.fp += fp.double().cpu()\n        self.fn += fn.double().cpu()\n\n    def getIoU(self):\n        num = self.tp\n        den = self.tp + self.fp + self.fn + 1e-15\n        iou = num / den\n        return torch.mean(iou), iou     #returns ""iou mean"", ""iou per class""\n\n# Class for colors\nclass colors:\n    RED       = \'\\033[31;1m\'\n    GREEN     = \'\\033[32;1m\'\n    YELLOW    = \'\\033[33;1m\'\n    BLUE      = \'\\033[34;1m\'\n    MAGENTA   = \'\\033[35;1m\'\n    CYAN      = \'\\033[36;1m\'\n    BOLD      = \'\\033[1m\'\n    UNDERLINE = \'\\033[4m\'\n    ENDC      = \'\\033[0m\'\n\n# Colored value output if colorized flag is activated.\ndef getColorEntry(val):\n    if not isinstance(val, float):\n        return colors.ENDC\n    if (val < .20):\n        return colors.RED\n    elif (val < .40):\n        return colors.YELLOW\n    elif (val < .60):\n        return colors.BLUE\n    elif (val < .80):\n        return colors.CYAN\n    else:\n        return colors.GREEN\n\n'"
train/main.py,23,"b'# Main code for training ERFNet model in Cityscapes dataset\n# Sept 2017\n# Eduardo Romera\n#######################\n\nimport os\nimport random\nimport time\nimport numpy as np\nimport torch\nimport math\n\nfrom PIL import Image, ImageOps\nfrom argparse import ArgumentParser\n\nfrom torch.optim import SGD, Adam, lr_scheduler\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import Compose, CenterCrop, Normalize, Resize, Pad\nfrom torchvision.transforms import ToTensor, ToPILImage\n\nfrom dataset import VOC12,cityscapes\nfrom transform import Relabel, ToLabel, Colorize\nfrom visualize import Dashboard\n\nimport importlib\nfrom iouEval import iouEval, getColorEntry\n\nfrom shutil import copyfile\n\nNUM_CHANNELS = 3\nNUM_CLASSES = 20 #pascal=22, cityscapes=20\n\ncolor_transform = Colorize(NUM_CLASSES)\nimage_transform = ToPILImage()\n\n#Augmentations - different function implemented to perform random augments on both image and target\nclass MyCoTransform(object):\n    def __init__(self, enc, augment=True, height=512):\n        self.enc=enc\n        self.augment = augment\n        self.height = height\n        pass\n    def __call__(self, input, target):\n        # do something to both images\n        input =  Resize(self.height, Image.BILINEAR)(input)\n        target = Resize(self.height, Image.NEAREST)(target)\n\n        if(self.augment):\n            # Random hflip\n            hflip = random.random()\n            if (hflip < 0.5):\n                input = input.transpose(Image.FLIP_LEFT_RIGHT)\n                target = target.transpose(Image.FLIP_LEFT_RIGHT)\n            \n            #Random translation 0-2 pixels (fill rest with padding\n            transX = random.randint(-2, 2) \n            transY = random.randint(-2, 2)\n\n            input = ImageOps.expand(input, border=(transX,transY,0,0), fill=0)\n            target = ImageOps.expand(target, border=(transX,transY,0,0), fill=255) #pad label filling with 255\n            input = input.crop((0, 0, input.size[0]-transX, input.size[1]-transY))\n            target = target.crop((0, 0, target.size[0]-transX, target.size[1]-transY))   \n\n        input = ToTensor()(input)\n        if (self.enc):\n            target = Resize(int(self.height/8), Image.NEAREST)(target)\n        target = ToLabel()(target)\n        target = Relabel(255, 19)(target)\n\n        return input, target\n\n\nclass CrossEntropyLoss2d(torch.nn.Module):\n\n    def __init__(self, weight=None):\n        super().__init__()\n\n        self.loss = torch.nn.NLLLoss2d(weight)\n\n    def forward(self, outputs, targets):\n        return self.loss(torch.nn.functional.log_softmax(outputs, dim=1), targets)\n\n\ndef train(args, model, enc=False):\n    best_acc = 0\n\n    #TODO: calculate weights by processing dataset histogram (now its being set by hand from the torch values)\n    #create a loder to run all images and calculate histogram of labels, then create weight array using class balancing\n\n    weight = torch.ones(NUM_CLASSES)\n    if (enc):\n        weight[0] = 2.3653597831726\t\n        weight[1] = 4.4237880706787\t\n        weight[2] = 2.9691488742828\t\n        weight[3] = 5.3442072868347\t\n        weight[4] = 5.2983593940735\t\n        weight[5] = 5.2275490760803\t\n        weight[6] = 5.4394111633301\t\n        weight[7] = 5.3659925460815\t\n        weight[8] = 3.4170460700989\t\n        weight[9] = 5.2414722442627\t\n        weight[10] = 4.7376127243042\t\n        weight[11] = 5.2286224365234\t\n        weight[12] = 5.455126285553\t\n        weight[13] = 4.3019247055054\t\n        weight[14] = 5.4264230728149\t\n        weight[15] = 5.4331531524658\t\n        weight[16] = 5.433765411377\t\n        weight[17] = 5.4631009101868\t\n        weight[18] = 5.3947434425354\n    else:\n        weight[0] = 2.8149201869965\t\n        weight[1] = 6.9850029945374\t\n        weight[2] = 3.7890393733978\t\n        weight[3] = 9.9428062438965\t\n        weight[4] = 9.7702074050903\t\n        weight[5] = 9.5110931396484\t\n        weight[6] = 10.311357498169\t\n        weight[7] = 10.026463508606\t\n        weight[8] = 4.6323022842407\t\n        weight[9] = 9.5608062744141\t\n        weight[10] = 7.8698215484619\t\n        weight[11] = 9.5168733596802\t\n        weight[12] = 10.373730659485\t\n        weight[13] = 6.6616044044495\t\n        weight[14] = 10.260489463806\t\n        weight[15] = 10.287888526917\t\n        weight[16] = 10.289801597595\t\n        weight[17] = 10.405355453491\t\n        weight[18] = 10.138095855713\t\n\n    weight[19] = 0\n\n    assert os.path.exists(args.datadir), ""Error: datadir (dataset directory) could not be loaded""\n\n    co_transform = MyCoTransform(enc, augment=True, height=args.height)#1024)\n    co_transform_val = MyCoTransform(enc, augment=False, height=args.height)#1024)\n    dataset_train = cityscapes(args.datadir, co_transform, \'train\')\n    dataset_val = cityscapes(args.datadir, co_transform_val, \'val\')\n\n    loader = DataLoader(dataset_train, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=True)\n    loader_val = DataLoader(dataset_val, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n\n    if args.cuda:\n        weight = weight.cuda()\n    criterion = CrossEntropyLoss2d(weight)\n    print(type(criterion))\n\n    savedir = f\'../save/{args.savedir}\'\n\n    if (enc):\n        automated_log_path = savedir + ""/automated_log_encoder.txt""\n        modeltxtpath = savedir + ""/model_encoder.txt""\n    else:\n        automated_log_path = savedir + ""/automated_log.txt""\n        modeltxtpath = savedir + ""/model.txt""    \n\n    if (not os.path.exists(automated_log_path)):    #dont add first line if it exists \n        with open(automated_log_path, ""a"") as myfile:\n            myfile.write(""Epoch\\t\\tTrain-loss\\t\\tTest-loss\\t\\tTrain-IoU\\t\\tTest-IoU\\t\\tlearningRate"")\n\n    with open(modeltxtpath, ""w"") as myfile:\n        myfile.write(str(model))\n\n\n    #TODO: reduce memory in first gpu: https://discuss.pytorch.org/t/multi-gpu-training-memory-usage-in-balance/4163/4        #https://github.com/pytorch/pytorch/issues/1893\n\n    #optimizer = Adam(model.parameters(), 5e-4, (0.9, 0.999),  eps=1e-08, weight_decay=2e-4)     ## scheduler 1\n    optimizer = Adam(model.parameters(), 5e-4, (0.9, 0.999),  eps=1e-08, weight_decay=1e-4)      ## scheduler 2\n\n    start_epoch = 1\n    if args.resume:\n        #Must load weights, optimizer, epoch and best value. \n        if enc:\n            filenameCheckpoint = savedir + \'/checkpoint_enc.pth.tar\'\n        else:\n            filenameCheckpoint = savedir + \'/checkpoint.pth.tar\'\n\n        assert os.path.exists(filenameCheckpoint), ""Error: resume option was used but checkpoint was not found in folder""\n        checkpoint = torch.load(filenameCheckpoint)\n        start_epoch = checkpoint[\'epoch\']\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        best_acc = checkpoint[\'best_acc\']\n        print(""=> Loaded checkpoint at epoch {})"".format(checkpoint[\'epoch\']))\n\n    #scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, \'min\', factor=0.5) # set up scheduler     ## scheduler 1\n    lambda1 = lambda epoch: pow((1-((epoch-1)/args.num_epochs)),0.9)  ## scheduler 2\n    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)                             ## scheduler 2\n\n    if args.visualize and args.steps_plot > 0:\n        board = Dashboard(args.port)\n\n    for epoch in range(start_epoch, args.num_epochs+1):\n        print(""----- TRAINING - EPOCH"", epoch, ""-----"")\n\n        scheduler.step(epoch)    ## scheduler 2\n\n        epoch_loss = []\n        time_train = []\n     \n        doIouTrain = args.iouTrain   \n        doIouVal =  args.iouVal      \n\n        if (doIouTrain):\n            iouEvalTrain = iouEval(NUM_CLASSES)\n\n        usedLr = 0\n        for param_group in optimizer.param_groups:\n            print(""LEARNING RATE: "", param_group[\'lr\'])\n            usedLr = float(param_group[\'lr\'])\n\n        model.train()\n        for step, (images, labels) in enumerate(loader):\n\n            start_time = time.time()\n            #print (labels.size())\n            #print (np.unique(labels.numpy()))\n            #print(""labels: "", np.unique(labels[0].numpy()))\n            #labels = torch.ones(4, 1, 512, 1024).long()\n            if args.cuda:\n                images = images.cuda()\n                labels = labels.cuda()\n\n            inputs = Variable(images)\n            targets = Variable(labels)\n            outputs = model(inputs, only_encode=enc)\n\n            #print(""targets"", np.unique(targets[:, 0].cpu().data.numpy()))\n\n            optimizer.zero_grad()\n            loss = criterion(outputs, targets[:, 0])\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss.append(loss.data[0])\n            time_train.append(time.time() - start_time)\n\n            if (doIouTrain):\n                #start_time_iou = time.time()\n                iouEvalTrain.addBatch(outputs.max(1)[1].unsqueeze(1).data, targets.data)\n                #print (""Time to add confusion matrix: "", time.time() - start_time_iou)      \n\n            #print(outputs.size())\n            if args.visualize and args.steps_plot > 0 and step % args.steps_plot == 0:\n                start_time_plot = time.time()\n                image = inputs[0].cpu().data\n                #image[0] = image[0] * .229 + .485\n                #image[1] = image[1] * .224 + .456\n                #image[2] = image[2] * .225 + .406\n                #print(""output"", np.unique(outputs[0].cpu().max(0)[1].data.numpy()))\n                board.image(image, f\'input (epoch: {epoch}, step: {step})\')\n                if isinstance(outputs, list):   #merge gpu tensors\n                    board.image(color_transform(outputs[0][0].cpu().max(0)[1].data.unsqueeze(0)),\n                    f\'output (epoch: {epoch}, step: {step})\')\n                else:\n                    board.image(color_transform(outputs[0].cpu().max(0)[1].data.unsqueeze(0)),\n                    f\'output (epoch: {epoch}, step: {step})\')\n                board.image(color_transform(targets[0].cpu().data),\n                    f\'target (epoch: {epoch}, step: {step})\')\n                print (""Time to paint images: "", time.time() - start_time_plot)\n            if args.steps_loss > 0 and step % args.steps_loss == 0:\n                average = sum(epoch_loss) / len(epoch_loss)\n                print(f\'loss: {average:0.4} (epoch: {epoch}, step: {step})\', \n                        ""// Avg time/img: %.4f s"" % (sum(time_train) / len(time_train) / args.batch_size))\n\n            \n        average_epoch_loss_train = sum(epoch_loss) / len(epoch_loss)\n        \n        iouTrain = 0\n        if (doIouTrain):\n            iouTrain, iou_classes = iouEvalTrain.getIoU()\n            iouStr = getColorEntry(iouTrain)+\'{:0.2f}\'.format(iouTrain*100) + \'\\033[0m\'\n            print (""EPOCH IoU on TRAIN set: "", iouStr, ""%"")  \n\n        #Validate on 500 val images after each epoch of training\n        print(""----- VALIDATING - EPOCH"", epoch, ""-----"")\n        model.eval()\n        epoch_loss_val = []\n        time_val = []\n\n        if (doIouVal):\n            iouEvalVal = iouEval(NUM_CLASSES)\n\n        for step, (images, labels) in enumerate(loader_val):\n            start_time = time.time()\n            if args.cuda:\n                images = images.cuda()\n                labels = labels.cuda()\n\n            inputs = Variable(images, volatile=True)    #volatile flag makes it free backward or outputs for eval\n            targets = Variable(labels, volatile=True)\n            outputs = model(inputs, only_encode=enc) \n\n            loss = criterion(outputs, targets[:, 0])\n            epoch_loss_val.append(loss.data[0])\n            time_val.append(time.time() - start_time)\n\n\n            #Add batch to calculate TP, FP and FN for iou estimation\n            if (doIouVal):\n                #start_time_iou = time.time()\n                iouEvalVal.addBatch(outputs.max(1)[1].unsqueeze(1).data, targets.data)\n                #print (""Time to add confusion matrix: "", time.time() - start_time_iou)\n\n            if args.visualize and args.steps_plot > 0 and step % args.steps_plot == 0:\n                start_time_plot = time.time()\n                image = inputs[0].cpu().data\n                board.image(image, f\'VAL input (epoch: {epoch}, step: {step})\')\n                if isinstance(outputs, list):   #merge gpu tensors\n                    board.image(color_transform(outputs[0][0].cpu().max(0)[1].data.unsqueeze(0)),\n                    f\'VAL output (epoch: {epoch}, step: {step})\')\n                else:\n                    board.image(color_transform(outputs[0].cpu().max(0)[1].data.unsqueeze(0)),\n                    f\'VAL output (epoch: {epoch}, step: {step})\')\n                board.image(color_transform(targets[0].cpu().data),\n                    f\'VAL target (epoch: {epoch}, step: {step})\')\n                print (""Time to paint images: "", time.time() - start_time_plot)\n            if args.steps_loss > 0 and step % args.steps_loss == 0:\n                average = sum(epoch_loss_val) / len(epoch_loss_val)\n                print(f\'VAL loss: {average:0.4} (epoch: {epoch}, step: {step})\', \n                        ""// Avg time/img: %.4f s"" % (sum(time_val) / len(time_val) / args.batch_size))\n                       \n\n        average_epoch_loss_val = sum(epoch_loss_val) / len(epoch_loss_val)\n        #scheduler.step(average_epoch_loss_val, epoch)  ## scheduler 1   # update lr if needed\n\n        iouVal = 0\n        if (doIouVal):\n            iouVal, iou_classes = iouEvalVal.getIoU()\n            iouStr = getColorEntry(iouVal)+\'{:0.2f}\'.format(iouVal*100) + \'\\033[0m\'\n            print (""EPOCH IoU on VAL set: "", iouStr, ""%"") \n           \n\n        # remember best valIoU and save checkpoint\n        if iouVal == 0:\n            current_acc = -average_epoch_loss_val\n        else:\n            current_acc = iouVal \n        is_best = current_acc > best_acc\n        best_acc = max(current_acc, best_acc)\n        if enc:\n            filenameCheckpoint = savedir + \'/checkpoint_enc.pth.tar\'\n            filenameBest = savedir + \'/model_best_enc.pth.tar\'    \n        else:\n            filenameCheckpoint = savedir + \'/checkpoint.pth.tar\'\n            filenameBest = savedir + \'/model_best.pth.tar\'\n        save_checkpoint({\n            \'epoch\': epoch + 1,\n            \'arch\': str(model),\n            \'state_dict\': model.state_dict(),\n            \'best_acc\': best_acc,\n            \'optimizer\' : optimizer.state_dict(),\n        }, is_best, filenameCheckpoint, filenameBest)\n\n        #SAVE MODEL AFTER EPOCH\n        if (enc):\n            filename = f\'{savedir}/model_encoder-{epoch:03}.pth\'\n            filenamebest = f\'{savedir}/model_encoder_best.pth\'\n        else:\n            filename = f\'{savedir}/model-{epoch:03}.pth\'\n            filenamebest = f\'{savedir}/model_best.pth\'\n        if args.epochs_save > 0 and step > 0 and step % args.epochs_save == 0:\n            torch.save(model.state_dict(), filename)\n            print(f\'save: {filename} (epoch: {epoch})\')\n        if (is_best):\n            torch.save(model.state_dict(), filenamebest)\n            print(f\'save: {filenamebest} (epoch: {epoch})\')\n            if (not enc):\n                with open(savedir + ""/best.txt"", ""w"") as myfile:\n                    myfile.write(""Best epoch is %d, with Val-IoU= %.4f"" % (epoch, iouVal))   \n            else:\n                with open(savedir + ""/best_encoder.txt"", ""w"") as myfile:\n                    myfile.write(""Best epoch is %d, with Val-IoU= %.4f"" % (epoch, iouVal))           \n\n        #SAVE TO FILE A ROW WITH THE EPOCH RESULT (train loss, val loss, train IoU, val IoU)\n        #Epoch\t\tTrain-loss\t\tTest-loss\tTrain-IoU\tTest-IoU\t\tlearningRate\n        with open(automated_log_path, ""a"") as myfile:\n            myfile.write(""\\n%d\\t\\t%.4f\\t\\t%.4f\\t\\t%.4f\\t\\t%.4f\\t\\t%.8f"" % (epoch, average_epoch_loss_train, average_epoch_loss_val, iouTrain, iouVal, usedLr ))\n    \n    return(model)   #return model (convenience for encoder-decoder training)\n\ndef save_checkpoint(state, is_best, filenameCheckpoint, filenameBest):\n    torch.save(state, filenameCheckpoint)\n    if is_best:\n        print (""Saving model as best"")\n        torch.save(state, filenameBest)\n\n\ndef main(args):\n    savedir = f\'../save/{args.savedir}\'\n\n    if not os.path.exists(savedir):\n        os.makedirs(savedir)\n\n    with open(savedir + \'/opts.txt\', ""w"") as myfile:\n        myfile.write(str(args))\n\n    #Load Model\n    assert os.path.exists(args.model + "".py""), ""Error: model definition not found""\n    model_file = importlib.import_module(args.model)\n    model = model_file.Net(NUM_CLASSES)\n    copyfile(args.model + "".py"", savedir + \'/\' + args.model + "".py"")\n    \n    if args.cuda:\n        model = torch.nn.DataParallel(model).cuda()\n    \n    if args.state:\n        #if args.state is provided then load this state for training\n        #Note: this only loads initialized weights. If you want to resume a training use ""--resume"" option!!\n        """"""\n        try:\n            model.load_state_dict(torch.load(args.state))\n        except AssertionError:\n            model.load_state_dict(torch.load(args.state,\n                map_location=lambda storage, loc: storage))\n        #When model is saved as DataParallel it adds a model. to each key. To remove:\n        #state_dict = {k.partition(\'model.\')[2]: v for k,v in state_dict}\n        #https://discuss.pytorch.org/t/prefix-parameter-names-in-saved-model-if-trained-by-multi-gpu/494\n        """"""\n        def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict keys are there\n            own_state = model.state_dict()\n            for name, param in state_dict.items():\n                if name not in own_state:\n                     continue\n                own_state[name].copy_(param)\n            return model\n\n        #print(torch.load(args.state))\n        model = load_my_state_dict(model, torch.load(args.state))\n\n    """"""\n    def weights_init(m):\n        classname = m.__class__.__name__\n        if classname.find(\'Conv\') != -1:\n            #m.weight.data.normal_(0.0, 0.02)\n            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            m.weight.data.normal_(0, math.sqrt(2. / n))\n        elif classname.find(\'BatchNorm\') != -1:\n            #m.weight.data.normal_(1.0, 0.02)\n            m.weight.data.fill_(1)\n            m.bias.data.fill_(0)\n\n    #TO ACCESS MODEL IN DataParallel: next(model.children())\n    #next(model.children()).decoder.apply(weights_init)\n    #Reinitialize weights for decoder\n    \n    next(model.children()).decoder.layers.apply(weights_init)\n    next(model.children()).decoder.output_conv.apply(weights_init)\n\n    #print(model.state_dict())\n    f = open(\'weights5.txt\', \'w\')\n    f.write(str(model.state_dict()))\n    f.close()\n    """"""\n\n    #train(args, model)\n    if (not args.decoder):\n        print(""========== ENCODER TRAINING ==========="")\n        model = train(args, model, True) #Train encoder\n    #CAREFUL: for some reason, after training encoder alone, the decoder gets weights=0. \n    #We must reinit decoder weights or reload network passing only encoder in order to train decoder\n    print(""========== DECODER TRAINING ==========="")\n    if (not args.state):\n        if args.pretrainedEncoder:\n            print(""Loading encoder pretrained in imagenet"")\n            from erfnet_imagenet import ERFNet as ERFNet_imagenet\n            pretrainedEnc = torch.nn.DataParallel(ERFNet_imagenet(1000))\n            pretrainedEnc.load_state_dict(torch.load(args.pretrainedEncoder)[\'state_dict\'])\n            pretrainedEnc = next(pretrainedEnc.children()).features.encoder\n            if (not args.cuda):\n                pretrainedEnc = pretrainedEnc.cpu()     #because loaded encoder is probably saved in cuda\n        else:\n            pretrainedEnc = next(model.children()).encoder\n        model = model_file.Net(NUM_CLASSES, encoder=pretrainedEnc)  #Add decoder to encoder\n        if args.cuda:\n            model = torch.nn.DataParallel(model).cuda()\n        #When loading encoder reinitialize weights for decoder because they are set to 0 when training dec\n    model = train(args, model, False)   #Train decoder\n    print(""========== TRAINING FINISHED ==========="")\n\nif __name__ == \'__main__\':\n    parser = ArgumentParser()\n    parser.add_argument(\'--cuda\', action=\'store_true\', default=True)  #NOTE: cpu-only has not been tested so you might have to change code if you deactivate this flag\n    parser.add_argument(\'--model\', default=""erfnet"")\n    parser.add_argument(\'--state\')\n\n    parser.add_argument(\'--port\', type=int, default=8097)\n    parser.add_argument(\'--datadir\', default=os.getenv(""HOME"") + ""/datasets/cityscapes/"")\n    parser.add_argument(\'--height\', type=int, default=512)\n    parser.add_argument(\'--num-epochs\', type=int, default=150)\n    parser.add_argument(\'--num-workers\', type=int, default=4)\n    parser.add_argument(\'--batch-size\', type=int, default=6)\n    parser.add_argument(\'--steps-loss\', type=int, default=50)\n    parser.add_argument(\'--steps-plot\', type=int, default=50)\n    parser.add_argument(\'--epochs-save\', type=int, default=0)    #You can use this value to save model every X epochs\n    parser.add_argument(\'--savedir\', required=True)\n    parser.add_argument(\'--decoder\', action=\'store_true\')\n    parser.add_argument(\'--pretrainedEncoder\') #, default=""../trained_models/erfnet_encoder_pretrained.pth.tar"")\n    parser.add_argument(\'--visualize\', action=\'store_true\')\n\n    parser.add_argument(\'--iouTrain\', action=\'store_true\', default=False) #recommended: False (takes more time to train otherwise)\n    parser.add_argument(\'--iouVal\', action=\'store_true\', default=True)  \n    parser.add_argument(\'--resume\', action=\'store_true\')    #Use this flag to load last checkpoint for training  \n\n    main(parser.parse_args())\n'"
train/transform.py,5,"b""import numpy as np\nimport torch\n\nfrom PIL import Image\n\ndef colormap_cityscapes(n):\n    cmap=np.zeros([n, 3]).astype(np.uint8)\n    cmap[0,:] = np.array([128, 64,128])\n    cmap[1,:] = np.array([244, 35,232])\n    cmap[2,:] = np.array([ 70, 70, 70])\n    cmap[3,:] = np.array([ 102,102,156])\n    cmap[4,:] = np.array([ 190,153,153])\n    cmap[5,:] = np.array([ 153,153,153])\n\n    cmap[6,:] = np.array([ 250,170, 30])\n    cmap[7,:] = np.array([ 220,220,  0])\n    cmap[8,:] = np.array([ 107,142, 35])\n    cmap[9,:] = np.array([ 152,251,152])\n    cmap[10,:] = np.array([ 70,130,180])\n\n    cmap[11,:] = np.array([ 220, 20, 60])\n    cmap[12,:] = np.array([ 255,  0,  0])\n    cmap[13,:] = np.array([ 0,  0,142])\n    cmap[14,:] = np.array([  0,  0, 70])\n    cmap[15,:] = np.array([  0, 60,100])\n\n    cmap[16,:] = np.array([  0, 80,100])\n    cmap[17,:] = np.array([  0,  0,230])\n    cmap[18,:] = np.array([ 119, 11, 32])\n    cmap[19,:] = np.array([ 0,  0,  0])\n    \n    return cmap\n\n\ndef colormap(n):\n    cmap=np.zeros([n, 3]).astype(np.uint8)\n\n    for i in np.arange(n):\n        r, g, b = np.zeros(3)\n\n        for j in np.arange(8):\n            r = r + (1<<(7-j))*((i&(1<<(3*j))) >> (3*j))\n            g = g + (1<<(7-j))*((i&(1<<(3*j+1))) >> (3*j+1))\n            b = b + (1<<(7-j))*((i&(1<<(3*j+2))) >> (3*j+2))\n\n        cmap[i,:] = np.array([r, g, b])\n\n    return cmap\n\nclass Relabel:\n\n    def __init__(self, olabel, nlabel):\n        self.olabel = olabel\n        self.nlabel = nlabel\n\n    def __call__(self, tensor):\n        assert (isinstance(tensor, torch.LongTensor) or isinstance(tensor, torch.ByteTensor)) , 'tensor needs to be LongTensor'\n        tensor[tensor == self.olabel] = self.nlabel\n        return tensor\n\n\nclass ToLabel:\n\n    def __call__(self, image):\n        return torch.from_numpy(np.array(image)).long().unsqueeze(0)\n\n\nclass Colorize:\n\n    def __init__(self, n=22):\n        #self.cmap = colormap(256)\n        self.cmap = colormap_cityscapes(256)\n        self.cmap[n] = self.cmap[-1]\n        self.cmap = torch.from_numpy(self.cmap[:n])\n\n    def __call__(self, gray_image):\n        size = gray_image.size()\n        #print(size)\n        color_image = torch.ByteTensor(3, size[1], size[2]).fill_(0)\n        #color_image = torch.ByteTensor(3, size[0], size[1]).fill_(0)\n\n        #for label in range(1, len(self.cmap)):\n        for label in range(0, len(self.cmap)):\n            mask = gray_image[0] == label\n            #mask = gray_image == label\n\n            color_image[0][mask] = self.cmap[label][0]\n            color_image[1][mask] = self.cmap[label][1]\n            color_image[2][mask] = self.cmap[label][2]\n\n        return color_image\n"""
train/visualize.py,1,"b""import numpy as np\n\nfrom torch.autograd import Variable\n\nfrom visdom import Visdom\n\nclass Dashboard:\n\n    def __init__(self, port):\n        self.vis = Visdom(port=port)\n\n    def loss(self, losses, title):\n        x = np.arange(1, len(losses)+1, 1)\n\n        self.vis.line(losses, x, env='loss', opts=dict(title=title))\n\n    def image(self, image, title):\n        if image.is_cuda:\n            image = image.cpu()\n        if isinstance(image, Variable):\n            image = image.data\n        image = image.numpy()\n\n        self.vis.image(image, env='images', opts=dict(title=title))"""
