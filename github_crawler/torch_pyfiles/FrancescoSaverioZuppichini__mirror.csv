file_path,api_count,code
custom_visualisation_example.py,0,"b'from mirror import mirror\nfrom PIL import Image\nfrom torchvision.models import vgg16\nfrom torchvision.transforms import ToTensor, Resize, Compose\nfrom mirror.visualisations.core import Visualisation\nfrom mirror.visualisations.web import WebInterface\nfrom functools import partial\n\nclass RepeatInput(Visualisation):\n\n    def __call__(self, inputs, layer, repeat=1):\n        return inputs.repeat(repeat, 1, 1, 1), None\n\nparams = {\'repeat\' : {\n                     \'type\' : \'slider\',\n                     \'min\' : 1,\n                     \'max\' : 100,\n                     \'value\' : 2,\n                     \'step\': 1,\n                     \'params\': {}\n                 }\n        }\n\nvisualisation = partial(WebInterface.from_visualisation, RepeatInput, params=params, name=\'Visualisation\')\n# create a model\nmodel = vgg16(pretrained=True)\n# open some images\ncat = Image.open(""./cat.jpg"")\ndog_and_cat = Image.open(""./dog_and_cat.jpg"")\n# resize the image and make it a tensor\nto_input = Compose([Resize((224, 224)), ToTensor()])\n# call mirror with the inputs and the model\nmirror([to_input(cat), to_input(dog_and_cat)], model,\n       visualisations=[visualisation])\n'"
example.py,0,"b'from mirror import mirror\nfrom mirror.visualisations.web import *\nfrom PIL import Image\nfrom torchvision.models import resnet101, resnet18, vgg16, alexnet\nfrom torchvision.transforms import ToTensor, Resize, Compose\n\n# create a model\nmodel = vgg16(pretrained=True)\n# open some images\ncat = Image.open(""./cat.jpg"")\ndog_and_cat = Image.open(""./dog_and_cat.jpg"")\n# resize the image and make it a tensor\nto_input = Compose([Resize((224, 224)), ToTensor()])\n# call mirror with the inputs and the model\nmirror([to_input(cat), to_input(dog_and_cat)], model,\n       visualisations=[BackProp, GradCam, DeepDream],\n       port=3001,\n       debug=True)\n'"
setup.py,0,"b'from setuptools import setup, find_packages\n\nsetup(\n\n     name=\'mirror\',\n     version=\'0.0.0\',\n     packages=find_packages(),\n     author=""Francesco Zuppichini"",\n     author_email=""francesco.zuppichini@gmail.com"",\n     description=""A visualization tool for cnn in Pytorch"",\n     include_package_data=True,\n     url=""https://github.com/FrancescoSaverioZuppichini/mirror"",\n )'"
mirror/App.py,1,"b""import json\nimport io\nimport torch\nimport time\n\nfrom flask import Flask, request, Response, send_file, jsonify\nfrom torchvision.transforms import ToPILImage\nfrom .visualisations.web import Weights\nfrom .ModuleTracer import ModuleTracer\n\n\nclass App(Flask):\n    default_visualisations = [Weights]\n    MAX_LINKS_EVERY_REQUEST = 64\n\n    def __init__(self, inputs, model, visualisations=[]):\n        super().__init__(__name__)\n        self.cache = {}  # internal cache used to store the results\n        self.outputs = None  # holds the current output from a visualisation\n        if len(inputs) <= 0: raise ValueError('At least one input is required.')\n\n        self.inputs, self.model = inputs, model\n        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n        self.current_input = self.inputs[0].unsqueeze(0).to(self.device)  # add 1 dim for batch\n        self.module = model.to(self.device).eval()\n        self.setup_tracer()\n        self.setup_visualisations(visualisations)\n\n        @self.route('/')\n        def root():\n            return self.send_static_file('index.html')\n\n        @self.route('/api/model', methods=['GET'])\n        def api_model():\n            model = self.tracer.to_JSON()\n\n            response = jsonify(model)\n\n            return response\n\n        @self.route('/api/inputs', methods=['GET', 'PUT'])\n        def api_inputs():\n            if request.method == 'GET':\n                self.outputs = self.inputs\n\n                response = ['/api/model/image/{}/{}/{}/{}/{}'.format(hash(None),\n                                                                     hash(None),\n                                                                     hash(time.time()),\n                                                                     id,\n                                                                     i) for i in range(len(self.inputs))]\n\n                response = jsonify({'links': response, 'next': False})\n\n            elif request.method == 'PUT':\n                data = json.loads(request.data.decode())\n\n                input_index = data['id']\n\n                self.current_input = self.inputs[input_index].unsqueeze(0).to(self.device)\n\n                response = jsonify(data)\n\n            return response\n\n        @self.route('/api/model/layer/<id>')\n        def api_model_layer(id):\n            id = int(id)\n            name = self.traced[id].name\n\n            return Response(response=name)\n\n        @self.route('/api/visualisation', methods=['GET'])\n        def api_visualisations():\n            serialised = [v.to_JSON() for v in self.visualisations]\n\n            response = jsonify({'visualisations': serialised,\n                                'current': self.current_vis.to_JSON()})\n\n            return response\n\n        @self.route('/api/visualisation', methods=['PUT'])\n        def api_visualisation():\n            data = json.loads(request.data.decode())\n\n            vis_key = data['name']\n            visualisations_not_exist = vis_key not in self.name2visualisations\n\n            if visualisations_not_exist:\n                response = Response(status=500,\n                                    response='Visualisation {} not supported or does not exist'.format(vis_key))\n            else:\n                self.current_vis = self.name2visualisations[vis_key]\n                self.current_vis.from_JSON(data['params'])\n                self.current_vis.clean_cache()\n                response = jsonify(self.current_vis.to_JSON())\n\n            return response\n\n        @self.route('/api/model/layer/output/<id>')\n        def api_model_layer_output(id):\n            try:\n\n                layer = self.traced[id].module\n\n                if self.current_input not in self.current_vis.cache: self.current_vis.cache[self.current_input] = {}\n\n                layer_cache = self.current_vis.cache[self.current_input]\n                # always clone the input to avoid being modified\n                input_clone = self.current_input.clone()\n\n                if layer not in layer_cache:\n                    layer_cache[layer] = self.current_vis(input_clone, layer)\n                    del input_clone\n                else:\n                    print('[INFO] cached')\n\n                self.outputs, _ = layer_cache[layer]\n\n                if len(self.outputs.shape) < 3:  raise ValueError\n\n                last = int(request.args['last'])\n                max = min((last + self.MAX_LINKS_EVERY_REQUEST), self.outputs.shape[0])\n\n                response = ['/api/model/image/{}/{}/{}/{}/{}'.format(hash(self.current_input),\n                                                                     hash(self.current_vis),\n                                                                     hash(time.time()),\n                                                                     id,\n                                                                     i) for i in range(last, max)]\n\n                response = jsonify({'links': response, 'next': last + 1 < max})\n\n\n            except KeyError:\n                response = Response(status=500, response='Index not found.')\n            except ValueError:\n                response = Response(status=404, response='Outputs must be an array of images')\n            except StopIteration:\n                response = jsonify({'links': [], 'next': False})\n\n            return response\n\n        @self.route('/api/model/image/<input_id>/<vis_id>/<layer_id>/<time>/<output_id>')\n        def api_model_layer_output_image(input_id, vis_id, layer_id, time, output_id):\n            output_id = int(output_id)\n            try:\n                output = self.outputs[output_id]\n                output = output.detach().cpu()\n                pil_img = ToPILImage()(output)\n                img_io = io.BytesIO()\n                pil_img.save(img_io, 'JPEG', quality=70)\n                img_io.seek(0)\n                return send_file(img_io, mimetype='image/jpeg')\n            except KeyError:\n                return Response(status=500, response='Index not found.')\n\n    def setup_tracer(self):\n        # instantiate a Tracer object and trace one input\n        self.tracer = ModuleTracer(module=self.module)\n        self.tracer(self.current_input)\n        # store the traced graph as a dictionary\n        self.traced = self.tracer.__dict__()\n\n    def setup_visualisations(self, visualisations):\n        visualisations = [*self.default_visualisations, *visualisations]\n        self.visualisations = [v(self.module, self.device) for v in visualisations]\n        self.name2visualisations = {v.name: v for v in self.visualisations}\n        self.current_vis = self.visualisations[0]\n"""
mirror/ModuleTracer.py,0,"b'import sys\n\n\nclass TracedNode():\n    def __init__(self, parent=None):\n        self.parent = parent\n        self.children = []\n        self.module = None\n        self.input = None\n        self.output = None\n\n    @property\n    def id(self):\n        return str(hash(self.module) % ((sys.maxsize + 1) * 2))\n\n    @property\n    def name(self):\n        return str(self.module)\n\n    def to_JSON(self):\n        return {\'id\': self.id, \'name\': self.name, \'children\': [child.to_JSON() for child in self.children]}\n\n    def __dict__(self):\n        """"""\n        Recursively add items with not children to flat the dictionary.\n        :return:\n        """"""\n        v = {}\n        if len(self.children) <= 0:\n            v = {self.id: self}\n        else:\n            for child in self.children:\n                v = {**v, **child.__dict__()}\n        return v\n\n    def __call__(self, module, input, output):\n        self.module = module\n        self.input = input\n        self.output = output\n\n\nclass ModuleTracer():\n    """"""\n    Hook to each submodule of a module and create a graph of TracedNode in order to store the\n    correct execution of an input through the model.\n    """"""\n\n    def __init__(self, module):\n        super().__init__()\n        self.module = module\n        self.handles = []\n        self.root = None\n\n    def trace(self, module, root=None):\n        """"""\n        Recursively creates a tree of TracedNode\n        :param module: Current module\n        :param root: Parent Node\n        :return: New Node\n        """"""\n        root = TracedNode(parent=root)\n        self.handles.append(module.register_forward_hook(root))\n        for child in module.children():\n            root.children.append(self.trace(child, root))\n        return root\n\n    def __call__(self, x):\n        self.root = self.trace(self.module, None)\n        _ = self.module(x)\n        self.clean()\n        return self\n\n    def __repr__(self):\n        return self.to_JSON().__repr__()\n\n    def clean(self):\n        [h.remove() for h in self.handles]\n\n    def __dict__(self):\n        return self.root.__dict__()\n\n    def to_JSON(self):\n        return self.root.to_JSON()\n'"
mirror/__init__.py,0,b'from .main import mirror'
mirror/main.py,0,"b'import webbrowser\n\nfrom .App import App\n\n\ndef mirror(input, model, visualisations=[], port=5000, debug=False):\n    app = App(input, model, visualisations)\n    if not debug: webbrowser.open_new(f\'http://localhost:{port}\')  # opens in default browser\n    app.run(host=""0.0.0.0"", port=port,  use_reloader=debug)\n\n'"
mirror/utils.py,0,"b""from collections import OrderedDict\nclass EnsureType(OrderedDict):\n    def __init__(self, *args, type=str, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.type = type\n\n    def __getitem__(self, item):\n        val = self.get(item)\n        if item is 'value':\n            val = self.type(self.get(item))\n        return val"""
test/ModuleTracerTest.py,2,"b'import unittest\nimport torch.nn as nn\nimport pprint\nimport torch\nfrom mirror.ModuleTracer import ModuleTracer\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n\n        self.linear = nn.Sequential(\n            nn.Conv2d(32, 32, kernel_size=1),\n            nn.BatchNorm2d(32)\n        )\n\n        self.conv = nn.Sequential(nn.Conv2d(3, 32, kernel_size=3),\n                                  nn.ReLU())\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.linear(x)\n        return x\n#\n\n\nclass ModuleTracerTest(unittest.TestCase):\n    def test(self):\n        module = MyModule()\n        tracer = ModuleTracer(module)\n        x = torch.ones((1,3,28,28))\n        tracer(x)\n        # tracer_dict = tracer.__dict__()\n        pprint.pprint(tracer.to_JSON())\n#         TODO test it properly\n\n\n'"
test/WebInterface.py,0,"b""import unittest\nfrom mirror.visualisations.core import Visualisation\nfrom mirror.visualisations.web import WebInterface\nfrom functools import partial\n\n\nclass ModuleTracerTest(unittest.TestCase):\n    def test(self):\n        class TestVisualisation(Visualisation):\n\n            def __call__(self, inputs, layer, something=1, *args, **kwargs):\n                self.something = something\n                return None\n\n        params = {'something': {\n            'type': 'slider',\n            'min': 1,\n            'max': 100,\n            'value': 1,\n            'step': 1,\n            'params': {}\n        }\n        }\n\n        TestVisWeb = partial(WebInterface.from_visualisation, TestVisualisation,\n                                       params=params,\n                                       name='something')\n        test_vis_web = TestVisWeb(None, None)\n\n        test_vis_web(None, None)\n\n        self.assertEqual(test_vis_web.callable.something,  1)\n\n        state = test_vis_web.to_JSON()\n        state['params']['something']['value'] = 4 #simulate change in the front end\n\n        test_vis_web.from_JSON(state)\n\n        self.assertEqual(test_vis_web.callable.something,  1)\n\n\n"""
test/__init__.py,0,b''
test/example.py,0,"b'from mirror.visualisations.core import Visualisation\nfrom mirror.visualisations.web import WebInterface\nfrom functools import partial\n\n\nclass RepeatInput(Visualisation):\n\n    def __call__(self, inputs, layer, repeat=1, *args, **kwargs):\n        return inputs.repeat(repeat, 1, 1, 1), None\n\n\nparams = {\'repeat\' : {\n                     \'type\' : \'slider\',\n                     \'min\' : 1,\n                     \'max\' : 100,\n                     \'value\' : 2,\n                     \'step\': 1,\n                     \'params\': {}\n                 }\n        }\n\n\nRepeatInput = partial(WebInterface.from_visualisation, RepeatInput, params=params, name=\'repeat\')\n\n# params = {\'repeat\' : {\n#                      \'type\' : \'slider\',\n#                      \'min\' : 1,\n#                      \'max\' : 100,\n#                      \'value\' : 2,\n#                      \'step\': 1,\n#                      \'params\': {}\n#                  }\n#         }\n#\n# vis = RepeatInput(params, \'repeat\')\n# print(vis.to_JSON())\n# print(vis.from_JSON({ \'repeat\' : { \'value\' : 2}}))\n# print(vis.to_JSON())\n\nfrom mirror import mirror\nfrom PIL import Image\nfrom torchvision.models import vgg16\nfrom torchvision.transforms import ToTensor, Resize, Compose\n\n# create a model\nmodel = vgg16(pretrained=True)\n# open some images\ncat = Image.open(""../cat.jpg"")\ndog_and_cat = Image.open(""../dog_and_cat.jpg"")\n# resize the image and make it a tensor\nto_input = Compose([Resize((224, 224)), ToTensor()])\n# call mirror with the inputs and the model\nmirror([to_input(cat), to_input(dog_and_cat)], model, visualisations=[RepeatInput])\n'"
mirror/visualisations/__init__.py,0,b''
mirror/visualisations/buttons.py,0,"b""class Radio():\n    def __init__(self, value=False):\n        self.value = value\n        self.type = 'radio'\n\n\nclass TextField():\n    def __init__(self, value=None, label=''):\n        self.value = value\n        self.label = label\n        self.type = 'textfield'\n\n\nclass Slider():\n    def __init__(self, value=None, min = 0, max = 1, step=0.1):\n        self.value = min if value is None else value\n        self.min, self.max, self.step = min, max, step\n        self.type = 'slider'\n"""
mirror/visualisations/core/ClassActivationMapping.py,5,"b'import torch\n\nfrom torch.nn import AvgPool2d, Conv2d, Linear, ReLU\nfrom torch.nn.functional import softmax\n\nfrom .Visualisation import Visualisation\n\nfrom .utils import module2traced, imshow, tensor2cam\n\nimport torch.nn.functional as F\n\n\nclass ClassActivationMapping(Visualisation):\n    """"""\n    Based on Learning Deep Features for Discriminative Localization (https://arxiv.org/abs/1512.04150).\n    Be aware,it requires feature maps to directly precede softmax layers.\n    It will work for resnet but not for alexnet for example\n    """"""\n\n    def hook(self, module, inputs, outputs):\n        self.conv_outputs = outputs\n\n    def __call__(self, inputs, layer, target_class=None, postprocessing=lambda x: x, guide=False):\n        modules = module2traced(self.module, inputs)\n        last_conv = None\n        last_linear = None\n        # TODO create a function in utils called like get_last_of(nn.Conv2d)\n        for i, module in enumerate(modules):\n            if isinstance(module, Conv2d):\n                last_conv = module\n            if isinstance(module, AvgPool2d):\n                pass\n            if isinstance(module, Linear):\n                last_linear = module\n\n        last_conv.register_forward_hook(self.hook)\n\n        predictions = self.module(inputs)\n\n        if target_class == None: _, target_class = torch.max(predictions, dim=1)\n        _, c, h, w = self.conv_outputs.shape\n        # get the weights relative to the target class\n        fc_weights_class = last_linear.weight.data[target_class]\n        # sum up the multiplication of each weight w_k for the relative channel in the last\n        # convolution output\n        cam = fc_weights_class @ self.conv_outputs.view((c, h * w))\n        cam = cam.view(h, w)\n\n        with torch.no_grad():\n            image_with_heatmap = tensor2cam(postprocessing(inputs.squeeze()), cam)\n\n        return image_with_heatmap.unsqueeze(0), {\'prediction\': target_class}\n'"
mirror/visualisations/core/DeepDream.py,4,"b""import torch\nimport torchvision.transforms.functional as TF\n\nfrom torch.autograd import Variable\nfrom PIL import Image, ImageFilter, ImageChops\nfrom .Visualisation import Visualisation\nfrom .utils import image_net_postprocessing, \\\n    image_net_preprocessing\n\nclass DeepDream(Visualisation):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.handle = None\n\n    def register_hooks(self):\n        if self.handle: self.handle.remove()\n\n        def hook(module, input, output):\n            if module == self.layer:\n                self.layer_output = output\n\n                self.optimizer.zero_grad()\n                loss = - torch.norm(self.layer_output)\n                loss.backward()\n                self.optimizer.step()\n\n                raise Exception('Layer found!')\n\n        return self.layer.register_forward_hook(hook)\n\n    def step(self, image, steps=5, save=False):\n\n        self.module.zero_grad()\n        image_pre = image_net_preprocessing(image.squeeze(0).cpu()).to(self.device).unsqueeze(0)\n        self.image_var = Variable(image_pre, requires_grad=True).to(self.device)\n\n        self.optimizer = torch.optim.Adam([self.image_var], lr=self.lr)\n\n        for i in range(steps):\n            try:\n                self.module(self.image_var)\n            except:\n                pass\n\n        dreamed = self.image_var.data.squeeze(0)\n        c, w, h = dreamed.shape\n\n        # dreamed = dreamed.view((w, h, c))\n        dreamed = image_net_postprocessing(dreamed.cpu()).to(self.device)\n        # dreamed = dreamed * self.std + self.mean\n        dreamed = torch.clamp(dreamed, 0.0, 1.0)\n        # dreamed = dreamed.view((c, w, h))\n\n        del self.image_var, image_pre\n\n        return dreamed\n\n    def deep_dream(self, image, n, top, scale):\n        if n > 0:\n            b, c, w, h = image.shape\n\n            image = TF.to_pil_image(image.squeeze(0).cpu())\n            image_down = TF.resize(image, (int(w * scale), int(h * scale)), Image.ANTIALIAS)\n            image_down = image_down.filter(ImageFilter.GaussianBlur(0.5))\n\n            image_down = TF.to_tensor(image_down).unsqueeze(0)\n            from_down = self.deep_dream(image_down, n - 1, top, scale)\n\n            from_down = TF.to_pil_image(from_down.squeeze(0).cpu())\n            from_down = TF.resize(from_down, (w, h), Image.ANTIALIAS)\n\n            image = ImageChops.blend(from_down, image, 0.6)\n\n            image = TF.to_tensor(image).to(self.device)\n        n = n - 1\n\n        return self.step(image, steps=8, save=top == n + 1)\n\n    def __call__(self, inputs, layer, octaves=6, scale=0.7, lr=0.1):\n        self.layer, self.lr = layer, lr\n        self.handle = self.register_hooks()\n        self.module.zero_grad()\n\n        dd = self.deep_dream(inputs, octaves,\n                             top=octaves,\n                             scale=scale)\n        self.handle.remove()\n\n        return dd.unsqueeze(0), {}\n"""
mirror/visualisations/core/GradCam.py,9,"b""import cv2\nimport numpy as np\nimport torch\n\nfrom torch.nn import ReLU\nfrom torch.autograd import Variable\nfrom .Visualisation import Visualisation\nfrom torch.nn import AvgPool2d, Conv2d, Linear, ReLU, MaxPool2d, BatchNorm2d\nimport torch.nn.functional as F\n\nfrom .utils import tensor2cam, module2traced, imshow\n\nclass GradCam(Visualisation):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.handles = []\n        self.gradients = None\n        self.conv_outputs = None\n\n    def store_outputs_and_grad(self, layer):\n        def store_grads(module, grad_in, grad_out):\n            self.gradients = grad_out[0]\n\n        def store_outputs(module, input, outputs):\n            if module == layer:\n                self.conv_outputs = outputs\n\n        self.handles.append(layer.register_forward_hook(store_outputs))\n        self.handles.append(layer.register_backward_hook(store_grads))\n\n    def guide(self, module):\n        def guide_relu(module, grad_in, grad_out):\n            return (torch.clamp(grad_out[0], min=0.0),)\n\n        for module in module.modules():\n            if isinstance(module, ReLU):\n                self.handles.append(module.register_backward_hook(guide_relu))\n\n    def __call__(self, input_image, layer, guide=False, target_class=None, postprocessing=lambda x: x):\n        self.clean()\n        self.module.zero_grad()\n\n        if layer is None:\n            modules = module2traced(self.module, input_image)\n            for i, module in enumerate(modules):\n                if isinstance(module, Conv2d):\n                    layer = module\n\n        self.store_outputs_and_grad(layer)\n\n        if guide: self.guide(self.module)\n\n        input_var = Variable(input_image, requires_grad=True).to(self.device)\n        predictions = self.module(input_var)\n\n        if target_class is None: values, target_class = torch.max(predictions, dim=1)\n\n        target = torch.zeros(predictions.size()).to(self.device)\n        target[0][int(target_class)] = 1\n\n        predictions.backward(gradient=target, retain_graph=True)\n\n        with torch.no_grad():\n            avg_channel_grad = F.adaptive_avg_pool2d(self.gradients.data, 1)\n            cam = F.relu(torch.sum(self.conv_outputs[0] * avg_channel_grad[0], dim=0))\n            image_with_heatmap = tensor2cam(postprocessing(input_image.squeeze(0)), cam)\n\n        self.clean()\n\n        return image_with_heatmap.unsqueeze(0), { 'prediction': target_class, 'cam' : cam }\n\n\n"""
mirror/visualisations/core/SaliencyMap.py,6,"b'import torch\n\nfrom .Visualisation import Visualisation\nfrom torch.nn import ReLU\nfrom torch.autograd import Variable\nfrom torchvision.transforms import *\nfrom .utils import convert_to_grayscale\n\nclass SaliencyMap(Visualisation):\n    """"""\n    Simonyan, Vedaldi, and Zisserman, \xe2\x80\x9cDeep Inside Convolutional Networks: Visualising Image Classification Models\n    and Saliency Maps\xe2\x80\x9d, ICLR Workshop 2014\n    https://arxiv.org/abs/1312.6034\n    """"""\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.gradients = None\n        self.handles = []\n        self.stored_grad = False\n\n    def store_first_layer_grad(self):\n\n        def hook_grad_input(module, inputs, outputs):\n            # stored only for the first time -> first layer\n            if not self.stored_grad:\n                self.handles.append(module.register_backward_hook(store_grad))\n                self.stored_grad = True\n\n        def store_grad(module, grad_in, grad_out):\n            self.gradients = grad_in[0]\n\n        for module in self.module.modules():\n            self.handles.append(module.register_forward_hook(hook_grad_input))\n\n    def guide(self, module):\n        def guide_relu(module, grad_in, grad_out):\n            return (torch.clamp(grad_in[0], min=0.0),)\n\n        for module in module.modules():\n            if isinstance(module, ReLU):\n                self.handles.append(module.register_backward_hook(guide_relu))\n\n    def __call__(self, input_image, layer, guide=False, target_class=None):\n        self.stored_grad = False\n\n        self.clean()\n        if guide: self.guide(self.module)\n\n        input_image = Variable(input_image, requires_grad=True).to(self.device)\n\n        self.store_first_layer_grad()\n\n        predictions = self.module(input_image)\n\n        if target_class == None: _, target_class = torch.max(predictions, dim=1)\n\n        one_hot_output = torch.zeros(predictions.size()).to(self.device)\n        one_hot_output[0][int(target_class)] = 1\n\n        self.module.zero_grad()\n\n        predictions.backward(gradient=one_hot_output)\n\n        image = self.gradients.data.cpu().numpy()[0]\n\n        image = convert_to_grayscale(image)\n        image = torch.from_numpy(image).to(self.device)\n\n        self.clean()\n\n        return image.unsqueeze(0), { \'prediction\': target_class }\n\n'"
mirror/visualisations/core/Visualisation.py,0,"b'class Visualisation:\n    def __init__(self, module, device):\n        self.module, self.device = module, device\n        self.handles = []\n\n    def clean(self):\n        [h.remove() for h in self.handles]\n\n\n    def __call__(self, inputs, layer, *args, **kwargs):\n        return inputs, {}\n'"
mirror/visualisations/core/Weights.py,0,"b'from .Visualisation import Visualisation\n\nclass Weights(Visualisation):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.outputs = None\n\n    def hook(self, module, input, output):\n        self.outputs = output\n\n    def __call__(self, inputs, layer, *args, **kwargs):\n        self.handles.append(layer.register_forward_hook(self.hook))\n        self.module(inputs)\n        self.clean()\n        b, c, h, w = self.outputs.shape\n        # reshape to make an array of images 1-Channel\n        outputs = self.outputs.view(c, b, h, w)\n        return outputs, {}\n'"
mirror/visualisations/core/__init__.py,0,b'from .SaliencyMap import SaliencyMap\nfrom .DeepDream import DeepDream\nfrom .GradCam import GradCam\nfrom .Weights import Weights\nfrom .Visualisation import Visualisation\nfrom .ClassActivationMapping import ClassActivationMapping\n'
mirror/visualisations/core/utils.py,6,"b'import torch\nimport numpy as np\nimport cv2\n\nfrom torchvision.transforms import Compose, Normalize\n\ndevice = torch.device(\'cuda:0\' if torch.cuda.is_available() else \'cpu\')\n\nimage_net_mean = torch.Tensor([0.485, 0.456, 0.406])\nimage_net_std = torch.Tensor([0.229, 0.224, 0.225])\n\nimport matplotlib.pyplot as plt\n\n\nclass NormalizeInverse(Normalize):\n    """"""\n    Undoes the normalization and returns the reconstructed images in the input domain.\n    """"""\n\n    def __init__(self, mean, std):\n        mean = torch.Tensor(mean)\n        std = torch.Tensor(std)\n        std_inv = 1 / (std + 1e-7)\n        mean_inv = -mean * std_inv\n        super().__init__(mean=mean_inv, std=std_inv)\n\n    def __call__(self, tensor):\n        return super().__call__(tensor.clone())\n\n\nimage_net_preprocessing = Compose([\n    Normalize(\n        mean=image_net_mean,\n        std=image_net_std\n    )\n])\n\nimage_net_postprocessing = Compose([\n    NormalizeInverse(\n        mean=image_net_mean,\n        std=image_net_std)\n])\n\n\ndef tensor2cam(image, cam):\n    image_with_heatmap = image2cam(image.permute(1, 2, 0).cpu().numpy(),\n                                   cam.detach().cpu().numpy())\n\n    return torch.from_numpy(image_with_heatmap).permute(2, 0, 1)\n\n\ndef image2cam(image, cam):\n    h, w, c = image.shape\n\n    cam -= np.min(cam)\n    cam /= np.max(cam)  # Normalize between 0-1\n    cam = cv2.resize(cam, (h, w))\n    cam = np.uint8(cam * 255.0)\n\n    img_with_cam = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n    img_with_cam = cv2.cvtColor(img_with_cam, cv2.COLOR_BGR2RGB)\n    img_with_cam = img_with_cam + (image * 255)\n    img_with_cam /= np.max(img_with_cam)\n\n    return img_with_cam\n\n\ndef convert_to_grayscale(cv2im):\n    """"""\n        Converts 3d image to grayscale\n\n    Args:\n        cv2im (numpy arr): RGB image with shape (D,W,H)\n\n    returns:\n        grayscale_im (numpy_arr): Grayscale image with shape (1,W,D)\n\n    credits to https://github.com/utkuozbulak/pytorch-cnn-visualizations\n    """"""\n    grayscale_im = np.sum(np.abs(cv2im), axis=0)\n    im_max = np.percentile(grayscale_im, 99)\n    im_min = np.min(grayscale_im)\n    grayscale_im = (np.clip((grayscale_im - im_min) / (im_max - im_min), 0, 1))\n    grayscale_im = np.expand_dims(grayscale_im, axis=0)\n    return grayscale_im\n\n\ndef imshow(tensor):\n    tensor = tensor.squeeze()\n    if len(tensor.shape) > 2: tensor = tensor.permute(1, 2, 0)\n    img = tensor.cpu().numpy()\n    plt.imshow(img, cmap=\'gray\')\n    plt.show()\n\n\ndef module2traced(module, inputs):\n    handles, modules = [], []\n\n    def trace(module, inputs, outputs):\n        modules.append(module)\n\n    def traverse(module):\n        for m in module.children():\n            traverse(m)\n        is_leaf = len(list(module.children())) == 0\n        if is_leaf: handles.append(module.register_forward_hook(trace))\n\n    traverse(module)\n\n    _ = module(inputs)\n\n    [h.remove() for h in handles]\n\n    return modules\n\n\nclass Hook():\n    def __init__(self):\n        self.handler = None\n        self.module, self.inputs, self.outputs = None, None, None\n\n    def clean(self):\n        if self.handler is not None: self.handler.remove()\n\n    def register_forward_hook(self, module):\n        self.clean()\n        self.handler = module.register_forward_hook(self)\n\n    def register_backward_hook(self, module):\n        self.clean()\n        self.handler = module.register_forward_hook(self)\n\n    def __call__(self, module, inputs, outputs):\n        self.module = module\n        self.inputs = inputs\n        self.outputs = outputs\n'"
mirror/visualisations/web/WebInterface.py,0,"b""import json\n\nclass WebInterface():\n    def __init__(self, callable, params, name):\n        self.callable = callable\n        self.params = params\n        self.name = name\n        self.outputs = []\n        self.cache = {}\n\n    def __call__(self, input_image, layer):\n        if self.callable is None: raise ValueError(\n            'You need to override this class and provide a visualisation in the field .visualisation.')\n        return self.callable(input_image, layer, **self.call_params)\n\n    @property\n    def call_params(self):\n        return {k: v['value'] for k, v in self.params.items()}\n\n    def to_JSON(self):\n        return {'name': self.name, 'params': self.params}\n\n    def from_JSON(self, data):\n        self.params = data\n\n    def clean_cache(self):\n        self.cache = {}\n\n    @classmethod\n    def from_visualisation(cls, visualisation, module, device, *args, **kwargs):\n        return cls(visualisation(module, device), *args, **kwargs)\n\n    def __repr__(self):\n        return str(self.call_params)"""
mirror/visualisations/web/__init__.py,0,b'from .interfaces import *\nfrom .WebInterface import WebInterface'
mirror/visualisations/web/interfaces.py,0,"b""from mirror.visualisations.web.WebInterface import WebInterface\nfrom mirror.visualisations.core import *\nfrom functools import partial\n\nclass_parameters = lambda: {'guide': {'type': 'radio',\n                                      'value': False\n                                      },\n                            'target_class': {\n                                'type': 'textfield',\n                                'label': 'id',\n                                'value': None }\n                            }\n\nWeights = partial(WebInterface.from_visualisation, Weights, params={}, name='Weights')\nBackProp = partial(WebInterface.from_visualisation, SaliencyMap, params=class_parameters(), name='Back Prop')\nGradCam = partial(WebInterface.from_visualisation, GradCam, params=class_parameters(), name='Grad CAM')\n\ndeep_dream_params = {'lr':\n    {\n        'type': 'slider',\n        'min': 0.001,\n        'max': 1,\n        'value': 0.1,\n        'step': 0.001,\n        'params': {}\n    },\n    'octaves': {\n        'type': 'slider',\n        'min': 1,\n        'max': 10,\n        'value': 4,\n        'step': 1,\n        'params': {}\n    },\n    'scale': {\n        'type': 'slider',\n        'min': 0.1,\n        'max': 1,\n        'value': 0.7,\n        'step': 0.1,\n        'params': {}\n    }\n}\n\nDeepDream = partial(WebInterface.from_visualisation, DeepDream, params=deep_dream_params, name='Deep Dream')\n"""
