file_path,api_count,code
code/laboratory.py,4,"b""import numpy as np\nfrom glob import glob\nimport os\nfrom datasets.datasetgetter import get_dataset\nimport argparse\nimport torchvision.utils as vutils\nimport torch.nn.functional as F\nimport torch\nfrom models.generator import Generator\nfrom models.discriminator import Discriminator\n\nparser = argparse.ArgumentParser(description='PyTorch Simultaneous Training')\nparser.add_argument('--data_dir', default='../data/', help='path to dataset')\nparser.add_argument('--model_name', type=str, default='SinGAN', help='model name')\nparser.add_argument('--workers', default=8, type=int, help='number of data loading workers (default: 8)')\nparser.add_argument('--epochs', default=80, type=int, help='number of total epochs to run')\nparser.add_argument('--start_epoch', default=0, type=int, help='manual epoch number (useful on restarts)')\nparser.add_argument('--batch_size', default=1, type=int,\n                    help='Total batch size - e.g) num_gpus = 2 , batch_size = 128 then, effectively, 64')\nparser.add_argument('--val_batch', default=1, type=int)\nparser.add_argument('--img_size_max', default=250, type=int, help='Input image size')\nparser.add_argument('--img_size_min', default=25, type=int, help='Input image size')\nparser.add_argument('--log_step', default=50, type=int, help='print frequency (default: 50)')\nparser.add_argument('--load_model', default=None, type=str, metavar='PATH',\n                    help='path to latest checkpoint (default: None)')\nparser.add_argument('--validation', dest='validation', action='store_true',\n                    help='evaluate model on validation set')\nparser.add_argument('--test', dest='test', action='store_true',\n                    help='test model on validation set')\nparser.add_argument('--world-size', default=1, type=int,\n                    help='number of nodes for distributed training')\nparser.add_argument('--rank', default=0, type=int,\n                    help='node rank for distributed training')\nparser.add_argument('--gpu', default=None, type=str,\n                    help='GPU id to use.')\nparser.add_argument('--multiprocessing-distributed', action='store_true',\n                    help='Use multi-processing distributed training to launch '\n                         'N processes per node, which has N GPUs. This is the '\n                         'fastest way to use PyTorch for either single node or '\n                         'multi node data parallel training')\nparser.add_argument('--port', default='8888', type=str)\n\n\nargs = parser.parse_args()\n\n# large = 250\n# small = 25\n#\n# scale = large / small\n#\n# # scale = (4/3)^N / N = log_{4/3}(scale) = log(scale)/log(4/3)\n#\n# num_scale = int(np.round(np.log(scale) / np.log(4/3)))\n#\n# N = int(np.log(scale) / np.log(4/3))\n#\n# train_, _ = get_dataset('photo', args)\n#\n# trainiter = iter(train_)\n#\n# x = next(trainiter)\n#\n# x = x.unsqueeze(0)\n#\n#\n# size_list = [int(args.img_size_min * (4/3)**i) for i in range(num_scale + 1)]\n# print(size_list)\n# G = Generator(args.img_size_min, num_scale)\n# D = Discriminator()\n#\n# z_list = [F.pad(torch.randn(args.batch_size, 3, size_list[i], size_list[i]), [5, 5, 5, 5], value=-1) for i in range(num_scale + 1)]\n#\n# print('latent vector sizes')\n# for z in z_list:\n#     print(z.shape)\n# print('-------------------')\n#\n# for i in range(num_scale + 1):\n#     print('output sizes')\n#     out = G(z_list)\n#     for o in out:\n#         print(o.shape)\n#     d_out = torch.mean(D(out[-1]), (2, 3))\n#     print(d_out.shape)\n#     G.progress()\n#     D.progress()\n#     print(G.current_scale)\n#     print(D.current_scale)\n#     print('-------------------')\n#\n# print('-------------------')\n#\n# for key, val in G.sub_generators[0].named_parameters():\n#     val.requires_grad = False\n#\n# print('-------------------')\n#\n# for i in range(len(out)):\n#     vutils.save_image(out[i], 'tmp{}.png'.format(i), 1, normalize=True)\n#\n\nx1 = torch.rand(1, 1, 1, 1)\n\nprint(x1)"""
code/main.py,38,"b'import argparse\nimport warnings\nfrom datetime import datetime\nfrom glob import glob\nfrom shutil import copyfile\nfrom datasets.datasetgetter import get_dataset\n\nimport torch\nfrom torch import autograd\nfrom torch.nn import functional as F\nimport torch.nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nimport torch.optim\nimport torch.multiprocessing as mp\nimport torch.utils.data\nimport torch.utils.data.distributed\n\nfrom models.generator import Generator\nfrom models.discriminator import Discriminator\n\nfrom train import *\nfrom validation import *\n\nfrom utils import *\n\nparser = argparse.ArgumentParser(description=\'PyTorch Simultaneous Training\')\nparser.add_argument(\'--data_dir\', default=\'../data/\', help=\'path to dataset\')\nparser.add_argument(\'--dataset\', default=\'PHOTO\', help=\'type of dataset\', choices=[\'PHOTO\'])\nparser.add_argument(\'--gantype\', default=\'zerogp\', help=\'type of GAN loss\', choices=[\'wgangp\', \'zerogp\', \'lsgan\'])\nparser.add_argument(\'--model_name\', type=str, default=\'SinGAN\', help=\'model name\')\nparser.add_argument(\'--workers\', default=8, type=int, help=\'number of data loading workers (default: 8)\')\nparser.add_argument(\'--batch_size\', default=1, type=int,\n                    help=\'Total batch size - e.g) num_gpus = 2 , batch_size = 128 then, effectively, 64\')\nparser.add_argument(\'--val_batch\', default=1, type=int)\nparser.add_argument(\'--img_size_max\', default=250, type=int, help=\'Input image size\')\nparser.add_argument(\'--img_size_min\', default=25, type=int, help=\'Input image size\')\nparser.add_argument(\'--img_to_use\', default=-999, type=int, help=\'Index of the input image to use < 6287\')\nparser.add_argument(\'--load_model\', default=None, type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: None)\')\nparser.add_argument(\'--validation\', dest=\'validation\', action=\'store_true\',\n                    help=\'evaluate model on validation set\')\nparser.add_argument(\'--test\', dest=\'test\', action=\'store_true\',\n                    help=\'test model on validation set\')\nparser.add_argument(\'--world-size\', default=1, type=int,\n                    help=\'number of nodes for distributed training\')\nparser.add_argument(\'--rank\', default=0, type=int,\n                    help=\'node rank for distributed training\')\nparser.add_argument(\'--gpu\', default=None, type=str,\n                    help=\'GPU id to use.\')\nparser.add_argument(\'--multiprocessing-distributed\', action=\'store_true\',\n                    help=\'Use multi-processing distributed training to launch \'\n                         \'N processes per node, which has N GPUs. This is the \'\n                         \'fastest way to use PyTorch for either single node or \'\n                         \'multi node data parallel training\')\nparser.add_argument(\'--port\', default=\'8888\', type=str)\n\n\ndef main():\n    args = parser.parse_args()\n\n    if args.gpu is not None:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n        warnings.warn(\'You have chosen a specific GPU. This will completely \'\n                      \'disable data parallelism.\')\n\n    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n\n    ngpus_per_node = torch.cuda.device_count()\n\n    if args.load_model is None:\n        args.model_name = \'{}_{}\'.format(args.model_name, datetime.now().strftime(""%Y-%m-%d_%H-%M-%S""))\n    else:\n        args.model_name = args.load_model\n\n    makedirs(\'./logs\')\n    makedirs(\'./results\')\n\n    args.log_dir = os.path.join(\'./logs\', args.model_name)\n    args.res_dir = os.path.join(\'./results\', args.model_name)\n\n    makedirs(args.log_dir)\n    makedirs(os.path.join(args.log_dir, \'codes\'))\n    makedirs(os.path.join(args.log_dir, \'codes\', \'models\'))\n    makedirs(args.res_dir)\n\n    if args.load_model is None:\n        pyfiles = glob(""./*.py"")\n        modelfiles = glob(\'./models/*.py\')\n        for py in pyfiles:\n            copyfile(py, os.path.join(args.log_dir, \'codes\') + ""/"" + py)\n        for py in modelfiles:\n            copyfile(py, os.path.join(args.log_dir, \'codes\', py[2:]))\n\n    formatted_print(\'Total Number of GPUs:\', ngpus_per_node)\n    formatted_print(\'Total Number of Workers:\', args.workers)\n    formatted_print(\'Batch Size:\', args.batch_size)\n    formatted_print(\'Max image Size:\', args.img_size_max)\n    formatted_print(\'Min image Size:\', args.img_size_min)\n    formatted_print(\'Log DIR:\', args.log_dir)\n    formatted_print(\'Result DIR:\', args.res_dir)\n    formatted_print(\'GAN TYPE:\', args.gantype)\n\n    if args.multiprocessing_distributed:\n        args.world_size = ngpus_per_node * args.world_size\n        mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))\n    else:\n        main_worker(args.gpu, ngpus_per_node, args)\n\n\ndef main_worker(gpu, ngpus_per_node, args):\n    if len(args.gpu) == 1:\n        args.gpu = 0\n    else:\n        args.gpu = gpu\n\n    if args.gpu is not None:\n        print(""Use GPU: {} for training"".format(args.gpu))\n\n    if args.distributed:\n        if args.multiprocessing_distributed:\n            args.rank = args.rank * ngpus_per_node + gpu\n        dist.init_process_group(backend=\'nccl\', init_method=\'tcp://127.0.0.1:\'+args.port,\n                                world_size=args.world_size, rank=args.rank)\n\n    ################\n    # Define model #\n    ################\n    # 4/3 : scale factor in the paper\n    scale_factor = 4/3\n    tmp_scale = args.img_size_max / args.img_size_min\n    args.num_scale = int(np.round(np.log(tmp_scale) / np.log(scale_factor)))\n    args.size_list = [int(args.img_size_min * scale_factor**i) for i in range(args.num_scale + 1)]\n\n    discriminator = Discriminator()\n    generator = Generator(args.img_size_min, args.num_scale, scale_factor)\n\n    networks = [discriminator, generator]\n\n    if args.distributed:\n        if args.gpu is not None:\n            print(\'Distributed to\', args.gpu)\n            torch.cuda.set_device(args.gpu)\n            networks = [x.cuda(args.gpu) for x in networks]\n            args.batch_size = int(args.batch_size / ngpus_per_node)\n            args.workers = int(args.workers / ngpus_per_node)\n            networks = [torch.nn.parallel.DistributedDataParallel(x, device_ids=[args.gpu], output_device=args.gpu) for x in networks]\n        else:\n            networks = [x.cuda() for x in networks]\n            networks = [torch.nn.parallel.DistributedDataParallel(x) for x in networks]\n\n    elif args.gpu is not None:\n        torch.cuda.set_device(args.gpu)\n        networks = [x.cuda(args.gpu) for x in networks]\n    else:\n        networks = [torch.nn.DataParallel(x).cuda() for x in networks]\n\n    discriminator, generator, = networks\n\n    ######################\n    # Loss and Optimizer #\n    ######################\n    if args.distributed:\n        d_opt = torch.optim.Adam(discriminator.module.sub_discriminators[0].parameters(), 5e-4, (0.5, 0.999))\n        g_opt = torch.optim.Adam(generator.module.sub_generators[0].parameters(), 5e-4, (0.5, 0.999))\n    else:\n        d_opt = torch.optim.Adam(discriminator.sub_discriminators[0].parameters(), 5e-4, (0.5, 0.999))\n        g_opt = torch.optim.Adam(generator.sub_generators[0].parameters(), 5e-4, (0.5, 0.999))\n\n    ##############\n    # Load model #\n    ##############\n    args.stage = 0\n    if args.load_model is not None:\n        check_load = open(os.path.join(args.log_dir, ""checkpoint.txt""), \'r\')\n        to_restore = check_load.readlines()[-1].strip()\n        load_file = os.path.join(args.log_dir, to_restore)\n        if os.path.isfile(load_file):\n            print(""=> loading checkpoint \'{}\'"".format(load_file))\n            checkpoint = torch.load(load_file, map_location=\'cpu\')\n            for _ in range(int(checkpoint[\'stage\'])):\n                generator.progress()\n                discriminator.progress()\n            networks = [discriminator, generator]\n            if args.distributed:\n                if args.gpu is not None:\n                    print(\'Distributed to\', args.gpu)\n                    torch.cuda.set_device(args.gpu)\n                    networks = [x.cuda(args.gpu) for x in networks]\n                    args.batch_size = int(args.batch_size / ngpus_per_node)\n                    args.workers = int(args.workers / ngpus_per_node)\n                    networks = [\n                        torch.nn.parallel.DistributedDataParallel(x, device_ids=[args.gpu], output_device=args.gpu) for\n                        x in networks]\n                else:\n                    networks = [x.cuda() for x in networks]\n                    networks = [torch.nn.parallel.DistributedDataParallel(x) for x in networks]\n\n            elif args.gpu is not None:\n                torch.cuda.set_device(args.gpu)\n                networks = [x.cuda(args.gpu) for x in networks]\n            else:\n                networks = [torch.nn.DataParallel(x).cuda() for x in networks]\n\n            discriminator, generator, = networks\n\n            args.stage = checkpoint[\'stage\']\n            args.img_to_use = checkpoint[\'img_to_use\']\n            discriminator.load_state_dict(checkpoint[\'D_state_dict\'])\n            generator.load_state_dict(checkpoint[\'G_state_dict\'])\n            d_opt.load_state_dict(checkpoint[\'d_optimizer\'])\n            g_opt.load_state_dict(checkpoint[\'g_optimizer\'])\n            print(""=> loaded checkpoint \'{}\' (stage {})""\n                  .format(load_file, checkpoint[\'stage\']))\n        else:\n            print(""=> no checkpoint found at \'{}\'"".format(args.log_dir))\n\n    cudnn.benchmark = True\n\n    ###########\n    # Dataset #\n    ###########\n    train_dataset, _ = get_dataset(args.dataset, args)\n\n    if args.distributed:\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n    else:\n        train_sampler = None\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size,\n                                               shuffle=(train_sampler is None), num_workers=args.workers,\n                                               pin_memory=True, sampler=train_sampler)\n\n    ######################\n    # Validate and Train #\n    ######################\n    z_fix_list = [F.pad(torch.randn(args.batch_size, 3, args.size_list[0], args.size_list[0]), [5, 5, 5, 5], value=0)]\n    zero_list = [F.pad(torch.zeros(args.batch_size, 3, args.size_list[zeros_idx], args.size_list[zeros_idx]),\n                       [5, 5, 5, 5], value=0) for zeros_idx in range(1, args.num_scale + 1)]\n    z_fix_list = z_fix_list + zero_list\n\n    if args.validation:\n        validateSinGAN(train_loader, networks, args.stage, args, {""z_rec"": z_fix_list})\n        return\n\n    elif args.test:\n        validateSinGAN(train_loader, networks, args.stage, args, {""z_rec"": z_fix_list})\n        return\n\n    if not args.multiprocessing_distributed or (args.multiprocessing_distributed and args.rank % ngpus_per_node == 0):\n        check_list = open(os.path.join(args.log_dir, ""checkpoint.txt""), ""a+"")\n        record_txt = open(os.path.join(args.log_dir, ""record.txt""), ""a+"")\n        record_txt.write(\'DATASET\\t:\\t{}\\n\'.format(args.dataset))\n        record_txt.write(\'GANTYPE\\t:\\t{}\\n\'.format(args.gantype))\n        record_txt.write(\'IMGTOUSE\\t:\\t{}\\n\'.format(args.img_to_use))\n        record_txt.close()\n\n    for stage in range(args.stage, args.num_scale + 1):\n        if args.distributed:\n            train_sampler.set_epoch(stage)\n\n        trainSinGAN(train_loader, networks, {""d_opt"": d_opt, ""g_opt"": g_opt}, stage, args, {""z_rec"": z_fix_list})\n        validateSinGAN(train_loader, networks, stage, args, {""z_rec"": z_fix_list})\n\n        if args.distributed:\n            discriminator.module.progress()\n            generator.module.progress()\n        else:\n            discriminator.progress()\n            generator.progress()\n\n        networks = [discriminator, generator]\n\n        if args.distributed:\n            if args.gpu is not None:\n                print(\'Distributed\', args.gpu)\n                torch.cuda.set_device(args.gpu)\n                networks = [x.cuda(args.gpu) for x in networks]\n                args.batch_size = int(args.batch_size / ngpus_per_node)\n                args.workers = int(args.workers / ngpus_per_node)\n                networks = [torch.nn.parallel.DistributedDataParallel(x, device_ids=[args.gpu], output_device=args.gpu)\n                            for x in networks]\n            else:\n                networks = [x.cuda() for x in networks]\n                networks = [torch.nn.parallel.DistributedDataParallel(x) for x in networks]\n\n        elif args.gpu is not None:\n            torch.cuda.set_device(args.gpu)\n            networks = [x.cuda(args.gpu) for x in networks]\n        else:\n            networks = [torch.nn.DataParallel(x).cuda() for x in networks]\n\n        discriminator, generator, = networks\n\n        # Update the networks at finest scale\n        if args.distributed:\n            for net_idx in range(generator.module.current_scale):\n                for param in generator.module.sub_generators[net_idx].parameters():\n                    param.requires_grad = False\n                for param in discriminator.module.sub_discriminators[net_idx].parameters():\n                    param.requires_grad = False\n\n            d_opt = torch.optim.Adam(discriminator.module.sub_discriminators[discriminator.current_scale].parameters(),\n                                     5e-4, (0.5, 0.999))\n            g_opt = torch.optim.Adam(generator.module.sub_generators[generator.current_scale].parameters(),\n                                     5e-4, (0.5, 0.999))\n        else:\n            for net_idx in range(generator.current_scale):\n                for param in generator.sub_generators[net_idx].parameters():\n                    param.requires_grad = False\n                for param in discriminator.sub_discriminators[net_idx].parameters():\n                    param.requires_grad = False\n\n            d_opt = torch.optim.Adam(discriminator.sub_discriminators[discriminator.current_scale].parameters(),\n                                     5e-4, (0.5, 0.999))\n            g_opt = torch.optim.Adam(generator.sub_generators[generator.current_scale].parameters(),\n                                     5e-4, (0.5, 0.999))\n\n        ##############\n        # Save model #\n        ##############\n        if not args.multiprocessing_distributed or (args.multiprocessing_distributed and args.rank % ngpus_per_node == 0):\n            if stage == 0:\n                check_list = open(os.path.join(args.log_dir, ""checkpoint.txt""), ""a+"")\n            save_checkpoint({\n                \'stage\': stage + 1,\n                \'D_state_dict\': discriminator.state_dict(),\n                \'G_state_dict\': generator.state_dict(),\n                \'d_optimizer\': d_opt.state_dict(),\n                \'g_optimizer\': g_opt.state_dict(),\n                \'img_to_use\': args.img_to_use\n            }, check_list, args.log_dir, stage + 1)\n            if stage == args.num_scale:\n                check_list.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
code/ops.py,57,"b'from torch import autograd\nfrom torch import nn\nfrom torch.nn import functional as F\nimport math\nfrom scipy import signal\nimport os\nimport pathlib\nfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n\nimport numpy as np\nimport torch\nfrom scipy import linalg\nfrom torch.nn.functional import adaptive_avg_pool2d\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    # If not tqdm is not available, provide a mock version of it\n    def tqdm(x): return x\n\n# from inception import InceptionV3\n\n\ndef compute_grad_gp(d_out, x_in):\n    batch_size = x_in.size(0)\n    grad_dout = autograd.grad(\n        outputs=d_out.sum(), inputs=x_in,\n        create_graph=True, retain_graph=True, only_inputs=True)[0]\n    grad_dout2 = grad_dout.pow(2)\n    assert(grad_dout2.size() == x_in.size())\n    reg = grad_dout2.view(batch_size, -1).sum(1)\n    return reg\n\n\ndef compute_grad_gp_wgan(D, x_real, x_fake, gpu):\n    alpha = torch.rand(x_real.size(0), 1, 1, 1).cuda(gpu)\n\n    x_interpolate = ((1 - alpha) * x_real + alpha * x_fake).detach()\n    x_interpolate.requires_grad = True\n    d_inter_logit = D(x_interpolate)\n    grad = torch.autograd.grad(d_inter_logit, x_interpolate,\n                               grad_outputs=torch.ones_like(d_inter_logit), create_graph=True)[0]\n\n    norm = grad.view(grad.size(0), -1).norm(p=2, dim=1)\n\n    d_gp = ((norm - 1) ** 2).mean()\n    return d_gp\n\n\ndef conv_cond_concat(x, y):\n    if y.device == torch.device(\'cpu\'):\n        return torch.cat((x, y * torch.ones(x.shape[0], y.shape[1], x.shape[2], x.shape[3])), 1)\n    else:\n        return torch.cat((x, y * torch.ones(x.shape[0], y.shape[1], x.shape[2], x.shape[3]).cuda(y.device, non_blocking=True)), 1)\n\n\ndef generate_y_rand(size):\n    # bang  black   blond   brown   gray    male    mustache     smile   glasses young\n    # 12/60 15/60   10/60   12/60   6/60    30/60   6/60         30/60   6/60    48/60\n    total_size = size\n    tmp_rand = []\n    tmp_rand.append(np.random.choice(2, total_size, p=[48. / 60., 12. / 60.]))\n    tmp = np.random.choice(5, total_size, p=[17. / 60., 15. / 60., 10. / 60., 12. / 60., 6. / 60.])\n    tmp_z = np.zeros([total_size, 4])\n    i = 0\n    for m in tmp:\n        if m != 0:\n            tmp_z[i][m - 1] = 1\n        i += 1\n\n    tmp_z = tmp_z.transpose()\n    for z in tmp_z:\n        tmp_rand.append(z)\n    tmp_rand.append(np.random.choice(2, total_size, p=[30. / 60., 30. / 60.]))\n    tmp_rand.append(np.random.choice(2, total_size, p=[54. / 60., 6. / 60.]))\n    tmp_rand.append(np.random.choice(2, total_size, p=[30. / 60., 30. / 60.]))\n    tmp_rand.append(np.random.choice(2, total_size, p=[54. / 60., 6. / 60.]))\n    tmp_rand.append(np.random.choice(2, total_size, p=[12. / 60., 48. / 60.]))\n\n    y_rand_total = np.asarray(tmp_rand).transpose()\n    return y_rand_total.astype(np.float32)\n\n\nclass CInstanceNorm(nn.Module):\n    def __init__(self, nfilter, nlabels):\n        super().__init__()\n        # Attributes\n        self.nlabels = nlabels\n        self.nfilter = nfilter\n        # Submodules\n        self.alpha_embedding = nn.Embedding(nlabels, nfilter)\n        self.beta_embedding = nn.Embedding(nlabels, nfilter)\n        self.bn = nn.InstanceNorm2d(nfilter, affine=False)\n        # Initialize\n        nn.init.uniform(self.alpha_embedding.weight, -1., 1.)\n        nn.init.constant_(self.beta_embedding.weight, 0.)\n\n    def forward(self, x, y):\n        dim = len(x.size())\n        batch_size = x.size(0)\n        assert(dim >= 2)\n        assert(x.size(1) == self.nfilter)\n\n        s = [batch_size, self.nfilter] + [1] * (dim - 2)\n        alpha = self.alpha_embedding(y)\n        alpha = alpha.view(s)\n        beta = self.beta_embedding(y)\n        beta = beta.view(s)\n\n        out = self.bn(x)\n        out = alpha * out + beta\n\n        return out\n\n\ndef toogle_grad(model, requires_grad):\n    for p in model.parameters():\n        p.requires_grad_(requires_grad)\n\n\ndef update_average(model_tgt, model_src, beta=0.999):\n    # model_tgt : deep copy of generator (used for test)\n    toogle_grad(model_src, False)\n    toogle_grad(model_tgt, False)\n\n    param_dict_src = dict(model_src.named_parameters())\n\n    for p_name, p_tgt in model_tgt.named_parameters():\n        p_src = param_dict_src[p_name]\n        assert(p_src is not p_tgt)\n        p_tgt.copy_(beta*p_tgt + (1. - beta)*p_src)\n\n\nclass ConditionalBatchNorm2d(nn.Module):\n    def __init__(self, num_features, num_classes, momentum=0.1):\n        super().__init__()\n        self.num_features = num_features\n        self.bn = nn.BatchNorm2d(num_features, affine=False, momentum=momentum)\n        self.embed = nn.Embedding(num_classes, num_features * 2)\n        self.embed.weight.data[:, :num_features].normal_(1, 0.02)  # Initialise scale at N(1, 0.02)\n        self.embed.weight.data[:, num_features:].zero_()  # Initialise bias at 0\n\n    def forward(self, x, y):\n        out = self.bn(x)\n        gamma, beta = self.embed(y).chunk(2, 1)\n        out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(-1, self.num_features, 1, 1)\n        return out\n\n\nclass MultiConditionalBatchNorm2d(nn.Module):\n    def __init__(self, num_features, num_classes, momentum=0.1):\n        super().__init__()\n        self.num_features = num_features\n        self.bn = nn.BatchNorm2d(num_features, affine=False, momentum=momentum)\n        self.gamma = nn.Linear(num_classes, num_features)\n        self.beta = nn.Linear(num_classes, num_features)\n\n    def forward(self, x, y):\n        out = self.bn(x)\n        gamma = self.gamma(y)\n        beta = self.beta(y)\n        out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(-1, self.num_features, 1, 1)\n        return out\n\n\nclass SpatialAdaptiveNorm2d(nn.Module):\n    def __init__(self, num_features, hid_features=64, momentum=0.1, num_classes=0):\n        super().__init__()\n        self.num_features = num_features\n        self.hid_features = hid_features\n        self.num_classes = num_classes\n        if num_classes > 0:\n            self.bn = MultiConditionalBatchNorm2d(num_features, num_classes)\n        else:\n            self.bn = nn.BatchNorm2d(num_features, affine=False, momentum=momentum)\n\n        # Not apply SN to SPADE (original code)\n        self.hidden = nn.Sequential(nn.Conv2d(3, hid_features, 3, 1, 1),\n                                    nn.LeakyReLU(2e-1))\n\n        self.gamma = nn.Conv2d(hid_features, num_features, 3, 1, 1)\n        self.beta = nn.Conv2d(hid_features, num_features, 3, 1, 1)\n        # make one more hidden for z and concat to out then, get gamma and beta\n        # or resize z * mask + img then, get hid -> gamma, beta\n\n    def forward(self, feat, img, y=None):\n        rimg = F.interpolate(img, size=feat.size()[2:])\n        if self.num_classes > 0 and y is not None:\n            feat = self.bn(feat, y)\n        else:\n            feat = self.bn(feat)\n        out = self.hidden(rimg)\n        gamma = self.gamma(out)\n        beta = self.beta(out)\n        out = gamma * feat + beta\n        return out\n\n\nclass SpatialModulatedNorm2d(nn.Module):\n    def __init__(self, num_features, hid_features=64, momentum=0.1, num_classes=0):\n        super().__init__()\n        self.num_features = num_features\n        self.hid_features = hid_features\n        self.num_classes = num_classes\n        if num_classes > 0:\n            self.bn = MultiConditionalBatchNorm2d(num_features, num_classes, momentum=momentum)\n        else:\n            self.bn = nn.BatchNorm2d(num_features, affine=False, momentum=momentum)\n\n        self.hidden_img = nn.Sequential(nn.Conv2d(3 + 16, hid_features, 3, 1, 1),\n                                        nn.LeakyReLU(2e-1))\n\n        # self.hidden_z = nn.Sequential(nn.Conv2d(16, hid_features//2, 3, 1, 1),\n        #                                 nn.LeakyReLU(2e-1))\n\n        self.gamma = nn.Conv2d(hid_features, num_features, 3, 1, 1)\n        self.beta = nn.Conv2d(hid_features, num_features, 3, 1, 1)\n        # make one more hidden for z and concat to out then, get gamma and beta\n        # or resize z * mask + img then, get hid -> gamma, beta\n\n    def forward(self, feat, img, z, y=None):\n        rimg = F.interpolate(img, size=feat.size()[2:])\n        rz = F.interpolate(z, size=feat.size()[2:])\n\n        rin = torch.cat((rimg, rz), 1)\n        out = self.hidden_img(rin)\n\n        if self.num_classes > 0 and y is not None:\n            feat = self.bn(feat, y)\n        else:\n            feat = self.bn(feat)\n\n        # out = torch.cat((hidimg, hidz), 1)\n        gamma = self.gamma(out)\n        beta = self.beta(out)\n        out = gamma * feat + beta\n\n        return out\n# class SpatialModulatedNorm2d(nn.Module):\n#     def __init__(self, num_features, hid_features=64, momentum=0.1, num_classes=0):\n#         super().__init__()\n#         self.num_features = num_features\n#         self.hid_features = hid_features\n#         self.num_classes = num_classes\n#         if num_classes > 0:\n#             # self.bn = MultiConditionalBatchNorm2d(num_features, num_classes, momentum=momentum)\n#             self.bn = SelfModulratedBatchNorm2d(num_features, num_latent=256, num_classes=num_classes, momentum=momentum)\n#         else:\n#             self.bn = SelfModulratedBatchNorm2d(num_features, num_latent=256, momentum=momentum)\n#\n#         self.hidden_img = nn.Sequential(nn.Conv2d(3, hid_features, 3, 1, 1),\n#                                     nn.LeakyReLU(2e-1))\n#\n#         self.gamma = nn.Conv2d(hid_features, num_features, 3, 1, 1)\n#         self.beta = nn.Conv2d(hid_features, num_features, 3, 1, 1)\n#         # make one more hidden for z and concat to out then, get gamma and beta\n#         # or resize z * mask + img then, get hid -> gamma, beta\n#\n#     def forward(self, feat, img, z, y=None):\n#         rimg = F.interpolate(img, size=feat.size()[2:])\n#\n#         hidimg = self.hidden_img(rimg)\n#\n#         if self.num_classes > 0 and y is not None:\n#             feat = self.bn(feat, z.view(z.size(0), -1), y)\n#         else:\n#             feat = self.bn(feat, z.view(z.size(0), -1))\n#\n#         gamma = self.gamma(hidimg)\n#         beta = self.beta(hidimg)\n#         out = gamma * feat + beta\n#\n#         return out\n\n\nclass SelfModulratedBatchNorm2d(nn.Module):\n    def __init__(self, num_features, num_latent, num_hidden=0, num_classes=0, momentum=0.1):\n        super(SelfModulratedBatchNorm2d, self).__init__()\n        self.num_features = num_features\n        self.num_latent = num_latent\n        self.num_hidden = num_hidden\n        self.num_classes = num_classes\n        self.bn = nn.BatchNorm2d(num_features, affine=False, momentum=momentum)\n        if num_hidden > 0:\n            self.fc_z = nn.Sequential(nn.Linear(num_latent, num_hidden), nn.ReLU(True))\n            num_latent = num_hidden\n        self.gamma = nn.Linear(num_latent, num_features)\n        self.beta = nn.Linear(num_latent, num_features)\n        if num_classes > 0:\n            self.fc_y1 = nn.Linear(num_classes, num_latent)\n            self.fc_y2 = nn.Linear(num_classes, num_latent)\n\n    def forward(self, h, z, y=None):\n        if self.num_hidden > 0:\n            z = self.fc_z(z)\n        if y is not None and self.num_classes > 0:\n            z = z + self.fc_y1(y) + z * self.fc_y2(y)\n\n        out = self.bn(h)\n        gamma = self.gamma(z)\n        beta = self.beta(z)\n        out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(-1, self.num_features, 1, 1)\n        return out\n\n\nclass Embedding(torch.nn.Embedding):\n    def __init__(self, *args, spectral_norm_pi=1, **kwargs):\n        super(Embedding, self).__init__(*args, **kwargs)\n        self.spectral_norm_pi = spectral_norm_pi\n        if spectral_norm_pi > 0:\n            self.register_buffer(""u"", torch.randn((1, self.num_embeddings), requires_grad=False))\n        else:\n            self.register_buffer(""u"", None)\n\n    def forward(self, input):\n        if self.spectral_norm_pi > 0:\n            w_mat = self.weight.view(self.num_embeddings, -1)\n            u, sigma, _ = max_singular_value(w_mat, self.u, self.spectral_norm_pi)\n            w_bar = torch.div(self.weight, sigma)\n            if self.training:\n                self.u = u\n        else:\n            w_bar = self.weight\n\n        return F.embedding(\n            input, w_bar, self.padding_idx, self.max_norm,\n            self.norm_type, self.scale_grad_by_freq, self.sparse)\n\n\ndef max_singular_value(w_mat, u, power_iterations):\n\n    for _ in range(power_iterations):\n        v = l2normalize(torch.mm(u, w_mat.data))\n\n        u = l2normalize(torch.mm(v, torch.t(w_mat.data)))\n\n    sigma = torch.sum(torch.mm(u, w_mat) * v)\n\n    return u, sigma, v\n\n\ndef l2normalize(v, eps=1e-12):\n    return v / (v.norm() + eps)\n\n\nclass GCRNNCellBase(torch.nn.Module):\n\n    def __init__(self, latent_size, ch, bias, num_hidden=3, sn=False):\n        super(GCRNNCellBase, self).__init__()\n        self.ch = ch\n        self.latent_size = latent_size\n        self.concat_size = self.latent_size//4\n\n        if sn:\n            # squeeze ... not necessary? ( concat_size -> latent_size ? )\n            self.layer_zh = torch.nn.utils.spectral_norm(torch.nn.Conv2d(latent_size, self.concat_size, 3, 1, 1, bias=bias))\n\n            self.layer_hh = torch.nn.ModuleList([torch.nn.utils.spectral_norm(torch.nn.Conv2d(ch, self.concat_size, 3, 1, 1, bias=bias))])\n\n            nf = 2 * self.concat_size\n\n            for i in range(num_hidden - 1):\n                self.layer_hh.append(torch.nn.Sequential(torch.nn.utils.spectral_norm(torch.nn.Conv2d(nf, 2 * nf, 3, 1, 1, bias=bias)),\n                                                         torch.nn.BatchNorm2d(2 * nf),\n                                                         torch.nn.ReLU(True)))\n                nf *= 2\n\n            self.layer_hh.append(torch.nn.Sequential(torch.nn.utils.spectral_norm(torch.nn.Conv2d(nf, 3, 3, 1, 1, bias=bias)),\n                                                     torch.nn.Tanh()))\n        else:\n            # squeeze ... unnecessary? ( concat_size -> latent_size ? )\n            self.layer_zh = torch.nn.Conv2d(latent_size, self.concat_size, 3, 1, 1, bias=bias)\n\n            self.layer_hh = torch.nn.ModuleList([torch.nn.Conv2d(ch, self.concat_size, 3, 1, 1, bias=bias)])\n\n            nf = 2 * self.concat_size\n\n            for i in range(num_hidden - 1):\n                self.layer_hh.append(torch.nn.Sequential(torch.nn.Conv2d(nf, 2 * nf, 3, 1, 1, bias=bias),\n                                                         torch.nn.BatchNorm2d(2 * nf),\n                                                         torch.nn.ReLU(True)))\n                nf *= 2\n\n            self.layer_hh.append(torch.nn.Sequential(torch.nn.Conv2d(nf, 3, 3, 1, 1, bias=bias),\n                                                     torch.nn.Tanh()))\n\n    def extra_repr(self):\n        s = \'{input_size}, {hidden_size}\'\n        if \'bias\' in self.__dict__ and self.bias is not True:\n            s += \', bias={bias}\'\n        if \'nonlinearity\' in self.__dict__ and self.nonlinearity != ""tanh"":\n            s += \', nonlinearity={nonlinearity}\'\n        return s.format(**self.__dict__)\n\n    def check_forward_input(self, input):\n        if input.size(1) != self.latent_size:\n            raise RuntimeError(\n                ""input has inconsistent input_size: got {}, expected {}"".format(\n                    input.size(1), self.latent_size))\n\n\nclass GCRNNCell(GCRNNCellBase):\n\n    def __init__(self, latent_size, ch=3, bias=True, num_hidden=3, base=4, sn=False):\n        super(GCRNNCell, self).__init__(latent_size, ch, bias, num_hidden=num_hidden, sn=sn)\n        self.base = base\n\n    def forward(self, z, hx=None, is_up=True):\n        self.check_forward_input(z)\n        if hx is None:\n            hx = torch.ones(z.size(0), 3, self.base, self.base, requires_grad=False)\n            if z.is_cuda:\n                hx = hx.cuda(z.device, non_blocking=True)\n\n        # print(z.device)\n        # with torch.cuda.device_of(z.data):\n        z = z.repeat(1, 1, hx.size(2), hx.size(3))\n        # print(z.device)\n        z_cat = self.layer_zh(z)\n        h_cat = self.layer_hh[0](hx)\n        # z_cat = z_cat.expand_as(h_cat)\n\n        h_out = torch.cat((h_cat, z_cat), 1)\n\n        for block in self.layer_hh[1:-1]:\n            h_out = block(h_out)\n\n        if is_up:\n            h_out = F.interpolate(h_out, scale_factor=2)\n\n        h_out = self.layer_hh[-1](h_out)\n\n        return h_out\n\n\nclass DCRNNCellBase(torch.nn.Module):\n\n    def __init__(self, nf, ch, bias, num_hidden=3, sn=False):\n        super(DCRNNCellBase, self).__init__()\n        self.ch = ch\n        self.nf = nf\n\n        if sn:\n            self.layer_xhh = torch.nn.ModuleList(\n                [torch.nn.Sequential(torch.nn.utils.spectral_norm(torch.nn.Conv2d(ch + self.nf, self.nf, 3, 1, 1, bias=bias)),\n                                     torch.nn.LeakyReLU(inplace=True))])\n\n            nf_ = self.nf\n\n            for i in range(num_hidden - 1):\n                self.layer_xhh.append(torch.nn.Sequential(torch.nn.utils.spectral_norm(torch.nn.Conv2d(nf_, 2 * nf_, 3, 1, 1, bias=bias)),\n                                                          torch.nn.BatchNorm2d(2 * nf_),\n                                                          torch.nn.LeakyReLU(True)))\n                nf_ *= 2\n\n            self.layer_xhh.append(torch.nn.Sequential(torch.nn.utils.spectral_norm(torch.nn.Conv2d(nf_, self.nf, 3, 1, 1, bias=bias)),\n                                                      torch.nn.BatchNorm2d(self.nf),\n                                                      torch.nn.LeakyReLU()))\n        else:\n            self.layer_xhh = torch.nn.ModuleList([torch.nn.Sequential(torch.nn.Conv2d(ch + self.nf, self.nf, 3, 1, 1, bias=bias),\n                                                                      torch.nn.LeakyReLU(inplace=True))])\n\n            nf_ = self.nf\n\n            for i in range(num_hidden - 1):\n                self.layer_xhh.append(torch.nn.Sequential(torch.nn.Conv2d(nf_, 2 * nf_, 3, 1, 1, bias=bias),\n                                                          torch.nn.BatchNorm2d(2 * nf_),\n                                                          torch.nn.LeakyReLU(True)))\n                nf_ *= 2\n\n            self.layer_xhh.append(torch.nn.Sequential(torch.nn.Conv2d(nf_, self.nf, 3, 1, 1, bias=bias),\n                                                      torch.nn.BatchNorm2d(self.nf),\n                                                      torch.nn.LeakyReLU()))\n\n    def extra_repr(self):\n        s = \'{input_size}, {hidden_size}\'\n        if \'bias\' in self.__dict__ and self.bias is not True:\n            s += \', bias={bias}\'\n        if \'nonlinearity\' in self.__dict__ and self.nonlinearity != ""tanh"":\n            s += \', nonlinearity={nonlinearity}\'\n        return s.format(**self.__dict__)\n\n    def check_forward_input(self, input):\n        if input.size(1) != self.latent_size:\n            raise RuntimeError(\n                ""input has inconsistent input_size: got {}, expected {}"".format(\n                    input.size(1), self.latent_size))\n\n\nclass DCRNNCell(DCRNNCellBase):\n\n    def __init__(self, nf=64, ch=3, bias=True, num_hidden=3, sn=False):\n        super(DCRNNCell, self).__init__(nf, ch, bias, num_hidden=num_hidden, sn=sn)\n\n    def forward(self, x, hx=None, is_down=True):\n        # self.check_forward_input(x)\n        if hx is None:\n            hx = torch.ones(x.size(0), self.nf, x.size(2), x.size(3), requires_grad=False)\n            if x.is_cuda:\n                hx = hx.cuda(x.device, non_blocking=True)\n\n        h_out = torch.cat((x, hx), 1)\n\n        for block in self.layer_xhh[:-1]:\n            h_out = block(h_out)\n\n        if is_down:\n            h_out = F.interpolate(h_out, scale_factor=0.5)\n\n        h_out = self.layer_xhh[-1](h_out)\n\n        return h_out\n\n\ndef mixup_criterion(x_orig, x_flip, x_recon, lam, loss=\'l1\'):\n    if loss == \'l1\':\n        return lam * F.l1_loss(x_recon, x_orig) + (1 - lam) * F.l1_loss(x_recon, x_flip)\n    else:\n        return lam * F.mse_loss(x_recon, x_orig) + (1 - lam) * F.mse_loss(x_recon, x_flip)\n\n\ndef eval_ssim(img1, img2, max_val=255, filter_size=11,\n                       filter_sigma=1.5, k1=0.01, k2=0.03):\n    """"""Return the Structural Similarity Map between `img1` and `img2`.\n\n    This function attempts to match the functionality of ssim_index_new.m by\n    Zhou Wang: http://www.cns.nyu.edu/~lcv/ssim/msssim.zip\n\n    Arguments:\n      img1: Numpy array holding the first RGB image batch.\n      img2: Numpy array holding the second RGB image batch.\n      max_val: the dynamic range of the images (i.e., the difference between the\n        maximum the and minimum allowed values).\n      filter_size: Size of blur kernel to use (will be reduced for small\n      images).\n      filter_sigma: Standard deviation for Gaussian blur kernel (will be reduced\n        for small images).\n      k1: Constant used to maintain stability in the SSIM calculation (0.01 in\n        the original paper).\n      k2: Constant used to maintain stability in the SSIM calculation (0.03 in\n        the original paper).\n\n    Returns:\n      Pair containing the mean SSIM and contrast sensitivity between `img1` and\n      `img2`.\n\n    Raises:\n      RuntimeError: If input images don\'t have the same shape or don\'t have four\n        dimensions: [batch_size, height, width, depth].\n    """"""\n    if img1.shape != img2.shape:\n        raise RuntimeError(\'Input images must have the same shape (%s vs. %s).\',\n                           img1.shape, img2.shape)\n    if img1.ndim != 4:\n        raise RuntimeError(\'Input images must have four dimensions, not %d\',\n                           img1.ndim)\n\n    img1 = img1.astype(np.float64)\n    img2 = img2.astype(np.float64)\n    _, height, width, _ = img1.shape\n\n    # Filter size can\'t be larger than height or width of images.\n    size = min(filter_size, height, width)\n\n    # Scale down sigma if a smaller filter size is used.\n    sigma = size * filter_sigma / filter_size if filter_size else 0\n\n    if filter_size:\n        window = np.reshape(_FSpecialGauss(size, sigma), (1, size, size, 1))\n        mu1 = signal.fftconvolve(img1, window, mode=\'valid\')\n        mu2 = signal.fftconvolve(img2, window, mode=\'valid\')\n        sigma11 = signal.fftconvolve(img1 * img1, window, mode=\'valid\')\n        sigma22 = signal.fftconvolve(img2 * img2, window, mode=\'valid\')\n        sigma12 = signal.fftconvolve(img1 * img2, window, mode=\'valid\')\n    else:\n        # Empty blur kernel so no need to convolve.\n        mu1, mu2 = img1, img2\n        sigma11 = img1 * img1\n        sigma22 = img2 * img2\n        sigma12 = img1 * img2\n\n    mu11 = mu1 * mu1\n    mu22 = mu2 * mu2\n    mu12 = mu1 * mu2\n    sigma11 -= mu11\n    sigma22 -= mu22\n    sigma12 -= mu12\n\n    # Calculate intermediate values used by both ssim and cs_map.\n    c1 = (k1 * max_val) ** 2\n    c2 = (k2 * max_val) ** 2\n    v1 = 2.0 * sigma12 + c2\n    v2 = sigma11 + sigma22 + c2\n    ssim = np.mean((((2.0 * mu12 + c1) * v1) / ((mu11 + mu22 + c1) * v2)))\n    cs = np.mean(v1 / v2)\n    return ssim, cs\n\n\ndef eval_psnr(img1, img2):\n    mse = np.mean( (img1 - img2) ** 2 )\n    if mse == 0:\n        return 100\n    PIXEL_MAX = 255.0\n    return 20 * math.log10(PIXEL_MAX / math.sqrt(mse))\n\n\ndef _FSpecialGauss(size, sigma):\n    """"""Function to mimic the \'fspecial\' gaussian MATLAB function.""""""\n    radius = size // 2\n    offset = 0.0\n    start, stop = -radius, radius + 1\n    if size % 2 == 0:\n        offset = 0.5\n        stop -= 1\n    x, y = np.mgrid[offset + start:stop, offset + start:stop]\n    assert len(x) == size\n    g = np.exp(-((x ** 2 + y ** 2) / (2.0 * sigma ** 2)))\n    return g / g.sum()\n\n\n# def calc_generator_fid(model, data_loader, args, dims=2048):\n#     eps = 1e-6\n#\n#     incept = InceptionV3([InceptionV3.BLOCK_INDEX_BY_DIM[dims]])\n#\n#     model.eval()\n#     incept.eval()\n#\n#     valiter = iter(data_loader)\n#\n#     tot_iter = len(valiter)\n#\n#     pred_fake = np.empty((args.val_batch * tot_iter, dims))\n#     pred_real = np.empty((args.val_batch * tot_iter, dims))\n#\n#     if not next(model.parameters()).device == torch.device(\'cpu\'):\n#         incept = incept.cuda(args.gpu)\n#\n#     for i in tqdm(range(tot_iter)):\n#         x_real, _ = next(valiter)\n#         z_in = torch.randn(args.val_batch, args.latent_size)\n#         if not next(model.parameters()).device == torch.device(\'cpu\'):\n#             x_real = x_real.cuda(args.gpu, non_blocking=True)\n#             z_in = z_in.cuda(args.gpu, non_blocking=True)\n#         out = model(z_in)\n#         x_fake = out[0]\n#         x_fake = (x_fake + 1.0) / 2.0\n#         x_real = (x_real + 1.0) / 2.0\n#\n#         tmp_fake = incept(x_fake)[0]\n#         tmp_real = incept(x_real)[0]\n#         if tmp_fake.shape[2] != 1 or tmp_fake.shape[3] != 1:\n#             tmp_fake = adaptive_avg_pool2d(tmp_fake, output_size=(1, 1))\n#             tmp_real = adaptive_avg_pool2d(tmp_real, output_size=(1, 1))\n#\n#         pred_fake[i * args.val_batch: (i + 1) * args.val_batch] = tmp_fake.cpu().data.numpy().reshape(args.val_batch, -1)\n#         pred_real[i * args.val_batch: (i + 1) * args.val_batch] = tmp_real.cpu().data.numpy().reshape(args.val_batch, -1)\n#\n#     mu_fake = np.atleast_1d(np.mean(pred_fake, axis=0))\n#     std_fake = np.atleast_2d(np.cov(pred_fake, rowvar=False))\n#\n#     mu_real = np.atleast_1d(np.mean(pred_real, axis=0))\n#     std_real = np.atleast_2d(np.cov(pred_real, rowvar=False))\n#\n#     assert mu_fake.shape == mu_real.shape\n#     assert std_fake.shape == std_real.shape\n#\n#     mu_diff = mu_fake - mu_real\n#\n#     covmean, _ = linalg.sqrtm(std_fake.dot(std_real), disp=False)\n#\n#     if not np.isfinite(covmean).all():\n#         msg = (\'fid calculation produces singular product; \'\n#                \'adding %s to diagonal of cov estimates\') % eps\n#         print(msg)\n#         offset = np.eye(std_fake.shape[0]) * eps\n#         covmean = linalg.sqrtm((std_fake + offset).dot(std_real + offset))\n#\n#     # Numerical error might give slight imaginary component\n#     if np.iscomplexobj(covmean):\n#         if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n#             m = np.max(np.abs(covmean.imag))\n#             raise ValueError(\'Imaginary component {}\'.format(m))\n#         covmean = covmean.real\n#\n#     tr_covmean = np.trace(covmean)\n#\n#     return mu_diff.dot(mu_diff) + np.trace(std_fake) + np.trace(std_real) - 2 * tr_covmean\n\n'"
code/train.py,20,"b'from tqdm import trange\nfrom torch.nn import functional as F\nimport torch.nn\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\nfrom utils import *\nfrom ops import compute_grad_gp_wgan, compute_grad_gp\nimport torchvision.utils as vutils\n\n\ndef trainSinGAN(data_loader, networks, opts, stage, args, additional):\n    # avg meter\n    d_losses = AverageMeter()\n    g_losses = AverageMeter()\n\n    # set nets\n    D = networks[0]\n    G = networks[1]\n    # set opts\n    d_opt = opts[\'d_opt\']\n    g_opt = opts[\'g_opt\']\n    # switch to train mode\n    D.train()\n    G.train()\n    # summary writer\n    # writer = additional[0]\n    train_it = iter(data_loader)\n    # total_iter = 2000 * (args.num_scale - stage + 1)\n    # decay_lr = 1600 * (args.num_scale - stage + 1)\n    total_iter = 2000\n    decay_lr = 1600\n\n    d_iter = 3\n    g_iter = 3\n\n    t_train = trange(0, total_iter, initial=0, total=total_iter)\n\n    z_rec = additional[\'z_rec\']\n\n    for z_idx in range(len(z_rec)):\n        z_rec[z_idx] = z_rec[z_idx].cuda(args.gpu, non_blocking=True)\n\n    x_in = next(train_it)\n\n    x_in = x_in.cuda(args.gpu, non_blocking=True)\n    x_org = x_in\n    x_in = F.interpolate(x_in, (args.size_list[stage], args.size_list[stage]), mode=\'bilinear\', align_corners=True)\n    vutils.save_image(x_in.detach().cpu(), os.path.join(args.res_dir, \'ORGTRAIN_{}.png\'.format(stage)),\n                      nrow=1, normalize=True)\n\n    x_in_list = [x_in]\n    for xidx in range(1, stage + 1):\n        x_tmp = F.interpolate(x_org, (args.size_list[xidx], args.size_list[xidx]), mode=\'bilinear\', align_corners=True)\n        x_in_list.append(x_tmp)\n\n    for i in t_train:\n        if i == decay_lr:\n            for param_group in d_opt.param_groups:\n                    param_group[\'lr\'] *= 0.1\n                    print(""DISCRIMINATOR LEARNING RATE UPDATE TO :"", param_group[\'lr\'])\n            for param_group in g_opt.param_groups:\n                    param_group[\'lr\'] *= 0.1\n                    print(""GENERATOR LEARNING RATE UPDATE TO :"", param_group[\'lr\'])\n\n        for _ in range(g_iter):\n            g_opt.zero_grad()\n\n            x_rec_list = G(z_rec)\n\n            g_rec = F.mse_loss(x_rec_list[-1], x_in)\n            # calculate rmse for each scale\n            rmse_list = [1.0]\n            for rmseidx in range(1, stage + 1):\n                rmse = torch.sqrt(F.mse_loss(x_rec_list[rmseidx], x_in_list[rmseidx]))\n                rmse_list.append(rmse)\n\n            z_list = [F.pad(rmse_list[z_idx] * torch.randn(args.batch_size, 3, args.size_list[z_idx],\n                                               args.size_list[z_idx]).cuda(args.gpu, non_blocking=True),\n                            [5, 5, 5, 5], value=0) for z_idx in range(stage + 1)]\n\n            x_fake_list = G(z_list)\n\n            g_fake_logit = D(x_fake_list[-1])\n\n            ones = torch.ones_like(g_fake_logit).cuda(args.gpu)\n\n            if args.gantype == \'wgangp\':\n                # wgan gp\n                g_fake = -torch.mean(g_fake_logit, (2, 3))\n                g_loss = g_fake + 10.0 * g_rec\n            elif args.gantype == \'zerogp\':\n                # zero centered GP\n                g_fake = F.binary_cross_entropy_with_logits(g_fake_logit, ones, reduction=\'none\').mean()\n                g_loss = g_fake + 100.0 * g_rec\n\n            elif args.gantype == \'lsgan\':\n                # lsgan\n                g_fake = F.mse_loss(torch.mean(g_fake_logit, (2, 3)), 0.9 * ones)\n                g_loss = g_fake + 50.0 * g_rec\n\n            g_loss.backward()\n            g_opt.step()\n\n            g_losses.update(g_loss.item(), x_in.size(0))\n\n        # Update discriminator\n        for _ in range(d_iter):\n            x_in.requires_grad = True\n\n            d_opt.zero_grad()\n            x_fake_list = G(z_list)\n\n            d_fake_logit = D(x_fake_list[-1].detach())\n            d_real_logit = D(x_in)\n\n            ones = torch.ones_like(d_real_logit).cuda(args.gpu)\n            zeros = torch.zeros_like(d_fake_logit).cuda(args.gpu)\n\n            if args.gantype == \'wgangp\':\n                # wgan gp\n                d_fake = torch.mean(d_fake_logit, (2, 3))\n                d_real = -torch.mean(d_real_logit, (2, 3))\n                d_gp = compute_grad_gp_wgan(D, x_in, x_fake_list[-1], args.gpu)\n                d_loss = d_real + d_fake + 0.1 * d_gp\n            elif args.gantype == \'zerogp\':\n                # zero centered GP\n                # d_fake = F.binary_cross_entropy_with_logits(torch.mean(d_fake_logit, (2, 3)), zeros)\n                d_fake = F.binary_cross_entropy_with_logits(d_fake_logit, zeros, reduction=\'none\').mean()\n                # d_real = F.binary_cross_entropy_with_logits(torch.mean(d_real_logit, (2, 3)), ones)\n                d_real = F.binary_cross_entropy_with_logits(d_real_logit, ones, reduction=\'none\').mean()\n                d_gp = compute_grad_gp(torch.mean(d_real_logit, (2, 3)), x_in)\n                d_loss = d_real + d_fake + 10.0 * d_gp\n\n            elif args.gantype == \'lsgan\':\n                # lsgan\n                d_fake = F.mse_loss(torch.mean(d_fake_logit, (2, 3)), zeros)\n                d_real = F.mse_loss(torch.mean(d_real_logit, (2, 3)), 0.9 * ones)\n                d_loss = d_real + d_fake\n\n            d_loss.backward()\n            d_opt.step()\n\n            d_losses.update(d_loss.item(), x_in.size(0))\n\n        t_train.set_description(\'Stage: [{}/{}] Avg Loss: D[{d_losses.avg:.3f}] G[{g_losses.avg:.3f}] RMSE[{rmse:.3f}]\'\n                                .format(stage, args.num_scale, d_losses=d_losses, g_losses=g_losses, rmse=rmse_list[-1]))\n'"
code/utils.py,15,"b'import os\nimport torch\nimport numpy as np\nfrom torch.autograd import Variable\nimport cv2\nimport matplotlib.pyplot as plt\nfrom torch.nn import functional as F\n\n\ndef makedirs(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef formatted_print(notice, value):\n    print(\'{0:<40}{1:<40}\'.format(notice, value))\n\n\ndef save_checkpoint(state, check_list, log_dir, epoch=0):\n    check_file = os.path.join(log_dir, \'model_{}.ckpt\'.format(epoch))\n    torch.save(state, check_file)\n    check_list.write(\'model_{}.ckpt\\n\'.format(epoch))\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef adjust_learning_rate(optimizer, epoch, args):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n    lr = args.lr * (0.1 ** (epoch // 30))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the accuracy over the k top predictions for the specified values of k""""""\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n\ndef mixup_data(x, y, alpha=0.2, use_cuda=True):\n    \'\'\'Returns mixed inputs, pairs of targets, and lambda\'\'\'\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n    if use_cuda:\n        index = torch.randperm(batch_size).cuda()\n    else:\n        index = torch.randperm(batch_size)\n\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\n\ndef mixup_criterion(pred, y_a, y_b, prob):\n    return prob * F.cross_entropy(pred, y_a) + (1 - prob) * F.cross_entropy(pred, y_b)\n\n\ndef mix_data(x, x_flip):\n    \'\'\'Returns mixed inputs, pairs of targets, and lambda\'\'\'\n    # lam = np.random.uniform(0.0, 1.0)\n    lam = np.random.beta(0.4, 0.4)\n\n    x_mix = lam * x + (1 - lam) * x_flip\n    return x_mix, lam\n\n\ndef get_att_map(feats, y_, norm=True):\n    with torch.no_grad():\n        y_ = y_.long()\n        att_map = Variable(torch.zeros([feats.shape[0], feats.shape[2], feats.shape[3]]))\n\n        for idx in range(feats.shape[0]):\n            att_map[idx, :, :] = torch.squeeze(feats[idx, y_.data[idx], :, :])\n\n        if norm:\n            att_map = norm_att_map(att_map)\n\n    return att_map\n\n\ndef norm_att_map(att_maps):\n    _min = att_maps.min(-1, keepdim=True)[0].min(-2, keepdim=True)[0]\n    _max = att_maps.max(-1, keepdim=True)[0].max(-2, keepdim=True)[0]\n    att_norm = (att_maps - _min) / (_max - _min)\n    return att_norm\n\n\ndef load_bbox_size(dataset_path=\'../data/CUB/CUB_200_2011\', img_size=224):\n    origin_bbox = {}\n    image_sizes = {}\n    resized_bbox = {}\n    with open(os.path.join(dataset_path, \'bounding_boxes.txt\')) as f:\n        for each_line in f:\n            file_info = each_line.strip().split()\n            image_id = int(file_info[0])\n\n            x, y, bbox_width, bbox_height = map(float, file_info[1:])\n\n            origin_bbox[image_id] = [x, y, bbox_width, bbox_height]\n\n    with open(os.path.join(dataset_path, \'sizes.txt\')) as f:\n        for each_line in f:\n            file_info = each_line.strip().split()\n            image_id = int(file_info[0])\n            image_width, image_height = map(float, file_info[1:])\n\n            image_sizes[image_id] = [image_width, image_height]\n\n    for i in origin_bbox.keys():\n        x, y, bbox_width, bbox_height = origin_bbox[i]\n        image_width, image_height = image_sizes[i]\n\n        x_scale = img_size / image_width\n        y_scale = img_size / image_height\n\n        x_new = int(np.round(x * x_scale))\n        y_new = int(np.round(y * y_scale))\n        x_max = int(np.round(bbox_width * x_scale))\n        y_max = int(np.round(bbox_height * y_scale))\n\n        resized_bbox[i] = [x_new, y_new, x_new + x_max, y_new + y_max]\n\n    return resized_bbox\n\n\ndef cammed_image(image, mask, require_norm=False):\n    if require_norm:\n        mask = mask - np.min(mask)\n        mask = mask / np.max(mask)\n    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n    heatmap = np.float32(heatmap) / 255\n    cam = heatmap + np.float32(image)\n    cam = cam / np.max(cam)\n    return heatmap * 255., cam * 255.\n\n\ndef intensity_to_rgb(intensity, normalize=False):\n    """"""\n    Convert a 1-channel matrix of intensities to an RGB image employing a colormap.\n    This function requires matplotlib. See `matplotlib colormaps\n    <http://matplotlib.org/examples/color/colormaps_reference.html>`_ for a\n    list of available colormap.\n    Args:\n        intensity (np.ndarray): array of intensities such as saliency.\n        cmap (str): name of the colormap to use.\n        normalize (bool): if True, will normalize the intensity so that it has\n            minimum 0 and maximum 1.\n    Returns:\n        np.ndarray: an RGB float32 image in range [0, 255], a colored heatmap.\n    """"""\n    assert intensity.ndim == 2, intensity.shape\n    intensity = intensity.astype(""float"")\n\n    if normalize:\n        intensity -= intensity.min()\n        intensity /= intensity.max()\n\n    cmap = \'jet\'\n    cmap = plt.get_cmap(cmap)\n    intensity = cmap(intensity)[..., :3]\n    return intensity.astype(\'float32\') * 255.0\n\n\ndef large_rect(rect):\n    # find largest recteangles\n    large_area = 0\n    target = 0\n    if len(rect) == 1:\n        x = rect[0][0]\n        y = rect[0][1]\n        w = rect[0][2]\n        h = rect[0][3]\n        return x, y, w, h\n    else:\n        for i in range(1, len(rect)):\n            area = rect[i][2] * rect[i][3]\n            if large_area < area:\n                large_area = area\n                target = i\n        x = rect[target][0]\n        y = rect[target][1]\n        w = rect[target][2]\n        h = rect[target][3]\n        return x, y, w, h\n\n\ndef calculate_IOU(boxA, boxB):\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    # compute the area of intersection rectangle\n    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n\n    # compute the area of both the prediction and ground-truth\n    # rectangles\n    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n\n    # return the intersection over union value\n    return iou\n\n\ndef calculate_IOU_multibox(boxA, boxesB):\n\n    interArea = 0.0\n    boxAArea = 0.0\n    boxBArea = 0.0\n\n    for boxB in boxesB:\n        xA = max(boxA[0], boxB[0])\n        yA = max(boxA[1], boxB[1])\n        xB = min(boxA[2], boxB[2])\n        yB = min(boxA[3], boxB[3])\n        # compute the area of intersection rectangle\n        interArea += max(0, xB - xA + 1) * max(0, yB - yA + 1)\n        boxBArea += (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n    # compute the area of both the prediction and ground-truth\n    # rectangles\n    boxAArea += (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n\n    # return the intersection over union value\n    return iou\n\n\ndef train_val_split(labels, n_labeled_per_class):\n    labels = np.array(labels)\n    train_labeled_idxs = []\n    train_unlabeled_idxs = []\n    val_idxs = []\n\n    for i in range(10):\n        idxs = np.where(labels == i)[0]\n        np.random.shuffle(idxs)\n        train_labeled_idxs.extend(idxs[:n_labeled_per_class])\n        train_unlabeled_idxs.extend(idxs[n_labeled_per_class:-500])\n        val_idxs.extend(idxs[-500:])\n    np.random.shuffle(train_labeled_idxs)\n    np.random.shuffle(train_unlabeled_idxs)\n    np.random.shuffle(val_idxs)\n\n    return train_labeled_idxs, train_unlabeled_idxs, val_idxs\n\n\nclass EMA(torch.nn.Module):\n    def __init__(self, mu=0.999):\n        super(EMA, self).__init__()\n        self.mu = mu\n        self.shadow = {}\n\n    def register(self, name, val):\n        self.shadow[name] = val.clone()\n\n    def forward(self, name, x):\n        assert name in self.shadow\n        new_average = (1.0 - self.mu) * x + self.mu * self.shadow[name]\n        self.shadow[name] = new_average.clone()\n        return new_average\n\n\ndef linear_rampup(current, rampup_length=16):\n    if rampup_length == 0:\n        return 1.0\n    else:\n        current = np.clip(current / rampup_length, 0.0, 1.0)\n        return float(current)\n\n\ndef cross_entropy(input, target):\n    """""" Cross entropy for one-hot labels\n    """"""\n    return -torch.mean(torch.sum(target * F.log_softmax(input), dim=1))\n\n\nclass SemiLoss(object):\n    def __call__(self, outputs_x, targets_x, outputs_u, targets_u, epoch, args):\n        probs_u = torch.softmax(outputs_u, dim=1)\n\n        Lx = -torch.mean(torch.sum(F.log_softmax(outputs_x, dim=1) * targets_x, dim=1))\n        Lu = torch.mean((probs_u - targets_u)**2)\n\n        return Lx, Lu, args.lambda_u * linear_rampup(epoch)\n\n\ndef interleave_offsets(batch, nu):\n    groups = [batch // (nu + 1)] * (nu + 1)\n    for x in range(batch - sum(groups)):\n        groups[-x - 1] += 1\n    offsets = [0]\n    for g in groups:\n        offsets.append(offsets[-1] + g)\n    assert offsets[-1] == batch\n    return offsets\n\n\ndef interleave(xy, batch):\n    nu = len(xy) - 1\n    offsets = interleave_offsets(batch, nu)\n    xy = [[v[offsets[p]:offsets[p + 1]] for p in range(nu + 1)] for v in xy]\n    for i in range(1, nu + 1):\n        xy[0][i], xy[i][i] = xy[i][i], xy[0][i]\n    return [torch.cat(v, dim=0) for v in xy]\n\n\n# my Rectangle = (x1, y1, x2, y2), a bit different from OP\'s x, y, w, h\ndef intersection(rectA, rectB): # check if rect A & B intersect\n    a, b = rectA, rectB\n    startX = max(min(a[0], a[2]), min(b[0], b[2]))\n    startY = max(min(a[1], a[3]), min(b[1], b[3]))\n    endX = min(max(a[0], a[2]), max(b[0], b[2]))\n    endY = min(max(a[1], a[3]), max(b[1], b[3]))\n    if startX < endX and startY < endY:\n        return True\n    else:\n        return False\n\n\ndef combineRect(rectA, rectB): # create bounding box for rect A & B\n    a, b = rectA, rectB\n    startX = min(a[0], b[0])\n    startY = min(a[1], b[1])\n    endX = max(a[2], b[2])\n    endY = max(a[3], b[3])\n    return (startX, startY, endX, endY)\n\n\ndef checkIntersectAndCombine(rects):\n    if rects is None:\n        return None\n    mainRects = rects\n    noIntersect = False\n    while noIntersect == False and len(mainRects) > 1:\n        # mainRects = list(set(mainRects))\n        # get the unique list of rect, or the noIntersect will be\n        # always true if there are same rect in mainRects\n        newRectsArray = []\n        for rectA, rectB in itertools.combinations(mainRects, 2):\n            newRect = []\n            if intersection(rectA, rectB):\n                newRect = combineRect(rectA, rectB)\n                newRectsArray.append(newRect)\n                noIntersect = False\n                # delete the used rect from mainRects\n                if rectA in mainRects:\n                    mainRects.remove(rectA)\n                if rectB in mainRects:\n                    mainRects.remove(rectB)\n        if len(newRectsArray) == 0:\n            # if no newRect is created = no rect in mainRect intersect\n            noIntersect = True\n        else:\n            # loop again the combined rect and those remaining rect in mainRects\n            mainRects = mainRects + newRectsArray\n    return mainRects\n\n'"
code/validation.py,9,"b""from tqdm import trange\nfrom torch.nn import functional as F\nimport torch.nn\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision.utils as vutils\nimport pickle\n\nfrom utils import *\n\n\ndef validateSinGAN(data_loader, networks, stage, args, additional=None):\n    # set nets\n    D = networks[0]\n    G = networks[1]\n    # switch to train mode\n    D.eval()\n    G.eval()\n    # summary writer\n    # writer = additional[0]\n    val_it = iter(data_loader)\n\n    z_rec = additional['z_rec']\n\n    x_in = next(val_it)\n    x_in = x_in.cuda(args.gpu, non_blocking=True)\n    x_org = x_in\n\n    x_in = F.interpolate(x_in, (args.size_list[stage], args.size_list[stage]), mode='bilinear', align_corners=True)\n    vutils.save_image(x_in.detach().cpu(), os.path.join(args.res_dir, 'ORG_{}.png'.format(stage)),\n                      nrow=1, normalize=True)\n    x_in_list = [x_in]\n    for xidx in range(1, stage + 1):\n        x_tmp = F.interpolate(x_org, (args.size_list[xidx], args.size_list[xidx]), mode='bilinear', align_corners=True)\n        x_in_list.append(x_tmp)\n\n    for z_idx in range(len(z_rec)):\n        z_rec[z_idx] = z_rec[z_idx].cuda(args.gpu, non_blocking=True)\n\n    with torch.no_grad():\n        x_rec_list = G(z_rec)\n\n        # calculate rmse for each scale\n        rmse_list = [1.0]\n        for rmseidx in range(1, stage + 1):\n            rmse = torch.sqrt(F.mse_loss(x_rec_list[rmseidx], x_in_list[rmseidx]))\n            if args.validation:\n                rmse /= 100.0\n            rmse_list.append(rmse)\n        if len(rmse_list) > 1:\n            rmse_list[-1] = 0.0\n\n        vutils.save_image(x_rec_list[-1].detach().cpu(), os.path.join(args.res_dir, 'REC_{}.png'.format(stage)),\n                          nrow=1, normalize=True)\n\n        for k in range(50):\n            z_list = [F.pad(rmse_list[z_idx] * torch.randn(args.batch_size, 3, args.size_list[z_idx],\n                                               args.size_list[z_idx]).cuda(args.gpu, non_blocking=True),\n                            [5, 5, 5, 5], value=0) for z_idx in range(stage + 1)]\n            x_fake_list = G(z_list)\n\n            vutils.save_image(x_fake_list[-1].detach().cpu(), os.path.join(args.res_dir, 'GEN_{}_{}.png'.format(stage, k)),\n                              nrow=1, normalize=True)\n\n\n\n"""
code/datasets/cars.py,1,"b'import torch.utils.data as data\nimport scipy.io\nimport numpy as np\nimport os\nfrom datasets.cub200 import pil_loader\n\n\nclass Cars(data.Dataset):\n\n    def __init__(self, data_dir, is_train, transform=None, with_id=False):\n        """"""\n        Args:\n            mat_anno (string): Path to the MATLAB annotation file.\n            data_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        """"""\n\n        self.transform = transform\n        self.with_id = with_id\n        self.data_dir = os.path.join(data_dir, \'stanfordCar\')\n        self.annoroot_dir = os.path.join(self.data_dir, \'devkit\')\n\n        if is_train:\n            self.image_dir = os.path.join(self.data_dir, \'cars_train\')\n            self.anno_dir = os.path.join(self.annoroot_dir, \'cars_train_annos.mat\')\n            self.size_dir = os.path.join(self.annoroot_dir, \'cars_train_size.txt\')\n        else:\n            self.image_dir = os.path.join(self.data_dir, \'cars_test\')\n            self.anno_dir = os.path.join(self.annoroot_dir, \'cars_test_annos_withlabels.mat\')\n            self.size_dir = os.path.join(self.annoroot_dir, \'cars_test_size.txt\')\n\n        # x1 y1 x2 y2 label filename\n        self.annotations = scipy.io.loadmat(self.anno_dir)\n        self.annotations = self.annotations[\'annotations\'][0]\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        # x1 y1 x2 y2 label filename\n        fileid = self.annotations[idx][-1].item()\n        img_dir = os.path.join(self.image_dir, fileid)\n\n        target = self.annotations[idx][-2].item()\n\n        img = pil_loader(img_dir)\n\n        if self.transform:\n            img = self.transform(img)\n\n        if self.with_id:\n            return img, target, fileid\n        else:\n            return img, target\n\n    def load_bboxes(self, img_size=224):\n        size_file = open(self.size_dir, \'r\')\n        sizes = size_file.readlines()\n        size_idx = 0\n        bboxes = dict()\n        for idx in range(len(self.annotations)):\n            w, h = int(sizes[size_idx].strip().split(\' \')[0]), int(sizes[size_idx].strip().split(\' \')[1])\n            size_idx += 1\n            x1, y1, x2, y2 = [self.annotations[idx][0].item(), self.annotations[idx][1].item(),\n                              self.annotations[idx][2].item(), self.annotations[idx][3].item()]\n\n            x_scale = img_size / w\n            y_scale = img_size / h\n\n            x_new = int(np.round(x1 * x_scale))\n            y_new = int(np.round(y1 * y_scale))\n            x_max = int(np.round(x2 * x_scale))\n            y_max = int(np.round(y2 * y_scale))\n\n            bboxes[self.annotations[idx][-1].item()] = [x_new, y_new, x_max, y_max]\n        return bboxes\n'"
code/datasets/cub200.py,3,"b'import os\nimport pandas as pd\nimport torch\nfrom torchvision.datasets.folder import default_loader\nfrom torchvision.datasets.utils import download_url\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nfrom torchvision.transforms import functional as vF\nfrom PIL import Image\nimport numpy as np\n\n\ndef pil_loader(path, gray=False):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        img = Image.open(f)\n        if gray:\n            return img.convert(\'L\')\n        else:\n            return img.convert(\'RGB\')\n\n\n# class Cub2011(Dataset):\n#     base_folder = \'CUB_200_2011/images\'\n#     url = \'http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz\'\n#     filename = \'CUB_200_2011.tgz\'\n#     tgz_md5 = \'97eceeb196236b17998738112f37df78\'\n#\n#     def __init__(self, root, train=True, transform=None, download=True, with_id=False):\n#         self.root = os.path.expanduser(root)\n#         self.transform = transform\n#         self.loader = default_loader\n#         self.train = train\n#         self.with_id = with_id\n#\n#         if download:\n#             self._download()\n#\n#         if not self._check_integrity():\n#             raise RuntimeError(\'Dataset not found or corrupted.\' +\n#                                \' You can use download=True to download it\')\n#\n#     def _load_metadata(self):\n#         images = pd.read_csv(os.path.join(self.root, \'CUB_200_2011\', \'images.txt\'), sep=\' \',\n#                              names=[\'img_id\', \'filepath\'])\n#         image_class_labels = pd.read_csv(os.path.join(self.root, \'CUB_200_2011\', \'image_class_labels.txt\'),\n#                                          sep=\' \', names=[\'img_id\', \'target\'])\n#         train_test_split = pd.read_csv(os.path.join(self.root, \'CUB_200_2011\', \'train_test_split.txt\'),\n#                                        sep=\' \', names=[\'img_id\', \'is_training_img\'])\n#\n#         data = images.merge(image_class_labels, on=\'img_id\')\n#         self.data = data.merge(train_test_split, on=\'img_id\')\n#\n#         if self.train:\n#             self.data = self.data[self.data.is_training_img == 1]\n#         else:\n#             self.data = self.data[self.data.is_training_img == 0]\n#\n#     def _check_integrity(self):\n#         try:\n#             self._load_metadata()\n#         except Exception:\n#             return False\n#\n#         for index, row in self.data.iterrows():\n#             filepath = os.path.join(self.root, self.base_folder, row.filepath)\n#             if not os.path.isfile(filepath):\n#                 print(filepath)\n#                 return False\n#         return True\n#\n#     def _download(self):\n#         import tarfile\n#\n#         if self._check_integrity():\n#             print(\'Files already downloaded and verified\')\n#             return\n#\n#         download_url(self.url, self.root, self.filename, self.tgz_md5)\n#\n#         with tarfile.open(os.path.join(self.root, self.filename), ""r:gz"") as tar:\n#             tar.extractall(path=self.root)\n#\n#     def __len__(self):\n#         return len(self.data)\n#\n#     def __getitem__(self, idx):\n#         sample = self.data.iloc[idx]\n#         path = os.path.join(self.root, self.base_folder, sample.filepath)\n#         target = sample.target - 1  # Targets start at 1 by default, so shift to 0\n#         img_id = sample.img_id\n#         img = self.loader(path)\n#\n#         if self.transform is not None:\n#             img = self.transform(img)\n#\n#         if self.with_id:\n#             return img, target, img_id\n#         else:\n#             return img, target\n\n\nclass Cub2011Rot(Dataset):\n    base_folder = \'CUB_200_2011/images\'\n    url = \'http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz\'\n    filename = \'CUB_200_2011.tgz\'\n    tgz_md5 = \'97eceeb196236b17998738112f37df78\'\n\n    def __init__(self, root, train=True, transform=None, download=True, with_id=False, num_tf=3):\n        self.root = os.path.expanduser(root)\n        self.transform = transform\n        self.loader = default_loader\n        self.train = train\n        self.with_id = with_id\n        self.num_tf = num_tf\n\n        self.normalize = transforms.Compose([transforms.ToTensor(),\n                                             transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                                  std=[0.229, 0.224, 0.225])])\n\n        if download:\n            self._download()\n\n        if not self._check_integrity():\n            raise RuntimeError(\'Dataset not found or corrupted.\' +\n                               \' You can use download=True to download it\')\n\n    def _load_metadata(self):\n        images = pd.read_csv(os.path.join(self.root, \'CUB_200_2011\', \'images.txt\'), sep=\' \',\n                             names=[\'img_id\', \'filepath\'])\n        image_class_labels = pd.read_csv(os.path.join(self.root, \'CUB_200_2011\', \'image_class_labels.txt\'),\n                                         sep=\' \', names=[\'img_id\', \'target\'])\n        train_test_split = pd.read_csv(os.path.join(self.root, \'CUB_200_2011\', \'train_test_split.txt\'),\n                                       sep=\' \', names=[\'img_id\', \'is_training_img\'])\n\n        data = images.merge(image_class_labels, on=\'img_id\')\n        self.data = data.merge(train_test_split, on=\'img_id\')\n\n        if self.train:\n            self.data = self.data[self.data.is_training_img == 1]\n        else:\n            self.data = self.data[self.data.is_training_img == 0]\n\n    def _check_integrity(self):\n        try:\n            self._load_metadata()\n        except Exception:\n            return False\n\n        for index, row in self.data.iterrows():\n            filepath = os.path.join(self.root, self.base_folder, row.filepath)\n            if not os.path.isfile(filepath):\n                print(filepath)\n                return False\n        return True\n\n    def _download(self):\n        import tarfile\n\n        if self._check_integrity():\n            print(\'Files already downloaded and verified\')\n            return\n\n        download_url(self.url, self.root, self.filename, self.tgz_md5)\n\n        with tarfile.open(os.path.join(self.root, self.filename), ""r:gz"") as tar:\n            tar.extractall(path=self.root)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data.iloc[idx]\n        path = os.path.join(self.root, self.base_folder, sample.filepath)\n        # target = sample.target - 1  # Targets start at 1 by default, so shift to 0\n        img_id = sample.img_id\n        img = self.loader(path)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        img = self.normalize(img)\n\n        rot = np.random.choice(4)\n\n        img_rot = img.rot90(rot, (1, 2))\n\n        if self.with_id:\n            return img, img_rot, rot, img_id\n        else:\n            return img, img_rot, rot\n\n\nclass Cub2011transform(Dataset):\n    base_folder = \'CUB_200_2011/images\'\n    url = \'http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz\'\n    filename = \'CUB_200_2011.tgz\'\n    tgz_md5 = \'97eceeb196236b17998738112f37df78\'\n\n    def __init__(self, root, train=True, transform=None, download=True, with_id=False, num_tf=3):\n        self.root = os.path.expanduser(root)\n        self.transform = transform\n        self.loader = default_loader\n        self.train = train\n        self.with_id = with_id\n        self.num_tf = num_tf\n\n        self.normalize = transforms.Compose([transforms.ToTensor(),\n                                             transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                                  std=[0.229, 0.224, 0.225])])\n\n        if download:\n            self._download()\n\n        if not self._check_integrity():\n            raise RuntimeError(\'Dataset not found or corrupted.\' +\n                               \' You can use download=True to download it\')\n\n    def _load_metadata(self):\n        images = pd.read_csv(os.path.join(self.root, \'CUB_200_2011\', \'images.txt\'), sep=\' \',\n                             names=[\'img_id\', \'filepath\'])\n        image_class_labels = pd.read_csv(os.path.join(self.root, \'CUB_200_2011\', \'image_class_labels.txt\'),\n                                         sep=\' \', names=[\'img_id\', \'target\'])\n        train_test_split = pd.read_csv(os.path.join(self.root, \'CUB_200_2011\', \'train_test_split.txt\'),\n                                       sep=\' \', names=[\'img_id\', \'is_training_img\'])\n\n        data = images.merge(image_class_labels, on=\'img_id\')\n        self.data = data.merge(train_test_split, on=\'img_id\')\n\n        if self.train:\n            self.data = self.data[self.data.is_training_img == 1]\n        else:\n            self.data = self.data[self.data.is_training_img == 0]\n\n    def _check_integrity(self):\n        try:\n            self._load_metadata()\n        except Exception:\n            return False\n\n        for index, row in self.data.iterrows():\n            filepath = os.path.join(self.root, self.base_folder, row.filepath)\n            if not os.path.isfile(filepath):\n                print(filepath)\n                return False\n        return True\n\n    def _download(self):\n        import tarfile\n\n        if self._check_integrity():\n            print(\'Files already downloaded and verified\')\n            return\n\n        download_url(self.url, self.root, self.filename, self.tgz_md5)\n\n        with tarfile.open(os.path.join(self.root, self.filename), ""r:gz"") as tar:\n            tar.extractall(path=self.root)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data.iloc[idx]\n        path = os.path.join(self.root, self.base_folder, sample.filepath)\n        target = sample.target - 1  # Targets start at 1 by default, so shift to 0\n        img_id = sample.img_id\n        img = self.loader(path)\n\n        if self.transform is not None:\n            img = self.transform(img)\n        imgtf = img.copy()\n\n        use_tf = np.random.binomial(1, 0.5, self.num_tf)\n\n        signs = np.random.choice([1, -1], size=3)\n\n        rot = signs[0] * np.random.randint(30, 180)\n        translate = signs[1] * np.random.randint(10, 60)\n        shear = signs[2] * np.random.randint(10, 30)\n\n        use_tf[1] = 0\n\n        if use_tf[0]:\n            imgtf = vF.affine(imgtf, rot, (0, 0), 1.0, 0, 0)\n        if use_tf[1]:\n            imgtf = vF.affine(imgtf, 0, (translate, translate), 1.0, 0, 0)\n        if use_tf[2]:\n            imgtf = vF.affine(imgtf, 0, (0, 0), 1.0, shear, 0)\n\n        img = self.normalize(img)\n        imgtf = self.normalize(imgtf)\n\n        if self.with_id:\n            return img, imgtf, target, use_tf, torch.Tensor([rot, translate, shear] * use_tf), img_id\n        else:\n            return img, imgtf, target, use_tf, torch.Tensor([rot, translate, shear] * use_tf)\n\n\nclass Cub2011(Dataset):\n    base_folder = \'CUB_200_2011/images\'\n    url = \'http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz\'\n    filename = \'CUB_200_2011.tgz\'\n    tgz_md5 = \'97eceeb196236b17998738112f37df78\'\n\n    def __init__(self, root, train=True, label=True, transform=None, download=True, with_id=False, n_labels=2000, mode=\'semi\'):\n        self.root = os.path.expanduser(root)\n        self.transform = transform\n        self.loader = default_loader\n        self.train = train\n        self.label = label\n        self.with_id = with_id\n        self.n_labels = n_labels\n        self.mode = mode\n\n        if download:\n            self._download()\n\n        if self.train:\n            self._train_split(n_labels)\n        if not self._check_integrity():\n            raise RuntimeError(\'Dataset not found or corrupted.\' +\n                               \' You can use download=True to download it\')\n\n    def _train_split(self, n_labels):\n\n        images = pd.read_csv(os.path.join(self.root, \'CUB_200_2011\', \'images.txt\'), sep=\' \',\n                             names=[\'img_id\', \'filepath\'])\n        train_list = pd.read_csv(os.path.join(self.root, \'CUB_200_2011\', \'train_list.txt\'), sep=\';\',\n                                 names=[\'filepath\', \'target\'])\n        data = images.merge(train_list, on=\'filepath\')\n\n        labeled_train_list = []\n        unlabeled_train_list = []\n        for i in range(200):\n            cur_data = data.loc[(data[\'target\'] == i), :]\n            cur_labeled_train_data = cur_data.iloc[:n_labels // 200, :]\n            cur_unlabeled_train_data = cur_data.iloc[n_labels // 200:, :]\n            labeled_train_list.append(cur_labeled_train_data)\n            unlabeled_train_list.append(cur_unlabeled_train_data)\n\n        labeled_df = pd.concat(labeled_train_list)\n        unlabeled_df = pd.concat(unlabeled_train_list)\n\n        labeled_df.to_csv(os.path.join(self.root, \'CUB_200_2011\', \'labeled_train_list.txt\'),\n                          sep=\' \',\n                          header=False,\n                          index=False)\n        unlabeled_df.to_csv(os.path.join(self.root, \'CUB_200_2011\', \'unlabeled_train_list.txt\'),\n                            sep=\' \',\n                            header=False,\n                            index=False)\n\n    def _load_metadata(self):\n        images = pd.read_csv(os.path.join(self.root, \'CUB_200_2011\', \'images.txt\'), sep=\' \',\n                             names=[\'img_id\', \'filepath\'])\n        image_class_labels = pd.read_csv(os.path.join(self.root, \'CUB_200_2011\', \'image_class_labels.txt\'),\n                                         sep=\' \', names=[\'img_id\', \'target\'])\n        train_test_split = pd.read_csv(os.path.join(self.root, \'CUB_200_2011\', \'train_test_split.txt\'),\n                                       sep=\' \', names=[\'img_id\', \'is_training_img\'])\n\n        data = images.merge(image_class_labels, on=\'img_id\')\n        self.data = data.merge(train_test_split, on=\'img_id\')\n\n        if self.train and self.label and self.mode == \'semi\':\n            self.data = pd.read_csv(os.path.join(self.root, \'CUB_200_2011\', \'labeled_train_list.txt\'),\n                                    sep=\' \',\n                                    names=[\'img_id\', \'filepath\', \'target\'])\n        elif self.train and self.mode == \'semi\':\n            self.data = pd.read_csv(os.path.join(self.root, \'CUB_200_2011\', \'unlabeled_train_list.txt\'),\n                                    sep=\' \',\n                                    names=[\'img_id\', \'filepath\', \'target\'])\n        elif self.train and self.mode == \'full\':\n            self.data = self.data[self.data.is_training_img == 1]\n        else:\n            self.data = self.data[self.data.is_training_img == 0]\n\n    def _check_integrity(self):\n        try:\n            self._load_metadata()\n        except Exception:\n            return False\n\n        for index, row in self.data.iterrows():\n            filepath = os.path.join(self.root, self.base_folder, row.filepath)\n            if not os.path.isfile(filepath):\n                print(filepath)\n                return False\n        return True\n\n    def _download(self):\n        import tarfile\n\n        if self._check_integrity():\n            print(\'Files already downloaded and verified\')\n            return\n\n        download_url(self.url, self.root, self.filename, self.tgz_md5)\n\n        with tarfile.open(os.path.join(self.root, self.filename), ""r:gz"") as tar:\n            tar.extractall(path=self.root)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data.iloc[idx]\n        path = os.path.join(self.root, self.base_folder, sample.filepath)\n        if self.train and self.mode == \'semi\':\n            target = sample.target\n        else:\n            target = sample.target - 1  # Targets start at 1 by default, so shift to 0\n        img_id = sample.img_id\n        img = self.loader(path)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.with_id:\n            return img, target, img_id\n        else:\n            return img, target\n\n\nclass TransformTwice:\n    def __init__(self, transform):\n        self.transform = transform\n\n    def __call__(self, inp):\n        out1 = self.transform(inp)\n        out2 = self.transform(inp)\n        return out1, out2\n\n\ndef get_cub200(root, n_labeled=2000, transform_train=None, transform_val=None, mode=\'semi\'):\n    train_labeled_dataset = Cub2011(root, train=True, label=True, transform=transform_train, n_labels=n_labeled, mode=mode)\n    if mode == \'semi\':\n        train_unlabeled_dataset = Cub2011(root, train=True, label=False, transform=TransformTwice(transform_train), n_labels=n_labeled, mode=mode)\n    else:\n        train_unlabeled_dataset = []\n    val_dataset = Cub2011(root, train=False, label=False, transform=transform_val, with_id=True, mode=mode)\n\n    print(""#Labeled: {} #Unlabeled: {} #Val: {}"".format(len(train_labeled_dataset), len(train_unlabeled_dataset), len(val_dataset)))\n    return train_labeled_dataset, train_unlabeled_dataset, val_dataset\n'"
code/datasets/datasetgetter.py,0,"b'# from torchvision.datasets import CIFAR10, CIFAR100, ImageFolder\nfrom datasets.testfolder import ImageFolderWithPath\nfrom datasets.cub200 import Cub2011\nimport os\nimport torchvision.transforms as transforms\nfrom datasets.dogs import Dogs\nfrom datasets.cars import Cars\nfrom datasets.photoimage import PhotoData\n\n\ndef get_dataset(dataset, args):\n    if dataset.lower() == \'cub\':\n        print(\'USE CUB DATASET\')\n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n        transforms_train = transforms.Compose([transforms.Resize((256, 256)),\n                                               transforms.RandomCrop(224, padding=4, padding_mode=\'reflect\'),\n                                               transforms.RandomHorizontalFlip(),\n                                               transforms.ToTensor(),\n                                               normalize])\n        transforms_val = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), normalize])\n\n        train_dataset = Cub2011(os.path.join(args.data_dir, args.dataset),\n                                transform=transforms_train, download=False, mode=\'full\')\n        val_dataset = Cub2011(os.path.join(args.data_dir, args.dataset),\n                              transform=transforms_val, train=False, with_id=True, mode=\'full\')\n    elif dataset.lower() == \'imagenet\':\n        print(""USE IMAGENET"")\n        datapath = args.data_dir\n        traindir = os.path.join(datapath, \'train\')\n        valdir = os.path.join(datapath, \'val\')\n\n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n        train_dataset = ImageFolderWithPath(traindir, transforms.Compose([transforms.RandomResizedCrop(224),\n                                                                 transforms.RandomHorizontalFlip(),\n                                                                 transforms.ToTensor(), normalize, ]), with_name=False)\n        val_dataset = ImageFolderWithPath(valdir, transforms.Compose([transforms.Resize((224, 224)),\n                                                              transforms.ToTensor(),\n                                                              normalize, ]), with_name=True)\n    elif dataset.lower() == \'cifar10\':\n        print(\'NOT IMPLEMENTED DATASET :\', dataset)\n        exit(-3)\n    elif dataset.lower() == \'cifar100\':\n        print(\'NOT IMPLEMENTED DATASET :\', dataset)\n        exit(-3)\n    elif dataset.lower() == \'cars\':\n        print(\'USE CARS DATASET\')\n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n\n        transforms_train = transforms.Compose([transforms.Resize((256, 256)),\n                                               transforms.RandomCrop(224, padding=4, padding_mode=\'reflect\'),\n                                               # transforms.RandomHorizontalFlip(),\n                                               transforms.ToTensor(),\n                                               normalize])\n        transforms_val = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), normalize])\n\n        train_dataset = Cars(args.data_dir, True, transforms_train, False)\n        val_dataset = Cars(args.data_dir, False, transforms_val, True)\n\n    elif dataset.lower() == \'dogs\':\n        print(\'USE DOGS DATASET\')\n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n\n        transforms_train = transforms.Compose([transforms.Resize((256, 256)),\n                                               transforms.RandomCrop(224, padding=4, padding_mode=\'reflect\'),\n                                               # transforms.RandomHorizontalFlip(),\n                                               transforms.ToTensor(),\n                                               normalize])\n        transforms_val = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), normalize])\n\n        train_dataset = Dogs(args.data_dir, transform=transforms_train, with_id=False)\n        val_dataset = Dogs(args.data_dir, transform=transforms_val, with_id=True, train=False)\n    elif dataset.lower() == \'photo\':\n        print(\'USE PHOTO DATASET\')\n        normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                         std=[0.5, 0.5, 0.5])\n        transforms_train = transforms.Compose([transforms.Resize((args.img_size_max, args.img_size_max)),\n                                               transforms.ToTensor(),\n                                               normalize])\n        transforms_val = transforms.Compose([transforms.Resize((args.img_size_max, args.img_size_max)),\n                                             transforms.ToTensor(),\n                                             normalize])\n\n        train_dataset = PhotoData(args.data_dir, True, transform=transforms_train, img_to_use=args.img_to_use)\n        val_dataset = PhotoData(args.data_dir, False, transform=transforms_val, img_to_use=args.img_to_use)\n\n        if train_dataset.randidx != -999:\n            args.img_to_use = train_dataset.randidx\n\n    else:\n        print(\'NOT IMPLEMENTED DATASET :\', dataset)\n        exit(-3)\n\n    return train_dataset, val_dataset\n'"
code/datasets/dogs.py,1,"b'# https://github.com/zrsmithson/Stanford-dogs/blob/master/data/stanford_dogs_data.py\nfrom __future__ import print_function\nfrom datasets.cub200 import pil_loader\nfrom os.path import join\nimport os\nimport scipy.io\n\nimport torch.utils.data as data\nfrom torchvision.datasets.utils import download_url, list_dir, list_files\nimport numpy as np\n\n\nclass Dogs(data.Dataset):\n    """"""`Stanford Dogs <http://vision.stanford.edu/aditya86/ImageNetDogs/>`_ Dataset.\n    Args:\n        root (string): Root directory of dataset where directory\n            ``omniglot-py`` exists.\n        cropped (bool, optional): If true, the images will be cropped into the bounding box specified\n            in the annotations\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        download (bool, optional): If true, downloads the dataset tar files from the internet and\n            puts it in root directory. If the tar files are already downloaded, they are not\n            downloaded again.\n    """"""\n    folder = \'stanfordDogs\'\n    download_url_prefix = \'http://vision.stanford.edu/aditya86/ImageNetDogs\'\n\n    def __init__(self,\n                 root,\n                 train=True,\n                 transform=None,\n                 download=False,\n                 with_id=False):\n\n        self.root = join(os.path.expanduser(root), self.folder)\n        self.train = train\n        self.transform = transform\n        self.with_id = with_id\n\n        if download:\n            self.download()\n\n        split = self.load_split()\n\n        self.images_folder = join(self.root, \'Images\')\n        self.annotations_folder = join(self.root, \'Annotation\')\n        self._breeds = list_dir(self.images_folder)\n\n        self._breed_annotations = [[(annotation, box, size, idx)\n                                    for box, size in self.get_boxes(join(self.annotations_folder, annotation))]\n                                    for annotation, idx in split]\n        self._flat_breed_annotations = sum(self._breed_annotations, [])\n\n        if with_id:\n            self._flat_breed_images = [(annotation + \'.jpg\', annotation, idx)\n                                       for annotation, box, size, idx in self._flat_breed_annotations]\n        else:\n            self._flat_breed_images = [(annotation+\'.jpg\', idx)\n                                       for annotation, box, size, idx in self._flat_breed_annotations]\n\n        self.classes = self.get_classes()\n\n    def __len__(self):\n        return len(self._flat_breed_images)\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (image, target) where target is index of the target character class.\n        """"""\n        if self.with_id:\n            image_name, image_id, target_class = self._flat_breed_images[index]\n        else:\n            image_name, target_class = self._flat_breed_images[index]\n        image_path = join(self.images_folder, image_name)\n        image = pil_loader(image_path)\n\n        if self.transform:\n            image = self.transform(image)\n\n        if self.with_id:\n            return image, target_class, image_id\n        else:\n            return image, target_class\n\n    def download(self):\n        import tarfile\n\n        if os.path.exists(join(self.root, \'Images\')) and os.path.exists(join(self.root, \'Annotation\')):\n            if len(os.listdir(join(self.root, \'Images\'))) == len(os.listdir(join(self.root, \'Annotation\'))) == 120:\n                print(\'Files already downloaded and verified\')\n                return\n\n        for filename in [\'images\', \'annotation\', \'lists\']:\n            tar_filename = filename + \'.tar\'\n            url = self.download_url_prefix + \'/\' + tar_filename\n            download_url(url, self.root, tar_filename, None)\n            print(\'Extracting downloaded file: \' + join(self.root, tar_filename))\n            with tarfile.open(join(self.root, tar_filename), \'r\') as tar_file:\n                tar_file.extractall(self.root)\n            os.remove(join(self.root, tar_filename))\n\n    def load_bboxes(self, img_size=224):\n        bboxes = {annotation: [box, size] for annotation, box, size, _ in self._flat_breed_annotations}\n        for key, val in bboxes.items():\n            x1, y1, x2, y2 = bboxes[key][0]\n            w, h = bboxes[key][1]\n            x_scale = img_size / w\n            y_scale = img_size / h\n\n            x_new = int(np.round(x1 * x_scale))\n            y_new = int(np.round(y1 * y_scale))\n            x_max = int(np.round(x2 * x_scale))\n            y_max = int(np.round(y2 * y_scale))\n\n            bboxes[key] = [x_new, y_new, x_max, y_max]\n        return bboxes\n\n    @staticmethod\n    def get_boxes(path):\n        import xml.etree.ElementTree\n        e = xml.etree.ElementTree.parse(path).getroot()\n        boxes = []\n        sizes = []\n        for objs in e.iter(\'object\'):\n            boxes.append([int(objs.find(\'bndbox\').find(\'xmin\').text),\n                          int(objs.find(\'bndbox\').find(\'ymin\').text),\n                          int(objs.find(\'bndbox\').find(\'xmax\').text),\n                          int(objs.find(\'bndbox\').find(\'ymax\').text)])\n        for size in e.iter(\'size\'):\n            sizes.append([int(size.find(\'width\').text),\n                          int(size.find(\'height\').text)])\n        return list(zip(boxes, sizes))\n\n    def load_split(self):\n        if self.train:\n            split = scipy.io.loadmat(join(self.root, \'train_list.mat\'))[\'annotation_list\']\n            labels = scipy.io.loadmat(join(self.root, \'train_list.mat\'))[\'labels\']\n        else:\n            split = scipy.io.loadmat(join(self.root, \'test_list.mat\'))[\'annotation_list\']\n            labels = scipy.io.loadmat(join(self.root, \'test_list.mat\'))[\'labels\']\n\n        split = [item[0][0] for item in split]\n        labels = [item[0]-1 for item in labels]\n        return list(zip(split, labels))\n\n    def stats(self):\n        counts = {}\n        for index in range(len(self._flat_breed_images)):\n            image_name, target_class = self._flat_breed_images[index]\n            if target_class not in counts.keys():\n                counts[target_class] = 1\n            else:\n                counts[target_class] += 1\n\n        print(""%d samples spanning %d classes (avg %f per class)""%(len(self._flat_breed_images), len(counts.keys()), float(len(self._flat_breed_images))/float(len(counts.keys()))))\n\n        return counts\n\n    def get_classes(self):\n        return [""Chihuaha"",\n                        ""Japanese Spaniel"",\n                        ""Maltese Dog"",\n                        ""Pekinese"",\n                        ""Shih-Tzu"",\n                        ""Blenheim Spaniel"",\n                        ""Papillon"",\n                        ""Toy Terrier"",\n                        ""Rhodesian Ridgeback"",\n                        ""Afghan Hound"",\n                        ""Basset Hound"",\n                        ""Beagle"",\n                        ""Bloodhound"",\n                        ""Bluetick"",\n                        ""Black-and-tan Coonhound"",\n                        ""Walker Hound"",\n                        ""English Foxhound"",\n                        ""Redbone"",\n                        ""Borzoi"",\n                        ""Irish Wolfhound"",\n                        ""Italian Greyhound"",\n                        ""Whippet"",\n                        ""Ibizian Hound"",\n                        ""Norwegian Elkhound"",\n                        ""Otterhound"",\n                        ""Saluki"",\n                        ""Scottish Deerhound"",\n                        ""Weimaraner"",\n                        ""Staffordshire Bullterrier"",\n                        ""American Staffordshire Terrier"",\n                        ""Bedlington Terrier"",\n                        ""Border Terrier"",\n                        ""Kerry Blue Terrier"",\n                        ""Irish Terrier"",\n                        ""Norfolk Terrier"",\n                        ""Norwich Terrier"",\n                        ""Yorkshire Terrier"",\n                        ""Wirehaired Fox Terrier"",\n                        ""Lakeland Terrier"",\n                        ""Sealyham Terrier"",\n                        ""Airedale"",\n                        ""Cairn"",\n                        ""Australian Terrier"",\n                        ""Dandi Dinmont"",\n                        ""Boston Bull"",\n                        ""Miniature Schnauzer"",\n                        ""Giant Schnauzer"",\n                        ""Standard Schnauzer"",\n                        ""Scotch Terrier"",\n                        ""Tibetan Terrier"",\n                        ""Silky Terrier"",\n                        ""Soft-coated Wheaten Terrier"",\n                        ""West Highland White Terrier"",\n                        ""Lhasa"",\n                        ""Flat-coated Retriever"",\n                        ""Curly-coater Retriever"",\n                        ""Golden Retriever"",\n                        ""Labrador Retriever"",\n                        ""Chesapeake Bay Retriever"",\n                        ""German Short-haired Pointer"",\n                        ""Vizsla"",\n                        ""English Setter"",\n                        ""Irish Setter"",\n                        ""Gordon Setter"",\n                        ""Brittany"",\n                        ""Clumber"",\n                        ""English Springer Spaniel"",\n                        ""Welsh Springer Spaniel"",\n                        ""Cocker Spaniel"",\n                        ""Sussex Spaniel"",\n                        ""Irish Water Spaniel"",\n                        ""Kuvasz"",\n                        ""Schipperke"",\n                        ""Groenendael"",\n                        ""Malinois"",\n                        ""Briard"",\n                        ""Kelpie"",\n                        ""Komondor"",\n                        ""Old English Sheepdog"",\n                        ""Shetland Sheepdog"",\n                        ""Collie"",\n                        ""Border Collie"",\n                        ""Bouvier des Flandres"",\n                        ""Rottweiler"",\n                        ""German Shepard"",\n                        ""Doberman"",\n                        ""Miniature Pinscher"",\n                        ""Greater Swiss Mountain Dog"",\n                        ""Bernese Mountain Dog"",\n                        ""Appenzeller"",\n                        ""EntleBucher"",\n                        ""Boxer"",\n                        ""Bull Mastiff"",\n                        ""Tibetan Mastiff"",\n                        ""French Bulldog"",\n                        ""Great Dane"",\n                        ""Saint Bernard"",\n                        ""Eskimo Dog"",\n                        ""Malamute"",\n                        ""Siberian Husky"",\n                        ""Affenpinscher"",\n                        ""Basenji"",\n                        ""Pug"",\n                        ""Leonberg"",\n                        ""Newfoundland"",\n                        ""Great Pyrenees"",\n                        ""Samoyed"",\n                        ""Pomeranian"",\n                        ""Chow"",\n                        ""Keeshond"",\n                        ""Brabancon Griffon"",\n                        ""Pembroke"",\n                        ""Cardigan"",\n                        ""Toy Poodle"",\n                        ""Miniature Poodle"",\n                        ""Standard Poodle"",\n                        ""Mexican Hairless"",\n                        ""Dingo"",\n                        ""Dhole"",\n                        ""African Hunting Dog""]'"
code/datasets/photoimage.py,1,"b""import torch.utils.data as data\nimport numpy as np\nimport os\nfrom datasets.cub200 import pil_loader\nfrom glob import glob\n\n\nclass PhotoData(data.Dataset):\n\n    def __init__(self, data_dir, is_train, transform=None, singlemode=True, img_to_use=-999):\n        self.data_dir = os.path.join(data_dir, 'SinGANdata')\n        self.transform = transform\n        self.singlemode = singlemode\n        self.randidx = -999\n\n        if is_train:\n            self.image_dir = os.path.join(self.data_dir, 'trainPhoto')\n        else:\n            self.image_dir = os.path.join(self.data_dir, 'testPhoto')\n\n        if singlemode:\n            if img_to_use == -999:\n                self.num_data = len(glob(os.path.join(self.image_dir, '*.jpg')))\n                randidx = np.random.randint(0, self.num_data)\n                self.randidx = randidx\n                self.image_dir = sorted(glob(os.path.join(self.image_dir, '*.jpg')))[randidx]\n            else:\n                self.image_dir = sorted(glob(os.path.join(self.image_dir, '*.jpg')))[img_to_use]\n        else:\n            self.image_dir = sorted(glob(os.path.join(self.image_dir, '*.jpg')))\n\n    def __len__(self):\n        return len(self.image_dir)\n\n    def __getitem__(self, idx):\n        if self.singlemode:\n            img = pil_loader(self.image_dir)\n        else:\n            img = pil_loader(self.image_dir[idx])\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img\n"""
code/datasets/testfolder.py,1,"b'import torch.utils.data as data\n\nfrom PIL import Image\n\nimport os\nimport os.path\nimport sys\n\n\ndef has_file_allowed_extension(filename, extensions):\n    """"""Checks if a file is an allowed extension.\n\n    Args:\n        filename (string): path to a file\n        extensions (iterable of strings): extensions to consider (lowercase)\n\n    Returns:\n        bool: True if the filename ends with one of given extensions\n    """"""\n    filename_lower = filename.lower()\n    return any(filename_lower.endswith(ext) for ext in extensions)\n\n\ndef is_image_file(filename):\n    """"""Checks if a file is an allowed image extension.\n\n    Args:\n        filename (string): path to a file\n\n    Returns:\n        bool: True if the filename ends with a known image extension\n    """"""\n    return has_file_allowed_extension(filename, IMG_EXTENSIONS)\n\n\ndef make_dataset(dir, class_to_idx, extensions):\n    images = []\n    dir = os.path.expanduser(dir)\n    for target in sorted(class_to_idx.keys()):\n        d = os.path.join(dir, target)\n        if not os.path.isdir(d):\n            continue\n\n        for root, _, fnames in sorted(os.walk(d)):\n            for fname in sorted(fnames):\n                if has_file_allowed_extension(fname, extensions):\n                    path = os.path.join(root, fname)\n                    item = (path, class_to_idx[target])\n                    images.append(item)\n\n    return images\n\n\nclass DatasetFolder(data.Dataset):\n    """"""A generic data loader where the samples are arranged in this way: ::\n\n        root/class_x/xxx.ext\n        root/class_x/xxy.ext\n        root/class_x/xxz.ext\n\n        root/class_y/123.ext\n        root/class_y/nsdf3.ext\n        root/class_y/asd932_.ext\n\n    Args:\n        root (string): Root directory path.\n        loader (callable): A function to load a sample given its path.\n        extensions (list[string]): A list of allowed extensions.\n        transform (callable, optional): A function/transform that takes in\n            a sample and returns a transformed version.\n            E.g, ``transforms.RandomCrop`` for images.\n        target_transform (callable, optional): A function/transform that takes\n            in the target and transforms it.\n\n     Attributes:\n        classes (list): List of the class names.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        samples (list): List of (sample path, class_index) tuples\n        targets (list): The class_index value for each image in the dataset\n    """"""\n\n    def __init__(self, root, loader, extensions, transform=None, target_transform=None):\n        classes, class_to_idx = self._find_classes(root)\n        samples = make_dataset(root, class_to_idx, extensions)\n        if len(samples) == 0:\n            raise(RuntimeError(""Found 0 files in subfolders of: "" + root + ""\\n""\n                               ""Supported extensions are: "" + "","".join(extensions)))\n\n        self.root = root\n        self.loader = loader\n        self.extensions = extensions\n\n        self.classes = classes\n        self.class_to_idx = class_to_idx\n        self.samples = samples\n        self.targets = [s[1] for s in samples]\n\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def _find_classes(self, dir):\n        """"""\n        Finds the class folders in a dataset.\n\n        Args:\n            dir (string): Root directory path.\n\n        Returns:\n            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n\n        Ensures:\n            No class is a subdirectory of another.\n        """"""\n        if sys.version_info >= (3, 5):\n            # Faster and available in Python 3.5 and above\n            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n        else:\n            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n        classes.sort()\n        class_to_idx = {classes[i]: i for i in range(len(classes))}\n        return classes, class_to_idx\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (sample, target) where target is class_index of the target class.\n        """"""\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        imgname = path.split(\'/\')[-1].replace(\'.JPEG\', \'\')\n        return sample, target, imgname\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __repr__(self):\n        fmt_str = \'Dataset \' + self.__class__.__name__ + \'\\n\'\n        fmt_str += \'    Number of datapoints: {}\\n\'.format(self.__len__())\n        fmt_str += \'    Root Location: {}\\n\'.format(self.root)\n        tmp = \'    Transforms (if any): \'\n        fmt_str += \'{0}{1}\\n\'.format(tmp, self.transform.__repr__().replace(\'\\n\', \'\\n\' + \' \' * len(tmp)))\n        tmp = \'    Target Transforms (if any): \'\n        fmt_str += \'{0}{1}\'.format(tmp, self.target_transform.__repr__().replace(\'\\n\', \'\\n\' + \' \' * len(tmp)))\n        return fmt_str\n\n\nIMG_EXTENSIONS = [\'.jpg\', \'.jpeg\', \'.png\', \'.ppm\', \'.bmp\', \'.pgm\', \'.tif\', \'.tiff\', \'webp\']\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        img = Image.open(f)\n        return img.convert(\'RGB\')\n\n\ndef accimage_loader(path):\n    import accimage\n    try:\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef default_loader(path):\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        return accimage_loader(path)\n    else:\n        return pil_loader(path)\n\n\nclass ImageFolderWithPath(DatasetFolder):\n    """"""A generic data loader where the images are arranged in this way: ::\n\n        root/dog/xxx.png\n        root/dog/xxy.png\n        root/dog/xxz.png\n\n        root/cat/123.png\n        root/cat/nsdf3.png\n        root/cat/asd932_.png\n\n    Args:\n        root (string): Root directory path.\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        loader (callable, optional): A function to load an image given its path.\n\n     Attributes:\n        classes (list): List of the class names.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        imgs (list): List of (image path, class_index) tuples\n    """"""\n    def __init__(self, root, transform=None, target_transform=None,\n                 loader=default_loader, with_name=False):\n        super(ImageFolderWithPath, self).__init__(root, loader, IMG_EXTENSIONS,\n                                          transform=transform,\n                                          target_transform=target_transform)\n        self.with_name = with_name\n        self.imgs = self.samples\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (sample, target) where target is class_index of the target class.\n        """"""\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        if self.with_name:\n            imgname = path.split(\'/\')[-1].replace(\'.JPEG\', \'\')\n            return sample, target, imgname\n        else:\n            return sample, target\n'"
code/models/discriminator.py,0,"b'from torch import nn\n\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.nf = 32\n        self.current_scale = 0\n\n        self.sub_discriminators = nn.ModuleList()\n\n        first_discriminator = nn.ModuleList()\n\n        first_discriminator.append(nn.Sequential(nn.Conv2d(3, self.nf, 3, 1, 1),\n                                             nn.LeakyReLU(2e-1)))\n        for _ in range(3):\n            first_discriminator.append(nn.Sequential(nn.Conv2d(self.nf, self.nf, 3, 1, 1),\n                                                 nn.BatchNorm2d(self.nf),\n                                                 nn.LeakyReLU(2e-1)))\n\n        first_discriminator.append(nn.Sequential(nn.Conv2d(self.nf, 1, 3, 1, 1)))\n\n        first_discriminator = nn.Sequential(*first_discriminator)\n\n        self.sub_discriminators.append(first_discriminator)\n\n    def forward(self, x):\n        out = self.sub_discriminators[self.current_scale](x)\n        return out\n\n    def progress(self):\n        self.current_scale += 1\n        # Lower scale discriminators are not used in later ... replace append to assign?\n        if self.current_scale % 4 == 0:\n            self.nf *= 2\n\n        tmp_discriminator = nn.ModuleList()\n        tmp_discriminator.append(nn.Sequential(nn.Conv2d(3, self.nf, 3, 1, 1),\n                                               nn.LeakyReLU(2e-1)))\n\n        for _ in range(3):\n            tmp_discriminator.append(nn.Sequential(nn.Conv2d(self.nf, self.nf, 3, 1, 1),\n                                                   nn.BatchNorm2d(self.nf),\n                                                   nn.LeakyReLU(2e-1)))\n\n        tmp_discriminator.append(nn.Sequential(nn.Conv2d(self.nf, 1, 3, 1, 1)))\n\n        tmp_discriminator = nn.Sequential(*tmp_discriminator)\n\n        if self.current_scale % 4 != 0:\n            prev_discriminator = self.sub_discriminators[-1]\n\n            # Initialize layers via copy\n            if self.current_scale >= 1:\n                tmp_discriminator.load_state_dict(prev_discriminator.state_dict())\n\n        self.sub_discriminators.append(tmp_discriminator)\n        print(""DISCRIMINATOR PROGRESSION DONE"")\n'"
code/models/generator.py,1,"b'from torch import nn\nfrom torch.nn import functional as F\nimport torch\n\n\nclass Generator(nn.Module):\n    def __init__(self, img_size_min, num_scale, scale_factor=4/3):\n        super(Generator, self).__init__()\n        self.img_size_min = img_size_min\n        self.scale_factor = scale_factor\n        self.num_scale = num_scale\n        self.nf = 32\n        self.current_scale = 0\n\n        self.size_list = [int(self.img_size_min * scale_factor**i) for i in range(num_scale + 1)]\n        print(self.size_list)\n\n        self.sub_generators = nn.ModuleList()\n\n        first_generator = nn.ModuleList()\n\n        first_generator.append(nn.Sequential(nn.Conv2d(3, self.nf, 3, 1),\n                                             nn.BatchNorm2d(self.nf),\n                                             nn.LeakyReLU(2e-1)))\n        for _ in range(3):\n            first_generator.append(nn.Sequential(nn.Conv2d(self.nf, self.nf, 3, 1),\n                                                 nn.BatchNorm2d(self.nf),\n                                                 nn.LeakyReLU(2e-1)))\n\n        first_generator.append(nn.Sequential(nn.Conv2d(self.nf, 3, 3, 1),\n                                             nn.Tanh()))\n\n        first_generator = nn.Sequential(*first_generator)\n\n        self.sub_generators.append(first_generator)\n\n    def forward(self, z, img=None):\n        x_list = []\n        x_first = self.sub_generators[0](z[0])\n        x_list.append(x_first)\n\n        if img is not None:\n            x_inter = img\n        else:\n            x_inter = x_first\n\n        for i in range(1, self.current_scale + 1):\n            x_inter = F.interpolate(x_inter, (self.size_list[i], self.size_list[i]), mode=\'bilinear\', align_corners=True)\n            x_prev = x_inter\n            x_inter = F.pad(x_inter, [5, 5, 5, 5], value=0)\n            x_inter = x_inter + z[i]\n            x_inter = self.sub_generators[i](x_inter) + x_prev\n            x_list.append(x_inter)\n\n        return x_list\n\n    def progress(self):\n        self.current_scale += 1\n\n        if self.current_scale % 4 == 0:\n            self.nf *= 2\n\n        tmp_generator = nn.ModuleList()\n        tmp_generator.append(nn.Sequential(nn.Conv2d(3, self.nf, 3, 1),\n                                           nn.BatchNorm2d(self.nf),\n                                           nn.LeakyReLU(2e-1)))\n\n        for _ in range(3):\n            tmp_generator.append(nn.Sequential(nn.Conv2d(self.nf, self.nf, 3, 1),\n                                               nn.BatchNorm2d(self.nf),\n                                               nn.LeakyReLU(2e-1)))\n\n        tmp_generator.append(nn.Sequential(nn.Conv2d(self.nf, 3, 3, 1),\n                                           nn.Tanh()))\n\n        tmp_generator = nn.Sequential(*tmp_generator)\n\n        if self.current_scale % 4 != 0:\n            prev_generator = self.sub_generators[-1]\n\n            # Initialize layers via copy\n            if self.current_scale >= 1:\n                tmp_generator.load_state_dict(prev_generator.state_dict())\n\n        self.sub_generators.append(tmp_generator)\n        print(""GENERATOR PROGRESSION DONE"")\n'"
