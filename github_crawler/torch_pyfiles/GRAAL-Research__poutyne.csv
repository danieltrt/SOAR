file_path,api_count,code
setup.py,0,"b'import os\nimport subprocess\nfrom setuptools import setup, find_packages\n\ncurrent_file_path = os.path.abspath(os.path.dirname(__file__))\n\n\ndef get_readme():\n    readme_file_path = os.path.join(current_file_path, \'README.md\')\n    with open(readme_file_path, \'r\', encoding=\'utf-8\') as f:\n        return f.read()\n\n\ndef get_version():\n    version_file_path = os.path.join(current_file_path, \'version.txt\')\n    with open(version_file_path, \'r\', encoding=\'utf-8\') as f:\n        version = f.read().strip()\n\n    try:\n        sha = subprocess.check_output([\'git\', \'rev-parse\', \'HEAD\']).decode(\'ascii\').strip()\n    except Exception:  # pylint: disable=broad-except\n        sha = \'Unknown\'\n\n    if os.getenv(\'POUTYNE_RELEASE_BUILD\') != \'1\':\n        version += \'.dev1\'\n        if sha != \'Unknown\':\n            version += \'+\' + sha[:7]\n    return version\n\n\ndef write_version_python_file(version):\n    version_python_file = os.path.join(current_file_path, \'poutyne/version.py\')\n    with open(version_python_file, \'w\', encoding=\'utf-8\') as f:\n        f.write(f""__version__ = {repr(version)}\\n"")\n\n\ndef main():\n    readme = get_readme()\n\n    version = get_version()\n    print(""Building version"", version)\n    write_version_python_file(version)\n\n    packages = find_packages()\n    setup(\n        name=\'Poutyne\',\n        version=version,\n        author=\'Fr\xc3\xa9d\xc3\xa9rik Paradis\',\n        author_email=\'fredy_14@live.fr\',\n        url=\'https://poutyne.org\',\n        download_url=\'https://github.com/GRAAL-Research/poutyne/archive/v\' + version + \'.zip\',\n        license=\'LGPLv3\',\n        classifiers=[\n            \'Development Status :: 3 - Alpha\',\n            \'Intended Audience :: Developers\',\n            \'Intended Audience :: Education\',\n            \'Intended Audience :: Science/Research\',\n            \'License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)\',\n            \'Programming Language :: Python :: 3\',\n            \'Programming Language :: Python :: 3.6\',\n            \'Programming Language :: Python :: 3.7\',\n            \'Programming Language :: Python :: 3.8\',\n            \'Topic :: Software Development :: Libraries\',\n            \'Topic :: Software Development :: Libraries :: Python Modules\'\n        ],\n        packages=packages,\n        install_requires=[\'numpy\', \'torch\'],\n        python_requires=\'>=3.6.1\',\n        description=\'A Keras-like framework and utilities for PyTorch.\',\n        long_description=readme,\n        long_description_content_type=\'text/markdown\',\n    )\n\n\nif __name__ == \'__main__\':\n    main()\n'"
poutyne/__init__.py,0,b'# pylint: disable=wildcard-import\nfrom .version import __version__\n\nfrom .utils import *\n'
poutyne/utils.py,6,"b'# -*- coding: utf-8 -*-\nimport random\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\n\n\ndef torch_to_numpy(obj, copy=False):\n    """"""\n    Convert to Numpy arrays all tensors inside a Python object composed of the supported types.\n\n    Args:\n        obj: The Python object to convert.\n        copy (bool): Whether to copy the memory. By default, if a tensor is already on CPU, the\n            Numpy array will be a view of the tensor.\n\n    Returns:\n        A new Python object with the same structure as `obj` but where the tensors are now Numpy\n        arrays. Not supported type are left as reference in the new object.\n\n    Example:\n        .. code-block:: python\n\n            >>> from poutyne import torch_to_numpy\n            >>> torch_to_numpy({\n            ...     \'first\': torch.tensor([1, 2, 3]),\n            ...     \'second\':[torch.tensor([4,5,6]), torch.tensor([7,8,9])],\n            ...     \'third\': 34\n            ... })\n            {\n                \'first\': array([1, 2, 3]),\n                \'second\': [array([4, 5, 6]), array([7, 8, 9])],\n                \'third\': 34\n            }\n\n    See:\n        :meth:`~poutyne.torch_apply` for supported types.\n    """"""\n    if copy:\n        func = lambda t: t.cpu().detach().numpy().copy()\n    else:\n        func = lambda t: t.cpu().detach().numpy()\n    return torch_apply(obj, func)\n\n\ndef torch_to(obj, *args, **kargs):\n    return torch_apply(obj, lambda t: t.to(*args, **kargs))\n\n\ndef torch_apply(obj, func):\n    """"""\n    Apply a function to all tensors inside a Python object composed of the supported types.\n\n    Supported types are: list, tuple and dict.\n\n    Args:\n        obj: The Python object to convert.\n        func: The function to apply.\n\n    Returns:\n        A new Python object with the same structure as `obj` but where the tensors have been applied\n        the function `func`. Not supported type are left as reference in the new object.\n    """"""\n    fn = lambda t: func(t) if torch.is_tensor(t) else t\n    return _apply(obj, fn)\n\n\ndef _apply(obj, func):\n    if isinstance(obj, (list, tuple)):\n        return type(obj)(_apply(el, func) for el in obj)\n    if isinstance(obj, dict):\n        return {k: _apply(el, func) for k, el in obj.items()}\n    return func(obj)\n\n\ndef _concat(obj):\n    if isinstance(obj[0], (list, tuple)):\n        return type(obj[0])(_concat(ele) for ele in zip(*obj))\n    if isinstance(obj[0], dict):\n        concat_dict = {}\n        for key in obj[0].keys():\n            concat_dict[key] = _concat([o[key] for o in obj])\n        return concat_dict\n    if isinstance(obj[0], np.ndarray) and obj[0].shape != ():\n        return np.concatenate(obj)\n    return obj\n\n\ndef numpy_to_torch(obj):\n    """"""\n    Convert to tensors all Numpy arrays inside a Python object composed of the supported types.\n\n    Args:\n        obj: The Python object to convert.\n\n    Returns:\n        A new Python object with the same structure as `obj` but where the Numpy arrays are now\n        tensors. Not supported type are left as reference in the new object.\n\n    Example:\n        .. code-block:: python\n\n            >>> from poutyne import numpy_to_torch\n            >>> numpy_to_torch({\n            ...     \'first\': np.array([1, 2, 3]),\n            ...     \'second\':[np.array([4,5,6]), np.array([7,8,9])],\n            ...     \'third\': 34\n            ... })\n            {\n                \'first\': tensor([1, 2, 3]),\n                \'second\': [tensor([4, 5, 6]), tensor([7, 8, 9])],\n                \'third\': 34\n            }\n\n\n    """"""\n    fn = lambda a: torch.from_numpy(a) if isinstance(a, np.ndarray) else a\n    return _apply(obj, fn)\n\n\nclass TensorDataset(Dataset):\n    """"""Dataset wrapping tensors.\n\n    Each sample will be retrieved by indexing tensors along the first dimension.\n\n    Arguments:\n        *tensors (Tensor): tensors that have the same size of the first dimension.\n    """"""\n\n    def __init__(self, *tensors):\n        self.tensors = tensors\n\n        def _rabbit_hole(obj):\n            if isinstance(obj, (list, tuple)):\n                lengths = [_rabbit_hole(o) for o in obj]\n                for length in lengths[1:]:\n                    assert length == lengths[0]\n                return lengths[0]\n            return len(obj)\n\n        self._len = _rabbit_hole(self.tensors)\n\n    def __getitem__(self, index):\n        def _rabbit_hole(obj, idx):\n            if isinstance(obj, (list, tuple)):\n                return type(obj)(_rabbit_hole(o, idx) for o in obj)\n            return obj[idx]\n\n        return _rabbit_hole(self.tensors, index)\n\n    def __len__(self):\n        return self._len\n\n\ndef set_seeds(seed):\n    """"""\n    Set Python, Numpy and Pytorch\'s random seeds in order to make\n    the random number generation procedure deterministic and reproducible.\n\n    Args:\n        seed (int): The random number generation seed to use.\n    """"""\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n'"
tests/__init__.py,0,b''
tests/context.py,0,"b""# -*- coding: utf-8 -*-\n\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\n# pylint: disable=unused-import,wrong-import-position\nimport poutyne\n"""
tests/test_utils.py,16,"b'# -*- coding: utf-8 -*-\nfrom unittest import TestCase\nfrom unittest.mock import MagicMock, call\nimport unittest\nimport torch\nimport numpy as np\nfrom poutyne.utils import TensorDataset, _concat\nfrom poutyne import torch_apply\n\n\nclass TorchApplyTest(TestCase):\n    def test_apply_on_list(self):\n        my_list = [MagicMock(spec=torch.Tensor) for _ in range(10)]\n        torch_apply(my_list, lambda t: t.cpu())\n        self._test_method_calls(my_list)\n\n    def test_apply_on_recursive_list(self):\n        my_list = [MagicMock(spec=torch.Tensor) for _ in range(2)]\n        my_list.append([MagicMock(spec=torch.Tensor) for _ in range(3)])\n        my_list += [MagicMock(spec=torch.Tensor) for _ in range(1)]\n        torch_apply(my_list, lambda t: t.cpu())\n        self._test_method_calls(my_list[:2] + my_list[2] + my_list[3:])\n\n    def test_apply_on_tuple(self):\n        my_tuple = tuple(MagicMock(spec=torch.Tensor) for _ in range(10))\n        torch_apply(my_tuple, lambda t: t.cpu())\n        self._test_method_calls(my_tuple)\n\n    def test_apply_on_recursive_tuple(self):\n        my_tuple = tuple(MagicMock(spec=torch.Tensor) for _ in range(2))\n        my_tuple += (tuple(MagicMock(spec=torch.Tensor) for _ in range(3)), )\n        my_tuple += tuple(MagicMock(spec=torch.Tensor) for _ in range(1))\n        torch_apply(my_tuple, lambda t: t.cpu())\n        self._test_method_calls(my_tuple[:2] + my_tuple[2] + my_tuple[3:])\n\n    def test_apply_on_dict(self):\n        my_dict = {}\n        for k in [\'a\', \'b\', \'c\']:\n            my_dict[k] = MagicMock(spec=torch.Tensor)\n        torch_apply(my_dict, lambda t: t.cpu())\n        self._test_method_calls(list(my_dict.values()))\n\n    def test_apply_on_recursive_dict(self):\n        my_dict = {}\n        my_dict[\'a\'] = MagicMock(spec=torch.Tensor)\n        my_dict[\'b\'] = {}\n        for k in [\'c\', \'d\']:\n            my_dict[\'b\'][k] = MagicMock(spec=torch.Tensor)\n        torch_apply(my_dict, lambda t: t.cpu())\n        self._test_method_calls([my_dict[\'a\'], *my_dict[\'b\'].values()])\n\n    def test_apply_on_recursive_data_structure(self):\n        my_obj = {\n            \'a\': [MagicMock(spec=torch.Tensor) for _ in range(3)],\n            \'b\': tuple(MagicMock(spec=torch.Tensor) for _ in range(2)),\n            \'c\': {\n                \'d\': [MagicMock(spec=torch.Tensor) for _ in range(3)]\n            },\n            \'e\': MagicMock(spec=torch.Tensor)\n        }\n        torch_apply(my_obj, lambda t: t.cpu())\n        self._test_method_calls(my_obj[\'a\'] + list(my_obj[\'b\']) + my_obj[\'c\'][\'d\'] + [my_obj[\'e\']])\n\n    def test_apply_on_object_with_no_tensor(self):\n        my_obj = {\'a\': 5, \'b\': 3.141592, \'c\': {\'d\': [1, 2, 3]}}\n        ret = torch_apply(my_obj, lambda t: t.cpu())\n        self.assertEqual(ret, my_obj)\n        self.assertFalse(ret is my_obj)\n\n    def test_apply_with_replacement_to_no_tensor(self):\n        my_obj = [MagicMock(spec=torch.Tensor)]\n        ret = torch_apply(my_obj, lambda t: 123)\n        self.assertEqual(ret, [123])\n\n    def _test_method_calls(self, mock_list):\n        print(mock_list)\n        for mock in mock_list:\n            self.assertEqual(mock.method_calls, [call.cpu()])\n\n\nclass TensorDatasetTest(TestCase):\n    def test_one_tensor(self):\n        range20 = np.expand_dims(np.arange(20), 1)\n        dataset = TensorDataset(range20)\n        self.assertEqual(len(dataset), 20)\n        for i in range(20):\n            self.assertEqual(dataset[i], np.array([i]))\n\n    def test_multiple_tensors(self):\n        range20 = np.expand_dims(np.arange(20), 1)\n        dataset = TensorDataset(range20, range20 * 2, range20 * 3)\n        self.assertEqual(len(dataset), 20)\n        self.assertEqual(type(dataset[0]), tuple)\n        for i in range(20):\n            self.assertEqual(dataset[i][0], i)\n            self.assertEqual(dataset[i][1], i * 2)\n            self.assertEqual(dataset[i][2], i * 3)\n\n    def test_list_of_tensors(self):\n        range20 = np.expand_dims(np.arange(20), 1)\n        dataset = TensorDataset((range20, range20 * 2), range20 * 3)\n        self.assertEqual(len(dataset), 20)\n        self.assertEqual(type(dataset[0]), tuple)\n        self.assertEqual(type(dataset[0][0]), tuple)\n        self.assertEqual(type(dataset[0][-1]), np.ndarray)\n        for i in range(20):\n            self.assertEqual(dataset[i][0][0], i)\n            self.assertEqual(dataset[i][0][1], i * 2)\n            self.assertEqual(dataset[i][1], i * 3)\n\n        dataset = TensorDataset((range20, range20 * 2), (range20 * 3, range20 * 4))\n        self.assertEqual(len(dataset), 20)\n\n        self.assertEqual(type(dataset[0]), tuple)\n        self.assertEqual(type(dataset[1]), tuple)\n        self.assertEqual(type(dataset[0][0]), tuple)\n        self.assertEqual(type(dataset[0][1]), tuple)\n        for i in range(20):\n            self.assertEqual(type(dataset[i][0][0]), np.ndarray)\n            self.assertEqual(type(dataset[i][0][1]), np.ndarray)\n            self.assertEqual(dataset[i][0][0], i)\n            self.assertEqual(dataset[i][0][1], i * 2)\n            self.assertEqual(type(dataset[i][1][0]), np.ndarray)\n            self.assertEqual(type(dataset[i][1][1]), np.ndarray)\n            self.assertEqual(dataset[i][1][0], i * 3)\n            self.assertEqual(dataset[i][1][1], i * 4)\n\n\nclass ConcatTest(TestCase):\n    def test_single_array(self):\n        """"""\n        Test the concatenation of a single array\n        """"""\n        obj = [np.arange(5)] * 5\n        concat = _concat(obj)\n        self.assertEqual(concat.shape, (25, ))\n\n    def test_tuple_1(self):\n        """"""\n        Test the concatenation of a [([], [])]\n        """"""\n        obj = [(np.arange(5), np.ones(5) * 2)] * 5\n        concat = _concat(obj)\n        self.assertEqual(concat[0].shape, (25, ))\n        self.assertEqual(concat[1].shape, (25, ))\n        for i in range(5):\n            for j in range(5):\n                self.assertTrue(concat[0][i * 5 + j] == j)\n        self.assertTrue((concat[1] == 2).all())\n\n    def test_tuple_2(self):\n        """"""\n        Test the concatenation of a [([], ([], []))]\n        """"""\n        obj = [(np.arange(5), (np.ones(5) * 2, np.ones(5) * 3))] * 5\n        concat = _concat(obj)\n        self.assertEqual(concat[0].shape, (25, ))\n        self.assertEqual(concat[1][0].shape, (25, ))\n        self.assertEqual(concat[1][1].shape, (25, ))\n        for i in range(5):\n            for j in range(5):\n                self.assertTrue(concat[0][i * 5 + j] == j)\n        self.assertTrue((concat[1][0] == 2).all())\n        self.assertTrue((concat[1][1] == 3).all())\n\n    def test_tuple_3(self):\n        """"""\n        Test the concatenation of a [(([], []), ([], []))]\n        """"""\n        obj = [((np.arange(5), np.ones(5)), (np.ones(5) * 2, np.ones(5) * 3))] * 5\n        concat = _concat(obj)\n        self.assertEqual(concat[0][0].shape, (25, ))\n        self.assertEqual(concat[0][1].shape, (25, ))\n        self.assertEqual(concat[1][0].shape, (25, ))\n        self.assertEqual(concat[1][1].shape, (25, ))\n        for i in range(5):\n            for j in range(5):\n                self.assertTrue(concat[0][0][i * 5 + j] == j)\n        self.assertTrue((concat[0][1] == 1).all())\n        self.assertTrue((concat[1][0] == 2).all())\n        self.assertTrue((concat[1][1] == 3).all())\n\n    def test_array_1(self):\n        """"""\n        Test the concatenation of a [[[], []]]\n        """"""\n        obj = [[np.arange(5), np.ones(5) * 2]] * 5\n        concat = _concat(obj)\n        self.assertEqual(concat[0].shape, (25, ))\n        self.assertEqual(concat[1].shape, (25, ))\n        for i in range(5):\n            for j in range(5):\n                self.assertTrue(concat[0][i * 5 + j] == j)\n        self.assertTrue((concat[1] == 2).all())\n\n    def test_array_2(self):\n        """"""\n        Test the concatenation of a [[[], ([], [])]]\n        """"""\n        obj = [[np.arange(5), [np.ones(5) * 2, np.ones(5) * 3]]] * 5\n        concat = _concat(obj)\n        self.assertEqual(concat[0].shape, (25, ))\n        self.assertEqual(concat[1][0].shape, (25, ))\n        self.assertEqual(concat[1][1].shape, (25, ))\n        for i in range(5):\n            for j in range(5):\n                self.assertTrue(concat[0][i * 5 + j] == j)\n        self.assertTrue((concat[1][0] == 2).all())\n        self.assertTrue((concat[1][1] == 3).all())\n\n    def test_array_3(self):\n        """"""\n        Test the concatenation of a [[[[], []], [[], []]]]\n        """"""\n        obj = [[[np.arange(5), np.ones(5)], [np.ones(5) * 2, np.ones(5) * 3]]] * 5\n        concat = _concat(obj)\n        self.assertEqual(concat[0][0].shape, (25, ))\n        self.assertEqual(concat[0][1].shape, (25, ))\n        self.assertEqual(concat[1][0].shape, (25, ))\n        self.assertEqual(concat[1][1].shape, (25, ))\n        for i in range(5):\n            for j in range(5):\n                self.assertTrue(concat[0][0][i * 5 + j] == j)\n        self.assertTrue((concat[0][1] == 1).all())\n        self.assertTrue((concat[1][0] == 2).all())\n        self.assertTrue((concat[1][1] == 3).all())\n\n    def test_dict_1(self):\n        """"""\n        Test list of dictionaries\n        """"""\n        obj = [{\'a\': np.arange(5), \'b\': np.ones(5) * 2}] * 5\n        concat = _concat(obj)\n        self.assertEqual(concat[\'a\'].shape, (25, ))\n        self.assertEqual(concat[\'b\'].shape, (25, ))\n        for i in range(5):\n            for j in range(5):\n                self.assertTrue(concat[\'a\'][i * 5 + j] == j)\n        self.assertTrue((concat[\'b\'] == 2).all())\n\n    def test_dict_2(self):\n        """"""\n        Test list of dictionaries\n        """"""\n        obj = [{\'a\': (np.arange(5), np.ones(5)), \'b\': np.ones(5) * 2}] * 5\n        concat = _concat(obj)\n        self.assertEqual(concat[\'a\'][0].shape, (25, ))\n        self.assertEqual(concat[\'a\'][1].shape, (25, ))\n        self.assertEqual(concat[\'b\'].shape, (25, ))\n\n        for i in range(5):\n            for j in range(5):\n                self.assertTrue(concat[\'a\'][0][i * 5 + j] == j)\n        self.assertTrue((concat[\'a\'][1] == 1).all())\n        self.assertTrue((concat[\'b\'] == 2).all())\n\n    def test_non_concatenable_values(self):\n        obj = [3] * 5\n        concat = _concat(obj)\n        self.assertEqual(concat, obj)\n        self.assertEqual(type(concat), type(obj))\n\n    def test_non_concatenable_values2(self):\n        obj = [{\'a\': (np.arange(5), np.ones(5), 2), \'b\': 3, \'c\': np.array(4)}] * 5\n        concat = _concat(obj)\n        self.assertEqual(concat[\'a\'][0].shape, (25, ))\n        self.assertEqual(concat[\'a\'][1].shape, (25, ))\n        self.assertEqual(concat[\'a\'][2], (2, ) * 5)\n        self.assertEqual(concat[\'b\'], [3] * 5)\n        self.assertEqual(concat[\'c\'], [4] * 5)\n\n        for i in range(5):\n            for j in range(5):\n                self.assertTrue(concat[\'a\'][0][i * 5 + j] == j)\n        self.assertTrue((concat[\'a\'][1] == 1).all())\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/utils.py,0,"b'from unittest.mock import MagicMock\nfrom copy import deepcopy\n\n\nclass CopyingMock(MagicMock):\n    def __call__(self, *args, **kwargs):\n        args = deepcopy(args)\n        kwargs = deepcopy(kwargs)\n        return super(CopyingMock, self).__call__(*args, **kwargs)\n'"
docs/source/conf.py,1,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/stable/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nimport datetime\nsys.path.insert(0, os.path.abspath(\'../..\'))\n\nfrom poutyne import __version__ as version\n\nyear = str(datetime.datetime.now().year)\n\n# -- Project information -----------------------------------------------------\n\nproject = \'Poutyne\'\ncopyright = \'2018-\' + year + \', Fr\xc3\xa9d\xc3\xa9rik Paradis\'\nauthor = \'Fr\xc3\xa9d\xc3\xa9rik Paradis\'\n\n# The short X.Y version\nversion = version\n# The full version, including alpha/beta/rc tags\nrelease = version\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\', \'sphinx.ext.doctest\', \'sphinx.ext.todo\', \'sphinx.ext.coverage\', \'sphinx.ext.mathjax\',\n    \'sphinx.ext.ifconfig\', \'sphinx.ext.viewcode\', \'sphinx.ext.githubpages\', \'sphinx.ext.napoleon\',\n    \'sphinx.ext.intersphinx\'\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    \'logo_only\': True,\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\nhtml_extra_path = [\'CNAME\', \'favicon.ico\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\nhtml_logo = \'_static/logos/poutyne-light.png\'\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'Poutynedoc\'\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'Poutyne.tex\', \'Poutyne Documentation\', \'Fr\xc3\xa9d\xc3\xa9rik Paradis\', \'manual\'),\n]\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, \'poutyne\', \'Poutyne Documentation\', [author], 1)]\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'Poutyne\', \'Poutyne Documentation\', author, \'Poutyne\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n# -- Intersphinx mappings ----------------------------------------------------\n\nintersphinx_mapping = {\n    \'PyTorch\': (\'https://pytorch.org/docs/master/\', None),\n    \'python\': (\'https://docs.python.org/3\', None),\n    \'numpy\': (\'http://docs.scipy.org/doc/numpy/\', None),\n    \'matplotlib\': (\'https://matplotlib.org/\', None)\n}\n\n# -- Extension configuration -------------------------------------------------\n\nautodoc_default_options = {\n    \'member-order\': \'bysource\',\n}\n\n# -- Options for todo extension ----------------------------------------------\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n'"
poutyne/framework/__init__.py,0,b'# pylint: disable=wildcard-import\nfrom .model import *\nfrom .callbacks import *\nfrom .experiment import *\nfrom .warning_manager import *\nfrom .exceptions import *\n'
poutyne/framework/exceptions.py,0,"b'class ModelConfigurationError(Exception):\n    """"""\n    The exception raised when a model is misconfigured (e.g. missing properties, invalid properties).\n    """"""\n\n    def __init__(self, message):\n        super().__init__()\n        self.message = message\n\n    def __str__(self):\n        return repr(self.message)\n'"
poutyne/framework/experiment.py,12,"b'# pylint: disable=redefined-builtin\nimport os\nimport warnings\n\nimport numpy as np\n\ntry:\n    import pandas as pd\nexcept ImportError:\n    pd = None\nimport torch\n\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept ImportError:\n    SummaryWriter = None\n\nfrom poutyne.framework import Model\nfrom poutyne.utils import set_seeds\nfrom poutyne.framework.callbacks import ModelCheckpoint, \\\n    OptimizerCheckpoint, \\\n    LRSchedulerCheckpoint, \\\n    PeriodicSaveLambda, \\\n    AtomicCSVLogger, \\\n    TensorBoardLogger, \\\n    BestModelRestore\n\n\nclass Experiment:\n    """"""\n    The Experiment class provides a straightforward experimentation tool for efficient and entirely\n    customizable finetuning of the whole neural network training procedure with PyTorch. The\n    ``Experiment`` object takes care of the training and testing processes while also managing to\n    keep traces of all pertinent information via the automatic logging option.\n\n    Args:\n        directory (str): Path to the experiment\'s working directory. Will be used for the automatic logging.\n        network (torch.nn.Module): A PyTorch network.\n        device (torch.torch.device): The device to which the model is sent. If None, the model will be\n            kept on its current device.\n            (Default value = None)\n        logging (bool): Whether or not to log the experiment\'s progress. If true, various logging\n            callbacks will be inserted to output training and testing stats as well as to automatically\n            save model checkpoints, for example. See :func:`~Experiment.train()` and :func:`~Experiment.test()`\n            for more details.\n            (Default value = True)\n        optimizer (Union[torch.optim.Optimizer, str]): If Pytorch Optimizer, must already be initialized.\n            If str, should be the optimizer\'s name in Pytorch (i.e. \'Adam\' for torch.optim.Adam).\n            (Default value = \'sgd\')\n        loss_function(Union[Callable, str]) It can be any PyTorch\n            loss layer or custom loss function. It can also be a string with the same name as a PyTorch\n            loss function (either the functional or object name). The loss function must have the signature\n            ``loss_function(input, target)`` where ``input`` is the prediction of the network and ``target``\n            is the ground truth. If ``None``, will default to, in priority order, either the model\'s own\n            loss function or the default loss function associated with the ``task``.\n            (Default value = None)\n        batch_metrics (list): List of functions with the same signature as the loss function. Each metric\n            can be any PyTorch loss function. It can also be a string with the same name as a PyTorch\n            loss function (either the functional or object name). \'accuracy\' (or just \'acc\') is also a\n            valid metric. Each metric function is called on each batch of the optimization and on the\n            validation batches at the end of the epoch.\n            (Default value = None)\n        epoch_metrics (list): List of functions with the same signature as\n            :class:`~poutyne.framework.metrics.epoch_metrics.EpochMetric`\n            (Default value = None)\n        monitor_metric (str): Which metric to consider for best model performance calculation. Should be in\n            the format \'{metric_name}\' or \'val_{metric_name}\' (i.e. \'val_loss\'). If None, will follow the value\n            suggested by ``task`` or default to \'val_loss\'.\n\n            .. warning:: If you do not plan on using a validation set, you must set the monitor metric to another\n                value.\n        monitor_mode (str): Which mode, either \'min\' or \'max\', should be used when considering the ``monitor_metric``\n            value. If None, will follow the value suggested by ``task`` or default to \'min\'.\n        task (str): Any str beginning with either \'classif\' or \'reg\'. Specifying a ``task`` can assign default\n            values to the ``loss_function``, ``batch_metrics``, ``monitor_mode`` and ``monitor_mode``. For ``task``\n            that begins with \'reg\', the only default value is the loss function that is the mean squared error. When\n            beginning with \'classif\', the default loss function is the cross-entropy loss, the default batch metrics\n            will be the accuracy, the default epoch metrics will be the F1 score and the default monitoring will be\n            set on \'val_acc\' with a \'max\' mode.\n            (Default value = None)\n\n    Example:\n        Using a PyTorch DataLoader, on classification task with SGD optimizer::\n\n            import torch\n            from torch.utils.data import DataLoader, TensorDataset\n            from poutyne.framework import Experiment\n\n            num_features = 20\n            num_classes = 5\n\n            # Our training dataset with 800 samples.\n            num_train_samples = 800\n            train_x = torch.rand(num_train_samples, num_features)\n            train_y = torch.randint(num_classes, (num_train_samples, ), dtype=torch.long)\n            train_dataset = TensorDataset(train_x, train_y)\n            train_generator = DataLoader(train_dataset, batch_size=32)\n\n            # Our validation dataset with 200 samples.\n            num_valid_samples = 200\n            valid_x = torch.rand(num_valid_samples, num_features)\n            valid_y = torch.randint(num_classes, (num_valid_samples, ), dtype=torch.long)\n            valid_dataset = TensorDataset(valid_x, valid_y)\n            valid_generator = DataLoader(valid_dataset, batch_size=32)\n\n            # Our network\n            pytorch_network = torch.nn.Linear(num_features, num_train_samples)\n\n            # Intialization of our experimentation and network training\n            exp = Experiment(\'./simple_example\',\n                             pytorch_network,\n                             optimizer=\'sgd\',\n                             task=\'classif\')\n            exp.train(train_generator, valid_generator, epochs=5)\n\n    The above code will yield an output similar to the below lines. Note the automatic checkpoint saving\n    in the experiment directory when the monitored metric improved.\n\n    .. code-block:: none\n\n            Epoch 1/5 0.09s Step 25/25: loss: 6.351375, acc: 1.375000, val_loss: 6.236106, val_acc: 5.000000\n            Epoch 1: val_acc improved from -inf to 5.00000, saving file to ./simple_example/checkpoint_epoch_1.ckpt\n            Epoch 2/5 0.10s Step 25/25: loss: 6.054254, acc: 14.000000, val_loss: 5.944495, val_acc: 19.500000\n            Epoch 2: val_acc improved from 5.00000 to 19.50000, saving file to ./simple_example/checkpoint_epoch_2.ckpt\n            Epoch 3/5 0.09s Step 25/25: loss: 5.759377, acc: 22.875000, val_loss: 5.655412, val_acc: 21.000000\n            Epoch 3: val_acc improved from 19.50000 to 21.00000, saving file to ./simple_example/checkpoint_epoch_3.ckpt\n            ...\n\n    Training can now easily be resumed from the best checkpoint::\n\n            exp.train(train_generator, valid_generator, epochs=10)\n\n    .. code-block:: none\n\n            Restoring model from ./simple_example/checkpoint_epoch_3.ckpt\n            Loading weights from ./simple_example/checkpoint.ckpt and starting at epoch 6.\n            Loading optimizer state from ./simple_example/checkpoint.optim and starting at epoch 6.\n            Epoch 6/10 0.16s Step 25/25: loss: 4.897135, acc: 22.875000, val_loss: 4.813141, val_acc: 20.500000\n            Epoch 7/10 0.10s Step 25/25: loss: 4.621514, acc: 22.625000, val_loss: 4.545359, val_acc: 20.500000\n            Epoch 8/10 0.24s Step 25/25: loss: 4.354721, acc: 23.625000, val_loss: 4.287117, val_acc: 20.500000\n            ...\n\n    Testing is also very intuitive::\n\n            exp.test(test_generator)\n\n    .. code-block:: none\n\n            Restoring model from ./simple_example/checkpoint_epoch_9.ckpt\n            Found best checkpoint at epoch: 9\n            lr: 0.01, loss: 4.09892, acc: 23.625, val_loss: 4.04057, val_acc: 21.5\n            On best model: test_loss: 4.06664, test_acc: 17.5\n\n\n    Finally, all the pertinent metrics specified to the Experiment at each epoch are stored in a specific logging\n    file, found here at \'./simple_example/log.tsv\'.\n\n    .. code-block:: none\n\n            epoch\ttime\t            lr\t    loss\t            acc\t    val_loss\t        val_acc\n            1\t    0.0721172170015052\t0.01\t6.351375141143799\t1.375\t6.23610631942749\t5.0\n            2\t    0.0298177790245972\t0.01\t6.054253826141357\t14.000\t5.94449516296386\t19.5\n            3\t    0.0637106419890187\t0.01\t5.759376544952392\t22.875\t5.65541223526001\t21.0\n            ...\n\n    """"""\n    BEST_CHECKPOINT_FILENAME = \'checkpoint_epoch_{epoch}.ckpt\'\n    BEST_CHECKPOINT_TMP_FILENAME = \'checkpoint_epoch.tmp.ckpt\'\n    MODEL_CHECKPOINT_FILENAME = \'checkpoint.ckpt\'\n    MODEL_CHECKPOINT_TMP_FILENAME = \'checkpoint.tmp.ckpt\'\n    OPTIMIZER_CHECKPOINT_FILENAME = \'checkpoint.optim\'\n    OPTIMIZER_CHECKPOINT_TMP_FILENAME = \'checkpoint.tmp.optim\'\n    LOG_FILENAME = \'log.tsv\'\n    LOG_TMP_FILENAME = \'log.tmp.tsv\'\n    TENSORBOARD_DIRECTORY = \'tensorboard\'\n    EPOCH_FILENAME = \'last.epoch\'\n    EPOCH_TMP_FILENAME = \'last.tmp.epoch\'\n    LR_SCHEDULER_FILENAME = \'lr_sched_%d.lrsched\'\n    LR_SCHEDULER_TMP_FILENAME = \'lr_sched_%d.tmp.lrsched\'\n    TEST_LOG_FILENAME = \'test_log.tsv\'\n\n    def __init__(self,\n                 directory,\n                 network,\n                 *,\n                 device=None,\n                 logging=True,\n                 optimizer=\'sgd\',\n                 loss_function=None,\n                 batch_metrics=None,\n                 epoch_metrics=None,\n                 monitor_metric=None,\n                 monitor_mode=None,\n                 task=None):\n        self.directory = directory\n        self.logging = logging\n\n        if task is not None and not task.startswith(\'classif\') and not task.startswith(\'reg\'):\n            raise ValueError(""Invalid task \'%s\'"" % task)\n\n        batch_metrics = [] if batch_metrics is None else batch_metrics\n        epoch_metrics = [] if epoch_metrics is None else epoch_metrics\n\n        loss_function = self._get_loss_function(loss_function, network, task)\n        batch_metrics = self._get_batch_metrics(batch_metrics, network, task)\n        epoch_metrics = self._get_epoch_metrics(epoch_metrics, network, task)\n        self._set_monitor(monitor_metric, monitor_mode, task)\n\n        self.model = Model(network, optimizer, loss_function, batch_metrics=batch_metrics, epoch_metrics=epoch_metrics)\n        if device is not None:\n            self.model.to(device)\n\n        self.best_checkpoint_filename = self.get_path(Experiment.BEST_CHECKPOINT_FILENAME)\n        self.best_checkpoint_tmp_filename = self.get_path(Experiment.BEST_CHECKPOINT_TMP_FILENAME)\n        self.model_checkpoint_filename = self.get_path(Experiment.MODEL_CHECKPOINT_FILENAME)\n        self.model_checkpoint_tmp_filename = self.get_path(Experiment.MODEL_CHECKPOINT_TMP_FILENAME)\n        self.optimizer_checkpoint_filename = self.get_path(Experiment.OPTIMIZER_CHECKPOINT_FILENAME)\n        self.optimizer_checkpoint_tmp_filename = self.get_path(Experiment.OPTIMIZER_CHECKPOINT_TMP_FILENAME)\n        self.log_filename = self.get_path(Experiment.LOG_FILENAME)\n        self.log_tmp_filename = self.get_path(Experiment.LOG_TMP_FILENAME)\n        self.tensorboard_directory = self.get_path(Experiment.TENSORBOARD_DIRECTORY)\n        self.epoch_filename = self.get_path(Experiment.EPOCH_FILENAME)\n        self.epoch_tmp_filename = self.get_path(Experiment.EPOCH_TMP_FILENAME)\n        self.lr_scheduler_filename = self.get_path(Experiment.LR_SCHEDULER_FILENAME)\n        self.lr_scheduler_tmp_filename = self.get_path(Experiment.LR_SCHEDULER_TMP_FILENAME)\n        self.test_log_filename = self.get_path(Experiment.TEST_LOG_FILENAME)\n\n    def get_path(self, *paths):\n        """"""\n        Returns the path inside the experiment directory.\n        """"""\n        return os.path.join(self.directory, *paths)\n\n    def _get_loss_function(self, loss_function, network, task):\n        if loss_function is None:\n            if hasattr(network, \'loss_function\'):\n                return network.loss_function\n            if task is not None:\n                if task.startswith(\'classif\'):\n                    return \'cross_entropy\'\n                if task.startswith(\'reg\'):\n                    return \'mse\'\n        return loss_function\n\n    def _get_batch_metrics(self, batch_metrics, network, task):\n        if batch_metrics is None or len(batch_metrics) == 0:\n            if hasattr(network, \'batch_metrics\'):\n                return network.batch_metrics\n            if task is not None and task.startswith(\'classif\'):\n                return [\'accuracy\']\n        return batch_metrics\n\n    def _get_epoch_metrics(self, epoch_metrics, network, task):\n        if epoch_metrics is None or len(epoch_metrics) == 0:\n            if hasattr(network, \'epoch_metrics\'):\n                return network.epoch_metrics\n            if task is not None and task.startswith(\'classif\'):\n                return [\'f1\']\n        return epoch_metrics\n\n    def _set_monitor(self, monitor_metric, monitor_mode, task):\n        if monitor_mode is not None and monitor_mode not in [\'min\', \'max\']:\n            raise ValueError(""Invalid mode \'%s\'"" % monitor_mode)\n\n        self.monitor_metric = \'val_loss\'\n        self.monitor_mode = \'min\'\n        if monitor_metric is not None:\n            self.monitor_metric = monitor_metric\n            if monitor_mode is not None:\n                self.monitor_mode = monitor_mode\n        elif task is not None and task.startswith(\'classif\'):\n            self.monitor_metric = \'val_acc\'\n            self.monitor_mode = \'max\'\n\n    def get_best_epoch_stats(self):\n        """"""\n        Returns all computed statistics corresponding to the best epoch according to the\n        ``monitor_metric`` and ``monitor_mode`` attributes.\n\n        Returns:\n            dict where each key is a column name in the logging output file\n            and values are the ones found at the best epoch.\n        """"""\n        if pd is None:\n            raise ImportError(""pandas needs to be installed to use this function."")\n\n        history = pd.read_csv(self.log_filename, sep=\'\\t\')\n        if self.monitor_mode == \'min\':\n            best_epoch_index = history[self.monitor_metric].idxmin()\n        else:\n            best_epoch_index = history[self.monitor_metric].idxmax()\n        return history.iloc[best_epoch_index:best_epoch_index + 1]\n\n    def get_saved_epochs(self):\n        """"""\n        Returns a pandas DataFrame which each row corresponds to an epoch having\n        a saved checkpoint.\n\n        Returns:\n            pandas DataFrame which each row corresponds to an epoch having a saved\n            checkpoint.\n        """"""\n        if pd is None:\n            raise ImportError(""pandas needs to be installed to use this function."")\n\n        history = pd.read_csv(self.log_filename, sep=\'\\t\')\n        metrics = history[self.monitor_metric].tolist()\n        if self.monitor_mode == \'min\':\n            monitor_op = lambda x, y: x < y\n            current_best = float(\'Inf\')\n        elif self.monitor_mode == \'max\':\n            monitor_op = lambda x, y: x > y\n            current_best = -float(\'Inf\')\n        saved_epoch_indices = []\n        for i, metric in enumerate(metrics):\n            if monitor_op(metric, current_best):\n                current_best = metric\n                saved_epoch_indices.append(i)\n        return history.iloc[saved_epoch_indices]\n\n    def _warn_missing_file(self, filename):\n        warnings.warn(""Missing checkpoint: %s."" % filename)\n\n    def _load_epoch_state(self, lr_schedulers):\n        # pylint: disable=broad-except\n        initial_epoch = 1\n        if os.path.isfile(self.epoch_filename):\n            try:\n                with open(self.epoch_filename, \'r\') as f:\n                    initial_epoch = int(f.read()) + 1\n            except Exception as e:\n                print(e)\n            if os.path.isfile(self.model_checkpoint_filename):\n                try:\n                    print(""Loading weights from %s and starting at epoch %d."" %\n                          (self.model_checkpoint_filename, initial_epoch))\n                    self.model.load_weights(self.model_checkpoint_filename)\n                except Exception as e:\n                    print(e)\n            else:\n                self._warn_missing_file(self.model_checkpoint_filename)\n            if os.path.isfile(self.optimizer_checkpoint_filename):\n                try:\n                    print(""Loading optimizer state from %s and starting at epoch %d."" %\n                          (self.optimizer_checkpoint_filename, initial_epoch))\n                    self.model.load_optimizer_state(self.optimizer_checkpoint_filename)\n                except Exception as e:\n                    print(e)\n            else:\n                self._warn_missing_file(self.optimizer_checkpoint_filename)\n            for i, lr_scheduler in enumerate(lr_schedulers):\n                filename = self.lr_scheduler_filename % i\n                if os.path.isfile(filename):\n                    try:\n                        print(""Loading LR scheduler state from %s and starting at epoch %d."" %\n                              (filename, initial_epoch))\n                        lr_scheduler.load_state(filename)\n                    except Exception as e:\n                        print(e)\n                else:\n                    self._warn_missing_file(filename)\n        return initial_epoch\n\n    def _init_model_restoring_callbacks(self, initial_epoch, save_every_epoch):\n        callbacks = []\n        best_checkpoint = ModelCheckpoint(self.best_checkpoint_filename,\n                                          monitor=self.monitor_metric,\n                                          mode=self.monitor_mode,\n                                          save_best_only=not save_every_epoch,\n                                          restore_best=not save_every_epoch,\n                                          verbose=not save_every_epoch,\n                                          temporary_filename=self.best_checkpoint_tmp_filename)\n        callbacks.append(best_checkpoint)\n\n        if save_every_epoch:\n            best_restore = BestModelRestore(monitor=self.monitor_metric, mode=self.monitor_mode, verbose=True)\n            callbacks.append(best_restore)\n\n        if initial_epoch > 1:\n            # We set the current best metric score in the ModelCheckpoint so that\n            # it does not save checkpoint it would not have saved if the\n            # optimization was not stopped.\n            best_epoch_stats = self.get_best_epoch_stats()\n            best_epoch = best_epoch_stats[\'epoch\'].item()\n            best_filename = self.best_checkpoint_filename.format(epoch=best_epoch)\n            if not save_every_epoch:\n                best_checkpoint.best_filename = best_filename\n                best_checkpoint.current_best = best_epoch_stats[self.monitor_metric].item()\n            else:\n                best_restore.best_weights = torch.load(best_filename, map_location=\'cpu\')\n                best_restore.current_best = best_epoch_stats[self.monitor_metric].item()\n\n        return callbacks\n\n    def _init_tensorboard_callbacks(self, disable_tensorboard):\n        tensorboard_writer = None\n        callbacks = []\n        if not disable_tensorboard:\n            if SummaryWriter is None:\n                warnings.warn(\n                    ""tensorboard does not seem to be installed. ""\n                    ""To remove this warning, set the \'disable_tensorboard\' ""\n                    ""flag to True or install tensorboard."",\n                    stacklevel=3)\n            else:\n                tensorboard_writer = SummaryWriter(self.tensorboard_directory)\n                callbacks += [TensorBoardLogger(tensorboard_writer)]\n        return tensorboard_writer, callbacks\n\n    def _init_lr_scheduler_callbacks(self, lr_schedulers):\n        callbacks = []\n        if self.logging:\n            for i, lr_scheduler in enumerate(lr_schedulers):\n                filename = self.lr_scheduler_filename % i\n                tmp_filename = self.lr_scheduler_tmp_filename % i\n                callbacks += [\n                    LRSchedulerCheckpoint(lr_scheduler, filename, verbose=False, temporary_filename=tmp_filename)\n                ]\n        else:\n            callbacks += lr_schedulers\n            callbacks += [BestModelRestore(monitor=self.monitor_metric, mode=self.monitor_mode, verbose=True)]\n        return callbacks\n\n    def train(self,\n              train_generator,\n              valid_generator=None,\n              *,\n              callbacks=None,\n              lr_schedulers=None,\n              save_every_epoch=False,\n              disable_tensorboard=False,\n              epochs=1000,\n              steps_per_epoch=None,\n              validation_steps=None,\n              batches_per_step=1,\n              seed=42):\n        # pylint: disable=too-many-locals\n        """"""\n        Trains or finetunes the attribute model on a dataset using a generator. If a previous training already occured\n        and lasted a total of `n_previous` epochs, then the model\'s weights will be set to the last checkpoint and the\n        training will be resumed for epochs range (`n_previous`, `epochs`].\n\n        If the Experiment has logging enabled (i.e. self.logging is True), numerous callbacks will be automatically\n        included. Notably, two :class:`~callbacks.ModelCheckpoint` objects will take care of saving the last and every\n        new best (according to monitor mode) model weights in appropriate checkpoint files.\n        :class:`~callbacks.OptimizerCheckpoint` and :class:`~callbacks.LRSchedulerCheckpoint` will also respectively\n        handle the saving of the optimizer and LR scheduler\'s respective states for future retrieval. Moreover, a\n        :class:`~callbacks.AtomicCSVLogger` will save all available epoch statistics in an output .tsv file. Lastly, a\n        :class:`~callbacks.TensorBoardLogger` handles automatic TensorBoard logging of various neural network\n        statistics.\n\n        Args:\n            train_generator: Generator-like object for the training set. See :func:`~Model.fit_generator()`\n                for details on the types of generators supported.\n            valid_generator (optional): Generator-like object for the validation set. See\n                :func:`~Model.fit_generator()` for details on the types of generators supported.\n                (Default value = None)\n            callbacks (List[~poutyne.framework.callbacks.Callback]): List of callbacks that will be called during\n                training.\n                (Default value = None)\n            lr_schedulers (List[~poutyne.framework.callbacks.lr_scheduler._PyTorchLRSchedulerWrapper]): List of\n                learning rate schedulers.\n                (Default value = None)\n            save_every_epoch (bool, optional): Whether or not to save the experiment model\'s weights after\n                every epoch.\n                (Default value = False)\n            disable_tensorboard (bool, optional): Whether or not to disable the automatic tensorboard logging\n                callbacks.\n                (Default value = False)\n            epochs (int): Number of times the entire training dataset is seen.\n                (Default value = 1000)\n            steps_per_epoch (int, optional): Number of batch used during one epoch. Obviously, using this\n                argument may cause one epoch not to see the entire training dataset or see it multiple times.\n                (Defaults the number of steps needed to see the entire\n                training dataset)\n            validation_steps (int, optional): Same as for ``steps_per_epoch`` but for the validation dataset.\n                (Defaults to ``steps_per_epoch`` if provided or the number of steps needed to see the entire\n                validation dataset)\n            batches_per_step (int): Number of batches on which to compute the running loss before\n                backpropagating it through the network. Note that the total loss used for backpropagation is\n                the mean of the `batches_per_step` batch losses.\n                (Default value = 1)\n            seed (int, optional): Seed used to make the sampling deterministic.\n                (Default value = 42)\n\n        Returns:\n            List of dict containing the history of each epoch.\n        """"""\n        set_seeds(seed)\n\n        callbacks = [] if callbacks is None else callbacks\n        lr_schedulers = [] if lr_schedulers is None else lr_schedulers\n\n        # Copy callback list.\n        callbacks = list(callbacks)\n\n        tensorboard_writer = None\n        initial_epoch = 1\n        if self.logging:\n            if not os.path.exists(self.directory):\n                os.makedirs(self.directory)\n\n            # Restarting optimization if needed.\n            initial_epoch = self._load_epoch_state(lr_schedulers)\n\n            callbacks += [\n                AtomicCSVLogger(self.log_filename,\n                                separator=\'\\t\',\n                                append=initial_epoch != 1,\n                                temporary_filename=self.log_tmp_filename)\n            ]\n\n            callbacks += self._init_model_restoring_callbacks(initial_epoch, save_every_epoch)\n            callbacks += [\n                ModelCheckpoint(self.model_checkpoint_filename,\n                                verbose=False,\n                                temporary_filename=self.model_checkpoint_tmp_filename)\n            ]\n            callbacks += [\n                OptimizerCheckpoint(self.optimizer_checkpoint_filename,\n                                    verbose=False,\n                                    temporary_filename=self.optimizer_checkpoint_tmp_filename)\n            ]\n\n            # We save the last epoch number after the end of the epoch so that the\n            # _load_epoch_state() knows which epoch to restart the optimization.\n            callbacks += [\n                PeriodicSaveLambda(lambda fd, epoch, logs: print(epoch, file=fd),\n                                   self.epoch_filename,\n                                   temporary_filename=self.epoch_tmp_filename,\n                                   open_mode=\'w\')\n            ]\n\n            tensorboard_writer, cb_list = self._init_tensorboard_callbacks(disable_tensorboard)\n            callbacks += cb_list\n\n        # This method returns callbacks that checkpoints the LR scheduler if logging is enabled.\n        # Otherwise, it just returns the list of LR schedulers with a BestModelRestore callback.\n        callbacks += self._init_lr_scheduler_callbacks(lr_schedulers)\n\n        try:\n            return self.model.fit_generator(train_generator,\n                                            valid_generator,\n                                            epochs=epochs,\n                                            steps_per_epoch=steps_per_epoch,\n                                            validation_steps=validation_steps,\n                                            batches_per_step=batches_per_step,\n                                            initial_epoch=initial_epoch,\n                                            callbacks=callbacks)\n        finally:\n            if tensorboard_writer is not None:\n                tensorboard_writer.close()\n\n    def load_checkpoint(self, checkpoint, *, verbose=False):\n        """"""\n        Loads the attribute model\'s weights with the weights at a given checkpoint epoch.\n\n        Args:\n            checkpoint (Union[int, str]): Which checkpoint to load the model\'s weights form.\n                If \'best\', will load the best weights according to ``monitor_metric`` and ``monitor_mode``.\n                If \'last\', will load the last model checkpoint. If int, will load the checkpoint of the\n                specified epoch.\n            verbose (bool, optional): Whether or not to print the best epoch number and stats when\n                checkpoint is \'best\'.\n                (Default value = False)\n\n        Returns:\n            If checkpoint is \'best\', will return the best epoch stats, as per :func:`~get_best_epoch_stats()`,\n            else None.\n        """"""\n        best_epoch_stats = None\n\n        if isinstance(checkpoint, int):\n            self._load_epoch_checkpoint(checkpoint)\n        elif checkpoint == \'best\':\n            best_epoch_stats = self._load_best_checkpoint(verbose=verbose)\n        elif checkpoint == \'last\':\n            self._load_last_checkpoint()\n        else:\n            raise ValueError(""checkpoint argument must be either \'best\', \'last\' or int. Found : {}"".format(checkpoint))\n\n        return best_epoch_stats\n\n    def _load_epoch_checkpoint(self, epoch):\n        ckpt_filename = self.best_checkpoint_filename.format(epoch=epoch)\n\n        if not os.path.isfile(ckpt_filename):\n            raise ValueError(""No checkpoint found for epoch {}"".format(epoch))\n\n        self.model.load_weights(ckpt_filename)\n\n    def _load_best_checkpoint(self, *, verbose=False):\n        best_epoch_stats = self.get_best_epoch_stats()\n        best_epoch = best_epoch_stats[\'epoch\'].item()\n\n        if verbose:\n            metrics_str = \', \'.join(\'%s: %g\' % (metric_name, best_epoch_stats[metric_name].item())\n                                    for metric_name in best_epoch_stats.columns[2:])\n            print(""Found best checkpoint at epoch: {}"".format(best_epoch))\n            print(metrics_str)\n\n        self._load_epoch_checkpoint(best_epoch)\n        return best_epoch_stats\n\n    def _load_last_checkpoint(self):\n        self.model.load_weights(self.model_checkpoint_filename)\n\n    def test(self, test_generator, *, callbacks=None, steps=None, checkpoint=\'best\', seed=42):\n        """"""\n        Computes and returns the loss and the metrics of the attribute model on a given test examples\n        generator.\n\n        If the Experiment has logging enabled (i.e. self.logging is True), test and validation statistics\n        are saved in a specific test output .tsv file.\n\n        Args:\n            test_generator: Generator-like object for the test set. See :func:`~Model.fit_generator()` for\n                details on the types of generators supported.\n            callbacks (List[~poutyne.framework.callbacks.Callback]): List of callbacks that will be called during\n                the testing.\n                (Default value = None)\n            steps (int, optional): Number of iterations done on ``generator``.\n                (Defaults the number of steps needed to see the entire dataset)\n            checkpoint (Union[str, int]): Which model checkpoint weights to load for the test evaluation.\n                If \'best\', will load the best weights according to ``monitor_metric`` and ``monitor_mode``.\n                If \'last\', will load the last model checkpoint. If int, will load the checkpoint of the\n                specified epoch.\n                (Default value = \'best\')\n            seed (int, optional): Seed used to make the sampling deterministic.\n                (Default value = 42)\n\n        If the Experiment has logging enabled (i.e. self.logging is True), one callback will be automatically\n        included to saved the test metrics. Moreover, a :class:`~callbacks.AtomicCSVLogger` will save the test\n        metrics in an output .tsv file.\n\n        Returns:\n            dict sorting of all the test metrics values by their names.\n        """"""\n        set_seeds(seed)\n\n        callbacks = [] if callbacks is None else callbacks\n\n        # Copy callback list.\n        callbacks = list(callbacks)\n\n        best_epoch_stats = self.load_checkpoint(checkpoint)\n\n        if len(self.model.metrics_names) > 0:\n            test_loss, test_metrics = self.model.evaluate_generator(test_generator, steps=steps, callbacks=callbacks)\n            if not isinstance(test_metrics, np.ndarray):\n                test_metrics = np.array([test_metrics])\n        else:\n            test_loss = self.model.evaluate_generator(test_generator, steps=steps, callbacks=callbacks)\n            test_metrics = np.array([])\n\n        test_metrics_names = [\'test_loss\'] + \\\n                             [\'test_\' + metric_name for metric_name in self.model.metrics_names]\n        test_metrics_values = np.concatenate(([test_loss], test_metrics))\n\n        test_metrics_dict = dict(zip(test_metrics_names, test_metrics_values))\n        test_metrics_str = \', \'.join(\'%s: %g\' % (col, val) for col, val in test_metrics_dict.items())\n        print(""On best model: %s"" % test_metrics_str)\n\n        if self.logging:\n            test_stats = pd.DataFrame([test_metrics_values], columns=test_metrics_names)\n            if best_epoch_stats is not None:\n                best_epoch_stats = best_epoch_stats.reset_index(drop=True)\n                test_stats = best_epoch_stats.join(test_stats)\n            test_stats.to_csv(self.test_log_filename, sep=\'\\t\', index=False)\n\n        return test_metrics_dict\n'"
poutyne/framework/iterators.py,0,"b'import itertools\nimport timeit\n\nimport numpy as np\n\n\nclass Step:\n    def __init__(self, number):\n        self.number = number\n\n        self.loss = None\n        self.metrics = None\n        self.size = None\n\n\ndef cycle(iterable):  # Equivalent to itertools cycle, without any extra memory requirement\n    while True:\n        for x in iterable:\n            yield x\n\n\ndef _get_step_iterator(steps, generator):\n    count_iterator = range(1, steps + 1) if steps is not None else itertools.count(1)\n    generator = cycle(generator) if steps is not None else generator\n    return zip(count_iterator, generator)\n\n\nclass StepIterator:\n    def __init__(self, generator, steps_per_epoch, batch_metrics_names, callback=None, mode=None):\n        # pylint: disable=too-many-arguments\n        self.generator = generator\n        self.steps_per_epoch = steps_per_epoch\n        self.batch_metrics_names = batch_metrics_names\n\n        self.on_batch_begin = lambda *_: None\n        self.on_batch_end = lambda *_: None\n\n        if mode == \'train\':\n            self.on_batch_begin = callback.on_train_batch_begin\n            self.on_batch_end = callback.on_train_batch_end\n        elif mode == \'test\':\n            self.on_batch_begin = callback.on_test_batch_begin\n            self.on_batch_end = callback.on_test_batch_end\n\n        self.losses_sum = 0.\n        self.metrics_sum = np.zeros(len(self.batch_metrics_names))\n        self.sizes_sum = 0.\n        self.epoch_metrics = None\n\n    @property\n    def loss(self):\n        return self.losses_sum / self.sizes_sum\n\n    @property\n    def metrics(self):\n        return self.metrics_sum / self.sizes_sum\n\n    def __iter__(self):\n        time_since_last_batch = timeit.default_timer()\n        for step, data in _get_step_iterator(self.steps_per_epoch, self.generator):\n            self.on_batch_begin(step, {})\n\n            step_data = Step(step)\n            yield step_data, data\n\n            self.losses_sum += step_data.loss * step_data.size\n            self.metrics_sum += step_data.metrics * step_data.size\n            self.sizes_sum += step_data.size\n\n            batch_end_time = timeit.default_timer()\n            batch_total_time = batch_end_time - time_since_last_batch\n            time_since_last_batch = batch_end_time\n\n            metrics_dict = dict(zip(self.batch_metrics_names, step_data.metrics))\n\n            batch_logs = {\n                \'batch\': step,\n                \'size\': step_data.size,\n                \'time\': batch_total_time,\n                \'loss\': step_data.loss,\n                **metrics_dict\n            }\n\n            self.on_batch_end(step, batch_logs)\n\n\nclass EpochIterator:\n    """"""\n    Epoch iterator used in the training phase of the model.\n    """"""\n\n    def __init__(self,\n                 train_generator,\n                 valid_generator,\n                 *,\n                 epochs,\n                 steps_per_epoch,\n                 validation_steps,\n                 initial_epoch=1,\n                 callback,\n                 batch_metrics_names,\n                 epoch_metrics_names):\n        self.train_generator = train_generator\n        self.valid_generator = valid_generator\n        self.epochs = epochs\n        self._init_steps(train_generator, valid_generator, steps_per_epoch, validation_steps)\n\n        self.initial_epoch = initial_epoch\n        self.callback = callback\n        self.batch_metrics_names = batch_metrics_names\n        self.epoch_metrics_names = epoch_metrics_names\n        self.epoch_logs = []\n        self.stop_training = False\n\n        params = {\'epochs\': self.epochs, \'steps\': self.steps_per_epoch}\n        self.callback.set_params(params)\n\n    def _init_steps(self, train_generator, valid_generator, steps_per_epoch, validation_steps):\n        self.steps_per_epoch = steps_per_epoch\n        self.validation_steps = validation_steps\n\n        if valid_generator is not None:\n            if validation_steps is None:\n                if hasattr(valid_generator, \'__len__\'):\n                    self.validation_steps = len(valid_generator)\n                elif steps_per_epoch is not None:\n                    self.validation_steps = steps_per_epoch\n        if steps_per_epoch is None and hasattr(train_generator, \'__len__\'):\n            self.steps_per_epoch = len(train_generator)\n\n    def __iter__(self):\n        self.callback.on_train_begin({})\n        for epoch in range(self.initial_epoch, self.epochs + 1):\n            self.callback.on_epoch_begin(epoch, {})\n            epoch_begin_time = timeit.default_timer()\n\n            train_step_iterator = StepIterator(self.train_generator,\n                                               self.steps_per_epoch,\n                                               self.batch_metrics_names,\n                                               self.callback,\n                                               mode=""train"")\n\n            valid_step_iterator = None\n            if self.valid_generator is not None:\n                valid_step_iterator = StepIterator(self.valid_generator, self.validation_steps,\n                                                   self.batch_metrics_names)\n\n            yield train_step_iterator, valid_step_iterator\n\n            val_dict = {}\n            if valid_step_iterator is not None:\n                val_metrics_dict = {\n                    \'val_\' + metric_name: metric\n                    for metric_name, metric in zip(self.batch_metrics_names, valid_step_iterator.metrics)\n                }\n                val_metrics_dict.update({\n                    \'val_\' + metric_name: metric\n                    for metric_name, metric in zip(self.epoch_metrics_names, valid_step_iterator.epoch_metrics)\n                })\n\n                val_dict = {\'val_loss\': valid_step_iterator.loss, **val_metrics_dict}\n\n            epoch_total_time = timeit.default_timer() - epoch_begin_time\n            metrics_dict = dict(zip(self.batch_metrics_names, train_step_iterator.metrics))\n            metrics_dict.update(dict(zip(self.epoch_metrics_names, train_step_iterator.epoch_metrics)))\n\n            epoch_log = {\n                \'epoch\': epoch,\n                \'loss\': train_step_iterator.loss,\n                \'time\': epoch_total_time,\n                **metrics_dict,\n                **val_dict\n            }\n            self.callback.on_epoch_end(epoch, epoch_log)\n\n            self.epoch_logs.append(epoch_log)\n\n            if self.stop_training:\n                break\n\n        self.callback.on_train_end({})\n'"
poutyne/framework/model.py,36,"b'# pylint: disable=too-many-lines\nimport contextlib\nimport warnings\nfrom collections import defaultdict\nfrom typing import Iterable, Mapping\nimport numbers\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom poutyne import torch_to_numpy, numpy_to_torch, torch_to\nfrom poutyne.framework.metrics import get_epoch_metric\nfrom poutyne.utils import TensorDataset\nfrom .callbacks import CallbackList, ProgressionCallback, Callback\nfrom .iterators import EpochIterator, _get_step_iterator, StepIterator\nfrom .metrics import get_loss_or_metric, get_callables_and_names, rename_doubles, flatten_metric_names\nfrom .optimizers import get_optimizer\nfrom .warning_manager import warning_settings\nfrom ..utils import _concat\n\n\nclass Model:\n    """"""\n    The Model class encapsulates a PyTorch network, a PyTorch optimizer, a loss function and\n    metric functions. It allows the user to train a neural network without hand-coding the\n    epoch/step logic.\n\n    Args:\n        network (torch.nn.Module): A PyTorch network.\n        optimizer (Union[torch.optim.Optimizer, str]): If torch.optim.Optimier, an initialized PyTorch.\n            If str, should be the optimizer\'s name in Pytorch (i.e. \'Adam\' for torch.optim.Adam).\n            (Default value = \'sgd\')\n        loss_function(Union[Callable, str]) It can be any PyTorch loss layer or custom loss function. It\n            can also be a string with the same name as a PyTorch loss function (either the functional or\n            object name). The loss function must have the signature ``loss_function(input, target)`` where\n            ``input`` is the prediction of the network and ``target`` is the ground truth.\n            (Default value = None)\n        batch_metrics (list): List of functions with the same signature as the loss function. Each metric\n            can be any PyTorch loss function. It can also be a string with the same name as a PyTorch\n            loss function (either the functional or object name). \'accuracy\' (or just \'acc\') is also a\n            valid metric. Each metric function is called on each batch of the optimization and on the\n            validation batches at the end of the epoch.\n            (Default value = None)\n        epoch_metrics (list): List of functions with the same signature as\n            :class:`~poutyne.framework.metrics.epoch_metrics.EpochMetric`\n            (Default value = None)\n\n    Note:\n        The name of each batch and epoch metric can be change by passing a tuple ``(name, metric)`` instead\n        of simply the metric function or object, where ``name`` is the alternative name of the metric.\n\n        Batch and epoch metrics can return multiple metrics (e.g. an epoch metric could return an F1-score\n        with the associated precision and recall). The metrics can returned via an iterable (tuple, list,\n        Numpy arrays, tensors, etc.) or via a mapping (e.g. a dict). However, in this case, the names of\n        the different metric has to be passed in some way. There are two ways to do so. The easiest one\n        is to pass the metric as a tuple ``(names, metric)`` where ``names`` is a tuple containing a name for\n        each metric returned. Another way is to override the attribute ``__name__`` of the function or object\n        so that it returns a tuple containing a name for all metrics returned. Note that, when the metric\n        returns a mapping, the names of the different metrics must be keys in the mapping.\n\n        Example:\n\n        .. code-block:: python\n\n            # Example with custom batch metrics\n            my_custom_metric = lambda input, target: 42.\n            my_custom_metric2 = lambda input, target: torch.tensor([42., 43.])\n            my_custom_metric3 = lambda input, target: {\'a\': 42., \'b\': 43.}\n            batch_metrics = [(\'custom_name\', my_custom_metric),\n                             ((\'metric_1\', \'metric_2\'), my_custom_metric2),\n                             ((\'a\', \'b\'), my_custom_metric3)]\n\n    Attributes:\n        network (torch.nn.Module): The associated PyTorch network.\n        optimizer (torch.optim.Optimizer): The associated PyTorch optimizer.\n        loss_function: The associated loss function.\n        batch_metrics (list): The associated metric functions for every batch.\n        epoch_metrics (list): The associated metric functions for every epoch.\n\n    Example:\n        Using Numpy arrays (or tensors) dataset::\n\n            from poutyne.framework import Model\n            import torch\n            import numpy as np\n\n            num_features = 20\n            num_classes = 5\n\n            # Our training dataset with 800 samples.\n            num_train_samples = 800\n            train_x = np.random.randn(num_train_samples, num_features).astype(\'float32\')\n            train_y = np.random.randint(num_classes, size=num_train_samples).astype(\'int64\')\n\n            # Our validation dataset with 200 samples.\n            num_valid_samples = 200\n            valid_x = np.random.randn(num_valid_samples, num_features).astype(\'float32\')\n            valid_y = np.random.randint(num_classes, size=num_valid_samples).astype(\'int64\')\n\n            pytorch_network = torch.nn.Linear(num_features, num_classes) # Our network\n\n            # We create and optimize our model\n            model = Model(pytorch_network, \'sgd\', \'cross_entropy\', batch_metrics=[\'accuracy\'])\n            model.fit(train_x, train_y,\n                      validation_data=(valid_x, valid_y),\n                      epochs=5,\n                      batch_size=32)\n\n        .. code-block:: none\n\n            Epoch 1/5 0.02s Step 25/25: loss: 1.719885, acc: 19.375000, val_loss: 1.667446, val_acc: 22.000000\n            Epoch 2/5 0.02s Step 25/25: loss: 1.705489, acc: 19.750000, val_loss: 1.660806, val_acc: 22.000000\n            Epoch 3/5 0.01s Step 25/25: loss: 1.692345, acc: 19.625000, val_loss: 1.655008, val_acc: 22.500000\n            ...\n\n        Using PyTorch DataLoader::\n\n           import torch\n           from torch.utils.data import DataLoader, TensorDataset\n           from poutyne.framework import Model\n\n           num_features = 20\n           num_classes = 5\n\n           # Our training dataset with 800 samples.\n           num_train_samples = 800\n           train_x = torch.rand(num_train_samples, num_features)\n           train_y = torch.randint(num_classes, (num_train_samples,), dtype=torch.long)\n           train_dataset = TensorDataset(train_x, train_y)\n           train_generator = DataLoader(train_dataset, batch_size=32)\n\n           # Our validation dataset with 200 samples.\n           num_valid_samples = 200\n           valid_x = torch.rand(num_valid_samples, num_features)\n           valid_y = torch.randint(num_classes, (num_valid_samples,), dtype=torch.long)\n           valid_dataset = TensorDataset(valid_x, valid_y)\n           valid_generator = DataLoader(valid_dataset, batch_size=32)\n\n           pytorch_network = torch.nn.Linear(num_features, num_train_samples)\n\n           model = Model(pytorch_network, \'sgd\', \'cross_entropy\', batch_metrics=[\'accuracy\'])\n           model.fit_generator(train_generator,\n                               valid_generator,\n                               epochs=5)\n\n        .. code-block:: none\n\n            Epoch 1/5 0.05s Step 25/25: loss: 6.752676, acc: 0.000000, val_loss: 6.575071, val_acc: 0.000000\n            Epoch 2/5 0.03s Step 25/25: loss: 6.454859, acc: 0.125000, val_loss: 6.279577, val_acc: 0.000000\n            Epoch 3/5 0.03s Step 25/25: loss: 6.158523, acc: 2.125000, val_loss: 5.985811, val_acc: 9.500000\n            ...\n\n    """"""\n\n    def __init__(self, network, optimizer, loss_function, *, batch_metrics=None, epoch_metrics=None):\n        batch_metrics = [] if batch_metrics is None else batch_metrics\n        epoch_metrics = [] if epoch_metrics is None else epoch_metrics\n\n        self.network = network\n        self.optimizer = get_optimizer(optimizer, self.network)\n        self.loss_function = get_loss_or_metric(loss_function)\n\n        self._set_metrics_attributes(batch_metrics, epoch_metrics)\n\n        self.device = None\n\n    def _set_metrics_attributes(self, batch_metrics, epoch_metrics):\n        batch_metrics = list(map(get_loss_or_metric, batch_metrics))\n        self.batch_metrics, batch_metrics_names = get_callables_and_names(batch_metrics)\n\n        epoch_metrics = list(map(get_epoch_metric, epoch_metrics))\n        self.epoch_metrics, epoch_metrics_names = get_callables_and_names(epoch_metrics)\n\n        batch_metrics_names, epoch_metrics_names = rename_doubles(batch_metrics_names, epoch_metrics_names)\n\n        self.unflatten_batch_metrics_names = batch_metrics_names\n        self.unflatten_epoch_metrics_names = epoch_metrics_names\n\n        self.batch_metrics_names = flatten_metric_names(batch_metrics_names)\n        self.epoch_metrics_names = flatten_metric_names(epoch_metrics_names)\n        self.metrics_names = self.batch_metrics_names + self.epoch_metrics_names\n\n    @contextlib.contextmanager\n    def _set_training_mode(self, training):\n        old_training = self.network.training\n        self.network.train(training)\n        with torch.set_grad_enabled(training):\n            yield\n        self.network.train(old_training)\n\n    def fit(self,\n            x,\n            y,\n            validation_data=None,\n            *,\n            batch_size=32,\n            epochs=1000,\n            steps_per_epoch=None,\n            validation_steps=None,\n            batches_per_step=1,\n            initial_epoch=1,\n            verbose=True,\n            callbacks=None):\n        # pylint: disable=line-too-long\n        """"""\n        Trains the network on a dataset. This method creates generators and calls\n        the :func:`~Model.fit_generator()` method.\n\n        Args:\n            x (Union[~torch.Tensor, ~numpy.ndarray] or Union[tuple, list] of Union[~torch.Tensor, ~numpy.ndarray]):\n                Training dataset. Union[Tensor, ndarray] if the model has a single input.\n                Union[tuple, list] of Union[Tensor, ndarray] if the model has multiple inputs.\n            y (Union[~torch.Tensor, ~numpy.ndarray] or Union[tuple, list] of Union[~torch.Tensor, ~numpy.ndarray]):\n                Target. Union[Tensor, ndarray] if the model has a single output.\n                Union[tuple, list] of Union[Tensor, ndarray] if the model has multiple outputs.\n            validation_data (Tuple[``x_val``, ``y_val``]):\n                Same format as ``x`` and ``y`` previously described. Validation dataset on which to\n                evaluate the loss and any model metrics at the end of each epoch. The model will not be\n                trained on this data.\n                (Default value = None)\n            batch_size (int): Number of samples given to the network at one time.\n                (Default value = 32)\n            epochs (int): Number of times the entire training dataset is seen.\n                (Default value = 1000)\n            steps_per_epoch (int, optional): Number of batch used during one epoch. Obviously, using\n                this argument may cause one epoch not to see the entire training dataset or see it\n                multiple times.\n                (Defaults the number of steps needed to see the entire training dataset)\n            validation_steps (int, optional): Same as for ``steps_per_epoch`` but for the validation\n                dataset.\n                (Defaults to the number of steps needed to see the entire validation dataset)\n            batches_per_step (int): Number of batches on which to compute the running loss before\n                backpropagating it through the network. Note that the total loss used for backpropagation is\n                the mean of the `batches_per_step` batch losses.\n                (Default value = 1)\n            initial_epoch (int, optional): Epoch at which to start training\n                (useful for resuming a previous training run).\n                (Default value = 1)\n            verbose (bool): Whether to display the progress of the training.\n                (Default value = True)\n            callbacks (List[~poutyne.framework.callbacks.Callback]): List of callbacks that will be called\n                during training.\n                (Default value = None)\n\n        Returns:\n            List of dict containing the history of each epoch.\n\n        Example:\n            .. code-block:: python\n\n                model = Model(pytorch_network, optimizer, loss_function)\n                history = model.fit(train_x, train_y,\n                                    validation_data=(valid_x, valid_y)\n                                    epochs=num_epochs,\n                                    batch_size=batch_size,\n                                    verbose=False)\n                print(*history, sep=""\\\\n"")\n\n            .. code-block:: python\n\n                {\'epoch\': 1, \'loss\': 1.7198852968215943, \'time\': 0.019999928001197986, \'acc\': 19.375, \'val_loss\': 1.6674459838867188, \'val_acc\': 22.0}\n                {\'epoch\': 2, \'loss\': 1.7054892110824584, \'time\': 0.015421080999658443, \'acc\': 19.75, \'val_loss\': 1.660806336402893, \'val_acc\': 22.0}\n                {\'epoch\': 3, \'loss\': 1.6923445892333984, \'time\': 0.01363091799794347, \'acc\': 19.625, \'val_loss\': 1.6550078630447387, \'val_acc\': 22.5}\n                ...\n\n        """"""\n        train_generator = self._dataloader_from_data((x, y), batch_size=batch_size)\n        valid_generator = None\n        if validation_data is not None:\n            valid_generator = self._dataloader_from_data(validation_data, batch_size=batch_size)\n\n        return self.fit_generator(train_generator,\n                                  valid_generator=valid_generator,\n                                  epochs=epochs,\n                                  steps_per_epoch=steps_per_epoch,\n                                  validation_steps=validation_steps,\n                                  batches_per_step=batches_per_step,\n                                  initial_epoch=initial_epoch,\n                                  verbose=verbose,\n                                  callbacks=callbacks)\n\n    def _dataloader_from_data(self, args, batch_size):\n        args = numpy_to_torch(args)\n        dataset = TensorDataset(*args) if len(args) > 1 else args[0]\n        generator = DataLoader(dataset, batch_size)\n        return generator\n\n    def fit_generator(self,\n                      train_generator,\n                      valid_generator=None,\n                      *,\n                      epochs=1000,\n                      steps_per_epoch=None,\n                      validation_steps=None,\n                      batches_per_step=1,\n                      initial_epoch=1,\n                      verbose=True,\n                      callbacks=None):\n        # pylint: disable=line-too-long\n        """"""\n        Trains the network on a dataset using a generator.\n\n        Args:\n            train_generator: Generator-like object for the training dataset. The generator must\n                yield a batch in the form of a tuple (x, y) where ``x`` is the input and ``y`` is the\n                target. The batch size is inferred from ``x`` and ``y``. See :func:`get_batch_size()` for\n                details on the inferring algorithm. The loss and the metrics are averaged using this\n                batch size. If the batch size cannot be inferred then a warning is raised and the\n                ""batch size"" defaults to 1.\n\n                If the generator does not have a method ``__len__()``, either the ``steps_per_epoch``\n                argument must be provided, or the iterator returned raises a StopIteration exception at\n                the end of the training dataset. PyTorch DataLoaders object do provide a ``__len__()``\n                method.\n\n                Before each epoch, the method ``__iter__()`` on the generator is called and the method\n                ``__next__()`` is called for each step on resulting object returned by ``__iter__()``.\n                Notice that a call to ``__iter__()`` on a generator made using the python keyword\n                ``yield`` returns the generator itself.\n            valid_generator (optional): Generator-like object for the validation dataset. This generator\n                is optional. The generator is used the same way as the  generator ``train_generator``. If\n                the generator does not have a method ``__len__()``, either the ``validation_steps`` or the\n                ``steps_per_epoch`` argument must be provided or the iterator returned raises a StopIteration\n                exception at the end of the validation dataset.\n                (Default value = None)\n            epochs (int): Number of times the entire training dataset is seen.\n                (Default value = 1000)\n            steps_per_epoch (int, optional): Number of batch used during one epoch. Obviously, using this\n                argument may cause one epoch not to see the entire training dataset or see it multiple times.\n                See argument ``train_generator`` and ``valid_generator`` for more details of how\n                ``steps_per_epoch`` is used.\n            validation_steps (int, optional): Same as for ``steps_per_epoch`` but for the validation dataset.\n                See argument ``valid_generator`` for more details of how ``validation_steps`` is used.\n            batches_per_step (int): Number of batches on which to compute the running loss before\n                backpropagating it through the network. Note that the total loss used for backpropagation is\n                the mean of the `batches_per_step` batch losses.\n                (Default value = 1)\n            initial_epoch (int, optional): Epoch at which to start training (useful for resuming a previous\n                training run).\n                (Default value = 1)\n            verbose (bool): Whether to display the progress of the training.\n                (Default value = True)\n            callbacks (List[~poutyne.framework.callbacks.Callback]): List of callbacks that will be called during\n                training. (Default value = None)\n\n        Returns:\n            List of dict containing the history of each epoch.\n\n        Example:\n            .. code-block:: python\n\n                model = Model(pytorch_network, optimizer, loss_function)\n                history = model.fit_generator(train_generator,\n                                              valid_generator,\n                                              epochs=num_epochs,\n                                              verbose=False)\n                print(*history, sep=""\\\\n"")\n\n            .. code-block:: python\n\n                {\'epoch\': 1, \'loss\': 1.7198852968215943, \'time\': 0.019999928001197986, \'acc\': 19.375, \'val_loss\': 1.6674459838867188, \'val_acc\': 22.0}\n                {\'epoch\': 2, \'loss\': 1.7054892110824584, \'time\': 0.015421080999658443, \'acc\': 19.75, \'val_loss\': 1.660806336402893, \'val_acc\': 22.0}\n                {\'epoch\': 3, \'loss\': 1.6923445892333984, \'time\': 0.01363091799794347, \'acc\': 19.625, \'val_loss\': 1.6550078630447387, \'val_acc\': 22.5}\n                ...\n\n        """"""\n        if self.optimizer is None:\n            raise ValueError(""Impossible to fit when optimizer is None."")\n\n        self._transfer_optimizer_state_to_right_device()\n\n        callbacks = [] if callbacks is None else callbacks\n\n        if verbose:\n            callbacks = [ProgressionCallback()] + callbacks\n        callback_list = CallbackList(callbacks)\n        callback_list.set_model(self)\n\n        self.stop_training = False\n\n        epoch_iterator = EpochIterator(train_generator,\n                                       valid_generator,\n                                       epochs=epochs,\n                                       steps_per_epoch=steps_per_epoch,\n                                       validation_steps=validation_steps,\n                                       initial_epoch=initial_epoch,\n                                       callback=callback_list,\n                                       batch_metrics_names=self.batch_metrics_names,\n                                       epoch_metrics_names=self.epoch_metrics_names)\n\n        if batches_per_step > 1:\n            self._fit_generator_n_batches_per_step(epoch_iterator, callback_list, batches_per_step)\n        else:\n            self._fit_generator_one_batch_per_step(epoch_iterator, callback_list)\n\n        return epoch_iterator.epoch_logs\n\n    def _fit_generator_n_batches_per_step(self, epoch_iterator, callback_list, batches_per_step):\n        for train_step_iterator, valid_step_iterator in epoch_iterator:\n            examples_in_step = 0\n\n            with self._set_training_mode(True):\n                for step, (x, y) in train_step_iterator:\n                    step.size = self.get_batch_size(x, y)\n\n                    examples_in_step += step.size\n\n                    step.loss, step.metrics, did_backprop, _ = self._fit_batch_n_batches_per_step(\n                        x, y, batches_per_step, examples_in_step, callback=callback_list, step=step)\n\n                    if did_backprop:\n                        examples_in_step = 0\n\n            if not did_backprop:\n                # Did not step after last batch\n                self._adjust_step_size(examples_in_step)\n                self.optimizer.step()\n\n            train_step_iterator.epoch_metrics = self._get_epoch_metrics()\n\n            if valid_step_iterator is not None:\n                self._validate(valid_step_iterator)\n                valid_step_iterator.epoch_metrics = self._get_epoch_metrics()\n\n            epoch_iterator.stop_training = self.stop_training\n\n    def _fit_batch_n_batches_per_step(self,\n                                      x,\n                                      y,\n                                      batches_per_step,\n                                      examples_in_step,\n                                      *,\n                                      callback=Callback(),\n                                      step=None,\n                                      return_pred=False):\n        # pylint: disable=too-many-locals\n        zero_all_gradients = ((step.number - 1) % batches_per_step == 0)\n        do_backprop = (step.number % batches_per_step == 0)\n\n        if zero_all_gradients:\n            self.optimizer.zero_grad()\n\n        loss_tensor, metrics, pred_y = self._compute_loss_and_metrics(x,\n                                                                      y,\n                                                                      return_loss_tensor=True,\n                                                                      return_pred=return_pred)\n\n        adjusted_loss_tensor = loss_tensor * step.size\n        adjusted_loss_tensor.backward()\n\n        callback.on_backward_end(step)\n\n        if do_backprop:\n            self._adjust_step_size(examples_in_step)\n            self.optimizer.step()\n\n        loss = float(loss_tensor)\n        return loss, metrics, do_backprop, pred_y\n\n    def _fit_generator_one_batch_per_step(self, epoch_iterator, callback_list):\n        for train_step_iterator, valid_step_iterator in epoch_iterator:\n            with self._set_training_mode(True):\n                for step, (x, y) in train_step_iterator:\n                    step.loss, step.metrics, _ = self._fit_batch(x, y, callback=callback_list, step=step.number)\n                    step.size = self.get_batch_size(x, y)\n\n            train_step_iterator.epoch_metrics = self._get_epoch_metrics()\n\n            if valid_step_iterator is not None:\n                self._validate(valid_step_iterator)\n                valid_step_iterator.epoch_metrics = self._get_epoch_metrics()\n\n            epoch_iterator.stop_training = self.stop_training\n\n    def _fit_batch(self, x, y, *, callback=Callback(), step=None, return_pred=False):\n        self.optimizer.zero_grad()\n\n        loss_tensor, metrics, pred_y = self._compute_loss_and_metrics(x,\n                                                                      y,\n                                                                      return_loss_tensor=True,\n                                                                      return_pred=return_pred)\n\n        loss_tensor.backward()\n        callback.on_backward_end(step)\n        self.optimizer.step()\n\n        loss = float(loss_tensor)\n        return loss, metrics, pred_y\n\n    def _adjust_step_size(self, examples_in_step):\n        for param in self.network.parameters():\n            if param.grad is not None:\n                param.grad /= examples_in_step\n\n    def _process_input(self, *args):\n        args = numpy_to_torch(args)\n        if self.device is not None:\n            args = torch_to(args, self.device)\n        return args[0] if len(args) == 1 else args\n\n    def train_on_batch(self, x, y, return_pred=False):\n        """"""\n        Trains the network for the batch ``(x, y)`` and computes the loss and the metrics, and\n        optionally returns the predictions.\n\n        Args:\n            x: Input data as a batch.\n            y: Target data as a batch.\n            return_pred (bool, optional): Whether to return the predictions.\n                (Default value = False)\n\n        Returns:\n            Float ``loss`` if no metrics were specified and ``return_pred`` is false.\n\n            Otherwise, tuple ``(loss, metrics)`` if ``return_pred`` is false.\n            ``metrics`` is a Numpy array of size ``n``, where ``n`` is the\n            number of metrics if ``n > 1``. If ``n == 1``, then ``metrics`` is a\n            float. If ``n == 0``, the ``metrics`` is omitted.\n\n            Tuple ``(loss, metrics, pred_y)`` if ``return_pred`` is true where\n            ``pred_y`` is the predictions with tensors converted into Numpy\n            arrays.\n        """"""\n        if self.optimizer is None:\n            raise ValueError(""Impossible to fit when optimizer is None."")\n\n        with self._set_training_mode(True):\n            self._transfer_optimizer_state_to_right_device()\n            loss, metrics, pred_y = self._fit_batch(x, y, return_pred=return_pred)\n        return self._format_return(loss, metrics, pred_y, return_pred)\n\n    def _format_return(self, loss, metrics, pred_y, return_pred, true_y=None, return_ground_truth=False):\n        # pylint: disable=too-many-arguments\n        ret = (loss, )\n\n        ret += tuple(metrics.tolist()) if len(metrics) <= 1 else (metrics, )\n\n        if return_pred:\n            ret += (pred_y, )\n\n        if return_ground_truth:\n            ret += (true_y, )\n\n        return ret[0] if len(ret) == 1 else ret\n\n    def predict(self, x, batch_size=32):\n        """"""\n        Returns the predictions of the network given a dataset ``x``, where the tensors are\n        converted into Numpy arrays.\n\n        Args:\n            x (Union[~torch.Tensor, ~numpy.ndarray] or Union[tuple, list] of Union[~torch.Tensor, ~numpy.ndarray]):\n                Input to the model. Union[Tensor, ndarray] if the model has a single input.\n                Union[tuple, list] of Union[Tensor, ndarray] if the model has multiple inputs.\n            batch_size (int): Number of samples given to the network at one time.\n                (Default value = 32)\n\n        Returns:\n            Numpy arrays of the predictions.\n        """"""\n        x = x if isinstance(x, (tuple, list)) else (x, )\n        generator = self._dataloader_from_data(x, batch_size=batch_size)\n        return self.predict_generator(generator, concatenate_returns=True)\n\n    def predict_generator(self, generator, *, steps=None, concatenate_returns=None):\n        """"""\n        Returns the predictions of the network given batches of samples ``x``, where the tensors are\n        converted into Numpy arrays.\n\n        generator: Generator-like object for the dataset. The generator must yield a batch of\n            samples. See the :func:`fit_generator()` method for details on the types of generators\n            supported. This should only yield input data ``x`` and not the target ``y``.\n        steps (int, optional): Number of iterations done on ``generator``.\n            (Defaults the number of steps needed to see the entire dataset)\n        concatenate_returns (bool, optional): Whether to concatenate the predictions\n            or the ground truths when returning them. Currently defaults to False but\n            will default to True in the next version. A warning is raised if not set in\n            the current version but the warning will be removed in the version. Disabling\n            the warning as instructed in it switches to the new behavior when\n            ``concatenate_returns`` is not set.\n\n        Returns:\n            List of the predictions of each batch with tensors converted into Numpy arrays.\n        """"""\n        if concatenate_returns is None and warning_settings[\'concatenate_returns\'] == \'warn\':\n            warnings.warn(""In the next version of Poutyne, the argument \'concatenate_returns\' ""\n                          ""of \'predict_generator\' will default to True. To avoid this warning, ""\n                          ""set \'concatenate_returns\' to an appropriate boolean value in the ""\n                          ""keyword arguments or get the new behavior by disabling this warning with\\n""\n                          ""from poutyne.framework import warning_settings\\n""\n                          ""warning_settings[\'concatenate_returns\'] = \'ignore\'\\n""\n                          ""This warning will be removed in the next version."")\n            concatenate_returns = False\n        elif concatenate_returns is None:\n            concatenate_returns = True\n\n        if steps is None and hasattr(generator, \'__len__\'):\n            steps = len(generator)\n        pred_y = []\n        with self._set_training_mode(False):\n            for _, x in _get_step_iterator(steps, generator):\n                x = self._process_input(x)\n                x = x if isinstance(x, (tuple, list)) else (x, )\n                pred_y.append(torch_to_numpy(self.network(*x)))\n        if concatenate_returns:\n            return _concat(pred_y)\n        return pred_y\n\n    def predict_on_batch(self, x):\n        """"""\n        Returns the predictions of the network given a batch ``x``, where the tensors are converted\n        into Numpy arrays.\n\n        Args:\n            x: Input data as a batch.\n        Returns:\n            The predictions with tensors converted into Numpy arrays.\n        """"""\n        with self._set_training_mode(False):\n            x = self._process_input(x)\n            x = x if isinstance(x, (tuple, list)) else (x, )\n            return torch_to_numpy(self.network(*x))\n\n    def evaluate(self, x, y, *, batch_size=32, return_pred=False, callbacks=None):\n        """"""\n        Computes the loss and the metrics of the network on batches of samples and optionally\n        returns the predictions.\n\n        Args:\n            x (Union[~torch.Tensor, ~numpy.ndarray] or Union[tuple, list] of Union[~torch.Tensor, ~numpy.ndarray]):\n                Input to the model. Union[Tensor, ndarray] if the model has a single input.\n                Union[tuple, list] of Union[Tensor, ndarray] if the model has multiple inputs.\n            y (Union[~torch.Tensor, ~numpy.ndarray] or Union[tuple, list] of Union[~torch.Tensor, ~numpy.ndarray]):\n                Target, corresponding ground truth.\n                Union[Tensor, ndarray] if the model has a single output.\n                Union[tuple, list] of Union[Tensor, ndarray] if the model has multiple outputs.\n            batch_size (int): Number of samples given to the network at one time.\n                (Default value = 32)\n            return_pred (bool, optional): Whether to return the predictions.\n                (Default value = False)\n            callbacks (List[~poutyne.framework.callbacks.Callback]): List of callbacks that will be called during\n                testing. (Default value = None)\n\n        Returns:\n            Tuple ``(loss, metrics, pred_y)`` where specific elements are omitted if not\n            applicable. If only loss is applicable, then it is returned as a float.\n\n            ``metrics`` is a Numpy array of size ``n``, where ``n`` is the\n            number of batch metrics plus the number of epoch metrics if ``n > 1``. If\n            ``n == 1``, then ``metrics`` is a float. If ``n == 0``, the ``metrics`` is\n            omitted. The first elements of ``metrics`` are the batch metrics and are\n            followed by the epoch metrics. See the :func:`~Model.fit_generator()` method\n            for examples with batch metrics and epoch metrics.\n\n            If ``return_pred`` is True, ``pred_y`` is the list of the predictions\n            of each batch with tensors converted into Numpy arrays. It is otherwise omitted.\n\n        """"""\n        callbacks = [] if callbacks is None else callbacks\n\n        # Copy callback list.\n        callbacks = list(callbacks)\n\n        generator = self._dataloader_from_data((x, y), batch_size=batch_size)\n        return self.evaluate_generator(generator,\n                                       steps=len(generator),\n                                       return_pred=return_pred,\n                                       concatenate_returns=True,\n                                       callbacks=callbacks)\n\n    def evaluate_generator(self,\n                           generator,\n                           *,\n                           steps=None,\n                           return_pred=False,\n                           return_ground_truth=False,\n                           concatenate_returns=None,\n                           callbacks=None):\n        # pylint: disable=too-many-locals\n        """"""\n        Computes the loss and the metrics of the network on batches of samples and optionaly returns\n        the predictions.\n\n        Args:\n            generator: Generator-like object for the dataset. See the :func:`~Model.fit_generator()` method for\n                details on the types of generators supported.\n            steps (int, optional): Number of iterations done on ``generator``.\n                (Defaults the number of steps needed to see the entire dataset)\n            return_pred (bool, optional): Whether to return the predictions.\n                (Default value = False)\n            return_ground_truth (bool, optional): Whether to return the ground truths.\n                (Default value = False)\n            concatenate_returns (bool, optional): Whether to concatenate the predictions\n                or the ground truths when returning them. Currently defaults to False but\n                will default to True in the next version. A warning is raised if not set in\n                the current version but the warning will be removed in the version. Disabling\n                the warning as instructed in it switches to the new behavior when\n                ``concatenate_returns`` is not set.\n            callbacks (List[~poutyne.framework.callbacks.Callback]): List of callbacks that will be called during\n                testing. (Default value = None)\n\n        Returns:\n            Tuple ``(loss, metrics, pred_y, true_y)`` where specific elements are\n            omitted if not applicable. If only loss is applicable, then it is returned\n            as a float.\n\n            ``metrics`` is a Numpy array of size ``n``, where ``n`` is the\n            number of batch metrics plus the number of epoch metrics if ``n > 1``. If\n            ``n == 1``, then ``metrics`` is a float. If ``n == 0``, the ``metrics`` is\n            omitted. The first elements of ``metrics`` are the batch metrics and are\n            followed by the epoch metrics.\n\n            If ``return_pred`` is True, ``pred_y`` is the list of the predictions\n            of each batch with tensors converted into Numpy arrays. It is otherwise ommited.\n\n            If ``return_ground_truth`` is True, ``true_y`` is the list of the ground truths\n            of each batch with tensors converted into Numpy arrays. It is otherwise ommited.\n        Example:\n            With no metrics:\n\n            .. code-block:: python\n\n                model = Model(pytorch_network, optimizer, loss_function,\n                              batch_metrics=None)\n                loss = model.evaluate_generator(test_generator)\n\n            With only one batch metric:\n\n            .. code-block:: python\n\n                model = Model(pytorch_network, optimizer, loss_function,\n                              batch_metrics=[my_metric_fn])\n                loss, my_metric = model.evaluate_generator(test_generator)\n\n            With several batch metrics:\n\n            .. code-block:: python\n\n                model = Model(pytorch_network, optimizer, loss_function,\n                              batch_metrics=[my_metric1_fn, my_metric2_fn])\n                loss, (my_metric1, my_metric2) = model.evaluate_generator(test_generator)\n\n            With one batch metric and one epoch metric:\n\n            .. code-block:: python\n\n                model = Model(pytorch_network, optimizer, loss_function,\n                              batch_metrics=[my_metric_fn], epoch_metrics=[MyEpochMetricClass()])\n                loss, (my_batch_metric, my__epoch_metric) = model.evaluate_generator(test_generator)\n\n            With batch metrics and ``return_pred`` flag:\n\n            .. code-block:: python\n\n                model = Model(pytorch_network, optimizer, loss_function,\n                              batch_metrics=[my_metric1_fn, my_metric2_fn])\n                loss, (my_metric1, my_metric2), pred_y = model.evaluate_generator(\n                    test_generator, return_pred=True\n                )\n\n            With batch metrics, ``return_pred`` and ``return_ground_truth`` flags:\n\n            .. code-block:: python\n\n                model = Model(pytorch_network, optimizer, loss_function,\n                              batch_metrics=[my_metric1_fn, my_metric2_fn])\n                loss, (my_metric1, my_metric2), pred_y, true_y = model.evaluate_generator(\n                    test_generator, return_pred=True, return_ground_truth=True\n                )\n        """"""\n        if (return_pred or return_ground_truth) \\\n                and concatenate_returns is None and warning_settings[\'concatenate_returns\'] == \'warn\':\n            warnings.warn(""In the next version of Poutyne, the argument \'concatenate_returns\' ""\n                          ""of \'evaluate_generator\' will default to True. To avoid this warning, ""\n                          ""set \'concatenate_returns\' to an appropriate boolean value in the ""\n                          ""keyword arguments or get the new behavior by disabling this warning with\\n""\n                          ""from poutyne.framework import warning_settings\\n""\n                          ""warning_settings[\'concatenate_returns\'] = \'ignore\'\\n""\n                          ""This warning will be removed in the next version."")\n            concatenate_returns = False\n        elif concatenate_returns is None:\n            concatenate_returns = True\n\n        callbacks = [] if callbacks is None else callbacks\n\n        callback_list = CallbackList(callbacks)\n        callback_list.set_model(self)\n\n        callback_list.on_test_begin({})\n\n        if steps is None:\n            steps = len(generator)\n        step_iterator = StepIterator(generator, steps, self.batch_metrics_names, callback_list, mode=""test"")\n        loss, batch_metrics, pred_y, true_y = self._validate(step_iterator,\n                                                             return_pred=return_pred,\n                                                             return_ground_truth=return_ground_truth)\n        epoch_metrics = self._get_epoch_metrics()\n        metrics = np.concatenate((batch_metrics, epoch_metrics))\n\n        if return_pred and concatenate_returns:\n            pred_y = _concat(pred_y)\n        if return_ground_truth and concatenate_returns:\n            true_y = _concat(true_y)\n\n        res = self._format_return(loss, metrics, pred_y, return_pred, true_y, return_ground_truth)\n\n        callback_list.on_test_end(res)\n\n        return res\n\n    def evaluate_on_batch(self, x, y, *, return_pred=False):\n        """"""\n        Computes the loss and the metrics of the network on a single batch of samples and optionally\n        returns the predictions.\n\n        Args:\n            x: Input data as a batch.\n            y: Target data as a batch.\n            return_pred (bool, optional): Whether to return the predictions for ``batch``.\n                (Default value = False)\n\n        Returns:\n            Tuple ``(loss, metrics, pred_y)`` where specific elements are omitted if not\n            applicable. If only loss is applicable, then it is returned as a float.\n\n            `metrics`` is a Numpy array of size ``n``, where ``n`` is the\n            number of metrics if ``n > 1``. If ``n == 1``, then ``metrics`` is a\n            float. If ``n == 0``, the ``metrics`` is omitted.\n\n            If ``return_pred`` is True, ``pred_y`` is the list of the predictions\n            of each batch with tensors converted into Numpy arrays. It is otherwise ommited.\n        """"""\n        with self._set_training_mode(False):\n            loss, metrics, pred_y = self._compute_loss_and_metrics(x, y, return_pred=return_pred)\n        return self._format_return(loss, metrics, pred_y, return_pred)\n\n    def _validate(self, step_iterator, return_pred=False, return_ground_truth=False):\n        pred_list = None\n        true_list = None\n        if return_pred:\n            pred_list = []\n        if return_ground_truth:\n            true_list = []\n\n        with self._set_training_mode(False):\n            for step, (x, y) in step_iterator:\n                step.loss, step.metrics, pred_y = self._compute_loss_and_metrics(x, y, return_pred=return_pred)\n                if return_pred:\n                    pred_list.append(pred_y)\n                if return_ground_truth:\n                    true_list.append(torch_to_numpy(y))\n\n                step.size = self.get_batch_size(x, y)\n\n        return step_iterator.loss, step_iterator.metrics, pred_list, true_list\n\n    def _compute_loss_and_metrics(self, x, y, return_loss_tensor=False, return_pred=False):\n        x, y = self._process_input(x, y)\n        x = x if isinstance(x, (list, tuple)) else (x, )\n        pred_y = self.network(*x)\n        loss = self.loss_function(pred_y, y)\n        if not return_loss_tensor:\n            loss = float(loss)\n        with torch.no_grad():\n            metrics = self._compute_batch_metrics(pred_y, y)\n            for epoch_metric in self.epoch_metrics:\n                epoch_metric(pred_y, y)\n\n        pred_y = torch_to_numpy(pred_y) if return_pred else None\n        return loss, metrics, pred_y\n\n    def _compute_batch_metrics(self, pred_y, y):\n        metrics = [metric(pred_y, y) for metric in self.batch_metrics]\n        return self._compute_metric_array(metrics, self.unflatten_batch_metrics_names)\n\n    def _get_epoch_metrics(self):\n        metrics = [epoch_metric.get_metric() for epoch_metric in self.epoch_metrics]\n        for epoch_metric in self.epoch_metrics:\n            epoch_metric.reset()\n        return self._compute_metric_array(metrics, self.unflatten_epoch_metrics_names)\n\n    def _compute_metric_array(self, metrics_list, names_list):\n        def _get_metric(names, metrics):\n            names = [names] if isinstance(names, str) else names\n            values = None\n            if (torch.is_tensor(metrics) or isinstance(metrics, np.ndarray)) and len(metrics.shape) == 0:\n                values = [float(metrics)]\n            elif isinstance(metrics, Mapping):\n                values = [float(metrics[name]) for name in names]\n            elif isinstance(metrics, Iterable):\n                values = [float(metric) for metric in metrics]\n            else:\n                values = [float(metrics)]\n            return values\n\n        return np.array(\n            [metric for names, metrics in zip(names_list, metrics_list) for metric in _get_metric(names, metrics)])\n\n    def get_batch_size(self, x, y):\n        """"""\n        This method infers the batch size of a batch. Here is the inferring algorithm used to compute the\n        batch size. ``x`` and ``y`` are tested in this order at each step of the inferring algorithm. If one\n        step succeed for one of ``x`` or ``y``, the algorithm stops.\n\n        - Step 1: if ``x`` or ``y`` is a tensor or a Numpy array, then the ``len()`` is returned.\n        - Step 2: if ``x`` or ``y`` is a list or a tuple, then the ``len()`` of the first element is returned if it\n          is a tensor or a Numpy array.\n        - Step 3: if ``x`` or ``y`` is a dict, then the value for the key ``\'batch_size\'`` is returned if it is of\n          integral type.\n        - Step 4: if ``x`` or ``y`` is a dict, then the ``len()`` of the first element of ``.values()`` is returned\n          if it is a tensor or a Numpy array.\n\n        If inferring the batch size is not possible, the batch size is set to 1 and, thus, the computed\n        loss and metrics at the end of each epoch is the mean of the batches\' losses and metrics. In which\n        case, a warning is also raised. To disable this warning, set\n\n        .. code-block:: python\n\n            from poutyne.framework import warning_settings\\n\n            warning_settings[\'batch_size\'] = \'ignore\'\\n\\n\n\n        Args:\n            x: Input data as a batch.\n            y: Target data as a batch.\n        """"""\n\n        def is_torch_or_numpy(v):\n            return torch.is_tensor(v) or isinstance(v, np.ndarray)\n\n        for v in [x, y]:\n            if is_torch_or_numpy(v):\n                return len(v)\n        for v in [x, y]:\n            if isinstance(v, (tuple, list)):\n                if is_torch_or_numpy(v[0]):\n                    return len(v[0])\n        for v in [x, y]:\n            if isinstance(v, dict):\n                if \'batch_size\' in v and isinstance(v[\'batch_size\'], numbers.Integral):\n                    return v[\'batch_size\']\n        for v in [x, y]:\n            if isinstance(v, dict):\n                first_value = list(v.values())[0]\n                if is_torch_or_numpy(first_value):\n                    return len(first_value)\n\n        if warning_settings[\'batch_size\'] == \'warn\':\n            warnings.warn(""Inferring the batch size is not possible. Hence, ""\n                          ""the batch size is set to 1 and, thus, the computed ""\n                          ""loss and metrics at the end of each epoch is the ""\n                          ""mean of the batches\' losses and metrics. To disable ""\n                          ""this warning, set\\n""\n                          ""from poutyne.framework import warning_settings\\n""\n                          ""warning_settings[\'batch_size\'] = \'ignore\'\\n\\n""\n                          ""Here is the inferring algorithm used to compute the ""\n                          ""batch size. \'x\' and \'y\' are tested in this order at ""\n                          ""each step of the inferring algorithm. If one step ""\n                          ""succeed for one of \'x\' or \'y\', the algorithm stops.\\n\\n""\n                          ""Step 1: if \'x\' or \'y\' is a tensor or a Numpy array, ""\n                          ""then the \'len()\' is returned.\\n""\n                          ""Step 2: if \'x\' or \'y\' is a list or a tuple, then the ""\n                          ""\'len()\' of the first element is returned if it is a ""\n                          ""tensor or a Numpy array.\\n""\n                          ""Step 3: if \'x\' or \'y\' is a dict, then the value for ""\n                          ""the key \'batch_size\' is returned if it is of integral ""\n                          ""type.\\n""\n                          ""Step 4: if \'x\' or \'y\' is a dict, then the \'len()\' of ""\n                          ""the first element of \'.values()\' is returned if it is a ""\n                          ""tensor or a Numpy array.\\n"")\n        return 1\n\n    def load_weights(self, f):\n        """"""\n        Loads the weights saved using the :func:`torch.save()` method or the :func:`save_weights()` method\n        of this class. Contrary to :func:`torch.load()`, the weights are not transfered to the device\n        from which they were saved from. In other words, the PyTorch module will stay on the same\n        device it already is on.\n\n        Args:\n            f: File-like object (has to implement fileno that returns a file descriptor) or string\n                containing a file name.\n        """"""\n        self.set_weights(torch.load(f, map_location=\'cpu\'))\n\n    def save_weights(self, f):\n        """"""\n        Saves the weights of the current network.\n\n        Args:\n            f: File-like object (has to implement fileno that returns a file descriptor) or string\n                containing a file name.\n        """"""\n        torch.save(self.network.state_dict(), f)\n\n    def load_optimizer_state(self, f):\n        """"""\n        Loads the optimizer state saved using the :func:`torch.save()` method or the\n        :func:`save_optimizer_state()` method of this class.\n\n        Args:\n            f: File-like object (has to implement fileno that returns a file descriptor) or string\n                containing a file name.\n        """"""\n        self.optimizer.load_state_dict(torch.load(f, map_location=\'cpu\'))\n\n    def save_optimizer_state(self, f):\n        """"""\n        Saves the state of the current optimizer.\n\n        Args:\n            f: File-like object (has to implement fileno that returns a file descriptor) or string\n                containing a file name.\n        """"""\n        torch.save(self.optimizer.state_dict(), f)\n\n    def _transfer_optimizer_state_to_right_device(self):\n        if self.optimizer is None:\n            return\n\n        # Since the optimizer state is loaded on CPU, it will crash when the optimizer will receive\n        # gradient for parameters not on CPU. Thus, for each parameter, we transfer its state in the\n        # optimizer on the same device as the parameter itself just before starting the\n        # optimization.\n        for group in self.optimizer.param_groups:\n            for p in group[\'params\']:\n                if p in self.optimizer.state:\n                    for _, v in self.optimizer.state[p].items():\n                        if torch.is_tensor(v) and p.device != v.device:\n                            v.data = v.data.to(p.device)\n\n    def _get_named_optimizer_attrs(self):\n        param_to_name = {param: name for name, param in self.network.named_parameters()}\n\n        param_name_groups = []\n        for group in self.optimizer.param_groups:\n            param_name_groups.append([param_to_name[param] for param in group[\'params\']])\n\n        named_state = {param_to_name[param]: state for param, state in self.optimizer.state.items()}\n\n        return param_name_groups, named_state\n\n    def _set_named_optimizer_attrs(self, param_name_groups, named_state):\n        name_to_param = dict(self.network.named_parameters())\n\n        for param_name_group, optim_group in zip(param_name_groups, self.optimizer.param_groups):\n            optim_group[\'params\'] = [\n                name_to_param[param_name] if optim_param is not name_to_param[param_name] else optim_param\n                for param_name, optim_param in zip(param_name_group, optim_group[\'params\'])\n            ]\n\n        self.optimizer.state = defaultdict(dict, {name_to_param[name]: state for name, state in named_state})\n\n    @contextlib.contextmanager\n    def _update_optim_device(self):\n        if self.optimizer is None:\n            yield\n            return\n\n        param_name_groups, named_state = self._get_named_optimizer_attrs()\n        try:\n            yield\n        finally:\n            self._set_named_optimizer_attrs(param_name_groups, named_state)\n\n    def get_weights(self):\n        """"""\n        Returns a dictionary containing the parameters of the network. The tensors are just\n        references to the parameters. To get copies of the weights, see the :func:`get_weight_copies()`\n        method.\n        """"""\n        return self.network.state_dict()\n\n    def get_weight_copies(self):\n        """"""\n        Returns a dictionary containing copies of the parameters of the network.\n        """"""\n        weights = self.get_weights()\n        for k in weights.keys():\n            weights[k] = weights[k].cpu().clone()\n        return weights\n\n    def set_weights(self, weights):\n        """"""\n        Modifies the weights of the network with the given weights.\n\n        Args:\n            weights (dict): Weights returned by either :func:`get_weights()` or :func:`get_weight_copies()`.\n        """"""\n        self.network.load_state_dict(weights)\n\n    def cuda(self, *args, **kwargs):\n        """"""\n        Tranfers the network on the GPU. The arguments are passed to the :meth:`torch.nn.Module.cuda()` method.\n        Notice that the device is saved so that the batches can send to the right device before passing it to\n        the network.\n\n        Note:\n            PyTorch optimizers assume that the parameters have been transfered to the right device\n            before their creations. Furthermore, future versions of PyTorch will no longer modify\n            the parameters of a PyTorch module in-place when transferring them to another device.\n            See this `issue <https://github.com/pytorch/pytorch/issues/7844>`_ and this\n            `pull request <https://github.com/pytorch/pytorch/pull/21613>`_ for details.\n\n            Since Poutyne supposes that the optimizer has been initialized before the Poutyne Model,\n            necessarily the parameters are not guaranteed to be in sync with those contained in the\n            optimizer once the PyTorch module is transferred to another device. Thus, this method\n            takes care of this inconsistency by updating the parameters inside the optimizer.\n\n        Returns:\n            `self`.\n        """"""\n        with self._update_optim_device():\n            self.network.cuda(*args, **kwargs)\n\n        # Assuming the PyTorch module has at least one parameter.\n        self.device = next(self.network.parameters()).device\n\n        self._transfer_loss_and_metrics_modules_to_right_device()\n\n        return self\n\n    def cpu(self, *args, **kwargs):\n        """"""\n        Tranfers the network on the CPU. The arguments are passed to the :meth:`torch.nn.Module.cpu()`\n        method. Notice that the device is saved so that the batches can send to the right device\n        before passing it to the network.\n\n        Note:\n            PyTorch optimizers assume that the parameters have been transfered to the right device\n            before their creations. Furthermore, future versions of PyTorch will no longer modify\n            the parameters of a PyTorch module in-place when transferring them to another device.\n            See this `issue <https://github.com/pytorch/pytorch/issues/7844>`_ and this\n            `pull request <https://github.com/pytorch/pytorch/pull/21613>`_ for details.\n\n            Since Poutyne supposes that the optimizer has been initialized before the Poutyne Model,\n            necessarily the parameters are not guaranteed to be in sync with those contained in the\n            optimizer once the PyTorch module is transferred to another device. Thus, this method\n            takes care of this inconsistency by updating the parameters inside the optimizer.\n\n        Returns:\n            `self`.\n        """"""\n        with self._update_optim_device():\n            self.network.cpu(*args, **kwargs)\n\n        # Assuming the PyTorch module has at least one parameter.\n        self.device = next(self.network.parameters()).device\n\n        self._transfer_loss_and_metrics_modules_to_right_device()\n\n        return self\n\n    def to(self, device):\n        """"""\n        Tranfers the network on the specified device. The device is saved so that the batches can\n        send to the right device before passing it to the network.\n\n        Note:\n            PyTorch optimizers assume that the parameters have been transfered to the right device\n            before their creations. Furthermore, future versions of PyTorch will no longer modify\n            the parameters of a PyTorch module in-place when transferring them to another device.\n            See this `issue <https://github.com/pytorch/pytorch/issues/7844>`_ and this\n            `pull request <https://github.com/pytorch/pytorch/pull/21613>`_ for details.\n\n            Since Poutyne supposes that the optimizer has been initialized before the Poutyne Model,\n            necessarily the parameters are not guaranteed to be in sync with those contained in the\n            optimizer once the PyTorch module is transferred to another device. Thus, this method\n            takes care of this inconsistency by updating the parameters inside the optimizer.\n\n        Args:\n            device (torch.torch.device): The device to which the network is sent.\n\n        Returns:\n            `self`.\n        """"""\n        self.device = device\n        with self._update_optim_device():\n            self.network.to(self.device)\n        self._transfer_loss_and_metrics_modules_to_right_device()\n        return self\n\n    def _transfer_loss_and_metrics_modules_to_right_device(self):\n        if isinstance(self.loss_function, torch.nn.Module):\n            self.loss_function.to(self.device)\n\n        for metric in self.batch_metrics + self.epoch_metrics:\n            if isinstance(metric, torch.nn.Module):\n                metric.to(self.device)\n\n        return self\n'"
poutyne/framework/optimizers.py,1,"b""import torch.optim as optim\n\nall_optimizers_dict = dict(\n    adadelta=optim.Adadelta,\n    adagrad=optim.Adagrad,\n    adam=optim.Adam,\n    sparseadam=optim.SparseAdam,\n    adamax=optim.Adamax,\n    asgd=optim.ASGD,\n    lbfgs=optim.LBFGS,\n    rmsprop=optim.RMSprop,\n    rprop=optim.Rprop,\n    sgd=optim.SGD,\n)\n\n\ndef get_optimizer(optimizer, module):\n    if isinstance(optimizer, str):\n        optimizer = optimizer.lower()\n        params = (p for p in module.parameters() if p.requires_grad)\n        if optimizer != 'sgd':\n            return all_optimizers_dict[optimizer](params)\n\n        return all_optimizers_dict[optimizer](params, lr=1e-2)\n\n    return optimizer\n"""
poutyne/framework/warning_manager.py,0,"b""warning_settings = {'batch_size': 'warn', 'concatenate_returns': 'warn'}\n"""
poutyne/layers/__init__.py,0,b'# pylint: disable=wildcard-import\nfrom .utils import *\n'
poutyne/layers/utils.py,2,"b'import torch.nn as nn\n\n\nclass Lambda(nn.Module):\n    """"""\n    Applies a function to the input tensor.\n\n    Args:\n        func (Callable[[~torch.Tensor], ~torch.Tensor]): The function to apply.\n\n    Example:\n\n        .. code-block:: python\n\n            # Alternate version to the ``nn.Flatten`` module.\n            my_flatten = Lambda(lambda x: x.flatten(1))\n\n    """"""\n\n    def __init__(self, func):\n        super(Lambda, self).__init__()\n        self.func = func\n\n    def forward(self, x):\n        return self.func(x)\n'"
tests/framework/__init__.py,0,b''
poutyne/framework/callbacks/__init__.py,0,b'# pylint: disable=wildcard-import\nfrom .best_model_restore import *\nfrom .callbacks import *\nfrom .checkpoint import *\nfrom .clip_grad import *\nfrom .delay import *\nfrom .earlystopping import *\nfrom .gradient_logger import *\nfrom .logger import *\nfrom .lr_scheduler import *\nfrom .periodic import *\nfrom .policies import *\nfrom .progress import *\nfrom .terminate_on_nan import *\nfrom .tracker import *\n'
poutyne/framework/callbacks/_utils.py,0,"b'import os\nimport tempfile\nimport warnings\nfrom typing import Callable\n\n\ndef atomic_lambda_save(filename: str,\n                       save_lambda: Callable,\n                       args,\n                       *,\n                       temporary_filename: str = None,\n                       open_mode: str = \'w\',\n                       atomic: bool = True):\n    if atomic:\n        fd = None\n        if temporary_filename is not None:\n            fd = open(temporary_filename, open_mode)\n            tmp_filename = temporary_filename\n        else:\n            fd = tempfile.NamedTemporaryFile(mode=open_mode, delete=False)\n            tmp_filename = fd.name\n\n        with fd:\n            save_lambda(fd, *args)\n\n        try:\n            os.replace(tmp_filename, filename)\n        except OSError as e:\n            # This may happen if the temp filesystem is not the same as the final destination\'s.\n            warnings.warn(""Impossible to move the file to its final destination: ""\n                          ""os.replace(%s, %s) -> %s"" % (tmp_filename, filename, e))\n            os.remove(tmp_filename)\n\n            warnings.warn(\'Saving %s non-atomically instead.\' % filename)\n            with open(filename, open_mode) as fd:\n                save_lambda(fd, *args)\n    else:\n        with open(filename, open_mode) as fd:\n            save_lambda(fd, *args)\n'"
poutyne/framework/callbacks/best_model_restore.py,0,"b'import warnings\nfrom typing import Dict\n\nfrom .callbacks import Callback\n\n\nclass BestModelRestore(Callback):\n    """"""\n    Restore the weights of the best model at the end of the training depending on a monitored quantity.\n\n    Args:\n        monitor (str): Quantity to monitor. (Default value = \'val_loss\')\n        mode (str): One of {\'min\', \'max\'}.\n            Whether the monitored has to be maximized or minimized. For instance, for `val_accuracy`,\n            this should be `max`, and for `val_loss`, this should be `min`, etc.\n            (Default value = \'min\')\n        verbose (bool): Whether to display a message when the model has improved or when restoring\n            the best model.\n            (Default value = False)\n    """"""\n\n    def __init__(self, *, monitor: str = \'val_loss\', mode: str = \'min\', verbose: bool = False):\n        super().__init__()\n        self.monitor = monitor\n\n        if mode not in [\'min\', \'max\']:\n            raise ValueError(""Invalid mode \'%s\'"" % mode)\n        if mode == \'min\':\n            self.monitor_op = lambda x, y: x < y\n            self.current_best = float(\'Inf\')\n        elif mode == \'max\':\n            self.monitor_op = lambda x, y: x > y\n            self.current_best = -float(\'Inf\')\n        self.best_weights = None\n        self.verbose = verbose\n\n    def on_epoch_end(self, epoch_number: int, logs: Dict):\n        if self.monitor_op(logs[self.monitor], self.current_best):\n            old_best = self.current_best\n            self.current_best = logs[self.monitor]\n\n            if self.verbose:\n                print(\'Epoch %d: %s improved from %0.5f to %0.5f\' %\n                      (epoch_number, self.monitor, old_best, self.current_best))\n            self.best_weights = self.model.get_weight_copies()\n\n    def on_train_end(self, logs: Dict):\n        if self.best_weights is not None:\n            if self.verbose:\n                print(\'Restoring best model\')\n            self.model.set_weights(self.best_weights)\n        else:\n            warnings.warn(\'No  weights to restore!\')\n'"
poutyne/framework/callbacks/callbacks.py,0,"b'""""""\nThe source code of this file was copied from the Keras project, and has been modified.\n\nCOPYRIGHT\n\nAll contributions by Fran\xc3\xa7ois Chollet:\nCopyright (c) 2015, Fran\xc3\xa7ois Chollet.\nAll rights reserved.\n\nAll contributions by Google:\nCopyright (c) 2015, Google, Inc.\nAll rights reserved.\n\nAll contributions by Microsoft:\nCopyright (c) 2017, Microsoft, Inc.\nAll rights reserved.\n\nAll other contributions:\nCopyright (c) 2015 - 2017, the respective contributors.\nAll rights reserved.\n\nEach contributor holds copyright over their respective contributions. The project versioning (Git)\nrecords all such contribution source information.\n\nLICENSE\n\nThe MIT License (MIT)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and\nassociated documentation files (the ""Software""), to deal in the Software without restriction,\nincluding without limitation the rights to use, copy, modify, merge, publish, distribute,\nsublicense, and/or sell copies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial\nportions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT\nNOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES\nOR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n""""""\nimport warnings\nfrom typing import Dict, List\n\n\nclass Callback:\n    """"""\n    Attributes:\n        params (dict): Contains \'epoch\' and \'steps_per_epoch\' keys which are passed to the\n            :func:`Model.fit() <poutyne.framework.Model.fit>` function. It may contain other keys.\n        model (Model): A reference to the :class:`~poutyne.framework.Model` object which is using the callback.\n    """"""\n\n    def __init__(self):\n        self.model = None\n        self.params = None\n\n    def set_params(self, params: Dict):\n        self.params = params\n\n    def set_model(self, model):\n        self.model = model\n\n    def on_epoch_begin(self, epoch_number: int, logs: Dict):\n        """"""\n        Is called before the beginning of each epoch.\n\n        Args:\n            epoch_number (int): The epoch number.\n            logs (dict): Usually an empty dict.\n        """"""\n        pass\n\n    def on_epoch_end(self, epoch_number: int, logs: Dict):\n        """"""\n        Is called before the end of each epoch.\n\n        Args:\n            epoch_number (int): The epoch number.\n            logs (dict): Contains the following keys:\n\n                 * \'epoch\': The epoch number.\n                 * \'loss\': The average loss of the batches.\n                 * \'time\': The computation time of the epoch.\n                 * Other metrics: One key for each type of metrics. The metrics are also averaged.\n                 * val_loss\': The average loss of the batches on the validation set.\n                 * Other metrics: One key for each type of metrics on the validation set. The metrics are also averaged.\n\n        Example::\n\n            logs = {\'epoch\': 6, \'time\': 3.141519837, \'loss\': 4.34462, \'accuracy\': 0.766,\n                    \'val_loss\': 5.2352, \'val_accuracy\': 0.682}\n        """"""\n        pass\n\n    def on_train_batch_begin(self, batch_number: int, logs: Dict):\n        """"""\n        Is called before the beginning of the training batch.\n\n        Args:\n            batch_number (int): The batch number.\n            logs (dict): Usually an empty dict.\n        """"""\n        pass\n\n    def on_train_batch_end(self, batch_number: int, logs: Dict):\n        """"""\n        Is called before the end of the training batch.\n\n        Args:\n            batch_number (int): The batch number.\n            logs (dict): Usually an empty dict.\n        """"""\n        pass\n\n    def on_test_batch_begin(self, batch_number: int, logs: Dict):\n        """"""\n        Is called before the beginning of the testing batch.\n\n        Args:\n            batch_number (int): The batch number.\n            logs (dict): Usually an empty dict.\n        """"""\n        pass\n\n    def on_test_batch_end(self, batch_number: int, logs: Dict):\n        """"""\n        Is called before the end of the testing batch.\n\n        Args:\n            batch_number (int): The batch number.\n            logs (dict): Usually an empty dict.\n        """"""\n        pass\n\n    def on_train_begin(self, logs: Dict):\n        """"""\n        Is called before the beginning of the training.\n\n        Args:\n            logs (dict): Usually an empty dict.\n        """"""\n        pass\n\n    def on_train_end(self, logs: Dict):\n        """"""\n        Is called before the end of the training.\n\n        Args:\n            logs (dict): Usually an empty dict.\n        """"""\n        pass\n\n    def on_test_begin(self, logs: Dict):\n        """"""\n        Is called before the beginning of the testing.\n\n        Args:\n            logs (dict): Usually an empty dict.\n        """"""\n        pass\n\n    def on_test_end(self, logs: Dict):\n        """"""\n        Is called before the end of the testing.\n\n        Args:\n            logs (dict): Usually an empty dict.\n        """"""\n        pass\n\n    def on_backward_end(self, batch_number: int):\n        """"""\n        Is called after the backpropagation but before the optimization step.\n\n        Args:\n            batch_number (int): The batch number.\n        """"""\n        pass\n\n\nclass CallbackList:\n    def __init__(self, callbacks: List[Callback]):\n        callbacks = callbacks or []\n        self.callbacks = list(callbacks)\n\n    def append(self, callback: Callback):\n        self.callbacks.append(callback)\n\n    def set_params(self, params: Dict):\n        for callback in self.callbacks:\n            callback.set_params(params)\n\n    def set_model(self, model):\n        for callback in self.callbacks:\n            callback.set_model(model)\n\n    def on_epoch_begin(self, epoch_number: int, logs: Dict):\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_epoch_begin(epoch_number, logs)\n\n    def on_epoch_end(self, epoch_number: int, logs: Dict):\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_epoch_end(epoch_number, logs)\n\n    def on_train_batch_begin(self, batch_number: int, logs: Dict):\n        logs = logs or {}\n        for callback in self.callbacks:\n            if hasattr(callback, \'on_batch_begin\'):\n                warnings.warn(\n                    \'on_batch_begin method for callback has been deprecated as of version 0.7. \'\n                    \'Use on_batch_train_begin instead.\',\n                    Warning,\n                    stacklevel=2)\n                callback.on_batch_begin(batch_number, logs)\n            else:\n                callback.on_train_batch_begin(batch_number, logs)\n\n    def on_train_batch_end(self, batch_number: int, logs: Dict):\n        logs = logs or {}\n        for callback in self.callbacks:\n            if hasattr(callback, \'on_batch_end\'):\n                warnings.warn(\n                    \'on_batch_end method for callback has been deprecated as of version 0.7. \'\n                    \'Use on_batch_train_end instead.\',\n                    Warning,\n                    stacklevel=2)\n                callback.on_batch_end(batch_number, logs)\n            else:\n                callback.on_train_batch_end(batch_number, logs)\n\n    def on_test_batch_begin(self, batch_number: int, logs: Dict):\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_test_batch_begin(batch_number, logs)\n\n    def on_test_batch_end(self, batch_number: int, logs: Dict):\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_test_batch_end(batch_number, logs)\n\n    def on_train_begin(self, logs: Dict):\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_train_begin(logs)\n\n    def on_train_end(self, logs: Dict):\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_train_end(logs)\n\n    def on_test_begin(self, logs: Dict):\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_test_begin(logs)\n\n    def on_test_end(self, logs: Dict):\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_test_end(logs)\n\n    def on_backward_end(self, batch_number: int):\n        for callback in self.callbacks:\n            callback.on_backward_end(batch_number)\n\n    def __iter__(self):\n        return iter(self.callbacks)\n'"
poutyne/framework/callbacks/checkpoint.py,0,"b'import warnings\nfrom typing import Dict, BinaryIO\n\nfrom .lr_scheduler import _PyTorchLRSchedulerWrapper, ReduceLROnPlateau\nfrom .periodic import PeriodicSaveCallback\n\n\nclass ModelCheckpoint(PeriodicSaveCallback):\n    """"""\n    Save the model after every epoch. See\n    :class:`~poutyne.framework.callbacks.PeriodicSaveCallback` for the arguments\' descriptions.\n\n    Args:\n        restore_best (bool): If `restore_best` is true, the weights of the network will be reset to\n            the last best checkpoint done. This option only works when `save_best_only` is also true.\n            (Default value = False)\n\n    See:\n        :class:`~poutyne.framework.callbacks.PeriodicSaveCallback`\n    """"""\n\n    def __init__(self, *args, restore_best: bool = False, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.restore_best = restore_best\n        if self.restore_best and not self.save_best_only:\n            raise ValueError(""The \'restore_best\' argument only works when \'save_best_only\' is also true."")\n\n    def save_file(self, fd: BinaryIO, epoch_number: int, logs: Dict):\n        self.model.save_weights(fd)\n\n    def on_train_end(self, logs: Dict):\n        if self.restore_best:\n            if self.best_filename is not None:\n                if self.verbose:\n                    print(\'Restoring model from %s\' % self.best_filename)\n                self.model.load_weights(self.best_filename)\n            else:\n                warnings.warn(\'No  weights to restore!\')\n\n\nclass OptimizerCheckpoint(PeriodicSaveCallback):\n    """"""\n    Save the state of the optimizer after every epoch. The optimizer can be reloaded as follows.\n\n    .. code-block:: python\n\n        model = Model(model, optimizer, loss_function)\n        model.load_optimizer_state(filename)\n\n    See :class:`~poutyne.framework.callbacks.PeriodicSaveCallback` for the arguments\' descriptions.\n\n    See:\n        :class:`~poutyne.framework.callbacks.PeriodicSaveCallback`\n    """"""\n\n    def save_file(self, fd: BinaryIO, epoch_number: int, logs: Dict):\n        self.model.save_optimizer_state(fd)\n\n\nclass LRSchedulerCheckpoint(PeriodicSaveCallback):\n    """"""\n    Save the state of an LR scheduler callback after every epoch. The LR scheduler callback should\n    not be passed to the fit*() methods since it is called by this callback instead. The LR\n    scheduler can be reloaded as follows.\n\n    .. code-block:: python\n\n        lr_scheduler = AnLRSchedulerCallback(...)\n        lr_scheduler.load_state(filename)\n\n    See :class:`~poutyne.framework.callbacks.PeriodicSaveCallback` for the arguments\' descriptions.\n\n    Args:\n        lr_scheduler: An LR scheduler callback.\n\n    See:\n        :class:`~poutyne.framework.callbacks.PeriodicSaveCallback`\n    """"""\n\n    def __init__(self, lr_scheduler: _PyTorchLRSchedulerWrapper, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.lr_scheduler = lr_scheduler\n\n        if not isinstance(self.lr_scheduler, (_PyTorchLRSchedulerWrapper, ReduceLROnPlateau)):\n            raise ValueError(""Unknown scheduler callback \'%s\'."" % lr_scheduler)\n\n    def save_file(self, fd: BinaryIO, epoch_number: int, logs: Dict):\n        self.lr_scheduler.save_state(fd)\n\n    def set_params(self, params: Dict):\n        self.lr_scheduler.set_params(params)\n        super().set_params(params)\n\n    def set_model(self, model):\n        self.lr_scheduler.set_model(model)\n        super().set_model(model)\n\n    def on_epoch_begin(self, epoch_number: int, logs: Dict):\n        self.lr_scheduler.on_epoch_begin(epoch_number, logs)\n        super().on_epoch_begin(epoch_number, logs)\n\n    def on_epoch_end(self, epoch_number: int, logs: Dict):\n        self.lr_scheduler.on_epoch_end(epoch_number, logs)\n        super().on_epoch_end(epoch_number, logs)\n\n    def on_train_batch_begin(self, batch_number: int, logs: Dict):\n        self.lr_scheduler.on_train_batch_begin(batch_number, logs)\n        super().on_train_batch_begin(batch_number, logs)\n\n    def on_train_batch_end(self, batch_number: int, logs: Dict):\n        self.lr_scheduler.on_train_batch_end(batch_number, logs)\n        super().on_train_batch_end(batch_number, logs)\n\n    def on_backward_end(self, batch_number: int):\n        self.lr_scheduler.on_backward_end(batch_number)\n        super().on_backward_end(batch_number)\n\n    def on_train_begin(self, logs: Dict):\n        self.lr_scheduler.on_train_begin(logs)\n        super().on_train_begin(logs)\n\n    def on_train_end(self, logs: Dict):\n        self.lr_scheduler.on_train_end(logs)\n        super().on_train_end(logs)\n'"
poutyne/framework/callbacks/clip_grad.py,5,"b'from typing import Dict, Union\n\nfrom torch.nn.utils import clip_grad_norm_, clip_grad_value_\n\nfrom .callbacks import Callback\n\n\nclass ClipNorm(Callback):\n    """"""\n    Uses PyTorch\'s :func:`~torch.nn.utils.clip_grad_norm_()`\n    method to clip gradient.\n\n    See:\n        :func:`torch.nn.utils.clip_grad_norm_()`\n\n    """"""\n\n    def __init__(self, parameters: Dict, max_norm: Union[float, int], *, norm_type: Union[float, int] = 2):\n        super().__init__()\n        self.parameters = list(parameters)\n        self.max_norm = max_norm\n        self.norm_type = norm_type\n\n    def on_backward_end(self, batch_number: int):\n        clip_grad_norm_(self.parameters, self.max_norm, norm_type=self.norm_type)\n\n\nclass ClipValue(Callback):\n    """"""\n    Uses PyTorch\'s :func:`~torch.nn.utils.clip_grad_value_()`\n    method to clip gradient.\n\n    See:\n        :func:`torch.nn.utils.clip_grad_value_()`\n\n    """"""\n\n    def __init__(self, parameters: Dict, clip_value: Union[float, int]):\n        super().__init__()\n        self.parameters = list(parameters)\n        self.clip_value = clip_value\n\n    def on_backward_end(self, batch_number: int):\n        clip_grad_value_(self.parameters, self.clip_value)\n'"
poutyne/framework/callbacks/delay.py,0,"b'from typing import Dict, Optional\n\nfrom .callbacks import Callback, CallbackList\n\n\nclass DelayCallback(Callback):\n    """"""\n    Delays one or many callbacks for a certain number of epochs or number of batches. If both\n    ``epoch_delay`` and ``batch_delay`` are provided, the biggest has precedence.\n\n    Args:\n        callbacks (Callback, List[Callback]): A callback or a list of callbacks to delay.\n        epoch_delay (int, optional): Number of epochs to delay.\n        batch_delay (int, optional): Number of batches to delay. The number of batches can span many\n            epochs. When the batch delay expires (i.e. there are more than `batch_delay` done), the\n            :func:`~poutyne.framework.callbacks.Callback.on_epoch_begin()` method is called on\n            the callback(s) before the :func:`~poutyne.framework.callbacks.Callback.on_train_batch_begin()` method.\n    """"""\n\n    def __init__(self, callbacks: Callback, *, epoch_delay: Optional[int] = None, batch_delay: Optional[int] = None):\n        super().__init__()\n        if isinstance(callbacks, CallbackList):\n            self.callbacks = callbacks\n        elif isinstance(callbacks, list):\n            self.callbacks = CallbackList(callbacks)\n        else:\n            self.callbacks = CallbackList([callbacks])\n\n        self.epoch_delay = epoch_delay if epoch_delay else 0\n        self.batch_delay = batch_delay if batch_delay else 0\n\n    def set_params(self, params: Dict):\n        self.callbacks.set_params(params)\n\n    def set_model(self, model):\n        self.callbacks.set_model(model)\n\n    def on_epoch_begin(self, epoch_number: int, logs: Dict):\n        self.current_epoch = epoch_number\n        if self.has_delay_passed():\n            self.has_on_epoch_begin_been_called = True\n            self.callbacks.on_epoch_begin(epoch_number, logs)\n\n    def on_epoch_end(self, epoch_number: int, logs: Dict):\n        if self.has_delay_passed():\n            self.callbacks.on_epoch_end(epoch_number, logs)\n\n    def on_train_batch_begin(self, batch_number: int, logs: Dict):\n        self.batch_counter += 1\n        if self.has_delay_passed():\n            if not self.has_on_epoch_begin_been_called:\n                self.has_on_epoch_begin_been_called = True\n                self.callbacks.on_epoch_begin(self.current_epoch, logs)\n            self.callbacks.on_train_batch_begin(batch_number, logs)\n\n    def on_train_batch_end(self, batch_number: int, logs: Dict):\n        if self.has_delay_passed():\n            self.callbacks.on_train_batch_end(batch_number, logs)\n\n    def on_backward_end(self, batch_number: int):\n        if self.has_delay_passed():\n            self.callbacks.on_backward_end(batch_number)\n\n    def on_train_begin(self, logs: Dict):\n        self.current_epoch = 0\n        self.batch_counter = 0\n        self.has_on_epoch_begin_been_called = False\n        self.callbacks.on_train_begin(logs)\n\n    def on_train_end(self, logs: Dict):\n        self.callbacks.on_train_end(logs)\n\n    def has_delay_passed(self):\n        return self.current_epoch > self.epoch_delay and \\\n               self.batch_counter > self.batch_delay\n'"
poutyne/framework/callbacks/earlystopping.py,0,"b'""""""\nThe source code of this file was copied from the Keras project, and has been modified.\n\nCOPYRIGHT\n\nAll contributions by Fran\xc3\xa7ois Chollet:\nCopyright (c) 2015, Fran\xc3\xa7ois Chollet.\nAll rights reserved.\n\nAll contributions by Google:\nCopyright (c) 2015, Google, Inc.\nAll rights reserved.\n\nAll contributions by Microsoft:\nCopyright (c) 2017, Microsoft, Inc.\nAll rights reserved.\n\nAll other contributions:\nCopyright (c) 2015 - 2017, the respective contributors.\nAll rights reserved.\n\nEach contributor holds copyright over their respective contributions. The project versioning (Git)\nrecords all such contribution source information.\n\nLICENSE\n\nThe MIT License (MIT)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and\nassociated documentation files (the ""Software""), to deal in the Software without restriction,\nincluding without limitation the rights to use, copy, modify, merge, publish, distribute,\nsublicense, and/or sell copies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial\nportions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT\nNOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES\nOR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n""""""\nfrom typing import Dict\n\nimport numpy as np\n\nfrom .callbacks import Callback\n\n\nclass EarlyStopping(Callback):\n    """"""\n    The source code of this class is under the MIT License and was copied from the Keras project,\n    and has been modified.\n\n    Stop training when a monitored quantity has stopped improving.\n\n    Args:\n        monitor (str): Quantity to be monitored.\n        min_delta (float): Minimum change in the monitored quantity to qualify as an improvement,\n            i.e. an absolute change of less than min_delta, will count as no improvement.\n            (Default value = 0)\n        patience (int): Number of epochs with no improvement after which training will be stopped.\n            (Default value = 0)\n        verbose (bool): Whether to print when early stopping is done.\n            (Default value = False)\n        mode (str): One of {\'min\', \'max\'}. In `min` mode, training will stop when the quantity\n            monitored has stopped decreasing; in `max` mode it will stop when the quantity monitored has\n            stopped increasing.\n            (Default value = \'min\')\n    """"""\n\n    def __init__(self,\n                 *,\n                 monitor: str = \'val_loss\',\n                 min_delta: float = 0.,\n                 patience: int = 0,\n                 verbose: bool = False,\n                 mode: str = \'min\'):\n        super(EarlyStopping, self).__init__()\n\n        self.monitor = monitor\n        self.patience = patience\n        self.verbose = verbose\n        self.min_delta = min_delta\n        self.wait = 0\n        self.stopped_epoch = 0\n\n        if mode not in [\'min\', \'max\']:\n            raise ValueError(""Invalid mode \'%s\'"" % mode)\n        self.mode = mode\n\n        if mode == \'min\':\n            self.min_delta *= -1\n            self.monitor_op = np.less\n        elif mode == \'max\':\n            self.min_delta *= 1\n            self.monitor_op = np.greater\n\n    def on_train_begin(self, logs: Dict):\n        # Allow instances to be re-used\n        self.wait = 0\n        self.stopped_epoch = 0\n        self.best = np.Inf if self.mode == \'min\' else -np.Inf\n\n    def on_epoch_end(self, epoch_number: int, logs: Dict):\n        current = logs[self.monitor]\n        if self.monitor_op(current - self.min_delta, self.best):\n            self.best = current\n            self.wait = 0\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.stopped_epoch = epoch_number\n                self.model.stop_training = True\n\n    def on_train_end(self, logs: Dict):\n        if self.stopped_epoch > 0 and self.verbose:\n            print(\'Epoch %05d: early stopping\' % (self.stopped_epoch + 1))\n'"
poutyne/framework/callbacks/gradient_logger.py,2,"b'import os\nfrom typing import Dict, List, Union, Optional, TextIO\nimport csv\n\nimport torch\n\nfrom ._utils import atomic_lambda_save\nfrom .callbacks import Callback\n\n\nclass GradientLoggerBase(Callback):\n    def __init__(self, keep_bias: bool = False, norm_type: Union[float, List[float]] = 2.) -> None:\n        super().__init__()\n        self.keep_bias = keep_bias\n        self.norm_type = [norm_type] if isinstance(norm_type, (float, int)) else norm_type\n\n        self.stats_names = [\'mean\', \'var\', \'min\', \'abs_min\', \'max\', \'abs_max\']\n        self.stats_names += [f\'l{norm}\' for norm in self.norm_type]\n\n        self.layers = []\n\n    def on_train_begin(self, logs: Dict):\n        self.layers = [n for n, p in self.model.network.named_parameters() if self._keep_layer(p, n)]\n\n    def on_epoch_begin(self, epoch: int, logs: Dict):\n        self.epoch = epoch\n\n    def on_train_batch_end(self, batch: int, logs: Dict):\n        # Just in case we want to support second-order derivatives\n        with torch.no_grad():\n            layer_stats = {}\n            for name, param in self.model.network.named_parameters():\n                if self._keep_layer(param, name):\n                    grad = param.grad\n                    grad_abs_values = grad.abs()\n\n                    stats = {}\n                    stats[\'mean\'] = grad_abs_values.mean().item()\n                    stats[\'var\'] = grad_abs_values.var().item()\n                    stats[\'min\'] = grad.min().item()\n                    stats[\'abs_min\'] = grad_abs_values.min().item()\n                    stats[\'max\'] = grad.max().item()\n                    stats[\'abs_max\'] = grad_abs_values.max().item()\n                    for norm in self.norm_type:\n                        stats[f\'l{norm}\'] = grad_abs_values.norm(norm).item()\n\n                    layer_stats[name] = stats\n        self.log_stats(self.epoch, batch, logs, layer_stats)\n\n    def log_stats(self, epoch: int, batch: int, logs: Dict, layer_stats: Dict[str, Dict[str, float]]) -> None:\n        raise NotImplementedError\n\n    def _keep_layer(self, param: torch.nn.parameter.Parameter, name: str):\n        if self.keep_bias:\n            return param.requires_grad\n        return param.requires_grad and (""bias"" not in name)\n\n\nclass MemoryGradientLogger(GradientLoggerBase):\n    def __init__(self, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.history = []\n\n    def on_train_begin(self, logs: Dict):\n        super().on_train_begin(logs)\n        self.history = {layer: [] for layer in self.layers}\n\n    def log_stats(self, epoch: int, batch: int, logs: Dict, layer_stats: Dict[str, Dict[str, float]]) -> None:\n        for layer, stats in layer_stats.items():\n            stats[\'epoch\'] = epoch\n            stats[\'batch\'] = batch\n            self.history[layer].append(stats)\n\n\nclass TensorBoardGradientLogger(GradientLoggerBase):\n    def __init__(self, writer, initial_step: int = 0, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.writer = writer\n        self.current_step = initial_step\n\n    def log_stats(self, epoch: int, batch: int, logs: Dict, layer_stats: Dict[str, Dict[str, float]]) -> None:\n        self.current_step += 1\n        for layer, stats in layer_stats.items():\n            for name, value in stats.items():\n                self.writer.add_scalars(\'gradient_stats/{}\'.format(layer), {name: value}, self.current_step)\n\n\nclass AtomicCSVGradientLogger(GradientLoggerBase):\n    def __init__(self,\n                 filename,\n                 *,\n                 separator: str = \',\',\n                 append: bool = False,\n                 temporary_filename: Optional[str] = None,\n                 **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.filename = filename\n        self.temporary_filename = temporary_filename\n        self.separator = separator\n        self.append = append\n        self.fieldnames = []\n\n    def _save_stats(self, fd: TextIO, filename: str, stats: Dict):\n        olddata = None\n        if os.path.exists(filename):\n            with open(filename, \'r\') as oldfile:\n                olddata = list(csv.DictReader(oldfile, delimiter=self.separator))\n        csvwriter = csv.DictWriter(fd, fieldnames=self.fieldnames, delimiter=self.separator)\n        csvwriter.writeheader()\n        if olddata is not None:\n            csvwriter.writerows(olddata)\n        csvwriter.writerow(stats)\n\n    def _write_header(self, fd: TextIO):\n        csvwriter = csv.DictWriter(fd, fieldnames=self.fieldnames, delimiter=self.separator)\n        csvwriter.writeheader()\n\n    def on_train_begin(self, logs: Dict):\n        super().on_train_begin(logs)\n        self.fieldnames = [\'epoch\', \'batch\'] + self.stats_names\n\n        if not self.append:\n            for layer in self.layers:\n                filename = self.filename.format(layer)\n                atomic_lambda_save(filename, self._write_header, (), temporary_filename=self.temporary_filename)\n\n    def log_stats(self, epoch: int, batch: int, logs: Dict, layer_stats: Dict[str, Dict[str, float]]) -> None:\n        for layer, stats in layer_stats.items():\n            filename = self.filename.format(layer)\n            stats[\'epoch\'] = epoch\n            stats[\'batch\'] = batch\n            atomic_lambda_save(filename,\n                               self._save_stats, (filename, stats),\n                               temporary_filename=self.temporary_filename)\n\n\nclass CSVGradientLogger(GradientLoggerBase):\n    def __init__(self, filename, *, separator: str = \',\', append: bool = False, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.filename = filename\n        self.separator = separator\n        self.append = append\n\n    def on_train_begin(self, logs: Dict):\n        super().on_train_begin(logs)\n        fieldnames = [\'epoch\', \'batch\'] + self.stats_names\n        open_flag = \'a\' if self.append else \'w\'\n\n        self.csvfiles = {}\n        self.writers = {}\n        for layer in self.layers:\n            filename = self.filename.format(layer)\n            self.csvfiles[layer] = open(filename, open_flag, newline=\'\')\n            self.writers[layer] = csv.DictWriter(self.csvfiles[layer], fieldnames=fieldnames, delimiter=self.separator)\n            if not self.append:\n                self.writers[layer].writeheader()\n                self.csvfiles[layer].flush()\n\n    def log_stats(self, epoch: int, batch: int, logs: Dict, layer_stats: Dict[str, Dict[str, float]]) -> None:\n        for layer, stats in layer_stats.items():\n            stats[\'epoch\'] = epoch\n            stats[\'batch\'] = batch\n            self.writers[layer].writerow(stats)\n            self.csvfiles[layer].flush()\n\n    def on_train_end(self, logs: Dict):\n        super().on_train_end(logs)\n        for layer in self.layers:\n            self.csvfiles[layer].close()\n'"
poutyne/framework/callbacks/logger.py,2,"b'import csv\nimport os\nfrom typing import Dict, Optional, TextIO\n\nfrom ._utils import atomic_lambda_save\nfrom .callbacks import Callback\n\n\nclass Logger(Callback):\n    def __init__(self, *, batch_granularity: bool = False):\n        super().__init__()\n        self.batch_granularity = batch_granularity\n        self.epoch_number = 0\n\n    def on_train_begin(self, logs: Dict):\n        metrics = [\'loss\'] + self.model.metrics_names\n\n        if self.batch_granularity:\n            self.fieldnames = [\'epoch\', \'batch\', \'size\', \'time\', \'lr\']\n        else:\n            self.fieldnames = [\'epoch\', \'time\', \'lr\']\n        self.fieldnames += metrics\n        self.fieldnames += [\'val_\' + metric for metric in metrics]\n        self._on_train_begin_write(logs)\n\n    def _on_train_begin_write(self, logs: Dict):\n        pass\n\n    def on_train_batch_end(self, batch_number: int, logs: Dict):\n        if self.batch_granularity:\n            logs = self._get_logs_without_unknown_keys(logs)\n            self._on_train_batch_end_write(batch_number, logs)\n\n    def _on_train_batch_end_write(self, batch_number: int, logs: Dict):\n        pass\n\n    def on_epoch_begin(self, epoch_number: int, logs: Dict):\n        self.epoch_number = epoch_number\n        self._on_epoch_begin_write(self.epoch_number, logs)\n\n    def _on_epoch_begin_write(self, epoch_number: int, logs: Dict):\n        pass\n\n    def on_epoch_end(self, epoch_number: int, logs: Dict):\n        logs = self._get_logs_without_unknown_keys(logs)\n        self._on_epoch_end_write(epoch_number, logs)\n\n    def _on_epoch_end_write(self, epoch_number: int, logs: Dict):\n        pass\n\n    def on_train_end(self, logs: Dict):\n        self._on_train_end_write(logs)\n\n    def _on_train_end_write(self, logs: Dict):\n        pass\n\n    def _get_logs_without_unknown_keys(self, logs: Dict):\n        return {k: logs[k] for k in self.fieldnames if logs.get(k) is not None}\n\n    def _get_current_learning_rates(self):\n        learning_rates = [param_group[\'lr\'] for param_group in self.model.optimizer.param_groups]\n        return learning_rates[0] if len(learning_rates) == 1 else learning_rates\n\n\nclass CSVLogger(Logger):\n    """"""\n    Callback that outputs the result of each epoch_number or batch into a CSV file.\n\n    Args:\n        filename (str): The filename of the CSV.\n        batch_granularity (bool): Whether to also output the result of each batch in addition to the epochs.\n            (Default value = False)\n        separator (str): The separator to use in the CSV.\n            (Default value = \',\')\n        append (bool): Whether to append to an existing file.\n\n    """"""\n\n    def __init__(self, filename: str, *, batch_granularity: bool = False, separator: str = \',\', append: bool = False):\n        super().__init__(batch_granularity=batch_granularity)\n        self.filename = filename\n        self.separator = separator\n        self.append = append\n\n    def _on_train_begin_write(self, logs: Dict):\n        open_flag = \'a\' if self.append else \'w\'\n        self.csvfile = open(self.filename, open_flag, newline=\'\')\n        self.writer = csv.DictWriter(self.csvfile, fieldnames=self.fieldnames, delimiter=self.separator)\n        if not self.append:\n            self.writer.writeheader()\n            self.csvfile.flush()\n\n    def _on_train_batch_end_write(self, batch_number: int, logs: Dict):\n        self.writer.writerow(logs)\n        self.csvfile.flush()\n\n    def _on_epoch_end_write(self, epoch_number: int, logs: Dict):\n        self.writer.writerow(dict(logs, lr=self._get_current_learning_rates()))\n        self.csvfile.flush()\n\n    def _on_train_end_write(self, logs: Dict):\n        self.csvfile.close()\n\n\nclass AtomicCSVLogger(Logger):\n    """"""\n    Callback that outputs the result of each epoch_number or batch into a CSV file in an atomic matter.\n\n    Args:\n        filename (str): The filename of the CSV.\n        temporary_filename (str, optional): Temporary filename for the CSV file so that it can be written\n            atomically.\n        batch_granularity (bool): Whether to also output the result of each batch in addition to the epochs.\n            (Default value = False)\n        separator (str): The separator to use in the CSV.\n            (Default value = \',\')\n        append (bool): Whether to append to an existing file.\n    """"""\n\n    def __init__(self,\n                 filename,\n                 *,\n                 batch_granularity: bool = False,\n                 separator: str = \',\',\n                 append: bool = False,\n                 temporary_filename: Optional[str] = None):\n        super().__init__(batch_granularity=batch_granularity)\n        self.filename = filename\n        self.temporary_filename = temporary_filename\n        self.separator = separator\n        self.append = append\n\n    def _save_log(self, fd: TextIO, logs: Dict):\n        olddata = None\n        if os.path.exists(self.filename):\n            with open(self.filename, \'r\') as oldfile:\n                olddata = list(csv.DictReader(oldfile, delimiter=self.separator))\n        csvwriter = csv.DictWriter(fd, fieldnames=self.fieldnames, delimiter=self.separator)\n        csvwriter.writeheader()\n        if olddata is not None:\n            csvwriter.writerows(olddata)\n        if logs is not None:\n            csvwriter.writerow(logs)\n\n    def _write_header(self, fd: TextIO):\n        csvwriter = csv.DictWriter(fd, fieldnames=self.fieldnames, delimiter=self.separator)\n        csvwriter.writeheader()\n\n    def _on_train_begin_write(self, logs: Dict):\n        if not self.append:\n            atomic_lambda_save(self.filename, self._write_header, (), temporary_filename=self.temporary_filename)\n\n    def _on_train_batch_end_write(self, batch_number: int, logs: Dict):\n        atomic_lambda_save(self.filename, self._save_log, (logs, ), temporary_filename=self.temporary_filename)\n\n    def _on_epoch_end_write(self, epoch_number: int, logs: Dict):\n        logs = dict(logs, lr=self._get_current_learning_rates())\n        atomic_lambda_save(self.filename, self._save_log, (logs, ), temporary_filename=self.temporary_filename)\n\n\nclass TensorBoardLogger(Logger):\n    """"""\n    Callback that outputs the result of each epoch_number or batch into a Tensorboard experiment folder.\n\n    Args:\n        writer (~torch.utils.tensorboard.writer.SummaryWriter): The tensorboard writer.\n\n    Example:\n        Using TensorBoardLogger::\n\n            from torch.utils.tensorboard import SummaryWriter\n            from poutyne.framework import Model\n            from poutyne.framework.callbacks import TensorBoardLogger\n\n            writer = SummaryWriter(\'runs\')\n            tb_logger = TensorBoardLogger(writer)\n\n            model = Model(...)\n            model.fit_generator(..., callbacks=[tb_logger])\n    """"""\n\n    def __init__(self, writer):\n        super().__init__(batch_granularity=False)\n        self.writer = writer\n\n    def _on_train_batch_end_write(self, batch_number: int, logs):\n        """"""\n        We don\'t handle tensorboard writing on batch granularity\n        """"""\n        pass\n\n    def _on_epoch_end_write(self, epoch_number: int, logs: dict):\n        grouped_items = dict()\n        for k, v in logs.items():\n            if \'val_\' in k:\n                primary_key = k[4:]\n                if primary_key not in grouped_items:\n                    grouped_items[primary_key] = dict()\n                grouped_items[k[4:]][k] = v\n            else:\n                if k not in grouped_items:\n                    grouped_items[k] = dict()\n                grouped_items[k][k] = v\n        for k, v in grouped_items.items():\n            self.writer.add_scalars(k, v, epoch_number)\n        lr = self._get_current_learning_rates()\n        if isinstance(lr, (list, )):\n            self.writer.add_scalars(\'lr\', {str(i): v for i, v in enumerate(lr)}, epoch_number)\n        else:\n            self.writer.add_scalars(\'lr\', {\'lr\': lr}, epoch_number)\n\n    def _on_train_end_write(self, logs: Dict):\n        self.writer.close()\n'"
poutyne/framework/callbacks/lr_scheduler.py,11,"b'""""""\nPoutyne\'s callbacks for learning rate schedulers are just wrappers around `PyTorch\'s learning rate\nschedulers <http://pytorch.org/docs/master/optim.html#how-to-adjust-learning-rate>`_ and thus have\nthe same arguments except for the optimizer that has to be omitted.\n""""""\nimport inspect\nimport sys\nfrom typing import Dict, BinaryIO\n\nimport torch.optim.lr_scheduler\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler\n\nfrom .callbacks import Callback\n\n\nclass _PyTorchLRSchedulerWrapper(Callback):\n    """"""\n    Default class for the LR scheduling callback. Proposes default comportment for the scheduler\n    loading and saving as well as for the epoch end handling.\n    """"""\n\n    def __init__(self, torch_lr_scheduler, *args, **kwargs):\n        super().__init__()\n        if len(args) > 0 and isinstance(args[0], Optimizer):\n            raise ValueError(""In the LR scheduler callbacks, the optimizer is ""\n                             ""automatically passed to the PyTorch\'s LR scheduler. ""\n                             ""You must remove it from the arguments."")\n        self.args = args\n        self.kwargs = kwargs\n        self.scheduler = None\n        self.state_to_load = None\n        self.torch_lr_scheduler = torch_lr_scheduler\n\n    def on_epoch_end(self, epoch_number: int, logs: Dict):\n        self.scheduler.step()\n\n    def on_train_begin(self, logs: Dict):\n        self.scheduler = self.torch_lr_scheduler(self.model.optimizer, *self.args, **self.kwargs)\n\n    def load_state(self, f: BinaryIO):\n        if self.scheduler is not None:\n            self.scheduler.load_state_dict(torch.load(f, map_location=\'cpu\'))\n        else:\n            self.state_to_load = torch.load(f, map_location=\'cpu\')\n\n    def save_state(self, f: BinaryIO):\n        torch.save(self.scheduler.state_dict(), f)\n\n    def _load_state_to_load(self):\n        if self.state_to_load is not None:\n            self.scheduler.load_state_dict(self.state_to_load)\n            self.state_to_load = None\n\n\ndef new_init(torch_lr_scheduler):\n    def f(self, *args, **kwargs):\n        super(type(self), self).__init__(torch_lr_scheduler, *args, **kwargs)\n\n    return f\n\n\nfor name, module_cls in torch.optim.lr_scheduler.__dict__.items():\n    if inspect.isclass(module_cls) and \\\n            issubclass(module_cls, _LRScheduler) and \\\n            module_cls != _LRScheduler:\n        _new_cls = type(\n            name, (_PyTorchLRSchedulerWrapper, ), {\n                \'__init__\':\n                new_init(module_cls),\n                \'__doc__\':\n                """"""\n                            See:\n                                :class:`~torch.optim.lr_scheduler.{name}`\n                            """""".format(name=name)\n            })\n        setattr(sys.modules[__name__], name, _new_cls)\n\n\nclass ReduceLROnPlateau(_PyTorchLRSchedulerWrapper):\n    """"""\n    Args:\n        monitor (str): The quantity to monitor. (Default value = \'val_loss\')\n    See:\n        :class:`~torch.optim.lr_scheduler.ReduceLROnPlateau`\n    """"""\n\n    def __init__(self, *args, monitor: str = \'val_loss\', **kwargs):\n        super().__init__(torch_lr_scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau, *args, **kwargs)\n        self.monitor = monitor\n\n    def on_epoch_end(self, epoch_number: int, logs: Dict):\n        self.scheduler.step(logs[self.monitor])\n'"
poutyne/framework/callbacks/periodic.py,0,"b'""""""\nThe source code of this file was copied from the Keras project, and has been modified.\n\nCOPYRIGHT\n\nAll contributions by Fran\xc3\xa7ois Chollet:\nCopyright (c) 2015, Fran\xc3\xa7ois Chollet.\nAll rights reserved.\n\nAll contributions by Google:\nCopyright (c) 2015, Google, Inc.\nAll rights reserved.\n\nAll contributions by Microsoft:\nCopyright (c) 2017, Microsoft, Inc.\nAll rights reserved.\n\nAll other contributions:\nCopyright (c) 2015 - 2017, the respective contributors.\nAll rights reserved.\n\nEach contributor holds copyright over their respective contributions. The project versioning (Git)\nrecords all such contribution source information.\n\nLICENSE\n\nThe MIT License (MIT)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and\nassociated documentation files (the ""Software""), to deal in the Software without restriction,\nincluding without limitation the rights to use, copy, modify, merge, publish, distribute,\nsublicense, and/or sell copies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial\nportions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT\nNOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES\nOR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n""""""\nfrom typing import BinaryIO, Dict, Optional, Callable\n\nfrom ._utils import atomic_lambda_save\nfrom .callbacks import Callback\n\n\nclass PeriodicSaveCallback(Callback):\n    """"""\n    The source code of this class is under the MIT License and was copied from the Keras project,\n    and has been modified.\n\n    Write a file after every epoch. `filename` can contain named formatting options, which will be\n    filled the value of `epoch` and keys in `logs` (passed in `on_epoch_end`). For example: if\n    `filename` is `weights.{epoch:02d}-{val_loss:.2f}.txt`, then `save_file()` will be called with a\n    file descriptor for a file with the epoch number and the validation loss in the filename.\n\n    By default, the file is written atomically to the specified filename so that the training can\n    be killed and restarted later using the same filename for periodic file saving. To do so, a\n    temporary file is created using the system\'s `tmp` directory and then is moved to the final\n    destination after the checkpoint is made. Sometimes, this move is not possible on some systems.\n    To address this problem, it is possible to specify the destination of the temporary file using\n    the ``temporary_filename`` argument.\n\n    Args:\n        filename (str): Path to save the model file.\n        monitor (str): Quantity to monitor.\n            (Default value = \'val_loss\')\n        verbose (bool): Whether to display a message when saving and restoring a checkpoint.\n            (Default value = False)\n        save_best_only (bool): If `save_best_only` is true, the latest best model according to the\n            quantity monitored will not be overwritten.\n            (Default value = False)\n        mode (str): One of {\'min\', \'max\'}.\n            If `save_best_only` is true, the decision to overwrite the current save file is made based\n            on either the maximization or the minimization of the monitored quantity. For\n            `val_accuracy`, this should be `max`, for `val_loss` this should be `min`, etc.\n            (Default value = \'min\')\n        period (int): Interval (number of epochs) between checkpoints.\n            (Default value = 1)\n        temporary_filename (str, optional): Temporary filename for the checkpoint so that the\n            last checkpoint can be written atomically. See the ``atomic_write`` argument.\n        atomic_write (bool): Whether to write atomically the checkpoint. See the description above\n            for details.\n            (Default value = True)\n        open_mode (str): ``mode`` option passed to :func:`open()`.\n            (Default value = \'wb\')\n    """"""\n\n    def __init__(self,\n                 filename: str,\n                 *,\n                 monitor: str = \'val_loss\',\n                 mode: str = \'min\',\n                 save_best_only: bool = False,\n                 period: int = 1,\n                 verbose: bool = False,\n                 temporary_filename: Optional[str] = None,\n                 atomic_write: bool = True,\n                 open_mode: str = \'wb\'):\n        super().__init__()\n        self.filename = filename\n        self.monitor = monitor\n        self.verbose = verbose\n        self.save_best_only = save_best_only\n        self.temporary_filename = temporary_filename\n        self.atomic_write = atomic_write\n        self.open_mode = open_mode\n        self.best_filename = None\n\n        if self.save_best_only:\n            if mode not in [\'min\', \'max\']:\n                raise ValueError(""Invalid mode \'%s\'"" % mode)\n            if mode == \'min\':\n                self.monitor_op = lambda x, y: x < y\n                self.current_best = float(\'Inf\')\n            elif mode == \'max\':\n                self.monitor_op = lambda x, y: x > y\n                self.current_best = -float(\'Inf\')\n\n        self.period = period\n\n    def save_file(self, fd: BinaryIO, epoch_number: int, logs: Dict):\n        raise NotImplementedError\n\n    def _save_file(self, filename: str, epoch_number: int, logs: Dict):\n        atomic_lambda_save(filename,\n                           self.save_file, (epoch_number, logs),\n                           temporary_filename=self.temporary_filename,\n                           open_mode=self.open_mode,\n                           atomic=self.atomic_write)\n\n    def on_epoch_end(self, epoch_number: int, logs: Dict):\n        filename = self.filename.format_map(logs)\n\n        if self.save_best_only:\n            if self.monitor_op(logs[self.monitor], self.current_best):\n                old_best = self.current_best\n                self.current_best = logs[self.monitor]\n                self.best_filename = filename\n\n                if self.verbose:\n                    print(\'Epoch %d: %s improved from %0.5f to %0.5f, saving file to %s\' %\n                          (epoch_number, self.monitor, old_best, self.current_best, self.best_filename))\n                self._save_file(self.best_filename, epoch_number, logs)\n        elif epoch_number % self.period == 0:\n            if self.verbose:\n                print(\'Epoch %d: saving file to %s\' % (epoch_number, filename))\n            self._save_file(filename, epoch_number, logs)\n\n\nclass PeriodicSaveLambda(PeriodicSaveCallback):\n    """"""\n    Call a lambda with a file descriptor after every epoch. See\n    :class:`~poutyne.framework.callbacks.PeriodicSaveCallback` for the arguments\' descriptions.\n\n    Args:\n        func (Callable[[fd, int, dict], None]): The lambda that will be called with a file descriptor, the\n            epoch number and the epoch logs.\n\n    See:\n        :class:`~poutyne.framework.callbacks.PeriodicSaveCallback`\n    """"""\n\n    def __init__(self, func: Callable, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.func = func\n\n    def save_file(self, fd: str, epoch_number: int, logs: Dict):\n        self.func(fd, epoch_number, logs)\n'"
poutyne/framework/callbacks/policies.py,0,"b'""""""\nThe ``policies`` module is an alternative way to configure your training process. It gives you fine\ngrained control over the process.\n\nThe training is divided into phases with the :class:`~poutyne.framework.callbacks.policies.Phase` class.\nA :class:`~poutyne.framework.callbacks.policies.Phase` contains parameter spaces (e.g. learning rate,\nor momentum, or both) for the optimizer. You chain :class:`~poutyne.framework.callbacks.policies.Phase`\ninstances by passing them to the :class:`~poutyne.framework.callbacks.policies.OptimizerPolicy`.\n:class:`~poutyne.framework.callbacks.policies.OptimizerPolicy` is a :class:`~poutyne.framework.callbacks.Callback`\nthat uses the phases, steps through them, and sets the parameters of the optimizer.\n""""""\n###############################################################################\nimport contextlib\nfrom collections import OrderedDict\nfrom itertools import islice, chain\nfrom math import cos, pi\nfrom typing import Dict, List, Tuple, Optional\n\nfrom .callbacks import Callback\n\n\n###############################################################################\n# Lazy parameter spaces\n# A space is just an iterable\nclass linspace:\n    """"""\n    A lazy linear parameter space that goes from ``start`` to ``end`` in ``steps`` steps.\n\n    Args:\n        start (int): the start point.\n        end (int): the end point.\n        steps (int): the number of steps between start and end.\n\n    Example:\n        >>> list(linspace(0, 1, 3))\n        [0.0, 0.5, 1.0]\n    """"""\n\n    def __init__(self, start: int, end: int, steps: int):\n        self.start = start\n        self.end = end\n        self.steps = steps\n\n    def _progress(self, i):\n        return i / (self.steps - 1)\n\n    def __iter__(self):\n        return (self.start + self._progress(i) * (self.end - self.start) for i in range(self.steps))\n\n\nclass cosinespace:\n    """"""\n    A lazy cosine parameter space that goes from ``start`` to ``end`` in ``steps`` steps.\n\n    Args:\n        start (int): the start point.\n        end (int): the end point.\n        steps (int): the number of steps between start and end.\n\n    Example:\n        >>> list(cosinespace(0, 1, 3))\n        [0.0, 0.5, 1.0]\n    """"""\n\n    def __init__(self, start: int, end: int, steps: int):\n        self.start = start\n        self.end = end\n        self.steps = steps\n\n    def _progress(self, i):\n        return i / (self.steps - 1)\n\n    def __iter__(self):\n        return (self.end + (self.start - self.end) * (1 + cos(self._progress(i) * pi)) / 2 for i in range(self.steps))\n\n\n###############################################################################\nclass Phase:\n    """"""\n    Defines how to configure an optimizer.\n\n    For each train step it returns a dictionary that contains the configuration for the optimizer.\n\n    Args:\n        lr (List[float], optional): a configuration space for the learning rate.\n        momentum (List[float], optional): a configuration space for the momentum.\n    """"""\n\n    def __init__(self, *, lr: Optional[float] = None, momentum: Optional[float] = None):\n        if lr is None and momentum is None:\n            raise ValueError(""You must specify lr and/or momentum."")\n\n        self.configuration = OrderedDict()\n        if lr is not None:\n            self.configuration[""lr""] = lr\n        if momentum is not None:\n            self.configuration[""momentum""] = momentum\n\n    def __iter__(self):\n        names = list(self.configuration.keys())\n        values = self.configuration.values()\n        for values in zip(*self.configuration.values()):\n            yield dict(zip(names, values))\n\n    def __repr__(self):\n        return ""\\n"".join([\n            ""Phase:"",\n            *[""    {}: {}"".format(name, val) for name, val in self.configuration.items()],\n        ])\n\n    def plot(self, param_name: str = ""lr"", ax=None):\n        """"""\n        Plot the phase for the given `param_name`.\n\n        Args:\n            param_name (str, optional): the name of the parameter to plot.\n            ax (~matplotlib.pyplot.axis, optional): a matplotlib axis to plot on, if given.\n\n        Returns:\n            The matplotlib axis.\n        """"""\n        # pylint: disable=import-error\n        import matplotlib.pyplot as plt\n\n        if ax is None:\n            _fig, ax = plt.subplots()\n\n        ax.plot(list(self.configuration[param_name]))\n        ax.set_xlabel(""steps"")\n        ax.set_ylabel(param_name)\n        return ax\n\n\n###############################################################################\n# complex policies build from simple phases\n# pylint\ndef one_cycle_phases(steps: int,\n                     lr: Tuple[float, float] = (0.1, 1),\n                     momentum: Tuple[float, float] = (0.95, 0.85),\n                     finetune_lr: float = .01,\n                     finetune_fraction: float = 0.1) -> List[Phase]:\n    """"""\n    The ""one-cycle"" policy as described in the paper `Super-Convergence: Very Fast Training of\n    Neural Networks Using Large Learning Rates <https://arxiv.org/abs/1708.07120>`_.\n\n    You might want to read the paper and adjust the parameters.\n\n    Args:\n        steps (int): the total number of steps to take.\n        lr (Tuple[float, float]): tuple for the triangular learning rate (start, middle).\n        momentum (Tuple[float, float]): tuple for the triangular momentum (start, middle).\n        finetune_lr (float): target learning rate for the final fine tuning. Should be smaller than\n            `min(lr)`.\n        finetune_fraction (float): fraction of steps used for the fine tuning.\n            Must be between 0 and 1.\n\n    Returns:\n        A list of configured :class:`~poutyne.framework.callbacks.policies.Phase` instances.\n\n    References:\n        `Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates\n        <https://arxiv.org/abs/1708.07120>`_\n    """"""\n    steps_annealing = int(steps * finetune_fraction)\n    steps_up = (steps - steps_annealing) // 2\n    steps_down = steps - steps_annealing - steps_up\n    return [\n        Phase(\n            lr=linspace(lr[0], lr[1], steps_up),\n            momentum=linspace(momentum[0], momentum[1], steps_up),\n        ),\n        Phase(\n            lr=linspace(lr[1], lr[0], steps_down),\n            momentum=linspace(momentum[1], momentum[0], steps_down),\n        ),\n        Phase(\n            lr=linspace(lr[0], finetune_lr, steps_annealing),\n            momentum=linspace(momentum[0], momentum[0], steps_annealing),\n        ),\n    ]\n\n\ndef sgdr_phases(\n        base_cycle_length: int,\n        cycles: int,\n        lr: Tuple[float, float] = (1., 0.1),\n        cycle_mult: int = 2,\n) -> List[Phase]:\n    """"""\n    The ""SGDR"" policy as described in the paper `SGDR: Stochastic Gradient Descent with Warm Restarts\n    <https://arxiv.org/abs/1608.03983>`_.\n\n    Note the total number of steps is calculated like this: `total_steps = sum(base_cycle_length *\n    (cycle_mult ** i) for i in range(cycles))`\n\n    You might want to read the paper and adjust the parameters.\n\n    Args:\n        base_cycle_length (int): number of steps for the first cycle.\n        cycles (int): the number of repetitions.\n        lr (Typle[float, float]): tuple for the learning rate for one cycle: (start, end).\n        cycle_mult (float): multiply the last cycle length with this every cycle. The length of a cycle\n            grows exponentially.\n\n    Returns:\n        A list of configured :class:`~poutyne.framework.callbacks.policies.Phase` instances.\n\n    References:\n        `SGDR: Stochastic Gradient Descent with Warm Restarts\n        <https://arxiv.org/abs/1608.03983>`_\n    """"""\n    steps = [base_cycle_length * (cycle_mult**i) for i in range(cycles)]\n    return [Phase(lr=cosinespace(lr[0], lr[1], step)) for step in steps]\n\n\n###############################################################################\nclass OptimizerPolicy(Callback):\n    """"""\n    Combine different :class:`~poutyne.framework.callbacks.policies.Phase` instances\n    in an :class:`~poutyne.framework.callbacks.policies.OptimizerPolicy` and execute the policies in a\n    row.\n\n    Args:\n        phases (List[~poutyne.framework.callbacks.policies.Phase]):\n            A list of :class:`~poutyne.framework.callbacks.policies.Phase` instances.\n        initial_step (int): The step to start the policy in. Used for restarting.\n    """"""\n\n    def __init__(self, phases: List, *, initial_step: int = 0):\n        super().__init__()\n        self.phases = phases\n        self.current_step = initial_step\n        self.phases_iter = iter(self)\n\n    def on_train_batch_begin(self, batch_number: int, logs: Dict):\n        # Don\'t do anything when we run out of phases.\n        with contextlib.suppress(StopIteration):\n            spec = next(self.phases_iter)\n            self._update_optimizer(spec)\n\n    def __iter__(self):\n        space_iter = islice(chain.from_iterable(self.phases), self.current_step, None)\n        for param_dict in space_iter:\n            self.current_step += 1\n            yield param_dict\n\n    def all_steps(self) -> List[Dict]:\n        """"""\n        Return the list of dictionaries of configurations for all steps.\n\n        This does not advance the current_step count.\n\n        Returns:\n            A list of dictionaries of all the parameters for each step.\n        """"""\n        return chain.from_iterable(self.phases)\n\n    def __repr__(self):\n        return ""OptimizerPolicy:\\n    phases: {}\\n    current_step: {}"".format(self.current_step, len(self.phases))\n\n    def _update_optimizer(self, param_dict: Dict):\n        for param_name, param_value in param_dict.items():\n            for group in self.model.optimizer.param_groups:\n                group[param_name] = param_value\n\n    def plot(self, param_name: str = ""lr"", ax=None):\n        """"""\n        Visualize all  :class:`~poutyne.framework.callbacks.policies.Phase`s of\n        this  :class:`~poutyne.framework.callbacks.policies.OptimizerPolicy`.\n\n        Args:\n            param_name (str, optional): the name of the parameter to plot.\n            ax (~matplotlib.pyplot.axis): a matplotlib axis to plot on, if given.\n\n        Returns:\n            The matplotlib axis.\n        """"""\n        # pylint: disable=import-error\n        import matplotlib.pyplot as plt\n\n        if ax is None:\n            _fig, ax = plt.subplots()\n\n        values = [step[param_name] for step in self.all_steps()]\n        ax.plot(values)\n        ax.set_ylabel(param_name)\n        ax.set_xlabel(""steps"")\n        return ax\n'"
poutyne/framework/callbacks/progress.py,0,"b'import itertools\nimport sys\nfrom typing import Dict\n\nfrom .callbacks import Callback\n\n\nclass ProgressionCallback(Callback):\n    def on_train_begin(self, logs: Dict):\n        self.metrics = [\'loss\'] + self.model.metrics_names\n        self.epochs = self.params[\'epochs\']\n        self.steps = self.params[\'steps\']\n\n    def on_epoch_begin(self, epoch_number: int, logs: Dict):\n        self.step_times_sum = 0.\n        self.epoch_number = epoch_number\n        sys.stdout.write(""\\rEpoch %d/%d"" % (self.epoch_number, self.epochs))\n        sys.stdout.flush()\n\n    def on_epoch_end(self, epoch_number: int, logs: Dict):\n        epoch_total_time = logs[\'time\']\n\n        metrics_str = self._get_metrics_string(logs)\n        if self.steps is not None:\n            print(""\\rEpoch %d/%d %.2fs Step %d/%d: %s"" %\n                  (self.epoch_number, self.epochs, epoch_total_time, self.steps, self.steps, metrics_str))\n        else:\n            print(""\\rEpoch %d/%d %.2fs: Step %d/%d: %s"" %\n                  (self.epoch_number, self.epochs, epoch_total_time, self.last_step, self.last_step, metrics_str))\n\n    def on_train_batch_end(self, batch_number: int, logs: Dict):\n        self.step_times_sum += logs[\'time\']\n\n        metrics_str = self._get_metrics_string(logs)\n\n        times_mean = self.step_times_sum / batch_number\n        if self.steps is not None:\n            remaining_time = times_mean * (self.steps - batch_number)\n\n            sys.stdout.write(""\\rEpoch %d/%d ETA %.0fs Step %d/%d: %s"" %\n                             (self.epoch_number, self.epochs, remaining_time, batch_number, self.steps, metrics_str))\n            sys.stdout.flush()\n        else:\n            sys.stdout.write(""\\rEpoch %d/%d %.2fs/step Step %d: %s"" %\n                             (self.epoch_number, self.epochs, times_mean, batch_number, metrics_str))\n            sys.stdout.flush()\n            self.last_step = batch_number\n\n    def _get_metrics_string(self, logs: Dict):\n        train_metrics_str_gen = (\'{}: {:f}\'.format(k, logs[k]) for k in self.metrics if logs.get(k) is not None)\n        val_metrics_str_gen = (\'{}: {:f}\'.format(\'val_\' + k, logs[\'val_\' + k]) for k in self.metrics\n                               if logs.get(\'val_\' + k) is not None)\n        return \', \'.join(itertools.chain(train_metrics_str_gen, val_metrics_str_gen))\n'"
poutyne/framework/callbacks/terminate_on_nan.py,0,"b'from typing import Dict\n\nimport numpy as np\n\nfrom .callbacks import Callback\n\n\nclass TerminateOnNaN(Callback):\n    """"""\n    Stops the training when the loss is either `NaN` or `inf`.\n    """"""\n\n    def on_train_batch_end(self, batch_number: int, logs: Dict):\n        loss = logs[\'loss\']\n        if np.isnan(loss) or np.isinf(loss):\n            print(\'Batch %d: Invalid loss, terminating training\' % (batch_number))\n            self.model.stop_training = True\n'"
poutyne/framework/callbacks/tracker.py,7,"b'from typing import Dict, List, Tuple, Iterable\n\nimport numpy as np\nimport torch\n\nfrom .callbacks import Callback\n\n\nclass WeightsGradientsStatsTracker:\n    """"""\n    The weights\' gradient statistic tracker will estimate the absolute mean (i.e. the mean of the absolute values of\n    the weights\' gradients), running absolute mean variance (i.e. the variance of the absolute mean), min, absolute min,\n     max and absolute max per layer. The tracker is using the `Welford\'s online algorithm\n     <https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford\'s_online_algorithm>`_\n    to estimate the running mean and running variance of the absolute weights\' gradients.\n    """"""\n\n    def __init__(self, number_layers) -> None:\n        self.number_layers = number_layers\n\n        self.reset()\n\n    def batch_statistic_upgrade(self, named_parameters: Iterable[Tuple[str, torch.nn.parameter.Parameter]]) -> None:\n        """"""\n        Accumulate the running absolute mean, running absolute mean variance, min, absolute min, max ant the absolute\n        max for all the layers.\n\n        Args:\n             named_parameters (Iterable[Tuple[str, ~torch.nn.parameter.Parameter]): The named parameters of the model to\n             track.\n        """"""\n        batch_layer_abs_means = []\n        batch_layer_min = []\n        batch_layer_abs_min = []\n        batch_layer_max = []\n        batch_layer_abs_max = []\n\n        # Just in case we want to support second-order derivatives\n        with torch.no_grad():\n            for _, layer_params in named_parameters:\n                layer_gradient = layer_params.grad\n\n                abs_value_layer_gradient = layer_gradient.abs()\n\n                batch_layer_abs_means.append(abs_value_layer_gradient.mean().cpu().numpy())\n\n                batch_layer_min.append(layer_gradient.min().cpu().numpy())\n                batch_layer_abs_min.append(abs_value_layer_gradient.min().cpu().numpy())\n\n                batch_layer_max.append(layer_gradient.max().cpu().numpy())\n                batch_layer_abs_max.append(abs_value_layer_gradient.max().cpu().numpy())\n\n        batch_layer_abs_means = np.array(batch_layer_abs_means)\n        previous_mean = self.running_abs_mean\n\n        self.running_abs_mean = previous_mean + (batch_layer_abs_means - previous_mean) / self.count\n\n        self.running_m2 = self.running_m2 + (batch_layer_abs_means - previous_mean) * (batch_layer_abs_means -\n                                                                                       self.running_abs_mean)\n\n        self.running_abs_mean_var = self.running_m2 / (self.count - 1) if self.count > 1 else self.running_abs_mean_var\n\n        batch_layer_min = np.array(batch_layer_min)\n        batch_layer_max = np.array(batch_layer_max)\n\n        self.running_min = np.minimum(batch_layer_min, self.running_min)\n        self.running_abs_min = np.minimum(batch_layer_abs_min, self.running_abs_min)\n\n        self.running_max = np.maximum(batch_layer_max, self.running_max)\n        self.running_abs_max = np.maximum(batch_layer_abs_max, self.running_abs_max)\n\n        self.count += 1\n\n    def get_stats(self, layer_names: List[str]) -> Dict:\n        """"""\n        Get the accumulated statistics of the layers.\n\n        Note: This will reset the gradient tracker statistics values.\n\n        Args:\n            layer_names (List[str]): The names of the layer to get statistics from.\n\n        Returns:\n            A dictionary where the keys are the layer names and the values are the statistics of the layer.\n            The statistics is also a dictionary where the keys are the logged statistics\n            (mean, mean +/- std deviation, min, absolute min, max and the absolute max) and the values are\n            the corresponding statistic values.\n        """"""\n        formatted_stats = {}\n        for index, layer_name in enumerate(layer_names):\n            stats = {\n                ""mean"": self.running_abs_mean[index],\n                ""mean_std_dev_up"": self.running_abs_mean[index] + np.sqrt(self.running_abs_mean_var[index]),\n                ""mean_std_dev_down"": self.running_abs_mean[index] - np.sqrt(self.running_abs_mean_var[index]),\n                ""min"": self.running_min[index],\n                ""abs_min"": self.running_abs_min[index],\n                ""max"": self.running_max[index],\n                ""abs_max"": self.running_abs_max[index]\n            }\n\n            formatted_stats.update({layer_name: stats})\n\n        self.reset()\n        return formatted_stats\n\n    def reset(self) -> None:\n        """"""\n        Reset the running absolute mean, absolute mean variance, min, absolute min, max, absolute max and count values.\n        """"""\n        self.running_abs_mean = np.zeros([self.number_layers], dtype=""float32"")\n        self.running_abs_mean_var = np.zeros([self.number_layers], dtype=""float32"")\n        self.running_m2 = np.zeros([self.number_layers], dtype=""float32"")\n        self.running_min = np.zeros([self.number_layers], dtype=""float32"")\n        self.running_abs_min = np.zeros([self.number_layers], dtype=""float32"")\n        self.running_max = np.zeros([self.number_layers], dtype=""float32"")\n        self.running_abs_max = np.zeros([self.number_layers], dtype=""float32"")\n        self.count = 1\n\n\nclass Tracker(Callback):\n    def __init__(self, keep_bias: bool = False) -> None:\n        super().__init__()\n\n        self.keep_bias = keep_bias\n        self.layer_names = []\n        self.number_layers = 0\n\n        self.tracker = None\n\n    def on_train_batch_end(self, batch: int, logs: Dict):\n        # pylint: disable=unused-argument\n        named_parameters = ((n, p) for n, p in self.model.network.named_parameters() if self._keep_layer(p, n))\n        self.tracker.batch_statistic_upgrade(named_parameters)\n\n    def on_train_begin(self, logs: Dict):\n        for layer_name, layer_params in self.model.network.named_parameters():\n            self._update_layers_to_track(layer_name, layer_params)\n        self.tracker = WeightsGradientsStatsTracker(self.number_layers)\n\n    def on_epoch_end(self, epoch: int, logs: Dict):\n        self._on_epoch_end_log(epoch, logs)\n\n    def _on_epoch_end_log(self, epoch: int, logs: Dict):\n        """"""\n        The method to define the behavior of the logging tracker.\n\n        Args:\n            epoch (int): The epoch number.\n            logs (Dict): The epoch logs dictionary.\n        """"""\n        pass\n\n    def _update_layers_to_track(self, layer_name: str, layer_params: torch.nn.parameter.Parameter):\n        if self._keep_layer(layer_params, layer_name):\n            self.layer_names.append(layer_name)\n\n        self.number_layers = len(self.layer_names)\n\n    def _keep_layer(self, layer_params: torch.nn.parameter.Parameter, layer_name: str):\n        layer_require_grad = layer_params.requires_grad\n        if self.keep_bias:\n            return layer_require_grad\n        return layer_require_grad and (""bias"" not in layer_name)\n\n\nclass TensorBoardGradientTracker(Tracker):\n    """"""\n    Wrapper to track the statistics of the weights\' gradient per layer and log them in TensorBoard per epoch.\n\n    Args:\n        writer (~torch.utils.tensorboard.writer.SummaryWriter): The TensorBoard writer.\n        keep_bias (bool): Either or not to log the bias of the network.\n\n    Example:\n        Using TensorBoardGradientTracker::\n\n            from torch.utils.tensorboard import SummaryWriter\n            from poutyne.framework import Model\n            from poutyne.framework.callbacks import TensorBoardGradientTracker\n\n            writer = SummaryWriter(\'runs\')\n            tb_tracker = TensorBoardGradientTracker(writer)\n\n            model = Model(...)\n            model.fit_generator(..., callbacks=[tb_tracker])\n    """"""\n\n    def __init__(self, writer, keep_bias: bool = False) -> None:\n        super().__init__(keep_bias)\n\n        self.writer = writer\n\n    def _on_epoch_end_log(self, epoch: int, logs: Dict) -> None:\n        gradient_distributions_stats = [""mean"", ""mean_std_dev_up"", ""mean_std_dev_down""]\n        other_gradient_stats = [""min"", ""max""]\n\n        formatted_stats = self.tracker.get_stats(self.layer_names)\n        for layer_name in self.layer_names:\n            stats = formatted_stats[layer_name]\n\n            for gradient_distributions_stat in gradient_distributions_stats:\n                self.writer.add_scalars(\'gradient_distributions/{}\'.format(layer_name),\n                                        {gradient_distributions_stat: stats[gradient_distributions_stat]}, epoch)\n            for other_gradient_stat in other_gradient_stats:\n                self.writer.add_scalars(\'other_gradient_stats/{}\'.format(layer_name),\n                                        {other_gradient_stat: stats[other_gradient_stat]}, epoch)\n'"
poutyne/framework/metrics/__init__.py,0,b'# pylint: disable=wildcard-import\nfrom .metrics_registering import *\nfrom .batch_metrics import *\nfrom .epoch_metrics import *\nfrom .utils import *\n'
poutyne/framework/metrics/batch_metrics.py,1,"b""import torch.nn.functional as F\n\nfrom .metrics_registering import register_batch_metric, register_batch_metric_function\n\n\n@register_batch_metric('acc', 'accuracy')\ndef acc(y_pred, y_true, ignore_index=-100):\n    y_pred = y_pred.argmax(1)\n    weights = (y_true != ignore_index).float()\n    num_labels = weights.sum()\n    acc_pred = ((y_pred == y_true).float() * weights).sum() / num_labels\n    return acc_pred * 100\n\n\n@register_batch_metric('binacc', 'binaryacc', 'binaryaccuracy')\ndef bin_acc(y_pred, y_true, threshold=0.):\n    y_pred = (y_pred > threshold).float()\n    acc_pred = (y_pred == y_true).float().mean()\n    return acc_pred * 100\n\n\ndef bce(y_pred, y_true):\n    return F.binary_cross_entropy(y_pred, y_true)\n\n\ndef bce_with_logits(y_pred, y_true):\n    return F.binary_cross_entropy_with_logits(y_pred, y_true)\n\n\nregister_batch_metric_function(F.cross_entropy)\nregister_batch_metric_function(bce, ['binary_cross_entropy', 'bce'])\nregister_batch_metric_function(bce_with_logits, ['binary_cross_entropy_with_logits', 'bce_with_logits'])\nregister_batch_metric_function(F.kl_div)\n\n\n@register_batch_metric\ndef poisson_nll(y_pred, y_true):\n    return F.poisson_nll_loss(y_pred, y_true)\n\n\n@register_batch_metric\ndef hinge_embedding(y_pred, y_true):\n    return F.hinge_embedding_loss(y_pred, y_true)\n\n\n@register_batch_metric\ndef l1(y_pred, y_true):\n    return F.l1_loss(y_pred, y_true)\n\n\n@register_batch_metric\ndef mse(y_pred, y_true):\n    return F.mse_loss(y_pred, y_true)\n\n\n@register_batch_metric\ndef multilabel_margin(y_pred, y_true):\n    return F.multilabel_margin_loss(y_pred, y_true)\n\n\n@register_batch_metric\ndef multilabel_soft_margin(y_pred, y_true):\n    return F.multilabel_soft_margin_loss(y_pred, y_true)\n\n\n@register_batch_metric\ndef multi_margin(y_pred, y_true):\n    return F.multi_margin_loss(y_pred, y_true)\n\n\n@register_batch_metric\ndef nll(y_pred, y_true):\n    return F.nll_loss(y_pred, y_true)\n\n\n@register_batch_metric\ndef smooth_l1(y_pred, y_true):\n    return F.smooth_l1_loss(y_pred, y_true)\n\n\n@register_batch_metric\ndef soft_margin(y_pred, y_true):\n    return F.soft_margin_loss(y_pred, y_true)\n"""
poutyne/framework/metrics/metrics_registering.py,0,"b""from .utils import camel_to_snake\n\n\ndef _get_registering_decorator(register_function):\n    def register(name_or_func, *extra_names):\n        if isinstance(name_or_func, str):\n            names = [name_or_func] + list(extra_names)\n\n            def decorator_func(func):\n                register_function(func, names)\n                return func\n\n            return decorator_func\n\n        func = name_or_func\n        register_function(func)\n        return func\n\n    return register\n\n\nbatch_metrics_dict = {}\n\n\ndef clean_batch_metric_name(name):\n    name = name.lower()\n    name = name[:-4] if name.endswith('loss') else name\n    name = name.replace('_', '')\n    return name\n\n\ndef register_batch_metric_function(func, names=None):\n    names = [func.__name__] if names is None else names\n    names = [names] if isinstance(names, str) else names\n    batch_metrics_dict.update({clean_batch_metric_name(name): func for name in names})\n\n\nregister_batch_metric = _get_registering_decorator(register_batch_metric_function)\n\n\ndef get_loss_or_metric(loss_metric):\n    if isinstance(loss_metric, str):\n        loss_metric = clean_batch_metric_name(loss_metric)\n        return batch_metrics_dict[loss_metric]\n\n    return loss_metric\n\n\nepochs_metrics_dict = {}\n\n\ndef clean_epoch_metric_name(name):\n    name = name.lower()\n    name = name[:-5] if name.endswith('score') else name\n    name = name.replace('_', '')\n    return name\n\n\ndef register_epoch_metric_class(clz, names=None):\n    names = [camel_to_snake(clz.__name__)] if names is None else names\n    names = [names] if isinstance(names, str) else names\n    epochs_metrics_dict.update({clean_epoch_metric_name(name): clz for name in names})\n\n\nregister_epoch_metric = _get_registering_decorator(register_epoch_metric_class)\n\n\ndef get_epoch_metric(epoch_metric):\n    if isinstance(epoch_metric, str):\n        epoch_metric = clean_epoch_metric_name(epoch_metric)\n        return epochs_metrics_dict[epoch_metric]()\n    return epoch_metric\n"""
poutyne/framework/metrics/utils.py,0,"b'import re\nfrom collections import Counter\n\n# From https://stackoverflow.com/a/1176023\npattern1 = re.compile(r\'(.)([A-Z][a-z]+)\')\npattern2 = re.compile(r\'([a-z0-9])([A-Z])\')\n\n\ndef camel_to_snake(name):\n    """"""\n    Convert CamelCase to snake_case.\n\n    From https://stackoverflow.com/a/1176023\n    """"""\n    name = pattern1.sub(r\'\\1_\\2\', name)\n    return pattern2.sub(r\'\\1_\\2\', name).lower()\n\n\ndef get_names_of_metric(metric):\n    if isinstance(metric, tuple):\n        names, metric = metric\n    elif hasattr(metric, \'__name__\'):\n        names = metric.__name__\n    elif hasattr(metric, \'__class__\'):\n        names = camel_to_snake(metric.__class__.__name__)\n    else:\n        names = \'unknown_metric\'\n    return names, metric\n\n\ndef flatten_metric_names(metric_names):\n    to_list = lambda names: names if isinstance(names, list) else [names]\n    return [name for names in metric_names for name in to_list(names)]\n\n\ndef rename_doubles(batch_metrics_names, epoch_metrics_names):\n    counts = Counter(flatten_metric_names(batch_metrics_names + epoch_metrics_names))\n    numbering = Counter()\n    batch_metrics_names = rename_doubles_from_counts(batch_metrics_names, counts, numbering)\n    epoch_metrics_names = rename_doubles_from_counts(epoch_metrics_names, counts, numbering)\n    return batch_metrics_names, epoch_metrics_names\n\n\ndef rename_doubles_from_counts(metric_names, counts, numbering):\n    """"""\n    This function takes a list in the format `[\'a\', [\'b\', \'a\'], \'c\', \'a\', \'c\']`\n    and returns a list where each double is added a number so that there are no\n    more doubles in the list: `[\'a1\', [\'b\', \'a2\'], \'c1\', \'a3\', \'c2\']`. It does so\n    using the provided counts and using the numbering Counter object.\n    """"""\n\n    def get_name(name):\n        if counts[name] > 1:\n            numbering[name] += 1\n            return name + str(numbering[name])\n        return name\n\n    return [[get_name(name) for name in names] if not isinstance(names, str) else get_name(names)\n            for names in metric_names]\n\n\ndef get_callables_and_names(metrics):\n    if len(metrics) != 0:\n        metrics = list(map(get_names_of_metric, metrics))\n        names, metrics = tuple(zip(*metrics))\n        # Make sure that batch_metrics and epoch_metrics are both lists.\n        return list(metrics), list(names)\n    return [], []\n'"
tests/framework/callbacks/__init__.py,0,b''
tests/framework/callbacks/test_best_model_restore.py,5,"b""import unittest\nfrom unittest import TestCase\n\nimport torch\nimport torch.nn as nn\n\nfrom poutyne import torch_to_numpy\nfrom poutyne.framework import Model\nfrom poutyne.framework.callbacks import BestModelRestore\n\n\ndef some_data_generator(batch_size):\n    while True:\n        x = torch.rand(batch_size, 1)\n        y = torch.rand(batch_size, 1)\n        yield x, y\n\n\nclass BestModelRestoreTest(TestCase):\n    batch_size = 20\n\n    def setUp(self):\n        torch.manual_seed(42)\n        self.pytorch_network = nn.Linear(1, 1)\n        self.loss_function = nn.MSELoss()\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=1e-3)\n        self.model = Model(self.pytorch_network, self.optimizer, self.loss_function)\n\n    def test_integration(self):\n        train_gen = some_data_generator(20)\n        valid_gen = some_data_generator(20)\n        model_restore = BestModelRestore(monitor='val_loss', verbose=True)\n        self.model.fit_generator(train_gen, valid_gen, epochs=10, steps_per_epoch=5, callbacks=[model_restore])\n\n    def test_basic_restore(self):\n        model_restore = BestModelRestore(monitor='val_loss')\n\n        val_losses = [3, 2, 8, 5, 4]\n        best_epoch = 2\n        self._test_restore_with_val_losses(model_restore, val_losses, best_epoch)\n\n    def test_save_best_only_with_max(self):\n        model_restore = BestModelRestore(monitor='val_loss', mode='max')\n\n        val_losses = [3, 2, 8, 5, 4]\n        best_epoch = 3\n        self._test_restore_with_val_losses(model_restore, val_losses, best_epoch)\n\n    def _test_restore_with_val_losses(self, checkpointer, val_losses, best_epoch):\n        generator = some_data_generator(BestModelRestoreTest.batch_size)\n\n        best_epoch_weights = None\n        checkpointer.set_params({'epochs': len(val_losses), 'steps': 1})\n        checkpointer.set_model(self.model)\n        checkpointer.on_train_begin({})\n        for epoch, val_loss in enumerate(val_losses, 1):\n            checkpointer.on_epoch_begin(epoch, {})\n            checkpointer.on_train_batch_begin(1, {})\n            loss = self._update_model(generator)\n            checkpointer.on_train_batch_end(1, {'batch': 1, 'size': BestModelRestoreTest.batch_size, 'loss': loss})\n            checkpointer.on_epoch_end(epoch, {'epoch': epoch, 'loss': loss, 'val_loss': val_loss})\n            if epoch == best_epoch:\n                best_epoch_weights = torch_to_numpy(self.model.get_weight_copies())\n        checkpointer.on_train_end({})\n\n        final_weights = torch_to_numpy(self.model.get_weight_copies())\n        self.assertEqual(best_epoch_weights, final_weights)\n\n    def _update_model(self, generator):\n        self.pytorch_network.zero_grad()\n\n        x, y = next(generator)\n        pred_y = self.pytorch_network(x)\n        loss = self.loss_function(pred_y, y)\n        loss.backward()\n\n        self.optimizer.step()\n\n        return float(loss)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/framework/callbacks/test_checkpoint.py,5,"b""import os\n\nimport unittest\nfrom unittest import TestCase\n\nfrom tempfile import TemporaryDirectory\n\nimport torch\nimport torch.nn as nn\n\nfrom poutyne import torch_to_numpy\nfrom poutyne.framework import Model\nfrom poutyne.framework.callbacks import ModelCheckpoint\n\n\ndef some_data_generator(batch_size):\n    while True:\n        x = torch.rand(batch_size, 1)\n        y = torch.rand(batch_size, 1)\n        yield x, y\n\n\nclass ModelCheckpointTest(TestCase):\n    batch_size = 20\n\n    def setUp(self):\n        torch.manual_seed(42)\n        self.pytorch_network = nn.Linear(1, 1)\n        self.loss_function = nn.MSELoss()\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=1e-3)\n        self.model = Model(self.pytorch_network, self.optimizer, self.loss_function)\n        self.temp_dir_obj = TemporaryDirectory()\n        self.checkpoint_filename = os.path.join(self.temp_dir_obj.name, 'my_checkpoint_{epoch}.ckpt')\n\n    def tearDown(self):\n        self.temp_dir_obj.cleanup()\n\n    def test_integration(self):\n        train_gen = some_data_generator(ModelCheckpointTest.batch_size)\n        valid_gen = some_data_generator(ModelCheckpointTest.batch_size)\n        checkpointer = ModelCheckpoint(self.checkpoint_filename, monitor='val_loss', verbose=True, save_best_only=True)\n        self.model.fit_generator(train_gen, valid_gen, epochs=10, steps_per_epoch=5, callbacks=[checkpointer])\n\n    def test_temporary_filename_arg(self):\n        tmp_filename = os.path.join(self.temp_dir_obj.name, 'my_checkpoint.tmp.ckpt')\n        checkpoint_filename = os.path.join(self.temp_dir_obj.name, 'my_checkpoint.ckpt')\n        train_gen = some_data_generator(ModelCheckpointTest.batch_size)\n        valid_gen = some_data_generator(ModelCheckpointTest.batch_size)\n        checkpointer = ModelCheckpoint(checkpoint_filename,\n                                       monitor='val_loss',\n                                       verbose=True,\n                                       period=1,\n                                       temporary_filename=tmp_filename)\n        self.model.fit_generator(train_gen, valid_gen, epochs=10, steps_per_epoch=5, callbacks=[checkpointer])\n        self.assertFalse(os.path.isfile(tmp_filename))\n        self.assertTrue(os.path.isfile(checkpoint_filename))\n\n    def test_temporary_filename_arg_with_differing_checkpoint_filename(self):\n        epochs = 10\n        tmp_filename = os.path.join(self.temp_dir_obj.name, 'my_checkpoint.tmp.ckpt')\n        checkpoint_filename = os.path.join(self.temp_dir_obj.name, 'my_checkpoint_{epoch}.ckpt')\n        train_gen = some_data_generator(ModelCheckpointTest.batch_size)\n        valid_gen = some_data_generator(ModelCheckpointTest.batch_size)\n        checkpointer = ModelCheckpoint(checkpoint_filename,\n                                       monitor='val_loss',\n                                       verbose=True,\n                                       period=1,\n                                       temporary_filename=tmp_filename)\n        self.model.fit_generator(train_gen, valid_gen, epochs=epochs, steps_per_epoch=5, callbacks=[checkpointer])\n        self.assertFalse(os.path.isfile(tmp_filename))\n        for i in range(1, epochs + 1):\n            self.assertTrue(os.path.isfile(checkpoint_filename.format(epoch=i)))\n\n    def test_non_atomic_write(self):\n        checkpoint_filename = os.path.join(self.temp_dir_obj.name, 'my_checkpoint.ckpt')\n        train_gen = some_data_generator(ModelCheckpointTest.batch_size)\n        valid_gen = some_data_generator(ModelCheckpointTest.batch_size)\n        checkpointer = ModelCheckpoint(checkpoint_filename,\n                                       monitor='val_loss',\n                                       verbose=True,\n                                       period=1,\n                                       atomic_write=False)\n        self.model.fit_generator(train_gen, valid_gen, epochs=10, steps_per_epoch=5, callbacks=[checkpointer])\n        self.assertTrue(os.path.isfile(checkpoint_filename))\n\n    def test_save_best_only(self):\n        checkpointer = ModelCheckpoint(self.checkpoint_filename, monitor='val_loss', verbose=True, save_best_only=True)\n\n        val_losses = [10, 3, 8, 5, 2]\n        has_checkpoints = [True, True, False, False, True]\n        self._test_checkpointer_with_val_losses(checkpointer, val_losses, has_checkpoints)\n\n    def test_save_best_only_with_restore_best(self):\n        checkpointer = ModelCheckpoint(self.checkpoint_filename,\n                                       monitor='val_loss',\n                                       verbose=True,\n                                       save_best_only=True,\n                                       restore_best=True)\n\n        val_losses = [10, 3, 8, 5, 2]\n        has_checkpoints = [True, True, False, False, True]\n        self._test_checkpointer_with_val_losses(checkpointer, val_losses, has_checkpoints)\n\n        self._test_restore_best(val_losses)\n\n    def test_restore_best_without_save_best_only(self):\n        with self.assertRaises(ValueError):\n            ModelCheckpoint(self.checkpoint_filename,\n                            monitor='val_loss',\n                            verbose=True,\n                            save_best_only=False,\n                            restore_best=True)\n\n        with self.assertRaises(ValueError):\n            ModelCheckpoint(self.checkpoint_filename, monitor='val_loss', verbose=True, restore_best=True)\n\n    def test_save_best_only_with_max(self):\n        checkpointer = ModelCheckpoint(self.checkpoint_filename,\n                                       monitor='val_loss',\n                                       mode='max',\n                                       verbose=True,\n                                       save_best_only=True)\n\n        val_losses = [2, 3, 8, 5, 2]\n        has_checkpoints = [True, True, True, False, False]\n        self._test_checkpointer_with_val_losses(checkpointer, val_losses, has_checkpoints)\n\n    def test_periodic_with_period_of_1(self):\n        checkpointer = ModelCheckpoint(self.checkpoint_filename,\n                                       monitor='val_loss',\n                                       verbose=True,\n                                       period=1,\n                                       save_best_only=False)\n\n        val_losses = [1] * 10\n        has_checkpoints = [True] * 10\n        self._test_checkpointer_with_val_losses(checkpointer, val_losses, has_checkpoints)\n\n    def test_periodic_with_period_of_2(self):\n        checkpointer = ModelCheckpoint(self.checkpoint_filename,\n                                       monitor='val_loss',\n                                       verbose=True,\n                                       period=2,\n                                       save_best_only=False)\n\n        val_losses = [1] * 10\n        has_checkpoints = [False, True] * 5\n        self._test_checkpointer_with_val_losses(checkpointer, val_losses, has_checkpoints)\n\n    def _test_checkpointer_with_val_losses(self, checkpointer, val_losses, has_checkpoints):\n        generator = some_data_generator(ModelCheckpointTest.batch_size)\n\n        checkpointer.set_params({'epochs': len(val_losses), 'steps': 1})\n        checkpointer.set_model(self.model)\n        checkpointer.on_train_begin({})\n        for epoch, (val_loss, has_checkpoint) in enumerate(zip(val_losses, has_checkpoints), 1):\n            checkpointer.on_epoch_begin(epoch, {})\n            checkpointer.on_train_batch_begin(1, {})\n            loss = self._update_model(generator)\n            checkpointer.on_train_batch_end(1, {'batch': 1, 'size': ModelCheckpointTest.batch_size, 'loss': loss})\n            checkpointer.on_epoch_end(epoch, {'epoch': epoch, 'loss': loss, 'val_loss': val_loss})\n            filename = self.checkpoint_filename.format(epoch=epoch)\n            self.assertEqual(has_checkpoint, os.path.isfile(filename))\n        checkpointer.on_train_end({})\n\n    def _update_model(self, generator):\n        self.pytorch_network.zero_grad()\n\n        x, y = next(generator)\n        pred_y = self.pytorch_network(x)\n        loss = self.loss_function(pred_y, y)\n        loss.backward()\n\n        self.optimizer.step()\n\n        return float(loss)\n\n    def _test_restore_best(self, val_losses):\n        final_weights = torch_to_numpy(self.model.get_weight_copies())\n\n        epoch = val_losses.index(min(val_losses)) + 1\n        best_epoch_filename = self.checkpoint_filename.format(epoch=epoch)\n        self.model.load_weights(best_epoch_filename)\n\n        best_weights = torch_to_numpy(self.model.get_weight_copies())\n\n        self.assertEqual(best_weights, final_weights)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/framework/callbacks/test_delay.py,5,"b""import unittest\nfrom unittest import TestCase\nfrom unittest.mock import MagicMock, call, ANY\n\nimport torch\nimport torch.nn as nn\n\nfrom poutyne.framework import Model\nfrom poutyne.framework.callbacks import DelayCallback, Callback\n\n\ndef some_data_generator(batch_size):\n    while True:\n        x = torch.rand(batch_size, 1)\n        y = torch.rand(batch_size, 1)\n        yield x, y\n\n\nclass DelayCallbackTest(TestCase):\n    epochs = 10\n    steps_per_epoch = 5\n    batch_size = 20\n\n    def setUp(self):\n        torch.manual_seed(42)\n        self.pytorch_network = nn.Linear(1, 1)\n        self.loss_function = nn.MSELoss()\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=1e-3)\n        self.model = Model(self.pytorch_network, self.optimizer, self.loss_function)\n        self.mock_callback = MagicMock(spec=Callback)\n        self.delay_callback = DelayCallback(self.mock_callback)\n        self.train_dict = {'loss': ANY, 'time': ANY}\n        self.log_dict = {'loss': ANY, 'val_loss': ANY, 'time': ANY}\n\n    def test_epoch_delay(self):\n        epoch_delay = 4\n        delay_callback = DelayCallback(self.mock_callback, epoch_delay=epoch_delay)\n        train_generator = some_data_generator(DelayCallbackTest.batch_size)\n        valid_generator = some_data_generator(DelayCallbackTest.batch_size)\n        self.model.fit_generator(train_generator,\n                                 valid_generator,\n                                 epochs=DelayCallbackTest.epochs,\n                                 steps_per_epoch=DelayCallbackTest.steps_per_epoch,\n                                 validation_steps=DelayCallbackTest.steps_per_epoch,\n                                 callbacks=[delay_callback])\n        params = {'epochs': DelayCallbackTest.epochs, 'steps': DelayCallbackTest.steps_per_epoch}\n\n        call_list = []\n        call_list.append(call.on_train_begin({}))\n        for epoch in range(epoch_delay + 1, DelayCallbackTest.epochs + 1):\n            call_list.append(call.on_epoch_begin(epoch, {}))\n            for step in range(1, params['steps'] + 1):\n                call_list.append(call.on_train_batch_begin(step, {}))\n                call_list.append(call.on_backward_end(step))\n                call_list.append(\n                    call.on_train_batch_end(step, {\n                        'batch': step,\n                        'size': DelayCallbackTest.batch_size,\n                        **self.train_dict\n                    }))\n            call_list.append(call.on_epoch_end(epoch, {'epoch': epoch, **self.log_dict}))\n        call_list.append(call.on_train_end({}))\n\n        method_calls = self.mock_callback.method_calls\n        self.assertIn(call.set_model(self.model), method_calls[:2])\n        self.assertIn(call.set_params(params), method_calls[:2])\n\n        self.assertEqual(len(method_calls), len(call_list) + 2)\n        self.assertEqual(method_calls[2:], call_list)\n\n    def test_batch_delay_in_middle_of_epoch(self):\n        self._test_batch_delay(epoch_delay=5, batch_in_epoch_delay=3)\n\n    def test_batch_delay_at_begin_of_epoch(self):\n        self._test_batch_delay(epoch_delay=5, batch_in_epoch_delay=0)\n\n    def test_batch_delay_when_no_delay(self):\n        self._test_batch_delay(epoch_delay=0, batch_in_epoch_delay=0)\n\n    def _test_batch_delay(self, epoch_delay, batch_in_epoch_delay):\n        batch_delay = epoch_delay * DelayCallbackTest.steps_per_epoch + batch_in_epoch_delay\n        delay_callback = DelayCallback(self.mock_callback, batch_delay=batch_delay)\n        train_generator = some_data_generator(DelayCallbackTest.batch_size)\n        valid_generator = some_data_generator(DelayCallbackTest.batch_size)\n        self.model.fit_generator(train_generator,\n                                 valid_generator,\n                                 epochs=DelayCallbackTest.epochs,\n                                 steps_per_epoch=DelayCallbackTest.steps_per_epoch,\n                                 validation_steps=DelayCallbackTest.steps_per_epoch,\n                                 callbacks=[delay_callback])\n        params = {'epochs': DelayCallbackTest.epochs, 'steps': DelayCallbackTest.steps_per_epoch}\n\n        call_list = []\n        call_list.append(call.on_train_begin({}))\n        for epoch in range(epoch_delay + 1, DelayCallbackTest.epochs + 1):\n            call_list.append(call.on_epoch_begin(epoch, {}))\n            start_step = batch_in_epoch_delay + 1 if epoch == epoch_delay + 1 else 1\n            for step in range(start_step, params['steps'] + 1):\n                call_list.append(call.on_train_batch_begin(step, {}))\n                call_list.append(call.on_backward_end(step))\n                call_list.append(\n                    call.on_train_batch_end(step, {\n                        'batch': step,\n                        'size': DelayCallbackTest.batch_size,\n                        **self.train_dict\n                    }))\n            call_list.append(call.on_epoch_end(epoch, {'epoch': epoch, **self.log_dict}))\n        call_list.append(call.on_train_end({}))\n\n        method_calls = self.mock_callback.method_calls\n        self.assertIn(call.set_model(self.model), method_calls[:2])\n        self.assertIn(call.set_params(params), method_calls[:2])\n\n        self.assertEqual(len(method_calls), len(call_list) + 2)\n        self.assertEqual(method_calls[2:], call_list)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/framework/callbacks/test_earlystopping.py,5,"b""import unittest\nfrom unittest import TestCase\n\nimport torch\nimport torch.nn as nn\n\nfrom poutyne.framework import Model\nfrom poutyne.framework.callbacks import EarlyStopping\n\n\ndef some_data_generator(batch_size):\n    while True:\n        x = torch.rand(batch_size, 1)\n        y = torch.rand(batch_size, 1)\n        yield x, y\n\n\nclass EarlyStoppingTest(TestCase):\n    batch_size = 20\n\n    def setUp(self):\n        torch.manual_seed(42)\n        self.pytorch_network = nn.Linear(1, 1)\n        self.loss_function = nn.MSELoss()\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=1e-3)\n        self.model = Model(self.pytorch_network, self.optimizer, self.loss_function)\n\n    def test_integration(self):\n        train_gen = some_data_generator(20)\n        valid_gen = some_data_generator(20)\n        earlystopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=False)\n        self.model.fit_generator(train_gen, valid_gen, epochs=10, steps_per_epoch=5, callbacks=[earlystopper])\n\n    def test_early_stopping_patience_of_1(self):\n        earlystopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=1, verbose=False)\n\n        val_losses = [8, 4, 5, 2]\n        early_stop_epoch = 3\n        self._test_early_stopping(earlystopper, val_losses, early_stop_epoch)\n\n    def test_early_stopping_with_delta(self):\n        earlystopper = EarlyStopping(monitor='val_loss', min_delta=3, patience=2, verbose=False)\n\n        val_losses = [8, 4, 5, 2, 2]\n        early_stop_epoch = 4\n        self._test_early_stopping(earlystopper, val_losses, early_stop_epoch)\n\n    def test_early_stopping_with_max(self):\n        earlystopper = EarlyStopping(monitor='val_loss', mode='max', min_delta=0, patience=2, verbose=False)\n\n        val_losses = [2, 8, 4, 5, 2]\n        early_stop_epoch = 4\n        self._test_early_stopping(earlystopper, val_losses, early_stop_epoch)\n\n    def _test_early_stopping(self, earlystopper, val_losses, early_stop_epoch):\n        generator = some_data_generator(EarlyStoppingTest.batch_size)\n\n        self.model.stop_training = False\n\n        earlystopper.set_params({'epochs': len(val_losses), 'steps': 1})\n        earlystopper.set_model(self.model)\n        earlystopper.on_train_begin({})\n        for epoch, val_loss in enumerate(val_losses, 1):\n            earlystopper.on_epoch_begin(epoch, {})\n            earlystopper.on_train_batch_begin(1, {})\n            loss = self._update_model(generator)\n            earlystopper.on_train_batch_end(1, {'batch': 1, 'size': EarlyStoppingTest.batch_size, 'loss': loss})\n            earlystopper.on_epoch_end(epoch, {'epoch': epoch, 'loss': loss, 'val_loss': val_loss})\n            self.assertEqual(self.model.stop_training, epoch == early_stop_epoch)\n            if epoch == early_stop_epoch:\n                break\n\n        earlystopper.on_train_end({})\n\n    def _update_model(self, generator):\n        self.pytorch_network.zero_grad()\n\n        x, y = next(generator)\n        pred_y = self.pytorch_network(x)\n        loss = self.loss_function(pred_y, y)\n        loss.backward()\n\n        self.optimizer.step()\n\n        return float(loss)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/framework/callbacks/test_gradient_logger.py,5,"b""import csv\nimport os\nfrom tempfile import TemporaryDirectory\nfrom unittest import TestCase\n\nimport torch\nimport torch.nn as nn\n\nfrom poutyne.framework import Model\nfrom poutyne.framework import CSVGradientLogger as NonAtomicCSVGradientLogger\nfrom poutyne.framework import AtomicCSVGradientLogger, MemoryGradientLogger\n\n\ndef some_data_generator(batch_size):\n    while True:\n        x = torch.rand(batch_size, 1)\n        y = torch.rand(batch_size, 1)\n        yield x, y\n\n\nclass BaseCSVGradientLoggerTest:\n    # pylint: disable=not-callable,no-member\n    CSVGradientLogger = None\n    batch_size = 20\n    num_epochs = 10\n\n    def setUp(self):\n        torch.manual_seed(42)\n        self.pytorch_network = nn.Sequential(nn.Linear(1, 2), nn.Linear(2, 1))\n        self.loss_function = nn.MSELoss()\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=1e-3)\n        self.model = Model(self.pytorch_network, self.optimizer, self.loss_function)\n        self.temp_dir_obj = TemporaryDirectory()\n        self.csv_filename = os.path.join(self.temp_dir_obj.name, 'layer_{}.csv')\n\n    def tearDown(self):\n        self.temp_dir_obj.cleanup()\n\n    def test_logging(self):\n        train_gen = some_data_generator(self.batch_size)\n        valid_gen = some_data_generator(self.batch_size)\n        memgrad = MemoryGradientLogger()\n        logger = self.CSVGradientLogger(self.csv_filename)\n        self.model.fit_generator(train_gen,\n                                 valid_gen,\n                                 epochs=self.num_epochs,\n                                 steps_per_epoch=5,\n                                 callbacks=[memgrad, logger])\n        self._test_logging(memgrad.history)\n\n    def test_logging_append(self):\n        train_gen = some_data_generator(self.batch_size)\n        valid_gen = some_data_generator(self.batch_size)\n        logger = self.CSVGradientLogger(self.csv_filename)\n        memgrad = MemoryGradientLogger()\n        self.model.fit_generator(train_gen,\n                                 valid_gen,\n                                 epochs=self.num_epochs,\n                                 steps_per_epoch=5,\n                                 callbacks=[memgrad, logger])\n        memgrad2 = MemoryGradientLogger()\n        logger = self.CSVGradientLogger(self.csv_filename, append=True)\n        self.model.fit_generator(train_gen,\n                                 valid_gen,\n                                 epochs=20,\n                                 steps_per_epoch=5,\n                                 initial_epoch=self.num_epochs,\n                                 callbacks=[memgrad2, logger])\n        history = {layer: stats1 + memgrad2.history[layer] for layer, stats1 in memgrad.history.items()}\n        self._test_logging(history)\n\n    def test_logging_overwrite(self):\n        train_gen = some_data_generator(self.batch_size)\n        valid_gen = some_data_generator(self.batch_size)\n        logger = self.CSVGradientLogger(self.csv_filename)\n        self.model.fit_generator(train_gen, valid_gen, epochs=self.num_epochs, steps_per_epoch=5, callbacks=[logger])\n        memgrad = MemoryGradientLogger()\n        logger = self.CSVGradientLogger(self.csv_filename, append=False)\n        self.model.fit_generator(train_gen,\n                                 valid_gen,\n                                 epochs=20,\n                                 steps_per_epoch=5,\n                                 initial_epoch=self.num_epochs,\n                                 callbacks=[memgrad, logger])\n        self._test_logging(memgrad.history)\n\n    def _test_logging(self, history):\n        for layer, stats in history.items():\n            filename = self.csv_filename.format(layer)\n            with open(filename) as csvfile:\n                reader = csv.DictReader(csvfile)\n                rows = list(reader)\n            self.assertEqual(len(rows), len(stats))\n            for row, stats_entry in zip(rows, stats):\n                self.assertEqual(row.keys(), stats_entry.keys())\n                for k in row.keys():\n                    self.assertAlmostEqual(float(row[k]), stats_entry[k])\n\n\nclass NonAtomicCSVGradientLoggerTest(BaseCSVGradientLoggerTest, TestCase):\n    CSVGradientLogger = NonAtomicCSVGradientLogger\n\n\nclass AtomicCSVGradientLoggerTest(BaseCSVGradientLoggerTest, TestCase):\n    CSVGradientLogger = AtomicCSVGradientLogger\n"""
tests/framework/callbacks/test_logger.py,8,"b'import csv\nimport os\nfrom tempfile import TemporaryDirectory\nfrom unittest import TestCase, skipIf, main\nfrom unittest.mock import MagicMock, call\n\nimport torch\nimport torch.nn as nn\n\ntry:\n    from torch.utils.tensorboard import SummaryWriter as TorchSummaryWriter\nexcept ImportError:\n    TorchSummaryWriter = None\n\ntry:\n    from tensorboardX import SummaryWriter as XSummaryWriter\nexcept ImportError:\n    XSummaryWriter = None\n\nfrom poutyne.framework import Model, Callback, TensorBoardLogger\nfrom poutyne.framework import CSVLogger as NonAtomicCSVLogger\nfrom poutyne.framework import AtomicCSVLogger\n\n\ndef some_data_generator(batch_size):\n    while True:\n        x = torch.rand(batch_size, 1)\n        y = torch.rand(batch_size, 1)\n        yield x, y\n\n\nclass History(Callback):\n    def on_epoch_end(self, epoch_number, logs):\n        self.history.append(logs)\n\n    def on_train_batch_end(self, batch_number, logs):\n        self.history.append(logs)\n\n    def on_train_begin(self, logs):\n        self.history = []\n\n\nclass BaseCSVLoggerTest:\n    # pylint: disable=not-callable,no-member\n    CSVLogger = None\n    batch_size = 20\n    lr = 1e-3\n    num_epochs = 10\n\n    def setUp(self):\n        torch.manual_seed(42)\n        self.pytorch_network = nn.Linear(1, 1)\n        self.loss_function = nn.MSELoss()\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=BaseCSVLoggerTest.lr)\n        self.model = Model(self.pytorch_network, self.optimizer, self.loss_function)\n        self.temp_dir_obj = TemporaryDirectory()\n        self.csv_filename = os.path.join(self.temp_dir_obj.name, \'my_log.csv\')\n\n    def tearDown(self):\n        self.temp_dir_obj.cleanup()\n\n    def test_logging(self):\n        train_gen = some_data_generator(20)\n        valid_gen = some_data_generator(20)\n        logger = self.CSVLogger(self.csv_filename)\n        history = self.model.fit_generator(train_gen,\n                                           valid_gen,\n                                           epochs=self.num_epochs,\n                                           steps_per_epoch=5,\n                                           callbacks=[logger])\n        self._test_logging(history)\n\n    def test_logging_with_batch_granularity(self):\n        train_gen = some_data_generator(20)\n        valid_gen = some_data_generator(20)\n        logger = self.CSVLogger(self.csv_filename, batch_granularity=True)\n        history = History()\n        self.model.fit_generator(train_gen,\n                                 valid_gen,\n                                 epochs=self.num_epochs,\n                                 steps_per_epoch=5,\n                                 callbacks=[logger, history])\n        self._test_logging(history.history)\n\n    def test_logging_append(self):\n        train_gen = some_data_generator(20)\n        valid_gen = some_data_generator(20)\n        logger = self.CSVLogger(self.csv_filename)\n        history = self.model.fit_generator(train_gen,\n                                           valid_gen,\n                                           epochs=self.num_epochs,\n                                           steps_per_epoch=5,\n                                           callbacks=[logger])\n        logger = self.CSVLogger(self.csv_filename, append=True)\n        history2 = self.model.fit_generator(train_gen,\n                                            valid_gen,\n                                            epochs=20,\n                                            steps_per_epoch=5,\n                                            initial_epoch=self.num_epochs,\n                                            callbacks=[logger])\n        self._test_logging(history + history2)\n\n    def test_logging_overwrite(self):\n        train_gen = some_data_generator(20)\n        valid_gen = some_data_generator(20)\n        logger = self.CSVLogger(self.csv_filename)\n        self.model.fit_generator(train_gen, valid_gen, epochs=self.num_epochs, steps_per_epoch=5, callbacks=[logger])\n        logger = self.CSVLogger(self.csv_filename, append=False)\n        history = self.model.fit_generator(train_gen,\n                                           valid_gen,\n                                           epochs=20,\n                                           steps_per_epoch=5,\n                                           initial_epoch=self.num_epochs,\n                                           callbacks=[logger])\n        self._test_logging(history)\n\n    def _test_logging(self, history):\n        with open(self.csv_filename) as csvfile:\n            reader = csv.DictReader(csvfile)\n            rows = []\n            for row in reader:\n                if row[\'epoch\'] != \'\':\n                    self.assertAlmostEqual(float(row[\'lr\']), BaseCSVLoggerTest.lr)\n                del row[\'lr\']\n                rows.append(row)\n        self.assertEqual(len(rows), len(history))\n        for row, hist_entry in zip(rows, history):\n            row = {k: v for k, v in row.items() if v != \'\'}\n            self.assertEqual(row.keys(), hist_entry.keys())\n            for k in row.keys():\n                if isinstance(hist_entry[k], float):\n                    self.assertAlmostEqual(float(row[k]), hist_entry[k])\n                else:\n                    self.assertEqual(str(row[k]), str(hist_entry[k]))\n\n\nclass NonAtomicCSVLoggerTest(BaseCSVLoggerTest, TestCase):\n    CSVLogger = NonAtomicCSVLogger\n\n\nclass AtomicCSVLoggerTest(BaseCSVLoggerTest, TestCase):\n    CSVLogger = AtomicCSVLogger\n\n\nclass BaseTensorBoardLoggerTest:\n    SummaryWriter = None\n    batch_size = 20\n    lr = 1e-3\n    num_epochs = 10\n\n    def setUp(self):\n        torch.manual_seed(42)\n        self.pytorch_network = nn.Linear(1, 1)\n        self.loss_function = nn.MSELoss()\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=BaseTensorBoardLoggerTest.lr)\n        self.model = Model(self.pytorch_network, self.optimizer, self.loss_function)\n        self.temp_dir_obj = TemporaryDirectory()\n        # pylint: disable=not-callable\n        self.writer = self.SummaryWriter(self.temp_dir_obj.name)\n        self.writer.add_scalars = MagicMock()\n\n    def tearDown(self):\n        self.temp_dir_obj.cleanup()\n\n    def test_logging(self):\n        train_gen = some_data_generator(20)\n        valid_gen = some_data_generator(20)\n        logger = TensorBoardLogger(self.writer)\n        history = self.model.fit_generator(train_gen,\n                                           valid_gen,\n                                           epochs=self.num_epochs,\n                                           steps_per_epoch=5,\n                                           callbacks=[logger])\n        self._test_logging(history)\n\n    def _test_logging(self, history):\n        calls = list()\n        for h in history:\n            calls.append(call(\'loss\', {\'loss\': h[\'loss\'], \'val_loss\': h[\'val_loss\']}, h[\'epoch\']))\n            calls.append(call(\'lr\', {\'lr\': self.lr}, h[\'epoch\']))\n        self.writer.add_scalars.assert_has_calls(calls, any_order=True)\n\n\n@skipIf(XSummaryWriter is None, ""Needs tensorboardX to run this test"")\nclass TensorboardXLoggerTest(BaseTensorBoardLoggerTest, TestCase):\n    SummaryWriter = XSummaryWriter\n\n\n@skipIf(TorchSummaryWriter is None, ""Unable to import SummaryWriter from torch"")\nclass TorchTensorboardLoggerTest(BaseTensorBoardLoggerTest, TestCase):\n    SummaryWriter = TorchSummaryWriter\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tests/framework/callbacks/test_lr_scheduler.py,5,"b""# Because of the way the callbacks are generated, we have to disable linting here.\n# pylint: disable=no-name-in-module\nimport unittest\nfrom unittest import TestCase\n\nimport torch\nimport torch.nn as nn\n\nfrom poutyne.framework import Model\nfrom poutyne.framework.callbacks import LambdaLR, StepLR, MultiStepLR, ExponentialLR, \\\n    CosineAnnealingLR, ReduceLROnPlateau\n\n\ndef some_data_generator(batch_size):\n    while True:\n        x = torch.rand(batch_size, 1)\n        y = torch.rand(batch_size, 1)\n        yield x, y\n\n\nclass LRSchedulersTest(TestCase):\n    batch_size = 20\n    epochs = 10\n    steps_per_epoch = 5\n\n    def setUp(self):\n        torch.manual_seed(42)\n        self.pytorch_network = nn.Linear(1, 1)\n        self.loss_function = nn.MSELoss()\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=1e-3)\n        self.model = Model(self.pytorch_network, self.optimizer, self.loss_function)\n        self.train_gen = some_data_generator(20)\n        self.valid_gen = some_data_generator(20)\n\n    def test_lambda_lr_integration(self):\n        my_lambda = lambda epoch: 0.95**epoch\n        lambda_lr = LambdaLR(lr_lambda=[my_lambda])\n        self._fit_with_callback_integration(lambda_lr)\n\n    def test_step_lr_integration(self):\n        step_lr = StepLR(step_size=3)\n        self._fit_with_callback_integration(step_lr)\n\n    def test_multistep_lr_integration(self):\n        multistep_lr = MultiStepLR(milestones=[2, 5, 7])\n        self._fit_with_callback_integration(multistep_lr)\n\n    def test_exponential_lr_integration(self):\n        exponential_lr = ExponentialLR(gamma=0.01)\n        self._fit_with_callback_integration(exponential_lr)\n\n    def test_cosine_annealing_lr_integration(self):\n        cosine_annealing_lr = CosineAnnealingLR(T_max=8)\n        self._fit_with_callback_integration(cosine_annealing_lr)\n\n    def test_reduce_lr_on_plateau_integration(self):\n        reduce_lr = ReduceLROnPlateau(monitor='loss', patience=3)\n        self._fit_with_callback_integration(reduce_lr)\n\n    def _fit_with_callback_integration(self, callback):\n        self.model.fit_generator(self.train_gen,\n                                 self.valid_gen,\n                                 epochs=LRSchedulersTest.epochs,\n                                 steps_per_epoch=LRSchedulersTest.steps_per_epoch,\n                                 callbacks=[callback])\n\n    def test_exception_is_thrown_on_optimizer_argument(self):\n        with self.assertRaises(ValueError):\n            StepLR(self.optimizer, step_size=3)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/framework/callbacks/test_lr_scheduler_checkpoint.py,5,"b""# Because of the way the callbacks are generated, we have to disable linting here.\n# pylint: disable=no-name-in-module\nimport os\n\nimport unittest\nfrom unittest import TestCase\nfrom tempfile import TemporaryDirectory\n\nimport torch\nimport torch.nn as nn\n\nfrom poutyne import torch_to_numpy\nfrom poutyne.framework import Model\nfrom poutyne.framework.callbacks import LRSchedulerCheckpoint\nfrom poutyne.framework.callbacks import ExponentialLR, ReduceLROnPlateau\n\n\ndef some_data_generator(batch_size):\n    while True:\n        x = torch.rand(batch_size, 1)\n        y = torch.rand(batch_size, 1)\n        yield x, y\n\n\nclass OptimizerCheckpointTest(TestCase):\n    batch_size = 20\n    epochs = 10\n\n    def setUp(self):\n        torch.manual_seed(42)\n        self.pytorch_network = nn.Linear(1, 1)\n        self.loss_function = nn.MSELoss()\n        self.optimizer = torch.optim.Adam(self.pytorch_network.parameters(), lr=1e-3)\n        self.model = Model(self.pytorch_network, self.optimizer, self.loss_function)\n        self.temp_dir_obj = TemporaryDirectory()\n        self.checkpoint_filename = os.path.join(self.temp_dir_obj.name, 'my_checkpoint_{epoch}.optim')\n\n    def tearDown(self):\n        self.temp_dir_obj.cleanup()\n\n    def test_any_scheduler_integration(self):\n        train_gen = some_data_generator(OptimizerCheckpointTest.batch_size)\n        valid_gen = some_data_generator(OptimizerCheckpointTest.batch_size)\n        lr_scheduler = ExponentialLR(gamma=0.01)\n        checkpointer = LRSchedulerCheckpoint(lr_scheduler, self.checkpoint_filename, period=1)\n        self.model.fit_generator(train_gen,\n                                 valid_gen,\n                                 epochs=OptimizerCheckpointTest.epochs,\n                                 steps_per_epoch=5,\n                                 callbacks=[checkpointer])\n\n    def test_reduce_lr_on_plateau_integration(self):\n        train_gen = some_data_generator(OptimizerCheckpointTest.batch_size)\n        valid_gen = some_data_generator(OptimizerCheckpointTest.batch_size)\n        reduce_lr = ReduceLROnPlateau(monitor='loss', patience=3)\n        checkpointer = LRSchedulerCheckpoint(reduce_lr, self.checkpoint_filename, period=1)\n        self.model.fit_generator(train_gen,\n                                 valid_gen,\n                                 epochs=OptimizerCheckpointTest.epochs,\n                                 steps_per_epoch=5,\n                                 callbacks=[checkpointer])\n\n    def test_any_scheduler_checkpoints(self):\n        lr_scheduler = ExponentialLR(gamma=0.01)\n        checkpointer = LRSchedulerCheckpoint(lr_scheduler, self.checkpoint_filename, period=1)\n        self._test_checkpointer(checkpointer, lr_scheduler)\n\n    def test_reduce_lr_checkpoints(self):\n        reduce_lr = ReduceLROnPlateau(monitor='loss', patience=3)\n        checkpointer = LRSchedulerCheckpoint(reduce_lr, self.checkpoint_filename, period=1)\n        self._test_checkpointer(checkpointer, reduce_lr)\n\n    def _test_checkpointer(self, checkpointer, lr_scheduler):\n        scheduler_states = {}\n        generator = some_data_generator(OptimizerCheckpointTest.batch_size)\n\n        checkpointer.set_params({'epochs': OptimizerCheckpointTest.epochs, 'steps': 1})\n        checkpointer.set_model(self.model)\n        checkpointer.on_train_begin({})\n        for epoch in range(1, OptimizerCheckpointTest.epochs + 1):\n            checkpointer.on_epoch_begin(epoch, {})\n            checkpointer.on_train_batch_begin(1, {})\n            loss = self._update_model(generator)\n            checkpointer.on_train_batch_end(1, {'batch': 1, 'size': OptimizerCheckpointTest.batch_size, 'loss': loss})\n            checkpointer.on_epoch_end(epoch, {'epoch': epoch, 'loss': loss, 'val_loss': 1})\n            filename = self.checkpoint_filename.format(epoch=epoch)\n            self.assertTrue(os.path.isfile(filename))\n            scheduler_states[epoch] = torch_to_numpy(lr_scheduler.scheduler.state_dict(), copy=True)\n        checkpointer.on_train_end({})\n\n        self._test_checkpoint(scheduler_states, lr_scheduler)\n\n    def _update_model(self, generator):\n        self.pytorch_network.zero_grad()\n\n        x, y = next(generator)\n        pred_y = self.pytorch_network(x)\n        loss = self.loss_function(pred_y, y)\n        loss.backward()\n\n        self.optimizer.step()\n\n        return float(loss)\n\n    def _test_checkpoint(self, scheduler_states, lr_scheduler):\n        for epoch, epoch_scheduler_state in scheduler_states.items():\n            filename = self.checkpoint_filename.format(epoch=epoch)\n            lr_scheduler.load_state(filename)\n            saved_scheduler_state = torch_to_numpy(lr_scheduler.scheduler.state_dict())\n\n            self.assertEqual(epoch_scheduler_state, saved_scheduler_state)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/framework/callbacks/test_optimizer_checkpoint.py,5,"b""import os\n\nimport unittest\nfrom unittest import TestCase\nfrom tempfile import TemporaryDirectory\n\nimport torch\nimport torch.nn as nn\n\nfrom poutyne import torch_to_numpy\nfrom poutyne.framework import Model\nfrom poutyne.framework.callbacks import OptimizerCheckpoint\n\n\ndef some_data_generator(batch_size):\n    while True:\n        x = torch.rand(batch_size, 1)\n        y = torch.rand(batch_size, 1)\n        yield x, y\n\n\nclass OptimizerCheckpointTest(TestCase):\n    batch_size = 20\n    epochs = 10\n\n    def setUp(self):\n        torch.manual_seed(42)\n        self.pytorch_network = nn.Linear(1, 1)\n        self.loss_function = nn.MSELoss()\n        self.optimizer = torch.optim.Adam(self.pytorch_network.parameters(), lr=1e-3)\n        self.model = Model(self.pytorch_network, self.optimizer, self.loss_function)\n        self.temp_dir_obj = TemporaryDirectory()\n        self.checkpoint_filename = os.path.join(self.temp_dir_obj.name, 'my_checkpoint_{epoch}.optim')\n\n    def tearDown(self):\n        self.temp_dir_obj.cleanup()\n\n    def test_integration(self):\n        train_gen = some_data_generator(OptimizerCheckpointTest.batch_size)\n        valid_gen = some_data_generator(OptimizerCheckpointTest.batch_size)\n        checkpointer = OptimizerCheckpoint(self.checkpoint_filename, period=1)\n        self.model.fit_generator(train_gen,\n                                 valid_gen,\n                                 epochs=OptimizerCheckpointTest.epochs,\n                                 steps_per_epoch=5,\n                                 callbacks=[checkpointer])\n\n    def test_checkpoints(self):\n        checkpointer = OptimizerCheckpoint(self.checkpoint_filename, period=1)\n        self._test_checkpointer(checkpointer)\n\n    def _test_checkpointer(self, checkpointer):\n        optimizer_states = {}\n        generator = some_data_generator(OptimizerCheckpointTest.batch_size)\n\n        checkpointer.set_params({'epochs': OptimizerCheckpointTest.epochs, 'steps': 1})\n        checkpointer.set_model(self.model)\n        checkpointer.on_train_begin({})\n        for epoch in range(1, OptimizerCheckpointTest.epochs + 1):\n            checkpointer.on_epoch_begin(epoch, {})\n            checkpointer.on_train_batch_begin(1, {})\n            loss = self._update_model(generator)\n            checkpointer.on_train_batch_end(1, {'batch': 1, 'size': OptimizerCheckpointTest.batch_size, 'loss': loss})\n            checkpointer.on_epoch_end(epoch, {'epoch': epoch, 'loss': loss, 'val_loss': 1})\n            filename = self.checkpoint_filename.format(epoch=epoch)\n            self.assertTrue(os.path.isfile(filename))\n            optimizer_states[epoch] = torch_to_numpy(self.optimizer.state_dict(), copy=True)\n        checkpointer.on_train_end({})\n\n        self._test_checkpoint(optimizer_states)\n\n    def _update_model(self, generator):\n        self.pytorch_network.zero_grad()\n\n        x, y = next(generator)\n        pred_y = self.pytorch_network(x)\n        loss = self.loss_function(pred_y, y)\n        loss.backward()\n\n        self.optimizer.step()\n\n        return float(loss)\n\n    def _test_checkpoint(self, optimizer_states):\n        for epoch, epoch_optimizer_state in optimizer_states.items():\n            filename = self.checkpoint_filename.format(epoch=epoch)\n            self.model.load_optimizer_state(filename)\n            saved_optimizer_state = torch_to_numpy(self.optimizer.state_dict())\n\n            self.assertEqual(epoch_optimizer_state, saved_optimizer_state)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/framework/callbacks/test_policies.py,0,"b'import unittest\n\nfrom poutyne.framework.callbacks.policies import linspace, cosinespace\nfrom poutyne.framework.callbacks.policies import Phase, OptimizerPolicy\nfrom poutyne.framework.callbacks.policies import one_cycle_phases, sgdr_phases\n\n\nclass TestSpaces(unittest.TestCase):\n    def assert_space(self, space, expected):\n        for val, exp in zip(space, expected):\n            self.assertAlmostEqual(val, exp, places=3)\n\n    def test_linspace_const(self):\n        self.assert_space(linspace(0, 0, 3), [0, 0, 0])\n\n    def test_linspace_increasing(self):\n        self.assert_space(linspace(0, 1, 3), [0, .5, 1])\n\n    def test_linspace_decreasing(self):\n        self.assert_space(linspace(1, 0, 3), [1, .5, 0])\n\n    def test_linspace_with_many_values(self):\n        self.assert_space(linspace(0, 1, 6), [0, .2, .4, .6, .8, 1])\n\n    def test_cosinespace_const(self):\n        self.assert_space(cosinespace(0, 0, 2), [0, 0])\n\n    def test_cosinespace_increasing(self):\n        self.assert_space(cosinespace(0, 1, 2), [0, 1])\n        self.assert_space(cosinespace(0, 1, 3), [0, .5, 1])\n\n    def test_cosinespace_decreasing(self):\n        self.assert_space(cosinespace(1, 0, 2), [1, 0])\n        self.assert_space(cosinespace(1, 0, 3), [1, .5, 0])\n\n    def test_cosinespace_with_many_values(self):\n        self.assert_space(cosinespace(0, 1, 5), [0, .1464, .5, .8535, 1])\n        self.assert_space(cosinespace(1, 0, 5), [1, .8535, .5, .1464, 0])\n\n    def test_space_has_desired_legth(self):\n        for space_fn in [linspace, cosinespace]:\n            space = list(space_fn(2, 1, 3))\n            assert len(space) == 3\n            with self.assertRaises(IndexError):\n                space[4]  # pylint: disable=pointless-statement\n\n\nclass TestPhase(unittest.TestCase):\n    def test_init_raises_without_lr_or_momentum(self):\n        with self.assertRaises(ValueError):\n            Phase(lr=None, momentum=None)\n        with self.assertRaises(ValueError):\n            Phase()\n\n    def test_phase_with_only_one_parameter_set(self):\n        for param_name in [""lr"", ""momentum""]:\n            steps = 3\n            phase = Phase(**{param_name: linspace(1, 0, steps)})\n            for params in phase:\n                self.assertIsInstance(params, dict)\n                self.assertTrue(param_name in params)\n                self.assertEqual(len(params), 1)\n                self.assertTrue(0 <= params[param_name] <= 1)\n\n    def test_phase_with_two_parameters(self):\n        steps = 4\n        phase = Phase(lr=linspace(1, 0, steps), momentum=cosinespace(.8, 1, steps))\n        self.assertEqual(len(list(phase)), steps)\n        for params in phase:\n            self.assertEqual(len(params), 2)\n\n            self.assertTrue(""lr"" in params)\n            self.assertTrue(0 <= params[""lr""] <= 1)\n\n            self.assertTrue(""momentum"" in params)\n            self.assertTrue(.8 <= params[""momentum""] <= 1)\n\n\nclass TestOptimizerPolicy(unittest.TestCase):\n    def setUp(self):\n        steps = 2\n        phases = [Phase(lr=linspace(1, 1, steps)), Phase(lr=linspace(0, 0, steps))]\n        self.policy = OptimizerPolicy(phases)\n\n    def test_basic_iteration(self):\n        policy_iter = iter(self.policy)\n        self.assertEqual(next(policy_iter), {""lr"": 1})\n        self.assertEqual(next(policy_iter), {""lr"": 1})\n\n        self.assertEqual(next(policy_iter), {""lr"": 0})\n        self.assertEqual(next(policy_iter), {""lr"": 0})\n\n        with self.assertRaises(StopIteration):\n            next(policy_iter)\n\n\nclass TestOptimizerPolicyRestart(unittest.TestCase):\n    def setUp(self):\n        steps = 2\n        phases = [\n            Phase(lr=linspace(0, 0, steps)),\n            Phase(lr=linspace(1, 1, steps)),\n            Phase(lr=linspace(2, 2, steps)),\n        ]\n        self.policy = OptimizerPolicy(phases=phases, initial_step=3)\n\n    def test_starts_at_correct_position(self):\n        policy_iter = iter(self.policy)\n        # The first three steps are ignored\n        # assert next(policy_iter) == {""lr"": 0}\n        # assert next(policy_iter) == {""lr"": 0}\n        # assert next(policy_iter) == {""lr"": 1}\n        assert next(policy_iter) == {""lr"": 1}\n        assert next(policy_iter) == {""lr"": 2}\n        assert next(policy_iter) == {""lr"": 2}\n\n        with self.assertRaises(StopIteration):\n            next(policy_iter)\n\n\nclass TestOneCycle(unittest.TestCase):\n    def test_length(self):\n        policy = OptimizerPolicy(one_cycle_phases(100))\n        self.assertEqual(len(list(policy.all_steps())), 100)\n\n        policy = OptimizerPolicy(one_cycle_phases(99))\n        self.assertEqual(len(list(policy.all_steps())), 99)\n\n\nclass TestSGDR(unittest.TestCase):\n    def test_length_with_cycle_mult_one(self):\n        policy = OptimizerPolicy(sgdr_phases(10, 1, cycle_mult=1))\n        self.assertEqual(len(list(policy.all_steps())), 10)\n\n        policy = OptimizerPolicy(sgdr_phases(10, 2, cycle_mult=1))\n        self.assertEqual(len(list(policy.all_steps())), 20)\n\n        policy = OptimizerPolicy(sgdr_phases(10, 10, cycle_mult=1))\n        self.assertEqual(len(list(policy.all_steps())), 100)\n\n    def test_length_with_higher_cycle_mult(self):\n        policy = OptimizerPolicy(sgdr_phases(10, 1, cycle_mult=2))\n        self.assertEqual(len(list(policy.all_steps())), 10)\n\n        policy = OptimizerPolicy(sgdr_phases(10, 2, cycle_mult=2))\n        self.assertEqual(len(list(policy.all_steps())), 30)\n\n        policy = OptimizerPolicy(sgdr_phases(10, 3, cycle_mult=2))\n        self.assertEqual(len(list(policy.all_steps())), 70)\n\n        policy = OptimizerPolicy(sgdr_phases(10, 1, cycle_mult=3))\n        self.assertEqual(len(list(policy.all_steps())), 10)\n\n        policy = OptimizerPolicy(sgdr_phases(10, 2, cycle_mult=3))\n        self.assertEqual(len(list(policy.all_steps())), 40)\n'"
tests/framework/callbacks/test_tracker.py,12,"b'import math\nfrom tempfile import TemporaryDirectory\nfrom unittest import TestCase, skipIf\nfrom unittest.mock import MagicMock, call, ANY\n\nimport torch\nimport torch.nn as nn\n\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept ImportError:\n    SummaryWriter = None\n\nfrom poutyne.framework import Model, TensorBoardGradientTracker, WeightsGradientsStatsTracker, Tracker\n\n\nclass TrackerTest(TestCase):\n    def test_keep_good_layer(self):\n        # pylint: disable=protected-access\n        tracker = Tracker(keep_bias=False)\n        layer_to_keep_params = MagicMock()\n        layer_to_keep_params.requires_grad = True\n        self.assertTrue(tracker._keep_layer(layer_to_keep_params, ""fake_layer_name_to_keep""))\n        self.assertFalse(tracker._keep_layer(layer_to_keep_params, ""bias_name_not_to_keep""))\n\n        layer_not_to_keep_params = MagicMock()\n        layer_not_to_keep_params.requires_grad = False\n        self.assertFalse(tracker._keep_layer(layer_not_to_keep_params, ""fake_layer_name_not_to_keep""))\n        self.assertFalse(tracker._keep_layer(layer_not_to_keep_params, ""bias_name_not_to_keep""))\n\n        tracker = Tracker(keep_bias=True)\n        layer_to_keep_params = MagicMock()\n        layer_to_keep_params.requires_grad = True\n        self.assertTrue(tracker._keep_layer(layer_to_keep_params, ""fake_layer_name_to_keep""))\n        self.assertTrue(tracker._keep_layer(layer_to_keep_params, ""bias_name_to_keep""))\n\n        layer_not_to_keep_params = MagicMock()\n        layer_not_to_keep_params.requires_grad = False\n        self.assertFalse(tracker._keep_layer(layer_not_to_keep_params, ""fake_layer_name_not_to_keep""))\n        self.assertFalse(tracker._keep_layer(layer_not_to_keep_params, ""bias_name_not_to_keep""))\n\n\nclass GradientStatsTrackerTest(TestCase):\n    def setUp(self):\n        self.tracker = WeightsGradientsStatsTracker(number_layers=2)\n\n        self.absolute_min_both_layer = 0.00\n        self.layer_1_min = -0.15\n        self.layer_1_max = 0.24\n        self.layer_1_gradients = torch.Tensor([[self.layer_1_max], [self.absolute_min_both_layer], [self.layer_1_min]])\n\n        self.layer_2_min = -0.25\n        self.layer_2_max = 0.16\n        self.layer_2_gradients = torch.Tensor([[self.layer_2_max], [self.layer_2_min], [self.absolute_min_both_layer]])\n\n        self.layer_1_name = ""fake_name_1""\n        self.layer_2_name = ""fake_name_2""\n        self.layer_names = [self.layer_1_name, self.layer_2_name]\n\n        # The value have been compute manual according to the Welford\'s online algorithm\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford\'s_online_algorithm\n        # We refer to the WelfordCompute document in tests/framework/callbacks for the expected value computation.\n        mean_1 = 0.13\n        mean_2 = 0.1366666\n        s2_1 = 0\n        s2_2 = 0\n        self.first_batch_expected_stats = {\n            self.layer_1_name: {\n                ""mean"": mean_1,\n                ""mean_std_dev_up"": mean_1 + math.sqrt(s2_1),\n                ""mean_std_dev_down"": mean_1 - math.sqrt(s2_1),\n                ""min"": self.layer_1_min,\n                ""abs_min"": self.absolute_min_both_layer,\n                ""max"": self.layer_1_max,\n                ""abs_max"": self.layer_1_max\n            },\n            self.layer_2_name: {\n                ""mean"": mean_2,\n                ""mean_std_dev_up"": mean_2 + math.sqrt(s2_2),\n                ""mean_std_dev_down"": mean_2 - math.sqrt(s2_2),\n                ""min"": self.layer_2_min,\n                ""abs_min"": self.absolute_min_both_layer,\n                ""max"": self.layer_2_max,\n                ""abs_max"": math.fabs(self.layer_2_min)  # since the absolute value is higher than the normal max\n            }\n        }\n\n        mean_1 = 0.195\n        mean_2 = 0.205\n        s2_1 = 0.00845\n        s2_2 = 0.0093889\n        # *2 for min and max since the weights are 3 times the layers gradients\n        self.second_batch_expected_stats = {\n            self.layer_1_name: {\n                ""mean"": mean_1,\n                ""mean_std_dev_up"": mean_1 + math.sqrt(s2_1),\n                ""mean_std_dev_down"": mean_1 - math.sqrt(s2_1),\n                ""min"": self.layer_1_min * 2,\n                ""abs_min"": self.absolute_min_both_layer,\n                ""max"": self.layer_1_max * 2,\n                ""abs_max"": self.layer_1_max * 2\n            },\n            self.layer_2_name: {\n                ""mean"": mean_2,\n                ""mean_std_dev_up"": mean_2 + math.sqrt(s2_2),\n                ""mean_std_dev_down"": mean_2 - math.sqrt(s2_2),\n                ""min"": self.layer_2_min * 2,\n                ""abs_min"": self.absolute_min_both_layer,\n                ""max"": self.layer_2_max * 2,\n                ""abs_max"": math.fabs(self.layer_2_min * 2)  # since the absolute value is higher than the normal max\n            }\n        }\n\n        mean_1 = 0.26\n        mean_2 = 0.27333333333\n        s2_1 = 0.0169\n        s2_2 = 0.018677778\n        # *3 for min and max since the weights are 3 times the layers gradients\n        self.third_batch_expected_stats = {\n            self.layer_1_name: {\n                ""mean"": mean_1,\n                ""mean_std_dev_up"": mean_1 + math.sqrt(s2_1),\n                ""mean_std_dev_down"": mean_1 - math.sqrt(s2_1),\n                ""min"": self.layer_1_min * 3,\n                ""abs_min"": self.absolute_min_both_layer,\n                ""max"": self.layer_1_max * 3,\n                ""abs_max"": self.layer_1_max * 3\n            },\n            self.layer_2_name: {\n                ""mean"": mean_2,\n                ""mean_std_dev_up"": mean_2 + math.sqrt(s2_2),\n                ""mean_std_dev_down"": mean_2 - math.sqrt(s2_2),\n                ""min"": self.layer_2_min * 3,\n                ""abs_min"": self.absolute_min_both_layer,\n                ""max"": self.layer_2_max * 3,\n                ""abs_max"": math.fabs(self.layer_2_min * 3)  # since the absolute value is higher than the normal max\n            }\n        }\n\n    def test_compute_two_layers_statistic(self):\n        self._run_n_batch(num_batch=1)\n        self._test_batch_stats(self.first_batch_expected_stats)\n\n        self._run_n_batch(num_batch=2)\n        self._test_batch_stats(self.second_batch_expected_stats)\n\n        self._run_n_batch(num_batch=3)\n        self._test_batch_stats(self.third_batch_expected_stats)\n\n    def _run_n_batch(self, num_batch):\n        for batch_number in range(1, num_batch + 1):\n            layer_1_params = MagicMock()\n            layer_1_params.grad = self.layer_1_gradients * batch_number\n            layer_2_params = MagicMock()\n            layer_2_params.grad = self.layer_2_gradients * batch_number\n\n            named_parameters = ((n, p)\n                                for n, p in [(self.layer_1_name, layer_1_params), (self.layer_2_name, layer_2_params)])\n\n            self.tracker.batch_statistic_upgrade(named_parameters=named_parameters)\n\n    def _test_batch_stats(self, batch_expected):\n        batch_actual_stats = self.tracker.get_stats(self.layer_names)\n\n        self._test_stats(batch_expected[self.layer_1_name], batch_actual_stats[self.layer_1_name])\n\n        self._test_stats(batch_expected[self.layer_2_name], batch_actual_stats[self.layer_2_name])\n\n    def _test_stats(self, expected, actual):\n        self.assertEqual(len(expected), len(actual))\n        self.assertEqual(expected.keys(), actual.keys())\n        for expected_value, actual_value in zip(expected.values(), actual.values()):\n            self.assertAlmostEqual(float(expected_value), float(actual_value), places=3)\n\n\ndef some_data_generator(batch_size):\n    while True:\n        x = torch.rand(batch_size, 1)\n        y = torch.rand(batch_size, 1)\n        yield x, y\n\n\n@skipIf(SummaryWriter is None, ""Unable to import SummaryWriter from torch"")\nclass TensorBoardGradientTrackerTest(TestCase):\n    batch_size = 20\n    lr = 1e-3\n    num_epochs = 10\n\n    def setUp(self):\n        torch.manual_seed(42)\n        self.loss_function = nn.MSELoss()\n        self.temp_dir_obj = TemporaryDirectory()\n        # pylint: disable=not-callable\n        self.writer = SummaryWriter(self.temp_dir_obj.name)\n        self.writer.add_scalars = MagicMock()\n\n    def tearDown(self):\n        self.temp_dir_obj.cleanup()\n\n    def test_tracking_one_layer_model(self):\n        self.num_layer = 1\n        self.pytorch_network = nn.Linear(1, 1)\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=self.lr)\n        self.model = Model(self.pytorch_network, self.optimizer, self.loss_function)\n\n        keep_bias = False\n        train_gen = some_data_generator(20)\n        valid_gen = some_data_generator(20)\n        tracker = TensorBoardGradientTracker(self.writer, keep_bias=keep_bias)\n        self.model.fit_generator(train_gen, valid_gen, epochs=self.num_epochs, steps_per_epoch=5, callbacks=[tracker])\n        self._test_tracking(keep_bias)\n\n    def test_tracking_one_layer_model_with_bias(self):\n        self.num_layer = 1\n        self.pytorch_network = nn.Linear(1, 1)\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=self.lr)\n        self.model = Model(self.pytorch_network, self.optimizer, self.loss_function)\n\n        keep_bias = True\n        train_gen = some_data_generator(20)\n        valid_gen = some_data_generator(20)\n        tracker = TensorBoardGradientTracker(self.writer, keep_bias=keep_bias)\n        self.model.fit_generator(train_gen, valid_gen, epochs=self.num_epochs, steps_per_epoch=5, callbacks=[tracker])\n        self._test_tracking(keep_bias)\n\n    def test_tracking_two_layers_model(self):\n        self.num_layer = 2\n        self.pytorch_network = nn.Sequential(nn.Linear(1, 1), nn.Linear(1, 1))\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=self.lr)\n        self.model = Model(self.pytorch_network, self.optimizer, self.loss_function)\n\n        keep_bias = False\n        train_gen = some_data_generator(20)\n        valid_gen = some_data_generator(20)\n        tracker = TensorBoardGradientTracker(self.writer, keep_bias=keep_bias)\n        self.model.fit_generator(train_gen, valid_gen, epochs=self.num_epochs, steps_per_epoch=5, callbacks=[tracker])\n        self._test_tracking(keep_bias)\n\n    def test_tracking_two_layers_shallow_model(self):\n        self.num_layer = 2\n        self.pytorch_network = nn.Sequential(nn.Linear(1, 4), nn.Linear(4, 1))\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=self.lr)\n        self.model = Model(self.pytorch_network, self.optimizer, self.loss_function)\n\n        keep_bias = False\n        train_gen = some_data_generator(20)\n        valid_gen = some_data_generator(20)\n        tracker = TensorBoardGradientTracker(self.writer, keep_bias=keep_bias)\n        self.model.fit_generator(train_gen, valid_gen, epochs=self.num_epochs, steps_per_epoch=5, callbacks=[tracker])\n        self._test_tracking(keep_bias)\n\n    def test_tracking_N_layers_model_with_bias(self):\n        self.num_layer = 4\n        self.pytorch_network = nn.Sequential(nn.Linear(1, 1), nn.Linear(1, 1), nn.Linear(1, 1), nn.Linear(1, 1))\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=self.lr)\n        self.model = Model(self.pytorch_network, self.optimizer, self.loss_function)\n\n        keep_bias = True\n        train_gen = some_data_generator(20)\n        valid_gen = some_data_generator(20)\n        tracker = TensorBoardGradientTracker(self.writer, keep_bias=keep_bias)\n        self.model.fit_generator(train_gen, valid_gen, epochs=self.num_epochs, steps_per_epoch=5, callbacks=[tracker])\n        self._test_tracking(keep_bias)\n\n    def _test_tracking(self, keep_bias):\n        expected_calls = []\n        for epoch in range(1, self.num_epochs + 1):\n            layer_names = [""""]\n            if self.num_layer > 1:\n                layer_names = []\n                for layer_idx in range(self.num_layer):\n                    layer_names.append(""{}."".format(layer_idx))\n            for layer_name in layer_names:\n                expected_calls.append(call(\'gradient_distributions/{}weight\'.format(layer_name), {\'mean\': ANY}, epoch))\n                expected_calls.append(\n                    call(\'gradient_distributions/{}weight\'.format(layer_name), {\'mean_std_dev_up\': ANY}, epoch))\n                expected_calls.append(\n                    call(\'gradient_distributions/{}weight\'.format(layer_name), {\'mean_std_dev_down\': ANY}, epoch))\n                expected_calls.append(call(\'other_gradient_stats/{}weight\'.format(layer_name), {\'min\': ANY}, epoch))\n                expected_calls.append(call(\'other_gradient_stats/{}weight\'.format(layer_name), {\'max\': ANY}, epoch))\n\n                if keep_bias:\n                    expected_calls.append(call(\'gradient_distributions/{}bias\'.format(layer_name), {\'mean\': ANY},\n                                               epoch))\n                    expected_calls.append(\n                        call(\'gradient_distributions/{}bias\'.format(layer_name), {\'mean_std_dev_up\': ANY}, epoch))\n                    expected_calls.append(\n                        call(\'gradient_distributions/{}bias\'.format(layer_name), {\'mean_std_dev_down\': ANY}, epoch))\n                    expected_calls.append(call(\'other_gradient_stats/{}bias\'.format(layer_name), {\'min\': ANY}, epoch))\n                    expected_calls.append(call(\'other_gradient_stats/{}bias\'.format(layer_name), {\'max\': ANY}, epoch))\n\n        method_calls = self.writer.add_scalars.mock_calls\n        self.assertEqual(len(method_calls), len(expected_calls))\n        self.assertEqual(method_calls, expected_calls)\n\n        self.assertIn(expected_calls, method_calls)\n'"
tests/framework/metrics/__init__.py,0,b''
tests/framework/metrics/test_fscores.py,6,"b'""""""\nThe source code of this file was copied from the AllenNLP project, and has been modified.\n\nCopyright 2019 AllenNLP\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\n# pylint: disable=protected-access\nfrom unittest import TestCase\n\nimport numpy\nimport torch\n\nfrom poutyne.framework.metrics import FBeta\n\n\nclass FBetaTest(TestCase):\n    def setUp(self):\n        # [0, 1, 1, 1, 3, 1]\n        self.predictions = torch.Tensor([[0.35, 0.25, 0.1, 0.1, 0.2], [0.1, 0.6, 0.1, 0.2, 0.0],\n                                         [0.1, 0.6, 0.1, 0.2, 0.0], [0.1, 0.5, 0.1, 0.2, 0.0],\n                                         [0.1, 0.2, 0.1, 0.7, 0.0], [0.1, 0.6, 0.1, 0.2, 0.0]])\n        self.targets = torch.Tensor([0, 4, 1, 0, 3, 0])\n\n        # detailed target state\n        self.pred_sum = [1, 4, 0, 1, 0]\n        self.true_sum = [3, 1, 0, 1, 1]\n        self.true_positive_sum = [1, 1, 0, 1, 0]\n        self.true_negative_sum = [3, 2, 6, 5, 5]\n        self.total_sum = [6, 6, 6, 6, 6]\n\n        desired_precisions = [1.00, 0.25, 0.00, 1.00, 0.00]\n        desired_recalls = [0.33, 1.00, 0.00, 1.00, 0.00]\n        desired_fscores = [(2 * p * r) / (p + r) if p + r != 0.0 else 0.0\n                           for p, r in zip(desired_precisions, desired_recalls)]\n        self.desired_precisions = desired_precisions\n        self.desired_recalls = desired_recalls\n        self.desired_fscores = desired_fscores\n\n    def test_config_errors(self):\n        # Bad beta\n        self.assertRaises(ValueError, FBeta, beta=0.0)\n\n        # Bad average option\n        self.assertRaises(ValueError, FBeta, average=\'mega\')\n\n    def test_runtime_errors(self):\n        fbeta = FBeta()\n        # Metric was never called.\n        self.assertRaises(RuntimeError, fbeta.get_metric)\n\n    def test_fbeta_multiclass_state(self):\n        fbeta = FBeta()\n        fbeta(self.predictions, self.targets)\n\n        # check state\n        numpy.testing.assert_almost_equal(fbeta._pred_sum.tolist(), self.pred_sum)\n        numpy.testing.assert_almost_equal(fbeta._true_sum.tolist(), self.true_sum)\n        numpy.testing.assert_almost_equal(fbeta._true_positive_sum.tolist(), self.true_positive_sum)\n        numpy.testing.assert_almost_equal(fbeta._total_sum.tolist(), self.total_sum)\n\n    def test_fbeta_multiclass_with_mask(self):\n        mask = torch.Tensor([1, 1, 1, 1, 1, 0])\n\n        fbeta = FBeta()\n        fbeta(self.predictions, (self.targets, mask))\n\n        numpy.testing.assert_almost_equal(fbeta._pred_sum.tolist(), [1, 3, 0, 1, 0])\n        numpy.testing.assert_almost_equal(fbeta._true_sum.tolist(), [2, 1, 0, 1, 1])\n        numpy.testing.assert_almost_equal(fbeta._true_positive_sum.tolist(), [1, 1, 0, 1, 0])\n\n    def test_fbeta_multiclass_macro_average_metric(self):\n        precision = self._compute(metric=\'precision\', average=\'macro\')\n        recall = self._compute(metric=\'recall\', average=\'macro\')\n        fscore = self._compute(metric=\'fscore\', average=\'macro\')\n\n        macro_precision = numpy.mean(self.desired_precisions)\n        macro_recall = numpy.mean(self.desired_recalls)\n        macro_fscore = numpy.mean(self.desired_fscores)\n\n        # check type\n        assert isinstance(precision, float)\n        assert isinstance(recall, float)\n        assert isinstance(fscore, float)\n\n        # check value\n        numpy.testing.assert_almost_equal(precision, macro_precision, decimal=2)\n        numpy.testing.assert_almost_equal(recall, macro_recall, decimal=2)\n        numpy.testing.assert_almost_equal(fscore, macro_fscore, decimal=2)\n\n    def test_fbeta_multiclass_micro_average_metric(self):\n        precision = self._compute(metric=\'precision\', average=\'micro\')\n        recall = self._compute(metric=\'recall\', average=\'micro\')\n        fscore = self._compute(metric=\'fscore\', average=\'micro\')\n\n        true_positives = [1, 1, 0, 1, 0]\n        false_positives = [0, 3, 0, 0, 0]\n        false_negatives = [2, 0, 0, 0, 1]\n        mean_true_positive = numpy.mean(true_positives)\n        mean_false_positive = numpy.mean(false_positives)\n        mean_false_negative = numpy.mean(false_negatives)\n\n        micro_precision = mean_true_positive / (mean_true_positive + mean_false_positive)\n        micro_recall = mean_true_positive / (mean_true_positive + mean_false_negative)\n        micro_fscore = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall)\n        # check value\n        numpy.testing.assert_almost_equal(precision, micro_precision, decimal=2)\n        numpy.testing.assert_almost_equal(recall, micro_recall, decimal=2)\n        numpy.testing.assert_almost_equal(fscore, micro_fscore, decimal=2)\n\n    def test_fbeta_multiclass_with_explicit_label(self):\n        # same prediction but with and explicit label\n        label = 3\n        precision = self._compute(metric=\'precision\', average=label)\n        recall = self._compute(metric=\'recall\', average=label)\n        fscore = self._compute(metric=\'fscore\', average=label)\n\n        desired_precision = self.desired_precisions[label]\n        desired_recall = self.desired_recalls[label]\n        desired_fscore = self.desired_fscores[label]\n        # check value\n        numpy.testing.assert_almost_equal(precision, desired_precision, decimal=2)\n        numpy.testing.assert_almost_equal(recall, desired_recall, decimal=2)\n        numpy.testing.assert_almost_equal(fscore, desired_fscore, decimal=2)\n\n    def test_fbeta_multiclass_macro_average_metric_multireturn(self):\n        fbeta = FBeta(average=\'macro\')\n        fbeta(self.predictions, self.targets)\n        fscore, precision, recall = fbeta.get_metric()\n\n        macro_precision = numpy.mean(self.desired_precisions)\n        macro_recall = numpy.mean(self.desired_recalls)\n        macro_fscore = numpy.mean(self.desired_fscores)\n\n        # check type\n        assert isinstance(precision, float)\n        assert isinstance(recall, float)\n        assert isinstance(fscore, float)\n\n        # check value\n        numpy.testing.assert_almost_equal(precision, macro_precision, decimal=2)\n        numpy.testing.assert_almost_equal(recall, macro_recall, decimal=2)\n        numpy.testing.assert_almost_equal(fscore, macro_fscore, decimal=2)\n\n    def test_fbeta_handles_batch_size_of_one(self):\n        predictions = torch.Tensor([[0.2862, 0.3479, 0.1627, 0.2033]])\n        targets = torch.Tensor([1])\n        mask = torch.Tensor([1])\n\n        fbeta = FBeta()\n        fbeta(predictions, (targets, mask))\n\n        numpy.testing.assert_almost_equal(fbeta._pred_sum.tolist(), [0.0, 1.0, 0.0, 0.0])\n        numpy.testing.assert_almost_equal(fbeta._true_sum.tolist(), [0.0, 1.0, 0.0, 0.0])\n        numpy.testing.assert_almost_equal(fbeta._true_positive_sum.tolist(), [0.0, 1.0, 0.0, 0.0])\n        numpy.testing.assert_almost_equal(fbeta._total_sum.tolist(), [1.0, 1.0, 1.0, 1.0])\n\n    def _compute(self, *args, **kwargs):\n        fbeta = FBeta(*args, **kwargs)\n        fbeta(self.predictions, self.targets)\n        return fbeta.get_metric()\n\n    def test_names(self):\n        fbeta = FBeta(average=\'macro\')\n        self.assertEqual([\'fscore_macro\', \'precision_macro\', \'recall_macro\'], fbeta.__name__)\n        fbeta = FBeta(average=\'micro\')\n        self.assertEqual([\'fscore_micro\', \'precision_micro\', \'recall_micro\'], fbeta.__name__)\n        fbeta = FBeta(average=\'micro\', names=[\'f\', \'p\', \'r\'])\n        self.assertEqual([\'f\', \'p\', \'r\'], fbeta.__name__)\n        fbeta = FBeta(average=0)\n        self.assertEqual([\'fscore_0\', \'precision_0\', \'recall_0\'], fbeta.__name__)\n        fbeta = FBeta(metric=\'fscore\', average=\'macro\')\n        self.assertEqual(\'fscore_macro\', fbeta.__name__)\n        fbeta = FBeta(metric=\'fscore\', average=\'micro\')\n        self.assertEqual(\'fscore_micro\', fbeta.__name__)\n        fbeta = FBeta(metric=\'fscore\', average=0)\n        self.assertEqual(\'fscore_0\', fbeta.__name__)\n        fbeta = FBeta(metric=\'precision\', average=\'macro\')\n        self.assertEqual(\'precision_macro\', fbeta.__name__)\n        fbeta = FBeta(metric=\'precision\', average=\'micro\')\n        self.assertEqual(\'precision_micro\', fbeta.__name__)\n        fbeta = FBeta(metric=\'precision\', average=0)\n        self.assertEqual(\'precision_0\', fbeta.__name__)\n        fbeta = FBeta(metric=\'recall\', average=\'macro\')\n        self.assertEqual(\'recall_macro\', fbeta.__name__)\n        fbeta = FBeta(metric=\'recall\', average=\'micro\')\n        self.assertEqual(\'recall_micro\', fbeta.__name__)\n        fbeta = FBeta(metric=\'recall\', average=0)\n        self.assertEqual(\'recall_0\', fbeta.__name__)\n        fbeta = FBeta(metric=\'fscore\', average=\'macro\', names=\'f\')\n        self.assertEqual(\'f\', fbeta.__name__)\n        fbeta = FBeta(average=\'macro\', names=[\'f\', ""p"", ""r""])\n        self.assertEqual([""f"", ""p"", ""r""], fbeta.__name__)\n'"
tests/framework/metrics/test_metrics_model_integration.py,19,"b'# pylint: disable=unused-argument\n\n# Bug with PyTorch source code makes torch.tensor as not callable for pylint.\n# pylint: disable=not-callable\nimport os\nimport unittest\nfrom unittest import skipIf\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom poutyne.framework import Model\nfrom poutyne.framework.metrics import EpochMetric\nfrom poutyne.framework.metrics.utils import rename_doubles\n\n\nclass ConstEpochMetric(EpochMetric):\n    def __init__(self, value):\n        super().__init__()\n        self.value = value\n\n    def forward(self, y_pred, y_true):\n        pass\n\n    def get_metric(self):\n        return self.value\n\n    def reset(self):\n        pass\n\n\ndef get_batch_metric(value):\n    def some_metric_name(y_pred, y_true):\n        return torch.FloatTensor([value])\n\n    return some_metric_name\n\n\nclass SomeMetricName(ConstEpochMetric):\n    def get_metric(self):\n        return torch.FloatTensor([self.value])\n\n    def reset(self):\n        pass\n\n\ndef get_const_batch_metric(value):\n    def const_batch_metric(y_pred, y_true):\n        return value\n\n    return const_batch_metric\n\n\nclass MetricsModelIntegrationTest(unittest.TestCase):\n    epochs = 2\n    steps_per_epoch = 3\n    batch_size = 10\n\n    cuda_device = int(os.environ.get(\'CUDA_DEVICE\', 0))\n\n    def setUp(self):\n        torch.manual_seed(42)\n        self.pytorch_network = nn.Linear(1, 1)\n        self.loss_function = nn.MSELoss()\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=1e-3)\n        dataset_size = MetricsModelIntegrationTest.batch_size * MetricsModelIntegrationTest.steps_per_epoch\n        self.train_x = torch.rand(dataset_size, 1)\n        self.train_y = torch.rand(dataset_size, 1)\n        self.valid_x = torch.rand(dataset_size, 1)\n        self.valid_y = torch.rand(dataset_size, 1)\n\n        self.metric_names = (\'a\', \'b\')\n        self.metric_values = [1, 2]\n\n    def test_repeated_batch_metrics_handling(self):\n        expected_names = [\'some_metric_name1\', \'some_metric_name2\']\n        model = Model(self.pytorch_network,\n                      self.optimizer,\n                      self.loss_function,\n                      batch_metrics=[get_batch_metric(1), get_batch_metric(2)])\n        self._test_history(model, expected_names, [1, 2])\n\n    def test_repeated_epoch_metrics_handling(self):\n        expected_names = [\'some_metric_name1\', \'some_metric_name2\']\n        model = Model(self.pytorch_network,\n                      self.optimizer,\n                      self.loss_function,\n                      epoch_metrics=[SomeMetricName(1), SomeMetricName(2)])\n        self._test_history(model, expected_names, [1, 2])\n\n    def test_repeated_batch_epoch_metrics_handling(self):\n        expected_names = [\'some_metric_name1\', \'some_metric_name2\']\n        model = Model(self.pytorch_network,\n                      self.optimizer,\n                      self.loss_function,\n                      batch_metrics=[get_batch_metric(1)],\n                      epoch_metrics=[SomeMetricName(2)])\n        self._test_history(model, expected_names, [1, 2])\n\n    def test_batch_metrics_with_multiple_names_returned_by_dict(self):\n        d = dict(zip(self.metric_names, self.metric_values))\n        batch_metric = get_const_batch_metric(d)\n        model = Model(self.pytorch_network,\n                      self.optimizer,\n                      self.loss_function,\n                      batch_metrics=[(self.metric_names, batch_metric)])\n        self._test_history(model, d.keys(), d.values())\n\n    def test_epoch_metrics_with_multiple_names_returned_by_dict(self):\n        d = dict(zip(self.metric_names, self.metric_values))\n        epoch_metric = ConstEpochMetric(d)\n        model = Model(self.pytorch_network,\n                      self.optimizer,\n                      self.loss_function,\n                      epoch_metrics=[(self.metric_names, epoch_metric)])\n        self._test_history(model, d.keys(), d.values())\n\n    def test_batch_metrics_with_multiple_names_returned_by_tensor(self):\n        batch_metric = get_const_batch_metric(torch.tensor(self.metric_values))\n        model = Model(self.pytorch_network,\n                      self.optimizer,\n                      self.loss_function,\n                      batch_metrics=[(self.metric_names, batch_metric)])\n        self._test_history(model, self.metric_names, self.metric_values)\n\n    def test_epoch_metrics_with_multiple_names_returned_by_tensor(self):\n        epoch_metric = ConstEpochMetric(torch.tensor(self.metric_values))\n        model = Model(self.pytorch_network,\n                      self.optimizer,\n                      self.loss_function,\n                      epoch_metrics=[(self.metric_names, epoch_metric)])\n        self._test_history(model, self.metric_names, self.metric_values)\n\n    def test_epoch_metrics_with_name_with_multiple_names_returned_by_tensor(self):\n        class EpochMetricWithName(ConstEpochMetric):\n            __name__ = self.metric_names\n\n        epoch_metric = EpochMetricWithName(torch.tensor(self.metric_values))\n        model = Model(self.pytorch_network, self.optimizer, self.loss_function, epoch_metrics=[epoch_metric])\n        self._test_history(model, self.metric_names, self.metric_values)\n\n    @skipIf(not torch.cuda.is_available(), ""no gpu available"")\n    def test_batch_metrics_with_multiple_names_returned_by_tensor_on_gpu(self):\n        with torch.cuda.device(MetricsModelIntegrationTest.cuda_device):\n            batch_metric = get_const_batch_metric(torch.tensor(self.metric_values).cuda())\n            model = Model(self.pytorch_network,\n                          self.optimizer,\n                          self.loss_function,\n                          batch_metrics=[(self.metric_names, batch_metric)])\n            model.cuda()\n            self._test_history(model, self.metric_names, self.metric_values)\n\n    @skipIf(not torch.cuda.is_available(), ""no gpu available"")\n    def test_epoch_metrics_with_multiple_names_returned_by_tensor_on_gpu(self):\n        with torch.cuda.device(MetricsModelIntegrationTest.cuda_device):\n            epoch_metric = ConstEpochMetric(torch.tensor(self.metric_values).cuda())\n            model = Model(self.pytorch_network,\n                          self.optimizer,\n                          self.loss_function,\n                          epoch_metrics=[(self.metric_names, epoch_metric)])\n            model.cuda()\n            self._test_history(model, self.metric_names, self.metric_values)\n\n    def test_batch_metrics_with_multiple_names_returned_by_ndarray(self):\n        batch_metric = get_const_batch_metric(np.array(self.metric_values))\n        model = Model(self.pytorch_network,\n                      self.optimizer,\n                      self.loss_function,\n                      batch_metrics=[(self.metric_names, batch_metric)])\n        self._test_history(model, self.metric_names, self.metric_values)\n\n    def test_epoch_metrics_with_multiple_names_returned_by_ndarray(self):\n        epoch_metric = ConstEpochMetric(np.array(self.metric_values))\n        model = Model(self.pytorch_network,\n                      self.optimizer,\n                      self.loss_function,\n                      epoch_metrics=[(self.metric_names, epoch_metric)])\n        self._test_history(model, self.metric_names, self.metric_values)\n\n    def test_batch_metrics_with_multiple_names_returned_by_list(self):\n        batch_metric = get_const_batch_metric(list(self.metric_values))\n        model = Model(self.pytorch_network,\n                      self.optimizer,\n                      self.loss_function,\n                      batch_metrics=[(self.metric_names, batch_metric)])\n        self._test_history(model, self.metric_names, self.metric_values)\n\n    def test_epoch_metrics_with_multiple_names_returned_by_list(self):\n        epoch_metric = ConstEpochMetric(list(self.metric_values))\n        model = Model(self.pytorch_network,\n                      self.optimizer,\n                      self.loss_function,\n                      epoch_metrics=[(self.metric_names, epoch_metric)])\n        self._test_history(model, self.metric_names, self.metric_values)\n\n    def test_batch_metrics_with_multiple_names_returned_by_tuple(self):\n        batch_metric = get_const_batch_metric(tuple(self.metric_values))\n        model = Model(self.pytorch_network,\n                      self.optimizer,\n                      self.loss_function,\n                      batch_metrics=[(self.metric_names, batch_metric)])\n        self._test_history(model, self.metric_names, self.metric_values)\n\n    def test_epoch_metrics_with_multiple_names_returned_by_tuple(self):\n        epoch_metric = ConstEpochMetric(tuple(self.metric_values))\n        model = Model(self.pytorch_network,\n                      self.optimizer,\n                      self.loss_function,\n                      epoch_metrics=[(self.metric_names, epoch_metric)])\n        self._test_history(model, self.metric_names, self.metric_values)\n\n    def _test_history(self, model, names, values):\n        history = model.fit(self.train_x,\n                            self.train_y,\n                            validation_data=(self.valid_x, self.valid_y),\n                            batch_size=MetricsModelIntegrationTest.batch_size,\n                            epochs=MetricsModelIntegrationTest.epochs)\n        for logs in history:\n            for name, value in zip(names, values):\n                self.assertIn(name, logs)\n                self.assertEqual(value, logs[name])\n                self.assertIn(\'val_\' + name, logs)\n                self.assertEqual(value, logs[\'val_\' + name])\n\n\nclass MetricsRenamingTest(unittest.TestCase):\n    def test_batch_metrics(self):\n        actual = rename_doubles([\'a\', \'a\'], [])\n        expected = [\'a1\', \'a2\'], []\n        self.assertEqual(expected, actual)\n\n        actual = rename_doubles([\'a\', \'b\', \'a\', \'a\', \'c\', \'d\'], [])\n        expected = [\'a1\', \'b\', \'a2\', \'a3\', \'c\', \'d\'], []\n        self.assertEqual(expected, actual)\n\n    def test_epoch_metrics(self):\n        actual = rename_doubles([], [\'a\', \'a\'])\n        expected = [], [\'a1\', \'a2\']\n        self.assertEqual(expected, actual)\n\n        actual = rename_doubles([], [\'a\', \'b\', \'a\', \'a\', \'c\', \'d\'])\n        expected = [], [\'a1\', \'b\', \'a2\', \'a3\', \'c\', \'d\']\n        self.assertEqual(expected, actual)\n\n    def test_batch_epoch_metrics(self):\n        actual = rename_doubles([\'a\', \'b\', \'c\'], [\'d\', \'a\', \'e\', \'a\'])\n        expected = [\'a1\', \'b\', \'c\'], [\'d\', \'a2\', \'e\', \'a3\']\n        self.assertEqual(expected, actual)\n\n        actual = rename_doubles([\'a\', \'b\', \'c\', \'b\'], [\'d\', \'a\', \'e\', \'a\', \'e\'])\n        expected = [\'a1\', \'b1\', \'c\', \'b2\'], [\'d\', \'a2\', \'e1\', \'a3\', \'e2\']\n        self.assertEqual(expected, actual)\n\n        actual = rename_doubles([\'a\', \'b\', \'c\'], [\'d\', \'a\', \'e\', \'a\', \'e\'])\n        expected = [\'a1\', \'b\', \'c\'], [\'d\', \'a2\', \'e1\', \'a3\', \'e2\']\n        self.assertEqual(expected, actual)\n\n    def test_nested_batch_metrics(self):\n        actual = rename_doubles([[\'a\', \'a\']], [])\n        expected = [[\'a1\', \'a2\']], []\n        self.assertEqual(expected, actual)\n\n        actual = rename_doubles([[\'a\', \'b\'], \'b\'], [])\n        expected = [[\'a\', \'b1\'], \'b2\'], []\n        self.assertEqual(expected, actual)\n\n    def test_nested_epoch_metrics(self):\n        actual = rename_doubles([], [[\'a\', \'a\']])\n        expected = [], [[\'a1\', \'a2\']]\n        self.assertEqual(expected, actual)\n\n        actual = rename_doubles([], [[\'a\', \'b\'], \'b\'])\n        expected = [], [[\'a\', \'b1\'], \'b2\']\n        self.assertEqual(expected, actual)\n\n    def test_nested_batch_epoch_metrics(self):\n        actual = rename_doubles([[\'a\']], [[\'a\']])\n        expected = [[\'a1\']], [[\'a2\']]\n        self.assertEqual(expected, actual)\n\n        actual = rename_doubles([[\'a\', \'b\']], [[\'c\', \'a\']])\n        expected = [[\'a1\', \'b\']], [[\'c\', \'a2\']]\n        self.assertEqual(expected, actual)\n\n        actual = rename_doubles([\'a\', \'b\', [\'b\', \'a\'], \'c\', \'a\', \'d\'], [\'e\', [\'c\', \'a\'], \'f\', \'a\'])\n        expected = [\'a1\', \'b1\', [\'b2\', \'a2\'], \'c1\', \'a3\', \'d\'], [\'e\', [\'c2\', \'a4\'], \'f\', \'a5\']\n        self.assertEqual(expected, actual)\n'"
tests/framework/metrics/test_sklearn_metrics.py,8,"b'from unittest import TestCase, skipIf\nfrom itertools import repeat\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\ntry:\n    from sklearn.metrics import roc_auc_score, average_precision_score, r2_score\n    is_sklearn_available = True\nexcept ImportError:\n    is_sklearn_available = False\nfrom poutyne.framework.metrics import SKLearnMetrics\nfrom poutyne import TensorDataset\n\n\ndef gini(y_true, y_pred, sample_weight=None):\n    sort_indices = np.argsort(y_pred)\n    if sample_weight is not None:\n        sample_weight = sample_weight[sort_indices]\n    else:\n        sample_weight = np.ones_like(y_true)\n    y_true = y_true[sort_indices]\n    random = (sample_weight / sample_weight.sum()).cumsum()\n    lorentz = y_true.cumsum() / y_true.sum()\n    return 1 - ((random[1:] - random[:-1]) * (lorentz[1:] + lorentz[:-1])).sum()\n\n\n@skipIf(not is_sklearn_available, ""Scikit-learn is not available"")\nclass SKLearnMetricsTest(TestCase):\n    threshold = 0.7\n    steps_per_epoch = 5\n    batch_size = 20\n    num_examples = batch_size * steps_per_epoch\n\n    def setUp(self):\n        pred_noise = 0.1 * torch.randn((SKLearnMetricsTest.num_examples, 1))\n\n        self.regression_pred = torch.randn((SKLearnMetricsTest.num_examples, 1))\n        self.regression_true = self.regression_pred + pred_noise\n        self.classification_pred = self.regression_pred\n        self.classification_true = self.regression_true.sigmoid() > SKLearnMetricsTest.threshold\n        self.multiclass_classification_pred = torch.randn((SKLearnMetricsTest.num_examples, 3)).softmax(1)\n        self.multiclass_classification_true = self.multiclass_classification_pred.argmax(1)\n        self.multiclass_errors_indices = torch.where(torch.rand(SKLearnMetricsTest.num_examples) > 0.9)[0]\n        self.multiclass_classification_true[self.multiclass_errors_indices] = torch.randint(\n            3, (len(self.multiclass_errors_indices), ))\n        self.sample_weight = torch.rand((SKLearnMetricsTest.num_examples, 1))\n\n    def _get_data_loader(self, *tensors):\n        return DataLoader(TensorDataset(*tensors), batch_size=SKLearnMetricsTest.batch_size)\n\n    def test_classification(self):\n        self._test_classification(roc_auc_score, True)\n        self._test_classification(roc_auc_score, False)\n\n        self._test_classification(average_precision_score, True)\n        self._test_classification(average_precision_score, False)\n\n        two_skl_metrics = [roc_auc_score, average_precision_score]\n        self._test_classification(two_skl_metrics, True)\n        self._test_classification(two_skl_metrics, False)\n\n    def test_multiclass_classification_with_kwargs(self):\n        roc_auc_kwargs = dict(multi_class=\'ovr\', average=\'macro\')\n        self._test_multiclass_classification(roc_auc_score, True, kwargs=roc_auc_kwargs)\n        self._test_multiclass_classification(roc_auc_score, False, kwargs=roc_auc_kwargs)\n\n    def test_classification_with_custom_names(self):\n        roc_names = \'roc\'\n        self._test_classification(roc_auc_score, True, names=roc_names)\n        self._test_classification(roc_auc_score, False, names=roc_names)\n\n        ap_names = \'ap\'\n        self._test_classification(average_precision_score, True, names=ap_names)\n        self._test_classification(average_precision_score, False, names=ap_names)\n\n        two_names = [\'roc\', \'ap\']\n        two_skl_metrics = [roc_auc_score, average_precision_score]\n        self._test_classification(two_skl_metrics, True, names=two_names)\n        self._test_classification(two_skl_metrics, False, names=two_names)\n\n    def test_regression(self):\n        self._test_regression(r2_score, True)\n        self._test_regression(r2_score, False)\n\n        self._test_regression(gini, True)\n        self._test_regression(gini, False)\n\n        two_skl_metrics = [r2_score, gini]\n        self._test_regression(two_skl_metrics, True)\n        self._test_regression(two_skl_metrics, False)\n\n    def _test_classification(self, sklearn_metrics, with_sample_weight, *, kwargs=None, names=None):\n        return self._test_epoch_metric(self.classification_pred,\n                                       self.classification_true,\n                                       sklearn_metrics,\n                                       with_sample_weight,\n                                       kwargs=kwargs,\n                                       names=names)\n\n    def _test_multiclass_classification(self, sklearn_metrics, with_sample_weight, *, kwargs=None, names=None):\n        return self._test_epoch_metric(self.multiclass_classification_pred,\n                                       self.multiclass_classification_true,\n                                       sklearn_metrics,\n                                       with_sample_weight,\n                                       kwargs=kwargs,\n                                       names=names)\n\n    def _test_regression(self, sklearn_metrics, with_sample_weight, *, kwargs=None, names=None):\n        return self._test_epoch_metric(self.regression_pred,\n                                       self.regression_true,\n                                       sklearn_metrics,\n                                       with_sample_weight,\n                                       kwargs=kwargs,\n                                       names=names)\n\n    def _test_epoch_metric(self, pred, true, sklearn_metrics, with_sample_weight, *, kwargs=None, names=None):\n        epoch_metric = SKLearnMetrics(sklearn_metrics, kwargs=kwargs, names=names)\n\n        if with_sample_weight:\n            loader = self._get_data_loader(pred, (true, self.sample_weight))\n            sample_weight = self.sample_weight.numpy()\n        else:\n            loader = self._get_data_loader(pred, true)\n            sample_weight = None\n\n        true = true.numpy()\n        pred = pred.numpy()\n\n        if not isinstance(sklearn_metrics, list):\n            sklearn_metrics = [sklearn_metrics]\n\n        if kwargs is not None and not isinstance(kwargs, list):\n            kwargs = [kwargs]\n        kwargs = kwargs if kwargs is not None else repeat({})\n\n        if names is not None and not isinstance(names, list):\n            names = [names]\n        names = [func.__name__ for func in sklearn_metrics] if names is None else names\n\n        expected = {\n            name: f(true, pred, sample_weight=sample_weight, **kw)\n            for name, f, kw in zip(names, sklearn_metrics, kwargs)\n        }\n\n        with torch.no_grad():\n            for y_pred, y_true in loader:\n                epoch_metric(y_pred, y_true)\n        actual = epoch_metric.get_metric()\n        self.assertEqual(expected, actual)\n'"
tests/framework/model/__init__.py,0,b''
tests/framework/model/base.py,2,"b'# pylint: disable=too-many-locals\nimport os\nfrom unittest import TestCase\nfrom unittest.mock import MagicMock, call, ANY\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom poutyne.framework import Callback\n\n\nclass ModelFittingTestCase(TestCase):\n    epochs = 10\n    steps_per_epoch = 5\n    batch_size = 20\n\n    evaluate_dataset_len = 107\n\n    cuda_device = int(os.environ.get(\'CUDA_DEVICE\', 0))\n\n    def setUp(self):\n        self.mock_callback = MagicMock(spec=Callback)\n        self.batch_metrics = []\n        self.batch_metrics_names = []\n        self.batch_metrics_values = []\n        self.epoch_metrics = []\n        self.epoch_metrics_names = []\n        self.epoch_metrics_values = []\n        self.model = None\n\n    def _test_callbacks_train(self, params, logs, has_valid=True, steps=None):\n        # pylint: disable=too-many-arguments\n        if steps is None:\n            steps = params[\'steps\']\n        self.assertEqual(len(logs), params[\'epochs\'])\n        train_batch_dict = dict(zip(self.batch_metrics_names, self.batch_metrics_values), loss=ANY, time=ANY)\n        train_epochs_dict = dict(zip(self.epoch_metrics_names, self.epoch_metrics_values))\n        log_dict = {**train_batch_dict, **train_epochs_dict}\n        if has_valid:\n            val_batch_metrics_names = [\'val_\' + metric_name for metric_name in self.batch_metrics_names]\n            val_batch_dict = dict(zip(val_batch_metrics_names, self.batch_metrics_values), val_loss=ANY)\n            val_epoch_metrics_names = [\'val_\' + metric_name for metric_name in self.epoch_metrics_names]\n            val_epochs_dict = dict(zip(val_epoch_metrics_names, self.epoch_metrics_values))\n            log_dict.update({**val_batch_dict, **val_epochs_dict})\n\n        for epoch, log in enumerate(logs, 1):\n            self.assertEqual(log, dict(log_dict, epoch=epoch))\n\n        call_list = []\n        call_list.append(call.on_train_begin({}))\n        for epoch in range(1, params[\'epochs\'] + 1):\n            call_list.append(call.on_epoch_begin(epoch, {}))\n            for step in range(1, steps + 1):\n                call_list.append(call.on_train_batch_begin(step, {}))\n                call_list.append(call.on_backward_end(step))\n                call_list.append(call.on_train_batch_end(step, {\'batch\': step, \'size\': ANY, **train_batch_dict}))\n            call_list.append(call.on_epoch_end(epoch, {\'epoch\': epoch, **log_dict}))\n        call_list.append(call.on_train_end({}))\n\n        method_calls = self.mock_callback.method_calls\n        self.assertIn(call.set_model(self.model), method_calls[:2])  # skip set_model and set param call\n        self.assertIn(call.set_params(params), method_calls[:2])\n\n        self.assertEqual(len(method_calls), len(call_list) + 2)  # for set_model and set param\n        self.assertEqual(method_calls[2:], call_list)\n\n    def _test_callbacks_test(self, params, result_log):\n        test_batch_dict = dict(zip(self.batch_metrics_names, self.batch_metrics_values), loss=ANY, time=ANY)\n\n        call_list = []\n        call_list.append(call.on_test_begin({}))\n        for batch in range(1, params[\'batch\'] + 1):\n            call_list.append(call.on_test_batch_begin(batch, {}))\n            call_list.append(call.on_test_batch_end(batch, {\'batch\': batch, \'size\': ANY, **test_batch_dict}))\n        call_list.append(call.on_test_end(result_log))\n\n        method_calls = self.mock_callback.method_calls\n        self.assertEqual(call.set_model(self.model), method_calls[0])  # skip set_model\n\n        self.assertEqual(len(method_calls), len(call_list) + 1)  # for set_model\n        self.assertEqual(method_calls[1:], call_list)\n\n    def _test_size_and_type_for_generator(self, pred_y, expected_size):\n        if isinstance(pred_y, (list, tuple)):\n            for o in pred_y:\n                self._test_size_and_type_for_generator(o, expected_size)\n        elif isinstance(pred_y, dict):\n            for val in pred_y.values():\n                self._test_size_and_type_for_generator(val, expected_size)\n        else:\n            self.assertEqual(type(pred_y), np.ndarray)\n            self.assertEqual(pred_y.shape, expected_size)\n\n\nclass MultiIOModel(nn.Module):\n    """"""Model to test multiple inputs/outputs""""""\n\n    def __init__(self, num_input=2, num_output=2):\n        super(MultiIOModel, self).__init__()\n        inputs = []\n        for _ in range(num_input):\n            inputs.append(nn.Linear(1, 1))\n        self.inputs = nn.ModuleList(inputs)\n\n        outputs = []\n        for _ in range(num_output):\n            outputs.append(nn.Linear(num_input, 1))\n        self.outputs = nn.ModuleList(outputs)\n\n    def forward(self, *x):\n        inp_to_cat = []\n        for i, inp in enumerate(self.inputs):\n            inp_to_cat.append(inp(x[i]))\n        inp_cat = torch.cat(inp_to_cat, dim=1)\n\n        outputs = []\n        for out in self.outputs:\n            outputs.append(out(inp_cat))\n\n        outputs = outputs if len(outputs) > 1 else outputs[0]\n        return outputs\n'"
tests/framework/model/test_dict_output.py,10,"b'import numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom poutyne.framework import Model\n\nfrom .base import ModelFittingTestCase\n\n\nclass DictOutputModel(nn.Module):\n    """"""Model to test multiple dictionnary output""""""\n\n    def __init__(self):\n        super(DictOutputModel, self).__init__()\n        self.input = nn.Linear(1, 1)\n        self.output1 = nn.Linear(1, 1)\n        self.output2 = nn.Linear(1, 1)\n\n    def forward(self, x):\n        out1 = self.output1(self.input(x))\n        out2 = self.output2(self.input(x))\n        return {\'out1\': out1, \'out2\': out2}\n\n\nclass ModelDictOutputTest(ModelFittingTestCase):\n    def setUp(self):\n        super().setUp()\n        torch.manual_seed(42)\n        self.pytorch_network = DictOutputModel()\n        self.loss_function = nn.MSELoss()\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=1e-3)\n\n        self.model = Model(\n            self.pytorch_network,\n            self.optimizer,\n            lambda y_p, y_t: self.loss_function(y_p[\'out1\'], y_t[0]) + self.loss_function(y_p[\'out2\'], y_t[1]),\n            batch_metrics=self.batch_metrics,\n            epoch_metrics=self.epoch_metrics)\n\n    def test_fitting_with_tensor_multi_output_dict(self):\n        train_real_steps_per_epoch = 30\n        train_batch_size = ModelDictOutputTest.batch_size\n        train_final_batch_missing_samples = 7\n        train_size = train_real_steps_per_epoch * train_batch_size - \\\n                     train_final_batch_missing_samples\n        train_x = torch.rand(train_size, 1)\n        train_y = (torch.rand(train_size, 1), torch.rand(train_size, 1))\n\n        valid_real_steps_per_epoch = 10\n        # valid_batch_size will be the same as train_batch_size in the fit method.\n        valid_batch_size = train_batch_size\n        valid_final_batch_missing_samples = 3\n        valid_size = valid_real_steps_per_epoch * valid_batch_size - \\\n                     valid_final_batch_missing_samples\n        valid_x = torch.rand(valid_size, 1)\n        valid_y = (torch.rand(valid_size, 1), torch.rand(valid_size, 1))\n\n        logs = self.model.fit(train_x,\n                              train_y,\n                              validation_data=(valid_x, valid_y),\n                              epochs=ModelDictOutputTest.epochs,\n                              batch_size=train_batch_size,\n                              steps_per_epoch=None,\n                              validation_steps=None,\n                              callbacks=[self.mock_callback])\n        params = {\'epochs\': ModelDictOutputTest.epochs, \'steps\': train_real_steps_per_epoch}\n        self._test_callbacks_train(params, logs)\n\n    def test_ndarray_train_on_batch_dict_output(self):\n        x = np.random.rand(ModelDictOutputTest.batch_size, 1).astype(np.float32)\n        y1 = np.random.rand(ModelDictOutputTest.batch_size, 1).astype(np.float32)\n        y2 = np.random.rand(ModelDictOutputTest.batch_size, 1).astype(np.float32)\n        loss = self.model.train_on_batch(x, (y1, y2))\n        self.assertEqual(type(loss), float)\n\n    def test_evaluate_with_pred_dict_output(self):\n        y = (torch.rand(ModelDictOutputTest.evaluate_dataset_len,\n                        1), torch.rand(ModelDictOutputTest.evaluate_dataset_len, 1))\n        x = torch.rand(ModelDictOutputTest.evaluate_dataset_len, 1)\n        # We also test the unpacking.\n        _, pred_y = self.model.evaluate(x, y, batch_size=ModelDictOutputTest.batch_size, return_pred=True)\n        for pred in pred_y.values():\n            self.assertEqual(pred.shape, (ModelDictOutputTest.evaluate_dataset_len, 1))\n'"
tests/framework/model/test_model.py,65,"b'# pylint: disable=unused-argument,too-many-locals\nimport warnings\nfrom collections import OrderedDict\nfrom math import ceil\nfrom unittest import skipIf, main\nfrom unittest.mock import MagicMock\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nfrom poutyne.framework import Model, warning_settings\nfrom poutyne.framework.metrics import EpochMetric\nfrom poutyne.utils import TensorDataset\nfrom .base import ModelFittingTestCase\n\nwarning_settings[\'concatenate_returns\'] = \'ignore\'\n\nsome_metric_1_value = 1.\nsome_metric_2_value = 2.\nrepeat_batch_metric_value = 3.\n\n\ndef some_batch_metric_1(y_pred, y_true):\n    return torch.FloatTensor([some_metric_1_value])\n\n\ndef some_batch_metric_2(y_pred, y_true):\n    return torch.FloatTensor([some_metric_2_value])\n\n\ndef repeat_batch_metric(y_pred, y_true):\n    return torch.FloatTensor([repeat_batch_metric_value])\n\n\nclass SomeEpochMetric(EpochMetric):\n    def __init__(self):\n        super().__init__()\n        self.increment = 0.0\n\n    def forward(self, y_pred, y_true):\n        self.increment += 1\n\n    def get_metric(self):\n        increment_value = self.increment\n        self.increment = 0\n        return increment_value\n\n    def reset(self):\n        pass\n\n\nsome_constant_epoch_metric_value = 3\n\n\nclass SomeConstantEpochMetric(EpochMetric):\n    def forward(self, y_pred, y_true):\n        pass\n\n    def get_metric(self):\n        return torch.FloatTensor([some_constant_epoch_metric_value])\n\n    def reset(self):\n        pass\n\n\ndef some_data_tensor_generator(batch_size):\n    while True:\n        x = torch.rand(batch_size, 1)\n        y = torch.rand(batch_size, 1)\n        yield x, y\n\n\ndef some_ndarray_generator(batch_size):\n    while True:\n        x = np.random.rand(batch_size, 1).astype(np.float32)\n        y = np.random.rand(batch_size, 1).astype(np.float32)\n        yield x, y\n\n\ndef some_mocked_optimizer():\n    optim = MagicMock()\n\n    return optim\n\n\nclass SomeDataGeneratorUsingStopIteration:\n    def __init__(self, batch_size, length):\n        self.batch_size = batch_size\n        self.length = length\n\n    def __iter__(self):\n        return ((np.random.rand(self.batch_size, 1).astype(np.float32), np.random.rand(self.batch_size,\n                                                                                       1).astype(np.float32))\n                for i in range(self.length))\n\n\nclass SomeDataGeneratorWithLen:\n    def __init__(self, batch_size, length, num_missing_samples):\n        self.batch_size = batch_size\n        self.length = length\n        self.num_generator_called = 0\n        self.x = torch.rand(length * batch_size - num_missing_samples, 1)\n        self.y = torch.rand(length * batch_size - num_missing_samples, 1)\n\n    def __len__(self):\n        return self.length\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        start_idx = self.num_generator_called * self.batch_size\n        end_idx = (self.num_generator_called + 1) * self.batch_size\n        x = self.x[start_idx:end_idx]\n        y = self.y[start_idx:end_idx]\n        self.num_generator_called += 1\n        if self.num_generator_called == self.length:\n            self.num_generator_called = 0\n        return x, y\n\n\nclass ModelTest(ModelFittingTestCase):\n    # pylint: disable=too-many-public-methods\n    def setUp(self):\n        super().setUp()\n        torch.manual_seed(42)\n        self.pytorch_network = nn.Linear(1, 1)\n        self.loss_function = nn.MSELoss()\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=1e-3)\n        self.batch_metrics = [\n            some_batch_metric_1, (\'custom_name\', some_batch_metric_2), repeat_batch_metric, repeat_batch_metric\n        ]\n        self.batch_metrics_names = [\n            \'some_batch_metric_1\', \'custom_name\', \'repeat_batch_metric1\', \'repeat_batch_metric2\'\n        ]\n        self.batch_metrics_values = [\n            some_metric_1_value, some_metric_2_value, repeat_batch_metric_value, repeat_batch_metric_value\n        ]\n        self.epoch_metrics = [SomeConstantEpochMetric()]\n        self.epoch_metrics_names = [\'some_constant_epoch_metric\']\n        self.epoch_metrics_values = [some_constant_epoch_metric_value]\n\n        self.model = Model(self.pytorch_network,\n                           self.optimizer,\n                           self.loss_function,\n                           batch_metrics=self.batch_metrics,\n                           epoch_metrics=self.epoch_metrics)\n\n    def test_fitting_tensor_generator(self):\n        train_generator = some_data_tensor_generator(ModelTest.batch_size)\n        valid_generator = some_data_tensor_generator(ModelTest.batch_size)\n        logs = self.model.fit_generator(train_generator,\n                                        valid_generator,\n                                        epochs=ModelTest.epochs,\n                                        steps_per_epoch=ModelTest.steps_per_epoch,\n                                        validation_steps=ModelTest.steps_per_epoch,\n                                        callbacks=[self.mock_callback])\n        params = {\'epochs\': ModelTest.epochs, \'steps\': ModelTest.steps_per_epoch}\n        self._test_callbacks_train(params, logs)\n\n    def test_fitting_without_valid_generator(self):\n        train_generator = some_data_tensor_generator(ModelTest.batch_size)\n        logs = self.model.fit_generator(train_generator,\n                                        None,\n                                        epochs=ModelTest.epochs,\n                                        steps_per_epoch=ModelTest.steps_per_epoch,\n                                        callbacks=[self.mock_callback])\n        params = {\'epochs\': ModelTest.epochs, \'steps\': ModelTest.steps_per_epoch}\n        self._test_callbacks_train(params, logs, has_valid=False)\n\n    def test_correct_optim_calls_1_batch_per_step(self):\n        train_generator = some_data_tensor_generator(ModelTest.batch_size)\n\n        mocked_optimizer = some_mocked_optimizer()\n        mocked_optim_model = Model(self.pytorch_network,\n                                   mocked_optimizer,\n                                   self.loss_function,\n                                   batch_metrics=self.batch_metrics,\n                                   epoch_metrics=self.epoch_metrics)\n        mocked_optim_model.fit_generator(train_generator, None, epochs=1, steps_per_epoch=1, batches_per_step=1)\n\n        self.assertEqual(1, mocked_optimizer.step.call_count)\n        self.assertEqual(1, mocked_optimizer.zero_grad.call_count)\n\n    def test_correct_optim_calls__valid_n_batches_per_step(self):\n        n_batches = 5\n        items_per_batch = int(ModelTest.batch_size / n_batches)\n\n        x = torch.rand(n_batches, items_per_batch, 1)\n        y = torch.rand(n_batches, items_per_batch, 1)\n\n        mocked_optimizer = some_mocked_optimizer()\n        mocked_optim_model = Model(self.pytorch_network,\n                                   mocked_optimizer,\n                                   self.loss_function,\n                                   batch_metrics=self.batch_metrics,\n                                   epoch_metrics=self.epoch_metrics)\n        mocked_optim_model.fit_generator(list(zip(x, y)), None, epochs=1, batches_per_step=n_batches)\n\n        self.assertEqual(1, mocked_optimizer.step.call_count)\n        self.assertEqual(1, mocked_optimizer.zero_grad.call_count)\n\n    def test_fitting_generator_n_batches_per_step(self):\n        total_batch_size = 6\n\n        x = torch.rand(1, total_batch_size, 1)\n        y = torch.rand(1, total_batch_size, 1)\n\n        initial_params = self.model.get_weight_copies()\n\n        self.model.fit_generator(list(zip(x, y)), None, epochs=1, batches_per_step=1)\n\n        expected_params = list(self.model.get_weight_copies().values())\n\n        for mini_batch_size in [1, 2, 5]:\n            self.model.set_weights(initial_params)\n\n            n_batches_per_step = int(total_batch_size / mini_batch_size)\n\n            x.resize_((n_batches_per_step, mini_batch_size, 1))\n            y.resize_((n_batches_per_step, mini_batch_size, 1))\n\n            self.model.fit_generator(list(zip(x, y)), None, epochs=1, batches_per_step=n_batches_per_step)\n\n            returned_params = list(self.model.get_weight_copies().values())\n\n            np.testing.assert_almost_equal(returned_params, expected_params, decimal=4)\n\n    def test_fitting_generator_n_batches_per_step_higher_than_num_batches(self):\n        total_batch_size = 6\n\n        x = torch.rand(1, total_batch_size, 1)\n        y = torch.rand(1, total_batch_size, 1)\n\n        initial_params = self.model.get_weight_copies()\n\n        self.model.fit_generator(list(zip(x, y)), None, epochs=1, batches_per_step=1)\n\n        expected_params = list(self.model.get_weight_copies().values())\n\n        self.model.set_weights(initial_params)\n\n        self.model.fit_generator(list(zip(x, y)), None, epochs=1, batches_per_step=2)\n\n        returned_params = list(self.model.get_weight_copies().values())\n\n        np.testing.assert_almost_equal(returned_params, expected_params, decimal=4)\n\n    def test_fitting_generator_n_batches_per_step_uneven_batches(self):\n        total_batch_size = 6\n\n        x = torch.rand(1, total_batch_size, 1)\n        y = torch.rand(1, total_batch_size, 1)\n\n        initial_params = self.model.get_weight_copies()\n\n        self.model.fit_generator(list(zip(x, y)), None, epochs=1, batches_per_step=1)\n\n        expected_params = list(self.model.get_weight_copies().values())\n\n        x.squeeze_(dim=0)\n        y.squeeze_(dim=0)\n\n        uneven_chunk_sizes = [4, 5]\n\n        for chunk_size in uneven_chunk_sizes:\n            self.model.set_weights(initial_params)\n\n            splitted_x = x.split(chunk_size)\n            splitted_y = y.split(chunk_size)\n\n            n_batches_per_step = ceil(total_batch_size / chunk_size)\n\n            self.model.fit_generator(list(zip(splitted_x, splitted_y)),\n                                     None,\n                                     epochs=1,\n                                     batches_per_step=n_batches_per_step)\n\n            returned_params = list(self.model.get_weight_copies().values())\n\n            np.testing.assert_almost_equal(returned_params, expected_params, decimal=4)\n\n    def test_fitting_ndarray_generator(self):\n        train_generator = some_ndarray_generator(ModelTest.batch_size)\n        valid_generator = some_ndarray_generator(ModelTest.batch_size)\n        logs = self.model.fit_generator(train_generator,\n                                        valid_generator,\n                                        epochs=ModelTest.epochs,\n                                        steps_per_epoch=ModelTest.steps_per_epoch,\n                                        validation_steps=ModelTest.steps_per_epoch,\n                                        callbacks=[self.mock_callback])\n        params = {\'epochs\': ModelTest.epochs, \'steps\': ModelTest.steps_per_epoch}\n        self._test_callbacks_train(params, logs)\n\n    def test_fitting_with_data_loader(self):\n        train_real_steps_per_epoch = 30\n        train_batch_size = ModelTest.batch_size\n        train_final_batch_missing_samples = 7\n        train_size = train_real_steps_per_epoch * train_batch_size - \\\n                     train_final_batch_missing_samples\n        train_x = torch.rand(train_size, 1)\n        train_y = torch.rand(train_size, 1)\n        train_dataset = TensorDataset(train_x, train_y)\n        train_generator = DataLoader(train_dataset, train_batch_size)\n\n        valid_real_steps_per_epoch = 10\n        valid_batch_size = 15\n        valid_final_batch_missing_samples = 3\n        valid_size = valid_real_steps_per_epoch * valid_batch_size - \\\n                     valid_final_batch_missing_samples\n        valid_x = torch.rand(valid_size, 1)\n        valid_y = torch.rand(valid_size, 1)\n        valid_dataset = TensorDataset(valid_x, valid_y)\n        valid_generator = DataLoader(valid_dataset, valid_batch_size)\n\n        logs = self.model.fit_generator(train_generator,\n                                        valid_generator,\n                                        epochs=ModelTest.epochs,\n                                        steps_per_epoch=None,\n                                        validation_steps=None,\n                                        callbacks=[self.mock_callback])\n        params = {\'epochs\': ModelTest.epochs, \'steps\': train_real_steps_per_epoch}\n        self._test_callbacks_train(params, logs)\n\n    def test_fitting_generator_calls(self):\n        train_real_steps_per_epoch = 30\n        train_batch_size = ModelTest.batch_size\n        train_final_batch_missing_samples = 7\n        train_size = train_real_steps_per_epoch * train_batch_size - \\\n                     train_final_batch_missing_samples\n        train_x = torch.rand(train_size, 1)\n        train_y = torch.rand(train_size, 1)\n        train_dataset = TensorDataset(train_x, train_y)\n        train_generator = DataLoader(train_dataset, train_batch_size)\n\n        valid_real_steps_per_epoch = 10\n        valid_batch_size = 15\n        valid_final_batch_missing_samples = 3\n        valid_size = valid_real_steps_per_epoch * valid_batch_size - \\\n                     valid_final_batch_missing_samples\n        valid_x = torch.rand(valid_size, 1)\n        valid_y = torch.rand(valid_size, 1)\n        valid_dataset = TensorDataset(valid_x, valid_y)\n        valid_generator = DataLoader(valid_dataset, valid_batch_size)\n\n        class IterableMock:\n            def __init__(self, iterable):\n                self.iterable = iterable\n                self.iter = None\n                self.calls = []\n\n            def __iter__(self):\n                self.calls.append(\'__iter__\')\n                self.iter = iter(self.iterable)\n                return self\n\n            def __next__(self):\n                self.calls.append(\'__next__\')\n                return next(self.iter)\n\n            def __len__(self):\n                self.calls.append(\'__len__\')\n                return len(self.iterable)\n\n        mock_train_generator = IterableMock(train_generator)\n        mock_valid_generator = IterableMock(valid_generator)\n        self.model.fit_generator(mock_train_generator, mock_valid_generator, epochs=ModelTest.epochs)\n        expected_train_calls = [\'__len__\'] + \\\n            ([\'__iter__\'] + [\'__next__\'] * train_real_steps_per_epoch) * ModelTest.epochs\n        expected_valid_calls = [\'__len__\'] + \\\n            ([\'__iter__\'] + [\'__next__\'] * valid_real_steps_per_epoch) * ModelTest.epochs\n        self.assertEqual(mock_train_generator.calls, expected_train_calls)\n        self.assertEqual(mock_valid_generator.calls, expected_valid_calls)\n\n    def test_fitting_with_tensor(self):\n        train_real_steps_per_epoch = 30\n        train_batch_size = ModelTest.batch_size\n        train_final_batch_missing_samples = 7\n        train_size = train_real_steps_per_epoch * train_batch_size - \\\n                     train_final_batch_missing_samples\n        train_x = torch.rand(train_size, 1)\n        train_y = torch.rand(train_size, 1)\n\n        valid_real_steps_per_epoch = 10\n        # valid_batch_size will be the same as train_batch_size in the fit method.\n        valid_batch_size = train_batch_size\n        valid_final_batch_missing_samples = 3\n        valid_size = valid_real_steps_per_epoch * valid_batch_size - \\\n                     valid_final_batch_missing_samples\n        valid_x = torch.rand(valid_size, 1)\n        valid_y = torch.rand(valid_size, 1)\n\n        logs = self.model.fit(train_x,\n                              train_y,\n                              validation_data=(valid_x, valid_y),\n                              epochs=ModelTest.epochs,\n                              batch_size=train_batch_size,\n                              steps_per_epoch=None,\n                              validation_steps=None,\n                              callbacks=[self.mock_callback])\n        params = {\'epochs\': ModelTest.epochs, \'steps\': train_real_steps_per_epoch}\n        self._test_callbacks_train(params, logs)\n\n    def test_fitting_with_np_array(self):\n        train_real_steps_per_epoch = 30\n        train_batch_size = ModelTest.batch_size\n        train_final_batch_missing_samples = 7\n        train_size = train_real_steps_per_epoch * train_batch_size - \\\n                     train_final_batch_missing_samples\n        train_x = np.random.rand(train_size, 1).astype(np.float32)\n        train_y = np.random.rand(train_size, 1).astype(np.float32)\n\n        valid_real_steps_per_epoch = 10\n        # valid_batch_size will be the same as train_batch_size in the fit method.\n        valid_batch_size = train_batch_size\n        valid_final_batch_missing_samples = 3\n        valid_size = valid_real_steps_per_epoch * valid_batch_size - \\\n                     valid_final_batch_missing_samples\n        valid_x = np.random.rand(valid_size, 1).astype(np.float32)\n        valid_y = np.random.rand(valid_size, 1).astype(np.float32)\n\n        logs = self.model.fit(train_x,\n                              train_y,\n                              validation_data=(valid_x, valid_y),\n                              epochs=ModelTest.epochs,\n                              batch_size=train_batch_size,\n                              steps_per_epoch=None,\n                              validation_steps=None,\n                              callbacks=[self.mock_callback])\n        params = {\'epochs\': ModelTest.epochs, \'steps\': train_real_steps_per_epoch}\n        self._test_callbacks_train(params, logs)\n\n    def test_fitting_with_generator_with_len(self):\n        train_real_steps_per_epoch = 30\n        train_generator = SomeDataGeneratorWithLen(batch_size=ModelTest.batch_size,\n                                                   length=train_real_steps_per_epoch,\n                                                   num_missing_samples=7)\n        valid_generator = SomeDataGeneratorWithLen(batch_size=15, length=10, num_missing_samples=3)\n        logs = self.model.fit_generator(train_generator,\n                                        valid_generator,\n                                        epochs=ModelTest.epochs,\n                                        steps_per_epoch=None,\n                                        validation_steps=None,\n                                        callbacks=[self.mock_callback])\n        params = {\'epochs\': ModelTest.epochs, \'steps\': train_real_steps_per_epoch}\n        self._test_callbacks_train(params, logs)\n\n    def test_fitting_with_generator_with_stop_iteration(self):\n        train_real_steps_per_epoch = 30\n        train_generator = SomeDataGeneratorUsingStopIteration(batch_size=ModelTest.batch_size,\n                                                              length=train_real_steps_per_epoch)\n        valid_generator = SomeDataGeneratorUsingStopIteration(batch_size=15, length=10)\n        logs = self.model.fit_generator(train_generator,\n                                        valid_generator,\n                                        epochs=ModelTest.epochs,\n                                        steps_per_epoch=None,\n                                        validation_steps=None,\n                                        callbacks=[self.mock_callback])\n        params = {\'epochs\': ModelTest.epochs, \'steps\': None}\n        self._test_callbacks_train(params, logs, steps=train_real_steps_per_epoch)\n\n    def test_tensor_train_on_batch(self):\n        x = torch.rand(ModelTest.batch_size, 1)\n        y = torch.rand(ModelTest.batch_size, 1)\n        loss, metrics = self.model.train_on_batch(x, y)\n        self.assertEqual(type(loss), float)\n        self.assertEqual(type(metrics), np.ndarray)\n        self.assertEqual(metrics.tolist(), self.batch_metrics_values)\n\n    def test_train_on_batch_with_pred(self):\n        x = torch.rand(ModelTest.batch_size, 1)\n        y = torch.rand(ModelTest.batch_size, 1)\n        loss, metrics, pred_y = self.model.train_on_batch(x, y, return_pred=True)\n        self.assertEqual(type(loss), float)\n        self.assertEqual(type(metrics), np.ndarray)\n        self.assertEqual(metrics.tolist(), self.batch_metrics_values)\n        self.assertEqual(pred_y.shape, (ModelTest.batch_size, 1))\n\n    def test_ndarray_train_on_batch(self):\n        x = np.random.rand(ModelTest.batch_size, 1).astype(np.float32)\n        y = np.random.rand(ModelTest.batch_size, 1).astype(np.float32)\n        loss, metrics = self.model.train_on_batch(x, y)\n        self.assertEqual(type(loss), float)\n        self.assertEqual(type(metrics), np.ndarray)\n        self.assertEqual(metrics.tolist(), self.batch_metrics_values)\n\n    def test_evaluate(self):\n        x = torch.rand(ModelTest.evaluate_dataset_len, 1)\n        y = torch.rand(ModelTest.evaluate_dataset_len, 1)\n        loss, metrics = self.model.evaluate(x, y, batch_size=ModelTest.batch_size)\n        self.assertEqual(type(loss), float)\n        self.assertEqual(type(metrics), np.ndarray)\n        self.assertEqual(metrics.tolist(), self.batch_metrics_values + self.epoch_metrics_values)\n\n    def test_evaluate_with_pred(self):\n        x = torch.rand(ModelTest.evaluate_dataset_len, 1)\n        y = torch.rand(ModelTest.evaluate_dataset_len, 1)\n        # We also test the unpacking.\n        _, _, pred_y = self.model.evaluate(x, y, batch_size=ModelTest.batch_size, return_pred=True)\n        self.assertEqual(pred_y.shape, (ModelTest.evaluate_dataset_len, 1))\n\n    def test_evaluate_with_callback(self):\n        x = torch.rand(ModelTest.evaluate_dataset_len, 1)\n        y = torch.rand(ModelTest.evaluate_dataset_len, 1)\n        # We also test the unpacking.\n        _, _, pred_y = self.model.evaluate(x,\n                                           y,\n                                           batch_size=ModelTest.batch_size,\n                                           return_pred=True,\n                                           callbacks=[self.mock_callback])\n        self.assertEqual(pred_y.shape, (ModelTest.evaluate_dataset_len, 1))\n\n    def test_evaluate_with_np_array(self):\n        x = np.random.rand(ModelTest.evaluate_dataset_len, 1).astype(np.float32)\n        y = np.random.rand(ModelTest.evaluate_dataset_len, 1).astype(np.float32)\n        loss, metrics, pred_y = self.model.evaluate(x, y, batch_size=ModelTest.batch_size, return_pred=True)\n        self.assertEqual(type(loss), float)\n        self.assertEqual(type(metrics), np.ndarray)\n        self.assertEqual(metrics.tolist(), self.batch_metrics_values + self.epoch_metrics_values)\n        self.assertEqual(pred_y.shape, (ModelTest.evaluate_dataset_len, 1))\n\n    def test_evaluate_data_loader(self):\n        x = torch.rand(ModelTest.evaluate_dataset_len, 1)\n        y = torch.rand(ModelTest.evaluate_dataset_len, 1)\n        dataset = TensorDataset(x, y)\n        generator = DataLoader(dataset, ModelTest.batch_size)\n        loss, metrics, pred_y = self.model.evaluate_generator(generator, return_pred=True)\n        self.assertEqual(type(loss), float)\n        self.assertEqual(type(metrics), np.ndarray)\n        self.assertEqual(metrics.tolist(), self.batch_metrics_values + self.epoch_metrics_values)\n        self.assertEqual(pred_y.shape, (ModelTest.evaluate_dataset_len, 1))\n\n    def test_evaluate_generator(self):\n        num_steps = 10\n        generator = some_data_tensor_generator(ModelTest.batch_size)\n        loss, metrics, pred_y = self.model.evaluate_generator(generator, steps=num_steps, return_pred=True)\n        self.assertEqual(type(loss), float)\n        self.assertEqual(type(metrics), np.ndarray)\n        self.assertEqual(metrics.tolist(), self.batch_metrics_values + self.epoch_metrics_values)\n        self.assertEqual(type(pred_y), np.ndarray)\n        self.assertEqual(pred_y.shape, (num_steps * ModelTest.batch_size, 1))\n\n    def test_evaluate_generator_with_callback(self):\n        num_steps = 10\n        generator = some_data_tensor_generator(ModelTest.batch_size)\n        result_log = self.model.evaluate_generator(generator,\n                                                   steps=num_steps,\n                                                   return_pred=True,\n                                                   callbacks=[self.mock_callback])\n\n        params = {\'batch\': ModelTest.epochs}\n        self._test_callbacks_test(params, result_log)\n\n    def test_evaluate_generator_with_ground_truth(self):\n        num_steps = 10\n        generator = some_data_tensor_generator(ModelTest.batch_size)\n        loss, metrics, pred_y, true_y = self.model.evaluate_generator(generator,\n                                                                      steps=num_steps,\n                                                                      return_pred=True,\n                                                                      return_ground_truth=True)\n        self.assertEqual(type(loss), float)\n        self.assertEqual(type(metrics), np.ndarray)\n        self.assertEqual(metrics.tolist(), self.batch_metrics_values + self.epoch_metrics_values)\n        self.assertEqual(type(pred_y), np.ndarray)\n        self.assertEqual(type(true_y), np.ndarray)\n        self.assertEqual(pred_y.shape, (num_steps * ModelTest.batch_size, 1))\n        self.assertEqual(true_y.shape, (num_steps * ModelTest.batch_size, 1))\n\n    def test_evaluate_generator_with_no_concatenation(self):\n        num_steps = 10\n        generator = some_data_tensor_generator(ModelTest.batch_size)\n        loss, metrics, pred_y, true_y = self.model.evaluate_generator(generator,\n                                                                      steps=num_steps,\n                                                                      return_pred=True,\n                                                                      return_ground_truth=True,\n                                                                      concatenate_returns=False)\n        self.assertEqual(type(loss), float)\n        self.assertEqual(type(metrics), np.ndarray)\n        self.assertEqual(metrics.tolist(), self.batch_metrics_values + self.epoch_metrics_values)\n\n        self.assertEqual(type(pred_y), list)\n        for pred in pred_y:\n            self.assertEqual(type(pred), np.ndarray)\n            self.assertEqual(pred.shape, (ModelTest.batch_size, 1))\n        self.assertEqual(type(true_y), list)\n        for true in true_y:\n            self.assertEqual(type(true), np.ndarray)\n            self.assertEqual(true.shape, (ModelTest.batch_size, 1))\n\n    def test_evaluate_with_only_one_metric(self):\n        model = Model(self.pytorch_network, self.optimizer, self.loss_function, batch_metrics=self.batch_metrics[:1])\n        x = torch.rand(ModelTest.evaluate_dataset_len, 1)\n        y = torch.rand(ModelTest.evaluate_dataset_len, 1)\n        loss, first_metric = model.evaluate(x, y, batch_size=ModelTest.batch_size)\n        self.assertEqual(type(loss), float)\n        self.assertEqual(type(first_metric), float)\n        self.assertEqual(first_metric, some_metric_1_value)\n\n    def test_metrics_integration(self):\n        num_steps = 10\n        model = Model(self.pytorch_network, self.optimizer, self.loss_function, batch_metrics=[F.mse_loss])\n        train_generator = some_data_tensor_generator(ModelTest.batch_size)\n        valid_generator = some_data_tensor_generator(ModelTest.batch_size)\n        model.fit_generator(train_generator,\n                            valid_generator,\n                            epochs=ModelTest.epochs,\n                            steps_per_epoch=ModelTest.steps_per_epoch,\n                            validation_steps=ModelTest.steps_per_epoch,\n                            callbacks=[self.mock_callback])\n        generator = some_data_tensor_generator(ModelTest.batch_size)\n        loss, mse = model.evaluate_generator(generator, steps=num_steps)\n        self.assertEqual(type(loss), float)\n        self.assertEqual(type(mse), float)\n\n    def test_epoch_metrics_integration(self):\n        model = Model(self.pytorch_network, self.optimizer, self.loss_function, epoch_metrics=[SomeEpochMetric()])\n        train_generator = some_data_tensor_generator(ModelTest.batch_size)\n        valid_generator = some_data_tensor_generator(ModelTest.batch_size)\n        logs = model.fit_generator(train_generator,\n                                   valid_generator,\n                                   epochs=1,\n                                   steps_per_epoch=ModelTest.steps_per_epoch,\n                                   validation_steps=ModelTest.steps_per_epoch)\n        actual_value = logs[-1][\'some_epoch_metric\']\n        val_actual_value = logs[-1][\'val_some_epoch_metric\']\n        expected_value = 5\n        self.assertEqual(val_actual_value, expected_value)\n        self.assertEqual(actual_value, expected_value)\n\n    def test_evaluate_with_no_metric(self):\n        model = Model(self.pytorch_network, self.optimizer, self.loss_function)\n        x = torch.rand(ModelTest.evaluate_dataset_len, 1)\n        y = torch.rand(ModelTest.evaluate_dataset_len, 1)\n        loss = model.evaluate(x, y, batch_size=ModelTest.batch_size)\n        self.assertEqual(type(loss), float)\n\n    def test_tensor_evaluate_on_batch(self):\n        x = torch.rand(ModelTest.batch_size, 1)\n        y = torch.rand(ModelTest.batch_size, 1)\n        loss, metrics = self.model.evaluate_on_batch(x, y)\n        self.assertEqual(type(loss), float)\n        self.assertEqual(type(metrics), np.ndarray)\n        self.assertEqual(metrics.tolist(), self.batch_metrics_values)\n\n    def test_evaluate_on_batch_with_pred(self):\n        x = torch.rand(ModelTest.batch_size, 1)\n        y = torch.rand(ModelTest.batch_size, 1)\n        loss, metrics, pred_y = self.model.evaluate_on_batch(x, y, return_pred=True)\n        self.assertEqual(type(loss), float)\n        self.assertEqual(type(metrics), np.ndarray)\n        self.assertEqual(metrics.tolist(), self.batch_metrics_values)\n        self.assertEqual(pred_y.shape, (ModelTest.batch_size, 1))\n\n    def test_ndarray_evaluate_on_batch(self):\n        x = np.random.rand(ModelTest.batch_size, 1).astype(np.float32)\n        y = np.random.rand(ModelTest.batch_size, 1).astype(np.float32)\n        loss, metrics = self.model.evaluate_on_batch(x, y)\n        self.assertEqual(type(loss), float)\n        self.assertEqual(type(metrics), np.ndarray)\n        self.assertEqual(metrics.tolist(), self.batch_metrics_values)\n\n    def test_predict(self):\n        x = torch.rand(ModelTest.evaluate_dataset_len, 1)\n        pred_y = self.model.predict(x, batch_size=ModelTest.batch_size)\n        self.assertEqual(pred_y.shape, (ModelTest.evaluate_dataset_len, 1))\n\n    def test_predict_with_np_array(self):\n        x = np.random.rand(ModelTest.evaluate_dataset_len, 1).astype(np.float32)\n        pred_y = self.model.predict(x, batch_size=ModelTest.batch_size)\n        self.assertEqual(type(pred_y), np.ndarray)\n        self.assertEqual(pred_y.shape, (ModelTest.evaluate_dataset_len, 1))\n\n    def test_predict_data_loader(self):\n        x = torch.rand(ModelTest.evaluate_dataset_len, 1)\n        generator = DataLoader(x, ModelTest.batch_size)\n        pred_y = self.model.predict_generator(generator)\n        self.assertEqual(type(pred_y), np.ndarray)\n        self.assertEqual(pred_y.shape, (ModelTest.evaluate_dataset_len, 1))\n\n    def test_predict_generator(self):\n        num_steps = 10\n        generator = some_data_tensor_generator(ModelTest.batch_size)\n        generator = (x for x, _ in generator)\n        pred_y = self.model.predict_generator(generator, steps=num_steps)\n        self.assertEqual(type(pred_y), np.ndarray)\n        self.assertEqual(pred_y.shape, (num_steps * ModelTest.batch_size, 1))\n\n    def test_predict_generator_with_no_concatenation(self):\n        num_steps = 10\n        generator = some_data_tensor_generator(ModelTest.batch_size)\n        generator = (x for x, _ in generator)\n        pred_y = self.model.predict_generator(generator, steps=num_steps, concatenate_returns=False)\n        self.assertEqual(type(pred_y), list)\n        for pred in pred_y:\n            self.assertEqual(type(pred), np.ndarray)\n            self.assertEqual(pred.shape, (ModelTest.batch_size, 1))\n\n    def test_tensor_predict_on_batch(self):\n        x = torch.rand(ModelTest.batch_size, 1)\n        pred_y = self.model.predict_on_batch(x)\n        self.assertEqual(pred_y.shape, (ModelTest.batch_size, 1))\n\n    def test_ndarray_predict_on_batch(self):\n        x = np.random.rand(ModelTest.batch_size, 1).astype(np.float32)\n        pred_y = self.model.predict_on_batch(x)\n        self.assertEqual(pred_y.shape, (ModelTest.batch_size, 1))\n\n    @skipIf(not torch.cuda.is_available(), ""no gpu available"")\n    def test_cpu_cuda(self):\n        train_generator = some_data_tensor_generator(ModelTest.batch_size)\n        valid_generator = some_data_tensor_generator(ModelTest.batch_size)\n\n        with torch.cuda.device(ModelTest.cuda_device):\n            self.model.cuda()\n            self.model.fit_generator(train_generator,\n                                     valid_generator,\n                                     epochs=ModelTest.epochs,\n                                     steps_per_epoch=ModelTest.steps_per_epoch,\n                                     validation_steps=ModelTest.steps_per_epoch,\n                                     callbacks=[self.mock_callback])\n\n        # The context manager is also used here because of this bug:\n        # https://github.com/pytorch/pytorch/issues/7320\n        with torch.cuda.device(ModelTest.cuda_device):\n            self.model.cuda(ModelTest.cuda_device)\n            self._test_device(torch.device(\'cuda:\' + str(ModelTest.cuda_device)))\n            self.model.fit_generator(train_generator,\n                                     valid_generator,\n                                     epochs=ModelTest.epochs,\n                                     steps_per_epoch=ModelTest.steps_per_epoch,\n                                     validation_steps=ModelTest.steps_per_epoch,\n                                     callbacks=[self.mock_callback])\n\n            self.model.cpu()\n            self._test_device(torch.device(\'cpu\'))\n            self.model.fit_generator(train_generator,\n                                     valid_generator,\n                                     epochs=ModelTest.epochs,\n                                     steps_per_epoch=ModelTest.steps_per_epoch,\n                                     validation_steps=ModelTest.steps_per_epoch,\n                                     callbacks=[self.mock_callback])\n\n            self.model.to(torch.device(\'cuda:\' + str(ModelTest.cuda_device)))\n            self._test_device(torch.device(\'cuda:\' + str(ModelTest.cuda_device)))\n            self.model.fit_generator(train_generator,\n                                     valid_generator,\n                                     epochs=ModelTest.epochs,\n                                     steps_per_epoch=ModelTest.steps_per_epoch,\n                                     validation_steps=ModelTest.steps_per_epoch,\n                                     callbacks=[self.mock_callback])\n\n            self.model.to(torch.device(\'cpu\'))\n            self._test_device(torch.device(\'cpu\'))\n            self.model.fit_generator(train_generator,\n                                     valid_generator,\n                                     epochs=ModelTest.epochs,\n                                     steps_per_epoch=ModelTest.steps_per_epoch,\n                                     validation_steps=ModelTest.steps_per_epoch,\n                                     callbacks=[self.mock_callback])\n\n    def _test_device(self, device):\n        for p in self.pytorch_network.parameters():\n            self.assertEqual(p.device, device)\n\n    def test_get_batch_size(self):\n        batch_size = ModelTest.batch_size\n        x = np.random.rand(batch_size, 1).astype(np.float32)\n        y = np.random.rand(batch_size, 1).astype(np.float32)\n\n        batch_size2 = ModelTest.batch_size + 1\n        x2 = np.random.rand(batch_size2, 1).astype(np.float32)\n        y2 = np.random.rand(batch_size2, 1).astype(np.float32)\n\n        other_batch_size = batch_size2 + 1\n\n        inf_batch_size = self.model.get_batch_size(x, y)\n        self.assertEqual(inf_batch_size, batch_size)\n\n        inf_batch_size = self.model.get_batch_size(x2, y2)\n        self.assertEqual(inf_batch_size, batch_size2)\n\n        inf_batch_size = self.model.get_batch_size(x, y2)\n        self.assertEqual(inf_batch_size, batch_size)\n\n        inf_batch_size = self.model.get_batch_size(x2, y)\n        self.assertEqual(inf_batch_size, batch_size2)\n\n        inf_batch_size = self.model.get_batch_size((x, x2), y)\n        self.assertEqual(inf_batch_size, batch_size)\n\n        inf_batch_size = self.model.get_batch_size((x2, x), y)\n        self.assertEqual(inf_batch_size, batch_size)\n\n        inf_batch_size = self.model.get_batch_size((x, x2), (y, y2))\n        self.assertEqual(inf_batch_size, batch_size)\n\n        inf_batch_size = self.model.get_batch_size((x2, x), (y, y2))\n        self.assertEqual(inf_batch_size, batch_size2)\n\n        inf_batch_size = self.model.get_batch_size([x, x2], y)\n        self.assertEqual(inf_batch_size, batch_size)\n\n        inf_batch_size = self.model.get_batch_size([x2, x], y)\n        self.assertEqual(inf_batch_size, batch_size)\n\n        inf_batch_size = self.model.get_batch_size([x, x2], [y, y2])\n        self.assertEqual(inf_batch_size, batch_size)\n\n        inf_batch_size = self.model.get_batch_size([x2, x], [y, y2])\n        self.assertEqual(inf_batch_size, batch_size2)\n\n        inf_batch_size = self.model.get_batch_size({\'batch_size\': other_batch_size, \'x\': x}, {\'y\': y})\n        self.assertEqual(inf_batch_size, other_batch_size)\n\n        inf_batch_size = self.model.get_batch_size({\'x\': x}, {\'batch_size\': other_batch_size, \'y\': y})\n        self.assertEqual(inf_batch_size, other_batch_size)\n\n        inf_batch_size = self.model.get_batch_size({\'x\': x}, {\'y\': y})\n        self.assertEqual(inf_batch_size, batch_size)\n\n        inf_batch_size = self.model.get_batch_size(OrderedDict([(\'x1\', x), (\'x2\', x2)]), {\'y\': y})\n        self.assertEqual(inf_batch_size, batch_size)\n\n        inf_batch_size = self.model.get_batch_size(OrderedDict([(\'x1\', x2), (\'x2\', x)]), {\'y\': y})\n        self.assertEqual(inf_batch_size, batch_size2)\n\n        inf_batch_size = self.model.get_batch_size([1, 2, 3], {\'y\': y})\n        self.assertEqual(inf_batch_size, batch_size)\n\n    def test_get_batch_size_warning(self):\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(""always"")\n            inf_batch_size = self.model.get_batch_size([1, 2, 3], [4, 5, 6])\n            self.assertEqual(inf_batch_size, 1)\n            self.assertEqual(len(w), 1)\n\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(""always"")\n            warning_settings[\'batch_size\'] = \'ignore\'\n            inf_batch_size = self.model.get_batch_size([1, 2, 3], [4, 5, 6])\n            self.assertEqual(inf_batch_size, 1)\n            self.assertEqual(len(w), 0)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tests/framework/model/test_multi_dict_io.py,10,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom poutyne.framework import Model, warning_settings\n\nfrom .base import ModelFittingTestCase\n\nwarning_settings[\'concatenate_returns\'] = \'ignore\'\n\n\nclass DictIOModel(nn.Module):\n    """"""Model to test multiple dict input/output""""""\n\n    def __init__(self, input_keys, output_keys):\n        super(DictIOModel, self).__init__()\n        assert len(input_keys) == len(output_keys)\n        inputs = {k: nn.Linear(1, 1) for k in input_keys}\n        self.inputs = nn.ModuleDict(inputs)\n\n        self.input_keys = input_keys\n        self.output_keys = output_keys\n\n    def forward(self, x):\n        return {out_k: self.inputs[in_k](x[in_k]) for in_k, out_k in zip(self.input_keys, self.output_keys)}\n\n\ndef dict_mse_loss(y_pred, y_true):\n    return sum(F.mse_loss(y_pred[k], y_true[k]) for k in y_true.keys())\n\n\ndef get_batch(batch_size):\n    x1 = torch.rand(batch_size, 1)\n    x2 = torch.rand(batch_size, 1)\n    y1 = torch.rand(batch_size, 1)\n    y2 = torch.rand(batch_size, 1)\n    return dict(x1=x1, x2=x2), dict(y1=y1, y2=y2)\n\n\ndef some_data_tensor_generator_dict_io(batch_size):\n    while True:\n        yield get_batch(batch_size)\n\n\nclass ModelMultiDictIOTest(ModelFittingTestCase):\n    def setUp(self):\n        super().setUp()\n        torch.manual_seed(42)\n        self.pytorch_network = DictIOModel([\'x1\', \'x2\'], [\'y1\', \'y2\'])\n        self.loss_function = dict_mse_loss\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=1e-3)\n\n        self.model = Model(self.pytorch_network,\n                           self.optimizer,\n                           self.loss_function,\n                           batch_metrics=self.batch_metrics,\n                           epoch_metrics=self.epoch_metrics)\n\n    def test_fitting_tensor_generator_multi_dict_io(self):\n        train_generator = some_data_tensor_generator_dict_io(ModelMultiDictIOTest.batch_size)\n        valid_generator = some_data_tensor_generator_dict_io(ModelMultiDictIOTest.batch_size)\n        logs = self.model.fit_generator(train_generator,\n                                        valid_generator,\n                                        epochs=ModelMultiDictIOTest.epochs,\n                                        steps_per_epoch=ModelMultiDictIOTest.steps_per_epoch,\n                                        validation_steps=ModelMultiDictIOTest.steps_per_epoch,\n                                        callbacks=[self.mock_callback])\n        params = {\'epochs\': ModelMultiDictIOTest.epochs, \'steps\': ModelMultiDictIOTest.steps_per_epoch}\n        self._test_callbacks_train(params, logs)\n\n    def test_tensor_train_on_batch_multi_dict_io(self):\n        x, y = get_batch(ModelMultiDictIOTest.batch_size)\n        loss = self.model.train_on_batch(x, y)\n        self.assertEqual(type(loss), float)\n\n    def test_train_on_batch_with_pred_multi_dict_io(self):\n        x, y = get_batch(ModelMultiDictIOTest.batch_size)\n        loss, pred_y = self.model.train_on_batch(x, y, return_pred=True)\n        self.assertEqual(type(loss), float)\n        for value in pred_y.values():\n            self.assertEqual(value.shape, (ModelMultiDictIOTest.batch_size, 1))\n\n    def test_ndarray_train_on_batch_multi_dict_io(self):\n        x1 = np.random.rand(ModelMultiDictIOTest.batch_size, 1).astype(np.float32)\n        x2 = np.random.rand(ModelMultiDictIOTest.batch_size, 1).astype(np.float32)\n        y1 = np.random.rand(ModelMultiDictIOTest.batch_size, 1).astype(np.float32)\n        y2 = np.random.rand(ModelMultiDictIOTest.batch_size, 1).astype(np.float32)\n        x, y = dict(x1=x1, x2=x2), dict(y1=y1, y2=y2)\n        loss = self.model.train_on_batch(x, y)\n        self.assertEqual(type(loss), float)\n\n    def test_evaluate_generator_multi_dict_io(self):\n        num_steps = 10\n        generator = some_data_tensor_generator_dict_io(ModelMultiDictIOTest.batch_size)\n        loss, pred_y = self.model.evaluate_generator(generator, steps=num_steps, return_pred=True)\n        self.assertEqual(type(loss), float)\n        self._test_size_and_type_for_generator(pred_y, (num_steps * ModelMultiDictIOTest.batch_size, 1))\n\n    def test_tensor_evaluate_on_batch_multi_dict_io(self):\n        x, y = get_batch(ModelMultiDictIOTest.batch_size)\n        loss = self.model.evaluate_on_batch(x, y)\n        self.assertEqual(type(loss), float)\n\n    def test_predict_generator_multi_dict_io(self):\n        num_steps = 10\n        generator = some_data_tensor_generator_dict_io(ModelMultiDictIOTest.batch_size)\n        generator = (x for x, _ in generator)\n        pred_y = self.model.predict_generator(generator, steps=num_steps)\n        self._test_size_and_type_for_generator(pred_y, (num_steps * ModelMultiDictIOTest.batch_size, 1))\n\n    def test_tensor_predict_on_batch_multi_dict_io(self):\n        x1 = torch.rand(ModelMultiDictIOTest.batch_size, 1)\n        x2 = torch.rand(ModelMultiDictIOTest.batch_size, 1)\n        pred_y = self.model.predict_on_batch(dict(x1=x1, x2=x2))\n        self._test_size_and_type_for_generator(pred_y, (ModelMultiDictIOTest.batch_size, 1))\n'"
tests/framework/model/test_multi_input.py,33,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nfrom poutyne.framework import Model, warning_settings\nfrom poutyne.utils import TensorDataset\n\nfrom .base import ModelFittingTestCase, MultiIOModel\n\nwarning_settings['concatenate_returns'] = 'ignore'\n\n\ndef some_data_tensor_generator_multi_input(batch_size):\n    while True:\n        x1 = torch.rand(batch_size, 1)\n        x2 = torch.rand(batch_size, 1)\n        y = torch.rand(batch_size, 1)\n        yield (x1, x2), y\n\n\nclass ModelMultiInputTest(ModelFittingTestCase):\n    def setUp(self):\n        super().setUp()\n        torch.manual_seed(42)\n        self.pytorch_network = MultiIOModel(num_input=1, num_output=1)\n        self.loss_function = nn.MSELoss()\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=1e-3)\n\n        self.model = Model(self.pytorch_network,\n                           self.optimizer,\n                           self.loss_function,\n                           batch_metrics=self.batch_metrics,\n                           epoch_metrics=self.epoch_metrics)\n\n    def test_fitting_tensor_generator_multi_input(self):\n        train_generator = some_data_tensor_generator_multi_input(ModelMultiInputTest.batch_size)\n        valid_generator = some_data_tensor_generator_multi_input(ModelMultiInputTest.batch_size)\n        logs = self.model.fit_generator(train_generator,\n                                        valid_generator,\n                                        epochs=ModelMultiInputTest.epochs,\n                                        steps_per_epoch=ModelMultiInputTest.steps_per_epoch,\n                                        validation_steps=ModelMultiInputTest.steps_per_epoch,\n                                        callbacks=[self.mock_callback])\n        params = {'epochs': ModelMultiInputTest.epochs, 'steps': ModelMultiInputTest.steps_per_epoch}\n        self._test_callbacks_train(params, logs)\n\n    def test_fitting_with_tensor_multi_input(self):\n        train_real_steps_per_epoch = 30\n        train_batch_size = ModelMultiInputTest.batch_size\n        train_final_batch_missing_samples = 7\n        train_size = train_real_steps_per_epoch * train_batch_size - \\\n                     train_final_batch_missing_samples\n        train_x = (torch.rand(train_size, 1), torch.rand(train_size, 1))\n        train_y = torch.rand(train_size, 1)\n\n        valid_real_steps_per_epoch = 10\n        # valid_batch_size will be the same as train_batch_size in the fit method.\n        valid_batch_size = train_batch_size\n        valid_final_batch_missing_samples = 3\n        valid_size = valid_real_steps_per_epoch * valid_batch_size - \\\n                     valid_final_batch_missing_samples\n        valid_x = (torch.rand(valid_size, 1), torch.rand(valid_size, 1))\n        valid_y = torch.rand(valid_size, 1)\n\n        logs = self.model.fit(train_x,\n                              train_y,\n                              validation_data=(valid_x, valid_y),\n                              epochs=ModelMultiInputTest.epochs,\n                              batch_size=train_batch_size,\n                              steps_per_epoch=None,\n                              validation_steps=None,\n                              callbacks=[self.mock_callback])\n        params = {'epochs': ModelMultiInputTest.epochs, 'steps': train_real_steps_per_epoch}\n        self._test_callbacks_train(params, logs)\n\n    def test_tensor_train_on_batch_multi_input(self):\n        x1 = torch.rand(ModelMultiInputTest.batch_size, 1)\n        x2 = torch.rand(ModelMultiInputTest.batch_size, 1)\n        y = torch.rand(ModelMultiInputTest.batch_size, 1)\n        loss = self.model.train_on_batch((x1, x2), y)\n        self.assertEqual(type(loss), float)\n\n    def test_train_on_batch_with_pred_multi_input(self):\n        x1 = torch.rand(ModelMultiInputTest.batch_size, 1)\n        x2 = torch.rand(ModelMultiInputTest.batch_size, 1)\n        y = torch.rand(ModelMultiInputTest.batch_size, 1)\n        loss, pred_y = self.model.train_on_batch((x1, x2), y, return_pred=True)\n        self.assertEqual(type(loss), float)\n        self.assertEqual(pred_y.shape, (ModelMultiInputTest.batch_size, 1))\n\n    def test_ndarray_train_on_batch_multi_input(self):\n        x1 = np.random.rand(ModelMultiInputTest.batch_size, 1).astype(np.float32)\n        x2 = np.random.rand(ModelMultiInputTest.batch_size, 1).astype(np.float32)\n        y = np.random.rand(ModelMultiInputTest.batch_size, 1).astype(np.float32)\n        loss = self.model.train_on_batch((x1, x2), y)\n        self.assertEqual(type(loss), float)\n\n    def test_evaluate_multi_input(self):\n        x = (torch.rand(ModelMultiInputTest.evaluate_dataset_len,\n                        1), torch.rand(ModelMultiInputTest.evaluate_dataset_len, 1))\n        y = torch.rand(ModelMultiInputTest.evaluate_dataset_len, 1)\n        loss = self.model.evaluate(x, y, batch_size=ModelMultiInputTest.batch_size)\n        self.assertEqual(type(loss), float)\n\n    def test_evaluate_with_pred_multi_input(self):\n        x = (torch.rand(ModelMultiInputTest.evaluate_dataset_len,\n                        1), torch.rand(ModelMultiInputTest.evaluate_dataset_len, 1))\n        y = torch.rand(ModelMultiInputTest.evaluate_dataset_len, 1)\n        # We also test the unpacking.\n        _, pred_y = self.model.evaluate(x, y, batch_size=ModelMultiInputTest.batch_size, return_pred=True)\n        self.assertEqual(pred_y.shape, (ModelMultiInputTest.evaluate_dataset_len, 1))\n\n    def test_evaluate_with_np_array_multi_input(self):\n        x1 = np.random.rand(ModelMultiInputTest.evaluate_dataset_len, 1).astype(np.float32)\n        x2 = np.random.rand(ModelMultiInputTest.evaluate_dataset_len, 1).astype(np.float32)\n        x = (x1, x2)\n        y = np.random.rand(ModelMultiInputTest.evaluate_dataset_len, 1).astype(np.float32)\n        loss, pred_y = self.model.evaluate(x, y, batch_size=ModelMultiInputTest.batch_size, return_pred=True)\n        self.assertEqual(type(loss), float)\n        self.assertEqual(pred_y.shape, (ModelMultiInputTest.evaluate_dataset_len, 1))\n\n    def test_evaluate_data_loader_multi_input(self):\n        x1 = torch.rand(ModelMultiInputTest.evaluate_dataset_len, 1)\n        x2 = torch.rand(ModelMultiInputTest.evaluate_dataset_len, 1)\n        y = torch.rand(ModelMultiInputTest.evaluate_dataset_len, 1)\n        dataset = TensorDataset((x1, x2), y)\n        generator = DataLoader(dataset, ModelMultiInputTest.batch_size)\n        loss, pred_y = self.model.evaluate_generator(generator, return_pred=True)\n        self.assertEqual(type(loss), float)\n        self.assertEqual(pred_y.shape, (ModelMultiInputTest.evaluate_dataset_len, 1))\n\n    def test_evaluate_generator_multi_input(self):\n        num_steps = 10\n        generator = some_data_tensor_generator_multi_input(ModelMultiInputTest.batch_size)\n        loss, pred_y = self.model.evaluate_generator(generator, steps=num_steps, return_pred=True)\n        self.assertEqual(type(loss), float)\n        self.assertEqual(pred_y.shape, (num_steps * ModelMultiInputTest.batch_size, 1))\n\n    def test_tensor_evaluate_on_batch_multi_input(self):\n        x1 = torch.rand(ModelMultiInputTest.batch_size, 1)\n        x2 = torch.rand(ModelMultiInputTest.batch_size, 1)\n        y = torch.rand(ModelMultiInputTest.batch_size, 1)\n        loss = self.model.evaluate_on_batch((x1, x2), y)\n        self.assertEqual(type(loss), float)\n\n    def test_predict_multi_input(self):\n        x = (torch.rand(ModelMultiInputTest.evaluate_dataset_len,\n                        1), torch.rand(ModelMultiInputTest.evaluate_dataset_len, 1))\n        pred_y = self.model.predict(x, batch_size=ModelMultiInputTest.batch_size)\n        self.assertEqual(pred_y.shape, (ModelMultiInputTest.evaluate_dataset_len, 1))\n\n    def test_predict_with_np_array_multi_input(self):\n        x1 = np.random.rand(ModelMultiInputTest.evaluate_dataset_len, 1).astype(np.float32)\n        x2 = np.random.rand(ModelMultiInputTest.evaluate_dataset_len, 1).astype(np.float32)\n        x = (x1, x2)\n        pred_y = self.model.predict(x, batch_size=ModelMultiInputTest.batch_size)\n        self.assertEqual(pred_y.shape, (ModelMultiInputTest.evaluate_dataset_len, 1))\n\n    def test_predict_generator_multi_input(self):\n        num_steps = 10\n        generator = some_data_tensor_generator_multi_input(ModelMultiInputTest.batch_size)\n        generator = (x for x, _ in generator)\n        pred_y = self.model.predict_generator(generator, steps=num_steps)\n        self.assertEqual(type(pred_y), np.ndarray)\n        self.assertEqual(pred_y.shape, (num_steps * ModelMultiInputTest.batch_size, 1))\n\n    def test_tensor_predict_on_batch_multi_input(self):\n        x1 = torch.rand(ModelMultiInputTest.batch_size, 1)\n        x2 = torch.rand(ModelMultiInputTest.batch_size, 1)\n        pred_y = self.model.predict_on_batch((x1, x2))\n        self.assertEqual(pred_y.shape, (ModelMultiInputTest.batch_size, 1))\n"""
tests/framework/model/test_multi_io.py,20,"b""import numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom poutyne.framework import Model, warning_settings\n\nfrom .base import ModelFittingTestCase, MultiIOModel\n\nwarning_settings['concatenate_returns'] = 'ignore'\n\n\ndef some_data_tensor_generator_multi_io(batch_size):\n    while True:\n        x1 = torch.rand(batch_size, 1)\n        x2 = torch.rand(batch_size, 1)\n        y1 = torch.rand(batch_size, 1)\n        y2 = torch.rand(batch_size, 1)\n        yield (x1, x2), (y1, y2)\n\n\nclass ModelMultiIOTest(ModelFittingTestCase):\n    def setUp(self):\n        super().setUp()\n        torch.manual_seed(42)\n        self.pytorch_network = MultiIOModel(num_input=2, num_output=2)\n        self.loss_function = nn.MSELoss()\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=1e-3)\n\n        self.model = Model(\n            self.pytorch_network,\n            self.optimizer,\n            lambda y_pred, y_true: self.loss_function(y_pred[0], y_true[0]) + self.loss_function(y_pred[1], y_true[1]),\n            batch_metrics=self.batch_metrics,\n            epoch_metrics=self.epoch_metrics)\n\n    def test_fitting_tensor_generator_multi_io(self):\n        train_generator = some_data_tensor_generator_multi_io(ModelMultiIOTest.batch_size)\n        valid_generator = some_data_tensor_generator_multi_io(ModelMultiIOTest.batch_size)\n        logs = self.model.fit_generator(train_generator,\n                                        valid_generator,\n                                        epochs=ModelMultiIOTest.epochs,\n                                        steps_per_epoch=ModelMultiIOTest.steps_per_epoch,\n                                        validation_steps=ModelMultiIOTest.steps_per_epoch,\n                                        callbacks=[self.mock_callback])\n        params = {'epochs': ModelMultiIOTest.epochs, 'steps': ModelMultiIOTest.steps_per_epoch}\n        self._test_callbacks_train(params, logs)\n\n    def test_fitting_with_tensor_multi_io(self):\n        train_real_steps_per_epoch = 30\n        train_batch_size = ModelMultiIOTest.batch_size\n        train_final_batch_missing_samples = 7\n        train_size = train_real_steps_per_epoch * train_batch_size - \\\n                     train_final_batch_missing_samples\n        train_x = (torch.rand(train_size, 1), torch.rand(train_size, 1))\n        train_y = (torch.rand(train_size, 1), torch.rand(train_size, 1))\n\n        valid_real_steps_per_epoch = 10\n        # valid_batch_size will be the same as train_batch_size in the fit method.\n        valid_batch_size = train_batch_size\n        valid_final_batch_missing_samples = 3\n        valid_size = valid_real_steps_per_epoch * valid_batch_size - \\\n                     valid_final_batch_missing_samples\n        valid_x = (torch.rand(valid_size, 1), torch.rand(valid_size, 1))\n        valid_y = (torch.rand(valid_size, 1), torch.rand(valid_size, 1))\n\n        logs = self.model.fit(train_x,\n                              train_y,\n                              validation_data=(valid_x, valid_y),\n                              epochs=ModelMultiIOTest.epochs,\n                              batch_size=train_batch_size,\n                              steps_per_epoch=None,\n                              validation_steps=None,\n                              callbacks=[self.mock_callback])\n        params = {'epochs': ModelMultiIOTest.epochs, 'steps': train_real_steps_per_epoch}\n        self._test_callbacks_train(params, logs)\n\n    def test_tensor_train_on_batch_multi_io(self):\n        x1 = torch.rand(ModelMultiIOTest.batch_size, 1)\n        x2 = torch.rand(ModelMultiIOTest.batch_size, 1)\n        y1 = torch.rand(ModelMultiIOTest.batch_size, 1)\n        y2 = torch.rand(ModelMultiIOTest.batch_size, 1)\n        loss = self.model.train_on_batch((x1, x2), (y1, y2))\n        self.assertEqual(type(loss), float)\n\n    def test_ndarray_train_on_batch_multi_io(self):\n        x1 = np.random.rand(ModelMultiIOTest.batch_size, 1).astype(np.float32)\n        x2 = np.random.rand(ModelMultiIOTest.batch_size, 1).astype(np.float32)\n        y1 = np.random.rand(ModelMultiIOTest.batch_size, 1).astype(np.float32)\n        y2 = np.random.rand(ModelMultiIOTest.batch_size, 1).astype(np.float32)\n        loss = self.model.train_on_batch((x1, x2), (y1, y2))\n        self.assertEqual(type(loss), float)\n\n    def test_evaluate_with_pred_multi_io(self):\n        x = (torch.rand(ModelMultiIOTest.evaluate_dataset_len, 1), torch.rand(ModelMultiIOTest.evaluate_dataset_len, 1))\n        y = (torch.rand(ModelMultiIOTest.evaluate_dataset_len, 1), torch.rand(ModelMultiIOTest.evaluate_dataset_len, 1))\n        # We also test the unpacking.\n        _, pred_y = self.model.evaluate(x, y, batch_size=ModelMultiIOTest.batch_size, return_pred=True)\n        for pred in pred_y:\n            self.assertEqual(pred.shape, (ModelMultiIOTest.evaluate_dataset_len, 1))\n\n    def test_tensor_evaluate_on_batch_multi_io(self):\n        y = (torch.rand(ModelMultiIOTest.batch_size, 1), torch.rand(ModelMultiIOTest.batch_size, 1))\n        x = (torch.rand(ModelMultiIOTest.batch_size, 1), torch.rand(ModelMultiIOTest.batch_size, 1))\n        loss = self.model.evaluate_on_batch(x, y)\n        self.assertEqual(type(loss), float)\n\n    def test_predict_with_np_array_multi_io(self):\n        x1 = np.random.rand(ModelMultiIOTest.evaluate_dataset_len, 1).astype(np.float32)\n        x2 = np.random.rand(ModelMultiIOTest.evaluate_dataset_len, 1).astype(np.float32)\n        x = (x1, x2)\n        pred_y = self.model.predict(x, batch_size=ModelMultiIOTest.batch_size)\n        for pred in pred_y:\n            self.assertEqual(pred.shape, (ModelMultiIOTest.evaluate_dataset_len, 1))\n\n    def test_predict_generator_multi_io(self):\n        num_steps = 10\n        generator = some_data_tensor_generator_multi_io(ModelMultiIOTest.batch_size)\n        generator = (x for x, _ in generator)\n        pred_y = self.model.predict_generator(generator, steps=num_steps)\n\n        for pred in pred_y:\n            self.assertEqual(pred.shape, (num_steps * ModelMultiIOTest.batch_size, 1))\n\n    def test_tensor_predict_on_batch_multi_io(self):\n        x = (torch.rand(ModelMultiIOTest.batch_size, 1), torch.rand(ModelMultiIOTest.batch_size, 1))\n        pred_y = self.model.predict_on_batch(x)\n        self._test_size_and_type_for_generator(pred_y, (ModelMultiIOTest.batch_size, 1))\n"""
tests/framework/model/test_multi_output.py,28,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nfrom poutyne.framework import Model, warning_settings\nfrom poutyne.utils import TensorDataset\n\nfrom .base import ModelFittingTestCase, MultiIOModel\n\nwarning_settings['concatenate_returns'] = 'ignore'\n\n\ndef some_data_tensor_generator_multi_output(batch_size):\n    while True:\n        x = torch.rand(batch_size, 1)\n        y1 = torch.rand(batch_size, 1)\n        y2 = torch.rand(batch_size, 1)\n        yield x, (y1, y2)\n\n\ndef some_data_tensor_generator_multi_io(batch_size):\n    while True:\n        x1 = torch.rand(batch_size, 1)\n        x2 = torch.rand(batch_size, 1)\n        y1 = torch.rand(batch_size, 1)\n        y2 = torch.rand(batch_size, 1)\n        yield (x1, x2), (y1, y2)\n\n\nclass ModelMultiOutputTest(ModelFittingTestCase):\n    def setUp(self):\n        super().setUp()\n        torch.manual_seed(42)\n        self.pytorch_network = MultiIOModel(num_input=1, num_output=2)\n        self.loss_function = nn.MSELoss()\n        self.optimizer = torch.optim.SGD(self.pytorch_network.parameters(), lr=1e-3)\n\n        self.model = Model(\n            self.pytorch_network,\n            self.optimizer,\n            lambda y_pred, y_true: self.loss_function(y_pred[0], y_true[0]) + self.loss_function(y_pred[1], y_true[1]),\n            batch_metrics=self.batch_metrics,\n            epoch_metrics=self.epoch_metrics)\n\n    def test_fitting_tensor_generator_multi_output(self):\n        train_generator = some_data_tensor_generator_multi_output(ModelMultiOutputTest.batch_size)\n        valid_generator = some_data_tensor_generator_multi_output(ModelMultiOutputTest.batch_size)\n        logs = self.model.fit_generator(train_generator,\n                                        valid_generator,\n                                        epochs=ModelMultiOutputTest.epochs,\n                                        steps_per_epoch=ModelMultiOutputTest.steps_per_epoch,\n                                        validation_steps=ModelMultiOutputTest.steps_per_epoch,\n                                        callbacks=[self.mock_callback])\n        params = {'epochs': ModelMultiOutputTest.epochs, 'steps': ModelMultiOutputTest.steps_per_epoch}\n        self._test_callbacks_train(params, logs)\n\n    def test_fitting_with_tensor_multi_output(self):\n        train_real_steps_per_epoch = 30\n        train_batch_size = ModelMultiOutputTest.batch_size\n        train_final_batch_missing_samples = 7\n        train_size = train_real_steps_per_epoch * train_batch_size - \\\n                     train_final_batch_missing_samples\n        train_x = torch.rand(train_size, 1)\n        train_y = (torch.rand(train_size, 1), torch.rand(train_size, 1))\n\n        valid_real_steps_per_epoch = 10\n        # valid_batch_size will be the same as train_batch_size in the fit method.\n        valid_batch_size = train_batch_size\n        valid_final_batch_missing_samples = 3\n        valid_size = valid_real_steps_per_epoch * valid_batch_size - \\\n                     valid_final_batch_missing_samples\n        valid_x = torch.rand(valid_size, 1)\n        valid_y = (torch.rand(valid_size, 1), torch.rand(valid_size, 1))\n\n        logs = self.model.fit(train_x,\n                              train_y,\n                              validation_data=(valid_x, valid_y),\n                              epochs=ModelMultiOutputTest.epochs,\n                              batch_size=train_batch_size,\n                              steps_per_epoch=None,\n                              validation_steps=None,\n                              callbacks=[self.mock_callback])\n        params = {'epochs': ModelMultiOutputTest.epochs, 'steps': train_real_steps_per_epoch}\n        self._test_callbacks_train(params, logs)\n\n    def test_tensor_train_on_batch_multi_output(self):\n        x = torch.rand(ModelMultiOutputTest.batch_size, 1)\n        y1 = torch.rand(ModelMultiOutputTest.batch_size, 1)\n        y2 = torch.rand(ModelMultiOutputTest.batch_size, 1)\n        loss = self.model.train_on_batch(x, (y1, y2))\n        self.assertEqual(type(loss), float)\n\n    def test_ndarray_train_on_batch_multi_output(self):\n        x = np.random.rand(ModelMultiOutputTest.batch_size, 1).astype(np.float32)\n        y1 = np.random.rand(ModelMultiOutputTest.batch_size, 1).astype(np.float32)\n        y2 = np.random.rand(ModelMultiOutputTest.batch_size, 1).astype(np.float32)\n        loss = self.model.train_on_batch(x, (y1, y2))\n        self.assertEqual(type(loss), float)\n\n    def test_evaluate_with_pred_multi_output(self):\n        y = (torch.rand(ModelMultiOutputTest.evaluate_dataset_len,\n                        1), torch.rand(ModelMultiOutputTest.evaluate_dataset_len, 1))\n        x = torch.rand(ModelMultiOutputTest.evaluate_dataset_len, 1)\n        # We also test the unpacking.\n        _, pred_y = self.model.evaluate(x, y, batch_size=ModelMultiOutputTest.batch_size, return_pred=True)\n        for pred in pred_y:\n            self.assertEqual(pred.shape, (ModelMultiOutputTest.evaluate_dataset_len, 1))\n\n    def test_evaluate_data_loader_multi_output(self):\n        x = torch.rand(ModelMultiOutputTest.evaluate_dataset_len, 1)\n        y1 = torch.rand(ModelMultiOutputTest.evaluate_dataset_len, 1)\n        y2 = torch.rand(ModelMultiOutputTest.evaluate_dataset_len, 1)\n        dataset = TensorDataset(x, (y1, y2))\n        generator = DataLoader(dataset, ModelMultiOutputTest.batch_size)\n        loss, pred_y = self.model.evaluate_generator(generator, return_pred=True)\n        self.assertEqual(type(loss), float)\n        for pred in pred_y:\n            self.assertEqual(type(pred), np.ndarray)\n            self.assertEqual(pred.shape, (ModelMultiOutputTest.evaluate_dataset_len, 1))\n\n    def test_evaluate_generator_multi_output(self):\n        num_steps = 10\n        generator = some_data_tensor_generator_multi_output(ModelMultiOutputTest.batch_size)\n        loss, pred_y = self.model.evaluate_generator(generator, steps=num_steps, return_pred=True)\n        self.assertEqual(type(loss), float)\n        for pred in pred_y:\n            self.assertEqual(pred.shape, (num_steps * ModelMultiOutputTest.batch_size, 1))\n\n    def test_evaluate_generator_multi_io(self):\n        num_steps = 10\n        generator = some_data_tensor_generator_multi_io(ModelMultiOutputTest.batch_size)\n        loss, pred_y = self.model.evaluate_generator(generator, steps=num_steps, return_pred=True)\n        self.assertEqual(type(loss), float)\n        for pred in pred_y:\n            self.assertEqual(pred.shape, (num_steps * ModelMultiOutputTest.batch_size, 1))\n\n    def test_tensor_evaluate_on_batch_multi_output(self):\n        y1 = torch.rand(ModelMultiOutputTest.batch_size, 1)\n        y2 = torch.rand(ModelMultiOutputTest.batch_size, 1)\n        x = torch.rand(ModelMultiOutputTest.batch_size, 1)\n        loss = self.model.evaluate_on_batch(x, (y1, y2))\n        self.assertEqual(type(loss), float)\n\n    def test_predict_with_np_array_multi_output(self):\n        x = np.random.rand(ModelMultiOutputTest.evaluate_dataset_len, 1).astype(np.float32)\n\n        pred_y = self.model.predict(x, batch_size=ModelMultiOutputTest.batch_size)\n        for pred in pred_y:\n            self.assertEqual(pred.shape, (ModelMultiOutputTest.evaluate_dataset_len, 1))\n\n    def test_predict_generator_multi_output(self):\n        num_steps = 10\n        generator = some_data_tensor_generator_multi_output(ModelMultiOutputTest.batch_size)\n        generator = (x for x, _ in generator)\n        pred_y = self.model.predict_generator(generator, steps=num_steps)\n\n        for pred in pred_y:\n            self.assertEqual(pred.shape, (num_steps * ModelMultiOutputTest.batch_size, 1))\n\n    def test_tensor_predict_on_batch_multi_output(self):\n        x = torch.rand(ModelMultiOutputTest.batch_size, 1)\n        pred_y = self.model.predict_on_batch(x)\n        self._test_size_and_type_for_generator(pred_y, (ModelMultiOutputTest.batch_size, 1))\n"""
poutyne/framework/metrics/epoch_metrics/__init__.py,0,b'# pylint: disable=wildcard-import\nfrom .base import *\nfrom .fscores import *\nfrom .sklearn_metrics import *\n'
poutyne/framework/metrics/epoch_metrics/base.py,1,"b'from abc import ABC, abstractmethod\n\nimport torch.nn as nn\n\n\nclass EpochMetric(ABC, nn.Module):\n    """"""\n    The abstract class representing a epoch metric which can be accumulated at each batch and calculated at the end\n    of the epoch.\n    """"""\n\n    @abstractmethod\n    def forward(self, y_pred, y_true) -> None:\n        """"""\n        To define the behavior of the metric when called.\n\n        Args:\n            y_pred: The prediction of the model.\n            y_true: Target to evaluate the model.\n        """"""\n        pass\n\n    @abstractmethod\n    def get_metric(self):\n        """"""\n        Compute and return the metric. Should not modify the state of\n        the epoch metric.\n        """"""\n        pass\n\n    @abstractmethod\n    def reset(self) -> None:\n        """"""\n        The information kept for the computation of the metric is cleaned so\n        that a new epoch can be done.\n        """"""\n        pass\n'"
poutyne/framework/metrics/epoch_metrics/fscores.py,16,"b'""""""\nThe source code of this file was copied from the AllenNLP project, and has been modified.\n\nCopyright 2019 AllenNLP\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nfrom typing import Optional, Union, List, Tuple\nimport torch\nfrom .base import EpochMetric\nfrom ..metrics_registering import register_epoch_metric\n\n\nclass FBeta(EpochMetric):\n    """"""\n    The source code of this class is under the Apache v2 License and was copied from\n    the AllenNLP project and has been modified.\n\n    Compute precision, recall, F-measure and support for each class.\n\n    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n    true positives and ``fp`` the number of false positives. The precision is\n    intuitively the ability of the classifier not to label as positive a sample\n    that is negative.\n\n    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n    true positives and ``fn`` the number of false negatives. The recall is\n    intuitively the ability of the classifier to find all the positive samples.\n\n    The F-beta score can be interpreted as a weighted harmonic mean of\n    the precision and recall, where an F-beta score reaches its best\n    value at 1 and worst score at 0.\n\n    If we have precision and recall, the F-beta score is simply:\n    ``F-beta = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall)``\n\n    The F-beta score weights recall more than precision by a factor of\n    ``beta``. ``beta == 1.0`` means recall and precision are equally important.\n\n    The support is the number of occurrences of each class in ``y_true``.\n\n    Args:\n        metric (Optional[str]): One of {\'fscore\', \'precision\', \'recall\'}.\n            Whether to return the F-score, the precision or the recall. When not\n            provided, all three metrics are returned. (Default value = None)\n        average (Union[str, int]): One of {\'micro\' (default), \'macro\', label_number}\n            If the argument is of type integer, the score for this class (the label number) is calculated.\n            Otherwise, this determines the type of averaging performed on the data:\n\n            ``\'micro\'``:\n                Calculate metrics globally by counting the total true positives,\n                false negatives and false positives.\n            ``\'macro\'``:\n                Calculate metrics for each label, and find their unweighted mean.\n                This does not take label imbalance into account.\n\n            (Default value = \'micro\')\n        beta (float):\n            The strength of recall versus precision in the F-score. (Default value = 1.0)\n        names (Optional[Union[str, List[str]]]): The names associated to the metrics. It is a string when\n            a single metric is requested. It is a list of 3 strings if all metrics are requested.\n            (Default value = None)\n    """"""\n\n    def __init__(self,\n                 metric: Optional[str] = None,\n                 average: Union[str, int] = \'micro\',\n                 beta: float = 1.0,\n                 names: Optional[Union[str, List[str]]] = None) -> None:\n        super().__init__()\n        self.metric_options = (\'fscore\', \'precision\', \'recall\')\n        if metric is not None and metric not in self.metric_options:\n            raise ValueError(""`metric` has to be one of {}."".format(self.metric_options))\n\n        average_options = (\'micro\', \'macro\')\n        if average not in average_options and not isinstance(average, int):\n            raise ValueError(""`average` has to be one of {} or an integer."".format(average_options))\n\n        if beta <= 0:\n            raise ValueError(""`beta` should be >0 in the F-beta score."")\n\n        self._metric = metric\n        self._average = average if average in average_options else None\n        self._label = average if isinstance(average, int) else None\n        self._beta = beta\n        self.__name__ = self._get_name(names)\n\n        # statistics\n        # the total number of true positive instances under each class\n        # Shape: (num_classes, )\n        self.register_buffer(\'_true_positive_sum\', None)\n        # the total number of instances\n        # Shape: (num_classes, )\n        self.register_buffer(\'_total_sum\', None)\n        # the total number of instances under each _predicted_ class,\n        # including true positives and false positives\n        # Shape: (num_classes, )\n        self.register_buffer(\'_pred_sum\', None)\n        # the total number of instances under each _true_ class,\n        # including true positives and false negatives\n        # Shape: (num_classes, )\n        self.register_buffer(\'_true_sum\', None)\n\n    def _get_name(self, names):\n        if self._metric is None:\n            if self._average is not None:\n                default_name = [m + \'_\' + self._average for m in self.metric_options]\n            else:\n                default_name = [m + \'_\' + str(self._label) for m in self.metric_options]\n        else:\n            if self._average is not None:\n                default_name = self._metric + \'_\' + self._average\n            else:\n                default_name = self._metric + \'_\' + str(self._label)\n\n        if names is not None:\n            self._validate_supplied_names(names, default_name)\n            return names\n\n        return default_name\n\n    def _validate_supplied_names(self, names, default_name):\n        names_list = [names] if isinstance(names, str) else names\n        default_name = [default_name] if isinstance(default_name, str) else default_name\n        if len(names_list) != len(default_name):\n            raise ValueError(""`names` should contain names for {} metrics."".format(len(default_name)))\n\n    def forward(self, y_pred: torch.Tensor, y_true: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]) -> None:\n        """"""\n        Update the confusion matrix for calculating the F-score.\n\n        Args:\n            y_pred (torch.Tensor): A tensor of predictions of shape (batch_size, ..., num_classes).\n            y_true (Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]):\n                Ground truths. A tensor of the integer class label of shape (batch_size, ...). It must\n                be the same shape as the ``y_pred`` tensor without the ``num_classes`` dimension.\n                It can also be a tuple with two tensors of the same shape, the first being the\n                ground truths and the second being a mask.\n        """"""\n\n        mask = None\n        if isinstance(y_true, tuple):\n            y_true, mask = y_true\n\n        # Calculate true_positive_sum, true_negative_sum, pred_sum, true_sum\n        num_classes = y_pred.size(1)\n        if (y_true >= num_classes).any():\n            raise ValueError(""A gold label passed to FBetaMeasure contains ""\n                             ""an id >= {}, the number of classes."".format(num_classes))\n\n        # It means we call this metric at the first time\n        # when `self._true_positive_sum` is None.\n        if self._true_positive_sum is None:\n            self._true_positive_sum = torch.zeros(num_classes, device=y_pred.device)\n            self._true_sum = torch.zeros(num_classes, device=y_pred.device)\n            self._pred_sum = torch.zeros(num_classes, device=y_pred.device)\n            self._total_sum = torch.zeros(num_classes, device=y_pred.device)\n\n        if mask is None:\n            mask = torch.ones_like(y_true)\n        mask = mask.to(dtype=torch.bool)\n        y_true = y_true.float()\n\n        argmax_y_pred = y_pred.max(dim=1)[1].float()\n        true_positives = (y_true == argmax_y_pred) * mask\n        true_positives_bins = y_true[true_positives]\n\n        # Watch it:\n        # The total numbers of true positives under all _predicted_ classes are zeros.\n        if true_positives_bins.shape[0] == 0:\n            true_positive_sum = torch.zeros(num_classes, device=y_pred.device)\n        else:\n            true_positive_sum = torch.bincount(true_positives_bins.long(), minlength=num_classes).float()\n\n        pred_bins = argmax_y_pred[mask].long()\n        # Watch it:\n        # When the `mask` is all 0, we will get an _empty_ tensor.\n        if pred_bins.shape[0] != 0:\n            pred_sum = torch.bincount(pred_bins, minlength=num_classes).float()\n        else:\n            pred_sum = torch.zeros(num_classes, device=y_pred.device)\n\n        y_true_bins = y_true[mask].long()\n        if y_true.shape[0] != 0:\n            true_sum = torch.bincount(y_true_bins, minlength=num_classes).float()\n        else:\n            true_sum = torch.zeros(num_classes, device=y_pred.device)\n\n        self._true_positive_sum += true_positive_sum\n        self._pred_sum += pred_sum\n        self._true_sum += true_sum\n        self._total_sum += mask.sum().to(torch.float)\n\n    def get_metric(self) -> Union[float, List[float]]:\n        """"""\n        Returns either a float if a single metric is set in the ``__init__`` or a list\n        of floats [f-score, precision, recall] if all metrics are requested.\n        """"""\n        if self._true_positive_sum is None:\n            raise RuntimeError(""You never call this metric before."")\n\n        tp_sum = self._true_positive_sum\n        pred_sum = self._pred_sum\n        true_sum = self._true_sum\n\n        if self._average == \'micro\':\n            tp_sum = tp_sum.sum()\n            pred_sum = pred_sum.sum()\n            true_sum = true_sum.sum()\n\n        beta2 = self._beta**2\n        # Finally, we have all our sufficient statistics.\n        precision = _prf_divide(tp_sum, pred_sum)\n        recall = _prf_divide(tp_sum, true_sum)\n        fscore = ((1 + beta2) * precision * recall / (beta2 * precision + recall))\n        fscore[tp_sum == 0] = 0.0\n\n        if self._average == \'macro\':\n            precision = precision.mean()\n            recall = recall.mean()\n            fscore = fscore.mean()\n\n        if self._label is not None:\n            # Retain only selected labels and order them\n            precision = precision[self._label]\n            recall = recall[self._label]\n            fscore = fscore[self._label]\n\n        if self._metric is None:\n            return [fscore.item(), precision.item(), recall.item()]\n\n        if self._metric == \'fscore\':\n            return fscore.item()\n        if self._metric == \'precision\':\n            return precision.item()\n        #if self._metric == \'recall\':\n        return recall.item()\n\n    def reset(self) -> None:\n        self._true_positive_sum = None\n        self._pred_sum = None\n        self._true_sum = None\n        self._total_sum = None\n\n\n@register_epoch_metric\nclass F1(FBeta):\n    """"""\n    Alias class for FBeta where ``metric == \'fscore\'`` and ``beta == 1``.\n    """"""\n\n    def __init__(self, average=\'micro\'):\n        super().__init__(metric=\'fscore\', average=average, beta=1)\n\n\ndef _prf_divide(numerator, denominator):\n    """"""Performs division and handles divide-by-zero.\n\n    On zero-division, sets the corresponding result elements to zero.\n    """"""\n    result = numerator / denominator\n    mask = denominator == 0.0\n    if not mask.any():\n        return result\n\n    # remove nan\n    result[mask] = 0.0\n    return result\n'"
poutyne/framework/metrics/epoch_metrics/sklearn_metrics.py,3,"b'from typing import Optional, Union, List, Callable, Dict, Tuple\nimport numpy as np\nimport torch\nfrom .base import EpochMetric\n\n\nclass SKLearnMetrics(EpochMetric):\n    """"""\n    Wrap metrics with Scikit-learn-like interface\n    (``metric(y_true, y_pred, sample_weight=sample_weight, **kwargs)``).\n    The ``SKLearnMetrics`` object has to keep in memory the ground truths and\n    predictions so that in can compute the metric at the end.\n\n    Example:\n        .. code-block:: python\n\n            from sklearn.metrics import roc_auc_score, average_precision_score\n            from poutyne.framework.metrics import SKLearnMetrics\n            my_epoch_metric = SKLearnMetrics([roc_auc_score, average_precision_score])\n\n    Args:\n        funcs (Union[Callable, List[Callable]]): A metric or a list of metrics with a\n            scikit-learn-like interface.\n        kwargs (Optional[Union[dict, List[dict]]]): Optional dictionary of list of dictionaries\n            corresponding to keyword arguments to pass to each corresponding metric.\n            (Default value = None)\n        names (Optional[Union[str, List[str]]]): Optional string or list of strings corresponding to\n            the names given to the metrics. (Default value = None)\n    """"""\n\n    def __init__(self,\n                 funcs: Union[Callable, List[Callable]],\n                 kwargs: Optional[Union[dict, List[dict]]] = None,\n                 names: Optional[Union[str, List[str]]] = None) -> None:\n        super().__init__()\n\n        self.funcs = funcs if isinstance(funcs, (list, tuple)) else [funcs]\n        self.kwargs = self._validate_kwargs(kwargs)\n        self.__name__ = self._validate_names(names)\n\n        self.reset()\n\n    def _validate_kwargs(self, kwargs):\n        if kwargs is not None:\n            kwargs = kwargs if isinstance(kwargs, (list, tuple)) else [kwargs]\n            if kwargs is not None and len(self.funcs) != len(kwargs):\n                raise ValueError(""`kwargs` has to have the same length as `funcs` when provided"")\n        else:\n            kwargs = [{}] * len(self.funcs) if kwargs is None else kwargs\n        return kwargs\n\n    def _validate_names(self, names):\n        if names is not None:\n            names = names if isinstance(names, (list, tuple)) else [names]\n            if len(self.funcs) != len(names):\n                raise ValueError(""`names` has to have the same length as `funcs` when provided"")\n        else:\n            names = [func.__name__ for func in self.funcs]\n        return names\n\n    def forward(self, y_pred: torch.Tensor, y_true: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]) -> None:\n        """"""\n        Accumulate the predictions, ground truths and sample weights if any.\n\n        Args:\n            y_pred (torch.Tensor): A tensor of predictions of the shape expected by\n                the metric functions passed to the class.\n            y_true (Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]):\n                Ground truths. A tensor of ground truths of the shape expected by\n                the metric functions passed to the class.\n                It can also be a tuple with two tensors, the first being the\n                ground truths and the second corresponding the ``sample_weight``\n                argument passed to the metric functions in Scikit-Learn.\n        """"""\n        self.y_pred_list.append(y_pred.cpu().numpy())\n        if isinstance(y_true, (tuple, list)):\n            y_true, sample_weight = y_true\n            self.sample_weight_list.append(sample_weight.cpu().numpy())\n        self.y_true_list.append(y_true.cpu().numpy())\n\n    def get_metric(self) -> Dict:\n        """"""\n        Returns the metrics as a dictionary with the names as keys.\n        Note: This will reset the epoch metric value.\n        """"""\n        sample_weight = None\n        if len(self.sample_weight_list) != 0:\n            sample_weight = np.concatenate(self.sample_weight_list)\n        y_pred = np.concatenate(self.y_pred_list)\n        y_true = np.concatenate(self.y_true_list)\n\n        return {\n            name: func(y_true, y_pred, sample_weight=sample_weight, **kwargs)\n            for name, func, kwargs in zip(self.__name__, self.funcs, self.kwargs)\n        }\n\n    def reset(self) -> None:\n        self.y_true_list = []\n        self.y_pred_list = []\n        self.sample_weight_list = []\n'"
