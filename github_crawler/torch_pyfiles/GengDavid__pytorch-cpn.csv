file_path,api_count,code
label_transform.py,0,"b""import sys\nsys.path.insert(0, 'cocoapi/PythonAPI')\nimport os\nfrom pycocotools.coco import COCO\nfrom tqdm import tqdm\nimport json\nfrom utils.osutils import isfile\n\nanno_root = 'data/COCO2017/annotations/'\n\ndef trans_anno(ori_file, target_file, is_val):\n\tfile_exist=False\n\tno_ori=False\n\ttrain_anno = os.path.join(anno_root, target_file)\n\tif isfile(train_anno):\n\t\tfile_exist = True\n\tori_anno = os.path.join(anno_root,ori_file)\n\tif isfile(ori_anno)==False:\n\t\tno_ori = True\n\tif file_exist==False and no_ori==False:\n\t\tcoco_kps = COCO(ori_anno)\n\t\tcoco_ids = coco_kps.getImgIds()\n\t\tcatIds = coco_kps.getCatIds(catNms=['person'])\n\t\ttrain_data = []\n\t\tprint('transforming annotations...')\n\t\tfor img_id in tqdm(coco_ids):\n\t\t\timg = coco_kps.loadImgs(img_id)[0]\n\t\t\tannIds = coco_kps.getAnnIds(imgIds=img['id'], catIds=catIds)\n\t\t\tanns = coco_kps.loadAnns(annIds)\n\t\t\tfor ann in anns:\n\t\t\t\tif ann['num_keypoints']==0:\n\t\t\t\t\tcontinue\n\t\t\t\tsingle_data = {}\n\t\t\t\tkeypoints = ann['keypoints']\n\t\t\t\tbbox = ann['bbox']\n\t\t\t\tnum_keypoints = ann['num_keypoints']\n\t\t\t\tfile_name = img['file_name']\n\t\t\t\tunit = {}\n\t\t\t\tunit['num_keypoints'] = num_keypoints\n\t\t\t\tunit['keypoints'] = keypoints\n\t\t\t\tx1,y1,width,height = bbox\n\t\t\t\tx2 = x1+width\n\t\t\t\ty2 = y1+height\n\t\t\t\tunit['GT_bbox'] = [int(x1),int(y1),int(x2),int(y2)]\n\t\t\t\tsingle_data['unit'] = unit\n\t\t\t\timgInfo = {}\n\t\t\t\timgInfo['imgID'] = img_id\n\t\t\t\timgInfo['img_paths'] = file_name\n\t\t\t\tsingle_data['imgInfo'] = imgInfo\n\t\t\t\tif is_val==False:\n\t\t\t\t\tfor i in range(4):\n\t\t\t\t\t\ttmp = single_data.copy()\n\t\t\t\t\t\ttmp['operation'] = i\n\t\t\t\t\t\ttrain_data.append(tmp)\n\t\t\t\telse:\n\t\t\t\t\tsingle_data['score'] = 1\n\t\t\t\t\ttrain_data.append(single_data)\n\t\tprint('saving transformed annotation...')\n\t\twith open(train_anno,'w') as wf:\n\t\t    json.dump(train_data, wf)\n\t\tprint('done')\n\tif no_ori:\n\t\tprint('''WARNING! There is no annotation file find at {}. \n\t\t\tMake sure you have put annotation files into the right folder.'''\n\t\t\t.format(ori_anno))\n\ntrans_anno('person_keypoints_train2017.json', 'COCO_2017_train.json', False)\ntrans_anno('person_keypoints_val2017.json', 'COCO_2017_val.json', True)\n"""
256.192.model/config.py,0,"b""import os\nimport os.path\nimport sys\nimport numpy as np\n\ndef add_pypath(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n        \nclass Config:\n    cur_dir = os.path.dirname(os.path.abspath(__file__))\n    this_dir_name = cur_dir.split('/')[-1]\n    root_dir = os.path.join(cur_dir, '..')\n\n    model = 'CPN50'\n\n    lr = 5e-4\n    lr_gamma = 0.5\n    lr_dec_epoch = list(range(6,40,6))\n\n    batch_size = 32\n    weight_decay = 1e-5\n\n    num_class = 17\n    img_path = os.path.join(root_dir, 'data', 'COCO2017', 'train2017')\n    symmetry = [(1, 2), (3, 4), (5, 6), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16)]\n    bbox_extend_factor = (0.1, 0.15) # x, y\n\n    # data augmentation setting\n    scale_factor=(0.7, 1.35)\n    rot_factor=45\n\n    pixel_means = np.array([122.7717, 115.9465, 102.9801]) # RGB\n    data_shape = (256, 192)\n    output_shape = (64, 48)\n    gaussain_kernel = (7, 7)\n    \n    gk15 = (15, 15)\n    gk11 = (11, 11)\n    gk9 = (9, 9)\n    gk7 = (7, 7)\n\n    gt_path = os.path.join(root_dir, 'data', 'COCO2017', 'annotations', 'COCO_2017_train.json')\n\ncfg = Config()\nadd_pypath(cfg.root_dir)\n\n"""
256.192.model/test.py,9,"b'import os\nimport sys\nimport argparse\nimport time\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torchvision.datasets as datasets\nimport cv2\nimport json\nimport numpy as np\n\nfrom test_config import cfg\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom utils.logger import Logger\nfrom utils.evaluation import accuracy, AverageMeter, final_preds\nfrom utils.misc import save_model, adjust_learning_rate\nfrom utils.osutils import mkdir_p, isfile, isdir, join\nfrom utils.transforms import fliplr, flip_back\nfrom utils.imutils import im_to_numpy, im_to_torch\nfrom networks import network \nfrom dataloader.mscocoMulti import MscocoMulti\nfrom tqdm import tqdm\n\ndef main(args):\n    # create model\n    model = network.__dict__[cfg.model](cfg.output_shape, cfg.num_class, pretrained = False)\n    model = torch.nn.DataParallel(model).cuda()\n\n    test_loader = torch.utils.data.DataLoader(\n        MscocoMulti(cfg, train=False),\n        batch_size=args.batch*args.num_gpus, shuffle=False,\n        num_workers=args.workers, pin_memory=True) \n\n    # load trainning weights\n    checkpoint_file = os.path.join(args.checkpoint, args.test+\'.pth.tar\')\n    checkpoint = torch.load(checkpoint_file)\n    model.load_state_dict(checkpoint[\'state_dict\'])\n    print(""=> loaded checkpoint \'{}\' (epoch {})"".format(checkpoint_file, checkpoint[\'epoch\']))\n    \n    # change to evaluation mode\n    model.eval()\n    \n    print(\'testing...\')\n    full_result = []\n    for i, (inputs, meta) in tqdm(enumerate(test_loader)):\n        with torch.no_grad():\n            input_var = torch.autograd.Variable(inputs.cuda())\n            if args.flip == True:\n                flip_inputs = inputs.clone()\n                for i, finp in enumerate(flip_inputs):\n                    finp = im_to_numpy(finp)\n                    finp = cv2.flip(finp, 1)\n                    flip_inputs[i] = im_to_torch(finp)\n                flip_input_var = torch.autograd.Variable(flip_inputs.cuda())\n\n            # compute output\n            global_outputs, refine_output = model(input_var)\n            score_map = refine_output.data.cpu()\n            score_map = score_map.numpy()\n\n            if args.flip == True:\n                flip_global_outputs, flip_output = model(flip_input_var)\n                flip_score_map = flip_output.data.cpu()\n                flip_score_map = flip_score_map.numpy()\n\n                for i, fscore in enumerate(flip_score_map):\n                    fscore = fscore.transpose((1,2,0))\n                    fscore = cv2.flip(fscore, 1)\n                    fscore = list(fscore.transpose((2,0,1)))\n                    for (q, w) in cfg.symmetry:\n                       fscore[q], fscore[w] = fscore[w], fscore[q] \n                    fscore = np.array(fscore)\n                    score_map[i] += fscore\n                    score_map[i] /= 2\n\n            ids = meta[\'imgID\'].numpy()\n            det_scores = meta[\'det_scores\']\n            for b in range(inputs.size(0)):\n                details = meta[\'augmentation_details\']\n                single_result_dict = {}\n                single_result = []\n                \n                single_map = score_map[b]\n                r0 = single_map.copy()\n                r0 /= 255\n                r0 += 0.5\n                v_score = np.zeros(17)\n                for p in range(17): \n                    single_map[p] /= np.amax(single_map[p])\n                    border = 10\n                    dr = np.zeros((cfg.output_shape[0] + 2*border, cfg.output_shape[1]+2*border))\n                    dr[border:-border, border:-border] = single_map[p].copy()\n                    dr = cv2.GaussianBlur(dr, (21, 21), 0)\n                    lb = dr.argmax()\n                    y, x = np.unravel_index(lb, dr.shape)\n                    dr[y, x] = 0\n                    lb = dr.argmax()\n                    py, px = np.unravel_index(lb, dr.shape)\n                    y -= border\n                    x -= border\n                    py -= border + y\n                    px -= border + x\n                    ln = (px ** 2 + py ** 2) ** 0.5\n                    delta = 0.25\n                    if ln > 1e-3:\n                        x += delta * px / ln\n                        y += delta * py / ln\n                    x = max(0, min(x, cfg.output_shape[1] - 1))\n                    y = max(0, min(y, cfg.output_shape[0] - 1))\n                    resy = float((4 * y + 2) / cfg.data_shape[0] * (details[b][3] - details[b][1]) + details[b][1])\n                    resx = float((4 * x + 2) / cfg.data_shape[1] * (details[b][2] - details[b][0]) + details[b][0])\n                    v_score[p] = float(r0[p, int(round(y)+1e-10), int(round(x)+1e-10)])                \n                    single_result.append(resx)\n                    single_result.append(resy)\n                    single_result.append(1)   \n                if len(single_result) != 0:\n                    single_result_dict[\'image_id\'] = int(ids[b])\n                    single_result_dict[\'category_id\'] = 1\n                    single_result_dict[\'keypoints\'] = single_result\n                    single_result_dict[\'score\'] = float(det_scores[b])*v_score.mean()\n                    full_result.append(single_result_dict)\n\n    result_path = args.result\n    if not isdir(result_path):\n        mkdir_p(result_path)\n    result_file = os.path.join(result_path, \'result.json\')\n    with open(result_file,\'w\') as wf:\n        json.dump(full_result, wf)\n\n    # evaluate on COCO\n    eval_gt = COCO(cfg.ori_gt_path)\n    eval_dt = eval_gt.loadRes(result_file)\n    cocoEval = COCOeval(eval_gt, eval_dt, iouType=\'keypoints\')\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    cocoEval.summarize()    \n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'PyTorch CPN Test\')\n    parser.add_argument(\'-j\', \'--workers\', default=12, type=int, metavar=\'N\',\n                        help=\'number of data loading workers (default: 12)\')\n    parser.add_argument(\'-g\', \'--num_gpus\', default=1, type=int, metavar=\'N\',\n                        help=\'number of GPU to use (default: 1)\')      \n    parser.add_argument(\'-c\', \'--checkpoint\', default=\'checkpoint\', type=str, metavar=\'PATH\',\n                        help=\'path to load checkpoint (default: checkpoint)\')\n    parser.add_argument(\'-f\', \'--flip\', default=True, type=bool,\n                        help=\'flip input image during test (default: True)\')\n    parser.add_argument(\'-b\', \'--batch\', default=128, type=int,\n                        help=\'test batch size (default: 128)\')\n    parser.add_argument(\'-t\', \'--test\', default=\'CPN256x192\', type=str,\n                        help=\'using which checkpoint to be tested (default: CPN256x192\')\n    parser.add_argument(\'-r\', \'--result\', default=\'result\', type=str,\n                        help=\'path to save save result (default: result)\')\n    main(parser.parse_args())'"
256.192.model/test_config.py,0,"b""import os\nimport os.path\nimport sys\nimport numpy as np\n\ndef add_pypath(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n        \nclass Config:\n    cur_dir = os.path.dirname(os.path.abspath(__file__))\n    this_dir_name = cur_dir.split('/')[-1]\n    root_dir = os.path.join(cur_dir, '..')\n\n    model = 'CPN50' # option 'CPN50', 'CPN101'\n\n    num_class = 17\n    img_path = os.path.join(root_dir, 'data', 'COCO2017', 'val2017')\n    symmetry = [(1, 2), (3, 4), (5, 6), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16)]\n    bbox_extend_factor = (0.1, 0.15) # x, y\n\n    pixel_means = np.array([122.7717, 115.9465, 102.9801]) # RGB\n    data_shape = (256, 192)\n    output_shape = (64, 48)\n\n    use_GT_bbox = True\n    if use_GT_bbox:\n        gt_path = os.path.join(root_dir, 'data', 'COCO2017', 'annotations', 'COCO_2017_val.json')\n    else:\n        # if False, make sure you have downloaded the val_dets.json and place it into annotation folder\n        gt_path = os.path.join(root_dir, 'data', 'COCO2017', 'annotations', 'val_dets.json')\n    ori_gt_path = os.path.join(root_dir, 'data', 'COCO2017', 'annotations', 'person_keypoints_val2017.json')\n\ncfg = Config()\nadd_pypath(cfg.root_dir)\nadd_pypath(os.path.join(cfg.root_dir, 'cocoapi/PythonAPI'))"""
256.192.model/train.py,18,"b'import os\nimport argparse\nimport time\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torchvision.datasets as datasets\n\nfrom config import cfg\nfrom utils.logger import Logger\nfrom utils.evaluation import accuracy, AverageMeter, final_preds\nfrom utils.misc import save_model, adjust_learning_rate\nfrom utils.osutils import mkdir_p, isfile, isdir, join\nfrom utils.transforms import fliplr, flip_back\nfrom networks import network \nfrom dataloader.mscocoMulti import MscocoMulti\n\n\ndef main(args):\n    # create checkpoint dir\n    if not isdir(args.checkpoint):\n        mkdir_p(args.checkpoint)\n\n    # create model\n    model = network.__dict__[cfg.model](cfg.output_shape, cfg.num_class, pretrained = True)\n    model = torch.nn.DataParallel(model).cuda()\n\n    # define loss function (criterion) and optimizer\n    criterion1 = torch.nn.MSELoss().cuda() # for Global loss\n    criterion2 = torch.nn.MSELoss(reduce=False).cuda() # for refine loss\n    optimizer = torch.optim.Adam(model.parameters(),\n                                lr = cfg.lr,\n                                weight_decay=cfg.weight_decay)\n    \n    if args.resume:\n        if isfile(args.resume):\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            pretrained_dict = checkpoint[\'state_dict\']\n            model.load_state_dict(pretrained_dict)\n            args.start_epoch = checkpoint[\'epoch\']\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.resume, checkpoint[\'epoch\']))\n            logger = Logger(join(args.checkpoint, \'log.txt\'), resume=True)\n        else:\n            print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n    else:        \n        logger = Logger(join(args.checkpoint, \'log.txt\'))\n        logger.set_names([\'Epoch\', \'LR\', \'Train Loss\'])\n\n    cudnn.benchmark = True\n    print(\'    Total params: %.2fMB\' % (sum(p.numel() for p in model.parameters())/(1024*1024)*4))\n\n    train_loader = torch.utils.data.DataLoader(\n        MscocoMulti(cfg),\n        batch_size=cfg.batch_size*args.num_gpus, shuffle=True,\n        num_workers=args.workers, pin_memory=True) \n\n    for epoch in range(args.start_epoch, args.epochs):\n        lr = adjust_learning_rate(optimizer, epoch, cfg.lr_dec_epoch, cfg.lr_gamma)\n        print(\'\\nEpoch: %d | LR: %.8f\' % (epoch + 1, lr)) \n\n        # train for one epoch\n        train_loss = train(train_loader, model, [criterion1, criterion2], optimizer)\n        print(\'train_loss: \',train_loss)\n\n        # append logger file\n        logger.append([epoch + 1, lr, train_loss])\n\n        save_model({\n            \'epoch\': epoch + 1,\n            \'state_dict\': model.state_dict(),\n            \'optimizer\' : optimizer.state_dict(),\n        }, checkpoint=args.checkpoint)\n\n    logger.close()\n\n\n\ndef train(train_loader, model, criterions, optimizer):\n    # prepare for refine loss\n    def ohkm(loss, top_k):\n        ohkm_loss = 0.\n        for i in range(loss.size()[0]):\n            sub_loss = loss[i]\n            topk_val, topk_idx = torch.topk(sub_loss, k=top_k, dim=0, sorted=False)\n            tmp_loss = torch.gather(sub_loss, 0, topk_idx)\n            ohkm_loss += torch.sum(tmp_loss) / top_k\n        ohkm_loss /= loss.size()[0]\n        return ohkm_loss\n    criterion1, criterion2 = criterions\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    for i, (inputs, targets, valid, meta) in enumerate(train_loader):     \n        input_var = torch.autograd.Variable(inputs.cuda())\n\n        target15, target11, target9, target7 = targets\n        refine_target_var = torch.autograd.Variable(target7.cuda(async=True))\n        valid_var = torch.autograd.Variable(valid.cuda(async=True))\n\n        # compute output\n        global_outputs, refine_output = model(input_var)\n        score_map = refine_output.data.cpu()\n\n        loss = 0.\n        global_loss_record = 0.\n        refine_loss_record = 0.\n        # comput global loss and refine loss\n        for global_output, label in zip(global_outputs, targets):\n            num_points = global_output.size()[1]\n            global_label = label * (valid > 1.1).type(torch.FloatTensor).view(-1, num_points, 1, 1)\n            global_loss = criterion1(global_output, torch.autograd.Variable(global_label.cuda(async=True))) / 2.0\n            loss += global_loss\n            global_loss_record += global_loss.data.item()\n        refine_loss = criterion2(refine_output, refine_target_var)\n        refine_loss = refine_loss.mean(dim=3).mean(dim=2)\n        refine_loss *= (valid_var > 0.1).type(torch.cuda.FloatTensor)\n        refine_loss = ohkm(refine_loss, 8)\n        loss += refine_loss\n        refine_loss_record = refine_loss.data.item()\n\n        # record loss\n        losses.update(loss.data.item(), inputs.size(0))\n\n        # compute gradient and do Optimization step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if(i%100==0 and i!=0):\n            print(\'iteration {} | loss: {}, global loss: {}, refine loss: {}, avg loss: {}\'\n                .format(i, loss.data.item(), global_loss_record, \n                    refine_loss_record, losses.avg)) \n\n    return losses.avg\n\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'PyTorch CPN Training\')\n    parser.add_argument(\'-j\', \'--workers\', default=12, type=int, metavar=\'N\',\n                        help=\'number of data loading workers (default: 12)\')\n    parser.add_argument(\'-g\', \'--num_gpus\', default=1, type=int, metavar=\'N\',\n                        help=\'number of GPU to use (default: 1)\')    \n    parser.add_argument(\'--epochs\', default=32, type=int, metavar=\'N\',\n                        help=\'number of total epochs to run (default: 32)\')\n    parser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                        help=\'manual epoch number (useful on restarts)\')\n    parser.add_argument(\'-c\', \'--checkpoint\', default=\'checkpoint\', type=str, metavar=\'PATH\',\n                        help=\'path to save checkpoint (default: checkpoint)\')\n    parser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                        help=\'path to latest checkpoint\')\n\n\n    main(parser.parse_args())\n'"
384.288.model/config.py,0,"b""import os\nimport os.path\nimport sys\nimport numpy as np\n\ndef add_pypath(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n\nclass Config:\n    cur_dir = os.path.dirname(os.path.abspath(__file__))\n    this_dir_name = cur_dir.split('/')[-1]\n    root_dir = os.path.join(cur_dir, '..')\n\n    model = 'CPN50'\n\n    lr = 5e-4\n    lr_gamma = 0.5\n    lr_dec_epoch = list(range(6,40,6))\n\n    batch_size = 12\n    weight_decay = 1e-5\n\n    num_class = 17\n    img_path = os.path.join(root_dir, 'data', 'COCO2017', 'train2017')\n    symmetry = [(1, 2), (3, 4), (5, 6), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16)]\n\n    # data augmentation setting\n    bbox_extend_factor = (0.1, 0.15) # x, y\n    scale_factor=(0.7, 1.35)\n    rot_factor=45\n\n    pixel_means = np.array([122.7717, 115.9465, 102.9801]) # RGB\n    data_shape = (384, 288) #height, width\n    output_shape = (96, 72) #height, width\n    gaussain_kernel = (13, 13)\n    #\n    gk15 = (23, 23)\n    gk11 = (17, 17)\n    gk9 = (13, 13)\n    gk7 = (9, 9)\n    gt_path = os.path.join(root_dir, 'data', 'COCO2017', 'annotations', 'COCO_2017_train.json')\n\ncfg = Config()\nadd_pypath(cfg.root_dir)\n\n"""
384.288.model/test.py,9,"b'import os\nimport sys\nimport argparse\nimport time\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torchvision.datasets as datasets\nimport cv2\nimport json\nimport numpy as np\n\nfrom test_config import cfg\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom utils.logger import Logger\nfrom utils.evaluation import accuracy, AverageMeter, final_preds\nfrom utils.misc import save_model, adjust_learning_rate\nfrom utils.osutils import mkdir_p, isfile, isdir, join\nfrom utils.transforms import fliplr, flip_back\nfrom utils.imutils import im_to_numpy, im_to_torch\nfrom networks import network \nfrom dataloader.mscocoMulti import MscocoMulti\nfrom tqdm import tqdm\n\ndef main(args):\n    # create model\n    model = network.__dict__[cfg.model](cfg.output_shape, cfg.num_class, pretrained = False)\n    model = torch.nn.DataParallel(model).cuda()\n\n    test_loader = torch.utils.data.DataLoader(\n        MscocoMulti(cfg, train=False),\n        batch_size=args.batch*args.num_gpus, shuffle=False,\n        num_workers=args.workers, pin_memory=True) \n\n    # load trainning weights\n    checkpoint_file = os.path.join(args.checkpoint, args.test+\'.pth.tar\')\n    checkpoint = torch.load(checkpoint_file)\n    model.load_state_dict(checkpoint[\'state_dict\'])\n    print(""=> loaded checkpoint \'{}\' (epoch {})"".format(checkpoint_file, checkpoint[\'epoch\']))\n    \n    # change to evaluation mode\n    model.eval()\n    \n    print(\'testing...\')\n    full_result = []\n    for i, (inputs, meta) in tqdm(enumerate(test_loader)):\n        with torch.no_grad():\n            input_var = torch.autograd.Variable(inputs.cuda())\n            if args.flip == True:\n                flip_inputs = inputs.clone()\n                for i, finp in enumerate(flip_inputs):\n                    finp = im_to_numpy(finp)\n                    finp = cv2.flip(finp, 1)\n                    flip_inputs[i] = im_to_torch(finp)\n                flip_input_var = torch.autograd.Variable(flip_inputs.cuda())\n\n            # compute output\n            global_outputs, refine_output = model(input_var)\n            score_map = refine_output.data.cpu()\n            score_map = score_map.numpy()\n\n            if args.flip == True:\n                flip_global_outputs, flip_output = model(flip_input_var)\n                flip_score_map = flip_output.data.cpu()\n                flip_score_map = flip_score_map.numpy()\n\n                for i, fscore in enumerate(flip_score_map):\n                    fscore = fscore.transpose((1,2,0))\n                    fscore = cv2.flip(fscore, 1)\n                    fscore = list(fscore.transpose((2,0,1)))\n                    for (q, w) in cfg.symmetry:\n                       fscore[q], fscore[w] = fscore[w], fscore[q] \n                    fscore = np.array(fscore)\n                    score_map[i] += fscore\n                    score_map[i] /= 2\n\n            ids = meta[\'imgID\'].numpy()\n            det_scores = meta[\'det_scores\']\n            for b in range(inputs.size(0)):\n                details = meta[\'augmentation_details\']\n                single_result_dict = {}\n                single_result = []\n                \n                single_map = score_map[b]\n                r0 = single_map.copy()\n                r0 /= 255\n                r0 += 0.5\n                v_score = np.zeros(17)\n                for p in range(17): \n                    single_map[p] /= np.amax(single_map[p])\n                    border = 10\n                    dr = np.zeros((cfg.output_shape[0] + 2*border, cfg.output_shape[1]+2*border))\n                    dr[border:-border, border:-border] = single_map[p].copy()\n                    dr = cv2.GaussianBlur(dr, (21, 21), 0)\n                    lb = dr.argmax()\n                    y, x = np.unravel_index(lb, dr.shape)\n                    dr[y, x] = 0\n                    lb = dr.argmax()\n                    py, px = np.unravel_index(lb, dr.shape)\n                    y -= border\n                    x -= border\n                    py -= border + y\n                    px -= border + x\n                    ln = (px ** 2 + py ** 2) ** 0.5\n                    delta = 0.25\n                    if ln > 1e-3:\n                        x += delta * px / ln\n                        y += delta * py / ln\n                    x = max(0, min(x, cfg.output_shape[1] - 1))\n                    y = max(0, min(y, cfg.output_shape[0] - 1))\n                    resy = float((4 * y + 2) / cfg.data_shape[0] * (details[b][3] - details[b][1]) + details[b][1])\n                    resx = float((4 * x + 2) / cfg.data_shape[1] * (details[b][2] - details[b][0]) + details[b][0])\n                    v_score[p] = float(r0[p, int(round(y)+1e-10), int(round(x)+1e-10)])                \n                    single_result.append(resx)\n                    single_result.append(resy)\n                    single_result.append(1)   \n                if len(single_result) != 0:\n                    single_result_dict[\'image_id\'] = int(ids[b])\n                    single_result_dict[\'category_id\'] = 1\n                    single_result_dict[\'keypoints\'] = single_result\n                    single_result_dict[\'score\'] = float(det_scores[b])*v_score.mean()\n                    full_result.append(single_result_dict)\n\n    result_path = args.result\n    if not isdir(result_path):\n        mkdir_p(result_path)\n    result_file = os.path.join(result_path, \'result.json\')\n    with open(result_file,\'w\') as wf:\n        json.dump(full_result, wf)\n\n    # evaluate on COCO\n    eval_gt = COCO(cfg.ori_gt_path)\n    eval_dt = eval_gt.loadRes(result_file)\n    cocoEval = COCOeval(eval_gt, eval_dt, iouType=\'keypoints\')\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    cocoEval.summarize()    \n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'PyTorch CPN Test\')\n    parser.add_argument(\'-j\', \'--workers\', default=12, type=int, metavar=\'N\',\n                        help=\'number of data loading workers (default: 12)\')\n    parser.add_argument(\'-g\', \'--num_gpus\', default=1, type=int, metavar=\'N\',\n                        help=\'number of GPU to use (default: 1)\')      \n    parser.add_argument(\'-c\', \'--checkpoint\', default=\'checkpoint\', type=str, metavar=\'PATH\',\n                        help=\'path to load checkpoint (default: checkpoint)\')\n    parser.add_argument(\'-f\', \'--flip\', default=True, type=bool,\n                        help=\'flip input image during test (default: True)\')\n    parser.add_argument(\'-b\', \'--batch\', default=128, type=int,\n                        help=\'test batch size (default: 128)\')\n    parser.add_argument(\'-t\', \'--test\', default=\'CPN384x288\', type=str,\n                        help=\'using which checkpoint to be tested (default: CPN256x192\')\n    parser.add_argument(\'-r\', \'--result\', default=\'result\', type=str,\n                        help=\'path to save save result (default: result)\')\n    main(parser.parse_args())\n'"
384.288.model/test_config.py,0,"b""import os\nimport os.path\nimport sys\nimport numpy as np\n\ndef add_pypath(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n        \nclass Config:\n    cur_dir = os.path.dirname(os.path.abspath(__file__))\n    this_dir_name = cur_dir.split('/')[-1]\n    root_dir = os.path.join(cur_dir, '..')\n\n    model = 'CPN50' # option 'CPN50', 'CPN101'\n\n    num_class = 17\n    img_path = os.path.join(root_dir, 'data', 'COCO2017', 'val2017')\n    symmetry = [(1, 2), (3, 4), (5, 6), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16)]\n    bbox_extend_factor = (0.1, 0.15) # x, y\n\n    pixel_means = np.array([122.7717, 115.9465, 102.9801]) # RGB\n    data_shape = (384, 288) #height, width\n    output_shape = (96, 72) #height, width\n\n    use_GT_bbox = True\n    if use_GT_bbox:\n        gt_path = os.path.join(root_dir, 'data', 'COCO2017', 'annotations', 'COCO_2017_val.json')\n    else:\n        # if False, make sure you have downloaded the val_dets.json and place it into annotation folder\n        gt_path = os.path.join(root_dir, 'data', 'COCO2017', 'annotations', 'val_dets.json')\n        \n    ori_gt_path = os.path.join(root_dir, 'data', 'COCO2017', 'annotations', 'person_keypoints_val2017.json')\n\ncfg = Config()\nadd_pypath(cfg.root_dir)\nadd_pypath(os.path.join(cfg.root_dir, 'cocoapi/PythonAPI'))\n"""
384.288.model/train.py,18,"b'import os\nimport argparse\nimport time\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torchvision.datasets as datasets\n\nfrom config import cfg\nfrom utils.logger import Logger\nfrom utils.evaluation import accuracy, AverageMeter, final_preds\nfrom utils.misc import save_model, adjust_learning_rate\nfrom utils.osutils import mkdir_p, isfile, isdir, join\nfrom utils.transforms import fliplr, flip_back\nfrom networks import network \nfrom dataloader.mscocoMulti import MscocoMulti\n\n\ndef main(args):\n    # create checkpoint dir\n    if not isdir(args.checkpoint):\n        mkdir_p(args.checkpoint)\n\n    # create model\n    model = network.__dict__[cfg.model](cfg.output_shape, cfg.num_class, pretrained = True)\n    model = torch.nn.DataParallel(model).cuda()\n\n    # define loss function (criterion) and optimizer\n    criterion1 = torch.nn.MSELoss().cuda() # for Global loss\n    criterion2 = torch.nn.MSELoss(reduce=False).cuda() # for refine loss\n    optimizer = torch.optim.Adam(model.parameters(),\n                                lr = cfg.lr,\n                                weight_decay=cfg.weight_decay)\n    \n    if args.resume:\n        if isfile(args.resume):\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            pretrained_dict = checkpoint[\'state_dict\']\n            model.load_state_dict(pretrained_dict)\n            args.start_epoch = checkpoint[\'epoch\']\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.resume, checkpoint[\'epoch\']))\n            logger = Logger(join(args.checkpoint, \'log.txt\'), resume=True)\n        else:\n            print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n    else:        \n        logger = Logger(join(args.checkpoint, \'log.txt\'))\n        logger.set_names([\'Epoch\', \'LR\', \'Train Loss\'])\n\n    cudnn.benchmark = True\n    print(\'    Total params: %.2fMB\' % (sum(p.numel() for p in model.parameters())/(1024*1024)*4))\n\n    train_loader = torch.utils.data.DataLoader(\n        MscocoMulti(cfg),\n        batch_size=cfg.batch_size*args.num_gpus, shuffle=True,\n        num_workers=args.workers, pin_memory=True) \n\n    for epoch in range(args.start_epoch, args.epochs):\n        lr = adjust_learning_rate(optimizer, epoch, cfg.lr_dec_epoch, cfg.lr_gamma)\n        print(\'\\nEpoch: %d | LR: %.8f\' % (epoch + 1, lr)) \n\n        # train for one epoch\n        train_loss = train(train_loader, model, [criterion1, criterion2], optimizer)\n        print(\'train_loss: \',train_loss)\n\n        # append logger file\n        logger.append([epoch + 1, lr, train_loss])\n\n        save_model({\n            \'epoch\': epoch + 1,\n            \'state_dict\': model.state_dict(),\n            \'optimizer\' : optimizer.state_dict(),\n        }, checkpoint=args.checkpoint)\n\n    logger.close()\n\n\n\ndef train(train_loader, model, criterions, optimizer):\n    # prepare for refine loss\n    def ohkm(loss, top_k):\n        ohkm_loss = 0.\n        for i in range(loss.size()[0]):\n            sub_loss = loss[i]\n            topk_val, topk_idx = torch.topk(sub_loss, k=top_k, dim=0, sorted=False)\n            tmp_loss = torch.gather(sub_loss, 0, topk_idx)\n            ohkm_loss += torch.sum(tmp_loss) / top_k\n        ohkm_loss /= loss.size()[0]\n        return ohkm_loss\n    criterion1, criterion2 = criterions\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    for i, (inputs, targets, valid, meta) in enumerate(train_loader):     \n        input_var = torch.autograd.Variable(inputs.cuda())\n\n        target15, target11, target9, target7 = targets\n        refine_target_var = torch.autograd.Variable(target7.cuda(async=True))\n        valid_var = torch.autograd.Variable(valid.cuda(async=True))\n\n        # compute output\n        global_outputs, refine_output = model(input_var)\n        score_map = refine_output.data.cpu()\n\n        loss = 0.\n        global_loss_record = 0.\n        refine_loss_record = 0.\n        # comput global loss and refine loss\n        for global_output, label in zip(global_outputs, targets):\n            num_points = global_output.size()[1]\n            global_label = label * (valid > 1.1).type(torch.FloatTensor).view(-1, num_points, 1, 1)\n            global_loss = criterion1(global_output, torch.autograd.Variable(global_label.cuda(async=True))) / 2.0\n            loss += global_loss\n            global_loss_record += global_loss.data.item()\n        refine_loss = criterion2(refine_output, refine_target_var)\n        refine_loss = refine_loss.mean(dim=3).mean(dim=2)\n        refine_loss *= (valid_var > 0.1).type(torch.cuda.FloatTensor)\n        refine_loss = ohkm(refine_loss, 8)\n        loss += refine_loss\n        refine_loss_record = refine_loss.data.item()\n\n        # record loss\n        losses.update(loss.data.item(), inputs.size(0))\n\n        # compute gradient and do Optimization step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if(i%100==0 and i!=0):\n            print(\'iteration {} | loss: {}, global loss: {}, refine loss: {}, avg loss: {}\'\n                .format(i, loss.data.item(), global_loss_record, \n                    refine_loss_record, losses.avg)) \n\n    return losses.avg\n\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'PyTorch CPN Training\')\n    parser.add_argument(\'-j\', \'--workers\', default=12, type=int, metavar=\'N\',\n                        help=\'number of data loading workers (default: 12)\')\n    parser.add_argument(\'-g\', \'--num_gpus\', default=1, type=int, metavar=\'N\',\n                        help=\'number of GPU to use (default: 1)\')    \n    parser.add_argument(\'--epochs\', default=32, type=int, metavar=\'N\',\n                        help=\'number of total epochs to run (default: 32)\')\n    parser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                        help=\'manual epoch number (useful on restarts)\')\n    parser.add_argument(\'-c\', \'--checkpoint\', default=\'checkpoint\', type=str, metavar=\'PATH\',\n                        help=\'path to save checkpoint (default: checkpoint)\')\n    parser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                        help=\'path to latest checkpoint\')\n\n\n    main(parser.parse_args())'"
dataloader/__init__.py,0,b''
dataloader/mscocoMulti.py,3,"b""import os\nimport numpy as np\nimport json\nimport random\nimport math\nimport cv2\nimport skimage\nimport skimage.transform\n\nimport torch\nimport torch.utils.data as data\n\nfrom utils.osutils import *\nfrom utils.imutils import *\nfrom utils.transforms import *\n\nclass MscocoMulti(data.Dataset):\n    def __init__(self, cfg, train=True):\n        self.img_folder = cfg.img_path\n        self.is_train = train\n        self.inp_res = cfg.data_shape\n        self.out_res = cfg.output_shape\n        self.pixel_means = cfg.pixel_means\n        self.num_class = cfg.num_class\n        self.cfg = cfg\n        self.bbox_extend_factor = cfg.bbox_extend_factor\n        if train:\n            self.scale_factor = cfg.scale_factor\n            self.rot_factor = cfg.rot_factor\n            self.symmetry = cfg.symmetry\n        with open(cfg.gt_path) as anno_file:   \n            self.anno = json.load(anno_file)\n\n    def augmentationCropImage(self, img, bbox, joints=None):  \n        height, width = self.inp_res[0], self.inp_res[1]\n        bbox = np.array(bbox).reshape(4, ).astype(np.float32)\n        add = max(img.shape[0], img.shape[1])\n        mean_value = self.pixel_means\n        bimg = cv2.copyMakeBorder(img, add, add, add, add, borderType=cv2.BORDER_CONSTANT, value=mean_value.tolist())\n        objcenter = np.array([(bbox[0] + bbox[2]) / 2., (bbox[1] + bbox[3]) / 2.])      \n        bbox += add\n        objcenter += add\n        if self.is_train:\n            joints[:, :2] += add\n            inds = np.where(joints[:, -1] == 0)\n            joints[inds, :2] = -1000000 # avoid influencing by data processing\n        crop_width = (bbox[2] - bbox[0]) * (1 + self.bbox_extend_factor[0] * 2)\n        crop_height = (bbox[3] - bbox[1]) * (1 + self.bbox_extend_factor[1] * 2)\n        if self.is_train:\n            crop_width = crop_width * (1 + 0.25)\n            crop_height = crop_height * (1 + 0.25)  \n        if crop_height / height > crop_width / width:\n            crop_size = crop_height\n            min_shape = height\n        else:\n            crop_size = crop_width\n            min_shape = width  \n\n        crop_size = min(crop_size, objcenter[0] / width * min_shape * 2. - 1.)\n        crop_size = min(crop_size, (bimg.shape[1] - objcenter[0]) / width * min_shape * 2. - 1)\n        crop_size = min(crop_size, objcenter[1] / height * min_shape * 2. - 1.)\n        crop_size = min(crop_size, (bimg.shape[0] - objcenter[1]) / height * min_shape * 2. - 1)\n\n        min_x = int(objcenter[0] - crop_size / 2. / min_shape * width)\n        max_x = int(objcenter[0] + crop_size / 2. / min_shape * width)\n        min_y = int(objcenter[1] - crop_size / 2. / min_shape * height)\n        max_y = int(objcenter[1] + crop_size / 2. / min_shape * height)                               \n\n        x_ratio = float(width) / (max_x - min_x)\n        y_ratio = float(height) / (max_y - min_y)\n\n        if self.is_train:\n            joints[:, 0] = joints[:, 0] - min_x\n            joints[:, 1] = joints[:, 1] - min_y\n\n            joints[:, 0] *= x_ratio\n            joints[:, 1] *= y_ratio\n            label = joints[:, :2].copy()\n            valid = joints[:, 2].copy()\n\n        img = cv2.resize(bimg[min_y:max_y, min_x:max_x, :], (width, height))  \n        details = np.asarray([min_x - add, min_y - add, max_x - add, max_y - add]).astype(np.float)\n\n        if self.is_train:\n            return img, joints, details\n        else:\n            return img, details\n\n\n\n    def data_augmentation(self, img, label, operation):\n        height, width = img.shape[0], img.shape[1]\n        center = (width / 2., height / 2.)\n        n = label.shape[0]\n        affrat = random.uniform(self.scale_factor[0], self.scale_factor[1])\n        \n        halfl_w = min(width - center[0], (width - center[0]) / 1.25 * affrat)\n        halfl_h = min(height - center[1], (height - center[1]) / 1.25 * affrat)\n        img = skimage.transform.resize(img[int(center[1] - halfl_h): int(center[1] + halfl_h + 1),\n                             int(center[0] - halfl_w): int(center[0] + halfl_w + 1)], (height, width))\n        for i in range(n):\n            label[i][0] = (label[i][0] - center[0]) / halfl_w * (width - center[0]) + center[0]\n            label[i][1] = (label[i][1] - center[1]) / halfl_h * (height - center[1]) + center[1]\n            label[i][2] *= (\n            (label[i][0] >= 0) & (label[i][0] < width) & (label[i][1] >= 0) & (label[i][1] < height))\n\n        # flip augmentation\n        if operation == 1:\n            img = cv2.flip(img, 1)\n            cod = []\n            allc = []\n            for i in range(n):\n                x, y = label[i][0], label[i][1]\n                if x >= 0:\n                    x = width - 1 - x\n                cod.append((x, y, label[i][2]))\n            # **** the joint index depends on the dataset ****    \n            for (q, w) in self.symmetry:\n                cod[q], cod[w] = cod[w], cod[q]\n            for i in range(n):\n                allc.append(cod[i][0])\n                allc.append(cod[i][1])\n                allc.append(cod[i][2])\n            label = np.array(allc).reshape(n, 3)\n\n        # rotated augmentation\n        if operation > 1:      \n            angle = random.uniform(0, self.rot_factor)\n            if random.randint(0, 1):\n                angle *= -1\n            rotMat = cv2.getRotationMatrix2D(center, angle, 1.0)\n            img = cv2.warpAffine(img, rotMat, (width, height))\n            \n            allc = []\n            for i in range(n):\n                x, y = label[i][0], label[i][1]\n                v = label[i][2]\n                coor = np.array([x, y])\n                if x >= 0 and y >= 0:\n                    R = rotMat[:, : 2]\n                    W = np.array([rotMat[0][2], rotMat[1][2]])\n                    coor = np.dot(R, coor) + W\n                allc.append(int(coor[0]))\n                allc.append(int(coor[1]))\n                v *= ((coor[0] >= 0) & (coor[0] < width) & (coor[1] >= 0) & (coor[1] < height))\n                allc.append(int(v))\n            label = np.array(allc).reshape(n, 3).astype(np.int)\n        return img, label\n\n  \n    def __getitem__(self, index):\n        a = self.anno[index]\n        image_name = a['imgInfo']['img_paths']\n        img_path = os.path.join(self.img_folder, image_name)\n        if self.is_train:\n            points = np.array(a['unit']['keypoints']).reshape(self.num_class, 3).astype(np.float32)\n        gt_bbox = a['unit']['GT_bbox']\n\n        image = scipy.misc.imread(img_path, mode='RGB')\n        if self.is_train:\n            image, points, details = self.augmentationCropImage(image, gt_bbox, points)\n        else:\n            image, details = self.augmentationCropImage(image, gt_bbox)\n\n        if self.is_train:\n            image, points = self.data_augmentation(image, points, a['operation'])  \n            img = im_to_torch(image)  # CxHxW\n            \n            # Color dithering\n            img[0, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n            img[1, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n            img[2, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n\n            points[:, :2] //= 4 # output size is 1/4 input size\n            pts = torch.Tensor(points)\n        else:\n            img = im_to_torch(image)\n        img = color_normalize(img, self.pixel_means)\n\n        if self.is_train:\n            target15 = np.zeros((self.num_class, self.out_res[0], self.out_res[1]))\n            target11 = np.zeros((self.num_class, self.out_res[0], self.out_res[1]))\n            target9 = np.zeros((self.num_class, self.out_res[0], self.out_res[1]))\n            target7 = np.zeros((self.num_class, self.out_res[0], self.out_res[1]))\n            for i in range(self.num_class):\n                if pts[i, 2] > 0: # COCO visible: 0-no label, 1-label + invisible, 2-label + visible\n                    target15[i] = generate_heatmap(target15[i], pts[i], self.cfg.gk15)\n                    target11[i] = generate_heatmap(target11[i], pts[i], self.cfg.gk11)\n                    target9[i] = generate_heatmap(target9[i], pts[i], self.cfg.gk9)\n                    target7[i] = generate_heatmap(target7[i], pts[i], self.cfg.gk7)\n                    \n            targets = [torch.Tensor(target15), torch.Tensor(target11), torch.Tensor(target9), torch.Tensor(target7)]\n            valid = pts[:, 2]\n\n        meta = {'index' : index, 'imgID' : a['imgInfo']['imgID'], \n        'GT_bbox' : np.array([gt_bbox[0], gt_bbox[1], gt_bbox[2], gt_bbox[3]]), \n        'img_path' : img_path, 'augmentation_details' : details}\n\n        if self.is_train:\n            return img, targets, valid, meta\n        else:\n            meta['det_scores'] = a['score']\n            return img, meta\n\n    def __len__(self):\n        return len(self.anno)\n\n\n"""
networks/__init__.py,0,b''
networks/globalNet.py,3,"b""import torch.nn as nn\nimport torch\nimport math\n\nclass globalNet(nn.Module):\n    def __init__(self, channel_settings, output_shape, num_class):\n        super(globalNet, self).__init__()\n        self.channel_settings = channel_settings\n        laterals, upsamples, predict = [], [], []\n        for i in range(len(channel_settings)):\n            laterals.append(self._lateral(channel_settings[i]))\n            predict.append(self._predict(output_shape, num_class))\n            if i != len(channel_settings) - 1:\n                upsamples.append(self._upsample())\n        self.laterals = nn.ModuleList(laterals)\n        self.upsamples = nn.ModuleList(upsamples)\n        self.predict = nn.ModuleList(predict)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _lateral(self, input_size):\n        layers = []\n        layers.append(nn.Conv2d(input_size, 256,\n            kernel_size=1, stride=1, bias=False))\n        layers.append(nn.BatchNorm2d(256))\n        layers.append(nn.ReLU(inplace=True))\n\n        return nn.Sequential(*layers)\n\n    def _upsample(self):\n        layers = []\n        layers.append(torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True))\n        layers.append(torch.nn.Conv2d(256, 256,\n            kernel_size=1, stride=1, bias=False))\n        layers.append(nn.BatchNorm2d(256))\n\n        return nn.Sequential(*layers)\n\n    def _predict(self, output_shape, num_class):\n        layers = []\n        layers.append(nn.Conv2d(256, 256,\n            kernel_size=1, stride=1, bias=False))\n        layers.append(nn.BatchNorm2d(256))\n        layers.append(nn.ReLU(inplace=True))\n\n        layers.append(nn.Conv2d(256, num_class,\n            kernel_size=3, stride=1, padding=1, bias=False))\n        layers.append(nn.Upsample(size=output_shape, mode='bilinear', align_corners=True))\n        layers.append(nn.BatchNorm2d(num_class))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        global_fms, global_outs = [], []\n        for i in range(len(self.channel_settings)):\n            if i == 0:\n                feature = self.laterals[i](x[i])\n            else:\n                feature = self.laterals[i](x[i]) + up\n            global_fms.append(feature)\n            if i != len(self.channel_settings) - 1:\n                up = self.upsamples[i](feature)\n            feature = self.predict[i](feature)\n            global_outs.append(feature)\n\n        return global_fms, global_outs\n"""
networks/network.py,1,"b""from .resnet import *\nimport torch.nn as nn\nimport torch\nfrom .globalNet import globalNet\nfrom .refineNet import refineNet\n\n__all__ = ['CPN50', 'CPN101']\n\nclass CPN(nn.Module):\n    def __init__(self, resnet, output_shape, num_class, pretrained=True):\n        super(CPN, self).__init__()\n        channel_settings = [2048, 1024, 512, 256]\n        self.resnet = resnet\n        self.global_net = globalNet(channel_settings, output_shape, num_class)\n        self.refine_net = refineNet(channel_settings[-1], output_shape, num_class)\n\n    def forward(self, x):\n        res_out = self.resnet(x)\n        global_fms, global_outs = self.global_net(res_out)\n        refine_out = self.refine_net(global_fms)\n\n        return global_outs, refine_out\n\ndef CPN50(out_size,num_class,pretrained=True):\n    res50 = resnet50(pretrained=pretrained)\n    model = CPN(res50, output_shape=out_size,num_class=num_class, pretrained=pretrained)\n    return model\n\ndef CPN101(out_size,num_class,pretrained=True):\n    res101 = resnet101(pretrained=pretrained)\n    model = CPN(res101, output_shape=out_size,num_class=num_class, pretrained=pretrained)\n    return model\n"""
networks/refineNet.py,2,"b""import torch.nn as nn\nimport torch\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 2, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 2)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.downsample = nn.Sequential(\n                nn.Conv2d(inplanes, planes * 2,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * 2),\n            )\n \n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass refineNet(nn.Module):\n    def __init__(self, lateral_channel, out_shape, num_class):\n        super(refineNet, self).__init__()\n        cascade = []\n        num_cascade = 4\n        for i in range(num_cascade):\n            cascade.append(self._make_layer(lateral_channel, num_cascade-i-1, out_shape))\n        self.cascade = nn.ModuleList(cascade)\n        self.final_predict = self._predict(4*lateral_channel, num_class)\n\n    def _make_layer(self, input_channel, num, output_shape):\n        layers = []\n        for i in range(num):\n            layers.append(Bottleneck(input_channel, 128))\n        layers.append(nn.Upsample(size=output_shape, mode='bilinear', align_corners=True))\n        return nn.Sequential(*layers)\n\n    def _predict(self, input_channel, num_class):\n        layers = []\n        layers.append(Bottleneck(input_channel, 128))\n        layers.append(nn.Conv2d(256, num_class,\n            kernel_size=3, stride=1, padding=1, bias=False))\n        layers.append(nn.BatchNorm2d(num_class))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        refine_fms = []\n        for i in range(4):\n            refine_fms.append(self.cascade[i](x[i]))\n        out = torch.cat(refine_fms, dim=1)\n        out = self.final_predict(out)\n        return out\n"""
networks/resnet.py,7,"b'import torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\n\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample \n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x1 = self.layer1(x)\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n\n        return [x4, x3, x2, x1]\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        from collections import OrderedDict\n        state_dict = model.state_dict()\n        pretrained_state_dict = model_zoo.load_url(model_urls[\'resnet18\'])\n        for k, v in pretrained_state_dict.items():\n            if k not in state_dict:\n                continue\n            state_dict[k] = v\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        from collections import OrderedDict\n        state_dict = model.state_dict()\n        pretrained_state_dict = model_zoo.load_url(model_urls[\'resnet34\'])\n        for k, v in pretrained_state_dict.items():\n            if k not in state_dict:\n                continue\n            state_dict[k] = v\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        print(\'Initialize with pre-trained ResNet\')\n        from collections import OrderedDict\n        state_dict = model.state_dict()\n        pretrained_state_dict = model_zoo.load_url(model_urls[\'resnet50\'])\n        for k, v in pretrained_state_dict.items():\n            if k not in state_dict:\n                continue\n            state_dict[k] = v\n        print(\'successfully load \'+str(len(state_dict.keys()))+\' keys\')\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        print(\'Initialize with pre-trained ResNet\')\n        from collections import OrderedDict\n        state_dict = model.state_dict()\n        pretrained_state_dict = model_zoo.load_url(model_urls[\'resnet101\'])\n        for k, v in pretrained_state_dict.items():\n            if k not in state_dict:\n                continue\n            state_dict[k] = v\n        print(\'successfully load \'+str(len(state_dict.keys()))+\' keys\')\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        from collections import OrderedDict\n        state_dict = model.state_dict()\n        pretrained_state_dict = model_zoo.load_url(model_urls[\'resnet152\'])\n        for k, v in pretrained_state_dict.items():\n            if k not in state_dict:\n                continue\n            state_dict[k] = v\n        model.load_state_dict(state_dict)\n    return model'"
utils/__init__.py,0,b''
utils/evaluation.py,8,"b'import math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\nfrom .misc import *\nfrom .transforms import transform, transform_preds\n\n__all__ = [\'accuracy\', \'AverageMeter\']\n\ndef get_preds(scores):\n    \'\'\' get predictions from score maps in torch Tensor\n        return type: torch.LongTensor\n    \'\'\'\n    assert scores.dim() == 4, \'Score maps should be 4-dim\'\n    maxval, idx = torch.max(scores.view(scores.size(0), scores.size(1), -1), 2)\n\n    maxval = maxval.view(scores.size(0), scores.size(1), 1)\n    idx = idx.view(scores.size(0), scores.size(1), 1) + 1\n\n    preds = idx.repeat(1, 1, 2).float()\n\n    preds[:,:,0] = (preds[:,:,0] - 1) % scores.size(3) + 1\n    preds[:,:,1] = torch.floor((preds[:,:,1] - 1) / scores.size(3)) + 1\n\n    pred_mask = maxval.gt(0).repeat(1, 1, 2).float()\n    preds *= pred_mask\n    return preds\n\ndef calc_dists(preds, target, normalize):\n    preds = preds.float()\n    target = target.float()\n    dists = torch.zeros(preds.size(1), preds.size(0))\n    for n in range(preds.size(0)):\n        for c in range(preds.size(1)):\n            if target[n,c,0] > 1 and target[n, c, 1] > 1:\n                dists[c, n] = torch.dist(preds[n,c,:], target[n,c,:])/normalize[n]\n            else:\n                dists[c, n] = -1\n    return dists\n\ndef dist_acc(dists, thr=0.5):\n    \'\'\' Return percentage below threshold while ignoring values with a -1 \'\'\'\n    if dists.ne(-1).sum() > 0:\n        return dists.le(thr).eq(dists.ne(-1)).sum()*1.0 / dists.ne(-1).sum()\n    else:\n        return -1\n\ndef accuracy(output, target, idxs, thr=0.5):\n    \'\'\' Calculate accuracy according to PCK, but uses ground truth heatmap rather than x,y locations\n        First value to be returned is average accuracy across \'idxs\', followed by individual accuracies\n    \'\'\'\n    preds   = get_preds(output)\n    gts     = get_preds(target)\n    norm    = torch.ones(preds.size(0))*output.size(3)/10\n    dists   = calc_dists(preds, gts, norm)\n\n    acc = torch.zeros(len(idxs)+1)\n    avg_acc = 0\n    cnt = 0\n\n    for i in range(len(idxs)):\n        acc[i+1] = dist_acc(dists[idxs[i]-1])\n        if acc[i+1] >= 0: \n            avg_acc = avg_acc + acc[i+1]\n            cnt += 1\n            \n    if cnt != 0:  \n        acc[0] = avg_acc / cnt\n    return acc\n\ndef final_preds(output, center, scale, res):\n    coords = get_preds(output) # float type\n\n    # pose-processing\n    for n in range(coords.size(0)):\n        for p in range(coords.size(1)):\n            hm = output[n][p]\n            px = int(math.floor(coords[n][p][0]))\n            py = int(math.floor(coords[n][p][1]))\n            if px > 1 and px < res[0] and py > 1 and py < res[1]:\n                diff = torch.Tensor([hm[py - 1][px] - hm[py - 1][px - 2], hm[py][px - 1]-hm[py - 2][px - 1]])\n                coords[n][p] += diff.sign() * .25\n    coords += 0.5\n    preds = coords.clone()\n\n    # Transform back\n    for i in range(coords.size(0)):\n        preds[i] = transform_preds(coords[i], center[i], scale[i], res)\n\n    if preds.dim() < 3:\n        preds = preds.view(1, preds.size())\n\n    return preds\n\n    \nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n'"
utils/imutils.py,3,"b""import torch\nimport torch.nn as nn\nimport numpy as np\nimport scipy.misc\nimport cv2\n\nfrom .misc import *\n\ndef im_to_numpy(img):\n    img = to_numpy(img)\n    img = np.transpose(img, (1, 2, 0)) # H*W*C\n    return img\n\ndef im_to_torch(img):\n    img = np.transpose(img, (2, 0, 1)) # C*H*W\n    img = to_torch(img).float()\n    if img.max() > 1:\n        img /= 255\n    return img\n\ndef load_image(img_path):\n    # H x W x C => C x H x W\n    return im_to_torch(scipy.misc.imread(img_path, mode='RGB'))\n\ndef resize(img, owidth, oheight):\n    img = im_to_numpy(img)\n    print('%f %f' % (img.min(), img.max()))\n    img = scipy.misc.imresize(\n            img,\n            (oheight, owidth)\n        )\n    img = im_to_torch(img)\n    print('%f %f' % (img.min(), img.max()))\n    return img\n\n\ndef generate_heatmap(heatmap, pt, sigma):\n    heatmap[int(pt[1])][int(pt[0])] = 1\n    heatmap = cv2.GaussianBlur(heatmap, sigma, 0)\n    am = np.amax(heatmap)\n    heatmap /= am / 255\n    return heatmap\n\n\n# =============================================================================\n# Helpful display functions\n# =============================================================================\n\ndef gauss(x, a, b, c, d=0):\n    return a * np.exp(-(x - b)**2 / (2 * c**2)) + d\n\ndef color_heatmap(x):\n    x = to_numpy(x)\n    color = np.zeros((x.shape[0],x.shape[1],3))\n    color[:,:,0] = gauss(x, .5, .6, .2) + gauss(x, 1, .8, .3)\n    color[:,:,1] = gauss(x, 1, .5, .3)\n    color[:,:,2] = gauss(x, 1, .2, .3)\n    color[color > 1] = 1\n    color = (color * 255).astype(np.uint8)\n    return color\n\ndef imshow(img):\n    npimg = im_to_numpy(img*255).astype(np.uint8)\n    plt.imshow(npimg)\n    plt.axis('off')\n\ndef show_joints(img, pts):\n    imshow(img)\n    \n    for i in range(pts.size(0)):\n        if pts[i, 2] > 0:\n            plt.plot(pts[i, 0], pts[i, 1], 'yo')\n    plt.axis('off')\n\ndef show_sample(inputs, target):\n    num_sample = inputs.size(0)\n    num_joints = target.size(1)\n    height = target.size(2)\n    width = target.size(3)\n\n    for n in range(num_sample):\n        inp = resize(inputs[n], width, height)\n        out = inp\n        for p in range(num_joints):\n            tgt = inp*0.5 + color_heatmap(target[n,p,:,:])*0.5\n            out = torch.cat((out, tgt), 2)\n        \n        imshow(out)\n        plt.show()\n\ndef sample_with_heatmap(inp, out, num_rows=2, parts_to_show=None):\n    inp = to_numpy(inp * 255)\n    out = to_numpy(out)\n\n    img = np.zeros((inp.shape[1], inp.shape[2], inp.shape[0]))\n    for i in range(3):\n        img[:, :, i] = inp[i, :, :]\n\n    if parts_to_show is None:\n        parts_to_show = np.arange(out.shape[0])\n\n    # Generate a single image to display input/output pair\n    num_cols = int(np.ceil(float(len(parts_to_show)) / num_rows))\n    size = img.shape[0] // num_rows\n\n    full_img = np.zeros((img.shape[0], size * (num_cols + num_rows), 3), np.uint8)\n    full_img[:img.shape[0], :img.shape[1]] = img\n\n    inp_small = scipy.misc.imresize(img, [size, size])\n\n    # Set up heatmap display for each part\n    for i, part in enumerate(parts_to_show):\n        part_idx = part\n        out_resized = scipy.misc.imresize(out[part_idx], [size, size])\n        out_resized = out_resized.astype(float)/255\n        out_img = inp_small.copy() * .3\n        color_hm = color_heatmap(out_resized)\n        out_img += color_hm * .7\n\n        col_offset = (i % num_cols + num_rows) * size\n        row_offset = (i // num_cols) * size\n        full_img[row_offset:row_offset + size, col_offset:col_offset + size] = out_img\n\n    return full_img\n\ndef batch_with_heatmap(inputs, outputs, mean=torch.Tensor([0.5, 0.5, 0.5]), num_rows=2, parts_to_show=None):\n    batch_img = []\n    for n in range(min(inputs.size(0), 4)):\n        inp = inputs[n] + mean.view(3, 1, 1).expand_as(inputs[n])\n        batch_img.append(\n            sample_with_heatmap(inp.clamp(0, 1), outputs[n], num_rows=num_rows, parts_to_show=parts_to_show)\n        )\n    return np.concatenate(batch_img)\n"""
utils/logger.py,0,"b'# A simple torch style logger\n# (C) Wei YANG 2017\nimport os\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n__all__ = [\'Logger\', \'LoggerMonitor\', \'savefig\']\n\ndef savefig(fname, dpi=None):\n    dpi = 150 if dpi == None else dpi\n    plt.savefig(fname, dpi=dpi)\n    \ndef plot_overlap(logger, names=None):\n    names = logger.names if names == None else names\n    numbers = logger.numbers\n    for _, name in enumerate(names):\n        x = np.arange(len(numbers[name]))\n        plt.plot(x, np.asarray(numbers[name]))\n    return [logger.title + \'(\' + name + \')\' for name in names]\n\nclass Logger(object):\n    \'\'\'Save training process to log file with simple plot function.\'\'\'\n    def __init__(self, fpath, title=None, resume=False): \n        self.file = None\n        self.resume = resume\n        self.title = \'\' if title == None else title\n        if fpath is not None:\n            if resume: \n                self.file = open(fpath, \'r\') \n                name = self.file.readline()\n                self.names = name.rstrip().split(\'\\t\')\n                self.numbers = {}\n                for _, name in enumerate(self.names):\n                    self.numbers[name] = []\n\n                for numbers in self.file:\n                    numbers = numbers.rstrip().split(\'\\t\')\n                    for i in range(0, len(numbers)):\n                        self.numbers[self.names[i]].append(numbers[i])\n                self.file.close()\n                self.file = open(fpath, \'a\')  \n            else:\n                self.file = open(fpath, \'w\')\n\n    def set_names(self, names):\n        if self.resume: \n            pass\n        # initialize numbers as empty list\n        self.numbers = {}\n        self.names = names\n        for _, name in enumerate(self.names):\n            self.file.write(name)\n            self.file.write(\'\\t\')\n            self.numbers[name] = []\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n\n    def append(self, numbers):\n        assert len(self.names) == len(numbers), \'Numbers do not match names\'\n        for index, num in enumerate(numbers):\n            self.file.write(""{0:.6f}"".format(num))\n            self.file.write(\'\\t\')\n            self.numbers[self.names[index]].append(num)\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n    def plot(self, names=None):   \n        names = self.names if names == None else names\n        numbers = self.numbers\n        for _, name in enumerate(names):\n            x = np.arange(len(numbers[name]))\n            plt.plot(x, np.asarray(numbers[name]))\n        plt.legend([self.title + \'(\' + name + \')\' for name in names])\n        plt.grid(True)\n\n    def close(self):\n        if self.file is not None:\n            self.file.close()\n\nclass LoggerMonitor(object):\n    \'\'\'Load and visualize multiple logs.\'\'\'\n    def __init__ (self, paths):\n        \'\'\'paths is a distionary with {name:filepath} pair\'\'\'\n        self.loggers = []\n        for title, path in paths.items():\n            logger = Logger(path, title=title, resume=True)\n            self.loggers.append(logger)\n\n    def plot(self, names=None):\n        plt.figure()\n        plt.subplot(121)\n        legend_text = []\n        for logger in self.loggers:\n            legend_text += plot_overlap(logger, names)\n        plt.legend(legend_text, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n        plt.grid(True)\n                    \nif __name__ == \'__main__\':\n    # # Example\n    # logger = Logger(\'test.txt\')\n    # logger.set_names([\'Train loss\', \'Valid loss\',\'Test loss\'])\n\n    # length = 100\n    # t = np.arange(length)\n    # train_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n    # valid_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n    # test_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n\n    # for i in range(0, length):\n    #     logger.append([train_loss[i], valid_loss[i], test_loss[i]])\n    # logger.plot()\n\n    # Example: logger monitor\n    paths = {\n    \'resadvnet20\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet20/log.txt\', \n    \'resadvnet32\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet32/log.txt\',\n    \'resadvnet44\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet44/log.txt\',\n    }\n\n    field = [\'Valid Acc.\']\n\n    monitor = LoggerMonitor(paths)\n    monitor.plot(names=field)\n    savefig(\'test.eps\')'"
utils/misc.py,5,"b'import os\nimport shutil\nimport torch \nimport math\nimport numpy as np\nimport scipy.io\nimport matplotlib.pyplot as plt\n\ndef to_numpy(tensor):\n    if torch.is_tensor(tensor):\n        return tensor.cpu().numpy()\n    elif type(tensor).__module__ != \'numpy\':\n        raise ValueError(""Cannot convert {} to numpy array""\n                         .format(type(tensor)))\n    return tensor\n\n\ndef to_torch(ndarray):\n    if type(ndarray).__module__ == \'numpy\':\n        return torch.from_numpy(ndarray)\n    elif not torch.is_tensor(ndarray):\n        raise ValueError(""Cannot convert {} to torch tensor""\n                         .format(type(ndarray)))\n    return ndarray\n\n\ndef save_checkpoint(state, preds, is_best, checkpoint=\'checkpoint\', filename=\'checkpoint.pth.tar\', snapshot=None):\n    preds = to_numpy(preds)\n    filepath = os.path.join(checkpoint, filename)\n    torch.save(state, filepath)\n    scipy.io.savemat(os.path.join(checkpoint, \'preds.mat\'), mdict={\'preds\' : preds})\n\n    if snapshot and state.epoch % snapshot == 0:\n        shutil.copyfile(filepath, os.path.join(checkpoint, \'checkpoint_{}.pth.tar\'.format(state.epoch)))\n\n    if is_best:\n        shutil.copyfile(filepath, os.path.join(checkpoint, \'model_best.pth.tar\'))\n        scipy.io.savemat(os.path.join(checkpoint, \'preds_best.mat\'), mdict={\'preds\' : preds})\n\ndef copy_log(filepath = \'checkpoint\'):\n    filepath = os.path.join(filepath, \'log.txt\')\n    shutil.copyfile(filepath, os.path.join(\'log_backup.txt\'))\n\ndef save_model(state, checkpoint=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n    filename = \'epoch\'+str(state[\'epoch\']) + filename\n    filepath = os.path.join(checkpoint, filename)\n    torch.save(state, filepath)\n\n    # if snapshot and state.epoch % snapshot == 0:\n    #     shutil.copyfile(filepath, os.path.join(checkpoint, \'checkpoint_{}.pth.tar\'.format(state.epoch)))\n\ndef save_pred(preds, checkpoint=\'checkpoint\', filename=\'preds_valid.mat\'):\n    preds = to_numpy(preds)\n    filepath = os.path.join(checkpoint, filename)\n    scipy.io.savemat(filepath, mdict={\'preds\' : preds})\n\n\ndef adjust_learning_rate(optimizer, epoch, schedule, gamma):\n    """"""Sets the learning rate to the initial LR decayed by schedule""""""\n    if epoch in schedule:\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] *= gamma\n    return optimizer.state_dict()[\'param_groups\'][0][\'lr\']\n'"
utils/osutils.py,0,"b'import os\nimport errno\n\ndef mkdir_p(dir_path):\n    try:\n        os.makedirs(dir_path)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n\ndef isfile(fname):\n    return os.path.isfile(fname) \n\ndef isdir(dirname):\n    return os.path.isdir(dirname)\n\ndef join(path, *paths):\n    return os.path.join(path, *paths)\n    \ndef add_pypath(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)'"
utils/transforms.py,3,"b'import os\nimport numpy as np\nimport scipy.misc\nimport matplotlib.pyplot as plt\nimport torch\n\nfrom .misc import *\nfrom .imutils import *\n\n\ndef color_normalize(x, mean):\n    if x.size(0) == 1:\n        x = x.repeat(3, 1, 1)\n    normalized_mean = mean / 255\n    for t, m in zip(x, normalized_mean):\n        t.sub_(m)\n    return x\n\n\ndef flip_back(flip_output, dataset=\'mpii\'):\n    """"""\n    flip output map\n    """"""\n    if dataset ==  \'mpii\':\n        matchedParts = (\n            [0,5],   [1,4],   [2,3],\n            [10,15], [11,14], [12,13]\n        )\n    else:\n        print(\'Not supported dataset: \' + dataset)\n\n    # flip output horizontally\n    flip_output = fliplr(flip_output.numpy())\n\n    # Change left-right parts\n    for pair in matchedParts:\n        tmp = np.copy(flip_output[:, pair[0], :, :])\n        flip_output[:, pair[0], :, :] = flip_output[:, pair[1], :, :]\n        flip_output[:, pair[1], :, :] = tmp\n\n    return torch.from_numpy(flip_output).float()\n\n\ndef shufflelr(x, width, dataset=\'mpii\'):\n    """"""\n    flip coords\n    """"""\n    if dataset ==  \'mpii\':\n        matchedParts = (\n            [0,5],   [1,4],   [2,3],\n            [10,15], [11,14], [12,13]\n        )\n    else:\n        print(\'Not supported dataset: \' + dataset)\n\n    # Flip horizontal\n    x[:, 0] = width - x[:, 0]\n\n    # Change left-right parts\n    for pair in matchedParts:\n        tmp = x[pair[0], :].clone()\n        x[pair[0], :] = x[pair[1], :]\n        x[pair[1], :] = tmp\n\n    return x\n\n\ndef fliplr(x):\n    if x.ndim == 3:\n        x = np.transpose(np.fliplr(np.transpose(x, (0, 2, 1))), (0, 2, 1))\n    elif x.ndim == 4:\n        for i in range(x.shape[0]):\n            x[i] = np.transpose(np.fliplr(np.transpose(x[i], (0, 2, 1))), (0, 2, 1))\n    return x.astype(float)\n\n\ndef get_transform(center, scale, res, rot=0):\n    """"""\n    General image processing functions\n    """"""\n    # Generate transformation matrix\n    h = 200 * scale\n    t = np.zeros((3, 3))\n    t[0, 0] = float(res[1]) / h\n    t[1, 1] = float(res[0]) / h\n    t[0, 2] = res[1] * (-float(center[0]) / h + .5)\n    t[1, 2] = res[0] * (-float(center[1]) / h + .5)\n    t[2, 2] = 1\n    if not rot == 0:\n        rot = -rot # To match direction of rotation from cropping\n        rot_mat = np.zeros((3,3))\n        rot_rad = rot * np.pi / 180\n        sn,cs = np.sin(rot_rad), np.cos(rot_rad)\n        rot_mat[0,:2] = [cs, -sn]\n        rot_mat[1,:2] = [sn, cs]\n        rot_mat[2,2] = 1\n        # Need to rotate around center\n        t_mat = np.eye(3)\n        t_mat[0,2] = -res[1]/2\n        t_mat[1,2] = -res[0]/2\n        t_inv = t_mat.copy()\n        t_inv[:2,2] *= -1\n        t = np.dot(t_inv,np.dot(rot_mat,np.dot(t_mat,t)))\n    return t\n\n\ndef transform(pt, center, scale, res, invert=0, rot=0):\n    # Transform pixel location to different reference\n    t = get_transform(center, scale, res, rot=rot)\n    if invert:\n        t = np.linalg.inv(t)\n    new_pt = np.array([pt[0] - 1, pt[1] - 1, 1.]).T\n    new_pt = np.dot(t, new_pt)\n    return new_pt[:2].astype(int) + 1\n\n\ndef transform_preds(coords, center, scale, res):\n    # size = coords.size()\n    # coords = coords.view(-1, coords.size(-1))\n    # print(coords.size())\n    for p in range(coords.size(0)):\n        coords[p, 0:2] = to_torch(transform(coords[p, 0:2], center, scale, res, 1, 0))\n    return coords\n\n\ndef crop(img, center, scale, res, rot=0):\n    img = im_to_numpy(img)\n\n    # Preprocessing for efficient cropping\n    ht, wd = img.shape[0], img.shape[1]\n    sf = scale * 200.0 / res[0]\n    if sf < 2:\n        sf = 1\n    else:\n        new_size = int(np.math.floor(max(ht, wd) / sf))\n        new_ht = int(np.math.floor(ht / sf))\n        new_wd = int(np.math.floor(wd / sf))\n        if new_size < 2:\n            return torch.zeros(res[0], res[1], img.shape[2]) \\\n                        if len(img.shape) > 2 else torch.zeros(res[0], res[1])\n        else:\n            img = scipy.misc.imresize(img, [new_ht, new_wd])\n            center = center * 1.0 / sf\n            scale = scale / sf\n\n    # Upper left point\n    ul = np.array(transform([0, 0], center, scale, res, invert=1))\n    # Bottom right point\n    br = np.array(transform(res, center, scale, res, invert=1))\n\n    # Padding so that when rotated proper amount of context is included\n    pad = int(np.linalg.norm(br - ul) / 2 - float(br[1] - ul[1]) / 2)\n    if not rot == 0:\n        ul -= pad\n        br += pad\n\n    new_shape = [br[1] - ul[1], br[0] - ul[0]]\n    if len(img.shape) > 2:\n        new_shape += [img.shape[2]]\n    new_img = np.zeros(new_shape)\n\n    # Range to fill new array\n    new_x = max(0, -ul[0]), min(br[0], len(img[0])) - ul[0]\n    new_y = max(0, -ul[1]), min(br[1], len(img)) - ul[1]\n    # Range to sample from original image\n    old_x = max(0, ul[0]), min(len(img[0]), br[0])\n    old_y = max(0, ul[1]), min(len(img), br[1])\n    new_img[new_y[0]:new_y[1], new_x[0]:new_x[1]] = img[old_y[0]:old_y[1], old_x[0]:old_x[1]]\n\n    if not rot == 0:\n        # Remove padding\n        new_img = scipy.misc.imrotate(new_img, rot)\n        new_img = new_img[pad:-pad, pad:-pad]\n\n    new_img = im_to_torch(scipy.misc.imresize(new_img, res))\n    return new_img\n'"
