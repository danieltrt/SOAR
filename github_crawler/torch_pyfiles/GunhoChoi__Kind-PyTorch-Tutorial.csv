file_path,api_count,code
04_MNIST_CNN/MNIST_CNN_cpu.py,11,"b'# MNIST CNN classifier \n# Code by GunhoChoi\n\nimport torch\nimport torch.nn as nn\nimport torch.utils as utils\nfrom torch.autograd import Variable\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n#%matplotlib inline\n\n# Set Hyperparameters\n\nepoch = 1\nbatch_size =16\nlearning_rate = 0.001\n\n# Download Data\n\nmnist_train = dset.MNIST(""./"", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\nmnist_test  = dset.MNIST(""./"", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)\n\n# Check the datasets downloaded\n\nprint(mnist_train.__len__())\nprint(mnist_test.__len__())\nimg1,label1 = mnist_train.__getitem__(0)\nimg2,label2 = mnist_test.__getitem__(0)\n\nprint(img1.size(), label1)\nprint(img2.size(), label2)\n\n# Set Data Loader(input pipeline)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=mnist_train,batch_size=batch_size,shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset=mnist_test,batch_size=batch_size,shuffle=True)\n\n# torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, \n#                 padding=0, dilation=1, groups=1, bias=True)\n# torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1,\n#                    return_indices=False, ceil_mode=False)\n# torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1,affine=True)\n# torch.nn.ReLU()\n# tensor.view(newshape)\n\nclass CNN(nn.Module):\n\n    def __init__(self):\n\n        super(CNN,self).__init__()\n        self.layer1 = nn.Sequential(\n                        nn.Conv2d(1,16,5),   # batch x 16 x 24 x 24\n                        nn.ReLU(),\n                        nn.BatchNorm2d(16),\n                        nn.Conv2d(16,32,5),  # batch x 32 x 20 x 20\n                        nn.ReLU(),\n                        nn.BatchNorm2d(32),\n                        nn.MaxPool2d(2,2)   # batch x 32 x 10 x 10\n        )\n        self.layer2 = nn.Sequential(\n                        nn.Conv2d(32,64,5),  # batch x 64 x 6 x 6\n                        nn.ReLU(),\n                        nn.BatchNorm2d(64),\n                        nn.Conv2d(64,128,5),  # batch x 128 x 2 x 2\n                        nn.ReLU()\n        )\n        self.fc = nn.Linear(2*2*128,10)\n        \n    def forward(self,x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(batch_size, -1)\n        out = self.fc(out)\n        return out\n        \ncnn = CNN()\n\nloss_func = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n\n# Train Model with train data\n# In order to use GPU you need to move all Variables and model by Module\n\nfor i in range(epoch):\n    for j,[image,label] in enumerate(train_loader):\n        image = Variable(image)\n        label = Variable(label)\n        \n        optimizer.zero_grad()\n        result = cnn.forward(image)\n        loss = loss_func(result,label)\n        loss.backward()\n        optimizer.step()\n        \n        if j % 100 == 0:\n            print(loss)\n\n# Test with test data\n# In order test, we need to change model mode to .eval()\n# and get the highest score label for accuracy\n# Change model to \'eval\' mode (BN uses moving mean/var)\n\ncnn.eval()  \ncorrect= 0\ntotal = 0\n\nfor image, label in test_loader:\n    image = Variable(image)\n    result = cnn(image)\n    _, predicted = torch.max(result.data, 1)\n    total += label.size(0)\n    correct += (predicted == label).sum()\n\nprint(\'Test Accuracy: %f %%\' % (100 * correct / total))\n  \n'"
04_MNIST_CNN/MNIST_CNN_gpu.py,11,"b'# MNIST CNN classifier \n# Code by GunhoChoi\n\nimport torch\nimport torch.nn as nn\nimport torch.utils as utils\nfrom torch.autograd import Variable\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n#%matplotlib inline\n\n# Set Hyperparameters\n\nepoch = 1\nbatch_size =16\nlearning_rate = 0.001\n\n# Download Data\n\nmnist_train = dset.MNIST(""./"", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\nmnist_test  = dset.MNIST(""./"", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)\n\n# Check the datasets downloaded\n\nprint(mnist_train.__len__())\nprint(mnist_test.__len__())\nimg1,label1 = mnist_train.__getitem__(0)\nimg2,label2 = mnist_test.__getitem__(0)\n\nprint(img1.size(), label1)\nprint(img2.size(), label2)\n\n# Set Data Loader(input pipeline)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=mnist_train,batch_size=batch_size,shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset=mnist_test,batch_size=batch_size,shuffle=True)\n\n# torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, \n#                 padding=0, dilation=1, groups=1, bias=True)\n# torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1,\n#                    return_indices=False, ceil_mode=False)\n# torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1,affine=True)\n# torch.nn.ReLU()\n# tensor.view(newshape)\n\nclass CNN(nn.Module):\n\n    def __init__(self):\n\n        super(CNN,self).__init__()\n        self.layer1 = nn.Sequential(\n                        nn.Conv2d(1,16,5),   # batch x 16 x 24 x 24\n                        nn.ReLU(),\n                        nn.BatchNorm2d(16),\n                        nn.Conv2d(16,32,5),  # batch x 32 x 20 x 20\n                        nn.ReLU(),\n                        nn.BatchNorm2d(32),\n                        nn.MaxPool2d(2,2)   # batch x 32 x 10 x 10\n        )\n        self.layer2 = nn.Sequential(\n                        nn.Conv2d(32,64,5),  # batch x 64 x 6 x 6\n                        nn.ReLU(),\n                        nn.BatchNorm2d(64),\n                        nn.Conv2d(64,128,5),  # batch x 128 x 2 x 2\n                        nn.ReLU()\n        )\n        self.fc = nn.Linear(2*2*128,10)\n        \n    def forward(self,x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(batch_size, -1)\n        out = self.fc(out)\n        return out\n        \ncnn = CNN()\n\nloss_func = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n\n# Train Model with train data\n# In order to use GPU you need to move all Variables and model by Module.cuda()\n\nfor i in range(epoch):\n    for j,[image,label] in enumerate(train_loader):\n        image = Variable(image).cuda()\n        label = Variable(label).cuda()\n        cnn.cuda()\n        \n        optimizer.zero_grad()\n        result = cnn.forward(image)\n        loss = loss_func(result,label)\n        loss.backward()\n        optimizer.step()\n        \n        if j % 100 == 0:\n            print(loss)\n\n# Test with test data\n# In order test, we need to change model mode to .eval()\n# and get the highest score label for accuracy\n# Change model to \'eval\' mode (BN uses moving mean/var)\n# label also has to be on GPU so .cuda() required -> this took an hour for me to figure out\n\ncnn.eval()  \ncorrect= 0\ntotal = 0\n\nfor image, label in test_loader:\n    image = Variable(image).cuda()\n    result = cnn(image).cuda()\n    _, predicted = torch.max(result.data, 1)\n    total += label.size(0)\n    correct += (predicted == label.cuda()).sum()\n\nprint(\'Test Accuracy of the model on the 10000 test images: %f %%\' % (100 * correct / total))\n  \n'"
06_Autoencoder_Model_Save/Autoencoder_Model_Save.py,9,"b'# Simple Convolutional Autoencoder\n# Code by GunhoChoi\n\nimport torch\nimport torch.nn as nn\nimport torch.utils as utils\nfrom torch.autograd import Variable\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set Hyperparameters\n\nepoch = 20\nbatch_size = 100\nlearning_rate = 0.001\n\n# Download Data\n\nmnist_train = dset.MNIST(""./"", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\nmnist_test  = dset.MNIST(""./"", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)\n\n# Set Data Loader(input pipeline)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=mnist_train,batch_size=batch_size,shuffle=True)\n\n# Encoder \n# torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n#                 stride=1, padding=0, dilation=1,\n#                 groups=1, bias=True)\n# batch x 1 x 28 x 28 -> batch x 512\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder,self).__init__()\n        self.layer1 = nn.Sequential(\n                        nn.Conv2d(1,16,3,padding=1),   # batch x 16 x 28 x 28\n                        nn.ReLU(),\n                        nn.BatchNorm2d(16),\n                        nn.Conv2d(16,32,3,padding=1),  # batch x 32 x 28 x 28\n                        nn.ReLU(),\n                        nn.BatchNorm2d(32),\n                        nn.Conv2d(32,64,3,padding=1),  # batch x 32 x 28 x 28\n                        nn.ReLU(),\n                        nn.BatchNorm2d(64),\n                        nn.MaxPool2d(2,2)   # batch x 64 x 14 x 14\n        )\n        self.layer2 = nn.Sequential(\n                        nn.Conv2d(64,128,3,padding=1),  # batch x 64 x 14 x 14\n                        nn.ReLU(),\n                        nn.BatchNorm2d(128),\n                        nn.MaxPool2d(2,2),\n                        nn.Conv2d(128,256,3,padding=1),  # batch x 64 x 7 x 7\n                        nn.ReLU()\n        )\n        \n                \n    def forward(self,x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(batch_size, -1)\n        return out\n    \nencoder = Encoder().cuda()\n\n# Decoder \n# torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size,\n#                          stride=1, padding=0, output_padding=0,\n#                          groups=1, bias=True)\n# output_height = (height-1)*stride + kernel_size - 2*padding + output_padding\n# batch x 512 -> batch x 1 x 28 x 28\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder,self).__init__()\n        self.layer1 = nn.Sequential(\n                        nn.ConvTranspose2d(256,128,3,2,1,1),\n                        nn.ReLU(),\n                        nn.BatchNorm2d(128),\n                        nn.ConvTranspose2d(128,64,3,1,1),\n                        nn.ReLU(),\n                        nn.BatchNorm2d(64)\n        )\n        self.layer2 = nn.Sequential(\n                        nn.ConvTranspose2d(64,16,3,1,1),\n                        nn.ReLU(),\n                        nn.BatchNorm2d(16),\n                        nn.ConvTranspose2d(16,1,3,2,1,1),\n                        nn.ReLU()\n        )\n        \n    def forward(self,x):\n        out = x.view(batch_size,256,7,7)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        return out\n\ndecoder = Decoder().cuda()\n\n# loss func and optimizer\n# we compute reconstruction after decoder so use Mean Squared Error\n# In order to use multi parameters with one optimizer,\n# concat parameters after changing into list\n\nparameters = list(encoder.parameters())+ list(decoder.parameters())\nloss_func = nn.MSELoss()\noptimizer = torch.optim.Adam(parameters, lr=learning_rate)\n\n# train encoder and decoder\n\ntry:\n\tencoder, decoder = torch.load(\'./model/autoencoder.pkl\')\n\tprint(""\\n--------model restored--------\\n"")\nexcept:\n\tpass\n\nfor i in range(epoch):\n    for image,label in train_loader:\n        image = Variable(image).cuda()\n        #label = Variable(label.float()).cuda()\n        optimizer.zero_grad()\n        output = encoder(image)\n        output = decoder(output)\n        loss = loss_func(output,image)\n        loss.backward()\n        optimizer.step()\n        \n    if i % 2 == 0:\n        torch.save([encoder,decoder],\'./model/autoencoder.pkl\')\n        print(loss)\n\ninput_img = image[0].cpu()\noutput_img = output[0].cpu()\n\ninp = input_img.data.numpy()\nout = output_img.data.numpy()\n\nplt.imshow(inp[0],cmap=\'gray\')\nplt.show()\n\nplt.imshow(out[0],cmap=""gray"")\nplt.show()\n'"
07_Denoising_Autoencoder/Denoising_Autoencoder.py,13,"b'# Simple Convolutional Autoencoder\n# Code by GunhoChoi\n\nimport torch\nimport torch.nn as nn\nimport torch.utils as utils\nfrom torch.autograd import Variable\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set Hyperparameters\n\nepoch = 100\nbatch_size = 100\nlearning_rate = 0.0002\n\n# Download Data\n\nmnist_train = dset.MNIST(""./"", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\nmnist_test  = dset.MNIST(""./"", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)\n\n# Set Data Loader(input pipeline)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=mnist_train,batch_size=batch_size,shuffle=True)\n\n# Encoder \n# torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n#                 stride=1, padding=0, dilation=1,\n#                 groups=1, bias=True)\n# batch x 1 x 28 x 28 -> batch x 512\n\n# Encoder \n# torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n#                 stride=1, padding=0, dilation=1,\n#                 groups=1, bias=True)\n# batch x 1 x 28 x 28 -> batch x 512\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder,self).__init__()\n        self.layer1 = nn.Sequential(\n                        nn.Conv2d(1,32,3,padding=1),   # batch x 32 x 28 x 28\n                        nn.ReLU(),\n                        nn.BatchNorm2d(32),\n                        nn.Conv2d(32,32,3,padding=1),   # batch x 32 x 28 x 28\n                        nn.ReLU(),\n                        nn.BatchNorm2d(32),\n                        nn.Conv2d(32,64,3,padding=1),  # batch x 64 x 28 x 28\n                        nn.ReLU(),\n                        nn.BatchNorm2d(64),\n                        nn.Conv2d(64,64,3,padding=1),  # batch x 64 x 28 x 28\n                        nn.ReLU(),\n                        nn.BatchNorm2d(64),\n                        nn.MaxPool2d(2,2)   # batch x 64 x 14 x 14\n        )\n        self.layer2 = nn.Sequential(\n                        nn.Conv2d(64,128,3,padding=1),  # batch x 128 x 14 x 14\n                        nn.ReLU(),\n                        nn.BatchNorm2d(128),\n                        nn.Conv2d(128,128,3,padding=1),  # batch x 128 x 14 x 14\n                        nn.ReLU(),\n                        nn.BatchNorm2d(128),\n                        nn.MaxPool2d(2,2),\n                        nn.Conv2d(128,256,3,padding=1),  # batch x 256 x 7 x 7\n                        nn.ReLU()\n        )\n        \n                \n    def forward(self,x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(batch_size, -1)\n        return out\n    \nencoder = Encoder().cuda()\n\n# Decoder \n# torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size,\n#                          stride=1, padding=0, output_padding=0,\n#                          groups=1, bias=True)\n# output_height = (height-1)*stride + kernel_size - 2*padding + output_padding\n# batch x 512 -> batch x 1 x 28 x 28\n\n# Decoder \n# torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size,\n#                          stride=1, padding=0, output_padding=0,\n#                          groups=1, bias=True)\n# output_height = (height-1)*stride + kernel_size - 2*padding + output_padding\n# batch x 512 -> batch x 1 x 28 x 28\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder,self).__init__()\n        self.layer1 = nn.Sequential(\n                        nn.ConvTranspose2d(256,128,3,2,1,1), # batch x 128 x 14 x 14\n                        nn.ReLU(),\n                        nn.BatchNorm2d(128),\n                        nn.ConvTranspose2d(128,128,3,1,1),   # batch x 128 x 14 x 14\n                        nn.ReLU(),\n                        nn.BatchNorm2d(128),\n                        nn.ConvTranspose2d(128,64,3,1,1),    # batch x 64 x 14 x 14\n                        nn.ReLU(),\n                        nn.BatchNorm2d(64),\n                        nn.ConvTranspose2d(64,64,3,1,1),     # batch x 64 x 14 x 14\n                        nn.ReLU(),\n                        nn.BatchNorm2d(64)\n        )\n        self.layer2 = nn.Sequential(\n                        nn.ConvTranspose2d(64,32,3,1,1),     # batch x 32 x 14 x 14\n                        nn.ReLU(),\n                        nn.BatchNorm2d(32),\n                        nn.ConvTranspose2d(32,32,3,1,1),     # batch x 32 x 14 x 14\n                        nn.ReLU(),\n                        nn.BatchNorm2d(32),\n                        nn.ConvTranspose2d(32,1,3,2,1,1),    # batch x 1 x 28 x 28\n                        nn.ReLU()\n        )\n        \n    def forward(self,x):\n        out = x.view(batch_size,256,7,7)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        return out\n\ndecoder = Decoder().cuda()\n\n# Noise \n\nnoise = torch.rand(batch_size,1,28,28)\n\n# loss func and optimizer\n# we compute reconstruction after decoder so use Mean Squared Error\n# In order to use multi parameters with one optimizer,\n# concat parameters after changing into list\n\nparameters = list(encoder.parameters())+ list(decoder.parameters())\nloss_func = nn.MSELoss()\noptimizer = torch.optim.Adam(parameters, lr=learning_rate)\n\n# train encoder and decoder\n\ntry:jupyter \n\tencoder, decoder = torch.load(\'./model/deno_autoencoder.pkl\')\n\tprint(""\\n--------model restored--------\\n"")\nexcept:\n\tpass\n\nfor i in range(epoch):\n    for image,label in train_loader:\n        image_n = torch.mul(image+0.25, 0.1 * noise)\n        image = Variable(image).cuda()\n        image_n = Variable(image_n).cuda()\n        optimizer.zero_grad()\n        output = encoder(image_n)\n        output = decoder(output)\n        loss = loss_func(output,image)\n        loss.backward()\n        optimizer.step()\n        break\n        \n    torch.save([encoder,decoder],\'./model/deno_autoencoder.pkl\')\n    print(loss)\n\n# check image with noise and denoised image\\\n\nimg = image[0].cpu()\ninput_img = image_n[0].cpu()\noutput_img = output[0].cpu()\n\norigin = img.data.numpy()\ninp = input_img.data.numpy()\nout = output_img.data.numpy()\n\nplt.imshow(origin[0],cmap=\'gray\')\nplt.show()\n\nplt.imshow(inp[0],cmap=\'gray\')\nplt.show()\n\nplt.imshow(out[0],cmap=""gray"")\nplt.show()\n\nprint(label[0])\n'"
08_Simple_Char_RNN/Simple_Char_RNN.py,8,"b'import torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport numpy as np\n\n# Preprocessing \n# alphabet(0-25), space(26), start(27), end(28) -> 29 chars (0-28)\n\nstring = ""hello pytorch""\nchars = ""abcdefghijklmnopqrstuvwxyz 01""\nchar_list = [i for i in chars]\nbatch_size = len(char_list)\n\n\ndef string_to_onehot(string):\n\tstart = np.zeros(shape=len(char_list) ,dtype=int)\n\tend = np.zeros(shape=len(char_list) ,dtype=int)\n\tstart[-2] = 1\n\tend[-1] = 1\n\tfor i in string:\n\t\tidx = char_list.index(i)\n\t\tzero = np.zeros(shape=batch_size ,dtype=int)\n\t\tzero[idx]=1\n\t\tstart = np.vstack([start,zero])\n\toutput = np.vstack([start,end])\n\treturn output\n\n\ndef onehot_to_word(onehot_1):\n\tonehot = torch.Tensor.numpy(onehot_1)\n\treturn char_list[onehot.argmax()]\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNN, self).__init__()\n        \n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        \n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n    \n    def forward(self, input, hidden):\n        combined = torch.cat((input, hidden), 1)\n        hidden = self.i2h(combined)\n        output = self.i2o(combined)\n        return output, hidden\n\n    def init_hidden(self):\n        return Variable(torch.zeros(1, self.hidden_size))\n\n\n# Hyperparameters\n\nn_letters = len(char_list)\nn_hidden = 100\n#n_categories = len(char_list)\nlr = 0.01\nepochs = 100\n\nrnn = RNN(n_letters, n_hidden, n_letters)\n\n# Loss function & Optimizer\n\none_hot = torch.from_numpy(string_to_onehot(string)).type_as(torch.FloatTensor())\nloss_func = nn.MSELoss()\noptimizer = torch.optim.Adam(rnn.parameters(), lr=lr)\n\n# train\n\nfor i in range(epochs):\n\trnn.zero_grad()\n\ttotal_loss = 0\n\thidden = rnn.init_hidden()\n\n\tfor j in range(one_hot.size()[0]-1):\n\t\tinput = Variable(one_hot[j:j+1,:])\n\t\toutput, hidden = rnn.forward(input, hidden)\n\t\ttarget = Variable(one_hot[j+1])\n\t\tloss = loss_func(output.view(-1),target.view(-1))\n\t\ttotal_loss += loss\n\t\tinput = output\n\n\ttotal_loss.backward()\n\toptimizer.step()\n\n\tif i % 10 == 0:\n\t\tprint(total_loss)\n\n# test \n\nhidden = rnn.init_hidden()\ninput = Variable(one_hot[0:1,:])\n\nfor i in range(len(string)):\n\toutput, hidden = rnn.forward(input, hidden)\n\tprint(onehot_to_word(output.data))\n\tinput = output\n'"
09_GAN_LayerName_MultiGPU/GAN_LayerName_MultiGPU.py,14,"b'import torch\nimport torch.nn as nn\nimport torch.utils as utils\nimport torch.nn.init as init\nfrom torch.autograd import Variable\nimport torchvision.datasets as dset\nimport torchvision.utils as v_utils\nimport torchvision.transforms as transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\n\n# Set Hyperparameters\n\nepoch = 1000\nbatch_size = 6000\nlearning_rate = 0.0002\nnum_gpus = 4\n\n# Download Data\n\nmnist_train = dset.MNIST(""./"", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\nmnist_test  = dset.MNIST(""./"", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)\n\n# Set Data Loader(input pipeline)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=mnist_train,batch_size=batch_size,shuffle=True)\n\nclass Generator(nn.Module):\n\tdef __init__(self):\n\t\tsuper(Generator,self).__init__()\n\t\tself.layer1 = nn.Linear(100,7*7*256)\n\t\tself.layer2 = nn.Sequential(OrderedDict([\n\t\t\t\t(\'conv1\', nn.ConvTranspose2d(256,128,3,2,1,1)),\n\t\t\t\t(\'relu1\', nn.LeakyReLU()),\n\t\t\t\t(\'bn1\', nn.BatchNorm2d(128)),\n\t\t\t\t(\'conv2\', nn.ConvTranspose2d(128,64,3,1,1)),\n\t\t\t\t(\'relu2\', nn.LeakyReLU()),\n\t\t\t\t(\'bn2\', nn.BatchNorm2d(64))\n\t\t\t]))\n\t\tself.layer3 = nn.Sequential(OrderedDict([\n\t\t\t\t(\'conv3\',nn.ConvTranspose2d(64,16,3,1,1)),\n\t\t\t\t(\'relu3\',nn.LeakyReLU()),\n\t\t\t\t(\'bn3\',nn.BatchNorm2d(16)),\n\t\t\t\t(\'conv4\',nn.ConvTranspose2d(16,1,3,2,1,1)),\n\t\t\t\t(\'relu4\',nn.LeakyReLU())\n\t\t\t]))\n\n\tdef forward(self,z):\n\t\tout = self.layer1(z)\n\t\tout = out.view(batch_size//num_gpus,256,7,7)\n\t\tout = self.layer2(out)\n\t\tout = self.layer3(out)\n\t\treturn out\n\n\nclass Discriminator(nn.Module):\n\tdef __init__(self):\n\t\tsuper(Discriminator,self).__init__()\n\t\tself.layer1 = nn.Sequential(OrderedDict([\n\t\t\t\t(\'conv1\',nn.Conv2d(1,16,3,padding=1)),   # batch x 16 x 28 x 28\n\t\t\t\t(\'relu1\',nn.LeakyReLU()),\n\t\t\t\t(\'bn1\',nn.BatchNorm2d(16)),\n\t\t\t\t(\'conv2\',nn.Conv2d(16,32,3,padding=1)),  # batch x 32 x 28 x 28\n\t\t\t\t(\'relu2\',nn.LeakyReLU()),\n\t\t\t\t(\'bn2\',nn.BatchNorm2d(32)),\n\t\t\t\t(\'max1\',nn.MaxPool2d(2,2))   # batch x 32 x 14 x 14\n\t\t\t]))\n\t\tself.layer2 = nn.Sequential(OrderedDict([\n\t\t\t\t(\'conv3\',nn.Conv2d(32,64,3,padding=1)),  # batch x 64 x 14 x 14\n\t\t\t\t(\'relu3\',nn.LeakyReLU()),\n\t\t\t\t(\'bn3\',nn.BatchNorm2d(64)),\n\t\t\t\t(\'max2\',nn.MaxPool2d(2,2)),\n\t\t\t\t(\'conv4\',nn.Conv2d(64,128,3,padding=1)),  # batch x 128 x 7 x 7\n\t\t\t\t(\'relu4\',nn.LeakyReLU())\n\t\t\t]))\n\t\tself.fc = nn.Sequential(\n\t\t\t\tnn.Linear(128*7*7,1),\n\t\t\t\tnn.Sigmoid()\n\t\t\t)\n\n\tdef forward(self,x):\n\t\tout = self.layer1(x)\n\t\tout = self.layer2(out)\n\t\tout = out.view(batch_size//num_gpus, -1)\n\t\tout = self.fc(out)\n\t\treturn out\n    \ngenerator = nn.DataParallel(Generator()).cuda()\ndiscriminator = nn.DataParallel(Discriminator()).cuda()\n\nloss_func = nn.BCELoss()\ngen_optim = torch.optim.Adam(generator.parameters(), lr=learning_rate)\ndis_optim = torch.optim.Adam(discriminator.parameters(), lr=learning_rate)\n\nones_label = Variable(torch.ones(batch_size,1)).cuda()\nzeros_label = Variable(torch.zeros(batch_size,1)).cuda()\n\n# parameter initialization\n\ngen_params = generator.state_dict().keys()\ndis_params = discriminator.state_dict().keys()\n\n# model restore\n\ntry:\n\tgenerator, discriminator = torch.load(\'./model/vanilla_gan.pkl\')\n\tprint(""\\n--------model restored--------\\n"")\nexcept:\n\tprint(""\\n--------model not restored--------\\n"")\n\tpass\n\n# train\n\nfor i in range(epoch):\n\tfor j,(image,label) in enumerate(train_loader):\n\t\timage = Variable(image).cuda()\n\t\t\n\t\tfor k in range(5):\n\t\t\tz = Variable(torch.rand(batch_size,100)).cuda()\n\t\t\tgen_optim.zero_grad()\n\t\t\tgen_fake = generator.forward(z)\n\t\t\tdis_fake = discriminator.forward(gen_fake)\n\t\t\tgen_loss = torch.sum(loss_func(dis_fake,ones_label))\n\t\t\tgen_loss.backward(retain_variables=True)\n\t\t\tgen_optim.step()\n\n\t\tdis_optim.zero_grad()\n\t\tdis_real = discriminator.forward(image)\n\t\tdis_loss = torch.sum(loss_func(dis_fake,zeros_label))+torch.sum(loss_func(dis_real,ones_label))\n\t\tdis_loss.backward()\n\t\tdis_optim.step()\n\t\t\n\tif i % 5 == 0:\n\t\ttorch.save([generator,discriminator],\'./model/vanilla_gan.pkl\')\n\t\n\tprint(""{}th iteration gen_loss: {} dis_loss: {}"".format(i,gen_loss.data,dis_loss.data))\n\tv_utils.save_image(gen_fake.data[0:20],""./gan_result/gen_{}.png"".format(i), nrow=5)\n'"
10_InfoGAN_Least_Squares_Loss/InfoGAN_Least_Squares_Loss.py,17,"b'import torch\nimport torch.nn as nn\nimport torch.utils as utils\nimport torch.nn.init as init\nfrom torch.autograd import Variable\nimport torchvision.datasets as dset\nimport torchvision.utils as v_utils\nimport torchvision.transforms as transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\n\n# Set Hyperparameters\n\nepoch = 1000\nbatch_size = 2000\nlearning_rate = 0.001\nnum_gpus = 2\ndiscrete_latent_size = 10\n\n# Download Data & Set Data Loader(input pipeline)\n\nmnist_train = dset.MNIST(""./"", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\ntrain_loader = torch.utils.data.DataLoader(dataset=mnist_train,batch_size=batch_size,shuffle=True)\n\n\ndef int_to_onehot(z_label):\n    one_hot_array = np.zeros(shape=[len(z_label), discrete_latent_size])\n    one_hot_array[np.arange(len(z_label)), z_label] = 1\n    return one_hot_array\n\n\nclass Generator(nn.Module):\n\tdef __init__(self):\n\t\tsuper(Generator,self).__init__()\n\t\tself.layer1 = nn.Linear(110,7*7*256)\t\t\t\t# [batch,110] -> [batch,7*7*256]\t\t\n\t\tself.layer2 = nn.Sequential(OrderedDict([\n\t\t\t\t(\'conv1\', nn.ConvTranspose2d(256,128,3,2,1,1)),\t# [batch,256,7,7] -> [batch,128,14,14]\n\t\t\t\t(\'relu1\', nn.LeakyReLU()),\n\t\t\t\t(\'bn1\', nn.BatchNorm2d(128)),\n\t\t\t\t(\'conv2\', nn.ConvTranspose2d(128,64,3,1,1)),\t# [batch,128,14,14] -> [batch,64,14,14]\n\t\t\t\t(\'relu2\', nn.LeakyReLU()),\n\t\t\t\t(\'bn2\', nn.BatchNorm2d(64))\n\t\t\t]))\n\t\tself.layer3 = nn.Sequential(OrderedDict([\n\t\t\t\t(\'conv3\',nn.ConvTranspose2d(64,16,3,1,1)),\t# [batch,64,14,14] -> [batch,16,14,14]\n\t\t\t\t(\'relu3\',nn.LeakyReLU()),\n\t\t\t\t(\'bn3\',nn.BatchNorm2d(16)),\n\t\t\t\t(\'conv4\',nn.ConvTranspose2d(16,1,3,2,1,1)),\t# [batch,16,14,14] -> [batch,1,28,28]\n\t\t\t\t(\'relu4\',nn.LeakyReLU())\n\t\t\t]))\n\n\tdef forward(self,z):\n\t\tout = self.layer1(z)\n\t\tout = out.view(batch_size//num_gpus,256,7,7)\n\t\tout = self.layer2(out)\n\t\tout = self.layer3(out)\n\t\treturn out\n\n\nclass Discriminator(nn.Module):\n\tdef __init__(self):\n\t\tsuper(Discriminator,self).__init__()\n\t\tself.layer1 = nn.Sequential(OrderedDict([\n\t\t\t\t(\'conv1\',nn.Conv2d(1,16,3,padding=1)),   # [batch,1,28,28] -> [batch,16,28,28]\n\t\t\t\t(\'relu1\',nn.LeakyReLU()),\n\t\t\t\t(\'bn1\',nn.BatchNorm2d(16)),\n\t\t\t\t(\'conv2\',nn.Conv2d(16,32,3,padding=1)),  # [batch,16,28,28] -> [batch,32,28,28]\n\t\t\t\t(\'relu2\',nn.LeakyReLU()),\n\t\t\t\t(\'bn2\',nn.BatchNorm2d(32)),\n\t\t\t\t(\'max1\',nn.MaxPool2d(2,2))   \t\t # [batch,32,28,28] -> [batch,32,14,14]\n\t\t\t]))\n\t\tself.layer2 = nn.Sequential(OrderedDict([\n\t\t\t\t(\'conv3\',nn.Conv2d(32,64,3,padding=1)),  # [batch,32,14,14] -> [batch,64,14,14] \n\t\t\t\t(\'relu3\',nn.LeakyReLU()),\n\t\t\t\t(\'bn3\',nn.BatchNorm2d(64)),\t\n\t\t\t\t(\'max2\',nn.MaxPool2d(2,2)),\t\t # [batch,64,14,14] -> [batch,64,7,7]\n\t\t\t\t(\'conv4\',nn.Conv2d(64,128,3,padding=1)), # [batch,64,7,7] -> [batch,128,7,7]\n\t\t\t\t(\'relu4\',nn.LeakyReLU())\n\t\t\t]))\n\t\tself.fc = nn.Sequential(\n\t\t\t\tnn.Linear(128*7*7,1),\n\t\t\t\tnn.Sigmoid()\n\t\t\t)\n\t\tself.fc2 = nn.Sequential(\n\t\t\t\t\tnn.Linear(128*7*7,10),\n\t\t\t\t\tnn.LeakyReLU(),\n\t\t\t)\n\n\tdef forward(self,x):\n\t\tout = self.layer1(x)\n\t\tout = self.layer2(out)\n\t\tout = out.view(batch_size//num_gpus, -1)\n\t\toutput = self.fc(out)\n\t\tlabel = self.fc2(out)\n\t\treturn output,label\n    \n# put class instance on multi gpu\n\ngenerator = nn.DataParallel(Generator(),device_ids=[0,1]).cuda()\ndiscriminator = nn.DataParallel(Discriminator(),device_ids=[0,1]).cuda()\n\n# put labels on multi gpu\n\nones_label = Variable(torch.ones(batch_size,1)).cuda()\nzeros_label = Variable(torch.zeros(batch_size,1)).cuda()\n\n# loss function and optimizer \n# this time, use LSGAN loss(https://arxiv.org/abs/1611.04076v2)\n\nloss_func = nn.MSELoss()\ngen_optim = torch.optim.Adam(generator.parameters(), lr=learning_rate)\ndis_optim = torch.optim.Adam(discriminator.parameters(), lr=learning_rate)\n\n# model restore\n\ntry:\n\tgenerator, discriminator = torch.load(\'./model/infogan.pkl\')\n\tprint(""\\n--------model restored--------\\n"")\nexcept:\n\tprint(""\\n--------model not restored--------\\n"")\n\tpass\n\n# train \n\nfor i in range(epoch):\n    for j,(image,label) in enumerate(train_loader):\n        \n        # put image & label on gpu\n        image = Variable(image).cuda()\n        label = torch.from_numpy(int_to_onehot(label.numpy()))\n        label = Variable(label.type_as(torch.FloatTensor())).cuda()\n    \n        # generator \n        for k in range(2):\n            z_random = np.random.rand(batch_size,100)\n            z_label = np.random.randint(0, 10, size=batch_size)\n            \n            # change first 10 labels from random to 0~9          \n            for l in range(10):\n                z_label[l]=l\n            \n            # preprocess z\n            z_label_onehot = int_to_onehot(z_label)\n            z_concat = np.concatenate([z_random, z_label_onehot], axis=1)\n            z = Variable(torch.from_numpy(z_concat).type_as(torch.FloatTensor())).cuda()\n            z_label_onehot = Variable(torch.from_numpy(z_label_onehot).type_as(torch.FloatTensor())).cuda()\n\n            # calculate loss and apply gradients\n            # gen_loss = gan loss(fake) + categorical loss\n            gen_optim.zero_grad()\n            gen_fake = generator.forward(z)\n            dis_fake,label_fake = discriminator.forward(gen_fake)\n            gen_loss = torch.sum(loss_func(dis_fake,ones_label)) + discrete_latent_size * torch.sum(loss_func(label_fake,z_label_onehot))\n            gen_loss.backward(retain_variables=True)\n            gen_optim.step()\n\n        # discriminator\n        # dis_loss = gan_loss(fake & real) + categorical loss\n        dis_optim.zero_grad()\n        dis_real, label_real = discriminator.forward(image)\n        dis_loss = torch.sum(loss_func(dis_fake,zeros_label))+torch.sum(loss_func(dis_real,ones_label)) + discrete_latent_size * torch.sum(loss_func(label_real,label))\n        dis_loss.backward()\n        dis_optim.step()\n    \n    # model save\n    if i % 5 == 0:\n        torch.save([generator,discriminator],\'./model/infogan.pkl\')\n\n    # print loss and image save\n    print(""{}th iteration gen_loss: {} dis_loss: {}"".format(i,gen_loss.data,dis_loss.data))\n    v_utils.save_image(gen_fake.data[0:20],""./result/gen_{}.png"".format(i), nrow=5)\n'"
11_StyleTransfer_ResNet/StyleTransfer_LBFGS_gpu.py,7,"b'# https://discuss.pytorch.org/t/how-to-extract-features-of-an-image-from-a-trained-model/119/3\n# https://github.com/leongatys/PytorchNeuralStyleTransfer\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils as utils\nimport torch.utils.data as data\nimport torchvision.models as models\nimport torchvision.utils as v_utils\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# hyperparameters\n\ncontent_dir = ""./image/content/Neckarfront_origin.jpg""\nstyle_dir = ""./image/style/monet.jpg""\n\ncontent_layer_num = 4\nimage_size = 512\nepoch = 10000\n\n# import pretrained resnet50 model\n# check what layers are in the model\n\nresnet = models.resnet50(pretrained=True)\nfor name,module in resnet.named_children():\n\tprint(name)\n\n# resnet without fully connected layers\n# return activations in each layers \n\nclass Resnet(nn.Module):\n    def __init__(self):\n        super(Resnet,self).__init__()\n        self.layer0 = nn.Sequential(*list(resnet.children())[0:1])\n        self.layer1 = nn.Sequential(*list(resnet.children())[1:4])\n        self.layer2 = nn.Sequential(*list(resnet.children())[4:5])\n        self.layer3 = nn.Sequential(*list(resnet.children())[5:6])\n        self.layer4 = nn.Sequential(*list(resnet.children())[6:7])\n        self.layer5 = nn.Sequential(*list(resnet.children())[7:8])\n\n    def forward(self,x):\n        out_0 = self.layer0(x)\n        out_1 = self.layer1(out_0)\n        out_2 = self.layer2(out_1)\n        out_3 = self.layer3(out_2)\n        out_4 = self.layer4(out_3)\n        out_5 = self.layer5(out_4)\n\n        return out_0, out_1, out_2, out_3, out_4, out_5\n# read & preprocess image\n\ndef image_preprocess(img_dir):\n\timg = Image.open(img_dir)\n\ttransform = transforms.Compose([\n\t\t\t\t\ttransforms.Scale(image_size),\n\t\t\t\t\ttransforms.CenterCrop(image_size),\n\t\t\t\t\ttransforms.ToTensor(),\n\t\t\t\t\ttransforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], \n                                         std=[1,1,1]),\n\t\t\t\t])\n\timg = transform(img).view((-1,3,image_size,image_size))\n\treturn img\n\n# image post process\n\ndef image_postprocess(tensor):\n\ttransform = transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], \n                                     std=[1,1,1])\n\timg = transform(tensor.clone())\n\timg[img>1] = 1    \n\timg[img<0] = 0\n\treturn 2*img-1\n\n# show image given a tensor as input\n\ndef imshow(tensor):\n    image = tensor.clone().cpu()\n    image = image.view(3, image_size, image_size)\n    image = transforms.ToPILImage()(image)\n    plt.imshow(image)\n    plt.show()\n\n# gram matrix\n\nclass GramMatrix(nn.Module):\n    def forward(self, input):\n        b,c,h,w = input.size()\n        F = input.view(b, c, h*w)\n        G = torch.bmm(F, F.transpose(1,2)) \n        return G\n\n# gram matrix mean squared error\n\nclass GramMSELoss(nn.Module):\n    def forward(self, input, target):\n        out = nn.MSELoss()(GramMatrix()(input), target)\n        return(out)\n\n# initialize resnet and put on gpu\n# model is not updated so .requires_grad = False\n\nresnet = Resnet().cuda()\n\nfor param in resnet.parameters():\n    param.requires_grad = False\n\n# get content and style image & image to be generated\n\ncontent = Variable(image_preprocess(content_dir), requires_grad=False).cuda()\nstyle = Variable(image_preprocess(style_dir), requires_grad=False).cuda()\ngenerated = Variable(content.data.clone(),requires_grad=True)\n\nv_utils.save_image(image_postprocess(image_preprocess(content_dir)),""./content.png"")\nv_utils.save_image(image_postprocess(image_preprocess(style_dir)),""./style.png"")\n\n# set targets and style weights\n\nstyle_target = list(GramMatrix().cuda()(i) for i in resnet(style))\ncontent_target = resnet(content)[content_layer_num]\nstyle_weight = [1/n**2 for n in [64,64,256,512,1024,2048]]\n\n# set LBFGS optimizer\n# change the image through training\n\noptimizer = optim.LBFGS([generated])\n\niteration = [0]\nwhile iteration[0] < epoch:\n\n\tdef closure():\n\t\toptimizer.zero_grad()\n\t\tout = resnet(generated)\n\t\tstyle_loss = [GramMSELoss().cuda()(out[i],style_target[i])*style_weight[i] for i in range(len(style_target))]\n\t\tcontent_loss = nn.MSELoss().cuda()(out[content_layer_num],content_target)\n\t\ttotal_loss = 1000 * sum(style_loss) + sum(content_loss)\n\t\ttotal_loss.backward()\n\n\t\tif iteration[0] % 100 == 0:\n\t\t\tprint(total_loss)\n\t\t\tv_utils.save_image(image_postprocess(generated.data),""./gen_{}.png"".format(iteration[0]))\n\t\titeration[0] += 1\n\n\t\treturn total_loss\n\t\n\toptimizer.step(closure)'"
12_Semantic_Segmentation/Basic_blocks.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.utils as utils\nimport torch.nn.init as init\nimport torch.utils.data as data\nimport torchvision.utils as v_utils\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\n\ndef conv_block(in_dim,out_dim,act_fn):\n    model = nn.Sequential(\n        nn.Conv2d(in_dim,out_dim, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(out_dim),\n        act_fn,\n    )\n    return model\n\n\ndef conv_trans_block(in_dim,out_dim,act_fn):\n    model = nn.Sequential(\n        nn.ConvTranspose2d(in_dim,out_dim, kernel_size=3, stride=2, padding=1,output_padding=1),\n        nn.BatchNorm2d(out_dim),\n        act_fn,\n    )\n    return model\n\n\ndef maxpool():\n    pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n    return pool\n\n\ndef conv_block_2(in_dim,out_dim,act_fn):\n    model = nn.Sequential(\n        conv_block(in_dim,out_dim,act_fn),\n        nn.Conv2d(out_dim,out_dim, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(out_dim),\n    )\n    return model    \n\n\ndef conv_block_3(in_dim,out_dim,act_fn):\n    model = nn.Sequential(\n        conv_block(in_dim,out_dim,act_fn),\n        conv_block(out_dim,out_dim,act_fn),\n        nn.Conv2d(out_dim,out_dim, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(out_dim),\n    )\n    return model'"
12_Semantic_Segmentation/FusionNet.py,1,"b'from Basic_blocks import * \n\n\nclass Conv_residual_conv(nn.Module):\n\n    def __init__(self,in_dim,out_dim,act_fn):\n        super(Conv_residual_conv,self).__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        act_fn = act_fn\n\n        self.conv_1 = conv_block(self.in_dim,self.out_dim,act_fn)\n        self.conv_2 = conv_block_3(self.out_dim,self.out_dim,act_fn)\n        self.conv_3 = conv_block(self.out_dim,self.out_dim,act_fn)\n\n    def forward(self,input):\n        conv_1 = self.conv_1(input)\n        conv_2 = self.conv_2(conv_1)\n        res = conv_1 + conv_2\n        conv_3 = self.conv_3(res)\n        return conv_3\n\n\nclass FusionGenerator(nn.Module):\n\n    def __init__(self,input_nc, output_nc, ngf):\n        super(FusionGenerator,self).__init__()\n        self.in_dim = input_nc\n        self.out_dim = ngf\n        self.final_out_dim = output_nc\n        act_fn = nn.LeakyReLU(0.2, inplace=True)\n        act_fn_2 = nn.ReLU()\n\n        print(""\\n------Initiating FusionNet------\\n"")\n\n        # encoder\n\n        self.down_1 = Conv_residual_conv(self.in_dim, self.out_dim, act_fn)\n        self.pool_1 = maxpool()\n        self.down_2 = Conv_residual_conv(self.out_dim, self.out_dim * 2, act_fn)\n        self.pool_2 = maxpool()\n        self.down_3 = Conv_residual_conv(self.out_dim * 2, self.out_dim * 4, act_fn)\n        self.pool_3 = maxpool()\n        self.down_4 = Conv_residual_conv(self.out_dim * 4, self.out_dim * 8, act_fn)\n        self.pool_4 = maxpool()\n\n        # bridge\n\n        self.bridge = Conv_residual_conv(self.out_dim * 8, self.out_dim * 16, act_fn)\n\n        # decoder\n\n        self.deconv_1 = conv_trans_block(self.out_dim * 16, self.out_dim * 8, act_fn_2)\n        self.up_1 = Conv_residual_conv(self.out_dim * 8, self.out_dim * 8, act_fn_2)\n        self.deconv_2 = conv_trans_block(self.out_dim * 8, self.out_dim * 4, act_fn_2)\n        self.up_2 = Conv_residual_conv(self.out_dim * 4, self.out_dim * 4, act_fn_2)\n        self.deconv_3 = conv_trans_block(self.out_dim * 4, self.out_dim * 2, act_fn_2)\n        self.up_3 = Conv_residual_conv(self.out_dim * 2, self.out_dim * 2, act_fn_2)\n        self.deconv_4 = conv_trans_block(self.out_dim * 2, self.out_dim, act_fn_2)\n        self.up_4 = Conv_residual_conv(self.out_dim, self.out_dim, act_fn_2)\n\n        # output\n\n        self.out = nn.Conv2d(self.out_dim,self.final_out_dim, kernel_size=3, stride=1, padding=1)\n        self.out_2 = nn.Tanh()\n        \'\'\'\n        self.out = nn.Sequential(\n            nn.Conv2d(self.out_dim,self.final_out_dim, kernel_size=3, stride=1, padding=1),\n            #nn.BatchNorm2d(self.final_out_dim),\n            nn.Tanh(),\n        )\n        \'\'\'\n\n        # initialization\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight.data.normal_(0.0, 0.02)\n                m.bias.data.fill_(0)\n            \n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.normal_(1.0, 0.02)\n                m.bias.data.fill_(0)\n\n\n    def forward(self,input):\n\n        down_1 = self.down_1(input)\n        pool_1 = self.pool_1(down_1)\n        down_2 = self.down_2(pool_1)\n        pool_2 = self.pool_2(down_2)\n        down_3 = self.down_3(pool_2)\n        pool_3 = self.pool_3(down_3)\n        down_4 = self.down_4(pool_3)\n        pool_4 = self.pool_4(down_4)\n\n        bridge = self.bridge(pool_4)\n\n        deconv_1 = self.deconv_1(bridge)\n        skip_1 = (deconv_1 + down_4)/2\n        up_1 = self.up_1(skip_1)\n        deconv_2 = self.deconv_2(up_1)\n        skip_2 = (deconv_2 + down_3)/2\n        up_2 = self.up_2(skip_2)\n        deconv_3 = self.deconv_3(up_2)\n        skip_3 = (deconv_3 + down_2)/2\n        up_3 = self.up_3(skip_3)\n        deconv_4 = self.deconv_4(up_3)\n        skip_4 = (deconv_4 + down_1)/2\n        up_4 = self.up_4(skip_4)\n\n        out = self.out(up_4)\n        out = self.out_2(out)\n        #out = torch.clamp(out, min=-1, max=1)\n\n        return out'"
12_Semantic_Segmentation/UNet.py,4,"b'from Basic_blocks import * \n\nclass UnetGenerator(nn.Module):\n\n\tdef __init__(self,in_dim,out_dim,num_filter):\n\t\tsuper(UnetGenerator,self).__init__()\n\t\tself.in_dim = in_dim\n\t\tself.out_dim = out_dim\n\t\tself.num_filter = num_filter\n\t\tact_fn = nn.LeakyReLU(0.2, inplace=True)\n\n\t\tprint(""\\n------Initiating U-Net------\\n"")\n\n\t\tself.down_1 = conv_block_2(self.in_dim,self.num_filter,act_fn)\n\t\tself.pool_1 = maxpool()\n\t\tself.down_2 = conv_block_2(self.num_filter*1,self.num_filter*2,act_fn)\n\t\tself.pool_2 = maxpool()\n\t\tself.down_3 = conv_block_2(self.num_filter*2,self.num_filter*4,act_fn)\n\t\tself.pool_3 = maxpool()\n\t\tself.down_4 = conv_block_2(self.num_filter*4,self.num_filter*8,act_fn)\n\t\tself.pool_4 = maxpool()\n\n\t\tself.bridge = conv_block_2(self.num_filter*8,self.num_filter*16,act_fn)\n\n\t\tself.trans_1 = conv_trans_block(self.num_filter*16,self.num_filter*8,act_fn)\n\t\tself.up_1 = conv_block_2(self.num_filter*16,self.num_filter*8,act_fn)\n\t\tself.trans_2 = conv_trans_block(self.num_filter*8,self.num_filter*4,act_fn)\n\t\tself.up_2 = conv_block_2(self.num_filter*8,self.num_filter*4,act_fn)\n\t\tself.trans_3 = conv_trans_block(self.num_filter*4,self.num_filter*2,act_fn)\n\t\tself.up_3 = conv_block_2(self.num_filter*4,self.num_filter*2,act_fn)\n\t\tself.trans_4 = conv_trans_block(self.num_filter*2,self.num_filter*1,act_fn)\n\t\tself.up_4 = conv_block_2(self.num_filter*2,self.num_filter*1,act_fn)\n\n\t\tself.out = nn.Sequential(\n\t\t\tnn.Conv2d(self.num_filter,self.out_dim,3,1,1),\n\t\t\tnn.Tanh(),\n\t\t)\n\n\tdef forward(self,input):\n\t\tdown_1 = self.down_1(input)\n\t\tpool_1 = self.pool_1(down_1)\n\t\tdown_2 = self.down_2(pool_1)\n\t\tpool_2 = self.pool_2(down_2)\n\t\tdown_3 = self.down_3(pool_2)\n\t\tpool_3 = self.pool_3(down_3)\n\t\tdown_4 = self.down_4(pool_3)\n\t\tpool_4 = self.pool_4(down_4)\n\n\t\tbridge = self.bridge(pool_4)\n\n\t\ttrans_1 = self.trans_1(bridge)\n\t\tconcat_1 = torch.cat([trans_1,down_4],dim=1)\n\t\tup_1 = self.up_1(concat_1)\n\t\ttrans_2 = self.trans_2(up_1)\n\t\tconcat_2 = torch.cat([trans_2,down_3],dim=1)\n\t\tup_2 = self.up_2(concat_2)\n\t\ttrans_3 = self.trans_3(up_2)\n\t\tconcat_3 = torch.cat([trans_3,down_2],dim=1)\n\t\tup_3 = self.up_3(concat_3)\n\t\ttrans_4 = self.trans_4(up_3)\n\t\tconcat_4 = torch.cat([trans_4,down_1],dim=1)\n\t\tup_4 = self.up_4(concat_4)\n\n\t\tout = self.out(up_4)\n\n\t\treturn out'"
12_Semantic_Segmentation/main.py,4,"b'# Semantic Segmentation\n# Code by GunhoChoi\n\nfrom FusionNet import * \nfrom UNet import *\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--network"",type=str,default=""fusionnet"",help=""choose between fusionnet & unet"")\nparser.add_argument(""--batch_size"",type=int,default=1,help=""batch size"")\nparser.add_argument(""--num_gpu"",type=int,default=1,help=""number of gpus"")\nargs = parser.parse_args()\n\n# hyperparameters\n\nbatch_size = args.batch_size\nimg_size = 256\nlr = 0.0002\nepoch = 100\n\n# input pipeline\n\nimg_dir = ""./maps/""\nimg_data = dset.ImageFolder(root=img_dir, transform = transforms.Compose([\n                                            transforms.Scale(size=img_size),\n                                            transforms.CenterCrop(size=(img_size,img_size*2)),\n                                            transforms.ToTensor(),\n                                            ]))\nimg_batch = data.DataLoader(img_data, batch_size=batch_size,\n                            shuffle=True, num_workers=2)\n\n# initiate Generator\n\nif args.network == ""fusionnet"":\n\tgenerator = nn.DataParallel(FusionGenerator(3,3,64),device_ids=[i for i in range(args.num_gpu)]).cuda()\nelif args.network == ""unet"":\n\tgenerator = nn.DataParallel(UnetGenerator(3,3,64),device_ids=[i for i in range(args.num_gpu)]).cuda()\n\n# load pretrained model\n\ntry:\n    generator = torch.load(\'./model/{}.pkl\'.format(args.network))\n    print(""\\n--------model restored--------\\n"")\nexcept:\n    print(""\\n--------model not restored--------\\n"")\n    pass\n\n# loss function & optimizer\n\nrecon_loss_func = nn.MSELoss()\ngen_optimizer = torch.optim.Adam(generator.parameters(),lr=lr)\n\n# training\n\nfile = open(\'./{}_mse_loss\'.format(args.network), \'w\')\nfor i in range(epoch):\n    for _,(image,label) in enumerate(img_batch):\n        satel_image, map_image = torch.chunk(image, chunks=2, dim=3) \n        \n        gen_optimizer.zero_grad()\n\n        x = Variable(satel_image).cuda(0)\n        y_ = Variable(map_image).cuda(0)\n        y = generator.forward(x)\n        \n        loss = recon_loss_func(y,y_)\n        file.write(str(loss)+""\\n"")\n        loss.backward()\n        gen_optimizer.step()\n\n        if _ % 400 ==0:\n            print(i)\n            print(loss)\n            v_utils.save_image(x.cpu().data,""./result/original_image_{}_{}.png"".format(i,_))\n            v_utils.save_image(y_.cpu().data,""./result/label_image_{}_{}.png"".format(i,_))\n            v_utils.save_image(y.cpu().data,""./result/gen_image_{}_{}.png"".format(i,_))\n            torch.save(generator,\'./model/{}.pkl\'.format(args.network))    \n'"
