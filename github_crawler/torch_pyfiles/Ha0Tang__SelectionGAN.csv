file_path,api_count,code
person_transfer/test.py,0,"b""import time\nimport os\nfrom options.test_options import TestOptions\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nfrom util.visualizer import Visualizer\nfrom util import html\nimport time\n\nopt = TestOptions().parse()\nopt.nThreads = 1   # test code only supports nThreads = 1\nopt.batchSize = 1  # test code only supports batchSize = 1\nopt.serial_batches = True  # no shuffle\nopt.no_flip = True  # no flip\n\ndata_loader = CreateDataLoader(opt)\ndataset = data_loader.load_data()\nmodel = create_model(opt)\nvisualizer = Visualizer(opt)\n# create website\nweb_dir = os.path.join(opt.results_dir, opt.name, '%s_%s' % (opt.phase, opt.which_epoch))\n\nwebpage = html.HTML(web_dir, 'Experiment = %s, Phase = %s, Epoch = %s' % (opt.name, opt.phase, opt.which_epoch))\n\nprint(opt.how_many)\nprint(len(dataset))\n\nmodel = model.eval()\nprint(model.training)\n\nopt.how_many = 999999\n# test\nfor i, data in enumerate(dataset):\n    print(' process %d/%d img ..'%(i,opt.how_many))\n    if i >= opt.how_many:\n        break\n    model.set_input(data)\n    startTime = time.time()\n    model.test()\n    endTime = time.time()\n    print(endTime-startTime)\n    visuals = model.get_current_visuals()\n    img_path = model.get_image_paths()\n    img_path = [img_path]\n    print(img_path)\n    visualizer.save_images(webpage, visuals, img_path)\n\nwebpage.save()\n\n\n\n\n"""
person_transfer/train.py,0,"b""import time\nfrom options.train_options import TrainOptions\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nfrom util.visualizer import Visualizer\n\nopt = TrainOptions().parse()\ndata_loader = CreateDataLoader(opt)\ndataset = data_loader.load_data()\ndataset_size = len(data_loader)\nprint('#training images = %d' % dataset_size)\n\nmodel = create_model(opt)\nvisualizer = Visualizer(opt)\ntotal_steps = 0\n\nfor epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):\n    epoch_start_time = time.time()\n    epoch_iter = 0\n\n    for i, data in enumerate(dataset):\n        iter_start_time = time.time()\n        visualizer.reset()\n        total_steps += opt.batchSize\n        epoch_iter += opt.batchSize\n        model.set_input(data)\n        model.optimize_parameters()\n\n        if total_steps % opt.display_freq == 0:\n            save_result = total_steps % opt.update_html_freq == 0\n            visualizer.display_current_results(model.get_current_visuals(), epoch, save_result)\n\n        if total_steps % opt.print_freq == 0:\n            errors = model.get_current_errors()\n            t = (time.time() - iter_start_time) / opt.batchSize\n            visualizer.print_current_errors(epoch, epoch_iter, errors, t)\n            if opt.display_id > 0:\n                visualizer.plot_current_errors(epoch, float(epoch_iter)/dataset_size, opt, errors)\n\n        if total_steps % opt.save_latest_freq == 0:\n            print('saving the latest model (epoch %d, total_steps %d)' %\n                  (epoch, total_steps))\n            model.save('latest')\n\n    if epoch % opt.save_epoch_freq == 0:\n        print('saving the model at the end of epoch %d, iters %d' %\n              (epoch, total_steps))\n        model.save('latest')\n        model.save(epoch)\n\n    print('End of epoch %d / %d \\t Time Taken: %d sec' %\n          (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n    model.update_learning_rate()\n"""
selectiongan_v1/test.py,0,"b""import os\nfrom options.test_options import TestOptions\nfrom data import CreateDataLoader\nfrom models import create_model\nfrom util.visualizer import save_images\nfrom util import html\n\n\nif __name__ == '__main__':\n    opt = TestOptions().parse()\n    opt.nThreads = 1   # test code only supports nThreads = 1\n    opt.batchSize = 1  # test code only supports batchSize = 1\n    opt.serial_batches = True  # no shuffle\n    opt.no_flip = True  # no flip\n    opt.display_id = -1  # no visdom display\n    data_loader = CreateDataLoader(opt)\n    dataset = data_loader.load_data()\n    model = create_model(opt)\n    model.setup(opt)\n    # create website\n    web_dir = os.path.join(opt.results_dir, opt.name, '%s_%s' % (opt.phase, opt.which_epoch))\n    webpage = html.HTML(web_dir, 'Experiment = %s, Phase = %s, Epoch = %s' % (opt.name, opt.phase, opt.which_epoch))\n    # test\n\n    # Set eval mode. This only affects layers like batch norm and drop out. \n    if opt.eval:\n        model.eval()\n\n    for i, data in enumerate(dataset):\n        if i >= opt.how_many:\n            break\n        model.set_input(data)\n        model.test()\n        visuals = model.get_current_visuals()\n        img_path = model.get_image_paths()\n        if i % 5 == 0:\n            print('processing (%04d)-th image... %s' % (i, img_path))\n        save_images(webpage, visuals, img_path, aspect_ratio=opt.aspect_ratio, width=opt.display_winsize)\n\n    webpage.save()\n"""
selectiongan_v1/train.py,0,"b""import time\nfrom options.train_options import TrainOptions\nfrom data import CreateDataLoader\nfrom models import create_model\nfrom util.visualizer import Visualizer\n\nif __name__ == '__main__':\n    opt = TrainOptions().parse()\n    data_loader = CreateDataLoader(opt)\n    dataset = data_loader.load_data()\n    dataset_size = len(data_loader)\n    print('#training images = %d' % dataset_size)\n\n    model = create_model(opt)\n    model.setup(opt)\n    visualizer = Visualizer(opt)\n    total_steps = 0\n\n    for epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):\n        epoch_start_time = time.time()\n        iter_data_time = time.time()\n        epoch_iter = 0\n\n        for i, data in enumerate(dataset):\n            iter_start_time = time.time()\n            if total_steps % opt.print_freq == 0:\n                t_data = iter_start_time - iter_data_time\n            visualizer.reset()\n            total_steps += opt.batchSize\n            epoch_iter += opt.batchSize\n            model.set_input(data)\n            model.optimize_parameters()\n\n            if total_steps % opt.display_freq == 0:\n                save_result = total_steps % opt.update_html_freq == 0\n                visualizer.display_current_results(model.get_current_visuals(), epoch, save_result)\n\n            if total_steps % opt.print_freq == 0:\n                losses = model.get_current_losses()\n                t = (time.time() - iter_start_time) / opt.batchSize\n                visualizer.print_current_losses(epoch, epoch_iter, losses, t, t_data)\n                if opt.display_id > 0:\n                    visualizer.plot_current_losses(epoch, float(epoch_iter) / dataset_size, opt, losses)\n\n            if total_steps % opt.save_latest_freq == 0:\n                print('saving the latest model (epoch %d, total_steps %d)' %\n                      (epoch, total_steps))\n                model.save_networks('latest')\n\n            iter_data_time = time.time()\n        if epoch % opt.save_epoch_freq == 0:\n            print('saving the model at the end of epoch %d, iters %d' %\n                  (epoch, total_steps))\n            model.save_networks('latest')\n            model.save_networks(epoch)\n\n        print('End of epoch %d / %d \\t Time Taken: %d sec' %\n              (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n        model.update_learning_rate()\n"""
selectiongan_v2/test.py,0,"b""import os\nfrom options.test_options import TestOptions\nfrom data import CreateDataLoader\nfrom models import create_model\nfrom util.visualizer import save_images\nfrom util import html\n\n\nif __name__ == '__main__':\n    opt = TestOptions().parse()\n    opt.nThreads = 1   # test code only supports nThreads = 1\n    opt.batchSize = 1  # test code only supports batchSize = 1\n    opt.serial_batches = True  # no shuffle\n    opt.no_flip = True  # no flip\n    opt.display_id = -1  # no visdom display\n    data_loader = CreateDataLoader(opt)\n    dataset = data_loader.load_data()\n    model = create_model(opt)\n    model.setup(opt)\n    # create website\n    web_dir = os.path.join(opt.results_dir, opt.name, '%s_%s' % (opt.phase, opt.which_epoch))\n    webpage = html.HTML(web_dir, 'Experiment = %s, Phase = %s, Epoch = %s' % (opt.name, opt.phase, opt.which_epoch))\n    # test\n\n    # Set eval mode. This only affects layers like batch norm and drop out. \n    if opt.eval:\n        model.eval()\n\n    for i, data in enumerate(dataset):\n        if i >= opt.how_many:\n            break\n        model.set_input(data)\n        model.test()\n        visuals = model.get_current_visuals()\n        img_path = model.get_image_paths()\n        if i % 5 == 0:\n            print('processing (%04d)-th image... %s' % (i, img_path))\n        save_images(webpage, visuals, img_path, aspect_ratio=opt.aspect_ratio, width=opt.display_winsize)\n\n    webpage.save()\n"""
selectiongan_v2/train.py,0,"b""import time\nfrom options.train_options import TrainOptions\nfrom data import CreateDataLoader\nfrom models import create_model\nfrom util.visualizer import Visualizer\n\nif __name__ == '__main__':\n    opt = TrainOptions().parse()\n    data_loader = CreateDataLoader(opt)\n    dataset = data_loader.load_data()\n    dataset_size = len(data_loader)\n    print('#training images = %d' % dataset_size)\n\n    model = create_model(opt)\n    model.setup(opt)\n    visualizer = Visualizer(opt)\n    total_steps = 0\n\n    for epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):\n        epoch_start_time = time.time()\n        iter_data_time = time.time()\n        epoch_iter = 0\n\n        for i, data in enumerate(dataset):\n            iter_start_time = time.time()\n            if total_steps % opt.print_freq == 0:\n                t_data = iter_start_time - iter_data_time\n            visualizer.reset()\n            total_steps += opt.batchSize\n            epoch_iter += opt.batchSize\n            model.set_input(data)\n            model.optimize_parameters()\n\n            if total_steps % opt.display_freq == 0:\n                save_result = total_steps % opt.update_html_freq == 0\n                visualizer.display_current_results(model.get_current_visuals(), epoch, save_result)\n\n            if total_steps % opt.print_freq == 0:\n                losses = model.get_current_losses()\n                t = (time.time() - iter_start_time) / opt.batchSize\n                visualizer.print_current_losses(epoch, epoch_iter, losses, t, t_data)\n                if opt.display_id > 0:\n                    visualizer.plot_current_losses(epoch, float(epoch_iter) / dataset_size, opt, losses)\n\n            if total_steps % opt.save_latest_freq == 0:\n                print('saving the latest model (epoch %d, total_steps %d)' %\n                      (epoch, total_steps))\n                model.save_networks('latest')\n\n            iter_data_time = time.time()\n        if epoch % opt.save_epoch_freq == 0:\n            print('saving the model at the end of epoch %d, iters %d' %\n                  (epoch, total_steps))\n            model.save_networks('latest')\n            model.save_networks(epoch)\n\n        print('End of epoch %d / %d \\t Time Taken: %d sec' %\n              (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n        model.update_learning_rate()\n"""
semantic_synthesis/test.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport os\nfrom collections import OrderedDict\n\nimport data\nfrom options.test_options import TestOptions\nfrom models.pix2pix_model import Pix2PixModel\nfrom util.visualizer import Visualizer\nfrom util import html\n\nopt = TestOptions().parse()\n\ndataloader = data.create_dataloader(opt)\n\nmodel = Pix2PixModel(opt)\nmodel.eval()\n\nvisualizer = Visualizer(opt)\n\n# create a webpage that summarizes the all results\nweb_dir = os.path.join(opt.results_dir, opt.name,\n                       \'%s_%s\' % (opt.phase, opt.which_epoch))\nwebpage = html.HTML(web_dir,\n                    \'Experiment = %s, Phase = %s, Epoch = %s\' %\n                    (opt.name, opt.phase, opt.which_epoch))\n\n# test\nfor i, data_i in enumerate(dataloader):\n    if i * opt.batchSize >= opt.how_many:\n        break\n\n    generated = model(data_i, mode=\'inference\')\n\n    img_path = data_i[\'path\']\n    for b in range(generated.shape[0]):\n        print(\'process image... %s\' % img_path[b])\n        visuals = OrderedDict([(\'input_label\', data_i[\'label\'][b]),\n                               (\'synthesized_image\', generated[b])])\n        visualizer.save_images(webpage, visuals, img_path[b:b + 1])\n\nwebpage.save()\n'"
semantic_synthesis/train.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport sys\nfrom collections import OrderedDict\nfrom options.train_options import TrainOptions\nimport data\nfrom util.iter_counter import IterationCounter\nfrom util.visualizer import Visualizer\nfrom trainers.pix2pix_trainer import Pix2PixTrainer\n\n# parse options\nopt = TrainOptions().parse()\n\n# print options to help debugging\nprint(\' \'.join(sys.argv))\n\n# load the dataset\ndataloader = data.create_dataloader(opt)\n\n# create trainer for our model\ntrainer = Pix2PixTrainer(opt)\n\n# create tool for counting iterations\niter_counter = IterationCounter(opt, len(dataloader))\n\n# create tool for visualization\nvisualizer = Visualizer(opt)\n\nfor epoch in iter_counter.training_epochs():\n    iter_counter.record_epoch_start(epoch)\n    for i, data_i in enumerate(dataloader, start=iter_counter.epoch_iter):\n        iter_counter.record_one_iteration()\n\n        # Training\n        # train generator\n        if i % opt.D_steps_per_G == 0:\n            trainer.run_generator_one_step(data_i)\n\n        # train discriminator\n        trainer.run_discriminator_one_step(data_i)\n\n        # Visualizations\n        if iter_counter.needs_printing():\n            losses = trainer.get_latest_losses()\n            visualizer.print_current_errors(epoch, iter_counter.epoch_iter,\n                                            losses, iter_counter.time_per_iter)\n            visualizer.plot_current_errors(losses, iter_counter.total_steps_so_far)\n\n        if iter_counter.needs_displaying():\n            visuals = OrderedDict([(\'input_label\', data_i[\'label\']),\n                                   (\'synthesized_image\', trainer.get_latest_generated()),\n                                   (\'real_image\', data_i[\'image\'])])\n            visualizer.display_current_results(visuals, epoch, iter_counter.total_steps_so_far)\n\n        if iter_counter.needs_saving():\n            print(\'saving the latest model (epoch %d, total_steps %d)\' %\n                  (epoch, iter_counter.total_steps_so_far))\n            trainer.save(\'latest\')\n            iter_counter.record_current_iter()\n\n    trainer.update_learning_rate(epoch)\n    iter_counter.record_epoch_end()\n\n    if epoch % opt.save_epoch_freq == 0 or \\\n       epoch == iter_counter.total_epochs:\n        print(\'saving the model at the end of epoch %d, iters %d\' %\n              (epoch, iter_counter.total_steps_so_far))\n        trainer.save(\'latest\')\n        trainer.save(epoch)\n\nprint(\'Training was successfully finished.\')\n'"
person_transfer/data/__init__.py,0,b''
person_transfer/data/base_data_loader.py,0,"b'\nclass BaseDataLoader():\n    def __init__(self):\n        pass\n    \n    def initialize(self, opt):\n        self.opt = opt\n        pass\n\n    def load_data():\n        return None\n\n        \n        \n'"
person_transfer/data/base_dataset.py,1,"b""import torch.utils.data as data\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\nclass BaseDataset(data.Dataset):\n    def __init__(self):\n        super(BaseDataset, self).__init__()\n\n    def name(self):\n        return 'BaseDataset'\n\n    def initialize(self, opt):\n        pass\n\ndef get_transform(opt):\n    transform_list = []\n    if opt.resize_or_crop == 'resize_and_crop':\n        osize = [opt.loadSize, opt.loadSize]\n        transform_list.append(transforms.Scale(osize, Image.BICUBIC))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == 'crop':\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == 'scale_width':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.fineSize)))\n    elif opt.resize_or_crop == 'scale_width_and_crop':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.loadSize)))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n\n    transform_list += [transforms.ToTensor(),\n                       transforms.Normalize((0.5, 0.5, 0.5),\n                                            (0.5, 0.5, 0.5))]\n    return transforms.Compose(transform_list)\n\ndef __scale_width(img, target_width):\n    ow, oh = img.size\n    if (ow == target_width):\n        return img\n    w = target_width\n    h = int(target_width * oh / ow)\n    return img.resize((w, h), Image.BICUBIC)\n"""
person_transfer/data/custom_dataset_data_loader.py,2,"b'import torch.utils.data\nfrom data.base_data_loader import BaseDataLoader\n\n\ndef CreateDataset(opt):\n    dataset = None\n    \n    if opt.dataset_mode == \'keypoint\':\n        from data.keypoint import KeyDataset\n        dataset = KeyDataset()\n\n    else:\n        raise ValueError(""Dataset [%s] not recognized."" % opt.dataset_mode)\n\n    print(""dataset [%s] was created"" % (dataset.name()))\n    dataset.initialize(opt)\n    return dataset\n\n\nclass CustomDatasetDataLoader(BaseDataLoader):\n    def name(self):\n        return \'CustomDatasetDataLoader\'\n\n    def initialize(self, opt):\n        BaseDataLoader.initialize(self, opt)\n        self.dataset = CreateDataset(opt)\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=opt.batchSize,\n            shuffle=not opt.serial_batches,\n            num_workers=int(opt.nThreads))\n\n    def load_data(self):\n        return self\n\n    def __len__(self):\n        return min(len(self.dataset), self.opt.max_dataset_size)\n\n    def __iter__(self):\n        for i, data in enumerate(self.dataloader):\n            if i >= self.opt.max_dataset_size:\n                break\n            yield data\n'"
person_transfer/data/data_loader.py,0,b'\ndef CreateDataLoader(opt):\n    from data.custom_dataset_data_loader import CustomDatasetDataLoader\n    data_loader = CustomDatasetDataLoader()\n    print(data_loader.name())\n    data_loader.initialize(opt)\n    return data_loader\n'
person_transfer/data/image_folder.py,1,"b'###############################################################################\n# Code from\n# https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n# Modified the original code so that it also loads images from the current\n# directory as well as the subdirectories\n###############################################################################\n\nimport torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef make_dataset(dir):\n    images = []\n    assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n\n    for root, _, fnames in sorted(os.walk(dir)):\n        for fname in fnames:\n            if is_image_file(fname):\n                path = os.path.join(root, fname)\n                images.append(path)\n\n    return images\n\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\n\nclass ImageFolder(data.Dataset):\n\n    def __init__(self, root, transform=None, return_paths=False,\n                 loader=default_loader):\n        imgs = make_dataset(root)\n        if len(imgs) == 0:\n            raise(RuntimeError(""Found 0 images in: "" + root + ""\\n""\n                               ""Supported image extensions are: "" +\n                               "","".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.imgs = imgs\n        self.transform = transform\n        self.return_paths = return_paths\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.return_paths:\n            return img, path\n        else:\n            return img\n\n    def __len__(self):\n        return len(self.imgs)\n'"
person_transfer/data/keypoint.py,4,"b""import os.path\nimport torchvision.transforms as transforms\nfrom data.base_dataset import BaseDataset, get_transform\nfrom data.image_folder import make_dataset\nfrom PIL import Image\nimport PIL\nimport random\nimport pandas as pd\nimport numpy as np\nimport torch\n\n\nclass KeyDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_P = os.path.join(opt.dataroot, opt.phase) #person images\n        self.dir_K = os.path.join(opt.dataroot, opt.phase + 'K') #keypoints\n\n        self.init_categories(opt.pairLst)\n        self.transform = get_transform(opt)\n\n    def init_categories(self, pairLst):\n        pairs_file_train = pd.read_csv(pairLst)\n        self.size = len(pairs_file_train)\n        self.pairs = []\n        print('Loading data pairs ...')\n        for i in range(self.size):\n            pair = [pairs_file_train.iloc[i]['from'], pairs_file_train.iloc[i]['to']]\n            self.pairs.append(pair)\n\n        print('Loading data pairs finished ...')\n\n    def __getitem__(self, index):\n        if self.opt.phase == 'train':\n            index = random.randint(0, self.size-1)\n\n        P1_name, P2_name = self.pairs[index]\n        P1_path = os.path.join(self.dir_P, P1_name) # person 1\n        BP1_path = os.path.join(self.dir_K, P1_name + '.npy') # bone of person 1\n\n        # person 2 and its bone\n        P2_path = os.path.join(self.dir_P, P2_name) # person 2\n        BP2_path = os.path.join(self.dir_K, P2_name + '.npy') # bone of person 2\n\n\n        P1_img = Image.open(P1_path).convert('RGB')\n        P2_img = Image.open(P2_path).convert('RGB')\n\n        BP1_img = np.load(BP1_path) # h, w, c\n        BP2_img = np.load(BP2_path) \n        # use flip\n        if self.opt.phase == 'train' and self.opt.use_flip:\n            # print ('use_flip ...')\n            flip_random = random.uniform(0,1)\n            \n            if flip_random > 0.5:\n                # print('fliped ...')\n                P1_img = P1_img.transpose(Image.FLIP_LEFT_RIGHT)\n                P2_img = P2_img.transpose(Image.FLIP_LEFT_RIGHT)\n\n                BP1_img = np.array(BP1_img[:, ::-1, :]) # flip\n                BP2_img = np.array(BP2_img[:, ::-1, :]) # flip\n\n            BP1 = torch.from_numpy(BP1_img).float() #h, w, c\n            BP1 = BP1.transpose(2, 0) #c,w,h\n            BP1 = BP1.transpose(2, 1) #c,h,w \n\n            BP2 = torch.from_numpy(BP2_img).float()\n            BP2 = BP2.transpose(2, 0) #c,w,h\n            BP2 = BP2.transpose(2, 1) #c,h,w \n\n            P1 = self.transform(P1_img)\n            P2 = self.transform(P2_img)\n\n        else:\n            BP1 = torch.from_numpy(BP1_img).float() #h, w, c\n            BP1 = BP1.transpose(2, 0) #c,w,h\n            BP1 = BP1.transpose(2, 1) #c,h,w \n\n            BP2 = torch.from_numpy(BP2_img).float()\n            BP2 = BP2.transpose(2, 0) #c,w,h\n            BP2 = BP2.transpose(2, 1) #c,h,w \n\n            P1 = self.transform(P1_img)\n            P2 = self.transform(P2_img)\n\n        return {'P1': P1, 'BP1': BP1, 'P2': P2, 'BP2': BP2,\n                'P1_path': P1_name, 'P2_path': P2_name}\n                \n\n    def __len__(self):\n        if self.opt.phase == 'train':\n            return 4000\n        elif self.opt.phase == 'test':\n            return self.size\n\n    def name(self):\n        return 'KeyDataset'\n"""
person_transfer/fashion/rename.py,0,"b'import os\nimport shutil\n\nIMG_EXTENSIONS = [\n\'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n\'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n]\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\ndef make_dataset(dir):\n    images = []\n    assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n    new_root = \'DeepFashion\'\n    if not os.path.exists(new_root):\n        os.mkdir(new_root)\n\n    for root, _, fnames in sorted(os.walk(dir)):\n        for fname in fnames:\n            if is_image_file(fname):\n                path = os.path.join(root, fname)\n                path_names = path.split(\'/\') \n                #path_names[2] = path_names[2].replace(\'_\', \'\')\n                path_names[3] = path_names[3].replace(\'_\', \'\')\n                path_names[4] = path_names[4].split(\'_\')[0] + ""_"" + """".join(path_names[4].split(\'_\')[1:])\n                path_names = """".join(path_names)\n                new_path = os.path.join(root, path_names)\n\n                os.rename(path, new_path)\n                shutil.move(new_path, os.path.join(new_root, path_names))\n\nmake_dataset(\'fashion\')\n'"
person_transfer/losses/L1_plus_perceptualLoss.py,5,"b'from __future__ import absolute_import\n\nimport torch\nfrom torch import nn\nimport numpy as np\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nclass L1_plus_perceptualLoss(nn.Module):\n    def __init__(self, lambda_L1, lambda_perceptual, perceptual_layers, gpu_ids, percep_is_l1):\n        super(L1_plus_perceptualLoss, self).__init__()\n\n        self.lambda_L1 = lambda_L1\n        self.lambda_perceptual = lambda_perceptual\n        self.gpu_ids = gpu_ids\n\n        self.percep_is_l1 = percep_is_l1\n\n        vgg = models.vgg19(pretrained=True).features\n        self.vgg_submodel = nn.Sequential()\n        for i,layer in enumerate(list(vgg)):\n            self.vgg_submodel.add_module(str(i),layer)\n            if i == perceptual_layers:\n                break\n        self.vgg_submodel = torch.nn.DataParallel(self.vgg_submodel, device_ids=gpu_ids).cuda()\n\n        print(self.vgg_submodel)\n\n    def forward(self, inputs, targets):\n        if self.lambda_L1 == 0 and self.lambda_perceptual == 0:\n            return torch.zeros(1).cuda(), torch.zeros(1), torch.zeros(1)\n        # normal L1\n        loss_l1 = F.l1_loss(inputs, targets) * self.lambda_L1\n\n        # perceptual L1\n        mean = torch.FloatTensor(3)\n        mean[0] = 0.485\n        mean[1] = 0.456\n        mean[2] = 0.406\n        mean = mean.resize(1, 3, 1, 1).cuda()\n\n        std = torch.FloatTensor(3)\n        std[0] = 0.229\n        std[1] = 0.224\n        std[2] = 0.225\n        std = std.resize(1, 3, 1, 1).cuda()\n\n        fake_p2_norm = (inputs + 1)/2 # [-1, 1] => [0, 1]\n        fake_p2_norm = (fake_p2_norm - mean)/std\n\n        input_p2_norm = (targets + 1)/2 # [-1, 1] => [0, 1]\n        input_p2_norm = (input_p2_norm - mean)/std\n\n\n        fake_p2_norm = self.vgg_submodel(fake_p2_norm)\n        input_p2_norm = self.vgg_submodel(input_p2_norm)\n        input_p2_norm_no_grad = input_p2_norm.detach()\n\n        if self.percep_is_l1 == 1:\n            # use l1 for perceptual loss\n            loss_perceptual = F.l1_loss(fake_p2_norm, input_p2_norm_no_grad) * self.lambda_perceptual\n        else:\n            # use l2 for perceptual loss\n            loss_perceptual = F.mse_loss(fake_p2_norm, input_p2_norm_no_grad) * self.lambda_perceptual\n\n        loss = loss_l1 + loss_perceptual\n\n        return loss, loss_l1, loss_perceptual\n\n'"
person_transfer/losses/__init__.py,0,b''
person_transfer/losses/ssim.py,4,"b'import torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nfrom math import exp\n\ndef gaussian(window_size, sigma):\n    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n    return gauss/gauss.sum()\n\ndef create_window(window_size, channel):\n    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n    return window\n\ndef _ssim(img1, img2, window, window_size, channel, size_average = True):\n    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    mu1_mu2 = mu1*mu2\n\n    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n\n    C1 = 0.01**2\n    C2 = 0.03**2\n\n    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n\n    if size_average:\n        return ssim_map.mean()\n    else:\n        return ssim_map.mean(1).mean(1).mean(1)\n\nclass SSIM(torch.nn.Module):\n    def __init__(self, window_size = 11, size_average = True):\n        # call __init__() of the parent class\n        super(SSIM, self).__init__()\n        self.window_size = window_size\n        self.size_average = size_average\n        self.channel = 1\n        self.window = create_window(window_size, self.channel)\n\n    def forward(self, img1, img2):\n        (_, channel, _, _) = img1.size()\n\n        if channel == self.channel and self.window.data.type() == img1.data.type():\n            window = self.window\n        else:\n            window = create_window(self.window_size, channel)\n            \n            if img1.is_cuda:\n                window = window.cuda(img1.get_device())\n            window = window.type_as(img1)\n            \n            self.window = window\n            self.channel = channel\n\n\n        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n\ndef ssim(img1, img2, window_size = 11, size_average = True):\n    (_, channel, _, _) = img1.size()\n    window = create_window(window_size, channel)\n    \n    if img1.is_cuda:\n        window = window.cuda(img1.get_device())\n    window = window.type_as(img1)\n    \n    return _ssim(img1, img2, window, window_size, channel, size_average)'"
person_transfer/models/PATN.py,31,"b""import numpy as np\nimport torch\nimport os\nfrom collections import OrderedDict\nimport itertools\nimport util.util as util\nfrom util.image_pool import ImagePool\nfrom .base_model import BaseModel\nfrom . import networks\n# losses\nfrom losses.L1_plus_perceptualLoss import L1_plus_perceptualLoss\nfrom losses.ssim import SSIM\n\nimport sys\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torch.nn as nn\n\nclass TransferModel(BaseModel):\n    def name(self):\n        return 'TransferModel'\n\n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n\n        nb = opt.batchSize\n        size = opt.fineSize\n\n        input_nc = [opt.P_input_nc, opt.BP_input_nc+opt.BP_input_nc]\n        self.netG1 = networks.define_G(input_nc, opt.P_input_nc,\n                                        opt.ngf, opt.which_model_netG, opt.norm, not opt.no_dropout, opt.init_type, self.gpu_ids,\n                                        n_downsampling=opt.G_n_downsampling)\n        self.netG2 = networks.define_G(input_nc, opt.P_input_nc,\n                                        opt.ngf, 'SelectionGAN', opt.norm, not opt.no_dropout, opt.init_type, self.gpu_ids,\n                                        n_downsampling=opt.G_n_downsampling)\n\n        if self.isTrain:\n            use_sigmoid = opt.no_lsgan\n            if opt.with_D_PB:\n                self.netD_PB = networks.define_D(opt.P_input_nc+opt.BP_input_nc, opt.ndf,\n                                            opt.which_model_netD,\n                                            opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, self.gpu_ids,\n                                            not opt.no_dropout_D,\n                                            n_downsampling = opt.D_n_downsampling)\n\n            if opt.with_D_PP:\n                self.netD_PP = networks.define_D(opt.P_input_nc+opt.P_input_nc, opt.ndf,\n                                            opt.which_model_netD,\n                                            opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, self.gpu_ids,\n                                            not opt.no_dropout_D,\n                                            n_downsampling = opt.D_n_downsampling)\n\n        if not self.isTrain or opt.continue_train:\n            which_epoch = opt.which_epoch\n            self.load_network(self.netG1, 'netG1', which_epoch)\n            self.load_network(self.netG2, 'netG2', which_epoch)\n            if self.isTrain:\n                if opt.with_D_PB:\n                    self.load_network(self.netD_PB, 'netD_PB', which_epoch)\n                if opt.with_D_PP:\n                    self.load_network(self.netD_PP, 'netD_PP', which_epoch)\n\n\n        if self.isTrain:\n            self.old_lr = opt.lr\n            self.fake_PP_pool = ImagePool(opt.pool_size)\n            self.fake_PB_pool = ImagePool(opt.pool_size)\n            # define loss functions\n            self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)\n            self.criterionSSIM = SSIM()\n\n            if opt.L1_type == 'origin':\n                self.criterionL1 = torch.nn.L1Loss()\n            elif opt.L1_type == 'l1_plus_perL1':\n                self.criterionL1 = L1_plus_perceptualLoss(opt.lambda_A, opt.lambda_B, opt.perceptual_layers, self.gpu_ids, opt.percep_is_l1)\n            else:\n                raise Excption('Unsurportted type of L1!')\n            # initialize optimizers\n            self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG1.parameters(), self.netG2.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))\n            if opt.with_D_PB:\n                self.optimizer_D_PB = torch.optim.Adam(self.netD_PB.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n            if opt.with_D_PP:\n                self.optimizer_D_PP = torch.optim.Adam(self.netD_PP.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n\n            self.optimizers = []\n            self.schedulers = []\n            self.optimizers.append(self.optimizer_G)\n            if opt.with_D_PB:\n                self.optimizers.append(self.optimizer_D_PB)\n            if opt.with_D_PP:\n                self.optimizers.append(self.optimizer_D_PP)\n            for optimizer in self.optimizers:\n                self.schedulers.append(networks.get_scheduler(optimizer, opt))\n\n        print('---------- Networks initialized -------------')\n        networks.print_network(self.netG1)\n        networks.print_network(self.netG2)\n        if self.isTrain:\n            if opt.with_D_PB:\n                networks.print_network(self.netD_PB)\n            if opt.with_D_PP:\n                networks.print_network(self.netD_PP)\n        print('-----------------------------------------------')\n\n    def set_input(self, input):\n        self.input_P1, self.input_BP1 = input['P1'], input['BP1']\n        self.input_P2, self.input_BP2 = input['P2'], input['BP2']\n        self.image_paths = input['P1_path'][0] + '___' + input['P2_path'][0]\n\n\n        # print('this is pytorch 1.0 version')\n        if len(self.gpu_ids) > 0:\n            self.input_P1 = self.input_P1.cuda()\n            self.input_BP1 = self.input_BP1.cuda()\n            self.input_P2 = self.input_P2.cuda()\n            self.input_BP2 = self.input_BP2.cuda()\n\n    def forward(self):\n        G_input = [self.input_P1, torch.cat((self.input_BP1, self.input_BP2), 1)]\n        self.fake_p2_1, self.feature = self.netG1(G_input)\n        # print('fake_p2_1', self.fake_p2_1.size()) [32, 3, 128, 64]\n\n        G2_input = torch.cat((self.input_P1, self.input_BP1, self.input_BP2, self.fake_p2_1, self.feature), 1)\n        # print('G2_input', G2_input.size()) [32, 106, 128, 64]\n        self.fake_p2, self.A = self.netG2(G2_input)\n\n    def test(self):\n        with torch.no_grad():\n            G_input = [self.input_P1,\n                       torch.cat((self.input_BP1, self.input_BP2), 1)]\n            self.fake_p2_1, self.feature = self.netG1(G_input)\n\n\n            G2_input = torch.cat((self.input_P1, self.input_BP1, self.input_BP2, self.fake_p2_1, self.feature), 1)\n            self.fake_p2, self.A = self.netG2(G2_input)\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n\n    def backward_G(self):\n        if self.opt.with_D_PB:\n            pred_fake_PB = self.netD_PB(torch.cat((self.fake_p2, self.input_BP2), 1))\n            self.loss_G_GAN_PB = self.criterionGAN(pred_fake_PB, True)\n\n            pred_fake_PB = self.netD_PB(torch.cat((self.fake_p2_1, self.input_BP2), 1))\n            self.loss_G_GAN_PB = self.criterionGAN(pred_fake_PB, True) + self.loss_G_GAN_PB * 4\n\n        if self.opt.with_D_PP:\n            pred_fake_PP = self.netD_PP(torch.cat((self.fake_p2, self.input_P1), 1))\n            self.loss_G_GAN_PP = self.criterionGAN(pred_fake_PP, True)\n\n            pred_fake_PP = self.netD_PP(torch.cat((self.fake_p2, self.input_P1), 1))\n            self.loss_G_GAN_PP = self.criterionGAN(pred_fake_PP, True) + self.loss_G_GAN_PP * 4\n\n        # L1 loss\n        if self.opt.L1_type == 'l1_plus_perL1' :\n            loss1 = self.criterionL1(self.fake_p2, self.input_P2)\n            loss2 = self.criterionL1(self.fake_p2_1, self.input_P2)\n            self.loss_G_L1 = loss1[0] *2 + loss2[0]\n            self.loss_originL1 = loss1[1].item() * 2 + loss2[1].item() + \\\n                                 torch.mean(torch.div(torch.abs(self.fake_p2 - self.input_P2), self.A) + torch.log(self.A)) * 2 * self.opt.lambda_A + \\\n                                 torch.mean(torch.div(torch.abs(self.fake_p2_1 - self.input_P2), self.A) + torch.log(self.A)) * self.opt.lambda_A\n            self.loss_perceptual = loss1[2].item() * 2 + loss2[2].item()\n        # else:\n        #     self.loss_G_L1 = self.criterionL1(self.fake_p2, self.input_P2) * self.opt.lambda_A * 2 + \\\n        #                      torch.mean(torch.div(torch.abs(self.fake_p2 - self.input_P2), self.A) + torch.log(self.A)) * self.opt.lambda_A * 2 + \\\n        #                      self.criterionL1(self.fake_p2_1, self.input_P2) * self.opt.lambda_A + \\\n        #                      torch.mean(torch.div(torch.abs(self.fake_p2_1 - self.input_P2), self.A) + torch.log(\n        #                          self.A)) * self.opt.lambda_A\n\n        pair_L1loss = self.loss_originL1 + self.loss_perceptual\n\n        if self.opt.with_D_PB:\n            pair_GANloss = self.loss_G_GAN_PB * self.opt.lambda_GAN\n            if self.opt.with_D_PP:\n                pair_GANloss += self.loss_G_GAN_PP * self.opt.lambda_GAN\n                pair_GANloss = pair_GANloss / 2\n        else:\n            if self.opt.with_D_PP:\n                pair_GANloss = self.loss_G_GAN_PP * self.opt.lambda_GAN\n\n        tvloss = 1e-6 * (\n                torch.sum(torch.abs(self.fake_p2[:, :, :, :-1] - self.fake_p2[:, :, :, 1:])) +\n                torch.sum(torch.abs(self.fake_p2[:, :, :-1, :] - self.fake_p2[:, :, 1:, :]))) + \\\n                 1e-6 * (\n                torch.sum(torch.abs(self.fake_p2_1[:, :, :, :-1] - self.fake_p2_1[:, :, :, 1:])) +\n                torch.sum(torch.abs(self.fake_p2_1[:, :, :-1, :] - self.fake_p2_1[:, :, 1:, :])))\n\n        # ssimloss = -self.criterionSSIM(self.input_P2, self.fake_p2) * 10\n        # ssimloss = - (self.criterionSSIM(self.input_P2, self.fake_p2) * 20 + self.criterionSSIM(self.input_P2, self.fake_p2_1) * 10)\n        # ssimloss = - (self.criterionSSIM(self.input_P2, self.fake_p2) * 2 + self.criterionSSIM(self.input_P2, self.fake_p2_1) * 1) # market\n        ssimloss = -self.criterionSSIM(self.input_P2, self.fake_p2) * 20 # fashion\n\n        if self.opt.with_D_PB or self.opt.with_D_PP:\n            pair_loss = pair_L1loss + pair_GANloss + tvloss + ssimloss\n        else:\n            pair_loss = pair_L1loss + tvloss\n\n        pair_loss.backward()\n\n        self.pair_L1loss = pair_L1loss.item()\n        self.tvloss = tvloss.item()\n        self.ssimloss = ssimloss.item()\n\n        if self.opt.with_D_PB or self.opt.with_D_PP:\n            self.pair_GANloss = pair_GANloss.item()\n\n\n    def backward_D_basic(self, netD, real, fake):\n        # Real\n        pred_real = netD(real)\n        loss_D_real = self.criterionGAN(pred_real, True) * self.opt.lambda_GAN\n        # Fake\n        pred_fake = netD(fake.detach())\n        loss_D_fake = self.criterionGAN(pred_fake, False) * self.opt.lambda_GAN\n        # Combined loss\n        loss_D = (loss_D_real + loss_D_fake) * 0.5\n        # backward\n        loss_D.backward()\n        return loss_D\n\n    # D: take(P, B) as input\n    def backward_D_PB(self):\n        real_PB = torch.cat((self.input_P2, self.input_BP2), 1)\n        # fake_PB = self.fake_PB_pool.query(torch.cat((self.fake_p2, self.input_BP2), 1))\n        fake_PB = self.fake_PB_pool.query( torch.cat((self.fake_p2, self.input_BP2), 1).data )\n        loss_D_PB = self.backward_D_basic(self.netD_PB, real_PB, fake_PB)\n\n        fake_PB = self.fake_PB_pool.query( torch.cat((self.fake_p2_1, self.input_BP2), 1).data )\n        loss_D_PB = self.backward_D_basic(self.netD_PB, real_PB, fake_PB) + loss_D_PB * 4\n\n        self.loss_D_PB = loss_D_PB.item()\n\n    # D: take(P, P') as input\n    def backward_D_PP(self):\n        real_PP = torch.cat((self.input_P2, self.input_P1), 1)\n        # fake_PP = self.fake_PP_pool.query(torch.cat((self.fake_p2, self.input_P1), 1))\n        fake_PP = self.fake_PP_pool.query( torch.cat((self.fake_p2, self.input_P1), 1).data )\n        loss_D_PP = self.backward_D_basic(self.netD_PP, real_PP, fake_PP)\n\n        fake_PP = self.fake_PP_pool.query( torch.cat((self.fake_p2_1, self.input_P1), 1).data )\n        loss_D_PP = self.backward_D_basic(self.netD_PP, real_PP, fake_PP) + loss_D_PP * 4\n\n        self.loss_D_PP = loss_D_PP.item()\n\n\n    def optimize_parameters(self):\n        # forward\n        self.forward()\n\n        self.optimizer_G.zero_grad()\n        self.backward_G()\n        self.optimizer_G.step()\n\n        # D_P\n        if self.opt.with_D_PP:\n            for i in range(self.opt.DG_ratio):\n                self.optimizer_D_PP.zero_grad()\n                self.backward_D_PP()\n                self.optimizer_D_PP.step()\n\n        # D_BP\n        if self.opt.with_D_PB:\n            for i in range(self.opt.DG_ratio):\n                self.optimizer_D_PB.zero_grad()\n                self.backward_D_PB()\n                self.optimizer_D_PB.step()\n\n\n    def get_current_errors(self):\n        ret_errors = OrderedDict([ ('pair_L1loss', self.pair_L1loss)])\n        if self.opt.with_D_PP:\n            ret_errors['D_PP'] = self.loss_D_PP\n        if self.opt.with_D_PB:\n            ret_errors['D_PB'] = self.loss_D_PB\n        if self.opt.with_D_PB or self.opt.with_D_PP:\n            ret_errors['pair_GANloss'] = self.pair_GANloss\n\n        if self.opt.L1_type == 'l1_plus_perL1':\n            ret_errors['origin_L1'] = self.loss_originL1\n            ret_errors['perceptual'] = self.loss_perceptual\n\n        ret_errors['tv'] = self.tvloss\n        ret_errors['ssim'] = self.ssimloss\n        return ret_errors\n\n    def get_current_visuals(self):\n        height, width = self.input_P1.size(2), self.input_P1.size(3)\n        input_P1 = util.tensor2im(self.input_P1.data)\n        input_P2 = util.tensor2im(self.input_P2.data)\n\n        input_BP1 = util.draw_pose_from_map(self.input_BP1.data)[0]\n        input_BP2 = util.draw_pose_from_map(self.input_BP2.data)[0]\n\n        fake_p2 = util.tensor2im(self.fake_p2.data)\n\n        vis = np.zeros((height, width*5, 3)).astype(np.uint8) #h, w, c\n        vis[:, :width, :] = input_P1\n        vis[:, width:width*2, :] = input_BP1\n        vis[:, width*2:width*3, :] = input_P2\n        vis[:, width*3:width*4, :] = input_BP2\n        vis[:, width*4:, :] = fake_p2\n\n        ret_visuals = OrderedDict([('vis', vis)])\n\n        return ret_visuals\n\n    def save(self, label):\n        self.save_network(self.netG1,  'netG1', label, self.gpu_ids)\n        self.save_network(self.netG2,  'netG2', label, self.gpu_ids)\n        if self.opt.with_D_PB:\n            self.save_network(self.netD_PB,  'netD_PB', label, self.gpu_ids)\n        if self.opt.with_D_PP:\n            self.save_network(self.netD_PP, 'netD_PP', label, self.gpu_ids)\n\n"""
person_transfer/models/__init__.py,0,b''
person_transfer/models/base_model.py,5,"b""import os\nimport torch\nimport torch.nn as nn\n\n\nclass BaseModel(nn.Module):\n\n    def __init__(self):\n        super(BaseModel, self).__init__()\n\n    def name(self):\n        return 'BaseModel'\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.gpu_ids = opt.gpu_ids\n        self.isTrain = opt.isTrain\n        self.Tensor = torch.cuda.FloatTensor if self.gpu_ids else torch.Tensor\n        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n\n    def set_input(self, input):\n        self.input = input\n\n    def forward(self):\n        pass\n\n    # used in test time, no backprop\n    def test(self):\n        pass\n\n    def get_image_paths(self):\n        pass\n\n    def optimize_parameters(self):\n        pass\n\n    def get_current_visuals(self):\n        return self.input\n\n    def get_current_errors(self):\n        return {}\n\n    def save(self, label):\n        pass\n\n    # helper saving function that can be used by subclasses\n    def save_network(self, network, network_label, epoch_label, gpu_ids):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(self.save_dir, save_filename)\n        torch.save(network.cpu().state_dict(), save_path)\n        if len(gpu_ids) and torch.cuda.is_available():\n            network.cuda(gpu_ids[0])\n\n    # helper loading function that can be used by subclasses\n    def load_network(self, network, network_label, epoch_label):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(self.save_dir, save_filename)\n        network.load_state_dict(torch.load(save_path))\n\n    # update learning rate (called once every epoch)\n    def update_learning_rate(self):\n        for scheduler in self.schedulers:\n            scheduler.step()\n        lr = self.optimizers[0].param_groups[0]['lr']\n        print('learning rate = %.7f' % lr)\n"""
person_transfer/models/model_variants.py,11,"b""import torch.nn as nn\nimport functools\nimport torch\nimport functools\nimport torch.nn.functional as F\n\n\nclass PATBlock(nn.Module):\n    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias, cated_stream2=False):\n        super(PATBlock, self).__init__()\n        self.conv_block_stream1 = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias, cal_att=False)\n        self.conv_block_stream2 = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias, cal_att=True, cated_stream2=cated_stream2)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias, cated_stream2=False, cal_att=False):\n        conv_block = []\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\n        if cated_stream2:\n            conv_block += [nn.Conv2d(dim*2, dim*2, kernel_size=3, padding=p, bias=use_bias),\n                       norm_layer(dim*2),\n                       nn.ReLU(True)]\n        else:\n            conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n                           norm_layer(dim),\n                           nn.ReLU(True)]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\n        if cal_att:\n            if cated_stream2:\n                conv_block += [nn.Conv2d(dim*2, dim, kernel_size=3, padding=p, bias=use_bias)]\n            else:\n                conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias)]\n        else:\n            conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n                       norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x1, x2):\n        # change here\n        x1_out = self.conv_block_stream1(x1)\n        x2_out = self.conv_block_stream2(x2)\n        # att = F.sigmoid(x2_out)\n        att = torch.sigmoid(x2_out)\n\n        x1_out = x1_out * att\n        out = x1 + x1_out # residual connection\n\n        # stream2 receive feedback from stream1\n        x2_out = torch.cat((x2_out, out), 1)\n        return out, x2_out, x1_out\n\nclass PATNModel(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, gpu_ids=[], padding_type='reflect', n_downsampling=2):\n        assert(n_blocks >= 0 and type(input_nc) == list)\n        super(PATNModel, self).__init__()\n        self.input_nc_s1 = input_nc[0]\n        self.input_nc_s2 = input_nc[1]\n        self.output_nc = output_nc\n        self.ngf = ngf\n        self.gpu_ids = gpu_ids\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        # down_sample\n        model_stream1_down = [nn.ReflectionPad2d(3),\n                    nn.Conv2d(self.input_nc_s1, ngf, kernel_size=7, padding=0,\n                           bias=use_bias),\n                    norm_layer(ngf),\n                    nn.ReLU(True)]\n\n        model_stream2_down = [nn.ReflectionPad2d(3),\n                    nn.Conv2d(self.input_nc_s2, ngf, kernel_size=7, padding=0,\n                           bias=use_bias),\n                    norm_layer(ngf),\n                    nn.ReLU(True)]\n\n        # n_downsampling = 2\n        for i in range(n_downsampling):\n            mult = 2**i\n            model_stream1_down += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n                                stride=2, padding=1, bias=use_bias),\n                            norm_layer(ngf * mult * 2),\n                            nn.ReLU(True)]\n            model_stream2_down += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n                                stride=2, padding=1, bias=use_bias),\n                            norm_layer(ngf * mult * 2),\n                            nn.ReLU(True)]\n\n        # att_block in place of res_block\n        mult = 2**n_downsampling\n        cated_stream2 = [True for i in range(n_blocks)]\n        cated_stream2[0] = False\n        attBlock = nn.ModuleList()\n        for i in range(n_blocks):\n            attBlock.append(PATBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias, cated_stream2=cated_stream2[i]))\n\n        # up_sample\n        model_stream1_up = []\n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model_stream1_up += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1,\n                                         bias=use_bias),\n                            norm_layer(int(ngf * mult / 2)),\n                            nn.ReLU(True)]\n\n        model_stream1_up2 = []\n        model_stream1_up2 += [nn.ReflectionPad2d(3)]\n        model_stream1_up2 += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model_stream1_up2 += [nn.Tanh()]\n\n        # self.model = nn.Sequential(*model)\n        self.stream1_down = nn.Sequential(*model_stream1_down)\n        self.stream2_down = nn.Sequential(*model_stream2_down)\n        # self.att = nn.Sequential(*attBlock)\n        self.att = attBlock\n        self.stream1_up = nn.Sequential(*model_stream1_up)\n        self.stream1_up2 = nn.Sequential(*model_stream1_up2)\n\n    def forward(self, input): # x from stream 1 and stream 2\n        # here x should be a tuple\n        x1, x2 = input\n        # down_sample\n        x1 = self.stream1_down(x1)\n        x2 = self.stream2_down(x2)\n        # att_block\n        for model in self.att:\n            x1, x2, _ = model(x1, x2)\n\n        # up_sample\n        feature = self.stream1_up(x1)\n        x1 = self.stream1_up2(feature)\n        # print('feature', feature.size())\xe3\x80\x80[32, 64, 128, 64]\n        # print('x1', x1.size()) [32, 3, 128, 64]\n        return x1, feature\n\nclass SelectionGANModel(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, gpu_ids=[], padding_type='reflect', n_downsampling=2):\n        assert(n_blocks >= 0 and type(input_nc) == list)\n        super(SelectionGANModel, self).__init__()\n        self.input_nc_s1 = input_nc[0]\n        self.input_nc_s2 = input_nc[1]\n        self.output_nc = output_nc\n        self.ngf = ngf\n        self.gpu_ids = gpu_ids\n\n        self.pool1 = nn.AvgPool2d(kernel_size=(1, 1))\n        self.pool2 = nn.AvgPool2d(kernel_size=(4, 4))\n        self.pool3 = nn.AvgPool2d(kernel_size=(9, 9))\n\n        self.conv106 = nn.Conv2d(106*4, 106, kernel_size=3, stride=1, padding=1, bias=nn.InstanceNorm2d)\n        self.model_attention = nn.Conv2d(106, 10, kernel_size=1, stride=1, padding=0)\n        self.model_image = nn.Conv2d(106, 30, kernel_size=3, stride=1, padding=1)\n\n        self.tanh = torch.nn.Tanh()\n        self.convolution_for_attention = torch.nn.Conv2d(10, 1, 1, stride=1, padding=0)\n    def forward(self, input): # x from stream 1 and stream 2\n        # input: [32, 106, 128, 64]\n\n        pool_feature1 = self.pool1(input)\n        pool_feature2 = self.pool2(input)\n        pool_feature3 = self.pool3(input)\n\n        b, c, h, w = input.size()\n\n        pool_feature1_up = F.upsample(input=pool_feature1, size=(h, w), mode='bilinear', align_corners=True)\n        pool_feature2_up = F.upsample(input=pool_feature2, size=(h, w), mode='bilinear', align_corners=True)\n        pool_feature3_up = F.upsample(input=pool_feature3, size=(h, w), mode='bilinear', align_corners=True)\n\n        f1 = input * pool_feature1_up\n        f2 = input * pool_feature2_up\n        f3 = input * pool_feature3_up\n\n        feature_image_combine = torch.cat((f1, f2, f3, input), 1) # feature_image_combine: 106*4\n        feature_image_combine = self.conv106(feature_image_combine) # feature_image_combine: 106\n\n        attention = self.model_attention(feature_image_combine) # attention: 10\n        image = self.model_image(feature_image_combine) # image: 30\n\n        softmax_ = torch.nn.Softmax(dim=1)\n        attention = softmax_(attention)\n        attention1_ = attention[:, 0:1, :, :]\n        attention2_ = attention[:, 1:2, :, :]\n        attention3_ = attention[:, 2:3, :, :]\n        attention4_ = attention[:, 3:4, :, :]\n        attention5_ = attention[:, 4:5, :, :]\n        attention6_ = attention[:, 5:6, :, :]\n        attention7_ = attention[:, 6:7, :, :]\n        attention8_ = attention[:, 7:8, :, :]\n        attention9_ = attention[:, 8:9, :, :]\n        attention10_ = attention[:, 9:10, :, :]\n\n        attention1 = attention1_.repeat(1, 3, 1, 1)\n        attention2 = attention2_.repeat(1, 3, 1, 1)\n        attention3 = attention3_.repeat(1, 3, 1, 1)\n        attention4 = attention4_.repeat(1, 3, 1, 1)\n        attention5 = attention5_.repeat(1, 3, 1, 1)\n        attention6 = attention6_.repeat(1, 3, 1, 1)\n        attention7 = attention7_.repeat(1, 3, 1, 1)\n        attention8 = attention8_.repeat(1, 3, 1, 1)\n        attention9 = attention9_.repeat(1, 3, 1, 1)\n        attention10 = attention10_.repeat(1, 3, 1, 1)\n\n        image = self.tanh(image)\n        image1 = image[:, 0:3, :, :]\n        image2 = image[:, 3:6, :, :]\n        image3 = image[:, 6:9, :, :]\n        image4 = image[:, 9:12, :, :]\n        image5 = image[:, 12:15, :, :]\n        image6 = image[:, 15:18, :, :]\n        image7 = image[:, 18:21, :, :]\n        image8 = image[:, 21:24, :, :]\n        image9 = image[:, 24:27, :, :]\n        image10 = image[:, 27:30, :, :]\n\n        output1 = image1 * attention1\n        output2 = image2 * attention2\n        output3 = image3 * attention3\n        output4 = image4 * attention4\n        output5 = image5 * attention5\n        output6 = image6 * attention6\n        output7 = image7 * attention7\n        output8 = image8 * attention8\n        output9 = image9 * attention9\n        output10 = image10 * attention10\n        output10 = image10 * attention10\n\n        final = output1 + output2 + output3 + output4 + output5 + output6 + output7 + output8 + output9 + output10\n        # print('final', final.size()) [32, 3, 128, 64]\n\n        sigmoid_ = torch.nn.Sigmoid()\n        uncertainty = self.convolution_for_attention(attention)\n\n        uncertainty = sigmoid_(uncertainty)\n        uncertainty_map = uncertainty.repeat(1, 3, 1, 1)\n\n        return final, uncertainty_map\n\n\nclass PATNetwork(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, gpu_ids=[], padding_type='reflect', n_downsampling=2):\n        super(PATNetwork, self).__init__()\n        assert type(input_nc) == list and len(input_nc) == 2, 'The AttModule take input_nc in format of list only!!'\n        self.gpu_ids = gpu_ids\n        self.model = PATNModel(input_nc, output_nc, ngf, norm_layer, use_dropout, n_blocks, gpu_ids, padding_type, n_downsampling=n_downsampling)\n\n    def forward(self, input):\n        if self.gpu_ids and isinstance(input[0].data, torch.cuda.FloatTensor):\n            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n        else:\n            return self.model(input)\n\nclass SelectionGANNetwork(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, gpu_ids=[], padding_type='reflect', n_downsampling=2):\n        super(SelectionGANNetwork, self).__init__()\n        # assert type(input_nc) == list and len(input_nc) == 2, 'The AttModule take input_nc in format of list only!!'\n        self.gpu_ids = gpu_ids\n        self.model = SelectionGANModel(input_nc, output_nc, ngf, norm_layer, use_dropout, n_blocks, gpu_ids, padding_type, n_downsampling=n_downsampling)\n\n    def forward(self, input):\n        if self.gpu_ids and isinstance(input[0].data, torch.cuda.FloatTensor):\n            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n        else:\n            return self.model(input)\n\n\n\n\n\n\n"""
person_transfer/models/models.py,0,"b'\ndef create_model(opt):\n    model = None\n    print(opt.model)\n\n    if opt.model == \'PATN\':\n        assert opt.dataset_mode == \'keypoint\'\n        from .PATN import TransferModel\n        model = TransferModel()\n\n    else:\n        raise ValueError(""Model [%s] not recognized."" % opt.model)\n    model.initialize(opt)\n    print(""model [%s] was created"" % (model.name()))\n    return model\n'"
person_transfer/models/networks.py,8,"b""import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport functools\nfrom torch.optim import lr_scheduler\nimport numpy as np\nimport torch.nn.functional as F\n\nimport sys\nfrom models.model_variants import PATNetwork, SelectionGANNetwork\n\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('Linear') != -1:\n        init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm2d') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef weights_init_xavier(m):\n    classname = m.__class__.__name__\n    # print(classname)\n    if classname.find('Conv') != -1:\n        init.xavier_normal(m.weight.data, gain=0.02)\n    elif classname.find('Linear') != -1:\n        init.xavier_normal(m.weight.data, gain=0.02)\n    elif classname.find('BatchNorm2d') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    # print(classname)\n    if classname.find('Conv') != -1:\n        init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n    elif classname.find('Linear') != -1:\n        init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n    elif classname.find('BatchNorm2d') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef weights_init_orthogonal(m):\n    classname = m.__class__.__name__\n    print(classname)\n    if classname.find('Conv') != -1:\n        init.orthogonal(m.weight.data, gain=1)\n    elif classname.find('Linear') != -1:\n        init.orthogonal(m.weight.data, gain=1)\n    elif classname.find('BatchNorm2d') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef init_weights(net, init_type='normal'):\n    print('initialization method [%s]' % init_type)\n    if init_type == 'normal':\n        net.apply(weights_init_normal)\n    elif init_type == 'xavier':\n        net.apply(weights_init_xavier)\n    elif init_type == 'kaiming':\n        net.apply(weights_init_kaiming)\n    elif init_type == 'orthogonal':\n        net.apply(weights_init_orthogonal)\n    else:\n        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n\n\ndef get_norm_layer(norm_type='instance'):\n    if norm_type == 'batch':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n    elif norm_type == 'batch_sync':\n        norm_layer = BatchNorm2d\n    elif norm_type == 'instance':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\n    elif norm_type == 'none':\n        norm_layer = None\n    else:\n        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n    return norm_layer\n\n\ndef get_scheduler(optimizer, opt):\n    if opt.lr_policy == 'lambda':\n        def lambda_rule(epoch):\n            lr_l = 1.0 - max(0, epoch + 1 + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\n            return lr_l\n\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n    elif opt.lr_policy == 'step':\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n    elif opt.lr_policy == 'plateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n    else:\n        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n    return scheduler\n\n\ndef define_G(input_nc, output_nc, ngf, which_model_netG, norm='batch', use_dropout=False, init_type='normal',\n             gpu_ids=[], n_downsampling=2):\n    netG = None\n    use_gpu = len(gpu_ids) > 0\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if use_gpu:\n        assert (torch.cuda.is_available())\n\n    if which_model_netG == 'PATN':\n        assert len(input_nc) == 2\n        netG = PATNetwork(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout,\n                                           n_blocks=9, gpu_ids=gpu_ids, n_downsampling=n_downsampling)\n    elif which_model_netG == 'SelectionGAN':\n        netG = SelectionGANNetwork(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout,\n                                           n_blocks=9, gpu_ids=gpu_ids, n_downsampling=n_downsampling)\n    else:\n        raise NotImplementedError('Generator model name [%s] is not recognized' % which_model_netG)\n    if len(gpu_ids) > 0:\n        netG.cuda(gpu_ids[0])\n    init_weights(netG, init_type=init_type)\n    return netG\n\n\ndef define_D(input_nc, ndf, which_model_netD,\n             n_layers_D=3, norm='batch', use_sigmoid=False, init_type='normal', gpu_ids=[], use_dropout=False,\n             n_downsampling=2):\n    netD = None\n    use_gpu = len(gpu_ids) > 0\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if use_gpu:\n        assert (torch.cuda.is_available())\n\n    if which_model_netD == 'resnet':\n        netD = ResnetDiscriminator(input_nc, ndf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=n_layers_D,\n                                   gpu_ids=[], padding_type='reflect', use_sigmoid=use_sigmoid,\n                                   n_downsampling=n_downsampling)\n    else:\n        raise NotImplementedError('Discriminator model name [%s] is not recognized' %\n                                  which_model_netD)\n    if use_gpu:\n        netD.cuda(gpu_ids[0])\n\n    return netD\n\n\ndef print_network(net):\n    num_params = 0\n    for param in net.parameters():\n        num_params += param.numel()\n    print(net)\n    print('Total number of parameters: %d' % num_params)\n\n\n##############################################################################\n# Classes\n##############################################################################    \n\n# Defines the GAN loss which uses either LSGAN or the regular GAN.\n# When LSGAN is used, it is basically same as MSELoss,\n# but it abstracts away the need to create the target label tensor\n# that has the same size as the input\nclass GANLoss(nn.Module):\n    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n                 tensor=torch.FloatTensor):\n        super(GANLoss, self).__init__()\n        self.real_label = target_real_label\n        self.fake_label = target_fake_label\n        self.real_label_var = None\n        self.fake_label_var = None\n        self.Tensor = tensor\n        if use_lsgan:\n            self.loss = nn.MSELoss()\n        else:\n            self.loss = nn.BCELoss()\n\n    def get_target_tensor(self, input, target_is_real):\n        target_tensor = None\n        if target_is_real:\n            create_label = ((self.real_label_var is None) or\n                            (self.real_label_var.numel() != input.numel()))\n            if create_label:\n                self.real_label_var = self.Tensor(input.size()).fill_(self.real_label)\n                # self.real_label_var = Variable(real_tensor, requires_grad=False)\n            target_tensor = self.real_label_var\n        else:\n            create_label = ((self.fake_label_var is None) or\n                            (self.fake_label_var.numel() != input.numel()))\n            if create_label:\n                self.fake_label_var = self.Tensor(input.size()).fill_(self.fake_label)\n                # self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n            target_tensor = self.fake_label_var\n        return target_tensor\n\n    def __call__(self, input, target_is_real):\n        target_tensor = self.get_target_tensor(input, target_is_real)\n        return self.loss(input, target_tensor)\n\n# Define a resnet block\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        conv_block = []\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n                       norm_layer(dim),\n                       nn.ReLU(True)]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n                       norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n        return out\n\nclass ResnetDiscriminator(nn.Module):\n    def __init__(self, input_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, gpu_ids=[],\n                 padding_type='reflect', use_sigmoid=False, n_downsampling=2):\n        assert (n_blocks >= 0)\n        super(ResnetDiscriminator, self).__init__()\n        self.input_nc = input_nc\n        self.ngf = ngf\n        self.gpu_ids = gpu_ids\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0,\n                           bias=use_bias),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        # n_downsampling = 2\n        if n_downsampling <= 2:\n            for i in range(n_downsampling):\n                mult = 2 ** i\n                model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n                                    stride=2, padding=1, bias=use_bias),\n                          norm_layer(ngf * mult * 2),\n                          nn.ReLU(True)]\n        elif n_downsampling == 3:\n            mult = 2 ** 0\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n                                stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n            mult = 2 ** 1\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n                                stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n            mult = 2 ** 2\n            model += [nn.Conv2d(ngf * mult, ngf * mult, kernel_size=3,\n                                stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult),\n                      nn.ReLU(True)]\n\n        if n_downsampling <= 2:\n            mult = 2 ** n_downsampling\n        else:\n            mult = 4\n        for i in range(n_blocks):\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout,\n                                  use_bias=use_bias)]\n\n        if use_sigmoid:\n            model += [nn.Sigmoid()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        if self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor):\n            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n        else:\n            return self.model(input)\n\n"""
person_transfer/models/test_model.py,1,"b""from torch.autograd import Variable\nfrom collections import OrderedDict\nimport util.util as util\nfrom .base_model import BaseModel\nfrom . import networks\n\n\nclass TestModel(BaseModel):\n    def name(self):\n        return 'TestModel'\n\n    def initialize(self, opt):\n        assert(not opt.isTrain)\n        BaseModel.initialize(self, opt)\n        self.input_A = self.Tensor(opt.batchSize, opt.input_nc, opt.fineSize, opt.fineSize)\n\n        self.netG = networks.define_G(opt.input_nc, opt.output_nc,\n                                      opt.ngf, opt.which_model_netG,\n                                      opt.norm, not opt.no_dropout,\n                                      opt.init_type,\n                                      self.gpu_ids)\n        which_epoch = opt.which_epoch\n        self.load_network(self.netG, 'G', which_epoch)\n\n        print('---------- Networks initialized -------------')\n        networks.print_network(self.netG)\n        print('-----------------------------------------------')\n\n    def set_input(self, input):\n        # we need to use single_dataset mode\n        input_A = input['A']\n        self.input_A.resize_(input_A.size()).copy_(input_A)\n        self.image_paths = input['A_paths']\n\n    def test(self):\n        self.real_A = Variable(self.input_A)\n        self.fake_B = self.netG(self.real_A)\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n    def get_current_visuals(self):\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n        return OrderedDict([('real_A', real_A), ('fake_B', fake_B)])\n"""
person_transfer/options/__init__.py,0,b''
person_transfer/options/base_options.py,1,"b'import argparse\nimport os\nfrom util import util\nimport torch\n\n\nclass BaseOptions():\n    def __init__(self):\n        self.parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n        self.initialized = False\n\n    def initialize(self):\n\n        self.parser.add_argument(\'--dataroot\', required=True, help=\'path to images (should have subfolders trainA, trainB, valA, valB, etc)\')\n        self.parser.add_argument(\'--batchSize\', type=int, default=1, help=\'input batch size\')\n        self.parser.add_argument(\'--loadSize\', type=int, default=286, help=\'scale images to this size\')\n        self.parser.add_argument(\'--fineSize\', type=int, default=256, help=\'then crop to this size\')\n        self.parser.add_argument(\'--input_nc\', type=int, default=3, help=\'# of input image channels\')\n        self.parser.add_argument(\'--output_nc\', type=int, default=3, help=\'# of output image channels\')\n        self.parser.add_argument(\'--ngf\', type=int, default=64, help=\'# of gen filters in first conv layer\')\n        self.parser.add_argument(\'--ndf\', type=int, default=64, help=\'# of discrim filters in first conv layer\')\n        self.parser.add_argument(\'--which_model_netD\', type=str, default=\'resnet\', help=\'selects model to use for netD\')\n        self.parser.add_argument(\'--which_model_netG\', type=str, default=\'PATN\', help=\'selects model to use for netG\')\n        self.parser.add_argument(\'--n_layers_D\', type=int, default=3, help=\'blocks used in D\')\n        self.parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU\')\n        self.parser.add_argument(\'--name\', type=str, default=\'experiment_name\', help=\'name of the experiment. It decides where to store samples and models\')\n        self.parser.add_argument(\'--dataset_mode\', type=str, default=\'unaligned\', help=\'chooses how datasets are loaded. [unaligned | aligned | single]\')\n        self.parser.add_argument(\'--model\', type=str, default=\'cycle_gan\',\n                                 help=\'chooses which model to use. cycle_gan, pix2pix, test\')\n        self.parser.add_argument(\'--which_direction\', type=str, default=\'AtoB\', help=\'AtoB or BtoA\')\n        self.parser.add_argument(\'--nThreads\', default=2, type=int, help=\'# threads for loading data\')\n        self.parser.add_argument(\'--checkpoints_dir\', type=str, default=\'./checkpoints\', help=\'models are saved here\')\n        self.parser.add_argument(\'--norm\', type=str, default=\'instance\', help=\'instance normalization or batch normalization\')\n        self.parser.add_argument(\'--serial_batches\', action=\'store_true\', help=\'if true, takes images in order to make batches, otherwise takes them randomly\')\n        self.parser.add_argument(\'--display_winsize\', type=int, default=256,  help=\'display window size\')\n        self.parser.add_argument(\'--display_id\', type=int, default=1, help=\'window id of the web display\')\n        self.parser.add_argument(\'--display_port\', type=int, default=8097, help=\'visdom port of the web display\')\n        self.parser.add_argument(\'--no_dropout\', action=\'store_true\', help=\'no dropout for the generator\')\n        self.parser.add_argument(\'--max_dataset_size\', type=int, default=float(""inf""), help=\'Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.\')\n        self.parser.add_argument(\'--resize_or_crop\', type=str, default=\'resize_and_crop\', help=\'scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop]\')\n        self.parser.add_argument(\'--no_flip\', action=\'store_true\', help=\'if specified, do not flip the images for data augmentation\')\n        self.parser.add_argument(\'--init_type\', type=str, default=\'normal\', help=\'network initialization [normal|xavier|kaiming|orthogonal]\')\n\n        self.parser.add_argument(\'--P_input_nc\', type=int, default=3, help=\'# of input image channels\')\n        self.parser.add_argument(\'--BP_input_nc\', type=int, default=1, help=\'# of input image channels\')\n        self.parser.add_argument(\'--padding_type\', type=str, default=\'reflect\', help=\'# of input image channels\')\n        self.parser.add_argument(\'--pairLst\', type=str, default=\'./keypoint_data/market-pairs-train.csv\', help=\'market pairs\')\n\n        self.parser.add_argument(\'--with_D_PP\', type=int, default=1, help=\'use D to judge P and P is pair or not\')\n        self.parser.add_argument(\'--with_D_PB\', type=int, default=1, help=\'use D to judge P and B is pair or not\')\n\n        self.parser.add_argument(\'--use_flip\', type=int, default=0, help=\'flip or not\')\n\n        # down-sampling times\n        self.parser.add_argument(\'--G_n_downsampling\', type=int, default=2, help=\'down-sampling blocks for generator\')\n        self.parser.add_argument(\'--D_n_downsampling\', type=int, default=2, help=\'down-sampling blocks for discriminator\')\n\n        self.initialized = True\n\n    def parse(self):\n        if not self.initialized:\n            self.initialize()\n        self.opt = self.parser.parse_args()\n        self.opt.isTrain = self.isTrain   # train or test\n\n        str_ids = self.opt.gpu_ids.split(\',\')\n        self.opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                self.opt.gpu_ids.append(id)\n\n        # set gpu ids\n        if len(self.opt.gpu_ids) > 0:\n            torch.cuda.set_device(self.opt.gpu_ids[0])\n\n        args = vars(self.opt)\n\n        print(\'------------ Options -------------\')\n        for k, v in sorted(args.items()):\n            print(\'%s: %s\' % (str(k), str(v)))\n        print(\'-------------- End ----------------\')\n\n        # save to the disk\n        expr_dir = os.path.join(self.opt.checkpoints_dir, self.opt.name)\n        util.mkdirs(expr_dir)\n        file_name = os.path.join(expr_dir, \'opt.txt\')\n        with open(file_name, \'wt\') as opt_file:\n            opt_file.write(\'------------ Options -------------\\n\')\n            for k, v in sorted(args.items()):\n                opt_file.write(\'%s: %s\\n\' % (str(k), str(v)))\n            opt_file.write(\'-------------- End ----------------\\n\')\n        return self.opt\n'"
person_transfer/options/test_options.py,0,"b'from .base_options import BaseOptions\n\n\nclass TestOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        self.parser.add_argument(\'--ntest\', type=int, default=float(""inf""), help=\'# of test examples.\')\n        self.parser.add_argument(\'--results_dir\', type=str, default=\'./results/\', help=\'saves results here.\')\n        self.parser.add_argument(\'--aspect_ratio\', type=float, default=1.0, help=\'aspect ratio of result images\')\n        self.parser.add_argument(\'--phase\', type=str, default=\'test\', help=\'train, val, test, etc\')\n        self.parser.add_argument(\'--which_epoch\', type=str, default=\'latest\', help=\'which epoch to load? set to latest to use latest cached model\')\n        self.parser.add_argument(\'--how_many\', type=int, default=200, help=\'how many test images to run\')\n\n        self.isTrain = False\n'"
person_transfer/options/train_options.py,0,"b""from .base_options import BaseOptions\n\n\nclass TrainOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        self.parser.add_argument('--display_freq', type=int, default=100, help='frequency of showing training results on screen')\n        self.parser.add_argument('--display_single_pane_ncols', type=int, default=0, help='if positive, display all images in a single visdom web panel with certain number of images per row.')\n        self.parser.add_argument('--update_html_freq', type=int, default=1000, help='frequency of saving training results to html')\n        self.parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n        self.parser.add_argument('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')\n        self.parser.add_argument('--save_epoch_freq', type=int, default=20, help='frequency of saving checkpoints at the end of epochs')\n        self.parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n        self.parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n        self.parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n        self.parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n        self.parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate')\n        self.parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero')\n        self.parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n        self.parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n        self.parser.add_argument('--no_lsgan', action='store_true', help='do *not* use least square GAN, if false, use vanilla GAN')\n        self.parser.add_argument('--lambda_A', type=float, default=10.0, help='weight for L1 loss')\n        self.parser.add_argument('--lambda_B', type=float, default=10.0, help='weight for perceptual L1 loss')\n        self.parser.add_argument('--lambda_GAN', type=float, default=5.0, help='weight of GAN loss')\n\n        self.parser.add_argument('--pool_size', type=int, default=50, help='the size of image buffer that stores previously generated images')\n        self.parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n        self.parser.add_argument('--lr_policy', type=str, default='lambda', help='learning rate policy: lambda|step|plateau')\n        self.parser.add_argument('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations')\n\n        self.parser.add_argument('--L1_type', type=str, default='origin', help='use which kind of L1 loss. (origin|l1_plus_perL1)')\n        self.parser.add_argument('--perceptual_layers', type=int, default=3, help='index of vgg layer for extracting perceptual features.')\n        self.parser.add_argument('--percep_is_l1', type=int, default=1, help='type of perceptual loss: l1 or l2')\n        self.parser.add_argument('--no_dropout_D', action='store_true', help='no dropout for the discriminator')\n        self.parser.add_argument('--DG_ratio', type=int, default=1, help='how many times for D training after training G once')\n\n        \n        \n\n        self.isTrain = True\n"""
person_transfer/ssd_score/__init__.py,0,b'import compute_ssd_score\n'
person_transfer/ssd_score/compute_ssd_score_fashion.py,0,"b'import numpy as np\n##Caffe from ssd branc\nimport caffe\ncaffe.set_device(0)\ncaffe.set_mode_gpu()\nfrom skimage import img_as_float\nfrom tqdm import tqdm\nfrom PIL import Image\n\nclass SSDScorer(object):\n    def __init__(self, model_def=\'deploy.prototxt\', model_weights=\'VGG_VOC0712_SSD_300x300_iter_120000.caffemodel\'):\n        self.net = caffe.Net(model_def, model_weights, caffe.TEST)\n\n        # input preprocessing: \'data\' is the name of the input blob == net.inputs[0]\n        self.transformer = caffe.io.Transformer({\'data\': self.net.blobs[\'data\'].data.shape})\n        self.transformer.set_transpose(\'data\', (2, 0, 1))\n        self.transformer.set_mean(\'data\', np.array([104,117,123])) # mean pixel\n        self.transformer.set_raw_scale(\'data\', 255)  # the reference model operates on images in [0,255]\n        self.transformer.set_channel_swap(\'data\', (2,1,0))  # the reference model has channels in BGR order\n\n        # set net to batch size of 1\n        self.img_side = 300\n        self.net.blobs[\'data\'].reshape(1,3,self.img_side,self.img_side)\n\n    def get_score(self, image, image_class):\n        image = img_as_float(image)\n        transformed_image = self.transformer.preprocess(\'data\', image)\n        self.net.blobs[\'data\'].data[...] = transformed_image\n\n        detections = self.net.forward()[\'detection_out\']\n\n        det_label = detections[0,0,:,1]\n        det_conf = detections[0,0,:,2]\n        top_indices = [i for i, label in enumerate(det_label) if label == image_class]\n        score = 0 if len(top_indices) == 0 else max(det_conf[top_indices])\n        return score\n\n    def get_score_image_set(self, imgs, image_class=15):\n        #image_class=15 Only persons\n        scores = []\n        for img in tqdm(imgs):\n            scores.append(self.get_score(img, image_class))\n        return np.mean(scores)\n\n\ndef addBounding(image, bound=40):\n    # print(image.shape)\n    h, w, c = image.shape\n    image_bound = np.ones((h, w+bound*2, c))*255\n    image_bound = image_bound.astype(np.uint8)\n    image_bound[:, bound:bound+w] = image\n\n    return image_bound\n\nif __name__ == ""__main__"":\n    from skimage.io import imread\n    import os\n    from argparse import ArgumentParser\n\n    split = lambda s: tuple(map(int, s.split(\',\')))\n    parser = ArgumentParser(description=""Computing ssd_score"")\n    parser.add_argument(""--image_size"", default=(256,176), type=split, help=\'Image size\')\n    parser.add_argument(""--input_dir"", default=\'../output/generated_images\', help=\'Folder with images\')\n    parser.add_argument(""--img_index"", default=4, type=int,  help=\'Index of image generated image \'\n                                                                  \'for results with multiple images\')\n    args = parser.parse_args()\n    print (args)\n\n    imgs = []\n    for name in os.listdir(args.input_dir):\n        img = imread(os.path.join(args.input_dir, name))\n        img = img[:, args.img_index * args.image_size[1]:(args.img_index + 1) * args.image_size[1]]\n        img = addBounding(img)\n        imgs.append(img)\n\n    sc = SSDScorer()\n    print (sc.get_score_image_set(imgs))\n'"
person_transfer/ssd_score/compute_ssd_score_market.py,0,"b'import numpy as np\n##Caffe from ssd branc\nimport caffe\ncaffe.set_device(0)\ncaffe.set_mode_gpu()\nfrom skimage import img_as_float\nfrom tqdm import tqdm\nfrom PIL import Image\n\nclass SSDScorer(object):\n    def __init__(self, model_def=\'deploy.prototxt\', model_weights=\'VGG_VOC0712_SSD_300x300_iter_120000.caffemodel\'):\n        self.net = caffe.Net(model_def, model_weights, caffe.TEST)\n\n        # input preprocessing: \'data\' is the name of the input blob == net.inputs[0]\n        self.transformer = caffe.io.Transformer({\'data\': self.net.blobs[\'data\'].data.shape})\n        self.transformer.set_transpose(\'data\', (2, 0, 1))\n        self.transformer.set_mean(\'data\', np.array([104,117,123])) # mean pixel\n        self.transformer.set_raw_scale(\'data\', 255)  # the reference model operates on images in [0,255]\n        self.transformer.set_channel_swap(\'data\', (2,1,0))  # the reference model has channels in BGR order\n\n        # set net to batch size of 1\n        self.img_side = 300\n        self.net.blobs[\'data\'].reshape(1,3,self.img_side,self.img_side)\n\n    def get_score(self, image, image_class):\n        image = img_as_float(image)\n        transformed_image = self.transformer.preprocess(\'data\', image)\n        self.net.blobs[\'data\'].data[...] = transformed_image\n\n        detections = self.net.forward()[\'detection_out\']\n\n        det_label = detections[0,0,:,1]\n        det_conf = detections[0,0,:,2]\n        top_indices = [i for i, label in enumerate(det_label) if label == image_class]\n        score = 0 if len(top_indices) == 0 else max(det_conf[top_indices])\n        return score\n\n    def get_score_image_set(self, imgs, image_class=15):\n        #image_class=15 Only persons\n        scores = []\n        for img in tqdm(imgs):\n            scores.append(self.get_score(img, image_class))\n        return np.mean(scores)\n\nif __name__ == ""__main__"":\n    from skimage.io import imread\n    import os\n    from argparse import ArgumentParser\n\n    split = lambda s: tuple(map(int, s.split(\',\')))\n    parser = ArgumentParser(description=""Computing ssd_score"")\n    # market\n    parser.add_argument(""--image_size"", default=(128,64), type=split, help=\'Image size\')\n    parser.add_argument(""--input_dir"", default=\'../output/generated_images\', help=\'Folder with images\')\n    parser.add_argument(""--img_index"", default=4, type=int,  help=\'Index of image generated image \'\n                                                                  \'for results with multiple images\')\n    args = parser.parse_args()\n    print (args)\n\n    imgs = []\n    for name in os.listdir(args.input_dir):\n        img = imread(os.path.join(args.input_dir, name))\n        img = img[:, args.img_index * args.image_size[1]:(args.img_index + 1) * args.image_size[1]]\n        imgs.append(img)\n\n    sc = SSDScorer()\n    print (sc.get_score_image_set(imgs))\n'"
person_transfer/tool/calPCKH_fashion.py,0,"b'import math\nimport pandas as pd\nimport numpy as np\nimport json\n\nMISSING_VALUE = -1\n\nPARTS_SEL = [0, 1, 14, 15, 16, 17]\n\ntarget_annotation = \'./fashion_data/fasion-resize-annotation-test.csv\'\npred_annotation = \'./results/fashion_PATN/pckh.csv\'\n\n\n\'\'\'\n  hz: head size\n  alpha: norm factor\n  px, py: predict coords\n  tx, ty: target coords\n\'\'\'\ndef isRight(px, py, tx, ty, hz, alpha):\n    if px == -1 or py == -1 or tx == -1 or ty == -1:\n        return 0\n\n    if abs(px - tx) < hz[0] * alpha and abs(py - ty) < hz[1] * alpha:\n        return 1\n    else:\n        return 0\n\n\ndef how_many_right_seq(px, py, tx, ty, hz, alpha):\n    nRight = 0\n    for i in range(len(px)):\n        nRight = nRight + isRight(px[i], py[i], tx[i], ty[i], hz, alpha)\n\n    return nRight\n\n\ndef ValidPoints(tx):\n    nValid = 0\n    for item in tx:\n        if item != -1:\n            nValid = nValid + 1\n    return nValid\n\n\ndef get_head_wh(x_coords, y_coords):\n    final_w, final_h = -1, -1\n    component_count = 0\n    save_componets = []\n    for component in PARTS_SEL:\n        if x_coords[component] == MISSING_VALUE or y_coords[component] == MISSING_VALUE:\n            continue\n        else:\n            component_count += 1\n            save_componets.append([x_coords[component], y_coords[component]])\n    if component_count >= 2:\n        x_cords = []\n        y_cords = []\n        for component in save_componets:\n            x_cords.append(component[0])\n            y_cords.append(component[1])\n        xmin = min(x_cords)\n        xmax = max(x_cords)\n        ymin = min(y_cords)\n        ymax = max(y_cords)\n        final_w = xmax - xmin\n        final_h = ymax - ymin\n    return final_w, final_h\n\n\ntAnno = pd.read_csv(target_annotation, sep=\':\')\npAnno = pd.read_csv(pred_annotation, sep=\':\')\n\npRows = pAnno.shape[0]\n\nnAll = 0\nnCorrect = 0\nalpha = 0.5\nfor i in range(pRows):\n    pValues = pAnno.iloc[i].values\n    pname = pValues[0]\n    pycords = json.loads(pValues[1])  # list of numbers\n    pxcords = json.loads(pValues[2])\n\n    if \'_vis\' in pname:\n        tname = pname[:-8]\n    else:\n        tname = pname[:-4]\n\n    if \'___\' in tname:\n        tname = tname.split(\'___\')[1]\n    else:\n        tname = tname.split(\'jpg_\')[1]\n\n    print(tname)\n    tValues = tAnno.query(\'name == ""%s""\' % (tname)).values[0]\n    tycords = json.loads(tValues[1])  # list of numbers\n    txcords = json.loads(tValues[2])\n\n\n    xBox, yBox = get_head_wh(txcords, tycords)\n    if xBox == -1 or yBox == -1:\n        continue\n\n    head_size = (xBox, yBox)\n    nAll = nAll + ValidPoints(tycords)\n    nCorrect = nCorrect + how_many_right_seq(pxcords, pycords, txcords, tycords, head_size, alpha)\n\nprint(\'%d/%d %f\' % (nCorrect, nAll, nCorrect * 1.0 / nAll))\n'"
person_transfer/tool/calPCKH_market.py,0,"b'import math\nimport pandas as pd\nimport numpy as np\nimport json\n\nMISSING_VALUE = -1\n\nPARTS_SEL = [0, 1, 14, 15, 16, 17]\n\n# fix the PATH\ntarget_annotation = \'./market_data/market-annotation-test.csv\'\npred_annotation = \'/results/market_PATN/pckh.csv\'\n\n\n\'\'\'\n  hz: head size\n  alpha: norm factor\n  px, py: predict coords\n  tx, ty: target coords\n\'\'\'\ndef isRight(px, py, tx, ty, hz, alpha):\n\tif px == -1 or py == -1 or tx == -1 or ty == -1:\n\t\treturn 0\n\n\tif abs(px-tx) < hz[0]*alpha and abs(py-ty) < hz[1]*alpha:\n\t\treturn 1\n\telse:\n\t\treturn 0\n\ndef how_many_right_seq(px, py, tx, ty, hz, alpha):\n\tnRight = 0\n\tfor i in range(len(px)):\n\t\tnRight = nRight + isRight(px[i], py[i], tx[i], ty[i], hz, alpha)\n\n\treturn nRight\n\ndef ValidPoints(tx):\n\tnValid = 0\n\tfor item in tx:\n\t\tif item != -1:\n\t\t\tnValid = nValid + 1\n\treturn nValid\n\ndef get_head_wh(x_coords, y_coords):\n\tfinal_w, final_h = -1, -1\n\tcomponent_count = 0\n\tsave_componets = []\n\tfor component in PARTS_SEL:\n\t\tif x_coords[component] == MISSING_VALUE or y_coords[component] == MISSING_VALUE:\n\t\t\tcontinue\n\t\telse:\n\t\t\tcomponent_count += 1\n\t\t\tsave_componets.append([x_coords[component], y_coords[component]])\n\tif component_count >= 2:\n\t\tx_cords = []\n\t\ty_cords = []\n\t\tfor component in save_componets:\n\t\t\tx_cords.append(component[0])\n\t\t\ty_cords.append(component[1])\n\t\txmin = min(x_cords)\n\t\txmax = max(x_cords)\n\t\tymin = min(y_cords)\n\t\tymax = max(y_cords)\n\t\tfinal_w = xmax - xmin\n\t\tfinal_h = ymax - ymin\n\treturn final_w, final_h\n\n\n\n\n\ntAnno = pd.read_csv(target_annotation, sep=\':\')\npAnno = pd.read_csv(pred_annotation, sep=\':\')\n\npRows = pAnno.shape[0]\n\nnAll = 0\nnCorrect = 0\nalpha = 0.5\nfor i in range(pRows):\n\tpValues = pAnno.iloc[i].values\n\tpname = pValues[0]\n\tpycords = json.loads(pValues[1]) #list of numbers\n\tpxcords = json.loads(pValues[2])\n\n\tif \'_vis\' in pname:\n\t\ttname = pname[:-8]\n\telse:\n\t\ttname = pname[:-4]\n\n\tif \'___\' in tname:\n\t\ttname = tname.split(\'___\')[1]\n\telse:\n\t\ttname = tname.split(\'jpg_\')[1]\n\n\tprint(tname)\n\ttValues = tAnno.query(\'name == ""%s""\' %(tname)).values[0]\n\ttycords = json.loads(tValues[1]) #list of numbers\n\ttxcords = json.loads(tValues[2])\n\n\txBox, yBox = get_head_wh(txcords, tycords)\n\tif xBox == -1 or yBox == -1:\n\t\tcontinue\n\n\thead_size = (xBox, yBox)\n\tnAll = nAll + ValidPoints(tycords)\n\tnCorrect = nCorrect + how_many_right_seq(pxcords, pycords, txcords, tycords, head_size, alpha)\n\n\nprint(\'%d/%d %f\' %(nCorrect, nAll, nCorrect*1.0/nAll))\n\n\n\n\n\n\n\n\n'"
person_transfer/tool/cmd.py,0,"b'import argparse\n\ndef args():\n    """"""\n        Define args that is used in project\n    """"""\n    parser = argparse.ArgumentParser(description=""Pose guided image generation usign deformable skip layers"")\n    parser.add_argument(""--output_dir"", default=\'output/displayed_samples\', help=""Directory with generated sample images"")\n    parser.add_argument(""--batch_size"", default=4, type=int, help=\'Size of the batch\')\n    parser.add_argument(""--training_ratio"", default=1, type=int,\n                        help=""The training ratio is the number of discriminator updates per generator update."")\n\n    parser.add_argument(""--l1_penalty_weight"", default=100, type=float, help=\'Weight of l1 loss\')\n    parser.add_argument(\'--gan_penalty_weight\', default=1, type=float, help=\'Weight of GAN loss\')\n    parser.add_argument(\'--tv_penalty_weight\', default=0, type=float, help=\'Weight of total variation loss\')\n    parser.add_argument(\'--lstruct_penalty_weight\', default=0, type=float, help=""Weight of lstruct"")\n    \n    parser.add_argument(""--number_of_epochs"", default=500, type=int, help=""Number of training epochs"")\n\n    parser.add_argument(""--content_loss_layer"", default=\'none\', help=\'Name of content layer (vgg19)\'\n                                                                     \' e.g. block4_conv1 or none\')\n\n    parser.add_argument(""--checkpoints_dir"", default=""output/checkpoints"", help=""Folder with checkpoints"")\n    parser.add_argument(""--checkpoint_ratio"", default=30, type=int, help=""Number of epochs between consecutive checkpoints"")\n    parser.add_argument(""--generator_checkpoint"", default=None, help=""Previosly saved model of generator"")\n    parser.add_argument(""--discriminator_checkpoint"", default=None, help=""Previosly saved model of discriminator"")\n    parser.add_argument(""--nn_loss_area_size"", default=1, type=int, help=""Use nearest neighbour loss"")\n    parser.add_argument(""--use_validation"", default=1, type=int, help=""Use validation"")\n\n    parser.add_argument(\'--dataset\', default=\'market\', choices=[\'market\', \'fasion\', \'fasion128\', \'fasion128128\'],\n                        help=\'Market or fasion\')\n\n\n    parser.add_argument(""--display_ratio"", default=1, type=int,  help=\'Number of epochs between ploting\')\n    parser.add_argument(""--start_epoch"", default=0, type=int, help=\'Start epoch for starting from checkpoint\')\n    parser.add_argument(""--pose_estimator"", default=\'pose_estimator.h5\',\n                            help=\'Pretrained model for cao pose estimator\')\n\n    parser.add_argument(""--images_for_test"", default=12000, type=int, help=""Number of images for testing"")\n\n    parser.add_argument(""--use_input_pose"", default=True, type=int, help=\'Feed to generator input pose\')\n    parser.add_argument(""--warp_skip"", default=\'stn\', choices=[\'none\', \'full\', \'mask\', \'stn\'],\n                        help=""Type of warping skip layers to use."")\n    parser.add_argument(""--warp_agg"", default=\'max\', choices=[\'max\', \'avg\'],\n                        help=""Type of aggregation."")\n\n    parser.add_argument(""--disc_type"", default=\'call\', choices=[\'call\', \'sim\', \'warp\'],\n                        help=""Type of discriminator call - concat all, sim - siamease, sharewarp - warp."")\n\n\n    parser.add_argument(""--generated_images_dir"", default=\'output/generated_images\',\n                        help=\'Folder with generated images from training dataset\')\n\n    parser.add_argument(\'--load_generated_images\', default=0, type=int,\n                        help=\'Load images from generated_images_dir or generate\')\n\n    parser.add_argument(\'--use_dropout_test\', default=0, type=int,\n                        help=\'To use dropout when generate images\')\n\n    args = parser.parse_args()\n\n    args.images_dir_train = \'data/\' + args.dataset + \'-dataset/train\'\n    args.images_dir_test = \'data/\' + args.dataset + \'-dataset/test\'\n\n    args.annotations_file_train = \'data/\' + args.dataset + \'-annotation-train.csv\'\n    args.annotations_file_test = \'data/\' + args.dataset + \'-annotation-test.csv\'\n\n    args.pairs_file_train = \'data/\' + args.dataset + \'-pairs-train.csv\'\n    args.pairs_file_test = \'data/\' + args.dataset + \'-pairs-test.csv\'\n\n    if args.dataset == \'fasion\':\n        args.image_size = (256, 256)\n    elif args.dataset == \'fasion128128\':\n        args.image_size = (128, 128)\n    else:\n        args.image_size = (128, 64)\n    \n    args.tmp_pose_dir = \'tmp/\' + args.dataset + \'/\'\n\n    del args.dataset\n\n    return args\n'"
person_transfer/tool/compute_coordinates.py,0,"b'import pose_utils\nimport os\nimport numpy as np\n\nfrom keras.models import load_model\nimport skimage.transform as st\nimport pandas as pd\nfrom tqdm import tqdm\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom scipy.ndimage import gaussian_filter\n\nfrom cmd import args\n\n\nargs = args()\n\nmodel = load_model(args.pose_estimator)\n\n\nmapIdx = [[31,32], [39,40], [33,34], [35,36], [41,42], [43,44], [19,20], [21,22],\n          [23,24], [25,26], [27,28], [29,30], [47,48], [49,50], [53,54], [51,52],\n          [55,56], [37,38], [45,46]]\n\nlimbSeq = [[2,3], [2,6], [3,4], [4,5], [6,7], [7,8], [2,9], [9,10],\n           [10,11], [2,12], [12,13], [13,14], [2,1], [1,15], [15,17],\n           [1,16], [16,18], [3,17], [6,18]]\n\n\ndef compute_cordinates(heatmap_avg, paf_avg, th1=0.1, th2=0.05):\n    all_peaks = []\n    peak_counter = 0\n\n    for part in range(18):\n        map_ori = heatmap_avg[:,:,part]\n        map = gaussian_filter(map_ori, sigma=3)\n\n        map_left = np.zeros(map.shape)\n        map_left[1:,:] = map[:-1,:]\n        map_right = np.zeros(map.shape)\n        map_right[:-1,:] = map[1:,:]\n        map_up = np.zeros(map.shape)\n        map_up[:,1:] = map[:,:-1]\n        map_down = np.zeros(map.shape)\n        map_down[:,:-1] = map[:,1:]\n\n        peaks_binary = np.logical_and.reduce((map>=map_left, map>=map_right, map>=map_up, map>=map_down, map > th1))\n        peaks = zip(np.nonzero(peaks_binary)[1], np.nonzero(peaks_binary)[0]) # note reverse\n\n        peaks_with_score = [x + (map_ori[x[1],x[0]],) for x in peaks]\n        id = range(peak_counter, peak_counter + len(peaks))\n        peaks_with_score_and_id = [peaks_with_score[i] + (id[i],) for i in range(len(id))]\n\n        all_peaks.append(peaks_with_score_and_id)\n        peak_counter += len(peaks)\n\n    connection_all = []\n    special_k = []\n    mid_num = 10\n\n    for k in range(len(mapIdx)):\n        score_mid = paf_avg[:,:,[x-19 for x in mapIdx[k]]]\n        candA = all_peaks[limbSeq[k][0]-1]\n        candB = all_peaks[limbSeq[k][1]-1]\n        nA = len(candA)\n        nB = len(candB)\n        indexA, indexB = limbSeq[k]\n        if(nA != 0 and nB != 0):\n            connection_candidate = []\n            for i in range(nA):\n                for j in range(nB):\n                    vec = np.subtract(candB[j][:2], candA[i][:2])\n                    norm = np.sqrt(vec[0]*vec[0] + vec[1]*vec[1])\n                    vec = np.divide(vec, norm)\n\n                    startend = zip(np.linspace(candA[i][0], candB[j][0], num=mid_num),\n                                   np.linspace(candA[i][1], candB[j][1], num=mid_num))\n\n                    vec_x = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 0]\n                                      for I in range(len(startend))])\n                    vec_y = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 1]\n                                      for I in range(len(startend))])\n\n                    score_midpts = np.multiply(vec_x, vec[0]) + np.multiply(vec_y, vec[1])\n                    score_with_dist_prior = sum(score_midpts)/len(score_midpts) + min(0.5*oriImg.shape[0]/norm-1, 0)\n                    criterion1 = len(np.nonzero(score_midpts > th2)[0]) > 0.8 * len(score_midpts)\n                    criterion2 = score_with_dist_prior > 0\n                    if criterion1 and criterion2:\n                        connection_candidate.append([i, j, score_with_dist_prior, score_with_dist_prior+candA[i][2]+candB[j][2]])\n\n            connection_candidate = sorted(connection_candidate, key=lambda x: x[2], reverse=True)\n            connection = np.zeros((0,5))\n            for c in range(len(connection_candidate)):\n                i,j,s = connection_candidate[c][0:3]\n                if(i not in connection[:,3] and j not in connection[:,4]):\n                    connection = np.vstack([connection, [candA[i][3], candB[j][3], s, i, j]])\n                    if(len(connection) >= min(nA, nB)):\n                        break\n\n            connection_all.append(connection)\n        else:\n            special_k.append(k)\n            connection_all.append([])\n\n    # last number in each row is the total parts number of that person\n    # the second last number in each row is the score of the overall configuration\n    subset = -1 * np.ones((0, 20))\n    candidate = np.array([item for sublist in all_peaks for item in sublist])\n\n    for k in range(len(mapIdx)):\n        if k not in special_k:\n            partAs = connection_all[k][:,0]\n            partBs = connection_all[k][:,1]\n            indexA, indexB = np.array(limbSeq[k]) - 1\n\n            for i in range(len(connection_all[k])): #= 1:size(temp,1)\n                found = 0\n                subset_idx = [-1, -1]\n                for j in range(len(subset)): #1:size(subset,1):\n                    if subset[j][indexA] == partAs[i] or subset[j][indexB] == partBs[i]:\n                        subset_idx[found] = j\n                        found += 1\n\n                if found == 1:\n                    j = subset_idx[0]\n                    if(subset[j][indexB] != partBs[i]):\n                        subset[j][indexB] = partBs[i]\n                        subset[j][-1] += 1\n                        subset[j][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n                elif found == 2: # if found 2 and disjoint, merge them\n                    j1, j2 = subset_idx\n                    print ""found = 2""\n                    # print(""found = 2"")\n                    membership = ((subset[j1]>=0).astype(int) + (subset[j2]>=0).astype(int))[:-2]\n                    if len(np.nonzero(membership == 2)[0]) == 0: #merge\n                        subset[j1][:-2] += (subset[j2][:-2] + 1)\n                        subset[j1][-2:] += subset[j2][-2:]\n                        subset[j1][-2] += connection_all[k][i][2]\n                        subset = np.delete(subset, j2, 0)\n                    else: # as like found == 1\n                        subset[j1][indexB] = partBs[i]\n                        subset[j1][-1] += 1\n                        subset[j1][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n\n                # if find no partA in the subset, create a new subset\n                elif not found and k < 17:\n                    row = -1 * np.ones(20)\n                    row[indexA] = partAs[i]\n                    row[indexB] = partBs[i]\n                    row[-1] = 2\n                    row[-2] = sum(candidate[connection_all[k][i,:2].astype(int), 2]) + connection_all[k][i][2]\n                    subset = np.vstack([subset, row])\n\n    # delete some rows of subset which has few parts occur\n    deleteIdx = [];\n    for i in range(len(subset)):\n        if subset[i][-1] < 4 or subset[i][-2]/subset[i][-1] < 0.4:\n            deleteIdx.append(i)\n    subset = np.delete(subset, deleteIdx, axis=0)\n\n    if len(subset) == 0:\n        return np.array([[-1, -1]] * 18).astype(int)\n\n    cordinates = []\n    result_image_index = np.argmax(subset[:, -2])\n\n    for part in subset[result_image_index, :18]:\n        if part == -1:\n            cordinates.append([-1, -1])\n        else:\n            Y = candidate[part.astype(int), 0]\n            X = candidate[part.astype(int), 1]\n            cordinates.append([X, Y])\n    return np.array(cordinates).astype(int)\n\n# input_folder = \'./results/fashion_PATN/test_latest/images_crop/\'\n# output_path = \'./results/fashion_PATN/test_latest/pckh.csv\'\n\ninput_folder = \'./results/market_PATN/test_latest/images_crop/\'\noutput_path = \'./results/market_PATN/test_latest/pckh.csv\'\n\n\nimg_list = os.listdir(input_folder)\n\nthreshold = 0.1\nboxsize = 368\nscale_search = [0.5, 1, 1.5, 2]\n\nif os.path.exists(output_path):\n    processed_names = set(pd.read_csv(output_path, sep=\':\')[\'name\'])\n    result_file = open(output_path, \'a\')\nelse:\n    result_file = open(output_path, \'w\')\n    processed_names = set()\n    print >> result_file, \'name:keypoints_y:keypoints_x\'\n\n# for image_name in tqdm(os.listdir(input_folder)):\nfor image_name in tqdm(img_list):\n    if image_name in processed_names:\n        continue\n\n    oriImg = imread(os.path.join(input_folder, image_name))[:, :, ::-1]  # B,G,R order\n\n    multiplier = [x * boxsize / oriImg.shape[0] for x in scale_search]\n\n    heatmap_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 19))\n    paf_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 38))\n\n    for m in range(len(multiplier)):\n        scale = multiplier[m]\n\n        new_size = (np.array(oriImg.shape[:2]) * scale).astype(np.int32)\n        imageToTest = resize(oriImg, new_size, order=3, preserve_range=True)\n        imageToTest_padded = imageToTest[np.newaxis, :, :, :]/255 - 0.5\n\n        output1, output2 = model.predict(imageToTest_padded)\n\n        heatmap = st.resize(output2[0], oriImg.shape[:2], preserve_range=True, order=1)\n        paf = st.resize(output1[0], oriImg.shape[:2], preserve_range=True, order=1)\n        heatmap_avg += heatmap\n        paf_avg += paf\n\n    heatmap_avg /= len(multiplier)\n\n    pose_cords = compute_cordinates(heatmap_avg, paf_avg)\n\n    print >> result_file, ""%s: %s: %s"" % (image_name, str(list(pose_cords[:, 0])), str(list(pose_cords[:, 1])))\n    result_file.flush()\n'"
person_transfer/tool/crop_fashion.py,0,"b""from PIL import Image\nimport os\n\nimg_dir = './results/fashion_PATN_test/test_latest/images'\nsave_dir = './results/fashion_PATN_test/test_latest/images_crop'\n\nif not os.path.exists(save_dir):\n\tos.mkdir(save_dir)\n\ncnt = 0\n\nfor item in os.listdir(img_dir):\n\tif not item.endswith('.jpg') and not item.endswith('.png'):\n\t\tcontinue\n\tcnt = cnt + 1\n\tprint('%d/8570 ...' %(cnt))\n\timg = Image.open(os.path.join(img_dir, item))\n\timgcrop = img.crop((704, 0, 880, 256))\n\timgcrop.save(os.path.join(save_dir, item))\n"""
person_transfer/tool/crop_market.py,0,"b""from PIL import Image\nimport os\n\nimg_dir = './results/market_PATN_test/test_latest/images'\nsave_dir = './results/market_PATN_test/test_latest/images_crop'\n\nif not os.path.exists(save_dir):\n\tos.mkdir(save_dir)\n\ncnt = 0\nfor item in os.listdir(img_dir):\n\tif not item.endswith('.jpg') and not item.endswith('.png'):\n\t\tcontinue\n\tcnt = cnt + 1\n\tprint('%d/12000 ...' %(cnt))\n\timg = Image.open(os.path.join(img_dir, item))\n\t# for 5 split\n\timgcrop = img.crop((256, 0, 320, 128))\n\timgcrop.save(os.path.join(save_dir, item))\n"""
person_transfer/tool/generate_fashion_datasets.py,0,"b""import os\nfrom shutil import copyfile\nfrom PIL import Image\n\n# path for downloaded fashion images\nroot_fashion_dir = '/mnt/data4/htang/Pose-Transfer/fashion/Img/DeepFashion'\nassert len(root_fashion_dir) > 0, 'please give the path of raw deep fashion dataset!'\n\n# print(len(root_fashion_dir) )\n\ntrain_images = []\ntrain_f = open('fashion_data/train.lst', 'r')\nfor lines in train_f:\n \tlines = lines.strip()\n \tif lines.endswith('.jpg'):\n \t\ttrain_images.append(lines)\n \t\t# print(train_images)\n\ntest_images = []\ntest_f = open('fashion_data/test.lst', 'r')\nfor lines in test_f:\n\tlines = lines.strip()\n\tif lines.endswith('.jpg'):\n\t\ttest_images.append(lines)\n\ntrain_path = 'fashion_data/train'\nif not os.path.exists(train_path):\n\tos.mkdir(train_path)\n\nfor item in train_images:\n\tfrom_ = os.path.join(root_fashion_dir, item)\n\timg = Image.open(from_)\n\timgcrop = img.crop((40, 0, 216, 256))\n\tto_ = os.path.join(train_path, item)\n\timgcrop.save(to_)\n\t#copyfile(from_, to_)\n\n\ntest_path = 'fashion_data/test'\nif not os.path.exists(test_path):\n\tos.mkdir(test_path)\n\nfor item in test_images:\n\tfrom_ = os.path.join(root_fashion_dir, item)\n\timg = Image.open(from_)\n\timgcrop = img.crop((40, 0, 216, 256))\n\tto_ = os.path.join(test_path, item)\n\t# os.system('cp %s %s' %(from_, to_))\n\t#copyfile(from_, to_)\n\timgcrop.save(to_)\n"""
person_transfer/tool/generate_pose_map_fashion.py,0,"b""import numpy as np\nimport pandas as pd \nimport json\nimport os \n\nMISSING_VALUE = -1\n# fix PATH\nimg_dir  = '/mnt/data4/htang/Pose-Transfer/fashion/Img/DeepFashion' #raw image path\nannotations_file = 'fashion_data/fasion-resize-annotation-train.csv' #pose annotation path\nsave_path = 'fashion_data/trainK' #path to store pose maps\n\ndef load_pose_cords_from_strings(y_str, x_str):\n    y_cords = json.loads(y_str)\n    x_cords = json.loads(x_str)\n    return np.concatenate([np.expand_dims(y_cords, -1), np.expand_dims(x_cords, -1)], axis=1)\n\ndef cords_to_map(cords, img_size, sigma=6):\n    result = np.zeros(img_size + cords.shape[0:1], dtype='uint8')\n    for i, point in enumerate(cords):\n        if point[0] == MISSING_VALUE or point[1] == MISSING_VALUE:\n            continue\n        xx, yy = np.meshgrid(np.arange(img_size[1]), np.arange(img_size[0]))\n        result[..., i] = np.exp(-((yy - point[0]) ** 2 + (xx - point[1]) ** 2) / (2 * sigma ** 2))\n        # result[..., i] = np.where(((yy - point[0]) ** 2 + (xx - point[1]) ** 2) < (sigma ** 2), 1, 0)\n    return result\n\ndef compute_pose(image_dir, annotations_file, savePath, sigma):\n    annotations_file = pd.read_csv(annotations_file, sep=':')\n    annotations_file = annotations_file.set_index('name')\n    image_size = (256, 176)\n    cnt = len(annotations_file)\n    for i in range(cnt):\n        print('processing %d / %d ...' %(i, cnt))\n        row = annotations_file.iloc[i]\n        name = row.name\n        print(savePath, name)\n        file_name = os.path.join(savePath, name + '.npy')\n        kp_array = load_pose_cords_from_strings(row.keypoints_y, row.keypoints_x)\n        pose = cords_to_map(kp_array, image_size, sigma)\n        np.save(file_name, pose)\n        # input()\n  \ncompute_pose(img_dir, annotations_file, save_path, sigma=6)\n"""
person_transfer/tool/generate_pose_map_market.py,0,"b""import numpy as np\nimport pandas as pd \nimport json\nimport os \n\nMISSING_VALUE = -1\n\nimg_dir  = 'market_data/train' #raw image path\nannotations_file = 'market_data/market-annotation-train.csv' #pose annotation path\nsave_path = 'market_data/trainK' #path to store pose maps\n\ndef load_pose_cords_from_strings(y_str, x_str):\n    y_cords = json.loads(y_str)\n    x_cords = json.loads(x_str)\n    return np.concatenate([np.expand_dims(y_cords, -1), np.expand_dims(x_cords, -1)], axis=1)\n\ndef cords_to_map(cords, img_size, sigma=6):\n    result = np.zeros(img_size + cords.shape[0:1], dtype='float32')\n    for i, point in enumerate(cords):\n        if point[0] == MISSING_VALUE or point[1] == MISSING_VALUE:\n            continue\n        xx, yy = np.meshgrid(np.arange(img_size[1]), np.arange(img_size[0]))\n        result[..., i] = np.exp(-((yy - point[0]) ** 2 + (xx - point[1]) ** 2) / (2 * sigma ** 2))\n    return result\n\ndef compute_pose(image_dir, annotations_file, savePath):\n    annotations_file = pd.read_csv(annotations_file, sep=':')\n    annotations_file = annotations_file.set_index('name')\n    image_size = (128, 64)\n    cnt = len(annotations_file)\n    for i in range(cnt):\n        print('processing %d / %d ...' %(i, cnt))\n        row = annotations_file.iloc[i]\n        name = row.name\n        print(savePath, name)\n        file_name = os.path.join(savePath, name + '.npy')\n        kp_array = load_pose_cords_from_strings(row.keypoints_y, row.keypoints_x)\n        pose = cords_to_map(kp_array, image_size)\n        np.save(file_name, pose)\n    \ncompute_pose(img_dir, annotations_file, save_path)\n\n"""
person_transfer/tool/getMetrics_fashion.py,0,"b'import os\nfrom inception_score import get_inception_score\n\nfrom skimage.io import imread, imsave\nfrom skimage.measure import compare_ssim\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport re\n\nfrom PIL import Image\n\ndef l1_score(generated_images, reference_images):\n    score_list = []\n    for reference_image, generated_image in zip(reference_images, generated_images):\n        score = np.abs(2 * (reference_image/255.0 - 0.5) - 2 * (generated_image/255.0 - 0.5)).mean()\n        score_list.append(score)\n    return np.mean(score_list)\n\n\ndef ssim_score(generated_images, reference_images):\n    ssim_score_list = []\n    for reference_image, generated_image in zip(reference_images, generated_images):\n        ssim = compare_ssim(reference_image, generated_image, gaussian_weights=True, sigma=1.5,\n                            use_sample_covariance=False, multichannel=True,\n                            data_range=generated_image.max() - generated_image.min())\n        ssim_score_list.append(ssim)\n    return np.mean(ssim_score_list)\n\n\ndef save_images(input_images, target_images, generated_images, names, output_folder):\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n    for images in zip(input_images, target_images, generated_images, names):\n        res_name = str(\'_\'.join(images[-1])) + \'.png\'\n        imsave(os.path.join(output_folder, res_name), np.concatenate(images[:-1], axis=1))\n\n\ndef create_masked_image(names, images, annotation_file):\n    import pose_utils\n    masked_images = []\n    df = pd.read_csv(annotation_file, sep=\':\')\n    for name, image in zip(names, images):\n        to = name[1]\n        ano_to = df[df[\'name\'] == to].iloc[0]\n\n        kp_to = pose_utils.load_pose_cords_from_strings(ano_to[\'keypoints_y\'], ano_to[\'keypoints_x\'])\n\n        mask = pose_utils.produce_ma_mask(kp_to, image.shape[:2])\n        masked_images.append(image * mask[..., np.newaxis])\n\n    return masked_images\n\n\n\ndef addBounding(image, bound=40):\n    h, w, c = image.shape\n    image_bound = np.ones((h, w+bound*2, c))*255\n    image_bound = image_bound.astype(np.uint8)\n    image_bound[:, bound:bound+w] = image\n\n    return image_bound\n\n\n\ndef load_generated_images(images_folder):\n    input_images = []\n    target_images = []\n    generated_images = []\n\n    names = []\n    for img_name in os.listdir(images_folder):\n        img = imread(os.path.join(images_folder, img_name))\n        w = int(img.shape[1] / 5) #h, w ,c\n\n        input_images.append(addBounding(img[:, :w]))\n        target_images.append(addBounding(img[:, 2*w:3*w]))\n        generated_images.append(addBounding(img[:, 4*w:5*w]))\n\n        assert img_name.endswith(\'_vis.png\') or img_name.endswith(\'_vis.jpg\'), \'unexpected img name: should end with _vis.png\'\n        img_name = img_name[:-8]\n        img_name = img_name.split(\'___\')\n        assert len(img_name) == 2, \'unexpected img split: length 2 expect!\'\n        fr = img_name[0]\n        to = img_name[1]\n\n        names.append([fr, to])\n\n\n    return input_images, target_images, generated_images, names\n\n\ndef test(generated_images_dir, annotations_file_test):\n    # load images\n    print (""Loading images..."")\n\n    input_images, target_images, generated_images, names = load_generated_images(generated_images_dir)\n\n    print (""Compute inception score..."")\n    inception_score = get_inception_score(generated_images)\n    print (""Inception score %s"" % inception_score[0])\n\n    print (""Compute structured similarity score (SSIM)..."")\n    structured_score = ssim_score(generated_images, target_images)\n    print (""SSIM score %s"" % structured_score)\n\n    print (""Inception score = %s; SSIM score = %s"" % (inception_score, structured_score))\n\n\nif __name__ == ""__main__"":\n    # fix these paths\n    generated_images_dir = \'results_v1.0/fashion_PATN_v1.0/test_latest/images\'\n    annotations_file_test = \'fashion_data/fasion-resize-annotation-test.csv\'\n\n    test(generated_images_dir, annotations_file_test)\n\n\n\n\n'"
person_transfer/tool/getMetrics_market.py,0,"b'import os\nfrom inception_score import get_inception_score\n\nfrom skimage.io import imread, imsave\nfrom skimage.measure import compare_ssim\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport re\n\ndef l1_score(generated_images, reference_images):\n    score_list = []\n    for reference_image, generated_image in zip(reference_images, generated_images):\n        score = np.abs(2 * (reference_image/255.0 - 0.5) - 2 * (generated_image/255.0 - 0.5)).mean()\n        score_list.append(score)\n    return np.mean(score_list)\n\n\ndef ssim_score(generated_images, reference_images):\n    ssim_score_list = []\n    for reference_image, generated_image in zip(reference_images, generated_images):\n        ssim = compare_ssim(reference_image, generated_image, gaussian_weights=True, sigma=1.5,\n                            use_sample_covariance=False, multichannel=True,\n                            data_range=generated_image.max() - generated_image.min())\n        ssim_score_list.append(ssim)\n    return np.mean(ssim_score_list)\n\n\ndef save_images(input_images, target_images, generated_images, names, output_folder):\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n    for images in zip(input_images, target_images, generated_images, names):\n        res_name = str(\'_\'.join(images[-1])) + \'.png\'\n        imsave(os.path.join(output_folder, res_name), np.concatenate(images[:-1], axis=1))\n\n\ndef create_masked_image(names, images, annotation_file):\n    import pose_utils\n    masked_images = []\n    df = pd.read_csv(annotation_file, sep=\':\')\n    for name, image in zip(names, images):\n        to = name[1]\n        ano_to = df[df[\'name\'] == to].iloc[0]\n\n        kp_to = pose_utils.load_pose_cords_from_strings(ano_to[\'keypoints_y\'], ano_to[\'keypoints_x\'])\n\n        mask = pose_utils.produce_ma_mask(kp_to, image.shape[:2])\n        masked_images.append(image * mask[..., np.newaxis])\n\n    return masked_images\n\n\ndef load_generated_images(images_folder):\n    input_images = []\n    target_images = []\n    generated_images = []\n\n    names = []\n    for img_name in os.listdir(images_folder):\n        img = imread(os.path.join(images_folder, img_name))\n        w = int(img.shape[1] / 5) #h, w ,c\n        input_images.append(img[:, :w])\n        target_images.append(img[:, 2*w:3*w])\n        generated_images.append(img[:, 4*w:5*w])\n\n        # assert img_name.endswith(\'_vis.png\'), \'unexpected img name: should end with _vis.png\'\n        assert img_name.endswith(\'_vis.png\') or img_name.endswith(\'_vis.jpg\'), \'unexpected img name: should end with _vis.png\'\n\n        img_name = img_name[:-8]\n        img_name = img_name.split(\'___\')\n        assert len(img_name) == 2, \'unexpected img split: length 2 expect!\'\n        fr = img_name[0]\n        to = img_name[1]\n\n        # m = re.match(r\'([A-Za-z0-9_]*.jpg)_([A-Za-z0-9_]*.jpg)\', img_name)\n        # m = re.match(r\'([A-Za-z0-9_]*.jpg)_([A-Za-z0-9_]*.jpg)_vis.png\', img_name)\n        # fr = m.groups()[0]\n        # to = m.groups()[1]\n        names.append([fr, to])\n\n    return input_images, target_images, generated_images, names\n\n\n\ndef test(generated_images_dir, annotations_file_test):\n    print(generated_images_dir, annotations_file_test)\n    print (""Loading images..."")\n    input_images, target_images, generated_images, names = load_generated_images(generated_images_dir)\n\n    print (""Compute inception score..."")\n    inception_score = get_inception_score(generated_images)\n    print (""Inception score %s"" % inception_score[0])\n\n\n    print (""Compute structured similarity score (SSIM)..."")\n    structured_score = ssim_score(generated_images, target_images)\n    print (""SSIM score %s"" % structured_score)\n\n    print (""Compute l1 score..."")\n    norm_score = l1_score(generated_images, target_images)\n    print (""L1 score %s"" % norm_score)\n\n    print (""Compute masked inception score..."")\n    generated_images_masked = create_masked_image(names, generated_images, annotations_file_test)\n    reference_images_masked = create_masked_image(names, target_images, annotations_file_test)\n    inception_score_masked = get_inception_score(generated_images_masked)\n    print (""Inception score masked %s"" % inception_score_masked[0])\n\n    print (""Compute masked SSIM..."")\n    structured_score_masked = ssim_score(generated_images_masked, reference_images_masked)\n    print (""SSIM score masked %s"" % structured_score_masked)\n\n    print (""Inception score = %s, masked = %s; SSIM score = %s, masked = %s; l1 score = %s"" %\n           (inception_score, inception_score_masked, structured_score, structured_score_masked, norm_score))\n\n\n\n\nif __name__ == ""__main__"":\n    generated_images_dir = \'results_v1.0/market_PATN_v1.0/test_latest/images\'\n    annotations_file_test = \'market_data/market-annotation-test.csv\'\n\n    test(generated_images_dir, annotations_file_test)\n\n\n\n\n\n\n'"
person_transfer/tool/inception_score.py,0,"b'# Code derived from tensorflow/tensorflow/models/image/imagenet/classify_image.py\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path\nimport sys\nimport tarfile\n\nimport numpy as np\nfrom six.moves import urllib\nimport tensorflow as tf\nimport glob\nimport scipy.misc\nimport math\nimport sys\n\nMODEL_DIR = \'~/models\'\nDATA_URL = \'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\'\nsoftmax = None\n\n# Call this function with list of images. Each of elements should be a\n# numpy array with values ranging from 0 to 255.\ndef get_inception_score(images, splits=10):\n  #assert(type(images) == list)\n  assert(type(images[0]) == np.ndarray)\n  assert(len(images[0].shape) == 3)\n  assert(np.max(images[0]) > 10)\n  assert(np.min(images[0]) >= 0.0)\n  inps = []\n  for img in images:\n    img = img.astype(np.float32)\n    inps.append(np.expand_dims(img, 0))\n  bs = 10\n  with tf.Session() as sess:\n    preds = []\n    n_batches = int(math.ceil(float(len(inps)) / float(bs)))\n    for i in range(n_batches):\n        sys.stdout.write(""."")\n        sys.stdout.flush()\n        inp = inps[(i * bs):min((i + 1) * bs, len(inps))]\n        inp = np.concatenate(inp, 0)\n        pred = sess.run(softmax, {\'ExpandDims:0\': inp})\n        preds.append(pred)\n    preds = np.concatenate(preds, 0)\n    scores = []\n    for i in range(splits):\n      part = preds[(i * preds.shape[0] // splits):((i + 1) * preds.shape[0] // splits), :]\n      kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n      kl = np.mean(np.sum(kl, 1))\n      scores.append(np.exp(kl))\n    return np.mean(scores), np.std(scores)\n\n# This function is called automatically.\ndef _init_inception():\n  global softmax\n  if not os.path.exists(MODEL_DIR):\n    os.makedirs(MODEL_DIR)\n  filename = DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(MODEL_DIR, filename)\n  if not os.path.exists(filepath):\n    def _progress(count, block_size, total_size):\n      sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n          filename, float(count * block_size) / float(total_size) * 100.0))\n      sys.stdout.flush()\n    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print(\'Succesfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n  tarfile.open(filepath, \'r:gz\').extractall(MODEL_DIR)\n  with tf.gfile.FastGFile(os.path.join(\n      MODEL_DIR, \'classify_image_graph_def.pb\'), \'rb\') as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n    _ = tf.import_graph_def(graph_def, name=\'\')\n  # Works with an arbitrary minibatch size.\n  with tf.Session() as sess:\n    pool3 = sess.graph.get_tensor_by_name(\'pool_3:0\')\n    ops = pool3.graph.get_operations()\n    for op_idx, op in enumerate(ops):\n        for o in op.outputs:\n            shape = o.get_shape()\n            shape = [s.value for s in shape]\n            new_shape = []\n            for j, s in enumerate(shape):\n                if s == 1 and j == 0:\n                    new_shape.append(None)\n                else:\n                    new_shape.append(s)\n            o._shape = tf.TensorShape(new_shape)\n    w = sess.graph.get_operation_by_name(""softmax/logits/MatMul"").inputs[1]\n    logits = tf.matmul(tf.squeeze(pool3), w)\n    softmax = tf.nn.softmax(logits)\n\nif softmax is None:\n    _init_inception()\n'"
person_transfer/tool/pose_utils.py,0,"b'import numpy as np\nfrom scipy.ndimage.filters import gaussian_filter\nfrom skimage.draw import circle, line_aa, polygon\nimport json\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom collections import defaultdict\nimport skimage.measure, skimage.transform\nimport sys\n\nLIMB_SEQ = [[1,2], [1,5], [2,3], [3,4], [5,6], [6,7], [1,8], [8,9],\n           [9,10], [1,11], [11,12], [12,13], [1,0], [0,14], [14,16],\n           [0,15], [15,17], [2,16], [5,17]]\n\nCOLORS = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],\n          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],\n          [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]\n\n\nLABELS = [\'nose\', \'neck\', \'Rsho\', \'Relb\', \'Rwri\', \'Lsho\', \'Lelb\', \'Lwri\',\n               \'Rhip\', \'Rkne\', \'Rank\', \'Lhip\', \'Lkne\', \'Lank\', \'Leye\', \'Reye\', \'Lear\', \'Rear\']\n\nMISSING_VALUE = -1\n\n\ndef map_to_cord(pose_map, threshold=0.1):\n    all_peaks = [[] for i in range(18)]\n    pose_map = pose_map[..., :18]\n\n    y, x, z = np.where(np.logical_and(pose_map == pose_map.max(axis = (0, 1)),\n                                     pose_map > threshold))\n    for x_i, y_i, z_i in zip(x, y, z):\n        all_peaks[z_i].append([x_i, y_i])\n\n    x_values = []\n    y_values = []\n\n    for i in range(18):\n        if len(all_peaks[i]) != 0:\n            x_values.append(all_peaks[i][0][0])\n            y_values.append(all_peaks[i][0][1])\n        else:\n            x_values.append(MISSING_VALUE)\n            y_values.append(MISSING_VALUE)\n\n    return np.concatenate([np.expand_dims(y_values, -1), np.expand_dims(x_values, -1)], axis=1)\n\n\ndef cords_to_map(cords, img_size, sigma=6):\n    result = np.zeros(img_size + cords.shape[0:1], dtype=\'float32\')\n    for i, point in enumerate(cords):\n        if point[0] == MISSING_VALUE or point[1] == MISSING_VALUE:\n            continue\n        xx, yy = np.meshgrid(np.arange(img_size[1]), np.arange(img_size[0]))\n        result[..., i] = np.exp(-((yy - point[0]) ** 2 + (xx - point[1]) ** 2) / (2 * sigma ** 2))\n    return result\n\n\ndef draw_pose_from_cords(pose_joints, img_size, radius=2, draw_joints=True):\n    colors = np.zeros(shape=img_size + (3, ), dtype=np.uint8)\n    mask = np.zeros(shape=img_size, dtype=bool)\n\n    if draw_joints:\n        for f, t in LIMB_SEQ:\n            from_missing = pose_joints[f][0] == MISSING_VALUE or pose_joints[f][1] == MISSING_VALUE\n            to_missing = pose_joints[t][0] == MISSING_VALUE or pose_joints[t][1] == MISSING_VALUE\n            if from_missing or to_missing:\n                continue\n            yy, xx, val = line_aa(pose_joints[f][0], pose_joints[f][1], pose_joints[t][0], pose_joints[t][1])\n            colors[yy, xx] = np.expand_dims(val, 1) * 255\n            mask[yy, xx] = True\n\n    for i, joint in enumerate(pose_joints):\n        if pose_joints[i][0] == MISSING_VALUE or pose_joints[i][1] == MISSING_VALUE:\n            continue\n        yy, xx = circle(joint[0], joint[1], radius=radius, shape=img_size)\n        colors[yy, xx] = COLORS[i]\n        mask[yy, xx] = True\n\n    return colors, mask\n\n\ndef draw_pose_from_map(pose_map, threshold=0.1, **kwargs):\n    cords = map_to_cord(pose_map, threshold=threshold)\n    return draw_pose_from_cords(cords, pose_map.shape[:2], **kwargs)\n\n\ndef load_pose_cords_from_strings(y_str, x_str):\n    y_cords = json.loads(y_str)\n    x_cords = json.loads(x_str)\n    return np.concatenate([np.expand_dims(y_cords, -1), np.expand_dims(x_cords, -1)], axis=1)\n\ndef mean_inputation(X):\n    X = X.copy()\n    for i in range(X.shape[1]):\n        for j in range(X.shape[2]):\n            val = np.mean(X[:, i, j][X[:, i, j] != -1])\n            X[:, i, j][X[:, i, j] == -1] = val\n    return X\n\ndef draw_legend():\n    handles = [mpatches.Patch(color=np.array(color) / 255.0, label=name) for color, name in zip(COLORS, LABELS)]\n    plt.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\ndef produce_ma_mask(kp_array, img_size, point_radius=4):\n    from skimage.morphology import dilation, erosion, square\n    mask = np.zeros(shape=img_size, dtype=bool)\n    limbs = [[2,3], [2,6], [3,4], [4,5], [6,7], [7,8], [2,9], [9,10],\n              [10,11], [2,12], [12,13], [13,14], [2,1], [1,15], [15,17],\n               [1,16], [16,18], [2,17], [2,18], [9,12], [12,6], [9,3], [17,18]]\n    limbs = np.array(limbs) - 1\n    for f, t in limbs:\n        from_missing = kp_array[f][0] == MISSING_VALUE or kp_array[f][1] == MISSING_VALUE\n        to_missing = kp_array[t][0] == MISSING_VALUE or kp_array[t][1] == MISSING_VALUE\n        if from_missing or to_missing:\n            continue\n\n        norm_vec = kp_array[f] - kp_array[t]\n        norm_vec = np.array([-norm_vec[1], norm_vec[0]])\n        norm_vec = point_radius * norm_vec / np.linalg.norm(norm_vec)\n\n\n        vetexes = np.array([\n            kp_array[f] + norm_vec,\n            kp_array[f] - norm_vec,\n            kp_array[t] - norm_vec,\n            kp_array[t] + norm_vec\n        ])\n        yy, xx = polygon(vetexes[:, 0], vetexes[:, 1], shape=img_size)\n        mask[yy, xx] = True\n\n    for i, joint in enumerate(kp_array):\n        if kp_array[i][0] == MISSING_VALUE or kp_array[i][1] == MISSING_VALUE:\n            continue\n        yy, xx = circle(joint[0], joint[1], radius=point_radius, shape=img_size)\n        mask[yy, xx] = True\n\n    mask = dilation(mask, square(5))\n    mask = erosion(mask, square(5))\n    return mask\n\nif __name__ == ""__main__"":\n    import pandas as pd\n    from skimage.io import imread\n    import pylab as plt\n    import os\n    i = 5\n    df = pd.read_csv(\'data/market-annotation-train.csv\', sep=\':\')\n\n    for index, row in df.iterrows():\n        pose_cords = load_pose_cords_from_strings(row[\'keypoints_y\'], row[\'keypoints_x\'])\n\n        colors, mask = draw_pose_from_cords(pose_cords, (128, 64))\n\n        mmm = produce_ma_mask(pose_cords, (128, 64)).astype(float)[..., np.newaxis].repeat(3, axis=-1)\n        # print mmm.shape\n        print(mmm.shape)\n        img = imread(\'data/market-dataset/train/\' + row[\'name\'])\n\n        mmm[mask] = colors[mask]\n\n        # print (mmm)\n        print(mmm)\n        plt.subplot(1, 1, 1)\n        plt.imshow(mmm)\n        plt.show()\n'"
person_transfer/tool/resize_fashion.py,0,"b""from skimage.io import imread, imsave\nfrom skimage.transform import resize\nimport os\nimport numpy as np\nimport pandas as pd\nimport json\n\ndef resize_dataset(folder, new_folder, new_size = (256, 176), crop_bord=40):\n    if not os.path.exists(new_folder):\n        os.makedirs(new_folder)\n    for name in os.listdir(folder):\n        old_name = os.path.join(folder, name)\n        new_name = os.path.join(new_folder, name)\n\n        img = imread(old_name)\n        if crop_bord == 0:\n            pass\n        else:\n            img = img[:, crop_bord:-crop_bord]\n\n        img = resize(img, new_size, preserve_range=True).astype(np.uint8)\n\n        imsave(new_name, img)\n\ndef resize_annotations(name, new_name, new_size = (256, 176), old_size = (256, 256), crop_bord=40):\n    df = pd.read_csv(name, sep=':')\n\n    ratio_y = new_size[0] / float(old_size[0])\n    ratio_x = new_size[1] / float(old_size[1] - 2 * crop_bord)\n\n    def modify(values, ratio, crop):\n        val = np.array(json.loads(values))\n        mask = val == -1\n        val = ((val - crop) * ratio).astype(int)\n        val[mask] = -1\n        return str(list(val))\n\n    df['keypoints_y'] = df.apply(lambda row: modify(row['keypoints_y'], ratio_y, 0), axis=1)\n    df['keypoints_x'] = df.apply(lambda row: modify(row['keypoints_x'], ratio_x, crop_bord), axis=1)\n\n    df.to_csv(new_name, sep=':', index=False)\n\n\nroot_dir = 'xxx'\nresize_dataset(root_dir + '/test', root_dir + 'fashion_resize/test')\nresize_annotations(root_dir + 'fasion-annotation-test.csv', root_dir + 'fasion-resize-annotation-test.csv')\n\nresize_dataset(root_dir + '/train', root_dir + 'fashion_resize/train')\nresize_annotations(root_dir + 'fasion-annotation-train.csv', root_dir + 'fasion-resize-annotation-train.csv')\n\n\n"""
person_transfer/tool/rm_insnorm_running_vars.py,2,"b'import torch\n\nckp_path = \'./checkpoints/fashion_PATN/latest_net_netG.pth\'\nsave_path = \'./checkpoints/fashion_PATN_v1.0/latest_net_netG.pth\'\nstates_dict = torch.load(ckp_path)\nstates_dict_new = states_dict.copy()\nfor key in states_dict.keys():\n\tif ""running_var"" in key or ""running_mean"" in key:\n\t\tdel states_dict_new[key]\n\ntorch.save(states_dict_new, save_path)'"
person_transfer/util/__init__.py,0,b''
person_transfer/util/get_data.py,0,"b'from __future__ import print_function\nimport os\nimport tarfile\nimport requests\nfrom warnings import warn\nfrom zipfile import ZipFile\nfrom bs4 import BeautifulSoup\nfrom os.path import abspath, isdir, join, basename\n\n\nclass GetData(object):\n    """"""\n\n    Download CycleGAN or Pix2Pix Data.\n\n    Args:\n        technique : str\n            One of: \'cyclegan\' or \'pix2pix\'.\n        verbose : bool\n            If True, print additional information.\n\n    Examples:\n        >>> from util.get_data import GetData\n        >>> gd = GetData(technique=\'cyclegan\')\n        >>> new_data_path = gd.get(save_path=\'./datasets\')  # options will be displayed.\n\n    """"""\n\n    def __init__(self, technique=\'cyclegan\', verbose=True):\n        url_dict = {\n            \'pix2pix\': \'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets\',\n            \'cyclegan\': \'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets\'\n        }\n        self.url = url_dict.get(technique.lower())\n        self._verbose = verbose\n\n    def _print(self, text):\n        if self._verbose:\n            print(text)\n\n    @staticmethod\n    def _get_options(r):\n        soup = BeautifulSoup(r.text, \'lxml\')\n        options = [h.text for h in soup.find_all(\'a\', href=True)\n                   if h.text.endswith((\'.zip\', \'tar.gz\'))]\n        return options\n\n    def _present_options(self):\n        r = requests.get(self.url)\n        options = self._get_options(r)\n        print(\'Options:\\n\')\n        for i, o in enumerate(options):\n            print(""{0}: {1}"".format(i, o))\n        choice = input(""\\nPlease enter the number of the ""\n                       ""dataset above you wish to download:"")\n        return options[int(choice)]\n\n    def _download_data(self, dataset_url, save_path):\n        if not isdir(save_path):\n            os.makedirs(save_path)\n\n        base = basename(dataset_url)\n        temp_save_path = join(save_path, base)\n\n        with open(temp_save_path, ""wb"") as f:\n            r = requests.get(dataset_url)\n            f.write(r.content)\n\n        if base.endswith(\'.tar.gz\'):\n            obj = tarfile.open(temp_save_path)\n        elif base.endswith(\'.zip\'):\n            obj = ZipFile(temp_save_path, \'r\')\n        else:\n            raise ValueError(""Unknown File Type: {0}."".format(base))\n\n        self._print(""Unpacking Data..."")\n        obj.extractall(save_path)\n        obj.close()\n        os.remove(temp_save_path)\n\n    def get(self, save_path, dataset=None):\n        """"""\n\n        Download a dataset.\n\n        Args:\n            save_path : str\n                A directory to save the data to.\n            dataset : str, optional\n                A specific dataset to download.\n                Note: this must include the file extension.\n                If None, options will be presented for you\n                to choose from.\n\n        Returns:\n            save_path_full : str\n                The absolute path to the downloaded data.\n\n        """"""\n        if dataset is None:\n            selected_dataset = self._present_options()\n        else:\n            selected_dataset = dataset\n\n        save_path_full = join(save_path, selected_dataset.split(\'.\')[0])\n\n        if isdir(save_path_full):\n            warn(""\\n\'{0}\' already exists. Voiding Download."".format(\n                save_path_full))\n        else:\n            self._print(\'Downloading Data...\')\n            url = ""{0}/{1}"".format(self.url, selected_dataset)\n            self._download_data(url, save_path=save_path)\n\n        return abspath(save_path_full)\n'"
person_transfer/util/html.py,0,"b'import dominate\nfrom dominate.tags import *\nimport os\n\n\nclass HTML:\n    def __init__(self, web_dir, title, reflesh=0):\n        self.title = title\n        self.web_dir = web_dir\n        self.img_dir = os.path.join(self.web_dir, \'images\')\n        if not os.path.exists(self.web_dir):\n            os.makedirs(self.web_dir)\n        if not os.path.exists(self.img_dir):\n            os.makedirs(self.img_dir)\n        # print(self.img_dir)\n\n        self.doc = dominate.document(title=title)\n        if reflesh > 0:\n            with self.doc.head:\n                meta(http_equiv=""reflesh"", content=str(reflesh))\n\n    def get_image_dir(self):\n        return self.img_dir\n\n    def add_header(self, str):\n        with self.doc:\n            h3(str)\n\n    def add_table(self, border=1):\n        self.t = table(border=border, style=""table-layout: fixed;"")\n        self.doc.add(self.t)\n\n    def add_images(self, ims, txts, links, width=400):\n        self.add_table()\n        with self.t:\n            with tr():\n                for im, txt, link in zip(ims, txts, links):\n                    with td(style=""word-wrap: break-word;"", halign=""center"", valign=""top""):\n                        with p():\n                            with a(href=os.path.join(\'images\', link)):\n                                img(style=""width:%dpx"" % width, src=os.path.join(\'images\', im))\n                            br()\n                            p(txt)\n\n    def save(self):\n        html_file = \'%s/index.html\' % self.web_dir\n        f = open(html_file, \'wt\')\n        f.write(self.doc.render())\n        f.close()\n\n\nif __name__ == \'__main__\':\n    html = HTML(\'web/\', \'test_html\')\n    html.add_header(\'hello world\')\n\n    ims = []\n    txts = []\n    links = []\n    for n in range(4):\n        ims.append(\'image_%d.png\' % n)\n        txts.append(\'text_%d\' % n)\n        links.append(\'image_%d.png\' % n)\n    html.add_images(ims, txts, links)\n    html.save()\n'"
person_transfer/util/image_pool.py,3,"b'import random\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\n\n\nclass ImagePool():\n    def __init__(self, pool_size):\n        self.pool_size = pool_size\n        if self.pool_size > 0:\n            self.num_imgs = 0\n            self.images = []\n\n    def query(self, images):\n        if self.pool_size == 0:\n            return Variable(images)\n        return_images = []\n        for image in images:\n            image = torch.unsqueeze(image, 0)\n            if self.num_imgs < self.pool_size:\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n                return_images.append(image)\n            else:\n                p = random.uniform(0, 1)\n                if p > 0.5:\n                    random_id = random.randint(0, self.pool_size-1)\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image\n                    return_images.append(tmp)\n                else:\n                    return_images.append(image)\n        return_images = Variable(torch.cat(return_images, 0))\n        return return_images\n'"
person_transfer/util/png.py,0,"b'import struct\nimport zlib\n\ndef encode(buf, width, height):\n  """""" buf: must be bytes or a bytearray in py3, a regular string in py2. formatted RGBRGB... """"""\n  assert (width * height * 3 == len(buf))\n  bpp = 3\n\n  def raw_data():\n    # reverse the vertical line order and add null bytes at the start\n    row_bytes = width * bpp\n    for row_start in range((height - 1) * width * bpp, -1, -row_bytes):\n      yield b\'\\x00\'\n      yield buf[row_start:row_start + row_bytes]\n\n  def chunk(tag, data):\n    return [\n        struct.pack(""!I"", len(data)),\n        tag,\n        data,\n        struct.pack(""!I"", 0xFFFFFFFF & zlib.crc32(data, zlib.crc32(tag)))\n      ]\n\n  SIGNATURE = b\'\\x89PNG\\r\\n\\x1a\\n\'\n  COLOR_TYPE_RGB = 2\n  COLOR_TYPE_RGBA = 6\n  bit_depth = 8\n  return b\'\'.join(\n      [ SIGNATURE ] +\n      chunk(b\'IHDR\', struct.pack(""!2I5B"", width, height, bit_depth, COLOR_TYPE_RGB, 0, 0, 0)) +\n      chunk(b\'IDAT\', zlib.compress(b\'\'.join(raw_data()), 9)) +\n      chunk(b\'IEND\', b\'\')\n    )\n'"
person_transfer/util/util.py,1,"b'from __future__ import print_function\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport inspect, re\nimport numpy as np\nimport os\nimport collections\n\nfrom skimage.draw import circle, line_aa, polygon\n\n# Converts a Tensor into a Numpy array\n# |imtype|: the desired type of the converted numpy array\ndef tensor2im(image_tensor, imtype=np.uint8):\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    if image_numpy.shape[0] == 1:\n        image_numpy = np.tile(image_numpy, (3, 1, 1))\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n    return image_numpy.astype(imtype)\n\n\n# draw pose img\nLIMB_SEQ = [[1,2], [1,5], [2,3], [3,4], [5,6], [6,7], [1,8], [8,9],\n           [9,10], [1,11], [11,12], [12,13], [1,0], [0,14], [14,16],\n           [0,15], [15,17], [2,16], [5,17]]\n\nCOLORS = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],\n          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],\n          [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]\n\n\nLABELS = [\'nose\', \'neck\', \'Rsho\', \'Relb\', \'Rwri\', \'Lsho\', \'Lelb\', \'Lwri\',\n               \'Rhip\', \'Rkne\', \'Rank\', \'Lhip\', \'Lkne\', \'Lank\', \'Leye\', \'Reye\', \'Lear\', \'Rear\']\n\nMISSING_VALUE = -1\n\ndef map_to_cord(pose_map, threshold=0.1):\n    all_peaks = [[] for i in range(18)]\n    pose_map = pose_map[..., :18]\n\n    y, x, z = np.where(np.logical_and(pose_map == pose_map.max(axis = (0, 1)),\n                                     pose_map > threshold))\n    for x_i, y_i, z_i in zip(x, y, z):\n        all_peaks[z_i].append([x_i, y_i])\n\n    x_values = []\n    y_values = []\n\n    for i in range(18):\n        if len(all_peaks[i]) != 0:\n            x_values.append(all_peaks[i][0][0])\n            y_values.append(all_peaks[i][0][1])\n        else:\n            x_values.append(MISSING_VALUE)\n            y_values.append(MISSING_VALUE)\n\n    return np.concatenate([np.expand_dims(y_values, -1), np.expand_dims(x_values, -1)], axis=1)\n\ndef draw_pose_from_map(pose_map, threshold=0.1, **kwargs):\n    # CHW -> HCW -> HWC\n    pose_map = pose_map[0].cpu().transpose(1, 0).transpose(2, 1).numpy()\n\n    cords = map_to_cord(pose_map, threshold=threshold)\n    return draw_pose_from_cords(cords, pose_map.shape[:2], **kwargs)\n\n\n# draw pose from map\ndef draw_pose_from_cords(pose_joints, img_size, radius=2, draw_joints=True):\n    colors = np.zeros(shape=img_size + (3, ), dtype=np.uint8)\n    mask = np.zeros(shape=img_size, dtype=bool)\n\n    if draw_joints:\n        for f, t in LIMB_SEQ:\n            from_missing = pose_joints[f][0] == MISSING_VALUE or pose_joints[f][1] == MISSING_VALUE\n            to_missing = pose_joints[t][0] == MISSING_VALUE or pose_joints[t][1] == MISSING_VALUE\n            if from_missing or to_missing:\n                continue\n            yy, xx, val = line_aa(pose_joints[f][0], pose_joints[f][1], pose_joints[t][0], pose_joints[t][1])\n            colors[yy, xx] = np.expand_dims(val, 1) * 255\n            mask[yy, xx] = True\n\n    for i, joint in enumerate(pose_joints):\n        if pose_joints[i][0] == MISSING_VALUE or pose_joints[i][1] == MISSING_VALUE:\n            continue\n        yy, xx = circle(joint[0], joint[1], radius=radius, shape=img_size)\n        colors[yy, xx] = COLORS[i]\n        mask[yy, xx] = True\n\n    return colors, mask\n\n\ndef diagnose_network(net, name=\'network\'):\n    mean = 0.0\n    count = 0\n    for param in net.parameters():\n        if param.grad is not None:\n            mean += torch.mean(torch.abs(param.grad.data))\n            count += 1\n    if count > 0:\n        mean = mean / count\n    print(name)\n    print(mean)\n\n\ndef save_image(image_numpy, image_path):\n    image_pil = Image.fromarray(image_numpy)\n    image_pil.save(image_path)\n\ndef info(object, spacing=10, collapse=1):\n    """"""Print methods and doc strings.\n    Takes module, class, list, dictionary, or string.""""""\n    methodList = [e for e in dir(object) if isinstance(getattr(object, e), collections.Callable)]\n    processFunc = collapse and (lambda s: "" "".join(s.split())) or (lambda s: s)\n    print( ""\\n"".join([""%s %s"" %\n                     (method.ljust(spacing),\n                      processFunc(str(getattr(object, method).__doc__)))\n                     for method in methodList]) )\n\ndef varname(p):\n    for line in inspect.getframeinfo(inspect.currentframe().f_back)[3]:\n        m = re.search(r\'\\bvarname\\s*\\(\\s*([A-Za-z_][A-Za-z0-9_]*)\\s*\\)\', line)\n        if m:\n            return m.group(1)\n\ndef print_numpy(x, val=True, shp=False):\n    x = x.astype(np.float64)\n    if shp:\n        print(\'shape,\', x.shape)\n    if val:\n        x = x.flatten()\n        print(\'mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f\' % (\n            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n'"
person_transfer/util/visualizer.py,0,"b'import numpy as np\nimport os\nimport ntpath\nimport time\nfrom . import util\nfrom . import html\n\n\nclass Visualizer():\n    def __init__(self, opt):\n        # self.opt = opt\n        self.display_id = opt.display_id\n        self.use_html = opt.isTrain and not opt.no_html\n        self.win_size = opt.display_winsize\n        self.name = opt.name\n        self.opt = opt\n        self.saved = False\n        if self.display_id > 0:\n            import visdom\n            self.vis = visdom.Visdom(port=opt.display_port)\n\n        if self.use_html:\n            self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, \'web\')\n            self.img_dir = os.path.join(self.web_dir, \'images\')\n            print(\'create web directory %s...\' % self.web_dir)\n            util.mkdirs([self.web_dir, self.img_dir])\n        self.log_name = os.path.join(opt.checkpoints_dir, opt.name, \'loss_log.txt\')\n        with open(self.log_name, ""a"") as log_file:\n            now = time.strftime(""%c"")\n            log_file.write(\'================ Training Loss (%s) ================\\n\' % now)\n\n    def reset(self):\n        self.saved = False\n\n    # |visuals|: dictionary of images to display or save\n    def display_current_results(self, visuals, epoch, save_result):\n        if self.display_id > 0:  # show images in the browser\n            ncols = self.opt.display_single_pane_ncols\n            if ncols > 0:\n                h, w = next(iter(visuals.values())).shape[:2]\n                table_css = """"""<style>\n                        table {border-collapse: separate; border-spacing:4px; white-space:nowrap; text-align:center}\n                        table td {width: %dpx; height: %dpx; padding: 4px; outline: 4px solid black}\n                        </style>"""""" % (w, h)\n                title = self.name\n                label_html = \'\'\n                label_html_row = \'\'\n                nrows = int(np.ceil(len(visuals.items()) / ncols))\n                images = []\n                idx = 0\n                for label, image_numpy in visuals.items():\n                    label_html_row += \'<td>%s</td>\' % label\n                    images.append(image_numpy.transpose([2, 0, 1]))\n                    idx += 1\n                    if idx % ncols == 0:\n                        label_html += \'<tr>%s</tr>\' % label_html_row\n                        label_html_row = \'\'\n                white_image = np.ones_like(image_numpy.transpose([2, 0, 1]))*255\n                while idx % ncols != 0:\n                    images.append(white_image)\n                    label_html_row += \'<td></td>\'\n                    idx += 1\n                if label_html_row != \'\':\n                    label_html += \'<tr>%s</tr>\' % label_html_row\n                # pane col = image row\n                self.vis.images(images, nrow=ncols, win=self.display_id + 1,\n                                padding=2, opts=dict(title=title + \' images\'))\n                label_html = \'<table>%s</table>\' % label_html\n                self.vis.text(table_css + label_html, win=self.display_id + 2,\n                              opts=dict(title=title + \' labels\'))\n            else:\n                idx = 1\n                for label, image_numpy in visuals.items():\n                    self.vis.image(image_numpy.transpose([2, 0, 1]), opts=dict(title=label),\n                                   win=self.display_id + idx)\n                    idx += 1\n\n        if self.use_html and (save_result or not self.saved):  # save images to a html file\n            self.saved = True\n            for label, image_numpy in visuals.items():\n                img_path = os.path.join(self.img_dir, \'epoch%.3d_%s.png\' % (epoch, label))\n                util.save_image(image_numpy, img_path)\n            # update website\n            webpage = html.HTML(self.web_dir, \'Experiment name = %s\' % self.name, reflesh=1)\n            for n in range(epoch, 0, -1):\n                webpage.add_header(\'epoch [%d]\' % n)\n                ims = []\n                txts = []\n                links = []\n\n                for label, image_numpy in visuals.items():\n                    img_path = \'epoch%.3d_%s.png\' % (n, label)\n                    ims.append(img_path)\n                    txts.append(label)\n                    links.append(img_path)\n                webpage.add_images(ims, txts, links, width=self.win_size)\n            webpage.save()\n\n    # errors: dictionary of error labels and values\n    def plot_current_errors(self, epoch, counter_ratio, opt, errors):\n        if not hasattr(self, \'plot_data\'):\n            self.plot_data = {\'X\': [], \'Y\': [], \'legend\': list(errors.keys())}\n        self.plot_data[\'X\'].append(epoch + counter_ratio)\n        self.plot_data[\'Y\'].append([errors[k] for k in self.plot_data[\'legend\']])\n        self.vis.line(\n            X=np.stack([np.array(self.plot_data[\'X\'])] * len(self.plot_data[\'legend\']), 1),\n            Y=np.array(self.plot_data[\'Y\']),\n            opts={\n                \'title\': self.name + \' loss over time\',\n                \'legend\': self.plot_data[\'legend\'],\n                \'xlabel\': \'epoch\',\n                \'ylabel\': \'loss\'},\n            win=self.display_id)\n\n    # errors: same format as |errors| of plotCurrentErrors\n    def print_current_errors(self, epoch, i, errors, t):\n        message = \'(epoch: %d, iters: %d, time: %.3f) \' % (epoch, i, t)\n        for k, v in errors.items():\n            message += \'%s: %.3f \' % (k, v)\n\n        print(message)\n        with open(self.log_name, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n\n    # save image to the disk\n    def save_images(self, webpage, visuals, image_path):\n        image_dir = webpage.get_image_dir()\n        short_path = ntpath.basename(image_path[0])\n        name = os.path.splitext(short_path)[0]\n\n        webpage.add_header(name)\n        ims = []\n        txts = []\n        links = []\n\n        for label, image_numpy in visuals.items():\n            image_name = \'%s_%s.jpg\' % (image_path[0], label)\n            save_path = os.path.join(image_dir, image_name)\n            print(save_path)\n            util.save_image(image_numpy, save_path)\n\n            ims.append(image_name)\n            txts.append(label)\n            links.append(image_name)\n        webpage.add_images(ims, txts, links, width=self.win_size)\n'"
selectiongan_v1/data/__init__.py,2,"b'import importlib\nimport torch.utils.data\nfrom data.base_data_loader import BaseDataLoader\nfrom data.base_dataset import BaseDataset\n\ndef find_dataset_using_name(dataset_name):\n    # Given the option --dataset_mode [datasetname],\n    # the file ""data/datasetname_dataset.py""\n    # will be imported.\n    dataset_filename = ""data."" + dataset_name + ""_dataset""\n    datasetlib = importlib.import_module(dataset_filename)\n\n    # In the file, the class called DatasetNameDataset() will\n    # be instantiated. It has to be a subclass of BaseDataset,\n    # and it is case-insensitive.\n    dataset = None\n    target_dataset_name = dataset_name.replace(\'_\', \'\') + \'dataset\'\n    for name, cls in datasetlib.__dict__.items():\n        if name.lower() == target_dataset_name.lower() \\\n           and issubclass(cls, BaseDataset):\n            dataset = cls\n            \n    if dataset is None:\n        print(""In %s.py, there should be a subclass of BaseDataset with class name that matches %s in lowercase."" % (dataset_filename, target_dataset_name))\n        exit(0)\n\n    return dataset\n\n\ndef get_option_setter(dataset_name):    \n    dataset_class = find_dataset_using_name(dataset_name)\n    return dataset_class.modify_commandline_options\n\n\ndef create_dataset(opt):\n    dataset = find_dataset_using_name(opt.dataset_mode)\n    instance = dataset()\n    instance.initialize(opt)\n    print(""dataset [%s] was created"" % (instance.name()))\n    return instance\n\n\ndef CreateDataLoader(opt):\n    data_loader = CustomDatasetDataLoader()\n    data_loader.initialize(opt)\n    return data_loader\n\n\n## Wrapper class of Dataset class that performs\n## multi-threaded data loading\nclass CustomDatasetDataLoader(BaseDataLoader):\n    def name(self):\n        return \'CustomDatasetDataLoader\'\n\n    def initialize(self, opt):\n        BaseDataLoader.initialize(self, opt)\n        self.dataset = create_dataset(opt)\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=opt.batchSize,\n            shuffle=not opt.serial_batches,\n            num_workers=int(opt.nThreads))\n\n    def load_data(self):\n        return self\n\n    def __len__(self):\n        return min(len(self.dataset), self.opt.max_dataset_size)\n\n    def __iter__(self):\n        for i, data in enumerate(self.dataloader):\n            if i * self.opt.batchSize >= self.opt.max_dataset_size:\n                break\n            yield data\n'"
selectiongan_v1/data/aligned_dataset.py,1,"b""import os.path\nimport random\nimport torchvision.transforms as transforms\nimport torch\nfrom data.base_dataset import BaseDataset\nfrom data.image_folder import make_dataset\nfrom PIL import Image\n\n\nclass AlignedDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_AB = os.path.join(opt.dataroot, opt.phase)\n        self.AB_paths = sorted(make_dataset(self.dir_AB))\n        assert(opt.resize_or_crop == 'resize_and_crop')\n\n    def __getitem__(self, index):\n        AB_path = self.AB_paths[index]\n        ABCD = Image.open(AB_path).convert('RGB')\n        w, h = ABCD.size\n        w2 = int(w / 4)\n        A = ABCD.crop((0, 0, w2, h)).resize((self.opt.loadSize, self.opt.loadSize), Image.BICUBIC)\n        B = ABCD.crop((w2, 0, w2+w2, h)).resize((self.opt.loadSize, self.opt.loadSize), Image.BICUBIC)\n        C = ABCD.crop((w2+w2, 0, w2+w2+w2, h)).resize((self.opt.loadSize, self.opt.loadSize), Image.BICUBIC)\n        D = ABCD.crop((w2+w2+w2, 0, w, h)).resize((self.opt.loadSize, self.opt.loadSize), Image.BICUBIC)\n\n        A = transforms.ToTensor()(A)\n        B = transforms.ToTensor()(B)\n        C = transforms.ToTensor()(C)\n        D = transforms.ToTensor()(D)\n        w_offset = random.randint(0, max(0, self.opt.loadSize - self.opt.fineSize - 1))\n        h_offset = random.randint(0, max(0, self.opt.loadSize - self.opt.fineSize - 1))\n\n        A = A[:, h_offset:h_offset + self.opt.fineSize, w_offset:w_offset + self.opt.fineSize]\n        B = B[:, h_offset:h_offset + self.opt.fineSize, w_offset:w_offset + self.opt.fineSize]\n        C = C[:, h_offset:h_offset + self.opt.fineSize, w_offset:w_offset + self.opt.fineSize]\n        D = D[:, h_offset:h_offset + self.opt.fineSize, w_offset:w_offset + self.opt.fineSize]\n\n        A = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(A)\n        B = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(B)\n        C = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(C)\n        D = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(D)\n\n        if self.opt.which_direction == 'BtoA':\n            input_nc = self.opt.output_nc\n            output_nc = self.opt.input_nc\n        else:\n            input_nc = self.opt.input_nc\n            output_nc = self.opt.output_nc\n\n        if (not self.opt.no_flip) and random.random() < 0.5:\n            idx = [i for i in range(A.size(2) - 1, -1, -1)]\n            idx = torch.LongTensor(idx)\n            A = A.index_select(2, idx)\n            B = B.index_select(2, idx)\n\n        if input_nc == 1:  # RGB to gray\n            tmp = A[0, ...] * 0.299 + A[1, ...] * 0.587 + A[2, ...] * 0.114\n            A = tmp.unsqueeze(0)\n\n        if output_nc == 1:  # RGB to gray\n            tmp = B[0, ...] * 0.299 + B[1, ...] * 0.587 + B[2, ...] * 0.114\n            B = tmp.unsqueeze(0)\n\n        return {'A': A, 'B': B, 'C': C, 'D': D,\n                'A_paths': AB_path, 'B_paths': AB_path}\n\n    def __len__(self):\n        return len(self.AB_paths)\n\n    def name(self):\n        return 'AlignedDataset'\n"""
selectiongan_v1/data/base_data_loader.py,0,"b'class BaseDataLoader():\n    def __init__(self):\n        pass\n\n    def initialize(self, opt):\n        self.opt = opt\n        pass\n\n    def load_data():\n        return None\n'"
selectiongan_v1/data/base_dataset.py,1,"b'import torch.utils.data as data\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\n\nclass BaseDataset(data.Dataset):\n    def __init__(self):\n        super(BaseDataset, self).__init__()\n\n    def name(self):\n        return \'BaseDataset\'\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def initialize(self, opt):\n        pass\n\n    def __len__(self):\n        return 0\n\n\ndef get_transform(opt):\n    transform_list = []\n    if opt.resize_or_crop == \'resize_and_crop\':\n        osize = [opt.loadSize, opt.loadSize]\n        transform_list.append(transforms.Resize(osize, Image.BICUBIC))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == \'crop\':\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == \'scale_width\':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.fineSize)))\n    elif opt.resize_or_crop == \'scale_width_and_crop\':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.loadSize)))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == \'none\':\n        transform_list.append(transforms.Lambda(\n            lambda img: __adjust(img)))\n    else:\n        raise ValueError(\'--resize_or_crop %s is not a valid option.\' % opt.resize_or_crop)\n\n    if opt.isTrain and not opt.no_flip:\n        transform_list.append(transforms.RandomHorizontalFlip())\n\n    transform_list += [transforms.ToTensor(),\n                       transforms.Normalize((0.5, 0.5, 0.5),\n                                            (0.5, 0.5, 0.5))]\n    return transforms.Compose(transform_list)\n\n# just modify the width and height to be multiple of 4\ndef __adjust(img):\n    ow, oh = img.size\n\n    # the size needs to be a multiple of this number, \n    # because going through generator network may change img size\n    # and eventually cause size mismatch error\n    mult = 4 \n    if ow % mult == 0 and oh % mult == 0:\n        return img\n    w = (ow - 1) // mult\n    w = (w + 1) * mult\n    h = (oh - 1) // mult\n    h = (h + 1) * mult\n\n    if ow != w or oh != h:\n        __print_size_warning(ow, oh, w, h)\n        \n    return img.resize((w, h), Image.BICUBIC)\n\n\ndef __scale_width(img, target_width):\n    ow, oh = img.size\n    \n    # the size needs to be a multiple of this number, \n    # because going through generator network may change img size\n    # and eventually cause size mismatch error    \n    mult = 4\n    assert target_width % mult == 0, ""the target width needs to be multiple of %d."" % mult\n    if (ow == target_width and oh % mult == 0):\n        return img\n    w = target_width\n    target_height = int(target_width * oh / ow)\n    m = (target_height - 1) // mult\n    h = (m + 1) * mult\n\n    if target_height != h:\n        __print_size_warning(target_width, target_height, w, h)\n    \n    return img.resize((w, h), Image.BICUBIC)\n\n\ndef __print_size_warning(ow, oh, w, h):\n    if not hasattr(__print_size_warning, \'has_printed\'):\n        print(""The image size needs to be a multiple of 4. ""\n              ""The loaded image size was (%d, %d), so it was adjusted to ""\n              ""(%d, %d). This adjustment will be done to all images ""\n              ""whose sizes are not multiples of 4"" % (ow, oh, w, h))\n        __print_size_warning.has_printed = True\n\n\n'"
selectiongan_v1/data/image_folder.py,1,"b'###############################################################################\n# Code from\n# https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n# Modified the original code so that it also loads images from the current\n# directory as well as the subdirectories\n###############################################################################\n\nimport torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef make_dataset(dir):\n    images = []\n    assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n\n    for root, _, fnames in sorted(os.walk(dir)):\n        for fname in fnames:\n            if is_image_file(fname):\n                path = os.path.join(root, fname)\n                images.append(path)\n\n    return images\n\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\n\nclass ImageFolder(data.Dataset):\n\n    def __init__(self, root, transform=None, return_paths=False,\n                 loader=default_loader):\n        imgs = make_dataset(root)\n        if len(imgs) == 0:\n            raise(RuntimeError(""Found 0 images in: "" + root + ""\\n""\n                               ""Supported image extensions are: "" +\n                               "","".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.imgs = imgs\n        self.transform = transform\n        self.return_paths = return_paths\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.return_paths:\n            return img, path\n        else:\n            return img\n\n    def __len__(self):\n        return len(self.imgs)\n'"
selectiongan_v1/models/__init__.py,0,"b'import importlib\nfrom models.base_model import BaseModel\n\n\ndef find_model_using_name(model_name):\n    # Given the option --model [modelname],\n    # the file ""models/modelname_model.py""\n    # will be imported.\n    model_filename = ""models."" + model_name + ""_model""\n    modellib = importlib.import_module(model_filename)\n\n    # In the file, the class called ModelNameModel() will\n    # be instantiated. It has to be a subclass of BaseModel,\n    # and it is case-insensitive.\n    model = None\n    target_model_name = model_name.replace(\'_\', \'\') + \'model\'\n    for name, cls in modellib.__dict__.items():\n        if name.lower() == target_model_name.lower() \\\n           and issubclass(cls, BaseModel):\n            model = cls\n\n    if model is None:\n        print(""In %s.py, there should be a subclass of BaseModel with class name that matches %s in lowercase."" % (model_filename, target_model_name))\n        exit(0)\n\n    return model\n\n\ndef get_option_setter(model_name):\n    model_class = find_model_using_name(model_name)\n    return model_class.modify_commandline_options\n\n\ndef create_model(opt):\n    model = find_model_using_name(opt.model)\n    instance = model()\n    instance.initialize(opt)\n    print(""model [%s] was created"" % (instance.name()))\n    return instance\n'"
selectiongan_v1/models/base_model.py,8,"b""import os\nimport torch\nfrom collections import OrderedDict\nfrom . import networks\n\n\nclass BaseModel():\n\n    # modify parser to add command line options,\n    # and also change the default values if needed\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def name(self):\n        return 'BaseModel'\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.gpu_ids = opt.gpu_ids\n        self.isTrain = opt.isTrain\n        self.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')\n        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n        if opt.resize_or_crop != 'scale_width':\n            torch.backends.cudnn.benchmark = True\n        self.loss_names = []\n        self.model_names = []\n        self.visual_names = []\n        self.image_paths = []\n\n    def set_input(self, input):\n        self.input = input\n\n    def forward(self):\n        pass\n\n    # load and print networks; create schedulers\n    def setup(self, opt, parser=None):\n        if self.isTrain:\n            self.schedulers = [networks.get_scheduler(optimizer, opt) for optimizer in self.optimizers]\n\n        if not self.isTrain or opt.continue_train:\n            self.load_networks(opt.which_epoch)\n        self.print_networks(opt.verbose)\n\n    # make models eval mode during test time\n    def eval(self):\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, 'net' + name)\n                net.eval()\n\n    # used in test time, wrapping `forward` in no_grad() so we don't save\n    # intermediate steps for backprop\n    def test(self):\n        with torch.no_grad():\n            self.forward()\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n    def optimize_parameters(self):\n        pass\n\n    # update learning rate (called once every epoch)\n    def update_learning_rate(self):\n        for scheduler in self.schedulers:\n            scheduler.step()\n        lr = self.optimizers[0].param_groups[0]['lr']\n        print('learning rate = %.7f' % lr)\n\n    # return visualization images. train.py will display these images, and save the images to a html\n    def get_current_visuals(self):\n        visual_ret = OrderedDict()\n        for name in self.visual_names:\n            if isinstance(name, str):\n                visual_ret[name] = getattr(self, name)\n        return visual_ret\n\n    # return traning losses/errors. train.py will print out these errors as debugging information\n    def get_current_losses(self):\n        errors_ret = OrderedDict()\n        for name in self.loss_names:\n            if isinstance(name, str):\n                # float(...) works for both scalar tensor and float number\n                errors_ret[name] = float(getattr(self, 'loss_' + name))\n        return errors_ret\n\n    # save models to the disk\n    def save_networks(self, which_epoch):\n        for name in self.model_names:\n            if isinstance(name, str):\n                save_filename = '%s_net_%s.pth' % (which_epoch, name)\n                save_path = os.path.join(self.save_dir, save_filename)\n                net = getattr(self, 'net' + name)\n\n                if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n                    torch.save(net.module.cpu().state_dict(), save_path)\n                    net.cuda(self.gpu_ids[0])\n                else:\n                    torch.save(net.cpu().state_dict(), save_path)\n\n    def __patch_instance_norm_state_dict(self, state_dict, module, keys, i=0):\n        key = keys[i]\n        if i + 1 == len(keys):  # at the end, pointing to a parameter/buffer\n            if module.__class__.__name__.startswith('InstanceNorm') and \\\n                    (key == 'running_mean' or key == 'running_var'):\n                if getattr(module, key) is None:\n                    state_dict.pop('.'.join(keys))\n            if module.__class__.__name__.startswith('InstanceNorm') and \\\n                    (key == 'num_batches_tracked'):\n                state_dict.pop('.'.join(keys))\n        else:\n            self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)\n\n    # load models from the disk\n    def load_networks(self, which_epoch):\n        for name in self.model_names:\n            if isinstance(name, str):\n                load_filename = '%s_net_%s.pth' % (which_epoch, name)\n                load_path = os.path.join(self.save_dir, load_filename)\n                net = getattr(self, 'net' + name)\n                if isinstance(net, torch.nn.DataParallel):\n                    net = net.module\n                print('loading the model from %s' % load_path)\n                # if you are using PyTorch newer than 0.4 (e.g., built from\n                # GitHub source), you can remove str() on self.device\n                state_dict = torch.load(load_path, map_location=str(self.device))\n                if hasattr(state_dict, '_metadata'):\n                    del state_dict._metadata\n\n                # patch InstanceNorm checkpoints prior to 0.4\n                for key in list(state_dict.keys()):  # need to copy keys here because we mutate in loop\n                    self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))\n                net.load_state_dict(state_dict)\n\n    # print network information\n    def print_networks(self, verbose):\n        print('---------- Networks initialized -------------')\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, 'net' + name)\n                num_params = 0\n                for param in net.parameters():\n                    num_params += param.numel()\n                if verbose:\n                    print(net)\n                print('[Network %s] Total number of parameters : %.5f M' % (name, num_params / 1e6))\n        print('-----------------------------------------------')\n\n    # set requies_grad=Fasle to avoid computation\n    def set_requires_grad(self, nets, requires_grad=False):\n        if not isinstance(nets, list):\n            nets = [nets]\n        for net in nets:\n            if net is not None:\n                for param in net.parameters():\n                    param.requires_grad = requires_grad\n"""
selectiongan_v1/models/networks.py,17,"b""import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport functools\nfrom torch.optim import lr_scheduler\nimport torch.nn.functional as F\n\n\n###############################################################################\n# Helper Functions\n###############################################################################\n\n\ndef get_norm_layer(norm_type='instance'):\n    if norm_type == 'batch':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n    elif norm_type == 'instance':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True)\n    elif norm_type == 'none':\n        norm_layer = None\n    else:\n        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n    return norm_layer\n\n\ndef get_scheduler(optimizer, opt):\n    if opt.lr_policy == 'lambda':\n        def lambda_rule(epoch):\n            lr_l = 1.0 - max(0, epoch + 1 + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\n            return lr_l\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n    elif opt.lr_policy == 'step':\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n    elif opt.lr_policy == 'plateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n    else:\n        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n    return scheduler\n\n\ndef init_weights(net, init_type='normal', gain=0.02):\n    def init_func(m):\n        classname = m.__class__.__name__\n        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n            if init_type == 'normal':\n                init.normal_(m.weight.data, 0.0, gain)\n            elif init_type == 'xavier':\n                init.xavier_normal_(m.weight.data, gain=gain)\n            elif init_type == 'kaiming':\n                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n            elif init_type == 'orthogonal':\n                init.orthogonal_(m.weight.data, gain=gain)\n            else:\n                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n            if hasattr(m, 'bias') and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find('BatchNorm2d') != -1:\n            init.normal_(m.weight.data, 1.0, gain)\n            init.constant_(m.bias.data, 0.0)\n\n    print('initialize network with %s' % init_type)\n    net.apply(init_func)\n\n\ndef init_net(net, init_type='normal', init_gain=0.02, gpu_ids=[]):\n    if len(gpu_ids) > 0:\n        assert(torch.cuda.is_available())\n        net.to(gpu_ids[0])\n        net = torch.nn.DataParallel(net, gpu_ids)\n    init_weights(net, init_type, gain=init_gain)\n    return net\n\ndef define_G(input_nc, output_nc, ngf, which_model_netG, norm='batch', use_dropout=False, init_type='normal', init_gain=0.02, gpu_ids=[]):\n    netG = None\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if which_model_netG == 'resnet_9blocks':\n        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9)\n    elif which_model_netG == 'resnet_6blocks':\n        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6)\n    elif which_model_netG == 'unet_128':\n        netG = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n    elif which_model_netG == 'unet_256':\n        # change 8 to 4 if your image size is 64*64\n        netG = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n    else:\n        raise NotImplementedError('Generator model name [%s] is not recognized' % which_model_netG)\n    return init_net(netG, init_type, init_gain, gpu_ids)\n\n\n\ndef define_Ga(input_nc, output_nc, ngf, which_model_netG, norm='batch', use_dropout=False, init_type='normal', init_gain=0.02, gpu_ids=[]):\n    netG = None\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if which_model_netG == 'unet_128':\n        netG = UnetGenerator_a(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n    elif which_model_netG == 'unet_256':\n        netG = UnetGenerator_a(input_nc, output_nc)\n    else:\n        raise NotImplementedError('Generator model name [%s] is not recognized' % which_model_netG)\n    return init_net(netG, init_type, init_gain, gpu_ids)\n\n\ndef define_D(input_nc, ndf, which_model_netD,\n             n_layers_D=3, norm='batch', use_sigmoid=False, init_type='normal', init_gain=0.02, gpu_ids=[]):\n    netD = None\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if which_model_netD == 'basic':\n        netD = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n    elif which_model_netD == 'n_layers':\n        netD = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n    elif which_model_netD == 'pixel':\n        netD = PixelDiscriminator(input_nc, ndf, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n    else:\n        raise NotImplementedError('Discriminator model name [%s] is not recognized' %\n                                  which_model_netD)\n    return init_net(netD, init_type, init_gain, gpu_ids)\n\n\n##############################################################################\n# Classes\n##############################################################################\n\n\n# Defines the GAN loss which uses either LSGAN or the regular GAN.\n# When LSGAN is used, it is basically same as MSELoss,\n# but it abstracts away the need to create the target label tensor\n# that has the same size as the input\nclass GANLoss(nn.Module):\n    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0):\n        super(GANLoss, self).__init__()\n        self.register_buffer('real_label', torch.tensor(target_real_label))\n        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n        if use_lsgan:\n            self.loss = nn.MSELoss()\n        else:\n            self.loss = nn.BCELoss()\n\n    def get_target_tensor(self, input, target_is_real):\n        if target_is_real:\n            target_tensor = self.real_label\n        else:\n            target_tensor = self.fake_label\n        return target_tensor.expand_as(input)\n\n    def __call__(self, input, target_is_real):\n        target_tensor = self.get_target_tensor(input, target_is_real)\n        return self.loss(input, target_tensor)\n\n\n# Defines the generator that consists of Resnet blocks between a few\n# downsampling/upsampling operations.\n# Code and idea originally from Justin Johnson's architecture.\n# https://github.com/jcjohnson/fast-neural-style/\nclass ResnetGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type='reflect'):\n        assert(n_blocks >= 0)\n        super(ResnetGenerator, self).__init__()\n        self.input_nc = input_nc\n        self.output_nc = output_nc\n        self.ngf = ngf\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0,\n                           bias=use_bias),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n                                stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2**n_downsampling\n        for i in range(n_blocks):\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n\n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1,\n                                         bias=use_bias),\n                      norm_layer(int(ngf * mult / 2)),\n                      nn.ReLU(True)]\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        return self.model(input)\n\n\n# Define a resnet block\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        conv_block = []\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n                       norm_layer(dim),\n                       nn.ReLU(True)]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n                       norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n        return out\n\n\n# Defines the Unet generator.\n# |num_downs|: number of downsamplings in UNet. For example,\n# if |num_downs| == 7, image of size 128x128 will become of size 1x1\n# at the bottleneck\nclass UnetGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\n                 norm_layer=nn.BatchNorm2d, use_dropout=False):\n        super(UnetGenerator, self).__init__()\n\n        # construct unet structure\n        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)\n        for i in range(num_downs - 5):\n            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)\n\n        self.model = unet_block\n\n    def forward(self, input):\n        # reture image and feature\n        feature, image = self.model(input)\n        return feature, image\n\nclass UnetGenerator_a(nn.Module):\n    def __init__(self, input_nc, output_nc):\n        super(UnetGenerator_a, self).__init__()\n        self.deconvolution_1 = torch.nn.ConvTranspose2d(136, 104, kernel_size=2, stride=2)\n        self.pool1 = nn.AvgPool2d(kernel_size=(1, 1))\n        self.pool2 = nn.AvgPool2d(kernel_size=(4, 4))\n        self.pool3 = nn.AvgPool2d(kernel_size=(9, 9))\n\n        self.model_attention = nn.Conv2d(input_nc, output_nc, kernel_size=1, stride=1, padding=0, bias=nn.InstanceNorm2d)\n        self.model_image = nn.Conv2d(input_nc, output_nc*3, kernel_size=3, stride=1, padding=1, bias=nn.InstanceNorm2d)\n        self.conv330 = nn.Conv2d(330, 110, kernel_size=3, stride=1, padding=1, bias=nn.InstanceNorm2d)\n        self.conv440 = nn.Conv2d(440, 110, kernel_size=3, stride=1, padding=1, bias=nn.InstanceNorm2d)\n\n        self.convolution_for_attention = torch.nn.Conv2d(10, 1, 1, stride=1, padding=0)\n\n        self.dropout=nn.Dropout(p=0.5)\n        self.relu=nn.ReLU()\n        self.tanh = torch.nn.Tanh()\n    def forward(self, feature_combine, image_combine):\n\n        output_feature = self.relu(self.dropout(self.deconvolution_1(feature_combine))) # feature_combine: 136, output_feature: 104\n        feature_image_combine = torch.cat((output_feature, image_combine), 1) # image_combine: 6; feature_image_combine: 110\n\n        pool_feature1 = self.pool1(feature_image_combine)\n        pool_feature2 = self.pool2(feature_image_combine)\n        pool_feature3 = self.pool3(feature_image_combine)\n\n        pool_feature1_up = F.upsample(input=pool_feature1, size=(256, 256), mode='bilinear', align_corners=True)\n        pool_feature2_up = F.upsample(input=pool_feature2, size=(256, 256), mode='bilinear', align_corners=True)\n        pool_feature3_up = F.upsample(input=pool_feature3, size=(256, 256), mode='bilinear', align_corners=True)\n\n        f1 = feature_image_combine * pool_feature1_up\n        f2 = feature_image_combine * pool_feature2_up\n        f3 = feature_image_combine * pool_feature3_up\n\n        feature_image_combine = torch.cat((f1, f2, f3, feature_image_combine), 1) # feature_image_combine: 440\n        feature_image_combine = self.conv440(feature_image_combine) # feature_image_combine: 110\n\n        attention = self.model_attention(feature_image_combine) # attention: 10\n        image = self.model_image(feature_image_combine) # image: 30\n\n        softmax_ = torch.nn.Softmax(dim=1)\n        attention = softmax_(attention)\n\n        attention1_ = attention[:, 0:1, :, :]\n        attention2_ = attention[:, 1:2, :, :]\n        attention3_ = attention[:, 2:3, :, :]\n        attention4_ = attention[:, 3:4, :, :]\n        attention5_ = attention[:, 4:5, :, :]\n        attention6_ = attention[:, 5:6, :, :]\n        attention7_ = attention[:, 6:7, :, :]\n        attention8_ = attention[:, 7:8, :, :]\n        attention9_ = attention[:, 8:9, :, :]\n        attention10_ = attention[:, 9:10, :, :]\n\n        attention1 = attention1_.repeat(1, 3, 1, 1)\n        attention2 = attention2_.repeat(1, 3, 1, 1)\n        attention3 = attention3_.repeat(1, 3, 1, 1)\n        attention4 = attention4_.repeat(1, 3, 1, 1)\n        attention5 = attention5_.repeat(1, 3, 1, 1)\n        attention6 = attention6_.repeat(1, 3, 1, 1)\n        attention7 = attention7_.repeat(1, 3, 1, 1)\n        attention8 = attention8_.repeat(1, 3, 1, 1)\n        attention9 = attention9_.repeat(1, 3, 1, 1)\n        attention10 = attention10_.repeat(1, 3, 1, 1)\n\n        image = self.tanh(image)\n\n        image1 = image[:, 0:3, :, :]\n        image2 = image[:, 3:6, :, :]\n        image3 = image[:, 6:9, :, :]\n        image4 = image[:, 9:12, :, :]\n        image5 = image[:, 12:15, :, :]\n        image6 = image[:, 15:18, :, :]\n        image7 = image[:, 18:21, :, :]\n        image8 = image[:, 21:24, :, :]\n        image9 = image[:, 24:27, :, :]\n        image10 = image[:, 27:30, :, :]\n\n        output1 = image1 * attention1\n        output2 = image2 * attention2\n        output3 = image3 * attention3\n        output4 = image4 * attention4\n        output5 = image5 * attention5\n        output6 = image6 * attention6\n        output7 = image7 * attention7\n        output8 = image8 * attention8\n        output9 = image9 * attention9\n        output10 = image10 * attention10\n\n        output11 = output1 + output2 + output3 + output4 + output5 + output6 + output7 + output8 + output9 + output10\n\n        ##uncertainty map generation\n        sigmoid_ = torch.nn.Sigmoid()\n        uncertainty = self.convolution_for_attention(attention)\n\n        uncertainty = sigmoid_(uncertainty)\n        uncertainty_map = uncertainty.repeat(1, 3, 1, 1)\n\n        return image1, image2, image3, image4, image5, image6, image7, image8, image9, image10,\\\n               attention1_, attention2_, attention3_, attention4_, attention5_, attention6_, attention7_, attention8_, attention9_, attention10_, \\\n               output1, output2, output3, output4, output5, output6, output7, output8, output9, output10, \\\n               uncertainty_map, output11\n\n\nclass UnetSkipConnectionBlock(nn.Module):\n    def __init__(self, outer_nc, inner_nc, input_nc=None,\n                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n        super(UnetSkipConnectionBlock, self).__init__()\n        self.outermost = outermost\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n        if input_nc is None:\n            input_nc = outer_nc\n        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n                             stride=2, padding=1, bias=use_bias)\n        downrelu = nn.LeakyReLU(0.2, True)\n        downnorm = norm_layer(inner_nc)\n        uprelu = nn.ReLU(True)\n        upnorm = norm_layer(outer_nc)\n\n        if outermost:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1)\n            down = [downconv]\n            self.down = nn.Sequential(*down)\n            up = [upconv, nn.Tanh()]\n            self.up = nn.Sequential(*up)\n            self.submodule = submodule\n            self.uprelu = uprelu\n            self.use_dropout = use_dropout\n            model = [submodule]\n\n        elif innermost:\n            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1, bias=use_bias)\n            down = [downrelu, downconv]\n            up = [uprelu, upconv, upnorm]\n            model = down + up\n        else:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1, bias=use_bias)\n            down = [downrelu, downconv, downnorm]\n            up = [uprelu, upconv, upnorm]\n\n            if use_dropout:\n                model = down + [submodule] + up + [nn.Dropout(0.5)]\n            else:\n                model = down + [submodule] + up\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        # returen feature and image\n        if self.outermost:\n            x1 = self.down(x)\n            x2 = self.submodule(x1)\n            x3 = self.uprelu(x2)\n            x4 = self.up(x3)\n            if self.use_dropout:\n                x5 = nn.Dropout(x4, 0.5)\n                return x3, x5\n            return x3, x4\n        else:\n            return torch.cat([x, self.model(x)], 1)\n\nclass UnetSkipConnectionBlock_a(nn.Module):\n    def __init__(self, outer_nc, inner_nc, input_nc=None,\n                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n        super(UnetSkipConnectionBlock_a, self).__init__()\n        self.outermost = outermost\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n        if input_nc is None:\n            input_nc = outer_nc\n        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n                             stride=2, padding=1, bias=use_bias)\n        downrelu = nn.LeakyReLU(0.2, True)\n        downnorm = norm_layer(inner_nc)\n        uprelu = nn.ReLU(True)\n        upnorm = norm_layer(outer_nc)\n\n        if outermost:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1)\n            down = [downconv]\n            # up = [uprelu, upconv, nn.Tanh()]\n            up = [uprelu, upconv]\n            model = down + [submodule] + up\n        elif innermost:\n            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1, bias=use_bias)\n            down = [downrelu, downconv]\n            up = [uprelu, upconv, upnorm]\n            model = down + up\n        else:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1, bias=use_bias)\n            down = [downrelu, downconv, downnorm]\n            up = [uprelu, upconv, upnorm]\n\n            if use_dropout:\n                model = down + [submodule] + up + [nn.Dropout(0.5)]\n            else:\n                model = down + [submodule] + up\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        if self.outermost:\n            return self.model(x)\n        else:\n            return torch.cat([x, self.model(x)], 1)\n\n\n# Defines the PatchGAN discriminator with the specified arguments.\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\n        super(NLayerDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        kw = 4\n        padw = 1\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n\n        if use_sigmoid:\n            sequence += [nn.Sigmoid()]\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        return self.model(input)\n\n\nclass PixelDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\n        super(PixelDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        self.net = [\n            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n            norm_layer(ndf * 2),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\n\n        if use_sigmoid:\n            self.net.append(nn.Sigmoid())\n\n        self.net = nn.Sequential(*self.net)\n\n    def forward(self, input):\n        return self.net(input)\n"""
selectiongan_v1/models/selectiongan_model.py,17,"b""import torch\nfrom util.image_pool import ImagePool\nfrom .base_model import BaseModel\nfrom . import networks\nimport itertools\n\nclass SelectionGANModel(BaseModel):\n    def name(self):\n        return 'SelectionGANModel'\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        parser.set_defaults(pool_size=0, no_lsgan=True, norm='instance')\n        parser.set_defaults(dataset_mode='aligned')\n        parser.set_defaults(which_model_netG='unet_256')\n        parser.add_argument('--REGULARIZATION', type=float, default=1e-6)\n        if is_train:\n            parser.add_argument('--lambda_L1', type=float, default=100.0, help='weight for image L1 loss')\n            parser.add_argument('--lambda_L1_seg', type=float, default=1.0, help='weight for segmentaion L1 loss')\n        return parser\n\n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n        self.isTrain = opt.isTrain\n\n        # specify the training losses you want to print out. The program will call base_model.get_current_losses\n        self.loss_names = ['D_G', 'L1','G','D_real','D_fake', 'D_D']\n\n        # specify the images you want to save/display. The program will call base_model.get_current_visuals\n        if self.opt.saveDisk:\n            self.visual_names = ['real_A', 'fake_B', 'real_B','fake_D','real_D', 'A', 'I']\n        else:\n            self.visual_names = ['I1','I2','I3','I4','I5','I6','I7','I8','I9','I10','A1','A2','A3','A4','A5','A6','A7','A8','A9','A10',\n                             'O1','O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8', 'O9', 'O10',\n                             'real_A', 'fake_B', 'real_B','fake_D','real_D', 'A', 'I']\n\n        # specify the models you want to save to the disk. The program will call base_model.save_networks and base_model.load_networks\n        if self.isTrain:\n            self.model_names = ['Gi','Gs','Ga','D']\n        else:\n            self.model_names = ['Gi','Gs','Ga']\n        # load/define networks\n        self.netGi = networks.define_G(6, 3, opt.ngf,\n                                      opt.which_model_netG, opt.norm, not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n\n        self.netGs = networks.define_G(3, 3, 4,\n                                      opt.which_model_netG, opt.norm, not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n        # 10: the number of attention maps\n        self.netGa = networks.define_Ga(110, 10, opt.ngaf,\n                                        opt.which_model_netG, opt.norm, not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n\n        if self.isTrain:\n            use_sigmoid = opt.no_lsgan\n            self.netD = networks.define_D(6, opt.ndf,\n                                          opt.which_model_netD,\n                                          opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, opt.init_gain, self.gpu_ids)\n\n        if self.isTrain:\n            self.fake_AB_pool = ImagePool(opt.pool_size)\n            self.fake_DB_pool = ImagePool(opt.pool_size)\n            self.fake_D_pool = ImagePool(opt.pool_size)\n            # define loss functions\n            self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan).to(self.device)\n            self.criterionL1 = torch.nn.L1Loss()\n\n            # initialize optimizers\n            self.optimizers = []\n            self.optimizer_G = torch.optim.Adam(itertools.chain(self.netGi.parameters(), self.netGs.parameters(), self.netGa.parameters()),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizer_D = torch.optim.Adam(self.netD.parameters(),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizers.append(self.optimizer_G)\n            self.optimizers.append(self.optimizer_D)\n\n\n    def set_input(self, input):\n        AtoB = self.opt.which_direction == 'AtoB'\n        self.real_A = input['A' if AtoB else 'B'].to(self.device)\n        self.real_B = input['B' if AtoB else 'A'].to(self.device)\n        self.real_C = input['C'].to(self.device)\n        self.real_D = input['D'].to(self.device)\n        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n\n    def forward(self):\n        combine_AD=torch.cat((self.real_A, self.real_D), 1)\n        # self.fake_B: the first stage image result\n        self.Gi_feature, self.fake_B = self.netGi(combine_AD)\n        # self.fake_D: the first stage segmantation result\n        self.Gs_feature, self.fake_D = self.netGs(self.fake_B)\n\n        feature_combine=torch.cat((self.Gi_feature, self.Gs_feature), 1)\n        image_combine=torch.cat((self.real_A, self.fake_B), 1)\n\n        # self.I1-I10: intermediate image generations\n        # self.A1-A10: intermediate attention maps\n        # self.O1-O10: multiplication results of intermediate generations and attention maps\n        # self.A: uncertainty map\n        # self.I: the second image result\n        self.I1, self.I2, self.I3, self.I4, self.I5, self.I6, self.I7, self.I8, self.I9, self.I10,\\\n        self.A1, self.A2, self.A3, self.A4, self.A5, self.A6, self.A7, self.A8, self.A9, self.A10,\\\n        self.O1, self.O2, self.O3, self.O4, self.O5, self.O6, self.O7, self.O8, self.O9, self.O10,\\\n        self.A, self.I= self.netGa(feature_combine, image_combine)\n\n        # self.Is: the second segmentation reuslt\n        _, self.Is = self.netGs(self.I)\n\n\n    def backward_D(self):\n        # fake_B\n        fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1))\n        pred_D_fake_AB = self.netD(fake_AB.detach())\n        self.loss_pred_D_fake_AB = self.criterionGAN(pred_D_fake_AB, False)\n\n        # fake_I\n        fake_AI = self.fake_AB_pool.query(torch.cat((self.real_A, self.I), 1))\n        pred_D_fake_AI = self.netD(fake_AI.detach())\n        self.loss_pred_D_fake_AI = self.criterionGAN(pred_D_fake_AI, False)*4\n        self.loss_D_fake = self.loss_pred_D_fake_AB + self.loss_pred_D_fake_AI\n\n        # Real\n        real_AB = torch.cat((self.real_A, self.real_B), 1)\n        pred_real_AB = self.netD(real_AB)\n        self.loss_pred_real_AB = self.criterionGAN(pred_real_AB, True)\n\n        self.loss_D_real = 5 * self.loss_pred_real_AB\n\n        # Combined loss\n        self.loss_D_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n\n        self.loss_D_D.backward()\n\n    def backward_G(self):\n        # fake_B\n        fake_AB = torch.cat((self.real_A, self.fake_B), 1)\n        pred_D_fake_AB = self.netD(fake_AB)\n        self.loss_D_fake_AB = self.criterionGAN(pred_D_fake_AB, True)\n\n        # fake_I\n        fake_AI = torch.cat((self.real_A, self.I), 1)\n        pred_D_fake_AI = self.netD(fake_AI)\n        self.loss_D_fake_AI = self.criterionGAN(pred_D_fake_AI, True)*4\n\n        self.loss_D_G = self.loss_D_fake_AB + self.loss_D_fake_AI\n\n        ## uncertainty guided pixel loss\n        # fake_B\n        self.loss_L1_1 = torch.mean(torch.div(torch.abs(self.fake_B-self.real_B), self.A) + torch.log(self.A)) * self.opt.lambda_L1 + self.criterionL1(self.fake_B, self.real_B) * self.opt.lambda_L1\n        # I\n        self.loss_L1_2 = torch.mean(torch.div(torch.abs(self.I-self.real_B), self.A) + torch.log(self.A)) * self.opt.lambda_L1*2 + self.criterionL1(self.I, self.real_B) * self.opt.lambda_L1 *2\n\n        # fake_D\n        self.loss_L1_3 = torch.mean(torch.div(torch.abs(self.fake_D - self.real_D), self.A) + torch.log(self.A)) * self.opt.lambda_L1_seg + self.criterionL1(self.fake_D, self.real_D) * self.opt.lambda_L1_seg\n        # Is\n        self.loss_L1_4 = torch.mean(torch.div(torch.abs(self.Is - self.real_D), self.A) + torch.log(self.A)) * self.opt.lambda_L1_seg*2 + self.criterionL1(self.Is, self.real_D) * self.opt.lambda_L1_seg*2\n        \n        # Combined loss\n        self.loss_L1 = self.loss_L1_1 + self.loss_L1_2 + self.loss_L1_3 + self.loss_L1_4\n\n        ## tv loss\n        self.loss_reg = self.opt.REGULARIZATION * (\n                torch.sum(torch.abs(self.I[:, :, :, :-1] - self.I[:, :, :, 1:])) +\n                torch.sum(torch.abs(self.I[:, :, :-1, :] - self.I[:, :, 1:, :])))\n\n        self.loss_G = self.loss_D_G + self.loss_L1 + self.loss_reg\n\n        self.loss_G.backward()\n\n    def optimize_parameters(self):\n        self.forward()\n        # update D\n        self.set_requires_grad(self.netD, True)\n        self.optimizer_D.zero_grad()\n        self.backward_D()\n        self.optimizer_D.step()\n\n        # update G\n        self.set_requires_grad(self.netD, False)\n        self.optimizer_G.zero_grad()\n        self.backward_G()\n        self.optimizer_G.step()\n"""
selectiongan_v1/options/__init__.py,0,b''
selectiongan_v1/options/base_options.py,1,"b'import argparse\nimport os\nfrom util import util\nimport torch\nimport models\nimport data\n\n\nclass BaseOptions():\n    def __init__(self):\n        self.initialized = False\n\n    def initialize(self, parser):\n        parser.add_argument(\'--dataroot\', type=str, default=\'/home/csdept/projects/pytorch-CycleGAN-and-pix2pix_sg2/datasets/dayton\', help=\'path to images (should have subfolders trainA, trainB, valA, valB, etc)\')\n        parser.add_argument(\'--batchSize\', type=int, default=4, help=\'input batch size\')\n        parser.add_argument(\'--loadSize\', type=int, default=286, help=\'scale images to this size\')\n        parser.add_argument(\'--fineSize\', type=int, default=256, help=\'then crop to this size\')\n        parser.add_argument(\'--input_nc\', type=int, default=3, help=\'# of input image channels\')\n        parser.add_argument(\'--output_nc\', type=int, default=3, help=\'# of output image channels\')\n        parser.add_argument(\'--ngf\', type=int, default=64, help=\'# of gen filters in first conv layer\')\n        parser.add_argument(\'--ngaf\', type=int, default=4, help=\'# of gen filters in first conv layer\')\n        parser.add_argument(\'--ndf\', type=int, default=64, help=\'# of discrim filters in first conv layer\')\n        parser.add_argument(\'--which_model_netD\', type=str, default=\'basic\', help=\'selects model to use for netD\')\n        parser.add_argument(\'--which_model_netG\', type=str, default=\'unet_256\', help=\'selects model to use for netG\')\n        parser.add_argument(\'--n_layers_D\', type=int, default=3, help=\'only used if which_model_netD==n_layers\')\n        parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU\')\n        parser.add_argument(\'--name\', type=str, default=\'ego2top_x_sg2_3_rmadangeGs_4_1\', help=\'name of the experiment. It decides where to store samples and models\')\n        parser.add_argument(\'--dataset_mode\', type=str, default=\'aligned\', help=\'chooses how datasets are loaded. [unaligned | aligned | single]\')\n        parser.add_argument(\'--model\', type=str, default=\'pix2pix\',\n                            help=\'chooses which model to use. cycle_gan, pix2pix, test\')\n        parser.add_argument(\'--which_direction\', type=str, default=\'AtoB\', help=\'AtoB or BtoA\')\n        parser.add_argument(\'--nThreads\', default=4, type=int, help=\'# threads for loading data\')\n        parser.add_argument(\'--checkpoints_dir\', type=str, default=\'./checkpoints\', help=\'models are saved here\')\n        parser.add_argument(\'--norm\', type=str, default=\'batch\', help=\'instance normalization or batch normalization\')\n        parser.add_argument(\'--serial_batches\', action=\'store_true\', help=\'if true, takes images in order to make batches, otherwise takes them randomly\')\n        parser.add_argument(\'--display_winsize\', type=int, default=256, help=\'display window size\')\n        parser.add_argument(\'--display_id\', type=int, default=0, help=\'window id of the web display\')\n        parser.add_argument(\'--display_server\', type=str, default=""http://localhost"", help=\'visdom server of the web display\')\n        parser.add_argument(\'--display_env\', type=str, default=\'main\', help=\'visdom display environment name (default is ""main"")\')\n        parser.add_argument(\'--display_port\', type=int, default=8097, help=\'visdom port of the web display\')\n        parser.add_argument(\'--no_dropout\', action=\'store_true\', help=\'no dropout for the generator\')\n        parser.add_argument(\'--max_dataset_size\', type=int, default=float(""inf""), help=\'Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.\')\n        parser.add_argument(\'--resize_or_crop\', type=str, default=\'resize_and_crop\', help=\'scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop]\')\n        parser.add_argument(\'--no_flip\', action=\'store_true\', help=\'if specified, do not flip the images for data augmentation\')\n        parser.add_argument(\'--init_type\', type=str, default=\'normal\', help=\'network initialization [normal|xavier|kaiming|orthogonal]\')\n        parser.add_argument(\'--init_gain\', type=float, default=0.02, help=\'scaling factor for normal, xavier and orthogonal.\')\n        parser.add_argument(\'--verbose\', action=\'store_true\', help=\'if specified, print more debugging information\')\n        parser.add_argument(\'--suffix\', default=\'\', type=str, help=\'customized suffix: opt.name = opt.name + suffix: e.g., {model}_{which_model_netG}_size{loadSize}\')\n        parser.add_argument(\'--saveDisk\', action=\'store_true\', help=\'save disk memory during testing time\')\n        self.initialized = True\n        return parser\n\n    def gather_options(self):\n        # initialize parser with basic options\n        if not self.initialized:\n            parser = argparse.ArgumentParser(\n                formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n            parser = self.initialize(parser)\n\n        # get the basic options\n        opt, _ = parser.parse_known_args()\n\n        # modify model-related parser options\n        model_name = opt.model\n        model_option_setter = models.get_option_setter(model_name)\n        parser = model_option_setter(parser, self.isTrain)\n        opt, _ = parser.parse_known_args()  # parse again with the new defaults\n\n        # modify dataset-related parser options\n        dataset_name = opt.dataset_mode\n        dataset_option_setter = data.get_option_setter(dataset_name)\n        parser = dataset_option_setter(parser, self.isTrain)\n\n        self.parser = parser\n\n        return parser.parse_args()\n\n    def print_options(self, opt):\n        message = \'\'\n        message += \'----------------- Options ---------------\\n\'\n        for k, v in sorted(vars(opt).items()):\n            comment = \'\'\n            default = self.parser.get_default(k)\n            if v != default:\n                comment = \'\\t[default: %s]\' % str(default)\n            message += \'{:>25}: {:<30}{}\\n\'.format(str(k), str(v), comment)\n        message += \'----------------- End -------------------\'\n        print(message)\n\n        # save to the disk\n        expr_dir = os.path.join(opt.checkpoints_dir, opt.name)\n        util.mkdirs(expr_dir)\n        file_name = os.path.join(expr_dir, \'opt.txt\')\n        with open(file_name, \'wt\') as opt_file:\n            opt_file.write(message)\n            opt_file.write(\'\\n\')\n\n    def parse(self):\n\n        opt = self.gather_options()\n        opt.isTrain = self.isTrain   # train or test\n\n        # process opt.suffix\n        if opt.suffix:\n            suffix = (\'_\' + opt.suffix.format(**vars(opt))) if opt.suffix != \'\' else \'\'\n            opt.name = opt.name + suffix\n\n        self.print_options(opt)\n\n        # set gpu ids\n        str_ids = opt.gpu_ids.split(\',\')\n        opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                opt.gpu_ids.append(id)\n        if len(opt.gpu_ids) > 0:\n            torch.cuda.set_device(opt.gpu_ids[0])\n\n        self.opt = opt\n        return self.opt\n'"
selectiongan_v1/options/test_options.py,0,"b'from .base_options import BaseOptions\n\n\nclass TestOptions(BaseOptions):\n    def initialize(self, parser):\n        parser = BaseOptions.initialize(self, parser)\n        parser.add_argument(\'--ntest\', type=int, default=float(""inf""), help=\'# of test examples.\')\n        parser.add_argument(\'--results_dir\', type=str, default=\'./results/\', help=\'saves results here.\')\n        parser.add_argument(\'--aspect_ratio\', type=float, default=1.0, help=\'aspect ratio of result images\')\n        parser.add_argument(\'--phase\', type=str, default=\'test\', help=\'train, val, test, etc\')\n        parser.add_argument(\'--which_epoch\', type=str, default=\'latest\', help=\'which epoch to load? set to latest to use latest cached model\')\n        #  Dropout and Batchnorm has different behavioir during training and test.\n        parser.add_argument(\'--eval\', action=\'store_true\', help=\'use eval mode during test time.\')\n        parser.add_argument(\'--how_many\', type=int, default=5, help=\'how many test images to run\')\n\n        parser.set_defaults(model=\'pix2pix\')\n        # To avoid cropping, the loadSize should be the same as fineSize\n        parser.set_defaults(loadSize=parser.get_default(\'fineSize\'))\n        \n        self.isTrain = False\n        return parser\n'"
selectiongan_v1/options/train_options.py,0,"b""from .base_options import BaseOptions\n\n\nclass TrainOptions(BaseOptions):\n    def initialize(self, parser):\n        parser = BaseOptions.initialize(self, parser)\n        parser.add_argument('--display_freq', type=int, default=10, help='frequency of showing training results on screen')\n        parser.add_argument('--display_ncols', type=int, default=7, help='if positive, display all images in a single visdom web panel with certain number of images per row.')\n        parser.add_argument('--update_html_freq', type=int, default=100, help='frequency of saving training results to html')\n        parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n        parser.add_argument('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')\n        parser.add_argument('--save_epoch_freq', type=int, default=1, help='frequency of saving checkpoints at the end of epochs')\n        parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n        parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n        parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n        parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n        parser.add_argument('--niter', type=int, default=20, help='# of iter at starting learning rate')\n        parser.add_argument('--niter_decay', type=int, default=15, help='# of iter to linearly decay learning rate to zero')\n        parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n        parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n        parser.add_argument('--no_lsgan', action='store_true', help='do *not* use least square GAN, if false, use vanilla GAN')\n        parser.add_argument('--pool_size', type=int, default=50, help='the size of image buffer that stores previously generated images')\n        parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n        parser.add_argument('--lr_policy', type=str, default='lambda', help='learning rate policy: lambda|step|plateau')\n        parser.add_argument('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations')\n\n        self.isTrain = True\n        return parser\n"""
selectiongan_v1/util/__init__.py,0,b''
selectiongan_v1/util/get_data.py,0,"b'from __future__ import print_function\nimport os\nimport tarfile\nimport requests\nfrom warnings import warn\nfrom zipfile import ZipFile\nfrom bs4 import BeautifulSoup\nfrom os.path import abspath, isdir, join, basename\n\n\nclass GetData(object):\n    """"""\n\n    Download CycleGAN or Pix2Pix Data.\n\n    Args:\n        technique : str\n            One of: \'cyclegan\' or \'pix2pix\'.\n        verbose : bool\n            If True, print additional information.\n\n    Examples:\n        >>> from util.get_data import GetData\n        >>> gd = GetData(technique=\'cyclegan\')\n        >>> new_data_path = gd.get(save_path=\'./datasets\')  # options will be displayed.\n\n    """"""\n\n    def __init__(self, technique=\'cyclegan\', verbose=True):\n        url_dict = {\n            \'pix2pix\': \'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets\',\n            \'cyclegan\': \'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets\'\n        }\n        self.url = url_dict.get(technique.lower())\n        self._verbose = verbose\n\n    def _print(self, text):\n        if self._verbose:\n            print(text)\n\n    @staticmethod\n    def _get_options(r):\n        soup = BeautifulSoup(r.text, \'lxml\')\n        options = [h.text for h in soup.find_all(\'a\', href=True)\n                   if h.text.endswith((\'.zip\', \'tar.gz\'))]\n        return options\n\n    def _present_options(self):\n        r = requests.get(self.url)\n        options = self._get_options(r)\n        print(\'Options:\\n\')\n        for i, o in enumerate(options):\n            print(""{0}: {1}"".format(i, o))\n        choice = input(""\\nPlease enter the number of the ""\n                       ""dataset above you wish to download:"")\n        return options[int(choice)]\n\n    def _download_data(self, dataset_url, save_path):\n        if not isdir(save_path):\n            os.makedirs(save_path)\n\n        base = basename(dataset_url)\n        temp_save_path = join(save_path, base)\n\n        with open(temp_save_path, ""wb"") as f:\n            r = requests.get(dataset_url)\n            f.write(r.content)\n\n        if base.endswith(\'.tar.gz\'):\n            obj = tarfile.open(temp_save_path)\n        elif base.endswith(\'.zip\'):\n            obj = ZipFile(temp_save_path, \'r\')\n        else:\n            raise ValueError(""Unknown File Type: {0}."".format(base))\n\n        self._print(""Unpacking Data..."")\n        obj.extractall(save_path)\n        obj.close()\n        os.remove(temp_save_path)\n\n    def get(self, save_path, dataset=None):\n        """"""\n\n        Download a dataset.\n\n        Args:\n            save_path : str\n                A directory to save the data to.\n            dataset : str, optional\n                A specific dataset to download.\n                Note: this must include the file extension.\n                If None, options will be presented for you\n                to choose from.\n\n        Returns:\n            save_path_full : str\n                The absolute path to the downloaded data.\n\n        """"""\n        if dataset is None:\n            selected_dataset = self._present_options()\n        else:\n            selected_dataset = dataset\n\n        save_path_full = join(save_path, selected_dataset.split(\'.\')[0])\n\n        if isdir(save_path_full):\n            warn(""\\n\'{0}\' already exists. Voiding Download."".format(\n                save_path_full))\n        else:\n            self._print(\'Downloading Data...\')\n            url = ""{0}/{1}"".format(self.url, selected_dataset)\n            self._download_data(url, save_path=save_path)\n\n        return abspath(save_path_full)\n'"
selectiongan_v1/util/html.py,0,"b'import dominate\nfrom dominate.tags import *\nimport os\n\n\nclass HTML:\n    def __init__(self, web_dir, title, reflesh=0):\n        self.title = title\n        self.web_dir = web_dir\n        self.img_dir = os.path.join(self.web_dir, \'images\')\n        if not os.path.exists(self.web_dir):\n            os.makedirs(self.web_dir)\n        if not os.path.exists(self.img_dir):\n            os.makedirs(self.img_dir)\n        # print(self.img_dir)\n\n        self.doc = dominate.document(title=title)\n        if reflesh > 0:\n            with self.doc.head:\n                meta(http_equiv=""reflesh"", content=str(reflesh))\n\n    def get_image_dir(self):\n        return self.img_dir\n\n    def add_header(self, str):\n        with self.doc:\n            h3(str)\n\n    def add_table(self, border=1):\n        self.t = table(border=border, style=""table-layout: fixed;"")\n        self.doc.add(self.t)\n\n    def add_images(self, ims, txts, links, width=400):\n        self.add_table()\n        with self.t:\n            with tr():\n                for im, txt, link in zip(ims, txts, links):\n                    with td(style=""word-wrap: break-word;"", halign=""center"", valign=""top""):\n                        with p():\n                            with a(href=os.path.join(\'images\', link)):\n                                img(style=""width:%dpx"" % width, src=os.path.join(\'images\', im))\n                            br()\n                            p(txt)\n\n    def save(self):\n        html_file = \'%s/index.html\' % self.web_dir\n        f = open(html_file, \'wt\')\n        f.write(self.doc.render())\n        f.close()\n\n\nif __name__ == \'__main__\':\n    html = HTML(\'web/\', \'test_html\')\n    html.add_header(\'hello world\')\n\n    ims = []\n    txts = []\n    links = []\n    for n in range(4):\n        ims.append(\'image_%d.png\' % n)\n        txts.append(\'text_%d\' % n)\n        links.append(\'image_%d.png\' % n)\n    html.add_images(ims, txts, links)\n    html.save()\n'"
selectiongan_v1/util/image_pool.py,2,"b'import random\nimport torch\n\n\nclass ImagePool():\n    def __init__(self, pool_size):\n        self.pool_size = pool_size\n        if self.pool_size > 0:\n            self.num_imgs = 0\n            self.images = []\n\n    def query(self, images):\n        if self.pool_size == 0:\n            return images\n        return_images = []\n        for image in images:\n            image = torch.unsqueeze(image.data, 0)\n            if self.num_imgs < self.pool_size:\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n                return_images.append(image)\n            else:\n                p = random.uniform(0, 1)\n                if p > 0.5:\n                    random_id = random.randint(0, self.pool_size - 1)  # randint is inclusive\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image\n                    return_images.append(tmp)\n                else:\n                    return_images.append(image)\n        return_images = torch.cat(return_images, 0)\n        return return_images\n'"
selectiongan_v1/util/util.py,2,"b""from __future__ import print_function\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport os\n\n\n# Converts a Tensor into an image array (numpy)\n# |imtype|: the desired type of the converted numpy array\ndef tensor2im(input_image, imtype=np.uint8):\n    if isinstance(input_image, torch.Tensor):\n        image_tensor = input_image.data\n    else:\n        return input_image\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    if image_numpy.shape[0] == 1:\n        image_numpy = np.tile(image_numpy, (3, 1, 1))\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n    return image_numpy.astype(imtype)\n\n\ndef diagnose_network(net, name='network'):\n    mean = 0.0\n    count = 0\n    for param in net.parameters():\n        if param.grad is not None:\n            mean += torch.mean(torch.abs(param.grad.data))\n            count += 1\n    if count > 0:\n        mean = mean / count\n    print(name)\n    print(mean)\n\n\ndef save_image(image_numpy, image_path):\n    image_pil = Image.fromarray(image_numpy)\n    image_pil.save(image_path)\n\n\ndef print_numpy(x, val=True, shp=False):\n    x = x.astype(np.float64)\n    if shp:\n        print('shape,', x.shape)\n    if val:\n        x = x.flatten()\n        print('mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f' % (\n            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n"""
selectiongan_v1/util/visualizer.py,0,"b'import numpy as np\nimport os\nimport ntpath\nimport time\nfrom . import util\nfrom . import html\nfrom scipy.misc import imresize\n\n\n# save image to the disk\ndef save_images(webpage, visuals, image_path, aspect_ratio=1.0, width=256):\n    image_dir = webpage.get_image_dir()\n    short_path = ntpath.basename(image_path[0])\n    name = os.path.splitext(short_path)[0]\n\n    webpage.add_header(name)\n    ims, txts, links = [], [], []\n\n    for label, im_data in visuals.items():\n        im = util.tensor2im(im_data)\n        image_name = \'%s_%s.png\' % (name, label)\n        save_path = os.path.join(image_dir, image_name)\n        h, w, _ = im.shape\n        if aspect_ratio > 1.0:\n            im = imresize(im, (h, int(w * aspect_ratio)), interp=\'bicubic\')\n        if aspect_ratio < 1.0:\n            im = imresize(im, (int(h / aspect_ratio), w), interp=\'bicubic\')\n        util.save_image(im, save_path)\n\n        ims.append(image_name)\n        txts.append(label)\n        links.append(image_name)\n    webpage.add_images(ims, txts, links, width=width)\n\n\nclass Visualizer():\n    def __init__(self, opt):\n        self.display_id = opt.display_id\n        self.use_html = opt.isTrain and not opt.no_html\n        self.win_size = opt.display_winsize\n        self.name = opt.name\n        self.opt = opt\n        self.saved = False\n        if self.display_id > 0:\n            import visdom\n            self.ncols = opt.display_ncols\n            self.vis = visdom.Visdom(server=opt.display_server, port=opt.display_port, env=opt.display_env, raise_exceptions=True, use_incoming_socket=False)\n\n        if self.use_html:\n            self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, \'web\')\n            self.img_dir = os.path.join(self.web_dir, \'images\')\n            print(\'create web directory %s...\' % self.web_dir)\n            util.mkdirs([self.web_dir, self.img_dir])\n        self.log_name = os.path.join(opt.checkpoints_dir, opt.name, \'loss_log.txt\')\n        with open(self.log_name, ""a"") as log_file:\n            now = time.strftime(""%c"")\n            log_file.write(\'================ Training Loss (%s) ================\\n\' % now)\n\n    def reset(self):\n        self.saved = False\n\n    def throw_visdom_connection_error(self):\n        print(\'\\n\\nCould not connect to Visdom server (https://github.com/facebookresearch/visdom) for displaying training progress.\\nYou can suppress connection to Visdom using the option --display_id -1. To install visdom, run \\n$ pip install visdom\\n, and start the server by \\n$ python -m visdom.server.\\n\\n\')\n        exit(1)\n\n    # |visuals|: dictionary of images to display or save\n    def display_current_results(self, visuals, epoch, save_result):\n        if self.display_id > 0:  # show images in the browser\n            ncols = self.ncols\n            if ncols > 0:\n                ncols = min(ncols, len(visuals))\n                h, w = next(iter(visuals.values())).shape[:2]\n                table_css = """"""<style>\n                        table {border-collapse: separate; border-spacing:4px; white-space:nowrap; text-align:center}\n                        table td {width: %dpx; height: %dpx; padding: 4px; outline: 4px solid black}\n                        </style>"""""" % (w, h)\n                title = self.name\n                label_html = \'\'\n                label_html_row = \'\'\n                images = []\n                idx = 0\n                for label, image in visuals.items():\n                    image_numpy = util.tensor2im(image)\n                    label_html_row += \'<td>%s</td>\' % label\n                    images.append(image_numpy.transpose([2, 0, 1]))\n                    idx += 1\n                    if idx % ncols == 0:\n                        label_html += \'<tr>%s</tr>\' % label_html_row\n                        label_html_row = \'\'\n                white_image = np.ones_like(image_numpy.transpose([2, 0, 1])) * 255\n                while idx % ncols != 0:\n                    images.append(white_image)\n                    label_html_row += \'<td></td>\'\n                    idx += 1\n                if label_html_row != \'\':\n                    label_html += \'<tr>%s</tr>\' % label_html_row\n                # pane col = image row\n                try:\n                    self.vis.images(images, nrow=ncols, win=self.display_id + 1,\n                                    padding=2, opts=dict(title=title + \' images\'))\n                    label_html = \'<table>%s</table>\' % label_html\n                    self.vis.text(table_css + label_html, win=self.display_id + 2,\n                                  opts=dict(title=title + \' labels\'))\n                except ConnectionError:\n                    self.throw_visdom_connection_error()\n\n            else:\n                idx = 1\n                for label, image in visuals.items():\n                    image_numpy = util.tensor2im(image)\n                    self.vis.image(image_numpy.transpose([2, 0, 1]), opts=dict(title=label),\n                                   win=self.display_id + idx)\n                    idx += 1\n\n        if self.use_html and (save_result or not self.saved):  # save images to a html file\n            self.saved = True\n            for label, image in visuals.items():\n                image_numpy = util.tensor2im(image)\n                img_path = os.path.join(self.img_dir, \'epoch%.3d_%s.png\' % (epoch, label))\n                util.save_image(image_numpy, img_path)\n            # update website\n            webpage = html.HTML(self.web_dir, \'Experiment name = %s\' % self.name, reflesh=1)\n            for n in range(epoch, 0, -1):\n                webpage.add_header(\'epoch [%d]\' % n)\n                ims, txts, links = [], [], []\n\n                for label, image_numpy in visuals.items():\n                    image_numpy = util.tensor2im(image)\n                    img_path = \'epoch%.3d_%s.png\' % (n, label)\n                    ims.append(img_path)\n                    txts.append(label)\n                    links.append(img_path)\n                webpage.add_images(ims, txts, links, width=self.win_size)\n            webpage.save()\n\n    # losses: dictionary of error labels and values\n    def plot_current_losses(self, epoch, counter_ratio, opt, losses):\n        if not hasattr(self, \'plot_data\'):\n            self.plot_data = {\'X\': [], \'Y\': [], \'legend\': list(losses.keys())}\n        self.plot_data[\'X\'].append(epoch + counter_ratio)\n        self.plot_data[\'Y\'].append([losses[k] for k in self.plot_data[\'legend\']])\n        try:\n            self.vis.line(\n                X=np.stack([np.array(self.plot_data[\'X\'])] * len(self.plot_data[\'legend\']), 1),\n                Y=np.array(self.plot_data[\'Y\']),\n                opts={\n                    \'title\': self.name + \' loss over time\',\n                    \'legend\': self.plot_data[\'legend\'],\n                    \'xlabel\': \'epoch\',\n                    \'ylabel\': \'loss\'},\n                win=self.display_id)\n        except ConnectionError:\n            self.throw_visdom_connection_error()\n\n    # losses: same format as |losses| of plot_current_losses\n    def print_current_losses(self, epoch, i, losses, t, t_data):\n        message = \'(epoch: %d, iters: %d, time: %.3f, data: %.3f) \' % (epoch, i, t, t_data)\n        for k, v in losses.items():\n            message += \'%s: %.3f \' % (k, v)\n\n        print(message)\n        with open(self.log_name, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n'"
selectiongan_v2/data/__init__.py,2,"b'import importlib\nimport torch.utils.data\nfrom data.base_data_loader import BaseDataLoader\nfrom data.base_dataset import BaseDataset\n\ndef find_dataset_using_name(dataset_name):\n    # Given the option --dataset_mode [datasetname],\n    # the file ""data/datasetname_dataset.py""\n    # will be imported.\n    dataset_filename = ""data."" + dataset_name + ""_dataset""\n    datasetlib = importlib.import_module(dataset_filename)\n\n    # In the file, the class called DatasetNameDataset() will\n    # be instantiated. It has to be a subclass of BaseDataset,\n    # and it is case-insensitive.\n    dataset = None\n    target_dataset_name = dataset_name.replace(\'_\', \'\') + \'dataset\'\n    for name, cls in datasetlib.__dict__.items():\n        if name.lower() == target_dataset_name.lower() \\\n           and issubclass(cls, BaseDataset):\n            dataset = cls\n            \n    if dataset is None:\n        print(""In %s.py, there should be a subclass of BaseDataset with class name that matches %s in lowercase."" % (dataset_filename, target_dataset_name))\n        exit(0)\n\n    return dataset\n\n\ndef get_option_setter(dataset_name):    \n    dataset_class = find_dataset_using_name(dataset_name)\n    return dataset_class.modify_commandline_options\n\n\ndef create_dataset(opt):\n    dataset = find_dataset_using_name(opt.dataset_mode)\n    instance = dataset()\n    instance.initialize(opt)\n    print(""dataset [%s] was created"" % (instance.name()))\n    return instance\n\n\ndef CreateDataLoader(opt):\n    data_loader = CustomDatasetDataLoader()\n    data_loader.initialize(opt)\n    return data_loader\n\n\n## Wrapper class of Dataset class that performs\n## multi-threaded data loading\nclass CustomDatasetDataLoader(BaseDataLoader):\n    def name(self):\n        return \'CustomDatasetDataLoader\'\n\n    def initialize(self, opt):\n        BaseDataLoader.initialize(self, opt)\n        self.dataset = create_dataset(opt)\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=opt.batchSize,\n            shuffle=not opt.serial_batches,\n            num_workers=int(opt.nThreads))\n\n    def load_data(self):\n        return self\n\n    def __len__(self):\n        return min(len(self.dataset), self.opt.max_dataset_size)\n\n    def __iter__(self):\n        for i, data in enumerate(self.dataloader):\n            if i * self.opt.batchSize >= self.opt.max_dataset_size:\n                break\n            yield data\n'"
selectiongan_v2/data/aligned_dataset.py,1,"b""import os.path\nimport random\nimport torchvision.transforms as transforms\nimport torch\nfrom data.base_dataset import BaseDataset\nfrom data.image_folder import make_dataset\nfrom PIL import Image\n\n\nclass AlignedDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_AB = os.path.join(opt.dataroot, opt.phase)\n        self.AB_paths = sorted(make_dataset(self.dir_AB))\n        assert(opt.resize_or_crop == 'resize_and_crop')\n\n    def __getitem__(self, index):\n        AB_path = self.AB_paths[index]\n        ABCD = Image.open(AB_path).convert('RGB')\n        w, h = ABCD.size\n        w2 = int(w / 4)\n        A = ABCD.crop((0, 0, w2, h)).resize((self.opt.loadSize, self.opt.loadSize), Image.BICUBIC)\n        B = ABCD.crop((w2, 0, w2+w2, h)).resize((self.opt.loadSize, self.opt.loadSize), Image.BICUBIC)\n        C = ABCD.crop((w2+w2, 0, w2+w2+w2, h)).resize((self.opt.loadSize, self.opt.loadSize), Image.BICUBIC)\n        D = ABCD.crop((w2+w2+w2, 0, w, h)).resize((self.opt.loadSize, self.opt.loadSize), Image.BICUBIC)\n\n        A = transforms.ToTensor()(A)\n        B = transforms.ToTensor()(B)\n        C = transforms.ToTensor()(C)\n        D = transforms.ToTensor()(D)\n        w_offset = random.randint(0, max(0, self.opt.loadSize - self.opt.fineSize - 1))\n        h_offset = random.randint(0, max(0, self.opt.loadSize - self.opt.fineSize - 1))\n\n        A = A[:, h_offset:h_offset + self.opt.fineSize, w_offset:w_offset + self.opt.fineSize]\n        B = B[:, h_offset:h_offset + self.opt.fineSize, w_offset:w_offset + self.opt.fineSize]\n        C = C[:, h_offset:h_offset + self.opt.fineSize, w_offset:w_offset + self.opt.fineSize]\n        D = D[:, h_offset:h_offset + self.opt.fineSize, w_offset:w_offset + self.opt.fineSize]\n\n        A = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(A)\n        B = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(B)\n        C = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(C)\n        D = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(D)\n\n        if self.opt.which_direction == 'BtoA':\n            input_nc = self.opt.output_nc\n            output_nc = self.opt.input_nc\n        else:\n            input_nc = self.opt.input_nc\n            output_nc = self.opt.output_nc\n\n        if (not self.opt.no_flip) and random.random() < 0.5:\n            idx = [i for i in range(A.size(2) - 1, -1, -1)]\n            idx = torch.LongTensor(idx)\n            A = A.index_select(2, idx)\n            B = B.index_select(2, idx)\n\n        if input_nc == 1:  # RGB to gray\n            tmp = A[0, ...] * 0.299 + A[1, ...] * 0.587 + A[2, ...] * 0.114\n            A = tmp.unsqueeze(0)\n\n        if output_nc == 1:  # RGB to gray\n            tmp = B[0, ...] * 0.299 + B[1, ...] * 0.587 + B[2, ...] * 0.114\n            B = tmp.unsqueeze(0)\n\n        return {'A': A, 'B': B, 'C': C, 'D': D,\n                'A_paths': AB_path, 'B_paths': AB_path}\n\n    def __len__(self):\n        return len(self.AB_paths)\n\n    def name(self):\n        return 'AlignedDataset'\n"""
selectiongan_v2/data/base_data_loader.py,0,"b'class BaseDataLoader():\n    def __init__(self):\n        pass\n\n    def initialize(self, opt):\n        self.opt = opt\n        pass\n\n    def load_data():\n        return None\n'"
selectiongan_v2/data/base_dataset.py,1,"b'import torch.utils.data as data\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\n\nclass BaseDataset(data.Dataset):\n    def __init__(self):\n        super(BaseDataset, self).__init__()\n\n    def name(self):\n        return \'BaseDataset\'\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def initialize(self, opt):\n        pass\n\n    def __len__(self):\n        return 0\n\n\ndef get_transform(opt):\n    transform_list = []\n    if opt.resize_or_crop == \'resize_and_crop\':\n        osize = [opt.loadSize, opt.loadSize]\n        transform_list.append(transforms.Resize(osize, Image.BICUBIC))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == \'crop\':\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == \'scale_width\':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.fineSize)))\n    elif opt.resize_or_crop == \'scale_width_and_crop\':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.loadSize)))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == \'none\':\n        transform_list.append(transforms.Lambda(\n            lambda img: __adjust(img)))\n    else:\n        raise ValueError(\'--resize_or_crop %s is not a valid option.\' % opt.resize_or_crop)\n\n    if opt.isTrain and not opt.no_flip:\n        transform_list.append(transforms.RandomHorizontalFlip())\n\n    transform_list += [transforms.ToTensor(),\n                       transforms.Normalize((0.5, 0.5, 0.5),\n                                            (0.5, 0.5, 0.5))]\n    return transforms.Compose(transform_list)\n\n# just modify the width and height to be multiple of 4\ndef __adjust(img):\n    ow, oh = img.size\n\n    # the size needs to be a multiple of this number, \n    # because going through generator network may change img size\n    # and eventually cause size mismatch error\n    mult = 4 \n    if ow % mult == 0 and oh % mult == 0:\n        return img\n    w = (ow - 1) // mult\n    w = (w + 1) * mult\n    h = (oh - 1) // mult\n    h = (h + 1) * mult\n\n    if ow != w or oh != h:\n        __print_size_warning(ow, oh, w, h)\n        \n    return img.resize((w, h), Image.BICUBIC)\n\n\ndef __scale_width(img, target_width):\n    ow, oh = img.size\n    \n    # the size needs to be a multiple of this number, \n    # because going through generator network may change img size\n    # and eventually cause size mismatch error    \n    mult = 4\n    assert target_width % mult == 0, ""the target width needs to be multiple of %d."" % mult\n    if (ow == target_width and oh % mult == 0):\n        return img\n    w = target_width\n    target_height = int(target_width * oh / ow)\n    m = (target_height - 1) // mult\n    h = (m + 1) * mult\n\n    if target_height != h:\n        __print_size_warning(target_width, target_height, w, h)\n    \n    return img.resize((w, h), Image.BICUBIC)\n\n\ndef __print_size_warning(ow, oh, w, h):\n    if not hasattr(__print_size_warning, \'has_printed\'):\n        print(""The image size needs to be a multiple of 4. ""\n              ""The loaded image size was (%d, %d), so it was adjusted to ""\n              ""(%d, %d). This adjustment will be done to all images ""\n              ""whose sizes are not multiples of 4"" % (ow, oh, w, h))\n        __print_size_warning.has_printed = True\n\n\n'"
selectiongan_v2/data/image_folder.py,1,"b'###############################################################################\n# Code from\n# https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n# Modified the original code so that it also loads images from the current\n# directory as well as the subdirectories\n###############################################################################\n\nimport torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef make_dataset(dir):\n    images = []\n    assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n\n    for root, _, fnames in sorted(os.walk(dir)):\n        for fname in fnames:\n            if is_image_file(fname):\n                path = os.path.join(root, fname)\n                images.append(path)\n\n    return images\n\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\n\nclass ImageFolder(data.Dataset):\n\n    def __init__(self, root, transform=None, return_paths=False,\n                 loader=default_loader):\n        imgs = make_dataset(root)\n        if len(imgs) == 0:\n            raise(RuntimeError(""Found 0 images in: "" + root + ""\\n""\n                               ""Supported image extensions are: "" +\n                               "","".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.imgs = imgs\n        self.transform = transform\n        self.return_paths = return_paths\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.return_paths:\n            return img, path\n        else:\n            return img\n\n    def __len__(self):\n        return len(self.imgs)\n'"
selectiongan_v2/models/__init__.py,0,"b'import importlib\nfrom models.base_model import BaseModel\n\n\ndef find_model_using_name(model_name):\n    # Given the option --model [modelname],\n    # the file ""models/modelname_model.py""\n    # will be imported.\n    model_filename = ""models."" + model_name + ""_model""\n    modellib = importlib.import_module(model_filename)\n\n    # In the file, the class called ModelNameModel() will\n    # be instantiated. It has to be a subclass of BaseModel,\n    # and it is case-insensitive.\n    model = None\n    target_model_name = model_name.replace(\'_\', \'\') + \'model\'\n    for name, cls in modellib.__dict__.items():\n        if name.lower() == target_model_name.lower() \\\n           and issubclass(cls, BaseModel):\n            model = cls\n\n    if model is None:\n        print(""In %s.py, there should be a subclass of BaseModel with class name that matches %s in lowercase."" % (model_filename, target_model_name))\n        exit(0)\n\n    return model\n\n\ndef get_option_setter(model_name):\n    model_class = find_model_using_name(model_name)\n    return model_class.modify_commandline_options\n\n\ndef create_model(opt):\n    model = find_model_using_name(opt.model)\n    instance = model()\n    instance.initialize(opt)\n    print(""model [%s] was created"" % (instance.name()))\n    return instance\n'"
selectiongan_v2/models/base_model.py,8,"b""import os\nimport torch\nfrom collections import OrderedDict\nfrom . import networks\n\n\nclass BaseModel():\n\n    # modify parser to add command line options,\n    # and also change the default values if needed\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def name(self):\n        return 'BaseModel'\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.gpu_ids = opt.gpu_ids\n        self.isTrain = opt.isTrain\n        self.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')\n        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n        if opt.resize_or_crop != 'scale_width':\n            torch.backends.cudnn.benchmark = True\n        self.loss_names = []\n        self.model_names = []\n        self.visual_names = []\n        self.image_paths = []\n\n    def set_input(self, input):\n        self.input = input\n\n    def forward(self):\n        pass\n\n    # load and print networks; create schedulers\n    def setup(self, opt, parser=None):\n        if self.isTrain:\n            self.schedulers = [networks.get_scheduler(optimizer, opt) for optimizer in self.optimizers]\n\n        if not self.isTrain or opt.continue_train:\n            self.load_networks(opt.which_epoch)\n        self.print_networks(opt.verbose)\n\n    # make models eval mode during test time\n    def eval(self):\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, 'net' + name)\n                net.eval()\n\n    # used in test time, wrapping `forward` in no_grad() so we don't save\n    # intermediate steps for backprop\n    def test(self):\n        with torch.no_grad():\n            self.forward()\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n    def optimize_parameters(self):\n        pass\n\n    # update learning rate (called once every epoch)\n    def update_learning_rate(self):\n        for scheduler in self.schedulers:\n            scheduler.step()\n        lr = self.optimizers[0].param_groups[0]['lr']\n        print('learning rate = %.7f' % lr)\n\n    # return visualization images. train.py will display these images, and save the images to a html\n    def get_current_visuals(self):\n        visual_ret = OrderedDict()\n        for name in self.visual_names:\n            if isinstance(name, str):\n                visual_ret[name] = getattr(self, name)\n        return visual_ret\n\n    # return traning losses/errors. train.py will print out these errors as debugging information\n    def get_current_losses(self):\n        errors_ret = OrderedDict()\n        for name in self.loss_names:\n            if isinstance(name, str):\n                # float(...) works for both scalar tensor and float number\n                errors_ret[name] = float(getattr(self, 'loss_' + name))\n        return errors_ret\n\n    # save models to the disk\n    def save_networks(self, which_epoch):\n        for name in self.model_names:\n            if isinstance(name, str):\n                save_filename = '%s_net_%s.pth' % (which_epoch, name)\n                save_path = os.path.join(self.save_dir, save_filename)\n                net = getattr(self, 'net' + name)\n\n                if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n                    torch.save(net.module.cpu().state_dict(), save_path)\n                    net.cuda(self.gpu_ids[0])\n                else:\n                    torch.save(net.cpu().state_dict(), save_path)\n\n    def __patch_instance_norm_state_dict(self, state_dict, module, keys, i=0):\n        key = keys[i]\n        if i + 1 == len(keys):  # at the end, pointing to a parameter/buffer\n            if module.__class__.__name__.startswith('InstanceNorm') and \\\n                    (key == 'running_mean' or key == 'running_var'):\n                if getattr(module, key) is None:\n                    state_dict.pop('.'.join(keys))\n            if module.__class__.__name__.startswith('InstanceNorm') and \\\n                    (key == 'num_batches_tracked'):\n                state_dict.pop('.'.join(keys))\n        else:\n            self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)\n\n    # load models from the disk\n    def load_networks(self, which_epoch):\n        for name in self.model_names:\n            if isinstance(name, str):\n                load_filename = '%s_net_%s.pth' % (which_epoch, name)\n                load_path = os.path.join(self.save_dir, load_filename)\n                net = getattr(self, 'net' + name)\n                if isinstance(net, torch.nn.DataParallel):\n                    net = net.module\n                print('loading the model from %s' % load_path)\n                # if you are using PyTorch newer than 0.4 (e.g., built from\n                # GitHub source), you can remove str() on self.device\n                state_dict = torch.load(load_path, map_location=str(self.device))\n                if hasattr(state_dict, '_metadata'):\n                    del state_dict._metadata\n\n                # patch InstanceNorm checkpoints prior to 0.4\n                for key in list(state_dict.keys()):  # need to copy keys here because we mutate in loop\n                    self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))\n                net.load_state_dict(state_dict)\n\n    # print network information\n    def print_networks(self, verbose):\n        print('---------- Networks initialized -------------')\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, 'net' + name)\n                num_params = 0\n                for param in net.parameters():\n                    num_params += param.numel()\n                if verbose:\n                    print(net)\n                print('[Network %s] Total number of parameters : %.5f M' % (name, num_params / 1e6))\n        print('-----------------------------------------------')\n\n    # set requies_grad=Fasle to avoid computation\n    def set_requires_grad(self, nets, requires_grad=False):\n        if not isinstance(nets, list):\n            nets = [nets]\n        for net in nets:\n            if net is not None:\n                for param in net.parameters():\n                    param.requires_grad = requires_grad\n"""
selectiongan_v2/models/networks.py,22,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport functools\nfrom torch.optim import lr_scheduler\nimport torch.nn.functional as F\nfrom torch.nn import Module, Parameter, Softmax\n\n\n###############################################################################\n# Helper Functions\n###############################################################################\n\n\ndef get_norm_layer(norm_type=\'instance\'):\n    if norm_type == \'batch\':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n    elif norm_type == \'instance\':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True)\n    elif norm_type == \'none\':\n        norm_layer = None\n    else:\n        raise NotImplementedError(\'normalization layer [%s] is not found\' % norm_type)\n    return norm_layer\n\n\ndef get_scheduler(optimizer, opt):\n    if opt.lr_policy == \'lambda\':\n        def lambda_rule(epoch):\n            lr_l = 1.0 - max(0, epoch + 1 + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\n            return lr_l\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n    elif opt.lr_policy == \'step\':\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n    elif opt.lr_policy == \'plateau\':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\'min\', factor=0.2, threshold=0.01, patience=5)\n    else:\n        return NotImplementedError(\'learning rate policy [%s] is not implemented\', opt.lr_policy)\n    return scheduler\n\n\ndef init_weights(net, init_type=\'normal\', gain=0.02):\n    def init_func(m):\n        classname = m.__class__.__name__\n        if hasattr(m, \'weight\') and (classname.find(\'Conv\') != -1 or classname.find(\'Linear\') != -1):\n            if init_type == \'normal\':\n                init.normal_(m.weight.data, 0.0, gain)\n            elif init_type == \'xavier\':\n                init.xavier_normal_(m.weight.data, gain=gain)\n            elif init_type == \'kaiming\':\n                init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\')\n            elif init_type == \'orthogonal\':\n                init.orthogonal_(m.weight.data, gain=gain)\n            else:\n                raise NotImplementedError(\'initialization method [%s] is not implemented\' % init_type)\n            if hasattr(m, \'bias\') and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find(\'BatchNorm2d\') != -1:\n            init.normal_(m.weight.data, 1.0, gain)\n            init.constant_(m.bias.data, 0.0)\n\n    print(\'initialize network with %s\' % init_type)\n    net.apply(init_func)\n\n\ndef init_net(net, init_type=\'normal\', init_gain=0.02, gpu_ids=[]):\n    if len(gpu_ids) > 0:\n        assert(torch.cuda.is_available())\n        net.to(gpu_ids[0])\n        net = torch.nn.DataParallel(net, gpu_ids)\n    init_weights(net, init_type, gain=init_gain)\n    return net\n\ndef define_G(input_nc, output_nc, ngf, which_model_netG, norm=\'batch\', use_dropout=False, init_type=\'normal\', init_gain=0.02, gpu_ids=[]):\n    netG = None\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if which_model_netG == \'resnet_9blocks\':\n        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9)\n    elif which_model_netG == \'resnet_6blocks\':\n        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6)\n    elif which_model_netG == \'unet_128\':\n        netG = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n    elif which_model_netG == \'unet_256\':\n        # change 8 to 4 if your image size is 64*64\n        netG = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n    else:\n        raise NotImplementedError(\'Generator model name [%s] is not recognized\' % which_model_netG)\n    return init_net(netG, init_type, init_gain, gpu_ids)\n\n\n\ndef define_Ga(input_nc, output_nc, ngf, which_model_netG, norm=\'batch\', use_dropout=False, init_type=\'normal\', init_gain=0.02, gpu_ids=[]):\n    netG = None\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if which_model_netG == \'unet_128\':\n        netG = UnetGenerator_a(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n    elif which_model_netG == \'unet_256\':\n        netG = UnetGenerator_a(input_nc, output_nc)\n    else:\n        raise NotImplementedError(\'Generator model name [%s] is not recognized\' % which_model_netG)\n    return init_net(netG, init_type, init_gain, gpu_ids)\n\n\ndef define_D(input_nc, ndf, which_model_netD,\n             n_layers_D=3, norm=\'batch\', use_sigmoid=False, init_type=\'normal\', init_gain=0.02, gpu_ids=[]):\n    netD = None\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if which_model_netD == \'basic\':\n        netD = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n    elif which_model_netD == \'n_layers\':\n        netD = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n    elif which_model_netD == \'pixel\':\n        netD = PixelDiscriminator(input_nc, ndf, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n    else:\n        raise NotImplementedError(\'Discriminator model name [%s] is not recognized\' %\n                                  which_model_netD)\n    return init_net(netD, init_type, init_gain, gpu_ids)\n\n\n##############################################################################\n# Classes\n##############################################################################\n\n\n# Defines the GAN loss which uses either LSGAN or the regular GAN.\n# When LSGAN is used, it is basically same as MSELoss,\n# but it abstracts away the need to create the target label tensor\n# that has the same size as the input\nclass GANLoss(nn.Module):\n    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0):\n        super(GANLoss, self).__init__()\n        self.register_buffer(\'real_label\', torch.tensor(target_real_label))\n        self.register_buffer(\'fake_label\', torch.tensor(target_fake_label))\n        if use_lsgan:\n            self.loss = nn.MSELoss()\n        else:\n            self.loss = nn.BCELoss()\n\n    def get_target_tensor(self, input, target_is_real):\n        if target_is_real:\n            target_tensor = self.real_label\n        else:\n            target_tensor = self.fake_label\n        return target_tensor.expand_as(input)\n\n    def __call__(self, input, target_is_real):\n        target_tensor = self.get_target_tensor(input, target_is_real)\n        return self.loss(input, target_tensor)\n\n\n# Defines the generator that consists of Resnet blocks between a few\n# downsampling/upsampling operations.\n# Code and idea originally from Justin Johnson\'s architecture.\n# https://github.com/jcjohnson/fast-neural-style/\nclass ResnetGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type=\'reflect\'):\n        assert(n_blocks >= 0)\n        super(ResnetGenerator, self).__init__()\n        self.input_nc = input_nc\n        self.output_nc = output_nc\n        self.ngf = ngf\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0,\n                           bias=use_bias),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n                                stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2**n_downsampling\n        for i in range(n_blocks):\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n\n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1,\n                                         bias=use_bias),\n                      norm_layer(int(ngf * mult / 2)),\n                      nn.ReLU(True)]\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        return self.model(input)\n\n\n# Define a resnet block\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        conv_block = []\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n                       norm_layer(dim),\n                       nn.ReLU(True)]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n                       norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n        return out\n\n\n# Defines the Unet generator.\n# |num_downs|: number of downsamplings in UNet. For example,\n# if |num_downs| == 7, image of size 128x128 will become of size 1x1\n# at the bottleneck\nclass UnetGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\n                 norm_layer=nn.BatchNorm2d, use_dropout=False):\n        super(UnetGenerator, self).__init__()\n\n        # construct unet structure\n        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)\n        for i in range(num_downs - 5):\n            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)\n\n        self.model = unet_block\n\n    def forward(self, input):\n        # reture image and feature\n        feature, image = self.model(input)\n        return feature, image\n\nclass UnetGenerator_a(nn.Module):\n    def __init__(self, input_nc, output_nc):\n        super(UnetGenerator_a, self).__init__()\n        self.deconvolution_1 = torch.nn.ConvTranspose2d(136, 104, kernel_size=2, stride=2)\n        self.pool1 = nn.AvgPool2d(kernel_size=(1, 1))\n        self.pool2 = nn.AvgPool2d(kernel_size=(4, 4))\n        self.pool3 = nn.AvgPool2d(kernel_size=(9, 9))\n\n        self.model_attention = nn.Conv2d(input_nc, output_nc, kernel_size=1, stride=1, padding=0, bias=nn.InstanceNorm2d)\n        self.model_image = nn.Conv2d(input_nc, output_nc*3, kernel_size=3, stride=1, padding=1, bias=nn.InstanceNorm2d)\n        self.conv330 = nn.Conv2d(330, 110, kernel_size=3, stride=1, padding=1, bias=nn.InstanceNorm2d)\n        self.conv440 = nn.Conv2d(440, 110, kernel_size=3, stride=1, padding=1, bias=nn.InstanceNorm2d)\n\n        self.convolution_for_attention = torch.nn.Conv2d(10, 1, 1, stride=1, padding=0)\n        self.sc = CAM_Module(440)\n        self.dropout=nn.Dropout(p=0.5)\n        self.relu=nn.ReLU()\n        self.tanh = torch.nn.Tanh()\n    def forward(self, feature_combine, image_combine):\n\n        output_feature = self.relu(self.dropout(self.deconvolution_1(feature_combine))) # feature_combine: 136, output_feature: 104\n        feature_image_combine = torch.cat((output_feature, image_combine), 1) # image_combine: 6; feature_image_combine: 110\n\n        pool_feature1 = self.pool1(feature_image_combine)\n        pool_feature2 = self.pool2(feature_image_combine)\n        pool_feature3 = self.pool3(feature_image_combine)\n\n        pool_feature1_up = F.upsample(input=pool_feature1, size=(256, 256), mode=\'bilinear\', align_corners=True)\n        pool_feature2_up = F.upsample(input=pool_feature2, size=(256, 256), mode=\'bilinear\', align_corners=True)\n        pool_feature3_up = F.upsample(input=pool_feature3, size=(256, 256), mode=\'bilinear\', align_corners=True)\n\n        f1 = feature_image_combine * pool_feature1_up\n        f2 = feature_image_combine * pool_feature2_up\n        f3 = feature_image_combine * pool_feature3_up\n\n        feature_image_combine = torch.cat((f1, f2, f3, feature_image_combine), 1) # feature_image_combine: 440\n        feature_image_combine = self.sc(feature_image_combine)  # 110\n        feature_image_combine = self.conv440(feature_image_combine) # feature_image_combine: 110\n\n        attention = self.model_attention(feature_image_combine) # attention: 10\n        image = self.model_image(feature_image_combine) # image: 30\n\n        softmax_ = torch.nn.Softmax(dim=1)\n        attention = softmax_(attention)\n\n        attention1_ = attention[:, 0:1, :, :]\n        attention2_ = attention[:, 1:2, :, :]\n        attention3_ = attention[:, 2:3, :, :]\n        attention4_ = attention[:, 3:4, :, :]\n        attention5_ = attention[:, 4:5, :, :]\n        attention6_ = attention[:, 5:6, :, :]\n        attention7_ = attention[:, 6:7, :, :]\n        attention8_ = attention[:, 7:8, :, :]\n        attention9_ = attention[:, 8:9, :, :]\n        attention10_ = attention[:, 9:10, :, :]\n\n        attention1 = attention1_.repeat(1, 3, 1, 1)\n        attention2 = attention2_.repeat(1, 3, 1, 1)\n        attention3 = attention3_.repeat(1, 3, 1, 1)\n        attention4 = attention4_.repeat(1, 3, 1, 1)\n        attention5 = attention5_.repeat(1, 3, 1, 1)\n        attention6 = attention6_.repeat(1, 3, 1, 1)\n        attention7 = attention7_.repeat(1, 3, 1, 1)\n        attention8 = attention8_.repeat(1, 3, 1, 1)\n        attention9 = attention9_.repeat(1, 3, 1, 1)\n        attention10 = attention10_.repeat(1, 3, 1, 1)\n\n        image = self.tanh(image)\n\n        image1 = image[:, 0:3, :, :]\n        image2 = image[:, 3:6, :, :]\n        image3 = image[:, 6:9, :, :]\n        image4 = image[:, 9:12, :, :]\n        image5 = image[:, 12:15, :, :]\n        image6 = image[:, 15:18, :, :]\n        image7 = image[:, 18:21, :, :]\n        image8 = image[:, 21:24, :, :]\n        image9 = image[:, 24:27, :, :]\n        image10 = image[:, 27:30, :, :]\n\n        output1 = image1 * attention1\n        output2 = image2 * attention2\n        output3 = image3 * attention3\n        output4 = image4 * attention4\n        output5 = image5 * attention5\n        output6 = image6 * attention6\n        output7 = image7 * attention7\n        output8 = image8 * attention8\n        output9 = image9 * attention9\n        output10 = image10 * attention10\n\n        output11 = output1 + output2 + output3 + output4 + output5 + output6 + output7 + output8 + output9 + output10\n\n        ##uncertainty map generation\n        sigmoid_ = torch.nn.Sigmoid()\n        uncertainty = self.convolution_for_attention(attention)\n\n        uncertainty = sigmoid_(uncertainty)\n        uncertainty_map = uncertainty.repeat(1, 3, 1, 1)\n\n        return image1, image2, image3, image4, image5, image6, image7, image8, image9, image10,\\\n               attention1_, attention2_, attention3_, attention4_, attention5_, attention6_, attention7_, attention8_, attention9_, attention10_, \\\n               output1, output2, output3, output4, output5, output6, output7, output8, output9, output10, \\\n               uncertainty_map, output11\n\n\nclass UnetSkipConnectionBlock(nn.Module):\n    def __init__(self, outer_nc, inner_nc, input_nc=None,\n                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n        super(UnetSkipConnectionBlock, self).__init__()\n        self.outermost = outermost\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n        if input_nc is None:\n            input_nc = outer_nc\n        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n                             stride=2, padding=1, bias=use_bias)\n        downrelu = nn.LeakyReLU(0.2, True)\n        downnorm = norm_layer(inner_nc)\n        uprelu = nn.ReLU(True)\n        upnorm = norm_layer(outer_nc)\n\n        if outermost:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1)\n            down = [downconv]\n            self.down = nn.Sequential(*down)\n            up = [upconv, nn.Tanh()]\n            self.up = nn.Sequential(*up)\n            self.submodule = submodule\n            self.uprelu = uprelu\n            self.use_dropout = use_dropout\n            model = [submodule]\n\n        elif innermost:\n            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1, bias=use_bias)\n            down = [downrelu, downconv]\n            up = [uprelu, upconv, upnorm]\n            model = down + up\n        else:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1, bias=use_bias)\n            down = [downrelu, downconv, downnorm]\n            up = [uprelu, upconv, upnorm]\n\n            if use_dropout:\n                model = down + [submodule] + up + [nn.Dropout(0.5)]\n            else:\n                model = down + [submodule] + up\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        # returen feature and image\n        if self.outermost:\n            x1 = self.down(x)\n            x2 = self.submodule(x1)\n            x3 = self.uprelu(x2)\n            x4 = self.up(x3)\n            if self.use_dropout:\n                x5 = nn.Dropout(x4, 0.5)\n                return x3, x5\n            return x3, x4\n        else:\n            return torch.cat([x, self.model(x)], 1)\n\nclass UnetSkipConnectionBlock_a(nn.Module):\n    def __init__(self, outer_nc, inner_nc, input_nc=None,\n                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n        super(UnetSkipConnectionBlock_a, self).__init__()\n        self.outermost = outermost\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n        if input_nc is None:\n            input_nc = outer_nc\n        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n                             stride=2, padding=1, bias=use_bias)\n        downrelu = nn.LeakyReLU(0.2, True)\n        downnorm = norm_layer(inner_nc)\n        uprelu = nn.ReLU(True)\n        upnorm = norm_layer(outer_nc)\n\n        if outermost:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1)\n            down = [downconv]\n            # up = [uprelu, upconv, nn.Tanh()]\n            up = [uprelu, upconv]\n            model = down + [submodule] + up\n        elif innermost:\n            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1, bias=use_bias)\n            down = [downrelu, downconv]\n            up = [uprelu, upconv, upnorm]\n            model = down + up\n        else:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1, bias=use_bias)\n            down = [downrelu, downconv, downnorm]\n            up = [uprelu, upconv, upnorm]\n\n            if use_dropout:\n                model = down + [submodule] + up + [nn.Dropout(0.5)]\n            else:\n                model = down + [submodule] + up\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        if self.outermost:\n            return self.model(x)\n        else:\n            return torch.cat([x, self.model(x)], 1)\n\n\n# Defines the PatchGAN discriminator with the specified arguments.\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\n        super(NLayerDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        kw = 4\n        padw = 1\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n\n        if use_sigmoid:\n            sequence += [nn.Sigmoid()]\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        return self.model(input)\n\n\nclass PixelDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\n        super(PixelDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        self.net = [\n            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n            norm_layer(ndf * 2),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\n\n        if use_sigmoid:\n            self.net.append(nn.Sigmoid())\n\n        self.net = nn.Sequential(*self.net)\n\n    def forward(self, input):\n        return self.net(input)\n\nclass CAM_Module(Module):\n    """""" Channel attention module""""""\n    def __init__(self, in_dim):\n        super(CAM_Module, self).__init__()\n        self.chanel_in = in_dim\n\n        self.gamma = Parameter(torch.zeros(1))\n        self.softmax  = Softmax(dim=-1)\n    def forward(self,x):\n        m_batchsize, C, height, width = x.size()\n        query = x.view(m_batchsize, C, -1)\n        key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n        energy = torch.bmm(query, key)\n        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy\n        attention = self.softmax(energy_new)\n        value = x.view(m_batchsize, C, -1)\n\n        out = torch.bmm(attention, value)\n        out = out.view(m_batchsize, C, height, width)\n\n        out = self.gamma*out + x\n        return out\n'"
selectiongan_v2/models/selectiongan_model.py,17,"b""import torch\nfrom util.image_pool import ImagePool\nfrom .base_model import BaseModel\nfrom . import networks\nimport itertools\n\nclass SelectionGANModel(BaseModel):\n    def name(self):\n        return 'SelectionGANModel'\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        parser.set_defaults(pool_size=0, no_lsgan=True, norm='instance')\n        parser.set_defaults(dataset_mode='aligned')\n        parser.set_defaults(which_model_netG='unet_256')\n        parser.add_argument('--REGULARIZATION', type=float, default=1e-6)\n        if is_train:\n            parser.add_argument('--lambda_L1', type=float, default=100.0, help='weight for image L1 loss')\n            parser.add_argument('--lambda_L1_seg', type=float, default=1.0, help='weight for segmentaion L1 loss')\n        return parser\n\n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n        self.isTrain = opt.isTrain\n\n        # specify the training losses you want to print out. The program will call base_model.get_current_losses\n        self.loss_names = ['D_G', 'L1','G','D_real','D_fake', 'D_D']\n\n        # specify the images you want to save/display. The program will call base_model.get_current_visuals\n        if self.opt.saveDisk:\n            self.visual_names = ['real_A', 'fake_B', 'real_B','fake_D','real_D', 'A', 'I']\n        else:\n            self.visual_names = ['I1','I2','I3','I4','I5','I6','I7','I8','I9','I10','A1','A2','A3','A4','A5','A6','A7','A8','A9','A10',\n                             'O1','O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8', 'O9', 'O10',\n                             'real_A', 'fake_B', 'real_B','fake_D','real_D', 'A', 'I']\n\n        # specify the models you want to save to the disk. The program will call base_model.save_networks and base_model.load_networks\n        if self.isTrain:\n            self.model_names = ['Gi','Gs','Ga','D']\n        else:\n            self.model_names = ['Gi','Gs','Ga']\n        # load/define networks\n        self.netGi = networks.define_G(6, 3, opt.ngf,\n                                      opt.which_model_netG, opt.norm, not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n\n        self.netGs = networks.define_G(3, 3, 4,\n                                      opt.which_model_netG, opt.norm, not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n        # 10: the number of attention maps\n        self.netGa = networks.define_Ga(110, 10, opt.ngaf,\n                                        opt.which_model_netG, opt.norm, not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n\n        if self.isTrain:\n            use_sigmoid = opt.no_lsgan\n            self.netD = networks.define_D(6, opt.ndf,\n                                          opt.which_model_netD,\n                                          opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, opt.init_gain, self.gpu_ids)\n\n        if self.isTrain:\n            self.fake_AB_pool = ImagePool(opt.pool_size)\n            self.fake_DB_pool = ImagePool(opt.pool_size)\n            self.fake_D_pool = ImagePool(opt.pool_size)\n            # define loss functions\n            self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan).to(self.device)\n            self.criterionL1 = torch.nn.L1Loss()\n\n            # initialize optimizers\n            self.optimizers = []\n            self.optimizer_G = torch.optim.Adam(itertools.chain(self.netGi.parameters(), self.netGs.parameters(), self.netGa.parameters()),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizer_D = torch.optim.Adam(self.netD.parameters(),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizers.append(self.optimizer_G)\n            self.optimizers.append(self.optimizer_D)\n\n\n    def set_input(self, input):\n        AtoB = self.opt.which_direction == 'AtoB'\n        self.real_A = input['A' if AtoB else 'B'].to(self.device)\n        self.real_B = input['B' if AtoB else 'A'].to(self.device)\n        self.real_C = input['C'].to(self.device)\n        self.real_D = input['D'].to(self.device)\n        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n\n    def forward(self):\n        combine_AD=torch.cat((self.real_A, self.real_D), 1)\n        # self.fake_B: the first stage image result\n        self.Gi_feature, self.fake_B = self.netGi(combine_AD)\n        # self.fake_D: the first stage segmantation result\n        self.Gs_feature, self.fake_D = self.netGs(self.fake_B)\n\n        feature_combine=torch.cat((self.Gi_feature, self.Gs_feature), 1)\n        image_combine=torch.cat((self.real_A, self.fake_B), 1)\n\n        # self.I1-I10: intermediate image generations\n        # self.A1-A10: intermediate attention maps\n        # self.O1-O10: multiplication results of intermediate generations and attention maps\n        # self.A: uncertainty map\n        # self.I: the second image result\n        self.I1, self.I2, self.I3, self.I4, self.I5, self.I6, self.I7, self.I8, self.I9, self.I10,\\\n        self.A1, self.A2, self.A3, self.A4, self.A5, self.A6, self.A7, self.A8, self.A9, self.A10,\\\n        self.O1, self.O2, self.O3, self.O4, self.O5, self.O6, self.O7, self.O8, self.O9, self.O10,\\\n        self.A, self.I= self.netGa(feature_combine, image_combine)\n\n        # self.Is: the second segmentation reuslt\n        _, self.Is = self.netGs(self.I)\n\n\n    def backward_D(self):\n        # fake_B\n        fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1))\n        pred_D_fake_AB = self.netD(fake_AB.detach())\n        self.loss_pred_D_fake_AB = self.criterionGAN(pred_D_fake_AB, False)\n\n        # fake_I\n        fake_AI = self.fake_AB_pool.query(torch.cat((self.real_A, self.I), 1))\n        pred_D_fake_AI = self.netD(fake_AI.detach())\n        self.loss_pred_D_fake_AI = self.criterionGAN(pred_D_fake_AI, False)*4\n        self.loss_D_fake = self.loss_pred_D_fake_AB + self.loss_pred_D_fake_AI\n\n        # Real\n        real_AB = torch.cat((self.real_A, self.real_B), 1)\n        pred_real_AB = self.netD(real_AB)\n        self.loss_pred_real_AB = self.criterionGAN(pred_real_AB, True)\n\n        self.loss_D_real = 5 * self.loss_pred_real_AB\n\n        # Combined loss\n        self.loss_D_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n\n        self.loss_D_D.backward()\n\n    def backward_G(self):\n        # fake_B\n        fake_AB = torch.cat((self.real_A, self.fake_B), 1)\n        pred_D_fake_AB = self.netD(fake_AB)\n        self.loss_D_fake_AB = self.criterionGAN(pred_D_fake_AB, True)\n\n        # fake_I\n        fake_AI = torch.cat((self.real_A, self.I), 1)\n        pred_D_fake_AI = self.netD(fake_AI)\n        self.loss_D_fake_AI = self.criterionGAN(pred_D_fake_AI, True)*4\n\n        self.loss_D_G = self.loss_D_fake_AB + self.loss_D_fake_AI\n\n        ## uncertainty guided pixel loss\n        # fake_B\n        self.loss_L1_1 = torch.mean(torch.div(torch.abs(self.fake_B-self.real_B), self.A) + torch.log(self.A)) * self.opt.lambda_L1 + self.criterionL1(self.fake_B, self.real_B) * self.opt.lambda_L1\n        # I\n        self.loss_L1_2 = torch.mean(torch.div(torch.abs(self.I-self.real_B), self.A) + torch.log(self.A)) * self.opt.lambda_L1*2 + self.criterionL1(self.I, self.real_B) * self.opt.lambda_L1 *2\n\n        # fake_D\n        self.loss_L1_3 = torch.mean(torch.div(torch.abs(self.fake_D - self.real_D), self.A) + torch.log(self.A)) * self.opt.lambda_L1_seg + self.criterionL1(self.fake_D, self.real_D) * self.opt.lambda_L1_seg\n        # Is\n        self.loss_L1_4 = torch.mean(torch.div(torch.abs(self.Is - self.real_D), self.A) + torch.log(self.A)) * self.opt.lambda_L1_seg*2 + self.criterionL1(self.Is, self.real_D) * self.opt.lambda_L1_seg*2\n        \n        # Combined loss\n        self.loss_L1 = self.loss_L1_1 + self.loss_L1_2 + self.loss_L1_3 + self.loss_L1_4\n\n        ## tv loss\n        self.loss_reg = self.opt.REGULARIZATION * (\n                torch.sum(torch.abs(self.I[:, :, :, :-1] - self.I[:, :, :, 1:])) +\n                torch.sum(torch.abs(self.I[:, :, :-1, :] - self.I[:, :, 1:, :])))\n\n        self.loss_G = self.loss_D_G + self.loss_L1 + self.loss_reg\n\n        self.loss_G.backward()\n\n    def optimize_parameters(self):\n        self.forward()\n        # update D\n        self.set_requires_grad(self.netD, True)\n        self.optimizer_D.zero_grad()\n        self.backward_D()\n        self.optimizer_D.step()\n\n        # update G\n        self.set_requires_grad(self.netD, False)\n        self.optimizer_G.zero_grad()\n        self.backward_G()\n        self.optimizer_G.step()\n"""
selectiongan_v2/options/__init__.py,0,b''
selectiongan_v2/options/base_options.py,1,"b'import argparse\nimport os\nfrom util import util\nimport torch\nimport models\nimport data\n\n\nclass BaseOptions():\n    def __init__(self):\n        self.initialized = False\n\n    def initialize(self, parser):\n        parser.add_argument(\'--dataroot\', type=str, default=\'/home/csdept/projects/pytorch-CycleGAN-and-pix2pix_sg2/datasets/dayton\', help=\'path to images (should have subfolders trainA, trainB, valA, valB, etc)\')\n        parser.add_argument(\'--batchSize\', type=int, default=4, help=\'input batch size\')\n        parser.add_argument(\'--loadSize\', type=int, default=286, help=\'scale images to this size\')\n        parser.add_argument(\'--fineSize\', type=int, default=256, help=\'then crop to this size\')\n        parser.add_argument(\'--input_nc\', type=int, default=3, help=\'# of input image channels\')\n        parser.add_argument(\'--output_nc\', type=int, default=3, help=\'# of output image channels\')\n        parser.add_argument(\'--ngf\', type=int, default=64, help=\'# of gen filters in first conv layer\')\n        parser.add_argument(\'--ngaf\', type=int, default=4, help=\'# of gen filters in first conv layer\')\n        parser.add_argument(\'--ndf\', type=int, default=64, help=\'# of discrim filters in first conv layer\')\n        parser.add_argument(\'--which_model_netD\', type=str, default=\'basic\', help=\'selects model to use for netD\')\n        parser.add_argument(\'--which_model_netG\', type=str, default=\'unet_256\', help=\'selects model to use for netG\')\n        parser.add_argument(\'--n_layers_D\', type=int, default=3, help=\'only used if which_model_netD==n_layers\')\n        parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU\')\n        parser.add_argument(\'--name\', type=str, default=\'ego2top_x_sg2_3_rmadangeGs_4_1\', help=\'name of the experiment. It decides where to store samples and models\')\n        parser.add_argument(\'--dataset_mode\', type=str, default=\'aligned\', help=\'chooses how datasets are loaded. [unaligned | aligned | single]\')\n        parser.add_argument(\'--model\', type=str, default=\'pix2pix\',\n                            help=\'chooses which model to use. cycle_gan, pix2pix, test\')\n        parser.add_argument(\'--which_direction\', type=str, default=\'AtoB\', help=\'AtoB or BtoA\')\n        parser.add_argument(\'--nThreads\', default=4, type=int, help=\'# threads for loading data\')\n        parser.add_argument(\'--checkpoints_dir\', type=str, default=\'./checkpoints\', help=\'models are saved here\')\n        parser.add_argument(\'--norm\', type=str, default=\'batch\', help=\'instance normalization or batch normalization\')\n        parser.add_argument(\'--serial_batches\', action=\'store_true\', help=\'if true, takes images in order to make batches, otherwise takes them randomly\')\n        parser.add_argument(\'--display_winsize\', type=int, default=256, help=\'display window size\')\n        parser.add_argument(\'--display_id\', type=int, default=0, help=\'window id of the web display\')\n        parser.add_argument(\'--display_server\', type=str, default=""http://localhost"", help=\'visdom server of the web display\')\n        parser.add_argument(\'--display_env\', type=str, default=\'main\', help=\'visdom display environment name (default is ""main"")\')\n        parser.add_argument(\'--display_port\', type=int, default=8097, help=\'visdom port of the web display\')\n        parser.add_argument(\'--no_dropout\', action=\'store_true\', help=\'no dropout for the generator\')\n        parser.add_argument(\'--max_dataset_size\', type=int, default=float(""inf""), help=\'Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.\')\n        parser.add_argument(\'--resize_or_crop\', type=str, default=\'resize_and_crop\', help=\'scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop]\')\n        parser.add_argument(\'--no_flip\', action=\'store_true\', help=\'if specified, do not flip the images for data augmentation\')\n        parser.add_argument(\'--init_type\', type=str, default=\'normal\', help=\'network initialization [normal|xavier|kaiming|orthogonal]\')\n        parser.add_argument(\'--init_gain\', type=float, default=0.02, help=\'scaling factor for normal, xavier and orthogonal.\')\n        parser.add_argument(\'--verbose\', action=\'store_true\', help=\'if specified, print more debugging information\')\n        parser.add_argument(\'--suffix\', default=\'\', type=str, help=\'customized suffix: opt.name = opt.name + suffix: e.g., {model}_{which_model_netG}_size{loadSize}\')\n        parser.add_argument(\'--saveDisk\', action=\'store_true\', help=\'save disk memory during testing time\')\n        self.initialized = True\n        return parser\n\n    def gather_options(self):\n        # initialize parser with basic options\n        if not self.initialized:\n            parser = argparse.ArgumentParser(\n                formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n            parser = self.initialize(parser)\n\n        # get the basic options\n        opt, _ = parser.parse_known_args()\n\n        # modify model-related parser options\n        model_name = opt.model\n        model_option_setter = models.get_option_setter(model_name)\n        parser = model_option_setter(parser, self.isTrain)\n        opt, _ = parser.parse_known_args()  # parse again with the new defaults\n\n        # modify dataset-related parser options\n        dataset_name = opt.dataset_mode\n        dataset_option_setter = data.get_option_setter(dataset_name)\n        parser = dataset_option_setter(parser, self.isTrain)\n\n        self.parser = parser\n\n        return parser.parse_args()\n\n    def print_options(self, opt):\n        message = \'\'\n        message += \'----------------- Options ---------------\\n\'\n        for k, v in sorted(vars(opt).items()):\n            comment = \'\'\n            default = self.parser.get_default(k)\n            if v != default:\n                comment = \'\\t[default: %s]\' % str(default)\n            message += \'{:>25}: {:<30}{}\\n\'.format(str(k), str(v), comment)\n        message += \'----------------- End -------------------\'\n        print(message)\n\n        # save to the disk\n        expr_dir = os.path.join(opt.checkpoints_dir, opt.name)\n        util.mkdirs(expr_dir)\n        file_name = os.path.join(expr_dir, \'opt.txt\')\n        with open(file_name, \'wt\') as opt_file:\n            opt_file.write(message)\n            opt_file.write(\'\\n\')\n\n    def parse(self):\n\n        opt = self.gather_options()\n        opt.isTrain = self.isTrain   # train or test\n\n        # process opt.suffix\n        if opt.suffix:\n            suffix = (\'_\' + opt.suffix.format(**vars(opt))) if opt.suffix != \'\' else \'\'\n            opt.name = opt.name + suffix\n\n        self.print_options(opt)\n\n        # set gpu ids\n        str_ids = opt.gpu_ids.split(\',\')\n        opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                opt.gpu_ids.append(id)\n        if len(opt.gpu_ids) > 0:\n            torch.cuda.set_device(opt.gpu_ids[0])\n\n        self.opt = opt\n        return self.opt\n'"
selectiongan_v2/options/test_options.py,0,"b'from .base_options import BaseOptions\n\n\nclass TestOptions(BaseOptions):\n    def initialize(self, parser):\n        parser = BaseOptions.initialize(self, parser)\n        parser.add_argument(\'--ntest\', type=int, default=float(""inf""), help=\'# of test examples.\')\n        parser.add_argument(\'--results_dir\', type=str, default=\'./results/\', help=\'saves results here.\')\n        parser.add_argument(\'--aspect_ratio\', type=float, default=1.0, help=\'aspect ratio of result images\')\n        parser.add_argument(\'--phase\', type=str, default=\'test\', help=\'train, val, test, etc\')\n        parser.add_argument(\'--which_epoch\', type=str, default=\'latest\', help=\'which epoch to load? set to latest to use latest cached model\')\n        #  Dropout and Batchnorm has different behavioir during training and test.\n        parser.add_argument(\'--eval\', action=\'store_true\', help=\'use eval mode during test time.\')\n        parser.add_argument(\'--how_many\', type=int, default=5, help=\'how many test images to run\')\n\n        parser.set_defaults(model=\'pix2pix\')\n        # To avoid cropping, the loadSize should be the same as fineSize\n        parser.set_defaults(loadSize=parser.get_default(\'fineSize\'))\n        \n        self.isTrain = False\n        return parser\n'"
selectiongan_v2/options/train_options.py,0,"b""from .base_options import BaseOptions\n\n\nclass TrainOptions(BaseOptions):\n    def initialize(self, parser):\n        parser = BaseOptions.initialize(self, parser)\n        parser.add_argument('--display_freq', type=int, default=10, help='frequency of showing training results on screen')\n        parser.add_argument('--display_ncols', type=int, default=7, help='if positive, display all images in a single visdom web panel with certain number of images per row.')\n        parser.add_argument('--update_html_freq', type=int, default=100, help='frequency of saving training results to html')\n        parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n        parser.add_argument('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')\n        parser.add_argument('--save_epoch_freq', type=int, default=1, help='frequency of saving checkpoints at the end of epochs')\n        parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n        parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n        parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n        parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n        parser.add_argument('--niter', type=int, default=20, help='# of iter at starting learning rate')\n        parser.add_argument('--niter_decay', type=int, default=15, help='# of iter to linearly decay learning rate to zero')\n        parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n        parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n        parser.add_argument('--no_lsgan', action='store_true', help='do *not* use least square GAN, if false, use vanilla GAN')\n        parser.add_argument('--pool_size', type=int, default=50, help='the size of image buffer that stores previously generated images')\n        parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n        parser.add_argument('--lr_policy', type=str, default='lambda', help='learning rate policy: lambda|step|plateau')\n        parser.add_argument('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations')\n\n        self.isTrain = True\n        return parser\n"""
selectiongan_v2/util/__init__.py,0,b''
selectiongan_v2/util/get_data.py,0,"b'from __future__ import print_function\nimport os\nimport tarfile\nimport requests\nfrom warnings import warn\nfrom zipfile import ZipFile\nfrom bs4 import BeautifulSoup\nfrom os.path import abspath, isdir, join, basename\n\n\nclass GetData(object):\n    """"""\n\n    Download CycleGAN or Pix2Pix Data.\n\n    Args:\n        technique : str\n            One of: \'cyclegan\' or \'pix2pix\'.\n        verbose : bool\n            If True, print additional information.\n\n    Examples:\n        >>> from util.get_data import GetData\n        >>> gd = GetData(technique=\'cyclegan\')\n        >>> new_data_path = gd.get(save_path=\'./datasets\')  # options will be displayed.\n\n    """"""\n\n    def __init__(self, technique=\'cyclegan\', verbose=True):\n        url_dict = {\n            \'pix2pix\': \'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets\',\n            \'cyclegan\': \'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets\'\n        }\n        self.url = url_dict.get(technique.lower())\n        self._verbose = verbose\n\n    def _print(self, text):\n        if self._verbose:\n            print(text)\n\n    @staticmethod\n    def _get_options(r):\n        soup = BeautifulSoup(r.text, \'lxml\')\n        options = [h.text for h in soup.find_all(\'a\', href=True)\n                   if h.text.endswith((\'.zip\', \'tar.gz\'))]\n        return options\n\n    def _present_options(self):\n        r = requests.get(self.url)\n        options = self._get_options(r)\n        print(\'Options:\\n\')\n        for i, o in enumerate(options):\n            print(""{0}: {1}"".format(i, o))\n        choice = input(""\\nPlease enter the number of the ""\n                       ""dataset above you wish to download:"")\n        return options[int(choice)]\n\n    def _download_data(self, dataset_url, save_path):\n        if not isdir(save_path):\n            os.makedirs(save_path)\n\n        base = basename(dataset_url)\n        temp_save_path = join(save_path, base)\n\n        with open(temp_save_path, ""wb"") as f:\n            r = requests.get(dataset_url)\n            f.write(r.content)\n\n        if base.endswith(\'.tar.gz\'):\n            obj = tarfile.open(temp_save_path)\n        elif base.endswith(\'.zip\'):\n            obj = ZipFile(temp_save_path, \'r\')\n        else:\n            raise ValueError(""Unknown File Type: {0}."".format(base))\n\n        self._print(""Unpacking Data..."")\n        obj.extractall(save_path)\n        obj.close()\n        os.remove(temp_save_path)\n\n    def get(self, save_path, dataset=None):\n        """"""\n\n        Download a dataset.\n\n        Args:\n            save_path : str\n                A directory to save the data to.\n            dataset : str, optional\n                A specific dataset to download.\n                Note: this must include the file extension.\n                If None, options will be presented for you\n                to choose from.\n\n        Returns:\n            save_path_full : str\n                The absolute path to the downloaded data.\n\n        """"""\n        if dataset is None:\n            selected_dataset = self._present_options()\n        else:\n            selected_dataset = dataset\n\n        save_path_full = join(save_path, selected_dataset.split(\'.\')[0])\n\n        if isdir(save_path_full):\n            warn(""\\n\'{0}\' already exists. Voiding Download."".format(\n                save_path_full))\n        else:\n            self._print(\'Downloading Data...\')\n            url = ""{0}/{1}"".format(self.url, selected_dataset)\n            self._download_data(url, save_path=save_path)\n\n        return abspath(save_path_full)\n'"
selectiongan_v2/util/html.py,0,"b'import dominate\nfrom dominate.tags import *\nimport os\n\n\nclass HTML:\n    def __init__(self, web_dir, title, reflesh=0):\n        self.title = title\n        self.web_dir = web_dir\n        self.img_dir = os.path.join(self.web_dir, \'images\')\n        if not os.path.exists(self.web_dir):\n            os.makedirs(self.web_dir)\n        if not os.path.exists(self.img_dir):\n            os.makedirs(self.img_dir)\n        # print(self.img_dir)\n\n        self.doc = dominate.document(title=title)\n        if reflesh > 0:\n            with self.doc.head:\n                meta(http_equiv=""reflesh"", content=str(reflesh))\n\n    def get_image_dir(self):\n        return self.img_dir\n\n    def add_header(self, str):\n        with self.doc:\n            h3(str)\n\n    def add_table(self, border=1):\n        self.t = table(border=border, style=""table-layout: fixed;"")\n        self.doc.add(self.t)\n\n    def add_images(self, ims, txts, links, width=400):\n        self.add_table()\n        with self.t:\n            with tr():\n                for im, txt, link in zip(ims, txts, links):\n                    with td(style=""word-wrap: break-word;"", halign=""center"", valign=""top""):\n                        with p():\n                            with a(href=os.path.join(\'images\', link)):\n                                img(style=""width:%dpx"" % width, src=os.path.join(\'images\', im))\n                            br()\n                            p(txt)\n\n    def save(self):\n        html_file = \'%s/index.html\' % self.web_dir\n        f = open(html_file, \'wt\')\n        f.write(self.doc.render())\n        f.close()\n\n\nif __name__ == \'__main__\':\n    html = HTML(\'web/\', \'test_html\')\n    html.add_header(\'hello world\')\n\n    ims = []\n    txts = []\n    links = []\n    for n in range(4):\n        ims.append(\'image_%d.png\' % n)\n        txts.append(\'text_%d\' % n)\n        links.append(\'image_%d.png\' % n)\n    html.add_images(ims, txts, links)\n    html.save()\n'"
selectiongan_v2/util/image_pool.py,2,"b'import random\nimport torch\n\n\nclass ImagePool():\n    def __init__(self, pool_size):\n        self.pool_size = pool_size\n        if self.pool_size > 0:\n            self.num_imgs = 0\n            self.images = []\n\n    def query(self, images):\n        if self.pool_size == 0:\n            return images\n        return_images = []\n        for image in images:\n            image = torch.unsqueeze(image.data, 0)\n            if self.num_imgs < self.pool_size:\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n                return_images.append(image)\n            else:\n                p = random.uniform(0, 1)\n                if p > 0.5:\n                    random_id = random.randint(0, self.pool_size - 1)  # randint is inclusive\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image\n                    return_images.append(tmp)\n                else:\n                    return_images.append(image)\n        return_images = torch.cat(return_images, 0)\n        return return_images\n'"
selectiongan_v2/util/util.py,2,"b""from __future__ import print_function\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport os\n\n\n# Converts a Tensor into an image array (numpy)\n# |imtype|: the desired type of the converted numpy array\ndef tensor2im(input_image, imtype=np.uint8):\n    if isinstance(input_image, torch.Tensor):\n        image_tensor = input_image.data\n    else:\n        return input_image\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    if image_numpy.shape[0] == 1:\n        image_numpy = np.tile(image_numpy, (3, 1, 1))\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n    return image_numpy.astype(imtype)\n\n\ndef diagnose_network(net, name='network'):\n    mean = 0.0\n    count = 0\n    for param in net.parameters():\n        if param.grad is not None:\n            mean += torch.mean(torch.abs(param.grad.data))\n            count += 1\n    if count > 0:\n        mean = mean / count\n    print(name)\n    print(mean)\n\n\ndef save_image(image_numpy, image_path):\n    image_pil = Image.fromarray(image_numpy)\n    image_pil.save(image_path)\n\n\ndef print_numpy(x, val=True, shp=False):\n    x = x.astype(np.float64)\n    if shp:\n        print('shape,', x.shape)\n    if val:\n        x = x.flatten()\n        print('mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f' % (\n            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n"""
selectiongan_v2/util/visualizer.py,0,"b'import numpy as np\nimport os\nimport ntpath\nimport time\nfrom . import util\nfrom . import html\nfrom scipy.misc import imresize\n\n\n# save image to the disk\ndef save_images(webpage, visuals, image_path, aspect_ratio=1.0, width=256):\n    image_dir = webpage.get_image_dir()\n    short_path = ntpath.basename(image_path[0])\n    name = os.path.splitext(short_path)[0]\n\n    webpage.add_header(name)\n    ims, txts, links = [], [], []\n\n    for label, im_data in visuals.items():\n        im = util.tensor2im(im_data)\n        image_name = \'%s_%s.png\' % (name, label)\n        save_path = os.path.join(image_dir, image_name)\n        h, w, _ = im.shape\n        if aspect_ratio > 1.0:\n            im = imresize(im, (h, int(w * aspect_ratio)), interp=\'bicubic\')\n        if aspect_ratio < 1.0:\n            im = imresize(im, (int(h / aspect_ratio), w), interp=\'bicubic\')\n        util.save_image(im, save_path)\n\n        ims.append(image_name)\n        txts.append(label)\n        links.append(image_name)\n    webpage.add_images(ims, txts, links, width=width)\n\n\nclass Visualizer():\n    def __init__(self, opt):\n        self.display_id = opt.display_id\n        self.use_html = opt.isTrain and not opt.no_html\n        self.win_size = opt.display_winsize\n        self.name = opt.name\n        self.opt = opt\n        self.saved = False\n        if self.display_id > 0:\n            import visdom\n            self.ncols = opt.display_ncols\n            self.vis = visdom.Visdom(server=opt.display_server, port=opt.display_port, env=opt.display_env, raise_exceptions=True, use_incoming_socket=False)\n\n        if self.use_html:\n            self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, \'web\')\n            self.img_dir = os.path.join(self.web_dir, \'images\')\n            print(\'create web directory %s...\' % self.web_dir)\n            util.mkdirs([self.web_dir, self.img_dir])\n        self.log_name = os.path.join(opt.checkpoints_dir, opt.name, \'loss_log.txt\')\n        with open(self.log_name, ""a"") as log_file:\n            now = time.strftime(""%c"")\n            log_file.write(\'================ Training Loss (%s) ================\\n\' % now)\n\n    def reset(self):\n        self.saved = False\n\n    def throw_visdom_connection_error(self):\n        print(\'\\n\\nCould not connect to Visdom server (https://github.com/facebookresearch/visdom) for displaying training progress.\\nYou can suppress connection to Visdom using the option --display_id -1. To install visdom, run \\n$ pip install visdom\\n, and start the server by \\n$ python -m visdom.server.\\n\\n\')\n        exit(1)\n\n    # |visuals|: dictionary of images to display or save\n    def display_current_results(self, visuals, epoch, save_result):\n        if self.display_id > 0:  # show images in the browser\n            ncols = self.ncols\n            if ncols > 0:\n                ncols = min(ncols, len(visuals))\n                h, w = next(iter(visuals.values())).shape[:2]\n                table_css = """"""<style>\n                        table {border-collapse: separate; border-spacing:4px; white-space:nowrap; text-align:center}\n                        table td {width: %dpx; height: %dpx; padding: 4px; outline: 4px solid black}\n                        </style>"""""" % (w, h)\n                title = self.name\n                label_html = \'\'\n                label_html_row = \'\'\n                images = []\n                idx = 0\n                for label, image in visuals.items():\n                    image_numpy = util.tensor2im(image)\n                    label_html_row += \'<td>%s</td>\' % label\n                    images.append(image_numpy.transpose([2, 0, 1]))\n                    idx += 1\n                    if idx % ncols == 0:\n                        label_html += \'<tr>%s</tr>\' % label_html_row\n                        label_html_row = \'\'\n                white_image = np.ones_like(image_numpy.transpose([2, 0, 1])) * 255\n                while idx % ncols != 0:\n                    images.append(white_image)\n                    label_html_row += \'<td></td>\'\n                    idx += 1\n                if label_html_row != \'\':\n                    label_html += \'<tr>%s</tr>\' % label_html_row\n                # pane col = image row\n                try:\n                    self.vis.images(images, nrow=ncols, win=self.display_id + 1,\n                                    padding=2, opts=dict(title=title + \' images\'))\n                    label_html = \'<table>%s</table>\' % label_html\n                    self.vis.text(table_css + label_html, win=self.display_id + 2,\n                                  opts=dict(title=title + \' labels\'))\n                except ConnectionError:\n                    self.throw_visdom_connection_error()\n\n            else:\n                idx = 1\n                for label, image in visuals.items():\n                    image_numpy = util.tensor2im(image)\n                    self.vis.image(image_numpy.transpose([2, 0, 1]), opts=dict(title=label),\n                                   win=self.display_id + idx)\n                    idx += 1\n\n        if self.use_html and (save_result or not self.saved):  # save images to a html file\n            self.saved = True\n            for label, image in visuals.items():\n                image_numpy = util.tensor2im(image)\n                img_path = os.path.join(self.img_dir, \'epoch%.3d_%s.png\' % (epoch, label))\n                util.save_image(image_numpy, img_path)\n            # update website\n            webpage = html.HTML(self.web_dir, \'Experiment name = %s\' % self.name, reflesh=1)\n            for n in range(epoch, 0, -1):\n                webpage.add_header(\'epoch [%d]\' % n)\n                ims, txts, links = [], [], []\n\n                for label, image_numpy in visuals.items():\n                    image_numpy = util.tensor2im(image)\n                    img_path = \'epoch%.3d_%s.png\' % (n, label)\n                    ims.append(img_path)\n                    txts.append(label)\n                    links.append(img_path)\n                webpage.add_images(ims, txts, links, width=self.win_size)\n            webpage.save()\n\n    # losses: dictionary of error labels and values\n    def plot_current_losses(self, epoch, counter_ratio, opt, losses):\n        if not hasattr(self, \'plot_data\'):\n            self.plot_data = {\'X\': [], \'Y\': [], \'legend\': list(losses.keys())}\n        self.plot_data[\'X\'].append(epoch + counter_ratio)\n        self.plot_data[\'Y\'].append([losses[k] for k in self.plot_data[\'legend\']])\n        try:\n            self.vis.line(\n                X=np.stack([np.array(self.plot_data[\'X\'])] * len(self.plot_data[\'legend\']), 1),\n                Y=np.array(self.plot_data[\'Y\']),\n                opts={\n                    \'title\': self.name + \' loss over time\',\n                    \'legend\': self.plot_data[\'legend\'],\n                    \'xlabel\': \'epoch\',\n                    \'ylabel\': \'loss\'},\n                win=self.display_id)\n        except ConnectionError:\n            self.throw_visdom_connection_error()\n\n    # losses: same format as |losses| of plot_current_losses\n    def print_current_losses(self, epoch, i, losses, t, t_data):\n        message = \'(epoch: %d, iters: %d, time: %.3f, data: %.3f) \' % (epoch, i, t, t_data)\n        for k, v in losses.items():\n            message += \'%s: %.3f \' % (k, v)\n\n        print(message)\n        with open(self.log_name, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n'"
semantic_synthesis/data/__init__.py,2,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport importlib\nimport torch.utils.data\nfrom data.base_dataset import BaseDataset\n\n\ndef find_dataset_using_name(dataset_name):\n    # Given the option --dataset [datasetname],\n    # the file ""datasets/datasetname_dataset.py""\n    # will be imported. \n    dataset_filename = ""data."" + dataset_name + ""_dataset""\n    datasetlib = importlib.import_module(dataset_filename)\n\n    # In the file, the class called DatasetNameDataset() will\n    # be instantiated. It has to be a subclass of BaseDataset,\n    # and it is case-insensitive.\n    dataset = None\n    target_dataset_name = dataset_name.replace(\'_\', \'\') + \'dataset\'\n    for name, cls in datasetlib.__dict__.items():\n        if name.lower() == target_dataset_name.lower() \\\n           and issubclass(cls, BaseDataset):\n            dataset = cls\n            \n    if dataset is None:\n        raise ValueError(""In %s.py, there should be a subclass of BaseDataset ""\n                         ""with class name that matches %s in lowercase."" %\n                         (dataset_filename, target_dataset_name))\n\n    return dataset\n\n\ndef get_option_setter(dataset_name):    \n    dataset_class = find_dataset_using_name(dataset_name)\n    return dataset_class.modify_commandline_options\n\n\ndef create_dataloader(opt):\n    dataset = find_dataset_using_name(opt.dataset_mode)\n    instance = dataset()\n    instance.initialize(opt)\n    print(""dataset [%s] of size %d was created"" %\n          (type(instance).__name__, len(instance)))\n    dataloader = torch.utils.data.DataLoader(\n        instance,\n        batch_size=opt.batchSize,\n        shuffle=not opt.serial_batches,\n        num_workers=int(opt.nThreads),\n        drop_last=opt.isTrain\n    )\n    return dataloader\n'"
semantic_synthesis/data/ade20k_dataset.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nfrom data.pix2pix_dataset import Pix2pixDataset\nfrom data.image_folder import make_dataset\n\n\nclass ADE20KDataset(Pix2pixDataset):\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser = Pix2pixDataset.modify_commandline_options(parser, is_train)\n        parser.set_defaults(preprocess_mode=\'resize_and_crop\')\n        if is_train:\n            parser.set_defaults(load_size=286)\n        else:\n            parser.set_defaults(load_size=256)\n        parser.set_defaults(crop_size=256)\n        parser.set_defaults(display_winsize=256)\n        parser.set_defaults(label_nc=150)\n        parser.set_defaults(contain_dontcare_label=True)\n        parser.set_defaults(cache_filelist_read=False)\n        parser.set_defaults(cache_filelist_write=False)\n        parser.set_defaults(no_instance=True)\n        return parser\n\n    def get_paths(self, opt):\n        root = opt.dataroot\n        phase = \'val\' if opt.phase == \'test\' else \'train\'\n\n        all_images = make_dataset(root, recursive=True, read_cache=False, write_cache=False)\n        image_paths = []\n        label_paths = []\n        for p in all_images:\n            if \'_%s_\' % phase not in p:\n                continue\n            if p.endswith(\'.jpg\'):\n                image_paths.append(p)\n            elif p.endswith(\'.png\'):\n                label_paths.append(p)\n\n        instance_paths = []  # don\'t use instance map for ade20k\n\n        return label_paths, image_paths, instance_paths\n\n    # In ADE20k, \'unknown\' label is of value 0.\n    # Change the \'unknown\' label to the last label to match other datasets.\n    def postprocess(self, input_dict):\n        label = input_dict[\'label\']\n        label = label - 1\n        label[label == -1] = self.opt.label_nc\n'"
semantic_synthesis/data/base_dataset.py,1,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport torch.utils.data as data\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport numpy as np\nimport random\n\n\nclass BaseDataset(data.Dataset):\n    def __init__(self):\n        super(BaseDataset, self).__init__()\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def initialize(self, opt):\n        pass\n\n\ndef get_params(opt, size):\n    w, h = size\n    new_h = h\n    new_w = w\n    if opt.preprocess_mode == \'resize_and_crop\':\n        new_h = new_w = opt.load_size\n    elif opt.preprocess_mode == \'scale_width_and_crop\':\n        new_w = opt.load_size\n        new_h = opt.load_size * h // w\n    elif opt.preprocess_mode == \'scale_shortside_and_crop\':\n        ss, ls = min(w, h), max(w, h)  # shortside and longside\n        width_is_shorter = w == ss\n        ls = int(opt.load_size * ls / ss)\n        new_w, new_h = (ss, ls) if width_is_shorter else (ls, ss)\n\n    x = random.randint(0, np.maximum(0, new_w - opt.crop_size))\n    y = random.randint(0, np.maximum(0, new_h - opt.crop_size))\n\n    flip = random.random() > 0.5\n    return {\'crop_pos\': (x, y), \'flip\': flip}\n\n\ndef get_transform(opt, params, method=Image.BICUBIC, normalize=True, toTensor=True):\n    transform_list = []\n    if \'resize\' in opt.preprocess_mode:\n        osize = [opt.load_size, opt.load_size]\n        transform_list.append(transforms.Resize(osize, interpolation=method))\n    elif \'scale_width\' in opt.preprocess_mode:\n        transform_list.append(transforms.Lambda(lambda img: __scale_width(img, opt.load_size, method)))\n    elif \'scale_shortside\' in opt.preprocess_mode:\n        transform_list.append(transforms.Lambda(lambda img: __scale_shortside(img, opt.load_size, method)))\n\n    if \'crop\' in opt.preprocess_mode:\n        transform_list.append(transforms.Lambda(lambda img: __crop(img, params[\'crop_pos\'], opt.crop_size)))\n\n    if opt.preprocess_mode == \'none\':\n        base = 32\n        transform_list.append(transforms.Lambda(lambda img: __make_power_2(img, base, method)))\n\n    if opt.preprocess_mode == \'fixed\':\n        w = opt.crop_size\n        h = round(opt.crop_size / opt.aspect_ratio)\n        transform_list.append(transforms.Lambda(lambda img: __resize(img, w, h, method)))\n\n    if opt.isTrain and not opt.no_flip:\n        transform_list.append(transforms.Lambda(lambda img: __flip(img, params[\'flip\'])))\n\n    if toTensor:\n        transform_list += [transforms.ToTensor()]\n\n    if normalize:\n        transform_list += [transforms.Normalize((0.5, 0.5, 0.5),\n                                                (0.5, 0.5, 0.5))]\n    return transforms.Compose(transform_list)\n\n\ndef normalize():\n    return transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n\n\ndef __resize(img, w, h, method=Image.BICUBIC):\n    return img.resize((w, h), method)\n\n\ndef __make_power_2(img, base, method=Image.BICUBIC):\n    ow, oh = img.size\n    h = int(round(oh / base) * base)\n    w = int(round(ow / base) * base)\n    if (h == oh) and (w == ow):\n        return img\n    return img.resize((w, h), method)\n\n\ndef __scale_width(img, target_width, method=Image.BICUBIC):\n    ow, oh = img.size\n    if (ow == target_width):\n        return img\n    w = target_width\n    h = int(target_width * oh / ow)\n    return img.resize((w, h), method)\n\n\ndef __scale_shortside(img, target_width, method=Image.BICUBIC):\n    ow, oh = img.size\n    ss, ls = min(ow, oh), max(ow, oh)  # shortside and longside\n    width_is_shorter = ow == ss\n    if (ss == target_width):\n        return img\n    ls = int(target_width * ls / ss)\n    nw, nh = (ss, ls) if width_is_shorter else (ls, ss)\n    return img.resize((nw, nh), method)\n\n\ndef __crop(img, pos, size):\n    ow, oh = img.size\n    x1, y1 = pos\n    tw = th = size\n    return img.crop((x1, y1, x1 + tw, y1 + th))\n\n\ndef __flip(img, flip):\n    if flip:\n        return img.transpose(Image.FLIP_LEFT_RIGHT)\n    return img\n'"
semantic_synthesis/data/cityscapes_dataset.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport os.path\nfrom data.pix2pix_dataset import Pix2pixDataset\nfrom data.image_folder import make_dataset\n\n\nclass CityscapesDataset(Pix2pixDataset):\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser = Pix2pixDataset.modify_commandline_options(parser, is_train)\n        parser.set_defaults(preprocess_mode=\'fixed\')\n        parser.set_defaults(load_size=512)\n        parser.set_defaults(crop_size=512)\n        parser.set_defaults(display_winsize=512)\n        parser.set_defaults(label_nc=35)\n        parser.set_defaults(aspect_ratio=2.0)\n        parser.set_defaults(batchSize=16)\n        opt, _ = parser.parse_known_args()\n        if hasattr(opt, \'num_upsampling_layers\'):\n            parser.set_defaults(num_upsampling_layers=\'more\')\n        return parser\n\n    def get_paths(self, opt):\n        root = opt.dataroot\n        phase = \'val\' if opt.phase == \'test\' else \'train\'\n\n        label_dir = os.path.join(root, \'gtFine\', phase)\n        label_paths_all = make_dataset(label_dir, recursive=True)\n        label_paths = [p for p in label_paths_all if p.endswith(\'_labelIds.png\')]\n\n        image_dir = os.path.join(root, \'leftImg8bit\', phase)\n        image_paths = make_dataset(image_dir, recursive=True)\n\n        if not opt.no_instance:\n            instance_paths = [p for p in label_paths_all if p.endswith(\'_instanceIds.png\')]\n        else:\n            instance_paths = []\n\n        return label_paths, image_paths, instance_paths\n\n    def paths_match(self, path1, path2):\n        name1 = os.path.basename(path1)\n        name2 = os.path.basename(path2)\n        # compare the first 3 components, [city]_[id1]_[id2]\n        return \'_\'.join(name1.split(\'_\')[:3]) == \\\n            \'_\'.join(name2.split(\'_\')[:3])\n'"
semantic_synthesis/data/coco_dataset.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport os.path\nfrom data.pix2pix_dataset import Pix2pixDataset\nfrom data.image_folder import make_dataset\n\n\nclass CocoDataset(Pix2pixDataset):\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser = Pix2pixDataset.modify_commandline_options(parser, is_train)\n        parser.add_argument(\'--coco_no_portraits\', action=\'store_true\')\n        parser.set_defaults(preprocess_mode=\'resize_and_crop\')\n        if is_train:\n            parser.set_defaults(load_size=286)\n        else:\n            parser.set_defaults(load_size=256)\n        parser.set_defaults(crop_size=256)\n        parser.set_defaults(display_winsize=256)\n        parser.set_defaults(label_nc=182)\n        parser.set_defaults(contain_dontcare_label=True)\n        parser.set_defaults(cache_filelist_read=True)\n        parser.set_defaults(cache_filelist_write=True)\n        return parser\n\n    def get_paths(self, opt):\n        root = opt.dataroot\n        phase = \'val\' if opt.phase == \'test\' else opt.phase\n\n        label_dir = os.path.join(root, \'%s_label\' % phase)\n        label_paths = make_dataset(label_dir, recursive=False, read_cache=True)\n\n        if not opt.coco_no_portraits and opt.isTrain:\n            label_portrait_dir = os.path.join(root, \'%s_label_portrait\' % phase)\n            if os.path.isdir(label_portrait_dir):\n                label_portrait_paths = make_dataset(label_portrait_dir, recursive=False, read_cache=True)\n                label_paths += label_portrait_paths\n\n        image_dir = os.path.join(root, \'%s_img\' % phase)\n        image_paths = make_dataset(image_dir, recursive=False, read_cache=True)\n\n        if not opt.coco_no_portraits and opt.isTrain:\n            image_portrait_dir = os.path.join(root, \'%s_img_portrait\' % phase)\n            if os.path.isdir(image_portrait_dir):\n                image_portrait_paths = make_dataset(image_portrait_dir, recursive=False, read_cache=True)\n                image_paths += image_portrait_paths\n\n        if not opt.no_instance:\n            instance_dir = os.path.join(root, \'%s_inst\' % phase)\n            instance_paths = make_dataset(instance_dir, recursive=False, read_cache=True)\n\n            if not opt.coco_no_portraits and opt.isTrain:\n                instance_portrait_dir = os.path.join(root, \'%s_inst_portrait\' % phase)\n                if os.path.isdir(instance_portrait_dir):\n                    instance_portrait_paths = make_dataset(instance_portrait_dir, recursive=False, read_cache=True)\n                    instance_paths += instance_portrait_paths\n\n        else:\n            instance_paths = []\n\n        return label_paths, image_paths, instance_paths\n'"
semantic_synthesis/data/custom_dataset.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nfrom data.pix2pix_dataset import Pix2pixDataset\nfrom data.image_folder import make_dataset\n\n\nclass CustomDataset(Pix2pixDataset):\n    """""" Dataset that loads images from directories\n        Use option --label_dir, --image_dir, --instance_dir to specify the directories.\n        The images in the directories are sorted in alphabetical order and paired in order.\n    """"""\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser = Pix2pixDataset.modify_commandline_options(parser, is_train)\n        parser.set_defaults(preprocess_mode=\'resize_and_crop\')\n        load_size = 286 if is_train else 256\n        parser.set_defaults(load_size=load_size)\n        parser.set_defaults(crop_size=256)\n        parser.set_defaults(display_winsize=256)\n        parser.set_defaults(label_nc=13)\n        parser.set_defaults(contain_dontcare_label=False)\n\n        parser.add_argument(\'--label_dir\', type=str, required=True,\n                            help=\'path to the directory that contains label images\')\n        parser.add_argument(\'--image_dir\', type=str, required=True,\n                            help=\'path to the directory that contains photo images\')\n        parser.add_argument(\'--instance_dir\', type=str, default=\'\',\n                            help=\'path to the directory that contains instance maps. Leave black if not exists\')\n        return parser\n\n    def get_paths(self, opt):\n        label_dir = opt.label_dir\n        label_paths = make_dataset(label_dir, recursive=False, read_cache=True)\n\n        image_dir = opt.image_dir\n        image_paths = make_dataset(image_dir, recursive=False, read_cache=True)\n\n        if len(opt.instance_dir) > 0:\n            instance_dir = opt.instance_dir\n            instance_paths = make_dataset(instance_dir, recursive=False, read_cache=True)\n        else:\n            instance_paths = []\n\n        assert len(label_paths) == len(image_paths), ""The #images in %s and %s do not match. Is there something wrong?""\n\n        return label_paths, image_paths, instance_paths\n'"
semantic_synthesis/data/facades_dataset.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport os.path\nfrom data.pix2pix_dataset import Pix2pixDataset\nfrom data.image_folder import make_dataset\n\n\nclass FacadesDataset(Pix2pixDataset):\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser = Pix2pixDataset.modify_commandline_options(parser, is_train)\n        parser.set_defaults(dataroot=\'./dataset/facades/\')\n        parser.set_defaults(preprocess_mode=\'resize_and_crop\')\n        load_size = 286 if is_train else 256\n        parser.set_defaults(load_size=load_size)\n        parser.set_defaults(crop_size=256)\n        parser.set_defaults(display_winsize=256)\n        parser.set_defaults(label_nc=13)\n        parser.set_defaults(contain_dontcare_label=False)\n        parser.set_defaults(no_instance=True)\n        return parser\n\n    def get_paths(self, opt):\n        root = opt.dataroot\n        phase = \'val\' if opt.phase == \'test\' else opt.phase\n\n        label_dir = os.path.join(root, \'%s_label\' % phase)\n        label_paths = make_dataset(label_dir, recursive=False, read_cache=True)\n\n        image_dir = os.path.join(root, \'%s_img\' % phase)\n        image_paths = make_dataset(image_dir, recursive=False, read_cache=True)\n\n        instance_paths = []\n\n        return label_paths, image_paths, instance_paths\n'"
semantic_synthesis/data/image_folder.py,1,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\n###############################################################################\n# Code from\n# https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n# Modified the original code so that it also loads images from the current\n# directory as well as the subdirectories\n###############################################################################\nimport torch.utils.data as data\nfrom PIL import Image\nimport os\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\', \'.tiff\', \'.webp\'\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef make_dataset_rec(dir, images):\n    assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n\n    for root, dnames, fnames in sorted(os.walk(dir, followlinks=True)):\n        for fname in fnames:\n            if is_image_file(fname):\n                path = os.path.join(root, fname)\n                images.append(path)\n\n\ndef make_dataset(dir, recursive=False, read_cache=False, write_cache=False):\n    images = []\n\n    if read_cache:\n        possible_filelist = os.path.join(dir, \'files.list\')\n        if os.path.isfile(possible_filelist):\n            with open(possible_filelist, \'r\') as f:\n                images = f.read().splitlines()\n                return images\n\n    if recursive:\n        make_dataset_rec(dir, images)\n    else:\n        assert os.path.isdir(dir) or os.path.islink(dir), \'%s is not a valid directory\' % dir\n\n        for root, dnames, fnames in sorted(os.walk(dir)):\n            for fname in fnames:\n                if is_image_file(fname):\n                    path = os.path.join(root, fname)\n                    images.append(path)\n\n    if write_cache:\n        filelist_cache = os.path.join(dir, \'files.list\')\n        with open(filelist_cache, \'w\') as f:\n            for path in images:\n                f.write(""%s\\n"" % path)\n            print(\'wrote filelist cache at %s\' % filelist_cache)\n\n    return images\n\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\n\nclass ImageFolder(data.Dataset):\n\n    def __init__(self, root, transform=None, return_paths=False,\n                 loader=default_loader):\n        imgs = make_dataset(root)\n        if len(imgs) == 0:\n            raise(RuntimeError(""Found 0 images in: "" + root + ""\\n""\n                               ""Supported image extensions are: "" +\n                               "","".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.imgs = imgs\n        self.transform = transform\n        self.return_paths = return_paths\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.return_paths:\n            return img, path\n        else:\n            return img\n\n    def __len__(self):\n        return len(self.imgs)\n'"
semantic_synthesis/data/pix2pix_dataset.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nfrom data.base_dataset import BaseDataset, get_params, get_transform\nfrom PIL import Image\nimport util.util as util\nimport os\n\n\nclass Pix2pixDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser.add_argument(\'--no_pairing_check\', action=\'store_true\',\n                            help=\'If specified, skip sanity check of correct label-image file pairing\')\n        return parser\n\n    def initialize(self, opt):\n        self.opt = opt\n\n        label_paths, image_paths, instance_paths = self.get_paths(opt)\n\n        util.natural_sort(label_paths)\n        util.natural_sort(image_paths)\n        if not opt.no_instance:\n            util.natural_sort(instance_paths)\n\n        label_paths = label_paths[:opt.max_dataset_size]\n        image_paths = image_paths[:opt.max_dataset_size]\n        instance_paths = instance_paths[:opt.max_dataset_size]\n\n        if not opt.no_pairing_check:\n            for path1, path2 in zip(label_paths, image_paths):\n                assert self.paths_match(path1, path2), \\\n                    ""The label-image pair (%s, %s) do not look like the right pair because the filenames are quite different. Are you sure about the pairing? Please see data/pix2pix_dataset.py to see what is going on, and use --no_pairing_check to bypass this."" % (path1, path2)\n\n        self.label_paths = label_paths\n        self.image_paths = image_paths\n        self.instance_paths = instance_paths\n\n        size = len(self.label_paths)\n        self.dataset_size = size\n\n    def get_paths(self, opt):\n        label_paths = []\n        image_paths = []\n        instance_paths = []\n        assert False, ""A subclass of Pix2pixDataset must override self.get_paths(self, opt)""\n        return label_paths, image_paths, instance_paths\n\n    def paths_match(self, path1, path2):\n        filename1_without_ext = os.path.splitext(os.path.basename(path1))[0]\n        filename2_without_ext = os.path.splitext(os.path.basename(path2))[0]\n        return filename1_without_ext == filename2_without_ext\n\n    def __getitem__(self, index):\n        # Label Image\n        label_path = self.label_paths[index]\n        label = Image.open(label_path)\n        params = get_params(self.opt, label.size)\n        transform_label = get_transform(self.opt, params, method=Image.NEAREST, normalize=False)\n        label_tensor = transform_label(label) * 255.0\n        label_tensor[label_tensor == 255] = self.opt.label_nc  # \'unknown\' is opt.label_nc\n\n        # input image (real images)\n        image_path = self.image_paths[index]\n        assert self.paths_match(label_path, image_path), \\\n            ""The label_path %s and image_path %s don\'t match."" % \\\n            (label_path, image_path)\n        image = Image.open(image_path)\n        image = image.convert(\'RGB\')\n\n        transform_image = get_transform(self.opt, params)\n        image_tensor = transform_image(image)\n\n        # if using instance maps\n        if self.opt.no_instance:\n            instance_tensor = 0\n        else:\n            instance_path = self.instance_paths[index]\n            instance = Image.open(instance_path)\n            if instance.mode == \'L\':\n                instance_tensor = transform_label(instance) * 255\n                instance_tensor = instance_tensor.long()\n            else:\n                instance_tensor = transform_label(instance)\n\n        input_dict = {\'label\': label_tensor,\n                      \'instance\': instance_tensor,\n                      \'image\': image_tensor,\n                      \'path\': image_path,\n                      }\n\n        # Give subclasses a chance to modify the final output\n        self.postprocess(input_dict)\n\n        return input_dict\n\n    def postprocess(self, input_dict):\n        return input_dict\n\n    def __len__(self):\n        return self.dataset_size\n'"
semantic_synthesis/datasets/coco_generate_instance_map.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport os\nimport argparse\nfrom pycocotools.coco import COCO\nimport numpy as np\nimport skimage.io as io\nfrom skimage.draw import polygon\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--annotation_file\', type=str, default=""./annotations/instances_train2017.json"",\n                    help=""Path to the annocation file. It can be downloaded at http://images.cocodataset.org/annotations/annotations_trainval2017.zip. Should be either instances_train2017.json or instances_val2017.json"")\nparser.add_argument(\'--input_label_dir\', type=str, default=""./train_label/"",\n                    help=""Path to the directory containing label maps. It can be downloaded at http://calvin.inf.ed.ac.uk/wp-content/uploads/data/cocostuffdataset/stuffthingmaps_trainval2017.zip"")\nparser.add_argument(\'--output_instance_dir\', type=str, default=""./train_inst/"",\n                    help=""Path to the output directory of instance maps"")\n\nopt = parser.parse_args()\n\nprint(""annotation file at {}"".format(opt.annotation_file))\nprint(""input label maps at {}"".format(opt.input_label_dir))\nprint(""output dir at {}"".format(opt.output_instance_dir))\n\n# initialize COCO api for instance annotations\ncoco = COCO(opt.annotation_file)\n\n\n# display COCO categories and supercategories\ncats = coco.loadCats(coco.getCatIds())\nimgIds = coco.getImgIds(catIds=coco.getCatIds(cats))\nfor ix, id in enumerate(imgIds):\n    if ix % 50 == 0:\n        print(""{} / {}"".format(ix, len(imgIds)))\n    img_dict = coco.loadImgs(id)[0]\n    filename = img_dict[""file_name""].replace(""jpg"", ""png"")\n    label_name = os.path.join(opt.input_label_dir, filename)\n    inst_name = os.path.join(opt.output_instance_dir, filename)\n    img = io.imread(label_name, as_grey=True)\n\n    annIds = coco.getAnnIds(imgIds=id, catIds=[], iscrowd=None)\n    anns = coco.loadAnns(annIds)\n    count = 0\n    for ann in anns:\n        if type(ann[""segmentation""]) == list:\n            if ""segmentation"" in ann:\n                for seg in ann[""segmentation""]:\n                    poly = np.array(seg).reshape((int(len(seg) / 2), 2))\n                    rr, cc = polygon(poly[:, 1] - 1, poly[:, 0] - 1)\n                    img[rr, cc] = count\n                count += 1\n\n    io.imsave(inst_name, img)\n'"
semantic_synthesis/models/__init__.py,3,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport importlib\nimport torch\n\n\ndef find_model_using_name(model_name):\n    # Given the option --model [modelname],\n    # the file ""models/modelname_model.py""\n    # will be imported.\n    model_filename = ""models."" + model_name + ""_model""\n    modellib = importlib.import_module(model_filename)\n\n    # In the file, the class called ModelNameModel() will\n    # be instantiated. It has to be a subclass of torch.nn.Module,\n    # and it is case-insensitive.\n    model = None\n    target_model_name = model_name.replace(\'_\', \'\') + \'model\'\n    for name, cls in modellib.__dict__.items():\n        if name.lower() == target_model_name.lower() \\\n           and issubclass(cls, torch.nn.Module):\n            model = cls\n\n    if model is None:\n        print(""In %s.py, there should be a subclass of torch.nn.Module with class name that matches %s in lowercase."" % (model_filename, target_model_name))\n        exit(0)\n\n    return model\n\n\ndef get_option_setter(model_name):\n    model_class = find_model_using_name(model_name)\n    return model_class.modify_commandline_options\n\n\ndef create_model(opt):\n    model = find_model_using_name(opt.model)\n    instance = model(opt)\n    print(""model [%s] was created"" % (type(instance).__name__))\n\n    return instance\n'"
semantic_synthesis/models/pix2pix_model.py,17,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport torch\nimport models.networks as networks\nimport util.util as util\n\n\nclass Pix2PixModel(torch.nn.Module):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        networks.modify_commandline_options(parser, is_train)\n        return parser\n\n    def __init__(self, opt):\n        super().__init__()\n        self.opt = opt\n        self.FloatTensor = torch.cuda.FloatTensor if self.use_gpu() \\\n            else torch.FloatTensor\n        self.ByteTensor = torch.cuda.ByteTensor if self.use_gpu() \\\n            else torch.ByteTensor\n\n        self.netG, self.netD, self.netE = self.initialize_networks(opt)\n\n        # set loss functions\n        if opt.isTrain:\n            self.criterionGAN = networks.GANLoss(opt.gan_mode, tensor=self.FloatTensor, opt=self.opt)\n            self.criterionFeat = torch.nn.L1Loss()\n            self.criterionL1 = torch.nn.L1Loss()\n            if not opt.no_vgg_loss:\n                self.criterionVGG = networks.VGGLoss(self.opt.gpu_ids)\n            if opt.use_vae:\n                self.KLDLoss = networks.KLDLoss()\n\n    # Entry point for all calls involving forward pass\n    # of deep networks. We used this approach since DataParallel module\n    # can\'t parallelize custom functions, we branch to different\n    # routines based on |mode|.\n    def forward(self, data, mode):\n        input_semantics, real_image = self.preprocess_input(data)\n\n        if mode == \'generator\':\n            g_loss, generated = self.compute_generator_loss(\n                input_semantics, real_image)\n            return g_loss, generated\n        elif mode == \'discriminator\':\n            d_loss = self.compute_discriminator_loss(\n                input_semantics, real_image)\n            return d_loss\n        elif mode == \'encode_only\':\n            z, mu, logvar = self.encode_z(real_image)\n            return mu, logvar\n        elif mode == \'inference\':\n            with torch.no_grad():\n                fake_image, _ = self.generate_fake(input_semantics, real_image)\n            return fake_image\n        else:\n            raise ValueError(""|mode| is invalid"")\n\n    def create_optimizers(self, opt):\n        G_params = list(self.netG.parameters())\n        if opt.use_vae:\n            G_params += list(self.netE.parameters())\n        if opt.isTrain:\n            D_params = list(self.netD.parameters())\n\n        beta1, beta2 = opt.beta1, opt.beta2\n        if opt.no_TTUR:\n            G_lr, D_lr = opt.lr, opt.lr\n        else:\n            G_lr, D_lr = opt.lr / 2, opt.lr * 2\n\n        optimizer_G = torch.optim.Adam(G_params, lr=G_lr, betas=(beta1, beta2))\n        optimizer_D = torch.optim.Adam(D_params, lr=D_lr, betas=(beta1, beta2))\n\n        return optimizer_G, optimizer_D\n\n    def save(self, epoch):\n        util.save_network(self.netG, \'G\', epoch, self.opt)\n        util.save_network(self.netD, \'D\', epoch, self.opt)\n        if self.opt.use_vae:\n            util.save_network(self.netE, \'E\', epoch, self.opt)\n\n    ############################################################################\n    # Private helper methods\n    ############################################################################\n\n    def initialize_networks(self, opt):\n        netG = networks.define_G(opt)\n        netD = networks.define_D(opt) if opt.isTrain else None\n        netE = networks.define_E(opt) if opt.use_vae else None\n\n        if not opt.isTrain or opt.continue_train:\n            netG = util.load_network(netG, \'G\', opt.which_epoch, opt)\n            if opt.isTrain:\n                netD = util.load_network(netD, \'D\', opt.which_epoch, opt)\n            if opt.use_vae:\n                netE = util.load_network(netE, \'E\', opt.which_epoch, opt)\n\n        return netG, netD, netE\n\n    # preprocess the input, such as moving the tensors to GPUs and\n    # transforming the label map to one-hot encoding\n    # |data|: dictionary of the input data\n\n    def preprocess_input(self, data):\n        # move to GPU and change data types\n        data[\'label\'] = data[\'label\'].long()\n        if self.use_gpu():\n            data[\'label\'] = data[\'label\'].cuda()\n            data[\'instance\'] = data[\'instance\'].cuda()\n            data[\'image\'] = data[\'image\'].cuda()\n\n        # create one-hot label map\n        label_map = data[\'label\']\n        bs, _, h, w = label_map.size()\n        nc = self.opt.label_nc + 1 if self.opt.contain_dontcare_label \\\n            else self.opt.label_nc\n        input_label = self.FloatTensor(bs, nc, h, w).zero_()\n        input_semantics = input_label.scatter_(1, label_map, 1.0)\n\n        # concatenate instance map if it exists\n        if not self.opt.no_instance:\n            inst_map = data[\'instance\']\n            instance_edge_map = self.get_edges(inst_map)\n            input_semantics = torch.cat((input_semantics, instance_edge_map), dim=1)\n\n        return input_semantics, data[\'image\']\n\n    def compute_generator_loss(self, input_semantics, real_image):\n        G_losses = {}\n\n        fake_image, KLD_loss = self.generate_fake(\n            input_semantics, real_image, compute_kld_loss=self.opt.use_vae)\n\n        if self.opt.use_vae:\n            G_losses[\'KLD\'] = KLD_loss\n\n        pred_fake, pred_real = self.discriminate(\n            input_semantics, fake_image, real_image)\n\n        G_losses[\'GAN\'] = self.criterionGAN(pred_fake, True,\n                                            for_discriminator=False)\n\n        if not self.opt.no_ganFeat_loss:\n            num_D = len(pred_fake)\n            GAN_Feat_loss = self.FloatTensor(1).fill_(0)\n            for i in range(num_D):  # for each discriminator\n                # last output is the final prediction, so we exclude it\n                num_intermediate_outputs = len(pred_fake[i]) - 1\n                for j in range(num_intermediate_outputs):  # for each layer output\n                    unweighted_loss = self.criterionFeat(\n                        pred_fake[i][j], pred_real[i][j].detach())\n                    GAN_Feat_loss += unweighted_loss * self.opt.lambda_feat / num_D\n            G_losses[\'GAN_Feat\'] = GAN_Feat_loss\n\n        if not self.opt.no_vgg_loss:\n            G_losses[\'VGG\'] = self.criterionVGG(fake_image, real_image) \\\n                * self.opt.lambda_vgg\n\n        if not self.opt.no_l1_loss:\n            G_losses[\'L1\'] = self.criterionL1(fake_image, real_image) * 10\n\n        return G_losses, fake_image\n\n    def compute_discriminator_loss(self, input_semantics, real_image):\n        D_losses = {}\n        with torch.no_grad():\n            fake_image, _ = self.generate_fake(input_semantics, real_image)\n            fake_image = fake_image.detach()\n            fake_image.requires_grad_()\n\n        pred_fake, pred_real = self.discriminate(\n            input_semantics, fake_image, real_image)\n\n        D_losses[\'D_Fake\'] = self.criterionGAN(pred_fake, False,\n                                               for_discriminator=True)\n        D_losses[\'D_real\'] = self.criterionGAN(pred_real, True,\n                                               for_discriminator=True)\n\n        return D_losses\n\n    def encode_z(self, real_image):\n        mu, logvar = self.netE(real_image)\n        z = self.reparameterize(mu, logvar)\n        return z, mu, logvar\n\n    def generate_fake(self, input_semantics, real_image, compute_kld_loss=False):\n        z = None\n        KLD_loss = None\n        if self.opt.use_vae:\n            z, mu, logvar = self.encode_z(real_image)\n            if compute_kld_loss:\n                KLD_loss = self.KLDLoss(mu, logvar) * self.opt.lambda_kld\n\n        fake_image = self.netG(input_semantics, z=z)\n\n        assert (not compute_kld_loss) or self.opt.use_vae, \\\n            ""You cannot compute KLD loss if opt.use_vae == False""\n\n        return fake_image, KLD_loss\n\n    # Given fake and real image, return the prediction of discriminator\n    # for each fake and real image.\n\n    def discriminate(self, input_semantics, fake_image, real_image):\n        fake_concat = torch.cat([input_semantics, fake_image], dim=1)\n        real_concat = torch.cat([input_semantics, real_image], dim=1)\n\n        # In Batch Normalization, the fake and real images are\n        # recommended to be in the same batch to avoid disparate\n        # statistics in fake and real images.\n        # So both fake and real images are fed to D all at once.\n        fake_and_real = torch.cat([fake_concat, real_concat], dim=0)\n\n        discriminator_out = self.netD(fake_and_real)\n\n        pred_fake, pred_real = self.divide_pred(discriminator_out)\n\n        return pred_fake, pred_real\n\n    # Take the prediction of fake and real images from the combined batch\n    def divide_pred(self, pred):\n        # the prediction contains the intermediate outputs of multiscale GAN,\n        # so it\'s usually a list\n        if type(pred) == list:\n            fake = []\n            real = []\n            for p in pred:\n                fake.append([tensor[:tensor.size(0) // 2] for tensor in p])\n                real.append([tensor[tensor.size(0) // 2:] for tensor in p])\n        else:\n            fake = pred[:pred.size(0) // 2]\n            real = pred[pred.size(0) // 2:]\n\n        return fake, real\n\n    def get_edges(self, t):\n        edge = self.ByteTensor(t.size()).zero_()\n        edge[:, :, :, 1:] = edge[:, :, :, 1:] | (t[:, :, :, 1:] != t[:, :, :, :-1])\n        edge[:, :, :, :-1] = edge[:, :, :, :-1] | (t[:, :, :, 1:] != t[:, :, :, :-1])\n        edge[:, :, 1:, :] = edge[:, :, 1:, :] | (t[:, :, 1:, :] != t[:, :, :-1, :])\n        edge[:, :, :-1, :] = edge[:, :, :-1, :] | (t[:, :, 1:, :] != t[:, :, :-1, :])\n        return edge.float()\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return eps.mul(std) + mu\n\n    def use_gpu(self):\n        return len(self.opt.gpu_ids) > 0\n'"
semantic_synthesis/options/__init__.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""'"
semantic_synthesis/options/base_options.py,1,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport sys\nimport argparse\nimport os\nfrom util import util\nimport torch\nimport models\nimport data\nimport pickle\n\n\nclass BaseOptions():\n    def __init__(self):\n        self.initialized = False\n\n    def initialize(self, parser):\n        # experiment specifics\n        parser.add_argument(\'--name\', type=str, default=\'label2coco\', help=\'name of the experiment. It decides where to store samples and models\')\n\n        parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU\')\n        parser.add_argument(\'--checkpoints_dir\', type=str, default=\'./checkpoints\', help=\'models are saved here\')\n        parser.add_argument(\'--model\', type=str, default=\'pix2pix\', help=\'which model to use\')\n        parser.add_argument(\'--norm_G\', type=str, default=\'spectralinstance\', help=\'instance normalization or batch normalization\')\n        parser.add_argument(\'--norm_D\', type=str, default=\'spectralinstance\', help=\'instance normalization or batch normalization\')\n        parser.add_argument(\'--norm_E\', type=str, default=\'spectralinstance\', help=\'instance normalization or batch normalization\')\n        parser.add_argument(\'--phase\', type=str, default=\'train\', help=\'train, val, test, etc\')\n\n        # input/output sizes\n        parser.add_argument(\'--batchSize\', type=int, default=1, help=\'input batch size\')\n        parser.add_argument(\'--preprocess_mode\', type=str, default=\'scale_width_and_crop\', help=\'scaling and cropping of images at load time.\', choices=(""resize_and_crop"", ""crop"", ""scale_width"", ""scale_width_and_crop"", ""scale_shortside"", ""scale_shortside_and_crop"", ""fixed"", ""none""))\n        parser.add_argument(\'--load_size\', type=int, default=1024, help=\'Scale images to this size. The final image will be cropped to --crop_size.\')\n        parser.add_argument(\'--crop_size\', type=int, default=512, help=\'Crop to the width of crop_size (after initially scaling the images to load_size.)\')\n        parser.add_argument(\'--aspect_ratio\', type=float, default=1.0, help=\'The ratio width/height. The final height of the load image will be crop_size/aspect_ratio\')\n        parser.add_argument(\'--label_nc\', type=int, default=182, help=\'# of input label classes without unknown class. If you have unknown class as class label, specify --contain_dopntcare_label.\')\n        parser.add_argument(\'--contain_dontcare_label\', action=\'store_true\', help=\'if the label map contains dontcare label (dontcare=255)\')\n        parser.add_argument(\'--output_nc\', type=int, default=3, help=\'# of output image channels\')\n\n        # for setting inputs\n        parser.add_argument(\'--dataroot\', type=str, default=\'./datasets/cityscapes/\')\n        parser.add_argument(\'--dataset_mode\', type=str, default=\'coco\')\n        parser.add_argument(\'--serial_batches\', action=\'store_true\', help=\'if true, takes images in order to make batches, otherwise takes them randomly\')\n        parser.add_argument(\'--no_flip\', action=\'store_true\', help=\'if specified, do not flip the images for data argumentation\')\n        parser.add_argument(\'--nThreads\', default=0, type=int, help=\'# threads for loading data\')\n        parser.add_argument(\'--max_dataset_size\', type=int, default=sys.maxsize, help=\'Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.\')\n        parser.add_argument(\'--load_from_opt_file\', action=\'store_true\', help=\'load the options from checkpoints and use that as default\')\n        parser.add_argument(\'--cache_filelist_write\', action=\'store_true\', help=\'saves the current filelist into a text file, so that it loads faster\')\n        parser.add_argument(\'--cache_filelist_read\', action=\'store_true\', help=\'reads from the file list cache\')\n\n        # for displays\n        parser.add_argument(\'--display_winsize\', type=int, default=400, help=\'display window size\')\n\n        # for generator\n        parser.add_argument(\'--netG\', type=str, default=\'spade\', help=\'selects model to use for netG (pix2pixhd | spade)\')\n        parser.add_argument(\'--ngf\', type=int, default=64, help=\'# of gen filters in first conv layer\')\n        parser.add_argument(\'--init_type\', type=str, default=\'xavier\', help=\'network initialization [normal|xavier|kaiming|orthogonal]\')\n        parser.add_argument(\'--init_variance\', type=float, default=0.02, help=\'variance of the initialization distribution\')\n        parser.add_argument(\'--z_dim\', type=int, default=256,\n                            help=""dimension of the latent z vector"")\n\n        # for instance-wise features\n        parser.add_argument(\'--no_instance\', action=\'store_true\', help=\'if specified, do *not* add instance map as input\')\n        parser.add_argument(\'--nef\', type=int, default=16, help=\'# of encoder filters in the first conv layer\')\n        parser.add_argument(\'--use_vae\', action=\'store_true\', help=\'enable training with an image encoder.\')\n\n        self.initialized = True\n        return parser\n\n    def gather_options(self):\n        # initialize parser with basic options\n        if not self.initialized:\n            parser = argparse.ArgumentParser(\n                formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n            parser = self.initialize(parser)\n\n        # get the basic options\n        opt, unknown = parser.parse_known_args()\n\n        # modify model-related parser options\n        model_name = opt.model\n        model_option_setter = models.get_option_setter(model_name)\n        parser = model_option_setter(parser, self.isTrain)\n\n        # modify dataset-related parser options\n        dataset_mode = opt.dataset_mode\n        dataset_option_setter = data.get_option_setter(dataset_mode)\n        parser = dataset_option_setter(parser, self.isTrain)\n\n        opt, unknown = parser.parse_known_args()\n\n        # if there is opt_file, load it.\n        # The previous default options will be overwritten\n        if opt.load_from_opt_file:\n            parser = self.update_options_from_file(parser, opt)\n\n        opt = parser.parse_args()\n        self.parser = parser\n        return opt\n\n    def print_options(self, opt):\n        message = \'\'\n        message += \'----------------- Options ---------------\\n\'\n        for k, v in sorted(vars(opt).items()):\n            comment = \'\'\n            default = self.parser.get_default(k)\n            if v != default:\n                comment = \'\\t[default: %s]\' % str(default)\n            message += \'{:>25}: {:<30}{}\\n\'.format(str(k), str(v), comment)\n        message += \'----------------- End -------------------\'\n        print(message)\n\n    def option_file_path(self, opt, makedir=False):\n        expr_dir = os.path.join(opt.checkpoints_dir, opt.name)\n        if makedir:\n            util.mkdirs(expr_dir)\n        file_name = os.path.join(expr_dir, \'opt\')\n        return file_name\n\n    def save_options(self, opt):\n        file_name = self.option_file_path(opt, makedir=True)\n        with open(file_name + \'.txt\', \'wt\') as opt_file:\n            for k, v in sorted(vars(opt).items()):\n                comment = \'\'\n                default = self.parser.get_default(k)\n                if v != default:\n                    comment = \'\\t[default: %s]\' % str(default)\n                opt_file.write(\'{:>25}: {:<30}{}\\n\'.format(str(k), str(v), comment))\n\n        with open(file_name + \'.pkl\', \'wb\') as opt_file:\n            pickle.dump(opt, opt_file)\n\n    def update_options_from_file(self, parser, opt):\n        new_opt = self.load_options(opt)\n        for k, v in sorted(vars(opt).items()):\n            if hasattr(new_opt, k) and v != getattr(new_opt, k):\n                new_val = getattr(new_opt, k)\n                parser.set_defaults(**{k: new_val})\n        return parser\n\n    def load_options(self, opt):\n        file_name = self.option_file_path(opt, makedir=False)\n        new_opt = pickle.load(open(file_name + \'.pkl\', \'rb\'))\n        return new_opt\n\n    def parse(self, save=False):\n\n        opt = self.gather_options()\n        opt.isTrain = self.isTrain   # train or test\n\n        self.print_options(opt)\n        if opt.isTrain:\n            self.save_options(opt)\n\n        # Set semantic_nc based on the option.\n        # This will be convenient in many places\n        opt.semantic_nc = opt.label_nc + \\\n            (1 if opt.contain_dontcare_label else 0) + \\\n            (0 if opt.no_instance else 1)\n\n        # set gpu ids\n        str_ids = opt.gpu_ids.split(\',\')\n        opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                opt.gpu_ids.append(id)\n        if len(opt.gpu_ids) > 0:\n            torch.cuda.set_device(opt.gpu_ids[0])\n\n        assert len(opt.gpu_ids) == 0 or opt.batchSize % len(opt.gpu_ids) == 0, \\\n            ""Batch size %d is wrong. It must be a multiple of # GPUs %d."" \\\n            % (opt.batchSize, len(opt.gpu_ids))\n\n        self.opt = opt\n        return self.opt\n'"
semantic_synthesis/options/test_options.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nfrom .base_options import BaseOptions\n\n\nclass TestOptions(BaseOptions):\n    def initialize(self, parser):\n        BaseOptions.initialize(self, parser)\n        parser.add_argument(\'--results_dir\', type=str, default=\'./results/\', help=\'saves results here.\')\n        parser.add_argument(\'--which_epoch\', type=str, default=\'latest\', help=\'which epoch to load? set to latest to use latest cached model\')\n        parser.add_argument(\'--how_many\', type=int, default=float(""inf""), help=\'how many test images to run\')\n\n        parser.set_defaults(preprocess_mode=\'scale_width_and_crop\', crop_size=256, load_size=256, display_winsize=256)\n        parser.set_defaults(serial_batches=True)\n        parser.set_defaults(no_flip=True)\n        parser.set_defaults(phase=\'test\')\n        self.isTrain = False\n        return parser\n'"
semantic_synthesis/options/train_options.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nfrom .base_options import BaseOptions\n\n\nclass TrainOptions(BaseOptions):\n    def initialize(self, parser):\n        BaseOptions.initialize(self, parser)\n        # for displays\n        parser.add_argument(\'--display_freq\', type=int, default=100, help=\'frequency of showing training results on screen\')\n        parser.add_argument(\'--print_freq\', type=int, default=100, help=\'frequency of showing training results on console\')\n        parser.add_argument(\'--save_latest_freq\', type=int, default=5000, help=\'frequency of saving the latest results\')\n        parser.add_argument(\'--save_epoch_freq\', type=int, default=10, help=\'frequency of saving checkpoints at the end of epochs\')\n        parser.add_argument(\'--no_html\', action=\'store_true\', help=\'do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/\')\n        parser.add_argument(\'--debug\', action=\'store_true\', help=\'only do one epoch and displays at each iteration\')\n        parser.add_argument(\'--tf_log\', action=\'store_true\', help=\'if specified, use tensorboard logging. Requires tensorflow installed\')\n\n        # for training\n        parser.add_argument(\'--continue_train\', action=\'store_true\', help=\'continue training: load the latest model\')\n        parser.add_argument(\'--which_epoch\', type=str, default=\'latest\', help=\'which epoch to load? set to latest to use latest cached model\')\n        parser.add_argument(\'--niter\', type=int, default=50, help=\'# of iter at starting learning rate. This is NOT the total #epochs. Totla #epochs is niter + niter_decay\')\n        parser.add_argument(\'--niter_decay\', type=int, default=0, help=\'# of iter to linearly decay learning rate to zero\')\n        parser.add_argument(\'--optimizer\', type=str, default=\'adam\')\n        parser.add_argument(\'--beta1\', type=float, default=0.0, help=\'momentum term of adam\')\n        parser.add_argument(\'--beta2\', type=float, default=0.9, help=\'momentum term of adam\')\n        parser.add_argument(\'--no_TTUR\', action=\'store_true\', help=\'Use TTUR training scheme\')\n\n        # the default values for beta1 and beta2 differ by TTUR option\n        opt, _ = parser.parse_known_args()\n        if opt.no_TTUR:\n            parser.set_defaults(beta1=0.5, beta2=0.999)\n\n        parser.add_argument(\'--lr\', type=float, default=0.0002, help=\'initial learning rate for adam\')\n        parser.add_argument(\'--D_steps_per_G\', type=int, default=1, help=\'number of discriminator iterations per generator iterations.\')\n\n        # for discriminators\n        parser.add_argument(\'--ndf\', type=int, default=64, help=\'# of discrim filters in first conv layer\')\n        parser.add_argument(\'--lambda_feat\', type=float, default=10.0, help=\'weight for feature matching loss\')\n        parser.add_argument(\'--lambda_vgg\', type=float, default=10.0, help=\'weight for vgg loss\')\n        parser.add_argument(\'--no_l1_loss\', action=\'store_true\', help=\'if specified, do *not* use pixel loss\')\n        parser.add_argument(\'--no_ganFeat_loss\', action=\'store_true\', help=\'if specified, do *not* use discriminator feature matching loss\')\n        parser.add_argument(\'--no_vgg_loss\', action=\'store_true\', help=\'if specified, do *not* use VGG feature matching loss\')\n        parser.add_argument(\'--gan_mode\', type=str, default=\'hinge\', help=\'(ls|original|hinge)\')\n        parser.add_argument(\'--netD\', type=str, default=\'multiscale\', help=\'(n_layers|multiscale|image)\')\n        parser.add_argument(\'--lambda_kld\', type=float, default=0.05)\n        self.isTrain = True\n        return parser\n'"
semantic_synthesis/trainers/__init__.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n'"
semantic_synthesis/trainers/pix2pix_trainer.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nfrom models.networks.sync_batchnorm import DataParallelWithCallback\nfrom models.pix2pix_model import Pix2PixModel\n\n\nclass Pix2PixTrainer():\n    """"""\n    Trainer creates the model and optimizers, and uses them to\n    updates the weights of the network while reporting losses\n    and the latest visuals to visualize the progress in training.\n    """"""\n\n    def __init__(self, opt):\n        self.opt = opt\n        self.pix2pix_model = Pix2PixModel(opt)\n        if len(opt.gpu_ids) > 0:\n            self.pix2pix_model = DataParallelWithCallback(self.pix2pix_model,\n                                                          device_ids=opt.gpu_ids)\n            self.pix2pix_model_on_one_gpu = self.pix2pix_model.module\n        else:\n            self.pix2pix_model_on_one_gpu = self.pix2pix_model\n\n        self.generated = None\n        if opt.isTrain:\n            self.optimizer_G, self.optimizer_D = \\\n                self.pix2pix_model_on_one_gpu.create_optimizers(opt)\n            self.old_lr = opt.lr\n\n    def run_generator_one_step(self, data):\n        self.optimizer_G.zero_grad()\n        g_losses, generated = self.pix2pix_model(data, mode=\'generator\')\n        g_loss = sum(g_losses.values()).mean()\n        g_loss.backward()\n        self.optimizer_G.step()\n        self.g_losses = g_losses\n        self.generated = generated\n\n    def run_discriminator_one_step(self, data):\n        self.optimizer_D.zero_grad()\n        d_losses = self.pix2pix_model(data, mode=\'discriminator\')\n        d_loss = sum(d_losses.values()).mean()\n        d_loss.backward()\n        self.optimizer_D.step()\n        self.d_losses = d_losses\n\n    def get_latest_losses(self):\n        return {**self.g_losses, **self.d_losses}\n\n    def get_latest_generated(self):\n        return self.generated\n\n    def update_learning_rate(self, epoch):\n        self.update_learning_rate(epoch)\n\n    def save(self, epoch):\n        self.pix2pix_model_on_one_gpu.save(epoch)\n\n    ##################################################################\n    # Helper functions\n    ##################################################################\n\n    def update_learning_rate(self, epoch):\n        if epoch > self.opt.niter:\n            lrd = self.opt.lr / self.opt.niter_decay\n            new_lr = self.old_lr - lrd\n        else:\n            new_lr = self.old_lr\n\n        if new_lr != self.old_lr:\n            if self.opt.no_TTUR:\n                new_lr_G = new_lr\n                new_lr_D = new_lr\n            else:\n                new_lr_G = new_lr / 2\n                new_lr_D = new_lr * 2\n\n            for param_group in self.optimizer_D.param_groups:\n                param_group[\'lr\'] = new_lr_D\n            for param_group in self.optimizer_G.param_groups:\n                param_group[\'lr\'] = new_lr_G\n            print(\'update learning rate: %f -> %f\' % (self.old_lr, new_lr))\n            self.old_lr = new_lr\n'"
semantic_synthesis/util/__init__.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n'"
semantic_synthesis/util/coco.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\n\ndef id2label(id):\n    if id == 182:\n        id = 0\n    else:\n        id = id + 1\n    labelmap = \\\n        {0: \'unlabeled\',\n         1: \'person\',\n         2: \'bicycle\',\n         3: \'car\',\n         4: \'motorcycle\',\n         5: \'airplane\',\n         6: \'bus\',\n         7: \'train\',\n         8: \'truck\',\n         9: \'boat\',\n         10: \'traffic light\',\n         11: \'fire hydrant\',\n         12: \'street sign\',\n         13: \'stop sign\',\n         14: \'parking meter\',\n         15: \'bench\',\n         16: \'bird\',\n         17: \'cat\',\n         18: \'dog\',\n         19: \'horse\',\n         20: \'sheep\',\n         21: \'cow\',\n         22: \'elephant\',\n         23: \'bear\',\n         24: \'zebra\',\n         25: \'giraffe\',\n         26: \'hat\',\n         27: \'backpack\',\n         28: \'umbrella\',\n         29: \'shoe\',\n         30: \'eye glasses\',\n         31: \'handbag\',\n         32: \'tie\',\n         33: \'suitcase\',\n         34: \'frisbee\',\n         35: \'skis\',\n         36: \'snowboard\',\n         37: \'sports ball\',\n         38: \'kite\',\n         39: \'baseball bat\',\n         40: \'baseball glove\',\n         41: \'skateboard\',\n         42: \'surfboard\',\n         43: \'tennis racket\',\n         44: \'bottle\',\n         45: \'plate\',\n         46: \'wine glass\',\n         47: \'cup\',\n         48: \'fork\',\n         49: \'knife\',\n         50: \'spoon\',\n         51: \'bowl\',\n         52: \'banana\',\n         53: \'apple\',\n         54: \'sandwich\',\n         55: \'orange\',\n         56: \'broccoli\',\n         57: \'carrot\',\n         58: \'hot dog\',\n         59: \'pizza\',\n         60: \'donut\',\n         61: \'cake\',\n         62: \'chair\',\n         63: \'couch\',\n         64: \'potted plant\',\n         65: \'bed\',\n         66: \'mirror\',\n         67: \'dining table\',\n         68: \'window\',\n         69: \'desk\',\n         70: \'toilet\',\n         71: \'door\',\n         72: \'tv\',\n         73: \'laptop\',\n         74: \'mouse\',\n         75: \'remote\',\n         76: \'keyboard\',\n         77: \'cell phone\',\n         78: \'microwave\',\n         79: \'oven\',\n         80: \'toaster\',\n         81: \'sink\',\n         82: \'refrigerator\',\n         83: \'blender\',\n         84: \'book\',\n         85: \'clock\',\n         86: \'vase\',\n         87: \'scissors\',\n         88: \'teddy bear\',\n         89: \'hair drier\',\n         90: \'toothbrush\',\n         91: \'hair brush\',  # Last class of Thing\n         92: \'banner\',  # Beginning of Stuff\n         93: \'blanket\',\n         94: \'branch\',\n         95: \'bridge\',\n         96: \'building-other\',\n         97: \'bush\',\n         98: \'cabinet\',\n         99: \'cage\',\n         100: \'cardboard\',\n         101: \'carpet\',\n         102: \'ceiling-other\',\n         103: \'ceiling-tile\',\n         104: \'cloth\',\n         105: \'clothes\',\n         106: \'clouds\',\n         107: \'counter\',\n         108: \'cupboard\',\n         109: \'curtain\',\n         110: \'desk-stuff\',\n         111: \'dirt\',\n         112: \'door-stuff\',\n         113: \'fence\',\n         114: \'floor-marble\',\n         115: \'floor-other\',\n         116: \'floor-stone\',\n         117: \'floor-tile\',\n         118: \'floor-wood\',\n         119: \'flower\',\n         120: \'fog\',\n         121: \'food-other\',\n         122: \'fruit\',\n         123: \'furniture-other\',\n         124: \'grass\',\n         125: \'gravel\',\n         126: \'ground-other\',\n         127: \'hill\',\n         128: \'house\',\n         129: \'leaves\',\n         130: \'light\',\n         131: \'mat\',\n         132: \'metal\',\n         133: \'mirror-stuff\',\n         134: \'moss\',\n         135: \'mountain\',\n         136: \'mud\',\n         137: \'napkin\',\n         138: \'net\',\n         139: \'paper\',\n         140: \'pavement\',\n         141: \'pillow\',\n         142: \'plant-other\',\n         143: \'plastic\',\n         144: \'platform\',\n         145: \'playingfield\',\n         146: \'railing\',\n         147: \'railroad\',\n         148: \'river\',\n         149: \'road\',\n         150: \'rock\',\n         151: \'roof\',\n         152: \'rug\',\n         153: \'salad\',\n         154: \'sand\',\n         155: \'sea\',\n         156: \'shelf\',\n         157: \'sky-other\',\n         158: \'skyscraper\',\n         159: \'snow\',\n         160: \'solid-other\',\n         161: \'stairs\',\n         162: \'stone\',\n         163: \'straw\',\n         164: \'structural-other\',\n         165: \'table\',\n         166: \'tent\',\n         167: \'textile-other\',\n         168: \'towel\',\n         169: \'tree\',\n         170: \'vegetable\',\n         171: \'wall-brick\',\n         172: \'wall-concrete\',\n         173: \'wall-other\',\n         174: \'wall-panel\',\n         175: \'wall-stone\',\n         176: \'wall-tile\',\n         177: \'wall-wood\',\n         178: \'water-other\',\n         179: \'waterdrops\',\n         180: \'window-blind\',\n         181: \'window-other\',\n         182: \'wood\'}\n    if id in labelmap:\n        return labelmap[id]\n    else:\n        return \'unknown\'\n'"
semantic_synthesis/util/html.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport datetime\nimport dominate\nfrom dominate.tags import *\nimport os\n\n\nclass HTML:\n    def __init__(self, web_dir, title, refresh=0):\n        if web_dir.endswith(\'.html\'):\n            web_dir, html_name = os.path.split(web_dir)\n        else:\n            web_dir, html_name = web_dir, \'index.html\'\n        self.title = title\n        self.web_dir = web_dir\n        self.html_name = html_name\n        self.img_dir = os.path.join(self.web_dir, \'images\')\n        if len(self.web_dir) > 0 and not os.path.exists(self.web_dir):\n            os.makedirs(self.web_dir)\n        if len(self.web_dir) > 0 and not os.path.exists(self.img_dir):\n            os.makedirs(self.img_dir)\n\n        self.doc = dominate.document(title=title)\n        with self.doc:\n            h1(datetime.datetime.now().strftime(""%I:%M%p on %B %d, %Y""))\n        if refresh > 0:\n            with self.doc.head:\n                meta(http_equiv=""refresh"", content=str(refresh))\n\n    def get_image_dir(self):\n        return self.img_dir\n\n    def add_header(self, str):\n        with self.doc:\n            h3(str)\n\n    def add_table(self, border=1):\n        self.t = table(border=border, style=""table-layout: fixed;"")\n        self.doc.add(self.t)\n\n    def add_images(self, ims, txts, links, width=512):\n        self.add_table()\n        with self.t:\n            with tr():\n                for im, txt, link in zip(ims, txts, links):\n                    with td(style=""word-wrap: break-word;"", halign=""center"", valign=""top""):\n                        with p():\n                            with a(href=os.path.join(\'images\', link)):\n                                img(style=""width:%dpx"" % (width), src=os.path.join(\'images\', im))\n                            br()\n                            p(txt.encode(\'utf-8\'))\n\n    def save(self):\n        html_file = os.path.join(self.web_dir, self.html_name)\n        f = open(html_file, \'wt\')\n        f.write(self.doc.render())\n        f.close()\n\n\nif __name__ == \'__main__\':\n    html = HTML(\'web/\', \'test_html\')\n    html.add_header(\'hello world\')\n\n    ims = []\n    txts = []\n    links = []\n    for n in range(4):\n        ims.append(\'image_%d.jpg\' % n)\n        txts.append(\'text_%d\' % n)\n        links.append(\'image_%d.jpg\' % n)\n    html.add_images(ims, txts, links)\n    html.save()\n'"
semantic_synthesis/util/iter_counter.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport os\nimport time\nimport numpy as np\n\n\n# Helper class that keeps track of training iterations\nclass IterationCounter():\n    def __init__(self, opt, dataset_size):\n        self.opt = opt\n        self.dataset_size = dataset_size\n\n        self.first_epoch = 1\n        self.total_epochs = opt.niter + opt.niter_decay\n        self.epoch_iter = 0  # iter number within each epoch\n        self.iter_record_path = os.path.join(self.opt.checkpoints_dir, self.opt.name, \'iter.txt\')\n        if opt.isTrain and opt.continue_train:\n            try:\n                self.first_epoch, self.epoch_iter = np.loadtxt(\n                    self.iter_record_path, delimiter=\',\', dtype=int)\n                print(\'Resuming from epoch %d at iteration %d\' % (self.first_epoch, self.epoch_iter))\n            except:\n                print(\'Could not load iteration record at %s. Starting from beginning.\' %\n                      self.iter_record_path)\n\n        self.total_steps_so_far = (self.first_epoch - 1) * dataset_size + self.epoch_iter\n\n    # return the iterator of epochs for the training\n    def training_epochs(self):\n        return range(self.first_epoch, self.total_epochs + 1)\n\n    def record_epoch_start(self, epoch):\n        self.epoch_start_time = time.time()\n        self.epoch_iter = 0\n        self.last_iter_time = time.time()\n        self.current_epoch = epoch\n\n    def record_one_iteration(self):\n        current_time = time.time()\n\n        # the last remaining batch is dropped (see data/__init__.py),\n        # so we can assume batch size is always opt.batchSize\n        self.time_per_iter = (current_time - self.last_iter_time) / self.opt.batchSize\n        self.last_iter_time = current_time\n        self.total_steps_so_far += self.opt.batchSize\n        self.epoch_iter += self.opt.batchSize\n\n    def record_epoch_end(self):\n        current_time = time.time()\n        self.time_per_epoch = current_time - self.epoch_start_time\n        print(\'End of epoch %d / %d \\t Time Taken: %d sec\' %\n              (self.current_epoch, self.total_epochs, self.time_per_epoch))\n        if self.current_epoch % self.opt.save_epoch_freq == 0:\n            np.savetxt(self.iter_record_path, (self.current_epoch + 1, 0),\n                       delimiter=\',\', fmt=\'%d\')\n            print(\'Saved current iteration count at %s.\' % self.iter_record_path)\n\n    def record_current_iter(self):\n        np.savetxt(self.iter_record_path, (self.current_epoch, self.epoch_iter),\n                   delimiter=\',\', fmt=\'%d\')\n        print(\'Saved current iteration count at %s.\' % self.iter_record_path)\n\n    def needs_saving(self):\n        return (self.total_steps_so_far % self.opt.save_latest_freq) < self.opt.batchSize\n\n    def needs_printing(self):\n        return (self.total_steps_so_far % self.opt.print_freq) < self.opt.batchSize\n\n    def needs_displaying(self):\n        return (self.total_steps_so_far % self.opt.display_freq) < self.opt.batchSize\n'"
semantic_synthesis/util/util.py,5,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport re\nimport importlib\nimport torch\nfrom argparse import Namespace\nimport numpy as np\nfrom PIL import Image\nimport os\nimport argparse\nimport dill as pickle\nimport util.coco\n\n\ndef save_obj(obj, name):\n    with open(name, \'wb\') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n\n\ndef load_obj(name):\n    with open(name, \'rb\') as f:\n        return pickle.load(f)\n\n# returns a configuration for creating a generator\n# |default_opt| should be the opt of the current experiment\n# |**kwargs|: if any configuration should be overriden, it can be specified here\n\n\ndef copyconf(default_opt, **kwargs):\n    conf = argparse.Namespace(**vars(default_opt))\n    for key in kwargs:\n        print(key, kwargs[key])\n        setattr(conf, key, kwargs[key])\n    return conf\n\n\ndef tile_images(imgs, picturesPerRow=4):\n    """""" Code borrowed from\n    https://stackoverflow.com/questions/26521365/cleanly-tile-numpy-array-of-images-stored-in-a-flattened-1d-format/26521997\n    """"""\n\n    # Padding\n    if imgs.shape[0] % picturesPerRow == 0:\n        rowPadding = 0\n    else:\n        rowPadding = picturesPerRow - imgs.shape[0] % picturesPerRow\n    if rowPadding > 0:\n        imgs = np.concatenate([imgs, np.zeros((rowPadding, *imgs.shape[1:]), dtype=imgs.dtype)], axis=0)\n\n    # Tiling Loop (The conditionals are not necessary anymore)\n    tiled = []\n    for i in range(0, imgs.shape[0], picturesPerRow):\n        tiled.append(np.concatenate([imgs[j] for j in range(i, i + picturesPerRow)], axis=1))\n\n    tiled = np.concatenate(tiled, axis=0)\n    return tiled\n\n\n# Converts a Tensor into a Numpy array\n# |imtype|: the desired type of the converted numpy array\ndef tensor2im(image_tensor, imtype=np.uint8, normalize=True, tile=False):\n    if isinstance(image_tensor, list):\n        image_numpy = []\n        for i in range(len(image_tensor)):\n            image_numpy.append(tensor2im(image_tensor[i], imtype, normalize))\n        return image_numpy\n\n    if image_tensor.dim() == 4:\n        # transform each image in the batch\n        images_np = []\n        for b in range(image_tensor.size(0)):\n            one_image = image_tensor[b]\n            one_image_np = tensor2im(one_image)\n            images_np.append(one_image_np.reshape(1, *one_image_np.shape))\n        images_np = np.concatenate(images_np, axis=0)\n        if tile:\n            images_tiled = tile_images(images_np)\n            return images_tiled\n        else:\n            return images_np\n\n    if image_tensor.dim() == 2:\n        image_tensor = image_tensor.unsqueeze(0)\n    image_numpy = image_tensor.detach().cpu().float().numpy()\n    if normalize:\n        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n    else:\n        image_numpy = np.transpose(image_numpy, (1, 2, 0)) * 255.0\n    image_numpy = np.clip(image_numpy, 0, 255)\n    if image_numpy.shape[2] == 1:\n        image_numpy = image_numpy[:, :, 0]\n    return image_numpy.astype(imtype)\n\n\n# Converts a one-hot tensor into a colorful label map\ndef tensor2label(label_tensor, n_label, imtype=np.uint8, tile=False):\n    if label_tensor.dim() == 4:\n        # transform each image in the batch\n        images_np = []\n        for b in range(label_tensor.size(0)):\n            one_image = label_tensor[b]\n            one_image_np = tensor2label(one_image, n_label, imtype)\n            images_np.append(one_image_np.reshape(1, *one_image_np.shape))\n        images_np = np.concatenate(images_np, axis=0)\n        if tile:\n            images_tiled = tile_images(images_np)\n            return images_tiled\n        else:\n            images_np = images_np[0]\n            return images_np\n\n    if label_tensor.dim() == 1:\n        return np.zeros((64, 64, 3), dtype=np.uint8)\n    if n_label == 0:\n        return tensor2im(label_tensor, imtype)\n    label_tensor = label_tensor.cpu().float()\n    if label_tensor.size()[0] > 1:\n        label_tensor = label_tensor.max(0, keepdim=True)[1]\n    label_tensor = Colorize(n_label)(label_tensor)\n    label_numpy = np.transpose(label_tensor.numpy(), (1, 2, 0))\n    result = label_numpy.astype(imtype)\n    return result\n\n\ndef save_image(image_numpy, image_path, create_dir=False):\n    if create_dir:\n        os.makedirs(os.path.dirname(image_path), exist_ok=True)\n    if len(image_numpy.shape) == 2:\n        image_numpy = np.expand_dims(image_numpy, axis=2)\n    if image_numpy.shape[2] == 1:\n        image_numpy = np.repeat(image_numpy, 3, 2)\n    image_pil = Image.fromarray(image_numpy)\n\n    # save to png\n    image_pil.save(image_path.replace(\'.jpg\', \'.png\'))\n\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef atoi(text):\n    return int(text) if text.isdigit() else text\n\n\ndef natural_keys(text):\n    \'\'\'\n    alist.sort(key=natural_keys) sorts in human order\n    http://nedbatchelder.com/blog/200712/human_sorting.html\n    (See Toothy\'s implementation in the comments)\n    \'\'\'\n    return [atoi(c) for c in re.split(\'(\\d+)\', text)]\n\n\ndef natural_sort(items):\n    items.sort(key=natural_keys)\n\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\n\ndef find_class_in_module(target_cls_name, module):\n    target_cls_name = target_cls_name.replace(\'_\', \'\').lower()\n    clslib = importlib.import_module(module)\n    cls = None\n    for name, clsobj in clslib.__dict__.items():\n        if name.lower() == target_cls_name:\n            cls = clsobj\n\n    if cls is None:\n        print(""In %s, there should be a class whose name matches %s in lowercase without underscore(_)"" % (module, target_cls_name))\n        exit(0)\n\n    return cls\n\n\ndef save_network(net, label, epoch, opt):\n    save_filename = \'%s_net_%s.pth\' % (epoch, label)\n    save_path = os.path.join(opt.checkpoints_dir, opt.name, save_filename)\n    torch.save(net.cpu().state_dict(), save_path)\n    if len(opt.gpu_ids) and torch.cuda.is_available():\n        net.cuda()\n\n\ndef load_network(net, label, epoch, opt):\n    save_filename = \'%s_net_%s.pth\' % (epoch, label)\n    save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n    save_path = os.path.join(save_dir, save_filename)\n    weights = torch.load(save_path)\n    net.load_state_dict(weights)\n    return net\n\n\n###############################################################################\n# Code from\n# https://github.com/ycszen/pytorch-seg/blob/master/transform.py\n# Modified so it complies with the Citscape label map colors\n###############################################################################\ndef uint82bin(n, count=8):\n    """"""returns the binary of integer n, count refers to amount of bits""""""\n    return \'\'.join([str((n >> y) & 1) for y in range(count - 1, -1, -1)])\n\n\ndef labelcolormap(N):\n    if N == 35:  # cityscape\n        cmap = np.array([(0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (111, 74, 0), (81, 0, 81),\n                         (128, 64, 128), (244, 35, 232), (250, 170, 160), (230, 150, 140), (70, 70, 70), (102, 102, 156), (190, 153, 153),\n                         (180, 165, 180), (150, 100, 100), (150, 120, 90), (153, 153, 153), (153, 153, 153), (250, 170, 30), (220, 220, 0),\n                         (107, 142, 35), (152, 251, 152), (70, 130, 180), (220, 20, 60), (255, 0, 0), (0, 0, 142), (0, 0, 70),\n                         (0, 60, 100), (0, 0, 90), (0, 0, 110), (0, 80, 100), (0, 0, 230), (119, 11, 32), (0, 0, 142)],\n                        dtype=np.uint8)\n    else:\n        cmap = np.zeros((N, 3), dtype=np.uint8)\n        for i in range(N):\n            r, g, b = 0, 0, 0\n            id = i + 1  # let\'s give 0 a color\n            for j in range(7):\n                str_id = uint82bin(id)\n                r = r ^ (np.uint8(str_id[-1]) << (7 - j))\n                g = g ^ (np.uint8(str_id[-2]) << (7 - j))\n                b = b ^ (np.uint8(str_id[-3]) << (7 - j))\n                id = id >> 3\n            cmap[i, 0] = r\n            cmap[i, 1] = g\n            cmap[i, 2] = b\n\n        if N == 182:  # COCO\n            important_colors = {\n                \'sea\': (54, 62, 167),\n                \'sky-other\': (95, 219, 255),\n                \'tree\': (140, 104, 47),\n                \'clouds\': (170, 170, 170),\n                \'grass\': (29, 195, 49)\n            }\n            for i in range(N):\n                name = util.coco.id2label(i)\n                if name in important_colors:\n                    color = important_colors[name]\n                    cmap[i] = np.array(list(color))\n\n    return cmap\n\n\nclass Colorize(object):\n    def __init__(self, n=35):\n        self.cmap = labelcolormap(n)\n        self.cmap = torch.from_numpy(self.cmap[:n])\n\n    def __call__(self, gray_image):\n        size = gray_image.size()\n        color_image = torch.ByteTensor(3, size[1], size[2]).fill_(0)\n\n        for label in range(0, len(self.cmap)):\n            mask = (label == gray_image[0]).cpu()\n            color_image[0][mask] = self.cmap[label][0]\n            color_image[1][mask] = self.cmap[label][1]\n            color_image[2][mask] = self.cmap[label][2]\n\n        return color_image\n'"
semantic_synthesis/util/visualizer.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport os\nimport ntpath\nimport time\nfrom . import util\nfrom . import html\nimport scipy.misc\ntry:\n    from StringIO import StringIO  # Python 2.7\nexcept ImportError:\n    from io import BytesIO         # Python 3.x\n\nclass Visualizer():\n    def __init__(self, opt):\n        self.opt = opt\n        self.tf_log = opt.isTrain and opt.tf_log\n        self.use_html = opt.isTrain and not opt.no_html\n        self.win_size = opt.display_winsize\n        self.name = opt.name\n        if self.tf_log:\n            import tensorflow as tf\n            self.tf = tf\n            self.log_dir = os.path.join(opt.checkpoints_dir, opt.name, \'logs\')\n            self.writer = tf.summary.FileWriter(self.log_dir)\n\n        if self.use_html:\n            self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, \'web\')\n            self.img_dir = os.path.join(self.web_dir, \'images\')\n            print(\'create web directory %s...\' % self.web_dir)\n            util.mkdirs([self.web_dir, self.img_dir])\n        if opt.isTrain:\n            self.log_name = os.path.join(opt.checkpoints_dir, opt.name, \'loss_log.txt\')\n            with open(self.log_name, ""a"") as log_file:\n                now = time.strftime(""%c"")\n                log_file.write(\'================ Training Loss (%s) ================\\n\' % now)\n\n    # |visuals|: dictionary of images to display or save\n    def display_current_results(self, visuals, epoch, step):\n\n        ## convert tensors to numpy arrays\n        visuals = self.convert_visuals_to_numpy(visuals)\n                \n        if self.tf_log: # show images in tensorboard output\n            img_summaries = []\n            for label, image_numpy in visuals.items():\n                # Write the image to a string\n                try:\n                    s = StringIO()\n                except:\n                    s = BytesIO()\n                if len(image_numpy.shape) >= 4:\n                    image_numpy = image_numpy[0]\n                scipy.misc.toimage(image_numpy).save(s, format=""jpeg"")\n                # Create an Image object\n                img_sum = self.tf.Summary.Image(encoded_image_string=s.getvalue(), height=image_numpy.shape[0], width=image_numpy.shape[1])\n                # Create a Summary value\n                img_summaries.append(self.tf.Summary.Value(tag=label, image=img_sum))\n\n            # Create and write Summary\n            summary = self.tf.Summary(value=img_summaries)\n            self.writer.add_summary(summary, step)\n\n        if self.use_html: # save images to a html file\n            for label, image_numpy in visuals.items():\n                if isinstance(image_numpy, list):\n                    for i in range(len(image_numpy)):\n                        img_path = os.path.join(self.img_dir, \'epoch%.3d_iter%.3d_%s_%d.png\' % (epoch, step, label, i))\n                        util.save_image(image_numpy[i], img_path)\n                else:\n                    img_path = os.path.join(self.img_dir, \'epoch%.3d_iter%.3d_%s.png\' % (epoch, step, label))\n                    if len(image_numpy.shape) >= 4:\n                        image_numpy = image_numpy[0]                    \n                    util.save_image(image_numpy, img_path)\n\n            # update website\n            webpage = html.HTML(self.web_dir, \'Experiment name = %s\' % self.name, refresh=5)\n            for n in range(epoch, 0, -1):\n                webpage.add_header(\'epoch [%d]\' % n)\n                ims = []\n                txts = []\n                links = []\n\n                for label, image_numpy in visuals.items():\n                    if isinstance(image_numpy, list):\n                        for i in range(len(image_numpy)):\n                            img_path = \'epoch%.3d_iter%.3d_%s_%d.png\' % (n, step, label, i)\n                            ims.append(img_path)\n                            txts.append(label+str(i))\n                            links.append(img_path)\n                    else:\n                        img_path = \'epoch%.3d_iter%.3d_%s.png\' % (n, step, label)\n                        ims.append(img_path)\n                        txts.append(label)\n                        links.append(img_path)\n                if len(ims) < 10:\n                    webpage.add_images(ims, txts, links, width=self.win_size)\n                else:\n                    num = int(round(len(ims)/2.0))\n                    webpage.add_images(ims[:num], txts[:num], links[:num], width=self.win_size)\n                    webpage.add_images(ims[num:], txts[num:], links[num:], width=self.win_size)\n            webpage.save()\n\n    # errors: dictionary of error labels and values\n    def plot_current_errors(self, errors, step):\n        if self.tf_log:\n            for tag, value in errors.items():\n                value = value.mean().float()\n                summary = self.tf.Summary(value=[self.tf.Summary.Value(tag=tag, simple_value=value)])\n                self.writer.add_summary(summary, step)\n\n    # errors: same format as |errors| of plotCurrentErrors\n    def print_current_errors(self, epoch, i, errors, t):\n        message = \'(epoch: %d, iters: %d, time: %.3f) \' % (epoch, i, t)\n        for k, v in errors.items():\n            #print(v)\n            #if v != 0:\n            v = v.mean().float()\n            message += \'%s: %.3f \' % (k, v)\n\n        print(message)\n        with open(self.log_name, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n\n    def convert_visuals_to_numpy(self, visuals):\n        for key, t in visuals.items():\n            tile = self.opt.batchSize > 8\n            if \'input_label\' == key:\n                t = util.tensor2label(t, self.opt.label_nc + 2, tile=tile)\n            else:\n                t = util.tensor2im(t, tile=tile)\n            visuals[key] = t\n        return visuals\n\n    # save image to the disk\n    def save_images(self, webpage, visuals, image_path):        \n        visuals = self.convert_visuals_to_numpy(visuals)        \n        \n        image_dir = webpage.get_image_dir()\n        short_path = ntpath.basename(image_path[0])\n        name = os.path.splitext(short_path)[0]\n\n        webpage.add_header(name)\n        ims = []\n        txts = []\n        links = []\n\n        for label, image_numpy in visuals.items():\n            image_name = os.path.join(label, \'%s.png\' % (name))\n            save_path = os.path.join(image_dir, image_name)\n            util.save_image(image_numpy, save_path, create_dir=True)\n\n            ims.append(image_name)\n            txts.append(label)\n            links.append(image_name)\n        webpage.add_images(ims, txts, links, width=self.win_size)\n'"
selectiongan_v1/scripts/evaluation/KL_model_data.py,4,"b'# code derived from PlacesCNN for scene classification Bolei Zhou\n#\n\nimport sys\nimport torch\nfrom torch.autograd import Variable as V\nimport torchvision.models as models\nfrom torchvision import transforms as trn\nfrom torch.nn import functional as F\nimport os\nfrom PIL import Image\nimport numpy as np\n\n\ndef compute_KL_P_Q(P, Q):\n    P_log = np.log(P)\n    Q_log = np.log(Q)\n    log_diff_mult = P * (P_log - Q_log)\n    sum_along_y = np.sum(log_diff_mult,1)\n    mean_value = np.mean(sum_along_y)\n\n    scores = (np.exp(mean_value))\n    std_dev = np.std(sum_along_y)\n    print scores, std_dev\n\n\n# architecture to use\narch = \'alexnet\' #resnet50, alexnet\n\n# load the pre-trained weights\nmodel_weight = \'whole_%s_places365.pth.tar\' % arch\nif not os.access(model_weight, os.W_OK):\n    weight_url = \'http://places2.csail.mit.edu/models_places365/whole_%s_places365.pth.tar\' % arch\n    os.system(\'wget \' + weight_url)\n\nuseGPU = 0\nif useGPU == 1:\n    model = torch.load(model_weight)\nelse:\n    model = torch.load(model_weight, map_location=lambda storage, loc: storage) # model trained in GPU could be deployed in CPU machine like this!\n\nmodel.eval()\n\ncentre_crop = trn.Compose([\n        trn.Scale(256),\n        trn.CenterCrop(224),\n        trn.ToTensor(),\n        trn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# load the class label\nfile_name = \'categories_places365.txt\'\nif not os.access(file_name, os.W_OK):\n    synset_url = \'https://raw.githubusercontent.com/csailvision/places365/master/categories_places365.txt\'\n    os.system(\'wget \' + synset_url)\nclasses = list()\nwith open(file_name) as class_file:\n    for line in class_file:\n        classes.append(line.strip().split(\' \')[0][3:])\nclasses = tuple(classes)\n\n\n# load the test image\n\nimg_real =sys.argv[1]\nimg_synthesized =sys.argv[2]\n\npath, root, files = os.walk(img_synthesized).next()\nn = len(files)\nprint n\n\np_y_x_real = np.ndarray((n, 365), )\np_y_x_synthesized = np.ndarray((n, 365), )\n\nx = []\nfor i in range(n):\n    img_name = files[i]\n    \n    img_path_real =  img_real + \'/\' + img_name\n    img_path_synthesized =  img_synthesized + \'/\' + img_name\n    \n    im_real = Image.open(img_path_real)\n    im_synthesized = Image.open(img_path_synthesized)\n   \n    input_img_real = V(centre_crop(im_real).unsqueeze(0), volatile=True)\n    input_img_synthesized = V(centre_crop(im_synthesized).unsqueeze(0), volatile=True)\n\n    logit_real = model.forward(input_img_real)\n    h_x_real = F.softmax(logit_real).data.squeeze()\n    p_y_x_real[i] = h_x_real.numpy()\n\n\n    # forward pass\n    logit_synthesized = model.forward(input_img_synthesized)\n    h_x_synthesized = F.softmax(logit_synthesized).data.squeeze()\n    p_y_x_synthesized[i] = h_x_synthesized.numpy()\n\n    if(i%500==0):\n        print i\n\nprint ""\\nKL between model and data: ""\ncompute_KL_P_Q(p_y_x_synthesized, p_y_x_real)\n\n####################################################################################################################################\n'"
selectiongan_v1/scripts/evaluation/compute_accuracies.py,4,"b'# code derived from PlacesCNN for scene classification Bolei Zhou\n#\n\nimport sys\nimport torch\nfrom torch.autograd import Variable as V\nimport torchvision.models as models\nfrom torchvision import transforms as trn\nfrom torch.nn import functional as F\nimport os\nfrom PIL import Image\nimport numpy as np\n\n# th architecture to use\narch = \'alexnet\' #resnet50, alexnet, \n\n# load the pre-trained weights\nmodel_weight = \'whole_%s_places365.pth.tar\' % arch\nif not os.access(model_weight, os.W_OK):\n    weight_url = \'http://places2.csail.mit.edu/models_places365/whole_%s_places365.pth.tar\' % arch\n    os.system(\'wget \' + weight_url)\n\nuseGPU = 0\nif useGPU == 1:\n    model = torch.load(model_weight)\nelse:\n    model = torch.load(model_weight, map_location=lambda storage, loc: storage) # model trained in GPU could be deployed in CPU machine like this!\n\nmodel.eval()\n\ncentre_crop = trn.Compose([\n        trn.Scale(256),\n        trn.CenterCrop(224),\n        trn.ToTensor(),\n        trn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# load the class label\nfile_name = \'categories_places365.txt\'\nif not os.access(file_name, os.W_OK):\n    synset_url = \'https://raw.githubusercontent.com/csailvision/places365/master/categories_places365.txt\'\n    os.system(\'wget \' + synset_url)\nclasses = list()\nwith open(file_name) as class_file:\n    for line in class_file:\n        classes.append(line.strip().split(\' \')[0][3:])\nclasses = tuple(classes)\n\n\n\n# load the test image\nimg_real =sys.argv[1]\nimg_synthesized =sys.argv[2]\n\npath, root, files = os.walk(img_synthesized).next()\nn = len(files)\nprint n\n\ncounter_5_synthesized= 0\ncounter_1_synthesized = 0  \n\ncounter_5_synthesized_prob = 0\ncounter_1_synthesized_prob = 0  \n\n\nimg_num = 0\n \nfor i in range(n):\n    img_name = files[i]\n    \n    img_path_real =  img_real + \'/\' + img_name\n    img_path_synthesized =  img_synthesized + \'/\' + img_name\n\n    im_real = Image.open(img_path_real)\n    im_synthesized = Image.open(img_path_synthesized)\n    \n    input_img_real = V(centre_crop(im_real).unsqueeze(0), volatile=True)\n    input_img_synthesized = V(centre_crop(im_synthesized).unsqueeze(0), volatile=True)\n    \n\n    # forward pass real data\n    logit_real = model.forward(input_img_real)\n    h_x_real = F.softmax(logit_real).data.squeeze()\n    probs, idx = h_x_real.sort(0, True)\n    class_real_img = classes[idx[0]]\n\n\n    # forward pass synthesized \n    logit_synthesized = model.forward(input_img_synthesized)\n    h_x_synthesized = F.softmax(logit_synthesized).data.squeeze()\n    probs_synthesized, idx_synthesized = h_x_synthesized.sort(0, True)\n\n    # accuracies for synthesized model\n    if(classes[idx_synthesized[0]]== class_real_img):\n        counter_1_synthesized = counter_1_synthesized + 1\n    for j in range(0, 5):\n       if(classes[idx_synthesized[j]]== class_real_img):\n            counter_5_synthesized = counter_5_synthesized + 1\n\n\t\n    # accuracies when considering confidence score value\n    if (probs[0]>0.5):\n    \timg_num = img_num + 1\n        if(classes[idx_synthesized[0]]== class_real_img):\n           counter_1_synthesized_prob = counter_1_synthesized_prob + 1\n        for j in range(0, 5):\n           if(classes[idx_synthesized[j]]== class_real_img):\n               counter_5_synthesized_prob = counter_5_synthesized_prob + 1\n\n    if(i%500==0):\n        print i\n\nprint ""Total Images into consideration: "" + str(i+1) \nprint ""Total Match Found for synthesized : "" + str(counter_1_synthesized) + "" for N = 1 and "" + str(counter_5_synthesized) + "" for N = 5""\nprint ""\\nTotal Images into consideration (with prob > 0.5): "" + str(img_num) \nprint ""Total Match Found for synthesized : "" + str(counter_1_synthesized_prob) + "" for N = 1 and "" + str(counter_5_synthesized_prob) + "" for N = 5""\n'"
selectiongan_v1/scripts/evaluation/compute_topK_KL.py,4,"b'# by Krishna Regmi\n\nimport sys\nimport torch\nfrom torch.autograd import Variable as V\nimport torchvision.models as models\nfrom torchvision import transforms as trn\nfrom torch.nn import functional as F\nimport os\nfrom PIL import Image\nimport numpy as np\nimport multiprocessing\nfrom numba import autojit, prange\n\ndef compute_KL(conditional_prob, n, stripout_zeros):\n    marginal_prob = np.mean(conditional_prob, 0)\n    marginal_prob_log = np.log(marginal_prob)\n    log_diff_mult = np.zeros_like(conditional_prob)\n\n    for i in range(n):\n        conditional_prob_log = np.log(conditional_prob[i][:])\n        log_diff = conditional_prob_log - marginal_prob_log\n        log_diff_mult[i][:] = conditional_prob[i][:] * log_diff\n\n    kl = np.mean(np.sum(log_diff_mult, 1))\n    scores = (np.exp(kl))\n    # print scores\n    return scores\n\n\ndef compute_topK_cond_prob_with_epsilon(conditional_prob, k, n):\n    cond_prob_top_k = np.zeros_like(conditional_prob)\n    epsilon = 0.000000001 \n    for j in range(n):\n        cond_prob = conditional_prob[j][:]\n        sorted_indices = [b[0] for b in sorted(enumerate(cond_prob),key=lambda i:i[1], reverse=True)]\n        indices_for_top_k = sorted_indices[0:k]\n        # print indices_for_top_k\n        for ind in range(len(cond_prob)):\n            if(ind not in indices_for_top_k ):\n                cond_prob[ind] = epsilon\n        cond_prob_top_k[j] = cond_prob\n    return cond_prob_top_k\n\n\ndef perform_eval(parameter_list):\n    n = parameter_list[0]\n    img_real = parameter_list[1]\n    img_pix2pix = parameter_list[2]\n    p_y_x_real = parameter_list[3]\n    p_y_x_pix2pix = parameter_list[4]\n    model = parameter_list[5]\n    img_name = files[n]    \n    img_path_real =  img_real + \'/\' + img_name\n    img_path_pix2pix =  img_pix2pix + \'/\' + img_name\n\n    im_real = Image.open(img_path_real)\n    im_pix2pix = Image.open(img_path_pix2pix)\n    \n    input_img_real = V(centre_crop(im_real).unsqueeze(0), volatile=True)\n    input_img_pix2pix = V(centre_crop(im_pix2pix).unsqueeze(0), volatile=True)\n\n    # forward pass\n    logit_real = model.forward(input_img_real)\n    h_x_real = F.softmax(logit_real).data.squeeze()\n    p_y_x_real[i] = h_x_real.numpy()\n\n\n    # forward pass\n    logit_pix2pix = model.forward(input_img_pix2pix)\n    h_x_pix2pix = F.softmax(logit_pix2pix).data.squeeze()\n    p_y_x_pix2pix[i] = h_x_pix2pix.numpy()\n\n    if(n % 500 == 0):\n        print n\n\narch = \'alexnet\' #resnet50, alexnet\n# load the pre-trained weights\nmodel_weight = \'whole_%s_places365.pth.tar\' % arch\nif not os.access(model_weight, os.W_OK):\n    weight_url = \'http://places2.csail.mit.edu/models_places365/whole_%s_places365.pth.tar\' % arch\n    os.system(\'wget \' + weight_url)\n\nuseGPU = 1\nif useGPU == 1:\n    model = torch.load(model_weight)\nelse:\n    model = torch.load(model_weight, map_location=lambda storage, loc: storage) # model trained in GPU could be deployed in CPU machine like this!\n\nmodel.eval()\n\ncentre_crop = trn.Compose([\n        trn.Resize(256),\n        trn.CenterCrop(224),\n        trn.ToTensor(),\n        trn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n\n# load the test image\n\nimg_real =sys.argv[1]\nimg_pix2pix =sys.argv[2]\n\n\npath, root, files = os.walk(img_pix2pix).next()\nn = len(files)\n# n = 10\nprint n\n\np_y_x_real = np.ndarray((n, 365), )\np_y_x_pix2pix = np.ndarray((n, 365), )\n# p_y_x_stacked = np.ndarray((n, 365), )\n# p_y_x_fork = np.ndarray((n, 365), )\n# p_y_x_x_seq = np.ndarray((n, 365), )\n\n# pool = multiprocessing.Pool(processes=16)\n# pool.map(perform_eval, [range(n), img_real, img_pix2pix, p_y_x_real, p_y_x_pix2pix, model])\n# pool.close()\n# pool.join()\n\n# x = []\nfor i in prange(n):\n    img_name = files[i]\n\n    img_path_real =  img_real + \'/\' + img_name\n    img_path_pix2pix =  img_pix2pix + \'/\' + img_name\n\n    im_real = Image.open(img_path_real)\n    im_pix2pix = Image.open(img_path_pix2pix)\n\n\n    input_img_real = V(centre_crop(im_real).unsqueeze(0), volatile=True)\n    input_img_pix2pix = V(centre_crop(im_pix2pix).unsqueeze(0), volatile=True)\n\n    # forward pass\n    logit_real = model.forward(input_img_real)\n    h_x_real = F.softmax(logit_real).data.squeeze()\n    p_y_x_real[i] = h_x_real.numpy()\n\n\n    # forward pass\n    logit_pix2pix = model.forward(input_img_pix2pix)\n    h_x_pix2pix = F.softmax(logit_pix2pix).data.squeeze()\n    p_y_x_pix2pix[i] = h_x_pix2pix.numpy()\n\n\n    if(i % 500 == 0):\n        print i\n        \np_y_x_real_clone = np.empty_like(p_y_x_real)\np_y_x_real_clone[:][:] = p_y_x_real\n\np_y_x_pix2pix_clone = np.empty_like(p_y_x_pix2pix)\np_y_x_pix2pix_clone[:][:] = p_y_x_pix2pix\n\nprint ""\\n""\n\nkl = compute_KL(p_y_x_real_clone, n, ""False"")\nprint ""kl for real is :""+ str(kl)\n\nkl = compute_KL(p_y_x_pix2pix_clone, n, ""False"")\nprint ""kl for pix2pix is :""+ str(kl)\n\n\nk = 1\nprint ""\\nFor k = "" + str(k)\n\n\n\ntopk_cond_prob_with_epsilon_real =  compute_topK_cond_prob_with_epsilon(p_y_x_real_clone, k, n)\nkl = compute_KL(topk_cond_prob_with_epsilon_real, n, ""False"")\nprint ""kl for real is :""+ str(kl)\n\ntopk_cond_prob_with_epsilon_pix2pix =  compute_topK_cond_prob_with_epsilon(p_y_x_pix2pix_clone, k, n)\nkl = compute_KL(topk_cond_prob_with_epsilon_pix2pix, n, ""False"")\nprint ""kl for pix2pix is :""+ str(kl)\n\n\nk = 5\nprint ""\\nFor k = "" + str(k)\n\ntopk_cond_prob_with_epsilon_real_5 =  compute_topK_cond_prob_with_epsilon(p_y_x_real, k, n)\nkl = compute_KL(topk_cond_prob_with_epsilon_real_5, n, ""False"")\nprint ""kl for real is :""+ str(kl)\n\ntopk_cond_prob_with_epsilon_pix2pix_5 =  compute_topK_cond_prob_with_epsilon(p_y_x_pix2pix, k, n)\nkl = compute_KL(topk_cond_prob_with_epsilon_pix2pix_5, n, ""False"")\nprint ""kl for pix2pix is :""+ str(kl)\n\n\n# # ####################################################################################################################\n\n'"
semantic_synthesis/models/networks/__init__.py,1,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport torch\nfrom models.networks.base_network import BaseNetwork\nfrom models.networks.loss import *\nfrom models.networks.discriminator import *\nfrom models.networks.generator import *\nfrom models.networks.encoder import *\nimport util.util as util\n\n\ndef find_network_using_name(target_network_name, filename):\n    target_class_name = target_network_name + filename\n    module_name = \'models.networks.\' + filename\n    network = util.find_class_in_module(target_class_name, module_name)\n\n    assert issubclass(network, BaseNetwork), \\\n        ""Class %s should be a subclass of BaseNetwork"" % network\n\n    return network\n\n\ndef modify_commandline_options(parser, is_train):\n    opt, _ = parser.parse_known_args()\n\n    netG_cls = find_network_using_name(opt.netG, \'generator\')\n    parser = netG_cls.modify_commandline_options(parser, is_train)\n    if is_train:\n        netD_cls = find_network_using_name(opt.netD, \'discriminator\')\n        parser = netD_cls.modify_commandline_options(parser, is_train)\n    netE_cls = find_network_using_name(\'conv\', \'encoder\')\n    parser = netE_cls.modify_commandline_options(parser, is_train)\n\n    return parser\n\n\ndef create_network(cls, opt):\n    net = cls(opt)\n    net.print_network()\n    if len(opt.gpu_ids) > 0:\n        assert(torch.cuda.is_available())\n        net.cuda()\n    net.init_weights(opt.init_type, opt.init_variance)\n    return net\n\n\ndef define_G(opt):\n    netG_cls = find_network_using_name(opt.netG, \'generator\')\n    return create_network(netG_cls, opt)\n\n\ndef define_D(opt):\n    netD_cls = find_network_using_name(opt.netD, \'discriminator\')\n    return create_network(netD_cls, opt)\n\n\ndef define_E(opt):\n    # there exists only one encoder type\n    netE_cls = find_network_using_name(\'conv\', \'encoder\')\n    return create_network(netE_cls, opt)\n'"
semantic_synthesis/models/networks/architecture.py,9,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torch.nn.utils.spectral_norm as spectral_norm\nfrom models.networks.normalization import SPADE\n\n\n# ResNet block that uses SPADE.\n# It differs from the ResNet block of pix2pixHD in that\n# it takes in the segmentation map as input, learns the skip connection if necessary,\n# and applies normalization first and then convolution.\n# This architecture seemed like a standard architecture for unconditional or\n# class-conditional GAN architecture using residual block.\n# The code was inspired from https://github.com/LMescheder/GAN_stability.\nclass SPADEResnetBlock(nn.Module):\n    def __init__(self, fin, fout, opt):\n        super().__init__()\n        # Attributes\n        self.learned_shortcut = (fin != fout)\n        fmiddle = min(fin, fout)\n\n        # create conv layers\n        self.conv_0 = nn.Conv2d(fin, fmiddle, kernel_size=3, padding=1)\n        self.conv_1 = nn.Conv2d(fmiddle, fout, kernel_size=3, padding=1)\n        if self.learned_shortcut:\n            self.conv_s = nn.Conv2d(fin, fout, kernel_size=1, bias=False)\n\n        # apply spectral norm if specified\n        if \'spectral\' in opt.norm_G:\n            self.conv_0 = spectral_norm(self.conv_0)\n            self.conv_1 = spectral_norm(self.conv_1)\n            if self.learned_shortcut:\n                self.conv_s = spectral_norm(self.conv_s)\n\n        # define normalization layers\n        spade_config_str = opt.norm_G.replace(\'spectral\', \'\')\n        self.norm_0 = SPADE(spade_config_str, fin, opt.semantic_nc)\n        self.norm_1 = SPADE(spade_config_str, fmiddle, opt.semantic_nc)\n        if self.learned_shortcut:\n            self.norm_s = SPADE(spade_config_str, fin, opt.semantic_nc)\n\n    # note the resnet block with SPADE also takes in |seg|,\n    # the semantic segmentation map as input\n    def forward(self, x, seg):\n        x_s = self.shortcut(x, seg)\n\n        dx = self.conv_0(self.actvn(self.norm_0(x, seg)))\n        dx = self.conv_1(self.actvn(self.norm_1(dx, seg)))\n\n        out = x_s + dx\n\n        return out\n\n    def shortcut(self, x, seg):\n        if self.learned_shortcut:\n            x_s = self.conv_s(self.norm_s(x, seg))\n        else:\n            x_s = x\n        return x_s\n\n    def actvn(self, x):\n        return F.leaky_relu(x, 2e-1)\n\n\n# ResNet block used in pix2pixHD\n# We keep the same architecture as pix2pixHD.\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, norm_layer, activation=nn.ReLU(False), kernel_size=3):\n        super().__init__()\n\n        pw = (kernel_size - 1) // 2\n        self.conv_block = nn.Sequential(\n            nn.ReflectionPad2d(pw),\n            norm_layer(nn.Conv2d(dim, dim, kernel_size=kernel_size)),\n            activation,\n            nn.ReflectionPad2d(pw),\n            norm_layer(nn.Conv2d(dim, dim, kernel_size=kernel_size))\n        )\n\n    def forward(self, x):\n        y = self.conv_block(x)\n        out = x + y\n        return out\n\n\n# VGG architecter, used for the perceptual loss using a pretrained VGG network\nclass VGG19(torch.nn.Module):\n    def __init__(self, requires_grad=False):\n        super().__init__()\n        vgg_pretrained_features = torchvision.models.vgg19(pretrained=True).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        for x in range(2):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(2, 7):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(7, 12):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(12, 21):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(21, 30):\n            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, X):\n        h_relu1 = self.slice1(X)\n        h_relu2 = self.slice2(h_relu1)\n        h_relu3 = self.slice3(h_relu2)\n        h_relu4 = self.slice4(h_relu3)\n        h_relu5 = self.slice5(h_relu4)\n        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n        return out\n'"
semantic_synthesis/models/networks/base_network.py,2,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport torch.nn as nn\nfrom torch.nn import init\n\n\nclass BaseNetwork(nn.Module):\n    def __init__(self):\n        super(BaseNetwork, self).__init__()\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def print_network(self):\n        if isinstance(self, list):\n            self = self[0]\n        num_params = 0\n        for param in self.parameters():\n            num_params += param.numel()\n        print(\'Network [%s] was created. Total number of parameters: %.1f million. \'\n              \'To see the architecture, do print(network).\'\n              % (type(self).__name__, num_params / 1000000))\n\n    def init_weights(self, init_type=\'normal\', gain=0.02):\n        def init_func(m):\n            classname = m.__class__.__name__\n            if classname.find(\'BatchNorm2d\') != -1:\n                if hasattr(m, \'weight\') and m.weight is not None:\n                    init.normal_(m.weight.data, 1.0, gain)\n                if hasattr(m, \'bias\') and m.bias is not None:\n                    init.constant_(m.bias.data, 0.0)\n            elif hasattr(m, \'weight\') and (classname.find(\'Conv\') != -1 or classname.find(\'Linear\') != -1):\n                if init_type == \'normal\':\n                    init.normal_(m.weight.data, 0.0, gain)\n                elif init_type == \'xavier\':\n                    init.xavier_normal_(m.weight.data, gain=gain)\n                elif init_type == \'xavier_uniform\':\n                    init.xavier_uniform_(m.weight.data, gain=1.0)\n                elif init_type == \'kaiming\':\n                    init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\')\n                elif init_type == \'orthogonal\':\n                    init.orthogonal_(m.weight.data, gain=gain)\n                elif init_type == \'none\':  # uses pytorch\'s default init method\n                    m.reset_parameters()\n                else:\n                    raise NotImplementedError(\'initialization method [%s] is not implemented\' % init_type)\n                if hasattr(m, \'bias\') and m.bias is not None:\n                    init.constant_(m.bias.data, 0.0)\n\n        self.apply(init_func)\n\n        # propagate to children\n        for m in self.children():\n            if hasattr(m, \'init_weights\'):\n                m.init_weights(init_type, gain)\n'"
semantic_synthesis/models/networks/discriminator.py,2,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom models.networks.base_network import BaseNetwork\nfrom models.networks.normalization import get_nonspade_norm_layer\nimport util.util as util\n\n\nclass MultiscaleDiscriminator(BaseNetwork):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser.add_argument(\'--netD_subarch\', type=str, default=\'n_layer\',\n                            help=\'architecture of each discriminator\')\n        parser.add_argument(\'--num_D\', type=int, default=2,\n                            help=\'number of discriminators to be used in multiscale\')\n        opt, _ = parser.parse_known_args()\n\n        # define properties of each discriminator of the multiscale discriminator\n        subnetD = util.find_class_in_module(opt.netD_subarch + \'discriminator\',\n                                            \'models.networks.discriminator\')\n        subnetD.modify_commandline_options(parser, is_train)\n\n        return parser\n\n    def __init__(self, opt):\n        super().__init__()\n        self.opt = opt\n\n        for i in range(opt.num_D):\n            subnetD = self.create_single_discriminator(opt)\n            self.add_module(\'discriminator_%d\' % i, subnetD)\n\n    def create_single_discriminator(self, opt):\n        subarch = opt.netD_subarch\n        if subarch == \'n_layer\':\n            netD = NLayerDiscriminator(opt)\n        else:\n            raise ValueError(\'unrecognized discriminator subarchitecture %s\' % subarch)\n        return netD\n\n    def downsample(self, input):\n        return F.avg_pool2d(input, kernel_size=3,\n                            stride=2, padding=[1, 1],\n                            count_include_pad=False)\n\n    # Returns list of lists of discriminator outputs.\n    # The final result is of size opt.num_D x opt.n_layers_D\n    def forward(self, input):\n        result = []\n        get_intermediate_features = not self.opt.no_ganFeat_loss\n        for name, D in self.named_children():\n            out = D(input)\n            if not get_intermediate_features:\n                out = [out]\n            result.append(out)\n            input = self.downsample(input)\n\n        return result\n\n\n# Defines the PatchGAN discriminator with the specified arguments.\nclass NLayerDiscriminator(BaseNetwork):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser.add_argument(\'--n_layers_D\', type=int, default=4,\n                            help=\'# layers in each discriminator\')\n        return parser\n\n    def __init__(self, opt):\n        super().__init__()\n        self.opt = opt\n\n        kw = 4\n        padw = int(np.ceil((kw - 1.0) / 2))\n        nf = opt.ndf\n        input_nc = self.compute_D_input_nc(opt)\n\n        norm_layer = get_nonspade_norm_layer(opt, opt.norm_D)\n        sequence = [[nn.Conv2d(input_nc, nf, kernel_size=kw, stride=2, padding=padw),\n                     nn.LeakyReLU(0.2, False)]]\n\n        for n in range(1, opt.n_layers_D):\n            nf_prev = nf\n            nf = min(nf * 2, 512)\n            stride = 1 if n == opt.n_layers_D - 1 else 2\n            sequence += [[norm_layer(nn.Conv2d(nf_prev, nf, kernel_size=kw,\n                                               stride=stride, padding=padw)),\n                          nn.LeakyReLU(0.2, False)\n                          ]]\n\n        sequence += [[nn.Conv2d(nf, 1, kernel_size=kw, stride=1, padding=padw)]]\n\n        # We divide the layers into groups to extract intermediate layer outputs\n        for n in range(len(sequence)):\n            self.add_module(\'model\' + str(n), nn.Sequential(*sequence[n]))\n\n    def compute_D_input_nc(self, opt):\n        input_nc = opt.label_nc + opt.output_nc\n        if opt.contain_dontcare_label:\n            input_nc += 1\n        if not opt.no_instance:\n            input_nc += 1\n        return input_nc\n\n    def forward(self, input):\n        results = [input]\n        for submodel in self.children():\n            intermediate_output = submodel(results[-1])\n            results.append(intermediate_output)\n\n        get_intermediate_features = not self.opt.no_ganFeat_loss\n        if get_intermediate_features:\n            return results[1:]\n        else:\n            return results[-1]\n'"
semantic_synthesis/models/networks/encoder.py,2,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom models.networks.base_network import BaseNetwork\nfrom models.networks.normalization import get_nonspade_norm_layer\n\n\nclass ConvEncoder(BaseNetwork):\n    """""" Same architecture as the image discriminator """"""\n\n    def __init__(self, opt):\n        super().__init__()\n\n        kw = 3\n        pw = int(np.ceil((kw - 1.0) / 2))\n        ndf = opt.ngf\n        norm_layer = get_nonspade_norm_layer(opt, opt.norm_E)\n        self.layer1 = norm_layer(nn.Conv2d(3, ndf, kw, stride=2, padding=pw))\n        self.layer2 = norm_layer(nn.Conv2d(ndf * 1, ndf * 2, kw, stride=2, padding=pw))\n        self.layer3 = norm_layer(nn.Conv2d(ndf * 2, ndf * 4, kw, stride=2, padding=pw))\n        self.layer4 = norm_layer(nn.Conv2d(ndf * 4, ndf * 8, kw, stride=2, padding=pw))\n        self.layer5 = norm_layer(nn.Conv2d(ndf * 8, ndf * 8, kw, stride=2, padding=pw))\n        if opt.crop_size >= 256:\n            self.layer6 = norm_layer(nn.Conv2d(ndf * 8, ndf * 8, kw, stride=2, padding=pw))\n\n        self.so = s0 = 4\n        self.fc_mu = nn.Linear(ndf * 8 * s0 * s0, 256)\n        self.fc_var = nn.Linear(ndf * 8 * s0 * s0, 256)\n\n        self.actvn = nn.LeakyReLU(0.2, False)\n        self.opt = opt\n\n    def forward(self, x):\n        if x.size(2) != 256 or x.size(3) != 256:\n            x = F.interpolate(x, size=(256, 256), mode=\'bilinear\')\n\n        x = self.layer1(x)\n        x = self.layer2(self.actvn(x))\n        x = self.layer3(self.actvn(x))\n        x = self.layer4(self.actvn(x))\n        x = self.layer5(self.actvn(x))\n        if self.opt.crop_size >= 256:\n            x = self.layer6(self.actvn(x))\n        x = self.actvn(x)\n\n        x = x.view(x.size(0), -1)\n        mu = self.fc_mu(x)\n        logvar = self.fc_var(x)\n\n        return mu, logvar\n'"
semantic_synthesis/models/networks/generator.py,5,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom models.networks.base_network import BaseNetwork\nfrom models.networks.normalization import get_nonspade_norm_layer\nfrom models.networks.architecture import ResnetBlock as ResnetBlock\nfrom models.networks.architecture import SPADEResnetBlock as SPADEResnetBlock\n\n\nclass SPADEGenerator(BaseNetwork):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser.set_defaults(norm_G=\'spectralspadesyncbatch3x3\')\n        parser.add_argument(\'--num_upsampling_layers\',\n                            choices=(\'normal\', \'more\', \'most\'), default=\'normal\',\n                            help=""If \'more\', adds upsampling layer between the two middle resnet blocks. If \'most\', also add one more upsampling + resnet layer at the end of the generator"")\n\n        return parser\n\n    def __init__(self, opt):\n        super().__init__()\n        self.opt = opt\n        nf = opt.ngf\n\n        self.sw, self.sh = self.compute_latent_vector_size(opt)\n\n        if opt.use_vae:\n            # In case of VAE, we will sample from random z vector\n            self.fc = nn.Linear(opt.z_dim, 16 * nf * self.sw * self.sh)\n        else:\n            # Otherwise, we make the network deterministic by starting with\n            # downsampled segmentation map instead of random z\n            self.fc = nn.Conv2d(self.opt.semantic_nc, 16 * nf, 3, padding=1)\n\n        self.head_0 = SPADEResnetBlock(16 * nf, 16 * nf, opt)\n\n        self.G_middle_0 = SPADEResnetBlock(16 * nf, 16 * nf, opt)\n        self.G_middle_1 = SPADEResnetBlock(16 * nf, 16 * nf, opt)\n\n        self.up_0 = SPADEResnetBlock(16 * nf, 8 * nf, opt)\n        self.up_1 = SPADEResnetBlock(8 * nf, 4 * nf, opt)\n        self.up_2 = SPADEResnetBlock(4 * nf, 2 * nf, opt)\n        self.up_3 = SPADEResnetBlock(2 * nf, 1 * nf, opt)\n\n        final_nc = nf\n\n        if opt.num_upsampling_layers == \'most\':\n            self.up_4 = SPADEResnetBlock(1 * nf, nf // 2, opt)\n            final_nc = nf // 2\n\n        self.conv_img = nn.Conv2d(final_nc, 3*10, 3, padding=1)\n        self.conv_att = nn.Conv2d(final_nc, 10, kernel_size=1, stride=1, padding=0)\n\n        self.up = nn.Upsample(scale_factor=2)\n\n    def compute_latent_vector_size(self, opt):\n        if opt.num_upsampling_layers == \'normal\':\n            num_up_layers = 5\n        elif opt.num_upsampling_layers == \'more\':\n            num_up_layers = 6\n        elif opt.num_upsampling_layers == \'most\':\n            num_up_layers = 7\n        else:\n            raise ValueError(\'opt.num_upsampling_layers [%s] not recognized\' %\n                             opt.num_upsampling_layers)\n\n        sw = opt.crop_size // (2**num_up_layers)\n        sh = round(sw / opt.aspect_ratio)\n\n        return sw, sh\n\n    def forward(self, input, z=None):\n        seg = input\n\n        if self.opt.use_vae:\n            # we sample z from unit normal and reshape the tensor\n            if z is None:\n                z = torch.randn(input.size(0), self.opt.z_dim,\n                                dtype=torch.float32, device=input.get_device())\n            x = self.fc(z)\n            x = x.view(-1, 16 * self.opt.ngf, self.sh, self.sw)\n        else:\n            # we downsample segmap and run convolution\n            x = F.interpolate(seg, size=(self.sh, self.sw))\n            x = self.fc(x)\n\n        x = self.head_0(x, seg)\n\n        x = self.up(x)\n        x = self.G_middle_0(x, seg)\n\n        if self.opt.num_upsampling_layers == \'more\' or \\\n           self.opt.num_upsampling_layers == \'most\':\n            x = self.up(x)\n\n        x = self.G_middle_1(x, seg)\n\n        x = self.up(x)\n        x = self.up_0(x, seg)\n        x = self.up(x)\n        x = self.up_1(x, seg)\n        x = self.up(x)\n        x = self.up_2(x, seg)\n        x = self.up(x)\n        x = self.up_3(x, seg)\n\n        if self.opt.num_upsampling_layers == \'most\':\n            x = self.up(x)\n            x = self.up_4(x, seg)\n\n        # x = self.conv_img(F.leaky_relu(x, 2e-1))\n        # x = F.tanh(x)\n\n        image = self.conv_img(F.leaky_relu(x, 2e-1))\n        image = F.tanh(image)\n\n        image1 = image[:, 0:3, :, :]\n        image2 = image[:, 3:6, :, :]\n        image3 = image[:, 6:9, :, :]\n        image4 = image[:, 9:12, :, :]\n        image5 = image[:, 12:15, :, :]\n        image6 = image[:, 15:18, :, :]\n        image7 = image[:, 18:21, :, :]\n        image8 = image[:, 21:24, :, :]\n        image9 = image[:, 24:27, :, :]\n        image10 = image[:, 27:30, :, :]\n\n        attention = self.conv_att(F.leaky_relu(x, 2e-1))\n        softmax_ = torch.nn.Softmax(dim=1)\n        attention = softmax_(attention)\n\n        attention1_ = attention[:, 0:1, :, :]\n        attention2_ = attention[:, 1:2, :, :]\n        attention3_ = attention[:, 2:3, :, :]\n        attention4_ = attention[:, 3:4, :, :]\n        attention5_ = attention[:, 4:5, :, :]\n        attention6_ = attention[:, 5:6, :, :]\n        attention7_ = attention[:, 6:7, :, :]\n        attention8_ = attention[:, 7:8, :, :]\n        attention9_ = attention[:, 8:9, :, :]\n        attention10_ = attention[:, 9:10, :, :]\n\n        attention1 = attention1_.repeat(1, 3, 1, 1)\n        attention2 = attention2_.repeat(1, 3, 1, 1)\n        attention3 = attention3_.repeat(1, 3, 1, 1)\n        attention4 = attention4_.repeat(1, 3, 1, 1)\n        attention5 = attention5_.repeat(1, 3, 1, 1)\n        attention6 = attention6_.repeat(1, 3, 1, 1)\n        attention7 = attention7_.repeat(1, 3, 1, 1)\n        attention8 = attention8_.repeat(1, 3, 1, 1)\n        attention9 = attention9_.repeat(1, 3, 1, 1)\n        attention10 = attention10_.repeat(1, 3, 1, 1)\n\n        output1 = image1 * attention1\n        output2 = image2 * attention2\n        output3 = image3 * attention3\n        output4 = image4 * attention4\n        output5 = image5 * attention5\n        output6 = image6 * attention6\n        output7 = image7 * attention7\n        output8 = image8 * attention8\n        output9 = image9 * attention9\n        output10 = image10 * attention10\n\n        final = output1 + output2 + output3 + output4 + output5 + output6 + output7 + output8 + output9 + output10\n\n        return final\n\n\nclass Pix2PixHDGenerator(BaseNetwork):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser.add_argument(\'--resnet_n_downsample\', type=int, default=4, help=\'number of downsampling layers in netG\')\n        parser.add_argument(\'--resnet_n_blocks\', type=int, default=9, help=\'number of residual blocks in the global generator network\')\n        parser.add_argument(\'--resnet_kernel_size\', type=int, default=3,\n                            help=\'kernel size of the resnet block\')\n        parser.add_argument(\'--resnet_initial_kernel_size\', type=int, default=7,\n                            help=\'kernel size of the first convolution\')\n        parser.set_defaults(norm_G=\'instance\')\n        return parser\n\n    def __init__(self, opt):\n        super().__init__()\n        input_nc = opt.label_nc + (1 if opt.contain_dontcare_label else 0) + (0 if opt.no_instance else 1)\n\n        norm_layer = get_nonspade_norm_layer(opt, opt.norm_G)\n        activation = nn.ReLU(False)\n\n        model = []\n\n        # initial conv\n        model += [nn.ReflectionPad2d(opt.resnet_initial_kernel_size // 2),\n                  norm_layer(nn.Conv2d(input_nc, opt.ngf,\n                                       kernel_size=opt.resnet_initial_kernel_size,\n                                       padding=0)),\n                  activation]\n\n        # downsample\n        mult = 1\n        for i in range(opt.resnet_n_downsample):\n            model += [norm_layer(nn.Conv2d(opt.ngf * mult, opt.ngf * mult * 2,\n                                           kernel_size=3, stride=2, padding=1)),\n                      activation]\n            mult *= 2\n\n        # resnet blocks\n        for i in range(opt.resnet_n_blocks):\n            model += [ResnetBlock(opt.ngf * mult,\n                                  norm_layer=norm_layer,\n                                  activation=activation,\n                                  kernel_size=opt.resnet_kernel_size)]\n\n        # upsample\n        for i in range(opt.resnet_n_downsample):\n            nc_in = int(opt.ngf * mult)\n            nc_out = int((opt.ngf * mult) / 2)\n            model += [norm_layer(nn.ConvTranspose2d(nc_in, nc_out,\n                                                    kernel_size=3, stride=2,\n                                                    padding=1, output_padding=1)),\n                      activation]\n            mult = mult // 2\n\n        # final output conv\n        model += [nn.ReflectionPad2d(3),\n                  nn.Conv2d(nc_out, opt.output_nc, kernel_size=7, padding=0),\n                  nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input, z=None):\n        return self.model(input)\n'"
semantic_synthesis/models/networks/loss.py,10,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom models.networks.architecture import VGG19\n\n\n# Defines the GAN loss which uses either LSGAN or the regular GAN.\n# When LSGAN is used, it is basically same as MSELoss,\n# but it abstracts away the need to create the target label tensor\n# that has the same size as the input\nclass GANLoss(nn.Module):\n    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0,\n                 tensor=torch.FloatTensor, opt=None):\n        super(GANLoss, self).__init__()\n        self.real_label = target_real_label\n        self.fake_label = target_fake_label\n        self.real_label_tensor = None\n        self.fake_label_tensor = None\n        self.zero_tensor = None\n        self.Tensor = tensor\n        self.gan_mode = gan_mode\n        self.opt = opt\n        if gan_mode == \'ls\':\n            pass\n        elif gan_mode == \'original\':\n            pass\n        elif gan_mode == \'w\':\n            pass\n        elif gan_mode == \'hinge\':\n            pass\n        else:\n            raise ValueError(\'Unexpected gan_mode {}\'.format(gan_mode))\n\n    def get_target_tensor(self, input, target_is_real):\n        if target_is_real:\n            if self.real_label_tensor is None:\n                self.real_label_tensor = self.Tensor(1).fill_(self.real_label)\n                self.real_label_tensor.requires_grad_(False)\n            return self.real_label_tensor.expand_as(input)\n        else:\n            if self.fake_label_tensor is None:\n                self.fake_label_tensor = self.Tensor(1).fill_(self.fake_label)\n                self.fake_label_tensor.requires_grad_(False)\n            return self.fake_label_tensor.expand_as(input)\n\n    def get_zero_tensor(self, input):\n        if self.zero_tensor is None:\n            self.zero_tensor = self.Tensor(1).fill_(0)\n            self.zero_tensor.requires_grad_(False)\n        return self.zero_tensor.expand_as(input)\n\n    def loss(self, input, target_is_real, for_discriminator=True):\n        if self.gan_mode == \'original\':  # cross entropy loss\n            target_tensor = self.get_target_tensor(input, target_is_real)\n            loss = F.binary_cross_entropy_with_logits(input, target_tensor)\n            return loss\n        elif self.gan_mode == \'ls\':\n            target_tensor = self.get_target_tensor(input, target_is_real)\n            return F.mse_loss(input, target_tensor)\n        elif self.gan_mode == \'hinge\':\n            if for_discriminator:\n                if target_is_real:\n                    minval = torch.min(input - 1, self.get_zero_tensor(input))\n                    loss = -torch.mean(minval)\n                else:\n                    minval = torch.min(-input - 1, self.get_zero_tensor(input))\n                    loss = -torch.mean(minval)\n            else:\n                assert target_is_real, ""The generator\'s hinge loss must be aiming for real""\n                loss = -torch.mean(input)\n            return loss\n        else:\n            # wgan\n            if target_is_real:\n                return -input.mean()\n            else:\n                return input.mean()\n\n    def __call__(self, input, target_is_real, for_discriminator=True):\n        # computing loss is a bit complicated because |input| may not be\n        # a tensor, but list of tensors in case of multiscale discriminator\n        if isinstance(input, list):\n            loss = 0\n            for pred_i in input:\n                if isinstance(pred_i, list):\n                    pred_i = pred_i[-1]\n                loss_tensor = self.loss(pred_i, target_is_real, for_discriminator)\n                bs = 1 if len(loss_tensor.size()) == 0 else loss_tensor.size(0)\n                new_loss = torch.mean(loss_tensor.view(bs, -1), dim=1)\n                loss += new_loss\n            return loss / len(input)\n        else:\n            return self.loss(input, target_is_real, for_discriminator)\n\n\n# Perceptual loss that uses a pretrained VGG network\nclass VGGLoss(nn.Module):\n    def __init__(self, gpu_ids):\n        super(VGGLoss, self).__init__()\n        self.vgg = VGG19().cuda()\n        self.criterion = nn.L1Loss()\n        self.weights = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1.0]\n\n    def forward(self, x, y):\n        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n        loss = 0\n        for i in range(len(x_vgg)):\n            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())\n        return loss\n\n\n# KL Divergence loss used in VAE with an image encoder\nclass KLDLoss(nn.Module):\n    def forward(self, mu, logvar):\n        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n'"
semantic_synthesis/models/networks/normalization.py,3,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport re\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom models.networks.sync_batchnorm import SynchronizedBatchNorm2d\nimport torch.nn.utils.spectral_norm as spectral_norm\n\n\n# Returns a function that creates a normalization function\n# that does not condition on semantic map\ndef get_nonspade_norm_layer(opt, norm_type=\'instance\'):\n    # helper function to get # output channels of the previous layer\n    def get_out_channel(layer):\n        if hasattr(layer, \'out_channels\'):\n            return getattr(layer, \'out_channels\')\n        return layer.weight.size(0)\n\n    # this function will be returned\n    def add_norm_layer(layer):\n        nonlocal norm_type\n        if norm_type.startswith(\'spectral\'):\n            layer = spectral_norm(layer)\n            subnorm_type = norm_type[len(\'spectral\'):]\n\n        if subnorm_type == \'none\' or len(subnorm_type) == 0:\n            return layer\n\n        # remove bias in the previous layer, which is meaningless\n        # since it has no effect after normalization\n        if getattr(layer, \'bias\', None) is not None:\n            delattr(layer, \'bias\')\n            layer.register_parameter(\'bias\', None)\n\n        if subnorm_type == \'batch\':\n            norm_layer = nn.BatchNorm2d(get_out_channel(layer), affine=True)\n        elif subnorm_type == \'sync_batch\':\n            norm_layer = SynchronizedBatchNorm2d(get_out_channel(layer), affine=True)\n        elif subnorm_type == \'instance\':\n            norm_layer = nn.InstanceNorm2d(get_out_channel(layer), affine=False)\n        else:\n            raise ValueError(\'normalization layer %s is not recognized\' % subnorm_type)\n\n        return nn.Sequential(layer, norm_layer)\n\n    return add_norm_layer\n\n\n# Creates SPADE normalization layer based on the given configuration\n# SPADE consists of two steps. First, it normalizes the activations using\n# your favorite normalization method, such as Batch Norm or Instance Norm.\n# Second, it applies scale and bias to the normalized output, conditioned on\n# the segmentation map.\n# The format of |config_text| is spade(norm)(ks), where\n# (norm) specifies the type of parameter-free normalization.\n#       (e.g. syncbatch, batch, instance)\n# (ks) specifies the size of kernel in the SPADE module (e.g. 3x3)\n# Example |config_text| will be spadesyncbatch3x3, or spadeinstance5x5.\n# Also, the other arguments are\n# |norm_nc|: the #channels of the normalized activations, hence the output dim of SPADE\n# |label_nc|: the #channels of the input semantic map, hence the input dim of SPADE\nclass SPADE(nn.Module):\n    def __init__(self, config_text, norm_nc, label_nc):\n        super().__init__()\n\n        assert config_text.startswith(\'spade\')\n        parsed = re.search(\'spade(\\D+)(\\d)x\\d\', config_text)\n        param_free_norm_type = str(parsed.group(1))\n        ks = int(parsed.group(2))\n\n        if param_free_norm_type == \'instance\':\n            self.param_free_norm = nn.InstanceNorm2d(norm_nc, affine=False)\n        elif param_free_norm_type == \'syncbatch\':\n            self.param_free_norm = SynchronizedBatchNorm2d(norm_nc, affine=False)\n        elif param_free_norm_type == \'batch\':\n            self.param_free_norm = nn.BatchNorm2d(norm_nc, affine=False)\n        else:\n            raise ValueError(\'%s is not a recognized param-free norm type in SPADE\'\n                             % param_free_norm_type)\n\n        # The dimension of the intermediate embedding space. Yes, hardcoded.\n        nhidden = 128\n\n        pw = ks // 2\n        self.mlp_shared = nn.Sequential(\n            nn.Conv2d(label_nc, nhidden, kernel_size=ks, padding=pw),\n            nn.ReLU()\n        )\n        self.mlp_gamma = nn.Conv2d(nhidden, norm_nc, kernel_size=ks, padding=pw)\n        self.mlp_beta = nn.Conv2d(nhidden, norm_nc, kernel_size=ks, padding=pw)\n\n    def forward(self, x, segmap):\n\n        # Part 1. generate parameter-free normalized activations\n        normalized = self.param_free_norm(x)\n\n        # Part 2. produce scaling and bias conditioned on semantic map\n        segmap = F.interpolate(segmap, size=x.size()[2:], mode=\'nearest\')\n        actv = self.mlp_shared(segmap)\n        gamma = self.mlp_gamma(actv)\n        beta = self.mlp_beta(actv)\n\n        # apply scale and bias\n        out = normalized * (1 + gamma) + beta\n\n        return out\n'"
