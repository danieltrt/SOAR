file_path,api_count,code
VAD_segments.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Tue Dec 18 16:22:41 2018\n\n@author: Harry\nModified from https://github.com/wiseman/py-webrtcvad/blob/master/example.py\n""""""\n\nimport collections\nimport contextlib\nimport numpy as np\nimport sys\nimport librosa\nimport wave\n\nimport webrtcvad\n\nfrom hparam import hparam as hp\n\ndef read_wave(path, sr):\n    """"""Reads a .wav file.\n    Takes the path, and returns (PCM audio data, sample rate).\n    Assumes sample width == 2\n    """"""\n    with contextlib.closing(wave.open(path, \'rb\')) as wf:\n        num_channels = wf.getnchannels()\n        assert num_channels == 1\n        sample_width = wf.getsampwidth()\n        assert sample_width == 2\n        sample_rate = wf.getframerate()\n        assert sample_rate in (8000, 16000, 32000, 48000)\n        pcm_data = wf.readframes(wf.getnframes())\n    data, _ = librosa.load(path, sr)\n    assert len(data.shape) == 1\n    assert sr in (8000, 16000, 32000, 48000)\n    return data, pcm_data\n    \nclass Frame(object):\n    """"""Represents a ""frame"" of audio data.""""""\n    def __init__(self, bytes, timestamp, duration):\n        self.bytes = bytes\n        self.timestamp = timestamp\n        self.duration = duration\n\n\ndef frame_generator(frame_duration_ms, audio, sample_rate):\n    """"""Generates audio frames from PCM audio data.\n    Takes the desired frame duration in milliseconds, the PCM data, and\n    the sample rate.\n    Yields Frames of the requested duration.\n    """"""\n    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n    offset = 0\n    timestamp = 0.0\n    duration = (float(n) / sample_rate) / 2.0\n    while offset + n < len(audio):\n        yield Frame(audio[offset:offset + n], timestamp, duration)\n        timestamp += duration\n        offset += n\n\n\ndef vad_collector(sample_rate, frame_duration_ms,\n                  padding_duration_ms, vad, frames):\n    """"""Filters out non-voiced audio frames.\n    Given a webrtcvad.Vad and a source of audio frames, yields only\n    the voiced audio.\n    Uses a padded, sliding window algorithm over the audio frames.\n    When more than 90% of the frames in the window are voiced (as\n    reported by the VAD), the collector triggers and begins yielding\n    audio frames. Then the collector waits until 90% of the frames in\n    the window are unvoiced to detrigger.\n    The window is padded at the front and back to provide a small\n    amount of silence or the beginnings/endings of speech around the\n    voiced frames.\n    Arguments:\n    sample_rate - The audio sample rate, in Hz.\n    frame_duration_ms - The frame duration in milliseconds.\n    padding_duration_ms - The amount to pad the window, in milliseconds.\n    vad - An instance of webrtcvad.Vad.\n    frames - a source of audio frames (sequence or generator).\n    Returns: A generator that yields PCM audio data.\n    """"""\n    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n    # We use a deque for our sliding window/ring buffer.\n    ring_buffer = collections.deque(maxlen=num_padding_frames)\n    # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\n    # NOTTRIGGERED state.\n    triggered = False\n\n    voiced_frames = []\n    for frame in frames:\n        is_speech = vad.is_speech(frame.bytes, sample_rate)\n\n        if not triggered:\n            ring_buffer.append((frame, is_speech))\n            num_voiced = len([f for f, speech in ring_buffer if speech])\n            # If we\'re NOTTRIGGERED and more than 90% of the frames in\n            # the ring buffer are voiced frames, then enter the\n            # TRIGGERED state.\n            if num_voiced > 0.9 * ring_buffer.maxlen:\n                triggered = True\n                start = ring_buffer[0][0].timestamp\n                # We want to yield all the audio we see from now until\n                # we are NOTTRIGGERED, but we have to start with the\n                # audio that\'s already in the ring buffer.\n                for f, s in ring_buffer:\n                    voiced_frames.append(f)\n                ring_buffer.clear()\n        else:\n            # We\'re in the TRIGGERED state, so collect the audio data\n            # and add it to the ring buffer.\n            voiced_frames.append(frame)\n            ring_buffer.append((frame, is_speech))\n            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n            # If more than 90% of the frames in the ring buffer are\n            # unvoiced, then enter NOTTRIGGERED and yield whatever\n            # audio we\'ve collected.\n            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n                triggered = False\n                yield (start, frame.timestamp + frame.duration)\n                ring_buffer.clear()\n                voiced_frames = []\n    # If we have any leftover voiced audio when we run out of input,\n    # yield it.\n    if voiced_frames:\n        yield (start, frame.timestamp + frame.duration)\n\n\ndef VAD_chunk(aggressiveness, path):\n    audio, byte_audio = read_wave(path, hp.data.sr)\n    vad = webrtcvad.Vad(int(aggressiveness))\n    frames = frame_generator(20, byte_audio, hp.data.sr)\n    frames = list(frames)\n    times = vad_collector(hp.data.sr, 20, 200, vad, frames)\n    speech_times = []\n    speech_segs = []\n    for i, time in enumerate(times):\n        start = np.round(time[0],decimals=2)\n        end = np.round(time[1],decimals=2)\n        j = start\n        while j + .4 < end:\n            end_j = np.round(j+.4,decimals=2)\n            speech_times.append((j, end_j))\n            speech_segs.append(audio[int(j*hp.data.sr):int(end_j*hp.data.sr)])\n            j = end_j\n        else:\n            speech_times.append((j, end))\n            speech_segs.append(audio[int(j*hp.data.sr):int(end*hp.data.sr)])\n    return speech_times, speech_segs\n\nif __name__ == \'__main__\':\n    speech_times, speech_segs = VAD_chunk(sys.argv[1], sys.argv[2])\n'"
data_load.py,3,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Mon Aug  6 20:55:52 2018\n\n@author: harry\n""""""\nimport glob\nimport numpy as np\nimport os\nimport random\nfrom random import shuffle\nimport torch\nfrom torch.utils.data import Dataset\n\nfrom hparam import hparam as hp\nfrom utils import mfccs_and_spec\n\nclass SpeakerDatasetTIMIT(Dataset):\n    \n    def __init__(self):\n\n        if hp.training:\n            self.path = hp.data.train_path_unprocessed\n            self.utterance_number = hp.train.M\n        else:\n            self.path = hp.data.test_path_unprocessed\n            self.utterance_number = hp.test.M\n        self.speakers = glob.glob(os.path.dirname(self.path))\n        shuffle(self.speakers)\n        \n    def __len__(self):\n        return len(self.speakers)\n\n    def __getitem__(self, idx):\n        \n        speaker = self.speakers[idx]\n        wav_files = glob.glob(speaker+\'/*.WAV\')\n        shuffle(wav_files)\n        wav_files = wav_files[0:self.utterance_number]\n        \n        mel_dbs = []\n        for f in wav_files:\n            _, mel_db, _ = mfccs_and_spec(f, wav_process = True)\n            mel_dbs.append(mel_db)\n        return torch.Tensor(mel_dbs)\n\nclass SpeakerDatasetTIMITPreprocessed(Dataset):\n    \n    def __init__(self, shuffle=True, utter_start=0):\n        \n        # data path\n        if hp.training:\n            self.path = hp.data.train_path\n            self.utter_num = hp.train.M\n        else:\n            self.path = hp.data.test_path\n            self.utter_num = hp.test.M\n        self.file_list = os.listdir(self.path)\n        self.shuffle=shuffle\n        self.utter_start = utter_start\n        \n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx):\n        \n        np_file_list = os.listdir(self.path)\n        \n        if self.shuffle:\n            selected_file = random.sample(np_file_list, 1)[0]  # select random speaker\n        else:\n            selected_file = np_file_list[idx]               \n        \n        utters = np.load(os.path.join(self.path, selected_file))        # load utterance spectrogram of selected speaker\n        if self.shuffle:\n            utter_index = np.random.randint(0, utters.shape[0], self.utter_num)   # select M utterances per speaker\n            utterance = utters[utter_index]       \n        else:\n            utterance = utters[self.utter_start: self.utter_start+self.utter_num] # utterances of a speaker [batch(M), n_mels, frames]\n\n        utterance = utterance[:,:,:160]               # TODO implement variable length batch size\n\n        utterance = torch.tensor(np.transpose(utterance, axes=(0,2,1)))     # transpose [batch, frames, n_mels]\n        return utterance\n'"
data_preprocess.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#Modified from https://github.com/JanhHyun/Speaker_Verification\nimport glob\nimport os\nimport librosa\nimport numpy as np\nfrom hparam import hparam as hp\n\n# downloaded dataset path\naudio_path = glob.glob(os.path.dirname(hp.unprocessed_data))                                        \n\ndef save_spectrogram_tisv():\n    """""" Full preprocess of text independent utterance. The log-mel-spectrogram is saved as numpy file.\n        Each partial utterance is splitted by voice detection using DB\n        and the first and the last 180 frames from each partial utterance are saved. \n        Need : utterance data set (VTCK)\n    """"""\n    print(""start text independent utterance feature extraction"")\n    os.makedirs(hp.data.train_path, exist_ok=True)   # make folder to save train file\n    os.makedirs(hp.data.test_path, exist_ok=True)    # make folder to save test file\n\n    utter_min_len = (hp.data.tisv_frame * hp.data.hop + hp.data.window) * hp.data.sr    # lower bound of utterance length\n    total_speaker_num = len(audio_path)\n    train_speaker_num= (total_speaker_num//10)*9            # split total data 90% train and 10% test\n    print(""total speaker number : %d""%total_speaker_num)\n    print(""train : %d, test : %d""%(train_speaker_num, total_speaker_num-train_speaker_num))\n    for i, folder in enumerate(audio_path):\n        print(""%dth speaker processing...""%i)\n        utterances_spec = []\n        for utter_name in os.listdir(folder):\n            if utter_name[-4:] == \'.WAV\':\n                utter_path = os.path.join(folder, utter_name)         # path of each utterance\n                utter, sr = librosa.core.load(utter_path, hp.data.sr)        # load utterance audio\n                intervals = librosa.effects.split(utter, top_db=30)         # voice activity detection \n                # this works fine for timit but if you get array of shape 0 for any other audio change value of top_db\n                # for vctk dataset use top_db=100\n                for interval in intervals:\n                    if (interval[1]-interval[0]) > utter_min_len:           # If partial utterance is sufficient long,\n                        utter_part = utter[interval[0]:interval[1]]         # save first and last 180 frames of spectrogram.\n                        S = librosa.core.stft(y=utter_part, n_fft=hp.data.nfft,\n                                              win_length=int(hp.data.window * sr), hop_length=int(hp.data.hop * sr))\n                        S = np.abs(S) ** 2\n                        mel_basis = librosa.filters.mel(sr=hp.data.sr, n_fft=hp.data.nfft, n_mels=hp.data.nmels)\n                        S = np.log10(np.dot(mel_basis, S) + 1e-6)           # log mel spectrogram of utterances\n                        utterances_spec.append(S[:, :hp.data.tisv_frame])    # first 180 frames of partial utterance\n                        utterances_spec.append(S[:, -hp.data.tisv_frame:])   # last 180 frames of partial utterance\n\n        utterances_spec = np.array(utterances_spec)\n        print(utterances_spec.shape)\n        if i<train_speaker_num:      # save spectrogram as numpy file\n            np.save(os.path.join(hp.data.train_path, ""speaker%d.npy""%i), utterances_spec)\n        else:\n            np.save(os.path.join(hp.data.test_path, ""speaker%d.npy""%(i-train_speaker_num)), utterances_spec)\n\n\nif __name__ == ""__main__"":\n    save_spectrogram_tisv()\n'"
dvector_create.py,2,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Dec 19 14:34:01 2018\n\n@author: Harry\n\nCreates ""segment level d vector embeddings"" compatible with\nhttps://github.com/google/uis-rnn\n\n""""""\n\nimport glob\nimport librosa\nimport numpy as np\nimport os\nimport torch\n\nfrom hparam import hparam as hp\nfrom speech_embedder_net import SpeechEmbedder\nfrom VAD_segments import VAD_chunk\n\n\ndef concat_segs(times, segs):\n    #Concatenate continuous voiced segments\n    concat_seg = []\n    seg_concat = segs[0]\n    for i in range(0, len(times)-1):\n        if times[i][1] == times[i+1][0]:\n            seg_concat = np.concatenate((seg_concat, segs[i+1]))\n        else:\n            concat_seg.append(seg_concat)\n            seg_concat = segs[i+1]\n    else:\n        concat_seg.append(seg_concat)\n    return concat_seg\n\ndef get_STFTs(segs):\n    #Get 240ms STFT windows with 50% overlap\n    sr = hp.data.sr\n    STFT_frames = []\n    for seg in segs:\n        S = librosa.core.stft(y=seg, n_fft=hp.data.nfft,\n                              win_length=int(hp.data.window * sr), hop_length=int(hp.data.hop * sr))\n        S = np.abs(S)**2\n        mel_basis = librosa.filters.mel(sr, n_fft=hp.data.nfft, n_mels=hp.data.nmels)\n        S = np.log10(np.dot(mel_basis, S) + 1e-6)           # log mel spectrogram of utterances\n        for j in range(0, S.shape[1], int(.12/hp.data.hop)):\n            if j + 24 < S.shape[1]:\n                STFT_frames.append(S[:,j:j+24])\n            else:\n                break\n    return STFT_frames\n\ndef align_embeddings(embeddings):\n    partitions = []\n    start = 0\n    end = 0\n    j = 1\n    for i, embedding in enumerate(embeddings):\n        if (i*.12)+.24 < j*.401:\n            end = end + 1\n        else:\n            partitions.append((start,end))\n            start = end\n            end = end + 1\n            j += 1\n    else:\n        partitions.append((start,end))\n    avg_embeddings = np.zeros((len(partitions),256))\n    for i, partition in enumerate(partitions):\n        avg_embeddings[i] = np.average(embeddings[partition[0]:partition[1]],axis=0) \n    return avg_embeddings\n#dataset path\naudio_path = glob.glob(os.path.dirname(hp.unprocessed_data))  \n\ntotal_speaker_num = len(audio_path)\ntrain_speaker_num= (total_speaker_num//10)*9            # split total data 90% train and 10% test\n\nembedder_net = SpeechEmbedder()\nembedder_net.load_state_dict(torch.load(hp.model.model_path))\nembedder_net.eval()\n\ntrain_sequence = []\ntrain_cluster_id = []\nlabel = 0\ncount = 0\ntrain_saved = False\nfor i, folder in enumerate(audio_path):\n    for file in os.listdir(folder):\n        if file[-4:] == \'.wav\':\n            times, segs = VAD_chunk(2, folder+\'/\'+file)\n            if segs == []:\n                print(\'No voice activity detected\')\n                continue\n            concat_seg = concat_segs(times, segs)\n            STFT_frames = get_STFTs(concat_seg)\n            STFT_frames = np.stack(STFT_frames, axis=2)\n            STFT_frames = torch.tensor(np.transpose(STFT_frames, axes=(2,1,0)))\n            embeddings = embedder_net(STFT_frames)\n            aligned_embeddings = align_embeddings(embeddings.detach().numpy())\n            train_sequence.append(aligned_embeddings)\n            for embedding in aligned_embeddings:\n                train_cluster_id.append(str(label))\n            count = count + 1\n            if count % 100 == 0:\n                print(\'Processed {0}/{1} files\'.format(count, len(audio_path)))\n    label = label + 1\n    \n    if not train_saved and i > train_speaker_num:\n        train_sequence = np.concatenate(train_sequence,axis=0)\n        train_cluster_id = np.asarray(train_cluster_id)\n        np.save(\'train_sequence\',train_sequence)\n        np.save(\'train_cluster_id\',train_cluster_id)\n        train_saved = True\n        train_sequence = []\n        train_cluster_id = []\n        \ntrain_sequence = np.concatenate(train_sequence,axis=0)\ntrain_cluster_id = np.asarray(train_cluster_id)\nnp.save(\'test_sequence\',train_sequence)\nnp.save(\'test_cluster_id\',train_cluster_id)\n'"
hparam.py,0,"b'# -*- coding: utf-8 -*-\n#!/usr/bin/env python\n\nimport yaml\n\n\ndef load_hparam(filename):\n    stream = open(filename, \'r\')\n    docs = yaml.load_all(stream)\n    hparam_dict = dict()\n    for doc in docs:\n        for k, v in doc.items():\n            hparam_dict[k] = v\n    return hparam_dict\n\n\ndef merge_dict(user, default):\n    if isinstance(user, dict) and isinstance(default, dict):\n        for k, v in default.items():\n            if k not in user:\n                user[k] = v\n            else:\n                user[k] = merge_dict(user[k], v)\n    return user\n\n\nclass Dotdict(dict):\n    """"""\n    a dictionary that supports dot notation \n    as well as dictionary access notation \n    usage: d = DotDict() or d = DotDict({\'val1\':\'first\'})\n    set attributes: d.val2 = \'second\' or d[\'val2\'] = \'second\'\n    get attributes: d.val2 or d[\'val2\']\n    """"""\n    __getattr__ = dict.__getitem__\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n\n    def __init__(self, dct=None):\n        dct = dict() if not dct else dct\n        for key, value in dct.items():\n            if hasattr(value, \'keys\'):\n                value = Dotdict(value)\n            self[key] = value\n\n\nclass Hparam(Dotdict):\n\n    def __init__(self, file=\'config/config.yaml\'):\n        super(Dotdict, self).__init__()\n        hp_dict = load_hparam(file)\n        hp_dotdict = Dotdict(hp_dict)\n        for k, v in hp_dotdict.items():\n            setattr(self, k, v)\n            \n    __getattr__ = Dotdict.__getitem__\n    __setattr__ = Dotdict.__setitem__\n    __delattr__ = Dotdict.__delitem__\n\n        \nhparam = Hparam()\n'"
speech_embedder_net.py,5,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Sep  5 20:58:34 2018\n\n@author: harry\n""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom hparam import hparam as hp\nfrom utils import get_centroids, get_cossim, calc_loss\n\nclass SpeechEmbedder(nn.Module):\n    \n    def __init__(self):\n        super(SpeechEmbedder, self).__init__()    \n        self.LSTM_stack = nn.LSTM(hp.data.nmels, hp.model.hidden, num_layers=hp.model.num_layer, batch_first=True)\n        for name, param in self.LSTM_stack.named_parameters():\n          if \'bias\' in name:\n             nn.init.constant_(param, 0.0)\n          elif \'weight\' in name:\n             nn.init.xavier_normal_(param)\n        self.projection = nn.Linear(hp.model.hidden, hp.model.proj)\n        \n    def forward(self, x):\n        x, _ = self.LSTM_stack(x.float()) #(batch, frames, n_mels)\n        #only use last frame\n        x = x[:,x.size(1)-1]\n        x = self.projection(x.float())\n        x = x / torch.norm(x, dim=1).unsqueeze(1)\n        return x\n\nclass GE2ELoss(nn.Module):\n    \n    def __init__(self, device):\n        super(GE2ELoss, self).__init__()\n        self.w = nn.Parameter(torch.tensor(10.0).to(device), requires_grad=True)\n        self.b = nn.Parameter(torch.tensor(-5.0).to(device), requires_grad=True)\n        self.device = device\n        \n    def forward(self, embeddings):\n        torch.clamp(self.w, 1e-6)\n        centroids = get_centroids(embeddings)\n        cossim = get_cossim(embeddings, centroids)\n        sim_matrix = self.w*cossim.to(self.device) + self.b\n        loss, _ = calc_loss(sim_matrix)\n        return loss\n'"
train_speech_embedder.py,16,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Sep  5 21:49:16 2018\n\n@author: harry\n""""""\n\nimport os\nimport random\nimport time\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom hparam import hparam as hp\nfrom data_load import SpeakerDatasetTIMIT, SpeakerDatasetTIMITPreprocessed\nfrom speech_embedder_net import SpeechEmbedder, GE2ELoss, get_centroids, get_cossim\n\ndef train(model_path):\n    device = torch.device(hp.device)\n    \n    if hp.data.data_preprocessed:\n        train_dataset = SpeakerDatasetTIMITPreprocessed()\n    else:\n        train_dataset = SpeakerDatasetTIMIT()\n    train_loader = DataLoader(train_dataset, batch_size=hp.train.N, shuffle=True, num_workers=hp.train.num_workers, drop_last=True) \n    \n    embedder_net = SpeechEmbedder().to(device)\n    if hp.train.restore:\n        embedder_net.load_state_dict(torch.load(model_path))\n    ge2e_loss = GE2ELoss(device)\n    #Both net and loss have trainable parameters\n    optimizer = torch.optim.SGD([\n                    {\'params\': embedder_net.parameters()},\n                    {\'params\': ge2e_loss.parameters()}\n                ], lr=hp.train.lr)\n    \n    os.makedirs(hp.train.checkpoint_dir, exist_ok=True)\n    \n    embedder_net.train()\n    iteration = 0\n    for e in range(hp.train.epochs):\n        total_loss = 0\n        for batch_id, mel_db_batch in enumerate(train_loader): \n            mel_db_batch = mel_db_batch.to(device)\n            \n            mel_db_batch = torch.reshape(mel_db_batch, (hp.train.N*hp.train.M, mel_db_batch.size(2), mel_db_batch.size(3)))\n            perm = random.sample(range(0, hp.train.N*hp.train.M), hp.train.N*hp.train.M)\n            unperm = list(perm)\n            for i,j in enumerate(perm):\n                unperm[j] = i\n            mel_db_batch = mel_db_batch[perm]\n            #gradient accumulates\n            optimizer.zero_grad()\n            \n            embeddings = embedder_net(mel_db_batch)\n            embeddings = embeddings[unperm]\n            embeddings = torch.reshape(embeddings, (hp.train.N, hp.train.M, embeddings.size(1)))\n            \n            #get loss, call backward, step optimizer\n            loss = ge2e_loss(embeddings) #wants (Speaker, Utterances, embedding)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(embedder_net.parameters(), 3.0)\n            torch.nn.utils.clip_grad_norm_(ge2e_loss.parameters(), 1.0)\n            optimizer.step()\n            \n            total_loss = total_loss + loss\n            iteration += 1\n            if (batch_id + 1) % hp.train.log_interval == 0:\n                mesg = ""{0}\\tEpoch:{1}[{2}/{3}],Iteration:{4}\\tLoss:{5:.4f}\\tTLoss:{6:.4f}\\t\\n"".format(time.ctime(), e+1,\n                        batch_id+1, len(train_dataset)//hp.train.N, iteration,loss, total_loss / (batch_id + 1))\n                print(mesg)\n                if hp.train.log_file is not None:\n                    with open(hp.train.log_file,\'a\') as f:\n                        f.write(mesg)\n                    \n        if hp.train.checkpoint_dir is not None and (e + 1) % hp.train.checkpoint_interval == 0:\n            embedder_net.eval().cpu()\n            ckpt_model_filename = ""ckpt_epoch_"" + str(e+1) + ""_batch_id_"" + str(batch_id+1) + "".pth""\n            ckpt_model_path = os.path.join(hp.train.checkpoint_dir, ckpt_model_filename)\n            torch.save(embedder_net.state_dict(), ckpt_model_path)\n            embedder_net.to(device).train()\n\n    #save model\n    embedder_net.eval().cpu()\n    save_model_filename = ""final_epoch_"" + str(e + 1) + ""_batch_id_"" + str(batch_id + 1) + "".model""\n    save_model_path = os.path.join(hp.train.checkpoint_dir, save_model_filename)\n    torch.save(embedder_net.state_dict(), save_model_path)\n    \n    print(""\\nDone, trained model saved at"", save_model_path)\n\ndef test(model_path):\n    \n    if hp.data.data_preprocessed:\n        test_dataset = SpeakerDatasetTIMITPreprocessed()\n    else:\n        test_dataset = SpeakerDatasetTIMIT()\n    test_loader = DataLoader(test_dataset, batch_size=hp.test.N, shuffle=True, num_workers=hp.test.num_workers, drop_last=True)\n    \n    embedder_net = SpeechEmbedder()\n    embedder_net.load_state_dict(torch.load(model_path))\n    embedder_net.eval()\n    \n    avg_EER = 0\n    for e in range(hp.test.epochs):\n        batch_avg_EER = 0\n        for batch_id, mel_db_batch in enumerate(test_loader):\n            assert hp.test.M % 2 == 0\n            enrollment_batch, verification_batch = torch.split(mel_db_batch, int(mel_db_batch.size(1)/2), dim=1)\n            \n            enrollment_batch = torch.reshape(enrollment_batch, (hp.test.N*hp.test.M//2, enrollment_batch.size(2), enrollment_batch.size(3)))\n            verification_batch = torch.reshape(verification_batch, (hp.test.N*hp.test.M//2, verification_batch.size(2), verification_batch.size(3)))\n            \n            perm = random.sample(range(0,verification_batch.size(0)), verification_batch.size(0))\n            unperm = list(perm)\n            for i,j in enumerate(perm):\n                unperm[j] = i\n                \n            verification_batch = verification_batch[perm]\n            enrollment_embeddings = embedder_net(enrollment_batch)\n            verification_embeddings = embedder_net(verification_batch)\n            verification_embeddings = verification_embeddings[unperm]\n            \n            enrollment_embeddings = torch.reshape(enrollment_embeddings, (hp.test.N, hp.test.M//2, enrollment_embeddings.size(1)))\n            verification_embeddings = torch.reshape(verification_embeddings, (hp.test.N, hp.test.M//2, verification_embeddings.size(1)))\n            \n            enrollment_centroids = get_centroids(enrollment_embeddings)\n            \n            sim_matrix = get_cossim(verification_embeddings, enrollment_centroids)\n            \n            # calculating EER\n            diff = 1; EER=0; EER_thresh = 0; EER_FAR=0; EER_FRR=0\n            \n            for thres in [0.01*i+0.5 for i in range(50)]:\n                sim_matrix_thresh = sim_matrix>thres\n                \n                FAR = (sum([sim_matrix_thresh[i].float().sum()-sim_matrix_thresh[i,:,i].float().sum() for i in range(int(hp.test.N))])\n                /(hp.test.N-1.0)/(float(hp.test.M/2))/hp.test.N)\n    \n                FRR = (sum([hp.test.M/2-sim_matrix_thresh[i,:,i].float().sum() for i in range(int(hp.test.N))])\n                /(float(hp.test.M/2))/hp.test.N)\n                \n                # Save threshold when FAR = FRR (=EER)\n                if diff> abs(FAR-FRR):\n                    diff = abs(FAR-FRR)\n                    EER = (FAR+FRR)/2\n                    EER_thresh = thres\n                    EER_FAR = FAR\n                    EER_FRR = FRR\n            batch_avg_EER += EER\n            print(""\\nEER : %0.2f (thres:%0.2f, FAR:%0.2f, FRR:%0.2f)""%(EER,EER_thresh,EER_FAR,EER_FRR))\n        avg_EER += batch_avg_EER/(batch_id+1)\n    avg_EER = avg_EER / hp.test.epochs\n    print(""\\n EER across {0} epochs: {1:.4f}"".format(hp.test.epochs, avg_EER))\n        \nif __name__==""__main__"":\n    if hp.training:\n        train(hp.model.model_path)\n    else:\n        test(hp.model.model_path)\n'"
utils.py,10,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Sep 20 16:56:19 2018\n\n@author: harry\n""""""\nimport librosa\nimport numpy as np\nimport torch\nimport torch.autograd as grad\nimport torch.nn.functional as F\n\nfrom hparam import hparam as hp\n\ndef get_centroids_prior(embeddings):\n    centroids = []\n    for speaker in embeddings:\n        centroid = 0\n        for utterance in speaker:\n            centroid = centroid + utterance\n        centroid = centroid/len(speaker)\n        centroids.append(centroid)\n    centroids = torch.stack(centroids)\n    return centroids\n\ndef get_centroids(embeddings):\n    centroids = embeddings.mean(dim=1)\n    return centroids\n\ndef get_centroid(embeddings, speaker_num, utterance_num):\n    centroid = 0\n    for utterance_id, utterance in enumerate(embeddings[speaker_num]):\n        if utterance_id == utterance_num:\n            continue\n        centroid = centroid + utterance\n    centroid = centroid/(len(embeddings[speaker_num])-1)\n    return centroid\n\ndef get_utterance_centroids(embeddings):\n    """"""\n    Returns the centroids for each utterance of a speaker, where\n    the utterance centroid is the speaker centroid without considering\n    this utterance\n\n    Shape of embeddings should be:\n        (speaker_ct, utterance_per_speaker_ct, embedding_size)\n    """"""\n    sum_centroids = embeddings.sum(dim=1)\n    # we want to subtract out each utterance, prior to calculating the\n    # the utterance centroid\n    sum_centroids = sum_centroids.reshape(\n        sum_centroids.shape[0], 1, sum_centroids.shape[-1]\n    )\n    # we want the mean but not including the utterance itself, so -1\n    num_utterances = embeddings.shape[1] - 1\n    centroids = (sum_centroids - embeddings) / num_utterances\n    return centroids\n\ndef get_cossim_prior(embeddings, centroids):\n    # Calculates cosine similarity matrix. Requires (N, M, feature) input\n    cossim = torch.zeros(embeddings.size(0),embeddings.size(1),centroids.size(0))\n    for speaker_num, speaker in enumerate(embeddings):\n        for utterance_num, utterance in enumerate(speaker):\n            for centroid_num, centroid in enumerate(centroids):\n                if speaker_num == centroid_num:\n                    centroid = get_centroid(embeddings, speaker_num, utterance_num)\n                output = F.cosine_similarity(utterance,centroid,dim=0)+1e-6\n                cossim[speaker_num][utterance_num][centroid_num] = output\n    return cossim\n\ndef get_cossim(embeddings, centroids):\n    # number of utterances per speaker\n    num_utterances = embeddings.shape[1]\n    utterance_centroids = get_utterance_centroids(embeddings)\n\n    # flatten the embeddings and utterance centroids to just utterance,\n    # so we can do cosine similarity\n    utterance_centroids_flat = utterance_centroids.view(\n        utterance_centroids.shape[0] * utterance_centroids.shape[1],\n        -1\n    )\n    embeddings_flat = embeddings.view(\n        embeddings.shape[0] * num_utterances,\n        -1\n    )\n    # the cosine distance between utterance and the associated centroids\n    # for that utterance\n    # this is each speaker\'s utterances against his own centroid, but each\n    # comparison centroid has the current utterance removed\n    cos_same = F.cosine_similarity(embeddings_flat, utterance_centroids_flat)\n\n    # now we get the cosine distance between each utterance and the other speakers\'\n    # centroids\n    # to do so requires comparing each utterance to each centroid. To keep the\n    # operation fast, we vectorize by using matrices L (embeddings) and\n    # R (centroids) where L has each utterance repeated sequentially for all\n    # comparisons and R has the entire centroids frame repeated for each utterance\n    centroids_expand = centroids.repeat((num_utterances * embeddings.shape[0], 1))\n    embeddings_expand = embeddings_flat.unsqueeze(1).repeat(1, embeddings.shape[0], 1)\n    embeddings_expand = embeddings_expand.view(\n        embeddings_expand.shape[0] * embeddings_expand.shape[1],\n        embeddings_expand.shape[-1]\n    )\n    cos_diff = F.cosine_similarity(embeddings_expand, centroids_expand)\n    cos_diff = cos_diff.view(\n        embeddings.size(0),\n        num_utterances,\n        centroids.size(0)\n    )\n    # assign the cosine distance for same speakers to the proper idx\n    same_idx = list(range(embeddings.size(0)))\n    cos_diff[same_idx, :, same_idx] = cos_same.view(embeddings.shape[0], num_utterances)\n    cos_diff = cos_diff + 1e-6\n    return cos_diff\n\ndef calc_loss_prior(sim_matrix):\n    # Calculates loss from (N, M, K) similarity matrix\n    per_embedding_loss = torch.zeros(sim_matrix.size(0), sim_matrix.size(1))\n    for j in range(len(sim_matrix)):\n        for i in range(sim_matrix.size(1)):\n            per_embedding_loss[j][i] = -(sim_matrix[j][i][j] - ((torch.exp(sim_matrix[j][i]).sum()+1e-6).log_()))\n    loss = per_embedding_loss.sum()    \n    return loss, per_embedding_loss\n\ndef calc_loss(sim_matrix):\n    same_idx = list(range(sim_matrix.size(0)))\n    pos = sim_matrix[same_idx, :, same_idx]\n    neg = (torch.exp(sim_matrix).sum(dim=2) + 1e-6).log_()\n    per_embedding_loss = -1 * (pos - neg)\n    loss = per_embedding_loss.sum()\n    return loss, per_embedding_loss\n\ndef normalize_0_1(values, max_value, min_value):\n    normalized = np.clip((values - min_value) / (max_value - min_value), 0, 1)\n    return normalized\n\ndef mfccs_and_spec(wav_file, wav_process = False, calc_mfccs=False, calc_mag_db=False):    \n    sound_file, _ = librosa.core.load(wav_file, sr=hp.data.sr)\n    window_length = int(hp.data.window*hp.data.sr)\n    hop_length = int(hp.data.hop*hp.data.sr)\n    duration = hp.data.tisv_frame * hp.data.hop + hp.data.window\n    \n    # Cut silence and fix length\n    if wav_process == True:\n        sound_file, index = librosa.effects.trim(sound_file, frame_length=window_length, hop_length=hop_length)\n        length = int(hp.data.sr * duration)\n        sound_file = librosa.util.fix_length(sound_file, length)\n        \n    spec = librosa.stft(sound_file, n_fft=hp.data.nfft, hop_length=hop_length, win_length=window_length)\n    mag_spec = np.abs(spec)\n    \n    mel_basis = librosa.filters.mel(hp.data.sr, hp.data.nfft, n_mels=hp.data.nmels)\n    mel_spec = np.dot(mel_basis, mag_spec)\n    \n    mag_db = librosa.amplitude_to_db(mag_spec)\n    #db mel spectrogram\n    mel_db = librosa.amplitude_to_db(mel_spec).T\n    \n    mfccs = None\n    if calc_mfccs:\n        mfccs = np.dot(librosa.filters.dct(40, mel_db.shape[0]), mel_db).T\n    \n    return mfccs, mel_db, mag_db\n\nif __name__ == ""__main__"":\n    w = grad.Variable(torch.tensor(1.0))\n    b = grad.Variable(torch.tensor(0.0))\n    embeddings = torch.tensor([[0,1,0],[0,0,1], [0,1,0], [0,1,0], [1,0,0], [1,0,0]]).to(torch.float).reshape(3,2,3)\n    centroids = get_centroids(embeddings)\n    cossim = get_cossim(embeddings, centroids)\n    sim_matrix = w*cossim + b\n    loss, per_embedding_loss = calc_loss(sim_matrix)\n'"
