file_path,api_count,code
bin/create_dataset_ytbid.py,0,"b'from IPython import embed\n\nimport numpy as np\nimport pickle\nimport os\nimport cv2\nimport functools\nimport xml.etree.ElementTree as ET\nimport sys\nimport multiprocessing as mp\n\nfrom multiprocessing import Pool\nfrom fire import Fire\nfrom tqdm import tqdm\nfrom glob import glob\n\nsys.path.append(os.getcwd())\nfrom net.config import config\nfrom lib.utils import get_instance_image, add_box_img\n\n\ndef worker(output_dir, video_dir):\n    instance_crop_size = 500\n    if \'YT-BB\' in video_dir:\n        image_names = glob(os.path.join(video_dir, \'*.jpg\'))\n        image_names = sorted(image_names, key=lambda x: int(x.split(\'/\')[-1].split(\'_\')[1]))\n        video_name = \'_\'.join(os.path.basename(video_dir).split(\'_\')[:-1])\n\n        with open(\'/dataset_ssd/std_xml_ytb/\' + video_name + \'.pkl\', \'rb\') as f:\n            std_xml_dict = pickle.load(f)\n\n        save_folder = os.path.join(output_dir, video_name)\n        if not os.path.exists(save_folder):\n            os.mkdir(save_folder)\n\n        trajs = {}\n        for image_name in image_names:\n            img = cv2.imread(image_name)\n            h_img, w_img, _ = img.shape\n            img_mean = tuple(map(int, img.mean(axis=(0, 1))))\n            frame = image_name.split(\'_\')[-2]\n\n            if int(frame) == 0:\n                anno = std_xml_dict[str(int(frame))]\n            else:\n                anno = std_xml_dict[frame]\n\n            filename = \'_\'.join(image_name.split(\'/\')[-1].split(\'_\')[:-1])\n            for class_id in anno.keys():\n                for track_id in anno[class_id].keys():\n                    class_name, present, xmin_scale, xmax_scale, ymin_scale, ymax_scale = anno[class_id][track_id]\n                    new_track_id = class_id.zfill(3) + track_id.zfill(3)\n                    bbox = np.array(list(map(float, [xmin_scale, xmax_scale, ymin_scale, ymax_scale]))) * np.array(\n                        [w_img, w_img, h_img, h_img])\n                    if present == \'present\':\n                        if new_track_id in trajs.keys():\n                            trajs[new_track_id].append(filename)\n                        else:\n                            trajs[new_track_id] = [filename]\n                        bbox = np.array(\n                            [(bbox[1] + bbox[0]) / 2, (bbox[3] + bbox[2]) / 2, bbox[1] - bbox[0] + 1,\n                             bbox[3] - bbox[2] + 1])\n                        instance_img, w, h, _ = get_instance_image(img, bbox,\n                                                                   config.exemplar_size, instance_crop_size,\n                                                                   config.context_amount,\n                                                                   img_mean)\n                        instance_img_name = os.path.join(save_folder,\n                                                         filename + "".{}.x_{:.2f}_{:.2f}_{:.0f}_{:.0f}.jpg"".format(\n                                                             new_track_id,\n                                                             w, h, w_img, h_img))\n                        cv2.imwrite(instance_img_name, instance_img)\n\n                    elif present == \'absent\':\n                        continue\n\n    else:\n        image_names = glob(os.path.join(video_dir, \'*.JPEG\'))\n        image_names = sorted(image_names, key=lambda x: int(x.split(\'/\')[-1].split(\'.\')[0]))\n        video_name = video_dir.split(\'/\')[-1]\n        save_folder = os.path.join(output_dir, video_name)\n        if not os.path.exists(save_folder):\n            os.mkdir(save_folder)\n        trajs = {}\n        for image_name in image_names:\n            img = cv2.imread(image_name)\n            h_img, w_img, _ = img.shape\n            img_mean = tuple(map(int, img.mean(axis=(0, 1))))\n            anno_name = image_name.replace(\'Data\', \'Annotations\')\n            anno_name = anno_name.replace(\'JPEG\', \'xml\')\n            tree = ET.parse(anno_name)\n            root = tree.getroot()\n            bboxes = []\n            filename = root.find(\'filename\').text\n            for obj in root.iter(\'object\'):\n                bbox = obj.find(\'bndbox\')\n                bbox = list(map(int, [bbox.find(\'xmin\').text,\n                                      bbox.find(\'ymin\').text,\n                                      bbox.find(\'xmax\').text,\n                                      bbox.find(\'ymax\').text]))\n                trkid = int(obj.find(\'trackid\').text)\n                if trkid in trajs:\n                    trajs[trkid].append(filename)\n                else:\n                    trajs[trkid] = [filename]\n                bbox = np.array(\n                    [(bbox[2] + bbox[0]) / 2, (bbox[3] + bbox[1]) / 2, bbox[2] - bbox[0] + 1,\n                     bbox[3] - bbox[1] + 1])\n\n                instance_img, w, h, _ = get_instance_image(img, bbox,\n                                                           config.exemplar_size, instance_crop_size,\n                                                           config.context_amount,\n                                                           img_mean)\n                instance_img_name = os.path.join(save_folder,\n                                                 filename + "".{:02d}.x_{:.2f}_{:.2f}_{:.0f}_{:.0f}.jpg"".format(trkid, w,\n                                                                                                               h, w_img,\n                                                                                                               h_img))\n                cv2.imwrite(instance_img_name, instance_img)\n    return video_name, trajs\n\n\ndef processing(vid_dir, ytb_dir, output_dir, num_threads=mp.cpu_count()):\n    # get all 4417 videos in vid and all video in ytbb\n    vid_video_dir = os.path.join(vid_dir, \'Data/VID\')\n    ytb_video_dir = ytb_dir\n\n    # -------------------------------------------------------------------------------------\n\n    # all_videos = glob(os.path.join(ytb_video_dir, \'v*/youtube_dection_frame_temp/*\'))\n    # all_videos = glob(\'/mnt/diska1/YT-BB/v1/youtube_dection_frame_temp/130dH0FNXio_*\')\n    # -------------------------------------------------------------------------------------\n\n    all_videos = glob(os.path.join(vid_video_dir, \'train/ILSVRC2015_VID_train_0000/*\')) + \\\n                 glob(os.path.join(vid_video_dir, \'train/ILSVRC2015_VID_train_0001/*\')) + \\\n                 glob(os.path.join(vid_video_dir, \'train/ILSVRC2015_VID_train_0002/*\')) + \\\n                 glob(os.path.join(vid_video_dir, \'train/ILSVRC2015_VID_train_0003/*\')) + \\\n                 glob(os.path.join(vid_video_dir, \'val/*\')) + \\\n                 glob(os.path.join(ytb_video_dir, \'v*/youtube_dection_frame_temp/*\'))\n    meta_data = []\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    all_videos = [x for x in all_videos if \'imagelist\' not in x]\n\n    # for video in tqdm(all_videos):\n    #     functools.partial(worker, output_dir)(video)\n\n    # -------------------------------------------------------------------------------------\n    # load former meta_data\n    # with open(\'/dataset_ssd/ytb_vid_rpn/meta_data.pkl\', \'rb\') as f:\n    #     former_pkl = pickle.load(f)\n    # former_dict = {x[0]: x[1] for x in former_pkl if \'ILSVRC2015\' in x[0]}\n    # former_vid = []\n    # for k in former_dict.keys():\n    #     former_vid.append((k, former_dict[k]))\n    # meta_data.extend(former_vid)\n    # -------------------------------------------------------------------------------------\n\n    with Pool(processes=num_threads) as pool:\n        for ret in tqdm(pool.imap_unordered(\n                functools.partial(worker, output_dir), all_videos), total=len(all_videos)):\n            meta_data.append(ret)\n            tqdm.write(ret[0])\n\n    # save meta data\n    pickle.dump(meta_data, open(os.path.join(output_dir, ""meta_data.pkl""), \'wb\'))\n\n\nif __name__ == \'__main__\':\n    Fire(processing)\n'"
bin/create_lmdb.py,0,"b""import lmdb\nimport cv2\nimport numpy as np\nimport os\nimport hashlib\nimport functools\n\nfrom glob import glob\nfrom fire import Fire\nfrom tqdm import tqdm\nfrom multiprocessing import Pool\nfrom IPython import embed\nimport multiprocessing as mp\n\n\ndef worker(video_name):\n    image_names = glob(video_name + '/*')\n    kv = {}\n    for image_name in image_names:\n        img = cv2.imread(image_name)\n        _, img_encode = cv2.imencode('.jpg', img)\n        img_encode = img_encode.tobytes()\n        kv[hashlib.md5(image_name.encode()).digest()] = img_encode\n    return kv\n\n\ndef create_lmdb(data_dir, output_dir, num_threads=mp.cpu_count()):\n    video_names = glob(data_dir + '/*')\n    video_names = [x for x in video_names if 'meta_data.pkl' not in x]\n    # video_names = [x for x in video_names if os.path.isdir(x)]\n    db = lmdb.open(output_dir, map_size=int(200e9))\n    with Pool(processes=num_threads) as pool:\n        for ret in tqdm(pool.imap_unordered(functools.partial(worker), video_names), total=len(video_names)):\n            with db.begin(write=True) as txn:\n                for k, v in ret.items():\n                    txn.put(k, v)\n\n\nif __name__ == '__main__':\n    Fire(create_lmdb)\n"""
bin/test_OTB.py,0,"b'import argparse\nimport os\nimport glob\nimport numpy as np\nimport re\nimport json\nimport matplotlib\nimport setproctitle\nimport functools\nimport multiprocessing as mp\nimport matplotlib.pyplot as plt\nimport sys\n\nsys.path.append(os.getcwd())\n\nfrom net.run_SiamRPN import run_SiamRPN\nfrom tqdm import tqdm\nfrom IPython import embed\nfrom multiprocessing import Pool\n\n\ndef embeded_numbers(s):\n    re_digits = re.compile(r\'(\\d+)\')\n    pieces = re_digits.split(s)\n    return int(pieces[1])\n\n\ndef embeded_numbers_results(s):\n    re_digits = re.compile(r\'(\\d+)\')\n    pieces = re_digits.split(s)\n    return int(pieces[-2])\n\n\ndef cal_iou(box1, box2):\n    r""""""\n\n    :param box1: x1,y1,w,h\n    :param box2: x1,y1,w,h\n    :return: iou\n    """"""\n    x11 = box1[0]\n    y11 = box1[1]\n    x21 = box1[0] + box1[2] - 1\n    y21 = box1[1] + box1[3] - 1\n    area_1 = (x21 - x11 + 1) * (y21 - y11 + 1)\n\n    x12 = box2[0]\n    y12 = box2[1]\n    x22 = box2[0] + box2[2] - 1\n    y22 = box2[1] + box2[3] - 1\n    area_2 = (x22 - x12 + 1) * (y22 - y12 + 1)\n\n    x_left = max(x11, x12)\n    x_right = min(x21, x22)\n    y_top = max(y11, y12)\n    y_down = min(y21, y22)\n\n    inter_area = max(x_right - x_left + 1, 0) * max(y_down - y_top + 1, 0)\n    iou = inter_area / (area_1 + area_2 - inter_area)\n    return iou\n\n\ndef cal_success(iou):\n    success_all = []\n    overlap_thresholds = np.arange(0, 1.05, 0.05)\n    for overlap_threshold in overlap_thresholds:\n        success = sum(np.array(iou) > overlap_threshold) / len(iou)\n        success_all.append(success)\n    return np.array(success_all)\n\n\n# def worker(video_paths, model_path):\n#     results_ = {}\n#     for video_path in tqdm(video_paths, total=len(video_paths)):\n#         groundtruth_path = video_path + \'/groundtruth_rect.txt\'\n#         assert os.path.isfile(groundtruth_path), \'groundtruth of \' + video_path + \' doesn\\\'t exist\'\n#         with open(groundtruth_path, \'r\') as f:\n#             boxes = f.readlines()\n#         if \',\' in boxes[0]:\n#             boxes = [list(map(int, box.split(\',\'))) for box in boxes]\n#         else:\n#             boxes = [list(map(int, box.split())) for box in boxes]\n#         result = run_SiamFC(video_path, model_path, boxes[0])\n#         results_box_video = result[\'res\']\n#         results_[os.path.abspath(model_path)][video_path.split(\'/\')[-1]] = results_box_video\n#         return results_\n\n\nif __name__ == \'__main__\':\n    program_name = os.getcwd().split(\'/\')[-1]\n    setproctitle.setproctitle(\'zrq test \' + program_name)\n    parser = argparse.ArgumentParser(description=\'Test some models on OTB2015 or OTB2013\')\n    parser.add_argument(\'--model_paths\', \'-ms\', dest=\'model_paths\', nargs=\'+\',\n                        help=\'the path of models or the path of a model or folder\')\n    parser.add_argument(\'--videos\', \'-v\', dest=\'videos\')  # choices=[\'tb50\', \'tb100\', \'cvpr2013\']\n    parser.add_argument(\'--save_name\', \'-n\', dest=\'save_name\', default=\'result.json\')\n    parser.add_argument(\'--data_path\', \'-d\', dest=\'data_path\', default=\'/dataset_ssd/OTB/data/\')\n    args = parser.parse_args()\n    model_paths = [1, 2, 3]\n\n    # ------------ prepare data  -----------\n    data_path = args.data_path\n    if \'50\' in args.videos:\n        direct_file = data_path + \'tb_50.txt\'\n    elif \'100\' in args.videos:\n        direct_file = data_path + \'tb_100.txt\'\n    elif \'13\' in args.videos:\n        direct_file = data_path + \'cvpr13.txt\'\n    else:\n        raise ValueError(\'videos setting wrong\')\n    with open(direct_file, \'r\') as f:\n        direct_lines = f.readlines()\n    video_names = np.sort([x.split(\'\\t\')[0] for x in direct_lines])\n    video_paths = [data_path + x for x in video_names]\n\n    # ------------ prepare models  -----------\n    # input_paths = [os.path.abspath(x) for x in args.model_paths]\n    # model_paths = []\n    # for input_path in input_paths:\n    #     if os.path.isdir(input_path):\n    #         input_path = os.path.abspath(input_path)\n    #         model_path = sorted([x for x in os.listdir(input_path) if \'pth\' in x], key=embeded_numbers)\n    #         model_path = [input_path + \'/\' + x for x in model_path]\n    #         model_paths.extend(model_path)\n    #     elif os.path.isfile(input_path):\n    #         model_path = os.path.abspath(input_path)\n    #         model_paths.append(model_path)\n    #     else:\n    #         raise ValueError(\'model_path setting wrong\')\n\n    # ------------ starting validation  -----------\n    results = {}\n    for model_path in tqdm(model_paths, total=len(model_paths)):\n        # results[os.path.abspath(model_path)] = {}\n        results[model_path] = {}\n\n        for video_path in tqdm(video_paths, total=len(video_paths)):\n            # video_path = video_paths[-10]\n            # video_path =\n            groundtruth_path = video_path + \'/groundtruth_rect.txt\'\n            assert os.path.isfile(groundtruth_path), \'groundtruth of \' + video_path + \' doesn\\\'t exist\'\n            with open(groundtruth_path, \'r\') as f:\n                boxes = f.readlines()\n            if \',\' in boxes[0]:\n                boxes = [list(map(int, box.split(\',\'))) for box in boxes]\n            else:\n                boxes = [list(map(int, box.split())) for box in boxes]\n            boxes = [np.array(box) - [1, 1, 0, 0] for box in boxes]\n            result = run_SiamRPN(video_path, model_path, boxes[0])\n            result_boxes = [np.array(box) + [1, 1, 0, 0] for box in result[\'res\']]\n            results[os.path.abspath(model_path)][video_path.split(\'/\')[-1]] = [box.tolist() for box in result_boxes]\n\n    # with Pool(processes=mp.cpu_count()) as pool:\n    #     for ret in tqdm(pool.imap_unordered(\n    #             functools.partial(worker, video_paths), model_paths), total=len(model_paths)):\n    #         results.update(ret)\n    json.dump(results, open(args.save_name, \'w\'))\n\n    # ------------ starting evaluation  -----------\n    results_eval = {}\n    for model in sorted(list(results.keys()), key=embeded_numbers_results):\n        results_eval[model] = {}\n        success_all_video = []\n        for video in results[model].keys():\n            result_boxes = results[model][video]\n            with open(data_path + video + \'/groundtruth_rect.txt\', \'r\') as f:\n                result_boxes_gt = f.readlines()\n            if \',\' in result_boxes_gt[0]:\n                result_boxes_gt = [list(map(int, box.split(\',\'))) for box in result_boxes_gt]\n            else:\n                result_boxes_gt = [list(map(int, box.split())) for box in result_boxes_gt]\n            result_boxes_gt = [np.array(box) for box in result_boxes_gt]\n            iou = list(map(cal_iou, result_boxes, result_boxes_gt))\n            success = cal_success(iou)\n            auc = np.mean(success)\n            success_all_video.append(success)\n            results_eval[model][video] = auc\n        results_eval[model][\'all_video\'] = np.mean(success_all_video)\n        print(model.split(\'/\')[-1] + \' : \', np.mean(success_all_video))\n    json.dump(results_eval, open(\'eval_\' + args.save_name, \'w\'))\n'"
bin/train_siamrpn.py,0,"b""import os\nimport sys\nimport setproctitle\n\nfrom fire import Fire\n\nsys.path.append(os.getcwd())\nfrom net.train import train\nfrom IPython import embed\n\nif __name__ == '__main__':\n    program_name = 'zrq train ' + os.getcwd().split('/')[-1]\n    setproctitle.setproctitle(program_name)\n    Fire(train)\n"""
lib/__init__.py,0,b''
lib/custom_transforms.py,1,"b'import torch\nimport numpy as np\nimport cv2\n\n\nclass RandomStretch(object):\n    def __init__(self, max_stretch=0.05):\n        """"""Random resize image according to the stretch\n        Args:\n            max_stretch(float): 0 to 1 value   \n        """"""\n        self.max_stretch = max_stretch\n\n    def __call__(self, sample):\n        """"""\n        Args:\n            sample(numpy array): 3 or 1 dim image\n        """"""\n        scale_h = 1.0 + np.random.uniform(-self.max_stretch, self.max_stretch)\n        scale_w = 1.0 + np.random.uniform(-self.max_stretch, self.max_stretch)\n        h, w = sample.shape[:2]\n        shape = int(w * scale_w), int(h * scale_h)\n        return cv2.resize(sample, shape, cv2.INTER_LINEAR)\n\n\nclass CenterCrop(object):\n    def __init__(self, size):\n        """"""Crop the image in the center according the given size \n            if size greater than image size, zero padding will adpot\n        Args:\n            size (tuple): desired size\n        """"""\n        self.size = size\n\n    def __call__(self, sample):\n        """"""\n        Args:\n            sample(numpy array): 3 or 1 dim image\n        """"""\n        shape = sample.shape[:2]\n        cy, cx = (shape[0] - 1) // 2, (shape[1] - 1) // 2\n        ymin, xmin = cy - self.size[0] // 2, cx - self.size[1] // 2\n        ymax, xmax = cy + self.size[0] // 2 + self.size[0] % 2, \\\n                     cx + self.size[1] // 2 + self.size[1] % 2\n        left = right = top = bottom = 0\n        im_h, im_w = shape\n        if xmin < 0:\n            left = int(abs(xmin))\n        if xmax > im_w:\n            right = int(xmax - im_w)\n        if ymin < 0:\n            top = int(abs(ymin))\n        if ymax > im_h:\n            bottom = int(ymax - im_h)\n\n        xmin = int(max(0, xmin))\n        xmax = int(min(im_w, xmax))\n        ymin = int(max(0, ymin))\n        ymax = int(min(im_h, ymax))\n        im_patch = sample[ymin:ymax, xmin:xmax]\n        if left != 0 or right != 0 or top != 0 or bottom != 0:\n            im_patch = cv2.copyMakeBorder(im_patch, top, bottom, left, right,\n                                          cv2.BORDER_CONSTANT, value=0)\n        return im_patch\n\n\nclass RandomCrop(object):\n    def __init__(self, size, max_translate):\n        """"""Crop the image in the center according the given size \n            if size greater than image size, zero padding will adpot\n        Args:\n            size (tuple): desired size\n            max_translate: max translate of random shift\n        """"""\n        self.size = size\n        self.max_translate = max_translate\n\n    def __call__(self, sample):\n        """"""\n        Args:\n            sample(numpy array): 3 or 1 dim image\n        """"""\n        shape = sample.shape[:2]\n        cy_o = (shape[0] - 1) // 2\n        cx_o = (shape[1] - 1) // 2\n        cy = np.random.randint(cy_o - self.max_translate,\n                               cy_o + self.max_translate + 1)\n        cx = np.random.randint(cx_o - self.max_translate,\n                               cx_o + self.max_translate + 1)\n        assert abs(cy - cy_o) <= self.max_translate and \\\n               abs(cx - cx_o) <= self.max_translate\n        ymin = cy - self.size[0] // 2\n        xmin = cx - self.size[1] // 2\n        ymax = cy + self.size[0] // 2 + self.size[0] % 2\n        xmax = cx + self.size[1] // 2 + self.size[1] % 2\n        left = right = top = bottom = 0\n        im_h, im_w = shape\n        if xmin < 0:\n            left = int(abs(xmin))\n        if xmax > im_w:\n            right = int(xmax - im_w)\n        if ymin < 0:\n            top = int(abs(ymin))\n        if ymax > im_h:\n            bottom = int(ymax - im_h)\n\n        xmin = int(max(0, xmin))\n        xmax = int(min(im_w, xmax))\n        ymin = int(max(0, ymin))\n        ymax = int(min(im_h, ymax))\n        im_patch = sample[ymin:ymax, xmin:xmax]\n        if left != 0 or right != 0 or top != 0 or bottom != 0:\n            im_patch = cv2.copyMakeBorder(im_patch, top, bottom, left, right,\n                                          cv2.BORDER_CONSTANT, value=0)\n        return im_patch\n\n\nclass ColorAug(object):\n    def __init__(self, type_in=\'z\'):\n        if type_in == \'z\':\n            rgb_var = np.array([[3.2586416e+03, 2.8992207e+03, 2.6392236e+03],\n                                [2.8992207e+03, 3.0958174e+03, 2.9321748e+03],\n                                [2.6392236e+03, 2.9321748e+03, 3.4533721e+03]])\n        if type_in == \'x\':\n            rgb_var = np.array([[2.4847285e+03, 2.1796064e+03, 1.9766885e+03],\n                                [2.1796064e+03, 2.3441289e+03, 2.2357402e+03],\n                                [1.9766885e+03, 2.2357402e+03, 2.7369697e+03]])\n        self.v, _ = np.linalg.eig(rgb_var)\n        self.v = np.sqrt(self.v)\n\n    def __call__(self, sample):\n        return sample + 0.1 * self.v * np.random.randn(3)\n\n\nclass RandomBlur(object):\n    def __init__(self, ratio):\n        self.ratio = ratio\n\n    def __call__(self, sample):\n        if np.random.rand(1) < self.ratio:\n            # random kernel size\n            kernel_size = np.random.choice([3, 5, 7])\n            # random gaussian sigma\n            sigma = np.random.rand() * 5\n            return cv2.GaussianBlur(sample, (kernel_size, kernel_size), sigma)\n        else:\n            return sample\n\n\nclass Normalize(object):\n    def __init__(self):\n        self.mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n        self.std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n\n    def __call__(self, sample):\n        return (sample / 255. - self.mean) / self.std\n\n\nclass ToTensor(object):\n    def __call__(self, sample):\n        sample = sample.transpose(2, 0, 1)\n        return torch.from_numpy(sample.astype(np.float32))\n'"
lib/generate_anchors.py,0,"b'import numpy as np\nfrom IPython import embed\n\n\ndef generate_anchors(total_stride, base_size, scales, ratios, score_size):\n    anchor_num = len(ratios) * len(scales)\n    anchor = np.zeros((anchor_num, 4), dtype=np.float32)\n    size = base_size * base_size\n    count = 0\n    for ratio in ratios:\n        # ws = int(np.sqrt(size * 1.0 / ratio))\n        ws = int(np.sqrt(size / ratio))\n        hs = int(ws * ratio)\n        for scale in scales:\n            wws = ws * scale\n            hhs = hs * scale\n            anchor[count, 0] = 0\n            anchor[count, 1] = 0\n            anchor[count, 2] = wws\n            anchor[count, 3] = hhs\n            count += 1\n\n    anchor = np.tile(anchor, score_size * score_size).reshape((-1, 4))\n    # (5,4x225) to (225x5,4)\n    ori = - (score_size // 2) * total_stride\n    # the left displacement\n    xx, yy = np.meshgrid([ori + total_stride * dx for dx in range(score_size)],\n                         [ori + total_stride * dy for dy in range(score_size)])\n    # (15,15)\n    xx, yy = np.tile(xx.flatten(), (anchor_num, 1)).flatten(), \\\n             np.tile(yy.flatten(), (anchor_num, 1)).flatten()\n    # (15,15) to (225,1) to (5,225) to (225x5,1)\n    anchor[:, 0], anchor[:, 1] = xx.astype(np.float32), yy.astype(np.float32)\n    return anchor\n'"
lib/loss.py,20,"b'import torch, pdb\nimport torch.nn\nfrom IPython import embed\nimport torch.nn.functional as F\nimport pdb\nimport random\nimport numpy as np\nimport time\nimport functools\n\nfrom .utils import nms, add_box_img, nms_worker\nfrom torch.multiprocessing import Pool, Manager\n\n\ndef rpn_cross_entropy(input, target):\n    r""""""\n    :param input: (15x15x5,2)\n    :param target: (15x15x5,)\n    :return:\n    """"""\n    mask_ignore = target == -1\n    mask_calcu = 1 - mask_ignore\n    loss = F.cross_entropy(input=input[mask_calcu], target=target[mask_calcu])\n    return loss\n\n\n# def rpn_cross_entropy_balance_worker(num_pos, num_neg, anchors, ohem, x):\n#     input, target = x\n#     min_pos = min(len(np.where(target.cpu() == 1)[0]), num_pos)\n#     min_neg = int(min(len(np.where(target.cpu() == 1)[0]) * num_neg / num_pos, num_neg))\n#     if not ohem:\n#         pos_index = random.sample(np.where(target.cpu() == 1)[0].tolist(), min_pos)\n#         neg_index = random.sample(np.where(target.cpu() == 0)[0].tolist(), min_neg)\n#         if len(pos_index) > 0:\n#             pos_loss_bid = F.cross_entropy(input=input[pos_index],\n#                                            target=target[pos_index], reduction=\'none\')\n#             neg_loss_bid = F.cross_entropy(input=input[neg_index],\n#                                            target=target[neg_index], reduction=\'none\')\n#         else:\n#             pos_loss_bid = torch.FloatTensor([0]).cuda()\n#             neg_index = random.sample(np.where(target.cpu() == 0)[0].tolist(), num_neg)\n#             neg_loss_bid = F.cross_entropy(input=input[neg_index],\n#                                            target=target[neg_index], reduction=\'none\')\n#         loss_bid = (pos_loss_bid.mean() + neg_loss_bid.mean()) / 2\n#     else:\n#         pos_index = np.where(target.cpu() == 1)[0].tolist()\n#         neg_index = np.where(target.cpu() == 0)[0].tolist()\n#         if len(pos_index) > 0:\n#             pos_loss_bid = F.cross_entropy(input=input[pos_index],\n#                                            target=target[pos_index], reduction=\'none\')\n#             selected_pos_index = nms(anchors[pos_index], pos_loss_bid.cpu().detach().numpy(), min_pos)\n#             pos_loss_bid_ohem = pos_loss_bid[selected_pos_index]\n#             neg_loss_bid = F.cross_entropy(input=input[neg_index],\n#                                            target=target[neg_index], reduction=\'none\')\n#             selected_neg_index = nms(anchors[neg_index], neg_loss_bid.cpu().detach().numpy(), min_neg)\n#             neg_loss_bid_ohem = neg_loss_bid[selected_neg_index]\n#         else:\n#             pos_loss_bid = torch.FloatTensor([0]).cuda()\n#             pos_loss_bid_ohem = pos_loss_bid\n#             neg_loss_bid = F.cross_entropy(input=input[neg_index],\n#                                            target=target[neg_index], reduction=\'none\')\n#             selected_neg_index = nms(anchors[neg_index], neg_loss_bid.cpu().detach().numpy(), num_neg)\n#             neg_loss_bid_ohem = neg_loss_bid[selected_neg_index]\n#         loss_bid = (pos_loss_bid_ohem.mean() + neg_loss_bid_ohem.mean()) / 2\n#     return loss_bid\n#\n#\n# def rpn_cross_entropy_balance_parallel(input, target, num_pos, num_neg, anchors, ohem=None, num_threads=4):\n#     loss_all = []\n#     input, target = input.cpu(), target.cpu()\n#     x = [[input[i], target[i]] for i in range(target.shape[0])]\n#     functools.partial(rpn_cross_entropy_balance_worker, num_pos, num_neg, anchors, ohem)([input[0], target[0]])\n#     with Pool(processes=num_threads) as pool:\n#         for loss_bid in pool.imap_unordered(\n#                 functools.partial(rpn_cross_entropy_balance_worker, num_pos, num_neg, anchors, ohem), x):\n#             loss_all.append(loss_bid)\n#     final_loss = torch.stack(loss_all).mean()\n#     return final_loss\n\n# def rpn_cross_entropy_balance_worker(num_pos, num_neg, anchors, ohem, x):\n#     batch_id, input, target = x\n#     min_pos = min(len(np.where(target.cpu() == 1)[0]), num_pos)\n#     min_neg = int(min(len(np.where(target.cpu() == 1)[0]) * num_neg / num_pos, num_neg))\n#     pos_index = np.where(target.cpu() == 1)[0].tolist()\n#     neg_index = np.where(target.cpu() == 0)[0].tolist()\n#     if len(pos_index) > 0:\n#         pos_loss_bid = F.cross_entropy(input=input[pos_index],\n#                                        target=target[pos_index], reduction=\'none\')\n#         selected_pos_index = nms(anchors[pos_index], pos_loss_bid.cpu().detach().numpy(), min_pos)\n#\n#         neg_loss_bid = F.cross_entropy(input=input[neg_index],\n#                                        target=target[neg_index], reduction=\'none\')\n#         selected_neg_index = nms(anchors[neg_index], neg_loss_bid.cpu().detach().numpy(), min_neg)\n#     else:\n#         selected_pos_index = [0]\n#\n#         neg_loss_bid = F.cross_entropy(input=input[neg_index],\n#                                        target=target[neg_index], reduction=\'none\')\n#         selected_neg_index = nms(anchors[neg_index], neg_loss_bid.cpu().detach().numpy(), num_neg)\n#\n#     return batch_id, selected_pos_index, selected_neg_index\n#\n#\n# def rpn_cross_entropy_balance_parallel(input, target, num_pos, num_neg, anchors, ohem=True, num_threads=4):\n#     selected_pos_index_all = {}\n#     selected_neg_index_all = {}\n#     # input, target = input.cpu().detach(), target.cpu().detach()\n#     x = [[i, input.cpu().detach()[i], target.cpu().detach()[i]] for i in range(target.shape[0])]\n#     # functools.partial(rpn_cross_entropy_balance_worker, num_pos, num_neg, anchors, ohem)(x[0])\n#     with Pool(processes=num_threads) as pool:\n#         for ret in pool.imap_unordered(\n#                 functools.partial(rpn_cross_entropy_balance_worker, num_pos, num_neg, anchors, ohem), x):\n#             selected_pos_index_all[ret[0]] = ret[1]\n#             selected_neg_index_all[ret[0]] = ret[2]\n#\n#     loss_all = []\n#     for batch_id in range(target.shape[0]):\n#         pos_index = np.where(target[batch_id].cpu() == 1)[0].tolist()\n#         neg_index = np.where(target[batch_id].cpu() == 0)[0].tolist()\n#         if len(pos_index) > 0:\n#             pos_loss_bid = F.cross_entropy(input=input[batch_id][pos_index],\n#                                            target=target[batch_id][pos_index], reduction=\'none\')\n#             selected_pos_index = selected_pos_index_all[batch_id]\n#             pos_loss_bid_ohem = pos_loss_bid[selected_pos_index]\n#             neg_loss_bid = F.cross_entropy(input=input[batch_id][neg_index],\n#                                            target=target[batch_id][neg_index], reduction=\'none\')\n#             selected_neg_index = selected_neg_index_all[batch_id]\n#             neg_loss_bid_ohem = neg_loss_bid[selected_neg_index]\n#         else:\n#             pos_loss_bid = torch.FloatTensor([0]).cuda()\n#             pos_loss_bid_ohem = pos_loss_bid\n#             neg_loss_bid = F.cross_entropy(input=input[batch_id][neg_index],\n#                                            target=target[batch_id][neg_index], reduction=\'none\')\n#             selected_neg_index = selected_neg_index_all[batch_id]\n#             neg_loss_bid_ohem = neg_loss_bid[selected_neg_index]\n#         loss_bid = (pos_loss_bid_ohem.mean() + neg_loss_bid_ohem.mean()) / 2\n#         loss_all.append(loss_bid)\n#     final_loss = torch.stack(loss_all).mean()\n#     return final_loss\n\n\n# def rpn_cross_entropy_balance_ohem_parallel(input, target, num_pos, num_neg, anchors, ohem=True, num_threads=4):\n#     r""""""\n#     :param input: (N,1125,2)\n#     :param target: (15x15x5,)\n#     :return:\n#     """"""\n#     loss_all = []\n#     loss_all_pos = []\n#     loss_all_neg = []\n#     pos_index_all = []\n#     neg_index_all = []\n#     selected_pos_index_all = []\n#     selected_neg_index_all = []\n#     min_pos_all = []\n#     min_neg_all = []\n#\n#     for batch_id in range(target.shape[0]):\n#         min_pos = min(len(np.where(target[batch_id].cpu() == 1)[0]), num_pos)\n#         min_neg = int(min(len(np.where(target[batch_id].cpu() == 1)[0]) * num_neg / num_pos, num_neg))\n#         pos_index = np.where(target[batch_id].cpu() == 1)[0].tolist()\n#         neg_index = np.where(target[batch_id].cpu() == 0)[0].tolist()\n#         if len(pos_index) > 0:\n#             pos_loss_bid = F.cross_entropy(input=input[batch_id][pos_index],\n#                                            target=target[batch_id][pos_index], reduction=\'none\')\n#             loss_all_pos.append(pos_loss_bid)\n#             pos_index_all.append(pos_index)\n#             min_pos_all.append(min_pos)\n#\n#             neg_loss_bid = F.cross_entropy(input=input[batch_id][neg_index],\n#                                            target=target[batch_id][neg_index], reduction=\'none\')\n#             loss_all_neg.append(neg_loss_bid)\n#             neg_index_all.append(neg_index)\n#             min_neg_all.append(min_neg)\n#         else:\n#             pos_loss_bid = torch.FloatTensor([0]).cuda()\n#             loss_all_pos.append(pos_loss_bid)\n#             pos_index_all.append(pos_index)\n#             min_pos_all.append(0)\n#\n#             neg_loss_bid = F.cross_entropy(input=input[batch_id][neg_index],\n#                                            target=target[batch_id][neg_index], reduction=\'none\')\n#             loss_all_neg.append(neg_loss_bid)\n#             neg_index_all.append(neg_index)\n#             min_neg_all.append(num_neg)\n#\n#     functools.partial(nms_worker)([anchors[pos_index_all[0]], loss_all_pos[0].cpu().detach().numpy(), min_pos_all[0]])\n#\n#     x_pos = [[anchors[pos_index_all[i]], loss_all_pos[i].cpu().detach().numpy(), min_pos_all[i]] for i in\n#              range(target.shape[0])]\n#     x_neg = [[anchors[neg_index_all[i]], loss_all_neg[i].cpu().detach().numpy(), min_neg_all[i]] for i in\n#              range(target.shape[0])]\n#     with Pool(processes=num_threads) as pool:\n#         for selected_pos_index in pool.imap(functools.partial(nms_worker), x_pos):\n#             selected_pos_index_all.append(selected_pos_index)\n#     with Pool(processes=num_threads) as pool:\n#         for selected_neg_index in pool.imap(functools.partial(nms_worker), x_neg):\n#             selected_neg_index_all.append(selected_neg_index)\n#     for batch_id in range(target.shape[0]):\n#         pos_loss_bid_ohem = loss_all_pos[batch_id][selected_pos_index_all[batch_id]]\n#         neg_loss_bid_ohem = loss_all_neg[batch_id][selected_neg_index_all[batch_id]]\n#         loss_bid = (pos_loss_bid_ohem.mean() + neg_loss_bid_ohem.mean()) / 2\n#         loss_all.append(loss_bid)\n#     final_loss = torch.stack(loss_all).mean()\n#     return final_loss\n\n\ndef rpn_cross_entropy_balance(input, target, num_pos, num_neg, anchors, ohem_pos=None, ohem_neg=None):\n    r""""""\n    :param input: (N,1125,2)\n    :param target: (15x15x5,)\n    :return:\n    """"""\n    # if ohem:\n    #     final_loss = rpn_cross_entropy_balance_parallel(input, target, num_pos, num_neg, anchors, ohem=True,\n    #                                                     num_threads=4)\n    # else:\n    loss_all = []\n    for batch_id in range(target.shape[0]):\n        min_pos = min(len(np.where(target[batch_id].cpu() == 1)[0]), num_pos)\n        min_neg = int(min(len(np.where(target[batch_id].cpu() == 1)[0]) * num_neg / num_pos, num_neg))\n        pos_index = np.where(target[batch_id].cpu() == 1)[0].tolist()\n        neg_index = np.where(target[batch_id].cpu() == 0)[0].tolist()\n\n        if ohem_pos:\n            if len(pos_index) > 0:\n                pos_loss_bid = F.cross_entropy(input=input[batch_id][pos_index],\n                                               target=target[batch_id][pos_index], reduction=\'none\')\n                selected_pos_index = nms(anchors[pos_index], pos_loss_bid.cpu().detach().numpy(), min_pos)\n                pos_loss_bid_final = pos_loss_bid[selected_pos_index]\n            else:\n                pos_loss_bid = torch.FloatTensor([0]).cuda()\n                pos_loss_bid_final = pos_loss_bid\n        else:\n            pos_index_random = random.sample(pos_index, min_pos)\n            if len(pos_index) > 0:\n                pos_loss_bid_final = F.cross_entropy(input=input[batch_id][pos_index_random],\n                                                     target=target[batch_id][pos_index_random], reduction=\'none\')\n            else:\n                pos_loss_bid_final = torch.FloatTensor([0]).cuda()\n\n        if ohem_neg:\n            if len(pos_index) > 0:\n                neg_loss_bid = F.cross_entropy(input=input[batch_id][neg_index],\n                                               target=target[batch_id][neg_index], reduction=\'none\')\n                selected_neg_index = nms(anchors[neg_index], neg_loss_bid.cpu().detach().numpy(), min_neg)\n                neg_loss_bid_final = neg_loss_bid[selected_neg_index]\n            else:\n                neg_loss_bid = F.cross_entropy(input=input[batch_id][neg_index],\n                                               target=target[batch_id][neg_index], reduction=\'none\')\n                selected_neg_index = nms(anchors[neg_index], neg_loss_bid.cpu().detach().numpy(), num_neg)\n                neg_loss_bid_final = neg_loss_bid[selected_neg_index]\n        else:\n            if len(pos_index) > 0:\n                neg_index_random = random.sample(np.where(target[batch_id].cpu() == 0)[0].tolist(), min_neg)\n                neg_loss_bid_final = F.cross_entropy(input=input[batch_id][neg_index_random],\n                                                     target=target[batch_id][neg_index_random], reduction=\'none\')\n            else:\n                neg_index_random = random.sample(np.where(target[batch_id].cpu() == 0)[0].tolist(), num_neg)\n                neg_loss_bid_final = F.cross_entropy(input=input[batch_id][neg_index_random],\n                                                     target=target[batch_id][neg_index_random], reduction=\'none\')\n        loss_bid = (pos_loss_bid_final.mean() + neg_loss_bid_final.mean()) / 2\n        loss_all.append(loss_bid)\n    final_loss = torch.stack(loss_all).mean()\n    return final_loss\n\n\ndef rpn_smoothL1(input, target, label, num_pos=16, ohem=None):\n    r\'\'\'\n    :param input: torch.Size([1, 1125, 4])\n    :param target: torch.Size([1, 1125, 4])\n            label: (torch.Size([1, 1125]) pos neg or ignore\n    :return:\n    \'\'\'\n    loss_all = []\n    for batch_id in range(target.shape[0]):\n        min_pos = min(len(np.where(label[batch_id].cpu() == 1)[0]), num_pos)\n        if ohem:\n            pos_index = np.where(label[batch_id].cpu() == 1)[0]\n            if len(pos_index) > 0:\n                loss_bid = F.smooth_l1_loss(input[batch_id][pos_index], target[batch_id][pos_index], reduction=\'none\')\n                sort_index = torch.argsort(loss_bid.mean(1))\n                loss_bid_ohem = loss_bid[sort_index[-num_pos:]]\n            else:\n                loss_bid_ohem = torch.FloatTensor([0]).cuda()[0]\n            loss_all.append(loss_bid_ohem.mean())\n        else:\n            pos_index = np.where(label[batch_id].cpu() == 1)[0]\n            pos_index = random.sample(pos_index.tolist(), min_pos)\n            if len(pos_index) > 0:\n                loss_bid = F.smooth_l1_loss(input[batch_id][pos_index], target[batch_id][pos_index])\n            else:\n                loss_bid = torch.FloatTensor([0]).cuda()[0]\n            loss_all.append(loss_bid.mean())\n    final_loss = torch.stack(loss_all).mean()\n    return final_loss\n'"
lib/utils.py,1,"b'import torch\nimport numpy as np\nimport cv2\nimport time\n\nfrom IPython import embed\n\n\ndef get_center(x):\n    return (x - 1.) / 2.\n\n\ndef xyxy2cxcywh(bbox):\n    return get_center(bbox[0] + bbox[2]), \\\n           get_center(bbox[1] + bbox[3]), \\\n           (bbox[2] - bbox[0]), \\\n           (bbox[3] - bbox[1])\n\n\ndef cxcywh2xyxy(bboxes):\n    if len(np.array(bboxes).shape) == 1:\n        bboxes = np.array(bboxes)[None, :]\n    else:\n        bboxes = np.array(bboxes)\n    x1 = bboxes[:, 0:1] + 1 / 2 - bboxes[:, 2:3] / 2\n    x2 = x1 + bboxes[:, 2:3] - 1\n    y1 = bboxes[:, 1:2] + 1 / 2 - bboxes[:, 3:4] / 2\n    y2 = y1 + bboxes[:, 3:4] - 1\n    return np.concatenate([x1, y1, x2, y2], 1)\n\n\ndef nms(bboxes, scores, num, threshold=0.7):\n    sort_index = np.argsort(scores)[::-1]\n    sort_boxes = bboxes[sort_index]\n    selected_bbox = [sort_boxes[0]]\n    selected_index = [sort_index[0]]\n    for i, bbox in enumerate(sort_boxes):\n        iou = compute_iou(selected_bbox, bbox)\n        # print(iou, bbox, selected_bbox)\n        if np.max(iou) < threshold:\n            selected_bbox.append(bbox)\n            selected_index.append(sort_index[i])\n            if len(selected_bbox) >= num:\n                break\n    return selected_index\n\n\ndef nms_worker(x, threshold=0.7):\n    bboxes, scores, num = x\n    if len(bboxes) == 0:\n        selected_index = [0]\n    else:\n        sort_index = np.argsort(scores)[::-1]\n        sort_boxes = bboxes[sort_index]\n        selected_bbox = [sort_boxes[0]]\n        selected_index = [sort_index[0]]\n        for i, bbox in enumerate(sort_boxes):\n            iou = compute_iou(selected_bbox, bbox)\n            # print(iou, bbox, selected_bbox)\n            if np.max(iou) < threshold:\n                selected_bbox.append(bbox)\n                selected_index.append(sort_index[i])\n                if len(selected_bbox) >= num:\n                    break\n    return selected_index\n\n\ndef round_up(value):\n    # \xe6\x9b\xbf\xe6\x8d\xa2\xe5\x86\x85\xe7\xbd\xaeround\xe5\x87\xbd\xe6\x95\xb0,\xe5\xae\x9e\xe7\x8e\xb0\xe4\xbf\x9d\xe7\x95\x992\xe4\xbd\x8d\xe5\xb0\x8f\xe6\x95\xb0\xe7\x9a\x84\xe7\xb2\xbe\xe7\xa1\xae\xe5\x9b\x9b\xe8\x88\x8d\xe4\xba\x94\xe5\x85\xa5\n    return round(value + 1e-6 + 1000) - 1000\n\n\ndef crop_and_pad(img, cx, cy, model_sz, original_sz, img_mean=None):\n    im_h, im_w, _ = img.shape\n\n    xmin = cx - (original_sz - 1) / 2\n    xmax = xmin + original_sz - 1\n    ymin = cy - (original_sz - 1) / 2\n    ymax = ymin + original_sz - 1\n\n    left = int(round_up(max(0., -xmin)))\n    top = int(round_up(max(0., -ymin)))\n    right = int(round_up(max(0., xmax - im_w + 1)))\n    bottom = int(round_up(max(0., ymax - im_h + 1)))\n\n    xmin = int(round_up(xmin + left))\n    xmax = int(round_up(xmax + left))\n    ymin = int(round_up(ymin + top))\n    ymax = int(round_up(ymax + top))\n    r, c, k = img.shape\n    if any([top, bottom, left, right]):\n        te_im = np.zeros((r + top + bottom, c + left + right, k), np.uint8)  # 0 is better than 1 initialization\n        te_im[top:top + r, left:left + c, :] = img\n        if top:\n            te_im[0:top, left:left + c, :] = img_mean\n        if bottom:\n            te_im[r + top:, left:left + c, :] = img_mean\n        if left:\n            te_im[:, 0:left, :] = img_mean\n        if right:\n            te_im[:, c + left:, :] = img_mean\n        im_patch_original = te_im[int(ymin):int(ymax + 1), int(xmin):int(xmax + 1), :]\n    else:\n        im_patch_original = img[int(ymin):int(ymax + 1), int(xmin):int(xmax + 1), :]\n    if not np.array_equal(model_sz, original_sz):\n        im_patch = cv2.resize(im_patch_original, (model_sz, model_sz))  # zzp: use cv to get a better speed\n    else:\n        im_patch = im_patch_original\n    scale = model_sz / im_patch_original.shape[0]\n    return im_patch, scale\n\n\ndef get_exemplar_image(img, bbox, size_z, context_amount, img_mean=None):\n    cx, cy, w, h = bbox\n    wc_z = w + context_amount * (w + h)\n    hc_z = h + context_amount * (w + h)\n    s_z = np.sqrt(wc_z * hc_z)\n    scale_z = size_z / s_z\n    exemplar_img, _ = crop_and_pad(img, cx, cy, size_z, s_z, img_mean)\n    return exemplar_img, scale_z, s_z\n\n\ndef get_instance_image(img, bbox, size_z, size_x, context_amount, img_mean=None):\n    cx, cy, w, h = bbox  # float type\n    wc_z = w + context_amount * (w + h)\n    hc_z = h + context_amount * (w + h)\n    s_z = np.sqrt(wc_z * hc_z)  # the width of the crop box\n    scale_z = size_z / s_z\n\n    s_x = s_z * size_x / size_z\n    instance_img, scale_x = crop_and_pad(img, cx, cy, size_x, s_x, img_mean)\n    w_x = w * scale_x\n    h_x = h * scale_x\n    # point_1 = (size_x + 1) / 2 - w_x / 2, (size_x + 1) / 2 - h_x / 2\n    # point_2 = (size_x + 1) / 2 + w_x / 2, (size_x + 1) / 2 + h_x / 2\n    # frame = cv2.rectangle(instance_img, (int(point_1[0]),int(point_1[1])), (int(point_2[0]),int(point_2[1])), (0, 255, 0), 2)\n    # cv2.imwrite(\'1.jpg\', frame)\n    return instance_img, w_x, h_x, scale_x\n\n\ndef box_transform(anchors, gt_box):\n    anchor_xctr = anchors[:, :1]\n    anchor_yctr = anchors[:, 1:2]\n    anchor_w = anchors[:, 2:3]\n    anchor_h = anchors[:, 3:]\n    gt_cx, gt_cy, gt_w, gt_h = gt_box\n\n    target_x = (gt_cx - anchor_xctr) / anchor_w\n    target_y = (gt_cy - anchor_yctr) / anchor_h\n    target_w = np.log(gt_w / anchor_w)\n    target_h = np.log(gt_h / anchor_h)\n    regression_target = np.hstack((target_x, target_y, target_w, target_h))\n    return regression_target\n\n\ndef box_transform_inv(anchors, offset):\n    anchor_xctr = anchors[:, :1]\n    anchor_yctr = anchors[:, 1:2]\n    anchor_w = anchors[:, 2:3]\n    anchor_h = anchors[:, 3:]\n    offset_x, offset_y, offset_w, offset_h = offset[:, :1], offset[:, 1:2], offset[:, 2:3], offset[:, 3:],\n\n    box_cx = anchor_w * offset_x + anchor_xctr\n    box_cy = anchor_h * offset_y + anchor_yctr\n    box_w = anchor_w * np.exp(offset_w)\n    box_h = anchor_h * np.exp(offset_h)\n    box = np.hstack([box_cx, box_cy, box_w, box_h])\n    return box\n\n\ndef get_topk_box(cls_score, pred_regression, anchors, topk=10):\n    # anchors xc,yc,w,h\n    regress_offset = pred_regression.cpu().detach().numpy()\n\n    scores, index = torch.topk(cls_score, topk, )\n    index = index.view(-1).cpu().detach().numpy()\n\n    topk_offset = regress_offset[index, :]\n    anchors = anchors[index, :]\n    pred_box = box_transform_inv(anchors, topk_offset)\n    return pred_box\n\n\ndef compute_iou(anchors, box):\n    if np.array(anchors).ndim == 1:\n        anchors = np.array(anchors)[None, :]\n    else:\n        anchors = np.array(anchors)\n    if np.array(box).ndim == 1:\n        box = np.array(box)[None, :]\n    else:\n        box = np.array(box)\n    gt_box = np.tile(box.reshape(1, -1), (anchors.shape[0], 1))\n\n    anchor_x1 = anchors[:, :1] - anchors[:, 2:3] / 2 + 0.5\n    anchor_x2 = anchors[:, :1] + anchors[:, 2:3] / 2 - 0.5\n    anchor_y1 = anchors[:, 1:2] - anchors[:, 3:] / 2 + 0.5\n    anchor_y2 = anchors[:, 1:2] + anchors[:, 3:] / 2 - 0.5\n\n    gt_x1 = gt_box[:, :1] - gt_box[:, 2:3] / 2 + 0.5\n    gt_x2 = gt_box[:, :1] + gt_box[:, 2:3] / 2 - 0.5\n    gt_y1 = gt_box[:, 1:2] - gt_box[:, 3:] / 2 + 0.5\n    gt_y2 = gt_box[:, 1:2] + gt_box[:, 3:] / 2 - 0.5\n\n    xx1 = np.max([anchor_x1, gt_x1], axis=0)\n    xx2 = np.min([anchor_x2, gt_x2], axis=0)\n    yy1 = np.max([anchor_y1, gt_y1], axis=0)\n    yy2 = np.min([anchor_y2, gt_y2], axis=0)\n\n    inter_area = np.max([xx2 - xx1, np.zeros(xx1.shape)], axis=0) * np.max([yy2 - yy1, np.zeros(xx1.shape)],\n                                                                           axis=0)\n    area_anchor = (anchor_x2 - anchor_x1) * (anchor_y2 - anchor_y1)\n    area_gt = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)\n    iou = inter_area / (area_anchor + area_gt - inter_area + 1e-6)\n    return iou\n\n\ndef get_pyramid_instance_image(img, center, size_x, size_x_scales, img_mean=None):\n    if img_mean is None:\n        img_mean = tuple(map(int, img.mean(axis=(0, 1))))\n    pyramid = [crop_and_pad(img, center[0], center[1], size_x, size_x_scale, img_mean)\n               for size_x_scale in size_x_scales]\n    return pyramid\n\n\ndef add_box_img(img, boxes, color=(0, 255, 0)):\n    # boxes (x,y,w,h)\n    if boxes.ndim == 1:\n        boxes = boxes[None, :]\n    img = img.copy()\n    img_ctx = (img.shape[1] - 1) / 2\n    img_cty = (img.shape[0] - 1) / 2\n    for box in boxes:\n        point_1 = [img_ctx - box[2] / 2 + box[0] + 0.5, img_cty - box[3] / 2 + box[1] + 0.5]\n        point_2 = [img_ctx + box[2] / 2 + box[0] - 0.5, img_cty + box[3] / 2 + box[1] - 0.5]\n        point_1[0] = np.clip(point_1[0], 0, img.shape[1])\n        point_2[0] = np.clip(point_2[0], 0, img.shape[1])\n        point_1[1] = np.clip(point_1[1], 0, img.shape[0])\n        point_2[1] = np.clip(point_2[1], 0, img.shape[0])\n        img = cv2.rectangle(img, (int(point_1[0]), int(point_1[1])), (int(point_2[0]), int(point_2[1])),\n                            color, 2)\n    return img\n\n\ndef adjust_learning_rate(optimizer, decay=0.1):\n    """"""Sets the learning rate to the initial LR decayed by 0.5 every 20 epochs""""""\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = decay * param_group[\'lr\']\ndef add_box_img_left_top(img, boxes, color=(0, 255, 0)):\n    # boxes (x,y,w,h)\n    if boxes.ndim == 1:\n        boxes = boxes[None, :]\n    img = img.copy()\n    for i, box in enumerate(boxes):\n        point_1 = [- box[2] / 2 + box[0] + 0.5, - box[3] / 2 + box[1] + 0.5]\n        point_2 = [+ box[2] / 2 + box[0] - 0.5, + box[3] / 2 + box[1] - 0.5]\n        point_1[0] = np.clip(point_1[0], 0, img.shape[1])\n        point_2[0] = np.clip(point_2[0], 0, img.shape[1])\n        point_1[1] = np.clip(point_1[1], 0, img.shape[0])\n        point_2[1] = np.clip(point_2[1], 0, img.shape[0])\n        img = cv2.rectangle(img, (int(point_1[0]), int(point_1[1])), (int(point_2[0]), int(point_2[1])),\n                            color, 2)\n    return img'"
lib/visual.py,0,"b""import visdom, pdb\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.switch_backend('agg')\nimport matplotlib.patches as patches\n\n\nclass visual():\n    def __init__(self, port=6003):\n        self.vis = visdom.Visdom(port=port)\n        self.counter = 0\n        self.mean = np.array([0.485, 0.456, 0.406]).reshape(1, 1, 3)\n        self.var = np.array([0.229, 0.224, 0.225]).reshape(1, 1, 3)\n\n    def denormalize(self, img):\n        img = img.cpu().detach().numpy().transpose(1, 2, 0)\n        img = np.clip(img, 0, 255).astype(np.uint8)\n        return img\n\n    def plot_error(self, errors, win=0, id_val=1):\n        if not hasattr(self, 'plot_data'):\n            self.plot_data = [{'X': [], 'Y': [], 'legend': list(errors.keys())}]\n        elif len(self.plot_data) != id_val:\n            self.plot_data.append({'X': [], 'Y': [], 'legend': list(errors.keys())})\n        id_val -= 1\n        self.plot_data[id_val]['X'].append(self.counter)\n        self.plot_data[id_val]['Y'].append([errors[k] for k in self.plot_data[id_val]['legend']])\n        self.vis.line(\n            X=np.stack([np.array(self.plot_data[id_val]['X'])] * len(self.plot_data[id_val]['legend']), 1),\n            Y=np.array(self.plot_data[id_val]['Y']),\n            opts={\n                'legend': self.plot_data[id_val]['legend'],\n                'xlabel': 'epoch',\n                'ylabel': 'loss'}, win=win)\n        self.counter += 1\n\n    def plot_img(self, img, win=1, name='img'):\n        self.vis.image(img, win=win, opts={'title': name})\n\n    def plot_img_list(self, img, name='img', win=1):\n        fig = plt.figure()\n        for i in range(len(img)):\n            ax = fig.add_subplot(1, len(img), i + 1)\n            ax.imshow(img[i])\n        self.vis.matplot(fig, win=win, opts={'title': name})\n        plt.clf()\n\n    def plot_box(self, im1, gt_box1, im2, gt_box2, box, name='img', win=1):\n        im1 = self.denormalize(im1)\n        im2 = self.denormalize(im2)\n        fig = plt.figure()\n        ax = fig.add_subplot(121)\n        ax.imshow(im1)\n        p = patches.Rectangle(\n            (gt_box1[0], gt_box1[1]), gt_box1[2] - gt_box1[0], gt_box1[3] - gt_box1[1],\n            fill=False, clip_on=False, color='r'\n        )\n        ax.add_patch(p)\n\n        ax = fig.add_subplot(122)\n        ax.imshow(im2)\n        p = patches.Rectangle(\n            (gt_box2[0], gt_box2[1]), gt_box2[2] - gt_box2[0], gt_box2[3] - gt_box2[1],\n            fill=False, clip_on=False, color='r'\n        )\n        ax.add_patch(p)\n        box = box.copy()\n        box[:, 2] -= box[:, 0]\n        box[:, 3] -= box[:, 1]\n        for i in range(box.shape[0]):\n            p = patches.Rectangle(\n                (box[i, 0], box[i, 1]), box[i, 2], box[i, 3],\n                fill=False, clip_on=False, color='b'\n            )\n            ax.add_patch(p)\n        self.vis.matplot(fig, win=win, opts={'title': name})\n        plt.clf()\n"""
net/__init__.py,0,b''
net/config.py,0,"b""import numpy as np\n\n\nclass Config:\n    # dataset related\n    exemplar_size = 127                    # exemplar size\n    instance_size = 271                    # instance size\n    context_amount = 0.5                   # context amount\n    sample_type = 'uniform'\n\n    # training related\n    exem_stretch = False\n    ohem_pos = False\n    ohem_neg = False\n    ohem_reg = False\n    fix_former_3_layers = True\n    scale_range = (0.001, 0.7)\n    ratio_range = (0.1, 10)\n    pairs_per_video_per_epoch = 2          # pairs per video\n    train_ratio = 0.99                     # training ratio of VID dataset\n    frame_range_vid = 100                  # frame range of choosing the instance\n    frame_range_ytb = 1\n    train_batch_size = 32                  # training batch size\n    valid_batch_size = 8                   # validation batch size\n    train_num_workers = 4                  # number of workers of train dataloader\n    valid_num_workers = 4                  # number of workers of validation dataloader\n    clip = 10                              # grad clip\n\n    start_lr = 3e-2\n    end_lr = 1e-5\n    epoch = 50\n    lr = np.logspace(np.log10(start_lr), np.log10(end_lr), num=epoch)[0]\n    gamma = np.logspace(np.log10(start_lr), np.log10(end_lr), num=epoch)[1] / \\\n            np.logspace(np.log10(start_lr), np.log10(end_lr), num=epoch)[0]\n                                           # decay rate of LR_Schedular\n    step_size = 1                          # step size of LR_Schedular\n    momentum = 0.9                         # momentum of SGD\n    weight_decay = 0.0005                  # weight decay of optimizator\n\n    seed = 6666                            # seed to sample training videos\n    log_dir = './data/logs'                # log dirs\n    max_translate = 12                     # max translation of random shift\n    scale_resize = 0.15                    # scale step of instance image\n    total_stride = 8                       # total stride of backbone\n    valid_scope = int((instance_size - exemplar_size) / total_stride / 2)\n    anchor_scales = np.array([8, ])\n    anchor_ratios = np.array([0.33, 0.5, 1, 2, 3])\n    anchor_num = len(anchor_scales) * len(anchor_ratios)\n    anchor_base_size = 8\n    pos_threshold = 0.6\n    neg_threshold = 0.3\n    num_pos = 16\n    num_neg = 48\n    lamb = 5\n    save_interval = 1\n    show_interval = 100\n    show_topK = 3\n    pretrained_model = '/mnt/usershare/zrq/pytorch/lab/model/zhangruiqi/finaltry/sharedata/alexnet.pth'\n\n    # tracking related\n    gray_ratio = 0.25\n    blur_ratio = 0.15\n    score_size = int((instance_size - exemplar_size) / 8 + 1)\n    penalty_k = 0.22\n    window_influence = 0.40\n    lr_box = 0.30\n    min_scale = 0.1\n    max_scale = 10\n\n\nconfig = Config()\n"""
net/dataset.py,1,"b'import torch\nimport cv2\nimport os\nimport numpy as np\nimport pickle\nimport lmdb\nimport hashlib\nimport glob\nimport xml.etree.ElementTree as ET\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data.dataset import Dataset\nfrom lib.generate_anchors import generate_anchors\nfrom .config import config\nfrom lib.utils import box_transform, compute_iou, add_box_img, crop_and_pad\n\nfrom IPython import embed\n\n\nclass ImagnetVIDDataset(Dataset):\n    def __init__(self, db, video_names, data_dir, z_transforms, x_transforms, training=True):\n        self.video_names = video_names\n        self.data_dir = data_dir\n        self.z_transforms = z_transforms\n        self.x_transforms = x_transforms\n        meta_data_path = os.path.join(data_dir, \'meta_data.pkl\')\n        self.meta_data = pickle.load(open(meta_data_path, \'rb\'))\n        self.meta_data = {x[0]: x[1] for x in self.meta_data}\n        # filter traj len less than 2\n        for key in self.meta_data.keys():\n            trajs = self.meta_data[key]\n            for trkid in list(trajs.keys()):\n                if len(trajs[trkid]) < 2:\n                    del trajs[trkid]\n\n        self.txn = db.begin(write=False)\n        self.num = len(self.video_names) if config.pairs_per_video_per_epoch is None or not training \\\n            else config.pairs_per_video_per_epoch * len(self.video_names)\n\n        # data augmentation\n        self.max_stretch = config.scale_resize\n        self.max_translate = config.max_translate\n        self.random_crop_size = config.instance_size\n        self.center_crop_size = config.exemplar_size\n\n        self.training = training\n\n        valid_scope = 2 * config.valid_scope + 1\n        self.anchors = generate_anchors(config.total_stride, config.anchor_base_size, config.anchor_scales,\n                                        config.anchor_ratios,\n                                        valid_scope)\n\n    def imread(self, path):\n        key = hashlib.md5(path.encode()).digest()\n        img_buffer = self.txn.get(key)\n        img_buffer = np.frombuffer(img_buffer, np.uint8)\n        img = cv2.imdecode(img_buffer, cv2.IMREAD_COLOR)\n        return img\n\n    def _sample_weights(self, center, low_idx, high_idx, s_type=\'uniform\'):\n        weights = list(range(low_idx, high_idx))\n        weights.remove(center)\n        weights = np.array(weights)\n        if s_type == \'linear\':\n            weights = abs(weights - center)\n        elif s_type == \'sqrt\':\n            weights = np.sqrt(abs(weights - center))\n        elif s_type == \'uniform\':\n            weights = np.ones_like(weights)\n        return weights / sum(weights)\n\n    def RandomStretch(self, sample, gt_w, gt_h):\n        scale_h = 1.0 + np.random.uniform(-self.max_stretch, self.max_stretch)\n        scale_w = 1.0 + np.random.uniform(-self.max_stretch, self.max_stretch)\n        h, w = sample.shape[:2]\n        shape = int(w * scale_w), int(h * scale_h)\n        scale_w = int(w * scale_w) / w\n        scale_h = int(h * scale_h) / h\n        gt_w = gt_w * scale_w\n        gt_h = gt_h * scale_h\n        return cv2.resize(sample, shape, cv2.INTER_LINEAR), gt_w, gt_h\n\n    def compute_target(self, anchors, box):\n        regression_target = box_transform(anchors, box)\n\n        iou = compute_iou(anchors, box).flatten()\n        # print(np.max(iou))\n        pos_index = np.where(iou > config.pos_threshold)[0]\n        neg_index = np.where(iou < config.neg_threshold)[0]\n        label = np.ones_like(iou) * -1\n        label[pos_index] = 1\n        label[neg_index] = 0\n        return regression_target, label\n\n    def __getitem__(self, idx):\n        all_idx = np.arange(self.num)\n        np.random.shuffle(all_idx)\n        all_idx = np.insert(all_idx, 0, idx, 0)\n        for idx in all_idx:\n            idx = idx % len(self.video_names)\n            video = self.video_names[idx]\n            trajs = self.meta_data[video]\n            # sample one trajs\n            if len(trajs.keys()) == 0:\n                continue\n\n            trkid = np.random.choice(list(trajs.keys()))\n            traj = trajs[trkid]\n            assert len(traj) > 1, ""video_name: {}"".format(video)\n            # sample exemplar\n            exemplar_idx = np.random.choice(list(range(len(traj))))\n            # exemplar_name = os.path.join(self.data_dir, video, traj[exemplar_idx] + "".{:02d}.x*.jpg"".format(trkid))\n\n            if \'ILSVRC2015\' in video:\n                exemplar_name = \\\n                    glob.glob(os.path.join(self.data_dir, video, traj[exemplar_idx] + "".{:02d}.x*.jpg"".format(trkid)))[\n                        0]\n            else:\n                exemplar_name = \\\n                    glob.glob(os.path.join(self.data_dir, video, traj[exemplar_idx] + "".{}.x*.jpg"".format(trkid)))[0]\n            exemplar_gt_w, exemplar_gt_h, exemplar_w_image, exemplar_h_image = \\\n                float(exemplar_name.split(\'_\')[-4]), float(exemplar_name.split(\'_\')[-3]), \\\n                float(exemplar_name.split(\'_\')[-2]), float(exemplar_name.split(\'_\')[-1][:-4])\n            exemplar_ratio = min(exemplar_gt_w / exemplar_gt_h, exemplar_gt_h / exemplar_gt_w)\n            exemplar_scale = exemplar_gt_w * exemplar_gt_h / (exemplar_w_image * exemplar_h_image)\n            if not config.scale_range[0] <= exemplar_scale < config.scale_range[1]:\n                continue\n            if not config.ratio_range[0] <= exemplar_ratio < config.ratio_range[1]:\n                continue\n\n            exemplar_img = self.imread(exemplar_name)\n            # exemplar_img = cv2.cvtColor(exemplar_img, cv2.COLOR_BGR2RGB)\n            # sample instance\n            if \'ILSVRC2015\' in exemplar_name:\n                frame_range = config.frame_range_vid\n            else:\n                frame_range = config.frame_range_ytb\n            low_idx = max(0, exemplar_idx - frame_range)\n            up_idx = min(len(traj), exemplar_idx + frame_range + 1)\n\n            # create sample weight, if the sample are far away from center\n            # the probability being choosen are high\n            weights = self._sample_weights(exemplar_idx, low_idx, up_idx, config.sample_type)\n            instance = np.random.choice(traj[low_idx:exemplar_idx] + traj[exemplar_idx + 1:up_idx], p=weights)\n\n            if \'ILSVRC2015\' in video:\n                instance_name = \\\n                    glob.glob(os.path.join(self.data_dir, video, instance + "".{:02d}.x*.jpg"".format(trkid)))[0]\n            else:\n                instance_name = glob.glob(os.path.join(self.data_dir, video, instance + "".{}.x*.jpg"".format(trkid)))[0]\n\n            instance_gt_w, instance_gt_h, instance_w_image, instance_h_image = \\\n                float(instance_name.split(\'_\')[-4]), float(instance_name.split(\'_\')[-3]), \\\n                float(instance_name.split(\'_\')[-2]), float(instance_name.split(\'_\')[-1][:-4])\n            instance_ratio = min(instance_gt_w / instance_gt_h, instance_gt_h / instance_gt_w)\n            instance_scale = instance_gt_w * instance_gt_h / (instance_w_image * instance_h_image)\n            if not config.scale_range[0] <= instance_scale < config.scale_range[1]:\n                continue\n            if not config.ratio_range[0] <= instance_ratio < config.ratio_range[1]:\n                continue\n\n            instance_img = self.imread(instance_name)\n            # instance_img = cv2.cvtColor(instance_img, cv2.COLOR_BGR2RGB)\n\n            if np.random.rand(1) < config.gray_ratio:\n                exemplar_img = cv2.cvtColor(exemplar_img, cv2.COLOR_RGB2GRAY)\n                exemplar_img = cv2.cvtColor(exemplar_img, cv2.COLOR_GRAY2RGB)\n                instance_img = cv2.cvtColor(instance_img, cv2.COLOR_RGB2GRAY)\n                instance_img = cv2.cvtColor(instance_img, cv2.COLOR_GRAY2RGB)\n            if config.exem_stretch:\n                exemplar_img, exemplar_gt_w, exemplar_gt_h = self.RandomStretch(exemplar_img, exemplar_gt_w,\n                                                                                exemplar_gt_h)\n            exemplar_img, _ = crop_and_pad(exemplar_img, (exemplar_img.shape[1] - 1) / 2,\n                                           (exemplar_img.shape[0] - 1) / 2, self.center_crop_size,\n                                           self.center_crop_size)\n\n            # exemplar_img_np = exemplar_img.copy()\n\n            instance_img, gt_w, gt_h = self.RandomStretch(instance_img, instance_gt_w, instance_gt_h)\n            im_h, im_w, _ = instance_img.shape\n            cy_o = (im_h - 1) / 2\n            cx_o = (im_w - 1) / 2\n            cy = cy_o + np.random.randint(- self.max_translate, self.max_translate + 1)\n            cx = cx_o + np.random.randint(- self.max_translate, self.max_translate + 1)\n            gt_cx = cx_o - cx\n            gt_cy = cy_o - cy\n\n            instance_img_1, scale = crop_and_pad(instance_img, cx, cy, self.random_crop_size, self.random_crop_size)\n            exemplar_img = self.z_transforms(exemplar_img)\n\n            instance_img_1 = self.x_transforms(instance_img_1)\n\n            regression_target, conf_target = self.compute_target(self.anchors,\n                                                                 np.array(list(map(round, [gt_cx, gt_cy, gt_w, gt_h]))))\n            return exemplar_img, instance_img_1, regression_target, conf_target.astype(np.int64)\n\n    def draw_img(self, img, boxes, name=\'1.jpg\', color=(0, 255, 0)):\n        # boxes (x,y,w,h)\n        img = img.copy()\n        img_ctx = (img.shape[1] - 1) / 2\n        img_cty = (img.shape[0] - 1) / 2\n        for box in boxes:\n            point_1 = img_ctx - box[2] / 2 + box[0], img_cty - box[3] / 2 + box[1]\n            point_2 = img_ctx + box[2] / 2 + box[0], img_cty + box[3] / 2 + box[1]\n            img = cv2.rectangle(img, (int(point_1[0]), int(point_1[1])), (int(point_2[0]), int(point_2[1])),\n                                color, 2)\n        cv2.imwrite(name, img)\n\n    def __len__(self):\n        return self.num\n'"
net/network.py,2,"b'import torch\nimport numpy as np\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom lib.custom_transforms import ToTensor\n\nfrom torchvision.models import alexnet\nfrom torch.autograd import Variable\nfrom torch import nn\n\nfrom IPython import embed\nfrom .config import config\n\n\nclass SiameseAlexNet(nn.Module):\n    def __init__(self, ):\n        super(SiameseAlexNet, self).__init__()\n        self.featureExtract = nn.Sequential(\n            nn.Conv2d(3, 96, 11, stride=2),\n            nn.BatchNorm2d(96),\n            nn.MaxPool2d(3, stride=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(96, 256, 5),\n            nn.BatchNorm2d(256),\n            nn.MaxPool2d(3, stride=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 384, 3),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 384, 3),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, 3),\n            nn.BatchNorm2d(256),\n        )\n        self.anchor_num = config.anchor_num\n        self.input_size = config.instance_size\n        self.score_displacement = int((self.input_size - config.exemplar_size) / config.total_stride)\n        self.conv_cls1 = nn.Conv2d(256, 256 * 2 * self.anchor_num, kernel_size=3, stride=1, padding=0)\n        self.conv_r1 = nn.Conv2d(256, 256 * 4 * self.anchor_num, kernel_size=3, stride=1, padding=0)\n\n        self.conv_cls2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=0)\n        self.conv_r2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=0)\n        self.regress_adjust = nn.Conv2d(4 * self.anchor_num, 4 * self.anchor_num, 1)\n\n    def forward(self, template, detection):\n        N = template.size(0)\n        template_feature = self.featureExtract(template)\n        detection_feature = self.featureExtract(detection)\n\n        kernel_score = self.conv_cls1(template_feature).view(N, 2 * self.anchor_num, 256, 4, 4)\n        kernel_regression = self.conv_r1(template_feature).view(N, 4 * self.anchor_num, 256, 4, 4)\n        conv_score = self.conv_cls2(detection_feature)\n        conv_regression = self.conv_r2(detection_feature)\n\n        conv_scores = conv_score.reshape(1, -1, self.score_displacement + 4, self.score_displacement + 4)\n        score_filters = kernel_score.reshape(-1, 256, 4, 4)\n        pred_score = F.conv2d(conv_scores, score_filters, groups=N).reshape(N, 10, self.score_displacement + 1,\n                                                                            self.score_displacement + 1)\n\n        conv_reg = conv_regression.reshape(1, -1, self.score_displacement + 4, self.score_displacement + 4)\n        reg_filters = kernel_regression.reshape(-1, 256, 4, 4)\n        pred_regression = self.regress_adjust(\n            F.conv2d(conv_reg, reg_filters, groups=N).reshape(N, 20, self.score_displacement + 1,\n                                                              self.score_displacement + 1))\n        return pred_score, pred_regression\n\n    def track_init(self, template):\n        N = template.size(0)\n        template_feature = self.featureExtract(template)\n\n        kernel_score = self.conv_cls1(template_feature).view(N, 2 * self.anchor_num, 256, 4, 4)\n        kernel_regression = self.conv_r1(template_feature).view(N, 4 * self.anchor_num, 256, 4, 4)\n        self.score_filters = kernel_score.reshape(-1, 256, 4, 4)\n        self.reg_filters = kernel_regression.reshape(-1, 256, 4, 4)\n\n    def track(self, detection):\n        N = detection.size(0)\n        detection_feature = self.featureExtract(detection)\n\n        conv_score = self.conv_cls2(detection_feature)\n        conv_regression = self.conv_r2(detection_feature)\n\n        conv_scores = conv_score.reshape(1, -1, self.score_displacement + 4, self.score_displacement + 4)\n        pred_score = F.conv2d(conv_scores, self.score_filters, groups=N).reshape(N, 10, self.score_displacement + 1,\n                                                                                 self.score_displacement + 1)\n        conv_reg = conv_regression.reshape(1, -1, self.score_displacement + 4, self.score_displacement + 4)\n        pred_regression = self.regress_adjust(\n            F.conv2d(conv_reg, self.reg_filters, groups=N).reshape(N, 20, self.score_displacement + 1,\n                                                                   self.score_displacement + 1))\n        return pred_score, pred_regression\n'"
net/run_SiamRPN.py,0,"b""import numpy as np\nimport time\nimport cv2\nimport glob\nimport os\nimport sys\n\nsys.path.append(os.getcwd())\nfrom .tracker import SiamRPNTracker\nfrom .config import config\nfrom tqdm import tqdm\nfrom IPython import embed\n\n\ndef run_SiamRPN(seq_path, model_path, init_box):\n    x, y, w, h = init_box\n    tracker = SiamRPNTracker(model_path)\n    res = []\n    frames = [seq_path + '/img/' + x for x in np.sort(os.listdir(seq_path + '/img'))]\n    frames = [x for x in frames if '.jpg' in x]\n    title = seq_path.split('/')[-1]\n    if title == 'David':\n        frames = frames[299:]\n    elif title == 'Football1':\n        frames = frames[0:74]\n    elif title == 'Freeman3':\n        frames = frames[0:460]\n    elif title == 'Freeman4':\n        frames = frames[0:283]\n    elif title == 'Diving':\n        frames = frames[:215]\n    elif title == 'Tiger1':\n        frames = frames[5:]\n    # starting tracking\n    tic = time.clock()\n    for idx, frame in tqdm(enumerate(frames), total=len(frames)):\n        frame = cv2.imread(frame)\n        # frame = cv2.cvtColor(cv2.imread(frame), cv2.COLOR_BGR2RGB)\n        if idx == 0:\n            tracker.init(frame, init_box)\n            bbox = (x + w / 2 - 1 / 2, y + h / 2 - 1 / 2, w, h)\n            bbox = np.array(bbox).astype(np.float64)\n        else:\n            bbox, score = tracker.update(frame)  # x,y,w,h\n            bbox = np.array(bbox)\n        res.append(list((bbox[0] - bbox[2] / 2 + 1 / 2, bbox[1] - bbox[3] / 2 + 1 / 2, bbox[2], bbox[3])))\n    duration = time.clock() - tic\n    result = {}\n    result['res'] = res\n    result['type'] = 'rect'\n    result['fps'] = round(len(frames) / duration, 3)\n    return result\n"""
net/tracker.py,5,"b'import numpy as np\nimport cv2\nimport torch\nimport torch.nn.functional as F\nimport time\nimport torchvision.transforms as transforms\n\nfrom .network import SiameseAlexNet\nfrom .config import config\nfrom lib.custom_transforms import ToTensor\nfrom lib.utils import get_exemplar_image, get_instance_image, box_transform_inv,add_box_img,add_box_img_left_top\nfrom lib.generate_anchors import generate_anchors\n\nfrom IPython import embed\n\ntorch.set_num_threads(1)  # otherwise pytorch will take all cpus\n\n\nclass SiamRPNTracker:\n    def __init__(self, model_path):\n        self.model = SiameseAlexNet()\n        # checkpoint = torch.load(model_path)\n        # if \'model\' in checkpoint.keys():\n        #     self.model.load_state_dict(torch.load(model_path)[\'model\'])\n        # else:\n        #     self.model.load_state_dict(torch.load(model_path))\n        self.model = self.model.cuda()\n        self.model.eval()\n        self.transforms = transforms.Compose([\n            ToTensor()\n        ])\n\n        valid_scope = 2 * config.valid_scope + 1\n        self.anchors = generate_anchors(config.total_stride, config.anchor_base_size, config.anchor_scales,\n                                        config.anchor_ratios,\n                                        valid_scope)\n        self.window = np.tile(np.outer(np.hanning(config.score_size), np.hanning(config.score_size))[None, :],\n                              [config.anchor_num, 1, 1]).flatten()\n\n    def _cosine_window(self, size):\n        """"""\n            get the cosine window\n        """"""\n        cos_window = np.hanning(int(size[0]))[:, np.newaxis].dot(np.hanning(int(size[1]))[np.newaxis, :])\n        cos_window = cos_window.astype(np.float32)\n        cos_window /= np.sum(cos_window)\n        return cos_window\n\n    def init(self, frame, bbox):\n        """""" initialize siamfc tracker\n        Args:\n            frame: an RGB image\n            bbox: one-based bounding box [x, y, width, height]\n        """"""\n        self.pos = np.array(\n            [bbox[0] + bbox[2] / 2 - 1 / 2, bbox[1] + bbox[3] / 2 - 1 / 2])  # center x, center y, zero based\n        # self.pos = np.array(\n        #     [bbox[0] + bbox[2] // 2, bbox[1] + bbox[3] // 2])  # same to original code\n        self.target_sz = np.array([bbox[2], bbox[3]])  # width, height\n        self.bbox = np.array([bbox[0] + bbox[2] / 2 - 1 / 2, bbox[1] + bbox[3] / 2 - 1 / 2, bbox[2], bbox[3]])\n        # self.bbox = np.array(\n        #     [bbox[0] + bbox[2] // 2, bbox[1] + bbox[3] // 2, bbox[2], bbox[3]])  # same to original code\n        self.origin_target_sz = np.array([bbox[2], bbox[3]])\n        # get exemplar img\n        self.img_mean = np.mean(frame, axis=(0, 1))\n\n        exemplar_img, scale_z, _ = get_exemplar_image(frame, self.bbox,\n                                                config.exemplar_size, config.context_amount, self.img_mean)\n        # img = add_box_img_left_top(frame,self.bbox)\n        # gt_box = np.array([0,0,scale_z*self.bbox[2],scale_z*self.bbox[3]])\n        # img = add_box_img(exemplar_img,gt_box)\n        # get exemplar feature\n        exemplar_img = self.transforms(exemplar_img)[None, :, :, :]\n        self.model.track_init(exemplar_img.cuda())\n\n    def update(self, frame):\n        """"""track object based on the previous frame\n        Args:\n            frame: an RGB image\n\n        Returns:\n            bbox: tuple of 1-based bounding box(xmin, ymin, xmax, ymax)\n        """"""\n        instance_img_np, _, _, scale_x = get_instance_image(frame, self.bbox, config.exemplar_size,\n                                                         config.instance_size,\n                                                         config.context_amount, self.img_mean)\n        instance_img = self.transforms(instance_img_np)[None, :, :, :]\n        pred_score, pred_regression = self.model.track(instance_img.cuda())\n\n        pred_conf = pred_score.reshape(-1, 2, config.anchor_num * config.score_size * config.score_size).permute(0,\n                                                                                                                 2,\n                                                                                                                 1)\n        pred_offset = pred_regression.reshape(-1, 4,\n                                              config.anchor_num * config.score_size * config.score_size).permute(0,\n                                                                                                                 2,\n                                                                                                                 1)\n        delta = pred_offset[0].cpu().detach().numpy()\n        box_pred = box_transform_inv(self.anchors, delta)\n        score_pred = F.softmax(pred_conf, dim=2)[0, :, 1].cpu().detach().numpy()\n\n        def change(r):\n            return np.maximum(r, 1. / r)\n\n        def sz(w, h):\n            pad = (w + h) * 0.5\n            sz2 = (w + pad) * (h + pad)\n            return np.sqrt(sz2)\n\n        def sz_wh(wh):\n            pad = (wh[0] + wh[1]) * 0.5\n            sz2 = (wh[0] + pad) * (wh[1] + pad)\n            return np.sqrt(sz2)\n\n        s_c = change(sz(box_pred[:, 2], box_pred[:, 3]) / (sz_wh(self.target_sz * scale_x)))  # scale penalty\n        r_c = change((self.target_sz[0] / self.target_sz[1]) / (box_pred[:, 2] / box_pred[:, 3]))  # ratio penalty\n        penalty = np.exp(-(r_c * s_c - 1.) * config.penalty_k)\n        pscore = penalty * score_pred\n        pscore = pscore * (1 - config.window_influence) + self.window * config.window_influence\n        best_pscore_id = np.argmax(pscore)\n\n        target = box_pred[best_pscore_id, :] / scale_x\n\n        lr = penalty[best_pscore_id] * score_pred[best_pscore_id] * config.lr_box\n\n        res_x = np.clip(target[0] + self.pos[0], 0, frame.shape[1])\n        res_y = np.clip(target[1] + self.pos[1], 0, frame.shape[0])\n\n        res_w = np.clip(self.target_sz[0] * (1 - lr) + target[2] * lr, config.min_scale * self.origin_target_sz[0],\n                        config.max_scale * self.origin_target_sz[0])\n        res_h = np.clip(self.target_sz[1] * (1 - lr) + target[3] * lr, config.min_scale * self.origin_target_sz[1],\n                        config.max_scale * self.origin_target_sz[1])\n\n        self.pos = np.array([res_x, res_y])\n        self.target_sz = np.array([res_w, res_h])\n        bbox = np.array([res_x, res_y, res_w, res_h])\n        self.bbox = (\n            np.clip(bbox[0], 0, frame.shape[1]).astype(np.float64),\n            np.clip(bbox[1], 0, frame.shape[0]).astype(np.float64),\n            np.clip(bbox[2], 10, frame.shape[1]).astype(np.float64),\n            np.clip(bbox[3], 10, frame.shape[0]).astype(np.float64))\n        return self.bbox, score_pred[best_pscore_id]\n'"
net/train.py,23,"b'import torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision\nimport numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport pickle\nimport lmdb\nimport torch.nn as nn\nimport time\n\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import DataLoader\nfrom glob import glob\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom tensorboardX import SummaryWriter\nfrom collections import OrderedDict\n\nfrom .config import config\nfrom .network import SiameseAlexNet\nfrom .dataset import ImagnetVIDDataset\nfrom lib.custom_transforms import Normalize, ToTensor, RandomStretch, \\\n    RandomCrop, CenterCrop, RandomBlur, ColorAug\nfrom lib.loss import rpn_smoothL1, rpn_cross_entropy_balance\nfrom lib.visual import visual\nfrom lib.utils import get_topk_box, add_box_img, compute_iou, box_transform_inv, adjust_learning_rate\n\nfrom IPython import embed\n\ntorch.manual_seed(config.seed)\n\n\ndef train(data_dir, model_path=None, vis_port=None, init=None):\n    # loading meta data\n    # -----------------------------------------------------------------------------------------------------\n    meta_data_path = os.path.join(data_dir, ""meta_data.pkl"")\n    meta_data = pickle.load(open(meta_data_path, \'rb\'))\n    all_videos = [x[0] for x in meta_data]\n\n    # split train/valid dataset\n    # -----------------------------------------------------------------------------------------------------\n    train_videos, valid_videos = train_test_split(all_videos,\n                                                  test_size=1 - config.train_ratio, random_state=config.seed)\n\n    # define transforms\n    train_z_transforms = transforms.Compose([\n        ToTensor()\n    ])\n    train_x_transforms = transforms.Compose([\n        ToTensor()\n    ])\n    valid_z_transforms = transforms.Compose([\n        ToTensor()\n    ])\n    valid_x_transforms = transforms.Compose([\n        ToTensor()\n    ])\n\n    # open lmdb\n    db = lmdb.open(data_dir + \'.lmdb\', readonly=True, map_size=int(200e9))\n\n    # create dataset\n    # -----------------------------------------------------------------------------------------------------\n    train_dataset = ImagnetVIDDataset(db, train_videos, data_dir, train_z_transforms, train_x_transforms)\n    anchors = train_dataset.anchors\n    # dic_num = {}\n    # ind_random = list(range(len(train_dataset)))\n    # import random\n    # random.shuffle(ind_random)\n    # for i in tqdm(ind_random):\n    #     exemplar_img, instance_img, regression_target, conf_target = train_dataset[i+1000]\n\n    valid_dataset = ImagnetVIDDataset(db, valid_videos, data_dir, valid_z_transforms, valid_x_transforms,\n                                      training=False)\n    # create dataloader\n    trainloader = DataLoader(train_dataset, batch_size=config.train_batch_size * torch.cuda.device_count(),\n                             shuffle=True, pin_memory=True,\n                             num_workers=config.train_num_workers * torch.cuda.device_count(), drop_last=True)\n    validloader = DataLoader(valid_dataset, batch_size=config.valid_batch_size * torch.cuda.device_count(),\n                             shuffle=False, pin_memory=True,\n                             num_workers=config.valid_num_workers * torch.cuda.device_count(), drop_last=True)\n\n    # create summary writer\n    if not os.path.exists(config.log_dir):\n        os.mkdir(config.log_dir)\n    summary_writer = SummaryWriter(config.log_dir)\n    if vis_port:\n        vis = visual(port=vis_port)\n\n    # start training\n    # -----------------------------------------------------------------------------------------------------\n    model = SiameseAlexNet()\n    model = model.cuda()\n    optimizer = torch.optim.SGD(model.parameters(), lr=config.lr,\n                                momentum=config.momentum, weight_decay=config.weight_decay)\n    # load model weight\n    # -----------------------------------------------------------------------------------------------------\n    start_epoch = 1\n    if model_path and init:\n        print(""init training with checkpoint %s"" % model_path + \'\\n\')\n        print(\'------------------------------------------------------------------------------------------------ \\n\')\n        checkpoint = torch.load(model_path)\n        if \'model\' in checkpoint.keys():\n            model.load_state_dict(checkpoint[\'model\'])\n        else:\n            model_dict = model.state_dict()\n            model_dict.update(checkpoint)\n            model.load_state_dict(model_dict)\n        del checkpoint\n        torch.cuda.empty_cache()\n        print(""inited checkpoint"")\n    elif model_path and not init:\n        print(""loading checkpoint %s"" % model_path + \'\\n\')\n        print(\'------------------------------------------------------------------------------------------------ \\n\')\n        checkpoint = torch.load(model_path)\n        start_epoch = checkpoint[\'epoch\'] + 1\n        model.load_state_dict(checkpoint[\'model\'])\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        del checkpoint\n        torch.cuda.empty_cache()\n        print(""loaded checkpoint"")\n    elif not model_path and config.pretrained_model:\n        print(""init with pretrained checkpoint %s"" % config.pretrained_model + \'\\n\')\n        print(\'------------------------------------------------------------------------------------------------ \\n\')\n        checkpoint = torch.load(config.pretrained_model)\n        # change name and load parameters\n        checkpoint = {k.replace(\'features.features\', \'featureExtract\'): v for k, v in checkpoint.items()}\n        model_dict = model.state_dict()\n        model_dict.update(checkpoint)\n        model.load_state_dict(model_dict)\n\n    # freeze layers\n    def freeze_layers(model):\n        print(\'------------------------------------------------------------------------------------------------\')\n        for layer in model.featureExtract[:10]:\n            if isinstance(layer, nn.BatchNorm2d):\n                layer.eval()\n                for k, v in layer.named_parameters():\n                    v.requires_grad = False\n            elif isinstance(layer, nn.Conv2d):\n                for k, v in layer.named_parameters():\n                    v.requires_grad = False\n            elif isinstance(layer, nn.MaxPool2d):\n                continue\n            elif isinstance(layer, nn.ReLU):\n                continue\n            else:\n                raise KeyError(\'error in fixing former 3 layers\')\n        print(""fixed layers:"")\n        print(model.featureExtract[:10])\n\n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n    for epoch in range(start_epoch, config.epoch + 1):\n        train_loss = []\n        model.train()\n        if config.fix_former_3_layers:\n            if torch.cuda.device_count() > 1:\n                freeze_layers(model.module)\n            else:\n                freeze_layers(model)\n        loss_temp_cls = 0\n        loss_temp_reg = 0\n        for i, data in enumerate(tqdm(trainloader)):\n            exemplar_imgs, instance_imgs, regression_target, conf_target = data\n            # conf_target (8,1125) (8,225x5)\n            regression_target, conf_target = regression_target.cuda(), conf_target.cuda()\n\n            pred_score, pred_regression = model(exemplar_imgs.cuda(), instance_imgs.cuda())\n\n            pred_conf = pred_score.reshape(-1, 2, config.anchor_num * config.score_size * config.score_size).permute(0,\n                                                                                                                     2,\n                                                                                                                     1)\n            pred_offset = pred_regression.reshape(-1, 4,\n                                                  config.anchor_num * config.score_size * config.score_size).permute(0,\n                                                                                                                     2,\n                                                                                                                     1)\n            cls_loss = rpn_cross_entropy_balance(pred_conf, conf_target, config.num_pos, config.num_neg, anchors,\n                                                 ohem_pos=config.ohem_pos, ohem_neg=config.ohem_neg)\n            reg_loss = rpn_smoothL1(pred_offset, regression_target, conf_target, config.num_pos, ohem=config.ohem_reg)\n            loss = cls_loss + config.lamb * reg_loss\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip)\n            optimizer.step()\n\n            step = (epoch - 1) * len(trainloader) + i\n            summary_writer.add_scalar(\'train/cls_loss\', cls_loss.data, step)\n            summary_writer.add_scalar(\'train/reg_loss\', reg_loss.data, step)\n            train_loss.append(loss.detach().cpu())\n            loss_temp_cls += cls_loss.detach().cpu().numpy()\n            loss_temp_reg += reg_loss.detach().cpu().numpy()\n            # if vis_port:\n            #     vis.plot_error({\'rpn_cls_loss\': cls_loss.detach().cpu().numpy().ravel()[0],\n            #                     \'rpn_regress_loss\': reg_loss.detach().cpu().numpy().ravel()[0]}, win=0)\n            if (i + 1) % config.show_interval == 0:\n                tqdm.write(""[epoch %2d][iter %4d] cls_loss: %.4f, reg_loss: %.4f lr: %.2e""\n                           % (epoch, i, loss_temp_cls / config.show_interval, loss_temp_reg / config.show_interval,\n                              optimizer.param_groups[0][\'lr\']))\n                loss_temp_cls = 0\n                loss_temp_reg = 0\n                if vis_port:\n                    anchors_show = train_dataset.anchors\n                    exem_img = exemplar_imgs[0].cpu().numpy().transpose(1, 2, 0)\n                    inst_img = instance_imgs[0].cpu().numpy().transpose(1, 2, 0)\n\n                    # show detected box with max score\n                    topk = config.show_topK\n                    vis.plot_img(exem_img.transpose(2, 0, 1), win=1, name=\'exemple\')\n                    cls_pred = conf_target[0]\n                    gt_box = get_topk_box(cls_pred, regression_target[0], anchors_show)[0]\n\n                    # show gt_box\n                    img_box = add_box_img(inst_img, gt_box, color=(255, 0, 0))\n                    vis.plot_img(img_box.transpose(2, 0, 1), win=2, name=\'instance\')\n\n                    # show anchor with max score\n                    cls_pred = F.softmax(pred_conf, dim=2)[0, :, 1]\n                    scores, index = torch.topk(cls_pred, k=topk)\n                    img_box = add_box_img(inst_img, anchors_show[index.cpu()])\n                    img_box = add_box_img(img_box, gt_box, color=(255, 0, 0))\n                    vis.plot_img(img_box.transpose(2, 0, 1), win=3, name=\'anchor_max_score\')\n\n                    cls_pred = F.softmax(pred_conf, dim=2)[0, :, 1]\n                    topk_box = get_topk_box(cls_pred, pred_offset[0], anchors_show, topk=topk)\n                    img_box = add_box_img(inst_img, topk_box)\n                    img_box = add_box_img(img_box, gt_box, color=(255, 0, 0))\n                    vis.plot_img(img_box.transpose(2, 0, 1), win=4, name=\'box_max_score\')\n\n                    # show anchor and detected box with max iou\n                    iou = compute_iou(anchors_show, gt_box).flatten()\n                    index = np.argsort(iou)[-topk:]\n                    img_box = add_box_img(inst_img, anchors_show[index])\n                    img_box = add_box_img(img_box, gt_box, color=(255, 0, 0))\n                    vis.plot_img(img_box.transpose(2, 0, 1), win=5, name=\'anchor_max_iou\')\n\n                    # detected box\n                    regress_offset = pred_offset[0].cpu().detach().numpy()\n                    topk_offset = regress_offset[index, :]\n                    anchors_det = anchors_show[index, :]\n                    pred_box = box_transform_inv(anchors_det, topk_offset)\n                    img_box = add_box_img(inst_img, pred_box)\n                    img_box = add_box_img(img_box, gt_box, color=(255, 0, 0))\n                    vis.plot_img(img_box.transpose(2, 0, 1), win=6, name=\'box_max_iou\')\n\n        train_loss = np.mean(train_loss)\n\n        valid_loss = []\n        model.eval()\n        for i, data in enumerate(tqdm(validloader)):\n            exemplar_imgs, instance_imgs, regression_target, conf_target = data\n\n            regression_target, conf_target = regression_target.cuda(), conf_target.cuda()\n\n            pred_score, pred_regression = model(exemplar_imgs.cuda(), instance_imgs.cuda())\n\n            pred_conf = pred_score.reshape(-1, 2, config.anchor_num * config.score_size * config.score_size).permute(0,\n                                                                                                                     2,\n                                                                                                                     1)\n            pred_offset = pred_regression.reshape(-1, 4,\n                                                  config.anchor_num * config.score_size * config.score_size).permute(0,\n                                                                                                                     2,\n                                                                                                                     1)\n            cls_loss = rpn_cross_entropy_balance(pred_conf, conf_target, config.num_pos, config.num_neg, anchors,\n                                                 ohem_pos=config.ohem_pos, ohem_neg=config.ohem_neg)\n            reg_loss = rpn_smoothL1(pred_offset, regression_target, conf_target, config.num_pos, ohem=config.ohem_reg)\n            loss = cls_loss + config.lamb * reg_loss\n            valid_loss.append(loss.detach().cpu())\n        valid_loss = np.mean(valid_loss)\n        print(""EPOCH %d valid_loss: %.4f, train_loss: %.4f"" % (epoch, valid_loss, train_loss))\n        summary_writer.add_scalar(\'valid/loss\',\n                                  valid_loss, (epoch + 1) * len(trainloader))\n        adjust_learning_rate(optimizer,\n                             config.gamma)  # adjust before save, and it will be epoch+1\'s lr when next load\n        if epoch % config.save_interval == 0:\n            if not os.path.exists(\'./data/models/\'):\n                os.makedirs(""./data/models/"")\n            save_name = ""./data/models/siamrpn_{}.pth"".format(epoch)\n            new_state_dict = model.state_dict()\n            if torch.cuda.device_count() > 1:\n                new_state_dict = OrderedDict()\n                for k, v in model.state_dict().items():\n                    namekey = k[7:]  # remove `module.`\n                    new_state_dict[namekey] = v\n            torch.save({\n                \'epoch\': epoch,\n                \'model\': new_state_dict,\n                \'optimizer\': optimizer.state_dict(),\n            }, save_name)\n            print(\'save model: {}\'.format(save_name))\n'"
